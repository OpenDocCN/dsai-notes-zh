- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:42:35'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:42:35
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2301.00394] Deep Learning Technique for Human Parsing: A Survey and Outlook'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2301.00394] 用于人类解析的深度学习技术：综述与展望'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2301.00394](https://ar5iv.labs.arxiv.org/html/2301.00394)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2301.00394](https://ar5iv.labs.arxiv.org/html/2301.00394)
- en: 'Deep Learning Technique for Human Parsing: A Survey and Outlook'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《用于人类解析的深度学习技术：综述与展望》
- en: 'Lu Yang, Wenhe Jia, Shan Li, Qing Song Lu Yang, Wenhe Jia, Shan Li, Qing Song
    are with the Beijing University of Posts and Telecommunications, Beijing, 100876,
    China (e-mail: soeaver@bupt.edu.cn; jiawh@bupt.edu.cn; ls1995@bupt.edu.cn; priv@bupt.edu.cn)
    Corresponding author: Qing Song (email: priv@bupt.edu.cn)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 陆阳、贾文赫、李珊、宋青均来自北京邮电大学，北京，100876，中国（电子邮件：soeaver@bupt.edu.cn；jiawh@bupt.edu.cn；ls1995@bupt.edu.cn；priv@bupt.edu.cn）通讯作者：宋青（电子邮件：priv@bupt.edu.cn）
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Human parsing aims to partition humans in image or video into multiple pixel-level
    semantic parts. In the last decade, it has gained significantly increased interest
    in the computer vision community and has been utilized in a broad range of practical
    applications, from security monitoring, to social media, to visual special effects,
    just to name a few. Although deep learning-based human parsing solutions have
    made remarkable achievements, many important concepts, existing challenges, and
    potential research directions are still confusing. In this survey, we comprehensively
    review three core sub-tasks: single human parsing, multiple human parsing, and
    video human parsing, by introducing their respective task settings, background
    concepts, relevant problems and applications, representative literature, and datasets.
    We also present quantitative performance comparisons of the reviewed methods on
    benchmark datasets. Additionally, to promote sustainable development of the community,
    we put forward a transformer-based human parsing framework, providing a high-performance
    baseline for follow-up research through universal, concise, and extensible solutions.
    Finally, we point out a set of under-investigated open issues in this field and
    suggest new directions for future study. We also provide a regularly updated project
    page, to continuously track recent developments in this fast-advancing field:
    [https://github.com/soeaver/awesome-human-parsing](https://github.com/soeaver/awesome-human-parsing).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人类解析旨在将图像或视频中的人分割成多个像素级语义部分。在过去十年中，这一领域在计算机视觉社区中引起了显著的关注，并已被广泛应用于安全监控、社交媒体、视觉特效等各种实际应用中。尽管基于深度学习的人类解析解决方案取得了显著成就，但许多重要概念、现有挑战以及潜在的研究方向仍然令人困惑。在这项综述中，我们全面回顾了三个核心子任务：单个人类解析、多个人类解析和视频人类解析，介绍了它们各自的任务设置、背景概念、相关问题和应用、代表性文献及数据集。我们还对所评审的方法在基准数据集上的定量性能进行了比较。此外，为了促进社区的可持续发展，我们提出了一种基于变换器的人类解析框架，通过通用、简洁和可扩展的解决方案，为后续研究提供了高性能的基准。最后，我们指出了该领域的一组尚未充分研究的开放问题，并提出了未来研究的新方向。我们还提供了一个定期更新的项目页面，以持续跟踪这一快速发展的领域的最新进展：[https://github.com/soeaver/awesome-human-parsing](https://github.com/soeaver/awesome-human-parsing)。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Human Parsing, Human Parsing Datasets, Deep Learning, Literature Survey
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人类解析，人类解析数据集，深度学习，文献综述
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Human parsing [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5)], considered as the fundamental task of human-centric visual understanding
    [[6](#bib.bib6)], aims to classify the human parts and clothing accessories in
    images or videos at pixel-level. Numerous studies have been conducted on human
    parsing due to its crucial role in widespread application areas, *e.g*., security
    monitoring, autonomous driving, social media, electronic commerce, visual special
    effects, artistic creation, giving birth to various excellent human parsing solutions
    and applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人类解析[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]，被视为以人为中心的视觉理解的基本任务[[6](#bib.bib6)]，旨在对图像或视频中的人类部位和衣物配件进行像素级分类。由于其在广泛应用领域中的关键作用，例如安全监控、自动驾驶、社交媒体、电子商务、视觉特效、艺术创作，已进行了大量研究，产生了各种优秀的人类解析解决方案和应用。
- en: '![Refer to caption](img/31053667441c87a22df39aa4f4b07914.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/31053667441c87a22df39aa4f4b07914.png)'
- en: 'Figure 1: Human parsing tasks reviewed in this survey: (a) single human parsing
    (SHP) [[7](#bib.bib7)]; (b) multiple human parsing (MHP) [[8](#bib.bib8)]; (c)
    video human parsing (VHP) [[9](#bib.bib9)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：本综述中回顾的人类解析任务：（a）单人类解析 (SHP) [[7](#bib.bib7)]；（b）多人人类解析 (MHP) [[8](#bib.bib8)]；（c）视频人类解析
    (VHP) [[9](#bib.bib9)]。
- en: As early as the beginning of this century, some studies tried to identify the
    level of upper body clothing [[10](#bib.bib10)], the grammatical representations
    of clothing [[11](#bib.bib11)] and the deformation of body contour [[12](#bib.bib12)]
    under very limited circumstances. These early studies facilitated the research
    on pixel-level human parts and clothing recognition, *i.e*., human parsing task.
    Immediately afterward, some traditional machine learning and computer vision techniques
    were utilized to solve human parsing problems, *e.g*., structured model [[13](#bib.bib13),
    [14](#bib.bib14), [1](#bib.bib1)], clustering algorithm [[15](#bib.bib15)], grammar
    model [[16](#bib.bib16), [17](#bib.bib17)], conditional random field [[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20)], template matching [[21](#bib.bib21), [22](#bib.bib22)]
    and super-pixel [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]. Afterward,
    the prosperity of deep learning and convolutional neural network [[26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32)] has further promoted the vigorous development of human parsing.
    Attention mechanism [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)],
    scale-aware features [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)],
    tree structure [[3](#bib.bib3), [41](#bib.bib41)], graph structure [[42](#bib.bib42),
    [4](#bib.bib4), [43](#bib.bib43)], edge-aware learning [[44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46)], pose-aware learning [[2](#bib.bib2), [47](#bib.bib47), [48](#bib.bib48)]
    and other technologies [[49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52)] greatly improved the performance of human parsing. However,
    some existing challenges and under-investigated issues make human parsing still
    a task worthy of further exploration.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 早在本世纪初，一些研究尝试在非常有限的情况下识别上半身服装的层次[[10](#bib.bib10)]、服装的语法表示[[11](#bib.bib11)]和身体轮廓的变形[[12](#bib.bib12)]。这些早期研究促进了对像素级人类部件和服装识别的研究，即人类解析任务。随后，一些传统的机器学习和计算机视觉技术被用于解决人类解析问题，例如，结构化模型[[13](#bib.bib13),
    [14](#bib.bib14), [1](#bib.bib1)]，聚类算法[[15](#bib.bib15)]，语法模型[[16](#bib.bib16),
    [17](#bib.bib17)]，条件随机场[[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)]，模板匹配[[21](#bib.bib21),
    [22](#bib.bib22)]和超像素[[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]。此后，深度学习和卷积神经网络[[26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32)]的繁荣进一步促进了人类解析的蓬勃发展。注意力机制[[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36)]，尺度感知特征[[37](#bib.bib37), [38](#bib.bib38),
    [39](#bib.bib39), [40](#bib.bib40)]，树结构[[3](#bib.bib3), [41](#bib.bib41)]，图结构[[42](#bib.bib42),
    [4](#bib.bib4), [43](#bib.bib43)]，边缘感知学习[[44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46)]，姿态感知学习[[2](#bib.bib2),
    [47](#bib.bib47), [48](#bib.bib48)]和其他技术[[49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52)]极大地提高了人类解析的性能。然而，一些现存的挑战和未被充分研究的问题使得人类解析仍然是一个值得进一步探索的任务。
- en: '![Refer to caption](img/6167ef5c49caeff741eb38dee59fa42c.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6167ef5c49caeff741eb38dee59fa42c.png)'
- en: 'Figure 2: Outline of this survey.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：本调查的概述。
- en: 'With the rapid development of human parsing, several literature reviews have
    been produced. However, existing surveys are not precise and in-depth: some surveys
    only provide a superficial introduction of human parsing from a macro fashion/social
    media perspective [[53](#bib.bib53), [54](#bib.bib54)], or only review a sub-task
    of human parsing from a micro face parsing perspective [[55](#bib.bib55)]. In
    addition, due to the fuzziness of taxonomy and the diversity of methods, comprehensive
    and in-depth investigation is highly needed and helpful. In response, we provide
    the first review that systematically introduces background concepts, recent advances,
    and an outlook on human parsing.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人类解析技术的快速发展，已产生了若干文献综述。然而，现有的综述既不精确也不深入：有些综述仅从宏观时尚/社交媒体视角提供了对人类解析的表面介绍[[53](#bib.bib53),
    [54](#bib.bib54)]，或者仅从微观面部解析的角度回顾了人类解析的一个子任务[[55](#bib.bib55)]。此外，由于分类模糊和方法多样性，迫切需要全面且深入的调查。为此，我们提供了首个系统介绍背景概念、最新进展及对人类解析的展望的综述。
- en: 1.1 Scope
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 范围
- en: 'This survey reviews human parsing from a comprehensive perspective, including
    not only single human parsing (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Deep Learning Technique for Human Parsing: A Survey and Outlook") (a)) but also
    multiple human parsing (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning
    Technique for Human Parsing: A Survey and Outlook") (b)) and video human parsing
    (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning Technique for Human
    Parsing: A Survey and Outlook") (c)). At the technical level, this survey focuses
    on the deep learning-based human parsing methods and datasets in recent ten years.
    To provide the necessary background, it also introduces some relevant literature
    from non-deep learning and other fields. At the practical level, the advantages
    and disadvantages of various methods are compared, and detailed performance comparisons
    are given. In addition to summarizing and analyzing the existing work, we also
    give an outlook for the future opportunities of human parsing and put forward
    a new transformer-based baseline to promote sustainable development of the community.
    A curated list of human parsing methods and datasets and the proposed transformer-based
    baseline can be found at [https://github.com/soeaver/awesome-human-parsing](https://github.com/soeaver/awesome-human-parsing).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查从全面的角度回顾了人体解析，包括单一人体解析（图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 基于深度学习的人体解析技术：调查与展望") (a)）、多人体解析（图 [1](#S1.F1
    "图 1 ‣ 1 介绍 ‣ 基于深度学习的人体解析技术：调查与展望") (b)）以及视频人体解析（图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 基于深度学习的人体解析技术：调查与展望")
    (c)）。在技术层面，本调查重点关注了过去十年基于深度学习的人体解析方法和数据集。为了提供必要的背景，还介绍了一些来自非深度学习和其他领域的相关文献。在实践层面，对各种方法的优缺点进行了比较，并给出了详细的性能比较。除了总结和分析现有工作外，我们还展望了人体解析的未来机会，并提出了一种基于新型变换器的基线，以促进社区的可持续发展。整理的人体解析方法和数据集列表以及所提出的基于变换器的基线可以在
    [https://github.com/soeaver/awesome-human-parsing](https://github.com/soeaver/awesome-human-parsing)
    找到。
- en: 1.2 Organization
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 组织结构
- en: 'Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning Technique for
    Human Parsing: A Survey and Outlook") shows the outline of this survey. §[2](#S2
    "2 Preliminaries ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")
    gives some brief background on problem formulation and challenges (§[2.1](#S2.SS1
    "2.1 Problem Formulation and Challenges ‣ 2 Preliminaries ‣ Deep Learning Technique
    for Human Parsing: A Survey and Outlook")), human parsing taxonomy (§[2.2](#S2.SS2
    "2.2 Human Parsing Taxonomy ‣ 2 Preliminaries ‣ Deep Learning Technique for Human
    Parsing: A Survey and Outlook")), relevant tasks (§[2.3](#S2.SS3 "2.3 Relevant
    Tasks ‣ 2 Preliminaries ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook")), and applications of human parsing (§[2.4](#S2.SS4 "2.4 Applications
    of Human Parsing ‣ 2 Preliminaries ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")). §[3](#S3 "3 Deep Learning Based Human Parsing ‣ Deep
    Learning Technique for Human Parsing: A Survey and Outlook") provides a detailed
    review of representative deep learning-based human parsing studies. Frequently
    used datasets and performance comparisons are reviewed in §[4](#S4 "4 Human Parsing
    Datasets ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook") and
    §[5](#S5 "5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook"). An outlook for the future opportunities of human parsing
    is presented in §[6](#S6 "6 An Outlook: Future Opportunities of Human Parsing
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook"), including
    a new transformer-based baseline (§[6.1](#S6.SS1 "6.1 A Transformer-based Baseline
    for Human Parsing ‣ 6 An Outlook: Future Opportunities of Human Parsing ‣ Deep
    Learning Technique for Human Parsing: A Survey and Outlook")), several under-investigated
    open issues (§[6.2](#S6.SS2 "6.2 Under-Investigated Open Issues ‣ 6 An Outlook:
    Future Opportunities of Human Parsing ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")) and new directions (§[6.3](#S6.SS3 "6.3 New Directions
    ‣ 6 An Outlook: Future Opportunities of Human Parsing ‣ Deep Learning Technique
    for Human Parsing: A Survey and Outlook")) for future study. Conclusions will
    be drawn in §[7](#S7 "7 Conclusions ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ 基于深度学习的人类解析技术：综述与展望")展示了本次调查的概要。§[2](#S2 "2 前言 ‣
    基于深度学习的人类解析技术：综述与展望")简要介绍了问题表述与挑战 (§[2.1](#S2.SS1 "2.1 问题表述与挑战 ‣ 2 前言 ‣ 基于深度学习的人类解析技术：综述与展望"))、人类解析分类
    (§[2.2](#S2.SS2 "2.2 人类解析分类 ‣ 2 前言 ‣ 基于深度学习的人类解析技术：综述与展望"))、相关任务 (§[2.3](#S2.SS3
    "2.3 相关任务 ‣ 2 前言 ‣ 基于深度学习的人类解析技术：综述与展望"))和人类解析的应用 (§[2.4](#S2.SS4 "2.4 人类解析的应用
    ‣ 2 前言 ‣ 基于深度学习的人类解析技术：综述与展望"))。§[3](#S3 "3 基于深度学习的人类解析 ‣ 基于深度学习的人类解析技术：综述与展望")详细回顾了代表性深度学习基础的人类解析研究。经常使用的数据集和性能比较在§[4](#S4
    "4 人类解析数据集 ‣ 基于深度学习的人类解析技术：综述与展望")和§[5](#S5 "5 性能比较 ‣ 基于深度学习的人类解析技术：综述与展望")中进行了回顾。人类解析的未来机会展望在§[6](#S6
    "6 展望：人类解析的未来机会 ‣ 基于深度学习的人类解析技术：综述与展望")中呈现，包括新的基于变换器的基线 (§[6.1](#S6.SS1 "6.1 基于变换器的人类解析基线
    ‣ 6 展望：人类解析的未来机会 ‣ 基于深度学习的人类解析技术：综述与展望"))、几个未被充分研究的开放问题 (§[6.2](#S6.SS2 "6.2 未被充分研究的开放问题
    ‣ 6 展望：人类解析的未来机会 ‣ 基于深度学习的人类解析技术：综述与展望"))和新的研究方向 (§[6.3](#S6.SS3 "6.3 新的研究方向 ‣
    6 展望：人类解析的未来机会 ‣ 基于深度学习的人类解析技术：综述与展望"))。结论将在§[7](#S7 "7 结论 ‣ 基于深度学习的人类解析技术：综述与展望")中得出。
- en: 2 Preliminaries
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 前言
- en: 2.1 Problem Formulation and Challenges
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题表述与挑战
- en: 'Formally, we use $x$ to represent input human-centric data, $y$ to represent
    pixel-level supervision target, $\mathcal{X}$ and $\mathcal{Y}$ to denote the
    space of input data and supervision target. Human parsing is to map data $x$ to
    target $y$: $\bm{\mathcal{X}}\mapsto\bm{\mathcal{Y}}$. The problem formulation
    is consistent with image segmentation [[56](#bib.bib56)], but $\mathcal{X}$ is
    limited to the human-centric space. Therefore, in many literatures, human parsing
    is regarded as fine-grained image segmentation.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，我们使用$x$表示输入的以人为中心的数据，$y$表示像素级的监督目标，$\mathcal{X}$和$\mathcal{Y}$表示输入数据和监督目标的空间。人类解析的任务是将数据$x$映射到目标$y$：$\bm{\mathcal{X}}\mapsto\bm{\mathcal{Y}}$。该问题的表述与图像分割[[56](#bib.bib56)]一致，但$\mathcal{X}$限于以人为中心的空间。因此，在许多文献中，人类解析被视为细粒度的图像分割。
- en: 'The central problem of human parsing is how to model human structures. As we
    all know, the human body presents a highly structured hierarchy, and all parts
    interact naturally. Most parsers hope to construct this interaction explicitly
    or implicitly. However, the following challenges make the problem more complicated:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 人体解析的核心问题是如何建模人体结构。众所周知，人体呈现出高度结构化的层次结构，所有部位自然地相互作用。大多数解析器希望显式或隐式地构建这种相互作用。然而，以下挑战使得问题变得更加复杂：
- en: $\bullet$ Large Intra-class Variation. In human parsing, objects with large
    visual appearance gaps may share the same semantic categories. For example, “upper
    clothes” is an abstract concept without strict visual constraints. Many kinds
    of objects of color, texture, and shape belong to this category, leading to significant
    intra-class variations. Further challenges may be added by illumination changes,
    different viewpoints, noise corruption, low-image resolution, and filtering distortion.
    Large intra-class variations will increase the difficulty of classifier learning
    decision boundaries, resulting in semantic inconsistency in prediction.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 类内大变异。在人体解析中，具有大视觉外观差异的物体可能属于相同的语义类别。例如，“上衣”是一个抽象概念，没有严格的视觉约束。许多种颜色、纹理和形状的物体都属于这个类别，导致类内变异显著。照明变化、不同视角、噪声污染、低图像分辨率和滤波失真可能会带来进一步的挑战。类内大变异将增加分类器学习决策边界的难度，导致预测中的语义不一致。
- en: $\bullet$ Unconstrained Poses. In the earlier human parsing benchmarks [[1](#bib.bib1),
    [25](#bib.bib25), [14](#bib.bib14), [37](#bib.bib37)], the data is usually collected
    from fashion media. From them people often stand or have a limited number of simple
    pose. However, in the wild, human pose is unconstrained, showing great diversity.
    Therefore, more and more studies begin to pay attention to real-world human parsing.
    Unconstrained poses will increase the state space of target geometrically, which
    brings great challenges to the human semantic representations. Moreover, the left-right
    discrimination problem in human parsing is widespread (*e.g*., left-arm vs right-arm,
    left-leg vs right-leg), and it is also severely affected by unconstrained poses
    [[49](#bib.bib49), [44](#bib.bib44), [57](#bib.bib57)].
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 不受约束的姿势。在早期的人体解析基准 [[1](#bib.bib1), [25](#bib.bib25), [14](#bib.bib14),
    [37](#bib.bib37)] 中，数据通常来自时尚媒体。在这些数据中，人们通常站立或有有限数量的简单姿势。然而，在实际场景中，人类姿势是不受约束的，表现出极大的多样性。因此，越来越多的研究开始关注现实世界中的人体解析。不受约束的姿势将几何地增加目标的状态空间，这给人体语义表示带来了极大的挑战。此外，人体解析中的左右辨别问题非常普遍（*例如*，左臂与右臂，左腿与右腿），它也受到不受约束姿势的严重影响
    [[49](#bib.bib49), [44](#bib.bib44), [57](#bib.bib57)]。
- en: '$\bullet$ Occlusion. Occlusion mainly presents two modes: (1) occlusion between
    humans and objects; (2) occlusion between humans. The former will destroy the
    continuity of human parts or clothing, resulting in incomplete apparent information
    of the targets, forming local semantic loss, and easily causing ambiguity [[37](#bib.bib37),
    [39](#bib.bib39)]. The latter is a more severe challenge. In addition to continuity
    destruction, it often causes foreground confusion. In human parsing, only the
    occluded target human is regarded as the foreground, while the others are regarded
    as the background. However, they have similar appearance, making it difficult
    to determine which part belongs to the foreground [[58](#bib.bib58)].'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 遮挡。遮挡主要表现为两种模式：（1）人与物体之间的遮挡；（2）人与人之间的遮挡。前者会破坏人体部位或衣物的连贯性，导致目标的显著信息不完整，形成局部语义丧失，并容易引起歧义
    [[37](#bib.bib37), [39](#bib.bib39)]。后者则是一个更为严重的挑战。除了连贯性的破坏，它通常还会导致前景混淆。在人体解析中，只有被遮挡的目标人体被视为前景，而其他人则被视为背景。然而，它们有相似的外观，使得确定哪个部分属于前景变得困难
    [[58](#bib.bib58)]。
- en: Remark. In addition to the above challenges, some scenario-based challenges
    also hinder the progress of human parsing, such as the trade-off between inference
    efficiency and accuracy in crowded scenes, motion blur, and camera position changes
    in movement scenes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。除了上述挑战外，一些基于场景的挑战也阻碍了人体解析的进展，例如在拥挤场景中的推理效率与准确性之间的权衡、运动模糊以及运动场景中的相机位置变化。
- en: 2.2 Human Parsing Taxonomy
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 人体解析分类
- en: 'According to the characteristics (number of humans, data modal) of the input
    space $\mathcal{X}$, human parsing can be categorized into three sub-tasks (see
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning Technique for Human
    Parsing: A Survey and Outlook")): single human parsing, multiple human parsing,
    and video human parsing.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '根据输入空间 $\mathcal{X}$ 的特征（人类数量，数据模态），人类解析可以分为三类子任务（见图 [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")）：单人解析、多人人解析和视频人类解析。'
- en: $\bullet$ Single Human Parsing (SHP). SHP is the cornerstone of human parsing,
    which assumes that there is only one foreground human instance in the image. Therefore,
    $y$ just contains corresponding semantic category supervision at the pixel-level.
    Simple and straightforward task definitions make most related research focus on
    how to model robust and generalized human parts relationship. In addition to being
    the cornerstone of human parsing, SHP is also often used as an auxiliary supervision
    for some tasks, *e.g*., person re-identification, human mesh reconstruction, virtual
    try-on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 单人解析（SHP）。SHP 是人类解析的基石，假设图像中只有一个前景人类实例。因此，$y$ 只包含像素级别的语义类别监督。简单直接的任务定义使得大多数相关研究集中于如何建模稳健且具有泛化能力的人体部件关系。除了作为人类解析的基石，SHP
    还经常用作某些任务的辅助监督，例如，*人物重识别*、*人体网格重建*、*虚拟试穿*。
- en: $\bullet$ Multiple Human Parsing (MHP). Multiple human parsing, also known as
    instance-level human parsing, aims to parse multiple human instances in a single
    pass. Besides category information, $y$ also provides instance supervision in
    pixel-level, *i.e*., the person identity of each pixel. The core problems of MHP
    are how to discriminate different human instances and how to learn each human
    feature in crowded scenes comprehensively. In addition, inference efficiency is
    also an important concern of MHP. Ideally, inference should be real-time and independent
    of human instance numbers. Except as an independent task, MHP sometimes is jointed
    with other human visual understanding tasks in a multi-task learning manner, such
    as pose estimation [[59](#bib.bib59), [60](#bib.bib60)], dense pose [[61](#bib.bib61)]
    or panoptic segmentation [[62](#bib.bib62)].
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 多人解析（MHP）。多人解析，也称为实例级别的人类解析，旨在一次性解析多个人体实例。除了类别信息，$y$ 还提供像素级别的实例监督，即每个像素的人物身份。MHP
    的核心问题是如何区分不同的人体实例以及如何在拥挤的场景中全面学习每个人体特征。此外，推理效率也是 MHP 的一个重要关注点。理想情况下，推理应实时进行，并且与人体实例的数量无关。除了作为独立任务，MHP
    有时还与其他人类视觉理解任务以多任务学习的方式联合，例如姿态估计 [[59](#bib.bib59), [60](#bib.bib60)]、密集姿态 [[61](#bib.bib61)]
    或全景分割 [[62](#bib.bib62)]。
- en: $\bullet$ Video Human Parsing (VHP). VHP needs to parse every human in the video
    data, which can be regarded as a complex visual task integrating video segmentation
    and image-level human parsing. The current VHP studies mainly adopt the unsupervised
    video object segmentation settings [[63](#bib.bib63)], *i.e*., $y$ is unknown
    in the training stage, and the ground-truth of the first frame is given in the
    inference stage. The temporal correspondence will only be approximated according
    to $x$. Relative to SHP and MHP, VHP faces more challenges that are inevitable
    in video segmentation settings, *e.g*., motion blur and camera position changes.
    Benefitting by the gradual popularity of video data, VHP has a wide range of application
    potential, and the typical cases are intelligent monitoring and video editing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 视频人类解析（VHP）。VHP 需要解析视频数据中的每个人体，这可以看作是一个复杂的视觉任务，结合了视频分割和图像级别的人体解析。目前的
    VHP 研究主要采用无监督的视频对象分割设置 [[63](#bib.bib63)]，即在训练阶段 $y$ 是未知的，而在推理阶段提供第一帧的真实数据。时间上的对应关系只能根据
    $x$ 进行近似。相对于 SHP 和 MHP，VHP 面临更多在视频分割设置中不可避免的挑战，例如，*运动模糊* 和 *摄像头位置变化*。得益于视频数据的逐渐普及，VHP
    具有广泛的应用潜力，典型案例包括智能监控和视频编辑。
- en: Remark. Over recent years, some potential research directions have also received
    attention, including weakly-supervised human parsing [[64](#bib.bib64), [51](#bib.bib51),
    [48](#bib.bib48)], one-shot human parsing [[65](#bib.bib65), [66](#bib.bib66)]
    and interactive human parsing [[67](#bib.bib67)].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。近年来，一些潜在的研究方向也受到了关注，包括弱监督人类解析 [[64](#bib.bib64), [51](#bib.bib51), [48](#bib.bib48)]、单次人类解析
    [[65](#bib.bib65), [66](#bib.bib66)] 和交互式人类解析 [[67](#bib.bib67)]。
- en: 2.3 Relevant Tasks
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 相关任务
- en: Among the research in computer vision, there are some tasks with strong relevance
    to human parsing, which are briefly described in the following.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉研究中，有一些任务与人体解析有很强的相关性，这些任务在以下内容中做了简要描述。
- en: $\bullet$ Pose Estimation. The purpose of pose estimation is to locate human
    parts and build body representations (such as skeletons) from input data. Human
    parsing and pose estimation share the same input space $\mathcal{X}$, but there
    are some differences in the supervision targets. The most crucial difference is
    that human parsing is a dense prediction task, which needs to predict the category
    of each pixel. Meanwhile, pose estimation is a sparse prediction task, only focusing
    on the location of a limited number of keypoints. These two tasks are also often
    presented in multi-task learning, or one of them is used as a guiding condition
    for the other. For example, human parsing as a guide can help pose estimation
    to reduce the impact of clothing on human appearance [[19](#bib.bib19)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 姿态估计。姿态估计的目的是定位人体部位并从输入数据中构建身体表示（例如骨架）。人体解析和姿态估计共享相同的输入空间 $\mathcal{X}$，但在监督目标上存在一些差异。最关键的区别在于人体解析是一个密集预测任务，需要预测每个像素的类别。而姿态估计是一个稀疏预测任务，仅关注有限数量的关键点的位置。这两个任务也常常在多任务学习中呈现，或者其中一个作为另一个的指导条件。例如，作为指导的人体解析可以帮助姿态估计减少衣物对人体外观的影响
    [[19](#bib.bib19)]。
- en: $\bullet$ Image Segmentation. Image segmentation is a fundamental topic in image
    processing and computer vision. It mainly includes semantic segmentation and instance
    segmentation. As a basic visual task, there are many research directions can be
    regarded as branches, and human parsing is one of them. In the pre-deep learning
    era, image segmentation focuses on the continuity of color, texture, and edge,
    while human parsing pays more attention to the body topology modeling. In the
    deep learning era, the methods in two fields show more similarities. However,
    more and more human parsing literature choose to model the parts relationship
    as the goal, which is significantly different from the general goal of image segmentation.
    Therefore, human parsing and image segmentation are closely related but independent
    problems.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 图像分割。图像分割是图像处理和计算机视觉中的一个基础主题。它主要包括语义分割和实例分割。作为一个基本的视觉任务，有许多研究方向可以视为分支，而人体解析就是其中之一。在深度学习前时代，图像分割关注颜色、纹理和边缘的连续性，而人体解析更关注身体拓扑建模。在深度学习时代，这两个领域的方法表现出更多的相似性。然而，越来越多的人体解析文献选择将部分关系建模作为目标，这与图像分割的一般目标显著不同。因此，人类解析和图像分割是紧密相关但独立的问题。
- en: Remark. Ordinarily, most human-centric dense prediction task show positively
    relevance with human parsing, *e.g*., human matting [[68](#bib.bib68), [69](#bib.bib69)],
    human mesh reconstruction [[70](#bib.bib70), [71](#bib.bib71)] and face/hand parsing
    [[72](#bib.bib72), [73](#bib.bib73)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注释。通常，大多数以人为中心的密集预测任务与人体解析呈现出积极相关性，*例如*，人类抠图 [[68](#bib.bib68), [69](#bib.bib69)]，人类网格重建
    [[70](#bib.bib70), [71](#bib.bib71)] 和面部/手部解析 [[72](#bib.bib72), [73](#bib.bib73)]。
- en: '![Refer to caption](img/39912788654f0e8a08f303b5a2e9d7c9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/39912788654f0e8a08f303b5a2e9d7c9.png)'
- en: 'Figure 3: Timeline of representative human parsing works from 2012 to 2022.
    The upper part represents the datasets of human parsing (§[4](#S4 "4 Human Parsing
    Datasets ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")),
    and the lower part represents the models of human parsing (§[3](#S3 "3 Deep Learning
    Based Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and
    Outlook")).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3：2012 至 2022 年代表性人体解析工作的时间线。上部分表示人体解析的数据集（§[4](#S4 "4 Human Parsing Datasets
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")），下部分表示人体解析的模型（§[3](#S3
    "3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")）。'
- en: 2.4 Applications of Human Parsing
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 人体解析的应用
- en: As a crucial task in computer vision, there are a large number of applications
    based on human parsing. We will introduce some common ones below.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作为计算机视觉中的一个关键任务，基于人体解析的应用有很多。我们将在下面介绍一些常见的应用。
- en: $\bullet$ Dense Pose Estimation. The goal of dense pose estimation is to map
    all human pixels in an RGB image to the 3D surface of the human body [[74](#bib.bib74)].
    Human parsing is an important pre-condition that can constrain the mapping of
    dense points. At present, the mainstream dense pose estimation methods explicitly
    integrate human parsing supervision, such as DensePose R-CNN [[74](#bib.bib74)],
    Parsing R-CNN [[61](#bib.bib61)], and SimPose [[75](#bib.bib75)]. Therefore, the
    performance of human parsing will directly affect dense pose estimation results.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ **密集姿态估计**。密集姿态估计的目标是将RGB图像中的所有人体像素映射到人体的3D表面[[74](#bib.bib74)]。人体解析是一个重要的前提条件，可以限制密集点的映射。目前，主流的密集姿态估计方法明确地整合了人体解析监督，例如**DensePose
    R-CNN**[[74](#bib.bib74)]、**Parsing R-CNN**[[61](#bib.bib61)]和**SimPose**[[75](#bib.bib75)]。因此，人体解析的性能将直接影响密集姿态估计结果。
- en: $\bullet$ Person Re-identification. Person re-identification seeks to predict
    whether two images from different cameras belong to the same person. The apparent
    characteristics of human body is an important factor affecting the accuracy. Human
    parsing can provide pixel-level semantic information, helping re-identification
    models perceive the position and composition of human parts/clothing. Various
    studies have introduced human parsing explicitly or implicitly into re-identification
    methods, which improves the model performance in multiple aspects, *e.g*., local
    visual cues [[76](#bib.bib76), [77](#bib.bib77)], spatial alignment [[78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80)], background-bias elimination [[81](#bib.bib81)],
    domain adaptation [[82](#bib.bib82)], clothes changing [[83](#bib.bib83), [84](#bib.bib84)].
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ **行人重识别**。行人重识别旨在预测来自不同摄像头的两张图像是否属于同一个人。人体的显著特征是影响准确性的一个重要因素。人体解析可以提供像素级的语义信息，帮助重识别模型感知人体部位/衣物的位置和组成。各种研究已经显式或隐式地将人体解析引入重识别方法，这在多个方面提高了模型性能，*例如*，局部视觉线索[[76](#bib.bib76),
    [77](#bib.bib77)]、空间对齐[[78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80)]、背景偏差消除[[81](#bib.bib81)]、领域适应[[82](#bib.bib82)]、换衣服[[83](#bib.bib83),
    [84](#bib.bib84)]。
- en: '$\bullet$ Virtual Try-on. Virtual try-on is a burgeoning and interesting application
    in the vision and graphic communities [[85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87),
    [88](#bib.bib88), [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91), [92](#bib.bib92)].
    Most of the research follows the three processes: human parsing, appearance generation,
    and refinement. Therefore, human parsing is a necessary step to obtain clothing
    masks, appearance constraints and pose maintenance. Recently, some work began
    to study the parser-free virtual try-on [[93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)].
    Through teacher-student learning, parsing-based pre-training, and other technologies,
    the virtual try-on can be realized without the human parsing map during inference.
    However, most of these works still introduced the parsing results during training,
    and the generation quality retains gap from parser-based methods.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ **虚拟试衣**。虚拟试衣是视觉和图形社区中的一个新兴且有趣的应用[[85](#bib.bib85), [86](#bib.bib86),
    [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92)]。大多数研究遵循三个过程：人体解析、外观生成和优化。因此，人体解析是获得衣物遮罩、外观约束和姿态维护的必要步骤。最近，一些研究开始探索**无解析虚拟试衣**[[93](#bib.bib93),
    [94](#bib.bib94), [95](#bib.bib95)]。通过教师-学生学习、基于解析的预训练等技术，可以在推理过程中无需人体解析图即可实现虚拟试衣。然而，这些工作中的大多数仍在训练过程中引入了解析结果，因此生成质量与基于解析的方法存在差距。
- en: $\bullet$ Conditional Human Image Generation. Image generation/synthesis as
    a field has seen a lot of progress in recent years [[96](#bib.bib96), [97](#bib.bib97),
    [98](#bib.bib98), [99](#bib.bib99)]. Non-existent but fidelity images can be created
    in large quantities. Among them, human image generation has attracted attention
    because of its rich downstream applications. Compared with unconditional generation,
    conditional generation can produce corresponding output as needed, and human parsing
    map is one of the most widely used pre-conditions. There have been a lot of excellent
    works on parsing-based conditional human image generation, *e.g*., CPFNet [[100](#bib.bib100)]
    and InsetGAN [[101](#bib.bib101)].
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 条件人类图像生成。近年来，图像生成/合成领域取得了很大进展[[96](#bib.bib96)，[97](#bib.bib97)，[98](#bib.bib98)，[99](#bib.bib99)]。大量高质量但虚构的图像可以被创建出来。其中，由于其丰富的下游应用，人类图像生成受到了关注。与无条件生成相比，有条件生成可以根据需要产生相应的输出，而人体关键点图是最常用的先决条件之一。在基于解析的有条件生成人类图像方面有很多优秀的作品，*例如*，CPFNet[[100](#bib.bib100)]和InsetGAN[[101](#bib.bib101)]。
- en: Remark. Besides the above cases, in general, most of the human-centric generation
    applications can be built with the help of human parsing, *e.g*., deepfakes [[102](#bib.bib102),
    [103](#bib.bib103)], style transfer [[104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106)],
    clothing editing [[107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。除了以上情况，在一般情况下，大部分以人为中心的生成应用可以借助人类解析的帮助构建，*例如*，deepfakes[[102](#bib.bib102)，[103](#bib.bib103)]，风格转换[[104](#bib.bib104)，[105](#bib.bib105)，[106](#bib.bib106)]，服装编辑[[107](#bib.bib107)，[108](#bib.bib108)，[109](#bib.bib109)]。
- en: 3 Deep Learning Based Human Parsing
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基于深度学习的人类解析
- en: 'The existing human parsing can be categorized into three sub-tasks: single
    human parsing, multiple human parsing, and video human parsing, focusing on parts
    relationship modeling, human instance discrimination, and temporal correspondence
    learning, respectively. According to this taxonomy, we sort out the representative
    works (lower part of Figure [3](#S2.F3 "Figure 3 ‣ 2.3 Relevant Tasks ‣ 2 Preliminaries
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")) and review
    them in detail below.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '现有的人类解析可以分为三个子任务：单人解析、多人解析和视频人体解析，分别侧重于部件关系建模、人体实例区分和时间对应学习。根据这个分类法，我们整理出代表性作品（图[3](#S2.F3
    "Figure 3 ‣ 2.3 Relevant Tasks ‣ 2 Preliminaries ‣ Deep Learning Technique for
    Human Parsing: A Survey and Outlook")的下部分）并对其进行详细审查。'
- en: 3.1 Single Human Parsing (SHP) Models
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 单人解析(SHP)模型
- en: 'SHP considers extracting human features through parts relationship modeling.
    According to the modeling strategy, SHP models can be divided into three main
    classes: context learning, structured representation, and multi-task learning.
    Moreover, considering some special but interesting methods, we will review them
    as “other modeling models”. Table [I](#S3.T1 "Table I ‣ 3.1 Single Human Parsing
    (SHP) Models ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for
    Human Parsing: A Survey and Outlook") summarizes the characteristics for reviewed
    SHP models.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'SHP通过部件关系建模提取人类特征。根据建模策略，SHP模型可以分为三个主要类别：上下文学习、结构化表示和多任务学习。此外，考虑到一些特殊但有趣的方法，我们将对其进行“其他建模模型”的回顾。表[I](#S3.T1
    "Table I ‣ 3.1 Single Human Parsing (SHP) Models ‣ 3 Deep Learning Based Human
    Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")总结了评审的SHP模型的特点。'
- en: 'Table I: Summary of essential characteristics for reviewed SHP models (§[3.1](#S3.SS1
    "3.1 Single Human Parsing (SHP) Models ‣ 3 Deep Learning Based Human Parsing ‣
    Deep Learning Technique for Human Parsing: A Survey and Outlook")). The training
    datasets and whether it is open source are also listed. See §[4](#S4 "4 Human
    Parsing Datasets ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")
    for more detailed descriptions of datasets. These notes also apply to the other
    tables.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '表I：总结了评审的SHP模型基本特征（§[3.1](#S3.SS1 "3.1 Single Human Parsing (SHP) Models ‣
    3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")）。训练数据集以及是否开源也在列表中列出。详细描述数据集，请参见§[4](#S4 "4 Human Parsing
    Datasets ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")。这些注释也适用于其他表格。'
- en: '|   |  |  | Context | Structured | Multi-task | Others |  |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  | 上下文 | 结构化 | 多任务 | 其他 |  |  |'
- en: '| Year | Method | Pub. | Attention | Scale-aware | Tree | Graph | Edge | Pose
    | Denoising | Adversarial | Datasets | Open Source |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 方法 | 发表 | 注意力 | 尺度感知 | 树状 | 图形 | 边缘 | 姿势 | 降噪 | 对抗性 | 数据集 | 开源 |'
- en: '| 2012 | Yamaguchi [[1](#bib.bib1)] | CVPR | - | - | - | - | - | ✓ | - | -
    | FS | - |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | Yamaguchi [[1](#bib.bib1)] | CVPR | - | - | - | - | - | ✓ | - | -
    | FS | - |'
- en: '| 2013 | DMPM [[14](#bib.bib14)] | ICCV | - | - | ✓ | - | - | - | - | - | FS/DP
    | - |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | DMPM [[14](#bib.bib14)] | ICCV | - | - | ✓ | - | - | - | - | - | FS/DP
    | - |'
- en: '| PaperDoll [[20](#bib.bib20)] | ICCV | - | - | - | - | - | ✓ | - | - | FS
    | - |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| PaperDoll [[20](#bib.bib20)] | ICCV | - | - | - | - | - | ✓ | - | - | FS
    | - |'
- en: '| CFPD [[25](#bib.bib25)] | TMM | - | - | - | - | - | ✓ | - | - | CFPD | -
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| CFPD [[25](#bib.bib25)] | TMM | - | - | - | - | - | ✓ | - | - | CFPD | -
    |'
- en: '| 2014 | HPM [[17](#bib.bib17)] | CVPR | - | - | ✓ | - | - | ✓ | - | - | FS/DP
    | - |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | HPM [[17](#bib.bib17)] | CVPR | - | - | ✓ | - | - | ✓ | - | - | FS/DP
    | - |'
- en: '| 2015 | M-CNN [[110](#bib.bib110)] | CVPR | - | - | - | ✓ | - | - | - | -
    | ATR | - |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | M-CNN [[110](#bib.bib110)] | CVPR | - | - | - | ✓ | - | - | - | -
    | ATR | - |'
- en: '| Co-CNN [[37](#bib.bib37)] | ICCV | - | ✓ | - | - | - | - | - | - | FS/ATR
    | - |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Co-CNN [[37](#bib.bib37)] | ICCV | - | ✓ | - | - | - | - | - | - | FS/ATR
    | - |'
- en: '| FPVC [[111](#bib.bib111)] | TMM | - | - | - | - | - | ✓ | - | - | FS/DP |
    - |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| FPVC [[111](#bib.bib111)] | TMM | - | - | - | - | - | ✓ | - | - | FS/DP |
    - |'
- en: '| ATR [[22](#bib.bib22)] | TPAMI | - | - | ✓ | - | - | - | - | - | FS/DP |
    - |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| ATR [[22](#bib.bib22)] | TPAMI | - | - | ✓ | - | - | - | - | - | FS/DP |
    - |'
- en: '| 2016 | AOG [[112](#bib.bib112)] | AAAI | - | - | ✓ | - | - | ✓ | - | - |
    - | - |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | AOG [[112](#bib.bib112)] | AAAI | - | - | ✓ | - | - | ✓ | - | - |
    - | - |'
- en: '| Attention [[33](#bib.bib33)] | CVPR | ✓ | ✓ | - | - | - | - | - | - | PPP
    | ✓ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Attention [[33](#bib.bib33)] | CVPR | ✓ | ✓ | - | - | - | - | - | - | PPP
    | ✓ |'
- en: '| LG-LSTM [[34](#bib.bib34)] | CVPR | ✓ | - | - | - | - | - | - | - | FS/ATR/PPP
    | - |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| LG-LSTM [[34](#bib.bib34)] | CVPR | ✓ | - | - | - | - | - | - | - | FS/ATR/PPP
    | - |'
- en: '| Graph-LSTM [[113](#bib.bib113)] | ECCV | ✓ | - | - | ✓ | - | - | - | - |
    FS/ATR/PPP | - |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Graph-LSTM [[113](#bib.bib113)] | ECCV | ✓ | - | - | ✓ | - | - | - | - |
    FS/ATR/PPP | - |'
- en: '| HAZN [[38](#bib.bib38)] | ECCV | - | ✓ | - | - | - | - | - | - | PPP | -
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| HAZN [[38](#bib.bib38)] | ECCV | - | ✓ | - | - | - | - | - | - | PPP | -
    |'
- en: '| SYSU-Clothes [[114](#bib.bib114)] | TMM | - | - | - | ✓ | - | - | - | - |
    SYSU-Clothes | - |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| SYSU-Clothes [[114](#bib.bib114)] | TMM | - | - | - | ✓ | - | - | - | - |
    SYSU-Clothes | - |'
- en: '| 2017 | Struc-LSTM [[115](#bib.bib115)] | CVPR | ✓ | - | ✓ | ✓ | - | - | -
    | - | ATR/PPP | - |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Struc-LSTM [[115](#bib.bib115)] | CVPR | ✓ | - | ✓ | ✓ | - | - | -
    | - | ATR/PPP | - |'
- en: '| SSL [[116](#bib.bib116)] | CVPR | - | - | - | - | - | ✓ | - | - | LIP/PPP
    | ✓ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| SSL [[116](#bib.bib116)] | CVPR | - | - | - | - | - | ✓ | - | - | LIP/PPP
    | ✓ |'
- en: '| Joint [[117](#bib.bib117)] | CVPR | - | - | - | - | - | ✓ | - | - | PPP |
    - |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Joint [[117](#bib.bib117)] | CVPR | - | - | - | - | - | ✓ | - | - | PPP |
    - |'
- en: '| 2018 | ProCNet [[118](#bib.bib118)] | AAAI | - | - | ✓ | - | - | - | - |
    - | PPP | - |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | ProCNet [[118](#bib.bib118)] | AAAI | - | - | ✓ | - | - | - | - |
    - | PPP | - |'
- en: '| AFLA [[49](#bib.bib49)] | AAAI | - | - | - | - | - | - | - | ✓ | LIP | -
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| AFLA [[49](#bib.bib49)] | AAAI | - | - | - | - | - | - | - | ✓ | LIP | -
    |'
- en: '| WSHP [[64](#bib.bib64)] | CVPR | - | - | - | - | - | ✓ | - | - | PPP | -
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| WSHP [[64](#bib.bib64)] | CVPR | - | - | - | - | - | ✓ | - | - | PPP | -
    |'
- en: '| TGPNet [[119](#bib.bib119)] | MM | - | ✓ | - | - | - | - | - | - | ATR |
    ✓ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| TGPNet [[119](#bib.bib119)] | MM | - | ✓ | - | - | - | - | - | - | ATR |
    ✓ |'
- en: '| MuLA [[47](#bib.bib47)] | ECCV | - | - | - | - | - | ✓ | - | - | LIP/PPP
    | - |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| MuLA [[47](#bib.bib47)] | ECCV | - | - | - | - | - | ✓ | - | - | LIP/PPP
    | - |'
- en: '| MMAN [[50](#bib.bib50)] | ECCV | - | - | - | - | - |  | - | ✓ | LIP/PPP/PPSS
    | ✓ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| MMAN [[50](#bib.bib50)] | ECCV | - | - | - | - | - |  | - | ✓ | LIP/PPP/PPSS
    | ✓ |'
- en: '| JPPNet [[2](#bib.bib2)] | TPAMI | - | - | - | - | - | ✓ | - | - | LIP/PPP
    | ✓ |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| JPPNet [[2](#bib.bib2)] | TPAMI | - | - | - | - | - | ✓ | - | - | LIP/PPP
    | ✓ |'
- en: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | - | ✓ | - | - | ✓ | - | - | - | LIP
    | ✓ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | - | ✓ | - | - | ✓ | - | - | - | LIP
    | ✓ |'
- en: '| Graphonomy [[42](#bib.bib42)] | CVPR | - | - | ✓ | ✓ | - | - | - | - | ATR/PPP
    | ✓ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Graphonomy [[42](#bib.bib42)] | CVPR | - | - | ✓ | ✓ | - | - | - | - | ATR/PPP
    | ✓ |'
- en: '| CNIF [[3](#bib.bib3)] | ICCV | - | - | ✓ | - | - | - | - | - | ATR/LIP/PPP
    | ✓ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| CNIF [[3](#bib.bib3)] | ICCV | - | - | ✓ | - | - | - | - | - | ATR/LIP/PPP
    | ✓ |'
- en: '| BSANet [[120](#bib.bib120)] | ICCV | - | ✓ | - | - | ✓ | - | - | - | PPP
    | - |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| BSANet [[120](#bib.bib120)] | ICCV | - | ✓ | - | - | ✓ | - | - | - | PPP
    | - |'
- en: '| SPGNet [[36](#bib.bib36)] | ICCV | ✓ | - | - | - | - | - | - | - | PPP |
    - |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| SPGNet [[36](#bib.bib36)] | ICCV | ✓ | - | - | - | - | - | - | - | PPP |
    - |'
- en: '| BraidNet [[57](#bib.bib57)] | MM | - | ✓ | - | - | - | - | - | - | LIP |
    - |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| BraidNet [[57](#bib.bib57)] | MM | - | ✓ | - | - | - | - | - | - | LIP |
    - |'
- en: '| 2020 | Grapy-ML [[121](#bib.bib121)] | AAAI | ✓ | - | ✓ | ✓ | - | - | - |
    - | ATR/PPP | ✓ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Grapy-ML [[121](#bib.bib121)] | AAAI | ✓ | - | ✓ | ✓ | - | - | - |
    - | ATR/PPP | ✓ |'
- en: '| HHP [[4](#bib.bib4)] | CVPR | - | - | ✓ | ✓ | - | - | - | - | ATR/LIP/PPP/PPSS
    | ✓ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| HHP [[4](#bib.bib4)] | CVPR | - | - | ✓ | ✓ | - | - | - | - | ATR/LIP/PPP/PPSS
    | ✓ |'
- en: '| SLRS [[51](#bib.bib51)] | CVPR | - | - | - | ✓ | ✓ | - | ✓ | - | ATR/LIP
    | - |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| SLRS [[51](#bib.bib51)] | CVPR | - | - | - | ✓ | ✓ | - | ✓ | - | ATR/LIP
    | - |'
- en: '| PCNet [[39](#bib.bib39)] | CVPR | - | ✓ | - | ✓ | - | - | - | - | LIP/PPP
    | - |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| PCNet [[39](#bib.bib39)] | CVPR | - | ✓ | - | ✓ | - | - | - | - | LIP/PPP
    | - |'
- en: '| CorrPM [[45](#bib.bib45)] | CVPR | - | - | - | - | ✓ | ✓ | - | - | ATR/LIP
    | ✓ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| CorrPM [[45](#bib.bib45)] | CVPR | - | - | - | - | ✓ | ✓ | - | - | ATR/LIP
    | ✓ |'
- en: '| DTCF [[46](#bib.bib46)] | MM | - | ✓ | - | - | ✓ | - | - | - | LIP/PPP |
    - |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| DTCF [[46](#bib.bib46)] | MM | - | ✓ | - | - | ✓ | - | - | - | LIP/PPP |
    - |'
- en: '| SemaTree [[41](#bib.bib41)] | ECCV | ✓ | - | ✓ | - | - | - | - | - | LIP
    | ✓ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| SemaTree [[41](#bib.bib41)] | ECCV | ✓ | - | ✓ | - | - | - | - | - | LIP
    | ✓ |'
- en: '| OCR [[122](#bib.bib122)] | ECCV | ✓ | ✓ | - | - | - | - | - | - | LIP | ✓
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| OCR [[122](#bib.bib122)] | ECCV | ✓ | ✓ | - | - | - | - | - | - | LIP | ✓
    |'
- en: '| BGNet [[123](#bib.bib123)] | ECCV | - | - | ✓ | ✓ | - | - | - | - | LIP/PPP/PPSS
    | - |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| BGNet [[123](#bib.bib123)] | ECCV | - | - | ✓ | ✓ | - | - | - | - | LIP/PPP/PPSS
    | - |'
- en: '| HRNet [[124](#bib.bib124)] | TPAMI | - | ✓ | - | - | - | - | - | - | LIP
    | ✓ |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| HRNet [[124](#bib.bib124)] | TPAMI | - | ✓ | - | - | - | - | - | - | LIP
    | ✓ |'
- en: '| SCHP [[52](#bib.bib52)] | TPAMI | - | - | - | - | ✓ | - | ✓ | - |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| SCHP [[52](#bib.bib52)] | TPAMI | - | - | - | - | ✓ | - | ✓ | - |'
- en: '&#124; ATR/LIP/PPP &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ATR/LIP/PPP &#124;'
- en: '| ✓ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ✓ |'
- en: '| 2021 | HIPN [[125](#bib.bib125)] | AAAI | - | - | - | - | - | - | ✓ | - |
    LIP/PPP | - |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | HIPN [[125](#bib.bib125)] | AAAI | - | - | - | - | - | - | ✓ | - |
    LIP/PPP | - |'
- en: '| POPNet [[65](#bib.bib65)] | AAAI | ✓ | - | - | - | - | - | - | - | ATR-OS
    | ✓ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| POPNet [[65](#bib.bib65)] | AAAI | ✓ | - | - | - | - | - | - | - | ATR-OS
    | ✓ |'
- en: '| MCIBI [[126](#bib.bib126)] | ICCV | ✓ | - | - | - | - | - | - | - | LIP |
    ✓ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| MCIBI [[126](#bib.bib126)] | ICCV | ✓ | - | - | - | - | - | - | - | LIP |
    ✓ |'
- en: '| ISNet [[127](#bib.bib127)] | ICCV | ✓ | - | - | - | - | - | - | - | LIP |
    ✓ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ISNet [[127](#bib.bib127)] | ICCV | ✓ | - | - | - | - | - | - | - | LIP |
    ✓ |'
- en: '| NPPNet [[128](#bib.bib128)] | ICCV | - | ✓ | - | - | - | ✓ | - | - | LIP/PPP
    | ✓ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| NPPNet [[128](#bib.bib128)] | ICCV | - | ✓ | - | - | - | ✓ | - | - | LIP/PPP
    | ✓ |'
- en: '| HTCorrM [[129](#bib.bib129)] | TPAMI | ✓ | - | - | - | ✓ | ✓ | - | - | ATR/LIP
    | - |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| HTCorrM [[129](#bib.bib129)] | TPAMI | ✓ | - | - | - | ✓ | ✓ | - | - | ATR/LIP
    | - |'
- en: '| PRHP [[130](#bib.bib130)] | TPAMI | - | - | ✓ | ✓ | - | - | - | - | ATR/LIP/PPP/PPSS
    | ✓ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| PRHP [[130](#bib.bib130)] | TPAMI | - | - | ✓ | ✓ | - | - | - | - | ATR/LIP/PPP/PPSS
    | ✓ |'
- en: '| 2022 | CDGNet [[131](#bib.bib131)] | CVPR | ✓ | - | - | - | ✓ | - | - | -
    | ATR/LIP | ✓ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | CDGNet [[131](#bib.bib131)] | CVPR | ✓ | - | - | - | ✓ | - | - | -
    | ATR/LIP | ✓ |'
- en: '| HSSN [[5](#bib.bib5)] | CVPR | - | ✓ | ✓ | - | - | - | - | - | LIP/PPP |
    ✓ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| HSSN [[5](#bib.bib5)] | CVPR | - | ✓ | ✓ | - | - | - | - | - | LIP/PPP |
    ✓ |'
- en: '| PRM [[43](#bib.bib43)] | TMM | - | - | - | ✓ | - | - | - | - | LIP/PPP |
    - |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| PRM [[43](#bib.bib43)] | TMM | - | - | - | ✓ | - | - | - | - | LIP/PPP |
    - |'
- en: '| PADNet [[48](#bib.bib48)] | TPAMI | - | - | - | - | - | ✓ | - | - | PPP |
    - |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| PADNet [[48](#bib.bib48)] | TPAMI | - | - | - | - | - | ✓ | - | - | PPP |
    - |'
- en: 3.1.1 Context Learning
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 上下文学习
- en: Context learning, a mainstream paradigm for single human parsing, seeks to learn
    the connection between local and global features to model human parts relationship.
    Recent studies have developed various context learning methods to handle single
    human parsing, including attention mechanism and scale-aware features.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习作为单人解析的主流范式，旨在学习局部和全局特征之间的联系，以建模人体部位之间的关系。最近的研究开发了多种上下文学习方法来处理单人解析问题，包括注意力机制和尺度感知特征。
- en: $\bullet$ Attention Mechanism. The first initiative was proposed in [[33](#bib.bib33)]
    that applies an attention mechanism for parts relationship modeling. Specifically,
    soft weights, learned by attention mechanism, are used to weight different scale
    features and merge them. At almost the same time, LG-LSTM [[34](#bib.bib34)],
    Graph-LSTM [[113](#bib.bib113)] and Struc-LSTM [[115](#bib.bib115)] exploit complex
    local and global context information through Long Short-Term Memory (LSTM) [[132](#bib.bib132)]
    and achieve very competitive results. Then, [[36](#bib.bib36)] proposes a Semantic
    Prediction Guidance (SPG) module that learns to re-weight the local features through
    the guidance from pixel-wise semantic prediction. With the rise of graph model,
    researchers realized that attention mechanism is able to establish the correlation
    between graph model nodes. For example, [[121](#bib.bib121)] introduces Graph
    Pyramid Mutual Learning (Grapy-ML) to address the cross-dataset human parsing
    problem, in which the self-attention is used to model the correlations between
    context nodes. Although attention mechanisms have achieved great results in previous
    work, global context dependency cannot be fully understood due to the lack of
    explicit prior supervision. CDGNet [[131](#bib.bib131)] adopts the human parsing
    labels accumulated in the horizontal and vertical directions as the supervisions,
    aiming to learn the position distribution of human parts, and weighting them to
    the global features through attention mechanism to achieve accurate parts relationship
    modeling.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 注意力机制。最初的倡议在 [[33](#bib.bib33)] 中提出，应用注意力机制进行部件关系建模。具体而言，注意力机制学习的软权重用于加权不同尺度的特征并将其合并。几乎同时，LG-LSTM
    [[34](#bib.bib34)]、Graph-LSTM [[113](#bib.bib113)] 和 Struc-LSTM [[115](#bib.bib115)]
    通过长短期记忆 (LSTM) [[132](#bib.bib132)] 开发了复杂的局部和全局上下文信息，取得了非常有竞争力的结果。随后，[[36](#bib.bib36)]
    提出了一个语义预测引导 (SPG) 模块，该模块通过像素级语义预测的指导来学习重新加权局部特征。随着图模型的兴起，研究人员认识到注意力机制能够建立图模型节点之间的关联。例如，[[121](#bib.bib121)]
    引入了图金字塔互学习 (Grapy-ML) 来解决跨数据集的人体解析问题，其中自注意力用于建模上下文节点之间的相关性。尽管注意力机制在以前的工作中取得了很好的结果，但由于缺乏明确的先验监督，全球上下文依赖性不能完全理解。CDGNet
    [[131](#bib.bib131)] 采用了在水平和垂直方向上积累的人体解析标签作为监督，旨在学习人体部件的位置分布，并通过注意力机制将其加权到全局特征中，以实现准确的部件关系建模。
- en: $\bullet$ Scale-aware Features. The most intuitive context learning method is
    to directly use scale-aware features (*e.g*. multi-scale features [[133](#bib.bib133),
    [134](#bib.bib134)], features pyramid networks [[135](#bib.bib135), [136](#bib.bib136)]),
    which has been widely verified in semantic segmentation [[56](#bib.bib56)]. The
    earliest effort can be tracked back to CoCNN [[37](#bib.bib37)]. It integrates
    cross layer context, global image-level context, super-pixel context, and cross
    super-pixel neighborhood context into a unified architecture, which solves the
    obstacle of low-resolution features in FCN [[31](#bib.bib31)] for modeling parts
    relationship. Subsequently, [[38](#bib.bib38)] proposes Hierarchical Auto-Zoom
    Net (HAZN), which adaptively zooms predicted image regions into their proper scales
    to refine the parsing. TGPNet [[119](#bib.bib119)] considers that the label fragmentation
    and complex annotation in human parsing datasets is a non-negligible problem to
    hinder accurate parts relationship modeling, trying to alleviate this limitation
    by supervising multi-scale context information. PCNet [[39](#bib.bib39)] further
    studies the adaptive contextual features, and captures the representative global
    context by mining the associated semantics of human parts through proposed part
    class module, relational aggregation module, and relational dispersion module.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 尺度感知特征。最直观的上下文学习方法是直接使用尺度感知特征（*例如*，多尺度特征 [[133](#bib.bib133), [134](#bib.bib134)]，特征金字塔网络
    [[135](#bib.bib135), [136](#bib.bib136)]），这些方法已在语义分割 [[56](#bib.bib56)] 中得到广泛验证。最早的努力可以追溯到
    CoCNN [[37](#bib.bib37)]。它将跨层上下文、全局图像级上下文、超像素上下文和跨超像素邻域上下文整合到一个统一的架构中，解决了 FCN
    [[31](#bib.bib31)] 中低分辨率特征在建模部件关系时的障碍。随后，[[38](#bib.bib38)] 提出了分层自适应缩放网络 (HAZN)，它自适应地将预测图像区域缩放到适当的尺度以细化解析。TGPNet
    [[119](#bib.bib119)] 认为标签碎片化和复杂的注释在人体解析数据集中是阻碍准确部件关系建模的不可忽视的问题，尝试通过监督多尺度上下文信息来缓解这一限制。PCNet
    [[39](#bib.bib39)] 进一步研究了自适应上下文特征，并通过提出的部件类别模块、关系聚合模块和关系分散模块，捕获了代表性的全局上下文，通过挖掘人体部件的相关语义。
- en: 3.1.2 Structured Representation
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 结构化表示
- en: 'The purpose of structured representation is to learn the inherent combination
    or decomposition mode of human parts, so as to model parts relationship. Research
    efforts in this field are mainly made along two directions: using a tree structure
    to represent the hierarchical relationship between body and parts, and using a
    graph structure to represent the connectivity relationship between different parts.
    These two ideas are complementary to each other, so they have often been adopted
    simultaneously in some recent work.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化表示的目的是学习人类各部分的固有组合或分解模式，以便建模各部分之间的关系。该领域的研究主要沿着两个方向进行：使用树形结构表示身体与部件之间的层次关系，以及使用图形结构表示不同部件之间的连接关系。这两个思路是相辅相成的，因此在一些最近的工作中经常被同时采用。
- en: $\bullet$ Tree Structure. DMPM [[14](#bib.bib14)] and HPM [[17](#bib.bib17)]
    solve the single human parsing issue by using the parselets representation, which
    construct a group of parsable segments by low-level over-segmentation algorithms,
    and represent these segments as leaf nodes, then search for the best graph configuration
    to obtain semantic human parsing results. Similarly, [[22](#bib.bib22)] formulates
    human parsing as an Active Template Regression (ATR) problem, where each human
    part is represented as the linear combination of learned mask templates and morphed
    to a more precise mask with the active shape parameters. Then the human parsing
    results are generated from the mask template coefficients and the active shape
    parameters. In the same line of work, ProCNet [[118](#bib.bib118)] deals with
    human parsing as a progressive recognition task, modeling structured parts relationship
    by locating the whole body and then segmenting hierarchical components gradually.
    CNIF [[3](#bib.bib3)] further extends the human tree structure and represents
    human body as a hierarchy of multi-level semantic parts, treating human parsing
    as a multi-source information fusion process. A more efficient solution is developed
    in [[41](#bib.bib41)], which uses a tree structure to encode human physiological
    composition, then designs a coarse to fine process in a cascade manner to generate
    accurate parsing results.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 树形结构。DMPM [[14](#bib.bib14)] 和 HPM [[17](#bib.bib17)] 通过使用解析表示解决了单一人体解析问题，该表示通过低级过度分割算法构建一组可解析的片段，并将这些片段表示为叶节点，然后搜索最佳图配置以获得语义人体解析结果。同样，[[22](#bib.bib22)]
    将人体解析公式化为一个主动模板回归（ATR）问题，其中每个人体部件被表示为学习到的掩膜模板的线性组合，并通过主动形状参数变形为更精确的掩膜。然后，人体解析结果由掩膜模板系数和主动形状参数生成。在同一工作方向上，ProCNet
    [[118](#bib.bib118)] 将人体解析视为一个逐步识别任务，通过定位整个身体然后逐渐分割层次组件来建模结构化部件关系。CNIF [[3](#bib.bib3)]
    进一步扩展了人体树形结构，将人体表示为多层次语义部件的层次结构，将人体解析视为一个多源信息融合过程。[[41](#bib.bib41)] 中开发了一种更高效的解决方案，该方案使用树形结构对人体生理组成进行编码，然后设计了一个粗到细的级联过程，以生成准确的解析结果。
- en: '$\bullet$ Graph Structure. Graph structure is an excellent relationship modeling
    method. Some researchers consider introducing it into human parsing networks for
    part-relation reasoning. A clothing co-parsing system is designed by [[114](#bib.bib114)],
    which takes the segmented regions as the vertices. It incorporates several contexts
    of clothing configuration to build a multi-image graphical model. To address the
    cross-dataset human parsing problem, Graphonomy [[42](#bib.bib42)] proposes a
    universal human parsing agent, introducing hierarchical graph transfer learning
    to encode the underlying label semantic elements and propagate relevant semantic
    information. BGNet [[123](#bib.bib123)] hopes to improve the accuracy of human
    parsing in similar or cluttered scenes through graph structure. It exploits the
    human inherent hierarchical structure and the relationship between different human
    parts employing grammar rules in both cascaded and paralleled manner to correct
    the segmentation performance of easily confused human parts. A landmark work on
    this line was proposed by Wang *et al*.[[4](#bib.bib4), [130](#bib.bib130)]. A
    hierarchical human parser (HHP) is constructed, representing the hierarchical
    human structure by three kinds of part relations: decomposition, composition,
    and dependency. Besides, HHP uses the prism of a message-passing, feed-back inference
    scheme to reason the human structure effectively. Following this idea, [[43](#bib.bib43)]
    proposes Part-aware Relation Modeling (PRM) to handle human parsing, generating
    features with adaptive context for various sizes and shapes of human parts.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 图结构。图结构是一种出色的关系建模方法。一些研究人员考虑将其引入到人体解析网络中以进行部件关系推理。[[114](#bib.bib114)]
    设计了一个服装共解析系统，该系统将分割区域作为顶点。它结合了服装配置的多个上下文来构建多图像图形模型。为了解决跨数据集的人体解析问题，Graphonomy
    [[42](#bib.bib42)] 提出了一个通用的人体解析代理，引入了层次图传输学习以编码潜在的标签语义元素并传播相关的语义信息。BGNet [[123](#bib.bib123)]
    希望通过图结构提高在类似或杂乱场景下的人体解析准确性。它利用人体固有的层次结构以及不同人体部位之间的关系，采用级联和并行的语法规则来纠正容易混淆的人体部位的分割性能。Wang
    *et al*.[[4](#bib.bib4), [130](#bib.bib130)] 提出了这一领域的一个标志性工作。构建了一个层次化人体解析器（HHP），通过分解、组合和依赖三种部件关系来表示层次化的人体结构。此外，HHP
    使用消息传递反馈推理方案的视角来有效推理人体结构。沿着这一思路，[[43](#bib.bib43)] 提出了部件感知关系建模（PRM）来处理人体解析，为各种大小和形状的人体部件生成具有自适应上下文的特征。
- en: 3.1.3 Multi-task Learning
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 多任务学习
- en: The auxiliary supervisions can help the parser better understand the relationship
    between parts, such as part edges or human pose. Therefore, multi-task learning
    has become an essential paradigm for single human parsing.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助监督可以帮助解析器更好地理解部件之间的关系，例如部件边缘或人体姿态。因此，多任务学习已经成为单一人体解析的重要范式。
- en: $\bullet$ Edge-aware Learning. Edge information is implicit in the human parsing
    dataset. Thus edge-aware supervision or feature can be introduced into the human
    parser without additional labeling costs. In particular, edge-aware learning can
    enhance the model’s ability to discriminate adjacent parts and improve the fineness
    of part boundaries. The typical work is [[44](#bib.bib44)], which proposes a Context
    Embedding with Edge Perceiving (CE2P) framework, using an edge perceiving module
    to integrate the characteristic of object contour to refine the part boundaries.
    Because of its excellent performance and scalability, CE2P has become the baseline
    for many subsequent works. CorrPM [[45](#bib.bib45)] and HTCorrM [[129](#bib.bib129)]
    are built on CE2P, and further use part edges to help model the parts relationship.
    They construct a heterogeneous non-local module to mix the edge, pose and semantic
    features into a hybrid representation, and explore the spatial affinity between
    the hybrid representation and the parsing feature map at all positions. BSANet
    [[120](#bib.bib120)] considers that edge information is helpful to eliminate the
    part-level ambiguities and proposes a joint parsing framework with boundary and
    semantic awareness to address this issue. Specifically, a boundary-aware module
    is employed to make intermediate-level features focus on part boundaries for accurate
    localization, which is then fused with high-level features for efficient part
    recognition. To further enrich the edge-aware features, a dual-task cascaded framework
    (DTCF) is developed in [[46](#bib.bib46)], which implicitly integrates parsing
    and edge features to refine the human parsing results progressively.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 边缘感知学习。边缘信息在人体解析数据集中是隐含的。因此，边缘感知的监督或特征可以在没有额外标注成本的情况下引入到人体解析器中。特别是，边缘感知学习可以增强模型区分相邻部件的能力，并改善部件边界的精细度。典型的工作是
    [[44](#bib.bib44)]，提出了一种具有边缘感知的上下文嵌入（CE2P）框架，使用边缘感知模块将对象轮廓的特征融入以细化部件边界。由于其优异的性能和可扩展性，CE2P
    已成为许多后续工作的基准。CorrPM [[45](#bib.bib45)] 和 HTCorrM [[129](#bib.bib129)] 是建立在 CE2P
    上的，进一步利用部件边缘帮助模型处理部件关系。它们构建了一个异质非局部模块，将边缘、姿态和语义特征混合成一个混合表示，并探索混合表示与解析特征图在所有位置之间的空间亲和力。BSANet
    [[120](#bib.bib120)] 认为边缘信息有助于消除部件级别的模糊性，提出了一个具有边界和语义感知的联合解析框架来解决这个问题。具体而言，采用一个边界感知模块使中间级特征专注于部件边界以实现准确定位，然后将其与高层特征融合以实现高效的部件识别。为了进一步丰富边缘感知特征，[[46](#bib.bib46)]
    中开发了一个双任务级联框架（DTCF），该框架隐式地整合了解析和边缘特征，以逐步细化人体解析结果。
- en: $\bullet$ Pose-aware Learning. Both human parsing and pose estimation seek to
    predict dense and structured human representation. There is a high intrinsic relationship
    between them. Therefore, some studies have tried to use pose-aware learning to
    assist in parts relationship modeling. As early as 2012, Yamaguchi *et al*. [[1](#bib.bib1),
    [20](#bib.bib20)] exploited the relationship between clothing and the underlying
    body pose, exploring techniques to accurately parse person wearing clothing into
    their constituent garment pieces. Almost immediately, Liu *et al*. [[25](#bib.bib25)]
    combined the human pose estimation module with an MRF-based color/category inference
    module and a super-pixel category classifier module to parse fashion items in
    images. Subsequently, Liu *et al*. [[111](#bib.bib111)] extends this idea to semi-supervised
    human parsing, collecting a large number of unlabeled videos, using cross-frame
    context for human pose co-estimation, and then performs video joint human parsing.
    SSL [[116](#bib.bib116)] and JPPNet [[2](#bib.bib2)] choose to impose human pose
    structures into parsing results without resorting to extra supervision, and adopt
    the multi-task learning manner to explore efficient human parts relationship modeling.
    A similar work is developed by [[47](#bib.bib47)], which presents a Mutual Learning
    to Adapt model (MuLA) for joint human parsing and pose estimation. MuLA can fast
    adjust the parsing and pose models to provide more robust and accurate results
    by incorporating information from corresponding models. Different from the above
    work, Zeng *et al*. [[128](#bib.bib128)]. focus on how to automatically design
    a unified model and perform two tasks simultaneously to benefit each other. Inspired
    by NAS [[137](#bib.bib137)], they propose to search for an efficient network architecture
    (NPPNet), searching the encoder-decoder architectures respectively, and embed
    NAS units in both multi-scale feature interaction and high-level feature fusion.
    To get rid of annotating pixel-wise human parts masks, a weakly-supervised human
    parsing approach is proposed by PADNet [[48](#bib.bib48)]. They develop an iterative
    training framework to transform pose knowledge into part priors, so that only
    pose annotations are required during training, greatly alleviating the annotation
    burdens.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 姿态感知学习。人体解析和姿态估计都试图预测密集和结构化的人体表示，它们之间存在高度的内在关系。因此，一些研究尝试使用姿态感知学习来辅助部件关系建模。早在2012年，Yamaguchi
    *et al*. [[1](#bib.bib1), [20](#bib.bib20)] 探索了服装与身体姿态之间的关系，研究了将穿着服装的人准确解析为其组成衣物片段的技术。几乎立即，Liu
    *et al*. [[25](#bib.bib25)] 将人体姿态估计模块与基于MRF的颜色/类别推断模块和超像素类别分类器模块结合，用于解析图像中的时尚物品。随后，Liu
    *et al*. [[111](#bib.bib111)] 将这一思想扩展到半监督人体解析，收集大量未标记的视频，使用跨帧上下文进行人体姿态共同估计，然后进行视频联合人体解析。SSL
    [[116](#bib.bib116)] 和 JPPNet [[2](#bib.bib2)] 选择将人体姿态结构强加到解析结果中，而不依赖额外的监督，并采用多任务学习方式探索有效的人体部件关系建模。类似的工作由
    [[47](#bib.bib47)] 开发，该工作提出了一种互学习适应模型（MuLA），用于联合人体解析和姿态估计。MuLA 可以通过整合来自相关模型的信息，快速调整解析和姿态模型，以提供更强健和准确的结果。与上述工作不同，Zeng
    *et al*. [[128](#bib.bib128)] 专注于如何自动设计一个统一模型并同时执行两个任务以相互受益。受到 NAS [[137](#bib.bib137)]
    的启发，他们提出了搜索高效网络架构（NPPNet），分别搜索编码器-解码器架构，并将 NAS 单元嵌入多尺度特征交互和高级特征融合中。为了摆脱标注像素级人体部件掩码，PADNet
    [[48](#bib.bib48)] 提出了一个弱监督人体解析方法。他们开发了一个迭代训练框架，将姿态知识转化为部件先验，从而在训练过程中只需要姿态注释，大大减轻了标注负担。
- en: 'Table II: Highlights of parts relationship modeling methods for SHP models
    (§[3.1](#S3.SS1 "3.1 Single Human Parsing (SHP) Models ‣ 3 Deep Learning Based
    Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")).
    Representative Works of each method are also give.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: SHP 模型的部件关系建模方法的亮点 (§[3.1](#S3.SS1 "3.1 单人体解析（SHP）模型 ‣ 3 深度学习基于的人体解析
    ‣ 深度学习技术用于人体解析：调查与展望"))。每种方法的代表性工作也有所给出。'
- en: '|   Method |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|   方法 |'
- en: '&#124; Representative &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代表性 &#124;'
- en: '&#124; Works &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 工作 &#124;'
- en: '| Highlights |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 亮点 |'
- en: '| Attention |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 注意 |'
- en: '&#124; [[33](#bib.bib33), [34](#bib.bib34), [113](#bib.bib113)] &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[33](#bib.bib33), [34](#bib.bib34), [113](#bib.bib113)] &#124;'
- en: '&#124; [[115](#bib.bib115), [36](#bib.bib36), [121](#bib.bib121)] &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[115](#bib.bib115), [36](#bib.bib36), [121](#bib.bib121)] &#124;'
- en: '&#124; [[131](#bib.bib131)] &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[131](#bib.bib131)] &#124;'
- en: '|'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; It is helpful to locate interested human parts, &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 有助于定位感兴趣的人的部分， &#124;'
- en: '&#124; suppress useless background information. &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 抑制无用的背景信息。 &#124;'
- en: '|'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Scale-aware |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 规模感知 |'
- en: '&#124; [[37](#bib.bib37), [38](#bib.bib38), [119](#bib.bib119)] &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[37](#bib.bib37), [38](#bib.bib38), [119](#bib.bib119)] &#124;'
- en: '&#124; [[39](#bib.bib39)] &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[39](#bib.bib39)] &#124;'
- en: '|'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fusion low-level texture and high-level semantic &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低级纹理与高级语义融合 &#124;'
- en: '&#124; features, help to parse small human parts. &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征，帮助解析小的人体部位。 &#124;'
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Tree |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 树 |'
- en: '&#124; [[14](#bib.bib14), [17](#bib.bib17), [22](#bib.bib22)] &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[14](#bib.bib14), [17](#bib.bib17), [22](#bib.bib22)] &#124;'
- en: '&#124; [[118](#bib.bib118), [3](#bib.bib3), [41](#bib.bib41)] &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[118](#bib.bib118), [3](#bib.bib3), [41](#bib.bib41)] &#124;'
- en: '|'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Simulate the composition and decomposition &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟组成和分解 &#124;'
- en: '&#124; relationship between human parts and body. &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人体部位与身体之间的关系。 &#124;'
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Graph |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 图 |'
- en: '&#124; [[114](#bib.bib114), [42](#bib.bib42), [123](#bib.bib123)] &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[114](#bib.bib114), [42](#bib.bib42), [123](#bib.bib123)] &#124;'
- en: '&#124; [[4](#bib.bib4), [130](#bib.bib130), [43](#bib.bib43)] &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[4](#bib.bib4), [130](#bib.bib130), [43](#bib.bib43)] &#124;'
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Modeling the correlation and difference between &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 建模解析结果与身体结构之间的相关性和差异 &#124;'
- en: '&#124; human parts. &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人体部位。 &#124;'
- en: '|'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Edge |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 边缘 |'
- en: '&#124; [[44](#bib.bib44), [45](#bib.bib45), [129](#bib.bib129)] &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[44](#bib.bib44), [45](#bib.bib45), [129](#bib.bib129)] &#124;'
- en: '&#124; [[120](#bib.bib120), [46](#bib.bib46)] &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[120](#bib.bib120), [46](#bib.bib46)] &#124;'
- en: '|'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Solve the pixel confusion problem on the boundary &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 解决边界上的像素混淆问题 &#124;'
- en: '&#124; of adjacent parts, generating finer boundary. &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成更精细的边界。 &#124;'
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Pose |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 姿态 |'
- en: '&#124; [[1](#bib.bib1), [20](#bib.bib20), [25](#bib.bib25)] &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[1](#bib.bib1), [20](#bib.bib20), [25](#bib.bib25)] &#124;'
- en: '&#124; [[111](#bib.bib111), [116](#bib.bib116), [2](#bib.bib2)] &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[111](#bib.bib111), [116](#bib.bib116), [2](#bib.bib2)] &#124;'
- en: '&#124; [[47](#bib.bib47), [128](#bib.bib128), [48](#bib.bib48)] &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[47](#bib.bib47), [128](#bib.bib128), [48](#bib.bib48)] &#124;'
- en: '|'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; As context clues to improve semantic consistency &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 作为上下文线索以提高语义一致性 &#124;'
- en: '&#124; between parsing results and body structure. &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 解析结果与身体结构之间的差异。 &#124;'
- en: '|'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Denoising |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 去噪 |'
- en: '&#124; [[51](#bib.bib51), [52](#bib.bib52), [125](#bib.bib125)] &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[51](#bib.bib51), [52](#bib.bib52), [125](#bib.bib125)] &#124;'
- en: '|'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Alleviate the impact of super-pixel or annotation &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 缓解超像素或标注的影响 &#124;'
- en: '&#124; errors, improving the robustness. &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误，提高鲁棒性。 &#124;'
- en: '|'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Adversarial |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 对抗 |'
- en: '&#124; [[49](#bib.bib49), [50](#bib.bib50)] &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[49](#bib.bib49), [50](#bib.bib50)] &#124;'
- en: '|'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Reduce the domain differences between training &#124;'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 减少训练数据与测试数据之间的领域差异 &#124;'
- en: '&#124; data and testing data, improving the generalization. &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据和测试数据，提高泛化能力。 &#124;'
- en: '|'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 3.1.4 Other Modeling Models
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 其他建模模型
- en: Other works attempt to employ techniques outside of the above taxonomy, such
    as denoising and adversarial learning, which also make specific contributions
    to the human parts relationship modeling and deserve a separate look.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 其他工作尝试使用上述分类之外的技术，如去噪和对抗学习，这些技术也对人体部位关系建模做出了特定贡献，值得单独探讨。
- en: $\bullet$ Denoising. To reduce the labeling cost, there is a large amount of
    noise in the mainstream SHP datasets [[22](#bib.bib22), [116](#bib.bib116)], so
    denoising learning for accurate human parts relationship modeling has also received
    some attention. SCHP [[52](#bib.bib52)] is the most representative work. It starts
    with using inaccurate parsing labels as the initialization and designs a cyclically
    learning scheduler to infer more reliable pseudo labels In the same period, Li
    *et al*. [[51](#bib.bib51)] attempt to combine denoising learning and semi-supervised
    learning, proposing Self-Learning with Rectification (SLR) strategy for human
    parsing. SLR generates pseudo labels for unlabeled data to retrain the parsing
    model and introduces a trainable graph reasoning method to correct typical errors
    in pseudo labels. Based on SLR, HIPN [[125](#bib.bib125)] further explores to
    combine denoising learning with semi-supervised learning, which develops the noise-tolerant
    hybrid learning, taking advantage of positive and negative learning to better
    handle noisy pseudo labels.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 去噪。为了降低标注成本，主流SHP数据集中存在大量噪声[[22](#bib.bib22), [116](#bib.bib116)]，因此，针对准确人体部件关系建模的去噪学习也受到了关注。SCHP
    [[52](#bib.bib52)] 是最具代表性的工作。它从使用不准确的解析标签作为初始化开始，并设计了一个循环学习调度器，以推断更可靠的伪标签。在同一时期，Li
    *et al*. [[51](#bib.bib51)] 尝试将去噪学习与半监督学习相结合，提出了用于人体解析的自学习与矫正（SLR）策略。SLR 生成伪标签用于未标记的数据，以重新训练解析模型，并引入一种可训练的图推理方法来修正伪标签中的典型错误。基于SLR，HIPN
    [[125](#bib.bib125)] 进一步探索将去噪学习与半监督学习相结合，发展了噪声容忍混合学习，利用正负学习来更好地处理噪声伪标签。
- en: '$\bullet$ Adversarial Learning. Earlier, inspired by the Generative Adversarial
    Nets (GAN) [[96](#bib.bib96)], a few works use adversarial learning to solve problems
    in parts relationship modeling. For example, to solve the domain adaptation problem,
    AFLA [[49](#bib.bib49)] proposes a cross-domain human parsing network, introducing
    a discriminative feature adversarial network and a structured label adversarial
    network to eliminate cross-domain differences in visual appearance and environment
    conditions. MMAN [[50](#bib.bib50)] hopes to solve the problem of low-level local
    and high-level semantic inconsistency in pixel-wise classification loss. It contains
    two discriminators: Macro D, acting on low-resolution label map and penalizing
    semantic inconsistency; Micro D, focusing on high-resolution label map and restraining
    local inconsistency.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 对抗学习。早期，受到生成对抗网络（GAN）[[96](#bib.bib96)]的启发，一些研究利用对抗学习解决部件关系建模中的问题。例如，为了解决领域适应问题，AFLA
    [[49](#bib.bib49)] 提出了一个跨领域人体解析网络，引入了一个判别特征对抗网络和一个结构化标签对抗网络，以消除视觉外观和环境条件的跨领域差异。MMAN
    [[50](#bib.bib50)] 希望解决像素级分类损失中的低级局部和高级语义不一致问题。它包含两个判别器：Macro D，作用于低分辨率标签图，惩罚语义不一致；Micro
    D，专注于高分辨率标签图，抑制局部不一致。
- en: 'Remark. In fact, many single human parsing models use a variety of parts relationship
    modeling methods. Therefore, our above taxonomy only introduces the core methods
    of each model. Table [II](#S3.T2 "Table II ‣ 3.1.3 Multi-task Learning ‣ 3.1 Single
    Human Parsing (SHP) Models ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning
    Technique for Human Parsing: A Survey and Outlook") summarizes the highlights
    of each parts relationship modeling method.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '备注。实际上，许多单一的人体解析模型使用了各种部件关系建模方法。因此，我们上述的分类仅介绍了每种模型的核心方法。表[II](#S3.T2 "Table
    II ‣ 3.1.3 Multi-task Learning ‣ 3.1 Single Human Parsing (SHP) Models ‣ 3 Deep
    Learning Based Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook") 总结了每种部件关系建模方法的亮点。'
- en: 3.2 Multiple Human Parsing (MHP) Models
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 多人类解析（MHP）模型
- en: 'MHP seeks to locate and parse each human in the image plane. The task setting
    is similar to instance segmentation, so it is also called instance-level human
    parsing. We divide MHP into three paradigms: bottom-up, one-stage top-down, and
    two-stage top-down, according to its pipeline of discriminating human instances.
    The essential characteristics of reviewed MHP models are illustrated in Table [III](#S3.T3
    "Table III ‣ 3.2 Multiple Human Parsing (MHP) Models ‣ 3 Deep Learning Based Human
    Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook").'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 'MHP 旨在定位和解析图像平面上的每个人。任务设置类似于实例分割，因此也被称为实例级人体解析。我们根据区分人体实例的管道将 MHP 分为三个范式：自下而上、一阶段自上而下和两阶段自上而下。所评审的
    MHP 模型的基本特征在表[III](#S3.T3 "Table III ‣ 3.2 Multiple Human Parsing (MHP) Models
    ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")中进行了说明。'
- en: 'Table III: Summary of essential characteristics for reviewed MHP models (§[3.2](#S3.SS2
    "3.2 Multiple Human Parsing (MHP) Models ‣ 3 Deep Learning Based Human Parsing
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")). “BU” indicates
    bottom-up; “1S-TD” indicates one-stage top-down; “2S-TD” indicates two-stage top-down.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：审查的MHP模型的主要特征总结（§[3.2](#S3.SS2 "3.2 多人体解析（MHP）模型 ‣ 3 深度学习基础的人体解析 ‣ 深度学习技术在人体解析中的应用：调查与展望")）。“BU”表示自下而上；“1S-TD”表示单阶段自上而下；“2S-TD”表示双阶段自上而下。
- en: '|   Year | Method | Pub. | Pipeline | Datasets | Open Source |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|   年份 | 方法 | 出版 | 流程 | 数据集 | 开源 |'
- en: '| 2017 | Holistic [[138](#bib.bib138)] | BMVC | 1S-TD | PPP | - |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Holistic [[138](#bib.bib138)] | BMVC | 1S-TD | PPP | - |'
- en: '| 2018 | PGN [[8](#bib.bib8)] | ECCV | BU | PPP/CIHP | ✓ |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | PGN [[8](#bib.bib8)] | ECCV | BU | PPP/CIHP | ✓ |'
- en: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | 2S-TD | CIHP/MHP-v2.0 | ✓ |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | 2S-TD | CIHP/MHP-v2.0 | ✓ |'
- en: '| Parsing R-CNN [[61](#bib.bib61)] | CVPR | 1S-TD | CIHP/MHP-v2.0 | ✓ |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Parsing R-CNN [[61](#bib.bib61)] | CVPR | 1S-TD | CIHP/MHP-v2.0 | ✓ |'
- en: '| BraidNet [[57](#bib.bib57)] | MM | 2S-TD | CIHP | - |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| BraidNet [[57](#bib.bib57)] | MM | 2S-TD | CIHP | - |'
- en: '| Unified [[139](#bib.bib139)] | BMVC | 1S-TD | PPP/CIHP | - |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Unified [[139](#bib.bib139)] | BMVC | 1S-TD | PPP/CIHP | - |'
- en: '| 2020 | RP R-CNN [[140](#bib.bib140)] | ECCV | 1S-TD | CIHP/MHP-v2.0 | ✓ |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | RP R-CNN [[140](#bib.bib140)] | ECCV | 1S-TD | CIHP/MHP-v2.0 | ✓ |'
- en: '| SemaTree [[41](#bib.bib41)] | ECCV | 2S-TD | CIHP/MHP-v2.0 | ✓ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| SemaTree [[41](#bib.bib41)] | ECCV | 2S-TD | CIHP/MHP-v2.0 | ✓ |'
- en: '| NAN [[141](#bib.bib141)] | IJCV | BU | MHP-v1.0/MHP-v2.0 | ✓ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| NAN [[141](#bib.bib141)] | IJCV | BU | MHP-v1.0/MHP-v2.0 | ✓ |'
- en: '| SCHP [[52](#bib.bib52)] | TPAMI | 2S-TD | CIHP/MHP-v2.0/VIP | ✓ |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| SCHP [[52](#bib.bib52)] | TPAMI | 2S-TD | CIHP/MHP-v2.0/VIP | ✓ |'
- en: '| 2021 | MGHR [[59](#bib.bib59)] | CVPR | BU |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | MGHR [[59](#bib.bib59)] | CVPR | BU |'
- en: '&#124; PPP/MHP-v2.0 &#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PPP/MHP-v2.0 &#124;'
- en: '&#124; /COCO-DP &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; /COCO-DP &#124;'
- en: '| ✓ |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| ✓ |'
- en: '| 2022 | AIParsing [[142](#bib.bib142)] | TIP | 1S-TD | CIHP/MHP-v2.0/VIP |
    - |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | AIParsing [[142](#bib.bib142)] | TIP | 1S-TD | CIHP/MHP-v2.0/VIP |
    - |'
- en: $\bullet$ Bottom-up. Bottom-up paradigm regards multiple human parsing as a
    fine-grained semantic segmentation task, which predicts the category of each pixel
    and grouping them into corresponding human instance. In a seminal work [[8](#bib.bib8)],
    Gong *et al*. propose a detection-free Part Grouping Network (PGN) that reformulates
    multiple human parsing as two twinned sub-tasks (semantic part segmentation and
    instance-aware edge detection) that can be jointly learned and mutually refined
    via a unified network. Among them, instance-aware edge detection task can group
    semantic parts into distinct human instances. Then, NAN [[141](#bib.bib141)] proposes
    a deep Nested Adversarial Network for multiple human parsing. NAN consists of
    three GAN-like sub-nets, performing semantic saliency prediction, instance-agnostic
    parsing, and instance-aware clustering, respectively. Recently, Zhou *et al*.
    [[59](#bib.bib59)] propose a new bottom-up regime to learn category-level multiple
    human parsing as well as pose estimation in a joint and end-to-end manner, called
    Multi-Granularity Human Representation (MGHR) learning. MGHR exploits structural
    information over different human granularities, transforming the difficult pixel
    grouping problem into an easier multi human joint assembling task to simplify
    the difficulty of human instances discrimination.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 自下而上的方法。自下而上的范式将多人体解析视为细粒度语义分割任务，预测每个像素的类别，并将其分组为相应的人体实例。在一项开创性的工作[[8](#bib.bib8)]中，Gong
    *等人* 提出了一个无检测的部件分组网络（PGN），将多人体解析重新定义为两个双胞胎子任务（语义部件分割和实例感知边缘检测），这些子任务可以通过统一的网络共同学习和相互优化。其中，实例感知边缘检测任务可以将语义部件分组为不同的人体实例。然后，NAN
    [[141](#bib.bib141)] 提出了一个深度嵌套对抗网络用于多人体解析。NAN 由三个类似 GAN 的子网络组成，分别执行语义显著性预测、实例无关解析和实例感知聚类。最近，Zhou
    *等人* [[59](#bib.bib59)] 提出了一个新的自下而上的模式，以联合和端到端的方式学习类别级别的多人体解析及姿态估计，称为多粒度人体表示（MGHR）学习。MGHR
    利用不同人体粒度的结构信息，将困难的像素分组问题转化为较易的多人体联合组装任务，以简化人体实例区分的难度。
- en: $\bullet$ One-stage Top-down. One-stage top-down is the mainstream paradigm
    of multiple human parsing. It first locates each human instance in the image plane,
    then segments each human part in an end-to-end manner. An early attempt is Holistic
    [[138](#bib.bib138)], which consists of a human detection network and a part semantic
    segmentation network, then passing the results of both networks to an instance
    CRF [[143](#bib.bib143)] to perform multiple human parsing. Inspired by Mask R-CNN
    [[144](#bib.bib144)], Qin *et al*. [[139](#bib.bib139)] propose a top-down unified
    framework that simultaneously performs human detection and single human parsing,
    identifying instances and parsing human parts in crowded scenes. A milestone one-stage
    top-down multiple human parsing model is proposed by Yang *et al*., that enhances
    Mask R-CNN in all aspects, and proposes Parsing R-CNN [[61](#bib.bib61)] network,
    greatly improving the accuracy of multiple human parsing concisely. Subsequently,
    Yang *et al*. propose an improved version of Parsing R-CNN, called RP R-CNN [[140](#bib.bib140)],
    which introduces a global semantic enhanced feature pyramid network and a parsing
    re-scoring network into the high-performance pipeline, achieving better performance.
    Later, AIParsing [[142](#bib.bib142)] introduces the anchor-free detector [[145](#bib.bib145)]
    into the one-stage top-down paradigm for discriminating human instances, avoiding
    the hyper-parameters sensitivity caused by anchors.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 一阶段自上而下。 一阶段自上而下是多人体解析的主流范式。它首先在图像平面上定位每个人体实例，然后以端到端的方式对每个人体部位进行分割。早期尝试是
    Holistic [[138](#bib.bib138)]，它包括一个人体检测网络和一个部位语义分割网络，然后将两个网络的结果传递给实例 CRF [[143](#bib.bib143)]
    进行多人体解析。受 Mask R-CNN [[144](#bib.bib144)] 启发，Qin *et al*. [[139](#bib.bib139)]
    提出了一个自上而下的统一框架，该框架同时进行人体检测和单人人体解析，识别实例并在拥挤场景中解析人体部位。杨 *et al* 提出了一个里程碑式的一阶段自上而下的多人体解析模型，该模型在各个方面增强了
    Mask R-CNN，并提出了 Parsing R-CNN [[61](#bib.bib61)] 网络，简洁地大幅提高了多人体解析的准确性。随后，杨 *et
    al* 提出了 Parsing R-CNN 的改进版，称为 RP R-CNN [[140](#bib.bib140)]，将全局语义增强特征金字塔网络和解析重新评分网络引入高性能管道，实现了更好的性能。后来，AIParsing
    [[142](#bib.bib142)] 将无锚检测器 [[145](#bib.bib145)] 引入一阶段自上而下范式，用于区分人体实例，避免了由锚点引起的超参数敏感性。
- en: $\bullet$ Two-stage Top-down. One-stage top-down and two-stage top-down paradigms
    are basically the same in operation flow. The difference between them is whether
    the detector is trained together with the segmentation sub-network in an end-to-end
    manner. All the two-stage bottom-up multiple human parsing methods consist of
    a human detector and a single human parser. The earliest attempt is CE2P [[44](#bib.bib44)],
    which designs a framework called M-CE2P on CE2P and Mask R-CNN, cropping the detected
    human instances, then sending them to the single human parser, finally combining
    the parsing results of all instances into a multiple human parsing prediction.
    Subsequent works, *e.g*., BraidNet [[57](#bib.bib57)], SemaTree [[41](#bib.bib41)],
    and SCHP [[52](#bib.bib52)], basically inherit this pipeline.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 二阶段自上而下。一阶段自上而下和二阶段自上而下范式在操作流程上基本相同。它们的区别在于检测器是否与分割子网络一起以端到端的方式训练。所有的二阶段自下而上的多人体解析方法都包括一个人体检测器和一个单人体解析器。最早的尝试是
    CE2P [[44](#bib.bib44)]，它设计了一个名为 M-CE2P 的框架，该框架基于 CE2P 和 Mask R-CNN，对检测到的人体实例进行裁剪，然后将其发送到单人体解析器，最后将所有实例的解析结果组合成多人体解析预测。后续工作，如
    BraidNet [[57](#bib.bib57)]、SemaTree [[41](#bib.bib41)] 和 SCHP [[52](#bib.bib52)]，基本上继承了这一流程。
- en: 'Remark. The advantage of bottom-up and one-stage top-down is efficiency, and
    the advantage of two-stage top-down is accuracy. But as a non-end-to-end pipeline,
    the inference speed of two-stage top-down is positively correlated with the number
    of human instances, which also limits its practical application value. The detailed
    highlights of three human instances discrimination methods are summarized in Table [IV](#S3.T4
    "Table IV ‣ 3.2 Multiple Human Parsing (MHP) Models ‣ 3 Deep Learning Based Human
    Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook").'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '备注。自下而上的一阶段自上而下的优势是效率，而二阶段自上而下的优势是准确性。但作为非端到端管道，二阶段自上而下的推理速度与人体实例的数量正相关，这也限制了其实际应用价值。三种人体实例区分方法的详细亮点总结在表
    [IV](#S3.T4 "Table IV ‣ 3.2 Multiple Human Parsing (MHP) Models ‣ 3 Deep Learning
    Based Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and
    Outlook")。'
- en: 'Table IV: Highlights of human instances discrimination methods for MHP models
    (§[3.2](#S3.SS2 "3.2 Multiple Human Parsing (MHP) Models ‣ 3 Deep Learning Based
    Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")).
    Representative Works of each method are also give.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: MHP 模型的人类实例区分方法的亮点 (§[3.2](#S3.SS2 "3.2 Multiple Human Parsing (MHP)
    Models ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for Human
    Parsing: A Survey and Outlook")）。每种方法的代表性工作也给出。'
- en: '|   Method |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|   方法 |'
- en: '&#124; Representative &#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代表性 &#124;'
- en: '&#124; Works &#124;'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 工作 &#124;'
- en: '| Highlights |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 亮点 |'
- en: '| Bottom-up |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 自下而上 |'
- en: '&#124; [[8](#bib.bib8), [141](#bib.bib141), [59](#bib.bib59)] &#124;'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[8](#bib.bib8), [141](#bib.bib141), [59](#bib.bib59)] &#124;'
- en: '|'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Good model efficiency, good accuracy on pixel-wise &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 良好的模型效率，像素级的准确性较高 &#124;'
- en: '&#124; segmentation, and poor accuracy on instances &#124;'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割，且实例的准确性较差 &#124;'
- en: '&#124; discrimination. &#124;'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 区分. &#124;'
- en: '|'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; One-stage &#124;'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一阶段 &#124;'
- en: '&#124; Top-down &#124;'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自上而下 &#124;'
- en: '|'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[138](#bib.bib138), [61](#bib.bib61), [139](#bib.bib139)] &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[138](#bib.bib138), [61](#bib.bib61), [139](#bib.bib139)] &#124;'
- en: '&#124; [[140](#bib.bib140), [142](#bib.bib142)] &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[140](#bib.bib140), [142](#bib.bib142)] &#124;'
- en: '|'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Better trade-off between model efficiency and accuracy. &#124;'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型效率与准确性之间的更好折衷. &#124;'
- en: '&#124; But pixel-wise segmentation, especially the part &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 但像素级分割，尤其是部件 &#124;'
- en: '&#124; boundary is not fine enough. &#124;'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 边界不够精细. &#124;'
- en: '|'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Two-stage &#124;'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段 &#124;'
- en: '&#124; Top-down &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自上而下 &#124;'
- en: '|'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[44](#bib.bib44), [57](#bib.bib57), [41](#bib.bib41)] &#124;'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[44](#bib.bib44), [57](#bib.bib57), [41](#bib.bib41)] &#124;'
- en: '&#124; [[52](#bib.bib52)] &#124;'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[52](#bib.bib52)] &#124;'
- en: '|'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Good accuracy and poor efficiency, the model inference &#124;'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确性良好但效率差，模型推理 &#124;'
- en: '&#124; time is proportional to human instances number. &#124;'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间与人类实例的数量成正比. &#124;'
- en: '|'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 3.3 Video Human Parsing (VHP) Models
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 视频人类解析（VHP）模型
- en: 'Existing VHP studies mainly focus to propagate the first frame into the entire
    video by the affinity matrix, which represents the temporal correspondences learnt
    from raw video data. Considering the unsupervised learning paradigms, we can group
    them into three classes: cycle-tracking, reconstructive learning, and contrastive
    learning. We summarize the essential characteristics of reviewed VHP models in
    Table [V](#S3.T5 "Table V ‣ 3.3 Video Human Parsing (VHP) Models ‣ 3 Deep Learning
    Based Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and
    Outlook").'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '现有的 VHP 研究主要集中在通过相似度矩阵将第一帧传播到整个视频中，该矩阵表示从原始视频数据中学习到的时间对应关系。考虑到无监督学习范式，我们可以将它们分为三类：循环跟踪、重建学习和对比学习。我们在表
    [V](#S3.T5 "Table V ‣ 3.3 Video Human Parsing (VHP) Models ‣ 3 Deep Learning Based
    Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")
    中总结了回顾的 VHP 模型的基本特征。'
- en: 'Table V: Summary of essential characteristics for reviewed VHP models (§[3.3](#S3.SS3
    "3.3 Video Human Parsing (VHP) Models ‣ 3 Deep Learning Based Human Parsing ‣
    Deep Learning Technique for Human Parsing: A Survey and Outlook")). “Cycle.” indicates
    cycle-tracking; “Recons.” indicates reconstructive learning; “Contra.” indicates
    contrastive learning. All models are test on the VIP dataset.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 回顾的 VHP 模型的基本特征总结 (§[3.3](#S3.SS3 "3.3 Video Human Parsing (VHP) Models
    ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")）。“循环”表示循环跟踪；“重建”表示重建学习；“对比”表示对比学习。所有模型都在 VIP 数据集上进行测试。'
- en: '|   Year | Method | Pub. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|   年份 | 方法 | 出版 |'
- en: '&#124; Cycle. &#124;'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环. &#124;'
- en: '|'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Recons. &#124;'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建. &#124;'
- en: '|'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Contra. &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对比. &#124;'
- en: '| Open Source |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 开源 |'
- en: '| 2018 | ATEN [[9](#bib.bib9)] | MM | ✓ | - | - | ✓ |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | ATEN [[9](#bib.bib9)] | MM | ✓ | - | - | ✓ |'
- en: '| 2019 | TimeCycle [[146](#bib.bib146)] | CVPR | ✓ | - | - | ✓ |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | TimeCycle [[146](#bib.bib146)] | CVPR | ✓ | - | - | ✓ |'
- en: '| UVC [[147](#bib.bib147)] | NeurIPS | ✓ | ✓ | - | ✓ |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| UVC [[147](#bib.bib147)] | NeurIPS | ✓ | ✓ | - | ✓ |'
- en: '| 2020 | CRW [[148](#bib.bib148)] | NeurIPS | ✓ | - | - | ✓ |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | CRW [[148](#bib.bib148)] | NeurIPS | ✓ | - | - | ✓ |'
- en: '| 2021 | ContrastCorr [[149](#bib.bib149)] | AAAI | ✓ | ✓ |  | ✓ |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | ContrastCorr [[149](#bib.bib149)] | AAAI | ✓ | ✓ |  | ✓ |'
- en: '| CLTC [[150](#bib.bib150)] | CVPR | - | - | ✓ | - |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| CLTC [[150](#bib.bib150)] | CVPR | - | - | ✓ | - |'
- en: '| VFS [[151](#bib.bib151)] | ICCV | - | - | ✓ | ✓ |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| VFS [[151](#bib.bib151)] | ICCV | - | - | ✓ | ✓ |'
- en: '| JSTG [[152](#bib.bib152)] | ICCV | ✓ | - | ✓ | - |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| JSTG [[152](#bib.bib152)] | ICCV | ✓ | - | ✓ | - |'
- en: '| 2022 | LIIR [[153](#bib.bib153)] | CVPR | - | ✓ | - | ✓ |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | LIIR [[153](#bib.bib153)] | CVPR | - | ✓ | - | ✓ |'
- en: '| SCC [[154](#bib.bib154)] | CVPR | ✓ | - | ✓ | - |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| SCC [[154](#bib.bib154)] | CVPR | ✓ | - | ✓ | - |'
- en: '| UVC+ [[155](#bib.bib155)] | ArXiv | ✓ | ✓ | ✓ | - |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| UVC+ [[155](#bib.bib155)] | ArXiv | ✓ | ✓ | ✓ | - |'
- en: $\bullet$ Cycle-tracking. Early VHP methods model the unsupervised learning
    target mainly by the cycle-consistency of video frames, *i.e.*, pixels/patches
    are expected to fall into the same locations after a cycle of forward-backward
    tracking. ATEN[[9](#bib.bib9)] first leverages convolutional gated recurrent units
    to encode temporal feature-level changes, optical flow of non-key frames is wrapped
    with the temporal memory to generate their features. TimeCycle [[146](#bib.bib146)]
    tracks the reference patch backward-forward in the video. The reference and the
    tracked patch at the end of the tracking cycle are considered to be consistent
    both in spatial coordinates and feature representation. Meanwhile, UVC [[147](#bib.bib147)]
    performs the region-level tracking and pixel-level corresponding with a shared
    affinity matrix, the tracked patch feature and the region-corresponding sub-affinity
    matrix are used to reconstruct the reference patch. Roles of the target and reference
    patches are then switched to regularizing the affinity matrix as orthogonal, which
    satisfies the cycle-consistency constraint. Its later version, UVC+ [[155](#bib.bib155)]
    combines features learned by image-based tasks with video-based counterparts to
    further boost the performance. Lately, CRW [[148](#bib.bib148)] represents video
    as a graph, where nodes are patches and edges are affinities between nodes in
    adjacent frames. A cross-entropy loss guides a graph walk to track the initial
    node bi-directionally in feature space, which is considered the target node after
    a bunch of cycle paths. However, the cycle-consistency in [[146](#bib.bib146)],
    [[148](#bib.bib148)] strictly assumes that the target patch preserves visible
    in consecutive frames. Once it is occluded or disappears, the correspondences
    will be incorrectly assigned, thus leaving an optimal transport problem between
    video frames.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 循环跟踪。早期的 VHP 方法主要通过视频帧的循环一致性来建模无监督学习目标，*即*，像素/补丁在经过一轮前向-后向跟踪后应落在相同位置。ATEN[[9](#bib.bib9)]
    首次利用卷积门控递归单元来编码时间特征级变化，非关键帧的光流与时间记忆结合以生成其特征。TimeCycle [[146](#bib.bib146)] 在视频中前向-后向跟踪参考补丁。跟踪周期结束时，参考补丁和跟踪补丁在空间坐标和特征表示上都被认为是一致的。同时，UVC
    [[147](#bib.bib147)] 通过共享亲和矩阵执行区域级跟踪和像素级对应，跟踪补丁特征和区域对应的子亲和矩阵用于重建参考补丁。然后将目标和参考补丁的角色交换，以将亲和矩阵正则化为正交，从而满足循环一致性约束。其后续版本
    UVC+ [[155](#bib.bib155)] 将基于图像的任务学到的特征与基于视频的任务特征结合起来，进一步提升性能。最近，CRW [[148](#bib.bib148)]
    将视频表示为图，其中节点是补丁，边是相邻帧中节点之间的亲和力。交叉熵损失指导图步行在特征空间中双向跟踪初始节点，经过一系列循环路径后被视为目标节点。然而，[[146](#bib.bib146)]、[[148](#bib.bib148)]
    中的循环一致性严格假设目标补丁在连续帧中可见。一旦它被遮挡或消失，对应关系将被错误分配，从而在视频帧之间留下一个最优传输问题。
- en: 'Table VI: Highlights of temporal correspondences learning methods for VHP models
    (§[3.3](#S3.SS3 "3.3 Video Human Parsing (VHP) Models ‣ 3 Deep Learning Based
    Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")).
    Representative Works of each method are also give.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：VHP 模型时间对应学习方法的重点 (§[3.3](#S3.SS3 "3.3 视频人体解析（VHP）模型 ‣ 3 基于深度学习的人体解析 ‣
    人体解析的深度学习技术：调查与展望"))。每种方法的代表性工作也被列出。
- en: '|   Method |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|   方法 |'
- en: '&#124; Representative &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代表性 &#124;'
- en: '&#124; Works &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 工作 &#124;'
- en: '| Highlights |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 亮点 |'
- en: '|'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Cycle- &#124;'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环- &#124;'
- en: '&#124; tracking &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跟踪 &#124;'
- en: '|'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[146](#bib.bib146), [147](#bib.bib147)] &#124;'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[146](#bib.bib146), [147](#bib.bib147)] &#124;'
- en: '&#124; [[155](#bib.bib155), [148](#bib.bib148)] &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[155](#bib.bib155), [148](#bib.bib148)] &#124;'
- en: '|'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Capturing temporal variations, may produce &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 捕捉时间变化，可能产生 &#124;'
- en: '&#124; wrong correspondences when occlusion occurs. &#124;'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 遮挡发生时错误的对应关系。 &#124;'
- en: '|'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Reconstructive &#124;'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建 &#124;'
- en: '&#124; Learning &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习 &#124;'
- en: '|'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[149](#bib.bib149), [153](#bib.bib153)] &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[149](#bib.bib149), [153](#bib.bib153)] &#124;'
- en: '|'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Modelling fine-grained temporal correspondence &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精细时间对应建模 &#124;'
- en: '&#124; and guiding focus on part details. &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并指导关注部件细节。 &#124;'
- en: '|'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Contrastive &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对比 &#124;'
- en: '&#124; Learning &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习 &#124;'
- en: '|'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[150](#bib.bib150), [151](#bib.bib151)] &#124;'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[150](#bib.bib150), [151](#bib.bib151)] &#124;'
- en: '&#124; [[152](#bib.bib152), [154](#bib.bib154)] &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[152](#bib.bib152), [154](#bib.bib154)] &#124;'
- en: '|'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Search for discriminative features to segment &#124;'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 寻找区分性特征以进行分割 &#124;'
- en: '&#124; similar or position-transformed human instances. &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相似或位置变换的人体实例。 &#124;'
- en: '| ![Refer to caption](img/5f83f893f98e371117932897845b12e0.png)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![参考标题](img/5f83f893f98e371117932897845b12e0.png)'
- en: 'Figure 4: Correlations of different SHP, MHP and VHP methods (§[3.4](#S3.SS4
    "3.4 Summary ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for
    Human Parsing: A Survey and Outlook")). We use the connections between the arc
    edges to summary the correlation between human parsing methods, each connecting
    line stands for a study that uses both methods. The longer the arc, the more methods
    of this kind, same for the width of connecting lines. This correlation summary
    reveals the prevalence of various human parsing methods.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：不同SHP、MHP和VHP方法的关联（§[3.4](#S3.SS4 "3.4 总结 ‣ 基于深度学习的人体解析 ‣ 人体解析的深度学习技术：调查与展望")）。我们通过弧边之间的连接来总结人体解析方法之间的关联，每条连接线代表一个使用这两种方法的研究。弧的长度越长，说明这种方法的数量越多，连接线的宽度也相同。这种关联总结揭示了各种人体解析方法的普及程度。
- en: $\bullet$ Reconstructive Learning. As video contents smoothly shift in time,
    pixels in a “query” frame can be considered as copies from a set of pixels in
    other reference frames [[156](#bib.bib156), [157](#bib.bib157)]. Following UVC
    [[147](#bib.bib147)] to establish pixel-level correspondence, several methods
    [[149](#bib.bib149), [153](#bib.bib153)] are proposed to learn temporal correspondence
    completely by reconstructing correlating frames. Subsequently, ContrastCorr [[149](#bib.bib149)]
    not only learns from intra-video self-supervision, but also steps further to introduce
    inter-video transformation as negative correspondence. The inter-video distinction
    enforces the feature extractor to learn discriminations between videos while preserving
    the fine-grained matching characteristic among intra-video frame pairs. Based
    on the intra-inter video correlation, LIIR [[153](#bib.bib153)] introduces a locality-aware
    reconstruction framework, which encodes position information and involves spatial
    compactness into intra-video correspondence learning, for locality-aware and efficient
    visual tracking.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 重建学习。由于视频内容在时间上平滑过渡，“查询”帧中的像素可以视为其他参考帧中一组像素的副本[[156](#bib.bib156),
    [157](#bib.bib157)]。沿袭UVC [[147](#bib.bib147)]建立像素级对应关系，提出了几种方法[[149](#bib.bib149),
    [153](#bib.bib153)]，通过重建相关帧来完全学习时间对应关系。随后，ContrastCorr [[149](#bib.bib149)]不仅从视频内部自监督中学习，还进一步引入了视频间变换作为负对应关系。视频间的区分强制特征提取器在保留视频内部帧对之间细粒度匹配特征的同时，学习视频之间的区分。基于视频内外关联，LIIR
    [[153](#bib.bib153)]引入了一个位置感知重建框架，该框架编码位置信息，并将空间紧凑性融入到视频内对应关系学习中，以实现位置感知和高效视觉跟踪。
- en: '![Refer to caption](img/47c61a8ddac735faf39c0c0099b6c779.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/47c61a8ddac735faf39c0c0099b6c779.png)'
- en: 'Figure 5: Correlations of different SHP, MHP and VHP studies (§[3.4](#S3.SS4
    "3.4 Summary ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for
    Human Parsing: A Survey and Outlook")). We list out all the involved human parsing
    studies by dots and use connecting lines to represent their citing relations.
    The citing relation here refers to the citation appears in experimental comparisons,
    to avoid citations of low correlation in background introduction. As each line
    represents a citation between two studies, so the larger the dot, the more times
    cited. These correlations highlight the relatively prominent studies.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：不同SHP、MHP和VHP研究的关联（§[3.4](#S3.SS4 "3.4 总结 ‣ 基于深度学习的人体解析 ‣ 人体解析的深度学习技术：调查与展望")）。我们通过点列出所有涉及的人体解析研究，并使用连接线表示它们的引用关系。这里的引用关系指的是实验比较中的引用，以避免背景介绍中低关联的引用。由于每条线代表两个研究之间的引用，因此点越大，引用次数越多。这些关联突出了相对突出的研究。
- en: $\bullet$ Contrastive Learning. Following the idea of pulling positive pairs
    close together and pushing negative pairs away from each other, considerable VHP
    algorithms adopt contrastive learning as a training objective. To solve the optimal
    transport problem, CLTC [[150](#bib.bib150)] proposes to mine positive and semi-hard
    negative correspondences via consistency estimation and dynamic hardness discrimination,
    respectively. The well-defined positive and negative pixel pairs prevent side-effects
    from non-consistent positive and too hard/easy negative samples to contrastive
    learning. Specifically, unlike most methods that perform patch-level contrastive
    learning, VFS [[151](#bib.bib151)] learns visual correspondences at frame level.
    Following data augmentation of image-level contrastive learning [[158](#bib.bib158)]
    and a well-designed temporal sampling strategy, VFS encourages convolutional features
    to find correspondences between similar objects and parts. Lately, [[152](#bib.bib152),
    [154](#bib.bib154)] extend the video graph with space relations of neighbor nodes,
    which determine the aggregation strength from intra-frame neighbors. The proposed
    space-time graph draws more attention to the association of center-neighbor pairs,
    thus explicitly helping learning correspondence between part instances. SCC [[154](#bib.bib154)]
    mixes sequential Bayesian filters to formulate the optimal paths that track nodes
    from one frame to others, to alleviate the correspondence missing caused by random
    occlusion.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 对比学习。遵循将正样本对拉近并将负样本对推远的思路，许多 VHP 算法采用对比学习作为训练目标。为了解决最优传输问题，CLTC [[150](#bib.bib150)]
    提出了通过一致性估计和动态难度区分分别挖掘正样本和半硬负样本的对应关系。明确的正负像素对防止了来自不一致正样本和过硬/过易负样本对对比学习的副作用。具体而言，与大多数在补丁级别进行对比学习的方法不同，VFS
    [[151](#bib.bib151)] 在帧级别学习视觉对应关系。结合图像级对比学习的数据显示增强 [[158](#bib.bib158)] 和精心设计的时间采样策略，VFS
    鼓励卷积特征找到类似物体和部分之间的对应关系。最近，[[152](#bib.bib152), [154](#bib.bib154)] 扩展了具有邻节点空间关系的视频图，这决定了来自帧内邻居的聚合强度。所提出的时空图更加关注中心-邻居对的关联，从而显著帮助学习部分实例之间的对应关系。SCC
    [[154](#bib.bib154)] 结合了顺序贝叶斯滤波器，以制定从一帧到其他帧跟踪节点的最佳路径，从而缓解随机遮挡造成的对应关系丢失。
- en: 'Remark. To our investigation scope, the current VHP research essentially follows
    an unsupervised semi-automatic video object segmentation setup. But considering
    the potential demand, it is more expectant to fully utilize the annotations and
    solve the VHP problem through an instance-discriminative manner, *i.e*., a fine-grained
    video instance segmentation task. The highlights of temporal correspondences learning
    methods for VHP are shown in Table [VI](#S3.T6 "Table VI ‣ 3.3 Video Human Parsing
    (VHP) Models ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for
    Human Parsing: A Survey and Outlook").'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '备注。就我们的调查范围而言，目前的 VHP 研究基本上遵循无监督半自动视频对象分割设置。但考虑到潜在需求，更期待充分利用标注并通过实例区分的方式解决
    VHP 问题，即细粒度视频实例分割任务。VHP 时间对应学习方法的亮点见表 [VI](#S3.T6 "Table VI ‣ 3.3 Video Human
    Parsing (VHP) Models ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique
    for Human Parsing: A Survey and Outlook")。'
- en: 3.4 Summary
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 摘要
- en: 'Through the detailed review, we have subdivided SHP, MHP, and VHP studies into
    multiple methods and discussed their characteristics. To further investigate the
    development picture of the human parsing community, we summarize the correlations
    of the methods in Figure [4](#S3.F4 "Figure 4 ‣ 3.3 Video Human Parsing (VHP)
    Models ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for Human
    Parsing: A Survey and Outlook") and correlations of the involved studies in Figure [5](#S3.F5
    "Figure 5 ‣ 3.3 Video Human Parsing (VHP) Models ‣ 3 Deep Learning Based Human
    Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook"), respectively.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '通过详细审查，我们将 SHP、MHP 和 VHP 研究细分为多种方法，并讨论了它们的特征。为了进一步探讨人类解析社区的发展情况，我们总结了方法的相关性，如图
    [4](#S3.F4 "Figure 4 ‣ 3.3 Video Human Parsing (VHP) Models ‣ 3 Deep Learning
    Based Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and
    Outlook") 和涉及研究的相关性，如图 [5](#S3.F5 "Figure 5 ‣ 3.3 Video Human Parsing (VHP) Models
    ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")。'
- en: 'Figure [4](#S3.F4 "Figure 4 ‣ 3.3 Video Human Parsing (VHP) Models ‣ 3 Deep
    Learning Based Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook") presents correlations between research methods, *i.e*., two methods
    are connected if a study uses both as its technical components, making the length
    of arcs represent the number of studies using them. The connecting line distribution
    first obviously shows that Graph (Structure), Attention (Mechanism), and Edge(-aware
    Learning) of SHP are more correlated with multiple other methods, which indicates
    their compatibility with others and prevalence in the community. It is worth noting
    that though Tree (Structure) has many correlations with others, a large proportion
    of them are with Graph method. This phenomenon indicates that Tree method is much
    less generalizable compared to Graph, Attention, and Edge methods. Regrettably,
    negligible relations between VHP and other methods show that current VHP studies
    have not yet gone deep into parts relationship modeling or human instance discrimination.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '图[4](#S3.F4 "Figure 4 ‣ 3.3 Video Human Parsing (VHP) Models ‣ 3 Deep Learning
    Based Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and
    Outlook")展示了研究方法之间的相关性，即，如果一项研究使用了两个方法作为其技术组件，则这两种方法之间会有连接线，弧线的长度代表使用它们的研究数量。连接线的分布首先明显显示，SHP的图（结构）、Attention（机制）和边缘（学习）与多个其他方法的相关性较高，这表明它们与其他方法的兼容性以及在社区中的普及程度。值得注意的是，尽管树（结构）与其他方法有许多相关性，但其中大部分与图方法相关。这一现象表明，与图、Attention和边缘方法相比，树方法的普遍性要低得多。遗憾的是，VHP与其他方法之间的微不足道的关系表明，当前的VHP研究尚未深入到部分关系建模或人类实例区分的研究中。'
- en: 'The correlations of human parsing studies are presented in form of citing relations
    as Figure [5](#S3.F5 "Figure 5 ‣ 3.3 Video Human Parsing (VHP) Models ‣ 3 Deep
    Learning Based Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook"), each line represents a citation between two studies. For reliable
    statistics, we only consider citations that appear in experimental comparisons
    for all studies. From the citing relations, we can easily observe that Attention
    [[33](#bib.bib33)], JPPNet [[2](#bib.bib2)], CE2P [[44](#bib.bib44)], CNIF [[3](#bib.bib3)]
    and PGN [[8](#bib.bib8)] have the largest dots, *i.e*., they are experimental
    compared by most other studies, this indicates they are recognized as baseline
    studies of great prominence by the community. Additionally, since CE2P proposed
    to handle MHP sub-task by 2S-TD pipeline and make a milestone, lots of SHP studies
    start to compare their algorithms with MHP studies, this trend breaks down the
    barriers between the two sub-tasks of human parsing. Lastly, similar to the method
    correlation, VHP studies form citations strictly along with the proposed order
    among their own, which once again shows that VHP studies have not focused on human-centric
    data.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '人类解析研究的相关性以引用关系的形式呈现，如图[5](#S3.F5 "Figure 5 ‣ 3.3 Video Human Parsing (VHP)
    Models ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for Human
    Parsing: A Survey and Outlook")所示，每一条线代表两项研究之间的引用。为了可靠的统计数据，我们只考虑所有研究中的实验比较中出现的引用。从引用关系中，我们可以很容易地观察到，Attention
    [[33](#bib.bib33)]、JPPNet [[2](#bib.bib2)]、CE2P [[44](#bib.bib44)]、CNIF [[3](#bib.bib3)]
    和 PGN [[8](#bib.bib8)] 拥有最大的点，即它们被大多数其他研究实验比较，这表明它们被社区认可为具有重要性的基准研究。此外，由于CE2P提出通过2S-TD流程处理MHP子任务并取得了里程碑，许多SHP研究开始将其算法与MHP研究进行比较，这一趋势打破了人类解析两个子任务之间的障碍。最后，类似于方法相关性，VHP研究的引用严格按照它们自己的提出顺序排列，这再次表明VHP研究没有专注于以人为中心的数据。'
- en: Synthesizing detailed review and correlation analysis, we can draw some conclusions
    about the historical evolution of human parsing models. First, the research focus
    has gradually shifted from SHP to MHP and VHP. As more challenging tasks, the
    latter two also have greater application potential. With the emergence of high-quality
    annotated datasets and the improvement of computing power, they have received
    increasing attention. Secondly, the technical diversity is insufficient, and the
    achievements of representation learning in recent years have not fully benefited
    the human parsing field. Finally, the number of open source work has increased
    significantly, but still insufficient. It is hoped that subsequent researchers
    will open source code and models as much as possible to benefit the follow-up
    researchers.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 综合详细的评审和相关分析，我们可以得出一些关于人体解析模型历史演变的结论。首先，研究重点逐渐从**SHP**转向**MHP**和**VHP**。作为更具挑战性的任务，后两者也具有更大的应用潜力。随着高质量标注数据集的出现和计算能力的提高，它们受到了越来越多的关注。其次，技术多样性不足，近年来表示学习的成果尚未完全惠及人体解析领域。最后，开源工作的数量显著增加，但仍然不足。希望后续研究者能够尽可能开源代码和模型，以造福后续研究人员。
- en: 4 Human Parsing Datasets
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 人体解析数据集
- en: 'In the past decades, a variety of visual datasets have been released for human
    parsing (upper part of Figure [3](#S2.F3 "Figure 3 ‣ 2.3 Relevant Tasks ‣ 2 Preliminaries
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")). We summarize
    the classical and commonly used datasets in Table [VII](#S4.T7 "Table VII ‣ 4
    Human Parsing Datasets ‣ Deep Learning Technique for Human Parsing: A Survey and
    Outlook"), and give a detailed review from multiple angles.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '在过去的几十年里，已经发布了各种视觉数据集用于人体解析（图[3](#S2.F3 "Figure 3 ‣ 2.3 Relevant Tasks ‣ 2
    Preliminaries ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")的上半部分）。我们在表[VII](#S4.T7
    "Table VII ‣ 4 Human Parsing Datasets ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")中总结了经典和常用的数据集，并从多个角度进行了详细评审。'
- en: 'Table VII: Statistics of existing human parsing datasets. See §[4.1](#S4.SS1
    "4.1 Single Human Parsing (SHP) Datasets ‣ 4 Human Parsing Datasets ‣ Deep Learning
    Technique for Human Parsing: A Survey and Outlook") - §[4.3](#S4.SS3 "4.3 Video
    Human Parsing (VHP) Datasets ‣ 4 Human Parsing Datasets ‣ Deep Learning Technique
    for Human Parsing: A Survey and Outlook") for more detailed descriptions. The
    14 datasets are divided into 3 groups according to the human parsing taxonomy.
    “Instance” indicates that instance-level human labels are provided; “Temporal”
    indicates that video-level labels are provided; “Super-pixel” indicates that super-pixels
    are used for labeling.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '表VII：现有人体解析数据集的统计信息。有关详细描述，请参见§[4.1](#S4.SS1 "4.1 Single Human Parsing (SHP)
    Datasets ‣ 4 Human Parsing Datasets ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook") - §[4.3](#S4.SS3 "4.3 Video Human Parsing (VHP) Datasets
    ‣ 4 Human Parsing Datasets ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook")。这14个数据集根据人体解析分类法分为3组。“实例”表示提供了实例级别的人体标签；“时间”表示提供了视频级别的标签；“超像素”表示使用超像素进行标注。'
- en: '|   Dataset | Year | Pub. | #Images | #Train/Val/Test/ | #Class | Purpose |
    Instance | Temporal | Super-pixel | Other Annotations |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|   数据集 | 年份 | 期刊 | 图片数量 | 训练/验证/测试 | 类别数 | 目的 | 实例 | 时间 | 超像素 | 其他标注 |'
- en: '| Fashionista [[1](#bib.bib1)] | 2012 | CVPR | 685 | 456/-/299 | 56 | Clothing
    | - | - | ✓ | Clothing-tag |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| Fashionista [[1](#bib.bib1)] | 2012 | CVPR | 685 | 456/-/299 | 56 | 服装 |
    - | - | ✓ | 服装标签 |'
- en: '| CFPD [[25](#bib.bib25)] | 2013 | TMM | 2,682 | 1,341/-/1,341 | 23 | Clothing
    | - | - | ✓ | Color-seg. |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| CFPD [[25](#bib.bib25)] | 2013 | TMM | 2,682 | 1,341/-/1,341 | 23 | 服装 |
    - | - | ✓ | 颜色分割 |'
- en: '| DailyPhotos [[14](#bib.bib14)] | 2013 | ICCV | 2,500 | 2,500/-/- | 19 | Clothing
    | - | - | ✓ | Clothing-tag |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| DailyPhotos [[14](#bib.bib14)] | 2013 | ICCV | 2,500 | 2,500/-/- | 19 | 服装
    | - | - | ✓ | 服装标签 |'
- en: '| PPSS [[159](#bib.bib159)] | 2013 | ICCV | 3,673 | 1,781/-/1,892 | 6 | Human
    | - | - | - | - |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| PPSS [[159](#bib.bib159)] | 2013 | ICCV | 3,673 | 1,781/-/1,892 | 6 | 人体
    | - | - | - | - |'
- en: '| ATR [[22](#bib.bib22)] | 2015 | TPAMI | 7,700 | 6,000/700/1,000 | 18 | Human
    | - | - | - | - |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| ATR [[22](#bib.bib22)] | 2015 | TPAMI | 7,700 | 6,000/700/1,000 | 18 | 人体
    | - | - | - | - |'
- en: '| Chictopia10k [[37](#bib.bib37)] | 2015 | ICCV | 10,000 | 10,000/-/- | 18
    | Clothing | - | - | - | Clothing-tag |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| Chictopia10k [[37](#bib.bib37)] | 2015 | ICCV | 10,000 | 10,000/-/- | 18
    | 服装 | - | - | - | 服装标签 |'
- en: '| SYSU-Clothes [[114](#bib.bib114)] | 2016 | TMM | 2,682 | 2,682/-/- | 57 |
    Clothing | - | - | ✓ | Clothing-tag |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| SYSU-Clothes [[114](#bib.bib114)] | 2016 | TMM | 2,682 | 2,682/-/- | 57 |
    服装 | - | - | ✓ | 服装标签 |'
- en: '| LIP [[116](#bib.bib116)] | 2017 | CVPR | 50,462 | 30,462/10,000/10,000 |
    20 | Human | - | - | ✓ | - |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| LIP [[116](#bib.bib116)] | 2017 | CVPR | 50,462 | 30,462/10,000/10,000 |
    20 | 人体 | - | - | ✓ | - |'
- en: '| HRHP [[7](#bib.bib7)] | 2021 | CVPRW | 7,500 | 6,000/500/1,000 | 20 | Human
    | - | - | - | - |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| HRHP [[7](#bib.bib7)] | 2021 | CVPRW | 7,500 | 6,000/500/1,000 | 20 | 人体
    | - | - | - | - |'
- en: '| PASCAL-Person-Part [[160](#bib.bib160)] | 2014 | CVPR | 3,533 | 1,716/-/1,817
    | 7 | Human | ✓ | - | - | Human-box |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| PASCAL-Person-Part [[160](#bib.bib160)] | 2014 | CVPR | 3,533 | 1,716/-/1,817
    | 7 | 人体 | ✓ | - | - | 人体-框 |'
- en: '| MHP-v1.0 [[161](#bib.bib161)] | 2017 | ArXiv | 4,980 | 3,000/1,000/980 |
    19 | Human | ✓ | - | - | Human-box |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| MHP-v1.0 [[161](#bib.bib161)] | 2017 | ArXiv | 4,980 | 3,000/1,000/980 |
    19 | 人体 | ✓ | - | - | 人体-框 |'
- en: '| MHP-v2.0 [[162](#bib.bib162)] | 2018 | MM | 25,403 | 15,403/5,000/5,000 |
    59 | Human | ✓ | - | - | Human-box |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| MHP-v2.0 [[162](#bib.bib162)] | 2018 | MM | 25,403 | 15,403/5,000/5,000 |
    59 | 人体 | ✓ | - | - | 人体-框 |'
- en: '| COCO-DensePose [[74](#bib.bib74)] | 2018 | CVPR | 27,659 | 26,151/-/1,508
    | 15 | Human | ✓ | - | - |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| COCO-DensePose [[74](#bib.bib74)] | 2018 | CVPR | 27,659 | 26,151/-/1,508
    | 15 | 人体 | ✓ | - | - |'
- en: '&#124; Human-box/ &#124;'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人体-框/ &#124;'
- en: '&#124; keypoints/densepoints &#124;'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关键点/密集点 &#124;'
- en: '|'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CIHP [[8](#bib.bib8)] | 2018 | ECCV | 38,280 | 28,280/5,000/5,000 | 20 |
    Human | ✓ | - | - | Human-box |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| CIHP [[8](#bib.bib8)] | 2018 | ECCV | 38,280 | 28,280/5,000/5,000 | 20 |
    人体 | ✓ | - | - | 人体-框 |'
- en: '| VIP [[9](#bib.bib9)] | 2018 | MM | 21,246 | 18,468/-/2,778 | 20 | Human |
    ✓ | ✓ | - | Human-box/identity |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| VIP [[9](#bib.bib9)] | 2018 | MM | 21,246 | 18,468/-/2,778 | 20 | 人体 | ✓
    | ✓ | - | 人体-框/身份 |'
- en: 4.1 Single Human Parsing (SHP) Datasets
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 单一人体解析 (SHP) 数据集
- en: $\bullet$ Fashionista (FS) [[1](#bib.bib1)] consists of 685 photographs collected
    from Chictopia.com, a social networking website for fashion bloggers. There are
    456 training images and 299 testing images annotated with 56-class semantic labels,
    and text tags of garment items and styling are also provided. Fashionista was
    once the main single human/clothing parsing dataset but was limited by its scale.
    It is rarely used now.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Fashionista (FS) [[1](#bib.bib1)] 由从 Chictopia.com 收集的 685 张照片组成，这是一个时尚博主的社交网络网站。共有
    456 张训练图像和 299 张测试图像，标注了 56 类语义标签，同时也提供了服装项目和造型的文本标签。Fashionista 曾是主要的单一人体/服装解析数据集，但由于规模限制，现在很少使用。
- en: $\bullet$ Colorful Fashion Parsing Data (CFPD) [[25](#bib.bib25)] is also collected
    from Chictopia.com, which provides 23-class noisy semantic labels and 13-class
    color labels. The annotated images are usually grouped into 1,341/1,341 for train/test.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Colorful Fashion Parsing Data (CFPD) [[25](#bib.bib25)] 也来自 Chictopia.com，提供了
    23 类噪声语义标签和 13 类颜色标签。标注的图像通常被分为 1,341/1,341 用于训练/测试。
- en: $\bullet$ DailyPhotos (DP) [[14](#bib.bib14)] contains 2,500 high resolution
    images, which are crawled following the same strategy as the Fashionista dataset
    and thoroughly annotated with 19 categories.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ DailyPhotos (DP) [[14](#bib.bib14)] 包含 2,500 张高分辨率图像，这些图像是按照与 Fashionista
    数据集相同的策略爬取的，并标注了 19 个类别。
- en: $\bullet$ PPSS [[159](#bib.bib159)] includes 3,673 annotated samples collected
    from 171 videos of different surveillance scenes and provides pixel-wise annotations
    for hair, face, upper-/lower-clothes, arm, and leg. It presents diverse real-word
    challenges, *e.g*. pose variations, illumination changes, and occlusions. There
    are 1,781 and 1,892 images for training and testing, respectively.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ PPSS [[159](#bib.bib159)] 包含了从 171 个不同监控场景的视频中收集的 3,673 个标注样本，并为头发、面部、上衣/下衣、手臂和腿提供了像素级标注。它呈现了多种真实世界的挑战，例如姿态变化、光照变化和遮挡。训练图像有
    1,781 张，测试图像有 1,892 张。
- en: '$\bullet$ ATR [[22](#bib.bib22)] contains data which combined from three small
    benchmark datasets: the Fashionista [[1](#bib.bib1)] containing 685 images, the
    CFPD [[25](#bib.bib25)] containing 2,682 images, and the DailyPhotos [[14](#bib.bib14)]
    containing 2,500 images. The labels are merged of Fashionista and CFPD datasets
    to 18 categories. To enlarge the diversity, another 1,833 challenging images are
    collected and annotated to construct the Human Parsing in the Wild (HPW) dataset.
    The final combined dataset contains 7,700 images, which consists of 6,000 images
    for training, 1,000 for testing, and 700 as the validation set.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ ATR [[22](#bib.bib22)] 包含的数据来源于三个小型基准数据集：Fashionista [[1](#bib.bib1)]（包含
    685 张图像）、CFPD [[25](#bib.bib25)]（包含 2,682 张图像）和 DailyPhotos [[14](#bib.bib14)]（包含
    2,500 张图像）。标签合并了 Fashionista 和 CFPD 数据集，形成了 18 个类别。为了增加多样性，还收集并标注了另外 1,833 张具有挑战性的图像，以构建“野外人体解析”（HPW）数据集。最终合并的数据集包含
    7,700 张图像，其中 6,000 张用于训练，1,000 张用于测试，700 张作为验证集。
- en: $\bullet$ Chictopia10k [[37](#bib.bib37)] contains 10,000 real-world human pictures
    from Chictopia.com, annotating pixel-wise labels following [[22](#bib.bib22)].
    The dataset mainly contains images in the wild (*e.g*., more challenging poses,
    occlusion, and clothes).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Chictopia10k [[37](#bib.bib37)] 包含来自 Chictopia.com 的 10,000 张真实世界的人物图片，按照
    [[22](#bib.bib22)] 的方法进行逐像素标注。该数据集主要包含自然环境中的图像（*例如*，更具挑战性的姿势、遮挡和服装）。
- en: $\bullet$ SYSU-Clothes [[114](#bib.bib114)] consists of 2,098 high resolution
    fashion photos in high-resolution (about 800$\times$500 on average) from the shopping
    website. In this dataset, six categories of clothing attributes (*e.g*., clothing
    category, clothing color, clothing length, clothing shape, collar shape, and sleeve
    length) and 124 attribute types of all categories are collected.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ SYSU-Clothes [[114](#bib.bib114)] 包含 2,098 张高分辨率的时尚照片（平均约 800$\times$500），这些照片来自购物网站。在该数据集中，收集了六类服装属性（*例如*，服装类别、服装颜色、服装长度、服装形状、领口形状和袖子长度）以及所有类别的
    124 种属性类型。
- en: $\bullet$ Look into Person (LIP) [[116](#bib.bib116)] is the most popular single
    human parsing dataset, which is annotated with pixel-wise annotations with 19
    semantic human part labels and one background label. LIP contains 50,462 annotated
    images and be grouped into 30,462/10,000/10,000 for train/val/test. The images
    in the LIP dataset are cropped person instances from COCO [[163](#bib.bib163)]
    training and validation sets.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Look into Person (LIP) [[116](#bib.bib116)] 是最受欢迎的单人解析数据集，标注了 19 个语义人体部位标签和一个背景标签的逐像素注释。LIP
    包含 50,462 张标注图像，并分为 30,462/10,000/10,000 用于训练/验证/测试。LIP 数据集中的图像是从 COCO [[163](#bib.bib163)]
    的训练和验证集中裁剪出的人物实例。
- en: Remark. ATR and LIP are the mainstream benchmarks among these single human parsing
    datasets. In recent years, the research purpose has changed from “clothing” to
    “human”, and the data scale and annotation quality have also been significantly
    improved.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。ATR 和 LIP 是这些单人解析数据集中主流的基准。在近年来，研究目的已从“服装”转变为“人体”，数据规模和注释质量也得到了显著提升。
- en: 4.2 Multiple Human Parsing (MHP) Datasets
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 多人解析 (MHP) 数据集
- en: $\bullet$ PASCAL-Person-Part (PPP) [[160](#bib.bib160)] is annotated from the
    PASCAL-VOC-2010 [[164](#bib.bib164)], which contains 3,533 multi-person images
    with challenging poses and splits into 1,716 training images and 1,817 test images.
    Each image is pixel-wise annotated with 7 classes, namely head, torso, upper/lower
    arms, upper/lower legs, and a background category.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ PASCAL-Person-Part (PPP) [[160](#bib.bib160)] 从 PASCAL-VOC-2010 [[164](#bib.bib164)]
    中标注而来，包含 3,533 张多人人物图像，具有挑战性的姿势，并分为 1,716 张训练图像和 1,817 张测试图像。每张图像都以 7 个类别逐像素标注，即头部、躯干、上/下臂、上/下腿和背景类别。
- en: $\bullet$ MHP-v1.0 [[161](#bib.bib161)] contains 4,980 multi-person images with
    fine-grained annotations at pixel-level. For each person, it defines 7 body parts,
    11 clothing/accessory categories, and one background label. The train/val/test
    sets contain 3,000/1,000/980 images, respectively.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ MHP-v1.0 [[161](#bib.bib161)] 包含 4,980 张多人人物图像，具有逐像素的详细注释。对于每个人，定义了
    7 个身体部位、11 类服装/配饰类别和一个背景标签。训练/验证/测试集分别包含 3,000/1,000/980 张图像。
- en: $\bullet$ MHP-v2.0 [[162](#bib.bib162)] is an extend version of MHP-v1.0 [[161](#bib.bib161)],
    which provides more images and richer categories. MHP-v2.0 contains 25,403 images
    and has great diversity in image resolution (from 85$\times$100 to 4,511$\times$6,919)
    and human instance number (from 2 to 26 persons). These images are split into
    15,403/5,000/5,000 for train/val/test with 59 categories.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ MHP-v2.0 [[162](#bib.bib162)] 是 MHP-v1.0 [[161](#bib.bib161)] 的扩展版本，提供了更多的图像和更丰富的类别。MHP-v2.0
    包含 25,403 张图像，在图像分辨率（从 85$\times$100 到 4,511$\times$6,919）和人物实例数量（从 2 到 26 人）上具有很大的多样性。这些图像被分为
    15,403/5,000/5,000 用于训练/验证/测试，共有 59 个类别。
- en: $\bullet$ COCO-DensePose (COCO-DP) [[74](#bib.bib74)] aims at establishing the
    mapping between all human pixels of an RGB image and the 3D surface of the human
    body, and has 27,659 images (26,151/1,508 for train/test splits) gathered from
    COCO [[163](#bib.bib163)]. The dataset provides 15 pixel-wise human parts with
    dense keypoints annotations.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ COCO-DensePose (COCO-DP) [[74](#bib.bib74)] 旨在建立 RGB 图像中所有人类像素与人体
    3D 表面之间的映射，包含 27,659 张图像（26,151/1,508 用于训练/测试拆分），这些图像收集自 COCO [[163](#bib.bib163)]。该数据集提供了
    15 个逐像素的人体部位密集关键点标注。
- en: $\bullet$ Crowd Instance-level Human Parsing (CIHP) [[8](#bib.bib8)] is the
    largest multiple human parsing dataset to date. With 38,280 diverse real-world
    images, the persons are labelled with pixel-wise annotations on 20 categories.
    It consists of 28,280 training and 5,000 validation images with publicly available
    annotations, as well as 5,000 test images with annotations withheld for benchmarking
    purposes. All images of the CIHP dataset contain two or more instances with an
    average of 3.4.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Crowd Instance-level Human Parsing (CIHP) [[8](#bib.bib8)] 是迄今为止最大的多人解析数据集。该数据集包含38,280张多样的真实世界图像，对每个人进行了20类的像素级标注。它包括28,280张训练图像和5,000张验证图像，标注信息公开可用，以及5,000张测试图像，标注信息被保留用于基准测试。CIHP数据集中的所有图像都包含两个或更多实例，平均每图像3.4个实例。
- en: Remark. So far, several multiple human parsing datasets have high-quality annotation
    and considerable data scale. In addition to pixel-wise parsing annotations, many
    datasets provide other rich annotations, such as box, keypoints/landmark and style.
    PPP, CIHP and MHP-v2.0 are widely studied datasets, and most classical multiple
    human parsing methods have been verified on them.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。到目前为止，几个人类解析数据集具有高质量的标注和可观的数据规模。除了像素级解析标注外，许多数据集还提供了其他丰富的标注，如框、关键点/地标和风格。PPP、CIHP和MHP-v2.0是广泛研究的数据集，大多数经典的多人解析方法已在这些数据集上得到验证。
- en: 4.3 Video Human Parsing (VHP) Datasets
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 视频人类解析 (VHP) 数据集
- en: $\bullet$ Video Instance-level Parsing (VIP) [[9](#bib.bib9)] is the first video
    human parsing dataset. VIP contains 404 multi-person Full HD sequences, which
    are collected from Youtube with great diversity. For every 25 consecutive frames
    in each sequence, one frame is densely annotated with 20 classes and identities.
    All the sequences are grouped into 354/50 for train/test, containing 18,468/2,778
    annotated frames respectively.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Video Instance-level Parsing (VIP) [[9](#bib.bib9)] 是第一个视频人类解析数据集。VIP包含404个多人人物的全高清序列，这些序列从Youtube收集，具有很大的多样性。在每个序列的每25帧中，有一帧进行了20类和身份的密集标注。所有序列分为354/50进行训练/测试，分别包含18,468/2,778个标注帧。
- en: Remark. Since video human parsing has only attracted attention in recent years,
    there are few publicly available datasets, and its data scale and richness still
    need to be continuously invested by the community.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。由于视频人类解析近年来才引起关注，目前公开可用的数据集很少，其数据规模和丰富度仍需要社区持续投入。
- en: 4.4 Summary
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 总结
- en: 'Through Table [VII](#S4.T7 "Table VII ‣ 4 Human Parsing Datasets ‣ Deep Learning
    Technique for Human Parsing: A Survey and Outlook"), we can observe that the human
    parsing datasets show several development trends. Firstly, the scale of datasets
    continues to increase, from hundreds in the early years [[1](#bib.bib1)] to a
    tens of thousands now [[116](#bib.bib116), [8](#bib.bib8)]. Secondly, the quality
    of annotation is constantly improving. Some early datasets use super-pixel [[1](#bib.bib1),
    [114](#bib.bib114), [116](#bib.bib116)] to reduce the annotation cost, while in
    recent years, pixel-wise accurate annotation has been adopted. Finally, the annotation
    dimensions are becoming increasingly diverse, *e.g*., COCO-DensePose [[74](#bib.bib74)]
    provides boxes, keypoints, and UVs annotation in addition to parsing.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '通过表 [VII](#S4.T7 "Table VII ‣ 4 Human Parsing Datasets ‣ Deep Learning Technique
    for Human Parsing: A Survey and Outlook")，我们可以观察到人类解析数据集显示出几个发展趋势。首先，数据集的规模不断增加，从早期的数百个到现在的数万个。其次，标注质量不断提高。一些早期数据集使用超像素
    [[1](#bib.bib1), [114](#bib.bib114), [116](#bib.bib116)] 来降低标注成本，而近年来已采用像素级准确标注。最后，标注维度越来越多样化，*例如*，COCO-DensePose
    [[74](#bib.bib74)] 除了解析外，还提供了框、关键点和UVs标注。'
- en: 5 Performance Comparisons
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 性能比较
- en: To provide a more intuitive comparison, we tabulate the performance of several
    previously discussed models. It should be noted that the experimental settings
    of each study are not entirely consistent (*e.g*., backbone, input size, training
    epochs). Therefore, we suggest only taking these comparisons as references, and
    a more specific analysis needs to study the original articles deeply.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供更直观的比较，我们列出了几种先前讨论的模型的性能。需要注意的是，每项研究的实验设置并不完全一致（*例如*，骨干网络、输入大小、训练周期）。因此，我们建议仅将这些比较作为参考，更具体的分析需要深入研究原始文章。
- en: 5.1 SHP Performance Benchmarking
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 SHP性能基准测试
- en: We select ATR [[22](#bib.bib22)] and LIP [[116](#bib.bib116)] as the benchmark
    for single human parsing performance comparison, and compared 14 and 26 models,
    respectively.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择 ATR [[22](#bib.bib22)] 和 LIP [[116](#bib.bib116)] 作为单人解析性能比较的基准，分别比较了 14
    个和 26 个模型。
- en: 5.1.1 Evaluation Metrics
  id: totrans-363
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 评估指标
- en: The evaluation metrics of single human parsing are basically consistent with
    semantic segmentation [[31](#bib.bib31)], including pixel accuracy, mean pixel
    accuracy, and mean IoU. In addition, foreground pixel accuracy and F-1 score are
    also commonly used metrics on the ATR dataset.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 单人解析的评估指标基本上与语义分割 [[31](#bib.bib31)] 一致，包括像素准确率、平均像素准确率和平均 IoU。此外，前景像素准确率和 F-1
    分数也是 ATR 数据集中常用的指标。
- en: $\bullet$ Pixel accuracy (pixAcc) is the simplest and intuitive metric, which
    expresses the proportion of pixels with correct prediction in the overall pixel.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 像素准确率 (pixAcc) 是最简单直观的指标，它表示在总体像素中正确预测的像素比例。
- en: $\bullet$ Foreground pixel accuracy (FGAcc) only calculates the pixel accuracy
    of foreground human parts.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 前景像素准确率 (FGAcc) 仅计算前景人部分的像素准确率。
- en: $\bullet$ Mean pixel accuracy (meanAcc) is a simple improvement of pixel accuracy,
    which calculates the proportion of correctly predicted pixels in each category.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 平均像素准确率 (meanAcc) 是像素准确率的简单改进，它计算每个类别中正确预测像素的比例。
- en: $\bullet$ Mean IoU (mIoU) is short for mean intersection over union, which calculates
    the ratio of the intersection and union of two sets. The two sets are the ground-truth
    and predicted results of each category respectively.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 平均 IoU (mIoU) 是“平均交并比”的简称，它计算两个集合的交集和并集的比例。这两个集合分别是每个类别的真实值和预测结果。
- en: $\bullet$ F-1 score (F-1) is the harmonic average of precision and recall, which
    is a common evaluation metric.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ F-1 分数 (F-1) 是精确率和召回率的调和平均数，是一种常见的评估指标。
- en: 'Table VIII: Quantitative SHP results on ATR test (§[5.1](#S5.SS1 "5.1 SHP Performance
    Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")) in terms of pixel accuracy (pixAcc), foreground pixel
    accuracy (FGAcc) and F-1 score (F-1). The three best scores are marked in red,
    blue, and green, respectively.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VIII：ATR 测试中的定量 SHP 结果 (§[5.1](#S5.SS1 "5.1 SHP Performance Benchmarking
    ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook"))，以像素准确率 (pixAcc)、前景像素准确率 (FGAcc) 和 F-1 分数 (F-1) 为标准。前三个最佳分数分别用红色、蓝色和绿色标记。'
- en: '|   Year | Method | Pub. | Backbone | #Input Size | #Epoch | pixAcc | FGAcc
    | F-1 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|   年份 | 方法 | 发表 | 基础网络 | 输入尺寸 | 轮次 | pixAcc | FGAcc | F-1 |'
- en: '| 2012 | Yamaguchi [[1](#bib.bib1)] | CVPR | - | - | - | 84.38 | 55.59 | 41.80
    |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | Yamaguchi [[1](#bib.bib1)] | CVPR | - | - | - | 84.38 | 55.59 | 41.80
    |'
- en: '| 2013 | Paperdoll [[20](#bib.bib20)] | ICCV | - | - | - | 88.96 | 62.18 |
    44.76 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | Paperdoll [[20](#bib.bib20)] | ICCV | - | - | - | 88.96 | 62.18 |
    44.76 |'
- en: '| 2015 | M-CNN [[110](#bib.bib110)] | CVPR | - | - | 50 | 89.57 | 73.98 | 62.81
    |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | M-CNN [[110](#bib.bib110)] | CVPR | - | - | 50 | 89.57 | 73.98 | 62.81
    |'
- en: '| Co-CNN [[37](#bib.bib37)] | ICCV | - | 150$\times$100 | 90 | 95.23 | 80.90
    | 76.95 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| Co-CNN [[37](#bib.bib37)] | ICCV | - | 150$\times$100 | 90 | 95.23 | 80.90
    | 76.95 |'
- en: '| ATR [[22](#bib.bib22)] | TPAMI | - | 227$\times$227 | 120 | 91.11 | 71.04
    | 64.38 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| ATR [[22](#bib.bib22)] | TPAMI | - | 227$\times$227 | 120 | 91.11 | 71.04
    | 64.38 |'
- en: '| 2016 | LG-LSTM [[34](#bib.bib34)] | CVPR | VGG16 | 321$\times$321 | 60 |
    96.18 | 84.79 | 80.97 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | LG-LSTM [[34](#bib.bib34)] | CVPR | VGG16 | 321$\times$321 | 60 |
    96.18 | 84.79 | 80.97 |'
- en: '| Graph-LSTM [[113](#bib.bib113)] | ECCV | VGG16 | 321$\times$321 | 60 | 97.60
    | 91.42 | 83.76 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| Graph-LSTM [[113](#bib.bib113)] | ECCV | VGG16 | 321$\times$321 | 60 | 97.60
    | 91.42 | 83.76 |'
- en: '| 2017 | Struc-LSTM [[115](#bib.bib115)] | CVPR | VGG16 | 321$\times$321 |
    60 | 97.71 | 91.76 | 87.88 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Struc-LSTM [[115](#bib.bib115)] | CVPR | VGG16 | 321$\times$321 |
    60 | 97.71 | 91.76 | 87.88 |'
- en: '| 2018 | TGPNet [[119](#bib.bib119)] | MM | VGG16 | 321$\times$321 | 35 | 96.45
    | 87.91 | 81.76 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | TGPNet [[119](#bib.bib119)] | MM | VGG16 | 321$\times$321 | 35 | 96.45
    | 87.91 | 81.76 |'
- en: '| 2019 | CNIF [[3](#bib.bib3)] | ICCV | ResNet101 | 473$\times$473 | 150 |
    96.26 | 87.91 | 85.51 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | CNIF [[3](#bib.bib3)] | ICCV | ResNet101 | 473$\times$473 | 150 |
    96.26 | 87.91 | 85.51 |'
- en: '| 2020 | CorrPM [[45](#bib.bib45)] | CVPR | ResNet101 | 384$\times$384 | 150
    | 97.12 | 90.40 | 86.12 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | CorrPM [[45](#bib.bib45)] | CVPR | ResNet101 | 384$\times$384 | 150
    | 97.12 | 90.40 | 86.12 |'
- en: '| HHP [[4](#bib.bib4)] | CVPR | ResNet101 | 473$\times$473 | 150 | 96.84 |
    89.23 | 87.25 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| HHP [[4](#bib.bib4)] | CVPR | ResNet101 | 473$\times$473 | 150 | 96.84 |
    89.23 | 87.25 |'
- en: '| SCHP [[52](#bib.bib52)] | TPAMI | ResNet101 | 473$\times$473 | 150 | 96.25
    | 87.97 | 85.55 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| SCHP [[52](#bib.bib52)] | TPAMI | ResNet101 | 473$\times$473 | 150 | 96.25
    | 87.97 | 85.55 |'
- en: '| 2022 | CDGNet [[131](#bib.bib131)] | CVPR | ResNet101 | 512$\times$512 |
    250 | 97.39 | 90.19 | 87.16 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | CDGNet [[131](#bib.bib131)] | CVPR | ResNet101 | 512$\times$512 |
    250 | 97.39 | 90.19 | 87.16 |'
- en: 'Table IX: Quantitative SHP results on LIP val (§[5.1](#S5.SS1 "5.1 SHP Performance
    Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")) in terms of pixel accuracy (pixAcc), mean pixel accuracy
    (meanAcc) and mean IoU (mIoU). The three best scores are marked in red, blue,
    and green, respectively.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IX：LIP 验证集上的定量 SHP 结果（§[5.1](#S5.SS1 "5.1 SHP Performance Benchmarking ‣
    5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook")），包括像素准确率（pixAcc）、平均像素准确率（meanAcc）和平均IoU（mIoU）。前三个最佳得分分别用红色、蓝色和绿色标记。'
- en: '|   Year | Method | Pub. | Backbone | #Input Size | #Epoch | pixAcc | meanAcc
    | mIoU |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|   年份 | 方法 | 发表 | 主干网络 | 输入大小 | 轮数 | 像素准确率 | 平均准确率 | 平均IoU |'
- en: '| 2017 | SSL [[116](#bib.bib116)] | CVPR | VGG16 | 321$\times$321 | 50 | -
    | - | 46.19 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | SSL [[116](#bib.bib116)] | CVPR | VGG16 | 321$\times$321 | 50 | -
    | - | 46.19 |'
- en: '| 2018 | HSP-PRI [[76](#bib.bib76)] | CVPR | InceptionV3 | - | - | 85.07 |
    60.54 | 48.16 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | HSP-PRI [[76](#bib.bib76)] | CVPR | InceptionV3 | - | - | 85.07 |
    60.54 | 48.16 |'
- en: '| MMAN [[50](#bib.bib50)] | ECCV | ResNet101 | 256$\times$256 | 30 | 85.24
    | 57.60 | 46.93 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| MMAN [[50](#bib.bib50)] | ECCV | ResNet101 | 256$\times$256 | 30 | 85.24
    | 57.60 | 46.93 |'
- en: '| MuLA [[47](#bib.bib47)] | ECCV | Hourglass | 256$\times$256 | 250 | 88.50
    | 60.50 | 49.30 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| MuLA [[47](#bib.bib47)] | ECCV | Hourglass | 256$\times$256 | 250 | 88.50
    | 60.50 | 49.30 |'
- en: '| JPPNet [[2](#bib.bib2)] | TPAMI | ResNet101 | 384$\times$384 | 60 | 86.39
    | 62.32 | 51.37 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| JPPNet [[2](#bib.bib2)] | TPAMI | ResNet101 | 384$\times$384 | 60 | 86.39
    | 62.32 | 51.37 |'
- en: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | ResNet101 | 473$\times$473 | 150
    | 87.37 | 63.20 | 53.10 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | ResNet101 | 473$\times$473 | 150
    | 87.37 | 63.20 | 53.10 |'
- en: '| CNIF [[3](#bib.bib3)] | ICCV | ResNet101 | 473$\times$473 | 150 | 88.03 |
    68.80 | 57.74 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| CNIF [[3](#bib.bib3)] | ICCV | ResNet101 | 473$\times$473 | 150 | 88.03 |
    68.80 | 57.74 |'
- en: '| BraidNet [[57](#bib.bib57)] | MM | ResNet101 | 384$\times$384 | 150 | 87.60
    | 66.09 | 54.42 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| BraidNet [[57](#bib.bib57)] | MM | ResNet101 | 384$\times$384 | 150 | 87.60
    | 66.09 | 54.42 |'
- en: '| 2020 | CorrPM [[45](#bib.bib45)] | CVPR | ResNet101 | 384$\times$384 | 150
    | - | - | 55.33 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | CorrPM [[45](#bib.bib45)] | CVPR | ResNet101 | 384$\times$384 | 150
    | - | - | 55.33 |'
- en: '| SLRS [[51](#bib.bib51)] | CVPR | ResNet101 | 384$\times$384 | 150 | 88.33
    | 66.53 | 56.34 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| SLRS [[51](#bib.bib51)] | CVPR | ResNet101 | 384$\times$384 | 150 | 88.33
    | 66.53 | 56.34 |'
- en: '| PCNet [[39](#bib.bib39)] | CVPR | ResNet101 | 473$\times$473 | 120 | - |
    - | 57.03 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| PCNet [[39](#bib.bib39)] | CVPR | ResNet101 | 473$\times$473 | 120 | - |
    - | 57.03 |'
- en: '| HHP [[4](#bib.bib4)] | CVPR | ResNet101 | 473$\times$473 | 150 | 89.05 |
    70.58 | 59.25 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| HHP [[4](#bib.bib4)] | CVPR | ResNet101 | 473$\times$473 | 150 | 89.05 |
    70.58 | 59.25 |'
- en: '| DTCF [[46](#bib.bib46)] | MM | ResNet101 | 473$\times$473 | 200 | 88.61 |
    68.89 | 57.82 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| DTCF [[46](#bib.bib46)] | MM | ResNet101 | 473$\times$473 | 200 | 88.61 |
    68.89 | 57.82 |'
- en: '| SemaTree [[41](#bib.bib41)] | ECCV | ResNet101 | 384$\times$384 | 200 | 88.05
    | 66.42 | 54.73 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| SemaTree [[41](#bib.bib41)] | ECCV | ResNet101 | 384$\times$384 | 200 | 88.05
    | 66.42 | 54.73 |'
- en: '| OCR [[122](#bib.bib122)] | ECCV | HRNetW48 | 473$\times$473 | $\scriptstyle\sim$100
    | - | - | 56.65 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| OCR [[122](#bib.bib122)] | ECCV | HRNetW48 | 473$\times$473 | $\scriptstyle\sim$100
    | - | - | 56.65 |'
- en: '| BGNet [[123](#bib.bib123)] | ECCV | ResNet101 | 473$\times$473 | 120 | -
    | - | 56.82 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| BGNet [[123](#bib.bib123)] | ECCV | ResNet101 | 473$\times$473 | 120 | -
    | - | 56.82 |'
- en: '| HRNet [[124](#bib.bib124)] | TPAMI | HRNetW48 | 473$\times$473 | $\scriptstyle\sim$150
    | 88.21 | 67.43 | 55.90 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| HRNet [[124](#bib.bib124)] | TPAMI | HRNetW48 | 473$\times$473 | $\scriptstyle\sim$150
    | 88.21 | 67.43 | 55.90 |'
- en: '| SCHP [[52](#bib.bib52)] | TPAMI | ResNet101 | 473$\times$473 | 150 | - |
    - | 59.36 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| SCHP [[52](#bib.bib52)] | TPAMI | ResNet101 | 473$\times$473 | 150 | - |
    - | 59.36 |'
- en: '| 2021 | HIPN [[125](#bib.bib125)] | AAAI | ResNet101 | 473$\times$473 | 150
    | 89.14 | 71.09 | 59.61 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | HIPN [[125](#bib.bib125)] | AAAI | ResNet101 | 473$\times$473 | 150
    | 89.14 | 71.09 | 59.61 |'
- en: '| MCIBI [[126](#bib.bib126)] | ICCV | ResNet101 | 473$\times$473 | 150 | -
    | - | 55.42 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| MCIBI [[126](#bib.bib126)] | ICCV | ResNet101 | 473$\times$473 | 150 | -
    | - | 55.42 |'
- en: '| ISNet [[127](#bib.bib127)] | ICCV | ResNet101 | 473$\times$473 | 160 | -
    | - | 56.96 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| ISNet [[127](#bib.bib127)] | ICCV | ResNet101 | 473$\times$473 | 160 | -
    | - | 56.96 |'
- en: '| NPPNet [[128](#bib.bib128)] | ICCV | NAS | 384$\times$384 | 120 | - | - |
    58.56 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| NPPNet [[128](#bib.bib128)] | ICCV | NAS | 384$\times$384 | 120 | - | - |
    58.56 |'
- en: '| HTCorrM [[129](#bib.bib129)] | TPAMI | HRNetW48 | 384$\times$384 | 180 |
    - | - | 56.85 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| HTCorrM [[129](#bib.bib129)] | TPAMI | HRNetW48 | 384$\times$384 | 180 |
    - | - | 56.85 |'
- en: '| 2022 | CDGNet [[131](#bib.bib131)] | CVPR | ResNet101 | 473$\times$473 |
    150 | 88.86 | 71.49 | 60.30 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | CDGNet [[131](#bib.bib131)] | CVPR | ResNet101 | 473$\times$473 |
    150 | 88.86 | 71.49 | 60.30 |'
- en: '| HSSN [[5](#bib.bib5)] | CVPR | ResNet101 | 480$\times$480 | $\scriptstyle\sim$84
    | - | - | 60.37 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| HSSN [[5](#bib.bib5)] | CVPR | ResNet101 | 480$\times$480 | $\scriptstyle\sim$84
    | - | - | 60.37 |'
- en: '| PRM [[43](#bib.bib43)] | TMM | ResNet101 | 473$\times$473 | 120 | - | - |
    58.86 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| PRM [[43](#bib.bib43)] | TMM | ResNet101 | 473$\times$473 | 120 | - | - |
    58.86 |'
- en: 5.1.2 Results
  id: totrans-414
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 结果
- en: 'Table [VIII](#S5.T8 "Table VIII ‣ 5.1.1 Evaluation Metrics ‣ 5.1 SHP Performance
    Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook") presents the performance of the reviewed SHP methods on
    ATR test set. Struc-LSTM [[115](#bib.bib115)] achieves the best performance, scoring
    91.71% pixAcc. and 87.88% F-1 score, which greatly surpassed other methods. Table [IX](#S5.T9
    "Table IX ‣ 5.1.1 Evaluation Metrics ‣ 5.1 SHP Performance Benchmarking ‣ 5 Performance
    Comparisons ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")
    shows the method results on the LIP benchmark since 2017\. Overall, HIPN [[125](#bib.bib125)]
    and HSSN [[5](#bib.bib5)] achieve remarkable results in various metrics, in which
    HIPN scored 89.14% pixelAcc and HSSN scored 60.37% mIoU.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [VIII](#S5.T8 "Table VIII ‣ 5.1.1 Evaluation Metrics ‣ 5.1 SHP Performance
    Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook") 展示了所评审的 SHP 方法在 ATR 测试集上的表现。Struc-LSTM [[115](#bib.bib115)]
    实现了最佳表现，获得了 91.71% 的像素准确率和 87.88% 的 F-1 分数，远远超过了其他方法。表 [IX](#S5.T9 "Table IX ‣
    5.1.1 Evaluation Metrics ‣ 5.1 SHP Performance Benchmarking ‣ 5 Performance Comparisons
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook") 显示了自 2017
    年以来在 LIP 基准测试上的方法结果。总体而言，HIPN [[125](#bib.bib125)] 和 HSSN [[5](#bib.bib5)] 在各种指标上取得了显著的成绩，其中
    HIPN 得分 89.14% 的像素准确率，HSSN 得分 60.37% 的 mIoU。'
- en: 5.2 MHP Performance Benchmarking
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 MHP 性能基准测试
- en: We select 7 models experimented on PASCAL-Person-Part [[160](#bib.bib160)],
    9 models experimented on CIHP [[8](#bib.bib8)] and 8 models experimented on MHP-v2
    [[162](#bib.bib162)] to compare the performance of multiple human parsing.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了在 PASCAL-Person-Part [[160](#bib.bib160)] 上实验的 7 个模型、在 CIHP [[8](#bib.bib8)]
    上实验的 9 个模型和在 MHP-v2 [[162](#bib.bib162)] 上实验的 8 个模型，以比较多人解析的性能。
- en: 'Table X: Quantitative MHP results on PASCAL-Person-Part test (§[5.2](#S5.SS2
    "5.2 MHP Performance Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning
    Technique for Human Parsing: A Survey and Outlook")) in terms of mIoU, AP${}^{\text{r}}_{\text{vol}}$
    and AP${}^{\text{r}}_{\text{50}}$. We only mark the best score in red color.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '表 X：PASCAL-Person-Part 测试的定量 MHP 结果 (§[5.2](#S5.SS2 "5.2 MHP Performance Benchmarking
    ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook")) 以 mIoU、AP${}^{\text{r}}_{\text{vol}}$ 和 AP${}^{\text{r}}_{\text{50}}$
    为标准。我们只用红色标记最佳分数。'
- en: '|   Year | Method | Pub. | Pipeline | Backbone | #Epoch | mIoU | AP${}^{\text{r}}_{\text{vol}}$
    | AP${}^{\text{r}}_{\text{50}}$ |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|   年份 | 方法 | 发表 | 流程 | 主干网络 | 训练轮数 | mIoU | AP${}^{\text{r}}_{\text{vol}}$
    | AP${}^{\text{r}}_{\text{50}}$ |'
- en: '| 2017 | Holistic [[138](#bib.bib138)] | BMVC | 1S-TD | ResNet101 | 100 | 66.34
    | 38.40 | 40.60 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Holistic [[138](#bib.bib138)] | BMVC | 1S-TD | ResNet101 | 100 | 66.34
    | 38.40 | 40.60 |'
- en: '| 2018 | PGN [[8](#bib.bib8)] | ECCV | BU | ResNet101 | $\scriptstyle\sim$80
    | 68.40 | 39.20 | 39.60 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | PGN [[8](#bib.bib8)] | ECCV | BU | ResNet101 | $\scriptstyle\sim$80
    | 68.40 | 39.20 | 39.60 |'
- en: '| 2019 | Parsing R-CNN [[61](#bib.bib61)] | CVPR | 1S-TD | ResNet50 | 75 |
    62.70 | 40.40 | 43.70 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Parsing R-CNN [[61](#bib.bib61)] | CVPR | 1S-TD | ResNet50 | 75 |
    62.70 | 40.40 | 43.70 |'
- en: '| Unified [[139](#bib.bib139)] | BMVC | 1S-TD | ResNet101 | $\scriptstyle\sim$600
    | - | 43.10 | 48.10 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| Unified [[139](#bib.bib139)] | BMVC | 1S-TD | ResNet101 | $\scriptstyle\sim$600
    | - | 43.10 | 48.10 |'
- en: '| 2020 | RP R-CNN [[140](#bib.bib140)] | ECCV | 1S-TD | ResNet50 | 75 | 63.30
    | 40.90 | 44.10 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | RP R-CNN [[140](#bib.bib140)] | ECCV | 1S-TD | ResNet50 | 75 | 63.30
    | 40.90 | 44.10 |'
- en: '| NAN [[141](#bib.bib141)] | IJCV | BU | - | 80 | - | 52.20 | 59.70 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| NAN [[141](#bib.bib141)] | IJCV | BU | - | 80 | - | 52.20 | 59.70 |'
- en: '| 2021 | MGHR [[59](#bib.bib59)] | CVPR | BU | ResNet101 | 150 | - | 55.90
    | 59.00 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | MGHR [[59](#bib.bib59)] | CVPR | BU | ResNet101 | 150 | - | 55.90
    | 59.00 |'
- en: 5.2.1 Evaluation Metrics
  id: totrans-427
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 评估指标
- en: Generally speaking, multiple human parsing uses mIoU to measure the semantic
    segmentation performance, and AP${}^{\text{r}}_{\text{vol}}$/AP${}^{\text{r}}_{\text{50}}$
    or AP${}^{\text{p}}_{\text{vol}}$/AP${}^{\text{p}}_{\text{50}}$ to measure the
    performance of instance discrimination.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，多人解析使用 mIoU 来衡量语义分割性能，使用 AP${}^{\text{r}}_{\text{vol}}$/AP${}^{\text{r}}_{\text{50}}$
    或 AP${}^{\text{p}}_{\text{vol}}$/AP${}^{\text{p}}_{\text{50}}$ 来衡量实例区分性能。
- en: $\bullet$ Average precision based on region (AP${}^{\text{r}}_{\text{vol}}$/AP${}^{\text{r}}_{\text{50}}$)
    [[165](#bib.bib165)] is similar to AP metrics in object detection [[163](#bib.bib163)].
    If the IoU between the predicted part and ground-truth part is higher than a certain
    threshold, the prediction is considered to be correct, and the mean Average Precision
    is calculated. The defined AP${}^{\text{r}}_{\text{vol}}$ is the mean of the AP
    score for overlap thresholds varying from 0.1 to 0.9 in increments of 0.1 and
    AP${}^{\text{r}}_{\text{50}}$ is the AP score for threshold equals 0.5.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于区域的平均精度 (AP${}^{\text{r}}_{\text{vol}}$/AP${}^{\text{r}}_{\text{50}}$)
    [[165](#bib.bib165)] 类似于物体检测中的 AP 指标 [[163](#bib.bib163)]。如果预测部件与真实部件之间的 IoU 高于某个阈值，则预测被认为是正确的，并计算平均精度。定义的
    AP${}^{\text{r}}_{\text{vol}}$ 是重叠阈值从 0.1 到 0.9 以 0.1 为增量的 AP 分数的平均值，AP${}^{\text{r}}_{\text{50}}$
    是阈值为 0.5 时的 AP 分数。
- en: $\bullet$ Average precision based on part (AP${}^{\text{p}}_{\text{vol}}$/AP${}^{\text{p}}_{\text{50}}$)
    [[161](#bib.bib161), [141](#bib.bib141)] is adopted to evaluate the instance-level
    human parsing performance. AP${}^{\text{p}}$ is very similar to AP${}^{\text{r}}$
    in calculation mode, except that it calculates mIoU with the whole human body.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于部件的平均精度 (AP${}^{\text{p}}_{\text{vol}}$/AP${}^{\text{p}}_{\text{50}}$)
    [[161](#bib.bib161), [141](#bib.bib141)] 被用来评估实例级的人体解析性能。AP${}^{\text{p}}$ 的计算方式与
    AP${}^{\text{r}}$ 非常相似，唯一不同的是它计算整个身体的 mIoU。
- en: 5.2.2 Results
  id: totrans-431
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 结果
- en: 'PASCAL-Person-Part benchmark is the classical benchmark in multiple human parsing.
    Table [X](#S5.T10 "Table X ‣ 5.2 MHP Performance Benchmarking ‣ 5 Performance
    Comparisons ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")
    gathers the results of 7 models on PASCAL-Person-Part test set. PGN [[8](#bib.bib8)]
    is the top one in mIoU metric. In AP${}^{\text{r}}_{\text{vol}}$/AP${}^{\text{r}}_{\text{50}}$
    metrics, MGHR [[59](#bib.bib59)], and NAN [[141](#bib.bib141)] are the best two
    methods at present. The results on CIHP val set are summarized in Table [XI](#S5.T11
    "Table XI ‣ 5.2.2 Results ‣ 5.2 MHP Performance Benchmarking ‣ 5 Performance Comparisons
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook"). As seen,
    SCHP [[52](#bib.bib52)] performs the best on all metrics, which yields 67.67%
    mIoU, 52.74% AP${}^{\text{r}}_{\text{vol}}$, and 58.95% AP${}^{\text{r}}_{\text{50}}$.
    Table [XII](#S5.T12 "Table XII ‣ 5.2.2 Results ‣ 5.2 MHP Performance Benchmarking
    ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook") summarizes 8 models on MHP-v2 val set. SCHP achieves the best mIoU
    again. In terms of AP${}^{\text{p}}_{\text{vol}}$/AP${}^{\text{p}}_{\text{50}}$,
    RP R-CNN [[140](#bib.bib140)] has won the best results so far.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 'PASCAL-Person-Part 基准是多人人体解析中的经典基准。表[X](#S5.T10 "Table X ‣ 5.2 MHP Performance
    Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")汇总了7种模型在 PASCAL-Person-Part 测试集上的结果。PGN [[8](#bib.bib8)]
    在 mIoU 指标中排名第一。在 AP${}^{\text{r}}_{\text{vol}}$/AP${}^{\text{r}}_{\text{50}}$
    指标中，MGHR [[59](#bib.bib59)] 和 NAN [[141](#bib.bib141)] 目前是最好的两种方法。CIHP 验证集上的结果总结在表[XI](#S5.T11
    "Table XI ‣ 5.2.2 Results ‣ 5.2 MHP Performance Benchmarking ‣ 5 Performance Comparisons
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")中。如图所示，SCHP
    [[52](#bib.bib52)] 在所有指标上表现最佳，分别取得了 67.67% 的 mIoU，52.74% 的 AP${}^{\text{r}}_{\text{vol}}$
    和 58.95% 的 AP${}^{\text{r}}_{\text{50}}$。表[XII](#S5.T12 "Table XII ‣ 5.2.2 Results
    ‣ 5.2 MHP Performance Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning
    Technique for Human Parsing: A Survey and Outlook") 总结了 8 种模型在 MHP-v2 验证集上的表现。SCHP
    再次取得了最佳的 mIoU。在 AP${}^{\text{p}}_{\text{vol}}$/AP${}^{\text{p}}_{\text{50}}$ 指标方面，RP
    R-CNN [[140](#bib.bib140)] 迄今为止获得了最佳结果。'
- en: 'Table XI: Quantitative MHP results on CIHP val (§[5.2](#S5.SS2 "5.2 MHP Performance
    Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")) in terms of mIoU, AP${}^{\text{r}}_{\text{vol}}$ and AP${}^{\text{r}}_{\text{50}}$.
    We only mark the best score in red color.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XI：CIHP 验证集上的定量 MHP 结果 (§[5.2](#S5.SS2 "5.2 MHP Performance Benchmarking
    ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook"))，涵盖了 mIoU、AP${}^{\text{r}}_{\text{vol}}$ 和 AP${}^{\text{r}}_{\text{50}}$。我们只用红色标记最佳得分。'
- en: '|   Year | Method | Pub. | Pipeline | Backbone | #Epoch | mIoU | AP${}^{\text{r}}_{\text{vol}}$
    | AP${}^{\text{r}}_{\text{50}}$ |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '|   年份 | 方法 | 发表 | 流水线 | 主干网络 | 训练轮次 | mIoU | AP${}^{\text{r}}_{\text{vol}}$
    | AP${}^{\text{r}}_{\text{50}}$ |'
- en: '| 2018 | PGN [[8](#bib.bib8)] | ECCV | BU | ResNet101 | $\scriptstyle\sim$80
    | 55.80 | 33.60 | 35.80 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | PGN [[8](#bib.bib8)] | ECCV | BU | ResNet101 | $\scriptstyle\sim$80
    | 55.80 | 33.60 | 35.80 |'
- en: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | 2S-TD | ResNet101 | 150 | 59.50 |
    42.80 | 48.70 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | 2S-TD | ResNet101 | 150 | 59.50 |
    42.80 | 48.70 |'
- en: '| Parsing R-CNN [[61](#bib.bib61)] | CVPR | 1S-TD | ResNet50 | 75 | 56.30 |
    36.50 | 40.90 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| Parsing R-CNN [[61](#bib.bib61)] | CVPR | 1S-TD | ResNet50 | 75 | 56.30 |
    36.50 | 40.90 |'
- en: '| BraidNet [[57](#bib.bib57)] | MM | 2S-TD | ResNet101 | 150 | 60.62 | 43.59
    | 48.99 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| BraidNet [[57](#bib.bib57)] | MM | 2S-TD | ResNet101 | 150 | 60.62 | 43.59
    | 48.99 |'
- en: '| Unified [[139](#bib.bib139)] | BMVC | 1S-TD | ResNet101 | $\scriptstyle\sim$36
    | 53.50 | 37.00 | 41.80 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| Unified [[139](#bib.bib139)] | BMVC | 1S-TD | ResNet101 | $\scriptstyle\sim$36
    | 53.50 | 37.00 | 41.80 |'
- en: '| 2020 | RP R-CNN [[140](#bib.bib140)] | ECCV | 1S-TD | ResNet50 | 150 | 60.20
    | 42.30 | 48.20 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | RP R-CNN [[140](#bib.bib140)] | ECCV | 1S-TD | ResNet50 | 150 | 60.20
    | 42.30 | 48.20 |'
- en: '| SemaTree [[41](#bib.bib41)] | ECCV | 2S-TD | ResNet101 | 200 | 60.87 | 43.96
    | 49.27 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| SemaTree [[41](#bib.bib41)] | ECCV | 2S-TD | ResNet101 | 200 | 60.87 | 43.96
    | 49.27 |'
- en: '| SCHP [[52](#bib.bib52)] | TPAMI | 2S-TD | ResNet101 | 150 | 67.47 | 52.74
    | 58.94 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| SCHP [[52](#bib.bib52)] | TPAMI | 2S-TD | ResNet101 | 150 | 67.47 | 52.74
    | 58.94 |'
- en: '| 2022 | AIParsing [[142](#bib.bib142)] | TIP | 1S-TD | ResNet101 | 75 | 60.70
    | - | - |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | AIParsing [[142](#bib.bib142)] | TIP | 1S-TD | ResNet101 | 75 | 60.70
    | - | - |'
- en: 'Table XII: Quantitative MHP results on MHP-v2 val (§[5.2](#S5.SS2 "5.2 MHP
    Performance Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning Technique
    for Human Parsing: A Survey and Outlook")) in terms of mIoU, AP${}^{\text{p}}_{\text{vol}}$
    and AP${}^{\text{p}}_{\text{50}}$. We only mark the best score in red color.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 表 XII：MHP-v2 验证集上的定量 MHP 结果（§[5.2](#S5.SS2 "5.2 MHP 性能基准测试 ‣ 5 性能比较 ‣ 深度学习技术在人体解析中的应用：调查与展望")），以
    mIoU、AP${}^{\text{p}}_{\text{vol}}$ 和 AP${}^{\text{p}}_{\text{50}}$ 为指标。我们仅用红色标记最佳分数。
- en: '|   Year | Method | Pub. | Pipeline | Backbone | #Epoch | mIoU | AP${}^{\text{p}}_{\text{vol}}$
    | AP${}^{\text{p}}_{\text{50}}$ |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '|   年份 | 方法 | 发表 | 流水线 | 主干网络 | #Epoch | mIoU | AP${}^{\text{p}}_{\text{vol}}$
    | AP${}^{\text{p}}_{\text{50}}$ |'
- en: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | 2S-TD | ResNet101 | 150 | 41.11 |
    42.70 | 34.47 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | 2S-TD | ResNet101 | 150 | 41.11 |
    42.70 | 34.47 |'
- en: '| Parsing R-CNN [[61](#bib.bib61)] | CVPR | 1S-TD | ResNet50 | 75 | 36.20 |
    38.50 | 24.50 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| Parsing R-CNN [[61](#bib.bib61)] | CVPR | 1S-TD | ResNet50 | 75 | 36.20 |
    38.50 | 24.50 |'
- en: '| 2020 | RP R-CNN [[140](#bib.bib140)] | ECCV | 1S-TD | ResNet50 | 150 | 38.60
    | 46.80 | 45.30 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | RP R-CNN [[140](#bib.bib140)] | ECCV | 1S-TD | ResNet50 | 150 | 38.60
    | 46.80 | 45.30 |'
- en: '| SemaTree [[41](#bib.bib41)] | ECCV | 2S-TD | ResNet101 | 200 | - | 42.51
    | 34.36 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| SemaTree [[41](#bib.bib41)] | ECCV | 2S-TD | ResNet101 | 200 | - | 42.51
    | 34.36 |'
- en: '| NAN [[141](#bib.bib141)] | IJCV | BU | - | 80 | - | 41.78 | 25.14 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| NAN [[141](#bib.bib141)] | IJCV | BU | - | 80 | - | 41.78 | 25.14 |'
- en: '| SCHP [[52](#bib.bib52)] | TPAMI | 2S-TD | ResNet101 | 150 | 45.21 | 45.25
    | 35.10 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| SCHP [[52](#bib.bib52)] | TPAMI | 2S-TD | ResNet101 | 150 | 45.21 | 45.25
    | 35.10 |'
- en: '| 2021 | MGHR [[59](#bib.bib59)] | CVPR | BU | ResNet101 | 150 | 41.40 | 44.30
    | 39.00 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | MGHR [[59](#bib.bib59)] | CVPR | BU | ResNet101 | 150 | 41.40 | 44.30
    | 39.00 |'
- en: '| 2022 | AIParsing [[142](#bib.bib142)] | TIP | 1S-TD | ResNet101 | 75 | 40.10
    | 46.60 | 43.20 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | AIParsing [[142](#bib.bib142)] | TIP | 1S-TD | ResNet101 | 75 | 40.10
    | 46.60 | 43.20 |'
- en: 5.3 VHP Performance Benchmarking
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 VHP 性能基准测试
- en: VIP datasets is widely used to benchmark video human parsing. We selected 11
    models since 2018.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: VIP 数据集广泛用于视频人体解析的基准测试。自 2018 年以来，我们选择了 11 个模型。
- en: 5.3.1 Evaluation Metrics
  id: totrans-456
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 评价指标
- en: Similar to multiple human parsing, mIoU and AP${}^{\text{r}}_{\text{vol}}$ are
    also adopted for video human parsing performance evaluation.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于多人体解析，mIoU 和 AP${}^{\text{r}}_{\text{vol}}$ 也被用于视频人体解析性能评估。
- en: 5.3.2 Results
  id: totrans-458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 结果
- en: 'Table [XIII](#S5.T13 "Table XIII ‣ 5.4 Summary ‣ 5 Performance Comparisons
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook") gives the
    results of recent methods on VIP val set. It is clear that LIIR [[59](#bib.bib59)]
    and UVC+ [[155](#bib.bib155)] have achieved the best performance in mIoU and AP${}^{\text{r}}_{\text{vol}}$
    metrics respectively.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [XIII](#S5.T13 "表 XIII ‣ 5.4 摘要 ‣ 5 性能比较 ‣ 深度学习技术在人体解析中的应用：调查与展望") 显示了 VIP
    验证集上近期方法的结果。显然，LIIR [[59](#bib.bib59)] 和 UVC+ [[155](#bib.bib155)] 在 mIoU 和 AP${}^{\text{r}}_{\text{vol}}$
    指标上分别达到了最佳性能。
- en: 5.4 Summary
  id: totrans-460
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 摘要
- en: Through the above performance comparison, we can observe several apparent phenomena.
    The first and most important is the fairness of the experimental setting. For
    single human parsing and multiple human parsing, many studies have not given detailed
    experimental settings, or there are great differences in several essential hyper-parameters,
    resulting fair comparison impossible. The second is that most methods do not give
    the parameters number and the inference time, which makes some methods occupy
    an advantage in comparison by increasing the model capacity, and also brings trouble
    to some computationally sensitive application scenarios, such as social media
    and automatic driving.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上述性能比较，我们可以观察到几个明显的现象。首先也是最重要的是实验设置的公平性。对于单人解析和多人解析，许多研究未给出详细的实验设置，或者在几个关键超参数上存在较大差异，导致公平比较变得不可能。其次，大多数方法未给出参数数量和推理时间，这使得某些方法通过增加模型容量在比较中占据优势，同时也给一些计算敏感的应用场景带来困扰，例如社交媒体和自动驾驶。
- en: In addition to the above phenomena, we can also summarize some positive signals.
    Firstly, in recent years, human parsing research has shown an upward trend, especially
    from 2020\. Secondly, although some studies have achieved high performance on
    LIP, CIHP and VIP, these benchmarks are still not saturated. Thus the community
    still needs to continue its efforts. Thirdly, some specific issues and hotspots
    of human parsing are gradually attracting people’s attention, which will further
    promote the progress of the whole field.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述现象外，我们还可以总结出一些积极的信号。首先，近年来，人类解析研究呈上升趋势，特别是从2020年开始。其次，尽管一些研究在LIP、CIHP和VIP上取得了高性能，这些基准测试仍未饱和。因此，社区仍需继续努力。第三，一些特定问题和热点问题逐渐引起了人们的关注，这将进一步促进整个领域的进步。
- en: 'Table XIII: Quantitative VHP results on VIP val (§[5.2](#S5.SS2 "5.2 MHP Performance
    Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")) in terms of mIoU and AP${}^{\text{r}}_{\text{vol}}$. The
    three best scores are marked in red, blue, and green, respectively.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XIII：VIP验证集上量化的VHP结果（§[5.2](#S5.SS2 "5.2 MHP Performance Benchmarking ‣ 5
    Performance Comparisons ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook")），以mIoU和AP${}^{\text{r}}_{\text{vol}}$为衡量标准。三项最佳成绩分别用红色、蓝色和绿色标记。'
- en: '|   Year | Method | Pub. | Backbone | mIoU | AP${}^{\text{r}}_{\text{vol}}$
    |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 方法 | 发表 | 骨干网络 | mIoU | AP${}^{\text{r}}_{\text{vol}}$ |'
- en: '| 2019 | TimeCycle [[146](#bib.bib146)] | CVPR | ResNet50 | 28.9 | 15.6 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | TimeCycle [[146](#bib.bib146)] | CVPR | ResNet50 | 28.9 | 15.6 |'
- en: '| UVC [[147](#bib.bib147)] | NeurIPS | ResNet18 | 34.1 | 17.7 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| UVC [[147](#bib.bib147)] | NeurIPS | ResNet18 | 34.1 | 17.7 |'
- en: '| 2020 | CRW [[148](#bib.bib148)] | NeurIPS | ResNet18 | 38.6 | - |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | CRW [[148](#bib.bib148)] | NeurIPS | ResNet18 | 38.6 | - |'
- en: '| 2021 | ContrastCorr [[149](#bib.bib149)] | AAAI | ResNet18 | 37.4 | 21.6
    |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | ContrastCorr [[149](#bib.bib149)] | AAAI | ResNet18 | 37.4 | 21.6
    |'
- en: '| CLTC [[150](#bib.bib150)] | CVPR | ResNet18 | 37.8 | 19.1 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| CLTC [[150](#bib.bib150)] | CVPR | ResNet18 | 37.8 | 19.1 |'
- en: '| VFS [[151](#bib.bib151)] | ICCV | ResNet18 | 39.9 | - |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| VFS [[151](#bib.bib151)] | ICCV | ResNet18 | 39.9 | - |'
- en: '| JSTG [[152](#bib.bib152)] | ICCV | ResNet18 | 40.2 | - |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| JSTG [[152](#bib.bib152)] | ICCV | ResNet18 | 40.2 | - |'
- en: '| 2022 | LIIR [[153](#bib.bib153)] | CVPR | ResNet18 | 41.2 | 22.1 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | LIIR [[153](#bib.bib153)] | CVPR | ResNet18 | 41.2 | 22.1 |'
- en: '| SCC [[154](#bib.bib154)] | CVPR | ResNet18 | 40.8 | - |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| SCC [[154](#bib.bib154)] | CVPR | ResNet18 | 40.8 | - |'
- en: '| UVC+ [[155](#bib.bib155)] | ArXiv | ResNet18 | 38.3 | 22.2 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| UVC+ [[155](#bib.bib155)] | ArXiv | ResNet18 | 38.3 | 22.2 |'
- en: '6 An Outlook: Future Opportunities of Human Parsing'
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 展望：未来的人体解析机遇
- en: After ten years of long development, with the whole community’s efforts, human
    parsing has made remarkable achievements, but it has also encountered a bottleneck.
    In this section, we will discuss the opportunities of human parsing in the next
    era from multiple perspectives, hoping to promote progress in the field.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在十年的长期发展中，经过整个社区的努力，人类解析取得了显著成就，但也遇到了瓶颈。在这一部分，我们将从多个角度讨论下一时代的人体解析机遇，希望能够促进该领域的进步。
- en: '![Refer to caption](img/7eaccd21a1936b0b3d97f7a2b69204d9.png)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7eaccd21a1936b0b3d97f7a2b69204d9.png)'
- en: 'Figure 6: Architecture of the proposed M2FP (§[6.1](#S6.SS1 "6.1 A Transformer-based
    Baseline for Human Parsing ‣ 6 An Outlook: Future Opportunities of Human Parsing
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")). Through
    the explicit construction of background, part and human queries, we can model
    the relationship between humans and parts, and predict high-quality masks.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：所提议的 M2FP 的架构 (§[6.1](#S6.SS1 "6.1 基于 Transformer 的人类解析基线 ‣ 6 展望：人类解析的未来机会
    ‣ 深度学习技术在人类解析中的调查与展望"))。通过明确构建背景、部件和人类查询，我们可以建模人类与部件之间的关系，并预测高质量的掩码。
- en: 6.1 A Transformer-based Baseline for Human Parsing
  id: totrans-479
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 基于 Transformer 的人类解析基线
- en: Although several mainstream benchmarks of human parsing have not been saturated,
    the accuracy growth has slowed down. The reason for this, we believe, is that
    some advances in deep learning have not yet benefited the human parsing task (*e.g*.,
    transformer [[166](#bib.bib166), [167](#bib.bib167), [168](#bib.bib168)], unsupervised
    representation learning [[169](#bib.bib169), [158](#bib.bib158), [170](#bib.bib170),
    [171](#bib.bib171)]), and the lack of a concise and easily extensible code base
    for researchers. Therefore, the community urgently needs a new and strong baseline.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些主流的人类解析基准尚未达到饱和，但准确率的增长已减缓。我们认为原因在于，深度学习的一些进展尚未对人类解析任务产生好处 (*例如*，transformer
    [[166](#bib.bib166), [167](#bib.bib167), [168](#bib.bib168)]，无监督表示学习 [[169](#bib.bib169),
    [158](#bib.bib158), [170](#bib.bib170), [171](#bib.bib171)])，以及缺乏一个简洁且易于扩展的代码库。因此，社区迫切需要一个新的强基线。
- en: 'We consider that a new human parsing baseline should have the following four
    characteristics: a) Universality, which can be applied to all mainstream human
    parsing tasks, including SHP, MHP, and VIP; b) Conciseness, the baseline method
    should not be too complex; c) Extensibility, complete code base, easy to modify
    or expand other modules or methods; d) High performance, state-of-the-arts or
    at least comparable performance can be achieved on the mainstream benchmarks under
    the fair experimental setting. Based on the above views, we design a new transformer-based
    baseline for human parsing. The proposed new baseline is based on the Mask2Former
    [[172](#bib.bib172)] architecture, with a few improvements adapted to human parsing,
    called Mask2Former for Parsing (M2FP). M2FP can adapt to almost all human parsing
    tasks and yield amazing performances.¹¹1Code and models are publicly available
    at [https://github.com/soeaver/M2FP](https://github.com/soeaver/M2FP)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，一个新的人的解析基线应该具备以下四个特点：a) 通用性，可以应用于所有主流的人类解析任务，包括 SHP、MHP 和 VIP；b) 简洁性，基线方法不应过于复杂；c)
    可扩展性，完整的代码库，易于修改或扩展其他模块或方法；d) 高性能，在公平的实验设置下，能够在主流基准上实现最先进的或至少是可比的性能。基于以上观点，我们设计了一个新的基于
    Transformer 的人类解析基线。所提的新基线基于 Mask2Former [[172](#bib.bib172)] 架构，并做了一些适应人类解析的改进，称为用于解析的
    Mask2Former (M2FP)。M2FP 可以适应几乎所有人类解析任务并产生惊人的性能。¹¹1 代码和模型可以在 [https://github.com/soeaver/M2FP](https://github.com/soeaver/M2FP)
    找到。
- en: 6.1.1 Mask2Former for Parsing
  id: totrans-482
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 用于解析的 Mask2Former
- en: '$\bullet$ Modeling Human as Group Queries. To solve the three human parsing
    sub-tasks, we need to simultaneously model the parts relationship and distinguish
    human instances. DETR series work [[168](#bib.bib168), [173](#bib.bib173), [174](#bib.bib174),
    [172](#bib.bib172)] regard objects as queries, and transform object detection
    or instance segmentation task into a direct set prediction problem. A naive idea
    is to regard human parts as queries, then use mask classification to predict the
    category and mask of each part. However, this creates two problems that cannot
    be ignored. Firstly, only modeling parts will make it difficult to learn the global
    relationship between parts and humans; Secondly, the subordination between part
    and human instance is unknown, resulting in the inadaptability for MHP task. Thus,
    we introduce the body hierarchy into the queries and use the powerful sequence
    encoding ability of transformer to build multiple hierarchical relationships between
    parts and humans. Specifically, we explicitly divide the queries into three groups:
    background queries, part queries and human queries. Through the relationship modeling
    ability of self-attention mechanism, besides the basic part-part relationship,
    the part-human, human-human, and part/human-background relationships are also
    modeled. Thanks to the direct modeling of parts and the introduction of multiple
    hierarchical granularities, M2FP can be applied to all supervised human parsing
    tasks.'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$  将人类建模为组查询。为了解决三个人体解析子任务，我们需要同时建模部件关系和区分人体实例。DETR系列工作[[168](#bib.bib168),
    [173](#bib.bib173), [174](#bib.bib174), [172](#bib.bib172)]将物体视为查询，并将物体检测或实例分割任务转化为直接的集合预测问题。一个简单的想法是将人体部件视为查询，然后使用掩码分类来预测每个部件的类别和掩码。然而，这会产生两个不可忽视的问题。首先，仅建模部件会使得学习部件与人体之间的全局关系变得困难；其次，部件与人体实例之间的从属关系未知，导致不适应MHP任务。因此，我们将身体层次结构引入查询中，并利用变换器的强大序列编码能力建立部件与人体之间的多层次关系。具体来说，我们明确将查询分为三组：背景查询、部件查询和人体查询。通过自注意力机制的关系建模能力，除了基本的部件-部件关系，还建模了部件-人体、人体-人体以及部件/人体-背景关系。由于直接建模部件和引入多层次粒度，M2FP可以应用于所有有监督的人体解析任务。
- en: '$\bullet$ Architecture and Pipeline. The architecture of proposed M2FP is illustrated
    in Figure [6](#S6.F6 "Figure 6 ‣ 6 An Outlook: Future Opportunities of Human Parsing
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook"). We try to
    make the smallest modification to the Mask2Former. An encoder is used to extract
    image or video features, which is composed of a backbone and a pixel decoder [[173](#bib.bib173)].
    Then the features are flattened and sent into a transformer decoder. The transformer
    decoder consists of multiple repeated units, each containing a masked attention
    module, a self-attention module, and a shared feed-forward network (FFN) in turn.
    The grouped queries and flattened features conduct sufficient information exchange
    through the transformer decoder, and finally use bipartite matcher to match between
    queries and ground-truths uniquely. For SHP, in the inference stage, the background
    and part masks are combined with their class predictions to compute the final
    semantic segmentation prediction through matrix multiplication. For MHP, the intersection
    ratio of semantic segmentation prediction and human masks is calculated to obtain
    the final instance-level human parsing prediction. M2FP can also be extended to
    supervised VHP task. Follow [[175](#bib.bib175)], the background, parts, and humans
    in the video can be regarded as 3D spatial-temporal masks, and using the sequence
    encoding ability of transformer to make an end-to-end prediction.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$  架构与流程。所提出的M2FP的架构如图[6](#S6.F6 "Figure 6 ‣ 6 An Outlook: Future Opportunities
    of Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")所示。我们尽量对Mask2Former进行最小的修改。使用编码器提取图像或视频特征，该编码器由骨干网和像素解码器[[173](#bib.bib173)]组成。然后将特征展平并送入变换器解码器。变换器解码器由多个重复的单元组成，每个单元依次包含掩码注意力模块、自注意力模块和共享前馈网络（FFN）。分组查询和展平特征通过变换器解码器进行充分的信息交换，最后使用二分匹配器在查询和真实值之间进行唯一匹配。对于SHP，在推理阶段，背景和部件掩码与其类别预测结合，通过矩阵乘法计算最终的语义分割预测。对于MHP，计算语义分割预测和人体掩码的交集比率，以获得最终的实例级人体解析预测。M2FP还可以扩展到有监督的VHP任务。根据[[175](#bib.bib175)]，视频中的背景、部件和人体可以视为3D时空掩码，并利用变换器的序列编码能力进行端到端预测。'
- en: 6.1.2 Experiments
  id: totrans-485
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 实验
- en: $\bullet$ Experimental Setup. We validate M2FP on several mainstream benchmarks,
    including LIP, PASCAL-Person-Part, CIHP, and MHP-v2\. All models are trained with
    nearly identical hyper-parameters under 8 NVIDIA V100 GPUs. Specifically, we use
    AdamW [[176](#bib.bib176)] optimizer with a mini-batch size of 16, an initial
    learning rate of 0.0004 with poly (LIP) or step (PASCAL-Person-Part, CIHP, and
    MHP-v2) learning rate schedule, then train each model for 150 epochs. Large scale
    jittering in the range of [0.1, 2.0] and typical data augmentation techniques,
    *e.g*., fixed size random crop (512$\times$384 for LIP, 800$\times$800 for PASCAL-Person-Part,
    CIHP, and MHP-v2), random rotation from [-40°, +40°], random color jittering and
    horizontal flip, are also used. For fair comparison, horizontal flipping is adopted
    during testing, and multi-scale test is used for LIP. The default backbone is
    ResNet-101 with pre-training on ImageNet-1K [[177](#bib.bib177)].
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 实验设置。我们在多个主流基准上验证了 M2FP，包括 LIP、PASCAL-Person-Part、CIHP 和 MHP-v2。所有模型在
    8 个 NVIDIA V100 GPU 上以几乎相同的超参数进行训练。具体而言，我们使用 AdamW [[176](#bib.bib176)] 优化器，迷你批量大小为
    16，初始学习率为 0.0004，采用多项式（LIP）或步长（PASCAL-Person-Part、CIHP 和 MHP-v2）学习率调度，然后训练每个模型
    150 个周期。还使用了范围为 [0.1, 2.0] 的大规模抖动和典型的数据增强技术，例如固定大小随机裁剪（LIP 为 512$\times$384，PASCAL-Person-Part、CIHP
    和 MHP-v2 为 800$\times$800）、从 [-40°, +40°] 随机旋转、随机颜色抖动和水平翻转。为了公平比较，测试期间采用水平翻转，并且对
    LIP 使用多尺度测试。默认的骨干网络是 ResNet-101，预训练于 ImageNet-1K [[177](#bib.bib177)]。
- en: '![Refer to caption](img/957bb4a416ed395b6833cc7582e4f6a3.png)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/957bb4a416ed395b6833cc7582e4f6a3.png)'
- en: 'Figure 7: Comparison of M2FP with previous human parsing state-of-the-art models.
    M2FP achieves state-of-the-art (PPP, CIHP and MHP-v2) or comparable performance
    (LIP) on all human parsing sub-tasks.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：M2FP 与以往人体解析最先进模型的比较。M2FP 在所有人体解析子任务上都达到了最先进（PPP、CIHP 和 MHP-v2）或可比的性能（LIP）。
- en: '$\bullet$ Results. As shown in Table [XIV](#S6.T14 "Table XIV ‣ 6.1.2 Experiments
    ‣ 6.1 A Transformer-based Baseline for Human Parsing ‣ 6 An Outlook: Future Opportunities
    of Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")
    and Figure [7](#S6.F7 "Figure 7 ‣ 6.1.2 Experiments ‣ 6.1 A Transformer-based
    Baseline for Human Parsing ‣ 6 An Outlook: Future Opportunities of Human Parsing
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook"), M2FP achieves
    state-of-the-art or comparable performance across a broad range of human parsing
    benchmarks. For SHP, M2FP only falls behind HIPN [[125](#bib.bib125)] and CDGNet
    [[131](#bib.bib131)], obtaining 88.93% pixAcc. and 59.86% mIoU, showing great
    potential in the parts relationship modeling. For MHP, M2FP shows amazing performance,
    greatly surpassing the existing methods on all metrics and even exceeding the
    state-of-the-art two-stage top-down method, *i.e*., SCHP [[52](#bib.bib52)]. Specifically,
    M2FP outperforms PGN [[8](#bib.bib8)] with 4.14 point mIoU and MGHR [[59](#bib.bib59)]
    with 0.56 point AP${}^{\text{r}}_{\text{vol}}$ on PASCAL-Person-Part. On the more
    challenging CIHP and MHP-v2, M2FP beats SCHP in terms of mIoU while running in
    an end-to-end manner. Meanwhile, M2FP is also 7.96 points ahead of SCHP in AP${}^{\text{r}}_{\text{vol}}$
    (CIHP) and 5.97 points ahead of RP R-CNN [[140](#bib.bib140)] in AP${}^{\text{p}}_{\text{vol}}$
    (MHP-v2). These results demonstrate that M2FP surpasses almost all human parsing
    methods in a concise, effective and universal way, and can be regarded as a new
    baseline in the next era.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 结果。正如表格 [XIV](#S6.T14 "Table XIV ‣ 6.1.2 Experiments ‣ 6.1 A Transformer-based
    Baseline for Human Parsing ‣ 6 An Outlook: Future Opportunities of Human Parsing
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook") 和图 [7](#S6.F7
    "Figure 7 ‣ 6.1.2 Experiments ‣ 6.1 A Transformer-based Baseline for Human Parsing
    ‣ 6 An Outlook: Future Opportunities of Human Parsing ‣ Deep Learning Technique
    for Human Parsing: A Survey and Outlook") 所示，M2FP 在广泛的人体解析基准中达到了最先进或可比的性能。对于 SHP，M2FP
    仅次于 HIPN [[125](#bib.bib125)] 和 CDGNet [[131](#bib.bib131)]，取得了 88.93% 的像素准确率（pixAcc）和
    59.86% 的平均交并比（mIoU），在部件关系建模方面展现了巨大的潜力。对于 MHP，M2FP 展现了惊人的表现，在所有指标上大幅超越了现有方法，甚至超越了最先进的两阶段自上而下方法，即
    SCHP [[52](#bib.bib52)]。具体而言，M2FP 在 PASCAL-Person-Part 上的 mIoU 超过 PGN [[8](#bib.bib8)]
    4.14 点，并在 AP${}^{\text{r}}_{\text{vol}}$ 上超越 MGHR [[59](#bib.bib59)] 0.56 点。在更具挑战性的
    CIHP 和 MHP-v2 上，M2FP 在 mIoU 上超过了 SCHP，同时以端到端的方式运行。与此同时，M2FP 在 AP${}^{\text{r}}_{\text{vol}}$（CIHP）上领先
    SCHP 7.96 点，在 AP${}^{\text{p}}_{\text{vol}}$（MHP-v2）上领先 RP R-CNN [[140](#bib.bib140)]
    5.97 点。这些结果表明 M2FP 在简洁、有效和通用的方式上超越了几乎所有的人体解析方法，并且可以被视为下一个时代的新基准。'
- en: 'Table XIV: Overview of M2FP results on various human parsing benchmarks.     
    denotes the previous state-of-the-art results. Bold results denote M2FP achieve
    new state-of-the-art.'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 表 XIV：M2FP在各种人体解析基准测试中的结果概述。      表示之前的最先进结果。粗体结果表示M2FP达到了新的最先进水平。
- en: '|   | LIP | PPP | CIHP | MHP-v2 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '|   | LIP | PPP | CIHP | MHP-v2 |'
- en: '| Method | pixAcc. | mIoU | mIoU | AP${}^{\text{r}}_{\text{vol}}$ | mIoU |
    AP${}^{\text{r}}_{\text{vol}}$ | mIoU | AP${}^{\text{p}}_{\text{vol}}$ |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | pixAcc. | mIoU | mIoU | AP${}^{\text{r}}_{\text{vol}}$ | mIoU | AP${}^{\text{r}}_{\text{vol}}$
    | mIoU | AP${}^{\text{p}}_{\text{vol}}$ |'
- en: '| HIPN [[125](#bib.bib125)] | 89.14 | 59.61 | - | - | - | - | - | - |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| HIPN [[125](#bib.bib125)] | 89.14 | 59.61 | - | - | - | - | - | - |'
- en: '| HSSN [[5](#bib.bib5)] | - | 60.37 | - | - | - | - | - | - |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| HSSN [[5](#bib.bib5)] | - | 60.37 | - | - | - | - | - | - |'
- en: '| PGN [[8](#bib.bib8)] | - | - | 68.40 | 39.20 | 55.80 | 33.60 | - | - |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| PGN [[8](#bib.bib8)] | - | - | 68.40 | 39.20 | 55.80 | 33.60 | - | - |'
- en: '| MGHR [[59](#bib.bib59)] | - | - | - | 55.90 | - | - | 41.40 | 44.30 |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| MGHR [[59](#bib.bib59)] | - | - | - | 55.90 | - | - | 41.40 | 44.30 |'
- en: '| SCHP [[52](#bib.bib52)] | - | - | - | - | 67.47 | 52.74 | 45.21 | 45.25 |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| SCHP [[52](#bib.bib52)] | - | - | - | - | 67.47 | 52.74 | 45.21 | 45.25 |'
- en: '| RP R-CNN [[140](#bib.bib140)] | - | - | 63.30 | 40.90 | 60.20 | 42.30 | 38.60
    | 46.80 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| RP R-CNN [[140](#bib.bib140)] | - | - | 63.30 | 40.90 | 60.20 | 42.30 | 38.60
    | 46.80 |'
- en: '| M2FP (ours) | 88.93 | 59.86 | 72.54 | 56.46 | 69.15 | 60.47 | 47.64 | 53.36
    |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| M2FP（我们的方法） | 88.93 | 59.86 | 72.54 | 56.46 | 69.15 | 60.47 | 47.64 | 53.36
    |'
- en: 6.2 Under-Investigated Open Issues
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 未充分研究的开放问题
- en: Based on the reviewed research, we list several under-investigated open issues
    that we believe should be pursued.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 基于回顾的研究，我们列出了一些未充分研究的开放问题，我们认为这些问题应该继续深入探讨。
- en: $\bullet$ Efficient Inference. In practical applications, human parsing models
    generally need real-time or even faster inference speed. The current research
    has not paid enough attention to this issue, especially the multiple human parsing
    research. Although some literature [[59](#bib.bib59), [142](#bib.bib142)] has
    discussed the model efficiency, it can not achieve real-time inference, and there
    is no human parser designed for this purpose. Therefore, from the perspective
    of practical application, it is an under-investigated open issue to design an
    efficient inference human parsing model.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 高效推理。在实际应用中，人体彩绘模型通常需要实时甚至更快的推理速度。目前的研究尚未对此问题给予足够关注，尤其是多人人体彩绘研究。尽管一些文献
    [[59](#bib.bib59), [142](#bib.bib142)] 讨论了模型效率，但无法实现实时推理，也没有针对这一目的设计的人体解析器。因此，从实际应用的角度来看，设计一个高效推理的人体彩绘模型是一个未充分研究的开放问题。
- en: $\bullet$ Synthetic Dataset. It is a common practice in many fields to use synthetic
    datasets to train models and transfer them to real scenes. Through CG technology
    (*e.g*., NVIDIA Omniverse²²2[https://developer.nvidia.com/nvidia-omniverse](https://developer.nvidia.com/nvidia-omniverse)),
    we can obtain almost unlimited synthetic human data at a very low cost, as well
    as parsing annotations. Considering the labeling cost of human parsing dataset,
    this is a very attractive scheme. Wood *et al*. have made a preliminary attempt
    on the face parsing task and achieved very excellent performance [[178](#bib.bib178)],
    but at present, there is a lack of research on the human parsing field.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 合成数据集。在许多领域，使用合成数据集来训练模型并将其转移到实际场景中是一种常见做法。通过CG技术（*例如*，NVIDIA Omniverse²²2[https://developer.nvidia.com/nvidia-omniverse](https://developer.nvidia.com/nvidia-omniverse)），我们可以以非常低的成本获得几乎无限的合成人体数据以及解析注释。考虑到人体解析数据集的标注成本，这是一个非常有吸引力的方案。Wood
    *et al*. 在面部解析任务上进行了初步尝试，并取得了非常优秀的性能 [[178](#bib.bib178)]，但目前在人体彩绘领域的研究仍然不足。
- en: $\bullet$ Long-tailed Phenomenon. The long-tailed distribution is the most common
    phenomenon in the real world, and also exists in the human parsing field. For
    example, the Gini coefficient of MHP-v2.0 is as high as 0.747 [[179](#bib.bib179)],
    exceeding some artificially created long-tailed datasets, but this problem is
    currently ignored. Therefore, the existing methods are often brittle once exposed
    to the real world, where they are unable to adapt and robustly deal with tail
    categories effectively. This calls for a more general human parsing model, with
    the ability to adapt to long-tailed distributions in the real world.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 长尾现象。长尾分布是现实世界中最常见的现象，同时也存在于人体解析领域。例如，MHP-v2.0的基尼系数高达0.747 [[179](#bib.bib179)]，超出了某些人工创建的长尾数据集，但这一问题目前被忽视。因此，现有的方法在面对现实世界时往往脆弱，无法有效地适应和处理尾部类别。这呼吁需要一个更通用的人体解析模型，能够适应现实世界中的长尾分布。
- en: 6.3 New Directions
  id: totrans-505
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 新方向
- en: Considering some potential applications, we shed light on several possible research
    directions.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到一些潜在应用，我们揭示了几个可能的研究方向。
- en: $\bullet$ Video Instance-level Human Parsing. The current VHP research basically
    follows an unsupervised semi-automatic video object segmentation setting, which
    reduces the labeling cost in a way that greatly loses accuracy. However, most
    of the practical requirements of video human parsing require extremely high precision.
    Therefore, making full use of annotations and solving the VHP issue through an
    instance-discriminative manner, *i.e*., a fine-grained video instance segmentation
    task, has great research prospects.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 视频实例级人类解析。目前的 VHP 研究基本上遵循无监督半自动视频对象分割设置，这种方法大大降低了标注成本，但也带来了准确性的显著损失。然而，大多数视频人类解析的实际需求要求极高的精度。因此，充分利用标注，通过实例区分的方法解决
    VHP 问题，*即*，细粒度视频实例分割任务，具有广阔的研究前景。
- en: '$\bullet$ Whole-body Human Parsing. Besides human parsing, face parsing and
    hand parsing [[72](#bib.bib72), [73](#bib.bib73)] are also important issues. To
    fully understand the pixel-wise temporal-spatial attributes of human in the wild,
    it is necessary to parse body, face, and hands simultaneously, which implies a
    new direction to end-to-end parse the whole body: Whole-body Human Parsing. Natural
    hierarchical annotation and large-scale variation bring new challenges to existing
    parsing techniques. Thus the targeted datasets and whole-body parsers are necessary.'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 全身人类解析。除了人类解析，面部解析和手部解析 [[72](#bib.bib72), [73](#bib.bib73)] 也是重要问题。为了全面理解自然环境中人类的像素级时空属性，有必要同时解析身体、面部和手部，这意味着一个新的方向是端到端地解析全身：全身人类解析。自然层次标注和大规模变化给现有解析技术带来了新的挑战。因此，针对性的数据库和全身解析器是必要的。
- en: $\bullet$ Cooperation across Different Human-centric Directions. Some human-centric
    visual tasks (*e.g*., human attribute recognition [[180](#bib.bib180)], pose estimation
    [[181](#bib.bib181)], human mesh reconstruction [[70](#bib.bib70)]) face similar
    challenges to human parsing. Different tasks can play a positive role in promoting
    each other, although developments of these fields are independent. Moreover, the
    settings of different human-centric visual tasks are related, while there are
    no precedents for modeling these tasks in a unified framework. Thus, we call for
    closer collaboration across different human-centric visual tasks.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 不同人本视觉方向的合作。一些人本视觉任务（*例如*，人类属性识别 [[180](#bib.bib180)]，姿态估计 [[181](#bib.bib181)]，人类网格重建
    [[70](#bib.bib70)]）面临与人类解析类似的挑战。虽然这些领域的发展是独立的，不同任务之间仍可以积极促进彼此的发展。此外，不同人本视觉任务的设置是相关的，但目前尚无统一框架对这些任务进行建模。因此，我们呼吁在不同人本视觉任务之间加强合作。
- en: 7 Conclusions
  id: totrans-510
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: 'As far as we know, this is the first survey to comprehensively review deep
    learning techniques in human parsing, covering three sub-tasks: SHP, MHP, and
    VHP. We first provided the readers with the necessary knowledge, including task
    settings, background concepts, relevant problems, and applications. Afterward,
    we summarized the mainstream deep learning methods based on human parsing taxonomy,
    and analyzing them according to the theoretical background, technical contributions,
    and solving strategies. We also reviewed 14 popular human parsing datasets, benchmarking
    results on the 6 most widely-used ones. To promote sustainable community development,
    we discussed the under-investigated open issues and provided insight into new
    directions. We also put forward a new transformer-based human parsing framework,
    servicing a high-performance baseline for follow-up research through universal,
    concise, and extensible solutions. In summary, we hope this survey to provide
    an effective way to understand the current state-of-the-art human parsing models
    and promote the sustainable development of this research field.'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，这是第一项全面回顾人类解析中的深度学习技术的调查，涵盖了三个子任务：SHP、MHP 和 VHP。我们首先向读者提供了必要的知识，包括任务设置、背景概念、相关问题和应用。之后，我们根据人类解析分类法总结了主流的深度学习方法，并根据理论背景、技术贡献和解决策略进行了分析。我们还回顾了14个流行的人类解析数据集，并对6个最广泛使用的数据集进行了基准测试。为了促进可持续的社区发展，我们讨论了尚未充分研究的开放问题，并提供了新的方向洞见。我们还提出了一种基于变换器的人类解析新框架，通过通用、简洁和可扩展的解决方案，为后续研究提供高性能基准。总之，我们希望这项调查能够有效地理解当前最先进的人类解析模型，并促进该研究领域的可持续发展。
- en: References
  id: totrans-512
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] K. Yamaguchi, M. H. Kiapour, L. E. Ortiz, and T. L. Berg, “Parsing clothing
    in fashion photographs,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2012, pp. 3570–3577.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] K. Yamaguchi, M. H. Kiapour, L. E. Ortiz, 和 T. L. Berg，“时尚照片中的服装解析”，发表于*IEEE计算机视觉与模式识别会议论文集*，2012年，页码3570–3577。'
- en: '[2] X. Liang, K. Gong, X. Shen, and L. Lin, “Look into person: Joint body parsing
    pose estimation network and a new benchmark,” *IEEE Transactions on Pattern Analysis
    and Machine Intelligence*, vol. 41, no. 4, pp. 871–885, 2018.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] X. Liang, K. Gong, X. Shen, 和 L. Lin，“观察人物：联合人体解析姿态估计网络和新基准”，*IEEE模式分析与机器智能汇刊*，第41卷，第4期，页码871–885，2018年。'
- en: '[3] W. Wang, Z. Zhang, S. Qi, J. Shen, Y. Pang, and L. Shao, “Learning compositional
    neural information fusion for human parsing,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2019, pp. 5703–5713.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] W. Wang, Z. Zhang, S. Qi, J. Shen, Y. Pang, 和 L. Shao，“用于人体解析的组合神经信息融合学习”，发表于*IEEE/CVF国际计算机视觉会议论文集*，2019年，页码5703–5713。'
- en: '[4] W. Wang, H. Zhu, J. Dai, Y. Pang, J. Shen, and L. Shao, “Hierarchical human
    parsing with typed part-relation reasoning,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 8929–8939.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] W. Wang, H. Zhu, J. Dai, Y. Pang, J. Shen, 和 L. Shao，“基于类型化部分关系推理的层次化人体解析”，发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，页码8929–8939。'
- en: '[5] L. Li, T. Zhou, W. Wang, J. Li, and Y. Yang, “Deep hierarchical semantic
    segmentation,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2022, pp. 1246–1257.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] L. Li, T. Zhou, W. Wang, J. Li, 和 Y. Yang，“深度层次语义分割”，发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，页码1246–1257。'
- en: '[6] L. Lin, D. Zhang, and W. Zuo, *Human centric visual analysis with deep
    learning*.   Singapore: Springer, 2020.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] L. Lin, D. Zhang, 和 W. Zuo，*以人为本的深度学习视觉分析*。新加坡：Springer，2020年。'
- en: '[7] “Learning from limited or imperfect data (l2id) workshop,” [https://l2id.github.io/challenge_localization.html](https://l2id.github.io/challenge_localization.html),
    2021.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] “有限或不完善数据学习（l2id）研讨会”，[https://l2id.github.io/challenge_localization.html](https://l2id.github.io/challenge_localization.html)，2021年。'
- en: '[8] K. Gong, X. Liang, Y. Li, Y. Chen, M. Yang, and L. Lin, “Instance-level
    human parsing via part grouping network,” in *Proceedings of the European Conference
    on Computer Vision*, 2018, pp. 770–785.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] K. Gong, X. Liang, Y. Li, Y. Chen, M. Yang, 和 L. Lin，“通过部分分组网络进行实例级人体解析”，发表于*欧洲计算机视觉会议论文集*，2018年，页码770–785。'
- en: '[9] Q. Zhou, X. Liang, K. Gong, and L. Lin, “Adaptive temporal encoding network
    for video instance-level human parsing,” in *Proceedings of the 26th ACM International
    Conference on Multimedia*, 2018, pp. 1527–1535.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Q. Zhou, X. Liang, K. Gong, 和 L. Lin，“用于视频实例级人体解析的自适应时间编码网络”，发表于*第26届ACM国际多媒体会议论文集*，2018年，页码1527–1535。'
- en: '[10] A. Borras, F. Tous, J. Llados, and M. Vanrell, “High-level clothes description
    based on colour-texture and structural features,” in *Iberian Conference on Pattern
    Recognition and Image Analysis*, 2003, pp. 108–116.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. Borras, F. Tous, J. Llados, 和 M. Vanrell，“基于颜色-纹理和结构特征的高级服装描述”，发表于*伊比利亚模式识别与图像分析会议*，2003年，页码108–116。'
- en: '[11] H. Chen, Z. Xu, Z. Liu, and S.-C. Zhu, “Composite templates for cloth
    modeling and sketching,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2006, pp. 943–950.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] H. Chen, Z. Xu, Z. Liu, 和 S.-C. Zhu，“用于服装建模和素描的复合模板”，发表于*IEEE计算机视觉与模式识别会议论文集*，2006年，页码943–950。'
- en: '[12] P. Guan, O. Freifeld, and M. J. Black, “A 2d human body model dressed
    in eigen clothing,” in *Proceedings of the European Conference on Computer Vision*,
    2010, pp. 285–298.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] P. Guan, O. Freifeld, 和 M. J. Black，“穿着特征服装的二维人体模型”，发表于*欧洲计算机视觉会议论文集*，2010年，页码285–298。'
- en: '[13] Y. Yang and D. Ramanan, “Articulated pose estimation with flexible mixtures-of-parts,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2011, pp. 1385–1392.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. Yang 和 D. Ramanan，“具有灵活部件混合的关节姿态估计”，发表于*IEEE计算机视觉与模式识别会议论文集*，2011年，页码1385–1392。'
- en: '[14] J. Dong, Q. Chen, W. Xia, Z. Huang, and S. Yan, “A deformable mixture
    parsing model with parselets,” in *Proceedings of the IEEE International Conference
    on Computer Vision*, 2013, pp. 3408–3415.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] J. Dong, Q. Chen, W. Xia, Z. Huang, 和 S. Yan，“带有解析片段的可变形混合解析模型”，发表于*IEEE国际计算机视觉会议论文集*，2013年，页码3408–3415。'
- en: '[15] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep clustering for
    unsupervised learning of visual features,” in *Proceedings of the European Conference
    on Computer Vision*, 2018, pp. 139–156.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] M. Caron、P. Bojanowski、A. Joulin 和 M. Douze，“用于无监督视觉特征学习的深度聚类”，发表于 *欧洲计算机视觉会议论文集*，2018年，页码139–156。'
- en: '[16] L. Zhu, Y. Chen, Y. Lu, C. Lin, and A. Yuille, “Max margin and/or graph
    learning for parsing the human body,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2008, pp. 1–8.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] L. Zhu、Y. Chen、Y. Lu、C. Lin 和 A. Yuille，“用于解析人体的最大边际和/或图学习”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2008年，页码1–8。'
- en: '[17] J. Dong, Q. Chen, X. Shen, J. Yang, and S. Yan, “Towards unified human
    parsing and pose estimation,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2014, pp. 843–850.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] J. Dong、Q. Chen、X. Shen、J. Yang 和 S. Yan，“朝着统一的人体解析和姿态估计”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2014年，页码843–850。'
- en: '[18] A. Kae, K. Sohn, H. Lee, and E. Learned-Miller, “Augmenting crfs with
    boltzmann machine shape priors for image labeling,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2013, pp. 2019–2026.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. Kae、K. Sohn、H. Lee 和 E. Learned-Miller，“通过布尔兹曼机形状先验增强CRF进行图像标注”，发表于
    *IEEE计算机视觉与模式识别会议论文集*，2013年，页码2019–2026。'
- en: '[19] L. Ladicky, P. H. Torr, and A. Zisserman, “Human pose estimation using
    a joint pixel-wise and part-wise formulation,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2013, pp. 3578–3585.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] L. Ladicky、P. H. Torr 和 A. Zisserman，“使用联合像素级和部件级公式进行人体姿态估计”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2013年，页码3578–3585。'
- en: '[20] K. Yamaguchi, M. Hadi Kiapour, and T. L. Berg, “Paper doll parsing: Retrieving
    similar styles to parse clothing items,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2013, pp. 3519–3526.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] K. Yamaguchi、M. Hadi Kiapour 和 T. L. Berg，“纸娃娃解析：检索类似风格以解析服装项目”，发表于 *IEEE国际计算机视觉会议论文集*，2013年，页码3519–3526。'
- en: '[21] Y. Bo and C. C. Fowlkes, “Shape-based pedestrian parsing,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2011, pp.
    2265–2272.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. Bo 和 C. C. Fowlkes，“基于形状的行人解析”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2011年，页码2265–2272。'
- en: '[22] X. Liang, S. Liu, X. Shen, J. Yang, L. Liu, J. Dong, L. Lin, and S. Yan,
    “Deep human parsing with active template regression,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, vol. 37, no. 12, pp. 2402–2414, 2015.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] X. Liang、S. Liu、X. Shen、J. Yang、L. Liu、J. Dong、L. Lin 和 S. Yan，“基于主动模板回归的深度人体解析”，*IEEE模式分析与机器智能学报*，第37卷，第12期，页码2402–2414，2015年。'
- en: '[23] B. Fulkerson, A. Vedaldi, and S. Soatto, “Class segmentation and object
    localization with superpixel neighborhoods,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2009, pp. 670–677.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] B. Fulkerson、A. Vedaldi 和 S. Soatto，“利用超像素邻域的类别分割和物体定位”，发表于 *IEEE国际计算机视觉会议论文集*，2009年，页码670–677。'
- en: '[24] J. Tighe and S. Lazebnik, “Superparsing: scalable nonparametric image
    parsing with superpixels,” in *Proceedings of the European Conference on Computer
    Vision*, 2010, pp. 352–365.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Tighe 和 S. Lazebnik，“超级解析：具有超像素的可扩展非参数图像解析”，发表于 *欧洲计算机视觉会议论文集*，2010年，页码352–365。'
- en: '[25] S. Liu, J. Feng, C. Domokos, H. Xu, J. Huang, Z. Hu, and S. Yan, “Fashion
    parsing with weak color-category labels,” *IEEE Transactions on Multimedia*, vol. 16,
    no. 1, pp. 253–265, 2013.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. Liu、J. Feng、C. Domokos、H. Xu、J. Huang、Z. Hu 和 S. Yan，“带有弱颜色类别标签的时尚解析”，*IEEE多媒体学报*，第16卷，第1期，页码253–265，2013年。'
- en: '[26] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classification with
    deep convolutional neural networks,” in *Advances in Neural Information Processing
    Systems*, 2012.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Krizhevsky、I. Sutskever 和 G. Hinton，“使用深度卷积神经网络进行Imagenet分类”，发表于 *神经信息处理系统进展*，2012年。'
- en: '[27] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
    for accurate object detection and semantic segmentation,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*, 2014, pp. 580–587.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] R. Girshick、J. Donahue、T. Darrell 和 J. Malik，“用于准确目标检测和语义分割的丰富特征层次结构”，发表于
    *IEEE计算机视觉与模式识别会议论文集*，2014年，页码580–587。'
- en: '[28] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama,
    and T. Darrell, “Caffe: Convolutional architecture for fast feature embedding,”
    in *Proceedings of the 22nd ACM international conference on Multimedia*, 2014,
    pp. 675–678.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Jia、E. Shelhamer、J. Donahue、S. Karayev、J. Long、R. Girshick、S. Guadarrama
    和 T. Darrell，“Caffe：用于快速特征嵌入的卷积架构”，发表于 *第22届ACM国际多媒体会议论文集*，2014年，页码675–678。'
- en: '[29] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Y. LeCun, Y. Bengio, 和 G. Hinton，“深度学习，”*自然*，第521卷，第7553期，第436–444页，2015年。'
- en: '[30] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2015, pp.
    1–9.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, 和 A. Rabinovich，“通过卷积深入研究，”发表于*IEEE计算机视觉与模式识别会议论文集*，2015年，第1–9页。'
- en: '[31] E. Shelhamer, J. Long, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 39, no. 4, pp. 640–651, 2016.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] E. Shelhamer, J. Long, 和 T. Darrell，“用于语义分割的全卷积网络，”*IEEE模式分析与机器智能汇刊*，第39卷，第4期，第640–651页，2016年。'
- en: '[32] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2016, pp. 770–778.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] K. He, X. Zhang, S. Ren, 和 J. Sun，“用于图像识别的深度残差学习，”发表于*IEEE计算机视觉与模式识别会议论文集*，2016年，第770–778页。'
- en: '[33] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille, “Attention to scale:
    Scale-aware semantic image segmentation,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016, pp. 3640–3649.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] L.-C. Chen, Y. Yang, J. Wang, W. Xu, 和 A. L. Yuille，“关注尺度：尺度感知的语义图像分割，”发表于*IEEE计算机视觉与模式识别会议论文集*，2016年，第3640–3649页。'
- en: '[34] X. Liang, X. Shen, D. Xiang, J. Feng, L. Lin, and S. Yan, “Semantic object
    parsing with local-global long short-term memory,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2016, pp. 3185–3193.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] X. Liang, X. Shen, D. Xiang, J. Feng, L. Lin, 和 S. Yan，“具有局部-全局长短期记忆的语义对象解析，”发表于*IEEE计算机视觉与模式识别会议论文集*，2016年，第3185–3193页。'
- en: '[35] L. Yang, Q. Song, Y. Wu, and M. Hu, “Attention inspiring receptive-fields
    network for learning invariant representations,” *IEEE Transactions on Neural
    Networks and Learning Systems*, vol. 30, no. 6, pp. 1744–1755, 2018.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] L. Yang, Q. Song, Y. Wu, 和 M. Hu，“激发接收场网络以学习不变表示，”*IEEE神经网络与学习系统汇刊*，第30卷，第6期，第1744–1755页，2018年。'
- en: '[36] B. Cheng, L.-C. Chen, Y. Wei, Y. Zhu, Z. Huang, J. Xiong, T. S. Huang,
    W.-M. Hwu, and H. Shi, “Spgnet: Semantic prediction guidance for scene parsing,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2019, pp. 5218–5228.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] B. Cheng, L.-C. Chen, Y. Wei, Y. Zhu, Z. Huang, J. Xiong, T. S. Huang,
    W.-M. Hwu, 和 H. Shi，“Spgnet：场景解析的语义预测指导，”发表于*IEEE/CVF国际计算机视觉会议论文集*，2019年，第5218–5228页。'
- en: '[37] X. Liang, C. Xu, X. Shen, J. Yang, S. Liu, J. Tang, L. Lin, and S. Yan,
    “Human parsing with contextualized convolutional neural network,” in *Proceedings
    of the IEEE International Conference on Computer Vision*, 2015, pp. 1386–1394.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] X. Liang, C. Xu, X. Shen, J. Yang, S. Liu, J. Tang, L. Lin, 和 S. Yan，“基于上下文化卷积神经网络的人类解析，”发表于*IEEE国际计算机视觉会议论文集*，2015年，第1386–1394页。'
- en: '[38] F. Xia, P. Wang, L.-C. Chen, and A. L. Yuille, “Zoom better to see clearer:
    Human and object parsing with hierarchical auto-zoom net,” in *Proceedings of
    the European Conference on Computer Vision*, 2016, pp. 648–663.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] F. Xia, P. Wang, L.-C. Chen, 和 A. L. Yuille，“放大以更清晰地看到：具有层次化自动缩放网络的人类和对象解析，”发表于*欧洲计算机视觉会议论文集*，2016年，第648–663页。'
- en: '[39] X. Zhang, Y. Chen, B. Zhu, J. Wang, and M. Tang, “Part-aware context network
    for human parsing,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2020, pp. 8971–8980.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] X. Zhang, Y. Chen, B. Zhu, J. Wang, 和 M. Tang，“面部意识上下文网络用于人类解析，”发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第8971–8980页。'
- en: '[40] L. Yang, Q. Song, Z. Wang, Z. Liu, S. Xu, and Z. Li, “Quality-aware network
    for human parsing,” *arXiv preprint arXiv:2103.05997*, 2021.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] L. Yang, Q. Song, Z. Wang, Z. Liu, S. Xu, 和 Z. Li，“质量感知网络用于人类解析，”*arXiv预印本arXiv:2103.05997*，2021年。'
- en: '[41] R. Ji, D. Du, L. Zhang, L. Wen, Y. Wu, C. Zhao, F. Huang, and S. Lyu,
    “Learning semantic neural tree for human parsing,” in *Proceedings of the European
    Conference on Computer Vision*, 2020, pp. 205–221.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] R. Ji, D. Du, L. Zhang, L. Wen, Y. Wu, C. Zhao, F. Huang, 和 S. Lyu，“学习语义神经树用于人类解析，”发表于*欧洲计算机视觉会议论文集*，2020年，第205–221页。'
- en: '[42] K. Gong, Y. Gao, X. Liang, X. Shen, M. Wang, and L. Lin, “Graphonomy:
    Universal human parsing via graph transfer learning,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2019, pp. 7450–7459.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] K. Gong, Y. Gao, X. Liang, X. Shen, M. Wang, 和 L. Lin，“Graphonomy: 通过图转移学习进行通用人类解析，”发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，第7450–7459页。'
- en: '[43] X. Zhang, Y. Chen, M. Tang, J. Wang, X. Zhu, and Z. Lei, “Human parsing
    with part-aware relation modeling,” *IEEE Transactions on Multimedia*, 2022.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] X. Zhang, Y. Chen, M. Tang, J. Wang, X. Zhu, 和 Z. Lei, “具有部位感知关系建模的人体解析,”
    *IEEE多媒体汇刊*, 2022年。'
- en: '[44] T. Ruan, T. Liu, Z. Huang, Y. Wei, S. Wei, and Y. Zhao, “Devil in the
    details: Towards accurate single and multiple human parsing,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, 2019, pp. 4814–4821.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] T. Ruan, T. Liu, Z. Huang, Y. Wei, S. Wei, 和 Y. Zhao, “细节中的恶魔：朝向准确的单人和多人解析,”
    见于 *AAAI人工智能会议论文集*, 2019年，第4814–4821页。'
- en: '[45] Z. Zhang, C. Su, L. Zheng, and X. Xie, “Correlating edge, pose with parsing,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 8900–8909.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Z. Zhang, C. Su, L. Zheng, 和 X. Xie, “边缘、姿态与解析的关联,” 见于 *IEEE/CVF计算机视觉与模式识别会议论文集*,
    2020年，第8900–8909页。'
- en: '[46] Y. Liu, L. Zhao, S. Zhang, and J. Yang, “Hybrid resolution network using
    edge guided region mutual information loss for human parsing,” in *Proceedings
    of the 28th ACM International Conference on Multimedia*, 2020, pp. 1670–1678.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Y. Liu, L. Zhao, S. Zhang, 和 J. Yang, “使用边缘引导区域互信息损失的混合分辨率网络进行人体解析,” 见于
    *第28届ACM国际多媒体会议论文集*, 2020年，第1670–1678页。'
- en: '[47] X. Nie, J. Feng, and S. Yan, “Mutual learning to adapt for joint human
    parsing and pose estimation,” in *Proceedings of the European Conference on Computer
    Vision*, 2018, pp. 502–517.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] X. Nie, J. Feng, 和 S. Yan, “互学习适应于联合人体解析和姿态估计,” 见于 *欧洲计算机视觉会议论文集*, 2018年，第502–517页。'
- en: '[48] Y. Zhao, J. Li, Y. Zhang, and Y. Tian, “From pose to part: Weakly-supervised
    pose evolution for human part segmentation,” *IEEE Transactions on Pattern Analysis
    and Machine Intelligence*, 2022.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Y. Zhao, J. Li, Y. Zhang, 和 Y. Tian, “从姿态到部分：弱监督姿态演化用于人体部位分割,” *IEEE模式分析与机器智能汇刊*,
    2022年。'
- en: '[49] S. Liu, Y. Sun, D. Zhu, G. Ren, Y. Chen, J. Feng, and J. Han, “Cross-domain
    human parsing via adversarial feature and label adaptation,” in *Proceedings of
    the AAAI Conference On Artificial Intelligence*, 2018, pp. 7146–7153.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] S. Liu, Y. Sun, D. Zhu, G. Ren, Y. Chen, J. Feng, 和 J. Han, “通过对抗特征和标签适应进行跨领域人体解析,”
    见于 *AAAI人工智能会议论文集*, 2018年，第7146–7153页。'
- en: '[50] Y. Luo, Z. Zheng, L. Zheng, T. Guan, J. Yu, and Y. Yang, “Macro-micro
    adversarial network for human parsing,” in *Proceedings of the European Conference
    on Computer Vision*, 2018, pp. 418–434.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Y. Luo, Z. Zheng, L. Zheng, T. Guan, J. Yu, 和 Y. Yang, “用于人体解析的宏观-微观对抗网络,”
    见于 *欧洲计算机视觉会议论文集*, 2018年，第418–434页。'
- en: '[51] T. Li, Z. Liang, S. Zhao, J. Gong, and J. Shen, “Self-learning with rectification
    strategy for human parsing,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020, pp. 9263–9272.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] T. Li, Z. Liang, S. Zhao, J. Gong, 和 J. Shen, “具有校正策略的自学习用于人体解析,” 见于 *IEEE/CVF计算机视觉与模式识别会议论文集*,
    2020年，第9263–9272页。'
- en: '[52] P. Li, Y. Xu, Y. Wei, and Y. Yang, “Self-correction for human parsing,”
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2020.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] P. Li, Y. Xu, Y. Wei, 和 Y. Yang, “人体解析的自我修正,” *IEEE模式分析与机器智能汇刊*, 2020年。'
- en: '[53] M. Mameli, M. Paolanti, R. Pietrini, G. Pazzaglia, E. Frontoni, and P. Zingaretti,
    “Deep learning approaches for fashion knowledge extraction from social media:
    a review,” *IEEE Access*, 2021.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] M. Mameli, M. Paolanti, R. Pietrini, G. Pazzaglia, E. Frontoni, 和 P. Zingaretti,
    “从社交媒体提取时尚知识的深度学习方法：综述,” *IEEE Access*, 2021年。'
- en: '[54] W. Cheng, S. Song, C.-Y. Chen, S. C. Hidayati, and J. Liu, “Fashion meets
    computer vision: A survey,” *ACM Computing Surveys*, vol. 54, no. 4, pp. 1–41,
    2021.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] W. Cheng, S. Song, C.-Y. Chen, S. C. Hidayati, 和 J. Liu, “时尚与计算机视觉的结合：综述,”
    *ACM计算机调查*, 第54卷，第4期，第1–41页, 2021年。'
- en: '[55] K. Khan, R. U. Khan, K. Ahmad, F. Ali, and K.-S. Kwak, “Face segmentation:
    A journey from classical to deep learning paradigm, approaches, trends, and directions,”
    *IEEE Access*, vol. 8, pp. 58 683–58 699, 2020.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] K. Khan, R. U. Khan, K. Ahmad, F. Ali, 和 K.-S. Kwak, “面部分割：从经典到深度学习范式的历程、方法、趋势和方向,”
    *IEEE Access*, 第8卷，第58 683–58 699页, 2020年。'
- en: '[56] S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and D. Terzopoulos,
    “Image segmentation using deep learning: A survey,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, 2021.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, 和 D. Terzopoulos,
    “使用深度学习的图像分割：综述,” *IEEE模式分析与机器智能汇刊*, 2021年。'
- en: '[57] X. Liu, M. Zhang, W. Liu, J. Song, and T. Mei, “Braidnet: Braiding semantics
    and details for accurate human parsing,” in *Proceedings of the 27th ACM International
    Conference on Multimedia*, 2019, pp. 338–346.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] X. Liu, M. Zhang, W. Liu, J. Song, 和 T. Mei, “Braidnet：编织语义和细节以实现准确的人体解析,”
    见于 *第27届ACM国际多媒体会议论文集*, 2019年，第338–346页。'
- en: '[58] L. Yang, Z. Liu, T. Zhou, and Q. Song, “Part decomposition and refinement
    network for human parsing,” *IEEE/CAA Journal of Automatica Sinica*, 2022.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] L. Yang, Z. Liu, T. Zhou, 和 Q. Song，“用于人体解析的部分分解与细化网络，” *IEEE/CAA 自动化学报*，2022年。'
- en: '[59] T. Zhou, W. Wang, S. Liu, Y. Yang, and L. V. Gool, “Differentiable multi-granularity
    human representation learning for instance-aware human semantic parsing,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 1622–1631.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] T. Zhou, W. Wang, S. Liu, Y. Yang, 和 L. V. Gool，“可微分的多粒度人体表示学习用于实例感知的人体语义解析，”
    见 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2021年，页1622–1631。'
- en: '[60] Z. Liu, X. Zhu, L. Yang, X. Yan, M. Tang, Z. Lei, G. Zhu, X. Feng, Y. Wang,
    and J. Wang, “Multi-initialization optimization network for accurate 3d human
    pose and shape estimation,” in *Proceedings of the 29th ACM International Conference
    on Multimedia*, 2021, pp. 1976–1984.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Z. Liu, X. Zhu, L. Yang, X. Yan, M. Tang, Z. Lei, G. Zhu, X. Feng, Y.
    Wang, 和 J. Wang，“用于准确3D人体姿态和形状估计的多初始化优化网络，” 见 *第29届ACM国际多媒体会议论文集*，2021年，页1976–1984。'
- en: '[61] L. Yang, Q. Song, Z. Wang, and M. Jiang, “Parsing r-cnn for instance-level
    human analysis,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2019, pp. 364–373.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] L. Yang, Q. Song, Z. Wang, 和 M. Jiang，“用于实例级人体分析的解析R-CNN，” 见 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2019年，页364–373。'
- en: '[62] D. de Geus, P. Meletis, C. Lu, X. Wen, and G. Dubbelman, “Part-aware panoptic
    segmentation,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2021, pp. 5485–5494.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] D. de Geus, P. Meletis, C. Lu, X. Wen, 和 G. Dubbelman，“部分感知全景分割，” 见 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2021年，页5485–5494。'
- en: '[63] W. Wang, T. Zhou, F. Porikli, D. Crandall, and L. V. Gool, “A survey on
    deep learning technique for video segmentation,” *arXiv preprint arXiv:2107.01153*,
    2021.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] W. Wang, T. Zhou, F. Porikli, D. Crandall, 和 L. V. Gool，“关于视频分割的深度学习技术综述，”
    *arXiv 预印本 arXiv:2107.01153*，2021年。'
- en: '[64] H.-S. Fang, G. Lu, X. Fang, J. Xie, Y.-W. Tai, and C. Lu, “Weakly and
    semi supervised human body part parsing via pose-guided knowledge transfer,” in
    *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2018, pp. 70–78.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] H.-S. Fang, G. Lu, X. Fang, J. Xie, Y.-W. Tai, 和 C. Lu，“通过姿态引导的知识迁移进行弱监督和半监督人体部件解析，”
    见 *IEEE 计算机视觉与模式识别会议论文集*，2018年，页70–78。'
- en: '[65] H. He, J. Zhang, B. Thuraisingham, and D. Tao, “Progressive one-shot human
    parsing,” in *Proceedings of the AAAI Conference on Artificial Intelligence*,
    2021, pp. 1522–1530.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] H. He, J. Zhang, B. Thuraisingham, 和 D. Tao，“渐进式一次性人体解析，” 见 *AAAI 人工智能会议论文集*，2021年，页1522–1530。'
- en: '[66] H. He, J. Zhang, B. Zhuang, J. Cai, and D. Tao, “End-to-end one-shot human
    parsing,” *arXiv preprint arXiv:2105.01241*, 2021.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] H. He, J. Zhang, B. Zhuang, J. Cai, 和 D. Tao，“端到端一次性人体解析，” *arXiv 预印本
    arXiv:2105.01241*，2021年。'
- en: '[67] Y. Gao, L. Liang, C. Lang, S. Feng, Y. Li, and Y. Wei, “Clicking matters:
    Towards interactive human parsing,” *IEEE Transactions on Multimedia*, 2022.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Y. Gao, L. Liang, C. Lang, S. Feng, Y. Li, 和 Y. Wei，“点击很重要: 走向交互式人体解析，”
    *IEEE 多媒体汇刊*，2022年。'
- en: '[68] Q. Chen, T. Ge, Y. Xu, Z. Zhang, X. Yang, and K. Gai, “Semantic human
    matting,” in *Proceedings of the 26th ACM International Conference on Multimedia*,
    2018, pp. 618–626.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Q. Chen, T. Ge, Y. Xu, Z. Zhang, X. Yang, 和 K. Gai，“语义人体抠图，” 见 *第26届ACM国际多媒体会议论文集*，2018年，页618–626。'
- en: '[69] J. Liu, Y. Yao, W. Hou, M. Cui, X. Xie, C. Zhang, and X.-S. Hua, “Boosting
    semantic human matting with coarse annotations,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2020, pp. 8563–8572.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] J. Liu, Y. Yao, W. Hou, M. Cui, X. Xie, C. Zhang, 和 X.-S. Hua，“利用粗略注释提升语义人体抠图，”
    见 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2020年，页8563–8572。'
- en: '[70] R. A. Guler and I. Kokkinos, “Holopose: Holistic 3d human reconstruction
    in-the-wild,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2019, pp. 10 884–10 894.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] R. A. Guler 和 I. Kokkinos，“Holopose: 野外环境下的整体3D人体重建，” 见 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2019年，页10 884–10 894。'
- en: '[71] Z. Zheng, T. Yu, Y. Wei, Q. Dai, and Y. Liu, “Deephuman: 3d human reconstruction
    from a single image,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2019, pp. 7739–7749.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Z. Zheng, T. Yu, Y. Wei, Q. Dai, 和 Y. Liu，“Deephuman: 从单张图像重建3D人体，” 见
    *IEEE/CVF 国际计算机视觉会议论文集*，2019年，页7739–7749。'
- en: '[72] H. Liang, J. Yuan, and D. Thalmann, “Parsing the hand in depth images,”
    *IEEE Transactions on Multimedia*, vol. 16, no. 5, pp. 1241–1253, 2014.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] H. Liang, J. Yuan, 和 D. Thalmann，“深度图像中的手部解析，” *IEEE 多媒体汇刊*，第16卷，第5期，页1241–1253，2014年。'
- en: '[73] J. Lin, H. Yang, D. Chen, M. Zeng, F. Wen, and L. Yuan, “Face parsing
    with roi tanh-warping,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 5654–5663.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] J. Lin, H. Yang, D. Chen, M. Zeng, F. Wen, 和 L. Yuan，“使用ROI tanh-warping进行面部解析，”在*IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，5654–5663页。'
- en: '[74] R. A. Guler, N. Neverova, and I. Kokkinos, “Densepose: Dense human pose
    estimation in the wild,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2018, pp. 7297–7306.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] R. A. Guler, N. Neverova, 和 I. Kokkinos，“Densepose：在实际环境中的密集人体姿态估计，”在*IEEE计算机视觉与模式识别会议论文集*，2018年，7297–7306页。'
- en: '[75] T. Zhu, P. Karlsson, and C. Bregler, “Simpose: Effectively learning densepose
    and surface normals of people from simulated data,” in *Proceedings of the European
    Conference on Computer Vision*, 2020, pp. 225–242.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] T. Zhu, P. Karlsson, 和 C. Bregler，“Simpose：从模拟数据中有效学习Densepose和表面法线，”在*欧洲计算机视觉会议论文集*，2020年，225–242页。'
- en: '[76] M. M. Kalayeh, E. Basaran, M. Gokmen, M. E. Kamasak, and M. Shah, “Human
    semantic parsing for person re-identification,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2018, pp. 1062–1071.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] M. M. Kalayeh, E. Basaran, M. Gokmen, M. E. Kamasak, 和 M. Shah，“用于人物重新识别的语义解析，”在*IEEE计算机视觉与模式识别会议论文集*，2018年，1062–1071页。'
- en: '[77] W. Yang, H. Huang, Z. Zhang, X. Chen, K. Huang, and S. Zhang, “Towards
    rich feature discovery with class activation maps augmentation for person re-identification,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2019, pp. 1389–1398.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] W. Yang, H. Huang, Z. Zhang, X. Chen, K. Huang, 和 S. Zhang，“通过类别激活图增强实现丰富特征发现以进行人物重新识别，”在*IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，1389–1398页。'
- en: '[78] Y. Sun, L. Zheng, Y. Li, Y. Yang, Q. Tian, and S. Wang, “Learning part-based
    convolutional features for person re-identification,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, vol. 43, no. 3, pp. 902–917, 2019.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Y. Sun, L. Zheng, Y. Li, Y. Yang, Q. Tian, 和 S. Wang，“学习基于部分的卷积特征以进行人物重新识别，”*IEEE模式分析与机器智能汇刊*，第43卷，第3期，902–917页，2019年。'
- en: '[79] H. Huang, W. Yang, J. Lin, G. Huang, J. Xu, G. Wang, X. Chen, and K. Huang,
    “Improve person re-identification with part awareness learning,” *2*, vol. 29,
    pp. 7468–7481, 2020.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] H. Huang, W. Yang, J. Lin, G. Huang, J. Xu, G. Wang, X. Chen, 和 K. Huang，“通过部件感知学习改进人物重新识别，”*2*，第29卷，7468–7481页，2020年。'
- en: '[80] Z. Li, J. Lv, Y. Chen, and J. Yuan, “Person re-identification with part
    prediction alignment,” *Computer Vision and Image Understanding*, vol. 205, 2021.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Z. Li, J. Lv, Y. Chen, 和 J. Yuan，“带有部件预测对齐的人物重新识别，”*计算机视觉与图像理解*，第205卷，2021年。'
- en: '[81] M. Tian, S. Yi, H. Li, S. Li, X. Zhang, J. Shi, J. Yan, and X. Wang, “Eliminating
    background-bias for robust person re-identification,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2018, pp. 5794–5803.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M. Tian, S. Yi, H. Li, S. Li, X. Zhang, J. Shi, J. Yan, 和 X. Wang，“消除背景偏差以实现稳健的人物重新识别，”在*IEEE计算机视觉与模式识别会议论文集*，2018年，5794–5803页。'
- en: '[82] Y. Chen, X. Zhu, and S. Gong, “Instance-guided context rendering for cross-domain
    person re-identification,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2019, pp. 232–242.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Y. Chen, X. Zhu, 和 S. Gong，“基于实例的上下文渲染用于跨域人物重新识别，”在*IEEE/CVF国际计算机视觉大会论文集*，2019年，232–242页。'
- en: '[83] S. Yu, S. Li, D. Chen, R. Zhao, J. Yan, and Y. Qiao, “Cocas: A large-scale
    clothes changing person dataset for re-identification,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020, pp. 3400–3409.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] S. Yu, S. Li, D. Chen, R. Zhao, J. Yan, 和 Y. Qiao，“Cocas：用于重新识别的大规模换衣数据集，”在*IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，3400–3409页。'
- en: '[84] X. Qian, W. Wang, L. Zhang, F. Zhu, Y. Fu, X. Tao, Y.-G. Jiang, and X. Xue,
    “Long-term cloth-changing person re-identification,” in *Proceedings of the Asian
    Conference on Computer Vision*, 2020, pp. 71–88.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] X. Qian, W. Wang, L. Zhang, F. Zhu, Y. Fu, X. Tao, Y.-G. Jiang, 和 X. Xue，“长期换衣人员重新识别，”在*亚洲计算机视觉大会论文集*，2020年，71–88页。'
- en: '[85] X. Han, Z. Wu, Z. Wu, R. Yu, and L. S. Davis, “Viton: An image-based virtual
    try-on network,” in *Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition*, 2018, pp. 7543–7552.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] X. Han, Z. Wu, Z. Wu, R. Yu, 和 L. S. Davis，“Viton：基于图像的虚拟试穿网络，”在*IEEE计算机视觉与模式识别会议论文集*，2018年，7543–7552页。'
- en: '[86] B. Wang, H. Zheng, X. Liang, Y. Chen, L. Lin, and M. Yang, “Toward characteristic-preserving
    image-based virtual try-on network,” in *Proceedings of the European Conference
    on Computer Vision*, 2018, pp. 589–604.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] B. Wang, H. Zheng, X. Liang, Y. Chen, L. Lin, 和 M. Yang, “面向特征保留的基于图像的虚拟试衣网络，”
    见于 *欧洲计算机视觉会议论文集*，2018，第589–604页。'
- en: '[87] R. Yu, X. Wang, and X. Xie, “Vtnfp: An image-based virtual try-on network
    with body and clothing feature preservation,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2019, pp. 10 511–10 520.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] R. Yu, X. Wang, 和 X. Xie, “Vtnfp: 一种基于图像的虚拟试衣网络，具有身体和服装特征保留，” 见于 *IEEE/CVF国际计算机视觉会议论文集*，2019，第10 511–10 520页。'
- en: '[88] Z. Wu, G. Lin, Q. Tao, and J. Cai, “M2e-try on net: Fashion from model
    to everyone,” in *Proceedings of the 27th ACM International Conference on Multimedia*,
    2019, pp. 293–301.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Z. Wu, G. Lin, Q. Tao, 和 J. Cai, “M2e-try on net: 从模型到每个人的时尚，” 见于 *第27届ACM国际多媒体会议论文集*，2019，第293–301页。'
- en: '[89] H. Dong, X. Liang, X. Shen, B. Wang, H. Lai, J. Zhu, Z. Hu, and J. Yin,
    “Towards multi-pose guided virtual try-on network,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2019, pp. 9026–9035.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] H. Dong, X. Liang, X. Shen, B. Wang, H. Lai, J. Zhu, Z. Hu, 和 J. Yin,
    “面向多姿势引导的虚拟试衣网络，” 见于 *IEEE/CVF国际计算机视觉会议论文集*，2019，第9026–9035页。'
- en: '[90] G. Liu, D. Song, R. Tong, and M. Tang, “Toward realistic virtual try-on
    through landmark-guided shape matching,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, 2021, pp. 2118–2126.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] G. Liu, D. Song, R. Tong, 和 M. Tang, “通过地标引导的形状匹配实现逼真的虚拟试衣，” 见于 *AAAI人工智能会议论文集*，2021，第2118–2126页。'
- en: '[91] Z. Xie, X. Zhang, F. Zhao, H. Dong, M. Kampffmeyer, H. Yan, and X. Liang,
    “Was-vton: Warping architecture search for virtual try-on network,” in *Proceedings
    of the 29th ACM International Conference on Multimedia*, 2021, pp. 3350–3359.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Z. Xie, X. Zhang, F. Zhao, H. Dong, M. Kampffmeyer, H. Yan, 和 X. Liang,
    “Was-vton: 用于虚拟试衣网络的变形架构搜索，” 见于 *第29届ACM国际多媒体会议论文集*，2021，第3350–3359页。'
- en: '[92] F. Zhao, Z. Xie, M. Kampffmeyer, H. Dong, S. Han, T. Zheng, T. Zhang,
    and X. Liang, “M3d-vton: A monocular-to-3d virtual try-on network,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 13 239–13 249.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] F. Zhao, Z. Xie, M. Kampffmeyer, H. Dong, S. Han, T. Zheng, T. Zhang,
    和 X. Liang, “M3d-vton: 单目到三维虚拟试衣网络，” 见于 *IEEE/CVF国际计算机视觉会议论文集*，2021，第13 239–13 249页。'
- en: '[93] T. Issenhuth, J. Mary, and C. Calauzenes, “Do not mask what you do not
    need to mask: a parser-free virtual try-on,” in *Proceedings of the European Conference
    on Computer Vision*, 2020, pp. 619–635.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] T. Issenhuth, J. Mary, 和 C. Calauzenes, “不要掩盖你不需要掩盖的东西: 无解析器虚拟试衣，” 见于
    *欧洲计算机视觉会议论文集*，2020，第619–635页。'
- en: '[94] Y. Chang, T. Peng, R. He, X. Hu, J. Liu, Z. Zhang, and M. Jiang, “Pf-vton:
    Toward high-quality parser-free virtual try-on network,” in *International Conference
    on Multimedia Modeling*, 2022, pp. 28–40.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Y. Chang, T. Peng, R. He, X. Hu, J. Liu, Z. Zhang, 和 M. Jiang, “Pf-vton:
    面向高质量无解析器虚拟试衣网络，” 见于 *多媒体建模国际会议*，2022，第28–40页。'
- en: '[95] C. Lin, Z. Li, S. Zhou, S. Hu, J. Zhang, L. Luo, J. Zhang, L. Huang, and
    Y. He, “Rmgn: A regional mask guided network for parser-free virtual try-on,”
    *arXiv preprint arXiv:2204.11258*, 2022.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] C. Lin, Z. Li, S. Zhou, S. Hu, J. Zhang, L. Luo, J. Zhang, L. Huang, 和
    Y. He, “Rmgn: 一种区域掩码引导网络用于无解析器虚拟试衣，” *arXiv预印本 arXiv:2204.11258*，2022。'
- en: '[96] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in Neural
    Information Processing Systems*, 2014.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio, “生成对抗网络，” 见于 *神经信息处理系统进展*，2014。'
- en: '[97] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture
    for generative adversarial networks,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 4401–4410.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] T. Karras, S. Laine, 和 T. Aila, “基于风格的生成器架构用于生成对抗网络，” 见于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2019，第4401–4410页。'
- en: '[98] M. Niemeyer and A. Geiger, “Giraffe: Representing scenes as compositional
    generative neural feature fields,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 11 453–11 464.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] M. Niemeyer 和 A. Geiger, “Giraffe: 将场景表示为组合生成神经特征场，” 见于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2021，第11 453–11 464页。'
- en: '[99] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever,
    and M. Chen, “Glide: Towards photorealistic image generation and editing with
    text-guided diffusion models,” *arXiv preprint arXiv:2112.10741*, 2021.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I.
    Sutskever, 和 M. Chen，“Glide: Towards photorealistic image generation and editing
    with text-guided diffusion models，” *arXiv 预印本 arXiv:2112.10741*，2021年。'
- en: '[100] B. Wu, Z. Xie, X. Liang, Y. Xiao, H. Dong, and L. Lin, “Image comes dancing
    with collaborative parsing-flow video synthesis,” *IEEE Transactions on Image
    Processing*, vol. 30, pp. 9259–9269, 2021.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] B. Wu, Z. Xie, X. Liang, Y. Xiao, H. Dong, 和 L. Lin，“图像舞动着与协作解析流视频合成，”
    *IEEE图像处理学报*，第30卷，第9259–9269页，2021年。'
- en: '[101] A. Fruhstuck, K. K. Singh, E. Shechtman, J. Mitra, Niloy, P. Wonka, and
    J. Lu, “Insetgan for full-body image generation,” *arXiv preprint arXiv:2203.07293*,
    2022.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] A. Fruhstuck, K. K. Singh, E. Shechtman, J. Mitra, Niloy, P. Wonka, 和
    J. Lu，“Insetgan用于全身图像生成，” *arXiv 预印本 arXiv:2203.07293*，2022年。'
- en: '[102] R. Chen, X. Chen, B. Ni, and Y. Ge, “Simswap: An efficient framework
    for high fidelity face swapping,” in *Proceedings of the 28th ACM International
    Conference on Multimedia*, 2020, pp. 2003–2011.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] R. Chen, X. Chen, B. Ni, 和 Y. Ge，“Simswap: 高保真面部交换的高效框架，” 收录于 *第28届ACM国际多媒体会议论文集*，2020年，第2003–2011页。'
- en: '[103] L. Yang, Q. Song, and Y. Wu, “Attacks on state-of-the-art face recognition
    using attentional adversarial attack generative network,” *Multimedia Tools and
    Applications*, vol. 80, no. 1, pp. 855–875, 2021.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] L. Yang, Q. Song, 和 Y. Wu，“利用注意力对抗攻击生成网络对先进面部识别系统的攻击，” *多媒体工具与应用*，第80卷，第1期，第855–875页，2021年。'
- en: '[104] Y. Liu, W. Chen, L. Liu, and M. S. Lew, “Swapgan: A multistage generative
    approach for person-to-person fashion style transfer,” *IEEE Transactions on Multimedia*,
    vol. 21, no. 9, pp. 2209–2222, 2019.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Y. Liu, W. Chen, L. Liu, 和 M. S. Lew，“Swapgan: 一种用于人对人时尚风格转移的多阶段生成方法，”
    *IEEE 多媒体学报*，第21卷，第9期，第2209–2222页，2019年。'
- en: '[105] J. Huo, S. Jin, W. Li, J. Wu, Y.-K. Lai, Y. Shi, and Y. Gao, “Manifold
    alignment for semantically aligned style transfer,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 14 861–14 869.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] J. Huo, S. Jin, W. Li, J. Wu, Y.-K. Lai, Y. Shi, 和 Y. Gao，“用于语义对齐风格转移的流形对齐，”
    收录于 *IEEE/CVF国际计算机视觉会议论文集*，2021年，第14 861–14 869页。'
- en: '[106] Z. Ma, T. Lin, X. Li, F. Li, D. He, E. Ding, N. Wang, and X. Gao, “Dual-affinity
    style embedding network for semantic-aligned image style transfer,” *IEEE Transactions
    on Neural Networks and Learning Systems*, 2022.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Z. Ma, T. Lin, X. Li, F. Li, D. He, E. Ding, N. Wang, 和 X. Gao，“用于语义对齐图像风格转移的双亲和风格嵌入网络，”
    *IEEE 神经网络与学习系统学报*，2022年。'
- en: '[107] B.-K. Kim, G. Kim, and S.-Y. Lee, “Style-controlled synthesis of clothing
    segments for fashion image manipulation,” *IEEE Transactions on Multimedia*, vol. 22,
    no. 2, pp. 298–310, 2019.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] B.-K. Kim, G. Kim, 和 S.-Y. Lee，“用于时尚图像操作的风格控制的服装分段合成，” *IEEE 多媒体学报*，第22卷，第2期，第298–310页，2019年。'
- en: '[108] E. Ntavelis, A. Romero, I. Kastanis, L. V. Gool, and R. Timofte, “Sesame:
    Semantic editing of scenes by adding, manipulating or erasing objects,” in *Proceedings
    of the European Conference on Computer Vision*, 2020, pp. 394–411.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] E. Ntavelis, A. Romero, I. Kastanis, L. V. Gool, 和 R. Timofte，“Sesame:
    通过添加、操控或擦除对象进行场景的语义编辑，” 收录于 *欧洲计算机视觉会议论文集*，2020年，第394–411页。'
- en: '[109] H.-Y. Tseng, M. Fisher, J. Lu, Y. Li, V. Kim, and M.-H. Yang, “Modeling
    artistic workflows for image generation and editing,” in *Proceedings of the European
    Conference on Computer Vision*, 2020, pp. 158–174.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] H.-Y. Tseng, M. Fisher, J. Lu, Y. Li, V. Kim, 和 M.-H. Yang，“艺术工作流程建模用于图像生成与编辑，”
    收录于 *欧洲计算机视觉会议论文集*，2020年，第158–174页。'
- en: '[110] S. Liu, X. Liang, L. Liu, X. Shen, J. Yang, C. Xu, and L. Lin, “Matching-cnn
    meets knn: Quasi-parametric human parsing,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2015, pp. 1419–1427.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] S. Liu, X. Liang, L. Liu, X. Shen, J. Yang, C. Xu, 和 L. Lin，“匹配-cnn 遇上
    knn: 准参数人类解析，” 收录于 *IEEE计算机视觉与模式识别会议论文集*，2015年，第1419–1427页。'
- en: '[111] S. Liu, X. Liang, L. Liu, K. Lu, L. Lin, X. Cao, and S. Yan, “Fashion
    parsing with video context,” *IEEE Transactions on Multimedia*, vol. 17, no. 8,
    pp. 1347–1358, 2015.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] S. Liu, X. Liang, L. Liu, K. Lu, L. Lin, X. Cao, 和 S. Yan，“基于视频上下文的时尚解析，”
    *IEEE 多媒体学报*，第17卷，第8期，第1347–1358页，2015年。'
- en: '[112] F. Xia, J. Zhu, P. Wang, and A. L. Yuille, “Pose-guided human parsing
    by an and/or graph using pose-context features,” *Proceedings of the AAAI Conference
    on Artificial Intelligence*, pp. 3632–3640, 2016.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] F. Xia, J. Zhu, P. Wang, 和 A. L. Yuille，“通过姿态图和/或图进行姿态引导的人类解析，” *AAAI人工智能会议论文集*，第3632–3640页，2016年。'
- en: '[113] X. Liang, X. Shen, J. Feng, L. Lin, and S. Yan, “Semantic object parsing
    with graph lstm,” in *Proceedings of the European Conference on Computer Vision*,
    2016, pp. 125–143.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] X. 梁，X. 沈，J. 冯，L. 林，和 S. 严，"具有图graph LSTM的语义对象解析"，载于 *欧洲计算机视觉会议论文集*，2016年，pp.
    125–143。'
- en: '[114] X. Liang, L. Lin, W. Yang, P. Luo, J. Huang, and S. Yan, “Clothes co-parsing
    via joint image segmentation and labeling with application to clothing retrieval,”
    *IEEE Transactions on Multimedia*, vol. 18, no. 6, pp. 1175–1186, 2016.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] X. 梁，L. 林，W. 杨，P. 罗，J. 黄，和 S. 严，"通过联合图像分割和标记进行服装协同解析，并应用于服装检索"，*IEEE多媒体交易*，卷18，第6期，2016年，pp.
    1175–1186。'
- en: '[115] X. Liang, L. Lin, X. Shen, J. Feng, S. Yan, and E. P. Xing, “Interpretable
    structure-evolving lstm,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2017, pp. 1010–1019.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] X. 梁，L. 林，X. 沈，J. 冯，S. 严，和 E. P. 辛，"可解释的结构演进LSTM"，载于 *计算机视觉和模式识别IEEE会议论文集*，2017年，pp.
    1010–1019。'
- en: '[116] K. Gong, X. Liang, D. Zhang, X. Shen, and L. Lin, “Look into person:
    Self-supervised structure-sensitive learning and a new benchmark for human parsing,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2017, pp. 932–940.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] K. 龚，X. 梁，D. 张，X. 沈，和 L. 林，"探究人体：自监督的结构敏感学习和人体解析新基准"，载于 *计算机视觉和模式识别IEEE会议论文集*，2017年，pp.
    932–940。'
- en: '[117] F. Xia, P. Wang, X. Chen, and A. L. Yuille, “Joint multi-person pose
    estimation and semantic part segmentation,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017, pp. 6769–6778.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] F. 夏，P. 王，X. 陈，和 A. L.于因，"联合多人姿势估计和语义部分分割"，载于 *计算机视觉和模式识别IEEE会议论文集*，2017年，pp.
    6769–6778。'
- en: '[118] B. Zhu, Y. Chen, M. Tang, and J. Wang, “Progressive cognitive human parsing,”
    in *Proceedings of the AAAI Conference on Artificial Intelligence*, 2018, pp.
    7607–7614.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] B. 朱，Y. 陈，M. 唐，和 J. 王，"渐进的认知人体解析"，载于 *人工智能AAAI会议论文集*，2018年，pp. 7607–7614。'
- en: '[119] X. Luo, Z. Su, and J. Guo, “Trusted guidance pyramid network for human
    parsing,” in *Proceedings of the 26th ACM International Conference on Multimedia*,
    2018, pp. 654–662.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] X. 罗，Z. 苏，和 J. 郭，"人体解析的可信引导金字塔网络"，载于 *ACM多媒体国际会议论文集*，2018年，pp. 654–662。'
- en: '[120] Y. Zhao, J. Li, Y. Zhang, and Y. Tian, “Multi-class part parsing with
    joint boundary-semantic awareness,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 9177–9186.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Y. 赵，J. 李，Y. 张，和 Y. 田，"具有联合边界-语义意识的多类部分解析"，载于 *计算机视觉IEEE/CVF国际会议论文集*，2019年，pp.
    9177–9186。'
- en: '[121] H. He, J. Zhang, Q. Zhang, and D. Tao, “Grapy-ml: Graph pyramid mutual
    learning for cross-dataset human parsing,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, 2020, pp. 10 949–10 956.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] H. 何，J. 张，Q. 张，和 D. 陶，"图金字塔相互学习实现跨数据集人体解析"，载于 *人工智能AAAI会议论文集*，2020年，pp.
    10 949–10 956。'
- en: '[122] Y. Yuan, X. Chen, and J. Wang, “Object-contextual representations for
    semantic segmentation,” in *Proceedings of the European Conference on Computer
    Vision*, 2020, pp. 173–190.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Y. 袁，X. 陈，和 J. 王，"语义分割的对象-背景表示"，载于 *欧洲计算机视觉会议论文集*，2020年，pp. 173–190。'
- en: '[123] X. Zhang, Y. Chen, B. Zhu, J. Wang, and M. Tang, “Blended grammar network
    for human parsing,” in *Proceedings of the European Conference on Computer Vision*,
    2020, pp. 189–205.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] X. 张，Y. 陈，B. 朱，J. 王，和 M. 唐，"人体解析的混合语法网络"，载于 *欧洲计算机视觉会议论文集*，2020年，pp.
    189–205。'
- en: '[124] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu,
    M. Tan, X. Wang, W. Liu, and B. Xiao, “Deep high-resolution representation learning
    for visual recognition,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 43, no. 10, pp. 3349–3364, 2020.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] J. 王，K. 孙，T. 程，B. 蒋，C. 邓，Y. 赵，D. 刘，Y. 穆，M. 谭，X. 王，W. 刘，和 B. 肖，"用于视觉识别的深度高分辨率表示学习"，*IEEE图像与机器智能交易*，卷43，第10期，2020年，pp.
    3349–3364。'
- en: '[125] Y. Liu, S. Zhang, J. Yang, and P. Yuen, “Hierarchical information passing
    based noise-tolerant hybrid learning for semi-supervised human parsing,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, 2021, pp. 2207–2215.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Y. 刘，S. 张，J. 杨，和 P. 袁，"基于分层信息传递的半监督人体解析的噪声容忍混合学习"，载于 *人工智能AAAI会议论文集*，2021年，pp.
    2207–2215。'
- en: '[126] Z. Jin, T. Gong, D. Yu, Q. Chu, J. Wang, C. Wang, and J. Shao, “Mining
    contextual information beyond image for semantic segmentation,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 7231–7241.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Z. Jin, T. Gong, D. Yu, Q. Chu, J. Wang, C. Wang, 和 J. Shao，“挖掘超越图像的上下文信息用于语义分割，”发表于
    *IEEE/CVF 国际计算机视觉会议论文集*，2021年，第7231–7241页。'
- en: '[127] Z. Jin, B. Liu, Q. Chu, and N. Yu, “Isnet: Integrate image-level and
    semantic-level context for semantic segmentation,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 7189–7198.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Z. Jin, B. Liu, Q. Chu, 和 N. Yu，“Isnet：整合图像级和语义级上下文用于语义分割，”发表于 *IEEE/CVF
    国际计算机视觉会议论文集*，2021年，第7189–7198页。'
- en: '[128] D. Zeng, Y. Huang, Q. Bao, J. Zhang, C. Su, and W. Liu, “Neural architecture
    search for joint human parsing and pose estimation,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 11 385–11 394.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] D. Zeng, Y. Huang, Q. Bao, J. Zhang, C. Su, 和 W. Liu，“用于联合人体解析和姿态估计的神经架构搜索，”发表于
    *IEEE/CVF 国际计算机视觉会议论文集*，2021年，第11 385–11 394页。'
- en: '[129] Z. Zhang, C. Su, L. Zheng, X. Xie, and Y. Li, “On the correlation among
    edge, pose and parsing,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2021.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Z. Zhang, C. Su, L. Zheng, X. Xie, 和 Y. Li，“边缘、姿态与解析之间的相关性，”*IEEE 模式分析与机器智能学报*，2021年。'
- en: '[130] W. Wang, T. Zhou, S. Qi, J. Shen, and S.-C. Zhu, “Hierarchical human
    semantic parsing with comprehensive part-relation modeling,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2021.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] W. Wang, T. Zhou, S. Qi, J. Shen, 和 S.-C. Zhu，“具有全面部分关系建模的分层人类语义解析，”*IEEE
    模式分析与机器智能学报*，2021年。'
- en: '[131] K. Liu, O. Choi, J. Wang, and W. Hwang, “Cdgnet: Class distribution guided
    network for human parsing,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2021, pp. 4473–4482.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] K. Liu, O. Choi, J. Wang, 和 W. Hwang，“Cdgnet：用于人体解析的类分布引导网络，”发表于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2021年，第4473–4482页。'
- en: '[132] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Computation*,
    vol. 9, no. 8, pp. 1735—1780, 1997.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] S. Hochreiter 和 J. Schmidhuber，“长短期记忆，”*神经计算*，第9卷，第8期，第1735—1780页，1997年。'
- en: '[133] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2017, pp. 2881–2890.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] H. Zhao, J. Shi, X. Qi, X. Wang, 和 J. Jia，“金字塔场景解析网络，”发表于 *IEEE 计算机视觉与模式识别会议论文集*，2017年，第2881–2890页。'
- en: '[134] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,
    “Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, vol. 40, no. 4, pp. 834–848, 2017.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, 和 A. L. Yuille，“Deeplab：通过深度卷积网络、空洞卷积和全连接CRF进行语义图像分割，”*IEEE
    模式分析与机器智能学报*，第40卷，第4期，第834–848页，2017年。'
- en: '[135] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017, pp. 2117–2125.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, 和 S. Belongie，“用于目标检测的特征金字塔网络，”发表于
    *IEEE 计算机视觉与模式识别会议论文集*，2017年，第2117–2125页。'
- en: '[136] A. Kirillov, R. Girshick, K. He, and P. Dollar, “Panoptic feature pyramid
    networks,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, 2019, pp. 6399–6408.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] A. Kirillov, R. Girshick, K. He, 和 P. Dollar，“全景特征金字塔网络，”发表于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2019年，第6399–6408页。'
- en: '[137] J. Fang, Y. Sun, Q. Zhang, Y. Li, W. Liu, and X. Wang, “Densely connected
    search space for more flexible neural architecture search,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020, pp.
    10 628–10 637.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] J. Fang, Y. Sun, Q. Zhang, Y. Li, W. Liu, 和 X. Wang，“用于更灵活神经架构搜索的密集连接搜索空间，”发表于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，2020年，第10 628–10 637页。'
- en: '[138] Q. Li, A. Arnab, and P. H. Torr, “Holistic, instance-level human parsing,”
    in *British Machine Vision Conference*, 2017.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Q. Li, A. Arnab, 和 P. H. Torr，“整体的实例级人体解析，”发表于 *英国机器视觉会议*，2017年。'
- en: '[139] H. Qin, W. Hong, W.-C. Hung, Y.-H. Tsai, and M.-H. Yang, “A top-down
    unified framework for instance-level human parsing,” in *British Machine Vision
    Conference*, 2019.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] H. Qin, W. Hong, W.-C. Hung, Y.-H. Tsai, 和 M.-H. Yang，“一个自上而下的统一框架用于实例级人体解析，”发表于
    *英国机器视觉会议*，2019年。'
- en: '[140] L. Yang, Q. Song, Z. Wang, M. Hu, C. Liu, X. Xin, W. Jia, and S. Xu,
    “Renovating parsing r-cnn for accurate multiple human parsing,” in *Proceedings
    of the European Conference on Computer Vision*, 2020, pp. 421–437.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] L. Yang, Q. Song, Z. Wang, M. Hu, C. Liu, X. Xin, W. Jia, 和 S. Xu, “改进解析
    r-cnn 以实现准确的多人人体解析，” 见于 *Proceedings of the European Conference on Computer Vision*,
    2020, 第421–437页。'
- en: '[141] J. Zhao, J. Li, H. Liu, S. Yan, and J. Feng, “Fine-grained multi-human
    parsing,” *International Journal of Computer Vision*, vol. 128, no. 8, pp. 2185–2203,
    2020.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] J. Zhao, J. Li, H. Liu, S. Yan, 和 J. Feng, “细粒度多人人体解析，” *International
    Journal of Computer Vision*, 卷128，第8期，第2185–2203页, 2020。'
- en: '[142] S. Zhang, X. Cao, G.-J. Qi, Z. Song, and J. Zhou, “Aiparsing: Anchor-free
    instance-level human parsing,” *IEEE Transactions on Image Processing*, 2022.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] S. Zhang, X. Cao, G.-J. Qi, Z. Song, 和 J. Zhou, “Aiparsing: 无锚点实例级人体解析，”
    *IEEE Transactions on Image Processing*, 2022。'
- en: '[143] M. Kiefel and P. Gehler, “Human pose estimation with fields of parts,”
    in *Proceedings of the European Conference on Computer Vision*, 2014, pp. 331—346.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] M. Kiefel 和 P. Gehler, “使用部件场的人体姿态估计，” 见于 *Proceedings of the European
    Conference on Computer Vision*, 2014, 第331–346页。'
- en: '[144] K. He, G. Gkioxari, P. Dollar, and R. Girshick, “Mask r-cnn,” in *Proceedings
    of the IEEE International Conference on Computer Vision*, 2017, pp. 2961–2969.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] K. He, G. Gkioxari, P. Dollar, 和 R. Girshick, “Mask r-cnn，” 见于 *Proceedings
    of the IEEE International Conference on Computer Vision*, 2017, 第2961–2969页。'
- en: '[145] Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: A simple and strong anchor-free
    object detector,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 44, no. 4, pp. 1922–1933, 2020.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Z. Tian, C. Shen, H. Chen, 和 T. He, “Fcos: 一种简单且强大的无锚目标检测器，” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 卷44，第4期，第1922–1933页, 2020。'
- en: '[146] X. Wang, A. Jabri, and A. A. Efros, “Learning correspondence from the
    cycle-consistency of time,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 2566–2576.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] X. Wang, A. Jabri, 和 A. A. Efros, “从时间的循环一致性中学习对应关系，” 见于 *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    第2566–2576页。'
- en: '[147] X. Li, S. Liu, S. D. Mello, X. Wang, J. Kautz, and M.-H. Yang, “Joint-task
    self-supervised learning for temporal correspondence,” in *Advances in Neural
    Information Processing Systems*, 2019, pp. 318–328.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] X. Li, S. Liu, S. D. Mello, X. Wang, J. Kautz, 和 M.-H. Yang, “用于时间对应的联合任务自监督学习，”
    见于 *Advances in Neural Information Processing Systems*, 2019, 第318–328页。'
- en: '[148] A. A. Jabri, A. Owens, and A. A. Efros, “Space-time correspondence as
    a contrastive random walk,” in *Advances in Neural Information Processing Systems*,
    2020, pp. 19 545–19 560.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] A. A. Jabri, A. Owens, 和 A. A. Efros, “作为对比随机游走的时空对应关系，” 见于 *Advances
    in Neural Information Processing Systems*, 2020, 第19 545–19 560页。'
- en: '[149] N. Wang, W. Zhou, and H. Li, “Contrastive transformation for self-supervised
    correspondence learning,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, 2021, pp. 10 174–10 182.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] N. Wang, W. Zhou, 和 H. Li, “自监督对应学习的对比变换，” 见于 *Proceedings of the AAAI
    Conference on Artificial Intelligence*, 2021, 第10 174–10 182页。'
- en: '[150] S. Jeon, D. Min, S. Kim, and K. Sohn, “Mining better samples for contrastive
    learning of temporal correspondence,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 1034–1044.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] S. Jeon, D. Min, S. Kim, 和 K. Sohn, “为时间对应的对比学习挖掘更好的样本，” 见于 *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    第1034–1044页。'
- en: '[151] J. Xu and X. Wang, “Rethinking self-supervised correspondence learning:
    A video frame-level similarity perspective,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 10 075–10 085.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] J. Xu 和 X. Wang, “重新思考自监督对应学习：从视频帧级相似性的视角出发，” 见于 *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2021, 第10 075–10 085页。'
- en: '[152] Z. Zhao, Y. Jin, and P.-A. Heng, “Modelling neighbor relation in joint
    space-time graph for video correspondence learning,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 9960–9969.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Z. Zhao, Y. Jin, 和 P.-A. Heng, “在时空图中建模邻域关系以进行视频对应学习，” 见于 *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, 第9960–9969页。'
- en: '[153] L. Li, T. Zhou, W. Wang, L. Yang, J. Li, and Y. Yang, “Locality-aware
    inter-and intra-video reconstruction for self-supervised correspondence learning,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] L. Li, T. Zhou, W. Wang, L. Yang, J. Li, 和 Y. Yang, “面向自监督对应学习的局部性感知视频间和视频内重建，”
    见于 *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022。'
- en: '[154] J. Son, “Contrastive learning for space-time correspondence via self-cycle
    consistency,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2022, pp. 14 679–14 688.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] J. Son, “通过自循环一致性进行时空对应的对比学习，” 收录于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，第14 679–14 688页。'
- en: '[155] D. Mckee, Z. Zhan, B. Shuai, D. Modolo, J. Tighe, and S. Lazebnik, “Transfer
    of representations to video label propagation: implementation factors matter,”
    *arXiv preprint arXiv:2203.05553.*, 2022.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] D. Mckee, Z. Zhan, B. Shuai, D. Modolo, J. Tighe, 和 S. Lazebnik, “表示迁移到视频标签传播：实现因素的重要性，”
    *arXiv预印本 arXiv:2203.05553*，2022年。'
- en: '[156] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and K. Murphy,
    “Tracking emerges by colorizing videos,” in *Proceedings of the European Conference
    on Computer Vision*, 2018, pp. 391–408.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, 和 K. Murphy, “通过为视频着色来实现跟踪，”
    收录于 *欧洲计算机视觉会议论文集*，2018年，第391–408页。'
- en: '[157] S. Liu, G. Zhong, S. D. Mello, J. Gu, V. Jampani, M.-H. Yang, and J. Kautz,
    “Switchable temporal propagation network,” in *Proceedings of the European Conference
    on Computer Vision*, 2018, pp. 87–102.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] S. Liu, G. Zhong, S. D. Mello, J. Gu, V. Jampani, M.-H. Yang, 和 J. Kautz,
    “可切换时间传播网络，” 收录于 *欧洲计算机视觉会议论文集*，2018年，第87–102页。'
- en: '[158] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for
    unsupervised visual representation learning,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2020, pp. 9729–9738.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] K. He, H. Fan, Y. Wu, S. Xie, 和 R. Girshick, “用于无监督视觉表示学习的动量对比，” 收录于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第9729–9738页。'
- en: '[159] P. Luo, X. Wang, and X. Tang, “Pedestrian parsing via deep decompositional
    network,” in *Proceedings of the IEEE International Conference on Computer Vision*,
    2013, pp. 2648–2655.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] P. Luo, X. Wang, 和 X. Tang, “通过深度分解网络进行行人解析，” 收录于 *IEEE国际计算机视觉会议论文集*，2013年，第2648–2655页。'
- en: '[160] X. Chen, R. Mottaghi, X. Liu, S. Fidler, R. Urtasun, and A. Yuille, “Detect
    what you can: Detecting and representing objects using holistic models and body
    parts,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2014, pp. 1971–1978.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] X. Chen, R. Mottaghi, X. Liu, S. Fidler, R. Urtasun, 和 A. Yuille, “检测你能检测到的：使用整体模型和身体部件进行对象检测和表示，”
    收录于 *IEEE计算机视觉与模式识别会议论文集*，2014年，第1971–1978页。'
- en: '[161] J. Li, J. Zhao, Y. Wei, C. Lang, Y. Li, T. Sim, S. Yan, and J. Feng,
    “Multiple-human parsing in the wild,” *arXiv preprint arXiv:1705.07206*, 2017.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] J. Li, J. Zhao, Y. Wei, C. Lang, Y. Li, T. Sim, S. Yan, 和 J. Feng, “野外的多人体解析，”
    *arXiv预印本 arXiv:1705.07206*，2017年。'
- en: '[162] J. Zhao, J. Li, Y. Cheng, T. Sim, S. Yan, and J. Feng, “Understanding
    humans in crowded scenes: Deep nested adversarial learning and a new benchmark
    for multi-human parsing,” in *Proceedings of the 26th ACM International Conference
    on Multimedia*, 2018, pp. 792–800.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] J. Zhao, J. Li, Y. Cheng, T. Sim, S. Yan, 和 J. Feng, “在拥挤场景中理解人类：深度嵌套对抗学习与多人体解析的新基准，”
    收录于 *第26届ACM国际多媒体会议论文集*，2018年，第792–800页。'
- en: '[163] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *Proceedings
    of the European Conference on Computer Vision*, 2014, pp. 740–755.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P.
    Dollár, 和 C. L. Zitnick, “Microsoft coco：背景中的常见物体，” 收录于 *欧洲计算机视觉会议论文集*，2014年，第740–755页。'
- en: '[164] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
    “The pascal visual object classes (voc) challenge,” *International Journal of
    Computer Vision*, vol. 88, no. 2, pp. 303–338, 2010.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, 和 A. Zisserman,
    “Pascal视觉目标分类（voc）挑战，” *计算机视觉国际期刊*，第88卷，第2期，第303–338页，2010年。'
- en: '[165] B. Hariharan, P. Arbelaez, R. Girshick, and J. Malik, “Simultaneous detection
    and segmentation,” in *Proceedings of the European Conference on Computer Vision*,
    2014, pp. 297–312.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] B. Hariharan, P. Arbelaez, R. Girshick, 和 J. Malik, “同时检测和分割，” 收录于 *欧洲计算机视觉会议论文集*，2014年，第297–312页。'
- en: '[166] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” in *Advances in Neural
    Information Processing Systems*, 2017, pp. 6000–6010.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, 和 I. Polosukhin, “注意力机制是你所需的一切，” 收录于 *神经信息处理系统进展*，2017年，第6000–6010页。'
- en: '[167] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby,
    “An image is worth 16x16 words: Transformers for image recognition at scale,”
    in *Proceedings of the International Conference on Learning Representations*,
    2020.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T.
    Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, 和 N.
    Houlsby，“一张图像等于16x16个单词：大规模图像识别的变换器，”发表于 *国际学习表征会议论文集*，2020年。'
- en: '[168] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko,
    “End-to-end object detection with transformers,” in *Proceedings of the European
    Conference on Computer Vision*, 2020, pp. 213–229.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, 和 S. Zagoruyko，“基于变换器的端到端目标检测，”发表于
    *欧洲计算机视觉会议论文集*，2020年，第213–229页。'
- en: '[169] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” in *Proceedings
    of the Annual Conference of the North American Chapter of the Association for
    Computational Linguistics: Human Language Technologies*, 2019, pp. 4171–4186.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova，“Bert：用于语言理解的深度双向变换器的预训练，”发表于
    *北美计算语言学协会年会：人类语言技术*，2019年，第4171–4186页。'
- en: '[170] H. Bao, L. Dong, S. Piao, and F. Wei, “Beit: Bert pre-training of image
    transformers,” in *Proceedings of the International Conference on Learning Representations*,
    2022.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] H. Bao, L. Dong, S. Piao, 和 F. Wei，“Beit：图像变换器的Bert预训练，”发表于 *国际学习表征会议论文集*，2022年。'
- en: '[171] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, “Masked autoencoders
    are scalable vision learners,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2022.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, 和 R. Girshick，“掩码自编码器是可扩展的视觉学习器，”发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2022年。'
- en: '[172] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar, “Masked-attention
    mask transformer for universal image segmentation,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2022.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, 和 R. Girdhar，“用于通用图像分割的掩码注意力掩码变换器，”发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2022年。'
- en: '[173] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr: Deformable
    transformers for end-to-end object detection,” in *Proceedings of the International
    Conference on Learning Representations*, 2021.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, 和 J. Dai，“可变形DETR：用于端到端目标检测的可变形变换器，”发表于
    *国际学习表征会议论文集*，2021年。'
- en: '[174] B. Cheng, A. G. Schwing, and A. Kirillov, “Per-pixel classification is
    not all you need for semantic segmentation,” in *Advances in Neural Information
    Processing Systems*, 2021, pp. 17 864–17 875.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] B. Cheng, A. G. Schwing, 和 A. Kirillov，“逐像素分类并非语义分割所需的一切，”发表于 *神经信息处理系统进展*，2021年，第17 864–17 875页。'
- en: '[175] B. Cheng, A. Choudhuri, I. Misra, A. Kirillov, R. Girdhar, and A. G.
    Schwing, “Mask2former for video instance segmentation,” *arXiv preprint arXiv:2112.10764*,
    2021.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] B. Cheng, A. Choudhuri, I. Misra, A. Kirillov, R. Girdhar, 和 A. G. Schwing，“用于视频实例分割的Mask2former，”*arXiv预印本
    arXiv:2112.10764*，2021年。'
- en: '[176] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”
    in *Proceedings of the International Conference on Learning Representations*,
    2018.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] I. Loshchilov 和 F. Hutter，“解耦权重衰减正则化，”发表于 *国际学习表征会议论文集*，2018年。'
- en: '[177] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and F.-F. Li, “Imagenet large
    scale visual recognition challenge,” *International Journal of Computer Vision*,
    vol. 115, no. 3, pp. 211–252, 2015.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, 和 F.-F. Li，“ImageNet大规模视觉识别挑战，”*计算机视觉国际期刊*，第115卷，第3期，第211–252页，2015年。'
- en: '[178] E. Wood, T. Baltrusaitis, C. Hewitt, S. Dziadzio, M. Johnson, V. Estellers,
    T. J. Cashman, and J. Shotton, “Fake it till you make it: Face analysis in the
    wild using synthetic data alone,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2021, pp. 3681–3691.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] E. Wood, T. Baltrusaitis, C. Hewitt, S. Dziadzio, M. Johnson, V. Estellers,
    T. J. Cashman, 和 J. Shotton，“伪造直到成功：仅用合成数据进行野外面部分析，”发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第3681–3691页。'
- en: '[179] L. Yang, H. Jiang, Q. Song, and J. Guo, “A survey on long-tailed visual
    recognition,” *International Journal of Computer Vision*, 2022.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] L. Yang, H. Jiang, Q. Song, 和 J. Guo，“关于长尾视觉识别的调查，”*计算机视觉国际期刊*，2022年。'
- en: '[180] L. Yang, Q. Song, Z. Wang, M. Hu, and C. Liu, “Hier r-cnn: Instance-level
    human parts detection and a new benchmark,” *IEEE Transactions on Image Processing*,
    vol. 30, pp. 39–54, 2020.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] L. Yang, Q. Song, Z. Wang, M. Hu 和 C. Liu，“Hier r-cnn：实例级人体部件检测及新基准”，*IEEE
    图像处理汇刊*，第30卷，第39–54页，2020年。'
- en: '[181] C. Zheng, W. Wu, T. Yang, S. Zhu, C. Chen, R. Liu, J. Shen, N. Kehtarnavaz,
    and M. Shah, “Deep learning-based human pose estimation: A survey,” *arXiv preprint
    arXiv:2012.13392*, 2020.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] C. Zheng, W. Wu, T. Yang, S. Zhu, C. Chen, R. Liu, J. Shen, N. Kehtarnavaz
    和 M. Shah，“基于深度学习的人体姿态估计：综述”，*arXiv 预印本 arXiv:2012.13392*，2020年。'
