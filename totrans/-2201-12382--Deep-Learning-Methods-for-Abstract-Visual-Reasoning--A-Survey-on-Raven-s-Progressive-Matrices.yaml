- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:48:09'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2201.12382] Deep Learning Methods for Abstract Visual Reasoning: A Survey
    on Raven’s Progressive Matrices'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2201.12382](https://ar5iv.labs.arxiv.org/html/2201.12382)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mikołaj Małkiński and Jacek Mańdziuk Mikołaj Małkiński is a Ph.D. student at
    the Doctoral School no. 3, Warsaw University of Technology, pl. Politechniki 1,
    00-661 Warsaw, Poland, m.malkinski@mini.pw.edu.pl.Jacek Mańdziuk is with the Faculty
    of Mathematics and Information Science, Warsaw University of Technology, Koszykowa
    75, 00-662 Warsaw, Poland, mandziuk@mini.pw.edu.pl.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Abstract visual reasoning (AVR) domain encompasses problems solving which requires
    the ability to reason about relations among entities present in a given scene.
    While humans, generally, solve AVR tasks in a “natural” way, even without prior
    experience, this type of problems has proven difficult for current machine learning
    systems. The paper summarises recent progress in applying deep learning methods
    to solving AVR problems, as a proxy for studying machine intelligence. We focus
    on the most common type of AVR tasks—the Raven’s Progressive Matrices (RPMs)—and
    provide a comprehensive review of the learning methods and deep neural models
    applied to solve RPMs, as well as, the RPM benchmark sets. Performance analysis
    of the state-of-the-art approaches to solving RPMs leads to formulation of certain
    insights and remarks on the current and future trends in this area. We conclude
    the paper by demonstrating how real-world problems can benefit from the discoveries
    of RPM studies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Abstract Visual Reasoning, Deep Learning, Raven’s Progressive Matrices
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Along-standing goal of human research endeavours is to understand the nature
    of intelligence. Even though existing literature points to a variety of definitions [[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)], most related to this work is intelligence as
    portrayed by the ability of applying existing knowledge, skills, and past experiences
    in entirely new settings. This perspective was taken in a number of cognitive
    studies that measured intelligence (IQ) with the help of abstract visual reasoning
    (AVR) tasks [[4](#bib.bib4), [5](#bib.bib5)]. AVR problems consist of images with
    simple 2D shapes governed by underlying abstract rules. In order to solve them,
    the test-taker has to identify and often understand never-encountered abstract
    patterns and generalise them to new settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although there exists a wide range of AVR tasks [[6](#bib.bib6)], in the cognitive
    literature one of them was studied with particular attention—the Raven’s Progressive
    Matrices (RPMs) [[7](#bib.bib7), [8](#bib.bib8)] (see Fig. [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey
    on Raven’s Progressive Matrices")). RPMs were found to be highly diagnostic of
    abstract and relational reasoning abilities [[4](#bib.bib4)] and representative
    for human intelligence in general [[5](#bib.bib5)]. In light of these observations,
    recent works have started to investigate whether automatic pattern discovery algorithms
    are capable of achieving performance comparable to humans in solving RPMs.'
  prefs: []
  type: TYPE_NORMAL
- en: A recent stream of research devoted to developing intelligent pattern analysis
    methods employs deep learning (DL) [[9](#bib.bib9)] for discovering regularities
    in complex settings. Motivated by the impressive performance of DL methods in
    various domains [[10](#bib.bib10)] a question whether DL approaches could be effectively
    applied to solving RPMs has been posed [[11](#bib.bib11), [12](#bib.bib12)]. Since
    these seminal works, a number of approaches have been proposed which are reviewed
    and compared in this survey.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4eb4eb207362aa353c0b0eaab77e6a99.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: RPM example. Remarkably, humans are able to intuitively solve the
    challenge even without the exact definition of the task (that is presented in
    Section [2](#S2 "2 Raven’s Progressive Matrices ‣ Deep Learning Methods for Abstract
    Visual Reasoning: A Survey on Raven’s Progressive Matrices")). The matrix is governed
    by multiple abstract patterns. Each row contains circle slices of 3 different
    colours split among columns, whereas squares have constant colour in each row.
    Moreover, positions of objects in the third column are determined by logical XOR
    applied row-wise in the case of squares and logical OR in the case of circle slices.
    The correct answer is F.'
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: 'for tree= line width=1pt, if=level()¡2 my rounded corners, draw=linecol, ,
    edge=color=linecol, ¿=Triangle[], -¿, if level=0l sep+=.75cm, align=center, font=,
    parent anchor=south, if level=1parent anchor=south west, child anchor=north, tier=parting
    ways, align=center, font=, for descendants= child anchor=west, parent anchor=west,
    anchor=west, align=left, , if level=2 shape=coordinate, no edge, grow’=0, calign
    with current edge, xshift=10pt, for descendants= parent anchor=south west, l sep+=-15pt
    , for children= edge path= [\forestoptionedge] (!to tier=parting ways.parent anchor)
    —- (.child anchor)\forestoptionedge label; , for descendants= no edge, , , , ,
    , [Raven’s Progressive Matrices [Datasets (Section [2](#S2 "2 Raven’s Progressive
    Matrices ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s
    Progressive Matrices")) [ [1\. Sandia [[13](#bib.bib13)]] [2\. Synthetic RPMs [[14](#bib.bib14)]]
    [3\. D-set and G-set [[15](#bib.bib15)]] [4\. PGM [[12](#bib.bib12)]] [5\. RAVEN
    sets [(a) RAVEN [[16](#bib.bib16)]'
  prefs: []
  type: TYPE_NORMAL
- en: (b) I-RAVEN [[17](#bib.bib17)]
  prefs: []
  type: TYPE_NORMAL
- en: '(c) RAVEN-FAIR [[18](#bib.bib18)]] ] ] ] [Learning methods (Section [3](#S3
    "3 Learning to solve RPMs ‣ Deep Learning Methods for Abstract Visual Reasoning:
    A Survey on Raven’s Progressive Matrices")) [ [1\. Supervised training (e.g. [[12](#bib.bib12),
    [16](#bib.bib16)])] [2\. Auxiliary training [(a) Multi-hot (dense) encoding [[12](#bib.bib12),
    [16](#bib.bib16), [19](#bib.bib19)]'
  prefs: []
  type: TYPE_NORMAL
- en: '(b) One-hot (sparse) encoding [[19](#bib.bib19)]] ] [3\. Contrastive training [[20](#bib.bib20),
    [19](#bib.bib19), [21](#bib.bib21)]] [4\. Feature Robust Abstract Reasoning [[22](#bib.bib22)]]
    [5\. Data augmentation [[19](#bib.bib19), [21](#bib.bib21)]] [6\. Disentangled [[23](#bib.bib23),
    [15](#bib.bib15), [21](#bib.bib21)]] [7\. Generative modeling [[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)]] [8\. Unsupervised learning [[28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30)]] ] ] [Deep learning models (Section [4](#S4
    "4 RPM Deep Learning Models ‣ Deep Learning Methods for Abstract Visual Reasoning:
    A Survey on Raven’s Progressive Matrices")) [ [1\. Baselines [(a) CNN + MLP [[11](#bib.bib11),
    [12](#bib.bib12), [16](#bib.bib16)]'
  prefs: []
  type: TYPE_NORMAL
- en: (b) LSTM + MLP [[12](#bib.bib12), [16](#bib.bib16)]
  prefs: []
  type: TYPE_NORMAL
- en: (c) ResNet + MLP [[12](#bib.bib12), [16](#bib.bib16)]
  prefs: []
  type: TYPE_NORMAL
- en: (d) Context-blind [[12](#bib.bib12), [17](#bib.bib17)]] ] [2\. Relational reasoning
    networks [(a) Wild ResNet [[12](#bib.bib12)]
  prefs: []
  type: TYPE_NORMAL
- en: (b) WReN [[12](#bib.bib12), [16](#bib.bib16)]
  prefs: []
  type: TYPE_NORMAL
- en: i. VAE-WReN [[23](#bib.bib23)]
  prefs: []
  type: TYPE_NORMAL
- en: ii. ARNe [[31](#bib.bib31)]
  prefs: []
  type: TYPE_NORMAL
- en: iii. MLRN [[32](#bib.bib32)]
  prefs: []
  type: TYPE_NORMAL
- en: (b) CoPINet [[20](#bib.bib20)]
  prefs: []
  type: TYPE_NORMAL
- en: (c) MRNet [[18](#bib.bib18)]
  prefs: []
  type: TYPE_NORMAL
- en: (d) LEN [[22](#bib.bib22)]
  prefs: []
  type: TYPE_NORMAL
- en: i. T-LEN [[22](#bib.bib22)]] ] [3\. Hierarchical networks [(a) SRAN [[17](#bib.bib17)]
  prefs: []
  type: TYPE_NORMAL
- en: (b) SCL [[33](#bib.bib33)]
  prefs: []
  type: TYPE_NORMAL
- en: (c) Rel-AIR [[34](#bib.bib34)]
  prefs: []
  type: TYPE_NORMAL
- en: i. Rel-Base [[34](#bib.bib34)]
  prefs: []
  type: TYPE_NORMAL
- en: (d) DCNet [[35](#bib.bib35)]
  prefs: []
  type: TYPE_NORMAL
- en: (e) MXGNet [[36](#bib.bib36)]
  prefs: []
  type: TYPE_NORMAL
- en: (f) NI [[37](#bib.bib37)]] ] ] ] ]
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: RPM taxonomy. A list of RPM benchmarks, learning methods and DL models
    considered in this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Motivation and scope
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A growing number of recent publications use RPMs as a proxy to study machine
    intelligence. Due to the increased interest in these problems among the DL community
    since the above-cited seminal papers [[11](#bib.bib11), [12](#bib.bib12)], a number
    of approaches have been proposed that vary in multiple aspects. Furthermore, several
    RPM benchmarks with particular characteristics have been recently proposed that
    allow to analyse various properties and capabilities of the tested methods. These
    methods vary in both the learning setup to solve the tasks and the model architecture.
    The latter oftentimes differs from the architectures used in other domains that
    do not require reasoning about abstract relations spanning multiple entities.
    This RPM-specific aspect imposes the use of dedicated neural components and makes
    analysis of RPM model architectures particularly interesting.
  prefs: []
  type: TYPE_NORMAL
- en: 'This survey collates existing works on RPMs – the prevalent AVR benchmark in
    DL literature. The review is performed along all three above-mentioned angles,
    i.e. benchmark datasets, learning methods, and DL reasoning models, outlined in
    Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning Methods for Abstract
    Visual Reasoning: A Survey on Raven’s Progressive Matrices").'
  prefs: []
  type: TYPE_NORMAL
- en: Some works that focus on RPMs have already reported superhuman performance on
    simpler benchmarks [[15](#bib.bib15)], whereas the essence of AVR, i.e. the ability
    to generalise to novel difficult environments, remains unattained [[12](#bib.bib12)].
    Indeed, a recent work [[38](#bib.bib38)] shows that current AI systems (including
    DL methods, symbolic approaches and probabilistic program induction) still lag
    far behind the human capabilities not only in solving AVR problems, but generally,
    in broader settings of forming abstractions and analogies. In order to assess
    and better understand the “degree” of intelligence represented by the current
    DL approaches for solving RPMs, we analyse and discuss in detail numerical results
    reported in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Related work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Initial attempts that tackled RPMs relied on manually prepared rules and heuristics [[39](#bib.bib39),
    [40](#bib.bib40)], in some cases identified by analysing approaches of well-performing
    human solvers [[41](#bib.bib41)]. Another stream of methods that build on the
    insights from cognitive studies utilised the structure mapping theory [[42](#bib.bib42),
    [43](#bib.bib43)] to propose a set of RPM solvers that automatically discover
    relevant rules [[44](#bib.bib44), [45](#bib.bib45)]. In addition, another set
    of methods eliminates the need for structure mapping and instead focuses on visual
    similarity of the RPM elements after inducing various image transformations [[46](#bib.bib46),
    [47](#bib.bib47)], potentially with different image resolutions that facilitates
    a fractal representation of the RPMs [[48](#bib.bib48), [49](#bib.bib49)]. The
    progress made by these seminal works is comprehensively summarised in [[50](#bib.bib50)].
  prefs: []
  type: TYPE_NORMAL
- en: Despite the existence of many creative approaches for solving RPMs, in recent
    days DL methods constantly prove their ability to tackle some of the most challenging
    RPM benchmarks, surpassing human performance in some problem settings. The recent
    abundance and superiority of DL methods motivated us to focus this survey exclusively
    on DL approaches. To our knowledge, such a review perspective has not been considered
    previously in the RPM literature.
  prefs: []
  type: TYPE_NORMAL
- en: While particular attention in DL literature on AVR problems is devoted to solving
    RPMs, the whole AVR domain is nowhere near limited to this task. In fact, recent
    works introduce a broad spectrum of complementary abstract visual reasoning tasks
    that allow to test different characteristics of DL approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to RPMs, the need for identifying abstract patterns is a recurring
    theme in all AVR tasks. However, depending on the specific problem, these patterns
    have to be extracted from different configurations of matrix panels and then applied
    in various contexts. In the odd one out tasks [[51](#bib.bib51), [52](#bib.bib52),
    [53](#bib.bib53)], the solver has to identify an odd element that breaks a rule
    instantiated in the remaining images. Similar idea is presented in Bongard Problems [[54](#bib.bib54)]
    where the goal is to describe a rule that is instantiated in a set of images and
    broken in a supplementary set of panels. Same-different tasks [[55](#bib.bib55)]
    present a related challenge, in which each problem instance contains two sets
    of images separated by an abstract pattern. Given a new image, the test-taker
    has to assign it to one of the two presented sets. Visual analogy problems [[56](#bib.bib56)]
    are structurally similar to RPMs and alike test the ability of making analogies
    based on abstract patterns that govern the objects and their attributes in images.
    However, in contrast to RPMs, these problems allow to test the ability of generalising
    an abstract concept from a given source domain to a different target domain. Additional
    problems test the extrapolation ability [[57](#bib.bib57)], capacity of recognizing
    abstract patterns from only few samples [[58](#bib.bib58)], or introduce numbers
    into the matrices and test the combined ability of abstract and numerical visual
    reasoning [[59](#bib.bib59)]. A comprehensive review of these emerging AVR tasks
    is conducted in [[60](#bib.bib60)].
  prefs: []
  type: TYPE_NORMAL
- en: Although each of the above-mentioned problems has its own specificity, we believe
    that in-depth overview of DL application to solving RPMs—a predominant AVR challenge—will
    inspire the readers to identify promising paths for solving related AVR tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The rest of this work is structured as follows. In Section [2](#S2 "2 Raven’s
    Progressive Matrices ‣ Deep Learning Methods for Abstract Visual Reasoning: A
    Survey on Raven’s Progressive Matrices") we introduce the Raven’s Progressive
    Matrices, discuss their importance in measuring intelligence and describe benchmarks
    together with their automatic generation methods. In Sections [3](#S3 "3 Learning
    to solve RPMs ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey
    on Raven’s Progressive Matrices") and [4](#S4 "4 RPM Deep Learning Models ‣ Deep
    Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices") we characterise a variety of learning setups to solve RPMs and the
    related DL architectures, respectively. Current evaluations of machine intelligence
    are aggregated and analysed in Section [5](#S5 "5 Evaluating machine intelligence
    with RPMs ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s
    Progressive Matrices"). Section [6](#S6 "6 Discussion ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices") links
    the RPM-related literature with advances in other fields, presents open questions,
    potential practical applications of AVR research, and directions for future studies.
    The survey is concluded in Section [7](#S7 "7 Conclusion ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Raven’s Progressive Matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ability to solve Raven’s Progressive Matrices [[7](#bib.bib7), [8](#bib.bib8)]
    is believed to be highly correlated with human intelligence [[5](#bib.bib5)] and
    is therefore also considered as a natural measure of intelligence of advanced
    artificial reasoning systems. RPMs allow to measure both structural and abstract
    reasoning skills [[4](#bib.bib4)], which characterise human fluid / high-level
    intelligence [[61](#bib.bib61)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Problem statement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although RPMs can have variable structure, the most common matrices investigated
    in recent machine learning (ML) literature are composed of images arranged into
    two distinct parts. The first part of the matrix usually contains 8 images arranged
    in a $3\times 3$ grid, referred to as the context panels, where the bottom-right
    image is missing. The goal is to select a panel which correctly completes this
    matrix from another set of several images, referred to as the answer panels (see
    Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Problem statement ‣ 2 Raven’s Progressive Matrices
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices")). In order to find the correct answer the test-taker is required to
    identify a set of underlying abstract rules which govern the visual attributes
    of the matrix. Generally, these rules describe how image features differ between
    the matrix panels. These relations, as well as objects and their attributes are
    defined differently depending on the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ebc0eb5064771ba2ae7d7e34cc94a22c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: RPM notation. A sample RPM from the RAVEN dataset [[16](#bib.bib16)],
    which consists of two parts – the context and the answer panels. The goal is to
    complete the context with appropriate answer panel. The chosen answer panel must
    fulfil all abstract rules governing the content of the context panels. In the
    example there are three such rules applied: (1) the same set of 3 shapes is present
    in each row, (2) the shapes in a given row are of the same size, and (3) the shape
    color intensity in the third column equals the sum of color intensities in two
    previous columns. Hence, the correct answer is C.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Automatic generation of RPMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The original set of RPMs (called the Standard Progressive Matrices, SPMs) proposed
    in [[7](#bib.bib7), [8](#bib.bib8)] contains a limited number of hand-crafted
    instances, which may be insufficient for training even simple statistical pattern
    recognition models that struggle to learn from small sample size [[62](#bib.bib62),
    [63](#bib.bib63)]. Since the process of successful training of DL models usually
    requires large volumes of training data points [[64](#bib.bib64), [65](#bib.bib65)],
    there is a strong need for methods of automatic generation of AVR problems that
    could provide sufficiently large numbers of training samples.
  prefs: []
  type: TYPE_NORMAL
- en: In order to procedurally generate new RPMs, several challenges have to be addressed.
    First, the generated problems should be visually diverse to make them non-repetitive
    and appealing to the test-taker. Moreover, multiple levels of difficulty are preferred
    that allow to better estimate the reasoning capabilities of the solver. Lastly,
    the generated RPM should be valid, i.e. there should be only one answer from the
    set of possible choices that correctly completes the matrix. The task of choosing
    the correct answer should be realisable by following the Occam’s razor principle
    – the answer should be justifiable with a minimal set of rules that govern relations
    between objects and their attributes across context panels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Geared toward satisfying these requirements, various attempts have been made
    to design an RPM generation algorithm capable of delivering huge number of instances.
    In effect, several popular RPM datasets have been proposed which include the Sandia
    matrices [[13](#bib.bib13)], D-set and G-set from [[15](#bib.bib15)], PGM [[12](#bib.bib12)],
    RAVEN [[16](#bib.bib16)], I-RAVEN [[17](#bib.bib17)] and RAVEN-FAIR [[18](#bib.bib18)].
    Samples from these datasets are illustrated in Fig. [4](#S2.F4 "Figure 4 ‣ 2.2
    Automatic generation of RPMs ‣ 2 Raven’s Progressive Matrices ‣ Deep Learning
    Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec655acba42af0dc1d93f96e4c56e36c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Sandia [[13](#bib.bib13)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/659a690ecb8cf7d2b183e83276ab75c1.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Synthetic [[14](#bib.bib14)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4fe0b6d0d0c25045923a5b620fbce8cd.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) G-set [[15](#bib.bib15)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3c09c0b0c3fe3b8d8f1816aece56b28b.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) PGM [[12](#bib.bib12)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/219ff25e2fde3119285f423e65ac8b8a.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) I-RAVEN [[16](#bib.bib16), [17](#bib.bib17)]
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: RPM examples. Correct answers are marked with a green boundary. Matrices
    from RAVEN, I-RAVEN and RAVEN-FAIR differ only in the way of generating answers,
    hence only a selected matrix from the I-RAVEN dataset is shown.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Sandia matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Sandia matrix generation process [[13](#bib.bib13)] marked the first widely-known
    attempt to expand the set of available RPM instances. Based on the analysis of
    the SPMs, the authors extracted a set of logic rules, shapes, and transformations
    that modify their attributes, which were later used to generate a large set of
    RPMs. The authors compared the quality of generated matrices to the original ones
    and found out, that although simpler instances (with 1 or 2 rules) were of similar
    difficulty to SPMs, instances with higher number of rules were generally more
    difficult than their SPMs counterparts. A sample RPM generated with the Sandia
    software is shown in Fig. [4a](#S2.F4.sf1 "In Figure 4 ‣ 2.2 Automatic generation
    of RPMs ‣ 2 Raven’s Progressive Matrices ‣ Deep Learning Methods for Abstract
    Visual Reasoning: A Survey on Raven’s Progressive Matrices").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Synthetic RPMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another approach represents abstract RPM structure using first-order logic
    and formulates sampling restrictions that allow to construct only valid RPMs [[14](#bib.bib14)],
    such as those presented in Fig. [4b](#S2.F4.sf2 "In Figure 4 ‣ 2.2 Automatic generation
    of RPMs ‣ 2 Raven’s Progressive Matrices ‣ Deep Learning Methods for Abstract
    Visual Reasoning: A Survey on Raven’s Progressive Matrices"). Moreover, the authors
    conducted a user study to validate whether the constructed matrices differ from
    the original set of SPMs proposed in [[7](#bib.bib7)]. The experiment revealed
    that the generated problems are statistically indistinguishable from the manually
    designed matrices among a group of 24 respondents.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 D-set and G-set
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another suite of automatically generated RPMs was proposed in [[15](#bib.bib15)].
    The D-set and G-set datasets have been generated based on design principles from
    prior works [[66](#bib.bib66), [5](#bib.bib5)]. The resultant matrices, with an
    example shown in Fig. [4c](#S2.F4.sf3 "In Figure 4 ‣ 2.2 Automatic generation
    of RPMs ‣ 2 Raven’s Progressive Matrices ‣ Deep Learning Methods for Abstract
    Visual Reasoning: A Survey on Raven’s Progressive Matrices"), although structurally
    similar to those from the Sandia suite, represented a different feature distribution.
    Namely, the authors of [[15](#bib.bib15)] utilised a completely different set
    of object shapes and supported full ranges for object attributes (shading, rotation
    and size) as compared to discrete values used in Sandia. This broader RPM configuration
    was realised in D-set, whereas G-set was curated to resemble the feature distributions
    found in Sandia. Thanks to synthesizing their own RPMs, the authors were able
    to use Sandia matrices as an additional evaluation dataset for verifying out-of-distribution
    generalisation of the proposed DeepIQ system in a transductive transfer learning
    setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4 PGM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Machine performance on the above-introduced datasets [[15](#bib.bib15), [13](#bib.bib13)]
    may be misleading, as the models are trained on large sets of matrices with similar
    structure to those found in the testing suite. In order to better evaluate the
    generalisation capabilities of DL models the PGM dataset [[12](#bib.bib12)] was
    proposed which arranges problems into 8 generalisation regimes with variable difficulty.
    To achieve this goal, each matrix from PGM (see example in Fig. [4d](#S2.F4.sf4
    "In Figure 4 ‣ 2.2 Automatic generation of RPMs ‣ 2 Raven’s Progressive Matrices
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices")) has an associated representation of the abstract structure, i.e. a
    set of triples $\mathcal{S}=\{[r,o,a]\ |\ r\in\mathcal{R},0\in\mathcal{O},a\in\mathcal{A}\}$,
    where $\mathcal{R}$ $=$ $\{\texttt{progression},$ $\texttt{XOR},$ $\texttt{OR},$
    $\texttt{AND},$ $\texttt{consistent union}\}$ defines the set of rules, $\mathcal{O}$
    $=$ $\{\texttt{shape},$ $\texttt{line}\}$ the set of objects and $\mathcal{A}$
    $=$ $\{\texttt{size},$ $\texttt{type},$ $\texttt{color},$ $\texttt{position},$
    $\texttt{number}\}$ the set of attributes. Based on these structures the dataset
    arranges RPM instances intro training, validation and test splits, such that both
    train and validation parts have matrices with the same structures, whereas test
    matrices are governed by relations not seen in the train and validation sets.
    Although being a perfect test bed for measuring generalisation in DL models, the
    dataset is characterised with an enormous size (each regime contains $1\,420\,000$
    RPMs, where $1.2$M belong to the training split) that is often a bottleneck for
    evaluating multiple models.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.5 RAVEN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another RPM dataset named RAVEN [[16](#bib.bib16)] aims to present a visually
    broader matrices with hierarchical structure (see Fig. [4e](#S2.F4.sf5 "In Figure
    4 ‣ 2.2 Automatic generation of RPMs ‣ 2 Raven’s Progressive Matrices ‣ Deep Learning
    Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")).
    This is achieved with the help of Attributed Stochastic Image Grammar (A-SIG) [[67](#bib.bib67),
    [68](#bib.bib68), [69](#bib.bib69)]. RAVEN contains matrices belonging to 7 visual
    configurations and similarly to PGM each RPM from RAVEN has an associatied abstract
    structure. However, in this case the structure is defined as a set of pairs $\mathcal{S}$
    $=$ $\{[r,a]$ $|$ $r\in\mathcal{R},$ $a\in\mathcal{A}\}$, where $\mathcal{R}$
    $=$ $\{\texttt{constant},$ $\texttt{progression},$ $\texttt{arithmetic},$ $\texttt{distribute
    three}\}$ defines the set of rules and $\mathcal{A}$ $=$ $\{\texttt{number},$
    $\texttt{position},$ $\texttt{type},$ $\texttt{size},$ $\texttt{color}\}$ the
    set of attributes. In contrast to PGM, the dataset contains supplementary structural
    annotations thanks to A-SIG that connects visual and structural representations.
    Moreover, Zhang et al. [[16](#bib.bib16)] have carried out a human evaluation
    on RAVEN matrices which allows to better assess the performance of DL approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is composed of $42\,000$ training RPMs, and additional $2\times
    14\,000$ problems allocated for validation and testing splits, respectively. In
    comparison to PGM, RAVEN’s matrices have a few times higher average number of
    rules: 6.29 vs 1.37. At the same time, PGM is better-suited for evaluating out-of-distribution
    generalisation due to defining explicit generalisation regimes.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.6 I-RAVEN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although a follow-up work reported superhuman performance on RAVEN [[20](#bib.bib20)],
    it was later revealed that such impressive results may arise from a shortcut solution
    due to biased answer sets. In fact, the problem of shortcut learning is prevalent
    in visual reasoning research and was identified across multiple related problems [[70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73)]. In the case of RAVEN, a
    context-blind model—one which processes only the answer panels and discards context
    ones—was shown to achieve close to perfect performance, bypassing the need for
    discovering abstract rules that govern the matrices. It was brought to light that
    correct answer to RPMs from RAVEN may be obtained by selecting answer panel with
    the most common attributes [[17](#bib.bib17)]. In order to fix this defect, the
    I-RAVEN dataset [[17](#bib.bib17)] was proposed that generates the set of answers
    with an iterative tree-based method. Context-blind models trained on such impartial
    dataset were shown to produce classifications equivalent to random guessing [[33](#bib.bib33),
    [17](#bib.bib17)], in effect demonstrating validity of this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.7 RAVEN-FAIR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similarly to I-RAVEN, RAVEN-FAIR [[18](#bib.bib18)] promises to solve the problem
    of biased choice panels of the original dataset. However, as noted in the supplementary
    material of [[18](#bib.bib18), Table 4], a Context-blind ResNet model scores 17.2%
    accuracy on the proposed dataset. At the same time, the same context-blind model
    evaluated on I-RAVEN scores 12.5%, which is equal to the random guess accuracy.
    This indicates that although RAVEN-FAIR is unquestionably less biased than the
    original RAVEN, not all bias sources were mitigated. Therefore, among three RAVEN-type
    datasets, it is recommended to use the unbiased I-RAVEN when testing RPM reasoning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Learning to solve RPMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the preliminary attempts [[11](#bib.bib11), [12](#bib.bib12)], multiple
    distinct approaches have been proposed for training DL models to solve RPMs, which
    are summarised in this section. Assume that an RPM instance $\mathcal{P}=(X,y)$
    has $16$ panels divided equally into context and answer images, where $X=\{x_{i}\}_{i=1}^{16}$
    is a set of all $16$ images and $y\in\{1,\ldots,8\}$ is an index of the correct
    answer. Moreover, we denote by $X_{c}=\{x_{i}\}_{i=1}^{8}\subset X$ the set of
    context panels, and by $X_{a}=\{x_{i}\}_{i=9}^{16}=\{a_{j}\}_{j=1}^{8}\subset
    X$ the set of answer panels (a.k.a candidate or choice panels). The above number
    of $8$ answer panels holds for RPMs from Sandia suite [[13](#bib.bib13)], for
    synthetic RPMs [[14](#bib.bib14)], for matrices from PGM [[12](#bib.bib12)], RAVEN [[16](#bib.bib16)],
    I-RAVEN [[17](#bib.bib17)], and RAVEN-FAIR [[18](#bib.bib18)], whereas RPMs used
    in [[15](#bib.bib15)] have 3 possible answers less. Let us also denote by $X_{c\cup
    a_{j}}=\{x_{i}\}_{i=1}^{8}\cup\{a_{j}\}\subset X$ an RPM with the missing panel
    completed by an answer panel with index $j$.
  prefs: []
  type: TYPE_NORMAL
- en: Consider an RPM reasoning model $\mathcal{N}(X)=\{h_{j}\}_{j=1}^{8}$, that given
    the RPM panels $X$ produces embedding vectors $\{h_{j}\}_{j=1}^{8}$, one for each
    of the answer panels, where $h_{j}\in\mathbb{R}^{d}$ for $d\geq 1$. Hereinafter
    we will refer to $\{h_{j}\}_{j=1}^{8}$ as candidate embeddings. Model $\mathcal{N}$
    can be implemented as any differentiable function – concrete examples from the
    literature are discussed in the following section. Based on the provided definitions
    let us now focus on the proposed schemes for learning to solve RPMs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Supervised training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the supervised training, the model is trained to predict an index of the
    answer panel which correctly completes the matrix. For this purpose, several works
    (e.g. [[11](#bib.bib11), [12](#bib.bib12), [15](#bib.bib15), [16](#bib.bib16)])
    employ a scoring module $\psi(h)=s\in\mathbb{R}$, which produces a single logit
    $s$ for each candidate embedding. Although in practice $\psi$ is often implemented
    as a multi-layer network (e.g. MLP in [[11](#bib.bib11), [15](#bib.bib15)] or
    Relation Network [[74](#bib.bib74)] in [[12](#bib.bib12)]), we consider these
    modules as part of $\mathcal{N}$ and consider the scoring module in the form of
    a simple linear layer with learnable weights. The supervised setup gathers individual
    logits into a set $\mathcal{S}=\{s_{j}\}_{j=1}^{8}$ and converts it to a probability
    distribution over the set of possible answers for an RPM $\mathcal{P}$, with $p(\mathcal{P})=\{p(\mathcal{P})_{j}\}_{j=1}^{8}=\text{softmax}(\mathcal{S})$.
    Using the estimated probability, the scoring module $\psi$ is optimised together
    with the base network $\mathcal{N}$ with a standard cross-entropy loss function.
    That is, for a batch of RPMs $\{\mathcal{P}_{i}\}_{i=1}^{N}$, where $N$ is the
    batch size, the following objective is minimised:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}^{\text{ce}}=-\frac{1}{N}\sum_{i=1}^{N}p(\mathcal{P}_{i})\log
    q(\mathcal{P}_{i})$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $q(\mathcal{P}_{i})=\text{onehot}(y_{i})$ is the one-hot encoded index
    of the correct answer for $\mathcal{P}_{i}$. The choice panel corresponding to
    the highest probability is considered as an answer chosen by the network, i.e.
    $\hat{y}=\text{argmax}_{j}\{p(\mathcal{P})_{j}\}_{j=1}^{8}$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Auxiliary training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to solve an RPM, one has to first recognise its abstract structure
    which governs the objects, attributes and relations present in the images. It
    was shown in [[12](#bib.bib12)] that training neural networks to justify their
    answer by predicting such an abstract structure leads to better performance in
    the final classification task. For this purpose Barrett et al. [[12](#bib.bib12)]
    proposed to redefine RPMs as triples $\mathcal{P}=(X,R,y)$, where $R\subset\mathcal{R}$
    defines the set of underlying abstract rules governing the RPM. The set of all
    abstract rules $\mathcal{R}$ is defined dependent on the dataset, as well as the
    maximal number of rules $n_{R}$ per RPM ($1\leq n_{R}\leq 4$ for PGM and $1\leq
    n_{R}\leq 8$ for RAVEN and its derivatives). Barrett et al. [[12](#bib.bib12)]
    proposed to encode PGM abstract rules with a multi-hot encoding and employ a rule
    prediction head $\rho(\sum_{j=1}^{8}h_{j})=\widehat{R}$, for estimating the set
    of RPM abstract rules, in the form of a vector $\widehat{R}$ of fixed length.
    The prediction $\widehat{R}$ was activated with a sigmoid function. A binary cross-entropy
    loss function $\mathcal{L}^{\text{aux}}$ was used to compare it with the ground-truth
    encoded rule representation $R$. During training a joint loss function $\mathcal{L}=\mathcal{L}^{\text{ce}}+\beta\mathcal{L}^{\text{aux}}$
    was minimised, where $\beta$ was a balancing coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Similar approach was validated on the RAVEN dataset in [[16](#bib.bib16)], where
    besides the rule related auxiliary target the authors proposed another loss function
    related to the prediction of RPM structure. Surprisingly, after training models
    with these supplementary objectives, their performance deteriorated. Analogous
    conclusions were drawn in multiple follow-up works [[20](#bib.bib20), [17](#bib.bib17),
    [33](#bib.bib33), [36](#bib.bib36)] that evaluated their approaches on RAVEN and
    its derivatives. However, in [[19](#bib.bib19)] we have shown that this inferior
    performance can be overcome by replacing the rule encoding method with the sparse
    encoding—an alternative rule representation based on one-hot encoding—that provides
    a more accurate training signal.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Contrastive training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ability to juxtapose correct and wrong answers was identified as a key component
    in adaptive problem solving across cognitive literature [[75](#bib.bib75), [76](#bib.bib76),
    [77](#bib.bib77)]. A number of works have proposed to incorporate such contrastive
    mechanisms either directly in the model architecture [[20](#bib.bib20)] or in
    the objective function [[20](#bib.bib20), [19](#bib.bib19), [21](#bib.bib21),
    [17](#bib.bib17)]. Zhang et al. [[20](#bib.bib20)] formulate an alternative to
    the supervised training setup by substituting cross-entropy with a variant of
    NCE loss that encourages contrast effects. The authors argue for shifting the
    view of solving RPMs from a classification task to ranking, where answer panels
    are ranked according to their probability of correctly completing the matrix.
    NCE-based objective functions are further utilised in the following works.
  prefs: []
  type: TYPE_NORMAL
- en: Hu et al. [[17](#bib.bib17)] employ a hierarchical model to generate embeddings
    of all possible pairs of RPM rows (and optionally columns). In effect, an embedding
    of the first two RPM rows (a so-called dominant embedding) is obtained, as well
    as embeddings of pairs of rows containing the last row completed by one of the
    answer panels (let us call it the candidate row pair embeddings). Then, an NCE-inspired
    $(N+1)$-tuplet loss [[78](#bib.bib78)] is used to maximise the similarity of the
    dominant embedding to the candidate row pair embeddings completed by the correct
    answer. At the same time, the loss function minimises the similarity of the dominant
    embedding to the candidate row pair embeddings completed by each of the wrong
    answers.
  prefs: []
  type: TYPE_NORMAL
- en: Another work casts the problem of solving RPMs into a multi-label classification
    framework and proposes the Multi-Label Contrastive Loss [[19](#bib.bib19)] – a
    contrastive objective function which builds on the Supervised Contrastive Loss [[79](#bib.bib79)].
    The authors propose a pre-training objective which builds similar representations
    to RPMs with common abstract structure and different representations for unrelated
    RPMs. In contrast to [[17](#bib.bib17)], the proposed method considers embeddings
    of the whole RPM context completed by an answer panel instead of the pairs of
    rows. Moreover, the method is combined with auxiliary training with sparse encoding
    into a joint learning method called Multi-Label Contrastive Learning (MLCL).
  prefs: []
  type: TYPE_NORMAL
- en: 'Meta-Analogical Contrastive Learning [[21](#bib.bib21)], similarly to MLCL,
    improves the efficacy of learning relational representations of AVR tasks by maximising
    similarity between analogical structural relations and minimising similarity between
    non-analogical ones. The work defines multiple analogy types: 1) intra-problem
    analogy draws an analogy between the original RPM context panels and its perturbation
    obtained via replacing randomly selected panel with noise, 2) inter-problem analogy
    collates two RPM instances with the same abstract structures but different attribute
    values and 3) non-analogy randomly shuffles the RPM context panels in order to
    break the abstract structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Learning with an optimal trajectory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [[22](#bib.bib22)] the authors recognise distracting features—image objects
    whose attributes change at random—as the main challenge in learning to solve RPMs.
    As a possible solution Feature Robust Abstract Reasoning (FRAR) [[22](#bib.bib22)]
    is proposed, which mitigates the impact of distracting features by means of a
    carefully designed learning trajectory based on a student-teacher reinforcement
    learning approach. Moreover, the paper conducts a wide study of optimal learning
    trajectory approaches including Curriculum learning [[80](#bib.bib80)], Self-paced
    learning [[81](#bib.bib81)], Learning to teach [[82](#bib.bib82)], Hard example
    mining [[83](#bib.bib83)], Focal loss [[84](#bib.bib84)] and Mentornet-PD [[85](#bib.bib85)].
    The experiments reveal that learning to solve RPMs is most effective with the
    proposed FRAR method.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8356bf4cf7361816c759194ff4f0995e.png)![Refer to caption](img/70fe1d0661e08f6c144c3acb5d57e9fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: RPM augmentation. The first row presents single rows from two I-RAVEN
    matrices with configurations 2x2Grid (left) and 3x3Grid (right). Rows 2–4 demonstrate
    image-level augmentations used in [[19](#bib.bib19)], whereas the last row illustrates
    structural perturbations applied in [[21](#bib.bib21)].'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Data augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data augmentation methods were shown to be of critical importance in applying
    DL across diverse domains [[86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88),
    [89](#bib.bib89)]. Similar conclusions were drawn in the field of AVR, where RPM
    data augmentation have proven to enhance abstract reasoning capabilities of neural
    learning models [[19](#bib.bib19), [21](#bib.bib21)]. In [[19](#bib.bib19)] it
    is proposed to use simple image transformations including horizontal/vertical
    flip, horizontal/vertical roll, shuffle 2x2/3x3, rotation and transposition, as
    illustrated in Fig. [5](#S3.F5 "Figure 5 ‣ 3.4 Learning with an optimal trajectory
    ‣ 3 Learning to solve RPMs ‣ Deep Learning Methods for Abstract Visual Reasoning:
    A Survey on Raven’s Progressive Matrices"), and apply them consistently to all
    panels of a given RPM. The authors have shown that data augmentation boosts the
    RPM solving performance irrespective of the chosen model and training setup. The
    topic of data augmentation was explored in parallel in [[21](#bib.bib21)], where
    the authors propose to shuffle the context panels of an RPM in order to break
    its abstract structure and use such modified instances as negative pairs in contrastive
    learning or to replace selected RPM panels with noise. These two approaches mainly
    differ in that [[19](#bib.bib19)] relies on image-level transformations, whereas [[21](#bib.bib21)]
    modifies the RPM structure by rearranging or replacing its panels.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Disentangled representations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recent studies have shown that disentangled representations are helpful for
    abstract reasoning tasks and can improve sample-efficiency [[90](#bib.bib90)].
    Specifically, it was demonstrated that a perception backbone of WReN [[12](#bib.bib12)]
    pre-trained as a disentangled Variational Autoencoder ($\beta$-VAE) [[91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93)] leads to better generalisation capabilities
    on the downstream task of solving RPMs than the same architecture trained in a
    purely supervised manner [[23](#bib.bib23)]. Moreover, autoencoders were successfully
    utilised in [[15](#bib.bib15)] for learning transferable features in simple AVR
    tasks, whereas the authors of [[21](#bib.bib21)] incorporated an autoencoder as
    part of their architecture for creating so-called generative analogies.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Generative modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another work [[24](#bib.bib24)] employs variational autoencoders for the purpose
    of training an effective generative model [[94](#bib.bib94), [95](#bib.bib95),
    [96](#bib.bib96)] capable of producing probable answer panels for RPMs. The method
    combines multiple components responsible for: 1) reconstruction of an answer image
    with a variational autoencoder, 2) predicting an index of the correct answer (supervised
    training) and the representation of abstract rules (auxiliary training), and 3)
    generating a new possible answer based on the latent embedding from VAE and the
    latent embedding of the recognition pathway. Such multi-task network was shown
    to be capable of generating plausible answer panels that preserve the underlying
    abstract rules and being competitive with models trained only with the supervised
    training.'
  prefs: []
  type: TYPE_NORMAL
- en: An alternative RPM answer generation approach was proposed in [[26](#bib.bib26)],
    where a deep latent variable model that utilises multiple Gaussian processes was
    constructed. The resultant method was shown to be interpretable via concept-specific
    latent variables and produced high-quality RPM panels. While both works [[24](#bib.bib24),
    [26](#bib.bib26)] train generative models using RPM panels, it was also demonstrated
    that an image generator pre-trained on real-world images may as well be effective
    in producing valid RPM panels [[25](#bib.bib25)].
  prefs: []
  type: TYPE_NORMAL
- en: Different from the above works, the Probabilistic Abduction and Execution (PrAE)
    model [[27](#bib.bib27)] is able to construct a probabilistic RPM representation
    and use it to generate answers to RPMs with a scene inference engine. In contrast
    to the previous end-to-end generative approaches, PrAE decouples the generation
    process into a neural perception backbone and a symbolic logical reasoning engine.
    In spite of this separation, the method can be optimised end-to-end with REINFORCE [[97](#bib.bib97)]
    and showcases the applicability of neuro-symbolic approaches to solving RPMs within
    a generative process.
  prefs: []
  type: TYPE_NORMAL
- en: 3.8 Unsupervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A couple of recent approaches to solving RPMs investigate whether useful representations
    can be learned in an usupervised manner. In this setup the model does not take
    into account information about the correct answer in the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Geared towards obtaining an effective RPM solving model in the unsupervised
    setting, Noisy Contrast and Decentralization (NCD) [[30](#bib.bib30)] considers
    10 rows obtained from each RPM (first and second rows together with the third
    row completed by each of the 8 answer panels). Next the method considers a binary
    classification task in which the model predicts which rows belong to RPM context
    and which are formed by completing the third row with one of the answers. During
    inference, NCD generates probabilities only for the 8 versions of the last row
    and outputs an index of the highest scoring candidate as the answer. The method
    builds upon Multi-label Classification with Pseudo Targets (MCPT) [[28](#bib.bib28)]
    proposed by the authors of NCD in their earlier work.
  prefs: []
  type: TYPE_NORMAL
- en: Another unsupervised approach, the Pairwise Relations Discriminator (PRD), is
    proposed in [[29](#bib.bib29)]. The method trains an underlying model to solve
    the task of discriminating between positive and negative pairs, similarly to other
    contrastive approaches (e.g. [[17](#bib.bib17)]). Positive pairs are formed by
    taking the first two rows from a given RPM, say $\mathcal{P}$. The negative pairs
    are obtained in several ways. First option is to pair one of the the first two
    rows from $\mathcal{P}$ with another row from a different RPM. Alternatively,
    the method randomly selects the first or the second row as $r$ and considers the
    other one as $r^{\prime}$. Then, the third panel of $r^{\prime}$ is replaced by
    one of the answers to form a negative pair with $r$. Analogously, the third row
    may be taken with its third panel replaced by the third image from $r^{\prime}$
    and considered a negative pair with $r$. Next, PRD maximises the similarity of
    elements within positive pairs and minimises the similarity of items within the
    negative pairs. To obtain an answer for $\mathcal{P}$, the method fills in the
    third row with each answer and calculates the average similarity between the completed
    row and the first two rows. The choice panel corresponding to the completed row
    with the highest similarity is considered as the answer.
  prefs: []
  type: TYPE_NORMAL
- en: 4 RPM Deep Learning Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c5c8cd165ab546077ba0c258df8f242d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: RPM-like problem from [[11](#bib.bib11)]. The task verifies the ability
    to identify progression rule applied to triangle rotation. Correct answer is highlighted
    with a green bounding box.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the first DL models for solving RPMs was proposed by Hoshen and Werman
    [[11](#bib.bib11)] who constructed a convolutional neural network (CNN) [[98](#bib.bib98)]
    for solving geometric pattern recognition problems, capable of generating images
    according to a specified pattern. Moreover, an additional network based on the
    same visual backbone was employed to solve problems involving rotation, reflection,
    color, size and shape of the patterns (see Fig. [6](#S4.F6 "Figure 6 ‣ 4 RPM Deep
    Learning Models ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey
    on Raven’s Progressive Matrices")). CNNs were further utilised in [[99](#bib.bib99)],
    where a generalised similarity-based approach to solving RPMs from the Sandia
    suite [[13](#bib.bib13)] (cf. Fig. [4a](#S2.F4.sf1 "In Figure 4 ‣ 2.2 Automatic
    generation of RPMs ‣ 2 Raven’s Progressive Matrices ‣ Deep Learning Methods for
    Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")) was presented.
    The method eliminates the need for structure mapping and instead relies on feature-based
    processing using relational and non-relational features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another feature-centric approach is discussed in [[15](#bib.bib15)], where
    an autoencoder with dense fully-connected layers is first trained to learn feature-based
    representations and then combined with an ensemble of shallow multilayer perceptrons
    (MLPs). On top, a scoring module is proposed that can be adjusted to the final
    downstream task. Mańdziuk and Żychowski [[15](#bib.bib15)] have shown that this
    approach facilitates transfer learning, by applying the model trained to solve
    RPMs (cf. Fig. [4c](#S2.F4.sf3 "In Figure 4 ‣ 2.2 Automatic generation of RPMs
    ‣ 2 Raven’s Progressive Matrices ‣ Deep Learning Methods for Abstract Visual Reasoning:
    A Survey on Raven’s Progressive Matrices")) to other problems with a similar input
    distribution, e.g. the odd-one-out tasks (see Fig. [7](#S4.F7 "Figure 7 ‣ 4 RPM
    Deep Learning Models ‣ Deep Learning Methods for Abstract Visual Reasoning: A
    Survey on Raven’s Progressive Matrices")). Although the above works have demonstrated
    that classic neural network architectures are capable of possessing abstract visual
    reasoning skills, the approaches were evaluated on visually-simple RPM benchmarks
    with limited possibility to verify out-of-distribution generalisation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d080ab0c98d3cfcd82ede1ef6fd81f34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Odd-one-out matrix. Example of an odd-one-out problem from [[15](#bib.bib15)].
    The task is to point the mismatched element out of several choices. In the example
    it is a triangle (all others are trapezoids) - marked with a red boundary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following sections we focus on consecutive works that considered RPMs
    with compositional structure and tackled o.o.d. generalisation challenges. A high-level
    overview of DL models proposed in these works is presented in Table [I](#S4.T1
    "TABLE I ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods for Abstract Visual
    Reasoning: A Survey on Raven’s Progressive Matrices"). Besides baseline methods,
    we categorise the followup models into two classes: relational reasoning networks
    and hierarchical networks. Models of the first type are inherently based on the
    Relation Network [[74](#bib.bib74)], whereas those of the second type, inspired
    by the hierarchical nature of RPMs, inject various structural inductive biases
    into network architectures used to solve RPMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Model overviews. A summary of key design choices adopted in RPM solving
    models. *Dataset* column shows the works in which a given (model, dataset) pair
    was included in the experiments. In the *Training setup* column, *Target cross-entropy
    loss* refers to the supervised training approaches, whereas *Meta-target binary
    cross-entropy loss* refers to the auxiliary training settings.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Dataset | Input | Design highlight | Training setup |'
  prefs: []
  type: TYPE_TB
- en: '| Baselines [[12](#bib.bib12), [16](#bib.bib16)] | PGM [[12](#bib.bib12)] RAVEN [[16](#bib.bib16)]
    I-RAVEN [[17](#bib.bib17)] | Single panels | Embeds single panels with a shallow
    CNN or ResNet-18 and aggregates information from RPM context panels with an MLP
    or LSTM. | Target cross-entropy loss Meta-target binary cross-entropy loss |'
  prefs: []
  type: TYPE_TB
- en: '| Context-blind models [[12](#bib.bib12)] | PGM [[12](#bib.bib12)] RAVEN [[33](#bib.bib33),
    [17](#bib.bib17)] I-RAVEN [[33](#bib.bib33), [17](#bib.bib17)] | Single panels
    | Reason only about answer panels and do not rely on RPM context. | Target cross-entropy
    loss |'
  prefs: []
  type: TYPE_TB
- en: '| Wild ResNet [[12](#bib.bib12)] | PGM [[12](#bib.bib12)] I-RAVEN [[17](#bib.bib17)]
    | Context | Uses ResNet on a stack of 9 context panels. | Target cross-entropy
    loss |'
  prefs: []
  type: TYPE_TB
- en: '| WReN [[12](#bib.bib12)] | PGM [[12](#bib.bib12)] RAVEN [[16](#bib.bib16)]
    I-RAVEN [[17](#bib.bib17)] | Single panels | Aggregates information from pairs
    of RPM context panels with the RN. | Target cross-entropy loss Meta-target binary
    cross-entropy loss |'
  prefs: []
  type: TYPE_TB
- en: '| VAE-WReN [[23](#bib.bib23)] | PGM [[12](#bib.bib12)] | Single panels | Replaces
    the CNN panel encoder of WReN with a disentangled $\beta$-VAE trained separately
    from the WReN model. | Target cross-entropy loss Meta-target binary cross-entropy
    loss Modified ELBO objective [[93](#bib.bib93)] |'
  prefs: []
  type: TYPE_TB
- en: '| ARNe [[31](#bib.bib31)] | PGM [[31](#bib.bib31)] RAVEN [[31](#bib.bib31)]
    | Single panels | Extends the WReN model with the Transformer located between
    panel embedding component and the RN module. | Target cross-entropy loss Meta-target
    binary cross-entropy loss |'
  prefs: []
  type: TYPE_TB
- en: '| CoPINet [[20](#bib.bib20)] | PGM [[20](#bib.bib20)] RAVEN [[20](#bib.bib20)]
    I-RAVEN [[17](#bib.bib17)] | Single panels | Introduces an explicit permutation-invariant
    contrasting neural module for distinguishing features between answer panels. |
    Variant of NCE loss [[20](#bib.bib20)] |'
  prefs: []
  type: TYPE_TB
- en: '| LEN [[22](#bib.bib22)] | PGM [[22](#bib.bib22)] RAVEN [[22](#bib.bib22)]
    I-RAVEN [[17](#bib.bib17)] | Single panels, Context$\setminus a_{k}$ | Aggregates
    information from triples of RPM context panels and a context embedding vector
    with the RN. | Target cross-entropy loss Meta-target binary cross-entropy loss
    Feature Robust Abstract Reasoning |'
  prefs: []
  type: TYPE_TB
- en: '| SRAN [[17](#bib.bib17)] | PGM [[17](#bib.bib17)] I-RAVEN [[17](#bib.bib17)]
    | Single panels, Rows, Cols, Pairs of rows Pairs of cols | Gradually aggregates
    features from panel hierarchies with a gated embedding fusion module. | ($N$+$1$)-tuplet
    loss [[78](#bib.bib78)] |'
  prefs: []
  type: TYPE_TB
- en: '| MXGNet [[36](#bib.bib36)] | PGM [[36](#bib.bib36)] RAVEN [[36](#bib.bib36)]
    | Single panels | Reasons about inter-panel relations with multiplex graph neural
    networks. | Target cross-entropy loss Meta-target binary cross-entropy loss |'
  prefs: []
  type: TYPE_TB
- en: '| SCL [[33](#bib.bib33)] | PGM [[33](#bib.bib33), [19](#bib.bib19)] RAVEN [[33](#bib.bib33)]
    I-RAVEN [[33](#bib.bib33), [19](#bib.bib19)] | Single panels | Reasons about inter-panel
    and intra-panel relations with a neural module based on a scattering transformation.
    | Target cross-entropy loss |'
  prefs: []
  type: TYPE_TB
- en: '| DCNet [[35](#bib.bib35)] | PGM [[35](#bib.bib35)] RAVEN [[35](#bib.bib35)]
    | Rows Cols | Considers similarity of third row/col to the first and second ones
    and differences in the last row completed by each answer panel. | Target binary
    cross-entropy loss |'
  prefs: []
  type: TYPE_TB
- en: '| MRNet [[18](#bib.bib18)] | PGM [[18](#bib.bib18)] RAVEN [[18](#bib.bib18)]
    RAVEN-FAIR [[18](#bib.bib18)] | Single panels | Applies separate RNs to panel
    embeddings in three different resolutions. | Weighted target binary cross-entropy
    loss |'
  prefs: []
  type: TYPE_TB
- en: '| Rel-Base [[34](#bib.bib34)] | PGM [[34](#bib.bib34)] RAVEN [[34](#bib.bib34)]
    | Single panels | Processes a stack of 9 context panel embeddings with a 1D convolution
    module. | Target cross-entropy loss |'
  prefs: []
  type: TYPE_TB
- en: '| Rel-AIR [[34](#bib.bib34)] | PGM [[34](#bib.bib34)] RAVEN [[34](#bib.bib34)]
    | Single panels | Reasons about segmented objects obtained with AIR unsupervised
    scene decomposition model. | Target cross-entropy loss Unsupervised scene decomposition
    pre-training |'
  prefs: []
  type: TYPE_TB
- en: '| MLRN [[32](#bib.bib32)] | PGM [[32](#bib.bib32)] | Single panels | Extends
    WReN with a Multi-Layer Relation Network, encodes the input images with a Magnitude
    Encoding and uses the LAMB optimiser. | Target cross-entropy loss |'
  prefs: []
  type: TYPE_TB
- en: '| PrAE [[27](#bib.bib27)] | RAVEN [[27](#bib.bib27)] | Single panels | Neuro-symbolic
    approach that produces a probabilistic RPM representation and uses it to generate
    probable answer via probabilistic abduction and execution. | REINFORCE [[97](#bib.bib97)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| NI [[37](#bib.bib37)] | PGM [[37](#bib.bib37)] | Single panels | Compositional
    model comprising reusable self-attention layers with a learnable routing mechanism.
    | Target cross-entropy loss |'
  prefs: []
  type: TYPE_TB
- en: 4.1 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similarly to [[11](#bib.bib11)], baseline models for solving RPMs are formed
    by using a CNN as a visual feature extractor, followed by a reasoning module.
    The perceptual backbone often consists of 4 convolution layers with non-linearities
    in-between or a variant of ResNet [[100](#bib.bib100)] with 18, 34 or 50 layers.
    In simple baseline models [[12](#bib.bib12), [16](#bib.bib16)], the visual component
    processes each image independently. Then, the features extracted by these visual
    modules from all matrix panels are either concatenated into a single vector and
    fed into an MLP or stacked to form a time-series and processed by an LSTM [[101](#bib.bib101)].
    Nonetheless, it was shown that such typical neural architectures struggle even
    in simpler RPM generalisation regimes [[12](#bib.bib12), [16](#bib.bib16)], which
    raises the need for dedicated AVR models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9f30012dab1e15ee5ac64f6d984b6f03.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Single panel.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da40cab5a2acd1d81528308bbcd5fc0e.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Single row.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a561bcd122574af54333140ada56dec0.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Single col.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2757597a7796fafb57c55092f7ec7939.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Pair of rows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/59c1fd0f110b4c70ca83f21d9eb89912.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Pair of Cols.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/54191e4be7499a07703e34147fea2134.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Context$\setminus a_{k}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1a48a7f6f369bf30693b726614309294.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) Context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: RPM panel hierarchies. Each hierarchy demonstrates how panels can
    be jointly processed. For instance, when building hierarchical perceptual model
    backbones (e.g. in SRAN [[17](#bib.bib17)]), panels are stacked on top of each
    other to form a matrix of the shape $(c\times w\times h)$, where $c$ is the number
    of panels in the given hierarchy, $w$ - the image width and $h$ - the image height.
    Alternatively, the embeddings of these panels can be combined in analogous manner
    (e.g. in LEN [[22](#bib.bib22)]) and concatenated into a single vector – a representation
    of given hierarchy. The hierachy which contains 8 RPM context images without the
    remaining missing panel is denoted as “Context$\setminus a_{k}$”, whereas the
    hierarchy with full RPM context where one of the answers is placed in the bottom-right
    panel as “Context”. Additional hierarchies may include diagonal panels (e.g. [[15](#bib.bib15),
    [36](#bib.bib36)]) or random combinations (e.g. [[22](#bib.bib22), [36](#bib.bib36)]).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Relational reasoning networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Seminal works that introduced challenging RPM reasoning benchmarks (PGM [[12](#bib.bib12)]
    and RAVEN [[16](#bib.bib16)]) have shown that baseline DL models generally lack
    the relational reasoning capability that is inherent to the AVR domain. Similar
    observations were noted in other (non AVR) problems involving relational reasoning
    that tried to tackle both artificial [[102](#bib.bib102), [74](#bib.bib74)] and
    real-world [[103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106),
    [107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111)] challenges. With the aim of equipping neural modules with
    relational reasoning abilities, Santoro et al. [[74](#bib.bib74)] proposed the
    Relation Network (RN) – a simple neural module for relational reasoning. RN arranges
    a set of objects $O$ into pairs (where an object $o\in O$ can be generally represented
    by any vector) and summarises the whole set of objects into a single descriptive
    representation with
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $RN(O)=f_{\phi}(\sum_{i,j}g_{\theta}(o_{i},o_{j}))$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $f_{\phi}$ and $g_{\theta}$ are functions typically implemented as MLPs.
    Santoro et al. [[74](#bib.bib74)] successfully applied RN to problems from diverse
    domains including VQA [[102](#bib.bib102)], text-based question answering [[104](#bib.bib104)]
    and reasoning about dynamic physical systems [[74](#bib.bib74)]. The module was
    later extended in several ways, e.g. to recurrent version capable of sequential
    relational reasoning [[112](#bib.bib112)], and its learning capacity was enhanced
    by stacking multiple layers [[113](#bib.bib113)]. As a follow-up to [[74](#bib.bib74)],
    in [[12](#bib.bib12)] the RN was integrated as part of an end-to-end architecture
    for solving RPMs – the Wild Relation Network (WReN). The model outperformed baselines
    by a significant margin and sparked interest within the AVR community in building
    models that employ RN.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple attempts have been made to either extend the originally proposed WReN
    model or to incorporate the RN into another end-to-end architecture. Steenbrugge
    et al. [[23](#bib.bib23)] introduced the VAE-WReN, which replaced the CNN backbone
    of WReN with a disentangled variational autoencoder [[91](#bib.bib91), [92](#bib.bib92),
    [93](#bib.bib93)] and showed improved generalisation when tested on PGM. The RN
    part of WReN was further extended to a multi-layer version [[113](#bib.bib113)].
    The Multi-Layer Relation Network (MLRN) combined with $\ell_{2}$-regularization,
    Magnitude Encoding and the LAMB optimiser [[114](#bib.bib114)] presented close
    to perfect performance in neutral PGM regime, however, its performance was worse
    than that of VAE-WReN in other generalisation regimes, suggesting that the model
    was prone to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by advances in psychology, which suggest that attention mechanisms
    play a crucial role in human visual reasoning capabilities, the Attention Relation
    Network (ARNe) was proposed in [[31](#bib.bib31)]. ARNe builds on WReN and equips
    it with attention mechanism borrowed from the Transformer [[115](#bib.bib115)].
    However, the increased model complexity was not fully justified, as the model
    demonstrated small improvements on PGM and performed worse than baselines on RAVEN.
    Nonetheless, Hahne et al. [[31](#bib.bib31)] have shown that after increasing
    RAVEN dataset size 5-fold, ARNe performance increased substantially, which suggests
    that the proposed attention mechanism for solving AVR tasks is promising, despite
    being inefficient w.r.t. the sample size. Possible directions for future work
    in this area may involve further research on incorporating the attention mechanism,
    which is additionally motivated by the impressive performance of attention-based
    models in other domains, e.g. [[115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117),
    [118](#bib.bib118)].
  prefs: []
  type: TYPE_NORMAL
- en: In the original WReN proposal [[12](#bib.bib12)], the relational module operated
    on pairs of panel embeddings. However, as described in [[5](#bib.bib5)], the rules
    in RPMs are inherently applied row- or column-wise. Therefore, some attempts have
    been made to incorporate this structural bias in the RN module. Benny et al. [[18](#bib.bib18)]
    proposed the MRNet, a model which first generates panel embeddings in three different
    resolutions and then processes them with distinct RN modules that consider triples
    of embeddings from each RPM row and column, respectively. In a similar spirit, Zheng
    et al. [[22](#bib.bib22)] introduced the Logic Embedding Network (LEN) – a model
    with several improvements to WReN in the context of solving RPMs. Firstly, the
    approach arranges RPM context panel embeddings into triples instead of pairs.
    The triples include row- and column-wise combinations (similarly to [[18](#bib.bib18)],
    although, only a single resolution is considered) processed by a neural module
    $g$ with parameters $\theta_{1}$. In contrast to [[18](#bib.bib18)], LEN additionally
    processes the remaining possible panel embedding combinations with a parallel
    module of the same structure as $g$, but with different parameters $\theta_{2}$.
    Moreover, with each triple a representation of the whole RPM context is concatenated.
    This representation is obtained with a supplementary CNN that processes a stack
    of 8 context panels. This structural inductive bias allows to reason about the
    relations between multiple panels more accurately and forms a basis for a group
    of hierarchical models discussed in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: At the intersection of relational reasoning and hierarchical networks, another
    model—the Contrastive Perceptual Inference Network (CoPINet) [[20](#bib.bib20)]—is
    located. It is a permutation-invariant approach with an explicit contrastive mechanism
    that helps to distinguish the correct RPM answer. CoPINet first extracts independent
    panel embeddings with a visual backbone and then iteratively applies the contrastive
    module capable of discovering features that differentiate among a set of choices.
    In contrast to RN that considers pairs of feature embeddings, CoPINet’s contrasting
    mechanism collates each object representation with an aggregated representation
    of all remaining objects. The initial object representation is obtained by summing
    outputs of the visual backbone for each image along RPM’s rows and columns, respectively.
    Next, these representations are iteratively refined using the contrast module.
    Although CoPINet was shown to possess impressive reasoning ability on matrices
    from RAVEN [[20](#bib.bib20)], it was later discovered that the model performs
    sub-par on the balanced version of this dataset (I-RAVEN) [[17](#bib.bib17)].
    Therefore, additional validation of CoPINet’s architecture in other AVR problems
    sets an interesting avenue for future work.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Hierarchical networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In multiple areas where DL models thrive, it has often been beneficial to incorporate
    domain-specific knowledge about the problem structure into the network architecture.
    Notable examples include making CNNs translation-invariant in problems with 2D
    images [[119](#bib.bib119)] or rotation-equivariant for spherical images [[120](#bib.bib120)].
    As demonstrated by the LEN model [[22](#bib.bib22)], such inductive biases are
    also helpful in solving RPMs, e.g. by specifying in what manner panel representations
    should be processed. Figure [8](#S4.F8 "Figure 8 ‣ 4.1 Baselines ‣ 4 RPM Deep
    Learning Models ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey
    on Raven’s Progressive Matrices") summarises the most common hierarchies utilised
    by RPM DL models. While LEN exploited a single row (Fig. [8b](#S4.F8.sf2 "In Figure
    8 ‣ 4.1 Baselines ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods for Abstract
    Visual Reasoning: A Survey on Raven’s Progressive Matrices")) and a single column
    (Fig. [8c](#S4.F8.sf3 "In Figure 8 ‣ 4.1 Baselines ‣ 4 RPM Deep Learning Models
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices")) hierarchies, respectively together with the 8 context panels (Fig. [8f](#S4.F8.sf6
    "In Figure 8 ‣ 4.1 Baselines ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")), subsequent
    DL approaches relied on additional techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While solving RPMs, it is often required not only to identify a set of rules
    that govern a single row/column but also to subsequently find an analogous set
    of rules applied to another row/column from the same matrix. Motivated by this
    observation, the Stratified Rule-Aware Network (SRAN) [[17](#bib.bib17)] devotes
    particular attention to pairs of rows (Fig. [8d](#S4.F8.sf4 "In Figure 8 ‣ 4.1
    Baselines ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods for Abstract Visual
    Reasoning: A Survey on Raven’s Progressive Matrices")) and pairs of columns (Fig. [8e](#S4.F8.sf5
    "In Figure 8 ‣ 4.1 Baselines ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")). From
    each such pair that includes 6 images, the panels are stacked on top of each other
    and processed by a dedicated convolutional pathway. SRAN gradually aggregates
    representations from consecutive hierarchies (single panel, single row/col, pair
    of rows/cols) using a gated embedding fusion module realised by an MLP.'
  prefs: []
  type: TYPE_NORMAL
- en: Such a gradual processing of latent features was also shown to be beneficial
    in the Scattering Compositional Learner (SCL) model [[33](#bib.bib33)]. Similarly
    to SRAN, SCL first computes embeddings for each RPM panel. Then, the matrix is
    iteratively completed by one of the answer panels. In each step, the model computes
    a joint representation of all 9 context images. The obtained embedding is finally
    fed to a scoring module which produces a probablity of correctness of the selected
    answer. In contrast to SRAN, SCL replaces MLP components with a scattering transformation
    that splits the input into multiple groups, applies the same neural module to
    each group, and merges the outputs into a single embedding. The approach shares
    some ideas with group convolution [[121](#bib.bib121)], ResNeXt [[122](#bib.bib122)]
    and Modular Networks [[123](#bib.bib123)].
  prefs: []
  type: TYPE_NORMAL
- en: Processing order similar to SCL was employed in Rel-Base and Rel-AIR models [[34](#bib.bib34)]
    that also start with building embeddings independently for each matrix panel.
    The models differ from SCL with the choice of the encoder network – instead of
    a combination of CNN and the scattering transformation they use shallow ResNet.
    Next, both models aggregate 9 context embeddings. However, in contrast to SCL,
    these representations are processed in Rel-Base/AIR models with a simple 1D convolution
    rather than the scattering transformation. In this view, Rel-Base and Rel-AIR
    relate to WReN which instead of 1D convolution uses more computationally expensive
    RN. Although the models proposed by Spratley et al. [[34](#bib.bib34)] are structurally
    similar, they operate on different inputs. Rel-Base processes original matrix
    panels, whereas Rel-AIR employs an unsupervised scene decomposition pre-processing
    step with the Attend-Infer-Repeat (AIR) [[124](#bib.bib124)] neural model. AIR
    decomposes each panel into several object slots that are used to compute a panel
    embedding. This allows to disentangle single objects from the overall scene and
    simplifies the process of discovering relations between them. Explicit scene representation
    is additionally proven to be useful in [[27](#bib.bib27)], where it is represented
    in a probabilistic manner. The authors combine this abstract scene representation
    with a scene inference engine to produce plausible answers to RPMs with a neuro-symbolic
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: In [[35](#bib.bib35)], similarly to SRAN, the authors further build on the importance
    of instantiating RPM rules row- and column-wise and propose a Dual-Contrast Network
    (DCNet). The model uses a rule contrast module that compares representations of
    the completed third row with the first and second row representations (resp. for
    columns) already present in the RPM. DCNet additionally incorporates a contrast
    module similar to that of CoPINet, which increases relative differences between
    the candidate embeddings. The model training algorithm resembles that of SRAN,
    where the similarity of the representation of the first two rows/columns from
    the RPM context to embeddings of row/column pairs including the correct answer
    panel is maximised, while the similarity to embeddings of row/column pairs with
    wrong answer panels is minimised.
  prefs: []
  type: TYPE_NORMAL
- en: RPM hierarchies were further exploited in [[36](#bib.bib36)], where the authors
    propose MXGNet – a deep graph neural network for solving AVR problems. In contrast
    to the already described approaches where structural inductive biases were hand-crafted,
    MXGNet builds on an adaptive mechanism which automatically selects key problem
    hierarchies depending on the task at hand. This is achieved by employing $\ell_{1}$-regularised
    gating variables that measure to which extent the panel subsets contribute to
    the model performance. The authors found out that indeed single row/column hierarchies
    are crucial for high model performance in solving RPMs.
  prefs: []
  type: TYPE_NORMAL
- en: Another unique approach was presented in [[37](#bib.bib37)], where the authors
    propose a method of dynamic inference with Neural Interpreters (NI). The model
    is composed of several reusable self-attention blocks with a learnable routing
    mechanism. The method is inspired by the design of programming languages and utilizes
    concepts analogous to scripts, functions, variables, and an interpreter. This
    perspective facilitates compositional reasoning, where model blocks can be reused
    across tasks. Another design highlight of the model lies in the input processing
    module – the model splits each RPM panel into smaller patches instead of taking
    the whole images as inputs. This division reduces input dimensionality, which
    allows to apply even computationally expensive modules – the authors employ the
    attention module, which computes pair-wise interactions between input elements
    (image pixels). The approach of splitting an image into patches was initially
    proposed in the Vision Transformer (ViT) [[117](#bib.bib117)], which Rahaman et al.
    [[37](#bib.bib37)] adapted to solve RPMs from PGM.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: PGM accuracy. Accuracy in all regimes of the PGM dataset [[12](#bib.bib12)]
    arranged in the ascending order by score on the test set of the Neutral regime.
    The Held-out Attribute Pairs regime is denoted as H.O. A.P., Held-out Triple Pairs
    as H.O. T.P., Held-out Triples as H.O. Triples, Held-out Attribute line-type as
    H.O. L-T and Held-out Attribute shape-colour as H.O. S-C.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Accuracy (%) |'
  prefs: []
  type: TYPE_TB
- en: '| Neutral | Interpolation | H.O. A.P. | H.O. T.P. | H.O. Triples | H.O. L-T
    | H.O. S-C | Extrapolation |'
  prefs: []
  type: TYPE_TB
- en: '| Val. | Test. | Val. | Test. | Val. | Test. | Val. | Test. | Val. | Test.
    | Val. | Test. | Val. | Test. | Val. | Test. |'
  prefs: []
  type: TYPE_TB
- en: '| Context-blind ResNet [[12](#bib.bib12)] | - | 22.4 | - | - | - | - | - |
    - | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CNN MLP [[12](#bib.bib12)] | - | 33.0 | - | - | - | - | - | - | - | - | -
    | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CNN LSTM [[12](#bib.bib12)] | - | 35.8 | - | - | - | - | - | - | - | - |
    - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-50 [[12](#bib.bib12)] | - | 42.0 | - | - | - | - | - | - | - | - |
    - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| NCD [[30](#bib.bib30)] | - | 47.6 | - | 47.0 | - | - | - | - | - | - | -
    | - | - | - | - | 24.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Wild-ResNet [[12](#bib.bib12)] | - | 48.0 | - | - | - | - | - | - | - | -
    | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CoPINet [[20](#bib.bib20)] | - | 56.4 | - | - | - | - | - | - | - | - | -
    | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| WReN $\beta=0$ [[12](#bib.bib12)] | 63.0 | 62.6 | 79.0 | 64.4 | 46.7 | 27.2
    | 63.9 | 41.9 | 63.4 | 19.0 | 59.5 | 14.4 | 59.1 | 12.5 | 69.3 | 17.2 |'
  prefs: []
  type: TYPE_TB
- en: '| VAE-WReN $\beta=4$ [[23](#bib.bib23)] | 64.8 | 64.2 | - | - | 70.1 | 36.8
    | 64.6 | 43.6 | 59.5 | 24.6 | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MXGNet $\beta=0$ [[36](#bib.bib36)] | 67.1 | 66.7 | 74.2 | 65.4 | 68.3 |
    33.6 | 67.1 | 43.3 | 63.7 | 19.9 | 60.1 | 16.7 | 68.5 | 16.6 | 69.1 | 18.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LEN $\beta=0$ [[22](#bib.bib22)] | - | 68.1 | - | - | - | - | - | - | - |
    - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DCNet [[35](#bib.bib35)] | - | 68.6 | - | 59.7 | - | - | - | - | - | - |
    - | - | - | - | - | 17.8 |'
  prefs: []
  type: TYPE_TB
- en: '| T-LEN $\beta=0$ [[22](#bib.bib22)] | - | 70.3 | - | - | - | - | - | - | -
    | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SCL MLCL [[19](#bib.bib19)] | 71.0 | 71.1 | 93.2 | 70.9 | 79.7 | 66.0 | 86.1
    | 71.7 | 84.0 | 22.1 | 86.1 | 16.1 | 94.1 | 12.8 | 83.0 | 21.9 |'
  prefs: []
  type: TYPE_TB
- en: '| SRAN [[17](#bib.bib17)] | - | 71.3 | - | - | - | - | - | - | - | - | - |
    - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ViT [[37](#bib.bib37)] | 73.3 | 72.7 | 89.9 | 67.7 | 69.4 | 34.1 | 67.6 |
    44.1 | 73.8 | 15.9 | - | - | - | - | 92.2 | 16.4 |'
  prefs: []
  type: TYPE_TB
- en: '| WReN $\beta=10$ [[12](#bib.bib12)] | 77.2 | 76.9 | 92.3 | 67.4 | 73.4 | 51.7
    | 74.5 | 56.3 | 80.0 | 20.1 | 78.1 | 16.4 | 85.2 | 13.0 | 93.6 | 15.5 |'
  prefs: []
  type: TYPE_TB
- en: '| NI [[37](#bib.bib37)] | 77.3 | 77.0 | 87.9 | 70.5 | 69.5 | 36.6 | 68.6 |
    45.2 | 79.9 | 20.0 | - | - | - | - | 91.8 | 19.4 |'
  prefs: []
  type: TYPE_TB
- en: '| WReN $\beta=10$ + TM[[22](#bib.bib22)] | - | 77.8 | - | - | - | - | - | -
    | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| LEN $\beta=10$ [[22](#bib.bib22)] | - | 82.3 | - | - | - | - | - | - | -
    | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| T-LEN $\beta=10$ [[22](#bib.bib22)] | - | 84.1 | - | - | - | - | - | - |
    - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Rel-Base [[34](#bib.bib34)] | - | 85.5 | - | - | - | - | - | - | - | - |
    - | - | - | - | - | 22.1 |'
  prefs: []
  type: TYPE_TB
- en: '| SCL CE [[19](#bib.bib19)] | 86.2 | 85.6 | 91.2 | 55.8 | 56.4 | 40.8 | 78.2
    | 64.5 | 78.6 | 27.0 | 87.6 | 15.1 | 96.9 | 12.7 | 96.3 | 17.3 |'
  prefs: []
  type: TYPE_TB
- en: '| LEN $\beta=10$ + TM[[22](#bib.bib22)] | - | 85.8 | - | - | - | - | - | -
    | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SCL AUX-dense [[19](#bib.bib19)] | 87.6 | 87.1 | 97.9 | 56.0 | 88.6 | 79.6
    | 88.7 | 76.6 | 88.1 | 23.0 | 87.9 | 14.1 | 98.3 | 12.6 | 99.1 | 19.8 |'
  prefs: []
  type: TYPE_TB
- en: '| SCL AUX-sparse [[19](#bib.bib19)] | 87.4 | 87.1 | 88.1 | 54.1 | 80.8 | 63.6
    | 77.5 | 64.0 | 86.0 | 30.8 | 93.5 | 17.0 | 98.0 | 12.7 | 96.9 | 17.5 |'
  prefs: []
  type: TYPE_TB
- en: '| ARNe $\beta=10$ [[31](#bib.bib31)] | - | 88.2 | - | - | - | - | - | - | -
    | - | - | - | - | - | 98.9 | 17.8 |'
  prefs: []
  type: TYPE_TB
- en: '| T-LEN $\beta=10$ + TM[[22](#bib.bib22)] | - | 88.9 | - | - | - | - | - |
    - | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SCL [[33](#bib.bib33)] | - | 88.9 | - | - | - | - | - | - | - | - | - | -
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MXGNet $\beta=10$ [[36](#bib.bib36)] | 89.9 | 89.6 | 91.5 | 84.6 | 81.9 |
    69.3 | 78.1 | 64.2 | 80.5 | 20.2 | 85.2 | 16.8 | 89.2 | 15.6 | 94.3 | 18.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MRNet [[18](#bib.bib18)] | - | 93.4 | - | 68.1 | - | 38.4 | - | 55.3 | -
    | 25.9 | - | 30.1 | - | 16.9 | - | 19.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MLRN [[32](#bib.bib32)] | - | 98.0 | - | 57.8 | - | - | - | - | - | - | -
    | - | - | - | - | 14.9 |'
  prefs: []
  type: TYPE_TB
- en: 5 Evaluating machine intelligence with RPMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In spite of many attempts at creating efficient models for solving RPMs, current
    approaches described in the previous section still struggle in more demanding
    benchmark setups. In this section, we summarise the main quantitative results
    of the discussed models on PGM, RAVEN and I-RAVEN datasets. Moreover, we highlight
    the most challenging setups for the current models and contrast their performance
    with the number of trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Results on PGM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start by discussing the results on PGM. Table [II](#S4.T2 "TABLE II ‣ 4.3
    Hierarchical networks ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods for
    Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices") compares
    performance of all discussed approaches based on the accuracy results reported
    in the respective papers. Firstly, it can be seen that the majority of presented
    approaches weren’t evaluated in all PGM regimes. Namely, out of 32 methods shown
    in the table, only 9 were tested in each regime (4 unique models: WReN, MXGNet,
    SCL, MRNet, trained with different setups) and only 8 other methods were evaluated
    in at least one regime other than Neutral.'
  prefs: []
  type: TYPE_NORMAL
- en: This observation raises a concern that, in practice, the PGM dataset is not
    utilized in a way it was intended for. Barrett et al. [[12](#bib.bib12)] defined
    the PGM benchmark as a tool for measuring generalisation performance across different
    regimes, whereas less than half of existing methods were evaluated on regimes
    other than Neutral. In the Neutral regime, all dataset splits (train/val/test)
    contain RPMs with objects, attributes and rules sampled from the same underlying
    distributions. As a consequence, the regime measures the capacity of the algorithm
    of understanding what an RPM is, rather than its ability to generalise to novel
    settings represented by the remaining regimes.
  prefs: []
  type: TYPE_NORMAL
- en: Although this limited evaluation of various works is partially explained by
    the extensive dataset size which is the main bottleneck for measuring generalisation,
    our beliefs are in line with the PGM authors that, actually, the performance across
    different regimes is what should be compared. The accuracy in the Neutral regime
    is only an initial indicator of the model reasoning capability, which can often
    be misleading as demonstrated by variation in the MLRN performance – the model
    achieves near perfect accuracy in the Neutral regime, while completely failing
    in the Extrapolation split. At the same time it performs subpar when compared
    to other models—potentially weaker when judging by Neutral regime scores—in the
    Interpolation regime. We believe that the ultimate goal of developing DL methods
    for solving AVR problems is not to obtain well-performing models on existing benchmarks,
    but rather to search for higher level approaches tackling generalisation that
    could be transferred to other domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: PGM mean accuracy. Average accuracy measured on the test split of
    models that were evaluated in each PGM regime, sorted in increasing order.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Mean test accuracy (%) |'
  prefs: []
  type: TYPE_TB
- en: '| WReN $\beta=0$ [[12](#bib.bib12)] | 32.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MXGNet $\beta=0$ [[36](#bib.bib36)] | 35.1 |'
  prefs: []
  type: TYPE_TB
- en: '| WReN $\beta=10$ [[12](#bib.bib12)] | 39.7 |'
  prefs: []
  type: TYPE_TB
- en: '| SCL CE [[19](#bib.bib19)] | 39.8 |'
  prefs: []
  type: TYPE_TB
- en: '| SCL AUX-sparse [[19](#bib.bib19)] | 43.3 |'
  prefs: []
  type: TYPE_TB
- en: '| MRNet [[18](#bib.bib18)] | 43.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SCL MLCL [[19](#bib.bib19)] | 44.1 |'
  prefs: []
  type: TYPE_TB
- en: '| SCL AUX-dense [[19](#bib.bib19)] | 46.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MXGNet $\beta=10$ [[36](#bib.bib36)] | 47.3 |'
  prefs: []
  type: TYPE_TB
- en: 'A closer look at the models’ performance in different regimes reveals that
    there isn’t a single method that performs superior across all regimes. On the
    contrary, each method seems to possess specific generalisation capabilities. The
    average accuracy of the models that were evaluated in all PGM regimes is summarised
    in Table [III](#S5.T3 "TABLE III ‣ 5.1 Results on PGM ‣ 5 Evaluating machine intelligence
    with RPMs ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s
    Progressive Matrices"). Best result is achieved by MXGNet trained using auxiliary
    training with $\beta=10$. The SCL model achieves competitive results with various
    training setups. The third model, MRNet is a close runner-up to SCL. The WReN
    model performs worst. However, since only a handful of the proposed methods were
    evaluated in each regime, it is difficult to ultimately choose a model with the
    best generalisation performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4b9b816ce900dca86c9b1de7d3b9cff0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: PGM regime difficulty. Mean accuracy on test (Test.) and validation
    (Val.) splits and their difference (Diff.) for the methods that were evaluated
    in each regime (listed in Table [III](#S5.T3 "TABLE III ‣ 5.1 Results on PGM ‣
    5 Evaluating machine intelligence with RPMs ‣ Deep Learning Methods for Abstract
    Visual Reasoning: A Survey on Raven’s Progressive Matrices")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the results of 9 models from Table [III](#S5.T3 "TABLE III ‣ 5.1 Results
    on PGM ‣ 5 Evaluating machine intelligence with RPMs ‣ Deep Learning Methods for
    Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices") that were
    evaluated in each PGM regime, we compare in Fig. [9](#S5.F9 "Figure 9 ‣ 5.1 Results
    on PGM ‣ 5 Evaluating machine intelligence with RPMs ‣ Deep Learning Methods for
    Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices") the levels
    of difficulty of individual regimes. It can be easily noticed that the regimes
    can be categorised into three distinct groups, depending on their difficulty.
    Firstly, there is the Neutral regime, where by definition the test split has the
    same distribution as the validation split. In this conventional setup, the performance
    on validation split is an accurate indicator of the results on the test split.
    This, however, is no longer the case in the remaining regimes. In the Interpolation,
    Held-out Triple Pairs and Held-out Attribute Pairs regimes, the accuracy of the
    models noticeably decreases between validation and test splits (by as large as
    34.7% in case of MXGNet $\beta=0$ in Held-out Attribute Pairs regime, see Table [II](#S4.T2
    "TABLE II ‣ 4.3 Hierarchical networks ‣ 4 RPM Deep Learning Models ‣ Deep Learning
    Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")).
    Generalisation performance practically diminishes in the remaining regimes, i.e. Held-out
    Triples, Extrapolation, Held-out Line-Type and Held-out Shape-Color, where some
    models even present the behaviour on the test set indistinguishable from random
    guessing. These observations demonstrate that although current approaches present
    satisfactory results in the Neutral regime, all other, more demanding regimes,
    remain a challenge. We believe that future works should detour from pushing the
    limits of i.i.d. generalisation and instead explicitly focus on building models
    capable of generalising to out-of-distribution samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: RAVEN vs I-RAVEN accuracy. Mean accuracy on the test splits for all
    configurations of both RAVEN [[16](#bib.bib16)] and I-RAVEN [[17](#bib.bib17)]
    datasets. The models are arranged according to their score on I-RAVEN and then
    on RAVEN in ascending order. The results of the best reported configuration are
    presented for each model. ²²footnotemark: 2MRNet was evaluated on RAVEN-FAIR.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Test accuracy (%) |'
  prefs: []
  type: TYPE_TB
- en: '| RAVEN | I-RAVEN |'
  prefs: []
  type: TYPE_TB
- en: '| CNN LSTM + DRT [[16](#bib.bib16)] | 14.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| WReN + DRT [[16](#bib.bib16)] | 15.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| ARNe [[31](#bib.bib31)] | 19.7 | - |'
  prefs: []
  type: TYPE_TB
- en: '| MCPT [[28](#bib.bib28)] | 28.5 | - |'
  prefs: []
  type: TYPE_TB
- en: '| WReN-Tag-Aux [[20](#bib.bib20)] | 34.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| CNN MLP [[16](#bib.bib16)] | 37.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| CNN MLP + DRT [[16](#bib.bib16)] | 39.4 | - |'
  prefs: []
  type: TYPE_TB
- en: '| PRD [[29](#bib.bib29)] | 50.7 | - |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-18 [[28](#bib.bib28)] | 77.2 | - |'
  prefs: []
  type: TYPE_TB
- en: '| LEN + TM [[22](#bib.bib22)] | 78.3 | - |'
  prefs: []
  type: TYPE_TB
- en: '| MXGNet [[36](#bib.bib36)] | 83.9 | - |'
  prefs: []
  type: TYPE_TB
- en: '| ²²footnotemark: 2MRNet [[18](#bib.bib18)] | 84.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-50 + pre-train [[28](#bib.bib28)] | 86.3 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Rel-Base [[34](#bib.bib34)] | 91.7 | - |'
  prefs: []
  type: TYPE_TB
- en: '| CoPINet + AL [[21](#bib.bib21)] | 93.5 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DCNet [[35](#bib.bib35)] | 93.6 | - |'
  prefs: []
  type: TYPE_TB
- en: '| CoPINet + ACL [[21](#bib.bib21)] | 93.7 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Rel-AIR [[34](#bib.bib34)] | 94.1 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Context-blind ResNet [[17](#bib.bib17)] | 71.9 | 12.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Context-blind SCL [[33](#bib.bib33)] | 94.2 | 12.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Context-blind CoPINet [[17](#bib.bib17)] | 94.2 | 14.2 |'
  prefs: []
  type: TYPE_TB
- en: '| CNN LSTM [[16](#bib.bib16), [17](#bib.bib17)] | 13.1 | 18.9 |'
  prefs: []
  type: TYPE_TB
- en: '| WReN [[16](#bib.bib16), [17](#bib.bib17)] | 14.7 | 23.8 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-18 [[16](#bib.bib16), [17](#bib.bib17)] | 53.4 | 40.3 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-18 + DRT [[16](#bib.bib16), [17](#bib.bib17)] | 59.6 | 40.4 |'
  prefs: []
  type: TYPE_TB
- en: '| LEN [[22](#bib.bib22), [17](#bib.bib17)] | 72.9 | 41.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Wild ResNet [[17](#bib.bib17)] | - | 44.3 |'
  prefs: []
  type: TYPE_TB
- en: '| CoPINet [[20](#bib.bib20), [17](#bib.bib17)] | 91.4 | 46.1 |'
  prefs: []
  type: TYPE_TB
- en: '| NCD [[30](#bib.bib30)] | 37.0 | 48.2 |'
  prefs: []
  type: TYPE_TB
- en: '| CoPINet MLCL+DA [[19](#bib.bib19)] | - | 57.1 |'
  prefs: []
  type: TYPE_TB
- en: '| SRAN [[17](#bib.bib17)] | - | 60.8 |'
  prefs: []
  type: TYPE_TB
- en: '| SRAN MLCL+DA [[19](#bib.bib19)] | - | 73.3 |'
  prefs: []
  type: TYPE_TB
- en: '| ²²footnotemark: 2MRNet [[18](#bib.bib18)] | - | 86.8 |'
  prefs: []
  type: TYPE_TB
- en: '| SCL [[33](#bib.bib33)] | 91.6 | 95.0 |'
  prefs: []
  type: TYPE_TB
- en: '| SCL MLCL+DA [[19](#bib.bib19)] | - | 96.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Human [[16](#bib.bib16)] | 84.4 | - |'
  prefs: []
  type: TYPE_TB
- en: 5.2 Results on (I-)RAVEN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Besides PGM, the methods for solving RPMs are often evaluated on the RAVEN
    dataset and its derivatives. The aggregated accuracy scores of the discussed approaches
    on test splits of both RAVEN and I-RAVEN are shown in Table [2](#footnotex2 "footnote
    2 ‣ TABLE IV ‣ 5.1 Results on PGM ‣ 5 Evaluating machine intelligence with RPMs
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices"). Firstly, it can be seen that the upper part of the table contains
    methods that were evaluated only on RAVEN. However, due to the hidden bias in
    the answer generation algorithm [[17](#bib.bib17)] the results reported on this
    dataset are inconclusive and can be misleading. This was demonstrated by remarkable
    performance of context-blind models, on the one hand, and by significant (45.3
    p.p.) drop of accuracy of CoPINet on the balanced dataset (91.4% on RAVEN vs 46.1%
    on I-RAVEN), on the other hand. Therefore, we strongly advocate for evaluating
    existing and future DL approaches on the unbiased version of the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Best results on I-RAVEN are achieved by SCL, a model which also performed very
    well on PGM. This suggests that SCL is, overall, the best-performing model for
    solving RPMs. This claim is further strengthened in the following section which
    sheds light on the relation between model performance and the number of its parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Detailed performance results on particular RAVEN and I-RAVEN configurations
    are presented in the *supplementary material*.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Model size. Number of trainable parameters based on model’s open-source
    implementation. C-B stands for Context-blind. ²²footnotemark: 2In C-B ResNet-18
    we have changed the number of input channels from 16 to 8 in the provided implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | # params | GitHub repository |'
  prefs: []
  type: TYPE_TB
- en: '| SCL [[33](#bib.bib33)] | 137,286 | [dhh1995/SCL](https://github.com/dhh1995/SCL)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CNN LSTM [[16](#bib.bib16)] | 143,960 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CNN MLP [[16](#bib.bib16)] | 299,400 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CNN LSTM + DRT [[16](#bib.bib16)] | 804,628 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| WReN [[12](#bib.bib12)] | 1,216,173 | [Fen9/WReN](https://github.com/Fen9/WReN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Rel-Base [[34](#bib.bib34)] | 1,226,673 | [SvenShade/Rel-AIR](https://github.com/SvenShade/Rel-AIR)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CoPINet [[20](#bib.bib20)] | 1,685,949 | [WellyZhang/CoPINet](https://github.com/WellyZhang/CoPINet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Rel-AIR [[34](#bib.bib34)] | 1,948,644 | [SvenShade/Rel-AIR](https://github.com/SvenShade/Rel-AIR)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CNN MLP + DRT [[16](#bib.bib16)] | 2,054,724 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LEN [[22](#bib.bib22)] | 5,520,673 | [zkcys001/distracting_feature](https://github.com/zkcys001/distracting_feature)
    |'
  prefs: []
  type: TYPE_TB
- en: '| DCNet [[35](#bib.bib35)] | 5,833,025 | [visiontao/dcnet](https://github.com/visiontao/dcnet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| NCD [[30](#bib.bib30)] | 11,177,025 | [visiontao/ncd](https://github.com/visiontao/ncd)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ²²footnotemark: 2C-B ResNet-18 [[16](#bib.bib16)] | 11,474,342 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-18 [[16](#bib.bib16)] | 11,499,430 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-18 + DRT [[16](#bib.bib16)] | 13,254,754 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MRNet [[18](#bib.bib18)] | 19,531,841 | [yanivbenny/MRNet](https://github.com/yanivbenny/MRNet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SRAN [[17](#bib.bib17)] | 44,030,217 | [husheng12345/SRAN](https://github.com/husheng12345/SRAN)
    |'
  prefs: []
  type: TYPE_TB
- en: <svg  class="ltx_picture" height="395.2" overflow="visible"
    version="1.1" width="483.94"><g transform="translate(0,395.2) matrix(1 0 0 -1
    0 0) translate(32.26,0) translate(0,9.2)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    clip-path="url(#pgfcp1)"><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 49.46 30.65)"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\displaystyle{0}$</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 109.53 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{20}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 173.06 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{40}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 236.6 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{60}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 300.13 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{80}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 360.21 30.65)"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{100}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 115.82 11.29)"><foreignobject
    width="191.85" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PGM
    neutral test accuracy (%)</foreignobject></g><g stroke="#000000" fill="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 41.11 39.61)"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{0}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 105.96)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{20}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 172.31)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{40}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 238.66)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{60}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 305.01)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{80}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 27.22 371.35)"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{100}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(0.0 1.0 -1.0 0.0 18.21 111.66)"><foreignobject
    width="198.83" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RAVEN
    mean test accuracy (%)</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -32.26 150.45)"><foreignobject width="380.01" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">CNN MLP [[12](#bib.bib12), [16](#bib.bib16)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 -27.5 71.16)"><foreignobject
    width="388.28" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CNN
    LSTM [[12](#bib.bib12), [16](#bib.bib16)]</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 74.36 103.01)"><foreignobject
    width="354.84" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">WReN [[12](#bib.bib12),
    [16](#bib.bib16)]</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 157.9 266.23)"><foreignobject width="222.69" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">LEN [[22](#bib.bib22)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 -21.68 344.19)"><foreignobject
    width="247.41" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CoPINet [[20](#bib.bib20)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 139.92 361.44)"><foreignobject
    width="230.07" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DCNet [[35](#bib.bib35)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 223.66 303.05)"><foreignobject
    width="220.16" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MRNet [[18](#bib.bib18)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 160.44 328.6)"><foreignobject
    width="245.56" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Rel-Base [[34](#bib.bib34)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 80.15 180.3)"><foreignobject
    width="247.95" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">NCD [[30](#bib.bib30)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 238.02 354.8)"><foreignobject
    width="213.66" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SCL [[33](#bib.bib33)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 70.48 352.1)"><foreignobject
    width="60.08" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">#
    params</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 332.2)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0.1 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 312.83)"><foreignobject
    width="34.98" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.3
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 293.46)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1.2 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 274.09)"><foreignobject
    width="34.98" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1.2
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 254.73)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1.7 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 235.36)"><foreignobject
    width="34.98" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5.5
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 215.99)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">5.8 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 196.63)"><foreignobject
    width="41.9" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">11.2
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 177.26)"><foreignobject width="41.9" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">19.5 M</foreignobject></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: (a) PGM – RAVEN.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg version="1.1" width="645.49" height="395.2" overflow="visible"><g transform="translate(0,395.2)
    scale(1,-1)"><g transform="translate(0,395.2) scale(1, -1)"><foreignobject width="645.49"
    height="395.2" overflow="visible"><svg height="395.2" overflow="visible" version="1.1"
    width="645.49"><g transform="translate(0,395.2) matrix(1 0 0 -1 0 0) translate(28.11,0)
    translate(0,9.2)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g clip-path="url(#pgfcp22)"><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 49.46 30.65)"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{0}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 109.53 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{20}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 173.06 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{40}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 236.6 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{60}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 300.13 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{80}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 360.21 30.65)"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{100}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 115.82 11.29)"><foreignobject
    width="191.85" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PGM
    neutral test accuracy (%)</foreignobject></g><g stroke="#000000" fill="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 41.11 39.61)"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{0}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 105.96)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{20}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 172.31)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{40}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 238.66)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{60}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 305.01)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{80}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 27.22 371.35)"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{100}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(0.0 1.0 -1.0 0.0 18.21 106.85)"><foreignobject
    width="208.44" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">I-RAVEN
    mean test accuracy (%)</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -28.11 90.4)"><foreignobject width="389.51" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">CNN LSTM [[12](#bib.bib12), [17](#bib.bib17)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 261.31 119.93)"><foreignobject
    width="356.07" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">WReN [[12](#bib.bib12),
    [17](#bib.bib17)]</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 247.01 161.73)"><foreignobject width="323.51" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">LEN [[22](#bib.bib22), [17](#bib.bib17)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 89.73 203.86)"><foreignobject
    width="348.23" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CoPINet [[17](#bib.bib17),
    [20](#bib.bib20)]</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 170.28 259.26)"><foreignobject width="218.27" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SRAN [[17](#bib.bib17)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 217.3 312.06)"><foreignobject
    width="220.16" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MRNet [[18](#bib.bib18)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 80.15 184.29)"><foreignobject
    width="247.95" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">NCD [[30](#bib.bib30)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 118.48 356.13)"><foreignobject
    width="213.66" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SCL [[33](#bib.bib33)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 70.48 352.1)"><foreignobject
    width="60.08" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">#
    params</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 332.2)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0.1 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 312.83)"><foreignobject
    width="34.98" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1.2
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 293.46)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1.7 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 274.09)"><foreignobject
    width="34.98" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5.5
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 254.73)"><foreignobject width="41.9" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">11.2 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 235.36)"><foreignobject
    width="41.9" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">19.5
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 215.99)"><foreignobject width="41.9" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">44.0 M</foreignobject></g></g></g></svg></foreignobject></g><g
    transform="translate(0,395.2) scale(1, -1)"><foreignobject width="645.49" height="395.2"
    overflow="visible">²²footnotemark: 2</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: (b) PGM – I-RAVEN.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg version="1.1" width="678.01" height="395.2" overflow="visible"><g transform="translate(0,395.2)
    scale(1,-1)"><g transform="translate(0,395.2) scale(1, -1)"><foreignobject width="678.01"
    height="395.2" overflow="visible"><svg height="395.2" overflow="visible" version="1.1"
    width="678.01"><g transform="translate(0,395.2) matrix(1 0 0 -1 0 0) translate(60.47,0)
    translate(0,9.2)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g clip-path="url(#pgfcp39)"><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 49.46 30.65)"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{0}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 109.53 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{20}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 173.06 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{40}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 236.6 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{60}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 300.13 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{80}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 360.21 30.65)"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{100}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 113.1 11.29)"><foreignobject
    width="198.83" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RAVEN
    mean test accuracy (%)</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 41.11 39.61)"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\displaystyle{0}$</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 105.96)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{20}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 172.31)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{40}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 238.66)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{60}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 305.01)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{80}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 27.22 371.35)"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{100}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(0.0 1.0 -1.0 0.0 18.21 106.85)"><foreignobject
    width="208.44" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">I-RAVEN
    mean test accuracy (%)</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -56.86 90.4)"><foreignobject width="353.61" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">CNN LSTM [[16](#bib.bib16), [17](#bib.bib17)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 32.8 158.08)"><foreignobject
    width="341.39" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ResNet-18 [[16](#bib.bib16),
    [17](#bib.bib17)]</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 145.91 64.86)"><foreignobject width="270.82" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">C-B ResNet-18 [[17](#bib.bib17)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 56.29 208.17)"><foreignobject
    width="390.97" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ResNet-18
    + DRT [[16](#bib.bib16), [17](#bib.bib17)]</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 -60.47 129.88)"><foreignobject
    width="320.17" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">WReN [[16](#bib.bib16),
    [17](#bib.bib17)]</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 294.03 178.31)"><foreignobject width="323.51" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">LEN [[22](#bib.bib22), [17](#bib.bib17)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 140.56 237.03)"><foreignobject
    width="348.23" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CoPINet [[17](#bib.bib17),
    [20](#bib.bib20)]</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 183.09 101.35)"><foreignobject width="261.9" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">C-B CoPINet [[17](#bib.bib17)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 203.32 312.34)"><foreignobject
    width="220.16" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MRNet [[18](#bib.bib18)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 46.48 184.29)"><foreignobject
    width="247.95" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">NCD [[30](#bib.bib30)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 127.06 356.13)"><foreignobject
    width="213.66" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SCL [[33](#bib.bib33)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 70.48 352.1)"><foreignobject
    width="60.08" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">#
    params</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 332.2)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0.1 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 312.83)"><foreignobject
    width="34.98" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1.2
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 293.46)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1.7 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 274.09)"><foreignobject
    width="34.98" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5.5
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 254.73)"><foreignobject width="41.9" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">11.2 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 235.36)"><foreignobject
    width="41.9" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">11.5
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 215.99)"><foreignobject width="41.9" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">13.3 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 196.63)"><foreignobject
    width="41.9" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">19.5
    M</foreignobject></g></g></g></svg></foreignobject></g><g transform="translate(0,395.2)
    scale(1, -1)"><foreignobject width="678.01" height="395.2" overflow="visible">²²footnotemark:
    2</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: (c) RAVEN – I-RAVEN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Size vs accuracy. Model performance evaluated on PGM, RAVEN and
    I-RAVEN against the number of its trainable parameters. The plots present only
    those models which were evaluated in the literature on the respective pairs of
    datasets. C-B stands for Context-blind. ²²footnotemark: 2MRNet was evaluated on
    RAVEN-FAIR instead of I-RAVEN.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Model sizes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To the best of our knowledge, in the existing literature on DL methods for
    solving RPMs, the size of the models—measured in the number of parameters—was
    never discussed. However, some models, e.g. MRNet [[18](#bib.bib18)] or SRAN [[17](#bib.bib17)]
    use deep visual backbones based on ResNet [[100](#bib.bib100)] which results in
    huge number of trainable parameters. The numbers of parameters of all discussed
    models with published open-source implementations are compared in Table [V](#S5.T5
    "TABLE V ‣ 5.2 Results on (I-)RAVEN ‣ 5 Evaluating machine intelligence with RPMs
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices"). Model sizes range from hundreds of thousands up to tens of millions,
    with the largest one (SRAN) having around 320 times more parameters than the smallest
    one (SCL).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although a large number of trainable parameters is often necessary in various
    real-world problems, e.g. in computer vision [[125](#bib.bib125), [126](#bib.bib126)]
    or natural language processing [[127](#bib.bib127), [128](#bib.bib128)] where
    practically unlimited supply of the training data exists, in the context of RPMs
    this is no longer the case. Figure [2](#footnotex11 "footnote 2 ‣ Figure 10 ‣
    5.2 Results on (I-)RAVEN ‣ 5 Evaluating machine intelligence with RPMs ‣ Deep
    Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices") shows that in RPM benchmarks the best results are achieved by the model
    with the smallest number of parameters – SCL. Although a bit surprising, this
    observation shows that a relatively small model can effectively learn to solve
    RPMs with competitive generalisation ability (cf. Table [III](#S5.T3 "TABLE III
    ‣ 5.1 Results on PGM ‣ 5 Evaluating machine intelligence with RPMs ‣ Deep Learning
    Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")).
    In other words, these results prove that in the case of RPMs, large (in terms
    of the number of parameters) visual backbones (e.g. deep ResNets) are not necessary,
    and instead, the existence of efficient parameter-sharing modules is crucial.'
  prefs: []
  type: TYPE_NORMAL
- en: While this is yet to be verified in other AVR tasks, we hypothesize that due
    to their visual similarity (e.g. lack of texture and presence of 2D greyscale
    shapes), analogous architectural decisions should be made when designing AVR machine
    solvers in general.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The initial reason for introducing RPMs into DL literature, was to use these
    problems as a proxy for estimating machine intelligence. It turned out, however,
    that the ability of spatial and abstract reasoning is also crucial for the development
    of intelligent systems in various other settings [[129](#bib.bib129)]. Consequently,
    methods proposed in the context of RPMs are oftentimes relevant in other research
    and practical contexts. In this section, we link the discussed approaches to advancements
    in other fields and highlight the main unsolved challenges and open questions
    left for investigation in future work.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Seeds and fruits of RPM research
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Relation Network, the fundamental component of discussed models, e.g. WReN,
    LEN or MLRN, has already demonstrated its usefulness in multiple tasks. In [[130](#bib.bib130)]
    the authors consider a 3D human pose estimation problem [[131](#bib.bib131), [132](#bib.bib132),
    [133](#bib.bib133)] and propose a DL algorithm to tackle this challenge. The designed
    method employs an RN to capture relations among different body parts. Similarly,
    in the context of semantic segmentation [[134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136)], the idea of capturing long-distance spatial relationships
    between entities using RN is further explored. It is shown that a relational reasoning
    component can be used to augment CNN feature maps by exploiting both channel-wise
    and spatial feature relations [[137](#bib.bib137)]. Such incorporation of the
    global context is a recurring theme in works that exploit RNs (and is also explored
    in other neural modules [[138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [141](#bib.bib141)]). RN was further applied to model spatio-temporal interactions
    between human actors, objects and scene elements in the context of action recognition [[142](#bib.bib142)],
    to train an effective image recognition model in a contrastive self-supervised
    setting [[143](#bib.bib143)], or to provide a mechanism for relational reasoning
    over structured representations for a deep reinforcement learning agent [[144](#bib.bib144)].
  prefs: []
  type: TYPE_NORMAL
- en: In order to successfully solve RPMs and learn to formulate analogies, multiple
    discussed works have employed various forms of contrastive mechanisms either directly
    in the model architecture [[20](#bib.bib20), [35](#bib.bib35)] or in the objective
    function [[20](#bib.bib20), [19](#bib.bib19), [21](#bib.bib21)]. Contrastive approaches [[145](#bib.bib145),
    [146](#bib.bib146), [147](#bib.bib147)] are especially useful for self-supervised
    learning, where the availability of labelled data is scarce. The importance of
    such methods was already demonstrated in the context of computer vision [[148](#bib.bib148),
    [149](#bib.bib149), [143](#bib.bib143)], natural language processing [[150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152)], speech recognition [[153](#bib.bib153),
    [154](#bib.bib154), [155](#bib.bib155)], or reinforcement learning [[156](#bib.bib156),
    [157](#bib.bib157), [158](#bib.bib158)].
  prefs: []
  type: TYPE_NORMAL
- en: RPM solving methods are often directly applicable, after minor adjustments only,
    to related abstract reasoning tasks. RN was found to be competitive to other models
    in solving arithmetic visual reasoning tasks [[59](#bib.bib59)], while WReN-based
    models were found to be one of the top performing methods in solving the visual
    analogy problems [[56](#bib.bib56)] and abstract reasoning matrices structurally
    similar to RPMs [[90](#bib.bib90)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e1b237606f30c8440d3667c222a80cd5.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) RAVEN, $i\in\{0,1,\ldots,6\}$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fe36157da9c525d662dd07b35031bed9.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) PGM, $i\in\{0,2,\ldots,12\}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: Low-sample regime. The plots present the performance of two models
    trained on splits of a) RAVEN and b) PGM datasets. The split sizes equal to $N/2^{i}$,
    where $N$ denotes the size of the original dataset, which equals to 42,000 for
    RAVEN and 1,200,000 for PGM. The results were reported in [[35](#bib.bib35)].'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Challenges and open problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Even though the discussed works have embarked on a quest to measure machine
    intelligence by means of evaluating their performance on RPM benchmarks, some
    may oppose the validity of this path. When solving an RPM, a human solver is often
    faced with a task that he/she has not encountered before, which tests the ability
    of adaptive problem solving. Contrary to humans, the majority of current DL approaches
    use thousands [[16](#bib.bib16)] or millions [[12](#bib.bib12)] training samples
    beforehand. In addition, the performance of these models rapidly deteriorates
    when the size of the training corpora decreases, as illustrated in Fig. [11](#S6.F11
    "Figure 11 ‣ 6.1 Seeds and fruits of RPM research ‣ 6 Discussion ‣ Deep Learning
    Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices").
    Conversely, humans are able not only to grasp the concepts after familiarising
    themselves with just a few examples, but also to extrapolate knowledge gained
    when solving simple matrices to more advanced ones. Given this notable contrast
    it is worth to advocate the search for efficient methods for solving RPMs without
    access to huge training sets. We are convinced that it is worthwhile to pursue
    certain emerging pathways discussed in this paper that are explicitly designed
    to probe DL algorithms in few-shot learning setups [[58](#bib.bib58), [159](#bib.bib159)].'
  prefs: []
  type: TYPE_NORMAL
- en: Another way of ensuring that the developed pattern analysis algorithms are benchmarked
    similarly to humans in new environments is to follow the perspective of the seminal
    PGM paper [[12](#bib.bib12)]. By explicitly defining various generalisation regimes
     Barrett et al. [[12](#bib.bib12)] allowed to directly measure generalisation
    (performance in new settings) of DL methods for solving RPMs. However, as of today
    no effective method capable of achieving human-like performance in all regimes
    was proposed. In fact, existing approaches seem to possess specific generalisation
    abilities that are rather a side effect than a deliberate choice. While recent
    work [[57](#bib.bib57)] shows that neural models can generalise in AVR tasks that
    focus on extrapolation, which constitutes one of the most demanding PGM regimes,
    there is still a long way to construct a universal learning system that would
    excel in all regimes.
  prefs: []
  type: TYPE_NORMAL
- en: Another key characteristic that differentiates humans from current AI systems
    is the ability to solve various types of AVR problems with limited training. While
    humans are known to be able to generalise and transfer knowledge between problems,
    such property has only been briefly demonstrated by the existing DL methods. Some
    works that evaluate DL approaches on the RAVEN dataset show that models trained
    on one configuration may learn to build abstractions useful for solving matrices
    belonging to other configurations [[16](#bib.bib16), [34](#bib.bib34), [27](#bib.bib27)].
    In [[15](#bib.bib15)], an RPM solving model was shown to be adaptable to solving
    odd-one-out tasks that consisted of similar input images. While the above examples
    indicate, to some extent, the ability of DL algorithms to i.i.d. generalisation
    in the context of AVR, knowledge reuse between multiple distinct AVR problems
    remains to be investigated in future work.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper discusses recent progress in applying DL methods to solving RPMs,
    summarises various methods of learning to solve these tasks, reviews the existing
    RPM benchmark sets, and categorises the DL models employed in this field. Also,
    by aggregating results of recently published methods, it brings to attention the
    most challenging aspects of RPM problems, which to this day remain primarily unsolved.
  prefs: []
  type: TYPE_NORMAL
- en: The paper argues that while RPMs were initially proposed as a task for measuring
    human intelligence and were later employed as a proxy for estimating machine intelligence,
    they additionally offer a comprehensible playground for developing and testing
    abstract and relational reasoning approaches. Viewed from this perspective, advancements
    in RPM research are applicable to a broad spectrum of other domains where spatial
    and abstract reasoning skills are required.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the task of solving RPMs has seen a wide interest within the DL
    community in recent years, its core challenges remain unattained. We hope that
    by collating the advances in methods for solving RPMs, this survey will stimulate
    progress in future AVR research.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Detterman and Sternberg [1986] D. K. Detterman and R. J. Sternberg, *What is
    intelligence?: Contemporary viewpoints on its nature and definition*.   Ablex,
    1986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Legg et al. [2007] S. Legg, M. Hutter *et al.*, “A collection of definitions
    of intelligence,” *Frontiers in Artificial Intelligence and applications*, vol.
    157, p. 17, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hernández-Orallo [2017] J. Hernández-Orallo, *The measure of all minds: evaluating
    natural and artificial intelligence*.   Cambridge University Press, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snow et al. [1984] R. E. Snow, P. C. Kyllonen, and B. Marshalek, “The topography
    of ability and learning correlations,” *Advances in the psychology of human intelligence*,
    vol. 2, no. S 47, p. 103, 1984.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carpenter et al. [1990] P. A. Carpenter, M. A. Just, and P. Shell, “What one
    intelligence test measures: a theoretical account of the processing in the Raven
    Progressive Matrices Test.” *Psychological review*, vol. 97, no. 3, p. 404, 1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hofstadter et al. [1979] D. R. Hofstadter *et al.*, *Gödel, Escher, Bach: an
    eternal golden braid*.   Basic books New York, 1979, vol. 13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Raven [1936] J. C. Raven, “Mental tests used in genetic studies: The performance
    of related individuals on tests mainly educative and mainly reproductive,” *Master’s
    thesis, University of London*, 1936.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raven and Court [1998] J. C. Raven and J. H. Court, *Raven’s progressive matrices
    and vocabulary scales*.   Oxford pyschologists Press Oxford, England, 1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. [2015] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*,
    vol. 521, no. 7553, pp. 436–444, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raghu and Schmidt [2020] M. Raghu and E. Schmidt, “A survey of deep learning
    for scientific discovery,” *arXiv preprint arXiv:2003.11755*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoshen and Werman [2017] D. Hoshen and M. Werman, “IQ of Neural Networks,” *arXiv
    preprint arXiv:1710.01692*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barrett et al. [2018] D. Barrett, F. Hill, A. Santoro, A. Morcos, and T. Lillicrap,
    “Measuring abstract reasoning in neural networks,” in *International Conference
    on Machine Learning*.   PMLR, 2018, pp. 511–520.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Matzen et al. [2010] L. E. Matzen, Z. O. Benz, K. R. Dixon, J. Posey, J. K.
    Kroger, and A. E. Speed, “Recreating raven’s: Software for systematically generating
    large numbers of raven-like matrix problems with normed properties,” *Behavior
    research methods*, vol. 42, no. 2, pp. 525–541, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Su [2015] K. Wang and Z. Su, “Automatic generation of raven’s progressive
    matrices,” in *Twenty-Fourth International Joint Conference on Artificial Intelligence*,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mańdziuk and Żychowski [2019] J. Mańdziuk and A. Żychowski, “DeepIQ: A Human-Inspired
    AI System for Solving IQ Test Problems,” in *2019 International Joint Conference
    on Neural Networks (IJCNN)*.   IEEE, 2019, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2019a] C. Zhang, F. Gao, B. Jia, Y. Zhu, and S.-C. Zhu, “Raven:
    A dataset for relational and analogical visual reasoning,” in *Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition*, 2019, pp. 5317–5327.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. [2021] S. Hu, Y. Ma, X. Liu, Y. Wei, and S. Bai, “Stratified rule-aware
    network for abstract visual reasoning,” in *AAAI Conference on Artificial Intelligence
    (AAAI)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benny et al. [2021] Y. Benny, N. Pekar, and L. Wolf, “Scale-localized abstract
    reasoning,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2021, pp. 12 557–12 565.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Małkiński and Mańdziuk [2020] M. Małkiński and J. Mańdziuk, “Multi-label contrastive
    learning for abstract visual reasoning,” *arXiv preprint arXiv:2012.01944*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2019b] C. Zhang, B. Jia, F. Gao, Y. Zhu, H. Lu, and S.-C. Zhu,
    “Learning perceptual inference by contrasting,” in *Advances in Neural Information
    Processing Systems*, 2019, pp. 1075–1087.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. [2020] Y. Kim, J. Shin, E. Yang, and S. J. Hwang, “Few-shot visual
    reasoning with meta-analogical contrastive learning,” *Advances in Neural Information
    Processing Systems*, vol. 33, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2019] K. Zheng, Z.-J. Zha, and W. Wei, “Abstract reasoning with
    distracting features,” in *Advances in Neural Information Processing Systems*,
    2019, pp. 5842–5853.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Steenbrugge et al. [2018] X. Steenbrugge, S. Leroux, T. Verbelen, and B. Dhoedt,
    “Improving generalization for abstract reasoning tasks using disentangled feature
    representations,” *arXiv preprint arXiv:1811.04784*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pekar et al. [2020] N. Pekar, Y. Benny, and L. Wolf, “Generating correct answers
    for progressive matrices intelligence tests,” in *Advances in Neural Information
    Processing Systems*, vol. 33, 2020, pp. 7390–7400.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hua and Kunda [2020] T. Hua and M. Kunda, “Modeling gestalt visual reasoning
    on raven’s progressive matrices using generative image inpainting techniques.”
    in *CogSci*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. [2021] F. Shi, B. Li, and X. Xue, “Raven’s progressive matrices completion
    with latent gaussian process priors,” in *Proceedings of the AAAI Conference on
    Artificial Intelligence*, vol. 35, no. 11, 2021, pp. 9612–9620.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2021] C. Zhang, B. Jia, S.-C. Zhu, and Y. Zhu, “Abstract spatial-temporal
    reasoning via probabilistic abduction and execution,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 9736–9746.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuo and Kankanhalli [2020] T. Zhuo and M. Kankanhalli, “Solving raven’s progressive
    matrices with neural networks,” *arXiv preprint arXiv:2002.01646*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kiat et al. [2020] N. Q. W. Kiat, D. Wang, and M. Jamnik, “Pairwise relations
    discriminator for unsupervised raven’s progressive matrices,” *arXiv preprint
    arXiv:2011.01306*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuo et al. [2021] T. Zhuo, Q. Huang, and M. Kankanhalli, “Unsupervised abstract
    reasoning for raven’s problem matrices,” *IEEE Transactions on Image Processing*,
    vol. 30, pp. 8332–8341, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hahne et al. [2019] L. Hahne, T. Lüddecke, F. Wörgötter, and D. Kappel, “Attention
    on abstract visual reasoning,” *arXiv preprint arXiv:1911.05990*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jahrens and Martinetz [2020] M. Jahrens and T. Martinetz, “Solving raven’s progressive
    matrices with multi-layer relation networks,” in *2020 International Joint Conference
    on Neural Networks (IJCNN)*.   IEEE, 2020, pp. 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2020] Y. Wu, H. Dong, R. Grosse, and J. Ba, “The Scattering Compositional
    Learner: Discovering Objects, Attributes, Relationships in Analogical Reasoning,”
    *arXiv preprint arXiv:2007.04212*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spratley et al. [2020] S. Spratley, K. Ehinger, and T. Miller, “A closer look
    at generalisation in raven,” in *Computer Vision – ECCV 2020*.   Springer, 2020,
    pp. 601–616.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuo and Kankanhalli [2021] T. Zhuo and M. Kankanhalli, “Effective abstract
    reasoning with dual-contrast network,” in *International Conference on Learning
    Representations (ICLR)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2020] D. Wang, M. Jamnik, and P. Lio, “Abstract diagrammatic reasoning
    with multiplex graph networks,” in *International Conference on Learning Representations
    (ICLR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rahaman et al. [2021] N. Rahaman, M. W. Gondal, S. Joshi, P. Gehler, Y. Bengio,
    F. Locatello, and B. Schölkopf, “Dynamic inference with neural interpreters,”
    *Advances in Neural Information Processing Systems*, vol. 34, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitchell [2021] M. Mitchell, “Abstraction and analogy-making in artificial intelligence,”
    *arXiv preprint arXiv:2102.10717*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evans [1964] T. G. Evans, “A heuristic program to solve geometric-analogy problems,”
    in *Proceedings of the April 21-23, 1964, spring joint computer conference*, 1964,
    pp. 327–338.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Foundalis [2006] H. E. Foundalis, “Phaeaco: A cognitive architecture inspired
    by bongard’s problems.” PhD dissertation, Indiana University, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strannegård et al. [2013] C. Strannegård, S. Cirillo, and V. Ström, “An anthropomorphic
    method for progressive matrix problems,” *Cognitive Systems Research*, vol. 22,
    pp. 35–46, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gentner [1980] D. Gentner, “The structure of analogical models in science.”
    BOLT BERANEK AND NEWMAN INC CAMBRIDGE MA, Tech. Rep., 1980.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Falkenhainer et al. [1986] B. Falkenhainer, K. D. Forbus, and D. Gentner, *The
    structure-mapping engine*.   Department of Computer Science, University of Illinois
    at Urbana-Champaign, 1986, vol. 1275.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lovett et al. [2007] A. Lovett, K. Forbus, and J. Usher, “Analogy with qualitative
    spatial representations can simulate solving raven’s progressive matrices,” in
    *Proceedings of the Annual Meeting of the Cognitive Science Society*, vol. 29,
    no. 29, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lovett et al. [2010] ——, “A structure-mapping model of raven’s progressive matrices,”
    in *Proceedings of the Annual Meeting of the Cognitive Science Society*, vol. 32,
    no. 32, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kunda et al. [2010] M. Kunda, K. McGreggor, and A. Goel, “Taking a look (literally!)
    at the raven’s intelligence test: Two visual solution strategies,” in *Proceedings
    of the Annual Meeting of the Cognitive Science Society*, vol. 32, no. 32, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kunda et al. [2012] ——, “Reasoning on the raven’s advanced progressive matrices
    test with iconic visual representations,” in *Proceedings of the Annual Meeting
    of the Cognitive Science Society*, vol. 34, no. 34, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McGreggor et al. [2010] K. McGreggor, M. Kunda, and A. Goel, “A fractal analogy
    approach to the raven’s test of intelligence,” in *Workshops at the Twenty-Fourth
    AAAI Conference on Artificial Intelligence*, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McGreggor and Goel [2014] K. McGreggor and A. Goel, “Confident reasoning on
    raven’s progressive matrices tests,” in *Proceedings of the AAAI Conference on
    Artificial Intelligence*, vol. 28, no. 1, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hernández-Orallo et al. [2016] J. Hernández-Orallo, F. Martínez-Plumed, U. Schmid,
    M. Siebers, and D. L. Dowe, “Computer models solving intelligence test problems:
    Progress and implications,” *Artificial Intelligence*, vol. 230, pp. 74–107, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gardner and Richards [2006] M. Gardner and D. Richards, *The colossal book of
    short puzzles and problems*.   Norton, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ruiz [2011] P. E. Ruiz, “Building and solving odd-one-out classification problems:
    A systematic approach,” *Intelligence*, vol. 39, no. 5, pp. 342–350, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smets and Vreeken [2011] K. Smets and J. Vreeken, “The odd one out: Identifying
    and characterising anomalies,” in *Proceedings of the 2011 SIAM international
    conference on data mining*.   SIAM, 2011, pp. 804–815.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bongard [1968] M. M. Bongard, “The recognition problem,” Foreign Technology
    Div Wright-Patterson AFB Ohio, Tech. Rep., 1968.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fleuret et al. [2011] F. Fleuret, T. Li, C. Dubout, E. K. Wampler, S. Yantis,
    and D. Geman, “Comparing machines and humans on a visual categorization test,”
    *Proceedings of the National Academy of Sciences*, vol. 108, no. 43, pp. 17 621–17 625,
    2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hill et al. [2019] F. Hill, A. Santoro, D. Barrett, A. Morcos, and T. Lillicrap,
    “Learning to Make Analogies by Contrasting Abstract Relational Structure,” in
    *International Conference on Learning Representations (ICLR)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Webb et al. [2020] T. Webb, Z. Dulberg, S. Frankland, A. Petrov, R. O’Reilly,
    and J. Cohen, “Learning representations that support extrapolation,” in *International
    Conference on Machine Learning*.   PMLR, 2020, pp. 10 136–10 146.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chollet [2019] F. Chollet, “On the measure of intelligence,” *arXiv preprint
    arXiv:1911.01547*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2020] W. Zhang, C. Zhang, Y. Zhu, and S.-C. Zhu, “Machine Number
    Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational Reasoning,”
    in *AAAI Conference on Artificial Intelligence (AAAI)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Małkiński and Mańdziuk [2022] M. Małkiński and J. Mańdziuk, “A review of emerging
    research directions in abstract visual reasoning,” *arXiv preprint arXiv:2202.10284*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaeggi et al. [2008] S. M. Jaeggi, M. Buschkuehl, J. Jonides, and W. J. Perrig,
    “Improving fluid intelligence with training on working memory,” *Proceedings of
    the National Academy of Sciences*, vol. 105, no. 19, pp. 6829–6833, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain and Chandrasekaran [1982] A. K. Jain and B. Chandrasekaran, “39 dimensionality
    and sample size considerations in pattern recognition practice,” *Handbook of
    statistics*, vol. 2, pp. 835–855, 1982.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Raudys et al. [1991] S. J. Raudys, A. K. Jain *et al.*, “Small sample size
    effects in statistical pattern recognition: Recommendations for practitioners,”
    *IEEE Transactions on pattern analysis and machine intelligence*, vol. 13, no. 3,
    pp. 252–264, 1991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2017] C. Sun, A. Shrivastava, S. Singh, and A. Gupta, “Revisiting
    unreasonable effectiveness of data in deep learning era,” in *Proceedings of the
    IEEE international conference on computer vision*, 2017, pp. 843–852.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahajan et al. [2018] D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri,
    Y. Li, A. Bharambe, and L. Van Der Maaten, “Exploring the limits of weakly supervised
    pretraining,” in *Proceedings of the European Conference on Computer Vision (ECCV)*,
    2018, pp. 181–196.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ragni and Neubert [2014] M. Ragni and S. Neubert, “Analyzing raven’s intelligence
    test: Cognitive model, demand, and complexity,” in *Computational Approaches to
    Analogical Reasoning: Current Trends*.   Springer, 2014, pp. 351–370.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu [1974] K. S. Fu, *Syntactic methods in pattern recognition*.   Elsevier,
    1974.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu and Mumford [2007] S.-C. Zhu and D. Mumford, *A stochastic grammar of images*.   Now
    Publishers Inc, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. [2009] L. Lin, T. Wu, J. Porway, and Z. Xu, “A stochastic graph grammar
    for compositional object representation and recognition,” *Pattern Recognition*,
    vol. 42, no. 7, pp. 1297–1307, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Agrawal et al. [2018] A. Agrawal, D. Batra, D. Parikh, and A. Kembhavi, “Don’t
    just assume; look and answer: Overcoming priors for visual question answering,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2018, pp. 4971–4980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D’Amour et al. [2020] A. D’Amour, K. Heller, D. Moldovan, B. Adlam, B. Alipanahi,
    A. Beutel, C. Chen, J. Deaton, J. Eisenstein, M. D. Hoffman *et al.*, “Underspecification
    presents challenges for credibility in modern machine learning,” *arXiv preprint
    arXiv:2011.03395*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geirhos et al. [2020] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel,
    M. Bethge, and F. A. Wichmann, “Shortcut learning in deep neural networks,” *Nature
    Machine Intelligence*, vol. 2, no. 11, pp. 665–673, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dancette et al. [2021] C. Dancette, R. Cadene, D. Teney, and M. Cord, “Beyond
    question-based biases: Assessing multimodal shortcut learning in visual question
    answering,” *arXiv preprint arXiv:2104.03149*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Santoro et al. [2017] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu,
    P. Battaglia, and T. Lillicrap, “A simple neural network module for relational
    reasoning,” in *Advances in neural information processing systems*, 2017, pp.
    4967–4976.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gentner [1983] D. Gentner, “Structure-mapping: A theoretical framework for
    analogy,” *Cognitive science*, vol. 7, no. 2, pp. 155–170, 1983.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hofstadter [1995] D. R. Hofstadter, *Fluid concepts and creative analogies:
    Computer models of the fundamental mechanisms of thought.*   Basic books, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smith and Gentner [2014] L. Smith and D. Gentner, “The role of difference-detection
    in learning contrastive categories,” in *Proceedings of the Annual Meeting of
    the Cognitive Science Society*, vol. 36, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohn [2016] K. Sohn, “Improved deep metric learning with multi-class n-pair
    loss objective,” in *Proceedings of the 30th International Conference on Neural
    Information Processing Systems*, 2016, pp. 1857–1865.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khosla et al. [2020] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola,
    A. Maschinot, C. Liu, and D. Krishnan, “Supervised contrastive learning,” in *Advances
    in Neural Information Processing Systems*, vol. 33, 2020, pp. 18 661–18 673.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. [2009] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum
    learning,” in *Proceedings of the 26th annual international conference on machine
    learning*, 2009, pp. 41–48.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. [2010] M. P. Kumar, B. Packer, and D. Koller, “Self-paced learning
    for latent variable models.” in *Advances in Neural Information Processing Systems*,
    vol. 1, 2010, p. 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Borko and Putnam [1996] H. Borko and R. T. Putnam, “Learning to teach,” in *International
    Conference on Learning Representations (ICLR)*, 1996.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malisiewicz et al. [2011] T. Malisiewicz, A. Gupta, and A. A. Efros, “Ensemble
    of exemplar-svms for object detection and beyond,” in *2011 International conference
    on computer vision*.   IEEE, 2011, pp. 89–96.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. [2017] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal
    loss for dense object detection,” in *Proceedings of the IEEE international conference
    on computer vision*, 2017, pp. 2980–2988.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. [2018] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei,
    “Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted
    labels,” in *International Conference on Machine Learning*.   PMLR, 2018, pp.
    2304–2313.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ko et al. [2015] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, “Audio augmentation
    for speech recognition,” in *Sixteenth Annual Conference of the International
    Speech Communication Association*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shorten and Khoshgoftaar [2019] C. Shorten and T. M. Khoshgoftaar, “A survey
    on image data augmentation for deep learning,” *Journal of Big Data*, vol. 6,
    no. 1, pp. 1–48, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei and Zou [2019] J. Wei and K. Zou, “EDA: Easy data augmentation techniques
    for boosting performance on text classification tasks,” in *Proceedings of the
    2019 Conference on Empirical Methods in Natural Language Processing and the 9th
    International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*.   Hong
    Kong, China: Association for Computational Linguistics, Nov. 2019, pp. 6382–6388.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wen et al. [2020] Q. Wen, L. Sun, F. Yang, X. Song, J. Gao, X. Wang, and H. Xu,
    “Time series data augmentation for deep learning: A survey,” *arXiv preprint arXiv:2002.12478*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van Steenkiste et al. [2019] S. van Steenkiste, F. Locatello, J. Schmidhuber,
    and O. Bachem, “Are disentangled representations helpful for abstract visual reasoning?”
    in *Advances in Neural Information Processing Systems*, 2019, pp. 14 245–14 258.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling [2013] D. P. Kingma and M. Welling, “Auto-encoding variational
    bayes,” in *International Conference on Learning Representations (ICLR)*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Higgins et al. [2017] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot,
    M. Botvinick, S. Mohamed, and A. Lerchner, “beta-vae: Learning basic visual concepts
    with a constrained variational framework,” in *International Conference on Learning
    Representations (ICLR)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Burgess et al. [2018] C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters,
    G. Desjardins, and A. Lerchner, “Understanding disentangling in $\beta$-vae,”
    *arXiv preprint arXiv:1804.03599*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. [2014] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances
    in Neural Information Processing Systems*, vol. 27, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Creswell et al. [2018] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran,
    B. Sengupta, and A. A. Bharath, “Generative adversarial networks: An overview,”
    *IEEE Signal Processing Magazine*, vol. 35, no. 1, pp. 53–65, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2021] Z. Wang, Q. She, and T. E. Ward, “Generative adversarial
    networks in computer vision: A survey and taxonomy,” *ACM Comput. Surv.*, vol. 54,
    no. 2, Feb. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams [1992] R. J. Williams, “Simple statistical gradient-following algorithms
    for connectionist reinforcement learning,” *Machine learning*, vol. 8, no. 3,
    pp. 229–256, 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. [1990] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E.
    Howard, W. E. Hubbard, and L. D. Jackel, “Handwritten digit recognition with a
    back-propagation network,” in *Advances in neural information processing systems*,
    1990, pp. 396–404.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mekik et al. [2018] C. S. Mekik, R. Sun, and D. Y. Dai, “Similarity-based reasoning,
    raven’s matrices, and general intelligence.” in *IJCAI*, 2018, pp. 1576–1582.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2016] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning
    for image recognition,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2016, pp. 770–778.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber [1997] S. Hochreiter and J. Schmidhuber, “Long short-term
    memory,” *Neural computation*, vol. 9, no. 8, pp. 1735–1780, 1997.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Johnson et al. [2017] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei,
    C. Lawrence Zitnick, and R. Girshick, “CLEVR: A diagnostic dataset for compositional
    language and elementary visual reasoning,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017, pp. 2901–2910.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malinowski and Fritz [2014] M. Malinowski and M. Fritz, “A multi-world approach
    to question answering about real-world scenes based on uncertain input,” *Advances
    in neural information processing systems*, vol. 27, pp. 1682–1690, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weston et al. [2015] J. Weston, A. Bordes, S. Chopra, A. M. Rush, B. van Merriënboer,
    A. Joulin, and T. Mikolov, “Towards ai-complete question answering: A set of prerequisite
    toy tasks,” *arXiv preprint arXiv:1502.05698*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Antol et al. [2015] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.
    Zitnick, and D. Parikh, “VQA: Visual question answering,” in *Proceedings of the
    IEEE international conference on computer vision*, 2015, pp. 2425–2433.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. [2015] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu, “Are
    you talking to a machine? dataset and methods for multilingual image question
    answering,” in *Proceedings of the 28th International Conference on Neural Information
    Processing Systems - Volume 2*, ser. NIPS’15, 2015, p. 2296–2304.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. [2015a] M. Ren, R. Kiros, and R. Zemel, “Image question answering:
    A visual semantic embedding model and a new dataset,” in *Advances in Neural Information
    Processing Systems*, vol. 1, no. 2, 2015, p. 5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. [2015b] ——, “Exploring models and data for image question answering,”
    in *Advances in Neural Information Processing Systems*, vol. 28, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krishna et al. [2017] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
    S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma *et al.*, “Visual genome: Connecting
    language and vision using crowdsourced dense image annotations,” *International
    journal of computer vision*, vol. 123, no. 1, pp. 32–73, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hudson and Manning [2019] D. A. Hudson and C. D. Manning, “GQA: A new dataset
    for real-world visual reasoning and compositional question answering,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 6700–6709.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Teney et al. [2020] D. Teney, P. Wang, J. Cao, L. Liu, C. Shen, and A. van den
    Hengel, “V-PROM: A benchmark for visual reasoning using visual progressive matrices,”
    in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 34, no. 07,
    2020, pp. 12 071–12 078.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Palm et al. [2018] R. Palm, U. Paquet, and O. Winther, “Recurrent relational
    networks,” *Advances in Neural Information Processing Systems*, vol. 31, pp. 3368–3378,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jahrens and Martinetz [2019] M. Jahrens and T. Martinetz, “Multi-layer relation
    networks for relational reasoning,” in *Proceedings of the 2nd International Conference
    on Applications of Intelligent Systems*, 2019, pp. 1–5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You et al. [2020] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli,
    X. Song, J. Demmel, K. Keutzer, and C.-J. Hsieh, “Large batch optimization for
    deep learning: Training bert in 76 minutes,” in *International Conference on Learning
    Representations*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, Ł. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in
    *Advances in neural information processing systems*, 2017, pp. 5998–6008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. [2018] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-recurrence
    sequence-to-sequence model for speech recognition,” in *2018 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   IEEE, 2018,
    pp. 5884–5888.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. [2021] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,
    X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,
    and N. Houlsby, “An image is worth 16x16 words: Transformers for image recognition
    at scale,” in *International Conference on Learning Representations*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaegle et al. [2021] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman,
    and J. Carreira, “Perceiver: General perception with iterative attention,” in
    *Proceedings of the 38th International Conference on Machine Learning*, ser. Proceedings
    of Machine Learning Research, vol. 139.   PMLR, 18–24 Jul 2021, pp. 4651–4664.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kayhan and Gemert [2020] O. S. Kayhan and J. C. v. Gemert, “On translation
    invariance in cnns: Convolutional layers can exploit absolute spatial location,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 14 274–14 285.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohen et al. [2018] T. S. Cohen, M. Geiger, J. Köhler, and M. Welling, “Spherical
    CNNs,” in *International Conference on Learning Representations*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. [2012] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet
    classification with deep convolutional neural networks,” *Advances in neural information
    processing systems*, vol. 25, pp. 1097–1105, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. [2017] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated
    residual transformations for deep neural networks,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 1492–1500.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kirsch et al. [2018] L. Kirsch, J. Kunze, and D. Barber, “Modular networks:
    Learning to decompose neural computation,” in *Advances in Neural Information
    Processing Systems*, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
    and R. Garnett, Eds., vol. 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eslami et al. [2016] S. M. A. Eslami, N. Heess, T. Weber, Y. Tassa, D. Szepesvari,
    k. kavukcuoglu, and G. E. Hinton, “Attend, infer, repeat: Fast scene understanding
    with generative models,” in *Advances in Neural Information Processing Systems*,
    vol. 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goyal et al. [2021] P. Goyal, M. Caron, B. Lefaudeux, M. Xu, P. Wang, V. Pai,
    M. Singh, V. Liptchinsky, I. Misra, A. Joulin *et al.*, “Self-supervised pretraining
    of visual features in the wild,” *arXiv preprint arXiv:2103.01988*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kolesnikov et al. [2020] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung,
    S. Gelly, and N. Houlsby, “Big transfer (bit): General visual representation learning,”
    in *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part V 16*.   Springer, 2020, pp. 491–507.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2019] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,
    “Language models are unsupervised multitask learners,” *OpenAI blog*, vol. 1,
    no. 8, p. 9, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
    A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,
    T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen,
    E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,
    I. Sutskever, and D. Amodei, “Language models are few-shot learners,” in *Advances
    in Neural Information Processing Systems*, vol. 33, 2020, pp. 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2020] Y. Zhu, T. Gao, L. Fan, S. Huang, M. Edmonds, H. Liu, F. Gao,
    C. Zhang, S. Qi, Y. N. Wu *et al.*, “Dark, beyond deep: A paradigm shift to cognitive
    ai with humanlike common sense,” *Engineering*, vol. 6, no. 3, pp. 310–345, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park and Kwak [2018] S. Park and N. Kwak, “3d human pose estimation with relational
    networks,” in *BMVC*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sarafianos et al. [2016] N. Sarafianos, B. Boteanu, B. Ionescu, and I. A. Kakadiaris,
    “3d human pose estimation: A review of the literature and analysis of covariates,”
    *Computer Vision and Image Understanding*, vol. 152, pp. 1–20, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2020a] Y. Chen, Y. Tian, and M. He, “Monocular human pose estimation:
    A survey of deep learning-based methods,” *Computer Vision and Image Understanding*,
    vol. 192, p. 102897, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2020] C. Zheng, W. Wu, T. Yang, S. Zhu, C. Chen, R. Liu, J. Shen,
    N. Kehtarnavaz, and M. Shah, “Deep learning-based human pose estimation: A survey,”
    *arXiv preprint arXiv:2012.13392*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garcia-Garcia et al. [2018] A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez,
    P. Martinez-Gonzalez, and J. Garcia-Rodriguez, “A survey on deep learning techniques
    for image and video semantic segmentation,” *Applied Soft Computing*, vol. 70,
    pp. 41–65, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hao et al. [2020] S. Hao, Y. Zhou, and Y. Guo, “A brief survey on semantic segmentation
    with deep learning,” *Neurocomputing*, vol. 406, pp. 302–321, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minaee et al. [2021] S. Minaee, Y. Y. Boykov, F. Porikli, A. J. Plaza, N. Kehtarnavaz,
    and D. Terzopoulos, “Image segmentation using deep learning: A survey,” *IEEE
    Transactions on Pattern Analysis and Machine Intelligence*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mou et al. [2019] L. Mou, Y. Hua, and X. X. Zhu, “A relation-augmented fully
    convolutional network for semantic segmentation in aerial scenes,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 12 416–12 425.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Battaglia et al. [2016] P. Battaglia, R. Pascanu, M. Lai, D. Jimenez Rezende,
    and k. kavukcuoglu, “Interaction networks for learning about objects, relations
    and physics,” in *Advances in Neural Information Processing Systems*, vol. 29,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2018] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural
    networks,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2018, pp. 7794–7803.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. [2018] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei, “Relation networks
    for object detection,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2018, pp. 3588–3597.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2019] Y. Chen, M. Rohrbach, Z. Yan, Y. Shuicheng, J. Feng, and
    Y. Kalantidis, “Graph-based global reasoning networks,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019, pp. 433–442.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2018] C. Sun, A. Shrivastava, C. Vondrick, K. Murphy, R. Sukthankar,
    and C. Schmid, “Actor-centric relation network,” in *Proceedings of the European
    Conference on Computer Vision (ECCV)*, 2018, pp. 318–334.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patacchiola and Storkey [2020] M. Patacchiola and A. J. Storkey, “Self-supervised
    relational reasoning for representation learning,” in *Advances in Neural Information
    Processing Systems*, vol. 33, 2020, pp. 4003–4014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zambaldi et al. [2018] V. Zambaldi, D. Raposo, A. Santoro, V. Bapst, Y. Li,
    I. Babuschkin, K. Tuyls, D. Reichert, T. Lillicrap, E. Lockhart *et al.*, “Deep
    reinforcement learning with relational inductive biases,” in *International Conference
    on Learning Representations*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gutmann and Hyvärinen [2010] M. Gutmann and A. Hyvärinen, “Noise-contrastive
    estimation: A new estimation principle for unnormalized statistical models,” in
    *Proceedings of the Thirteenth International Conference on Artificial Intelligence
    and Statistics*.   JMLR Workshop and Conference Proceedings, 2010, pp. 297–304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyvärinen and Morioka [2016] A. Hyvärinen and H. Morioka, “Unsupervised feature
    extraction by time-contrastive learning and nonlinear ica,” in *Advances in Neural
    Information Processing Systems*, vol. 29, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oord et al. [2018] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning
    with contrastive predictive coding,” *arXiv preprint arXiv:1807.03748*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2020b] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple
    framework for contrastive learning of visual representations,” in *International
    conference on machine learning*.   PMLR, 2020, pp. 1597–1607.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2020] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast
    for unsupervised visual representation learning,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2020, pp. 9729–9738.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. [2013] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
    J. Dean, “Distributed representations of words and phrases and their compositionality,”
    in *Advances in Neural Information Processing Systems*, vol. 26, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saunshi et al. [2019] N. Saunshi, O. Plevrakis, S. Arora, M. Khodak, and H. Khandeparkar,
    “A theoretical analysis of contrastive unsupervised representation learning,”
    in *International Conference on Machine Learning*.   PMLR, 2019, pp. 5628–5637.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Klein and Nabi [2020] T. Klein and M. Nabi, “Contrastive self-supervised learning
    for commonsense reasoning,” in *Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics*, jul 2020, pp. 7517–7523.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schneider et al. [2019] S. Schneider, A. Baevski, R. Collobert, and M. Auli,
    “wav2vec: Unsupervised Pre-Training for Speech Recognition,” in *Proc. Interspeech*,
    2019, pp. 3465–3469.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kreuk et al. [2020] F. Kreuk, J. Keshet, and Y. Adi, “Self-supervised contrastive
    learning for unsupervised phoneme segmentation,” in *Proc. Interspeech*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Al-Tahan and Mohsenzadeh [2021] H. Al-Tahan and Y. Mohsenzadeh, “Clar: Contrastive
    learning of auditory representations,” in *International Conference on Artificial
    Intelligence and Statistics*.   PMLR, 2021, pp. 2530–2538.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf et al. [2020] T. Kipf, E. van der Pol, and M. Welling, “Contrastive learning
    of structured world models,” in *International Conference on Learning Representations*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laskin et al. [2020] M. Laskin, A. Srinivas, and P. Abbeel, “Curl: Contrastive
    unsupervised representations for reinforcement learning,” in *International Conference
    on Machine Learning*.   PMLR, 2020, pp. 5639–5650.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2021] G. Liu, C. Zhang, L. Zhao, T. Qin, J. Zhu, L. Jian, N. Yu,
    and T.-Y. Liu, “Return-based contrastive representation learning for reinforcement
    learning,” in *International Conference on Learning Representations*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nie et al. [2020] W. Nie, Z. Yu, L. Mao, A. B. Patel, Y. Zhu, and A. Anandkumar,
    “Bongard-logo: A new benchmark for human-level concept learning and reasoning,”
    *Advances in Neural Information Processing Systems*, vol. 33, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Detailed results]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional results on particular configurations from RAVEN [[16](#bib.bib16)]
    and I-RAVEN [[17](#bib.bib17)] datasets are provided in Table [VI](#A0.T6 "TABLE
    VI ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s
    Progressive Matrices") and Table [VII](#A0.T7 "TABLE VII ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices"), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: RAVEN accuracy. Accuracy on the test split of the RAVEN dataset [[16](#bib.bib16)].
    Mean denotes the mean accuracy for all configurations. The Left-Right configuration
    is denoted as L-R, Up-Down as U-D, Out-InCenter as O-IC and Out-InGrid as O-IG.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Test accuracy (%) |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | Center | 2x2Grid | 3x3Grid | L-R | U-D | O-IC | O-IG |'
  prefs: []
  type: TYPE_TB
- en: '| CNN LSTM [[16](#bib.bib16)] | 13.1 | 13.2 | 14.1 | 13.7 | 12.8 | 12.4 | 12.2
    | 13.0 |'
  prefs: []
  type: TYPE_TB
- en: '| CNN LSTM + DRT [[16](#bib.bib16)] | 14.0 | 14.3 | 15.1 | 14.1 | 13.8 | 13.2
    | 14.0 | 13.3 |'
  prefs: []
  type: TYPE_TB
- en: '| WReN [[16](#bib.bib16)] | 14.7 | 13.1 | 28.6 | 28.3 | 7.5 | 6.3 | 8.4 | 10.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| WReN + DRT [[16](#bib.bib16)] | 15.0 | 15.4 | 23.3 | 29.5 | 7.0 | 8.4 | 8.9
    | 12.4 |'
  prefs: []
  type: TYPE_TB
- en: '| ARNe [[31](#bib.bib31)] | 19.7 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MCPT [[28](#bib.bib28)] | 28.5 | 35.9 | 26.0 | 27.2 | 29.3 | 27.4 | 33.1
    | 20.7 |'
  prefs: []
  type: TYPE_TB
- en: '| WReN-Tag-Aux [[20](#bib.bib20)] | 34.0 | 58.4 | 38.9 | 37.7 | 21.6 | 19.7
    | 38.8 | 22.6 |'
  prefs: []
  type: TYPE_TB
- en: '| NCD [[30](#bib.bib30)] | 37.0 | 45.5 | 35.5 | 39.5 | 34.9 | 33.4 | 40.3 |
    30.0 |'
  prefs: []
  type: TYPE_TB
- en: '| CNN MLP [[16](#bib.bib16)] | 37.0 | 33.6 | 30.3 | 33.5 | 39.4 | 41.3 | 43.2
    | 37.5 |'
  prefs: []
  type: TYPE_TB
- en: '| CNN MLP + DRT [[16](#bib.bib16)] | 39.4 | 37.3 | 30.1 | 34.6 | 45.5 | 45.5
    | 45.9 | 37.5 |'
  prefs: []
  type: TYPE_TB
- en: '| PRD [[29](#bib.bib29)] | 50.7 | 74.6 | 38.7 | 34.9 | 60.8 | 60.3 | 62.5 |
    23.4 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-18 [[16](#bib.bib16)] | 53.4 | 52.8 | 41.9 | 44.3 | 58.8 | 60.2 |
    63.2 | 53.1 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-18 + DRT [[16](#bib.bib16)] | 59.6 | 58.1 | 46.5 | 50.4 | 65.8 | 67.1
    | 69.1 | 60.1 |'
  prefs: []
  type: TYPE_TB
- en: '| LEN [[22](#bib.bib22)] | 72.9 | 80.2 | 57.5 | 62.1 | 73.5 | 81.2 | 84.4 |
    71.5 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-18 [[28](#bib.bib28)] | 77.2 | 72.8 | 57.0 | 62.7 | 91.0 | 89.6 |
    88.4 | 78.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LEN + TM [[22](#bib.bib22)] | 78.3 | 82.3 | 58.5 | 64.3 | 87.0 | 85.5 | 88.9
    | 81.9 |'
  prefs: []
  type: TYPE_TB
- en: '| MXGNet [[36](#bib.bib36)] | 83.9 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MRNet [[18](#bib.bib18)] | 84.0 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-50 + pre-train [[28](#bib.bib28)] | 86.3 | 89.5 | 66.6 | 68.0 | 97.9
    | 98.2 | 96.6 | 87.2 |'
  prefs: []
  type: TYPE_TB
- en: '| CoPINet [[20](#bib.bib20)] | 91.4 | 95.1 | 77.5 | 78.9 | 99.1 | 99.7 | 98.5
    | 91.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SCL [[33](#bib.bib33)] | 91.6 | 98.1 | 91.0 | 82.5 | 96.8 | 96.5 | 96.0 |
    80.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Rel-Base [[34](#bib.bib34)] | 91.7 | 97.6 | 85.9 | 86.9 | 93.5 | 96.5 | 97.6
    | 83.8 |'
  prefs: []
  type: TYPE_TB
- en: '| CoPINet + AL [[21](#bib.bib21)] | 93.5 | 98.6 | 80.5 | 83.2 | 99.7 | 99.8
    | 99.4 | 93.3 |'
  prefs: []
  type: TYPE_TB
- en: '| DCNet [[35](#bib.bib35)] | 93.6 | 97.8 | 81.7 | 86.7 | 99.8 | 99.8 | 99.0
    | 91.5 |'
  prefs: []
  type: TYPE_TB
- en: '| CoPINet + ACL [[21](#bib.bib21)] | 93.7 | 98.4 | 81.0 | 84.0 | 99.7 | 99.8
    | 99.4 | 93.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Rel-AIR [[34](#bib.bib34)] | 94.1 | 99.0 | 92.4 | 87.1 | 98.7 | 97.9 | 98.0
    | 85.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Context-blind ResNet [[17](#bib.bib17)] | 71.9 | - | - | - | - | - | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| Context-blind SCL [[33](#bib.bib33)] | 94.2 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Context-blind CoPINet [[17](#bib.bib17)] | 94.2 | - | - | - | - | - | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| Human [[16](#bib.bib16)] | 84.4 | 95.4 | 81.8 | 79.6 | 86.4 | 81.8 | 86.4
    | 81.8 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VII: I-RAVEN accuracy. Accuracy on the test split of the I-RAVEN dataset [[17](#bib.bib17)].
    Mean denotes the mean accuracy for all configurations. The Left-Right configuration
    is denoted as L-R, Up-Down as U-D, Out-InCenter as O-IC and Out-InGrid as O-IG.
    ²²footnotemark: 2MRNet was evaluated on RAVEN-FAIR in [[18](#bib.bib18)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Test accuracy (%) |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | Center | 2x2Grid | 3x3Grid | L-R | U-D | O-IC | O-IG |'
  prefs: []
  type: TYPE_TB
- en: '| CNN LSTM [[17](#bib.bib17)] | 18.9 | 26.2 | 16.7 | 15.1 | 14.6 | 16.5 | 21.9
    | 21.1 |'
  prefs: []
  type: TYPE_TB
- en: '| WReN [[17](#bib.bib17)] | 23.8 | 29.4 | 26.8 | 23.5 | 21.9 | 21.4 | 22.5
    | 21.5 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-18 [[17](#bib.bib17)] | 40.3 | 44.7 | 29.3 | 27.9 | 51.2 | 47.4 |
    46.2 | 35.8 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-18 + DRT [[17](#bib.bib17)] | 40.4 | 46.5 | 28.8 | 27.3 | 50.1 | 49.8
    | 46.0 | 34.2 |'
  prefs: []
  type: TYPE_TB
- en: '| LEN [[17](#bib.bib17)] | 41.4 | 56.4 | 31.7 | 29.7 | 44.2 | 44.2 | 52.1 |
    31.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Wild ResNet [[17](#bib.bib17)] | 44.3 | 50.9 | 33.1 | 30.8 | 53.1 | 52.6
    | 50.9 | 38.7 |'
  prefs: []
  type: TYPE_TB
- en: '| CoPINet [[17](#bib.bib17)] | 46.1 | 54.4 | 36.8 | 31.9 | 51.9 | 52.5 | 52.2
    | 42.8 |'
  prefs: []
  type: TYPE_TB
- en: '| NCD [[30](#bib.bib30)] | 48.2 | 60.0 | 31.2 | 30.0 | 58.9 | 57.2 | 62.4 |
    39.0 |'
  prefs: []
  type: TYPE_TB
- en: '| CoPINet MLCL+DA [[19](#bib.bib19)] | 57.1 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SRAN [[17](#bib.bib17)] | 60.8 | 78.2 | 50.1 | 42.4 | 70.1 | 70.3 | 68.2
    | 46.3 |'
  prefs: []
  type: TYPE_TB
- en: '| SRAN MLCL+DA [[19](#bib.bib19)] | 73.3 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ²²footnotemark: 2MRNet [[18](#bib.bib18)] | 86.8 | 97.0 | 72.7 | 69.5 | 98.7
    | 98.9 | 97.6 | 73.3 |'
  prefs: []
  type: TYPE_TB
- en: '| SCL [[33](#bib.bib33)] | 95.0 | 99.0 | 96.2 | 89.5 | 97.9 | 97.1 | 97.6 |
    87.7 |'
  prefs: []
  type: TYPE_TB
- en: '| SCL MLCL+DA [[19](#bib.bib19)] | 96.8 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Context-blind SCL [[33](#bib.bib33)] | 12.2 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Context-blind ResNet [[17](#bib.bib17)] | 12.2 | - | - | - | - | - | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| Context-blind CoPINet [[17](#bib.bib17)] | 14.2 | - | - | - | - | - | - |
    - |'
  prefs: []
  type: TYPE_TB
