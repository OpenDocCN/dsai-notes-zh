- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:47:36'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2203.13450] A Comparative Survey of Deep Active Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2203.13450](https://ar5iv.labs.arxiv.org/html/2203.13450)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Comparative Survey of Deep Active Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Xueying Zhan
  prefs: []
  type: TYPE_NORMAL
- en: City University of Hong Kong
  prefs: []
  type: TYPE_NORMAL
- en: xyzhan2-c@my.cityu.edu.hk &Qingzhong Wang
  prefs: []
  type: TYPE_NORMAL
- en: Baidu Research
  prefs: []
  type: TYPE_NORMAL
- en: wangqingzhong@baidu.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Kuan-Hao Huang'
  prefs: []
  type: TYPE_NORMAL
- en: University of California
  prefs: []
  type: TYPE_NORMAL
- en: khhuang@cs.ucla.edu
  prefs: []
  type: TYPE_NORMAL
- en: \ANDHaoyi Xiong
  prefs: []
  type: TYPE_NORMAL
- en: Baidu Research
  prefs: []
  type: TYPE_NORMAL
- en: xionghaoyi@baidu.com &Dejing Dou
  prefs: []
  type: TYPE_NORMAL
- en: Baidu Research
  prefs: []
  type: TYPE_NORMAL
- en: doudejing@baidu.com &Antoni B. Chan
  prefs: []
  type: TYPE_NORMAL
- en: City University of Hong Kong
  prefs: []
  type: TYPE_NORMAL
- en: abchan@cityu.edu.hk Work was completed while the first author was at Baidu Research.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While deep learning (DL) is data-hungry and usually relies on extensive labeled
    data to deliver good performance, Active Learning (AL) reduces labeling costs
    by selecting a small proportion of samples from unlabeled data for labeling and
    training. Therefore, Deep Active Learning (DAL) has risen as a feasible solution
    for maximizing model performance under a limited labeling cost/budget in recent
    years. Although abundant methods of DAL have been developed and various literature
    reviews conducted, the performance evaluation of DAL methods under fair comparison
    settings is not yet available. Our work intends to fill this gap. In this work,
    We construct a DAL toolkit, *$\text{DeepAL}^{+}$*, by re-implementing 19 highly-cited
    DAL methods. We survey and categorize DAL-related works and construct comparative
    experiments across frequently used datasets and DAL algorithms. Additionally,
    we explore some factors (e.g., batch size, number of epochs in the training process)
    that influence the efficacy of DAL, which provides better references for researchers
    to design their DAL experiments or carry out DAL-related applications.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Blessed by the capacity of representation learning in an over-parameterized
    architecture, Deep Neural Networks (DNNs) have been used as significant workhorses
    in various machine learning (ML) tasks. While DNNs can work with extensive training
    datasets and deliver decent performance, collecting and annotating data to feed
    DNNs training becomes extremely expensive and time-consuming. On the other hand,
    given a large pool of unlabeled data, AL improves learning efficiency by selecting
    small subsets of samples for annotating and training [[76](#bib.bib76)]. In this
    way, a sweet spot appears at the intersection of DNNs and AL, where representation
    learning can be achieved with reduced labeling costs. Deep Active Learning (DAL)
    has been employed in various tasks, e.g., named entity recognition [[9](#bib.bib9),
    [62](#bib.bib62)], semantic parsing [[16](#bib.bib16)], object detection [[53](#bib.bib53),
    [23](#bib.bib23)], image segmentation [[6](#bib.bib6), [55](#bib.bib55)], counting
    [[81](#bib.bib81)], etc. Besides these applications, multiple unified DAL frameworks
    have been designed and perform well on various tasks [[57](#bib.bib57), [2](#bib.bib2),
    [50](#bib.bib50), [63](#bib.bib63)].
  prefs: []
  type: TYPE_NORMAL
- en: 'DAL originated from AL for classical ML tasks, which has been well studied
    in past years. The application of AL to classical ML tasks appear in a wealth
    of literature surveys [[59](#bib.bib59), [71](#bib.bib71), [19](#bib.bib19), [1](#bib.bib1),
    [17](#bib.bib17), [39](#bib.bib39), [74](#bib.bib74)] and comparative studies
    [[36](#bib.bib36), [56](#bib.bib56), [60](#bib.bib60), [68](#bib.bib68), [7](#bib.bib7),
    [65](#bib.bib65), [51](#bib.bib51), [49](#bib.bib49), [44](#bib.bib44), [80](#bib.bib80)].
    Some traditional AL methods for classical ML have been generalized to DL tasks
    [[69](#bib.bib69), [20](#bib.bib20), [3](#bib.bib3)]. Adapting AL methods to work
    well on classical ML tasks has several issues to overcome [[52](#bib.bib52)]:
    1) different from traditional AL methods that use fixed pre-processed features
    to calculate uncertainty/representativeness, in DL tasks, feature representations
    are jointly learned with DNNs. Therefore, feature representations are dynamically
    changing during DAL processes, and thus pairwise distances/similarities used by
    representativeness-based measures need to be re-computed in every stage, whereas
    for AL with classical ML tasks, these pairwise terms can be pre-computed. 2) DNNs
    are typically over-confident with their predictions and thus evaluating the uncertainty
    of unlabeled data might be unreliable. Ren et al. [[52](#bib.bib52)] conducted
    a comprehensive review of DAL, which systematically summarizes and categorizes
    $189$ existing works. Indeed it is a comprehensive study of DAL and guides new
    and experienced researchers who want to use it. However, due to the lack of experimental
    comparisons among various branches of DAL algorithms across different datasets/tasks,
    it is difficult for researchers to distinguish which DAL algorithms are suitable
    for which task. Our work aims to fill this gap.'
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we construct a DAL toolkit, called *$\text{DeepAL}^{+}$*, by re-implementing
    19 DAL methods surveyed in this paper. *$\text{DeepAL}^{+}$* is sequel to our
    previous work *DeepAL* [[27](#bib.bib27)]. Compared to *DeepAL*, which includes
    11 highly-cited DAL approaches prior to 2018, in *$\text{DeepAL}^{+}$*, 1) we
    upgraded and optimized some algorithms that already were implemented in *DeepAL*;
    2) we re-implemented more highly-cited DAL algorithms, most of which are proposed
    after 2018; 3) besides well-studied datasets adopted in *DeepAL* like *MNIST*
    [[14](#bib.bib14)], *CIFAR* [[38](#bib.bib38)] and *SVHN* [[45](#bib.bib45)],
    we integrated more complicated tasks in *$\text{DeepAL}^{+}$* like medical image
    analysis [[66](#bib.bib66), [31](#bib.bib31)] and object recognition with correlated
    backgrounds (containing spurious correlations) [[54](#bib.bib54)]. We conduct
    comparative experiments between a variety of DAL approaches based on *$\text{DeepAL}^{+}$*
    on multiple tasks and also explore factors of interest to researchers, such as
    the influence of batch size and the number of training epochs in each AL iteration,
    and timing-cost comparison. More descriptions of *$\text{DeepAL}^{+}$* are in
    Section B in Appendix.
  prefs: []
  type: TYPE_NORMAL
- en: We hope that our comparative study/benchmarking test brings authentic comparative
    evaluation for DAL, provides a quick look at which DAL models are more effective
    and what are the challenges and possible research directions in DAL, as well as
    offering guidelines for conducting fair comparative experiments for future DAL
    methods. More importantly, we expect that our *$\text{DeepAL}^{+}$* will contribute
    to the development of DAL since *$\text{DeepAL}^{+}$* is extensible, allowing
    easy incorporation of new basic tasks/datasets, new DAL algorithms, and new basic
    learned models. This makes the application of DAL to downstream tasks, and designing
    new DAL algorithms becomes easier. *$\text{DeepAL}^{+}$* is an ongoing process.
    We will keep expanding it by incorporating more basic tasks, models, and DAL algorithms.
    Our *$\text{DeepAL}^{+}$* is available on [https://github.com/SineZHAN/deepALplus](https://github.com/SineZHAN/deepALplus).
  prefs: []
  type: TYPE_NORMAL
- en: 2 DAL Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section provides an overview of highly-cited DAL methods in recent years,
    including the perspectives of querying strategies and techniques for enhancing
    DAL methods.
  prefs: []
  type: TYPE_NORMAL
- en: Problem Definition.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We only discuss pool-based AL, since most DAL approaches belong to this category.
    Pool-based AL selects most informative data iteratively from a large pool of unlabeled
    $i.i.d.$ data samples until either the basic learner(s) reaches a certain level
    of performance or a fixed budget is exhausted [[11](#bib.bib11)]. We consider
    a general process of DAL, taking classification tasks as example, where other
    tasks (e.g., image segmentation) follow the common definition of their tasks domain.
    We have an initial labeled set ${\mathcal{D}}_{l}=\{(\mathbf{x}_{j},y_{j})\}_{j=1}^{M}$
    and a large unlabeled data pool ${\mathcal{D}}_{u}=\{\mathbf{x}_{i},\}_{i=1}^{N}$,
    where $M\ll N$, $y_{i}\in\{0,1\}$ is the class label of $\mathbf{x}_{i}$ for binary
    classification, or $y_{i}\in\{1,...,k\}$ for multi-class classification. In each
    iteration, we select batch of samples ${\mathcal{D}}_{q}$ with batch size $b$
    from ${\mathcal{D}}_{u}$ based on basic learned model $\mathcal{M}$ and an acquisition
    function $\alpha(\mathbf{x},\mathcal{M})$, and query their labels from the oracle.
    Data samples are selected by ${\mathcal{D}}_{q}^{*}=\arg\max\nolimits_{\mathbf{x}\in{\mathcal{D}}_{u}}^{b}\alpha(\mathbf{x},\mathcal{M})$,
    where the superscript $b$ indicates selection of the top $b$ points. ${\mathcal{D}}_{l}$
    and ${\mathcal{D}}_{u}$ are then updated, and $\mathcal{M}$ is retrained on ${\mathcal{D}}_{l}$.
    DAL terminates when the budget $Q$ is exhausted or a desired model performance
    is reached.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Querying Strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DAL can be categorized into 3 branches from the perspective of querying strategy:
    uncertainty-based, representativeness/diversity-based and combined strategies,
    as shown in Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Querying Strategies ‣ 2 DAL Approaches
    ‣ A Comparative Survey of Deep Active Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad1ddfea7826a1bc9afa29b1437faf4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Categorization of DAL sampling/querying strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Uncertainty-based Querying Strategies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Uncertainty-based DAL selects data samples with high aleatoric uncertainty
    or epistemic uncertainty, where aleatoric uncertainty refers to the natural uncertainty
    in data due to influences on data generation processes that are inherently random.
    Epistemic uncertainty comes from the modeling/learning process and is caused by
    a lack of knowledge [[59](#bib.bib59), [58](#bib.bib58), [46](#bib.bib46), [30](#bib.bib30)].
    Many uncertainty-based DAL measures are adapted from pool-based AL techniques
    for classical ML tasks. Typical methods include:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Maximum Entropy (Entropy) [[61](#bib.bib61)] selects data $\mathbf{x}$ that
    maximize the predictive entropy $H_{\mathcal{M}}[y|\mathbf{x}]$: $\alpha_{\textbf{entropy}}(\mathbf{x},\mathcal{M})=H_{\mathcal{M}}[y|\mathbf{x}]=-\sum\nolimits_{k}p_{\mathcal{M}}(y=k|\mathbf{x})\log
    p_{\mathcal{M}}(y=k|\mathbf{x})$, where $p_{\mathcal{M}}(y|\mathbf{x})$ is the
    posterior label probability from the classifier $\mathcal{M}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Margin [[45](#bib.bib45)] selects data $\mathbf{x}$ whose two most likely labels
    $(\hat{y}_{1},\hat{y}_{2})$ have smallest difference in posterior probabilities:
    $\alpha_{\textbf{margin}}(\mathbf{x},\mathcal{M})=-[p_{\mathcal{M}}(\hat{y}_{1}|\mathbf{x})-p_{\mathcal{M}}(\hat{y}_{2}|\mathbf{x})]$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Least Confidence (LeastConf) [[69](#bib.bib69)] selects data $\mathbf{x}$ whose
    most likely label $\hat{y}$ has lowest posterior probability: $\alpha_{\textbf{LeastConf}}(\mathbf{x},\mathcal{M})=-p_{\mathcal{M}}(\hat{y}|\mathbf{x})$.
    A similar method is Variation Ratios (VarRatio) [[18](#bib.bib18)], which measures
    the lack of confidence like LeastConf: $\alpha_{\textbf{VarRatio}}(\mathbf{x},\mathcal{M})=1-p_{\mathcal{M}}(\hat{y}|\mathbf{x})$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bayesian Active Learning by Disagreements (BALD) [[26](#bib.bib26), [20](#bib.bib20)]
    chooses data points that are expected to maximize the information gained from
    the model parameters $\mathbf{\omega}$, i.e. the mutual information between predictions
    and model posterior: $\alpha_{\textbf{BALD}}(\mathbf{x},\mathcal{M})=H_{\mathcal{M}}[y|\mathbf{x}]-\mathbb{E}_{p(\mathbf{\omega}|D_{l})}[H_{\mathcal{M}}[y|\mathbf{x},\mathbf{\omega}]]$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mean Standard Deviation (MeanSTD) [[29](#bib.bib29)] maximizes the mean standard
    deviation of the predicted probabilities over all $k$ classes: $\alpha_{\textbf{MeanSTD}}(\mathbf{x},\mathcal{M})=\frac{1}{k}\sum\nolimits_{k}\sqrt{\text{Var}_{q(\omega)}[p(y=k|\mathbf{x},\omega)]}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Inspired by recent advances in generating adversarial examples, some DAL methods
    utilize adversarial attacks to rank the uncertainty/informativeness of each unlabeled
    data sample. The DeepFool Active Learning method (AdvDeepFool) [[15](#bib.bib15)]
    queries the unlabeled samples that are closest to their adversarial attacks (DeepFool).
    Specifically, $\alpha_{\textbf{AdvDeepFool}}(\mathbf{x},\mathcal{M})=\mathbf{r}_{\mathbf{x}}$,
    where $\mathbf{r}_{\mathbf{x}}$ is the minimal perturbation that causes the changing
    of labels, e.g., for binary classification, $\mathbf{r}_{\mathbf{x}}=\mathop{\mathrm{argmin}}_{\mathbf{r},~{}\mathcal{M}(\mathbf{x})\neq\mathcal{M}(\mathbf{x}+\mathbf{r})}-\tfrac{\mathcal{M}(\mathbf{x}+\mathbf{r})}{||\nabla\mathcal{M}(\mathbf{x}+\mathbf{r})||^{2}_{2}}\nabla\mathcal{M}(\mathbf{x}+\mathbf{r})$.
    DeepFool attack can be replaced by other attack methods, e.g., Basic Interactive
    Method (BIM) [[40](#bib.bib40)], called AdvBIM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative Adversarial Active Learning (GAAL) [[83](#bib.bib83)] synthesizes
    queries via Generative Adversarial Networks (GANs). In contrast to regular AL
    that selects points from the unlabeled data pool, GAAL generates images from GAN
    for querying human annotators. However, the generated data very close to the classifier
    decision boundary may be meaningless, and even human annotators could not distinguish
    its category. An improved approach called Bayesian Generative Active Deep Learning
    (BGADL) [[67](#bib.bib67)] combines active learning with data augmentation. BGADL
    utilizes typical Bayesian DAL approaches for its acquisition function (e.g., $\alpha_{\textbf{BALD}}$)
    and then trains a VAE-ACGAN to generate synthetic data samples to enlarge the
    training set. Other practical uncertainty-based measures include i) utilizing
    gradient: Wang et al. [[72](#bib.bib72)] found that gradient norm can effectively
    guide unlabeled data selection; that is, selecting unlabeled data of higher gradient
    norm can reduce the upper bound of the test loss. Another work that utilizes gradient
    is Batch Active learning by Diverse Gradient Embeddings (BADGE)[[2](#bib.bib2)]
    measures uncertainty as the gradient magnitude with respect to parameters in the
    output layer since DNNs are optimized using gradient-based methods like SGD. ii)
    Loss Prediction Loss (LPL) [[78](#bib.bib78)] uses a loss prediction strategy
    by attaching a small parametric module that is trained to predict the loss of
    unlabeled inputs with respect to the target model, by minimizing the loss prediction
    loss between predicted loss and target loss. LPL picks the top $b$ data samples
    with the highest predicted loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Representative/Diversity-based Querying Strategies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Representative/diversity-based strategies select batches of samples representative
    of the unlabeled set and are based on the intuition that the selected representative
    examples, once labeled, can act as a surrogate for the entire dataset [[2](#bib.bib2)].
    Clustering methods are widely used in representative-based strategies. A typical
    method is KMeans, which selects centroids by iteratively sampling points in proportion
    to their squared distances from the nearest previously selected centroid. Another
    widely adopted approach [[21](#bib.bib21), [57](#bib.bib57)] selects a batch of
    representative points based on a core set, which is a sub-sample of a dataset
    that can be used as a proxy for the full set. CoreSet is measured in the penultimate
    layer space $h(\mathbf{x})$ of the current model. Firstly, given ${\mathcal{D}}_{l}$,
    an example $\mathbf{x}_{u}$ is selected with the greatest distance to its nearest
    neighbor in the hidden space $u=\arg\max\nolimits_{\mathbf{x}_{i}\in{\mathcal{D}}_{u}}\min\nolimits_{\mathbf{x}_{j}\in{\mathcal{D}}_{l}}\Delta(h(\mathbf{x}_{i},\mathbf{x}_{j}))$.
    Sampling is then repeated until batch size $b$ is reached. In another method,
    Cluster-Margin [[12](#bib.bib12)] selects a diverse set of examples on which the
    model is least confident. It first runs hierarchical agglomerate clustering with
    average-linkage as pre-processing, and then selects an unlabeled subset with lowest
    margin scores (Margin), which is then filtered down to a diverse set with $b$
    samples. In contrast to CoreSet, Cluster-Margin only runs clustering once as pre-processing.
  prefs: []
  type: TYPE_NORMAL
- en: Point Processes are also adopted in representative-based DAL, e.g., Active-DPP
    [[4](#bib.bib4)]. A determinantal point process (DPP) captures diversity by constructing
    a pair-wise (dis)similarity matrix and calculating its determinant. BADGE also
    utilizes DPPs as a representative measure. Discriminative AL (DiscAL) [[22](#bib.bib22)]
    is a representative measure that, reminiscent of GANs, attempts to fool a discriminator
    that tries to distinguish between data coming from two different distributions
    (unlabeled/labeled). Variational Adversarial AL (VAAL) [[64](#bib.bib64)] learns
    a distribution of labeled data in latent space using a VAE and an adversarial
    network trained to discriminate between unlabeled and labeled data. The network
    is optimized using both reconstruction and adversarial losses. $\alpha_{\textbf{VAAL}}$
    is formed with the discriminator that estimates the probability that the data
    comes from the unlabeled data. Wasserstein Adversarial AL (WAAL) [[63](#bib.bib63)]
    searches the diverse unlabeled batch that also has larger diversity than the labeled
    samples through adversarial training by $\mathcal{H}$-divergence.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 Combined Querying Strategies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the demand for larger batch size (representative/diversity) and more
    precise decision boundaries for higher model performance (uncertainty) in DAL,
    combined strategies have become the dominant approaches to DAL. It aims to achieve
    a trade-off between uncertainty and representativeness/diversity in query selection.
    We mainly discuss the optimization methods with respect to multiple objectives
    (uncertainty, diversity, etc.) in this paper, including weighted-sum and two-stage
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Weighted-sum optimization is both simple and flexible, where the objective
    functions are summed up with weight $\beta$: $\alpha_{\textbf{weighted-sum}}=\alpha_{\textbf{uncertainty}}+\beta\alpha_{\textbf{representative}}$.
    However, two factors limit its usage in Combined DAL: 1) it introduces extra hyper-parameter
    $\beta$ for tuning; 2) unlike uncertainty-based measure that provide a single
    score per sample, representativeness is usually expressed in matrix form, which
    is not straightforward to convert into a single per-sample score. A example of
    weighted-sum optimization is Exploitation-Exploration [[77](#bib.bib77)] selects
    samples that are most uncertain and least redundant (exploitation), as well as
    most diverse (exploration). Specifically, in the exploitation step, $\alpha_{\textbf{exploitation}}=\alpha_{\textbf{entropy}}({\mathcal{D}}_{q},\mathcal{M})-\tfrac{\beta}{|{\mathcal{D}}_{q}|}\alpha_{\textbf{similarity}}({\mathcal{D}}_{q})$.
    Using DPPs is a natural way to balance uncertainty score and pairwise diversity
    well without introducing additional hyper-parameters [[4](#bib.bib4), [2](#bib.bib2),
    [79](#bib.bib79)]. However, sampling from DPPs in DAL is not trivial since DPPs
    have a time complexity of $O(N^{3})$.'
  prefs: []
  type: TYPE_NORMAL
- en: Two-stage (multi-stage) optimization is a popular combined strategy, Each stage
    refines the previous stage’s selections using different criteria. E.g., stage
    1 selects an informative subset with a size larger than $b$, and then stage 2
    selects $b$ samples with maximum diversity. WAAL uses two stage optimization for
    implementing discriminative learning via training a DNN for discriminative features
    in stage 1, and making batch selections in stage 2 [[63](#bib.bib63)]. BADGE computes
    gradient embeddings for each unlabeled data samples in stage 1, then clusters
    by KMeans++ in stage 2 [[2](#bib.bib2)]. Diverse mini-Batch Active Learning (DBAL)
    [[82](#bib.bib82)] first pre-filters unlabeled data pool to the top $\rho b$ most
    informative/uncertain examples ($\rho$ is pre-filter factor), then clusters these
    samples to $b$ clusters with (weighted) KMeans and selects $b$ samples closest
    to the cluster centers.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Enhancing of DAL Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Section [2.1](#S2.SS1 "2.1 Querying Strategies ‣ 2 DAL Approaches ‣ A Comparative
    Survey of Deep Active Learning"), many highly-cited DAL methods have designed
    acquisition functions, e.g., Entropy, CoreSet and BADGE. These methods are easily
    adapted to various tasks since they only involve the data selection process, not
    the training process of the backbone. However, how well these DAL methods can
    perform is limited, e.g., one might not exceed the performance of training on
    full data. Some DAL models are proposed for enhancing DAL methods that can break
    the limitation, which can be categorized into two branches: data and model aspect.
    The data aspect includes data augmentation and pseudo labeling, while the model
    aspect includes attaching extra networks, modifying loss functions, and ensemble.
    Due to limited space, we exclude related joint tasks that modify DAL methods like
    semi-/ self-/un-/supervised, transfer, or reinforcement learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Data aspect.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Pseudo-labeling utilizes large-scale unlabeled data for training. Cost-Effective
    AL (CEAL) [[70](#bib.bib70)] assigns high-confident (low entropy $H_{\mathcal{M}}[y|\mathbf{x}]$)
    pseudo labels predicted by $\mathcal{M}$ for training in the next iteration. However,
    this introduces new hyperparameters to threshold the prediction confidence, which,
    if badly tuned, can corrupt the training set with wrong labels [[15](#bib.bib15)].
    Data augmentation uses labeled samples for enlarging the training set. However,
    data augmentation might waste computational resources because it indiscriminately
    generates samples that are not guaranteed to be informative. AdvDeepFool adds
    adversarial samples to the training set [[15](#bib.bib15)], while BGADL employs
    ACGAN and Bayesian Data Augmentation for producing new artificial samples that
    are as informative as the selected samples [[67](#bib.bib67)].
  prefs: []
  type: TYPE_NORMAL
- en: Model aspect.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some researchers utilize extra modules to improve target model performance and
    make selections in DAL. For instance, LPL jointly learns the target backbone model
    and loss prediction model, which can predict when the target model is likely to
    produce a wrong prediction. Choi et al. [[10](#bib.bib10)] constructs mixture
    density networks to estimate a probability distribution for each localization
    and classification head’s output for the object detection task. Revising the loss
    function of the target model is also promising. WAAL adopts min-max loss by leveraging
    the unlabeled data for better distinguishing labeled and unlabeled samples [[63](#bib.bib63)].
    Another approach is ensemble learning. DNNs use a softmax layer to obtain the
    label’s posterior probability and tend to be overconfident when calculating the
    uncertainty. To increase uncertainty, Gal et al. [[20](#bib.bib20)] leverages
    Monte-Carlo (MC) Dropout, where uncertainty in the weights $\omega$ induces prediction
    uncertainty by marginalizing over the approximate posterior using MC integration.
    It can be viewed as an ensemble of models sampled with dropouts. Beluch et al.
    [[3](#bib.bib3)] found that ensembles of multiple classifiers perform better than
    MC Dropout for calculating uncertainty scores.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Comparative Experiments of DAL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We conduct comparisons on $19$ methods across $10$ public available datasets,
    in which these datasets are selected with reference of [[52](#bib.bib52)] (see
    Table 2 in [[52](#bib.bib52)]) and highly-cited DAL papers.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Considering some DAL approaches currently only support computer vision tasks
    like VAAL, for consistency and fairness of our experiments, we adopt the image
    classification tasks, similar to most DAL papers. We use: *MNIST* [[14](#bib.bib14)],
    *FashionMNIST* [[75](#bib.bib75)], *EMNIST* [[13](#bib.bib13)], *SVHN* [[45](#bib.bib45)],
    *CIFAR10*, *CIFAR100* [[38](#bib.bib38)] and *TinyImageNet* [[41](#bib.bib41)].
    We construct an imbalanced dataset based on *CIFAR10*, called *CIFAR10-imb*, which
    sub-samples the training set with ratios of 1:2:$\cdots$:10 for classes 0 through
    9\. We also consider medical imaging analysis tasks, including Breast Cancer Histopathological
    Image Classification (*BreakHis*) [[66](#bib.bib66)] and Chest X-Ray Pneuomonia
    classification (Pneumonia-MNIST) [[31](#bib.bib31)]. Additionally, we adopted
    an object recognition dataset with correlated backgrounds (*Waterbird*) [[54](#bib.bib54),
    [35](#bib.bib35)]. This dataset contains waterbird and landbird classes, which
    are manually mixed to water and land backgrounds. It is challenging since DNNs
    might spuriously rely on the background instead of learning to recognize the object
    semantics.'
  prefs: []
  type: TYPE_NORMAL
- en: DAL methods.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We test Random Sampling (Random), Entropy, Margin, LeastConf and their MC Dropout
    versions [[3](#bib.bib3)] (denoted as EntropyD, MarginD, LeastConfD, respectively),
    BALD, MeanSTD, VarRatio, CEAL(Entropy), KMeans, the greedy version of CoreSet
    (denoted as KCenter), BADGE, AdversarialBIM, WAAL, VAAL, and LPL. For KMeans,
    considering that we need to cluster large amounts of data, the original KMeans
    implementation based on the scikit-learn library [[48](#bib.bib48)] will be too
    time-consuming on large-scale unlabeled data pools. Thus, to save the time cost
    and let our *$\text{DeepAL}^{+}$* be more adaptable to DL tasks, we implemented
    KMeans with GPU (KMeans (GPU)) based on the faiss library [[28](#bib.bib28)].
  prefs: []
  type: TYPE_NORMAL
- en: For all AL methods, we employed ResNet18 (w/o pre-training) [[24](#bib.bib24)]
    as the basic learner. For a fair comparison, consistent experimental settings
    of the basic classifier are used across all DAL methods. We run these experiments
    using *$\text{DeepAL}^{+}$* toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Experimental protocol.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We repeat each experiment for $3$ trials with random splits of the initial labeled
    and unlabeled pool (using the same random seed) and report average testing performance.
    For evaluation metrics, for brevity, we report *overall performance* using *area
    under the budget curve (AUBC)* [[80](#bib.bib80), [79](#bib.bib79)], where the
    performance-budget curve is generated by evaluating the DAL method for varying
    budgets (e.g., accuracy vs. budget). Higher AUBC values indicate better overall
    performance. We also report the final accuracy (F-acc), which is the accuracy
    after the budget $Q$ is exhausted. More details of experimental settings (i.e.,
    datasets, implementations) are in Section D in Appendix.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | *MNIST* | *FashionMNIST* | *EMNIST* | *SVHN* | *PneumoniaMNIST* |'
  prefs: []
  type: TYPE_TB
- en: '|  | Model | AUBC | F-acc | AUBC | F-acc | AUBC | F-acc | AUBC | F-acc | AUBC
    | F-acc |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | $-$ | $0.9916$ | $-$ | $0.9120$ | $-$ | $0.8684$ | $-$ | $0.9190$
    | $-$ | $0.9039$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Random | $0.9570$ | $0.9738$ | $0.8313$ | $0.8434$ | $0.8057$ | $0.8377$
    | $0.8110$ | $0.8806$ | $0.8283$ | $\mathbf{0.9077}$ |'
  prefs: []
  type: TYPE_TB
- en: '|   Unc | LeastConf | $0.9677$ | $0.9892$ | $0.8377$ | $0.8820$ | $0.8113$
    | $0.8479$ | $0.8350$ | $0.9094$ | $0.8520$ | $\mathbf{0.9097}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LeastConfD | $0.9750$ | $0.9915$ | $0.8450$ | $0.8744$ | $0.8117$ | $0.8483$
    | $0.8320$ | $0.9083$ | $0.8243$ | $0.8654$ |'
  prefs: []
  type: TYPE_TB
- en: '| Margin | $0.9733$ | $0.9881$ | $0.8427$ | $0.8772$ | $0.8103$ | $0.8468$
    | $0.8373$ | $0.9138$ | $0.8580$ | $0.8859$ |'
  prefs: []
  type: TYPE_TB
- en: '| MarginD | $0.9703$ | $0.9899$ | $0.8417$ | $0.8756$ | $0.8197$ | $0.8472$
    | $0.8357$ | $0.9104$ | $0.8230$ | $\mathbf{0.9149}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Entropy | $0.9723$ | $0.9883$ | $0.8397$ | $0.8660$ | $0.8090$ | $0.8458$
    | $0.8297$ | $0.9099$ | $0.8570$ | $\mathbf{0.9132}$ |'
  prefs: []
  type: TYPE_TB
- en: '| EntropyD | $0.9683$ | $0.9887$ | $0.8417$ | $0.8784$ | $0.8167$ | $0.8507$
    | $0.8290$ | $0.9091$ | $0.8177$ | $0.8710$ |'
  prefs: []
  type: TYPE_TB
- en: '| BALD | $0.9697$ | $0.9885$ | $0.8423$ | $0.8888$ | $0.8197$ | $0.8448$ |
    $0.8333$ | $0.9020$ | $0.8270$ | $\mathbf{0.9204}$ |'
  prefs: []
  type: TYPE_TB
- en: '| MeanSTD | $0.9713$ | $0.9735$ | $0.8457$ | $0.8766$ | $0.8110$ | $0.8426$
    | $0.8323$ | $0.9087$ | $0.7827$ | $0.8802$ |'
  prefs: []
  type: TYPE_TB
- en: '| VarRatio | $0.9717$ | $0.9841$ | $0.8410$ | $0.8754$ | $0.8107$ | $0.8497$
    | $0.8357$ | $0.9079$ | $0.8530$ | $0.8672$ |'
  prefs: []
  type: TYPE_TB
- en: '| CEAL(Entropy) | $0.9787$ | $0.9889$ | $0.8477$ | $0.8826$ | $0.8163$ | $0.8459$
    | $0.8430$ | $0.9142$ | $0.8543$ | $\mathbf{0.9179}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Repr/Div | KMeans | $0.9640$ | $0.9813$ | $0.8260$ | $0.8525$ | $0.7903$
    | $0.8264$ | $0.8027$ | $0.8671$ | $0.8243$ | $\mathbf{0.9044}$ |'
  prefs: []
  type: TYPE_TB
- en: '| KMeans (GPU) | $0.9637$ | $0.9747$ | $0.8343$ | $0.8657$ | $0.7990$ | $0.8362$
    | $0.8120$ | $0.8688$ | $0.8333$ | $\mathbf{0.9155}$ |'
  prefs: []
  type: TYPE_TB
- en: '| KCenter | $0.9740$ | $0.9877$ | $0.8353$ | $0.8466$ | $*$ | $*$ | $0.8283$
    | $0.9000$ | $0.8130$ | $\mathbf{0.9189}$ |'
  prefs: []
  type: TYPE_TB
- en: '| VAAL | $0.9623$ | $0.9573$ | $0.8297$ | $0.8535$ | $0.8027$ | $0.8363$ |
    $0.8117$ | $0.8813$ | $0.8393$ | $0.9064$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | BADGE(KMeans++) | $0.9707$ | $0.9904$ | $0.8437$ | $0.8662$ | $*$ | $*$
    | $0.8377$ | $0.9057$ | $0.8340$ | $\mathbf{0.9066}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Enhance | AdvBIM | $0.9680$ | $0.9840$ | $0.8437$ | $0.8729$ | # | # | #
    | # | $0.8297$ | $0.9197$ |'
  prefs: []
  type: TYPE_TB
- en: '| LPL | $0.8913$ | $0.9732$ | $0.7600$ | $0.8471$ | $0.5640$ | $0.6474$ | $0.8737$
    | $\mathbf{0.9452}$ | $0.8593$ | $\mathbf{0.9346}$ |'
  prefs: []
  type: TYPE_TB
- en: '| WAAL | $0.9890$ | $\mathbf{0.9946}$ | $0.8703$ | $0.8984$ | $0.8293$ | $0.8423$
    | $0.8603$ | $0.9135$ | $0.9663$ | $\mathbf{0.9564}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | *CIFAR10* | *CIFAR100* | *CIFAR10-imb* | *Tiny ImageNet* | *BreakHis*
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Full | $-$ | $0.8793$ | $-$ | $0.6062$ | $-$ | $0.8036$ | $-$ | $0.4583$
    | $-$ | $0.8306$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Random | $0.7967$ | $0.8679$ | $0.4667$ | $0.5903$ | $0.7103$ | $\mathbf{0.8105}$
    | $0.2577$ | $0.3544$ | $0.8010$ | $0.8150$ |'
  prefs: []
  type: TYPE_TB
- en: '|   Unc | LeastConf | $0.8150$ | $0.8785$ | $0.4747$ | $\mathbf{0.6072}$ |
    $0.7330$ | $0.8022$ | $0.2417$ | $0.3470$ | $0.8213$ | $0.8302$ |'
  prefs: []
  type: TYPE_TB
- en: '| LeastConfD | $0.8137$ | $\mathbf{0.8825}$ | $0.4730$ | $0.5997$ | $0.7323$
    | $\mathbf{0.8065}$ | $0.2620$ | $0.3698$ | $0.8140$ | $\mathbf{0.8313}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Margin | $0.8153$ | $\mathbf{0.8834}$ | $0.4790$ | $0.6010$ | $0.7367$ |
    $0.8029$ | $0.2557$ | $0.3611$ | $0.8217$ | $0.8289$ |'
  prefs: []
  type: TYPE_TB
- en: '| MarginD | $0.8140$ | $\mathbf{0.8837}$ | $0.4777$ | $0.6000$ | $0.7260$ |
    $\mathbf{0.8128}$ | $0.2607$ | $0.3541$ | $0.8253$ | $\mathbf{0.8364}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Entropy | $0.8130$ | $0.8784$ | $0.4693$ | $0.6048$ | $0.7320$ | $\mathbf{0.8187}$
    | $0.2343$ | $0.3346$ | $0.8213$ | $0.8251$ |'
  prefs: []
  type: TYPE_TB
- en: '| EntropyD | $0.8140$ | $0.8787$ | $0.4677$ | $0.6004$ | $0.7317$ | $0.7963$
    | $0.2627$ | $0.3716$ | $0.8017$ | $0.8115$ |'
  prefs: []
  type: TYPE_TB
- en: '| BALD | $0.8103$ | $0.8762$ | $0.4760$ | $0.5942$ | $0.7210$ | $0.7927$ |
    $0.2623$ | $0.3648$ | $0.8147$ | $0.8296$ |'
  prefs: []
  type: TYPE_TB
- en: '| MeanSTD | $0.8087$ | $\mathbf{0.8821}$ | $0.4717$ | $0.5963$ | $0.7203$ |
    $0.7996$ | $0.2510$ | $0.3551$ | $0.8053$ | $0.8202$ |'
  prefs: []
  type: TYPE_TB
- en: '| VarRatio | $0.8150$ | $08780$ | $0.4747$ | $0.5959$ | $0.7353$ | $\mathbf{0.8165}$
    | $0.2407$ | $0.3426$ | $0.8197$ | $0.8264$ |'
  prefs: []
  type: TYPE_TB
- en: '| CEAL(Entropy) | $0.8150$ | $\mathbf{0.8794}$ | $0.4693$ | $0.6043$ | $0.7327$
    | $\mathbf{0.8187}$ | $0.2347$ | $0.3400$ | $0.8163$ | $0.8181$ |'
  prefs: []
  type: TYPE_TB
- en: '|   Repr/Div | KMeans | $0.7910$ | $0.8713$ | $0.4570$ | $0.5834$ | $0.7070$
    | $0.7908$ | $0.2447$ | $0.3385$ | $0.8203$ | $\mathbf{0.8394}$ |'
  prefs: []
  type: TYPE_TB
- en: '| KMeans (GPU) | $0.7977$ | $0.8718$ | $0.4687$ | $0.5842$ | $0.7140$ | $0.7921$
    | $0.1340$ | $0.2288$ | $0.8140$ | $\mathbf{0.8323}$ |'
  prefs: []
  type: TYPE_TB
- en: '| KCenter | $0.8047$ | $0.8741$ | $0.4770$ | $0.5993$ | $0.7233$ | $0.7826$
    | $0.2540$ | $0.346$ | $0.8027$ | $0.8289$ |'
  prefs: []
  type: TYPE_TB
- en: '| VAAL | $0.7973$ | $0.8679$ | $0.4693$ | $0.5870$ | $0.7113$ | $0.7950$ |
    $0.1313$ | $0.2191$ | $0.8197$ | $\mathbf{0.8344}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | BADGE(KMeans++) | $0.8143$ | $\mathbf{0.8794}$ | $0.4803$ | $0.6034$ |
    $0.7347$ | $\mathbf{0.8126}$ | # | # | $0.8343$ | $\mathbf{0.8470}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Enhance | AdvBIM | $0.7997$ | $0.8750$ | $0.4713$ | $0.5855$ | # | # | #
    | # | $0.8240$ | $\mathbf{0.8337}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LPL | $0.8220$ | $\mathbf{0.9028}$ | $0.4640$ | $\mathbf{0.6369}$ | $0.7477$
    | $\mathbf{0.8478}$ | $0.0090$ | $0.0051$ | $0.8277$ | $\mathbf{0.8316}$ |'
  prefs: []
  type: TYPE_TB
- en: '| WAAL | $0.8253$ | $0.8717$ | $0.4277$ | $0.5560$ | $0.7523$ | $0.7993$ |
    $0.0157$ | $0.0050$ | $0.8620$ | $\mathbf{0.8698}$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Overall results of DAL comparative experiments. We bold F-acc values
    that are higher than full performance. We rank F-acc and AUBC of each task with
    top 1st, 2nd and 3rd with red, teal and blue respectively. “$*$” refers to the
    experiment needed too much memory, e.g., KCenter on *EMNIST*. “#” refers to the
    experiment that has not been completed yet. Completed tables of all tasks are
    shown in Tables 4, 5, 6, 7, and 8 in Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Analysis of Comparative Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For analyzing the experiment results, we roughly divided our tasks into three
    groups: 1) standard image classification, including *MNIST*, *FashionMNIST*, *EMNIST*,
    *SVHN*, *CIFAR10*, *CIFAR10-imb*, *CIFAR100* and *TinyImageNet*; 2) medical image
    analysis, including *BreakHis* and *PneumoniaMNIST*; 3) comparative studies, including
    *MNIST* and *Waterbird*, which would be introduced in Section [3.4](#S3.SS4 "3.4
    How pre-training influence DAL performance? ‣ 3 Comparative Experiments of DAL
    ‣ A Comparative Survey of Deep Active Learning"). We report *AUBC (acc)* and F-acc
    performance in Table [1](#S3.T1 "Table 1 ‣ Experimental protocol. ‣ 3.1 Experimental
    Settings ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey of Deep Active
    Learning"). We provided overall accuracy-budget curves and summarizing tables,
    as shown in Figure 5 and Tables 4 to 9 in Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: The typical uncertainty-based strategies on group 1, standard image classification
    tasks (in Table [1](#S3.T1 "Table 1 ‣ Experimental protocol. ‣ 3.1 Experimental
    Settings ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey of Deep Active
    Learning"), from LeastConf to CEAL) are generally $1\%\sim 3\%$ higher than Random
    on average performance across the whole AL process (*AUBC*). Among these uncertainty-based
    methods, we have conducted paired t-tests of each method with the other methods
    comparing AUBCs across group 1, standard image classification tasks, and no method
    performs significantly better than the others (all $p$-value are larger than $0.05$).
    Considering dropout, there are only negligible effects (or even counter-intuitive)
    compared with the original versions (e.g., EntropyD vs. Entropy) on the normal
    image classification task, which is consistent with the observations in [[3](#bib.bib3)],
    except for *TinyImageNet*. On *TinyImageNet*, dropout versions are generally $1\%\sim
    3\%$ higher than the original versions. One possible explanation is that it is
    not accurate to use the feature representations provided by a single backbone
    model for calculating uncertainty score, while the dropout technique could help
    increase the uncertainty, and the differences among the uncertainty scores of
    unlabeled data samples will be increased. Therefore dropout versions provide better
    acquisition functions on *TinyImageNet*. Another comparison group is CEAL(Entropy)
    and Entropy, CEAL improved Entropy by an average of $0.5\%$ on $8$ datasets with
    threshold of confidence/entropy 1e-5\. This idea seems effective, but the threshold
    must be carefully tuned to get better performance. On medical image analysis tasks
    (e.g., *PneumoniaMNIST*), performances are slightly different; VarRatio is even
    $4.5\%$ lower than Random. Additionally, we observed the F-acc of many DAL algorithms
    are higher than full performance (e.g., F-acc is $0.9189$ on KCenter and $0.9039$
    on full data), and the performances of dropout versions are worse than normal
    methods, e.g., $0.857$ on Entropy and $0.8177$ on EntropyD. These abnormal phenomena
    could be caused by the distribution shift between training/test sets and data
    redundancy in AL processes. The detailed explanations are in Section E.1 in Appendix.
  prefs: []
  type: TYPE_NORMAL
- en: Compared with uncertainty-based measures, the performances of representativeness/diversity-based
    methods (KMeans, KCenter and VAAL) do not show much advantage. Furthermore, they
    have relatively high time and memory costs since the pairwise distance matrix
    used by KMeans and KCenter need to be re-computed in each iteration with current
    feature representations, while VAAL requires re-training a VAE. Also, a high memory
    load is needed for storing the pairwise distance matrix for large unlabeled data
    pools like *EMNIST*).
  prefs: []
  type: TYPE_NORMAL
- en: Compared with the performance of representativeness-based AL strategies on classical
    ML tasks [[80](#bib.bib80)], we believe that good representativeness-based DAL
    performance is based on good feature representations. Our analysis is consistent
    with the implicit analyses in [[43](#bib.bib43)]. Compared with the CPU-version
    KMeans, KMeans (GPU) is more time-efficient (see Section E.1 in Appendix) and
    performs better (see Table [1](#S3.T1 "Table 1 ‣ Experimental protocol. ‣ 3.1
    Experimental Settings ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey
    of Deep Active Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: Combined strategy BADGE shows its advantage on multiple datasets, where it consistently
    has relatively better performance. BADGE has $1\%\sim 3\%$ higher AUBC performance
    compared with single KMeans and achieves comparable performance with uncertainty-based
    strategies and higher AUBC on *CIFAR100* dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For enhanced DAL methods like LPL, WAAL, AdvBIM and CEAL, we are delighted to
    see their potential over typical DAL methods. For instance, LPL improves F-acc
    over full training on *SVHN* ($0.9452$ vs. $0.8793$), *CIFAR10* ($0.9028$ vs.
    $0.8793$), *CIFAR100* ($0.6369$ vs. $0.6062$), and *CIFAR10-imb* ($0.8478$ vs.
    $0.8036$). However, LPL is sensitive to hyper-parameters in the LossNet used to
    predict the target loss, e.g., the feature size determined by FC layer in LossNet.
    The LPL results on *EMNIST* and *TinyImageNet* indicate that it does not work
    on all datasets (we have tried many hyper-parameter settings on LossNet but did
    not work). A similar phenomenon occurs with WAAL. A potential explanation is that
    both *EMNIST* and *TinyImageNet* contain too many categories, which brings difficulty
    to the loss prediction in LPL and extracting diversified features in WAAL. This
    explanation is further verified in Section [3.4](#S3.SS4 "3.4 How pre-training
    influence DAL performance? ‣ 3 Comparative Experiments of DAL ‣ A Comparative
    Survey of Deep Active Learning") – we adopt a pre-trained ResNet18 as the basic
    classifier, which obtains better feature representations for loss prediction,
    yielding better performance of LPL compared to the non-pre-trained version ($0.9923$
    vs. $0.8913$). The performance comparison on CIFAR100 also supported this explanation.
    AdvBIM, which adds adversarial samples for training, does not achieve ground-breaking
    performances like LPL. These adversarial samples are learned by the current backbone
    model, thus the improvement provided by AdvBIM is marginal. Moreover, AdvBIM is
    extremely time-consuming since it requires calculating adversarial distance $\mathbf{r}$
    for every unlabeled data sample in every iteration. AdvBIM on *EMNIST* and *TinyImageNet*
    could not be completed due to the prohibitive computing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Ablation Study – numbers of training epochs and batch size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to classical ML tasks [[80](#bib.bib80)] that are typically convex
    optimization problems and have globally optimal solutions, DL typically involves
    non-convex optimization problems with local minima. Different hyper-parameters
    like learning rate, optimizer, number of training epochs, and AL batch size lead
    to other solutions with various performances. Here we conduct ablation studies
    on the effect of the number of training epochs in each AL iteration and batch
    size $b$. Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Ablation Study – numbers of training
    epochs and batch size ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey
    of Deep Active Learning") presents the results (see Figure 6 in the Appendix for
    more figures). Compared with Random, Entropy achieves better performance when
    the model is trained using more epochs, e.g., Entropy boosts AUBC by around $1.5$
    when the model is trained $30$ epochs. We also see that using more training epochs
    results in better performance, e.g., AUBC gradually increases from around $0.66$
    to $0.80$ for Random. It is worth noting that the improvement of AUBC brought
    by increasing the number of epochs has diminishing returns. Some researchers prefer
    to use a vast number of epochs during DAL training processes, e.g., Yoo and Kweon
    [[78](#bib.bib78)] used $200$ epochs. However, others like [[32](#bib.bib32)]
    suggest that increasing the number of training epochs will not effectively improve
    testing performance due to generalization problems. Therefore, selecting an optimal
    number of training epochs is vital for reducing computation costs while maintaining
    good model performance. Interestingly, AL batch size has less impact on the performance,
    e.g., Entropy achieves similar performance using different AL batch sizes of $500$,
    $1000$, and $2000$, which is important for DAL since we can use a relatively large
    batch size to reduce the number of training cycles.
  prefs: []
  type: TYPE_NORMAL
- en: WAAL performs consistently better than Entropy and BADGE and the number of training
    epochs has less impact on its performance, e.g., training with $5$ epochs, WAAL
    achieves AUBC $0.825$ when training with 5 epochs and the AUBC remains consistent
    when increasing to 30 epochs. The possible reason is that WAAL considers diversity
    among samples and constructs a good representation through leveraging the unlabeled
    data information, thus reducing data bias. Moreover, we present the accuracy-budget
    curves using different batch sizes and training epochs in Figure A3\. We also
    conclude that training epochs have less impact on WAAL, which is important for
    active learning approaches since fewer training epochs will save training time.
    In addition, WAAL outperforms its counterparts (i.e., Badge and Entropy).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2ac10b04c4ff49755db71663f1ef9126.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Ablation studies on varying number of epochs and batch size on *CIFAR10*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d5539a9a187490995294fe01fcc4f1a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Activation maps of ResNet18 w/ and w/o ImageNet1K pre-training.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b836abd2f05cc40f8f0346959d531f7b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/55f09371a0f79c68e6a2fb655a247992.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1c574e6a7f3a2d95e9f4140995e74bbe.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f0a5ef9be7c1a35fff5b43a966611f58.png)'
  prefs: []
  type: TYPE_IMG
- en: (d)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4b5fc3a6eab859590a86f7196a887c53.png)'
  prefs: []
  type: TYPE_IMG
- en: (e)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ef6847d6b4529a0cc13eb7e4b53f16d2.png)'
  prefs: []
  type: TYPE_IMG
- en: (f)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bbfe707a6ad0f4598df51f937a4d750a.png)'
  prefs: []
  type: TYPE_IMG
- en: (g)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/970c96c4069823f3db75ca93cea03d4f.png)'
  prefs: []
  type: TYPE_IMG
- en: (h)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Overall (mismatch, worst group) accuracy vs. budget curves on *MNIST*
    and *Waterbird* datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 How pre-training influence DAL performance?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pre-training has become a central technique in research and applications of
    DNNs in recent years [[25](#bib.bib25)]. In our work, we selected *Waterbird*
    and *MNIST* datasets to conduct comparative experiments, with non pre-trained
    ResNet18 and pre-trained ResNet18 (pre-trained on ImageNet-1K data, ResNet18P
    for short). *Waterbird* and *MNIST* have completely different natures. *Waterbird*
    dataset contains spurious correlations, and non pre-trained model will focus more
    on backgrounds, e.g., the classifier will wrongly classify a landbird as a waterbird
    when the background is water. Pre-trained models provide better feature representations
    and allow better object semantics (see Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Ablation
    Study – numbers of training epochs and batch size ‣ 3 Comparative Experiments
    of DAL ‣ A Comparative Survey of Deep Active Learning")). *Waterbird* is separated
    to four groups based on object and background: $\{(\text{waterbird, water}),\text{(waterbird,
    land}),\text{(landbird, land}),\text{(landbird, land})\}$. Besides overall accuracy,
    we also report mismatch and worst group accuracy [[54](#bib.bib54), [42](#bib.bib42)],
    which refers to the accuracy of groups $\{(\text{waterbird, land}),\text{(landbird,
    water})\}$ and the lowest accuracy among four groups respectively. On *MNIST*,
    there is no valid background information. Therefore both models w/ and w/o pre-training
    would focus on semantic itself. The goal of this experiment is to observe how
    the pre-training technique influences DAL on hard tasks (i.e., *Waterbird*) and
    easy/well-studied tasks (i.e., *MNIST*) and how feature representations generated
    by basic learners influence DAL methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [4](#S3.F4 "Figure 4 ‣ 3.3 Ablation Study – numbers of training epochs
    and batch size ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey of Deep
    Active Learning") presents overall (also mismatch and worst group in *Waterbird*)
    accuracy vs. budget curves for *MNIST* and *Waterbird*. On *MNIST*, pre-training
    does enhance overall DAL performance, but the ranking across these methods is
    not change (except for LPL), e.g., Entropy $>$ EntropyD $>$ KMeans in both *MNIST*
    with ResNet18 and ResNet18P. LPL performs far better based on ResNet18P, since
    loss prediction is more accurate with better feature representations. In *Waterbird*,
    considering ResNet18 w/o pre-training, normal DAL methods like Entropy, EntropyD,
    CEAL and KMeans are affected by the quality of backbone, which influences the
    DAL selections. Moreover, selecting more data even induces more biases (*Waterbird*
    is imbalanced among four groups) and cause performance reduction. These DAL methods
    return to normal when using the pre-trained ResNet18P, since it helps generate
    predictions with accurate directions (i.e., focus on the object itself, as shown
    in Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Ablation Study – numbers of training epochs
    and batch size ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey of Deep
    Active Learning")). On *Waterbird*, LPL and WAAL w/o pre-training has better performance,
    possibly because they acquire more information with help of enhancing techniques
    (i.e., loss prediction and collecting unlabeled data information). However, LPL
    and WAAL could not well learn worst group under both w/ and w/o pre-training situations.
    A possible explanation is that they are affected by the imbalance problems of
    these groups, which induces bias problems in loss prediction and collecting unlabeled
    data information, and results in poor performance of the worst group.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Challenges and Future Directions of DAL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since there is little room for improving DAL by only designing acquisition functions
    as shown in Sections [2.2](#S2.SS2 "2.2 Enhancing of DAL Methods ‣ 2 DAL Approaches
    ‣ A Comparative Survey of Deep Active Learning") and [3.2](#S3.SS2 "3.2 Analysis
    of Comparative Experiments ‣ 3 Comparative Experiments of DAL ‣ A Comparative
    Survey of Deep Active Learning"), researchers focus on proposing effective ways
    to enhance DAL methods like LPL and enlarging batch size in each round to reduce
    the time and computation cost. However, enhancing methods might not work well
    on all tasks (as shown in Table [1](#S3.T1 "Table 1 ‣ Experimental protocol. ‣
    3.1 Experimental Settings ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey
    of Deep Active Learning")). Better and more universal enhancement methods are
    needed in DAL. Cluster-Margin have scaled to batch sizes (100K-1M) several orders
    of magnitude larger than previous studies [[12](#bib.bib12)], it is hard to be
    transcended.
  prefs: []
  type: TYPE_NORMAL
- en: Another notable situation is the research trend shifting towards developing
    new methodologies to utilize better unlabeled data like semi- and self-supervised
    learning (S4L). Chan et al. [[8](#bib.bib8)] integrated S4L and DAL and conducted
    extensive experiments with contemporary methods, demonstrating that much of the
    benefit of DAL is subsumed by S4L techniques. A clear direction is to better leverage
    the unlabeled data during training in the DAL process under the very few label
    regimes.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, some researchers employed DAL on more complex tasks like Visual Question
    Answering (VQA) [[30](#bib.bib30)] and observed that DAL methods might not perform
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many potential reasons limit DAL performance: i) task-specific DAL may be needed
    for those specific tasks; ii) better feature representations are needed; and iii)
    various dataset properties need to be considered, like collective outliers in
    VQA tasks [[30](#bib.bib30)]. These are possible research directions that demand
    prompt solutions. Therefore, with the increasing demand for dealing with larger
    and more complex data in realistic scenarios, e.g., out-of-distribution (OOD),
    rare classes, imbalanced data, and larger unlabeled data pool [[37](#bib.bib37),
    [12](#bib.bib12)], DAL under different data types are becoming more popular. E.g.,
    Kothawade et al. [[37](#bib.bib37)] let DAL work on rare classes, redundancy,
    imbalanced, and OOD data scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: Another possible research direction is to apply DAL techniques with new data-insufficient
    tasks like automatic driving, medical image analysis, etc. Haussmann et al. [[23](#bib.bib23)]
    have applied AL in an autonomous driving setting of nighttime detection of pedestrians
    and bicycles to improve nighttime detection of pedestrians and bicycles. It has
    shown improved detection accuracy of self-driving DNNs over manual curation –
    data selected with AL yields relative improvements in mean average precision of
    $3\times$ on pedestrian detection and $4.4\times$ on detection of bicycles over
    manually-selected data. Budd et al. [[5](#bib.bib5)] presents a survey on AL adoption
    in the medical domain. Different tasks have different concerns when integrating
    DAL techniques. For instance, In medical imaging, there are many rare yet important
    diseases (e.g., various forms of cancers), while non-cancerous images are much
    more than compared to the cancerous ones [[37](#bib.bib37)]. Therefore, rare classes
    must be considered when designing AL strategies in medical image analysis. More
    importantly, labeling medical images require expertise, and annotation costs and
    effort remain significant. Task-specific DAL is also worthy of research in recent
    years.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments and Disclosure of Funding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parts of experiments (including experiments on *PneumoniaMNIST* and *BreakHis*
    datasets) in this paper were carried out on Baidu Data Federation Platform (Baidu
    FedCube). For usages, please contact us via [{fedcube,shubang}@baidu.com](%7Bfedcube,shubang%7D@baidu.com).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Aggarwal et al. [2014] Charu C Aggarwal, Xiangnan Kong, Quanquan Gu, Jiawei
    Han, and S Yu Philip. Active learning: A survey. In *Data Classification*, pages
    599–634\. Chapman and Hall/CRC, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ash et al. [2020] Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John
    Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain
    gradient lower bounds. In *8th International Conference on Learning Representations,
    ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beluch et al. [2018] William H Beluch, Tim Genewein, Andreas Nürnberger, and
    Jan M Köhler. The power of ensembles for active learning in image classification.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    pages 9368–9377, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bıyık et al. [2019] Erdem Bıyık, Kenneth Wang, Nima Anari, and Dorsa Sadigh.
    Batch active learning using determinantal point processes. *arXiv preprint arXiv:1906.07975*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Budd et al. [2021] Samuel Budd, Emma C Robinson, and Bernhard Kainz. A survey
    on active learning and human-in-the-loop deep learning for medical image analysis.
    *Medical Image Analysis*, 71:102062, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Casanova et al. [2020] Arantxa Casanova, Pedro O Pinheiro, Negar Rostamzadeh,
    and Christopher J Pal. Reinforced active learning for image segmentation. *arXiv
    preprint arXiv:2002.06583*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cawley [2011] Gavin C Cawley. Baseline methods for active learning. In *Active
    Learning and Experimental Design workshop In conjunction with AISTATS 2010*, pages
    47–57\. JMLR Workshop and Conference Proceedings, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. [2021] Yao-Chun Chan, Mingchen Li, and Samet Oymak. On the marginal
    benefit of active learning: Does self-supervision eat its cake? In *ICASSP 2021-2021
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    pages 3455–3459\. IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2015] Yukun Chen, Thomas A Lasko, Qiaozhu Mei, Joshua C Denny,
    and Hua Xu. A study of active learning methods for named entity recognition in
    clinical text. *Journal of biomedical informatics*, 58:11–18, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choi et al. [2021] Jiwoong Choi, Ismail Elezi, Hyuk-Jae Lee, Clement Farabet,
    and Jose M Alvarez. Active learning for deep object detection via probabilistic
    modeling. *arXiv preprint arXiv:2103.16130*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chu et al. [2011] Wei Chu, Martin Zinkevich, Lihong Li, Achint Thomas, and Belle
    Tseng. Unbiased online active learning in data streams. In *Proceedings of the
    17th ACM SIGKDD international conference on Knowledge discovery and data mining*,
    pages 195–203, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Citovsky et al. [2021] Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros
    Karydas, Anand Rajagopalan, Afshin Rostamizadeh, and Sanjiv Kumar. Batch active
    learning at scale. *Advances in Neural Information Processing Systems*, 34, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cohen et al. [2017] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre
    Van Schaik. Emnist: Extending mnist to handwritten letters. In *2017 International
    Joint Conference on Neural Networks (IJCNN)*, pages 2921–2926\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng [2012] Li Deng. The mnist database of handwritten digit images for machine
    learning research. *IEEE Signal Processing Magazine*, 29(6):141–142, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ducoffe and Precioso [2018] Melanie Ducoffe and Frederic Precioso. Adversarial
    active learning for deep networks: a margin based approach. *arXiv preprint arXiv:1802.09841*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duong et al. [2018] Long Duong, Hadi Afshar, Dominique Estival, Glen Pink,
    Philip R Cohen, and Mark Johnson. Active learning for deep semantic parsing. In
    *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics
    (Volume 2: Short Papers)*, pages 43–48, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elahi et al. [2016] Mehdi Elahi, Francesco Ricci, and Neil Rubens. A survey
    of active learning in collaborative filtering recommender systems. *Computer Science
    Review*, 20:29–50, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Freeman and Freeman [1965] Linton C Freeman and Linton C Freeman. *Elementary
    applied statistics: for students in behavioral science*. New York: Wiley, 1965.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. [2013] Yifan Fu, Xingquan Zhu, and Bin Li. A survey on instance selection
    for active learning. *Knowledge and information systems*, 35(2):249–283, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gal et al. [2017] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian
    active learning with image data. In *International Conference on Machine Learning*,
    pages 1183–1192\. PMLR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geifman and El-Yaniv [2017] Yonatan Geifman and Ran El-Yaniv. Deep active learning
    over the long tail. *arXiv preprint arXiv:1711.00941*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gissin and Shalev-Shwartz [2019] Daniel Gissin and Shai Shalev-Shwartz. Discriminative
    active learning. *arXiv preprint arXiv:1907.06347*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haussmann et al. [2020] Elmar Haussmann, Michele Fenzi, Kashyap Chitta, Jan
    Ivanecky, Hanson Xu, Donna Roy, Akshita Mittel, Nicolas Koumchatzky, Clement Farabet,
    and Jose M Alvarez. Scalable active learning for object detection. In *2020 IEEE
    Intelligent Vehicles Symposium (IV)*, pages 1430–1435\. IEEE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pages 770–778, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. [2019] Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using
    pre-training can improve model robustness and uncertainty. In *International Conference
    on Machine Learning*, pages 2712–2721\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. [2011] Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté
    Lengyel. Bayesian active learning for classification and preference learning.
    *arXiv preprint arXiv:1112.5745*, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang [2021] Kuan-Hao Huang. Deepal: Deep active learning in python. *arXiv
    preprint arXiv:2111.15258*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson et al. [2019] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale
    similarity search with GPUs. *IEEE Transactions on Big Data*, 7(3):535–547, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kampffmeyer et al. [2016] Michael Kampffmeyer, Arnt-Borre Salberg, and Robert
    Jenssen. Semantic segmentation of small objects and modeling of uncertainty in
    urban remote sensing images using deep convolutional neural networks. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition workshops*,
    pages 1–9, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karamcheti et al. [2021] Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, and
    Christopher D Manning. Mind your outliers! investigating the negative impact of
    outliers on active learning for visual question answering. *arXiv preprint arXiv:2107.02331*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kermany et al. [2018] Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS
    Valentim, Huiying Liang, Sally L Baxter, Alex McKeown, Ge Yang, Xiaokang Wu, Fangbing
    Yan, et al. Identifying medical diagnoses and treatable diseases by image-based
    deep learning. *Cell*, 172(5):1122–1131, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keskar et al. [2016] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal,
    Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep
    learning: Generalization gap and sharp minima. *arXiv preprint arXiv:1609.04836*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Killamsetty et al. [2021] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh
    Ramakrishnan, and Rishabh Iyer. Glister: Generalization based data subset selection
    for efficient and robust learning. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, volume 35, pages 8110–8118, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kirsch et al. [2019] Andreas Kirsch, Joost Van Amersfoort, and Yarin Gal. Batchbald:
    Efficient and diverse batch acquisition for deep bayesian active learning. *Advances
    in neural information processing systems*, 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Koh et al. [2021] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael
    Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas
    Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts.
    In *International Conference on Machine Learning*, pages 5637–5664\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Körner and Wrobel [2006] Christine Körner and Stefan Wrobel. Multi-class ensemble-based
    active learning. In *European conference on machine learning*, pages 687–694.
    Springer, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kothawade et al. [2021] Suraj Kothawade, Nathan Beck, Krishnateja Killamsetty,
    and Rishabh Iyer. Similar: Submodular information measures based active learning
    in realistic scenarios. *Advances in Neural Information Processing Systems*, 34,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
    layers of features from tiny images. 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kumar and Gupta [2020] Punit Kumar and Atul Gupta. Active learning query strategies
    for classification, regression, and clustering: a survey. *Journal of Computer
    Science and Technology*, 35(4):913–945, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kurakin et al. [2016] Alexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial
    examples in the physical world, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Le and Yang [2015] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge.
    *CS 231N*, 7(7):3, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2022] Ziquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Rong Jin,
    Xiangyang Ji, and Antoni B Chan. An empirical study on distribution shift robustness
    from the perspective of pre-training and data augmentation. *arXiv preprint arXiv:2205.12753*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Munjal et al. [2020] Prateek Munjal, Nasir Hayat, Munawar Hayat, Jamshid Sourati,
    and Shadab Khan. Towards robust and reproducible active learning using neural
    networks. *arXiv preprint arXiv:2002.09564*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naseem et al. [2021] Usman Naseem, Matloob Khushi, Shah Khalid Khan, Kamran
    Shaukat, and Mohammad Ali Moni. A comparative analysis of active learning for
    biomedical text mining. *Applied System Innovation*, 4(1):23, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Netzer et al. [2011] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco,
    Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature
    learning. 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. [2019] Vu-Linh Nguyen, Sébastien Destercke, and Eyke Hüllermeier.
    Epistemic uncertainty sampling. In *International Conference on Discovery Science*,
    pages 72–86\. Springer, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.
    *Advances in neural information processing systems*, 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pedregosa et al. [2011] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort,
    Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer,
    Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. *the
    Journal of machine Learning research*, 12:2825–2830, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pereira-Santos et al. [2019] Davi Pereira-Santos, Ricardo Bastos Cavalcante
    Prudêncio, and André CPLF de Carvalho. Empirical investigation of active learning
    strategies. *Neurocomputing*, 326:15–27, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pinsler et al. [2019] Robert Pinsler, Jonathan Gordon, Eric Nalisnick, and José Miguel
    Hernández-Lobato. Bayesian batch active learning as sparse subset approximation.
    *Advances in neural information processing systems*, 32:6359–6370, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramirez-Loaiza et al. [2017] Maria E Ramirez-Loaiza, Manali Sharma, Geet Kumar,
    and Mustafa Bilgic. Active learning: an empirical study of common baselines. *Data
    mining and knowledge discovery*, 31(2):287–313, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. [2021] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui
    Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning.
    *ACM Computing Surveys (CSUR)*, 54(9):1–40, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy et al. [2018] Soumya Roy, Asim Unmesh, and Vinay P Namboodiri. Deep active
    learning for object detection. In *BMVC*, volume 362, page 91, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sagawa et al. [2019] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and
    Percy Liang. Distributionally robust neural networks. In *International Conference
    on Learning Representations*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saidu and Csató [2021] Isah Charles Saidu and Lehel Csató. Active learning with
    bayesian unet for efficient semantic image segmentation. *Journal of Imaging*,
    7(2):37, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schein and Ungar [2007] Andrew I Schein and Lyle H Ungar. Active learning for
    logistic regression: an evaluation. *Machine Learning*, 68(3):235–265, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sener and Savarese [2017] Ozan Sener and Silvio Savarese. Active learning for
    convolutional neural networks: A core-set approach. *arXiv preprint arXiv:1708.00489*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Senge et al. [2014] Robin Senge, Stefan Bösner, Krzysztof Dembczyński, Jörg
    Haasenritter, Oliver Hirsch, Norbert Donner-Banzhoff, and Eyke Hüllermeier. Reliable
    classification: Learning classifiers that distinguish aleatoric and epistemic
    uncertainty. *Information Sciences*, 255:16–29, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Settles [2009] Burr Settles. Active learning literature survey. 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Settles and Craven [2008] Burr Settles and Mark Craven. An analysis of active
    learning strategies for sequence labeling tasks. In *proceedings of the 2008 conference
    on empirical methods in natural language processing*, pages 1070–1079, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shannon [2001] Claude Elwood Shannon. A mathematical theory of communication.
    *ACM SIGMOBILE mobile computing and communications review*, 5(1):3–55, 2001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. [2017] Yanyao Shen, Hyokun Yun, Zachary C Lipton, Yakov Kronrod,
    and Animashree Anandkumar. Deep active learning for named entity recognition.
    *arXiv preprint arXiv:1707.05928*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shui et al. [2020] Changjian Shui, Fan Zhou, Christian Gagné, and Boyu Wang.
    Deep active learning: Unified and principled method for query and training. In
    *International Conference on Artificial Intelligence and Statistics*, pages 1308–1318\.
    PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sinha et al. [2019] Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variational
    adversarial active learning. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 5972–5981, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sivaraman and Trivedi [2014] Sayanan Sivaraman and Mohan M Trivedi. Active
    learning for on-road vehicle detection: A comparative study. *Machine vision and
    applications*, 25(3):599–611, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spanhol et al. [2015] Fabio A Spanhol, Luiz S Oliveira, Caroline Petitjean,
    and Laurent Heutte. A dataset for breast cancer histopathological image classification.
    *Ieee transactions on biomedical engineering*, 63(7):1455–1462, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tran et al. [2019] Toan Tran, Thanh-Toan Do, Ian Reid, and Gustavo Carneiro.
    Bayesian generative active deep learning. In *International Conference on Machine
    Learning*, pages 6295–6304\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuia et al. [2011] Devis Tuia, Michele Volpi, Loris Copa, Mikhail Kanevski,
    and Jordi Munoz-Mari. A survey of active learning algorithms for supervised remote
    sensing image classification. *IEEE Journal of Selected Topics in Signal Processing*,
    5(3):606–617, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Shang [2014] Dan Wang and Yi Shang. A new active labeling method for
    deep learning. In *2014 International joint conference on neural networks (IJCNN)*,
    pages 112–119\. IEEE, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2016] Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, and Liang Lin.
    Cost-effective active learning for deep image classification. *IEEE Transactions
    on Circuits and Systems for Video Technology*, 27(12):2591–2600, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Hua [2011] Meng Wang and Xian-Sheng Hua. Active learning in multimedia
    annotation and retrieval: A survey. *ACM Transactions on Intelligent Systems and
    Technology (TIST)*, 2(2):1–21, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2021] Tianyang Wang, Xingjian Li, Pengkun Yang, Guosheng Hu, Xiangrui
    Zeng, Siyu Huang, Cheng-Zhong Xu, and Min Xu. Boosting active learning via improving
    test performance. *arXiv preprint arXiv:2112.05683*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2015] Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data
    subset selection and active learning. In *International conference on machine
    learning*, pages 1954–1963\. PMLR, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2020] Jian Wu, Victor S Sheng, Jing Zhang, Hua Li, Tetiana Dadakova,
    Christine Leon Swisher, Zhiming Cui, and Pengpeng Zhao. Multi-label active learning
    algorithms for image classification: Overview and future promise. *ACM Computing
    Surveys (CSUR)*, 53(2):1–35, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. [2017] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist:
    a novel image dataset for benchmarking machine learning algorithms. *arXiv preprint
    arXiv:1708.07747*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. [2021] Yichen Xie, Masayoshi Tomizuka, and Wei Zhan. Towards general
    and efficient active learning. *arXiv preprint arXiv:2112.07963*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. [2017] Changchang Yin, Buyue Qian, Shilei Cao, Xiaoyu Li, Jishang
    Wei, Qinghua Zheng, and Ian Davidson. Deep similarity-based batch mode active
    learning with exploration-exploitation. In *2017 IEEE International Conference
    on Data Mining (ICDM)*, pages 575–584\. IEEE, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yoo and Kweon [2019] Donggeun Yoo and In So Kweon. Learning loss for active
    learning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 93–102, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhan et al. [2021a] Xueying Zhan, Qing Li, and Antoni B Chan. Multiple-criteria
    based active learning with fixed-size determinantal point processes. *arXiv preprint
    arXiv:2107.01622*, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhan et al. [2021b] Xueying Zhan, Huan Liu, Qing Li, and Antoni B. Chan. A
    comparative survey: Benchmarking for pool-based active learning. In Zhi-Hua Zhou,
    editor, *Proceedings of the Thirtieth International Joint Conference on Artificial
    Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021*,
    pages 4679–4686. ijcai.org, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2020] Zhen Zhao, Miaojing Shi, Xiaoxiao Zhao, and Li Li. Active
    crowd counting with limited supervision. In *Computer Vision–ECCV 2020: 16th European
    Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XX 16*, pages 565–581.
    Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhdanov [2019] Fedor Zhdanov. Diverse mini-batch active learning. *arXiv preprint
    arXiv:1901.05954*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu and Bento [2017] Jia-Jie Zhu and José Bento. Generative adversarial active
    learning. *arXiv preprint arXiv:1702.07956*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Appendix A Related Work: comparison with other existing DAL toolkits'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The existing major DAL toolkits/libraries include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our previous work *DeepAL*¹¹1[https://github.com/ej0cl6/deep-active-learning](https://github.com/ej0cl6/deep-active-learning)
    [[27](#bib.bib27)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pytorch Active Learning (PAL) Library ²²2[https://github.com/rmunro/pytorch_active_learning](https://github.com/rmunro/pytorch_active_learning).
    Accompanied with the book Human-in-the-Loop Machine Learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DISTIL library³³3[https://github.com/decile-team/distil](https://github.com/decile-team/distil).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compared with our previous work *DeepAL*, we 1) add more built-in support datasets/tasks,
    where *DeepAL* only support *CIFAR10*, *MNIST*, *FashionMNIST* and *SVHN*, while
    in *$\text{DeepAL}^{+}$*, we add *EMNIST*, *TinyImageNet*, *PneumoniaMNIST*, *BreakHis*
    and wilds-series tasks [[35](#bib.bib35)] like *waterbirds*. 2) We then optimize
    part of existing algorithms to make it better to be adopted on Deep Active Learning
    tasks like KMeans, we re-implemented it by using faiss-gpu libirary [[4](#bib.bib4)],
    it is much faster and perform better than scikit-learn library [[48](#bib.bib48)]
    based KMeans implementation. We conduct Principal Component Analysis (PCA) to
    reduce dimension on representativeness-based approach, i.e., KCenter since it
    costs too much memory for storing pair-wise similarity matrix on DAL tasks. Note
    that both *DeepAL* and *$\text{DeepAL}^{+}$* remove CoreSet approach [[57](#bib.bib57)]
    since Coreset uses the greedy 2-OPT solution for the k-Center problem as an initialization
    and checks the feasibility of a mixed integer program (MIP). They adopted Gourbi
    optimizer⁴⁴4[https://www.gurobi.com/](https://www.gurobi.com/) to solve MIP and
    it is not a free optimizer. The users can use KCenter, a greedy optimization of
    CoreSet. 3) We Add more independent DAL methods implementations like MeanSTD,
    VarRatio, BADGE, LPL, VAAL, WAAL and CEAL.
  prefs: []
  type: TYPE_NORMAL
- en: PAL library is a fundamental human-in-loop framework. Users need to interact
    with computers by inputting the ground truth labels of the instance asked by the
    computer. Besides typical uncertainty-/representativeness-/diversity-/adaptive-based
    AL approaches like Least Confidence, PAL also includes AL with transfer learning
    (ALTL). PAL is more likely to provide a template and tell people how to apply
    AL to different human-in-loop tasks. If someone is new to the AL research field
    and he could try to use this library to understand how AL works.
  prefs: []
  type: TYPE_NORMAL
- en: DISTIL library majorly serves for the submodular functions proposed by the authors
    group [[37](#bib.bib37)], besides the implementations that are already implemented
    in *DeepAL* and *$\text{DeepAL}^{+}$*, they have own implementations like FASS
    [[73](#bib.bib73)], BatchBALD [[34](#bib.bib34)] (we have also implemented this
    method, it is based on BALD, but it is really memory consuming so we finally deleted
    it.), Glister [[33](#bib.bib33)] (for robust learning) and Submodular (Conditional)
    Mutual Information (S(C)MI) [[37](#bib.bib37)] for AL. It is a easy-to-use library
    especially if someone want to use their submodular functions. They have no implementations
    of AL with enhanced techniques like LPL and WAAL.
  prefs: []
  type: TYPE_NORMAL
- en: 'We made a brief comparison between our *$\text{DeepAL}^{+}$* and existing DAL
    libraries, see Table [2](#A1.T2 "Table 2 ‣ Appendix A Related Work: comparison
    with other existing DAL toolkits ‣ A Comparative Survey of Deep Active Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Toolkit | # of implemetations | Comparison with *$\text{DeepAL}^{+}$* |'
  prefs: []
  type: TYPE_TB
- en: '| *DeepAL* | 11 | Our previous work, we updated it. |'
  prefs: []
  type: TYPE_TB
- en: '| *PAL* | 11 | It contains ALTL methods, *$\text{DeepAL}^{+}$* have more concrete
    algorithm re-implementations |'
  prefs: []
  type: TYPE_TB
- en: '| *DISTIL* | 20 | Majorly for submodular related functions implementations,
    no AL with enhanced techniques implementations. |'
  prefs: []
  type: TYPE_TB
- en: '| *$\text{DeepAL}^{+}$* | $19$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Comparison between our *$\text{DeepAL}^{+}$* and existing DAL libraries.
    We excluded Random since strictly speaking, it does not belong to AL approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B More introduction of DeepAL$+$ toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We listed some introductions of our *$\text{DeepAL}^{+}$* in previous Section
     [A](#A1 "Appendix A Related Work: comparison with other existing DAL toolkits
    ‣ A Comparative Survey of Deep Active Learning"). *$\text{DeepAL}^{+}$* is user-friendly,
    using a single command can run experiments, we construct the framework/benchmark
    by easy-to-separate mode, we split the basic networks, querying strategies, dataset/task
    design, and parameters add-in (e.g., set numbers of training epochs, optimizer
    parameters). It is simple to add new AL sampling strategies, new basic backbones,
    and new datasets/tasks in these benchmarks. It makes the users propose new AL
    sampling strategies easier, test new methods on multiple basic tasks, and compare
    them with most SOTA DAL methods. We sincerely hope our *$\text{DeepAL}^{+}$* would
    help researchers in the DAL research field reduce unnecessary workload and focus
    on designing new DAL approaches more. This work is ongoing; we would continually
    add the latest and well-perform DAL approaches and incorporate more datasets/tasks.
    Moreover, if newly proposed DAL methods are designed based on *DeepAL* or *$\text{DeepAL}^{+}$*,
    it would be easier to be further incorporated into our toolkit like BADGE and
    WAAL.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Licences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Datasets.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We listed the licence of datasets we used in our experiments, all datasets
    employed in our comparative experiments are public datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CIFAR10 and CIFAR100 [[38](#bib.bib38)]: MIT Licence.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MNIST [[14](#bib.bib14)], EMNIST [[13](#bib.bib13)]: Creative Commons Attribution-Share
    Alike 3.0 license.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FashionMNIST [[75](#bib.bib75)]: MIT Licence.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PneumoniaMNIST [[31](#bib.bib31)]: CC BY 4.0 License.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BreakHis [[66](#bib.bib66)]: Creative Commons Attribution 4.0 International
    License.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Waterbird [[54](#bib.bib54), [35](#bib.bib35)]: MIT License.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Methods.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We listed all related license of the original implementations of DAL methods
    that we re-implemented and basic backbone models in our *DeepAL$+$* toolkit:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch [[47](#bib.bib47)]: Modified BSD.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scikit-Learn [[48](#bib.bib48)]: BSD License.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BADGE [[2](#bib.bib2)]: Not listed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LPL [[78](#bib.bib78)]: Not listed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VAAL [[64](#bib.bib64)]: BSD 2-Clause “Simplified” License.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WAAL [[63](#bib.bib63)]: Not listed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CEAL [[70](#bib.bib70)]: Not listed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Methods originated from DeepAL [[27](#bib.bib27)] library implementation: MIT
    Licence.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KMeans (faiss library [[28](#bib.bib28)] implementation): MIT Licence.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ResNet18 [[24](#bib.bib24)]: MIT License.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix D Experimental Settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: D.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Considering some DAL approaches currently only support computer vision tasks
    like VAAL, for consistency and fairness of our experiments, we adopt 1) the image
    classification tasks, similar to most DAL papers. We use the following datasets
    (details in Table [3](#A4.T3 "Table 3 ‣ D.1 Datasets ‣ Appendix D Experimental
    Settings ‣ A Comparative Survey of Deep Active Learning")): *MNIST* [[14](#bib.bib14)],
    *FashionMNIST* [[75](#bib.bib75)], *EMNIST* [[13](#bib.bib13)], *SVHN* [[45](#bib.bib45)],
    *CIFAR10* and *CIFAR100* [[38](#bib.bib38)] and *Tiny ImageNet* [[41](#bib.bib41)].
    Additionally, to explore DAL performance on imbalanced data, we construct an imbalanced
    dataset based on *CIFAR10*, called *CIFAR10-imb*, which sub-samples the training
    set with ratios of 1:2:$\cdots$:10 for classes 0 through 9\. 2) The medical image
    analysis tasks, including Breast Cancer Histopathological Image Classification
    (*BreakHis*) [[66](#bib.bib66)] and Pneumonia-MNIST (pediatric chest X-ray) (*PneumoniaMNIST*)
    [[31](#bib.bib31)]. Additionally, we adopted an object recognition with correlated
    backgrounds dataset (*Waterbird*) [[54](#bib.bib54)], it considers two classes:
    waterbird and landbird. These objected were manually mixed to water and land background,
    and waterbirds (landbirds) more frequently appearing against a water (land) background.
    It is a challenging task since DNNs might spuriously rely on background instead
    of learning to recognize semantic/object.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | $\#i$ | $\#u$ | $\#t$ | $b$ | $Q$ | $\#k$ | $\#e$ |'
  prefs: []
  type: TYPE_TB
- en: '| *MNIST* | $500$ | $59,500$ | $10,000$ | $250$ | $10,000$ | $10$ | $20$ |'
  prefs: []
  type: TYPE_TB
- en: '| *FashionMNIST* | $500$ | $59,500$ | $10,000$ | $250$ | $10,000$ | $10$ |
    $20$ |'
  prefs: []
  type: TYPE_TB
- en: '| *EMNIST* | $1,000$ | $696,932$ | $116,323$ | $500$ | $50,000$ | $62$ | $20$
    |'
  prefs: []
  type: TYPE_TB
- en: '| *SVHN* | $500$ | $72,757$ | $26,032$ | $250$ | $20,000$ | $10$ | $20$ |'
  prefs: []
  type: TYPE_TB
- en: '| *CIFAR10* | $1,000$ | $49,000$ | $10,000$ | $500$ | $40,000$ | $10$ | $30$
    |'
  prefs: []
  type: TYPE_TB
- en: '| *CIFAR100* | $1,000$ | $49,000$ | $10,000$ | $500$ | $40,000$ | $100$ | $40$
    |'
  prefs: []
  type: TYPE_TB
- en: '| *Tiny ImageNet* | $1,000$ | $99,000$ | $10,000$ | $500$ | $40,000$ | $200$
    | $40$ |'
  prefs: []
  type: TYPE_TB
- en: '| *CIFAR10-imb* | $1,000$ | $26,499$ | $10,000$ | $500$ | $20,000$ | $10$ |
    $30$ |'
  prefs: []
  type: TYPE_TB
- en: '| *BreakHis* | $100$ | $5,436$ | $2,373$ | $100$ | $5,000$ | $2$ | $10$ |'
  prefs: []
  type: TYPE_TB
- en: '| *PneumoniaMNIST* | $100$ | $5,132$ | $5,232$ | $100$ | $5,000$ | $2$ | $10$
    |'
  prefs: []
  type: TYPE_TB
- en: '| *Waterbird* | $100$ | $4,695$ | $5,794$ | $100$ | $4,000$ | $2$ | $10$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Datasets used in comparative experiments. $\#i$ is the size of initial
    labeled pool, $\#u$ is the size of unlabeled data pool, $\#t$ is the size of testing
    set, $\#k$ is number of categories and $\#e$ is number of epochs used to train
    the basic classifier in each AL round.'
  prefs: []
  type: TYPE_NORMAL
- en: D.2 Implementation details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We employed ResNet18⁵⁵5[https://pytorch.org/vision/stable/models.html#id10](https://pytorch.org/vision/stable/models.html#id10)
    [[24](#bib.bib24)] as the basic learner. On *MNIST*, *EMNIST*, *FashionMNIST*,
    *TinyImagenet*, *CIFAR10* and *CIFAR100*, we adopted *Adam* optimizer (learning
    rate: $1e-3$) . On *PneumoniaMNIST*, *BreakHis* and *Waterbird*, since Adam would
    cause overfitting, we use SGD optimizer with learning rate: 1e-2 on *BreakHis*
    and *PneumoniaMNIST*, learning rate: $0.0005$, weight decay: 1e-5, momentum: $0.9$
    on *Waterbird*.'
  prefs: []
  type: TYPE_NORMAL
- en: For a fair comparison, consistent experimental settings of the basic classifier
    are used across all DAL methods. The dataset-specific implementation details are
    discussed as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MNIST*, *FashionMNIST* and *EMNIST*: number of training epochs is $20$, the
    kernel size of the first convolutional layer in ResNet18 is $7\times 7$ (consistent
    with the original PyTorch implementation), input pre-processing step include normalization.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CIFAR10*, *CIFAR100*: number of training epochs is $30$, the kernel size of
    the first convolutional layer in ResNet18 is $3\times 3$ (consistent with PyTorch-CIFAR
    implementation⁶⁶6[https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py](https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py)),
    input pre-processing steps include random crop (pad=4), random horizontal flip
    ($p=0.5$) and normalization.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TinyImageNet*: number of training epochs is $40$, the same implementation
    of ResNet18 as *CIFAR*, input pre-processing steps include random rotation (degree=20),
    random horizontal flip ($p=0.5$) and normalization.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SVHN*: number of training epochs is $20$, the same implementation of ResNet18
    as *MNIST*, input pre-processing steps include normalization.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*BreakHis*: number of training epochs is $10$, the same implementation of ResNet10
    as *CIFAR*, input pre-proccessing steps include random rotation (degree=90), random
    horizontal flip ($p=0.8$), random resize crop (scale=224), randomly change the
    brightness, contrast, saturation and hue of image – ColorJitter (brightness=0.4,
    contrast=0.4, saturation=0.4, hue=0.1) and normalization.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PneumoniaMNIST*: number of training epochs is $10$, the same implementation
    o ResNet18 as *CIFAR*, input pre-processing steps include resize (shape=255),
    center crop (shape = 224), random horizontal flip ($p=0.5$), random rotation (degree=10),
    random gray scale, random affine (translate=(0.05, 0.05), degree=0).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Waterbirds*: number of training epochs is $30$, the same implementation of
    ResNet18 as *MNIST*, input pre-processing steps include random horizontal flip
    ($p=0.5$).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The model-specific implementation details are discussed as follows. For MC
    Dropout implementation, we employed $10$ forward passes. For CEAL(Entropy), we
    set threshold of confidence/entropy score for assigning pseudo labels as $1e-5$.
    For KCenter, since using the full feature vector would take too much memory in
    pair-wise distance calculation, we employ Principal components analysis (PCA)
    to reduce feature dimension to $32$ according to [[30](#bib.bib30)]. For VAE in
    VAAL, we followed the same architecture in [[64](#bib.bib64)] and train VAE with
    $30$ epochs with *Adam* optimizer (learning rate: $1e-3$). For LPL, we train LossNet
    with *Adam* optimizer (learning rate: $1e-2$); since LossNet is co-trained with
    basic classifier, we firstly co-trained LossNet and basic classifier followed
    by normal training processes, then we detached feature updating (the same as stop
    training basic classifier) and assign $20$ extra epochs for training LossNet.'
  prefs: []
  type: TYPE_NORMAL
- en: D.3 Experimental environments in our comparative experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conduct experiments on a single Tesla V100-SXM2 GPU with 16GB memory except
    for running experiments on *PneumoniaMNIST* and *BreakHis*, since running them
    need $>16GB$ and $<32GB$ memories. We run experiments of *PneumoniaMNIST* and
    *BreakHis* on another single Tesla V100-SXM2 GPU with 32GB memory. We only use
    a single GPU for each experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Completed Experimental Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: E.1 Overall experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: E.1.1 Performance of Standard Image Classification tasks.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tables [4](#A5.T4 "Table 4 ‣ E.1.2 Performance of Medical Image Analysis tasks.
    ‣ E.1 Overall experiments ‣ Appendix E Completed Experimental Results ‣ A Comparative
    Survey of Deep Active Learning"), [5](#A5.T5 "Table 5 ‣ E.1.2 Performance of Medical
    Image Analysis tasks. ‣ E.1 Overall experiments ‣ Appendix E Completed Experimental
    Results ‣ A Comparative Survey of Deep Active Learning"), [6](#A5.T6 "Table 6
    ‣ E.1.2 Performance of Medical Image Analysis tasks. ‣ E.1 Overall experiments
    ‣ Appendix E Completed Experimental Results ‣ A Comparative Survey of Deep Active
    Learning") record the overall performances of standard image classification tasks
    group, including *MNIST*, *FashionMNIST*, *EMNIST*, *SVHN*, *CIFAR10*, *CIFAR1O-imb*,
    *CIFAR100* and *TinyImageNet* datasets. Including AUBC (acc) performance with
    mean and standard deviation over 3 trials, the average running time that takes
    the running time of Random as unit and the F-acc score.
  prefs: []
  type: TYPE_NORMAL
- en: Note that KMeans(GPU) performs better than KMeans on major tasks, i.e., Table [5](#A5.T5
    "Table 5 ‣ E.1.2 Performance of Medical Image Analysis tasks. ‣ E.1 Overall experiments
    ‣ Appendix E Completed Experimental Results ‣ A Comparative Survey of Deep Active
    Learning"). However, from the average running time, KMeans(GPU) seems to have
    more time than KMeans, it does not mean KMeans(GPU) run slower than KMeans, since
    the running time calculation does not count the waiting time, e.g., wait for memory
    allocation, the time for data load from GPU to CPU or from CPU to GPU. In KMeans,
    in every AL iteration, we need to load data (feature embedding) from GPU to CPU
    and use scikit-learn library to calculate. At this step, the program must waste
    time waiting for the operating system to allocate memory for calculation. Nevertheless,
    these waiting times could be saved in KMeans(GPU). So actually KMeans(GPU) run
    faster than KMeans on DAL tasks that use GPU for calculation.
  prefs: []
  type: TYPE_NORMAL
- en: E.1.2 Performance of Medical Image Analysis tasks.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [7](#A5.T7 "Table 7 ‣ E.1.2 Performance of Medical Image Analysis tasks.
    ‣ E.1 Overall experiments ‣ Appendix E Completed Experimental Results ‣ A Comparative
    Survey of Deep Active Learning") records the overall performances of medical image
    analysis group, including *PneumoniaMNIST* and *BreakHis* datasets. Both LPL,
    WAAL and BADGE perform well on these medical image analysis tasks. Another thing
    that worth to pay attention is: all MC dropout based versions (i.e., LeastConfD,
    MarginD. EntropyD, as well as BALD), perform worse than original versions (i.e.,
    LeastConf, Margin and Entropy), especially on *PneumonialMNIST*. For example,
    on *PneumonialMNIST*, the AUBC value of LeastConf is 0.852, while LeastConfD only
    have 0.8243 AUBC value. A potential reason is in *PneumoniaMNIST*, to justify
    whether an image – a chest X-ray report pneumonia, one needs to check the local
    lesions and observe the lung’s overall condition. The basic learner needs both
    local and global features to make an accurate prediction. While MC dropout reduces
    the model capacity and might ignore some feature information, making less convincing
    predictions [[3](#bib.bib3)] and hurt DAL performance. Another phenomenon is,
    considering F-acc, we noticed that many DAL approaches’ F-acc are higher than
    the accuracy trained on full dataset (0.9039), e.g., 0.9149 on MarginD, 0.9204
    on BALD, 0.9179 on CEAL, 0.9197 on AdvBIM. These results can be summarized as
    one phenomenon: the subset selected from the full subset would contribute to better
    performance. This is because *PheumoniaMNIST* contains distribution/dataset shift
    between training and testing set. Also, this dataset might contain redundant data
    samples and confusing information. That is, these medical images are not one-to-one.
    They are many-to-one. One patient would correspond to several chest X-ray images,
    which causes redundancy. Additionally, some patients may have more than one disease
    e.g., we can see on some X-ray images that there is a posterior spinal fixator
    that the patient used to fix his spine. These features also might influence the
    predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Compared with standard tasks (i.e., standard image classification tasks in our
    comparative survey), real-life applications would encounter more unexpected problems
    like we discussed aforementioned *PneumoniaMNIST* dataset. That is why we are
    working on adding more different kinds of tasks for testing DAL approaches. We
    also encourage DAL researchers to try DAL approaches on various data scenarios
    and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | *MNIST* | *MNIST (w/ pre-train)* | *Waterbird* | *Waterbird (w/ pre-train)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| Model | AUBC | F-acc | Time | AUBC | F-acc | Time | AUBC | F-acc | Time |
    AUBC | F-acc | Time |'
  prefs: []
  type: TYPE_TB
- en: '| Full | $-$ | $0.9916$ | $-$ | $-$ | $0.9931$ | $-$ | $-$ | $0.5678$ | $-$
    | $-$ | $0.8459$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 0.9570 $\pm$ 0.0036 | 0.9738 | 1.00 | 0.9767 $\pm$ 0.0005 | 0.9822
    | 1.00 | 0.5950 $\pm$ 0.0092 | 0.5657 | 1.00 | 0.8070 $\pm$ 0.0043 | 0.8511 |
    Time |'
  prefs: []
  type: TYPE_TB
- en: '| LeastConf | 0.9677 $\pm$ 0.0041 | 0.9892 | 1.14 | 0.9833 $\pm$ 0.0012 | 0.9794
    | 1.38 | 0.5843 $\pm$ 0.0054 | 0.5612 | 2.78 | 0.8473 $\pm$ 0.0021 | 0.8605 |
    1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| LeastConfD | 0.9745 $\pm$ 0.0008 | 0.9915 | 2.17 | 0.9840 $\pm$ 0.0029 |
    0.9926 | 2.37 | 0.5847 $\pm$ 0.0009 | 0.5947 | 2.87 | 0.7947 $\pm$ 0.0173 | 0.8516
    | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Margin | 0.9733 $\pm$ 0.0012 | 0.9881 | 1.25 | 0.9813 $\pm$ 0.0029 | 0.9875
    | 1.35 | 0.5840 $\pm$ 0.0029 | 0.6064 | 0.99 | 0.8460 $\pm$ 0.0029 | 0.8629 |
    2.01 |'
  prefs: []
  type: TYPE_TB
- en: '| MarginD | 0.9703 $\pm$ 0.0025 | 0.9899 | 2.84 | 0.9843 $\pm$ 0.0005 | 0.9883
    | 2.41 | 0.5960 $\pm$ 0.0049 | 0.5629 | 1.19 | 0.7983 $\pm$ 0.0166 | 0.8458 |
    1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Entropy | 0.9723 $\pm$ 0.0052 | 0.9883 | 1.54 | 0.9830 $\pm$ 0.0024 | 0.9907
    | 1.09 | 0.5823 $\pm$ 0.0074 | 0.6204 | 1.02 | 0.8473 $\pm$ 0.0019 | 0.8557 |
    1.19 |'
  prefs: []
  type: TYPE_TB
- en: '| EntropyD | 0.9643 $\pm$ 0.0045 | 0.9887 | 3.14 | 0.9840 $\pm$ 0.0022 | 0.9912
    | 2.08 | 0.5817 $\pm$ 0.0101 | 0.6321 | 1.19 | 0.8000 $\pm$ 0.0222 | 0.8477 |
    1.03 |'
  prefs: []
  type: TYPE_TB
- en: '| BALD | 0.9697 $\pm$ 0.0034 | 0.9885 | 3.12 | 0.9807 $\pm$ 0.0009 | 0.9834
    | 2.16 | 0.5970 $\pm$ 0.0070 | 0.6136 | 2.01 | 0.7773 $\pm$ 0.0012 | 0.8452 |
    2.32 |'
  prefs: []
  type: TYPE_TB
- en: '| MeanSTD | 0.9713 $\pm$ 0.0034 | 0.9735 | 2.50 | 0.9847 $\pm$ 0.0005 | 0.9907
    | 2.20 | 0.5890 $\pm$ 0.0063 | 0.5758 | 2.20 | 0.7890 $\pm$ 0.0107 | 0.8401 |
    1.64 |'
  prefs: []
  type: TYPE_TB
- en: '| VarRatio | 0.9717 $\pm$ 0.0083 | 0.9841 | 1.77 | 0.9847 $\pm$ 0.0005 | 0.9902
    | 1.26 | 0.5803 $\pm$ 0.0026 | 0.5570 | 1.87 | 0.8460 $\pm$ 0.0029 | 0.8577 |
    2.32 |'
  prefs: []
  type: TYPE_TB
- en: '| CEAL(Entropy) | 0.9787 $\pm$ 0.0019 | 0.9889 | 3.33 | 0.9863 $\pm$ 0.0005
    | 0.9872 | 2.27 | 0.5943 $\pm$ 0.0071 | 0.5811 | 1.93 | 0.8460 $\pm$ 0.0022 |
    0.8518 | 1.06 |'
  prefs: []
  type: TYPE_TB
- en: '| KMeans | 0.9640 $\pm$ 0.0016 | 0.9813 | 8.78 | # | # | # | 0.5920 $\pm$ 0.0022
    | 0.5846 | 2.31 | 0.7823 $\pm$ 0.0066 | 0.8410 | 1.43 |'
  prefs: []
  type: TYPE_TB
- en: '| Kmeans(GPU) | 0.9637 $\pm$ 0.0021 | 0.9747 | 10.14 | 0.9743 $\pm$ 0.0005
    | 0.98 | 3.34 | 0.5663 $\pm$ 0.0054 | 0.5987 | 1.30 | 0.7937 $\pm$ 0.0041 | 0.8365
    | 2.33 |'
  prefs: []
  type: TYPE_TB
- en: '| KCenter | 0.9740 $\pm$ 0.0014 | 0.9877 | 7.10 | 0.9523 $\pm$ 0.0039 | 0.9659
    | 8.60 | 0.6097 $\pm$ 0.0108 | 0.6373 | 2.13 | 0.8297 $\pm$ 0.0031 | 0.8555 |
    1.31 |'
  prefs: []
  type: TYPE_TB
- en: '| VAAL | 0.9623 $\pm$ 0.0024 | 0.9573 | 19.20 | 0.9737 $\pm$ 0.0005 | 0.9718
    | 6.93 | 0.6217 $\pm$ 0.0025 | 0.5758 | 9.78 | 0.8070 $\pm$ 0.0029 | 0.8546 |
    9.69 |'
  prefs: []
  type: TYPE_TB
- en: '| BADGE(KMeans++) | 0.9707 $\pm$ 0.0062 | 0.9904 | 32.51 | 0.9647 $\pm$ 0.0026
    | 0.9841 | 7.04 | 0.5837 $\pm$ 0.0074 | 0.6194 | 2.47 | 0.8460 $\pm$ 0.0014 |
    0.8538 | 1.94 |'
  prefs: []
  type: TYPE_TB
- en: '| AdvBIM | 0.9680 $\pm$ 0.0037 | 0.9840 | 20.74 | # | # | # | # | # | # | 0.8033
    $\pm$ 0.0082 | 0.8380 | 10.64 |'
  prefs: []
  type: TYPE_TB
- en: '| LPL | 0.8913 $\pm$ 0.0062 | 0.9732 | 5.44 | 0.9923 $\pm$ 0.0005 | 0.9955
    | 2.29 | 0.7277 $\pm$ 0.0017 | 0.7783 | 4.50 | 0.7803 $\pm$ 0.0073 | 0.7817 |
    4.51 |'
  prefs: []
  type: TYPE_TB
- en: '| WAAL | 0.9890 $\pm$ 0.0014 | 0.9946 | 36.10 | 0.9780 $\pm$ 0.0008 | 0.9943
    | 2.39 | 0.6837 $\pm$ 0.0073 | 0.7784 | 6.87 | 0.7009 $\pm$ 0.0067 | 0.7783 |
    6.81 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Results of DAL comparative experiments with *MNIST* w/ and w/o pre-train
    and *Waterbird* w/ and w/o pre-train. We report the AUBC for overall accuracy,
    final accuracy (F-acc) after quota $Q$ is exhausted, and the average running time
    of the whole AL processes (including training and querying processes) relative
    to Random. We rank F-acc and AUBC of each task with top 1st, 2nd and 3rd with
    red, teal and blue respectively. “#” indicates that the experiment has not been
    completed yet.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | *CIFAR10* | *CIFAR10-imb* | *CIFAR100* | *SVHN* |'
  prefs: []
  type: TYPE_TB
- en: '| Model | AUBC | F-acc | Time | AUBC | F-acc | Time | AUBC | F-acc | Time |
    AUBC | F-acc | Time |'
  prefs: []
  type: TYPE_TB
- en: '| Full | $-$ | $0.8793$ | $-$ | $-$ | $0.8036$ | $-$ | $-$ | $0.6062$ | $-$
    | $-$ | $0.9190$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 0.7967 $\pm$ 0.0005 | 0.8679 | 1.00 | 0.7103 $\pm$ 0.0017 | 0.8015
    | 1.00 | 0.4667 $\pm$ 0.0009 | 0.5903 | 1.00 | 0.8110 $\pm$ 0.0008 | 0.8806 |
    1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| LeastConf | 0.8150 $\pm$ 0.0000 | 0.8785 | 1.04 | 0.7330 $\pm$ 0.0022 | 0.8022
    | 1.04 | 0.4747 $\pm$ 0.0009 | 0.6072 | 1.02 | 0.8350 $\pm$ 0.0028 | 0.9094 |
    1.05 |'
  prefs: []
  type: TYPE_TB
- en: '| LeastConfD | 0.8137 $\pm$ 0.0012 | 0.8825 | 1.10 | 0.7323 $\pm$ 0.0033 |
    0.8065 | 1.18 | 0.4730 $\pm$ 0.0008 | 0.5997 | 1.13 | 0.8320 $\pm$ 0.0008 | 0.9083
    | 1.68 |'
  prefs: []
  type: TYPE_TB
- en: '| Margin | 0.8153 $\pm$ 0.0005 | 0.8834 | 1.01 | 0.7367 $\pm$ 0.0033 | 0.8029
    | 0.80 | 0.4790 $\pm$ 0.0008 | 0.6010 | 0.93 | 0.8373 $\pm$ 0.0005 | 0.9138 |
    1.35 |'
  prefs: []
  type: TYPE_TB
- en: '| MarginD | 0.8140 $\pm$ 0.0008 | 0.8837 | 1.17 | 0.7260 $\pm$ 0.0014 | 0.8128
    | 0.86 | 0.4777 $\pm$ 0.0005 | 0.6000 | 1.06 | 0.8357 $\pm$ 0.0034 | 0.9104 |
    1.46 |'
  prefs: []
  type: TYPE_TB
- en: '| Entropy | 0.8130 $\pm$ 0.0008 | 0.8784 | 1.07 | 0.7320 $\pm$ 0.0019 | 0.8187
    | 0.73 | 0.4693 $\pm$ 0.0017 | 0.6048 | 0.78 | 0.8297 $\pm$ 0.0009 | 0.9099 |
    1.33 |'
  prefs: []
  type: TYPE_TB
- en: '| EntropyD | 0.8140 $\pm$ 0.0000 | 0.8787 | 1.12 | 0.7317 $\pm$ 0.0021 | 0.7963
    | 0.78 | 0.4677 $\pm$ 0.0005 | 0.6004 | 0.87 | 0.8290 $\pm$ 0.0008 | 0.9091 |
    1.43 |'
  prefs: []
  type: TYPE_TB
- en: '| BALD | 0.8103 $\pm$ 0.0009 | 0.8762 | 1.18 | 0.7210 $\pm$ 0.0024 | 0.7927
    | 1.20 | 0.4760 $\pm$ 0.0008 | 0.5942 | 1.03 | 0.8333 $\pm$ 0.0005 | 0.9020 |
    1.51 |'
  prefs: []
  type: TYPE_TB
- en: '| MeanSTD | 0.8087 $\pm$ 0.0009 | 0.8821 | 1.11 | 0.7203 $\pm$ 0.0017 | 0.7996
    | 0.78 | 0.4717 $\pm$ 0.0012 | 0.5963 | 1.11 | 0.8323 $\pm$ 0.0026 | 0.9087 |
    2.52 |'
  prefs: []
  type: TYPE_TB
- en: '| VarRatio | 0.8150 $\pm$ 0.0008 | 0.8780 | 1.00 | 0.7353 $\pm$ 0.0024 | 0.8165
    | 1.03 | 0.4747 $\pm$ 0.0012 | 0.5959 | 0.97 | 0.8357 $\pm$ 0.0009 | 0.9079 |
    1.45 |'
  prefs: []
  type: TYPE_TB
- en: '| CEAL(Entropy) | 0.8150 $\pm$ 0.0016 | 0.8794 | 1.00 | 0.7327 $\pm$ 0.0050
    | 0.8187 | 0.75 | 0.4693 $\pm$ 0.0005 | 0.6043 | 0.94 | 0.8430 $\pm$ 0.0028 |
    0.9142 | 1.16 |'
  prefs: []
  type: TYPE_TB
- en: '| KMeans | 0.7910 $\pm$ 0.0016 | 0.8713 | 0.50 | 0.7070 $\pm$ 0.0029 | 0.7908
    | 3.06 | 0.4570 $\pm$ 0.0008 | 0.5834 | 1.01 | 0.8027 $\pm$ 0.0012 | 0.8671 |
    5.22 |'
  prefs: []
  type: TYPE_TB
- en: '| KMeans(GPU) | 0.7977 $\pm$ 0.0009 | 0.8718 | 1.64 | 0.7140 $\pm$ 0.0008 |
    0.7921 | 1.54 | 0.4687 $\pm$ 0.0005 | 0.5842 | 1.28 | 0.8120 $\pm$ 0.0008 | 0.8688
    | 5.76 |'
  prefs: []
  type: TYPE_TB
- en: '| KCenter | 0.8047 $\pm$ 0.0012 | 0.8741 | 0.98 | 0.7233 $\pm$ 0.0009 | 0.7826
    | 2.87 | 0.4770 $\pm$ 0.0016 | 0.5993 | 1.02 | 0.8283 $\pm$ 0.0017 | 0.9000 |
    5.66 |'
  prefs: []
  type: TYPE_TB
- en: '| VAAL | 0.7973 $\pm$ 0.0009 | 0.8679 | 1.26 | 0.7113 $\pm$ 0.0012 | 0.7950
    | 4.58 | 0.4693 $\pm$ 0.0005 | 0.5870 | 1.20 | 0.8117 $\pm$ 0.0012 | 0.8813 |
    9.84 |'
  prefs: []
  type: TYPE_TB
- en: '| BADGE(KMeans++) | 0.8143 $\pm$ 0.0005 | 0.8794 | 2.08 | 0.7347 $\pm$ 0.0019
    | 0.8126 | 5.91 | 0.4803 $\pm$ 0.0005 | 0.6028 | 1.12 | 0.8377 $\pm$ 0.0017 |
    0.9057 | 10.27 |'
  prefs: []
  type: TYPE_TB
- en: '| AdvBIM | 0.7997 $\pm$ 0.0005 | 0.8750 | 2.59 | # | # | # | # | # | # | #
    | # | # |'
  prefs: []
  type: TYPE_TB
- en: '| LPL | 0.8220 $\pm$ 0.0014 | 0.9028 | 2.19 | 0.7477 $\pm$ 0.0060 | 0.8478
    | 3.14 | 0.4640 $\pm$ 0.0024 | 0.6369 | 0.71 | 0.8737 $\pm$ 0.0061 | 0.9452 |
    2.26 |'
  prefs: []
  type: TYPE_TB
- en: '| WAAL | 0.8253 $\pm$ 0.0005 | 0.8717 | 1.65 | 0.7523 $\pm$ 0.0021 | 0.7993
    | 4.00 | 0.4277 $\pm$ 0.0005 | 0.5560 | 1.13 | 0.8603 $\pm$ 0.0017 | 0.9139 |
    9.88 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Results of DAL comparative experiments, including *CIFAR10*, *CIFAR10-imb*,
    *CIFAR100* and *SVHN*. We report the AUBC for overall accuracy, final accuracy
    (F-acc) after quota $Q$ is exhausted, and the average running time of the whole
    AL processes (including training and querying processes) relative to Random. We
    rank F-acc and AUBC of each task with top 1st, 2nd and 3rd with red, teal and
    blue respectively. “#” indicates that the experiment has not been completed yet.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | *EMNIST* | *FashionMNIST* | *TinyImageNet* |'
  prefs: []
  type: TYPE_TB
- en: '| Model | AUBC | F-acc | Time | AUBC | F-acc | Time | AUBC | F-acc | Time |'
  prefs: []
  type: TYPE_TB
- en: '| Full | $-$ | $0.8684$ | $-$ | $-$ | $0.9120$ | $-$ | $-$ | $0.4583$ | $-$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 0.8057 $\pm$ 0.0026 | 0.8377 | 1.00 | 0.8313 $\pm$ 0.0034 | 0.8434
    | 1.00 | 0.2577 $\pm$ 0.0017 | 0.3544 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| LeastConf | 0.8113 $\pm$ 0.0068 | 0.8479 | 1.11 | 0.8377 $\pm$ 0.0029 | 0.8820
    | 1.10 | 0.2417 $\pm$ 0.0009 | 0.3470 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| LeastConfD | 0.8177 $\pm$ 0.0005 | 0.8483 | 1.90 | 0.8450 $\pm$ 0.0036 |
    0.8744 | 1.80 | 0.2620 $\pm$ 0.0000 | 0.3698 | 0.93 |'
  prefs: []
  type: TYPE_TB
- en: '| Margin | 0.8103 $\pm$ 0.0041 | 0.8468 | 0.85 | 0.8427 $\pm$ 0.0040 | 0.8772
    | 1.21 | 0.2557 $\pm$ 0.0012 | 0.3611 | 1.02 |'
  prefs: []
  type: TYPE_TB
- en: '| MarginD | 0.8197 $\pm$ 0.0017 | 0.8472 | 0.72 | 0.8417 $\pm$ 0.0012 | 0.8756
    | 2.07 | 0.2607 $\pm$ 0.0017 | 0.3541 | 1.32 |'
  prefs: []
  type: TYPE_TB
- en: '| Entropy | 0.8090 $\pm$ 0.0057 | 0.8458 | 0.97 | 0.8397 $\pm$ 0.0029 | 0.8660
    | 1.22 | 0.2343 $\pm$ 0.0005 | 0.3346 | 1.02 |'
  prefs: []
  type: TYPE_TB
- en: '| EntropyD | 0.8167 $\pm$ 0.0019 | 0.8507 | 0.15 | 0.8417 $\pm$ 0.0033 | 0.8784
    | 2.02 | 0.2627 $\pm$ 0.0005 | 0.3716 | 0.09 |'
  prefs: []
  type: TYPE_TB
- en: '| BALD | 0.8197 $\pm$ 0.0024 | 0.8448 | 1.87 | 0.8423 $\pm$ 0.0095 | 0.8888
    | 2.07 | 0.2623 $\pm$ 0.0017 | 0.3648 | 0.91 |'
  prefs: []
  type: TYPE_TB
- en: '| MeanSTD | 0.8110 $\pm$ 0.0014 | 0.8426 | 0.68 | 0.8457 $\pm$ 0.0017 | 0.8766
    | 2.18 | 0.2510 $\pm$ 0.0008 | 0.3551 | 0.17 |'
  prefs: []
  type: TYPE_TB
- en: '| VarRatio | 0.8107 $\pm$ 0.0060 | 0.8497 | 1.14 | 0.8410 $\pm$ 0.0037 | 0.8754
    | 1.27 | 0.2407 $\pm$ 0.0005 | 0.3426 | 1.06 |'
  prefs: []
  type: TYPE_TB
- en: '| CEAL(Entropy) | 0.8167 $\pm$ 0.0039 | 0.8459 | 2.05 | 0.8477 $\pm$ 0.0026
    | 0.8826 | 1.29 | 0.2347 $\pm$ 0.0009 | 0.3400 | 1.03 |'
  prefs: []
  type: TYPE_TB
- en: '| KMeans | 0.7863 $\pm$ 0.0068 | 0.8222 | 1.56 | 0.8260 $\pm$ 0.0036 | 0.8525
    | 5.93 | 0.2447 $\pm$ 0.0009 | 0.3385 | 0.55 |'
  prefs: []
  type: TYPE_TB
- en: '| KMeans(GPU) | 0.7990 $\pm$ 0.0022 | 0.8362 | 1.90 | 0.8343 $\pm$ 0.0012 |
    0.8657 | 7.58 | 0.1340 $\pm$ 0.0008 | 0.2288 | 0.79 |'
  prefs: []
  type: TYPE_TB
- en: '| KCenter | * | * | * | 0.8353 $\pm$ 0.0019 | 0.8466 | 0.76 | 0.2540 $\pm$
    0.0000 | 0.3460 | 0.43 |'
  prefs: []
  type: TYPE_TB
- en: '| VAAL | 0.8027 $\pm$ 0.0019 | 0.8363 | 1.78 | 0.8297 $\pm$ 0.0012 | 0.8535
    | 12.78 | 0.1313 $\pm$ 0.0005 | 0.2191 | 0.15 |'
  prefs: []
  type: TYPE_TB
- en: '| BADGE(KMeans++) | * | * | * | 0.8437 $\pm$ 0.0019 | 0.8662 | 19.36 | # |
    # | # |'
  prefs: []
  type: TYPE_TB
- en: '| AdvBIM | # | # | # | 0.8390 $\pm$ 0.0016 | 0.8737 | 22.86 | # | # | # |'
  prefs: []
  type: TYPE_TB
- en: '| LPL | 0.5447 $\pm$ 0.0023 | 0.6555 | 1.09 | 0.7600 $\pm$ 0.0094 | 0.8471
    | 4.00 | 0.0090 $\pm$ 0.0000 | 0.0051 | 0.40 |'
  prefs: []
  type: TYPE_TB
- en: '| WAAL | 0.8293 $\pm$ 0.0005 | 0.8423 | 1.59 | 0.8703 $\pm$ 0.0012 | 0.8984
    | 18.42 | 0.0157 $\pm$ 0.0005 | 0.0050 | 0.58 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Results of DAL comparative experiments, including *EMNIST*, *FashionMNIST*
    and *TinyImageNet*. We report the AUBC for accuracy, final accuracy (F-acc) after
    quota $Q$ is exhausted, and the average running time of the whole AL processes
    (including training and querying processes) relative to Random. We rank F-acc
    and AUBC of each task with top 1st, 2nd and 3rd with red, teal and blue respectively.
    “$*$” indicates that the experiment needed too much memory, e.g., KCenter on *EMNIST*,
    while “#” indicates that the experiment has not been completed yet.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | *PneumoniaMNIST* | *BreakHis* |'
  prefs: []
  type: TYPE_TB
- en: '| Model | AUBC | F-acc | Time | AUBC | F-acc | Time |'
  prefs: []
  type: TYPE_TB
- en: '| Full | $-$ | $0.9039$ | $-$ | $-$ | $0.8306$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 0.8283 $\pm$ 0.0073 | 0.9077 | 1.00 | 0.8010 $\pm$ 0.0014 | 0.8150
    | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| LeastConf | 0.8520 $\pm$ 0.0022 | 0.9097 | 0.62 | 0.8213 $\pm$ 0.0017 | 0.8302
    | 0.91 |'
  prefs: []
  type: TYPE_TB
- en: '| LeastConfD | 0.8243 $\pm$ 0.0127 | 0.8654 | 1.10 | 0.8130 $\pm$ 0.0022 |
    0.8269 | 1.02 |'
  prefs: []
  type: TYPE_TB
- en: '| Margin | 0.8580 $\pm$ 0.0045 | 0.8859 | 0.96 | 0.8217 $\pm$ 0.0009 | 0.8289
    | 1.12 |'
  prefs: []
  type: TYPE_TB
- en: '| MarginD | 0.8230 $\pm$ 0.0054 | 0.9149 | 1.27 | 0.8257 $\pm$ 0.0012 | 0.8364
    | 1.16 |'
  prefs: []
  type: TYPE_TB
- en: '| Entropy | 0.8570 $\pm$ 0.0028 | 0.9132 | 0.95 | 0.8213 $\pm$ 0.0005 | 0.8251
    | 1.30 |'
  prefs: []
  type: TYPE_TB
- en: '| EntropyD | 0.8177 $\pm$ 0.0045 | 0.8710 | 1.26 | 0.8017 $\pm$ 0.0009 | 0.8115
    | 1.49 |'
  prefs: []
  type: TYPE_TB
- en: '| BALD | 0.8270 $\pm$ 0.0014 | 0.9204 | 0.87 | 0.8150 $\pm$ 0.0016 | 0.8334
    | 0.89 |'
  prefs: []
  type: TYPE_TB
- en: '| MeanSTD | 0.7827 $\pm$ 0.0041 | 0.8802 | 1.18 | 0.7960 $\pm$ 0.0016 | 0.8076
    | 0.98 |'
  prefs: []
  type: TYPE_TB
- en: '| VarRatio | 0.8530 $\pm$ 0.0065 | 0.8672 | 0.72 | 0.8270 $\pm$ 0.0008 | 0.8365
    | 0.74 |'
  prefs: []
  type: TYPE_TB
- en: '| CEAL(Entropy) | 0.8543 $\pm$ 0.0102 | 0.9179 | 0.75 | 0.8143 $\pm$ 0.0025
    | 0.8206 | 0.80 |'
  prefs: []
  type: TYPE_TB
- en: '| KMeans | 0.8243 $\pm$ 0.0042 | 0.9044 | 0.64 | 0.8203 $\pm$ 0.0024 | 0.8394
    | 0.68 |'
  prefs: []
  type: TYPE_TB
- en: '| KMeans(GPU) | 0.8333 $\pm$ 0.0053 | 0.9155 | 1.04 | 0.8140 $\pm$ 0.0016 |
    0.8323 | 1.38 |'
  prefs: []
  type: TYPE_TB
- en: '| KCenter | 0.8130 $\pm$ 0.0057 | 0.9189 | 1.01 | 0.8027 $\pm$ 0.0012 | 0.8289
    | 1.45 |'
  prefs: []
  type: TYPE_TB
- en: '| VAAL | 0.8393 $\pm$ 0.0063 | 0.9064 | 5.33 | 0.8197 $\pm$ 0.0021 | 0.8344
    | 2.81 |'
  prefs: []
  type: TYPE_TB
- en: '| BADGE(KMeans++) | 0.8340 $\pm$ 0.0022 | 0.9066 | 0.56 | 0.8343 $\pm$ 0.0012
    | 0.8470 | 0.68 |'
  prefs: []
  type: TYPE_TB
- en: '| AdvBIM | 0.8297 $\pm$ 0.0087 | 0.9197 | 3.61 | 0.8240 $\pm$ 0.0008 | 0.8337
    | 2.52 |'
  prefs: []
  type: TYPE_TB
- en: '| LPL | 0.8593 $\pm$ 0.0087 | 0.9346 | 4.53 | 0.8277 $\pm$ 0.0009 | 0.8316
    | 2.66 |'
  prefs: []
  type: TYPE_TB
- en: '| WAAL | 0.9663 $\pm$ 0.0012 | 0.9564 | 5.24 | 0.8620 $\pm$ 0.0036 | 0.8698
    | 2.78 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Results of DAL comparative experiments, including *PneumoniaMNIST*
    and *BreakHis*. We report the AUBC for overall accuracy, final accuracy (F-acc)
    after quota $Q$ is exhausted, and the average running time of the whole AL processes
    (including training and querying processes) relative to Random.We rank F-acc and
    AUBC of each task with top 1st, 2nd and 3rd with red, teal and blue respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | *Waterbird (mismatch)* | *Waterbird (mismatch w/ pre-train)* | *Waterbird
    (worst group)* | *Waterbird (worst group w/ pre-train)* |'
  prefs: []
  type: TYPE_TB
- en: '| Model | AUBC | F-acc | Time | AUBC | F-acc | Time | AUBC | F-acc | Time |
    AUBC | F-acc | Time |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 0.2892 $\pm$ 0.1309 | 0.2713 | 1.00 | 0.6378 $\pm$ 0.0720 | 0.7209
    | 1.00 | 0.0405 $\pm$ 0.0321 | 0.0535 | 1.00 | 0.3893 $\pm$ 0.1329 | 0.5177 |
    1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| LeastConf | 0.2587 $\pm$ 0.1158 | 0.2224 | 2.78 | 0.7139 $\pm$ 0.0418 | 0.7402
    | 1.00 | 0.0452 $\pm$ 0.0310 | 0.0467 | 2.78 | 0.5508 $\pm$ 0.0872 | 0.5348 |
    1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| LeastConfD | 0.2691 $\pm$ 0.1525 | 0.2635 | 2.88 | 0.6115 $\pm$ 0.0795 |
    0.7229 | 2.01 | 0.0458 $\pm$ 0.0334 | 0.0621 | 2.88 | 0.3869 $\pm$ 0.1383 | 0.5576
    | 2.01 |'
  prefs: []
  type: TYPE_TB
- en: '| Margin | 0.2592 $\pm$ 0.1167 | 0.3034 | 0.99 | 0.7117 $\pm$ 0.0408 | 0.7450
    | 1.00 | 0.0449 $\pm$ 0.0308 | 0.0389 | 0.99 | 0.5509 $\pm$ 0.0851 | 0.5475 |
    1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| MarginD | 0.2923 $\pm$ 0.1512 | 0.3089 | 1.19 | 0.6190 $\pm$ 0.0779 | 0.7109
    | 1.19 | 0.0381 $\pm$ 0.0303 | 0.0581 | 1.19 | 0.3943 $\pm$ 0.1358 | 0.5675 |
    1.19 |'
  prefs: []
  type: TYPE_TB
- en: '| Entropy | 0.2578 $\pm$ 0.1173 | 0.3255 | 1.03 | 0.7133 $\pm$ 0.0396 | 0.7289
    | 1.03 | 0.0475 $\pm$ 0.0337 | 0.0498 | 1.03 | 0.5516 $\pm$ 0.0825 | 0.5540 |
    1.03 |'
  prefs: []
  type: TYPE_TB
- en: '| EntropyD | 0.2612 $\pm$ 0.1463 | 0.3608 | 1.19 | 0.6221 $\pm$ 0.0842 | 0.7163
    | 2.32 | 0.0466 $\pm$ 0.0342 | 0.1158 | 1.19 | 0.3910 $\pm$ 0.1393 | 0.5582 |
    2.32 |'
  prefs: []
  type: TYPE_TB
- en: '| BALD | 0.2929 $\pm$ 0.1333 | 0.3421 | 2.02 | 0.5798 $\pm$ 0.0943 | 0.7107
    | 1.64 | 0.0405 $\pm$ 0.0287 | 0.1022 | 2.02 | 0.3788 $\pm$ 0.1488 | 0.5680 |
    1.64 |'
  prefs: []
  type: TYPE_TB
- en: '| MeanSTD | 0.2789 $\pm$ 0.1386 | 0.2237 | 2.20 | 0.5999 $\pm$ 0.0925 | 0.7003
    | 2.32 | 0.0461 $\pm$ 0.0298 | 0.0472 | 2.20 | 0.3782 $\pm$ 0.1473 | 0.5665 |
    2.32 |'
  prefs: []
  type: TYPE_TB
- en: '| VarRatio | 0.2512 $\pm$ 0.1063 | 0.2307 | 1.88 | 0.7111 $\pm$ 0.0413 | 0.7333
    | 1.06 | 0.0465 $\pm$ 0.0305 | 0.0696 | 1.88 | 0.5509 $\pm$ 0.0854 | 0.5488 |
    1.06 |'
  prefs: []
  type: TYPE_TB
- en: '| CEAL(Entropy) | 0.2863 $\pm$ 0.1510 | 0.3436 | 1.93 | 0.7119 $\pm$ 0.0432
    | 0.7224 | 1.43 | 0.0440 $\pm$ 0.0333 | 0.0363 | 1.93 | 0.5458 $\pm$ 0.0852 |
    0.5680 | 1.43 |'
  prefs: []
  type: TYPE_TB
- en: '| KMeans | 0.2809 $\pm$ 0.1130 | 0.2984 | 2.31 | 0.5867 $\pm$ 0.0992 | 0.7005
    | 2.33 | 0.0428 $\pm$ 0.0275 | 0.0457 | 2.31 | 0.3499 $\pm$ 0.1335 | 0.4943 |
    2.33 |'
  prefs: []
  type: TYPE_TB
- en: '| KMeans(GPU) | 0.2361 $\pm$ 0.1354 | 0.2831 | 1.31 | 0.6131 $\pm$ 0.0811 |
    0.6922 | 1.31 | 0.0544 $\pm$ 0.0384 | 0.0524 | 1.31 | 0.4669 $\pm$ 0.1173 | 0.5774
    | 1.31 |'
  prefs: []
  type: TYPE_TB
- en: '| KCenter | 0.3272 $\pm$ 0.1520 | 0.3870 | 2.13 | 0.6810 $\pm$ 0.0665 | 0.7156
    | 2.12 | 0.0315 $\pm$ 0.0257 | 0.0363 | 2.13 | 0.4872 $\pm$ 0.1188 | 0.5696 |
    2.12 |'
  prefs: []
  type: TYPE_TB
- en: '| VAAL | 0.3521 $\pm$ 0.1716 | 0.3500 | 9.79 | 0.6469 $\pm$ 0.0517 | 0.7301
    | 9.69 | 0.0323 $\pm$ 0.0306 | 0.0633 | 9.79 | 0.3688 $\pm$ 0.1656 | 0.5135 |
    9.69 |'
  prefs: []
  type: TYPE_TB
- en: '| BADGE(KMeans++) | 0.2637 $\pm$ 0.1347 | 0.3215 | 2.48 | 0.7118 $\pm$ 0.0449
    | 0.7260 | 1.94 | 0.0458 $\pm$ 0.0328 | 0.0639 | 2.48 | 0.5391 $\pm$ 0.1005 |
    0.5587 | 1.94 |'
  prefs: []
  type: TYPE_TB
- en: '| AdvBIM | # | # | # | 0.6309 $\pm$ 0.0836 | 0.6965 | 10.64 | # | # | # | 0.4164
    $\pm$ 0.1499 | 0.5992 | 10.64 |'
  prefs: []
  type: TYPE_TB
- en: '| LPL | 0.6332 $\pm$ 0.1979 | 0.7783 | 4.50 | 0.5494 $\pm$ 0.2175 | 0.7786
    | 4.51 | 0.0187 $\pm$ 0.0035 | 0.0005 | 4.50 | 0.1094 $\pm$ 0.0112 | 0.0099 |
    4.51 |'
  prefs: []
  type: TYPE_TB
- en: '| WAAL | 0.5543 $\pm$ 0.2544 | 0.7784 | 6.88 | 0.6036 $\pm$ 0.2138 | 0.7782
    | 6.81 | 0.0382 $\pm$ 0.0054 | 0.0005 | 6.88 | 0.0390 $\pm$ 0.0055 | 0.0010 |
    6.81 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Results of *Waterbird*. We report the AUBC for mismatch and worst
    group accuracy, final accuracy (F-acc) after quota $Q$ is exhausted, and the average
    running time of the whole AL processes (including training and querying processes)
    relative to Random. We bold F-acc values that are higher than full performance.
    We did not rank the top three methods since labeling them is not of great reference
    value. “#” indicates that the experiment has not been completed yet.'
  prefs: []
  type: TYPE_NORMAL
- en: E.2 DAL method ranking and summarizing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We next consider the overall performance of DAL methods on the eight standard
    image classification datasets and two medical image analysis datasets using win-tie-loss
    counts, respectively, as shown in Table [9](#A5.T9 "Table 9 ‣ E.2 DAL method ranking
    and summarizing ‣ Appendix E Completed Experimental Results ‣ A Comparative Survey
    of Deep Active Learning"). We use a margin of $0.5\%$, e.g., a “win” is counted
    for method A if it outperforms method B by $0.5\%$ in pairwise comparison. Table
    [9](#A5.T9 "Table 9 ‣ E.2 DAL method ranking and summarizing ‣ Appendix E Completed
    Experimental Results ‣ A Comparative Survey of Deep Active Learning") shows the
    advantage of uncertainty-based DAL methods likeLeastConfD (3rd), and pseudo labeling
    for enhancing uncertainty-based DAL methods, i.e., CEAL (2nd). WAAL perform the
    best. Additionally, Dropout method also can improve DAL methods, e.g., LeastConfD
    ranks 3nd while LeastConf only ranks 9th. LPL only 13th. Although they achieve
    the best performances on some datasets (e.g., *SVHN*, *CIFAR10*) and have high
    win counts, they also perform extremely poorly on other datasets (e.g., *Tiny
    ImageNet*), which contributes to their low ranking. VAAL and KMeans perform even
    worse than Random. AdvBIM ranks far behind due to many incomplete tasks. This
    is a drawback of AdvBIM (also of AdvDeepFool), that is, these methods spend too
    much for re-calculating adversarial distance $\mathbf{r}$ for each unlabeled data
    sample per AL round.
  prefs: []
  type: TYPE_NORMAL
- en: On medical image analysis tasks, both WAAL and LPL outperform other DAL approaches,
    which constantly shows the advantage of DAL with enhancing techniques. Combined
    strategy, BADGE also obtained a good ranking (4th). More noticeably, BADGE always
    obtains comparable performances on various tasks. Therefore, for new/unseen tasks/data,
    we recommend first trying combined DAL approaches. In medical image analysis tasks,
    VAAL perform better than standard image classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | *Standard Image Classification (8 datasets)* | *Medical Image Analysis
    (2 datasets)* |'
  prefs: []
  type: TYPE_TB
- en: '| Rank | Method | $\text{win}-\text{tie}-\text{loss}$ | Method | $\text{win}-\text{tie}-\text{loss}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | WAAL | $103-2-31$ | WAAL | $34-0-0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | CEAL | $74-35-27$ | LPL | $25-6-3$ |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | LeastConfD | $63-59-14$ | VarRatio | $23-7-4$ |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | MarginD | $61-55-20$ | BADGE | $24-1-9$ |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Margin | $60-57-19$ | Margin | $19-10-5$ |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | BALD | $56-59-21$ | Entropy | $18-11-5$ |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | EntropyD | $54-55-27$ | LeastConf | $18-9-7$ |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | VarRatio | $52-58-26$ | VAAL | $16-9-12$ |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | LeastConf | $51-53-32$ | CEAL | $15-7-12$ |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Badge | $46-49-41$ | AdvBIM | $13-11-10$ |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | MeanSTD | $44-50-42$ | MarginD | $12-9-13$ |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | Entropy | $40-54-42$ | KMeans | $10-9-15$ |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | LPL | $57-4-75$ | BALD | $7-8-19$ |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | KCenter | $41-34-61$ | LeastConfD | $7-6-21$ |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | Random | $26-23-87$ | Random | $4-7-23$ |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | VAAL | $20-15-101$ | EntropyD | $2-3-29$ |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | KMeans | $18-9-109$ | KCenter | $2-3-29$ |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | AdvBIM | $10-25-101$ | MeanSTD | $0-1-33$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Comparison of DAL methods using win-tie-loss across 8 datasets on
    standard image classification tasks and 2 medical image analysis tasks with AUBC
    (acc). Methods are ranked by $2\times win+tie$.'
  prefs: []
  type: TYPE_NORMAL
- en: To observe the performance differences on various DAL methods in varying AL
    stages, we provide overall accuracy-budget curves on multiple datasets, as shown
    in Figure [5](#A5.F5 "Figure 5 ‣ E.2 DAL method ranking and summarizing ‣ Appendix
    E Completed Experimental Results ‣ A Comparative Survey of Deep Active Learning").
    From this figure, it could be observed that LPL is weak in the early stage of
    AL processes due to the inaccurate loss prediction trained on insufficient labeled
    data. In later stages, by co-training LossNet and the basic classifier on more
    labeled data, LossNet has demonstrated its ability to enhance the basic classifier.
    In contrast, WAAL performs better in the early stage of AL processes due to the
    design of the loss function that is more suitable for AL. It helps distinguish
    labeled and unlabeled samples and select more representative data samples in the
    early stage. Therefore, WAAL brings more benefits when limiting the budget for
    labeling costs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6a262f3a12aa7e2787701c35c257c3db.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) MNIST
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3b076e9f3271f9c8a92e1e69c1d76b11.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) FashionMNIST
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4fc1155b1075c5548f6bd6e2567e7347.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) SVHN
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7de4d6df4057d5d18a332df33d7456e2.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) CIFAR10
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5b1a031f7616d96746ceb45f8f3a8bbc.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) CIFAR10-imb
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/73511e7976b9ec8046cc7e7626806027.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) CIFAR100
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Overall accuracy-budget curve of *MNIST*, *FashionMNIST*, *CIFAR10*,
    *CIFAR10 (imb)* and *CIFAR100* datasets. The mean and standard deviation of the
    AUBC (acc) performance over $3$ trials is shown in parentheses in the legend.'
  prefs: []
  type: TYPE_NORMAL
- en: 'E.3 Ablation study: numbers of training epochs and batch size'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We present the accuracy-budget curves using different batch sizes and training
    epochs, as shown in Figure [6](#A5.F6 "Figure 6 ‣ E.3 Ablation study: numbers
    of training epochs and batch size ‣ Appendix E Completed Experimental Results
    ‣ A Comparative Survey of Deep Active Learning"). A detailed analysis of this
    ablation study is in the main paper.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4b660fbd811402351c67f4e889d6be3d.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) $b=1000$, $\#e=5$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d9f9bc149d15b6cc7c7dca771974139f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) $b=2000$, $\#e=5$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/19ddccf130e52180ac51ae2a8f2a5158.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) $b=4000$, $\#e=5$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/596612cfddf21c51b0c11bebc3025eff.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) $b=10000$, $\#e=5$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6198a95b9cd9e8e2d5a7cc668c15ae64.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) $b=1000$, $\#e=10$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/033192d9dcf3561b8fc4f6cdd4c9317f.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) $b=2000$, $\#e=10$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad7d1630b8edb5ab7148dd5032796940.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) $b=4000$, $\#e=10$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0cf093ceb14ce548ecb362f1bac20e66.png)'
  prefs: []
  type: TYPE_IMG
- en: (h) $b=10000$, $\#e=10$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f5a25f38deac5e4ada158cfcffcbf064.png)'
  prefs: []
  type: TYPE_IMG
- en: (i) $b=1000$, $\#e=15$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/69c82c7a00419cdaf7e8c236b96953e6.png)'
  prefs: []
  type: TYPE_IMG
- en: (j) $b=2000$, $\#e=15$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b1af6eed1564f7e8ad6cbda5f614cfd8.png)'
  prefs: []
  type: TYPE_IMG
- en: (k) $b=4000$, $\#e=15$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9be919672968d59e527c64b1ba87b3b3.png)'
  prefs: []
  type: TYPE_IMG
- en: (l) $b=10000$, $\#e=15$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/58cc7e809e3ba2f3ca9793f49d730141.png)'
  prefs: []
  type: TYPE_IMG
- en: (m) $b=1000$, $\#e=20$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7a349a4b455daa94cffeb3ed53e19320.png)'
  prefs: []
  type: TYPE_IMG
- en: (n) $b=2000$, $\#e=20$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b5d420d4f3f979d080e20211fffc35db.png)'
  prefs: []
  type: TYPE_IMG
- en: (o) $b=4000$, $\#e=20$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f4e7ae94904cd930226c56e1b2bc31e9.png)'
  prefs: []
  type: TYPE_IMG
- en: (p) $b=10000$, $\#e=20$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f2f7ee06c799358ac48fa394b616c92d.png)'
  prefs: []
  type: TYPE_IMG
- en: (q) $b=1000$, $\#e=25$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4aecf3bd27a0f21fc23d66b1c90a3f3c.png)'
  prefs: []
  type: TYPE_IMG
- en: (r) $b=2000$, $\#e=25$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/249cc7e265a86ed660bf714c88f7cfb6.png)'
  prefs: []
  type: TYPE_IMG
- en: (s) $b=4000$, $\#e=25$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/74f77db68b88434273a83e14665f1014.png)'
  prefs: []
  type: TYPE_IMG
- en: (t) $b=10000$, $\#e=25$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8187aa3cbb0fe268c3fd373398fec636.png)'
  prefs: []
  type: TYPE_IMG
- en: (u) $b=1000$, $\#e=30$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a7b70e2f24b6e4d9fba2c177533e02fc.png)'
  prefs: []
  type: TYPE_IMG
- en: (v) $b=2000$, $\#e=30$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da57b746beb8627cd52cce9278a03265.png)'
  prefs: []
  type: TYPE_IMG
- en: (w) $b=4000$, $\#e=30$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1ee7cf8eeed9273d12190afc2d3f4361.png)'
  prefs: []
  type: TYPE_IMG
- en: (x) $b=10000$, $\#e=30$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: The accuracy-budget curve of different DAL methods on *CIFAR10* dataset
    with different batch sizes and numbers of training epochs in each active learning
    round. From left to right, the value of $b$ equals to $1,000$, $2,000$, $4,000$
    and $10,000$. From top to bottom, the value of training epoch equals to $5$, $10$,
    $15$, $20$, $25$ and $30$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'E.4 Ablation study: w/ and w/o pre-training techniques'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The detailed results of MNIST, Waterbird w/ and w/o pre-training techniques
    are shown in Tables [4](#A5.T4 "Table 4 ‣ E.1.2 Performance of Medical Image Analysis
    tasks. ‣ E.1 Overall experiments ‣ Appendix E Completed Experimental Results ‣
    A Comparative Survey of Deep Active Learning") and [8](#A5.T8 "Table 8 ‣ E.1.2
    Performance of Medical Image Analysis tasks. ‣ E.1 Overall experiments ‣ Appendix
    E Completed Experimental Results ‣ A Comparative Survey of Deep Active Learning"),
    including the overall accuracy, mismatch group accuracy and worst group accuracy.
    We record AUBC (acc) with mean and standard deviation over 3 trials, F-acc and
    average running time. We have detailed analysed this experiment in main paper.
    From Tables [4](#A5.T4 "Table 4 ‣ E.1.2 Performance of Medical Image Analysis
    tasks. ‣ E.1 Overall experiments ‣ Appendix E Completed Experimental Results ‣
    A Comparative Survey of Deep Active Learning") and [8](#A5.T8 "Table 8 ‣ E.1.2
    Performance of Medical Image Analysis tasks. ‣ E.1 Overall experiments ‣ Appendix
    E Completed Experimental Results ‣ A Comparative Survey of Deep Active Learning"),
    we could observe that on *Waterbird* w/o pre-train, most typical DAL sampling
    methods like LeastConf, Margin, Entropy, KCenter, etc., perform even worse than
    Random.
  prefs: []
  type: TYPE_NORMAL
