- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:03:08'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:03:08
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2001.02600] Deep Learning for Free-Hand Sketch: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2001.02600] 深度学习与自由手绘：一项调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2001.02600](https://ar5iv.labs.arxiv.org/html/2001.02600)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2001.02600](https://ar5iv.labs.arxiv.org/html/2001.02600)
- en: 'Deep Learning for Free-Hand Sketch: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习与自由手绘：一项调查
- en: Peng Xu, Timothy M. Hospedales, Qiyue Yin, Yi-Zhe Song, Tao Xiang, and Liang
    Wang This paper is accepted by IEEE TPAMI.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 彭旭，Timothy M. Hospedales，Qiyue Yin，Yi-Zhe Song，Tao Xiang，和梁旺 本文已被IEEE TPAMI接受。
- en: 'Corresponding author: Peng Xu who is with Department of Engineering Science,
    University of Oxford.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通讯作者：彭旭，现为牛津大学工程科学系成员。
- en: 'E-mail: peng.xu@eng.ox.ac.uk,'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：peng.xu@eng.ox.ac.uk,
- en: 'HomePage: [http://www.pengxu.net/](http://www.pengxu.net/)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 主页：[http://www.pengxu.net/](http://www.pengxu.net/)
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Free-hand sketches are highly illustrative, and have been widely used by humans
    to depict objects or stories from ancient times to the present. The recent prevalence
    of touchscreen devices has made sketch creation a much easier task than ever and
    consequently made sketch-oriented applications increasingly popular. The progress
    of deep learning has immensely benefited free-hand sketch research and applications.
    This paper presents a comprehensive survey of the deep learning techniques oriented
    at free-hand sketch data, and the applications that they enable. The main contents
    of this survey include: (i) A discussion of the intrinsic traits and unique challenges
    of free-hand sketch, to highlight the essential differences between sketch data
    and other data modalities, e.g., natural photos. (ii) A review of the developments
    of free-hand sketch research in the deep learning era, by surveying existing datasets,
    research topics, and the state-of-the-art methods through a detailed taxonomy
    and experimental evaluation. (iii) Promotion of future work via a discussion of
    bottlenecks, open problems, and potential research directions for the community.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自由手绘非常具有表现力，人类自古至今广泛使用它来描绘对象或故事。触摸屏设备的普及使得素描创作变得比以往更容易，因此以素描为导向的应用也越来越受欢迎。深度学习的发展极大地推动了自由手绘研究和应用。本文对面向自由手绘数据的深度学习技术及其应用进行了全面的调查。此调查的主要内容包括：（i）讨论自由手绘的内在特征和独特挑战，以突出素描数据与其他数据模态（如自然照片）之间的本质差异。（ii）回顾深度学习时代自由手绘研究的发展，通过详细的分类和实验评估，调查现有的数据集、研究主题和最先进的方法。（iii）通过讨论瓶颈、开放问题和潜在的研究方向，促进未来的研究工作。
- en: 'Index Terms:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Free-Hand Sketch, Deep Learning, Survey, Introductory, Taxonomy.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自由手绘，深度学习，调查，介绍，分类。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Free-hand sketch is a universal communication and art modality that transcends
    barriers to link human societies. It has been used from ancient times to today,
    comes naturally to children before writing, and transcends language barriers.
    Different from other related forms of expression such as professional sketch,
    forensic sketch, cartoons, technical drawing, and oil paintings, it requires no
    training and no special equipment. As such free-hand sketch is not bound by age,
    race, language, geography, or national boundaries. It can be regarded as an expression
    of the brain’s internal representation of the world, whether perceived or imagined.
    Smiling faces, for example, are always recognized by humans (Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Deep Learning for Free-Hand Sketch: A Survey")).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '自由手绘是一种普遍的沟通和艺术方式，能够超越障碍将人类社会联系起来。它自古至今一直被使用，对儿童在写字之前自然产生，并且超越语言障碍。不同于其他相关的表达形式，如专业素描、法医素描、漫画、技术图纸和油画，它不需要培训和特殊设备。因此，自由手绘不受年龄、种族、语言、地理或国界的限制。它可以被视为大脑对世界的内部表征的表达，无论是感知还是想象。例如，微笑的面孔总是被人类识别（图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning for Free-Hand Sketch: A
    Survey")）。'
- en: 'Sketches can convey many words, or even concepts that are hard to convey at
    all in words. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning for
    Free-Hand Sketch: A Survey") shows several examples covering ancient and contemporary;
    literal and emotional; iconic and descriptive; abstract and concrete; and different
    media of drawing.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '素描可以传达许多词汇，甚至是那些用词语难以传达的概念。图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning
    for Free-Hand Sketch: A Survey") 展示了几种涵盖古代与现代、字面与情感、图标与描述、抽象与具体以及不同绘画媒介的示例。'
- en: Free-hand sketch can be illustrative, despite its highly concise and abstract
    nature, making it useful in various scenarios such as communication and design.
    Therefore, free-hand sketch has been widely studied in computer vision and pattern
    recognition  [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5)], computer graphics [[6](#bib.bib6), [7](#bib.bib7)], human computer
    interaction [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)],
    robotics [[12](#bib.bib12)], and cognitive science [[13](#bib.bib13)] communities.
    In particular, early research can be traced back to the 1960s and 1970s [[14](#bib.bib14),
    [15](#bib.bib15)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管手绘草图高度简洁和抽象，但它可以具有说明性，使其在诸如交流和设计等各种场景中非常有用。因此，手绘草图在计算机视觉和模式识别[[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]、计算机图形学[[6](#bib.bib6),
    [7](#bib.bib7)]、人机交互[[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]、机器人技术[[12](#bib.bib12)]和认知科学[[13](#bib.bib13)]领域得到了广泛研究。特别是，早期研究可以追溯到1960年代和1970年代[[14](#bib.bib14),
    [15](#bib.bib15)]。
- en: 'However, free-hand sketch is fundamentally different from natural photos¹¹1Images
    can include both free-hand sketch and natural photos, etc. In this survey, “photo”
    denotes natural photo images obtained by a camera such as in ImageNet unless otherwise
    specified.. Sketch images provide a special data modality/domain that has both
    domain-unique challenges (e.g., highly sparse, abstract, artist-dependent) and
    advantages (e.g., lack of background, use of iconic representation). It is also
    unique in that free-hand sketch can be stored and processed in multiple representations
    as its source is a dynamic ‘pen’ movement. These include static pixel space (when
    rendered as an image), dynamic stroke coordinate space (when considered as a time
    series), and geometric graph space (when considered topologically) – as discussed
    in Section [2](#S2 "2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey").
    Thus, from a pattern recognition or machine intelligence perspective, these unique
    traits of free-hand sketch often lead to sketch-specific model designs in order
    to exploit sketch-specific data properties and overcome sketch-specific challenges
    when analyzing sketches for recognition, generation, and so on. This survey will
    review these considerations and designs in detail.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，手绘草图在本质上与自然照片不同¹¹1 图像可以包括手绘草图和自然照片等。在本次调查中，“照片”指的是通过相机获得的自然照片，如ImageNet，除非另有说明。草图图像提供了一种特殊的数据模态/领域，具有领域特有的挑战（例如，高度稀疏、抽象、艺术家依赖）和优势（例如，缺乏背景、使用图标表示）。此外，手绘草图独特之处在于它可以以多种表示方式进行存储和处理，因为其源是动态的‘笔’运动。这些包括静态像素空间（当作为图像呈现时）、动态笔画坐标空间（当视为时间序列时）以及几何图图空间（当从拓扑角度考虑时）——如第[2](#S2
    "2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey")节所述。因此，从模式识别或机器智能的角度来看，这些手绘草图的独特特性通常导致草图特定的模型设计，以利用草图特定的数据特性并克服草图特定的挑战，以便在分析草图时进行识别、生成等。本次调查将详细回顾这些考虑因素和设计。'
- en: '![Refer to caption](img/b06108050236e9a8278f81dded0a2dcb.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b06108050236e9a8278f81dded0a2dcb.png)'
- en: 'Figure 1: Diverse free-hand sketches in human daily life. The masks (rough
    and simplified) on the bottom are from [[16](#bib.bib16)]. The scene-level sketch
    (cloud, trees, and giraffes) on the bottom right corner is from SketchyCOCO dataset [[17](#bib.bib17)].'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：人类日常生活中的多样化手绘草图。底部的面具（粗糙和简化的）来自于[[16](#bib.bib16)]。右下角的场景级草图（云、树木和长颈鹿）来自于SketchyCOCO数据集[[17](#bib.bib17)]。
- en: '![Refer to caption](img/a4a1398aa33da67d7dd95f885307aa12.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a4a1398aa33da67d7dd95f885307aa12.png)'
- en: 'Figure 2: Drawing samples out-of-scope of our focus on free-hand sketch.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：超出我们关注范围的绘画样本。
- en: 'Sketch research and applications in both industry and academia have boomed
    in recent years due to the prevalence of touchscreen devices (e.g., smartphone,
    tablet) that make acquiring sketch data much easier than ever; as well as the
    rapid development of deep learning techniques that are achieving state-of-the-art performance
    in diverse artificial intelligence tasks. This boom has occurred on several fronts:
    (i) Some classic research topics (e.g., sketch recognition, sketch-based image
    retrieval, sketch-based 3D shape retrieval) have been re-studied in a deep learning
    context [[18](#bib.bib18), [3](#bib.bib3), [7](#bib.bib7), [4](#bib.bib4), [2](#bib.bib2),
    [19](#bib.bib19)] resulting in significant performance improvements. (ii) Some
    brand-new topics have been proposed based on deep learning, e.g., deep learning
    based sketch generation/synthesis [[5](#bib.bib5)], sketch-based model generation [[20](#bib.bib20)],
    reinforcement learning based sketch abstraction [[21](#bib.bib21)], adversarial
    sketch based image editing [[22](#bib.bib22)], graph neural network based sketch
    recognition [[23](#bib.bib23)], graph convolution-based sketch semantic segmentation [[24](#bib.bib24)],
    and sketch based software prototyping [[9](#bib.bib9)]. (iii) Beyond global representation
    based tasks (e.g., sketch recognition), more instance-level and stroke-level tasks
    have been further studied or proposed, e.g., instance-level sketch-based image
    retrieval [[4](#bib.bib4)], and deep stroke-level sketch segmentation [[25](#bib.bib25)].
    (iv) Compared with the conventional approach of representing sketches as static
    images [[1](#bib.bib1)], the trends of touchscreen acquisition and deep learning
    have underpinned progress on designing deep network architectures to exploit richer
    representations of sketch. Thanks to works such as SketchRNN [[5](#bib.bib5)],
    the sequential nature of free-hand sketches is now widely modeled by recurrent
    neural network (RNN). (v) More sketch-based applications have appeared, e.g.,
    the online sketch game QuickDraw²²2[https://quickdraw.withgoogle.com](https://quickdraw.withgoogle.com) [[5](#bib.bib5)],
    and sketch-based commodity search engine³³3[http://sketchx.eecs.qmul.ac.uk/demos/](http://sketchx.eecs.qmul.ac.uk/demos/) [[4](#bib.bib4),
    [26](#bib.bib26)]. (vi) Some large-scale sketch datasets have been collected,
    e.g., Sketchy [[7](#bib.bib7)] and Google QuickDraw⁴⁴4[https://github.com/googlecreativelab/quickdraw-dataset](https://github.com/googlecreativelab/quickdraw-dataset) [[5](#bib.bib5)]
    – a million-scale sketch dataset (50M+).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于触屏设备（如智能手机、平板电脑）的普及，使得获取草图数据比以往更容易，以及深度学习技术的迅猛发展在各种人工智能任务中取得了最先进的性能，草图研究和应用在工业和学术界近年来蓬勃发展。这种繁荣表现为几个方面：（i）一些经典研究主题（如草图识别、基于草图的图像检索、基于草图的3D形状检索）在深度学习背景下被重新研究[[18](#bib.bib18),
    [3](#bib.bib3), [7](#bib.bib7), [4](#bib.bib4), [2](#bib.bib2), [19](#bib.bib19)]，从而显著提高了性能。（ii）一些全新的主题已基于深度学习提出，例如，基于深度学习的草图生成/合成[[5](#bib.bib5)]，基于草图的模型生成[[20](#bib.bib20)]，基于强化学习的草图抽象[[21](#bib.bib21)]，对抗性草图图像编辑[[22](#bib.bib22)]，基于图神经网络的草图识别[[23](#bib.bib23)]，基于图卷积的草图语义分割[[24](#bib.bib24)]，以及基于草图的软件原型设计[[9](#bib.bib9)]。（iii）相比于基于全局表示的任务（如草图识别），更多的实例级和笔画级任务已被进一步研究或提出，例如，基于实例的草图图像检索[[4](#bib.bib4)]，以及深度笔画级草图分割[[25](#bib.bib25)]。（iv）与传统的将草图表示为静态图像[[1](#bib.bib1)]的方法相比，触屏采集和深度学习的趋势推动了设计深度网络架构以利用更丰富的草图表示。得益于如SketchRNN[[5](#bib.bib5)]等工作的出现，自由手绘草图的序列性质现在被广泛建模为递归神经网络（RNN）。（v）更多的基于草图的应用出现，例如，在线草图游戏QuickDraw²²2[https://quickdraw.withgoogle.com](https://quickdraw.withgoogle.com)[[5](#bib.bib5)]，以及基于草图的商品搜索引擎³³3[http://sketchx.eecs.qmul.ac.uk/demos/](http://sketchx.eecs.qmul.ac.uk/demos/)
    [[4](#bib.bib4), [26](#bib.bib26)]。（vi）一些大规模的草图数据集已被收集，例如Sketchy[[7](#bib.bib7)]和Google
    QuickDraw⁴⁴4[https://github.com/googlecreativelab/quickdraw-dataset](https://github.com/googlecreativelab/quickdraw-dataset)
    [[5](#bib.bib5)]——一个百万级草图数据集（50M+）。
- en: 1.1 Overview
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 概述
- en: This survey aims to review the state of the free-hand sketch community in deep
    learning era, hoping to bring insights to researchers and assist practitioners
    aiming to build sketch-based applications.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查旨在回顾深度学习时代自由手绘草图社区的现状，期望为研究人员提供见解，并协助致力于构建基于草图的应用程序的从业者。
- en: 'Previous Surveys and Scope This survey focuses on free-hand sketches, not including
    professional (forensic) facial sketch [[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31)], professional pencil sketch (professional
    line drawing/art) [[32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)], professional landscape
    sketch [[39](#bib.bib39)], photo-like edge-maps (artificially rendered ‘sketch’) [[40](#bib.bib40),
    [41](#bib.bib41), [42](#bib.bib42)], cartoon/manga [[43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45)], well-drawn 3D sketch [[46](#bib.bib46)]. (See Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Deep Learning for Free-Hand Sketch: A Survey").)
    In this survey, “sketch” refers to “free-hand sketch”, unless otherwise specified.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '以往的调查和范围 本次调查聚焦于自由手绘草图，不包括专业（法医）面部草图 [[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31)], 专业铅笔画（专业线条绘画/艺术） [[32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)],
    专业风景草图 [[39](#bib.bib39)], 类似照片的边缘图（人工渲染的‘草图’） [[40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42)], 卡通/漫画 [[43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45)],
    画得很好的3D草图 [[46](#bib.bib46)]。 (参见图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep
    Learning for Free-Hand Sketch: A Survey")。) 在本次调查中，“草图”指的是“自由手绘草图”，除非另有说明。'
- en: 'To our knowledge, only a few survey papers [[47](#bib.bib47), [48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51)] were published in the free-hand
    sketch community in the past decade. However, these survey papers [[47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51)]: (i) only
    focus on two research topics, i.e., free-hand sketch based recognition and image/3D
    retrieval. (ii) mainly review classic non-deep techniques. In contrast, the current
    boom in advanced deep methodologies, techniques (hashing), representations (sequential,
    topological), and novel applications (generation, segmentation) makes it timely
    to provide an up-to-date survey of the big picture of research on free-hand sketch.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们了解，在过去十年中，自由手绘草图社区仅发布了少数几篇调查论文 [[47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51)]。然而，这些调查论文 [[47](#bib.bib47), [48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51)]： (i) 仅关注两个研究主题，即基于自由手绘草图的识别和图像/3D检索。
    (ii) 主要回顾经典的非深度技术。相比之下，当前在高级深度方法、技术（哈希）、表示（序列、拓扑）和新应用（生成、分割）方面的蓬勃发展，使得提供一份关于自由手绘草图研究全貌的最新调查显得尤为及时。
- en: 'Contributions We provide a comprehensive survey reviewing the state of the
    field with regards to deep learning techniques, as well as applications of free-hand
    sketch. In particular: (a) We discuss the intrinsic traits, and unique challenges
    and opportunities posed when working with free-hand sketch data. (b) We provide
    a detailed taxonomy of both datasets, and applications covering both uni-modal
    (sketch alone) and multi-modal (relating sketches to photos, text, etc) cases.
    For each specific task, the contemporary landscape of deep learning solutions
    is summarized, and milestone works are described in detail. (c) We discuss current
    bottlenecks, open problems, and potential research directions for free-hand sketch.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一份全面的调查报告，回顾了深度学习技术的领域现状，以及自由手绘草图的应用。特别是： (a) 我们讨论了处理自由手绘草图数据时的内在特征、独特挑战和机遇。
    (b) 我们提供了详细的分类法，涵盖了数据集和应用，涉及单模态（仅草图）和多模态（将草图与照片、文本等相关联）情况。对于每个具体任务，总结了当代深度学习解决方案的现状，并详细描述了里程碑式的工作。
    (c) 我们讨论了自由手绘草图的当前瓶颈、开放问题和潜在的研究方向。
- en: 'Organization of This Survey The rest of this survey is organized as follows.
    Section [2](#S2 "2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey")
    provides background on free-hand sketch, including intrinsic traits, domain-unique
    challenges, milestone techniques of the existing sketch-oriented deep learning
    works, etc. Section [3](#S3 "3 Free-Hand Sketch Datasets ‣ Deep Learning for Free-Hand
    Sketch: A Survey") summarizes representative free-hand sketch datasets. In Section [4](#S4
    "4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey"),
    we provide a comprehensive taxonomy for various sketch-based tasks, and describe
    representative deep learning techniques in detail. Section [4](#S4 "4 Tasks and
    Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey") also presents
    some experiment comparisons based on  TorchSketch⁵⁵5An open source sketch-oriented
    deep learning software library. Please see its GitHub page for details [https://github.com/PengBoXiangShang/torchsketch](https://github.com/PengBoXiangShang/torchsketch). implementation.
    Section [5](#S5 "5 Discussion ‣ Deep Learning for Free-Hand Sketch: A Survey")
    discusses open problems, bottlenecks, and potential research directions before
    the survey concludes in Section [6](#S6 "6 Conclusion ‣ Deep Learning for Free-Hand
    Sketch: A Survey").'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查的组织结构如下：第[2](#S2 "2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey")节提供了关于自由手绘草图的背景，包括内在特征、领域独特的挑战、现有草图导向深度学习工作的里程碑技术等。第[3](#S3
    "3 Free-Hand Sketch Datasets ‣ Deep Learning for Free-Hand Sketch: A Survey")节总结了代表性的自由手绘草图数据集。在第[4](#S4
    "4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")节中，我们提供了各种基于草图任务的全面分类，并详细描述了代表性的深度学习技术。第[4](#S4
    "4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")节还展示了一些基于TorchSketch⁵⁵5的实验比较。请参阅其GitHub页面了解详细信息[https://github.com/PengBoXiangShang/torchsketch](https://github.com/PengBoXiangShang/torchsketch)实现。第[5](#S5
    "5 Discussion ‣ Deep Learning for Free-Hand Sketch: A Survey")节讨论了开放问题、瓶颈和潜在研究方向，最后在第[6](#S6
    "6 Conclusion ‣ Deep Learning for Free-Hand Sketch: A Survey")节中总结了调查内容。'
- en: 'Throughout this survey, bold uppercase and bold lowercase characters denote
    matrices and vectors, respectively. Unless specified otherwise, mathematical symbols
    and abbreviated terms follow the conventions in Table [I](#S1.T1 "TABLE I ‣ 1.1
    Overview ‣ 1 Introduction ‣ Deep Learning for Free-Hand Sketch: A Survey").'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '在本调查中，粗体大写和粗体小写字符分别表示矩阵和向量。除非另有说明，数学符号和缩写术语遵循表[I](#S1.T1 "TABLE I ‣ 1.1 Overview
    ‣ 1 Introduction ‣ Deep Learning for Free-Hand Sketch: A Survey")中的约定。'
- en: 'TABLE I: Notation and abbreviations used in this survey.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE I: 本调查中使用的符号和缩写。'
- en: '| Notations | Descriptions |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 描述 |'
- en: '| --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $\mathcal{X}=\{{\bf X}_{n}\}_{n=1}^{N}$ | sketch sample set |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{X}=\{{\bf X}_{n}\}_{n=1}^{N}$ | sketch样本集 |'
- en: '| ${\bf X}_{m}$, ${\bf X}_{n}$ | $m$-th and $n$-th sketch samples in the sketch
    sample set $\mathcal{X}$ |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| ${\bf X}_{m}$, ${\bf X}_{n}$ |  sketch样本集$\mathcal{X}$中的第$m$个和第$n$个样本 |'
- en: '| $\mathcal{Y}=\{y_{n}\}_{n=1}^{N}$ | associated label set of $\mathcal{X}$
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{Y}=\{y_{n}\}_{n=1}^{N}$ | $\mathcal{X}$ 的相关标签集 |'
- en: '| $y_{n}$ | label of ${\bf X}_{n}$ |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| $y_{n}$ | ${\bf X}_{n}$ 的标签 |'
- en: '| $\mathcal{L}$ | loss function |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}$ | 损失函数 |'
- en: '| ${\bf\Theta}$ | learnable parameters of neural network |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| ${\bf\Theta}$ | 神经网络的可学习参数 |'
- en: '| $\mathcal{F}(\cdot)$ | function mapping or feature extraction |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{F}(\cdot)$ | 函数映射或特征提取 |'
- en: '| $\mathcal{F}_{\Theta}(\cdot)$ | neural network feature extraction, parameterized
    by $\Theta$ |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{F}_{\Theta}(\cdot)$ | 神经网络特征提取，由$\Theta$参数化 |'
- en: '| $\mathcal{D}(\cdot,\cdot)$ | distance metric, e.g., $\ell_{2}$ distance |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{D}(\cdot,\cdot)$ | 距离度量，例如 $\ell_{2}$ 距离 |'
- en: '| $\lambda$ | weighting factor |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| $\lambda$ | 权重因子 |'
- en: '| $\sum$ | summation |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| $\sum$ | 求和 |'
- en: '| $\alpha$, $\beta$, $\gamma$ | hyper parameters set manually |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha$, $\beta$, $\gamma$ | 手动设置的超参数集 |'
- en: '|'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Abbreviated &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 缩写 &#124;'
- en: '&#124; Terms &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 术语 &#124;'
- en: '| Descriptions |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 描述 |'
- en: '| CNN | convolutional neural network |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| CNN | 卷积神经网络 |'
- en: '| GNN | graph neural network |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| GNN | 图神经网络 |'
- en: '| GCN | graph convolutional network |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 图卷积网络 |'
- en: '| RNN | recurrent neural network |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| RNN | 循环神经网络 |'
- en: '| LSTM | Long Short Term Memory |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | 长短期记忆网络 |'
- en: '| GRU | Gated Recurrent Unit |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| GRU | 门控递归单元 |'
- en: '| BERT | Bidirectional Encoder Representations from Transformers |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| BERT | 双向编码器表示的转换器 |'
- en: '| TCN | temporal convolutional neural network |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| TCN | 时序卷积神经网络 |'
- en: '| GAN | generative adversarial network |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| GAN | 生成对抗网络 |'
- en: '| VAE | Variational Auto Encoder |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| VAE | 变分自编码器 |'
- en: '| RL | Reinforcement Learning |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| RL | 强化学习 |'
- en: 2 Background
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 'This section presents background knowledge, including: the intrinsic traits,
    and domain-unique challenges and opportunities of free-hand sketch. In particular,
    we cover the essential differences to natural photos; and a brief development
    history of deep learning for free-hand sketch, summarizing the milestone techniques.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍背景知识，包括：自由手绘草图的内在特征和领域独特挑战与机遇。特别是，我们涵盖了与自然照片的基本差异；以及自由手绘草图深度学习的简要发展历史，总结了里程碑技术。
- en: 2.1 Intrinsic Traits and Domain-Unique Challenges
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 内在特征与领域独特挑战
- en: '![Refer to caption](img/2ba846afef1bb6ea449679db4ec1c29f.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2ba846afef1bb6ea449679db4ec1c29f.png)'
- en: 'Figure 3: Sketch-specific representations. Representations from left to right:
    sparse matrix (black background with white lines), dense picture (white background
    with black lines), graph, stroke sequence. Both graph and stroke sequence representations
    are based on the key stroke points. In stroke sequence, each key point is denoted
    as a four-tuple, where the first two entries and the last two entries represent
    the coordinates and pen state, respectively. See details in text.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：草图特定表示。左到右的表示：稀疏矩阵（黑色背景白色线条）、密集图像（白色背景黑色线条）、图形、笔画序列。图形和笔画序列表示均基于关键笔画点。在笔画序列中，每个关键点表示为一个四元组，其中前两个条目和最后两个条目分别表示坐标和笔状态。详细信息见文本。
- en: '![Refer to caption](img/697dedf1c0d7026a7723ece56cb37426.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/697dedf1c0d7026a7723ece56cb37426.png)'
- en: 'Figure 4: Illustrations of the major domain-unique challenges of free-hand
    sketch. Each column is a photo-sketch pair. Sketch is highly abstract. A pyramid
    can be depicted as a triangle in sketch, and a few strokes depict a fancy handbag.
    Sketch is highly diverse. Different people draw distinctive sketches when given
    the identical reference, due to subjective salience (head vs. body), and drawing
    style.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：自由手绘草图主要领域独特挑战的插图。每列是一对照片-草图。草图具有高度抽象性。一个金字塔在草图中可以表现为一个三角形，而几笔勾勒出一个时尚手袋。草图具有高度多样性。不同的人在给定相同参考时会绘制出不同的草图，这取决于主观显著性（头部
    vs. 身体）和绘画风格。
- en: 'Representation Free-hand sketch is a special kind of visual data, intrinsically
    different to natural photos that are the pixel-perfect copies of the real world.
    For efficient storage and fast calculation, free-hand sketch can be saved as a
    sparse matrix, or as a black and white image that ignores its sparsity (Figure [3](#S2.F3
    "Figure 3 ‣ 2.1 Intrinsic Traits and Domain-Unique Challenges ‣ 2 Background ‣
    Deep Learning for Free-Hand Sketch: A Survey"), left images). Since sketch generation
    is a dynamic process, suitably captured sketches can also be represented as a
    sequence of strokes or pen coordinates (Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Intrinsic
    Traits and Domain-Unique Challenges ‣ 2 Background ‣ Deep Learning for Free-Hand
    Sketch: A Survey"), right). In this regard, sketches share similarities with hand-written
    characters, yet are fundamentally different given their highly abstract and free-style
    nature (c.f., alphabetic hand-writing is subject to specific rules and a teaching
    process). From another perspective, free-hand sketches can also be modeled as
    a sparsely connected graph where lines are edges in the graph. Compared with a
    sequence of Euclidean coordinates, topological representation as a graph can provide
    a more flexible and abstract representation. As a result of this diversity of
    possible representations, various deep learning paradigms can be used to process
    sketches including CNNs, RNNs, GCNs, and TCNs.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表示 自由手绘草图是一种特殊的视觉数据，本质上不同于自然照片，后者是现实世界的像素完美复制品。为了高效存储和快速计算，自由手绘草图可以保存为稀疏矩阵，或作为忽略其稀疏性的黑白图像（图[3](#S2.F3
    "图 3 ‣ 2.1 内在特征与领域独特挑战 ‣ 2 背景 ‣ 自由手绘草图的深度学习：综述")，左侧图像）。由于草图生成是一个动态过程，适当捕捉的草图也可以表示为笔画或笔坐标的序列（图[3](#S2.F3
    "图 3 ‣ 2.1 内在特征与领域独特挑战 ‣ 2 背景 ‣ 自由手绘草图的深度学习：综述")，右侧）。在这方面，草图与手写字符有相似之处，但由于其高度抽象和自由风格的特性，基本上是不同的（例如，字母书写遵循特定规则和教学过程）。从另一个角度看，自由手绘草图也可以建模为稀疏连接的图，其中线条是图中的边。与欧几里得坐标序列相比，作为图的拓扑表示可以提供更灵活和抽象的表示。由于这种可能表示的多样性，可以使用各种深度学习范式来处理草图，包括
    CNN、RNN、GCN 和 TCN。
- en: 'Unique Challenges and Opportunities The unique challenges of free-hand sketch
    can be summarized as follows: (i) Abstraction: Humans use sketch to depict an
    object or event in very few strokes, reflecting the high-level semantics of a
    mental image. As shown in Figure [4](#S2.F4 "Figure 4 ‣ 2.1 Intrinsic Traits and
    Domain-Unique Challenges ‣ 2 Background ‣ Deep Learning for Free-Hand Sketch:
    A Survey"), a pyramid can be depicted as a triangle in sketch, and few strokes
    can depict a fancy handbag. (ii) Diversity: Different people have different drawing
    styles. For example, a near ‘realistic’ (close to photo edge-map) sketch image
    could be portrayed in different ways as exaggerated (c.f. caricatures), iconic
    (where details are omitted and the sketch is near symbolic), or artistic. Depending
    on subjective opinion about salience, different parts may also be included or
    omitted in a sketch. For instance, given a concept “cat”, people differ on choice
    of drawing with/without body (Figure [4](#S2.F4 "Figure 4 ‣ 2.1 Intrinsic Traits
    and Domain-Unique Challenges ‣ 2 Background ‣ Deep Learning for Free-Hand Sketch:
    A Survey")). Finally, there is the mental viewpoint of different users, e.g.,
    whether they imagine an orthographic or perspective projection image. In Figure [4](#S2.F4
    "Figure 4 ‣ 2.1 Intrinsic Traits and Domain-Unique Challenges ‣ 2 Background ‣
    Deep Learning for Free-Hand Sketch: A Survey"), we can see that different people
    draw differing perspective views of an identical slipper. (iii) Sparsity: No matter
    the representation, free-hand sketch is a highly sparse signal compared to photographs.
    (iv) Invariance: People can still recognize sketches after they are shifted, rescaled,
    rotated, or flipped. In Section [4.3.2](#S4.SS3.SSS2 "4.3.2 Robustness Study on
    Spatial Transformation ‣ 4.3 Experimental Comparison ‣ 4 Tasks and Methodology
    Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey"), we conduct a robustness
    study to evaluate whether deep networks are sensitive to spatial transformations
    in sketch related tasks. (v) Finally, there are some unique challenges when collecting
    sketch, which will be discussed in detail as follows (see Section [3.2](#S3.SS2
    "3.2 Unique Challenges of Sketch Collection ‣ 3 Free-Hand Sketch Datasets ‣ Deep
    Learning for Free-Hand Sketch: A Survey")).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '独特的挑战与机遇 自由手绘草图的独特挑战可以总结如下：(i) 抽象性：人们使用草图以极少的笔触描绘物体或事件，反映出心理图像的高级语义。如图[4](#S2.F4
    "Figure 4 ‣ 2.1 Intrinsic Traits and Domain-Unique Challenges ‣ 2 Background ‣
    Deep Learning for Free-Hand Sketch: A Survey")所示，草图中的金字塔可以用三角形来表示，几笔就可以描绘出一个花哨的手袋。(ii)
    多样性：不同的人有不同的绘画风格。例如，接近‘真实’（接近照片边缘图）的草图图像可以以夸张（如漫画）、标志性（省略细节，草图接近符号）或艺术化的方式表现。根据对显著性的主观意见，草图中可能会包含或省略不同的部分。例如，给定一个“猫”的概念，人们在选择是否绘制身体时会有所不同（图[4](#S2.F4
    "Figure 4 ‣ 2.1 Intrinsic Traits and Domain-Unique Challenges ‣ 2 Background ‣
    Deep Learning for Free-Hand Sketch: A Survey")）。最后，还有不同用户的心理视角，例如，他们是否想象一个正投影或透视投影图像。在图[4](#S2.F4
    "Figure 4 ‣ 2.1 Intrinsic Traits and Domain-Unique Challenges ‣ 2 Background ‣
    Deep Learning for Free-Hand Sketch: A Survey")中，我们可以看到不同的人绘制了相同拖鞋的不同透视视图。(iii)
    稀疏性：无论何种表示方式，与照片相比，自由手绘草图是高度稀疏的信号。(iv) 不变性：即使草图被移动、缩放、旋转或翻转，人们仍然能够识别出来。在第[4.3.2](#S4.SS3.SSS2
    "4.3.2 Robustness Study on Spatial Transformation ‣ 4.3 Experimental Comparison
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")节中，我们进行了一项稳健性研究，以评估深度网络是否对草图相关任务中的空间变换敏感。(v)
    最后，收集草图时还存在一些独特的挑战，这些将在后续部分详细讨论（见第[3.2](#S3.SS2 "3.2 Unique Challenges of Sketch
    Collection ‣ 3 Free-Hand Sketch Datasets ‣ Deep Learning for Free-Hand Sketch:
    A Survey")节）。'
- en: '![Refer to caption](img/e72653dec0a54992c499afbee6a7160a.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e72653dec0a54992c499afbee6a7160a.png)'
- en: (a) A sketch is drawn with the device on the back of the hand [[52](#bib.bib52)].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 使用设备在手背上绘制的草图[[52](#bib.bib52)]。
- en: '![Refer to caption](img/f2636babc8016b0a1db0f7c978ba0c42.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f2636babc8016b0a1db0f7c978ba0c42.png)'
- en: '(b) A walking cycle sequence. Left: input hand-drawn sketch, middle: inflated
    3D model with control points, right: walking cycle animation created by recording
    trajectories of individual control points specified by the user [[53](#bib.bib53)].'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 一个行走周期序列。左侧：输入的手绘草图，中间：带有控制点的膨胀 3D 模型，右侧：通过记录用户指定的各个控制点的轨迹创建的行走周期动画[[53](#bib.bib53)]。
- en: 'Figure 5: Novel applications that sketch supports.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：草图支持的创新应用。
- en: 'Sketch also provides some unique opportunities compared to photos: (i) As a
    counterpoint to the sparsity challenge, sketch often lacks distracting background
    clutter compared to photos, which can benefit automated analysis [[3](#bib.bib3)].
    (ii) If captured appropriately, the sequential nature of sketch generation can
    further be exploited to benefit analysis compared to static images [[54](#bib.bib54)].
    (iii) The sparse and sequential nature of sketch also provides opportunities for
    high quality sketch generation, where image generation is hampered by the need
    to fill in pixel detail [[5](#bib.bib5), [55](#bib.bib55), [56](#bib.bib56)].
    (iv) Sketches can serve as a computer-interaction modality in a way that photos
    cannot [[4](#bib.bib4), [54](#bib.bib54)], due to the intuitive way humans can
    generate them without training. For example, people without professional painting
    training can do casual sketch-based design via sketch-to-photo generation techniques,
    e.g., scene photo generation [[17](#bib.bib17)]. Figure [5(a)](#S2.F5.sf1 "In
    Figure 5 ‣ 2.1 Intrinsic Traits and Domain-Unique Challenges ‣ 2 Background ‣
    Deep Learning for Free-Hand Sketch: A Survey") presents another example: People
    could sketch on the back of the hand to make notes conveniently, and the sketch
    could be shown and recorded in the watch [[52](#bib.bib52)]. (v) Sketches natively
    can express motion trajectories, thus can be applied to dynamic modeling. As shown
    in Figure [5(b)](#S2.F5.sf2 "In Figure 5 ‣ 2.1 Intrinsic Traits and Domain-Unique
    Challenges ‣ 2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey"), walking
    cycle animation can be created by recording trajectories of individual control
    points specified by the user [[53](#bib.bib53)].'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 草图与照片相比提供了一些独特的机会：（i）作为应对稀疏性挑战的对策，草图通常缺少照片中令人分心的背景杂物，这对自动化分析有益[[3](#bib.bib3)]。
    （ii）如果捕捉得当，草图生成的顺序特性可以进一步利用，从而比静态图像更有利于分析[[54](#bib.bib54)]。 （iii）草图的稀疏和顺序特性也为高质量草图生成提供了机会，而图像生成则受到需要填补像素细节的限制[[5](#bib.bib5),
    [55](#bib.bib55), [56](#bib.bib56)]。 （iv）草图可以作为计算机交互的模式，而照片则不能[[4](#bib.bib4),
    [54](#bib.bib54)]，因为人们可以直观地生成草图而无需培训。例如，未受过专业绘画培训的人可以通过草图到照片生成技术进行随意的草图设计，例如场景照片生成[[17](#bib.bib17)]。图[5(a)](#S2.F5.sf1
    "在图5 ‣ 2.1 内在特征和领域独特挑战 ‣ 2 背景 ‣ 深度学习在自由手绘草图中的应用：综述")展示了另一个例子：人们可以在手背上做笔记，草图可以在手表上显示和记录[[52](#bib.bib52)]。
    （v）草图本身可以表达运动轨迹，因此可以应用于动态建模。如图[5(b)](#S2.F5.sf2 "在图5 ‣ 2.1 内在特征和领域独特挑战 ‣ 2 背景
    ‣ 深度学习在自由手绘草图中的应用：综述")所示，通过记录用户指定的单个控制点的轨迹，可以创建步态循环动画[[53](#bib.bib53)]。
- en: Given these unique challenges and opportunities, it is often beneficial to design
    sketch-specific models to obtain best performance in various sketch-related applications.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些独特的挑战和机遇，设计特定于草图的模型以在各种草图相关应用中获得最佳性能通常是有益的。
- en: '![Refer to caption](img/57823e47d49d522928207494332e204f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/57823e47d49d522928207494332e204f.png)'
- en: 'Figure 6: Milestones of deep learning based free-hand sketch research, from
    the perspectives of task, dataset, supervision, and representation. Note that
    self-supervised learning is a branch of unsupervised learning.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：从任务、数据集、监督和表示的角度看，基于深度学习的自由手绘草图研究的里程碑。注意，自监督学习是无监督学习的一个分支。
- en: 2.2 A Brief History of Sketch in the Deep Learning Era
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 深度学习时代草图的简要历史
- en: 'In the past five years, the free-hand sketch community has developed rapidly
    as summarized by Figure [6](#S2.F6 "Figure 6 ‣ 2.1 Intrinsic Traits and Domain-Unique
    Challenges ‣ 2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey") from
    the perspectives of: tasks, datasets, representations and supervision. (i) In
    2015, Sketch-a-Net [[18](#bib.bib18)] was proposed as a CNN engineered specifically
    for free-hand sketch. It gained note as the first to achieve a recognition rate
    surpassing humans and helped to popularize deep learning for sketch analysis.
    (ii) In 2016, three fine-grained⁶⁶6The phrase “fine-grained” in this paper has
    different meanings according to the context. For sketch tasks, fine-grained sketch-based
    image retrieval means instance-level sketch-photo matching, while other fine-grained
    tasks (e.g., generation, segmentation) emphasize that machine needs to perceive
    sketches on stroke or part or group levels. For sketch datasets, “fine-grained”
    datasets mean that their sketches provide visual details and/or detailed manual
    annotations (e.g., stroke/part/group level annotations, instance-level pairing
    information). sketch-based image retrieval (FG-SBIR) datasets were released, i.e.,
    QMUL Shoe [[4](#bib.bib4)] and Chair [[4](#bib.bib4)], and Sketchy [[7](#bib.bib7)].
    Combined with deep triplet ranking [[57](#bib.bib57)], these fine-grained cross-modal
    datasets motivated a wave of follow-up FG-SBIR and other fine-grained tasks. (iii)
    In 2017, Google released a million-scale sketch dataset, i.e., Google QuickDraw,
    via the online game “QuickDraw”. QuickDraw contains over 50M sketches collected
    from players around the world, making it a rich and diverse dataset. Furthermore,
    based on the QuickDraw dataset, Ha et al. proposed “SketchRNN”, a RNN-based deep
    Variational Auto Encoder (VAE) that can generate diverse sketches [[5](#bib.bib5)].
    This work motivated the community to go beyond considering sketches as static
    pictures to be processed by CNN; and inspired subsequent work to use stroke sequences
    as input and study temporal processing of sketches. In 2017, some sketch-based
    deep generative image models [[58](#bib.bib58)] began to appear in the top conferences
    in computer vision. (iv) From 2018 to date, based on deep learning techniques,
    various novel methodologies – e.g., sketch hashing [[19](#bib.bib19)], sketch
    transformers [[23](#bib.bib23)]; and applications – e.g., sketch abstraction [[21](#bib.bib21)],
    sketch-based photo classifier generation [[20](#bib.bib20)], sketch perceptual
    grouping [[59](#bib.bib59)], and sketch vectorization [[56](#bib.bib56)] have
    been proposed. See Figure [6](#S2.F6 "Figure 6 ‣ 2.1 Intrinsic Traits and Domain-Unique
    Challenges ‣ 2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey") for
    a chronological summary.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '在过去五年中，手绘草图社区迅速发展，如图[6](#S2.F6 "Figure 6 ‣ 2.1 Intrinsic Traits and Domain-Unique
    Challenges ‣ 2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey")所总结，从任务、数据集、表示和监督的角度来看。(i)
    2015年，Sketch-a-Net [[18](#bib.bib18)] 被提出，它是一种专门为手绘草图设计的CNN。它因首个实现超过人类识别率而受到关注，并推动了深度学习在草图分析中的普及。(ii)
    2016年，发布了三个细粒度⁶⁶6本文中的“细粒度”一词根据上下文有不同含义。对于草图任务，细粒度草图图像检索指的是实例级别的草图-照片匹配，而其他细粒度任务（如生成、分割）则强调机器需要在笔触、部分或组级别上感知草图。对于草图数据集，“细粒度”数据集意味着其草图提供视觉细节和/或详细的手动注释（例如，笔触/部分/组级别注释、实例级配对信息）草图图像检索（FG-SBIR）数据集，即QMUL
    Shoe [[4](#bib.bib4)] 和 Chair [[4](#bib.bib4)]，以及 Sketchy [[7](#bib.bib7)]。结合深度三元组排序
    [[57](#bib.bib57)]，这些细粒度的跨模态数据集激发了一波后续的FG-SBIR及其他细粒度任务。(iii) 2017年，谷歌通过在线游戏“QuickDraw”发布了百万级草图数据集，即Google
    QuickDraw。QuickDraw包含来自全球玩家的超过5000万张草图，使其成为一个丰富且多样的数据集。此外，基于QuickDraw数据集，Ha等人提出了“SketchRNN”，一种基于RNN的深度变分自编码器（VAE），能够生成多样的草图
    [[5](#bib.bib5)]。这项工作促使社区超越将草图视为静态图片的观念，激发了随后使用笔触序列作为输入并研究草图的时间处理的工作。在2017年，一些基于草图的深度生成图像模型
    [[58](#bib.bib58)] 开始在计算机视觉顶级会议上出现。(iv) 从2018年至今，基于深度学习技术，提出了各种新方法——例如，草图哈希 [[19](#bib.bib19)]、草图变换器
    [[23](#bib.bib23)]；以及应用——例如，草图抽象 [[21](#bib.bib21)]、基于草图的照片分类器生成 [[20](#bib.bib20)]、草图感知分组
    [[59](#bib.bib59)] 和草图矢量化 [[56](#bib.bib56)]。有关时间顺序的总结，请参见图[6](#S2.F6 "Figure
    6 ‣ 2.1 Intrinsic Traits and Domain-Unique Challenges ‣ 2 Background ‣ Deep Learning
    for Free-Hand Sketch: A Survey")。'
- en: 'TABLE II: Summary of the representative sketch datasets. Both ‘grouping” and
    “segmentation” annotations refer to stroke-level. “K” and “M” mean “thousand”
    and “million”, respectively. “Cat.” means “category”. Stroke “✓” denotes sketches
    provided as SVG files or coordinate arrays.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE II: 代表性草图数据集的总结。‘分组’和‘分割’注释指的是笔画级别。‘K’和‘M’分别表示‘千’和‘百万’。‘类’表示‘类别’。笔画“✓”表示草图以
    SVG 文件或坐标数组的形式提供。'
- en: '|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Single-Modal &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单模态 &#124;'
- en: '&#124; Datasets &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集 &#124;'
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fine- &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精细- &#124;'
- en: '&#124; Grained &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 粗粒度 &#124;'
- en: '| Public | Modalities & Sample Amount | Cat. | Stroke |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 公共 | 模态 & 样本数量 | 类别 | 笔画 |'
- en: '&#124; Object/ &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对象/ &#124;'
- en: '&#124; Scene &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 场景 &#124;'
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Instance &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 实例 &#124;'
- en: '&#124; Pairing &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 配对 &#124;'
- en: '| Annotations | Remarks |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 注释 | 备注 |'
- en: '| TU-Berlin [[6](#bib.bib6)] |  | ✓ | 20K sketches | 250 | ✓ | o | - | class
    |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| TU-Berlin [[6](#bib.bib6)] |  | ✓ | 20K 张草图 | 250 | ✓ | o | - | 类别 |  |'
- en: '| QuickDraw [[5](#bib.bib5)] |  | ✓ | 50M+ sketches | 345 | ✓ | o | - | class
    |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| QuickDraw [[5](#bib.bib5)] |  | ✓ | 50M+ 张草图 | 345 | ✓ | o | - | 类别 |  |'
- en: '| QuickDraw-5-step [[60](#bib.bib60)] |  |  | 38M+ sketches | 345 |  | o |
    - | class |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| QuickDraw-5-step [[60](#bib.bib60)] |  |  | 38M+ 张草图 | 345 |  | o | - | 类别
    |  |'
- en: '| SPG [[59](#bib.bib59)] | ✓ | ✓ | 20K sketches | 25 | ✓ | o | - | class, grouping
    |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| SPG [[59](#bib.bib59)] | ✓ | ✓ | 20K 张草图 | 25 | ✓ | o | - | 类别，分组 |  |'
- en: '| SketchSeg-150K [[25](#bib.bib25)] | ✓ |  | 150K sketches | 20 | ✓ | o | -
    | class, segmentation | 57 semantic labels |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| SketchSeg-150K [[25](#bib.bib25)] | ✓ |  | 150K 张草图 | 20 | ✓ | o | - | 类别，分割
    | 57 个语义标签 |'
- en: '| SketchSeg-10K [[61](#bib.bib61)] | ✓ | ✓ | 10K sketches | 10 |  | o | - |
    class, segmentation | 24 semantic labels |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| SketchSeg-10K [[61](#bib.bib61)] | ✓ | ✓ | 10K 张草图 | 10 |  | o | - | 类别，分割
    | 24 个语义标签 |'
- en: '| SketchFix-160 [[62](#bib.bib62)] |  | ✓ | 3904 sketches | 160 | ✓ | o | -
    | class, eye fixation |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| SketchFix-160 [[62](#bib.bib62)] |  | ✓ | 3904 张草图 | 160 | ✓ | o | - | 类别，眼动注视
    |  |'
- en: '| Sheep 10K [[63](#bib.bib63)] | ✓ | ✓ | 10K sheep sketches | 1 | ✓ | o | -
    | class |  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Sheep 10K [[63](#bib.bib63)] | ✓ | ✓ | 10K 张羊的草图 | 1 | ✓ | o | - | 类别 |  |'
- en: '| COAD [[64](#bib.bib64)] | ✓ | ✓ | 620 sketches | 20 | ✓ | o | - | class |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| COAD [[64](#bib.bib64)] | ✓ | ✓ | 620 张草图 | 20 | ✓ | o | - | 类别 |  |'
- en: '|'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-Modal &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多模态 &#124;'
- en: '&#124; Datasets &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集 &#124;'
- en: '|'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fine- &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精细- &#124;'
- en: '&#124; Grained &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 粗粒度 &#124;'
- en: '| Public | Modalities & Sample Amount | Cat. | Stroke |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 公共 | 模态 & 样本数量 | 类别 | 笔画 |'
- en: '&#124; Object/ &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对象/ &#124;'
- en: '&#124; Scene &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 场景 &#124;'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Instance &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 实例 &#124;'
- en: '&#124; Pairing &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 配对 &#124;'
- en: '| Annotations | Remarks |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 注释 | 备注 |'
- en: '| QMUL Shoe [[4](#bib.bib4)] | ✓ | ✓ | 419 sketches, 419 photos | 1 |  | o
    | ✓ | pairing,triplet,attribute | 21 binary attributes |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| QMUL Shoe [[4](#bib.bib4)] | ✓ | ✓ | 419 张草图，419 张照片 | 1 |  | o | ✓ | 配对，三元组，属性
    | 21 个二进制属性 |'
- en: '| QMUL Chair [[4](#bib.bib4)] | ✓ | ✓ | 297 sketches, 297 photos | 1 |  | o
    | ✓ | pairing,triplet,attribute | 15 binary attributes |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| QMUL Chair [[4](#bib.bib4)] | ✓ | ✓ | 297 张草图，297 张照片 | 1 |  | o | ✓ | 配对，三元组，属性
    | 15 个二进制属性 |'
- en: '| QMUL Handbag [[26](#bib.bib26)] | ✓ | ✓ | 568 sketches, 568 photos | 1 |  |
    o | ✓ | pairing |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| QMUL Handbag [[26](#bib.bib26)] | ✓ | ✓ | 568 张草图，568 张照片 | 1 |  | o | ✓
    | 配对 |  |'
- en: '| Sketchy [[7](#bib.bib7)] | ✓ | ✓ | 75K sketches, 12K photos | 125 | ✓ | o
    |  | class | 12K objects |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Sketchy [[7](#bib.bib7)] | ✓ | ✓ | 75K 张草图，12K 张照片 | 125 | ✓ | o |  | 类别
    | 12K 个对象 |'
- en: '| Sketch&UI [[8](#bib.bib8)] | ✓ |  | 1998 sketches, 1998 photos | 23 |  |
    o | ✓ | class, pairing | UI |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Sketch&UI [[8](#bib.bib8)] | ✓ |  | 1998 张草图，1998 张照片 | 23 |  | o | ✓ | 类别，配对
    | 用户界面 |'
- en: '| QuickDrawExtended [[65](#bib.bib65)] |  | ✓ | 330K sketches, 204K photos
    | 110 |  | o |  | class |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| QuickDrawExtended [[65](#bib.bib65)] |  | ✓ | 330K 张草图，204K 张照片 | 110 |  |
    o |  | 类别 |  |'
- en: '| SketchTransfer [[66](#bib.bib66)] |  |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| SketchTransfer [[66](#bib.bib66)] |  |  |'
- en: '&#124; 112.5K sketches, &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 112.5K 张草图， &#124;'
- en: '&#124; 90K CIFAR-10 photos &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 90K CIFAR-10 照片 &#124;'
- en: '| 9 |  | o |  | class | resolution of 32x32 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 9 |  | o |  | 类别 | 32x32 的分辨率 |'
- en: '| TU-Berlin Extended [[67](#bib.bib67)] |  |  | 20K sketches, 191K photos |
    250 |  | o |  | class |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| TU-Berlin Extended [[67](#bib.bib67)] |  |  | 20K 张草图，191K 张照片 | 250 |  |
    o |  | 类别 |  |'
- en: '| Sketch Flickr15K [[1](#bib.bib1)] |  | ✓ | 330 sketches, 15K photos | 33
    |  | o |  | class |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Sketch Flickr15K [[1](#bib.bib1)] |  | ✓ | 330 张草图，15K 张照片 | 33 |  | o |  |
    类别 |  |'
- en: '| Aerial-SI [[68](#bib.bib68), [69](#bib.bib69)] |  |  | 400 sketches, 3.3K
    photos | 10 |  | o, s |  | class | aerial scene |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Aerial-SI [[68](#bib.bib68), [69](#bib.bib69)] |  |  | 400 张草图，3.3K 张照片 |
    10 |  | o, s |  | 类别 | 空中场景 |'
- en: '| HUST-SI [[70](#bib.bib70)] |  | ✓ | 20K sketches, 31K photos | 250 | ✓ |
    o |  | class |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| HUST-SI [[70](#bib.bib70)] |  | ✓ | 20K 张草图，31K 张照片 | 250 | ✓ | o |  | 类别
    |  |'
- en: '| SBSR [[71](#bib.bib71)] |  | ✓ | 1814 sketches, 1814 3D models | 161 |  |
    o |  | class |  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| SBSR [[71](#bib.bib71)] |  | ✓ | 1814 幅素描，1814 个 3D 模型 | 161 |  | o |  |
    类别 |  |'
- en: '| SHREC’13 [[47](#bib.bib47)] | ✓ | ✓ | 7200 sketches, 1258 3D models | 90
    |  | o |  | class |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| SHREC’13 [[47](#bib.bib47)] | ✓ | ✓ | 7200 幅素描，1258 个 3D 模型 | 90 |  | o |  |
    类别 |  |'
- en: '| SHREC’14 [[72](#bib.bib72)] | ✓ | ✓ | 12680 sketches, 8987 3D models | 171
    |  | o |  | class |  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| SHREC’14 [[72](#bib.bib72)] | ✓ | ✓ | 12680 幅素描，8987 个 3D 模型 | 171 |  | o
    |  | 类别 |  |'
- en: '| PACS DG [[73](#bib.bib73)] |  | ✓ |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| PACS DG [[73](#bib.bib73)] |  | ✓ |'
- en: '&#124; 9991 (sketches, photos, &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 9991（素描，照片，&#124;'
- en: '&#124; cartoons, paintings) &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 卡通，绘画）&#124;'
- en: '| 7 |  | o |  | class |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 7 |  | o |  | 类别 |'
- en: '&#124; domain &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 域 &#124;'
- en: '&#124; generalization &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 泛化 &#124;'
- en: '|'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Flickr1M [[74](#bib.bib74)] |  |  | 500 sketches, 1.3M photos | 100 |  |
    o |  | class |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Flickr1M [[74](#bib.bib74)] |  |  | 500 幅素描，130 万张照片 | 100 |  | o |  | 类别
    |  |'
- en: '| Cross-Modal Places [[75](#bib.bib75)] |  | ✓ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Cross-Modal Places [[75](#bib.bib75)] |  | ✓ |'
- en: '&#124; 16K sketches, 11K descriptions, &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1.6 万幅素描，1.1 万个描述，&#124;'
- en: '&#124; 458K spatial texts, 12K clip arts, &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 45.8 万个空间文本，1.2 万个剪贴画，&#124;'
- en: '&#124; 1.5M photos &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 150 万张照片 &#124;'
- en: '| 205 |  | s |  | class |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 205 |  | s |  | 类别 |  |'
- en: '| SketchyScene [[76](#bib.bib76)] | ✓ | ✓ | 29K sketches, 7K photos |  |  |
    s | ✓ | pairing, segmentation |  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| SketchyScene [[76](#bib.bib76)] | ✓ | ✓ | 29K 幅素描，7K 张照片 |  |  | s | ✓ |
    配对，分割 |  |'
- en: '| DomainNet [[77](#bib.bib77)] |  | ✓ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| DomainNet [[77](#bib.bib77)] |  | ✓ |'
- en: '&#124; 0.6M (cliparts, infographs, &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 60 万（剪贴画，信息图表，&#124;'
- en: '&#124; paintings, QuickDraw skteches, &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 绘画，QuickDraw 素描，&#124;'
- en: '&#124; real photos, &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 真实照片，&#124;'
- en: '&#124; professional pencil sketches) &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 专业铅笔素描）&#124;'
- en: '| 345 | ✓ | o |  | class |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 345 | ✓ | o |  | 类别 |  |'
- en: '| SketchyCOCO [[17](#bib.bib17)] | ✓ | ✓ |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| SketchyCOCO [[17](#bib.bib17)] | ✓ | ✓ |'
- en: '&#124; 14K+ (sketches, photos, &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 14K+（素描，照片，&#124;'
- en: '&#124; edge-maps) &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 边缘图）&#124;'
- en: '| 17 | ✓ | o, s | ✓ |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 17 | ✓ | o, s | ✓ |'
- en: '&#124; class, pairing, &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类别，配对，&#124;'
- en: '&#124; five-tuple, &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 五元组，&#124;'
- en: '&#124; segmentation &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3 background classes &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3 背景类别 &#124;'
- en: '&#124; 14 foreground classes &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 14 个前景类别 &#124;'
- en: '|'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SceneSketcher [[78](#bib.bib78)] | ✓ | ✓ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| SceneSketcher [[78](#bib.bib78)] | ✓ | ✓ |'
- en: '&#124; 1225 scene &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1225 个场景 &#124;'
- en: '&#124; sketch-photo pairs &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 素描-照片对 &#124;'
- en: '| 14 | ✓ | o, s | ✓ |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 14 | ✓ | o, s | ✓ |'
- en: '&#124; class,pairing &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类别，配对 &#124;'
- en: '&#124; segmentation &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '|  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 3 Free-Hand Sketch Datasets
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 自由手素描数据集
- en: In the last decade numerous new free-hand sketch datasets have been collected,
    to satisfy the need for large-scale deep network training, and the growing diversity
    of sketch-related tasks considered by the community. This section will summarize
    these datasets, and further discuss some of the unique challenges in sketch-related
    data collection.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年中，收集了大量新的自由手素描数据集，以满足大规模深度网络训练的需求以及社区对素描相关任务的日益多样化的需求。本节将总结这些数据集，并进一步讨论一些素描相关数据收集中的独特挑战。
- en: 3.1 Sketch Datasets
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 素描数据集
- en: 'Free-hand sketch datasets can be grouped in terms of: (i) single vs. multi-modal,
    and (ii) coarse vs. fine-grained. Single-modal datasets consist only of sketches
    and are typically used for recognition, sketch-sketch retrieval, grouping, segmentation,
    and generation. Multi-modal datasets support cross-modal tasks by pairing sketches
    with samples from other modalities such as natural photo, 3D shape, text, or video.
    These are mainly used for cross-modal retrieval/matching, or cross-modal generation/synthesis.
    Coarse-grained datasets (e.g., TU-Berlin [[6](#bib.bib6)], QuickDraw [[5](#bib.bib5)])
    are usually used for sketch recognition, sketch retrieval; while fine-grained
    datasets (e.g., QMUL Shoe [[4](#bib.bib4)]) provide fine-grained visual details
    and manual annotations.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 自由手素描数据集可以根据以下方面进行分类：(i) 单模态与多模态，以及 (ii) 粗粒度与细粒度。单模态数据集仅包含素描，通常用于识别、素描-素描检索、分组、分割和生成。多模态数据集通过将素描与其他模态的样本（如自然照片、3D
    形状、文本或视频）配对，支持跨模态任务。这些主要用于跨模态检索/匹配或跨模态生成/合成。粗粒度数据集（例如，TU-Berlin [[6](#bib.bib6)]，QuickDraw [[5](#bib.bib5)]）通常用于素描识别、素描检索；而细粒度数据集（例如，QMUL
    Shoe [[4](#bib.bib4)]）提供细致的视觉细节和手动注释。
- en: More specifically, coarse-grained single-modal datasets [[71](#bib.bib71), [5](#bib.bib5)]
    support sketch recognition and retrieval; while coarse-grained multi-modal datasets
    (e.g., QuickDraw-Extended [[65](#bib.bib65)]) support category-level sketch-based
    image retrieval. Fine-grained single-modal datasets [[59](#bib.bib59), [25](#bib.bib25)]
    support perceptual grouping, segmentation, and parsing. Fined-grained multi-modal
    datasets (e.g., QMUL Shoe [[4](#bib.bib4)]) provide the instance-level pairing
    information to support retrieval.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，粗粒度单模态数据集 [[71](#bib.bib71), [5](#bib.bib5)] 支持草图识别和检索；而粗粒度多模态数据集（例如 QuickDraw-Extended
    [[65](#bib.bib65)]）支持类别级别的草图图像检索。细粒度单模态数据集 [[59](#bib.bib59), [25](#bib.bib25)]
    支持感知分组、分割和解析。细粒度多模态数据集（例如 QMUL Shoe [[4](#bib.bib4)]）提供实例级配对信息以支持检索。
- en: 'Table [II](#S2.T2 "TABLE II ‣ 2.2 A Brief History of Sketch in the Deep Learning
    Era ‣ 2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey") summarizes
    representative sketch datasets of each type in terms of: modalities, size, number
    of categories, stroke information, annotation, etc. Note that SVG files are able
    to generate static picture files such as JPEG and PNG, whilst these static files
    cannot store or provide the original drawing process (stroke ordering). We exclude
    some well-known but overly-small datasets such as [[79](#bib.bib79)].'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '表格[II](#S2.T2 "TABLE II ‣ 2.2 A Brief History of Sketch in the Deep Learning
    Era ‣ 2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey") 总结了每种类型的代表性草图数据集，包括：模态、大小、类别数量、笔画信息、注释等。注意，SVG
    文件能够生成静态图片文件，如 JPEG 和 PNG，但这些静态文件无法存储或提供原始绘图过程（笔画顺序）。我们排除了某些知名但过小的数据集，如 [[79](#bib.bib79)]。'
- en: 3.2 Unique Challenges of Sketch Collection
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 草图收集的独特挑战
- en: 'Sketch Collection Strategies Existing collection approaches mainly include:
    (i) bespoke creation by researchers [[59](#bib.bib59), [61](#bib.bib61), [25](#bib.bib25)],
    (ii) crowd-sourcing selecting and matching on existing datasets, e.g., Doodle2Sketch
    QuickDraw-Extended [[65](#bib.bib65)], (iii) crowd-sourcing drawing from scratch,
    e.g., QMUL Shoe [[4](#bib.bib4)], Sketchy [[7](#bib.bib7)]. (iv) collecting via
    online drawing games, e.g., Google QuickDraw [[5](#bib.bib5)]. (v) Web crawling
    of existing sketches [[77](#bib.bib77), [73](#bib.bib73)]. In particular, for
    fine-grained multi-modal sketch datasets, crowd-sourcing is widespread, since
    fine-grained drawing, selection, and matching are time-consuming. Note that a
    sketch dataset’s potential applications are determined by both its collection
    and annotation protocol.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 草图收集策略 现有的收集方法主要包括：（i）研究人员定制创建 [[59](#bib.bib59), [61](#bib.bib61), [25](#bib.bib25)]，（ii）在现有数据集上进行众包选择和匹配，例如
    Doodle2Sketch QuickDraw-Extended [[65](#bib.bib65)]，（iii）众包从头开始绘制，例如 QMUL Shoe
    [[4](#bib.bib4)]、Sketchy [[7](#bib.bib7)]，（iv）通过在线绘图游戏进行收集，例如 Google QuickDraw
    [[5](#bib.bib5)]，（v）网络爬虫获取现有草图 [[77](#bib.bib77), [73](#bib.bib73)]。特别地，对于细粒度多模态草图数据集，众包是广泛使用的，因为细粒度绘图、选择和匹配是耗时的。注意，草图数据集的潜在应用取决于其收集和注释协议。
- en: 'Sketch Collection Challenges Free-hand sketch poses some unique data collection
    challenges compared to other image types: (i) Time-Sequence Nature Sketching is
    a dynamic and temporally extended process. Thus, collecting sketches as static
    raster images (e.g., JPEG, PNG) is very limiting, and recording vector representation
    (e.g., SVG⁷⁷7[https://en.wikipedia.org/wiki/Scalable_Vector_Graphics](https://en.wikipedia.org/wiki/Scalable_Vector_Graphics))
    together with stroke position and timing is preferred to support research. As
    a consequence, this means that collecting by web-crawling (which typically retrieves
    raster images), is less useful. (ii) Cross-Modal Pairing Collecting cross-modal
    datasets provides the additional challenge of pairing sketches and associated
    data in other modalities. One can start with existing images and sketches and
    pair them [[80](#bib.bib80)], or draw sketches specifically corresponding to given
    examples in other modalities [[7](#bib.bib7), [4](#bib.bib4)]. But both options
    are time-consuming. (iii) Demographic Information Since sketches are human-created,
    rather than pixel-perfect images captured by camera, it is important to ensure
    that sketch datasets are created by diverse demographics to ensure they are representative
    (an even stronger version of the challenge [[81](#bib.bib81)] posed by photo images).
    Furthermore, meta-data about the artists (e.g., gender, nationality, skill level)
    may be important to store, both in order to study how different humans sketch,
    and also to ensure that any sketch-based applications are fair and unbiased [[82](#bib.bib82)].
    However to our knowledge, this kind of meta-data has not been recorded in existing
    datasets.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 草图收集挑战 相比其他图像类型，自由手绘草图提出了一些独特的数据收集挑战：(i) 时间序列特性 草图是一个动态且时间扩展的过程。因此，将草图收集为静态栅格图像（例如，JPEG，PNG）非常有限，而记录矢量表示（例如，[SVG](https://en.wikipedia.org/wiki/Scalable_Vector_Graphics)）以及笔划位置和时间更能支持研究。因此，利用网页爬虫进行收集（通常获取栅格图像）效果较差。(ii)
    跨模态配对 收集跨模态数据集还面临将草图与其他模态的相关数据配对的额外挑战。可以从现有图像和草图开始进行配对[[80](#bib.bib80)]，或者绘制与其他模态中的给定示例相对应的草图[[7](#bib.bib7)，[4](#bib.bib4)]。但这两种选择都很耗时。(iii)
    人口统计信息 由于草图是人类创作的，而不是由相机捕捉的像素完美图像，因此确保草图数据集由不同人口统计群体创建，以确保其代表性（这是照片图像所面临挑战的一个更强版本[[81](#bib.bib81)]）非常重要。此外，关于艺术家的元数据（例如，性别、国籍、技能水平）可能需要保存，以研究不同人类如何进行草图创作，并确保任何基于草图的应用程序公平无偏见[[82](#bib.bib82)]。然而，据我们所知，现有数据集中尚未记录这类元数据。
- en: 'Discussion These challenges all mean that the standard computer vision approach
    of web-crawling is poorly suited to collection of sketch data: It does not typically
    retrieve vector-graphic and time-stamped sketch generation, does not make it easy
    to obtain matched cross-modal pairs, and does not come with any demographic information.
    For these reasons, bespoke creation, crowd-sourcing, and generation as a byproduct
    through gamification are the recommended methods of collection.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论 这些挑战表明，标准的计算机视觉方法——网页爬虫——不适合收集草图数据：它通常无法获取矢量图形和时间戳的草图生成，无法轻松获取匹配的跨模态对，也没有任何人口统计信息。因此，定制创建、众包以及通过游戏化作为副产品的生成是推荐的收集方法。
- en: '![Refer to caption](img/c6b56cefbf6b69805490b6b7c8042c15.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c6b56cefbf6b69805490b6b7c8042c15.png)'
- en: 'Figure 7: A tree diagram of the sketch task taxonomy. Generative tasks are
    framed by dashed lines. Sketch domain-unique tasks are framed by green lines.
    Best viewed in color.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：草图任务分类的树状图。生成性任务由虚线框定。草图领域独特任务由绿色线框定。最佳效果在彩色显示下查看。
- en: 4 Tasks and Methodology Taxonomy
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 任务和方法论分类
- en: 'In this section, we aim to provide an overview of deep learning related tasks
    and methods from the perspective of the whole free-hand sketch research area,
    rather than categorizing methods for a specific task. We observe that several
    trends emerging in contemporary sketch research including: (i) More novel tasks
    are continually being proposed, each of which has different task-specific challenges,
    and thus different task-specific methods/designs. This inspires us to review the
    existing methods from a task-driven perspective. (ii) Free-hand sketch is often
    associated with data from other modalities. This inspires us to categorize the
    existing tasks on the basis of the data modalities involved.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 本节旨在从整个自由手绘草图研究领域的角度提供深度学习相关任务和方法的概述，而不是对特定任务的方法进行分类。我们观察到现代草图研究中出现了几种趋势，包括：(i)
    不断提出更多新颖的任务，每个任务都有不同的特定挑战，因此也有不同的特定方法/设计。这激励我们从任务驱动的角度回顾现有方法。(ii) 自由手绘草图通常与来自其他模态的数据相关联。这激励我们根据涉及的数据模态对现有任务进行分类。
- en: 'According to the data modalities involved, free-hand sketch related tasks can
    be divided into single- and multi-modal tasks, with single-modality sketch analysis
    techniques often used as building blocks for multi-modal methods. This section
    will define the popular sketch analysis tasks and introduce the corresponding
    deep learning methods, providing a detailed taxonomy. Figure [7](#S3.F7 "Figure
    7 ‣ 3.2 Unique Challenges of Sketch Collection ‣ 3 Free-Hand Sketch Datasets ‣
    Deep Learning for Free-Hand Sketch: A Survey") provides a tree diagram of the
    existing free-hand sketch tasks. The main advantages of this task taxonomy include:
    (i) Straightforward and efficient. The tree has balanced depth and width, and
    does not contain redundant nodes. (ii) Extensible. The single-modality and multi-modal
    sub-trees are uncoupled for independent update. E.g., the single-modality sub-tree
    can remain fixed when we insert new modalities in future.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 根据涉及的数据模态，自由手绘草图相关任务可以分为单模态和多模态任务，其中单模态草图分析技术通常作为多模态方法的构建模块。本节将定义流行的草图分析任务，并介绍相应的深度学习方法，提供详细的分类。图[7](#S3.F7
    "图 7 ‣ 3.2 草图收集的独特挑战 ‣ 3 自由手绘草图数据集 ‣ 自由手绘草图的深度学习：综述")提供了现有自由手绘草图任务的树状图。该任务分类的主要优点包括：(i)
    直接且高效。树状图具有平衡的深度和宽度，不包含冗余节点。(ii) 可扩展。单模态和多模态子树是解耦的，可以独立更新。例如，当我们未来插入新模态时，单模态子树可以保持不变。
- en: 'Some uni-modal tasks listed in Figure [7](#S3.F7 "Figure 7 ‣ 3.2 Unique Challenges
    of Sketch Collection ‣ 3 Free-Hand Sketch Datasets ‣ Deep Learning for Free-Hand
    Sketch: A Survey") are also studied in the natural photo domain, while the sketch-based
    multi-modal tasks are unique.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7](#S3.F7 "图 7 ‣ 3.2 草图收集的独特挑战 ‣ 3 自由手绘草图数据集 ‣ 自由手绘草图的深度学习：综述")中列出的一些单模态任务也在自然照片领域进行了研究，而基于草图的多模态任务则是独特的。
- en: '4.1 Uni-Modal Tasks: Pure Sketch Analysis'
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 单模态任务：纯草图分析
- en: These tasks study sketches in isolation without other data modalities. Key deep
    learning-based applications in this area include recognition, retrieval/hashing,
    generation, grouping, segmentation, and abstraction.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务在没有其他数据模态的情况下单独研究草图。该领域的关键深度学习应用包括识别、检索/哈希、生成、分组、分割和抽象。
- en: 4.1.1 Recognition
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 识别
- en: 'Sketch recognition [[6](#bib.bib6)] aims to predict the class label of a given
    sketch, which is one of the most fundamental tasks in computer vision. It has
    a variety of practical applications including: interactive drawing systems that
    provide feedback to users [[83](#bib.bib83)], sketch-based science education  [[84](#bib.bib84)],
    games [[54](#bib.bib54), [5](#bib.bib5)], etc. Both object [[6](#bib.bib6), [5](#bib.bib5)]
    and scene [[85](#bib.bib85), [86](#bib.bib86)] categories have been studied from
    a recognition perspective. Notably sketch recognition techniques underpin the
    popular web game QuickDraw and WeChat mini-app Caihua Xiaoge, both released by
    Google.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 草图识别[[6](#bib.bib6)]旨在预测给定草图的类别标签，这是计算机视觉中最基本的任务之一。它有多种实际应用，包括：提供反馈的互动绘图系统[[83](#bib.bib83)]，基于草图的科学教育[[84](#bib.bib84)]，游戏[[54](#bib.bib54),
    [5](#bib.bib5)]等。从识别角度出发，已经研究了物体[[6](#bib.bib6), [5](#bib.bib5)]和场景[[85](#bib.bib85),
    [86](#bib.bib86)]类别。值得注意的是，草图识别技术支持了流行的网页游戏QuickDraw和微信小程序彩画小哥，均由Google发布。
- en: Sketch recognition can be categorized into (i) offline and (ii) online recognition
    settings. Offline recognition systems take the whole sketch as input and predict
    a class label based on the complete sketch. Online recognition systems take the
    accumulated sketch strokes and continuously predict the class label, during sketching.
    Offline recognition methods are more common, but online methods can be used in
    more interactive real-time applications such as real-time drawing guidance [[60](#bib.bib60)],
    tracing, and interactive sketch retrieval.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 草图识别可以分为（i）离线识别和（ii）在线识别两种设置。离线识别系统将整个草图作为输入，并基于完整的草图预测一个类别标签。在线识别系统则根据累积的草图笔画在绘制过程中持续预测类别标签。离线识别方法更为常见，但在线方法可以用于更多互动的实时应用，如实时绘图指导 [[60](#bib.bib60)]、跟踪和互动草图检索。
- en: 'There are several current trends in sketch recognition, building on underpinning
    deep learning progress: (i) From raster image to sequence representation; (ii)
    From global representation to local analysis; (iii) From Euclidean (CNN, RNN based)
    to topological analysis (GNN based), (iv) From fully-supervised learning to self-supervised
    learning.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 草图识别的当前趋势有几个，基于深度学习的进展：（i）从栅格图像到序列表示；（ii）从全局表示到局部分析；（iii）从欧几里得（基于 CNN、RNN）到拓扑分析（基于
    GNN）；（iv）从完全监督学习到自监督学习。
- en: 'Numerous deep models have now been proposed for free-hand sketch recognition [[87](#bib.bib87),
    [88](#bib.bib88), [89](#bib.bib89), [67](#bib.bib67), [90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96),
    [97](#bib.bib97), [98](#bib.bib98)]. In the following, we review these models
    from the perspectives of network architectures and loss functions. Data augmentations
    will be discussed separately in Section [4.1.6](#S4.SS1.SSS6 "4.1.6 Data Augmentations
    ‣ 4.1 Uni-Modal Tasks: Pure Sketch Analysis ‣ 4 Tasks and Methodology Taxonomy
    ‣ Deep Learning for Free-Hand Sketch: A Survey").'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '目前已提出了许多深度模型用于自由手绘草图识别 [[87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89),
    [67](#bib.bib67), [90](#bib.bib90), [91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93),
    [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98)]。接下来，我们将从网络架构和损失函数的角度回顾这些模型。数据增强将在第 [4.1.6](#S4.SS1.SSS6
    "4.1.6 Data Augmentations ‣ 4.1 Uni-Modal Tasks: Pure Sketch Analysis ‣ 4 Tasks
    and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")节单独讨论。'
- en: 'TABLE III: Comparison of representative sketch recognition networks. “–” indicates
    not mentioned or unclear in the original paper. Reported performance is top-1
    accuracy. Abbreviations in this table: “stroke accu. pic.”: stroke accumulated
    pictures; “R-FC”: residual fully-connected layer; “pad.”: padding; “tru.”: truncation;
    “augm.”: specific augmentations; “tran.”: transformer.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：代表性草图识别网络的比较。“–”表示原始论文中未提及或不明确。报告的性能为 top-1 准确率。此表中的缩写：“stroke accu. pic.”：笔画累积图片；“R-FC”：残差全连接层；“pad.”：填充；“tru.”：截断；“augm.”：特定增强；“tran.”：变压器。
- en: '| Year | Model | Architecture | Layers | Params | Ensemble | Pretrain | Input
    | Preprocess | Dataset | Accuracy |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 模型 | 架构 | 层数 | 参数 | 集成 | 预训练 | 输入 | 预处理 | 数据集 | 准确率 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2015 | Sketch-a-Net [[18](#bib.bib18)] | CNN | 5 conv. | 8.5M | ✓ |  | picture
    | augm. [[18](#bib.bib18)] | TU-Berlin [[6](#bib.bib6)] 250 cat. | 0.7490 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | Sketch-a-Net [[18](#bib.bib18)] | CNN | 5 conv. | 8.5M | ✓ |  | picture
    | augm. [[18](#bib.bib18)] | TU-Berlin [[6](#bib.bib6)] 250 cat. | 0.7490 |'
- en: '| 2016 | AlexNet-FC-GRU [[99](#bib.bib99)] |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | AlexNet-FC-GRU [[99](#bib.bib99)] |'
- en: '&#124; CNN-to-RNN &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN-to-RNN &#124;'
- en: '&#124; cascaded &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; cascaded &#124;'
- en: '| – | – |  |  |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| – | – |  |  |'
- en: '&#124; stroke &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; stroke &#124;'
- en: '&#124; accu. pic. &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; accu. pic. &#124;'
- en: '| – | TU-Berlin 160 cat. | 0.8510 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| – | TU-Berlin 160 cat. | 0.8510 |'
- en: '| 2018 | SketchMate [[19](#bib.bib19), [100](#bib.bib100)] | RNN | 2 GRU |
    – |  |  | stroke vector | tru. & pad. [[19](#bib.bib19)] | QuickDraw 3.8M [[19](#bib.bib19)]
    | 0.7788 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | SketchMate [[19](#bib.bib19), [100](#bib.bib100)] | RNN | 2 GRU |
    – |  |  | stroke vector | tru. & pad. [[19](#bib.bib19)] | QuickDraw 3.8M [[19](#bib.bib19)]
    | 0.7788 |'
- en: '| 2018 | SketchMate [[19](#bib.bib19), [100](#bib.bib100)] |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | SketchMate [[19](#bib.bib19), [100](#bib.bib100)] |'
- en: '&#124; CNN-RNN &#124;'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN-RNN &#124;'
- en: '&#124; dual-branch &#124;'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; dual-branch &#124;'
- en: '|'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 5 conv. &#124;'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5 conv. &#124;'
- en: '&#124; & 2 GRU &#124;'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; & 2 GRU &#124;'
- en: '| – |  |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| – |  |  |'
- en: '&#124; picture &#124;'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; picture &#124;'
- en: '&#124; & stroke vector &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; & stroke vector &#124;'
- en: '| tru. & pad. [[19](#bib.bib19)] | QuickDraw 3.8M [[19](#bib.bib19)] | 0.7949
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| tru. & pad. [[19](#bib.bib19)] | QuickDraw 3.8M [[19](#bib.bib19)] | 0.7949
    |'
- en: '| 2017 | Jia et al.  [[101](#bib.bib101)] |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Jia et al.  [[101](#bib.bib101)] |'
- en: '&#124; RNN-RNN &#124;'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RNN-RNN &#124;'
- en: '&#124; dual branch &#124;'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 双分支 &#124;'
- en: '| – | – | ✓ | ✓ |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| – | – | ✓ | ✓ |'
- en: '&#124; CNN features &#124;'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN 特征 &#124;'
- en: '&#124; of stroke &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 笔画 &#124;'
- en: '&#124; accu. pic. &#124;'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; accu. pic. &#124;'
- en: '|'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; reflection, &#124;'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 反射，&#124;'
- en: '&#124; rotation, etc. &#124;'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 旋转等 &#124;'
- en: '| TU-Berlin | 0.9220 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| TU-Berlin | 0.9220 |'
- en: '| 2017 | DVSF [[102](#bib.bib102)] |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | DVSF [[102](#bib.bib102)] |'
- en: '&#124; R-FC and RNN &#124;'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; R-FC 和 RNN &#124;'
- en: '&#124; dual branch &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 双分支 &#124;'
- en: '| – | – |  | ✓ |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| – | – |  | ✓ |'
- en: '&#124; CNN features &#124;'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN 特征 &#124;'
- en: '&#124; of stroke &#124;'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 笔画 &#124;'
- en: '&#124; accu. pic. &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; accu. pic. &#124;'
- en: '| – | TU-Berlin | 0.7960 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| – | TU-Berlin | 0.7960 |'
- en: '| 2018 | FBin DAB-Net [[103](#bib.bib103)] | binary CNN | – | – |  |  | picture
    | – | TU-Berlin | 0.7370 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | FBin DAB-Net [[103](#bib.bib103)] | 二值 CNN | – | – |  |  | 图片 | –
    | TU-Berlin | 0.7370 |'
- en: '| 2018 | RNN$\rightarrow$CNN [[104](#bib.bib104)] |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | RNN$\rightarrow$CNN [[104](#bib.bib104)] |'
- en: '&#124; RNN-to-CNN &#124;'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RNN-to-CNN &#124;'
- en: '&#124; cascaded &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 级联 &#124;'
- en: '|'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2 LSTM &#124;'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2 LSTM &#124;'
- en: '&#124; & 5 conv. &#124;'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; & 5 conv. &#124;'
- en: '| – |  | ✓ | stroke vector | augm. [[3](#bib.bib3)] | TU-Berlin | 0.7849 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| – |  | ✓ | 笔画向量 | augm. [[3](#bib.bib3)] | TU-Berlin | 0.7849 |'
- en: '| 2019 | multi-graph tran. [[23](#bib.bib23)] | GNN | 4 tran. | 10M |  |  |
    stroke vector | – | QuickDraw subset [[23](#bib.bib23)] | 0.7070 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | 多图转换 [[23](#bib.bib23)] | GNN | 4 转换 | 10M |  |  | 笔画向量 | – | QuickDraw
    子集 [[23](#bib.bib23)] | 0.7070 |'
- en: 'Networks Figure [6](#S2.F6 "Figure 6 ‣ 2.1 Intrinsic Traits and Domain-Unique
    Challenges ‣ 2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey") (below)
    summarizes the evolution of the deep learning-based sketch representations. Moreover,
    Table [III](#S4.T3 "TABLE III ‣ 4.1.1 Recognition ‣ 4.1 Uni-Modal Tasks: Pure
    Sketch Analysis ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand
    Sketch: A Survey") lists various networks that are engineered for free-hand sketches
    and capable of sketch recognition. We next introduce some representative networks.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '网络图 [6](#S2.F6 "Figure 6 ‣ 2.1 Intrinsic Traits and Domain-Unique Challenges
    ‣ 2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey")（下方）总结了基于深度学习的草图表示的发展。此外，表 [III](#S4.T3
    "TABLE III ‣ 4.1.1 Recognition ‣ 4.1 Uni-Modal Tasks: Pure Sketch Analysis ‣ 4
    Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")
    列出了为自由手绘草图工程设计的各种网络，并具有草图识别能力。接下来，我们介绍一些代表性的网络。'
- en: '(i) Sketch-a-Net [[18](#bib.bib18), [3](#bib.bib3)] was the first deep CNN
    designed for free-hand sketch. Compared with classic photo-oriented CNN architecture [[105](#bib.bib105)],
    the sketch-specific aspects of its architecture mainly include: (a) Considered
    the sparse low-texture nature of sketch, larger size ($15\times 15$) first layer
    filters are used to capture more context. (b) Local response normalization (LRN) [[105](#bib.bib105)]
    layers are removed for faster learning without sacrificing performance, since
    LRN is for “pixel brightness normalization”, but most sketches are binary images.
    (c) Two novel sketch-specific data augmentation strategies are proposed, leveraging
    stroke appearance and stroke sequence. (d) Finally, an ensemble of Sketch-A-Net
    were combined using joint Bayesian fusion [[106](#bib.bib106)].'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: (i) Sketch-a-Net [[18](#bib.bib18), [3](#bib.bib3)] 是首个为自由手绘草图设计的深度 CNN。与经典的以照片为导向的
    CNN 架构 [[105](#bib.bib105)] 相比，其架构的草图特定方面主要包括：（a）考虑到草图的稀疏低纹理特性，使用更大的尺寸（$15\times
    15$）的第一层滤波器以捕捉更多上下文。（b）为了更快的学习而不牺牲性能，移除了局部响应归一化（LRN） [[105](#bib.bib105)] 层，因为
    LRN 主要用于“像素亮度归一化”，而大多数草图为二值图像。（c）提出了两种新颖的草图特定数据增强策略，利用笔画外观和笔画序列。（d）最后，使用联合贝叶斯融合 [[106](#bib.bib106)]
    结合了多个 Sketch-A-Net。
- en: '(ii) Sarvadevabhatla et al.  [[99](#bib.bib99)] proposed a sketch recognition
    network to leverage the sequential process of sketching, where each training sketch
    is plotted as a continuous sequence of cumulative stroke pictures and the corresponding
    AlexNet [[105](#bib.bib105)] based deep features will be sent into a Gated Recurrent
    Unit (GRU) [[107](#bib.bib107)] network in sequence. This network is also able
    to work in online recognition mode, since it involves the intermediate status
    of the sketch. Furthermore, Jia et al.  [[101](#bib.bib101)] proposed a multiple
    feature based model to improve this idea and obtained good performance on sketch
    recognition as reported in Table [III](#S4.T3 "TABLE III ‣ 4.1.1 Recognition ‣
    4.1 Uni-Modal Tasks: Pure Sketch Analysis ‣ 4 Tasks and Methodology Taxonomy ‣
    Deep Learning for Free-Hand Sketch: A Survey"), where multiple GRU networks separately
    encode multiple features of the cumulative stroke groups and their outputs are
    combined by time-step-based weights.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '(ii) Sarvadevabhatla等人[[99](#bib.bib99)]提出了一种草图识别网络，以利用草图的顺序过程，其中每个训练草图被绘制为一个连续的笔划图片序列，相应的基于AlexNet [[105](#bib.bib105)]的深度特征将被输入到一个Gated
    Recurrent Unit (GRU) [[107](#bib.bib107)]网络中。该网络也能够在在线识别模式下工作，因为它涉及草图的中间状态。此外，Jia等人[[101](#bib.bib101)]提出了一种多特征基础的模型来改进这一思想，并在草图识别上取得了良好的表现，如表[III](#S4.T3
    "TABLE III ‣ 4.1.1 Recognition ‣ 4.1 Uni-Modal Tasks: Pure Sketch Analysis ‣ 4
    Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")所报告，其中多个GRU网络分别对累积笔划组的多个特征进行编码，并通过时间步长权重将其输出结合起来。'
- en: (iii) Similarly, He et al. [[102](#bib.bib102)] proposed the deep visual-sequential
    fusion (DVSF) net to capture spatial and temporal patterns of sketches simultaneously.
    For each training sketch, its three accumulation sub-pictures (with $60\%$, $80\%$,
    $100\%$ of strokes) go through three-way CNNs (ResNet-18 [[108](#bib.bib108)])
    to produce deep features, which are fed into both visual and sequential networks.
    In particular, the visual and sequential networks are implemented by residual
    fully-connected (R-FC) and Residual Long Short Term Memory (R-LSTM) [[109](#bib.bib109)]
    layers respectively. The visual and sequential paths are integrated by a fusion
    layer for final recognition.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) 类似地，He等人[[102](#bib.bib102)]提出了深度视觉-序列融合（DVSF）网络，以同时捕捉草图的空间和时间模式。对于每个训练草图，其三个累计子图（含有$60\%$、$80\%$、$100\%$的笔划）经过三路CNN（ResNet-18 [[108](#bib.bib108)]）以生成深度特征，这些特征被输入到视觉和序列网络中。具体而言，视觉和序列网络分别由残差全连接（R-FC）和残差长短期记忆（R-LSTM）[[109](#bib.bib109)]层实现。视觉和序列路径通过融合层进行集成，以进行最终识别。
- en: '(iv) In 2017, Ha and Eck proposed the groundbreaking SketchRNN [[5](#bib.bib5)],
    which performs representation learning through its Variational Inference (VI) [[110](#bib.bib110)]
    based sequential sketch generation model. Distinctively different to the prior
    stroke accumulated sub-picture representations, the key points of sketch strokes
    are directly fed into the RNN backbone of SketchRNN. In particular, as illustrated
    in Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Intrinsic Traits and Domain-Unique Challenges
    ‣ 2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey"), each key point
    is denoted as a vector consisting of two coordinate bits (i.e., horizontal and
    vertical coordinates) and the corresponding flag bits. The flag bits indicate
    the start/end of a stroke by pen state. Although initially proposed for generative
    modeling, the encoder backbone of SketchRNN also performs well for sketch recognition⁸⁸8[https://github.com/payalbajaj/sketch_rnn_classification](https://github.com/payalbajaj/sketch_rnn_classification).'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '(iv) 在2017年，Ha和Eck提出了开创性的SketchRNN [[5](#bib.bib5)]，该模型通过其变分推断（VI）[[110](#bib.bib110)]基础的顺序草图生成模型进行表示学习。与先前的笔划累计子图像表示显著不同，草图笔划的关键点直接输入SketchRNN的RNN骨干网络。具体而言，如图[3](#S2.F3
    "Figure 3 ‣ 2.1 Intrinsic Traits and Domain-Unique Challenges ‣ 2 Background ‣
    Deep Learning for Free-Hand Sketch: A Survey")所示，每个关键点表示为由两个坐标位（即水平和垂直坐标）及相应的标志位组成的向量。标志位通过笔状态指示笔划的开始/结束。尽管最初是为生成建模提出的，SketchRNN的编码器骨干网络在草图识别中也表现良好⁸⁸8[https://github.com/payalbajaj/sketch_rnn_classification](https://github.com/payalbajaj/sketch_rnn_classification)。'
- en: (v) Xu et al. proposed the sketch hashing network SketchMate [[19](#bib.bib19)],
    where the backbone is a CNN-RNN dual branch network, using CNN to extract abstract
    visual concepts and RNN to model human temporal stroke order. The CNN branch takes
    in the raster pixel sketch pictures; and the RNN branch process the vector sketch
    (i.e., key point coordinates). The branches are combined by a late-fusion layer.
    This network demonstrates the complementarity of visual and temporal embedding
    spaces of sketch representation. This CNN-RNN dual-branch modeling idea has been
    widely applied to other sketch tasks, e.g., SPFusionNet [[61](#bib.bib61)] for
    sketch semantic segmentation. In addition to the parallel pipelines of CNN and
    RNN, some cascaded pipelines (e.g., RNN-to-CNN [[104](#bib.bib104)]) have also
    been studied.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: (v) 徐等人提出了草图哈希网络 SketchMate [[19](#bib.bib19)]，其中骨干网络为 CNN-RNN 双分支网络，利用 CNN
    提取抽象视觉概念，RNN 则用于建模人体的时间笔划顺序。CNN 分支处理栅格像素草图图像；RNN 分支处理矢量草图（即关键点坐标）。这两个分支通过一个晚融合层结合在一起。该网络展示了草图表示中视觉和时间嵌入空间的互补性。这种
    CNN-RNN 双分支建模思路已广泛应用于其他草图任务，例如用于草图语义分割的 SPFusionNet [[61](#bib.bib61)]。除了 CNN
    和 RNN 的并行管道，还研究了某些级联管道（如 RNN-to-CNN [[104](#bib.bib104)]）。
- en: (vi) A sketch can be represented as the sparsely connected graphs in topological
    space. Multi-Graph Transformer (MGT) [[23](#bib.bib23)] is a GNN model that learns
    both geometric structure and temporal information from sketch graphs. MGT injects
    domain knowledge into Graph Transformers⁹⁹9Transformer is essentially a GNN that
    encodes input as a fully-connected graph. through sketch-specific graphs. In particular,
    MGT represents each sketch as multiple intra-stroke and extra-stroke graphs, to
    model its local and global topological stroke structure, respectively.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: (vi) 草图可以表示为拓扑空间中的稀疏连接图。Multi-Graph Transformer (MGT) [[23](#bib.bib23)] 是一种
    GNN 模型，能够从草图图中学习几何结构和时间信息。MGT 将领域知识注入到图 Transformer 中，特别是将每个草图表示为多个内部笔划和外部笔划图，以分别建模其局部和全局拓扑笔划结构。
- en: (vii) While prior approaches to sketch recognition are based on supervised learning
    (e.g., [[3](#bib.bib3), [99](#bib.bib99), [102](#bib.bib102)]), [[111](#bib.bib111)]
    provided the first investigation of self-supervised representation learning for
    sketch, proposing a rotation- and deformation- based deep self-supervised model
    (rot. & def. model). This model uses multi-branch CNN and TCN network to represent
    sketch in a self-supervised setting.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: (vii) 尽管先前的草图识别方法基于监督学习（例如 [[3](#bib.bib3), [99](#bib.bib99), [102](#bib.bib102)]），[[111](#bib.bib111)]
    提出了首个草图自监督表示学习的研究，提出了基于旋转和变形的深度自监督模型（旋转和变形模型）。该模型使用多分支 CNN 和 TCN 网络在自监督设置中表示草图。
- en: '![Refer to caption](img/cb7a86f90050700d502ed38acd81e2c5.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cb7a86f90050700d502ed38acd81e2c5.png)'
- en: 'Figure 8: Image entropy histogram of 9K sketch ‘stars’ [[19](#bib.bib19)].
    The blue bars denote the bin counts within different entropy ranges. Some representative
    sketches corresponding to different entropy values are illustrated.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：9K 草图“星星”的图像熵直方图 [[19](#bib.bib19)]。蓝色条形表示不同熵范围内的计数。一些代表性草图与不同熵值的对应关系进行了说明。
- en: 'Loss Functions Most of the previous deep sketch recognition methods use cross-entropy
    softmax loss to train deep neural networks. An active research question is whether
    sketch-specific loss functions can help to further improve recognition performance.
    To that end, Xu et al. proposed the sketch-specific center loss  [[19](#bib.bib19)]
    for million-scale sketches, based on a staged-training strategy. The basis is
    that the image entropy distribution of each sketch category is a truncated Gaussian
    distribution (see Figure [8](#S4.F8 "Figure 8 ‣ 4.1.1 Recognition ‣ 4.1 Uni-Modal
    Tasks: Pure Sketch Analysis ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning
    for Free-Hand Sketch: A Survey") for an example). Inspired by classic Bayesian
    decision theory [[112](#bib.bib112)], Mishra et al. proposed a novel metric loss
    to drive the pretrained deep neural network to minimize the Bayesian risk of mis-classifying
    sketch pairs that were randomly selected within each mini-batch [[113](#bib.bib113)].
    Based on this Bayesian risk loss, sketch recognition needs two-stage training.
    After obtaining the features, a linear SVM [[114](#bib.bib114)] is trained as
    the classifier.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数 大多数以前的深度素描识别方法使用交叉熵softmax损失来训练深度神经网络。一个活跃的研究问题是，是否可以通过特定于素描的损失函数进一步提高识别性能。为此，Xu
    等人提出了针对百万规模素描的素描特定中心损失[[19](#bib.bib19)]，基于分阶段训练策略。其基础是每个素描类别的图像熵分布是一个截断高斯分布（请参见图[8](#S4.F8
    "图 8 ‣ 4.1.1 识别 ‣ 4.1 单模态任务：纯素描分析 ‣ 4 任务和方法分类 ‣ 深度学习在自由手素描中的应用：综述")的示例）。受到经典贝叶斯决策理论[[112](#bib.bib112)]的启发，Mishra
    等人提出了一种新颖的度量损失，以驱动预训练的深度神经网络最小化在每个mini-batch中随机选择的素描对的贝叶斯风险[[113](#bib.bib113)]。基于这种贝叶斯风险损失，素描识别需要两阶段训练。在获得特征后，训练线性SVM[[114](#bib.bib114)]作为分类器。
- en: 'Summary and Discussion In this subsection, we reviewed sketch recognition related
    deep learning works, from the perspective of architecture and loss functions.
    Promising areas of future work include: Online sketch recognition, motivated by
    practical human-computer interaction applications; going beyond the mainstream
    line of supervised sketch recognition to investigate semi-supervised, self-supervised
    [[115](#bib.bib115)] and unsupervised learning for recognition; zero-shot [[86](#bib.bib86)]
    and few-shot sketch recognition [[116](#bib.bib116)]; and multi-task learning
    [[93](#bib.bib93), [117](#bib.bib117)] to simultaneously solve sketch recognition
    with other tasks.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 总结与讨论 在本小节中，我们从架构和损失函数的角度回顾了与素描识别相关的深度学习工作。未来工作中有前景的领域包括：在线素描识别，受到实际人机交互应用的驱动；超越主流的监督素描识别，探索半监督、自监督[[115](#bib.bib115)]和无监督学习的识别；零-shot[[86](#bib.bib86)]和少-shot素描识别[[116](#bib.bib116)]；以及多任务学习[[93](#bib.bib93),
    [117](#bib.bib117)]以同时解决素描识别和其他任务。
- en: 4.1.2 Retrieval and Hashing
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 检索和哈希
- en: Sketch retrieval [[118](#bib.bib118), [119](#bib.bib119), [60](#bib.bib60)]
    methods aim to use a query sketch to retrieve similar samples from a gallery or
    database of sketches. Sketch retrieval is a challenging task due to abstraction,
    intra-class variation, drawing style variation, and feature sparsity. These properties
    make it difficult to localize repeatable feature points across sketches (e.g.,
    the manner of SIFT [[120](#bib.bib120)]) in order to perform classic interest-point
    based retrieval approaches [[120](#bib.bib120)]. In the deep learning era, end-to-end
    feature learning has outperformed shallow features on various retrieval tasks
    in computer vision, and CNNs have also been used for sketch retrieval.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 素描检索[[118](#bib.bib118), [119](#bib.bib119), [60](#bib.bib60)]方法旨在使用查询素描从素描库或数据库中检索相似的样本。由于抽象、类内变异、绘图风格变化和特征稀疏性，素描检索是一项具有挑战性的任务。这些特性使得在素描中定位可重复的特征点变得困难（例如，SIFT[[120](#bib.bib120)]的方式），以执行经典的基于兴趣点的检索方法[[120](#bib.bib120)]。在深度学习时代，端到端特征学习在计算机视觉的各种检索任务中超过了浅层特征，CNN也被用于素描检索。
- en: 'Common practice in image retrieval is to use CNNs to learn a vector embedding,
    and then perform retrieval/matching as Nearest Neighbor search. Most existing
    deep sketch retrieval models work in a similar metric learning manner, with research
    focusing on CNN architectures and loss function designs for effective sketch matching.
    Wang et al. [[118](#bib.bib118)] proposed a representative sketch retrieval pipeline,
    which has two key components: A pure convolutional layer based Siamese CNN backbone,
    and an $\ell_{1}$ norm distance based pair-wise loss between query and gallery
    images. The idea is two-fold: (i) Use the convolutional feature map to preserve
    the spatial information for sketches without point correspondence. (ii) Compute
    distance in feature space, and optimize for similar pairs to be nearby while different
    pairs to be far apart.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像检索中，常见做法是使用CNN学习向量嵌入，然后执行最近邻搜索进行检索/匹配。大多数现有的深度素描检索模型以类似的度量学习方式工作，研究集中于CNN架构和损失函数设计，以实现有效的素描匹配。Wang等人 [[118](#bib.bib118)]
    提出了一个代表性的素描检索流水线，其有两个关键组件：一个纯卷积层的Siamese CNN骨干网络，以及一个基于$\ell_{1}$范数的查询图像和图库图像之间的成对损失。其理念有两个方面：（i）使用卷积特征图来保留没有点对应的素描的空间信息。（ii）在特征空间中计算距离，并优化使得相似对彼此接近，而不同对彼此远离。
- en: Given the growing number of images available, there is also an increasing concern
    about scalability of retrieval, leading to studies of hashing-based methods where
    all sketches are encoded and searched as binary hash code vectors, rather than
    real-valued vectors. Xu et al. [[19](#bib.bib19)] proposed the first deep sketch-hashing
    model. Their deep sketch hashing used a dual-branch CNN-RNN network to exploit
    both global appearance and local sequential stroke information, as well as a new
    center loss variant to ensure the learned embedding is more semantically meaningful.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 随着可用图像数量的增加，关于检索的可扩展性的关注也在增加，这导致了基于哈希的方法研究，其中所有素描被编码并作为二进制哈希码向量进行搜索，而不是实值向量。Xu等人 [[19](#bib.bib19)]
    提出了第一个深度素描哈希模型。他们的深度素描哈希使用了一个双分支CNN-RNN网络，利用了全局外观和局部序列笔划信息，并且使用了新的中心损失变体来确保学习到的嵌入具有更具语义意义。
- en: The sketch retrieval/hashing methods mentioned so far exploit supervised information.
    If class labels are unavailable, adversarial training can be used to learn a feature
    representation for sketches. Based on the Generative Adversarial Network (GAN) [[121](#bib.bib121)],
    Creswell et al. [[119](#bib.bib119)] proposed Sketch-GAN for unsupervised sketch
    retrieval, where both the query and gallery sketches are represented by the output
    features of the discriminator network.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止提到的素描检索/哈希方法利用了监督信息。如果没有类别标签，可以使用对抗训练来学习素描的特征表示。基于生成对抗网络（GAN） [[121](#bib.bib121)]，Creswell等人 [[119](#bib.bib119)]
    提出了用于无监督素描检索的Sketch-GAN，其中查询素描和图库素描都由鉴别器网络的输出特征表示。
- en: 4.1.3 Generation
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 生成
- en: 'TABLE IV: Comparison of the representative sketch generation deep models. “gen.”,
    “rec.”, and “com.” denote “generation”, “reconstruction”, and “completion”, respectively.
    “RNN+VAE” means “RNN backbone based VAE”.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 代表性素描生成深度模型的比较。“gen.”、“rec.”和“com.”分别表示“生成”、“重建”和“完成”。“RNN+VAE”表示“基于RNN骨干的VAE”。'
- en: '| Pipelines | Representative Ref. | Applications | Advantages & Disadvantages
    |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 流水线 | 代表性参考 | 应用 | 优势与劣势 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| RNN+VAE | SketchRNN [[5](#bib.bib5)] | gen., rec., com. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| RNN+VAE | SketchRNN [[5](#bib.bib5)] | 生成，重建，组合 |'
- en: '&#124; A: brief, flexible. &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A: 简洁，灵活。 &#124;'
- en: '&#124; D: scribble effect, single-class gen. &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D: 涂鸦效果，单类生成 &#124;'
- en: '|'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| RNN+GAN | SkeGAN [[122](#bib.bib122)] | gen., com. |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| RNN+GAN | SkeGAN [[122](#bib.bib122)] | 生成，组合 |'
- en: '&#124; A: less scribble effect, faster convergence. &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A: 较少的涂鸦效果，更快的收敛。 &#124;'
- en: '&#124; D: single-class gen. &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D: 单类生成。 &#124;'
- en: '|'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| RNN+VAE+GAN | VASkeGAN [[122](#bib.bib122)] | gen., rec., com. |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| RNN+VAE+GAN | VASkeGAN [[122](#bib.bib122)] | 生成，重建，组合 |'
- en: '&#124; A: less scribble effect. &#124;'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A: 较少的涂鸦效果。 &#124;'
- en: '&#124; D: high complexity, single-class gen., slower convergence &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D: 高复杂度，单类生成，收敛较慢 &#124;'
- en: '|'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| BERT |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| BERT |'
- en: '&#124; Sketchformer [[123](#bib.bib123)] &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Sketchformer [[123](#bib.bib123)] &#124;'
- en: '&#124; Sketch-BERT [[124](#bib.bib124)] &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Sketch-BERT [[124](#bib.bib124)] &#124;'
- en: '| gen., rec., com. |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 生成，重建，组合 |'
- en: '&#124; A: good at handling longer stroke sequences &#124;'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A: 擅长处理较长的笔划序列 &#124;'
- en: '&#124; D: more parameters, high complexity &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D: 参数更多，复杂度高 &#124;'
- en: '|'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CNN+RL | Doodle-SDQ [[125](#bib.bib125)] | imitating a reference |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| CNN+RL | Doodle-SDQ [[125](#bib.bib125)] | 模仿参考 |'
- en: '&#124; A: can handle unseen classes. &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A: 可以处理未见过的类别。 &#124;'
- en: '&#124; D: hybrid training (supervised learning & RL), high complexity &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D: 混合训练（监督学习与RL），高复杂度 &#124;'
- en: '|'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| VAE+Renderer | Cloud2Curve [[56](#bib.bib56)] | gen. rec. |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| VAE+Renderer | Cloud2Curve [[56](#bib.bib56)] | 生成记录 |'
- en: '&#124; A: Scaleable vector sketch generation. Long sketches. &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A: 可扩展的矢量草图生成。长草图。 &#124;'
- en: '&#124; D: High complexity. &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D: 高复杂度。 &#124;'
- en: '| ![Refer to caption](img/8f19234851019282a787a783f50e9010.png)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![参见说明](img/8f19234851019282a787a783f50e9010.png)'
- en: 'Figure 9: The pipeline of SketchRNN [[5](#bib.bib5)]. The dashed arrow line
    denotes the recurrent processing of LSTM decoder. For simplicity, the recurrent
    processing of bi-LSTM encoder is not shown here.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：SketchRNN [[5](#bib.bib5)]的流程图。虚线箭头表示LSTM解码器的递归处理。为了简化起见，这里未显示双向LSTM编码器的递归处理。
- en: 'Sketch generation [[5](#bib.bib5), [126](#bib.bib126), [127](#bib.bib127),
    [128](#bib.bib128), [129](#bib.bib129), [130](#bib.bib130), [56](#bib.bib56),
    [131](#bib.bib131)] has grown rapidly in recent years as deep learning-based approaches
    easily outperform earlier classic sketch generators [[132](#bib.bib132), [133](#bib.bib133)].
    Sketch generation has several practical applications, e.g., synthesizing novel
    pictures, assisting artist design, and finishing incomplete sketches and games
    [[131](#bib.bib131)]. It can be addressed using various deep learning tools, e.g.,
    VAE [[5](#bib.bib5), [126](#bib.bib126), [134](#bib.bib134), [131](#bib.bib131),
    [130](#bib.bib130)], GAN [[122](#bib.bib122)], VAE-GAN [[122](#bib.bib122)], Bidirectional
    Encoder Representations from Transformers (BERT) [[135](#bib.bib135)], and reinforcement
    learning (RL) [[125](#bib.bib125), [122](#bib.bib122)]. We compare these pipelines
    and their representative models in Table [IV](#S4.T4 "TABLE IV ‣ 4.1.3 Generation
    ‣ 4.1 Uni-Modal Tasks: Pure Sketch Analysis ‣ 4 Tasks and Methodology Taxonomy
    ‣ Deep Learning for Free-Hand Sketch: A Survey"). Most of these pipelines are
    flexible and able to use GRU, LSTM, Transformer as backbones to achieve the stroke-by-stroke
    generation.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '草图生成[[5](#bib.bib5), [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129), [130](#bib.bib130), [56](#bib.bib56), [131](#bib.bib131)]近年来迅速发展，因为基于深度学习的方法轻松超越了早期经典草图生成器[[132](#bib.bib132),
    [133](#bib.bib133)]。草图生成具有若干实际应用，例如合成新颖图像、辅助艺术家设计以及完成不完整的草图和游戏[[131](#bib.bib131)]。可以使用各种深度学习工具解决这些问题，例如VAE[[5](#bib.bib5),
    [126](#bib.bib126), [134](#bib.bib134), [131](#bib.bib131), [130](#bib.bib130)]、GAN[[122](#bib.bib122)]、VAE-GAN[[122](#bib.bib122)]、双向编码器表示（BERT）[[135](#bib.bib135)]和强化学习（RL）[[125](#bib.bib125),
    [122](#bib.bib122)]。我们在表[IV](#S4.T4 "TABLE IV ‣ 4.1.3 Generation ‣ 4.1 Uni-Modal
    Tasks: Pure Sketch Analysis ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning
    for Free-Hand Sketch: A Survey")中比较了这些管道及其代表性模型。这些管道大多灵活，并且能够使用GRU、LSTM、Transformer作为骨干进行逐笔生成。'
- en: The seminal model SketchRNN [[5](#bib.bib5)] is a sequence-to-sequence VAE for
    conditional and unconditional generation of vector sketches. Its encoder and decoder
    are implemented by bidirectional RNN [[136](#bib.bib136)] and unidirectional RNN,
    respectively. As stated earlier, free-hand sketches can be represented as a sequence
    of keypoints defining strokes. The main idea of SketchRNN is to simulate human
    sketching by sequential generation of these key points in terms of location and
    pen up/down status.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 开创性模型SketchRNN [[5](#bib.bib5)] 是一个序列到序列的VAE，用于条件和无条件的矢量草图生成。其编码器和解码器分别由双向RNN[[136](#bib.bib136)]和单向RNN实现。如前所述，手绘草图可以表示为定义笔触的关键点序列。SketchRNN的主要思想是通过按位置和笔的升起/落下状态顺序生成这些关键点来模拟人类草图绘制。
- en: 'As shown in Figure [9](#S4.F9 "Figure 9 ‣ 4.1.3 Generation ‣ 4.1 Uni-Modal
    Tasks: Pure Sketch Analysis ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning
    for Free-Hand Sketch: A Survey"), the VAE encoder of SketchRNN takes vector sketches
    as input, and encodes it as a vector ${\bf h}$, which is the RNN’s last hidden
    state. This vector will be further encoded as two parameters ${\bf\mu}$ and ${\bf\sigma}$
    to model a Gaussian distribution $N({\bf\mu},{\bf\sigma})$, from which a latent
    vector ${\bf z}$ will be sampled. Then, the LSTM based VAE decoder will generate
    the coordinates and pen states of the key stroke points, conditional on ${\bf
    z}$. In particular, the coordinate and state for each key point is sampled from
    a Gaussian mixture model (GMM), and also used as input for the next decoder step.
    To improve SketchRNN to deal with multi-class generation, Cao et al. [[137](#bib.bib137)]
    propose a generative model named as “AI-Sketcher”, which is also a VAE based network.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [9](#S4.F9 "Figure 9 ‣ 4.1.3 Generation ‣ 4.1 Uni-Modal Tasks: Pure Sketch
    Analysis ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch:
    A Survey") 所示，SketchRNN 的 VAE 编码器以矢量草图作为输入，并将其编码为向量 ${\bf h}$，这是 RNN 的最后隐藏状态。该向量将进一步编码为两个参数
    ${\bf\mu}$ 和 ${\bf\sigma}$ 以建模高斯分布 $N({\bf\mu},{\bf\sigma})$，然后从中采样一个潜在向量 ${\bf
    z}$。接着，基于 LSTM 的 VAE 解码器将根据 ${\bf z}$ 生成关键笔画点的坐标和笔状态。特别地，每个关键点的坐标和状态从高斯混合模型（GMM）中采样，并用作下一步解码器的输入。为了改进
    SketchRNN 以处理多类生成，Cao 等人 [[137](#bib.bib137)] 提出了一个名为“AI-Sketcher”的生成模型，这也是一个基于
    VAE 的网络。'
- en: Another line of work within sketch generation uses differentiable rendering
    [[138](#bib.bib138), [130](#bib.bib130), [56](#bib.bib56)] or reinforcement learning
    [[139](#bib.bib139), [122](#bib.bib122), [125](#bib.bib125), [140](#bib.bib140),
    [131](#bib.bib131)] to train policies that draw sketches iteratively according
    to different criteria such as adversarial training against human sketches [[139](#bib.bib139)].
    This line of work often considers factors not addressed by SketchRNN such as brush
    style and color. By considering an interpretable latent representation of sketches,
    such methods can also potentially be used to de-render sketches into programs
    or symbols [[141](#bib.bib141), [142](#bib.bib142)].
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 草图生成中的另一个研究方向使用可微渲染 [[138](#bib.bib138), [130](#bib.bib130), [56](#bib.bib56)]
    或强化学习 [[139](#bib.bib139), [122](#bib.bib122), [125](#bib.bib125), [140](#bib.bib140),
    [131](#bib.bib131)] 来训练策略，根据不同标准（如对抗训练对抗人类草图 [[139](#bib.bib139)]）迭代地绘制草图。这些工作通常考虑了
    SketchRNN 未涉及的因素，如笔刷风格和颜色。通过考虑草图的可解释潜在表示，这些方法也有可能用于将草图转换为程序或符号 [[141](#bib.bib141),
    [142](#bib.bib142)]。
- en: 'Going forward, there are several emerging trends in sketch generation, notably:
    (i) Fine-grained sketch generation [[134](#bib.bib134)]. (ii) A novel evaluation
    metric “Ske-score” [[122](#bib.bib122)], aims to provide a better metric to quantify
    the goodness of generated vector sketches. (iii) Transformer-based architectures
     [[123](#bib.bib123), [143](#bib.bib143)] are being applied to sketch generation.
    (iv) Competitive generation, which aims to render understandable sketches in the
    fewest possible strokes [[131](#bib.bib131)]. (v) Finally there is scaleable vector-graphic
    generation [[130](#bib.bib130), [56](#bib.bib56)], which aims to generate sketch
    strokes via parametric strokes rather than standard waypoint lists.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，草图生成领域出现了几个新兴趋势，特别是：（i）细粒度草图生成 [[134](#bib.bib134)]。（ii）一种新颖的评估指标“Ske-score”
    [[122](#bib.bib122)]，旨在提供一种更好的指标来量化生成的矢量草图的优劣。（iii）基于 Transformer 的架构 [[123](#bib.bib123),
    [143](#bib.bib143)] 正在应用于草图生成。（iv）竞争性生成，旨在用尽可能少的笔触呈现可理解的草图 [[131](#bib.bib131)]。（v）最后是可扩展的矢量图形生成
    [[130](#bib.bib130), [56](#bib.bib56)]，旨在通过参数化笔触而非标准路径点列表来生成草图笔触。
- en: 4.1.4 Grouping, Segmentation, and Parsing
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 分组、分割和解析
- en: 'Compared with sketch recognition, retrieval, and generation, there are several
    more fine-grained single-modal sketch analysis tasks: perceptual grouping, segmentation,
    and parsing. These tasks need sketch analysis at the local (stroke) level. Besides
    their intrinsic interest, these local sketch understanding techniques can also
    benefit other global tasks such as sketch-based image retrieval, sketch-based
    video retrieval [[144](#bib.bib144)], and sketch generation/synthesis. We next
    review recent advances in these areas.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 与草图识别、检索和生成相比，还有一些更细粒度的单模态草图分析任务：感知分组、分割和解析。这些任务需要在局部（笔触）层面上进行草图分析。除了其固有的兴趣外，这些局部草图理解技术还可以惠及其他全球任务，如基于草图的图像检索、基于草图的视频检索
    [[144](#bib.bib144)] 和草图生成/合成。我们接下来将回顾这些领域的最新进展。
- en: Sketch Perceptual Grouping (SPG) Humans have the ability to perceptually group
    visual cues into semantic object parts/components, which has been widely researched
    in Gestalt psychology area [[145](#bib.bib145), [146](#bib.bib146)]. Humans are
    able to perceptually group sketch strokes into semantic parts, e.g., airplane
    strokes grouped into fuselage and wings. Thus, sketch perceptual grouping (SPG)
    is to imitate the human ability to group strokes into semantic parts. SPG has
    been studied with pre-deep learning methods  [[147](#bib.bib147), [148](#bib.bib148),
    [149](#bib.bib149)], however progress has advanced rapidly since then. One representative
    application of SPG is to simplify sketches [[150](#bib.bib150)]. Moreover, SPG
    can also be used for sketch recognition [[151](#bib.bib151)], sketch semantic
    segmentation, synthesis [[133](#bib.bib133)], retrieval, fine-grained sketch-based
    image retrieval (FG-SBIR), sketch-based video retrieval [[144](#bib.bib144)],
    etc.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 草图感知分组（SPG） 人类具有将视觉线索感知性地分组为语义对象部件/组件的能力，这在格式塔心理学领域得到了广泛研究[[145](#bib.bib145),
    [146](#bib.bib146)]。人类能够将草图笔画感知性地分组为语义部件，例如将飞机的笔画分组为机身和机翼。因此，草图感知分组（SPG）是模仿人类将笔画分组为语义部件的能力。SPG
    已经通过深度学习前的方法[[147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149)]进行过研究，但自那时以来进展迅速。SPG
    的一个代表性应用是简化草图[[150](#bib.bib150)]。此外，SPG 还可以用于草图识别[[151](#bib.bib151)]、草图语义分割、合成[[133](#bib.bib133)]、检索、细粒度草图图像检索（FG-SBIR）、草图视频检索[[144](#bib.bib144)]等。
- en: 'Li et al. [[59](#bib.bib59), [152](#bib.bib152)] contributed the largest SPG
    dataset to date of $20,000$ manually-annotated sketches across $25$ object categories,
    and propose a universal deep grouper that can be applied to sketches of any category.
    Specifically, this deep universal grouper is also a sequence-to-sequence VAE with
    both generative and discriminative objectives: (i) Its generative loss provides
    the ability to handle unseen object categories and datasets. (ii) Its discriminative
    loss consists of local and global grouping losses, to guarantee both local and
    global consistency in the grouping outputs.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人[[59](#bib.bib59), [152](#bib.bib152)]贡献了迄今为止最大的一套 SPG 数据集，共 $20,000$ 张手工标注的草图，涵盖
    $25$ 个对象类别，并提出了一种可以应用于任何类别草图的通用深度分组器。具体来说，这种深度通用分组器也是一个序列到序列的变分自编码器（VAE），具有生成和判别目标：（i）其生成损失提供了处理未见过的对象类别和数据集的能力。（ii）其判别损失包括局部和全局分组损失，以保证分组输出的局部和全局一致性。
- en: Discussion Shallow grouping methods mainly relied on thresholding low-level
    geometric properties among the strokes, often resulting in strokes with similar
    geometry but different semantics being grouped. Contemporary SPG methods consider
    more high-level semantic and temporal information due to their deep and recurrent
    representations.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论 浅层分组方法主要依赖于笔画之间的低级几何属性阈值，通常会导致几何相似但语义不同的笔画被分组。现代 SPG 方法考虑了更多高级的语义和时间信息，因为它们具有深层和递归表示。
- en: Sketch Semantic Segmentation (SSS) Sketch semantic segmentation has drawn attention [[147](#bib.bib147),
    [153](#bib.bib153), [154](#bib.bib154)] in the free-hand sketch community as a
    classic topic prior to deep learning.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 草图语义分割（SSS） 草图语义分割在自由手绘草图社区作为经典主题引起了关注[[147](#bib.bib147), [153](#bib.bib153),
    [154](#bib.bib154)]，在深度学习之前就已存在。
- en: Sketch semantic segmentation can potentially be addressed by conventional photo
    segmentation CNNs. However, these do not exploit the vector representation of
    strokes or their temporal patterns, loading to the development of sketch-specific
    segmentation models.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 草图语义分割可以通过传统的照片分割 CNN 来解决。然而，这些方法并未利用笔画的向量表示或其时间模式，因此推动了草图特定分割模型的发展。
- en: 'Existing deep models for sketch semantic segmentation [[155](#bib.bib155),
    [156](#bib.bib156), [157](#bib.bib157), [25](#bib.bib25), [61](#bib.bib61), [24](#bib.bib24)]
    can be grouped according to architectures: CNN, RNN, GCN based models, etc.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的深度模型用于草图语义分割[[155](#bib.bib155), [156](#bib.bib156), [157](#bib.bib157),
    [25](#bib.bib25), [61](#bib.bib61), [24](#bib.bib24)]可以根据架构分为：CNN、RNN、GCN 基础模型等。
- en: Li et al. [[158](#bib.bib158)] trained a CNN-based network to transfer well-annotated
    segmentation and labels from a 3D dataset to sketch domain. They used annotated
    3D data [[153](#bib.bib153), [159](#bib.bib159)] to produce edge-maps with partial
    annotations as the synthetic sketch data to train the segmentation network.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人[[158](#bib.bib158)]训练了一个基于 CNN 的网络，将标注良好的分割和标签从 3D 数据集转移到草图领域。他们使用了标注的
    3D 数据[[153](#bib.bib153), [159](#bib.bib159)]，生成了部分标注的边缘图作为合成草图数据，以训练分割网络。
- en: Qi et al. proposed SketchSegNet [[157](#bib.bib157)] and SketchSegNet+ [[25](#bib.bib25)].
    SketchSegNet+ [[25](#bib.bib25)] considers sketch stroke orderings and is able
    to process multiple object categories. In particular, SketchSegNet and SketchSegNet+
    work in an RNN-based VAE pipeline, where the Gaussian mixture model (GMM) layers
    of SketchRNN are replaced with fully-connected softmax layers to predict the part
    labels. Stroke-RNN [[156](#bib.bib156)] uses the same encoder as SketchRNN but
    extends the decoder to predict segmentation.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Qi 等人提出了 SketchSegNet [[157](#bib.bib157)] 和 SketchSegNet+ [[25](#bib.bib25)]。SketchSegNet+
    [[25](#bib.bib25)] 考虑了素描笔画顺序，并能够处理多个对象类别。特别是，SketchSegNet 和 SketchSegNet+ 在基于
    RNN 的 VAE 管道中工作，其中 SketchRNN 的高斯混合模型 (GMM) 层被完全连接的 softmax 层替代，以预测部分标签。Stroke-RNN
    [[156](#bib.bib156)] 使用与 SketchRNN 相同的编码器，但扩展解码器以预测分割。
- en: Besides the RNN backbone, other backbones have also been explored on sketch
    segmentation. SPFusionNet [[61](#bib.bib61)] uses late fusion of CNN-RNN branches
    to represent sketches for segmentation. SketchGCN [[24](#bib.bib24)] is a graph
    convolutional neural network for sketch semantic segmentation. It uses a mixed
    pooling block to fuse the intra-stroke and inter-stroke features from its two-branch
    architecture.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 除了RNN骨干网，其他骨干网也已在素描分割中进行探索。SPFusionNet [[61](#bib.bib61)] 使用CNN-RNN分支的晚期融合来表示素描以进行分割。SketchGCN
    [[24](#bib.bib24)] 是用于素描语义分割的图卷积神经网络。它使用混合池化块来融合其双分支架构中的笔画内特征和笔画间特征。
- en: Discussion SPG essentially performs stroke-level clustering, while SSS provides
    stroke-level classification. i.e., SSS provides explicit part labels (category
    names) for each stroke, while grouping only provides aggregation relationships.
    SSS and SPG are analogous to the classic (supervised) semantic segmentation [[160](#bib.bib160)]
    and unsupervised segmentation [[161](#bib.bib161)] respectively [[59](#bib.bib59)].
    Therefore, SSS needs stronger supervision during training, namely stroke categories,
    in contrast to SPG’s stroke grouping.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论 SPG 本质上执行笔画级的聚类，而SSS提供笔画级的分类。即，SSS为每个笔画提供明确的部分标签（类别名称），而分组仅提供聚合关系。SSS 和 SPG
    类似于经典的（有监督）语义分割 [[160](#bib.bib160)] 和无监督分割 [[161](#bib.bib161)] 分别 [[59](#bib.bib59)]。因此，SSS
    需要在训练期间更强的监督，即笔画类别，而SPG则关注笔画分组。
- en: Vectorization Sketch vectorization is a widely studied topic for well-drawn
    pencil sketches (particularly pencil-and-paper scanned sketches) [[162](#bib.bib162),
    [155](#bib.bib155), [163](#bib.bib163), [164](#bib.bib164), [165](#bib.bib165),
    [142](#bib.bib142), [166](#bib.bib166), [167](#bib.bib167)]; with a few studies
    begining to address it for free-hand sketches [[115](#bib.bib115), [130](#bib.bib130),
    [56](#bib.bib56)]. It aims to generate vector representations for raster sketch
    photographs. Sketch vectorization is essentially different from sketch semantic
    segmentation, in that sketch vectorization aims for instance segmentation on the
    stroke level, rather than semantic classification for a stroke or a stroke group.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化素描的向量化是一个广泛研究的主题，特别是针对素描（尤其是铅笔和纸扫描素描）[[162](#bib.bib162), [155](#bib.bib155),
    [163](#bib.bib163), [164](#bib.bib164), [165](#bib.bib165), [142](#bib.bib142),
    [166](#bib.bib166), [167](#bib.bib167)]；少数研究开始涉及自由手绘素描[[115](#bib.bib115), [130](#bib.bib130),
    [56](#bib.bib56)]。它旨在为栅格素描照片生成向量表示。素描向量化本质上不同于素描语义分割，因为素描向量化旨在实现笔画级的实例分割，而不是对单个笔画或笔画组的语义分类。
- en: 'Sketch Parsing Recently, a new concept of “sketch parsing” [[168](#bib.bib168),
    [169](#bib.bib169), [170](#bib.bib170), [171](#bib.bib171)] has gained traction.
    As a kind of fine-grained semantic understanding of sketch, sketch parsing has
    already been applied to assist other sketch tasks [[168](#bib.bib168)], e.g.,
    sketch-based image retrieval (SBIR). Sketch parse is related to sketch semantic
    segmentation. However, as shown in Figure [10](#S4.F10 "Figure 10 ‣ 4.1.4 Grouping,
    Segmentation, and Parsing ‣ 4.1 Uni-Modal Tasks: Pure Sketch Analysis ‣ 4 Tasks
    and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey"), the
    goal is to perform pixel-wise segmentation of the semantic regions defined by
    the sketch, rather than the sketch strokes as in SSS. Existing models for sketch
    parsing thus far only use CNN-base networks to represent sketch the, e.g., SFSegNet [[169](#bib.bib169)]
    uses Deep Fully Convolutional Networks (FCN) [[172](#bib.bib172)].'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 草图解析 最近，"草图解析" [[168](#bib.bib168), [169](#bib.bib169), [170](#bib.bib170),
    [171](#bib.bib171)] 这一新概念受到了关注。作为对草图的细粒度语义理解，草图解析已经被应用于辅助其他草图任务 [[168](#bib.bib168)]，例如，基于草图的图像检索（SBIR）。草图解析与草图语义分割相关。然而，如图
    [10](#S4.F10 "图 10 ‣ 4.1.4 分组、分割与解析 ‣ 4.1 单模态任务：纯草图分析 ‣ 4 任务与方法论分类 ‣ 深度学习与自由手绘草图：综述")
    所示，目标是对草图定义的语义区域进行逐像素分割，而不是像在SSS中那样对草图笔触进行分割。现有的草图解析模型迄今仅使用CNN基础网络来表示草图，例如，SFSegNet
    [[169](#bib.bib169)] 使用深度全卷积网络 (FCN) [[172](#bib.bib172)]。
- en: '![Refer to caption](img/dbb900c51c76cf5863be102faa4149a3.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/dbb900c51c76cf5863be102faa4149a3.png)'
- en: 'Figure 10: Sketches (bus, car, cat) and ground truth annotations selected from
    sketch parsing paper [[171](#bib.bib171)]. The semantic parts and background are
    annotated by colors. Best viewed in color.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 从草图解析论文 [[171](#bib.bib171)] 中选出的草图（公交车、汽车、猫）及其地面真相标注。语义部分和背景以颜色标注。建议使用彩色查看。'
- en: '![Refer to caption](img/ed2f574c86014327232a1ab1ce71ad6b.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ed2f574c86014327232a1ab1ce71ad6b.png)'
- en: 'Figure 11: Comparison between fine-grained (instance-level) and coarse-grained
    (category-level) sketch-based image retrieval. True match photos are ticked.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 细粒度（实例级）与粗粒度（类别级）基于草图的图像检索的比较。真实匹配的照片已标记。'
- en: 4.1.5 Simplification and Abstraction
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5 简化与抽象
- en: 'Sketch simplification has been widely studied [[173](#bib.bib173), [174](#bib.bib174),
    [150](#bib.bib150), [175](#bib.bib175)] by the computer graphics community to
    simplify sketches by merging redundant strokes [[175](#bib.bib175)]. A typical
    pipeline [[176](#bib.bib176)] is two-stage: geometrically clustering strokes into
    groups (e.g., by Gestalt principles); and generating a new line to replace each
    group.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 草图简化已被计算机图形学界广泛研究 [[173](#bib.bib173), [174](#bib.bib174), [150](#bib.bib150),
    [175](#bib.bib175)]，以通过合并冗余笔触来简化草图 [[175](#bib.bib175)]。一个典型的流程 [[176](#bib.bib176)]
    是两阶段的：将笔触按几何方式聚类成组（例如，通过格式塔原理）；然后生成一条新线以替代每个组。
- en: With the prevalence of deep learning, CNN-based sketch representations have
    been used for sketch simplification. Simo-Serra et al.  [[16](#bib.bib16)] proposed
    a fully-convolutional network (FCN) for simplifying sketches directly from raster
    images of rough sketches, where a pixel level mean square error (MSE) loss is
    used to compare the training pairs of rough (input) and simplified (target) sketches.
    This approach is fully automatic and requires no user intervention. Thanks to
    these advantages, this FCN based model was further studied. However, this fully
    supervised approach needs large amounts of supervised pairs of rough sketches
    and their corresponding sketch simplifications as annotation. To alleviate this
    limitation, Simo-Serra et al.  [[177](#bib.bib177)] integrated their FCN model
    into the generative adversarial pipeline, where a fully-convolutional network
    works as the generator. This upgraded model can be jointly trained from both supervised
    and unsupervised data, and obtained significant performance improvements. To further
    study how discriminator networks can improve sketch simplification, Xu et al.  [[178](#bib.bib178)]
    proposed a multi-layer discriminator by fusing all VGG [[179](#bib.bib179)] feature
    layers to differentiate sketches and simplify lines, where weights used in layer
    fusing are automatically optimized via an intelligent adjustment mechanism. The
    experimental results demonstrate that this multi-layer discriminator helps the
    FCN based generator further improve its simplification performance. Comparing
    the experimental results of these three representative methods [[16](#bib.bib16),
    [177](#bib.bib177), [178](#bib.bib178)], suggests that pixel-level losses (i.e.,
    MSE loss) and Vanilla discriminator loss may fail to provide adequate supervision
    to help models retain semantically meaningful details when simplifying relatively
    complicated sketches.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的普及，基于CNN的草图表示已被用于草图简化。Simo-Serra等人[[16](#bib.bib16)] 提出了一个全卷积网络（FCN），用于直接从粗糙草图的光栅图像中简化草图，其中使用像素级均方误差（MSE）损失来比较粗糙（输入）和简化（目标）草图的训练对。这种方法是完全自动化的，不需要用户干预。由于这些优势，这种基于FCN的模型得到了进一步的研究。然而，这种完全监督的方法需要大量的粗糙草图及其对应的草图简化对作为注释。为了缓解这一限制，Simo-Serra等人[[177](#bib.bib177)]
    将他们的FCN模型集成到生成对抗管道中，其中一个全卷积网络作为生成器。这个升级的模型可以从有监督和无监督数据中共同训练，并获得了显著的性能提升。为了进一步研究判别网络如何提高草图简化，Xu等人[[178](#bib.bib178)]
    提出了一个多层判别器，通过融合所有VGG[[179](#bib.bib179)] 特征层来区分草图并简化线条，其中用于层融合的权重通过智能调整机制自动优化。实验结果表明，这个多层判别器帮助基于FCN的生成器进一步提高其简化性能。对这三种代表性方法[[16](#bib.bib16),
    [177](#bib.bib177), [178](#bib.bib178)] 的实验结果进行比较，表明像素级损失（即MSE损失）和Vanilla判别器损失可能无法提供足够的监督，帮助模型在简化相对复杂的草图时保留语义上的重要细节。
- en: 'A different take on simplification focuses on the epitome of a sketch [[180](#bib.bib180)].
    This was recently studied under the guise of “stroke-level sketch abstraction”
    in the free-hand sketch community. Stroke-level abstraction [[21](#bib.bib21),
    [181](#bib.bib181)] aims to abstract sketches by removing strokes that do not
    affect the recognizability of the sketch. Solving this problem provides several
    benefits: (1) It learns stroke saliency as a byproduct [[21](#bib.bib21)] – strokes
    that contribute the most to recognizability are the most salient. (2) It can be
    used to synthesize sketches of variable abstraction for generation, or data augmentation
    of discriminative sketch models [[21](#bib.bib21)]. (3) It can be used for summarization
    and compression more broadly [[181](#bib.bib181)].'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种简化方法关注于草图的典型 [[180](#bib.bib180)]。这最近在自由手绘草图社区以“笔画级草图抽象”的名义进行了研究。笔画级抽象[[21](#bib.bib21),
    [181](#bib.bib181)] 旨在通过去除不影响草图识别性的笔画来抽象化草图。解决这个问题提供了几个好处：（1）它作为副产品学习了笔画显著性[[21](#bib.bib21)]
    – 对识别性贡献最大的笔画最为显著。（2）它可以用于生成不同抽象程度的草图，或用于判别草图模型的数据增强[[21](#bib.bib21)]。（3）它可以用于更广泛的总结和压缩[[181](#bib.bib181)]。
- en: The stroke-level abstraction task can be seen as a discrete combinatorial optimization
    problem, and thus is intractable to solve with traditional methods. This was tackled
    in [[21](#bib.bib21)] by training a reinforcement learning (RL) policy to include/exclude
    each stroke in the sequence, while trading off between the number of included
    strokes and recognizability. The RL-based abstraction idea was extended by [[181](#bib.bib181)]
    to re-order input strokes, rather than being constrained to the original input
    sequence; and further to enable customizing of the abstraction goal to preserve
    different aspects of ‘recognizability’ such as category vs. attributes.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 笔画级抽象任务可以看作是一个离散组合优化问题，因此用传统方法解决是不可行的。在[[21](#bib.bib21)]中，通过训练一个强化学习（RL）策略来包括/排除序列中的每一笔，同时在包含的笔画数量和可识别性之间进行权衡。RL
    基础的抽象思想在[[181](#bib.bib181)]中得到了扩展，用于重新排序输入的笔画，而不是被限制于原始输入序列；进一步地，使得可以自定义抽象目标，以保留“可识别性”的不同方面，例如类别与属性。
- en: Discussion Despite this good progress, simplification through *merging* multiple
    strokes into a coarser replacement, rather than simply filtering them, remains
    an open question for deep learning-based sketch analysis.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论 尽管取得了这些良好的进展，通过*合并*多个笔画成更粗糙的替代品，而不仅仅是简单地过滤它们，仍然是基于深度学习的素描分析中的一个未解之谜。
- en: Discussion A bottleneck for sketch simplification is that in the existing literature
    the experimental results are mainly evaluated by visual comparison and user studies.
    Defining a good quantifiable and automatic metric to evaluate simplified sketches
    is an open problem and a big challenge. A good metric would be of great help in
    designing more well-defined loss functions for this task.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论 在现有文献中，素描简化的瓶颈在于实验结果主要通过视觉比较和用户研究进行评估。定义一个好的量化和自动化的度量来评估简化的素描仍然是一个未解决的问题和重大挑战。一个好的度量将对设计更明确的损失函数非常有帮助。
- en: 4.1.6 Data Augmentations
  id: totrans-325
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.6 数据增强
- en: The sketch-specific data augmentation methods discussed in this subsection can
    be applied to both sketch recognition and all the other sketch involved tasks
    (both single-modal and multi-modal), e.g., sketch-based image retrieval, sketch-related
    generation.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节讨论的素描特定的数据增强方法可以应用于素描识别以及所有其他涉及素描的任务（包括单模态和多模态），例如，基于素描的图像检索，素描相关的生成。
- en: (i) When represented as raster pictures, most common data augmentations designed
    for natural photos [[182](#bib.bib182)] can be applied to sketch, e.g., horizontal
    reflection/mirroring, rotation, horizontal shift, vertical shift, central zoom.
    These augmentations have already been evaluated by the early sketch-oriented deep
    learning works [[18](#bib.bib18), [183](#bib.bib183)]. However, random cropping
    is likely unsuitable for sketch since partial sketches are often too sparse to
    recognize even for humans, and some image enhancement methods based on statistics
    like contrast/histogram/brightness enhancement cannot be applied to sketches.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 当表示为栅格图像时，大多数为自然照片设计的常见数据增强方法[[182](#bib.bib182)]可以应用于素描，例如，水平反射/镜像、旋转、水平移动、垂直移动、中心缩放。这些增强方法已在早期素描导向的深度学习工作中得到评估[[18](#bib.bib18),
    [183](#bib.bib183)]。然而，随机裁剪可能不适用于素描，因为部分素描往往过于稀疏，甚至人类也难以识别，并且一些基于统计的图像增强方法如对比度/直方图/亮度增强不能应用于素描。
- en: (ii) Stroke thickening/dilation can be used for free-hand sketch. As discussed
    in some previous works on spatially-sparse convolutional neural networks [[184](#bib.bib184)],
    the subtle details of sparse sketch strokes can be lost after multiple layers
    of convolution. Thus, this can be useful for deep neural networks that process
    sketches as image inputs.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) 笔画加粗/膨胀可以用于手绘素描。正如一些关于空间稀疏卷积神经网络的早期研究[[184](#bib.bib184)]所讨论的那样，多层卷积之后，稀疏素描笔画的细微细节可能会丢失。因此，这对于将素描作为图像输入进行处理的深度神经网络是有用的。
- en: (iii) Yu et al. [[3](#bib.bib3)] proposed to remove the strokes to obtain more
    diverse sketches. Based on the observation [[6](#bib.bib6)] that humans tend to
    draw outlines first before the detail, heuristics can be proposed to remove strokes
    with probability dependent on their sequential order.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) Yu 等人[[3](#bib.bib3)] 提出了通过去除笔画来获得更多样化的素描。基于观察[[6](#bib.bib6)]，人们往往首先绘制轮廓再绘制细节，可以提出启发式方法来根据笔画的顺序以一定的概率去除笔画。
- en: (iv) Zheng et al. [[185](#bib.bib185)] proposed a Bezier pivot based deformation
    (BPD) strategy and a mean stroke reconstruction (MSR) approach. These do not need
    any temporal information in the sketch. The main idea of MSR is to generate novel
    sketches with smaller intra-class variance.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: (iv) Zheng等人[[185](#bib.bib185)] 提出了基于贝塞尔支点的变形（BPD）策略和均值笔画重建（MSR）方法。这些方法不需要素描中的任何时间信息。MSR的主要思想是生成具有较小类内方差的新素描。
- en: '(v) Liu et al. [[186](#bib.bib186)] proposed two sketch-specific data augmentation
    strategies: (a) Manually extract some strokes from sketch SVG files to construct
    noise stroke masks. Then, randomly apply the noise stroke masks to the original
    sketches to synthesize augmented sketches. (b) Randomly extract a patch from one
    sketch, and attach it to a given sketch.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: (v) Liu等人[[186](#bib.bib186)] 提出了两种素描特定的数据增强策略：（a）手动从素描SVG文件中提取一些笔画以构建噪声笔画掩模。然后，随机将噪声笔画掩模应用于原始素描，以合成增强素描。（b）从一个素描中随机提取一个补丁，并将其附加到给定素描上。
- en: (vi) Muhammad et al. [[21](#bib.bib21)] applied reinforcement learning to learn
    a sketch abstraction model that preserves the semantics of the sketch. Once trained,
    this model can be applied to generate augmentations of an input sketch at different
    abstraction levels.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: (vi) Muhammad等人[[21](#bib.bib21)] 应用强化学习来学习一个素描抽象模型，该模型保留了素描的语义。一旦训练完成，该模型可以用于生成不同抽象级别的输入素描的增强。
- en: Discussion Compared with augmentations on full images (e.g., rotation, shift),
    the augmentation strategies above make better use of stroke information both locally
    and globally. However, only [[21](#bib.bib21)] makes (limited) use of human sketch
    variability to perform augmentation. In future an interesting direction is to
    learn from the variation in sketch style between different humans, and treat sketch
    augmentation as a cross-human style transfer problem.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 与对完整图像（例如，旋转、平移）进行的数据增强相比，上述增强策略更好地利用了局部和全局的笔画信息。然而，只有[[21](#bib.bib21)]利用了（有限的）人类素描变异性来执行数据增强。未来，一个有趣的方向是从不同人类之间的素描风格变异中学习，并将素描增强视为跨人类风格迁移问题。
- en: '4.2 Multi-Modal Tasks: Sketch with Other Modalities'
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 多模态任务：与其他模态的素描
- en: Free-hand sketch has several cross-modal applications when paired with other
    data modalities. In this section we review sketch-related cross-modal topics including
    visual (e.g., natural photo, 3D shape, video) and text domains.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 自由手素描在与其他数据模态配对时具有几个跨模态应用。在本节中，我们回顾与素描相关的跨模态主题，包括视觉（例如，自然照片、3D形状、视频）和文本领域。
- en: 'Nowadays, most visual retrieval approaches work under the “query-by-example” (QBE) [[187](#bib.bib187)]
    setting where users provide examples of the content that they seek. Compared with
    other query modalities (e.g., photo, video, text), sketch has several unique advantages:
    In some scenarios users do not know the name of the object that they seek, or
    find it hard to describe (such as fine-grained details of a fashion item) in order
    to query-by-text. Meanwhile, it may be difficult or impractical to provide photos
    or video examples of the object that they seek. Sketch-based image retrieval provides
    a query modality where users express their target object by rendering their mental
    image in sketch. It is particularly useful when searching at the fine-grained
    instance-level. Thus, sketch can be used as a modality to retrieve natural photo,
    manga [[188](#bib.bib188)], 3D shape, video, etc.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大多数视觉检索方法在“示例查询”（QBE）[[187](#bib.bib187)]设置下工作，即用户提供他们所寻找内容的示例。与其他查询模态（例如，照片、视频、文本）相比，素描有几个独特的优势：在某些情况下，用户不知道他们所寻找对象的名称，或者难以描述（例如时尚物品的细节），从而无法通过文本查询。同时，提供所寻找对象的照片或视频示例可能会很困难或不切实际。基于素描的图像检索提供了一种查询模态，用户通过绘制他们的心理图像来表达目标对象。它在细粒度实例级别的搜索中特别有用。因此，素描可以用作检索自然照片、漫画[[188](#bib.bib188)]、3D形状、视频等的模态。
- en: 4.2.1 Sketch-Photo Retrieval
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 素描-照片检索
- en: Sketch-photo retrieval is also known as sketch-based image retrieval (SBIR) [[1](#bib.bib1),
    [189](#bib.bib189), [4](#bib.bib4), [190](#bib.bib190), [191](#bib.bib191), [192](#bib.bib192),
    [193](#bib.bib193), [194](#bib.bib194), [195](#bib.bib195)]^(10)^(10)10Note that
    the common setting for SBIR is sketch as a query modality for images, but most
    methods enable either modality to be used as a query if desired. . SBIR is challenging
    for all the reasons that sketch-analysis in general is challenging (sparse and
    abstract input). It is particularly challenging because of the difficulty of comparing
    sparse line drawings with dense pixel representations, especially when the input
    could be a very abstract, or iconic (symbolic) representation that is hard to
    compare directly to accurate perspective projection photos.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 草图照片检索也被称为基于草图的图像检索（SBIR）[[1](#bib.bib1), [189](#bib.bib189), [4](#bib.bib4),
    [190](#bib.bib190), [191](#bib.bib191), [192](#bib.bib192), [193](#bib.bib193),
    [194](#bib.bib194), [195](#bib.bib195)]^(10)^(10)10注意，SBIR的常见设置是将草图作为图像查询模式，但大多数方法也允许使用其他模式作为查询（如有需要）。SBIR具有所有草图分析普遍存在的挑战（稀疏和抽象的输入）。特别是，由于将稀疏线条绘图与密集像素表示进行比较的难度，SBIR尤其具有挑战性，特别是当输入可能是非常抽象或图标式（符号性）表示时，这些表示很难直接与准确的透视投影照片进行比较。
- en: 'Figure [7](#S3.F7 "Figure 7 ‣ 3.2 Unique Challenges of Sketch Collection ‣
    3 Free-Hand Sketch Datasets ‣ Deep Learning for Free-Hand Sketch: A Survey") includes
    a taxonomy for SBIR. From the perspective of evaluation criterion, SBIR can be
    divided into conventional/coarse-grained SBIR (i.e., category-level SBIR), mid-grained [[196](#bib.bib196)],
    and fine-grained SBIR (i.e., instance-level SBIR). FG-SBIR is essentially a kind
    of instance-level retrieval [[197](#bib.bib197)]. From the perspective of retrieval
    embedding space, SBIR can be divided into common nearest-neighbor and fast hashing-based
    retrieval. From the perspective of supervision involved in training, SBIR can
    be divided into fully-supervised and zero-shot retrieval.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7](#S3.F7 "图 7 ‣ 3.2 草图收集的独特挑战 ‣ 3 自由手绘草图数据集 ‣ 自由手绘草图的深度学习：综述")包含了SBIR的分类。根据评估标准，SBIR可以分为传统/粗粒度SBIR（即类别级SBIR）、中粒度[[196](#bib.bib196)]和细粒度SBIR（即实例级SBIR）。FG-SBIR本质上是一种实例级检索[[197](#bib.bib197)]。从检索嵌入空间的角度来看，SBIR可以分为常见的最近邻检索和基于快速哈希的检索。从涉及训练的监督角度来看，SBIR可以分为完全监督和零样本检索。
- en: 'Category and Instance Level SBIR In coarse-grained SBIR, given a sketch as
    query, a ranked list of images is returned based on the similarity (e.g., Euclidean
    or Hamming distance). The retrieval is judged as correct, if the photo ranked
    at the top has the identical class label as the query. However, in fine-grained
    SBIR, the retrieval is judged as correct only when the returned photo is from
    the same *instance* pair as the query sketch. Figure [11](#S4.F11 "Figure 11 ‣
    4.1.4 Grouping, Segmentation, and Parsing ‣ 4.1 Uni-Modal Tasks: Pure Sketch Analysis
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")
    provides an illustration. Based on SBIR ideas, several sketch-based commodity
    search engines have been implemented, e.g., sketch-based skirt image retrieval [[198](#bib.bib198)],
    fine-grained sketch-based shoe [[4](#bib.bib4), [26](#bib.bib26)], chair [[4](#bib.bib4),
    [26](#bib.bib26)], and handbag [[26](#bib.bib26)] retrieval systems.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 类别和实例级SBIR 在粗粒度SBIR中，给定一个草图作为查询，返回的图像列表是根据相似度（例如欧几里得或汉明距离）排序的。如果排名靠前的照片与查询的类别标签相同，则检索被认为是正确的。然而，在细粒度SBIR中，检索被认为是正确的只有当返回的照片来自与查询草图相同的*实例*对时。图[11](#S4.F11
    "图 11 ‣ 4.1.4 分组、分割和解析 ‣ 4.1 单模态任务：纯草图分析 ‣ 4 任务和方法分类 ‣ 自由手绘草图的深度学习：综述")提供了一个示意图。基于SBIR的思想，已经实现了多个基于草图的商品搜索引擎，例如基于草图的裙子图像检索[[198](#bib.bib198)]、细粒度基于草图的鞋子[[4](#bib.bib4),
    [26](#bib.bib26)]、椅子[[4](#bib.bib4), [26](#bib.bib26)]和手袋[[26](#bib.bib26)]检索系统。
- en: Some previous SBIR works [[199](#bib.bib199), [200](#bib.bib200), [201](#bib.bib201)]
    have used edge-maps (image contours) of photos as an approximation to corresponding
    sketch images in order to perform matching. Canny edge detector [[202](#bib.bib202)],
    Edge Boxes toolbox [[203](#bib.bib203)], and holistically-nested edge detection
    (HED) [[204](#bib.bib204)] were usually used to extract the edges from natural
    photos. However, this kind of hand-designed process is now commonly replaced by
    end-to-end deep feature learning.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 一些早期的 SBIR 工作 [[199](#bib.bib199), [200](#bib.bib200), [201](#bib.bib201)] 使用了照片的边缘图（图像轮廓）作为与相应草图图像的近似，以进行匹配。Canny
    边缘检测器 [[202](#bib.bib202)]，Edge Boxes 工具箱 [[203](#bib.bib203)] 和全局嵌套边缘检测（HED） [[204](#bib.bib204)]
    通常用于从自然照片中提取边缘。然而，这种手工设计的过程现在通常被端到端的深度特征学习所取代。
- en: 'Deep sketch-based image retrieval (SBIR) has been widely studied [[205](#bib.bib205),
    [4](#bib.bib4), [7](#bib.bib7), [206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208),
    [209](#bib.bib209), [210](#bib.bib210), [211](#bib.bib211), [212](#bib.bib212),
    [213](#bib.bib213), [214](#bib.bib214), [215](#bib.bib215), [216](#bib.bib216),
    [217](#bib.bib217)] in recent years. Existing SBIR solutions generally aim to
    train a joint embedding space where sketch and photo can be compared using nearest
    neighbor techniques. Common embedding learning approaches include: (a) contrastive
    comparison based methods (implemented by pair-wise loss [[118](#bib.bib118)]),
    (b) ranking based methods [[4](#bib.bib4), [7](#bib.bib7)], (c) reinforcement
    learning based methods [[218](#bib.bib218)], (d) deep canonical correlation analysis
    (DCCA) [[219](#bib.bib219)] based methods [[220](#bib.bib220)], (e) cross-domain
    dictionary learning [[221](#bib.bib221)], etc. The most widely-studied methods
    are ranking-based, including triplet ranking [[209](#bib.bib209), [222](#bib.bib222),
    [223](#bib.bib223)] and quadruplet ranking [[224](#bib.bib224)].'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 深度基于草图的图像检索（SBIR）近年来得到了广泛的研究 [[205](#bib.bib205), [4](#bib.bib4), [7](#bib.bib7),
    [206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208), [209](#bib.bib209),
    [210](#bib.bib210), [211](#bib.bib211), [212](#bib.bib212), [213](#bib.bib213),
    [214](#bib.bib214), [215](#bib.bib215), [216](#bib.bib216), [217](#bib.bib217)]。现有的
    SBIR 解决方案通常旨在训练一个联合嵌入空间，其中草图和照片可以使用最近邻技术进行比较。常见的嵌入学习方法包括：（a）基于对比比较的方法（通过成对损失实现 [[118](#bib.bib118)]），（b）基于排序的方法 [[4](#bib.bib4),
    [7](#bib.bib7)]，（c）基于强化学习的方法 [[218](#bib.bib218)]，（d）基于深度典型相关分析（DCCA）的方法 [[219](#bib.bib219)] [[220](#bib.bib220)]，（e）跨领域字典学习 [[221](#bib.bib221)]，等等。最广泛研究的方法是基于排序的方法，包括三元组排序 [[209](#bib.bib209),
    [222](#bib.bib222), [223](#bib.bib223)] 和四元组排序 [[224](#bib.bib224)]。
- en: Ranking-Based SBIR We next introduce the popular triplet- and quadruplet-ranking
    SBIR methods in detail.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 基于排序的 SBIR 我们接下来详细介绍流行的三元组和四元组排序 SBIR 方法。
- en: '![Refer to caption](img/9dc6c60259281b3728442315b3fed58d.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9dc6c60259281b3728442315b3fed58d.png)'
- en: (a) triplet optimizing (single-modal)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: （a）三元组优化（单模态）
- en: '![Refer to caption](img/14bae4065aa233451526824c2ceb6c82.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/14bae4065aa233451526824c2ceb6c82.png)'
- en: (b) triplet optimizing (multi-modal)
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: （b）三元组优化（多模态）
- en: '![Refer to caption](img/0fce7f9ba4b556ae53c16235241d3d3c.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0fce7f9ba4b556ae53c16235241d3d3c.png)'
- en: (c) quadruplet optimizing (single-modal)
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: （c）四元组优化（单模态）
- en: '![Refer to caption](img/ac436965b1ffc88df34f63b0551085f5.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ac436965b1ffc88df34f63b0551085f5.png)'
- en: (d) quadruplet optimizing (multi-modal)
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: （d）四元组优化（多模态）
- en: 'Figure 12: Illustration of triplet and quadruplet ranking based optimization
    objectives. The lengths of solid arrows illustrate the distances in embedding
    spaces. In the quadruplet illustration, the “$negative,-$” sample denotes the
    negative sample from the anchor category, while the “$negative,--$” one denotes
    the negative sample from the remaining categories.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：三元组和四元组排序优化目标的示意图。实线箭头的长度表示嵌入空间中的距离。在四元组示意图中，“$negative,-$”样本表示来自锚点类别的负样本，而“$negative,--$”样本表示来自其他类别的负样本。
- en: 'As shown in Figure [12](#S4.F12 "Figure 12 ‣ 4.2.1 Sketch-Photo Retrieval ‣
    4.2 Multi-Modal Tasks: Sketch with Other Modalities ‣ 4 Tasks and Methodology
    Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey"), given a sketch anchor
    ${\bf X}_{n}$ and its positive and negative photo retrieval candidates (${\bf
    X}_{n,+}$, ${\bf X}_{n,-}$), the goal of triplet ranking is'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [12](#S4.F12 "Figure 12 ‣ 4.2.1 Sketch-Photo Retrieval ‣ 4.2 Multi-Modal
    Tasks: Sketch with Other Modalities ‣ 4 Tasks and Methodology Taxonomy ‣ Deep
    Learning for Free-Hand Sketch: A Survey") 所示，给定一个草图锚点 ${\bf X}_{n}$ 及其正例和负例照片检索候选（${\bf
    X}_{n,+}$，${\bf X}_{n,-}$），三元组排序的目标是'
- en: '|  | $\begin{split}\mathcal{D}(\mathcal{F}({\bf X}_{n}),\mathcal{F}({\bf X}_{n,+}))<\mathcal{D}(\mathcal{F}({\bf
    X}_{n}),\mathcal{F}({\bf X}_{n,-})).\end{split}$ |  | (1) |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{D}(\mathcal{F}({\bf X}_{n}),\mathcal{F}({\bf X}_{n,+}))<\mathcal{D}(\mathcal{F}({\bf
    X}_{n}),\mathcal{F}({\bf X}_{n,-})).\end{split}$ |  | (1) |'
- en: where $\mathcal{D}(\cdot,\cdot)$ is a distance metric (e.g., $\ell_{2}$ distance).
    In common FG-SBIR practice [[4](#bib.bib4), [209](#bib.bib209)], the negative
    sample is usually selected from the same class as the anchor.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{D}(\cdot,\cdot)$ 是一种距离度量（例如 $\ell_{2}$ 距离）。在常见的 FG-SBIR 实践中 [[4](#bib.bib4),
    [209](#bib.bib209)]，负样本通常从与锚点相同的类别中选择。
- en: 'For quadruplet ranking [[224](#bib.bib224)], the input atom is a quadruplet
    of anchor ${\bf X}_{n}$, positive candidate ${\bf X}_{n,+}$, negative candidate
    ${\bf X}_{n,-}$ from the class of anchor, negative candidate ${\bf X}_{n,--}$
    from a different class to anchor. As illustrated in Figure [12](#S4.F12 "Figure
    12 ‣ 4.2.1 Sketch-Photo Retrieval ‣ 4.2 Multi-Modal Tasks: Sketch with Other Modalities
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey"),
    the goal of quadruplet ranking is to ensure'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 对于四元组排名 [[224](#bib.bib224)]，输入的原子是一个包含锚点 ${\bf X}_{n}$、正样本 ${\bf X}_{n,+}$、来自锚点类别的负样本
    ${\bf X}_{n,-}$ 和来自不同类别的负样本 ${\bf X}_{n,--}$ 的四元组。如图 [12](#S4.F12 "图 12 ‣ 4.2.1
    草图-照片检索 ‣ 4.2 多模态任务：草图与其他模态 ‣ 4 任务和方法论分类 ‣ 深度学习手绘草图：综述") 所示，四元组排名的目标是确保
- en: '|  | $\begin{split}\mathcal{D}(\mathcal{F}({\bf X}_{n}),\mathcal{F}({\bf X}_{n,+}))<&amp;\mathcal{D}(\mathcal{F}({\bf
    X}_{n}),\mathcal{F}({\bf X}_{n,-}))\\ &amp;<\mathcal{D}(\mathcal{F}({\bf X}_{n}),\mathcal{F}({\bf
    X}_{n,--})).\end{split}$ |  | (2) |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{D}(\mathcal{F}({\bf X}_{n}),\mathcal{F}({\bf X}_{n,+}))<&amp;\mathcal{D}(\mathcal{F}({\bf
    X}_{n}),\mathcal{F}({\bf X}_{n,-}))\\ &amp;<\mathcal{D}(\mathcal{F}({\bf X}_{n}),\mathcal{F}({\bf
    X}_{n,--})).\end{split}$ |  | (2) |'
- en: Based on this, quadruplet ranking is essentially multi-task or multiple triplet
    ranking by constructing two extra triplet relationships, in order to encode more
    semantic information into the embedding space. For example, Seddati et al. [[224](#bib.bib224)]
    constructed three triplets from each quadruplet, including $triplet_{a}=\{{\bf
    X}_{n},{\bf X}_{n,+},{\bf X}_{n,-}\}$, $triplet_{b}=\{{\bf X}_{n},{\bf X}_{n,+},{\bf
    X}_{n,--}\}$ , and $triplet_{c}=\{{\bf X}_{n},{\bf X}_{n,-},{\bf X}_{n,--}\}$.
    Therefore, the quadruplet ranking loss is defined as
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，四元组排名本质上是通过构造两个额外的三元组关系来实现多任务或多个三元组排名，以便将更多语义信息编码到嵌入空间中。例如，Seddati 等人 [[224](#bib.bib224)]
    从每个四元组中构造了三个三元组，包括 $triplet_{a}=\{{\bf X}_{n},{\bf X}_{n,+},{\bf X}_{n,-}\}$、$triplet_{b}=\{{\bf
    X}_{n},{\bf X}_{n,+},{\bf X}_{n,--}\}$ 和 $triplet_{c}=\{{\bf X}_{n},{\bf X}_{n,-},{\bf
    X}_{n,--}\}$。因此，四元组排名损失定义为
- en: '|  | $\begin{split}\mathcal{L}_{quadruplet}=\mathcal{L}_{triplet_{a}}+\lambda_{b}\mathcal{L}_{triplet_{b}}+\lambda_{c}\mathcal{L}_{triplet_{c}},\end{split}$
    |  | (3) |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}_{quadruplet}=\mathcal{L}_{triplet_{a}}+\lambda_{b}\mathcal{L}_{triplet_{b}}+\lambda_{c}\mathcal{L}_{triplet_{c}},\end{split}$
    |  | (3) |'
- en: where $\lambda_{b}$, and $\lambda_{c}$ are the weights.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{b}$ 和 $\lambda_{c}$ 是权重。
- en: In ranking-based SBIR, the anchor is usually from the sketch domain, and other
    samples are photos. Both triplet-ranking and quadruplet-ranking can be used for
    either category-level or instance-level SBIR tasks.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于排名的 SBIR 中，锚点通常来自草图领域，其他样本则为照片。无论是三元组排名还是四元组排名，都可以用于类别级别或实例级别的 SBIR 任务。
- en: '![Refer to caption](img/ce9cbc1310540f3cc85a53109a575575.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ce9cbc1310540f3cc85a53109a575575.png)'
- en: 'Figure 13: Examples of SceneSketcher dataset [[78](#bib.bib78)], a fine-grained
    scene-level sketch dataset.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：SceneSketcher 数据集的示例 [[78](#bib.bib78)]，一个细粒度场景级草图数据集。
- en: 'Recently, Liu et al. released the first scene-level fine-grained SBIR dataset,
    i.e., SceneSketcher [[78](#bib.bib78)] (see Figure [13](#S4.F13 "Figure 13 ‣ 4.2.1
    Sketch-Photo Retrieval ‣ 4.2 Multi-Modal Tasks: Sketch with Other Modalities ‣
    4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")).
    This seminal work opens a novel direction for the future SBIR research, and contributes
    an effective solution to fine-grained scene sketch cross-modal matching that uses
    a GCN to encode the layout information of scene sketches in fine-grained triplet
    ranking.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，刘等人发布了第一个场景级细粒度 SBIR 数据集，即 SceneSketcher [[78](#bib.bib78)]（见图 [13](#S4.F13
    "图 13 ‣ 4.2.1 草图-照片检索 ‣ 4.2 多模态任务：草图与其他模态 ‣ 4 任务和方法论分类 ‣ 深度学习手绘草图：综述")）。这项开创性的工作为未来的
    SBIR 研究开辟了新的方向，并为细粒度场景草图跨模态匹配贡献了有效的解决方案，该方案使用 GCN 来编码细粒度三元组排名中的场景草图布局信息。
- en: 'Comment The essential principle of triplet loss is using local partial orderings
    to establish a global ordered relationship in the embedding space. Thus triplet
    ranking [[57](#bib.bib57)] can be understood as Topological Sorting^(11)^(11)11[https://en.wikipedia.org/wiki/Topological_sorting](https://en.wikipedia.org/wiki/Topological_sorting).
    The triplet annotations work as a partially ordered set. Compared with other loss
    functions, the main advantages of triplet loss are: (i) It helps to involve more
    local partial orderings and annotations to learn more fine-grained embedding space.
    (ii) Given a limited number of $N$ training samples, their triplet orderings have
    $C_{N}^{3}$ combinations, producing significant annotation augmentation. This
    is beneficial for training deeper networks on smaller sketch datasets. It should
    be noted that the performance of triplet loss is heavily dependent on (a) the
    choice of margin parameter and (b) the triplet construction strategy.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 评论 三元组损失的基本原理是利用局部部分排序来建立嵌入空间中的全局有序关系。因此，三元组排序[[57](#bib.bib57)]可以理解为拓扑排序^(11)^(11)11[https://en.wikipedia.org/wiki/Topological_sorting](https://en.wikipedia.org/wiki/Topological_sorting)。三元组标注作为一个部分有序集。与其他损失函数相比，三元组损失的主要优点是：（i）它有助于引入更多局部部分排序和标注，从而学习到更精细的嵌入空间。（ii）在有限数量的$N$训练样本下，其三元组排序有$C_{N}^{3}$种组合，产生显著的标注扩增。这对在较小的草图数据集上训练更深的网络非常有利。需要注意的是，三元组损失的性能严重依赖于（a）边际参数的选择和（b）三元组构造策略。
- en: Comment We remark that ranking-based SBIR models can also be improved by multi-task
    training along with classification [[7](#bib.bib7), [225](#bib.bib225), [226](#bib.bib226)].
    Furthermore, rather than purely discriminative training, SBIR can also be tackled
    by generating one modality from the other [[227](#bib.bib227)], e.g., using conditional
    GAN [[228](#bib.bib228)]; or using generative losses to regularize discriminative
    training [[229](#bib.bib229)]. SBIR training can also be combined with post-processing
    re-ranking [[199](#bib.bib199), [201](#bib.bib201), [230](#bib.bib230)] to refine
    the initially learned embedding spaces.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 评论 我们指出，基于排名的SBIR模型也可以通过多任务训练与分类[[7](#bib.bib7), [225](#bib.bib225), [226](#bib.bib226)]来改进。此外，SBIR不仅仅可以通过判别训练来解决，还可以通过从一种模态生成另一种模态[[227](#bib.bib227)]，例如，使用条件GAN[[228](#bib.bib228)]；或使用生成损失来规范判别训练[[229](#bib.bib229)]。SBIR训练还可以与后处理重排序[[199](#bib.bib199),
    [201](#bib.bib201), [230](#bib.bib230)]结合，以优化初步学习的嵌入空间。
- en: '![Refer to caption](img/dc8d08e55268dd420ae27cde12989ed6.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/dc8d08e55268dd420ae27cde12989ed6.png)'
- en: 'Figure 14: Different weight sharing manners (left: Siamese, middle: semi-heterogeneous,
    right: heterogeneous) for CNN-based cross-modal networks. The hollow and shaded
    networks denote the branches for sketches and photos, respectively. The double
    sided arrows indicate sharing of weights.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：CNN基础的跨模态网络中不同的权重共享方式（左：孪生网络，中：半异质，右：异质）。空心和阴影网络分别表示草图和照片的分支。双向箭头表示权重的共享。
- en: 'Network Architectures in SBIR SBIR models generally need two or more branches
    to process sketches and photos for comparison using the metrics introduced above.
    As shown in Figure [14](#S4.F14 "Figure 14 ‣ 4.2.1 Sketch-Photo Retrieval ‣ 4.2
    Multi-Modal Tasks: Sketch with Other Modalities ‣ 4 Tasks and Methodology Taxonomy
    ‣ Deep Learning for Free-Hand Sketch: A Survey"), both triplet and quadruplet
    ranking models can use backbone networks that are Siamese, semi-heterogeneous,
    or heterogeneous. (i) Siamese networks [[4](#bib.bib4)] use full weight-parameter
    sharing across branches. (ii) Semi-heterogeneous network [[231](#bib.bib231),
    [232](#bib.bib232)] use partial weight sharing across the branches. Typically
    early layers are modality-specific, and weight-tied layers are at deeper layers.
    (iii) In heterogeneous networks [[233](#bib.bib233)], the sketch branch uses independent
    parameters to the photo branches. The trade-offs underlying these architectures
    are that sharing weights enables more data (both sketch and photo) to be used
    to estimate parameters, reducing overfitting. But separating weights enables the
    sketch/photo branches to adapt more specifically to their respective domains.
    Weight sharing considerations are discussed in more detail in [[225](#bib.bib225)].'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 'SBIR 中的网络架构 SBIR 模型通常需要两个或更多分支来处理草图和照片，以进行比较，使用上述引入的度量标准。如图 [14](#S4.F14 "Figure
    14 ‣ 4.2.1 Sketch-Photo Retrieval ‣ 4.2 Multi-Modal Tasks: Sketch with Other Modalities
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")
    所示，三元组和四元组排名模型可以使用后台网络，这些网络可以是孪生网络、半异质网络或异质网络。（i）孪生网络 [[4](#bib.bib4)] 在各个分支之间使用完全的权重参数共享。（ii）半异质网络
    [[231](#bib.bib231), [232](#bib.bib232)] 在各个分支之间使用部分权重共享。通常，早期层是特定于模态的，而加权绑定层位于较深层次。（iii）在异质网络
    [[233](#bib.bib233)] 中，草图分支使用与照片分支独立的参数。这些架构的权衡在于，共享权重能够利用更多数据（包括草图和照片）来估计参数，减少过拟合。但分开权重使草图/照片分支能够更具体地适应其各自的领域。权重共享的考虑在
    [[225](#bib.bib225)] 中有更详细的讨论。'
- en: Hashing-Based SBIR In order to achieve faster sketch-based image retrieval,
    recent research has studied optimizing the feature coding (e.g., sketch-image
    hashing [[234](#bib.bib234), [235](#bib.bib235)]), and the feature map (e.g.,
    Asymmetric Feature Maps [[236](#bib.bib236)]).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 基于哈希的 SBIR 为了实现更快的基于草图的图像检索，近期的研究已经开始优化特征编码（例如，草图-图像哈希 [[234](#bib.bib234),
    [235](#bib.bib235)]）和特征图（例如，不对称特征图 [[236](#bib.bib236)]）。
- en: 'In particular, sketch-image hashing (or hashing SBIR) has gained attention.
    Liu et al. [[234](#bib.bib234)], propose the first deep hashing model for SBIR,
    which is a classic deep hashing pipeline including: (i) feature extractor network,
    (ii) hashing layer with binary constraints, and (iii) hashing loss. This classic
    pipeline has been widely studied in photo-oriented deep hashing [[237](#bib.bib237),
    [238](#bib.bib238)], where the hashing layer is typically fully-connected with
    sigmoid or tanh activation, and a discrete binary constraint. The loss functions
    of deep hashing models are often non-differentiable, due to the discrete binary
    constraints. Thus, common practice is that the feature extractor backbone and
    hashing layer are alternatively optimized in two separate steps by fixing one
    and optimizing the other.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，草图-图像哈希（或哈希 SBIR）已经引起了关注。Liu 等人 [[234](#bib.bib234)] 提出了第一个用于 SBIR 的深度哈希模型，这是一个经典的深度哈希流程，包括：（i）特征提取网络，（ii）带有二进制约束的哈希层，以及（iii）哈希损失。这个经典流程在以照片为导向的深度哈希中得到了广泛研究
    [[237](#bib.bib237), [238](#bib.bib238)]，其中哈希层通常是全连接的，使用 sigmoid 或 tanh 激活函数，并具有离散的二进制约束。由于离散的二进制约束，深度哈希模型的损失函数通常是不可微分的。因此，常见的做法是通过固定一个部分并优化另一个部分，以两步交替优化特征提取骨干网和哈希层。
- en: Existing SBIR hashing models work on the SBIR benchmarks, i.e., Sketchy [[7](#bib.bib7)]
    (75K sketches) and TU-Berlin Extended [[67](#bib.bib67)] (20K). The scale of these
    benchmarks is not yet large enough to thoroughly test hashing SBIR methods.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的 SBIR 哈希模型在 SBIR 基准测试上进行工作，即 Sketchy [[7](#bib.bib7)]（75K 草图）和 TU-Berlin
    Extended [[67](#bib.bib67)]（20K）。这些基准测试的规模尚不足以彻底测试哈希 SBIR 方法。
- en: 'Discussion Current issues in SBIR include: Self-supervised pre-training for
    SBIR [[239](#bib.bib239), [115](#bib.bib115)], optimizing SBIR for early retrieval
    using partially drawn sketches, for example using reinforcement learning [[218](#bib.bib218)];
    investigating whether costly sketch-photo annotation pairs can be replaced with
    edge-maps [[240](#bib.bib240)] and cross-category generalization of SBIR which
    is discussed next.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论 当前SBIR中的问题包括：用于SBIR的自监督预训练[[239](#bib.bib239), [115](#bib.bib115)]，例如使用强化学习[[218](#bib.bib218)]优化SBIR以进行早期检索；调查是否可以用边缘图替代昂贵的草图-照片标注对[[240](#bib.bib240)]以及接下来讨论的SBIR的跨类别泛化。
- en: 4.2.2 Zero-Shot SBIR
  id: totrans-374
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 零-Shot SBIR
- en: Many existing SBIR works assume that categories to be queried are included in
    the training set. In recent years, motivated by the zero-shot setting for supervised
    photo retrieval [[241](#bib.bib241)], zero-shot sketch-based image retrieval (ZS-SBIR)
    has also been studied [[242](#bib.bib242), [243](#bib.bib243), [244](#bib.bib244),
    [65](#bib.bib65), [245](#bib.bib245), [246](#bib.bib246), [247](#bib.bib247),
    [248](#bib.bib248), [249](#bib.bib249), [250](#bib.bib250), [251](#bib.bib251),
    [252](#bib.bib252), [216](#bib.bib216), [253](#bib.bib253), [254](#bib.bib254),
    [255](#bib.bib255)]. Similar to natural photo zero-shot learning/recognition [[256](#bib.bib256),
    [257](#bib.bib257), [258](#bib.bib258)], ZS-SBIR systems aim to enable query and
    retrieval of categories that are from unseen categories. i.e., categories that
    have not been involved in training stage. This is important in practice, e.g.,
    for an e-commerce application of SBIR, where new products should ideally be enrolled
    in the search engine without requiring re-training.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现有的SBIR（草图基础图像检索）工作假设要查询的类别已经包含在训练集中。近年来，受到监督下照片检索零-shot 设置的启发[[241](#bib.bib241)]，零-shot
    草图基础图像检索（ZS-SBIR）也得到了研究[[242](#bib.bib242), [243](#bib.bib243), [244](#bib.bib244),
    [65](#bib.bib65), [245](#bib.bib245), [246](#bib.bib246), [247](#bib.bib247),
    [248](#bib.bib248), [249](#bib.bib249), [250](#bib.bib250), [251](#bib.bib251),
    [252](#bib.bib252), [216](#bib.bib216), [253](#bib.bib253), [254](#bib.bib254),
    [255](#bib.bib255)]。类似于自然照片的零-shot 学习/识别[[256](#bib.bib256), [257](#bib.bib257),
    [258](#bib.bib258)]，ZS-SBIR 系统旨在使得查询和检索来自未见类别的类别成为可能，即那些在训练阶段未涉及的类别。这在实践中非常重要，例如，在SBIR的电子商务应用中，新产品应理想地能够在搜索引擎中被纳入，而不需要重新训练。
- en: 'Discussion ZS-SBIR systems can follow conventional zero-shot learning methods
    [[256](#bib.bib256), [257](#bib.bib257), [258](#bib.bib258)] in exploiting auxiliary
    knowledge such as word vectors [[259](#bib.bib259)], attributes [[260](#bib.bib260)],
    or class hierarchy to define the model for the unseen class. However, directly
    synthesizing a retrieval model for novel-classes with auxiliary knowledge leads
    to the same challenges of ZSL (cross-category domain-shift [[256](#bib.bib256)]
    and inconvenient need to specify nameable categories at testing-time [[256](#bib.bib256)]).
    Meanwhile, it would entail new challenges specific to SBIR: (i) Knowledge transfer
    needs to occur across both sketch and photo views. (ii) Some kinds of auxiliary
    knowledge may not make sense for sketch (e.g., *banana-is-yellow* may be visible
    in photo but not sketch). Meanwhile, auxiliary knowledge transfer is not strictly
    necessary for retrieval in the way that it is for category recognition. Therefore
    many ZS-SBIR methods tackle the problem in a *domain generalization* [[73](#bib.bib73)]
    manner. That is, training a matching network on the training categories that is
    robust enough to support direct application to unseen testing categories. Thus,
    common approaches are to train ranking [[65](#bib.bib65), [251](#bib.bib251)]
    or generative [[245](#bib.bib245), [246](#bib.bib246)] models for retrieval, which
    are enhanced and made robust by constraints such as domain-alignment losses [[65](#bib.bib65),
    [251](#bib.bib251), [245](#bib.bib245)] and auxiliary semantic knowledge reconstruction
    [[65](#bib.bib65), [245](#bib.bib245)]. In these cases the auxiliary semantic
    knowledge is only used to constrain representation learning at train time and
    is not required during testing time as for conventional ZSL – thus maintaining
    the vision that SBIR should only depend on ability to depict and not to verbally
    describe.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论 ZS-SBIR系统可以遵循传统的零样本学习方法 [[256](#bib.bib256), [257](#bib.bib257), [258](#bib.bib258)]，利用辅助知识如词向量
    [[259](#bib.bib259)]、属性 [[260](#bib.bib260)] 或类别层级来定义未见类别的模型。然而，直接为新类别合成检索模型与辅助知识相关会导致与ZSL相同的挑战（跨类别领域转移
    [[256](#bib.bib256)] 和在测试时需要指定可命名类别 [[256](#bib.bib256)]）。同时，这会带来SBIR特有的新挑战：（i）知识转移需要在草图和照片视图之间发生。（ii）某些类型的辅助知识可能对草图没有意义（例如，*香蕉是黄色的*
    在照片中可能可见，但在草图中不可见）。同时，辅助知识转移对于检索并不像类别识别那样严格必要。因此，许多ZS-SBIR方法以*领域泛化* [[73](#bib.bib73)]
    方式解决问题。也就是说，在训练类别上训练一个足够强大的匹配网络，以支持直接应用于未见的测试类别。因此，常见的方法是训练排名 [[65](#bib.bib65),
    [251](#bib.bib251)] 或生成 [[245](#bib.bib245), [246](#bib.bib246)] 模型进行检索，这些模型通过领域对齐损失
    [[65](#bib.bib65), [251](#bib.bib251), [245](#bib.bib245)] 和辅助语义知识重构 [[65](#bib.bib65),
    [245](#bib.bib245)] 等约束进行增强和稳健化。在这些情况下，辅助语义知识仅用于在训练时约束表示学习，而不像传统ZSL那样在测试时需要——从而保持SBIR只依赖于描绘能力而非语言描述的愿景。
- en: Current directions include extending SBIR to the generalized zero-shot setting,
    where testing categories are a mix of training and unseen categories [[251](#bib.bib251),
    [245](#bib.bib245)]; extending sketch-photo hashing to the zero-shot setting [[261](#bib.bib261)];
    and training SBIR without paired samples [[244](#bib.bib244)].
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 当前方向包括将SBIR扩展到广义的零样本设置，其中测试类别是训练类别和未见类别的混合 [[251](#bib.bib251), [245](#bib.bib245)];
    将草图-照片哈希扩展到零样本设置 [[261](#bib.bib261)]; 以及在没有配对样本的情况下训练SBIR [[244](#bib.bib244)]。
- en: 4.2.3 Sketch-Photo Generation
  id: totrans-378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 草图-照片生成
- en: 'TABLE V: Comparison of the representative pipelines of deep sketch-photo generation.
    “s $\rightarrow$ p” and “p $\rightarrow$ s” denote “sketch $\rightarrow$ photo”
    and “photo $\rightarrow$ sketch”, respectively. The backbones of the representative
    references here are implemented by CNNs.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 深度草图-照片生成的代表性管道比较。 “s $\rightarrow$ p”和“p $\rightarrow$ s”分别表示“草图 $\rightarrow$
    照片”和“照片 $\rightarrow$ 草图”。此处代表性参考文献的骨干由CNN实现。'
- en: '| Tasks | Pipelines | Representative Ref. | Sketch-Specific Designs | (Dis)Advantages
    |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 管道 | 代表性参考文献 | 草图特定设计 | (优)缺点 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| s $\rightarrow$ p | GAN | [[262](#bib.bib262), [263](#bib.bib263), [264](#bib.bib264)]
    | Sketch-specific designs are generally injected in the generators. |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| s $\rightarrow$ p | GAN | [[262](#bib.bib262), [263](#bib.bib263), [264](#bib.bib264)]
    | 草图特定设计通常注入生成器中。 |'
- en: '&#124; A: simple, end-to-end training &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A: 简单，端到端训练 &#124;'
- en: '&#124; D: suboptimal performance &#124;'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D: 次优性能 &#124;'
- en: '|'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| s $\rightarrow$ p | GAN #1 $\rightarrow$ GAN #2 | [[265](#bib.bib265), [266](#bib.bib266)]
    | GAN #1 for stroke refinement, GAN #2 for photo synthesis |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| s $\rightarrow$ p | GAN #1 $\rightarrow$ GAN #2 | [[265](#bib.bib265), [266](#bib.bib266)]
    | GAN #1 用于笔画细化，GAN #2 用于照片合成 |'
- en: '&#124; A: clear motivation &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A: 动机明确 &#124;'
- en: '&#124; D: multi-stage training &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D: 多阶段训练 &#124;'
- en: '|'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| s $\rightarrow$ p | CGAN | [[267](#bib.bib267), [268](#bib.bib268)] | Specific
    designs and domain knowledge can be injected as conditions. |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| s $\rightarrow$ p | CGAN | [[267](#bib.bib267), [268](#bib.bib268)] | 特定设计和领域知识可以作为条件注入。
    |'
- en: '&#124; A: clear motivation &#124;'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A: 动机明确 &#124;'
- en: '&#124; D: sensitive to conditions &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D: 对条件敏感 &#124;'
- en: '|'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| s $\rightarrow$ p | ContextualGAN | [[269](#bib.bib269)] | learns joint distribution
    of sketch-photo pairs |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| s $\rightarrow$ p | ContextualGAN | [[269](#bib.bib269)] | 学习草图-照片对的联合分布
    |'
- en: '&#124; A: appearance freedom, less strict alignment &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A: 外观自由度高，较少严格对齐 &#124;'
- en: '&#124; D: multi-stage training &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D: 多阶段训练 &#124;'
- en: '|'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| s $\rightarrow$ p | TextureGAN | [[270](#bib.bib270)] | supports local texture
    constraints |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| s $\rightarrow$ p | TextureGAN | [[270](#bib.bib270)] | 支持局部纹理约束 |'
- en: '&#124; A: more fine-grained &#124;'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A: 更精细的粒度 &#124;'
- en: '&#124; D: high complexity &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D: 高复杂度 &#124;'
- en: '|'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| p $\rightarrow$ s | CGAN | [[271](#bib.bib271)] | Specific designs and domain
    knowledge can be injected as conditions. |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| p $\rightarrow$ s | CGAN | [[271](#bib.bib271)] | 特定设计和领域知识可以作为条件注入。 |'
- en: '&#124; A: clear motivation &#124;'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A: 动机明确 &#124;'
- en: '&#124; D: sensitive to conditions &#124;'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D: 对条件敏感 &#124;'
- en: '|'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| p $\rightarrow$ s | conditional encoder-decoder | [[272](#bib.bib272)] |
    Conditions a convolutional decoder on a class prior. |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| p $\rightarrow$ s | 条件编码器-解码器 | [[272](#bib.bib272)] | 在类别先验上对卷积解码器进行条件处理。
    |'
- en: '&#124; A: simple, end-to-end training &#124;'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A: 简单，端到端训练 &#124;'
- en: '&#124; D: suboptimal performance &#124;'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D: 性能不佳 &#124;'
- en: '|'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| p $\rightarrow$ s | VAE (CNN encoder + RNN decoder) | [[55](#bib.bib55)]
    | shortcut cycle consistency, RNN decoder |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| p $\rightarrow$ s | VAE (CNN 编码器 + RNN 解码器) | [[55](#bib.bib55)] | 快捷循环一致性，RNN
    解码器 |'
- en: '&#124; A: stroke by stroke, end-to-end training &#124;'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A: 一步步进行，端到端训练 &#124;'
- en: '&#124; D: suboptimal for long strokes &#124;'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D: 对长笔画效果不佳 &#124;'
- en: '|'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Sketch and photo based mutual generation (translation/synthesis) is a classic
    cross-modal topic of sketch research covering both: (i) sketch-to-photo generation [[17](#bib.bib17),
    [273](#bib.bib273), [274](#bib.bib274)], (ii) photo-to-sketch generation [[55](#bib.bib55),
    [271](#bib.bib271), [275](#bib.bib275), [272](#bib.bib272)]. In particular, sketch-to-photo
    generation methods have addressed: (a) sketch to photo [[267](#bib.bib267)], (b)
    sketch & photo to photo [[22](#bib.bib22), [263](#bib.bib263)], (c) sketch/edge
    & color to photo [[58](#bib.bib58)]. Sketch and photo based generation can be
    used to help users to create or design novel images in various practical applications:
    sketch-based photo editing [[22](#bib.bib22), [263](#bib.bib263), [264](#bib.bib264)],
    sketch to painting generation [[276](#bib.bib276)], cloth design [[277](#bib.bib277),
    [278](#bib.bib278)], sketch to natural photo generation [[266](#bib.bib266)],
    etc. In some cases, sketch-photo generation also involves style transfer [[279](#bib.bib279),
    [268](#bib.bib268)].'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 基于草图和照片的互生（转换/合成）是草图研究中的经典跨模态课题，包括：(i) 草图到照片生成 [[17](#bib.bib17), [273](#bib.bib273),
    [274](#bib.bib274)]，(ii) 照片到草图生成 [[55](#bib.bib55), [271](#bib.bib271), [275](#bib.bib275),
    [272](#bib.bib272)]。特别是，草图到照片生成方法已解决：(a) 草图到照片 [[267](#bib.bib267)]，(b) 草图和照片到照片
    [[22](#bib.bib22), [263](#bib.bib263)]，(c) 草图/边缘和颜色到照片 [[58](#bib.bib58)]。基于草图和照片的生成可用于帮助用户创建或设计新图像于各种实际应用中：草图基础的照片编辑
    [[22](#bib.bib22), [263](#bib.bib263), [264](#bib.bib264)]，草图到绘画生成 [[276](#bib.bib276)]，服装设计
    [[277](#bib.bib277), [278](#bib.bib278)]，草图到自然照片生成 [[266](#bib.bib266)]，等。在某些情况下，草图-照片生成也涉及风格迁移
    [[279](#bib.bib279), [268](#bib.bib268)]。
- en: 'Note that: (i) Sketch-to-photo generation aims to solve cross-modal translation
    from abstract and sparse line drawings to pixel space, different to well-drawn
    sketch colorization [[280](#bib.bib280), [281](#bib.bib281), [282](#bib.bib282)].
    (ii) Photo-to-sketch generation does not refer to extracting edge-map from natural
    photos [[203](#bib.bib203), [283](#bib.bib283)] (edge-maps of literal perspective
    projections), but instead needs models that learn to mimic human sketching and
    abstract drawing style [[55](#bib.bib55)].'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：(i) 草图到照片的生成旨在解决从抽象和稀疏的线条绘图到像素空间的跨模态翻译，与良好绘制的草图着色不同[[280](#bib.bib280), [281](#bib.bib281),
    [282](#bib.bib282)]。 (ii) 照片到草图的生成并不是指从自然照片中提取边缘图[[203](#bib.bib203), [283](#bib.bib283)]（字面透视投影的边缘图），而是需要学习模仿人类草图和抽象绘画风格的模型[[55](#bib.bib55)]。
- en: 'Sketch and photo generation methods have been widely studied [[284](#bib.bib284),
    [285](#bib.bib285), [262](#bib.bib262), [265](#bib.bib265), [186](#bib.bib186)]
    based on various GAN [[121](#bib.bib121)] variants including conditional GAN [[286](#bib.bib286)],
    cycle GAN [[284](#bib.bib284)], and texture GAN [[270](#bib.bib270)]. Meanwhile,
    VAE also works well for sketch-photo generation. We compare the existing deep
    sketch-photo generation pipelines and their representative models in Table [V](#S4.T5
    "TABLE V ‣ 4.2.3 Sketch-Photo Generation ‣ 4.2 Multi-Modal Tasks: Sketch with
    Other Modalities ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand
    Sketch: A Survey"). The domain gap from sketch to photo is large. As demonstrated
    in Figure [15](#S4.F15 "Figure 15 ‣ 4.2.3 Sketch-Photo Generation ‣ 4.2 Multi-Modal
    Tasks: Sketch with Other Modalities ‣ 4 Tasks and Methodology Taxonomy ‣ Deep
    Learning for Free-Hand Sketch: A Survey"), an intuitive idea is to decompose the
    large gap into two smaller gaps. Some previous works [[265](#bib.bib265), [266](#bib.bib266)]
    proposed to use two GANs to achieve sketch-to-photo generation in two steps: (i)
    use the first GAN to refine rough sketches, e.g., generating good contours [[266](#bib.bib266)],
    and (ii) input the refined sketches into the second GAN to generate the target
    photos.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '草图和照片生成方法已经被广泛研究[[284](#bib.bib284), [285](#bib.bib285), [262](#bib.bib262),
    [265](#bib.bib265), [186](#bib.bib186)]，基于各种生成对抗网络（GAN）[[121](#bib.bib121)]的变体，包括条件GAN[[286](#bib.bib286)]、循环GAN[[284](#bib.bib284)]和纹理GAN[[270](#bib.bib270)]。同时，变分自编码器（VAE）在草图-照片生成中也表现良好。我们在表[V](#S4.T5
    "TABLE V ‣ 4.2.3 Sketch-Photo Generation ‣ 4.2 Multi-Modal Tasks: Sketch with
    Other Modalities ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand
    Sketch: A Survey")中比较了现有的深度草图-照片生成管道及其代表性模型。草图到照片的领域差距很大。如图[15](#S4.F15 "Figure
    15 ‣ 4.2.3 Sketch-Photo Generation ‣ 4.2 Multi-Modal Tasks: Sketch with Other
    Modalities ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch:
    A Survey")所示，一个直观的想法是将大的差距分解为两个较小的差距。一些早期工作[[265](#bib.bib265), [266](#bib.bib266)]建议使用两个GAN在两个步骤中实现草图到照片的生成：（i）使用第一个GAN来细化粗略的草图，例如，生成好的轮廓[[266](#bib.bib266)]，并且（ii）将细化的草图输入到第二个GAN中以生成目标照片。'
- en: '![Refer to caption](img/fe6b532e744683041400aaee8a3bcca6.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fe6b532e744683041400aaee8a3bcca6.png)'
- en: 'Figure 15: Illustration of two-stage two-GAN based sketch-to-photo cross-modal
    generation idea [[265](#bib.bib265), [266](#bib.bib266)]. The arrow lengths denote
    the distances of cross-domain gap. See text for details.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：基于两个阶段和两个GAN的草图到照片跨模态生成理念的示意图[[265](#bib.bib265), [266](#bib.bib266)]。箭头的长度表示跨领域差距的距离。详细信息见正文。
- en: Discussion Sketch-photo cross-modal generation is distinctively different from
    conventional photo-to-photo translation [[285](#bib.bib285)]. Existing photo-to-photo
    models can assume pixel-wise alignment between inputs and outputs. However, this
    requirement is strongly violated in the case of sketches and photos. Indeed no
    simple warping can provide pixel-wise alignment between sketch and photo given
    the potentially abstract or iconic nature of sketches. Thus, existing sketch-photo
    synthesis work has made efforts to work around this issue, e.g., using contextual
    GAN [[269](#bib.bib269)].
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论草图与照片的跨模态生成明显不同于传统的照片到照片翻译[[285](#bib.bib285)]。现有的照片到照片模型可以假设输入和输出之间的像素级对齐。然而，在草图和照片的情况下，这种要求被强烈违反。确实，考虑到草图的潜在抽象或图标性质，没有简单的变形可以在草图和照片之间提供像素级对齐。因此，现有的草图-照片合成工作已经努力绕过这个问题，例如，使用上下文生成对抗网络（GAN）[[269](#bib.bib269)]。
- en: Discussion Generating sketch images as pixels suffers from the problem that
    blurriness in an image that should be made of sharp edges is very visible. However,
    an important difference between sketch-photo translation and photo-photo translation
    is that the raw format of sketch data is often a time-series of way-points. If
    such raw sketch representation is to be used, the encoder or decoder should be
    an RNN rather than CNN. The first example of such sequential vectorized photo-sketch
    translation is in [[55](#bib.bib55)], where sharp sketches are sequentially produced
    using a recurrent decoder.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：将草图图像生成像素会遇到一个问题，即图像中的模糊度在应由锐利边缘构成的图像中非常明显。然而，草图-照片翻译和照片-照片翻译之间的重要区别在于，草图数据的原始格式通常是时间序列的途径点。如果要使用这种原始草图表示，编码器或解码器应该是
    RNN 而不是 CNN。这种序列化的向量化照片-草图翻译的第一个例子见于 [[55](#bib.bib55)]，其中使用递归解码器逐步生成锐利的草图。
- en: Discussion Other current considerations are similar to that in image-image translation
    including building sketch-photo translation models that work based on unpaired
    samples.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：其他当前考虑因素与图像-图像翻译相似，包括基于无配对样本的草图-照片翻译模型的构建。
- en: Discussion Sketch inpainting is an interesting task related to sketch generation,
    with a wide range of practical applications in engineering [[287](#bib.bib287),
    [288](#bib.bib288)], medicine [[288](#bib.bib288)], agriculture [[289](#bib.bib289),
    [288](#bib.bib288), [290](#bib.bib290)], etc. This task can be considered as a
    single-modal generation task from deteriorated sketch (with discontinuities strokes)
    to a restored complete sketch. e.g., old sketches [[33](#bib.bib33)] and engineering
    sketches [[287](#bib.bib287)]. Sketch inpainting is also studied for generating/parsing
    the sketch structures from non-sketch images (e.g., retina [[288](#bib.bib288)],
    plant roots [[289](#bib.bib289), [288](#bib.bib288), [290](#bib.bib290)], road
    networks [[288](#bib.bib288)] from satellites), which is a cross-modal photo-to-sketch
    generation task. How to solve this problem in end-to-end framework is still challenging
    and under-studied.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：草图修复是一项与草图生成相关的有趣任务，具有广泛的实际应用，包括工程 [[287](#bib.bib287), [288](#bib.bib288)]、医学 [[288](#bib.bib288)]、农业 [[289](#bib.bib289),
    [288](#bib.bib288), [290](#bib.bib290)] 等。这个任务可以看作是一个单模态生成任务，从退化的草图（带有中断的笔触）到恢复的完整草图。例如，旧草图 [[33](#bib.bib33)]
    和工程草图 [[287](#bib.bib287)]。草图修复也被研究用于从非草图图像中生成/解析草图结构（例如，视网膜 [[288](#bib.bib288)]、植物根系 [[289](#bib.bib289),
    [288](#bib.bib288), [290](#bib.bib290)]、卫星图像中的道路网络 [[288](#bib.bib288)]），这是一项跨模态的照片到草图生成任务。在端到端框架中解决这个问题仍然具有挑战性且研究不足。
- en: 4.2.4 Sketch-3D Retrieval
  id: totrans-423
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 草图-3D 检索
- en: Sketch-3D retrieval refers to using sketch as query to retrieve 3D models [[291](#bib.bib291)].
    Compared to SBIR, 3D retrieval is more challenging due to the larger domain gap
    between 2D sketch and 3D model.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 草图-3D 检索指的是使用草图作为查询来检索3D模型 [[291](#bib.bib291)]。与 SBIR 相比，由于 2D 草图和 3D 模型之间存在更大的领域差距，3D
    检索更具挑战性。
- en: 'Sketch-3D retrieval was studied well before the deep learning era [[47](#bib.bib47)].
    Classic approaches often proceeded in a two-stage manner [[292](#bib.bib292)]:
    (i) View selection: Use an automatic procedure to select representative viewpoints
    of a given 3D model, hoping that one of the selected viewpoints is similar to
    that of the query sketches; and (ii) Projection and Matching: Project each selected
    view of the 3D model into 2D space by line rendering algorithm [[293](#bib.bib293)].
    Then match the sketch against the 2D projections of the model based on pre-defined
    features such as SIFT [[120](#bib.bib120)]. See Figure [16](#S4.F16 "Figure 16
    ‣ 4.2.4 Sketch-3D Retrieval ‣ 4.2 Multi-Modal Tasks: Sketch with Other Modalities
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")
    for an illustration. However, as argued in some previous work [[2](#bib.bib2)],
    view selection is a bottleneck of the two-stage approach as the “best” views are
    subjective and ambiguous. Moreover, matching based on hand-crafted features is
    inaccurate.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '在深度学习时代之前，草图-3D检索已经得到了很好的研究 [[47](#bib.bib47)]。经典方法通常分为两个阶段 [[292](#bib.bib292)]：
    (i) 视图选择：使用自动程序选择给定3D模型的代表性视点，希望选择的一个视点与查询草图的视点相似； (ii) 投影与匹配：通过线渲染算法将每个选定的3D模型视图投影到2D空间中 [[293](#bib.bib293)]。然后基于预定义特征如
    SIFT [[120](#bib.bib120)]，将草图与模型的2D投影进行匹配。有关说明，请参见图 [16](#S4.F16 "Figure 16 ‣
    4.2.4 Sketch-3D Retrieval ‣ 4.2 Multi-Modal Tasks: Sketch with Other Modalities
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")。然而，正如一些之前的工作所指出的 [[2](#bib.bib2)]，视图选择是两阶段方法的瓶颈，因为“最佳”视图是主观和模糊的。此外，基于手工制作特征的匹配是不准确的。'
- en: '![Refer to caption](img/fac1e00d76750d5150ff390309d8d0f8.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fac1e00d76750d5150ff390309d8d0f8.png)'
- en: 'Figure 16: Illustration of view matching across sketch and 3D shape. Images
    (from left to right: sketch, 3D shape, three random views of the 3D shape) are
    selected from [[294](#bib.bib294)].'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：草图与3D形状之间视图匹配的示例。图像（从左到右：草图、3D形状、3D形状的三个随机视图）选自 [[294](#bib.bib294)]。
- en: Gradually, sketch based 3D model retrieval has been studied within the end-to-end
    deep learning paradigm [[2](#bib.bib2), [295](#bib.bib295), [296](#bib.bib296),
    [294](#bib.bib294), [297](#bib.bib297), [298](#bib.bib298), [299](#bib.bib299)].
    As a representative method, Wang et al. [[2](#bib.bib2)] proposed to use two Siamese
    networks to learn the sketch and projected views directly in the end-to-end manner,
    which takes a quadruplet of sketches and projected viewpoints as input, and uses
    multiple pair-wise losses. In each quadruplet, two sketches (${\bf X}_{1}$, ${\bf
    X}_{2}$) and two viewpoints (${\bf V}_{1}$, ${\bf V}_{2}$) are randomly selected
    from sketch and 3D domains, respectively. For simplicity they assume that ${\bf
    X}_{1}$ and ${\bf X}_{2}$ are from the same category sharing a Siamese network,
    while ${\bf V}_{1}$ and ${\bf V}_{2}$ are also from the same category sharing
    another Siamese network. The quadruplet loss is
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 渐进地，基于草图的3D模型检索在端到端深度学习范式中得到了研究 [[2](#bib.bib2), [295](#bib.bib295), [296](#bib.bib296),
    [294](#bib.bib294), [297](#bib.bib297), [298](#bib.bib298), [299](#bib.bib299)]。作为一种代表性方法，Wang
    等人 [[2](#bib.bib2)] 提出了使用两个孪生网络直接在端到端的方式中学习草图和投影视图，该方法以草图和投影视点的四元组作为输入，并使用多个成对损失。在每个四元组中，从草图和3D领域中分别随机选择两个草图
    (${\bf X}_{1}$, ${\bf X}_{2}$) 和两个视点 (${\bf V}_{1}$, ${\bf V}_{2}$)。为了简化，他们假设
    ${\bf X}_{1}$ 和 ${\bf X}_{2}$ 来自共享一个孪生网络的相同类别，而 ${\bf V}_{1}$ 和 ${\bf V}_{2}$
    也来自共享另一个孪生网络的相同类别。四元组损失为：
- en: '|  | $\begin{split}\mathcal{L}({\bf X}_{1},{\bf X}_{2},&amp;{\bf V}_{1},{\bf
    V}_{2})=\mathcal{L}_{pair}({\bf X}_{1},{\bf X}_{2})\\ &amp;+\mathcal{L}_{pair}({\bf
    V}_{1},{\bf V}_{2})+\mathcal{L}_{pair}({\bf X}_{1},{\bf V}_{1}),\end{split}$ |  |
    (4) |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}({\bf X}_{1},{\bf X}_{2},&amp;{\bf V}_{1},{\bf
    V}_{2})=\mathcal{L}_{pair}({\bf X}_{1},{\bf X}_{2})\\ &amp;+\mathcal{L}_{pair}({\bf
    V}_{1},{\bf V}_{2})+\mathcal{L}_{pair}({\bf X}_{1},{\bf V}_{1}),\end{split}$ |  |
    (4) |'
- en: 'The loss function terms $\mathcal{L}_{pair}({\bf X}_{1},{\bf X}_{2})$ and $\mathcal{L}_{pair}({\bf
    V}_{1},{\bf V}_{2})$ enable the network to learn category-level similarity within
    each domain; while the $\mathcal{L}_{pair}({\bf X}_{1},{\bf V}_{1})$ term forces
    the network to learn cross-modal similarity. Given input samples $a$ and $b$,
    the pair-wise loss function is defined as:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数项 $\mathcal{L}_{pair}({\bf X}_{1},{\bf X}_{2})$ 和 $\mathcal{L}_{pair}({\bf
    V}_{1},{\bf V}_{2})$ 使得网络能够在每个领域内学习类别级相似性；而 $\mathcal{L}_{pair}({\bf X}_{1},{\bf
    V}_{1})$ 项则强制网络学习跨模态相似性。给定输入样本 $a$ 和 $b$，成对损失函数定义为：
- en: '|  | $\begin{split}\mathcal{L}_{pair}(a,b)=\left\{\begin{aligned} \alpha{\mathcal{D}(\mathcal{F}_{a}(a),\mathcal{F}_{b}(b))},&amp;~{}~{}\textrm{
    if }y_{a}\neq y_{b},\\ \beta e^{\gamma\mathcal{D}(\mathcal{F}_{a}(a),\mathcal{F}_{b}(b))},&amp;~{}~{}\textrm{
    otherwise },\end{aligned}\right.\end{split}$ |  | (5) |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}_{pair}(a,b)=\left\{\begin{aligned} \alpha{\mathcal{D}(\mathcal{F}_{a}(a),\mathcal{F}_{b}(b))},&amp;~{}~{}\textrm{
    如果 }y_{a}\neq y_{b},\\ \beta e^{\gamma\mathcal{D}(\mathcal{F}_{a}(a),\mathcal{F}_{b}(b))},&amp;~{}~{}\textrm{
    否则 },\end{aligned}\right.\end{split}$ |  | (5) |'
- en: where $y_{a}$ and $y_{b}$ are the corresponding class labels, and $\mathcal{F}_{a}(\cdot)$
    and $\mathcal{F}_{b}(\cdot)$ denote the feature extractions that have been applied
    to $a$ and $b$, respectively.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $y_{a}$ 和 $y_{b}$ 是相应的类别标签，$\mathcal{F}_{a}(\cdot)$ 和 $\mathcal{F}_{b}(\cdot)$
    分别表示应用于 $a$ 和 $b$ 的特征提取。
- en: Besides this pair-wise deep metric learning, other deep metric learning methods
    also can be applied to Sketch-3D matching, e.g., triplet ranking [[300](#bib.bib300),
    [301](#bib.bib301)] and deep correlation metric learning [[302](#bib.bib302),
    [303](#bib.bib303)].
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这种对偶深度度量学习，其他深度度量学习方法也可以应用于草图-3D匹配，例如三元组排序 [[300](#bib.bib300), [301](#bib.bib301)]
    和深度相关度量学习 [[302](#bib.bib302), [303](#bib.bib303)]。
- en: Moreover, some previous works also studied how to represent 3D models more comprehensively
    in sketch based 3D retrieval tasks. For instance, Xie et al. [[304](#bib.bib304)]
    proposed to represent 3D models by computing the Wasserstein distance [[305](#bib.bib305)]
    based barycenters of multiple projections of 3D models.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些早期的研究也探讨了如何在基于草图的3D检索任务中更全面地表示3D模型。例如，Xie 等人 [[304](#bib.bib304)] 提出了通过计算3D模型多个投影的
    Wasserstein 距离 [[305](#bib.bib305)] 基于的质心来表示3D模型。
- en: 4.2.5 Sketch-3D Generation
  id: totrans-435
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5 草图-3D生成
- en: 'Sketch to 3D model generation is also an interesting cross-modal research topic
    analogous to the sketch-to-image generation discussed in Section [4.2.3](#S4.SS2.SSS3
    "4.2.3 Sketch-Photo Generation ‣ 4.2 Multi-Modal Tasks: Sketch with Other Modalities
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey").
    Using sketch to generate 3D models/shapes [[306](#bib.bib306), [307](#bib.bib307)]
    is extremely challenging but has important applications such as sketch-based product
    design [[308](#bib.bib308), [309](#bib.bib309)]. Compared to the other tasks discussed,
    this is relatively under-studied thus far. Most of the existing deep learning
    based sketch-to-3D generation models are engineered for highly well-drawn or professional
    pencil sketches [[310](#bib.bib310), [311](#bib.bib311)]. Recently, 3D-to-sketch [[312](#bib.bib312)]
    generation has also been explored in a deep learning manner.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 从草图到3D模型生成也是一个有趣的跨模态研究主题，类似于第 [4.2.3](#S4.SS2.SSS3 "4.2.3 草图-照片生成 ‣ 4.2 多模态任务：草图与其他模态
    ‣ 4 任务和方法论分类 ‣ 深度学习中的自由手绘草图：综述") 节中讨论的草图到图像生成。使用草图生成3D模型/形状 [[306](#bib.bib306),
    [307](#bib.bib307)] 极具挑战性，但具有重要应用，如基于草图的产品设计 [[308](#bib.bib308), [309](#bib.bib309)]。与其他讨论过的任务相比，这个领域至今相对较少研究。目前，大多数现有的基于深度学习的草图到3D生成模型主要针对高质量绘制或专业铅笔草图
    [[310](#bib.bib310), [311](#bib.bib311)]。最近，3D到草图 [[312](#bib.bib312)] 的生成也在深度学习方法中进行了探索。
- en: 'DeepSketchHair [[308](#bib.bib308)] is a representative deep model that generates
    realistic 3D hairstyle models from 2D sketches. As shown in Figure [17](#S4.F17
    "Figure 17 ‣ 4.2.5 Sketch-3D Generation ‣ 4.2 Multi-Modal Tasks: Sketch with Other
    Modalities ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch:
    A Survey"), given a 3D bust model as a reference, the system takes in a user-drawn
    sketch (consisting of hair contour (red lines) and a few strokes indicating the
    hair growing directions (blue lines) within a hair region), and automatically
    generates a 3D hair model, which matches the input sketch both globally (for contour)
    and locally (for growing directions). This model solves two challenging cross-domain
    mappings: (i) mapping sketch to dense 2D hair orientation field, by S2ONet, and
    (ii) mapping 2D orientation field to 3D vector field, by O2VNet.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSketchHair [[308](#bib.bib308)] 是一个代表性的深度模型，它从2D草图生成逼真的3D发型模型。如图 [17](#S4.F17
    "图17 ‣ 4.2.5 草图-3D生成 ‣ 4.2 多模态任务：草图与其他模态 ‣ 4 任务和方法论分类 ‣ 深度学习中的自由手绘草图：综述") 所示，给定一个3D半身模型作为参考，系统接受用户绘制的草图（包括头发轮廓（红线）和指示头发生长方向的几条线（蓝线）），并自动生成一个3D头发模型，该模型在全局（轮廓）和局部（生长方向）上与输入草图匹配。该模型解决了两个具有挑战性的跨领域映射：（i）通过
    S2ONet 将草图映射到密集的2D头发方向场，（ii）通过 O2VNet 将2D方向场映射到3D矢量场。
- en: '![Refer to caption](img/5aac119384c030cbeed554eceafb174c.png)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5aac119384c030cbeed554eceafb174c.png)'
- en: 'Figure 17: Pipeline of a sketch-based 3D hair modeling deep model [[308](#bib.bib308)].
    This system takes 2D sketch as input and generates a realistic 3D hairstyle.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：基于草图的3D发型建模深度模型的流程图[[308](#bib.bib308)]。该系统将2D草图作为输入，生成逼真的3D发型。
- en: 4.2.6 Sketch-Video Retrieval
  id: totrans-440
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.6 草图-视频检索
- en: Sketch based video retrieval (SBVR) has been studied [[313](#bib.bib313)] prior
    to contemporary deep learning. SBVR is also highly challenging due to the huge
    domain gap between free-hand sketch and video. In SBVR applications using sketch
    as query has the advantage that humans can use lines or arrow vectors to describe
    moving or other dynamic scenes. Thus, sketch can be used to not only depict static
    objects and scenes, but also motion information.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 基于草图的视频检索（SBVR）在当代深度学习之前已经得到了研究。SBVR由于自由手绘草图与视频之间的巨大领域差距，也具有很高的挑战性。在使用草图作为查询的SBVR应用中，人们可以使用线条或箭头向量来描述运动或其他动态场景。因此，草图不仅可以用来描绘静态对象和场景，还可以用于表示运动信息。
- en: Performing SBVR using sketches conveying both appearance and motion is challenging
    due to the need to segment the motion and appearance information from the sketch
    and use it to address the corresponding channels in the video gallery for retrieval.
    To address this challenge, a multi-stream multi-modality deep network was proposed
    in  [[314](#bib.bib314)]. This study further extended SBVR to fine-grained SBVR
    to perform instance-level retrieval of videos given sketch queries.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 使用草图进行基于草图的视频检索（SBVR）涉及同时传达外观和运动，这具有挑战性，因为需要将运动和外观信息从草图中分离出来，并利用这些信息来访问视频库中的相应频道进行检索。为了应对这一挑战，提出了一种多流多模态深度网络[[314](#bib.bib314)]。这项研究进一步将SBVR扩展为细粒度SBVR，以实现根据草图查询进行实例级视频检索。
- en: Finally, we note that, motion sketch based crowd video retrieval  [[315](#bib.bib315),
    [316](#bib.bib316)] has been studied recently, which is useful for video surveillance
    analysis.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们注意到，基于运动草图的人群视频检索[[315](#bib.bib315), [316](#bib.bib316)]最近得到了研究，这对于视频监控分析非常有用。
- en: Discussion Which kinds of videos are appropriate and feasible for sketch-based
    retrieval is still an open question. As are human-computer-interaction questions
    around how time, shot change, and motions should be depicted in sketch.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论哪些类型的视频适合且可行于基于草图的检索仍然是一个悬而未决的问题。关于如何在草图中描绘时间、镜头变化和运动的人机交互问题也是如此。
- en: 4.2.7 Other Sketch-Related Multi-Modal Tasks
  id: totrans-445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.7 其他与草图相关的多模态任务
- en: Recently some other interesting sketch based multi-modal tasks have emerged,
    e.g., text-to-sketch generation [[317](#bib.bib317)] (e.g., text instruction based
    conversational authoring of sketches [[318](#bib.bib318)]), sketch-based photo
    classifier generation [[20](#bib.bib20)], sketch-based segmentation model generation
    [[319](#bib.bib319)], sketch-based pictionary games [[320](#bib.bib320), [54](#bib.bib54),
    [131](#bib.bib131)], sketch to photo contour transfer [[321](#bib.bib321)], and
    sketch-guided object localization in natural photos [[322](#bib.bib322)].
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 最近出现了一些有趣的基于草图的多模态任务，例如，文本到草图生成[[317](#bib.bib317)]（例如，基于文本指令的对话草图创作[[318](#bib.bib318)]），基于草图的照片分类器生成[[20](#bib.bib20)]，基于草图的分割模型生成[[319](#bib.bib319)]，基于草图的猜词游戏[[320](#bib.bib320),
    [54](#bib.bib54), [131](#bib.bib131)]，草图到照片轮廓转移[[321](#bib.bib321)]，以及自然照片中的草图引导物体定位[[322](#bib.bib322)]。
- en: 4.3 Experimental Comparison
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 实验比较
- en: 4.3.1 Representing Sketch
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 草图表示
- en: 'As discussed in Section [2.1](#S2.SS1 "2.1 Intrinsic Traits and Domain-Unique
    Challenges ‣ 2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey"), sketch
    can be represented in several diverse formats such as raster image, and waypoint
    sequence. These representations lend themselves to different neural network architectures.
    We therefore take the opportunity to use TorchSketch to perform the first thorough
    comparison of neural network architectures for sketch recognition/representing.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 正如第[2.1](#S2.SS1 "2.1 内在特征与领域独特挑战 ‣ 2 背景 ‣ 深度学习用于自由手绘草图：综述")节中讨论的，草图可以以多种不同的格式表示，例如光栅图像和关键点序列。这些表示形式适用于不同的神经网络架构。因此，我们借此机会使用TorchSketch对草图识别/表示的神经网络架构进行首次彻底比较。
- en: 'TABLE VI: Comparison of recognition accuracy for different network architectures
    on a subset [[23](#bib.bib23)] of QuickDraw [[5](#bib.bib5)].'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 表VI：在QuickDraw[[23](#bib.bib23)]的一个子集上，不同网络架构的识别精度比较[[5](#bib.bib5)]。
- en: '| Architecture & Network | Input | Recognition Accuracy | Parameter Amount
    |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 架构与网络 | 输入 | 识别精度 | 参数数量 |'
- en: '| acc.@1 | acc.@5 | acc.@10 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| acc.@1 | acc.@5 | acc.@10 |'
- en: '| Convolutional Neural Networks (CNNs) | AlexNet [[105](#bib.bib105)] | picture
    | 0.6808 | 0.8847 | 0.9203 | 58,417,305 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| Convolutional Neural Networks (CNNs) | AlexNet [[105](#bib.bib105)] | picture
    | 0.6808 | 0.8847 | 0.9203 | 58,417,305 |'
- en: '| VGG-11 [[179](#bib.bib179)] | 0.6743 | 0.8814 | 0.9191 | 130,179,801 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| VGG-11 [[179](#bib.bib179)] | 0.6743 | 0.8814 | 0.9191 | 130,179,801 |'
- en: '| VGG-13 [[179](#bib.bib179)] | 0.6808 | 0.8881 | 0.9232 | 130,364,313 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| VGG-13 [[179](#bib.bib179)] | 0.6808 | 0.8881 | 0.9232 | 130,364,313 |'
- en: '| VGG-16 [[179](#bib.bib179)] | 0.6837 | 0.8889 | 0.9253 | 135,674,009 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| VGG-16 [[179](#bib.bib179)] | 0.6837 | 0.8889 | 0.9253 | 135,674,009 |'
- en: '| VGG-19 [[179](#bib.bib179)] | 0.6908 | 0.8839 | 0.9208 | 140,983,705 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| VGG-19 [[179](#bib.bib179)] | 0.6908 | 0.8839 | 0.9208 | 140,983,705 |'
- en: '| Inception V3 [[323](#bib.bib323)] | 0.7422 | 0.9189 | 0.9437 | 25,315,474
    |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| Inception V3 [[323](#bib.bib323)] | 0.7422 | 0.9189 | 0.9437 | 25,315,474
    |'
- en: '| ResNet-18 [[108](#bib.bib108)] | 0.7164 | 0.9072 | 0.9381 | 11,353,497 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-18 [[108](#bib.bib108)] | 0.7164 | 0.9072 | 0.9381 | 11,353,497 |'
- en: '| ResNet-34 [[108](#bib.bib108)] | 0.7154 | 0.9083 | 0.9375 | 21,461,657 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-34 [[108](#bib.bib108)] | 0.7154 | 0.9083 | 0.9375 | 21,461,657 |'
- en: '| ResNet-50 [[108](#bib.bib108)] | 0.7043 | 0.8987 | 0.9303 | 24,214,937 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-50 [[108](#bib.bib108)] | 0.7043 | 0.8987 | 0.9303 | 24,214,937 |'
- en: '| ResNet-101 [[108](#bib.bib108)] | 0.7071 | 0.8992 | 0.9317 | 43,207,065 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-101 [[108](#bib.bib108)] | 0.7071 | 0.8992 | 0.9317 | 43,207,065 |'
- en: '| ResNet-152 [[108](#bib.bib108)] | 0.6924 | 0.8973 | 0.9312 | 58,850,713 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-152 [[108](#bib.bib108)] | 0.6924 | 0.8973 | 0.9312 | 58,850,713 |'
- en: '| DenseNet-161 [[324](#bib.bib324)] | 0.7008 | 0.8971 | 0.9302 | 27,234,105
    |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| DenseNet-161 [[324](#bib.bib324)] | 0.7008 | 0.8971 | 0.9302 | 27,234,105
    |'
- en: '| DenseNet-169 [[324](#bib.bib324)] | 0.7173 | 0.9050 | 0.9358 | 13,058,905
    |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| DenseNet-169 [[324](#bib.bib324)] | 0.7173 | 0.9050 | 0.9358 | 13,058,905
    |'
- en: '| DenseNet-201 [[324](#bib.bib324)] | 0.7050 | 0.9013 | 0.9331 | 18,755,673
    |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| DenseNet-201 [[324](#bib.bib324)] | 0.7050 | 0.9013 | 0.9331 | 18,755,673
    |'
- en: '| MobileNet V2 [[325](#bib.bib325)] | 0.7310 | 0.9161 | 0.9429 | 2,665,817
    |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet V2 [[325](#bib.bib325)] | 0.7310 | 0.9161 | 0.9429 | 2,665,817
    |'
- en: '| Recurrent Neural Networks (RNNs) | LSTM | stroke vector | 0.6068 | 0.8416
    | 0.8931 | 2,593,881 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| Recurrent Neural Networks (RNNs) | LSTM | stroke vector | 0.6068 | 0.8416
    | 0.8931 | 2,593,881 |'
- en: '| Bi-directional LSTM | 0.6665 | 0.8820 | 0.9189 | 5,553,241 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| Bi-directional LSTM | 0.6665 | 0.8820 | 0.9189 | 5,553,241 |'
- en: '|  | GRU | 0.6224 | 0.8574 | 0.9055 | 2,000,473 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '|  | GRU | 0.6224 | 0.8574 | 0.9055 | 2,000,473 |'
- en: '|  | Bi-directional GRU | 0.6768 | 0.8854 | 0.9234 | 5,419,097 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '|  | Bi-directional GRU | 0.6768 | 0.8854 | 0.9234 | 5,419,097 |'
- en: '| Graph Neural Networks (GNNs) | Graph Convolutional Network (GCN) [[326](#bib.bib326)]
    | stroke vector | 0.6800 | 0.8869 | 0.9224 | 6,948,441 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| Graph Neural Networks (GNNs) | Graph Convolutional Network (GCN) [[326](#bib.bib326)]
    | stroke vector | 0.6800 | 0.8869 | 0.9224 | 6,948,441 |'
- en: '| Graph Attention Network (GAT) [[327](#bib.bib327)] | 0.6977 | 0.8952 | 0.9298
    | 11,660,889 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| Graph Attention Network (GAT) [[327](#bib.bib327)] | 0.6977 | 0.8952 | 0.9298
    | 11,660,889 |'
- en: '| Vanilla Transformer [[328](#bib.bib328)] | 0.5249 | 0.7802 | 0.8486 | 14,029,401
    |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| Vanilla Transformer [[328](#bib.bib328)] | 0.5249 | 0.7802 | 0.8486 | 14,029,401
    |'
- en: '| Multi-Graph Transformer (Base) [[23](#bib.bib23)] | 0.7070 | 0.9030 | 0.9351
    | 10,096,601 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| Multi-Graph Transformer (Base) [[23](#bib.bib23)] | 0.7070 | 0.9030 | 0.9351
    | 10,096,601 |'
- en: '| Multi-Graph Transformer (Large) [[23](#bib.bib23)] | 0.7280 | 0.9106 | 0.9387
    | 39,984,729 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| Multi-Graph Transformer (Large) [[23](#bib.bib23)] | 0.7280 | 0.9106 | 0.9387
    | 39,984,729 |'
- en: '| Textual Convolutional Network (TCN) | TCN [[111](#bib.bib111)] | stroke vector
    | 0.5511 | 0.8020 | 0.8646 | 2,750,873 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| Textual Convolutional Network (TCN) | TCN [[111](#bib.bib111)] | stroke vector
    | 0.5511 | 0.8020 | 0.8646 | 2,750,873 |'
- en: To this end we follow [[23](#bib.bib23)] in using 414K sketches drawn from QuickDraw.
    These are organized into training, validation, and test splits composed of 1000,
    100, and 100 sketches respectively from each of the 345 QuickDraw categories.
    Following [[19](#bib.bib19)], we truncate or pad sketch samples to a uniform length
    of 100 key points/steps to facilitate efficient training of RNN-, GNN-, and TCN-based
    models, where each time-step is a $4D$ input (i.e., two coordinates and two pen
    state bits). Sketch recognition is a fundamental topic within the field, and the
    QuickDraw sketches are subject to realistic abstraction, noise, and drawing diversity.
    We therefore hope that this benchmark can help practitioners while supporting
    future research in the field.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们遵循 [[23](#bib.bib23)]使用从 QuickDraw 获得的 414K 草图。这些草图被组织为训练、验证和测试集，每个 QuickDraw
    类别中分别包含 1000、100 和 100 张草图。根据 [[19](#bib.bib19)]，我们将草图样本截断或填充到统一的长度，即 100 个关键点/步骤，以便有效训练基于
    RNN、GNN 和 TCN 的模型，其中每个时间步长是一个 $4D$ 输入（即两个坐标和两个笔状态位）。草图识别是该领域的基础课题，QuickDraw 草图具有现实的抽象性、噪声和绘图多样性。因此，我们希望这个基准可以帮助从业者，同时支持未来在该领域的研究。
- en: 'The recognition accuracy across these architectures, as implemented by TorchSketch,
    are reported in Table [VI](#S4.T6 "TABLE VI ‣ 4.3.1 Representing Sketch ‣ 4.3
    Experimental Comparison ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for
    Free-Hand Sketch: A Survey"). We can analyze these results with comparisons within
    and across architecture categories. Within each architecture, we can observe from
    Table [VI](#S4.T6 "TABLE VI ‣ 4.3.1 Representing Sketch ‣ 4.3 Experimental Comparison
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey"):
    (i) For CNNs, the deeper networks (e.g., DenseNet-161) have no obvious advantage
    compared to the shallower networks (e.g., AlexNet, VGG). This is likely due to
    the sparsity of sketch where redundant convolution and pooling operations lose
    information about the sparse pixels. (ii) For RNNs, bidirectional networks outperform
    the unidirectional networks by a clear margin (0.66+ vs. 0.60+). This makes sense
    as human sketch ordering is only loosely consistent [[133](#bib.bib133)]. (iii)
    For GNNs, multi-graph transformer (MGT) outperforms graph convolutional network (GCN)
    and graph attention network (GAT).'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '这些架构的识别准确率由 TorchSketch 实现，如表格[VI](#S4.T6 "TABLE VI ‣ 4.3.1 Representing Sketch
    ‣ 4.3 Experimental Comparison ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning
    for Free-Hand Sketch: A Survey")所示。我们可以通过在架构类别内外进行比较来分析这些结果。在每个架构中，我们可以从表格[VI](#S4.T6
    "TABLE VI ‣ 4.3.1 Representing Sketch ‣ 4.3 Experimental Comparison ‣ 4 Tasks
    and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")观察到：(i)
    对于 CNN，较深的网络（例如 DenseNet-161）与较浅的网络（例如 AlexNet、VGG）相比没有明显优势。这可能是因为素描稀疏性导致冗余的卷积和池化操作丢失了关于稀疏像素的信息。(ii)
    对于 RNN，双向网络明显优于单向网络（0.66+ 对比 0.60+）。这很有道理，因为人类素描的顺序仅 loosely 一致 [[133](#bib.bib133)]。(iii)
    对于 GNN，多图变换器 (MGT) 优于图卷积网络 (GCN) 和图注意力网络 (GAT)。'
- en: 'Across the architectures, we can observe: (i) Thus far the best CNN networks
    (InceptionV3) outperform the best sequential networks. This may be because the
    sequential networks (RNN, GNN, and TCN) truncate the input coordinate sequences.
    (ii) Among GNNs, the multi-graph transformer [[23](#bib.bib23)] comes closest
    to matching peak CNN performance. (iii) Compared with CNNs and GNNs, RNNs and
    TCN have significantly fewer parameters. However (iv) TCN, performs unsatisfactorily
    in this fully-supervised setting.'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些架构中，我们可以观察到：(i) 迄今为止，最佳的 CNN 网络（InceptionV3）优于最佳的序列网络。这可能是因为序列网络（RNN、GNN
    和 TCN）截断了输入坐标序列。(ii) 在 GNN 中，多图变换器 [[23](#bib.bib23)] 最接近于匹配 CNN 的最佳性能。(iii) 与
    CNN 和 GNN 相比，RNN 和 TCN 参数显著较少。然而 (iv) TCN 在这种完全监督设置下表现不佳。
- en: 'TABLE VII: Robustness of ResNet-18 CNN backbone to perturbations in the form
    of random spatial transformations.'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：ResNet-18 CNN 主干对随机空间变换形式的扰动的鲁棒性。
- en: Each setting is tested $10$ times, and mean ($\%$) and standard deviation of
    performance are reported.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 每个设置测试了 $10$ 次，报告了性能的均值 ($\%$) 和标准差。
- en: '| Spatial Transform | Recognition on TU-Berlin |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 空间变换 | 在 TU-Berlin 上的识别率 |'
- en: '&#124; Coarse-Grained SBIR &#124;'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 粗粒度 SBIR &#124;'
- en: '&#124; on TU-Berlin Extended &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在 TU-Berlin 扩展数据集上 &#124;'
- en: '|'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fine-Grained SBIR &#124;'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 细粒度 SBIR &#124;'
- en: '&#124; on QMUL Shoe &#124;'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在 QMUL 鞋子数据集上 &#124;'
- en: '|'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| acc.@1 | acc.@5 | acc.@10 | rank@1 | rank@5 | rank@10 | mAP | rank@1 | rank@5
    | rank@10 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| acc.@1 | acc.@5 | acc.@10 | rank@1 | rank@5 | rank@10 | mAP | rank@1 | rank@5
    | rank@10 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| shift | 67.16$\pm$0.31 | 88.20$\pm$0.24 | 92.82$\pm$0.32 | 51.17$\pm$0.36
    | 71.04$\pm$0.31 | 77.33$\pm$0.21 | 28.91$\pm$0.05 | 21.13$\pm$1.42 | 50.17$\pm$2.42
    | 67.39$\pm$2.10 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| shift | 67.16$\pm$0.31 | 88.20$\pm$0.24 | 92.82$\pm$0.32 | 51.17$\pm$0.36
    | 71.04$\pm$0.31 | 77.33$\pm$0.21 | 28.91$\pm$0.05 | 21.13$\pm$1.42 | 50.17$\pm$2.42
    | 67.39$\pm$2.10 |'
- en: '| scale | 44.07$\pm$0.80 | 68.49$\pm$0.62 | 77.08$\pm$0.48 | 31.24$\pm$0.33
    | 51.02$\pm$0.56 | 59.06$\pm$0.50 | 15.75$\pm$0.24 | 8.78$\pm$2.15 | 25.74$\pm$2.10
    | 39.65$\pm$3.02 |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| scale | 44.07$\pm$0.80 | 68.49$\pm$0.62 | 77.08$\pm$0.48 | 31.24$\pm$0.33
    | 51.02$\pm$0.56 | 59.06$\pm$0.50 | 15.75$\pm$0.24 | 8.78$\pm$2.15 | 25.74$\pm$2.10
    | 39.65$\pm$3.02 |'
- en: '| horizontal flip | 70.60$\pm$0.22 | 89.58$\pm$0.12 | 93.88$\pm$0.14 | 52.30$\pm$0.13
    | 72.05$\pm$0.21 | 78.32$\pm$0.20 | 30.12$\pm$0.04 | 16.08$\pm$2.85 | 44.00$\pm$2.63
    | 58.61$\pm$2.70 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| 水平翻转 | 70.60$\pm$0.22 | 89.58$\pm$0.12 | 93.88$\pm$0.14 | 52.30$\pm$0.13
    | 72.05$\pm$0.21 | 78.32$\pm$0.20 | 30.12$\pm$0.04 | 16.08$\pm$2.85 | 44.00$\pm$2.63
    | 58.61$\pm$2.70 |'
- en: '| vertical flip | 50.44$\pm$0.74 | 70.77$\pm$0.59 | 78.00$\pm$0.36 | 36.25$\pm$0.33
    | 52.57$\pm$0.42 | 58.32$\pm$0.32 | 20.75$\pm$0.11 | 13.04$\pm$1.88 | 40.35$\pm$3.18
    | 56.17$\pm$3.36 |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| 垂直翻转 | 50.44$\pm$0.74 | 70.77$\pm$0.59 | 78.00$\pm$0.36 | 36.25$\pm$0.33
    | 52.57$\pm$0.42 | 58.32$\pm$0.32 | 20.75$\pm$0.11 | 13.04$\pm$1.88 | 40.35$\pm$3.18
    | 56.17$\pm$3.36 |'
- en: '| rotation | 44.34$\pm$0.69 | 68.38$\pm$0.44 | 76.83$\pm$0.50 | 32.84$\pm$0.66
    | 50.14$\pm$0.48 | 57.14$\pm$0.73 | 17.94$\pm$0.21 | 6.35$\pm$2.39 | 17.48$\pm$2.64
    | 27.92$\pm$3.19 |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| 旋转 | 44.34$\pm$0.69 | 68.38$\pm$0.44 | 76.83$\pm$0.50 | 32.84$\pm$0.66 |
    50.14$\pm$0.48 | 57.14$\pm$0.73 | 17.94$\pm$0.21 | 6.35$\pm$2.39 | 17.48$\pm$2.64
    | 27.92$\pm$3.19 |'
- en: '| None | 72.06 | 90.1 | 94.02 | 53.12 | 73.14 | 78.94 | 30.62 | 20.00 | 53.91
    | 68.70 | ![Refer to caption](img/bcf56d4872e63755e27d853433a1b244.png)'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '| 无 | 72.06 | 90.1 | 94.02 | 53.12 | 73.14 | 78.94 | 30.62 | 20.00 | 53.91
    | 68.70 | ![参见标题](img/bcf56d4872e63755e27d853433a1b244.png)'
- en: (a) Recognition on TU-Berlin
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: (a) TU-Berlin上的识别
- en: '![Refer to caption](img/a5da2c875aeb91abb5df98bf85b5b0be.png)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a5da2c875aeb91abb5df98bf85b5b0be.png)'
- en: (b) SBIR on TU-Berlin Extended
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: (b) TU-Berlin Extended上的SBIR
- en: '![Refer to caption](img/e658be96c9dcee0f3d2973c407335a09.png)'
  id: totrans-502
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e658be96c9dcee0f3d2973c407335a09.png)'
- en: (c) Fine-Grained SBIR on QMUL Shoe
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: (c) QMUL Shoe上的细粒度SBIR
- en: 'Figure 18: Boxplots for robustness evaluation on spatial transformations.'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：空间变换下的稳健性评估箱线图。
- en: 4.3.2 Robustness Study on Spatial Transformation
  id: totrans-505
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 空间变换的稳健性研究
- en: 'As discussed in Section [2.1](#S2.SS1 "2.1 Intrinsic Traits and Domain-Unique
    Challenges ‣ 2 Background ‣ Deep Learning for Free-Hand Sketch: A Survey"), even
    if sketches are shifted, rescaled, rotated, or flipped, they still can be recognized
    easily by people. It will be interesting to evaluate how sensitive the current
    deep networks are to the spatial transformations on different sketch tasks and
    datasets.'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在第[2.1节](#S2.SS1 "2.1 内在特性与领域特有挑战 ‣ 2 背景 ‣ 深度学习用于手绘草图：综述")中讨论的，即使草图被移位、缩放、旋转或翻转，人们仍然能够轻松识别。评估当前深度网络对不同草图任务和数据集的空间变换的敏感性将是很有趣的。
- en: To facilitate comparison, we choose three quantitatively comparable tasks as
    target tasks involving both single-modal and multi-modal settings, i.e., sketch
    recognition, coarse-grained SBIR, fine-grained SBIR, which will be respectively
    conducted on three commonly used datasets, i.e., TU-Berlin, TU-Berlin Extended,
    QMUL Shoe. To make it more intuitive, we choose a CNN backbone, i.e., ResNet-18.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于比较，我们选择三个定量可比的任务作为目标任务，包括单模态和多模态设置，即草图识别、粗粒度SBIR、细粒度SBIR，这些任务将分别在三个常用数据集上进行，即TU-Berlin、TU-Berlin
    Extended、QMUL Shoe。为了更直观，我们选择了CNN骨干网络，即ResNet-18。
- en: 'Cross-entropy loss and triplet loss are used for recognition and SBIR tasks,
    respectively. To perform early-stopping and select models based on validation
    performance, we need to split the datasets: (i) For TU-Berlin, $40$, $20$, and
    $20$ sketches per category are randomly selected for training, validation, and
    testing, respectively. (ii) For TU-Berlin Extended, both sketches and photos are
    randomly divided into training, validation, and testing sets as a ratio of $2:1:1$.
    (iii) For QMUL Shoe, $50$ sketch-photo pairs are selected randomly from its training
    set for validation. To fully verify the sensitivity, we did not adopt any data
    augmentations in the training stage.'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失和三元组损失分别用于识别和SBIR任务。为了进行早期停止并根据验证性能选择模型，我们需要拆分数据集：（i）对于TU-Berlin，每个类别随机选择$40$、$20$和$20$个草图用于训练、验证和测试。（ii）对于TU-Berlin
    Extended，草图和照片随机分为训练、验证和测试集，比例为$2:1:1$。（iii）对于QMUL Shoe，从其训练集中随机选择$50$对草图-照片对用于验证。为了充分验证敏感性，我们在训练阶段没有采用任何数据增强。
- en: 'We choose five representative randomly spatial transformations for robustness
    testing, including position shift, scale, horizontal flip ($p=0.5$), vertical
    flip ($p=0.5$), and center rotation ($-45^{\circ}$ to $45^{\circ}$). Considering
    the randomness, testing was repeated $10$ times with each spatial transformation,
    and both mean ($\%$) and standard deviation for each indicator are reported in
    Table [VII](#S4.T7 "TABLE VII ‣ 4.3.1 Representing Sketch ‣ 4.3 Experimental Comparison
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey"),
    where top-K accuracy and rank accuracy are used as metrics for recognition and
    retrieval. Moreover, mean average precision (mAP) is also reported to evaluate
    the performance for coarse-grained SBIR as multiple true matches are provided
    by gallery. For a clear comparison, the testing results without any spatial transformations
    are provided in the bottom row of Table [VII](#S4.T7 "TABLE VII ‣ 4.3.1 Representing
    Sketch ‣ 4.3 Experimental Comparison ‣ 4 Tasks and Methodology Taxonomy ‣ Deep
    Learning for Free-Hand Sketch: A Survey"), while we draw boxplots (Figure [18](#S4.F18
    "Figure 18 ‣ 4.3.1 Representing Sketch ‣ 4.3 Experimental Comparison ‣ 4 Tasks
    and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")) based
    on “acc.@1”, “mAP”, and “rank@1” for the selected tasks, respectively.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '我们选择了五种具有代表性的随机空间变换进行鲁棒性测试，包括位置偏移、尺度变化、水平翻转 ($p=0.5$)、垂直翻转 ($p=0.5$) 和中心旋转
    ($-45^{\circ}$ 到 $45^{\circ}$)。考虑到随机性，每种空间变换的测试重复了 $10$ 次，并报告了每个指标的均值 ($\%$) 和标准差，如表
    [VII](#S4.T7 "TABLE VII ‣ 4.3.1 Representing Sketch ‣ 4.3 Experimental Comparison
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")
    所示，其中 top-K 准确率和排名准确率用作识别和检索的度量指标。此外，还报告了均值平均精度（mAP）以评估粗粒度 SBIR 的性能，因为画廊提供了多个真实匹配。为了清晰对比，表
    [VII](#S4.T7 "TABLE VII ‣ 4.3.1 Representing Sketch ‣ 4.3 Experimental Comparison
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")
    底部行中提供了没有任何空间变换的测试结果，同时我们基于“acc.@1”、“mAP”和“rank@1”绘制了箱线图（图 [18](#S4.F18 "Figure
    18 ‣ 4.3.1 Representing Sketch ‣ 4.3 Experimental Comparison ‣ 4 Tasks and Methodology
    Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey)")。'
- en: 'In Table [VII](#S4.T7 "TABLE VII ‣ 4.3.1 Representing Sketch ‣ 4.3 Experimental
    Comparison ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch:
    A Survey"), we observed that: (i) Deep sketch models are vulnerable to the spatial
    transformations that can result in performance degradation. (ii) Compared with
    shift and horizontal flip, the three other transformations cause more noticeable
    performance changes. In particular, for TU-Berlin sketches, horizontal flip causes
    very small performance changes. (iii) On QMUL Shoe, spatial transformation based
    perturbations have relatively large standard deviations. Moreover, it is interesting
    to see that shift transformation slightly improves fine-grained SBIR accuracy.
    This is also demonstrated by Figure [18](#S4.F18 "Figure 18 ‣ 4.3.1 Representing
    Sketch ‣ 4.3 Experimental Comparison ‣ 4 Tasks and Methodology Taxonomy ‣ Deep
    Learning for Free-Hand Sketch: A Survey"). This is likely due to the small sample
    size (only $115$ sketch-photo pairs for testing).'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [VII](#S4.T7 "TABLE VII ‣ 4.3.1 Representing Sketch ‣ 4.3 Experimental Comparison
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")
    中，我们观察到：（i）深度素描模型易受到空间变换的影响，从而导致性能下降。（ii）与位移和水平翻转相比，另外三种变换会导致更明显的性能变化。特别是对于 TU-Berlin
    素描，水平翻转导致的性能变化非常小。（iii）在 QMUL 鞋类数据集上，基于空间变换的扰动具有相对较大的标准差。此外，值得注意的是，位移变换略微提高了细粒度
    SBIR 的准确性，这也由图 [18](#S4.F18 "Figure 18 ‣ 4.3.1 Representing Sketch ‣ 4.3 Experimental
    Comparison ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch:
    A Survey") 所示。这可能是由于样本量小（仅有 $115$ 对素描-照片配对用于测试）。'
- en: These observations indicate that sketch based spatial transformations are able
    to attack deep networks. The current deep learning technique still needs to be
    improved to achieve sketch-oriented robustness.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观察结果表明，基于素描的空间变换能够攻击深度网络。目前的深度学习技术仍需改进，以实现素描导向的鲁棒性。
- en: 5 Discussion
  id: totrans-512
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: 'TABLE VIII: Comparison of deep learning and traditional approaches to sketch-related
    tasks.'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VIII：深度学习与传统方法在素描相关任务中的比较。
- en: '| Method | Advantages | Disadvantages |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 优点 | 缺点 |'
- en: '| Deep Learning | relatively superior performance | manually-designed network
    structures |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | 相对优越的性能 | 手动设计的网络结构 |'
- en: '| end-to-end mapping | more parameters/resource cost |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| 端到端映射 | 更多的参数/资源成本 |'
- en: '| larger model capacity to support big data | vulnerable to overly fit |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| 更大的模型容量以支持大数据 | 易于过拟合 |'
- en: '|  | more training data brings higher performance in general | less rigorous
    in mathematical expression and interpretability |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '|  | 更多训练数据通常带来更高性能 | 数学表达和可解释性较差 |'
- en: '| Traditional | clear in mathematical expression and interpretability | relatively
    suboptimal performance |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| 传统方法 | 数学表达和可解释性清晰 | 性能相对次优 |'
- en: '| fewer parameters/resource | manually-designed features |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| 参数/资源较少 | 手动设计特征 |'
- en: '| less over-fitting | under-parameterized for big data |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| 较少过拟合 | 对大数据的参数不足 |'
- en: 'TABLE IX: Comparison of different deep network architectures for sketch-oriented
    tasks.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IX: 不同深度网络架构在草图导向任务中的比较。'
- en: '| Arc. | Input | Model Space | Motivations | Advantages | Disadvantages |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 输入 | 模型空间 | 动机 | 优点 | 缺点 |'
- en: '| CNNs | picture | spatial | imitate human reception field | good performance
    in global-level tasks | weak in stroke-level tasks |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| CNNs | 图片 | 空间 | 模仿人类接收领域 | 在全局任务中表现良好 | 在笔画级任务中表现差 |'
- en: '| (full sketch) | (Euclidean) | regard sketch as binary pixel matrix | perceive
    full sketch (no information lost) | fail in temporal tasks |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| （完整草图） | （欧几里得） | 将草图视为二值像素矩阵 | 感知完整草图（无信息丢失） | 时间任务表现差 |'
- en: '|  |  |  | can stack deeper layers | relatively more parameters |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 可以堆叠更深的层次 | 参数相对更多 |'
- en: '|  |  |  | allow a variety of perception granularities | vulnerable to overly
    fit |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 允许多种感知粒度 | 易过拟合 |'
- en: '| RNNs | stroke vector | spatial&temporal | imitate human sketching process
    | recurrently capture stroke temporal patterns | weak in stacking deeper layers
    |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| RNNs | 笔画向量 | 空间&时间 | 模仿人类草图过程 | 反复捕捉笔画时间模式 | 深层堆叠能力弱 |'
- en: '| (key points) |  |  | relatively fewer parameters | weak in long stroke sequences
    |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| （关键点） |  |  | 参数相对较少 | 长笔画序列表现差 |'
- en: '|  |  |  |  | perception granularity is fixed |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | 感知粒度固定 |'
- en: '| GNNs + Transformers | stroke vector | spatial&temporal | represent sketch
    as graph | higher flexibility in network design | weak in stacking deeper layers
    |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| GNNs + Transformers | 笔画向量 | 空间&时间 | 将草图表示为图 | 网络设计灵活性高 | 深层堆叠能力弱 |'
- en: '| (key points) |  | (key point $\rightarrow$ node, | relatively fewer parameters
    | abstract network structure |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| （关键点） |  | （关键点 $\rightarrow$ 节点， | 参数相对较少 | 抽象网络结构 |'
- en: '|  |  | stroke $\rightarrow$ node, etc) | can handle long stroke sequences
    |  |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 笔画 $\rightarrow$ 节点等 | 可以处理长笔画序列 |  |'
- en: '| TCNs | stroke vector | spatial&temporal | imitate temporal reception field
    | concise network architecture | weak in stacking deeper layers |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| TCNs | 笔画向量 | 空间&时间 | 模仿时间接收领域 | 网络架构简洁 | 深层堆叠能力弱 |'
- en: '| (key points) |  | (key point $\rightarrow$ word, | relatively fewer parameters
    | fail to recurrently perceive strokes |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| （关键点） |  | （关键点 $\rightarrow$ 单词， | 参数相对较少 | 无法反复感知笔画 |'
- en: '|  |  | stroke $\rightarrow$ sentence) | allow a variety of perception granularities
    |  |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 笔画 $\rightarrow$ 句子) | 允许多种感知粒度 |  |'
- en: 5.1 Open Problems
  id: totrans-537
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 未解问题
- en: 5.1.1 Deep Learning vs. Traditional Methods
  id: totrans-538
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 深度学习与传统方法
- en: In recent years, deep learning methods have achieved the state-of-the-art  in
    all the sketch tasks. However, some open problems still need further study, e.g.,
    (i) What are the advantages and disadvantages of deep learning and traditional
    methods on sketch? (ii) Why do deep learning networks work well on sketch? (iii)
    How are sketch-unique characteristics modelled by various deep learning networks?
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习方法在所有草图任务中已达到**最先进的水平**。然而，一些未解问题仍需进一步研究，例如，(i) 深度学习和传统方法在草图上的优缺点是什么？
    (ii) 为什么深度学习网络在草图上表现良好？ (iii) 草图独特特征如何通过各种深度学习网络建模？
- en: 'Generally traditional methods work in two stages, i.e., feature engineering
    from sketch space to feature space, mapping from feature space to target space,
    while generally deep learning methods do end-to-end mapping directly from sketch
    space to target space. Thus the essential opportunity for exploiting human insight
    is in hand-designing network structures vs. hand-designing features. Due to the
    sketch domain challenges (e.g., abstract, noisy, sparse, diverse), the difficulty
    of feature engineering for sketch is one of the main bottlenecks of the traditional
    methods. This is somewhat ameliorated by deep learning methods which have the
    capacity to learn strong feature representations given sufficient data to model
    their variability and diversity. Furthermore, deep learning networks have various
    motivations and mechanisms to handle the sketch-unique characteristics: (i) CNNs
    use convolution filters to imitate the human reception field, and treat sketch
    as a binary matrix in pixel space. (ii) RNNs imitate the temporally-extended human
    sketching process to recurrently capture spatial and temporal patterns of stroke,
    (iii) GNNs represent sketches as graphs, and can encode both topological/spatial
    and temporal patterns of stroke sequences, and (iv) TCNs imitate a temporal reception
    field on stroke sequence.'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，传统方法分为两个阶段，即从素描空间到特征空间的特征工程，从特征空间到目标空间的映射，而深度学习方法通常直接从素描空间到目标空间进行端到端的映射。因此，利用人类洞察的核心机会在于手动设计网络结构与手动设计特征之间的选择。由于素描领域的挑战（例如抽象、噪声、稀疏、多样化），素描特征工程的困难是传统方法的主要瓶颈之一。深度学习方法通过能够学习强特征表示（前提是有足够的数据来建模其变异性和多样性）在一定程度上缓解了这一问题。此外，深度学习网络有多种动机和机制来处理素描的独特特征：(i)
    CNNs 使用卷积滤波器模仿人类的感受场，并将素描视为像素空间中的二进制矩阵。(ii) RNNs 模仿时间扩展的人类素描过程，递归地捕捉笔划的空间和时间模式。(iii)
    GNNs 将素描表示为图形，能够编码笔划序列的拓扑/空间和时间模式。(iv) TCNs 模仿笔划序列上的时间感受场。
- en: 'We summarize the advantages and disadvantages of deep learning and traditional
    methods on sketch in Table [VIII](#S5.T8 "TABLE VIII ‣ 5 Discussion ‣ Deep Learning
    for Free-Hand Sketch: A Survey"); and we compare different network architectures
    for sketch oriented tasks in Table [IX](#S5.T9 "TABLE IX ‣ 5 Discussion ‣ Deep
    Learning for Free-Hand Sketch: A Survey").'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表[VIII](#S5.T8 "TABLE VIII ‣ 5 Discussion ‣ Deep Learning for Free-Hand
    Sketch: A Survey")中总结了深度学习和传统方法在素描上的优缺点；在表[IX](#S5.T9 "TABLE IX ‣ 5 Discussion
    ‣ Deep Learning for Free-Hand Sketch: A Survey")中比较了不同网络架构在素描导向任务中的表现。'
- en: 5.1.2 Data and Annotations
  id: totrans-542
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 数据与注释
- en: 'Annotation Uni-modal sketch datasets have begun to provide some annotation
    such as grouping [[152](#bib.bib152)] and segmentation [[25](#bib.bib25)]. However
    more fine-grained annotation of this type is necessary, wit sketch attributes
    in particular being lacking. As discussed in Section [4.2.3](#S4.SS2.SSS3 "4.2.3
    Sketch-Photo Generation ‣ 4.2 Multi-Modal Tasks: Sketch with Other Modalities
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey"),
    existing multi-modal sketch benchmark annotation is primarily in terms of *pairings*
    (e.g., sketch-photo, sketch-3D). However, fine-grained/local annotations (such
    as stroke-contour, parts, and attributes) would enable richer cross-modal alignment
    models to be learned.'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '注释 单模态素描数据集已经开始提供一些注释，例如分组[[152](#bib.bib152)]和分割[[25](#bib.bib25)]。然而，这类注释仍需更细粒度的标注，特别是素描属性方面。正如在第[4.2.3节](#S4.SS2.SSS3
    "4.2.3 Sketch-Photo Generation ‣ 4.2 Multi-Modal Tasks: Sketch with Other Modalities
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")中讨论的那样，现有的多模态素描基准注释主要集中在*配对*（例如，素描-照片，素描-3D）上。然而，细粒度/局部注释（例如笔划轮廓、部分和属性）将能够学习到更丰富的跨模态对齐模型。'
- en: Meta-Data and Fairness Unlike photos, sketches are uniquely influenced by the
    demographics, perception, memory, and drawing style of the artist; as well as
    the conditions under which they are drawn (i.e., time-limited or not). Most existing
    datasets do not take care to acquire balanced samples of users across background,
    age, gender, etc; or record such meta-data about their participants. However,
    such sampling and meta-data are necessary for studying changes in drawing style
    with these covariates, as well as for ensuring that sketch-based applications
    work well for users of different backgrounds. Furthermore, existing sketch datasets
    are mainly created as bespoke efforts by researchers or casual participants in
    online games with time-pressure. These may lead to sketches that are either excessively
    well dawn, or too poorly drawn. Data collected under a variety of drawing conditions,
    and meta-data about those conditions, would help sketch research in future.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据与公平性 与照片不同，素描独特地受到艺术家的背景、感知、记忆和绘画风格的影响；以及绘制条件（即是否有时间限制）。大多数现有的数据集并未关注平衡用户背景、年龄、性别等样本的获取，也没有记录关于参与者的元数据。然而，这些采样和元数据对于研究绘画风格随这些共变量的变化以及确保素描应用程序对不同背景的用户有效性是必要的。此外，现有的素描数据集主要是由研究人员或在时间压力下的在线游戏中随意参与者创建的。这可能导致素描要么过于精美，要么绘制得过于粗糙。在各种绘画条件下收集的数据及其相关的元数据将有助于未来的素描研究。
- en: 5.1.3 Architectures and Sketch-Specific Design
  id: totrans-545
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 架构与素描特定设计
- en: 'Architectures As discussed in Section [4.3](#S4.SS3 "4.3 Experimental Comparison
    ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch: A Survey")
    and Table [VI](#S4.T6 "TABLE VI ‣ 4.3.1 Representing Sketch ‣ 4.3 Experimental
    Comparison ‣ 4 Tasks and Methodology Taxonomy ‣ Deep Learning for Free-Hand Sketch:
    A Survey"), there are a variety of network architectures that can be applied to
    sketch, and these lead to a range of performances. These results suggest that
    performance will continue to advance as better architectures within each family
    are developed. The best architecture for sketch perception is still an open question.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '架构 如第[4.3](#S4.SS3 "4.3 Experimental Comparison ‣ 4 Tasks and Methodology Taxonomy
    ‣ Deep Learning for Free-Hand Sketch: A Survey")节和表[VI](#S4.T6 "TABLE VI ‣ 4.3.1
    Representing Sketch ‣ 4.3 Experimental Comparison ‣ 4 Tasks and Methodology Taxonomy
    ‣ Deep Learning for Free-Hand Sketch: A Survey")所讨论的，有多种网络架构可以应用于素描，这些架构导致了不同的性能。这些结果表明，随着每个家族内更好的架构的开发，性能将继续提升。素描感知的最佳架构仍然是一个未解之谜。'
- en: Sketch-Specific Design Another open question is to what extent sketch-specific
    designs are important vs. generic computer vision architectures and learning algorithms.
    Clearly sketch has unique challenges (sequential time-series nature, sparsity,
    abstraction, artist style, etc) that can be better taken into account with sketch-specific
    designs. However the broader vision and learning community can bring greater effort
    to bear on developing more advanced general purpose models. Therefore it remains
    to be seen in which sketch applications sketch-specific designs can take a decisive
    lead over generic architectures and algorithms. For example, sketch-specific designs
    may be more important in fine-grained tasks such as segmentation, grouping, and
    FG-SBIR compared to the simplest coarse-grained object categorization task.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 素描特定设计 另一个未解问题是素描特定设计与通用计算机视觉架构和学习算法的相对重要性。显然，素描具有独特的挑战（序列时间序列特性、稀疏性、抽象、艺术家风格等），这些挑战可以通过素描特定设计得到更好的考虑。然而，更广泛的视觉和学习社区可以更大力地开发更先进的通用模型。因此，仍需观察在哪些素描应用中，素描特定设计能够在通用架构和算法中占据决定性优势。例如，素描特定设计在细粒度任务如分割、分组和FG-SBIR中可能比最简单的粗粒度对象分类任务更为重要。
- en: 'Association with Other Sketches Some CNN based models designed for other kinds
    of sketches (e.g., well-drawn line drawings [[16](#bib.bib16), [329](#bib.bib329),
    [287](#bib.bib287), [177](#bib.bib177)], cartoons [[16](#bib.bib16), [177](#bib.bib177)])
    can be applied to free-hand sketch. In particular, it has been verified by relevant
    literature that CNN models oriented at high-quality line drawings can be successful
    for free-hand sketch tasks, including vectorization [[155](#bib.bib155), [330](#bib.bib330),
    [164](#bib.bib164), [331](#bib.bib331)], shading [[332](#bib.bib332)], inking [[333](#bib.bib333)],
    etc. It is interesting to evaluate which methods designed for other sketches can
    or cannot be applied to free-hand sketches. This will depend on to what extent
    architectures designed for well-drawn sketches (Figure [2](#S1.F2 "Figure 2 ‣
    1 Introduction ‣ Deep Learning for Free-Hand Sketch: A Survey")) become over-specialized
    to that type of data, or can generalize to the more abstract, diverse, and sparse
    free-hand sketches (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning
    for Free-Hand Sketch: A Survey")).'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他草图的关联 一些设计用于其他类型草图（例如，精美线条图 [[16](#bib.bib16), [329](#bib.bib329), [287](#bib.bib287),
    [177](#bib.bib177)]，漫画 [[16](#bib.bib16), [177](#bib.bib177)]）的CNN模型可以应用于手绘草图。特别是，相关文献已验证，针对高质量线条图的CNN模型可以成功地应用于手绘草图任务，包括矢量化 [[155](#bib.bib155),
    [330](#bib.bib330), [164](#bib.bib164), [331](#bib.bib331)]，着色 [[332](#bib.bib332)]，墨线 [[333](#bib.bib333)]
    等。评估设计用于其他草图的方法是否可以或不能应用于手绘草图是很有趣的。这将取决于为精美草图（图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ 手绘草图的深度学习：综述")）设计的架构在多大程度上过度专门化于该类型的数据，或者是否能够推广到更抽象、多样化和稀疏的手绘草图（图 [1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 手绘草图的深度学习：综述")）。
- en: 5.2 Potential Research Directions
  id: totrans-549
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 潜在研究方向
- en: In this section, we outline some potential research directions that we believe
    are promising in future, from both the perspectives of potential application and
    underpinning research value.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了一些我们认为在未来具有潜力的研究方向，从潜在应用和基础研究价值两个角度进行探讨。
- en: 5.2.1 Potential Application-Oriented Research
  id: totrans-551
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 潜在的应用导向研究
- en: Scene Sketches Scene-level sketch oriented deep learning is still under-studied.
    Recently, several seminal works (e.g., SketchyCOCO [[17](#bib.bib17)], SceneSketcher [[78](#bib.bib78)])
    have opened up promising directions for scene-level sketch research. They not
    only contribute large-scale scene-level sketch datasets but also propose novel
    research topics, e.g., scene sketch based image generation [[17](#bib.bib17)],
    fine-grained scene sketch based image retrieval [[78](#bib.bib78)]. These novel
    topics are useful for the practical applications, e.g., sketch based scene design.
    It remains to be seen up to complexity of scenes are technically feasible - and
    practically appealing for users - to retrieve using sketches.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 场景草图 场景级草图导向的深度学习仍然处于研究初期。最近，一些开创性的工作（例如，SketchyCOCO [[17](#bib.bib17)]，SceneSketcher [[78](#bib.bib78)]）为场景级草图研究开辟了有希望的方向。这些工作不仅贡献了大规模的场景级草图数据集，还提出了新颖的研究主题，例如基于场景草图的图像生成 [[17](#bib.bib17)]，细粒度场景草图的图像检索 [[78](#bib.bib78)]。这些新颖的主题对于实际应用非常有用，例如基于草图的场景设计。尚需观察复杂的场景在技术上是否可行，并且在实际中对用户是否具有吸引力，以便通过草图进行检索。
- en: 3D Sketches Collecting 3D sketches [[334](#bib.bib334)] is now easier thanks
    to new data collection equipment. This could support many interesting 3D sketch
    related research topics, e.g., combining virtual reality (VR) [[335](#bib.bib335)]
    and augmented reality (AR) [[336](#bib.bib336), [10](#bib.bib10), [11](#bib.bib11)].
    3D sketch research will help to bring sketch-based human-computer interaction
    from the 2D plane of the touch-screens to 3D spaces, enabling more immersive experience.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 草图 得益于新的数据收集设备，收集3D草图 [[334](#bib.bib334)] 现在变得更加容易。这可以支持许多有趣的3D草图相关研究主题，例如将虚拟现实（VR） [[335](#bib.bib335)]
    和增强现实（AR） [[336](#bib.bib336), [10](#bib.bib10), [11](#bib.bib11)] 结合起来。3D草图研究将有助于将基于草图的人机交互从触摸屏的二维平面拓展到三维空间，实现更具沉浸感的体验。
- en: Diverse Sketch Subjects Existing sketch datasets and applications mainly focus
    on sketches depicting objects. However, in practical applications users may be
    interested in machines understanding more diverse sketched concepts, e.g., sheets,
    curves, histograms [[337](#bib.bib337)], maps [[338](#bib.bib338)], engineering
    sketches [[309](#bib.bib309)], and user interface (UI) prototype drawings [[339](#bib.bib339)].
    Sketch also can be studied together with hand-written characters.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 多样化的草图主题 现有的草图数据集和应用主要集中在描绘对象的草图上。然而，在实际应用中，用户可能对机器理解更为多样化的草图概念感兴趣，例如，图纸、曲线、直方图
    [[337](#bib.bib337)]、地图 [[338](#bib.bib338)]、工程草图 [[309](#bib.bib309)] 和用户界面 (UI)
    原型图 [[339](#bib.bib339)]。草图还可以与手写字符一同研究。
- en: Sketch Color and Pressure Existing free-hand sketches are collected by common
    touch-screen devices, e.g., phone or tablet. Here the position of strokes is the
    main feature, with color and texture not being widely collected. Thus, existing
    sketch analysis has mostly focused on black or grayscale sketches. However, sketches
    can already be colored, and recent devices can increasingly sense pressure along
    strokes. Upgrading models to exploit color, texture, and pressure properties of
    sketches remains as outstanding work.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 草图颜色和压力 现有的手绘草图通常通过常见的触摸屏设备（如手机或平板电脑）收集。在这里，笔画的位置是主要特征，而颜色和纹理则未被广泛收集。因此，现有的草图分析主要集中在黑色或灰度草图上。然而，草图已经可以上色，且近期的设备可以越来越好地感知笔画的压力。升级模型以利用草图的颜色、纹理和压力特性仍然是一个突出的研究课题。
- en: Sketch Beautification How to beautify sketch [[340](#bib.bib340)] in various
    sketch-related HCI applications is interesting and challenging. Sketch beautification
    will not only refine user experience but also improve interaction efficiency in
    the sketch-based design applications, e.g., modifying a non-professional sketch
    to a professional sketch [[128](#bib.bib128)].
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 草图美化 如何在各种与草图相关的 HCI 应用中美化草图 [[340](#bib.bib340)] 是一个有趣且具有挑战性的课题。草图美化不仅会提升用户体验，还能提高草图设计应用中的交互效率，例如，将非专业草图修改为专业草图
    [[128](#bib.bib128)]。
- en: Sketch-Based Design People often use very simple or even scrawled sketches at
    the beginning of a design to brainstorm and generate inspiration. Thus, there
    are some very useful sketch-based designing applications that can help people
    to design 2D or 3D products, e.g., sketch-to-comic [[341](#bib.bib341), [342](#bib.bib342),
    [343](#bib.bib343), [344](#bib.bib344)], sketch-to-shadow [[34](#bib.bib34), [332](#bib.bib332)],
    and sketch-to-normal [[345](#bib.bib345), [346](#bib.bib346)]. We believe that
    in future these techniques can be continually improved further by future deep
    learning techniques. These sketch-based designing methods can improve the efficiency
    in HCI.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 基于草图的设计 人们在设计的开始阶段经常使用非常简单甚至潦草的草图来头脑风暴和激发灵感。因此，有一些非常有用的基于草图的设计应用可以帮助人们设计 2D
    或 3D 产品，例如，草图到漫画 [[341](#bib.bib341), [342](#bib.bib342), [343](#bib.bib343),
    [344](#bib.bib344)]、草图到阴影 [[34](#bib.bib34), [332](#bib.bib332)] 和草图到法线 [[345](#bib.bib345),
    [346](#bib.bib346)]。我们相信，未来这些技术可以通过深度学习技术进一步改进。这些基于草图的设计方法可以提高 HCI 的效率。
- en: Efficient Models For sketch-based perception and user-interfaces in mobile devices
    such as phones, tablets, or AR/VR gear, processing should be real-time and light-weight
    enough to run on embedded battery powered devices. How to compress sketch models
    and improve their efficiency is an important question for future work.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 高效模型 对于手机、平板电脑或 AR/VR 设备等移动设备中的基于草图的感知和用户界面处理，应实现实时且足够轻量化，以便在嵌入式电池供电的设备上运行。如何压缩草图模型并提高其效率是未来工作的一个重要问题。
- en: 5.2.2 Potential Theoretical Research
  id: totrans-559
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 潜在理论研究
- en: Sketch-oriented deep learning models have achieved good performance within well
    curated datasets. However, how well they can generalise to uncontrolled real-world
    conditions remains to be seen.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 面向草图的深度学习模型在经过精心策划的数据集上表现良好。然而，它们在实际控制条件下的泛化能力如何还有待观察。
- en: Diversity, Style, and Robustness The impact of dataset shift [[347](#bib.bib347)]
    has only begun to be studied in sketches. More uniquely, sketch is particularly
    subjective in terms of influence by the user’s drawing style, culture and potentially
    demographics. Robustness to these stylistic aspects is an under-studied area of
    sketch research. Similarly, taking the native format of sketch, existing adversarial
    attack studies could be extended to sketch domain by studying adversarial *strokes*
    or *waypoints*, rather than pixel perturbations.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性、风格和鲁棒性 数据集偏移的影响[[347](#bib.bib347)]才刚刚开始在草图中进行研究。更独特的是，草图在用户绘画风格、文化和潜在的人口统计学方面具有特别的主观性。对这些风格方面的鲁棒性是草图研究中的一个未充分研究的领域。同样，考虑到草图的原生格式，现有的对抗攻击研究可以通过研究对抗*笔画*或*路标*来扩展到草图领域，而不是像素扰动。
- en: Sketch as a Robustness Test Sketch images differ dramatically from photos, yet
    are easily recognized by humans. Sketch images such as ImageNet-Sketch [[41](#bib.bib41)],
    SketchTransfer [[66](#bib.bib66)], and PACS [[73](#bib.bib73)] can thus be used
    to benchmark the robustness and generalization ability of more general image-oriented
    deep learning models. Going beyond recognition, similar robustness evaluations
    could be performed on other instance-level retrieval problems such as person Re-ID.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 草图作为鲁棒性测试 草图图像与照片差异巨大，但人类容易识别。像ImageNet-Sketch [[41](#bib.bib41)]、SketchTransfer
    [[66](#bib.bib66)] 和PACS [[73](#bib.bib73)]这样的草图图像可以用于基准测试更通用的图像导向深度学习模型的鲁棒性和泛化能力。超越识别，类似的鲁棒性评估可以应用于其他实例级的检索问题，如人物再识别。
- en: 'Data-Efficient Sketch Models Major process in deep learning for sketch research
    has been driven by increasingly large sketch datasets (Table [II](#S2.T2 "TABLE
    II ‣ 2.2 A Brief History of Sketch in the Deep Learning Era ‣ 2 Background ‣ Deep
    Learning for Free-Hand Sketch: A Survey")). However, ultimately these datasets
    are harder to scale than corresponding photo datasets due to the need for manual
    sketching. Therefore data-efficient approaches to all the main sketch-analysis
    tasks of interest from recognition to SBIR are of major importance going forward.
    Whether this is best achieved by few-shot learning, self-supervised learning,
    or cross-modality knowledge transfer from photo domain remains to be seen.'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '数据高效的草图模型 深度学习中草图研究的主要进展是由日益增多的草图数据集推动的（见表[II](#S2.T2 "TABLE II ‣ 2.2 A Brief
    History of Sketch in the Deep Learning Era ‣ 2 Background ‣ Deep Learning for
    Free-Hand Sketch: A Survey")）。然而，**最终**这些数据集比相应的照片数据集更难以扩展，因为需要人工绘制草图。因此，从识别到SBIR的所有主要草图分析任务的数据高效方法在未来至关重要。是否通过少样本学习、自监督学习或从照片领域的跨模态知识迁移来最好实现这一点仍待观察。'
- en: 6 Conclusion
  id: totrans-564
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This survey reviewed the landscape of contemporary deep-learning based sketch
    research. We introduced the unique aspects of sketch in terms of sketch-specific
    challenges and diverse potential representations, and analyzed both existing datasets
    and existing methods in terms of a rich ecosystem of uni-modal and multi-modal
    sketch analysis tasks. We discussed open problems and under-studied research directions
    throughout. We hope this survey will help new researchers and practitioners get
    up to speed, provide a convenient reference for sketch experts, and encourage
    future progress in this exciting field.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查回顾了当代基于深度学习的草图研究。我们介绍了草图在草图特定挑战和多样潜在表示方面的独特之处，并分析了现有数据集和方法的丰富生态系统，包括单模态和多模态草图分析任务。我们在整个过程中讨论了未解决的问题和研究方向。我们希望本调查能帮助新研究人员和从业者快速上手，为草图专家提供便利的参考，并鼓励在这一激动人心的领域取得未来进展。
- en: References
  id: totrans-566
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. Hu and J. Collomosse, “A performance evaluation of gradient field hog
    descriptor for sketch based image retrieval,” *CVIU*, 2013.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] R. Hu 和 J. Collomosse, “用于基于草图的图像检索的梯度场HOG描述符的性能评估”，*CVIU*，2013年。'
- en: '[2] F. Wang, L. Kang, and Y. Li, “Sketch-based 3d shape retrieval using convolutional
    neural networks,” in *CVPR*, 2015.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] F. Wang, L. Kang, 和 Y. Li, “基于草图的3D形状检索，使用卷积神经网络”，发表于*CVPR*，2015年。'
- en: '[3] Q. Yu, Y. Yang, F. Liu, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “Sketch-a-net:
    A deep neural network that beats humans,” *IJCV*, 2017.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Q. Yu, Y. Yang, F. Liu, Y.-Z. Song, T. Xiang, 和 T. M. Hospedales, “Sketch-a-net:
    一种超过人类的深度神经网络”，*IJCV*，2017年。'
- en: '[4] Q. Yu, F. Liu, Y.-Z. Song, T. Xiang, T. M. Hospedales, and C.-C. Loy, “Sketch
    me that shoe,” in *CVPR*, 2016.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Q. Yu, F. Liu, Y.-Z. Song, T. Xiang, T. M. Hospedales, 和 C.-C. Loy, “给我画那只鞋子”，发表于*CVPR*，2016年。'
- en: '[5] D. Ha and D. Eck, “A neural representation of sketch drawings,” in *ICLR*,
    2018.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] D. Ha 和 D. Eck, “草图绘制的神经表示”，发表于*ICLR*，2018年。'
- en: '[6] M. Eitz, J. Hays, and M. Alexa, “How do humans sketch objects?” *TOG*,
    2012.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. Eitz, J. Hays, 和 M. Alexa，“人类如何绘制物体？” *TOG*，2012年。'
- en: '[7] P. Sangkloy, N. Burnell, C. Ham, and J. Hays, “The sketchy database: learning
    to retrieve badly drawn bunnies,” *TOG*, 2016.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] P. Sangkloy, N. Burnell, C. Ham, 和 J. Hays，“Sketchy 数据库：学习检索画得不好的兔子，” *TOG*，2016年。'
- en: '[8] F. Huang, J. F. Canny, and J. Nichols, “Swire: Sketch-based user interface
    retrieval,” in *CHI*, 2019.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] F. Huang, J. F. Canny, 和 J. Nichols，“Swire：基于草图的用户界面检索，” 见于*CHI*，2019年。'
- en: '[9] S. Suleri, V. P. Sermuga Pandian, S. Shishkovets, and M. Jarke, “Eve: A
    sketch-based software prototyping workbench,” in *CHI*, 2019.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Suleri, V. P. Sermuga Pandian, S. Shishkovets, 和 M. Jarke，“Eve：基于草图的软件原型工作台，”
    见于*CHI*，2019年。'
- en: '[10] K. C. Kwan and H. Fu, “Mobi3dsketch: 3d sketching in mobile ar,” in *CHI*,
    2019.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] K. C. Kwan 和 H. Fu，“Mobi3dsketch：移动增强现实中的 3D 草图，” 见于*CHI*，2019年。'
- en: '[11] D. Gasques, J. G. Johnson, T. Sharkey, and N. Weibel, “What you sketch
    is what you get: Quick and easy augmented reality prototyping with pintar,” in
    *CHI*, 2019.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] D. Gasques, J. G. Johnson, T. Sharkey, 和 N. Weibel，“你画的就是你得到的：使用 pintar
    快速轻松地进行增强现实原型设计，” 见于*CHI*，2019年。'
- en: '[12] A. Kotani and S. Tellex, “Teaching robots to draw,” in *ICRA*, 2019.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Kotani 和 S. Tellex，“教机器人绘画，” 见于*ICRA*，2019年。'
- en: '[13] J. E. Fan, M. Dinculescu, and D. Ha, “collabdraw: an environment for collaborative
    sketching with an artificial agent,” in *ACM SIGCHI Conference on Creativity and
    Cognition*, 2019.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. E. Fan, M. Dinculescu, 和 D. Ha，“collabdraw：一个与人工代理协作草图的环境，” 见于*ACM
    SIGCHI Conference on Creativity and Cognition*，2019年。'
- en: '[14] I. E. Sutherland, “Sketchpad a man-machine graphical communication system,”
    *Simulation*, 1964.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] I. E. Sutherland，“Sketchpad 一种人机图形通信系统，” *Simulation*，1964年。'
- en: '[15] C. F. Herot, “Graphical input through machine recognition of sketches,”
    *TOG*, 1976.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] C. F. Herot，“通过机器识别草图进行图形输入，” *TOG*，1976年。'
- en: '[16] E. Simo-Serra, S. Iizuka, K. Sasaki, and H. Ishikawa, “Learning to simplify:
    fully convolutional networks for rough sketch cleanup,” *TOG*, 2016.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] E. Simo-Serra, S. Iizuka, K. Sasaki, 和 H. Ishikawa，“学习简化：用于粗糙草图清理的全卷积网络，”
    *TOG*，2016年。'
- en: '[17] C. Gao, Q. Liu, Q. Xu, L. Wang, J. Liu, and C. Zou, “Sketchycoco: Image
    generation from freehand scene sketches,” in *CVPR*, 2020.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] C. Gao, Q. Liu, Q. Xu, L. Wang, J. Liu, 和 C. Zou，“Sketchycoco：从自由手场景草图生成图像，”
    见于*CVPR*，2020年。'
- en: '[18] Q. Yu, Y. Yang, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “Sketch-a-net
    that beats humans,” in *BMVC*, 2015.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Q. Yu, Y. Yang, Y.-Z. Song, T. Xiang, 和 T. M. Hospedales，“Sketch-a-net
    战胜人类，” 见于*BMVC*，2015年。'
- en: '[19] P. Xu, Y. Huang, T. Yuan, K. Pang, Y.-Z. Song, T. Xiang, T. M. Hospedales,
    Z. Ma, and J. Guo, “Sketchmate: Deep hashing for million-scale human sketch retrieval,”
    in *CVPR*, 2018.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] P. Xu, Y. Huang, T. Yuan, K. Pang, Y.-Z. Song, T. Xiang, T. M. Hospedales,
    Z. Ma, 和 J. Guo，“Sketchmate：用于百万规模人类草图检索的深度哈希，” 见于*CVPR*，2018年。'
- en: '[20] C. Hu, D. Li, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “Sketch-a-classifier:
    Sketch-based photo classifier generation,” in *CVPR*, 2018.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] C. Hu, D. Li, Y.-Z. Song, T. Xiang, 和 T. M. Hospedales，“Sketch-a-classifier：基于草图的照片分类器生成，”
    见于*CVPR*，2018年。'
- en: '[21] U. Riaz Muhammad, Y. Yang, Y.-Z. Song, T. Xiang, and T. M. Hospedales,
    “Learning deep sketch abstraction,” in *CVPR*, 2018.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] U. Riaz Muhammad, Y. Yang, Y.-Z. Song, T. Xiang, 和 T. M. Hospedales，“学习深度草图抽象，”
    见于*CVPR*，2018年。'
- en: '[22] T. Portenier, Q. Hu, A. Szabo, S. A. Bigdeli, P. Favaro, and M. Zwicker,
    “Faceshop: Deep sketch-based face image editing,” *TOG*, 2018.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] T. Portenier, Q. Hu, A. Szabo, S. A. Bigdeli, P. Favaro, 和 M. Zwicker，“Faceshop：深度基于草图的面部图像编辑，”
    *TOG*，2018年。'
- en: '[23] P. Xu, C. K. Joshi, and X. Bresson, “Multigraph transformer for free-hand
    sketch recognition,” *IEEE Transactions on Neural Networks and Learning Systems*,
    2021.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] P. Xu, C. K. Joshi, 和 X. Bresson，“用于自由手草图识别的多图变换器，” *IEEE Transactions
    on Neural Networks and Learning Systems*，2021年。'
- en: '[24] L. Yang, J. Zhuang, H. Fu, K. Zhou, and Y. Zheng, “Sketchgcn: Semantic
    sketch segmentation with graph convolutional networks,” *arXiv preprint arXiv:2003.00678*,
    2020.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] L. Yang, J. Zhuang, H. Fu, K. Zhou, 和 Y. Zheng，“Sketchgcn：使用图卷积网络进行语义草图分割，”
    *arXiv preprint arXiv:2003.00678*，2020年。'
- en: '[25] Y. Qi and Z.-H. Tan, “Sketchsegnet+: An end-to-end learning of rnn for
    multi-class sketch semantic segmentation,” *Access*, 2019.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. Qi 和 Z.-H. Tan，“Sketchsegnet+：用于多类草图语义分割的端到端 RNN 学习，” *Access*，2019年。'
- en: '[26] J. Song, Q. Yu, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “Deep spatial-semantic
    attention for fine-grained sketch-based image retrieval,” in *ICCV*, 2017.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. Song, Q. Yu, Y.-Z. Song, T. Xiang, 和 T. M. Hospedales，“用于细粒度基于草图的图像检索的深度空间-语义注意力，”
    见于*ICCV*，2017年。'
- en: '[27] S. Ouyang, T. M. Hospedales, Y.-Z. Song, and X. Li, “Forgetmenot: Memory-aware
    forensic facial sketch matching,” in *CVPR*, 2016.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. Ouyang, T. M. Hospedales, Y.-Z. Song, 和 X. Li，“Forgetmenot：记忆感知的法医面部草图匹配，”
    见于*CVPR*，2016年。'
- en: '[28] C. Hu, D. Li, Y.-Z. Song, and T. M. Hospedales, “Now you see me: Deep
    face hallucination for unviewed sketches.” in *BMVC*, 2017.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] C. Hu, D. Li, Y.-Z. Song, 和 T. M. Hospedales，“现在你看到了我：未查看素描的深度面部幻觉，”发表于
    *BMVC*，2017年。'
- en: '[29] S. Nagpal, M. Singh, R. Singh, M. Vatsa, A. Noore, and A. Majumdar, “Face
    sketch matching via coupled deep transform learning,” in *ICCV*, 2017.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S. Nagpal, M. Singh, R. Singh, M. Vatsa, A. Noore, 和 A. Majumdar，“通过耦合深度变换学习进行面部素描匹配，”发表于
    *ICCV*，2017年。'
- en: '[30] D.-P. Fan, S. Zhang, Y.-H. Wu, Y. Liu, M.-M. Cheng, B. Ren, P. L. Rosin,
    and R. Ji, “Scoot: A perceptual metric for facial sketches,” in *ICCV*, 2019.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] D.-P. Fan, S. Zhang, Y.-H. Wu, Y. Liu, M.-M. Cheng, B. Ren, P. L. Rosin,
    和 R. Ji，“Scoot: 面部素描的感知度量，”发表于 *ICCV*，2019年。'
- en: '[31] L. Pang, Y. Wang, Y.-Z. Song, T. Huang, and Y. Tian, “Cross-domain adversarial
    feature learning for sketch re-identification,” in *MM*, 2018.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] L. Pang, Y. Wang, Y.-Z. Song, T. Huang, 和 Y. Tian，“跨域对抗特征学习用于素描重识别，”发表于
    *MM*，2018年。'
- en: '[32] M. Huang, J. Lin, N. Chen, W. An, and W. Zhu, “Reversed sketch: A scalable
    and comparable shape representation,” *PR*, 2018.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] M. Huang, J. Lin, N. Chen, W. An, 和 W. Zhu，“反向素描：一种可扩展且可比较的形状表示，” *PR*，2018年。'
- en: '[33] S. Kazuma, S. Iizuka, E. Simo-Serra, and H. Ishikawa, “Learning to Restore
    Deteriorated Line Drawing,” *The Visual Computer*, 2018.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. Kazuma, S. Iizuka, E. Simo-Serra, 和 H. Ishikawa，“学习恢复退化的线条绘画，” *The
    Visual Computer*，2018年。'
- en: '[34] Q. Zheng, Z. Li, and A. Bargteil, “Learning to shadow hand-drawn sketches,”
    in *CVPR*, 2020.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Q. Zheng, Z. Li, 和 A. Bargteil，“学习为手绘素描添加阴影，”发表于 *CVPR*，2020年。'
- en: '[35] J. Lee, E. Kim, Y. Lee, D. Kim, J. Chang, and J. Choo, “Reference-based
    sketch image colorization using augmented-self reference and dense semantic correspondence,”
    in *CVPR*, 2020.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] J. Lee, E. Kim, Y. Lee, D. Kim, J. Chang, 和 J. Choo，“基于参考的素描图像上色，使用增强自我参考和密集语义对应，”发表于
    *CVPR*，2020年。'
- en: '[36] S. Gui, Y. Zhu, X. Qin, and X. Ling, “Learning multi-level domain invariant
    features for sketch re-identification,” *Neurocomputing*, 2020.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Gui, Y. Zhu, X. Qin, 和 X. Ling，“学习多层次领域不变特征用于素描重识别，” *Neurocomputing*，2020年。'
- en: '[37] C. Yan, D. Vanderhaeghe, and Y. Gingold, “A benchmark for rough sketch
    cleanup,” *TOG*, 2020.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] C. Yan, D. Vanderhaeghe, 和 Y. Gingold，“粗略素描清理基准测试，” *TOG*，2020年。'
- en: '[38] L. Zhang, C. Li, E. Simo-Serra, Y. Ji, T.-T. Wong, and C. Liu, “User-Guided
    Line Art Flat Filling with Split Filling Mechanism,” in *CVPR*, 2021.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] L. Zhang, C. Li, E. Simo-Serra, Y. Ji, T.-T. Wong, 和 C. Liu，“用户引导的线条艺术平面填充与分裂填充机制，”发表于
    *CVPR*，2021年。'
- en: '[39] S.-B. Chen, P.-C. Wang, B. Luo, C. H. Ding, and J. Zhang, “Sragan: Generating
    colour landscape photograph from sketch,” in *IJCNN*, 2019.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S.-B. Chen, P.-C. Wang, B. Luo, C. H. Ding, 和 J. Zhang，“Sragan: 从素描生成彩色风景摄影，”发表于
    *IJCNN*，2019年。'
- en: '[40] M. R. Amer, S. Yousefi, R. Raich, and S. Todorovic, “Monocular extraction
    of 2.1 d sketch using constrained convex optimization,” *IJCV*, 2015.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] M. R. Amer, S. Yousefi, R. Raich, 和 S. Todorovic，“使用约束凸优化提取单目2.1D素描，”
    *IJCV*，2015年。'
- en: '[41] H. Wang, S. Ge, Z. Lipton, and E. P. Xing, “Learning robust global representations
    by penalizing local predictive power,” in *NeurIPS*, 2019.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] H. Wang, S. Ge, Z. Lipton, 和 E. P. Xing，“通过惩罚局部预测能力学习鲁棒的全局表示，”发表于 *NeurIPS*，2019年。'
- en: '[42] K. Chen, I. Rabkina, M. D. McLure, and K. D. Forbus, “Human-like sketch
    object recognition via analogical learning,” in *AAAI*, 2019.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] K. Chen, I. Rabkina, M. D. McLure, 和 K. D. Forbus，“通过类比学习进行类人素描对象识别，”发表于
    *AAAI*，2019年。'
- en: '[43] X. Han, K. Hou, D. Du, Y. Qiu, Y. Yu, K. Zhou, and S. Cui, “Caricatureshop:
    Personalized and photorealistic caricature sketching,” *arXiv preprint arXiv:1807.09064*,
    2018.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] X. Han, K. Hou, D. Du, Y. Qiu, Y. Yu, K. Zhou, 和 S. Cui，“Caricatureshop:
    个性化和逼真的漫画素描，” *arXiv preprint arXiv:1807.09064*，2018年。'
- en: '[44] L. Zhao, F. Han, X. Peng, X. Zhang, M. Kapadia, V. Pavlovic, and D. N.
    Metaxas, “Cartoonish sketch-based face editing in videos using identity deformation
    transfer,” *Computers Graphics*, 2019.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] L. Zhao, F. Han, X. Peng, X. Zhang, M. Kapadia, V. Pavlovic, 和 D. N. Metaxas，“在视频中使用身份变形转移进行卡通风格素描面部编辑，”
    *Computers Graphics*，2019年。'
- en: '[45] M. Yuan and E. Simo-Serra, “Line Art Colorization with Concatenated Spatial
    Attention,” in *CVPR Workshops*, 2021.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] M. Yuan 和 E. Simo-Serra，“具有串联空间注意力的线条艺术上色，”发表于 *CVPR Workshops*，2021年。'
- en: '[46] J. Delanoy, M. Aubry, P. Isola, A. A. Efros, and A. Bousseau, “3d sketching
    using multi-view deep volumetric prediction,” *CGIT*, 2018.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Delanoy, M. Aubry, P. Isola, A. A. Efros, 和 A. Bousseau，“使用多视角深度体积预测进行三维素描，”
    *CGIT*，2018年。'
- en: '[47] B. Li, Y. Lu, A. Godil, T. Schreck, B. Bustos, A. Ferreira, T. Furuya,
    M. J. Fonseca, H. Johan, T. Matsuda *et al.*, “A comparison of methods for sketch-based
    3d shape retrieval,” *CVIU*, 2014.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] B. Li, Y. Lu, A. Godil, T. Schreck, B. Bustos, A. Ferreira, T. Furuya,
    M. J. Fonseca, H. Johan, T. Matsuda *等*，“用于素描基础三维形状检索的方法比较，” *CVIU*，2014年。'
- en: '[48] N. Prajapati and G. Prajapti, “Sketch based image retrieval system for
    the web-a survey,” *IJCSIT*, 2015.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] N. Prajapati 和 G. Prajapti，“基于草图的图像检索系统综述”，发表于*IJCSIT*，2015年。'
- en: '[49] M. Indu and K. Kavitha, “Survey on sketch based image retrieval methods,”
    in *ICCPCT*, 2016.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] M. Indu 和 K. Kavitha，“基于草图的图像检索方法综述”，发表于*ICCPCT*，2016年。'
- en: '[50] Y. Li and W. Li, “A survey of sketch-based image retrieval,” *Machine
    Vision and Applications*, 2018.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Y. Li 和 W. Li，“基于草图的图像检索综述”，发表于*Machine Vision and Applications*，2018年。'
- en: '[51] X. Zhang, X. Li, Y. Liu, and F. Feng, “A survey on freehand sketch recognition
    and retrieval,” *Image and Vision Computing*, 2019.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] X. Zhang, X. Li, Y. Liu, 和 F. Feng，“自由手绘草图识别与检索综述”，发表于*Image and Vision
    Computing*，2019年。'
- en: '[52] M. Schrapel, F. Herzog, S. Ryll, and M. Rohs, “Watch my painting: The
    back of the hand as a drawing space for smartwatches,” *CHI*, 2020.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] M. Schrapel, F. Herzog, S. Ryll, 和 M. Rohs，“观看我的绘画：手背作为智能手表的绘图空间”，发表于*CHI*，2020年。'
- en: '[53] M. Dvorožňák, D. Sýkora, C. Curtis, B. Curless, O. Sorkine-Hornung, and
    D. Salesin, “Monster Mash: A single-view approach to casual 3d modeling and animation,”
    *ACM Transactions on Graphics*, 2020.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] M. Dvorožňák, D. Sýkora, C. Curtis, B. Curless, O. Sorkine-Hornung, 和
    D. Salesin，“怪物大混战：一种单视角的休闲3D建模和动画方法”，发表于*ACM Transactions on Graphics*，2020年。'
- en: '[54] R. K. Sarvadevabhatla, S. Surya, T. Mittal, and V. B. Radhakrishnan, “Pictionary-style
    word-guessing on hand-drawn object sketches: dataset, analysis and deep network
    models,” *TPAMI*, 2020.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] R. K. Sarvadevabhatla, S. Surya, T. Mittal, 和 V. B. Radhakrishnan，“类似拼图的手绘对象草图词语猜测：数据集、分析和深度网络模型”，发表于*TPAMI*，2020年。'
- en: '[55] J. Song, K. Pang, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “Learning
    to sketch with shortcut cycle consistency,” in *CVPR*, 2018.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] J. Song, K. Pang, Y.-Z. Song, T. Xiang, 和 T. M. Hospedales，“通过快捷循环一致性学习草图”，发表于*CVPR*，2018年。'
- en: '[56] A. Das, Y. Yang, T. M. Hospedales, T. Xiang, and Y.-Z. Song, “Cloud2curve:
    Generation and vectorization of parametric sketches,” in *CVPR*, 2021.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] A. Das, Y. Yang, T. M. Hospedales, T. Xiang, 和 Y.-Z. Song，“Cloud2curve：参数化草图的生成与矢量化”，发表于*CVPR*，2021年。'
- en: '[57] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified embedding
    for face recognition and clustering,” in *CVPR*, 2015.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] F. Schroff, D. Kalenichenko, 和 J. Philbin，“Facenet：面部识别和聚类的统一嵌入”，发表于*CVPR*，2015年。'
- en: '[58] P. Sangkloy, J. Lu, C. Fang, F. Yu, and J. Hays, “Scribbler: Controlling
    deep image synthesis with sketch and color,” in *CVPR*, 2017.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] P. Sangkloy, J. Lu, C. Fang, F. Yu, 和 J. Hays，“Scribbler：通过草图和颜色控制深度图像合成”，发表于*CVPR*，2017年。'
- en: '[59] K. Li, K. Pang, J. Song, Y.-Z. Song, T. Xiang, T. M. Hospedales, and H. Zhang,
    “Universal sketch perceptual grouping,” in *ECCV*, 2018.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] K. Li, K. Pang, J. Song, Y.-Z. Song, T. Xiang, T. M. Hospedales, 和 H.
    Zhang，“通用草图感知分组”，发表于*ECCV*，2018年。'
- en: '[60] J. Choi, H. Cho, J. Song, and S. M. Yoon, “Sketchhelper: Real-time stroke
    guidance for freehand sketch retrieval,” *TMM*, 2019.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] J. Choi, H. Cho, J. Song, 和 S. M. Yoon，“Sketchhelper：用于自由手绘草图检索的实时笔画指导”，发表于*TMM*，2019年。'
- en: '[61] F. Wang, S. Lin, H. Wu, H. Li, R. Wang, X. Luo, and X. He, “Spfusionnet:
    Sketch segmentation using multi-modal data fusion,” in *ICME*, 2019.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] F. Wang, S. Lin, H. Wu, H. Li, R. Wang, X. Luo, 和 X. He，“Spfusionnet：利用多模态数据融合的草图分割”，发表于*ICME*，2019年。'
- en: '[62] R. K. Sarvadevabhatla, S. Suresh, and R. V. Babu, “Object category understanding
    via eye fixations on freehand sketches,” *TIP*, 2017.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] R. K. Sarvadevabhatla, S. Suresh, 和 R. V. Babu，“通过对自由手绘草图的眼动注视理解对象类别”，发表于*TIP*，2017年。'
- en: '[63] “Aaron koblin sheep dataset,” [https://github.com/hardmaru/sketch-rnn-datasets/tree/master/aaron_sheep](https://github.com/hardmaru/sketch-rnn-datasets/tree/master/aaron_sheep).'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] “Aaron koblin 羊数据集”，[https://github.com/hardmaru/sketch-rnn-datasets/tree/master/aaron_sheep](https://github.com/hardmaru/sketch-rnn-datasets/tree/master/aaron_sheep)。'
- en: '[64] C. Tirkaz, B. Yanikoglu, and T. M. Sezgin, “Sketched symbol recognition
    with auto-completion,” *PR*, 2012.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] C. Tirkaz, B. Yanikoglu, 和 T. M. Sezgin，“带自动补全的草图符号识别”，发表于*PR*，2012年。'
- en: '[65] S. Dey, P. Riba, A. Dutta, J. Llados, and Y.-Z. Song, “Doodle to search:
    Practical zero-shot sketch-based image retrieval,” in *CVPR*, 2019.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] S. Dey, P. Riba, A. Dutta, J. Llados, 和 Y.-Z. Song，“涂鸦搜索：实用的零样本草图基础图像检索”，发表于*CVPR*，2019年。'
- en: '[66] A. Lamb, S. Ozair, V. Verma, and D. Ha, “Sketchtransfer: A new dataset
    for exploring detail-invariance and the abstractions learned by deep networks,”
    in *WACV*, 2020.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] A. Lamb, S. Ozair, V. Verma, 和 D. Ha，“Sketchtransfer：探索细节不变性和深度网络学习的抽象的新数据集”，发表于*WACV*，2020年。'
- en: '[67] H. Zhang, S. Liu, C. Zhang, W. Ren, R. Wang, and X. Cao, “Sketchnet: Sketch
    classification with web images,” in *CVPR*, 2016.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] H. Zhang, S. Liu, C. Zhang, W. Ren, R. Wang, 和 X. Cao，“Sketchnet：基于网络图像的草图分类”，发表于*CVPR*，2016年。'
- en: '[68] T. Jiang, G.-S. Xia, and Q. Lu, “Sketch-based aerial image retrieval,”
    in *ICIP*, 2017.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] T. Jiang, G.-S. Xia, 和 Q. Lu，“基于草图的航空图像检索”，发表于*ICIP*，2017年。'
- en: '[69] T.-B. Jiang, G.-S. Xia, Q.-K. Lu, and W.-M. Shen, “Retrieving aerial scene
    images with learned deep image-sketch features,” *JCST*, 2017.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] T.-B. Jiang, G.-S. Xia, Q.-K. Lu, 和 W.-M. Shen，“利用学习到的深度图像-草图特征检索航拍场景图像，”*JCST*，2017年。'
- en: '[70] X. Wang, X. Duan, and X. Bai, “Deep sketch feature for cross-domain image
    retrieval,” *Neurocomputing*, 2016.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] X. Wang, X. Duan, 和 X. Bai，“用于跨领域图像检索的深度草图特征，”*Neurocomputing*，2016年。'
- en: '[71] M. Eitz, R. Richter, T. Boubekeur, K. Hildebrand, and M. Alexa, “Sketch-based
    shape retrieval,” *TOG*, 2012.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] M. Eitz, R. Richter, T. Boubekeur, K. Hildebrand, 和 M. Alexa，“基于草图的形状检索，”*TOG*，2012年。'
- en: '[72] B. Li, Y. Lu, C. Li, A. Godil, T. Schreck, M. Aono, M. Burtscher, H. Fu,
    T. Furuya, H. Johan *et al.*, “Shrec’14 track: Extended large scale sketch-based
    3d shape retrieval,” in *Eurographics workshop on 3D object retrieval*, 2014.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] B. Li, Y. Lu, C. Li, A. Godil, T. Schreck, M. Aono, M. Burtscher, H. Fu,
    T. Furuya, H. Johan *等*，“Shrec’14 轨道：扩展的大规模基于草图的3D形状检索，”在*Eurographics 3D对象检索研讨会*，2014年。'
- en: '[73] D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales, “Deeper, broader and
    artier domain generalization,” in *ICCV*, 2017.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] D. Li, Y. Yang, Y.-Z. Song, 和 T. M. Hospedales，“更深、更广泛、更艺术的领域泛化，”在*ICCV*，2017年。'
- en: '[74] C. Xiao, C. Wang, L. Zhang, and L. Zhang, “Sketch-based image retrieval
    via shape words,” in *ICMR*, 2015.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] C. Xiao, C. Wang, L. Zhang, 和 L. Zhang，“基于草图的图像检索通过形状词，”在*ICMR*，2015年。'
- en: '[75] L. Castrejon, Y. Aytar, C. Vondrick, H. Pirsiavash, and A. Torralba, “Learning
    aligned cross-modal representations from weakly aligned data,” in *CVPR*, 2016.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] L. Castrejon, Y. Aytar, C. Vondrick, H. Pirsiavash, 和 A. Torralba，“从弱对齐数据中学习对齐的跨模态表示，”在*CVPR*，2016年。'
- en: '[76] C. Zou, Q. Yu, R. Du, H. Mo, Y.-Z. Song, T. Xiang, C. Gao, B. Chen, and
    H. Zhang, “Sketchyscene: Richly-annotated scene sketches,” in *ECCV*, 2018.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] C. Zou, Q. Yu, R. Du, H. Mo, Y.-Z. Song, T. Xiang, C. Gao, B. Chen, 和
    H. Zhang，“Sketchyscene：丰富标注的场景草图，”在*ECCV*，2018年。'
- en: '[77] X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang, “Moment matching
    for multi-source domain adaptation,” in *ICCV*, 2019.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, 和 B. Wang，“用于多源领域适应的时刻匹配，”在*ICCV*，2019年。'
- en: '[78] F. Liu, C. Zou, X. Deng, R. Zuo, Y.-K. Lai, C. Ma, Y.-J. Liu, and H. Wang,
    “Scenesketcher: Fine-grained image retrieval with scene sketches,” in *ECCV*,
    2020.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] F. Liu, C. Zou, X. Deng, R. Zuo, Y.-K. Lai, C. Ma, Y.-J. Liu, 和 H. Wang，“Scenesketcher：使用场景草图的细粒度图像检索，”在*ECCV*，2020年。'
- en: '[79] T. Kato, T. Kurita, N. Otsu, and K. Hirata, “A sketch retrieval method
    for full color image database-query by visual example,” in *ICPR*, 1992.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] T. Kato, T. Kurita, N. Otsu, 和 K. Hirata，“用于全彩图像数据库的草图检索方法——通过视觉示例查询，”在*ICPR*，1992年。'
- en: '[80] Y. Li, T. M. Hospedales, Y.-Z. Song, and S. Gong, “Fine-grained sketch-based
    image retrieval by matching deformable part models,” in *BMVC*, 2014.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Y. Li, T. M. Hospedales, Y.-Z. Song, 和 S. Gong，“通过匹配可变形部件模型进行细粒度草图基图像检索，”在*BMVC*，2014年。'
- en: '[81] T. de Vries, I. Misra, C. Wang, and L. van der Maaten, “Does object recognition
    work for everyone?” in *CVPR Workshops*, 2019.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] T. de Vries, I. Misra, C. Wang, 和 L. van der Maaten，“对象识别是否适用于所有人？”在*CVPR
    Workshops*，2019年。'
- en: '[82] S. Barocas, M. Hardt, and A. Narayanan, *Fairness and Machine Learning*.   fairmlbook.org,
    2019, [http://www.fairmlbook.org](http://www.fairmlbook.org).'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] S. Barocas, M. Hardt, 和 A. Narayanan，*公平性与机器学习*。fairmlbook.org，2019年，[http://www.fairmlbook.org](http://www.fairmlbook.org)。'
- en: '[83] Y. Matsui, T. Shiratori, and K. Aizawa, “Drawfromdrawings: 2d drawing
    assistance via stroke interpolation with a sketch database,” *TVCG*, 2016.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. Matsui, T. Shiratori, 和 K. Aizawa，“Drawfromdrawings：通过草图数据库的笔画插值进行2D绘图辅助，”*TVCG*，2016年。'
- en: '[84] K. D. Forbus, B. Garnier, B. Tikoff, W. Marko, M. Usher, and M. McLure,
    “Sketch worksheets in stem classrooms: Two deployments,” in *AAAI*, 2018.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] K. D. Forbus, B. Garnier, B. Tikoff, W. Marko, M. Usher, 和 M. McLure，“STEM教室中的草图工作表：两次部署，”在*AAAI*，2018年。'
- en: '[85] Y. Ye, Y. Lu, and H. Jiang, “Human’s scene sketch understanding,” in *ICMR*,
    2016.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Y. Ye, Y. Lu, 和 H. Jiang，“人类的场景草图理解，”在*ICMR*，2016年。'
- en: '[86] Y. Xie, P. Xu, and Z. Ma, “Deep zero-shot learning for scene sketch,”
    *arXiv preprint arXiv:1905.04510*, 2019.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Y. Xie, P. Xu, 和 Z. Ma，“用于场景草图的深度零样本学习，”*arXiv 预印本 arXiv:1905.04510*，2019年。'
- en: '[87] O. Seddati, S. Dupont, and S. Mahmoudi, “Deepsketch: deep convolutional
    neural networks for sketch recognition and similarity search,” in *CBMI*, 2015.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] O. Seddati, S. Dupont, 和 S. Mahmoudi，“Deepsketch：用于草图识别和相似性搜索的深度卷积神经网络，”在*CBMI*，2015年。'
- en: '[88] Y. Zhang, Y. Zhang, and X. Qian, “Deep neural networks for free-hand sketch
    recognition,” in *Pacific Rim Conference on Multimedia*, 2016.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Y. Zhang, Y. Zhang, 和 X. Qian，“用于自由手草图识别的深度神经网络，”在*Pacific Rim Conference
    on Multimedia*，2016年。'
- en: '[89] J. Guo, C. Wang, E. Roman-Rangel, H. Chao, and Y. Rui, “Building hierarchical
    representations for oracle character and sketch recognition,” *TIP*, 2016.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] J. Guo, C. Wang, E. Roman-Rangel, H. Chao, 和 Y. Rui, “为神谕字符和草图识别建立层次化表示”，*TIP*，2016年。'
- en: '[90] P. Ballester and R. M. Araujo, “On the performance of googlenet and alexnet
    applied to sketches,” in *AAAI*, 2016.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] P. Ballester 和 R. M. Araujo, “关于应用于草图的 Googlenet 和 Alexnet 的性能”，在 *AAAI*，2016年。'
- en: '[91] O. Seddati, S. Dupont, and S. Mahmoudi, “Deepsketch 2: Deep convolutional
    neural networks for partial sketch recognition,” in *CBMI*, 2016.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] O. Seddati, S. Dupont, 和 S. Mahmoudi, “Deepsketch 2: 深度卷积神经网络用于部分草图识别”，在
    *CBMI*，2016年。'
- en: '[92] H. Zhang, P. She, Y. Liu, J. Gan, X. Cao, and H. Foroosh, “Learning structural
    representations via dynamic object landmarks discovery for sketch recognition
    and retrieval,” *TIP*, 2019.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] H. Zhang, P. She, Y. Liu, J. Gan, X. Cao, 和 H. Foroosh, “通过动态对象标志发现学习结构表示用于草图识别和检索”，*TIP*，2019年。'
- en: '[93] O. Seddati, S. Dupont, and S. Mahmoudi, “Deepsketch2image: deep convolutional
    neural networks for partial sketch recognition and image retrieval,” in *MM*,
    2016.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] O. Seddati, S. Dupont, 和 S. Mahmoudi, “Deepsketch2image: 深度卷积神经网络用于部分草图识别和图像检索”，在
    *MM*，2016年。'
- en: '[94] K. Zhang, W. Luo, L. Ma, and H. Li, “Cousin network guided sketch recognition
    via latent attribute warehouse,” in *AAAI*, 2019.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] K. Zhang, W. Luo, L. Ma, 和 H. Li, “通过潜在属性仓库引导的表亲网络草图识别”，在 *AAAI*，2019年。'
- en: '[95] X. Zhang, Y. Huang, Q. Zou, Y. Pei, R. Zhang, and S. Wang, “A hybrid convolutional
    neural network for sketch recognition,” *PRL*, 2020.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] X. Zhang, Y. Huang, Q. Zou, Y. Pei, R. Zhang, 和 S. Wang, “一种混合卷积神经网络用于草图识别”，*PRL*，2020年。'
- en: '[96] G. Jain, S. Chopra, S. Chopra, and A. S. Parihar, “Transsketchnet: Attention-based
    sketch recognition using transformers,” in *European Conference on Artificial
    Intelligence*, 2020.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] G. Jain, S. Chopra, S. Chopra, 和 A. S. Parihar, “Transsketchnet: 基于注意力的变换器草图识别”，在
    *欧洲人工智能会议*，2020年。'
- en: '[97] J. Jiao, Y. Cao, M. Lau, and R. Lau, “Tactile sketch saliency,” in *MM*,
    2020.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] J. Jiao, Y. Cao, M. Lau, 和 R. Lau, “触觉草图显著性”，在 *MM*，2020年。'
- en: '[98] Q. Jia, X. Fan, M. Yu, Y. Liu, D. Wang, and L. J. Latecki, “Coupling deep
    textural and shape features for sketch recognition,” in *MM*, 2020.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Q. Jia, X. Fan, M. Yu, Y. Liu, D. Wang, 和 L. J. Latecki, “结合深度纹理和形状特征进行草图识别”，在
    *MM*，2020年。'
- en: '[99] R. K. Sarvadevabhatla, J. Kundu *et al.*, “Enabling my robot to play pictionary:
    Recurrent neural networks for sketch recognition,” in *MM*, 2016.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] R. K. Sarvadevabhatla, J. Kundu *等*，“让我的机器人玩猜画游戏: 循环神经网络用于草图识别”，在 *MM*，2016年。'
- en: '[100] P. Xu, Y. Huang, T. Yuan, T. Xiang, T. M. Hospedales, Y.-Z. Song, and
    L. Wang, “On learning semantic representations for million-scale free-hand sketches,”
    *arXiv preprint arXiv:2007.04101*, 2020.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] P. Xu, Y. Huang, T. Yuan, T. Xiang, T. M. Hospedales, Y.-Z. Song, 和 L.
    Wang, “关于学习百万规模自由手绘草图的语义表示”，*arXiv 预印本 arXiv:2007.04101*，2020年。'
- en: '[101] Q. Jia, M. Yu, X. Fan, and H. Li, “Sequential dual deep learning with
    shape and texture features for sketch recognition,” *arXiv preprint arXiv:1708.02716*,
    2017.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Q. Jia, M. Yu, X. Fan, 和 H. Li, “基于形状和纹理特征的序列双重深度学习用于草图识别”，*arXiv 预印本
    arXiv:1708.02716*，2017年。'
- en: '[102] J.-Y. He, X. Wu, Y.-G. Jiang, B. Zhao, and Q. Peng, “Sketch recognition
    with deep visual-sequential fusion model,” in *MM*, 2017.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J.-Y. He, X. Wu, Y.-G. Jiang, B. Zhao, 和 Q. Peng, “使用深度视觉-序列融合模型进行草图识别”，在
    *MM*，2017年。'
- en: '[103] A. Prabhu, V. Batchu, S. A. Munagala, R. Gajawada, and A. Namboodiri,
    “Distribution-aware binarization of neural networks for sketch recognition,” in
    *WACV*, 2018.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] A. Prabhu, V. Batchu, S. A. Munagala, R. Gajawada, 和 A. Namboodiri, “面向草图识别的神经网络分布感知二值化”，在
    *WACV*，2018年。'
- en: '[104] L. Li, C. Zou, Y. Zheng, Q. Su, H. Fu, and C.-L. Tai, “Sketch-r2cnn:
    An attentive network for vector sketch recognition,” *arXiv preprint arXiv:1811.08170*,
    2018.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] L. Li, C. Zou, Y. Zheng, Q. Su, H. Fu, 和 C.-L. Tai, “Sketch-r2cnn: 一种用于矢量草图识别的注意力网络”，*arXiv
    预印本 arXiv:1811.08170*，2018年。'
- en: '[105] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *NeurIPS*, 2012.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “利用深度卷积神经网络进行Imagenet分类”，在
    *NeurIPS*，2012年。'
- en: '[106] D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun, “Bayesian face revisited:
    A joint formulation,” in *ECCV*, 2012.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] D. Chen, X. Cao, L. Wang, F. Wen, 和 J. Sun, “贝叶斯面孔再探: 一种联合公式”，在 *ECCV*，2012年。'
- en: '[107] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder-decoder for
    statistical machine translation,” *arXiv preprint arXiv:1406.1078*, 2014.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H.
    Schwenk, 和 Y. Bengio, “使用 RNN 编码器-解码器学习短语表示用于统计机器翻译”，*arXiv 预印本 arXiv:1406.1078*，2014年。'
- en: '[108] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] K. He, X. Zhang, S. Ren 和 J. Sun，“用于图像识别的深度残差学习，” 在 *CVPR*，2016。'
- en: '[109] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu, “Pixel recurrent
    neural networks,” *arXiv preprint arXiv:1601.06759*, 2016.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] A. v. d. Oord, N. Kalchbrenner 和 K. Kavukcuoglu，“像素递归神经网络，” *arXiv 预印本
    arXiv:1601.06759*，2016。'
- en: '[110] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” *arXiv
    preprint arXiv:1312.6114*, 2013.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] D. P. Kingma 和 M. Welling，“自编码变分贝叶斯，” *arXiv 预印本 arXiv:1312.6114*，2013。'
- en: '[111] P. Xu, Z. Song, Q. Yin, Y.-Z. Song, and L. Wang, “Deep self-supervised
    representation learning for free-hand sketch,” *arXiv preprint arXiv:2002.00867*,
    2020.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] P. Xu, Z. Song, Q. Yin, Y.-Z. Song 和 L. Wang，“用于自由手草图的深度自监督表示学习，” *arXiv
    预印本 arXiv:2002.00867*，2020。'
- en: '[112] R. O. Duda, P. E. Hart, and D. G. Stork, *Pattern classification*.   John
    Wiley & Sons, 2012.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] R. O. Duda, P. E. Hart 和 D. G. Stork，*模式分类*。 约翰·威利父子公司，2012。'
- en: '[113] A. Mishra and A. K. Singh, “Deep embedding using bayesian risk minimization
    with application to sketch recognition,” in *ACCV*, 2018.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] A. Mishra 和 A. K. Singh，“通过贝叶斯风险最小化进行深度嵌入，应用于草图识别，” 在 *ACCV*，2018。'
- en: '[114] C. Cortes and V. Vapnik, “Support-vector networks,” *Machine learning*,
    1995.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] C. Cortes 和 V. Vapnik，“支持向量网络，” *机器学习*，1995。'
- en: '[115] A. K. Bhunia, P. N. Chowdhury, Y. Yang, T. M. Hospedales, T. Xiang, and
    Y.-Z. Song, “Vectorization and rasterization: Self-supervised learning for sketch
    and handwriting,” in *CVPR*, 2021.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] A. K. Bhunia, P. N. Chowdhury, Y. Yang, T. M. Hospedales, T. Xiang 和
    Y.-Z. Song，“向量化和栅格化：用于草图和手写的自监督学习，” 在 *CVPR*，2021。'
- en: '[116] C. Pan, J. Huang, J. Gong, and C. Chen, “Teach machine to learn: hand-drawn
    multi-symbol sketch recognition in one-shot,” *Applied Intelligence*, 2020.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] C. Pan, J. Huang, J. Gong 和 C. Chen，“教机器学习：手绘多符号草图识别一次性完成，” *应用智能*，2020。'
- en: '[117] F. Liu, X. Deng, Y.-K. Lai, Y.-J. Liu, C. Ma, and H. Wang, “Sketchgan:
    Joint sketch completion and recognition with generative adversarial network,”
    in *CVPR*, 2019.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] F. Liu, X. Deng, Y.-K. Lai, Y.-J. Liu, C. Ma 和 H. Wang，“Sketchgan：使用生成对抗网络的联合草图完成与识别，”
    在 *CVPR*，2019。'
- en: '[118] F. Wang and Y. Li, “Spatial matching of sketches without point correspondence,”
    in *ICIP*, 2015.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] F. Wang 和 Y. Li，“无需点对应的草图空间匹配，” 在 *ICIP*，2015。'
- en: '[119] A. Creswell and A. A. Bharath, “Adversarial training for sketch retrieval,”
    in *ECCV*, 2016.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] A. Creswell 和 A. A. Bharath，“草图检索的对抗训练，” 在 *ECCV*，2016。'
- en: '[120] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
    *IJCV*, 2004.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] D. G. Lowe，“来自尺度不变关键点的独特图像特征，” *IJCV*，2004。'
- en: '[121] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *NeurIPS*, 2014.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville 和 Y. Bengio，“生成对抗网络，” 在 *NeurIPS*，2014。'
- en: '[122] S. Balasubramanian, V. N. Balasubramanian *et al.*, “Teaching gans to
    sketch in vector format,” *arXiv preprint arXiv:1904.03620*, 2019.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] S. Balasubramanian, V. N. Balasubramanian *等*，“教 GANs 以矢量格式绘制，” *arXiv
    预印本 arXiv:1904.03620*，2019。'
- en: '[123] L. S. F. Ribeiro, T. Bui, J. Collomosse, and M. Ponti, “Sketchformer:
    Transformer-based representation for sketched structure,” in *CVPR*, 2020.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] L. S. F. Ribeiro, T. Bui, J. Collomosse 和 M. Ponti，“Sketchformer：基于变换器的草图结构表示，”
    在 *CVPR*，2020。'
- en: '[124] H. Lin, Y. Fu, X. Xue, and Y.-G. Jiang, “Sketch-bert: Learning sketch
    bidirectional encoder representation from transformers by self-supervised learning
    of sketch gestalt,” in *CVPR*, 2020.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] H. Lin, Y. Fu, X. Xue 和 Y.-G. Jiang，“Sketch-bert：通过自监督学习 sketch gestalt
    从变换器中学习草图双向编码器表示，” 在 *CVPR*，2020。'
- en: '[125] T. Zhou, C. Fang, Z. Wang, J. Yang, B. Kim, Z. Chen, J. Brandt, and D. Terzopoulos,
    “Learning to doodle with deep q-networks and demonstrated strokes,” in *BMVC*,
    2018.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] T. Zhou, C. Fang, Z. Wang, J. Yang, B. Kim, Z. Chen, J. Brandt 和 D. Terzopoulos，“通过深度
    Q 网络和演示笔画学习涂鸦，” 在 *BMVC*，2018。'
- en: '[126] N. Jaques, J. McCleary, J. Engel, D. Ha, F. Bertsch, R. Picard, and D. Eck,
    “Learning via social awareness: Improving a deep generative sketching model with
    facial feedback,” *arXiv preprint arXiv:1802.04877*, 2018.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] N. Jaques, J. McCleary, J. Engel, D. Ha, F. Bertsch, R. Picard 和 D. Eck，“通过社交意识学习：通过面部反馈改进深度生成草图模型，”
    *arXiv 预印本 arXiv:1802.04877*，2018。'
- en: '[127] K. Sasaki and T. Ogata, “Adaptive drawing behavior by visuomotor learning
    using recurrent neural networks,” *IEEE Transactions on Cognitive and Developmental
    Systems*, 2019.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] K. Sasaki 和 T. Ogata，“通过使用递归神经网络的视运动学习实现适应性绘图行为，” *IEEE 认知与发展系统学报*，2019。'
- en: '[128] J. Li, N. Gao, T. Shen, W. Zhang, T. Mei, and H. Ren, “Sketchman: Learning
    to create professional sketches,” in *MM*, 2020.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] J. Li, N. Gao, T. Shen, W. Zhang, T. Mei 和 H. Ren, “Sketchman：学习创建专业草图，”
    发表在 *MM*，2020年。'
- en: '[129] S. Ge, V. Goswami, L. Zitnick, and D. Parikh, “Creative sketch generation,”
    in *ICLR*, 2021.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] S. Ge, V. Goswami, L. Zitnick 和 D. Parikh, “创造性草图生成，” 发表在 *ICLR*，2021年。'
- en: '[130] A. Das, Y. Yang, T. Hospedales, T. Xiang, and Y.-Z. Song, “Béziersketch:
    A generative model for scalable vector sketches,” in *ECCV*, 2020.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] A. Das, Y. Yang, T. Hospedales, T. Xiang 和 Y.-Z. Song, “Béziersketch：用于可扩展矢量草图的生成模型，”
    发表在 *ECCV*，2020年。'
- en: '[131] A. K. Bhunia, A. Das, U. R. Muhammad, Y. Yang, T. M. Hospedales, T. Xiang,
    Y. Gryaditskaya, and Y.-Z. Song, “Pixelor: A competitive sketching ai agent. so
    you think you can beat me?” *ACM Transactions on Graphics*, vol. 39, no. 6, 2020.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] A. K. Bhunia, A. Das, U. R. Muhammad, Y. Yang, T. M. Hospedales, T. Xiang,
    Y. Gryaditskaya 和 Y.-Z. Song, “Pixelor：一个竞争性的草图AI代理。你觉得你能打败我吗？” *ACM图形学论文*，第39卷，第6期，2020年。'
- en: '[132] G. Hinton and V. Nair, “Inferring motor programs from images of handwritten
    digits,” in *NeurIPS*, 2005.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] G. Hinton 和 V. Nair, “从手写数字图像中推断运动程序，” 发表在 *NeurIPS*，2005年。'
- en: '[133] Y. Li, Y.-Z. Song, T. M. Hospedales, and S. Gong, “Free-hand sketch synthesis
    with deformable stroke models,” *IJCV*, 2017.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Y. Li, Y.-Z. Song, T. M. Hospedales 和 S. Gong, “利用可变形笔画模型进行自由手绘草图合成，”
    *IJCV*，2017年。'
- en: '[134] A. Jenal, N. Savinov, T. Sattler, and G. Chaurasia, “Rnn-based generative
    model for fine-grained sketching,” *arXiv preprint arXiv:1901.03991*, 2019.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] A. Jenal, N. Savinov, T. Sattler 和 G. Chaurasia, “基于RNN的细粒度草图生成模型，” *arXiv预印本
    arXiv:1901.03991*，2019年。'
- en: '[135] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” *arXiv preprint
    arXiv:1810.04805*, 2018.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] J. Devlin, M.-W. Chang, K. Lee 和 K. Toutanova, “Bert：深度双向变换器的预训练用于语言理解，”
    *arXiv预印本 arXiv:1810.04805*，2018年。'
- en: '[136] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,”
    *TSP*, 1997.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] M. Schuster 和 K. K. Paliwal, “双向递归神经网络，” *TSP*，1997年。'
- en: '[137] N. Cao, X. Yan, Y. Shi, and C. Chen, “Ai-sketcher: A deep generative
    model for producing high-quality sketches,” in *AAAI*, 2019.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] N. Cao, X. Yan, Y. Shi 和 C. Chen, “Ai-sketcher：用于生成高质量草图的深度生成模型，” 发表在
    *AAAI*，2019年。'
- en: '[138] N. Zheng, Y. Jiang, and D. Huang, “Strokenet: A neural painting environment,”
    in *ICLR*, 2019.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] N. Zheng, Y. Jiang 和 D. Huang, “Strokenet：一个神经绘画环境，” 发表在 *ICLR*，2019年。'
- en: '[139] Y. Ganin, T. Kulkarni, I. Babuschkin, S. M. A. Eslami, and O. Vinyals,
    “Synthesizing programs for images using reinforced adversarial learning,” in *ICML*,
    2018.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Y. Ganin, T. Kulkarni, I. Babuschkin, S. M. A. Eslami 和 O. Vinyals, “使用强化对抗学习合成图像程序，”
    发表在 *ICML*，2018年。'
- en: '[140] J. F. Mellor, E. Park, Y. Ganin, I. Babuschkin, T. Kulkarni, D. Rosenbaum,
    A. Ballard, T. Weber, O. Vinyals, and S. Eslami, “Unsupervised doodling and painting
    with improved spiral,” *arXiv preprint arXiv:1910.01007*, 2019.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] J. F. Mellor, E. Park, Y. Ganin, I. Babuschkin, T. Kulkarni, D. Rosenbaum,
    A. Ballard, T. Weber, O. Vinyals 和 S. Eslami, “使用改进的螺旋进行无监督涂鸦和绘画，” *arXiv预印本 arXiv:1910.01007*，2019年。'
- en: '[141] K. Ellis, D. Ritchie, A. Solar-Lezama, and J. B. Tenenbaum, “Learning
    to infer graphics programs from hand-drawn images,” in *NeurIPS*, 2018.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] K. Ellis, D. Ritchie, A. Solar-Lezama 和 J. B. Tenenbaum, “从手绘图像中推断图形程序的学习，”
    发表在 *NeurIPS*，2018年。'
- en: '[142] V. Egiazarian, O. Voynov, A. Artemov, D. Volkhonskiy, A. Safin, M. Taktasheva,
    D. Zorin, and E. Burnaev, “Deep vectorization of technical drawings,” *arXiv preprint
    arXiv:2003.05471*, 2020.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] V. Egiazarian, O. Voynov, A. Artemov, D. Volkhonskiy, A. Safin, M. Taktasheva,
    D. Zorin 和 E. Burnaev, “技术绘图的深度矢量化，” *arXiv预印本 arXiv:2003.05471*，2020年。'
- en: '[143] S. Wieluch and F. Schwenker, “Strokecoder: Path-based image generation
    from single examples using transformers,” *arXiv preprint arXiv:2003.11958*, 2020.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] S. Wieluch 和 F. Schwenker, “Strokecoder：基于路径的图像生成，通过单个示例使用变换器，” *arXiv预印本
    arXiv:2003.11958*，2020年。'
- en: '[144] J. P. Collomosse, G. McNeill, and L. Watts, “Free-hand sketch grouping
    for video retrieval,” in *ICPR*, 2008.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] J. P. Collomosse, G. McNeill 和 L. Watts, “用于视频检索的自由手绘草图分组，” 发表在 *ICPR*，2008年。'
- en: '[145] J. Wagemans, J. H. Elder, M. Kubovy, S. E. Palmer, M. A. Peterson, M. Singh,
    and R. von der Heydt, “A century of gestalt psychology in visual perception: I.
    perceptual grouping and figure–ground organization.” *Psychological bulletin*,
    2012.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] J. Wagemans, J. H. Elder, M. Kubovy, S. E. Palmer, M. A. Peterson, M.
    Singh 和 R. von der Heydt, “视觉感知中的格式塔心理学：I. 知觉分组和图形–背景组织。” *心理学通报*，2012年。'
- en: '[146] K. Koffka, *Principles of Gestalt psychology*.   Routledge, 2013.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] K. Koffka, *格式塔心理学原理*。 Routledge，2013年。'
- en: '[147] Z. Sun, C. Wang, L. Zhang, and L. Zhang, “Free hand-drawn sketch segmentation,”
    in *ECCV*, 2012.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Z. Sun, C. Wang, L. Zhang 和 L. Zhang, “自由手绘草图分割，” 发表在 *ECCV*，2012年。'
- en: '[148] Y. Qi, J. Guo, Y.-Z. Song, T. Xiang, H. Zhang, and Z.-H. Tan, “Im2sketch:
    Sketch generation by unconflicted perceptual grouping,” *Neurocomputing*, 2015.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Y. Qi, J. Guo, Y.-Z. Song, T. Xiang, H. Zhang, 和 Z.-H. Tan，“Im2sketch：通过无冲突的感知分组生成草图”，发表于*Neurocomputing*，2015年。'
- en: '[149] Y. Qi, Y.-Z. Song, T. Xiang, H. Zhang, T. Hospedales, Y. Li, and J. Guo,
    “Making better use of edges via perceptual grouping,” in *CVPR*, 2015.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Y. Qi, Y.-Z. Song, T. Xiang, H. Zhang, T. Hospedales, Y. Li, 和 J. Guo，“通过感知分组更好地利用边缘”，发表于*CVPR*，2015年。'
- en: '[150] X. Liu, T.-T. Wong, and P.-A. Heng, “Closure-aware sketch simplification,”
    *TOG*, 2015.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] X. Liu, T.-T. Wong, 和 P.-A. Heng，“闭合感知草图简化”，发表于*TOG*，2015年。'
- en: '[151] X. Wang, X. Chen, and Z. Zha, “Sketchpointnet: A compact network for
    robust sketch recognition,” in *ICIP*, 2018.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] X. Wang, X. Chen, 和 Z. Zha，“Sketchpointnet：一种紧凑的网络用于鲁棒的草图识别”，发表于*ICIP*，2018年。'
- en: '[152] K. Li, K. Pang, Y.-Z. Song, T. Xiang, T. M. Hospedales, and H. Zhang,
    “Toward deep universal sketch perceptual grouper,” *TIP*, 2019.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] K. Li, K. Pang, Y.-Z. Song, T. Xiang, T. M. Hospedales, 和 H. Zhang，“面向深度通用草图感知分组器”，发表于*TIP*，2019年。'
- en: '[153] Z. Huang, H. Fu, and R. W. Lau, “Data-driven segmentation and labeling
    of freehand sketches,” *TOG*, 2014.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Z. Huang, H. Fu, 和 R. W. Lau，“基于数据的自由手绘草图分割与标注”，发表于*TOG*，2014年。'
- en: '[154] R. G. Schneider and T. Tuytelaars, “Example-based sketch segmentation
    and labeling using crfs,” *TOG*, 2016.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] R. G. Schneider 和 T. Tuytelaars，“基于示例的草图分割与标注使用 CRFs”，发表于*TOG*，2016年。'
- en: '[155] B. Kim, O. Wang, A. C. Öztireli, and M. Gross, “Semantic segmentation
    for line drawing vectorization using neural networks,” in *Computer Graphics Forum*,
    2018.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] B. Kim, O. Wang, A. C. Öztireli, 和 M. Gross，“基于神经网络的线条绘图矢量化的语义分割”，发表于*Computer
    Graphics Forum*，2018年。'
- en: '[156] K. Kaiyrbekov and M. Sezgin, “Stroke-based sketched symbol reconstruction
    and segmentation,” *arXiv preprint arXiv:1901.03427*, 2019.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] K. Kaiyrbekov 和 M. Sezgin，“基于笔画的草图符号重建与分割”，发表于*arXiv preprint arXiv:1901.03427*，2019年。'
- en: '[157] X. Wu, Y. Qi, J. Liu, and J. Yang, “Sketchsegnet: A rnn model for labeling
    sketch strokes,” in *IEEE International Workshop on Machine Learning for Signal
    Processing (MLSP)*, 2018.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] X. Wu, Y. Qi, J. Liu, 和 J. Yang，“Sketchsegnet：一种用于标注草图笔画的 RNN 模型”，发表于*IEEE
    International Workshop on Machine Learning for Signal Processing (MLSP)*，2018年。'
- en: '[158] L. Li, H. Fu, and C.-L. Tai, “Fast sketch segmentation and labeling with
    deep learning,” *IEEE computer graphics and applications*, 2018.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] L. Li, H. Fu, 和 C.-L. Tai，“基于深度学习的快速草图分割与标注”，发表于*IEEE computer graphics
    and applications*，2018年。'
- en: '[159] L. Yi, V. G. Kim, D. Ceylan, I. Shen, M. Yan, H. Su, C. Lu, Q. Huang,
    A. Sheffer, L. Guibas *et al.*, “A scalable active framework for region annotation
    in 3d shape collections,” *TOG*, 2016.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] L. Yi, V. G. Kim, D. Ceylan, I. Shen, M. Yan, H. Su, C. Lu, Q. Huang,
    A. Sheffer, L. Guibas *等*，“一个可扩展的活动框架用于 3D 形状集合中的区域标注”，发表于*TOG*，2016年。'
- en: '[160] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,
    “Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *TPAMI*, 2017.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, 和 A. L. Yuille，“Deeplab：基于深度卷积网络、空洞卷积和全连接
    CRFs 的语义图像分割”，发表于*TPAMI*，2017年。'
- en: '[161] C. Wang, B. Yang, and Y. Liao, “Unsupervised image segmentation using
    convolutional autoencoder with total variation regularization as preprocessing,”
    in *ICASSP*, 2017.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] C. Wang, B. Yang, 和 Y. Liao，“利用卷积自编码器与全变差正则化进行无监督图像分割”，发表于*ICASSP*，2017年。'
- en: '[162] L. Donati, S. Cesano, and A. Prati, “An accurate system for fashion hand-drawn
    sketches vectorization,” in *ICCV Workshops*, 2017.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] L. Donati, S. Cesano, 和 A. Prati，“一个准确的时尚手绘草图矢量化系统”，发表于*ICCV Workshops*，2017年。'
- en: '[163] J. Chen, M. Du, X. Qin, and Y. Miao, “An improved topology extraction
    approach for vectorization of sketchy line drawings,” *The Visual Computer*, 2018.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] J. Chen, M. Du, X. Qin, 和 Y. Miao，“一种改进的拓扑提取方法用于草图线条绘制的矢量化”，发表于*The Visual
    Computer*，2018年。'
- en: '[164] Y. Guo, Z. Zhang, C. Han, W. Hu, C. Li, and T.-T. Wong, “Deep line drawing
    vectorization via line subdivision and topology reconstruction,” in *Computer
    Graphics Forum*, 2019.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Y. Guo, Z. Zhang, C. Han, W. Hu, C. Li, 和 T.-T. Wong，“通过线条细分和拓扑重建的深度线条绘图矢量化”，发表于*Computer
    Graphics Forum*，2019年。'
- en: '[165] L. Donati, S. Cesano, and A. Prati, “A complete hand-drawn sketch vectorization
    framework,” *Multimedia Tools and Applications*, 2019.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] L. Donati, S. Cesano, 和 A. Prati，“完整的手绘草图矢量化框架”，发表于*Multimedia Tools
    and Applications*，2019年。'
- en: '[166] T. Stanko, M. Bessmeltsev, D. Bommes, and A. Bousseau, “Integer-grid
    sketch simplification and vectorization,” in *Computer Graphics Forum*, 2020.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] T. Stanko, M. Bessmeltsev, D. Bommes, 和 A. Bousseau，“整数网格草图简化与矢量化”，发表于*Computer
    Graphics Forum*，2020年。'
- en: '[167] A. D. Parakkat, M.-P. R. Cani, and K. Singh, “Color by numbers: Interactive
    structuring and vectorization of sketch imagery,” in *CHI*, 2021.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] A. D. Parakkat, M.-P. R. Cani 和 K. Singh， “按数字上色：交互式结构化和矢量化素描图像，” 发表在
    *CHI*，2021年。'
- en: '[168] R. K. Sarvadevabhatla, I. Dwivedi, A. Biswas, S. Manocha *et al.*, “Sketchparse:
    Towards rich descriptions for poorly drawn sketches using multi-task hierarchical
    deep networks,” in *MM*, 2017.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] R. K. Sarvadevabhatla, I. Dwivedi, A. Biswas, S. Manocha *等*， “Sketchparse：使用多任务层次深度网络为绘制不佳的素描提供丰富描述，”
    发表在 *MM*，2017年。'
- en: '[169] J. Jiang, R. Wang, S. Lin, and F. Wang, “Sfsegnet: Parse freehand sketches
    using deep fully convolutional networks,” in *IJCNN*, 2019.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] J. Jiang, R. Wang, S. Lin 和 F. Wang， “Sfsegnet：使用深度全卷积网络解析手绘素描，” 发表在
    *IJCNN*，2019年。'
- en: '[170] K. Mukherjee, R. X. Hawkins, and J. E. Fan, “Communicating semantic part
    information in drawings,” in *Annual Conference of the Cognitive Science Society*,
    2019.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] K. Mukherjee, R. X. Hawkins 和 J. E. Fan， “在绘图中传达语义部件信息，” 发表在 *认知科学学会年会*，2019年。'
- en: '[171] Y. Zheng, H. Yao, and X. Sun, “Deep semantic parsing of freehand sketches
    with homogeneous transformation, soft-weighted loss, and staged learning,” *arXiv
    preprint arXiv:1910.06023*, 2019.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Y. Zheng, H. Yao 和 X. Sun， “通过同质变换、软加权损失和分阶段学习对手绘素描进行深度语义解析，” *arXiv
    预印本 arXiv:1910.06023*，2019年。'
- en: '[172] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
    for semantic segmentation,” in *CVPR*, 2015.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] J. Long, E. Shelhamer 和 T. Darrell， “用于语义分割的全卷积网络，” 发表在 *CVPR*，2015年。'
- en: '[173] X. Hilaire and K. Tombre, “Robust and accurate vectorization of line
    drawings,” *TPAMI*, 2006.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] X. Hilaire 和 K. Tombre， “线条绘图的鲁棒且准确的矢量化，” *TPAMI*，2006年。'
- en: '[174] Y. Chien, W.-C. Lin, T.-S. Huang, and J.-H. Chuang, “Line drawing simplification
    by stroke translation and combination,” in *ICGIP*, 2014.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Y. Chien, W.-C. Lin, T.-S. Huang 和 J.-H. Chuang， “通过笔划转换和组合简化线条绘图，” 发表在
    *ICGIP*，2014年。'
- en: '[175] T. Ogawa, Y. Matsui, T. Yamasaki, and K. Aizawa, “Sketch simplification
    by classifying strokes,” in *ICPR*, 2016.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] T. Ogawa, Y. Matsui, T. Yamasaki 和 K. Aizawa， “通过分类笔划简化素描，” 发表在 *ICPR*，2016年。'
- en: '[176] P. Barla, J. Thollot, and F. X. Sillion, “Geometric clustering for line
    drawing simplification,” in *TOG*, 2005.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] P. Barla, J. Thollot 和 F. X. Sillion， “用于线条绘图简化的几何聚类，” 发表在 *TOG*，2005年。'
- en: '[177] E. Simo-Serra, S. Iizuka, and H. Ishikawa, “Mastering sketching: adversarial
    augmentation for structured prediction,” *TOG*, 2018.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] E. Simo-Serra, S. Iizuka 和 H. Ishikawa， “掌握素描：用于结构化预测的对抗性增强，” *TOG*，2018年。'
- en: '[178] X. Xu, M. Xie, P. Miao, W. Qu, W. Xiao, H. Zhang, X. Liu, and T.-T. Wong,
    “Perceptual-aware sketch simplification based on integrated vgg layers,” *TVCG*,
    2019.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] X. Xu, M. Xie, P. Miao, W. Qu, W. Xiao, H. Zhang, X. Liu 和 T.-T. Wong，
    “基于集成 vgg 层的感知意识素描简化，” *TVCG*，2019年。'
- en: '[179] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] K. Simonyan 和 A. Zisserman， “用于大规模图像识别的非常深层卷积网络，” *arXiv 预印本 arXiv:1409.1556*，2014年。'
- en: '[180] R. K. Sarvadevabhatla *et al.*, “Eye of the dragon: Exploring discriminatively
    minimalist sketch-based abstractions for object categories,” in *MM*, 2015.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] R. K. Sarvadevabhatla *等*， “龙之眼：探索针对物体类别的判别性简约素描抽象，” 发表在 *MM*，2015年。'
- en: '[181] U. R. Muhammad, Y. Yang, T. M. Hospedales, T. Xiang, and Y.-Z. Song,
    “Goal-driven sequential data abstraction,” in *ICCV*, 2019.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] U. R. Muhammad, Y. Yang, T. M. Hospedales, T. Xiang 和 Y.-Z. Song， “目标驱动的序列数据抽象，”
    发表在 *ICCV*，2019年。'
- en: '[182] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “Imagenet large
    scale visual recognition challenge,” *IJCV*, 2015.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg 和 L. Fei-Fei， “Imagenet 大规模视觉识别挑战，”
    *IJCV*，2015年。'
- en: '[183] R. K. Sarvadevabhatla and R. V. Babu, “Freehand sketch recognition using
    deep features,” *arXiv preprint arXiv:1502.00254*, 2015.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] R. K. Sarvadevabhatla 和 R. V. Babu， “使用深度特征的手绘素描识别，” *arXiv 预印本 arXiv:1502.00254*，2015年。'
- en: '[184] B. Graham, “Spatially-sparse convolutional neural networks,” *arXiv preprint
    arXiv:1409.6070*, 2014.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] B. Graham， “空间稀疏卷积神经网络，” *arXiv 预印本 arXiv:1409.6070*，2014年。'
- en: '[185] Y. Zheng, H. Yao, X. Sun, S. Zhang, S. Zhao, and F. Porikli, “Sketch-specific
    data augmentation for freehand sketch recognition,” *arXiv preprint arXiv:1910.06038*,
    2019.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Y. Zheng, H. Yao, X. Sun, S. Zhang, S. Zhao 和 F. Porikli， “针对手绘素描识别的素描特定数据增强，”
    *arXiv 预印本 arXiv:1910.06038*，2019年。'
- en: '[186] R. Liu, Q. Yu, and S. Yu, “An unpaired sketch-to-photo translation model,”
    *arXiv preprint arXiv:1909.08313*, 2019.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] R. Liu, Q. Yu 和 S. Yu， “一个无配对素描到照片的转换模型，” *arXiv 预印本 arXiv:1909.08313*，2019年。'
- en: '[187] Y.-P. Tan, S. R. Kulkarni, and P. J. Ramadge, “A framework for measuring
    video similarity and its application to video query by example,” in *ICIP*, 1999.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Y.-P. Tan, S. R. Kulkarni, 和 P. J. Ramadge，“视频相似度测量框架及其在视频示例查询中的应用，”发表于*ICIP*，1999年。'
- en: '[188] Y. Matsui, “Challenge for manga processing: Sketch-based manga retrieval,”
    in *MM*, 2015.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Y. Matsui，“漫画处理的挑战：基于草图的漫画检索，”发表于*MM*，2015年。'
- en: '[189] Y. Li, T. M. Hospedales, Y.-Z. Song, and S. Gong, “Fine-grained sketch-based
    image retrieval by matching deformable part models,” in *BMVC*, 2014.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Y. Li, T. M. Hospedales, Y.-Z. Song, 和 S. Gong，“通过匹配可变形部件模型进行细粒度草图图像检索，”发表于*BMVC*，2014年。'
- en: '[190] P. Xu, K. Li, Z. Ma, Y.-Z. Song, L. Wang, and J. Guo, “Cross-modal subspace
    learning for sketch-based image retrieval: A comparative study,” in *IC-NIDC*,
    2016.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] P. Xu, K. Li, Z. Ma, Y.-Z. Song, L. Wang, 和 J. Guo，“基于草图的图像检索的跨模态子空间学习：比较研究，”发表于*IC-NIDC*，2016年。'
- en: '[191] P. Xu, Q. Yin, Y. Huang, Y.-Z. Song, Z. Ma, L. Wang, T. Xiang, W. B.
    Kleijn, and J. Guo, “Cross-modal subspace learning for fine-grained sketch-based
    image retrieval,” *Neurocomputing*, 2018.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] P. Xu, Q. Yin, Y. Huang, Y.-Z. Song, Z. Ma, L. Wang, T. Xiang, W. B.
    Kleijn, 和 J. Guo，“用于细粒度草图图像检索的跨模态子空间学习，”*Neurocomputing*，2018年。'
- en: '[192] U. Chaudhuri, B. Banerjee, A. Bhattacharya, and M. Datcu, “Crossatnet-a
    novel cross-attention based framework for sketch-based image retrieval,” *Image
    and Vision Computing*, 2020.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] U. Chaudhuri, B. Banerjee, A. Bhattacharya, 和 M. Datcu，“Crossatnet——一种基于交叉注意力的草图图像检索新框架，”*Image
    and Vision Computing*，2020年。'
- en: '[193] F. Yang, Y. Wu, Z. Wang, X. Li, S. Sakti, and S. Nakamura, “Instance-level
    heterogeneous domain adaptation for limited-labeled sketch-to-photo retrieval,”
    *TMM*, 2020.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] F. Yang, Y. Wu, Z. Wang, X. Li, S. Sakti, 和 S. Nakamura，“实例级异构领域适应用于有限标记草图到照片检索，”*TMM*，2020年。'
- en: '[194] A. Fuentes and J. M. Saavedra, “Sketch-qnet: A quadruplet convnet for
    color sketch-based image retrieval,” in *CVPR Workshops*, 2021.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] A. Fuentes 和 J. M. Saavedra，“Sketch-qnet：一种用于彩色草图图像检索的四重卷积网络，”发表于*CVPR
    Workshops*，2021年。'
- en: '[195] P. Torres and J. M. Saavedra, “Compact and effective representations
    for sketch-based image retrieval,” in *CVPR Workshops*, 2021.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] P. Torres 和 J. M. Saavedra，“用于基于草图的图像检索的紧凑有效表示，”发表于*CVPR Workshops*，2021年。'
- en: '[196] T. Bui, L. Ribeiro, M. Ponti, and J. Collomosse, “Deep manifold alignment
    for mid-grain sketch based image retrieval,” in *ACCV*, 2018.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] T. Bui, L. Ribeiro, M. Ponti, 和 J. Collomosse，“中粒度草图图像检索的深度流形对齐，”发表于*ACCV*，2018年。'
- en: '[197] L. Zheng, Y. Yang, and Q. Tian, “Sift meets cnn: A decade survey of instance
    retrieval,” *TPAMI*, 2017.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] L. Zheng, Y. Yang, 和 Q. Tian，“Sift meets cnn：实例检索的十年回顾，”*TPAMI*，2017年。'
- en: '[198] S.-i. Kondo, M. Toyoura, and X. Mao, “Sketch based skirt image retrieval,”
    in *Proceedings of the 4th Joint Symposium on Computational Aesthetics, Non-Photorealistic
    Animation and Rendering, and Sketch-Based Interfaces and Modeling*, 2014.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] S.-i. Kondo, M. Toyoura, 和 X. Mao，“基于草图的裙子图像检索，”发表于*第4届计算美学、非写实动画与渲染以及基于草图的接口与建模联合研讨会*，2014年。'
- en: '[199] S. D. Bhattacharjee, J. Yuan, W. Hong, and X. Ruan, “Query adaptive instance
    search using object sketches,” in *MM*, 2016.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] S. D. Bhattacharjee, J. Yuan, W. Hong, 和 X. Ruan，“基于对象草图的查询自适应实例检索，”发表于*MM*，2016年。'
- en: '[200] J. Lei, K. Zheng, H. Zhang, X. Cao, N. Ling, and Y. Hou, “Sketch based
    image retrieval via image-aided cross domain learning,” in *ICIP*, 2017.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] J. Lei, K. Zheng, H. Zhang, X. Cao, N. Ling, 和 Y. Hou，“通过图像辅助的跨域学习进行基于草图的图像检索，”发表于*ICIP*，2017年。'
- en: '[201] S. D. Bhattacharjee, J. Yuan, Y. Huang, J. Meng, and L. Duan, “Query
    adaptive multiview object instance search and localization using sketches,” *TMM*,
    2018.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] S. D. Bhattacharjee, J. Yuan, Y. Huang, J. Meng, 和 L. Duan，“基于草图的查询自适应多视角对象实例检索与定位，”*TMM*，2018年。'
- en: '[202] J. Canny, “A computational approach to edge detection,” *TPAMI*, 1986.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] J. Canny，“边缘检测的计算方法，”*TPAMI*，1986年。'
- en: '[203] C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals from
    edges,” in *ECCV*, 2014.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] C. L. Zitnick 和 P. Dollár，“边缘盒子：从边缘定位对象提案，”发表于*ECCV*，2014年。'
- en: '[204] S. Xie and Z. Tu, “Holistically-nested edge detection,” in *ICCV*, 2015.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] S. Xie 和 Z. Tu，“全局嵌套边缘检测，”发表于*ICCV*，2015年。'
- en: '[205] S. Chopra, R. Hadsell, Y. LeCun *et al.*, “Learning a similarity metric
    discriminatively, with application to face verification,” in *CVPR*, 2005.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] S. Chopra, R. Hadsell, Y. LeCun *等*，“通过区分学习相似度度量，应用于人脸验证，”发表于*CVPR*，2005年。'
- en: '[206] P. Xu, Q. Yin, Y. Qi, Y.-Z. Song, Z. Ma, L. Wang, and J. Guo, “Instance-level
    coupled subspace learning for fine-grained sketch-based image retrieval,” in *ECCV
    Workshops*, 2016.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] P. Xu, Q. Yin, Y. Qi, Y.-Z. Song, Z. Ma, L. Wang, 和 J. Guo，“实例级耦合子空间学习用于细粒度草图图像检索，”发表于*ECCV
    Workshops*，2016年。'
- en: '[207] Y. Qi, Y.-Z. Song, H. Zhang, and J. Liu, “Sketch-based image retrieval
    via siamese convolutional neural network,” in *ICIP*, 2016.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] Y. Qi, Y.-Z. Song, H. Zhang, 和 J. Liu，“通过孪生卷积神经网络进行基于草图的图像检索”，发表于 *ICIP*，2016年。'
- en: '[208] J. Song, Y.-Z. Song, T. Xiang, T. M. Hospedales, and X. Ruan, “Deep multi-task
    attribute-driven ranking for fine-grained sketch-based image retrieval.” in *BMVC*,
    2016.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] J. Song, Y.-Z. Song, T. Xiang, T. M. Hospedales, 和 X. Ruan，“细粒度草图图像检索的深度多任务属性驱动排序”，发表于
    *BMVC*，2016年。'
- en: '[209] Q. Yu, X. Chang, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “The devil
    is in the middle: Exploiting mid-level representations for cross-domain instance
    matching,” *arXiv preprint arXiv:1711.08106*, 2017.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Q. Yu, X. Chang, Y.-Z. Song, T. Xiang, 和 T. M. Hospedales，“魔鬼在细节中：利用中级表示进行跨领域实例匹配”，*arXiv
    preprint arXiv:1711.08106*，2017年。'
- en: '[210] Y. Yan, X. Wang, X. Yang, X. Bai, and W. Liu, “Joint classification loss
    and histogram loss for sketch-based image retrieval,” in *ICIG*, 2017.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] Y. Yan, X. Wang, X. Yang, X. Bai, 和 W. Liu，“草图图像检索中的联合分类损失和直方图损失”，发表于
    *ICIG*，2017年。'
- en: '[211] J. Collomosse, T. Bui, M. J. Wilber, C. Fang, and H. Jin, “Sketching
    with style: Visual search with sketches and aesthetic context,” in *ICCV*, 2017.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] J. Collomosse, T. Bui, M. J. Wilber, C. Fang, 和 H. Jin，“风格化草图：带有美学背景的视觉搜索”，发表于
    *ICCV*，2017年。'
- en: '[212] J. Song, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “Fine-grained image
    retrieval: the text/sketch input dilemma,” in *BMVC*, 2017.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] J. Song, Y.-Z. Song, T. Xiang, 和 T. M. Hospedales，“细粒度图像检索：文本/草图输入困境”，发表于
    *BMVC*，2017年。'
- en: '[213] F. Huang, Y. Cheng, C. Jin, Y. Zhang, and T. Zhang, “Deep multimodal
    embedding model for fine-grained sketch-based image retrieval,” in *SIGIR*, 2017.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] F. Huang, Y. Cheng, C. Jin, Y. Zhang, 和 T. Zhang，“细粒度草图图像检索的深度多模态嵌入模型”，发表于
    *SIGIR*，2017年。'
- en: '[214] S. Dey, A. Dutta, S. K. Ghosh, E. Valveny, J. Lladós, and U. Pal, “Learning
    cross-modal deep embeddings for multi-object image retrieval using text and sketch,”
    in *ICPR*, 2018.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] S. Dey, A. Dutta, S. K. Ghosh, E. Valveny, J. Lladós, 和 U. Pal，“通过文本和草图学习跨模态深嵌入用于多对象图像检索”，发表于
    *ICPR*，2018年。'
- en: '[215] Y. Wang, F. Huang, Y. Zhang, R. Feng, T. Zhang, and W. Fan, “Deep cascaded
    cross-modal correlation learning for fine-grained sketch-based image retrieval,”
    *PR*, 2019.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] Y. Wang, F. Huang, Y. Zhang, R. Feng, T. Zhang, 和 W. Fan，“细粒度草图图像检索的深度级联跨模态相关学习”，*PR*，2019年。'
- en: '[216] K. Pang, K. Li, Y. Yang, H. Zhang, T. M. Hospedales, T. Xiang, and Y.-Z.
    Song, “Generalising fine-grained sketch-based image retrieval,” in *CVPR*, 2019.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] K. Pang, K. Li, Y. Yang, H. Zhang, T. M. Hospedales, T. Xiang, 和 Y.-Z.
    Song，“泛化细粒度草图图像检索”，发表于 *CVPR*，2019年。'
- en: '[217] T. Dutta and S. Biswas, “s-sbir: Style augmented sketch based image retrieval,”
    in *WACV*, 2020.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] T. Dutta 和 S. Biswas，“s-sbir：风格增强的基于草图的图像检索”，发表于 *WACV*，2020年。'
- en: '[218] A. K. Bhunia, Y. Yang, T. M. Hospedales, T. Xiang, and Y.-Z. Song, “Sketch
    less for more: On-the-fly fine-grained sketch based image retrieval,” in *CVPR*,
    2020.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] A. K. Bhunia, Y. Yang, T. M. Hospedales, T. Xiang, 和 Y.-Z. Song，“少画更多：实时细粒度草图图像检索”，发表于
    *CVPR*，2020年。'
- en: '[219] G. Andrew, R. Arora, J. Bilmes, and K. Livescu, “Deep canonical correlation
    analysis,” in *ICML*, 2013.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] G. Andrew, R. Arora, J. Bilmes, 和 K. Livescu，“深度典型相关分析”，发表于 *ICML*，2013年。'
- en: '[220] F. Huang, C. Jin, Y. Zhang, and T. Zhang, “Towards sketch-based image
    retrieval with deep cross-modal correlation learning,” in *ICME*, 2017.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] F. Huang, C. Jin, Y. Zhang, 和 T. Zhang，“基于草图的图像检索：深度跨模态相关学习”，发表于 *ICME*，2017年。'
- en: '[221] D. Xu, X. Alameda-Pineda, J. Song, E. Ricci, and N. Sebe, “Cross-paced
    representation learning with partial curricula for sketch-based image retrieval,”
    *TIP*, 2018.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] D. Xu, X. Alameda-Pineda, J. Song, E. Ricci, 和 N. Sebe，“基于草图的图像检索中的跨步表示学习与部分课程”，*TIP*，2018年。'
- en: '[222] C. Li, Y. Zhou, and J. Yang, “Sketch-based image retrieval via a semi-heterogeneous
    cross-domain network,” in *ICME Workshops*, 2019.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] C. Li, Y. Zhou, 和 J. Yang，“通过半异质跨域网络进行基于草图的图像检索”，发表于 *ICME Workshops*，2019年。'
- en: '[223] J. Collomosse, T. Bui, and H. Jin, “Livesketch: Query perturbations for
    guided sketch-based visual search,” in *CVPR*, 2019.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] J. Collomosse, T. Bui, 和 H. Jin，“Livesketch：引导草图视觉搜索的查询扰动”，发表于 *CVPR*，2019年。'
- en: '[224] O. Seddati, S. Dupont, and S. Mahmoudi, “Quadruplet networks for sketch-based
    image retrieval,” in *ICMR*, 2017.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] O. Seddati, S. Dupont, 和 S. Mahmoudi，“基于草图的图像检索中的四元组网络”，发表于 *ICMR*，2017年。'
- en: '[225] T. Bui, L. Ribeiro, M. Ponti, and J. Collomosse, “Generalisation and
    sharing in triplet convnets for sketch based visual search,” *arXiv preprint arXiv:1611.05301*,
    2016.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] T. Bui, L. Ribeiro, M. Ponti, 和 J. Collomosse，“基于草图的视觉搜索中的泛化与共享”，*arXiv
    preprint arXiv:1611.05301*，2016年。'
- en: '[226] H. Lin, Y. Fu, P. Lu, S. Gong, X. Xue, and Y.-G. Jiang, “Tc-net for isbir:
    Triplet classification network for instance-level sketch based image retrieval,”
    in *MM*, 2019.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] H. Lin, Y. Fu, P. Lu, S. Gong, X. Xue 和 Y.-G. Jiang， “Tc-net用于ISBIR：实例级基于草图的图像检索的三元组分类网络”，发表于
    *MM*，2019年。'
- en: '[227] L. Guo, J. Liu, Y. Wang, Z. Luo, W. Wen, and H. Lu, “Sketch-based image
    retrieval using generative adversarial networks,” in *MM*, 2017.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] L. Guo, J. Liu, Y. Wang, Z. Luo, W. Wen 和 H. Lu， “使用生成对抗网络的基于草图的图像检索”，发表于
    *MM*，2017年。'
- en: '[228] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning
    with deep convolutional generative adversarial networks,” *arXiv preprint arXiv:1511.06434*,
    2015.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] A. Radford, L. Metz 和 S. Chintala， “使用深度卷积生成对抗网络的无监督表示学习”，*arXiv preprint
    arXiv:1511.06434*，2015年。'
- en: '[229] K. Pang, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “Cross-domain generative
    learning for fine-grained sketch-based image retrieval.” in *BMVC*, 2017.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] K. Pang, Y.-Z. Song, T. Xiang 和 T. M. Hospedales， “跨域生成学习用于细粒度基于草图的图像检索”，发表于
    *BMVC*，2017年。'
- en: '[230] F. Huang, C. Jin, Y. Zhang, K. Weng, T. Zhang, and W. Fan, “Sketch-based
    image retrieval with deep visual semantic descriptor,” *PR*, 2018.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] F. Huang, C. Jin, Y. Zhang, K. Weng, T. Zhang 和 W. Fan， “基于草图的图像检索与深度视觉语义描述符”，*PR*，2018年。'
- en: '[231] T. Bui, L. Ribeiro, M. Ponti, and J. Collomosse, “Compact descriptors
    for sketch-based image retrieval using a triplet loss convolutional neural network,”
    *CVIU*, 2017.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] T. Bui, L. Ribeiro, M. Ponti 和 J. Collomosse， “基于草图的图像检索的紧凑描述符使用三元组损失卷积神经网络”，*CVIU*，2017年。'
- en: '[232] J. Lei, Y. Song, B. Peng, Z. Ma, L. Shao, and Y.-Z. Song, “Semi-heterogeneous
    three-way joint embedding network for sketch-based image retrieval,” *TCSVT*,
    2019.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] J. Lei, Y. Song, B. Peng, Z. Ma, L. Shao 和 Y.-Z. Song， “用于基于草图的图像检索的半异质三路联合嵌入网络”，*TCSVT*，2019年。'
- en: '[233] H. Zhang, C. Zhang, and M. Wu, “Sketch-based cross-domain image retrieval
    via heterogeneous network,” in *VCIP*, 2017.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] H. Zhang, C. Zhang 和 M. Wu， “通过异质网络进行基于草图的跨域图像检索”，发表于 *VCIP*，2017年。'
- en: '[234] L. Liu, F. Shen, Y. Shen, X. Liu, and L. Shao, “Deep sketch hashing:
    Fast free-hand sketch-based image retrieval,” in *CVPR*, 2017.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] L. Liu, F. Shen, Y. Shen, X. Liu 和 L. Shao， “深度草图哈希：快速自由手草图图像检索”，发表于
    *CVPR*，2017年。'
- en: '[235] J. Zhang, F. Shen, L. Liu, F. Zhu, M. Yu, L. Shao, H. Tao Shen, and L. Van Gool,
    “Generative domain-migration hashing for sketch-to-image retrieval,” in *ECCV*,
    2018.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] J. Zhang, F. Shen, L. Liu, F. Zhu, M. Yu, L. Shao, H. Tao Shen 和 L. Van
    Gool， “生成领域迁移哈希用于草图到图像检索”，发表于 *ECCV*，2018年。'
- en: '[236] G. Tolias and O. Chum, “Asymmetric feature maps with application to sketch
    based retrieval,” in *CVPR*, 2017.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] G. Tolias 和 O. Chum， “不对称特征图及其在基于草图的检索中的应用”，发表于 *CVPR*，2017年。'
- en: '[237] K. Lin, H.-F. Yang, J.-H. Hsiao, and C.-S. Chen, “Deep learning of binary
    hash codes for fast image retrieval,” in *CVPR Workshops*, 2015.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] K. Lin, H.-F. Yang, J.-H. Hsiao 和 C.-S. Chen， “用于快速图像检索的二进制哈希码的深度学习”，发表于
    *CVPR Workshops*，2015年。'
- en: '[238] J. Wang, T. Zhang, N. Sebe, H. T. Shen *et al.*, “A survey on learning
    to hash,” *TPAMI*, 2017.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] J. Wang, T. Zhang, N. Sebe, H. T. Shen *等*， “学习哈希的综述”，*TPAMI*，2017年。'
- en: '[239] K. Pang, Y. Yang, T. Hospedales, T. Xiang, and Y.-Z. Song, “Solving mixed-modal
    jigsaw puzzle for fine-grained sketch-based image retrieval,” in *CVPR*, 2020.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] K. Pang, Y. Yang, T. Hospedales, T. Xiang 和 Y.-Z. Song， “解决混合模态拼图以进行细粒度基于草图的图像检索”，发表于
    *CVPR*，2020年。'
- en: '[240] F. Radenovic, G. Tolias, and O. Chum, “Deep shape matching,” in *ECCV*,
    2018.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] F. Radenovic, G. Tolias 和 O. Chum， “深度形状匹配”，发表于 *ECCV*，2018年。'
- en: '[241] A. Sablayrolles, M. Douze, N. Usunier, and H. Jégou, “How should we evaluate
    supervised hashing?” in *ICASSP*, 2017.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] A. Sablayrolles, M. Douze, N. Usunier 和 H. Jégou， “我们应该如何评估监督哈希？”，发表于
    *ICASSP*，2017年。'
- en: '[242] P. Lu, G. Huang, Y. Fu, G. Guo, and H. Lin, “Learning large euclidean
    margin for sketch-based image retrieval,” *arXiv preprint arXiv:1812.04275*, 2018.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] P. Lu, G. Huang, Y. Fu, G. Guo 和 H. Lin， “学习大欧几里得间隔以进行基于草图的图像检索”，*arXiv
    preprint arXiv:1812.04275*，2018年。'
- en: '[243] W. Thong, P. Mettes, and C. G. Snoek, “Open cross-domain visual search,”
    *arXiv preprint arXiv:1911.08621*, 2019.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] W. Thong, P. Mettes 和 C. G. Snoek， “开放跨域视觉搜索”，*arXiv preprint arXiv:1911.08621*，2019年。'
- en: '[244] T. Dutta and S. Biswas, “Style-guided zero-shot sketch-based image retrieval,”
    in *BMVC*, 2019.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] T. Dutta 和 S. Biswas， “风格引导的零样本草图图像检索”，发表于 *BMVC*，2019年。'
- en: '[245] A. Dutta and Z. Akata, “Semantically tied paired cycle consistency for
    zero-shot sketch-based image retrieval,” in *CVPR*, 2019.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] A. Dutta 和 Z. Akata， “语义关联的配对循环一致性用于零样本基于草图的图像检索”，发表于 *CVPR*，2019年。'
- en: '[246] S. Kiran Yelamarthi, S. Krishna Reddy, A. Mishra, and A. Mittal, “A zero-shot
    framework for sketch based image retrieval,” in *ECCV*, 2018.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] S. Kiran Yelamarthi, S. Krishna Reddy, A. Mishra 和 A. Mittal， “用于基于草图的图像检索的零样本框架”，发表于
    *ECCV*，2018年。'
- en: '[247] J. Li, Z. Ling, L. Niu, and L. Zhang, “Bi-directional domain translation
    for zero-shot sketch-based image retrieval,” *arXiv preprint arXiv:1911.13251*,
    2019.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] J. Li, Z. Ling, L. Niu 和 L. Zhang，“用于零-shot基于草图的图像检索的双向领域转换”，发表于*arXiv
    preprint arXiv:1911.13251*，2019年。'
- en: '[248] A. Pandey, A. Mishra, V. Kumar Verma, and A. Mittal, “Adversarial joint-distribution
    learning for novel class sketch-based image retrieval,” in *ICCV Workshops*, 2019.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] A. Pandey, A. Mishra, V. Kumar Verma 和 A. Mittal，“针对新类基于草图的图像检索的对抗性联合分布学习”，发表于*ICCV
    Workshops*，2019年。'
- en: '[249] V. Kumar Verma, A. Mishra, A. Mishra, and P. Rai, “Generative model for
    zero-shot sketch-based image retrieval,” in *CVPR Workshops*, 2019.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] V. Kumar Verma, A. Mishra, A. Mishra 和 P. Rai，“用于零-shot基于草图的图像检索的生成模型”，发表于*CVPR
    Workshops*，2019年。'
- en: '[250] Q. Liu, L. Xie, H. Wang, and A. L. Yuille, “Semantic-aware knowledge
    preservation for zero-shot sketch-based image retrieval,” in *ICCV*, 2019.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] Q. Liu, L. Xie, H. Wang 和 A. L. Yuille，“用于零-shot基于草图的图像检索的语义感知知识保留”，发表于*ICCV*，2019年。'
- en: '[251] A. Pandey, A. Mishra, V. K. Verma, A. Mittal, and H. Murthy, “Stacked
    adversarial network for zero-shot sketch based image retrieval,” in *WACV*, 2020.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] A. Pandey, A. Mishra, V. K. Verma, A. Mittal 和 H. Murthy，“用于零-shot基于草图的图像检索的堆叠对抗网络”，发表于*WACV*，2020年。'
- en: '[252] X. Xu, C. Deng, M. Yang, and H. Wang, “Progressive domain-independent
    feature decomposition network for zero-shot sketch-based image retrieval,” *arXiv
    preprint arXiv:2003.09869*, 2020.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] X. Xu, C. Deng, M. Yang 和 H. Wang，“用于零-shot基于草图的图像检索的渐进式领域无关特征分解网络”，发表于*arXiv
    preprint arXiv:2003.09869*，2020年。'
- en: '[253] U. Chaudhuri, B. Banerjee, A. Bhattacharya, and M. Datcu, “A simplified
    framework for zero-shot cross-modal sketch data retrieval,” in *CVPR Workshops*,
    2020.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] U. Chaudhuri, B. Banerjee, A. Bhattacharya 和 M. Datcu，“用于零-shot跨模态草图数据检索的简化框架”，发表于*CVPR
    Workshops*，2020年。'
- en: '[254] Z. Zhang, Y. Zhang, R. Feng, T. Zhang, and W. Fan, “Zero-shot sketch-based
    image retrieval via graph convolution network,” in *AAAI*, 2020.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] Z. Zhang, Y. Zhang, R. Feng, T. Zhang 和 W. Fan，“通过图卷积网络进行零-shot基于草图的图像检索”，发表于*AAAI*，2020年。'
- en: '[255] U. Chaudhuri, B. Banerjee, A. Bhattacharya, and M. Datcu, “A zero-shot
    sketch-based inter-modal object retrieval scheme for remote sensing images,” *arXiv
    preprint arXiv:2008.05225*, 2020.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] U. Chaudhuri, B. Banerjee, A. Bhattacharya 和 M. Datcu，“用于遥感图像的零-shot基于草图的跨模态对象检索方案”，发表于*arXiv
    preprint arXiv:2008.05225*，2020年。'
- en: '[256] Y. Fu, T. M. Hospedales, T. Xiang, and S. Gong, “Transductive multi-view
    zero-shot learning,” *TPAMI*, 2015.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] Y. Fu, T. M. Hospedales, T. Xiang 和 S. Gong，“传导式多视图零-shot学习”，发表于*TPAMI*，2015年。'
- en: '[257] Y. Fu, T. Xiang, Y.-G. Jiang, X. Xue, L. Sigal, and S. Gong, “Recent
    advances in zero-shot recognition: Toward data-efficient understanding of visual
    content,” *IEEE Signal Processing Magazine*, 2018.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] Y. Fu, T. Xiang, Y.-G. Jiang, X. Xue, L. Sigal 和 S. Gong，“零-shot识别的最新进展：迈向数据高效的视觉内容理解”，发表于*IEEE
    Signal Processing Magazine*，2018年。'
- en: '[258] W. Wang, V. W. Zheng, H. Yu, and C. Miao, “A survey of zero-shot learning:
    Settings, methods, and applications,” *TIST*, 2019.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] W. Wang, V. W. Zheng, H. Yu 和 C. Miao，“零-shot学习的综述：设置、方法和应用”，发表于*TIST*，2019年。'
- en: '[259] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed
    representations of words and phrases and their compositionality,” in *NeurIPS*,
    2013.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado 和 J. Dean，“词汇和短语的分布式表示及其组合性”，发表于*NeurIPS*，2013年。'
- en: '[260] C. H. Lampert, H. Nickisch, and S. Harmeling, “Attribute-based classification
    for zero-shot visual object categorization,” *TPAMI*, 2013.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] C. H. Lampert, H. Nickisch 和 S. Harmeling，“基于属性的分类用于零-shot视觉对象分类”，发表于*TPAMI*，2013年。'
- en: '[261] Y. Shen, L. Liu, F. Shen, and L. Shao, “Zero-shot sketch-image hashing,”
    in *CVPR*, 2018.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] Y. Shen, L. Liu, F. Shen 和 L. Shao，“零-shot草图-图像哈希”，发表于*CVPR*，2018年。'
- en: '[262] W. Chen and J. Hays, “Sketchygan: Towards diverse and realistic sketch
    to image synthesis,” in *CVPR*, 2018.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] W. Chen 和 J. Hays，“Sketchygan：迈向多样化和真实的草图到图像合成”，发表于*CVPR*，2018年。'
- en: '[263] Y. Jo and J. Park, “Sc-fegan: Face editing generative adversarial network
    with user’s sketch and color,” in *ICCV*, 2019.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] Y. Jo 和 J. Park，“Sc-fegan：使用用户草图和颜色的面部编辑生成对抗网络”，发表于*ICCV*，2019年。'
- en: '[264] S. Yang, Z. Wang, J. Liu, and Z. Guo, “Deep plastic surgery: Robust and
    controllable image editing with human-drawn sketches,” *arXiv preprint arXiv:2001.02890*,
    2020.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] S. Yang, Z. Wang, J. Liu 和 Z. Guo，“深度整形手术：基于人工草图的稳健和可控的图像编辑”，发表于*arXiv
    preprint arXiv:2001.02890*，2020年。'
- en: '[265] W. Xia, Y. Yang, and J.-H. Xue, “Cali-sketch: Stroke calibration and
    completion for high-quality face image generation from poorly-drawn sketches,”
    *arXiv preprint arXiv:1911.00426*, 2019.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] W. Xia, Y. Yang 和 J.-H. Xue，“Cali-sketch：从绘制不佳的草图中生成高质量人脸图像的笔画校准和完成”，发表于*arXiv
    preprint arXiv:1911.00426*，2019年。'
- en: '[266] A. Ghosh, R. Zhang, P. K. Dokania, O. Wang, A. A. Efros, P. H. S. Torr,
    and E. Shechtman, “Interactive sketch & fill: Multiclass sketch-to-image translation,”
    in *ICCV*, 2019.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] A. Ghosh, R. Zhang, P. K. Dokania, O. Wang, A. A. Efros, P. H. S. Torr,
    和 E. Shechtman，“互动素描与填充：多类别素描到图像转换，”发表于*ICCV*，2019年。'
- en: '[267] Z. Li, C. Deng, E. Yang, and D. Tao, “Staged sketch-to-image synthesis
    via semi-supervised generative adversarial networks,” *TMM*, 2020.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] Z. Li, C. Deng, E. Yang, 和 D. Tao，“通过半监督生成对抗网络进行分阶段素描到图像合成，”*TMM*，2020年。'
- en: '[268] B. Liu, K. Song, and A. Elgammal, “Sketch-to-art: Synthesizing stylized
    art images from sketches,” *arXiv preprint arXiv:2002.12888*, 2020.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] B. Liu, K. Song, 和 A. Elgammal，“素描到艺术：从素描合成风格化艺术图像，”*arXiv预印本arXiv:2002.12888*，2020年。'
- en: '[269] Y. Lu, S. Wu, Y.-W. Tai, and C.-K. Tang, “Image generation from sketch
    constraint using contextual gan,” in *ECCV*, 2018.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] Y. Lu, S. Wu, Y.-W. Tai, 和 C.-K. Tang，“基于上下文的GAN进行素描约束的图像生成，”发表于*ECCV*，2018年。'
- en: '[270] W. Xian, P. Sangkloy, V. Agrawal, A. Raj, J. Lu, C. Fang, F. Yu, and
    J. Hays, “Texturegan: Controlling deep image synthesis with texture patches,”
    in *CVPR*, 2018.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] W. Xian, P. Sangkloy, V. Agrawal, A. Raj, J. Lu, C. Fang, F. Yu, 和 J. Hays，“Texturegan：使用纹理补丁控制深度图像合成，”发表于*CVPR*，2018年。'
- en: '[271] M. Li, Z. Lin, R. Mech, E. Yumer, and D. Ramanan, “Photo-sketching: Inferring
    contour drawings from images,” in *WACV*, 2019.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] M. Li, Z. Lin, R. Mech, E. Yumer, 和 D. Ramanan，“Photo-sketching：从图像推断轮廓图，”发表于*WACV*，2019年。'
- en: '[272] M. Kampelmuhler and A. Pinz, “Synthesizing human-like sketches from natural
    images using a conditional convolutional decoder,” in *WACV*, 2020.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] M. Kampelmuhler 和 A. Pinz，“使用条件卷积解码器从自然图像合成类人素描，”发表于*WACV*，2020年。'
- en: '[273] S.-Y. Chen, W. Su, L. Gao, S. Xia, and H. Fu, “Deepfacedrawing: deep
    generation of face images from sketches,” *TOG*, 2020.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] S.-Y. Chen, W. Su, L. Gao, S. Xia, 和 H. Fu，“Deepfacedrawing：从素描中深度生成面部图像，”*TOG*，2020年。'
- en: '[274] J. Huang, J. Liao, Z. Tan, and S. Kwong, “Multi-density sketch-to-image
    translation network,” *arXiv preprint arXiv:2006.10649*, 2020.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] J. Huang, J. Liao, Z. Tan, 和 S. Kwong，“多密度素描到图像转换网络，”*arXiv预印本arXiv:2006.10649*，2020年。'
- en: '[275] Y. Zhang, G. Su, Y. Qi, and J. Yang, “Unpaired image-to-sketch translation
    network for sketch synthesis,” in *VCIP*, 2019.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] Y. Zhang, G. Su, Y. Qi, 和 J. Yang，“无配对图像到素描转换网络用于素描合成，”发表于*VCIP*，2019年。'
- en: '[276] J. Li, S. Liu, and M. Cao, “Line artist: A multiple style sketch to painting
    synthesis scheme,” *arXiv preprint arXiv:1803.06647*, 2018.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] J. Li, S. Liu, 和 M. Cao，“Line artist：多风格素描到绘画合成方案，”*arXiv预印本arXiv:1803.06647*，2018年。'
- en: '[277] M. Li, A. Sheffer, E. Grinspun, and N. Vining, “Foldsketch: Enriching
    garments with physically reproducible folds,” *TOG*, 2018.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] M. Li, A. Sheffer, E. Grinspun, 和 N. Vining，“Foldsketch：用物理可重现的褶皱丰富服装，”*TOG*，2018年。'
- en: '[278] T. Y. Wang, D. Ceylan, J. Popovic, and N. J. Mitra, “Learning a shared
    shape space for multimodal garment design,” *arXiv preprint arXiv:1806.11335*,
    2018.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] T. Y. Wang, D. Ceylan, J. Popovic, 和 N. J. Mitra，“为多模态服装设计学习共享形状空间，”*arXiv预印本arXiv:1806.11335*，2018年。'
- en: '[279] J. Collomosse, T. Bui, M. J. Wilber, C. Fang, and H. Jin, “Sketching
    with style: Visual search with sketches and aesthetic context,” in *ICCV*, 2017.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] J. Collomosse, T. Bui, M. J. Wilber, C. Fang, 和 H. Jin，“风格素描：具有风格和美学背景的视觉搜索，”发表于*ICCV*，2017年。'
- en: '[280] C. Zou, H. Mo, R. Du, X. Wu, C. Gao, and H. Fu, “Lucss: Language-based
    user-customized colourization of scene sketches,” *arXiv preprint arXiv:1808.10544*,
    2018.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] C. Zou, H. Mo, R. Du, X. Wu, C. Gao, 和 H. Fu，“Lucss：基于语言的用户自定义场景素描着色，”*arXiv预印本arXiv:1808.10544*，2018年。'
- en: '[281] L. Zhang, C. Li, T.-T. Wong, Y. Ji, and C. Liu, “Two-stage sketch colorization,”
    *TOG*, 2018.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] L. Zhang, C. Li, T.-T. Wong, Y. Ji, 和 C. Liu，“双阶段素描着色，”*TOG*，2018年。'
- en: '[282] C. Zou, H. Mo, C. Gao, R. Du, and H. Fu, “Language-based colorization
    of scene sketches,” *TOG*, 2019.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] C. Zou, H. Mo, C. Gao, R. Du, 和 H. Fu，“基于语言的场景素描着色，”*TOG*，2019年。'
- en: '[283] X. Soria, E. Riba, and A. D. Sappa, “Dense extreme inception network:
    Towards a robust cnn model for edge detection,” *arXiv preprint arXiv:1909.01955*,
    2019.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[283] X. Soria, E. Riba, 和 A. D. Sappa，“Dense extreme inception network：一种用于边缘检测的鲁棒CNN模型，”*arXiv预印本arXiv:1909.01955*，2019年。'
- en: '[284] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” in *ICCV*, 2017.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[284] J.-Y. Zhu, T. Park, P. Isola, 和 A. A. Efros，“使用循环一致对抗网络的无配对图像到图像转换，”发表于*ICCV*，2017年。'
- en: '[285] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
    with conditional adversarial networks,” in *CVPR*, 2017.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[285] P. Isola, J.-Y. Zhu, T. Zhou, 和 A. A. Efros，“使用条件对抗网络的图像到图像转换，”发表于*CVPR*，2017年。'
- en: '[286] M. Mirza and S. Osindero, “Conditional generative adversarial nets,”
    *arXiv preprint arXiv:1411.1784*, 2014.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[286] M. Mirza 和 S. Osindero，“条件生成对抗网络，”*arXiv预印本arXiv:1411.1784*，2014年。'
- en: '[287] K. Sasaki, S. Iizuka, E. Simo-Serra, and H. Ishikawa, “Joint gap detection
    and inpainting of line drawings,” in *CVPR*, 2017.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[287] K. Sasaki, S. Iizuka, E. Simo-Serra, 和 H. Ishikawa，“线条绘图的联合缺口检测和修复，”
    发表在 *CVPR*，2017年。'
- en: '[288] H. Chen, M. V. Giuffrida, P. Doerner, and S. A. Tsaftaris, “Blind inpainting
    of large-scale masks of thin structures with adversarial and reinforcement learning,”
    *arXiv preprint arXiv:1912.02470*, 2019.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[288] H. Chen, M. V. Giuffrida, P. Doerner, 和 S. A. Tsaftaris，“对大规模薄结构掩膜的盲修复，结合对抗性和强化学习，”
    发表在 *arXiv preprint arXiv:1912.02470*，2019年。'
- en: '[289] H. Chen, M. V. Giuffrida, S. A. Tsaftaris, and P. Doerner, “Root gap
    correction with a deep inpainting model,” in *BMVC*, 2018.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[289] H. Chen, M. V. Giuffrida, S. A. Tsaftaris, 和 P. Doerner，“使用深度修复模型的根缺口校正，”
    发表在 *BMVC*，2018年。'
- en: '[290] H. Chen, M. Valerio Giuffrida, P. Doerner, and S. A. Tsaftaris, “Adversarial
    large-scale root gap inpainting,” in *CVPR Workshops*, 2019.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[290] H. Chen, M. Valerio Giuffrida, P. Doerner, 和 S. A. Tsaftaris，“对抗性大规模根缺口修复，”
    发表在 *CVPR Workshops*，2019年。'
- en: '[291] T. Shao, W. Xu, K. Yin, J. Wang, K. Zhou, and B. Guo, “Discriminative
    sketch-based 3d model retrieval via robust shape matching,” in *Computer Graphics
    Forum*, 2011.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[291] T. Shao, W. Xu, K. Yin, J. Wang, K. Zhou, 和 B. Guo，“通过鲁棒形状匹配的判别性草图基3D模型检索，”
    发表在 *Computer Graphics Forum*，2011年。'
- en: '[292] B. Li, Y. Lu, and J. Shen, “A semantic tree-based approach for sketch-based
    3d model retrieval,” in *ICPR*, 2016.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[292] B. Li, Y. Lu, 和 J. Shen，“一种基于语义树的草图到3D模型检索方法，” 发表在 *ICPR*，2016年。'
- en: '[293] D. DeCarlo, A. Finkelstein, S. Rusinkiewicz, and A. Santella, “Suggestive
    contours for conveying shape,” *TOG*, 2003.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[293] D. DeCarlo, A. Finkelstein, S. Rusinkiewicz, 和 A. Santella，“传达形状的建议轮廓，”
    发表在 *TOG*，2003年。'
- en: '[294] H. Li, H. Wu, X. He, S. Lin, R. Wang, and X. Luo, “Multi-view pairwise
    relationship learning for sketch based 3d shape retrieval,” in *ICME*, 2017.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[294] H. Li, H. Wu, X. He, S. Lin, R. Wang, 和 X. Luo，“基于多视图成对关系学习的草图3D形状检索，”
    发表在 *ICME*，2017年。'
- en: '[295] F. Zhu, J. Xie, and Y. Fang, “Learning cross-domain neural networks for
    sketch-based 3d shape retrieval,” in *AAAI*, 2016.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[295] F. Zhu, J. Xie, 和 Y. Fang，“用于草图基3D形状检索的跨域神经网络学习，” 发表在 *AAAI*，2016年。'
- en: '[296] Y. Ye, B. Li, and Y. Lu, “3d sketch-based 3d model retrieval with convolutional
    neural network,” in *ICPR*, 2016.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[296] Y. Ye, B. Li, 和 Y. Lu，“基于卷积神经网络的3D草图到3D模型检索，” 发表在 *ICPR*，2016年。'
- en: '[297] J. Chen and Y. Fang, “Deep cross-modality adaptation via semantics preserving
    adversarial learning for sketch-based 3d shape retrieval,” in *ECCV*, 2018.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[297] J. Chen 和 Y. Fang，“通过保持语义对抗学习的深度跨模态适应，用于草图基3D形状检索，” 发表在 *ECCV*，2018年。'
- en: '[298] J. Chen, J. Qin, L. Liu, F. Zhu, F. Shen, J. Xie, and L. Shao, “Deep
    sketch-shape hashing with segmented 3d stochastic viewing,” in *CVPR*, 2019.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[298] J. Chen, J. Qin, L. Liu, F. Zhu, F. Shen, J. Xie, 和 L. Shao，“深度草图-形状哈希与分段3D随机视图，”
    发表在 *CVPR*，2019年。'
- en: '[299] A. Qi, Y. Gryaditskaya, J. Song, Y. Yang, Y. Qi, T. M. Hospedales, T. Xiang,
    and Y.-Z. Song, “Toward fine-grained sketch-based 3d shape retrieval,” *IEEE Transactions
    on Image Processing*, vol. 30, pp. 8595–8606, 2021.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[299] A. Qi, Y. Gryaditskaya, J. Song, Y. Yang, Y. Qi, T. M. Hospedales, T.
    Xiang, 和 Y.-Z. Song，“朝着细粒度草图基3D形状检索的方向，” 发表在 *IEEE Transactions on Image Processing*，第30卷，第8595-8606页，2021年。'
- en: '[300] A. Qi, Y.-Z. Song, and T. Xiang, “Semantic embedding for sketch-based
    3d shape retrieval.” in *BMVC*, 2018.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[300] A. Qi, Y.-Z. Song, 和 T. Xiang，“基于草图的3D形状检索的语义嵌入。” 发表在 *BMVC*，2018年。'
- en: '[301] S. Kuwabara, R. Ohbuchi, and T. Furuya, “Query by partially-drawn sketches
    for 3d shape retrieval,” in *2019 International Conference on Cyberworlds*, 2019.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[301] S. Kuwabara, R. Ohbuchi, 和 T. Furuya，“通过部分绘制草图进行3D形状检索，” 发表在 *2019 International
    Conference on Cyberworlds*，2019年。'
- en: '[302] G. Dai, J. Xie, F. Zhu, and Y. Fang, “Deep correlated metric learning
    for sketch-based 3d shape retrieval,” in *AAAI*, 2017.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[302] G. Dai, J. Xie, F. Zhu, 和 Y. Fang，“用于草图基3D形状检索的深度相关度量学习，” 发表在 *AAAI*，2017年。'
- en: '[303] G. Dai, J. Xie, and Y. Fang, “Deep correlated holistic metric learning
    for sketch-based 3d shape retrieval,” *TIP*, 2018.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[303] G. Dai, J. Xie, 和 Y. Fang，“用于草图基3D形状检索的深度相关整体度量学习，” 发表在 *TIP*，2018年。'
- en: '[304] J. Xie, G. Dai, F. Zhu, and Y. Fang, “Learning barycentric representations
    of 3d shapes for sketch-based 3d shape retrieval,” in *CVPR*, 2017.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[304] J. Xie, G. Dai, F. Zhu, 和 Y. Fang，“基于草图的3D形状检索的3D形状重心表示学习，” 发表在 *CVPR*，2017年。'
- en: '[305] V. I. Bogachev and A. V. Kolesnikov, “The monge-kantorovich problem:
    achievements, connections, and perspectives,” *Russian Mathematical Surveys*,
    2012.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[305] V. I. Bogachev 和 A. V. Kolesnikov，“蒙日-坎托罗维奇问题：成就、联系和前景，” 发表在 *Russian
    Mathematical Surveys*，2012年。'
- en: '[306] L. Wang, C. Qian, J. Wang, and Y. Fang, “Unsupervised learning of 3d
    model reconstruction from hand-drawn sketches,” in *MM*, 2018.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[306] L. Wang, C. Qian, J. Wang, 和 Y. Fang，“从手绘草图中无监督学习3D模型重建，” 发表在 *MM*，2018年。'
- en: '[307] S.-H. Zhang, Y.-C. Guo, and Q.-W. Gu, “Sketch2model: View-aware 3d modeling
    from single free-hand sketches,” in *CVPR*, 2021.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[307] S.-H. Zhang, Y.-C. Guo 和 Q.-W. Gu，“Sketch2model：基于视图的 3D 建模从单个手绘草图，”
    在 *CVPR*，2021。'
- en: '[308] Y. Shen, C. Zhang, H. Fu, K. Zhou, and Y. Zheng, “Deepsketchhair: Deep
    sketch-based 3d hair modeling,” *arXiv preprint arXiv:1908.07198*, 2019.'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[308] Y. Shen, C. Zhang, H. Fu, K. Zhou 和 Y. Zheng，“Deepsketchhair：基于深度学习的
    3D 发型建模，” *arXiv 预印本 arXiv:1908.07198*，2019。'
- en: '[309] K. D. Willis, P. K. Jayaraman, J. G. Lambourne, H. Chu, and Y. Pu, “Engineering
    sketch generation for computer-aided design,” in *CVPR Workshops*, 2021.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[309] K. D. Willis, P. K. Jayaraman, J. G. Lambourne, H. Chu 和 Y. Pu，“计算机辅助设计的工程草图生成，”
    在 *CVPR Workshops*，2021。'
- en: '[310] H. Huang, E. Kalogerakis, E. Yumer, and R. Mech, “Shape synthesis from
    sketches via procedural models and convolutional networks,” *TVCG*, 2016.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[310] H. Huang, E. Kalogerakis, E. Yumer 和 R. Mech，“通过程序模型和卷积网络从草图合成形状，” *TVCG*，2016。'
- en: '[311] X. Han, C. Gao, and Y. Yu, “Deepsketch2face: a deep learning based sketching
    system for 3d face and caricature modeling,” *TOG*, 2017.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311] X. Han, C. Gao 和 Y. Yu，“Deepsketch2face：基于深度学习的 3D 面部和漫画建模草图系统，” *TOG*，2017。'
- en: '[312] M. Ye, S. Zhou, and H. Fu, “Deepshapesketch: Generating hand drawing
    sketches from 3d objects,” in *IJCNN*, 2019.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[312] M. Ye, S. Zhou 和 H. Fu，“Deepshapesketch：从 3D 对象生成手绘草图，” 在 *IJCNN*，2019。'
- en: '[313] J. P. Collomosse, G. McNeill, and Y. Qian, “Storyboard sketches for content
    based video retrieval,” in *ICCV*, 2009.'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[313] J. P. Collomosse, G. McNeill 和 Y. Qian，“基于内容的视频检索的故事板草图，” 在 *ICCV*，2009。'
- en: '[314] P. Xu, K. Liu, T. Xiang, T. M. Hospedales, Z. Ma, J. Guo, and Y.-Z. Song,
    “Fine-grained instance-level sketch-based video retrieval,” *arXiv preprint arXiv:2002.09461*,
    2020.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[314] P. Xu, K. Liu, T. Xiang, T. M. Hospedales, Z. Ma, J. Guo 和 Y.-Z. Song，“细粒度实例级草图视频检索，”
    *arXiv 预印本 arXiv:2002.09461*，2020。'
- en: '[315] S. Wu, H. Su, S. Zheng, H. Yang, and Q. Zhou, “Motion sketch based crowd
    video retrieval via motion structure coding,” in *ICIP*, 2016.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[315] S. Wu, H. Su, S. Zheng, H. Yang 和 Q. Zhou，“通过运动结构编码的运动草图人群视频检索，” 在 *ICIP*，2016。'
- en: '[316] S. Wu, H. Yang, S. Zheng, H. Su, Q. Zhou, and X. Lu, “Motion sketch based
    crowd video retrieval,” *Multimedia Tools and Applications*, 2017.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[316] S. Wu, H. Yang, S. Zheng, H. Su, Q. Zhou 和 X. Lu，“基于运动草图的人群视频检索，” *Multimedia
    Tools and Applications*，2017。'
- en: '[317] F. Huang and J. F. Canny, “Sketchforme: Composing sketched scenes from
    text descriptions for interactive applications,” *arXiv preprint arXiv:1904.04399*,
    2019.'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[317] F. Huang 和 J. F. Canny，“Sketchforme：从文本描述中组成草图场景以用于交互式应用，” *arXiv 预印本
    arXiv:1904.04399*，2019。'
- en: '[318] F. Huang, E. Schoop, D. Ha, and J. Canny, “Scones: towards conversational
    authoring of sketches,” in *International Conference on Intelligent User Interfaces*,
    2020.'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[318] F. Huang, E. Schoop, D. Ha 和 J. Canny，“Scones：面向对话式草图创作，” 在 *国际智能用户界面会议*，2020。'
- en: '[319] C. Hu, D. Li, Y. Yang, T. M. Hospedales, and Y.-Z. Song, “Sketch-a-segmenter:
    Sketch-based photo segmenter generation,” *IEEE Transactions on Image Processing*,
    vol. 29, pp. 9470–9481, 2020.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[319] C. Hu, D. Li, Y. Yang, T. M. Hospedales 和 Y.-Z. Song，“Sketch-a-segmenter：基于草图的照片分割生成，”
    *IEEE 图像处理交易*，第 29 卷，第 9470-9481 页，2020。'
- en: '[320] R. K. Sarvadevabhatla, S. Surya, T. Mittal, and R. V. Babu, “Game of
    sketches: Deep recurrent models of pictionary-style word guessing,” in *AAAI*,
    2018.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[320] R. K. Sarvadevabhatla, S. Surya, T. Mittal 和 R. V. Babu，“草图游戏：Pictionary
    风格词汇猜测的深度递归模型，” 在 *AAAI*，2018。'
- en: '[321] K. Pang, D. Li, J. Song, Y.-Z. Song, T. Xiang, and T. M. Hospedales,
    “Deep factorised inverse-sketching,” in *ECCV*, 2018.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[321] K. Pang, D. Li, J. Song, Y.-Z. Song, T. Xiang 和 T. M. Hospedales，“深度因式分解逆草图，”
    在 *ECCV*，2018。'
- en: '[322] A. Tripathi, R. R. Dani, A. Mishra, and A. Chakraborty, “Sketch-guided
    object localization in natural images,” *arXiv preprint arXiv:2008.06551*, 2020.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[322] A. Tripathi, R. R. Dani, A. Mishra 和 A. Chakraborty，“基于草图的自然图像对象定位，”
    *arXiv 预印本 arXiv:2008.06551*，2020。'
- en: '[323] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
    the inception architecture for computer vision,” in *CVPR*, 2016.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[323] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens 和 Z. Wojna，“重新思考计算机视觉的
    Inception 架构，” 在 *CVPR*，2016。'
- en: '[324] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *CVPR*, 2017.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[324] G. Huang, Z. Liu, L. Van Der Maaten 和 K. Q. Weinberger，“密集连接卷积网络，” 在
    *CVPR*，2017。'
- en: '[325] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mobilenetv2:
    Inverted residuals and linear bottlenecks,” in *CVPR*, 2018.'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[325] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov 和 L.-C. Chen，“Mobilenetv2：反向残差和线性瓶颈，”
    在 *CVPR*，2018。'
- en: '[326] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *ICLR*, 2017.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[326] T. N. Kipf 和 M. Welling，“基于图卷积网络的半监督分类，” 在 *ICLR*，2017。'
- en: '[327] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio,
    “Graph Attention Networks,” in *ICLR*, 2018.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[327] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò, 和 Y. Bengio，“图注意力网络，”发表于
    *ICLR*，2018年。'
- en: '[328] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *NeurIPS*, 2017.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[328] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，“注意力机制就是你所需要的，”发表于 *NeurIPS*，2017年。'
- en: '[329] J.-D. Favreau, F. Lafarge, and A. Bousseau, “Fidelity vs. simplicity:
    a global approach to line drawing vectorization,” *TOG*, 2016.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[329] J.-D. Favreau, F. Lafarge, 和 A. Bousseau，“忠实度与简洁性：线条绘图矢量化的全球方法，” *TOG*，2016年。'
- en: '[330] M. Bessmeltsev and J. Solomon, “Vectorization of line drawings via polyvector
    fields,” *TOG*, 2019.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[330] M. Bessmeltsev 和 J. Solomon，“通过多矢量场的线条绘图矢量化，” *TOG*，2019年。'
- en: '[331] H. Mo, E. Simo-Serra, C. Gao, C. Zou, and R. Wang, “General Virtual Sketching
    Framework for Vector Line Art,” *TOG*, 2021.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[331] H. Mo, E. Simo-Serra, C. Gao, C. Zou, 和 R. Wang，“用于矢量线条艺术的通用虚拟草图框架，”
    *TOG*，2021年。'
- en: '[332] R. B. Venkataramaiyer, A. Joshi, S. Narang, and V. P. Namboodiri, “Shad3s:
    A model to sketch, shade and shadow,” in *WACV*, 2021.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[332] R. B. Venkataramaiyer, A. Joshi, S. Narang, 和 V. P. Namboodiri，“Shad3s：一个用于草图、阴影和阴影的模型，”发表于
    *WACV*，2021年。'
- en: '[333] E. Simo-Serra, S. Iizuka, and H. Ishikawa, “Real-Time Data-Driven Interactive
    Rough Sketch Inking,” *TOG*, 2018.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[333] E. Simo-Serra, S. Iizuka, 和 H. Ishikawa，“实时数据驱动的交互式粗略草图墨迹处理，” *TOG*，2018年。'
- en: '[334] P. Xu, H. Fu, Y. Zheng, K. Singh, H. Huang, and C.-L. Tai, “Model-guided
    3d sketching,” *TVCG*, 2018.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[334] P. Xu, H. Fu, Y. Zheng, K. Singh, H. Huang, 和 C.-L. Tai，“模型引导的3D草图绘制，”
    *TVCG*，2018年。'
- en: '[335] B. Jackson and D. F. Keefe, “Lift-off: Using reference imagery and freehand
    sketching to create 3d models in vr,” *TVCG*, 2016.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[335] B. Jackson 和 D. F. Keefe，“Lift-off：使用参考图像和自由手绘创建虚拟现实中的3D模型，” *TVCG*，2016年。'
- en: '[336] D. Giunchi, D. Degraen, A. Steed *et al.*, “Mixing realities for sketch
    retrieval in virtual reality,” *arXiv preprint arXiv:1910.11637*, 2019.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[336] D. Giunchi, D. Degraen, A. Steed *等*，“混合现实用于虚拟现实中的草图检索，” *arXiv预印本 arXiv:1910.11637*，2019年。'
- en: '[337] J. C. Roberts, C. Headleand, and P. D. Ritsos, “Sketching designs using
    the five design-sheet methodology,” *TVCG*, 2015.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[337] J. C. Roberts, C. Headleand, 和 P. D. Ritsos，“使用五个设计表单方法的草图设计，” *TVCG*，2015年。'
- en: '[338] F. Boniardi, A. Valada, W. Burgard, and G. D. Tipaldi, “Autonomous indoor
    robot navigation using a sketch interface for drawing maps and routes,” in *ICRA*,
    2016.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[338] F. Boniardi, A. Valada, W. Burgard, 和 G. D. Tipaldi，“使用草图界面进行室内机器人自主导航，以绘制地图和路线，”发表于
    *ICRA*，2016年。'
- en: '[339] V. Jain, P. Agrawal, S. Banga, R. Kapoor, and S. Gulyani, “Sketch2code:
    Transformation of sketches to ui in real-time using deep neural network,” *arXiv
    preprint arXiv:1910.08930*, 2019.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[339] V. Jain, P. Agrawal, S. Banga, R. Kapoor, 和 S. Gulyani，“Sketch2code：使用深度神经网络实时将草图转换为UI，”
    *arXiv预印本 arXiv:1910.08930*，2019年。'
- en: '[340] B. Paulson and T. Hammond, “Paleosketch: accurate primitive sketch recognition
    and beautification,” in *Proceedings of the 13th international conference on Intelligent
    user interfaces*, 2008, pp. 1–10.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[340] B. Paulson 和 T. Hammond，“Paleosketch：准确的原始草图识别与美化，”发表于 *第13届国际智能用户界面会议论文集*，2008年，第1-10页。'
- en: '[341] D. Sỳkora, J. Dingliana, and S. Collins, “Lazybrush: Flexible painting
    tool for hand-drawn cartoons,” in *Computer Graphics Forum*, 2009.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[341] D. Sỳkora, J. Dingliana, 和 S. Collins，“Lazybrush：用于手绘卡通的灵活绘画工具，”发表于 *Computer
    Graphics Forum*，2009年。'
- en: '[342] D. Sỳkora, M. Ben-Chen, M. Čadík, B. Whited, and M. Simmons, “Textoons:
    practical texture mapping for hand-drawn cartoon animations,” in *Proceedings
    of the ACM SIGGRAPH/Eurographics Symposium on Non-Photorealistic Animation and
    Rendering*, 2011.'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[342] D. Sỳkora, M. Ben-Chen, M. Čadík, B. Whited, 和 M. Simmons，“Textoons：实用的纹理映射用于手绘卡通动画，”发表于
    *ACM SIGGRAPH/Eurographics 非真实感动画与渲染研讨会论文集*，2011年。'
- en: '[343] Y. Liu, Z. Qin, T. Wan, and Z. Luo, “Auto-painter: Cartoon image generation
    from sketch by using conditional wasserstein generative adversarial networks,”
    *Neurocomputing*, 2018.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[343] Y. Liu, Z. Qin, T. Wan, 和 Z. Luo，“Auto-painter：通过条件Wasserstein生成对抗网络从草图生成卡通图像，”
    *Neurocomputing*，2018年。'
- en: '[344] H. Kim, H. Y. Jhoo, E. Park, and S. Yoo, “Tag2pix: Line art colorization
    using text tag with secat and changing loss,” in *ICCV*, 2019.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[344] H. Kim, H. Y. Jhoo, E. Park, 和 S. Yoo，“Tag2pix：使用文本标签和secat及变化损失的线条艺术着色，”发表于
    *ICCV*，2019年。'
- en: '[345] M. Hudon, M. Grogan, A. Smolic *et al.*, “Deep normal estimation for
    automatic shading of hand-drawn characters,” in *ECCV Workshops*, 2018.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[345] M. Hudon, M. Grogan, A. Smolic *等*，“深度法线估计用于手绘字符的自动阴影处理，”发表于 *ECCV Workshops*，2018年。'
- en: '[346] W. Su, D. Du, X. Yang, S. Zhou, and H. Fu, “Interactive sketch-based
    normal map generation with deep neural networks,” *Proceedings of the ACM on Computer
    Graphics and Interactive Techniques*, 2018.'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[346] W. Su, D. Du, X. Yang, S. Zhou 和 H. Fu，“基于深度神经网络的交互式草图法线图生成，” *ACM计算机图形学与交互技术会议录*，2018年。'
- en: '[347] K. T. Yesilbek and M. Sezgin, “On training sketch recognizers for new
    domains,” in *CVPR Workshops*, 2021.'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[347] K. T. Yesilbek 和 M. Sezgin，“针对新领域训练草图识别器，” 收录于 *CVPR工作坊*，2021年。'
