- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:00:27'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2007.00753] Opportunities and Challenges in Deep Learning Adversarial Robustness:
    A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2007.00753](https://ar5iv.labs.arxiv.org/html/2007.00753)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Samuel Henrique Silva,  and Peyman Najafirad S.H. Silva and P. Najafirad are
    members of the Secure AI & Autonomy Laboratory, with the Department of Electrical
    and Computer Engineering, University of Texas at San Antonio, San Antonio, TX,
    78249.
  prefs: []
  type: TYPE_NORMAL
- en: P. Najafirad (corresponding author) is also with Department of Information Systems
    and Cyber Security, University of Texas at San Antonio, San Antonio, TX, 78249.
  prefs: []
  type: TYPE_NORMAL
- en: 'E-mail: peyman.najafirad@utsa.eduThe authors gratefully acknowledge the use
    of the services of Jetstream cloud, funded by National Science Foundation, United
    States award 1445604.This work has been submitted to the IEEE for possible publication.
    Copyright may be transferred without notice, after which this version may no longer
    be accessible.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As we seek to deploy machine learning models beyond virtual and controlled
    domains, it is critical to analyze not only the accuracy or the fact that it works
    most of the time, but if such a model is truly robust and reliable. This paper
    studies strategies to implement adversary robustly trained algorithms towards
    guaranteeing safety in machine learning algorithms. We provide a taxonomy to classify
    adversarial attacks and defenses, formulate the Robust Optimization problem in
    a min-max setting, and divide it into 3 subcategories, namely: Adversarial (re)Training,
    Regularization Approach, and Certified Defenses. We survey the most recent and
    important results in adversarial example generation, defense mechanisms with adversarial
    (re)Training as their main defense against perturbations. We also survey mothods
    that add regularization terms which change the behavior of the gradient, making
    it harder for attackers to achieve their objective. Alternatively, we’ve surveyed
    methods which formally derive certificates of robustness by exactly solving the
    optimization problem or by approximations using upper or lower bounds. In addition
    we discuss the challenges faced by most of the recent algorithms presenting future
    research perspectives.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Artificial Intelligence, Deep Learning, Robustness, Adversarial Examples, Robust
    Optimization, Certified Defenses.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep Learning (DL) ([[1](#bib.bib1)]) models are changing the way we solve problems
    that have required many attempts from the most diverse fields of science. DL is
    an improvement over Artificial Intelligence (AI) Neural Networks (NN), in which
    more layers are stacked to grant a bigger level of abstraction and better reasoning
    over the data when compared to other Machine Learning (ML) algorithms ([[2](#bib.bib2)]).
    Since the raise of DL, supported in many cases by cloud environments [[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)], the base architecture and its variations have
    been applied in many scientific breakthroughs in the most diverse fields of knowledge,
    e.g. in predicting AMD disease progression ([[6](#bib.bib6)]), predicting DNA
    enhancers for gene expression programmes ([[7](#bib.bib7)]), elections and demographic
    analysis based on satellite images ([[8](#bib.bib8)]), filtering data for gravitational-wave
    signals ([[9](#bib.bib9)]). DL approach has also become one of the most used approaches
    for natural language processing ([[10](#bib.bib10)]) and speech recognition ([[11](#bib.bib11)]).
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular variations of DL architecture, Convolutional Neural
    Networks (CNN) have significantly boosted the performance of DL algorithms in
    computer vision (CV) applications ([[12](#bib.bib12)]), bringing it to several
    areas of CV such as, object detection ([[13](#bib.bib13), [14](#bib.bib14)], action
    recognition [[15](#bib.bib15), [16](#bib.bib16)], pose estimation [[17](#bib.bib17),
    [18](#bib.bib18)], image segmentation [[19](#bib.bib19), [20](#bib.bib20)], and
    motion tracking [[21](#bib.bib21)]. Starting with ImageNet [[22](#bib.bib22)],
    proposed in 2012, the field of CNN’s have seen great improvement with super-human
    performance in specific tasks, providing solutions even to medical problems [[23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: '<svg   height="153.79" overflow="visible"
    version="1.1" width="327.78"><g transform="translate(0,153.79) matrix(1 0 0 -1
    0 0) translate(166.84,0) translate(0,134.22)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -20.74 -15.61)" fill="#000000"
    stroke="#000000"><g  transform="matrix(1 0 0 -1 0 27.365)"><g
     transform="matrix(1 0 0 1 0 31.21)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1
    0 0 -1 0 0)">Defenses</text></g></g></g></g><g transform="matrix(1.0 0.0 0.0 1.0
    -162.88 -80.19)" fill="#000000" stroke="#000000"><g  transform="matrix(1
    0 0 -1 0 36.745)"><g  transform="matrix(1 0 0 1 0 31.21)"><g
     transform="matrix(1 0 0 -1
    30.76 0)"><g  transform="matrix(1 0 0 -1 0 27.365)"><g 
    transform="matrix(1 0 0 1 0 31.21)"><g 
    transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Gradient</text></g></g></g></g></g><g
     transform="matrix(1 0 0 1 0 39.51)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1
    0 0 -1 0 0)">Masking/Obfuscation</text></g></g></g></g><g transform="matrix(1.0
    0.0 0.0 1.0 -31.98 -79.52)" fill="#000000" stroke="#000000"><g 
    transform="matrix(1 0 0 -1 0 36.07)"><g  transform="matrix(1
    0 0 1 0 31.21)"><g  transform="matrix(1
    0 0 -1 14.65 0)"><g  transform="matrix(1 0 0 -1 0 27.365)"><g
     transform="matrix(1 0 0 1 0 31.21)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="34.65"
    height="7.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Robust</foreignobject></g></g></g></g></g><g
     transform="matrix(1 0 0 1 0 38.77)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="63.96"
    height="9.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Optimization</foreignobject></g></g></g></g>
    <g stroke-width="0.4pt" fill="#E6E6E6"><g transform="matrix(1.0 0.0 0.0 1.0 -154.97
    -114.1)" fill="#000000" stroke="#000000"><foreignobject width="88.56" height="23.52"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Adversarial Training <path
    d="M -35.95 -78.22 L -81.23 -102.38" style="fill:none"><g stroke-width="0.4pt"
    fill="#E6E6E6"><clippath ><path d="M 42.43 -110.55 L -42.43 -110.55
    C -45.49 -110.55 -47.97 -113.02 -47.97 -116.08 L -47.97 -120.14 C -47.97 -123.2
    -45.49 -125.67 -42.43 -125.67 L 42.43 -125.67 C 45.49 -125.67 47.97 -123.2 47.97
    -120.14 L 47.97 -116.08 C 47.97 -113.02 45.49 -110.55 42.43 -110.55 Z M -47.97
    -125.67"></path></clippath><g clip-path="url(#pgfcp17)"><g transform="matrix(1.0
    0.0 0.0 1.0 0 -118.11) matrix(1.0 0.0 0.0 1.0 0 0) matrix(1.38165 0.0 0.0 0.21786
    0 0)"><defs><lineargradient gradienttransform="rotate(90)" ></lineargradient></defs></g></g><path
    d="M 42.43 -110.55 L -42.43 -110.55 C -45.49 -110.55 -47.97 -113.02 -47.97 -116.08
    L -47.97 -120.14 C -47.97 -123.2 -45.49 -125.67 -42.43 -125.67 L 42.43 -125.67
    C 45.49 -125.67 47.97 -123.2 47.97 -120.14 L 47.97 -116.08 C 47.97 -113.02 45.49
    -110.55 42.43 -110.55 Z M -47.97 -125.67" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 -44.28 -121.98)" fill="#000000" stroke="#000000"><foreignobject width="88.56"
    height="7.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Certified
    Defenses</foreignobject></g> <path d="M 0 -83.48 L 0 -110.27" style="fill:none"></path></path></g><g
    stroke-width="0.4pt" fill="#E6E6E6"><clippath ><path d="M 153.13 -102.27
    L 68.26 -102.27 C 65.21 -102.27 62.73 -104.75 62.73 -107.81 L 62.73 -128.41 C
    62.73 -131.47 65.21 -133.95 68.26 -133.95 L 153.13 -133.95 C 156.19 -133.95 158.66
    -131.47 158.66 -128.41 L 158.66 -107.81 C 158.66 -104.75 156.19 -102.27 153.13
    -102.27 Z M 62.73 -133.95"></path></clippath><g clip-path="url(#pgfcp19)"><g transform="matrix(1.0
    0.0 0.0 1.0 110.7 -118.11) matrix(1.0 0.0 0.0 1.0 0 0) matrix(1.38165 0.0 0.0
    0.45612 0 0)"><defs><lineargradient gradienttransform="rotate(90)" ></lineargradient></defs></g></g><path
    d="M 153.13 -102.27 L 68.26 -102.27 C 65.21 -102.27 62.73 -104.75 62.73 -107.81
    L 62.73 -128.41 C 62.73 -131.47 65.21 -133.95 68.26 -133.95 L 153.13 -133.95 C
    156.19 -133.95 158.66 -131.47 158.66 -128.41 L 158.66 -107.81 C 158.66 -104.75
    156.19 -102.27 153.13 -102.27 Z M 62.73 -133.95" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 66.42 -113.71)" fill="#000000" stroke="#000000"><foreignobject width="88.56"
    height="24.29" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Regularization
    Approach</foreignobject></g> <path d="M 35.95 -78.22 L 80.51 -102" style="fill:none"></path></path></g><clippath
    ><path d="M 155.13 -34.84 L 66.26 -34.84 C 63.21 -34.84 60.73 -37.32
    60.73 -40.38 L 60.73 -77.73 C 60.73 -80.79 63.21 -83.27 66.26 -83.27 L 155.13
    -83.27 C 158.18 -83.27 160.66 -80.79 160.66 -77.73 L 160.66 -40.38 C 160.66 -37.32
    158.18 -34.84 155.13 -34.84 Z M 60.73 -83.27"></path></clippath><g clip-path="url(#pgfcp21)"><g
    transform="matrix(1.0 0.0 0.0 1.0 110.7 -59.06) matrix(1.0 0.0 0.0 1.0 0 0) matrix(1.43922
    0.0 0.0 0.69746 0 0)"><defs><lineargradient gradienttransform="rotate(90)" ></lineargradient></defs></g></g><path
    d="M 155.13 -34.84 L 66.26 -34.84 C 63.21 -34.84 60.73 -37.32 60.73 -40.38 L 60.73
    -77.73 C 60.73 -80.79 63.21 -83.27 66.26 -83.27 L 155.13 -83.27 C 158.18 -83.27
    160.66 -80.79 160.66 -77.73 L 160.66 -40.38 C 160.66 -37.32 158.18 -34.84 155.13
    -34.84 Z M 60.73 -83.27" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0
    64.42 -79.58)" fill="#000000" stroke="#000000"><g  transform="matrix(1
    0 0 -1 0 36.13)"><g  transform="matrix(1 0 0 1 0 31.21)"><g
     transform="matrix(1 0 0 -1
    18.7 0)"><g  transform="matrix(1 0 0 -1 0 27.365)"><g 
    transform="matrix(1 0 0 1 0 31.21)"><g 
    transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Adversarial</text></g></g></g></g></g><g
     transform="matrix(1 0 0 1 0 38.9)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1
    0 0 -1 0 0)">Example Detection</text></g></g></g></g> <path d="M 24.71 -13.17
    L 64.81 -34.56" style="fill:none">Figure 1: Defenses against adversarial attacks
    are divided in 3 categories: Gradient Masking/Obfuscation, Robust Optimization,
    and Adversarial Example Detection. The focus of this survey is Robust Optimization
    which we subdivide in: Adversarial Training, Certified Defenses, and Regularization
    Approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Fueled by the fact that new frameworks, libraries, and hardware resources are
    being improved and made available to the public and scientific community [[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26)], Deep Neural networks (DNN) are being improved
    constantly and achieving new performance breakthroughs [[27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29)]. With the current maturity of DNN algorithms, its being applied
    in solving safety and security-critical problems [[30](#bib.bib30)], such as self-driving
    cars [[31](#bib.bib31), [32](#bib.bib32)], multi-agent aerial vehicle systems
    with face identification [[33](#bib.bib33)], robotics [[34](#bib.bib34), [35](#bib.bib35)],
    social engineering detection [[36](#bib.bib36)], network anomaly detection [[37](#bib.bib37)],
    deep packet inspection in networks [[38](#bib.bib38)]. DNN applications are already
    part of our day-to-day life (personal assistants [[39](#bib.bib39)], product recommendation
    [[40](#bib.bib40)], biometric identification [[41](#bib.bib41)]) and tend to occupy
    a bigger space as time passes.
  prefs: []
  type: TYPE_NORMAL
- en: As seen in many publications, DNN has been shown to have human-level accuracy
    even for significantly complex tasks such as playing games with no prior rule
    known, except the current frames [[42](#bib.bib42)]. In contrast to the aforementioned
    accuracy of DNN models, its been shown in earlier publications [[43](#bib.bib43),
    [44](#bib.bib44), [45](#bib.bib45)], that DNN models are susceptible to small
    input perturbations, in most cases imperceptible to the human eye. The results
    from this publications have shown the facility with which small additive targeted
    noise to the input image, makes models to misclassify objects which before could
    be identified with 99.99% confidence. More alarming is the fact that such models
    report high confidence in the predictions. Such perturbations, which can fool
    a trained model, are known as adversarial attacks. With such alarming consequences,
    the study of adversarial attacks and robustness against them became a great deal
    of research in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: A considerably large number of research papers is now available concerning methods
    to identify adversarial attacks and defend from incursions against the model [[46](#bib.bib46),
    [47](#bib.bib47)]. One way of solving this issue is adding better intuition on
    the models, through explainability [[48](#bib.bib48)], but such models do not
    target the direct improvement of the model. On the other hand, several approaches
    have been published to generate models which are robust against adversarial attacks
    [[49](#bib.bib49)], the target of the researchers is to introduce in their models’
    layers of robustness such that the models are not fooled by out of distribution
    examples, known or unknown attacks, targeted or untargeted attacks. Guaranteeing
    the accuracy of such models while safety is taken into consideration, is of utmost
    importance for system architects, mainly making them robust to the presence of
    adversarial attacks, noise, model misspecification, and uncertainty. This survey
    aims to bring together the recent advances in robustness for DNN’s, pointing the
    main research directions being followed recently to generate robust DNN models.
    We bring to light both applied and theoretical recent developments.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by [[50](#bib.bib50)], we analyze the robustness of DNN’s under the
    perspective of how adversarial examples can be generated, and how defenses can
    be formulated against such algorithms. In general words, we define the robustness
    against adversarial attacks problem as a dual optimization problem, in which the
    attackers try to maximize the loss while the defenses try to minimize the chance
    of a model being fooled by the attacker. In such formulation, current existing
    models based in non-linear activation functions, introduce non-linear inequality
    constraints in the optimization, which generates an inherent trade-off between
    exact solutions and scalability of the model. This trade-off comes in the form
    of either exact slow solutions through mixed-integer linear programming, or in
    approximations to the objective function which either, relies on the existing
    attack methods to provide a local heuristic estimation to the maximization function,
    or approximations of the bounds of the constraints or objective function to generate
    certification regions, in which no adversarial example exists.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically this paper presents the following contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We characterize defenses to adversarial attacks as a min-max optimization problem,
    investigating solutions involving heuristic approximation, exact solution, and
    upper/lower bound approximations to generate models robust to adversarial attacks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We investigate, analyze, and categorize the most recent and/or important approaches
    to generate adversarial examples, as they are the basis to generate strong defenses,
    through Adversarial (re)Training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We investigate, analyze, and categorize the most recent and important approaches
    to generate defenses against adversarial attacks, providing a taxonomy, description
    of the methods, and the main results in such approaches.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We organize this survey in the following manner. In [section 2](#S2 "2 Taxonomy
    of Adversarial Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey") we describe taxonomies for both adversarial
    example generation and defenses. We classify the adversarial models concerning
    the time of the attack, information available to the attacker, objective, and
    the algorithm computation method. Moreover, we classify the perturbation type
    used by the attackers. We divide the defense methods into three categories, namely
    gradient-masking/obfuscation, robust optimization, and adversarial example detection.
    We focus this research in Robust Optimization, and further sub-divide in 3 groups:
    Adversarial Training, Certified Defenses, and Regularization Approach. In [section 3](#S3
    "3 Methods for Generating Adversarial Attacks ‣ Opportunities and Challenges in
    Deep Learning Adversarial Robustness: A Survey"), we describe several relevant
    adversarial attacks, and summarize them in [Table II](#S3.T2 "TABLE II ‣ 3.3.1
    Other Attacks ‣ 3.3 Physical World Attack ‣ 3 Methods for Generating Adversarial
    Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness:
    A Survey"). In [section 4](#S4 "4 Defense Mechanisms based on Robust Optimization
    Against Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey") we describe the most relevant results in Robust Optimization,
    and provide a tree that maps these publications to the 3 sub-groups of Robust
    Optimization. In [section 5](#S5 "5 Challenges and Future Opportunities ‣ Opportunities
    and Challenges in Deep Learning Adversarial Robustness: A Survey") we discuss
    current challenges and opportunities in robust defenses.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Taxonomy of Adversarial Attacks and Defenses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We keep a consistent notation set along with the survey, and for easiness of
    reading we summarize in [Table I](#S2.T1 "TABLE I ‣ 2 Taxonomy of Adversarial
    Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey") the most used notations and symbols which we will use along
    with this survey. For papers requiring some specific terms, we define them in
    the section in which they are presented.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Symbols and notations used in the mathematical definitions'
  prefs: []
  type: TYPE_NORMAL
- en: '| Symbol | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $x$ | original (clean, unmodified) input data |'
  prefs: []
  type: TYPE_TB
- en: '| $\hat{y}$ | model’s prediction |'
  prefs: []
  type: TYPE_TB
- en: '| $t$ | class label |'
  prefs: []
  type: TYPE_TB
- en: '| $x^{\prime}$ | adversarial example |'
  prefs: []
  type: TYPE_TB
- en: '| $y^{\prime}$ | target class of adversarial example |'
  prefs: []
  type: TYPE_TB
- en: '| $f(.)$ | DL model |'
  prefs: []
  type: TYPE_TB
- en: '| $\theta$ | parameters of the model |'
  prefs: []
  type: TYPE_TB
- en: '| $\delta$ | perturbation generated by adv. algorithm |'
  prefs: []
  type: TYPE_TB
- en: '| $\Delta$, $\epsilon$ | perturbation constraint |'
  prefs: []
  type: TYPE_TB
- en: '| $\nabla$ | gradient function |'
  prefs: []
  type: TYPE_TB
- en: '| $\left\&#124;.\right\&#124;_{p}$ | the $l_{p}$-norm |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{L}$ | loss function (e.g., cross-entropy) |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{D}$ | Training data distribution |'
  prefs: []
  type: TYPE_TB
- en: '| $KL$-divergence | Kullback-Leibler divergence function |'
  prefs: []
  type: TYPE_TB
- en: 2.1 Attack Threat Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Several attempts have been made to categorize attacks on machine learning.
    We here distill the most important aspects which characterize adversarial examples
    generating models concerning their architecture. We focus on the aspects that
    are most relevant to the discussion of adversarial robustness. To that end, we
    classify the attacks concerning timing, information, goals, and attack frequency
    following the proposed in [[51](#bib.bib51)]:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Timing: A first crucial feature for modeling the adversarial attacks is when
    it occurs. To that end we have two possibilities, evasion and poisoning attacks.
    Evasion attacks are the ones in the time of inference and assume the model has
    already been trained. Poisoning attacks in general targets the data, and the training
    phase of the model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Information: Another feature of the attack references the information to which
    the attacker has access. In the white box context the adversary has full access
    to information concerning the model and the model itself, as opposed to black
    box setting, in which very few or no information is available. White box attacks
    refer to those in which the adversary can unrestrictedly query the model for any
    information, such as weights, gradient, model hyper-parameters, prediction scores.
    Whereas in black-box attacks the adversary has limited or no information about
    these parameters, although may obtain some of the information indirectly, for
    example, through queries. Some also define grey-box attacks, in which attackers
    might only know the feature representation and the type of model that was used
    but have no access to dataset or the model information. A fourth setting is called
    restricted black-box, or also known as no-box attack. Under such an assumption,
    no information is available to the attacker, and the research is mainly focused
    on attack transferability. In wich the focus is to evaluate the possibility of
    transferring the attack performed in one DNN to the inaccessible objective model
    [[52](#bib.bib52)]. In this work, we evaluate models in a binary setting, the
    adversary either has comprehensive access to the DNN or black box having limited
    access through queries, which can also provide class scores.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goals: The attackers may have different reasons to target a specific algorithm.
    But mostly the attacker has either a specific goal, and needs the algorithm to
    output a specific output, case in which it is a targeted attack, or just wants
    to reduce the reliability of the algorithm by forcing a mistake. In the latter,
    we have an untargeted attack.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attack Frequency: The attack on the victim’s model can be either iterative
    or one-time. In the one-time, the optimization of the objective function of the
    attacker happens in a single step, whereas the iterative method takes several
    steps to generate the perturbation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.2 Attack Perturbation Type
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The size of the perturbation is in the core of the adversarial attack, a small
    perturbation is the fundamental premise of such models. When designing an adversarial
    example, the attacker wants the perturbed input to be as close as possible to
    the original one, in the case of images, close enough that a human can not distinguish
    one image from the other. We analyze the perturbation concerning scope, limitation,
    and measurement.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perturbation Scope: The attacker can generate perturbations that are input
    specific, in which we call individual, or it can generate a single perturbation
    which will be effective to all inputs in the training dataset, which we call universal
    perturbation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perturbation Limitation: Two options are possible, optimized perturbation and
    constraint perturbation. The optimized perturbation is the goal of the optimization
    problem, while the constraint perturbation is the set as the constraint to the
    optimization problem.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perturbation Measurement: Is the metric used to measure the magnitude of the
    perturbation. The most commonly used metric is the $l_{p}$-norm, with many algorithms
    applying $l_{0},l_{2},l_{\infty}$ norms.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.3 Defense Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As seen in [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Opportunities and
    Challenges in Deep Learning Adversarial Robustness: A Survey") based on [[53](#bib.bib53)],
    we sub-divide the defenses to adversarial attacks in 3 main categories: Gradient
    Masking/Obfuscation, Robust Optimization, and Adversarial Example Detection, which
    are described as:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gradient Masking/Obfuscation: The core aspect of defense mechanisms based on
    gradient masking is constructing models with gradients that are not useful for
    attackers. The gradient masked/obfuscated models, in general, produce loss functions
    that are very smooth in the neighborhood of the input data. This smoothness around
    training data points makes it difficult for exploiting algorithms to find meaningful
    directions towards the generation of an adversarial example.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Robust Optimization: Is a defense strategy that is composed of methods that
    improve the optimization function either by adding regularization terms, certification
    bounds, adversarial examples in the objective function, or modifying the model
    to add uncertainty in the model layers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adversarial Example Detection: Recent work has turned to detect adversarial
    examples rather than making the DNN robust against creating them. Detecting adversarial
    examples is usually done by finding statistical outliers or training separate
    sub-networks that can distinguish between perturbed and normal images.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: About the defense mechanisms, we focus this survey on methods related to Robust
    Optimization. Among the several publications in this survey, each author has its
    representation and view of robust optimization. In general, even with different
    notations and representations, most of the papers we have surveyed fit the general
    representation of Robust Optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training objective in a DL model is the minimization of the desired loss.
    The objective is to ajust the model parameters with respect to the labeled data,
    as seen in [Equation 1](#S2.E1 "1 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial
    Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey"),'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta}\mathcal{L}(\theta,x,y)$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: in which $\theta$ are the model parameters, $x$ is the input to the model, $\mathcal{L}$
    is the defined loss function, and $y$ is its true label. With such a formulation,
    we seek to minimize w.r.t. $\theta$, the loss function. Such formulation fit the
    parameters to the data points, such that $f(x)$ yields predictions $\hat{y}$ which
    are equal to the true label $y$. In an adversarial setting this scene changes,
    in which the objective is different,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\delta\leq\Delta}\mathcal{L}(\theta,x+\delta,y)$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: in which we are searching for a perturbation $\delta$, smaller than a maximum
    perturbation $\Delta$, capable of changing the decision of the classifier from
    prediction $\hat{y}$, to $y^{\prime}$. The restriction on the perturbation is
    a designer parameter which is in general defined by the $l_{p}$-norm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Equations [1](#S2.E1 "In 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial Attacks
    and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness:
    A Survey") and [2](#S2.E2 "In 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial
    Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey") do not incorporate the data distribution or the restrictions
    which come from the fact that most of the training datasets do not incorporate
    the true distribution of the data in which the models will perform inference.
    Based on the definition from [[54](#bib.bib54)], we have that, if $\mathbb{D}$
    is the true distribution of the data, a training set is draw i.i.d. from $\mathbb{D}$,
    and is defined as $\mathcal{D}=\{(x_{i},y_{i})\sim\mathbb{D}\},\text{ for }i=1,...,m$.
    And the empirical risk of a classifier, which is based on the training set, is
    defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R(F,\mathcal{D})=\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(x,y)\in\mathcal{D}}\mathcal{L}(f(x),y)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'in which $|\mathcal{D}|$ is the size of the training set $\mathcal{D}$. With
    that definition the empirical adversarial risk is defined in:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R_{adv}(F,\mathcal{D})=\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(x,y)\in\mathcal{D}}\mathcal{L}(f(x+\delta),y)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: When dealing with adversarial defenses in the lenses of Robust Optimization,
    the one first solution, is to solve the combined worst-case loss, with the empirical
    adversarial risk $R_{adv}$, known as adversarial training.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta}\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(x,y)\in\mathcal{D}}\max_{\delta\in\Delta}\mathcal{L}(f(x+\delta),y)$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'The solution of [Equation 3](#S2.E3 "3 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of
    Adversarial Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey"), require special handling or a completely different
    formulation which define how we categorize the defense mechanisms for adversarial
    attacks, namely: Adversarial (re)Training, Bayesian Approach, Regularization Approach,
    and Certified Defenses.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Adversarial (re)Training as a Defense Mechanism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The solution of [Equation 3](#S2.E3 "3 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of
    Adversarial Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey") requires solving the inner maximization ([Equation 2](#S2.E2
    "2 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial Attacks and Defenses ‣ Opportunities
    and Challenges in Deep Learning Adversarial Robustness: A Survey")), which is
    a high dimensional non-convex optimization problem prohibitively hard to solve
    exactly by standard optimization techniques. The most popular approach to solve
    such a problem is by approximating [Equation 2](#S2.E2 "2 ‣ 2.3 Defense Methods
    ‣ 2 Taxonomy of Adversarial Attacks and Defenses ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey") with the use of a heuristics
    in which we are searching for a lower bound for [Equation 2](#S2.E2 "2 ‣ 2.3 Defense
    Methods ‣ 2 Taxonomy of Adversarial Attacks and Defenses ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey"). While promising and shown
    to improve robustness even for large models (ImageNet [[55](#bib.bib55)]), such
    models come with a drawback which when instantiated in practice with the approximation
    heuristics, they are unable to provide robustness guarantees or certifications.
    This class of defenses even though very practical to implement can not provide
    a guarantee that no adversarial example exists in the neighborhood of $x$ capable
    of fooling $f(.)$.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Certified Defenses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The problem stated in [Equation 3](#S2.E3 "3 ‣ 2.3 Defense Methods ‣ 2 Taxonomy
    of Adversarial Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey"), defines the general objective of adversarial
    training. But as seen, even with the best methods to find a local approximation
    to the maximization problem, we are subjective to the effectiveness of the attacking
    method. A way around this inconvenience has been proposed in the literature, which
    is to exactly solve the maximization problem or approximate to a solvable set
    of constraints. To formally define Certified Defenses, initially, we consider
    a threat model where the adversary is allowed to transform an input $x\in\mathbb{R}^{d}$
    into any point from a set $\mathbb{S}_{0}(x)\subseteq\mathbb{R}^{d}$. Such set
    represents the neighborhood of the point $x$ generated by either $l_{p}$ perturbations,
    geometric transformations, semantic perturbations, or another kind of transformation
    in $x$. In case of an $l_{p}$ perturbation, the set is defined as $\mathbb{S}_{0}(x)=\{x^{\prime}\in\mathbb{R}^{d},\left\|x-x^{\prime}\right\|_{p}<\epsilon\}$.'
  prefs: []
  type: TYPE_NORMAL
- en: We further expand the model $f(.)$ as a function of its $k$ hidden layers and
    parameters $\theta$, where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f(x)=f^{k}_{\theta}\circ f^{k-1}_{\theta}\circ\dots\circ f^{1}_{\theta}$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'in which $f^{i}_{\theta}:\mathbb{R}^{d_{i-1}}\rightarrow\mathbb{R}^{d_{i}}$
    denotes the nonlinear transformation applied in hidden layer $i$. The objective
    is to prove a property on the output of the neural network, encoded via a linear
    constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $c^{T}f_{\theta}(x^{\prime})+d<0,\forall x^{\prime}\in\mathbb{S}_{0}(x)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: in which $c$ and $d$ are property specific vector and scalar values.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the complexity of the certification, based on [Equation 4](#S2.E4
    "4 ‣ 2.3.2 Certified Defenses ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial
    Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey"), we define the layer-wise adversarial optimization objective.
    For $z_{1}=x,z_{i+1}=f_{i}(W_{i}z_{i}+b_{i})$ :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\displaystyle z_{1,\dots,d+1}}{\mathrm{max}}\quad(e_{y}-e_{y_{targ}})^{T}z_{d+1}\hfil\hfil\hfil\hfil$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{z_{1,\ldots,
    d+1}}}{\mathrm{max}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{max}><quot;)][l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle z_{1}^{\prime}\in\mathbb{S}_{0},\hfil\hfil$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle z_{i+1}=f_{i}(W_{i}z_{i}+b_{i}),~{}i=1,\dots,d-1,\hfil\hfil$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle z_{d+1}=W_{d}z_{d}+b_{d}$ |  |'
  prefs: []
  type: TYPE_TB
- en: in which $e_{i}$ unit basis, vectors with value 1 in the class $i^{th}$ position
    and zeros everywhere else. Such formulation requires special handling given that
    we have a nonlinear constraint defined by the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several techniques have been proposed to solve such a problem and they are
    within the scope of study on this survey. The method to train certified neural
    networks is based on the computation of an upper bound to the inner loss, as opposed
    to a lower bound computed for adversarial training. These methods are typically
    referred to as provable defenses as they provide guarantees on the robustness
    of the resulting network, under any kind of attack inside the threat model. Typical
    methods to compute the certifications are based on convex relaxations, interval
    propagation, SMT solvers, abstract interpretation, mixed-integer linear programs,
    linear relaxations, or combinations of these methods. We explore the diverse techniques
    in [subsection 4.3](#S4.SS3 "4.3 Certified Defenses ‣ 4 Defense Mechanisms based
    on Robust Optimization Against Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3 Regularization Approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Regularization techniques focus on making small modifications to the learning
    algorithm, such that it can generalize better. In a certain way, it improves the
    performance of the model in unseen data. It prevents model over-fitting to the
    noise data, by penalizing weight matrices of the nodes. In the specific case of
    Robust Optimization, the objective of regularization techniques is similar, but
    focusing on avoiding that small variation on the input, can generate changes in
    the decision of the algorithm. It does so by either expanding the decision boundaries
    or limiting changes in the gradient of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many regularization techniques have been proposed with the most used being
    the $l_{p}$ based ones. The $l_{2}$ regularization technique is introduced to
    reduce the parameters value which translates to variance reduction. It introduces
    a penalty term to the original objective function (Loss), adding the weighted
    sum of the squared parameters of the model. With that, we have a regularized loss
    $\mathcal{L}_{R}$ defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{R}(x+\delta,y)=\mathcal{L}(x+\delta,y)+\lambda\left\&#124;\theta\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'in which a small $\lambda$ lets the parameters to grow unchecked while a large
    $\lambda$ encourages the reduction of the model parameters. Regularization methods
    are not restricted to $L_{p}$ approaches and can involve Lipschitz Regularization,
    the Jacobian Matrix, and other techniques that we survey on [subsection 4.2](#S4.SS2
    "4.2 Regularization Techniques ‣ 4 Defense Mechanisms based on Robust Optimization
    Against Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methods for Generating Adversarial Attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Studying adversarial attacks in the image classification domain improve our
    insights, as we can visually analyze the dissimilarities between disturbed and
    non-disturbed inputs. Moreover, the image data, even though high dimensional,
    are simpler represented than other domains such as audio, graphs, and cyber-security
    data. Along this section, we’ll revise the attack generating algorithms in the
    image classification domain which can be applied to standard deep neural networks
    (DNN) and convolutional neural networks (CNN). In which we classify in: white
    box, black box, and applied real world attacks. In [Table II](#S3.T2 "TABLE II
    ‣ 3.3.1 Other Attacks ‣ 3.3 Physical World Attack ‣ 3 Methods for Generating Adversarial
    Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness:
    A Survey") we summarize all the attacks described in this section, highlighting
    the distance metric used, information access level, algorithm type, and the domain
    it was applied in the specific publication.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ff36cb2c619c8e6a57a6e4ec5cac6cb1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: White box and black box attacks diverge mainly on the information
    the attacker have access to.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sub-sections, we list and describe the most popular approaches
    for the adversarial attack in ML models. We list them in chronological order and
    focus on giving the most important details on these methods.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 White-box Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As stated, in white box attacks, there is no restriction in the level of information
    to which the attacker can access. As a consequence the adversary knows model parameters,
    dataset, or any other information regarding the model. Under such assumption,
    given a model $f(.)$, an input $(x,y)$, the main objective is to produce $x^{\prime}$,
    which is within certain distance from the original $x$ and maximizes the loss
    $\mathcal{L}(f(x+\delta),y)$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\delta\in\Delta}\mathcal{L}(f(x+\delta),y)$ |  |'
  prefs: []
  type: TYPE_TB
- en: 3.1.1 Box-Constrained L-BFGS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In, [[43](#bib.bib43)], the existence of small perturbations capable of misleading
    a classifier were first demonstrated. In the paper, Szegedy et. al. proposed to
    compute an additive noise $\delta$, which could be added to the original input
    $x$, capable of misleading the classifier but with minimal or no perceptible distortion
    to the image. We find the optimal delta, $\delta$, with:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\displaystyle\delta}{\mathrm{min}}\quad c\left\&#124;\delta\right\&#124;_{2}\hfil\hfil\hfil\hfil$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{min}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{min}><quot;)][l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle f(x+\delta)=y^{\prime},\hfil\hfil$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\text{ all pixel in }(x+\delta)\in[0,1]$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'in which $f(.)$ is the parameterized DNN model, $y$ is the true label, $y^{\prime}$
    is the target label. As is this is a hard problem. Using a box constrained L-BFGS
    the authors proposed an approximate solution to the problem stated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\displaystyle\delta}{\mathrm{min}}\quad c\left\&#124;\delta\right\&#124;_{2}+\mathcal{L}(f(x+\delta),y^{\prime})\hfil\hfil\hfil\hfil$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{min}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{min}><quot;)][l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle\text{ all pixel in }(x+\delta)\in[0,1]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: In this model, Szegedy et. al., managed to generate images that were visually
    indistinguishable from the original ones, but were able to fool the classifiers
    into classifying them as another class. This was the first result and publication
    which has exploited this weakness of deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Fast Sign Gradient Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In , [[56](#bib.bib56)], introduced a one step adversarial attack framework.
    The attack image, $x^{\prime}$, is obtained by a simple additive disturb:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle x^{\prime}=$ | $\displaystyle~{}x+\delta_{ut}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle x^{\prime}=$ | $\displaystyle~{}x-\delta_{tg}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'in which for the untargeted setting, $\delta_{ut}$, we obtain the perturbation
    from:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\left\&#124;\delta_{ut}\right\&#124;_{p}\leq\epsilon}\mathcal{L}(f(x+\delta),y)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'and in the the targeted setting, $\delta_{tg}$, from:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\left\&#124;\delta_{tg}\right\&#124;_{p}\leq\epsilon}(\mathcal{L}(f(x+\delta),y)-\mathcal{L}(f(x+\delta),y^{\prime}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: in which $\epsilon$ is the ball defined normally by an $l_{p}$-norm.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/64f9d014af7879c321f4b14deb339a7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: By adding an unoticeble perturbation the dog, previously classified
    as from the breed redbone, is classified as a Hand blower, with high confidence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The core of fast sign gradient method maximize the norm of the vector between
    originally labeled class and the currently assigned label, while in the targeted
    setting, it focus on minimizing the distance to the target class. As it is a one-step
    algorithm, it is not very resilient to current defenses but is very fast implementation.
    [Figure 3](#S3.F3 "Figure 3 ‣ 3.1.2 Fast Sign Gradient Method ‣ 3.1 White-box
    Attacks ‣ 3 Methods for Generating Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey") shows the attack of an image
    and the false prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 DeepFool
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Deepfool attack proposed by [[57](#bib.bib57)], is a white box attack which
    explores the boundaries of the classification model. In the multi-class algorithm,
    Deepfool initializes with an input $x$ which is assumed to be within the boundaries
    of the classifier model $f(x)$. With an iterative process, the image is perturbed
    by a small vector towards the direction of the decision boundaries. The boundaries
    are approximated by linear functions, more specifically a hyperplane, defined
    in the algorithm as $\hat{l}$. At each step, the perturbations are accumulated
    to form the final perturbation to the image. With smaller perturbations than in
    FGSM ([[56](#bib.bib56)]), the authors have shown similar or better attack success
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 Jacobian-based Saliency Map Attack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Jacobian-based Saliency Map attack (JSMA) differs from most of the adversarial
    attack literature with respect to the norm it uses on the perturbation restriction.
    While most of the attacks focus on the $l_{\infty}$ or $l_{2}$ norms, JSMA, proposed
    in [[58](#bib.bib58)], focus on the $l_{0}$ norm. Under this norm, penalizes the
    change in a binary way, if the pixel has been changed or not, opposed to $l_{2}$
    based algorithms which takes into consideration the size of the change in the
    pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this attack, Papernot et. al., calculates the Jacobian of a score matrix
    $F$. The model executes the attack in a greedy way. It modifies the pixel which
    has the highest impact on the model’s decision. The Jacobian Matrix is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $J_{F}(x)=\frac{\partial F(x)}{\partial(x)}=\{\frac{\partial F_{j}(x)}{\partial
    x_{i}}\}_{x\times j}$ |  |'
  prefs: []
  type: TYPE_TB
- en: it models the influence of changes in the input $x$ to the predicted label $\hat{y}$.
    One at a time pixels from the unperturbed image are modified by the algorithm
    in order to create a salience map. The main idea of salience map is the correlation
    between the gradient of the output and the input. It is a guide to the most influential
    variables of the input, or the ones that probably can deceive the classifier with
    less manipulation. Based on that, the algorithm performs modifications in the
    most influential pixel.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.5 Projected Gradient Descend (PGD)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Also known as the basic iterative method, was initially proposed in [[59](#bib.bib59)].
    It is based on the FGSM, but instead a single step of the projected gradient descend,
    it iterates through more steps, as in:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\delta:=P(\delta+\alpha\nabla_{\delta}\mathcal{L}(f(x+\delta),y))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: in which $P$ denotes the projection over the ball of interest. With such formulation,
    the PGD, requires more fine-tuning, in choosing the step size $\alpha$. In [[54](#bib.bib54)],
    Madry et. al. proposed an iterative method with a random initialization for $\delta$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.6 Carlini and Wagner Attacks (CW)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In [[60](#bib.bib60)], 3 $l_{p}$-norm attacks ($l_{0}$,$l_{2}$,$l_{\infty}$)
    were proposed as a response to [[61](#bib.bib61)], which proposed the use of Distillation
    as a defense strategy. In their paper, Papernot et. al., successfully presented
    a defense mechanism capable of reducing the effectiveness of the FGSM and L-BFGS.
    CW proposes to solve the same problem stated in FGSM, which is given an input
    $x$ find a minimal perturbation $\delta$ capable of shifting the classification
    prediction of the model. The problem is addressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\displaystyle\delta}{\mathrm{min}}\quad c\left\&#124;\delta\right\&#124;_{p}+\mathcal{L}(f(x+\delta),y^{\prime})\hfil\hfil\hfil\hfil$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{min}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{min}><quot;)][l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle(x+\delta)\in[0,1]^{n}$ |  |'
  prefs: []
  type: TYPE_TB
- en: in which $\mathcal{L}(f(x+\delta),y^{\prime})=\max_{i\neq y^{\prime}}(Z(x^{\prime})_{i})-Z(x^{\prime})_{y})^{+}$,
    and $Z(x)=z$ are the logits. As the algorithm minimizes the metrics $\mathcal{L}(.)$,
    it finds the input $x^{\prime}$ that has larger score to be classified as $y^{\prime}$.
    As we search for the value of $c$, we look for the constant which will produce
    the smaller distance between $x$ and $x^{\prime}$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.7 Ground Truth Adversarial Example (GTAE)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So far most of the attacks, even if motivated by the generation of new defenses,
    are independent of the defense algorithm. In the algorithm proposed by [[62](#bib.bib62)],
    the certified defense proposed in [[63](#bib.bib63)], is used as a base for the
    optimization and search for adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm abstract the $\theta$ and dataset $(x,y)$ with the use of an SMT
    solver, and solves the system to check if there exist $x^{\prime}$ near $x$, within
    the established norm distance, which can cause a misclassification. The ground
    truth adversarial example is found by reducing the size of $\epsilon$ up to the
    point that the system can no longer find a suitable $x^{\prime}$. The adversarial
    example is considered the $x^{\prime}$ found with the immediately larger $\epsilon$.
    It is the first method to calculate an exact provable solution to a minimal perturbation
    which can fool ML models. In contrast, as stated by the authors, the fact that
    the model relies on an SMT solver, restrict the applicability of the algorithm
    to models with no more than a few hundred nodes. This attack has been revisited
    by [[64](#bib.bib64)] and [[65](#bib.bib65)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.8 Universal Adversarial Perturbations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Different from the previous methods, the universal adversarial perturbation
    (UAP), proposed in [[66](#bib.bib66)], search for a single perturbation capable
    of fooling all samples from the training dataset. The perturbations, independent
    of the input, are also restricted to not be detected by humans. The perturbations
    are constructed based on:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P_{x\sim\mathcal{D}}(f(x)\neq f(x+\delta))\geq\beta,\text{s. t. }\left\&#124;\delta\right\&#124;_{p}\leq\epsilon$
    |  |'
  prefs: []
  type: TYPE_TB
- en: in which $\epsilon$ defines the size of the perturbation based on an $l_{p}$-norm
    and $\beta$ defines the probability of an image sampled from the training dataset
    being fooled by the generated perturbation. In this case, the algorithm optimizes
    the probability of fooling the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The method to calculate the universal perturbations is based on the DeepFool
    algorithm, in which the input is gradually pushed towards the model’s decision
    boundary. It differs from DeepFool in the fact that instead of pushing a single
    input, all members of $\mathcal{D}$ are modified in the direction of the decision
    boundary. The perturbations, calculated for each image, are accumulated in a gradual
    manner. The accumulator is then projected back towards the specified $B_{\epsilon}$
    ball, of radius $\epsilon$. Its been shown that with variations of $4\%$, a fooling
    accuracy of $80\%$ has been achieved.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.9 Shadow Attack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In [[67](#bib.bib67)], an attack targeting certified defenses was proposed.
    In their work, they target defenses that certify the model with respect to a radius
    defined by the $l_{p}$-norm. One intuitive idea to construct a certified defense
    is to check within a certain radius $B_{\epsilon}$ of input, the existence of
    a perturbation $\delta$, capable of changing the decision of the classifier. The
    shadow attack is constructed to leverage this premise, and construct a perturbation
    outside of the certification zone. It is claimed that after labeling an image,
    these defenses check whether there exists an image of a different label within
    $\epsilon$ distance (in $l_{p}$ metric) of the input, where $\epsilon$ is a security
    parameter selected by the user. If within the $B_{\epsilon}$ ball all inputs are
    classified with the same label, then the model is robustly certified. Their model
    targets not only the classifier but also the certificate. It is done by adding
    adversarial perturbations to images that are large in the $l_{p}$-norm and produce
    attack images that are surrounded by a large ball exclusively containing images
    of the same label. In order to produce images that are close to the original,
    in a perception way, but can fool the classifier, they use the following objective
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\underset{\displaystyle y^{\prime}\neq y,~{}\delta}{\mathrm{max}}\quad$
    | $\displaystyle-\mathcal{L}(\theta,x+\delta&#124;y^{\prime})-\lambda_{c}C(\delta)\hfil\hfil\hfil\hfil$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{y''
    \neq y,~{}\delta}}{\mathrm{max}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{max}><quot;)][l]{}}\quad-\lambda_{tv}TV(\delta)-\lambda_{s}Dissim(\delta)\hfil\hfil\hfil\hfil$
    |  |'
  prefs: []
  type: TYPE_TB
- en: in which $\mathcal{L}(\theta,x+\delta|\bar{y})$ refers to the adversarial training
    loss, $\lambda_{c}C(\delta)$ is a color regularization term, $\lambda_{tv}TV(\delta)$
    is a smoothness penalty term, and $\lambda_{s}Dissim(\delta)$ guarantees that
    all color channels receive similar perturbation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.10 Other Attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The presented attacks are just part of many more which have been published in
    many different venues. Here we list some other relevant attack methods available
    in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'EAD: Elastic-net attack - Similar to L-BFGS the algorithm in [[68](#bib.bib68)]
    proposes to find the minimum additive perturbation, which misleads the classifier.
    Differently it incorporates an association of the norms $l_{1}$ and $l_{2}$. It
    has been shown that strong defenses against $l_{\infty}$ and $l_{2}$ norms still
    fail to reject $l_{1}$ based attacks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objective Metrics and Gradient Descend Algorithm (OMGDA) - The algorithm proposed
    by [[69](#bib.bib69)], is very similar to DeepFool, with the optimization of the
    step size. Instead of utilizing a fixed and heuristically determined step size
    in the optimization, in Jang et al., the step size utilizes insights from the
    softmax layer. The step size is determined based on the size of the desired perturbation
    and varies over time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spatially Transformed Attack (STA) - In [[70](#bib.bib70)], instead of generating
    changes in the intensity of the pixels, the authors have proposed a method based
    on small translational and rotational perturbations. The perturbations are still
    not noticeable by the human eyes. Similarly in [[71](#bib.bib71)] the spatial
    aspect of the input is also exploited for the adversarial example generation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unrestricted Adversarial Examples with Generative Models (UAEGM) - Based on
    AC-GAN ([[72](#bib.bib72)]), [[73](#bib.bib73)], has proposed the use of generative
    networks to generate examples which are not restricted to being in the neighborhood
    of the input data. The generated attacks, are not necessarily similar to the ones
    in the dataset but are similar enough to humans not notice and fool the classifiers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2 Black-Box Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Under Black box restriction, models are different from the currently exposed
    white box, with respect to the information the attacker has access to. In most
    cases, the adversary does not have any or some information about the targeted
    model, like the algorithm used, dataset, or parameters, as seen in [Figure 2](#S3.F2
    "Figure 2 ‣ 3 Methods for Generating Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey"). An important modeling challenge
    for black-box attacks is to model precisely what information the attacker has
    about either the learned model or the algorithm. In this sub-section, we list
    the most relevant methods for black attack generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Practical Black-Box Attacks (PBBA)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While assuming access to the information of the model enables a series of attacks,
    the work in [[74](#bib.bib74)], introduces the possibility of attacking models
    about which the attacker has less knowledge. In this work, no knowledge about
    the architecture is assumed, only some idea of the domain of interest. It is also
    limited the requests for output sent to the model, which requires the attacker
    to choose wisely the inference requests from the victim’s model. To achieve the
    goal, Papernot et. al. introduce the substitute model framework. The attack strategy
    is to train a substitute network on a small number of initial queries, then iteratively
    perturb inputs based on the substitute network’s gradient information to augment
    the training set.
  prefs: []
  type: TYPE_NORMAL
- en: '1:Input: Substitute dataset $S_{0}$.2:Input: Substitute model architecture
    $F$.3:while $\rho\leq$ epochs do4:     Label $S_{0}$ based on queries from original
    model5:     Train substitute model based on (3).6:     Augment dataset based on
    Jacobian, $S_{\rho}\leftarrow S_{\rho+1}$7:return'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Substitute Model
  prefs: []
  type: TYPE_NORMAL
- en: With the boundaries of substitute model adjusted to be close to the original
    model, any of the methods presented in the previous section can be used to generate
    the perturbed image.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Zeroth Order Optimization Based Attack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Chen et al. [[52](#bib.bib52)], in the attack also known as ZOO, the authors
    assume accessibility to both, the input data and the confidence scores of the
    model in which they target their attack. They differ from [[74](#bib.bib74)] in
    the fact that the model does not focus on transferability (creating a substitute
    model) to achieve the adversarial examples. In their work, they propose a zeroth-order
    optimization attack which estimates the gradient of the targeted DNN. Instead
    of traditional gradient descend, they use order 0 coordinate SGD. Moreover, to
    improve their model and enable the adversarial example generation, they implement
    dimensionality reduction, hierarchical techniques, and importance sampling. As
    the pixels are tuned, the algorithm observes the changes in the confidence scores.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the ZOO, in the one-pixel attack [[75](#bib.bib75)], it is proposed
    the use of the score confidence to perturb the input and change the decision of
    the classifier. This paper focuses on modifying a single pixel of the input. With
    the use of differential evolution, the single pixel is modified in a black-box
    setting. The authors base the update of the perturbation in the variation of the
    probability scores for each class.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Query-Efficient Black-Box Attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the biggest challenges in black-box attacks is the fact that many inference
    models have mechanisms to restrict the number of queries (when cloud-based or
    system embedded), or the inference time can restrict the number of queries. One
    line of research in black-box models looks into making such models more query
    efficient, for example, the work from [[76](#bib.bib76)], based on natural evolution
    strategies reduces by 2 or 3 order of magnitude the amount of information requests
    sent to the model to successfully generate a misclassified perturbed image. The
    algorithm set queries in the neighborhood of the input $x$. The output of the
    model is then sampled, and these samples are used to estimate the expectation
    of the gradient around the point of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm sample the model’s output based on the queries around the input
    $x$, and estimate the expectation of a gradient of $F$ on $x$. More on the topic,
    [[77](#bib.bib77)], proposes a family of algorithms based on a new gradient direction
    estimate using only the binary classification of the model. In their work, it
    is included $l_{\infty}$ and $l_{2}$ norm-based attacks as well as targeted and
    untargeted attacks. [Figure 4](#S3.F4 "Figure 4 ‣ 3.2.3 Query-Efficient Black-Box
    Attacks ‣ 3.2 Black-Box Attacks ‣ 3 Methods for Generating Adversarial Attacks
    ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey")
    shows an intuition on how the gradient is updated and the boundaries of the decision
    are used to generate the adversarial attack. Algorithm [2](#alg2 "Algorithm 2
    ‣ 3.2.3 Query-Efficient Black-Box Attacks ‣ 3.2 Black-Box Attacks ‣ 3 Methods
    for Generating Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey") shows how the dimensionality reduction $d^{r}$
    is defined.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/50a2f96da378bcc750491ef0813aff99.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The HopSkipJumpAttack. (a) With binary search find the boundary of
    the decision. (b) Generate an estimate of the gradient near the decision limit.
    (c) Update the decision limit point with the use of geometric progression. (d)
    Do a binary search and update the estimate of the boundary point. Image Source:
    [[77](#bib.bib77)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, in the search for query efficient black box attack, [[78](#bib.bib78)]
    introduces a method that is independent of the gradient based on Bayesian optimization
    and Gaussian process surrogate models to find effective adversarial examples.
    In the model it is assumed that the attacker has no knowledge of the network architecture,
    weights, gradient or training data of the target model. But it is assumed that
    the attacker can query model with input $x$, to obtain the prediction scores on
    all classes $C$. They restrict the perturbation to the $l_{\infty}$ norm. Their
    objective is to maximize over the perturbations:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\delta^{*}={}$ | $\displaystyle\underset{\displaystyle\delta}{\mathrm{arg~{}max}}\quad[log(f(x_{origin}+g(\delta))_{t})\hfil\hfil\hfil\hfil$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{arg~{}max}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{arg~{}max}><quot;)][l]{}}\quad-log(\sum_{j\neq
    t}^{C}f(x_{origin}+g(\delta))_{j})]\hfil\hfil\hfil\hfil$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{arg~{}max}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{arg~{}max}><quot;)][l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle\delta\in[-\delta_{max},\delta_{max}]^{d_{r}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The Bayesian optimization proposed to improve the query efficiency requires
    the use of a surrogate model to approximate the objective function, in their work
    a Gaussian Process is used. Moreover to define the next query point is defined
    by an acquisition function. A big differential in their work is the fact that
    instead of searching in a high-dimensional space for a perturbation $\delta$,
    they utilize a function to reduce the dimensionality of the perturbation and later
    reconstitute to the true image size.
  prefs: []
  type: TYPE_NORMAL
- en: '1:Input: Decoder g(.), observation $\mathcal{D}^{d}_{t-1}={g(\delta_{i}),y_{i}}^{t-1}_{i=1}$
    where $g(\delta_{i})\in\mathcal{R}^{d}$ and a set of possible $d^{r}:{d^{r}_{j}}^{N}_{j=1}$2:Output:
    The optimal reduced dimension $d^{r*}$ and corresponding GP model.3:for $j=1,...,N$ do4:     $\mathcal{D}^{d^{r}_{j}}_{t-1}=\{g^{-1}(g(\delta_{i})),y_{i}\}^{t-1}_{i=1},$
    $\triangleright$ $g^{-1}(g(\delta_{i}))\in R^{d^{r}_{j}}$5:     Fit a GP model
    to $\mathcal{D}^{d^{r}_{j}}_{t-1}$ and compute its maximum marginal likelihood
    $p(\mathcal{D}^{d}_{t-1}|\theta^{*},d^{r}_{j})$6:$d^{r*}=\operatorname*{arg\,max}_{d^{r}_{j}\in{d^{r}_{j}}^{N}_{j=1}}p(\mathcal{D}^{d}_{t-1}|\theta^{*},d^{r}_{j})$
    and its7:return'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Bayesian selection of $d^{r}$
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Attack on RL algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In [[79](#bib.bib79)], a method for generating adversarial examples in reinforcement
    learning (RL) algorithms was proposed. In RL, an adversarial example can either
    be a modified image used to capture a state or in the case of this publication,
    an adversarial policy. It is important to highlight that an adversarial policy
    is not a strong adversary as we have in two-player games, but one that with a
    certain behavior triggers a failure in the victim’s policy. In this paper, a black-box
    attack is proposed to trigger bad behaviors in the victim’s policy. The victim’s
    policy is trained using Proximal Policy Optimization and learns to "play" against
    a fair opponent. The adversarial policy is trained to trigger failures in the
    victim’s policy. [Figure 5](#S3.F5 "Figure 5 ‣ 3.2.4 Attack on RL algorithm ‣
    3.2 Black-Box Attacks ‣ 3 Methods for Generating Adversarial Attacks ‣ Opportunities
    and Challenges in Deep Learning Adversarial Robustness: A Survey") shows the difference
    between an opponent’s policy and an adversarial manipulated policy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7d3d8e8a2c8c6bb2da1e5b4c107236f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: In the first sequence, a strong opponent has to enter collide with
    the agent to prevent it from winning the game. In the second line, in the adversarial
    example, the opponent simply crumbles on the floor, which triggers a bad behavior
    in the victim’s policy. Image Source: [[79](#bib.bib79)]'
  prefs: []
  type: TYPE_NORMAL
- en: Also in the paper, it was shown the dependence of the size of the input space
    and the effectiveness of adversarial policies. The greater the dimensionality
    of the observation space under the control of the adversary, the more vulnerable
    the victim is to attack.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Physical World Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The research presented so far is mostly focused on applying the attacks in virtual
    applications and controlled datasets, but the great concern about the existence
    of adversarial examples is the extent to which they can imply severe consequences
    to the users of the system. With that objective, we dedicate this session on exploring
    publications with real-world applications and consequences clearly stated.
  prefs: []
  type: TYPE_NORMAL
- en: In [[80](#bib.bib80)], road signs were physically attacked by placing a sticker
    in a specific position of the sign. Their attack consisted of initially finding,
    in a sign image, the location with the most influence for the decision of the
    classifier. For that objective, an $l_{1}norm$ was used because it renders sparse
    perturbations in the image, making it easier to locate the modification patches.
    Based on the defined location, an $l_{2}norm$ was used to identify the most appropriate
    color for the sticker.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, as face recognition is becoming very popular as a biometric security
    measure it is the focus of several adversarial attacks. We highlight 4 attacks
    in face recognition and identification.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation of the robustness of DNN models to Face Recognition against adversarial
    attacks - In this publication Goswami et al. [[81](#bib.bib81)] evaluates how
    the depth of the architecture impacts the robustness of the model in identifying
    faces. They evaluate the robustness with respect to adversarial settings taking
    into consideration distortions which are normally observed in a common scene.
    These distortions are handled with ease by shallow networks on the contrary of
    deep networks. In their approach, they’ve used Open-Face and VGG-Face networks,
    and have achieved a high fooling rate. It is important to notice that in their
    attack no restrictions are made in the visibility of the perturbations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial Attacks on Face Detectors using Neural Net based Constrained Optimization
    In this work, also focusing on prevent face identification, [[82](#bib.bib82)]
    has generated an attack, based on Carlini and Wagner attack, which was able to
    fool R-CNN. Their perturbations are not visible in the adversarial example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating Adversarial Examples by Makeup Attacks on Face Recognition - In this
    research, [[83](#bib.bib83)] implement a GAN network to generate make-up perturbation.
    When the perturbation is applied to the face, the classifier shifts its decision
    to the target class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient decision-based black-box adversarial attacks on face recognition -
    Dong et al.[[84](#bib.bib84)] propose an evolutionary attack method. The proposed
    adversarial example generator is constrained in a black-box setting. The algorithm
    focus on reducing the number of dimensions of the search space by modeling the
    local geometry of the search vectors. Such an algorithm has been shown to be applicable
    to most recognition tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.3.1 Other Attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the field of Cyber-security machine learning models are, in general, applied
    to detect malware, malicious connections, malicious domain classifier, and others.
    In Suciu et al. [[85](#bib.bib85)] an evaluation of the robustness of current
    malware detection models is performed. The authors retrain the model in a production-scale
    dataset to perform the evaluation. With the new data, the model which was previously
    vulnerable to attacks was shown to be stronger and architectural weaknesses were
    reported. The work of Suciu et al. [[85](#bib.bib85)], explores how attacks transfer
    in the cyber-security domain, and mainly the inherent trade-off between effectiveness
    and transferability. With respect to malicious connection and domain, Chernikova
    et al. [[86](#bib.bib86)] builds a model that takes into consideration the formal
    dependencies generated by the normal operations applied in the feature space.
    The model to generate adversarial examples simultaneously consider both the mathematical
    dependencies and the real-world constraints of such applications. The algorithm
    focus on determining the features with higher variability and the ones with a
    higher correlation with these features. This search is performed for each iteration.
    All the identified features are modified but constrained to preserve an upper
    bound on the maximum variation of the features. The upper bound respects physical-world
    application limitations.
  prefs: []
  type: TYPE_NORMAL
- en: In the Cyber-Physical domain, several publications demonstrate the brittleness
    of ML models and the generation of adversarial examples. [[87](#bib.bib87)] generated
    adversarial examples to an iCub Humanoid Robot. The attack proposed simply extends
    over the attacks in [[88](#bib.bib88)]. The main aspect to be considered in this
    paper is the fact that it highlights the high consequences of the adversarial
    examples in the decision process of safety-critical applications. Moreover, in
    self-driving cars several attacks have been derived like DARTS ([[89](#bib.bib89)])
    and the work of [[90](#bib.bib90)] which shows the attack of traffic signs, the
    latter with real experiments. In a different sensor type, the work of [[91](#bib.bib91)]
    demonstrates the attack to a LIDAR sensor, in which they attack the point cloud
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover in [[92](#bib.bib92)], a novel technique was proposed to attack object
    tracking algorithms. In their approach, the bounding box is attacked in a single
    frame, which is enough to fool the algorithm and generate an offset in the placement
    of the bounding box. Such an attack would be critical to self-driving cars to
    recognize the position of obstacles, other vehicles, and pedestrians on the road.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Dichotomy of the Attacks'
  prefs: []
  type: TYPE_NORMAL
- en: '| WHITE BOX ATTACKS |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Algorithm | Metric | Step^a | Domain^b |'
  prefs: []
  type: TYPE_TB
- en: '| L-BFGS ([[43](#bib.bib43)]) | $l_{2}$ | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| FGSM ([[56](#bib.bib56)]) | $l_{\infty},l_{2}$ | S-Stp | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| Deepfool ([[57](#bib.bib57)]) | $l_{2}$ | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| JSMA ([[58](#bib.bib58)]) | $l_{2}$ | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| PGD ([[54](#bib.bib54)]) | $l_{\infty},l_{2}$ | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| C and W ([[60](#bib.bib60)]) | $l_{0}$,$l_{2}$,$l_{\infty}$ | Iter. | Im-C
    |'
  prefs: []
  type: TYPE_TB
- en: '| GTAE ([[62](#bib.bib62)]) | $l_{0}$ | SMT | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| UAP ([[66](#bib.bib66)]) | $l_{\infty},l_{2}$ | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| EAD ([[68](#bib.bib68)]) | $l_{1},l_{2}$ | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| OMGDA ([[69](#bib.bib69)]) | $l_{2}$ | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| STA ([[70](#bib.bib70)]) | Spt-Var | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| UAEGM ([[69](#bib.bib69)]) | $l_{2}$ | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| Shadow Attack ([[67](#bib.bib67)]) | $l_{p}$ | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| RoadSign ([[80](#bib.bib80)]) | $l_{1}$,$l_{2}$ | Iter. | S-R |'
  prefs: []
  type: TYPE_TB
- en: '| FRA1 ([[81](#bib.bib81)]) | NR | S-Stp | F-Rec |'
  prefs: []
  type: TYPE_TB
- en: '| FRA2 ([[82](#bib.bib82)]) | $l_{2}$ | Iter. | F-Rec |'
  prefs: []
  type: TYPE_TB
- en: '| FRA3 ([[83](#bib.bib83)]) | $l_{1}$ | Iter. | F-Rec |'
  prefs: []
  type: TYPE_TB
- en: '| CSA2 ([[86](#bib.bib86)]) | $l_{2}$ | Iter. | Cyb-Sec |'
  prefs: []
  type: TYPE_TB
- en: '| CPA1 ([[87](#bib.bib87)]) | $l_{2}$ | Iter. | Cyb-Phy |'
  prefs: []
  type: TYPE_TB
- en: '| CPA3 ([[90](#bib.bib90)]) | $l_{0}$ | Iter. | Cyb-Phy |'
  prefs: []
  type: TYPE_TB
- en: '| CPA4 ([[91](#bib.bib91)]) | NR | Iter. | Cyb-Phy |'
  prefs: []
  type: TYPE_TB
- en: '| CPA5 ([[92](#bib.bib92)]) | $l_{1},l_{2}$ | Iter. | Cyb-Phy |'
  prefs: []
  type: TYPE_TB
- en: '| BLACK BOX ATTACKS |'
  prefs: []
  type: TYPE_TB
- en: '| Method | Metric | Step^a | Domain^b |'
  prefs: []
  type: TYPE_TB
- en: '| PBBA ([[74](#bib.bib74)]) | $l_{p}$ | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| ZOO ([[52](#bib.bib52)]) | $l_{p}$ | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| One-Pixel ([[75](#bib.bib75)]) | $l_{0}$ | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| DBA ([[93](#bib.bib93)]) | $l_{2}$ | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| HopSkipJumpAttack ([[77](#bib.bib77)]) | $l_{2}$,$l_{\infty}$ | Iter. | Im-C
    |'
  prefs: []
  type: TYPE_TB
- en: '| UPSET & ANGRI ([[94](#bib.bib94)]) | $l_{\infty}$ | Iter. | Im-C |'
  prefs: []
  type: TYPE_TB
- en: '| RLAttack ([[79](#bib.bib79)]) | NR | Iter. | RL |'
  prefs: []
  type: TYPE_TB
- en: '| FRA4 ([[84](#bib.bib84)]) | $l_{2}$ | Iter. | F-Rec |'
  prefs: []
  type: TYPE_TB
- en: '| CSA1 ([[85](#bib.bib85)]) | $l_{\infty}$ | Iter. | Cyb-Sec |'
  prefs: []
  type: TYPE_TB
- en: '| CPA2 ([[89](#bib.bib89)]) | $l_{1}$ | Iter. | Cyb-Phy |'
  prefs: []
  type: TYPE_TB
- en: '| a - Iter.: Iterative, S-Stp.: Single Step; b - Im-C.: Image Classification,
    |'
  prefs: []
  type: TYPE_TB
- en: '| S-R.: Signal Recognition, F-Rec.: Face Recognition, Cyb-Sec.: Cyber- |'
  prefs: []
  type: TYPE_TB
- en: '| Security, Cyb-Phys.: Cyber-Physical, RL.: Reinforcement Learning |'
  prefs: []
  type: TYPE_TB
- en: <svg   height="701.36" overflow="visible"
    version="1.1" width="320.01"><g transform="translate(0,701.36) matrix(1 0 0 -1
    0 0) translate(158.76,0) translate(0,685.4)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt" ><g transform="matrix(1.0 0.0 0.0 1.0
    -39.37 4.25)" fill="#000000" stroke="#000000"><foreignobject width="78.74" height="24"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Robust Optimization <g
    stroke-width="0.4pt" ><g stroke-opacity="0.5" fill="#808080"
    fill-opacity="0.5" transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -70.57
    -43.6 L -149.9 -43.6 C -154.49 -43.6 -158.2 -47.32 -158.2 -51.91 L -158.2 -66.2
    C -158.2 -70.79 -154.49 -74.51 -149.9 -74.51 L -70.57 -74.51 C -65.99 -74.51 -62.27
    -70.79 -62.27 -66.2 L -62.27 -51.91 C -62.27 -47.32 -65.99 -43.6 -70.57 -43.6
    Z M -158.2 -74.51" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#E6E6E6"><path
    d="M -70.57 -43.6 L -149.9 -43.6 C -154.49 -43.6 -158.2 -47.32 -158.2 -51.91 L
    -158.2 -66.2 C -158.2 -70.79 -154.49 -74.51 -149.9 -74.51 L -70.57 -74.51 C -65.99
    -74.51 -62.27 -70.79 -62.27 -66.2 L -62.27 -51.91 C -62.27 -47.32 -65.99 -43.6
    -70.57 -43.6 Z M -158.2 -74.51"></path></g><g transform="matrix(1.0 0.0 0.0 1.0
    -154.51 -55.04)" fill="#000000" stroke="#000000"><foreignobject width="88.56"
    height="23.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Adversarial
    Training</foreignobject></g></g> <g stroke-width="0.4pt"><g ><path
    d="M -29.79 -15.96 L -76.5 -40.98" style="fill:none"></path></g><g transform="matrix(-0.88156
    -0.47208 0.47208 -0.88156 -76.5 -40.98)" ><path d="M 4.98
    0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98
    0" style="stroke:none"></path></g><g stroke-width="0.4pt" ><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.38 -2.38)"><path d="M 39.67 -51.49 L -39.67 -51.49 C -44.25 -51.49 -47.97
    -55.21 -47.97 -59.79 L -47.97 -58.32 C -47.97 -62.9 -44.25 -66.62 -39.67 -66.62
    L 39.67 -66.62 C 44.25 -66.62 47.97 -62.9 47.97 -58.32 L 47.97 -59.79 C 47.97
    -55.21 44.25 -51.49 39.67 -51.49 Z M -47.97 -66.62" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#E6E6E6"><path d="M 39.67 -51.49 L -39.67 -51.49 C
    -44.25 -51.49 -47.97 -55.21 -47.97 -59.79 L -47.97 -58.32 C -47.97 -62.9 -44.25
    -66.62 -39.67 -66.62 L 39.67 -66.62 C 44.25 -66.62 47.97 -62.9 47.97 -58.32 L
    47.97 -59.79 C 47.97 -55.21 44.25 -51.49 39.67 -51.49 Z M -47.97 -66.62"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -44.28 -62.93)" fill="#000000" stroke="#000000"><foreignobject
    width="88.56" height="7.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Certified
    Defenses</foreignobject></g></g> <g ><path d="M 0 -15.96
    L 0 -46.23" style="fill:none"></path></g><g transform="matrix(0.0 -1.0 1.0 0.0
    0 -46.23)" ><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55
    2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><g
    stroke-width="0.4pt" ><g stroke-opacity="0.5" fill="#808080"
    fill-opacity="0.5" transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M 149.9
    -43.22 L 70.57 -43.22 C 65.99 -43.22 62.27 -46.94 62.27 -51.52 L 62.27 -66.59
    C 62.27 -71.17 65.99 -74.89 70.57 -74.89 L 149.9 -74.89 C 154.49 -74.89 158.2
    -71.17 158.2 -66.59 L 158.2 -51.52 C 158.2 -46.94 154.49 -43.22 149.9 -43.22 Z
    M 62.27 -74.89" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#E6E6E6"><path
    d="M 149.9 -43.22 L 70.57 -43.22 C 65.99 -43.22 62.27 -46.94 62.27 -51.52 L 62.27
    -66.59 C 62.27 -71.17 65.99 -74.89 70.57 -74.89 L 149.9 -74.89 C 154.49 -74.89
    158.2 -71.17 158.2 -66.59 L 158.2 -51.52 C 158.2 -46.94 154.49 -43.22 149.9 -43.22
    Z M 62.27 -74.89"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 65.96 -54.66)"
    fill="#000000" stroke="#000000"><foreignobject width="88.56" height="24.29" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Regularization Approach</foreignobject></g></g>
    <g ><path d="M 29.8 -15.96 L 75.78 -40.59" style="fill:none"></path></g><g
    transform="matrix(0.88155 -0.4721 0.4721 0.88155 75.78 -40.59)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><g stroke-width="0.4pt" ><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.38 -2.38)"><path d="M -138.83 -118.72 h 79.33 v 40.59 h -79.33 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#F2F2F2"><path d="M -138.83 -118.72 h 79.33 v 40.59
    h -79.33 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -135.14 -89.57)" fill="#000000"
    stroke="#000000"><foreignobject width="71.95" height="33.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Wang et al. 2020 [[95](#bib.bib95)]</foreignobject></g></g>
    <g stroke-width="0.4pt" ><g stroke-opacity="0.5" fill="#808080"
    fill-opacity="0.5" transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -138.83
    -158.09 h 79.33 v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt"
    fill="#F2F2F2"><path d="M -138.83 -158.09 h 79.33 v 40.59 h -79.33 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -135.14 -128.94)" fill="#000000" stroke="#000000"><foreignobject
    width="71.95" height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Song
    et al. 2019 [[96](#bib.bib96)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -138.83 -197.46 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -138.83 -197.46 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -135.14 -168.31)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Wu et al.
    2019 [[97](#bib.bib97)]</foreignobject></g></g> <g stroke-width="0.4pt" ><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.38 -2.38)"><path d="M -138.83 -236.83 h 79.33 v 40.59 h -79.33 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#F2F2F2"><path d="M -138.83 -236.83 h 79.33 v 40.59
    h -79.33 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -135.14 -207.68)"
    fill="#000000" stroke="#000000"><foreignobject width="71.95" height="33.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Wong et al. 2020 [[98](#bib.bib98)]</foreignobject></g></g>
    <g stroke-width="0.4pt" ><g stroke-opacity="0.5" fill="#808080"
    fill-opacity="0.5" transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -138.83
    -276.2 h 79.33 v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt"
    fill="#F2F2F2"><path d="M -138.83 -276.2 h 79.33 v 40.59 h -79.33 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -135.14 -247.05)" fill="#000000" stroke="#000000"><foreignobject
    width="71.95" height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Hu
    et al. 2020 [[99](#bib.bib99)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -138.83 -315.57 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -138.83 -315.57 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -135.14 -286.42)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Chen et
    al. 2019 [[100](#bib.bib100)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -138.83 -354.94 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -138.83 -354.94 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -135.14 -325.79)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Yang et
    al. 2019 [[101](#bib.bib101)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -138.83 -394.31 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -138.83 -394.31 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -135.14 -365.16)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Kannan et
    al. 2018 [[102](#bib.bib102)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -138.83 -440.6 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -138.83 -440.6 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -135.14 -411.45)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Matyasko
    et al. 2018 [[103](#bib.bib103)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -138.83 -479.97 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -138.83 -479.97 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -135.14 -450.82)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Sinha et
    al. 2018 [[104](#bib.bib104)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -138.83 -519.34 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -138.83 -519.34 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -135.14 -490.19)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Sen et al.
    2020 [[105](#bib.bib105)]</foreignobject></g></g> <g stroke-width="0.4pt" ><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.38 -2.38)"><path d="M -138.83 -555.94 h 79.33 v 35.05 h -79.33 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#F2F2F2"><path d="M -138.83 -555.94 h 79.33 v 35.05
    h -79.33 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -135.14 -532.33)"
    fill="#000000" stroke="#000000"><foreignobject width="71.95" height="27.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Liu et al. 2018 [[106](#bib.bib106)]</foreignobject></g></g>
    <g stroke-width="0.4pt" ><g stroke-opacity="0.5" fill="#808080"
    fill-opacity="0.5" transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -138.83
    -598.08 h 79.33 v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt"
    fill="#F2F2F2"><path d="M -138.83 -598.08 h 79.33 v 40.59 h -79.33 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -135.14 -568.93)" fill="#000000" stroke="#000000"><foreignobject
    width="71.95" height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Tramer
    et al. 2018 [[107](#bib.bib107)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -138.83 -637.45 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -138.83 -637.45 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -135.14 -608.3)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Madry et
    al. 2018 [[54](#bib.bib54)]</foreignobject></g></g> <g stroke-width="0.4pt" ><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.38 -2.38)"><path d="M -138.83 -685.12 h 79.33 v 40.59 h -79.33 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#F2F2F2"><path d="M -138.83 -685.12 h 79.33 v 40.59
    h -79.33 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -135.14 -655.97)"
    fill="#000000" stroke="#000000"><foreignobject width="71.95" height="33.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Goodfellow et al. 2015 [[56](#bib.bib56)]</foreignobject></g></g>
    <g stroke-width="0.4pt" ><g stroke-opacity="0.5" fill="#808080"
    fill-opacity="0.5" transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -28.6
    -118.72 h 79.33 v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt"
    fill="#F2F2F2"><path d="M -28.6 -118.72 h 79.33 v 40.59 h -79.33 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -24.91 -89.57)" fill="#000000" stroke="#000000"><foreignobject
    width="71.95" height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Wang
    et al. 2019 [[108](#bib.bib108)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -28.6 -158.09 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -28.6 -158.09 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -24.91 -128.94)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Zhai et
    al. 2020 [[109](#bib.bib109)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -28.6 -204.38 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -28.6 -204.38 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -24.91 -175.23)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Croce et
    al. 2019 [[110](#bib.bib110)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -28.6 -250.67 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -28.6 -250.67 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -24.91 -221.52)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Raghunathan
    et al. 2018 [[111](#bib.bib111)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -28.6 -303.87 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -28.6 -303.87 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -24.91 -274.72)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Wong et
    al. 2018 [[112](#bib.bib112)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -28.6 -357.08 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -28.6 -357.08 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -24.91 -327.93)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Boopathy
    et al. 2019 [[113](#bib.bib113)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -28.6 -403.37 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -28.6 -403.37 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -24.91 -374.22)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Zhang et
    al. 2018 [[114](#bib.bib114)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -28.6 -442.74 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -28.6 -442.74 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -24.91 -413.59)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Weng et
    al. 2018 [[115](#bib.bib115)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -28.6 -482.11 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -28.6 -482.11 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -24.91 -452.96)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Weng et
    al. 2018 [[116](#bib.bib116)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -28.6 -528.4 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -28.6 -528.4 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -24.91 -499.25)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Hein et
    al. 2017 [[117](#bib.bib117)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -28.6 -574.69 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -28.6 -574.69 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -24.91 -545.54)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Singh et
    al. 2018 [[118](#bib.bib118)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -28.6 -614.06 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -28.6 -614.06 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -24.91 -584.91)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Gehr et
    al. 2018 [[119](#bib.bib119)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M -28.6 -653.43 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M -28.6 -653.43 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -24.91 -624.28)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Katz et
    al. 2017 [[63](#bib.bib63)]</foreignobject></g></g> <g stroke-width="0.4pt" ><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.38 -2.38)"><path d="M 81.64 -115.95 h 79.33 v 35.05 h -79.33 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#F2F2F2"><path d="M 81.64 -115.95 h 79.33 v 35.05 h
    -79.33 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 85.33 -92.34)" fill="#000000"
    stroke="#000000"><foreignobject width="71.95" height="27.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Xie et al. 2019 [[55](#bib.bib55)]</foreignobject></g></g>
    <g stroke-width="0.4pt" ><g stroke-opacity="0.5" fill="#808080"
    fill-opacity="0.5" transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M 81.64
    -158.09 h 79.33 v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt"
    fill="#F2F2F2"><path d="M 81.64 -158.09 h 79.33 v 40.59 h -79.33 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 85.33 -128.94)" fill="#000000" stroke="#000000"><foreignobject
    width="71.95" height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Mao
    et al. 2019 [[120](#bib.bib120)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M 81.64 -197.46 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M 81.64 -197.46 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 85.33 -168.31)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Zhang et
    al. 2019 [[121](#bib.bib121)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M 81.64 -236.83 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M 81.64 -236.83 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 85.33 -207.68)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Tang et
    al. 2019 [[122](#bib.bib122)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M 81.64 -273.43 h 79.33
    v 35.05 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M 81.64 -273.43 h 79.33 v 35.05 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 85.33 -249.82)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Yan et al.
    2018 [[123](#bib.bib123)]</foreignobject></g></g> <g stroke-width="0.4pt" ><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.38 -2.38)"><path d="M 81.64 -321.1 h 79.33 v 35.05 h -79.33 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#F2F2F2"><path d="M 81.64 -321.1 h 79.33 v 35.05 h
    -79.33 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 85.33 -297.49)" fill="#000000"
    stroke="#000000"><foreignobject width="71.95" height="27.67" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Ros et al. 2018 [[124](#bib.bib124)]</foreignobject></g></g>
    <g stroke-width="0.4pt" ><g stroke-opacity="0.5" fill="#808080"
    fill-opacity="0.5" transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M 81.64
    -371.54 h 79.33 v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt"
    fill="#F2F2F2"><path d="M 81.64 -371.54 h 79.33 v 40.59 h -79.33 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 85.33 -342.39)" fill="#000000" stroke="#000000"><foreignobject
    width="71.95" height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Cisse
    et al. 2017 [[125](#bib.bib125)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M 81.64 -410.91 h 79.33
    v 40.59 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M 81.64 -410.91 h 79.33 v 40.59 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 85.33 -381.76)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Sokolic
    et al. 2017 [[126](#bib.bib126)]</foreignobject></g></g> <g stroke-width="0.4pt"
    ><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.38 -2.38)"><path d="M 81.64 -455.82 h 79.33
    v 35.05 h -79.33 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#F2F2F2"><path
    d="M 81.64 -455.82 h 79.33 v 35.05 h -79.33 Z"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 85.33 -432.2)" fill="#000000" stroke="#000000"><foreignobject width="71.95"
    height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Gu et al.
    2015 [[127](#bib.bib127)]</foreignobject></g></g><path d="M -158.48 -67.56 L -158.48
    -98.43 L -144.09 -98.43" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 -144.09 -98.43)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M -158.48 -67.56 L -158.48 -137.8 L -144.09 -137.8" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 -144.09 -137.8)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M -158.48 -67.56 L -158.48 -177.17
    L -144.09 -177.17" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 -144.09 -177.17)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M -158.48 -67.56 L -158.48 -216.54 L -144.09 -216.54" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 -144.09 -216.54)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M -158.48 -67.56 L -158.48 -255.91
    L -144.09 -255.91" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 -144.09 -255.91)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M -158.48 -67.56 L -158.48 -295.28 L -144.09 -295.28" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 -144.09 -295.28)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M -158.48 -67.56 L -158.48 -334.65
    L -144.09 -334.65" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 -144.09 -334.65)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M -158.48 -67.56 L -158.48 -374.02 L -144.09 -374.02" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 -144.09 -374.02)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M -158.48 -67.56 L -158.48 -420.3
    L -144.09 -420.3" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 -144.09 -420.3)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M -158.48 -67.56 L -158.48 -459.67 L -144.09 -459.67" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 -144.09 -459.67)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M -158.48 -67.56 L -158.48 -499.04
    L -144.09 -499.04" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 -144.09 -499.04)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M -158.48 -67.56 L -158.48 -538.41 L -144.09 -538.41" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 -144.09 -538.41)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M -158.48 -67.56 L -158.48 -577.78
    L -144.09 -577.78" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 -144.09 -577.78)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M -158.48 -67.56 L -158.48 -617.15 L -144.09 -617.15" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 -144.09 -617.15)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M -158.48 -67.56 L -158.48 -664.83
    L -144.09 -664.83" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 -144.09 -664.83)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M -44.48 -66.9 L -44.48 -98.43 L -33.85 -98.43" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 -33.85 -98.43)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M -44.48 -66.9 L -44.48 -137.8
    L -33.85 -137.8" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 -33.85 -137.8)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M -44.48 -66.9 L -44.48 -184.08 L -33.85 -184.08" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 -33.85 -184.08)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M -44.48 -66.9 L -44.48 -230.37
    L -33.85 -230.37" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 -33.85 -230.37)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M -44.48 -66.9 L -44.48 -283.58 L -33.85 -283.58" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 -33.85 -283.58)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M -44.48 -66.9 L -44.48 -336.79
    L -33.85 -336.79" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 -33.85 -336.79)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M -44.48 -66.9 L -44.48 -383.08 L -33.85 -383.08" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 -33.85 -383.08)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M -44.48 -66.9 L -44.48 -422.45
    L -33.85 -422.45" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 -33.85 -422.45)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M -44.48 -66.9 L -44.48 -461.82 L -33.85 -461.82" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 -33.85 -461.82)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M -44.48 -66.9 L -44.48 -508.1
    L -33.85 -508.1" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 -33.85 -508.1)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M -44.48 -66.9 L -44.48 -554.39 L -33.85 -554.39" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 -33.85 -554.39)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M -44.48 -66.9 L -44.48 -593.76
    L -33.85 -593.76" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 -33.85 -593.76)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M -44.48 -66.9 L -44.48 -633.13 L -33.85 -633.13" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 -33.85 -633.13)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M 61.99 -67.56 L 61.99 -98.43
    L 76.38 -98.43" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 76.38 -98.43)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M 61.99 -67.56 L 61.99 -137.8 L 76.38 -137.8" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 76.38 -137.8)" ><path d="M
    4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M 61.99 -67.56 L 61.99 -177.17
    L 76.38 -177.17" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 76.38 -177.17)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M 61.99 -67.56 L 61.99 -216.54 L 76.38 -216.54" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 76.38 -216.54)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M 61.99 -67.56 L 61.99 -255.91
    L 76.38 -255.91" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 76.38 -255.91)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M 61.99 -67.56 L 61.99 -303.58 L 76.38 -303.58" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 76.38 -303.58)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M 61.99 -67.56 L 61.99 -351.25
    L 76.38 -351.25" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 76.38 -351.25)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path
    d="M 61.99 -67.56 L 61.99 -390.62 L 76.38 -390.62" style="fill:none" ><g
    transform="matrix(1.0 0.0 0.0 1.0 76.38 -390.62)" ><path
    d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28
    4.98 0" style="stroke:none"></path></g><path d="M 61.99 -67.56 L 61.99 -438.29
    L 76.38 -438.29" style="fill:none" ><g transform="matrix(1.0
    0.0 0.0 1.0 76.38 -438.29)" ><path d="M 4.98 0 C 3.51 0.28
    1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></g>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Summary of the defense mechanisms sub-divided in the 3 categories,
    namely: Adversarial Training, Certified Defenses, and Regularization Approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Defense Mechanisms based on Robust Optimization Against Adversarial Attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From the examples presented so far, we see that both DNN’s and CNN’s are very
    unstable locally and susceptible to misclassify examples with perturbations that
    are barely perceivable by the human eye. Several works have reported structured
    algorithms and formulations to improve the robustness of ML models employing robust
    optimization. The goal of this section is to visit the most commonly identified
    techniques to achieve robustness (through the eyes of the optimization), namely
    Adversarial Training, Bayesian Approach, Certified Defenses, and Regularization
    Approaches, which are summarized in [Figure 6](#S3.F6 "Figure 6 ‣ 3.3.1 Other
    Attacks ‣ 3.3 Physical World Attack ‣ 3 Methods for Generating Adversarial Attacks
    ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'We summarize in [Table III](#S4.T3 "TABLE III ‣ 4.1.12 Input Transformation
    Methods ‣ 4.1 Defending through Adversarial (re)Training ‣ 4 Defense Mechanisms
    based on Robust Optimization Against Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey") the results presented in the
    surveyed papers. We’ve compiled the information available in each of those papers
    concerning the error rate under the tested conditions. Each of the papers has
    different evaluation criteria and conditions. The dataset is highly influential
    in the accuracy of the model. To that end, we’ve been careful to better express
    the conditions in which the models were evaluated. Papers in which the results
    were not clear or have used a very specific metric are not listed in these tables
    to keep consistency among the results.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Defending through Adversarial (re)Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the first introduction of adversarial examples ([[43](#bib.bib43)] and
    [[88](#bib.bib88)]), defense mechanisms to train robust neural networks were built
    based on the inclusion of adversarial examples to the training set. Models trained
    using adversarial training with projected gradient descent (PGD) were shown to
    be robust against the strongest known attacks. This is in contrast to other defense
    mechanisms that have been broken by new attack techniques. In this section, we
    explore robustness mechanisms that explicitly or implicitly address the robustness
    in deep learning through either adding adversarial examples in the dataset or
    incorporate them in the objective function for the optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Harnessing Adversarial examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In his work, [[56](#bib.bib56)] Goodfellow et. al. suggests the use of adversarial
    examples in the training process to improve the robustness of machine learning
    models. It is a very simple idea, which worked for the proposed configuration.
    The algorithm, using FGSM in an untargeted setting, would generate a set of adversarial
    examples $x^{\prime}$, which were fed to the learning algorithm with the true
    label, $(x^{\prime},y)$. Important to notice the limitation of such a framework,
    it is robust against FGSM attacks, but susceptible to other attacks, such as iterative
    methods. Such weakness was pointed in a later work by [[107](#bib.bib107)], which
    also shown single-step attacks could fool such a defense.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[98](#bib.bib98)], it is studied an adaptation of FGSM in adversarial training.
    The initially proposed FGSM training was shown to create a massive over-fitting
    in the model having it not robust to iterative attack methods such as PGD. In
    this new publication, Wong et. al., proposes small modifications in the initialization
    of the FGSM algorithm to accommodate randomness as a way to prevent over-fitting
    in the training process. Instead of having a fixed initialization, the perturbations
    are generated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $\delta=Uniform(-\epsilon,\epsilon)$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $\delta=\delta+\alpha sign(\nabla_{\delta}\mathcal{L}(f_{\theta}(x_{i}+\delta),y_{i}))$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $\delta=\max(\min(\delta,\epsilon),-\epsilon)$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The addition of the sampling from a uniform distribution to the initial perturbation
    allowed the algorithm to better estimate the inner maximization, improving the
    robustness of adversarially trained models with FGSM, maintaining the speed of
    the adversarial example generation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Towards Deep Learning Models Resistant to Adversarial Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In their publication, Madry et. al. [[54](#bib.bib54)], among an extensive
    discussion of robustness in the machine learning models, propose to incorporate
    iterative methods to approximate the inner maximization problem shown in:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta}(\frac{1}{\mathcal{D}}\sum_{(x,y)\in~{}\mathcal{D}}\max_{\delta\in\Delta(x)}\mathcal{L}(f(x+\delta),y))$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where $\delta$ is the perturbation, $f(.)$ is the model.
  prefs: []
  type: TYPE_NORMAL
- en: In this publication, the authors have approximated the inner maximization problem
    with the PGD attack method, but it is important to highlight in this formulation,
    that the model will be as robust as the attack it was trained on. If new and more
    efficient adversarial examples are presented to the model at inference time, nothing
    can be stated regarding the robustness of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Ensemble Adversarial Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In their research [[107](#bib.bib107)], the authors show that when the model
    is trained directly on single/iterative methods, the model trains on examples
    crafted to maximize a linear approximation of the loss. Moreover, such a model
    training method converges to a degenerate global minimum. These artifacts near
    the inputs $x$ obfuscate the approximation of the loss. According to their findings,
    the model learns weak perturbations rather than generating robustness against
    strong ones.
  prefs: []
  type: TYPE_NORMAL
- en: As a countermeasure, the paper implements a framework in which the model is
    trained with adversarial examples from similar classifiers. In their method, the
    generation of adversarial examples is dissociated from the network being optimized.
    Under such circumstances, their proposed framework approximates to the work in
    which black-box attacks are generated with the use of auxiliary models, and consequently
    makes their approach more resilient to it. In the algorithm, to train classifier
    $F_{0}$, they train other sets of classifiers, $F_{1},F_{2},...,F_{n}$, generate
    adversarial examples for these classifiers, and use the adversarially generated
    examples to train $F_{0}$. Their defense model even for the ImageNet dataset,
    has shown robustness against black-box attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a different work, using a similar concept, [[106](#bib.bib106)] proposes
    the augmentation of the model robustness with the use of random noise layers to
    prevent the strong gradient attacks. It is a gradient masking attack, but the
    authors position the algorithm as an improvement to ensemble adversarial defense,
    by stating that the defense is equivalent to ensembling an infinite number of
    noisy models. Such ensembling is claimed by the authors to be equivalent to training
    the original model with a Lipschitz regularizing term. They’ve achieved significant
    results against Carlini and Wagner’s strong attacks. Algorithm [3](#alg3 "Algorithm
    3 ‣ 4.1.3 Ensemble Adversarial Training ‣ 4.1 Defending through Adversarial (re)Training
    ‣ 4 Defense Mechanisms based on Robust Optimization Against Adversarial Attacks
    ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey")
    shows the steps to implement the proposed algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '1:Training2:for iter = 1,2,… do3:     Randomly sample $(x_{i},y_{i})$ in dataset4:     Randomly
    generate $\epsilon\sim\mathcal{N}(0,\sigma^{2})$ for each noisy layer5:     Compute
    the noise gradient, $g=\nabla_{\theta}\mathcal{L}(f_{\epsilon}(\theta,x_{i}),y_{i})$6:     Update
    weights: $\theta^{\prime}\leftarrow\theta-g$7:Testing Given a test image $x$,
    initialize $p=(0,...,0)$8:for $j={1,2,...,Ensembles}$ do9:     Generate $\epsilon$10:     Forward
    propagation and generate probability output, $p^{j}$11:     Update, $p\leftarrow
    p+p^{j}$12:Return class with with maximum probability'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 Random Self-Ensemble
  prefs: []
  type: TYPE_NORMAL
- en: More recently in [[105](#bib.bib105)], a mixed-precision ensemble was proposed.
    Ensemble of Mixed Precision Deep Networks for Increased Robustness (EMPIR) is
    based on the observation that quantized neural networks often show higher robustness
    to adversarial attacks than full precision networks. Such models sacrifice accuracy.
    EMPIR combines the accuracy of full models with the robustness of quantized models
    by composing them in an ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Principled Adversarial Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [[104](#bib.bib104)], Sinha et. al. have presented a method to solve the
    outer maximization with a Lagrange relaxation approach. By doing so, they presented
    both, an adversarial training method based on the min-max formulation and a method
    to prove the robustness of the model. In addition to the adversarial training,
    a penalty is added on the loss term to help regularize the learning. It is used
    as a Lagrangian formulation to generate this penalty. It perturbs the underlying
    data distribution within a Wasserstein ball. The model’s efficiency is restricted
    to smooth losses, but under such constraint, it can achieve a moderate level of
    robustness. The computational or statistical cost, when compared to empirical
    risk minimization is small.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.5 Network Robustness with Adversary Critic
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Adversarial Training formulation is an optimization problem that naturally
    involves minimization and maximization. In [[103](#bib.bib103)] and [[116](#bib.bib116)]
    the GAN framework is proposed to generate the noisy perturbations, which will
    lead to the adversarial example. On the other-hand the discriminator of the network
    act as a critic which will discern if the presented input $x$ is adversarial or
    not. In the work, it is highlighted that the generated adversarial networks are
    also robust to black-box attacks, showing similar or better performance than state-of-the-art
    defense mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.6 Adversarial Logit Pairing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In [[54](#bib.bib54)] Madry et. al. suggested the use of the [Equation 11](#S4.E11
    "11 ‣ 4.1.2 Towards Deep Learning Models Resistant to Adversarial Examples ‣ 4.1
    Defending through Adversarial (re)Training ‣ 4 Defense Mechanisms based on Robust
    Optimization Against Adversarial Attacks ‣ Opportunities and Challenges in Deep
    Learning Adversarial Robustness: A Survey") to adversarial train the model, and
    achieve robustness in their model. Kannan et al.[[102](#bib.bib102)] implements
    a mixed version of this defense. Instead of training the robust model only on
    adversary perturbed images, they incorporate a mix of clean $(x,y)$ and perturbed
    $(x^{\prime},y)$ batches, which they call mixed mini-batch PGD (M-PGD).'
  prefs: []
  type: TYPE_NORMAL
- en: In their work, they go beyond to analyze the fact that most of the adversarial
    training framework, train the models with information that $x^{\prime}$ should
    belong to the class $t$, but the model is not given any information indicating
    that $x^{\prime}$ is more similar to the actual sample $x$ than any other belonging
    to the same class. To that extent, they propose another algorithm called adversarial
    logit pairing.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a model $f(.)$ trained on a mini-batch $\Gamma$ of clean examples $\{x_{1},x_{2},...,x_{m}\}$
    and corresponding adversarial examples $\{x^{\prime}_{1},x^{\prime}_{2},...,x^{\prime}_{m}\}$,
    with $f(x)$ mapping the input to a prediction. With $\mathcal{L}(\Gamma,\theta)$
    being the cost function used to perform the adversarial training, the adversarial
    logit pairing consists of minimizing the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}(\Gamma,\theta)+\lambda\frac{1}{m}\sum_{i=1}^{m}\mathcal{L}(f(x_{i}),f(x^{\prime}_{i}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 4.1.7 ME-Net
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ME-Net [[101](#bib.bib101)] introduces the concept of utilizing matrix estimation
    as a way to augment the adversarial sample size and eliminate the perturbations
    from adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: The training procedure to ME-Net is described as follows, the algorithm creates
    a mask in which each pixel is preserved with probability $p$, and set to zero
    with probability $1-p$. For each input image $x$, $n$ masks are applied, with
    different pixel drop probability. The generated masked images, $X$, are then processed
    through a Matrix Estimation algorithm, which would obtain the reconstructed images
    $\hat{X}$. The DNN model is trained on the reconstructed images, which can be
    further processed with the use of more adversarial training techniques. For inference,
    to each input $x$, a mask is randomly sampled from the pool of masks obtained
    in the training time, applied to $x$, and then reconstruct to generate $\hat{x}$.
    The process of masking and reconstructing the images is claimed by the authors
    to reduce the effects of adversarial perturbations in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.8 Robust Dynamic Inference Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [[99](#bib.bib99)], an input-adaptive dynamic inference model to the adversarial
    defense is proposed. In this method each input, regardless of clean or adversarial
    samples, adaptively chooses which output layer to take for its prediction. Therefore,
    a large portion of input inferences can be terminated early when the samples can
    already be inferred with high confidence. The benefit of the use of such models
    comes from the fact that the multiple sources of losses provide much larger flexibility
    to compose attacks (and defenses), compared to the typical framework. In this
    work, a methodology to attack and defend in such models is proposed.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.9 Defending against Occlusion Attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [[97](#bib.bib97)], the authors investigate defenses against physically realizable
    attacks, more specifically they investigate defenses to attacks in which part
    of the object is occluded by a physical patch. It’s been demonstrated that adversarial
    training with either PGD or Randomized Smoothing did not improve the robustness
    of the models significantly. In their work, they propose, implement, and use a
    Rectangular Occlusion Attack(ROA). The ROA enabled the emulation of physical attacks
    in the virtual world and the consequent training of adversarial resistant models.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.10 Robust Local Features for Improving the generalization of Adversarial
    Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In their research,Song et al. [[96](#bib.bib96)] investigates the fact that
    when models are trained with no adversarial techniques they gather better information
    about local features, which improves the model’s generalization ability. Opposed,
    DNN models which were adversarially trained tend to have a bias in favor of a
    global understanding of the features. In their work, they propose a method to
    train adversarial robust models, which are biased towards local features. In their
    work they define the Random Block Shuffle, to randomize the features of input
    inside the image. Such an approach prevents the adversarial learning method from
    learning only global features. The model learns from a combination of shuffled
    and unshuffled images.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\displaystyle\begin{split}\mathcal{L}_{RLFAT_{P}}(F;x,y)=&amp;\mathcal{L}_{PGDAT}^{RLFL}(F;x,y)+\\
    &amp;\eta\mathcal{L}_{PGDAT}^{RLFT}(F;x,y)\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mathcal{L}_{RLFAT_{T}}(F;x,y)=&amp;\mathcal{L}_{TRADES}^{RLFL}(F;x,y)+\\
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;\eta\mathcal{L}_{TRADES}^{RLFT}(F;x,y)\end{split}" display="inline"><semantics
    ><mtable columnspacing="0pt" rowspacing="0pt" 
    ><mtr  ><mtd
     columnalign="right"  ><mrow
     ><mrow 
    ><msub  ><mi
      >ℒ</mi><mrow
     ><mi 
    >R</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >L</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >F</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >A</mi><mo lspace="0em" rspace="0em" 
    >​</mo><msub 
    ><mi  >T</mi><mi
     >P</mi></msub></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mi 
    >F</mi><mo  >;</mo><mi
     >x</mi><mo 
    >,</mo><mi  >y</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >=</mo></mrow></mtd><mtd
     columnalign="left"  ><mrow
     ><mrow 
    ><msubsup  ><mi
      >ℒ</mi><mrow
     ><mi 
    >P</mi><mo lspace="0em" rspace="0em"
     >​</mo><mi
     >G</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >D</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >A</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >T</mi></mrow><mrow
     ><mi 
    >R</mi><mo lspace="0em" rspace="0em"
     >​</mo><mi
     >L</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >F</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >L</mi></mrow></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mi 
    >F</mi><mo 
    >;</mo><mi  >x</mi><mo
     >,</mo><mi 
    >y</mi><mo stretchy="false" 
    >)</mo></mrow></mrow><mo 
    >+</mo></mrow></mtd></mtr><mtr 
    ><mtd  columnalign="left" 
    ><mrow  ><mi
     >η</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msubsup
     ><mi 
     >ℒ</mi><mrow
     ><mi 
    >P</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >G</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >D</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >A</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >T</mi></mrow><mrow 
    ><mi  >R</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >L</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >F</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >T</mi></mrow></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mi 
    >F</mi><mo  >;</mo><mi
     >x</mi><mo 
    >,</mo><mi  >y</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="right"  ><mrow
     ><mrow 
    ><msub  ><mi
      >ℒ</mi><mrow
     ><mi 
    >R</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >L</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >F</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >A</mi><mo lspace="0em" rspace="0em" 
    >​</mo><msub 
    ><mi 
    >T</mi><mi 
    >T</mi></msub></mrow></msub><mo lspace="0em"
    rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mi 
    >F</mi><mo  >;</mo><mi
     >x</mi><mo 
    >,</mo><mi  >y</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >=</mo></mrow></mtd><mtd
     columnalign="left"  ><mrow
     ><mrow 
    ><msubsup  ><mi
      >ℒ</mi><mrow
     ><mi 
    >T</mi><mo lspace="0em" rspace="0em"
     >​</mo><mi
     >R</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >A</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >D</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >E</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >S</mi></mrow><mrow
     ><mi 
    >R</mi><mo lspace="0em" rspace="0em"
     >​</mo><mi
     >L</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >F</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >L</mi></mrow></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mi 
    >F</mi><mo 
    >;</mo><mi  >x</mi><mo
     >,</mo><mi 
    >y</mi><mo stretchy="false" 
    >)</mo></mrow></mrow><mo 
    >+</mo></mrow></mtd></mtr><mtr 
    ><mtd  columnalign="left" 
    ><mrow  ><mi
     >η</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msubsup
     ><mi 
     >ℒ</mi><mrow
     ><mi 
    >T</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >R</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >A</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >D</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >E</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >S</mi></mrow><mrow 
    ><mi  >R</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >L</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >F</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >T</mi></mrow></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mi 
    >F</mi><mo  >;</mo><mi
     >x</mi><mo 
    >,</mo><mi  >y</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ℒ</ci><apply  ><ci
     >𝑅</ci><ci
     >𝐿</ci><ci
     >𝐹</ci><ci
     >𝐴</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑇</ci><ci
     >𝑃</ci></apply></apply></apply><list
     ><ci 
    >𝐹</ci><ci  >𝑥</ci><ci
     >𝑦</ci></list></apply><apply
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >ℒ</ci><apply
     ><ci 
    >𝑃</ci><ci 
    >𝐺</ci><ci 
    >𝐷</ci><ci 
    >𝐴</ci><ci 
    >𝑇</ci></apply></apply><apply 
    ><ci 
    >𝑅</ci><ci 
    >𝐿</ci><ci 
    >𝐹</ci><ci 
    >𝐿</ci></apply></apply><list 
    ><ci  >𝐹</ci><ci
     >𝑥</ci><ci
     >𝑦</ci></list></apply><apply
     ><ci 
    >𝜂</ci><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ℒ</ci><apply 
    ><ci  >𝑃</ci><ci
     >𝐺</ci><ci
     >𝐷</ci><ci
     >𝐴</ci><ci
     >𝑇</ci></apply></apply><apply
     ><ci 
    >𝑅</ci><ci 
    >𝐿</ci><ci 
    >𝐹</ci><ci 
    >𝑇</ci></apply></apply><list 
    ><ci  >𝐹</ci><ci
     >𝑥</ci><ci 
    >𝑦</ci></list><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ℒ</ci><apply 
    ><ci  >𝑅</ci><ci
     >𝐿</ci><ci
     >𝐹</ci><ci
     >𝐴</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑇</ci><ci
     >𝑇</ci></apply></apply></apply><list
     ><ci 
    >𝐹</ci><ci  >𝑥</ci><ci
     >𝑦</ci></list></apply></apply></apply><apply
     ><apply 
    ><apply  ><apply
     ><csymbol cd="ambiguous"
     >superscript</csymbol><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >ℒ</ci><apply
     ><ci 
    >𝑇</ci><ci 
    >𝑅</ci><ci 
    >𝐴</ci><ci 
    >𝐷</ci><ci 
    >𝐸</ci><ci 
    >𝑆</ci></apply></apply><apply 
    ><ci 
    >𝑅</ci><ci 
    >𝐿</ci><ci 
    >𝐹</ci><ci 
    >𝐿</ci></apply></apply><list 
    ><ci  >𝐹</ci><ci
     >𝑥</ci><ci
     >𝑦</ci></list></apply><apply
     ><ci 
    >𝜂</ci><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ℒ</ci><apply 
    ><ci  >𝑇</ci><ci
     >𝑅</ci><ci
     >𝐴</ci><ci
     >𝐷</ci><ci
     >𝐸</ci><ci
     >𝑆</ci></apply></apply><apply
     ><ci 
    >𝑅</ci><ci 
    >𝐿</ci><ci 
    >𝐹</ci><ci 
    >𝑇</ci></apply></apply><list 
    ><ci  >𝐹</ci><ci
     >𝑥</ci><ci 
    >𝑦</ci></list></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle\begin{split}\mathcal{L}_{RLFAT_{P}}(F;x,y)=&\mathcal{L}_{PGDAT}^{RLFL}(F;x,y)+\\
    &\eta\mathcal{L}_{PGDAT}^{RLFT}(F;x,y)\\ \mathcal{L}_{RLFAT_{T}}(F;x,y)=&\mathcal{L}_{TRADES}^{RLFL}(F;x,y)+\\
    &\eta\mathcal{L}_{TRADES}^{RLFT}(F;x,y)\end{split}</annotation></semantics></math>
    |  | (12) |'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the loss defined in [Equation 12](#S4.E12 "12 ‣ 4.1.10 Robust Local Features
    for Improving the generalization of Adversarial Training ‣ 4.1 Defending through
    Adversarial (re)Training ‣ 4 Defense Mechanisms based on Robust Optimization Against
    Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey"), the factor $\eta$ balances the contribution between the
    local feature-oriented loss and the global oriented.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.11 Misclassification Aware Adversarial Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [[95](#bib.bib95)], the authors propose the analysis of the misclassification
    examples, intending to improve the accuracy of the model against perturbed inputs.
    To perform such a study they’ve trained a classifier with 10-step PGD, obtaining
    87% training accuracy, they extracted the 13% misclassified examples and sampled
    13% correctly classified examples from the training dataset. The examples originally
    misclassified by the model, are the ones that impact the most the final robustness.
    Compared to standard adversarial training the final robustness drops drastically
    if misclassified examples are not perturbed during adversarial training. In contrast,
    the same operation on sampled correctly classified examples only slightly affects
    the final robustness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the observations a regularization term is proposed to incorporate
    an explicit differentiation of misclassified examples. Initially they propose
    a Boosted Cross Entropy loss, defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{BCE}(p(\hat{x}_{i}^{\prime},\theta),y_{i})=$ | $\displaystyle-log(p_{y_{i}}(\hat{x}_{i}^{\prime},\theta))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-log(1-\max_{k\neq y_{i}}p_{t}(\hat{x}_{i}^{\prime},\theta))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'in which $p_{t}(\hat{x}_{i}^{\prime},\theta)$ is the softmax on logits of $x_{i}$
    belonging to class $t$. With that the objective function is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}^{\text{MART}}(\theta)$ | $\displaystyle=\frac{1}{n}\sum^{n}_{i=1}\mathcal{L}(x_{i},y_{i},\theta)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathcal{L}(x_{i},y_{i},\theta)$ | $\displaystyle:=BCE(p(\hat{x}_{i}^{\prime},\theta),y_{i}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\lambda\text{KL}(p(x_{i},\theta)&#124;&#124;p(\hat{x}_{i}^{\prime},\theta),y_{i}))(1-p_{y_{i}}(x_{i},\theta))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 4.1.12 Input Transformation Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Input transformation methods propose to train the network in transformed images,
    such as bit-depth reduction, JPEG compression, total variance minimization, and
    image quilting or an ensemble of these methods to improve the robustness of the
    model.In [[128](#bib.bib128)], the use of JPEG compression was proposed as a countermeasure
    of the pixel displacement generated by the adversarial attacks. In [[129](#bib.bib129)],
    a combination of total variation minimization and image quilting is used to defend
    against strong attacks. Even-though these transformations are nonlinear, a neural
    network was used to approximate the transformations, making them differentiable,
    and consequently easier to obtain the gradient. Different in [[130](#bib.bib130)],
    an ensemble of weak transformation defenses was proposed to improve the robustness
    of the models, among the transformations included in the defense are: color precision
    reduction, JPEG noise, Swirl, Noise Injection, FFT perturbation, Zoom Group, Color
    Space Group, Contrast Group, Grey Scale Group, and Denoising Group.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Results of defenses based on Adversarial (re)Training and Regularization'
  prefs: []
  type: TYPE_NORMAL
- en: '| ADVERSARIAL (RE)TRAINING |'
  prefs: []
  type: TYPE_TB
- en: '| Publication | Architecture | Dataset | Norm | Adversarial | $\epsilon$ |
    Error Rate |'
  prefs: []
  type: TYPE_TB
- en: '| Goodfellow et al. 2015 [[56](#bib.bib56)] | DNN | MNIST | $l_{2}$ | L-BFGS
    | 0.25 | 17.9% |'
  prefs: []
  type: TYPE_TB
- en: '| Tramer et al. 2018 [[107](#bib.bib107)] | Inception ResNet v2 | ImageNet
    | $l_{\infty}$ | Step-LL | 16/256 | 7.9% |'
  prefs: []
  type: TYPE_TB
- en: '| Madry et al. 2018 [[54](#bib.bib54)] | ResNet | CIFAR-10 | $l_{2}$ | PGD
    | 8 | 54.2% |'
  prefs: []
  type: TYPE_TB
- en: '| Liu el al. 2018 [[106](#bib.bib106)] | VGG16 | CIFAR-10 | $l_{\infty}$ |
    C&W | 8/256 | 10% |'
  prefs: []
  type: TYPE_TB
- en: '| Sen et al. 2020 [[105](#bib.bib105)] | AlexNet | ImageNet | $l_{\infty}$
    | C&W | 0.3 | 70.64% |'
  prefs: []
  type: TYPE_TB
- en: '| Kannan et al. 2018 [[102](#bib.bib102)] | ResNet-101 | ImageNet | $l_{2}$
    | PGD | 12/255 | 55.60% |'
  prefs: []
  type: TYPE_TB
- en: '| Hu et al. 2020 [[99](#bib.bib99)] | ResNet38 | CIFAR-10 | $l_{2}$ | PGD |
    8/255 | 30.29% |'
  prefs: []
  type: TYPE_TB
- en: '| Wong et al. 2020 [[98](#bib.bib98)] | PreAct ResNet18 | ImageNet | $l_{\infty}$
    | R-FGSM | 2/255 | 56.7% |'
  prefs: []
  type: TYPE_TB
- en: '| Song et al. 2019 [[96](#bib.bib96)] | ResNet w32-10 | CIFAR-100 | $l_{\infty}$
    | PGD | 0.03 | 68.01% |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. 2020 [[95](#bib.bib95)] | ResNet-18 | CIFAR-10 | $l_{\infty}$
    | PGD | 8/255 | 45.13% |'
  prefs: []
  type: TYPE_TB
- en: '| REGULARIZATION APPROACH |'
  prefs: []
  type: TYPE_TB
- en: '| Publication | Architecture | Dataset | Norm | Adversarial | $\epsilon$ |
    Error Rate |'
  prefs: []
  type: TYPE_TB
- en: '| Cisse et al. 2017 [[125](#bib.bib125)] | ResNet | CIFAR-100 | SNR($x$,$\delta$),
    $l_{\infty}$ | FGSM | 33 | 47.4% |'
  prefs: []
  type: TYPE_TB
- en: '| Yan et al. 2018 [[123](#bib.bib123)] | ResNet | ImageNet | $l_{2}$ | FGSM
    | 2.43E-3 | 50% |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. 2019 [[121](#bib.bib121)] | ResNet | CIFAR-10 | $l_{\infty}$
    | C&W | 3.1E-2 | 18.76% |'
  prefs: []
  type: TYPE_TB
- en: '| Xie et al. 2019 [[55](#bib.bib55)] | ResNet-152 | ImageNet | $l_{\infty}$
    | PGD | 16 | 57.4% |'
  prefs: []
  type: TYPE_TB
- en: '| Mao et al. 2019 [[120](#bib.bib120)] | Modified LeNet | Tiny ImageNet | $l_{\infty}$
    | C&W | 8/255 | 82.52% |'
  prefs: []
  type: TYPE_TB
- en: '| Shafahi et al. 2019 [[131](#bib.bib131)] | Wide-ResNet-32 | CIFAR-100+$\rightarrow$10+
    | $l_{2}$ | PGD | 8 | 82.3% |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Regularization Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in [subsection 3.1](#S3.SS1 "3.1 White-box Attacks ‣ 3 Methods for
    Generating Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey"), several algorithms depend on the model’s gradient
    to estimate the local optima perturbations which will fool the classifier. A stream
    of research towards adversarial robust optimization, look into applying regularization
    approaches to reduce the influence of small perturbations in the input on the
    output decisions. In this section, we review some relevant publications in the
    field in which the author’s objective is to improve the robustness of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Towards DNN Architectures Robust to Adversarial Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In their work, Gu et. al. [[127](#bib.bib127)] propose the use of Contractive
    Autoencoder. To regularize the gradient, they add a penalty to the loss in the
    back-propagation concerning the partial derivatives at each layer. By incorporating
    a layer-wise contractive penalty (in partial derivatives), they show that adversarial
    generated from such networks have significantly higher distortion. In their approach,
    the network could still be fooled by adversarial examples, but the level of noise
    necessary to fool such a network is considerably higher than standard networks
    in which there is no contractive penalty.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Robust Large Margin Deep Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The work presented in [[126](#bib.bib126)] analyzes the generalization error
    of DNN’s through their classification margin. In their work, they initially derive
    bounds to the Generalization Error (GE) (adversarial attacks as consequence) and
    express these bounds as a dependence of the model’s Jacobian Matrix (JM). In their
    work, it was shown that the depth of the architecture does not affect the existence
    of a GE bound, conditioned to fact that the spectral norm of the JM is also bounded,
    around the training inputs. With this definition a weigh and batch normalization
    regularizer is derived. The regularizer is based on the bound derived based on
    the JM.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Input Gradient Regularization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Parseval networks [[125](#bib.bib125)] is a layer-wise regularization method
    to reduce the networks variability to small disturbances in the input $x$. The
    work starts with the principle that DNN’s are a composition of functions presented
    by layers. To keep the variability of the output controlled, they propose to maintain
    the Lipschitz constant small at every hidden layer (for fully connected, convolutional,
    or residual layer). For that, they analyze the spectral norm of the weight matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[124](#bib.bib124)], Ros et. al. based on the same principle of gradient
    regularization, propose the use of such techniques to improve both the robustness
    and interpretability of Deep Neural Networks. The authors claim that raw input
    gradients are what many attacks use to generate adversarial examples. Explanation
    techniques that smooth out gradients in background pixels may be inappropriately
    hiding the fact that the model is quite sensitive to them. They hypothesize that
    by training a model to have smooth input gradients with fewer extreme values,
    it would not only make the model more interpretable but also more resistant to
    adversarial examples. Their gradient regularization is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\theta^{*}$ | $\displaystyle=\operatorname*{arg\,min}_{\theta}\sum^{N}_{n=1}\sum^{K}_{k=1}-y_{nk}log(f_{\theta}(X_{n})_{k})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\lambda\sum^{D}_{d=1}\sum^{N}_{n=1}(\frac{\partial}{\partial
    x_{d}}\sum_{k=1}^{K}-y_{nk}log(f_{\theta}(X_{n})_{k}))^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: in which $\lambda$ specifies the penalty strength. The goal of this update is
    to ensure that if any input changes slightly the KL divergence between predictions
    will not change significantly.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 DeepDefense
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Yan et al.[[123](#bib.bib123)] propose the algorithm called DeepDefense focusing
    on the improvement of the robustness of the DNN models, which is an regularization
    method based on the generated adversarial perturbation. Similar to adversarial
    (re)Training the algorithm incorporates the adversarial generation the loss function.
    It does not occur as shown in [Equation 3](#S2.E3 "3 ‣ 2.3 Defense Methods ‣ 2
    Taxonomy of Adversarial Attacks and Defenses ‣ Opportunities and Challenges in
    Deep Learning Adversarial Robustness: A Survey"), differently it is presented
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{\theta}\sum_{k}\mathcal{L}(x_{k},y_{k},\theta)+\lambda\sum_{k}R(-\frac{\left\&#124;\delta_{x_{k}}\right\&#124;_{p}}{\left\&#124;x_{k}\right\&#124;_{p}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: in which we see that at the same time that the loss is minimized a regularization
    term based on the perturbation $\delta_{x_{k}}$ is added to penalize the norm
    of the adversarial perturbations. The penalty function $R(.)$, treats the samples
    different depoending if they were correctly classified or not, it increases monotonically
    when the sample is correctly classified. With such a behavior the function gives
    preference to those parameter settings which are able to resist even to small
    $\frac{\left\|\delta_{x_{k}}\right\|_{p}}{\left\|x_{k}\right\|_{p}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.5 TRADES
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In their work, Zhang et. al. [[121](#bib.bib121)] states the intrinsic trade-off
    between robustness and accuracy. In their work, they derive a differentiable upper
    bound for the natural and boundary errors of the DNN model. To derive such bound,
    the error generated by the adversarial examples (called robust error) is decomposed
    in two parts: 1 - the natural misclassification, and 2 - the boundary errors.
    The bounds are shown to be the tightest overall probability distributions. Based
    on these bounds a defense mechanism called TRADES is proposed. In its core, the
    algorithm still minimizes the natural loss, consequently increasing the accuracy
    of the model, but at the same time, it introduces a regularization term, which
    induces a shift of the decision boundaries away from the training data points.
    The expansion of the decision boundaries can be seen in [Figure 7](#S4.F7 "Figure
    7 ‣ 4.2.5 TRADES ‣ 4.2 Regularization Techniques ‣ 4 Defense Mechanisms based
    on Robust Optimization Against Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/89dbbc63e7745176ded64225a3810540.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: In the left we see the boundaries of a model trained with standard
    DNN. In the right the boundaries of the decision are pushed further from the data
    points, showing not so sharp transitions compared to naturally trained methods.
    Image Source: [[121](#bib.bib121)]'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.6 Metric Learning for adversarial Robustness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[[120](#bib.bib120)] focuses on learning a distance metric for the latent representation
    of the input. Through an empirical analysis, the authors have observed that inputs
    under PGD adversarial attacks shift its latent representation to a false class.
    The shift in the latent representations spread in the false class and become indistinguishable
    from the original images in the class. In their paper, they’ve added a new constraint
    to the model with metric learning. Their model implements a variation of the naive
    triplet loss, called TLA (triplet loss adversarial training), which overcome the
    variance of the adversarial data over the false class. The TLA works by approximating
    the samples from the same class, independently if they are adversarial or unperturbed
    samples, and enlarge the boundary distance concerning other classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.7 Adversarially Robust Transfer Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [[131](#bib.bib131)], a study on transfer learning for robust models is performed.
    In their paper, they robustly train a Wide-ResNet 32-10 [[132](#bib.bib132)] using
    the algorithm proposed in [[54](#bib.bib54)]. In their first experiment, they
    sub-divide the model in blocks of layers and analyze the impact of changing the
    number of unfrozen blocks in the transfer learning process. It was seen that when
    only the fully connected layer and the batch normalization blocks were re-trained,
    the network had similar or improved robustness in the new domain. Opposed, when
    more blocks were re-trained the accuracy and robustness dropped drastically. The
    authors claimed that the robust model’s feature extractors act as filters that
    ignore the irrelevant parts of the images.
  prefs: []
  type: TYPE_NORMAL
- en: With the intent of improving the overall performance of classifiers transferred
    from a robust source model by improving their generalization on natural images,
    the authors proposed an end-to-end transfer learning model with a no-forgetting
    property. To do so, they only fine-tune the feature extraction parameters $\theta$.
    It consists basically of the addition of a regularization term to the loss of
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta,w}\mathcal{L}(f(x,\theta),y,w)+\lambda l_{p}(f(x,\theta),f_{0}(x,\theta^{*}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 4.3 Certified Defenses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Certified defenses try to theoretically find certificates in distances or probability
    to certify the robustness of DNN models. These methods are explored in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Exact Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The work of [[63](#bib.bib63)], was a big first step in the direction of formalizing
    methods to certify the robustness of neural networks. In their work, also, to
    provide a formulation for SMT solver for the ReLU activation function, they’ve
    proven safety in a small neural network for aircraft collision prediction. They
    were able to use their solver to prove/disprove local adversarial robustness for
    their example DNN for a few arbitrary combinations of input $x$ and disturbances
    $\delta$. In their experiments, the verification took from few seconds to a few
    hours depending on the size of the perturbation, with bigger perturbations taking
    longer to be verified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Expanding over the Reluplex idea, the work in [[119](#bib.bib119)], provide
    a more complete framework to prove formulas over a DNN. The key concept of $AI^{2}$
    is the use of abstract interpretation to approximate mathematical functions with
    an infinity set of behaviors into logical functions witch are finite and consequently
    computable. To evaluate a DNN they propose to over-approximate the model in the
    abstract domain with the use of logical formulas capable of capturing certain
    shapes. [Figure 8](#S4.F8 "Figure 8 ‣ 4.3.1 Exact Methods ‣ 4.3 Certified Defenses
    ‣ 4 Defense Mechanisms based on Robust Optimization Against Adversarial Attacks
    ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey")
    shows the exemplification of how the abstraction of the layers is used to evaluate
    properties in the DNN.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b46846ec16e06c0d0899d9c70ee6b759.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: From left to right, initially the algorithm generates an abstract
    element which encompass all perturbed images. It propagates through all the abstractions
    of the layers. The verification is successful if all images are in the same classification
    group in the end.Image Source: [[119](#bib.bib119)]'
  prefs: []
  type: TYPE_NORMAL
- en: Building on the work of [[119](#bib.bib119)], Singh et al. [[118](#bib.bib118)]
    published a work in which not only the ReLU activation function is available,
    but also Sigmoid and TanH. In their work, they implement a parallel version of
    the layer transformation which improved significantly the verification speed.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Estimating the lower bound
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In their work,Hein et al. [[117](#bib.bib117)] the search of model robustness
    through the optics of formal guarantees. They’ve proposed in their work, a proven
    lower bound which establishes the minimal necessary perturbation to change a model’s
    decision. "We provide a guarantee that the classifier decision does not change
    in a certain ball around the considered instance" [[117](#bib.bib117)]. Moreover,
    based on the proposed Cross-Lipschitz Regularazation method, they show the increase
    in the adversarial robustness of the models trained with such regularization.
    The generated bound is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left\&#124;\delta\right\&#124;_{p}\leq\max_{\epsilon>0}\min\Psi$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\Psi=\{\min_{j\neq c}\frac{f_{c}(x)-f_{j}(x)}{\max_{y\in
    B_{p}(x,\epsilon)}\left\&#124;\nabla f_{c}(y)-\nabla f_{j}(y)\right\&#124;_{q}}\}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: It is known that an exact solution for the optimization problem which leads
    to the certification of DNN’s is intractable for large networks. In [[116](#bib.bib116)],
    they propose CLEVER (Cross Lipschitz Extreme Value for nEtwork Robustness). In
    their work, the lower bound is defined as the minimum $\epsilon$ necessary to
    be added in the input to change the decision of the model in an adversary setting.
  prefs: []
  type: TYPE_NORMAL
- en: Based on extreme value theory, the CLEVER metric is attack agnostic and is capable
    of estimating the lower bound for an attack to be effective in any model. But
    it restricts itself on providing a certification, it only provides an estimate
    of the lower bound. Improving from CLEVER, [[115](#bib.bib115)] provides a certified
    lower bound for multi-layer perceptrons restricted to ReLU activation. In [[114](#bib.bib114)],
    CROWN is proposed extending the exact certification to general activations. In
    [[113](#bib.bib113)] the same research group proposed the CNN-Cert a framework
    to certify more general DNN.
  prefs: []
  type: TYPE_NORMAL
- en: In Zhang et al. [[133](#bib.bib133)], a mix between inner bound propagation
    and linear relaxation is proposed. Linear relaxation of Neural Networks is one
    of the most popular methods to provide certified defenses and uses linear programming
    to provide linear relaxation, also known as the (convex adversarial polytope).
    Even though such methods generate an implementation that is tractable and solvable,
    they are still very demanding of computational resources. IBP on the contrary
    is less complex and brings more efficiency when optimizing verifiable networks.
    Also known as interval bound propagation (IBP), are in general loose during the
    initial training phase which generates instabilities in the training and makes
    the model very sensitive to the hyper-parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model proposed by Zhang et. al. in [[133](#bib.bib133)] unifies linear-relaxation
    and IBP. They generate a model which is very efficient for low output dimensions.
    It is used the convex relaxation in the backward pass of the bound, and the IBP
    in the forward bound pass of ther network. The optimization problem solved in
    CROWN-IBP can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{\theta}\mathbb{E}_{(x,y)\in\mathcal{D}}[\lambda\mathcal{L}(x,y,\theta)+(1-\lambda)\mathcal{L}(-(\Psi)+\Phi),y,\theta]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\Psi=(1-\beta)\underline{m}_{IBP}(x,\epsilon)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\Phi=\beta\underline{m}_{CROWN-IBP}(x,\epsilon)$ |  |'
  prefs: []
  type: TYPE_TB
- en: in which $\Psi$ is the IBP bound, $\Phi$ is the CROWN-IBP bound, and $\underline{m}(x,\epsilon)$
    is the combination of both bounds.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Upper Bounding the adversarial Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the works of [[112](#bib.bib112)] and [[111](#bib.bib111)] the certification
    of robustness is searched through means of defining an upper bound for the adversarial
    loss. For an adversarial loss defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{adv}=$ | $\displaystyle\max_{x^{\prime}}\{max_{i\neq
    y}Z_{i}(x^{\prime})-Z_{y}(x^{\prime})\}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\text{subj. to }x^{\prime}\in B_{\epsilon}(x)$ |  |'
  prefs: []
  type: TYPE_TB
- en: both try to find a larger certificate $C(x,F)$ when compared to the loss of
    the perturbed example. If the certificate is smaller than 0, it is guaranteed
    that the true label will have the bigger score, and it can be stated that within
    this distance the model is safe. The works differ on the means to find the certificate.
    [[112](#bib.bib112)] transforms the problem into a linear programming problem.
    [[111](#bib.bib111)] derives the certificate using semidefinite programming. An
    upper-bound estimation based on statistical methods is also proposed in Webb et
    al. [[134](#bib.bib134)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4 Randomized Smoothing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Randomized Smoothing is a set of algorithms based on a mathematical formalist
    inspired in cryptography, differential privacy (DP). This set of algorithms explore
    the connection between DP and robustness against norm-bounded adversarial examples
    in ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'A classifier $f:R^{d}\rightarrow[0,1]^{k}$ which maps the input $x$ with probability
    $[0,1]$ to any of the $d$ classes, is said to be $\epsilon$-robust at $x$ if:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $F(x+\delta)=c(x),\forall\delta:\left\&#124;\delta\right\&#124;\leq\epsilon$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'moreover, if $f$ is $L-Lipschitz$ then $f$ is $\epsilon$-robust at $x$ with:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\epsilon=\frac{1}{2L}(P_{A}-P_{B})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where, $P_{A}=\max_{i\in[k]}f_{i}(x)$ and $P_{B}$ is the second-max. Neural
    networks are known for being non-lipschitz. Recent work, from Lecuyer et al. [[135](#bib.bib135)]
    have proposed to smoothing the classifier, as in:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{f}(x)=\mathbb{E}_{Z\sim\mathcal{N}(0,I_{d})}(f(x+\sigma Z))$ |  |'
  prefs: []
  type: TYPE_TB
- en: which is proven to be lipschitz.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cohen et al. [[136](#bib.bib136)] propose the use of a smoothed classifier
    to generate a tighter certification bound. The proposed certification radius is
    defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\epsilon=\frac{\sigma}{2}(\Phi^{-1}(\underline{P_{A}})-\Phi^{-1}(\bar{P_{B}})),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\underline{P_{A}}$ the lower bound for the top class probability, $\bar{P_{B}}$
    is the upper bound probability for all other classes, and $\Phi^{-1}(.)$ is the
    inverse of the standard Gaussian cumulative distribution function. In which $\epsilon$
    is large when the noise level $\sigma$ is high, the probability of top class $A$
    is high, and the probability of each other class is low. [Figure 9](#S4.F9 "Figure
    9 ‣ 4.3.4 Randomized Smoothing ‣ 4.3 Certified Defenses ‣ 4 Defense Mechanisms
    based on Robust Optimization Against Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey") shows the decision boundaries
    and the certification radius proposed by Cohen et al. [[136](#bib.bib136)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/943ddd7be3e898103e291d5bbae4b9dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The smoothed classifier at an input x. In the left the decision boundaries
    for a classifier $f$, for each class represented in differente colours. The doted
    lines represents the sets of the distribution $\mathcal{N}(x,\sigma I)$. In the
    right the class probabilities, and the lower bound for class $p_{A}$ and the upper
    bound for each other class $p_{B}$. Image Source: [[136](#bib.bib136)]'
  prefs: []
  type: TYPE_NORMAL
- en: In [[109](#bib.bib109)], the authors propose an attack-free and scalable method
    to train robust deep neural networks. They mostly build upon Randomized Smoothing.
    The randomized smoothing classifier is defined as $g(x)=\mathbb{E}_{\eta}f(x+\eta)$,
    in which $\eta\sim\mathcal{N}(0,\sigma^{2}\text{{I}})$. Different from adversarial
    training as proposed in the original technique, the authors propose to robustly
    optimize including certification radius as part of the defined objective.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.5 MMR-Universal
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In [[110](#bib.bib110)], a provable defense against all $l_{p}$-norms are proposed,
    for all $p\geq 1$. The case studied is the non-trivial case in which none of the
    $l_{p}$ balls are contained in the others. For an input with dimensions $d$, the
    robustness to an $l_{p}$ perturbation requires $d^{\frac{1}{p}-\frac{1}{q}}\epsilon_{q}>\epsilon_{p}>\epsilon_{q}$
    for $p<q$. In which $\epsilon_{p}$ is the radius of the ball defined by the $l_{p}$-norm.
    In their paper a minimal $l_{p}$ norm of the complement of the union of $l_{1}$,
    $l_{\infty}$ norm, and its convex hull is derived as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{x\in\mathbb{R}^{d}\text{\textbackslash}C}\left\&#124;x\right\&#124;_{p}=\frac{\epsilon_{1}}{(\frac{\epsilon_{1}}{\epsilon_{\infty}}-\alpha+\alpha^{q})^{\frac{1}{q}}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'for $C$ the convex hull formed by the union of the $l_{1}$ and $l_{\infty}$
    norm balls, $\alpha=\frac{\epsilon_{1}}{\epsilon_{\infty}}-\left\lfloor\frac{\epsilon_{1}}{\epsilon_{\infty}}\right\rfloor$,
    $\frac{1}{p}+\frac{1}{q}=1$, $d\geq 2$ and $\epsilon_{1}\in(\epsilon_{\infty},d\epsilon_{\infty})$.
    Based on this derivation, a lower bound for the robustness at point $x$ is defined
    and a regularizer term is expressed in term of the distances and the lower bound
    MMR-Unirversal. The regularizer is then incorporated in the loss function to improve
    the robustness of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}(x_{i},y_{i})=$ | $\displaystyle\frac{1}{T}\sum_{i=1}^{T}\mathcal{L}_{c-e}(f(x_{i}),y_{i})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+MMR\text{-}Universal(x_{i})$ |  |'
  prefs: []
  type: TYPE_TB
- en: During the optimization, the regularizer aims at pushing both the polytope boundaries
    and the decision hyperplanes farther than $l_{\infty}$ and $l_{1}$ distances from
    the training point $x$, to achieve robustness close or better than $l_{\infty}$
    and $l_{1}$ respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Challenges and Future Opportunities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve discussed and presented results in three methods for generating robust
    machine learning algorithms: adversarial (re)training, regularization, and certified
    defenses. The search for an optimal method to strengthen DL algorithms against
    adversaries has a solid structure but still requires a significant effort to achieve
    the major objective. While a great number of new algorithms have been published
    every year, both in the realm of attacks and defenses, no algorithm based on adversarial
    (re)Training or attack generation can claim to be the final and optimal method.
    At the speed new defenses arise, attackers exploit the gradient or other nuances
    from these defensive algorithms to generate their effective low norm perturbation.
    The reason for this arms race, is the fact that the defenses are not absolute,
    or that while trying to solve maximization in [Equation 3](#S2.E3 "3 ‣ 2.3 Defense
    Methods ‣ 2 Taxonomy of Adversarial Attacks and Defenses ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey"), only an empirical approximation
    is used, and no global optimality is achieved. Moreover as the algorithms fail
    to account for all possible scenarios, there will always be an example in the
    neighborhood of $x$, such that $\left\|x-x^{\prime}\right\|_{p}\leq\delta$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other than optimizing [Equation 3](#S2.E3 "3 ‣ 2.3 Defense Methods ‣ 2 Taxonomy
    of Adversarial Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey"), with an empirical solution, certified defense
    mechanisms present a formal alternative to achieve robust deep learning models,
    but this certified solution comes with the cost of efficiency, and high computational
    cost. It derives from [Equation 5](#S2.E5 "5 ‣ 2.3.2 Certified Defenses ‣ 2.3
    Defense Methods ‣ 2 Taxonomy of Adversarial Attacks and Defenses ‣ Opportunities
    and Challenges in Deep Learning Adversarial Robustness: A Survey"). The non-linear
    equality constraint for each layer of the neural network is unsolvable with standard
    techniques, and several methods have been proposed to achieve the optimal solution
    namely: linear relaxations, convex relaxations, interval propagation, abstract
    interpretation, mixed integer linear programming, or combination of these methods.
    We see great research opportunities and challenges in further improving such algorithms.
    They are in the early stages of development, in which the abstractions and formal
    approximations for the non-linearity constraints shown in [Equation 5](#S2.E5
    "5 ‣ 2.3.2 Certified Defenses ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial
    Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey") are not optimized for parallel computation as their numerical
    counterparts are. Such restriction makes it almost impractical to have such mechanisms
    to be applied in larger datasets. More in the topic, approximations of the optimization
    constraints either by a lower bound or upper bound has shown to speed-up the training
    process of such algorithms, but at the cost of having over-estimated bounds for
    the maximum allowed perturbation. These algorithms have not yet demonstrated high
    accuracy in large datasets corrupted by adversaries, or smaller datasets with
    high level of corruption. Part of the issue comes from the imposed convex relaxations.
    As they are not tight enough, it requires the verification algorithms to explore
    a bigger region, than actually necessary to verify the existence of adversarial
    examples in the neighborhood of the input.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover as seen in [[67](#bib.bib67)], even certified defenses can be broken
    when a big enough disturbance is applied to the model. It is arguable that even
    with the rigorous mathematical formulation of the defenses and certifications
    the constraint imposed by $l_{p}$ norm is weak. Most of the models can not achieve
    certifications beyond $\epsilon=0.3$ disturbance in $l_{2}$ norm, while disturbances
    $\epsilon=4$ added to the target input are barely noticeable by human eyes, and
    $\epsilon=100$, when applied to the original image are still easily classified
    by humans as belonging to the same class. As discussed by many authors, the perception
    of multi-dimensional space by human eyes goes beyond what the $l_{p}$ norm is
    capable of capturing and synthesizing. It is yet to be proposed more comprehensive
    metrics and algorithms capable of capturing the correlation between pixels of
    an image or input data which can better translate to optimization algorithms how
    humans distinguish features of an input image. Such a metric would allow the optimization
    algorithms to have better intuition on the subtle variations introduced by adversaries
    in the input data.
  prefs: []
  type: TYPE_NORMAL
- en: As we seek to apply machine learning algorithms in safe-critical applications,
    a model that works most of the time is not enough to guarantee safety of such
    implementations. It is imperative to know the operational restrictions of the
    algorithm, and the level of corruption they can safely handle. For such, formally
    certifying the algorithms is crucial, but increasing the neighborhood around the
    input that the certification can be guaranteed is fundamental for the practical
    application of current available techiniques.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training safe and robust DNN algorithms is essential for their usage in safe-critical
    applications. This paper studies strategies to implement adversary robustly trained
    algorithms towards guaranteeing safety in machine learning models. We initially
    provide a taxonomy to classify adversarial attacks and defenses. Moreover, we
    provide a mathematical formulation for the Robust Optimization problem and divide
    it into a set of sub-categories which are: Adversarial (re)Training, Regularization
    Approach, and Certified Defenses. With the objective of elucidating methods to
    approximate the maximization problem, we present the most recent and important
    results in adversarial example generation. Furthermore, we describe several defense
    mechanisms that incorporate adversarial (re)Training as their main defense against
    perturbations or add regularization terms that change the behavior of the gradient,
    making it harder for attackers to achieve their objective. In addition, we surveyed
    methods which formally derive certificates of robustness by exactly solving the
    optimization problem or by approximations using upper or lower bounds.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] H. Greenspan, B. Van Ginneken, and R. M. Summers, “Guest editorial deep
    learning in medical imaging: Overview and future promise of an exciting new technique,”
    *IEEE Transactions on Medical Imaging*, vol. 35, no. 5, pp. 1153–1159, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] C. A. Stewart, T. M. Cockerill, I. Foster, D. Hancock, N. Merchant, E. Skidmore,
    D. Stanzione, J. Taylor, S. Tuecke, G. Turner *et al.*, “Jetstream: a self-provisioned,
    scalable science and engineering cloud environment,” in *Proceedings of the 2015
    XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure*,
    2015, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. Towns, T. Cockerill, M. Dahan, I. Foster, K. Gaither, A. Grimshaw, V. Hazlewood,
    S. Lathrop, D. Lifka, G. D. Peterson *et al.*, “Xsede: accelerating scientific
    discovery,” *Computing in science & engineering*, vol. 16, no. 5, pp. 62–74, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. Das, W.-M. Lin, and P. Rad, “A distributed secure machine-learning cloud
    architecture for semantic analysis,” in *Applied Cloud Deep Semantic Recognition*.   Auerbach
    Publications, 2018, pp. 145–174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Das, P. Rad, K.-K. R. Choo, B. Nouhi, J. Lish, and J. Martel, “Distributed
    machine learning cloud teleophthalmology iot for predicting amd disease progression,”
    *Future Generation Computer Systems*, vol. 93, pp. 486–498, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] B. Yang, F. Liu, C. Ren, Z. Ouyang, Z. Xie, X. Bo, and W. Shu, “Biren:
    predicting enhancers with a deep-learning-based model using the dna sequence alone,”
    *Bioinformatics*, vol. 33, no. 13, pp. 1930–1936, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] T. Gebru, J. Krause, Y. Wang, D. Chen, J. Deng, E. L. Aiden, and L. Fei-Fei,
    “Using deep learning and google street view to estimate the demographic makeup
    of neighborhoods across the united states,” *Proceedings of the National Academy
    of Sciences*, vol. 114, no. 50, pp. 13 108–13 113, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] H. Gabbard, M. Williams, F. Hayes, and C. Messenger, “Matching matched
    filtering with deep networks for gravitational-wave astronomy,” *Physical review
    letters*, vol. 120, no. 14, p. 141103, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] N. Ebadi, B. Lwowski, M. Jaloli, and P. Rad, “Implicit life event discovery
    from call transcripts using temporal input transformation network,” *IEEE Access*,
    vol. 7, pp. 172 178–172 189, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case,
    J. Casper, B. Catanzaro, Q. Cheng, G. Chen *et al.*, “Deep speech 2: End-to-end
    speech recognition in english and mandarin,” in *International conference on machine
    learning*, 2016, pp. 173–182.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis, “Deep
    learning for computer vision: A brief review,” *Computational intelligence and
    neuroscience*, vol. 2018, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
    Unified, real-time object detection,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2016, pp. 779–788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] J. Dai, Y. Li, K. He, and J. Sun, “R-fcn: Object detection via region-based
    fully convolutional networks,” in *Advances in neural information processing systems*,
    2016, pp. 379–387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] G. Varol, I. Laptev, and C. Schmid, “Long-term temporal convolutions for
    action recognition,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 40, no. 6, pp. 1510–1517, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] N. Bendre, N. Ebadi, J. J. Prevost, and P. Najafirad, “Human action performance
    using deep neuro-fuzzy recurrent attention model,” *IEEE Access*, vol. 8, pp.
    57 749–57 761, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person 2d
    pose estimation using part affinity fields,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017, pp. 7291–7299.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] A. Newell, K. Yang, and J. Deng, “Stacked hourglass networks for human
    pose estimation,” in *European conference on computer vision*.   Springer, 2016,
    pp. 483–499.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional
    encoder-decoder architecture for image segmentation,” *IEEE transactions on pattern
    analysis and machine intelligence*, vol. 39, no. 12, pp. 2481–2495, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
    Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 40, no. 4, pp. 834–848, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] D. Held, S. Thrun, and S. Savarese, “Learning to track at 100 fps with
    deep regression networks,” in *European Conference on Computer Vision*.   Springer,
    2016, pp. 749–765.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in neural information processing
    systems*, 2012, pp. 1097–1105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, and C. I. Sánchez, “A survey on deep learning
    in medical image analysis,” *Medical image analysis*, vol. 42, pp. 60–88, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat,
    G. Irving, M. Isard *et al.*, “Tensorflow: A system for large-scale machine learning,”
    in *12th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation
    ($\{$OSDI$\}$ 16)*, 2016, pp. 265–283.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    Z. Lin, N. Gimelshein, L. Antiga *et al.*, “Pytorch: An imperative style, high-performance
    deep learning library,” in *Advances in Neural Information Processing Systems*,
    2019, pp. 8024–8035.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] K. Keahey, P. Riteau, D. Stanzione, T. Cockerill, J. Mambretti, P. Rad,
    and P. Ruth, “Chameleon: a scalable production testbed for computer science research,”
    in *Contemporary High Performance Computing*.   CRC Press, 2019, pp. 123–148.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
    the inception architecture for computer vision,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2016, pp. 2818–2826.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-v4, inception-resnet
    and the impact of residual connections on learning,” in *Thirty-first AAAI conference
    on artificial intelligence*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] N. Akhtar and A. Mian, “Threat of adversarial attacks on deep learning
    in computer vision: A survey,” *IEEE Access*, vol. 6, pp. 14 410–14 430, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang *et al.*, “End to end learning for
    self-driving cars,” *arXiv preprint arXiv:1604.07316*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] S. Silva, R. Suresh, F. Tao, J. Votion, and Y. Cao, “A multi-layer k-means
    approach for multi-sensor data pattern recognition in multi-target localization,”
    *arXiv preprint arXiv:1705.10757*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] S. H. Silva, P. Rad, N. Beebe, K.-K. R. Choo, and M. Umapathy, “Cooperative
    unmanned aerial vehicles with privacy preserving deep vision for real-time object
    identification and tracking,” *Journal of Parallel and Distributed Computing*,
    vol. 131, pp. 147–160, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement learning
    for robotic manipulation with asynchronous off-policy updates,” in *2017 IEEE
    international conference on robotics and automation (ICRA)*.   IEEE, 2017, pp.
    3389–3396.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] S. H. Silva, A. Alaeddini, and P. Najafirad, “Temporal graph traversals
    using reinforcement learning with proximal policy optimization,” *IEEE Access*,
    vol. 8, pp. 63 910–63 922, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] M. Lansley, N. Polatidis, S. Kapetanakis, K. Amin, G. Samakovitis, and
    M. Petridis, “Seen the villains: Detecting social engineering attacks using case-based
    reasoning and deep learning,” in *Workshops Proceedings for the Twenty-seventh
    International Conference on Case-Based Reasoning: Case-based reasoning and deep
    learning workshop*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] D. Kwon, H. Kim, J. Kim, S. C. Suh, I. Kim, and K. J. Kim, “A survey of
    deep learning-based network anomaly detection,” *Cluster Computing*, pp. 1–13,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] G. De La Torre, P. Rad, and K.-K. R. Choo, “Implementation of deep packet
    inspection in smart grids and industrial internet of things: Challenges and opportunities,”
    *Journal of Network and Computer Applications*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] V. Kepuska and G. Bohouta, “Next-generation of virtual personal assistants
    (microsoft cortana, apple siri, amazon alexa and google home),” in *2018 IEEE
    8th Annual Computing and Communication Workshop and Conference (CCWC)*.   IEEE,
    2018, pp. 99–103.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye, G. Anderson,
    G. Corrado, W. Chai, M. Ispir *et al.*, “Wide & deep learning for recommender
    systems,” in *Proceedings of the 1st workshop on deep learning for recommender
    systems*, 2016, pp. 7–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] G. De La Torre, P. Rad, and K.-K. R. Choo, “Driverless vehicle security:
    Challenges and future research opportunities,” *Future Generation Computer Systems*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, pp.
    529–533, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] C. Szegedy, W. Zaremba, I. Sutskever, J. B. Estrach, D. Erhan, I. Goodfellow,
    and R. Fergus, “Intriguing properties of neural networks,” in *2nd International
    Conference on Learning Representations, ICLR 2014*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are easily
    fooled: High confidence predictions for unrecognizable images,” in *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2015, pp.
    427–436.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] B. Biggio, I. Corona, B. Nelson, B. I. Rubinstein, D. Maiorca, G. Fumera,
    G. Giacinto, and F. Roli, “Security evaluation of support vector machines in adversarial
    environments,” in *Support Vector Machines Applications*.   Springer, 2014, pp.
    105–153.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] A. Chakraborty, M. Alam, V. Dey, A. Chattopadhyay, and D. Mukhopadhyay,
    “Adversarial attacks and defences: A survey,” *arXiv preprint arXiv:1810.00069*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] H. Chacon, S. Silva, and P. Rad, “Deep learning poison data attack detection,”
    in *2019 IEEE 31st International Conference on Tools with Artificial Intelligence
    (ICTAI)*.   IEEE, 2019, pp. 971–978.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] A. Das and P. Rad, “Opportunities and challenges in explainable artificial
    intelligence (xai): A survey,” *arXiv preprint arXiv:2006.11371*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
    networks,” in *2017 Ieee symposium on security and privacy (sp)*.   IEEE, 2017,
    pp. 39–57.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] X. Yuan, P. He, Q. Zhu, and X. Li, “Adversarial examples: Attacks and
    defenses for deep learning,” *IEEE transactions on neural networks and learning
    systems*, vol. 30, no. 9, pp. 2805–2824, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Y. Vorobeychik and M. Kantarcioglu, *Adversarial machine learning*.   Morgan
    & Claypool Publishers, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh, “Zoo: Zeroth
    order optimization based black-box attacks to deep neural networks without training
    substitute models,” in *Proceedings of the 10th ACM Workshop on Artificial Intelligence
    and Security*, 2017, pp. 15–26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] H. Xu, Y. Ma, H.-C. Liu, D. Deb, H. Liu, J.-L. Tang, and A. K. Jain, “Adversarial
    attacks and defenses in images, graphs and text: A review,” *International Journal
    of Automation and Computing*, vol. 17, no. 2, pp. 151–178, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards deep
    learning models resistant to adversarial attacks,” in *International Conference
    on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] C. Xie, Y. Wu, L. v. d. Maaten, A. L. Yuille, and K. He, “Feature denoising
    for improving adversarial robustness,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 501–509.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
    adversarial examples,” in *3rd International Conference on Learning Representations,
    ICLR 2015 - Conference Track Proceedings*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple
    and accurate method to fool deep neural networks,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2016, pp. 2574–2582.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami,
    “The limitations of deep learning in adversarial settings,” in *2016 IEEE European
    symposium on security and privacy (EuroS&P)*.   IEEE, 2016, pp. 372–387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial machine learning
    at scale,” in *5th International Conference on Learning Representations, ICLR
    2017 - Conference Track Proceedings*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
    networks,” in *2017 ieee symposium on security and privacy (sp)*.   IEEE, 2017,
    pp. 39–57.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation as
    a defense to adversarial perturbations against deep neural networks,” in *2016
    IEEE Symposium on Security and Privacy (SP)*.   IEEE, 2016, pp. 582–597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] N. Carlini, G. Katz, C. Barrett, and D. L. Dill, “Ground-Truth Adversarial
    Examples,” *Iclr 2018*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, “Reluplex:
    An efficient smt solver for verifying deep neural networks,” in *International
    Conference on Computer Aided Verification*.   Springer, 2017, pp. 97–117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] V. Tjeng, K. Y. Xiao, and R. Tedrake, “Evaluating robustness of neural
    networks with mixed integer programming,” in *International Conference on Learning
    Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] K. Y. Xiao, V. Tjeng, N. M. M. Shafiullah, and A. Madry, “Training for
    faster adversarial robustness verification via inducing relu stability,” in *International
    Conference on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Universal
    adversarial perturbations,” in *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2017, pp. 1765–1773.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] A. Ghiasi, A. Shafahi, and T. Goldstein, “Breaking certified defenses:
    Semantic adversarial examples with spoofed robustness certificates,” in *International
    Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] P.-Y. Chen, Y. Sharma, H. Zhang, J. Yi, and C.-J. Hsieh, “Ead: elastic-net
    attacks to deep neural networks via adversarial examples,” in *Thirty-second AAAI
    conference on artificial intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] U. Jang, X. Wu, and S. Jha, “Objective metrics and gradient descent algorithms
    for adversarial examples in machine learning,” in *Proceedings of the 33rd Annual
    Computer Security Applications Conference*, 2017, pp. 262–277.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] C. Xiao, J. Y. Zhu, B. Li, W. He, M. Liu, and D. Song, “Spatially transformed
    adversarial examples,” in *6th International Conference on Learning Representations,
    ICLR 2018*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] L. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry, “Exploring
    the landscape of spatial robustness,” in *ICML*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] A. Odena, C. Olah, and J. Shlens, “Conditional image synthesis with auxiliary
    classifier gans,” in *Proceedings of the 34th International Conference on Machine
    Learning-Volume 70*.   JMLR. org, 2017, pp. 2642–2651.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Y. Song, R. Shu, N. Kushman, and S. Ermon, “Constructing unrestricted
    adversarial examples with generative models,” in *Advances in Neural Information
    Processing Systems*, 2018, pp. 8312–8323.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami,
    “Practical black-box attacks against machine learning,” in *Proceedings of the
    2017 ACM on Asia conference on computer and communications security*, 2017, pp.
    506–519.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling deep
    neural networks,” *IEEE Transactions on Evolutionary Computation*, vol. 23, no. 5,
    pp. 828–841, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] A. Ilyas, L. Engstrom, A. Athalye, and J. Lin, “Query-efficient black-box
    adversarial examples,” *arXiv preprint arXiv:1712.07113*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] J. Chen, M. Jordan, and M. Wainwright, “Hopskipjumpattack: A query-efficient
    decision-based attack,” in *2020 IEEE Symposium on Security and Privacy (SP)*,
    pp. 668–685.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] B. Ru, A. Cobb, A. Blaas, and Y. Gal, “Bayesopt adversarial attack,” in
    *International Conference on Learning Representations*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine, and S. Russell, “Adversarial
    policies: Attacking deep reinforcement learning,” in *International Conference
    on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash,
    T. Kohno, and D. Song, “Robust physical-world attacks on deep learning visual
    classification,” in *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2018, pp. 1625–1634.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] G. Goswami, N. Ratha, A. Agarwal, R. Singh, and M. Vatsa, “Unravelling
    robustness of deep learning based face recognition against adversarial attacks,”
    in *Thirty-Second AAAI Conference on Artificial Intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] A. J. Bose and P. Aarabi, “Adversarial attacks on face detectors using
    neural net based constrained optimization,” in *2018 IEEE 20th International Workshop
    on Multimedia Signal Processing (MMSP)*.   IEEE, 2018, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Z.-A. Zhu, Y.-Z. Lu, and C.-K. Chiang, “Generating adversarial examples
    by makeup attacks on face recognition,” in *2019 IEEE International Conference
    on Image Processing (ICIP)*.   IEEE, 2019, pp. 2516–2520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Y. Dong, H. Su, B. Wu, Z. Li, W. Liu, T. Zhang, and J. Zhu, “Efficient
    decision-based black-box adversarial attacks on face recognition,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2019, pp.
    7714–7722.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] O. Suciu, S. E. Coull, and J. Johns, “Exploring adversarial examples in
    malware detection,” in *2019 IEEE Security and Privacy Workshops (SPW)*.   IEEE,
    2019, pp. 8–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] A. Chernikova and A. Oprea, “Adversarial examples for deep learning cyber
    security analytics,” *arXiv preprint arXiv:1909.10480*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] M. Melis, A. Demontis, B. Biggio, G. Brown, G. Fumera, and F. Roli, “Is
    deep learning safe for robot vision? adversarial examples against the icub humanoid,”
    in *Proceedings of the IEEE International Conference on Computer Vision Workshops*,
    2017, pp. 751–759.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndić, P. Laskov, G. Giacinto,
    and F. Roli, “Evasion attacks against machine learning at test time,” in *Joint
    European conference on machine learning and knowledge discovery in databases*.   Springer,
    2013, pp. 387–402.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] C. Sitawarin, A. N. Bhagoji, A. Mosenia, M. Chiang, and P. Mittal, “Darts:
    Deceiving autonomous cars with toxic signs,” *arXiv preprint arXiv:1802.06430*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] N. Morgulis, A. Kreines, S. Mendelowitz, and Y. Weisglass, “Fooling a
    real car with adversarial traffic signs,” *arXiv preprint arXiv:1907.00374*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. Cao, C. Xiao, B. Cyr, Y. Zhou, W. Park, S. Rampazzi, Q. A. Chen, K. Fu,
    and Z. M. Mao, “Adversarial sensor attack on lidar-based perception in autonomous
    driving,” in *Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications
    Security*, 2019, pp. 2267–2281.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Y. Jia, Y. Lu, J. Shen, Q. A. Chen, H. Chen, Z. Zhong, and T. Wei, “Fooling
    detection alone is not enough: Adversarial attack against multiple object tracking,”
    in *International Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] W. Brendel, J. Rauber, and M. Bethge, “Decision-based adversarial attacks:
    Reliable attacks against black-box machine learning models,” in *International
    Conference on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] S. Sarkar, A. Bansal, U. Mahbub, and R. Chellappa, “Upset and angri: Breaking
    high performance image classifiers,” *arXiv preprint arXiv:1707.01159*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Y. Wang, D. Zou, J. Yi, J. Bailey, X. Ma, and Q. Gu, “Improving adversarial
    robustness requires revisiting misclassified examples,” in *International Conference
    on Learning Representations*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] C. Song, K. He, J. Lin, L. Wang, and J. E. Hopcroft, “Robust local features
    for improving the generalization of adversarial training,” in *International Conference
    on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] T. Wu, L. Tong, and Y. Vorobeychik, “Defending against physically realizable
    attacks on image classification,” in *International Conference on Learning Representations*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] E. Wong, L. Rice, and J. Z. Kolter, “Fast is better than free: Revisiting
    adversarial training,” in *International Conference on Learning Representations*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] T.-K. Hu, T. Chen, H. Wang, and Z. Wang, “Triple wins: Boosting accuracy,
    robustness and efficiency together by enabling input-adaptive inference,” in *International
    Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] H. Chen, H. Zhang, D. Boning, and C.-J. Hsieh, “Robust decision trees
    against adversarial examples,” in *International Conference on Machine Learning*,
    2019, pp. 1122–1131.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Y. Yang, G. Zhang, D. Katabi, and Z. Xu, “Me-net: Towards effective adversarial
    robustness with matrix estimation,” in *International Conference on Machine Learning*,
    2019, pp. 7025–7034.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] H. Kannan, A. Kurakin, and I. Goodfellow, “Adversarial logit pairing,”
    *arXiv preprint arXiv:1803.06373*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] A. Matyasko and L.-P. Chau, “Improved network robustness with adversary
    critic,” in *Advances in Neural Information Processing Systems*, 2018, pp. 10 578–10 587.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] A. Sinha, H. Namkoong, and J. Duchi, “Certifying some distributional
    robustness with principled adversarial training,” in *International Conference
    on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] S. Sen, B. Ravindran, and A. Raghunathan, “Empir: Ensembles of mixed
    precision deep networks for increased robustness against adversarial attacks,”
    in *International Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] X. Liu, M. Cheng, H. Zhang, and C.-J. Hsieh, “Towards robust neural networks
    via random self-ensemble,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*, 2018, pp. 369–385.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel,
    “Ensemble adversarial training: Attacks and defenses,” in *International Conference
    on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] B. Wang, S. Webb, and T. Rainforth, “Statistically robust neural network
    classification,” *arXiv preprint arXiv:1912.04884*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] R. Zhai, C. Dan, D. He, H. Zhang, B. Gong, P. Ravikumar, C.-J. Hsieh,
    and L. Wang, “Macer: Attack-free and scalable robust training via maximizing certified
    radius,” in *International Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] F. Croce and M. Hein, “Provable robustness against all adversarial $l\_p$-perturbations
    for $p$ $geq1$,” in *International Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] A. Raghunathan, J. Steinhardt, and P. Liang, “Certified defenses against
    adversarial examples,” in *International Conference on Learning Representations*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] E. Wong and Z. Kolter, “Provable defenses against adversarial examples
    via the convex outer adversarial polytope,” in *International Conference on Machine
    Learning*, 2018, pp. 5286–5295.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] A. Boopathy, T.-W. Weng, P.-Y. Chen, S. Liu, and L. Daniel, “Cnn-cert:
    An efficient framework for certifying robustness of convolutional neural networks,”
    in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 33, 2019,
    pp. 3240–3247.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] H. Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L. Daniel, “Efficient
    neural network robustness certification with general activation functions,” in
    *Advances in neural information processing systems*, 2018, pp. 4939–4948.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, L. Daniel, and I. Dhillon,
    “Towards fast computation of certified robustness for relu networks,” in *International
    Conference on Machine Learning (ICML)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] T.-W. Weng, H. Zhang, P.-Y. Chen, J. Yi, D. Su, Y. Gao, C.-J. Hsieh,
    and L. Daniel, “Evaluating the robustness of neural networks: An extreme value
    theory approach,” *arXiv preprint arXiv:1801.10578*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] M. Hein and M. Andriushchenko, “Formal guarantees on the robustness of
    a classifier against adversarial manipulation,” in *Advances in Neural Information
    Processing Systems*, 2017, pp. 2266–2276.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] G. Singh, T. Gehr, M. Mirman, M. Püschel, and M. Vechev, “Fast and effective
    robustness certification,” in *Advances in Neural Information Processing Systems*,
    2018, pp. 10 802–10 813.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and
    M. Vechev, “Ai2: Safety and robustness certification of neural networks with abstract
    interpretation,” in *2018 IEEE Symposium on Security and Privacy (SP)*.   IEEE,
    2018, pp. 3–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] C. Mao, Z. Zhong, J. Yang, C. Vondrick, and B. Ray, “Metric learning
    for adversarial robustness,” in *Advances in Neural Information Processing Systems*,
    2019, pp. 478–489.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. I. Jordan, “Theoretically
    principled trade-off between robustness and accuracy,” in *ICML*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] C. Tang, Y. Fan, and A. Yezzi, “An adaptive view of adversarial robustness
    from test-time smoothing defense,” *arXiv preprint arXiv:1911.11881*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Z. Yan, Y. Guo, and C. Zhang, “Deep defense: Training dnns with improved
    adversarial robustness,” in *Advances in Neural Information Processing Systems*,
    2018, pp. 419–428.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] A. S. Ross and F. Doshi-Velez, “Improving the adversarial robustness
    and interpretability of deep neural networks by regularizing their input gradients,”
    in *Thirty-second AAAI conference on artificial intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier, “Parseval
    networks: improving robustness to adversarial examples,” in *Proceedings of the
    34th International Conference on Machine Learning-Volume 70*, 2017, pp. 854–863.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] J. Sokolić, R. Giryes, G. Sapiro, and M. R. Rodrigues, “Robust large
    margin deep neural networks,” *IEEE Transactions on Signal Processing*, vol. 65,
    no. 16, pp. 4265–4280, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] S. Gu and L. Rigazio, “Towards deep neural network architectures robust
    to adversarial examples,” 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] N. Das, M. Shanbhogue, S.-T. Chen, F. Hohman, S. Li, L. Chen, M. E. Kounavis,
    and D. H. Chau, “Shield: Fast, practical defense and vaccination for deep learning
    using jpeg compression,” in *Proceedings of the 24th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, 2018, pp. 196–204.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] C. Guo, M. Rana, M. Cisse, and L. van der Maaten, “Countering adversarial
    images using input transformations,” in *International Conference on Learning
    Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] E. Raff, J. Sylvester, S. Forsyth, and M. McLean, “Barrage of random
    transforms for adversarially robust defense,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 6528–6537.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] A. Shafahi, P. Saadatpanah, C. Zhu, A. Ghiasi, C. Studer, D. Jacobs,
    and T. Goldstein, “Adversarially robust transfer learning,” in *International
    Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] S. Zagoruyko and N. Komodakis, “Paying more attention to attention: Improving
    the performance of convolutional neural networks via attention transfer,” *arXiv
    preprint arXiv:1612.03928*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] H. Zhang, H. Chen, C. Xiao, S. Gowal, R. Stanforth, B. Li, D. Boning,
    and C.-J. Hsieh, “Towards stable and efficient training of verifiably robust neural
    networks,” in *International Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] S. Webb, T. Rainforth, Y. W. Teh, and M. P. Kumar, “A statistical approach
    to assessing neural network robustness,” in *International Conference on Learning
    Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana, “Certified
    robustness to adversarial examples with differential privacy,” in *2019 IEEE Symposium
    on Security and Privacy (SP)*.   IEEE, 2019, pp. 656–672.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] J. Cohen, E. Rosenfeld, and Z. Kolter, “Certified adversarial robustness
    via randomized smoothing,” in *International Conference on Machine Learning*,
    2019, pp. 1310–1320.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/a40b46eca09dc0cf6d728c148ac6273d.png) | Samuel
    Henrique Silva is currently a Ph.D. student and research fellow at the Open Cloud
    Institute of University of Texas at San Antonio (UTSA), San Antonio, TX, USA.
    Samuel received the Bachelor of Science (B.Sc.) degree in Control and Automation
    Engineering from State University of Campinas, Campinas, Brazil, in 2012 and the
    M.S. degree in Electrical Engineering from the University of Notre Dame, Notre
    Dame, IN, USA in 2016\. He is a member of the IEEE, Eta Kappa Nu honor society.
    Samuel’s research interests are in the areas of artificial intelligence, robustness
    in deep learning models, autonomous decision making, multi-agent systems and adversarial
    environments. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/25db4e7410eba7c4686e3373ceecbcfb.png) | Paul Rad
    is a co-founder and Associate Director of the Open Cloud Institute (OCI), and
    an Associate Professor with the Information Systems and Cyber Security Department
    at the University of Texas at San Antonio. He received his first B.S. degree from
    Sharif University of Technology in Computer Engineering in 1994, his 1st master
    in artificial intelligence from the Tehran Polytechnic, his 2nd master in computer
    science from the University of Texas at San Antonio (Magna Cum Laude) in 1999,
    and his Ph.D. in electrical and computer engineering from the University of Texas
    at San Antonio. He was a recipient of the Most Outstanding Graduate Student in
    the College of Engineering, 2016, earned the Rackspace Innovation Mentor Program
    Award for establishing Rackspace patent community board structure and mentoring
    employees (2012), earned the Dell Corporation Company Excellence (ACE) Award for
    exceptional performance and innovative product research and development contributions
    (2007), and earned the Dell Inventor Milestone Award, Top 3 Dell Inventor of the
    year (2005). He holds 15 U.S. patents on cyber infrastructure, cloud computing,
    and big data analytics with over 300 product citations by top fortune 500 leading
    technology companies such as Amazon, Microsoft, IBM, Cisco, Amazon Technologies,
    HP, and VMware. He has advised over 200 companies on cloud computing and data
    analytics with over 50 keynote presentations. High performance cloud group chair
    at the Cloud Advisory Council (CAC), OpenStack Foundation Member (the #1 open
    source cloud software), San Antonio Tech Bloc Founding Member, and Children’s
    Hospital of San Antonio Foundation board member. |'
  prefs: []
  type: TYPE_TB
