- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:00:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:00:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2007.00753] Opportunities and Challenges in Deep Learning Adversarial Robustness:
    A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2007.00753] 深度学习对抗鲁棒性的机遇与挑战：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2007.00753](https://ar5iv.labs.arxiv.org/html/2007.00753)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2007.00753](https://ar5iv.labs.arxiv.org/html/2007.00753)
- en: 'Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习对抗鲁棒性的机遇与挑战：综述
- en: Samuel Henrique Silva,  and Peyman Najafirad S.H. Silva and P. Najafirad are
    members of the Secure AI & Autonomy Laboratory, with the Department of Electrical
    and Computer Engineering, University of Texas at San Antonio, San Antonio, TX,
    78249.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Samuel Henrique Silva 和 Peyman Najafirad。S.H. Silva 和 P. Najafirad 是德克萨斯大学圣安东尼奥分校电气与计算机工程系的安全AI与自主实验室的成员，地址：德克萨斯州圣安东尼奥市78249。
- en: P. Najafirad (corresponding author) is also with Department of Information Systems
    and Cyber Security, University of Texas at San Antonio, San Antonio, TX, 78249.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: P. Najafirad（通讯作者）还在德克萨斯大学圣安东尼奥分校信息系统与网络安全系工作，地址：德克萨斯州圣安东尼奥市78249。
- en: 'E-mail: peyman.najafirad@utsa.eduThe authors gratefully acknowledge the use
    of the services of Jetstream cloud, funded by National Science Foundation, United
    States award 1445604.This work has been submitted to the IEEE for possible publication.
    Copyright may be transferred without notice, after which this version may no longer
    be accessible.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：peyman.najafirad@utsa.edu 作者感谢使用由美国国家科学基金会资助的Jetstream云服务（奖号1445604）。此工作已提交给IEEE以备可能发表。版权可能会在未通知的情况下转移，转移后此版本可能不再可用。
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'As we seek to deploy machine learning models beyond virtual and controlled
    domains, it is critical to analyze not only the accuracy or the fact that it works
    most of the time, but if such a model is truly robust and reliable. This paper
    studies strategies to implement adversary robustly trained algorithms towards
    guaranteeing safety in machine learning algorithms. We provide a taxonomy to classify
    adversarial attacks and defenses, formulate the Robust Optimization problem in
    a min-max setting, and divide it into 3 subcategories, namely: Adversarial (re)Training,
    Regularization Approach, and Certified Defenses. We survey the most recent and
    important results in adversarial example generation, defense mechanisms with adversarial
    (re)Training as their main defense against perturbations. We also survey mothods
    that add regularization terms which change the behavior of the gradient, making
    it harder for attackers to achieve their objective. Alternatively, we’ve surveyed
    methods which formally derive certificates of robustness by exactly solving the
    optimization problem or by approximations using upper or lower bounds. In addition
    we discuss the challenges faced by most of the recent algorithms presenting future
    research perspectives.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们寻求将机器学习模型应用于虚拟和受控领域之外时，至关重要的是不仅要分析模型的准确性或它大多数时间都能正常工作，还要检验该模型是否真正鲁棒和可靠。本文研究了实施对抗鲁棒训练算法的策略，以保证机器学习算法的安全性。我们提供了一个分类法来分类对抗攻击和防御，提出了在最小-最大设置中形成鲁棒优化问题，并将其分为3个子类别，即：对抗（再）训练、正则化方法和认证防御。我们综述了对抗样本生成、防御机制以及以对抗（再）训练作为其主要防御手段的最新重要结果。我们还综述了添加正则化项以改变梯度行为的的方法，使攻击者更难实现目标的技术。或者，我们调查了通过精确解决优化问题或使用上下界近似方法正式推导鲁棒性证书的方法。此外，我们讨论了大多数近期算法面临的挑战，并提出了未来研究的展望。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Artificial Intelligence, Deep Learning, Robustness, Adversarial Examples, Robust
    Optimization, Certified Defenses.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能、深度学习、鲁棒性、对抗样本、鲁棒优化、认证防御。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Deep Learning (DL) ([[1](#bib.bib1)]) models are changing the way we solve problems
    that have required many attempts from the most diverse fields of science. DL is
    an improvement over Artificial Intelligence (AI) Neural Networks (NN), in which
    more layers are stacked to grant a bigger level of abstraction and better reasoning
    over the data when compared to other Machine Learning (ML) algorithms ([[2](#bib.bib2)]).
    Since the raise of DL, supported in many cases by cloud environments [[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)], the base architecture and its variations have
    been applied in many scientific breakthroughs in the most diverse fields of knowledge,
    e.g. in predicting AMD disease progression ([[6](#bib.bib6)]), predicting DNA
    enhancers for gene expression programmes ([[7](#bib.bib7)]), elections and demographic
    analysis based on satellite images ([[8](#bib.bib8)]), filtering data for gravitational-wave
    signals ([[9](#bib.bib9)]). DL approach has also become one of the most used approaches
    for natural language processing ([[10](#bib.bib10)]) and speech recognition ([[11](#bib.bib11)]).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）（[[1](#bib.bib1)]）模型正在改变我们解决问题的方式，这些问题曾需要来自各种科学领域的多次尝试。DL相对于人工智能（AI）神经网络（NN）是一种改进，叠加了更多层级以提供更高水平的抽象和更好的数据推理，与其他机器学习（ML）算法相比（[[2](#bib.bib2)]）。自DL兴起以来，在许多情况下得到云环境（[[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]）的支持，其基础架构及其变体已应用于许多科学突破，涵盖最广泛的知识领域，例如预测AMD疾病进展（[[6](#bib.bib6)]）、预测基因表达程序中的DNA增强子（[[7](#bib.bib7)]）、基于卫星图像的选举和人口统计分析（[[8](#bib.bib8)]）、过滤引力波信号的数据（[[9](#bib.bib9)]）。DL方法也已成为自然语言处理（[[10](#bib.bib10)]）和语音识别（[[11](#bib.bib11)]）最常用的方法之一。
- en: One of the most popular variations of DL architecture, Convolutional Neural
    Networks (CNN) have significantly boosted the performance of DL algorithms in
    computer vision (CV) applications ([[12](#bib.bib12)]), bringing it to several
    areas of CV such as, object detection ([[13](#bib.bib13), [14](#bib.bib14)], action
    recognition [[15](#bib.bib15), [16](#bib.bib16)], pose estimation [[17](#bib.bib17),
    [18](#bib.bib18)], image segmentation [[19](#bib.bib19), [20](#bib.bib20)], and
    motion tracking [[21](#bib.bib21)]. Starting with ImageNet [[22](#bib.bib22)],
    proposed in 2012, the field of CNN’s have seen great improvement with super-human
    performance in specific tasks, providing solutions even to medical problems [[23](#bib.bib23)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）架构中最受欢迎的变体之一，卷积神经网络（CNN），显著提升了计算机视觉（CV）应用中的DL算法性能（[[12](#bib.bib12)]），扩展到了CV的多个领域，如目标检测（[[13](#bib.bib13),
    [14](#bib.bib14)]）、动作识别（[[15](#bib.bib15), [16](#bib.bib16)]）、姿态估计（[[17](#bib.bib17),
    [18](#bib.bib18)]）、图像分割（[[19](#bib.bib19), [20](#bib.bib20)]）和运动跟踪（[[21](#bib.bib21)]）。自2012年提出的ImageNet（[[22](#bib.bib22)]）以来，CNN领域已取得重大进展，在特定任务中达到了超人类的表现，甚至为医疗问题提供了解决方案（[[23](#bib.bib23)]）。
- en: '<svg   height="153.79" overflow="visible" version="1.1" width="327.78"><g transform="translate(0,153.79)
    matrix(1 0 0 -1 0 0) translate(166.84,0) translate(0,134.22)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -20.74 -15.61)" fill="#000000"
    stroke="#000000"><g  transform="matrix(1 0 0 -1 0 27.365)"><g transform="matrix(1
    0 0 1 0 31.21)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Defenses</text></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -162.88 -80.19)" fill="#000000" stroke="#000000"><g  transform="matrix(1
    0 0 -1 0 36.745)"><g  transform="matrix(1 0 0 1 0 31.21)"><g transform="matrix(1
    0 0 -1 30.76 0)"><g  transform="matrix(1 0 0 -1 0 27.365)"><g transform="matrix(1
    0 0 1 0 31.21)"><g transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1
    0 0 -1 0 0)">Gradient</text></g></g></g></g></g><g transform="matrix(1 0 0 1 0
    39.51)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Masking/Obfuscation</text></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -31.98 -79.52)" fill="#000000" stroke="#000000"><g
    transform="matrix(1 0 0 -1 0 36.07)"><g  transform="matrix(1 0 0 1 0 31.21)"><g  transform="matrix(1
    0 0 -1 14.65 0)"><g  transform="matrix(1 0 0 -1 0 27.365)"><g transform="matrix(1
    0 0 1 0 31.21)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><foreignobject width="34.65" height="7.69" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">Robust</foreignobject></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 38.77)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><foreignobject width="63.96" height="9.72" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Optimization</foreignobject></g></g></g></g>
    <g stroke-width="0.4pt" fill="#E6E6E6"><g transform="matrix(1.0 0.0 0.0 1.0 -154.97
    -114.1)" fill="#000000" stroke="#000000"><foreignobject width="88.56" height="23.52"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Adversarial Training <path
    d="M -35.95 -78.22 L -81.23 -102.38" style="fill:none"><g stroke-width="0.4pt"
    fill="#E6E6E6"><clippath ><path d="M 42.43 -110.55 L -42.43 -110.55 C -45.49 -110.55
    -47.97 -113.02 -47.97 -116.08 L -47.97 -120.14 C -47.97 -123.2 -45.49 -125.67
    -42.43 -125.67 L 42.43 -125.67 C 45.49 -125.67 47.97 -123.2 47.97 -120.14 L 47.97
    -116.08 C 47.97 -113.02 45.49 -110.55 42.43 -110.55 Z M -47.97 -125.67"></path></clippath><g
    clip-path="url(#pgfcp17)"><g transform="matrix(1.0 0.0 0.0 1.0 0 -118.11) matrix(1.0
    0.0 0.0 1.0 0 0) matrix(1.38165 0.0 0.0 0.21786 0 0)"><defs><lineargradient gradienttransform="rotate(90)"
    ></lineargradient></defs></g></g><path d="M 42.43 -110.55 L -42.43 -110.55 C -45.49
    -110.55 -47.97 -113.02 -47.97 -116.08 L -47.97 -120.14 C -47.97 -123.2 -45.49
    -125.67 -42.43 -125.67 L 42.43 -125.67 C 45.49 -125.67 47.97 -123.2 47.97 -120.14
    L 47.97 -116.08 C 47.97 -113.02 45.49 -110.55 42.43 -110.55 Z M -47.97 -125.67"
    style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 -44.28 -121.98)" fill="#000000"
    stroke="#000000"><foreignobject width="88.56" height="7.75" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Certified Defenses</foreignobject></g> <path
    d="M 0 -83.48 L 0 -110.27" style="fill:none"></path></path></g><g stroke-width="0.4pt"
    fill="#E6E6E6"><clippath ><path d="M 153.13 -102.27 L 68.26 -102.27 C 65.21 -102.27
    62.73 -104.75 62.73 -107.81 L 62.73 -128.41 C 62.73 -131.47 65.21 -133.95 68.26
    -133.95 L 153.13 -133.95 C 156.19 -133.95 158.66 -131.47 158.66 -128.41 L 158.66
    -107.81 C 158.66 -104.75 156.19 -102.27 153.13 -102.27 Z M 62.73 -133.95"></path></clippath><g
    clip-path="url(#pgfcp19)"><g transform="matrix(1.0 0.0 0.0 1.0 110.7 -118.11)
    matrix(1.0 0.0 0.0 1.0 0 0) matrix(1.38165 0.0 0.0 0.45612 0 0)"><defs><lineargradient
    gradienttransform="rotate(90)" ></lineargradient></defs></g></g><path d="M 153.13
    -102.27 L 68.26 -102.27 C 65.21 -102.27 62.73 -104.75 62.73 -107.81 L 62.73 -128.41
    C 62.73 -131.47 65.21 -133.95 68.26 -133.95 L 153.13 -133.95 C 156.19 -133.95
    158.66 -131.47 158.66 -128.41 L 158.66 -107.81 C 158.66 -104.75 156.19 -102.27
    153.13 -102.27 Z M 62.73 -133.95" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 66.42 -113.71)" fill="#000000" stroke="#000000"><foreignobject width="88.56"
    height="24.29" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Regularization
    Approach</foreignobject></g> <path d="M 35.95 -78.22 L 80.51 -102" style="fill:none"></path></path></g><clippath
    ><path d="M 155.13 -34.84 L 66.26 -34.84 C 63.21 -34.84 60.73 -37.32 60.73 -40.38
    L 60.73 -77.73 C 60.73 -80.79 63.21 -83.27 66.26 -83.27 L 155.13 -83.27 C 158.18
    -83.27 160.66 -80.79 160.66 -77.73 L 160.66 -40.38 C 160.66 -37.32 158.18 -34.84
    155.13 -34.84 Z M 60.73 -83.27"></path></clippath><g clip-path="url(#pgfcp21)"><g
    transform="matrix(1.0 0.0 0.0 1.0 110.7 -59.06) matrix(1.0 0.0 0.0 1.0 0 0) matrix(1.43922
    0.0 0.0 0.69746 0 0)"><defs><lineargradient gradienttransform="rotate(90)" ></lineargradient></defs></g></g><path
    d="M 155.13 -34.84 L 66.26 -34.84 C 63.21 -34.84 60.73 -37.32 60.73 -40.38 L 60.73
    -77.73 C 60.73 -80.79 63.21 -83.27 66.26 -83.27 L 155.13 -83.27 C 158.18 -83.27
    160.66 -80.79 160.66 -77.73 L 160.66 -40.38 C 160.66 -37.32 158.18 -34.84 155.13
    -34.84 Z M 60.73 -83.27" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0
    64.42 -79.58)" fill="#000000" stroke="#000000"><g  transform="matrix(1 0 0 -1
    0 36.13)"><g  transform="matrix(1 0 0 1 0 31.21)"><g transform="matrix(1 0 0 -1
    18.7 0)"><g  transform="matrix(1 0 0 -1 0 27.365)"><g transform="matrix(1 0 0
    1 0 31.21)"><g transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0
    -1 0 0)">Adversarial</text></g></g></g></g></g><g transform="matrix(1 0 0 1 0
    38.9)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Example Detection</text></g></g></g></g>
    <path d="M 24.71 -13.17 L 64.81 -34.56" style="fill:none">Figure 1: Defenses against
    adversarial attacks are divided in 3 categories: Gradient Masking/Obfuscation,
    Robust Optimization, and Adversarial Example Detection. The focus of this survey
    is Robust Optimization which we subdivide in: Adversarial Training, Certified
    Defenses, and Regularization Approach.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：对抗性攻击的防御分为三类：**梯度掩蔽/混淆**、**稳健优化**和**对抗样本检测**。本调查的重点是**稳健优化**，我们将其细分为：**对抗训练**、**认证防御**和**正则化方法**。
- en: Fueled by the fact that new frameworks, libraries, and hardware resources are
    being improved and made available to the public and scientific community [[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26)], Deep Neural networks (DNN) are being improved
    constantly and achieving new performance breakthroughs [[27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29)]. With the current maturity of DNN algorithms, its being applied
    in solving safety and security-critical problems [[30](#bib.bib30)], such as self-driving
    cars [[31](#bib.bib31), [32](#bib.bib32)], multi-agent aerial vehicle systems
    with face identification [[33](#bib.bib33)], robotics [[34](#bib.bib34), [35](#bib.bib35)],
    social engineering detection [[36](#bib.bib36)], network anomaly detection [[37](#bib.bib37)],
    deep packet inspection in networks [[38](#bib.bib38)]. DNN applications are already
    part of our day-to-day life (personal assistants [[39](#bib.bib39)], product recommendation
    [[40](#bib.bib40)], biometric identification [[41](#bib.bib41)]) and tend to occupy
    a bigger space as time passes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于新的框架、库和硬件资源不断改进并向公众和科学社区开放[[24](#bib.bib24)、[25](#bib.bib25)、[26](#bib.bib26)]，深度神经网络（DNN）不断得到改进并取得了新的性能突破[[27](#bib.bib27)、[28](#bib.bib28)、[29](#bib.bib29)]。随着
    DNN 算法的当前成熟度，它被应用于解决安全和保密性关键问题[[30](#bib.bib30)]，例如自动驾驶汽车[[31](#bib.bib31)、[32](#bib.bib32)]、具有人脸识别的多智能体空中载具系统[[33](#bib.bib33)]、机器人技术[[34](#bib.bib34)、[35](#bib.bib35)]、社会工程检测[[36](#bib.bib36)]、网络异常检测[[37](#bib.bib37)]、网络深度包检测[[38](#bib.bib38)]。DNN
    应用已经成为我们日常生活的一部分（个人助手[[39](#bib.bib39)]、产品推荐[[40](#bib.bib40)]、生物识别[[41](#bib.bib41)]），并随着时间的推移占据越来越大的空间。
- en: As seen in many publications, DNN has been shown to have human-level accuracy
    even for significantly complex tasks such as playing games with no prior rule
    known, except the current frames [[42](#bib.bib42)]. In contrast to the aforementioned
    accuracy of DNN models, its been shown in earlier publications [[43](#bib.bib43),
    [44](#bib.bib44), [45](#bib.bib45)], that DNN models are susceptible to small
    input perturbations, in most cases imperceptible to the human eye. The results
    from this publications have shown the facility with which small additive targeted
    noise to the input image, makes models to misclassify objects which before could
    be identified with 99.99% confidence. More alarming is the fact that such models
    report high confidence in the predictions. Such perturbations, which can fool
    a trained model, are known as adversarial attacks. With such alarming consequences,
    the study of adversarial attacks and robustness against them became a great deal
    of research in recent years.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从许多出版物中可以看出，DNN 已经展示了即使在规则未知的复杂任务中（如玩游戏）也能达到人类水平的准确性，除当前帧之外没有任何先前规则[[42](#bib.bib42)]。与上述
    DNN 模型的准确性相比，早期出版物[[43](#bib.bib43)、[44](#bib.bib44)、[45](#bib.bib45)]中已经显示出 DNN
    模型对微小的输入扰动敏感，这些扰动在大多数情况下肉眼不可察觉。这些出版物的结果表明，向输入图像添加微小的目标噪声会使模型误分类原本可以以 99.99% 的信心识别的对象。更令人担忧的是，这些模型在预测时报告的信心很高。这种可以欺骗训练过的模型的扰动被称为对抗攻击。由于其令人震惊的后果，对抗攻击及其鲁棒性的研究近年来成为了一个重要的研究领域。
- en: A considerably large number of research papers is now available concerning methods
    to identify adversarial attacks and defend from incursions against the model [[46](#bib.bib46),
    [47](#bib.bib47)]. One way of solving this issue is adding better intuition on
    the models, through explainability [[48](#bib.bib48)], but such models do not
    target the direct improvement of the model. On the other hand, several approaches
    have been published to generate models which are robust against adversarial attacks
    [[49](#bib.bib49)], the target of the researchers is to introduce in their models’
    layers of robustness such that the models are not fooled by out of distribution
    examples, known or unknown attacks, targeted or untargeted attacks. Guaranteeing
    the accuracy of such models while safety is taken into consideration, is of utmost
    importance for system architects, mainly making them robust to the presence of
    adversarial attacks, noise, model misspecification, and uncertainty. This survey
    aims to bring together the recent advances in robustness for DNN’s, pointing the
    main research directions being followed recently to generate robust DNN models.
    We bring to light both applied and theoretical recent developments.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 目前已有大量研究论文涉及识别对抗攻击的方法以及防御模型的侵袭[[46](#bib.bib46), [47](#bib.bib47)]。解决此问题的一种方法是通过可解释性[[48](#bib.bib48)]增强模型的直观性，但这种模型并不直接改善模型的性能。另一方面，已经发布了多种方法来生成对抗攻击鲁棒的模型[[49](#bib.bib49)]，研究人员的目标是将鲁棒性引入模型的层中，使得模型不会被分布外样本、已知或未知攻击、目标攻击或非目标攻击欺骗。在考虑安全性的同时保证这些模型的准确性，对系统架构师来说至关重要，主要是使它们在面对对抗攻击、噪声、模型误设和不确定性时保持鲁棒性。本调查旨在汇集最近在DNN鲁棒性方面的进展，指出最近为生成鲁棒DNN模型而跟随的主要研究方向。我们揭示了最新的应用和理论发展。
- en: Inspired by [[50](#bib.bib50)], we analyze the robustness of DNN’s under the
    perspective of how adversarial examples can be generated, and how defenses can
    be formulated against such algorithms. In general words, we define the robustness
    against adversarial attacks problem as a dual optimization problem, in which the
    attackers try to maximize the loss while the defenses try to minimize the chance
    of a model being fooled by the attacker. In such formulation, current existing
    models based in non-linear activation functions, introduce non-linear inequality
    constraints in the optimization, which generates an inherent trade-off between
    exact solutions and scalability of the model. This trade-off comes in the form
    of either exact slow solutions through mixed-integer linear programming, or in
    approximations to the objective function which either, relies on the existing
    attack methods to provide a local heuristic estimation to the maximization function,
    or approximations of the bounds of the constraints or objective function to generate
    certification regions, in which no adversarial example exists.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 受到[[50](#bib.bib50)]的启发，我们从对抗样本如何生成的角度分析了深度神经网络（DNN）的鲁棒性，以及如何针对这些算法制定防御措施。一般而言，我们将对抗攻击的鲁棒性问题定义为一个双重优化问题，在这个问题中，攻击者试图最大化损失，而防御者试图最小化模型被攻击者欺骗的可能性。在这种表述中，当前基于非线性激活函数的现有模型在优化中引入了非线性不等式约束，这产生了精确解决方案与模型可扩展性之间的固有权衡。这种权衡表现为通过混合整数线性规划获得的精确但缓慢的解决方案，或者对目标函数的近似，这些近似要么依赖现有攻击方法提供局部启发式估计，要么近似约束或目标函数的界限以生成认证区域，其中不存在对抗样本。
- en: 'More specifically this paper presents the following contributions:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，本文提出了以下贡献：
- en: '1.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We characterize defenses to adversarial attacks as a min-max optimization problem,
    investigating solutions involving heuristic approximation, exact solution, and
    upper/lower bound approximations to generate models robust to adversarial attacks.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将对抗攻击的防御特征描述为一个最小-最大优化问题，研究包括启发式近似、精确解决方案以及上下界近似在内的解决方案，以生成对抗攻击具有鲁棒性的模型。
- en: '2.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We investigate, analyze, and categorize the most recent and/or important approaches
    to generate adversarial examples, as they are the basis to generate strong defenses,
    through Adversarial (re)Training.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们调查、分析和分类了生成对抗样本的最新和/或重要方法，因为它们是通过对抗（再）训练生成强大防御的基础。
- en: '3.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We investigate, analyze, and categorize the most recent and important approaches
    to generate defenses against adversarial attacks, providing a taxonomy, description
    of the methods, and the main results in such approaches.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们调查、分析和分类了最近期和最重要的对抗攻击防御方法，提供了分类法、方法描述和这些方法的主要结果。
- en: 'We organize this survey in the following manner. In [section 2](#S2 "2 Taxonomy
    of Adversarial Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey") we describe taxonomies for both adversarial
    example generation and defenses. We classify the adversarial models concerning
    the time of the attack, information available to the attacker, objective, and
    the algorithm computation method. Moreover, we classify the perturbation type
    used by the attackers. We divide the defense methods into three categories, namely
    gradient-masking/obfuscation, robust optimization, and adversarial example detection.
    We focus this research in Robust Optimization, and further sub-divide in 3 groups:
    Adversarial Training, Certified Defenses, and Regularization Approach. In [section 3](#S3
    "3 Methods for Generating Adversarial Attacks ‣ Opportunities and Challenges in
    Deep Learning Adversarial Robustness: A Survey"), we describe several relevant
    adversarial attacks, and summarize them in [Table II](#S3.T2 "TABLE II ‣ 3.3.1
    Other Attacks ‣ 3.3 Physical World Attack ‣ 3 Methods for Generating Adversarial
    Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness:
    A Survey"). In [section 4](#S4 "4 Defense Mechanisms based on Robust Optimization
    Against Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey") we describe the most relevant results in Robust Optimization,
    and provide a tree that maps these publications to the 3 sub-groups of Robust
    Optimization. In [section 5](#S5 "5 Challenges and Future Opportunities ‣ Opportunities
    and Challenges in Deep Learning Adversarial Robustness: A Survey") we discuss
    current challenges and opportunities in robust defenses.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '我们以以下方式组织这项调查。在[第 2 节](#S2 "2 Taxonomy of Adversarial Attacks and Defenses
    ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey")中，我们描述了对抗样本生成和防御的分类法。我们根据攻击时间、攻击者可用的信息、目标和算法计算方法来分类对抗模型。此外，我们还对攻击者使用的扰动类型进行分类。我们将防御方法分为三类：梯度掩盖/混淆、鲁棒优化和对抗样本检测。我们将研究重点放在鲁棒优化上，并进一步细分为三个组：对抗训练、认证防御和正则化方法。在[第
    3 节](#S3 "3 Methods for Generating Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey")中，我们描述了几个相关的对抗攻击，并在[表 II](#S3.T2
    "TABLE II ‣ 3.3.1 Other Attacks ‣ 3.3 Physical World Attack ‣ 3 Methods for Generating
    Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey")中总结。 在[第 4 节](#S4 "4 Defense Mechanisms based on Robust
    Optimization Against Adversarial Attacks ‣ Opportunities and Challenges in Deep
    Learning Adversarial Robustness: A Survey")中，我们描述了鲁棒优化中的最相关结果，并提供了一个树形图，将这些出版物映射到鲁棒优化的三个子组。在[第
    5 节](#S5 "5 Challenges and Future Opportunities ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey")中，我们讨论了鲁棒防御中的当前挑战和机会。'
- en: 2 Taxonomy of Adversarial Attacks and Defenses
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 对抗攻击与防御的分类
- en: 'We keep a consistent notation set along with the survey, and for easiness of
    reading we summarize in [Table I](#S2.T1 "TABLE I ‣ 2 Taxonomy of Adversarial
    Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey") the most used notations and symbols which we will use along
    with this survey. For papers requiring some specific terms, we define them in
    the section in which they are presented.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在调查中保持一致的符号集，为了便于阅读，我们在[表 I](#S2.T1 "TABLE I ‣ 2 Taxonomy of Adversarial
    Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey")中总结了最常用的符号和表示法，我们将在本调查中使用它们。对于需要特定术语的论文，我们将在相关章节中定义这些术语。'
- en: 'TABLE I: Symbols and notations used in the mathematical definitions'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 数学定义中使用的符号和表示法'
- en: '| Symbol | Description |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 描述 |'
- en: '| --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $x$ | original (clean, unmodified) input data |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| $x$ | 原始（干净、未修改）输入数据 |'
- en: '| $\hat{y}$ | model’s prediction |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| $\hat{y}$ | 模型预测 |'
- en: '| $t$ | class label |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| $t$ | 类别标签 |'
- en: '| $x^{\prime}$ | adversarial example |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| $x^{\prime}$ | 对抗样本 |'
- en: '| $y^{\prime}$ | target class of adversarial example |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| $y^{\prime}$ | 对抗样本的目标类别 |'
- en: '| $f(.)$ | DL model |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| $f(.)$ | 深度学习模型 |'
- en: '| $\theta$ | parameters of the model |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| $\theta$ | 模型参数 |'
- en: '| $\delta$ | perturbation generated by adv. algorithm |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| $\delta$ | 对抗算法生成的扰动 |'
- en: '| $\Delta$, $\epsilon$ | perturbation constraint |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| $\Delta$, $\epsilon$ | 扰动约束 |'
- en: '| $\nabla$ | gradient function |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| $\nabla$ | 梯度函数 |'
- en: '| $\left\&#124;.\right\&#124;_{p}$ | the $l_{p}$-norm |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| $\left\|.\right\|_{p}$ | $l_{p}$-范数 |'
- en: '| $\mathcal{L}$ | loss function (e.g., cross-entropy) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}$ | 损失函数（例如，交叉熵） |'
- en: '| $\mathcal{D}$ | Training data distribution |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{D}$ | 训练数据分布 |'
- en: '| $KL$-divergence | Kullback-Leibler divergence function |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| $KL$-散度 | Kullback-Leibler 散度函数 |'
- en: 2.1 Attack Threat Model
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 攻击威胁模型
- en: 'Several attempts have been made to categorize attacks on machine learning.
    We here distill the most important aspects which characterize adversarial examples
    generating models concerning their architecture. We focus on the aspects that
    are most relevant to the discussion of adversarial robustness. To that end, we
    classify the attacks concerning timing, information, goals, and attack frequency
    following the proposed in [[51](#bib.bib51)]:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 已经尝试对机器学习中的攻击进行分类。我们在这里提炼了最重要的方面，这些方面特征化了生成对抗样本的模型及其架构。我们重点讨论了对抗鲁棒性讨论中最相关的方面。因此，我们根据[[51](#bib.bib51)]中提出的分类标准，按时间、信息、目标和攻击频率对攻击进行分类：
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Timing: A first crucial feature for modeling the adversarial attacks is when
    it occurs. To that end we have two possibilities, evasion and poisoning attacks.
    Evasion attacks are the ones in the time of inference and assume the model has
    already been trained. Poisoning attacks in general targets the data, and the training
    phase of the model.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 时间：建模对抗攻击的第一个关键特征是攻击发生的时间。为此，我们有两种可能性，规避攻击和中毒攻击。规避攻击是在推断时发生的，并假设模型已经被训练。中毒攻击通常针对数据和模型的训练阶段。
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Information: Another feature of the attack references the information to which
    the attacker has access. In the white box context the adversary has full access
    to information concerning the model and the model itself, as opposed to black
    box setting, in which very few or no information is available. White box attacks
    refer to those in which the adversary can unrestrictedly query the model for any
    information, such as weights, gradient, model hyper-parameters, prediction scores.
    Whereas in black-box attacks the adversary has limited or no information about
    these parameters, although may obtain some of the information indirectly, for
    example, through queries. Some also define grey-box attacks, in which attackers
    might only know the feature representation and the type of model that was used
    but have no access to dataset or the model information. A fourth setting is called
    restricted black-box, or also known as no-box attack. Under such an assumption,
    no information is available to the attacker, and the research is mainly focused
    on attack transferability. In wich the focus is to evaluate the possibility of
    transferring the attack performed in one DNN to the inaccessible objective model
    [[52](#bib.bib52)]. In this work, we evaluate models in a binary setting, the
    adversary either has comprehensive access to the DNN or black box having limited
    access through queries, which can also provide class scores.'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息：攻击的另一个特征是攻击者可以访问的信息。在白盒环境中，对手可以完全访问有关模型的信息以及模型本身，而在黑盒环境中，几乎没有或没有信息可用。白盒攻击指的是对手可以不受限制地查询模型获取任何信息，如权重、梯度、模型超参数、预测得分。而在黑盒攻击中，对手对这些参数的了解有限或没有，尽管可以通过查询间接获得一些信息。有些还定义了灰盒攻击，其中攻击者可能只知道特征表示和使用的模型类型，但无法访问数据集或模型信息。第四种设置称为受限黑盒，或称为无盒攻击。在这种假设下，对攻击者没有信息可用，研究主要集中在攻击的可转移性上。在这种情况下，重点是评估在一个深度神经网络中进行的攻击转移到不可访问的目标模型的可能性[[52](#bib.bib52)]。在本工作中，我们在二元设置中评估模型，对手要么对深度神经网络有全面的访问权限，要么是通过查询拥有有限访问权限的黑盒，这也可以提供类别得分。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Goals: The attackers may have different reasons to target a specific algorithm.
    But mostly the attacker has either a specific goal, and needs the algorithm to
    output a specific output, case in which it is a targeted attack, or just wants
    to reduce the reliability of the algorithm by forcing a mistake. In the latter,
    we have an untargeted attack.'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标：攻击者可能有不同的原因来针对特定算法。但通常，攻击者要么有一个特定的目标，需要算法输出特定的结果，这种情况下为定向攻击，要么只是想通过强制出现错误来降低算法的可靠性。在后者中，我们称之为非定向攻击。
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Attack Frequency: The attack on the victim’s model can be either iterative
    or one-time. In the one-time, the optimization of the objective function of the
    attacker happens in a single step, whereas the iterative method takes several
    steps to generate the perturbation.'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 攻击频率：对受害者模型的攻击可以是迭代式的，也可以是一次性的。在一次性攻击中，攻击者的目标函数优化在单步内完成，而迭代方法则需要多个步骤来生成扰动。
- en: 2.2 Attack Perturbation Type
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 攻击扰动类型
- en: The size of the perturbation is in the core of the adversarial attack, a small
    perturbation is the fundamental premise of such models. When designing an adversarial
    example, the attacker wants the perturbed input to be as close as possible to
    the original one, in the case of images, close enough that a human can not distinguish
    one image from the other. We analyze the perturbation concerning scope, limitation,
    and measurement.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 扰动的大小是对抗攻击的核心，一个小的扰动是这些模型的基本前提。在设计对抗样本时，攻击者希望扰动后的输入尽可能接近原始输入，对于图像来说，足够接近到以至于人类无法区分两幅图像。我们将从范围、限制和测量方面分析扰动。
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Perturbation Scope: The attacker can generate perturbations that are input
    specific, in which we call individual, or it can generate a single perturbation
    which will be effective to all inputs in the training dataset, which we call universal
    perturbation.'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 扰动范围：攻击者可以生成针对特定输入的扰动，我们称之为个体扰动，或者可以生成一个对训练数据集中的所有输入都有效的单一扰动，我们称之为通用扰动。
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Perturbation Limitation: Two options are possible, optimized perturbation and
    constraint perturbation. The optimized perturbation is the goal of the optimization
    problem, while the constraint perturbation is the set as the constraint to the
    optimization problem.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 扰动限制：有两种可能的选项，优化扰动和约束扰动。优化扰动是优化问题的目标，而约束扰动则是设置为优化问题的约束条件。
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Perturbation Measurement: Is the metric used to measure the magnitude of the
    perturbation. The most commonly used metric is the $l_{p}$-norm, with many algorithms
    applying $l_{0},l_{2},l_{\infty}$ norms.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 扰动测量：用于衡量扰动幅度的度量标准。最常用的度量标准是 $l_{p}$-范数，许多算法应用 $l_{0},l_{2},l_{\infty}$ 范数。
- en: 2.3 Defense Methods
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 防御方法
- en: 'As seen in [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Opportunities and
    Challenges in Deep Learning Adversarial Robustness: A Survey") based on [[53](#bib.bib53)],
    we sub-divide the defenses to adversarial attacks in 3 main categories: Gradient
    Masking/Obfuscation, Robust Optimization, and Adversarial Example Detection, which
    are described as:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 1](#S1.F1 "图 1 ‣ 1 引言 ‣ 深度学习对抗鲁棒性的机会与挑战：综述") 所示，根据 [[53](#bib.bib53)]，我们将对抗攻击的防御细分为三大类：梯度掩蔽/混淆、鲁棒优化和对抗样本检测，具体描述如下：
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Gradient Masking/Obfuscation: The core aspect of defense mechanisms based on
    gradient masking is constructing models with gradients that are not useful for
    attackers. The gradient masked/obfuscated models, in general, produce loss functions
    that are very smooth in the neighborhood of the input data. This smoothness around
    training data points makes it difficult for exploiting algorithms to find meaningful
    directions towards the generation of an adversarial example.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 梯度掩蔽/混淆：基于梯度掩蔽的防御机制的核心是构建梯度对攻击者无用的模型。一般来说，梯度掩蔽/混淆模型在输入数据的邻域内生成的损失函数非常光滑。这种训练数据点周围的光滑性使得利用算法难以找到生成对抗样本的有效方向。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Robust Optimization: Is a defense strategy that is composed of methods that
    improve the optimization function either by adding regularization terms, certification
    bounds, adversarial examples in the objective function, or modifying the model
    to add uncertainty in the model layers.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鲁棒优化：是一种防御策略，包含通过添加正则化项、认证界限、目标函数中的对抗样本，或修改模型以在模型层中添加不确定性等方法来改善优化函数的策略。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adversarial Example Detection: Recent work has turned to detect adversarial
    examples rather than making the DNN robust against creating them. Detecting adversarial
    examples is usually done by finding statistical outliers or training separate
    sub-networks that can distinguish between perturbed and normal images.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗样本检测：近期的研究转向了检测对抗样本，而不是使深度神经网络（DNN）对生成对抗样本变得更加鲁棒。检测对抗样本通常是通过寻找统计异常值或训练能够区分扰动图像和正常图像的独立子网络来完成的。
- en: About the defense mechanisms, we focus this survey on methods related to Robust
    Optimization. Among the several publications in this survey, each author has its
    representation and view of robust optimization. In general, even with different
    notations and representations, most of the papers we have surveyed fit the general
    representation of Robust Optimization.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 关于防御机制，我们将本调查集中在与鲁棒优化相关的方法上。在本调查中的多个出版物中，每位作者对鲁棒优化都有其表示和观点。一般而言，即使存在不同的符号和表示方式，我们调查的大多数论文也符合鲁棒优化的一般表示。
- en: 'The training objective in a DL model is the minimization of the desired loss.
    The objective is to ajust the model parameters with respect to the labeled data,
    as seen in [Equation 1](#S2.E1 "1 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial
    Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey"),'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '在深度学习模型中的训练目标是最小化期望的损失。目标是根据标记数据调整模型参数，如在[方程 1](#S2.E1 "1 ‣ 2.3 Defense Methods
    ‣ 2 Taxonomy of Adversarial Attacks and Defenses ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey")中所示，'
- en: '|  | $\min_{\theta}\mathcal{L}(\theta,x,y)$ |  | (1) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta}\mathcal{L}(\theta,x,y)$ |  | (1) |'
- en: in which $\theta$ are the model parameters, $x$ is the input to the model, $\mathcal{L}$
    is the defined loss function, and $y$ is its true label. With such a formulation,
    we seek to minimize w.r.t. $\theta$, the loss function. Such formulation fit the
    parameters to the data points, such that $f(x)$ yields predictions $\hat{y}$ which
    are equal to the true label $y$. In an adversarial setting this scene changes,
    in which the objective is different,
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta$ 是模型参数，$x$ 是模型的输入，$\mathcal{L}$ 是定义的损失函数，而 $y$ 是其真实标签。通过这种形式化，我们寻求相对于
    $\theta$ 最小化损失函数。这种形式化将参数拟合到数据点，使得 $f(x)$ 产生的预测 $\hat{y}$ 等于真实标签 $y$。在对抗设置中，这种情况会改变，其中目标不同，
- en: '|  | $\max_{\delta\leq\Delta}\mathcal{L}(\theta,x+\delta,y)$ |  | (2) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\delta\leq\Delta}\mathcal{L}(\theta,x+\delta,y)$ |  | (2) |'
- en: in which we are searching for a perturbation $\delta$, smaller than a maximum
    perturbation $\Delta$, capable of changing the decision of the classifier from
    prediction $\hat{y}$, to $y^{\prime}$. The restriction on the perturbation is
    a designer parameter which is in general defined by the $l_{p}$-norm.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们寻找一个扰动 $\delta$，小于最大扰动 $\Delta$，能够将分类器的决策从预测 $\hat{y}$ 改变为 $y^{\prime}$。对扰动的限制是一个设计参数，通常由
    $l_{p}$-范数定义。
- en: 'Equations [1](#S2.E1 "In 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial Attacks
    and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness:
    A Survey") and [2](#S2.E2 "In 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial
    Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey") do not incorporate the data distribution or the restrictions
    which come from the fact that most of the training datasets do not incorporate
    the true distribution of the data in which the models will perform inference.
    Based on the definition from [[54](#bib.bib54)], we have that, if $\mathbb{D}$
    is the true distribution of the data, a training set is draw i.i.d. from $\mathbb{D}$,
    and is defined as $\mathcal{D}=\{(x_{i},y_{i})\sim\mathbb{D}\},\text{ for }i=1,...,m$.
    And the empirical risk of a classifier, which is based on the training set, is
    defined as:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '方程 [1](#S2.E1 "In 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial Attacks and
    Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness:
    A Survey") 和 [2](#S2.E2 "In 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial Attacks
    and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness:
    A Survey") 并没有包含数据分布或来自训练数据集大多数没有包含模型推理数据的真实分布的限制。根据[[54](#bib.bib54)]的定义，如果 $\mathbb{D}$
    是数据的真实分布，训练集是从 $\mathbb{D}$ 中独立同分布（i.i.d.）抽取的，并定义为 $\mathcal{D}=\{(x_{i},y_{i})\sim\mathbb{D}\},\text{
    for }i=1,...,m$。而分类器的经验风险，基于训练集，定义为：'
- en: '|  | $R(F,\mathcal{D})=\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(x,y)\in\mathcal{D}}\mathcal{L}(f(x),y)$
    |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(F,\mathcal{D})=\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(x,y)\in\mathcal{D}}\mathcal{L}(f(x),y)$
    |  |'
- en: 'in which $|\mathcal{D}|$ is the size of the training set $\mathcal{D}$. With
    that definition the empirical adversarial risk is defined in:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $|\mathcal{D}|$ 是训练集 $\mathcal{D}$ 的大小。根据这个定义，经验对抗风险定义为：
- en: '|  | $R_{adv}(F,\mathcal{D})=\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(x,y)\in\mathcal{D}}\mathcal{L}(f(x+\delta),y)$
    |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $R_{adv}(F,\mathcal{D})=\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(x,y)\in\mathcal{D}}\mathcal{L}(f(x+\delta),y)$
    |  |'
- en: When dealing with adversarial defenses in the lenses of Robust Optimization,
    the one first solution, is to solve the combined worst-case loss, with the empirical
    adversarial risk $R_{adv}$, known as adversarial training.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在稳健优化的视角下处理对抗性防御时，第一个解决方案是求解组合的最坏情况损失，使用经验对抗风险 $R_{adv}$，这被称为对抗训练。
- en: '|  | $\min_{\theta}\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(x,y)\in\mathcal{D}}\max_{\delta\in\Delta}\mathcal{L}(f(x+\delta),y)$
    |  | (3) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta}\frac{1}{&#124;\mathcal{D}&#124;}\sum_{(x,y)\in\mathcal{D}}\max_{\delta\in\Delta}\mathcal{L}(f(x+\delta),y)$
    |  | (3) |'
- en: 'The solution of [Equation 3](#S2.E3 "3 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of
    Adversarial Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey"), require special handling or a completely different
    formulation which define how we categorize the defense mechanisms for adversarial
    attacks, namely: Adversarial (re)Training, Bayesian Approach, Regularization Approach,
    and Certified Defenses.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 3](#S2.E3 "3 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial Attacks
    and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness:
    A Survey")的解决方案需要特殊处理或完全不同的公式，这定义了我们如何对对抗攻击进行防御机制的分类，即：对抗（重新）训练、贝叶斯方法、正则化方法和认证防御。'
- en: 2.3.1 Adversarial (re)Training as a Defense Mechanism
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 对抗（重新）训练作为防御机制
- en: 'The solution of [Equation 3](#S2.E3 "3 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of
    Adversarial Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey") requires solving the inner maximization ([Equation 2](#S2.E2
    "2 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial Attacks and Defenses ‣ Opportunities
    and Challenges in Deep Learning Adversarial Robustness: A Survey")), which is
    a high dimensional non-convex optimization problem prohibitively hard to solve
    exactly by standard optimization techniques. The most popular approach to solve
    such a problem is by approximating [Equation 2](#S2.E2 "2 ‣ 2.3 Defense Methods
    ‣ 2 Taxonomy of Adversarial Attacks and Defenses ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey") with the use of a heuristics
    in which we are searching for a lower bound for [Equation 2](#S2.E2 "2 ‣ 2.3 Defense
    Methods ‣ 2 Taxonomy of Adversarial Attacks and Defenses ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey"). While promising and shown
    to improve robustness even for large models (ImageNet [[55](#bib.bib55)]), such
    models come with a drawback which when instantiated in practice with the approximation
    heuristics, they are unable to provide robustness guarantees or certifications.
    This class of defenses even though very practical to implement can not provide
    a guarantee that no adversarial example exists in the neighborhood of $x$ capable
    of fooling $f(.)$.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 3](#S2.E3 "3 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial Attacks
    and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness:
    A Survey")的解决方案需要解决内部的最大化问题（[方程 2](#S2.E2 "2 ‣ 2.3 Defense Methods ‣ 2 Taxonomy
    of Adversarial Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey")），这是一个高维非凸优化问题，标准优化技术无法准确解决。解决此类问题的最流行方法是通过启发式方法近似
    [方程 2](#S2.E2 "2 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial Attacks and
    Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness:
    A Survey")，我们在寻找 [方程 2](#S2.E2 "2 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial
    Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey") 的下界。虽然这种方法很有前景，甚至显示出能提高大模型（ImageNet [[55](#bib.bib55)]）的稳健性，但这种模型有一个缺陷，即在实际应用中，当使用近似启发式时，无法提供稳健性保证或认证。这类防御机制尽管在实现上非常实用，但无法保证在
    $x$ 的邻域内不存在能够欺骗 $f(.)$ 的对抗样本。'
- en: 2.3.2 Certified Defenses
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 认证防御
- en: 'The problem stated in [Equation 3](#S2.E3 "3 ‣ 2.3 Defense Methods ‣ 2 Taxonomy
    of Adversarial Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey"), defines the general objective of adversarial
    training. But as seen, even with the best methods to find a local approximation
    to the maximization problem, we are subjective to the effectiveness of the attacking
    method. A way around this inconvenience has been proposed in the literature, which
    is to exactly solve the maximization problem or approximate to a solvable set
    of constraints. To formally define Certified Defenses, initially, we consider
    a threat model where the adversary is allowed to transform an input $x\in\mathbb{R}^{d}$
    into any point from a set $\mathbb{S}_{0}(x)\subseteq\mathbb{R}^{d}$. Such set
    represents the neighborhood of the point $x$ generated by either $l_{p}$ perturbations,
    geometric transformations, semantic perturbations, or another kind of transformation
    in $x$. In case of an $l_{p}$ perturbation, the set is defined as $\mathbb{S}_{0}(x)=\{x^{\prime}\in\mathbb{R}^{d},\left\|x-x^{\prime}\right\|_{p}<\epsilon\}$.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在[方程 3](#S2.E3 "3 ‣ 2.3 防御方法 ‣ 2 对抗攻击和防御的分类 ‣ 深度学习对抗鲁棒性的机遇与挑战：综述")中提出的问题定义了对抗训练的总体目标。但如所见，即使使用最佳方法来找到最大化问题的局部近似，我们仍然受到攻击方法有效性的影响。文献中提出了一种解决这种不便的方法，即精确解决最大化问题或近似到可解约束集。为了正式定义认证防御，最初我们考虑一个威胁模型，其中对手被允许将输入
    $x\in\mathbb{R}^{d}$ 转换为从集合 $\mathbb{S}_{0}(x)\subseteq\mathbb{R}^{d}$ 中的任何点。这样的集合表示点
    $x$ 的邻域，由 $l_{p}$ 扰动、几何变换、语义扰动或其他类型的变换生成。如果是 $l_{p}$ 扰动，则集合定义为 $\mathbb{S}_{0}(x)=\{x^{\prime}\in\mathbb{R}^{d},\left\|x-x^{\prime}\right\|_{p}<\epsilon\}$。
- en: We further expand the model $f(.)$ as a function of its $k$ hidden layers and
    parameters $\theta$, where
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步扩展模型 $f(.)$ 作为其 $k$ 个隐藏层和参数 $\theta$ 的函数，其中
- en: '|  | $f(x)=f^{k}_{\theta}\circ f^{k-1}_{\theta}\circ\dots\circ f^{1}_{\theta}$
    |  | (4) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(x)=f^{k}_{\theta}\circ f^{k-1}_{\theta}\circ\dots\circ f^{1}_{\theta}$
    |  | (4) |'
- en: 'in which $f^{i}_{\theta}:\mathbb{R}^{d_{i-1}}\rightarrow\mathbb{R}^{d_{i}}$
    denotes the nonlinear transformation applied in hidden layer $i$. The objective
    is to prove a property on the output of the neural network, encoded via a linear
    constraint:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f^{i}_{\theta}:\mathbb{R}^{d_{i-1}}\rightarrow\mathbb{R}^{d_{i}}$ 表示在隐藏层
    $i$ 中应用的非线性变换。目标是证明神经网络输出上的一个属性，通过线性约束进行编码：
- en: '|  | $c^{T}f_{\theta}(x^{\prime})+d<0,\forall x^{\prime}\in\mathbb{S}_{0}(x)$
    |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $c^{T}f_{\theta}(x^{\prime})+d<0,\forall x^{\prime}\in\mathbb{S}_{0}(x)$
    |  |'
- en: in which $c$ and $d$ are property specific vector and scalar values.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c$ 和 $d$ 是特定属性的向量和标量值。
- en: 'To understand the complexity of the certification, based on [Equation 4](#S2.E4
    "4 ‣ 2.3.2 Certified Defenses ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial
    Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey"), we define the layer-wise adversarial optimization objective.
    For $z_{1}=x,z_{i+1}=f_{i}(W_{i}z_{i}+b_{i})$ :'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '为了理解认证的复杂性，基于[方程 4](#S2.E4 "4 ‣ 2.3.2 认证防御 ‣ 2.3 防御方法 ‣ 2 对抗攻击和防御的分类 ‣ 深度学习对抗鲁棒性的机遇与挑战：综述")，我们定义了逐层对抗优化目标。对于
    $z_{1}=x,z_{i+1}=f_{i}(W_{i}z_{i}+b_{i})$ :'
- en: '|  |  | $\displaystyle\underset{\displaystyle z_{1,\dots,d+1}}{\mathrm{max}}\quad(e_{y}-e_{y_{targ}})^{T}z_{d+1}\hfil\hfil\hfil\hfil$
    |  | (5) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\displaystyle z_{1,\dots,d+1}}{\mathrm{max}}\quad(e_{y}-e_{y_{targ}})^{T}z_{d+1}\hfil\hfil\hfil\hfil$
    |  | (5) |'
- en: '|  |  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{z_{1,\ldots,
    d+1}}}{\mathrm{max}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{max}><quot;)][l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle z_{1}^{\prime}\in\mathbb{S}_{0},\hfil\hfil$
    |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{z_{1,\ldots,
    d+1}}}{\mathrm{max}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{max}><quot;)][l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle z_{1}^{\prime}\in\mathbb{S}_{0},\hfil\hfil$
    |  |'
- en: '|  |  | $\displaystyle z_{i+1}=f_{i}(W_{i}z_{i}+b_{i}),~{}i=1,\dots,d-1,\hfil\hfil$
    |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle z_{i+1}=f_{i}(W_{i}z_{i}+b_{i}),~{}i=1,\dots,d-1,\hfil\hfil$
    |  |'
- en: '|  |  | $\displaystyle z_{d+1}=W_{d}z_{d}+b_{d}$ |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle z_{d+1}=W_{d}z_{d}+b_{d}$ |  |'
- en: in which $e_{i}$ unit basis, vectors with value 1 in the class $i^{th}$ position
    and zeros everywhere else. Such formulation requires special handling given that
    we have a nonlinear constraint defined by the activation function.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $e_{i}$ 单位基向量，在第 $i^{th}$ 位置的值为 1，其他位置为 0。由于我们有由激活函数定义的非线性约束，这种形式需要特别处理。
- en: 'Several techniques have been proposed to solve such a problem and they are
    within the scope of study on this survey. The method to train certified neural
    networks is based on the computation of an upper bound to the inner loss, as opposed
    to a lower bound computed for adversarial training. These methods are typically
    referred to as provable defenses as they provide guarantees on the robustness
    of the resulting network, under any kind of attack inside the threat model. Typical
    methods to compute the certifications are based on convex relaxations, interval
    propagation, SMT solvers, abstract interpretation, mixed-integer linear programs,
    linear relaxations, or combinations of these methods. We explore the diverse techniques
    in [subsection 4.3](#S4.SS3 "4.3 Certified Defenses ‣ 4 Defense Mechanisms based
    on Robust Optimization Against Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '为解决此类问题，已提出多种技术，这些技术在本调查的研究范围内。训练认证神经网络的方法基于内损失的上界计算，而不是对抗训练中计算的下界。这些方法通常被称为可证明的防御，因为它们在威胁模型内的任何攻击下提供了对结果网络的鲁棒性保证。计算认证的典型方法基于凸松弛、区间传播、SMT求解器、抽象解释、混合整数线性规划、线性松弛或这些方法的组合。我们在[小节
    4.3](#S4.SS3 "4.3 Certified Defenses ‣ 4 Defense Mechanisms based on Robust Optimization
    Against Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey")中探讨了各种技术。'
- en: 2.3.3 Regularization Approach
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 正则化方法
- en: Regularization techniques focus on making small modifications to the learning
    algorithm, such that it can generalize better. In a certain way, it improves the
    performance of the model in unseen data. It prevents model over-fitting to the
    noise data, by penalizing weight matrices of the nodes. In the specific case of
    Robust Optimization, the objective of regularization techniques is similar, but
    focusing on avoiding that small variation on the input, can generate changes in
    the decision of the algorithm. It does so by either expanding the decision boundaries
    or limiting changes in the gradient of the model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化技术侧重于对学习算法进行小幅修改，以便它能够更好地泛化。从某种程度上讲，它提高了模型在未见数据上的表现。通过惩罚节点的权重矩阵，它防止了模型对噪声数据的过拟合。在鲁棒优化的特定情况下，正则化技术的目标类似，但重点是避免输入的微小变化引起算法决策的改变。它通过扩展决策边界或限制模型梯度的变化来实现这一点。
- en: 'Many regularization techniques have been proposed with the most used being
    the $l_{p}$ based ones. The $l_{2}$ regularization technique is introduced to
    reduce the parameters value which translates to variance reduction. It introduces
    a penalty term to the original objective function (Loss), adding the weighted
    sum of the squared parameters of the model. With that, we have a regularized loss
    $\mathcal{L}_{R}$ defined as:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出了许多正则化技术，其中最常用的是基于 $l_{p}$ 的方法。$l_{2}$ 正则化技术旨在减少参数值，这相当于方差减少。它向原始目标函数（损失）引入一个惩罚项，添加了模型参数平方的加权和。因此，我们有一个正则化损失
    $\mathcal{L}_{R}$ 定义为：
- en: '|  | $\mathcal{L}_{R}(x+\delta,y)=\mathcal{L}(x+\delta,y)+\lambda\left\&#124;\theta\right\&#124;_{2}$
    |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{R}(x+\delta,y)=\mathcal{L}(x+\delta,y)+\lambda\left\&#124;\theta\right\&#124;_{2}$
    |  |'
- en: 'in which a small $\lambda$ lets the parameters to grow unchecked while a large
    $\lambda$ encourages the reduction of the model parameters. Regularization methods
    are not restricted to $L_{p}$ approaches and can involve Lipschitz Regularization,
    the Jacobian Matrix, and other techniques that we survey on [subsection 4.2](#S4.SS2
    "4.2 Regularization Techniques ‣ 4 Defense Mechanisms based on Robust Optimization
    Against Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey").'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '在这种情况下，小的 $\lambda$ 使参数不受限制地增长，而大的 $\lambda$ 则鼓励减少模型参数。正则化方法不限于 $L_{p}$ 方法，还可以涉及Lipschitz正则化、雅可比矩阵和其他技术，我们在[小节
    4.2](#S4.SS2 "4.2 Regularization Techniques ‣ 4 Defense Mechanisms based on Robust
    Optimization Against Adversarial Attacks ‣ Opportunities and Challenges in Deep
    Learning Adversarial Robustness: A Survey")中进行了调查。'
- en: 3 Methods for Generating Adversarial Attacks
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 生成对抗攻击的方法
- en: 'Studying adversarial attacks in the image classification domain improve our
    insights, as we can visually analyze the dissimilarities between disturbed and
    non-disturbed inputs. Moreover, the image data, even though high dimensional,
    are simpler represented than other domains such as audio, graphs, and cyber-security
    data. Along this section, we’ll revise the attack generating algorithms in the
    image classification domain which can be applied to standard deep neural networks
    (DNN) and convolutional neural networks (CNN). In which we classify in: white
    box, black box, and applied real world attacks. In [Table II](#S3.T2 "TABLE II
    ‣ 3.3.1 Other Attacks ‣ 3.3 Physical World Attack ‣ 3 Methods for Generating Adversarial
    Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness:
    A Survey") we summarize all the attacks described in this section, highlighting
    the distance metric used, information access level, algorithm type, and the domain
    it was applied in the specific publication.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '研究图像分类领域的对抗攻击可以改善我们的洞察，因为我们可以直观地分析扰动输入和非扰动输入之间的差异。此外，尽管图像数据维度较高，但其表示方式比音频、图形和网络安全数据等其他领域更简单。在本节中，我们将回顾图像分类领域的攻击生成算法，这些算法可以应用于标准深度神经网络（DNN）和卷积神经网络（CNN）。我们将其分类为：白盒攻击、黑盒攻击和实际应用攻击。在
    [表 II](#S3.T2 "TABLE II ‣ 3.3.1 Other Attacks ‣ 3.3 Physical World Attack ‣ 3
    Methods for Generating Adversarial Attacks ‣ Opportunities and Challenges in Deep
    Learning Adversarial Robustness: A Survey") 中，我们总结了本节描述的所有攻击，重点介绍了使用的距离度量、信息访问级别、算法类型以及在特定出版物中应用的领域。'
- en: '![Refer to caption](img/ff36cb2c619c8e6a57a6e4ec5cac6cb1.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ff36cb2c619c8e6a57a6e4ec5cac6cb1.png)'
- en: 'Figure 2: White box and black box attacks diverge mainly on the information
    the attacker have access to.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：白盒攻击和黑盒攻击主要在于攻击者可以访问的信息的不同。
- en: In the following sub-sections, we list and describe the most popular approaches
    for the adversarial attack in ML models. We list them in chronological order and
    focus on giving the most important details on these methods.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下子节中，我们列出了并描述了最流行的对机器学习模型的对抗攻击方法。我们按时间顺序列出它们，并重点介绍这些方法的最重要细节。
- en: 3.1 White-box Attacks
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 白盒攻击
- en: As stated, in white box attacks, there is no restriction in the level of information
    to which the attacker can access. As a consequence the adversary knows model parameters,
    dataset, or any other information regarding the model. Under such assumption,
    given a model $f(.)$, an input $(x,y)$, the main objective is to produce $x^{\prime}$,
    which is within certain distance from the original $x$ and maximizes the loss
    $\mathcal{L}(f(x+\delta),y)$.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如所述，在白盒攻击中，攻击者可以访问的信息级别没有限制。因此，敌手了解模型参数、数据集或任何其他关于模型的信息。在这种假设下，给定一个模型 $f(.)$
    和一个输入 $(x,y)$，主要目标是生成 $x^{\prime}$，使其与原始 $x$ 在一定距离内，并最大化损失 $\mathcal{L}(f(x+\delta),y)$。
- en: '|  | $\max_{\delta\in\Delta}\mathcal{L}(f(x+\delta),y)$ |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\delta\in\Delta}\mathcal{L}(f(x+\delta),y)$ |  |'
- en: 3.1.1 Box-Constrained L-BFGS
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 盒约束 L-BFGS
- en: 'In, [[43](#bib.bib43)], the existence of small perturbations capable of misleading
    a classifier were first demonstrated. In the paper, Szegedy et. al. proposed to
    compute an additive noise $\delta$, which could be added to the original input
    $x$, capable of misleading the classifier but with minimal or no perceptible distortion
    to the image. We find the optimal delta, $\delta$, with:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[43](#bib.bib43)] 中，首次展示了能够误导分类器的小扰动的存在。在论文中，Szegedy 等人提出计算一个可添加到原始输入 $x$
    的加性噪声 $\delta$，该噪声能够误导分类器，但对图像的失真最小或没有可感知的失真。我们通过以下方式找到最佳的 delta $\delta$：
- en: '|  |  | $\displaystyle\underset{\displaystyle\delta}{\mathrm{min}}\quad c\left\&#124;\delta\right\&#124;_{2}\hfil\hfil\hfil\hfil$
    |  | (6) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\displaystyle\delta}{\mathrm{min}}\quad c\left\&#124;\delta\right\&#124;_{2}\hfil\hfil\hfil\hfil$
    |  | (6) |'
- en: '|  |  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{min}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{min}><quot;)][l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle f(x+\delta)=y^{\prime},\hfil\hfil$
    |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathmakebox[width("&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{min}}<quot;)[c]{\mathmakebox[width("&quot;$\mathrm{min}<quot;)[l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle f(x+\delta)=y^{\prime},\hfil\hfil$
    |  |'
- en: '|  |  | $\displaystyle\text{ all pixel in }(x+\delta)\in[0,1]$ |  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\text{ 所有像素在 }(x+\delta)\in[0,1]$ |  |'
- en: 'in which $f(.)$ is the parameterized DNN model, $y$ is the true label, $y^{\prime}$
    is the target label. As is this is a hard problem. Using a box constrained L-BFGS
    the authors proposed an approximate solution to the problem stated as:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$f(.)$是参数化的DNN模型，$y$是实际标签，$y^{\prime}$是目标标签。这是一个困难的问题。作者使用了带盒约束的L-BFGS，提出了问题的近似解，表示为：
- en: '|  |  | $\displaystyle\underset{\displaystyle\delta}{\mathrm{min}}\quad c\left\&#124;\delta\right\&#124;_{2}+\mathcal{L}(f(x+\delta),y^{\prime})\hfil\hfil\hfil\hfil$
    |  | (7) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\displaystyle\delta}{\mathrm{min}}\quad c\left\&#124;\delta\right\&#124;_{2}+\mathcal{L}(f(x+\delta),y^{\prime})\hfil\hfil\hfil\hfil$
    |  | (7) |'
- en: '|  |  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{min}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{min}><quot;)][l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle\text{ all pixel in }(x+\delta)\in[0,1]$
    |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{min}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{min}><quot;)][l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle\text{ all pixel in }(x+\delta)\in[0,1]$
    |  |'
- en: In this model, Szegedy et. al., managed to generate images that were visually
    indistinguishable from the original ones, but were able to fool the classifiers
    into classifying them as another class. This was the first result and publication
    which has exploited this weakness of deep learning models.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，Szegedy等人成功生成了在视觉上与原始图像无法区分的图像，但却能欺骗分类器将其分类为另一个类别。这是第一个利用深度学习模型弱点的结果和发布。
- en: 3.1.2 Fast Sign Gradient Method
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 Fast Sign Gradient Method
- en: 'In , [[56](#bib.bib56)], introduced a one step adversarial attack framework.
    The attack image, $x^{\prime}$, is obtained by a simple additive disturb:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[56](#bib.bib56)]中，引入了一个单步对抗攻击框架。攻击图像$x^{\prime}$是通过简单的加性扰动获得的：
- en: '|  | $\displaystyle x^{\prime}=$ | $\displaystyle~{}x+\delta_{ut}$ |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle x^{\prime}=$ | $\displaystyle~{}x+\delta_{ut}$ |  |'
- en: '|  | $\displaystyle x^{\prime}=$ | $\displaystyle~{}x-\delta_{tg}$ |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle x^{\prime}=$ | $\displaystyle~{}x-\delta_{tg}$ |  |'
- en: 'in which for the untargeted setting, $\delta_{ut}$, we obtain the perturbation
    from:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中对于非目标设置，$\delta_{ut}$，我们从以下公式中获得扰动：
- en: '|  | $\max_{\left\&#124;\delta_{ut}\right\&#124;_{p}\leq\epsilon}\mathcal{L}(f(x+\delta),y)$
    |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\left\&#124;\delta_{ut}\right\&#124;_{p}\leq\epsilon}\mathcal{L}(f(x+\delta),y)$
    |  |'
- en: 'and in the the targeted setting, $\delta_{tg}$, from:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标设置中，$\delta_{tg}$，来自：
- en: '|  | $\max_{\left\&#124;\delta_{tg}\right\&#124;_{p}\leq\epsilon}(\mathcal{L}(f(x+\delta),y)-\mathcal{L}(f(x+\delta),y^{\prime}))$
    |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\left\&#124;\delta_{tg}\right\&#124;_{p}\leq\epsilon}(\mathcal{L}(f(x+\delta),y)-\mathcal{L}(f(x+\delta),y^{\prime}))$
    |  |'
- en: in which $\epsilon$ is the ball defined normally by an $l_{p}$-norm.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\epsilon$是由$l_{p}$-范数定义的球体。
- en: '![Refer to caption](img/64f9d014af7879c321f4b14deb339a7b.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/64f9d014af7879c321f4b14deb339a7b.png)'
- en: 'Figure 3: By adding an unoticeble perturbation the dog, previously classified
    as from the breed redbone, is classified as a Hand blower, with high confidence.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：通过添加一个不易察觉的扰动，之前被分类为红骨犬的狗被以高度置信度分类为吹风机。
- en: 'The core of fast sign gradient method maximize the norm of the vector between
    originally labeled class and the currently assigned label, while in the targeted
    setting, it focus on minimizing the distance to the target class. As it is a one-step
    algorithm, it is not very resilient to current defenses but is very fast implementation.
    [Figure 3](#S3.F3 "Figure 3 ‣ 3.1.2 Fast Sign Gradient Method ‣ 3.1 White-box
    Attacks ‣ 3 Methods for Generating Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey") shows the attack of an image
    and the false prediction.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 快速符号梯度方法的核心是最大化原始标记类别和当前分配标签之间的向量范数，而在目标设置中，它则专注于最小化到目标类别的距离。由于这是一个单步算法，它对当前防御措施不够稳健，但实现速度非常快。[图3](#S3.F3
    "图3 ‣ 3.1.2 快速符号梯度方法 ‣ 3.1 白盒攻击 ‣ 3 生成对抗攻击的方法 ‣ 深度学习对抗鲁棒性中的机会与挑战：调查")展示了图像攻击和错误预测。
- en: 3.1.3 DeepFool
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 DeepFool
- en: The Deepfool attack proposed by [[57](#bib.bib57)], is a white box attack which
    explores the boundaries of the classification model. In the multi-class algorithm,
    Deepfool initializes with an input $x$ which is assumed to be within the boundaries
    of the classifier model $f(x)$. With an iterative process, the image is perturbed
    by a small vector towards the direction of the decision boundaries. The boundaries
    are approximated by linear functions, more specifically a hyperplane, defined
    in the algorithm as $\hat{l}$. At each step, the perturbations are accumulated
    to form the final perturbation to the image. With smaller perturbations than in
    FGSM ([[56](#bib.bib56)]), the authors have shown similar or better attack success
    rate.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[[57](#bib.bib57)] 提出的 Deepfool 攻击是一种白盒攻击，探索分类模型的边界。在多类算法中，Deepfool 以输入 $x$
    初始化，假设其在分类器模型 $f(x)$ 的边界内。通过迭代过程，图像会受到一个小向量的扰动，朝着决策边界的方向移动。这些边界被线性函数，即算法中定义的超平面
    $\hat{l}$ 所近似。在每一步中，扰动会累积以形成最终的图像扰动。与 FGSM ([[56](#bib.bib56)]) 相比，作者显示出类似或更好的攻击成功率。'
- en: 3.1.4 Jacobian-based Saliency Map Attack
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 基于雅可比的显著性图攻击
- en: The Jacobian-based Saliency Map attack (JSMA) differs from most of the adversarial
    attack literature with respect to the norm it uses on the perturbation restriction.
    While most of the attacks focus on the $l_{\infty}$ or $l_{2}$ norms, JSMA, proposed
    in [[58](#bib.bib58)], focus on the $l_{0}$ norm. Under this norm, penalizes the
    change in a binary way, if the pixel has been changed or not, opposed to $l_{2}$
    based algorithms which takes into consideration the size of the change in the
    pixels.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 基于雅可比的显著性图攻击（JSMA）与大多数对抗攻击文献在扰动限制所使用的范数方面有所不同。虽然大多数攻击集中在 $l_{\infty}$ 或 $l_{2}$
    范数上，JSMA 在 [[58](#bib.bib58)] 中提出，专注于 $l_{0}$ 范数。在这种范数下，以二进制方式惩罚像素是否发生了变化，而不是像
    $l_{2}$ 基于算法那样考虑像素变化的大小。
- en: 'In this attack, Papernot et. al., calculates the Jacobian of a score matrix
    $F$. The model executes the attack in a greedy way. It modifies the pixel which
    has the highest impact on the model’s decision. The Jacobian Matrix is defined
    as:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种攻击中，Papernot 等人计算了评分矩阵 $F$ 的雅可比矩阵。模型以贪婪的方式执行攻击。它修改对模型决策影响最大的像素。雅可比矩阵定义为：
- en: '|  | $J_{F}(x)=\frac{\partial F(x)}{\partial(x)}=\{\frac{\partial F_{j}(x)}{\partial
    x_{i}}\}_{x\times j}$ |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | $J_{F}(x)=\frac{\partial F(x)}{\partial(x)}=\{\frac{\partial F_{j}(x)}{\partial
    x_{i}}\}_{x\times j}$ |  |'
- en: it models the influence of changes in the input $x$ to the predicted label $\hat{y}$.
    One at a time pixels from the unperturbed image are modified by the algorithm
    in order to create a salience map. The main idea of salience map is the correlation
    between the gradient of the output and the input. It is a guide to the most influential
    variables of the input, or the ones that probably can deceive the classifier with
    less manipulation. Based on that, the algorithm performs modifications in the
    most influential pixel.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 它模拟输入 $x$ 对预测标签 $\hat{y}$ 的影响。算法逐次修改未扰动图像中的像素，以创建显著性图。显著性图的主要思想是输出梯度与输入之间的相关性。这是对输入最具影响力的变量的指导，或者是那些可能通过较少的操控欺骗分类器的变量。基于此，算法在最具影响力的像素中进行修改。
- en: 3.1.5 Projected Gradient Descend (PGD)
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5 投影梯度下降（PGD）
- en: 'Also known as the basic iterative method, was initially proposed in [[59](#bib.bib59)].
    It is based on the FGSM, but instead a single step of the projected gradient descend,
    it iterates through more steps, as in:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为基本迭代方法，最初在 [[59](#bib.bib59)] 中提出。它基于 FGSM，但不同于单步的投影梯度下降，它通过更多步骤进行迭代，如下所示：
- en: '|  | $\displaystyle\delta:=P(\delta+\alpha\nabla_{\delta}\mathcal{L}(f(x+\delta),y))$
    |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\delta:=P(\delta+\alpha\nabla_{\delta}\mathcal{L}(f(x+\delta),y))$
    |  |'
- en: in which $P$ denotes the projection over the ball of interest. With such formulation,
    the PGD, requires more fine-tuning, in choosing the step size $\alpha$. In [[54](#bib.bib54)],
    Madry et. al. proposed an iterative method with a random initialization for $\delta$.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P$ 表示对感兴趣区域的投影。使用这种公式，PGD 需要更多的微调，以选择步长 $\alpha$。在 [[54](#bib.bib54)] 中，Madry
    等人提出了一种随机初始化 $\delta$ 的迭代方法。
- en: 3.1.6 Carlini and Wagner Attacks (CW)
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.6 Carlini 和 Wagner 攻击（CW）
- en: 'In [[60](#bib.bib60)], 3 $l_{p}$-norm attacks ($l_{0}$,$l_{2}$,$l_{\infty}$)
    were proposed as a response to [[61](#bib.bib61)], which proposed the use of Distillation
    as a defense strategy. In their paper, Papernot et. al., successfully presented
    a defense mechanism capable of reducing the effectiveness of the FGSM and L-BFGS.
    CW proposes to solve the same problem stated in FGSM, which is given an input
    $x$ find a minimal perturbation $\delta$ capable of shifting the classification
    prediction of the model. The problem is addressed as:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[60](#bib.bib60)]中，提出了3种 $l_{p}$-范数攻击（$l_{0}$，$l_{2}$，$l_{\infty}$）作为对[[61](#bib.bib61)]提出的使用蒸馏作为防御策略的回应。在他们的论文中，Papernot
    等人成功提出了一种防御机制，能够降低 FGSM 和 L-BFGS 的有效性。CW 提出了解决 FGSM 中陈述的相同问题，即给定输入 $x$，寻找最小扰动
    $\delta$，使模型的分类预测发生变化。这个问题可以表述为：
- en: '|  |  | $\displaystyle\underset{\displaystyle\delta}{\mathrm{min}}\quad c\left\&#124;\delta\right\&#124;_{p}+\mathcal{L}(f(x+\delta),y^{\prime})\hfil\hfil\hfil\hfil$
    |  | (8) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\displaystyle\delta}{\mathrm{min}}\quad c\left\&#124;\delta\right\&#124;_{p}+\mathcal{L}(f(x+\delta),y^{\prime})\hfil\hfil\hfil\hfil$
    |  | (8) |'
- en: '|  |  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{min}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{min}><quot;)][l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle(x+\delta)\in[0,1]^{n}$ |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{min}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{min}><quot;)][l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle(x+\delta)\in[0,1]^{n}$ |  |'
- en: in which $\mathcal{L}(f(x+\delta),y^{\prime})=\max_{i\neq y^{\prime}}(Z(x^{\prime})_{i})-Z(x^{\prime})_{y})^{+}$,
    and $Z(x)=z$ are the logits. As the algorithm minimizes the metrics $\mathcal{L}(.)$,
    it finds the input $x^{\prime}$ that has larger score to be classified as $y^{\prime}$.
    As we search for the value of $c$, we look for the constant which will produce
    the smaller distance between $x$ and $x^{\prime}$.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}(f(x+\delta),y^{\prime})=\max_{i\neq y^{\prime}}(Z(x^{\prime})_{i})-Z(x^{\prime})_{y})^{+}$，$Z(x)=z$
    是 logits。算法通过最小化度量 $\mathcal{L}(.)$，找到得分较大的 $x^{\prime}$，使其被分类为 $y^{\prime}$。当我们寻找常数
    $c$ 时，我们寻找的是产生 $x$ 和 $x^{\prime}$ 之间较小距离的常数。
- en: 3.1.7 Ground Truth Adversarial Example (GTAE)
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.7 真实对抗样本（GTAE）
- en: So far most of the attacks, even if motivated by the generation of new defenses,
    are independent of the defense algorithm. In the algorithm proposed by [[62](#bib.bib62)],
    the certified defense proposed in [[63](#bib.bib63)], is used as a base for the
    optimization and search for adversarial examples.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，大多数攻击，即使是受到生成新防御的激励，也与防御算法无关。在[[62](#bib.bib62)]中提出的算法中，作为优化和寻找对抗样本基础的认证防御是[[63](#bib.bib63)]中提出的。
- en: The algorithm abstract the $\theta$ and dataset $(x,y)$ with the use of an SMT
    solver, and solves the system to check if there exist $x^{\prime}$ near $x$, within
    the established norm distance, which can cause a misclassification. The ground
    truth adversarial example is found by reducing the size of $\epsilon$ up to the
    point that the system can no longer find a suitable $x^{\prime}$. The adversarial
    example is considered the $x^{\prime}$ found with the immediately larger $\epsilon$.
    It is the first method to calculate an exact provable solution to a minimal perturbation
    which can fool ML models. In contrast, as stated by the authors, the fact that
    the model relies on an SMT solver, restrict the applicability of the algorithm
    to models with no more than a few hundred nodes. This attack has been revisited
    by [[64](#bib.bib64)] and [[65](#bib.bib65)].
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 算法利用 SMT 解算器抽象出 $\theta$ 和数据集 $(x,y)$，并求解系统以检查是否存在距离 $x$ 近的 $x^{\prime}$，在已建立的范数距离内，可能导致误分类。通过将
    $\epsilon$ 的大小减小到系统无法找到合适的 $x^{\prime}$ 为止，从而找到真实的对抗样本。对抗样本被认为是找到的具有立即更大 $\epsilon$
    的 $x^{\prime}$。这是第一个计算可证明的最小扰动的精确方法，可以欺骗机器学习模型。与此相对的是，正如作者所述，由于模型依赖于 SMT 解算器，这限制了算法在节点数不超过几百的模型上的适用性。这种攻击已被[[64](#bib.bib64)]和[[65](#bib.bib65)]重新审视。
- en: 3.1.8 Universal Adversarial Perturbations
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.8 通用对抗扰动
- en: 'Different from the previous methods, the universal adversarial perturbation
    (UAP), proposed in [[66](#bib.bib66)], search for a single perturbation capable
    of fooling all samples from the training dataset. The perturbations, independent
    of the input, are also restricted to not be detected by humans. The perturbations
    are constructed based on:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的方法不同，通用对抗扰动（UAP），如[[66](#bib.bib66)]中提出的那样，寻找一个能够欺骗训练数据集中所有样本的单一扰动。这些扰动与输入无关，还受到限制，以免被人眼检测到。扰动的构造基于：
- en: '|  | $P_{x\sim\mathcal{D}}(f(x)\neq f(x+\delta))\geq\beta,\text{s. t. }\left\&#124;\delta\right\&#124;_{p}\leq\epsilon$
    |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $P_{x\sim\mathcal{D}}(f(x)\neq f(x+\delta))\geq\beta,\text{s. t. }\left\&#124;\delta\right\&#124;_{p}\leq\epsilon$
    |  |'
- en: in which $\epsilon$ defines the size of the perturbation based on an $l_{p}$-norm
    and $\beta$ defines the probability of an image sampled from the training dataset
    being fooled by the generated perturbation. In this case, the algorithm optimizes
    the probability of fooling the classifier.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中，$\epsilon$ 定义了基于 $l_{p}$-范数的扰动大小，$\beta$ 定义了从训练数据集中采样的图像被生成的扰动欺骗的概率。在这种情况下，算法优化了欺骗分类器的概率。
- en: The method to calculate the universal perturbations is based on the DeepFool
    algorithm, in which the input is gradually pushed towards the model’s decision
    boundary. It differs from DeepFool in the fact that instead of pushing a single
    input, all members of $\mathcal{D}$ are modified in the direction of the decision
    boundary. The perturbations, calculated for each image, are accumulated in a gradual
    manner. The accumulator is then projected back towards the specified $B_{\epsilon}$
    ball, of radius $\epsilon$. Its been shown that with variations of $4\%$, a fooling
    accuracy of $80\%$ has been achieved.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 计算通用扰动的方法基于 DeepFool 算法，该算法逐渐推动输入靠近模型的决策边界。与 DeepFool 的不同之处在于，它不是推动单个输入，而是将
    $\mathcal{D}$ 的所有成员都朝决策边界的方向进行修改。为每个图像计算出的扰动是逐渐积累的。然后将累积值投影回指定的 $B_{\epsilon}$
    球体，半径为 $\epsilon$。研究表明，通过 $4\%$ 的变化，已经实现了 $80\%$ 的欺骗准确率。
- en: 3.1.9 Shadow Attack
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.9 阴影攻击
- en: 'In [[67](#bib.bib67)], an attack targeting certified defenses was proposed.
    In their work, they target defenses that certify the model with respect to a radius
    defined by the $l_{p}$-norm. One intuitive idea to construct a certified defense
    is to check within a certain radius $B_{\epsilon}$ of input, the existence of
    a perturbation $\delta$, capable of changing the decision of the classifier. The
    shadow attack is constructed to leverage this premise, and construct a perturbation
    outside of the certification zone. It is claimed that after labeling an image,
    these defenses check whether there exists an image of a different label within
    $\epsilon$ distance (in $l_{p}$ metric) of the input, where $\epsilon$ is a security
    parameter selected by the user. If within the $B_{\epsilon}$ ball all inputs are
    classified with the same label, then the model is robustly certified. Their model
    targets not only the classifier but also the certificate. It is done by adding
    adversarial perturbations to images that are large in the $l_{p}$-norm and produce
    attack images that are surrounded by a large ball exclusively containing images
    of the same label. In order to produce images that are close to the original,
    in a perception way, but can fool the classifier, they use the following objective
    function:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[67](#bib.bib67)] 中，提出了一种针对认证防御的攻击。在他们的工作中，他们针对那些依据 $l_{p}$-范数定义的半径对模型进行认证的防御方法。一种直观的构建认证防御的想法是检查输入的某个半径
    $B_{\epsilon}$ 内是否存在扰动 $\delta$，能够改变分类器的决策。阴影攻击旨在利用这一前提，构造一个位于认证区域之外的扰动。据称，在标记图像后，这些防御会检查是否存在一个不同标签的图像在
    $\epsilon$ 距离（在 $l_{p}$ 度量下）内，其中 $\epsilon$ 是用户选择的安全参数。如果在 $B_{\epsilon}$ 球体内所有输入都被分类为相同的标签，则模型被认为是稳健认证的。他们的模型不仅针对分类器，还针对证书。这是通过将对图像的对抗扰动添加到
    $l_{p}$-范数较大的图像中，从而产生被大量同标签图像包围的攻击图像。为了生成与原始图像在感知上相近，但能欺骗分类器的图像，他们使用以下目标函数：
- en: '|  |  | $\displaystyle\underset{\displaystyle y^{\prime}\neq y,~{}\delta}{\mathrm{max}}\quad$
    | $\displaystyle-\mathcal{L}(\theta,x+\delta&#124;y^{\prime})-\lambda_{c}C(\delta)\hfil\hfil\hfil\hfil$
    |  | (9) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\underset{\displaystyle y^{\prime}\neq y,~{}\delta}{\mathrm{max}}\quad$
    | $\displaystyle-\mathcal{L}(\theta,x+\delta&#124;y^{\prime})-\lambda_{c}C(\delta)\hfil\hfil\hfil\hfil$
    |  | (9) |'
- en: '|  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{y''
    \neq y,~{}\delta}}{\mathrm{max}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{max}><quot;)][l]{}}\quad-\lambda_{tv}TV(\delta)-\lambda_{s}Dissim(\delta)\hfil\hfil\hfil\hfil$
    |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{y''
    \neq y,~{}\delta}}{\mathrm{max}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{max}><quot;)][l]{}}\quad-\lambda_{tv}TV(\delta)-\lambda_{s}Dissim(\delta)\hfil\hfil\hfil\hfil$
    |  |'
- en: in which $\mathcal{L}(\theta,x+\delta|\bar{y})$ refers to the adversarial training
    loss, $\lambda_{c}C(\delta)$ is a color regularization term, $\lambda_{tv}TV(\delta)$
    is a smoothness penalty term, and $\lambda_{s}Dissim(\delta)$ guarantees that
    all color channels receive similar perturbation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}(\theta,x+\delta|\bar{y})$ 指的是对抗训练损失，$\lambda_{c}C(\delta)$ 是颜色正则化项，$\lambda_{tv}TV(\delta)$
    是平滑惩罚项，$\lambda_{s}Dissim(\delta)$ 确保所有颜色通道接收到相似的扰动。
- en: 3.1.10 Other Attacks
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.10 其他攻击
- en: The presented attacks are just part of many more which have been published in
    many different venues. Here we list some other relevant attack methods available
    in the literature.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 所展示的攻击方法只是许多已在不同场所发表的攻击中的一部分。这里列出了一些文献中相关的攻击方法。
- en: •
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'EAD: Elastic-net attack - Similar to L-BFGS the algorithm in [[68](#bib.bib68)]
    proposes to find the minimum additive perturbation, which misleads the classifier.
    Differently it incorporates an association of the norms $l_{1}$ and $l_{2}$. It
    has been shown that strong defenses against $l_{\infty}$ and $l_{2}$ norms still
    fail to reject $l_{1}$ based attacks.'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'EAD: 弹性网攻击 - 类似于L-BFGS，[[68](#bib.bib68)]中的算法提出寻找最小的附加扰动，这种扰动会误导分类器。不同之处在于它结合了
    $l_{1}$ 和 $l_{2}$ 的范数。已经证明，针对 $l_{\infty}$ 和 $l_{2}$ 范数的强大防御仍然无法拒绝基于 $l_{1}$ 的攻击。'
- en: •
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Objective Metrics and Gradient Descend Algorithm (OMGDA) - The algorithm proposed
    by [[69](#bib.bib69)], is very similar to DeepFool, with the optimization of the
    step size. Instead of utilizing a fixed and heuristically determined step size
    in the optimization, in Jang et al., the step size utilizes insights from the
    softmax layer. The step size is determined based on the size of the desired perturbation
    and varies over time.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标度量和梯度下降算法（OMGDA） - [[69](#bib.bib69)]提出的算法非常类似于DeepFool，通过优化步长。与Jang等人使用的固定和启发式确定的步长不同，步长利用了softmax层的见解。步长基于所需扰动的大小确定，并随着时间变化。
- en: •
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Spatially Transformed Attack (STA) - In [[70](#bib.bib70)], instead of generating
    changes in the intensity of the pixels, the authors have proposed a method based
    on small translational and rotational perturbations. The perturbations are still
    not noticeable by the human eyes. Similarly in [[71](#bib.bib71)] the spatial
    aspect of the input is also exploited for the adversarial example generation.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 空间变换攻击（STA） - 在[[70](#bib.bib70)]中，作者提出了一种基于小的平移和旋转扰动的方法，而不是生成像素强度的变化。这些扰动仍然不会被人眼察觉。类似地，在[[71](#bib.bib71)]中，输入的空间方面也被利用于对抗样本的生成。
- en: •
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Unrestricted Adversarial Examples with Generative Models (UAEGM) - Based on
    AC-GAN ([[72](#bib.bib72)]), [[73](#bib.bib73)], has proposed the use of generative
    networks to generate examples which are not restricted to being in the neighborhood
    of the input data. The generated attacks, are not necessarily similar to the ones
    in the dataset but are similar enough to humans not notice and fool the classifiers.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用生成模型的无限制对抗样本（UAEGM） - 基于AC-GAN（[[72](#bib.bib72)]，[ [73](#bib.bib73)]），提出使用生成网络生成不受输入数据邻域限制的样本。生成的攻击不一定与数据集中样本类似，但足够类似以至于人眼无法察觉，并能欺骗分类器。
- en: 3.2 Black-Box Attacks
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 黑箱攻击
- en: 'Under Black box restriction, models are different from the currently exposed
    white box, with respect to the information the attacker has access to. In most
    cases, the adversary does not have any or some information about the targeted
    model, like the algorithm used, dataset, or parameters, as seen in [Figure 2](#S3.F2
    "Figure 2 ‣ 3 Methods for Generating Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey"). An important modeling challenge
    for black-box attacks is to model precisely what information the attacker has
    about either the learned model or the algorithm. In this sub-section, we list
    the most relevant methods for black attack generation.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '在黑箱限制下，模型与当前暴露的白箱不同，攻击者访问的信息也不同。在大多数情况下，攻击者对目标模型没有或仅有部分信息，如所用算法、数据集或参数，如[图2](#S3.F2
    "Figure 2 ‣ 3 Methods for Generating Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey")所示。黑箱攻击中的一个重要建模挑战是准确建模攻击者对学习模型或算法的了解程度。在本小节中，我们列出了最相关的黑箱攻击生成方法。'
- en: 3.2.1 Practical Black-Box Attacks (PBBA)
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 实用黑箱攻击（PBBA）
- en: While assuming access to the information of the model enables a series of attacks,
    the work in [[74](#bib.bib74)], introduces the possibility of attacking models
    about which the attacker has less knowledge. In this work, no knowledge about
    the architecture is assumed, only some idea of the domain of interest. It is also
    limited the requests for output sent to the model, which requires the attacker
    to choose wisely the inference requests from the victim’s model. To achieve the
    goal, Papernot et. al. introduce the substitute model framework. The attack strategy
    is to train a substitute network on a small number of initial queries, then iteratively
    perturb inputs based on the substitute network’s gradient information to augment
    the training set.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然假设访问模型信息可以实现一系列攻击，但 [[74](#bib.bib74)] 中的工作引入了攻击对攻击者了解较少的模型的可能性。在这项工作中，假设没有关于模型架构的知识，仅对感兴趣的领域有一些了解。还限制了发送给模型的输出请求，这要求攻击者明智地选择来自受害者模型的推理请求。为了实现目标，Papernot
    等人引入了替代模型框架。攻击策略是在少量初始查询上训练一个替代网络，然后基于替代网络的梯度信息迭代扰动输入，以扩展训练集。
- en: '1:Input: Substitute dataset $S_{0}$.2:Input: Substitute model architecture
    $F$.3:while $\rho\leq$ epochs do4:     Label $S_{0}$ based on queries from original
    model5:     Train substitute model based on (3).6:     Augment dataset based on
    Jacobian, $S_{\rho}\leftarrow S_{\rho+1}$7:return'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入: 替代数据集 $S_{0}$。2: 输入: 替代模型架构 $F$。3: 当 $\rho\leq$ epochs 时4:     根据原始模型的查询标记
    $S_{0}$5:     基于 (3) 训练替代模型6:     基于雅可比矩阵扩展数据集，$S_{\rho}\leftarrow S_{\rho+1}$7:
    返回'
- en: Algorithm 1 Substitute Model
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 替代模型
- en: With the boundaries of substitute model adjusted to be close to the original
    model, any of the methods presented in the previous section can be used to generate
    the perturbed image.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在将替代模型的边界调整到接近原始模型的情况下，可以使用上一节中介绍的任何方法来生成扰动图像。
- en: 3.2.2 Zeroth Order Optimization Based Attack
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 零阶优化基攻击
- en: In Chen et al. [[52](#bib.bib52)], in the attack also known as ZOO, the authors
    assume accessibility to both, the input data and the confidence scores of the
    model in which they target their attack. They differ from [[74](#bib.bib74)] in
    the fact that the model does not focus on transferability (creating a substitute
    model) to achieve the adversarial examples. In their work, they propose a zeroth-order
    optimization attack which estimates the gradient of the targeted DNN. Instead
    of traditional gradient descend, they use order 0 coordinate SGD. Moreover, to
    improve their model and enable the adversarial example generation, they implement
    dimensionality reduction, hierarchical techniques, and importance sampling. As
    the pixels are tuned, the algorithm observes the changes in the confidence scores.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Chen 等人 [[52](#bib.bib52)] 的研究中，在被称为 ZOO 的攻击中，作者假设可以访问输入数据和模型的置信度分数，并以此为目标进行攻击。他们与
    [[74](#bib.bib74)] 的不同之处在于，模型不专注于转移性（创建替代模型）来生成对抗样本。在他们的工作中，他们提出了一种零阶优化攻击，估计目标
    DNN 的梯度。与传统的梯度下降方法不同，他们使用零阶坐标 SGD。此外，为了改进他们的模型并生成对抗样本，他们实施了降维、层次技术和重要性采样。随着像素的调整，算法观察到置信度分数的变化。
- en: Similar to the ZOO, in the one-pixel attack [[75](#bib.bib75)], it is proposed
    the use of the score confidence to perturb the input and change the decision of
    the classifier. This paper focuses on modifying a single pixel of the input. With
    the use of differential evolution, the single pixel is modified in a black-box
    setting. The authors base the update of the perturbation in the variation of the
    probability scores for each class.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 ZOO，在一像素攻击 [[75](#bib.bib75)] 中，提出了使用置信度分数来扰动输入并改变分类器的决策。本文重点在于修改输入的单个像素。利用差分进化算法，在黑箱设置下对单个像素进行修改。作者基于每个类别的概率分数变化来更新扰动。
- en: 3.2.3 Query-Efficient Black-Box Attacks
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 查询高效的黑箱攻击
- en: One of the biggest challenges in black-box attacks is the fact that many inference
    models have mechanisms to restrict the number of queries (when cloud-based or
    system embedded), or the inference time can restrict the number of queries. One
    line of research in black-box models looks into making such models more query
    efficient, for example, the work from [[76](#bib.bib76)], based on natural evolution
    strategies reduces by 2 or 3 order of magnitude the amount of information requests
    sent to the model to successfully generate a misclassified perturbed image. The
    algorithm set queries in the neighborhood of the input $x$. The output of the
    model is then sampled, and these samples are used to estimate the expectation
    of the gradient around the point of interest.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒攻击中最大的挑战之一是许多推理模型都有机制限制查询次数（在基于云的或系统嵌入的情况下），或者推理时间可能限制查询次数。 黑盒模型的一种研究方向是使这些模型更加高效，例如，基于自然进化策略的工作[[76](#bib.bib76)]能够将成功生成误分类扰动图像的信息请求量减少2到3个数量级。
    该算法在输入$x$的邻域内设置查询。 然后对模型的输出进行采样，并使用这些样本来估计感兴趣点周围的梯度期望。
- en: 'The algorithm sample the model’s output based on the queries around the input
    $x$, and estimate the expectation of a gradient of $F$ on $x$. More on the topic,
    [[77](#bib.bib77)], proposes a family of algorithms based on a new gradient direction
    estimate using only the binary classification of the model. In their work, it
    is included $l_{\infty}$ and $l_{2}$ norm-based attacks as well as targeted and
    untargeted attacks. [Figure 4](#S3.F4 "Figure 4 ‣ 3.2.3 Query-Efficient Black-Box
    Attacks ‣ 3.2 Black-Box Attacks ‣ 3 Methods for Generating Adversarial Attacks
    ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey")
    shows an intuition on how the gradient is updated and the boundaries of the decision
    are used to generate the adversarial attack. Algorithm [2](#alg2 "Algorithm 2
    ‣ 3.2.3 Query-Efficient Black-Box Attacks ‣ 3.2 Black-Box Attacks ‣ 3 Methods
    for Generating Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey") shows how the dimensionality reduction $d^{r}$
    is defined.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法基于围绕输入$x$的查询对模型的输出进行采样，并估计了期望的梯度$F$在$x$上的情况。 此外，[[77](#bib.bib77)]提出了一种基于模型的二元分类的新梯度方向估计的算法族。
    在他们的工作中，包括基于$l_{\infty}$和$l_{2}$范数的攻击，以及有针对性和无针对性的攻击。 [图 4](#S3.F4 "图 4 ‣ 3.2.3
    查询高效的黑盒攻击 ‣ 3.2 黑盒攻击 ‣ 3 生成对抗攻击的方法 ‣ 深度学习对抗性鲁棒性的机遇与挑战：综述") 显示了梯度如何更新以及决策边界如何用于生成对抗攻击的直觉。
    算法[2](#alg2 "算法 2 ‣ 3.2.3 查询高效的黑盒攻击 ‣ 3.2 黑盒攻击 ‣ 3 生成对抗攻击的方法 ‣ 深度学习对抗性鲁棒性的机遇与挑战：综述")
    显示了降维 $d^{r}$ 的定义。
- en: '![Refer to caption](img/50a2f96da378bcc750491ef0813aff99.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/50a2f96da378bcc750491ef0813aff99.png)'
- en: 'Figure 4: The HopSkipJumpAttack. (a) With binary search find the boundary of
    the decision. (b) Generate an estimate of the gradient near the decision limit.
    (c) Update the decision limit point with the use of geometric progression. (d)
    Do a binary search and update the estimate of the boundary point. Image Source:
    [[77](#bib.bib77)]'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：HopSkipJump攻击。 (a) 用二分法找到决策边界。 (b) 生成决策边界附近的梯度估计。 (c) 使用几何级数更新决策边界点。 (d)
    进行二分搜索并更新对边界点的估计。 图像来源：[[77](#bib.bib77)]
- en: 'Moreover, in the search for query efficient black box attack, [[78](#bib.bib78)]
    introduces a method that is independent of the gradient based on Bayesian optimization
    and Gaussian process surrogate models to find effective adversarial examples.
    In the model it is assumed that the attacker has no knowledge of the network architecture,
    weights, gradient or training data of the target model. But it is assumed that
    the attacker can query model with input $x$, to obtain the prediction scores on
    all classes $C$. They restrict the perturbation to the $l_{\infty}$ norm. Their
    objective is to maximize over the perturbations:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在搜索查询高效的黑盒攻击中，[[78](#bib.bib78)]引入了一种独立于梯度的基于贝叶斯优化和高斯过程替代模型的方法，以找到有效的对抗性样本。
    该模型假设攻击者对目标模型的网络架构、权重、梯度或训练数据一无所知。 但假定攻击者能够用输入$x$查询模型，以获取所有类别$C$的预测分数。 他们将扰动限制在$l_{\infty}$范数内。
    他们的目标是最大化扰动：
- en: '|  | $\displaystyle\delta^{*}={}$ | $\displaystyle\underset{\displaystyle\delta}{\mathrm{arg~{}max}}\quad[log(f(x_{origin}+g(\delta))_{t})\hfil\hfil\hfil\hfil$
    |  | (10) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\delta^{*}={}$ | $\displaystyle\underset{\displaystyle\delta}{\mathrm{arg~{}max}}\quad[log(f(x_{origin}+g(\delta))_{t})\hfil\hfil\hfil\hfil$
    |  | (10) |'
- en: '|  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{arg~{}max}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{arg~{}max}><quot;)][l]{}}\quad-log(\sum_{j\neq
    t}^{C}f(x_{origin}+g(\delta))_{j})]\hfil\hfil\hfil\hfil$ |  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{arg~{}max}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{arg~{}max}><quot;)][l]{}}\quad-log(\sum_{j\neq
    t}^{C}f(x_{origin}+g(\delta))_{j})]\hfil\hfil\hfil\hfil$ |  |'
- en: '|  |  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{arg~{}max}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{arg~{}max}><quot;)][l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle\delta\in[-\delta_{max},\delta_{max}]^{d_{r}}$
    |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathmakebox[width(&quot;$\underset{\displaystyle\phantom{\delta}}{\mathrm{arg~{}max}}><quot;)][c]{\mathmakebox[width(&quot;$\mathrm{arg~{}max}><quot;)][l]{\mathrm{\kern
    1.00006pts.t.}}}\quad$ |  | $\displaystyle\delta\in[-\delta_{max},\delta_{max}]^{d_{r}}$
    |  |'
- en: The Bayesian optimization proposed to improve the query efficiency requires
    the use of a surrogate model to approximate the objective function, in their work
    a Gaussian Process is used. Moreover to define the next query point is defined
    by an acquisition function. A big differential in their work is the fact that
    instead of searching in a high-dimensional space for a perturbation $\delta$,
    they utilize a function to reduce the dimensionality of the perturbation and later
    reconstitute to the true image size.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的贝叶斯优化方法旨在提高查询效率，需要使用代理模型来近似目标函数，他们的工作中使用了高斯过程。此外，定义下一个查询点是由一个获取函数决定的。他们工作的一个重要差异在于，他们不是在高维空间中寻找扰动
    $\delta$，而是利用一个函数来降低扰动的维度，然后再恢复到真实图像尺寸。
- en: '1:Input: Decoder g(.), observation $\mathcal{D}^{d}_{t-1}={g(\delta_{i}),y_{i}}^{t-1}_{i=1}$
    where $g(\delta_{i})\in\mathcal{R}^{d}$ and a set of possible $d^{r}:{d^{r}_{j}}^{N}_{j=1}$2:Output:
    The optimal reduced dimension $d^{r*}$ and corresponding GP model.3:for $j=1,...,N$ do4:     $\mathcal{D}^{d^{r}_{j}}_{t-1}=\{g^{-1}(g(\delta_{i})),y_{i}\}^{t-1}_{i=1},$
    $\triangleright$ $g^{-1}(g(\delta_{i}))\in R^{d^{r}_{j}}$5:     Fit a GP model
    to $\mathcal{D}^{d^{r}_{j}}_{t-1}$ and compute its maximum marginal likelihood
    $p(\mathcal{D}^{d}_{t-1}|\theta^{*},d^{r}_{j})$6:$d^{r*}=\operatorname*{arg\,max}_{d^{r}_{j}\in{d^{r}_{j}}^{N}_{j=1}}p(\mathcal{D}^{d}_{t-1}|\theta^{*},d^{r}_{j})$
    and its7:return'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入: 解码器 g(.), 观察 $\mathcal{D}^{d}_{t-1}={g(\delta_{i}),y_{i}}^{t-1}_{i=1}$
    其中 $g(\delta_{i})\in\mathcal{R}^{d}$ 和一个可能的 $d^{r}:{d^{r}_{j}}^{N}_{j=1}$集合 2:
    输出: 最优的降维 $d^{r*}$ 和相应的高斯过程模型 3: 对于 $j=1,...,N$ 执行 4: $\mathcal{D}^{d^{r}_{j}}_{t-1}=\{g^{-1}(g(\delta_{i})),y_{i}\}^{t-1}_{i=1},$
    $\triangleright$ $g^{-1}(g(\delta_{i}))\in R^{d^{r}_{j}}$ 5: 拟合一个高斯过程模型到 $\mathcal{D}^{d^{r}_{j}}_{t-1}$
    并计算其最大边际似然 $p(\mathcal{D}^{d}_{t-1}|\theta^{*},d^{r}_{j})$ 6: $d^{r*}=\operatorname*{arg\,max}_{d^{r}_{j}\in{d^{r}_{j}}^{N}_{j=1}}p(\mathcal{D}^{d}_{t-1}|\theta^{*},d^{r}_{j})$
    7: 返回'
- en: Algorithm 2 Bayesian selection of $d^{r}$
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 贝叶斯选择 $d^{r}$
- en: 3.2.4 Attack on RL algorithm
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 对强化学习算法的攻击
- en: 'In [[79](#bib.bib79)], a method for generating adversarial examples in reinforcement
    learning (RL) algorithms was proposed. In RL, an adversarial example can either
    be a modified image used to capture a state or in the case of this publication,
    an adversarial policy. It is important to highlight that an adversarial policy
    is not a strong adversary as we have in two-player games, but one that with a
    certain behavior triggers a failure in the victim’s policy. In this paper, a black-box
    attack is proposed to trigger bad behaviors in the victim’s policy. The victim’s
    policy is trained using Proximal Policy Optimization and learns to "play" against
    a fair opponent. The adversarial policy is trained to trigger failures in the
    victim’s policy. [Figure 5](#S3.F5 "Figure 5 ‣ 3.2.4 Attack on RL algorithm ‣
    3.2 Black-Box Attacks ‣ 3 Methods for Generating Adversarial Attacks ‣ Opportunities
    and Challenges in Deep Learning Adversarial Robustness: A Survey") shows the difference
    between an opponent’s policy and an adversarial manipulated policy.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[79](#bib.bib79)]中，提出了一种在强化学习（RL）算法中生成对抗性样本的方法。在RL中，对抗性样本可以是用于捕捉状态的修改图像，或者在本出版物中，是一种对抗性策略。重要的是要强调，对抗性策略并不像我们在双人游戏中看到的那样是强大的对手，而是具有某种行为的策略，会触发受害者策略的失败。本文提出了一种黑箱攻击方法，以在受害者策略中触发不良行为。受害者策略使用近端策略优化进行训练，并学习如何与公平对手“对抗”。对抗性策略被训练以触发受害者策略中的失败。[图
    5](#S3.F5 "图 5 ‣ 3.2.4 对RL算法的攻击 ‣ 3.2 黑箱攻击 ‣ 3 生成对抗攻击的方法 ‣ 深度学习对抗鲁棒性的机会与挑战：综述")展示了对手策略与对抗性操控策略之间的差异。
- en: '![Refer to caption](img/7d3d8e8a2c8c6bb2da1e5b4c107236f5.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7d3d8e8a2c8c6bb2da1e5b4c107236f5.png)'
- en: 'Figure 5: In the first sequence, a strong opponent has to enter collide with
    the agent to prevent it from winning the game. In the second line, in the adversarial
    example, the opponent simply crumbles on the floor, which triggers a bad behavior
    in the victim’s policy. Image Source: [[79](#bib.bib79)]'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在第一序列中，一个强大的对手必须与代理碰撞，以阻止其赢得游戏。在第二行中，在对抗性样本中，对手只是简单地摔倒在地，这触发了受害者策略中的不良行为。图像来源：[[79](#bib.bib79)]
- en: Also in the paper, it was shown the dependence of the size of the input space
    and the effectiveness of adversarial policies. The greater the dimensionality
    of the observation space under the control of the adversary, the more vulnerable
    the victim is to attack.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，还展示了输入空间的大小与对抗策略的有效性之间的依赖关系。对手控制下观察空间的维度越大，受害者就越容易受到攻击。
- en: 3.3 Physical World Attack
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 物理世界攻击
- en: The research presented so far is mostly focused on applying the attacks in virtual
    applications and controlled datasets, but the great concern about the existence
    of adversarial examples is the extent to which they can imply severe consequences
    to the users of the system. With that objective, we dedicate this session on exploring
    publications with real-world applications and consequences clearly stated.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止的研究主要集中在虚拟应用和受控数据集中的攻击应用，但关于对抗性样本存在的主要担忧是它们对系统用户可能带来的严重后果。为此，我们专门探讨了真实世界应用及其明确陈述的后果的文献。
- en: In [[80](#bib.bib80)], road signs were physically attacked by placing a sticker
    in a specific position of the sign. Their attack consisted of initially finding,
    in a sign image, the location with the most influence for the decision of the
    classifier. For that objective, an $l_{1}norm$ was used because it renders sparse
    perturbations in the image, making it easier to locate the modification patches.
    Based on the defined location, an $l_{2}norm$ was used to identify the most appropriate
    color for the sticker.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[80](#bib.bib80)]中，通过在标志的特定位置放置贴纸对道路标志进行了物理攻击。他们的攻击方法包括首先在标志图像中找到对分类器决策影响最大的地点。为此目标，使用了$l_{1}norm$，因为它在图像中产生稀疏的扰动，使得定位修改区域更加容易。基于定义的位置，使用$l_{2}norm$来识别贴纸的最合适颜色。
- en: Moreover, as face recognition is becoming very popular as a biometric security
    measure it is the focus of several adversarial attacks. We highlight 4 attacks
    in face recognition and identification.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于面部识别作为生物识别安全措施变得非常流行，它也成为了多个对抗攻击的重点。我们强调了面部识别和身份识别中的4种攻击。
- en: •
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Evaluation of the robustness of DNN models to Face Recognition against adversarial
    attacks - In this publication Goswami et al. [[81](#bib.bib81)] evaluates how
    the depth of the architecture impacts the robustness of the model in identifying
    faces. They evaluate the robustness with respect to adversarial settings taking
    into consideration distortions which are normally observed in a common scene.
    These distortions are handled with ease by shallow networks on the contrary of
    deep networks. In their approach, they’ve used Open-Face and VGG-Face networks,
    and have achieved a high fooling rate. It is important to notice that in their
    attack no restrictions are made in the visibility of the perturbations.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DNN 模型在面部识别中的对抗攻击鲁棒性评估 - 在这项出版物中，Goswami 等人 [[81](#bib.bib81)] 评估了模型架构的深度如何影响其在识别面部时的鲁棒性。他们在考虑到通常在常见场景中观察到的失真情况下评估了鲁棒性。这些失真被浅层网络轻松处理，而深层网络则相对困难。在他们的方法中，他们使用了
    Open-Face 和 VGG-Face 网络，并且取得了很高的欺骗率。值得注意的是，在他们的攻击中，扰动的可见性没有受到限制。
- en: •
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adversarial Attacks on Face Detectors using Neural Net based Constrained Optimization
    In this work, also focusing on prevent face identification, [[82](#bib.bib82)]
    has generated an attack, based on Carlini and Wagner attack, which was able to
    fool R-CNN. Their perturbations are not visible in the adversarial example.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于神经网络约束优化的面部检测对抗攻击 - 在这项工作中，也专注于防止面部识别，[[82](#bib.bib82)] 基于Carlini 和 Wagner
    攻击生成了一种攻击，能够欺骗 R-CNN。他们的扰动在对抗样本中不可见。
- en: •
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Generating Adversarial Examples by Makeup Attacks on Face Recognition - In this
    research, [[83](#bib.bib83)] implement a GAN network to generate make-up perturbation.
    When the perturbation is applied to the face, the classifier shifts its decision
    to the target class.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过化妆攻击生成对抗样本用于面部识别 - 在这项研究中，[[83](#bib.bib83)] 实现了一个生成对抗网络（GAN）来生成化妆扰动。当扰动施加到面部时，分类器将其决策转移到目标类别。
- en: •
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Efficient decision-based black-box adversarial attacks on face recognition -
    Dong et al.[[84](#bib.bib84)] propose an evolutionary attack method. The proposed
    adversarial example generator is constrained in a black-box setting. The algorithm
    focus on reducing the number of dimensions of the search space by modeling the
    local geometry of the search vectors. Such an algorithm has been shown to be applicable
    to most recognition tasks.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 针对面部识别的基于决策的高效黑箱对抗攻击 - Dong 等人[[84](#bib.bib84)] 提出了一个进化攻击方法。所提出的对抗样本生成器被限制在黑箱环境中。该算法专注于通过建模搜索向量的局部几何来减少搜索空间的维度。这种算法已被证明适用于大多数识别任务。
- en: 3.3.1 Other Attacks
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 其他攻击
- en: In the field of Cyber-security machine learning models are, in general, applied
    to detect malware, malicious connections, malicious domain classifier, and others.
    In Suciu et al. [[85](#bib.bib85)] an evaluation of the robustness of current
    malware detection models is performed. The authors retrain the model in a production-scale
    dataset to perform the evaluation. With the new data, the model which was previously
    vulnerable to attacks was shown to be stronger and architectural weaknesses were
    reported. The work of Suciu et al. [[85](#bib.bib85)], explores how attacks transfer
    in the cyber-security domain, and mainly the inherent trade-off between effectiveness
    and transferability. With respect to malicious connection and domain, Chernikova
    et al. [[86](#bib.bib86)] builds a model that takes into consideration the formal
    dependencies generated by the normal operations applied in the feature space.
    The model to generate adversarial examples simultaneously consider both the mathematical
    dependencies and the real-world constraints of such applications. The algorithm
    focus on determining the features with higher variability and the ones with a
    higher correlation with these features. This search is performed for each iteration.
    All the identified features are modified but constrained to preserve an upper
    bound on the maximum variation of the features. The upper bound respects physical-world
    application limitations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络安全领域，机器学习模型通常用于检测恶意软件、恶意连接、恶意域名分类器等。在Suciu等人[[85](#bib.bib85)]的研究中，对当前恶意软件检测模型的鲁棒性进行了评估。作者在生产规模的数据集上重新训练了模型以进行评估。利用新数据，之前易受攻击的模型显示出更强的能力，并报告了架构上的弱点。Suciu等人[[85](#bib.bib85)]的工作探讨了攻击如何在网络安全领域转移，主要关注有效性和可转移性之间的固有权衡。关于恶意连接和域，Chernikova等人[[86](#bib.bib86)]建立了一个模型，考虑了在特征空间中由正常操作生成的正式依赖关系。生成对抗性样本的模型同时考虑了这些应用的数学依赖关系和现实世界的限制。该算法专注于确定具有较高变异性的特征以及与这些特征相关性较高的特征。这个搜索在每次迭代中进行。所有识别出的特征都被修改，但受到限制以保持特征最大变异的上限。上限尊重现实世界应用的限制。
- en: In the Cyber-Physical domain, several publications demonstrate the brittleness
    of ML models and the generation of adversarial examples. [[87](#bib.bib87)] generated
    adversarial examples to an iCub Humanoid Robot. The attack proposed simply extends
    over the attacks in [[88](#bib.bib88)]. The main aspect to be considered in this
    paper is the fact that it highlights the high consequences of the adversarial
    examples in the decision process of safety-critical applications. Moreover, in
    self-driving cars several attacks have been derived like DARTS ([[89](#bib.bib89)])
    and the work of [[90](#bib.bib90)] which shows the attack of traffic signs, the
    latter with real experiments. In a different sensor type, the work of [[91](#bib.bib91)]
    demonstrates the attack to a LIDAR sensor, in which they attack the point cloud
    image.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络物理领域，几篇文献展示了机器学习模型的脆弱性和对抗性样本的生成。[[87](#bib.bib87)] 为iCub类人机器人生成了对抗性样本。提出的攻击方法简单地扩展了[[88](#bib.bib88)]中的攻击。本文中需要考虑的主要方面是它突出了对抗性样本在安全关键应用决策过程中的严重后果。此外，在自动驾驶汽车中，已经衍生出多种攻击方法，如DARTS
    ([[89](#bib.bib89)]) 和[[90](#bib.bib90)]的工作，该工作展示了对交通标志的攻击，并进行了实际实验。在不同的传感器类型中，[[91](#bib.bib91)]
    的工作展示了对LIDAR传感器的攻击，其中他们攻击了点云图像。
- en: Moreover in [[92](#bib.bib92)], a novel technique was proposed to attack object
    tracking algorithms. In their approach, the bounding box is attacked in a single
    frame, which is enough to fool the algorithm and generate an offset in the placement
    of the bounding box. Such an attack would be critical to self-driving cars to
    recognize the position of obstacles, other vehicles, and pedestrians on the road.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在[[92](#bib.bib92)]中，提出了一种新技术用于攻击目标跟踪算法。在他们的方法中，攻击是在单帧中对边界框进行的，这足以欺骗算法并在边界框的位置上产生偏移。这种攻击对自动驾驶汽车识别道路上的障碍物、其他车辆和行人至关重要。
- en: 'TABLE II: Dichotomy of the Attacks'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：攻击的二分法
- en: '| WHITE BOX ATTACKS |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 白盒攻击 |'
- en: '| --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Algorithm | Metric | Step^a | Domain^b |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 量度 | 步骤^a | 领域^b |'
- en: '| L-BFGS ([[43](#bib.bib43)]) | $l_{2}$ | Iter. | Im-C |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| L-BFGS ([[43](#bib.bib43)]) | $l_{2}$ | 迭代 | Im-C |'
- en: '| FGSM ([[56](#bib.bib56)]) | $l_{\infty},l_{2}$ | S-Stp | Im-C |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| FGSM ([[56](#bib.bib56)]) | $l_{\infty},l_{2}$ | S-Stp | Im-C |'
- en: '| Deepfool ([[57](#bib.bib57)]) | $l_{2}$ | Iter. | Im-C |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Deepfool ([[57](#bib.bib57)]) | $l_{2}$ | 迭代 | Im-C |'
- en: '| JSMA ([[58](#bib.bib58)]) | $l_{2}$ | Iter. | Im-C |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| JSMA ([[58](#bib.bib58)]) | $l_{2}$ | Iter. | Im-C |'
- en: '| PGD ([[54](#bib.bib54)]) | $l_{\infty},l_{2}$ | Iter. | Im-C |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| PGD ([[54](#bib.bib54)]) | $l_{\infty},l_{2}$ | Iter. | Im-C |'
- en: '| C and W ([[60](#bib.bib60)]) | $l_{0}$,$l_{2}$,$l_{\infty}$ | Iter. | Im-C
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| C and W ([[60](#bib.bib60)]) | $l_{0}$,$l_{2}$,$l_{\infty}$ | Iter. | Im-C
    |'
- en: '| GTAE ([[62](#bib.bib62)]) | $l_{0}$ | SMT | Im-C |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| GTAE ([[62](#bib.bib62)]) | $l_{0}$ | SMT | Im-C |'
- en: '| UAP ([[66](#bib.bib66)]) | $l_{\infty},l_{2}$ | Iter. | Im-C |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| UAP ([[66](#bib.bib66)]) | $l_{\infty},l_{2}$ | Iter. | Im-C |'
- en: '| EAD ([[68](#bib.bib68)]) | $l_{1},l_{2}$ | Iter. | Im-C |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| EAD ([[68](#bib.bib68)]) | $l_{1},l_{2}$ | Iter. | Im-C |'
- en: '| OMGDA ([[69](#bib.bib69)]) | $l_{2}$ | Iter. | Im-C |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| OMGDA ([[69](#bib.bib69)]) | $l_{2}$ | Iter. | Im-C |'
- en: '| STA ([[70](#bib.bib70)]) | Spt-Var | Iter. | Im-C |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| STA ([[70](#bib.bib70)]) | Spt-Var | Iter. | Im-C |'
- en: '| UAEGM ([[69](#bib.bib69)]) | $l_{2}$ | Iter. | Im-C |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| UAEGM ([[69](#bib.bib69)]) | $l_{2}$ | Iter. | Im-C |'
- en: '| Shadow Attack ([[67](#bib.bib67)]) | $l_{p}$ | Iter. | Im-C |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| Shadow Attack ([[67](#bib.bib67)]) | $l_{p}$ | Iter. | Im-C |'
- en: '| RoadSign ([[80](#bib.bib80)]) | $l_{1}$,$l_{2}$ | Iter. | S-R |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| RoadSign ([[80](#bib.bib80)]) | $l_{1}$,$l_{2}$ | Iter. | S-R |'
- en: '| FRA1 ([[81](#bib.bib81)]) | NR | S-Stp | F-Rec |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| FRA1 ([[81](#bib.bib81)]) | NR | S-Stp | F-Rec |'
- en: '| FRA2 ([[82](#bib.bib82)]) | $l_{2}$ | Iter. | F-Rec |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| FRA2 ([[82](#bib.bib82)]) | $l_{2}$ | Iter. | F-Rec |'
- en: '| FRA3 ([[83](#bib.bib83)]) | $l_{1}$ | Iter. | F-Rec |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| FRA3 ([[83](#bib.bib83)]) | $l_{1}$ | Iter. | F-Rec |'
- en: '| CSA2 ([[86](#bib.bib86)]) | $l_{2}$ | Iter. | Cyb-Sec |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| CSA2 ([[86](#bib.bib86)]) | $l_{2}$ | Iter. | Cyb-Sec |'
- en: '| CPA1 ([[87](#bib.bib87)]) | $l_{2}$ | Iter. | Cyb-Phy |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| CPA1 ([[87](#bib.bib87)]) | $l_{2}$ | Iter. | Cyb-Phy |'
- en: '| CPA3 ([[90](#bib.bib90)]) | $l_{0}$ | Iter. | Cyb-Phy |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| CPA3 ([[90](#bib.bib90)]) | $l_{0}$ | Iter. | Cyb-Phy |'
- en: '| CPA4 ([[91](#bib.bib91)]) | NR | Iter. | Cyb-Phy |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| CPA4 ([[91](#bib.bib91)]) | NR | Iter. | Cyb-Phy |'
- en: '| CPA5 ([[92](#bib.bib92)]) | $l_{1},l_{2}$ | Iter. | Cyb-Phy |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| CPA5 ([[92](#bib.bib92)]) | $l_{1},l_{2}$ | Iter. | Cyb-Phy |'
- en: '| BLACK BOX ATTACKS |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| BLACK BOX ATTACKS |'
- en: '| Method | Metric | Step^a | Domain^b |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Method | Metric | Step^a | Domain^b |'
- en: '| PBBA ([[74](#bib.bib74)]) | $l_{p}$ | Iter. | Im-C |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| PBBA ([[74](#bib.bib74)]) | $l_{p}$ | Iter. | Im-C |'
- en: '| ZOO ([[52](#bib.bib52)]) | $l_{p}$ | Iter. | Im-C |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| ZOO ([[52](#bib.bib52)]) | $l_{p}$ | Iter. | Im-C |'
- en: '| One-Pixel ([[75](#bib.bib75)]) | $l_{0}$ | Iter. | Im-C |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| One-Pixel ([[75](#bib.bib75)]) | $l_{0}$ | Iter. | Im-C |'
- en: '| DBA ([[93](#bib.bib93)]) | $l_{2}$ | Iter. | Im-C |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| DBA ([[93](#bib.bib93)]) | $l_{2}$ | Iter. | Im-C |'
- en: '| HopSkipJumpAttack ([[77](#bib.bib77)]) | $l_{2}$,$l_{\infty}$ | Iter. | Im-C
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| HopSkipJumpAttack ([[77](#bib.bib77)]) | $l_{2}$,$l_{\infty}$ | Iter. | Im-C
    |'
- en: '| UPSET & ANGRI ([[94](#bib.bib94)]) | $l_{\infty}$ | Iter. | Im-C |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| UPSET & ANGRI ([[94](#bib.bib94)]) | $l_{\infty}$ | Iter. | Im-C |'
- en: '| RLAttack ([[79](#bib.bib79)]) | NR | Iter. | RL |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| RLAttack ([[79](#bib.bib79)]) | NR | Iter. | RL |'
- en: '| FRA4 ([[84](#bib.bib84)]) | $l_{2}$ | Iter. | F-Rec |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| FRA4 ([[84](#bib.bib84)]) | $l_{2}$ | Iter. | F-Rec |'
- en: '| CSA1 ([[85](#bib.bib85)]) | $l_{\infty}$ | Iter. | Cyb-Sec |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| CSA1 ([[85](#bib.bib85)]) | $l_{\infty}$ | Iter. | Cyb-Sec |'
- en: '| CPA2 ([[89](#bib.bib89)]) | $l_{1}$ | Iter. | Cyb-Phy |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| CPA2 ([[89](#bib.bib89)]) | $l_{1}$ | Iter. | Cyb-Phy |'
- en: '| a - Iter.: Iterative, S-Stp.: Single Step; b - Im-C.: Image Classification,
    |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| a - Iter.: 迭代，S-Stp.: 单步； b - Im-C.: 图像分类， |'
- en: '| S-R.: Signal Recognition, F-Rec.: Face Recognition, Cyb-Sec.: Cyber- |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| S-R.: 信号识别，F-Rec.: 人脸识别，Cyb-Sec.: 网络安全 |'
- en: '| Security, Cyb-Phys.: Cyber-Physical, RL.: Reinforcement Learning |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 安全，Cyb-Phys.: 网络物理，RL.: 强化学习 |'
- en: 'Figure 6: Summary of the defense mechanisms sub-divided in the 3 categories,
    namely: Adversarial Training, Certified Defenses, and Regularization Approach.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：防御机制的总结，分为三个类别：对抗训练、认证防御和正则化方法。
- en: 4 Defense Mechanisms based on Robust Optimization Against Adversarial Attacks
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基于鲁棒优化的对抗攻击防御机制
- en: 'From the examples presented so far, we see that both DNN’s and CNN’s are very
    unstable locally and susceptible to misclassify examples with perturbations that
    are barely perceivable by the human eye. Several works have reported structured
    algorithms and formulations to improve the robustness of ML models employing robust
    optimization. The goal of this section is to visit the most commonly identified
    techniques to achieve robustness (through the eyes of the optimization), namely
    Adversarial Training, Bayesian Approach, Certified Defenses, and Regularization
    Approaches, which are summarized in [Figure 6](#S3.F6 "Figure 6 ‣ 3.3.1 Other
    Attacks ‣ 3.3 Physical World Attack ‣ 3 Methods for Generating Adversarial Attacks
    ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey").'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '从目前展示的例子来看，我们发现 DNN 和 CNN 在局部非常不稳定，并且容易错误分类那些人眼几乎无法察觉的微小扰动的样本。多个研究报告提出了结构化算法和公式，以通过稳健优化提高
    ML 模型的鲁棒性。本节的目标是探讨实现鲁棒性的最常见技术（从优化的角度），即对抗训练、贝叶斯方法、认证防御和正则化方法，这些在[图 6](#S3.F6 "Figure
    6 ‣ 3.3.1 Other Attacks ‣ 3.3 Physical World Attack ‣ 3 Methods for Generating
    Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey")中总结。'
- en: 'We summarize in [Table III](#S4.T3 "TABLE III ‣ 4.1.12 Input Transformation
    Methods ‣ 4.1 Defending through Adversarial (re)Training ‣ 4 Defense Mechanisms
    based on Robust Optimization Against Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey") the results presented in the
    surveyed papers. We’ve compiled the information available in each of those papers
    concerning the error rate under the tested conditions. Each of the papers has
    different evaluation criteria and conditions. The dataset is highly influential
    in the accuracy of the model. To that end, we’ve been careful to better express
    the conditions in which the models were evaluated. Papers in which the results
    were not clear or have used a very specific metric are not listed in these tables
    to keep consistency among the results.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在[表 III](#S4.T3 "TABLE III ‣ 4.1.12 Input Transformation Methods ‣ 4.1 Defending
    through Adversarial (re)Training ‣ 4 Defense Mechanisms based on Robust Optimization
    Against Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey")中总结了调研论文中展示的结果。我们已整理了每篇论文在测试条件下的错误率信息。每篇论文有不同的评估标准和条件。数据集在模型的准确性中具有重要影响。因此，我们尽力更好地表达了模型评估的条件。结果不明确或使用了非常特定度量的论文未列入这些表格，以保持结果的一致性。'
- en: 4.1 Defending through Adversarial (re)Training
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 通过对抗性（再）训练进行防御
- en: After the first introduction of adversarial examples ([[43](#bib.bib43)] and
    [[88](#bib.bib88)]), defense mechanisms to train robust neural networks were built
    based on the inclusion of adversarial examples to the training set. Models trained
    using adversarial training with projected gradient descent (PGD) were shown to
    be robust against the strongest known attacks. This is in contrast to other defense
    mechanisms that have been broken by new attack techniques. In this section, we
    explore robustness mechanisms that explicitly or implicitly address the robustness
    in deep learning through either adding adversarial examples in the dataset or
    incorporate them in the objective function for the optimization.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在首次介绍对抗性样本（[[43](#bib.bib43)] 和 [[88](#bib.bib88)]）之后，训练鲁棒神经网络的防御机制基于将对抗性样本纳入训练集。使用投影梯度下降（PGD）的对抗训练模型显示出对已知最强攻击的鲁棒性。这与其他防御机制被新攻击技术攻破的情况形成对比。在本节中，我们探讨那些明确或隐式解决深度学习鲁棒性的机制，通过在数据集中添加对抗性样本或将其纳入优化的目标函数中。
- en: 4.1.1 Harnessing Adversarial examples
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 利用对抗性样本
- en: In his work, [[56](#bib.bib56)] Goodfellow et. al. suggests the use of adversarial
    examples in the training process to improve the robustness of machine learning
    models. It is a very simple idea, which worked for the proposed configuration.
    The algorithm, using FGSM in an untargeted setting, would generate a set of adversarial
    examples $x^{\prime}$, which were fed to the learning algorithm with the true
    label, $(x^{\prime},y)$. Important to notice the limitation of such a framework,
    it is robust against FGSM attacks, but susceptible to other attacks, such as iterative
    methods. Such weakness was pointed in a later work by [[107](#bib.bib107)], which
    also shown single-step attacks could fool such a defense.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的工作中，[[56](#bib.bib56)] Goodfellow 等人建议在训练过程中使用对抗样本以提高机器学习模型的鲁棒性。这是一个非常简单的想法，适用于所提议的配置。该算法在未目标设置下使用
    FGSM，生成一组对抗样本 $x^{\prime}$，这些样本与真实标签 $(x^{\prime},y)$ 一起输入到学习算法中。值得注意的是这种框架的局限性，它对
    FGSM 攻击具有鲁棒性，但对其他攻击（如迭代方法）则较为脆弱。这种弱点在 [[107](#bib.bib107)] 的后续工作中得到了指出，该工作还表明单步攻击可以欺骗这种防御。
- en: 'In [[98](#bib.bib98)], it is studied an adaptation of FGSM in adversarial training.
    The initially proposed FGSM training was shown to create a massive over-fitting
    in the model having it not robust to iterative attack methods such as PGD. In
    this new publication, Wong et. al., proposes small modifications in the initialization
    of the FGSM algorithm to accommodate randomness as a way to prevent over-fitting
    in the training process. Instead of having a fixed initialization, the perturbations
    are generated as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[98](#bib.bib98)] 中，研究了 FGSM 在对抗训练中的一种适配方法。最初提出的 FGSM 训练显示在模型中产生了大量的过拟合，使其对如
    PGD 这样的迭代攻击方法不够鲁棒。在这篇新出版的文章中，Wong 等人提出对 FGSM 算法的初始化进行小的修改，以引入随机性作为防止训练过程中过拟合的一种方法。扰动不再是固定初始化的，而是按以下方式生成：
- en: '1.'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: $\delta=Uniform(-\epsilon,\epsilon)$
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\delta=Uniform(-\epsilon,\epsilon)$
- en: '2.'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: $\delta=\delta+\alpha sign(\nabla_{\delta}\mathcal{L}(f_{\theta}(x_{i}+\delta),y_{i}))$
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\delta=\delta+\alpha sign(\nabla_{\delta}\mathcal{L}(f_{\theta}(x_{i}+\delta),y_{i}))$
- en: '3.'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: $\delta=\max(\min(\delta,\epsilon),-\epsilon)$
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\delta=\max(\min(\delta,\epsilon),-\epsilon)$
- en: The addition of the sampling from a uniform distribution to the initial perturbation
    allowed the algorithm to better estimate the inner maximization, improving the
    robustness of adversarially trained models with FGSM, maintaining the speed of
    the adversarial example generation.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在初始扰动中添加来自均匀分布的采样，算法能够更好地估计内部最大化，从而提高了使用 FGSM 训练的模型的鲁棒性，同时保持了对抗样本生成的速度。
- en: 4.1.2 Towards Deep Learning Models Resistant to Adversarial Examples
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 面向抗对抗样本的深度学习模型
- en: 'In their publication, Madry et. al. [[54](#bib.bib54)], among an extensive
    discussion of robustness in the machine learning models, propose to incorporate
    iterative methods to approximate the inner maximization problem shown in:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的出版物中，Madry 等人 [[54](#bib.bib54)] 在对机器学习模型的鲁棒性进行广泛讨论后，提出了将迭代方法纳入以近似内部最大化问题的方法。
- en: '|  | $\min_{\theta}(\frac{1}{\mathcal{D}}\sum_{(x,y)\in~{}\mathcal{D}}\max_{\delta\in\Delta(x)}\mathcal{L}(f(x+\delta),y))$
    |  | (11) |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta}(\frac{1}{\mathcal{D}}\sum_{(x,y)\in~{}\mathcal{D}}\max_{\delta\in\Delta(x)}\mathcal{L}(f(x+\delta),y))$
    |  | (11) |'
- en: where $\delta$ is the perturbation, $f(.)$ is the model.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\delta$ 是扰动，$f(.)$ 是模型。
- en: In this publication, the authors have approximated the inner maximization problem
    with the PGD attack method, but it is important to highlight in this formulation,
    that the model will be as robust as the attack it was trained on. If new and more
    efficient adversarial examples are presented to the model at inference time, nothing
    can be stated regarding the robustness of the model.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，作者用 PGD 攻击方法近似了内部最大化问题，但需要强调的是，根据这种公式，模型的鲁棒性将取决于它所训练的攻击方法。如果在推理时向模型展示了新的、更有效的对抗样本，那么无法确定模型的鲁棒性。
- en: 4.1.3 Ensemble Adversarial Training
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 集成对抗训练
- en: In their research [[107](#bib.bib107)], the authors show that when the model
    is trained directly on single/iterative methods, the model trains on examples
    crafted to maximize a linear approximation of the loss. Moreover, such a model
    training method converges to a degenerate global minimum. These artifacts near
    the inputs $x$ obfuscate the approximation of the loss. According to their findings,
    the model learns weak perturbations rather than generating robustness against
    strong ones.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的研究 [[107](#bib.bib107)] 中，作者表明，当模型直接在单一/迭代方法上训练时，模型会训练在旨在最大化损失的线性近似的样本上。此外，这种模型训练方法会收敛到退化的全局最小值。这些在输入
    $x$ 附近的伪影模糊了损失的近似。根据他们的发现，该模型学习到的是弱扰动，而不是生成对强扰动的鲁棒性。
- en: As a countermeasure, the paper implements a framework in which the model is
    trained with adversarial examples from similar classifiers. In their method, the
    generation of adversarial examples is dissociated from the network being optimized.
    Under such circumstances, their proposed framework approximates to the work in
    which black-box attacks are generated with the use of auxiliary models, and consequently
    makes their approach more resilient to it. In the algorithm, to train classifier
    $F_{0}$, they train other sets of classifiers, $F_{1},F_{2},...,F_{n}$, generate
    adversarial examples for these classifiers, and use the adversarially generated
    examples to train $F_{0}$. Their defense model even for the ImageNet dataset,
    has shown robustness against black-box attacks.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种对策，论文实现了一个框架，其中模型使用来自类似分类器的对抗样本进行训练。在他们的方法中，对抗样本的生成与正在优化的网络分离。在这种情况下，他们提出的框架接近于使用辅助模型生成黑箱攻击的工作，从而使他们的方法对黑箱攻击更具弹性。在算法中，为了训练分类器
    $F_{0}$，他们训练其他分类器集 $F_{1},F_{2},...,F_{n}$，为这些分类器生成对抗样本，并使用这些对抗生成的样本来训练 $F_{0}$。即使对于
    ImageNet 数据集，他们的防御模型也表现出了对黑箱攻击的鲁棒性。
- en: 'In a different work, using a similar concept, [[106](#bib.bib106)] proposes
    the augmentation of the model robustness with the use of random noise layers to
    prevent the strong gradient attacks. It is a gradient masking attack, but the
    authors position the algorithm as an improvement to ensemble adversarial defense,
    by stating that the defense is equivalent to ensembling an infinite number of
    noisy models. Such ensembling is claimed by the authors to be equivalent to training
    the original model with a Lipschitz regularizing term. They’ve achieved significant
    results against Carlini and Wagner’s strong attacks. Algorithm [3](#alg3 "Algorithm
    3 ‣ 4.1.3 Ensemble Adversarial Training ‣ 4.1 Defending through Adversarial (re)Training
    ‣ 4 Defense Mechanisms based on Robust Optimization Against Adversarial Attacks
    ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey")
    shows the steps to implement the proposed algorithm.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '在另一项工作中，使用类似的概念，[[106](#bib.bib106)] 提出了通过使用随机噪声层来增强模型鲁棒性，以防止强梯度攻击。这是一种梯度掩蔽攻击，但作者将该算法定位为对集成对抗防御的改进，声称该防御相当于集成无限数量的噪声模型。作者声称，这种集成相当于用
    Lipschitz 正则化项训练原始模型。他们在抵御 Carlini 和 Wagner 的强攻击方面取得了显著成果。算法 [3](#alg3 "Algorithm
    3 ‣ 4.1.3 Ensemble Adversarial Training ‣ 4.1 Defending through Adversarial (re)Training
    ‣ 4 Defense Mechanisms based on Robust Optimization Against Adversarial Attacks
    ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey")
    显示了实施所提算法的步骤。'
- en: '1:Training2:for iter = 1,2,… do3:     Randomly sample $(x_{i},y_{i})$ in dataset4:     Randomly
    generate $\epsilon\sim\mathcal{N}(0,\sigma^{2})$ for each noisy layer5:     Compute
    the noise gradient, $g=\nabla_{\theta}\mathcal{L}(f_{\epsilon}(\theta,x_{i}),y_{i})$6:     Update
    weights: $\theta^{\prime}\leftarrow\theta-g$7:Testing Given a test image $x$,
    initialize $p=(0,...,0)$8:for $j={1,2,...,Ensembles}$ do9:     Generate $\epsilon$10:     Forward
    propagation and generate probability output, $p^{j}$11:     Update, $p\leftarrow
    p+p^{j}$12:Return class with with maximum probability'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '1:训练2:对于 iter = 1,2,… 执行3: 随机从数据集中抽取 $(x_{i},y_{i})$4: 对每个噪声层随机生成 $\epsilon\sim\mathcal{N}(0,\sigma^{2})$5:
    计算噪声梯度，$g=\nabla_{\theta}\mathcal{L}(f_{\epsilon}(\theta,x_{i}),y_{i})$6: 更新权重:
    $\theta^{\prime}\leftarrow\theta-g$7: 测试 给定测试图像 $x$，初始化 $p=(0,...,0)$8: 对于 $j={1,2,...,Ensembles}$
    执行9: 生成 $\epsilon$10: 正向传播并生成概率输出，$p^{j}$11: 更新，$p\leftarrow p+p^{j}$12: 返回最大概率的类别'
- en: Algorithm 3 Random Self-Ensemble
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 随机自集成
- en: More recently in [[105](#bib.bib105)], a mixed-precision ensemble was proposed.
    Ensemble of Mixed Precision Deep Networks for Increased Robustness (EMPIR) is
    based on the observation that quantized neural networks often show higher robustness
    to adversarial attacks than full precision networks. Such models sacrifice accuracy.
    EMPIR combines the accuracy of full models with the robustness of quantized models
    by composing them in an ensemble.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在[[105](#bib.bib105)]中，提出了混合精度集成。混合精度深度网络集成（EMPIR）基于这样的观察：量化神经网络通常对对抗攻击表现出比全精度网络更高的鲁棒性。这些模型牺牲了准确性。EMPIR通过将全模型的准确性与量化模型的鲁棒性组合在一起，形成了一个集成模型。
- en: 4.1.4 Principled Adversarial Training
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 原则性对抗训练
- en: In [[104](#bib.bib104)], Sinha et. al. have presented a method to solve the
    outer maximization with a Lagrange relaxation approach. By doing so, they presented
    both, an adversarial training method based on the min-max formulation and a method
    to prove the robustness of the model. In addition to the adversarial training,
    a penalty is added on the loss term to help regularize the learning. It is used
    as a Lagrangian formulation to generate this penalty. It perturbs the underlying
    data distribution within a Wasserstein ball. The model’s efficiency is restricted
    to smooth losses, but under such constraint, it can achieve a moderate level of
    robustness. The computational or statistical cost, when compared to empirical
    risk minimization is small.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[104](#bib.bib104)]中，Sinha等人提出了一种使用拉格朗日松弛方法解决外部最大化问题的方法。通过这样做，他们提出了一种基于最小-最大公式的对抗训练方法，并提出了一种证明模型鲁棒性的方法。除了对抗训练，还在损失项上添加了惩罚，以帮助正则化学习。它被用作拉格朗日公式来生成这种惩罚。它在Wasserstein球内扰动基础数据分布。模型的效率受到平滑损失的限制，但在这种约束下，它可以实现中等水平的鲁棒性。与经验风险最小化相比，其计算或统计成本较小。
- en: 4.1.5 Network Robustness with Adversary Critic
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5 使用对抗批评者的网络鲁棒性
- en: The Adversarial Training formulation is an optimization problem that naturally
    involves minimization and maximization. In [[103](#bib.bib103)] and [[116](#bib.bib116)]
    the GAN framework is proposed to generate the noisy perturbations, which will
    lead to the adversarial example. On the other-hand the discriminator of the network
    act as a critic which will discern if the presented input $x$ is adversarial or
    not. In the work, it is highlighted that the generated adversarial networks are
    also robust to black-box attacks, showing similar or better performance than state-of-the-art
    defense mechanisms.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练公式是一个自然涉及最小化和最大化的优化问题。在[[103](#bib.bib103)]和[[116](#bib.bib116)]中，提出了生成噪声扰动的GAN框架，这将导致对抗样本。另一方面，网络的判别器充当批评者，判断所呈现的输入$x$是否具有对抗性。在这项工作中，强调了生成的对抗网络对黑箱攻击也具有鲁棒性，表现出与最先进的防御机制相似或更好的性能。
- en: 4.1.6 Adversarial Logit Pairing
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.6 对抗Logit配对
- en: 'In [[54](#bib.bib54)] Madry et. al. suggested the use of the [Equation 11](#S4.E11
    "11 ‣ 4.1.2 Towards Deep Learning Models Resistant to Adversarial Examples ‣ 4.1
    Defending through Adversarial (re)Training ‣ 4 Defense Mechanisms based on Robust
    Optimization Against Adversarial Attacks ‣ Opportunities and Challenges in Deep
    Learning Adversarial Robustness: A Survey") to adversarial train the model, and
    achieve robustness in their model. Kannan et al.[[102](#bib.bib102)] implements
    a mixed version of this defense. Instead of training the robust model only on
    adversary perturbed images, they incorporate a mix of clean $(x,y)$ and perturbed
    $(x^{\prime},y)$ batches, which they call mixed mini-batch PGD (M-PGD).'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[54](#bib.bib54)]中，Madry等人建议使用[方程式11](#S4.E11 "11 ‣ 4.1.2 Towards Deep Learning
    Models Resistant to Adversarial Examples ‣ 4.1 Defending through Adversarial (re)Training
    ‣ 4 Defense Mechanisms based on Robust Optimization Against Adversarial Attacks
    ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey")对模型进行对抗训练，以实现模型的鲁棒性。Kannan等人[[102](#bib.bib102)]实现了这种防御的混合版本。他们没有仅在对抗扰动图像上训练鲁棒模型，而是结合了干净的$(x,y)$和扰动的$(x^{\prime},y)$批次，他们称之为混合迷你批次PGD（M-PGD）。'
- en: In their work, they go beyond to analyze the fact that most of the adversarial
    training framework, train the models with information that $x^{\prime}$ should
    belong to the class $t$, but the model is not given any information indicating
    that $x^{\prime}$ is more similar to the actual sample $x$ than any other belonging
    to the same class. To that extent, they propose another algorithm called adversarial
    logit pairing.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的工作中，他们进一步分析了大多数对抗训练框架的问题，即这些框架训练模型时仅告知 $x^{\prime}$ 应属于类别 $t$，但模型没有任何指示
    $x^{\prime}$ 比同类中的其他样本更类似于实际样本 $x$ 的信息。为此，他们提出了另一种算法，称为对抗 logit 配对。
- en: 'For a model $f(.)$ trained on a mini-batch $\Gamma$ of clean examples $\{x_{1},x_{2},...,x_{m}\}$
    and corresponding adversarial examples $\{x^{\prime}_{1},x^{\prime}_{2},...,x^{\prime}_{m}\}$,
    with $f(x)$ mapping the input to a prediction. With $\mathcal{L}(\Gamma,\theta)$
    being the cost function used to perform the adversarial training, the adversarial
    logit pairing consists of minimizing the loss:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个在干净样本 $\{x_{1},x_{2},...,x_{m}\}$ 和对应的对抗样本 $\{x^{\prime}_{1},x^{\prime}_{2},...,x^{\prime}_{m}\}$
    的小批量 $\Gamma$ 上训练的模型 $f(.)$，$f(x)$ 将输入映射到预测。使用 $\mathcal{L}(\Gamma,\theta)$ 作为对抗训练的成本函数，对抗
    logit 配对包括最小化损失：
- en: '|  | $\mathcal{L}(\Gamma,\theta)+\lambda\frac{1}{m}\sum_{i=1}^{m}\mathcal{L}(f(x_{i}),f(x^{\prime}_{i}))$
    |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(\Gamma,\theta)+\lambda\frac{1}{m}\sum_{i=1}^{m}\mathcal{L}(f(x_{i}),f(x^{\prime}_{i}))$
    |  |'
- en: 4.1.7 ME-Net
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.7 ME-Net
- en: ME-Net [[101](#bib.bib101)] introduces the concept of utilizing matrix estimation
    as a way to augment the adversarial sample size and eliminate the perturbations
    from adversarial examples.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ME-Net [[101](#bib.bib101)] 引入了利用矩阵估计来扩展对抗样本数量并消除对抗样本中的扰动的概念。
- en: The training procedure to ME-Net is described as follows, the algorithm creates
    a mask in which each pixel is preserved with probability $p$, and set to zero
    with probability $1-p$. For each input image $x$, $n$ masks are applied, with
    different pixel drop probability. The generated masked images, $X$, are then processed
    through a Matrix Estimation algorithm, which would obtain the reconstructed images
    $\hat{X}$. The DNN model is trained on the reconstructed images, which can be
    further processed with the use of more adversarial training techniques. For inference,
    to each input $x$, a mask is randomly sampled from the pool of masks obtained
    in the training time, applied to $x$, and then reconstruct to generate $\hat{x}$.
    The process of masking and reconstructing the images is claimed by the authors
    to reduce the effects of adversarial perturbations in the image.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ME-Net 的训练过程描述如下：算法创建一个掩码，其中每个像素以概率 $p$ 保留，以概率 $1-p$ 设为零。对于每个输入图像 $x$，应用 $n$
    个掩码，每个掩码具有不同的像素丢失概率。生成的掩码图像 $X$ 经过矩阵估计算法处理，得到重建图像 $\hat{X}$。DNN 模型在重建图像上进行训练，之后可以使用更多对抗训练技术进一步处理。对于推理，每个输入
    $x$ 从训练时获得的掩码池中随机抽取一个掩码，应用于 $x$，然后重建生成 $\hat{x}$。作者声称，这种掩码和重建图像的过程可以减少对抗扰动的影响。
- en: 4.1.8 Robust Dynamic Inference Networks
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.8 强健的动态推理网络
- en: In [[99](#bib.bib99)], an input-adaptive dynamic inference model to the adversarial
    defense is proposed. In this method each input, regardless of clean or adversarial
    samples, adaptively chooses which output layer to take for its prediction. Therefore,
    a large portion of input inferences can be terminated early when the samples can
    already be inferred with high confidence. The benefit of the use of such models
    comes from the fact that the multiple sources of losses provide much larger flexibility
    to compose attacks (and defenses), compared to the typical framework. In this
    work, a methodology to attack and defend in such models is proposed.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[99](#bib.bib99)] 中，提出了一种针对对抗防御的输入自适应动态推理模型。在这种方法中，每个输入，无论是干净样本还是对抗样本，自适应地选择哪个输出层用于预测。因此，当样本已能以高置信度进行推断时，大部分输入推理可以提前终止。使用此类模型的好处在于，相较于典型框架，多源损失提供了更大的灵活性以组合攻击（和防御）。在这项工作中，提出了一种在此类模型中攻击和防御的方法论。
- en: 4.1.9 Defending against Occlusion Attacks
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.9 防御遮挡攻击
- en: In [[97](#bib.bib97)], the authors investigate defenses against physically realizable
    attacks, more specifically they investigate defenses to attacks in which part
    of the object is occluded by a physical patch. It’s been demonstrated that adversarial
    training with either PGD or Randomized Smoothing did not improve the robustness
    of the models significantly. In their work, they propose, implement, and use a
    Rectangular Occlusion Attack(ROA). The ROA enabled the emulation of physical attacks
    in the virtual world and the consequent training of adversarial resistant models.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[97](#bib.bib97)]中，作者研究了针对物理上可实现攻击的防御，具体来说，他们研究了在物体的一部分被物理补丁遮挡的攻击防御。研究表明，无论是使用PGD还是随机平滑的对抗训练，都没有显著提高模型的鲁棒性。在他们的工作中，他们提出、实现并使用了一种矩形遮挡攻击（ROA）。ROA使得在虚拟世界中模拟物理攻击成为可能，从而训练出对抗性强的模型。
- en: 4.1.10 Robust Local Features for Improving the generalization of Adversarial
    Training
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.10 提高对抗训练泛化能力的鲁棒局部特征
- en: In their research,Song et al. [[96](#bib.bib96)] investigates the fact that
    when models are trained with no adversarial techniques they gather better information
    about local features, which improves the model’s generalization ability. Opposed,
    DNN models which were adversarially trained tend to have a bias in favor of a
    global understanding of the features. In their work, they propose a method to
    train adversarial robust models, which are biased towards local features. In their
    work they define the Random Block Shuffle, to randomize the features of input
    inside the image. Such an approach prevents the adversarial learning method from
    learning only global features. The model learns from a combination of shuffled
    and unshuffled images.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的研究中，Song等人[[96](#bib.bib96)]研究了在没有对抗技术的情况下训练模型能够更好地收集局部特征信息，从而提高模型的泛化能力。相反，经过对抗训练的DNN模型往往偏向于对特征的全局理解。在他们的工作中，他们提出了一种训练对抗性鲁棒模型的方法，该模型偏向于局部特征。在他们的研究中，他们定义了随机块混洗（Random
    Block Shuffle）方法，用于随机化图像内的输入特征。这种方法防止对抗学习方法仅学习全局特征。模型通过混洗和未混洗的图像组合进行学习。
- en: '|  | <math   alttext="\displaystyle\begin{split}\mathcal{L}_{RLFAT_{P}}(F;x,y)=&amp;\mathcal{L}_{PGDAT}^{RLFL}(F;x,y)+\\
    &amp;\eta\mathcal{L}_{PGDAT}^{RLFT}(F;x,y)\\'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\displaystyle\begin{split}\mathcal{L}_{RLFAT_{P}}(F;x,y)=&amp;\mathcal{L}_{PGDAT}^{RLFL}(F;x,y)+\\
    &amp;\eta\mathcal{L}_{PGDAT}^{RLFT}(F;x,y)\\'
- en: \mathcal{L}_{RLFAT_{T}}(F;x,y)=&amp;\mathcal{L}_{TRADES}^{RLFL}(F;x,y)+\\
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: \mathcal{L}_{RLFAT_{T}}(F;x,y)=&amp;\mathcal{L}_{TRADES}^{RLFL}(F;x,y)+\\
- en: '&amp;\eta\mathcal{L}_{TRADES}^{RLFT}(F;x,y)\end{split}" display="inline"><semantics
    ><mtable columnspacing="0pt" rowspacing="0pt" ><mtr  ><mtd columnalign="right"  ><mrow
    ><mrow ><msub  ><mi >ℒ</mi><mrow ><mi >R</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >L</mi><mo lspace="0em" rspace="0em" >​</mo><mi >F</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >A</mi><mo lspace="0em" rspace="0em" >​</mo><msub ><mi  >T</mi><mi
    >P</mi></msub></mrow></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><mi >F</mi><mo  >;</mo><mi >x</mi><mo >,</mo><mi  >y</mi><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo></mrow></mtd><mtd columnalign="left"  ><mrow ><mrow ><msubsup  ><mi >ℒ</mi><mrow
    ><mi >P</mi><mo lspace="0em" rspace="0em" >​</mo><mi >G</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >D</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >A</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >T</mi></mrow><mrow ><mi >R</mi><mo lspace="0em" rspace="0em" >​</mo><mi >L</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >F</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >L</mi></mrow></msubsup><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><mi >F</mi><mo >;</mo><mi  >x</mi><mo >,</mo><mi >y</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo >+</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow  ><mi >η</mi><mo lspace="0em" rspace="0em"  >​</mo><msubsup ><mi >ℒ</mi><mrow
    ><mi >P</mi><mo lspace="0em" rspace="0em" >​</mo><mi >G</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >D</mi><mo lspace="0em" rspace="0em" >​</mo><mi >A</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >T</mi></mrow><mrow ><mi  >R</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >L</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >F</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >T</mi></mrow></msubsup><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><mi >F</mi><mo  >;</mo><mi >x</mi><mo >,</mo><mi  >y</mi><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><mrow ><msub  ><mi >ℒ</mi><mrow ><mi >R</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >L</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >F</mi><mo lspace="0em" rspace="0em" >​</mo><mi >A</mi><mo lspace="0em" rspace="0em"
    >​</mo><msub ><mi >T</mi><mi >T</mi></msub></mrow></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><mi >F</mi><mo  >;</mo><mi >x</mi><mo >,</mo><mi  >y</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >=</mo></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><msubsup  ><mi >ℒ</mi><mrow ><mi >T</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >R</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >A</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >D</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >E</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >S</mi></mrow><mrow ><mi >R</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >L</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >F</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >L</mi></mrow></msubsup><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mi >F</mi><mo >;</mo><mi  >x</mi><mo
    >,</mo><mi >y</mi><mo stretchy="false" >)</mo></mrow></mrow><mo >+</mo></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow  ><mi >η</mi><mo lspace="0em" rspace="0em"  >​</mo><msubsup
    ><mi >ℒ</mi><mrow ><mi >T</mi><mo lspace="0em" rspace="0em" >​</mo><mi >R</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >A</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >D</mi><mo lspace="0em" rspace="0em" >​</mo><mi >E</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >S</mi></mrow><mrow ><mi  >R</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >L</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >F</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >T</mi></mrow></msubsup><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><mi >F</mi><mo  >;</mo><mi >x</mi><mo >,</mo><mi  >y</mi><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ℒ</ci><apply  ><ci >𝑅</ci><ci >𝐿</ci><ci >𝐹</ci><ci >𝐴</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑇</ci><ci >𝑃</ci></apply></apply></apply><list
    ><ci >𝐹</ci><ci  >𝑥</ci><ci >𝑦</ci></list></apply><apply ><apply ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ℒ</ci><apply ><ci >𝑃</ci><ci >𝐺</ci><ci >𝐷</ci><ci >𝐴</ci><ci >𝑇</ci></apply></apply><apply
    ><ci >𝑅</ci><ci >𝐿</ci><ci >𝐹</ci><ci >𝐿</ci></apply></apply><list ><ci  >𝐹</ci><ci
    >𝑥</ci><ci >𝑦</ci></list></apply><apply ><ci >𝜂</ci><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ℒ</ci><apply ><ci  >𝑃</ci><ci >𝐺</ci><ci >𝐷</ci><ci >𝐴</ci><ci >𝑇</ci></apply></apply><apply
    ><ci >𝑅</ci><ci >𝐿</ci><ci >𝐹</ci><ci >𝑇</ci></apply></apply><list ><ci  >𝐹</ci><ci
    >𝑥</ci><ci >𝑦</ci></list><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ℒ</ci><apply ><ci  >𝑅</ci><ci >𝐿</ci><ci >𝐹</ci><ci >𝐴</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑇</ci><ci >𝑇</ci></apply></apply></apply><list ><ci >𝐹</ci><ci  >𝑥</ci><ci >𝑦</ci></list></apply></apply></apply><apply
    ><apply ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ℒ</ci><apply ><ci >𝑇</ci><ci
    >𝑅</ci><ci >𝐴</ci><ci >𝐷</ci><ci >𝐸</ci><ci >𝑆</ci></apply></apply><apply ><ci
    >𝑅</ci><ci >𝐿</ci><ci >𝐹</ci><ci >𝐿</ci></apply></apply><list ><ci  >𝐹</ci><ci
    >𝑥</ci><ci >𝑦</ci></list></apply><apply ><ci >𝜂</ci><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ℒ</ci><apply ><ci  >𝑇</ci><ci >𝑅</ci><ci >𝐴</ci><ci >𝐷</ci><ci >𝐸</ci><ci >𝑆</ci></apply></apply><apply
    ><ci >𝑅</ci><ci >𝐿</ci><ci >𝐹</ci><ci >𝑇</ci></apply></apply><list ><ci  >𝐹</ci><ci
    >𝑥</ci><ci >𝑦</ci></list></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle\begin{split}\mathcal{L}_{RLFAT_{P}}(F;x,y)=&\mathcal{L}_{PGDAT}^{RLFL}(F;x,y)+\\
    &\eta\mathcal{L}_{PGDAT}^{RLFT}(F;x,y)\\ \mathcal{L}_{RLFAT_{T}}(F;x,y)=&\mathcal{L}_{TRADES}^{RLFL}(F;x,y)+\\
    &\eta\mathcal{L}_{TRADES}^{RLFT}(F;x,y)\end{split}</annotation></semantics></math>
    |  | (12) |'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;\eta\mathcal{L}_{TRADES}^{RLFT}(F;x,y)\end{split}'
- en: 'In the loss defined in [Equation 12](#S4.E12 "12 ‣ 4.1.10 Robust Local Features
    for Improving the generalization of Adversarial Training ‣ 4.1 Defending through
    Adversarial (re)Training ‣ 4 Defense Mechanisms based on Robust Optimization Against
    Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey"), the factor $\eta$ balances the contribution between the
    local feature-oriented loss and the global oriented.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在[方程 12](#S4.E12 "12 ‣ 4.1.10 提高对抗训练泛化能力的鲁棒局部特征 ‣ 4.1 通过对抗（再）训练进行防御 ‣ 4 基于鲁棒优化的对抗攻击防御机制
    ‣ 深度学习对抗鲁棒性中的机遇与挑战：综述")中定义的损失函数中，因子 $\eta$ 在局部特征导向损失和全局导向损失之间平衡贡献。
- en: 4.1.11 Misclassification Aware Adversarial Training
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.11 误分类感知对抗训练
- en: In [[95](#bib.bib95)], the authors propose the analysis of the misclassification
    examples, intending to improve the accuracy of the model against perturbed inputs.
    To perform such a study they’ve trained a classifier with 10-step PGD, obtaining
    87% training accuracy, they extracted the 13% misclassified examples and sampled
    13% correctly classified examples from the training dataset. The examples originally
    misclassified by the model, are the ones that impact the most the final robustness.
    Compared to standard adversarial training the final robustness drops drastically
    if misclassified examples are not perturbed during adversarial training. In contrast,
    the same operation on sampled correctly classified examples only slightly affects
    the final robustness.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[95](#bib.bib95)] 中，作者提出了对误分类示例的分析，旨在提高模型对扰动输入的准确性。为了进行这样的研究，他们用 10 步 PGD
    训练了一个分类器，获得了 87% 的训练准确率，提取了 13% 的误分类示例，并从训练数据集中抽取了 13% 的正确分类示例。模型原本误分类的示例，是对最终鲁棒性影响最大的。与标准对抗训练相比，如果误分类示例在对抗训练期间没有被扰动，最终鲁棒性会大幅下降。相比之下，对抽样的正确分类示例进行相同操作，仅对最终鲁棒性有轻微影响。
- en: 'Based on the observations a regularization term is proposed to incorporate
    an explicit differentiation of misclassified examples. Initially they propose
    a Boosted Cross Entropy loss, defined as:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 基于观察结果，提出了一种正则化项，以明确区分误分类示例。最初，他们提出了一种加权交叉熵损失，定义为：
- en: '|  | $\displaystyle\text{BCE}(p(\hat{x}_{i}^{\prime},\theta),y_{i})=$ | $\displaystyle-log(p_{y_{i}}(\hat{x}_{i}^{\prime},\theta))$
    |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{BCE}(p(\hat{x}_{i}^{\prime},\theta),y_{i})=$ | $\displaystyle-log(p_{y_{i}}(\hat{x}_{i}^{\prime},\theta))$
    |  |'
- en: '|  |  | $\displaystyle-log(1-\max_{k\neq y_{i}}p_{t}(\hat{x}_{i}^{\prime},\theta))$
    |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-log(1-\max_{k\neq y_{i}}p_{t}(\hat{x}_{i}^{\prime},\theta))$
    |  |'
- en: 'in which $p_{t}(\hat{x}_{i}^{\prime},\theta)$ is the softmax on logits of $x_{i}$
    belonging to class $t$. With that the objective function is defined as:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中，$p_{t}(\hat{x}_{i}^{\prime},\theta)$ 是 $x_{i}$ 属于类别 $t$ 的 logits 上的 softmax。这样，目标函数被定义为：
- en: '|  | $\displaystyle\mathcal{L}^{\text{MART}}(\theta)$ | $\displaystyle=\frac{1}{n}\sum^{n}_{i=1}\mathcal{L}(x_{i},y_{i},\theta)$
    |  |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}^{\text{MART}}(\theta)$ | $\displaystyle=\frac{1}{n}\sum^{n}_{i=1}\mathcal{L}(x_{i},y_{i},\theta)$
    |  |'
- en: '|  | $\displaystyle\mathcal{L}(x_{i},y_{i},\theta)$ | $\displaystyle:=BCE(p(\hat{x}_{i}^{\prime},\theta),y_{i}))$
    |  |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}(x_{i},y_{i},\theta)$ | $\displaystyle:=BCE(p(\hat{x}_{i}^{\prime},\theta),y_{i}))$
    |  |'
- en: '|  |  | $\displaystyle+\lambda\text{KL}(p(x_{i},\theta)&#124;&#124;p(\hat{x}_{i}^{\prime},\theta),y_{i}))(1-p_{y_{i}}(x_{i},\theta))$
    |  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\lambda\text{KL}(p(x_{i},\theta)&#124;&#124;p(\hat{x}_{i}^{\prime},\theta),y_{i}))(1-p_{y_{i}}(x_{i},\theta))$
    |  |'
- en: 4.1.12 Input Transformation Methods
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.12 输入变换方法
- en: 'Input transformation methods propose to train the network in transformed images,
    such as bit-depth reduction, JPEG compression, total variance minimization, and
    image quilting or an ensemble of these methods to improve the robustness of the
    model.In [[128](#bib.bib128)], the use of JPEG compression was proposed as a countermeasure
    of the pixel displacement generated by the adversarial attacks. In [[129](#bib.bib129)],
    a combination of total variation minimization and image quilting is used to defend
    against strong attacks. Even-though these transformations are nonlinear, a neural
    network was used to approximate the transformations, making them differentiable,
    and consequently easier to obtain the gradient. Different in [[130](#bib.bib130)],
    an ensemble of weak transformation defenses was proposed to improve the robustness
    of the models, among the transformations included in the defense are: color precision
    reduction, JPEG noise, Swirl, Noise Injection, FFT perturbation, Zoom Group, Color
    Space Group, Contrast Group, Grey Scale Group, and Denoising Group.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 输入变换方法提出在变换后的图像中训练网络，如位深度减少、JPEG 压缩、总方差最小化、图像拼接或这些方法的组合，以提高模型的鲁棒性。在 [[128](#bib.bib128)]
    中，建议使用 JPEG 压缩作为对抗攻击产生的像素位移的对策。在 [[129](#bib.bib129)] 中，结合总变差最小化和图像拼接来抵御强攻击。尽管这些变换是非线性的，但使用神经网络来近似这些变换，使其可微，从而更容易获得梯度。在
    [[130](#bib.bib130)] 中，提出了一种弱变换防御的组合，以提高模型的鲁棒性，包括的变换有：颜色精度减少、JPEG 噪声、旋涡、噪声注入、FFT
    扰动、缩放组、颜色空间组、对比度组、灰度组和去噪组。
- en: 'TABLE III: Results of defenses based on Adversarial (re)Training and Regularization'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：基于对抗（再）训练和正则化的防御结果
- en: '| ADVERSARIAL (RE)TRAINING |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 对抗（再）训练 |'
- en: '| Publication | Architecture | Dataset | Norm | Adversarial | $\epsilon$ |
    Error Rate |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 发表的文献 | 架构 | 数据集 | 范数 | 对抗性 | $\epsilon$ | 错误率 |'
- en: '| Goodfellow et al. 2015 [[56](#bib.bib56)] | DNN | MNIST | $l_{2}$ | L-BFGS
    | 0.25 | 17.9% |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| Goodfellow 等人 2015 [[56](#bib.bib56)] | DNN | MNIST | $l_{2}$ | L-BFGS |
    0.25 | 17.9% |'
- en: '| Tramer et al. 2018 [[107](#bib.bib107)] | Inception ResNet v2 | ImageNet
    | $l_{\infty}$ | Step-LL | 16/256 | 7.9% |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| Tramer 等人 2018 [[107](#bib.bib107)] | Inception ResNet v2 | ImageNet | $l_{\infty}$
    | Step-LL | 16/256 | 7.9% |'
- en: '| Madry et al. 2018 [[54](#bib.bib54)] | ResNet | CIFAR-10 | $l_{2}$ | PGD
    | 8 | 54.2% |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| Madry 等人 2018 [[54](#bib.bib54)] | ResNet | CIFAR-10 | $l_{2}$ | PGD | 8
    | 54.2% |'
- en: '| Liu el al. 2018 [[106](#bib.bib106)] | VGG16 | CIFAR-10 | $l_{\infty}$ |
    C&W | 8/256 | 10% |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 2018 [[106](#bib.bib106)] | VGG16 | CIFAR-10 | $l_{\infty}$ | C&W
    | 8/256 | 10% |'
- en: '| Sen et al. 2020 [[105](#bib.bib105)] | AlexNet | ImageNet | $l_{\infty}$
    | C&W | 0.3 | 70.64% |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Sen 等人 2020 [[105](#bib.bib105)] | AlexNet | ImageNet | $l_{\infty}$ | C&W
    | 0.3 | 70.64% |'
- en: '| Kannan et al. 2018 [[102](#bib.bib102)] | ResNet-101 | ImageNet | $l_{2}$
    | PGD | 12/255 | 55.60% |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Kannan 等人 2018 [[102](#bib.bib102)] | ResNet-101 | ImageNet | $l_{2}$ | PGD
    | 12/255 | 55.60% |'
- en: '| Hu et al. 2020 [[99](#bib.bib99)] | ResNet38 | CIFAR-10 | $l_{2}$ | PGD |
    8/255 | 30.29% |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| Hu 等人 2020 [[99](#bib.bib99)] | ResNet38 | CIFAR-10 | $l_{2}$ | PGD | 8/255
    | 30.29% |'
- en: '| Wong et al. 2020 [[98](#bib.bib98)] | PreAct ResNet18 | ImageNet | $l_{\infty}$
    | R-FGSM | 2/255 | 56.7% |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| Wong 等人 2020 [[98](#bib.bib98)] | PreAct ResNet18 | ImageNet | $l_{\infty}$
    | R-FGSM | 2/255 | 56.7% |'
- en: '| Song et al. 2019 [[96](#bib.bib96)] | ResNet w32-10 | CIFAR-100 | $l_{\infty}$
    | PGD | 0.03 | 68.01% |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| Song 等人 2019 [[96](#bib.bib96)] | ResNet w32-10 | CIFAR-100 | $l_{\infty}$
    | PGD | 0.03 | 68.01% |'
- en: '| Wang et al. 2020 [[95](#bib.bib95)] | ResNet-18 | CIFAR-10 | $l_{\infty}$
    | PGD | 8/255 | 45.13% |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 2020 [[95](#bib.bib95)] | ResNet-18 | CIFAR-10 | $l_{\infty}$ | PGD
    | 8/255 | 45.13% |'
- en: '| REGULARIZATION APPROACH |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 正则化方法 |'
- en: '| Publication | Architecture | Dataset | Norm | Adversarial | $\epsilon$ |
    Error Rate |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 发表的文献 | 架构 | 数据集 | 范数 | 对抗性 | $\epsilon$ | 错误率 |'
- en: '| Cisse et al. 2017 [[125](#bib.bib125)] | ResNet | CIFAR-100 | SNR($x$,$\delta$),
    $l_{\infty}$ | FGSM | 33 | 47.4% |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| Cisse 等人 2017 [[125](#bib.bib125)] | ResNet | CIFAR-100 | SNR($x$,$\delta$),
    $l_{\infty}$ | FGSM | 33 | 47.4% |'
- en: '| Yan et al. 2018 [[123](#bib.bib123)] | ResNet | ImageNet | $l_{2}$ | FGSM
    | 2.43E-3 | 50% |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Yan 等人 2018 [[123](#bib.bib123)] | ResNet | ImageNet | $l_{2}$ | FGSM | 2.43E-3
    | 50% |'
- en: '| Zhang et al. 2019 [[121](#bib.bib121)] | ResNet | CIFAR-10 | $l_{\infty}$
    | C&W | 3.1E-2 | 18.76% |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等人 2019 [[121](#bib.bib121)] | ResNet | CIFAR-10 | $l_{\infty}$ | C&W
    | 3.1E-2 | 18.76% |'
- en: '| Xie et al. 2019 [[55](#bib.bib55)] | ResNet-152 | ImageNet | $l_{\infty}$
    | PGD | 16 | 57.4% |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| Xie 等人 2019 [[55](#bib.bib55)] | ResNet-152 | ImageNet | $l_{\infty}$ | PGD
    | 16 | 57.4% |'
- en: '| Mao et al. 2019 [[120](#bib.bib120)] | Modified LeNet | Tiny ImageNet | $l_{\infty}$
    | C&W | 8/255 | 82.52% |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| Mao 等人 2019 [[120](#bib.bib120)] | 修改版 LeNet | Tiny ImageNet | $l_{\infty}$
    | C&W | 8/255 | 82.52% |'
- en: '| Shafahi et al. 2019 [[131](#bib.bib131)] | Wide-ResNet-32 | CIFAR-100+$\rightarrow$10+
    | $l_{2}$ | PGD | 8 | 82.3% |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| Shafahi et al. 2019 [[131](#bib.bib131)] | Wide-ResNet-32 | CIFAR-100+$\rightarrow$10+
    | $l_{2}$ | PGD | 8 | 82.3% |'
- en: 4.2 Regularization Techniques
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 正则化技术
- en: 'As shown in [subsection 3.1](#S3.SS1 "3.1 White-box Attacks ‣ 3 Methods for
    Generating Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey"), several algorithms depend on the model’s gradient
    to estimate the local optima perturbations which will fool the classifier. A stream
    of research towards adversarial robust optimization, look into applying regularization
    approaches to reduce the influence of small perturbations in the input on the
    output decisions. In this section, we review some relevant publications in the
    field in which the author’s objective is to improve the robustness of the model.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '如在[subsection 3.1](#S3.SS1 "3.1 White-box Attacks ‣ 3 Methods for Generating
    Adversarial Attacks ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey")中所示，多个算法依赖于模型的梯度来估计局部最优扰动，从而欺骗分类器。针对对抗鲁棒优化的研究方向，关注于应用正则化方法以减少输入中小扰动对输出决策的影响。在本节中，我们回顾了该领域一些相关的出版物，作者的目标是提高模型的鲁棒性。'
- en: 4.2.1 Towards DNN Architectures Robust to Adversarial Examples
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 朝向对抗样本鲁棒的 DNN 架构
- en: In their work, Gu et. al. [[127](#bib.bib127)] propose the use of Contractive
    Autoencoder. To regularize the gradient, they add a penalty to the loss in the
    back-propagation concerning the partial derivatives at each layer. By incorporating
    a layer-wise contractive penalty (in partial derivatives), they show that adversarial
    generated from such networks have significantly higher distortion. In their approach,
    the network could still be fooled by adversarial examples, but the level of noise
    necessary to fool such a network is considerably higher than standard networks
    in which there is no contractive penalty.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的工作中，Gu 等人 [[127](#bib.bib127)] 提出了使用收缩自编码器。为了对梯度进行正则化，他们在反向传播中的损失函数中增加了一项对每一层的偏导数的惩罚。通过引入逐层收缩惩罚（在偏导数中），他们展示了这种网络生成的对抗样本具有显著更高的失真。在他们的方法中，网络仍可能被对抗样本欺骗，但欺骗这种网络所需的噪声水平明显高于没有收缩惩罚的标准网络。
- en: 4.2.2 Robust Large Margin Deep Neural Networks
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 鲁棒的大间隔深度神经网络
- en: The work presented in [[126](#bib.bib126)] analyzes the generalization error
    of DNN’s through their classification margin. In their work, they initially derive
    bounds to the Generalization Error (GE) (adversarial attacks as consequence) and
    express these bounds as a dependence of the model’s Jacobian Matrix (JM). In their
    work, it was shown that the depth of the architecture does not affect the existence
    of a GE bound, conditioned to fact that the spectral norm of the JM is also bounded,
    around the training inputs. With this definition a weigh and batch normalization
    regularizer is derived. The regularizer is based on the bound derived based on
    the JM.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[126](#bib.bib126)]中提出的工作分析了 DNN 的分类间隔的泛化误差。他们的工作最初推导了泛化误差（GE）的界限（作为对抗攻击的结果），并将这些界限表示为模型雅可比矩阵（JM）的依赖关系。在他们的研究中，显示了架构的深度不会影响
    GE 界限的存在，前提是 JM 的谱范数也有界，在训练输入周围。根据这个定义，导出了一个加权和批量归一化正则化器。这个正则化器基于基于 JM 推导的界限。
- en: 4.2.3 Input Gradient Regularization
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 输入梯度正则化
- en: Parseval networks [[125](#bib.bib125)] is a layer-wise regularization method
    to reduce the networks variability to small disturbances in the input $x$. The
    work starts with the principle that DNN’s are a composition of functions presented
    by layers. To keep the variability of the output controlled, they propose to maintain
    the Lipschitz constant small at every hidden layer (for fully connected, convolutional,
    or residual layer). For that, they analyze the spectral norm of the weight matrix.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: Parseval 网络 [[125](#bib.bib125)] 是一种逐层正则化方法，用于减少网络对输入 $x$ 中小扰动的变异性。该工作以 DNN
    是由层表示的函数的组合这一原则开始。为了控制输出的变异性，他们建议在每个隐藏层（对于全连接层、卷积层或残差层）保持小的 Lipschitz 常数。为此，他们分析了权重矩阵的谱范数。
- en: 'In [[124](#bib.bib124)], Ros et. al. based on the same principle of gradient
    regularization, propose the use of such techniques to improve both the robustness
    and interpretability of Deep Neural Networks. The authors claim that raw input
    gradients are what many attacks use to generate adversarial examples. Explanation
    techniques that smooth out gradients in background pixels may be inappropriately
    hiding the fact that the model is quite sensitive to them. They hypothesize that
    by training a model to have smooth input gradients with fewer extreme values,
    it would not only make the model more interpretable but also more resistant to
    adversarial examples. Their gradient regularization is given by:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[124](#bib.bib124)]中，Ros等基于相同的梯度正则化原理，提出使用这些技术来提高深度神经网络的鲁棒性和可解释性。作者声称，原始输入梯度是许多攻击生成对抗样本所使用的。平滑背景像素中的梯度的解释技术可能会不适当地掩盖模型对这些像素非常敏感的事实。他们假设，通过训练一个模型，使输入梯度平滑且极端值较少，这不仅会使模型更具可解释性，还能使其更具抵抗对抗样本的能力。他们的梯度正则化定义为：
- en: '|  | $\displaystyle\theta^{*}$ | $\displaystyle=\operatorname*{arg\,min}_{\theta}\sum^{N}_{n=1}\sum^{K}_{k=1}-y_{nk}log(f_{\theta}(X_{n})_{k})$
    |  |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{*}$ | $\displaystyle=\operatorname*{arg\,min}_{\theta}\sum^{N}_{n=1}\sum^{K}_{k=1}-y_{nk}log(f_{\theta}(X_{n})_{k})$
    |  |'
- en: '|  |  | $\displaystyle+\lambda\sum^{D}_{d=1}\sum^{N}_{n=1}(\frac{\partial}{\partial
    x_{d}}\sum_{k=1}^{K}-y_{nk}log(f_{\theta}(X_{n})_{k}))^{2}$ |  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\lambda\sum^{D}_{d=1}\sum^{N}_{n=1}(\frac{\partial}{\partial
    x_{d}}\sum_{k=1}^{K}-y_{nk}log(f_{\theta}(X_{n})_{k}))^{2}$ |  |'
- en: in which $\lambda$ specifies the penalty strength. The goal of this update is
    to ensure that if any input changes slightly the KL divergence between predictions
    will not change significantly.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\lambda$指定惩罚强度。这个更新的目标是确保如果输入略有变化，预测之间的KL散度不会发生显著变化。
- en: 4.2.4 DeepDefense
  id: totrans-356
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 DeepDefense
- en: 'Yan et al.[[123](#bib.bib123)] propose the algorithm called DeepDefense focusing
    on the improvement of the robustness of the DNN models, which is an regularization
    method based on the generated adversarial perturbation. Similar to adversarial
    (re)Training the algorithm incorporates the adversarial generation the loss function.
    It does not occur as shown in [Equation 3](#S2.E3 "3 ‣ 2.3 Defense Methods ‣ 2
    Taxonomy of Adversarial Attacks and Defenses ‣ Opportunities and Challenges in
    Deep Learning Adversarial Robustness: A Survey"), differently it is presented
    as:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 'Yan等人[[123](#bib.bib123)]提出了一种名为DeepDefense的算法，专注于提高DNN模型的鲁棒性，这是一种基于生成对抗扰动的正则化方法。与对抗（再）训练类似，该算法将对抗生成纳入了损失函数。然而，与[方程3](#S2.E3
    "3 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial Attacks and Defenses ‣ Opportunities
    and Challenges in Deep Learning Adversarial Robustness: A Survey")中所示的情况不同，它以如下方式呈现：'
- en: '|  | $\displaystyle\min_{\theta}\sum_{k}\mathcal{L}(x_{k},y_{k},\theta)+\lambda\sum_{k}R(-\frac{\left\&#124;\delta_{x_{k}}\right\&#124;_{p}}{\left\&#124;x_{k}\right\&#124;_{p}})$
    |  |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{\theta}\sum_{k}\mathcal{L}(x_{k},y_{k},\theta)+\lambda\sum_{k}R(-\frac{\left\|\delta_{x_{k}}\right\|_{p}}{\left\|x_{k}\right\|_{p}})$
    |  |'
- en: in which we see that at the same time that the loss is minimized a regularization
    term based on the perturbation $\delta_{x_{k}}$ is added to penalize the norm
    of the adversarial perturbations. The penalty function $R(.)$, treats the samples
    different depoending if they were correctly classified or not, it increases monotonically
    when the sample is correctly classified. With such a behavior the function gives
    preference to those parameter settings which are able to resist even to small
    $\frac{\left\|\delta_{x_{k}}\right\|_{p}}{\left\|x_{k}\right\|_{p}}$.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在最小化损失的同时，会添加一个基于扰动$\delta_{x_{k}}$的正则化项，以惩罚对抗扰动的范数。惩罚函数$R(.)$对样本的处理取决于它们是否被正确分类，当样本被正确分类时，它会单调增加。这样的行为使得函数更偏好那些能够抵抗小的$\frac{\left\|\delta_{x_{k}}\right\|_{p}}{\left\|x_{k}\right\|_{p}}$的参数设置。
- en: 4.2.5 TRADES
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5 TRADES
- en: 'In their work, Zhang et. al. [[121](#bib.bib121)] states the intrinsic trade-off
    between robustness and accuracy. In their work, they derive a differentiable upper
    bound for the natural and boundary errors of the DNN model. To derive such bound,
    the error generated by the adversarial examples (called robust error) is decomposed
    in two parts: 1 - the natural misclassification, and 2 - the boundary errors.
    The bounds are shown to be the tightest overall probability distributions. Based
    on these bounds a defense mechanism called TRADES is proposed. In its core, the
    algorithm still minimizes the natural loss, consequently increasing the accuracy
    of the model, but at the same time, it introduces a regularization term, which
    induces a shift of the decision boundaries away from the training data points.
    The expansion of the decision boundaries can be seen in [Figure 7](#S4.F7 "Figure
    7 ‣ 4.2.5 TRADES ‣ 4.2 Regularization Techniques ‣ 4 Defense Mechanisms based
    on Robust Optimization Against Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey").'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的研究中，张等人[[121](#bib.bib121)]阐述了鲁棒性与准确性之间的内在权衡。他们推导了 DNN 模型自然误差和边界误差的可微分上界。为了推导这样的界限，对抗性样本生成的误差（称为鲁棒误差）被分解为两部分：1
    - 自然误分类，2 - 边界误差。界限被证明是最紧凑的总体概率分布。基于这些界限，提出了一种称为 TRADES 的防御机制。其核心是算法仍然最小化自然损失，从而提高模型的准确性，但同时引入了一个正则化项，这使得决策边界从训练数据点中偏移。决策边界的扩展可以在[图
    7](#S4.F7 "图 7 ‣ 4.2.5 TRADES ‣ 4.2 正则化技术 ‣ 4 基于鲁棒优化的防御机制 ‣ 深度学习对抗鲁棒性中的机遇与挑战：调查")中看到。
- en: '![Refer to caption](img/89dbbc63e7745176ded64225a3810540.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/89dbbc63e7745176ded64225a3810540.png)'
- en: 'Figure 7: In the left we see the boundaries of a model trained with standard
    DNN. In the right the boundaries of the decision are pushed further from the data
    points, showing not so sharp transitions compared to naturally trained methods.
    Image Source: [[121](#bib.bib121)]'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：左侧是使用标准 DNN 训练的模型边界。右侧则是决策边界被推离数据点，显示出与自然训练方法相比，过渡不那么明显。图片来源：[[121](#bib.bib121)]
- en: 4.2.6 Metric Learning for adversarial Robustness
  id: totrans-364
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.6 对抗鲁棒性的度量学习
- en: '[[120](#bib.bib120)] focuses on learning a distance metric for the latent representation
    of the input. Through an empirical analysis, the authors have observed that inputs
    under PGD adversarial attacks shift its latent representation to a false class.
    The shift in the latent representations spread in the false class and become indistinguishable
    from the original images in the class. In their paper, they’ve added a new constraint
    to the model with metric learning. Their model implements a variation of the naive
    triplet loss, called TLA (triplet loss adversarial training), which overcome the
    variance of the adversarial data over the false class. The TLA works by approximating
    the samples from the same class, independently if they are adversarial or unperturbed
    samples, and enlarge the boundary distance concerning other classes.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '[[120](#bib.bib120)] 关注于学习输入的潜在表示的距离度量。通过实证分析，作者观察到在 PGD 对抗攻击下，输入的潜在表示会转移到错误类别中。这种潜在表示的转移在错误类别中扩散，并与该类别中的原始图像难以区分。在他们的论文中，他们在模型中加入了一个新的度量学习约束。他们的模型实现了名为
    TLA（triplet loss adversarial training）的朴素三重损失变体，克服了对抗数据在错误类别中的方差。TLA 的工作方式是近似来自同一类别的样本，无论是对抗样本还是未扰动样本，并扩大相对于其他类别的边界距离。'
- en: 4.2.7 Adversarially Robust Transfer Learning
  id: totrans-366
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.7 对抗性鲁棒转移学习
- en: In [[131](#bib.bib131)], a study on transfer learning for robust models is performed.
    In their paper, they robustly train a Wide-ResNet 32-10 [[132](#bib.bib132)] using
    the algorithm proposed in [[54](#bib.bib54)]. In their first experiment, they
    sub-divide the model in blocks of layers and analyze the impact of changing the
    number of unfrozen blocks in the transfer learning process. It was seen that when
    only the fully connected layer and the batch normalization blocks were re-trained,
    the network had similar or improved robustness in the new domain. Opposed, when
    more blocks were re-trained the accuracy and robustness dropped drastically. The
    authors claimed that the robust model’s feature extractors act as filters that
    ignore the irrelevant parts of the images.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[131](#bib.bib131)] 中，进行了有关鲁棒模型的迁移学习研究。在他们的论文中，他们使用 [[54](#bib.bib54)] 提出的算法鲁棒地训练了一个
    Wide-ResNet 32-10 [[132](#bib.bib132)]。在他们的第一个实验中，他们将模型细分为层块，并分析了在迁移学习过程中改变未冻结块数量的影响。结果发现，当仅重新训练全连接层和批量归一化块时，网络在新领域的鲁棒性相似或有所改善。相反，当更多块被重新训练时，准确性和鲁棒性急剧下降。作者声称，鲁棒模型的特征提取器充当了忽略图像中无关部分的过滤器。
- en: With the intent of improving the overall performance of classifiers transferred
    from a robust source model by improving their generalization on natural images,
    the authors proposed an end-to-end transfer learning model with a no-forgetting
    property. To do so, they only fine-tune the feature extraction parameters $\theta$.
    It consists basically of the addition of a regularization term to the loss of
    the model.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过提高自然图像的泛化能力来改善从鲁棒源模型迁移过来的分类器的整体性能，作者提出了一种具有不遗忘特性的端到端迁移学习模型。为此，他们仅微调特征提取参数
    $\theta$。基本上，这包括在模型损失中添加正则化项。
- en: '|  | $\min_{\theta,w}\mathcal{L}(f(x,\theta),y,w)+\lambda l_{p}(f(x,\theta),f_{0}(x,\theta^{*}))$
    |  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta,w}\mathcal{L}(f(x,\theta),y,w)+\lambda l_{p}(f(x,\theta),f_{0}(x,\theta^{*}))$
    |  |'
- en: 4.3 Certified Defenses
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 认证防御
- en: Certified defenses try to theoretically find certificates in distances or probability
    to certify the robustness of DNN models. These methods are explored in this section.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 认证防御试图在距离或概率上理论性地寻找证书以认证 DNN 模型的鲁棒性。本节将探讨这些方法。
- en: 4.3.1 Exact Methods
  id: totrans-372
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 精确方法
- en: The work of [[63](#bib.bib63)], was a big first step in the direction of formalizing
    methods to certify the robustness of neural networks. In their work, also, to
    provide a formulation for SMT solver for the ReLU activation function, they’ve
    proven safety in a small neural network for aircraft collision prediction. They
    were able to use their solver to prove/disprove local adversarial robustness for
    their example DNN for a few arbitrary combinations of input $x$ and disturbances
    $\delta$. In their experiments, the verification took from few seconds to a few
    hours depending on the size of the perturbation, with bigger perturbations taking
    longer to be verified.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[[63](#bib.bib63)] 的研究是正式化神经网络鲁棒性认证方法的重要一步。在他们的工作中，为了给 ReLU 激活函数提供 SMT 求解器的公式，他们在一个小型神经网络中证明了飞机碰撞预测的安全性。他们能够使用他们的求解器来证明/反驳他们的
    DNN 示例在某些任意输入 $x$ 和扰动 $\delta$ 组合下的局部对抗鲁棒性。在他们的实验中，验证时间从几秒到几小时不等，具体取决于扰动的大小，较大的扰动需要更长时间来验证。'
- en: 'Expanding over the Reluplex idea, the work in [[119](#bib.bib119)], provide
    a more complete framework to prove formulas over a DNN. The key concept of $AI^{2}$
    is the use of abstract interpretation to approximate mathematical functions with
    an infinity set of behaviors into logical functions witch are finite and consequently
    computable. To evaluate a DNN they propose to over-approximate the model in the
    abstract domain with the use of logical formulas capable of capturing certain
    shapes. [Figure 8](#S4.F8 "Figure 8 ‣ 4.3.1 Exact Methods ‣ 4.3 Certified Defenses
    ‣ 4 Defense Mechanisms based on Robust Optimization Against Adversarial Attacks
    ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey")
    shows the exemplification of how the abstraction of the layers is used to evaluate
    properties in the DNN.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '扩展了 Reluplex 思想，[[119](#bib.bib119)] 中的工作提供了一个更完整的框架，用于在 DNN 上证明公式。$AI^{2}$
    的关键概念是使用抽象解释，将具有无限行为集合的数学函数近似为有限且可计算的逻辑函数。为了评估 DNN，他们建议使用能够捕捉特定形状的逻辑公式，在抽象域中过度近似模型。[图8](#S4.F8
    "Figure 8 ‣ 4.3.1 Exact Methods ‣ 4.3 Certified Defenses ‣ 4 Defense Mechanisms
    based on Robust Optimization Against Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey") 展示了如何利用层的抽象来评估 DNN 的属性。'
- en: '![Refer to caption](img/b46846ec16e06c0d0899d9c70ee6b759.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/b46846ec16e06c0d0899d9c70ee6b759.png)'
- en: 'Figure 8: From left to right, initially the algorithm generates an abstract
    element which encompass all perturbed images. It propagates through all the abstractions
    of the layers. The verification is successful if all images are in the same classification
    group in the end.Image Source: [[119](#bib.bib119)]'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：从左到右，最初算法生成一个包含所有扰动图像的抽象元素。它通过所有层的抽象进行传播。如果最终所有图像都在相同的分类组中，则验证成功。图像来源：[[119](#bib.bib119)]
- en: Building on the work of [[119](#bib.bib119)], Singh et al. [[118](#bib.bib118)]
    published a work in which not only the ReLU activation function is available,
    but also Sigmoid and TanH. In their work, they implement a parallel version of
    the layer transformation which improved significantly the verification speed.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 [[119](#bib.bib119)] 的工作，Singh 等人 [[118](#bib.bib118)] 发表了一项工作，其中不仅提供了 ReLU
    激活函数，还包括 Sigmoid 和 TanH。他们在工作中实现了层转换的并行版本，显著提高了验证速度。
- en: 4.3.2 Estimating the lower bound
  id: totrans-378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 估计下界
- en: 'In their work,Hein et al. [[117](#bib.bib117)] the search of model robustness
    through the optics of formal guarantees. They’ve proposed in their work, a proven
    lower bound which establishes the minimal necessary perturbation to change a model’s
    decision. "We provide a guarantee that the classifier decision does not change
    in a certain ball around the considered instance" [[117](#bib.bib117)]. Moreover,
    based on the proposed Cross-Lipschitz Regularazation method, they show the increase
    in the adversarial robustness of the models trained with such regularization.
    The generated bound is defined as:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的工作中，Hein 等人 [[117](#bib.bib117)] 从形式保障的角度探索模型鲁棒性。他们在工作中提出了一个经过验证的下界，建立了改变模型决策所需的最小扰动。“我们提供了一个保证，即分类器决策在考虑的实例周围的某个球体内不会改变”[[117](#bib.bib117)]。此外，基于提出的交叉
    Lipschitz 正则化方法，他们展示了经过这种正则化训练的模型在对抗鲁棒性上的提高。生成的下界定义为：
- en: '|  | $\displaystyle\left\&#124;\delta\right\&#124;_{p}\leq\max_{\epsilon>0}\min\Psi$
    |  |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left\&#124;\delta\right\&#124;_{p}\leq\max_{\epsilon>0}\min\Psi$
    |  |'
- en: '|  | $\displaystyle\Psi=\{\min_{j\neq c}\frac{f_{c}(x)-f_{j}(x)}{\max_{y\in
    B_{p}(x,\epsilon)}\left\&#124;\nabla f_{c}(y)-\nabla f_{j}(y)\right\&#124;_{q}}\}$
    |  |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Psi=\{\min_{j\neq c}\frac{f_{c}(x)-f_{j}(x)}{\max_{y\in
    B_{p}(x,\epsilon)}\left\&#124;\nabla f_{c}(y)-\nabla f_{j}(y)\right\&#124;_{q}}\}$
    |  |'
- en: It is known that an exact solution for the optimization problem which leads
    to the certification of DNN’s is intractable for large networks. In [[116](#bib.bib116)],
    they propose CLEVER (Cross Lipschitz Extreme Value for nEtwork Robustness). In
    their work, the lower bound is defined as the minimum $\epsilon$ necessary to
    be added in the input to change the decision of the model in an adversary setting.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 已知对于大规模网络，导致 DNN 认证的优化问题的精确解是不可行的。在 [[116](#bib.bib116)] 中，他们提出了 CLEVER（用于网络鲁棒性的交叉
    Lipschitz 极值）。在他们的工作中，下界被定义为在对抗设置中改变模型决策所需添加的最小 $\epsilon$。
- en: Based on extreme value theory, the CLEVER metric is attack agnostic and is capable
    of estimating the lower bound for an attack to be effective in any model. But
    it restricts itself on providing a certification, it only provides an estimate
    of the lower bound. Improving from CLEVER, [[115](#bib.bib115)] provides a certified
    lower bound for multi-layer perceptrons restricted to ReLU activation. In [[114](#bib.bib114)],
    CROWN is proposed extending the exact certification to general activations. In
    [[113](#bib.bib113)] the same research group proposed the CNN-Cert a framework
    to certify more general DNN.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 基于极值理论，CLEVER度量标准对攻击具有不可知性，并能够估计任何模型中攻击有效的下限。但它局限于提供认证，它仅提供下限的估计。相较于CLEVER，[[115](#bib.bib115)]
    提供了一个针对限制在ReLU激活的多层感知机的认证下限。在[[114](#bib.bib114)]中，提出了CROWN，将精确认证扩展到一般激活函数。在[[113](#bib.bib113)]中，同一研究小组提出了CNN-Cert，一个框架来认证更一般的深度神经网络。
- en: In Zhang et al. [[133](#bib.bib133)], a mix between inner bound propagation
    and linear relaxation is proposed. Linear relaxation of Neural Networks is one
    of the most popular methods to provide certified defenses and uses linear programming
    to provide linear relaxation, also known as the (convex adversarial polytope).
    Even though such methods generate an implementation that is tractable and solvable,
    they are still very demanding of computational resources. IBP on the contrary
    is less complex and brings more efficiency when optimizing verifiable networks.
    Also known as interval bound propagation (IBP), are in general loose during the
    initial training phase which generates instabilities in the training and makes
    the model very sensitive to the hyper-parameters.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在张等人[[133](#bib.bib133)]的研究中，提出了一种内部界限传播和线性松弛的混合方法。神经网络的线性松弛是提供认证防御的最流行方法之一，使用线性规划来提供线性松弛，也称为（凸对抗多面体）。尽管这些方法生成了一个可处理且可解的实现，但仍然非常要求计算资源。相反，IBP较少复杂，在优化可验证网络时带来更多的效率。IBP，也称为区间界限传播（IBP），在初始训练阶段通常较为松弛，这会导致训练不稳定，并使模型对超参数非常敏感。
- en: 'The model proposed by Zhang et. al. in [[133](#bib.bib133)] unifies linear-relaxation
    and IBP. They generate a model which is very efficient for low output dimensions.
    It is used the convex relaxation in the backward pass of the bound, and the IBP
    in the forward bound pass of ther network. The optimization problem solved in
    CROWN-IBP can be defined as:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 张等人提出的模型[[133](#bib.bib133)] 统一了线性松弛和IBP。他们生成了一个在低输出维度上非常高效的模型。该模型在界限的反向传递中使用了凸松弛，而在网络的正向界限传递中使用了IBP。在CROWN-IBP中解决的优化问题可以定义为：
- en: '|  | $\displaystyle\min_{\theta}\mathbb{E}_{(x,y)\in\mathcal{D}}[\lambda\mathcal{L}(x,y,\theta)+(1-\lambda)\mathcal{L}(-(\Psi)+\Phi),y,\theta]$
    |  |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{\theta}\mathbb{E}_{(x,y)\in\mathcal{D}}[\lambda\mathcal{L}(x,y,\theta)+(1-\lambda)\mathcal{L}(-(\Psi)+\Phi),y,\theta]$
    |  |'
- en: '|  | $\displaystyle\Psi=(1-\beta)\underline{m}_{IBP}(x,\epsilon)$ |  |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Psi=(1-\beta)\underline{m}_{IBP}(x,\epsilon)$ |  |'
- en: '|  | $\displaystyle\Phi=\beta\underline{m}_{CROWN-IBP}(x,\epsilon)$ |  |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Phi=\beta\underline{m}_{CROWN-IBP}(x,\epsilon)$ |  |'
- en: in which $\Psi$ is the IBP bound, $\Phi$ is the CROWN-IBP bound, and $\underline{m}(x,\epsilon)$
    is the combination of both bounds.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 的对抗损失，其中 $\Psi$ 是IBP界限，$\Phi$ 是CROWN-IBP界限，$\underline{m}(x,\epsilon)$ 是这两个界限的组合。
- en: 4.3.3 Upper Bounding the adversarial Loss
  id: totrans-390
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 对抗损失的上界
- en: 'In the works of [[112](#bib.bib112)] and [[111](#bib.bib111)] the certification
    of robustness is searched through means of defining an upper bound for the adversarial
    loss. For an adversarial loss defined as:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[112](#bib.bib112)]和[[111](#bib.bib111)]的工作中，通过定义对抗损失的上界来寻找稳健性的认证。对于定义为：
- en: '|  | $\displaystyle\mathcal{L}_{adv}=$ | $\displaystyle\max_{x^{\prime}}\{max_{i\neq
    y}Z_{i}(x^{\prime})-Z_{y}(x^{\prime})\}$ |  |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{adv}=$ | $\displaystyle\max_{x^{\prime}}\{max_{i\neq
    y}Z_{i}(x^{\prime})-Z_{y}(x^{\prime})\}$ |  |'
- en: '|  |  | $\displaystyle\text{subj. to }x^{\prime}\in B_{\epsilon}(x)$ |  |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\text{subj. to }x^{\prime}\in B_{\epsilon}(x)$ |  |'
- en: both try to find a larger certificate $C(x,F)$ when compared to the loss of
    the perturbed example. If the certificate is smaller than 0, it is guaranteed
    that the true label will have the bigger score, and it can be stated that within
    this distance the model is safe. The works differ on the means to find the certificate.
    [[112](#bib.bib112)] transforms the problem into a linear programming problem.
    [[111](#bib.bib111)] derives the certificate using semidefinite programming. An
    upper-bound estimation based on statistical methods is also proposed in Webb et
    al. [[134](#bib.bib134)].
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 两者都试图找到一个比扰动样本的损失更大的认证 $C(x,F)$。如果认证值小于 0，可以保证真实标签会有更高的得分，并且可以声明在这个距离内模型是安全的。这些工作在寻找认证的方法上有所不同。[[112](#bib.bib112)]
    将问题转化为线性规划问题。[[111](#bib.bib111)] 使用半正定规划推导认证。Webb 等人 [[134](#bib.bib134)] 还提出了一种基于统计方法的上界估计。
- en: 4.3.4 Randomized Smoothing
  id: totrans-395
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 随机平滑
- en: Randomized Smoothing is a set of algorithms based on a mathematical formalist
    inspired in cryptography, differential privacy (DP). This set of algorithms explore
    the connection between DP and robustness against norm-bounded adversarial examples
    in ML.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 随机平滑是一组基于数学形式主义的算法，这些形式主义受到密码学、差分隐私（DP）的启发。这些算法探索了 DP 与对 ML 中范数界限对抗样本的鲁棒性之间的联系。
- en: 'A classifier $f:R^{d}\rightarrow[0,1]^{k}$ which maps the input $x$ with probability
    $[0,1]$ to any of the $d$ classes, is said to be $\epsilon$-robust at $x$ if:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分类器 $f:R^{d}\rightarrow[0,1]^{k}$，它将输入 $x$ 映射到任何 $d$ 类中的概率 $[0,1]$，如果它在 $x$
    处是 $\epsilon$-鲁棒的，则满足：
- en: '|  | $F(x+\delta)=c(x),\forall\delta:\left\&#124;\delta\right\&#124;\leq\epsilon$
    |  |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  | $F(x+\delta)=c(x),\forall\delta:\left\&#124;\delta\right\&#124;\leq\epsilon$
    |  |'
- en: 'moreover, if $f$ is $L-Lipschitz$ then $f$ is $\epsilon$-robust at $x$ with:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果 $f$ 是 $L$-Lipschitz 的，则 $f$ 在 $x$ 处是 $\epsilon$-鲁棒的，其公式为：
- en: '|  | $\epsilon=\frac{1}{2L}(P_{A}-P_{B})$ |  |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  | $\epsilon=\frac{1}{2L}(P_{A}-P_{B})$ |  |'
- en: 'where, $P_{A}=\max_{i\in[k]}f_{i}(x)$ and $P_{B}$ is the second-max. Neural
    networks are known for being non-lipschitz. Recent work, from Lecuyer et al. [[135](#bib.bib135)]
    have proposed to smoothing the classifier, as in:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$P_{A}=\max_{i\in[k]}f_{i}(x)$ 和 $P_{B}$ 是第二大值。神经网络通常被认为是非 Lipschitz 的。最近，Lecuyer
    等人 [[135](#bib.bib135)] 提出了对分类器进行平滑处理，如下所示：
- en: '|  | $\hat{f}(x)=\mathbb{E}_{Z\sim\mathcal{N}(0,I_{d})}(f(x+\sigma Z))$ |  |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{f}(x)=\mathbb{E}_{Z\sim\mathcal{N}(0,I_{d})}(f(x+\sigma Z))$ |  |'
- en: which is proven to be lipschitz.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点已被证明是 Lipschitz 的。
- en: 'Cohen et al. [[136](#bib.bib136)] propose the use of a smoothed classifier
    to generate a tighter certification bound. The proposed certification radius is
    defined by:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: Cohen 等人 [[136](#bib.bib136)] 提出了使用平滑分类器来生成更紧的认证界限。提出的认证半径定义为：
- en: '|  | $\epsilon=\frac{\sigma}{2}(\Phi^{-1}(\underline{P_{A}})-\Phi^{-1}(\bar{P_{B}})),$
    |  |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  | $\epsilon=\frac{\sigma}{2}(\Phi^{-1}(\underline{P_{A}})-\Phi^{-1}(\bar{P_{B}})),$
    |  |'
- en: 'where $\underline{P_{A}}$ the lower bound for the top class probability, $\bar{P_{B}}$
    is the upper bound probability for all other classes, and $\Phi^{-1}(.)$ is the
    inverse of the standard Gaussian cumulative distribution function. In which $\epsilon$
    is large when the noise level $\sigma$ is high, the probability of top class $A$
    is high, and the probability of each other class is low. [Figure 9](#S4.F9 "Figure
    9 ‣ 4.3.4 Randomized Smoothing ‣ 4.3 Certified Defenses ‣ 4 Defense Mechanisms
    based on Robust Optimization Against Adversarial Attacks ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey") shows the decision boundaries
    and the certification radius proposed by Cohen et al. [[136](#bib.bib136)].'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$\underline{P_{A}}$ 是顶级类别概率的下界，$\bar{P_{B}}$ 是所有其他类别的上界概率，$\Phi^{-1}(.)$
    是标准高斯累积分布函数的逆函数。 $\epsilon$ 当噪声水平 $\sigma$ 高、顶级类别 $A$ 的概率高以及其他每个类别的概率低时较大。 [图
    9](#S4.F9 "Figure 9 ‣ 4.3.4 Randomized Smoothing ‣ 4.3 Certified Defenses ‣ 4
    Defense Mechanisms based on Robust Optimization Against Adversarial Attacks ‣
    Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey")
    显示了 Cohen 等人 [[136](#bib.bib136)] 提出的决策边界和认证半径。'
- en: '![Refer to caption](img/943ddd7be3e898103e291d5bbae4b9dd.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/943ddd7be3e898103e291d5bbae4b9dd.png)'
- en: 'Figure 9: The smoothed classifier at an input x. In the left the decision boundaries
    for a classifier $f$, for each class represented in differente colours. The doted
    lines represents the sets of the distribution $\mathcal{N}(x,\sigma I)$. In the
    right the class probabilities, and the lower bound for class $p_{A}$ and the upper
    bound for each other class $p_{B}$. Image Source: [[136](#bib.bib136)]'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 输入 x 的平滑分类器。在左侧是分类器 $f$ 的决策边界，每个类别用不同的颜色表示。虚线表示分布 $\mathcal{N}(x,\sigma
    I)$ 的集合。在右侧是类别的概率，以及类别 $p_{A}$ 的下界和其他类别 $p_{B}$ 的上界。图片来源: [[136](#bib.bib136)]'
- en: In [[109](#bib.bib109)], the authors propose an attack-free and scalable method
    to train robust deep neural networks. They mostly build upon Randomized Smoothing.
    The randomized smoothing classifier is defined as $g(x)=\mathbb{E}_{\eta}f(x+\eta)$,
    in which $\eta\sim\mathcal{N}(0,\sigma^{2}\text{{I}})$. Different from adversarial
    training as proposed in the original technique, the authors propose to robustly
    optimize including certification radius as part of the defined objective.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[109](#bib.bib109)] 中，作者提出了一种无攻击和可扩展的方法来训练鲁棒的深度神经网络。他们主要基于随机平滑。随机平滑分类器定义为
    $g(x)=\mathbb{E}_{\eta}f(x+\eta)$，其中 $\eta\sim\mathcal{N}(0,\sigma^{2}\text{{I}})$。与原始技术中提出的对抗训练不同，作者提出鲁棒优化，包括认证半径作为定义目标的一部分。
- en: 4.3.5 MMR-Universal
  id: totrans-410
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.5 MMR-Universal
- en: 'In [[110](#bib.bib110)], a provable defense against all $l_{p}$-norms are proposed,
    for all $p\geq 1$. The case studied is the non-trivial case in which none of the
    $l_{p}$ balls are contained in the others. For an input with dimensions $d$, the
    robustness to an $l_{p}$ perturbation requires $d^{\frac{1}{p}-\frac{1}{q}}\epsilon_{q}>\epsilon_{p}>\epsilon_{q}$
    for $p<q$. In which $\epsilon_{p}$ is the radius of the ball defined by the $l_{p}$-norm.
    In their paper a minimal $l_{p}$ norm of the complement of the union of $l_{1}$,
    $l_{\infty}$ norm, and its convex hull is derived as:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[110](#bib.bib110)] 中，提出了一种对所有 $l_{p}$-范数的可证明防御方法，适用于所有 $p\geq 1$。研究的案例是非平凡的情况，即没有
    $l_{p}$ 球包含在其他球中。对于具有 $d$ 维的输入，$l_{p}$ 扰动的鲁棒性要求 $d^{\frac{1}{p}-\frac{1}{q}}\epsilon_{q}>\epsilon_{p}>\epsilon_{q}$，其中
    $p<q$。其中 $\epsilon_{p}$ 是由 $l_{p}$-范数定义的球的半径。在他们的论文中，导出了 $l_{p}$ 范数的最小值，它是 $l_{1}$、$l_{\infty}$
    范数及其凸包的并集的补集：
- en: '|  | $\min_{x\in\mathbb{R}^{d}\text{\textbackslash}C}\left\&#124;x\right\&#124;_{p}=\frac{\epsilon_{1}}{(\frac{\epsilon_{1}}{\epsilon_{\infty}}-\alpha+\alpha^{q})^{\frac{1}{q}}}$
    |  |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{x\in\mathbb{R}^{d}\text{\textbackslash}C}\left\&#124;x\right\&#124;_{p}=\frac{\epsilon_{1}}{(\frac{\epsilon_{1}}{\epsilon_{\infty}}-\alpha+\alpha^{q})^{\frac{1}{q}}}$
    |  |'
- en: 'for $C$ the convex hull formed by the union of the $l_{1}$ and $l_{\infty}$
    norm balls, $\alpha=\frac{\epsilon_{1}}{\epsilon_{\infty}}-\left\lfloor\frac{\epsilon_{1}}{\epsilon_{\infty}}\right\rfloor$,
    $\frac{1}{p}+\frac{1}{q}=1$, $d\geq 2$ and $\epsilon_{1}\in(\epsilon_{\infty},d\epsilon_{\infty})$.
    Based on this derivation, a lower bound for the robustness at point $x$ is defined
    and a regularizer term is expressed in term of the distances and the lower bound
    MMR-Unirversal. The regularizer is then incorporated in the loss function to improve
    the robustness of the model:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $C$，这是由 $l_{1}$ 和 $l_{\infty}$ 范数球的并集形成的凸包，$\alpha=\frac{\epsilon_{1}}{\epsilon_{\infty}}-\left\lfloor\frac{\epsilon_{1}}{\epsilon_{\infty}}\right\rfloor$，$\frac{1}{p}+\frac{1}{q}=1$，$d\geq
    2$，且 $\epsilon_{1}\in(\epsilon_{\infty},d\epsilon_{\infty})$。基于这个推导，定义了点 $x$ 的鲁棒性下界，并将正则项用距离和下界
    MMR-Unirversal 表示。然后将正则项纳入损失函数中，以提高模型的鲁棒性：
- en: '|  | $\displaystyle\mathcal{L}(x_{i},y_{i})=$ | $\displaystyle\frac{1}{T}\sum_{i=1}^{T}\mathcal{L}_{c-e}(f(x_{i}),y_{i})$
    |  |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}(x_{i},y_{i})=$ | $\displaystyle\frac{1}{T}\sum_{i=1}^{T}\mathcal{L}_{c-e}(f(x_{i}),y_{i})$
    |  |'
- en: '|  |  | $\displaystyle+MMR\text{-}Universal(x_{i})$ |  |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+MMR\text{-}Universal(x_{i})$ |  |'
- en: During the optimization, the regularizer aims at pushing both the polytope boundaries
    and the decision hyperplanes farther than $l_{\infty}$ and $l_{1}$ distances from
    the training point $x$, to achieve robustness close or better than $l_{\infty}$
    and $l_{1}$ respectively.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化过程中，正则项旨在将多面体边界和决策超平面推得比 $l_{\infty}$ 和 $l_{1}$ 距离更远，以实现接近或优于 $l_{\infty}$
    和 $l_{1}$ 的鲁棒性。
- en: 5 Challenges and Future Opportunities
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 挑战与未来机遇
- en: 'We’ve discussed and presented results in three methods for generating robust
    machine learning algorithms: adversarial (re)training, regularization, and certified
    defenses. The search for an optimal method to strengthen DL algorithms against
    adversaries has a solid structure but still requires a significant effort to achieve
    the major objective. While a great number of new algorithms have been published
    every year, both in the realm of attacks and defenses, no algorithm based on adversarial
    (re)Training or attack generation can claim to be the final and optimal method.
    At the speed new defenses arise, attackers exploit the gradient or other nuances
    from these defensive algorithms to generate their effective low norm perturbation.
    The reason for this arms race, is the fact that the defenses are not absolute,
    or that while trying to solve maximization in [Equation 3](#S2.E3 "3 ‣ 2.3 Defense
    Methods ‣ 2 Taxonomy of Adversarial Attacks and Defenses ‣ Opportunities and Challenges
    in Deep Learning Adversarial Robustness: A Survey"), only an empirical approximation
    is used, and no global optimality is achieved. Moreover as the algorithms fail
    to account for all possible scenarios, there will always be an example in the
    neighborhood of $x$, such that $\left\|x-x^{\prime}\right\|_{p}\leq\delta$.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '我们已经讨论并展示了生成鲁棒机器学习算法的三种方法：对抗性（再）训练、正则化和认证防御。寻找一种最优的方法来增强深度学习算法对抗敌手的能力具有明确的结构，但仍需要付出大量努力才能实现主要目标。尽管每年都有大量新算法问世，无论是在攻击还是防御领域，基于对抗性（再）训练或攻击生成的算法都无法宣称是最终的最优方法。随着新防御方法的不断出现，攻击者利用这些防御算法的梯度或其他细节来生成其有效的低范数扰动。这场军备竞赛的原因在于防御并不是绝对的，或者在尝试解决[方程式
    3](#S2.E3 "3 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial Attacks and Defenses
    ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey")中的最大化问题时，只使用了经验近似，而没有实现全局最优。此外，由于算法未能考虑所有可能的场景，始终会存在一个在$x$邻域中的例子，使得$\left\|x-x^{\prime}\right\|_{p}\leq\delta$。'
- en: 'Other than optimizing [Equation 3](#S2.E3 "3 ‣ 2.3 Defense Methods ‣ 2 Taxonomy
    of Adversarial Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning
    Adversarial Robustness: A Survey"), with an empirical solution, certified defense
    mechanisms present a formal alternative to achieve robust deep learning models,
    but this certified solution comes with the cost of efficiency, and high computational
    cost. It derives from [Equation 5](#S2.E5 "5 ‣ 2.3.2 Certified Defenses ‣ 2.3
    Defense Methods ‣ 2 Taxonomy of Adversarial Attacks and Defenses ‣ Opportunities
    and Challenges in Deep Learning Adversarial Robustness: A Survey"). The non-linear
    equality constraint for each layer of the neural network is unsolvable with standard
    techniques, and several methods have been proposed to achieve the optimal solution
    namely: linear relaxations, convex relaxations, interval propagation, abstract
    interpretation, mixed integer linear programming, or combination of these methods.
    We see great research opportunities and challenges in further improving such algorithms.
    They are in the early stages of development, in which the abstractions and formal
    approximations for the non-linearity constraints shown in [Equation 5](#S2.E5
    "5 ‣ 2.3.2 Certified Defenses ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial
    Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey") are not optimized for parallel computation as their numerical
    counterparts are. Such restriction makes it almost impractical to have such mechanisms
    to be applied in larger datasets. More in the topic, approximations of the optimization
    constraints either by a lower bound or upper bound has shown to speed-up the training
    process of such algorithms, but at the cost of having over-estimated bounds for
    the maximum allowed perturbation. These algorithms have not yet demonstrated high
    accuracy in large datasets corrupted by adversaries, or smaller datasets with
    high level of corruption. Part of the issue comes from the imposed convex relaxations.
    As they are not tight enough, it requires the verification algorithms to explore
    a bigger region, than actually necessary to verify the existence of adversarial
    examples in the neighborhood of the input.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '除了优化[方程3](#S2.E3 "3 ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial Attacks
    and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial Robustness:
    A Survey")的经验解决方案外，认证防御机制提供了一种正式的替代方案，以实现鲁棒的深度学习模型，但这种认证解决方案的代价是效率和高计算成本。它源于[方程5](#S2.E5
    "5 ‣ 2.3.2 Certified Defenses ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial
    Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey")。神经网络每一层的非线性等式约束用标准技术无法解决，已有若干方法被提出以实现**最优解**，即：线性松弛、凸松弛、区间传播、抽象解释、混合整数线性规划，或这些方法的组合。我们看到在进一步改进这些算法方面有巨大的研究机会和挑战。它们仍处于早期发展阶段，其中[方程5](#S2.E5
    "5 ‣ 2.3.2 Certified Defenses ‣ 2.3 Defense Methods ‣ 2 Taxonomy of Adversarial
    Attacks and Defenses ‣ Opportunities and Challenges in Deep Learning Adversarial
    Robustness: A Survey")中显示的非线性约束的抽象和正式近似尚未针对并行计算进行优化，而它们的数值对应物却已优化。这种限制使得在更大数据集上应用这些机制几乎不切实际。关于这个话题，通过下界或上界的优化约束的近似已显示出加速这些算法的训练过程，但代价是对允许的最大扰动的估计过高。这些算法尚未在被对手破坏的大数据集或高度破坏的小数据集上展示出高准确性。部分问题来自于施加的凸松弛。由于这些松弛不够紧凑，它需要验证算法探索比实际需要的更大区域，以验证输入邻域中对抗样本的存在。'
- en: Moreover as seen in [[67](#bib.bib67)], even certified defenses can be broken
    when a big enough disturbance is applied to the model. It is arguable that even
    with the rigorous mathematical formulation of the defenses and certifications
    the constraint imposed by $l_{p}$ norm is weak. Most of the models can not achieve
    certifications beyond $\epsilon=0.3$ disturbance in $l_{2}$ norm, while disturbances
    $\epsilon=4$ added to the target input are barely noticeable by human eyes, and
    $\epsilon=100$, when applied to the original image are still easily classified
    by humans as belonging to the same class. As discussed by many authors, the perception
    of multi-dimensional space by human eyes goes beyond what the $l_{p}$ norm is
    capable of capturing and synthesizing. It is yet to be proposed more comprehensive
    metrics and algorithms capable of capturing the correlation between pixels of
    an image or input data which can better translate to optimization algorithms how
    humans distinguish features of an input image. Such a metric would allow the optimization
    algorithms to have better intuition on the subtle variations introduced by adversaries
    in the input data.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如在[[67](#bib.bib67)]中所见，即使是认证防御也可能在对模型施加足够大的干扰时被突破。可以争辩的是，即使拥有严格的防御和认证数学公式，由$l_{p}$范数施加的约束也是薄弱的。大多数模型在$l_{2}$范数中不能获得超过$\epsilon=0.3$的认证，而添加到目标输入中的$\epsilon=4$扰动几乎无法被人眼察觉，而$\epsilon=100$的扰动，尽管应用于原始图像中，仍然很容易被人类分类为相同类别。正如许多作者所讨论的，人眼对多维空间的感知超出了$l_{p}$范数所能捕捉和合成的范围。尚需提出更全面的度量标准和算法，能够捕捉图像或输入数据中像素之间的相关性，这可以更好地转换为优化算法，使其理解人类如何区分输入图像的特征。这种度量标准将使优化算法对对手在输入数据中引入的微妙变化有更好的直观感受。
- en: As we seek to apply machine learning algorithms in safe-critical applications,
    a model that works most of the time is not enough to guarantee safety of such
    implementations. It is imperative to know the operational restrictions of the
    algorithm, and the level of corruption they can safely handle. For such, formally
    certifying the algorithms is crucial, but increasing the neighborhood around the
    input that the certification can be guaranteed is fundamental for the practical
    application of current available techiniques.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们寻求将机器学习算法应用于安全关键应用时，一个大多数时间都能工作的模型不足以保证此类实施的安全性。了解算法的操作限制及其可以安全处理的腐败水平是至关重要的。因此，正式认证算法是关键的，但扩大认证可以保证的输入周围的邻域对于当前可用技术的实际应用至关重要。
- en: 6 Conclusion
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'Training safe and robust DNN algorithms is essential for their usage in safe-critical
    applications. This paper studies strategies to implement adversary robustly trained
    algorithms towards guaranteeing safety in machine learning models. We initially
    provide a taxonomy to classify adversarial attacks and defenses. Moreover, we
    provide a mathematical formulation for the Robust Optimization problem and divide
    it into a set of sub-categories which are: Adversarial (re)Training, Regularization
    Approach, and Certified Defenses. With the objective of elucidating methods to
    approximate the maximization problem, we present the most recent and important
    results in adversarial example generation. Furthermore, we describe several defense
    mechanisms that incorporate adversarial (re)Training as their main defense against
    perturbations or add regularization terms that change the behavior of the gradient,
    making it harder for attackers to achieve their objective. In addition, we surveyed
    methods which formally derive certificates of robustness by exactly solving the
    optimization problem or by approximations using upper or lower bounds.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 训练安全且稳健的深度神经网络（DNN）算法对于在安全关键应用中的使用至关重要。本文研究了实现对抗性鲁棒训练算法的策略，以保证机器学习模型的安全性。我们首先提供了一个分类系统，以对抗性攻击和防御进行分类。此外，我们提供了鲁棒优化问题的数学公式，并将其分为一组子类别，包括：对抗性（再）训练、正则化方法和认证防御。为了阐明近似最大化问题的方法，我们展示了对抗性示例生成中的最新和重要结果。此外，我们描述了几种防御机制，这些机制将对抗性（再）训练作为主要防御措施，以抵御扰动，或者添加正则化项，这些项改变了梯度的行为，使攻击者更难实现他们的目标。此外，我们调查了通过精确解决优化问题或通过使用上界或下界的近似来正式推导鲁棒性证书的方法。
- en: References
  id: totrans-424
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y. LeCun, Y. Bengio, 和 G. Hinton, “深度学习,” *nature*, vol. 521, no. 7553,
    pp. 436–444, 2015.'
- en: '[2] H. Greenspan, B. Van Ginneken, and R. M. Summers, “Guest editorial deep
    learning in medical imaging: Overview and future promise of an exciting new technique,”
    *IEEE Transactions on Medical Imaging*, vol. 35, no. 5, pp. 1153–1159, 2016.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] H. Greenspan, B. Van Ginneken, 和 R. M. Summers，“特邀编辑：医学成像中的深度学习：激动人心的新技术概述与未来前景，”
    *IEEE医学成像汇刊*，第35卷，第5期，页码1153–1159，2016。'
- en: '[3] C. A. Stewart, T. M. Cockerill, I. Foster, D. Hancock, N. Merchant, E. Skidmore,
    D. Stanzione, J. Taylor, S. Tuecke, G. Turner *et al.*, “Jetstream: a self-provisioned,
    scalable science and engineering cloud environment,” in *Proceedings of the 2015
    XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure*,
    2015, pp. 1–8.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] C. A. Stewart, T. M. Cockerill, I. Foster, D. Hancock, N. Merchant, E.
    Skidmore, D. Stanzione, J. Taylor, S. Tuecke, G. Turner *等*，"Jetstream: 一个自我配置、可扩展的科学与工程云环境"，在
    *2015年XSEDE会议论文集：通过增强的网络基础设施实现的科学进展*，2015，页码1–8。'
- en: '[4] J. Towns, T. Cockerill, M. Dahan, I. Foster, K. Gaither, A. Grimshaw, V. Hazlewood,
    S. Lathrop, D. Lifka, G. D. Peterson *et al.*, “Xsede: accelerating scientific
    discovery,” *Computing in science & engineering*, vol. 16, no. 5, pp. 62–74, 2014.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. Towns, T. Cockerill, M. Dahan, I. Foster, K. Gaither, A. Grimshaw, V.
    Hazlewood, S. Lathrop, D. Lifka, G. D. Peterson *等*，"Xsede: 加速科学发现"， *计算与科学工程*，第16卷，第5期，页码62–74，2014。'
- en: '[5] A. Das, W.-M. Lin, and P. Rad, “A distributed secure machine-learning cloud
    architecture for semantic analysis,” in *Applied Cloud Deep Semantic Recognition*.   Auerbach
    Publications, 2018, pp. 145–174.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. Das, W.-M. Lin, 和 P. Rad，“一种用于语义分析的分布式安全机器学习云架构，”在 *应用云深度语义识别*。Auerbach
    Publications，2018，页码145–174。'
- en: '[6] A. Das, P. Rad, K.-K. R. Choo, B. Nouhi, J. Lish, and J. Martel, “Distributed
    machine learning cloud teleophthalmology iot for predicting amd disease progression,”
    *Future Generation Computer Systems*, vol. 93, pp. 486–498, 2019.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Das, P. Rad, K.-K. R. Choo, B. Nouhi, J. Lish, 和 J. Martel，“分布式机器学习云远程眼科物联网用于预测年龄相关性黄斑变性疾病进展，”
    *未来一代计算机系统*，第93卷，页码486–498，2019。'
- en: '[7] B. Yang, F. Liu, C. Ren, Z. Ouyang, Z. Xie, X. Bo, and W. Shu, “Biren:
    predicting enhancers with a deep-learning-based model using the dna sequence alone,”
    *Bioinformatics*, vol. 33, no. 13, pp. 1930–1936, 2017.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] B. Yang, F. Liu, C. Ren, Z. Ouyang, Z. Xie, X. Bo, 和 W. Shu，“Biren: 使用仅基于DNA序列的深度学习模型预测增强子，”
    *生物信息学*，第33卷，第13期，页码1930–1936，2017。'
- en: '[8] T. Gebru, J. Krause, Y. Wang, D. Chen, J. Deng, E. L. Aiden, and L. Fei-Fei,
    “Using deep learning and google street view to estimate the demographic makeup
    of neighborhoods across the united states,” *Proceedings of the National Academy
    of Sciences*, vol. 114, no. 50, pp. 13 108–13 113, 2017.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] T. Gebru, J. Krause, Y. Wang, D. Chen, J. Deng, E. L. Aiden, 和 L. Fei-Fei，“使用深度学习和Google街景视图来估计美国各地社区的人口组成，”
    *国家科学院院刊*，第114卷，第50期，页码13108–13113，2017。'
- en: '[9] H. Gabbard, M. Williams, F. Hayes, and C. Messenger, “Matching matched
    filtering with deep networks for gravitational-wave astronomy,” *Physical review
    letters*, vol. 120, no. 14, p. 141103, 2018.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] H. Gabbard, M. Williams, F. Hayes, 和 C. Messenger，“将匹配滤波与深度网络相匹配，用于引力波天文学，”
    *物理评论快报*，第120卷，第14期，页码141103，2018。'
- en: '[10] N. Ebadi, B. Lwowski, M. Jaloli, and P. Rad, “Implicit life event discovery
    from call transcripts using temporal input transformation network,” *IEEE Access*,
    vol. 7, pp. 172 178–172 189, 2019.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] N. Ebadi, B. Lwowski, M. Jaloli, 和 P. Rad，“从电话记录中隐式发现生活事件，使用时间输入转换网络，”
    *IEEE Access*，第7卷，页码172178–172189，2019。'
- en: '[11] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case,
    J. Casper, B. Catanzaro, Q. Cheng, G. Chen *et al.*, “Deep speech 2: End-to-end
    speech recognition in english and mandarin,” in *International conference on machine
    learning*, 2016, pp. 173–182.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C.
    Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen *等*，"Deep speech 2: 英语和普通话的端到端语音识别"，在
    *国际机器学习大会*，2016，页码173–182。'
- en: '[12] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis, “Deep
    learning for computer vision: A brief review,” *Computational intelligence and
    neuroscience*, vol. 2018, 2018.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Voulodimos, N. Doulamis, A. Doulamis, 和 E. Protopapadakis，“计算机视觉中的深度学习：简要综述，”
    *计算智能与神经科学*，第2018卷，2018。'
- en: '[13] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
    Unified, real-time object detection,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2016, pp. 779–788.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Redmon, S. Divvala, R. Girshick, 和 A. Farhadi，“你只看一次: 统一的实时目标检测，”在
    *IEEE计算机视觉与模式识别会议论文集*，2016，页码779–788。'
- en: '[14] J. Dai, Y. Li, K. He, and J. Sun, “R-fcn: Object detection via region-based
    fully convolutional networks,” in *Advances in neural information processing systems*,
    2016, pp. 379–387.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] J. Dai, Y. Li, K. He, 和 J. Sun，“R-fcn: 基于区域的全卷积网络进行对象检测，” *在《神经信息处理系统进展》*，2016年，第379–387页。'
- en: '[15] G. Varol, I. Laptev, and C. Schmid, “Long-term temporal convolutions for
    action recognition,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 40, no. 6, pp. 1510–1517, 2017.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] G. Varol, I. Laptev, 和 C. Schmid，“用于动作识别的长期时间卷积，” *《IEEE模式分析与机器智能汇刊》*，第40卷，第6期，第1510–1517页，2017年。'
- en: '[16] N. Bendre, N. Ebadi, J. J. Prevost, and P. Najafirad, “Human action performance
    using deep neuro-fuzzy recurrent attention model,” *IEEE Access*, vol. 8, pp.
    57 749–57 761, 2020.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] N. Bendre, N. Ebadi, J. J. Prevost, 和 P. Najafirad，“使用深度神经模糊递归注意力模型进行人类动作表现，”
    *《IEEE Access》*，第8卷，第57 749–57 761页，2020年。'
- en: '[17] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person 2d
    pose estimation using part affinity fields,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017, pp. 7291–7299.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Z. Cao, T. Simon, S.-E. Wei, 和 Y. Sheikh，“实时多人体2D姿态估计使用部件关联场，” *在《IEEE计算机视觉与模式识别会议论文集》*，2017年，第7291–7299页。'
- en: '[18] A. Newell, K. Yang, and J. Deng, “Stacked hourglass networks for human
    pose estimation,” in *European conference on computer vision*.   Springer, 2016,
    pp. 483–499.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. Newell, K. Yang, 和 J. Deng，“用于人体姿态估计的堆叠小时玻璃网络，” *在《欧洲计算机视觉会议》*。 Springer，2016年，第483–499页。'
- en: '[19] V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional
    encoder-decoder architecture for image segmentation,” *IEEE transactions on pattern
    analysis and machine intelligence*, vol. 39, no. 12, pp. 2481–2495, 2017.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] V. Badrinarayanan, A. Kendall, 和 R. Cipolla，“Segnet: 一种用于图像分割的深度卷积编码器-解码器架构，”
    *《IEEE模式分析与机器智能汇刊》*，第39卷，第12期，第2481–2495页，2017年。'
- en: '[20] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
    Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 40, no. 4, pp. 834–848, 2017.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, 和 A. L. Yuille，“Deeplab:
    具有深度卷积网络、空洞卷积和全连接条件随机场的语义图像分割，” *《IEEE模式分析与机器智能汇刊》*，第40卷，第4期，第834–848页，2017年。'
- en: '[21] D. Held, S. Thrun, and S. Savarese, “Learning to track at 100 fps with
    deep regression networks,” in *European Conference on Computer Vision*.   Springer,
    2016, pp. 749–765.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] D. Held, S. Thrun, 和 S. Savarese，“使用深度回归网络以100 fps跟踪学习，” *在《欧洲计算机视觉会议》*。
    Springer，2016年，第749–765页。'
- en: '[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in neural information processing
    systems*, 2012, pp. 1097–1105.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton，“使用深度卷积神经网络进行Imagenet分类，”
    *在《神经信息处理系统进展》*，2012年，第1097–1105页。'
- en: '[23] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, and C. I. Sánchez, “A survey on deep learning
    in medical image analysis,” *Medical image analysis*, vol. 42, pp. 60–88, 2017.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, 和 C. I. Sánchez，“关于医学图像分析中的深度学习的调查，” *《医学图像分析》*，第42卷，第60–88页，2017年。'
- en: '[24] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat,
    G. Irving, M. Isard *et al.*, “Tensorflow: A system for large-scale machine learning,”
    in *12th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation
    ($\{$OSDI$\}$ 16)*, 2016, pp. 265–283.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S.
    Ghemawat, G. Irving, M. Isard *等*，“Tensorflow: 一个用于大规模机器学习的系统，” *在《第12届$\{$USENIX$\}$操作系统设计与实现研讨会($\{$OSDI$\}$
    16)*，2016年，第265–283页。'
- en: '[25] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    Z. Lin, N. Gimelshein, L. Antiga *et al.*, “Pytorch: An imperative style, high-performance
    deep learning library,” in *Advances in Neural Information Processing Systems*,
    2019, pp. 8024–8035.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    Z. Lin, N. Gimelshein, L. Antiga *等*，“Pytorch: 一种命令式风格、高性能的深度学习库，” *在《神经信息处理系统进展》*，2019年，第8024–8035页。'
- en: '[26] K. Keahey, P. Riteau, D. Stanzione, T. Cockerill, J. Mambretti, P. Rad,
    and P. Ruth, “Chameleon: a scalable production testbed for computer science research,”
    in *Contemporary High Performance Computing*.   CRC Press, 2019, pp. 123–148.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] K. Keahey, P. Riteau, D. Stanzione, T. Cockerill, J. Mambretti, P. Rad,
    和 P. Ruth，“Chameleon: 一个可扩展的计算机科学研究生产测试平台，” *在《当代高性能计算》*。 CRC Press，2019年，第123–148页。'
- en: '[27] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] K. He, X. Zhang, S. Ren, 和 J. Sun, “用于图像识别的深度残差学习,” 见于 *IEEE 计算机视觉与模式识别会议论文集*,
    2016年, 第770–778页。'
- en: '[28] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
    the inception architecture for computer vision,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2016, pp. 2818–2826.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, 和 Z. Wojna, “重新思考计算机视觉中的
    Inception 架构,” 见于 *IEEE 计算机视觉与模式识别会议论文集*, 2016年, 第2818–2826页。'
- en: '[29] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-v4, inception-resnet
    and the impact of residual connections on learning,” in *Thirty-first AAAI conference
    on artificial intelligence*, 2017.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] C. Szegedy, S. Ioffe, V. Vanhoucke, 和 A. A. Alemi, “Inception-v4、Inception-ResNet
    以及残差连接对学习的影响,” 见于 *第31届 AAAI 人工智能大会*, 2017年。'
- en: '[30] N. Akhtar and A. Mian, “Threat of adversarial attacks on deep learning
    in computer vision: A survey,” *IEEE Access*, vol. 6, pp. 14 410–14 430, 2018.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] N. Akhtar 和 A. Mian, “计算机视觉中深度学习面临的对抗攻击威胁：一项调查,” *IEEE Access*, 第6卷, 第14,410–14,430页,
    2018年。'
- en: '[31] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang *et al.*, “End to end learning for
    self-driving cars,” *arXiv preprint arXiv:1604.07316*, 2016.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang *等*, “端到端自驾车学习,” *arXiv 预印本 arXiv:1604.07316*,
    2016年。'
- en: '[32] S. Silva, R. Suresh, F. Tao, J. Votion, and Y. Cao, “A multi-layer k-means
    approach for multi-sensor data pattern recognition in multi-target localization,”
    *arXiv preprint arXiv:1705.10757*, 2017.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] S. Silva, R. Suresh, F. Tao, J. Votion, 和 Y. Cao, “一种多层 K-means 方法用于多传感器数据模式识别中的多目标定位,”
    *arXiv 预印本 arXiv:1705.10757*, 2017年。'
- en: '[33] S. H. Silva, P. Rad, N. Beebe, K.-K. R. Choo, and M. Umapathy, “Cooperative
    unmanned aerial vehicles with privacy preserving deep vision for real-time object
    identification and tracking,” *Journal of Parallel and Distributed Computing*,
    vol. 131, pp. 147–160, 2019.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. H. Silva, P. Rad, N. Beebe, K.-K. R. Choo, 和 M. Umapathy, “具有隐私保护深度视觉的合作无人机用于实时对象识别和跟踪,”
    *并行与分布计算期刊*, 第131卷, 第147–160页, 2019年。'
- en: '[34] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement learning
    for robotic manipulation with asynchronous off-policy updates,” in *2017 IEEE
    international conference on robotics and automation (ICRA)*.   IEEE, 2017, pp.
    3389–3396.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. Gu, E. Holly, T. Lillicrap, 和 S. Levine, “用于机器人操作的深度强化学习与异步离策略更新,”
    见于 *2017 IEEE 国际机器人与自动化会议 (ICRA)*。 IEEE, 2017年, 第3389–3396页。'
- en: '[35] S. H. Silva, A. Alaeddini, and P. Najafirad, “Temporal graph traversals
    using reinforcement learning with proximal policy optimization,” *IEEE Access*,
    vol. 8, pp. 63 910–63 922, 2020.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] S. H. Silva, A. Alaeddini, 和 P. Najafirad, “使用近端策略优化的强化学习进行时序图遍历,” *IEEE
    Access*, 第8卷, 第63,910–63,922页, 2020年。'
- en: '[36] M. Lansley, N. Polatidis, S. Kapetanakis, K. Amin, G. Samakovitis, and
    M. Petridis, “Seen the villains: Detecting social engineering attacks using case-based
    reasoning and deep learning,” in *Workshops Proceedings for the Twenty-seventh
    International Conference on Case-Based Reasoning: Case-based reasoning and deep
    learning workshop*, 2019.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] M. Lansley, N. Polatidis, S. Kapetanakis, K. Amin, G. Samakovitis, 和 M.
    Petridis, “发现恶意行为者：使用案例推理和深度学习检测社会工程攻击,” 见于 *第二十七届国际案例推理会议研讨会：案例推理与深度学习研讨会*, 2019年。'
- en: '[37] D. Kwon, H. Kim, J. Kim, S. C. Suh, I. Kim, and K. J. Kim, “A survey of
    deep learning-based network anomaly detection,” *Cluster Computing*, pp. 1–13,
    2017.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] D. Kwon, H. Kim, J. Kim, S. C. Suh, I. Kim, 和 K. J. Kim, “基于深度学习的网络异常检测调查,”
    *集群计算*, 第1–13页, 2017年。'
- en: '[38] G. De La Torre, P. Rad, and K.-K. R. Choo, “Implementation of deep packet
    inspection in smart grids and industrial internet of things: Challenges and opportunities,”
    *Journal of Network and Computer Applications*, 2019.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] G. De La Torre, P. Rad, 和 K.-K. R. Choo, “智能电网和工业物联网中深度包检测的实施：挑战与机遇,”
    *网络与计算机应用期刊*, 2019年。'
- en: '[39] V. Kepuska and G. Bohouta, “Next-generation of virtual personal assistants
    (microsoft cortana, apple siri, amazon alexa and google home),” in *2018 IEEE
    8th Annual Computing and Communication Workshop and Conference (CCWC)*.   IEEE,
    2018, pp. 99–103.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] V. Kepuska 和 G. Bohouta, “下一代虚拟个人助理（微软 Cortana、苹果 Siri、亚马逊 Alexa 和谷歌 Home）,”
    见于 *2018 IEEE 第八届年度计算与通信研讨会暨会议 (CCWC)*。 IEEE, 2018, 第99–103页。'
- en: '[40] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye, G. Anderson,
    G. Corrado, W. Chai, M. Ispir *et al.*, “Wide & deep learning for recommender
    systems,” in *Proceedings of the 1st workshop on deep learning for recommender
    systems*, 2016, pp. 7–10.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye, G.
    Anderson, G. Corrado, W. Chai, M. Ispir *等*，“推荐系统的宽度与深度学习，” 收录于 *第1届推荐系统深度学习研讨会论文集*，2016年，页码7–10。'
- en: '[41] G. De La Torre, P. Rad, and K.-K. R. Choo, “Driverless vehicle security:
    Challenges and future research opportunities,” *Future Generation Computer Systems*,
    2018.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] G. De La Torre, P. Rad, 和 K.-K. R. Choo, “无人驾驶车辆安全：挑战与未来研究机会，” *未来一代计算机系统*，2018年。'
- en: '[42] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, pp.
    529–533, 2015.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *等*，“通过深度强化学习实现人类级控制，”
    *自然*，第518卷，第7540期，页码529–533，2015年。'
- en: '[43] C. Szegedy, W. Zaremba, I. Sutskever, J. B. Estrach, D. Erhan, I. Goodfellow,
    and R. Fergus, “Intriguing properties of neural networks,” in *2nd International
    Conference on Learning Representations, ICLR 2014*, 2014.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] C. Szegedy, W. Zaremba, I. Sutskever, J. B. Estrach, D. Erhan, I. Goodfellow,
    和 R. Fergus, “神经网络的有趣特性，” 收录于 *第二届国际学习表征会议，ICLR 2014*，2014年。'
- en: '[44] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are easily
    fooled: High confidence predictions for unrecognizable images,” in *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2015, pp.
    427–436.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] A. Nguyen, J. Yosinski, 和 J. Clune, “深度神经网络容易被欺骗：对不可识别图像的高置信度预测，” 收录于
    *IEEE计算机视觉与模式识别大会论文集*，2015年，页码427–436。'
- en: '[45] B. Biggio, I. Corona, B. Nelson, B. I. Rubinstein, D. Maiorca, G. Fumera,
    G. Giacinto, and F. Roli, “Security evaluation of support vector machines in adversarial
    environments,” in *Support Vector Machines Applications*.   Springer, 2014, pp.
    105–153.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] B. Biggio, I. Corona, B. Nelson, B. I. Rubinstein, D. Maiorca, G. Fumera,
    G. Giacinto, 和 F. Roli, “对抗环境中支持向量机的安全评估，” 收录于 *支持向量机应用*。施普林格出版社，2014年，页码105–153。'
- en: '[46] A. Chakraborty, M. Alam, V. Dey, A. Chattopadhyay, and D. Mukhopadhyay,
    “Adversarial attacks and defences: A survey,” *arXiv preprint arXiv:1810.00069*,
    2018.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] A. Chakraborty, M. Alam, V. Dey, A. Chattopadhyay, 和 D. Mukhopadhyay,
    “对抗性攻击与防御：综述，” *arXiv 预印本 arXiv:1810.00069*，2018年。'
- en: '[47] H. Chacon, S. Silva, and P. Rad, “Deep learning poison data attack detection,”
    in *2019 IEEE 31st International Conference on Tools with Artificial Intelligence
    (ICTAI)*.   IEEE, 2019, pp. 971–978.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] H. Chacon, S. Silva, 和 P. Rad, “深度学习毒害数据攻击检测，” 收录于 *2019年IEEE第31届人工智能工具国际会议（ICTAI）*。IEEE，2019年，页码971–978。'
- en: '[48] A. Das and P. Rad, “Opportunities and challenges in explainable artificial
    intelligence (xai): A survey,” *arXiv preprint arXiv:2006.11371*, 2020.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] A. Das 和 P. Rad, “可解释人工智能（XAI）中的机会与挑战：一项调查，” *arXiv 预印本 arXiv:2006.11371*，2020年。'
- en: '[49] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
    networks,” in *2017 Ieee symposium on security and privacy (sp)*.   IEEE, 2017,
    pp. 39–57.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] N. Carlini 和 D. Wagner, “评估神经网络鲁棒性的方法，” 收录于 *2017年IEEE安全与隐私研讨会（SP）*。IEEE，2017年，页码39–57。'
- en: '[50] X. Yuan, P. He, Q. Zhu, and X. Li, “Adversarial examples: Attacks and
    defenses for deep learning,” *IEEE transactions on neural networks and learning
    systems*, vol. 30, no. 9, pp. 2805–2824, 2019.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] X. Yuan, P. He, Q. Zhu, 和 X. Li, “对抗样本：深度学习的攻击与防御，” *IEEE 神经网络与学习系统汇刊*，第30卷，第9期，页码2805–2824，2019年。'
- en: '[51] Y. Vorobeychik and M. Kantarcioglu, *Adversarial machine learning*.   Morgan
    & Claypool Publishers, 2018.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Y. Vorobeychik 和 M. Kantarcioglu, *对抗性机器学习*。摩根与克莱普出版社，2018年。'
- en: '[52] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh, “Zoo: Zeroth
    order optimization based black-box attacks to deep neural networks without training
    substitute models,” in *Proceedings of the 10th ACM Workshop on Artificial Intelligence
    and Security*, 2017, pp. 15–26.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, 和 C.-J. Hsieh, “Zoo: 基于零阶优化的黑箱攻击深度神经网络，无需训练替代模型，”
    收录于 *第10届ACM人工智能与安全研讨会论文集*，2017年，页码15–26。'
- en: '[53] H. Xu, Y. Ma, H.-C. Liu, D. Deb, H. Liu, J.-L. Tang, and A. K. Jain, “Adversarial
    attacks and defenses in images, graphs and text: A review,” *International Journal
    of Automation and Computing*, vol. 17, no. 2, pp. 151–178, 2020.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] H. Xu, Y. Ma, H.-C. Liu, D. Deb, H. Liu, J.-L. Tang, 和 A. K. Jain, “图像、图形和文本中的对抗性攻击与防御：综述，”
    *国际自动化与计算期刊*，第17卷，第2期，页码151–178，2020年。'
- en: '[54] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards deep
    learning models resistant to adversarial attacks,” in *International Conference
    on Learning Representations*, 2018.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] A. Madry, A. Makelov, L. Schmidt, D. Tsipras 和 A. Vladu， “迈向对抗攻击鲁棒的深度学习模型”，在
    *国际学习表征会议* 中，2018。'
- en: '[55] C. Xie, Y. Wu, L. v. d. Maaten, A. L. Yuille, and K. He, “Feature denoising
    for improving adversarial robustness,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 501–509.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] C. Xie, Y. Wu, L. v. d. Maaten, A. L. Yuille 和 K. He， “特征去噪以提高对抗鲁棒性”，在
    *IEEE 计算机视觉与模式识别会议论文集* 中，2019，页码 501–509。'
- en: '[56] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
    adversarial examples,” in *3rd International Conference on Learning Representations,
    ICLR 2015 - Conference Track Proceedings*, 2015.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] I. J. Goodfellow, J. Shlens 和 C. Szegedy， “解释和利用对抗样本”，在 *第三届国际学习表征会议，ICLR
    2015 - 会议论文集* 中，2015。'
- en: '[57] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple
    and accurate method to fool deep neural networks,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2016, pp. 2574–2582.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] S.-M. Moosavi-Dezfooli, A. Fawzi 和 P. Frossard， “Deepfool：一种简单而准确的欺骗深度神经网络的方法”，在
    *IEEE 计算机视觉与模式识别会议论文集* 中，2016，页码 2574–2582。'
- en: '[58] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami,
    “The limitations of deep learning in adversarial settings,” in *2016 IEEE European
    symposium on security and privacy (EuroS&P)*.   IEEE, 2016, pp. 372–387.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik 和 A. Swami，
    “深度学习在对抗环境中的局限性”，在 *2016 IEEE 欧洲安全与隐私研讨会 (EuroS&P)* 中。 IEEE，2016，页码 372–387。'
- en: '[59] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial machine learning
    at scale,” in *5th International Conference on Learning Representations, ICLR
    2017 - Conference Track Proceedings*, 2017.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] A. Kurakin, I. J. Goodfellow 和 S. Bengio， “大规模对抗机器学习”，在 *第五届国际学习表征会议，ICLR
    2017 - 会议论文集* 中，2017。'
- en: '[60] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
    networks,” in *2017 ieee symposium on security and privacy (sp)*.   IEEE, 2017,
    pp. 39–57.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] N. Carlini 和 D. Wagner， “迈向评估神经网络的鲁棒性”，在 *2017 IEEE 安全与隐私研讨会 (SP)* 中。
    IEEE，2017，页码 39–57。'
- en: '[61] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation as
    a defense to adversarial perturbations against deep neural networks,” in *2016
    IEEE Symposium on Security and Privacy (SP)*.   IEEE, 2016, pp. 582–597.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] N. Papernot, P. McDaniel, X. Wu, S. Jha 和 A. Swami， “蒸馏作为对抗扰动的防御手段”，在
    *2016 IEEE 安全与隐私研讨会 (SP)* 中。 IEEE，2016，页码 582–597。'
- en: '[62] N. Carlini, G. Katz, C. Barrett, and D. L. Dill, “Ground-Truth Adversarial
    Examples,” *Iclr 2018*, 2018.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] N. Carlini, G. Katz, C. Barrett 和 D. L. Dill， “真实对抗样本”， *ICLR 2018*，2018。'
- en: '[63] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, “Reluplex:
    An efficient smt solver for verifying deep neural networks,” in *International
    Conference on Computer Aided Verification*.   Springer, 2017, pp. 97–117.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] G. Katz, C. Barrett, D. L. Dill, K. Julian 和 M. J. Kochenderfer， “Reluplex：一种高效的
    SMT 求解器用于验证深度神经网络”，在 *国际计算机辅助验证会议* 中。 Springer，2017，页码 97–117。'
- en: '[64] V. Tjeng, K. Y. Xiao, and R. Tedrake, “Evaluating robustness of neural
    networks with mixed integer programming,” in *International Conference on Learning
    Representations*, 2018.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] V. Tjeng, K. Y. Xiao 和 R. Tedrake， “通过混合整数规划评估神经网络的鲁棒性”，在 *国际学习表征会议* 中，2018。'
- en: '[65] K. Y. Xiao, V. Tjeng, N. M. M. Shafiullah, and A. Madry, “Training for
    faster adversarial robustness verification via inducing relu stability,” in *International
    Conference on Learning Representations*, 2018.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] K. Y. Xiao, V. Tjeng, N. M. M. Shafiullah 和 A. Madry， “通过诱导 Relu 稳定性来加速对抗鲁棒性验证的训练”，在
    *国际学习表征会议* 中，2018。'
- en: '[66] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Universal
    adversarial perturbations,” in *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2017, pp. 1765–1773.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi 和 P. Frossard， “通用对抗扰动”，在 *IEEE
    计算机视觉与模式识别会议论文集* 中，2017，页码 1765–1773。'
- en: '[67] A. Ghiasi, A. Shafahi, and T. Goldstein, “Breaking certified defenses:
    Semantic adversarial examples with spoofed robustness certificates,” in *International
    Conference on Learning Representations*, 2019.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] A. Ghiasi, A. Shafahi 和 T. Goldstein， “突破认证防御：具有伪造鲁棒性证书的语义对抗样本”，在 *国际学习表征会议*
    中，2019。'
- en: '[68] P.-Y. Chen, Y. Sharma, H. Zhang, J. Yi, and C.-J. Hsieh, “Ead: elastic-net
    attacks to deep neural networks via adversarial examples,” in *Thirty-second AAAI
    conference on artificial intelligence*, 2018.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] P.-Y. Chen, Y. Sharma, H. Zhang, J. Yi, 和 C.-J. Hsieh，“EAD：通过对抗样本对深度神经网络进行弹性网攻击，”发表于*第三十二届美国人工智能协会会议*，2018年。'
- en: '[69] U. Jang, X. Wu, and S. Jha, “Objective metrics and gradient descent algorithms
    for adversarial examples in machine learning,” in *Proceedings of the 33rd Annual
    Computer Security Applications Conference*, 2017, pp. 262–277.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] U. Jang, X. Wu, 和 S. Jha，“机器学习中对抗样本的客观度量和梯度下降算法，”发表于*第33届年度计算机安全应用会议论文集*，2017年，第262–277页。'
- en: '[70] C. Xiao, J. Y. Zhu, B. Li, W. He, M. Liu, and D. Song, “Spatially transformed
    adversarial examples,” in *6th International Conference on Learning Representations,
    ICLR 2018*, 2018.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] C. Xiao, J. Y. Zhu, B. Li, W. He, M. Liu, 和 D. Song，“空间变换对抗样本，”发表于*第六届国际学习表征会议，ICLR
    2018*，2018年。'
- en: '[71] L. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry, “Exploring
    the landscape of spatial robustness,” in *ICML*, 2019.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] L. Engstrom, B. Tran, D. Tsipras, L. Schmidt, 和 A. Madry，“探索空间鲁棒性的景观，”发表于*国际机器学习会议（ICML）*，2019年。'
- en: '[72] A. Odena, C. Olah, and J. Shlens, “Conditional image synthesis with auxiliary
    classifier gans,” in *Proceedings of the 34th International Conference on Machine
    Learning-Volume 70*.   JMLR. org, 2017, pp. 2642–2651.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] A. Odena, C. Olah, 和 J. Shlens，“具有辅助分类器GANs的条件图像合成，”发表于*第34届国际机器学习会议论文集-第70卷*，JMLR.
    org，2017年，第2642–2651页。'
- en: '[73] Y. Song, R. Shu, N. Kushman, and S. Ermon, “Constructing unrestricted
    adversarial examples with generative models,” in *Advances in Neural Information
    Processing Systems*, 2018, pp. 8312–8323.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Y. Song, R. Shu, N. Kushman, 和 S. Ermon，“构建无限制对抗样本的生成模型，”发表于*神经信息处理系统进展*，2018年，第8312–8323页。'
- en: '[74] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami,
    “Practical black-box attacks against machine learning,” in *Proceedings of the
    2017 ACM on Asia conference on computer and communications security*, 2017, pp.
    506–519.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, 和 A. Swami，“针对机器学习的实用黑箱攻击，”发表于*2017年ACM亚洲计算机与通信安全会议论文集*，2017年，第506–519页。'
- en: '[75] J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling deep
    neural networks,” *IEEE Transactions on Evolutionary Computation*, vol. 23, no. 5,
    pp. 828–841, 2019.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] J. Su, D. V. Vargas, 和 K. Sakurai，“一种像素攻击以欺骗深度神经网络，”*IEEE进化计算学报*，第23卷，第5期，第828–841页，2019年。'
- en: '[76] A. Ilyas, L. Engstrom, A. Athalye, and J. Lin, “Query-efficient black-box
    adversarial examples,” *arXiv preprint arXiv:1712.07113*, 2017.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] A. Ilyas, L. Engstrom, A. Athalye, 和 J. Lin，“查询高效的黑箱对抗样本，”*arXiv预印本 arXiv:1712.07113*，2017年。'
- en: '[77] J. Chen, M. Jordan, and M. Wainwright, “Hopskipjumpattack: A query-efficient
    decision-based attack,” in *2020 IEEE Symposium on Security and Privacy (SP)*,
    pp. 668–685.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] J. Chen, M. Jordan, 和 M. Wainwright，“Hopskipjumpattack：一种查询高效的基于决策的攻击，”发表于*2020年IEEE安全与隐私研讨会（SP）*，第668–685页。'
- en: '[78] B. Ru, A. Cobb, A. Blaas, and Y. Gal, “Bayesopt adversarial attack,” in
    *International Conference on Learning Representations*, 2020.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] B. Ru, A. Cobb, A. Blaas, 和 Y. Gal，“Bayesopt对抗攻击，”发表于*国际学习表征会议*，2020年。'
- en: '[79] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine, and S. Russell, “Adversarial
    policies: Attacking deep reinforcement learning,” in *International Conference
    on Learning Representations*, 2019.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine, 和 S. Russell，“对抗性策略：攻击深度强化学习，”发表于*国际学习表征会议*，2019年。'
- en: '[80] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash,
    T. Kohno, and D. Song, “Robust physical-world attacks on deep learning visual
    classification,” in *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2018, pp. 1625–1634.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash,
    T. Kohno, 和 D. Song，“深度学习视觉分类的鲁棒物理世界攻击，”发表于*2018年IEEE/CVF计算机视觉与模式识别会议*，2018年，第1625–1634页。'
- en: '[81] G. Goswami, N. Ratha, A. Agarwal, R. Singh, and M. Vatsa, “Unravelling
    robustness of deep learning based face recognition against adversarial attacks,”
    in *Thirty-Second AAAI Conference on Artificial Intelligence*, 2018.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] G. Goswami, N. Ratha, A. Agarwal, R. Singh, 和 M. Vatsa，“揭示基于深度学习的人脸识别在对抗性攻击下的鲁棒性，”发表于*第三十二届美国人工智能协会会议*，2018年。'
- en: '[82] A. J. Bose and P. Aarabi, “Adversarial attacks on face detectors using
    neural net based constrained optimization,” in *2018 IEEE 20th International Workshop
    on Multimedia Signal Processing (MMSP)*.   IEEE, 2018, pp. 1–6.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] A. J. Bose 和 P. Aarabi，“使用基于神经网络的约束优化进行面部检测器的对抗性攻击，”发表于*2018年IEEE第20届多媒体信号处理国际研讨会（MMSP）*，IEEE，2018年，第1–6页。'
- en: '[83] Z.-A. Zhu, Y.-Z. Lu, and C.-K. Chiang, “Generating adversarial examples
    by makeup attacks on face recognition,” in *2019 IEEE International Conference
    on Image Processing (ICIP)*.   IEEE, 2019, pp. 2516–2520.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Z.-A. Zhu, Y.-Z. Lu 和 C.-K. Chiang，“通过化妆攻击生成对抗性样本以进行人脸识别”，收录于 *2019 IEEE
    国际图像处理会议（ICIP）*。IEEE，2019 年，第 2516–2520 页。'
- en: '[84] Y. Dong, H. Su, B. Wu, Z. Li, W. Liu, T. Zhang, and J. Zhu, “Efficient
    decision-based black-box adversarial attacks on face recognition,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2019, pp.
    7714–7722.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Y. Dong, H. Su, B. Wu, Z. Li, W. Liu, T. Zhang 和 J. Zhu，“高效的基于决策的黑盒对抗性攻击人脸识别”，收录于
    *IEEE 计算机视觉与模式识别大会论文集*，2019 年，第 7714–7722 页。'
- en: '[85] O. Suciu, S. E. Coull, and J. Johns, “Exploring adversarial examples in
    malware detection,” in *2019 IEEE Security and Privacy Workshops (SPW)*.   IEEE,
    2019, pp. 8–14.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] O. Suciu, S. E. Coull 和 J. Johns，“在恶意软件检测中探索对抗性样本”，收录于 *2019 IEEE 安全与隐私研讨会（SPW）*。IEEE，2019
    年，第 8–14 页。'
- en: '[86] A. Chernikova and A. Oprea, “Adversarial examples for deep learning cyber
    security analytics,” *arXiv preprint arXiv:1909.10480*, 2019.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] A. Chernikova 和 A. Oprea，“深度学习网络安全分析中的对抗性样本”，*arXiv 预印本 arXiv:1909.10480*，2019
    年。'
- en: '[87] M. Melis, A. Demontis, B. Biggio, G. Brown, G. Fumera, and F. Roli, “Is
    deep learning safe for robot vision? adversarial examples against the icub humanoid,”
    in *Proceedings of the IEEE International Conference on Computer Vision Workshops*,
    2017, pp. 751–759.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] M. Melis, A. Demontis, B. Biggio, G. Brown, G. Fumera 和 F. Roli，“深度学习对机器人视觉是否安全？针对
    icub 类人的对抗性样本”，收录于 *IEEE 国际计算机视觉研讨会论文集*，2017 年，第 751–759 页。'
- en: '[88] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndić, P. Laskov, G. Giacinto,
    and F. Roli, “Evasion attacks against machine learning at test time,” in *Joint
    European conference on machine learning and knowledge discovery in databases*.   Springer,
    2013, pp. 387–402.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndić, P. Laskov, G.
    Giacinto 和 F. Roli，“测试时的机器学习规避攻击”，收录于 *联合欧洲机器学习与数据库知识发现会议*。Springer，2013 年，第 387–402
    页。'
- en: '[89] C. Sitawarin, A. N. Bhagoji, A. Mosenia, M. Chiang, and P. Mittal, “Darts:
    Deceiving autonomous cars with toxic signs,” *arXiv preprint arXiv:1802.06430*,
    2018.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] C. Sitawarin, A. N. Bhagoji, A. Mosenia, M. Chiang 和 P. Mittal，“Darts：用有毒标志欺骗自动驾驶汽车”，*arXiv
    预印本 arXiv:1802.06430*，2018 年。'
- en: '[90] N. Morgulis, A. Kreines, S. Mendelowitz, and Y. Weisglass, “Fooling a
    real car with adversarial traffic signs,” *arXiv preprint arXiv:1907.00374*, 2019.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] N. Morgulis, A. Kreines, S. Mendelowitz 和 Y. Weisglass，“用对抗性交通标志欺骗真实汽车”，*arXiv
    预印本 arXiv:1907.00374*，2019 年。'
- en: '[91] Y. Cao, C. Xiao, B. Cyr, Y. Zhou, W. Park, S. Rampazzi, Q. A. Chen, K. Fu,
    and Z. M. Mao, “Adversarial sensor attack on lidar-based perception in autonomous
    driving,” in *Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications
    Security*, 2019, pp. 2267–2281.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Y. Cao, C. Xiao, B. Cyr, Y. Zhou, W. Park, S. Rampazzi, Q. A. Chen, K.
    Fu 和 Z. M. Mao，“对基于激光雷达的自动驾驶感知的对抗性传感器攻击”，收录于 *2019 ACM SIGSAC 计算机与通信安全会议论文集*，2019
    年，第 2267–2281 页。'
- en: '[92] Y. Jia, Y. Lu, J. Shen, Q. A. Chen, H. Chen, Z. Zhong, and T. Wei, “Fooling
    detection alone is not enough: Adversarial attack against multiple object tracking,”
    in *International Conference on Learning Representations*, 2019.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Y. Jia, Y. Lu, J. Shen, Q. A. Chen, H. Chen, Z. Zhong 和 T. Wei，“仅仅欺骗检测是不够的：对多个目标跟踪的对抗性攻击”，收录于
    *国际学习表征会议*，2019 年。'
- en: '[93] W. Brendel, J. Rauber, and M. Bethge, “Decision-based adversarial attacks:
    Reliable attacks against black-box machine learning models,” in *International
    Conference on Learning Representations*, 2018.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] W. Brendel, J. Rauber 和 M. Bethge，“基于决策的对抗性攻击：针对黑盒机器学习模型的可靠攻击”，收录于 *国际学习表征会议*，2018
    年。'
- en: '[94] S. Sarkar, A. Bansal, U. Mahbub, and R. Chellappa, “Upset and angri: Breaking
    high performance image classifiers,” *arXiv preprint arXiv:1707.01159*, 2017.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] S. Sarkar, A. Bansal, U. Mahbub 和 R. Chellappa，“Upset 和 angri：破解高性能图像分类器”，*arXiv
    预印本 arXiv:1707.01159*，2017 年。'
- en: '[95] Y. Wang, D. Zou, J. Yi, J. Bailey, X. Ma, and Q. Gu, “Improving adversarial
    robustness requires revisiting misclassified examples,” in *International Conference
    on Learning Representations*, 2020.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Y. Wang, D. Zou, J. Yi, J. Bailey, X. Ma 和 Q. Gu，“提高对抗性鲁棒性需要重新审视被误分类的样本”，收录于
    *国际学习表征会议*，2020 年。'
- en: '[96] C. Song, K. He, J. Lin, L. Wang, and J. E. Hopcroft, “Robust local features
    for improving the generalization of adversarial training,” in *International Conference
    on Learning Representations*, 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] C. Song, K. He, J. Lin, L. Wang 和 J. E. Hopcroft，“改进对抗性训练的鲁棒局部特征”，收录于
    *国际学习表征会议*，2019 年。'
- en: '[97] T. Wu, L. Tong, and Y. Vorobeychik, “Defending against physically realizable
    attacks on image classification,” in *International Conference on Learning Representations*,
    2020.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] T. 吴, L. Tong, 和 Y. Vorobeychik, “防御对图像分类的物理可实现攻击，” 发表在*国际学习表征会议*，2020年。'
- en: '[98] E. Wong, L. Rice, and J. Z. Kolter, “Fast is better than free: Revisiting
    adversarial training,” in *International Conference on Learning Representations*,
    2019.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] E. Wong, L. Rice, 和 J. Z. Kolter, “快速优于免费的：重审对抗训练，” 发表在*国际学习表征会议*，2019年。'
- en: '[99] T.-K. Hu, T. Chen, H. Wang, and Z. Wang, “Triple wins: Boosting accuracy,
    robustness and efficiency together by enabling input-adaptive inference,” in *International
    Conference on Learning Representations*, 2019.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] T.-K. 胡, T. 陈, H. 王, 和 Z. 王, “三重胜利：通过启用输入自适应推理来提高准确性、鲁棒性和效率，” 发表在*国际学习表征会议*，2019年。'
- en: '[100] H. Chen, H. Zhang, D. Boning, and C.-J. Hsieh, “Robust decision trees
    against adversarial examples,” in *International Conference on Machine Learning*,
    2019, pp. 1122–1131.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] H. 陈, H. 张, D. 博宁, 和 C.-J. Hsieh, “针对对抗样本的鲁棒决策树，” 发表在*国际机器学习会议*，2019年，pp.
    1122–1131。'
- en: '[101] Y. Yang, G. Zhang, D. Katabi, and Z. Xu, “Me-net: Towards effective adversarial
    robustness with matrix estimation,” in *International Conference on Machine Learning*,
    2019, pp. 7025–7034.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Y. 杨, G. 张, D. Katabi, 和 Z. 徐, “Me-net：通过矩阵估计实现有效的对抗鲁棒性，” 发表在*国际机器学习会议*，2019年，pp.
    7025–7034。'
- en: '[102] H. Kannan, A. Kurakin, and I. Goodfellow, “Adversarial logit pairing,”
    *arXiv preprint arXiv:1803.06373*, 2018.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] H. 坎南, A. 库拉金, 和 I. 古德费洛, “对抗性逻辑配对，” *arXiv预印本 arXiv:1803.06373*，2018年。'
- en: '[103] A. Matyasko and L.-P. Chau, “Improved network robustness with adversary
    critic,” in *Advances in Neural Information Processing Systems*, 2018, pp. 10 578–10 587.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] A. Matyasko 和 L.-P. Chau, “通过对抗批评家改进网络鲁棒性，” 发表在*神经信息处理系统进展*，2018年，pp.
    10 578–10 587。'
- en: '[104] A. Sinha, H. Namkoong, and J. Duchi, “Certifying some distributional
    robustness with principled adversarial training,” in *International Conference
    on Learning Representations*, 2018.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] A. Sinha, H. 南孔, 和 J. 杜奇, “通过原则性对抗训练认证某些分布鲁棒性，” 发表在*国际学习表征会议*，2018年。'
- en: '[105] S. Sen, B. Ravindran, and A. Raghunathan, “Empir: Ensembles of mixed
    precision deep networks for increased robustness against adversarial attacks,”
    in *International Conference on Learning Representations*, 2019.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] S. Sen, B. Ravindran, 和 A. Raghunathan, “Empir：用于提高对抗攻击鲁棒性的混合精度深度网络集成，”
    发表在*国际学习表征会议*，2019年。'
- en: '[106] X. Liu, M. Cheng, H. Zhang, and C.-J. Hsieh, “Towards robust neural networks
    via random self-ensemble,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*, 2018, pp. 369–385.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] X. 刘, M. 程, H. 张, 和 C.-J. Hsieh, “通过随机自集成实现鲁棒神经网络，” 发表在*欧洲计算机视觉会议 (ECCV)*，2018年，pp.
    369–385。'
- en: '[107] F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel,
    “Ensemble adversarial training: Attacks and defenses,” in *International Conference
    on Learning Representations*, 2018.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] F. Tramèr, A. 库拉金, N. Papernot, I. 古德费洛, D. Boneh, 和 P. McDaniel, “集成对抗训练：攻击与防御，”
    发表在*国际学习表征会议*，2018年。'
- en: '[108] B. Wang, S. Webb, and T. Rainforth, “Statistically robust neural network
    classification,” *arXiv preprint arXiv:1912.04884*, 2019.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] B. 王, S. Webb, 和 T. Rainforth, “统计上鲁棒的神经网络分类，” *arXiv预印本 arXiv:1912.04884*，2019年。'
- en: '[109] R. Zhai, C. Dan, D. He, H. Zhang, B. Gong, P. Ravikumar, C.-J. Hsieh,
    and L. Wang, “Macer: Attack-free and scalable robust training via maximizing certified
    radius,” in *International Conference on Learning Representations*, 2019.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] R. Zhai, C. Dan, D. He, H. Zhang, B. Gong, P. Ravikumar, C.-J. Hsieh,
    和 L. Wang, “Macer：通过最大化认证半径实现无攻击和可扩展的鲁棒训练，” 发表在*国际学习表征会议*，2019年。'
- en: '[110] F. Croce and M. Hein, “Provable robustness against all adversarial $l\_p$-perturbations
    for $p$ $geq1$,” in *International Conference on Learning Representations*, 2019.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] F. Croce 和 M. Hein, “针对所有对抗性 $l\_p$-扰动的可证明鲁棒性，$p$ $geq1$，” 发表在*国际学习表征会议*，2019年。'
- en: '[111] A. Raghunathan, J. Steinhardt, and P. Liang, “Certified defenses against
    adversarial examples,” in *International Conference on Learning Representations*,
    2018.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] A. Raghunathan, J. Steinhardt, 和 P. Liang, “对抗样本的认证防御，” 发表在*国际学习表征会议*，2018年。'
- en: '[112] E. Wong and Z. Kolter, “Provable defenses against adversarial examples
    via the convex outer adversarial polytope,” in *International Conference on Machine
    Learning*, 2018, pp. 5286–5295.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] E. Wong 和 Z. Kolter, “通过凸外对抗多面体证明对抗样本的防御，” 发表在*国际机器学习会议*，2018年，pp. 5286–5295。'
- en: '[113] A. Boopathy, T.-W. Weng, P.-Y. Chen, S. Liu, and L. Daniel, “Cnn-cert:
    An efficient framework for certifying robustness of convolutional neural networks,”
    in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 33, 2019,
    pp. 3240–3247.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] A. Boopathy, 翁特瓦，陈平一，刘颂和丹尼尔,“Cnn-cert: 卷积神经网络鲁棒性认证的高效框架”，发表于《人工智能AAAI学术会议论文集》，第33卷，2019年，3240-3247页。'
- en: '[114] H. Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L. Daniel, “Efficient
    neural network robustness certification with general activation functions,” in
    *Advances in neural information processing systems*, 2018, pp. 4939–4948.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] 张华，翁特瓦，陈平一，谢朝君和丹尼尔，"通用激活函数下的高效神经网络鲁棒性认证"，收录于《神经信息处理系统进展》2018年，4939-4948页。'
- en: '[115] T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, L. Daniel, and I. Dhillon,
    “Towards fast computation of certified robustness for relu networks,” in *International
    Conference on Machine Learning (ICML)*, 2018.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] T.-W. Weng, 张华, 陈平一, 宋哲, 谢朝君, 丹尼尔 和迪隆,“针对Relu网络快速计算鲁棒性的方法”，发表于《国际机器学习会议(ICML)》，2018年。'
- en: '[116] T.-W. Weng, H. Zhang, P.-Y. Chen, J. Yi, D. Su, Y. Gao, C.-J. Hsieh,
    and L. Daniel, “Evaluating the robustness of neural networks: An extreme value
    theory approach,” *arXiv preprint arXiv:1801.10578*, 2018.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] T.-W. Weng, 张华, 陈平一, 宜杰, 苏迪, 高阳, 谢朝君 和丹尼尔,“评估神经网络的鲁棒性：基于极值理论方法”， 《arXiv预印本arXiv:1801.10578》，2018年。'
- en: '[117] M. Hein and M. Andriushchenko, “Formal guarantees on the robustness of
    a classifier against adversarial manipulation,” in *Advances in Neural Information
    Processing Systems*, 2017, pp. 2266–2276.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] M. Hein 和 M. Andriushchenko, “对抗操纵下分类器鲁棒性的正式保证”，发表于《神经信息处理系统进展》，2017年，2266-2276页。'
- en: '[118] G. Singh, T. Gehr, M. Mirman, M. Püschel, and M. Vechev, “Fast and effective
    robustness certification,” in *Advances in Neural Information Processing Systems*,
    2018, pp. 10 802–10 813.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] G. Singh, T. Gehr, M. Mirman, M. Püschel 和 M. Vechev, “快速有效的鲁棒性认证”，发表于《神经信息处理系统进展》，2018年，10 802–10 813页。'
- en: '[119] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and
    M. Vechev, “Ai2: Safety and robustness certification of neural networks with abstract
    interpretation,” in *2018 IEEE Symposium on Security and Privacy (SP)*.   IEEE,
    2018, pp. 3–18.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri 和 M. Vechev，“Ai2：具有抽象解释的神经网络安全鲁棒性认证”，发表于《2018年IEEE安全与隐私研讨会》.   IEEE,
    2018年，3-18页。'
- en: '[120] C. Mao, Z. Zhong, J. Yang, C. Vondrick, and B. Ray, “Metric learning
    for adversarial robustness,” in *Advances in Neural Information Processing Systems*,
    2019, pp. 478–489.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] C. Mao, Z. Zhong, J. Yang, C. Vondrick 和 B. Ray, “对抗鲁棒性的度量学习”，发表于《神经信息处理系统进展》，2019年，478-489页。'
- en: '[121] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. I. Jordan, “Theoretically
    principled trade-off between robustness and accuracy,” in *ICML*, 2019.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] 张华, 于洋, 焦佳伦, 狄东, 艾尔, 潮伊 和约旦M. I.,“理论上基于准确性和鲁棒性的权衡”，发表于《ICML》，2019年。'
- en: '[122] C. Tang, Y. Fan, and A. Yezzi, “An adaptive view of adversarial robustness
    from test-time smoothing defense,” *arXiv preprint arXiv:1911.11881*, 2019.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] C. Tang, Y. Fan, 和 A. Yezzi, “一种适应性的测试时间平滑防御下的对抗鲁棒性的视图”，《arXiv预印本arXiv:1911.11881》，2019年。'
- en: '[123] Z. Yan, Y. Guo, and C. Zhang, “Deep defense: Training dnns with improved
    adversarial robustness,” in *Advances in Neural Information Processing Systems*,
    2018, pp. 419–428.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] 阎智，郭阳和张超，“深度防御：训练具有改进对抗鲁棒性的深度神经网络”，发表于《神经信息处理系统进展》，2018年，419-428页。'
- en: '[124] A. S. Ross and F. Doshi-Velez, “Improving the adversarial robustness
    and interpretability of deep neural networks by regularizing their input gradients,”
    in *Thirty-second AAAI conference on artificial intelligence*, 2018.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] A. S. Ross 和 F. Doshi-Velez, “通过正则化它们的输入梯度提高深度神经网络的对抗鲁棒性和可解释性”，发表于《第三十二届AAAI人工智能会议》，2018年。'
- en: '[125] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier, “Parseval
    networks: improving robustness to adversarial examples,” in *Proceedings of the
    34th International Conference on Machine Learning-Volume 70*, 2017, pp. 854–863.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, 和 N. Usunier, “Parseval网络：提高对抗样本的鲁棒性”，发表于《第34届国际机器学习大会论文集70卷》，2017年，854-863页。'
- en: '[126] J. Sokolić, R. Giryes, G. Sapiro, and M. R. Rodrigues, “Robust large
    margin deep neural networks,” *IEEE Transactions on Signal Processing*, vol. 65,
    no. 16, pp. 4265–4280, 2017.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] J. Sokolić, R. Giryes, G. Sapiro 和 M. R. Rodrigues, “鲁棒大边界深度神经网络”，《IEEE信号处理杂志》，第65卷，第16期，2017年，4265-4280页。'
- en: '[127] S. Gu and L. Rigazio, “Towards deep neural network architectures robust
    to adversarial examples,” 2015.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] S. Gu 和 L. Rigazio，“朝着对抗样本鲁棒的深度神经网络架构发展，”2015。'
- en: '[128] N. Das, M. Shanbhogue, S.-T. Chen, F. Hohman, S. Li, L. Chen, M. E. Kounavis,
    and D. H. Chau, “Shield: Fast, practical defense and vaccination for deep learning
    using jpeg compression,” in *Proceedings of the 24th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, 2018, pp. 196–204.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] N. Das, M. Shanbhogue, S.-T. Chen, F. Hohman, S. Li, L. Chen, M. E. Kounavis,
    和 D. H. Chau，“Shield：使用JPEG压缩的深度学习快速实用防御与疫苗接种，”在*第24届ACM SIGKDD国际知识发现与数据挖掘大会论文集*，2018，第196–204页。'
- en: '[129] C. Guo, M. Rana, M. Cisse, and L. van der Maaten, “Countering adversarial
    images using input transformations,” in *International Conference on Learning
    Representations*, 2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] C. Gu, M. Rana, M. Cisse, 和 L. van der Maaten，“使用输入变换对抗对抗性图像，”在*国际学习表征会议*，2018。'
- en: '[130] E. Raff, J. Sylvester, S. Forsyth, and M. McLean, “Barrage of random
    transforms for adversarially robust defense,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 6528–6537.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] E. Raff, J. Sylvester, S. Forsyth, 和 M. McLean，“对抗性鲁棒防御的随机变换策略，”在*IEEE
    计算机视觉与模式识别会议论文集*，2019，第6528–6537页。'
- en: '[131] A. Shafahi, P. Saadatpanah, C. Zhu, A. Ghiasi, C. Studer, D. Jacobs,
    and T. Goldstein, “Adversarially robust transfer learning,” in *International
    Conference on Learning Representations*, 2019.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] A. Shafahi, P. Saadatpanah, C. Zhu, A. Ghiasi, C. Studer, D. Jacobs,
    和 T. Goldstein，“对抗性鲁棒迁移学习，”在*国际学习表征会议*，2019。'
- en: '[132] S. Zagoruyko and N. Komodakis, “Paying more attention to attention: Improving
    the performance of convolutional neural networks via attention transfer,” *arXiv
    preprint arXiv:1612.03928*, 2016.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] S. Zagoruyko 和 N. Komodakis，“更多关注注意力：通过注意力转移提高卷积神经网络的性能，”*arXiv 预印本 arXiv:1612.03928*，2016。'
- en: '[133] H. Zhang, H. Chen, C. Xiao, S. Gowal, R. Stanforth, B. Li, D. Boning,
    and C.-J. Hsieh, “Towards stable and efficient training of verifiably robust neural
    networks,” in *International Conference on Learning Representations*, 2019.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] H. Zhang, H. Chen, C. Xiao, S. Gowal, R. Stanforth, B. Li, D. Boning,
    和 C.-J. Hsieh，“朝着稳定高效的可验证鲁棒神经网络训练方向发展，”在*国际学习表征会议*，2019。'
- en: '[134] S. Webb, T. Rainforth, Y. W. Teh, and M. P. Kumar, “A statistical approach
    to assessing neural network robustness,” in *International Conference on Learning
    Representations*, 2018.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] S. Webb, T. Rainforth, Y. W. Teh, 和 M. P. Kumar，“评估神经网络鲁棒性的统计方法，”在*国际学习表征会议*，2018。'
- en: '[135] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana, “Certified
    robustness to adversarial examples with differential privacy,” in *2019 IEEE Symposium
    on Security and Privacy (SP)*.   IEEE, 2019, pp. 656–672.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, 和 S. Jana，“通过差分隐私对抗样本的认证鲁棒性，”在*2019
    IEEE 安全与隐私研讨会（SP）*。IEEE，2019，第656–672页。'
- en: '[136] J. Cohen, E. Rosenfeld, and Z. Kolter, “Certified adversarial robustness
    via randomized smoothing,” in *International Conference on Machine Learning*,
    2019, pp. 1310–1320.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] J. Cohen, E. Rosenfeld, 和 Z. Kolter，“通过随机平滑实现认证对抗鲁棒性，”在*国际机器学习会议*，2019，第1310–1320页。'
- en: '| ![[Uncaptioned image]](img/a40b46eca09dc0cf6d728c148ac6273d.png) | Samuel
    Henrique Silva is currently a Ph.D. student and research fellow at the Open Cloud
    Institute of University of Texas at San Antonio (UTSA), San Antonio, TX, USA.
    Samuel received the Bachelor of Science (B.Sc.) degree in Control and Automation
    Engineering from State University of Campinas, Campinas, Brazil, in 2012 and the
    M.S. degree in Electrical Engineering from the University of Notre Dame, Notre
    Dame, IN, USA in 2016\. He is a member of the IEEE, Eta Kappa Nu honor society.
    Samuel’s research interests are in the areas of artificial intelligence, robustness
    in deep learning models, autonomous decision making, multi-agent systems and adversarial
    environments. |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图像]](img/a40b46eca09dc0cf6d728c148ac6273d.png) | Samuel Henrique Silva
    目前是德克萨斯大学圣安东尼奥分校（UTSA）开放云研究所的博士生和研究员。Samuel于2012年在巴西坎皮纳斯州立大学获得控制与自动化工程学士学位，并于2016年在美国印第安纳大学诺特丹分校获得电气工程硕士学位。他是IEEE会员和Eta
    Kappa Nu荣誉学会成员。Samuel的研究兴趣包括人工智能、深度学习模型的鲁棒性、自主决策、多智能体系统和对抗性环境。 |'
- en: '| ![[Uncaptioned image]](img/25db4e7410eba7c4686e3373ceecbcfb.png) | Paul Rad
    is a co-founder and Associate Director of the Open Cloud Institute (OCI), and
    an Associate Professor with the Information Systems and Cyber Security Department
    at the University of Texas at San Antonio. He received his first B.S. degree from
    Sharif University of Technology in Computer Engineering in 1994, his 1st master
    in artificial intelligence from the Tehran Polytechnic, his 2nd master in computer
    science from the University of Texas at San Antonio (Magna Cum Laude) in 1999,
    and his Ph.D. in electrical and computer engineering from the University of Texas
    at San Antonio. He was a recipient of the Most Outstanding Graduate Student in
    the College of Engineering, 2016, earned the Rackspace Innovation Mentor Program
    Award for establishing Rackspace patent community board structure and mentoring
    employees (2012), earned the Dell Corporation Company Excellence (ACE) Award for
    exceptional performance and innovative product research and development contributions
    (2007), and earned the Dell Inventor Milestone Award, Top 3 Dell Inventor of the
    year (2005). He holds 15 U.S. patents on cyber infrastructure, cloud computing,
    and big data analytics with over 300 product citations by top fortune 500 leading
    technology companies such as Amazon, Microsoft, IBM, Cisco, Amazon Technologies,
    HP, and VMware. He has advised over 200 companies on cloud computing and data
    analytics with over 50 keynote presentations. High performance cloud group chair
    at the Cloud Advisory Council (CAC), OpenStack Foundation Member (the #1 open
    source cloud software), San Antonio Tech Bloc Founding Member, and Children’s
    Hospital of San Antonio Foundation board member. |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/25db4e7410eba7c4686e3373ceecbcfb.png) | 保罗·拉德是开放云研究所（OCI）的联合创始人和副主任，同时也是德克萨斯大学圣安东尼奥分校信息系统与网络安全系的副教授。他于1994年从沙里夫大学获得计算机工程学士学位，随后从德黑兰理工学院获得人工智能硕士学位，从德克萨斯大学圣安东尼奥分校获得计算机科学硕士学位（优等）和电气与计算机工程博士学位。他曾获得工程学院2016年度最杰出研究生奖，因建立Rackspace专利社区委员会结构并指导员工而获得Rackspace创新导师计划奖（2012年），因在产品研发领域的卓越表现而获得戴尔公司卓越奖（ACE）（2007年），以及戴尔发明者里程碑奖，被评为年度前三大戴尔发明者（2005年）。他拥有15项美国专利，涉及网络基础设施、云计算和大数据分析，并且其产品被包括亚马逊、微软、IBM、思科、亚马逊技术、惠普和VMware等顶级财富500强技术公司引用超过300次。他曾为200多家公司提供云计算和数据分析咨询，并进行了超过50次主题演讲。担任云顾问委员会（CAC）高性能云组主席，OpenStack基金会成员（全球排名第一的开源云软件），圣安东尼奥科技区创始成员，以及圣安东尼奥儿童医院基金会董事会成员。'
