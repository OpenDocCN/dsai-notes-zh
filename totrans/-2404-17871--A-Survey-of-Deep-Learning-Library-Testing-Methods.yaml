- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:33:05'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:33:05
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2404.17871] A Survey of Deep Learning Library Testing Methods'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2404.17871] 深度学习库测试方法综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.17871](https://ar5iv.labs.arxiv.org/html/2404.17871)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.17871](https://ar5iv.labs.arxiv.org/html/2404.17871)
- en: A Survey of Deep Learning Library Testing Methods
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习库测试方法综述
- en: Xiaoyu Zhang [zxy0927@stu.xjtu.edu.cn](mailto:zxy0927@stu.xjtu.edu.cn) School
    of Cyber Science and Engineering, Xi’an Jiaotong UniversityP.O. Box 1212Xi’anChina
    ,  Weipeng Jiang [lenijwp@stu.xjtu.edu.cn](mailto:lenijwp@stu.xjtu.edu.cn) School
    of Cyber Science and Engineering, Xi’an Jiaotong UniversityXi’anChina ,  Chao
    Shen [chaoshen@mail.xjtu.edu.cn](mailto:chaoshen@mail.xjtu.edu.cn) School of Cyber
    Science and Engineering, Xi’an Jiaotong UniversityXi’anChina ,  Qi Li [qli01@tsinghua.edu.cn](mailto:qli01@tsinghua.edu.cn)
    Institute for Network Sciences and Cyberspace, Tsinghua UniversityBeijingChina
    ,  Qian Wang [qianwang@whu.edu.cn](mailto:qianwang@whu.edu.cn) School of Cyber
    Science and Engineering, Wuhan UniversityWuhanChina ,  Chenhao Lin [linchenhao@xjtu.edu.cn](mailto:linchenhao@xjtu.edu.cn)
    School of Cyber Science and Engineering, Xi’an Jiaotong UniversityXi’anChina  and 
    Xiaohong Guan [xhguan@mail.xjtu.edu.cn](mailto:xhguan@mail.xjtu.edu.cn) School
    of Cyber Science and Engineering, Xi’an Jiaotong UniversityXi’anChina
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 张小宇 [zxy0927@stu.xjtu.edu.cn](mailto:zxy0927@stu.xjtu.edu.cn) 西安交通大学网络科学与工程学院
    邮政编码 1212 西安 中国，姜伟鹏 [lenijwp@stu.xjtu.edu.cn](mailto:lenijwp@stu.xjtu.edu.cn)
    西安交通大学网络科学与工程学院 西安 中国，沈超 [chaoshen@mail.xjtu.edu.cn](mailto:chaoshen@mail.xjtu.edu.cn)
    西安交通大学网络科学与工程学院 西安 中国，李琦 [qli01@tsinghua.edu.cn](mailto:qli01@tsinghua.edu.cn)
    清华大学网络科学与 cyberspace 研究所 北京 中国，王千 [qianwang@whu.edu.cn](mailto:qianwang@whu.edu.cn)
    武汉大学网络科学与工程学院 武汉 中国，林晨浩 [linchenhao@xjtu.edu.cn](mailto:linchenhao@xjtu.edu.cn)
    西安交通大学网络科学与工程学院 西安 中国，关晓红 [xhguan@mail.xjtu.edu.cn](mailto:xhguan@mail.xjtu.edu.cn)
    西安交通大学网络科学与工程学院 西安 中国
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: In recent years, software systems powered by deep learning (DL) techniques have
    significantly facilitated people’s lives in many aspects. As the backbone of these
    DL systems, various DL libraries undertake the underlying optimization and computation.
    However, like traditional software, DL libraries are not immune to bugs, which
    can pose serious threats to users’ personal property and safety. Studying the
    characteristics of DL libraries, their associated bugs, and the corresponding
    testing methods is crucial for enhancing the security of DL systems and advancing
    the widespread application of DL technology. This paper provides an overview of
    the testing research related to various DL libraries, discusses the strengths
    and weaknesses of existing methods, and provides guidance and reference for the
    application of the DL library. This paper first introduces the workflow of DL
    underlying libraries and the characteristics of three kinds of DL libraries involved,
    namely DL framework, DL compiler, and DL hardware library. It then provides definitions
    for DL underlying library bugs and testing. Additionally, this paper summarizes
    the existing testing methods and tools tailored to these DL libraries separately
    and analyzes their effectiveness and limitations. It also discusses the existing
    challenges of DL library testing and outlines potential directions for future
    research.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，依靠深度学习（DL）技术的软件系统在许多方面显著改善了人们的生活。作为这些DL系统的支柱，各种DL库承担了底层优化和计算。然而，与传统软件一样，DL库也难免出现缺陷，这可能对用户的个人财产和安全构成严重威胁。研究DL库的特征、相关缺陷及对应的测试方法，对于提高DL系统的安全性和推动DL技术的广泛应用至关重要。本文概述了与各种DL库相关的测试研究，讨论了现有方法的优缺点，并为DL库的应用提供了指导和参考。本文首先介绍了DL底层库的工作流程以及涉及的三种DL库的特征，即DL框架、DL编译器和DL硬件库。接着，提供了DL底层库缺陷和测试的定义。此外，本文分别总结了针对这些DL库的现有测试方法和工具，并分析了它们的有效性和局限性。还讨论了DL库测试的现有挑战，并概述了未来研究的潜在方向。
- en: 'Deep Learning Testing, Deep Learning Library Testing, Deep Learning Security,
    Deep Learning, Software Testing^†^†copyright: none^†^†ccs: Security and privacy Software
    security engineering^†^†ccs: Security and privacy Systems security'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习测试，深度学习库测试，深度学习安全，深度学习，软件测试^†^†版权：无^†^†ccs：安全与隐私 软件安全工程^†^†ccs：安全与隐私 系统安全
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: With the development of deep learning (DL) techniques, the DL systems that are
    driven by DL models have been applied in many fields, providing societal benefits
    in areas like image recognition (He et al., [2016](#bib.bib38)), self-driving (Grigorescu
    et al., [2020](#bib.bib31)), and natural language processing (Li, [2018](#bib.bib58)).
    With the wide application of DL systems in various fields, the security and safety
    of the underlying DL library have received more and more attention. As the backbone
    of DL systems, the DL library (e.g., PyTorch) is responsible for performing specific
    computations for training or inference DL models and implementing optimized operations
    on DL hardware. The DL library is important for DL development and systems. Tesla
    relies on PyTorch which is one of the most popular DL libraries to solve problems
    related to the self-driving domain (Sun, [2020](#bib.bib101)). TensorFlow, which
    is another popular DL library, undertakes many important business tasks of Google,
    Intel and other companies (TensorFlow, [2020](#bib.bib103)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习（DL）技术的发展，基于DL模型的DL系统已被应用于多个领域，为图像识别（He等，[2016](#bib.bib38)）、自动驾驶（Grigorescu等，[2020](#bib.bib31)）和自然语言处理（Li，[2018](#bib.bib58)）等领域带来了社会效益。随着DL系统在各个领域的广泛应用，底层DL库的安全性和可靠性受到越来越多的关注。作为DL系统的核心，DL库（例如PyTorch）负责执行训练或推理DL模型的特定计算，并在DL硬件上实现优化操作。DL库对于DL的开发和系统至关重要。特斯拉依赖于PyTorch，这是最受欢迎的DL库之一，用于解决与自动驾驶领域相关的问题（Sun，[2020](#bib.bib101)）。另一个流行的DL库TensorFlow承担了Google、Intel等公司的许多重要业务任务（TensorFlow，[2020](#bib.bib103)）。
- en: Similar to traditional software, the DL library also has bugs, which can cause
    the DL systems it supports to make erroneous predictions, generate huge overhead,
    and even crash (Pham et al., [2019](#bib.bib89); Wei et al., [2022](#bib.bib110)),
    thereby jeopardizing user property and personal safety. For example, in recent
    years, the self-driving systems developed by Tesla and Uber have experienced abnormal
    behaviors during driving and eventually led to fatal crashes (Ken, [[n. d.]](#bib.bib51);
    staff, [[n. d.]](#bib.bib99)), which further arouse people’s concerns about the
    vulnerabilities of DL systems and the underlying libraries.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于传统软件，DL库也存在漏洞，这可能导致其支持的DL系统做出错误预测、产生巨大开销，甚至崩溃（Pham等，[2019](#bib.bib89)；Wei等，[2022](#bib.bib110)），从而危害用户财产和个人安全。例如，近年来，特斯拉和优步开发的自动驾驶系统在驾驶过程中出现异常行为，最终导致致命的碰撞（Ken，[n. d.](#bib.bib51)；staff，[n. d.](#bib.bib99)），这进一步引发了人们对DL系统及其底层库漏洞的关注。
- en: '![Refer to caption](img/6d73a73f6f59bed907b2775cd47c597c.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6d73a73f6f59bed907b2775cd47c597c.png)'
- en: Fig. 1\. Overview of This Paper
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 本文概述
- en: At present, researchers have proposed a series of tools and methods (Pham et al.,
    [2019](#bib.bib89); Wei et al., [2022](#bib.bib110); Deng et al., [2022](#bib.bib23))
    to discover vulnerabilities and bugs such as crashes, overflows, and numerical
    errors on the DL libraries represented by DL frameworks (e.g., TensorFlow, PyTorch),
    aiming to guarantee the security and usability of the DL system it supports. In
    addition, many countries have established relevant acts and policies, proposing
    that technical means should be used to test and verify DL systems and software.
    For example, the National AI Initiative Act of 2020 introduced by the United States
    proposed to support research on the security of ‘software and hardware used in
    artificial intelligence systems’ (of Science and Policy, [[n. d.]](#bib.bib86)).
    In 2023, the European Union proposed the AI act that called to organize testing
    of the high-risk AI system through technical means (Commission, [2021](#bib.bib19)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，研究人员提出了一系列工具和方法（Pham等，[2019](#bib.bib89)；Wei等，[2022](#bib.bib110)；Deng等，[2022](#bib.bib23)）以发现DL库（例如TensorFlow、PyTorch）中的漏洞和错误，如崩溃、溢出和数值错误，旨在保障所支持DL系统的安全性和可用性。此外，许多国家已经制定了相关法规和政策，提议应使用技术手段测试和验证DL系统和软件。例如，美国提出的2020年国家AI倡议法提议支持对“用于人工智能系统的软件和硬件”的安全性进行研究（of
    Science and Policy，[n. d.](#bib.bib86)）。2023年，欧盟提出了AI法案，呼吁通过技术手段组织对高风险AI系统的测试（Commission，[2021](#bib.bib19)）。
- en: How to deeply understand the vulnerabilities and bugs of the underlying libraries
    of the DL system and design testing methods for these libraries needs to be solved
    urgently and is of great significance. Although researchers have proposed various
    DL library testing methods, there are still many challenges. Firstly, there are
    various types of DL libraries, including DL frameworks, DL compilers, etc. Different
    DL libraries undertake different calculation and optimization functions, and there
    are significant differences between their inputs, outputs and implementation.
    As a result, existing testing methods are diverse and highly targeted to specific
    libraries, leading to a lack of general and systematic testing methods for different
    DL libraries and evaluation metrics for different test methods. Furthermore, existing
    research has a limited understanding of DL library bugs and mainly focuses on
    crashes and numerical errors. They lack the ability to comprehensively evaluate
    other bugs in DL libraries (e.g., performance bugs), which limits the effectiveness
    of these methods. Therefore, it is significant to conduct induction, analysis,
    and discussion on the existing research in the field of DL library testing to
    find limitations and provide guidance for subsequent research directions in related
    fields.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如何深入理解深度学习系统底层库的漏洞和错误，并为这些库设计测试方法，需要紧急解决并具有重要意义。尽管研究人员提出了各种深度学习库测试方法，但仍然面临许多挑战。首先，深度学习库类型多样，包括深度学习框架、深度学习编译器等。不同的深度学习库承担不同的计算和优化功能，其输入、输出和实现之间存在显著差异。因此，现有的测试方法多种多样且高度针对特定库，导致不同深度学习库的通用和系统化测试方法以及不同测试方法的评估指标缺乏。此外，现有研究对深度学习库的错误理解有限，主要关注崩溃和数值错误。它们缺乏全面评估深度学习库中其他错误（如性能错误）的能力，这限制了这些方法的有效性。因此，对深度学习库测试领域现有研究进行归纳、分析和讨论，找出其局限性并为相关领域后续研究方向提供指导具有重要意义。
- en: 'However, existing surveys on the DL library are limited. Researchers either
    focused on some parts of the DL underlying library, for example, Zhang et al. (Zhang
    et al., [2020a](#bib.bib120)) and Ma et al. (Ma et al., [2023a](#bib.bib75)) focused
    on testing methods on DL models and frameworks and Li et al. (Li et al., [2020](#bib.bib61))
    paid attention to DL compilers, or investigated the vulnerabilities in DL software
    and model from a macroscopic perspective, but could not provide a fine-grained
    introduction and analysis to the library testing methods (Braiek and Khomh, [2020](#bib.bib5);
    Gezici and Tarhan, [2022](#bib.bib30); Martínez-Fernández et al., [2022](#bib.bib77);
    Zhang et al., [2022](#bib.bib123)). There is a lack of comprehensive and detailed
    research on the testing methods on DL underlying libraries and their bugs. To
    fill the gap, this paper systematically summarizes testing methods on three kinds
    of DL libraries, namely, DL framework, DL compiler, and DL hardware library, and
    analyzes their strengths and weaknesses. Building upon this foundation, this paper
    delves into the challenges and future research opportunities in DL library testing.
    Our goal is to promote the further development of DL library testing research,
    so as to ensure the safety and security of DL systems and accelerate their application.
    The main contribution of this work can be summarized as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现有的深度学习库调查仍然有限。研究者们要么集中于深度学习库的某些部分，例如，张等人（Zhang et al., [2020a](#bib.bib120)）和马等人（Ma
    et al., [2023a](#bib.bib75)）关注于深度学习模型和框架的测试方法，而李等人（Li et al., [2020](#bib.bib61)）关注于深度学习编译器，要么从宏观角度研究深度学习软件和模型中的漏洞，但无法对库测试方法进行细致的介绍和分析（Braiek
    and Khomh, [2020](#bib.bib5); Gezici and Tarhan, [2022](#bib.bib30); Martínez-Fernández
    et al., [2022](#bib.bib77); Zhang et al., [2022](#bib.bib123)）。对深度学习底层库及其漏洞的全面详细研究仍然缺乏。为填补这一空白，本文系统总结了三种深度学习库的测试方法，即深度学习框架、深度学习编译器和深度学习硬件库，并分析了它们的优缺点。在此基础上，本文深入探讨了深度学习库测试中的挑战和未来研究机会。我们的目标是推动深度学习库测试研究的进一步发展，以确保深度学习系统的安全性并加速其应用。该工作的主要贡献总结如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present the first comprehensive and detailed survey of testing techniques
    for DL libraries, filling the gap of existing DL testing survey work that focuses
    heavily on DL models and lacks insight into the underlying bugs on DL libraries.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了首个全面详细的深度学习（DL）库测试技术调查，填补了现有深度学习测试调查中主要集中于深度学习模型而对深度学习库中的潜在错误缺乏洞察的空白。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a novel taxonomy in three test components to provide an accessible
    overview of works that focus on libraries at different stages in the DL workflow,
    namely DL framework, DL compiler, and DL hardware library, as shown in [Fig. 1](#S1.F1
    "Fig. 1 ‣ 1\. Introduction ‣ A Survey of Deep Learning Library Testing Methods").
    For each stage, we systematically summarize and present existing work according
    to their testing techniques and provide an in-depth analysis at the end to characterize
    some critical problems.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种在三个测试组件中的新分类法，以提供一个关于DL工作流程中不同阶段库的工作概述，即DL框架、DL编译器和DL硬件库，如[图1](#S1.F1
    "Fig. 1 ‣ 1\. Introduction ‣ A Survey of Deep Learning Library Testing Methods")所示。对于每个阶段，我们系统总结并展示了现有工作，按照其测试技术，并在最后提供深入分析以表征一些关键问题。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discuss and outline the main challenges that need to be addressed and future
    research directions, aiming to promote the development of DL software security
    and safety.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论并概述了需要解决的主要挑战和未来的研究方向，旨在推动DL软件安全和可靠性的进展。
- en: The overarching structure of this paper is shown in [Fig. 1](#S1.F1 "Fig. 1
    ‣ 1\. Introduction ‣ A Survey of Deep Learning Library Testing Methods"). We begin
    by building fundamental definitions of the DL library bug and testing. Considering
    the underlying workflow of DL systems and programs, we divide the test objects
    of existing testing methods into three components, namely DL framework, DL compiler,
    and DL hardware library (marked in blue, green, and orange). Based on these three
    test components, we systematically summarize and analyze existing testing methods
    on DL library testing, according to different test techniques (which mainly include
    differential testing, fuzz testing, and metamorphic testing). Furthermore, we
    observe that existing research mainly focuses on two testing properties of DL
    libraries, namely correctness and efficiency. Based on our observation, we have
    analyzed bugs detected by representative methods, aiming to study the advantages
    and limitations of these methods. Finally, we provide a future-gazing on the challenges
    and possible research directions in DL library testing.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的总体结构如[图1](#S1.F1 "Fig. 1 ‣ 1\. Introduction ‣ A Survey of Deep Learning Library
    Testing Methods")所示。我们首先建立了DL库错误和测试的基本定义。考虑到DL系统和程序的底层工作流程，我们将现有测试方法的测试对象分为三个组件，即DL框架、DL编译器和DL硬件库（分别标记为蓝色、绿色和橙色）。基于这三个测试组件，我们系统总结和分析了现有的DL库测试方法，按照不同的测试技术（主要包括差异测试、模糊测试和变形测试）。此外，我们观察到现有研究主要关注DL库的两个测试属性，即正确性和效率。基于我们的观察，我们分析了代表性方法检测到的错误，旨在研究这些方法的优缺点。最后，我们对DL库测试中的挑战和可能的研究方向进行了展望。
- en: More detailed, the paper is organized as follows. [§ 2](#S2 "2\. Preliminary
    ‣ A Survey of Deep Learning Library Testing Methods") describes the workflow of
    the DL underlying libraries, which lead to three test components, and other preliminary
    knowledge. The [§ 3](#S3 "3\. DL Framework Testing ‣ A Survey of Deep Learning
    Library Testing Methods"), [§ 4](#S4 "4\. DL Compiler Testing ‣ A Survey of Deep
    Learning Library Testing Methods") and [§ 5](#S5 "5\. DL Hardware Library Testing
    ‣ A Survey of Deep Learning Library Testing Methods") respectively introduce the
    DL library testing research on the DL framework, DL compiler and DL hardware library,
    and discuss the advantages and limitations of these methods. Then [§ 6](#S6 "6\.
    Future Gazing ‣ A Survey of Deep Learning Library Testing Methods") analyzes the
    current challenges and future research opportunities in the field of DL library
    testing, and [§ 7](#S7 "7\. Conclusion ‣ A Survey of Deep Learning Library Testing
    Methods") concludes this paper.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地说，本文的组织结构如下。[§2](#S2 "2\. Preliminary ‣ A Survey of Deep Learning Library
    Testing Methods")描述了DL底层库的工作流程，这导致了三个测试组件和其他初步知识。[§3](#S3 "3\. DL Framework Testing
    ‣ A Survey of Deep Learning Library Testing Methods")、[§4](#S4 "4\. DL Compiler
    Testing ‣ A Survey of Deep Learning Library Testing Methods")和[§5](#S5 "5\. DL
    Hardware Library Testing ‣ A Survey of Deep Learning Library Testing Methods")分别介绍了DL框架、DL编译器和DL硬件库的DL库测试研究，并讨论了这些方法的优缺点。然后，[§6](#S6
    "6\. Future Gazing ‣ A Survey of Deep Learning Library Testing Methods")分析了DL库测试领域的当前挑战和未来研究机会，[§7](#S7
    "7\. Conclusion ‣ A Survey of Deep Learning Library Testing Methods")总结了本文内容。
- en: 2\. Preliminary
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 初步讨论
- en: 2.1\. DL Model
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. DL模型
- en: A DL model is a parameterized function $F_{\theta}:X\mapsto Y$, where $x\in
    X$ is an $m-$dimensional input and $y\in Y$ is the corresponding output label.
    Typically, a DL model is composed of several connected layers, and an $n$-layered
    model can be represented as $F_{\theta}=l_{1}\circ l_{2}\circ\cdot\cdot\cdot\circ
    l_{n}$, where $l$ represents a layer and $\theta$ is the model weight. The developers
    first need to train the DL model on the given data and update the model weight
    $\theta$ in the training progress. Then, in the inference process, the trained
    DL model can predict the output for the given input (e.g., an image, or a sentence).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型是一个参数化的函数 $F_{\theta}:X\mapsto Y$，其中 $x\in X$ 是一个 $m$ 维输入，$y\in Y$ 是相应的输出标签。通常，深度学习模型由几个连接的层组成，一个
    $n$ 层的模型可以表示为 $F_{\theta}=l_{1}\circ l_{2}\circ\cdot\cdot\cdot\circ l_{n}$，其中
    $l$ 表示一个层，$\theta$ 是模型权重。开发者首先需要在给定的数据上训练深度学习模型，并在训练过程中更新模型权重 $\theta$。然后，在推断过程中，训练好的深度学习模型可以对给定的输入（例如图像或句子）进行预测。
- en: The training process of a DL model consists of the forward propagation stage
    and the backward propagation stage, and the model inference process only uses
    the former. The forward propagation stage calculates model output $F_{\theta}(x_{i})$
    based on the input tensor $x_{i}$ and initialized model weight $\theta$. The backward
    propagation stage evaluates the difference between $F_{\theta}(x_{i})$ and the
    ground truth label $y_{i}$ by a loss function $\mathcal{L}(F_{\theta}(x),y)$ and
    updates model weights $\theta$ to minimize the value of $\mathcal{L}$. The forward
    propagation and backward propagation stages will be repeated until the training
    reaches the predetermined stopping criteria. In the DL program, developers call
    the APIs provided by the DL framework to build and train a DL model and each layer
    $l_{i}$ in the model can be directly constructed by one or several DL framework
    APIs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的训练过程包括前向传播阶段和反向传播阶段，而模型推断过程只使用前向传播阶段。前向传播阶段根据输入张量 $x_{i}$ 和初始化的模型权重 $\theta$
    计算模型输出 $F_{\theta}(x_{i})$。反向传播阶段通过损失函数 $\mathcal{L}(F_{\theta}(x),y)$ 评估 $F_{\theta}(x_{i})$
    和真实标签 $y_{i}$ 之间的差异，并更新模型权重 $\theta$ 以最小化 $\mathcal{L}$ 的值。前向传播和反向传播阶段将重复进行，直到训练达到预定的停止标准。在深度学习程序中，开发者调用深度学习框架提供的
    API 来构建和训练深度学习模型，模型中的每一层 $l_{i}$ 都可以通过一个或多个深度学习框架 API 直接构建。
- en: 2.2\. DL Library
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 深度学习库
- en: Various DL libraries implement the abstract DL model based on the program and
    perform specific operations and optimizations on the underlying hardware to obtain
    computation results for inference and training of DL models. Executing the DL
    program and building a DL model on the DL underlying libraries mainly involves
    three components, namely DL framework, DL compiler, and DL hardware library. [Fig. 2](#S2.F2
    "Fig. 2 ‣ 2.2\. DL Library ‣ 2\. Preliminary ‣ A Survey of Deep Learning Library
    Testing Methods") shows the overarching workflow. The code in dotted boxes presents
    the demo inputs of each component. Developers first need to call the DL framework
    APIs to construct a DL program. The DL framework executes the DL program and constructs
    the corresponding DL model. Then the abstract model will be passed to the DL compiler,
    and the DL compiler translates the input model into intermediate representations
    (IRs), optimizes it according to the target hardware, and outputs optimized operators
    and code. Finally, the DL hardware library (e.g., cuDNN) accepts the output of
    the compiler and maps calculations to the DL hardware to perform calculations
    and obtain the results. In this paper, we focus on the above three kinds of DL
    libraries.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 各种深度学习库在程序的基础上实现了抽象的深度学习模型，并在底层硬件上执行特定操作和优化，以获得深度学习模型推断和训练的计算结果。执行深度学习程序并在深度学习底层库上构建深度学习模型主要涉及三个组件，即深度学习框架、深度学习编译器和深度学习硬件库。[图
    2](#S2.F2 "图 2 ‣ 2.2\. 深度学习库 ‣ 2\. 初步 ‣ 深度学习库测试方法综述") 显示了总体工作流程。虚线框中的代码展示了每个组件的演示输入。开发者首先需要调用深度学习框架的
    API 来构建深度学习程序。深度学习框架执行深度学习程序并构建相应的深度学习模型。然后，抽象模型将传递给深度学习编译器，深度学习编译器将输入模型转换为中间表示（IR），根据目标硬件进行优化，并输出优化后的操作符和代码。最后，深度学习硬件库（例如
    cuDNN）接受编译器的输出，并将计算映射到深度学习硬件上，以执行计算并获得结果。本文重点关注上述三种深度学习库。
- en: '![Refer to caption](img/24bf02a1c06c145892c6866af51d8369.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/24bf02a1c06c145892c6866af51d8369.png)'
- en: Fig. 2\. Overarching Workflow of the DL system and DL program
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 深度学习系统和深度学习程序的总体工作流程
- en: 2.2.1\. DL Framework
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. 深度学习框架
- en: To allow developers to design and build DL models conveniently and simplify
    the implementation of models, industry and academia have proposed various DL frameworks,
    including TensorFlow (Abadi, [2016](#bib.bib4)), Pytorch (Paszke et al., [2019](#bib.bib87)),
    ONNX (Microsoft, [2023](#bib.bib80)), etc. TensorFlow supports a variety of program
    languages (e.g., C++, Python, and Go) and is currently one of the most popular
    DL frameworks. TensorFlow Lite is designed to implement and optimize DL techniques
    for mobile and embedded scenarios. PyTorch is rewritten and optimized based on
    the DL framework Torch. Nowadays, PyTorch and TensorFlow have active developer
    communities and are the most commonly used test objects in DL framework testing
    research. Open Neural Network Exchange (ONNX) defines an extensible computation
    graph model and provides an open-sourced format for DL models, which allows models
    of different DL frameworks can be easily transformed into ONNX and facilitates
    model conversion between frameworks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便开发人员设计和构建DL模型，并简化模型的实现，工业界和学术界提出了各种DL框架，包括TensorFlow（Abadi，[2016](#bib.bib4)），PyTorch（Paszke等，[2019](#bib.bib87)），ONNX（Microsoft，[2023](#bib.bib80)）等。
    TensorFlow支持多种编程语言（例如C++，Python和Go），目前是最流行的DL框架之一。 TensorFlow Lite的设计用于在移动和嵌入式场景中实现和优化DL技术。
    PyTorch是基于DL框架Torch重写和优化的。 如今，PyTorch和TensorFlow拥有活跃的开发者社区，并且是DL框架测试研究中最常用的测试对象。
    开放式神经网络交换（ONNX）定义了可扩展的计算图模型，并提供了DL模型的开源格式，使得不同DL框架的模型可以轻松转换为ONNX，并促进了各种框架之间的模型转换。
- en: Like traditional software, DL frameworks provide numerous APIs to call functions
    and perform operations. Taking PyTorch (Paszke et al., [2019](#bib.bib87)) as
    an example, its API includes performing basic matrix operations (e.g., torch.mul
    for multiply operation), calculating loss functions (e.g., torch.nn.MSELoss for
    measuring mean squared error), and building models layers (e.g., torch.nn.Conv2d
    for convolution layers). After the developer calls these APIs in the program,
    the DL framework will build the corresponding abstract DL model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统软件类似，DL框架提供了许多API来调用函数并执行操作。 以PyTorch（Paszke et al.，[2019](#bib.bib87)）为例，其API包括执行基本的矩阵运算（例如，torch.mul用于乘法操作），计算损失函数（例如，torch.nn.MSELoss用于测量均方误差），和构建模型层（例如，torch.nn.Conv2d用于卷积层）。
    开发人员在程序中调用这些API之后，DL框架将构建相应的抽象DL模型。
- en: 2.2.2\. DL Compiler
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. DL编译器
- en: To reduce the burden of manually optimizing DL models on various DL hardware
    (e.g., TPU) and hardware libraries (e.g., cuDNN), researchers have developed the
    DL compiler (Li et al., [2020](#bib.bib61)). It takes the abstract model described
    by the DL framework as input, and then automatically optimizes and generates operators
    and codes as output to ensure that the DL hardware library can efficiently execute
    calculations of the DL model. Therefore, DL compilers are generally closely related
    to and work together with the DL framework that builds abstract DL models. The
    currently popular DL compilers include Glow (Rotem et al., [2018](#bib.bib94)),
    TVM (Chen et al., [2018](#bib.bib14)), etc. Glow is designed to implement state-of-the-art
    optimizations and generate code for neural network graphs. TVM provides graph-level
    and operator-level optimizations for DL models, whose optimized performance on
    some hardware is competitive with state-of-the-art hand-tuned libraries.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻手动优化DL模型在各种DL硬件（例如TPU）和硬件库（例如cuDNN）上的负担，研究人员开发了DL编译器（Li等，[2020](#bib.bib61)）。
    它以DL框架描述的抽象模型作为输入，然后自动优化生成运算符和代码作为输出，以确保DL硬件库能够高效执行DL模型的计算。 因此，DL编译器通常与构建抽象DL模型的DL框架密切相关并共同工作。
    目前流行的DL编译器包括Glow（Rotem等，[2018](#bib.bib94)），TVM（Chen等，[2018](#bib.bib14)）等。 Glow旨在实现最先进的优化并为神经网络图生成代码。
    TVM为DL模型提供了图级和运算符级的优化，在某些硬件上的优化性能与最先进的手动调优库相媲美。
- en: Similar to traditional compilers, DL compilers implement a layered design, which
    mainly consists of the compiler frontend and the compiler backend. The intermediate
    representation (IR), as the abstract of the program, exists in both the frontend
    and the backend (Li et al., [2020](#bib.bib61)). The front end converts the DL
    model from the DL framework into a computation graph and optimizes the graph with
    various methods (e.g., reducing redundancy). In this process, IR is mainly used
    to express DL models, construct the control flow and dependencies between operators
    and data, etc. For the computation graph, the backend performs hardware-specific
    optimizations by leveraging third-party tools and customizing compilation passes
    based on prior knowledge. Finally, DL compilers convert the DL model into operators
    and code which can be used to perform calculations on the DL hardware and hardware
    libraries.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于传统编译器，DL编译器实现了分层设计，主要包括编译器前端和编译器后端。中间表示（IR）作为程序的抽象，存在于前端和后端（Li等，[2020](#bib.bib61)）。前端将DL模型从DL框架转换为计算图，并通过各种方法（例如，减少冗余）优化该图。在这个过程中，IR主要用于表示DL模型，构建操作符和数据之间的控制流和依赖关系等。对于计算图，后端通过利用第三方工具和基于先验知识自定义编译过程来执行特定硬件的优化。最后，DL编译器将DL模型转换为操作符和代码，这些代码可以用于在DL硬件和硬件库上执行计算。
- en: 2.2.3\. DL Hardware Library
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3. DL硬件库
- en: Researchers have designed a variety of DL hardware, such as CPU, GPU, and TPU,
    to apply DL techniques in different scenarios. To adapt to given DL hardware and
    map the computation to DL hardware efficiently, researchers have developed a series
    of DL hardware libraries (e.g., cuDNN, cuBLAS) that implement optimized linear
    algebras, matrix multiplication, DL operators, etc. For example, cuDNN (Chetlur
    et al., [2014](#bib.bib17)) is a DL accelerate library developed by NVIDIA, which
    is used to achieve high-performance computing on its developed GPU (e.g., RTX3090).
    In addition, cuBLAS provides a basic linear algebra library for the computation
    on GPU. HiAI is a DL accelerator introduced by Huawei for the specialized neural
    processing unit (NPU) in mobile chips (e.g., the Kirin 970). It provides acceleration
    for 16-bit float models and 8-bit quantized models and supports typical DL frameworks
    such as TensorFlow and Caffe (Ignatov et al., [2018](#bib.bib43)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员设计了多种深度学习（DL）硬件，如CPU、GPU和TPU，以便在不同场景中应用DL技术。为了适应给定的DL硬件并将计算有效地映射到DL硬件上，研究人员开发了一系列DL硬件库（例如，cuDNN、cuBLAS），这些库实现了优化的线性代数、矩阵乘法、DL操作符等。例如，cuDNN（Chetlur等，
    [2014](#bib.bib17)）是NVIDIA开发的一个DL加速库，用于在其开发的GPU（例如RTX3090）上实现高性能计算。此外，cuBLAS为GPU上的计算提供了一个基础线性代数库。HiAI是华为推出的DL加速器，专为移动芯片中的专用神经处理单元（NPU）（例如，Kirin
    970）设计。它为16位浮点模型和8位量化模型提供加速，并支持典型的DL框架，如TensorFlow和Caffe（Ignatov等，[2018](#bib.bib43)）。
- en: These DL hardware-related libraries select appropriate algorithms for different
    hardware and deployment environments, thus they can realize specific calculations
    on DL hardware. They also call different operators according to the data type
    to speed up the operation of the model on the given data. For example, cuBLAS
    implements various matrix-matrix multiplication operators (e.g., cublasHgemm,
    cublasSgemm) to map the calculations of different data types to the hardware.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些与DL硬件相关的库为不同的硬件和部署环境选择适当的算法，从而可以在DL硬件上实现特定的计算。它们还根据数据类型调用不同的操作符，以加快模型在给定数据上的操作速度。例如，cuBLAS实现了各种矩阵-矩阵乘法操作符（例如，cublasHgemm，cublasSgemm），以将不同数据类型的计算映射到硬件上。
- en: 2.3\. DL Library Testing
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. DL库测试
- en: 2.3.1\. Definition
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1. 定义
- en: DL library testing aims to discover bugs, vulnerabilities, and defects in the
    DL libraries. The DL library bug is essentially a kind of software bug. Referring
    to the prior research (Zhang et al., [2020a](#bib.bib120); IEE, [2010](#bib.bib2)),
    we define the behavior that the actual function of the DL library does not meet
    the requirements and specifications as a DL library bug.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: DL库测试旨在发现DL库中的错误、漏洞和缺陷。DL库错误本质上是一种软件错误。参考先前的研究（Zhang等，[2020a](#bib.bib120)；IEE，[2010](#bib.bib2)），我们定义为DL库错误的是指DL库的实际功能不符合要求和规格的行为。
- en: Definition 2.1 (DL Library Bug).
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2.1（DL库错误）。
- en: A DL library bug refers to any imperfection or deficiency in a DL library that
    causes the actual function performed by the DL library to fail to meet the expected
    requirements or specifications.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: DL库的错误是指DL库中任何不完善或缺陷，导致DL库实际执行的功能未能满足预期的要求或规范。
- en: Based on the above definition of DL library bugs, we define DL library testing
    as follows.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述DL库错误的定义，我们将DL库测试定义如下。
- en: Definition 2.2 (DL Library Testing).
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义2.2（DL库测试）。
- en: DL library testing refers to any activity designed to discover and identify
    DL library bugs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: DL库测试是指任何旨在发现和识别DL库错误的活动。
- en: To elaborate on the definition of the DL library bug and testing in detail,
    in the following section, we explain the test objects of the DL library testing
    and the expected requirements in testing. We use testing components and testing
    properties to refer to the above two aspects. In addition, we also summarize and
    introduce some common DL library testing techniques.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为详细阐述DL库错误和测试的定义，在接下来的部分中，我们将解释DL库测试的测试对象及其预期要求。我们使用测试组件和测试属性来指代上述两个方面。此外，我们还总结和介绍了一些常见的DL库测试技术。
- en: 2.3.2\. DL Library Testing Component
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2\. DL库测试组件
- en: DL library testing component points out the application and test objects of
    various DL library testing methods. [Fig. 2](#S2.F2 "Fig. 2 ‣ 2.2\. DL Library
    ‣ 2\. Preliminary ‣ A Survey of Deep Learning Library Testing Methods") shows
    the underlying workflow of a DL system and DL program. In the computation, the
    DL model mainly relies on three DL underlying libraries, namely the DL framework,
    DL compiler, and DL hardware library. The code written by developers is converted
    and optimized by these DL libraries before it can perform calculations, training
    and inference on the DL hardware. Considering that we have introduced these libraries
    in [§ 2.2](#S2.SS2 "2.2\. DL Library ‣ 2\. Preliminary ‣ A Survey of Deep Learning
    Library Testing Methods"), we mainly describe the characteristics of the testing
    research on the three kinds of DL libraries in follows.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: DL库测试组件指出了各种DL库测试方法的应用和测试对象。[图2](#S2.F2 "Fig. 2 ‣ 2.2\. DL Library ‣ 2\. Preliminary
    ‣ A Survey of Deep Learning Library Testing Methods")展示了DL系统和DL程序的基础工作流程。在计算中，DL模型主要依赖三个DL基础库，即DL框架、DL编译器和DL硬件库。开发者编写的代码在进行计算、训练和推理之前，需要经过这些DL库的转换和优化。考虑到我们在[§
    2.2](#S2.SS2 "2.2\. DL Library ‣ 2\. Preliminary ‣ A Survey of Deep Learning Library
    Testing Methods")中介绍了这些库，我们主要描述了对这三种DL库的测试研究特征。
- en: DL framework testing aims to discover bugs in DL frameworks. The DL framework
    plays a vital role in the build and calculation of the DL model. Any imperfection
    or deficiency in the implementation of the DL framework can directly affect the
    DL model, which will further lead to erroneous results and even cause the program
    to crash. DL framework testing research has two characteristics. 1) DL framework
    testing research has the most testing methods and is the most well-developed field
    in DL library testing. Existing research (Islam et al., [2019](#bib.bib44); Yang
    et al., [2022](#bib.bib119); Chen et al., [2023a](#bib.bib10)) has investigated
    the characteristics and symptoms of bugs in DL frameworks (e.g., PyTorch and TensorFlow)
    through open-source communities (e.g., GitHub and Stack Overflow), and has gained
    a deep understanding of the root cause of bugs. On this basis, researchers proposed
    a variety of testing methods and tools based on differential testing, fuzz testing,
    etc. to discover and identify DL framework bugs (Pham et al., [2019](#bib.bib89);
    Deng et al., [2022](#bib.bib23)). These methods provide a reference and guide
    for testing other DL libraries. 2) DL framework bugs detected by existing methods
    cover most of the bug types, including status bugs (e.g., crash, segmentation
    fault), numerical bugs (e.g., inconsistent output, NaN value), and performance
    bugs (e.g., unexpected time overhead). Since the emergence of the first batch
    of DL framework testing research (e.g., CRADLE (Pham et al., [2019](#bib.bib89))),
    researchers have continuously improved the testing methods. Today, a DL framework
    tool can usually discover dozens and even hundreds of bugs, which range from simple
    inconsistencies and crashes to complex performance bugs and security vulnerabilities,
    which effectively promotes the development of DL framework security. In contrast,
    current tests on other DL libraries are still in the early stages of development
    and are different in detecting a large number and variety of bugs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: DL 框架测试旨在发现 DL 框架中的漏洞。DL 框架在 DL 模型的构建和计算中发挥着至关重要的作用。DL 框架实现中的任何瑕疵或不足都可能直接影响
    DL 模型，进而导致错误结果，甚至使程序崩溃。DL 框架测试研究具有两个特点。1) DL 框架测试研究拥有最多的测试方法，并且是 DL 库测试中最成熟的领域。现有研究（Islam
    等，[2019](#bib.bib44)；Yang 等，[2022](#bib.bib119)；Chen 等，[2023a](#bib.bib10)）通过开源社区（如
    GitHub 和 Stack Overflow）调查了 DL 框架（例如 PyTorch 和 TensorFlow）中漏洞的特征和症状，并深入了解了漏洞的根本原因。在此基础上，研究人员提出了多种基于差异测试、模糊测试等的测试方法和工具，以发现和识别
    DL 框架漏洞（Pham 等，[2019](#bib.bib89)；Deng 等，[2022](#bib.bib23)）。这些方法为测试其他 DL 库提供了参考和指导。2)
    现有方法检测出的 DL 框架漏洞涵盖了大多数漏洞类型，包括状态漏洞（例如崩溃、段错误）、数值漏洞（例如输出不一致、NaN 值）和性能漏洞（例如意外的时间开销）。自从第一批
    DL 框架测试研究（例如 CRADLE（Pham 等，[2019](#bib.bib89)））出现以来，研究人员不断改进测试方法。如今，一个 DL 框架工具通常能够发现几十个甚至上百个漏洞，这些漏洞范围从简单的不一致性和崩溃到复杂的性能漏洞和安全漏洞，有效促进了
    DL 框架安全的发展。相比之下，目前对其他 DL 库的测试仍处于初期阶段，检测到的大量和多样化漏洞存在差异。
- en: DL compiler testing aims to find DL compiler bugs that cause the DL compiler
    to generate incorrect code, resulting in unexpected model behaviors (Shen et al.,
    [2021](#bib.bib97)). Similar to the DL framework testing, the DL compiler testing
    detects and identifies those anomaly functions and behaviors from the compiler
    through various methods, such as fuzz testing and metamorphic testing (Liu et al.,
    [2022b](#bib.bib71); Xiao et al., [2022](#bib.bib113)), but it has two unique
    features. 1) DL compiler testing research focuses on three bug prone stages, namely
    model loading, high-level IR transformation, and low-level LR transformation stages,
    due to the special architecture of DL compilers. The former loads an abstract
    DL model and transforms it into a computation graph, and the latter two respectively
    implement optimizations on high-level and low-level IRs. Since specific conversion
    and transformation are involved, bugs are more likely to occur in the latter two
    stages (Shen et al., [2021](#bib.bib97)), therefore, the DL compiler testing methods
    often pay more attention to these two stages. However, DL framework testing methods
    mainly focus on
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习编译器测试旨在找到导致深度学习编译器生成错误代码的错误，从而导致意外的模型行为 (Shen et al., [2021](#bib.bib97))。与深度学习框架测试类似，深度学习编译器测试通过各种方法（如模糊测试和变形测试
    (Liu et al., [2022b](#bib.bib71); Xiao et al., [2022](#bib.bib113))）检测和识别编译器中的异常函数和行为，但具有两个独特的特点。1)
    由于深度学习编译器的特殊架构，深度学习编译器测试研究集中在三个容易出错的阶段，即模型加载、高级 IR 转换和低级 IR 转换阶段。前者加载一个抽象的深度学习模型并将其转换为计算图，而后两者分别对高级和低级
    IR 进行优化。由于涉及特定的转换和变换，后两阶段更容易发生错误 (Shen et al., [2021](#bib.bib97))，因此，深度学习编译器测试方法往往更加关注这两个阶段。然而，深度学习框架测试方法主要关注
- en: '![Refer to caption](img/a8cee3b3043269fb2e5f7481e271b594.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a8cee3b3043269fb2e5f7481e271b594.png)'
- en: Fig. 3\. An Example Optimization Bug (Xiao et al., [2022](#bib.bib113)) on the
    Glow Compiler.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 一个优化错误示例 (Xiao et al., [2022](#bib.bib113)) 在 Glow 编译器上。
- en: forward and backward propagation stages in the model training and inference
    process. 2) DL compiler testing discovers different types of bugs compared with
    DL framework testing. Existing DL compiler testing methods can detect optimization
    bugs, which can cause the DL compiler to output incorrect results or middle results (Shen
    et al., [2021](#bib.bib97)), resulting in semantic changes and inequality after
    the compilation process and ultimately incorrect calculation results.  [Fig. 3](#S2.F3
    "Fig. 3 ‣ 2.3.2\. DL Library Testing Component ‣ 2.3\. DL Library Testing ‣ 2\.
    Preliminary ‣ A Survey of Deep Learning Library Testing Methods") shows an optimization
    bug in the Glow compiler, which incorrectly deletes layers (marked in the grey
    box) during the Dead Code Elimination(DCE) optimization, resulting in optimized
    operators outputting unexpected results. However, existing DL framework testing
    rarely involves optimization bugs and mainly focuses on status, numerical, and
    performance bugs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练和推断过程中的前向和后向传播阶段。2) 深度学习编译器测试发现的错误类型与深度学习框架测试不同。现有的深度学习编译器测试方法可以检测到优化错误，这些错误可能导致深度学习编译器输出不正确的结果或中间结果
    (Shen et al., [2021](#bib.bib97))，从而导致编译过程后的语义变化和不等式，最终导致计算结果错误。[图 3](#S2.F3 "图
    3 ‣ 2.3.2\. 深度学习库测试组件 ‣ 2.3\. 深度学习库测试 ‣ 2\. 初步 ‣ 深度学习库测试方法综述") 显示了 Glow 编译器中的一个优化错误，该错误在死代码消除
    (DCE) 优化过程中错误地删除了层（标记在灰色框中），导致优化后的操作符输出意外结果。然而，现有的深度学习框架测试很少涉及优化错误，主要关注状态、数值和性能错误。
- en: DL hardware library testing discovers bugs on the DL hardware-related libraries.
    DL hardware library bugs can cause the DL model to obtain wrong results and abnormal
    runtime overhead in the calculation, which is difficult to perceive when executing
    the DL program or training a DL model. Existing testing methods mainly focus on
    functional errors in the DL hardware library. They use metamorphic testing or
    test pattern generation methods to verify the correctness of DL hardware library
    implementation (Wang et al., [2020a](#bib.bib106); Uezono et al., [2022](#bib.bib105)).
    Since the DL hardware library is at the bottom of the entire DL program workflow,
    as shown in [Fig. 2](#S2.F2 "Fig. 2 ‣ 2.2\. DL Library ‣ 2\. Preliminary ‣ A Survey
    of Deep Learning Library Testing Methods"), it is difficult to generate valid
    test input on a large scale or construct test oracles during testing. Therefore,
    existing research on DL hardware libraries generally focuses on the study validation
    of the functionality of different libraries, while little work is done to detect
    real-world bugs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: DL硬件库测试发现DL硬件相关库中的漏洞。DL硬件库中的漏洞可能导致DL模型获取错误的结果和计算中的异常运行开销，这在执行DL程序或训练DL模型时很难察觉。现有的测试方法主要集中在DL硬件库中的功能错误上。它们使用变形测试或测试模式生成方法来验证DL硬件库实现的正确性（Wang
    et al., [2020a](#bib.bib106); Uezono et al., [2022](#bib.bib105)）。由于DL硬件库位于整个DL程序工作流程的底层，如[图
    2](#S2.F2 "Fig. 2 ‣ 2.2\. DL Library ‣ 2\. Preliminary ‣ A Survey of Deep Learning
    Library Testing Methods")所示，因此很难在大规模上生成有效的测试输入或在测试过程中构建测试预言。因此，现有的DL硬件库研究通常集中于不同库功能的验证研究，而很少有工作致力于检测实际世界中的漏洞。
- en: 2.3.3\. DL Library Testing Property
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3\. DL库测试属性
- en: The testing property refers to what the DL library testing methods test. It
    defines where the implementation of the DL library should meet the requirements
    and expectations. Existing studies (Zhang et al., [2020a](#bib.bib120), [2022](#bib.bib123))
    on machine learning (ML) testing comprehensively study and summarize various ML
    test properties, including correctness, robustness, privacy, efficiency, fairness,
    etc. However, the existing DL library testing research mainly focuses on the basic
    functionality of the DL libraries, which pay little attention to security-related
    properties, like robustness and privacy. Therefore, we mainly discuss correctness
    and efficiency in this paper.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 测试属性指的是DL库测试方法测试的内容。它定义了DL库的实现应满足的要求和期望。现有的研究（Zhang et al., [2020a](#bib.bib120),
    [2022](#bib.bib123)）在机器学习（ML）测试方面对各种ML测试属性进行了全面研究和总结，包括正确性、鲁棒性、隐私性、效率、公平性等。然而，现有的DL库测试研究主要关注DL库的基本功能，而很少关注与安全相关的属性，如鲁棒性和隐私性。因此，本文主要讨论正确性和效率。
- en: Correctness measures the ability of a DL library to correctly perform its functions
    and complete a given task. Correctness plays a vital role in the application and
    deployment of DL systems, which ensures the usability and trustworthiness of DL
    libraries. When the correctness of the DL library is compromised, the intended
    function cannot be executed, which may cause three types of bugs, namely status
    bug, numerical bug and optimization bug (Chen et al., [2023a](#bib.bib10); Shen
    et al., [2021](#bib.bib97)), which could be further exploited to endanger the
    safety and security of the whole DL system. The status bug refers to the unexpected
    termination of valid inputs or illegal execution of invalid inputs on the DL library.
    It includes various crashes, segmentation faults, exceptions, etc. The numerical
    bug and optimization bug occur when the DL library has incorrect behavior on valid
    inputs but does not crash. At this time, the DL library will output wrong results
    and further affect subsequent calculations. The former mainly consists of inconsistent
    outputs (i.e., inconsistency between expected and actual result) and NaN outputs
    (i.e., Not A Number, which is caused by overflow in the backend). The latter happens
    when the DL library (especially the DL compiler) gets erroneous results or middle
    results in the optimization process, resulting in inequalities between before
    and after optimization. Existing testing methods mainly test and validate the
    correctness of DL frameworks and compilers (Pham et al., [2019](#bib.bib89); Deng
    et al., [2022](#bib.bib23); Xiao et al., [2022](#bib.bib113)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正确性衡量了深度学习库准确执行其功能和完成给定任务的能力。正确性在深度学习系统的应用和部署中扮演着至关重要的角色，它确保了深度学习库的可用性和可信度。当深度学习库的正确性受到威胁时，预期功能无法执行，这可能导致三种类型的错误，即状态错误、数值错误和优化错误（Chen
    et al., [2023a](#bib.bib10); Shen et al., [2021](#bib.bib97)），这些错误可能进一步被利用，危害整个深度学习系统的安全性和可靠性。状态错误指的是深度学习库在有效输入时意外终止或在无效输入时非法执行。它包括各种崩溃、段错误、异常等。数值错误和优化错误发生在深度学习库在有效输入时表现不正确但没有崩溃的情况。此时，深度学习库将输出错误的结果，并进一步影响后续计算。前者主要包括输出不一致（即预期结果与实际结果不一致）和NaN输出（即不是一个数字，由于后台溢出导致）。后者发生在深度学习库（尤其是深度学习编译器）在优化过程中得到错误的结果或中间结果，导致优化前后存在差异。现有的测试方法主要测试和验证深度学习框架和编译器的正确性（Pham
    et al., [2019](#bib.bib89); Deng et al., [2022](#bib.bib23); Xiao et al., [2022](#bib.bib113)）。
- en: Efficiency evaluates the overhead of time, GPU memory, and other performance
    indicators of the DL library in executing a given task. It determines the cost
    of large-scale deployment of DL libraries, which is of great significance for
    DL systems from the perspectives of performance, economics, and the environment.
    The problem in DL library efficiency leads to the performance bug. This kind of
    bug not only severely compromises the usability of DL libraries, leading to less
    responsiveness and waste of computational resources, but also puts pressure on
    both execution costs and the environment and results in a high carbon footprint (Nistor
    et al., [2015](#bib.bib84); Jin et al., [2012](#bib.bib47); Patterson et al.,
    [2021](#bib.bib88)). However, due to the limitations of testing methods and test
    oracles, existing work has paid limited attention to such bugs (Guo et al., [2019](#bib.bib35);
    Levental and Orlova, [2020](#bib.bib57)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 效率评估了深度学习库在执行给定任务时的时间、GPU内存及其他性能指标的开销。它决定了深度学习库大规模部署的成本，这对深度学习系统在性能、经济和环境方面具有重要意义。深度学习库效率中的问题会导致性能错误。这种错误不仅严重影响深度学习库的可用性，导致响应时间变慢和计算资源浪费，还对执行成本和环境造成压力，并导致高碳足迹（Nistor
    et al., [2015](#bib.bib84); Jin et al., [2012](#bib.bib47); Patterson et al.,
    [2021](#bib.bib88)）。然而，由于测试方法和测试预言的限制，现有工作对这种错误的关注有限（Guo et al., [2019](#bib.bib35);
    Levental and Orlova, [2020](#bib.bib57)）。
- en: 2.3.4\. DL Library Testing Technique
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4. 深度学习库测试技术
- en: The testing technique determines how the DL library testing methods test. To
    effectively discover and identify bugs in DL libraries, how to generate test cases/inputs
    at scale and build test oracles are vital problems to be solved. Nowadays, researchers
    have referenced traditional software testing techniques to design a variety of
    DL library testing methods. Here we introduce the three most widely spread DL
    library testing techniques, namely differential testing, fuzz testing, and metamorphic
    testing.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 测试技术决定了深度学习（DL）库测试方法的测试方式。为了有效地发现和识别深度学习库中的错误，如何大规模生成测试用例/输入以及构建测试或acles是亟待解决的重要问题。如今，研究人员参考了传统的软件测试技术来设计各种深度学习库测试方法。这里我们介绍三种最广泛传播的深度学习库测试技术，即差异测试、模糊测试和变形测试。
- en: Differential testing is one of the most classic testing methods in the field
    of software engineering (SE), which usually processes the same input on two or
    more comparable implementations of a given software, uses the outputs between
    each other as the pseudo test oracle, and utilizes the difference between outputs
    to reveal potential bugs (McKeeman, [1998](#bib.bib78)). As a simple but effective
    pseudo test oracle, differential testing has not only achieved excellent results
    in traditional software testing and verification tasks (Groce et al., [2007](#bib.bib32);
    Wang et al., [2021](#bib.bib108)) but also shined in DL library testing (Pham
    et al., [2019](#bib.bib89); Deng et al., [2022](#bib.bib23)). Different implementations
    of the same operator in different DL libraries or on different devices greatly
    facilitate the application of differential testing in DL library testing. However,
    the differential testing technique has two limitations. Firstly, its testing effect
    is directly related to the design of test oracles. Simple test oracles (e.g.,
    merely comparing the behaviors of one operator between different devices) are
    difficult to detect complex performance bugs, which could limit the effectiveness
    of testing methods. Second, differential testing may introduce false positives
    (FPs) in tests. Researchers have found that DL libraries implement certain operations
    in different ways, which leads to large differences in results, but this is not
    a real bug (Guo et al., [2020](#bib.bib36)). How to design advanced test oracles
    and reduce the FP rate are challenges in DL library differential testing.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 差异测试是软件工程（SE）领域中最经典的测试方法之一，它通常对给定软件的两个或多个可比实现处理相同的输入，使用各自之间的输出作为伪测试或acles，并利用输出之间的差异来揭示潜在的错误（McKeeman，[1998](#bib.bib78)）。作为一种简单但有效的伪测试或acles，差异测试不仅在传统软件测试和验证任务中取得了优异的成果（Groce
    et al., [2007](#bib.bib32)；Wang et al., [2021](#bib.bib108)），而且在深度学习库测试中也表现出色（Pham
    et al., [2019](#bib.bib89)；Deng et al., [2022](#bib.bib23)）。同一操作符在不同深度学习库或不同设备上的不同实现极大地促进了差异测试在深度学习库测试中的应用。然而，差异测试技术有两个局限性。首先，它的测试效果直接与测试或acles的设计有关。简单的测试或acles（例如，仅仅比较不同设备之间一个操作符的行为）难以检测复杂的性能错误，这可能限制测试方法的有效性。其次，差异测试可能会在测试中引入假阳性（FPs）。研究人员发现，深度学习库以不同的方式实现某些操作，这导致结果差异很大，但这并不是一个真正的错误（Guo
    et al., [2020](#bib.bib36)）。如何设计高级测试或acles并降低FP率是深度学习库差异测试中的挑战。
- en: Fuzz testing is a classic software testing technique, which is widely used to
    automatically detect bugs such as crashes in various software and systems (Liang
    et al., [2018](#bib.bib65); Manès et al., [2019](#bib.bib76)). Fuzz testing typically
    generates a large number of test inputs and observes whether the target software
    or system fails when executing the test input (Liu et al., [2012](#bib.bib67)).
    Therefore, it is usually used to generate valid/invalid test cases in DL library
    testing. According to the test input generation methods, fuzzing can mainly be
    divided into generation-based fuzzing and mutation-based fuzzing (Oehlert, [2005](#bib.bib85)).
    The former generates test cases and inputs from scratch based on constraints or
    randomly, while the latter mainly mutates existing inputs to test more potential
    behaviors of DL libraries. Since fuzz testing can generate a large number of inputs
    and achieve high API or code coverage in tests, testing methods based on it can
    often achieve outstanding results in real-world bug detection (Xie et al., [2022](#bib.bib114);
    Deng et al., [2023a](#bib.bib21)). Nowadays, fuzz testing has been widely used
    in DL framework and compiler testing. However, fuzz testing cannot build test
    oracles by itself. It often works with other testing techniques that can build
    pseudo test oracles (e.g., differential testing) (Wei et al., [2022](#bib.bib110)),
    or directly observes whether the test case has unexpected behavior (e.g., crash) (Xie
    et al., [2022](#bib.bib114)) to identify bugs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊测试是一种经典的软件测试技术，广泛用于自动检测各种软件和系统中的崩溃等漏洞 (Liang et al., [2018](#bib.bib65); Manès
    et al., [2019](#bib.bib76))。模糊测试通常生成大量测试输入，并观察目标软件或系统在执行测试输入时是否出现故障 (Liu et al.,
    [2012](#bib.bib67))。因此，它通常用于生成 DL 库测试中的有效/无效测试用例。根据测试输入生成方法，模糊测试主要分为生成基础模糊测试和变异基础模糊测试
    (Oehlert, [2005](#bib.bib85))。前者基于约束或随机生成测试用例和输入，而后者主要变异现有输入以测试 DL 库的更多潜在行为。由于模糊测试可以生成大量输入并在测试中实现高
    API 或代码覆盖率，基于它的测试方法通常能在真实世界漏洞检测中取得出色成果 (Xie et al., [2022](#bib.bib114); Deng
    et al., [2023a](#bib.bib21))。如今，模糊测试已广泛用于 DL 框架和编译器测试。然而，模糊测试本身不能建立测试 oracle。它通常与可以建立伪测试
    oracle 的其他测试技术 (例如，差异测试) (Wei et al., [2022](#bib.bib110)) 或直接观察测试用例是否有意外行为 (例如，崩溃)
    (Xie et al., [2022](#bib.bib114)) 一起工作以识别漏洞。
- en: Metamorphic testing was proposed by Chen et al. (Chen et al., [1998](#bib.bib15))
    in 1998 to address the test oracle problem. Metamorphic testing constructs a series
    of metamorphic relations (MRs) that are from the necessary properties of the program
    under test. An MR describes the expected change in the outputs of a target program
    when the inputs are changed. Metamorphic testing can generate a series of test
    samples and judge whether the functionality of the program is as expected by comparing
    whether their results conform to the metamorphic relationship (Segura et al.,
    [2016](#bib.bib96)). Researchers have designed some DL library testing methods
    based on the metamorphic testing technique to verify the computation and optimization
    of DL libraries, and successfully detected real-world bugs in the DL framework
    and compiler (Wei et al., [2022](#bib.bib110); Xiao et al., [2022](#bib.bib113)).
    However, metamorphic testing is not a silver bullet for DL library testing. Since
    MR determines which part of the functionality of the target library will be verified
    in the test, the effect of metamorphic testing is greatly limited by the design
    of MRs. A well-designed MR can improve the possibility of finding real-world bugs
    in DL library testing and help discover complex optimization bugs e.g., memory
    allocation bugs (Xiao et al., [2022](#bib.bib113)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 变异测试由 Chen 等人 (Chen et al., [1998](#bib.bib15)) 在 1998 年提出，旨在解决测试 oracle 问题。变异测试构建了一系列来自被测程序必要属性的变异关系
    (MRs)。一个 MR 描述了当输入发生变化时，目标程序输出的预期变化。变异测试可以生成一系列测试样本，并通过比较它们的结果是否符合变异关系来判断程序的功能是否如预期。研究人员基于变异测试技术设计了一些
    DL 库测试方法，以验证 DL 库的计算和优化，并成功检测到了 DL 框架和编译器中的真实世界漏洞 (Wei et al., [2022](#bib.bib110);
    Xiao et al., [2022](#bib.bib113))。然而，变异测试并不是 DL 库测试的终极解决方案。由于 MR 决定了测试中目标库功能的哪个部分会被验证，变异测试的效果受到
    MR 设计的极大限制。一个设计良好的 MR 可以提高发现 DL 库测试中真实世界漏洞的可能性，并帮助发现复杂的优化漏洞，例如内存分配漏洞 (Xiao et
    al., [2022](#bib.bib113))。
- en: 2.4\. DL Library Testing vs. DL Model Testing
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. DL 库测试与 DL 模型测试
- en: Table 1\. DL Library Testing vs. DL Model Testing
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. DL 库测试与 DL 模型测试
- en: '|  | Test Input | Test Object | Test Property | Test Oracle |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | 测试输入 | 测试对象 | 测试属性 | 测试方法 |'
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DL Library &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DL 库 &#124;'
- en: '&#124; Testing &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 测试 &#124;'
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Seceral lines of code to &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 几行代码用于 &#124;'
- en: '&#124; call DL libray APIs &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 调用 DL 库 API &#124;'
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Program bugs in &#124;'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 程序错误在 &#124;'
- en: '&#124; the DL library &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DL 库 &#124;'
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Mainly correctness and efficiency &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 主要是正确性和效率 &#124;'
- en: '&#124; of the DL library &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DL 库的 &#124;'
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Difficult to construct test &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 难以构造测试 &#124;'
- en: '&#124; oracles in bug detection &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在错误检测中的预言 &#124;'
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DL Model &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DL 模型 &#124;'
- en: '&#124; Testing &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 测试 &#124;'
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Image, text, or &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像、文本或 &#124;'
- en: '&#124; matrix data &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 矩阵数据 &#124;'
- en: '|'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Secure problems on &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 安全问题在 &#124;'
- en: '&#124; DL model properties &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DL 模型属性 &#124;'
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Correctness, robustness, interpretability, &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确性、鲁棒性、可解释性，&#124;'
- en: '&#124; efficiency, etc. of the DL model &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DL 模型的效率等 &#124;'
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Metrics existing for testing &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 现有的测试指标 &#124;'
- en: '&#124; and detecting problems &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以及检测问题 &#124;'
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Prior work has organized and summarized the DL model testing methods (Zhang
    et al., [2022](#bib.bib123)). However, researchers still pay limited attention
    to testing on the DL underlying library. In fact, DL library testing and DL model
    testing have similarities. For example, they both rely on the behaviors and results
    of DL models to detect potential problems. However, there are still many differences
    between DL library testing and DL model testing. We discuss the difference between
    the two tests from the following perspectives.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的工作已经整理和总结了 DL 模型测试的方法（Zhang et al., [2022](#bib.bib123)）。然而，研究人员对 DL 底层库的测试关注仍然有限。实际上，DL
    库测试和 DL 模型测试有相似之处。例如，它们都依赖于 DL 模型的行为和结果来检测潜在问题。然而，DL 库测试和 DL 模型测试之间仍然存在许多差异。我们从以下几个方面讨论这两种测试的区别。
- en: 1) Test Input. In DL model testing, the test input is usually the input data
    of a complete model under tests. The input data can be meaningful images or text,
    or randomly generated matrix data. The testing methods generate and mutate a series
    of input data to test different properties and discover vulnerabilities in the
    given model. However, in DL library testing, the test input is often a program.
    It can be several lines of code that construct a DL model in the DL framework
    or a concatenation of some simple library APIs, which perform simple mathematical
    operations (e.g., addition, division).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 测试输入。在 DL 模型测试中，测试输入通常是待测模型的完整输入数据。输入数据可以是有意义的图像或文本，或随机生成的矩阵数据。测试方法生成并变异一系列输入数据以测试不同的属性，并发现给定模型中的漏洞。然而，在
    DL 库测试中，测试输入通常是一个程序。它可以是几行构建 DL 模型的代码，或一些简单库 API 的连接，这些 API 执行简单的数学操作（例如，加法、除法）。
- en: 2) Test Object. DL library testing and DL model testing have different test
    objects. The DL library testing aims to discover the implementation loopholes
    in the DL library, and its test objects are DL frameworks, DL compilers, and other
    third-party libraries that provide underlying support for DL models and systems.
    Therefore, in the repair process, developers usually fix bugs in the DL software (Deng
    et al., [2022](#bib.bib23); Pham et al., [2019](#bib.bib89)). In contrast, DL
    model testing aims to find potential correctness, fairness and other problems
    in the model, and its test objects are various DL models. Therefore, for the problems
    exposed in DL model testing, developers usually repair and improve the model by
    retraining and adjusting the model structure (Zhang et al., [2021d](#bib.bib126);
    Udeshi et al., [2018](#bib.bib104)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 测试对象。DL 库测试和 DL 模型测试的测试对象不同。DL 库测试的目的是发现 DL 库中的实现漏洞，其测试对象是 DL 框架、DL 编译器以及其他为
    DL 模型和系统提供基础支持的第三方库。因此，在修复过程中，开发人员通常修复 DL 软件中的漏洞（Deng et al., [2022](#bib.bib23);
    Pham et al., [2019](#bib.bib89)）。相比之下，DL 模型测试旨在发现模型中的潜在正确性、公平性及其他问题，其测试对象是各种 DL
    模型。因此，对于 DL 模型测试中暴露出的问题，开发人员通常通过重新训练和调整模型结构来修复和改进模型（Zhang et al., [2021d](#bib.bib126);
    Udeshi et al., [2018](#bib.bib104)）。
- en: 3) Test Property. As mentioned on [§ 2.3.3](#S2.SS3.SSS3 "2.3.3\. DL Library
    Testing Property ‣ 2.3\. DL Library Testing ‣ 2\. Preliminary ‣ A Survey of Deep
    Learning Library Testing Methods"), the research on DL model testing not merely
    focuses on the correctness of the implementation of model functionality (i.e.,
    correctness), but also focuses on the ability of the model to perform correctly
    under invalid input (i.e., robustness), the ability of the model not to be affected
    by sensitive and irrelevant input attributes (i.e., fairness), and the ability
    of the model to preserve private information (i.e., privacy), etc (Zhang et al.,
    [2020a](#bib.bib120), [2022](#bib.bib123)). However, the existing DL library testing
    mainly focuses on the correctness and efficiency of the library implementation,
    which determines whether the DL library can be widely and reliable deployed and
    applied.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 测试属性。如在[§ 2.3.3](#S2.SS3.SSS3 "2.3.3\. DL 库测试属性 ‣ 2.3\. DL 库测试 ‣ 2\. 初步 ‣
    深度学习库测试方法综述")中提到的，DL 模型测试的研究不仅关注模型功能实现的正确性（即正确性），还关注模型在无效输入下正确执行的能力（即鲁棒性）、模型不受敏感和无关输入属性影响的能力（即公平性）、以及模型保持私人信息的能力（即隐私）等（Zhang
    等，[2020a](#bib.bib120)，[2022](#bib.bib123)）。然而，现有的 DL 库测试主要关注库实现的正确性和效率，这决定了 DL
    库是否可以广泛而可靠地部署和应用。
- en: 4) Test Oracle. In DL model testing, for test properties such as correctness,
    robustness, and fairness, the accuracy or other metrics of the model on the given
    input can be used to judge whether there is a problem with the model. For example,
    if the model is susceptible to subtle adversarial perturbations in the inference
    process, and it has low test accuracy on the adversarial samples, then the model
    can be considered to have an adversarial robustness problem. However, it is difficult
    to obtain the test oracle and identify potential correctness and efficiency problems
    and bugs in DL library testing. On the one hand, due to stochastic operations
    such as initialization and gradient descent, it is difficult for developers to
    determine the expected output of a DL program. On the other hand, various hardware
    architectures and underlying optimization methods make it difficult for developers
    to quantify and estimate the time and memory overhead when performing specified
    operators in DL libraries. Therefore, there are still challenges in the generation
    and design of test oracles for DL library testing. Existing research usually constructs
    pseudo oracles based on the concept of differential testing, which detects bugs
    by comparing the performance and outputs of the same API under different implementations (Pham
    et al., [2019](#bib.bib89); Deng et al., [2022](#bib.bib23)).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 测试预言。 在 DL 模型测试中，对于正确性、鲁棒性和公平性等测试属性，可以通过模型在给定输入上的准确性或其他指标来判断模型是否存在问题。例如，如果模型在推断过程中容易受到细微对抗性扰动的影响，并且在对抗样本上的测试准确性较低，那么可以认为模型存在对抗鲁棒性问题。然而，获取测试预言并识别
    DL 库测试中的潜在正确性和效率问题以及漏洞是困难的。一方面，由于初始化和梯度下降等随机操作，开发人员难以确定 DL 程序的预期输出。另一方面，各种硬件架构和底层优化方法使得开发人员在执行
    DL 库中指定操作时难以量化和估算时间和内存开销。因此，在 DL 库测试的测试预言生成和设计方面仍然存在挑战。现有研究通常基于差异化测试的概念构建伪预言，通过比较同一
    API 在不同实现下的性能和输出，来检测漏洞（Pham 等，[2019](#bib.bib89)；Deng 等，[2022](#bib.bib23)）。
- en: 3\. DL Framework Testing
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. DL 框架测试
- en: DL framework testing aims to design methods and generate test samples for DL
    frameworks(e.g., TensorFlow (Abadi, [2016](#bib.bib4)) and PyTorch (Paszke et al.,
    [2019](#bib.bib87))) and detect bugs such as crashes, overflows, inconsistent
    outputs, and unexcepted overheads. Excluding the empirical study that analyzes
    and understands the DL framework bugs, existing DL framework testing methods can
    be divided into three categories based on the techniques they use, differential
    testing, fuzz testing, and metamorphic testing. We summarize several state-of-the-art
    methods in [Table 2](#S3.T2 "Table 2 ‣ 3\. DL Framework Testing ‣ A Survey of
    Deep Learning Library Testing Methods"), whose columns show the methods category,
    a brief description of each work, the test object, the number of bugs each work
    reported, and the type of detected bugs. The following sections will describe
    existing testing methods in detail, and summarize the strengths and weaknesses
    of each category of methods.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Representative DL Framework Testing Methods
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method Description | Test Object | # Bugs | Bug Type |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| Empirical Study |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '&#124; Summarized program bug reports and analyzed &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the challenges of testing and localizing DL bugs (Zhang et al., [2018](#bib.bib127))
    &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '| TensorFlow | / | / |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Conducted large-scale study on DL frameworks &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and summarized bug symptoms and root causes (Chen et al., [2023a](#bib.bib10))
    &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/DL4J/ &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch/MXNet &#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '| / | / |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| Differential Testing |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '&#124; Detected DL framework bugs via inconsistencies &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; between DL frameworks outputs (Pham et al., [2019](#bib.bib89)) &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/ &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNTK/Theano &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '| 12 | status/numerical |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mutated DL models to explore DL framework behaviors &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and precisely localized the buggy layer in models (Guo et al., [2020](#bib.bib36))
    &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/CNTK/ &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Theano/PyTorch &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '| 26 | status/numerical |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Extracted DL equivalence rules from documentation and open &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; source and constructed equivalent graphs to test (Wang et al., [2022](#bib.bib107))
    &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/ &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '| 25 | status/numerical |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Leveraged LLMs to generate test code and identified &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; bugs from the different results on different devices (Deng et al., [2023a](#bib.bib21))
    &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '| 65 | status/numerical |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| Fuzz Testing |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '&#124; Generate test cases for DL framework APIs and fuzz &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DL framework based on open-sourced data (Wei et al., [2022](#bib.bib110))
    &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/ &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '| 49 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '&#124; status/numerical/ &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; performance &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Extractd constraints from documentations to guide &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; test case generation and fuzz DL framework (Xie et al., [2022](#bib.bib114))
    &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/ &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PyTorch &#124;'
- en: '| 94 | status |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 94 | 状态 |'
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Utilized the behavior of similar APIs as test &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 利用类似 API 的行为作为测试 &#124;'
- en: '&#124; oracles and fuzz DL framework APIs (Deng et al., [2022](#bib.bib23))
    &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预言器和模糊深度学习框架 API（Deng 等，[2022](#bib.bib23)) &#124;'
- en: '|'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TensorFlow/ &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TensorFlow/ &#124;'
- en: '&#124; PyTorch &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PyTorch &#124;'
- en: '| 162 | status/numerical |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 162 | 状态/数值 |'
- en: '|'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Designed AI semantics to compute and construct test &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 设计 AI 语义来计算和构建测试 &#124;'
- en: '&#124; oracle and generate valid test cases in tests (Schumi and Sun, [2022](#bib.bib95))
    &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预言器并在测试中生成有效的测试用例（Schumi 和 Sun，[2022](#bib.bib95)）&#124;'
- en: '|'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TensorFlow &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TensorFlow &#124;'
- en: '| 14 | status |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 状态 |'
- en: '|'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Mutated and explored the search space of models and &#124;'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 变异和探索模型的搜索空间 &#124;'
- en: '&#124; conducted a coverage-guided testing for DL frameworks (Li et al., [2023b](#bib.bib60))
    &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 进行覆盖引导测试以评估深度学习框架（Li 等，[2023b](#bib.bib60)) &#124;'
- en: '|'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TensorFlow/MXNet &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TensorFlow/MXNet &#124;'
- en: '&#124; PyTorch &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PyTorch &#124;'
- en: '| 32 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 32 |'
- en: '&#124; status/numerical/ &#124;'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 状态/数值/ &#124;'
- en: '&#124; performance &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 性能 &#124;'
- en: '|'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Metamorphic &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 变换性 &#124;'
- en: '&#124; Testing &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 测试 &#124;'
- en: '|'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Designed 11 metamorphic relations to verify the &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 设计了 11 种变换性关系以验证 &#124;'
- en: '&#124; correctness of DL framework functionality (Ding et al., [2017](#bib.bib24))
    &#124;'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度学习框架功能的正确性（Ding 等，[2017](#bib.bib24)）&#124;'
- en: '| Caffe | / | status/numerical |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Caffe | / | 状态/数值 |'
- en: '|'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Other &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 其他 &#124;'
- en: '|'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Generated high-level code based on the low-level &#124;'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于低级代码生成高级代码 &#124;'
- en: '&#124; offending inputs that trigger errors in DL frameworks (Christou et al.,
    [2023](#bib.bib18)) &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 触发深度学习框架错误的有问题输入（Christou 等，[2023](#bib.bib18)) &#124;'
- en: '|'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TensorFlow/ &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TensorFlow/ &#124;'
- en: '&#124; PyTorch &#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PyTorch &#124;'
- en: '| 61 | status/numerical |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 61 | 状态/数值 |'
- en: 3.1\. Empirical Study on DL Framework
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 深度学习框架的实证研究
- en: Researchers have conducted some empirical studies on DL framework testing to
    understand the characteristics of DL framework bugs and clarify the feasible research
    directions.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员已经进行了一些关于深度学习框架测试的实证研究，以了解深度学习框架漏洞的特征，并明确可行的研究方向。
- en: Zhang et al. (Zhang et al., [2018](#bib.bib127)) are one of the first research
    teams to focus on the characteristics of coding defects in the DL framework program.
    They collected and summarized the program bug reports related to the Tensorflow
    framework in the open-source community, and then analyzed the challenges of localizing
    these program bugs. They found that in the execution phase, the buggy behavior
    becomes stochastic and hard to localize and suggested replacing the network hyperparameters
    to solve the localization problem. Their work is more inclined to analyze some
    programs and projects based on the DL framework and pays limited attention to
    the bugs in the DL framework itself. Zhang et al. (Zhang et al., [2020b](#bib.bib121))
    also conducted an empirical study on failures and errors of DL programs and projects.
    They pointed out that the root causes of many faults are actually related to the
    execution environment rather than code logic, and existing tools have limited
    support for fault localization of DL program errors. In addition, other researchers (Zhang
    et al., [2019](#bib.bib125)) interviewed and surveyed 195 practitioners to sort
    out and understand the current challenges in the DL software development life
    cycle from the perspective of SE, and they proposed 7 suggestions for practitioners
    and researchers, such as junior practitioners should start with well-known DL
    frameworks.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等（Zhang 等，[2018](#bib.bib127)）是首批关注深度学习框架程序编码缺陷特征的研究团队之一。他们在开源社区中收集并总结了与
    Tensorflow 框架相关的程序漏洞报告，然后分析了这些程序漏洞的定位挑战。他们发现，在执行阶段，漏洞行为变得随机且难以定位，并建议更换网络超参数以解决定位问题。他们的工作更倾向于分析一些基于深度学习框架的程序和项目，并对深度学习框架本身的漏洞关注有限。Zhang
    等（Zhang 等，[2020b](#bib.bib121)）还对深度学习程序和项目的失败和错误进行了实证研究。他们指出，许多故障的根本原因实际上与执行环境有关，而不是代码逻辑，现有工具对深度学习程序错误的故障定位支持有限。此外，其他研究人员（Zhang
    等，[2019](#bib.bib125)）对 195 名从业者进行了访谈和调查，从软件工程的角度梳理并理解了深度学习软件开发生命周期中的当前挑战，并提出了
    7 条建议，例如初级从业者应从知名的深度学习框架开始。
- en: Islam et al. (Islam et al., [2019](#bib.bib44)) further studied over 3000 bug
    posts and fixes which are related to five popular DL frameworks in the open source
    communities to understand the bug types, root causes and their effects. Their
    work is one of the first research to systematically summarize and analyze the
    bugs on multiple DL frameworks. They summarized 6 kinds of bug effects, including
    bad performance, crash, incorrect functionality, data corruption, hang, and memory
    out of bound. They found that the first three bug effects are the most common,
    which is consistent with a recent research (Chen et al., [2023a](#bib.bib10)).
    Similarly collecting information from open source, Chen et al. (Chen et al., [2020](#bib.bib16))
    paid attention to the program errors related to the deployment of DL models and
    frameworks, and organized and analyzed the challenges in deploying on platforms
    such as server, cloud, mobile, and browser.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Islam等人（Islam et al., [2019](#bib.bib44)）进一步研究了3000多个与五个流行DL框架相关的错误帖子和修复，以了解错误类型、根本原因及其影响。他们的工作是系统总结和分析多个DL框架上的错误的首次研究之一。他们总结了6种错误影响，包括性能差、崩溃、不正确的功能、数据损坏、挂起和内存越界。他们发现前三种错误影响是最常见的，这与最近的研究（Chen
    et al., [2023a](#bib.bib10)）一致。Chen等人（Chen et al., [2020](#bib.bib16)）也从开源社区收集信息，关注与DL模型和框架部署相关的程序错误，并整理和分析了在服务器、云、移动和浏览器等平台上部署的挑战。
- en: Recently, researchers have conducted more in-depth and subdivided research on
    DL frameworks and testing methods on the basis of prior work.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，研究人员在之前工作的基础上，对DL框架和测试方法进行了更深入和细化的研究。
- en: Some researchers deeply investigated and analyzed the DL framework bug and further
    proposed new testing methods. Chen et al. (Chen et al., [2023a](#bib.bib10)) conducted
    a large-scale study on 1000 bugs on 4 DL frameworks. They provided an in-depth
    summary of 13 types of root causes, including API misuse, numerical issues, etc.,
    and analyzed the symptoms of DL framework bugs. They further compared the indicators
    (e.g., line coverage) of the three existing test methods (Pham et al., [2019](#bib.bib89);
    Guo et al., [2020](#bib.bib36); Wang et al., [2020b](#bib.bib109)) to evaluate
    the effect of different testing methods and proposed a preliminary mutation-based
    testing tools TenFuzz. They took a first step to evaluate the effectiveness of
    existing DL library testing methods, but unfortunately failed to evaluate more
    state-of-the-art open-sourced testing tools, and their evaluation metrics were
    relatively simple. In addition, Yang et al. (Yang et al., [2022](#bib.bib119))
    also conducted a large-scale empirical study on the characteristics, root causes
    and fix patches of DL framework bugs. Instead of researching the characteristics
    of the DL framework bugs itself, Jia et al. (Jia et al., [2022](#bib.bib45)) studied
    the impact of injected bugs in the DL framework on the execution. They designed
    8 mutation operators and leveraged these operators to inject bugs into TensorFlow,
    Theano, and Keras frameworks and compare the runtime differences between the clean
    version and the buggy version. They found that most bugs do not cause observable
    errors and only introduce insignificant differences in the trained model accuracy.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员深入调查和分析了DL框架的错误，并进一步提出了新的测试方法。Chen等人（Chen et al., [2023a](#bib.bib10)）对4个DL框架上的1000个错误进行了大规模研究。他们深入总结了13种根本原因，包括API误用、数值问题等，并分析了DL框架错误的症状。他们进一步比较了三种现有测试方法（Pham
    et al., [2019](#bib.bib89); Guo et al., [2020](#bib.bib36); Wang et al., [2020b](#bib.bib109)）的指标（例如，行覆盖率），以评估不同测试方法的效果，并提出了初步的基于突变的测试工具TenFuzz。他们迈出了评估现有DL库测试方法有效性的第一步，但遗憾的是未能评估更多最先进的开源测试工具，他们的评估指标也相对简单。此外，Yang等人（Yang
    et al., [2022](#bib.bib119)）还对DL框架错误的特征、根本原因和修复补丁进行了大规模实证研究。Jia等人（Jia et al.,
    [2022](#bib.bib45)）则研究了注入错误对DL框架执行的影响。他们设计了8个突变算子，并利用这些算子向TensorFlow、Theano和Keras框架注入错误，并比较了干净版本和有错误版本的运行时差异。他们发现，大多数错误不会导致可观察的错误，只会在训练模型准确度上引入微不足道的差异。
- en: On more subdivided research fields, Quan et al. (Quan et al., [2022](#bib.bib91))
    first investigated the faults and root causes of JavaScript-based DL systems and
    related frameworks. These DL systems that are built on top of DL frameworks such
    as TensorFlow.js and are widely deployed on browsers and mobile platforms are
    also compromised by a series of bugs such as poor performance and crashes. Aach
    et al. (Aach et al., [2023](#bib.bib3)) focus on the distributed DL frameworks
    and study the performance of ResNet models on PyTorch, Horovod, and DeepSpeed
    frameworks and different data loaders. They found that using a suitable data loader
    can significantly accelerate the computation of ResNet models on these DL frameworks.
    Cao et al. (Cao et al., [2022](#bib.bib7)) also paid attention to the performance.
    They systematically studied the performance problems in DL frameworks such as
    TensorFlow and Keras, and summarized five types of root causes from the aspects
    of API usage, model parameter selection, etc. Based on the finding in the empirical
    study, they proposed and implemented a rule-based static checker, DeepPerf, to
    detect potential performance problems in DL systems. Liu et al. (Liu et al., [2022c](#bib.bib73))
    focused on the software aging problems on TensorFlow, MindSpore, MXNet, and PaddlePaddle.
    Software aging is a fault that leads to the accumulation of errors either inside
    the program, resulting in an increased failure rate and degraded performance.
    Their findings show that the training and evaluation stage is the most vulnerable
    to software aging problems. Du et al. (Du et al., [2022](#bib.bib25)) manually
    analyzed over 3,000 bug reports and further studied Bohrbug and Mandelbug on three
    DL frameworks and their triggers and root causes. The former is a kind of bug
    that is easy to reproduce under certain conditions, and they are the most common
    bugs on DL frameworks. The latter can be divided into two categories (i.e., aging-related
    bug and non-aging related Mandelbug) depending on whether they are related to
    aging problems. The mechanism of Mandelbug is relatively complicated, and it is
    often related to the interaction within the software, therefore, it may not always
    be reproduced under the same conditions. Their observations point out that most
    Bohrbugs and Mandelbugs cause crashes and exceptions, and Mandelbugs are more
    difficult to fix than Bohrbugs. Even worse, traditional software testing methods
    are hardly effective on Mandelbugs. Harzevili et al. (Harzevili et al., [2023](#bib.bib37))
    further studied vulnerabilities in DL frameworks and the corresponding fix patterns.
    Based on their findings, they developed a mutation-based testing tool, DeepMut,
    and deployed and applied it to the TensorFlow framework.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在更细分的研究领域中，Quan 等人 (Quan et al., [2022](#bib.bib91)) 首次研究了基于 JavaScript 的 DL
    系统及相关框架的故障和根本原因。这些基于 TensorFlow.js 等 DL 框架构建的 DL 系统，在浏览器和移动平台上广泛部署，也遭遇了一系列的错误，如性能差和崩溃等问题。Aach
    等人 (Aach et al., [2023](#bib.bib3)) 关注分布式 DL 框架，并研究了 ResNet 模型在 PyTorch、Horovod
    和 DeepSpeed 框架及不同数据加载器上的性能。他们发现，使用合适的数据加载器可以显著加速这些 DL 框架上 ResNet 模型的计算。Cao 等人
    (Cao et al., [2022](#bib.bib7)) 也关注了性能问题。他们系统地研究了 TensorFlow 和 Keras 等 DL 框架中的性能问题，并从
    API 使用、模型参数选择等方面总结了五种根本原因。基于实证研究的发现，他们提出并实现了一种基于规则的静态检查器 DeepPerf，用于检测 DL 系统中的潜在性能问题。Liu
    等人 (Liu et al., [2022c](#bib.bib73)) 关注 TensorFlow、MindSpore、MXNet 和 PaddlePaddle
    上的软件老化问题。软件老化是导致程序内部错误积累的故障，从而增加故障率和性能下降。他们的发现表明，训练和评估阶段最容易受到软件老化问题的影响。Du 等人 (Du
    et al., [2022](#bib.bib25)) 手动分析了 3,000 多个 bug 报告，进一步研究了三种 DL 框架上的 Bohrbug 和 Mandelbug
    及其触发因素和根本原因。前者是一种在特定条件下容易重现的 bug，是 DL 框架上最常见的错误。后者可以分为两类（即与老化相关的 bug 和非老化相关的 Mandelbug），取决于它们是否与老化问题相关。Mandelbug
    的机制相对复杂，通常与软件内部的交互有关，因此，可能不会总是在相同条件下重现。他们的观察指出，大多数 Bohrbug 和 Mandelbug 导致崩溃和异常，且
    Mandelbug 比 Bohrbug 更难修复。更糟糕的是，传统的软件测试方法对 Mandelbug 几乎无效。Harzevili 等人 (Harzevili
    et al., [2023](#bib.bib37)) 进一步研究了 DL 框架中的漏洞及其相应的修复模式。基于他们的发现，他们开发了一个基于突变的测试工具
    DeepMut，并将其部署并应用于 TensorFlow 框架。
- en: 'Summary and Analysis: Existing research leveraged interviews and empirical
    studies to summarize and analyze software bugs and vulnerabilities in DL systems
    and frameworks and pointed out potential directions and challenges for the following
    DL framework testing work. Compared with earlier studies, recent work has spent
    extra effort on detailedly analyzing the symptoms and root causes of bugs on multiple
    DL frameworks or subdivided software problems such as aging problems. Some researchers
    even further proposed new test tools based on their findings (Harzevili et al.,
    [2023](#bib.bib37); Chen et al., [2023a](#bib.bib10)). Studies on granular and
    targeted software vulnerabilities on multiple DL frameworks may be a future research
    direction. Furthermore, comparative research on the effects of existing DL framework
    tests may also be one of the future directions.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 总结与分析：现有研究利用访谈和实证研究总结和分析了DL系统和框架中的软件错误和漏洞，并指出了未来DL框架测试工作的潜在方向和挑战。与早期研究相比，近期工作在详细分析多个DL框架上的错误症状和根本原因方面付出了额外的努力，或者关注了细化的软件问题，例如老化问题。一些研究人员甚至根据他们的发现进一步提出了新的测试工具（Harzevili
    et al., [2023](#bib.bib37); Chen et al., [2023a](#bib.bib10)）。对多个DL框架上的细粒度和有针对性的软件漏洞的研究可能是未来的研究方向。此外，对现有DL框架测试效果的比较研究也可能成为未来的方向之一。
- en: 3.2\. Differential Testing on DL Framework
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 深度学习框架的差分测试
- en: To design test oracles for DL frameworks and judge whether their behaviors are
    expected and identify DL framework bugs, researchers proposed various testing
    methods on the basis of the concept of differential testing (Pham et al., [2019](#bib.bib89);
    Guo et al., [2020](#bib.bib36)). In the early stage of DL framework testing, researchers
    compared the behaviors of various frameworks and deployment platforms and leveraged
    the differences to expose potential bugs (Liu et al., [2018](#bib.bib72); Guo
    et al., [2019](#bib.bib35)). Liu et al. (Liu et al., [2018](#bib.bib72)) conducted
    a comparative study on the three popular DL frameworks(i.e., TensorFlow, Caffe,
    and Torch). They found that there are significant differences in model optimization,
    performance, and adversarial robustness between different DL frameworks. Guo et
    al. (Guo et al., [2019](#bib.bib35)) conducted an empirical study to investigate
    the differences in architecture designs and implementations of existing frameworks
    and platforms and found several DL library bugs.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为深度学习（DL）框架设计测试神谕，并判断其行为是否符合预期及识别DL框架中的错误，研究人员基于差分测试的概念提出了各种测试方法（Pham et al.,
    [2019](#bib.bib89); Guo et al., [2020](#bib.bib36)）。在DL框架测试的早期阶段，研究人员比较了各种框架和部署平台的行为，并利用这些差异来揭示潜在的错误（Liu
    et al., [2018](#bib.bib72); Guo et al., [2019](#bib.bib35)）。Liu et al.（Liu et
    al., [2018](#bib.bib72)）对三种流行的DL框架（即TensorFlow、Caffe和Torch）进行了比较研究。他们发现不同DL框架在模型优化、性能和对抗鲁棒性方面存在显著差异。Guo
    et al.（Guo et al., [2019](#bib.bib35)）进行了实证研究，调查了现有框架和平台的架构设计和实现差异，并发现了若干个DL库中的错误。
- en: Although these studies have revealed the correctness of some DL frameworks in
    implementation, they have not formed any complete and systematic testing method
    or tool. Building on the perspective provided by the above work, researchers widely
    used the concept of differential testing to automatically and efficiently detect
    framework bugs by comparing the output results of multiple implementations of
    the test case. Depending on the generated test cases, the existing differential
    testing methods can be mainly divided into model-level testing and API-level testing (Deng
    et al., [2022](#bib.bib23)). We introduce two kinds of testing methods separately
    as follows.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些研究揭示了某些DL框架在实现上的正确性，但它们尚未形成任何完整的系统测试方法或工具。基于上述工作的视角，研究人员广泛使用了差分测试的概念，通过比较测试用例的多个实现的输出结果，自动且高效地检测框架错误。根据生成的测试用例，现有的差分测试方法主要可以分为模型级测试和API级测试（Deng
    et al., [2022](#bib.bib23)）。我们将分别介绍这两种测试方法。
- en: 3.2.1\. Model-level Differential Testing
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 模型级差分测试
- en: The model-level differential testing usually leverages the different results
    of a widely-used DL model (e.g., ResNet-50) on different platforms or frameworks
    to detect bugs. Therefore, although some model-level methods mutate the model
    layers and weights in tests, the model architecture and the API coverage in tests
    will not change significantly.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 模型级差分测试通常利用广泛使用的 DL 模型（例如 ResNet-50）在不同平台或框架上的不同结果来检测错误。因此，尽管一些模型级方法在测试中改变了模型层和权重，但是测试中的模型架构和
    API 覆盖率不会发生显著变化。
- en: '![Refer to caption](img/46cedf30d522cb820fbfca3c8bf9e74c.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/46cedf30d522cb820fbfca3c8bf9e74c.png)'
- en: Fig. 4\. Trigger Inputs for Inconsistencies Between DL Frameworks
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. DL 框架之间一致性不一致的触发输入
- en: CRADLE (Pham et al., [2019](#bib.bib89)) is one of the first tools to detect
    and identify bugs based on the concept of differential testing. Based on Keras (Ketkar
    and Ketkar, [2017](#bib.bib52)) which can build and train models on different
    DL frameworks as backends, CRADLE conducted differential testing on three frameworks(i.e.,
    TensorFlow, CNTK, and Theano) and finally detected 12 bugs. [Fig. 4](#S3.F4 "Fig.
    4 ‣ 3.2.1\. Model-level Differential Testing ‣ 3.2\. Differential Testing on DL
    Framework ‣ 3\. DL Framework Testing ‣ A Survey of Deep Learning Library Testing
    Methods") shows two trigger figures of the inconsistencies that cause one model
    to have different prediction results and accuracy on different DL frameworks.
    CRADLE compared the model layer outputs between multiple DL frameworks and detected
    such inconsistencies. In addition, as a model-level testing method, CRADLE has
    designed a bug localizer to analyze and localize the bug in the input model. By
    comparing the differences in the execution graphs of one model on different frameworks,
    CRADLE calculated the rate of change in deviation between consecutive model layer
    outputs and found the specific buggy layers that affect the given model (i.e.,
    cause inconsistent model outputs). However, the inconsistent outputs of one layer
    may further lead to inconsistencies in subsequent layers, therefore this localization
    method is prone to false positives (FPs) and false negatives (FNs). To break the
    limitation, Guo et al. (Guo et al., [2020](#bib.bib36)) proposed a new automated
    framework testing and bug localization method, Audee, which is also model-level
    testing and supports the testing on the PyTorch framework. It generated a large
    number of test inputs by mutating the parameters of model layers, thereby exploring
    the potential behavior of the given model and detecting bugs. Furthermore, Audee
    leveraged a causal-testing-based technique to localize buggy layers and fine-grained
    fixed them by adjusting the parameters of the given layer and continued to identify
    the next buggy layer, thereby effectively reducing the FP rate in bug localization.
    Finally, Audee detected 26 bugs including NaNs, crashes, and inconsistencies and
    conducted an empirical study to understand the root causes of bugs and inconsistencies.
    [Fig. 5](#S3.F5 "Fig. 5 ‣ 3.2.1\. Model-level Differential Testing ‣ 3.2\. Differential
    Testing on DL Framework ‣ 3\. DL Framework Testing ‣ A Survey of Deep Learning
    Library Testing Methods") shows a NaN bug in torch.nn.AvgPool2d on the PyTorch
    framework. Audee detected this bug by comparing the model prediction results and
    behaviors between different DL frameworks and localized the root cause based on
    the causal testing technique, which is that its implementation only reduced ‘outputSize’
    variable iff ‘pad_l=True’ and led to division-by-zero overflow when ‘pad_l=False’
    and NaN bug¹¹1https://github.com/pytorch/pytorch/issues/36977. LEMON (Wang et al.,
    [2020b](#bib.bib109)) also explored bugs such as output inconsistencies through
    mutating DL models. It mainly focused on four DL frameworks including TensorFlow,
    Theano, CNTK, and MXNet, and identified 24 bugs including 1 performance bug in
    testing. Gu et al. (Gu et al., [2022a](#bib.bib34)) proposed another model-level
    differential testing tool, Muffin. Similar to the ideas of CRADLE and other prior
    work, Muffin generated a large number of DL models with different architectures
    and detected inconsistencies in model layers on TensorFlow, CNTK, and Theano frameworks
    to identify DL framework bugs. In addition, Muffin creatively provides differential
    testing in both model training and inference processes, thus it obtained more
    opportunities to identify new bugs and finally detected 39 new bugs.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: CRADLE（Pham et al., [2019](#bib.bib89)）是第一个基于差异测试概念来检测和识别错误的工具之一。基于Keras（Ketkar和Ketkar，[2017](#bib.bib52)），它可以在不同的深度学习框架作为后端上构建和训练模型，CRADLE对三个框架（即TensorFlow、CNTK和Theano）进行了差异测试，并最终检测到12个错误。[图4](#S3.F4
    "Fig. 4 ‣ 3.2.1\. Model-level Differential Testing ‣ 3.2\. Differential Testing
    on DL Framework ‣ 3\. DL Framework Testing ‣ A Survey of Deep Learning Library
    Testing Methods") 显示了导致一个模型在不同深度学习框架上产生不同预测结果和准确性的两个触发图。CRADLE比较了多个深度学习框架之间的模型层输出，并检测到这些不一致之处。此外，作为一种模型级测试方法，CRADLE设计了一个错误定位器来分析和定位输入模型中的错误。通过比较一个模型在不同框架下的执行图的差异，CRADLE计算了连续模型层输出的偏差变化率，并找出了影响给定模型的特定错误层（即导致模型输出不一致的层）。然而，一个层的不一致输出可能进一步导致后续层的不一致，因此这种定位方法容易出现假阳性（FPs）和假阴性（FNs）。为了解决这一限制，Guo
    et al.（Guo et al., [2020](#bib.bib36)）提出了一种新的自动化框架测试和错误定位方法Audee，它也是一种模型级测试，并支持对PyTorch框架的测试。通过变异模型层的参数，Audee生成了大量测试输入，从而探索了给定模型的潜在行为并检测错误。此外，Audee利用基于因果测试的技术来定位错误层，并通过调整给定层的参数来细化修复，从而有效降低了错误定位中的FP率。最后，Audee检测到26个错误，包括NaNs、崩溃和不一致，并进行了实证研究以了解错误和不一致的根本原因。[图5](#S3.F5
    "Fig. 5 ‣ 3.2.1\. Model-level Differential Testing ‣ 3.2\. Differential Testing
    on DL Framework ‣ 3\. DL Framework Testing ‣ A Survey of Deep Learning Library
    Testing Methods") 显示了PyTorch框架下torch.nn.AvgPool2d中的NaN错误。Audee通过比较不同深度学习框架之间的模型预测结果和行为来检测这个错误，并基于因果测试技术定位了根本原因，即其实现仅在‘pad_l=True’时减少了‘outputSize’变量，并导致当‘pad_l=False’时出现除零溢出和NaN错误¹¹1https://github.com/pytorch/pytorch/issues/36977。LEMON（Wang
    et al., [2020b](#bib.bib109)）也通过变异深度学习模型来探索诸如输出不一致等错误。它主要关注了包括TensorFlow、Theano、CNTK和MXNet在内的四个深度学习框架，并在测试中识别出了24个错误，包括1个性能错误。Gu
    et al.（Gu et al., [2022a](#bib.bib34)）提出了另一种模型级差异测试工具Muffin。类似于CRADLE和其他先前工作的思想，Muffin生成了大量具有不同架构的深度学习模型，并在TensorFlow、CNTK和Theano框架上检测模型层的不一致，以识别深度学习框架错误。此外，Muffin创造性地在模型训练和推理过程中提供了差异测试，从而获得了更多识别新错误的机会，并最终检测到39个新错误。
- en: Some researchers compare and observe the model performance on different frameworks
    based on the concept of differential testing. Levental et al.(Levental and Orlova,
    [2020](#bib.bib57)) compared the overhead of four DL libraries including PyTorch
    in the process of abstracting and implementing DL models. They found that, compared
    with other DL libraries (e.g., cuDNN), although the DL model implemented by the
    PyTorch framework had a significant advantage in accuracy, it also brought higher
    time and memory overhead in abstracting DL models. But instead of deeply analyzing
    the implementation of several DL libraries to find the root cause of such differences,
    they only gave some hypotheses to explain their observations, which limited the
    contribution of their work.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究者基于差异测试的概念比较并观察了不同框架下模型的性能。Levental 等人（Levental 和 Orlova，[2020](#bib.bib57)）比较了包括
    PyTorch 在内的四个 DL 库在抽象和实现 DL 模型过程中的开销。他们发现，尽管 PyTorch 框架实现的 DL 模型在准确性上具有显著优势，但在抽象
    DL 模型时也带来了更高的时间和内存开销。然而，他们没有深入分析几个 DL 库的实现以找出这些差异的根本原因，而仅提出了一些假设来解释他们的观察，这限制了他们工作的贡献。
- en: '![Refer to caption](img/f40f6b32e42b1a3a4ee4f39c6a95f049.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f40f6b32e42b1a3a4ee4f39c6a95f049.png)'
- en: Fig. 5\. An NaN bug Detected by Audee on PyTorch
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. Audee 在 PyTorch 上检测到的 NaN 错误
- en: Although model-level differential testing methods obtain outstanding test results,
    they still have several limitations in applications. Restricted by the test model,
    these methods often only support several APIs related to the models, for example,
    existing research (Wei et al., [2022](#bib.bib110)) indicates that LEMON only
    covers 35 TensorFlow APIs. In addition, because the test oracle depends on the
    implementations of the model on multiple frameworks, the inconsistencies found
    in the test are difficult to confirm whether they are bugs, which affects the
    effectiveness of the test (Guo et al., [2020](#bib.bib36); Pham et al., [2019](#bib.bib89)).
    Furthermore, the implementation bugs in the model conversion tool, such as Keras (Ketkar
    and Ketkar, [2017](#bib.bib52)), ONNX (Microsoft, [2023](#bib.bib80)), can also
    lead to inconsistent behaviors of a model between two DL frameworks. Therefore,
    in recent years, model-level differential testing methods are gradually replaced
    by API-level differential testing methods that cover more DL framework APIs and
    test more efficiently.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型级差异测试方法获得了卓越的测试结果，但它们在应用中仍然存在一些局限性。由于测试模型的限制，这些方法通常只支持与模型相关的几个 API，例如，现有研究（Wei
    等人，[2022](#bib.bib110)）表明 LEMON 仅覆盖 35 个 TensorFlow API。此外，由于测试预言依赖于模型在多个框架上的实现，测试中发现的不一致难以确认是否为错误，这影响了测试的有效性（Guo
    等人，[2020](#bib.bib36)；Pham 等人，[2019](#bib.bib89)）。此外，模型转换工具中的实现错误，例如 Keras（Ketkar
    和 Ketkar，[2017](#bib.bib52)），ONNX（Microsoft，[2023](#bib.bib80)），也可能导致模型在两个 DL
    框架之间的行为不一致。因此，近年来，模型级差异测试方法逐渐被覆盖更多 DL 框架 API 并更高效的 API 级差异测试方法所取代。
- en: 3.2.2\. API-level Differential Testing
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. API级差异测试
- en: Different from the model-level methods, the test cases generated by the API-level
    method are simple calls or combinations of DL framework APIs. Some test cases
    even merely call one operator or transformation APIs to calculate or process a
    set of randomly generated input data. API-level testing eliminates the need to
    build and mutate DL models, which liberates the test cases from model shape constraints
    and enables the test to detect potential bugs on various framework APIs.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与模型级方法不同，API 级方法生成的测试用例是简单的 DL 框架 API 调用或组合。有些测试用例甚至只是调用一个操作符或转换 API 来计算或处理一组随机生成的输入数据。API
    级测试消除了构建和变异 DL 模型的需要，这使得测试用例摆脱了模型形状约束，并能够检测到各种框架 API 上的潜在错误。
- en: The API-level differential testing methods construct and leverage equivalence
    between APIs to test DL framework APIs. One of the most simple approaches is to
    execute DL APIs and operators on different devices and observe the differences
    between the behaviors. Zhang et al. (Zhang et al., [2021b](#bib.bib124)) tested
    the precision errors by comparing the behaviors of seven DL framework operators
    on CPU and GPU. Zhang et al. (Zhang et al., [2021a](#bib.bib122)) combined the
    differential testing with fuzz testing and designed nine mutation operators to
    test the DL framework APIs on TensorFlow, PyTorch, MNN, and MXNet. To efficiently
    and automatically generate test code at scale, Wei et al. (Wei et al., [2022](#bib.bib110))
    proposed FreeFuzz, the first DL libraries testing method via mining from open
    source. FreeFuzz first collected code that calls DL framework APIs from API documentation,
    DL framework test cases, and open-source models. Then, it tracked and extracted
    the input and parameter constraints of each API from the execution of the collected
    code and constructed new test cases. In the test, based on the concept of differential
    testing, FreeFuzz detected bugs by comparing the performance of test cases on
    different devices (i.e., CPU and GPU). FreeFuzz provided an effective test case
    generator that has high API coverage, and finally, it detected 49 bugs on PyTorch
    and Tensorflow frameworks.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: API级别的差异测试方法构建并利用API之间的等价性来测试深度学习（DL）框架的API。一种最简单的方法是执行DL API和操作符在不同设备上的操作，并观察行为之间的差异。张等人（张等人，[2021b](#bib.bib124)）通过比较七种DL框架操作符在CPU和GPU上的行为来测试精度错误。张等人（张等人，[2021a](#bib.bib122)）将差异测试与模糊测试结合起来，并设计了九种变异操作符来测试TensorFlow、PyTorch、MNN和MXNet上的DL框架API。为了高效自动地大规模生成测试代码，魏等人（魏等人，[2022](#bib.bib110)）提出了FreeFuzz，这是第一个通过挖掘开源项目来测试DL库的方法。FreeFuzz首先从API文档、DL框架测试用例和开源模型中收集调用DL框架API的代码。然后，它从收集到的代码执行中跟踪并提取每个API的输入和参数约束，并构建新的测试用例。在测试中，基于差异测试的概念，FreeFuzz通过比较不同设备（即CPU和GPU）上测试用例的性能来检测错误。FreeFuzz提供了一个有效的测试用例生成器，具有很高的API覆盖率，最终在PyTorch和TensorFlow框架上检测到了49个错误。
- en: Researchers also compare the results of equivalent APIs on different frameworks
    to conduct differential testing. Gu et al. (Gu et al., [2022b](#bib.bib33)) compared
    the behaviors of operators between four frameworks, including TensorFlow, TensorFlow
    Lite, MXNet, and PyTorch, to identify detects on the training and inference process.
    They compared 20 operators on four DL frameworks in experiments. However, they
    did not directly report the number of bugs identified in their tests. In addition,
    they did not propose a method to automatically determine whether the difference
    in tests comes from implementation differences or bugs on a certain framework,
    which is a significant challenge to solve in testing on multiple frameworks. Recently,
    Diffwatch (Prochnow and Yang, [2022](#bib.bib90)) compared the output of the unit
    test code of one DL framework with the output of an equivalent function from another
    DL framework and supported the tests on TensorFlow, PyTorch, Keras, and Theano.
    Although they also did not directly report the number of detected bugs, they provided
    an open-sourced tool for community developers to reproduce and use.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员还比较不同框架上等价API的结果以进行差异测试。顾等人（顾等人，[2022b](#bib.bib33)）比较了四个框架（包括TensorFlow、TensorFlow
    Lite、MXNet和PyTorch）之间操作符的行为，以识别训练和推理过程中的检测点。他们在实验中比较了四个DL框架上的20个操作符。然而，他们没有直接报告测试中识别的错误数量。此外，他们没有提出自动确定测试差异是否来自实现差异或特定框架错误的方法，这在多框架测试中是一个重大挑战。最近，Diffwatch（Prochnow
    和 Yang，[2022](#bib.bib90)）比较了一个DL框架的单元测试代码输出与另一个DL框架的等效函数输出，并支持在TensorFlow、PyTorch、Keras和Theano上进行测试。尽管他们也没有直接报告检测到的错误数量，但他们提供了一个开源工具，供社区开发者复现和使用。
- en: '![Refer to caption](img/0351d284dd8ec4fc106f30af15c0e175.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0351d284dd8ec4fc106f30af15c0e175.png)'
- en: Fig. 6\. An Example PyTorch bug that is only Detected by TitanFuzz.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 仅由TitanFuzz检测到的PyTorch错误示例。
- en: Recently, with the popularity of large language models (LLMs), large models
    such as Codex can efficiently generate or analyze code based on several lines
    of prompts, providing more opportunities for software engineering and testing
    research. TitanFuzz (Deng et al., [2023a](#bib.bib21)) used two open-sourced LLMs,
    namely Codex (Chen et al., [2021](#bib.bib13)) and InCoder (Fried et al., [2022](#bib.bib28))
    to generate test cases for DL framework APIs on a large scale and executed them
    on CPU and GPU separately to conduct the differential testing. With the support
    of the knowledge and code processing capabilities of LLMs, TitanFuzz finally detected
    65 status and numerical bugs. [Fig. 6](#S3.F6 "Fig. 6 ‣ 3.2.2\. API-level Differential
    Testing ‣ 3.2\. Differential Testing on DL Framework ‣ 3\. DL Framework Testing
    ‣ A Survey of Deep Learning Library Testing Methods") presents an example bug
    that cannot be detected by previous API-level and model-level testing. TitanFuzz
    constructed test cases by LLMs and compared the behaviors between different devices
    to identify this bug on the PyTorch framework. In addition, Deng et al. (Deng
    et al., [2023b](#bib.bib22)) proposed FuzzGPT based on LLMs, which collected data
    from open source communities and leveraged few shot learning on large models such
    as Codex and CodeGen (Nijkamp et al., [2022](#bib.bib83)) and generated generate
    edge cases for DL framework APIs to perform differential testing on CPU and GPU
    to discover status bugs (i.e., crashes). FuzzGPT finally achieved better results
    than TitanFuzz and detected 76 bugs on PyTorch and TensorFlow frameworks²²2https://github.com/pytorch/pytorch/issues/82282.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，随着大语言模型（LLMs）的普及，大型模型如 Codex 能够基于几行提示高效生成或分析代码，为软件工程和测试研究提供了更多机会。TitanFuzz
    (Deng et al., [2023a](#bib.bib21)) 使用了两个开源的 LLMs，即 Codex (Chen et al., [2021](#bib.bib13))
    和 InCoder (Fried et al., [2022](#bib.bib28))，在大规模上生成 DL 框架 API 的测试用例，并分别在 CPU
    和 GPU 上执行它们，以进行差异测试。在 LLMs 的知识和代码处理能力支持下，TitanFuzz 最终检测到 65 个状态和数值错误。[图 6](#S3.F6
    "Fig. 6 ‣ 3.2.2\. API-level Differential Testing ‣ 3.2\. Differential Testing
    on DL Framework ‣ 3\. DL Framework Testing ‣ A Survey of Deep Learning Library
    Testing Methods") 展示了一个无法通过以前的 API 级别和模型级别测试检测到的示例错误。TitanFuzz 通过 LLMs 构建测试用例，并比较不同设备之间的行为，以在
    PyTorch 框架上识别这个错误。此外，Deng et al. (Deng et al., [2023b](#bib.bib22)) 提出了基于 LLMs
    的 FuzzGPT，它从开源社区收集数据，并利用少量样本学习在大型模型如 Codex 和 CodeGen (Nijkamp et al., [2022](#bib.bib83))
    上生成边界用例，对 DL 框架 API 进行差异测试，以发现状态错误（即崩溃）。FuzzGPT 最终取得了比 TitanFuzz 更好的结果，并在 PyTorch
    和 TensorFlow 框架上检测到了 76 个错误²²2https://github.com/pytorch/pytorch/issues/82282。
- en: Differential testing based on different devices is relatively simple and easy
    to implement, but the types of bugs that can be detected are relatively single.
    In order to detect more types and more complex DL framework bugs, researchers
    tried to construct and leverage the equivalence relationship between APIs to design
    differential tests. Wang et al. (Wang et al., [2022](#bib.bib107)) proposed EAGLE,
    which created equivalent graphs to test DL frameworks. They extracted 16 new DL
    equivalence rules from DL framework API documentation and non-crash issues in
    open-source communities and designed elaborate equivalent graphs that use different
    DL framework APIs, data types, or optimizations to produce identical output given
    the same input. EAGLE focuses on the numerical bug (i.e., inconsistencies) and
    finally detected 25 bugs on TensorFlow and PyTorch. Deng et al. (Deng et al.,
    [2022](#bib.bib23)) designed DeepRel on top of FreeFuzz, which can infer equivalent
    APIs and conduct differential testing between them to identify bugs effectively.
    DeepRel designed two elaborated equivalence (i.e., status equivalence and value
    equivalence) and matched API pairs based on these equivalence relations. It considered
    the output values and status of APIs in a pair as test oracles for each other
    and detected a total of 162 status and numerical bugs. Furthermore, Yang et al. (Yang
    et al., [2023](#bib.bib117)) proposed $\nabla$Fuzz, which focuses on the bugs
    in the backward propagation process and computes the gradients under different
    implementations of automatic differentiation (e.g., the reverse accumulation and
    the forward accumulation) in DL frameworks. It identifies bugs if there is any
    inconsistency in execution status and output results of automatic differentiation.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 基于不同设备的差分测试相对简单且易于实施，但能够检测到的错误类型相对单一。为了检测更多类型和更复杂的DL框架错误，研究人员尝试构建和利用API之间的等效关系来设计差分测试。王等人（Wang
    et al., [2022](#bib.bib107)）提出了EAGLE，它创建了等效图来测试DL框架。他们从DL框架API文档和开源社区的非崩溃问题中提取了16条新的DL等效规则，并设计了精心制作的等效图，这些图使用不同的DL框架API、数据类型或优化来在相同输入下产生相同的输出。EAGLE专注于数值错误（即不一致性），最终在TensorFlow和PyTorch上检测到了25个错误。邓等人（Deng
    et al., [2022](#bib.bib23)）在FreeFuzz的基础上设计了DeepRel，它可以推断等效API并在它们之间进行差分测试，以有效识别错误。DeepRel设计了两种详细的等效性（即状态等效性和数值等效性），并基于这些等效关系匹配API对。它将API对中的输出值和状态视为彼此的测试oracle，检测到了总共162个状态和数值错误。此外，杨等人（Yang
    et al., [2023](#bib.bib117)）提出了$\nabla$Fuzz，它专注于反向传播过程中的错误，并计算了DL框架中自动微分的不同实现（例如反向累积和正向累积）下的梯度。如果自动微分的执行状态和输出结果存在不一致，它将识别错误。
- en: 'Summary and Analysis: In DL library testing, differential testing is a popular
    software testing method that detects bugs by observing whether similar implementations
    have different outputs regarding identical input (Davis and Weyuker, [1981](#bib.bib20)),
    which can help solve the problem of lack of test oracles in DL library testing.
    In the early stage of DL framework differential testing, researchers usually compare
    the output results of a model on multiple frameworks to detect potential bugs
    in the model layers. However, this approach can only cover limited APIs in the
    framework and relies on model conversion tools (Ketkar and Ketkar, [2017](#bib.bib52);
    Microsoft, [2023](#bib.bib80)). It can be observed that the differential testing
    methods based on multiple frameworks usually test TensorFlow, CNTK, and Theano,
    which can conveniently convert models to each other by the Keras library. With
    Keras 2.4.0 release³³3https://github.com/keras-team/keras/releases/tag/2.4.0 no
    longer supporting multi-backend, and CNTK and Theano stopping maintenance, differential
    testing methods based on multiple frameworks have become increasingly rare. Later,
    researchers started to design or construct differential testing scenarios, such
    as using different APIs to construct equivalent calculation graphs, matching similar
    APIs with equivalence relations, etc., and achieving outstanding results. Until
    now, differential testing is still one of the most popular DL framework testing
    methods. In recent research, the capability of LLMs in code generation enables
    new methods to efficiently generate test cases. These methods are usually able
    to generate numerous test cases and compare the execution results on different
    devices to detect potential bugs. In the future, DL framework test case generation
    and testing research may integrate more closely with LLMs, and the research on
    leveraging LLMs to generate implementations for a DL framework API as a pseudo
    test oracle for differential testing may be a future research interest.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 总结与分析：在深度学习库测试中，差异测试是一种流行的软件测试方法，通过观察类似实现对相同输入的输出是否不同来检测错误 (Davis and Weyuker,
    [1981](#bib.bib20))，这有助于解决深度学习库测试中缺乏测试oracle的问题。在深度学习框架差异测试的早期阶段，研究人员通常比较一个模型在多个框架上的输出结果，以检测模型层中的潜在错误。然而，这种方法只能覆盖框架中的有限API，并依赖于模型转换工具 (Ketkar
    and Ketkar, [2017](#bib.bib52); Microsoft, [2023](#bib.bib80))。可以观察到，基于多个框架的差异测试方法通常测试TensorFlow、CNTK和Theano，这些框架可以通过Keras库相互转换。随着Keras
    2.4.0发布³³3https://github.com/keras-team/keras/releases/tag/2.4.0 不再支持多后端，以及CNTK和Theano停止维护，基于多个框架的差异测试方法变得越来越少。后来，研究人员开始设计或构建差异测试场景，如使用不同的API构建等效计算图、匹配类似API与等效关系等，并取得了显著成果。直到现在，差异测试仍然是最受欢迎的深度学习框架测试方法之一。在最近的研究中，LLM在代码生成中的能力使得新的方法能够高效地生成测试用例。这些方法通常能够生成大量的测试用例，并比较不同设备上的执行结果以检测潜在错误。未来，深度学习框架测试用例生成和测试研究可能与LLM更加紧密地整合，利用LLM生成深度学习框架API实现作为差异测试的伪测试oracle的研究可能成为未来的研究兴趣。
- en: 3.3\. Fuzz Testing on DL Framework
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 深度学习框架上的模糊测试
- en: According to the methods of generating test input, fuzzing can mainly be divided
    into generation-based fuzzing and mutation-based fuzzing (Oehlert, [2005](#bib.bib85)).
    In the following, we detailedly introduce the generation-based DL framework fuzzing
    methods and the mutation-based DL framework fuzzing methods.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 根据生成测试输入的方法，模糊测试主要可以分为基于生成的模糊测试和基于变异的模糊测试 (Oehlert, [2005](#bib.bib85))。接下来，我们详细介绍基于生成的深度学习框架模糊测试方法和基于变异的深度学习框架模糊测试方法。
- en: 3.3.1\. Generation-based Fuzz Testing
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 基于生成的模糊测试
- en: Generation-based fuzzing generates test inputs randomly or based on the specifications
    of test inputs. As the test input of DL framework testing, DL programs usually
    have complex specifications (e.g., specific value ranges of API parameters, input
    sizes and dimensions), and the input violating specification will lead to a termination
    in execution, therefore existing work rarely generates test inputs randomly.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 基于生成的模糊测试随机生成测试输入或基于测试输入的规格进行生成。作为深度学习框架测试的测试输入，深度学习程序通常具有复杂的规格（例如，API参数的特定值范围、输入大小和维度），输入不符合规格将导致执行终止，因此现有工作很少随机生成测试输入。
- en: Some DL framework fuzzing tools extract and leverage the API constraints in
    the documentation or source code to guide the testing. Xie et al. (Xie et al.,
    [2022](#bib.bib114)) designed DocTer that analyzed documentation and extracted
    DL framework API constraints and further automatically built test cases. The test
    case generation in DocTer generated both valid and invalid cases according to
    the constraints and specifications to comprehensively evaluate whether the DL
    framework has unexpected behaviors. Valid test cases are used to test whether
    the functions of the DL framework have expected implementation, while invalid
    test cases verify whether the DL framework can correctly identify and prevent
    invalid input. DocTer finally found 94 status bugs on three different frameworks.
    DocTer provides a DL API constraint extraction method and an effective test input
    generation tool for DL framework testing, which facilitates and promotes the development
    of other methods. For example, Wang et al. (Wang et al., [2022](#bib.bib107))
    leveraged DocTer to generate test inputs and compared the execution results of
    the equivalent test inputs to detect bugs for 1,427 APIs on PyTorch and TensorFlow.
    Recently, Shi et al. (Shi et al., [2023](#bib.bib98)) proposed ACETest, which
    collected DL operators’ information from the source code and extracted input validation
    constraints by analyzing the execution path, thus they can build valid test cases
    to uncover crashes in DL frameworks. In the experiments, ACETest achieved better
    results in terms of test case generation and bug detection than state-of-the-art
    methods. In addition, Schumi et al. (Schumi and Sun, [2022](#bib.bib95)) designed
    an executable semantics for TensorFlow, which can precisely compute the output
    constraints to provide reliable test oracles and effectively generate valid DL
    models for testing. Their DL semantics help to expose 14 issues and bugs on TensorFlow.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 一些深度学习（DL）框架模糊测试工具提取并利用文档或源代码中的 API 约束来指导测试。谢等人（Xie et al., [2022](#bib.bib114)）设计了
    DocTer，该工具分析文档并提取 DL 框架 API 约束，进一步自动生成测试用例。DocTer 中的测试用例生成根据约束和规范生成了有效和无效的用例，以全面评估
    DL 框架是否具有意外行为。有效的测试用例用于测试 DL 框架的功能是否按预期实现，而无效的测试用例则验证 DL 框架是否能够正确识别和防止无效输入。DocTer
    最终在三个不同的框架上发现了 94 个状态漏洞。DocTer 提供了一种 DL API 约束提取方法和一个有效的测试输入生成工具，促进和推动了其他方法的发展。例如，王等人（Wang
    et al., [2022](#bib.bib107)）利用 DocTer 生成测试输入，并比较了等效测试输入的执行结果，以检测 PyTorch 和 TensorFlow
    上的 1,427 个 API 的漏洞。最近，施等人（Shi et al., [2023](#bib.bib98)）提出了 ACETest，该工具通过分析执行路径从源代码中收集
    DL 操作符的信息并提取输入验证约束，从而可以构建有效的测试用例来揭示 DL 框架中的崩溃。在实验中，ACETest 在测试用例生成和漏洞检测方面取得了比最先进方法更好的结果。此外，舒米等人（Schumi
    and Sun, [2022](#bib.bib95)）为 TensorFlow 设计了可执行语义，该语义可以精确计算输出约束，以提供可靠的测试预言机并有效生成用于测试的有效
    DL 模型。他们的 DL 语义帮助暴露了 TensorFlow 上的 14 个问题和漏洞。
- en: Researchers also proposed the ML-based method to solve the constraints problem
    in tests. SkipFuzz (Kang et al., [2022](#bib.bib50)) used active learning to learn
    the input constraints of different library APIs and generated valid test inputs
    for TensorFlow and PyTorch. It had finally identified 43 crashes on DL frameworks,
    including 13 CVEs assigned. Recently, Liu et al. (Liu et al., [2023b](#bib.bib70))
    designed a novel DL model-based testing method. Unlike the prior work that mutated
    popular DL models to generate more test cases, their method collected and extracted
    the constraints of DL framework operators and efficiently synthesized DL models.
    Their method finally found 87 new bugs on PyTorch and TensorFlow. FuzzGPT (Deng
    et al., [2023b](#bib.bib22)) designed zero-shot and few-shot learning to prime
    LLMs to generate edge cases while ensuring the semantic validity of generated
    test cases. LLMs’ knowledge of DL API calls enabled FuzzGPT to skip the step of
    collecting API constraints and directly generate effective test code for over
    3,000 APIs.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员还提出了基于 ML 的方法来解决测试中的约束问题。SkipFuzz（Kang et al., [2022](#bib.bib50)）使用主动学习来学习不同库
    API 的输入约束，并为 TensorFlow 和 PyTorch 生成有效的测试输入。最终识别出了 DL 框架上的 43 个崩溃，其中包括 13 个分配的
    CVE。最近，刘等人（Liu et al., [2023b](#bib.bib70)）设计了一种新颖的 DL 模型基础测试方法。与以往通过突变流行的 DL
    模型来生成更多测试用例的方法不同，他们的方法收集并提取了 DL 框架操作符的约束，并高效地合成 DL 模型。他们的方法最终在 PyTorch 和 TensorFlow
    上发现了 87 个新漏洞。FuzzGPT（Deng et al., [2023b](#bib.bib22)）设计了零样本和少样本学习来引导 LLMs 生成边界用例，同时确保生成测试用例的语义有效性。LLMs
    对 DL API 调用的知识使 FuzzGPT 能够跳过收集 API 约束的步骤，直接为超过 3,000 个 API 生成有效的测试代码。
- en: These methods can automatically and efficiently extract constraints and specifications
    from a given DL framework and generate a large number of test cases to discover
    status bugs such as crashes. However, limited by the design of test oracles, some
    of these methods can hardly identify numerical and performance bugs.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法可以自动高效地从给定的DL框架中提取约束和规范，并生成大量测试用例以发现崩溃等状态错误。然而，由于测试oracle的设计限制，这些方法中的一些很难识别数值和性能错误。
- en: 3.3.2\. Mutation-based Fuzz Testing
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2. 基于变异的模糊测试
- en: In DL framework testing, mutation-based fuzzing usually applies various mutation
    strategies to several valid test inputs collected from sources (e.g., open-source
    communities and API call samples). These mutation strategies often introduce small
    changes to the test input, such as adjusting the parameter value of a function
    and substituting the data type, so as to explore potential behaviors of DL frameworks
    while ensuring the validity of test cases as much as possible.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在DL框架测试中，基于变异的模糊测试通常对从来源（例如开源社区和API调用样本）收集的多个有效测试输入应用各种变异策略。这些变异策略通常会对测试输入进行小的调整，例如调整函数的参数值和替换数据类型，从而在尽可能保证测试用例有效性的同时探索DL框架的潜在行为。
- en: Some testing methods and tools (Guo et al., [2020](#bib.bib36); Wang et al.,
    [2020b](#bib.bib109); Zhang et al., [2021a](#bib.bib122); Chen et al., [2023a](#bib.bib10);
    Harzevili et al., [2023](#bib.bib37)) mentioned above have implemented mutation
    operators to further explore potential behaviors in DL frameworks. Zhang et al. (Zhang
    et al., [2021b](#bib.bib124)) proposed a fuzzing method, Predoo, to conduct precision
    testing on 7 TensorFlow operators. Predoo magnifies the accuracy errors of DL
    framework operators by mutating the test input, thereby discovering potential
    bugs. Although Predoo promoted precision testing on DL frameworks, it supported
    a few operators on one DL framework and required manual reasoning and validation
    to obtain test oracles, making it difficult to apply on a large scale. Li et al. (Li
    et al., [2022a](#bib.bib59)) designed a mutation operator scheduling strategy
    to improve the test efficiency by selecting the effective mutation operators in
    the tests. Their method finally discovered 9 bugs on TensorFlow, Theano, CNTK
    and MXNet. Recently, Zou et al. (Zou et al., [2023](#bib.bib130)) proposed a hierarchical
    heuristic testing method, Ramos, to detect crashes and precision bugs on DL frameworks.
    Ramos leveraged a mutation-based hierarchical method to generate new models and
    increase the error of models. There are two mutation modes in Ramos, namely random
    mutation and heuristic mutation. The former randomly modified the layers and operators
    of the model, and the latter was based on the results of the previously generated
    model and tended to select mutation operators that can increase the error of the
    model. As a result, Ramos can effectively detect potential precision bugs on DL
    frameworks and finally detect 154 bugs on three frameworks, which is far beyond
    the results of previous research.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 上述一些测试方法和工具（Guo et al., [2020](#bib.bib36); Wang et al., [2020b](#bib.bib109);
    Zhang et al., [2021a](#bib.bib122); Chen et al., [2023a](#bib.bib10); Harzevili
    et al., [2023](#bib.bib37)）实现了变异操作符，以进一步探索DL框架中的潜在行为。Zhang et al.（Zhang et al.,
    [2021b](#bib.bib124)）提出了一种模糊测试方法Predoo，对7个TensorFlow操作符进行精度测试。Predoo通过变异测试输入来放大DL框架操作符的精度错误，从而发现潜在的错误。尽管Predoo推动了DL框架的精度测试，但它仅支持少数操作符并且需要手动推理和验证以获得测试oracle，使得其在大规模应用上存在困难。Li
    et al.（Li et al., [2022a](#bib.bib59)）设计了一种变异操作符调度策略，通过选择有效的变异操作符来提高测试效率。他们的方法最终发现了9个TensorFlow、Theano、CNTK和MXNet上的错误。最近，Zou
    et al.（Zou et al., [2023](#bib.bib130)）提出了一种分层启发式测试方法Ramos，用于检测DL框架上的崩溃和精度错误。Ramos利用基于变异的分层方法生成新模型并增加模型的错误。Ramos有两种变异模式，即随机变异和启发式变异。前者随机修改模型的层和操作符，后者基于之前生成模型的结果，倾向于选择能够增加模型错误的变异操作符。因此，Ramos能够有效地检测DL框架上的潜在精度错误，最终在三个框架上检测到154个错误，远超以往研究的结果。
- en: FreeFuzz (Wei et al., [2022](#bib.bib110)) collected code snippets from open
    source and implemented 3 categories of 15 mutation strategies on the data type
    and value to conduct fuzz testing. The mutation strategies include mutating the
    dimension and datatype of a tensor, mutating the value of a tensor, etc. Based
    on these strategies, FreeFuzz generated variants of a given test case from open
    source and further tested and revealed status, numerical and performance bugs
    for 1158 APIs on DL frameworks. [Fig. 7](#S3.F7 "Fig. 7 ‣ 3.3.2\. Mutation-based
    Fuzz Testing ‣ 3.3\. Fuzz Testing on DL Framework ‣ 3\. DL Framework Testing ‣
    A Survey of Deep Learning Library Testing Methods") shows a invalid input on torch.nn.MaxUnpool2d
    that only leads to a crash on the CPU device but does not throw any error message
    on GPU⁴⁴4https://github.com/pytorch/pytorch/issues/68727. FreeFuzz generated the
    invalid input by mutating the value of the input tensor and identified this status
    bug. Currently, developers have fixed this bug by adding a check for abnormal
    input values. Furthermore, DeepRel and $\nabla$Fuzz which implement on top of
    FreeFuzz are also mutation-based fuzz testing methods.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: FreeFuzz（魏等，[2022](#bib.bib110)）从开源项目中收集了代码片段，并在数据类型和值上实现了 3 类 15 种变异策略以进行模糊测试。这些变异策略包括变异张量的维度和数据类型、变异张量的值等。基于这些策略，FreeFuzz
    从开源项目中生成了给定测试用例的变体，并进一步测试和揭示了 1158 个 DL 框架 API 的状态、数值和性能漏洞。[图 7](#S3.F7 "图 7 ‣
    3.3.2. 基于变异的模糊测试 ‣ 3.3. 深度学习框架的模糊测试 ‣ 3. DL 框架测试 ‣ 深度学习库测试方法综述") 显示了 torch.nn.MaxUnpool2d
    上的无效输入，这只会导致 CPU 设备上的崩溃，但不会在 GPU 上抛出任何错误消息⁴⁴4https://github.com/pytorch/pytorch/issues/68727。FreeFuzz
    通过变异输入张量的值生成了这个无效输入，并识别出了这个状态漏洞。目前，开发人员已通过添加对异常输入值的检查来修复此漏洞。此外，基于 FreeFuzz 实现的
    DeepRel 和 $\nabla$Fuzz 也是基于变异的模糊测试方法。
- en: '![Refer to caption](img/cbf4e234a077e479fd14fa9fc566de1e.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/cbf4e234a077e479fd14fa9fc566de1e.png)'
- en: Fig. 7\. Invalid Input for MaxUnpool2d to Trigger Crash on CPU.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7. MaxUnpool2d 的无效输入在 CPU 上触发崩溃。
- en: TitanFuzz (Deng et al., [2023a](#bib.bib21)) further utilized the ability of
    LLMs in code generation. It leveraged a Codex model to generate test seeds for
    a given API and implemented four well-designed mutation operators. These mutation
    operators masked the parameters of the API, the suffix and prefix of the test
    code, and the method under test in the seed, respectively, and then used the InCoder
    model to populate the mask and generate variants at scale. TitanFuzz is currently
    one of the testing tools, which covers the most APIs, which covers a total of
    3,544 APIs on PyTorch and TensorFlow. In addition, Li et al. (Li et al., [2023b](#bib.bib60))
    proposed a coverage-guided testing tool to effectively identify status, numeric,
    and performance bugs on the TensorFlow, MXNet, and PyTorch. They designed a series
    of mutation operators to explore the search space of model input, parameter, model
    structure, and anomaly and cover more DL framework APIs and found 32 DL framework
    bugs.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: TitanFuzz（邓等，[2023a](#bib.bib21)）进一步利用了大语言模型在代码生成中的能力。它利用了一个 Codex 模型来生成给定 API
    的测试种子，并实现了四种精心设计的变异操作符。这些变异操作符分别掩盖了 API 的参数、测试代码的后缀和前缀以及测试中的方法，然后使用 InCoder 模型来填充掩盖内容并大规模生成变体。TitanFuzz
    目前是覆盖 API 最多的测试工具之一，共覆盖了 PyTorch 和 TensorFlow 上的 3,544 个 API。此外，李等（李等，[2023b](#bib.bib60)）提出了一种基于覆盖的测试工具，以有效识别
    TensorFlow、MXNet 和 PyTorch 上的状态、数值和性能漏洞。他们设计了一系列变异操作符以探索模型输入、参数、模型结构和异常的搜索空间，并覆盖了更多的深度学习框架
    API，发现了 32 个深度学习框架的漏洞。
- en: 'Summary and Analysis: Fuzz testing provides an effective means of generating
    test cases for DL frameworks. It can explore potential anomalous behaviors on
    APIs while ensuring that the generated test cases are valid. However, fuzz testing
    methods cannot provide complex test oracles for DL frameworks. Therefore, some
    DL framework fuzzing tools usually leverage differential testing to construct
    test oracles and identify inconsistencies by comparing the performance of one
    test case on different devices or multiple equivalent test cases on the same device.
    With the development and application of LLMs, the recent fuzzing methods use the
    pre-trained code language model to efficiently generate test code and detect various
    API bugs. In future research, LLM could provide more opportunities for fuzz testing.
    The current LLMs are limited by the quality of open-sourced training data, and
    there is still room for improvement in performance on complex code generation
    tasks. How to improve data quality (e.g., collecting closed-sourced data) and
    finetune the open source LLMs (e.g., StarCoder (Li et al., [2023a](#bib.bib62)))
    to improve their generation performance and design a universal test case generation
    pipeline for various bugs (i.e., status, numerical, and performance bug) on DL
    frameworks may be a future research direction.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要与分析：模糊测试提供了一种有效的手段来生成 DL 框架的测试用例。它可以探索 API 上潜在的异常行为，同时确保生成的测试用例是有效的。然而，模糊测试方法无法为
    DL 框架提供复杂的测试 oracle。因此，一些 DL 框架模糊测试工具通常利用差分测试来构建测试 oracle，并通过比较一个测试用例在不同设备上的表现或在同一设备上的多个等效测试用例来识别不一致性。随着
    LLM 的发展和应用，最近的模糊测试方法使用预训练的代码语言模型来高效地生成测试代码并检测各种 API 错误。在未来的研究中，LLM 可能为模糊测试提供更多机会。目前的
    LLM 受限于开源训练数据的质量，在复杂代码生成任务上的性能仍有改进空间。如何提高数据质量（例如，收集闭源数据）和微调开源 LLM（例如，StarCoder
    (Li et al., [2023a](#bib.bib62)））以提升其生成性能，并设计一种通用的测试用例生成管道来处理 DL 框架中的各种错误（即状态、数值和性能错误）可能是未来的研究方向。
- en: 3.4\. Metamorphic Testing on DL Framework
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. DL 框架的变形测试
- en: In DL testing, metamorphic testing is currently widely used to detect the problems
    in the design of DNN models and whether the models meet expectations (Xie et al.,
    [2009](#bib.bib115); Murphy et al., [2009](#bib.bib81); Xie et al., [2011](#bib.bib116)).
    Dwarakanath et al. (Dwarakanath et al., [2018](#bib.bib27)) implemented several
    MRs, including implementing permutation to the dataset, shifting data and scaling
    the dataset, etc, to identify implement bugs in DL applications, such as ResNet
    models. In addition, Zhang et al. (Zhang et al., [2021c](#bib.bib128)) proposed
    a metamorphic testing method for DL-driven image recognition systems. In the test,
    they implemented a series of MRs, including adjusting background brightness and
    adding noise to input images of the system and observed the results to detect
    potential problems. Despite the success of metamorphic testing on DL systems and
    models, researchers have paid limited attention to the metamorphic testing of
    DL frameworks. Ding et al. (Ding et al., [2017](#bib.bib24)) constructed 11 MRS
    and validated AlexNet on the Caffe DL Framework. However, they focused on the
    DL model accuracy in the test, and pay less attention to DL framework bugs. Wei
    et al. (Wei et al., [2022](#bib.bib110)) combined fuzzing with metamorphic testing
    to detect performance bugs in DL frameworks by comparing the execution time of
    test cases under the two data types of float16 and float32. The MR they designed
    is that programs carrying less precision information should execute faster. It
    is undeniable that they have taken an important step in detecting performance
    bugs. However, their MR can only provide a qualitative evaluation of API performance
    and cannot accurately and quantitatively detect and identify unexcepted runtime
    overhead.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DL 测试中，变形测试目前广泛用于检测 DNN 模型设计中的问题以及模型是否符合预期 (Xie et al., [2009](#bib.bib115);
    Murphy et al., [2009](#bib.bib81); Xie et al., [2011](#bib.bib116))。Dwarakanath
    等人 (Dwarakanath et al., [2018](#bib.bib27)) 实施了几个 MRs，包括对数据集进行排列，移动数据和缩放数据集等，以识别
    DL 应用中的实现错误，如 ResNet 模型。此外，Zhang 等人 (Zhang et al., [2021c](#bib.bib128)) 提出了用于
    DL 驱动的图像识别系统的变形测试方法。在测试中，他们实施了一系列 MRs，包括调整背景亮度和向系统输入图像中添加噪声，并观察结果以检测潜在问题。尽管变形测试在
    DL 系统和模型中取得了成功，但研究人员对 DL 框架的变形测试关注较少。Ding 等人 (Ding et al., [2017](#bib.bib24))
    构建了 11 个 MRs，并在 Caffe DL 框架上验证了 AlexNet。然而，他们在测试中侧重于 DL 模型的准确性，对 DL 框架中的错误关注较少。Wei
    等人 (Wei et al., [2022](#bib.bib110)) 结合模糊测试与变形测试，通过比较浮点 16 和浮点 32 数据类型下测试用例的执行时间来检测
    DL 框架中的性能错误。他们设计的 MR 是承载较少精度信息的程序应该执行得更快。不可否认的是，他们在检测性能错误方面迈出了重要一步。然而，他们的 MR 只能提供
    API 性能的定性评估，无法准确和定量地检测和识别意外的运行时开销。
- en: 'Summary and Analysis: The test results of metamorphic testing are directly
    related to the quality of the constructed MRs. Existing metamorphic testing for
    DL models and frameworks often design simple MRs, such as processing or supplementing
    training data, thus their effectiveness is limited. The metamorphic testing methods
    can verify whether the target model and system are implemented as expected in
    some aspects (i.e., the aspects related to MRs), but it cannot discover potential
    bugs in other aspects not covered. Therefore, simply using metamorphic testing
    is often difficult to obtain satisfactory test results. How to design more complex
    MRs (se.g., constructing functional equivalent test cases (Xiao et al., [2022](#bib.bib113)))
    and combine them with other testing methods to comprehensively evaluate DL frameworks
    may be a future research interest.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 总结与分析：变形测试的结果直接与构造的 MRs 的质量相关。现有的 DL 模型和框架的变形测试通常设计简单的 MRs，如处理或补充训练数据，因此其有效性有限。变形测试方法可以验证目标模型和系统在某些方面（即与
    MRs 相关的方面）是否按预期实现，但无法发现其他未覆盖方面的潜在错误。因此，单独使用变形测试通常难以获得令人满意的测试结果。如何设计更复杂的 MRs（例如，构建功能等效的测试用例
    (Xiao et al., [2022](#bib.bib113))）并将其与其他测试方法结合，以全面评估 DL 框架，可能是未来的研究兴趣。
- en: 3.5\. Other Testing on DL Framework
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 其他 DL 框架的测试
- en: In addition to the testing methods introduced above, researchers also designed
    some other targeted testing methods for specific tasks. Herbold et al. (Herbold
    and Haar, [2022](#bib.bib40)) leveraged a traditional software testing method,
    smoke testing, to test and validate whether the DL frameworks (e.g., TensorFlow)
    can correctly handle common data distributions and have expected functional implementation.
    They used different valid inputs in the training and prediction of models, and
    observed whether the program crashed. Their work provided a new perspective for
    DL framework testing research and discovered 11 unknown bugs on three DL libraries.
    Ge et al. (Ge et al., [2023](#bib.bib29)) proposed an ML-based just-in-time defect
    prediction method for TensorFlow, MXNet, Mindspore, and PaddlePaddle frameworks.
    They collected and extracted various features such as code submission information
    and trained a random forest model to effectively predict potential defects on
    the DL frameworks at the code submission level. From the perspective of DL software
    security, Christou et al. (Christou et al., [2023](#bib.bib18)) designed IvySyn,
    which is an automated vulnerability testing tool for TensorFlow and PyTorch. Different
    from the prior work that implemented fuzzing on the high-level DL framework APIs,
    IvySyn constructed code blocks by DL framework APIs (e.g., in Python program language)
    based on a set of offending inputs that trigger memory safety errors in the underlying
    implementation of DL frameworks (e.g., in C/C++ program language) to trigger security
    vulnerabilities. Their approach helped developers to find 61 security vulnerabilities
    and assign 39 unique CVEs to DL frameworks. Furthermore, to promote the development
    of the software testing methods for DL frameworks and systems, Kim et al. (Kim
    et al., [2021](#bib.bib53)) proposed an open-source DL bug benchmark, covering
    4,577 bugs in 8 categories of DL software, including DL frameworks, platforms,
    and compilers, etc.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述介绍的测试方法外，研究人员还为特定任务设计了一些其他有针对性的测试方法。Herbold等（Herbold和Haar，[2022](#bib.bib40)）利用了一种传统的软件测试方法——冒烟测试，来测试和验证深度学习框架（如TensorFlow）是否能够正确处理常见的数据分布并具有预期的功能实现。他们在模型的训练和预测中使用了不同的有效输入，并观察程序是否崩溃。他们的工作为深度学习框架测试研究提供了新的视角，并发现了三个深度学习库中的11个未知漏洞。Ge等（Ge等，[2023](#bib.bib29)）提出了一种基于机器学习的实时缺陷预测方法，适用于TensorFlow、MXNet、Mindspore和PaddlePaddle框架。他们收集并提取了各种特征，例如代码提交信息，并训练了一个随机森林模型，以有效预测深度学习框架在代码提交级别上的潜在缺陷。从深度学习软件安全的角度来看，Christou等（Christou等，[2023](#bib.bib18)）设计了IvySyn，这是一个针对TensorFlow和PyTorch的自动化漏洞测试工具。与之前在高层次深度学习框架API上实现模糊测试的工作不同，IvySyn通过深度学习框架API（例如Python程序语言）构建代码块，基于一组触发内存安全错误的输入，从而触发深度学习框架底层实现（例如C/C++程序语言）的安全漏洞。他们的方法帮助开发者发现了61个安全漏洞，并为深度学习框架分配了39个独特的CVE。此外，为了促进深度学习框架和系统的软件测试方法的发展，Kim等（Kim等，[2021](#bib.bib53)）提出了一个开源深度学习漏洞基准，涵盖了8类深度学习软件中的4,577个漏洞，包括深度学习框架、平台和编译器等。
- en: 'Summary and Analysis: DL framework tests are mainly based on methods such as
    fuzz testing to test and validate the correctness and efficiency of DL frameworks.
    There is limited work (Christou et al., [2023](#bib.bib18)) that conducted the
    tests from the perspective of software security and detects traditional security
    vulnerabilities such as buffer overflows. It is foreseeable that, with the deepening
    of DL library testing research, researchers will pay more attention to security
    vulnerabilities within the DL framework rather than bugs of the functional implementation.
    In addition, although there are a variety of framework testing methods, there
    is still a lack of effective methods to evaluate and select testing methods or
    tools for specific application scenarios. Furthermore, for identified bugs and
    defects, existing research paid limited attention to the localization and repair
    methods. Automated bug location and patch generation can intuitively improve developers’
    work efficiency, which is valuable and could be a potential future research direction.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 总结与分析：DL 框架测试主要基于模糊测试等方法来测试和验证 DL 框架的正确性和效率。针对软件安全角度进行测试的工作有限（Christou et al.,
    [2023](#bib.bib18)），并且很少检测传统的安全漏洞如缓冲区溢出。可以预见，随着 DL 库测试研究的深入，研究人员将更加关注 DL 框架中的安全漏洞而非功能实现中的缺陷。此外，尽管框架测试方法多种多样，但仍缺乏有效的方法来评估和选择特定应用场景的测试方法或工具。此外，对于已识别的缺陷和缺陷，现有研究对定位和修复方法关注较少。自动化缺陷定位和补丁生成可以直观地提高开发者的工作效率，这具有重要价值，并可能成为未来研究的一个潜在方向。
- en: 4\. DL Compiler Testing
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. DL 编译器测试
- en: In traditional software, the compiler converts high-level program language (e.g.,
    C++) to low-lever program language (e.g., assembly language) to create an executable
    program. SE Researchers have leveraged techniques such as fuzzing (Wu et al.,
    [2023](#bib.bib112)), metamorphic testing (Tao et al., [2010](#bib.bib102)), differential
    testing (Zhong, [2022](#bib.bib129)) and machine learning (Chen and Suo, [2022](#bib.bib11);
    Chen et al., [2023b](#bib.bib12)) to design a variety of testing methods for these
    traditional compilers (e.g., GCC, LLVM).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统软件中，编译器将高级编程语言（例如 C++）转换为低级编程语言（例如汇编语言）以创建可执行程序。软件工程研究人员利用了模糊测试（Wu et al.,
    [2023](#bib.bib112)）、变形测试（Tao et al., [2010](#bib.bib102)）、差异测试（Zhong, [2022](#bib.bib129)）和机器学习（Chen
    and Suo, [2022](#bib.bib11); Chen et al., [2023b](#bib.bib12)）等技术，为这些传统编译器（例如
    GCC, LLVM）设计了多种测试方法。
- en: Different from traditional compilers, DL compilers convert abstracted DL models
    into optimized operators and code, which facilitates DL hardware libraries to
    perform calculations efficiently. Compared with DL frameworks, DL compilers require
    higher professional knowledge requirements and are often used to solve task-specific
    problems of the underlying optimization of DL models. Therefore, DL compilers
    have received less attention and are still in their infancy in recent years. Recently,
    researchers have designed and implemented DL compiler testing based on the compiler
    fuzzing techniques in traditional software engineering (Liu et al., [2022b](#bib.bib71),
    [2023a](#bib.bib69)).  [Table 3](#S4.T3 "Table 3 ‣ 4\. DL Compiler Testing ‣ A
    Survey of Deep Learning Library Testing Methods") shows several representative
    DL compiler testing work. Similar to [§ 3](#S3 "3\. DL Framework Testing ‣ A Survey
    of Deep Learning Library Testing Methods"), the following section will introduce
    the existing DL compiler testing methods and their strengths and limitations by
    category in detail.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统编译器不同，DL 编译器将抽象的 DL 模型转换为优化的操作符和代码，这有助于 DL 硬件库高效地执行计算。与 DL 框架相比，DL 编译器需要更高的专业知识要求，并且通常用于解决
    DL 模型底层优化的任务特定问题。因此，DL 编译器受到的关注较少，近年来仍处于起步阶段。最近，研究人员基于传统软件工程中的编译器模糊测试技术（Liu et
    al., [2022b](#bib.bib71), [2023a](#bib.bib69)）设计并实现了 DL 编译器测试。 [表 3](#S4.T3 "Table
    3 ‣ 4\. DL Compiler Testing ‣ A Survey of Deep Learning Library Testing Methods")
    展示了几项代表性的 DL 编译器测试工作。类似于 [§ 3](#S3 "3\. DL Framework Testing ‣ A Survey of Deep
    Learning Library Testing Methods")，以下部分将详细介绍现有的 DL 编译器测试方法及其优缺点。
- en: Table 3\. Representative DL Compiler Testing Methods
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 代表性 DL 编译器测试方法
- en: '| Category | Method Description | Test Object | #Bugs | Bug Type |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法描述 | 测试对象 | #缺陷 | 缺陷类型 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Empirical &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 实证 &#124;'
- en: '&#124; Study &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 研究 &#124;'
- en: '|'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Studied and analyzed DL compiler bugs and &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 研究和分析 DL 编译器缺陷和 &#124;'
- en: '&#124; summarized the bug symptoms and root causes (Shen et al., [2021](#bib.bib97))
    &#124;'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总结了漏洞症状和根本原因 (申等， [2021](#bib.bib97)) &#124;'
- en: '|'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TVM, Glow, &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TVM、Glow, &#124;'
- en: '&#124; and nGraph &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以及 nGraph &#124;'
- en: '| / | / |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| / | / |'
- en: '| Fuzz Testing |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 模糊测试 |'
- en: '&#124; Generated valid test cases based on constraints &#124;'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于约束生成有效的测试用例 &#124;'
- en: '&#124; and conducted differential testing on DL compilers (Liu et al., [2023a](#bib.bib69))
    &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并对 DL 编译器进行了差分测试 (刘等， [2023a](#bib.bib69)) &#124;'
- en: '|'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TVM, ONNXRuntime, &#124;'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TVM、ONNXRuntime, &#124;'
- en: '&#124; TensorRT, and PyTorch &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TensorRT 和 PyTorch &#124;'
- en: '| 72 | status/numerical |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 72 | 状态/数值 |'
- en: '|'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Mutated low-level IR of DL compilers and &#124;'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 变异 DL 编译器的低级 IR 和 &#124;'
- en: '&#124; conducted coverage guided fuzz testing &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 进行了覆盖指导模糊测试 &#124;'
- en: (Liu et al., [2022b](#bib.bib71)) | TVM | 49 |
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: (刘等， [2022b](#bib.bib71)) | TVM | 49 |
- en: '&#124; status/numerical/ &#124;'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 状态/数值/ &#124;'
- en: '&#124; performance &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 性能 &#124;'
- en: '|'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Metamorphic &#124;'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 变形 &#124;'
- en: '&#124; Testing &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 测试 &#124;'
- en: '|'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Designed elaborated MRs to test the compiler behaviors &#124;'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 设计了详细的 MRs 以测试编译器行为 &#124;'
- en: '&#124; and detected buggy test inputs on DL compilers (Xiao et al., [2022](#bib.bib113))
    &#124;'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并检测到 DL 编译器上的有漏洞测试输入 (肖等， [2022](#bib.bib113)) &#124;'
- en: '| TVM, Glow, and XLA | 4 | status/numerical |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| TVM、Glow 和 XLA | 4 | 状态/数值 |'
- en: 4.1\. Empirical Study on DL Compiler
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. DL 编译器的实证研究
- en: Shen et al. (Shen et al., [2021](#bib.bib97)) took the lead in conducting the
    empirical study on DL compiler bugs. They analyzed and understood 603 bugs and
    their root causes on 3 popular DL compilers, including TVM from Apache, Glow from
    Facebook, and nGraph from Intel. They found that crash and wrong code (i.e., optimization
    bug) are the most common bugs in the DL compiler and the IR transformation stages
    are the most buggy stages. To further promote bug detection and improve the quality
    of DL compilers, they provided several suggestions for subsequent research and
    development of compilers, including adding more assertions to detect the wrong
    code bugs and designing localization methods to identify optimization-related
    bugs. In addition, Du et al. (Du et al., [2021](#bib.bib26)) collected and analyzed
    over 2,700 bug reports of TVM, Glow, nGraph, PlaidML, and Tensor Comprehensions
    (TC). They found that the most common root causes of DL compiler bugs are caused
    by semantics and compatibility, and most bugs lead to crashes or expectations,
    which is confirmed in other research (Shen et al., [2021](#bib.bib97)). However,
    possibly related to the data collection and classification method, the DL compiler
    bug types they summarized did not list performance-related bugs separately, but
    as part of the ‘warning style error’. Therefore, they did not analyze the DL compiler
    performance in depth.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 申等 (申等， [2021](#bib.bib97)) 主导了 DL 编译器漏洞的实证研究。他们分析并理解了 603 个漏洞及其根本原因，这些漏洞发生在包括
    Apache 的 TVM、Facebook 的 Glow 和 Intel 的 nGraph 在内的 3 个流行 DL 编译器上。他们发现崩溃和错误代码（即优化错误）是
    DL 编译器中最常见的漏洞，而 IR 转换阶段是最容易出错的阶段。为了进一步促进漏洞检测并提高 DL 编译器的质量，他们为后续编译器的研究和开发提供了几个建议，包括增加更多断言以检测错误代码漏洞，并设计定位方法以识别与优化相关的错误。此外，杜等
    (杜等， [2021](#bib.bib26)) 收集并分析了 TVM、Glow、nGraph、PlaidML 和 Tensor Comprehensions
    (TC) 的 2,700 多个漏洞报告。他们发现 DL 编译器漏洞的最常见根本原因是语义和兼容性问题，大多数漏洞导致崩溃或期望值问题，这在其他研究中也得到了证实
    (申等， [2021](#bib.bib97))。然而，可能与数据收集和分类方法有关，他们总结的 DL 编译器漏洞类型并未单独列出与性能相关的漏洞，而是作为‘警告风格错误’的一部分。因此，他们未能深入分析
    DL 编译器的性能。
- en: 'Summary and Analysis: The existing empirical studies on DL compilers collected
    bug reports from the open-source community, analyzed their symptoms and root causes,
    and further provided valuable suggestions for the development, application, and
    testing of DL compilers. However, the current research still lacks investigation
    and research on some DL compilers (e.g., XLA), although the bugs on them may have
    a similar distribution and characteristics to the bugs on other DL compilers.
    Therefore, a comprehensive assessment and evaluation of various DL compilers,
    their vulnerabilities and bugs, and testing methods may be one of the feasible
    future research directions.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要与分析：现有的关于深度学习（DL）编译器的实证研究收集了来自开源社区的错误报告，分析了其症状和根本原因，并进一步提供了对DL编译器开发、应用和测试的宝贵建议。然而，目前的研究仍然缺乏对一些DL编译器（例如，XLA）的调查和研究，尽管这些编译器上的错误可能与其他DL编译器上的错误具有类似的分布和特征。因此，对各种DL编译器、其漏洞和错误以及测试方法进行全面评估和评价可能是未来一个可行的研究方向。
- en: 4.2\. Fuzz Testing on DL Compiler
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2. 深度学习编译器的模糊测试
- en: DL compiler testing faces the challenge of generating valid test cases. On the
    one hand, a randomly generated test case has a quite low possibility of satisfying
    the semantic specification, especially for those complex compiler operations,
    which could be terminated before compilation. On the other hand, too simple test
    cases cannot effectively explore the potential behaviors of the DL compiler and
    find potential bugs. To overcome the challenge, researchers have designed a variety
    of fuzz testing methods based on generation and mutation to generate a large number
    of valid test cases and detect DL compiler bugs.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: DL编译器测试面临生成有效测试用例的挑战。一方面，随机生成的测试用例满足语义规范的可能性非常低，尤其是对于那些复杂的编译器操作，这些操作可能在编译前就已终止。另一方面，过于简单的测试用例无法有效地探索DL编译器的潜在行为并发现潜在的错误。为克服这一挑战，研究人员设计了多种基于生成和变异的模糊测试方法，以生成大量有效的测试用例并检测DL编译器的错误。
- en: 4.2.1\. Genration-based Fuzz Testing
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1. 基于生成的模糊测试
- en: '![Refer to caption](img/dbf855de033508a44aae4550136b43ac.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dbf855de033508a44aae4550136b43ac.png)'
- en: Fig. 8\. A Crash on the relay.build of TVM Compiler and was Detected by HirGen.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图8. TVM编译器的relay.build崩溃，并被HirGen检测到
- en: Ren et al. (Ren et al., [2023](#bib.bib93)) proposed an open-source grammar-based
    fuzzing tool, Isra, for DL compilers. Grammarly-based fuzzing is one common technique
    to generate valid test cases for compilers in software engineering (Yang et al.,
    [2011](#bib.bib118); Holler et al., [2012](#bib.bib41)). Isra implemented a domain-specific
    constraint solver to resolve and simplify constraints and found semantically valid
    input for the DL compiler and efficiently generated test cases. It has found a
    total of 33 bugs (i.e., status bugs and inconsistency bugs) for the three DL compilers
    of TVM, Glow and SophGo in the experiments. Although Isra achieved outstanding
    results in bug detection, it lacked the ability to construct complex test oracles
    and failed to detect performance and optimization bugs. Recently, researchers
    have paid more attention to the methods that conduct testing for multiple DL compilers.
    Liu et al. (Liu et al., [2023a](#bib.bib69)) proposed NNSmith to identify bugs
    on four DL compilers, including TVM, TensorRT, ONNXRuntime, and PyTorch. NNSmith
    leveraged the operator constraints provided by users to generate valid graphs
    for DL compilers and cross-validated the compilation and execution results of
    different DL compilers on the same graph to detect potential bugs. In addition,
    NNSmitch designed a loss function to guide the search for valid inputs of generated
    graphs to avoid false positives. NNSmitch identified 72 new bugs on four DL compilers,
    which demonstrates its effectiveness in bug detection. Similar to the results
    of Isra, bugs detected by NNSmitch consist of crashes and inconsistencies. Both
    of the fuzzing methods lack the ability to identify complicated bugs like optimization
    bugs.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Ren 等人 (Ren et al., [2023](#bib.bib93)) 提出了一个开源的基于语法的模糊测试工具 Isra，专为 DL 编译器设计。基于语法的模糊测试是生成有效编译器测试用例的常见技术之一
    (Yang et al., [2011](#bib.bib118); Holler et al., [2012](#bib.bib41))。Isra 实现了一个领域特定的约束求解器，用于解决和简化约束，找到
    DL 编译器的语义有效输入，并高效生成测试用例。在实验中，它为 TVM、Glow 和 SophGo 三个 DL 编译器发现了总共 33 个漏洞（即状态漏洞和不一致性漏洞）。尽管
    Isra 在漏洞检测方面取得了优秀的结果，但它缺乏构建复杂测试预言的能力，未能检测出性能和优化漏洞。最近，研究人员越来越关注对多个 DL 编译器进行测试的方法。Liu
    等人 (Liu et al., [2023a](#bib.bib69)) 提出了 NNSmith，以识别四个 DL 编译器中的漏洞，包括 TVM、TensorRT、ONNXRuntime
    和 PyTorch。NNSmith 利用用户提供的操作符约束生成 DL 编译器的有效图，并通过在相同图上交叉验证不同 DL 编译器的编译和执行结果来检测潜在漏洞。此外，NNSmith
    设计了一个损失函数来指导生成图的有效输入搜索，以避免误报。NNSmith 在四个 DL 编译器上识别了 72 个新漏洞，这证明了它在漏洞检测方面的有效性。与
    Isra 的结果类似，NNSmith 发现的漏洞包括崩溃和不一致性。这两种模糊测试方法都缺乏识别复杂漏洞（如优化漏洞）的能力。
- en: 4.2.2\. Mutation-based Fuzz Testing
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 基于变异的模糊测试
- en: In addition to generation-based fuzzing methods, researchers also pay attention
    to mutation-based fuzz testing. Shen et al. (Shen et al., [2021](#bib.bib97)),
    as one of the first research teams focusing on fuzzing DL compilers, took the
    lead in designing the testing tool, TVMfuzz, for the TVM compiler. TVMfuzz conducted
    fuzz testing based on the directed graph built by TVM APIs, and mutated the shape
    and type of tensors to construct new unit tests. It finally detected 8 crash bugs
    by performing differential testing on the two versions of TVM. Liu et al. (Liu
    et al., [2022b](#bib.bib71)) further proposed the first coverage-guided fuzz testing
    tool for testing the tensor compiler (i.e., TVM), Tzer. They designed six kinds
    of mutation operators (e.g., inserting loops, replacing operations) for the low-level
    IR of DL compilers to trigger more potential behaviors, and added the mutated
    IR to the seed pool when it covered more code. Tzer leveraged differential testing
    and runtime failure to identify crashes, performance bugs, and inconsistencies,
    leading to a higher coverage of bug type than the prior work. It finally detected
    49 new bugs on the TVM compiler, surpassing prior tools (e.g., TVMfuzz) in terms
    of coverage and bug detection. Based on the findings of prior work (Shen et al.,
    [2021](#bib.bib97)), Ma et al. (Ma et al., [2023b](#bib.bib74)) focuses on high-level
    IR which is the most error-prone compilation stage and proposed HirGen to fuzzing
    DL compilers. HirGen implemented three coverage criteria to guide the fuzzing
    to cover more data types, data shapes, and edges of computation graphs and finally
    detected 21 bugs on the TVM compiler. HirGen designed three kinds of test oracles,
    including crashes and inconsistent results between IRs and between devices, to
    detect different types of failures and bugs. The bugs detected by HirGen mainly
    contain status bugs and inconsistent outputs. [Fig. 8](#S4.F8 "Fig. 8 ‣ 4.2.1\.
    Genration-based Fuzz Testing ‣ 4.2\. Fuzz Testing on DL Compiler ‣ 4\. DL Compiler
    Testing ‣ A Survey of Deep Learning Library Testing Methods") presents a crash
    on the TVM compiler⁵⁵5https://github.com/apache/tvm/pull/10502. When relay.build
    receives an ‘IRModule’ in which a function whose returned value includes another
    function exists, TVM will crash with a segmentation fault. HirGen constructed
    diverse high-level IRs that satisfy integrity constraints to conduct fuzz testing
    and exposed and reported this bug.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于生成的方法，研究人员还关注基于变异的模糊测试。沈等人（沈等人，[2021](#bib.bib97)）作为最早关注模糊测试深度学习编译器的研究团队之一，率先为TVM编译器设计了测试工具TVMfuzz。TVMfuzz基于TVM
    API构建的有向图进行模糊测试，并变异张量的形状和类型以构建新的单元测试。它最终通过对两个版本的TVM进行差分测试检测到了8个崩溃漏洞。刘等人（刘等人，[2022b](#bib.bib71)）进一步提出了第一个针对张量编译器（即TVM）的覆盖指导模糊测试工具Tzer。他们为深度学习编译器的低级IR设计了六种变异操作符（例如，插入循环、替换操作），以触发更多潜在行为，并在变异IR覆盖更多代码时将其添加到种子池中。Tzer利用差分测试和运行时失败来识别崩溃、性能漏洞和不一致性，导致了比先前工作更高的漏洞覆盖率。它最终在TVM编译器上检测到49个新漏洞，在覆盖率和漏洞检测方面超过了先前的工具（例如TVMfuzz）。基于先前工作的发现（沈等人，[2021](#bib.bib97)），马等人（马等人，[2023b](#bib.bib74)）关注于最易出错的编译阶段——高级IR，并提出了HirGen来对深度学习编译器进行模糊测试。HirGen实现了三种覆盖标准，以指导模糊测试覆盖更多数据类型、数据形状和计算图的边缘，最终在TVM编译器上检测到21个漏洞。HirGen设计了三种测试神谕，包括崩溃和IR之间以及设备之间的不一致结果，以检测不同类型的失败和漏洞。HirGen检测到的漏洞主要包括状态漏洞和不一致的输出。[图8](#S4.F8
    "Fig. 8 ‣ 4.2.1\. Genration-based Fuzz Testing ‣ 4.2\. Fuzz Testing on DL Compiler
    ‣ 4\. DL Compiler Testing ‣ A Survey of Deep Learning Library Testing Methods")展示了TVM编译器上的一次崩溃⁵⁵5https://github.com/apache/tvm/pull/10502。当relay.build接收到一个包含另一个函数的返回值的‘IRModule’时，TVM将因段错误而崩溃。HirGen构建了满足完整性约束的多样化高级IR，以进行模糊测试，并暴露和报告了这个漏洞。
- en: 'Summary and Analysis: Fuzz testing can effectively generate test inputs for
    DL compilers and explore potential behaviors and has already helped developers
    identify hundreds of bugs on these compilers. However, existing DL compiler fuzzers
    are still based on the mutator and input constraints, which have a lot of room
    for development compared with the existing research on DL frameworks. Prior research (Deng
    et al., [2023b](#bib.bib22); Li et al., [2022b](#bib.bib63)) has shown that machine
    learning technology can effectively generate valid test cases for the target program
    and guide the fuzz testing. It is foreseeable that how to leverage LLMs or other
    ML techniques to design automated and efficient testing tools for DL compilers
    may be one of the future research interests.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 总结与分析：模糊测试可以有效生成深度学习编译器的测试输入并探索潜在行为，已经帮助开发者在这些编译器上识别了数百个漏洞。然而，现有的深度学习编译器模糊测试器仍基于变异器和输入约束，与现有的深度学习框架研究相比，还有很大的发展空间。先前的研究（Deng
    et al., [2023b](#bib.bib22); Li et al., [2022b](#bib.bib63)）表明，机器学习技术可以有效生成目标程序的有效测试用例并指导模糊测试。可以预见，如何利用大型语言模型（LLMs）或其他机器学习技术设计自动化和高效的深度学习编译器测试工具，可能是未来的研究兴趣之一。
- en: 4.3\. Metamorphic Testing on DL Compiler
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 深度学习编译器上的变形测试
- en: Except for building valid test cases, how to construct test oracles to verify
    the output of the DL compiler is also a challenge. The DL compiler fuzzers mentioned
    above mainly leveraged the concept of differential testing to detect bugs by comparing
    the behavior differences between different devices or between IRs before and after
    optimization. Although they help detect a considerable amount of bugs, they cannot
    effectively identify complex optimizations or performance bugs. To construct test
    oracles to discover more complex compiler bugs and loopholes, researchers have
    delved into metamorphic testing. As mentioned above, metamorphic testing can effectively
    construct test oracles and verify whether the functional implementation is as
    expected. Xiao et al. (Xiao et al., [2022](#bib.bib113)) designed elaborated metamorphic
    relations to test DL compilers. For example, they introduced a series of operators
    whose final result is 0 in the input, and then in tests and then observed the
    change of the DL compiler result to detect potential bugs. These MRs can generate
    mutants for DL models to test the correctness of DL compiler compilation. They
    developed a testing tool, MT-DLComp, based on the above methods, and detected
    435 test inputs that could cause compiler errors and 4 DL compiler bugs on three
    compilers (i.e., TVM, Glow, and XLA). [Fig. 3](#S2.F3 "Fig. 3 ‣ 2.3.2\. DL Library
    Testing Component ‣ 2.3\. DL Library Testing ‣ 2\. Preliminary ‣ A Survey of Deep
    Learning Library Testing Methods") provides an example bug detected by MT-Comp.
    This vulnerability causes the DCE optimization in Glow to mistakenly delete active
    and valid nodes in the computation graph, resulting in the generated optimized
    operators outputting unexpected results. MT-DLComp identified this bug by generating
    variant test cases and observing whether their behaviors violate MRs after being
    compilation.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 除了构建有效的测试用例之外，如何构建测试神谕以验证深度学习编译器的输出也是一个挑战。上述提到的深度学习编译器模糊测试器主要利用了差异测试的概念，通过比较不同设备之间或优化前后的中间表示（IR）的行为差异来检测漏洞。尽管它们帮助发现了大量的漏洞，但仍无法有效识别复杂的优化或性能问题。为了构建测试神谕以发现更复杂的编译器漏洞和缺陷，研究人员深入探讨了变形测试。如上所述，变形测试可以有效地构建测试神谕，并验证功能实现是否符合预期。肖等人（Xiao
    et al., [2022](#bib.bib113)）设计了精心制定的变形关系来测试深度学习编译器。例如，他们引入了一系列输入最终结果为0的操作符，然后在测试中观察深度学习编译器结果的变化以检测潜在的漏洞。这些变形关系可以生成深度学习模型的变异体，以测试深度学习编译器编译的正确性。他们基于上述方法开发了一个测试工具MT-DLComp，并在三个编译器（即TVM、Glow和XLA）上检测到了435个可能导致编译器错误的测试输入和4个深度学习编译器漏洞。[图3](#S2.F3
    "Fig. 3 ‣ 2.3.2\. DL Library Testing Component ‣ 2.3\. DL Library Testing ‣ 2\.
    Preliminary ‣ A Survey of Deep Learning Library Testing Methods")展示了MT-Comp检测到的一个示例漏洞。该漏洞导致Glow中的DCE优化错误地删除计算图中的活动和有效节点，导致生成的优化操作符输出意外结果。MT-DLComp通过生成变异测试用例并观察其行为是否违反变形关系来识别此漏洞。
- en: 'Summary and Analysis: Metamorphic testing can effectively solve the test oracle
    problem by designing different MRs for DL compilers, and existing metamorphic
    testing research on DL compilers has achieved certain test results. However, metamorphic
    testing on DL compilers is still under development and needs more kinds of MRs
    to comprehensively verify the functions of DL compilers. In addition, metamorphic
    testing is different in generating a large number of test cases. How to combine
    metamorphic testing with other test methods (e.g., fuzz testing) to solve the
    test oracle and valid test case challenges at the same time may be a feasible
    research direction.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要与分析：变形测试可以通过为 DL 编译器设计不同的 MRs 来有效解决测试预言问题，现有的 DL 编译器变形测试研究已取得了一定的测试结果。然而，DL
    编译器上的变形测试仍在发展中，需要更多类型的 MRs 以全面验证 DL 编译器的功能。此外，变形测试在生成大量测试用例方面有所不同。如何将变形测试与其他测试方法（如模糊测试）结合起来，以同时解决测试预言和有效测试用例的挑战，可能是一个可行的研究方向。
- en: 5\. DL Hardware Library Testing
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. DL 硬件库测试
- en: Table 4\. Representative DL Hardware Library Testing Methods
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 代表性的 DL 硬件库测试方法
- en: '| Category | Method Description | Test Libraries |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法描述 | 测试库 |'
- en: '| --- | --- | --- |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Testing on Functionality |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 功能测试 |'
- en: '&#124; Generated test patterns for DL accelerator to ensure &#124;'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成 DL 加速器的测试模式以确保 &#124;'
- en: '&#124; and reliability of its functional implementation (He et al., [2021](#bib.bib39))
    &#124;'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 及其功能实现的可靠性 (He et al., [2021](#bib.bib39)) &#124;'
- en: '| NVDLA |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| NVDLA |'
- en: '|'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Designed several MRs and conducted metamorphic testing &#124;'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 设计了多个 MRs 并进行了变形测试 &#124;'
- en: '&#124; for the operators of DL accelerators (Wang et al., [2020a](#bib.bib106))
    &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 适用于 DL 加速器的操作员 (Wang et al., [2020a](#bib.bib106)) &#124;'
- en: '| HiAI/SNPE |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| HiAI/SNPE |'
- en: '| Testing on Performance |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 性能测试 |'
- en: '&#124; Conducted large-scale experiments to evaluate &#124;'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 进行了大规模实验以评估 &#124;'
- en: '&#124; the performance of the convolution operators in cuDNN (Jorda et al.,
    [2019](#bib.bib48)) &#124;'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; cuDNN 中卷积操作符的性能 (Jorda et al., [2019](#bib.bib48)) &#124;'
- en: '| cuDNN |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| cuDNN |'
- en: '|'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Compared and evaluated the performance of the fixed &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 比较并评估了固定 &#124;'
- en: '&#124; CNN architectures on three DL hardware libraries (Nazir et al., [2023](#bib.bib82))
    &#124;'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在三个 DL 硬件库上的 CNN 架构 (Nazir et al., [2023](#bib.bib82)) &#124;'
- en: '|'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; cuBLAS, cuDNN, &#124;'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; cuBLAS, cuDNN, &#124;'
- en: '&#124; and TensorRT &#124;'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和 TensorRT &#124;'
- en: '|'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: DL hardware relies on related third-party libraries (such as cuDNN (Chetlur
    et al., [2014](#bib.bib17)), a popular accelerated library for GPU) for optimization
    and acceleration when implementing and calculating DL operators. These libraries
    are often designed for specific hardware and have strong pertinence. For example,
    CuDNN can mainly optimize and accelerate GPUs developed by NVIDIA. At present,
    researchers have conducted research on testing and verifying both DL hardware
    and the related libraries. For DL hardware, existing research has proposed several
    methods and tools to effectively detect the Functional Safty (FuSa) violation (Kundu
    et al., [2021](#bib.bib54); Chaudhuri et al., [2022](#bib.bib9); Kundu et al.,
    [2023](#bib.bib55); Chaudhuri et al., [2023](#bib.bib8)) and validate and estimate
    the performance (Buber and Banu, [2018](#bib.bib6); Liu et al., [2022a](#bib.bib68))
    on the hardware. However, these works focus on the bugs and vulnerabilities in
    the hardware itself, which is a different field from our topic. In this section,
    we mainly focus on the testing methods on the DL hardware-related library. Similar
    to DL compiler testing, DL hardware library test also has challenges in designing
    valid test input for low-level hardware libraries and constructing test oracles
    to identify errors and bugs. We introduce several state-of-the-art testing methods
    in [Table 4](#S5.T4 "Table 4 ‣ 5\. DL Hardware Library Testing ‣ A Survey of Deep
    Learning Library Testing Methods") and explain how they overcome the above challenges
    in the following section. Since we cannot find the number of bugs or vulnerabilities
    detected by these methods and the bug types they identified, we do not show the
    relevant information in the table. In the following, we will introduce the testing
    research that focuses on the functional correctness and efficiency of the DL hardware
    library respectively.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: DL 硬件在实现和计算 DL 操作时依赖于相关的第三方库（例如 cuDNN（Chetlur 等，[2014](#bib.bib17)），这是一个流行的
    GPU 加速库）进行优化和加速。这些库通常为特定硬件设计，具有很强的针对性。例如，CuDNN 主要优化和加速 NVIDIA 开发的 GPU。目前，研究人员已对
    DL 硬件及相关库进行了测试和验证研究。对于 DL 硬件，现有研究提出了几种方法和工具来有效检测功能安全（FuSa）违规（Kundu 等，[2021](#bib.bib54)；Chaudhuri
    等，[2022](#bib.bib9)；Kundu 等，[2023](#bib.bib55)；Chaudhuri 等，[2023](#bib.bib8)）并验证和评估硬件上的性能（Buber
    和 Banu，[2018](#bib.bib6)；Liu 等，[2022a](#bib.bib68)）。然而，这些工作关注的是硬件本身的错误和漏洞，这是一个与我们主题不同的领域。在这一部分，我们主要关注
    DL 硬件相关库的测试方法。类似于 DL 编译器测试，DL 硬件库测试在为低级硬件库设计有效的测试输入和构建测试 oracle 以识别错误和漏洞方面也面临挑战。我们在[表
    4](#S5.T4 "Table 4 ‣ 5\. DL Hardware Library Testing ‣ A Survey of Deep Learning
    Library Testing Methods")中介绍了几种最先进的测试方法，并在下一节中解释它们如何克服上述挑战。由于我们无法找到这些方法检测到的漏洞数量或类型，因此表格中未显示相关信息。接下来，我们将分别介绍关注
    DL 硬件库功能正确性和效率的测试研究。
- en: 5.1\. Testing on DL Hardware Library Functional Correctness
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1. 对 DL 硬件库功能正确性的测试
- en: In terms of empirical research, Huang et al. (Huang et al., [2023](#bib.bib42))
    investigated and understood the dependency bug in the DL stack. They investigated
    the symptoms and root causes of a total of 326 bugs in DL libraries which include
    DL applications, DL frameworks, DL accelerators, and DL hardware. They found that
    violating the constraints among dependencies is the main root cause of bugs and
    then suggested that developers should receive systematic training to fully understand
    the DL stack and life cycle to reduce the occurrence of bugs. Their work promoted
    the following research on DL hardware-related libraries research and DL stack.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在实证研究方面，Huang 等（Huang 等，[2023](#bib.bib42)）调查并了解了 DL 堆栈中的依赖性错误。他们调查了 DL 库中包括
    DL 应用、DL 框架、DL 加速器和 DL 硬件在内的总共 326 个错误的症状和根本原因。他们发现，违反依赖关系中的约束是错误的主要根本原因，并建议开发人员应该接受系统化的培训，以充分了解
    DL 堆栈及其生命周期，从而减少错误的发生。他们的工作推动了关于 DL 硬件相关库和 DL 堆栈的后续研究。
- en: Researchers also designed a variety of methods to generate functional test patterns
    for DL hardware libraries and verify the correctness of their implementation.
    He et al. (He et al., [2021](#bib.bib39); Uezono et al., [2022](#bib.bib105))
    designed a set of functional testing methods for the NVIDIA Deep Learning Accelerator
    (NVDLA) to ensure the safety, reliability and correctness of its functional implementation.
    They designed test strategies for the accelerator’ compute units and control unit
    separately and achieved high test coverage and low test time overhead. For the
    former, they mapped the patterns of Automatic Test Pattern Generation (ATPG) of
    the computing unit to equivalent DL models, so that the execution of these models
    can reflect the response of ATPG patterns, and finally achieved 99.9% single stuck-at
    functional test coverage. For the latter, they achieved 100% coverage of the fault
    control model by deploying DL models with well-designed inputs and weights on
    a given application scenario. One step further, they conducted an industry case
    study on in-vehicle systems, and the results proved that their testing method
    can achieve high test coverage on real scenarios, which successfully achieves
    the reliability and safety requirements for DL accelerators in automotive applications.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员还设计了各种方法来生成深度学习硬件库的功能测试模式并验证其实现的正确性。He等人（He et al., [2021](#bib.bib39);
    Uezono et al., [2022](#bib.bib105)）为NVIDIA深度学习加速器（NVDLA）设计了一套功能测试方法，以确保其功能实现的安全性、可靠性和正确性。他们分别为加速器的计算单元和控制单元设计了测试策略，并实现了高测试覆盖率和低测试时间开销。对于前者，他们将计算单元的自动测试模式生成（ATPG）模式映射到等效的深度学习模型中，以便这些模型的执行能够反映ATPG模式的响应，最终实现了99.9%的单一失效功能测试覆盖率。对于后者，他们通过在给定应用场景中部署具有精心设计的输入和权重的深度学习模型，达到了故障控制模型的100%覆盖率。更进一步，他们在车载系统上进行了行业案例研究，结果证明他们的测试方法能够在真实场景中实现高测试覆盖率，成功满足了汽车应用中对深度学习加速器的可靠性和安全性要求。
- en: The metamorphic testing method is also applied in testing and validating DL
    hardware libraries. Wang et al. (Wang et al., [2020a](#bib.bib106)) conducted
    metamorphic testing and designed a series of MRs on the convolution and softmax
    operators of DL accelerators HiAI and Snapdragon Neural Processing Engine (SNPE)
    to verify the accuracy of accelerators and explore potential accuracy defects.
    Their results show that HiAI has better accuracy performance than SNPE on the
    float16 data type. However, they did not further verify the bugs or vulnerabilities
    in the implementation of the two accelerators, such as crashes and inconsistencies.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 变形测试方法也被应用于测试和验证深度学习硬件库。Wang等人（Wang et al., [2020a](#bib.bib106)）进行了变形测试，并在深度学习加速器HiAI和Snapdragon
    Neural Processing Engine（SNPE）的卷积和softmax操作符上设计了一系列变形规则（MRs），以验证加速器的准确性并探索潜在的准确性缺陷。他们的结果显示，HiAI在float16数据类型上的准确性性能优于SNPE。然而，他们并未进一步验证这两个加速器实现中的错误或漏洞，例如崩溃和不一致性。
- en: Moreover, researchers designed and applied the fuzz testing method for the DL
    hardware library. As early as 2015, Lidbury et al. (Lidbury et al., [2015](#bib.bib66))
    have designed fuzz testing for the OpenCL compiler which is an open standard for
    writing code that runs across heterogeneous platforms including CPUs, GPUs etc.
    However, their fuzz testing method can not transfer to test CUDA compiler because
    of the difference between their execution models. Recently, CUDAsmith (Jiang et al.,
    [2020](#bib.bib46)) has been designed as a test case generation tool for the underlying
    NVCC and Clang library of the DL computing platform CUDA. It implements a generation-based
    kernel function generator to create valid test inputs that are adapted for the
    CUDA context and construct test oracles based on random differential testing to
    identify bugs.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，研究人员为深度学习硬件库设计并应用了模糊测试方法。早在2015年，Lidbury等人（Lidbury et al., [2015](#bib.bib66)）就为OpenCL编译器设计了模糊测试，该编译器是一个用于编写在包括CPU、GPU等异构平台上运行的代码的开放标准。然而，由于执行模型的差异，他们的模糊测试方法无法转移到CUDA编译器的测试中。最近，CUDAsmith（Jiang
    et al., [2020](#bib.bib46)）被设计为深度学习计算平台CUDA的底层NVCC和Clang库的测试用例生成工具。它实现了一个基于生成的内核函数生成器，用于创建适用于CUDA上下文的有效测试输入，并基于随机差分测试构建测试神谕以识别错误。
- en: 'Summary and Analysis: Although the DL hardware library has received limited
    attention, there are still various methods to construct test inputs and test oracles
    for these libraries to verify their functional correctness. However, these methods
    are usually designed to test specific DL hardware libraries, have limited generalization
    ability, and lack the ability to identify real-world bugs. How to improve the
    effectiveness and generalization of these testing methods will be a challenge
    for future research.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 总结与分析：尽管DL硬件库受到的关注有限，但仍然有多种方法可以为这些库构建测试输入和测试预言，以验证其功能正确性。然而，这些方法通常是针对特定的DL硬件库设计的，具有有限的通用性，且缺乏识别真实世界错误的能力。如何提高这些测试方法的有效性和通用性将是未来研究的挑战。
- en: 5.2\. Testing on DL Hardware Library Efficiency
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2. DL硬件库效率测试
- en: In addition to functional correctness, researchers also pay attention to the
    efficiency of these libraries. Jorda et al. (Jorda et al., [2019](#bib.bib48))
    conducted a large-scale experiment to comprehensively evaluate the performance
    and efficiency of the convolution operators implemented by cuDNN. They implemented
    a total of 602 configurations of different batch sizes, input sizes, filters,
    and depths and finally provided a guideline on the performance of DL accelerators
    for subsequent research and development. Nazir et al. (Nazir et al., [2023](#bib.bib82))
    leveraged fixed CNN architectures to evaluate and analyze the efficiency of three
    GPU libraries, namely cuBLAS, cuDNN, and TensorRT. They found that TensorRT can
    significantly shorten the execution time of DL models. Furthermore, a suitable
    CNN architecture can also reduce the time overhead of DL models.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 除了功能正确性，研究人员还关注这些库的效率。Jorda等人（Jorda et al., [2019](#bib.bib48)）进行了大规模实验，全面评估了cuDNN实现的卷积算子的性能和效率。他们实现了总共602种不同的批量大小、输入大小、滤波器和深度的配置，最终为后续研究和开发提供了DL加速器性能的指导。Nazir等人（Nazir
    et al., [2023](#bib.bib82)）利用固定的CNN架构来评估和分析三个GPU库的效率，即cuBLAS、cuDNN和TensorRT。他们发现TensorRT可以显著缩短DL模型的执行时间。此外，合适的CNN架构也可以减少DL模型的时间开销。
- en: 'Summary and Analysis: The recent research on the efficiency of DL hardware
    libraries is still very limited. Compared with the testing on functional correctness,
    the testing methods on efficiency are relatively simple, and there is no systematic
    work on testing and identifying performance bugs. This research status may be
    related to the lack of effective test oracles for DL hardware library testing.
    How to solve the problem of test oracle and design the efficiency test method
    of DL hardware libraries will be a valuable and challenging research direction
    in the future.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 总结与分析：最近关于DL硬件库效率的研究仍然非常有限。与对功能正确性的测试相比，效率测试方法相对简单，且没有系统性的性能错误测试与识别工作。这种研究现状可能与缺乏有效的测试预言相关。如何解决测试预言问题并设计DL硬件库的效率测试方法将是未来一个有价值且具有挑战性的研究方向。
- en: 6\. Future Gazing
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6. 未来展望
- en: Although the testing research on the DL library has made considerable progress,
    this field is still in its nascent stage. There are some research questions not
    received enough attention, and the testing effects of existing methods and tools
    also have room for improvement. Furthermore, there is currently a lack of benchmarks
    or tools to systematically evaluate existing testing methods and analyze their
    strengths and weaknesses in different application scenarios. In this section,
    we first comprehensively summarize and analyze the progress and status of existing
    testing research, and then discuss the future challenges and opportunities, aiming
    to provide guidance and reference for subsequent testing research.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管DL库的测试研究已经取得了相当大的进展，但该领域仍处于初期阶段。有一些研究问题没有受到足够关注，现有方法和工具的测试效果也有待改进。此外，目前缺乏基准或工具来系统地评估现有测试方法，并分析它们在不同应用场景下的优缺点。在本节中，我们首先全面总结和分析现有测试研究的进展和状态，然后讨论未来的挑战和机遇，旨在为后续测试研究提供指导和参考。
- en: 6.1\. Status of Existing Testing Research
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1. 现有测试研究的状态
- en: In this section, we analyze the development status of existing testing research
    from the perspective of different components and testing methods.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从不同组件和测试方法的角度分析现有测试研究的发展状态。
- en: From the perspective of different components, DL framework testing is the most
    well-developed part of the testing of DL workflow. There are a variety of testing
    methods and tools for most of the popular DL frameworks, for example, PyTorch
    and TensorFlow. Some empirical studies comprehensively analyzed and revealed the
    characteristics of DL frameworks and their bugs. Promoted by their investigation
    results, researchers designed test case generation methods and constructed test
    oracles based on fuzz testing, differential testing, etc., which effectively discovered
    hundreds of DL framework bugs and vulnerabilities. In contrast, the testing research
    of DL compilers and DL hardware libraries is small in number and methodologically
    scattered. Although existing DL compiler tests also discover hundreds of DL compiler
    bugs, their methods are still relatively simple compared with DL framework testing,
    and the types of detected bugs are limited. In addition, their coverage of various
    optimization operations of the DL compiler also has room to improve. Existing
    DL compiler testing lacks the ability to generate large-scale test inputs and
    identify complicated performance and optimization bugs. The DL hardware library
    testing is the least developed and receives the least attention among the three
    fields. Its testing methods are more about verifying the functionality and performance
    of these DL hard libraries, but cannot detect and identify unexpected behaviors,
    thus the number of real-world bugs they can detect is relatively small. Although
    these compilers and hardware libraries may have been well maintained and tested
    during the development process, the gap in the number of testing methods and detected
    bugs also reflects the inadequacy of related research to a certain extent.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 从不同组件的角度来看，DL 框架测试是 DL 工作流程测试中最成熟的部分。对于大多数流行的 DL 框架，例如 PyTorch 和 TensorFlow，存在多种测试方法和工具。一些实证研究全面分析并揭示了
    DL 框架及其漏洞的特征。在这些调查结果的推动下，研究人员设计了测试用例生成方法，并基于模糊测试、差分测试等构建了测试预言，这些方法有效地发现了数百个 DL
    框架漏洞和缺陷。相比之下，DL 编译器和 DL 硬件库的测试研究数量较少，方法也比较分散。尽管现有的 DL 编译器测试也发现了数百个 DL 编译器缺陷，但它们的方法与
    DL 框架测试相比仍较为简单，且发现的缺陷类型有限。此外，DL 编译器的各种优化操作的覆盖范围也有待改进。现有的 DL 编译器测试缺乏生成大规模测试输入和识别复杂性能及优化缺陷的能力。DL
    硬件库测试在这三个领域中发展最为滞后，受到的关注也最少。它的测试方法更多的是验证这些 DL 硬件库的功能和性能，但无法检测和识别意外行为，因此能够检测到的真实世界缺陷数量相对较少。尽管这些编译器和硬件库在开发过程中可能已得到良好的维护和测试，但测试方法和发现缺陷的数量上的差距也在一定程度上反映了相关研究的不足。
- en: From the perspective of specific testing methods, fuzz testing is currently
    the most popular and effective method in DL library testing. When the input constraints
    and mutation methods can be determined, fuzz testing methods can automatically
    generate a large number of test cases and code blocks for different DL libraries
    and efficiently validate whether each function can be executed successfully under
    certain inputs, thereby reducing human overhead in testing. In recent years, different
    from the traditional fuzzing methods guided by constraints and mutators, ML-guided
    fuzzing, such as using ML to learn test input constraints (Kang et al., [2022](#bib.bib50))
    or leveraging LLMs to generate valid test cases (Deng et al., [2023a](#bib.bib21)),
    has achieved outstanding results. Differential testing is also a commonly used
    testing method, which compares the results of multiple implementations of the
    same function to solve the problem of test oracle. Since it cannot directly generate
    test cases, it is often combined with fuzz testing and provides simple but effective
    test oracles in recent research. For metamorphic testing, the testing effect is
    intuitively affected by MRs designed manually, therefore, its effectiveness is
    often limited in detecting real-world bugs.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 从具体测试方法的角度来看，模糊测试目前是DL库测试中最受欢迎且最有效的方法。当输入约束和变异方法可以确定时，模糊测试方法可以自动生成大量测试用例和代码块，针对不同的DL库高效地验证每个函数在特定输入下是否能够成功执行，从而减少测试中的人工开销。近年来，与传统的受约束和变异指导的模糊测试方法不同，基于机器学习的模糊测试（例如，利用机器学习学习测试输入约束 (Kang
    et al., [2022](#bib.bib50))或利用大型语言模型生成有效测试用例 (Deng et al., [2023a](#bib.bib21))）取得了显著成果。差分测试也是一种常用的测试方法，它通过比较同一功能的多个实现结果来解决测试
    oracle 问题。由于它不能直接生成测试用例，因此常与模糊测试结合，并在近期研究中提供了简单但有效的测试 oracle。对于变形测试，其测试效果直观地受到手动设计的变形规则（MRs）的影响，因此，在检测现实世界漏洞时，其有效性通常有限。
- en: 6.2\. Challenges and Opportunities
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 挑战与机会
- en: There are several limitations and challenges in existing DL library testing.
    We summarize the main challenges and the corresponding research opportunities
    as follows.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的深度学习（DL）库测试存在若干限制和挑战。我们总结了主要挑战及相应的研究机会，如下所示。
- en: 6.2.1\. Conducting Comprehensive Evaluation of DL Framework Testing Methods
  id: totrans-352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1\. 进行深度学习框架测试方法的全面评估
- en: In the recent few years, dozens of DL framework testing methods and tools have
    been proposed, which conducted comprehensive testing for different bugs such as
    crashes, inconsistent output, NaN, performance problems, and documentation bugs (Guo
    et al., [2020](#bib.bib36); Wei et al., [2022](#bib.bib110); Xie et al., [2022](#bib.bib114)).
    Some researchers have compared several test tools by some indicators (e.g., line
    coverage) (Chen et al., [2023a](#bib.bib10)), there is still no benchmark or research
    to comprehensively compare and evaluate the effect of these methods, and there
    are still research questions to be explored and answered. For example, does the
    most recent method always have advantages in terms of test results, execution
    overhead, types of bugs, etc.? Will the characteristics of the DL framework code
    and structure affect the test results of a testing method, resulting in poor test
    performance or even unusability of the method on specific frameworks?
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，已经提出了几十种深度学习框架测试方法和工具，这些方法对崩溃、不一致的输出、NaN、性能问题和文档漏洞等不同漏洞进行了全面测试 (Guo et al.,
    [2020](#bib.bib36); Wei et al., [2022](#bib.bib110); Xie et al., [2022](#bib.bib114))。一些研究人员通过某些指标（例如，行覆盖率）比较了几种测试工具 (Chen
    et al., [2023a](#bib.bib10))，但仍然没有基准或研究可以全面比较和评估这些方法的效果，仍有待探讨和回答的研究问题。例如，最近的方法是否在测试结果、执行开销、漏洞类型等方面始终具有优势？DL框架代码和结构的特点是否会影响测试方法的测试结果，导致方法在特定框架上的测试性能差甚至无法使用？
- en: The above challenges and research questions bring the following research opportunities.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 上述挑战和研究问题带来了以下研究机会。
- en: First of all, researchers could refer to traditional software testing datasets
    (e.g., Defects4J (Just et al., [2014](#bib.bib49))) to build and construct a DL
    framework bug dataset that covers plenty of APIs and various types of bugs, and
    supports multiple DL frameworks, thereby promoting the evaluation and development
    of DL framework testing methods.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，研究人员可以参考传统的软件测试数据集（例如，Defects4J (Just et al., [2014](#bib.bib49)))，构建一个覆盖大量API和各种类型漏洞、支持多种DL框架的DL框架漏洞数据集，从而促进DL框架测试方法的评估和发展。
- en: Secondly, how to design a variety of metrics to build test benchmarks and comprehensively
    evaluate the effectiveness and efficiency of DL framework testing methods is also
    a potential future direction. The metrics can include the coverage of bug types,
    API coverage, the number of bugs discovered during the limited time, the ratio
    of generated test cases that can trigger bugs, etc.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，如何设计多种指标来构建测试基准，并全面评估深度学习（DL）框架测试方法的有效性和效率也是一个潜在的未来方向。这些指标可以包括漏洞类型的覆盖范围、API覆盖率、在有限时间内发现的漏洞数量、能触发漏洞的生成测试用例的比例等。
- en: Last but not least, how to study lightweight DL library testing or evaluation
    methods and combine them with the DL library development process is also an important
    research opportunity. Existing DL library research generally leveraged GitHub
    issues and CVE reports to expose the security problem of released libraries, and
    DL libraries often only use simple unit tests in development. Combining DL library
    testing methods in the production and development of DL libraries can effectively
    reduce the cost of fixing bugs and updating versions and the threat of bugs and
    vulnerabilities to DL library production, which could further promote the application
    of DL libraries and techniques.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，如何研究轻量级的DL库测试或评估方法，并将其与DL库开发过程相结合，也是一个重要的研究机会。现有的DL库研究通常利用GitHub问题和CVE报告来暴露已发布库的安全问题，并且DL库在开发中往往只使用简单的单元测试。将DL库测试方法结合到DL库的生产和开发中，可以有效减少修复漏洞和更新版本的成本，以及漏洞和安全隐患对DL库生产的威胁，这可以进一步推动DL库和技术的应用。
- en: 6.2.2\. Testing and Validating DL Compiler and Hardware Library
  id: totrans-358
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2\. 测试和验证DL编译器与硬件库
- en: DL compilers and hardware libraries optimize the abstract DL models and execute
    corresponding operators on specific hardware. The bugs in these libraries can
    directly affect the calculation results of DL models. Even worse, they can hardly
    be detected in higher-level tests (e.g., tests on DL systems and DL frameworks).
    Nevertheless, existing DL compilers and hardware libraries have garnered relatively
    less attention from researchers. Among the 74 DL library testing papers we collected
    and surveyed, only 12.16% and 18.82% are associated with DL compiler testing and
    DL hardware library testing, respectively. Furthermore, the existing work lacks
    systematic and comprehensive testing methodologies aimed at effectively identifying
    real-world bugs in the DL compilers and hardware libraries.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: DL编译器和硬件库优化抽象的DL模型，并在特定硬件上执行相应的操作。这些库中的漏洞可以直接影响DL模型的计算结果。更糟糕的是，它们在更高层次的测试（如DL系统和DL框架测试）中几乎无法被检测到。然而，现有的DL编译器和硬件库受到研究者的关注相对较少。在我们收集并调查的74篇DL库测试论文中，只有12.16%和18.82%与DL编译器测试和DL硬件库测试相关。此外，现有的工作缺乏系统和全面的测试方法，旨在有效识别DL编译器和硬件库中的实际漏洞。
- en: There are challenges in efficiently constructing test cases for DL compilers
    and hardware libraries. The test inputs of these libraries often consist of specific
    programming languages, which makes it difficult to find enough usable reference
    code and test cases in the open-source community and extract input constraints.
    In addition, how to build valid test oracles for these libraries is also challenging
    in testing. Differential tests based on different implementations of the same
    function may not be directly applicable to DL hardware libraries designed for
    specific hardware.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 为DL编译器和硬件库有效构造测试用例存在挑战。这些库的测试输入通常包括特定的编程语言，这使得在开源社区中找到足够可用的参考代码和测试用例，以及提取输入约束变得困难。此外，为这些库构建有效的测试预言机也是测试中的一大挑战。基于相同函数不同实现的差异测试可能不直接适用于为特定硬件设计的DL硬件库。
- en: For research opportunities, researchers can refer to the idea of DL framework
    testing to design new testing methods for these DL libraries. For example, recent
    DL framework testing has introduced active learning techniques to learn input
    constraints and construct valid test cases (Kang et al., [2022](#bib.bib50)).
    DL compiler testing could also leverage ML techniques to achieve effective test
    case generation. In addition, existing DL framework differential testing research (Deng
    et al., [2022](#bib.bib23)) proposed that it is possible to perform differential
    testing by constructing equivalence relations between APIs. This concept can also
    be migrated to the DL compiler and hardware library testing research. Analyzing
    the relationship between operators and using equivalent operators to implement
    differential testing in DL compilers and hardware libraries might be a future
    research interest.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3\. Identifying Performance Bugs on DL Libraries
  id: totrans-362
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Existing work mainly detected and identified the status bugs and numerical bugs,
    which are related to the correctness of DL libraries. However, they paid limited
    attention to efficiency and could hardly discover performance bugs in these libraries.
    The main challenge in identifying performance bugs is how to design a valid test
    oracle. The symptoms of performance bugs are usually unexpected time or memory
    overhead, which is hard to predict before execution on real-world devices.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Existing research is mainly based on the concept of metamorphic testing to detect
    performance bugs. For example, they compared the executions of the same function
    with different data types and observed whether the memory or time overhead has
    increased or decreased as expected (Wei et al., [2022](#bib.bib110)). This method
    can provide a qualitative and rough test oracle, but cannot precisely predict
    the performance of the given API or operator quantitatively, and the performance
    bugs that can be tested are relatively limited. Recently, the development of ML
    techniques has provided new opportunities for constructing test oracles for performance
    bugs. Researchers could leverage ML technology to learn and predict the runtime
    overhead of DL library operators and APIs on a given hardware to construct test
    oracles, and then detect whether there is unexcepted performance and behaviors
    in subsequent large-scale testing.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.4\. Localizing and Repairing DL Library Bugs
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Nowadays, DL library research has proposed a variety of testing methods and
    tools to discover bugs, but identification should not be the end of bug detection.
    As developers have claimed, ‘One strong (bug) report is 10000x better than 100
    poor ones’⁶⁶6https://github.com/tensorflow/tensorflow/issues/61605#issuecomment-1687623365.
    A detailed bug report, localization and analysis can effectively speed up developers’
    efficiency, not to mention providing a potential fix patch in the report. Therefore,
    methods to locate the root causes and generate patches are highly needed.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: However, how to localize the root cause of these bugs in the source code and
    automatically generate fix patches is still a challenge. On the one hand, there
    is no comprehensive and usable DL library bug dataset, which makes it difficult
    to carry out bug/fault localization (FL) and automated program repair (APR) research.
    On the other hand, some DL libraries (especially DL frameworks) usually use Python
    code to call top-level APIs, while the underlying function implementation is done
    in C/C++. Localizing bugs in C/C++ program language according to Python test cases
    requires the realization of bug/fault localization across multiple programming
    languages, while traditional software testing mainly focuses on FL within one
    single programming language, which cannot be directly used in this scenario.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如何在源代码中定位这些错误的根本原因并自动生成修复补丁仍然是一个挑战。一方面，目前没有一个全面且可用的DL库错误数据集，这使得进行错误/故障定位（FL）和自动化程序修复（APR）研究变得困难。另一方面，一些DL库（特别是DL框架）通常使用Python代码调用顶层API，而底层函数实现则使用C/C++。根据Python测试用例在C/C++程序语言中定位错误需要实现跨多编程语言的错误/故障定位，而传统软件测试主要关注单一编程语言中的FL，这在这种情况下无法直接使用。
- en: The following research opportunities may help address these challenges.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 以下研究机会可能有助于解决这些挑战。
- en: Firstly, researchers can refer to the existing FL and APR datasets (e.g., Defects4J (Just
    et al., [2014](#bib.bib49))) to build a DL library bug dataset to promote the
    research on DL framework bug/fault localization and repair.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，研究人员可以参考现有的FL和APR数据集（例如，Defects4J (Just et al., [2014](#bib.bib49)））来构建一个DL库错误数据集，以促进对DL框架错误/故障定位和修复的研究。
- en: Secondly, recent research (Sun et al., [2022](#bib.bib100)) demonstrated the
    effectiveness of causality analysis in debugging and analyzing DL models and localizing
    buggy neurons, which also provides opportunities for DL library bug localization.
    Researchers can design localization methods by combining causality analysis with
    existing FL methods (Li et al., [2021](#bib.bib64); Meng et al., [2022](#bib.bib79))
    to determine the functions and lines in the source code that cause the bug.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，最近的研究 (Sun et al., [2022](#bib.bib100)) 证明了因果分析在调试和分析DL模型以及定位有问题的神经元方面的有效性，这也为DL库错误定位提供了机会。研究人员可以通过将因果分析与现有的FL方法 (Li
    et al., [2021](#bib.bib64); Meng et al., [2022](#bib.bib79)) 结合起来，设计定位方法，以确定导致错误的源代码中的函数和行。
- en: 6.2.5\. Designing Tests for the Maintenance and Evolution of DL Libraries
  id: totrans-371
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.5\. 为DL库的维护和演进设计测试
- en: Existing work on DL library testing often focuses on identifying bugs in specific
    versions of the library. However, libraries continue to have new versions released,
    which may fix some inherent bugs while potentially introducing new ones. For example,
    previous testing efforts focused heavily on PyTorch 1.x versions, but now PyTorch
    has entered the 2.x era, introducing numerous new features. New bugs in PyTorch
    2.x are continuously being identified and corresponding bug-fix versions are being
    frequently released⁷⁷7https://github.com/pytorch/pytorch/releases. Regression
    bugs are also huge pitfalls in PyTorch⁸⁸8https://github.com/pytorch/pytorch/issues/95432,
    where previously functional functions in older versions become problematic in
    the new version. This implies that testing for DL libraries needs to be continuously
    focused over time.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的DL库测试工作通常集中在识别库的特定版本中的错误。然而，库会不断发布新版本，这些新版本可能修复了一些固有的错误，同时也可能引入新的错误。例如，以前的测试工作重点关注PyTorch
    1.x版本，但现在PyTorch已经进入2.x时代，引入了许多新特性。PyTorch 2.x中的新错误不断被识别，并且相应的错误修复版本也在频繁发布⁷⁷7https://github.com/pytorch/pytorch/releases。回归错误也是PyTorch中的巨大陷阱⁸⁸8https://github.com/pytorch/pytorch/issues/95432，之前在旧版本中功能正常的函数在新版本中变得有问题。这意味着对DL库的测试需要随着时间的推移持续关注。
- en: Furthermore, with the continuous development of DL techniques (especially as
    we are currently entering the era of large models), new DL libraries (including
    frameworks, compilers, and hardware libraries) are being developed to meet new
    demands. For example, HuggingFace Transformers (Wolf et al., [2020](#bib.bib111))
    has become the most popular open-source model hosting library, supporting training
    and inference of large models; there are also various libraries optimized for
    computational requirements of large models, such as vLLM (Kwon et al., [2023](#bib.bib56)),
    DeepSpeed (Rasley et al., [2020](#bib.bib92)). These new DL libraries are often
    implemented based on previous DL libraries, such as PyTorch/TensorFlow, combined
    with some of the underlying compilers and hardware-level optimizations. Due to
    the complex nature of software aggregation, these new libraries may also be affected
    by bugs in underlying libraries, and concomitantly, new bugs may be introduced
    during the rapid development and release⁹⁹9https://github.com/vllm-project/vllm/issues/2248.
    However, there has been little attention paid to systematic bug analysis and fixing
    for those new DL libraries of the large models’ era.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'The above-mentioned issues and challenges bring about many new opportunities,
    as follows:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, researchers should collect and maintain a comprehensive test set of
    the DL library, which can help to support the evolution of DL library testing.
    On the one hand, it should contain historical data on the testing of older versions
    of libraries, such as which test cases were generated by existing methods, which
    can be used to support regression testing of subsequent versions of existing libraries.
    On the other hand, it should also collect bugs found and discussed in the community
    for new DL libraries (e.g., vLLM, DeepSpeed).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, existing test suites can provide guidance for testing the emergent
    libraries. One potential method is that based on the concepts of code similarity
    measurement and software component analysis (SCA), researchers can identify functional-similar
    components between the emerging libraries and existing libraries, and migrate
    the test suites of the latter to the former, thereby effectively identifying potential
    bugs.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Conclusion
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rapid development and widespread deployment of DL-driven software systems
    have attracted researchers in academia and industry to investigate and study the
    DL library that supports DL systems. Existing research has achieved fruitful results
    in testing DL library bugs and vulnerabilities. However, with the development
    and iteration of DL techniques and software, there is still room for improvement
    in these testing methods. On the one hand, their generalization ability still
    needs to be improved to discover vulnerabilities in more kinds of DL libraries.
    On the other hand, state-of-the-art DL techniques represented by large models
    bring both challenges and opportunities for DL library testing. To comprehensively
    assess the testing research of the DL underlying library, understand the effects
    and deficiencies of the existing testing methods, and discuss the challenges and
    directions of future research, this paper first describes the definition of DL
    library bugs and testing ,and then summarizes and reviews the existing testing
    research on three kinds of DL libraries (i.e., DL framework, DL compiler, and
    DL hardware library). Finally, this paper discusses the challenges of DL library
    testing and future research opportunities from four aspects, aiming to promote
    further development and application of DL library testing research.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IEE (2010) 2010. IEEE Standard Classification for Software Anomalies. *IEEE
    Std 1044-2009 (Revision of IEEE Std 1044-1993)* (2010), 1–23. [https://doi.org/10.1109/IEEESTD.2010.5399061](https://doi.org/10.1109/IEEESTD.2010.5399061)
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aach et al. (2023) Marcel Aach, Eray Inanc, Rakesh Sarma, Morris Riedel, and
    Andreas Lintermann. 2023. Large scale performance analysis of distributed deep
    learning frameworks for convolutional neural networks. *Journal of Big Data* 10,
    1 (2023), 1–23.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abadi (2016) Martín Abadi. 2016. TensorFlow: learning functions at scale. In
    *Proceedings of the 21st ACM SIGPLAN international conference on functional programming*.
    1–1.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Braiek and Khomh (2020) Houssem Ben Braiek and Foutse Khomh. 2020. On testing
    machine learning programs. *Journal of Systems and Software* 164 (2020), 110542.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Buber and Banu (2018) Ebubekir Buber and DIRI Banu. 2018. Performance analysis
    and CPU vs GPU comparison for deep learning. In *2018 6th International Conference
    on Control Engineering & Information Technology (CEIT)*. IEEE, 1–6.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2022) Junming Cao, Bihuan Chen, Chao Sun, Longjie Hu, Shuaihong
    Wu, and Xin Peng. 2022. Understanding performance problems in deep learning systems.
    In *Proceedings of the 30th ACM Joint European Software Engineering Conference
    and Symposium on the Foundations of Software Engineering*. 357–369.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaudhuri et al. (2023) Arjun Chaudhuri, Ching-Yuan Chen, Jonti Talukdar, and
    Krishnendu Chakrabarty. 2023. Functional Test Generation for AI Accelerators using
    Bayesian Optimization. In *2023 IEEE 41st VLSI Test Symposium (VTS)*. IEEE, 1–6.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaudhuri et al. (2022) Arjun Chaudhuri, Jonti Talukdar, Fei Su, and Krishnendu
    Chakrabarty. 2022. Functional criticality analysis of structural faults in AI
    accelerators. *IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems* 41, 12 (2022), 5657–5670.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023a) Junjie Chen, Yihua Liang, Qingchao Shen, Jiajun Jiang, and
    Shuochuan Li. 2023a. Toward understanding deep learning framework bugs. *ACM Transactions
    on Software Engineering and Methodology* 32, 6 (2023), 1–31.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen and Suo (2022) Junjie Chen and Chenyao Suo. 2022. Boosting compiler testing
    via compiler optimization exploration. *ACM Transactions on Software Engineering
    and Methodology (TOSEM)* 31, 4 (2022), 1–33.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023b) Junjie Chen, Chenyao Suo, Jiajun Jiang, Peiqi Chen, and
    Xingjian Li. 2023b. Compiler test-program generation via memoized configuration
    search. In *Proc. 45th International Conference on Software Engineering*.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374* (2021).
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018) Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng,
    Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al.
    2018. $\{$TVM$\}$: An automated $\{$End-to-End$\}$ optimizing compiler for deep
    learning. In *13th USENIX Symposium on Operating Systems Design and Implementation
    (OSDI 18)*. 578–594.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (1998) Tsong Y Chen, Shing C Cheung, and Shiu Ming Yiu. 1998. Metamorphic
    testing: a new approach for generating next test cases. *technical report hkust-cs98-01*
    (1998).'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020) Zhenpeng Chen, Yanbin Cao, Yuanqiang Liu, Haoyu Wang, Tao
    Xie, and Xuanzhe Liu. 2020. A comprehensive study on challenges in deploying deep
    learning based software. In *Proceedings of the 28th ACM Joint Meeting on European
    Software Engineering Conference and Symposium on the Foundations of Software Engineering*.
    750–762.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chetlur et al. (2014) Sharan Chetlur, Cliff Woolley, Philippe Vandermersch,
    Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. cudnn: Efficient
    primitives for deep learning. *arXiv preprint arXiv:1410.0759* (2014).'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Christou et al. (2023) Neophytos Christou, Di Jin, Vaggelis Atlidakis, Baishakhi
    Ray, and Vasileios P Kemerlis. 2023. IvySyn: Automated Vulnerability Discovery
    in Deep Learning Frameworks. In *32nd USENIX Security Symposium (USENIX Security
    23)*. 2383–2400.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commission (2021) European Commission. 2021. *Artificial Intelligence Act*.
    [https://artificialintelligenceact.eu/wp-content/uploads/2021/08/The-AI-Act.pdf](https://artificialintelligenceact.eu/wp-content/uploads/2021/08/The-AI-Act.pdf)
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Davis and Weyuker (1981) Martin D Davis and Elaine J Weyuker. 1981. Pseudo-oracles
    for non-testable programs. In *Proceedings of the ACM’81 Conference*. 254–257.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2023a) Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan
    Yang, and Lingming Zhang. 2023a. Large language models are zero-shot fuzzers:
    Fuzzing deep-learning libraries via large language models. In *Proceedings of
    the 32nd ACM SIGSOFT international symposium on software testing and analysis*.
    423–435.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2023b) Yinlin Deng, Chunqiu Steven Xia, Chenyuan Yang, Shizhuo Dylan
    Zhang, Shujing Yang, and Lingming Zhang. 2023b. Large language models are edge-case
    fuzzers: Testing deep learning libraries via fuzzgpt. *arXiv preprint arXiv:2304.02014*
    (2023).'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. (2022) Yinlin Deng, Chenyuan Yang, Anjiang Wei, and Lingming Zhang.
    2022. Fuzzing deep-learning libraries via automated relational api inference.
    In *Proceedings of the 30th ACM Joint European Software Engineering Conference
    and Symposium on the Foundations of Software Engineering*. 44–56.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding et al. (2017) Junhua Ding, Xiaojun Kang, and Xin-Hua Hu. 2017. Validating
    a deep learning framework by metamorphic testing. In *2017 IEEE/ACM 2nd International
    Workshop on Metamorphic Testing (MET)*. IEEE, 28–34.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2022) Xiaoting Du, Yulei Sui, Zhihao Liu, and Jun Ai. 2022. An empirical
    study of fault triggers in deep learning frameworks. *IEEE Transactions on Dependable
    and Secure Computing* (2022).
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2021) Xiaoting Du, Zheng Zheng, Lei Ma, and Jianjun Zhao. 2021. An
    Empirical Study on Common Bugs in Deep Learning Compilers. In *2021 IEEE 32nd
    International Symposium on Software Reliability Engineering (ISSRE)*. IEEE, 184–195.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dwarakanath et al. (2018) Anurag Dwarakanath, Manish Ahuja, Samarth Sikand,
    Raghotham M Rao, RP Jagadeesh Chandra Bose, Neville Dubash, and Sanjay Podder.
    2018. Identifying implementation bugs in machine learning based image classifiers
    using metamorphic testing. In *Proceedings of the 27th ACM SIGSOFT international
    symposium on software testing and analysis*. 118–128.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fried et al. (2022) Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric
    Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.
    2022. Incoder: A generative model for code infilling and synthesis. *arXiv preprint
    arXiv:2204.05999* (2022).'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ge et al. (2023) Jian Ge, Huiqun Yu, Guisheng Fan, Jianhao Tang, and Zijie Huang.
    2023. Just-In-Time Defect Prediction for Intellignet Computing Frameworks (in
    Chinese). *Journal of Software* 34, 9 (2023), 0–0.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gezici and Tarhan (2022) Bahar Gezici and Ayça Kolukısa Tarhan. 2022. Systematic
    literature review on software quality for AI-based software. *Empirical Software
    Engineering* 27, 3 (2022), 66.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grigorescu et al. (2020) Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and
    Gigel Macesanu. 2020. A survey of deep learning techniques for autonomous driving.
    *Journal of Field Robotics* 37, 3 (2020), 362–386.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Groce et al. (2007) Alex Groce, Gerard Holzmann, and Rajeev Joshi. 2007. Randomized
    differential testing as a prelude to formal verification. In *29th International
    Conference on Software Engineering (ICSE’07)*. IEEE, 621–631.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2022b) Diandian Gu, Yining Shi, Haozhe Liu, Ge Wu, Haiou Jiang, Yaoshuai
    Zhao, and Yun Ma. 2022b. Defect Detection for Deep Learning Frameworks Based on
    Meta Operators (in Chinese). *Chinese Journal Of Computers* 45, 2 (2022), 240–255.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2022a) Jiazhen Gu, Xuchuan Luo, Yangfan Zhou, and Xin Wang. 2022a.
    Muffin: Testing deep learning libraries via neural architecture fuzzing. In *Proceedings
    of the 44th International Conference on Software Engineering*. 1418–1430.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2019) Qianyu Guo, Sen Chen, Xiaofei Xie, Lei Ma, Qiang Hu, Hongtao
    Liu, Yang Liu, Jianjun Zhao, and Xiaohong Li. 2019. An empirical study towards
    characterizing deep learning development and deployment across different frameworks
    and platforms. In *2019 34th IEEE/ACM International Conference on Automated Software
    Engineering (ASE)*. IEEE, 810–822.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2020) Qianyu Guo, Xiaofei Xie, Yi Li, Xiaoyu Zhang, Yang Liu, Xiaohong
    Li, and Chao Shen. 2020. Audee: Automated testing for deep learning frameworks.
    In *Proceedings of the 35th IEEE/ACM International Conference on Automated Software
    Engineering*. 486–498.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harzevili et al. (2023) Nima Shiri Harzevili, Jiho Shin, Junjie Wang, Song Wang,
    and Nachiappan Nagappan. 2023. Characterizing and understanding software security
    vulnerabilities in machine learning libraries. In *2023 IEEE/ACM 20th International
    Conference on Mining Software Repositories (MSR)*. IEEE, 27–38.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2021) Yi He, Takumi Uezono, and Yanjing Li. 2021. Efficient functional
    in-field self-test for deep learning accelerators. In *2021 IEEE International
    Test Conference (ITC)*. IEEE, 93–102.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Herbold and Haar (2022) Steffen Herbold and Tobias Haar. 2022. Smoke testing
    for machine learning: simple tests to discover severe bugs. *Empirical Software
    Engineering* 27, 2 (2022), 45.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holler et al. (2012) Christian Holler, Kim Herzig, and Andreas Zeller. 2012.
    Fuzzing with code fragments. In *21st USENIX Security Symposium (USENIX Security
    12)*. 445–458.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2023) Kaifeng Huang, Bihuan Chen, Susheng Wu, Junming Cao, Lei
    Ma, and Xin Peng. 2023. Demystifying dependency bugs in deep learning stack. In
    *Proceedings of the 31st ACM Joint European Software Engineering Conference and
    Symposium on the Foundations of Software Engineering*. 450–462.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ignatov et al. (2018) Andrey Ignatov, Radu Timofte, William Chou, Ke Wang,
    Max Wu, Tim Hartley, and Luc Van Gool. 2018. Ai benchmark: Running deep neural
    networks on android smartphones. In *Proceedings of the European Conference on
    Computer Vision (ECCV) Workshops*. 0–0.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Islam et al. (2019) Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh
    Rajan. 2019. A comprehensive study on deep learning bug characteristics. In *Proceedings
    of the 2019 27th ACM Joint Meeting on European Software Engineering Conference
    and Symposium on the Foundations of Software Engineering*. 510–520.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. (2022) Li Jia, Hao Zhong, Xiaoyin Wang, Linpeng Huang, and Zexuan
    Li. 2022. How Do Injected Bugs Affect Deep Learning?. In *2022 IEEE International
    Conference on Software Analysis, Evolution and Reengineering (SANER)*. IEEE, 793–804.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2020) Bo Jiang, Xiaoyan Wang, Wing Kwong Chan, TH Tse, Na Li,
    Yongfeng Yin, and Zhenyu Zhang. 2020. Cudasmith: A fuzzer for CUDA compilers.
    In *2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)*.
    IEEE, 861–871.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2012) Guoliang Jin, Linhai Song, Xiaoming Shi, Joel Scherpelz, and
    Shan Lu. 2012. Understanding and detecting real-world performance bugs. *ACM SIGPLAN
    Notices* 47, 6 (2012), 77–88.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jorda et al. (2019) Marc Jorda, Pedro Valero-Lara, and Antonio J Pena. 2019.
    Performance evaluation of cudnn convolution algorithms on nvidia volta gpus. *IEEE
    Access* 7 (2019), 70461–70473.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Just et al. (2014) René Just, Darioush Jalali, and Michael D Ernst. 2014. Defects4J:
    A database of existing faults to enable controlled testing studies for Java programs.
    In *Proceedings of the 2014 international symposium on software testing and analysis*.
    437–440.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. (2022) Hong Jin Kang, Pattarakrit Rattanukul, Stefanus Agus Haryono,
    Truong Giang Nguyen, Chaiyong Ragkhitwetsagul, Corina Pasareanu, and David Lo.
    2022. SkipFuzz: Active Learning-based Input Selection for Fuzzing Deep Learning
    Libraries. *arXiv preprint arXiv:2212.04038* (2022).'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ken ([n. d.]) K Ken. [n. d.]. *Exclusive: surveillance footage of tesla crash
    on sf’s bay bridge hours after elon musk announces “self-driving” feature*. [https://theintercept.com/2023/01/10/tesla-crash-footage-autopilot/](https://theintercept.com/2023/01/10/tesla-crash-footage-autopilot/)'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ketkar and Ketkar (2017) Nikhil Ketkar and Nikhil Ketkar. 2017. Introduction
    to keras. *Deep learning with python: a hands-on introduction* (2017), 97–111.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2021) Misoo Kim, Youngkyoung Kim, and Eunseok Lee. 2021. Denchmark:
    A bug benchmark of deep learning-related software. In *2021 IEEE/ACM 18th International
    Conference on Mining Software Repositories (MSR)*. IEEE, 540–544.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kundu et al. (2021) Shamik Kundu, Suvadeep Banerjee, Arnab Raha, Suriyaprakash
    Natarajan, and Kanad Basu. 2021. Toward functional safety of systolic array-based
    deep learning hardware accelerators. *IEEE Transactions on Very Large Scale Integration
    (VLSI) Systems* 29, 3 (2021), 485–498.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kundu et al. (2023) Shamik Kundu, Suvadeep Banerjee, Arnab Raha, Fei Su, Suriyaprakash
    Natarajan, and Kanad Basu. 2023. Trouble-shooting at GAN Point: Improving Functional
    Safety in Deep Learning Accelerators. *IEEE Trans. Comput.* (2023).'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient
    Memory Management for Large Language Model Serving with PagedAttention. In *Proceedings
    of the ACM SIGOPS 29th Symposium on Operating Systems Principles*.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levental and Orlova (2020) Maksim Levental and Elena Orlova. 2020. Comparing
    the costs of abstraction for dl frameworks. *arXiv preprint arXiv:2012.07163*
    (2020).
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li (2018) Hang Li. 2018. Deep learning for natural language processing: advantages
    and challenges. *National Science Review* 5, 1 (2018), 24–26.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022a) Junqiang Li, Senyi Li, Jiawei Wu, Long Luo, Yang Bai, and
    Hongfang Yu. 2022a. MMOS: Multi-Staged Mutation Operator Scheduling for Deep Learning
    Library Testing. In *GLOBECOM 2022-2022 IEEE Global Communications Conference*.
    IEEE, 6103–6108.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Meiziniu Li, Jialun Cao, Yongqiang Tian, Tsz On Li, Ming
    Wen, and Shing-Chi Cheung. 2023b. Comet: Coverage-guided model generation for
    deep learning library testing. *ACM Transactions on Software Engineering and Methodology*
    32, 5 (2023), 1–34.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong
    Yang, Zhongzhi Luan, Lin Gan, Guangwen Yang, and Depei Qian. 2020. The deep learning
    compiler: A comprehensive survey. *IEEE Transactions on Parallel and Distributed
    Systems* 32, 3 (2020), 708–727.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    et al. 2023a. StarCoder: may the source be with you! *arXiv preprint arXiv:2305.06161*
    (2023).'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022b) Xiaoting Li, Xiao Liu, Lingwei Chen, Rupesh Prajapati, and
    Dinghao Wu. 2022b. ALPHAPROG: reinforcement generation of valid programs for compiler
    fuzzing. In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 36.
    12559–12565.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021) Yi Li, Shaohua Wang, and Tien Nguyen. 2021. Fault localization
    with code coverage representation learning. In *2021 IEEE/ACM 43rd International
    Conference on Software Engineering (ICSE)*. IEEE, 661–673.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2018) Jie Liang, Mingzhe Wang, Yuanliang Chen, Yu Jiang, and
    Renwei Zhang. 2018. Fuzz testing in practice: Obstacles and solutions. In *2018
    IEEE 25th International Conference on Software Analysis, Evolution and Reengineering
    (SANER)*. IEEE, 562–566.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lidbury et al. (2015) Christopher Lidbury, Andrei Lascu, Nathan Chong, and Alastair F
    Donaldson. 2015. Many-core compiler fuzzing. *ACM SIGPLAN Notices* 50, 6 (2015),
    65–76.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2012) Bingchang Liu, Liang Shi, Zhuhua Cai, and Min Li. 2012. Software
    vulnerability discovery techniques: A survey. In *2012 fourth international conference
    on multimedia information networking and security*. IEEE, 152–156.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022a) Haiyi Liu, Shaoying Liu, Chenglong Wen, and W Eric Wong.
    2022a. TBEM: Testing-Based GPU-Memory Consumption Estimation for Deep Learning.
    *IEEE Access* 10 (2022), 39674–39680.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Jiawei Liu, Jinkun Lin, Fabian Ruffy, Cheng Tan, Jinyang
    Li, Aurojit Panda, and Lingming Zhang. 2023a. Nnsmith: Generating diverse and
    valid test cases for deep learning compilers. In *Proceedings of the 28th ACM
    International Conference on Architectural Support for Programming Languages and
    Operating Systems, Volume 2*. 530–543.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Jiawei Liu, Jinjun Peng, Yuyao Wang, and Lingming Zhang.
    2023b. Neuri: Diversifying dnn generation via inductive rule inference. *arXiv
    preprint arXiv:2302.02261* (2023).'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022b) Jiawei Liu, Yuxiang Wei, Sen Yang, Yinlin Deng, and Lingming
    Zhang. 2022b. Coverage-guided tensor compiler fuzzing with joint ir-pass mutation.
    *Proceedings of the ACM on Programming Languages* 6, OOPSLA1 (2022), 1–26.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018) Ling Liu, Yanzhao Wu, Wenqi Wei, Wenqi Cao, Semih Sahin,
    and Qi Zhang. 2018. Benchmarking deep learning frameworks: Design considerations,
    metrics and beyond. In *2018 IEEE 38th International Conference on Distributed
    Computing Systems (ICDCS)*. IEEE, 1258–1269.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022c) Zhihao Liu, Yang Zheng, Xiaoting Du, Zheng Hu, Wenjie Ding,
    Yanming Miao, and Zheng Zheng. 2022c. Taxonomy of Aging-related Bugs in Deep Learning
    Libraries. In *2022 IEEE 33rd International Symposium on Software Reliability
    Engineering (ISSRE)*. IEEE, 423–434.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2023b) Haoyang Ma, Qingchao Shen, Yongqiang Tian, Junjie Chen, and
    Shing-Chi Cheung. 2023b. Fuzzing Deep Learning Compilers with HirGen. In *Proceedings
    of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis*.
    248–260.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2023a) Xiangyue Ma, Xiaoting Du, Qing Cai, Yang Zheng, Jing Hu, and
    Zheng Zheng. 2023a. A Survey on Testing of Deep Learning Frameworks (in Chinese).
    *Journal of Software* (2023).
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manès et al. (2019) Valentin JM Manès, HyungSeok Han, Choongwoo Han, Sang Kil
    Cha, Manuel Egele, Edward J Schwartz, and Maverick Woo. 2019. The art, science,
    and engineering of fuzzing: A survey. *IEEE Transactions on Software Engineering*
    47, 11 (2019), 2312–2331.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Martínez-Fernández et al. (2022) Silverio Martínez-Fernández, Justus Bogner,
    Xavier Franch, Marc Oriol, Julien Siebert, Adam Trendowicz, Anna Maria Vollmer,
    and Stefan Wagner. 2022. Software engineering for AI-based systems: a survey.
    *ACM Transactions on Software Engineering and Methodology (TOSEM)* 31, 2 (2022),
    1–59.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McKeeman (1998) William M McKeeman. 1998. Differential testing for software.
    *Digital Technical Journal* 10, 1 (1998), 100–107.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meng et al. (2022) Xiangxin Meng, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong
    Liu. 2022. Improving fault localization and program repair with deep semantic
    features and transferred knowledge. In *Proceedings of the 44th International
    Conference on Software Engineering*. 1169–1180.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft (2023) Microsoft. 2023. *ONNX Github repository*. [https://github.com/onnx/onnx](https://github.com/onnx/onnx)
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Murphy et al. (2009) Christian Murphy, Kuang Shen, and Gail Kaiser. 2009. Using
    JML runtime assertion checking to automate metamorphic testing in applications
    without test oracles. In *2009 International Conference on Software Testing Verification
    and Validation*. IEEE, 436–445.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nazir et al. (2023) Zhumakhan Nazir, Vladislav Yarovenko, and Jurn-Gyu Park.
    2023. Interpretable ML enhanced CNN Performance Analysis of cuBLAS, cuDNN and
    TensorRT. In *Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing*.
    1260–1265.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nijkamp et al. (2022) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open
    large language model for code with multi-turn program synthesis. *arXiv preprint
    arXiv:2203.13474* (2022).'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nistor et al. (2015) Adrian Nistor, Po-Chun Chang, Cosmin Radoi, and Shan Lu.
    2015. Caramel: Detecting and fixing performance problems that have non-intrusive
    fixes. In *2015 IEEE/ACM 37th IEEE International Conference on Software Engineering*,
    Vol. 1\. IEEE, 902–912.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oehlert (2005) Peter Oehlert. 2005. Violating assumptions with fuzzing. *IEEE
    Security & Privacy* 3, 2 (2005), 58–62.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: of Science and Policy ([n. d.]) Office of Science and Technology Policy. [n. d.].
    *National Artificial Intelligence Initiative Act of 2020*. [https://www.ai.gov/wp-content/uploads/2023/04/National-Artificial-Intelligence-Initiative-Act-of-2020.pdf](https://www.ai.gov/wp-content/uploads/2023/04/National-Artificial-Intelligence-Initiative-Act-of-2020.pdf)
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning
    library. *Advances in neural information processing systems* 32 (2019).'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patterson et al. (2021) David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang,
    Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.
    2021. Carbon emissions and large neural network training. *arXiv preprint arXiv:2104.10350*
    (2021).
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pham et al. (2019) Hung Viet Pham, Thibaud Lutellier, Weizhen Qi, and Lin Tan.
    2019. CRADLE: cross-backend validation to detect and localize bugs in deep learning
    libraries. In *2019 IEEE/ACM 41st International Conference on Software Engineering
    (ICSE)*. IEEE, 1027–1038.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prochnow and Yang (2022) Alexander Prochnow and Jinqiu Yang. 2022. DiffWatch:
    watch out for the evolving differential testing in deep learning libraries. In
    *Proceedings of the ACM/IEEE 44th International Conference on Software Engineering:
    Companion Proceedings*. 46–50.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quan et al. (2022) Lili Quan, Qianyu Guo, Xiaofei Xie, Sen Chen, Xiaohong Li,
    and Yang Liu. 2022. Towards understanding the faults of javascript-based deep
    learning systems. In *Proceedings of the 37th IEEE/ACM International Conference
    on Automated Software Engineering*. 1–13.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
    Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning
    models with over 100 billion parameters. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 3505–3506.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2023) Luyao Ren, ZiHeng Wang, Yingfei Xiong, Li Zhang, Guoyue Jiang,
    and Tao Xie. 2023. Effective Random Test Generation for Deep Learning Compilers.
    *arXiv preprint arXiv:2302.00842* (2023).
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rotem et al. (2018) Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret Catron,
    Summer Deng, Roman Dzhabarov, Nick Gibson, James Hegeman, Meghan Lele, Roman Levenstein,
    et al. 2018. Glow: Graph lowering compiler techniques for neural networks. *arXiv
    preprint arXiv:1805.00907* (2018).'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schumi and Sun (2022) Richard Schumi and Jun Sun. 2022. ExAIS: executable AI
    semantics. In *Proceedings of the 44th International Conference on Software Engineering*.
    859–870.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segura et al. (2016) Sergio Segura, Gordon Fraser, Ana B Sanchez, and Antonio
    Ruiz-Cortés. 2016. A survey on metamorphic testing. *IEEE Transactions on software
    engineering* 42, 9 (2016), 805–824.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2021) Qingchao Shen, Haoyang Ma, Junjie Chen, Yongqiang Tian, Shing-Chi
    Cheung, and Xiang Chen. 2021. A comprehensive study of deep learning compiler
    bugs. In *Proceedings of the 29th ACM Joint meeting on european software engineering
    conference and symposium on the foundations of software engineering*. 968–980.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2023) Jingyi Shi, Yang Xiao, Yuekang Li, Yeting Li, Dongsong Yu,
    Chendong Yu, Hui Su, Yufeng Chen, and Wei Huo. 2023. ACETest: Automated Constraint
    Extraction for Testing Deep Learning Operators. *arXiv preprint arXiv:2305.17914*
    (2023).'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: staff ([n. d.]) ABC7.com staff. [n. d.]. *Uber gives up testing of self-driving
    cars in California in wake of fatal Arizona crash*. [https://abc7.com/self-driving-uber-crash-video-pedestrian-hit-by-car-autonomous-vehicles/3269690/](https://abc7.com/self-driving-uber-crash-video-pedestrian-hit-by-car-autonomous-vehicles/3269690/)
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2022) Bing Sun, Jun Sun, Long H Pham, and Jie Shi. 2022. Causality-based
    neural network repair. In *Proceedings of the 44th International Conference on
    Software Engineering*. 338–349.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun (2020) Y Sun. 2020. *Tesla and PyTorch: PyTorch Developer Conference Highlights*.
    [https://medium.com/data-science-bootcamp/tesla-and-pytorch-pytorch-developer-conference-highlights-part-3ed36f2c9d5e](https://medium.com/data-science-bootcamp/tesla-and-pytorch-pytorch-developer-conference-highlights-part-3ed36f2c9d5e)'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tao et al. (2010) Qiuming Tao, Wei Wu, Chen Zhao, and Wuwei Shen. 2010. An automatic
    testing approach for compiler based on metamorphic testing technique. In *2010
    Asia Pacific Software Engineering Conference*. IEEE, 270–279.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow (2020) TensorFlow. 2020. *Learn how TensorFlow solves real, everyday
    machine learning problems*. [https://www.tensorflow.org/about/case-studies](https://www.tensorflow.org/about/case-studies)
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Udeshi et al. (2018) Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay.
    2018. Automated directed fairness testing. In *Proceedings of the 33rd ACM/IEEE
    International Conference on Automated Software Engineering*. 98–108.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uezono et al. (2022) Takumi Uezono, Yi He, and Yanjing Li. 2022. Achieving automotive
    safety requirements through functional in-field self-test for deep learning accelerators.
    In *2022 IEEE International Test Conference (ITC)*. IEEE, 465–473.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020a) Chaojin Wang, Jian Shen, Chunrong Fang, Xiangsheng Guan,
    Kaitao Wu, and Jiang Wang. 2020a. Accuracy measurement of deep neural network
    accelerator via metamorphic testing. In *2020 IEEE International Conference On
    Artificial Intelligence Testing (AITest)*. IEEE, 55–61.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Jiannan Wang, Thibaud Lutellier, Shangshu Qian, Hung Viet
    Pham, and Lin Tan. 2022. EAGLE: creating equivalent graphs to test deep learning
    libraries. In *Proceedings of the 44th International Conference on Software Engineering*.
    798–810.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Jiyuan Wang, Qian Zhang, Guoqing Harry Xu, and Miryung Kim.
    2021. Qdiff: Differential testing of quantum software stacks. In *2021 36th IEEE/ACM
    International Conference on Automated Software Engineering (ASE)*. IEEE, 692–704.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020b) Zan Wang, Ming Yan, Junjie Chen, Shuang Liu, and Dongdi
    Zhang. 2020b. Deep learning library testing via effective model generation. In
    *Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference
    and Symposium on the Foundations of Software Engineering*. 788–799.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2022) Anjiang Wei, Yinlin Deng, Chenyuan Yang, and Lingming Zhang.
    2022. Free lunch for testing: Fuzzing deep-learning libraries from open source.
    In *Proceedings of the 44th International Conference on Software Engineering*.
    995–1007.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language Processing.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations*. Association for Computational Linguistics,
    Online, 38–45. [https://www.aclweb.org/anthology/2020.emnlp-demos.6](https://www.aclweb.org/anthology/2020.emnlp-demos.6)'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Mingyuan Wu, Minghai Lu, Heming Cui, Junjie Chen, Yuqun Zhang,
    and Lingming Zhang. 2023. Jitfuzz: Coverage-guided fuzzing for jvm just-in-time
    compilers. In *2023 IEEE/ACM 45th International Conference on Software Engineering
    (ICSE)*. IEEE, 56–68.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2022) Dongwei Xiao, Zhibo Liu, Yuanyuan Yuan, Qi Pang, and Shuai
    Wang. 2022. Metamorphic testing of deep learning compilers. *Proceedings of the
    ACM on Measurement and Analysis of Computing Systems* 6, 1 (2022), 1–28.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2022) Danning Xie, Yitong Li, Mijung Kim, Hung Viet Pham, Lin Tan,
    Xiangyu Zhang, and Michael W Godfrey. 2022. DocTer: documentation-guided fuzzing
    for testing deep learning API functions. In *Proceedings of the 31st ACM SIGSOFT
    International Symposium on Software Testing and Analysis*. 176–188.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2009) Xiaoyuan Xie, Joshua Ho, Christian Murphy, Gail Kaiser, Baowen
    Xu, and Tsong Yueh Chen. 2009. Application of metamorphic testing to supervised
    classifiers. In *2009 Ninth International Conference on Quality Software*. IEEE,
    135–144.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2011) Xiaoyuan Xie, Joshua WK Ho, Christian Murphy, Gail Kaiser,
    Baowen Xu, and Tsong Yueh Chen. 2011. Testing and validating machine learning
    classifiers by metamorphic testing. *Journal of Systems and Software* 84, 4 (2011),
    544–558.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023) Chenyuan Yang, Yinlin Deng, Jiayi Yao, Yuxing Tu, Hanchi
    Li, and Lingming Zhang. 2023. Fuzzing automatic differentiation in deep-learning
    libraries. *arXiv preprint arXiv:2302.04351* (2023).
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2011) Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011.
    Finding and understanding bugs in C compilers. In *Proceedings of the 32nd ACM
    SIGPLAN conference on Programming language design and implementation*. 283–294.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022) Yilin Yang, Tianxing He, Zhilong Xia, and Yang Feng. 2022.
    A comprehensive empirical study on bug characteristics of deep learning frameworks.
    *Information and Software Technology* 151 (2022), 107004.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020a) Jie M Zhang, Mark Harman, Lei Ma, and Yang Liu. 2020a.
    Machine learning testing: Survey, landscapes and horizons. *IEEE Transactions
    on Software Engineering* 48, 1 (2020), 1–36.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020b) Ru Zhang, Wencong Xiao, Hongyu Zhang, Yu Liu, Haoxiang
    Lin, and Mao Yang. 2020b. An empirical study on program failures of deep learning
    jobs. In *Proceedings of the ACM/IEEE 42nd International Conference on Software
    Engineering*. 1159–1170.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021a) Xufan Zhang, Jiawei Liu, Ning Sun, Chunrong Fang, Jia
    Liu, Jiang Wang, Dong Chai, and Zhenyu Chen. 2021a. Duo: Differential fuzzing
    for deep learning operators. *IEEE Transactions on Reliability* 70, 4 (2021),
    1671–1685.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022) Xiaoyu Zhang, Chao Shen, Chenhao Lin, Qian Li, Qian Wang,
    Qi Li, and Xiaohong Guan. 2022. The Testing and Repairing Methods for Machine
    Learning Model Security. *ACTA ELECTONICA SINICA* 50, 12 (2022), 2884.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021b) Xufan Zhang, Ning Sun, Chunrong Fang, Jiawei Liu, Jia
    Liu, Dong Chai, Jiang Wang, and Zhenyu Chen. 2021b. Predoo: precision testing
    of deep learning operators. In *Proceedings of the 30th ACM SIGSOFT International
    Symposium on Software Testing and Analysis*. 400–412.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019) Xufan Zhang, Yilin Yang, Yang Feng, and Zhenyu Chen. 2019.
    Software engineering practice in the development of deep learning applications.
    *arXiv preprint arXiv:1910.03156* (2019).
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021d) Xiaoyu Zhang, Juan Zhai, Shiqing Ma, and Chao Shen. 2021d.
    Autotrainer: An automatic dnn training problem detection and repair system. In
    *2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)*.
    IEEE, 359–371.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Yuhao Zhang, Yifan Chen, Shing-Chi Cheung, Yingfei Xiong,
    and Lu Zhang. 2018. An empirical study on TensorFlow program bugs. In *Proceedings
    of the 27th ACM SIGSOFT international symposium on software testing and analysis*.
    129–140.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021c) Zhiyi Zhang, Pu Wang, Hongjing Guo, Ziyuan Wang, Yuqian
    Zhou, and Zhiqiu Huang. 2021c. Deepbackground: Metamorphic testing for deep-learning-driven
    image recognition systems accompanied by background-relevance. *Information and
    Software Technology* 140 (2021), 106701.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong (2022) Hao Zhong. 2022. Enriching compiler testing with real program from
    bug report. In *Proceedings of the 37th IEEE/ACM International Conference on Automated
    Software Engineering*. 1–12.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2023) Yinglong Zou, Haofeng Sun, Chunrong Fang, Jiawei Liu, and
    Zhenping Zhang. 2023. Deep learning framework testing via hierarchical and heuristic
    model generation. *Journal of Systems and Software* 201 (2023), 111681.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
