- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:33:05'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2404.17871] A Survey of Deep Learning Library Testing Methods'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.17871](https://ar5iv.labs.arxiv.org/html/2404.17871)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Deep Learning Library Testing Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Xiaoyu Zhang [zxy0927@stu.xjtu.edu.cn](mailto:zxy0927@stu.xjtu.edu.cn) School
    of Cyber Science and Engineering, Xi’an Jiaotong UniversityP.O. Box 1212Xi’anChina
    ,  Weipeng Jiang [lenijwp@stu.xjtu.edu.cn](mailto:lenijwp@stu.xjtu.edu.cn) School
    of Cyber Science and Engineering, Xi’an Jiaotong UniversityXi’anChina ,  Chao
    Shen [chaoshen@mail.xjtu.edu.cn](mailto:chaoshen@mail.xjtu.edu.cn) School of Cyber
    Science and Engineering, Xi’an Jiaotong UniversityXi’anChina ,  Qi Li [qli01@tsinghua.edu.cn](mailto:qli01@tsinghua.edu.cn)
    Institute for Network Sciences and Cyberspace, Tsinghua UniversityBeijingChina
    ,  Qian Wang [qianwang@whu.edu.cn](mailto:qianwang@whu.edu.cn) School of Cyber
    Science and Engineering, Wuhan UniversityWuhanChina ,  Chenhao Lin [linchenhao@xjtu.edu.cn](mailto:linchenhao@xjtu.edu.cn)
    School of Cyber Science and Engineering, Xi’an Jiaotong UniversityXi’anChina  and 
    Xiaohong Guan [xhguan@mail.xjtu.edu.cn](mailto:xhguan@mail.xjtu.edu.cn) School
    of Cyber Science and Engineering, Xi’an Jiaotong UniversityXi’anChina
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In recent years, software systems powered by deep learning (DL) techniques have
    significantly facilitated people’s lives in many aspects. As the backbone of these
    DL systems, various DL libraries undertake the underlying optimization and computation.
    However, like traditional software, DL libraries are not immune to bugs, which
    can pose serious threats to users’ personal property and safety. Studying the
    characteristics of DL libraries, their associated bugs, and the corresponding
    testing methods is crucial for enhancing the security of DL systems and advancing
    the widespread application of DL technology. This paper provides an overview of
    the testing research related to various DL libraries, discusses the strengths
    and weaknesses of existing methods, and provides guidance and reference for the
    application of the DL library. This paper first introduces the workflow of DL
    underlying libraries and the characteristics of three kinds of DL libraries involved,
    namely DL framework, DL compiler, and DL hardware library. It then provides definitions
    for DL underlying library bugs and testing. Additionally, this paper summarizes
    the existing testing methods and tools tailored to these DL libraries separately
    and analyzes their effectiveness and limitations. It also discusses the existing
    challenges of DL library testing and outlines potential directions for future
    research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning Testing, Deep Learning Library Testing, Deep Learning Security,
    Deep Learning, Software Testing^†^†copyright: none^†^†ccs: Security and privacy Software
    security engineering^†^†ccs: Security and privacy Systems security'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the development of deep learning (DL) techniques, the DL systems that are
    driven by DL models have been applied in many fields, providing societal benefits
    in areas like image recognition (He et al., [2016](#bib.bib38)), self-driving (Grigorescu
    et al., [2020](#bib.bib31)), and natural language processing (Li, [2018](#bib.bib58)).
    With the wide application of DL systems in various fields, the security and safety
    of the underlying DL library have received more and more attention. As the backbone
    of DL systems, the DL library (e.g., PyTorch) is responsible for performing specific
    computations for training or inference DL models and implementing optimized operations
    on DL hardware. The DL library is important for DL development and systems. Tesla
    relies on PyTorch which is one of the most popular DL libraries to solve problems
    related to the self-driving domain (Sun, [2020](#bib.bib101)). TensorFlow, which
    is another popular DL library, undertakes many important business tasks of Google,
    Intel and other companies (TensorFlow, [2020](#bib.bib103)).
  prefs: []
  type: TYPE_NORMAL
- en: Similar to traditional software, the DL library also has bugs, which can cause
    the DL systems it supports to make erroneous predictions, generate huge overhead,
    and even crash (Pham et al., [2019](#bib.bib89); Wei et al., [2022](#bib.bib110)),
    thereby jeopardizing user property and personal safety. For example, in recent
    years, the self-driving systems developed by Tesla and Uber have experienced abnormal
    behaviors during driving and eventually led to fatal crashes (Ken, [[n. d.]](#bib.bib51);
    staff, [[n. d.]](#bib.bib99)), which further arouse people’s concerns about the
    vulnerabilities of DL systems and the underlying libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6d73a73f6f59bed907b2775cd47c597c.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 1\. Overview of This Paper
  prefs: []
  type: TYPE_NORMAL
- en: At present, researchers have proposed a series of tools and methods (Pham et al.,
    [2019](#bib.bib89); Wei et al., [2022](#bib.bib110); Deng et al., [2022](#bib.bib23))
    to discover vulnerabilities and bugs such as crashes, overflows, and numerical
    errors on the DL libraries represented by DL frameworks (e.g., TensorFlow, PyTorch),
    aiming to guarantee the security and usability of the DL system it supports. In
    addition, many countries have established relevant acts and policies, proposing
    that technical means should be used to test and verify DL systems and software.
    For example, the National AI Initiative Act of 2020 introduced by the United States
    proposed to support research on the security of ‘software and hardware used in
    artificial intelligence systems’ (of Science and Policy, [[n. d.]](#bib.bib86)).
    In 2023, the European Union proposed the AI act that called to organize testing
    of the high-risk AI system through technical means (Commission, [2021](#bib.bib19)).
  prefs: []
  type: TYPE_NORMAL
- en: How to deeply understand the vulnerabilities and bugs of the underlying libraries
    of the DL system and design testing methods for these libraries needs to be solved
    urgently and is of great significance. Although researchers have proposed various
    DL library testing methods, there are still many challenges. Firstly, there are
    various types of DL libraries, including DL frameworks, DL compilers, etc. Different
    DL libraries undertake different calculation and optimization functions, and there
    are significant differences between their inputs, outputs and implementation.
    As a result, existing testing methods are diverse and highly targeted to specific
    libraries, leading to a lack of general and systematic testing methods for different
    DL libraries and evaluation metrics for different test methods. Furthermore, existing
    research has a limited understanding of DL library bugs and mainly focuses on
    crashes and numerical errors. They lack the ability to comprehensively evaluate
    other bugs in DL libraries (e.g., performance bugs), which limits the effectiveness
    of these methods. Therefore, it is significant to conduct induction, analysis,
    and discussion on the existing research in the field of DL library testing to
    find limitations and provide guidance for subsequent research directions in related
    fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, existing surveys on the DL library are limited. Researchers either
    focused on some parts of the DL underlying library, for example, Zhang et al. (Zhang
    et al., [2020a](#bib.bib120)) and Ma et al. (Ma et al., [2023a](#bib.bib75)) focused
    on testing methods on DL models and frameworks and Li et al. (Li et al., [2020](#bib.bib61))
    paid attention to DL compilers, or investigated the vulnerabilities in DL software
    and model from a macroscopic perspective, but could not provide a fine-grained
    introduction and analysis to the library testing methods (Braiek and Khomh, [2020](#bib.bib5);
    Gezici and Tarhan, [2022](#bib.bib30); Martínez-Fernández et al., [2022](#bib.bib77);
    Zhang et al., [2022](#bib.bib123)). There is a lack of comprehensive and detailed
    research on the testing methods on DL underlying libraries and their bugs. To
    fill the gap, this paper systematically summarizes testing methods on three kinds
    of DL libraries, namely, DL framework, DL compiler, and DL hardware library, and
    analyzes their strengths and weaknesses. Building upon this foundation, this paper
    delves into the challenges and future research opportunities in DL library testing.
    Our goal is to promote the further development of DL library testing research,
    so as to ensure the safety and security of DL systems and accelerate their application.
    The main contribution of this work can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present the first comprehensive and detailed survey of testing techniques
    for DL libraries, filling the gap of existing DL testing survey work that focuses
    heavily on DL models and lacks insight into the underlying bugs on DL libraries.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a novel taxonomy in three test components to provide an accessible
    overview of works that focus on libraries at different stages in the DL workflow,
    namely DL framework, DL compiler, and DL hardware library, as shown in [Fig. 1](#S1.F1
    "Fig. 1 ‣ 1\. Introduction ‣ A Survey of Deep Learning Library Testing Methods").
    For each stage, we systematically summarize and present existing work according
    to their testing techniques and provide an in-depth analysis at the end to characterize
    some critical problems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discuss and outline the main challenges that need to be addressed and future
    research directions, aiming to promote the development of DL software security
    and safety.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The overarching structure of this paper is shown in [Fig. 1](#S1.F1 "Fig. 1
    ‣ 1\. Introduction ‣ A Survey of Deep Learning Library Testing Methods"). We begin
    by building fundamental definitions of the DL library bug and testing. Considering
    the underlying workflow of DL systems and programs, we divide the test objects
    of existing testing methods into three components, namely DL framework, DL compiler,
    and DL hardware library (marked in blue, green, and orange). Based on these three
    test components, we systematically summarize and analyze existing testing methods
    on DL library testing, according to different test techniques (which mainly include
    differential testing, fuzz testing, and metamorphic testing). Furthermore, we
    observe that existing research mainly focuses on two testing properties of DL
    libraries, namely correctness and efficiency. Based on our observation, we have
    analyzed bugs detected by representative methods, aiming to study the advantages
    and limitations of these methods. Finally, we provide a future-gazing on the challenges
    and possible research directions in DL library testing.
  prefs: []
  type: TYPE_NORMAL
- en: More detailed, the paper is organized as follows. [§ 2](#S2 "2\. Preliminary
    ‣ A Survey of Deep Learning Library Testing Methods") describes the workflow of
    the DL underlying libraries, which lead to three test components, and other preliminary
    knowledge. The [§ 3](#S3 "3\. DL Framework Testing ‣ A Survey of Deep Learning
    Library Testing Methods"), [§ 4](#S4 "4\. DL Compiler Testing ‣ A Survey of Deep
    Learning Library Testing Methods") and [§ 5](#S5 "5\. DL Hardware Library Testing
    ‣ A Survey of Deep Learning Library Testing Methods") respectively introduce the
    DL library testing research on the DL framework, DL compiler and DL hardware library,
    and discuss the advantages and limitations of these methods. Then [§ 6](#S6 "6\.
    Future Gazing ‣ A Survey of Deep Learning Library Testing Methods") analyzes the
    current challenges and future research opportunities in the field of DL library
    testing, and [§ 7](#S7 "7\. Conclusion ‣ A Survey of Deep Learning Library Testing
    Methods") concludes this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Preliminary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. DL Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A DL model is a parameterized function $F_{\theta}:X\mapsto Y$, where $x\in
    X$ is an $m-$dimensional input and $y\in Y$ is the corresponding output label.
    Typically, a DL model is composed of several connected layers, and an $n$-layered
    model can be represented as $F_{\theta}=l_{1}\circ l_{2}\circ\cdot\cdot\cdot\circ
    l_{n}$, where $l$ represents a layer and $\theta$ is the model weight. The developers
    first need to train the DL model on the given data and update the model weight
    $\theta$ in the training progress. Then, in the inference process, the trained
    DL model can predict the output for the given input (e.g., an image, or a sentence).
  prefs: []
  type: TYPE_NORMAL
- en: The training process of a DL model consists of the forward propagation stage
    and the backward propagation stage, and the model inference process only uses
    the former. The forward propagation stage calculates model output $F_{\theta}(x_{i})$
    based on the input tensor $x_{i}$ and initialized model weight $\theta$. The backward
    propagation stage evaluates the difference between $F_{\theta}(x_{i})$ and the
    ground truth label $y_{i}$ by a loss function $\mathcal{L}(F_{\theta}(x),y)$ and
    updates model weights $\theta$ to minimize the value of $\mathcal{L}$. The forward
    propagation and backward propagation stages will be repeated until the training
    reaches the predetermined stopping criteria. In the DL program, developers call
    the APIs provided by the DL framework to build and train a DL model and each layer
    $l_{i}$ in the model can be directly constructed by one or several DL framework
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. DL Library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Various DL libraries implement the abstract DL model based on the program and
    perform specific operations and optimizations on the underlying hardware to obtain
    computation results for inference and training of DL models. Executing the DL
    program and building a DL model on the DL underlying libraries mainly involves
    three components, namely DL framework, DL compiler, and DL hardware library. [Fig. 2](#S2.F2
    "Fig. 2 ‣ 2.2\. DL Library ‣ 2\. Preliminary ‣ A Survey of Deep Learning Library
    Testing Methods") shows the overarching workflow. The code in dotted boxes presents
    the demo inputs of each component. Developers first need to call the DL framework
    APIs to construct a DL program. The DL framework executes the DL program and constructs
    the corresponding DL model. Then the abstract model will be passed to the DL compiler,
    and the DL compiler translates the input model into intermediate representations
    (IRs), optimizes it according to the target hardware, and outputs optimized operators
    and code. Finally, the DL hardware library (e.g., cuDNN) accepts the output of
    the compiler and maps calculations to the DL hardware to perform calculations
    and obtain the results. In this paper, we focus on the above three kinds of DL
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24bf02a1c06c145892c6866af51d8369.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2\. Overarching Workflow of the DL system and DL program
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1\. DL Framework
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To allow developers to design and build DL models conveniently and simplify
    the implementation of models, industry and academia have proposed various DL frameworks,
    including TensorFlow (Abadi, [2016](#bib.bib4)), Pytorch (Paszke et al., [2019](#bib.bib87)),
    ONNX (Microsoft, [2023](#bib.bib80)), etc. TensorFlow supports a variety of program
    languages (e.g., C++, Python, and Go) and is currently one of the most popular
    DL frameworks. TensorFlow Lite is designed to implement and optimize DL techniques
    for mobile and embedded scenarios. PyTorch is rewritten and optimized based on
    the DL framework Torch. Nowadays, PyTorch and TensorFlow have active developer
    communities and are the most commonly used test objects in DL framework testing
    research. Open Neural Network Exchange (ONNX) defines an extensible computation
    graph model and provides an open-sourced format for DL models, which allows models
    of different DL frameworks can be easily transformed into ONNX and facilitates
    model conversion between frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Like traditional software, DL frameworks provide numerous APIs to call functions
    and perform operations. Taking PyTorch (Paszke et al., [2019](#bib.bib87)) as
    an example, its API includes performing basic matrix operations (e.g., torch.mul
    for multiply operation), calculating loss functions (e.g., torch.nn.MSELoss for
    measuring mean squared error), and building models layers (e.g., torch.nn.Conv2d
    for convolution layers). After the developer calls these APIs in the program,
    the DL framework will build the corresponding abstract DL model.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2\. DL Compiler
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To reduce the burden of manually optimizing DL models on various DL hardware
    (e.g., TPU) and hardware libraries (e.g., cuDNN), researchers have developed the
    DL compiler (Li et al., [2020](#bib.bib61)). It takes the abstract model described
    by the DL framework as input, and then automatically optimizes and generates operators
    and codes as output to ensure that the DL hardware library can efficiently execute
    calculations of the DL model. Therefore, DL compilers are generally closely related
    to and work together with the DL framework that builds abstract DL models. The
    currently popular DL compilers include Glow (Rotem et al., [2018](#bib.bib94)),
    TVM (Chen et al., [2018](#bib.bib14)), etc. Glow is designed to implement state-of-the-art
    optimizations and generate code for neural network graphs. TVM provides graph-level
    and operator-level optimizations for DL models, whose optimized performance on
    some hardware is competitive with state-of-the-art hand-tuned libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to traditional compilers, DL compilers implement a layered design, which
    mainly consists of the compiler frontend and the compiler backend. The intermediate
    representation (IR), as the abstract of the program, exists in both the frontend
    and the backend (Li et al., [2020](#bib.bib61)). The front end converts the DL
    model from the DL framework into a computation graph and optimizes the graph with
    various methods (e.g., reducing redundancy). In this process, IR is mainly used
    to express DL models, construct the control flow and dependencies between operators
    and data, etc. For the computation graph, the backend performs hardware-specific
    optimizations by leveraging third-party tools and customizing compilation passes
    based on prior knowledge. Finally, DL compilers convert the DL model into operators
    and code which can be used to perform calculations on the DL hardware and hardware
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3\. DL Hardware Library
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Researchers have designed a variety of DL hardware, such as CPU, GPU, and TPU,
    to apply DL techniques in different scenarios. To adapt to given DL hardware and
    map the computation to DL hardware efficiently, researchers have developed a series
    of DL hardware libraries (e.g., cuDNN, cuBLAS) that implement optimized linear
    algebras, matrix multiplication, DL operators, etc. For example, cuDNN (Chetlur
    et al., [2014](#bib.bib17)) is a DL accelerate library developed by NVIDIA, which
    is used to achieve high-performance computing on its developed GPU (e.g., RTX3090).
    In addition, cuBLAS provides a basic linear algebra library for the computation
    on GPU. HiAI is a DL accelerator introduced by Huawei for the specialized neural
    processing unit (NPU) in mobile chips (e.g., the Kirin 970). It provides acceleration
    for 16-bit float models and 8-bit quantized models and supports typical DL frameworks
    such as TensorFlow and Caffe (Ignatov et al., [2018](#bib.bib43)).
  prefs: []
  type: TYPE_NORMAL
- en: These DL hardware-related libraries select appropriate algorithms for different
    hardware and deployment environments, thus they can realize specific calculations
    on DL hardware. They also call different operators according to the data type
    to speed up the operation of the model on the given data. For example, cuBLAS
    implements various matrix-matrix multiplication operators (e.g., cublasHgemm,
    cublasSgemm) to map the calculations of different data types to the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. DL Library Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.3.1\. Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DL library testing aims to discover bugs, vulnerabilities, and defects in the
    DL libraries. The DL library bug is essentially a kind of software bug. Referring
    to the prior research (Zhang et al., [2020a](#bib.bib120); IEE, [2010](#bib.bib2)),
    we define the behavior that the actual function of the DL library does not meet
    the requirements and specifications as a DL library bug.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2.1 (DL Library Bug).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A DL library bug refers to any imperfection or deficiency in a DL library that
    causes the actual function performed by the DL library to fail to meet the expected
    requirements or specifications.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the above definition of DL library bugs, we define DL library testing
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2.2 (DL Library Testing).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: DL library testing refers to any activity designed to discover and identify
    DL library bugs.
  prefs: []
  type: TYPE_NORMAL
- en: To elaborate on the definition of the DL library bug and testing in detail,
    in the following section, we explain the test objects of the DL library testing
    and the expected requirements in testing. We use testing components and testing
    properties to refer to the above two aspects. In addition, we also summarize and
    introduce some common DL library testing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2\. DL Library Testing Component
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DL library testing component points out the application and test objects of
    various DL library testing methods. [Fig. 2](#S2.F2 "Fig. 2 ‣ 2.2\. DL Library
    ‣ 2\. Preliminary ‣ A Survey of Deep Learning Library Testing Methods") shows
    the underlying workflow of a DL system and DL program. In the computation, the
    DL model mainly relies on three DL underlying libraries, namely the DL framework,
    DL compiler, and DL hardware library. The code written by developers is converted
    and optimized by these DL libraries before it can perform calculations, training
    and inference on the DL hardware. Considering that we have introduced these libraries
    in [§ 2.2](#S2.SS2 "2.2\. DL Library ‣ 2\. Preliminary ‣ A Survey of Deep Learning
    Library Testing Methods"), we mainly describe the characteristics of the testing
    research on the three kinds of DL libraries in follows.
  prefs: []
  type: TYPE_NORMAL
- en: DL framework testing aims to discover bugs in DL frameworks. The DL framework
    plays a vital role in the build and calculation of the DL model. Any imperfection
    or deficiency in the implementation of the DL framework can directly affect the
    DL model, which will further lead to erroneous results and even cause the program
    to crash. DL framework testing research has two characteristics. 1) DL framework
    testing research has the most testing methods and is the most well-developed field
    in DL library testing. Existing research (Islam et al., [2019](#bib.bib44); Yang
    et al., [2022](#bib.bib119); Chen et al., [2023a](#bib.bib10)) has investigated
    the characteristics and symptoms of bugs in DL frameworks (e.g., PyTorch and TensorFlow)
    through open-source communities (e.g., GitHub and Stack Overflow), and has gained
    a deep understanding of the root cause of bugs. On this basis, researchers proposed
    a variety of testing methods and tools based on differential testing, fuzz testing,
    etc. to discover and identify DL framework bugs (Pham et al., [2019](#bib.bib89);
    Deng et al., [2022](#bib.bib23)). These methods provide a reference and guide
    for testing other DL libraries. 2) DL framework bugs detected by existing methods
    cover most of the bug types, including status bugs (e.g., crash, segmentation
    fault), numerical bugs (e.g., inconsistent output, NaN value), and performance
    bugs (e.g., unexpected time overhead). Since the emergence of the first batch
    of DL framework testing research (e.g., CRADLE (Pham et al., [2019](#bib.bib89))),
    researchers have continuously improved the testing methods. Today, a DL framework
    tool can usually discover dozens and even hundreds of bugs, which range from simple
    inconsistencies and crashes to complex performance bugs and security vulnerabilities,
    which effectively promotes the development of DL framework security. In contrast,
    current tests on other DL libraries are still in the early stages of development
    and are different in detecting a large number and variety of bugs.
  prefs: []
  type: TYPE_NORMAL
- en: DL compiler testing aims to find DL compiler bugs that cause the DL compiler
    to generate incorrect code, resulting in unexpected model behaviors (Shen et al.,
    [2021](#bib.bib97)). Similar to the DL framework testing, the DL compiler testing
    detects and identifies those anomaly functions and behaviors from the compiler
    through various methods, such as fuzz testing and metamorphic testing (Liu et al.,
    [2022b](#bib.bib71); Xiao et al., [2022](#bib.bib113)), but it has two unique
    features. 1) DL compiler testing research focuses on three bug prone stages, namely
    model loading, high-level IR transformation, and low-level LR transformation stages,
    due to the special architecture of DL compilers. The former loads an abstract
    DL model and transforms it into a computation graph, and the latter two respectively
    implement optimizations on high-level and low-level IRs. Since specific conversion
    and transformation are involved, bugs are more likely to occur in the latter two
    stages (Shen et al., [2021](#bib.bib97)), therefore, the DL compiler testing methods
    often pay more attention to these two stages. However, DL framework testing methods
    mainly focus on
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a8cee3b3043269fb2e5f7481e271b594.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 3\. An Example Optimization Bug (Xiao et al., [2022](#bib.bib113)) on the
    Glow Compiler.
  prefs: []
  type: TYPE_NORMAL
- en: forward and backward propagation stages in the model training and inference
    process. 2) DL compiler testing discovers different types of bugs compared with
    DL framework testing. Existing DL compiler testing methods can detect optimization
    bugs, which can cause the DL compiler to output incorrect results or middle results (Shen
    et al., [2021](#bib.bib97)), resulting in semantic changes and inequality after
    the compilation process and ultimately incorrect calculation results.  [Fig. 3](#S2.F3
    "Fig. 3 ‣ 2.3.2\. DL Library Testing Component ‣ 2.3\. DL Library Testing ‣ 2\.
    Preliminary ‣ A Survey of Deep Learning Library Testing Methods") shows an optimization
    bug in the Glow compiler, which incorrectly deletes layers (marked in the grey
    box) during the Dead Code Elimination(DCE) optimization, resulting in optimized
    operators outputting unexpected results. However, existing DL framework testing
    rarely involves optimization bugs and mainly focuses on status, numerical, and
    performance bugs.
  prefs: []
  type: TYPE_NORMAL
- en: DL hardware library testing discovers bugs on the DL hardware-related libraries.
    DL hardware library bugs can cause the DL model to obtain wrong results and abnormal
    runtime overhead in the calculation, which is difficult to perceive when executing
    the DL program or training a DL model. Existing testing methods mainly focus on
    functional errors in the DL hardware library. They use metamorphic testing or
    test pattern generation methods to verify the correctness of DL hardware library
    implementation (Wang et al., [2020a](#bib.bib106); Uezono et al., [2022](#bib.bib105)).
    Since the DL hardware library is at the bottom of the entire DL program workflow,
    as shown in [Fig. 2](#S2.F2 "Fig. 2 ‣ 2.2\. DL Library ‣ 2\. Preliminary ‣ A Survey
    of Deep Learning Library Testing Methods"), it is difficult to generate valid
    test input on a large scale or construct test oracles during testing. Therefore,
    existing research on DL hardware libraries generally focuses on the study validation
    of the functionality of different libraries, while little work is done to detect
    real-world bugs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3\. DL Library Testing Property
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The testing property refers to what the DL library testing methods test. It
    defines where the implementation of the DL library should meet the requirements
    and expectations. Existing studies (Zhang et al., [2020a](#bib.bib120), [2022](#bib.bib123))
    on machine learning (ML) testing comprehensively study and summarize various ML
    test properties, including correctness, robustness, privacy, efficiency, fairness,
    etc. However, the existing DL library testing research mainly focuses on the basic
    functionality of the DL libraries, which pay little attention to security-related
    properties, like robustness and privacy. Therefore, we mainly discuss correctness
    and efficiency in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: Correctness measures the ability of a DL library to correctly perform its functions
    and complete a given task. Correctness plays a vital role in the application and
    deployment of DL systems, which ensures the usability and trustworthiness of DL
    libraries. When the correctness of the DL library is compromised, the intended
    function cannot be executed, which may cause three types of bugs, namely status
    bug, numerical bug and optimization bug (Chen et al., [2023a](#bib.bib10); Shen
    et al., [2021](#bib.bib97)), which could be further exploited to endanger the
    safety and security of the whole DL system. The status bug refers to the unexpected
    termination of valid inputs or illegal execution of invalid inputs on the DL library.
    It includes various crashes, segmentation faults, exceptions, etc. The numerical
    bug and optimization bug occur when the DL library has incorrect behavior on valid
    inputs but does not crash. At this time, the DL library will output wrong results
    and further affect subsequent calculations. The former mainly consists of inconsistent
    outputs (i.e., inconsistency between expected and actual result) and NaN outputs
    (i.e., Not A Number, which is caused by overflow in the backend). The latter happens
    when the DL library (especially the DL compiler) gets erroneous results or middle
    results in the optimization process, resulting in inequalities between before
    and after optimization. Existing testing methods mainly test and validate the
    correctness of DL frameworks and compilers (Pham et al., [2019](#bib.bib89); Deng
    et al., [2022](#bib.bib23); Xiao et al., [2022](#bib.bib113)).
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency evaluates the overhead of time, GPU memory, and other performance
    indicators of the DL library in executing a given task. It determines the cost
    of large-scale deployment of DL libraries, which is of great significance for
    DL systems from the perspectives of performance, economics, and the environment.
    The problem in DL library efficiency leads to the performance bug. This kind of
    bug not only severely compromises the usability of DL libraries, leading to less
    responsiveness and waste of computational resources, but also puts pressure on
    both execution costs and the environment and results in a high carbon footprint (Nistor
    et al., [2015](#bib.bib84); Jin et al., [2012](#bib.bib47); Patterson et al.,
    [2021](#bib.bib88)). However, due to the limitations of testing methods and test
    oracles, existing work has paid limited attention to such bugs (Guo et al., [2019](#bib.bib35);
    Levental and Orlova, [2020](#bib.bib57)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.4\. DL Library Testing Technique
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The testing technique determines how the DL library testing methods test. To
    effectively discover and identify bugs in DL libraries, how to generate test cases/inputs
    at scale and build test oracles are vital problems to be solved. Nowadays, researchers
    have referenced traditional software testing techniques to design a variety of
    DL library testing methods. Here we introduce the three most widely spread DL
    library testing techniques, namely differential testing, fuzz testing, and metamorphic
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: Differential testing is one of the most classic testing methods in the field
    of software engineering (SE), which usually processes the same input on two or
    more comparable implementations of a given software, uses the outputs between
    each other as the pseudo test oracle, and utilizes the difference between outputs
    to reveal potential bugs (McKeeman, [1998](#bib.bib78)). As a simple but effective
    pseudo test oracle, differential testing has not only achieved excellent results
    in traditional software testing and verification tasks (Groce et al., [2007](#bib.bib32);
    Wang et al., [2021](#bib.bib108)) but also shined in DL library testing (Pham
    et al., [2019](#bib.bib89); Deng et al., [2022](#bib.bib23)). Different implementations
    of the same operator in different DL libraries or on different devices greatly
    facilitate the application of differential testing in DL library testing. However,
    the differential testing technique has two limitations. Firstly, its testing effect
    is directly related to the design of test oracles. Simple test oracles (e.g.,
    merely comparing the behaviors of one operator between different devices) are
    difficult to detect complex performance bugs, which could limit the effectiveness
    of testing methods. Second, differential testing may introduce false positives
    (FPs) in tests. Researchers have found that DL libraries implement certain operations
    in different ways, which leads to large differences in results, but this is not
    a real bug (Guo et al., [2020](#bib.bib36)). How to design advanced test oracles
    and reduce the FP rate are challenges in DL library differential testing.
  prefs: []
  type: TYPE_NORMAL
- en: Fuzz testing is a classic software testing technique, which is widely used to
    automatically detect bugs such as crashes in various software and systems (Liang
    et al., [2018](#bib.bib65); Manès et al., [2019](#bib.bib76)). Fuzz testing typically
    generates a large number of test inputs and observes whether the target software
    or system fails when executing the test input (Liu et al., [2012](#bib.bib67)).
    Therefore, it is usually used to generate valid/invalid test cases in DL library
    testing. According to the test input generation methods, fuzzing can mainly be
    divided into generation-based fuzzing and mutation-based fuzzing (Oehlert, [2005](#bib.bib85)).
    The former generates test cases and inputs from scratch based on constraints or
    randomly, while the latter mainly mutates existing inputs to test more potential
    behaviors of DL libraries. Since fuzz testing can generate a large number of inputs
    and achieve high API or code coverage in tests, testing methods based on it can
    often achieve outstanding results in real-world bug detection (Xie et al., [2022](#bib.bib114);
    Deng et al., [2023a](#bib.bib21)). Nowadays, fuzz testing has been widely used
    in DL framework and compiler testing. However, fuzz testing cannot build test
    oracles by itself. It often works with other testing techniques that can build
    pseudo test oracles (e.g., differential testing) (Wei et al., [2022](#bib.bib110)),
    or directly observes whether the test case has unexpected behavior (e.g., crash) (Xie
    et al., [2022](#bib.bib114)) to identify bugs.
  prefs: []
  type: TYPE_NORMAL
- en: Metamorphic testing was proposed by Chen et al. (Chen et al., [1998](#bib.bib15))
    in 1998 to address the test oracle problem. Metamorphic testing constructs a series
    of metamorphic relations (MRs) that are from the necessary properties of the program
    under test. An MR describes the expected change in the outputs of a target program
    when the inputs are changed. Metamorphic testing can generate a series of test
    samples and judge whether the functionality of the program is as expected by comparing
    whether their results conform to the metamorphic relationship (Segura et al.,
    [2016](#bib.bib96)). Researchers have designed some DL library testing methods
    based on the metamorphic testing technique to verify the computation and optimization
    of DL libraries, and successfully detected real-world bugs in the DL framework
    and compiler (Wei et al., [2022](#bib.bib110); Xiao et al., [2022](#bib.bib113)).
    However, metamorphic testing is not a silver bullet for DL library testing. Since
    MR determines which part of the functionality of the target library will be verified
    in the test, the effect of metamorphic testing is greatly limited by the design
    of MRs. A well-designed MR can improve the possibility of finding real-world bugs
    in DL library testing and help discover complex optimization bugs e.g., memory
    allocation bugs (Xiao et al., [2022](#bib.bib113)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.4\. DL Library Testing vs. DL Model Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table 1\. DL Library Testing vs. DL Model Testing
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Test Input | Test Object | Test Property | Test Oracle |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DL Library &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Testing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Seceral lines of code to &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; call DL libray APIs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Program bugs in &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the DL library &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mainly correctness and efficiency &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of the DL library &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Difficult to construct test &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; oracles in bug detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DL Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Testing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image, text, or &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; matrix data &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Secure problems on &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DL model properties &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Correctness, robustness, interpretability, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; efficiency, etc. of the DL model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Metrics existing for testing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and detecting problems &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Prior work has organized and summarized the DL model testing methods (Zhang
    et al., [2022](#bib.bib123)). However, researchers still pay limited attention
    to testing on the DL underlying library. In fact, DL library testing and DL model
    testing have similarities. For example, they both rely on the behaviors and results
    of DL models to detect potential problems. However, there are still many differences
    between DL library testing and DL model testing. We discuss the difference between
    the two tests from the following perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: 1) Test Input. In DL model testing, the test input is usually the input data
    of a complete model under tests. The input data can be meaningful images or text,
    or randomly generated matrix data. The testing methods generate and mutate a series
    of input data to test different properties and discover vulnerabilities in the
    given model. However, in DL library testing, the test input is often a program.
    It can be several lines of code that construct a DL model in the DL framework
    or a concatenation of some simple library APIs, which perform simple mathematical
    operations (e.g., addition, division).
  prefs: []
  type: TYPE_NORMAL
- en: 2) Test Object. DL library testing and DL model testing have different test
    objects. The DL library testing aims to discover the implementation loopholes
    in the DL library, and its test objects are DL frameworks, DL compilers, and other
    third-party libraries that provide underlying support for DL models and systems.
    Therefore, in the repair process, developers usually fix bugs in the DL software (Deng
    et al., [2022](#bib.bib23); Pham et al., [2019](#bib.bib89)). In contrast, DL
    model testing aims to find potential correctness, fairness and other problems
    in the model, and its test objects are various DL models. Therefore, for the problems
    exposed in DL model testing, developers usually repair and improve the model by
    retraining and adjusting the model structure (Zhang et al., [2021d](#bib.bib126);
    Udeshi et al., [2018](#bib.bib104)).
  prefs: []
  type: TYPE_NORMAL
- en: 3) Test Property. As mentioned on [§ 2.3.3](#S2.SS3.SSS3 "2.3.3\. DL Library
    Testing Property ‣ 2.3\. DL Library Testing ‣ 2\. Preliminary ‣ A Survey of Deep
    Learning Library Testing Methods"), the research on DL model testing not merely
    focuses on the correctness of the implementation of model functionality (i.e.,
    correctness), but also focuses on the ability of the model to perform correctly
    under invalid input (i.e., robustness), the ability of the model not to be affected
    by sensitive and irrelevant input attributes (i.e., fairness), and the ability
    of the model to preserve private information (i.e., privacy), etc (Zhang et al.,
    [2020a](#bib.bib120), [2022](#bib.bib123)). However, the existing DL library testing
    mainly focuses on the correctness and efficiency of the library implementation,
    which determines whether the DL library can be widely and reliable deployed and
    applied.
  prefs: []
  type: TYPE_NORMAL
- en: 4) Test Oracle. In DL model testing, for test properties such as correctness,
    robustness, and fairness, the accuracy or other metrics of the model on the given
    input can be used to judge whether there is a problem with the model. For example,
    if the model is susceptible to subtle adversarial perturbations in the inference
    process, and it has low test accuracy on the adversarial samples, then the model
    can be considered to have an adversarial robustness problem. However, it is difficult
    to obtain the test oracle and identify potential correctness and efficiency problems
    and bugs in DL library testing. On the one hand, due to stochastic operations
    such as initialization and gradient descent, it is difficult for developers to
    determine the expected output of a DL program. On the other hand, various hardware
    architectures and underlying optimization methods make it difficult for developers
    to quantify and estimate the time and memory overhead when performing specified
    operators in DL libraries. Therefore, there are still challenges in the generation
    and design of test oracles for DL library testing. Existing research usually constructs
    pseudo oracles based on the concept of differential testing, which detects bugs
    by comparing the performance and outputs of the same API under different implementations (Pham
    et al., [2019](#bib.bib89); Deng et al., [2022](#bib.bib23)).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. DL Framework Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DL framework testing aims to design methods and generate test samples for DL
    frameworks(e.g., TensorFlow (Abadi, [2016](#bib.bib4)) and PyTorch (Paszke et al.,
    [2019](#bib.bib87))) and detect bugs such as crashes, overflows, inconsistent
    outputs, and unexcepted overheads. Excluding the empirical study that analyzes
    and understands the DL framework bugs, existing DL framework testing methods can
    be divided into three categories based on the techniques they use, differential
    testing, fuzz testing, and metamorphic testing. We summarize several state-of-the-art
    methods in [Table 2](#S3.T2 "Table 2 ‣ 3\. DL Framework Testing ‣ A Survey of
    Deep Learning Library Testing Methods"), whose columns show the methods category,
    a brief description of each work, the test object, the number of bugs each work
    reported, and the type of detected bugs. The following sections will describe
    existing testing methods in detail, and summarize the strengths and weaknesses
    of each category of methods.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Representative DL Framework Testing Methods
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method Description | Test Object | # Bugs | Bug Type |'
  prefs: []
  type: TYPE_TB
- en: '| Empirical Study |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Summarized program bug reports and analyzed &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the challenges of testing and localizing DL bugs (Zhang et al., [2018](#bib.bib127))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| TensorFlow | / | / |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Conducted large-scale study on DL frameworks &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and summarized bug symptoms and root causes (Chen et al., [2023a](#bib.bib10))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/DL4J/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch/MXNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| / | / |'
  prefs: []
  type: TYPE_TB
- en: '| Differential Testing |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Detected DL framework bugs via inconsistencies &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; between DL frameworks outputs (Pham et al., [2019](#bib.bib89)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNTK/Theano &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 12 | status/numerical |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mutated DL models to explore DL framework behaviors &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and precisely localized the buggy layer in models (Guo et al., [2020](#bib.bib36))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/CNTK/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Theano/PyTorch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 26 | status/numerical |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Extracted DL equivalence rules from documentation and open &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; source and constructed equivalent graphs to test (Wang et al., [2022](#bib.bib107))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 25 | status/numerical |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Leveraged LLMs to generate test code and identified &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; bugs from the different results on different devices (Deng et al., [2023a](#bib.bib21))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 65 | status/numerical |'
  prefs: []
  type: TYPE_TB
- en: '| Fuzz Testing |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Generate test cases for DL framework APIs and fuzz &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DL framework based on open-sourced data (Wei et al., [2022](#bib.bib110))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 49 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; status/numerical/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; performance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Extractd constraints from documentations to guide &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; test case generation and fuzz DL framework (Xie et al., [2022](#bib.bib114))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 94 | status |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Utilized the behavior of similar APIs as test &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; oracles and fuzz DL framework APIs (Deng et al., [2022](#bib.bib23))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 162 | status/numerical |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Designed AI semantics to compute and construct test &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; oracle and generate valid test cases in tests (Schumi and Sun, [2022](#bib.bib95))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 14 | status |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mutated and explored the search space of models and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; conducted a coverage-guided testing for DL frameworks (Li et al., [2023b](#bib.bib60))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/MXNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 32 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; status/numerical/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; performance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Metamorphic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Testing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Designed 11 metamorphic relations to verify the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; correctness of DL framework functionality (Ding et al., [2017](#bib.bib24))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Caffe | / | status/numerical |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Other &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Generated high-level code based on the low-level &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; offending inputs that trigger errors in DL frameworks (Christou et al.,
    [2023](#bib.bib18)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorFlow/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 61 | status/numerical |'
  prefs: []
  type: TYPE_TB
- en: 3.1\. Empirical Study on DL Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Researchers have conducted some empirical studies on DL framework testing to
    understand the characteristics of DL framework bugs and clarify the feasible research
    directions.
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al. (Zhang et al., [2018](#bib.bib127)) are one of the first research
    teams to focus on the characteristics of coding defects in the DL framework program.
    They collected and summarized the program bug reports related to the Tensorflow
    framework in the open-source community, and then analyzed the challenges of localizing
    these program bugs. They found that in the execution phase, the buggy behavior
    becomes stochastic and hard to localize and suggested replacing the network hyperparameters
    to solve the localization problem. Their work is more inclined to analyze some
    programs and projects based on the DL framework and pays limited attention to
    the bugs in the DL framework itself. Zhang et al. (Zhang et al., [2020b](#bib.bib121))
    also conducted an empirical study on failures and errors of DL programs and projects.
    They pointed out that the root causes of many faults are actually related to the
    execution environment rather than code logic, and existing tools have limited
    support for fault localization of DL program errors. In addition, other researchers (Zhang
    et al., [2019](#bib.bib125)) interviewed and surveyed 195 practitioners to sort
    out and understand the current challenges in the DL software development life
    cycle from the perspective of SE, and they proposed 7 suggestions for practitioners
    and researchers, such as junior practitioners should start with well-known DL
    frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Islam et al. (Islam et al., [2019](#bib.bib44)) further studied over 3000 bug
    posts and fixes which are related to five popular DL frameworks in the open source
    communities to understand the bug types, root causes and their effects. Their
    work is one of the first research to systematically summarize and analyze the
    bugs on multiple DL frameworks. They summarized 6 kinds of bug effects, including
    bad performance, crash, incorrect functionality, data corruption, hang, and memory
    out of bound. They found that the first three bug effects are the most common,
    which is consistent with a recent research (Chen et al., [2023a](#bib.bib10)).
    Similarly collecting information from open source, Chen et al. (Chen et al., [2020](#bib.bib16))
    paid attention to the program errors related to the deployment of DL models and
    frameworks, and organized and analyzed the challenges in deploying on platforms
    such as server, cloud, mobile, and browser.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, researchers have conducted more in-depth and subdivided research on
    DL frameworks and testing methods on the basis of prior work.
  prefs: []
  type: TYPE_NORMAL
- en: Some researchers deeply investigated and analyzed the DL framework bug and further
    proposed new testing methods. Chen et al. (Chen et al., [2023a](#bib.bib10)) conducted
    a large-scale study on 1000 bugs on 4 DL frameworks. They provided an in-depth
    summary of 13 types of root causes, including API misuse, numerical issues, etc.,
    and analyzed the symptoms of DL framework bugs. They further compared the indicators
    (e.g., line coverage) of the three existing test methods (Pham et al., [2019](#bib.bib89);
    Guo et al., [2020](#bib.bib36); Wang et al., [2020b](#bib.bib109)) to evaluate
    the effect of different testing methods and proposed a preliminary mutation-based
    testing tools TenFuzz. They took a first step to evaluate the effectiveness of
    existing DL library testing methods, but unfortunately failed to evaluate more
    state-of-the-art open-sourced testing tools, and their evaluation metrics were
    relatively simple. In addition, Yang et al. (Yang et al., [2022](#bib.bib119))
    also conducted a large-scale empirical study on the characteristics, root causes
    and fix patches of DL framework bugs. Instead of researching the characteristics
    of the DL framework bugs itself, Jia et al. (Jia et al., [2022](#bib.bib45)) studied
    the impact of injected bugs in the DL framework on the execution. They designed
    8 mutation operators and leveraged these operators to inject bugs into TensorFlow,
    Theano, and Keras frameworks and compare the runtime differences between the clean
    version and the buggy version. They found that most bugs do not cause observable
    errors and only introduce insignificant differences in the trained model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: On more subdivided research fields, Quan et al. (Quan et al., [2022](#bib.bib91))
    first investigated the faults and root causes of JavaScript-based DL systems and
    related frameworks. These DL systems that are built on top of DL frameworks such
    as TensorFlow.js and are widely deployed on browsers and mobile platforms are
    also compromised by a series of bugs such as poor performance and crashes. Aach
    et al. (Aach et al., [2023](#bib.bib3)) focus on the distributed DL frameworks
    and study the performance of ResNet models on PyTorch, Horovod, and DeepSpeed
    frameworks and different data loaders. They found that using a suitable data loader
    can significantly accelerate the computation of ResNet models on these DL frameworks.
    Cao et al. (Cao et al., [2022](#bib.bib7)) also paid attention to the performance.
    They systematically studied the performance problems in DL frameworks such as
    TensorFlow and Keras, and summarized five types of root causes from the aspects
    of API usage, model parameter selection, etc. Based on the finding in the empirical
    study, they proposed and implemented a rule-based static checker, DeepPerf, to
    detect potential performance problems in DL systems. Liu et al. (Liu et al., [2022c](#bib.bib73))
    focused on the software aging problems on TensorFlow, MindSpore, MXNet, and PaddlePaddle.
    Software aging is a fault that leads to the accumulation of errors either inside
    the program, resulting in an increased failure rate and degraded performance.
    Their findings show that the training and evaluation stage is the most vulnerable
    to software aging problems. Du et al. (Du et al., [2022](#bib.bib25)) manually
    analyzed over 3,000 bug reports and further studied Bohrbug and Mandelbug on three
    DL frameworks and their triggers and root causes. The former is a kind of bug
    that is easy to reproduce under certain conditions, and they are the most common
    bugs on DL frameworks. The latter can be divided into two categories (i.e., aging-related
    bug and non-aging related Mandelbug) depending on whether they are related to
    aging problems. The mechanism of Mandelbug is relatively complicated, and it is
    often related to the interaction within the software, therefore, it may not always
    be reproduced under the same conditions. Their observations point out that most
    Bohrbugs and Mandelbugs cause crashes and exceptions, and Mandelbugs are more
    difficult to fix than Bohrbugs. Even worse, traditional software testing methods
    are hardly effective on Mandelbugs. Harzevili et al. (Harzevili et al., [2023](#bib.bib37))
    further studied vulnerabilities in DL frameworks and the corresponding fix patterns.
    Based on their findings, they developed a mutation-based testing tool, DeepMut,
    and deployed and applied it to the TensorFlow framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary and Analysis: Existing research leveraged interviews and empirical
    studies to summarize and analyze software bugs and vulnerabilities in DL systems
    and frameworks and pointed out potential directions and challenges for the following
    DL framework testing work. Compared with earlier studies, recent work has spent
    extra effort on detailedly analyzing the symptoms and root causes of bugs on multiple
    DL frameworks or subdivided software problems such as aging problems. Some researchers
    even further proposed new test tools based on their findings (Harzevili et al.,
    [2023](#bib.bib37); Chen et al., [2023a](#bib.bib10)). Studies on granular and
    targeted software vulnerabilities on multiple DL frameworks may be a future research
    direction. Furthermore, comparative research on the effects of existing DL framework
    tests may also be one of the future directions.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Differential Testing on DL Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To design test oracles for DL frameworks and judge whether their behaviors are
    expected and identify DL framework bugs, researchers proposed various testing
    methods on the basis of the concept of differential testing (Pham et al., [2019](#bib.bib89);
    Guo et al., [2020](#bib.bib36)). In the early stage of DL framework testing, researchers
    compared the behaviors of various frameworks and deployment platforms and leveraged
    the differences to expose potential bugs (Liu et al., [2018](#bib.bib72); Guo
    et al., [2019](#bib.bib35)). Liu et al. (Liu et al., [2018](#bib.bib72)) conducted
    a comparative study on the three popular DL frameworks(i.e., TensorFlow, Caffe,
    and Torch). They found that there are significant differences in model optimization,
    performance, and adversarial robustness between different DL frameworks. Guo et
    al. (Guo et al., [2019](#bib.bib35)) conducted an empirical study to investigate
    the differences in architecture designs and implementations of existing frameworks
    and platforms and found several DL library bugs.
  prefs: []
  type: TYPE_NORMAL
- en: Although these studies have revealed the correctness of some DL frameworks in
    implementation, they have not formed any complete and systematic testing method
    or tool. Building on the perspective provided by the above work, researchers widely
    used the concept of differential testing to automatically and efficiently detect
    framework bugs by comparing the output results of multiple implementations of
    the test case. Depending on the generated test cases, the existing differential
    testing methods can be mainly divided into model-level testing and API-level testing (Deng
    et al., [2022](#bib.bib23)). We introduce two kinds of testing methods separately
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. Model-level Differential Testing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The model-level differential testing usually leverages the different results
    of a widely-used DL model (e.g., ResNet-50) on different platforms or frameworks
    to detect bugs. Therefore, although some model-level methods mutate the model
    layers and weights in tests, the model architecture and the API coverage in tests
    will not change significantly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/46cedf30d522cb820fbfca3c8bf9e74c.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4\. Trigger Inputs for Inconsistencies Between DL Frameworks
  prefs: []
  type: TYPE_NORMAL
- en: CRADLE (Pham et al., [2019](#bib.bib89)) is one of the first tools to detect
    and identify bugs based on the concept of differential testing. Based on Keras (Ketkar
    and Ketkar, [2017](#bib.bib52)) which can build and train models on different
    DL frameworks as backends, CRADLE conducted differential testing on three frameworks(i.e.,
    TensorFlow, CNTK, and Theano) and finally detected 12 bugs. [Fig. 4](#S3.F4 "Fig.
    4 ‣ 3.2.1\. Model-level Differential Testing ‣ 3.2\. Differential Testing on DL
    Framework ‣ 3\. DL Framework Testing ‣ A Survey of Deep Learning Library Testing
    Methods") shows two trigger figures of the inconsistencies that cause one model
    to have different prediction results and accuracy on different DL frameworks.
    CRADLE compared the model layer outputs between multiple DL frameworks and detected
    such inconsistencies. In addition, as a model-level testing method, CRADLE has
    designed a bug localizer to analyze and localize the bug in the input model. By
    comparing the differences in the execution graphs of one model on different frameworks,
    CRADLE calculated the rate of change in deviation between consecutive model layer
    outputs and found the specific buggy layers that affect the given model (i.e.,
    cause inconsistent model outputs). However, the inconsistent outputs of one layer
    may further lead to inconsistencies in subsequent layers, therefore this localization
    method is prone to false positives (FPs) and false negatives (FNs). To break the
    limitation, Guo et al. (Guo et al., [2020](#bib.bib36)) proposed a new automated
    framework testing and bug localization method, Audee, which is also model-level
    testing and supports the testing on the PyTorch framework. It generated a large
    number of test inputs by mutating the parameters of model layers, thereby exploring
    the potential behavior of the given model and detecting bugs. Furthermore, Audee
    leveraged a causal-testing-based technique to localize buggy layers and fine-grained
    fixed them by adjusting the parameters of the given layer and continued to identify
    the next buggy layer, thereby effectively reducing the FP rate in bug localization.
    Finally, Audee detected 26 bugs including NaNs, crashes, and inconsistencies and
    conducted an empirical study to understand the root causes of bugs and inconsistencies.
    [Fig. 5](#S3.F5 "Fig. 5 ‣ 3.2.1\. Model-level Differential Testing ‣ 3.2\. Differential
    Testing on DL Framework ‣ 3\. DL Framework Testing ‣ A Survey of Deep Learning
    Library Testing Methods") shows a NaN bug in torch.nn.AvgPool2d on the PyTorch
    framework. Audee detected this bug by comparing the model prediction results and
    behaviors between different DL frameworks and localized the root cause based on
    the causal testing technique, which is that its implementation only reduced ‘outputSize’
    variable iff ‘pad_l=True’ and led to division-by-zero overflow when ‘pad_l=False’
    and NaN bug¹¹1https://github.com/pytorch/pytorch/issues/36977. LEMON (Wang et al.,
    [2020b](#bib.bib109)) also explored bugs such as output inconsistencies through
    mutating DL models. It mainly focused on four DL frameworks including TensorFlow,
    Theano, CNTK, and MXNet, and identified 24 bugs including 1 performance bug in
    testing. Gu et al. (Gu et al., [2022a](#bib.bib34)) proposed another model-level
    differential testing tool, Muffin. Similar to the ideas of CRADLE and other prior
    work, Muffin generated a large number of DL models with different architectures
    and detected inconsistencies in model layers on TensorFlow, CNTK, and Theano frameworks
    to identify DL framework bugs. In addition, Muffin creatively provides differential
    testing in both model training and inference processes, thus it obtained more
    opportunities to identify new bugs and finally detected 39 new bugs.
  prefs: []
  type: TYPE_NORMAL
- en: Some researchers compare and observe the model performance on different frameworks
    based on the concept of differential testing. Levental et al.(Levental and Orlova,
    [2020](#bib.bib57)) compared the overhead of four DL libraries including PyTorch
    in the process of abstracting and implementing DL models. They found that, compared
    with other DL libraries (e.g., cuDNN), although the DL model implemented by the
    PyTorch framework had a significant advantage in accuracy, it also brought higher
    time and memory overhead in abstracting DL models. But instead of deeply analyzing
    the implementation of several DL libraries to find the root cause of such differences,
    they only gave some hypotheses to explain their observations, which limited the
    contribution of their work.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f40f6b32e42b1a3a4ee4f39c6a95f049.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 5\. An NaN bug Detected by Audee on PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: Although model-level differential testing methods obtain outstanding test results,
    they still have several limitations in applications. Restricted by the test model,
    these methods often only support several APIs related to the models, for example,
    existing research (Wei et al., [2022](#bib.bib110)) indicates that LEMON only
    covers 35 TensorFlow APIs. In addition, because the test oracle depends on the
    implementations of the model on multiple frameworks, the inconsistencies found
    in the test are difficult to confirm whether they are bugs, which affects the
    effectiveness of the test (Guo et al., [2020](#bib.bib36); Pham et al., [2019](#bib.bib89)).
    Furthermore, the implementation bugs in the model conversion tool, such as Keras (Ketkar
    and Ketkar, [2017](#bib.bib52)), ONNX (Microsoft, [2023](#bib.bib80)), can also
    lead to inconsistent behaviors of a model between two DL frameworks. Therefore,
    in recent years, model-level differential testing methods are gradually replaced
    by API-level differential testing methods that cover more DL framework APIs and
    test more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. API-level Differential Testing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different from the model-level methods, the test cases generated by the API-level
    method are simple calls or combinations of DL framework APIs. Some test cases
    even merely call one operator or transformation APIs to calculate or process a
    set of randomly generated input data. API-level testing eliminates the need to
    build and mutate DL models, which liberates the test cases from model shape constraints
    and enables the test to detect potential bugs on various framework APIs.
  prefs: []
  type: TYPE_NORMAL
- en: The API-level differential testing methods construct and leverage equivalence
    between APIs to test DL framework APIs. One of the most simple approaches is to
    execute DL APIs and operators on different devices and observe the differences
    between the behaviors. Zhang et al. (Zhang et al., [2021b](#bib.bib124)) tested
    the precision errors by comparing the behaviors of seven DL framework operators
    on CPU and GPU. Zhang et al. (Zhang et al., [2021a](#bib.bib122)) combined the
    differential testing with fuzz testing and designed nine mutation operators to
    test the DL framework APIs on TensorFlow, PyTorch, MNN, and MXNet. To efficiently
    and automatically generate test code at scale, Wei et al. (Wei et al., [2022](#bib.bib110))
    proposed FreeFuzz, the first DL libraries testing method via mining from open
    source. FreeFuzz first collected code that calls DL framework APIs from API documentation,
    DL framework test cases, and open-source models. Then, it tracked and extracted
    the input and parameter constraints of each API from the execution of the collected
    code and constructed new test cases. In the test, based on the concept of differential
    testing, FreeFuzz detected bugs by comparing the performance of test cases on
    different devices (i.e., CPU and GPU). FreeFuzz provided an effective test case
    generator that has high API coverage, and finally, it detected 49 bugs on PyTorch
    and Tensorflow frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers also compare the results of equivalent APIs on different frameworks
    to conduct differential testing. Gu et al. (Gu et al., [2022b](#bib.bib33)) compared
    the behaviors of operators between four frameworks, including TensorFlow, TensorFlow
    Lite, MXNet, and PyTorch, to identify detects on the training and inference process.
    They compared 20 operators on four DL frameworks in experiments. However, they
    did not directly report the number of bugs identified in their tests. In addition,
    they did not propose a method to automatically determine whether the difference
    in tests comes from implementation differences or bugs on a certain framework,
    which is a significant challenge to solve in testing on multiple frameworks. Recently,
    Diffwatch (Prochnow and Yang, [2022](#bib.bib90)) compared the output of the unit
    test code of one DL framework with the output of an equivalent function from another
    DL framework and supported the tests on TensorFlow, PyTorch, Keras, and Theano.
    Although they also did not directly report the number of detected bugs, they provided
    an open-sourced tool for community developers to reproduce and use.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0351d284dd8ec4fc106f30af15c0e175.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 6\. An Example PyTorch bug that is only Detected by TitanFuzz.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, with the popularity of large language models (LLMs), large models
    such as Codex can efficiently generate or analyze code based on several lines
    of prompts, providing more opportunities for software engineering and testing
    research. TitanFuzz (Deng et al., [2023a](#bib.bib21)) used two open-sourced LLMs,
    namely Codex (Chen et al., [2021](#bib.bib13)) and InCoder (Fried et al., [2022](#bib.bib28))
    to generate test cases for DL framework APIs on a large scale and executed them
    on CPU and GPU separately to conduct the differential testing. With the support
    of the knowledge and code processing capabilities of LLMs, TitanFuzz finally detected
    65 status and numerical bugs. [Fig. 6](#S3.F6 "Fig. 6 ‣ 3.2.2\. API-level Differential
    Testing ‣ 3.2\. Differential Testing on DL Framework ‣ 3\. DL Framework Testing
    ‣ A Survey of Deep Learning Library Testing Methods") presents an example bug
    that cannot be detected by previous API-level and model-level testing. TitanFuzz
    constructed test cases by LLMs and compared the behaviors between different devices
    to identify this bug on the PyTorch framework. In addition, Deng et al. (Deng
    et al., [2023b](#bib.bib22)) proposed FuzzGPT based on LLMs, which collected data
    from open source communities and leveraged few shot learning on large models such
    as Codex and CodeGen (Nijkamp et al., [2022](#bib.bib83)) and generated generate
    edge cases for DL framework APIs to perform differential testing on CPU and GPU
    to discover status bugs (i.e., crashes). FuzzGPT finally achieved better results
    than TitanFuzz and detected 76 bugs on PyTorch and TensorFlow frameworks²²2https://github.com/pytorch/pytorch/issues/82282.
  prefs: []
  type: TYPE_NORMAL
- en: Differential testing based on different devices is relatively simple and easy
    to implement, but the types of bugs that can be detected are relatively single.
    In order to detect more types and more complex DL framework bugs, researchers
    tried to construct and leverage the equivalence relationship between APIs to design
    differential tests. Wang et al. (Wang et al., [2022](#bib.bib107)) proposed EAGLE,
    which created equivalent graphs to test DL frameworks. They extracted 16 new DL
    equivalence rules from DL framework API documentation and non-crash issues in
    open-source communities and designed elaborate equivalent graphs that use different
    DL framework APIs, data types, or optimizations to produce identical output given
    the same input. EAGLE focuses on the numerical bug (i.e., inconsistencies) and
    finally detected 25 bugs on TensorFlow and PyTorch. Deng et al. (Deng et al.,
    [2022](#bib.bib23)) designed DeepRel on top of FreeFuzz, which can infer equivalent
    APIs and conduct differential testing between them to identify bugs effectively.
    DeepRel designed two elaborated equivalence (i.e., status equivalence and value
    equivalence) and matched API pairs based on these equivalence relations. It considered
    the output values and status of APIs in a pair as test oracles for each other
    and detected a total of 162 status and numerical bugs. Furthermore, Yang et al. (Yang
    et al., [2023](#bib.bib117)) proposed $\nabla$Fuzz, which focuses on the bugs
    in the backward propagation process and computes the gradients under different
    implementations of automatic differentiation (e.g., the reverse accumulation and
    the forward accumulation) in DL frameworks. It identifies bugs if there is any
    inconsistency in execution status and output results of automatic differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary and Analysis: In DL library testing, differential testing is a popular
    software testing method that detects bugs by observing whether similar implementations
    have different outputs regarding identical input (Davis and Weyuker, [1981](#bib.bib20)),
    which can help solve the problem of lack of test oracles in DL library testing.
    In the early stage of DL framework differential testing, researchers usually compare
    the output results of a model on multiple frameworks to detect potential bugs
    in the model layers. However, this approach can only cover limited APIs in the
    framework and relies on model conversion tools (Ketkar and Ketkar, [2017](#bib.bib52);
    Microsoft, [2023](#bib.bib80)). It can be observed that the differential testing
    methods based on multiple frameworks usually test TensorFlow, CNTK, and Theano,
    which can conveniently convert models to each other by the Keras library. With
    Keras 2.4.0 release³³3https://github.com/keras-team/keras/releases/tag/2.4.0 no
    longer supporting multi-backend, and CNTK and Theano stopping maintenance, differential
    testing methods based on multiple frameworks have become increasingly rare. Later,
    researchers started to design or construct differential testing scenarios, such
    as using different APIs to construct equivalent calculation graphs, matching similar
    APIs with equivalence relations, etc., and achieving outstanding results. Until
    now, differential testing is still one of the most popular DL framework testing
    methods. In recent research, the capability of LLMs in code generation enables
    new methods to efficiently generate test cases. These methods are usually able
    to generate numerous test cases and compare the execution results on different
    devices to detect potential bugs. In the future, DL framework test case generation
    and testing research may integrate more closely with LLMs, and the research on
    leveraging LLMs to generate implementations for a DL framework API as a pseudo
    test oracle for differential testing may be a future research interest.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Fuzz Testing on DL Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to the methods of generating test input, fuzzing can mainly be divided
    into generation-based fuzzing and mutation-based fuzzing (Oehlert, [2005](#bib.bib85)).
    In the following, we detailedly introduce the generation-based DL framework fuzzing
    methods and the mutation-based DL framework fuzzing methods.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1\. Generation-based Fuzz Testing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generation-based fuzzing generates test inputs randomly or based on the specifications
    of test inputs. As the test input of DL framework testing, DL programs usually
    have complex specifications (e.g., specific value ranges of API parameters, input
    sizes and dimensions), and the input violating specification will lead to a termination
    in execution, therefore existing work rarely generates test inputs randomly.
  prefs: []
  type: TYPE_NORMAL
- en: Some DL framework fuzzing tools extract and leverage the API constraints in
    the documentation or source code to guide the testing. Xie et al. (Xie et al.,
    [2022](#bib.bib114)) designed DocTer that analyzed documentation and extracted
    DL framework API constraints and further automatically built test cases. The test
    case generation in DocTer generated both valid and invalid cases according to
    the constraints and specifications to comprehensively evaluate whether the DL
    framework has unexpected behaviors. Valid test cases are used to test whether
    the functions of the DL framework have expected implementation, while invalid
    test cases verify whether the DL framework can correctly identify and prevent
    invalid input. DocTer finally found 94 status bugs on three different frameworks.
    DocTer provides a DL API constraint extraction method and an effective test input
    generation tool for DL framework testing, which facilitates and promotes the development
    of other methods. For example, Wang et al. (Wang et al., [2022](#bib.bib107))
    leveraged DocTer to generate test inputs and compared the execution results of
    the equivalent test inputs to detect bugs for 1,427 APIs on PyTorch and TensorFlow.
    Recently, Shi et al. (Shi et al., [2023](#bib.bib98)) proposed ACETest, which
    collected DL operators’ information from the source code and extracted input validation
    constraints by analyzing the execution path, thus they can build valid test cases
    to uncover crashes in DL frameworks. In the experiments, ACETest achieved better
    results in terms of test case generation and bug detection than state-of-the-art
    methods. In addition, Schumi et al. (Schumi and Sun, [2022](#bib.bib95)) designed
    an executable semantics for TensorFlow, which can precisely compute the output
    constraints to provide reliable test oracles and effectively generate valid DL
    models for testing. Their DL semantics help to expose 14 issues and bugs on TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers also proposed the ML-based method to solve the constraints problem
    in tests. SkipFuzz (Kang et al., [2022](#bib.bib50)) used active learning to learn
    the input constraints of different library APIs and generated valid test inputs
    for TensorFlow and PyTorch. It had finally identified 43 crashes on DL frameworks,
    including 13 CVEs assigned. Recently, Liu et al. (Liu et al., [2023b](#bib.bib70))
    designed a novel DL model-based testing method. Unlike the prior work that mutated
    popular DL models to generate more test cases, their method collected and extracted
    the constraints of DL framework operators and efficiently synthesized DL models.
    Their method finally found 87 new bugs on PyTorch and TensorFlow. FuzzGPT (Deng
    et al., [2023b](#bib.bib22)) designed zero-shot and few-shot learning to prime
    LLMs to generate edge cases while ensuring the semantic validity of generated
    test cases. LLMs’ knowledge of DL API calls enabled FuzzGPT to skip the step of
    collecting API constraints and directly generate effective test code for over
    3,000 APIs.
  prefs: []
  type: TYPE_NORMAL
- en: These methods can automatically and efficiently extract constraints and specifications
    from a given DL framework and generate a large number of test cases to discover
    status bugs such as crashes. However, limited by the design of test oracles, some
    of these methods can hardly identify numerical and performance bugs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Mutation-based Fuzz Testing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In DL framework testing, mutation-based fuzzing usually applies various mutation
    strategies to several valid test inputs collected from sources (e.g., open-source
    communities and API call samples). These mutation strategies often introduce small
    changes to the test input, such as adjusting the parameter value of a function
    and substituting the data type, so as to explore potential behaviors of DL frameworks
    while ensuring the validity of test cases as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Some testing methods and tools (Guo et al., [2020](#bib.bib36); Wang et al.,
    [2020b](#bib.bib109); Zhang et al., [2021a](#bib.bib122); Chen et al., [2023a](#bib.bib10);
    Harzevili et al., [2023](#bib.bib37)) mentioned above have implemented mutation
    operators to further explore potential behaviors in DL frameworks. Zhang et al. (Zhang
    et al., [2021b](#bib.bib124)) proposed a fuzzing method, Predoo, to conduct precision
    testing on 7 TensorFlow operators. Predoo magnifies the accuracy errors of DL
    framework operators by mutating the test input, thereby discovering potential
    bugs. Although Predoo promoted precision testing on DL frameworks, it supported
    a few operators on one DL framework and required manual reasoning and validation
    to obtain test oracles, making it difficult to apply on a large scale. Li et al. (Li
    et al., [2022a](#bib.bib59)) designed a mutation operator scheduling strategy
    to improve the test efficiency by selecting the effective mutation operators in
    the tests. Their method finally discovered 9 bugs on TensorFlow, Theano, CNTK
    and MXNet. Recently, Zou et al. (Zou et al., [2023](#bib.bib130)) proposed a hierarchical
    heuristic testing method, Ramos, to detect crashes and precision bugs on DL frameworks.
    Ramos leveraged a mutation-based hierarchical method to generate new models and
    increase the error of models. There are two mutation modes in Ramos, namely random
    mutation and heuristic mutation. The former randomly modified the layers and operators
    of the model, and the latter was based on the results of the previously generated
    model and tended to select mutation operators that can increase the error of the
    model. As a result, Ramos can effectively detect potential precision bugs on DL
    frameworks and finally detect 154 bugs on three frameworks, which is far beyond
    the results of previous research.
  prefs: []
  type: TYPE_NORMAL
- en: FreeFuzz (Wei et al., [2022](#bib.bib110)) collected code snippets from open
    source and implemented 3 categories of 15 mutation strategies on the data type
    and value to conduct fuzz testing. The mutation strategies include mutating the
    dimension and datatype of a tensor, mutating the value of a tensor, etc. Based
    on these strategies, FreeFuzz generated variants of a given test case from open
    source and further tested and revealed status, numerical and performance bugs
    for 1158 APIs on DL frameworks. [Fig. 7](#S3.F7 "Fig. 7 ‣ 3.3.2\. Mutation-based
    Fuzz Testing ‣ 3.3\. Fuzz Testing on DL Framework ‣ 3\. DL Framework Testing ‣
    A Survey of Deep Learning Library Testing Methods") shows a invalid input on torch.nn.MaxUnpool2d
    that only leads to a crash on the CPU device but does not throw any error message
    on GPU⁴⁴4https://github.com/pytorch/pytorch/issues/68727. FreeFuzz generated the
    invalid input by mutating the value of the input tensor and identified this status
    bug. Currently, developers have fixed this bug by adding a check for abnormal
    input values. Furthermore, DeepRel and $\nabla$Fuzz which implement on top of
    FreeFuzz are also mutation-based fuzz testing methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cbf4e234a077e479fd14fa9fc566de1e.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 7\. Invalid Input for MaxUnpool2d to Trigger Crash on CPU.
  prefs: []
  type: TYPE_NORMAL
- en: TitanFuzz (Deng et al., [2023a](#bib.bib21)) further utilized the ability of
    LLMs in code generation. It leveraged a Codex model to generate test seeds for
    a given API and implemented four well-designed mutation operators. These mutation
    operators masked the parameters of the API, the suffix and prefix of the test
    code, and the method under test in the seed, respectively, and then used the InCoder
    model to populate the mask and generate variants at scale. TitanFuzz is currently
    one of the testing tools, which covers the most APIs, which covers a total of
    3,544 APIs on PyTorch and TensorFlow. In addition, Li et al. (Li et al., [2023b](#bib.bib60))
    proposed a coverage-guided testing tool to effectively identify status, numeric,
    and performance bugs on the TensorFlow, MXNet, and PyTorch. They designed a series
    of mutation operators to explore the search space of model input, parameter, model
    structure, and anomaly and cover more DL framework APIs and found 32 DL framework
    bugs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary and Analysis: Fuzz testing provides an effective means of generating
    test cases for DL frameworks. It can explore potential anomalous behaviors on
    APIs while ensuring that the generated test cases are valid. However, fuzz testing
    methods cannot provide complex test oracles for DL frameworks. Therefore, some
    DL framework fuzzing tools usually leverage differential testing to construct
    test oracles and identify inconsistencies by comparing the performance of one
    test case on different devices or multiple equivalent test cases on the same device.
    With the development and application of LLMs, the recent fuzzing methods use the
    pre-trained code language model to efficiently generate test code and detect various
    API bugs. In future research, LLM could provide more opportunities for fuzz testing.
    The current LLMs are limited by the quality of open-sourced training data, and
    there is still room for improvement in performance on complex code generation
    tasks. How to improve data quality (e.g., collecting closed-sourced data) and
    finetune the open source LLMs (e.g., StarCoder (Li et al., [2023a](#bib.bib62)))
    to improve their generation performance and design a universal test case generation
    pipeline for various bugs (i.e., status, numerical, and performance bug) on DL
    frameworks may be a future research direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Metamorphic Testing on DL Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In DL testing, metamorphic testing is currently widely used to detect the problems
    in the design of DNN models and whether the models meet expectations (Xie et al.,
    [2009](#bib.bib115); Murphy et al., [2009](#bib.bib81); Xie et al., [2011](#bib.bib116)).
    Dwarakanath et al. (Dwarakanath et al., [2018](#bib.bib27)) implemented several
    MRs, including implementing permutation to the dataset, shifting data and scaling
    the dataset, etc, to identify implement bugs in DL applications, such as ResNet
    models. In addition, Zhang et al. (Zhang et al., [2021c](#bib.bib128)) proposed
    a metamorphic testing method for DL-driven image recognition systems. In the test,
    they implemented a series of MRs, including adjusting background brightness and
    adding noise to input images of the system and observed the results to detect
    potential problems. Despite the success of metamorphic testing on DL systems and
    models, researchers have paid limited attention to the metamorphic testing of
    DL frameworks. Ding et al. (Ding et al., [2017](#bib.bib24)) constructed 11 MRS
    and validated AlexNet on the Caffe DL Framework. However, they focused on the
    DL model accuracy in the test, and pay less attention to DL framework bugs. Wei
    et al. (Wei et al., [2022](#bib.bib110)) combined fuzzing with metamorphic testing
    to detect performance bugs in DL frameworks by comparing the execution time of
    test cases under the two data types of float16 and float32. The MR they designed
    is that programs carrying less precision information should execute faster. It
    is undeniable that they have taken an important step in detecting performance
    bugs. However, their MR can only provide a qualitative evaluation of API performance
    and cannot accurately and quantitatively detect and identify unexcepted runtime
    overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary and Analysis: The test results of metamorphic testing are directly
    related to the quality of the constructed MRs. Existing metamorphic testing for
    DL models and frameworks often design simple MRs, such as processing or supplementing
    training data, thus their effectiveness is limited. The metamorphic testing methods
    can verify whether the target model and system are implemented as expected in
    some aspects (i.e., the aspects related to MRs), but it cannot discover potential
    bugs in other aspects not covered. Therefore, simply using metamorphic testing
    is often difficult to obtain satisfactory test results. How to design more complex
    MRs (se.g., constructing functional equivalent test cases (Xiao et al., [2022](#bib.bib113)))
    and combine them with other testing methods to comprehensively evaluate DL frameworks
    may be a future research interest.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Other Testing on DL Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the testing methods introduced above, researchers also designed
    some other targeted testing methods for specific tasks. Herbold et al. (Herbold
    and Haar, [2022](#bib.bib40)) leveraged a traditional software testing method,
    smoke testing, to test and validate whether the DL frameworks (e.g., TensorFlow)
    can correctly handle common data distributions and have expected functional implementation.
    They used different valid inputs in the training and prediction of models, and
    observed whether the program crashed. Their work provided a new perspective for
    DL framework testing research and discovered 11 unknown bugs on three DL libraries.
    Ge et al. (Ge et al., [2023](#bib.bib29)) proposed an ML-based just-in-time defect
    prediction method for TensorFlow, MXNet, Mindspore, and PaddlePaddle frameworks.
    They collected and extracted various features such as code submission information
    and trained a random forest model to effectively predict potential defects on
    the DL frameworks at the code submission level. From the perspective of DL software
    security, Christou et al. (Christou et al., [2023](#bib.bib18)) designed IvySyn,
    which is an automated vulnerability testing tool for TensorFlow and PyTorch. Different
    from the prior work that implemented fuzzing on the high-level DL framework APIs,
    IvySyn constructed code blocks by DL framework APIs (e.g., in Python program language)
    based on a set of offending inputs that trigger memory safety errors in the underlying
    implementation of DL frameworks (e.g., in C/C++ program language) to trigger security
    vulnerabilities. Their approach helped developers to find 61 security vulnerabilities
    and assign 39 unique CVEs to DL frameworks. Furthermore, to promote the development
    of the software testing methods for DL frameworks and systems, Kim et al. (Kim
    et al., [2021](#bib.bib53)) proposed an open-source DL bug benchmark, covering
    4,577 bugs in 8 categories of DL software, including DL frameworks, platforms,
    and compilers, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary and Analysis: DL framework tests are mainly based on methods such as
    fuzz testing to test and validate the correctness and efficiency of DL frameworks.
    There is limited work (Christou et al., [2023](#bib.bib18)) that conducted the
    tests from the perspective of software security and detects traditional security
    vulnerabilities such as buffer overflows. It is foreseeable that, with the deepening
    of DL library testing research, researchers will pay more attention to security
    vulnerabilities within the DL framework rather than bugs of the functional implementation.
    In addition, although there are a variety of framework testing methods, there
    is still a lack of effective methods to evaluate and select testing methods or
    tools for specific application scenarios. Furthermore, for identified bugs and
    defects, existing research paid limited attention to the localization and repair
    methods. Automated bug location and patch generation can intuitively improve developers’
    work efficiency, which is valuable and could be a potential future research direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. DL Compiler Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In traditional software, the compiler converts high-level program language (e.g.,
    C++) to low-lever program language (e.g., assembly language) to create an executable
    program. SE Researchers have leveraged techniques such as fuzzing (Wu et al.,
    [2023](#bib.bib112)), metamorphic testing (Tao et al., [2010](#bib.bib102)), differential
    testing (Zhong, [2022](#bib.bib129)) and machine learning (Chen and Suo, [2022](#bib.bib11);
    Chen et al., [2023b](#bib.bib12)) to design a variety of testing methods for these
    traditional compilers (e.g., GCC, LLVM).
  prefs: []
  type: TYPE_NORMAL
- en: Different from traditional compilers, DL compilers convert abstracted DL models
    into optimized operators and code, which facilitates DL hardware libraries to
    perform calculations efficiently. Compared with DL frameworks, DL compilers require
    higher professional knowledge requirements and are often used to solve task-specific
    problems of the underlying optimization of DL models. Therefore, DL compilers
    have received less attention and are still in their infancy in recent years. Recently,
    researchers have designed and implemented DL compiler testing based on the compiler
    fuzzing techniques in traditional software engineering (Liu et al., [2022b](#bib.bib71),
    [2023a](#bib.bib69)).  [Table 3](#S4.T3 "Table 3 ‣ 4\. DL Compiler Testing ‣ A
    Survey of Deep Learning Library Testing Methods") shows several representative
    DL compiler testing work. Similar to [§ 3](#S3 "3\. DL Framework Testing ‣ A Survey
    of Deep Learning Library Testing Methods"), the following section will introduce
    the existing DL compiler testing methods and their strengths and limitations by
    category in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Representative DL Compiler Testing Methods
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method Description | Test Object | #Bugs | Bug Type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Empirical &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Study &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Studied and analyzed DL compiler bugs and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; summarized the bug symptoms and root causes (Shen et al., [2021](#bib.bib97))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TVM, Glow, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and nGraph &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| / | / |'
  prefs: []
  type: TYPE_TB
- en: '| Fuzz Testing |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Generated valid test cases based on constraints &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and conducted differential testing on DL compilers (Liu et al., [2023a](#bib.bib69))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TVM, ONNXRuntime, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TensorRT, and PyTorch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 72 | status/numerical |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mutated low-level IR of DL compilers and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; conducted coverage guided fuzz testing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: (Liu et al., [2022b](#bib.bib71)) | TVM | 49 |
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; status/numerical/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; performance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Metamorphic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Testing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Designed elaborated MRs to test the compiler behaviors &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and detected buggy test inputs on DL compilers (Xiao et al., [2022](#bib.bib113))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| TVM, Glow, and XLA | 4 | status/numerical |'
  prefs: []
  type: TYPE_TB
- en: 4.1\. Empirical Study on DL Compiler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Shen et al. (Shen et al., [2021](#bib.bib97)) took the lead in conducting the
    empirical study on DL compiler bugs. They analyzed and understood 603 bugs and
    their root causes on 3 popular DL compilers, including TVM from Apache, Glow from
    Facebook, and nGraph from Intel. They found that crash and wrong code (i.e., optimization
    bug) are the most common bugs in the DL compiler and the IR transformation stages
    are the most buggy stages. To further promote bug detection and improve the quality
    of DL compilers, they provided several suggestions for subsequent research and
    development of compilers, including adding more assertions to detect the wrong
    code bugs and designing localization methods to identify optimization-related
    bugs. In addition, Du et al. (Du et al., [2021](#bib.bib26)) collected and analyzed
    over 2,700 bug reports of TVM, Glow, nGraph, PlaidML, and Tensor Comprehensions
    (TC). They found that the most common root causes of DL compiler bugs are caused
    by semantics and compatibility, and most bugs lead to crashes or expectations,
    which is confirmed in other research (Shen et al., [2021](#bib.bib97)). However,
    possibly related to the data collection and classification method, the DL compiler
    bug types they summarized did not list performance-related bugs separately, but
    as part of the ‘warning style error’. Therefore, they did not analyze the DL compiler
    performance in depth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary and Analysis: The existing empirical studies on DL compilers collected
    bug reports from the open-source community, analyzed their symptoms and root causes,
    and further provided valuable suggestions for the development, application, and
    testing of DL compilers. However, the current research still lacks investigation
    and research on some DL compilers (e.g., XLA), although the bugs on them may have
    a similar distribution and characteristics to the bugs on other DL compilers.
    Therefore, a comprehensive assessment and evaluation of various DL compilers,
    their vulnerabilities and bugs, and testing methods may be one of the feasible
    future research directions.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Fuzz Testing on DL Compiler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DL compiler testing faces the challenge of generating valid test cases. On the
    one hand, a randomly generated test case has a quite low possibility of satisfying
    the semantic specification, especially for those complex compiler operations,
    which could be terminated before compilation. On the other hand, too simple test
    cases cannot effectively explore the potential behaviors of the DL compiler and
    find potential bugs. To overcome the challenge, researchers have designed a variety
    of fuzz testing methods based on generation and mutation to generate a large number
    of valid test cases and detect DL compiler bugs.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1\. Genration-based Fuzz Testing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dbf855de033508a44aae4550136b43ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 8\. A Crash on the relay.build of TVM Compiler and was Detected by HirGen.
  prefs: []
  type: TYPE_NORMAL
- en: Ren et al. (Ren et al., [2023](#bib.bib93)) proposed an open-source grammar-based
    fuzzing tool, Isra, for DL compilers. Grammarly-based fuzzing is one common technique
    to generate valid test cases for compilers in software engineering (Yang et al.,
    [2011](#bib.bib118); Holler et al., [2012](#bib.bib41)). Isra implemented a domain-specific
    constraint solver to resolve and simplify constraints and found semantically valid
    input for the DL compiler and efficiently generated test cases. It has found a
    total of 33 bugs (i.e., status bugs and inconsistency bugs) for the three DL compilers
    of TVM, Glow and SophGo in the experiments. Although Isra achieved outstanding
    results in bug detection, it lacked the ability to construct complex test oracles
    and failed to detect performance and optimization bugs. Recently, researchers
    have paid more attention to the methods that conduct testing for multiple DL compilers.
    Liu et al. (Liu et al., [2023a](#bib.bib69)) proposed NNSmith to identify bugs
    on four DL compilers, including TVM, TensorRT, ONNXRuntime, and PyTorch. NNSmith
    leveraged the operator constraints provided by users to generate valid graphs
    for DL compilers and cross-validated the compilation and execution results of
    different DL compilers on the same graph to detect potential bugs. In addition,
    NNSmitch designed a loss function to guide the search for valid inputs of generated
    graphs to avoid false positives. NNSmitch identified 72 new bugs on four DL compilers,
    which demonstrates its effectiveness in bug detection. Similar to the results
    of Isra, bugs detected by NNSmitch consist of crashes and inconsistencies. Both
    of the fuzzing methods lack the ability to identify complicated bugs like optimization
    bugs.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Mutation-based Fuzz Testing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to generation-based fuzzing methods, researchers also pay attention
    to mutation-based fuzz testing. Shen et al. (Shen et al., [2021](#bib.bib97)),
    as one of the first research teams focusing on fuzzing DL compilers, took the
    lead in designing the testing tool, TVMfuzz, for the TVM compiler. TVMfuzz conducted
    fuzz testing based on the directed graph built by TVM APIs, and mutated the shape
    and type of tensors to construct new unit tests. It finally detected 8 crash bugs
    by performing differential testing on the two versions of TVM. Liu et al. (Liu
    et al., [2022b](#bib.bib71)) further proposed the first coverage-guided fuzz testing
    tool for testing the tensor compiler (i.e., TVM), Tzer. They designed six kinds
    of mutation operators (e.g., inserting loops, replacing operations) for the low-level
    IR of DL compilers to trigger more potential behaviors, and added the mutated
    IR to the seed pool when it covered more code. Tzer leveraged differential testing
    and runtime failure to identify crashes, performance bugs, and inconsistencies,
    leading to a higher coverage of bug type than the prior work. It finally detected
    49 new bugs on the TVM compiler, surpassing prior tools (e.g., TVMfuzz) in terms
    of coverage and bug detection. Based on the findings of prior work (Shen et al.,
    [2021](#bib.bib97)), Ma et al. (Ma et al., [2023b](#bib.bib74)) focuses on high-level
    IR which is the most error-prone compilation stage and proposed HirGen to fuzzing
    DL compilers. HirGen implemented three coverage criteria to guide the fuzzing
    to cover more data types, data shapes, and edges of computation graphs and finally
    detected 21 bugs on the TVM compiler. HirGen designed three kinds of test oracles,
    including crashes and inconsistent results between IRs and between devices, to
    detect different types of failures and bugs. The bugs detected by HirGen mainly
    contain status bugs and inconsistent outputs. [Fig. 8](#S4.F8 "Fig. 8 ‣ 4.2.1\.
    Genration-based Fuzz Testing ‣ 4.2\. Fuzz Testing on DL Compiler ‣ 4\. DL Compiler
    Testing ‣ A Survey of Deep Learning Library Testing Methods") presents a crash
    on the TVM compiler⁵⁵5https://github.com/apache/tvm/pull/10502. When relay.build
    receives an ‘IRModule’ in which a function whose returned value includes another
    function exists, TVM will crash with a segmentation fault. HirGen constructed
    diverse high-level IRs that satisfy integrity constraints to conduct fuzz testing
    and exposed and reported this bug.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary and Analysis: Fuzz testing can effectively generate test inputs for
    DL compilers and explore potential behaviors and has already helped developers
    identify hundreds of bugs on these compilers. However, existing DL compiler fuzzers
    are still based on the mutator and input constraints, which have a lot of room
    for development compared with the existing research on DL frameworks. Prior research (Deng
    et al., [2023b](#bib.bib22); Li et al., [2022b](#bib.bib63)) has shown that machine
    learning technology can effectively generate valid test cases for the target program
    and guide the fuzz testing. It is foreseeable that how to leverage LLMs or other
    ML techniques to design automated and efficient testing tools for DL compilers
    may be one of the future research interests.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Metamorphic Testing on DL Compiler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Except for building valid test cases, how to construct test oracles to verify
    the output of the DL compiler is also a challenge. The DL compiler fuzzers mentioned
    above mainly leveraged the concept of differential testing to detect bugs by comparing
    the behavior differences between different devices or between IRs before and after
    optimization. Although they help detect a considerable amount of bugs, they cannot
    effectively identify complex optimizations or performance bugs. To construct test
    oracles to discover more complex compiler bugs and loopholes, researchers have
    delved into metamorphic testing. As mentioned above, metamorphic testing can effectively
    construct test oracles and verify whether the functional implementation is as
    expected. Xiao et al. (Xiao et al., [2022](#bib.bib113)) designed elaborated metamorphic
    relations to test DL compilers. For example, they introduced a series of operators
    whose final result is 0 in the input, and then in tests and then observed the
    change of the DL compiler result to detect potential bugs. These MRs can generate
    mutants for DL models to test the correctness of DL compiler compilation. They
    developed a testing tool, MT-DLComp, based on the above methods, and detected
    435 test inputs that could cause compiler errors and 4 DL compiler bugs on three
    compilers (i.e., TVM, Glow, and XLA). [Fig. 3](#S2.F3 "Fig. 3 ‣ 2.3.2\. DL Library
    Testing Component ‣ 2.3\. DL Library Testing ‣ 2\. Preliminary ‣ A Survey of Deep
    Learning Library Testing Methods") provides an example bug detected by MT-Comp.
    This vulnerability causes the DCE optimization in Glow to mistakenly delete active
    and valid nodes in the computation graph, resulting in the generated optimized
    operators outputting unexpected results. MT-DLComp identified this bug by generating
    variant test cases and observing whether their behaviors violate MRs after being
    compilation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary and Analysis: Metamorphic testing can effectively solve the test oracle
    problem by designing different MRs for DL compilers, and existing metamorphic
    testing research on DL compilers has achieved certain test results. However, metamorphic
    testing on DL compilers is still under development and needs more kinds of MRs
    to comprehensively verify the functions of DL compilers. In addition, metamorphic
    testing is different in generating a large number of test cases. How to combine
    metamorphic testing with other test methods (e.g., fuzz testing) to solve the
    test oracle and valid test case challenges at the same time may be a feasible
    research direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. DL Hardware Library Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Table 4\. Representative DL Hardware Library Testing Methods
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method Description | Test Libraries |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Testing on Functionality |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Generated test patterns for DL accelerator to ensure &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and reliability of its functional implementation (He et al., [2021](#bib.bib39))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| NVDLA |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Designed several MRs and conducted metamorphic testing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; for the operators of DL accelerators (Wang et al., [2020a](#bib.bib106))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| HiAI/SNPE |'
  prefs: []
  type: TYPE_TB
- en: '| Testing on Performance |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Conducted large-scale experiments to evaluate &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the performance of the convolution operators in cuDNN (Jorda et al.,
    [2019](#bib.bib48)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| cuDNN |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Compared and evaluated the performance of the fixed &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN architectures on three DL hardware libraries (Nazir et al., [2023](#bib.bib82))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cuBLAS, cuDNN, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and TensorRT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: DL hardware relies on related third-party libraries (such as cuDNN (Chetlur
    et al., [2014](#bib.bib17)), a popular accelerated library for GPU) for optimization
    and acceleration when implementing and calculating DL operators. These libraries
    are often designed for specific hardware and have strong pertinence. For example,
    CuDNN can mainly optimize and accelerate GPUs developed by NVIDIA. At present,
    researchers have conducted research on testing and verifying both DL hardware
    and the related libraries. For DL hardware, existing research has proposed several
    methods and tools to effectively detect the Functional Safty (FuSa) violation (Kundu
    et al., [2021](#bib.bib54); Chaudhuri et al., [2022](#bib.bib9); Kundu et al.,
    [2023](#bib.bib55); Chaudhuri et al., [2023](#bib.bib8)) and validate and estimate
    the performance (Buber and Banu, [2018](#bib.bib6); Liu et al., [2022a](#bib.bib68))
    on the hardware. However, these works focus on the bugs and vulnerabilities in
    the hardware itself, which is a different field from our topic. In this section,
    we mainly focus on the testing methods on the DL hardware-related library. Similar
    to DL compiler testing, DL hardware library test also has challenges in designing
    valid test input for low-level hardware libraries and constructing test oracles
    to identify errors and bugs. We introduce several state-of-the-art testing methods
    in [Table 4](#S5.T4 "Table 4 ‣ 5\. DL Hardware Library Testing ‣ A Survey of Deep
    Learning Library Testing Methods") and explain how they overcome the above challenges
    in the following section. Since we cannot find the number of bugs or vulnerabilities
    detected by these methods and the bug types they identified, we do not show the
    relevant information in the table. In the following, we will introduce the testing
    research that focuses on the functional correctness and efficiency of the DL hardware
    library respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Testing on DL Hardware Library Functional Correctness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In terms of empirical research, Huang et al. (Huang et al., [2023](#bib.bib42))
    investigated and understood the dependency bug in the DL stack. They investigated
    the symptoms and root causes of a total of 326 bugs in DL libraries which include
    DL applications, DL frameworks, DL accelerators, and DL hardware. They found that
    violating the constraints among dependencies is the main root cause of bugs and
    then suggested that developers should receive systematic training to fully understand
    the DL stack and life cycle to reduce the occurrence of bugs. Their work promoted
    the following research on DL hardware-related libraries research and DL stack.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers also designed a variety of methods to generate functional test patterns
    for DL hardware libraries and verify the correctness of their implementation.
    He et al. (He et al., [2021](#bib.bib39); Uezono et al., [2022](#bib.bib105))
    designed a set of functional testing methods for the NVIDIA Deep Learning Accelerator
    (NVDLA) to ensure the safety, reliability and correctness of its functional implementation.
    They designed test strategies for the accelerator’ compute units and control unit
    separately and achieved high test coverage and low test time overhead. For the
    former, they mapped the patterns of Automatic Test Pattern Generation (ATPG) of
    the computing unit to equivalent DL models, so that the execution of these models
    can reflect the response of ATPG patterns, and finally achieved 99.9% single stuck-at
    functional test coverage. For the latter, they achieved 100% coverage of the fault
    control model by deploying DL models with well-designed inputs and weights on
    a given application scenario. One step further, they conducted an industry case
    study on in-vehicle systems, and the results proved that their testing method
    can achieve high test coverage on real scenarios, which successfully achieves
    the reliability and safety requirements for DL accelerators in automotive applications.
  prefs: []
  type: TYPE_NORMAL
- en: The metamorphic testing method is also applied in testing and validating DL
    hardware libraries. Wang et al. (Wang et al., [2020a](#bib.bib106)) conducted
    metamorphic testing and designed a series of MRs on the convolution and softmax
    operators of DL accelerators HiAI and Snapdragon Neural Processing Engine (SNPE)
    to verify the accuracy of accelerators and explore potential accuracy defects.
    Their results show that HiAI has better accuracy performance than SNPE on the
    float16 data type. However, they did not further verify the bugs or vulnerabilities
    in the implementation of the two accelerators, such as crashes and inconsistencies.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, researchers designed and applied the fuzz testing method for the DL
    hardware library. As early as 2015, Lidbury et al. (Lidbury et al., [2015](#bib.bib66))
    have designed fuzz testing for the OpenCL compiler which is an open standard for
    writing code that runs across heterogeneous platforms including CPUs, GPUs etc.
    However, their fuzz testing method can not transfer to test CUDA compiler because
    of the difference between their execution models. Recently, CUDAsmith (Jiang et al.,
    [2020](#bib.bib46)) has been designed as a test case generation tool for the underlying
    NVCC and Clang library of the DL computing platform CUDA. It implements a generation-based
    kernel function generator to create valid test inputs that are adapted for the
    CUDA context and construct test oracles based on random differential testing to
    identify bugs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary and Analysis: Although the DL hardware library has received limited
    attention, there are still various methods to construct test inputs and test oracles
    for these libraries to verify their functional correctness. However, these methods
    are usually designed to test specific DL hardware libraries, have limited generalization
    ability, and lack the ability to identify real-world bugs. How to improve the
    effectiveness and generalization of these testing methods will be a challenge
    for future research.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Testing on DL Hardware Library Efficiency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to functional correctness, researchers also pay attention to the
    efficiency of these libraries. Jorda et al. (Jorda et al., [2019](#bib.bib48))
    conducted a large-scale experiment to comprehensively evaluate the performance
    and efficiency of the convolution operators implemented by cuDNN. They implemented
    a total of 602 configurations of different batch sizes, input sizes, filters,
    and depths and finally provided a guideline on the performance of DL accelerators
    for subsequent research and development. Nazir et al. (Nazir et al., [2023](#bib.bib82))
    leveraged fixed CNN architectures to evaluate and analyze the efficiency of three
    GPU libraries, namely cuBLAS, cuDNN, and TensorRT. They found that TensorRT can
    significantly shorten the execution time of DL models. Furthermore, a suitable
    CNN architecture can also reduce the time overhead of DL models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary and Analysis: The recent research on the efficiency of DL hardware
    libraries is still very limited. Compared with the testing on functional correctness,
    the testing methods on efficiency are relatively simple, and there is no systematic
    work on testing and identifying performance bugs. This research status may be
    related to the lack of effective test oracles for DL hardware library testing.
    How to solve the problem of test oracle and design the efficiency test method
    of DL hardware libraries will be a valuable and challenging research direction
    in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Future Gazing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the testing research on the DL library has made considerable progress,
    this field is still in its nascent stage. There are some research questions not
    received enough attention, and the testing effects of existing methods and tools
    also have room for improvement. Furthermore, there is currently a lack of benchmarks
    or tools to systematically evaluate existing testing methods and analyze their
    strengths and weaknesses in different application scenarios. In this section,
    we first comprehensively summarize and analyze the progress and status of existing
    testing research, and then discuss the future challenges and opportunities, aiming
    to provide guidance and reference for subsequent testing research.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Status of Existing Testing Research
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we analyze the development status of existing testing research
    from the perspective of different components and testing methods.
  prefs: []
  type: TYPE_NORMAL
- en: From the perspective of different components, DL framework testing is the most
    well-developed part of the testing of DL workflow. There are a variety of testing
    methods and tools for most of the popular DL frameworks, for example, PyTorch
    and TensorFlow. Some empirical studies comprehensively analyzed and revealed the
    characteristics of DL frameworks and their bugs. Promoted by their investigation
    results, researchers designed test case generation methods and constructed test
    oracles based on fuzz testing, differential testing, etc., which effectively discovered
    hundreds of DL framework bugs and vulnerabilities. In contrast, the testing research
    of DL compilers and DL hardware libraries is small in number and methodologically
    scattered. Although existing DL compiler tests also discover hundreds of DL compiler
    bugs, their methods are still relatively simple compared with DL framework testing,
    and the types of detected bugs are limited. In addition, their coverage of various
    optimization operations of the DL compiler also has room to improve. Existing
    DL compiler testing lacks the ability to generate large-scale test inputs and
    identify complicated performance and optimization bugs. The DL hardware library
    testing is the least developed and receives the least attention among the three
    fields. Its testing methods are more about verifying the functionality and performance
    of these DL hard libraries, but cannot detect and identify unexpected behaviors,
    thus the number of real-world bugs they can detect is relatively small. Although
    these compilers and hardware libraries may have been well maintained and tested
    during the development process, the gap in the number of testing methods and detected
    bugs also reflects the inadequacy of related research to a certain extent.
  prefs: []
  type: TYPE_NORMAL
- en: From the perspective of specific testing methods, fuzz testing is currently
    the most popular and effective method in DL library testing. When the input constraints
    and mutation methods can be determined, fuzz testing methods can automatically
    generate a large number of test cases and code blocks for different DL libraries
    and efficiently validate whether each function can be executed successfully under
    certain inputs, thereby reducing human overhead in testing. In recent years, different
    from the traditional fuzzing methods guided by constraints and mutators, ML-guided
    fuzzing, such as using ML to learn test input constraints (Kang et al., [2022](#bib.bib50))
    or leveraging LLMs to generate valid test cases (Deng et al., [2023a](#bib.bib21)),
    has achieved outstanding results. Differential testing is also a commonly used
    testing method, which compares the results of multiple implementations of the
    same function to solve the problem of test oracle. Since it cannot directly generate
    test cases, it is often combined with fuzz testing and provides simple but effective
    test oracles in recent research. For metamorphic testing, the testing effect is
    intuitively affected by MRs designed manually, therefore, its effectiveness is
    often limited in detecting real-world bugs.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Challenges and Opportunities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several limitations and challenges in existing DL library testing.
    We summarize the main challenges and the corresponding research opportunities
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1\. Conducting Comprehensive Evaluation of DL Framework Testing Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the recent few years, dozens of DL framework testing methods and tools have
    been proposed, which conducted comprehensive testing for different bugs such as
    crashes, inconsistent output, NaN, performance problems, and documentation bugs (Guo
    et al., [2020](#bib.bib36); Wei et al., [2022](#bib.bib110); Xie et al., [2022](#bib.bib114)).
    Some researchers have compared several test tools by some indicators (e.g., line
    coverage) (Chen et al., [2023a](#bib.bib10)), there is still no benchmark or research
    to comprehensively compare and evaluate the effect of these methods, and there
    are still research questions to be explored and answered. For example, does the
    most recent method always have advantages in terms of test results, execution
    overhead, types of bugs, etc.? Will the characteristics of the DL framework code
    and structure affect the test results of a testing method, resulting in poor test
    performance or even unusability of the method on specific frameworks?
  prefs: []
  type: TYPE_NORMAL
- en: The above challenges and research questions bring the following research opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, researchers could refer to traditional software testing datasets
    (e.g., Defects4J (Just et al., [2014](#bib.bib49))) to build and construct a DL
    framework bug dataset that covers plenty of APIs and various types of bugs, and
    supports multiple DL frameworks, thereby promoting the evaluation and development
    of DL framework testing methods.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, how to design a variety of metrics to build test benchmarks and comprehensively
    evaluate the effectiveness and efficiency of DL framework testing methods is also
    a potential future direction. The metrics can include the coverage of bug types,
    API coverage, the number of bugs discovered during the limited time, the ratio
    of generated test cases that can trigger bugs, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, how to study lightweight DL library testing or evaluation
    methods and combine them with the DL library development process is also an important
    research opportunity. Existing DL library research generally leveraged GitHub
    issues and CVE reports to expose the security problem of released libraries, and
    DL libraries often only use simple unit tests in development. Combining DL library
    testing methods in the production and development of DL libraries can effectively
    reduce the cost of fixing bugs and updating versions and the threat of bugs and
    vulnerabilities to DL library production, which could further promote the application
    of DL libraries and techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2\. Testing and Validating DL Compiler and Hardware Library
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DL compilers and hardware libraries optimize the abstract DL models and execute
    corresponding operators on specific hardware. The bugs in these libraries can
    directly affect the calculation results of DL models. Even worse, they can hardly
    be detected in higher-level tests (e.g., tests on DL systems and DL frameworks).
    Nevertheless, existing DL compilers and hardware libraries have garnered relatively
    less attention from researchers. Among the 74 DL library testing papers we collected
    and surveyed, only 12.16% and 18.82% are associated with DL compiler testing and
    DL hardware library testing, respectively. Furthermore, the existing work lacks
    systematic and comprehensive testing methodologies aimed at effectively identifying
    real-world bugs in the DL compilers and hardware libraries.
  prefs: []
  type: TYPE_NORMAL
- en: There are challenges in efficiently constructing test cases for DL compilers
    and hardware libraries. The test inputs of these libraries often consist of specific
    programming languages, which makes it difficult to find enough usable reference
    code and test cases in the open-source community and extract input constraints.
    In addition, how to build valid test oracles for these libraries is also challenging
    in testing. Differential tests based on different implementations of the same
    function may not be directly applicable to DL hardware libraries designed for
    specific hardware.
  prefs: []
  type: TYPE_NORMAL
- en: For research opportunities, researchers can refer to the idea of DL framework
    testing to design new testing methods for these DL libraries. For example, recent
    DL framework testing has introduced active learning techniques to learn input
    constraints and construct valid test cases (Kang et al., [2022](#bib.bib50)).
    DL compiler testing could also leverage ML techniques to achieve effective test
    case generation. In addition, existing DL framework differential testing research (Deng
    et al., [2022](#bib.bib23)) proposed that it is possible to perform differential
    testing by constructing equivalence relations between APIs. This concept can also
    be migrated to the DL compiler and hardware library testing research. Analyzing
    the relationship between operators and using equivalent operators to implement
    differential testing in DL compilers and hardware libraries might be a future
    research interest.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3\. Identifying Performance Bugs on DL Libraries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Existing work mainly detected and identified the status bugs and numerical bugs,
    which are related to the correctness of DL libraries. However, they paid limited
    attention to efficiency and could hardly discover performance bugs in these libraries.
    The main challenge in identifying performance bugs is how to design a valid test
    oracle. The symptoms of performance bugs are usually unexpected time or memory
    overhead, which is hard to predict before execution on real-world devices.
  prefs: []
  type: TYPE_NORMAL
- en: Existing research is mainly based on the concept of metamorphic testing to detect
    performance bugs. For example, they compared the executions of the same function
    with different data types and observed whether the memory or time overhead has
    increased or decreased as expected (Wei et al., [2022](#bib.bib110)). This method
    can provide a qualitative and rough test oracle, but cannot precisely predict
    the performance of the given API or operator quantitatively, and the performance
    bugs that can be tested are relatively limited. Recently, the development of ML
    techniques has provided new opportunities for constructing test oracles for performance
    bugs. Researchers could leverage ML technology to learn and predict the runtime
    overhead of DL library operators and APIs on a given hardware to construct test
    oracles, and then detect whether there is unexcepted performance and behaviors
    in subsequent large-scale testing.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.4\. Localizing and Repairing DL Library Bugs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Nowadays, DL library research has proposed a variety of testing methods and
    tools to discover bugs, but identification should not be the end of bug detection.
    As developers have claimed, ‘One strong (bug) report is 10000x better than 100
    poor ones’⁶⁶6https://github.com/tensorflow/tensorflow/issues/61605#issuecomment-1687623365.
    A detailed bug report, localization and analysis can effectively speed up developers’
    efficiency, not to mention providing a potential fix patch in the report. Therefore,
    methods to locate the root causes and generate patches are highly needed.
  prefs: []
  type: TYPE_NORMAL
- en: However, how to localize the root cause of these bugs in the source code and
    automatically generate fix patches is still a challenge. On the one hand, there
    is no comprehensive and usable DL library bug dataset, which makes it difficult
    to carry out bug/fault localization (FL) and automated program repair (APR) research.
    On the other hand, some DL libraries (especially DL frameworks) usually use Python
    code to call top-level APIs, while the underlying function implementation is done
    in C/C++. Localizing bugs in C/C++ program language according to Python test cases
    requires the realization of bug/fault localization across multiple programming
    languages, while traditional software testing mainly focuses on FL within one
    single programming language, which cannot be directly used in this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: The following research opportunities may help address these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, researchers can refer to the existing FL and APR datasets (e.g., Defects4J (Just
    et al., [2014](#bib.bib49))) to build a DL library bug dataset to promote the
    research on DL framework bug/fault localization and repair.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, recent research (Sun et al., [2022](#bib.bib100)) demonstrated the
    effectiveness of causality analysis in debugging and analyzing DL models and localizing
    buggy neurons, which also provides opportunities for DL library bug localization.
    Researchers can design localization methods by combining causality analysis with
    existing FL methods (Li et al., [2021](#bib.bib64); Meng et al., [2022](#bib.bib79))
    to determine the functions and lines in the source code that cause the bug.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.5\. Designing Tests for the Maintenance and Evolution of DL Libraries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Existing work on DL library testing often focuses on identifying bugs in specific
    versions of the library. However, libraries continue to have new versions released,
    which may fix some inherent bugs while potentially introducing new ones. For example,
    previous testing efforts focused heavily on PyTorch 1.x versions, but now PyTorch
    has entered the 2.x era, introducing numerous new features. New bugs in PyTorch
    2.x are continuously being identified and corresponding bug-fix versions are being
    frequently released⁷⁷7https://github.com/pytorch/pytorch/releases. Regression
    bugs are also huge pitfalls in PyTorch⁸⁸8https://github.com/pytorch/pytorch/issues/95432,
    where previously functional functions in older versions become problematic in
    the new version. This implies that testing for DL libraries needs to be continuously
    focused over time.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, with the continuous development of DL techniques (especially as
    we are currently entering the era of large models), new DL libraries (including
    frameworks, compilers, and hardware libraries) are being developed to meet new
    demands. For example, HuggingFace Transformers (Wolf et al., [2020](#bib.bib111))
    has become the most popular open-source model hosting library, supporting training
    and inference of large models; there are also various libraries optimized for
    computational requirements of large models, such as vLLM (Kwon et al., [2023](#bib.bib56)),
    DeepSpeed (Rasley et al., [2020](#bib.bib92)). These new DL libraries are often
    implemented based on previous DL libraries, such as PyTorch/TensorFlow, combined
    with some of the underlying compilers and hardware-level optimizations. Due to
    the complex nature of software aggregation, these new libraries may also be affected
    by bugs in underlying libraries, and concomitantly, new bugs may be introduced
    during the rapid development and release⁹⁹9https://github.com/vllm-project/vllm/issues/2248.
    However, there has been little attention paid to systematic bug analysis and fixing
    for those new DL libraries of the large models’ era.
  prefs: []
  type: TYPE_NORMAL
- en: 'The above-mentioned issues and challenges bring about many new opportunities,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, researchers should collect and maintain a comprehensive test set of
    the DL library, which can help to support the evolution of DL library testing.
    On the one hand, it should contain historical data on the testing of older versions
    of libraries, such as which test cases were generated by existing methods, which
    can be used to support regression testing of subsequent versions of existing libraries.
    On the other hand, it should also collect bugs found and discussed in the community
    for new DL libraries (e.g., vLLM, DeepSpeed).
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, existing test suites can provide guidance for testing the emergent
    libraries. One potential method is that based on the concepts of code similarity
    measurement and software component analysis (SCA), researchers can identify functional-similar
    components between the emerging libraries and existing libraries, and migrate
    the test suites of the latter to the former, thereby effectively identifying potential
    bugs.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rapid development and widespread deployment of DL-driven software systems
    have attracted researchers in academia and industry to investigate and study the
    DL library that supports DL systems. Existing research has achieved fruitful results
    in testing DL library bugs and vulnerabilities. However, with the development
    and iteration of DL techniques and software, there is still room for improvement
    in these testing methods. On the one hand, their generalization ability still
    needs to be improved to discover vulnerabilities in more kinds of DL libraries.
    On the other hand, state-of-the-art DL techniques represented by large models
    bring both challenges and opportunities for DL library testing. To comprehensively
    assess the testing research of the DL underlying library, understand the effects
    and deficiencies of the existing testing methods, and discuss the challenges and
    directions of future research, this paper first describes the definition of DL
    library bugs and testing ,and then summarizes and reviews the existing testing
    research on three kinds of DL libraries (i.e., DL framework, DL compiler, and
    DL hardware library). Finally, this paper discusses the challenges of DL library
    testing and future research opportunities from four aspects, aiming to promote
    further development and application of DL library testing research.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IEE (2010) 2010. IEEE Standard Classification for Software Anomalies. *IEEE
    Std 1044-2009 (Revision of IEEE Std 1044-1993)* (2010), 1–23. [https://doi.org/10.1109/IEEESTD.2010.5399061](https://doi.org/10.1109/IEEESTD.2010.5399061)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aach et al. (2023) Marcel Aach, Eray Inanc, Rakesh Sarma, Morris Riedel, and
    Andreas Lintermann. 2023. Large scale performance analysis of distributed deep
    learning frameworks for convolutional neural networks. *Journal of Big Data* 10,
    1 (2023), 1–23.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abadi (2016) Martín Abadi. 2016. TensorFlow: learning functions at scale. In
    *Proceedings of the 21st ACM SIGPLAN international conference on functional programming*.
    1–1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Braiek and Khomh (2020) Houssem Ben Braiek and Foutse Khomh. 2020. On testing
    machine learning programs. *Journal of Systems and Software* 164 (2020), 110542.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Buber and Banu (2018) Ebubekir Buber and DIRI Banu. 2018. Performance analysis
    and CPU vs GPU comparison for deep learning. In *2018 6th International Conference
    on Control Engineering & Information Technology (CEIT)*. IEEE, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2022) Junming Cao, Bihuan Chen, Chao Sun, Longjie Hu, Shuaihong
    Wu, and Xin Peng. 2022. Understanding performance problems in deep learning systems.
    In *Proceedings of the 30th ACM Joint European Software Engineering Conference
    and Symposium on the Foundations of Software Engineering*. 357–369.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaudhuri et al. (2023) Arjun Chaudhuri, Ching-Yuan Chen, Jonti Talukdar, and
    Krishnendu Chakrabarty. 2023. Functional Test Generation for AI Accelerators using
    Bayesian Optimization. In *2023 IEEE 41st VLSI Test Symposium (VTS)*. IEEE, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaudhuri et al. (2022) Arjun Chaudhuri, Jonti Talukdar, Fei Su, and Krishnendu
    Chakrabarty. 2022. Functional criticality analysis of structural faults in AI
    accelerators. *IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems* 41, 12 (2022), 5657–5670.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023a) Junjie Chen, Yihua Liang, Qingchao Shen, Jiajun Jiang, and
    Shuochuan Li. 2023a. Toward understanding deep learning framework bugs. *ACM Transactions
    on Software Engineering and Methodology* 32, 6 (2023), 1–31.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen and Suo (2022) Junjie Chen and Chenyao Suo. 2022. Boosting compiler testing
    via compiler optimization exploration. *ACM Transactions on Software Engineering
    and Methodology (TOSEM)* 31, 4 (2022), 1–33.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023b) Junjie Chen, Chenyao Suo, Jiajun Jiang, Peiqi Chen, and
    Xingjian Li. 2023b. Compiler test-program generation via memoized configuration
    search. In *Proc. 45th International Conference on Software Engineering*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018) Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng,
    Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al.
    2018. $\{$TVM$\}$: An automated $\{$End-to-End$\}$ optimizing compiler for deep
    learning. In *13th USENIX Symposium on Operating Systems Design and Implementation
    (OSDI 18)*. 578–594.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (1998) Tsong Y Chen, Shing C Cheung, and Shiu Ming Yiu. 1998. Metamorphic
    testing: a new approach for generating next test cases. *technical report hkust-cs98-01*
    (1998).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020) Zhenpeng Chen, Yanbin Cao, Yuanqiang Liu, Haoyu Wang, Tao
    Xie, and Xuanzhe Liu. 2020. A comprehensive study on challenges in deploying deep
    learning based software. In *Proceedings of the 28th ACM Joint Meeting on European
    Software Engineering Conference and Symposium on the Foundations of Software Engineering*.
    750–762.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chetlur et al. (2014) Sharan Chetlur, Cliff Woolley, Philippe Vandermersch,
    Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. cudnn: Efficient
    primitives for deep learning. *arXiv preprint arXiv:1410.0759* (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Christou et al. (2023) Neophytos Christou, Di Jin, Vaggelis Atlidakis, Baishakhi
    Ray, and Vasileios P Kemerlis. 2023. IvySyn: Automated Vulnerability Discovery
    in Deep Learning Frameworks. In *32nd USENIX Security Symposium (USENIX Security
    23)*. 2383–2400.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commission (2021) European Commission. 2021. *Artificial Intelligence Act*.
    [https://artificialintelligenceact.eu/wp-content/uploads/2021/08/The-AI-Act.pdf](https://artificialintelligenceact.eu/wp-content/uploads/2021/08/The-AI-Act.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Davis and Weyuker (1981) Martin D Davis and Elaine J Weyuker. 1981. Pseudo-oracles
    for non-testable programs. In *Proceedings of the ACM’81 Conference*. 254–257.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2023a) Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan
    Yang, and Lingming Zhang. 2023a. Large language models are zero-shot fuzzers:
    Fuzzing deep-learning libraries via large language models. In *Proceedings of
    the 32nd ACM SIGSOFT international symposium on software testing and analysis*.
    423–435.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2023b) Yinlin Deng, Chunqiu Steven Xia, Chenyuan Yang, Shizhuo Dylan
    Zhang, Shujing Yang, and Lingming Zhang. 2023b. Large language models are edge-case
    fuzzers: Testing deep learning libraries via fuzzgpt. *arXiv preprint arXiv:2304.02014*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. (2022) Yinlin Deng, Chenyuan Yang, Anjiang Wei, and Lingming Zhang.
    2022. Fuzzing deep-learning libraries via automated relational api inference.
    In *Proceedings of the 30th ACM Joint European Software Engineering Conference
    and Symposium on the Foundations of Software Engineering*. 44–56.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding et al. (2017) Junhua Ding, Xiaojun Kang, and Xin-Hua Hu. 2017. Validating
    a deep learning framework by metamorphic testing. In *2017 IEEE/ACM 2nd International
    Workshop on Metamorphic Testing (MET)*. IEEE, 28–34.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2022) Xiaoting Du, Yulei Sui, Zhihao Liu, and Jun Ai. 2022. An empirical
    study of fault triggers in deep learning frameworks. *IEEE Transactions on Dependable
    and Secure Computing* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2021) Xiaoting Du, Zheng Zheng, Lei Ma, and Jianjun Zhao. 2021. An
    Empirical Study on Common Bugs in Deep Learning Compilers. In *2021 IEEE 32nd
    International Symposium on Software Reliability Engineering (ISSRE)*. IEEE, 184–195.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dwarakanath et al. (2018) Anurag Dwarakanath, Manish Ahuja, Samarth Sikand,
    Raghotham M Rao, RP Jagadeesh Chandra Bose, Neville Dubash, and Sanjay Podder.
    2018. Identifying implementation bugs in machine learning based image classifiers
    using metamorphic testing. In *Proceedings of the 27th ACM SIGSOFT international
    symposium on software testing and analysis*. 118–128.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fried et al. (2022) Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric
    Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.
    2022. Incoder: A generative model for code infilling and synthesis. *arXiv preprint
    arXiv:2204.05999* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ge et al. (2023) Jian Ge, Huiqun Yu, Guisheng Fan, Jianhao Tang, and Zijie Huang.
    2023. Just-In-Time Defect Prediction for Intellignet Computing Frameworks (in
    Chinese). *Journal of Software* 34, 9 (2023), 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gezici and Tarhan (2022) Bahar Gezici and Ayça Kolukısa Tarhan. 2022. Systematic
    literature review on software quality for AI-based software. *Empirical Software
    Engineering* 27, 3 (2022), 66.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grigorescu et al. (2020) Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and
    Gigel Macesanu. 2020. A survey of deep learning techniques for autonomous driving.
    *Journal of Field Robotics* 37, 3 (2020), 362–386.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Groce et al. (2007) Alex Groce, Gerard Holzmann, and Rajeev Joshi. 2007. Randomized
    differential testing as a prelude to formal verification. In *29th International
    Conference on Software Engineering (ICSE’07)*. IEEE, 621–631.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2022b) Diandian Gu, Yining Shi, Haozhe Liu, Ge Wu, Haiou Jiang, Yaoshuai
    Zhao, and Yun Ma. 2022b. Defect Detection for Deep Learning Frameworks Based on
    Meta Operators (in Chinese). *Chinese Journal Of Computers* 45, 2 (2022), 240–255.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2022a) Jiazhen Gu, Xuchuan Luo, Yangfan Zhou, and Xin Wang. 2022a.
    Muffin: Testing deep learning libraries via neural architecture fuzzing. In *Proceedings
    of the 44th International Conference on Software Engineering*. 1418–1430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2019) Qianyu Guo, Sen Chen, Xiaofei Xie, Lei Ma, Qiang Hu, Hongtao
    Liu, Yang Liu, Jianjun Zhao, and Xiaohong Li. 2019. An empirical study towards
    characterizing deep learning development and deployment across different frameworks
    and platforms. In *2019 34th IEEE/ACM International Conference on Automated Software
    Engineering (ASE)*. IEEE, 810–822.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2020) Qianyu Guo, Xiaofei Xie, Yi Li, Xiaoyu Zhang, Yang Liu, Xiaohong
    Li, and Chao Shen. 2020. Audee: Automated testing for deep learning frameworks.
    In *Proceedings of the 35th IEEE/ACM International Conference on Automated Software
    Engineering*. 486–498.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harzevili et al. (2023) Nima Shiri Harzevili, Jiho Shin, Junjie Wang, Song Wang,
    and Nachiappan Nagappan. 2023. Characterizing and understanding software security
    vulnerabilities in machine learning libraries. In *2023 IEEE/ACM 20th International
    Conference on Mining Software Repositories (MSR)*. IEEE, 27–38.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2021) Yi He, Takumi Uezono, and Yanjing Li. 2021. Efficient functional
    in-field self-test for deep learning accelerators. In *2021 IEEE International
    Test Conference (ITC)*. IEEE, 93–102.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Herbold and Haar (2022) Steffen Herbold and Tobias Haar. 2022. Smoke testing
    for machine learning: simple tests to discover severe bugs. *Empirical Software
    Engineering* 27, 2 (2022), 45.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holler et al. (2012) Christian Holler, Kim Herzig, and Andreas Zeller. 2012.
    Fuzzing with code fragments. In *21st USENIX Security Symposium (USENIX Security
    12)*. 445–458.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2023) Kaifeng Huang, Bihuan Chen, Susheng Wu, Junming Cao, Lei
    Ma, and Xin Peng. 2023. Demystifying dependency bugs in deep learning stack. In
    *Proceedings of the 31st ACM Joint European Software Engineering Conference and
    Symposium on the Foundations of Software Engineering*. 450–462.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ignatov et al. (2018) Andrey Ignatov, Radu Timofte, William Chou, Ke Wang,
    Max Wu, Tim Hartley, and Luc Van Gool. 2018. Ai benchmark: Running deep neural
    networks on android smartphones. In *Proceedings of the European Conference on
    Computer Vision (ECCV) Workshops*. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Islam et al. (2019) Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh
    Rajan. 2019. A comprehensive study on deep learning bug characteristics. In *Proceedings
    of the 2019 27th ACM Joint Meeting on European Software Engineering Conference
    and Symposium on the Foundations of Software Engineering*. 510–520.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. (2022) Li Jia, Hao Zhong, Xiaoyin Wang, Linpeng Huang, and Zexuan
    Li. 2022. How Do Injected Bugs Affect Deep Learning?. In *2022 IEEE International
    Conference on Software Analysis, Evolution and Reengineering (SANER)*. IEEE, 793–804.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2020) Bo Jiang, Xiaoyan Wang, Wing Kwong Chan, TH Tse, Na Li,
    Yongfeng Yin, and Zhenyu Zhang. 2020. Cudasmith: A fuzzer for CUDA compilers.
    In *2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)*.
    IEEE, 861–871.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2012) Guoliang Jin, Linhai Song, Xiaoming Shi, Joel Scherpelz, and
    Shan Lu. 2012. Understanding and detecting real-world performance bugs. *ACM SIGPLAN
    Notices* 47, 6 (2012), 77–88.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jorda et al. (2019) Marc Jorda, Pedro Valero-Lara, and Antonio J Pena. 2019.
    Performance evaluation of cudnn convolution algorithms on nvidia volta gpus. *IEEE
    Access* 7 (2019), 70461–70473.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Just et al. (2014) René Just, Darioush Jalali, and Michael D Ernst. 2014. Defects4J:
    A database of existing faults to enable controlled testing studies for Java programs.
    In *Proceedings of the 2014 international symposium on software testing and analysis*.
    437–440.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. (2022) Hong Jin Kang, Pattarakrit Rattanukul, Stefanus Agus Haryono,
    Truong Giang Nguyen, Chaiyong Ragkhitwetsagul, Corina Pasareanu, and David Lo.
    2022. SkipFuzz: Active Learning-based Input Selection for Fuzzing Deep Learning
    Libraries. *arXiv preprint arXiv:2212.04038* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ken ([n. d.]) K Ken. [n. d.]. *Exclusive: surveillance footage of tesla crash
    on sf’s bay bridge hours after elon musk announces “self-driving” feature*. [https://theintercept.com/2023/01/10/tesla-crash-footage-autopilot/](https://theintercept.com/2023/01/10/tesla-crash-footage-autopilot/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ketkar and Ketkar (2017) Nikhil Ketkar and Nikhil Ketkar. 2017. Introduction
    to keras. *Deep learning with python: a hands-on introduction* (2017), 97–111.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2021) Misoo Kim, Youngkyoung Kim, and Eunseok Lee. 2021. Denchmark:
    A bug benchmark of deep learning-related software. In *2021 IEEE/ACM 18th International
    Conference on Mining Software Repositories (MSR)*. IEEE, 540–544.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kundu et al. (2021) Shamik Kundu, Suvadeep Banerjee, Arnab Raha, Suriyaprakash
    Natarajan, and Kanad Basu. 2021. Toward functional safety of systolic array-based
    deep learning hardware accelerators. *IEEE Transactions on Very Large Scale Integration
    (VLSI) Systems* 29, 3 (2021), 485–498.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kundu et al. (2023) Shamik Kundu, Suvadeep Banerjee, Arnab Raha, Fei Su, Suriyaprakash
    Natarajan, and Kanad Basu. 2023. Trouble-shooting at GAN Point: Improving Functional
    Safety in Deep Learning Accelerators. *IEEE Trans. Comput.* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient
    Memory Management for Large Language Model Serving with PagedAttention. In *Proceedings
    of the ACM SIGOPS 29th Symposium on Operating Systems Principles*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levental and Orlova (2020) Maksim Levental and Elena Orlova. 2020. Comparing
    the costs of abstraction for dl frameworks. *arXiv preprint arXiv:2012.07163*
    (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li (2018) Hang Li. 2018. Deep learning for natural language processing: advantages
    and challenges. *National Science Review* 5, 1 (2018), 24–26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022a) Junqiang Li, Senyi Li, Jiawei Wu, Long Luo, Yang Bai, and
    Hongfang Yu. 2022a. MMOS: Multi-Staged Mutation Operator Scheduling for Deep Learning
    Library Testing. In *GLOBECOM 2022-2022 IEEE Global Communications Conference*.
    IEEE, 6103–6108.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Meiziniu Li, Jialun Cao, Yongqiang Tian, Tsz On Li, Ming
    Wen, and Shing-Chi Cheung. 2023b. Comet: Coverage-guided model generation for
    deep learning library testing. *ACM Transactions on Software Engineering and Methodology*
    32, 5 (2023), 1–34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong
    Yang, Zhongzhi Luan, Lin Gan, Guangwen Yang, and Depei Qian. 2020. The deep learning
    compiler: A comprehensive survey. *IEEE Transactions on Parallel and Distributed
    Systems* 32, 3 (2020), 708–727.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    et al. 2023a. StarCoder: may the source be with you! *arXiv preprint arXiv:2305.06161*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022b) Xiaoting Li, Xiao Liu, Lingwei Chen, Rupesh Prajapati, and
    Dinghao Wu. 2022b. ALPHAPROG: reinforcement generation of valid programs for compiler
    fuzzing. In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 36.
    12559–12565.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021) Yi Li, Shaohua Wang, and Tien Nguyen. 2021. Fault localization
    with code coverage representation learning. In *2021 IEEE/ACM 43rd International
    Conference on Software Engineering (ICSE)*. IEEE, 661–673.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2018) Jie Liang, Mingzhe Wang, Yuanliang Chen, Yu Jiang, and
    Renwei Zhang. 2018. Fuzz testing in practice: Obstacles and solutions. In *2018
    IEEE 25th International Conference on Software Analysis, Evolution and Reengineering
    (SANER)*. IEEE, 562–566.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lidbury et al. (2015) Christopher Lidbury, Andrei Lascu, Nathan Chong, and Alastair F
    Donaldson. 2015. Many-core compiler fuzzing. *ACM SIGPLAN Notices* 50, 6 (2015),
    65–76.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2012) Bingchang Liu, Liang Shi, Zhuhua Cai, and Min Li. 2012. Software
    vulnerability discovery techniques: A survey. In *2012 fourth international conference
    on multimedia information networking and security*. IEEE, 152–156.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022a) Haiyi Liu, Shaoying Liu, Chenglong Wen, and W Eric Wong.
    2022a. TBEM: Testing-Based GPU-Memory Consumption Estimation for Deep Learning.
    *IEEE Access* 10 (2022), 39674–39680.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Jiawei Liu, Jinkun Lin, Fabian Ruffy, Cheng Tan, Jinyang
    Li, Aurojit Panda, and Lingming Zhang. 2023a. Nnsmith: Generating diverse and
    valid test cases for deep learning compilers. In *Proceedings of the 28th ACM
    International Conference on Architectural Support for Programming Languages and
    Operating Systems, Volume 2*. 530–543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Jiawei Liu, Jinjun Peng, Yuyao Wang, and Lingming Zhang.
    2023b. Neuri: Diversifying dnn generation via inductive rule inference. *arXiv
    preprint arXiv:2302.02261* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022b) Jiawei Liu, Yuxiang Wei, Sen Yang, Yinlin Deng, and Lingming
    Zhang. 2022b. Coverage-guided tensor compiler fuzzing with joint ir-pass mutation.
    *Proceedings of the ACM on Programming Languages* 6, OOPSLA1 (2022), 1–26.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018) Ling Liu, Yanzhao Wu, Wenqi Wei, Wenqi Cao, Semih Sahin,
    and Qi Zhang. 2018. Benchmarking deep learning frameworks: Design considerations,
    metrics and beyond. In *2018 IEEE 38th International Conference on Distributed
    Computing Systems (ICDCS)*. IEEE, 1258–1269.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022c) Zhihao Liu, Yang Zheng, Xiaoting Du, Zheng Hu, Wenjie Ding,
    Yanming Miao, and Zheng Zheng. 2022c. Taxonomy of Aging-related Bugs in Deep Learning
    Libraries. In *2022 IEEE 33rd International Symposium on Software Reliability
    Engineering (ISSRE)*. IEEE, 423–434.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2023b) Haoyang Ma, Qingchao Shen, Yongqiang Tian, Junjie Chen, and
    Shing-Chi Cheung. 2023b. Fuzzing Deep Learning Compilers with HirGen. In *Proceedings
    of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis*.
    248–260.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2023a) Xiangyue Ma, Xiaoting Du, Qing Cai, Yang Zheng, Jing Hu, and
    Zheng Zheng. 2023a. A Survey on Testing of Deep Learning Frameworks (in Chinese).
    *Journal of Software* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manès et al. (2019) Valentin JM Manès, HyungSeok Han, Choongwoo Han, Sang Kil
    Cha, Manuel Egele, Edward J Schwartz, and Maverick Woo. 2019. The art, science,
    and engineering of fuzzing: A survey. *IEEE Transactions on Software Engineering*
    47, 11 (2019), 2312–2331.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Martínez-Fernández et al. (2022) Silverio Martínez-Fernández, Justus Bogner,
    Xavier Franch, Marc Oriol, Julien Siebert, Adam Trendowicz, Anna Maria Vollmer,
    and Stefan Wagner. 2022. Software engineering for AI-based systems: a survey.
    *ACM Transactions on Software Engineering and Methodology (TOSEM)* 31, 2 (2022),
    1–59.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McKeeman (1998) William M McKeeman. 1998. Differential testing for software.
    *Digital Technical Journal* 10, 1 (1998), 100–107.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meng et al. (2022) Xiangxin Meng, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong
    Liu. 2022. Improving fault localization and program repair with deep semantic
    features and transferred knowledge. In *Proceedings of the 44th International
    Conference on Software Engineering*. 1169–1180.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft (2023) Microsoft. 2023. *ONNX Github repository*. [https://github.com/onnx/onnx](https://github.com/onnx/onnx)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Murphy et al. (2009) Christian Murphy, Kuang Shen, and Gail Kaiser. 2009. Using
    JML runtime assertion checking to automate metamorphic testing in applications
    without test oracles. In *2009 International Conference on Software Testing Verification
    and Validation*. IEEE, 436–445.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nazir et al. (2023) Zhumakhan Nazir, Vladislav Yarovenko, and Jurn-Gyu Park.
    2023. Interpretable ML enhanced CNN Performance Analysis of cuBLAS, cuDNN and
    TensorRT. In *Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing*.
    1260–1265.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nijkamp et al. (2022) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open
    large language model for code with multi-turn program synthesis. *arXiv preprint
    arXiv:2203.13474* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nistor et al. (2015) Adrian Nistor, Po-Chun Chang, Cosmin Radoi, and Shan Lu.
    2015. Caramel: Detecting and fixing performance problems that have non-intrusive
    fixes. In *2015 IEEE/ACM 37th IEEE International Conference on Software Engineering*,
    Vol. 1\. IEEE, 902–912.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oehlert (2005) Peter Oehlert. 2005. Violating assumptions with fuzzing. *IEEE
    Security & Privacy* 3, 2 (2005), 58–62.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: of Science and Policy ([n. d.]) Office of Science and Technology Policy. [n. d.].
    *National Artificial Intelligence Initiative Act of 2020*. [https://www.ai.gov/wp-content/uploads/2023/04/National-Artificial-Intelligence-Initiative-Act-of-2020.pdf](https://www.ai.gov/wp-content/uploads/2023/04/National-Artificial-Intelligence-Initiative-Act-of-2020.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning
    library. *Advances in neural information processing systems* 32 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patterson et al. (2021) David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang,
    Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.
    2021. Carbon emissions and large neural network training. *arXiv preprint arXiv:2104.10350*
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pham et al. (2019) Hung Viet Pham, Thibaud Lutellier, Weizhen Qi, and Lin Tan.
    2019. CRADLE: cross-backend validation to detect and localize bugs in deep learning
    libraries. In *2019 IEEE/ACM 41st International Conference on Software Engineering
    (ICSE)*. IEEE, 1027–1038.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prochnow and Yang (2022) Alexander Prochnow and Jinqiu Yang. 2022. DiffWatch:
    watch out for the evolving differential testing in deep learning libraries. In
    *Proceedings of the ACM/IEEE 44th International Conference on Software Engineering:
    Companion Proceedings*. 46–50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quan et al. (2022) Lili Quan, Qianyu Guo, Xiaofei Xie, Sen Chen, Xiaohong Li,
    and Yang Liu. 2022. Towards understanding the faults of javascript-based deep
    learning systems. In *Proceedings of the 37th IEEE/ACM International Conference
    on Automated Software Engineering*. 1–13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
    Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning
    models with over 100 billion parameters. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 3505–3506.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2023) Luyao Ren, ZiHeng Wang, Yingfei Xiong, Li Zhang, Guoyue Jiang,
    and Tao Xie. 2023. Effective Random Test Generation for Deep Learning Compilers.
    *arXiv preprint arXiv:2302.00842* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rotem et al. (2018) Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret Catron,
    Summer Deng, Roman Dzhabarov, Nick Gibson, James Hegeman, Meghan Lele, Roman Levenstein,
    et al. 2018. Glow: Graph lowering compiler techniques for neural networks. *arXiv
    preprint arXiv:1805.00907* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schumi and Sun (2022) Richard Schumi and Jun Sun. 2022. ExAIS: executable AI
    semantics. In *Proceedings of the 44th International Conference on Software Engineering*.
    859–870.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segura et al. (2016) Sergio Segura, Gordon Fraser, Ana B Sanchez, and Antonio
    Ruiz-Cortés. 2016. A survey on metamorphic testing. *IEEE Transactions on software
    engineering* 42, 9 (2016), 805–824.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2021) Qingchao Shen, Haoyang Ma, Junjie Chen, Yongqiang Tian, Shing-Chi
    Cheung, and Xiang Chen. 2021. A comprehensive study of deep learning compiler
    bugs. In *Proceedings of the 29th ACM Joint meeting on european software engineering
    conference and symposium on the foundations of software engineering*. 968–980.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2023) Jingyi Shi, Yang Xiao, Yuekang Li, Yeting Li, Dongsong Yu,
    Chendong Yu, Hui Su, Yufeng Chen, and Wei Huo. 2023. ACETest: Automated Constraint
    Extraction for Testing Deep Learning Operators. *arXiv preprint arXiv:2305.17914*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: staff ([n. d.]) ABC7.com staff. [n. d.]. *Uber gives up testing of self-driving
    cars in California in wake of fatal Arizona crash*. [https://abc7.com/self-driving-uber-crash-video-pedestrian-hit-by-car-autonomous-vehicles/3269690/](https://abc7.com/self-driving-uber-crash-video-pedestrian-hit-by-car-autonomous-vehicles/3269690/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2022) Bing Sun, Jun Sun, Long H Pham, and Jie Shi. 2022. Causality-based
    neural network repair. In *Proceedings of the 44th International Conference on
    Software Engineering*. 338–349.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun (2020) Y Sun. 2020. *Tesla and PyTorch: PyTorch Developer Conference Highlights*.
    [https://medium.com/data-science-bootcamp/tesla-and-pytorch-pytorch-developer-conference-highlights-part-3ed36f2c9d5e](https://medium.com/data-science-bootcamp/tesla-and-pytorch-pytorch-developer-conference-highlights-part-3ed36f2c9d5e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tao et al. (2010) Qiuming Tao, Wei Wu, Chen Zhao, and Wuwei Shen. 2010. An automatic
    testing approach for compiler based on metamorphic testing technique. In *2010
    Asia Pacific Software Engineering Conference*. IEEE, 270–279.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow (2020) TensorFlow. 2020. *Learn how TensorFlow solves real, everyday
    machine learning problems*. [https://www.tensorflow.org/about/case-studies](https://www.tensorflow.org/about/case-studies)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Udeshi et al. (2018) Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay.
    2018. Automated directed fairness testing. In *Proceedings of the 33rd ACM/IEEE
    International Conference on Automated Software Engineering*. 98–108.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uezono et al. (2022) Takumi Uezono, Yi He, and Yanjing Li. 2022. Achieving automotive
    safety requirements through functional in-field self-test for deep learning accelerators.
    In *2022 IEEE International Test Conference (ITC)*. IEEE, 465–473.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020a) Chaojin Wang, Jian Shen, Chunrong Fang, Xiangsheng Guan,
    Kaitao Wu, and Jiang Wang. 2020a. Accuracy measurement of deep neural network
    accelerator via metamorphic testing. In *2020 IEEE International Conference On
    Artificial Intelligence Testing (AITest)*. IEEE, 55–61.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Jiannan Wang, Thibaud Lutellier, Shangshu Qian, Hung Viet
    Pham, and Lin Tan. 2022. EAGLE: creating equivalent graphs to test deep learning
    libraries. In *Proceedings of the 44th International Conference on Software Engineering*.
    798–810.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Jiyuan Wang, Qian Zhang, Guoqing Harry Xu, and Miryung Kim.
    2021. Qdiff: Differential testing of quantum software stacks. In *2021 36th IEEE/ACM
    International Conference on Automated Software Engineering (ASE)*. IEEE, 692–704.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020b) Zan Wang, Ming Yan, Junjie Chen, Shuang Liu, and Dongdi
    Zhang. 2020b. Deep learning library testing via effective model generation. In
    *Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference
    and Symposium on the Foundations of Software Engineering*. 788–799.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2022) Anjiang Wei, Yinlin Deng, Chenyuan Yang, and Lingming Zhang.
    2022. Free lunch for testing: Fuzzing deep-learning libraries from open source.
    In *Proceedings of the 44th International Conference on Software Engineering*.
    995–1007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language Processing.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations*. Association for Computational Linguistics,
    Online, 38–45. [https://www.aclweb.org/anthology/2020.emnlp-demos.6](https://www.aclweb.org/anthology/2020.emnlp-demos.6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Mingyuan Wu, Minghai Lu, Heming Cui, Junjie Chen, Yuqun Zhang,
    and Lingming Zhang. 2023. Jitfuzz: Coverage-guided fuzzing for jvm just-in-time
    compilers. In *2023 IEEE/ACM 45th International Conference on Software Engineering
    (ICSE)*. IEEE, 56–68.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2022) Dongwei Xiao, Zhibo Liu, Yuanyuan Yuan, Qi Pang, and Shuai
    Wang. 2022. Metamorphic testing of deep learning compilers. *Proceedings of the
    ACM on Measurement and Analysis of Computing Systems* 6, 1 (2022), 1–28.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2022) Danning Xie, Yitong Li, Mijung Kim, Hung Viet Pham, Lin Tan,
    Xiangyu Zhang, and Michael W Godfrey. 2022. DocTer: documentation-guided fuzzing
    for testing deep learning API functions. In *Proceedings of the 31st ACM SIGSOFT
    International Symposium on Software Testing and Analysis*. 176–188.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2009) Xiaoyuan Xie, Joshua Ho, Christian Murphy, Gail Kaiser, Baowen
    Xu, and Tsong Yueh Chen. 2009. Application of metamorphic testing to supervised
    classifiers. In *2009 Ninth International Conference on Quality Software*. IEEE,
    135–144.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2011) Xiaoyuan Xie, Joshua WK Ho, Christian Murphy, Gail Kaiser,
    Baowen Xu, and Tsong Yueh Chen. 2011. Testing and validating machine learning
    classifiers by metamorphic testing. *Journal of Systems and Software* 84, 4 (2011),
    544–558.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023) Chenyuan Yang, Yinlin Deng, Jiayi Yao, Yuxing Tu, Hanchi
    Li, and Lingming Zhang. 2023. Fuzzing automatic differentiation in deep-learning
    libraries. *arXiv preprint arXiv:2302.04351* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2011) Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011.
    Finding and understanding bugs in C compilers. In *Proceedings of the 32nd ACM
    SIGPLAN conference on Programming language design and implementation*. 283–294.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022) Yilin Yang, Tianxing He, Zhilong Xia, and Yang Feng. 2022.
    A comprehensive empirical study on bug characteristics of deep learning frameworks.
    *Information and Software Technology* 151 (2022), 107004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020a) Jie M Zhang, Mark Harman, Lei Ma, and Yang Liu. 2020a.
    Machine learning testing: Survey, landscapes and horizons. *IEEE Transactions
    on Software Engineering* 48, 1 (2020), 1–36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020b) Ru Zhang, Wencong Xiao, Hongyu Zhang, Yu Liu, Haoxiang
    Lin, and Mao Yang. 2020b. An empirical study on program failures of deep learning
    jobs. In *Proceedings of the ACM/IEEE 42nd International Conference on Software
    Engineering*. 1159–1170.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021a) Xufan Zhang, Jiawei Liu, Ning Sun, Chunrong Fang, Jia
    Liu, Jiang Wang, Dong Chai, and Zhenyu Chen. 2021a. Duo: Differential fuzzing
    for deep learning operators. *IEEE Transactions on Reliability* 70, 4 (2021),
    1671–1685.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022) Xiaoyu Zhang, Chao Shen, Chenhao Lin, Qian Li, Qian Wang,
    Qi Li, and Xiaohong Guan. 2022. The Testing and Repairing Methods for Machine
    Learning Model Security. *ACTA ELECTONICA SINICA* 50, 12 (2022), 2884.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021b) Xufan Zhang, Ning Sun, Chunrong Fang, Jiawei Liu, Jia
    Liu, Dong Chai, Jiang Wang, and Zhenyu Chen. 2021b. Predoo: precision testing
    of deep learning operators. In *Proceedings of the 30th ACM SIGSOFT International
    Symposium on Software Testing and Analysis*. 400–412.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019) Xufan Zhang, Yilin Yang, Yang Feng, and Zhenyu Chen. 2019.
    Software engineering practice in the development of deep learning applications.
    *arXiv preprint arXiv:1910.03156* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021d) Xiaoyu Zhang, Juan Zhai, Shiqing Ma, and Chao Shen. 2021d.
    Autotrainer: An automatic dnn training problem detection and repair system. In
    *2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)*.
    IEEE, 359–371.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Yuhao Zhang, Yifan Chen, Shing-Chi Cheung, Yingfei Xiong,
    and Lu Zhang. 2018. An empirical study on TensorFlow program bugs. In *Proceedings
    of the 27th ACM SIGSOFT international symposium on software testing and analysis*.
    129–140.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021c) Zhiyi Zhang, Pu Wang, Hongjing Guo, Ziyuan Wang, Yuqian
    Zhou, and Zhiqiu Huang. 2021c. Deepbackground: Metamorphic testing for deep-learning-driven
    image recognition systems accompanied by background-relevance. *Information and
    Software Technology* 140 (2021), 106701.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong (2022) Hao Zhong. 2022. Enriching compiler testing with real program from
    bug report. In *Proceedings of the 37th IEEE/ACM International Conference on Automated
    Software Engineering*. 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2023) Yinglong Zou, Haofeng Sun, Chunrong Fang, Jiawei Liu, and
    Zhenping Zhang. 2023. Deep learning framework testing via hierarchical and heuristic
    model generation. *Journal of Systems and Software* 201 (2023), 111681.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
