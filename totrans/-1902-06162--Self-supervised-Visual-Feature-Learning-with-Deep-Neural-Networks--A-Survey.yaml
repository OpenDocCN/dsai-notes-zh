- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:06:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:06:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1902.06162] Self-supervised Visual Feature Learning with Deep Neural Networks:
    A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1902.06162] 自监督视觉特征学习与深度神经网络：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1902.06162](https://ar5iv.labs.arxiv.org/html/1902.06162)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1902.06162](https://ar5iv.labs.arxiv.org/html/1902.06162)
- en: 'Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自监督视觉特征学习与深度神经网络：综述
- en: 'Longlong Jing and Yingli Tian^∗ L. Jing is with the Department of Computer
    Science, The Graduate Center, The City University of New York, NY, 10016\. E-mail:
    ljing@gradcenter.cuny.edu'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'Longlong Jing 和 Yingli Tian^∗ L. Jing 就职于纽约城市大学研究生中心的计算机科学系，E-mail: ljing@gradcenter.cuny.edu'
- en: 'Y. Tian is with the Department of Electrical Engineering, The City College,
    and the Department of Computer Science, the Graduate Center, the City University
    of New York, NY, 10031\. E-mail: ytian@ccny.cuny.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 'Y. Tian 就职于纽约城市大学的电气工程系和计算机科学系，E-mail: ytian@ccny.cuny.edu'
- en: ^∗Corresponding author
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗通讯作者
- en: This material is based upon work supported by the National Science Foundation
    under award number IIS-1400802.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本材料基于国家科学基金会资助的工作，资助编号 IIS-1400802。
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large-scale labeled data are generally required to train deep neural networks
    in order to obtain better performance in visual feature learning from images or
    videos for computer vision applications. To avoid extensive cost of collecting
    and annotating large-scale datasets, as a subset of unsupervised learning methods,
    self-supervised learning methods are proposed to learn general image and video
    features from large-scale unlabeled data without using any human-annotated labels.
    This paper provides an extensive review of deep learning-based self-supervised
    general visual feature learning methods from images or videos. First, the motivation,
    general pipeline, and terminologies of this field are described. Then the common
    deep neural network architectures that used for self-supervised learning are summarized.
    Next, the schema and evaluation metrics of self-supervised learning methods are
    reviewed followed by the commonly used image and video datasets and the existing
    self-supervised visual feature learning methods. Finally, quantitative performance
    comparisons of the reviewed methods on benchmark datasets are summarized and discussed
    for both image and video feature learning. At last, this paper is concluded and
    lists a set of promising future directions for self-supervised visual feature
    learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模标注数据通常是训练深度神经网络的必要条件，以在计算机视觉应用中从图像或视频中获得更好的视觉特征学习效果。为了避免收集和标注大规模数据集的高昂成本，作为无监督学习方法的一个子集，自监督学习方法被提出用于从大规模未标记数据中学习通用图像和视频特征，而无需使用任何人工标注的标签。本文提供了基于深度学习的自监督通用视觉特征学习方法的广泛综述。首先，描述了该领域的动机、一般流程和术语。然后，总结了用于自监督学习的常见深度神经网络架构。接下来，回顾了自监督学习方法的模式和评估指标，随后介绍了常用的图像和视频数据集以及现有的自监督视觉特征学习方法。最后，总结和讨论了所评审方法在基准数据集上的定量性能比较，涵盖了图像和视频特征学习。最后，本文作出结论并列出了一组自监督视觉特征学习的有前景的未来方向。
- en: 'Index Terms:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Self-supervised Learning, Unsupervised Learning, Convolutional Neural Network,
    Transfer Learning, Deep Learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习、无监督学习、卷积神经网络、迁移学习、深度学习。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 1.1 Motivation
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 动机
- en: 'Due to the powerful ability to learn different levels of general visual features,
    deep neural networks have been used as the basic structure to many computer vision
    applications such as object detection [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)],
    semantic segmentation [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)], image
    captioning [[7](#bib.bib7)], etc. The models trained from large-scale image datasets
    like ImageNet are widely used as the pre-trained models and fine-tuned for other
    tasks for two main reasons: (1) the parameters learned from large-scale diverse
    datasets provide a good starting point, therefore, networks training on other
    tasks can converge faster, (2) the network trained on large-scale datasets already
    learned the hierarchy features which can help to reduce over-fitting problem during
    the training of other tasks, especially when datasets of other tasks are small
    or training labels are scarce.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于强大的学习不同层次通用视觉特征的能力，深度神经网络已经被用作许多计算机视觉应用的基础结构，如目标检测 [[1](#bib.bib1)、[2](#bib.bib2)、[3](#bib.bib3)]、语义分割
    [[4](#bib.bib4)、[5](#bib.bib5)、[6](#bib.bib6)]、图像描述 [[7](#bib.bib7)] 等。从大规模图像数据集如
    ImageNet 训练出的模型被广泛用作预训练模型，并针对其他任务进行微调，主要有两个原因：（1）从大规模多样数据集中学习到的参数提供了一个良好的起点，因此，训练其他任务的网络可以更快收敛，（2）在大规模数据集上训练的网络已经学会了层次特征，这可以帮助减少在其他任务训练过程中，尤其是在其他任务的数据集较小或训练标签稀缺时的过拟合问题。
- en: The performance of deep convolutional neural networks (ConvNets) greatly depends
    on their capability and the amount of training data. Different kinds of network
    architectures were developed to increase the capacity of network models, and larger
    and larger datasets were collected these days. Various networks including AlexNet
    [[8](#bib.bib8)], VGG [[9](#bib.bib9)], GoogLeNet [[10](#bib.bib10)], ResNet [[11](#bib.bib11)],
    and DenseNet [[12](#bib.bib12)] and large scale datasets such as ImageNet [[13](#bib.bib13)],
    OpenImage [[14](#bib.bib14)] have been proposed to train very deep ConvNets. With
    the sophisticated architectures and large-scale datasets, the performance of ConvNets
    keeps breaking the state-of-the-arts for many computer vision tasks [[15](#bib.bib15),
    [1](#bib.bib1), [4](#bib.bib4), [7](#bib.bib7), [16](#bib.bib16)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度卷积神经网络（ConvNets）的性能在很大程度上依赖于其能力和训练数据的数量。为了增加网络模型的容量，开发了不同类型的网络架构，并且这些天收集了越来越大的数据集。各种网络，包括
    AlexNet [[8](#bib.bib8)]、VGG [[9](#bib.bib9)]、GoogLeNet [[10](#bib.bib10)]、ResNet
    [[11](#bib.bib11)] 和 DenseNet [[12](#bib.bib12)]，以及大规模数据集如 ImageNet [[13](#bib.bib13)]
    和 OpenImage [[14](#bib.bib14)]，被提出用于训练非常深的 ConvNets。凭借复杂的架构和大规模的数据集，ConvNets 在许多计算机视觉任务中不断突破**最先进的水平**
    [[15](#bib.bib15)、[1](#bib.bib1)、[4](#bib.bib4)、[7](#bib.bib7)、[16](#bib.bib16)]。
- en: However, collection and annotation of large-scale datasets are time-consuming
    and expensive. As one of the most widely used datasets for pre-training very deep
    2D convolutional neural networks (2DConvNets), ImageNet [[13](#bib.bib13)] contains
    about $1.3$ million labeled images covering $1,000$ classes while each image is
    labeled by human workers with one class label. Compared to image datasets, collection
    and annotation of video datasets are more expensive due to the temporal dimension.
    The Kinetics dataset [[17](#bib.bib17)], which is mainly used to train ConvNets
    for video human action recognition, consists of $500,000$ videos belonging to
    $600$ categories and each video lasts around $10$ seconds. It took many Amazon
    Turk workers a lot of time to collect and annotate a dataset at such a large scale.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，收集和标注大规模数据集是耗时且昂贵的。作为一种用于预训练非常深的 2D 卷积神经网络（2DConvNets）的最广泛使用的数据集之一，ImageNet
    [[13](#bib.bib13)] 包含约 $1.3$ 百万张标注图像，覆盖 $1,000$ 个类别，每张图像由人工标注一个类别标签。与图像数据集相比，由于时间维度的存在，视频数据集的收集和标注成本更高。Kinetics
    数据集 [[17](#bib.bib17)]，主要用于训练用于视频人类动作识别的 ConvNets，由 $500,000$ 个视频组成，涵盖 $600$ 个类别，每个视频时长约
    $10$ 秒。许多 Amazon Turk 工作者花费了大量时间来收集和标注这样大规模的数据集。
- en: '![Refer to caption](img/f5c351dc9059b2dbb3ee4c4b109ec424.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f5c351dc9059b2dbb3ee4c4b109ec424.png)'
- en: 'Figure 1: The general pipeline of self-supervised learning. The visual feature
    is learned through the process of training ConvNets to solve a pre-defined pretext
    task. After self-supervised pretext task training finished, the learned parameters
    serve as a pre-trained model and are transferred to other downstream computer
    vision tasks by fine-tuning. The performance on these downstream tasks is used
    to evaluate the quality of the learned features. During the knowledge transfer
    for downstream tasks, the general features from only the first several layers
    are unusually transferred to downstream tasks.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：自监督学习的一般流程。视觉特征通过训练ConvNets解决预定义的预训练任务的过程来学习。在自监督预训练任务训练完成后，学习到的参数作为预训练模型被转移到其他下游计算机视觉任务中，通过微调来使用。这些下游任务的表现用来评估学习到的特征的质量。在下游任务的知识迁移过程中，通常只有前几层的通用特征会被转移到下游任务中。
- en: 'To avoid time-consuming and expensive data annotations, many self-supervised
    methods were proposed to learn visual features from large-scale unlabeled images
    or videos without using any human annotations. To learn visual features from unlabeled
    data, a popular solution is to propose various pretext tasks for networks to solve,
    while the networks can be trained by learning objective functions of the pretext
    tasks and the features are learned through this process. Various pretext tasks
    have been proposed for self-supervised learning including colorizing grayscale
    images [[18](#bib.bib18)], image inpainting [[19](#bib.bib19)], image jigsaw puzzle
    [[20](#bib.bib20)], etc. The pretext tasks share two common properties: (1) visual
    features of images or videos need to be captured by ConvNets to solve the pretext
    tasks, (2) pseudo labels for the pretext task can be automatically generated based
    on the attributes of images or videos.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免耗时且昂贵的数据注释，提出了许多自监督方法，以从大规模未标记的图像或视频中学习视觉特征，而不使用任何人工注释。为了从未标记的数据中学习视觉特征，一种流行的解决方案是提出各种预训练任务供网络解决，同时网络可以通过学习预训练任务的目标函数来训练，特征也通过这个过程学习。已经提出了各种用于自监督学习的预训练任务，包括为灰度图像着色[[18](#bib.bib18)]、图像修复[[19](#bib.bib19)]、图像拼图[[20](#bib.bib20)]等。预训练任务具有两个共同特征：(1)
    需要通过ConvNets捕捉图像或视频的视觉特征以解决预训练任务，(2) 伪标签可以基于图像或视频的属性自动生成。
- en: 'The general pipeline of self-supervised learning is shown in Fig. [1](#S1.F1
    "Figure 1 ‣ 1.1 Motivation ‣ 1 Introduction ‣ Self-supervised Visual Feature Learning
    with Deep Neural Networks: A Survey"). During the self-supervised training phase,
    a pre-defined pretext task is designed for ConvNets to solve, and the pseudo labels
    for the pretext task are automatically generated based on some attributes of data.
    Then the ConvNet is trained to learn object functions of the pretext task. After
    the self-supervised training finished, the learned visual features can be further
    transferred to downstream tasks (especially when only relatively small data available)
    as pre-trained models to improve performance and overcome over-fitting. Generally,
    shallow layers capture general low-level features like edges, corners, and textures
    while deeper layers capture task related high-level features. Therefore, visual
    features from only the first several layers are transferred during the supervised
    downstream task training phase.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '自监督学习的一般流程如图[1](#S1.F1 "Figure 1 ‣ 1.1 Motivation ‣ 1 Introduction ‣ Self-supervised
    Visual Feature Learning with Deep Neural Networks: A Survey")所示。在自监督训练阶段，设计了一个预定义的预训练任务供ConvNets解决，预训练任务的伪标签是基于数据的一些属性自动生成的。然后，ConvNet被训练以学习预训练任务的目标函数。自监督训练完成后，学习到的视觉特征可以进一步转移到下游任务（特别是在只有相对少量数据可用时）作为预训练模型，以提高性能并克服过拟合。一般而言，浅层捕捉一般的低级特征，如边缘、角点和纹理，而深层捕捉与任务相关的高级特征。因此，在监督下游任务训练阶段，只有前几层的视觉特征被转移。'
- en: 1.2 Term Definition
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 术语定义
- en: To make this survey easy to read, we first define the terms used in the remaining
    sections.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使本调查易于阅读，我们首先定义剩余部分中使用的术语。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Human-annotated label: Human-annotated labels refer to labels of data that
    are manually annotated by human workers.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人工标注标签：人工标注标签是指由人工工作人员手动注释的数据标签。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pseudo label: Pseudo labels are automatically generated labels based on data
    attributes for pretext tasks.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 伪标签：伪标签是基于数据属性为预训练任务自动生成的标签。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pretext Task: Pretext tasks are pre-designed tasks for networks to solve, and
    visual features are learned by learning objective functions of pretext tasks.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预任务：预任务是为网络解决的预设计任务，视觉特征通过学习预任务的目标函数来学习。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Downstream Task: Downstream tasks are computer vision applications that are
    used to evaluate the quality of features learned by self-supervised learning.
    These applications can greatly benefit from the pre-trained models when training
    data are scarce. In general, human-annotated labels are needed to solve the downstream
    tasks. However, in some applications, the downstream task can be the same as the
    pretext task without using any human-annotated labels.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下游任务：下游任务是用于评估自监督学习所学特征质量的计算机视觉应用。当训练数据稀缺时，这些应用可以大大受益于预训练模型。通常，下游任务需要人工标注标签。然而，在某些应用中，下游任务可以与预任务相同，而无需使用任何人工标注标签。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Supervised Learning: Supervised learning indicates learning methods using data
    with fine-grained human-annotated labels to train networks.'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 监督学习：监督学习指的是使用带有细粒度人工标注标签的数据来训练网络的学习方法。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Semi-supervised Learning: Semi-supervised learning refers to learning methods
    using a small amount of labeled data in conjunction with a large amount of unlabeled
    data.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 半监督学习：半监督学习是指使用少量标记数据与大量未标记数据结合的学习方法。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Weakly-supervised Learning: Weakly supervised learning refers to learning methods
    to learn with coarse-grained labels or inaccurate labels. The cost of obtaining
    weak supervision labels is generally much cheaper than fine-grained labels for
    supervised methods.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 弱监督学习：弱监督学习是指使用粗粒度标签或不准确标签进行学习的方法。获取弱监督标签的成本通常比获取细粒度标签便宜得多。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Unsupervised Learning: Unsupervised learning refers to learning methods without
    using any human-annotated labels.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无监督学习：无监督学习是指不使用任何人工标注标签的学习方法。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Self-supervised Learning: Self-supervised learning is a subset of unsupervised
    learning methods. Self-supervised learning refers to learning methods in which
    ConvNets are explicitly trained with automatically generated labels. This review
    only focuses on self-supervised learning methods for visual feature learning with
    ConvNets in which the features can be transferred to multiple different computer
    vision tasks.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自监督学习：自监督学习是无监督学习方法的一个子集。自监督学习指的是通过自动生成标签明确训练ConvNets的学习方法。本综述仅关注用于视觉特征学习的自监督学习方法，其中特征可以转移到多个不同的计算机视觉任务。
- en: 'Since no human annotations are needed to generate pseudo labels during self-supervised
    training, very large-scale datasets can be used for self-supervised training.
    Trained with these pseudo labels, self-supervised methods achieved promising results
    and the gap with supervised methods in performance on downstream tasks becomes
    smaller. This paper provides a comprehensive survey of deep ConvNets-based self-supervised
    visual feature learning methods. The key contributions of this paper are as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自监督训练过程中不需要人工注释来生成伪标签，因此可以使用非常大规模的数据集进行自监督训练。通过这些伪标签进行训练，自监督方法取得了令人鼓舞的结果，并且在下游任务上的性能差距变小。本文提供了基于深度ConvNets的自监督视觉特征学习方法的综合调查。本文的主要贡献如下：
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, this is the first comprehensive survey about self-supervised
    visual feature learning with deep ConvNets which will be helpful for researchers
    in this field.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是关于深度ConvNets自监督视觉特征学习的首个综合调查，对该领域的研究人员将非常有帮助。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An in-depth review of recently developed self-supervised learning methods and
    datasets.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对最近开发的自监督学习方法和数据集进行了深入综述。
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Quantitative performance analysis and comparison of the existing methods are
    provided.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供了现有方法的定量性能分析和比较。
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A set of possible future directions for self-supervised learning is pointed
    out.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指出了自监督学习的若干可能未来方向。
- en: 2 Formulation of Different Learning Schemas
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同学习方案的公式化
- en: 'Based on the training labels, visual feature learning methods can be grouped
    into the following four categories: supervised, semi-supervised, weakly supervised,
    and unsupervised. In this section, the four types of learning methods are compared
    and key terminologies are defined.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 基于训练标签，视觉特征学习方法可以分为以下四类：监督、半监督、弱监督和无监督。在本节中，将对这四种学习方法进行比较，并定义关键术语。
- en: 2.1 Supervised Learning Formulation
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 监督学习的公式
- en: 'For supervised learning, given a dataset X, for each data $X_{i}$ in X, there
    is a corresponding human-annotated label $Y_{i}$. For a set of $N$ labeled training
    data $D=\{X_{i}\}_{i=0}^{N}$, the training loss function is defined as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于监督学习，给定一个数据集 X，X 中的每个数据 $X_{i}$ 都有一个对应的人类标注标签 $Y_{i}$。对于一组 $N$ 个标记训练数据 $D=\{X_{i}\}_{i=0}^{N}$，训练损失函数定义为：
- en: '|  | $loss(D)=\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}loss(X_{i},Y_{i}).$ |  |
    (1) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $loss(D)=\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}loss(X_{i},Y_{i}).$ |  |
    (1) |'
- en: Trained with accurate human-annotated labels, the supervised learning methods
    obtained break-through results on different computer vision applications [[8](#bib.bib8),
    [4](#bib.bib4), [1](#bib.bib1), [16](#bib.bib16)]. However, data collection and
    annotation usually are expensive and may require special skills. Therefore, semi-supervised,
    weakly supervised, and unsupervised learning methods were proposed to reduce the
    cost.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通过准确的人类标注标签进行训练，监督学习方法在不同的计算机视觉应用中取得了突破性的结果 [[8](#bib.bib8), [4](#bib.bib4),
    [1](#bib.bib1), [16](#bib.bib16)]。然而，数据收集和标注通常成本高昂，可能需要特殊技能。因此，提出了半监督、弱监督和无监督学习方法以降低成本。
- en: 2.2 Semi-Supervised Learning Formulation
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 半监督学习的公式
- en: 'For semi-supervised visual feature learning, given a small labeled dataset
    $X$ and a large unlabeled dataset $Z$, for each data $X_{i}$ in X, there is a
    corresponding human-annotated label $Y_{i}$. For a set of $N$ labeled training
    data $D_{1}=\{X_{i}\}_{i=0}^{N}$ and $M$ unlabeled training data $D_{2}=\{Z_{i}\}_{i=0}^{M}$,
    the training loss function is defined as:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于半监督视觉特征学习，给定一个小的标记数据集 $X$ 和一个大规模的未标记数据集 $Z$，X 中的每个数据 $X_{i}$ 都有一个对应的人类标注标签
    $Y_{i}$。对于一组 $N$ 个标记训练数据 $D_{1}=\{X_{i}\}_{i=0}^{N}$ 和 $M$ 个未标记训练数据 $D_{2}=\{Z_{i}\}_{i=0}^{M}$，训练损失函数定义为：
- en: '|  | $\small loss(D_{1},D_{2})=\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}loss(X_{i},Y_{i})+\frac{1}{M}\sum_{i=1}^{M}loss(Z_{i},R(Z_{i},X)),$
    |  | (2) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small loss(D_{1},D_{2})=\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}loss(X_{i},Y_{i})+\frac{1}{M}\sum_{i=1}^{M}loss(Z_{i},R(Z_{i},X)),$
    |  | (2) |'
- en: where the $R(Z_{i},X)$ is a task-specific function to represent the relation
    between each unlabeled training data $Z_{i}$ with the labeled dataset $X$.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$R(Z_{i},X)$ 是一个任务特定的函数，用于表示每个未标记训练数据 $Z_{i}$ 与标记数据集 $X$ 之间的关系。
- en: 2.3 Weakly Supervised Learning Formulation
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 弱监督学习的公式
- en: 'For weakly supervised visual feature learning, given a dataset X, for each
    data $X_{i}$ in X, there is a corresponding coarse-grained label $C_{i}$. For
    a set of $N$ training data $D=\{X_{i}\}_{i=0}^{N}$, the training loss function
    is defined as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于弱监督视觉特征学习，给定一个数据集 X，X 中的每个数据 $X_{i}$ 都有一个对应的粗粒度标签 $C_{i}$。对于一组 $N$ 个训练数据 $D=\{X_{i}\}_{i=0}^{N}$，训练损失函数定义为：
- en: '|  | $loss(D)=\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}loss(X_{i},C_{i}).$ |  |
    (3) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $loss(D)=\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}loss(X_{i},C_{i}).$ |  |
    (3) |'
- en: Since the cost of weak supervision is much lower than the fine-grained label
    for supervised methods, large-scale datasets are relatively easier to obtain.
    Recently, several papers proposed to learn image features from web collected images
    using hashtags as category labels [[21](#bib.bib21), [22](#bib.bib22)], and obtained
    very good performance [[21](#bib.bib21)].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于弱监督的成本远低于监督方法所需的精细标签，大规模数据集相对较容易获得。最近，一些论文提出利用网页收集的图像中的标签作为类别标签来学习图像特征 [[21](#bib.bib21),
    [22](#bib.bib22)]，并取得了非常好的性能 [[21](#bib.bib21)]。
- en: 2.4 Unsupervised Learning Formulation
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 无监督学习的公式
- en: Unsupervised learning refers to learning methods that do not need any human-annotated
    labels. This type of methods including fully unsupervised learning methods in
    which the methods do not need any labels at all, as well as self-supervised learning
    methods in which networks are explicitly trained with automatically generated
    pseudo labels without involving any human annotation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习指的是不需要任何人类标注标签的学习方法。这类方法包括完全无监督学习方法，即完全不需要任何标签的方法，以及自监督学习方法，即网络通过自动生成的伪标签进行训练，而无需涉及任何人工标注。
- en: 2.4.1 Self-supervised Learning
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1 自监督学习
- en: Recently, many self-supervised learning methods for visual feature learning
    have been developed without using any human-annotated labels [[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [33](#bib.bib33), [35](#bib.bib35)]. Some papers refer to this
    type of learning methods as unsupervised learning [[36](#bib.bib36), [37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48)]. Compared to supervised learning methods which require a data
    pair $X_{i}$ and $Y_{i}$ while $Y_{i}$ is annotated by human labors, self-supervised
    learning also trained with data $X_{i}$ along with its pseudo label $P_{i}$ while
    $P_{i}$ is automatically generated for a pre-defined pretext task without involving
    any human annotation. The pseudo label $P_{i}$ can be generated by using attributes
    of images or videos such as the context of images [[20](#bib.bib20), [36](#bib.bib36),
    [18](#bib.bib18), [19](#bib.bib19)], or by traditional hand-designed methods [[49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51)].
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，许多用于视觉特征学习的自监督学习方法在没有使用任何人工标注标签的情况下被开发出来[[23](#bib.bib23), [24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34),
    [33](#bib.bib33), [35](#bib.bib35)]。一些论文将这种类型的学习方法称为无监督学习[[36](#bib.bib36), [37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48)]。与需要数据对$X_{i}$和$Y_{i}$的监督学习方法相比，其中$Y_{i}$由人工标注，自监督学习也使用数据$X_{i}$进行训练，同时其伪标签$P_{i}$是为预定义的前置任务自动生成的，不涉及任何人工标注。伪标签$P_{i}$可以通过使用图像或视频的属性生成，例如图像的上下文[[20](#bib.bib20),
    [36](#bib.bib36), [18](#bib.bib18), [19](#bib.bib19)]，或通过传统的手工设计方法[[49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51)]。
- en: 'Given a set of $N$ training data $D=\{P_{i}\}_{i=0}^{N}$, the training loss
    function is defined as:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组$N$训练数据$D=\{P_{i}\}_{i=0}^{N}$，训练损失函数定义为：
- en: '|  | $loss(D)=\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}loss(X_{i},P_{i}).$ |  |
    (4) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $loss(D)=\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}loss(X_{i},P_{i}).$ |  |
    (4) |'
- en: As long as the pseudo labels $P$ are automatically generated without involving
    human annotations, then the methods belong to self-supervised learning. Recently,
    self-supervised learning methods have achieved great progress. This paper focuses
    on the self-supervised learning methods that mainly designed for visual feature
    learning, while the features have the ability to be transferred to multiple visual
    tasks and to perform new tasks by learning from limited labeled data. This paper
    summarizes these self-supervised feature learning methods from different perspectives
    including network architectures, commonly used pretext tasks, datasets, and applications,
    etc.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 只要伪标签$P$是自动生成的而不涉及人工标注，那么这些方法就属于自监督学习。最近，自监督学习方法取得了巨大进展。本文重点关注主要为视觉特征学习设计的自监督学习方法，同时这些特征具备迁移到多个视觉任务并通过从有限标注数据中学习来执行新任务的能力。本文从网络架构、常用前置任务、数据集和应用等不同角度总结了这些自监督特征学习方法。
- en: 3 Common Deep Network Architectures
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 常见深度网络架构
- en: No matter the categories of learning methods, they share similar network architectures.
    This section reviews common architectures for learning both image and video features.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 无论学习方法的类别如何，它们都共享类似的网络架构。本节回顾了用于学习图像和视频特征的常见架构。
- en: 3.1 Architectures for Learning Image Features
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 学习图像特征的架构
- en: Various 2DConvNets have been designed for image feature learning. Here, five
    milestone architectures for image feature learning including AlexNet [[8](#bib.bib8)],
    VGG [[9](#bib.bib9)], GoogLeNet [[10](#bib.bib10)], ResNet [[11](#bib.bib11)],
    and DenseNet [[12](#bib.bib12)] are reviewed.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 各种2DConvNets已经被设计用于图像特征学习。在这里，回顾了五种图像特征学习的里程碑架构，包括AlexNet[[8](#bib.bib8)], VGG[[9](#bib.bib9)],
    GoogLeNet[[10](#bib.bib10)], ResNet[[11](#bib.bib11)], 和DenseNet[[12](#bib.bib12)]。
- en: 3.1.1 AlexNet
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 AlexNet
- en: 'AlexNet obtained a big improvement in the performance of image classification
    on ImageNet dataset compared to the previous state-of-the-art methods [[8](#bib.bib8)].
    With the support of powerful GPUs, AlexNet which has $62.4$ million parameters
    were trained on ImageNet with $1.3$ million images. As shown in Fig. [2](#S3.F2
    "Figure 2 ‣ 3.1.1 AlexNet ‣ 3.1 Architectures for Learning Image Features ‣ 3
    Common Deep Network Architectures ‣ Self-supervised Visual Feature Learning with
    Deep Neural Networks: A Survey"), the architecture of AlexNet has $8$ layers in
    which $5$ are convolutional layers and $3$ are fully connected layers. The ReLU
    is applied after each convolutional layers. $94$% of the network parameters come
    from the fully connected layers. With this scale of parameters, the network can
    easily be over-fitting. Therefore, different kinds of techniques are applied to
    avoid over-fitting problem including data augmentation, dropout, and normalization.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '相较于之前的最先进方法，AlexNet 在 ImageNet 数据集上的图像分类性能有了很大改进 [[8](#bib.bib8)]。在强大 GPU 的支持下，具有
    $62.4$ 百万参数的 AlexNet 在包含 $1.3$ 百万张图像的 ImageNet 上进行了训练。如图 [2](#S3.F2 "Figure 2
    ‣ 3.1.1 AlexNet ‣ 3.1 Architectures for Learning Image Features ‣ 3 Common Deep
    Network Architectures ‣ Self-supervised Visual Feature Learning with Deep Neural
    Networks: A Survey") 所示，AlexNet 的架构有 $8$ 层，其中 $5$ 层是卷积层，$3$ 层是全连接层。ReLU 应用于每个卷积层后。$94$%
    的网络参数来自全连接层。由于这个参数规模，网络容易发生过拟合。因此，采用了包括数据增强、dropout 和归一化在内的不同技术来避免过拟合问题。'
- en: '![Refer to caption](img/4e04ead76e6430ba2da6d81f42da2175.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4e04ead76e6430ba2da6d81f42da2175.png)'
- en: 'Figure 2: The architecture of AlexNet [[8](#bib.bib8)]. The numbers indicate
    the number of channels of each feature map. Figure is reproduced based on AlexNet
    [[8](#bib.bib8)].'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: AlexNet 的架构 [[8](#bib.bib8)]。数字表示每个特征图的通道数。图基于 AlexNet [[8](#bib.bib8)]
    复制。'
- en: 3.1.2 VGG
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 VGG
- en: 'VGG is proposed by Simonyan and Zisserman and won the first place for ILSVRC
    2013 competition [[9](#bib.bib9)]. Simonyan and Zisserman proposed various depth
    of networks, while the 16-layer VGG is the most widely used one due to its moderate
    model size and its superior performance. The architecture of VGG-16 is shown in
    Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.2 VGG ‣ 3.1 Architectures for Learning Image Features
    ‣ 3 Common Deep Network Architectures ‣ Self-supervised Visual Feature Learning
    with Deep Neural Networks: A Survey"). It has $16$ convolutional layers belong
    to five convolution blocks. The main difference between VGG and AlexNet is that
    AlexNet has large convolution stride and large kernel size while all the convolution
    kernels in VGG have same small size ($3\times 3$) and small convolution stride
    ($1\times 1$). The large kernel size leads to too many parameters and large model
    size, while the large convolution stride may cause the network to miss some fine
    features in the lower layers. The smaller kernel size makes the training of very
    deep convolution neural network feasible while still reserving the fine-grained
    information in the network.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 'VGG 由 Simonyan 和 Zisserman 提出，并在 ILSVRC 2013 比赛中获得第一名 [[9](#bib.bib9)]。Simonyan
    和 Zisserman 提出了各种深度的网络，其中 16 层 VGG 由于其适中的模型大小和优越的性能成为最广泛使用的。VGG-16 的架构如图 [3](#S3.F3
    "Figure 3 ‣ 3.1.2 VGG ‣ 3.1 Architectures for Learning Image Features ‣ 3 Common
    Deep Network Architectures ‣ Self-supervised Visual Feature Learning with Deep
    Neural Networks: A Survey") 所示。它有 $16$ 个卷积层，属于五个卷积块。VGG 和 AlexNet 的主要区别在于 AlexNet
    具有较大的卷积步幅和较大的核大小，而 VGG 的所有卷积核都有相同的小尺寸 ($3\times 3$) 和小的卷积步幅 ($1\times 1$)。较大的核大小导致参数过多和模型尺寸较大，而较大的卷积步幅可能导致网络在较低层错过一些细微特征。较小的核大小使得训练非常深的卷积神经网络成为可能，同时仍然保留网络中的细粒度信息。'
- en: '![Refer to caption](img/c3bdd5cbcb0b6b65ba384beb086bfb8d.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c3bdd5cbcb0b6b65ba384beb086bfb8d.png)'
- en: 'Figure 3: The architecture of VGG [[9](#bib.bib9)]. Figure is reproduced based
    on VGG [[9](#bib.bib9)].'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: VGG 的架构 [[9](#bib.bib9)]。图基于 VGG [[9](#bib.bib9)] 复制。'
- en: 3.1.3 ResNet
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 ResNet
- en: 'VGG demonstrated that deeper networks are possible to obtain better performance.
    However, deeper networks are more difficult to train due to two problems: gradient
    vanishing and gradient explosion. ResNet is proposed by He et al. to use the skip
    connection in convolution blocks by sending the previous feature map to the next
    convolution block to overcome the gradient vanishing and gradient explosion [[11](#bib.bib11)].
    The details of the skip connection are shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.3
    ResNet ‣ 3.1 Architectures for Learning Image Features ‣ 3 Common Deep Network
    Architectures ‣ Self-supervised Visual Feature Learning with Deep Neural Networks:
    A Survey"). With the skip connection, training of very deep neural networks on
    GPUs becomes feasible.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: VGG 证明了更深的网络能够获得更好的性能。然而，更深的网络由于梯度消失和梯度爆炸两个问题而更难以训练。He 等人提出了 ResNet，通过在卷积块中使用跳跃连接，将先前的特征图发送到下一个卷积块，以克服梯度消失和梯度爆炸
    [[11](#bib.bib11)]。跳跃连接的细节如图 [4](#S3.F4 "图 4 ‣ 3.1.3 ResNet ‣ 3.1 图像特征学习架构 ‣ 3
    常见深度网络架构 ‣ 自监督视觉特征学习：深度神经网络的综述") 所示。通过跳跃连接，GPU 上训练非常深的神经网络变得可行。
- en: '![Refer to caption](img/ec9c7413e370b60f3ea28f513f133977.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec9c7413e370b60f3ea28f513f133977.png)'
- en: 'Figure 4: The architecture of Residual block [[11](#bib.bib11)]. The identity
    mapping can effectively reduce gradient vanishing and explosion which make the
    training of very deep network feasible. Figure is reproduced based on ResNet [[11](#bib.bib11)].'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 残差块的架构 [[11](#bib.bib11)]。恒等映射可以有效减少梯度消失和梯度爆炸，从而使得训练非常深的网络成为可能。图像基于 ResNet
    [[11](#bib.bib11)] 进行再现。'
- en: In ResNet [[11](#bib.bib11)], He et al. also evaluated networks with different
    depths for image classification. Due to its smaller model size and superior performance,
    ResNet is often used as the base network for other computer vision tasks. The
    convolution blocks with skip connection also widely used as the basic building
    blocks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ResNet [[11](#bib.bib11)] 中，He 等人还评估了具有不同深度的网络用于图像分类。由于其较小的模型尺寸和卓越的性能，ResNet
    通常作为其他计算机视觉任务的基础网络。具有跳跃连接的卷积块也广泛用作基本构建模块。
- en: 3.1.4 GoogLeNet
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 GoogLeNet
- en: '![Refer to caption](img/d24244214572d933b39e2c516637117f.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d24244214572d933b39e2c516637117f.png)'
- en: 'Figure 5: The architecture of Inception block [[10](#bib.bib10)]. Figure is
    reproduced based on GoogLeNet [[10](#bib.bib10)].'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: Inception 块的架构 [[10](#bib.bib10)]。图像基于 GoogLeNet [[10](#bib.bib10)] 进行再现。'
- en: 'GoogLeNet, a $22$-layer deep network, is proposed by Szegedy et al. which won
    ILSVRC-2014 challenge with a top-5 test accuracy of $93.3$% [[10](#bib.bib10)].
    Compared to previous work that to build a deeper network, Szegedy et al. explored
    to build a wider network in which each layer has multiple parallel convolution
    layers. The basic block of GoogLeNet is inception block which consists of $4$
    parallel convolution layers with different kernel sizes and followed by $1\times
    1$ convolution for dimension reduction purpose. The architecture for the inception
    block of GoogLeNet is shown in Fig. [5](#S3.F5 "Figure 5 ‣ 3.1.4 GoogLeNet ‣ 3.1
    Architectures for Learning Image Features ‣ 3 Common Deep Network Architectures
    ‣ Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey").
    With a carefully crafted design, they increased the depth and width of the network
    while keeping the computational cost constant.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: GoogLeNet 是一个 $22$ 层深的网络，由 Szegedy 等人提出，并在 ILSVRC-2014 挑战中获得了 $93.3$% 的 top-5
    测试准确率 [[10](#bib.bib10)]。与之前的工作试图构建更深的网络不同，Szegedy 等人探索了构建更宽的网络，其中每一层具有多个并行的卷积层。GoogLeNet
    的基本块是 inception 块，由 $4$ 个具有不同卷积核大小的并行卷积层组成，随后进行 $1\times 1$ 卷积以进行维度缩减。GoogLeNet
    的 inception 块架构如图 [5](#S3.F5 "图 5 ‣ 3.1.4 GoogLeNet ‣ 3.1 图像特征学习架构 ‣ 3 常见深度网络架构
    ‣ 自监督视觉特征学习：深度神经网络的综述") 所示。通过精心设计，他们在保持计算成本不变的情况下增加了网络的深度和宽度。
- en: 3.1.5 DenseNet
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5 DenseNet
- en: '![Refer to caption](img/a9571d5ce74f8834d8fe7eaf02a512ba.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a9571d5ce74f8834d8fe7eaf02a512ba.png)'
- en: 'Figure 6: The architecture of the Dense Block proposed in DenseNet [[12](#bib.bib12)].
    Figure is reproduced based on [[12](#bib.bib12)].'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: DenseNet [[12](#bib.bib12)] 中提出的 Dense Block 的架构。图像基于 [[12](#bib.bib12)]
    进行再现。'
- en: Most of the networks including AlexNet, VGG, and ResNet follow a hierarchy architecture.
    The images are fed to the network and features are extracted by different layers.
    The shallow layers extract low-level general features, while the deep layers extract
    high-level task-specific features [[52](#bib.bib52)]. However, when a network
    goes deeper, the deeper layers may suffer from memorizing the low-level features
    needed by the network to accomplish the task.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数网络，包括AlexNet、VGG和ResNet，都遵循层次结构。图像被输入到网络中，并通过不同的层提取特征。浅层提取低级通用特征，而深层提取高级任务特定特征[[52](#bib.bib52)]。然而，当网络变得更深时，较深的层可能会面临记住网络完成任务所需的低级特征的困难。
- en: 'To alleviate this problem, Huang et al. proposed the dense connection to send
    all the features before a convolution block as the input to the next convolution
    block in the neural network [[12](#bib.bib12)]. As shown in Fig. [6](#S3.F6 "Figure
    6 ‣ 3.1.5 DenseNet ‣ 3.1 Architectures for Learning Image Features ‣ 3 Common
    Deep Network Architectures ‣ Self-supervised Visual Feature Learning with Deep
    Neural Networks: A Survey"), the output features of all the previous convolution
    blocks serve as the input to the current block. In this way, the shallower blocks
    focus on the low-level general features while the deeper blocks can focus on the
    high-level task-specific features.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这个问题，黄等人提出了密集连接（dense connection），将所有卷积块之前的特征作为输入传递给神经网络中的下一个卷积块[[12](#bib.bib12)]。如图[6](#S3.F6
    "图 6 ‣ 3.1.5 DenseNet ‣ 3.1 学习图像特征的架构 ‣ 3 常见深度网络架构 ‣ 自监督视觉特征学习：深度神经网络综述")所示，所有前面卷积块的输出特征作为当前块的输入。这样，较浅的块专注于低级的通用特征，而较深的块则可以专注于高级的任务特定特征。
- en: 3.2 Architectures for Learning Video Features
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 学习视频特征的架构
- en: To extract both spatial and temporal information from videos, several architectures
    have been designed for video feature learning including 2DConvNet-based methods
    [[53](#bib.bib53)], 3DConvNet-based methods [[16](#bib.bib16)], and LSTM-based
    methods [[54](#bib.bib54)]. The 2DConvNet-based methods apply 2DConvNet on every
    single frame and the image features of multiple frames are fused as video features.
    The 3DConvNet-based methods employ 3D convolution operation to simultaneously
    extract both spatial and temporal features from multiple frames. The LSTM-based
    methods employ LSTM to model long term dynamics within a video. This section briefly
    summarizes these three types of architectures of video feature learning.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从视频中提取空间和时间信息，已经设计了几种视频特征学习的架构，包括基于2DConvNet的方法[[53](#bib.bib53)]、基于3DConvNet的方法[[16](#bib.bib16)]和基于LSTM的方法[[54](#bib.bib54)]。基于2DConvNet的方法在每一帧上应用2DConvNet，并将多帧的图像特征融合为视频特征。基于3DConvNet的方法利用3D卷积操作同时从多帧中提取空间和时间特征。基于LSTM的方法使用LSTM建模视频中的长期动态。本节简要总结了这三种视频特征学习架构。
- en: 3.2.1 Two-Stream Network
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 双流网络
- en: '![Refer to caption](img/289f1c986efe0ea7c6c5b6ab0e5d1451.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/289f1c986efe0ea7c6c5b6ab0e5d1451.png)'
- en: 'Figure 7: The general architecture of the two-stream network which including
    one spatial stream and one temporal stream. Figure is reproduced based on [[53](#bib.bib53)].'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：包含一个空间流和一个时间流的双流网络的一般架构。图基于[[53](#bib.bib53)]复现。
- en: 'Videos generally are composed of various numbers of frames. To recognize actions
    in a video, networks are required to capture appearance features as well as temporal
    dynamics from frame sequences. As shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.2.1 Two-Stream
    Network ‣ 3.2 Architectures for Learning Video Features ‣ 3 Common Deep Network
    Architectures ‣ Self-supervised Visual Feature Learning with Deep Neural Networks:
    A Survey"), a two-stream 2DConvNet-based network is proposed by Simonyan and Zisserman
    for human action recognition, while using a 2DConvNet to capture spatial features
    from RGB stream and another 2DConvNet to capture temporal features from optical
    flow stream [[53](#bib.bib53)]. Optical flow encodes boundary of moving objects,
    therefore, the temporal stream ConvNet is relatively easier to capture the motion
    information within the frames.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '视频通常由各种数量的帧组成。为了识别视频中的动作，网络需要捕捉外观特征以及帧序列中的时间动态。如图 [7](#S3.F7 "Figure 7 ‣ 3.2.1
    Two-Stream Network ‣ 3.2 Architectures for Learning Video Features ‣ 3 Common
    Deep Network Architectures ‣ Self-supervised Visual Feature Learning with Deep
    Neural Networks: A Survey") 所示，Simonyan 和 Zisserman 提出了基于两流 2DConvNet 的网络用于人类动作识别，其中使用一个
    2DConvNet 从 RGB 流中捕捉空间特征，另一个 2DConvNet 从光流流中捕捉时间特征 [[53](#bib.bib53)]。光流编码了运动物体的边界，因此时间流
    ConvNet 相对较容易捕捉帧中的运动信息。'
- en: Experiments showed that the fusion of the two streams can significantly improve
    action recognition accuracy. Later, this work has been extended to multi-stream
    network [[55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58),
    [59](#bib.bib59)] to fuse features from different types of inputs such as dynamic
    images [[60](#bib.bib60)] and difference of frames [[61](#bib.bib61)].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，两个流的融合可以显著提高动作识别的准确性。后来，这项工作被扩展到多流网络 [[55](#bib.bib55), [56](#bib.bib56),
    [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59)]，融合来自不同类型输入的特征，如动态图像 [[60](#bib.bib60)]
    和帧差异 [[61](#bib.bib61)]。
- en: 3.2.2 Spatiotemporal Convolutional Neural Network
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 时空卷积神经网络
- en: 3D convolution operation was first proposed in 3DNet [[62](#bib.bib62)] for
    human action recognition. Compared to 2DConvNets which individually extract the
    spatial information of each frame and then fuse them together as video features,
    3DConvNets are able to simultaneously extract both spatial and temporal features
    from multiple frames.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 卷积操作最早在 3DNet [[62](#bib.bib62)] 中提出，用于人类动作识别。与 2DConvNets 单独提取每帧的空间信息并将它们融合为视频特征相比，3DConvNets
    能够同时从多个帧中提取空间和时间特征。
- en: C3D [[16](#bib.bib16)] is a VGG-like 11-layer 3DConvNet designed for human action
    recognition. The network contains $8$ convolutional layers, and $3$ fully connected
    layers. All the kernels have the size of $3\times 3\times 3$, the convolution
    stride is fixed to $1$ pixel. Due to its powerful ability of simultaneously extracting
    both spatial and temporal features from multiple frames, the network achieved
    state-of-the-art on several video analysis tasks including human action recognition
    [[63](#bib.bib63)], action similarity labeling [[64](#bib.bib64)], scene classification
    [[65](#bib.bib65)], and object recognition in videos [[66](#bib.bib66)].
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: C3D [[16](#bib.bib16)] 是一个类似于 VGG 的 11 层 3DConvNet，旨在用于人类动作识别。该网络包含 $8$ 个卷积层和
    $3$ 个全连接层。所有的卷积核大小为 $3\times 3\times 3$，卷积步幅固定为 $1$ 像素。由于其同时从多个帧中提取空间和时间特征的强大能力，该网络在包括人类动作识别
    [[63](#bib.bib63)]、动作相似性标注 [[64](#bib.bib64)]、场景分类 [[65](#bib.bib65)] 和视频中的物体识别
    [[66](#bib.bib66)] 等多个视频分析任务上达到了最先进水平。
- en: The input of C3D is $16$ consecutive RGB frames where the appearance and temporal
    cues from 16-frame clips are extracted. However, the paper of long-term temporal
    convolutions (LTC) [[67](#bib.bib67)] argues that, for the long-lasting actions,
    16 frames are insufficient to represent whole actions which last longer. Therefore,
    larger numbers of frames are employed to train 3DConvNets and achieved better
    performance than C3D [[67](#bib.bib67), [68](#bib.bib68)].
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: C3D 的输入是 $16$ 个连续的 RGB 帧，从 16 帧剪辑中提取外观和时间线索。然而，长期时间卷积 (LTC) 的论文 [[67](#bib.bib67)]
    认为，对于持续时间较长的动作，16 帧不足以表示持续时间更长的整个动作。因此，使用更多帧来训练 3DConvNets，并且表现优于 C3D [[67](#bib.bib67),
    [68](#bib.bib68)]。
- en: With the success of applying 3D convolution on video analysis tasks, various
    3DConvNet architectures have been proposed [[69](#bib.bib69), [70](#bib.bib70),
    [71](#bib.bib71)]. Hara et al. proposed 3DResNet by replacing all the 2D convolution
    layers in ResNet with 3D convolution layers and showed comparable performance
    with the state-of-the-art performance on action recognition task on several datasets
    [[70](#bib.bib70)].
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 随着将3D卷积应用于视频分析任务的成功，提出了各种3DConvNet架构[[69](#bib.bib69), [70](#bib.bib70), [71](#bib.bib71)]。Hara等人通过用3D卷积层替换ResNet中的所有2D卷积层，提出了3DResNet，并在多个数据集上的动作识别任务中展示了与最新技术相媲美的性能[[70](#bib.bib70)]。
- en: 3.2.3 Recurrent Neural Network
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 递归神经网络
- en: '![Refer to caption](img/246edfe2171a2c372ac449ed83e87f2a.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/246edfe2171a2c372ac449ed83e87f2a.png)'
- en: 'Figure 8: The architecture of long-term recurrent convolutional networks (LRCN)
    [[54](#bib.bib54)]. LSTM is employed to model the long term temporal information
    within a frame sequence. Figure is reproduced based on [[54](#bib.bib54)].'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：长期递归卷积网络（LRCN）的架构[[54](#bib.bib54)]。LSTM用于建模帧序列中的长期时间信息。图示基于[[54](#bib.bib54)]再现。
- en: Due to the ability to model the temporal dynamics within a sequence, recurrent
    neural networks (RNN) are often applied to videos as ordered frame sequences.
    Compared to standard RNN [[72](#bib.bib72)], long short term memory (LSTM) uses
    memory cells to store, modify, and access internal states, to better model the
    long-term temporal relationships within video frames [[73](#bib.bib73)].
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于能够建模序列中的时间动态，递归神经网络（RNN）通常被应用于作为有序帧序列的视频。与标准RNN[[72](#bib.bib72)]相比，长短期记忆（LSTM）使用记忆单元来存储、修改和访问内部状态，以更好地建模视频帧中的长期时间关系[[73](#bib.bib73)]。
- en: 'Based on the advantage of the LSTM, Donahue et al. proposed long-term recurrent
    convolutional networks (LRCN) for human action recognition [[54](#bib.bib54)].
    The framework of the LRCN is shown in Fig. [8](#S3.F8 "Figure 8 ‣ 3.2.3 Recurrent
    Neural Network ‣ 3.2 Architectures for Learning Video Features ‣ 3 Common Deep
    Network Architectures ‣ Self-supervised Visual Feature Learning with Deep Neural
    Networks: A Survey"). The LSTM is sequentially applied to the features extracted
    by ConvNets to model the temporal dynamics in the frame sequence. With the LSTM
    to model a video as frame sequences, this model is able to explicitly model the
    long-term temporal dynamics within a video. Later on, this model is extended to
    a deeper LSTM for action recognition [[74](#bib.bib74), [75](#bib.bib75)], video
    captioning [[76](#bib.bib76)], and gesture recognition tasks [[77](#bib.bib77)].'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '基于LSTM的优势，Donahue等人提出了用于人体动作识别的长期递归卷积网络（LRCN）[[54](#bib.bib54)]。LRCN的框架如图[8](#S3.F8
    "Figure 8 ‣ 3.2.3 Recurrent Neural Network ‣ 3.2 Architectures for Learning Video
    Features ‣ 3 Common Deep Network Architectures ‣ Self-supervised Visual Feature
    Learning with Deep Neural Networks: A Survey")所示。LSTM顺序地应用于ConvNets提取的特征，以建模帧序列中的时间动态。通过使用LSTM将视频建模为帧序列，这个模型能够明确地建模视频中的长期时间动态。后来，这个模型被扩展到更深的LSTM用于动作识别[[74](#bib.bib74),
    [75](#bib.bib75)]、视频字幕生成[[76](#bib.bib76)]和手势识别任务[[77](#bib.bib77)]。'
- en: 3.3 Summary of ConvNet Architectures
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 卷积网络架构总结
- en: Deep ConvNets have demonstrated great potential in various computer vision tasks.
    And the visualization of the image and video features has shown that these networks
    truly learned meaningful features that required by the corresponding tasks [[52](#bib.bib52),
    [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80)]. However, one common drawback
    is that these networks can be easily over-fit when training data are scarce since
    there are over millions of parameters in each network.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 深度ConvNets在各种计算机视觉任务中展现了巨大潜力。图像和视频特征的可视化显示，这些网络确实学习了相应任务所需的有意义特征[[52](#bib.bib52),
    [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80)]。然而，一个共同的缺点是，当训练数据稀缺时，这些网络容易过拟合，因为每个网络中有数百万个参数。
- en: Take 3DResNet for an example, the performance of an $18$-layer 3DResNet on UCF101
    action recognition dataset [[63](#bib.bib63)] is $42$% when trained from scratch.
    However, with a supervised pre-trained model on the large-scale Kinetics dataset
    ($500,000$ videos of $600$ classes) with human-annotated class labels and then
    fine-tuned on UCF101 dataset, the performance can increase to $84$%. Pre-trained
    models on large-scale datasets can speed up the training process and improve the
    performance on relatively small datasets. However, the cost of collecting and
    annotating large-scale datasets is very expensive and time-consuming.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以3DResNet为例，$18$层3DResNet在UCF101动作识别数据集 [[63](#bib.bib63)] 上从头训练时的性能为$42$%。然而，使用在大规模Kinetics数据集（$500,000$个视频，$600$个类别）上进行监督预训练的模型，再在UCF101数据集上进行微调，性能可以提高到$84$%。在大规模数据集上预训练的模型可以加速训练过程并提高在相对较小数据集上的性能。然而，收集和标注大规模数据集的成本非常高且耗时。
- en: In order to obtain pre-trained models from large-scale datasets without expensive
    human annotations, many self-supervised learning methods were proposed to learn
    image and video features from pre-designed pretext tasks. The next section describes
    the general pipeline of the self-supervised image and video feature learning.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从大规模数据集中获得预训练模型而不需要昂贵的人工标注，提出了许多自监督学习方法来从预设计的前置任务中学习图像和视频特征。下一节描述了自监督图像和视频特征学习的一般流程。
- en: 4 Commonly used Pretext and Downstream Tasks
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 常用的前置任务和下游任务
- en: '![Refer to caption](img/2dc27bb555b037a71f64ac3a86d3bd71.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2dc27bb555b037a71f64ac3a86d3bd71.png)'
- en: 'Figure 9: Self-supervised visual feature learning schema. The ConvNet is trained
    by minimizing errors between pseudo labels $P$ and predictions $O$ of the ConvNet.
    Since the pseudo labels are automatically generated, no human annotations are
    involved during the whole process.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：自监督视觉特征学习框架。通过最小化伪标签$P$与ConvNet预测$O$之间的误差来训练ConvNet。由于伪标签是自动生成的，整个过程不涉及人工标注。
- en: 'Most existing self-supervised learning approaches follow the schema shown in
    Fig [9](#S4.F9 "Figure 9 ‣ 4 Commonly used Pretext and Downstream Tasks ‣ Self-supervised
    Visual Feature Learning with Deep Neural Networks: A Survey"). Generally, a pretext
    task is defined for ConvNets to solve and visual features can be learned through
    the process of accomplishing this pretext task. The pseudo labels $P$ for pretext
    task can be automatically generated without human annotations. ConvNet is optimized
    by minimizing the error between the prediction of ConvNet $O$ and the pseudo labels
    $P$. After the training on the pretext task is finished, ConvNet models that can
    capture visual features for images or videos are obtained.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '大多数现有的自监督学习方法遵循图 [9](#S4.F9 "Figure 9 ‣ 4 Commonly used Pretext and Downstream
    Tasks ‣ Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey")
    中所示的框架。一般来说，定义一个前置任务供ConvNets解决，通过完成这个前置任务来学习视觉特征。前置任务的伪标签$P$可以在没有人工标注的情况下自动生成。通过最小化ConvNet
    $O$的预测与伪标签$P$之间的误差来优化ConvNet。完成前置任务的训练后，得到能够捕捉图像或视频视觉特征的ConvNet模型。'
- en: 4.1 Learning Visual Features from Pretext Tasks
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 从前置任务中学习视觉特征
- en: To relieve the burden of large-scale dataset annotation, a pretext task is generally
    designed for networks to solve while pseudo labels for the pretext task are automatically
    generated based on data attributes. Many pretext tasks have been designed and
    applied for self-supervised learning such as foreground object segmentation [[81](#bib.bib81)],
    image inpainting [[19](#bib.bib19)], clustering [[44](#bib.bib44)], image colorization
    [[82](#bib.bib82)], temporal order verification [[40](#bib.bib40)], visual audio
    correspondence verification [[25](#bib.bib25)], and so on. Effective pretext tasks
    ensure that semantic features are learned through the process of accomplishing
    the pretext tasks.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻大规模数据集标注的负担，通常设计一个前置任务供网络解决，同时基于数据属性自动生成前置任务的伪标签。许多前置任务已被设计并应用于自监督学习，如前景物体分割
    [[81](#bib.bib81)]、图像修复 [[19](#bib.bib19)]、聚类 [[44](#bib.bib44)]、图像着色 [[82](#bib.bib82)]、时间顺序验证
    [[40](#bib.bib40)]、视觉音频对应关系验证 [[25](#bib.bib25)] 等。有效的前置任务确保通过完成这些任务的过程来学习语义特征。
- en: Take image colorization as an example, image colorization is a task to colorize
    gray-scale images into colorful images. To generate realistic colorful images,
    networks are required to learn the structure and context information of images.
    In this pretext task, the data $X$ is the gray-scale images which can be generated
    by performing a linear transformation in RGB images, while the pseudo label $P$
    is the RGB image itself. The training pair $X_{i}$ and $P_{i}$ can be generated
    in real time with negligible cost. Self-Supervised learning with other pretext
    tasks follow a similar pipeline.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以图像上色为例，图像上色任务是将灰度图像上色为彩色图像。为了生成逼真的彩色图像，网络需要学习图像的结构和上下文信息。在这个预文本任务中，数据 $X$ 是通过对RGB图像进行线性变换生成的灰度图像，而伪标签
    $P$ 是RGB图像本身。训练对 $X_{i}$ 和 $P_{i}$ 可以实时生成，成本微乎其微。其他预文本任务的自监督学习遵循类似的流程。
- en: 4.2 Commonly Used Pretext Tasks
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 常用的预文本任务
- en: '![Refer to caption](img/4c2551988e8e3be72a6659e518c727dc.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4c2551988e8e3be72a6659e518c727dc.png)'
- en: 'Figure 10: Categories of pretext tasks for self-supervised visual feature learning:
    generation-based, context-based, free semantic label-based, and cross modal-based.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：自监督视觉特征学习的预文本任务类别：基于生成的、基于上下文的、基于自由语义标签的和跨模态的。
- en: 'According to the data attributes used to design pretext tasks, as shown in
    Fig. [10](#S4.F10 "Figure 10 ‣ 4.2 Commonly Used Pretext Tasks ‣ 4 Commonly used
    Pretext and Downstream Tasks ‣ Self-supervised Visual Feature Learning with Deep
    Neural Networks: A Survey"), we summarize the pretext tasks into four categories:
    generation-based, context-based, free semantic label-based, and cross modal-based.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '根据用于设计预文本任务的数据属性，如图[10](#S4.F10 "Figure 10 ‣ 4.2 Commonly Used Pretext Tasks
    ‣ 4 Commonly used Pretext and Downstream Tasks ‣ Self-supervised Visual Feature
    Learning with Deep Neural Networks: A Survey")所示，我们将预文本任务总结为四类：基于生成的、基于上下文的、基于自由语义标签的和跨模态的。'
- en: 'Generation-based Methods: This type of methods learn visual features by solving
    pretext tasks that involve image or video generation.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 基于生成的方法：这类方法通过解决涉及图像或视频生成的预文本任务来学习视觉特征。
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Image Generation: Visual features are learned through the process of image
    generation tasks. This type of methods includes image colorization [[18](#bib.bib18)],
    image super resolution [[15](#bib.bib15)], image inpainting [[19](#bib.bib19)],
    image generation with Generative Adversarial Networks (GANs) [[83](#bib.bib83),
    [84](#bib.bib84)].'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像生成：通过图像生成任务的过程来学习视觉特征。这类方法包括图像上色 [[18](#bib.bib18)]、图像超分辨率 [[15](#bib.bib15)]、图像修复
    [[19](#bib.bib19)]、使用生成对抗网络（GANs）生成图像 [[83](#bib.bib83), [84](#bib.bib84)]。
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Video Generation: Visual features are learned through the process of video
    generation tasks. This type of methods includes video generation with GANs [[85](#bib.bib85),
    [86](#bib.bib86)] and video prediction [[37](#bib.bib37)].'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视频生成：通过视频生成任务的过程来学习视觉特征。这类方法包括使用GANs生成视频 [[85](#bib.bib85), [86](#bib.bib86)]
    和视频预测 [[37](#bib.bib37)]。
- en: 'Context-based pretext tasks: The design of context-based pretext tasks mainly
    employ the context features of images or videos such as context similarity, spatial
    structure, temporal structure, etc.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上下文的预文本任务：基于上下文的预文本任务的设计主要利用图像或视频的上下文特征，如上下文相似性、空间结构、时间结构等。
- en: •
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Context Similarity: Pretext tasks are designed based on the context similarity
    between image patches. This type of methods includes image clustering-based methods
    [[34](#bib.bib34), [44](#bib.bib44)], and graph constraint-based methods [[43](#bib.bib43)].'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上下文相似性：预文本任务基于图像块之间的上下文相似性设计。这类方法包括基于图像聚类的方法 [[34](#bib.bib34), [44](#bib.bib44)]
    和基于图约束的方法 [[43](#bib.bib43)]。
- en: •
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Spatial Context Structure: Pretext tasks are used to train ConvNets are based
    on the spatial relations among image patches. This type of methods includes image
    jigsaw puzzle [[20](#bib.bib20), [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89)],
    context prediction [[41](#bib.bib41)], and geometric transformation recognition
    [[36](#bib.bib36), [28](#bib.bib28)], etc.'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 空间上下文结构：用于训练卷积网络的预文本任务基于图像块之间的空间关系。这类方法包括图像拼图 [[20](#bib.bib20), [87](#bib.bib87),
    [88](#bib.bib88), [89](#bib.bib89)]、上下文预测 [[41](#bib.bib41)] 和几何变换识别 [[36](#bib.bib36),
    [28](#bib.bib28)] 等。
- en: •
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Temporal Context Structure: The temporal order from videos is used as supervision
    signal. The ConvNet is trained to verify whether the input frame sequence in correct
    order [[40](#bib.bib40), [90](#bib.bib90)] or to recognize the order of the frame
    sequence [[39](#bib.bib39)].'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 时间上下文结构：视频中的时间顺序被用作监督信号。卷积网络被训练以验证输入帧序列是否按正确顺序[[40](#bib.bib40)、[90](#bib.bib90)]或识别帧序列的顺序[[39](#bib.bib39)]。
- en: 'Free Semantic Label-based Methods: This type of pretext tasks train networks
    with automatically generated semantic labels. The labels are generated by traditional
    hard-code algorithms [[50](#bib.bib50), [51](#bib.bib51)] or by game engines [[30](#bib.bib30)].
    The pretext tasks include moving object segmentation [[91](#bib.bib91), [81](#bib.bib81)],
    contour detection [[47](#bib.bib47), [30](#bib.bib30)], relative depth prediction
    [[92](#bib.bib92)], and etc.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 基于自由语义标签的方法：这种类型的预任务使用自动生成的语义标签训练网络。标签由传统的硬编码算法[[50](#bib.bib50)、[51](#bib.bib51)]或游戏引擎[[30](#bib.bib30)]生成。预任务包括移动物体分割[[91](#bib.bib91)、[81](#bib.bib81)]、轮廓检测[[47](#bib.bib47)、[30](#bib.bib30)]、相对深度预测[[92](#bib.bib92)]等。
- en: 'Cross Modal-based Methods: This type of pretext tasks train ConvNets to verify
    whether two different channels of input data are corresponding to each other.
    This type of methods includes Visual-Audio Correspondence Verification [[93](#bib.bib93),
    [25](#bib.bib25)], RGB-Flow Correspondence Verification [[24](#bib.bib24)], and
    egomotion [[94](#bib.bib94), [95](#bib.bib95)].'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基于跨模态的方法：这种类型的预任务训练卷积网络以验证两个不同通道的输入数据是否相互对应。这类方法包括视觉-音频对应验证[[93](#bib.bib93)、[25](#bib.bib25)]、RGB-光流对应验证[[24](#bib.bib24)]和自运动[[94](#bib.bib94)、[95](#bib.bib95)]。
- en: 4.3 Commonly Used Downstream Tasks for Evaluation
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 常用下游任务的评估
- en: To evaluate the quality of the learned image or video features by self-supervised
    methods, the learned parameters by self-supervised learning are employed as pre-trained
    models and then fine-tuned on downstream tasks such as image classification, semantic
    segmentation, object detection, and action recognition etc. The performance of
    the transfer learning on these high-level vision tasks demonstrates the generalization
    ability of the learned features. If ConvNets of self-supervised learning can learn
    general features, then the pre-trained models can be used as a good starting point
    for other vision tasks that require capturing similar features from images or
    videos.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估自监督方法所学图像或视频特征的质量，利用自监督学习获得的参数作为预训练模型，然后在下游任务上进行微调，如图像分类、语义分割、目标检测和动作识别等。这些高级视觉任务上的迁移学习表现展示了所学特征的泛化能力。如果自监督学习的卷积网络（ConvNets）能够学习到通用特征，那么预训练模型可以作为捕捉图像或视频中类似特征的其他视觉任务的良好起点。
- en: Image classification, semantic segmentation, and object detection usually are
    used as the tasks to evaluate the generalization ability of the learned image
    features by self-supervised learning methods, while human action recognition in
    videos is used to evaluate the quality of video features obtained from self-supervised
    learning methods. Below are brief introductions of the commonly used high-level
    tasks for visual feature evaluation.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类、语义分割和目标检测通常被用作评估自监督学习方法所学图像特征泛化能力的任务，而视频中的人类动作识别则用于评估从自监督学习方法中获得的视频特征的质量。以下是用于视觉特征评估的常见高级任务的简要介绍。
- en: 4.3.1 Semantic Segmentation
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 语义分割
- en: Semantic segmentation, the task of assigning semantic labels to each pixel in
    images, is of great importance in many applications such as autonomous driving,
    human-machine interaction, and robotics. The community has recently made promising
    progress and various networks have been proposed such as Fully Convolutional Network
    (FCN) [[4](#bib.bib4)], DeepLab [[5](#bib.bib5)], PSPNet [[6](#bib.bib6)] and
    datasets such as PASCAL VOC [[96](#bib.bib96)], CityScape [[97](#bib.bib97)],
    ADE20K [[98](#bib.bib98)].
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割是将语义标签分配给图像中每个像素的任务，在许多应用中具有重要意义，如自动驾驶、人机交互和机器人技术。最近，社区取得了令人鼓舞的进展，并提出了各种网络，如全卷积网络（FCN）[[4](#bib.bib4)]、DeepLab
    [[5](#bib.bib5)]、PSPNet [[6](#bib.bib6)]以及数据集，如PASCAL VOC [[96](#bib.bib96)]、CityScape
    [[97](#bib.bib97)]、ADE20K [[98](#bib.bib98)]。
- en: '![Refer to caption](img/d50b17db63c249f4be84236fd6a3a914.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d50b17db63c249f4be84236fd6a3a914.png)'
- en: 'Figure 11: The framework of the Fully Convolutional Neural Network proposed
    for semantic segmentation [[4](#bib.bib4)]. Figure is reproduced based on [[4](#bib.bib4)].'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：为语义分割提出的全卷积神经网络的框架 [[4](#bib.bib4)]。图基于 [[4](#bib.bib4)] 进行复现。
- en: 'Among all these methods, FCN [[4](#bib.bib4)] is a milestone work for semantic
    segmentation since it started the era of applying fully convolution network (FCN)
    to solve this task. The architecture of FCN is shown in Fig. [11](#S4.F11 "Figure
    11 ‣ 4.3.1 Semantic Segmentation ‣ 4.3 Commonly Used Downstream Tasks for Evaluation
    ‣ 4 Commonly used Pretext and Downstream Tasks ‣ Self-supervised Visual Feature
    Learning with Deep Neural Networks: A Survey"). 2DConvNet such as AlexNet, VGG,
    ResNet is used as the base network for feature extraction while the fully connected
    layer is replaced by transposed convolution layer to obtain the dense prediction.
    The network is trained end-to-end with pixel-wise annotations.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '在所有这些方法中，FCN [[4](#bib.bib4)] 是语义分割的一个里程碑，因为它开启了将全卷积网络（FCN）应用于解决该任务的时代。FCN
    的架构如图 [11](#S4.F11 "Figure 11 ‣ 4.3.1 Semantic Segmentation ‣ 4.3 Commonly Used
    Downstream Tasks for Evaluation ‣ 4 Commonly used Pretext and Downstream Tasks
    ‣ Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey")
    所示。使用 AlexNet、VGG、ResNet 等 2DConvNet 作为特征提取的基础网络，同时将全连接层替换为转置卷积层以获得密集预测。网络通过像素级注释进行端到端训练。'
- en: When using semantic segmentation as downstream task to evaluate the quality
    of image features learned by self-supervised learning methods, the FCN is initialized
    with the parameters trained with the pretext task and fine-tuned on the semantic
    segmentation dataset, then the performance on the semantic segmentation task is
    evaluated and compared with that of other self-supervised methods.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用语义分割作为下游任务来评估自监督学习方法所学图像特征的质量时，FCN 以预训练任务的参数初始化，并在语义分割数据集上进行微调，然后评估在语义分割任务上的性能，并与其他自监督方法的性能进行比较。
- en: 4.3.2 Object Detection
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 对象检测
- en: Object Detection, a task of localizing the position of objects in images and
    recognizing the category of the objects, is also very import for many computer
    vision applications such as autonomous driving, robotics, scene text detection
    and so on. Recently, many datasets such as MSCOCO [[99](#bib.bib99)] and OpenImage
    [[14](#bib.bib14)] have been proposed for object detection and many ConvNet-based
    models [[1](#bib.bib1)], [[2](#bib.bib2)], [[3](#bib.bib3)], [[100](#bib.bib100)],
    [[101](#bib.bib101)], [[102](#bib.bib102)], [[103](#bib.bib103)], [[104](#bib.bib104)]
    have been proposed and obtained great performance.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对象检测是本地化图像中对象的位置并识别对象类别的任务，对于许多计算机视觉应用（如自动驾驶、机器人、场景文本检测等）也非常重要。近年来，许多数据集如 MSCOCO
    [[99](#bib.bib99)] 和 OpenImage [[14](#bib.bib14)] 被提出用于对象检测，许多基于 ConvNet 的模型 [[1](#bib.bib1)],
    [[2](#bib.bib2)], [[3](#bib.bib3)], [[100](#bib.bib100)], [[101](#bib.bib101)],
    [[102](#bib.bib102)], [[103](#bib.bib103)], [[104](#bib.bib104)] 已被提出并取得了出色的表现。
- en: 'Fast-RCNN [[2](#bib.bib2)] is a two-stage network for object detection. The
    framework of Fast-RCNN is shown in Fig. [12](#S4.F12 "Figure 12 ‣ 4.3.2 Object
    Detection ‣ 4.3 Commonly Used Downstream Tasks for Evaluation ‣ 4 Commonly used
    Pretext and Downstream Tasks ‣ Self-supervised Visual Feature Learning with Deep
    Neural Networks: A Survey"). Object proposals are generated based on feature maps
    produced by a convolution neural network, then these proposals are fed to several
    fully connected layers to generate the bounding box of objects and the categories
    of these objects.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 'Fast-RCNN [[2](#bib.bib2)] 是一个用于对象检测的两阶段网络。Fast-RCNN 的框架如图 [12](#S4.F12 "Figure
    12 ‣ 4.3.2 Object Detection ‣ 4.3 Commonly Used Downstream Tasks for Evaluation
    ‣ 4 Commonly used Pretext and Downstream Tasks ‣ Self-supervised Visual Feature
    Learning with Deep Neural Networks: A Survey") 所示。基于卷积神经网络生成的特征图生成对象提议，然后将这些提议输入到几个全连接层中，以生成对象的边界框和这些对象的类别。'
- en: '![Refer to caption](img/d4450ffad6847ed2267c96dcd3e6c046.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d4450ffad6847ed2267c96dcd3e6c046.png)'
- en: 'Figure 12: The pipeline of the Fast-RCNN for object detection. Figure is reproduced
    based on [[3](#bib.bib3)].'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：Fast-RCNN 对象检测的流程图。图基于 [[3](#bib.bib3)] 进行复现。
- en: When using object detection as downstream task to evaluate the quality of the
    self-supervised image features, networks that trained with the pretext task on
    unlabeled large data are served as the pre-trained model for the Fast-RCNN [[2](#bib.bib2)]
    and then fine-tuned on object detection datasets, then the performance on the
    object detection task is evaluated to demonstrate the generalization ability of
    self-supervised learned features.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用目标检测作为下游任务来评估自监督图像特征的质量时，使用在无标签大数据上通过预训练任务训练的网络作为Fast-RCNN的预训练模型[[2](#bib.bib2)]，然后在目标检测数据集上进行微调，然后评估目标检测任务的性能，以展示自监督学习特征的泛化能力。
- en: 4.3.3 Image Classification
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 图像分类
- en: Image Classification is a task of recognizing the category of objects in each
    image. Many networks have been designed for this task such as AlexNet [[8](#bib.bib8)],
    VGG [[9](#bib.bib9)], ResNet [[11](#bib.bib11)], GoogLeNet [[10](#bib.bib10)],
    DenseNet [[12](#bib.bib12)], etc. Usually, only one class label is available for
    each image although the image may contains different classes of objects.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类是识别每张图像中对象类别的任务。许多网络已经为此任务设计，如AlexNet[[8](#bib.bib8)]、VGG[[9](#bib.bib9)]、ResNet[[11](#bib.bib11)]、GoogLeNet[[10](#bib.bib10)]、DenseNet[[12](#bib.bib12)]等。通常，每张图像只有一个类别标签，尽管图像可能包含不同类别的对象。
- en: When choosing image classification as a downstream task to evaluate the quality
    of image features learned from self-supervised learning methods, the self-supervised
    learned model is applied on each image to extract features which then are used
    to train a classifier such as Support Vector Machine (SVM) [[105](#bib.bib105)].
    The classification performance on testing data is compared with other self-supervised
    models to evaluate the quality of the learned features.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当选择图像分类作为下游任务来评估从自监督学习方法中学习的图像特征质量时，自监督学习模型会应用于每张图像以提取特征，然后这些特征用于训练分类器，如支持向量机（SVM）[[105](#bib.bib105)]。测试数据上的分类性能与其他自监督模型进行比较，以评估学习特征的质量。
- en: 4.3.4 Human Action Recognition
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 人体动作识别
- en: Human action recognition is a task of identifying what people doing in videos
    for a list of pre-defined action classes. Generally, videos in human action recognition
    datasets contain only one action in each video [[63](#bib.bib63), [106](#bib.bib106),
    [17](#bib.bib17)]. Both the spatial and temporal features are needed to accomplish
    this task.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 人体动作识别是一个识别视频中人物行为的任务，基于一系列预定义的动作类别。通常，人类动作识别数据集中的视频每个视频只包含一个动作[[63](#bib.bib63)、[106](#bib.bib106)、[17](#bib.bib17)]。完成此任务需要空间特征和时间特征。
- en: The action recognition task is often used to evaluate the quality of video features
    learned by self-supervised learning methods. The network is first trained on unlabeled
    video data with pretext tasks, then it is fine-tuned on action recognition datasets
    with human annotations to recognize the actions. The testing performance on action
    recognition task is compared with other self-supervised learning methods to evaluate
    the quality of the learned features.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 动作识别任务通常用于评估自监督学习方法所学习的视频特征的质量。网络首先在无标签视频数据上进行预训练任务训练，然后在带有人类注释的动作识别数据集上进行微调以识别动作。通过将动作识别任务的测试性能与其他自监督学习方法进行比较，以评估学习特征的质量。
- en: 4.3.5 Qualitative Evaluation
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.5 定性评价
- en: 'In addition to these quantitative evaluations of the learned features, there
    are also some qualitative visualization methods to evaluate the quality of self-supervised
    learning features. Three methods are often used for this purpose: kernel visualization,
    feature map visualization, and image retrieval visualization [[44](#bib.bib44),
    [36](#bib.bib36), [28](#bib.bib28), [41](#bib.bib41)].'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些对学习特征的定量评估，还有一些定性可视化方法来评估自监督学习特征的质量。通常使用三种方法：卷积核可视化、特征图可视化和图像检索可视化[[44](#bib.bib44)、[36](#bib.bib36)、[28](#bib.bib28)、[41](#bib.bib41)]。
- en: 'Kernel Visualization: Qualitatively visualize the kernels of the first convolution
    layer learned with the pretext tasks and compare the kernels from supervised models.
    The similarity of the kernels learned by supervised and self-supervised models
    are compared to indicate the effectiveness of self-supervised methods [[44](#bib.bib44),
    [28](#bib.bib28)].'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 核心可视化：定性地可视化通过预训练任务学习的第一层卷积核，并与监督模型的卷积核进行比较。通过比较监督模型和自监督模型学习的卷积核的相似性，以指示自监督方法的有效性[[44](#bib.bib44)、[28](#bib.bib28)]。
- en: 'Feature Map Visualization: Feature maps are visualized to show the attention
    of networks. Larger activation represents the neural network pays more attention
    to the corresponding region in the image. Feature maps are usually qualitatively
    visualized and compared with that of supervised models [[36](#bib.bib36), [28](#bib.bib28)].'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 特征图可视化：特征图的可视化显示了网络的注意力。更大的激活表示神经网络对图像中的相应区域给予了更多的关注。特征图通常是定性地可视化，并与监督模型的特征图进行比较
    [[36](#bib.bib36), [28](#bib.bib28)]。
- en: 'Nearest Neighbor Retrieval: In general, images with similar appearance usually
    are closer in the feature space. The nearest neighbor method is used to find the
    top $K$ nearest neighbors from the feature space of the features learned by the
    self-supervised learned model [[40](#bib.bib40), [41](#bib.bib41), [43](#bib.bib43)].'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻检索：一般而言，外观相似的图像通常在特征空间中距离较近。最近邻方法用于从自监督学习模型学习的特征空间中找到前 $K$ 个最近邻 [[40](#bib.bib40),
    [41](#bib.bib41), [43](#bib.bib43)]。
- en: 'TABLE I: Summary of commonly used image and video datasets. Note that image
    datasets can be used to learn image features, while video datasets can be used
    to learn both image and video features.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 常用图像和视频数据集的总结。注意，图像数据集可用于学习图像特征，而视频数据集可用于学习图像和视频特征。'
- en: '| Dataset | Data Type | Size | Synthetic | # classes | Label |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 数据类型 | 大小 | 合成 | 类别数 | 标签 |'
- en: '| ImageNet [[13](#bib.bib13)] | Image | $1.3$ million images | ✗ | $1,000$
    | Object category label |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet [[13](#bib.bib13)] | 图像 | $130$ 万张图像 | ✗ | $1,000$ | 物体类别标签 |'
- en: '| Places [[107](#bib.bib107)] | Image | $2.5$ million images | ✗ | $205$ |
    scene categories label |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Places [[107](#bib.bib107)] | 图像 | $250$ 万张图像 | ✗ | $205$ | 场景类别标签 |'
- en: '| Places365 [[108](#bib.bib108)] | Image | $10$ million images | ✗ | $434$
    | scene categories label |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Places365 [[108](#bib.bib108)] | 图像 | $1000$ 万张图像 | ✗ | $434$ | 场景类别标签 |'
- en: '| SUNCG [[109](#bib.bib109)] | Image | $150,000$ images | ✓ | $84$ | depth,
    volumetric data |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| SUNCG [[109](#bib.bib109)] | 图像 | $150,000$ 张图像 | ✓ | $84$ | 深度，体积数据 |'
- en: '| MNIST [[110](#bib.bib110)] | Image | $70,000$ images | ✗ | $10$ | Digit class
    label |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| MNIST [[110](#bib.bib110)] | 图像 | $70,000$ 张图像 | ✗ | $10$ | 数字类别标签 |'
- en: '| SVHN [[111](#bib.bib111)] | Image | $600,000$ Images | ✗ | $10$ | Digit class
    label |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| SVHN [[111](#bib.bib111)] | 图像 | $600,000$ 张图像 | ✗ | $10$ | 数字类别标签 |'
- en: '| CIFAR10 [[112](#bib.bib112)] | Image | $60,000$ Images | ✗ | $10$ | Object
    category label |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR10 [[112](#bib.bib112)] | 图像 | $60,000$ 张图像 | ✗ | $10$ | 物体类别标签 |'
- en: '| STL-10 [[113](#bib.bib113)] | Image | $101,300$ Images | ✗ | $10$ | Object
    category label |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| STL-10 [[113](#bib.bib113)] | 图像 | $101,300$ 张图像 | ✗ | $10$ | 物体类别标签 |'
- en: '| PASCAL VOC [[96](#bib.bib96)] | Image | $2,913$ images | ✗ | $20$ | Category
    label, bounding box, segmentation mask |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| PASCAL VOC [[96](#bib.bib96)] | 图像 | $2,913$ 张图像 | ✗ | $20$ | 类别标签，边界框，分割掩码
    |'
- en: '| YFCC100M [[114](#bib.bib114)] | Image/Video | $100$ million media data |
    ✗ | — | Hashtags |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| YFCC100M [[114](#bib.bib114)] | 图像/视频 | $1$ 亿媒体数据 | ✗ | — | 标签 |'
- en: '| SceneNet RGB-D [[115](#bib.bib115)] | Video | $5$ million images | ✓ | $13$
    | Depth, Instance Segmentation, Optical Flow |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| SceneNet RGB-D [[115](#bib.bib115)] | 视频 | $500$ 万张图像 | ✓ | $13$ | 深度，实例分割，光流
    |'
- en: '| Moment-in-Time [[116](#bib.bib116)] | Video | $1$ million 3-second videos
    | ✗ | $339$ | Video category class |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Moment-in-Time [[116](#bib.bib116)] | 视频 | $100$ 万个 3 秒视频 | ✗ | $339$ | 视频类别
    |'
- en: '| Kinetics [[17](#bib.bib17)] | Video | $0.5$ million 10-second videos | ✗
    | $600$ | Human action class |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Kinetics [[17](#bib.bib17)] | 视频 | $50$ 万个 10 秒视频 | ✗ | $600$ | 人类动作类别 |'
- en: '| AudioSet [[117](#bib.bib117)] | Video | $2$ million 10-second videos | ✗
    | $632$ | Audio event class |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| AudioSet [[117](#bib.bib117)] | 视频 | $200$ 万个 10 秒视频 | ✗ | $632$ | 音频事件类别
    |'
- en: '| KITTI [[118](#bib.bib118)] | Video | $28$ videos | ✗ | — | Data captured
    by various sensors are available |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| KITTI [[118](#bib.bib118)] | 视频 | $28$ 个视频 | ✗ | — | 提供了由各种传感器捕获的数据 |'
- en: '| UCF101 [[63](#bib.bib63)] | Video | $10,031$ videos | ✗ | $101$ | Human action
    class |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| UCF101 [[63](#bib.bib63)] | 视频 | $10,031$ 个视频 | ✗ | $101$ | 人类动作类别 |'
- en: '| HMDB51 [[106](#bib.bib106)] | Video | $6,766$ videos | ✗ | $51$ | Human action
    class |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| HMDB51 [[106](#bib.bib106)] | 视频 | $6,766$ 个视频 | ✗ | $51$ | 人类动作类别 |'
- en: 5 Datasets
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个数据集
- en: 'This section summarizes the commonly used image and video datasets for training
    and evaluating of self-supervised visual feature learning methods. Self-supervised
    learning methods can be trained with images or videos by discarding human-annotated
    labels, therefore, any datasets that collected for supervised learning can be
    used for self-supervised visual feature learning without using human-annotated
    labels. The evaluation of the quality of learned features is normally conducted
    by fine-tuned on high-level vision tasks with relatively small datasets (normally
    with accurate labels) such as video action recognition, object detection, semantic
    segmentation, etc. It is worth noting that networks use these synthetic datasets
    for visual feature learning are considered as self-supervised learning in this
    paper since labels of synthetic datasets are automatically generated by game engines
    and no human annotations are involved. Table [I](#S4.T1 "TABLE I ‣ 4.3.5 Qualitative
    Evaluation ‣ 4.3 Commonly Used Downstream Tasks for Evaluation ‣ 4 Commonly used
    Pretext and Downstream Tasks ‣ Self-supervised Visual Feature Learning with Deep
    Neural Networks: A Survey") summarizes the commonly used image and video datasets.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '本节总结了用于训练和评估自监督视觉特征学习方法的常用图像和视频数据集。自监督学习方法可以通过丢弃人工注释标签来用图像或视频进行训练，因此，任何为监督学习收集的数据集都可以用于自监督视觉特征学习而无需人工注释标签。学习特征的质量评估通常通过在高层视觉任务上微调来进行，使用相对较小的数据集（通常具有准确标签），如视频动作识别、对象检测、语义分割等。值得注意的是，本文认为使用这些合成数据集进行视觉特征学习的网络属于自监督学习，因为合成数据集的标签是由游戏引擎自动生成的，没有涉及人工注释。表
    [I](#S4.T1 "TABLE I ‣ 4.3.5 Qualitative Evaluation ‣ 4.3 Commonly Used Downstream
    Tasks for Evaluation ‣ 4 Commonly used Pretext and Downstream Tasks ‣ Self-supervised
    Visual Feature Learning with Deep Neural Networks: A Survey") 总结了常用的图像和视频数据集。'
- en: 5.1 Image Datasets
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 图像数据集
- en: •
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ImageNet: The ImageNet dataset [[13](#bib.bib13)] contains $1.3$ million images
    uniformly distributed into $1,000$ classes and is organized according to the WordNet
    hierarchy. Each image is assigned with only one class label. ImageNet is the most
    widely used dataset for self-supervised image feature learning.'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ImageNet: ImageNet 数据集 [[13](#bib.bib13)] 包含 $1.3$ 百万张图像，均匀分布于 $1,000$ 个类别中，并根据
    WordNet 层级组织。每张图像仅分配一个类别标签。ImageNet 是自监督图像特征学习中使用最广泛的数据集。'
- en: •
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Places: The Places dataset [[107](#bib.bib107)] is proposed for scene recognition
    and contains more than $2.5$ million images covering more than $205$ scene categories
    with more than $5,000$ images per category.'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Places: Places 数据集 [[107](#bib.bib107)] 旨在场景识别，包含超过 $2.5$ 百万张图像，涵盖了超过 $205$
    个场景类别，每个类别有超过 $5,000$ 张图像。'
- en: •
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Places365: The Places365 is the 2nd generation of the Places database which
    is built for high-level visual understanding tasks, such as scene context, object
    recognition, action and event prediction, and theory-of-mind inference [[108](#bib.bib108)].
    There are more than $10$ million images covering more than $400$ classes and $5,000$
    to $30,000$ training images per class.'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Places365: Places365 是 Places 数据库的第二代，旨在用于高层次视觉理解任务，如场景背景、对象识别、动作和事件预测以及心智理论推理
    [[108](#bib.bib108)]。数据集中包含超过 $10$ 百万张图像，涵盖了超过 $400$ 个类别，每个类别有 $5,000$ 到 $30,000$
    张训练图像。'
- en: •
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SUNCG: The SUNCG dataset is a large synthetic 3D scene repository for indoor
    scenes which consists of over $45,000$ different scenes with manually created
    realistic room and furniture layouts [[109](#bib.bib109)]. The synthetic depth,
    object level semantic labels, and volumetric ground truth are available.'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SUNCG: SUNCG 数据集是一个大型的合成 3D 场景库，专为室内场景设计，包括了超过 $45,000$ 个不同的场景，并手动创建了现实的房间和家具布局
    [[109](#bib.bib109)]。合成深度、对象级别的语义标签和体积真相数据均可用。'
- en: •
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MNIST: The MNIST is a dataset of handwritten digits consisting of $70,000$
    images while $60,000$ images belong to training set and the rest $10,000$ images
    are for testing [[110](#bib.bib110)]. All digits have been size-normalized and
    centered in fixed-size images.'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'MNIST: MNIST 是一个手写数字数据集，包括 $70,000$ 张图像，其中 $60,000$ 张图像用于训练集，其余 $10,000$ 张图像用于测试
    [[110](#bib.bib110)]。所有数字已进行大小标准化，并在固定大小的图像中居中显示。'
- en: •
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SVHN: SVHN is a dataset for recognizing digits and numbers in natural scene
    images which obtained from house numbers from Google Street View images [[111](#bib.bib111)].
    The dataset consists of over $600,000$ images and all digits have been resized
    to a fixed resolution of $32\times 32$ pixels.'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SVHN: SVHN是一个用于识别自然场景图像中数字和数字的数据集，数据来源于Google街景图像中的门牌号码[[111](#bib.bib111)]。该数据集包含超过$600,000$张图像，所有数字都被调整为固定分辨率$32\times
    32$像素。'
- en: •
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CIFAR10: The CIFAR10 dataset is a collection of tiny images for image classification
    task [[112](#bib.bib112)]. It consists of $60,000$ images of size $32\times 32$
    that covers $10$ different classes. The $10$ classes include airplane, automobile,
    bird, cat, deer, dog, frog, horse, ship, and truck. The dataset is balanced and
    there are $6,000$ images of each class.'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CIFAR10: CIFAR10数据集是一个用于图像分类任务的小图像集合[[112](#bib.bib112)]。它包含$60,000$张$32\times
    32$大小的图像，涵盖$10$个不同的类别。这$10$个类别包括飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。数据集是平衡的，每个类别有$6,000$张图像。'
- en: •
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'STL-10: The STL-10 dataset is specifically designed for developing unsupervised
    feature learning [[113](#bib.bib113)]. It consists of $500$ labeled training images,
    $800$ testing images, and $100,000$ unlabeled images covering $10$ classes which
    include airplane, bird, car, cat, deer, dog, horse, monkey, ship, and truck.'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'STL-10: STL-10数据集专门设计用于开发无监督特征学习[[113](#bib.bib113)]。它包含$500$张带标签的训练图像、$800$张测试图像和$100,000$张未标记的图像，覆盖$10$个类别，包括飞机、鸟、车、猫、鹿、狗、马、猴子、船和卡车。'
- en: •
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'PASCAL Visual Object Classes (VOC): The VOC 2,012 dataset [[96](#bib.bib96)]
    contains $20$ object categories including vehicles, household, animals, and other:
    aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table,
    potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person.
    Each image in this dataset has pixel-level segmentation annotations, bounding
    box annotations, and object class annotations. This dataset has been widely used
    as a benchmark for object detection, semantic segmentation, and classification
    tasks. The PASCAL VOC dataset is split into three subsets: $1,464$ images for
    training, $1,449$ images for validation and a private testing [[96](#bib.bib96)].
    All the self-supervised image representation learning methods are evaluated on
    this dataset with the three tasks.'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PASCAL视觉目标类别（VOC）：VOC 2,012数据集[[96](#bib.bib96)]包含$20$个目标类别，包括车辆、家居、动物及其他：飞机、自行车、船、公交车、汽车、摩托车、火车、瓶子、椅子、餐桌、盆栽、沙发、电视/显示器、鸟、猫、牛、狗、马、羊和人。该数据集中的每张图像都有像素级分割注释、边界框注释和目标类别注释。这个数据集被广泛用于目标检测、语义分割和分类任务的基准测试。PASCAL
    VOC数据集分为三个子集：$1,464$张用于训练，$1,449$张用于验证，还有一个私有测试集[[96](#bib.bib96)]。所有自监督图像表示学习方法都在这个数据集上进行三项任务的评估。
- en: 5.2 Video Datasets
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 视频数据集
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'YFCC100M: The Yahoo Flickr Creative Commons $100$ Million Dataset (YFCC100M)
    is a large public multimedia collection from Flickr, consisting of $100$ million
    media data, of which around $99.2$ million are images and $0.8$ million are videos
    [[114](#bib.bib114)]. The statistics on hashtags used in the YFCC100M dataset
    show that the data distribution is severely unbalanced [[119](#bib.bib119)].'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'YFCC100M: Yahoo Flickr Creative Commons $100$百万数据集（YFCC100M）是来自Flickr的大型公共多媒体集合，包含$100$百万媒体数据，其中约$99.2$百万是图像，$0.8$百万是视频[[114](#bib.bib114)]。YFCC100M数据集的标签统计数据显示，数据分布严重不平衡[[119](#bib.bib119)]。'
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SceneNet RGB-D: The SceneNet RGB-D dataset is a large indoor synthetic video
    dataset which consists of $5$ million rendered RGB-D images from over 15K trajectories
    in synthetic layouts with random but physically simulated object poses [[115](#bib.bib115)].
    It provides pixel-level annotations for scene understanding problems such as semantic
    segmentation, instance segmentation, and object detection, and also for geometric
    computer vision problems such as optical flow, depth estimation, camera pose estimation,
    and 3D reconstruction [[115](#bib.bib115)].'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SceneNet RGB-D: SceneNet RGB-D数据集是一个大型室内合成视频数据集，由$5$百万张渲染的RGB-D图像组成，来自$15K$多个在随机但物理模拟的物体姿态下的合成布局[[115](#bib.bib115)]。它提供了像素级注释，用于场景理解问题，如语义分割、实例分割和目标检测，也用于几何计算机视觉问题，如光流、深度估计、相机姿态估计和3D重建[[115](#bib.bib115)]。'
- en: •
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Moment in Time: The Moment-in-Time dataset is a large balanced and diverse
    dataset for video understanding [[116](#bib.bib116)]. The dataset consists of
    $1$ million video clips that cover $339$ classes, and each video lasts around
    $3$ seconds. The average number of video clips for each class is $1,757$ with
    a median of $2,775$. The video in this dataset contains videos that capturing
    visual and/or audible actions, produced by humans, animals, objects or nature
    [[116](#bib.bib116)].'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kinetics: The Kinetics dataset is a large-scale, high-quality dataset for human
    action recognition in videos [[17](#bib.bib17)]. The dataset consists of around
    $500,000$ video clips covering $600$ human action classes with at least $600$
    video clips for each action class. Each video clip lasts around 10 seconds and
    is labeled with a single action class.'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AudioSet: The AudioSet consists of $2,084,320$ human-labeled 10-second sound
    clips drawn from YouTube videos covers ontology of $632$ audio event classes [[117](#bib.bib117)].
    The event classes cover a wide range of human and animal sounds, musical instruments
    and genres, and common everyday environmental sounds. This dataset is mainly used
    for the self-supervised learning from video and audio consistence [[26](#bib.bib26)].'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KITTI: The KITTI dataset is collected from driving a car around a city which
    equipped with various sensors including high-resolution RGB camera, gray-scale
    stereo camera, a 3D laser scanner, and high-precision GPS measurements and IMU
    accelerations from a combined GPS/IMU system [[118](#bib.bib118)]. Videos with
    various modalities captured by these sensors are available in this dataset.'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'UCF101: The UCF101 is a widely used video dataset for human action recognition
    [[63](#bib.bib63)]. The dataset consists of $13,370$ video clips with more than
    $27$ hours belonging to $101$ categories in this dataset. The videos in this dataset
    have a spatial resolution of $320\times 240$ pixels and $25$ FPS frame rate. This
    dataset has been widely used for evaluating the performance of human action recognition.
    In the self-supervised sensorial, the self-supervised models are fine-tuned on
    the dataset and the accuracy of the action recognition are reported to evaluate
    the quality of the features.'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HMDB51: Compared to other datasets, the HMDB51 dataset is a smaller video dataset
    for human action recognition. There are around $7,000$ video clips in this dataset
    belong to $51$ human action categories [[106](#bib.bib106)]. The videos in HMDB51
    dataset have $320\times 240$ pixels spatial resolution and $30$ FPS frame rate.
    In the self-supervised sensorial, the self-supervised models are fine-tuned on
    the dataset to evaluate the quality of the learned video features.'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6 Image Feature Learning
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, three groups of self-supervised image feature learning methods
    are reviewed including generation-based methods, context-based methods, and free
    semantic label-based methods. A list of the image feature self-supervised learning
    methods can be found in Table [II](#S6.T2 "TABLE II ‣ 6 Image Feature Learning
    ‣ Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey").
    Since the cross modal-based methods mainly learn features from videos and most
    methods of this type can be used for both image and video feature learning, so
    cross modal-based methods are reviewed in the video feature learning section.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Summary of self-supervised image feature learning methods based on
    the category of pretext tasks. Multi-task means the method explicitly or implicitly
    uses multiple pretext tasks for image feature learning.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Category | Code | Contribution |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| GAN [[83](#bib.bib83)] | Generation | ✓ | Forerunner of GAN |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| DCGAN [[120](#bib.bib120)] | Generation | ✓ | Deep convolutional GAN for
    image generation |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| WGAN [[121](#bib.bib121)] | Generation | ✓ | Proposed WGAN which makes the
    training of GAN more stable |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| BiGAN [[122](#bib.bib122)] | Generation | ✓ | Bidirectional GAN to project
    data into latent space |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| SelfGAN [[123](#bib.bib123)] | Multiple | ✗ | Use rotation recognition and
    GAN for self-supervised learning |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| ColorfulColorization [[18](#bib.bib18)] | Generation | ✓ | Posing image colorization
    as a classification task |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| Colorization [[82](#bib.bib82)] | Generation | ✓ | Using image colorization
    as the pretext task |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| AutoColor [[124](#bib.bib124)] | Generation | ✓ | Training ConvNet to predict
    per-pixel color histograms |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| Split-Brain [[42](#bib.bib42)] | Generation | ✓ | Using split-brain auto-encoder
    as the pretext task |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| Context Encoder [[19](#bib.bib19)] | Generation | ✓ | Employing ConvNet to
    solve image inpainting |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| CompletNet [[125](#bib.bib125)] | Generation | ✓ | Employing two discriminators
    to guarantee local and global consistent |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| SRGAN [[15](#bib.bib15)] | Generation | ✓ | Employing GAN for single image
    super-resolution |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| SpotArtifacts [[126](#bib.bib126)] | Generation | ✓ | Learning by recognizing
    synthetic artifacts in images |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| ImproveContext [[33](#bib.bib33)] | Context | ✗ | Techniques to improve context
    based self-supervised learning methods |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| Context Prediction [[41](#bib.bib41)] | Context | ✓ | Learning by predicting
    the relative position of two patches from an image |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| Jigsaw [[20](#bib.bib20)] | Context | ✓ | Image patch Jigsaw puzzle as the
    pretext task for self-supervised learning |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| Damaged Jigsaw [[89](#bib.bib89)] | Multiple | ✗ | Learning by solving jigsaw
    puzzle, inpainting, and colorization together |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| Arbitrary Jigsaw [[88](#bib.bib88)] | Context | ✗ | Learning with jigsaw
    puzzles with arbitrary grid size and dimension |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: '| DeepPermNet [[127](#bib.bib127)] | Context | ✓ | A new method to solve image
    patch jigsaw puzzle |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| RotNet [[36](#bib.bib36)] | Context | ✓ | Learning by recognizing rotations
    of images |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| RotNet [[36](#bib.bib36)] | 上下文 | ✓ | 通过识别图像的旋转进行学习 |'
- en: '| Boosting [[34](#bib.bib34)] | Multiple | ✗ | Using clustering to boost the
    self-supervised learning methods |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 提升 [[34](#bib.bib34)] | 多重 | ✗ | 使用聚类来提升自监督学习方法 |'
- en: '| JointCluster [[128](#bib.bib128)] | Context | ✓ | Jointly learning of deep
    representations and image clusters |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| JointCluster [[128](#bib.bib128)] | 上下文 | ✓ | 联合学习深度表示和图像簇 |'
- en: '| DeepCluster [[44](#bib.bib44)] | Context | ✓ | Using clustering as the pretext
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| DeepCluster [[44](#bib.bib44)] | 上下文 | ✓ | 使用聚类作为前置任务 |'
- en: '| ClusterEmbegging [[129](#bib.bib129)] | Context | ✓ | Deep embedded clustering
    for self-supervised learning |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 聚类嵌入 [[129](#bib.bib129)] | 上下文 | ✓ | 用于自监督学习的深度嵌入聚类 |'
- en: '| GraphConstraint [[43](#bib.bib43)] | Context | ✓ | Learning with image pairs
    mined with Fisher Vector |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 图约束 [[43](#bib.bib43)] | 上下文 | ✓ | 通过Fisher向量挖掘的图像对进行学习 |'
- en: '| Ranking [[38](#bib.bib38)] | Context | ✓ | Learning by ranking video frames
    with a triplet loss |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 排名 [[38](#bib.bib38)] | 上下文 | ✓ | 通过三元组损失对视频帧进行排名学习 |'
- en: '| PredictNoise [[46](#bib.bib46)] | Context | ✓ | Learning by mapping images
    to a uniform distribution over a manifold |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| PredictNoise [[46](#bib.bib46)] | 上下文 | ✓ | 通过将图像映射到流形上的均匀分布进行学习 |'
- en: '| MultiTask [[32](#bib.bib32)] | Multiple | ✓ | Using multiple pretext tasks
    for self-supervised feature learning |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 多任务 [[32](#bib.bib32)] | 多重 | ✓ | 使用多个前置任务进行自监督特征学习 |'
- en: '| Learning2Count [[130](#bib.bib130)] | Context | ✓ | Learning by counting
    visual primitive |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Learning2Count [[130](#bib.bib130)] | 上下文 | ✓ | 通过计数视觉原语进行学习 |'
- en: '| Watching Move [[81](#bib.bib81)] | Free Semantic Label | ✓ | Learning by
    grouping pixels of moving objects in videos |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| Watching Move [[81](#bib.bib81)] | 自由语义标签 | ✓ | 通过将视频中移动物体的像素进行分组学习 |'
- en: '| Edge Detection [[81](#bib.bib81)] | Free Semantic Label | ✓ | Learning by
    detecting edges |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 边缘检测 [[81](#bib.bib81)] | 自由语义标签 | ✓ | 通过检测边缘进行学习 |'
- en: '| Cross Domain [[81](#bib.bib81)] | Free Semantic Label | ✓ | Utilizing synthetic
    data and its labels rendered by game engines |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 跨领域 [[81](#bib.bib81)] | 自由语义标签 | ✓ | 利用合成数据及游戏引擎生成的标签 |'
- en: 6.1 Generation-based Image Feature Learning
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 基于生成的图像特征学习
- en: Generation-based self-supervised methods for learning image features involve
    the process of generating images including image generation with GAN (to generate
    fake images), super-resolution (to generate high-resolution images), image inpainting
    (to predict missing image regions), and image colorization (to colorize gray-scale
    images into colorful images). For these tasks, pseudo training labels $P$ usually
    are the images themselves and no human-annotated labels are needed during training,
    therefore, these methods belong to self-supervised learning methods.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 基于生成的自监督学习方法涉及生成图像的过程，包括使用GAN生成虚假图像、超分辨率生成高分辨率图像、图像修复预测缺失的图像区域，以及图像上色将灰度图像转化为彩色图像。对于这些任务，伪训练标签$P$通常是图像本身，不需要人工标注的标签，因此这些方法属于自监督学习方法。
- en: The pioneer work about the image generation-based methods is the Autoencoder
    [[131](#bib.bib131)] which learns to compress an image into a low-dimension vector
    which then is uncompressed into the image that closes to the original image with
    a bunch of layers. With an auto-encoder, networks can reduce the dimension of
    an image into a lower dimension vector that contains the main information of the
    original image. The current image generation-based methods follow a similar idea
    but with different pipelines to learn visual features through the process of image
    generation.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基于图像生成的方法的开创性工作是自编码器[[131](#bib.bib131)]，它学习将图像压缩为低维向量，然后通过一系列层解压缩为接近原始图像的图像。使用自编码器，网络可以将图像的维度减少为包含原始图像主要信息的低维向量。当前的基于图像生成的方法遵循类似的理念，但通过不同的流程通过图像生成过程来学习视觉特征。
- en: 6.1.1 Image Generation with GAN
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 使用GAN进行图像生成
- en: 'Generative Adversarial Network (GAN) is a type of deep generative model that
    was proposed by Goodfellow et al. [[83](#bib.bib83)]. A GAN model generally consists
    of two kinds of networks: a generator which is to generate images from latent
    vectors and a discriminator which is to distinguish whether the input image is
    generated by the generator. By playing the two-player game, the discriminator
    forces the generator to generate realistic images, while the generator forces
    the discriminator to improve its differentiation ability. During the training,
    the two networks are competing against with each other and make each other stronger.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'The common architecture for the image generation from a latent variable task
    is shown in Fig. [13](#S6.F13 "Figure 13 ‣ 6.1.1 Image Generation with GAN ‣ 6.1
    Generation-based Image Feature Learning ‣ 6 Image Feature Learning ‣ Self-supervised
    Visual Feature Learning with Deep Neural Networks: A Survey"). The generator is
    trained to map any latent vector sampled from latent space into an image, while
    the discriminator is forced to distinguish whether the image from the real data
    distribution or generated data distribution. Therefore, the discriminator is required
    to capture the semantic features from images to accomplish the task. The parameters
    of the discriminator can server as the pre-trained model for other computer vision
    tasks.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6f8c3b75e9ae22732707da14ebea69be.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The pipeline of Generative Adversarial Networks [[83](#bib.bib83)].
    By playing the two-player game, the discriminator forces the generator to generate
    realistic images, while the generator forces the discriminator to improve its
    differentiation ability.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the generator $G$ is trained to learn a distribution $p_{z}$
    of real word image data to generate realist data that undistinguished from the
    real data, while the discriminator $D$ is trained to distinguish the distribution
    of the real data $p_{data}$ and of the data distribution $p_{z}$ generated by
    the generator $G$. The min-max game between the generator $G$ and the discriminator
    $D$ is formulated as:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{G}\max_{D}\mathbb{E}_{x\sim p_{data}(x)[logD(x)]}+\mathbb{E}_{z\sim
    p_{z}(z)[log(1-D(G(z)))]},$ |  | (5) |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: where $x$ is the real data, $G(z)$ is the generated data.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator $D$ is trained to maximize the probability for the real data
    $x$ (that is, $\mathbb{E}_{x\sim p_{data}(x)[logD(x)]}$) and minimize the probability
    for the generated data $G(z)$ (that is, $\mathbb{E}_{x\sim p_{data}(x)[logD(x)]}$).
    The generator is trained to generate data that close to real data $x$, so as the
    output of the discriminator is maximized $\mathbb{E}_{x\sim p_{data}(x)[logD(G(z))]}$.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Most of the methods for image generation from random variables do not need any
    human-annotated labels. However, the main purpose of this type of task is to generate
    realistic images instead of obtaining better performance on downstream applications.
    Generally, the inception scores of the generated images are used to evaluate the
    quality of the generated images [[132](#bib.bib132), [133](#bib.bib133)]. And
    only a few methods evaluated the quality of the feature learned by the discriminator
    on the high-level tasks and compared with others [[123](#bib.bib123), [122](#bib.bib122),
    [120](#bib.bib120)].
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数从随机变量生成图像的方法不需要任何人工标注的标签。然而，这类任务的主要目的是生成真实的图像，而不是在下游应用中获得更好的性能。通常，生成图像的评价是通过生成图像的Inception分数来进行的[[132](#bib.bib132),
    [133](#bib.bib133)]。而只有少数方法评估了鉴别器在高层任务上学习的特征质量，并与其他方法进行了比较[[123](#bib.bib123),
    [122](#bib.bib122), [120](#bib.bib120)]。
- en: The adversarial training can help the network to capture the real distribution
    of the real data and generate realists data, and it has been widely used in computer
    vision tasks such as image generation [[134](#bib.bib134), [135](#bib.bib135)],
    video generation [[85](#bib.bib85)],[[86](#bib.bib86)], super-resolution [[15](#bib.bib15)],
    image translation [[136](#bib.bib136)], and image inpainting [[125](#bib.bib125),
    [19](#bib.bib19)]. When there is no human-annotated label involves, the method
    falls into the self-supervised learning.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练可以帮助网络捕捉真实数据的真实分布并生成逼真的数据，它已被广泛应用于计算机视觉任务，如图像生成[[134](#bib.bib134), [135](#bib.bib135)]、视频生成[[85](#bib.bib85)],
    [[86](#bib.bib86)]、超分辨率[[15](#bib.bib15)]、图像翻译[[136](#bib.bib136)]和图像修复[[125](#bib.bib125),
    [19](#bib.bib19)]。当不涉及人工标注标签时，该方法属于自监督学习。
- en: 6.1.2 Image Generation with Inpainting
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 图像生成与修复
- en: '![Refer to caption](img/6dba18c5c7d8eabede689343696bc48e.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6dba18c5c7d8eabede689343696bc48e.png)'
- en: 'Figure 14: Qualitative illustration of image inpainting task. Given an image
    with a missing region (a), a human artist has no trouble inpainting it (b). Automatic
    inpainting using context encoder proposed in [[19](#bib.bib19)] trained with L2
    reconstruction loss and adversarial loss is shown in (c). Figure is reproduced
    based on [[19](#bib.bib19)].'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：图像修复任务的定性示意图。给定一幅有缺失区域的图像（a），人类艺术家在修复它时毫不费力（b）。使用[[19](#bib.bib19)]中提出的上下文编码器进行的自动修复，经过L2重建损失和对抗性损失训练，如图（c）所示。该图是基于[[19](#bib.bib19)]重新制作的。
- en: 'Image inpainting is a task of predicting arbitrary missing regions based on
    the rest of an image. A qualitative illustration of the image inpainting task
    is shown in Fig. [14](#S6.F14 "Figure 14 ‣ 6.1.2 Image Generation with Inpainting
    ‣ 6.1 Generation-based Image Feature Learning ‣ 6 Image Feature Learning ‣ Self-supervised
    Visual Feature Learning with Deep Neural Networks: A Survey"). The Fig. [14](#S6.F14
    "Figure 14 ‣ 6.1.2 Image Generation with Inpainting ‣ 6.1 Generation-based Image
    Feature Learning ‣ 6 Image Feature Learning ‣ Self-supervised Visual Feature Learning
    with Deep Neural Networks: A Survey")(a) is an image with a missing region, while
    the Fig [14](#S6.F14 "Figure 14 ‣ 6.1.2 Image Generation with Inpainting ‣ 6.1
    Generation-based Image Feature Learning ‣ 6 Image Feature Learning ‣ Self-supervised
    Visual Feature Learning with Deep Neural Networks: A Survey")(c) is the prediction
    of networks. To correctly predict missing regions, networks are required to learn
    the common knowledge including the color and structure of the common objects.
    Only by knowing this knowledge, networks are able to infer missing regions based
    on the rest part of the image.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图像修复是一个基于图像其余部分预测任意缺失区域的任务。图像修复任务的定性示意图如图[14](#S6.F14 "图 14 ‣ 6.1.2 图像生成与修复
    ‣ 6.1 基于生成的图像特征学习 ‣ 6 图像特征学习 ‣ 自监督视觉特征学习与深度神经网络：综述")所示。图[14](#S6.F14 "图 14 ‣ 6.1.2
    图像生成与修复 ‣ 6.1 基于生成的图像特征学习 ‣ 6 图像特征学习 ‣ 自监督视觉特征学习与深度神经网络：综述")(a) 是一幅有缺失区域的图像，而图[14](#S6.F14
    "图 14 ‣ 6.1.2 图像生成与修复 ‣ 6.1 基于生成的图像特征学习 ‣ 6 图像特征学习 ‣ 自监督视觉特征学习与深度神经网络：综述")(c)
    是网络的预测结果。为了正确预测缺失区域，网络需要学习包括常见物体的颜色和结构在内的常识。只有掌握了这些知识，网络才能根据图像的其余部分推断缺失区域。
- en: 'By analogy with auto-encoders, Pathak et al. made the first step to train a
    ConvNet to generate the contents of an arbitrary image region based on the rest
    of the image [[19](#bib.bib19)]. Their contributions are in two folds: using a
    ConvNet to tackle image inpainting problem, and using the adversarial loss to
    help the network generate a realistic hypothesis. Most of the recent methods follow
    a similar pipeline [[125](#bib.bib125)]. Usually, there are two kinds of networks:
    a generator network is to generate the missing region with the pixel-wise reconstruction
    loss and a discriminator network is to distinguish whether the input image is
    real with an adversarial loss. With the adversarial loss, the network is able
    to generate sharper and realistic hypothesis for the missing image region. Both
    the two kinds of networks are able to learn the semantic features from images
    and can be transferred to other computer vision tasks. However, only Pathak et
    al. [[19](#bib.bib19)] studied the performance of transfer learning for the learned
    parameters of the generator from the image inpainting task.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator network which is a fully convolutional network has two parts:
    encoder and decoder. The input of the encoder is the image that needs to be inpainted
    and the context encoder learns the semantic feature of the image. The context
    decoder is to predict the missing region based on this feature. The generator
    is required to understand the content of the image in order to generate a plausible
    hypothesis. The discriminator is trained to distinguish whether the input image
    is the output of the generator. To accomplish the image inpainting task, both
    networks are required to learn semantic features of images.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 Image Generation with Super Resolution
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Image super-resolution (SR) is a task of enhancing the resolution of images.
    With the help of fully convolutional networks, finer and realistic high-resolution
    images can be generated from low-resolution images. SRGAN is a generative adversarial
    network for single image super-resolution proposed by Ledig et al. [[15](#bib.bib15)].
    The insight of this approach is to take advantage of the perceptual loss which
    consists of an adversarial loss and a content loss. With the perceptron loss,
    the SRGAN is able to recover photo-realistic textures from heavily downsampled
    images and show significant gains in perceptual quality.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two networks: one is generator which is to enhance the resolution
    of the input low-resolution image and the other is the discriminator which is
    to distinguish whether the input image is the output of the generator. The loss
    function for the generator is the pixel-wise L2 loss plus the content loss which
    is the similarity of the feature of the predicted high-resolution image and the
    high-resolution original image, while the loss for the discriminator is the binary
    classification loss. Compared to the network that only minimizing the Mean Squared
    Error (MSE) which generally leads to high peak signal-to-noise ratios but lacking
    high-frequency details, the SRGAN is able to recover the fine details of the high-resolution
    image since the adversarial loss pushes the output to the natural image manifold
    by the discriminator network.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个网络：一个是生成器，用于提升输入低分辨率图像的分辨率；另一个是判别器，用于区分输入图像是否为生成器的输出。生成器的损失函数是逐像素L2损失加上内容损失，内容损失是预测的高分辨率图像的特征与高分辨率原始图像特征的相似度，而判别器的损失是二分类损失。与仅最小化均方误差（MSE）的网络相比，这种网络通常能获得较高的峰值信噪比，但缺乏高频细节，SRGAN能够恢复高分辨率图像的细节，因为对抗损失通过判别器网络将输出推向自然图像流形。
- en: The networks for image super-resolution task are able to learn the semantic
    features of images. Similar to other GANs, the parameters of the discriminator
    network can be transferred to other downstream tasks. However, no one tested the
    performance of the transferred learning on other tasks yet. The quality of the
    enhanced image is mainly compared to evaluate the performance of the network.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图像超分辨率任务的网络能够学习图像的语义特征。类似于其他GANs，判别器网络的参数可以转移到其他下游任务。然而，目前尚未有人测试转移学习在其他任务上的表现。增强图像的质量主要通过对比来评估网络的性能。
- en: 6.1.4 Image Generation with Colorization
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.4 图像生成与上色
- en: '![Refer to caption](img/80a896424f6c6b421734688050358cf3.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/80a896424f6c6b421734688050358cf3.png)'
- en: 'Figure 15: The architecture of image colorization proposed in [[18](#bib.bib18)].
    The figure is from [[18](#bib.bib18)] with author’s permission.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：图像上色的架构，见[[18](#bib.bib18)]。该图来自[[18](#bib.bib18)]，已获得作者许可。
- en: 'Image colorization is a task of predicting a plausible color version of the
    photograph given a gray-scale photograph as input. A qualitative illustration
    of the image colorization task is shown in Fig. [15](#S6.F15 "Figure 15 ‣ 6.1.4
    Image Generation with Colorization ‣ 6.1 Generation-based Image Feature Learning
    ‣ 6 Image Feature Learning ‣ Self-supervised Visual Feature Learning with Deep
    Neural Networks: A Survey"). To correctly colorize each pixel, networks need to
    recognize objects and to group pixels of the same part together. Therefore, visual
    features can be learned in the process of accomplishing this task.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '图像上色是一项任务，旨在根据给定的灰度图像预测其合理的彩色版本。图像上色任务的定性示例如图[15](#S6.F15 "Figure 15 ‣ 6.1.4
    Image Generation with Colorization ‣ 6.1 Generation-based Image Feature Learning
    ‣ 6 Image Feature Learning ‣ Self-supervised Visual Feature Learning with Deep
    Neural Networks: A Survey")所示。为了正确地为每个像素上色，网络需要识别物体并将同一部分的像素分组。因此，视觉特征可以在完成此任务的过程中进行学习。'
- en: 'Many deep learning-based colorization methods have been proposed in recent
    years [[18](#bib.bib18), [137](#bib.bib137), [138](#bib.bib138)]. A straightforward
    idea would be to employ a fully convolution neural network which consists of an
    encoder for feature extraction and a decoder for the color hallucination to colorization.
    The network can be optimized with L2 loss between the predicted color and its
    original color. Zhang et al. proposed to handle the uncertainty by posting the
    task as a classification task and used class-rebalancing to increase the diversity
    of predicted colors [[18](#bib.bib18)]. The framework for image colorization proposed
    by Zhang et al. is shown in Fig. [15](#S6.F15 "Figure 15 ‣ 6.1.4 Image Generation
    with Colorization ‣ 6.1 Generation-based Image Feature Learning ‣ 6 Image Feature
    Learning ‣ Self-supervised Visual Feature Learning with Deep Neural Networks:
    A Survey"). Trained in large-scale image collections, the method shows great results
    and fools human on $32$% of the trials during the colorization test.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '近年来提出了许多基于深度学习的着色方法[[18](#bib.bib18), [137](#bib.bib137), [138](#bib.bib138)]。一个直接的思路是使用一个完全卷积神经网络，其中包括一个用于特征提取的编码器和一个用于着色的颜色幻觉解码器。网络可以通过预测颜色与原始颜色之间的L2损失进行优化。Zhang等人提出通过将任务作为分类任务来处理不确定性，并使用类别重平衡来增加预测颜色的多样性[[18](#bib.bib18)]。Zhang等人提出的图像着色框架如图 [15](#S6.F15
    "Figure 15 ‣ 6.1.4 Image Generation with Colorization ‣ 6.1 Generation-based Image
    Feature Learning ‣ 6 Image Feature Learning ‣ Self-supervised Visual Feature Learning
    with Deep Neural Networks: A Survey")所示。该方法在大规模图像集合中训练，显示了出色的结果，在着色测试中有$32$%的试验能让人类误判。'
- en: Some work specifically employs the image colorization task as the pretext for
    self-supervised image representation learning [[82](#bib.bib82), [18](#bib.bib18),
    [124](#bib.bib124), [42](#bib.bib42)]. After the image colorization training is
    finished, the features learned through the colorization process are specifically
    evaluated on other downstream high-level tasks with transfer learning.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作专门利用图像着色任务作为自监督图像表示学习的前置任务[[82](#bib.bib82), [18](#bib.bib18), [124](#bib.bib124),
    [42](#bib.bib42)]。图像着色训练完成后，通过着色过程学到的特征会专门在其他下游高级任务中通过迁移学习进行评估。
- en: 6.2 Context-Based Image Feature Learning
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 基于上下文的图像特征学习
- en: The context-based pretext tasks mainly employ the context features of images
    including context similarity, spatial structure, and temporal structure as the
    supervision signal. Features are learned by ConvNet through the process of solving
    the pretext tasks designed based on attributes of the context of images.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上下文的前置任务主要利用图像的上下文特征，包括上下文相似性、空间结构和时间结构，作为监督信号。特征通过解决基于图像上下文属性设计的前置任务的过程，由ConvNet学习得到。
- en: 6.2.1 Learning with Context Similarity
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 基于上下文相似性的学习
- en: '![Refer to caption](img/5d52009e41c49e7e443f1a5859595907.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5d52009e41c49e7e443f1a5859595907.png)'
- en: 'Figure 16: The architecture of DeepClustering [[44](#bib.bib44)]. The features
    of images are iteratively clustered and the cluster assignments are used as pseudo-labels
    to learn the parameters of the ConvNet. The figure is from [[44](#bib.bib44)]
    with author’s permission.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '图16: DeepClustering的架构[[44](#bib.bib44)]。图像的特征被迭代地聚类，簇分配被用作伪标签来学习ConvNet的参数。该图来自[[44](#bib.bib44)]，获得了作者的许可。'
- en: Clustering is a method of grouping sets of similar data in the same clusters.
    Due to its powerful ability of grouping data by using the attributes of the data,
    it is widely used in many fields such as machine learning, image processing, computer
    graphics, etc. Many classical clustering algorithms have been proposed for various
    applications [[139](#bib.bib139)].
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种将相似数据集分组到相同簇中的方法。由于它利用数据属性进行数据分组的强大能力，它在许多领域如机器学习、图像处理、计算机图形学等得到广泛应用。已经提出了许多经典的聚类算法以适应各种应用[[139](#bib.bib139)]。
- en: In the self-supervised scenario, the clustering methods mainly employed as a
    tool to cluster image data. A naive method would be to cluster the image data
    based on the hand-designed feature such as HOG [[140](#bib.bib140)], SIFT [[141](#bib.bib141)],
    or Fisher Vector [[49](#bib.bib49)]. After the clustering, several clusters are
    obtained while the image within one cluster has a smaller distance in feature
    space and images from different clusters have a larger distance in feature space.
    The smaller the distance in feature space, the more similar the image in the appearance
    in the RGB space. Then a ConvNet can be trained to classify the data by using
    the cluster assignment as the pseudo class label. To accomplish this task, the
    ConvNet needs to learn the invariance within one class and the variance among
    different classes. Therefore, the ConvNet is able to learn semantic meaning of
    images.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在自监督场景下，主要采用聚类方法作为工具来对图像数据进行聚类。一个简单的方法是基于手工设计的特征，如HOG [[140](#bib.bib140)]、SIFT
    [[141](#bib.bib141)] 或 Fisher Vector [[49](#bib.bib49)] 对图像数据进行聚类。聚类后，会得到若干个簇，其中一个簇中的图像在特征空间中的距离较小，而不同簇中的图像在特征空间中的距离较大。特征空间中的距离越小，RGB空间中的图像外观越相似。然后可以训练一个ConvNet，通过使用簇分配作为伪类标签来对数据进行分类。为了完成这一任务，ConvNet需要学习一个类别内的不变性和不同类别间的变异性。因此，ConvNet能够学习图像的语义意义。
- en: 'The existing methods about using the clustering variants as the pretext task
    follow these principals [[44](#bib.bib44), [43](#bib.bib43), [34](#bib.bib34),
    [128](#bib.bib128), [129](#bib.bib129)]. Firstly, the image is clustered into
    different clusters which the images from the same cluster have smaller distance
    and images from different clusters have larger distance. Then a ConvNet is trained
    to recognize the cluster assignment [[44](#bib.bib44), [34](#bib.bib34)] or to
    recognize whether two imaged are from same cluster [[43](#bib.bib43)]. The pipeline
    of DeepCluster, a clustering based methods, is shown in Fig. [16](#S6.F16 "Figure
    16 ‣ 6.2.1 Learning with Context Similarity ‣ 6.2 Context-Based Image Feature
    Learning ‣ 6 Image Feature Learning ‣ Self-supervised Visual Feature Learning
    with Deep Neural Networks: A Survey"). DeepCluster iteratively clusters images
    with Kmeans and use the subsequent assignments as supervision to update the weights
    of the network. And it is the current state-of-the-art for the self-supervised
    image representation learning.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '关于使用聚类变体作为前置任务的现有方法遵循这些原则 [[44](#bib.bib44)、[43](#bib.bib43)、[34](#bib.bib34)、[128](#bib.bib128)、[129](#bib.bib129)]。首先，将图像聚类成不同的簇，其中来自同一簇的图像具有较小的距离，而来自不同簇的图像具有较大的距离。然后训练一个ConvNet来识别簇分配
    [[44](#bib.bib44)、[34](#bib.bib34)] 或识别两个图像是否来自同一簇 [[43](#bib.bib43)]。DeepCluster的流程图，如图[16](#S6.F16
    "Figure 16 ‣ 6.2.1 Learning with Context Similarity ‣ 6.2 Context-Based Image
    Feature Learning ‣ 6 Image Feature Learning ‣ Self-supervised Visual Feature Learning
    with Deep Neural Networks: A Survey")所示。DeepCluster使用Kmeans迭代聚类图像，并使用随后的分配作为监督来更新网络的权重。它是当前自监督图像表示学习的最先进方法。'
- en: 6.2.2 Learning with Spatial Context Structure
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 使用空间上下文结构学习
- en: Images contain rich spatial context information such as the relative positions
    among different patches from an image which can be used to design the pretext
    task for self-supervised learning. The pretext task can be to predict the relative
    positions of two patches from same image [[41](#bib.bib41)], or to recognize the
    order of the shuffled a sequence of patches from same image [[20](#bib.bib20),
    [89](#bib.bib89), [88](#bib.bib88)]. The context of full images can also be used
    as a supervision signal to design pretext tasks such as to recognize the rotating
    angles of the whole images [[36](#bib.bib36)]. To accomplish these pretext tasks,
    ConvNets need to learn spatial context information such as the shape of the objects
    and the relative positions of different parts of an object.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图像包含丰富的空间上下文信息，例如图像中不同区域之间的相对位置，这些信息可以用于设计自监督学习的前置任务。前置任务可以是预测同一图像中两个区域的相对位置
    [[41](#bib.bib41)]，或者识别同一图像中打乱的区域序列的顺序 [[20](#bib.bib20)、[89](#bib.bib89)、[88](#bib.bib88)]。完整图像的上下文也可以作为监督信号来设计前置任务，例如识别整个图像的旋转角度
    [[36](#bib.bib36)]。为了完成这些前置任务，ConvNets需要学习空间上下文信息，例如物体的形状和物体不同部分的相对位置。
- en: '![Refer to caption](img/da1f26085a12a2ac7a557a0288b53116.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/da1f26085a12a2ac7a557a0288b53116.png)'
- en: 'Figure 17: The visualization of the Jigsaw Image Puzzle [[20](#bib.bib20)].
    (a) is an image with $9$ sampled image patches, (b) is an example of shuffled
    image patches, and (c) shows the correct order of the sampled $9$ patches. Figure
    is reproduced based on [[20](#bib.bib20)].'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: The method proposed by Doersch et al. is one of the pioneer work of using spatial
    context cues for self-supervised visual feature learning [[41](#bib.bib41)]. Random
    pairs of image patches are extracted from each image, then a ConvNet is trained
    to recognize the relative positions of the two image patches. To solve this puzzle,
    ConvNets need to recognize objects in images and learn the relationships among
    different parts of objects. To avoid the network learns trivial solutions such
    as simply using edges in patches to accomplish the task, heavy data augmentation
    is applied during the training phase.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this idea, more methods are proposed to learn image features by solving
    more difficult spatial puzzles [[20](#bib.bib20), [89](#bib.bib89), [88](#bib.bib88),
    [87](#bib.bib87), [27](#bib.bib27)]. As illustrated in Fig. [17](#S6.F17 "Figure
    17 ‣ 6.2.2 Learning with Spatial Context Structure ‣ 6.2 Context-Based Image Feature
    Learning ‣ 6 Image Feature Learning ‣ Self-supervised Visual Feature Learning
    with Deep Neural Networks: A Survey"), one typical work proposed by Noroozi et
    al. attempted to solve an image Jigsaw puzzle with ConvNet [[20](#bib.bib20)].
    Fig. [17](#S6.F17 "Figure 17 ‣ 6.2.2 Learning with Spatial Context Structure ‣
    6.2 Context-Based Image Feature Learning ‣ 6 Image Feature Learning ‣ Self-supervised
    Visual Feature Learning with Deep Neural Networks: A Survey")(a) is an image with
    $9$ sampled image patches, Fig [17](#S6.F17 "Figure 17 ‣ 6.2.2 Learning with Spatial
    Context Structure ‣ 6.2 Context-Based Image Feature Learning ‣ 6 Image Feature
    Learning ‣ Self-supervised Visual Feature Learning with Deep Neural Networks:
    A Survey")(b) is an example of shuffled image patches, and Fig [17](#S6.F17 "Figure
    17 ‣ 6.2.2 Learning with Spatial Context Structure ‣ 6.2 Context-Based Image Feature
    Learning ‣ 6 Image Feature Learning ‣ Self-supervised Visual Feature Learning
    with Deep Neural Networks: A Survey")(c) shows the correct order of the sampled
    $9$ patches. The shuffled image patches are fed to the network which trained to
    recognize the correct spatial locations of the input patches by learning spatial
    context structures of images such as object color, structure, and high-level semantic
    information.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Given $9$ image patches from an image, there are $362,880$ $(9!)$ possible permutations
    and a network is very unlikely to recognize all of them because of the ambiguity
    of the task. To limit the number of permutations, usually, hamming distance is
    employed to choose only a subset of permutations among all the permutations that
    with relative large hamming distance. Only the selected permutations are used
    to train ConvNet to recognize the permutation of shuffled image patches [[88](#bib.bib88),
    [89](#bib.bib89), [35](#bib.bib35), [20](#bib.bib20)].
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: The main principle of designing puzzle tasks is to find a suitable task which
    is not too difficult and not too easy for a network to solve. If it is too difficult,
    the network may not converge due to the ambiguity of the task or can easily learn
    trivial solutions if it is too easy. Therefore, a reduction in the search space
    is usually employed to reduce the difficulty of the task.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Free Semantic Label-based Image Feature Learning
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The free semantic label refers to labels with semantic meanings that obtained
    without involving any human annotations. Generally, the free semantic labels such
    as segmentation masks, depth images, optic flows, and surface normal images can
    be rendered by game engine or generated by hard-code methods. Since these semantic
    labels are automatically generated, the methods using the synthetic datasets or
    using them in conjunction with a large unlabeled image or video datasets are considered
    as self-supervised learning methods.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Learning with Labels Generated by Game Engines
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given models of various objects and layouts of environments, game engines are
    able to render realistic images and provide accurate pixel-level labels. Since
    game engines can generate large-scale datasets with negligible cost, various game
    engines such as Airsim [[142](#bib.bib142)] and Carla [[143](#bib.bib143)] have
    been used to generate large-scale synthetic datasets with high-level semantic
    labels including depth, contours, surface normal, segmentation mask, and optical
    flow for training deep networks. An example of an RGB image with its generated
    accurate labels is shown in Fig. [18](#S6.F18 "Figure 18 ‣ 6.3.1 Learning with
    Labels Generated by Game Engines ‣ 6.3 Free Semantic Label-based Image Feature
    Learning ‣ 6 Image Feature Learning ‣ Self-supervised Visual Feature Learning
    with Deep Neural Networks: A Survey").'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d9f9fa16ba02e9d0c04649ff2ca5afa9.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: An example of an indoor scene generated by a game engine [[115](#bib.bib115)].
    For each synthetic image, the corresponding depth, instance segmentation, and
    optical flow can be automatically generated by the engine.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Game engines can generate realistic images with accurate pixel-level labels
    with very low cost. However, due to the domain gap between synthetic and real-world
    images, the ConvNet purely trained on synthetic images cannot be directly applied
    to real-world images. To utilize synthetic datasets for self-supervised feature
    learning, the domain gap needs to be explicitly bridged. In this way, the ConvNet
    trained with the semantic labels of the synthetic dataset can be effectively applied
    to real-world images.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome the problem, Ren and Lee proposed an unsupervised feature space
    domain adaptation method based on adversarial learning [[30](#bib.bib30)]. As
    shown in Fig. [19](#S6.F19 "Figure 19 ‣ 6.3.1 Learning with Labels Generated by
    Game Engines ‣ 6.3 Free Semantic Label-based Image Feature Learning ‣ 6 Image
    Feature Learning ‣ Self-supervised Visual Feature Learning with Deep Neural Networks:
    A Survey"), the network predicts surface normal, depth, and instance contour for
    the synthetic images and a discriminator network $D$ is employed to minimize the
    difference of feature space domains between real-world and synthetic data. Helped
    with adversarial training and accurate semantic labels of synthetic images, the
    network is able to capture visual features for real-world images.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/388f3e7846a91515df89024c28124cdb.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: The architecture for utilizing synthetic and real-world images for
    self-supervised feature learning [[30](#bib.bib30)]. Figure is reproduced based
    on [[30](#bib.bib30)].'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Compared to other pretext tasks in which the pretext tasks implicitly force
    ConvNets to learn semantic features, this type of methods are trained with accurate
    semantic labels which explicitly force ConvNets to learn features that highly
    related to the objects in images.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Learning with Labels Generated by Hard-code programs
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Applying hard-code programs is another way to automatically generate semantic
    labels such as salience, foreground masks, contours, depth for images and videos.
    With these methods, very large-scale datasets with generated semantic labels can
    be used for self-supervised feature learning. This type of methods generally has
    two steps: (1) label generation by employing hard-code programs on images or videos
    to obtain labels, (2) train ConvNets with the generated labels.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Various hard-code programs have been applied to generate labels for self-supervised
    learning methods include methods for foreground object segmentation [[81](#bib.bib81)],
    edge detection [[47](#bib.bib47)], and relative depth prediction [[92](#bib.bib92)].
    Pathak et al. proposed to learn features by training a ConvNet to segment foreground
    objects in each frame of a video while the label is the mask of moving objects
    in videos [[81](#bib.bib81)]. Li et al. proposed to learn features by training
    a ConvNet for edge prediction while labels are motion edges obtained from flow
    fields from videos [[47](#bib.bib47)]. Jing et al. proposed to learn features
    by training a ConvNet to predict relative scene depths while the labels are generated
    from optical flow [[92](#bib.bib92)].
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: No matter what kind of labels used to train ConvNets, the general idea of this
    type of methods is to distill knowledge from hard-code detector. The hard-code
    detector can be edge detector, salience detector, relative detector, etc. As long
    as no human-annotations are involved through the design of detectors, then the
    detectors can be used to generate labels for self-supervised training.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Compared to other self-supervised learning methods, the supervision signal in
    these pretext tasks is semantic labels which can directly drive the ConvNet to
    learn semantic features. However, one drawback is that the semantic labels generated
    by hard-code detector usually are very noisy which need to specifically cope with.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 7 Video Feature Learning
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section reviews the self-supervised methods for learning video features,
    as listed in Table [III](#S7.T3 "TABLE III ‣ 7 Video Feature Learning ‣ Self-supervised
    Visual Feature Learning with Deep Neural Networks: A Survey"), they can be categorized
    into four classes: generation-based methods, context-based methods, free semantic
    label-based methods, and cross modal-based methods.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Since video features can be obtained by various kinds of networks including
    2DConvNet, 3DConvNet, and LSTM combined with 2DConvNet or 3DConvNet. When 2DConvNet
    is employed for video self-supervised feature learning, then the 2DConvNet is
    able to extract both image and video features after the self-supervised pretext
    task training finished.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Summary of self-supervised video feature learning methods based
    on the category of pretext tasks.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '| Mehtod | SubCategory | Code | Contribution |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: '| VideoGAN [[85](#bib.bib85)] | Generation | ✓ | Forerunner of video generation
    with GAN |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
- en: '| MocoGAN [[86](#bib.bib86)] | Generation | ✓ | Decomposing motion and content
    for video generation with GAN |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: '| TemporalGAN [[144](#bib.bib144)] | Generation | ✓ | Decomposing temporal
    and image generator for video generation |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
- en: '| Video Colorization [[145](#bib.bib145)] | Generation | ✓ | Employing video
    colorization as the pretext task |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: '| Un-LSTM [[37](#bib.bib37)] | Generation | ✓ | Forerunner of video prediction
    with LSTM |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: '| ConvLSTM [[146](#bib.bib146)] | Generation | ✓ | Employing Convolutional
    LSTM for video prediction |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: '| MCNet [[147](#bib.bib147)] | Generation | ✓ | Disentangling motion and content
    for video prediction |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: '| LSTMDynamics [[148](#bib.bib148)] | Generation | ✗ | Learning by predicting
    long-term temporal dynamic in videos |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: '| Video Jigsaw [[87](#bib.bib87)] | Context | ✗ | Learning by jointly reasoning
    about spatial and temporal context |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
- en: '| Transitive [[31](#bib.bib31)] | Context | ✗ | Learning inter and intra instance
    variations with a Triplet loss |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: '| 3DRotNet [[28](#bib.bib28)] | Context | ✗ | Learning by recognizing rotations
    of video clips |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: '| CubicPuzzles [[27](#bib.bib27)] | Context | ✗ | Learning by solving video
    cubic puzzles |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: '| ShuffleLearn [[40](#bib.bib40)] | Context | ✓ | Employing temporal order
    verification as the pretext task |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
- en: '| LSTMPermute [[149](#bib.bib149)] | Context | ✓ | Learning by temporal order
    verification with LSTM |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
- en: '| OPN [[39](#bib.bib39)] | Context | ✓ | Using frame sequence order recognition
    as the pretext task |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
- en: '| O3N [[29](#bib.bib29)] | Context | ✗ | Learning by identifying odd video
    sequences |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
- en: '| ArrowTime [[90](#bib.bib90)] | Context | ✓ | Learning by recognizing the
    arrow of time in videos |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: '| TemporalCoherence [[150](#bib.bib150)] | Context | ✗ | Learning with the
    temporal coherence of features of frame sequence |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '| FlowNet [[151](#bib.bib151)] | Cross Modal | ✓ | Forerunner of optical flow
    estimation with ConvNet |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '| FlowNet2 [[152](#bib.bib152)] | Cross Modal | ✓ | Better architecture and
    better performance on optical flow estimation |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: '| UnFlow [[153](#bib.bib153)] | Cross Modal | ✓ | An unsupervised loss for
    optical flow estimation |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| CrossPixel [[23](#bib.bib23)] | Cross Modal | ✗ | Learning by predicting
    motion from a single image as the pretext task |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: '| CrossModel [[24](#bib.bib24)] | Cross Modal | ✗ | Optical flow and RGB correspondence
    verification as pretext task |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: '| AVTS [[25](#bib.bib25)] | Cross Modal | ✗ | Visual and Audio correspondence
    verification as pretext task |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| AudioVisual [[26](#bib.bib26)] | Cross Modal | ✓ | Jointly modeling visual
    and audio as fused multisensory representation |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| LookListenLearn [[93](#bib.bib93)] | Cross Modal | ✓ | Forerunner of Audio-Visual
    Correspondence for self-supervised learning |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| AmbientSound [[154](#bib.bib154)] | Cross Modal | ✗ | Predicting a statistical
    summary of the sound from a video frame |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '| EgoMotion [[155](#bib.bib155)] | Cross Modal | ✓ | Learning by predicting
    camera motion and the scene structure from videos |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '| LearnByMove [[94](#bib.bib94)] | Cross Modal | ✓ | Learning by predicting
    the camera transformation from a pairs of images |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: '| TiedEgoMotion [[95](#bib.bib95)] | Cross Modal | ✗ | Learning from ego-motor
    signals and video sequence |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| GoNet [[156](#bib.bib156)] | Cross Modal | ✓ | Jointly learning monocular
    depth, optical flow and ego-motion estimation from videos |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| DepthFlow [[157](#bib.bib157)] | Cross Modal | ✓ | Depth and optical flow
    learning using cross-task consistency from videos |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| VisualOdometry [[158](#bib.bib158)] | Cross Modal | ✓ | An unsupervised paradigm
    for deep visual odometry learning |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| ActivesStereoNet [[159](#bib.bib159)] | Cross Modal | ✓ | End-to-end self-supervised
    learning of depth from active stereo systems |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: 7.1 Generation-based Video Feature Learning
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning from video generation refers to the methods that visual features are
    learned through the process of video generation while without using any human-annotated
    labels. This type of methods includes video generation with GAN [[85](#bib.bib85)],
    video colorization [[145](#bib.bib145)] and video prediction [[37](#bib.bib37)].
    For these pretext tasks, the pseudo training label $P$ usually is the video itself
    and no human-annotated labels are needed during training, therefore, these methods
    belong to self-supervised learning.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.1 Learning from Video Generation
  id: totrans-368
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a0d2839742af9fc3c4644002e0dea100.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: The architecture of the generator in VideoGan for video generation
    with GAN proposed in [[85](#bib.bib85)]. The figure is from [[85](#bib.bib85)]
    with author’s permission.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'After GAN-based methods obtained breakthrough results in image generation,
    researchers employed GAN to generate videos [[86](#bib.bib86), [85](#bib.bib85),
    [144](#bib.bib144)]. One pioneer work of video generation with GAN is VideoGAN
    [[85](#bib.bib85)], and the architecture of the generator network is shown in
    Fig. [20](#S7.F20 "Figure 20 ‣ 7.1.1 Learning from Video Generation ‣ 7.1 Generation-based
    Video Feature Learning ‣ 7 Video Feature Learning ‣ Self-supervised Visual Feature
    Learning with Deep Neural Networks: A Survey"). To model the motion of objects
    in videos, a two-stream network is proposed for video generation while one stream
    is to model the static regions in in videos as background and another stream is
    to model moving object in videos as foreground [[85](#bib.bib85)]. Videos are
    generated by the combination of the foreground and background streams. The underline
    assumption is that each random variable in the latent space represents one video
    clip. This method is able to generate videos with dynamic contents. However, Tulyakov
    et al. argues that this assumption increases difficulties of the generation, instead,
    they proposed MocoGAN to use the combination of two subspace to represent a video
    by disentangling the context and motions in videos [[86](#bib.bib86)]. One space
    is context space which each variable from this space represents one identity,
    and another space is motion space while the trajectory in this space represents
    the motion of the identity. With the two sub-spaces, the network is able to generate
    videos with higher inception score.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: The generator learns to map latent vectors from latent space into videos, while
    discriminator learns to distinguish the real world videos with generated videos.
    Therefore, the discriminator needs to capture the semantic features from videos
    to accomplish this task. When no human-annotated labels are used in these frameworks,
    they belong to the self-supervised learning methods. After the video generation
    training on large-scale unlabeled dataset finished, the parameters of discriminator
    can be transferred to other downstream tasks [[85](#bib.bib85)].
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.2 Learning from Video Colorization
  id: totrans-373
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Temporal coherence in videos refers to that consecutive frames within a short
    time have similar coherent appearance. The coherence of color can be used to design
    pretext tasks for self-supervised learning. One way to utilize color coherence
    is to use video colorization as a pretext task for self-supervised video feature
    learning.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Video colorization is a task to colorize gray-scale frames into colorful frames.
    Vondrick et al. proposed to constrain colorization models to solve video colorization
    by learning to copy colors from a reference frame [[145](#bib.bib145)]. Given
    the reference RGB frame and a gray-scale image, the network needs to learn the
    internal connection between the reference RGB frame and gray-scale image to colorize
    it.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Another perspective is to tackle video colorization by employing a fully convolution
    neural network. Tran et al. proposed an U-shape convolution neural network for
    video colorization [[160](#bib.bib160)]. The network is an encoder-decoder based
    3DConvNet. The input of the network is a clip of grayscale video clip, while the
    output if a colorful video clip. The encoder is a bunch of 3D convolution layers
    to extract features while the decoder is a bunch of 3D deconvolution layers to
    generate colorful video clips from the extracted feature.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: The color coherence in videos is a strong supervision signal. However, only
    a few work studied to employ it for self-supervised video feature learning [[145](#bib.bib145)].
    More work can be done by studying using color coherence as a supervision signal
    for self-supervised video feature learning.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.3 Learning from Video Prediction
  id: totrans-378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5070f45c78c2fe071e072f942f817bfe.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: The architecture for video prediction task proposed by [[147](#bib.bib147)].
    Figure is reproduced based on [[147](#bib.bib147)].'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Video prediction is a task of predicting future frame sequences based on a limited
    number of frames of a video. To predict future frames, network must learn the
    change in appearance within a given frame sequence. The pioneer of applying deep
    learning for video prediction is Un-LSTM [[37](#bib.bib37)]. Due to the powerful
    ability of modeling long-term dynamic in videos, LSTM is used in both the encoder
    and decoder [[37](#bib.bib37)].
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Many methods have been proposed for video prediction [[37](#bib.bib37), [147](#bib.bib147),
    [161](#bib.bib161), [162](#bib.bib162), [163](#bib.bib163), [164](#bib.bib164),
    [165](#bib.bib165)]. Since its superior ability to model temporal dynamics, most
    of them use LSTM or LSTM variant to encode temporal dynamics in videos or to infer
    the future frames [[37](#bib.bib37), [147](#bib.bib147), [146](#bib.bib146), [164](#bib.bib164),
    [165](#bib.bib165)]. These methods can be employed for self-supervised feature
    learning without using human-annotations.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the frameworks follow the encoder-decoder pipeline in which the encoder
    to model spatial and temporal features from the given video clips and the decoder
    to generate future frames based on feature extracted by encoder. Fig. [21](#S7.F21
    "Figure 21 ‣ 7.1.3 Learning from Video Prediction ‣ 7.1 Generation-based Video
    Feature Learning ‣ 7 Video Feature Learning ‣ Self-supervised Visual Feature Learning
    with Deep Neural Networks: A Survey") shows a pipeline of MCnet proposed by Villegas
    et al. in [[147](#bib.bib147)]. McNet is built on Encoder-Decoder Convolutional
    Neural Network and Convolutional LSTM for video prediction. It has two encoders,
    one is Content Encoder to capture the spatial layout of an image, and the other
    is Motion Encoder to model temporal dynamics within video clips. The spatial features
    and temporal features are concatenated to feed to the decoder to generate the
    next frame. By separately modeling temporal and spatial features, this model can
    effectively generate future frames recursively.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Video prediction is a self-supervised learning task and the learned features
    can be transferred to other tasks. However, no work has been done to study the
    generalization ability of features learned by video prediction. Generally, The
    Structural Similarity Index (SSIM) and Peak Signal to Noise Ratio (PSNR) are employed
    to evaluate the difference between the generated frame sequence and the ground
    truth frame sequence.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Temporal Context-based Learning
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9028afb64ce5ed2594034de269e24ada.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: The pipeline of Shuffle and Learn [[40](#bib.bib40)]. The network
    is trained to verify whether the input frames are in correct temporal order. Figure
    is reproduced based on [[40](#bib.bib40)].'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Videos consist of various lengths of frames which have rich spatial and temporal
    information. The inherent temporal information within videos can be used as supervision
    signal for self-supervised feature learning. Various pretext tasks have been proposed
    by utilizing temporal context relations including temporal order verification
    [[40](#bib.bib40), [29](#bib.bib29), [90](#bib.bib90)] and temporal order recognition
    [[39](#bib.bib39), [27](#bib.bib27)]. Temporal order verification is to verify
    whether a sequence of input frames is in correct temporal order, while temporal
    order recognition is to recognize the order of a sequence of input frames.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [22](#S7.F22 "Figure 22 ‣ 7.2 Temporal Context-based Learning
    ‣ 7 Video Feature Learning ‣ Self-supervised Visual Feature Learning with Deep
    Neural Networks: A Survey"), Misra et al. proposed to use the temporal order verification
    as the pretext task to learn image features from videos with 2DConvNet [[40](#bib.bib40)]
    which has two main steps: (1) The frames with significant motions are sampled
    from videos according to the magnitude of optical flow, (2) The sampled frames
    are shuffled and fed to the network which is trained to verify whether the input
    data is in correct order. To successfully verify the order of the input frames,
    the network is required to capture the subtle difference between the frames such
    as the movement of the person. Therefore, semantic features can be learned through
    the process of accomplishing this task. The temporal order recognition tasks use
    networks of similar architecture.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: However, the methods usually suffer from a massive dataset preparation step.
    The frame sequences that used to train the network are selected based on the magnitude
    of the optical flow, and the computation process of optical flow is expensive
    and slow. Therefore, more straightforward and time-efficiency methods are needed
    for self-supervised video feature learning.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Cross Modal-based Learning
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cross modal-based learning methods usually learn video features from the correspondence
    of multiple data streams including RGB frame sequence, optical flow sequence,
    audio data, and camera pose.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to rich temporal and spatial information in videos, optical flow
    sequence can be generated to specifically indicate the motion in videos, and the
    difference of frames can be computed with negligible time and space-time complexity
    to indicate the boundary of the moving objects. Similarly, audio data also provide
    a useful hint about the content of videos. Based on the type of data used, these
    methods fall into three groups: (1) methods that learn features by using the RGB
    and optical flow correspondence [[24](#bib.bib24), [23](#bib.bib23)], (2) methods
    that learn features by utilizing the video and audio correspondence [[25](#bib.bib25),
    [93](#bib.bib93)], (3) ego-motion that learn by utilizing the correspondence between
    egocentric video and ego-motor sensor signals [[95](#bib.bib95), [94](#bib.bib94)].
    Usually, the network is trained to recognize if the two kinds of input data are
    corresponding to each other [[24](#bib.bib24), [25](#bib.bib25)], or is trained
    to learn the transformation between different modalities [[94](#bib.bib94)].'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1 Learning from RGB-Flow Correspondence
  id: totrans-394
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Optical flow encodes object motions between adjacent frames, while RGB frames
    contain appearance information. The correspondence of the two types of data can
    be used to learn general features [[23](#bib.bib23), [24](#bib.bib24), [151](#bib.bib151),
    [152](#bib.bib152)]. This type of pretext tasks include optical flow estimation
    [[151](#bib.bib151), [152](#bib.bib152)] and RGB and optical flow correspondence
    verification [[23](#bib.bib23)].
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: Sayed et al. proposed to learn video features by verifying whether the input
    RGB frames and the optical flow corresponding to each other. Two networks are
    employed while one is for extracting features from RGB input and another is for
    extracting features from optical flow input [[24](#bib.bib24)]. To verify whether
    two input data correspond to each other, the network needs to capture mutual information
    between the two modalities. The mutual information across different modalities
    usually has higher semantic meaning compared to information which is modality
    specific. Through this pretext task, the mutual information that invariant to
    specific modality can be captured by ConvNet.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Optical flow estimation is another type of pretext tasks that can be used for
    self-supervised video feature learning. Fischer et al. proposed FlowNet which
    is an end-to-end convolution neural network for optical flow estimation from two
    consecutive frames [[151](#bib.bib151), [152](#bib.bib152)]. To correctly estimate
    optical flow from two frames, the ConvNet needs to capture appearance changes
    of two frames. Optical flow estimation can be used for self-supervised feature
    learning because it can be automatically generated by simulators such as game
    engines or by hard-code programs without human annotation.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2 Learning from Visual-Audio Correspondence
  id: totrans-398
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently, some researchers proposed to use the correspondence between visual
    and audio streams to design “Visual-Audio Correspondence” learning task [[93](#bib.bib93),
    [25](#bib.bib25), [26](#bib.bib26), [154](#bib.bib154)].
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fdd158141dac8c62343efa7abb8207d5.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: The architecture of video and audio correspondence verification
    task [[93](#bib.bib93)].'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 'The general framework of this type of pretext tasks is shown in Fig. [23](#S7.F23
    "Figure 23 ‣ 7.3.2 Learning from Visual-Audio Correspondence ‣ 7.3 Cross Modal-based
    Learning ‣ 7 Video Feature Learning ‣ Self-supervised Visual Feature Learning
    with Deep Neural Networks: A Survey"). There are two subnetworks: the vision subnetwork
    and the audio subnetwork. The input of vision subnetwork is a single frame or
    a stack of image frames and the vision subnetwork learns to capture visual features
    of the input data. The audio network is a 2DConvNet and the input is the Fast
    Fourier Transform (FFT) of the audio from the video. Positive data are sampled
    by extracting video frames and audio from the same time of one video, while negative
    training data are generated by extracting video frames and audio from different
    videos or from different times of one video. Therefore, the networks are trained
    to discover the correlation of video data and audio data to accomplish this task.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Since the inputs of the ConvNets are two kinds of data, the networks are able
    to learn the two kinds of information jointly by solving the pretext task. The
    performance of the two networks obtained very good performance on the downstream
    applications [[25](#bib.bib25)].
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.3 Ego-motion
  id: totrans-404
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the self-driving car which usually equipped with various sensors, the large-scale
    egocentric video along with ego-motor signal can be easily collected with very
    low cost by driving the car in the street. Recently, some researchers proposed
    to use the correspondence between visual signal and motor signal for self-supervised
    feature learning [[155](#bib.bib155), [94](#bib.bib94), [95](#bib.bib95)].
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4dc6d9ce18ac2584857443f1f8e7a4f8.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: The architecture of camera pose transformation estimation from egocentric
    videos [[94](#bib.bib94)].'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'The underline intuition of this type of methods is that a self-driving car
    can be treated as a camera moving in a scene and thus the egomotion of the visual
    data captured by the camera is as same as that of the car. Therefore, the correspondence
    between visual data and egomotion can be utilized for self-supervised feature
    learning. A typical network of using ego-motor signal is shown in Fig. [24](#S7.F24
    "Figure 24 ‣ 7.3.3 Ego-motion ‣ 7.3 Cross Modal-based Learning ‣ 7 Video Feature
    Learning ‣ Self-supervised Visual Feature Learning with Deep Neural Networks:
    A Survey") proposed by Agrawal et al. for self-supervised image feature learning
    [[94](#bib.bib94)]. The inputs to the network are two frames sampled from an egocentric
    video within a short time. The labels for the network indicate the rotation and
    translation relation between the two sampled images which can be derived from
    the odometry data of the dataset. With this task, the ConvNet is forced to identify
    visual elements that are present in both sampled images.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: The ego-motor signal is a type of accurate supervision signal. In addition to
    directly applying it for self-supervised feature learning, it has also been used
    for unsupervised learning of depth and ego-motion [[155](#bib.bib155)]. All these
    networks can be used for self-supervised feature learning and transferred for
    downstream tasks.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 8 Performance Comparison
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section compares the performance of image and video feature self-supervised
    learning methods on public datasets. For image feature self-supervised learning,
    the performance on downstream tasks including image classification, semantic segmentation,
    and object detection are compared. For video feature self-supervised learning,
    the performance on a downstream task which is human action recognition in videos
    is reported.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Performance of Image Feature Learning
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As described in Section 4.3, the quality of features learned by self-supervised
    learned models is evaluated by fine-tuning them on downstream tasks such as semantic
    segmentation, object detection, and image classification. This section summarizes
    the performance of the existing image feature self-supervised learning methods.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [IV](#S8.T4 "TABLE IV ‣ 8.1 Performance of Image Feature Learning ‣ 8
    Performance Comparison ‣ Self-supervised Visual Feature Learning with Deep Neural
    Networks: A Survey") lists the performance of image classification performance
    on ImageNet [[13](#bib.bib13)] and Places [[107](#bib.bib107)] datasets. During
    self-supervised pretext tasks training, most of the methods are trained on ImageNet
    dataset with AlexNet as based network without using the category labels. After
    pretext task self-supervised training finished, a linear classifier is trained
    on top of different frozen convolutional layers of the ConvNet on the training
    split of ImageNet and Places datasets. The classification performances on the
    two datasets are used to demonstrate the quality of the learned features.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [IV](#S8.T4 "TABLE IV ‣ 8.1 Performance of Image Feature
    Learning ‣ 8 Performance Comparison ‣ Self-supervised Visual Feature Learning
    with Deep Neural Networks: A Survey"), the overall performance of the self-supervised
    models is lower than that of models trained either with ImageNet labels or with
    Places labels. Among all the self-supervised methods, the DeepCluster [[44](#bib.bib44)]
    achieved the best performance on the two dataset. Three conclusions can be drawn
    based on the performance from the Table: (1) The features from different layers
    are always benefited from the self-supervised pretext task training. The performance
    of self-supervised learning methods is always better than the performance of the
    model trained from scratch. (2) All of the self-supervised methods perform well
    with the features from conv3 and conv4 layers while performing worse with the
    features from conv1, conv2, and conv5 layers. This is probably because shallow
    layers capture general low-level features, while deep layers capture pretext task-related
    features. (3) When there is a domain gap between dataset for pretext task training
    and the dataset of downstream task, the self-supervised learning method is able
    to reach comparable performance with the model trained with ImageNet labels.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Linear classification on ImageNet and Places datasets using activations
    from the convolutional layers of an AlexNet as features. ”Convn” means the linear
    classifier is trained based on the n-th convolution layer of AlexNet. ”Places
    Labels” and ”ImageNet Labels” indicate using supervised model trained with human-annotated
    labels as the pre-trained model.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | ImageNet |  | Places |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
- en: '| Method | Pretext Tasks | conv1 | conv2 | conv3 | conv4 | conv5 |  | conv1
    | conv2 | conv3 | conv4 | conv5 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
- en: '| Places labels [[8](#bib.bib8)] | — | — | — | — | — | — |  | $22.1$ | $35.1$
    | $40.2$ | $43.3$ | $44.6$ |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
- en: '| ImageNet labels [[8](#bib.bib8)] | — | $19.3$ | $36.3$ | $44.2$ | $48.3$
    | $50.5$ |  | $22.7$ | $34.8$ | $38.4$ | $39.4$ | $38.7$ |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
- en: '| Random(Scratch) [[8](#bib.bib8)] | — | $11.6$ | $17.1$ | $16.9$ | $16.3$
    | $14.1$ |  | $15.7$ | $20.3$ | $19.8$ | $19.1$ | $17.5$ |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
- en: '| ColorfulColorization [[18](#bib.bib18)] | Generation | $12.5$ | $24.5$ |
    $30.4$ | $31.5$ | $30.3$ |  | $16.0$ | $25.7$ | $29.6$ | $30.3$ | $29.7$ |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
- en: '| BiGAN [[122](#bib.bib122)] | Generation | $17.7$ | $24.5$ | $31.0$ | $29.9$
    | $28.0$ |  | $21.4$ | $26.2$ | $27.1$ | $26.1$ | $24.0$ |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
- en: '| SplitBrain [[42](#bib.bib42)] | Generation | $17.7$ | $29.3$ | $35.4$ | $35.2$
    | $32.8$ |  | $21.3$ | $30.7$ | $34.0$ | $34.1$ | $32.5$ |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
- en: '| ContextEncoder [[19](#bib.bib19)] | Context | $14.1$ | $20.7$ | $21.0$ |
    $19.8$ | $15.5$ |  | $18.2$ | $23.2$ | $23.4$ | $21.9$ | $18.4$ |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
- en: '| ContextPrediction [[41](#bib.bib41)] | Context | $16.2$ | $23.3$ | $30.2$
    | $31.7$ | $29.6$ |  | $19.7$ | $26.7$ | $31.9$ | $32.7$ | $30.9$ |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '| Jigsaw [[20](#bib.bib20)] | Context | 18.2 | $28.8$ | $34.0$ | $33.9$ | $27.1$
    |  | $23.0$ | $32.1$ | $35.5$ | $34.8$ | $31.3$ |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
- en: '| Learning2Count [[130](#bib.bib130)] | Context | $18.0$ | $30.6$ | $34.3$
    | $32.5$ | $25.7$ |  | 23.3 | 33.9 | $36.3$ | $34.7$ | $29.6$ |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
- en: '| DeepClustering [[44](#bib.bib44)] | Context | $13.4$ | 32.3 | 41.0 | 39.6
    | 38.2 |  | $19.6$ | $33.2$ | 39.2 | 39.8 | 34.7 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: Comparison of the self-supervised image feature learning methods on
    classification, detection, and segmentation on Pascal VOC dataset. ”ImageNet Labels”
    indicates using supervised model trained with human-annotated labels as the pre-trained
    model.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Pretext Tasks | Classification |  | Detection |  | Segmentation
    |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
- en: '| ImageNet Labels  [[8](#bib.bib8)] | — | $~{}79.9~{}$ |  | $~{}56.8~{}$ |  |
    $~{}48.0~{}$ |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
- en: '| Random(Scratch) [[8](#bib.bib8)] | — | $~{}57.0~{}$ |  | $~{}44.5~{}$ |  |
    $~{}30.1~{}$ |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
- en: '| ContextEncoder  [[19](#bib.bib19)] | Generation | $~{}56.5~{}$ |  | $~{}44.5~{}$
    |  | $~{}29.7~{}$ |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
- en: '| BiGAN  [[122](#bib.bib122)] | Generation | $~{}60.1~{}$ |  | $~{}46.9~{}$
    |  | $~{}35.2~{}$ |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
- en: '| ColorfulColorization  [[18](#bib.bib18)] | Generation | $~{}65.9~{}$ |  |
    $~{}46.9~{}$ |  | $~{}35.6~{}$ |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '| SplitBrain [[42](#bib.bib42)] | Generation | $~{}67.1~{}$ |  | $~{}46.7~{}$
    |  | $~{}36.0~{}$ |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '| RankVideo  [[38](#bib.bib38)] | Context | $~{}63.1~{}$ |  | $~{}47.2~{}$
    |  | $~{}35.4^{\dagger}$ |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| PredictNoise  [[46](#bib.bib46)] | Context | $~{}65.3~{}$ |  | $~{}49.4~{}$
    |  | $~{}37.1^{\dagger}$ |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| JigsawPuzzle [[20](#bib.bib20)] | Context | $~{}67.6~{}$ |  | $~{}53.2~{}$
    |  | $~{}37.6~{}$ |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| ContextPrediction [[41](#bib.bib41)] | Context | $~{}65.3~{}$ |  | $~{}51.1~{}$
    |  | — |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '| Learning2Count [[130](#bib.bib130)] | Context | $~{}67.7~{}$ |  | $~{}51.4~{}$
    |  | $~{}36.6~{}$ |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: '| DeepClustering  [[44](#bib.bib44)] | Context | 73.7 |  | 55.4 |  | 45.1 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
- en: '| WatchingVideo  [[81](#bib.bib81)] | Free Semantic Label | $~{}61.0~{}$ |  |
    $~{}52.2~{}$ |  | — |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
- en: '| CrossDomain  [[30](#bib.bib30)] | Free Semantic Label | $~{}68.0~{}$ |  |
    $~{}52.6~{}$ |  | — |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
- en: '| AmbientSound  [[154](#bib.bib154)] | Cross Modal | $~{}61.3~{}$ |  | — |  |
    — |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
- en: '| TiedToEgoMotion  [[95](#bib.bib95)] | Cross Modal | — |  | $41.7$ |  | —
    |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
- en: '| EgoMotion  [[94](#bib.bib94)] | Cross Modal | $~{}54.2~{}$ |  | $43.9$ |  |
    — |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
- en: 'In addition to image classification, object detection and semantic segmentation
    are also used as the downstream tasks to evaluate the quality of the features
    learned by self-supervised learning. Usually, ImageNet is used for self-supervised
    pretext task pre-training by discarding category labels, while the AlexNet is
    used as the base network and fine-tuned on the three tasks. Table [V](#S8.T5 "TABLE
    V ‣ 8.1 Performance of Image Feature Learning ‣ 8 Performance Comparison ‣ Self-supervised
    Visual Feature Learning with Deep Neural Networks: A Survey") lists the performance
    of image classification, object detection, and semantic segmentation tasks on
    the PASCAL VOC dataset. The performance of classification and detection is obtained
    by testing the model on the test split of PASCAL VOC 2007 dataset, while the performance
    of semantic segmentation is obtained by testing the model on the validation split
    of PASCAL VOC 2012 dataset.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [V](#S8.T5 "TABLE V ‣ 8.1 Performance of Image Feature Learning
    ‣ 8 Performance Comparison ‣ Self-supervised Visual Feature Learning with Deep
    Neural Networks: A Survey"), the performance of the self-supervised models on
    segmentation and detection dataset are very close to that of the supervised method
    which is trained with ImageNet labels during pre-training. Specifically, the margins
    of the performance differences on the object detection and semantic segmentation
    tasks are less than $3$% which indicate that the learned features by self-supervised
    learning have a good generalization ability. Among all the self-supervised learning
    methods, the DeepClustering [[44](#bib.bib44)] obtained the best performance on
    all the tasks.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Performance of Video Feature Learning
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE VI: Comparison of the existing self-supervised methods for action recognition
    on the UCF101 and HMDB51 datasets. * indicates the average accuracy over three
    splits. ”Kinetics Labels” indicates using supervised model trained with human-annotated
    labels as the pre-trained model.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Pretext Task | UCF101 | HMDB51 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
- en: '| Kinetics Labels* [[70](#bib.bib70)] | — | $84.4$ | $56.4$ |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
- en: '| VideoGAN [[85](#bib.bib85)] | Generation | $52.1$ | — |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
- en: '| VideoRank [[38](#bib.bib38)] | Context | $40.7$ | $15.6$ |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
- en: '| ShuffleLearn [[40](#bib.bib40)] | Context | $50.9$ | $19.8$ |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
- en: '| OPN [[29](#bib.bib29)] | Context | $56.3$ | $22.1$ |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
- en: '| RL [[35](#bib.bib35)] | Context | $58.6$ | $25.0$ |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
- en: '| AOT [[90](#bib.bib90)] | Context | $58.6$ | — |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
- en: '| 3DRotNet [[28](#bib.bib28)] | Context | $62.9$ | 33.7 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
- en: '| CubicPuzzle*  [[27](#bib.bib27)] | Context | 65.8 | 33.7 |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
- en: '| RGB-Flow [[24](#bib.bib24)] | Cross Modal | $59.3$ | $27.7$ |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
- en: '| PoseAction [[48](#bib.bib48)] | Cross Modal | $55.4$ | $23.6$ |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
- en: 'For self-supervised video feature learning methods, human action recognition
    task is used to evaluate the quality of learned features. Various video datasets
    have been used for self-supervised pre-training, and different network architectures
    have been used as the base network. Usually after the pretext task pre-training
    finished, networks are fine-tuned and tested on the commonly used UCF101 and HMDB51
    datasets for human action recognition task. Table  [VI](#S8.T6 "TABLE VI ‣ 8.2
    Performance of Video Feature Learning ‣ 8 Performance Comparison ‣ Self-supervised
    Visual Feature Learning with Deep Neural Networks: A Survey") compares the performance
    of existing self-supervised video feature learning methods on UCF101 and HMDB51
    datasets.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [VI](#S8.T6 "TABLE VI ‣ 8.2 Performance of Video Feature
    Learning ‣ 8 Performance Comparison ‣ Self-supervised Visual Feature Learning
    with Deep Neural Networks: A Survey"), the best performance of the fine-tune results
    on UCF101 is less than $66$%. However, the supervised model which trained with
    Kinetics labels can easily obtain an accuracy of more than $84$%. The performance
    of the self-supervised model is still much lower than the performance of the supervised
    model. More effective self-supervised video feature learning methods are desired.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Summary
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the results, conclusions can be drawn about the performance and reproducibility
    of the self-supervised learning methods.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance: For image feature self-supervised learning, due to the well-designed
    pretext tasks, the performance of self-supervised methods are comparable to the
    supervised methods on some downstream tasks, especially for the object detection
    and semantic segmentation tasks. The margins of the performance differences on
    the object detection and semantic segmentation tasks are less than $3$% which
    indicate that the learned features by self-supervised learning have a good generalization
    ability. However, the performance of video feature self-supervised learning methods
    is still much lower than that of the supervised models on downstream tasks. The
    best performance of the 3DConvNet-based methods on UCF101 dataset is more than
    $18$% lower than that of the supervised model [[70](#bib.bib70)]. The poor performance
    of 3DCovnNet self-supervised learning methods probably because 3DConvNets usually
    have more parameters which lead to easily over-fitting and the complexity of video
    feature learning due to the temporal dimension of the video.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: 'Reproducibility: As we can observe, for the image feature self-supervised learning
    methods, most of the networks use AlexNet as a base network to pre-train on ImageNet
    dataset and then evaluate on same downstream tasks for quality evaluation. Also,
    the code of most methods are released which is a great help for reproducing results.
    However, for the video self-supervised learning, various datasets and networks
    have been used for self-supervised pre-training, therefore, it is unfair to directly
    compare different methods. Furthermore, some methods use UCF101 as self-supervised
    pre-training dataset which is a relatively small video dataset. With this size
    of the dataset, the power of a more powerful model such as 3DCovnNet may not be
    fully discovered and may suffer from server over-fitting. Therefore, larger datasets
    for video feature self-supervised pre-training should be used.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Metrics: Another fact is that more evaluation metrics are needed
    to evaluate the quality of the learned features in different levels. The current
    solution is to use the performance on downstream tasks to indicate the quality
    of the features. However, this evaluation metric does not give insight what the
    network learned through the self-supervsied pre-training. More evaluation metrics
    such as network dissection [[78](#bib.bib78)] should be employed to analysis the
    interpretability of the self-supervised learned features.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: 9 Future Directions
  id: totrans-474
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Self-supervised learning methods have been achieving great success and obtaining
    good performance that close to supervised models on some computer vision tasks.
    Here, some future directions of self-supervised learning are discussed.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning Features from Synthetic Data: A rising trend of self-supervised learning
    is to train networks with synthetic data which can be easily rendered by game
    engines with very limited human involvement. With the help of game engines, millions
    of synthetic images and videos with accuracy pixel-level annotations can be easily
    generated. With accurate and detailed annotations, various pretext tasks can be
    designed to learn features from synthetic data. One problem needed to solve is
    how to bridge the domain gap between synthetic data and real-world data. Only
    a few work explored self-supervised learning from synthetic data by using GAN
    to bridge the domain gap [[30](#bib.bib30), [166](#bib.bib166)]. With more available
    large-scale synthetic data, more self-supervised learning methods will be proposed.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning from Web Data: Another rising trend is to train networks with web
    collected data [[167](#bib.bib167), [22](#bib.bib22), [168](#bib.bib168)] based
    on their existing associated tags. With the search engine, millions of images
    and videos can be downloaded from websites like Flickr and YouTube with negligible
    cost. In addition to its raw data, the title, keywords, and reviews can also be
    available as part of the data which can be used as extra information to train
    networks. With carefully curated queries, the web data retrieved by reliable search
    engines can be relatively clean. With large-scale web data and their associated
    metadata, the performance of self-supervised methods may be boosted up. One open
    problem about learning from web data is how to handle the noise in web data and
    their associated metadata.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning Spatiotemporal Features from Videos: Self-supervised image feature
    learning has been well studied and the margin of the performance between supervised
    models and self-supervised models are very small on some downstream tasks such
    as semantic segmentation and object detection. However, self-supervised video
    spatiotemporal feature learning with 3DConvNet is not well addressed yet. More
    effective pretext tasks that specifically designed to learn spatiotemporal features
    from videos are needed.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning with Data from Different Sensors: Most existing self-supervised visual
    feature learning methods focused on only images or videos. However, if other types
    of data from different sensors are available, the constraint between different
    types of data can be used as additional sources to train networks to learn features
    [[155](#bib.bib155)]. The self-driving cars usually are equipped with various
    sensors including RGB cameras, gray-scale cameras, 3D laser scanners, and high-precision
    GPS measurements and IMU accelerations. Very large-scale datasets can be easily
    obtained through the driving, and the correspondence of data captured by different
    devices can be used as a supervision signal for self-supervised feature learning.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning with Multiple Pretext Tasks: Most existing self-supervised visual
    feature learning methods learn features by training ConvNet to solve one pretext
    tasks. Different pretext tasks provide different supervision signals which can
    help the network learn more representative features. Only a few work explored
    the multiple pretext tasks learning for self-supervised feature learning [[32](#bib.bib32),
    [30](#bib.bib30)]. More work can be done by studying the multiple pretext task
    self-supervised feature learning.'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: 10 Conclusion
  id: totrans-481
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Self-supervised image feature learning with deep convolution neural network
    has obtained great success and the margin between the performance of self-supervised
    methods and that of supervised methods on some downstream tasks becomes very small.
    This paper has extensively reviewed recently deep convolution neural network-based
    methods for self-supervised image and video feature learning from all perspectives
    including common network architectures, pretext tasks, algorithms, datasets, performance
    comparison, discussions, and future directions etc. The comparative summary of
    the methods, datasets, and performance in tabular forms clearly demonstrate their
    properties which will benefit researchers in the computer vision community.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-483
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
    for accurate object detection and semantic segmentation,” in CVPR, pp. 580–587,
    2014.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] R. Girshick, “Fast R-CNN,” in ICCV, 2015.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” in NIPS, pp. 91–99, 2015.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” in CVPR, pp. 3431–3440, 2015.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
    Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” TPAMI, 2018.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”
    in CVPR, pp. 2881–2890, 2017.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural
    image caption generator,” in CVPR, pp. 3156–3164, 2015.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in NIPS, pp. 1097–1105, 2012.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” ICLR, 2015.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” CVPR, 2015.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in CVPR, pp. 770–778, 2016.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten, “Densely connected
    convolutional networks,” in CVPR, vol. 1, p. 3, 2017.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in CVPR, pp. 248–255, IEEE, 2009.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset,
    S. Kamali, S. Popov, M. Malloci, and T. Duerig, “The open images dataset v4: Unified
    image classification, object detection, and visual relationship detection at scale,”
    arXiv preprint arXiv:1811.00982, 2018.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta,
    A. P. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, “Photo-realistic single
    image super-resolution using a generative adversarial network,” in CVPR.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
    spatiotemporal features with 3D convolutional networks,” in ICCV, 2015.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan,
    F. Viola, T. Green, T. Back, P. Natsev, et al., “The kinetics human action video
    dataset,” arXiv preprint arXiv:1705.06950, 2017.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] R. Zhang, P. Isola, and A. A. Efros, “Colorful image colorization,” in
    ECCV, pp. 649–666, Springer, 2016.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros, “Context
    encoders: Feature learning by inpainting,” in CVPR, pp. 2536–2544, 2016.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] M. Noroozi and P. Favaro, “Unsupervised learning of visual representions
    by solving jigsaw puzzles,” in ECCV, 2016.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] D. Mahajan, R. B. Girshick, V. Ramanathan, K. He, M. Paluri, Y. Li, A. Bharambe,
    and L. van der Maaten, “Exploring the limits of weakly supervised pretraining,”
    in ECCV, pp. 185–201, 2018.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] W. Li, L. Wang, W. Li, E. Agustsson, and L. Van Gool, “Webvision database:
    Visual learning and understanding from web data,” arXiv preprint arXiv:1708.02862,
    2017.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] A. Mahendran, J. Thewlis, and A. Vedaldi, “Cross pixel optical flow similarity
    for self-supervised learning,” arXiv preprint arXiv:1807.05636, 2018.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] N. Sayed, B. Brattoli, and B. Ommer, “Cross and learn: Cross-modal self-supervision,”
    arXiv preprint arXiv:1811.03879, 2018.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] B. Korbar, D. Tran, and L. Torresani, “Cooperative learning of audio and
    video models from self-supervised synchronization,” in NIPS, pp. 7773–7784, 2018.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A. Owens and A. A. Efros, “Audio-visual scene analysis with self-supervised
    multisensory features,” arXiv preprint arXiv:1804.03641, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] D. Kim, D. Cho, and I. S. Kweon, “Self-supervised video representation
    learning with space-time cubic puzzles,” arXiv preprint arXiv:1811.09795, 2018.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] L. Jing and Y. Tian, “Self-supervised spatiotemporal feature learning
    by video geometric transformations,” arXiv preprint arXiv:1811.11387, 2018.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] B. Fernando, H. Bilen, E. Gavves, and S. Gould, “Self-supervised video
    representation learning with odd-one-out networks,” in CVPR, 2017.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Z. Ren and Y. J. Lee, “Cross-domain self-supervised multi-task feature
    learning using synthetic imagery,” in CVPR, 2018.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] X. Wang, K. He, and A. Gupta, “Transitive invariance for self-supervised
    visual representation learning,” in ICCV, 2017.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] C. Doersch and A. Zisserman, “Multi-task self-supervised visual learning,”
    in ICCV, 2017.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] T. N. Mundhenk, D. Ho, and B. Y. Chen, “Improvements to context based
    self-supervised learning,” in CVPR, 2018.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] M. Noroozi, A. Vinjimoor, P. Favaro, and H. Pirsiavash, “Boosting self-supervised
    learning via knowledge transfer,” arXiv preprint arXiv:1805.00385, 2018.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] U. Büchler, B. Brattoli, and B. Ommer, “Improving spatiotemporal self-supervision
    by deep reinforcement learning,” in ECCV, pp. 770–786, 2018.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised representation learning
    by predicting image rotations,” in ICLR, 2018.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] N. Srivastava, E. Mansimov, and R. Salakhutdinov, “Unsupervised Learning
    of Video Representations using LSTMs,” in ICML, 2015.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] X. Wang and A. Gupta, “Unsupervised learning of visual representations
    using videos,” in ICCV, 2015.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] H.-Y. Lee, J.-B. Huang, M. Singh, and M.-H. Yang, “Unsupervised representation
    learning by sorting sequences,” in ICCV, pp. 667–676, IEEE, 2017.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] I. Misra, C. L. Zitnick, and M. Hebert, “Shuffle and learn: unsupervised
    learning using temporal order verification,” in ECCV, pp. 527–544, Springer, 2016.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] C. Doersch, A. Gupta, and A. A. Efros, “Unsupervised visual representation
    learning by context prediction,” in ICCV, pp. 1422–1430, 2015.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] R. Zhang, P. Isola, and A. A. Efros, “Split-brain autoencoders: Unsupervised
    learning by cross-channel prediction,” in CVPR, 2017.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] D. Li, W.-C. Hung, J.-B. Huang, S. Wang, N. Ahuja, and M.-H. Yang, “Unsupervised
    visual representation learning by graph-based consistent constraints,” in ECCV,
    2016.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep clustering for
    unsupervised learning of visual features,” in ECCV, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] E. Hoffer, I. Hubara, and N. Ailon, “Deep unsupervised learning through
    spatial contrasting,” arXiv preprint arXiv:1610.00243, 2016.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] P. Bojanowski and A. Joulin, “Unsupervised learning by predicting noise,”
    arXiv preprint arXiv:1704.05310, 2017.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Y. Li, M. Paluri, J. M. Rehg, and P. Dollár, “Unsupervised learning of
    edges,” CVPR, pp. 1619–1627, 2016.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] S. Purushwalkam and A. Gupta, “Pose from action: Unsupervised learning
    of pose features based on motion,” arXiv preprint arXiv:1609.05420, 2016.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] J. Sánchez, F. Perronnin, T. Mensink, and J. Verbeek, “Image classification
    with the fisher vector: Theory and practice,” IJCV, vol. 105, no. 3, pp. 222–245,
    2013.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] A. Faktor and M. Irani, “Video segmentation by non-local consensus voting.,”
    in BMVC, vol. 2, 2014.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] O. Stretcu and M. Leordeanu, “Multiple frames matching for object discovery
    in video.,” in BMVC, vol. 1, 2015.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Understanding
    deep learning requires rethinking generalization,” arXiv preprint arXiv:1611.03530,
    2016.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] K. Simonyan and A. Zisserman, “Two-Stream Convolutional Networks for Action
    Recognition in Videos,” in NIPS, 2014.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan,
    K. Saenko, and T. Darrell, “Long-term recurrent convolutional networks for visual
    recognition and description,” in CVPR, 2015.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] C. Feichtenhofer, A. Pinz, and R. Wildes, “Spatiotemporal residual networks
    for video action recognition,” in NIPS, pp. 3468–3476, 2016.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] C. Feichtenhofer, A. Pinz, and R. P. Wildes, “Spatiotemporal multiplier
    networks for video action recognition,” in CVPR.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] C. Feichtenhofer, A. Pinz, and A. Zisserman, “Convolutional two-stream
    network fusion for video action recognition,” in CVPR, pp. 1933–1941, 2016.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] L. Wang, Y. Qiao, and X. Tang, “Action recognition with trajectory-pooled
    deep-convolutional descriptors,” in CVPR, pp. 4305–4314, 2015.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] L. Wang, Y. Xiong, Z. Wang, and Y. Qiao, “Towards good practices for very
    deep two-stream convnets,” arXiv preprint arXiv:1507.02159, 2015.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] H. Bilen, B. Fernando, E. Gavves, A. Vedaldi, and S. Gould, “Dynamic image
    networks for action recognition,” in CVPR, 2016.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool,
    “Temporal segment networks: towards good practices for deep action recognition,”
    in ECCV, 2016.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] S. Ji, W. Xu, M. Yang, and K. Yu, “3D convolutional neural networks for
    human action recognition,” TPAMI, vol. 35, no. 1, pp. 221–231, 2013.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] K. Soomro, A. R. Zamir, and M. Shah, “UCF101: A dataset of 101 human actions
    classes from videos in the wild,” CRCV-TR, vol. 12-01, 2012.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] X. Peng, L. Wang, X. Wang, and Y. Qiao, “Bag of visual words and fusion
    methods for action recognition: Comprehensive study and good practice,” CVIU,
    vol. 150, pp. 109–125, 2016.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] C. Feichtenhofer, A. Pinz, and R. P. Wildes, “Bags of spacetime energies
    for dynamic scene recognition,” in CVPR, pp. 2681–2688, 2014.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] X. Ren and M. Philipose, “Egocentric recognition of handled objects: Benchmark
    and analysis,” in CVPRW, pp. 1–8, IEEE, 2009.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] G. Varol, I. Laptev, and C. Schmid, “Long-term Temporal Convolutions for
    Action Recognition,” TPAMI, 2017.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] L. Jing, X. Yang, and Y. Tian, “Video you only look once: Overall temporal
    convolutions for action recognition,” JVCIR, vol. 52, pp. 58–65, 2018.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new model
    and the kinetics dataset,” in CVPR, pp. 4724–4733, IEEE, 2017.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] K. Hara, H. Kataoka, and Y. Satoh, “Can spatiotemporal 3d cnns retrace
    the history of 2d cnns and imagenet,” in CVPR, pp. 18–22, 2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Z. Qiu, T. Yao, and T. Mei, “Learning spatio-temporal representation with
    pseudo-3d residual networks,” in ICCV, 2017.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term dependencies
    with gradient descent is difficult,” IEEE transactions on neural networks, vol. 5,
    no. 2, pp. 157–166, 1994.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] J. Y. Ng, M. J. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga,
    and G. Toderici, “Beyond Short Snippets: Deep Networks for Video Classification,”
    in CVPR, 2015.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Z. Li, K. Gavrilyuk, E. Gavves, M. Jain, and C. G. Snoek, “Videolstm convolves,
    attends and flows for action recognition,” CVIU, vol. 166, pp. 41–50, 2018.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, and K. Saenko,
    “Sequence to sequence – video to text,” in ICCV, 2015.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] P. Molchanov, X. Yang, S. Gupta, K. Kim, S. Tyree, and J. Kautz, “Online
    detection and classification of dynamic hand gestures with recurrent 3d convolutional
    neural network,” in CVPR, pp. 4207–4215, 2016.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba, “Network dissection:
    Quantifying interpretability of deep visual representations,” in CVPR, 2017.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] D. Bau, J.-Y. Zhu, H. Strobelt, B. Zhou, J. B. Tenenbaum, W. T. Freeman,
    and A. Torralba, “Gan dissection: Visualizing and understanding generative adversarial
    networks,” arXiv preprint arXiv:1811.10597, 2018.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional
    networks,” in ECCV, pp. 818–833, Springer, 2014.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] D. Pathak, R. Girshick, P. Dollár, T. Darrell, and B. Hariharan, “Learning
    features by watching objects move,” in CVPR, vol. 2, 2017.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] G. Larsson, M. Maire, and G. Shakhnarovich, “Colorization as a proxy task
    for visual understanding,” in CVPR, 2017.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in NIPS, pp. 2672–2680,
    2014.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” in ICCV, 2017.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] C. Vondrick, H. Pirsiavash, and A. Torralba, “Generating videos with scene
    dynamics,” in NIPS, pp. 613–621, 2016.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] S. Tulyakov, M.-Y. Liu, X. Yang, and J. Kautz, “Mocogan: Decomposing motion
    and content for video generation,” CVPR, 2018.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] U. Ahsan, R. Madhok, and I. Essa, “Video jigsaw: Unsupervised learning
    of spatiotemporal context for video action recognition,” arXiv preprint arXiv:1808.07507,
    2018.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] C. Wei, L. Xie, X. Ren, Y. Xia, C. Su, J. Liu, Q. Tian, and A. L. Yuille,
    “Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw
    puzzles for unsupervised representation learning,” arXiv preprint arXiv:1812.00329,
    2018.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] D. Kim, D. Cho, D. Yoo, and I. S. Kweon, “Learning image representations
    by completing damaged jigsaw puzzles,” arXiv preprint arXiv:1802.01880, 2018.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] D. Wei, J. Lim, A. Zisserman, and W. T. Freeman, “Learning and using the
    arrow of time,” in CVPR, pp. 8052–8060, 2018.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] I. Croitoru, S.-V. Bogolin, and M. Leordeanu, “Unsupervised learning from
    video to detect foreground objects in single images,” arXiv preprint arXiv:1703.10901,
    2017.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] H. Jiang, G. Larsson, M. Maire Greg Shakhnarovich, and E. Learned-Miller,
    “Self-supervised relative depth learning for urban scene understanding,” in ECCV,
    pp. 19–35, 2018.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] R. Arandjelovic and A. Zisserman, “Look, listen and learn,” in ICCV, pp. 609–617,
    IEEE, 2017.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] P. Agrawal, J. Carreira, and J. Malik, “Learning to see by moving,” in
    ICCV, pp. 37–45, 2015.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] D. Jayaraman and K. Grauman, “Learning image representations tied to ego-motion,”
    in ICCV, pp. 1413–1421, 2015.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
    “The pascal visual object classes (voc) challenge,” IJCV, vol. 88, no. 2, pp. 303–338,
    2010.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban
    scene understanding,” in CVPR, pp. 3213–3223, 2016.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba, “Semantic
    understanding of scenes through the ade20k dataset,” arXiv preprint arXiv:1608.05442,
    2016.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in ECCV, pp. 740–755,
    Springer, 2014.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
    Unified, real-time object detection,” in CVPR, pp. 779–788, 2016.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] J. Redmon and A. Farhadi, “Yolo9000: Better, faster, stronger,” in CVPR,
    2017.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “Ssd: Single shot multibox detector,” in ECCV, pp. 21–37, Springer, 2016.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] T.-Y. Lin, P. Dollár, R. B. Girshick, K. He, B. Hariharan, and S. J.
    Belongie, “Feature pyramid networks for object detection.,” in CVPR, vol. 1, p. 4,
    2017.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
    dense object detection,” TPAMI, 2018.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] J. A. Suykens and J. Vandewalle, “Least squares support vector machine
    classifiers,” Neural processing letters, vol. 9, no. 3, pp. 293–300, 1999.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] H. Kuehne, H. Jhuang, R. Stiefelhagen, and T. Serre, “Hmdb51: A large
    video database for human motion recognition,” in HPCSE, pp. 571–582, Springer,
    2013.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva, “Learning
    deep features for scene recognition using places database,” in NIPS, pp. 487–495,
    2014.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] B. Zhou, A. Khosla, A. Lapedriza, A. Torralba, and A. Oliva, “Places:
    An image database for deep scene understanding,” arXiv preprint arXiv:1610.02055,
    2016.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser, “Semantic
    scene completion from a single depth image,” in CVPR, pp. 190–198, IEEE, 2017.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
    applied to document recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324,
    1998.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, “Reading
    digits in natural images with unsupervised feature learning,” in NIPSW, vol. 2011,
    p. 5, 2011.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from
    tiny images,” tech. rep., Citeseer, 2009.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] A. Coates, A. Ng, and H. Lee, “An analysis of single-layer networks in
    unsupervised feature learning,” in Proceedings of the fourteenth international
    conference on artificial intelligence and statistics, pp. 215–223, 2011.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland,
    D. Borth, and L.-J. Li, “The new data and new challenges in multimedia research,”
    arXiv preprint arXiv:1503.01817, vol. 1, no. 8, 2015.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] J. McCormac, A. Handa, S. Leutenegger, and A. J. Davison, “Scenenet rgb-d:
    Can 5m synthetic images beat generic imagenet pre-training on indoor segmentation,”
    in ICCV, vol. 4, 2017.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] M. Monfort, B. Zhou, S. A. Bargal, T. Yan, A. Andonian, K. Ramakrishnan,
    L. Brown, Q. Fan, D. Gutfruend, C. Vondrick, et al., “Moments in time dataset:
    one million videos for event understanding,”'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C.
    Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset
    for audio events,” in ICASSP, pp. 776–780, IEEE, 2017.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in CVPR, pp. 3354–3361, IEEE, 2012.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] A. Joulin, L. van der Maaten, A. Jabri, and N. Vasilache, “Learning visual
    features from large weakly supervised data,” in ECCV, pp. 67–84, 2016.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning
    with deep convolutional generative adversarial networks,” arXiv preprint arXiv:1511.06434,
    2015.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv preprint
    arXiv:1701.07875, 2017.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] J. Donahue, P. Krähenbühl, and T. Darrell, “Adversarial feature learning,”
    arXiv preprint arXiv:1605.09782, 2016.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] T. Chen, X. Zhai, and N. Houlsby, “Self-supervised gan to counter forgetting,”
    arXiv preprint arXiv:1810.11598, 2018.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] G. Larsson, M. Maire, and G. Shakhnarovich, “Learning representations
    for automatic colorization,” in ECCV, pp. 577–593, Springer, 2016.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] S. Iizuka, E. Simo-Serra, and H. Ishikawa, “Globally and Locally Consistent
    Image Completion,” SIGGRAPH, 2017.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] S. Jenni and P. Favaro, “Self-supervised feature learning by learning
    to spot artifacts,” arXiv preprint arXiv:1806.05024, 2018.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] R. Santa Cruz, B. Fernando, A. Cherian, and S. Gould, “Visual permutation
    learning,” TPAMI, 2018.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] J. Yang, D. Parikh, and D. Batra, “Joint unsupervised learning of deep
    representations and image clusters,” in CVPR, pp. 5147–5156, 2016.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] J. Xie, R. Girshick, and A. Farhadi, “Unsupervised deep embedding for
    clustering analysis,” in ICML, pp. 478–487, 2016.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] M. Noroozi, H. Pirsiavash, and P. Favaro, “Representation learning by
    learning to count,” in ICCV, 2017.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of
    data with neural networks,” science, vol. 313, no. 5786, pp. 504–507, 2006.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen,
    “Improved techniques for training gans,” in NIPS, pp. 2234–2242, 2016.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
    “Gans trained by a two time-scale update rule converge to a local nash equilibrium,”
    in NIPS, pp. 6626–6637, 2017.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] A. Brock, J. Donahue, and K. Simonyan, “Large scale gan training for
    high fidelity natural image synthesis,” arXiv preprint arXiv:1809.11096, 2018.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture
    for generative adversarial networks,” arXiv preprint arXiv:1812.04948, 2018.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
    with conditional adversarial networks,” arxiv, 2016.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] R. Zhang, J.-Y. Zhu, P. Isola, X. Geng, A. S. Lin, T. Yu, and A. A. Efros,
    “Real-time user-guided image colorization with learned deep priors,” arXiv preprint
    arXiv:1705.02999, 2017.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] S. Iizuka, E. Simo-Serra, and H. Ishikawa, “Let there be color!: joint
    end-to-end learning of global and local image priors for automatic image colorization
    with simultaneous classification,” TOG, vol. 35, no. 4, p. 110, 2016.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] A. K. Jain, M. N. Murty, and P. J. Flynn, “Data clustering: a review,”
    ACM computing surveys (CSUR), vol. 31, no. 3, pp. 264–323, 1999.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,”
    in CVPR, vol. 1, pp. 886–893, IEEE, 2005.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] J. Sánchez, F. Perronnin, T. Mensink, and J. Verbeek, “Image classification
    with the fisher vector: Theory and practice,” IJCV, vol. 105, no. 3, pp. 222–245,
    2013.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity visual
    and physical simulation for autonomous vehicles,” in Field and Service Robotics,
    2017.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “CARLA:
    An open urban driving simulator,” in Proceedings of the 1st Annual Conference
    on Robot Learning, pp. 1–16, 2017.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] M. Saito, E. Matsumoto, and S. Saito, “Temporal generative adversarial
    nets with singular value clipping,” in ICCV, vol. 2, p. 5, 2017.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and K. Murphy,
    “Tracking emerges by colorizing videos,” in ECCV, 2018.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo,
    “Convolutional lstm network: A machine learning approach for precipitation nowcasting,”
    in NIPS, pp. 802–810, 2015.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] R. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee, “Decomposing motion
    and content for natural video sequence prediction,” in ICLR, 2017.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Z. Luo, B. Peng, D.-A. Huang, A. Alahi, and L. Fei-Fei, “Unsupervised
    learning of long-term motion dynamics for videos,” in CVPR, 2017.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] B. Brattoli, U. Büchler, A.-S. Wahl, M. E. Schwab, and B. Ommer, “Lstm
    self-supervision for detailed behavior analysis,” in CVPR, pp. 3747–3756, IEEE,
    2017.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] D. Jayaraman and K. Grauman, “Slow and steady feature analysis: higher
    order temporal coherence in video,” in CVPR, pp. 3852–3861, 2016.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,
    P. Van Der Smagt, D. Cremers, and T. Brox, “Flownet: Learning optical flow with
    convolutional networks,” in ICCV, pp. 2758–2766, 2015.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox,
    “Flownet 2.0: Evolution of optical flow estimation with deep networks,” in CVPR,
    pp. 1647–1655, IEEE, 2017.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] S. Meister, J. Hur, and S. Roth, “Unflow: Unsupervised learning of optical
    flow with a bidirectional census loss,” in AAAI, 2018.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] A. Owens, J. Wu, J. H. McDermott, W. T. Freeman, and A. Torralba, “Ambient
    sound provides supervision for visual learning,” in ECCV, pp. 801–816, Springer,
    2016.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised learning
    of depth and ego-motion from video,” in CVPR, vol. 2, p. 7, 2017.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Z. Yin and J. Shi, “Geonet: Unsupervised learning of dense depth, optical
    flow and camera pose,” in CVPR, vol. 2, 2018.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Y. Zou, Z. Luo, and J.-B. Huang, “Df-net: Unsupervised joint learning
    of depth and flow using cross-task consistency,” in ECCV, pp. 38–55, Springer,
    2018.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] G. Iyer, J. K. Murthy, G. Gupta, K. M. Krishna, and L. Paull, “Geometric
    consistency for self-supervised end-to-end visual odometry,” arXiv preprint arXiv:1804.03789,
    2018.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Y. Zhang, S. Khamis, C. Rhemann, J. Valentin, A. Kowdle, V. Tankovich,
    M. Schoenberg, S. Izadi, T. Funkhouser, and S. Fanello, “Activestereonet: End-to-end
    self-supervised learning for active stereo systems,” in ECCV, pp. 784–801, 2018.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Deep end2end
    voxel2voxel prediction,” in CVPRW, pp. 17–24, 2016.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] M. Mathieu, C. Couprie, and Y. LeCun, “Deep multi-scale video prediction
    beyond mean square error,” in ICLR, 2016.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] F. A. Reda, G. Liu, K. J. Shih, R. Kirby, J. Barker, D. Tarjan, A. Tao,
    and B. Catanzaro, “Sdc-net: Video prediction using spatially-displaced convolution,”
    in ECCV, pp. 718–733, 2018.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] M. Babaeizadeh, C. Finn, D. Erhan, R. H. Campbell, and S. Levine, “Stochastic
    variational video prediction,” arXiv preprint arXiv:1710.11252, 2017.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] X. Liang, L. Lee, W. Dai, and E. P. Xing, “Dual motion gan for future-flow
    embedded video prediction,” in ICCV, vol. 1, 2017.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] C. Finn, I. Goodfellow, and S. Levine, “Unsupervised learning for physical
    interaction through video prediction,” in NIPS, pp. 64–72, 2016.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] P. Krähenbühl, “Free supervision from video games,” in CVPR, June 2018.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici, B. Varadarajan,
    and S. Vijayanarasimhan, “Youtube-8m: A large-scale video classification benchmark,”
    arXiv preprint arXiv:1609.08675, 2016.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] L. Gomez, Y. Patel, M. Rusiñol, D. Karatzas, and C. Jawahar, “Self-supervised
    learning of visual features through embedding images into text topic spaces,”
    in CVPR, IEEE, 2017.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
