- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:54:03'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2106.08962] Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2106.08962](https://ar5iv.labs.arxiv.org/html/2106.08962)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster,
    and Better'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gaurav Menghani [gmenghani@google.com](mailto:gmenghani@google.com) [0000-0003-2912-2522](https://orcid.org/0000-0003-2912-2522
    "ORCID identifier") Google ResearchMountain ViewCaliforniaUSA95054
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep Learning has revolutionized the fields of computer vision, natural language
    understanding, speech recognition, information retrieval and more. However, with
    the progressive improvements in deep learning models, their number of parameters,
    latency, resources required to train, etc. have all have increased significantly.
    Consequently, it has become important to pay attention to these footprint metrics
    of a model as well, not just its quality. We present and motivate the problem
    of efficiency in deep learning, followed by a thorough survey of the five core
    areas of model efficiency (spanning modeling techniques, infrastructure, and hardware)
    and the seminal work there. We also present an experiment-based guide along with
    code, for practitioners to optimize their model training and deployment. We believe
    this is the first comprehensive survey in the efficient deep learning space that
    covers the landscape of model efficiency from modeling techniques to hardware
    support. Our hope is that this survey would provide the reader with the mental
    model and the necessary understanding of the field to apply generic efficiency
    techniques to immediately get significant improvements, and also equip them with
    ideas for further research and experimentation to achieve additional gains.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep Learning with neural networks has been the dominant methodology of training
    new machine learning models for the past decade. Its rise to prominence is often
    attributed to the ImageNet competition (Deng et al., [2009](#bib.bib46)) in 2012\.
    That year, a University of Toronto team submitted a deep convolutional network
    (AlexNet (Krizhevsky et al., [2012](#bib.bib93)), named after the lead developer
    Alex Krizhevsky), performed 41% better than the next best submission. As a result
    of this trailblazing work, there was a race to create deeper networks with an
    ever increasing number of parameters and complexity. Several model architectures
    such as VGGNet (Simonyan and Zisserman, [2014](#bib.bib142)), Inception (Szegedy
    et al., [2015](#bib.bib147)), ResNet (He et al., [2016](#bib.bib74)) etc. successively
    beat previous records at ImageNet competitions in the subsequent years, while
    also increasing in their footprint (model size, latency, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5666049517ceb7f464c21a388a64e46b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Computer Vision Models
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb1cc800839b0f6963270ad17905fed0.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Natural Language Models
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. Growth in the number of parameters in Computer Vision models over
    time. (PapersWithCode.com, [2021](#bib.bib119))
  prefs: []
  type: TYPE_NORMAL
- en: 'This effect has also been noted in natural language understanding (NLU), where
    the Transformer (Vaswani et al., [2017](#bib.bib156)) architecture based on primarily
    Attention layers, spurred the development of general purpose language encoders
    like BERT (Devlin et al., [2018](#bib.bib48)), GPT-3 (Brown et al., [2020](#bib.bib27)),
    etc. BERT specifically beat 11 NLU benchmarks when it was published. GPT-3 has
    also been used in several places in the industry via its API. The common aspect
    amongst these domains is the rapid growth in the model footprint (Refer to Figure
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Efficient Deep Learning: A Survey on
    Making Deep Learning Models Smaller, Faster, and Better")), and the cost associated
    with training and deploying them.'
  prefs: []
  type: TYPE_NORMAL
- en: Since deep learning research has been focused on improving the state of the
    art, progressive improvements on benchmarks like image classification, text classification,
    etc. have been correlated with an increase in the network complexity, number of
    parameters, the amount of training resources required to train the network, prediction
    latency, etc. For instance, GPT-3 comprises of 175 billion parameters, and costs
    millions of dollars to train just one iteration ((Brown et al., [2020](#bib.bib27))).
    This excludes the cost of experimentation / trying combinations of different hyper-parameters,
    which is also computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: While these models perform well on the tasks they are trained on, they might
    not necessarily be efficient enough for direct deployment in the real world. A
    deep learning practitioner might face the following challenges when training or
    deploying a model.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sustainable Server-Side Scaling: Training and deploying large deep learning
    models is costly. While training could be a one-time cost (or could be free if
    one is using a pre-trained model), deploying and letting inference run for over
    a long period of time could still turn out to be expensive in terms of consumption
    of server-side RAM, CPU, etc.. There is also a very real concern around the carbon
    footprint of datacenters even for organizations like Google, Facebook, Amazon,
    etc. which spend several billion dollars each per year in capital expenditure
    on their data-centers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Enabling On-Device Deployment: Certain deep learning applications need to run
    realtime on IoT and smart devices (where the model inference happens directly
    on the device), for a multitude of reasons (privacy, connectivity, responsiveness).
    Thus, it becomes imperative to optimize the models for the target devices.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Privacy & Data Sensitivity: Being able to use as little data as possible for
    training is critical when the user-data might be sensitive. Hence, efficiently
    training models with a fraction of the data means lesser data-collection required.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'New Applications: Certain new applications offer new constraints (around model
    quality or footprint) that existing off-the-shelf models might not be able to
    address.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Explosion of Models: While a singular model might work well, training and/or
    deploying multiple models on the same infrastructure (colocation) for different
    applications might end up exhausting the available resources.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1.1\. Efficient Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The common theme around the above challenges is *efficiency*. We can break
    it down further as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inference Efficiency: This primarily deals with questions that someone deploying
    a model for inference (computing the model outputs for a given input), would ask.
    Is the model small? Is it fast, etc.? More concretely, how many parameters does
    the model have, what is the disk size, RAM consumption during inference, inference
    latency, etc.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training Efficiency: This involves questions someone training a model would
    ask, such as How long does the model take to train? How many devices? Can the
    model fit in memory?, etc. It might also include questions like, how much data
    would the model need to achieve the desired performance on the given task?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If we were to be given two models, performing equally well on a given task,
    we might want to choose a model which does better in either one, or ideally both
    of the above aspects. If one were to be deploying a model on devices where inference
    is constrained (such as mobile and embedded devices), or expensive (cloud servers),
    it might be worth paying attention to inference efficiency. Similarly, if one
    is training a large model from scratch on either with limited or costly training
    resources, developing models that are designed for training efficiency would help.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c8f0b15164247b11b0dd00dbb5d1c784.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2\. Pareto Optimality: Green dots represent pareto-optimal models (together
    forming the pareto-frontier), where none of the other models (red dots) get better
    accuracy with the same inference latency, or the other way around.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of what one might be optimizing for, we want to achieve *pareto-optimality*.
    This implies that any model that we choose is the best for the tradeoffs that
    we care about. As an example in Figure [2](#S1.F2 "Figure 2 ‣ 1.1\. Efficient
    Deep Learning ‣ 1\. Introduction ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better"), the green dots represent pareto-optimal
    models, where none of the other models (red dots) get better accuracy with the
    same inference latency, or the other way around. Together, the pareto-optimal
    models (green dots) form our *pareto-frontier*. The models in the pareto-frontier
    are by definition more efficient than the other models, since they perform the
    best for their given tradeoff. Hence, when we seek efficiency, we should be thinking
    about discovering and improving on the pareto-frontier.'
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this goal, we propose turning towards a collection of algorithms,
    techniques, tools, and infrastructure that work together to allow users to train
    and deploy *pareto-optimal* models with respect to model quality and its footprint.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. A Mental Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we present the mental model to think about the collection of
    algorithms, techniques, and tools related to efficient deep learning. We propose
    to structure them in five major areas, with the first four focused on modeling,
    and the final one around infrastructure and tools.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fabdaeea4f34ffafb788f2dc3d56a587.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. A mental model for thinking about algorithms, techniques, and tools
    related to efficiency in Deep Learning.
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compression Techniques: These are general techniques and algorithms that look
    at optimizing the model’s architecture, typically by compressing its layers. A
    classical example is quantization (Jacob et al., [2018](#bib.bib83)), which tries
    to compress the weight matrices of a layer, by reducing its precision (eg., from
    32-bit floating point values to 8-bit unsigned integers), with minimal loss in
    quality.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Learning Techniques: These are algorithms which focus on training the model
    differently (to make fewer prediction errors, require less data, converge faster,
    etc.). The improved quality can then be exchanged for a smaller footprint / a
    more efficient model by trimming the number of parameters if needed. An example
    of a learning technique is distillation (Hinton et al., [2015](#bib.bib76)), which
    allows improving the accuracy of a smaller model by learning to mimic a larger
    model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Automation: These are tools for improving the core metrics of the given model
    using automation. An example is hyper-parameter optimization (HPO) (Golovin et al.,
    [2017](#bib.bib62)) where optimizing the hyper-parameters helps increase the accuracy,
    which could then be then exchanged for a model with lesser parameters. Similarly,
    architecture search (Zoph and Le, [2016](#bib.bib169)) falls in this category
    too, where the architecture itself is tuned and the search helps find a model
    that optimizes both the loss / accuracy, and some other metric such as model latency,
    model size, etc.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Efficient Architectures: These are fundamental blocks that were designed from
    scratch (convolutional layers, attention, etc.), that are a significant leap over
    the baseline methods used before them (fully connected layers, and RNNs respectively).
    As an example, convolutional layers introduced parameter sharing for use in image
    classification, which avoids having to learn separate weights for each input pixel,
    and also makes them robust to overfitting. Similarly, attention layers (Bahdanau
    et al., [2014](#bib.bib22)) solved the problem of Information Bottleneck in Seq2Seq
    models. These architectures can be used directly for efficiency gains.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (5)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Infrastructure: Finally, we also need a foundation of infrastructure and tools
    that help us build and leverage efficient models. This includes the model training
    framework, such as Tensorflow (Abadi et al., [2016](#bib.bib2)), PyTorch (Paszke
    et al., [2019](#bib.bib120)), etc. (along with the tools required specifically
    for deploying efficient models such as Tensorflow Lite (TFLite), PyTorch Mobile,
    etc.). We depend on the infrastructure and tooling to leverage gains from efficient
    models. For example, to get both size and latency improvements with quantized
    models, we need the inference platform to support common neural network layers
    in quantized mode.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will survey each of these areas in depth in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Landscape of Efficient Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1\. Compression Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compression techniques as mentioned earlier, are usually generic techniques
    for achieving a more efficient representation of one or more layers in a neural
    network, with a possible quality trade off. The efficiency goal could be to optimize
    the model for one or more of the footprint metrics, such as model size, inference
    latency, training time required for convergence, etc. in exchange for as little
    quality loss as possible. In some cases if the model is over-parameterized, these
    techniques can improve model generalization.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1\. Pruning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given a neural network $f(X,W)$, where $X$ is the input and $W$ is the set of
    parameters (or weights), pruning is a technique for coming up with a minimal subset
    $W^{\prime}$ such that the rest of the parameters of $W$ are pruned (or set to
    0), while ensuring that the quality of the model remains above the desired threshold.
    After pruning, we can say the network has been made *sparse*, where the sparsity
    can be quantified as the ratio of the number of parameters that were pruned to
    the number of parameters in the original network ($s=(1-\frac{|W^{\prime}|}{|W|})$).
    The higher the sparsity, the lesser the number of non-zero parameters in the pruned
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a9065a48b4901e83bf765de85dcb91e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. A simplified illustration of pruning weights (connections) and neurons
    (nodes) in a neural network comprising of fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the classical works in this area are Optimal Brain Damage (OBD) by
    LeCun et al. (LeCun et al., [1990](#bib.bib99)), and Optimal Brain Surgeon paper
    (OBD) by Hassibi et al. (Hassibi et al., [1993](#bib.bib73)). These methods usually
    take a network that has been pre-trained to a reasonable quality and then iteratively
    prune the parameters which have the lowest ‘saliency’ score, such that the impact
    on the validation loss is minimized. Once pruning concludes, the network is fine-tuned
    with the remaining parameters. This process is repeated a number of times until
    the desired number of original parameters are pruned (Algorithm [1](#algorithm1
    "In 3.1.1\. Pruning ‣ 3.1\. Compression Techniques ‣ 3\. Landscape of Efficient
    Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data: Pre-trained dense network with weights $W$, inputs $X$, number of pruning
    rounds $N$, fraction of parameters to prune per round $p$.Result: Pruned network
    with weights $W^{\prime}$.1  $W^{\prime}\leftarrow W$;2  for *$i\leftarrow 1$
    to $N$* do3        $S\leftarrow\texttt{compute\_saliency\_scores}(W^{\prime})$;4      5      $W^{\prime}\leftarrow
    W^{\prime}-\texttt{select\_min\_k}\large(S,\frac{|W^{\prime}|}{p}\large)$;6        $W^{\prime}\leftarrow$fine_tune($X$,
    $W^{\prime}$)7 end forreturn $W^{\prime}$'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Standard Network Pruning with Fine-Tuning
  prefs: []
  type: TYPE_NORMAL
- en: OBD approximates the saliency score by using a second-derivative of the parameters
    ($\large\frac{\partial^{2}L}{\partial w_{i}^{2}}$), where $L$ is the loss function,
    and $w_{i}$ is the candidate parameter for removal. The intuition is that the
    higher this value for a given parameter, the larger the change in the loss function’s
    gradient if it were to be pruned.
  prefs: []
  type: TYPE_NORMAL
- en: For the purpose of speeding up the computation of the second-derivatives, OBD
    ignores cross-interaction between the weights ($\large\frac{\partial^{2}L}{\partial
    w_{i}\partial w_{j}}$), and hence computes only the diagonal elements of the Hessian
    matrix. Otherwise, computing the full Hessian matrix is unwieldy for even a reasonable
    number of weights (with $n=10^{4}$, the size of the matrix is $10^{4}\times 10^{4}=10^{8}$).
    In terms of results, LeCun et al. demonstrate that pruning reduced the parameters
    in a well-trained neural net by   8x (combination of both automatic and manual
    removal) without a drop in classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Across different pruning strategies, the core algorithm could remain similar,
    with changes in the following aspects.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saliency: While (LeCun et al., [1990](#bib.bib99); Hassibi et al., [1993](#bib.bib73))
    use second-order derivatives, other methods rely on simpler magnitude based pruning
    (Han et al., [2015b](#bib.bib70), [a](#bib.bib69)), or momentum based pruning
    (Dettmers and Zettlemoyer, [2019](#bib.bib47)) etc. to determine the saliency
    score.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Structured v/s Unstructured: The most flexible way of pruning is unstructured
    (or random) pruning, where all given parameters are treated equally. In structured
    pruning, parameters are pruned in blocks (such as pruning row-wise in a weight
    matrix, or pruning channel-wise in a convolutional filter (Li et al., [2016](#bib.bib101);
    Anwar et al., [2017](#bib.bib6); Molchanov et al., [2016](#bib.bib113); Liu et al.,
    [2019](#bib.bib107)), etc.). The latter allows easier leveraging of inference-time
    gains in size and latency, since these blocks of pruned parameters can be intelligently
    skipped for storage and inference. Note that unstructured pruning can also be
    viewed as structured pruning with block size = 1.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distribution: The decision about how to distribute the sparsity budget (number
    of parameters to be pruned), could be made either by pooling in all the parameters
    from the network and then deciding which parameters to prune, or by smartly selecting
    how much to prune in each layer individually (Dong et al., [2017](#bib.bib51);
    He et al., [2018](#bib.bib75)). (Elsen et al., [2020](#bib.bib53); google research,
    [2021](#bib.bib67)) have found that some architectures like MobileNetV2, EfficientNet
    (Tan et al., [2019](#bib.bib148)) have thin first layers that do not contribute
    significantly to the number of parameters and pruning them leads to an accuracy
    drop without much gain. Hence, intuitively it would be helpful to allocate sparsity
    on a per-layer basis.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scheduling: Another question is how much to prune, and when? Should we prune
    an equal number of parameters every round (LeCun et al., [1990](#bib.bib99); Hassibi
    et al., [1993](#bib.bib73); Han et al., [2015b](#bib.bib70)), or should we prune
    at a higher pace in the beginning and gradually decrease (Zhu and Gupta, [2018](#bib.bib168);
    Dettmers and Zettlemoyer, [2019](#bib.bib47)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regrowth: Some methods allow regrowing pruned connections (Evci et al., [2020](#bib.bib55);
    Dettmers and Zettlemoyer, [2019](#bib.bib47)) to keep the same level of sparsity
    through constant cycles of prune-redistribute-regrow. Dettmers et al. (Dettmers
    and Zettlemoyer, [2019](#bib.bib47)) estimate training time speedups between 2.7x
    - 5.6x by starting and operating with a sparse model throughout. However there
    is a gap in terms of implementation of sparse operations on CPU, GPU, and other
    hardware.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| Model Architecture | Sparsity Type | Sparsity % | FLOPs | Top-1 Accuracy
    % | Source |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MobileNet v2 - 1.0 | Dense (Baseline) | 0% | 1x | 72.0% | Sandler et al.
    (Sandler et al., [2018](#bib.bib134)) |'
  prefs: []
  type: TYPE_TB
- en: '| Unstructured | 75% | 0.27x | 67.7% | Zhu et al. (Zhu and Gupta, [2018](#bib.bib168))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Unstructured | 75% | 0.52x | 71.9% | Evci et al. (Evci et al., [2020](#bib.bib55))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Structured (block-wise) | 85% | 0.11x | 69.7% | Elsen et al. |'
  prefs: []
  type: TYPE_TB
- en: '| Unstructured | 90% | 0.12x | 61.8% | Zhu et al. (Zhu and Gupta, [2018](#bib.bib168))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Unstructured | 90% | 0.12x | 69.7% | Evci et al. (Evci et al., [2020](#bib.bib55))
    |'
  prefs: []
  type: TYPE_TB
- en: Table 1\. A sample of various sparsity results on the MobileNet v2 architecture
    with depth multiplier = 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond Model Optimization: Frankle et al.’s (Frankle and Carbin, [2018](#bib.bib58))
    work on the Lottery Ticket Hypothesis took a different look at pruning, and postulated
    that within every large network lies a smaller network, which can be extracted
    with the original initialization of its parameters, and retrained on its own to
    match or exceed the performance of the larger network. The authors demonstrated
    these results on multiple datasets, but others such as (Gale et al., [2019](#bib.bib59);
    Liu et al., [2018b](#bib.bib108)) were not able to replicate this on larger datasets
    such as ImageNet (Deng et al., [2009](#bib.bib46)). Rather Liu et al. (Liu et al.,
    [2018b](#bib.bib108)) demonstrate that the pruned architecture with random initialization
    does no worse than the pruned architecture with the trained weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussion: There is a significant body of work that demonstrates impressive
    theoretical reduction in the model size (via number of parameters), or estimates
    the savings in FLOPs (Table [1](#S3.T1 "Table 1 ‣ 3.1.1\. Pruning ‣ 3.1\. Compression
    Techniques ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning:
    A Survey on Making Deep Learning Models Smaller, Faster, and Better")). However,
    a large fraction of the results are on *unstructured* pruning, where it is not
    currently clear how these improvements can lead to reduction in footprint metrics
    (apart from using standard file compression tools like GZip).'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, structured pruning with a meaningful block size is conducive
    to latency improvements. Elsen et al. (Elsen et al., [2020](#bib.bib53); google
    research, [2021](#bib.bib67)) construct sparse convolutional networks that outperform
    their dense counterparts by $1.3$ - $2.4\times$ with $\approx$ 66% of the parameters,
    while retaining the same Top-1 accuracy. They do this via their library to convert
    from the NHWC (channels-last) standard dense representation to a special NCHW
    (channels-first) ‘Block Compressed Sparse Row’ (BCSR) representation which is
    suitable for fast inference using their fast kernels on ARM devices, WebAssembly
    etc. (Authors, [2021k](#bib.bib19)). Although they also introduce some constraints
    on the kinds of sparse networks that can be accelerated (Authors, [2021l](#bib.bib20)).
    Overall, this is a promising step towards practical improvements in footprint
    metrics with pruned networks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2\. Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Almost all the weights and activations of a typical network are in 32-bit floating-point
    values. One of the ideas of reducing model footprint is to reduce the precision
    for the weights and activations by *quantizing* to a lower-precision datatype
    (often 8-bit fixed-point integers). There are two kinds of gains that we can get
    from quantization: (a) lower model size, and (b) lower inference latency. Often,
    only the model size is a constraint, and in this case we can employ a technique
    called weight quantization and get model size improvements (Authors, [2021f](#bib.bib14)),
    where only the model weights are in reduced precision. In order to get latency
    improvements, the activations need to be in fixed-point as well (Activation Quantization
    (Vanhoucke et al., [2011](#bib.bib154); Jacob et al., [2018](#bib.bib83)), such
    that all the operations in the quantized graph are happening in fixed-point math
    as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Weight Quantization: A simple *scheme* for quantizing weights to get model
    size improvements (similar to (Krishnamoorthi, [2018](#bib.bib91))) is as follows.
    Given a 32-bit floating-point weight matrix in a model, we can map the minimum
    weight value ($x_{min}$) in that matrix to $0$, and the maximum value ($x_{max}$)
    to $2^{b}-1$ (where $b$ is the number of bits of precision, and $b<32$). Then
    we can linearly extrapolate all values between them to an integer value in [$0,2^{b}-1$]
    (Figure  [5](#S3.F5 "Figure 5 ‣ 3.1.2\. Quantization ‣ 3.1\. Compression Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better")). Thus, we are able
    to map each floating point value to a fixed-point value where the latter requires
    a lesser number of bits than the floating-point representation. This process can
    also be done for signed $b$-bit fixed-point integers, where the output values
    will be in the range [-$2^{\frac{b}{2}}-1$, $2^{\frac{b}{2}}-1$]. One of the reasonable
    values of $b$ is $8$, since this would lead to a $32/8=4\times$ reduction in space,
    and also because of the near-universal support for uint8_t and int8_t datatypes.'
  prefs: []
  type: TYPE_NORMAL
- en: During inference, we go in the reverse direction where we recover a lossy estimate
    of the original floating point value (*dequantization*) using just the $x_{min}$
    and $x_{max}$. This estimate is lossy since we lost $32-b$ bits of information
    when did the rounding (another way to look at it is that a range of floating point
    values map to the same quantized value).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a238d6417900d87f3700f271423324e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Quantizing floating-point continuous values to discrete fixed-point
    values. The continuous values are clamped to the range $x_{min}$ to $x_{max}$,
    and are mapped to discrete values in [$0$, $2^{b}-1$] (in the above figure, $b=3$,
    hence the quantized values are in the range [$0,7$].
  prefs: []
  type: TYPE_NORMAL
- en: '(Jacob et al., [2018](#bib.bib83); Krishnamoorthi, [2018](#bib.bib91)) formalize
    the quantization scheme with the following two constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The quantization scheme should be linear (affine transformation), so that the
    precision bits are linearly distributed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $0.0$ should map exactly to a fixed-point value $x_{q_{0}}$, such that dequantizing
    $x_{q_{0}}$ gives us $0.0$. This is an implementation constraint, since $0$ is
    also used for padding to signify missing elements in tensors, and if dequantizing
    $x_{q_{0}}$ leads to a non-zero value, then it might be interpreted incorrectly
    as a valid element at that index.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The second constraint described above requires that $0$ be a part of the quantization
    range, which in turn requires updating $x_{min}$ and $x_{max}$, followed by clamping
    $x$ to lie in $[x_{min},x_{max}]$. Following this, we can quantize $x$ by constructing
    a piece-wise linear transformation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\small\textrm{quantize}(x)=x_{q}=\textrm{round}\bigg{(}\frac{x}{s}\bigg{)}+z$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '$s$ is the floating-point *scale* value (can be thought of as the inverse of
    the slope, which can be computed using $x_{min}$, $x_{max}$ and the range of the
    fixed-point values). $z$ is an integer *zero-point* value which is the quantized
    value that is assigned to $x=0.0$. This is the terminology followed in literature
    (Jacob et al., [2018](#bib.bib83); Krishnamoorthi, [2018](#bib.bib91)) (Algorithm
    [2](#algorithm2 "In 3.1.2\. Quantization ‣ 3.1\. Compression Techniques ‣ 3\.
    Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dequantization step constructs $\hat{x}$, which is a lossy estimate of
    $x$, since we lose precision when quantizing to a lower number of bits. We can
    compute it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\small\textrm{dequantize}(x_{q})=\hat{x}=s(x_{q}-z)$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Since $s$ is in floating-point, $\hat{x}$ is also a floating-point value (Algorithm
    [3](#algorithm3 "In 3.1.2\. Quantization ‣ 3.1\. Compression Techniques ‣ 3\.
    Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better")). Note that the quantization
    and dequantization steps can be performed for signed integers too by appropriately
    changing the value $x_{q_{min}}$ (which is the lowest fixed-point value in $b$-bits)
    in Algorithm [2](#algorithm2 "In 3.1.2\. Quantization ‣ 3.1\. Compression Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data: Floating-point tensor to compress $\mathbf{X}$, number of precision bits
    $b$ for the fixed-point representation.Result: Quantized tensor $\mathbf{X_{q}}$.1  $\textbf{X}_{min},\textbf{X}_{max}\leftarrow\textrm{min}(\mathbf{X},0),\textrm{max}(\mathbf{X},0)$;2  $\mathbf{X}\leftarrow\textrm{clamp}(\mathbf{X},\textbf{X}_{min},\textbf{X}_{max})$;3  $s\leftarrow\frac{\displaystyle
    x_{max}-x_{min}}{\displaystyle 2^{b}-1}$;4  $z\leftarrow\textrm{round}\bigg{(}x_{q_{min}}-\frac{\displaystyle
    x_{min}}{\displaystyle s}\bigg{)}$;56$\mathbf{X_{q}}\leftarrow\textrm{round}\bigg{(}\frac{\displaystyle\mathbf{X}}{\displaystyle
    s}\bigg{)}+z$;7 return $\mathbf{X_{q}}$;'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Quantizing a given weight matrix $\mathbf{X}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Data: Fixed-point matrix to dequantize $\mathbf{X_{q}}$, along with the scale
    $s$, and zero-point $z$ values which were calculated during quantization.Result:
    Dequantized floating-point weight matrix $\widehat{\mathbf{X}}$.1  $\widehat{\mathbf{X}}\leftarrow
    s(\mathbf{X_{q}}-z)$;2 return $\widehat{\mathbf{X}}$;'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 Dequantizing a given fixed-point weight matrix $\mathbf{X_{q}}$
  prefs: []
  type: TYPE_NORMAL
- en: We can utilize the above two algorithms for quantizing and dequantizing the
    model’s weight matrices. Quantizing a pre-trained model’s weights for reducing
    the size is termed as *post-training quantization* in literature (Authors, [2021f](#bib.bib14)).
    This might be sufficient for the purpose of reducing the model size when there
    is sufficient representational capacity in the model.
  prefs: []
  type: TYPE_NORMAL
- en: There are other works in literature (Rastegari et al., [2016](#bib.bib128);
    Hubara et al., [2016](#bib.bib81); Li et al., [2016](#bib.bib100)) that demonstrate
    slightly different variants of quantization. XNOR-Net (Rastegari et al., [2016](#bib.bib128)),
    Binarized Neural Networks (Hubara et al., [2016](#bib.bib81)) and others use $b=1$,
    and thus have weight matrices which just have two possible values $0$ or $1$,
    and the quantization function there is simply the $\textrm{sign}(x)$ function
    (assuming the weights are symmetrically distributed around $0$).
  prefs: []
  type: TYPE_NORMAL
- en: The promise with such extreme quantization approaches is the theoretical $32/1=32\times$
    reduction in model size without much quality loss. Some of the works claim improvements
    on larger networks like AlexNet (Krizhevsky et al., [2012](#bib.bib93)), VGG (Simonyan
    and Zisserman, [2014](#bib.bib142)), Inception (Szegedy et al., [2015](#bib.bib147))
    etc., which might already be more amenable to compression. A more informative
    task would be to demonstrate extreme quantization on smaller networks like the
    MobileNet family (Sandler et al., [2018](#bib.bib134); Howard et al., [2019](#bib.bib78)).
    Additionally binary quantization (and other quantization schemes like ternary
    (Li et al., [2016](#bib.bib100)), bit-shift based networks (Rastegari et al.,
    [2016](#bib.bib128)), etc.) promise latency-efficient implementations of standard
    operations where multiplications and divisions are replaced by cheaper operations
    like addition, subtraction, etc. These claims need to be verified because even
    if these lead to theoretical reduction in FLOPs, the implementations still need
    support from the underlying hardware. A fair comparison would be using standard
    quantization with $b=8$, where the multiplications and divisions also become cheaper,
    and are supported by the hardware efficiently via SIMD instructions which allow
    for low-level data parallelism (for example, on x86 via the SSE instruction set,
    on ARM via the Neon (Ltd., [2021](#bib.bib109)) intrinsics, and even on specialized
    DSPs like the Qualcomm Hexagon (Authors, [2021l](#bib.bib20))).
  prefs: []
  type: TYPE_NORMAL
- en: 'Activation Quantization: To be able to get *latency improvements* with quantized
    networks, the math operations have to be done in fixed-point representations too.
    This means all intermediate layer inputs and outputs are also in fixed-point,
    and there is no need to dequantize the weight-matrices since they can be used
    directly along with the inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: Vanhoucke et al. (Vanhoucke et al., [2011](#bib.bib154)) demonstrated a $3\times$
    inference speedup using a fully fixed-point model on an x86 CPU, when compared
    to a floating-point model on the same CPU, without sacrificing accuracy. The weights
    are still quantized similar to post-training quantization, however all layer inputs
    (except the first layer) and the activations are fixed-point. In terms of performance,
    the primary driver for this improvement was the availability of fixed-point SIMD
    instructions in Intel’s SSE4 instruction set (Contributors to Wikimedia projects,
    [2021e](#bib.bib42)), where commonly used building-block operations like the Multiply-Accumulate
    (MAC) (Contributors to Wikimedia projects, [2021d](#bib.bib41)) can be parallelized.
    Since the paper was published, Intel has released two more iterations of these
    instruction sets (Contributors to Wikimedia projects, [2021a](#bib.bib38)) which
    might further improve the speedups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantization-Aware Training (QAT): The network that Vanhoucke et al. mention
    was a 5 layer feed-forward network that was post-training quantized. However post-training
    quantization can lead to quality loss during inference as highlighted in (Krishnamoorthi,
    [2018](#bib.bib91); Jacob et al., [2018](#bib.bib83); Wang et al., [2020](#bib.bib157))
    as the networks become more complex. These could be because of: (a) outlier weights
    that skew the computation of the quantized values for the entire input range towards
    the outliers, leading to less number of bits being allocated to the bulk of the
    range, or (b) Different distribution of weights within the weight matrix, for
    eg. within a convolutional layer the distribution of weights between each filter
    might be different, but they are quantized the same way. These effects might be
    more pronounced at low-bit widths due to an even worse loss of precision. Wang
    et al. (Wang et al., [2020](#bib.bib157)) try to retain the post-training quantization
    but with new heuristics to allocate the precision bits in a learned fashion. Tools
    like the TFLite Converter (TensorFlow, [2019](#bib.bib150)) augment post-training
    quantization with a representative dataset provided by the user, to actively correct
    for errors at different points in the model by comparing the error between the
    activations of the quantized and unquantized graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: Jacob et al. (Jacob et al., [2018](#bib.bib83)) propose (and further detailed
    by Krishnamoorthi et al. (Krishnamoorthi, [2018](#bib.bib91))) a training regime
    which is *quantization-aware*. In this setting, the training happens in floating-point
    but the forward-pass simulates the quantization behavior during inference. Both
    weights and activations are passed through a function that simulates this quantization
    behavior (*fake-quantized* is the term used by many works (Jacob et al., [2018](#bib.bib83);
    Krishnamoorthi, [2018](#bib.bib91))).
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming $\mathbf{X}$ is the tensor to be fake-quantized, Jacob et al. (Jacob
    et al., [2018](#bib.bib83)) propose adding special quantization nodes in the training
    graph that collect the statistics (moving averages of $x_{min}$ and $x_{max}$)
    related to the weights and activations to be quantized (see Figure [6b](#S3.F6.sf2
    "In Figure 6 ‣ 3.1.2\. Quantization ‣ 3.1\. Compression Techniques ‣ 3\. Landscape
    of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep
    Learning Models Smaller, Faster, and Better")(a) for an illustration). Once we
    have these values for each $\mathbf{X}$, we can derive the respective $\widehat{\mathbf{X}}$
    using equations ([1](#S3.E1 "In 3.1.2\. Quantization ‣ 3.1\. Compression Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") and [2](#S3.E2 "In
    3.1.2\. Quantization ‣ 3.1\. Compression Techniques ‣ 3\. Landscape of Efficient
    Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better")) as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | <math id="S3.E3.m1.42" class="ltx_Math" alttext="\small\begin{split}\widehat{\mathbf{X}}{}&amp;=\textrm{FakeQuant}(\mathbf{X})\\
    &amp;=\textrm{Dequantize}(\textrm{Quantize}(\mathbf{X}))\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;=s((\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}+z)-z)\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;=s\bigg{(}\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}\bigg{)}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}" display="block"><semantics id="S3.E3.m1.42a"><mtable columnspacing="0pt"
    displaystyle="true" rowspacing="0pt" id="S3.E3.m1.42.42.6" xref="S3.E3.m1.39.39.3.cmml"><mtr
    id="S3.E3.m1.42.42.6a" xref="S3.E3.m1.39.39.3.cmml"><mtd class="ltx_align_right"
    columnalign="right" id="S3.E3.m1.42.42.6b" xref="S3.E3.m1.39.39.3.cmml"><mover
    accent="true" id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mi mathsize="90%"
    id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml">𝐗</mi><mo mathsize="90%"
    id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">^</mo></mover></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S3.E3.m1.42.42.6c" xref="S3.E3.m1.39.39.3.cmml"><mrow
    id="S3.E3.m1.6.6.6.6.5" xref="S3.E3.m1.39.39.3.cmml"><mo mathsize="90%" id="S3.E3.m1.2.2.2.2.1.1"
    xref="S3.E3.m1.2.2.2.2.1.1.cmml">=</mo><mrow id="S3.E3.m1.6.6.6.6.5.7" xref="S3.E3.m1.39.39.3.cmml"><mtext
    mathsize="90%" id="S3.E3.m1.3.3.3.3.2.2" xref="S3.E3.m1.3.3.3.3.2.2a.cmml">FakeQuant</mtext><mo
    lspace="0em" rspace="0em" id="S3.E3.m1.6.6.6.6.5.7.1" xref="S3.E3.m1.39.39.3a.cmml">​</mo><mrow
    id="S3.E3.m1.6.6.6.6.5.7.2" xref="S3.E3.m1.39.39.3.cmml"><mo maxsize="90%" minsize="90%"
    id="S3.E3.m1.4.4.4.4.3.3" xref="S3.E3.m1.39.39.3a.cmml">(</mo><mi mathsize="90%"
    id="S3.E3.m1.5.5.5.5.4.4" xref="S3.E3.m1.5.5.5.5.4.4.cmml">𝐗</mi><mo maxsize="90%"
    minsize="90%" id="S3.E3.m1.6.6.6.6.5.5" xref="S3.E3.m1.39.39.3a.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    id="S3.E3.m1.42.42.6d" xref="S3.E3.m1.39.39.3.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E3.m1.42.42.6f" xref="S3.E3.m1.39.39.3.cmml"><mrow id="S3.E3.m1.40.40.4.37.9.9"
    xref="S3.E3.m1.39.39.3.cmml"><mo mathsize="90%" id="S3.E3.m1.7.7.7.1.1.1" xref="S3.E3.m1.7.7.7.1.1.1.cmml">=</mo><mrow
    id="S3.E3.m1.40.40.4.37.9.9.9" xref="S3.E3.m1.39.39.3.cmml"><mtext mathsize="90%"
    id="S3.E3.m1.8.8.8.2.2.2" xref="S3.E3.m1.8.8.8.2.2.2a.cmml">Dequantize</mtext><mo
    lspace="0em" rspace="0em" id="S3.E3.m1.40.40.4.37.9.9.9.2" xref="S3.E3.m1.39.39.3a.cmml">​</mo><mrow
    id="S3.E3.m1.40.40.4.37.9.9.9.1.1" xref="S3.E3.m1.39.39.3.cmml"><mo maxsize="90%"
    minsize="90%" id="S3.E3.m1.9.9.9.3.3.3" xref="S3.E3.m1.39.39.3a.cmml">(</mo><mrow
    id="S3.E3.m1.40.40.4.37.9.9.9.1.1.1" xref="S3.E3.m1.39.39.3.cmml"><mtext mathsize="90%"
    id="S3.E3.m1.10.10.10.4.4.4" xref="S3.E3.m1.10.10.10.4.4.4a.cmml">Quantize</mtext><mo
    lspace="0em" rspace="0em" id="S3.E3.m1.40.40.4.37.9.9.9.1.1.1.1" xref="S3.E3.m1.39.39.3a.cmml">​</mo><mrow
    id="S3.E3.m1.40.40.4.37.9.9.9.1.1.1.2" xref="S3.E3.m1.39.39.3.cmml"><mo maxsize="90%"
    minsize="90%" id="S3.E3.m1.11.11.11.5.5.5" xref="S3.E3.m1.39.39.3a.cmml">(</mo><mi
    mathsize="90%" id="S3.E3.m1.12.12.12.6.6.6" xref="S3.E3.m1.12.12.12.6.6.6.cmml">𝐗</mi><mo
    maxsize="90%" minsize="90%" id="S3.E3.m1.13.13.13.7.7.7" xref="S3.E3.m1.39.39.3a.cmml">)</mo></mrow></mrow><mo
    maxsize="90%" minsize="90%" id="S3.E3.m1.14.14.14.8.8.8" xref="S3.E3.m1.39.39.3a.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    id="S3.E3.m1.42.42.6g" xref="S3.E3.m1.39.39.3.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E3.m1.42.42.6i" xref="S3.E3.m1.39.39.3.cmml"><mrow id="S3.E3.m1.41.41.5.38.15.15"
    xref="S3.E3.m1.39.39.3.cmml"><mo mathsize="90%" id="S3.E3.m1.15.15.15.1.1.1" xref="S3.E3.m1.15.15.15.1.1.1.cmml">=</mo><mrow
    id="S3.E3.m1.41.41.5.38.15.15.15" xref="S3.E3.m1.39.39.3.cmml"><mi mathsize="90%"
    id="S3.E3.m1.16.16.16.2.2.2" xref="S3.E3.m1.16.16.16.2.2.2.cmml">s</mi><mo lspace="0em"
    rspace="0em" id="S3.E3.m1.41.41.5.38.15.15.15.2" xref="S3.E3.m1.39.39.3a.cmml">​</mo><mrow
    id="S3.E3.m1.41.41.5.38.15.15.15.1.1" xref="S3.E3.m1.39.39.3.cmml"><mo maxsize="90%"
    minsize="90%" id="S3.E3.m1.17.17.17.3.3.3" xref="S3.E3.m1.39.39.3a.cmml">(</mo><mrow
    id="S3.E3.m1.41.41.5.38.15.15.15.1.1.1" xref="S3.E3.m1.39.39.3.cmml"><mrow id="S3.E3.m1.41.41.5.38.15.15.15.1.1.1.1.1"
    xref="S3.E3.m1.39.39.3.cmml"><mo maxsize="90%" minsize="90%" id="S3.E3.m1.18.18.18.4.4.4"
    xref="S3.E3.m1.39.39.3a.cmml">(</mo><mrow id="S3.E3.m1.41.41.5.38.15.15.15.1.1.1.1.1.1"
    xref="S3.E3.m1.39.39.3.cmml"><mrow id="S3.E3.m1.41.41.5.38.15.15.15.1.1.1.1.1.1.1"
    xref="S3.E3.m1.39.39.3.cmml"><mtext mathsize="90%" id="S3.E3.m1.19.19.19.5.5.5"
    xref="S3.E3.m1.19.19.19.5.5.5a.cmml">round</mtext><mo lspace="0em" rspace="0em"
    id="S3.E3.m1.41.41.5.38.15.15.15.1.1.1.1.1.1.1.1" xref="S3.E3.m1.39.39.3a.cmml">​</mo><mrow
    id="S3.E3.m1.41.41.5.38.15.15.15.1.1.1.1.1.1.1.2" xref="S3.E3.m1.39.39.3.cmml"><mo
    maxsize="210%" minsize="210%" id="S3.E3.m1.20.20.20.6.6.6" xref="S3.E3.m1.39.39.3a.cmml">(</mo><mfrac
    id="S3.E3.m1.21.21.21.7.7.7" xref="S3.E3.m1.21.21.21.7.7.7.cmml"><mrow id="S3.E3.m1.21.21.21.7.7.7.3"
    xref="S3.E3.m1.21.21.21.7.7.7.3.cmml"><mtext mathsize="90%" id="S3.E3.m1.21.21.21.7.7.7.3.5"
    xref="S3.E3.m1.21.21.21.7.7.7.3.5a.cmml">clamp</mtext><mo lspace="0em" rspace="0em"
    id="S3.E3.m1.21.21.21.7.7.7.3.4" xref="S3.E3.m1.21.21.21.7.7.7.3.4.cmml">​</mo><mrow
    id="S3.E3.m1.21.21.21.7.7.7.3.3.2" xref="S3.E3.m1.21.21.21.7.7.7.3.3.3.cmml"><mo
    maxsize="90%" minsize="90%" id="S3.E3.m1.21.21.21.7.7.7.3.3.2.3" xref="S3.E3.m1.21.21.21.7.7.7.3.3.3.cmml">(</mo><mi
    mathsize="90%" id="S3.E3.m1.21.21.21.7.7.7.1.1" xref="S3.E3.m1.21.21.21.7.7.7.1.1.cmml">𝐗</mi><mo
    mathsize="90%" id="S3.E3.m1.21.21.21.7.7.7.3.3.2.4" xref="S3.E3.m1.21.21.21.7.7.7.3.3.3.cmml">,</mo><msub
    id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.cmml"><mi
    mathsize="90%" id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.2" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.2.cmml">x</mi><mrow
    id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.cmml"><mi
    mathsize="90%" id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.2" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.2.cmml">m</mi><mo
    lspace="0em" rspace="0em" id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.1" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.1.cmml">​</mo><mi
    mathsize="90%" id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.3" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.3.cmml">i</mi><mo
    lspace="0em" rspace="0em" id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.1a" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.1.cmml">​</mo><mi
    mathsize="90%" id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.4" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.4.cmml">n</mi></mrow></msub><mo
    mathsize="90%" id="S3.E3.m1.21.21.21.7.7.7.3.3.2.5" xref="S3.E3.m1.21.21.21.7.7.7.3.3.3.cmml">,</mo><msub
    id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.cmml"><mi
    mathsize="90%" id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.2" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.2.cmml">x</mi><mrow
    id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.cmml"><mi
    mathsize="90%" id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.2" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.2.cmml">m</mi><mo
    lspace="0em" rspace="0em" id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.1" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.1.cmml">​</mo><mi
    mathsize="90%" id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.3" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.3.cmml">a</mi><mo
    lspace="0em" rspace="0em" id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.1a" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.1.cmml">​</mo><mi
    mathsize="90%" id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.4" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.4.cmml">x</mi></mrow></msub><mo
    maxsize="90%" minsize="90%" id="S3.E3.m1.21.21.21.7.7.7.3.3.2.6" xref="S3.E3.m1.21.21.21.7.7.7.3.3.3.cmml">)</mo></mrow></mrow><mi
    mathsize="90%" id="S3.E3.m1.21.21.21.7.7.7.5" xref="S3.E3.m1.21.21.21.7.7.7.5.cmml">s</mi></mfrac><mo
    maxsize="210%" minsize="210%" id="S3.E3.m1.22.22.22.8.8.8" xref="S3.E3.m1.39.39.3a.cmml">)</mo></mrow></mrow><mo
    mathsize="90%" id="S3.E3.m1.23.23.23.9.9.9" xref="S3.E3.m1.23.23.23.9.9.9.cmml">+</mo><mi
    mathsize="90%" id="S3.E3.m1.24.24.24.10.10.10" xref="S3.E3.m1.24.24.24.10.10.10.cmml">z</mi></mrow><mo
    maxsize="90%" minsize="90%" id="S3.E3.m1.25.25.25.11.11.11" xref="S3.E3.m1.39.39.3a.cmml">)</mo></mrow><mo
    mathsize="90%" id="S3.E3.m1.26.26.26.12.12.12" xref="S3.E3.m1.26.26.26.12.12.12.cmml">−</mo><mi
    mathsize="90%" id="S3.E3.m1.27.27.27.13.13.13" xref="S3.E3.m1.27.27.27.13.13.13.cmml">z</mi></mrow><mo
    maxsize="90%" minsize="90%" id="S3.E3.m1.28.28.28.14.14.14" xref="S3.E3.m1.39.39.3a.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    id="S3.E3.m1.42.42.6j" xref="S3.E3.m1.39.39.3.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E3.m1.42.42.6l" xref="S3.E3.m1.39.39.3.cmml"><mrow id="S3.E3.m1.42.42.6.39.9.9"
    xref="S3.E3.m1.39.39.3.cmml"><mo mathsize="90%" id="S3.E3.m1.29.29.29.1.1.1" xref="S3.E3.m1.29.29.29.1.1.1.cmml">=</mo><mrow
    id="S3.E3.m1.42.42.6.39.9.9.9" xref="S3.E3.m1.39.39.3.cmml"><mi mathsize="90%"
    id="S3.E3.m1.30.30.30.2.2.2" xref="S3.E3.m1.30.30.30.2.2.2.cmml">s</mi><mo lspace="0em"
    rspace="0em" id="S3.E3.m1.42.42.6.39.9.9.9.2" xref="S3.E3.m1.39.39.3a.cmml">​</mo><mrow
    id="S3.E3.m1.42.42.6.39.9.9.9.1.1" xref="S3.E3.m1.39.39.3.cmml"><mo maxsize="210%"
    minsize="210%" id="S3.E3.m1.31.31.31.3.3.3" xref="S3.E3.m1.39.39.3a.cmml">(</mo><mrow
    id="S3.E3.m1.42.42.6.39.9.9.9.1.1.1" xref="S3.E3.m1.39.39.3.cmml"><mtext mathsize="90%"
    id="S3.E3.m1.32.32.32.4.4.4" xref="S3.E3.m1.32.32.32.4.4.4a.cmml">round</mtext><mo
    lspace="0em" rspace="0em" id="S3.E3.m1.42.42.6.39.9.9.9.1.1.1.1" xref="S3.E3.m1.39.39.3a.cmml">​</mo><mrow
    id="S3.E3.m1.42.42.6.39.9.9.9.1.1.1.2" xref="S3.E3.m1.39.39.3.cmml"><mo maxsize="210%"
    minsize="210%" id="S3.E3.m1.33.33.33.5.5.5" xref="S3.E3.m1.39.39.3a.cmml">(</mo><mfrac
    id="S3.E3.m1.34.34.34.6.6.6" xref="S3.E3.m1.34.34.34.6.6.6.cmml"><mrow id="S3.E3.m1.34.34.34.6.6.6.3"
    xref="S3.E3.m1.34.34.34.6.6.6.3.cmml"><mtext mathsize="90%" id="S3.E3.m1.34.34.34.6.6.6.3.5"
    xref="S3.E3.m1.34.34.34.6.6.6.3.5a.cmml">clamp</mtext><mo lspace="0em" rspace="0em"
    id="S3.E3.m1.34.34.34.6.6.6.3.4" xref="S3.E3.m1.34.34.34.6.6.6.3.4.cmml">​</mo><mrow
    id="S3.E3.m1.34.34.34.6.6.6.3.3.2" xref="S3.E3.m1.34.34.34.6.6.6.3.3.3.cmml"><mo
    maxsize="90%" minsize="90%" id="S3.E3.m1.34.34.34.6.6.6.3.3.2.3" xref="S3.E3.m1.34.34.34.6.6.6.3.3.3.cmml">(</mo><mi
    mathsize="90%" id="S3.E3.m1.34.34.34.6.6.6.1.1" xref="S3.E3.m1.34.34.34.6.6.6.1.1.cmml">𝐗</mi><mo
    mathsize="90%" id="S3.E3.m1.34.34.34.6.6.6.3.3.2.4" xref="S3.E3.m1.34.34.34.6.6.6.3.3.3.cmml">,</mo><msub
    id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.cmml"><mi
    mathsize="90%" id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.2" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.2.cmml">x</mi><mrow
    id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.cmml"><mi
    mathsize="90%" id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.2" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.2.cmml">m</mi><mo
    lspace="0em" rspace="0em" id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.1" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.1.cmml">​</mo><mi
    mathsize="90%" id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.3" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.3.cmml">i</mi><mo
    lspace="0em" rspace="0em" id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.1a" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.1.cmml">​</mo><mi
    mathsize="90%" id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.4" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.4.cmml">n</mi></mrow></msub><mo
    mathsize="90%" id="S3.E3.m1.34.34.34.6.6.6.3.3.2.5" xref="S3.E3.m1.34.34.34.6.6.6.3.3.3.cmml">,</mo><msub
    id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.cmml"><mi
    mathsize="90%" id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.2" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.2.cmml">x</mi><mrow
    id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.cmml"><mi
    mathsize="90%" id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.2" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.2.cmml">m</mi><mo
    lspace="0em" rspace="0em" id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.1" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.1.cmml">​</mo><mi
    mathsize="90%" id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.3" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.3.cmml">a</mi><mo
    lspace="0em" rspace="0em" id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.1a" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.1.cmml">​</mo><mi
    mathsize="90%" id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.4" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.4.cmml">x</mi></mrow></msub><mo
    maxsize="90%" minsize="90%" id="S3.E3.m1.34.34.34.6.6.6.3.3.2.6" xref="S3.E3.m1.34.34.34.6.6.6.3.3.3.cmml">)</mo></mrow></mrow><mi
    mathsize="90%" id="S3.E3.m1.34.34.34.6.6.6.5" xref="S3.E3.m1.34.34.34.6.6.6.5.cmml">s</mi></mfrac><mo
    maxsize="210%" minsize="210%" id="S3.E3.m1.35.35.35.7.7.7" xref="S3.E3.m1.39.39.3a.cmml">)</mo></mrow></mrow><mo
    maxsize="210%" minsize="210%" id="S3.E3.m1.36.36.36.8.8.8" xref="S3.E3.m1.39.39.3a.cmml">)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" id="S3.E3.m1.42b"><apply id="S3.E3.m1.39.39.3.cmml"
    xref="S3.E3.m1.42.42.6"><apply id="S3.E3.m1.39.39.3b.cmml" xref="S3.E3.m1.42.42.6"><apply
    id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><ci id="S3.E3.m1.1.1.1.1.1.1.2.cmml"
    xref="S3.E3.m1.1.1.1.1.1.1.2">^</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">𝐗</ci></apply><apply
    id="S3.E3.m1.39.39.3.7.cmml" xref="S3.E3.m1.42.42.6"><ci id="S3.E3.m1.3.3.3.3.2.2a.cmml"
    xref="S3.E3.m1.3.3.3.3.2.2"><mtext mathsize="90%" id="S3.E3.m1.3.3.3.3.2.2.cmml"
    xref="S3.E3.m1.3.3.3.3.2.2">FakeQuant</mtext></ci><ci id="S3.E3.m1.5.5.5.5.4.4.cmml"
    xref="S3.E3.m1.5.5.5.5.4.4">𝐗</ci></apply></apply><apply id="S3.E3.m1.39.39.3c.cmml"
    xref="S3.E3.m1.42.42.6"><apply id="S3.E3.m1.37.37.1.1.cmml" xref="S3.E3.m1.42.42.6"><ci
    id="S3.E3.m1.8.8.8.2.2.2a.cmml" xref="S3.E3.m1.8.8.8.2.2.2"><mtext mathsize="90%"
    id="S3.E3.m1.8.8.8.2.2.2.cmml" xref="S3.E3.m1.8.8.8.2.2.2">Dequantize</mtext></ci><apply
    id="S3.E3.m1.37.37.1.1.1.1.1.cmml" xref="S3.E3.m1.42.42.6"><ci id="S3.E3.m1.10.10.10.4.4.4a.cmml"
    xref="S3.E3.m1.10.10.10.4.4.4"><mtext mathsize="90%" id="S3.E3.m1.10.10.10.4.4.4.cmml"
    xref="S3.E3.m1.10.10.10.4.4.4">Quantize</mtext></ci><ci id="S3.E3.m1.12.12.12.6.6.6.cmml"
    xref="S3.E3.m1.12.12.12.6.6.6">𝐗</ci></apply></apply></apply><apply id="S3.E3.m1.39.39.3e.cmml"
    xref="S3.E3.m1.42.42.6"><apply id="S3.E3.m1.38.38.2.2.cmml" xref="S3.E3.m1.42.42.6"><ci
    id="S3.E3.m1.16.16.16.2.2.2.cmml" xref="S3.E3.m1.16.16.16.2.2.2">𝑠</ci><apply
    id="S3.E3.m1.38.38.2.2.1.1.1.cmml" xref="S3.E3.m1.42.42.6"><apply id="S3.E3.m1.38.38.2.2.1.1.1.1.1.1.cmml"
    xref="S3.E3.m1.42.42.6"><apply id="S3.E3.m1.38.38.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.42.42.6"><ci
    id="S3.E3.m1.19.19.19.5.5.5a.cmml" xref="S3.E3.m1.19.19.19.5.5.5"><mtext mathsize="90%"
    id="S3.E3.m1.19.19.19.5.5.5.cmml" xref="S3.E3.m1.19.19.19.5.5.5">round</mtext></ci><apply
    id="S3.E3.m1.21.21.21.7.7.7.cmml" xref="S3.E3.m1.21.21.21.7.7.7"><apply id="S3.E3.m1.21.21.21.7.7.7.3.cmml"
    xref="S3.E3.m1.21.21.21.7.7.7.3"><ci id="S3.E3.m1.21.21.21.7.7.7.3.5a.cmml" xref="S3.E3.m1.21.21.21.7.7.7.3.5"><mtext
    mathsize="90%" id="S3.E3.m1.21.21.21.7.7.7.3.5.cmml" xref="S3.E3.m1.21.21.21.7.7.7.3.5">clamp</mtext></ci><vector
    id="S3.E3.m1.21.21.21.7.7.7.3.3.3.cmml" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2"><ci
    id="S3.E3.m1.21.21.21.7.7.7.1.1.cmml" xref="S3.E3.m1.21.21.21.7.7.7.1.1">𝐗</ci><apply
    id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.cmml" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1"><csymbol
    cd="ambiguous" id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.1.cmml" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1">subscript</csymbol><ci
    id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.2.cmml" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.2">𝑥</ci><apply
    id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.cmml" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3"><ci
    id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.2.cmml" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.2">𝑚</ci><ci
    id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.3.cmml" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.3">𝑖</ci><ci
    id="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.4.cmml" xref="S3.E3.m1.21.21.21.7.7.7.2.2.1.1.3.4">𝑛</ci></apply></apply><apply
    id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.cmml" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2"><csymbol
    cd="ambiguous" id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.1.cmml" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2">subscript</csymbol><ci
    id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.2.cmml" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.2">𝑥</ci><apply
    id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.cmml" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3"><ci
    id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.2.cmml" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.2">𝑚</ci><ci
    id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.3.cmml" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.3">𝑎</ci><ci
    id="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.4.cmml" xref="S3.E3.m1.21.21.21.7.7.7.3.3.2.2.3.4">𝑥</ci></apply></apply></vector></apply><ci
    id="S3.E3.m1.21.21.21.7.7.7.5.cmml" xref="S3.E3.m1.21.21.21.7.7.7.5">𝑠</ci></apply></apply><ci
    id="S3.E3.m1.24.24.24.10.10.10.cmml" xref="S3.E3.m1.24.24.24.10.10.10">𝑧</ci></apply><ci
    id="S3.E3.m1.27.27.27.13.13.13.cmml" xref="S3.E3.m1.27.27.27.13.13.13">𝑧</ci></apply></apply></apply><apply
    id="S3.E3.m1.39.39.3g.cmml" xref="S3.E3.m1.42.42.6"><apply id="S3.E3.m1.39.39.3.3.cmml"
    xref="S3.E3.m1.42.42.6"><ci id="S3.E3.m1.30.30.30.2.2.2.cmml" xref="S3.E3.m1.30.30.30.2.2.2">𝑠</ci><apply
    id="S3.E3.m1.39.39.3.3.1.1.1.cmml" xref="S3.E3.m1.42.42.6"><ci id="S3.E3.m1.32.32.32.4.4.4a.cmml"
    xref="S3.E3.m1.32.32.32.4.4.4"><mtext mathsize="90%" id="S3.E3.m1.32.32.32.4.4.4.cmml"
    xref="S3.E3.m1.32.32.32.4.4.4">round</mtext></ci><apply id="S3.E3.m1.34.34.34.6.6.6.cmml"
    xref="S3.E3.m1.34.34.34.6.6.6"><apply id="S3.E3.m1.34.34.34.6.6.6.3.cmml" xref="S3.E3.m1.34.34.34.6.6.6.3"><ci
    id="S3.E3.m1.34.34.34.6.6.6.3.5a.cmml" xref="S3.E3.m1.34.34.34.6.6.6.3.5"><mtext
    mathsize="90%" id="S3.E3.m1.34.34.34.6.6.6.3.5.cmml" xref="S3.E3.m1.34.34.34.6.6.6.3.5">clamp</mtext></ci><vector
    id="S3.E3.m1.34.34.34.6.6.6.3.3.3.cmml" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2"><ci
    id="S3.E3.m1.34.34.34.6.6.6.1.1.cmml" xref="S3.E3.m1.34.34.34.6.6.6.1.1">𝐗</ci><apply
    id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.cmml" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1"><csymbol
    cd="ambiguous" id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.1.cmml" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1">subscript</csymbol><ci
    id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.2.cmml" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.2">𝑥</ci><apply
    id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.cmml" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3"><ci
    id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.2.cmml" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.2">𝑚</ci><ci
    id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.3.cmml" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.3">𝑖</ci><ci
    id="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.4.cmml" xref="S3.E3.m1.34.34.34.6.6.6.2.2.1.1.3.4">𝑛</ci></apply></apply><apply
    id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.cmml" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2"><csymbol
    cd="ambiguous" id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.1.cmml" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2">subscript</csymbol><ci
    id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.2.cmml" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.2">𝑥</ci><apply
    id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.cmml" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3"><ci
    id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.2.cmml" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.2">𝑚</ci><ci
    id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.3.cmml" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.3">𝑎</ci><ci
    id="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.4.cmml" xref="S3.E3.m1.34.34.34.6.6.6.3.3.2.2.3.4">𝑥</ci></apply></apply></vector></apply><ci
    id="S3.E3.m1.34.34.34.6.6.6.5.cmml" xref="S3.E3.m1.34.34.34.6.6.6.5">𝑠</ci></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E3.m1.42c">\small\begin{split}\widehat{\mathbf{X}}{}&=\textrm{FakeQuant}(\mathbf{X})\\
    &=\textrm{Dequantize}(\textrm{Quantize}(\mathbf{X}))\\ &=s((\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}+z)-z)\\ &=s\bigg{(}\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}\bigg{)}\\ \end{split}</annotation></semantics></math> |  |
  prefs: []
  type: TYPE_NORMAL
- en: Since the above equation is not directly differentiable because of the rounding
    behavior, to optimize a loss function $L$ w.r.t. $\mathbf{X}$, we can compute
    $\frac{\displaystyle\partial{L}}{\displaystyle\partial{\mathbf{X}}}$ by chain-rule
    using the Straight-Through Estimator (STE) (Bengio et al., [2013](#bib.bib23)).
    This allows us to make the staircase function differentiable with a linear approximation
    (See (Krishnamoorthi, [2018](#bib.bib91)) for details).
  prefs: []
  type: TYPE_NORMAL
- en: Quantization-Aware Training allows the network to adapt to tolerate the noise
    introduced by the clamping and rounding behavior during inference. Once the network
    is trained, tools such as the TFLite Model Converter (Authors, [2021h](#bib.bib16))
    can generate the appropriate fixed-point inference model from a network annotated
    with the quantization nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other Notable Works: Polino et al. (Polino et al., [2018](#bib.bib125)) allow
    non-uniform distribution of precision with learning a vector of quantization-points
    $p$, along with using distillation to further reduce loss of accuracy. The results
    for simpler datasets like CIFAR-10 are comparable to (Krishnamoorthi, [2018](#bib.bib91);
    Jacob et al., [2018](#bib.bib83)). However, when working with ResNet architecture
    on the ImageNet dataset, they achieve lower model size and faster inference by
    using shallower student networks. This is not a fair comparison, since other works
    do not mix distillation along with quantization. Fan et al. (Fan et al., [2020](#bib.bib56))
    demonstrate accuracy improvement on top of standard QAT ((Jacob et al., [2018](#bib.bib83)))
    with $b<8$. They hypothesize that the networks will learn better if the fake-quantization
    is not applied to the complete tensor at the same time to allow unbiased gradients
    to flow (instead of the STE approximation). Instead, they apply the fake-quantization
    operation stochastically in a block-wise manner on the given tensor. They also
    demonstrate improvements over QAT on 4-bit quantized Transformer and EfficientNet
    (Tan et al., [2019](#bib.bib148)) networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results: Refer to Table [2](#S3.T2 "Table 2 ‣ 3.1.2\. Quantization ‣ 3.1\.
    Compression Techniques ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    for a comparison between the baseline floating-point model, post-training quantized,
    and quantization-aware trained models (Authors, [2021f](#bib.bib14)). The model
    with post-training quantization gets close to the baseline, but there is still
    a significant accuracy difference. The model size is $4\times$ smaller, however
    the latency is slightly higher due to the need to dequantize the weights during
    inference. The model with 8-bit Quantization-Aware Training (QAT) gets quite close
    to the baseline floating point model while requiring $4\times$ less disk space
    and being $1.64\times$ faster.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model Architecture | Quantization Type | Top-1 Accuracy | Size (MB) | Latency
    (ms, Pixel2) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MobileNet v2-1.0 (224) | Baseline | 71.9% | 14 | 89 |'
  prefs: []
  type: TYPE_TB
- en: '| Post-Training Quantization | 63.7% | 3.6 | 98 |'
  prefs: []
  type: TYPE_TB
- en: '| Quantization-Aware Training | 70.9% | 3.6 | 54 |'
  prefs: []
  type: TYPE_TB
- en: Table 2\. A sample of various quantization results on the MobileNet v2 architecture
    for 8-bit quantization (TensorFlow, [2021](#bib.bib151)). We picked results on
    8-bit, since from they can be readily used with hardware and software that exists
    today.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2d9889535c7e3fe8c6c2e8f54c07c036.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Quantization-Aware Training
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/292b3ab51f8eec3f51e4b46e10985788.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Final fixed-point inference graph
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6\. (a) shows the injection of fake-quantization nodes to simulate quantization
    effect and collecting tensor statistics, for exporting a fully fixed-point inference
    graph. (b) shows the inference graph derived from the same graph as (a). Inputs
    and weights are in uint8, and results of common operations are in uint32. Biases
    are kept in uint32 (Jacob et al., [2018](#bib.bib83); Krishnamoorthi, [2018](#bib.bib91))
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussion:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantization is a well-studied technique for model optimization and can help
    with very significant reduction in model size (often $4\times$ when using 8-bit
    quantization) and inference latency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight quantization is straight-forward enough that it can be implemented by
    itself for reducing model size. Activation quantization should be strongly considered
    because it enables both latency reduction, as well as lower working memory required
    for intermediate computations in the model (which is essential for devices with
    low memory availability)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When possible, Quantization-Aware Training should be used. It has been shown
    to dominate post-training quantization in terms of accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, tools like Tensorflow Lite have made it easy to rely on post-training
    quantization. (TensorFlow, [2019](#bib.bib150)) shows that often there is minimal
    loss when using post-training quantization, and with the help of a representative
    dataset this is further shrunk down. Wherever there is an opportunity for switching
    to fixed-point operations, the infrastructure allows using them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For performance reasons, it is best to consider the common operations that follow
    a typical layer such as Batch-Norm, Activation, etc. and ‘fold’ them in the quantization
    operations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.1.3\. Other Compression Techniques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are other compression techniques like Low-Rank Matrix Factorization, K-Means
    Clustering, Weight-Sharing etc. which are also actively being used for model compression
    (Panigrahy, [2021](#bib.bib118)) and might be suitable for further compressing
    hotspots in a model.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Learning Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning techniques try to train a model differently in order to obtain better
    quality metrics (accuracy, F1 score, precision, recall, etc.) while allowing supplementing,
    or in some cases replacing the traditional supervised learning. The improvement
    in quality can sometimes be traded off for a smaller footprint by reducing the
    number of parameters / layers in the model and achieving the same baseline quality
    with a smaller model. An incentive of paying attention to learning techniques
    is that they are applied only on the training, without impacting the inference.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. Distillation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ensembles are well known to help with generalization (Krogh and Vedelsby, [1994](#bib.bib94);
    Hansen and Salamon, [1990](#bib.bib72)). The intuition is that this enables learning
    multiple independent hypotheses, which are likely to be better than learning a
    single hypothesis. (Dietterich, [2000](#bib.bib49)) goes over some of the standard
    ensembling methods such as bagging (learning models that are trained on non-overlapping
    data and then ensembling them), boosting (learning models that are trained to
    fix the classification errors of other models in the ensemble), averaging (voting
    by all the ensemble models), etc.Bucila et al. (Buciluǎ et al., [2006](#bib.bib28))
    used large ensembles to label synthetic data that they generated using various
    schemes. A smaller neural net is then trained to learn not just from the labeled
    data but also from this weakly labeled synthetic data. They found that single
    neural nets were able to mimic the performance of larger ensembles, while being
    $1000\times$ smaller and faster. This demonstrated that it is possible to transfer
    the cumulative knowledge of ensembles to a single small model. Though it might
    not be sufficient to rely on just the existing labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Hinton et al. (Hinton et al., [2015](#bib.bib76)), in their seminal work explored
    how smaller networks (students) can be taught to extract ‘dark knowledge’ from
    larger models / ensembles of larger models (teachers) in a slightly different
    manner. Instead of having to generate synthetic-data, they use the larger teacher
    model to generate *soft-labels* on existing labeled data. The soft-labels assign
    a probability to each class, instead of hard binary values in the original data.
    The intuition is that these soft-labels capture the relationship between the different
    classes which the model can learn from. For example, a truck is more similar to
    a car than to an apple, which the model might not be able to learn directly from
    hard labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The student network learns to minimize the cross-entropy loss on these soft
    labels, along with the original ground-truth hard labels. Since the probabilities
    of the incorrect classes might be very small, the logits are scaled down by a
    ‘temperature’ value $\geq 1.0$, so that the distribution is ‘softened’. If the
    input vector is $\mathbf{X}$, and the teacher model’s logits are $\mathbf{Z^{(t)}}$,
    the teacher model’s softened probabilities with temperature $T$ can be calculated
    as follows using the familiar softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\small\mathbf{Y}_{i}^{(t)}=\frac{\displaystyle\exp(\mathbf{Z_{i}^{(t)}}/T)}{\displaystyle\sum_{j=1}^{n}\exp(\mathbf{Z_{j}^{(t)}}/T)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Note that as $T$ increases, the relative differences between the various elements
    of $Y^{(t)}$ decreases. This happens because if all elements are divided by the
    same constant, the softmax function would lead to a larger drop for the bigger
    values. Hence, as the temperature $T$ increases, we see the distribution of $Y^{(t)}$
    ‘soften’ further.
  prefs: []
  type: TYPE_NORMAL
- en: 'When training along with labeled data ($\mathbf{X}$, $\mathbf{Y}$), and the
    student model’s output ($\mathbf{Y^{(s)}}$), we can describe the loss function
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $\small\begin{split}L&amp;=\lambda_{1}\cdot L_{\rm ground-truth}+\lambda_{2}\cdot
    L_{\rm distillation}\\ &amp;=\lambda_{1}\cdot\textrm{CrossEntropy}(\mathbf{Y},\mathbf{Y^{(s)}};\theta)+\lambda_{2}\cdot\textrm{CrossEntropy}(\mathbf{Y^{(t)}},\mathbf{Y^{(s)}};\theta)\end{split}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'CrossEntropy is the cross-entropy loss function, which takes in the labels
    and the output. For the first loss term, we pass along the ground truth labels,
    and for the second loss term we pass the corresponding soft labels from the teacher
    model for the same input. $\lambda_{1}$ and $\lambda_{2}$ control the relative
    importance of the standard ground truth loss and the distillation loss respectively.
    When $\lambda_{1}=0$, the student model is trained with just the distillation
    loss. Similarly, when $\lambda_{2}=0$, it is equivalent to training with just
    the ground-truth labels. Usually, the teacher network is pre-trained and frozen
    during this process, and only the student network is updated. Refer to Figure
    [7](#S3.F7 "Figure 7 ‣ 3.2.1\. Distillation ‣ 3.2\. Learning Techniques ‣ 3\.
    Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better") for an illustration of this
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/88ff33534ccce386d9eb2316a948b12a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. Distillation of a smaller student model from a larger pre-trained
    teacher model. Both the teacher and student models receive the same input. The
    teacher is used to generate ‘soft-labels’ for the student, which gives the student
    more information than just hard binary labels. The student is trained using the
    regular cross-entropy loss with the hard labels, as well as using the distillation
    loss function which uses the soft labels from the teacher. In this setting, the
    teacher is frozen, and only the student receives the gradient updates.
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, Hinton et al. (Hinton et al., [2015](#bib.bib76)) were able to
    closely match the accuracy of a 10 model ensemble for a speech recognition task
    with a single distilled model. Urban et al. (Urban et al., [2016](#bib.bib153))
    did a comprehensive study demonstrating that distillation significantly improves
    performance of shallow student networks as small as an MLP with one hidden layer
    on tasks like CIFAR-10\. Sanh et al. (Sanh et al., [2019](#bib.bib135)) use the
    distillation loss for compressing a BERT (Devlin et al., [2018](#bib.bib48)) model
    (along with a cosine loss that minimizes the cosine distance between two internal
    vector representation of the input as seen by the teacher and student models).
    Their model retains 97% of the performance of BERT-Base while being 40% smaller
    and 60% faster on CPU.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to adapt the general idea of distillation to work on intermediate
    outputs of teachers and students. Zagoruyko et al. (Zagoruyko and Komodakis, [2016](#bib.bib166))
    transfer intermediate ‘attention maps’ between teacher and student convolutional
    networks. The intuition is to make the student focus on the parts of the image
    where the teacher is paying attention to. MobileBERT (Sun et al., [2020](#bib.bib145))
    uses a progressive-knowledge transfer strategy where they do layer-wise distillation
    between the BERT student and teacher models, but they do so in stages, where the
    first $l$ layers are distilled in the $l$-th stage. Along with other architecture
    improvements, they obtain a 4.3$\times$ smaller and 5.5$\times$ faster BERT with
    small losses in quality.
  prefs: []
  type: TYPE_NORMAL
- en: Another idea that has been well explored is exploiting a model trained in a
    supervised training to label unlabeled data. Blum et al. (Blum and Mitchell, [1998](#bib.bib25))
    in their paper from 1998, report halving the error rate of their classifiers by
    retraining on a subset of pseudo-labels generated using the previous classifiers.
    This has been extended through distillation to use the teacher model to label
    a large corpus of unlabeled data, which can then be used to improve the quality
    of the student model (Menghani and Ravi, [2019](#bib.bib110); Xie et al., [2020](#bib.bib162);
    Yalniz et al., [2019](#bib.bib163)).
  prefs: []
  type: TYPE_NORMAL
- en: Overall, distillation has been empirically shown to improve both the accuracy
    as well as the speed of convergence of student models across many domains. Hence,
    it enables training smaller models which might otherwise not be have an acceptable
    quality for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussion:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distillation is an adaptable technique that needs minimal changes in the training
    infrastructure to be used. Even if the teacher model cannot be executed at the
    same time as the student model, the teacher model’s predictions can be collected
    offline and treated as another source of labels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When there is sufficient label data, there is ample evidence that distillation
    is likely to improve the student model’s predictions. If there is a large corpus
    of unlabeled data, the teacher model can be used to generate pseudo-labels on
    the unlabeled data, which can further improve the student model’s accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strategies for intermediate-layer distillation have also shown to be effective
    in the case of complex networks. In such scenarios, a new loss term minimizing
    the difference between the outputs of the two networks at some semantically identical
    intermediate point(s) needs to be added.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2.2\. Data Augmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When training large models for complex tasks in a supervised learning regime,
    the size of the training data corpus correlates with improvement in generalization.
    (Sun et al., [2017](#bib.bib144)) demonstrates logarithmic increase in the prediction
    accuracy with increase in the number of labeled examples. However, getting high-quality
    labeled data often requires a human in the loop and could be expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Data Augmentation is a nifty way of addressing the scarcity of labeled data,
    by synthetically inflating the existing dataset through some *augmentation methods*.
    These augmentation methods are transformations that can be applied cheaply on
    the given examples, such that the new label of the augmented example does not
    change, or can be cheaply inferred. As an example, consider the classical image
    classification task of labeling a given image to be a cat or a dog. Given an image
    of a dog, translating the image horizontally / vertically by a small number of
    pixels, rotating it by a small angle, etc. would not materially change the image,
    so the transformed image should still be labeled as ‘dog’ by the classifier. This
    forces the classifier to learn a robust representation of the image that generalizes
    better across these transformations.
  prefs: []
  type: TYPE_NORMAL
- en: The transformations as described above have long been demonstrated to improve
    accuracy of convolutional networks (Simard et al., [2003](#bib.bib141); Cireşan
    et al., [2011](#bib.bib37)). They have also been a core part of seminal works
    in Image Classification. A prime example is AlexNet (Krizhevsky et al., [2012](#bib.bib93)),
    where such transformations were used to increase the effective size of the training
    dataset by 2048 $\times$, which won the ImageNet competition in 2012\. Since then
    it has became common to use such transformations for Image Classification models
    (Inception (Szegedy et al., [2015](#bib.bib147)), XCeption (Chollet, [2017](#bib.bib35)),
    ResNet (He et al., [2016](#bib.bib74)), etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can categorize data-augmentation methods as follows (also refer to Figure
    [8](#S3.F8 "Figure 8 ‣ 3.2.2\. Data Augmentation ‣ 3.2\. Learning Techniques ‣
    3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on
    Making Deep Learning Models Smaller, Faster, and Better")):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Label-Invariant Transformations: These are some of the most common transformations,
    where the transformed example retains the original label. These can include simple
    geometric transformations such as translation, flipping, cropping, rotation, distortion,
    scaling, shearing, etc. However the user has to verify the label-invariance property
    with each transformation for the specific task at hand.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Label-Mixing Transformations: Transformations such as Mixup (Zhang et al.,
    [2017](#bib.bib167)), mix inputs from two different classes in a weighted manner
    and treat the label to be a correspondingly weighted combination of the two classes
    (in the same ratio). The intuition is that the model should be able to extract
    out features that are relevant for both the classes. Other transformations like
    Sample Pairing also seem to help (Inoue, [2018](#bib.bib82)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data-Dependent Transformations: In this case, transformations are chosen such
    that they maximize the loss for that example (Fawzi et al., [2016](#bib.bib57)),
    or are adversarially chosen so as to fool the classifier (Gopalan et al., [2021](#bib.bib68)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Synthetic Sampling: These methods synthetically create new training examples.
    Algorithms like SMOTE (Chawla et al., [2002](#bib.bib31)) allow re-balancing the
    dataset to make up for skew in the datasets, and GANs can be used to synthetically
    create new samples (Zhu and Gupta, [2018](#bib.bib168)) to improve model accuracy.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Composition of Transformations: These are transformations that are themselves
    composed of other transformations, and the labels are computed depending on the
    nature of transformations that stacked.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4eeb88bff5b03fad44b18c8bbfc30034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8\. Some common types of data augmentations. Source: (Li, [2020](#bib.bib103))'
  prefs: []
  type: TYPE_NORMAL
- en: '| Transformation | Validation Accuracy Improvement (%) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| rotate | 1.3 |'
  prefs: []
  type: TYPE_TB
- en: '| shear-x | 0.9 |'
  prefs: []
  type: TYPE_TB
- en: '| shear-y | 0.9 |'
  prefs: []
  type: TYPE_TB
- en: '| translate-x | 0.4 |'
  prefs: []
  type: TYPE_TB
- en: '| translate-y | 0.4 |'
  prefs: []
  type: TYPE_TB
- en: '| sharpness | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| autoContrast | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3\. A breakdown of the contribution of various transformations on the
    validation accuracy of a model trained on the CIFAR-10 dataset. Source: (Cubuk
    et al., [2020](#bib.bib45)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussion: Apart from Computer Vision, Data-Augmentation has also been used
    in NLP, and Speech. In NLP, a common idea that has been used is ‘back-translation’
    (Yu et al., [2018](#bib.bib164)) where augmented examples are created by training
    two translation models, one going from the source language to the target language,
    and the other going back from the target language to the original source language.
    Since the back-translation is not exact, this process is able to generate augmented
    samples for the given input. Other methods like WordDropout (Sennrich et al.,
    [2016](#bib.bib140)) stochastically set embeddings of certain words to zero. SwitchOut
    (Wang et al., [2018](#bib.bib159)) introduces a similarity measure to disallow
    augmentations that are too dissimilar to the original input. In Speech (Hannun
    et al., [2014](#bib.bib71)), the input audio samples are translated to the left
    / right before being passed to the decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: While the augmentation policies are usually hand-tuned, there are also methods
    such as AutoAugment (Cubuk et al., [2019](#bib.bib44)) where the augmentation
    policy is learned through a Reinforcement-Learning (RL) based search, searching
    for the transformations to be applied, as well as their respective hyper-parameters.
    Though this is shown to improve accuracy, it is also complicated and expensive
    to setup a separate search for augmentation, taking as many as 15000 GPU hours
    to learn the optimal policy on ImageNet. The RandAugment (Cubuk et al., [2020](#bib.bib45))
    paper demonstrated that it is possible to achieve similar results while reducing
    the search space to just two hyper-parameters (number of augmentation methods,
    and the strength of the distortion) for a given model and dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we see that data-augmentation leads to better generalization of the
    given models. Some techniques can be specific for their domains RandAugment (Vision),
    BackTranslation and SwitchOut (NLP), etc. However, the core principles behind
    them make it likely that similar methods can be derived for other domains too
    (refer to our categorization of data-augmentation methods above).
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3\. Self-Supervised Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Supervised-Learning paradigm relies heavily on labeled data. As mentioned
    earlier, it requires human intervention, and is expensive as well. To achieve
    reasonable quality on a non-trivial task, the amount of labeled data requires
    is large too. While techniques like Data-Augmentation, Distillation etc., help,
    they too rely on the presence of some labeled data to achieve a baseline performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-Supervised learning (SSL) avoids the need for labeled data to learn generalized
    representations, by aiming to extract more supervisory bits from each example.
    Since it focuses on learning robust representations of the example itself, it
    does not need to focus narrowly on the label. This is typically done by solving
    a *pretext task* where the model pretends that a part / structure of the input
    is missing and learns to predict it (Refer to Figure [9](#S3.F9 "Figure 9 ‣ 3.2.3\.
    Self-Supervised Learning ‣ 3.2\. Learning Techniques ‣ 3\. Landscape of Efficient
    Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better") for examples). Since unlabeled data is vast in many
    domains (Books, Wikipedia, and other text for NLP, Web Images & Videos for Computer
    Vision, etc.), the model would not be bottlenecked by data for learning to solve
    these pretext tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/99ebb4d4aa1ba09a1925a9e674e1a23d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9\. General theme of pretext tasks. Source: (LeCun, [2018](#bib.bib97))'
  prefs: []
  type: TYPE_NORMAL
- en: Once the models learn generic representations that transfer well across tasks,
    they can be adapted to solve the target task by adding some layers that project
    the representation to the label space, and fine-tuning the model with the labeled
    data. Since the labeled data is not being used for learning rudimentary features,
    but rather how to map the high-level representations into the label space, the
    quantum of labeled data is going to be a fraction of what would have been required
    for training the model from scratch. From this lens, fine-tuning models pre-trained
    with Self-Supervised learning are *data-efficient* (they converge faster, attain
    better quality for the same amount of labeled data when compared to training from
    scratch, etc.) ((Howard and Ruder, [2018](#bib.bib79); Devlin et al., [2018](#bib.bib48))).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e5578f99c309d06d7e9ba218bdb137f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10\. Validation Error w.r.t. number of training examples for different
    training methods on IMDb (from scratch, ULMFiT Supervised: pre-training with WikiText-103
    and fine-tuning using labeled data, ULMFit Semi-Supervised: Pre-Training with
    WikiText-103 as well as unlabeled data from the target dataset and fine-tuning
    with labeled data). Source: (Howard and Ruder, [2018](#bib.bib79))'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of this two step process of pre-training on unlabeled data and fine-tuning
    on labeled data has gained rapid acceptance in the NLP community. ULMFiT (Howard
    and Ruder, [2018](#bib.bib79)) pioneered the idea of training a general purpose
    language model, where the model learns to solve the pretext task of predicting
    the next word in a given sentence, without the neeof an associated label. The
    authors found that using a large corpus of preprocessed unlabeled data such as
    the WikiText-103 dataset (derived from English Wikipedia pages) was a good choice
    for the pre-training step. This was sufficient for the model to learn general
    properties about the language, and the authors found that fine-tuning such a pre-trained
    model for a binary classification problem (IMDb dataset) required only 100 labeled
    examples ($\approx 10\times$ less labeled examples otherwise). Refer to Figure
    [10](#S3.F10 "Figure 10 ‣ 3.2.3\. Self-Supervised Learning ‣ 3.2\. Learning Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better"). If we add a middle-step
    of pre-training using unlabeled data from the same target dataset, the authors
    report needing $\approx 20\times$ fewer labeled examples.'
  prefs: []
  type: TYPE_NORMAL
- en: This idea of pre-training followed by fine-tuning is also used in BERT (Devlin
    et al., [2018](#bib.bib48)) (and other related models like GPT, RoBERTa, T5, etc.)
    where the pre-training steps involves learning to solve two tasks. Firstly, the
    Masked Language Model where about 15% of the tokens in the given sentence are
    masked and the model needs to predict the masked token. The second task is, given
    two sentences $A$ and $B$, predict if $B$ follows $A$. The pre-training loss is
    the mean of the losses for the two tasks. Once pre-trained the model can then
    be used for classification or seq2seq tasks by adding additional layers on top
    of the last hidden layer. When it was published, BERT beat the State-of-the-Art
    on eleven NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to NLP, the pretext tasks in Vision have been used to train models
    that learn general representations. (Doersch et al., [2015](#bib.bib50)) extracts
    two patches from a training example and then trains the model to predict their
    relative position in the image (Refer to Figure [11](#S3.F11 "Figure 11 ‣ 3.2.3\.
    Self-Supervised Learning ‣ 3.2\. Learning Techniques ‣ 3\. Landscape of Efficient
    Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better")(a)). They demonstrate that using a network pre-trained
    in this fashion improves the quality of the final object detection task, as compared
    to randomly initializing the network. Similarly, another task is to predict the
    degree of rotation for a given rotated image (Gidaris et al., [2018](#bib.bib60)).
    The authors report that the network trained in a self-supervised manner this way
    can be fine-tuned to perform nearly as well as a fully supervised network.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb75dc500ca9d5a594d1ee592301c566.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Detecting relative order of patches. Source: (Doersch et al., [2015](#bib.bib50)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5133d594f68ecb7b1e044476c74a2b8c.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Predicting the degree of rotation of a given image.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11\. Pretext tasks for vision problems.
  prefs: []
  type: TYPE_NORMAL
- en: Another common theme is Contrastive Learning, where the model is trained to
    distinguish between similar and dissimilar inputs. Frameworks such as SimCLR (Chen
    et al., [2020a](#bib.bib33); Chen et al., [2020b](#bib.bib34)), try to learn representations
    $h_{i}$ and $h_{j}$ for two given inputs $\tilde{x_{i}}$ and $\tilde{x_{j}}$,
    where the latter two are differently augmented views of the same input, such that
    the cosine similarity of the projections of $h_{i}$ and $h_{j}$, $z_{i}$ and $z_{j}$
    (using a separate function $g(.)$) can be maximized. Similarly, for dissimilar
    inputs the cosine similarity of $z_{i}$ and $z_{j}$ should be minimized. The authors
    report a Top-1 accuracy of $73.9\%$ on ImageNet with only 1% labels (13 labels
    per class), and outperform the ResNet-50 supervised baseline with only 10% labels.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8a2e40f8d451474d2e3ac2bc15a97f43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12\. SimCLR framework for learning visual representations. Source: (Chen
    et al., [2020a](#bib.bib33))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussion: Self-Supervised Learning (SSL) has demonstrated significant success
    in the general representational learning with unlabeled data, followed by fine-tuning
    to adapt the model to the target task with a modest number of labeled examples.
    Yann LeCun has likened Self-Supervision as the cake, and Supervised Learning as
    the icing on top (LeCun, [2018](#bib.bib97)), implying that SSL will be the primary
    way of training high-quality models in the future as we move beyond tasks where
    labeled data is abundant.'
  prefs: []
  type: TYPE_NORMAL
- en: With unlabeled data being practically limitless, SSL’s success is dependent
    on creating useful pretext tasks for the domain of interest. As demonstrated across
    NLP (Howard and Ruder, [2018](#bib.bib79); Devlin et al., [2018](#bib.bib48)),
    Vision (Chen et al., [2020a](#bib.bib33); Patrick et al., [2020](#bib.bib121)),
    Speech (Glass, [2012](#bib.bib61)), etc., Self-Supervision is indeed not just
    helpful in speeding and improving convergence, but also enabling achieving high
    quality in tasks where it was intractable to get enough labeled samples.
  prefs: []
  type: TYPE_NORMAL
- en: Practically, for someone training Deep Learning models on a custom task (say
    a speech recognition model for a remote African dialect), having a pre-trained
    checkpoint of a model trained in a self-supervised fashion (such as wav2vec (Baevski
    et al., [2020](#bib.bib21)), which pre-trained in a similar way to BERT (Devlin
    et al., [2018](#bib.bib48))), enables them to only spend an extremely tiny fraction
    of resources on both data labeling, as well as training to fine-tune to a good
    enough quality. In some cases, such as SimCLR (Chen et al., [2020b](#bib.bib34)),
    SSL approaches have actually beaten previous supervised baselines with sophisticated
    models like ResNet-50\. Hence, we are hopeful that SSL methods will be crucial
    for ML practitioners for training high-quality models cheaply.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Automation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is possible to delegate some of the work around efficiency to automation,
    and letting automated approaches search for ways of training more efficient models.
    Apart from reducing work for humans, it also lowers the bias that manual decisions
    might introduce in model training, apart from systematically and automatically
    looking for optimal solutions. The trade-off is that these methods might require
    large computational resources, and hence have to be carefully applied.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1\. Hyper-Parameter Optimization (HPO)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the commonly used methods that fall under this category is Hyper-Parameter
    Optimization (HPO) (Yu and Zhu, [2020](#bib.bib165)). Hyper-parameters such as
    initial learning rate, weight decay, etc. have to be carefully tuned for faster
    convergence (Jeremy Jordan, [2020](#bib.bib86)). They can also decide the network
    architecture such as the number of fully connected layers, number of filters in
    a convolutional layer, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Experimentation can help us build an intuition for the *range* in which these
    parameters might lie, but finding the best values requires a search for the exact
    values that optimize the given objective function (typically the loss value on
    the validation set). Manually searching for these quickly becomes tedious with
    the growth in the number of hyper-parameters and/or their possible values. Hence,
    let us explore possible algorithms for automating the search. To formalize this,
    let us assume without the loss of generalization, that we are optimizing the loss
    value on the given dataset’s validation split. Then, let $\mathcal{L}$ be the
    loss function, $f$ be the model function that is learnt with the set of hyper-parameters
    ($\lambda$), $x$ be the input, and $\theta$ be the model parameters. With the
    search, we are trying to find $\lambda^{*}$ such that,
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\small\lambda^{*}=\operatorname*{argmin}_{\lambda\in\Lambda}\mathcal{L}(f_{\lambda}(x;\theta),y)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '$\Lambda$ is the set of all possible hyper-parameters. In practice, the $\Lambda$
    can be a very large set containing all possible combinations of the hyper-parameters,
    which would often be intractable since hyper-parameters like learning rate are
    real-valued. A common strategy is to approximate $\Lambda$ by picking a finite
    set of *trials*, $S=\{\lambda^{(1)},\lambda^{(2)},...,\lambda^{(n)}\}$, such that
    $S\in\Lambda$, and then we can approximate Equation ([6](#S3.E6 "In 3.3.1\. Hyper-Parameter
    Optimization (HPO) ‣ 3.3\. Automation ‣ 3\. Landscape of Efficient Deep Learning
    ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster,
    and Better")) with:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $\small\lambda^{*}\approx\operatorname*{argmin}_{\lambda\in\{\lambda^{(1)},...,\lambda^{(n)}\}}\mathcal{L}(f_{\lambda}(x;\theta),y)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: As we see, the choice of $S$ is crucial for the approximation to work. The user
    has to construct a range of reasonable values for each hyper-parameter $\lambda_{i}\in\lambda$.
    This can be based on prior experience with those hyper-parameters.
  prefs: []
  type: TYPE_NORMAL
- en: A simple algorithm for automating HPO is Grid Search (also referred to as Parameter
    Sweep), where $S$ consists of all the distinct and valid combinations of the given
    hyper-parameters based on their specified ranges. Each trial can then be run in
    parallel since each trial is independent of the others, and the optimal combination
    of the hyper-parameters is found once all the trials have completed. Since this
    approach tries all possible combinations, it suffers from the *curse of dimensionality*,
    where the total number of trials grow very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach is Random Search where trials are sampled randomly from the
    search space (Bergstra and Bengio, [2012](#bib.bib24)). Since each trial is independent
    of the others, it can still be executed randomly. However, there are few critical
    benefits of Random Search:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the trials are i.i.d. (not the case for Grid Search), the resolution of
    the search can be changed on-the-fly (if the computational budget has changed,
    or certain trials have failed).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Likelihood of finding the optimal $\lambda^{*}$ increases with the number of
    trials, which is not the case with Grid Search.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there are $K$ real-valued hyper-parameters, and $N$ total trials, grid search
    would pick $N^{\frac{1}{K}}$ for each hyper-parameter. However, not all hyper-parameters
    might be important. Random Search picks a random value for each hyper-parameter
    per trial. Hence, in cases with low effective dimensionality of the search space,
    Random Search performs better than Grid Search.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9032f6169c971fb06443a5b21db78c71.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Grid Search
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/324cf46f9f81b4d3202e401f115c8c94.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Random Search
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/320d5fe8693b77c73055ab4017fb0126.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Bayesian Optimization
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13\. Hyper-Parameter Search algorithms. Source: (Contributors to Wikimedia
    projects, [2021c](#bib.bib40))'
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Optimization (BO) based search (Močkus, [1975](#bib.bib112); Agnihotri
    and Batra, [2020](#bib.bib3)) is a *model-based* sequential approach where the
    search is guided by actively estimating the value of the objective function at
    different points in the search space, and then spawning trials based on the information
    gathered so far. The estimation of the objective function is done using a *surrogate
    function* that starts off with a prior estimate. The trials are created using
    an *acquisition function* which picks the next trial using the surrogate function,
    the likelihood of improving on the optimum so far, whether to explore / exploit
    etc. As the trials complete, both these functions will refine their estimates.
    Since the method keeps an internal model of how the objective function looks and
    plans the next trials based on that knowledge, it is model-based. Also, since
    the selection of trials depends on the results of the past trials, this method
    is sequential. BO improves over Random Search in that the search is guided rather
    than random, thus fewer trials are required to reach the optimum. However, it
    also makes the search sequential (though it is possible to run multiple trials
    in parallel, overall it will lead to some wasted trials).
  prefs: []
  type: TYPE_NORMAL
- en: One of the strategies to save training resources with the above search algorithms
    is the Early Stopping of trials that are not promising. Google’s Vizier (Golovin
    et al., [2017](#bib.bib62)) uses Median Stopping Rule for early stopping, where
    a trial is terminated if it’s performance at a time step $t$ is below the the
    median performance of all trials run till that point of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other algorithms for HPO include:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Population Based Training (PBT) (Jaderberg et al., [2017](#bib.bib84)): This
    method is similar to evolutionary approaches like genetic algorithms, where a
    fixed number of trials (referred to as the population) are spawned and trained
    to convergence. Each trial starts with a random set of hyper-parameters, and trained
    to a pre-determined number of steps. At this point, all trials are paused, and
    every trial’s weights and parameters might be replaced by the weights and parameters
    from the ‘best’ trial in the population so far. This is the *exploitation* part
    of the search. For *exploration*, these hyper-parameters are perturbed from their
    original values. This process repeats till convergence. It combines both the search
    and training in a fixed number of trials that run in parallel. It also only works
    with adaptive hyper-parameters like learning rate, weight-decay, etc. but cannot
    be used where hyper-parameters change the model structure. Note that the criteria
    for picking the ‘best’ trial does not have to be differentiable.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Multi-Armed Bandit Algorithms: Methods like Successive Halving (SHA) (Jamieson
    and Talwalkar, [2016](#bib.bib85)) and Hyper-Band (Li et al., [2017](#bib.bib102))
    are similar to random search, but they allocate more resources to the trials which
    are performing well. Both these methods need the user to specify the total computational
    budget $B$ for the search (can be the total number of epochs of training, for
    instance). They then spawn and train a fixed number of trials with randomly sampled
    hyper-parameters while allocating the training budget. Once the budget is exhausted,
    the worse performing fraction ($\frac{\eta-1}{\eta}$) of the trials are eliminated,
    and the remaining trials’ new budget is multiplied by $\eta$. In the case of SHA,
    $\eta$ is 2, so the bottom $\frac{1}{2}$ of the trials are dropped, and the training
    budget for the remaining trials is doubled. For Hyper-Band $\eta$ is 3 or 4\.
    Hyper-Band differs from SHA in that the user does not need to specify the maximum
    number of parallel trials, which introduces a trade-off between the total budget
    and the per-trial allocation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'HPO Toolkits: There are several software toolkits that incorporate HPO algorithms
    as well as an easy to use interface (UI, as well as a way to specify the hyper-parameters
    and their ranges). Vizier (Golovin et al., [2017](#bib.bib62)) (an internal Google
    tool, also available via Google Cloud for blackbox tuning). Amazon offers Sagemaker
    (Perrone et al., [2020](#bib.bib123)) which is functionally similar and can also
    be accessed as an AWS service. NNI (Research, [2019](#bib.bib132)), Tune (Liaw
    et al., [2018](#bib.bib104)), Advisor (Chen, [2021](#bib.bib32)) are other open-source
    HPO software packages that can be used locally.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Neural Architecture Search (NAS)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Neural Architecture Search can be thought of an extension of Hyper-Parameter
    Optimization wherein we are searching for parameters that change the network architecture
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'We find that there is consensus in the literature (Elsken et al., [2019](#bib.bib54))
    around categorizing NAS as a system comprising of the following parts:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Search Space: These are the operations that are allowed in the graph (Convolution
    ($1\times 1,3\times 3,5\times 5$), Fully Connected, Pooling, etc.), as well as
    the semantics of how these operations and their outputs connect to other parts
    of the network.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Search Algorithm & State: This is the algorithm that controls the architecture
    search itself. Typically the standard algorithms that apply in HPO (Grid Search,
    Random Search, Bayesian Optimization, Evolutionary Algorithms), can be used for
    NAS as well. However, using Reinforcement Learning (RL) (Zoph and Le, [2016](#bib.bib169)),
    and Gradient Descent (Liu et al., [2018a](#bib.bib106)) are popular alternatives
    too.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Evaluation Strategy: This defines how we evaluate a model for fitness. It can
    simply be a conventional metric like validation loss, accuracy, etc. Or it can
    also be a compound metric, as in the case of MNasNet (Tan et al., [2019](#bib.bib148))
    which creates a single custom metric based on accuracy as well as latency.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2f90de7a225110a02b92535c81cb47e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14\. Neural Architecture Search: The controller can be thought of as
    a unit that encodes the search space, the search algorithm itself, and the state
    it maintains (typically the model that helps generate the candidates). The algorithm
    generates candidate models in the search space $S$, and receives an evaluation
    feedback. This feedback is used to update the state, and generate better candidate
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The user is supposed to either explicitly or implicitly encode the search space.
    Together with the search algorithm, we can view this as a ‘controller’ which generates
    sample candidate networks (Refer to Figure [14](#S3.F14 "Figure 14 ‣ 3.3.2\. Neural
    Architecture Search (NAS) ‣ 3.3\. Automation ‣ 3\. Landscape of Efficient Deep
    Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller,
    Faster, and Better")). The evaluation stage will then train and evaluate these
    candidates for fitness. This fitness value is then passed as feedback to the search
    algorithm, which will use it for generating better candidates. While the implementation
    of each of these blocks vary, this structure is common across the seminal work
    in this area.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zoph et. al’s paper from 2016 (Zoph and Le, [2016](#bib.bib169)), demonstrated
    that end-to-end neural network architectures can be generated using Reinforcement
    Learning. In this case, the controller is a Recurrent Neural Network, which generates
    the architectural hyper-parameters of a feed-forward network one layer at a time,
    for example, number of filters, stride, filter size, etc. They also support adding
    skip connections (refer Figure [15](#S3.F15 "Figure 15 ‣ 3.3.2\. Neural Architecture
    Search (NAS) ‣ 3.3\. Automation ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")).
    The network semantics are baked into the controller, so generating a network that
    behaves differently requires changing the controller. Also, training the controller
    itself is expensive (taking 22,400 GPU hours (Zoph et al., [2018](#bib.bib170))),
    since the entire candidate network has to be trained from scratch for a single
    gradient update to happen. In a follow up paper (Zoph et al., [2018](#bib.bib170)),
    they come up with a refined search space where instead of searching for the end-to-end
    architecture, they search for *cells*: A ‘Normal Cell’ that takes in an input,
    processes it, and returns an output of the same spatial dimensions. And a ‘Reduction
    Cell’ that process its input, and returns an output whose spatial dimensions are
    scaled down by a factor of 2\. Each cell is a combination of $B$ blocks. The controller’s
    RNN generates one block at a time, where it picks outputs of two blocks in the
    past, the respective operations to apply on them, and how to combine them into
    a single output. The Normal and Reduction cells are stacked in alternating fashion
    ($N$ Normal cells followed by 1 Reduction cell, where $N$ is tunable) to construct
    an end-to-end network for CIFAR-10 and ImageNet. Learning these cells individually
    rather than learning the entire network seems to improve the search time by 7$\times$,
    when compared to the end-to-end network search in (Zoph and Le, [2016](#bib.bib169)),
    while beating the state-of-the-art in CIFAR-10 at that time.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a6c2fa92561ad575a124d509c7cd571.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15\. A NASNet controller generating the architecture, recursively making
    one decision at a time and generating a single block in the image (making a total
    of 5 decisions). Source: (Zoph et al., [2018](#bib.bib170)).'
  prefs: []
  type: TYPE_NORMAL
- en: Other approaches such as evolutionary techniques (Real et al., [2019](#bib.bib131)),
    differentiable architecture search (Liu et al., [2018a](#bib.bib106)), progressive
    search (Liu et al., [2018c](#bib.bib105)), parameter sharing (Pham et al., [2018](#bib.bib124)),
    etc. try to reduce the cost of architecture search (in some cases reducing the
    compute cost to a couple of GPU days instead of thousands of GPU days). These
    are covered in detail in (Elsken et al., [2019](#bib.bib54)).
  prefs: []
  type: TYPE_NORMAL
- en: 'While most of the early papers focused on finding the architectures that performed
    best on quality metrics like accuracy, unconstrained by the footprint metrics.
    However, when focusing on efficiency, we are often interested in specific tradeoffs
    between quality and footprint. Architecture Search can help with multi-objective
    searches that optimize for both quality and footprint. MNasNet (Tan et al., [2019](#bib.bib148))
    is one such work. It incorporates the model’s latency on the target device into
    the objective function directly, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $\small\underset{m}{\operatorname{maximize}}\quad ACC(m)\times\left[\frac{LAT(m)}{T}\right]^{w}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Where $m$ is the candidate model, $ACC$ is the accuracy metric, and $LAT$ is
    the latency of the given model on the desired device. $T$ is the target latency.
    $w$ is recommended to be $-0.07$. FBNet (Wu et al., [2019](#bib.bib161)) uses
    a similar approach with a compound reward function that has a weighted combination
    of the loss value on the validation set and the latency. However instead of measuring
    the latency of the candidate model on device, they use a pre-computed lookup table
    to approximate the latency to speed up the search process. They achieve networks
    that are upto $2.4\times$ smaller and $1.5\times$ faster than MobileNet, while
    finishing the search in 216 GPU Hours. Other works such as MONAS (Hsu et al.,
    [2018](#bib.bib80)) use Reinforcement Learning to incorporate power consumption
    into the reward function along with hard constraints on the number of MAC operations
    in the model, and discover pareto-frontiers under the given constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussion: Automation plays a critical role in model efficiency. Hyper-Parameter
    Optimization (HPO) is now a natural step in training models and can extract significant
    quality improvements, while minimizing human involvement. In case the cost HPO
    becomes large, algorithms like Bayesian Optimization, Hyper-Band etc. with early
    stopping techniques can be used. HPO is also available in ready-to-use software
    packages like Tune (Liaw et al., [2018](#bib.bib104)), Vizier via Google Cloud
    (Golovin et al., [2017](#bib.bib62)), NNI (Research, [2019](#bib.bib132)), etc.
    Similarly, recent advances in Neural Architecture Search (NAS) also make it feasible
    to construct architectures in a learned manner, while having constraints on both
    quality and footprint (Tan et al., [2019](#bib.bib148)). Assuming several hundred
    GPU hours worth of compute required for the NAS run to finish, and an approx cost
    of $3 GPU / hour on leading cloud computing services, this makes using NAS methods
    financially feasible and not similar in cost to manual experimentation with model
    architecture when optimizing for multiple objectives.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Efficient Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another common theme for tackling efficiency problems is to go back to the drawing
    board, and design layers and models that are efficient by design to replace the
    baseline. They are typically designed with some insight which might lead to a
    design that is better in general, or it might be better suited for the specific
    task. In this section, we lay out an examples of such efficient layers and models
    to illustrate this idea.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1\. Vision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the classical example of efficient layers in the Vision domain are the
    Convolutional layers, which improved over Fully Connected (FC) layers in Vision
    models. FC layers suffer from two primary issues:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: FC layers ignore the spatial information of the input pixels. Intuitively, it
    is hard to build an understanding of the given input by looking at individual
    pixel values in isolation. They also ignore the spatial locality in nearby regions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Secondly, using FC layers also leads to an explosion in the number of parameters
    when working with even moderately sized inputs. A $100\times 100$ RGB image with
    3 channels, would lead to each neuron in the first layer having $3\times 10^{4}$
    connections. This makes the network susceptible to overfitting also.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Convolutional layers avoid this by learning ‘filters’, where each filter is
    a 3D weight matrix of a fixed size ($3\times 3$, $5\times 5$, etc.), with the
    third dimension being the same as the number of channels in the input. Each filter
    is convolved over the input to generate a feature map for that given filter. These
    filters learn to detect specific features, and convolving them with a particular
    input patch results in a single scalar value that is higher if the feature is
    present in that input patch.
  prefs: []
  type: TYPE_NORMAL
- en: These learned features are simpler in lower layers (such as edges (horizontal,
    vertical, diagonal, etc.)), and more complex in subsequent layers (texture, shapes,
    etc.). This happens because the subsequent layers use the feature maps generated
    by previous layers, and each pixel in the input feature map of the $i$-th layer,
    depends on the past $i-1$ layers. This increases the *receptive field* of the
    said pixel as $i$ increases, progressively increasing the complexity of the features
    that can be encoded in a filter.
  prefs: []
  type: TYPE_NORMAL
- en: The core idea behind the efficiency of Convolutional Layers is that the same
    filter is used everywhere in the image, regardless of where the filter is applied.
    Hence, enforcing spatial invariance while sharing the parameters. Going back to
    the example of a $100\times 100$ RGB image with 3 channels, a $5\times 5$ filter
    would imply a total of $75$ ($5\times 5\times 3$) parameters. Each layer can learn
    multiple unique filters, and still be within a very reasonable parameter budget.
    This also has a regularizing effect, wherein a dramatically reduced number of
    parameters allow for easier optimization, and reducing the likelihood of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Layers are usually coupled with Pooling Layers, which allow dimensionality
    reduction by subsampling the input (aggregating a sliding 2-D window of pixels,
    using functions like max, avg, etc.). Pooling would lead to smaller feature maps
    for the next layer to process, which makes it faster to process. LeNet5 (Lecun
    et al., [1998](#bib.bib98)) was the first Convolutional Network which included
    convolutional layers, pooling, etc. Subsequently, many iterations of these networks
    have been proposed with various improvements. AlexNet (Krizhevsky et al., [2012](#bib.bib93)),
    Inception (Szegedy et al., [2015](#bib.bib147)), ResNet (He et al., [2016](#bib.bib74)),
    etc. have all made significant improvements over time on known image classification
    benchmarks using Convolutional Layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depth-Separable Convolutional Layers: In the convolution operation, each filter
    is used to convolve over the two spatial dimensions and the third channel dimension.
    As a result, the size of each filter is $s_{x}\times s_{y}\times$ input_channels,
    where $s_{x}$ and $s_{y}$ are typically equal. This is done for each filter, resulting
    in the convolution operation happening both spatially in the $x$ and $y$ dimensions,
    and depthwise in the $z$ dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c2bbc39b0e4bc690ded3bb25e4211b40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16\. Depth-Separable Convolution. Source: (Tsang, [2019](#bib.bib152)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Depth-separable convolution breaks this into two steps (Refer to Figure [16](#S3.F16
    "Figure 16 ‣ 3.4.1\. Vision ‣ 3.4\. Efficient Architectures ‣ 3\. Landscape of
    Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better")):'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Doing a point-wise convolution with $1\times 1$ filters, such that the resulting
    feature map now has a depth of output_channels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Doing a spatial convolution with $s_{x}\times s_{y}$ filters in the $x$ and
    $y$ dimensions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These two operations stacked together (without any intermediate non-linear activation)
    results in an output of the same shape as a regular convolution, with much fewer
    parameters ($1\times 1\times$input_channels$\times$ output_channels$)+(s_{x}\times
    s_{y}\times$ output_channels$)$, v/s $s_{x}\times s_{y}\times$ input_channels
    $\times$ output_channels for the regular convolution). Similarly there is an order
    of magnitude less computation since the point-wise convolution is much cheaper
    for convolving with each input channel depth-wise (for more calculations refer
    to (Sandler et al., [2018](#bib.bib134))). The Xception model architecture (Chollet,
    [2017](#bib.bib35)) demonstrated that using depth-wise separable convolutions
    in the Inception architecture, allowed reaching convergence sooner in terms of
    steps and a higher accuracy on the ImageNet dataset while keeping the number of
    parameters the same.
  prefs: []
  type: TYPE_NORMAL
- en: The MobileNet model architecture (Sandler et al., [2018](#bib.bib134)) which
    was designed for mobile and embedded devices, also uses the depth-wise separable
    layers instead of the regular convolutional layers. This helps them reduce the
    number of parameters as well as the number of multiply-add operations by $7-10\times$
    and allows deployment on Mobile for Computer Vision tasks. Users can expect a
    latency between 10-100ms depending on the model. MobileNet also provides a knob
    via the depth-multiplier for scaling the network to allow the user to trade-off
    between accuracy and latency.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2\. Natural Language Understanding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Attention Mechanism & Transformer Family: One of the issues plaguing classical
    Sequence-to-Sequence (Seq2Seq) models for solving tasks such as Machine Translation
    (MT), was that of the information-bottleneck. Seq2Seq models typically have one
    or more encoder layers which encode the given input sequence ($\mathbf{x}=(x_{1},x_{2},...,x_{T})$)
    into a fixed length vector(s) (also referred to as the context, $\mathbf{c}$),
    and one or more decoder layers which generate another sequence using this context.
    In the case of MT, the input sequence can be a sentence in the source language,
    and the output sequence can be the sentence in the target language.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in classical Seq2Seq models such as (Sutskever et al., [2014](#bib.bib146))
    the decoder layers could only see the hidden state of the final encoder step ($c=h_{T}$).
    This is a *bottleneck* because the encoder block has to squash all the information
    about the sequence in a single context vector for all the decoding steps, and
    the decoder block has to somehow infer the entire encoded sequence from it (Refer
    to Figure [17](#S3.F17 "Figure 17 ‣ 3.4.2\. Natural Language Understanding ‣ 3.4\.
    Efficient Architectures ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")).
    It is possible to increase the size of the context vector, but it would lead to
    an increase in the hidden state of all the intermediate steps, and make the model
    larger and slower.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c07896369b09a5d2acadddef0edfce3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17\. Information Bottleneck in a Seq2Seq model for translating from English
    to Hindi. The context vector $c$ that the decoder has access to is fixed, and
    is typically the last hidden state ($h_{T}$).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad6d03cd0582600920ae7aaefc36c76e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18\. Attention module learning a weighted context vector for each output
    token from the hidden states. Source: (Bahdanau et al., [2014](#bib.bib22)).'
  prefs: []
  type: TYPE_NORMAL
- en: The Attention mechanism was introduced in Bahdanau et al. (Bahdanau et al.,
    [2014](#bib.bib22)) to be able to create a custom context vector for each output
    token, by allowing all hidden states to be visible to the decoder and then creating
    a weighted context vector, based on the output token’s alignment with each input
    token. Essentially, the new weighted context vector is $c_{i}=\sum_{j}^{T}\alpha_{ij}.h_{j}$,
    where $\alpha_{ij}$ is the learned alignment (attention weight) between the decoder
    hidden state $s_{i-1}$ and the hidden state for the $j$-th token ($h_{j}$). $\alpha_{ij}$
    could be viewed as how much attention should the $i$-th input token be given when
    processing the $j$-th input token. This model is generalized in some cases by
    having explicit Query ($Q$), Key ($K$), and Value ($V$) vectors. Where we seek
    to learn the attention weight distribution ($\mathbf{\alpha}$) between $Q$ and
    $K$, and use it to compute the weighted context vector ($\mathbf{c}$) over $V$.
    In the above encoder-decoder architecture, $Q$ is the decoder hidden state $s_{i-1}$,
    and $K=V$ is the encoder hidden state $h_{j}$. Attention has been used to solve
    a variety of NLU tasks (MT, Question Answering, Text Classification, Sentiment
    Analysis), as well as Vision, Multi-Modal Tasks etc. (Chaudhari et al., [2019](#bib.bib30)).
    We refer the reader to (Chaudhari et al., [2019](#bib.bib30)) for further details
    on the taxonomy of attention models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1ae24c1cc1da5c3815470e059f74cbbe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19\. Transformer with its Encoder and Decoder blocks. Source: (Alammar,
    [2021](#bib.bib4)).'
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer architecture (Vaswani et al., [2017](#bib.bib156)) was proposed
    in 2017, which introduced using Self-Attention layers for both the Encoder and
    the Decoder. They demonstrated that Attention layers could be used to replace
    traditional RNN based Seq2Seq models. The Self-Attention layer the query, key,
    and value vectors are all derived from the same sequence by using different projection
    matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Attention also allows parallelizing the process of deriving relationships
    between the tokens in the input sequences. RNNs inherently force the process to
    occur one step at a time, i.e., learning long range dependencies is $O(n)$, where
    $n$ is the number of tokens. With Self-Attention, all tokens are processed together
    and pairwise relationships can be learnt in $O(1)$ (Vaswani et al., [2017](#bib.bib156)).
    This makes it easier to leverage optimized training devices like GPUs and TPUs.
    The authors reported up to $300\times$ less training FLOPs as required to converge
    to a similar quality when compared to other recurrent and convolutional models.
    Tay et al. (Tay et al., [2020](#bib.bib149)) discuss the computation and memory
    efficiency of several Transformer variants and their underlying self-attention
    mechanisms in detail.
  prefs: []
  type: TYPE_NORMAL
- en: As introduced earlier, the BERT model architecture (Devlin et al., [2018](#bib.bib48))
    beat the state-of-the-art in several NLU benchmarks. BERT is a stack of Transformer
    encoder layers that are pre-trained using a bi-directional masked language model
    training objective. It can also be used as a general purpose encoder which can
    then be used for other tasks. Other similar models like the GPT family (Brown
    et al., [2020](#bib.bib27)) have also been used for solving many NLU tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Random Projection Layers & Models Pre-trained token representations such as
    word2vec (Mikolov et al., [2017](#bib.bib111)), GLoVE (Pennington et al., [2014](#bib.bib122)),
    etc. are common for NLU tasks. However, since they require a $d$-dimensional vector
    for storing each token, the total size consumed by the table quickly grows very
    large if the vocabulary size $V$ is substantial ($O(V.d)$).
  prefs: []
  type: TYPE_NORMAL
- en: If model size is a constraint for deployment, we can either rely on compression
    techniques (as illustrated earlier) to help with Embedding Table compression,
    or evaluate layers and models that can work around the need for embedding tables.
    Random Projection based methods (Ravi, [2017](#bib.bib129); Ravi and Kozareva,
    [2018](#bib.bib130); Kaliamoorthi et al., [2019](#bib.bib88); Kaliamoorthi et al.,
    [2021](#bib.bib89)) are one such family of models that do so. They propose replacing
    the embedding table and lookup by mapping the input feature $x$ (unicode token
    / word token, etc.), into a lower dimensional space. This is done using the random
    projection operator $\mathbb{P}$, such that $\mathbb{P}(x)\in\{0,1\}^{T.r}$, which
    can be decomposed into $T$ individual projection operations each generating an
    $r$-bit representation ($\mathbb{P}(x)=[\mathbb{P}_{1}(x),...,\mathbb{P}_{T}(x)]$,
    where $\mathbb{P}_{i}(x)\in{0,1}^{r}$). $T$ and $r$ can be manually chosen.
  prefs: []
  type: TYPE_NORMAL
- en: Each random projection operation $\mathbb{P}_{i}$ is implemented using Locality
    Sensitive Hashing (LSH) (Charikar, [2002](#bib.bib29); Ravi and Kozareva, [2018](#bib.bib130)),
    each using a different hash function (via different seeds). For theoretical guarantees
    about the Random Projection operation, refer to (Charikar, [2002](#bib.bib29)),
    which demonstrates that the operation preserves the similarity between two points
    in the lower-dimensional space it maps these points to (this is crucial for the
    model to be learn the semantics about the inputs). If this relationship holds
    in the lower-dimensional space, the projection operation can be used to learn
    discriminative features for the given input. The core-benefit of the projection
    operation when compared to embedding tables is $O(T)$ space required instead of
    $O(V.d)$ ($T$ seeds required for $T$ hash functions). On the other hand, random-projection
    computation is $O(T)$ too v/s $O(1)$ for embedding table lookup. Hence, the projection
    layer is clearly useful when model size is the primary focus of optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Across the various papers in the projection model family, there are subtle differences
    in implementation (computing complex features before ((Ravi and Kozareva, [2018](#bib.bib130)))
    v/s after the projection operation ((Kaliamoorthi et al., [2019](#bib.bib88);
    Sankar et al., [2019](#bib.bib136))), generating a ternary representation instead
    of binary ((Kaliamoorthi et al., [2019](#bib.bib88); Kaliamoorthi et al., [2021](#bib.bib89))),
    applying complex layers and networks on top like Attention ((Kaliamoorthi et al.,
    [2019](#bib.bib88))), QRNN ((Kaliamoorthi et al., [2021](#bib.bib89)))), etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f9d2a0056c8713d046cc9a58b29ce7fb.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) PRADO Model. Source: (Kaliamoorthi et al., [2019](#bib.bib88)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2d5a716162080dfe45626291fc3dc837.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) PQRNN Model. Source: (Kaliamoorthi et al., [2021](#bib.bib89))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a832eab0563cbc1932b8146fe044ce99.png)'
  prefs: []
  type: TYPE_IMG
- en: '(c) Proformer Model. Source: (Sankar et al., [2020](#bib.bib137)).'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 20\. Collection of notable Random-Projection based models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the Projection-based models (refer to Figure [20](#S3.F20 "Figure 20
    ‣ 3.4.2\. Natural Language Understanding ‣ 3.4\. Efficient Architectures ‣ 3\.
    Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better")) have demonstrated impressive
    results on NLU tasks. PRADO ((Kaliamoorthi et al., [2019](#bib.bib88))) generates
    n-gram features from the projected inputs, followed by having a Multi-Headed Attention
    layer on top. It achieved accuracies comparable to standard LSTM models, while
    being  $100\times$ smaller, and taking 20-40 ms for inference on a Nexus 5X device.
    PQRNN (Kaliamoorthi et al., [2021](#bib.bib89)), another Projection-based model
    that additionally uses a fast RNN implementation (QRNN) (Bradbury et al., [2016](#bib.bib26))
    on top of the projected features. They report outperforming LSTMs while being
    $140\times$ smaller, and achieving $97.1\%$ of the quality of a BERT-like model
    while being $350\times$ smaller.'
  prefs: []
  type: TYPE_NORMAL
- en: Proformer (Sankar et al., [2020](#bib.bib137)) introduces a Local Projected
    Attention (LPA) Layer, which combines the Projection operation with localized
    attention. They demonstrate reaching $\approx$ 97.2% BERT-base’s performance while
    occupying only 13% of BERT-base’s memory. ProFormer also had 14.4 million parameters,
    compared to 110 million parameters of BERT-base.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Infrastructure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to be able to train and run inference efficiently, there has to be
    a robust software and hardware infrastructure foundation. In this section we go
    over both these aspects. Refer to Figure [21](#S3.F21 "Figure 21 ‣ 3.5\. Infrastructure
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") for a mental model
    of the software and hardware infrastructure, and how they interact with each other.
    In this section we provide a non-exhaustive but comprehensive survey of leading
    software and hardware infrastructure components that are critical to model efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/26638b7b8bbe071083117579c80d76d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 21\. A visualization of the hardware and software infrastructure with
    emphasis on efficiency. On the left hand side is the model-training phase, which
    generates a trained model checkpoint. This model is then used on the inference
    side, which could either be server-side (conventional machines in cloud or on-prem),
    or on-device (mobile phones, IoT, edge devices, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1\. Tensorflow Ecosystem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tensorflow (TF) (Abadi et al., [2016](#bib.bib2); Authors, [2021g](#bib.bib15))
    is a popular machine learning framework, that has been used in production by many
    large enterprises. It has some of the most extensive software support for model
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensorflow Lite for On-Device Usecases: Tensorflow Lite (TFLite) (Authors,
    [2021i](#bib.bib17)) is a collection of tools and libraries designed for inference
    in low-resource environments. At a high-level we can break down the TFLite project
    into two core parts:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interpreter and Op Kernels: TFLite provides an interpreter for running specialized
    TFLite models, along with implementations of common neural net operations (Fully
    Connected, Convolution, Max Pooling, ReLu, Softmax, etc. each of which as an *Op*).
    The implementation of such an operation is known as an *Op Kernel*. Both the interpreter
    and Op Kernels are primarily optimized for inference on ARM-based processors as
    of the time of writing this paper. They can also leverage smartphone DSPs such
    as Qualcomm’s Hexagon (Authors, [2021l](#bib.bib20)) for faster execution. The
    interpreter also allows the user to set multiple threads for execution.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Converter: The TFLite converter as the name suggests is useful for converting
    the given TF model into a single flatbuffer file for inference by the interpreter.
    Apart from the conversion itself, it handles a lot of internal details like getting
    a graph ready for quantized inference, fusing operations, adding other metadata
    to the model, etc. With respect to quantization, it also allows post-training
    quantization as mentioned earlier with an optional representative dataset to improve
    accuracy.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Other Tools for On-Device Inference: TF Micro (Warden and Situnayake, [2019](#bib.bib160))
    goes further, and consists of a slimmed down interpreter, and a smaller set of
    ops for inference on very low resource microcontrollers. TF Model Optimization
    toolkit (Authors, [2020](#bib.bib13)) is a Tensorflow library for applying common
    compression techniques like quantization, pruning, clustering etc. TensorflowJS
    (TF.JS) is a library within the TF ecosystem that can be used to train and run
    neural networks within the browser or using Node.js (Node.js Authors, [2021](#bib.bib114)).
    These models can also accelerated through GPUs via the WebGL interface (Contributors
    to Wikimedia projects, [2021f](#bib.bib43)). It supports both, importing models
    trained in TF, as well as creating new models from scratch in TF.JS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'XLA for Server-Side Acceleration: Typically a TF model graph is executed by
    TF’s executor process and it uses standard optimized kernels for running it on
    CPU, GPU, etc. XLA (Authors, [2021j](#bib.bib18)) is a graph compiler that can
    optimize linear algebra computations in a model, by generating new kernels that
    are customized for the graph. These kernels are optimized for the model graph
    in question. For example, certain operations which can be fused together are combined
    in a single composite op. This avoids having to do multiple costly writes to RAM,
    when the operands can directly be operated on while they are still in cheaper
    caches. Kanwar et al. (Kanwar et al., [2021](#bib.bib90)) report a 7$\times$ increase
    in training throughput, and 5$\times$ increase in the maximum batch size that
    can be used for BERT training. This allows training a BERT model for $32 on Google
    Cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2\. PyTorch Ecosystem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PyTorch (Paszke et al., [2019](#bib.bib120)) is another popular machine-learning
    platform actively used by both academia and industry. It is often compared with
    Tensorflow in terms of usability and features.
  prefs: []
  type: TYPE_NORMAL
- en: 'On-Device Usecases: PyTorch also has a light-weight interpreter that enables
    running PyTorch models on Mobile (Authors, [2021c](#bib.bib10)), with native runtimes
    for Android and iOS. This is analogous to the TFLite interpreter and runtime as
    introduced earlier. Similar to TFLite, PyTorch offers post-training quantization
    (Authors, [2021d](#bib.bib11)), and other graph optimization steps such as constant
    folding, fusing certain operations together, putting the channels last (NHWC)
    format for optimizing convolutional layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'General Model Optimization: PyTorch also offers the Just-in-Time (JIT) compilation
    facility (Authors, [2021e](#bib.bib12)), which might seem similar to Tensorflow’s
    XLA, but is actually a mechanism for generating a serializable intermediate representation
    (high-level IR, per (Li, [2020](#bib.bib103))) of the model from the code in TorchScript
    (Authors, [2021e](#bib.bib12)), which is a subset of Python. TorchScript adds
    constraints on the code that it can convert, such as type-checks, which allows
    it to sidestep some pitfalls of typical Python programming, while being Python
    compatible. It allows creating a bridge between the flexible PyTorch code for
    research and development, to a representation that can be deployed for inference
    in production. For example, exporting to TorchScript is a requirement to run on
    mobile devices (Authors, [2021c](#bib.bib10)). This representation is analogous
    to the static inference mode graphs generated by TensorFlow. The alternative for
    XLA in the PyTorch world seem to be the Glow (Rotem et al., [2018](#bib.bib133))
    and TensorComprehension (Vasilache et al., [2018](#bib.bib155)) compilers. They
    help in generating the lower-level intermediate representation that is derived
    from the higher-level IR (TorchScript, TF Graph). These low-level deep learning
    compilers are compared in detail in (Li, [2020](#bib.bib103)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch offers a model tuning guide (Authors, [2021b](#bib.bib9)), which details
    various options that ML practitioners have at their disposal. Some of the core
    ideas in there are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turn on mixed-precision training (Authors, [2021a](#bib.bib8)) when using NVIDIA
    GPUs. This is described further in detail in the GPU sub-section in 3.5.4.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fusion of pointwise-operations (add, subtract, multiply, divide, etc.) using
    PyTorch JIT. Even though this should happen automatically, but adding the torch.jit.script
    decorator to methods which are completely composed of pointwise operations can
    force the TorchScript compiler to fuse them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling buffer checkpointing allows keeping the outputs of only certain layers
    in memory, and computing the rest during the backward pass. This specifically
    helps with cheap to compute layers with large outputs like activations. A reduced
    memory usage can be exchanged for a larger batch size which improves utilization
    of the training platform (CPU, GPU, TPU, etc.).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling device-specific optimizations, such as the cuDNN library, and Mixed
    Precision Training with NVIDIA GPUs (explained in the GPU subsection).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train with Distributed Data Parallel Training, which is suitable when there
    is a large amount of data and multiple GPUs are available for training. Each GPU
    gets its own copy of the model and optimizer, and operates on its own subset of
    the data. Each replicas gradients are periodically accumulated and then averaged.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.5.3\. Hardware-Optimized Libraries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can further extract efficiency by optimizing for the hardware the neural
    networks run on. A prime deployment target is ARM’s Cortex-family of processors.
    Cortex supports SIMD (Single-Instruction Multiple Data) instructions via the Neon
    (Ltd., [2021](#bib.bib109)) architecture extension. SIMD instructions are useful
    for operating upon registers with vectors of data, which are essential for speeding
    up linear algebra operations through vectorization of these operations. QNNPACK
    (Dukhan et al., [2020](#bib.bib52)) and XNNPACK (Authors, [2021k](#bib.bib19))
    libraries are optimized for ARM Neon for mobile and embedded devices, and for
    x86 SSE2, AVX architectures, etc. QNNPACK supports several common ops in quantized
    inference mode for PyTorch. XNNPACK supports 32-bit floating point models and
    16-bit floating point for TFLite. If a certain operation isn’t supported in XNNPACK,
    it falls back to the default implementation in TFLite.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, there are other low-level libraries like Accelerate for iOS (Apple
    Authors, [2021](#bib.bib7)), and NNAPI for Android (Android Developers, [2021](#bib.bib5))
    that try to abstract away the hardware-level acceleration decision from higher
    level ML frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.4\. Hardware
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'GPU: Graphics Processing Units (GPUs) were originally designed for acclerating
    computer graphics, but began to be used for general-purpose usecases with the
    availability of the CUDA library (Contributors to Wikimedia projects, [2021b](#bib.bib39))
    in 2007, and libraries like like cuBLAS for speeding up linear algebra operations.
    In 2009, Raina et al. (Raina et al., [2009](#bib.bib126)) demonstrated that GPUs
    can be used to accelerate deep learning models. In 2012, following the AlexNet
    model’s (Krizhevsky et al., [2012](#bib.bib93)) substantial improvement over the
    next entrant in the ImageNet competition further standardized the use of GPUs
    for deep learning models. Since then Nvidia has released several iterations of
    its GPU microarchitectures with increasing focus on deep learning performance.
    It has also introduced Tensor Cores (NVIDIA, [2020b](#bib.bib116); Stosic, [2020](#bib.bib143))
    which are dedicated execution units in their GPUs, which are specialized for Deep
    Learning applications. TensorCores support training and inference in a range of
    precisions (fp32, TensorFloat32, fp16, bfloat16, int8, int4). As demonstrated
    earlier in quantization, switching to a lower precision is not always a significant
    trade-off, since the difference in model quality might often be minimal.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c17aa85e7f05d74e51e614c19bf6333e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22\. Reduced Precision Multiply-Accumulate (MAC) operation: An illustration
    of the $\mathbf{A}=(\mathbf{B}\times\mathbf{C})+\mathbf{D}$ operation. $\mathbf{B}$
    and $\mathbf{C}$ are in a reduced precision (fp16, bfloat16, TensorFloat32 etc.),
    while $\mathbf{A}$ and $\mathbf{D}$ are in fp32\. The speedup comes from doing
    the expensive matrix-multiplication with a reduced precision format.'
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Cores optimize the standard Multiply-and-Accumulate (MAC) operation (Contributors
    to Wikimedia projects, [2021d](#bib.bib41)), $\mathbf{A}=(\mathbf{B}\times\mathbf{C})+\mathbf{D}$.
    Where, $\mathbf{B}$ and $\mathbf{C}$ are in a reduced precision (fp16, bfloat16,
    TensorFloat32), while $\mathbf{A}$ and $\mathbf{D}$ are in fp32\. The core speedup
    comes from doing the expensive matrix-multiplication in a lower-precision. The
    result of the multiplication is in fp32, which can be relatively cheaply added
    with $\mathbf{D}.$ When training with reduced-precision, NVIDIA reports between
    1$\times$ to 15$\times$ training speedup depending on the model architecture and
    the GPU chosen (Stosic, [2020](#bib.bib143)). Tensor Cores in NVidia’s latest
    Ampere architecture GPUs also support faster inference with sparsity (specifically,
    structured sparsity in the ratio 2:4, where 2 elements out of a block of 4 elements
    are sparse) (NVIDIA, [2020a](#bib.bib115)). They demonstrate an up to 1.5$\times$
    speed up in inference time, and up to 1.8$\times$ speedup in individual layers.
    NVIDIA also offers the cuDNN libary (NVIDIA, [2020a](#bib.bib115)) that contains
    optimized versions of standard neural network operations such as fully-connected,
    convolution, batch-norm, activation, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f15154751507a175bc63217988432b1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23\. Common floating point format used in Training & Inference: fp32
    is the standard 32-bit floating point number from IEEE-754 standard (Wang and
    Kanwar, [2021](#bib.bib158)). One bit is allocated for storing the sign. The exponent
    controls the range of the floating point value that can be expressed with that
    format, and the mantissa controls the precision. Note that fp16 reduces the precision
    as well as range. The bfloat16 format is a reasonable compromise because it keeps
    the same range as fp32 while trading of precision to take up a total of 16 bits.
    NVidia GPUs also support Tensor Float 32 format that allocates 3 more bits to
    the mantissa than bfloat16 to achieve better precision. However, it takes up a
    total of 19 bits which does not make it a trivially portable format.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TPU: TPUs are proprietary application-specific integrated circuits (ASICs)
    that Google has designed to accelerate deep learning applications with Tensorflow.
    Because they are not general purpose devices, they need not cater for any non-ML
    applications (which most GPUs have had to), hence they are finely tuned for parallelizing
    and accelerating linear algebra operations. The first iteration of the TPU was
    designed for inference with 8-bit integers, and was being used in Google for a
    year prior to their announcement in 2016 (Jouppi et al., [2017](#bib.bib87)).
    Subsequent iterations of the TPU architectures enabled both training and inference
    with TPUs in floating point too. Google also opened up access to these TPUs via
    their Google Cloud service in 2018 (Google, [2021a](#bib.bib63)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9c7047448530aba2f3e565dd6bf4f3cf.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) A Systolic Array Cell implementing a Multiply-Accumulate (MAC) operation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/237c6a26043b4f432471d9bdf1452fe8.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) 4x4 Matrix Multiplication using Systolic Array
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 24\. Systolic Arrays in TPUs: Figure (a) shows a Systolic Array implementing
    a MAC operation, where the variables $A$ and $B$ are received by the cell, and
    $C$ is the resident memory. $A$ is passed to the horizontally adjacent cell on
    the right, and $B$ is passed to the vertically adjacent cell below on the next
    clock tick. Figure (b) demonstrates how two 4$\times$4 matrices are multiplied
    using Systolic Arrays which is a mesh of cells constructed in Figure (a). The
    $i$-th row of array is fed the $i$-th column of $A$ (preceded by $i-1$ 0s, which
    act as a delay). Similarly, the $i$-th column of the array is fed the $i$-th column
    of $B$ (preceded by $i-1$ 0s). The corresponding $a_{ij}$ and $b_{jk}$ are passed
    to the neighboring cells on the next clock tick.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The core architecture of the TPU chips leverages the Systolic Array design
    (Kung and Leiserson, [1980](#bib.bib95); Kung, [1982](#bib.bib96)) (refer to Figure
    [24](#S3.F24 "Figure 24 ‣ 3.5.4\. Hardware ‣ 3.5\. Infrastructure ‣ 3\. Landscape
    of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep
    Learning Models Smaller, Faster, and Better")), where a large computation is split
    across a mesh-like topology, where each cell computes a partial result and passes
    it on to the next cell in the order, every clock-step (in a rhythmic manner analogous
    to the systolic cardiac rhythm). Since there is no need to access registers for
    the intermediate results, once the required data is fetched the computation is
    not memory bound. Each TPU chip has two Tensor Cores (not to be confused with
    NVidia’s Tensor Cores), each of which has a mesh of systolic arrays. There are
    4 inter-connected TPU chips on a single TPU board. To further scale training and
    inference, a larger number of TPU boards can be connected in a mesh topology to
    form a ’pod’. As per publicly released numbers, each TPU chip (v3) can achieve
    420 teraflops, and a TPU pod can reach 100+ petaflops (Sato, [2021](#bib.bib138)).'
  prefs: []
  type: TYPE_NORMAL
- en: TPUs have been used inside Google for applications like training models for
    Google Search, general purpose BERT models (Devlin et al., [2018](#bib.bib48)),
    for applications like DeepMind’s world beating AlphaGo and AlphaZero models (Schrittwieser
    et al., [2020](#bib.bib139)), and many other research applications (Tan et al.,
    [2019](#bib.bib148)). They have also set model training time records in the MLPerf
    benchmarks. Similar to the GPUs, TPUs support the bfloat16 data-type (Wang and
    Kanwar, [2021](#bib.bib158)) which is a reduced-precision alternative to training
    in full floating point 32-bit precision. XLA support allows transparently switching
    to bfloat16 without any model changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'EdgeTPU: EdgeTPU is a custom ASIC chip designed by Google for running inference
    on edge devices, with low power requirements (4 Tera Ops / sec (TOPS) using 2
    watts of power (Google, [2021c](#bib.bib65))). Like the TPU, it is specialized
    for accelerating linear algebra operations, but only for inference and with a
    much lower compute budget. It is further limited to only a subset of operations
    (Google, [2021d](#bib.bib66)), and works only with int8 quantized Tensorflow Lite
    models. Google releases the EdgeTPU using the Coral platform in various form-factors,
    ranging from a Raspberry-Pi like Dev Board to independent solderable modules (Google,
    [2021b](#bib.bib64)). It has also been released with the Pixel 4 smartphones as
    the Pixel Neural Core (Rakowski, [2019](#bib.bib127)), for accelerating on-device
    deep learning applications. The EdgeTPU chip itself is smaller than a US penny,
    making it amenable for deployment in many kinds of IoT devices.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Jetson: Jetson (NVIDIA, [2021](#bib.bib117)) is a family of accelerators by
    Nvidia to enable deep learning applications for embedded and IoT devices. It comprises
    of the Nano, which is a low-powered "system on a module" (SoM) designed for lightweight
    deployments, as well as the more powerful Xavier and TX variants, which are based
    on the NVidia Volta and Pascal GPU architectures. As expected, the difference
    within the Jetson family is primarily the type and number of GPU cores on the
    accelerators. This makes the Nano suited for applications like home automation,
    and the rest for more compute intensive applications like industrial robotics.'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. A Practitioner’s Guide to Efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1850d890bed1a157a8ec2244253459f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25\. Trade off between Model Quality and Footprint: There exists a trade-off
    between model quality and model footprint. Model quality can be improved with
    techniques like distillation, data-augmentation, hyper-param tuning etc. Compression
    techniques can in turn help trade off some model quality for a better model footprint.
    Some / all of the improvement in footprint metrics can also be traded for better
    quality by simply adding more model capacity.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we presented a broad set of tools and techniques in the Efficient Deep
    Learning landscape. In this section, we present a practical guide for practitioners
    to use, and how these tools and techniques work with each other. As mentioned
    earlier, what we seek are *pareto-optimal* models, where we would like to achieve
    the best possible result in one dimension, while holding the other dimensions
    constant. Typically, one of these dimensions is Quality, and the other is Footprint.
    Quality related metrics could included Accuracy, F1, Precision, Recall, AUC, etc.
    While Footprint related metrics can include Model Size, Latency, RAM, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, there exists a trade-off between Quality and Footprint metrics.
    A higher-capacity / deeper model is more likely to achieve a better accuracy,
    but at the cost of model size, latency, etc. On the other hand a model with lesser
    capcity / shallower, while possibly suitable for deployment, is also likely to
    be worse in accuracy. As illustrated in Figure [25](#S4.F25 "Figure 25 ‣ 4\. A
    Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better"), we can traverse from a model
    with better quality metrics, and exchange some of the quality for better footprint
    by naively compressing the model / reducing the model capacity (Shrink). Similarly
    it is possible to naively improve quality by adding more capacity to the model
    (Grow). Growing can be addressed by the author of the model via appropriately
    increasing model capacity and tweaking other hyper-parameters to improve model
    quality. Shrinking can be achieved via Compression Techniques (Quantization, Pruning,
    Low-Rank Approximation, etc.), Efficient Layers & Models, Architecture Search
    via Automation, etc. In addition, we can also Improve the quality metrics, while
    keeping the footprint same through Learning Techniques (Distillation, Data Augmentation,
    Self-Supervised Tuning), Hyper-Parameter Tuning, etc. (See Table [4](#S4.T4 "Table
    4 ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") for more examples.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Grow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Model Capacity) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Shrink &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Footprint) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Improve &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Quality) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Add layers, width, etc. either manually or using width / depth / compound
    scaling multipliers |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Reduce layers, width, etc. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; either manually or using &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; width / depth / compound &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; scaling multipliers &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Manual Tuning (Architecture / &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hyper-Parameters / &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Features, etc.) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Compression Techniques: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Quantization, Pruning, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Low-Rank Factorization, etc. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Learning Techniques: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Data-Augmentation, Distillation, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Unsupervised Learning, etc. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Automation: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hyper-Param Optimization, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Architecture Search, etc. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Automation: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hyper-Param Optimization, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Architecture Search, etc. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Efficient Layers & Models: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Projection, PQRNN, (NLP), &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Separable Convolution (Vision), &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; etc. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Efficient Layers & Models: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Transformers (NLP), &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Vi-T (Vision), etc. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. Examples of techniques to use in the Grow, Shrink, and Improve phases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Combining these three phases, we propose two strategies towards achieving pareto-optimal
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Shrink-and-Improve for Footprint-Sensitive Models: If as a practitioner, you
    want to reduce your footprint, while keeping the quality the same, this could
    be a useful strategy for on-device deployments and server-side model optimization.
    Shrinking should ideally be minimally lossy in terms of quality (can be achieved
    via learned compression techniques, architecture search etc.), but in some cases
    even naively reducing capacity can also be compensated by the Improve phase. It
    is also possible to do the Improve phase before the Shrink phase.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Grow-Improve-and-Shrink for Quality-Sensitive Models: When you want to deploy
    models that have better quality while keeping the same footprint, it might make
    sense to follow this strategy. Here, the capacity is first added by growing the
    model as illustrated earlier. The model is then improved using via learning techniques,
    automation, etc. and then shrunk back either naively or in a learned manner. Alternatively,
    the model could be shrunk back either in a learned manner directly after growing
    the model too.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We consider both these strategies as a way of going from a potentially non pareto-optimal
    model to another one that lies on the pareto-frontier with the trade-off that
    is appropriate for the user. Each efficiency technique individually helps move
    us closer to that target model.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to demonstrate what we proposed above, we undertook the task of going
    through the exercise of making a given Deep Learning model efficient. Concretely,
    we had the following goals with this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Achieve a new pareto-frontier using the efficiency techniques. Hence, demonstrating
    that these techniques can be used in isolation as well as in combination with
    other techniques, in the real-world by ML Practitioners.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With various combinations of efficiency techniques and model scaling, demonstrate
    the tradeoffs for both ‘Shrink-and-Improve’, and ‘Grow-Improve-and-Shrink’ strategies
    for discovering and traversing the pareto-frontier. In other words, provide empirical
    evidence that it is possible for practitioners to either reduce model capacity
    to bring down the footprint (shrink) and then recover the model quality that they
    traded off (improve), or increase the model capacity to improve quality (growing)
    followed by model compression (shrinking) to improve model footprint.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We picked the problem of classifying images in the CIFAR-10 dataset (Krizhevsky
    et al., [2009](#bib.bib92)) on compute constrained devices such as smartphones,
    IoT devices etc. We designed a deep convolutional architecture where we could
    scale the model capacity up or down, by increasing or decreasing the ‘width multiplier’
    ($w$) value. In the implementation, $w$ scales the number of filters for the convolutional
    layers (except the first two). Hence, using different values of $w$ in $[0.1,0.25,0.5,0.75,1.0]$
    we obtain a family of models with different quality and footprint tradeoffs. We
    trained these models with some manual tuning to achieve a baseline of quality
    v/s footprint metrics. In this case, we measured quality through accuracy, and
    footprint through number of parameters, model size, and latency. In terms of techniques,
    we used Quantization for Shrinking, and Data Augmentation and Distillation for
    Improving. Many other techniques could be used to further drive the point home
    (Automation such as Hyper-Parameter Tuning, Efficient Layers such as Separable
    Convolutions), but were skipped to keep the interpretation of the results simpler.
    We used the Tensorflow-backed Keras APIs (Chollet, [2020](#bib.bib36)) for training,
    and the TFLite (Authors, [2021i](#bib.bib17)) framework for inference. The latencies
    were measured on three kinds of devices, low-end (Oppo A5), mid-end (Pixel 3XL),
    and high-end (Galaxy S10), in order of their increasing CPU compute power. The
    model size numbers reported are the sizes of the generated TFLite models, and
    the latency numbers are the average single-threaded CPU latency after warmup on
    the target device. The code for the experiments is available via an IPython notebook
    [here](https://github.com/reddragon/efficient-dl-survey-paper/blob/main/CIFAR_10_End_to_End.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide
    to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better") compiles the results for 6 width-multipliers in
    increasing order, ranging from $0.05$ to $1.0$. Between the smallest to the largest
    models, the number of params grows by $\approx 91.4\times$, and the model size
    grows by $\approx 80.2\times$. The latency numbers also grow between $3.5-10\times$
    based on the device. Within the same row, footprint metrics will not change since
    we are not changing the model architecture. In Table [5](#S4.T5 "Table 5 ‣ 4.1\.
    Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning:
    A Survey on Making Deep Learning Models Smaller, Faster, and Better") we purely
    work with techniques that will improve the model quality (Data Augmentation and
    Distillation). Table [6](#S4.T6 "Table 6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s
    Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better") reports the numbers for the Quantized versions
    of the corresponding models in Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣
    4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey on
    Making Deep Learning Models Smaller, Faster, and Better"). We use Quantization
    for the Shrink phase, to reduce model size by $\approx 4\times$, and reduce the
    average latency by $1.5-2.65\times$. Figures [26](#S4.F26 "Figure 26 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") and [27](#S4.F27
    "Figure 27 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    plot the notable results from Tables [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣
    4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey on
    Making Deep Learning Models Smaller, Faster, and Better") and [6](#S4.T6 "Table
    6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep
    Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Width Multiplier | # Params (K) | Model Size (KB) |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Accuracy &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (%) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Average Latency (ms) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | Augmentation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Augmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; + Distillation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Oppo &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; A5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pixel &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3XL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Galaxy &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S10 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.05 | 14.7 | 65.45 | 70.17 | 71.71 | 72.89 | 6.72 | 0.6 | 0.78 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 26 | 109.61 | 75.93 | 78.22 | 78.93 | 6.85 | 1.7 | 0.85 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.25 | 98.57 | 392.49 | 80.6 | 84.14 | 84.51 | 8.15 | 2.02 | 0.93 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 350.05 | 1374.11 | 83.04 | 87.47 | 88.03 | 11.46 | 2.8 | 1.33 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.75 | 764.87 | 2993.71 | 83.79 | 89.06 | 89.51 | 16.7 | 4.09 | 1.92 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1343.01 | 5251.34 | 84.42 | 89.41 | 89.92 | 24 | 5.99 | 2.68 |'
  prefs: []
  type: TYPE_TB
- en: Table 5\. Quality and Footprint metrics for Floating-Point models for the CIFAR-10
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| Width Multiplier | # Params (K) | Model Size (KB) |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Accuracy &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (%) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Average Latency (ms) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | Augmentation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Augmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; + Distillation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Oppo &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; A5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pixel &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3XL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Galaxy &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S10 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.05 | 14.7 | 26.87 | 69.9 | 71.72 | 72.7 | 4.06 | 0.49 | 0.43 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 26 | 38.55 | 75.98 | 78.19 | 78.55 | 4.5 | 1.25 | 0.47 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.25 | 98.57 | 111 | 80.76 | 83.98 | 84.18 | 4.52 | 1.31 | 0.48 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 350.05 | 359.31 | 83 | 87.32 | 87.86 | 6.32 | 1.73 | 0.58 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.75 | 764.87 | 767.09 | 83.6 | 88.57 | 89.29 | 8.53 | 2.36 | 0.77 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1343.01 | 1334.41 | 84.52 | 89.28 | 89.91 | 11.73 | 3.27 | 1.01 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6\. Quality and Footprint metrics for *Quantized* models for the CIFAR-10
    dataset. Each model is the quantized equivalent of the corresponding model in
    Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to
    Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7619ccc3cb5f7598bc8a351b1aa1f6af.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Number of Params v/s Accuracy
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/007f6cf4a51d3ec99de87dbb4ab1ae39.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Model Size v/s Accuracy
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 26\. Change in Accuracy with respect to Number of Params and Model Size.
    Each point on a curve is a model from Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") in figure (a) and
    from Table [6](#S4.T6 "Table 6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide
    to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better") in figure (b).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f55914e6f499a13e2c51e8c240c864db.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Low-End Device Latency
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/aa9f166d8e5ba3c57c2e4c2d8f3d5652.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Mid-Tier Device Latency
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/08c4064b319de0a020d218bf14c00f89.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) High-End Device Latency
  prefs: []
  type: TYPE_NORMAL
- en: Figure 27\. Average latency of models on different devices (low-, mid-, and
    high-end smartphones). The orange curve denotes the quantized models in addition
    to being trained with distillation and data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let us try to interpret the above data to validate if our strategies can be
    used practically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shrink-and-Improve for Footprint-Sensitive Models: Refer to Table [5](#S4.T5
    "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    and Figure [26](#S4.F26 "Figure 26 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s
    Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better"). If our goal was to deploy the model with
    Width Multiplier ($w$) = $1.0$ and accuracy $84.42\%$, but the bottleneck was
    the model size (5.25 MB) and latency on a low-end device (24 ms on Oppo A5). This
    is the classic case of the footprint metrics not meeting the bar, hence we could
    apply the Shrink-and-Improve strategy, by first naively scaling our model down
    to a Width Multiplier ($w$) of $0.25$. This smaller model when manually tuned,
    as seen in Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s
    Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better"), achieves an accuracy of $80.76\%$. However,
    when we use a combination of Data Augmentation & Distillation from a separately
    trained larger teacher model with an accuracy of $90.86\%$, the accuracy of the
    smaller model improves to $84.18\%$, very close to the target model that we want
    to deploy. The size of this smaller model is 392.49 KB, which is $13.8\times$
    smaller, and the latency is 8.15 ms, which is $2.94\times$ faster at a comparable
    accuracy. It is possible to further compress this model by using Quantization
    for some additional shrinking. The same smaller model ($w=0.25$) when Quantized
    in Table [6](#S4.T6 "Table 6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide
    to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better"), is 111 KB in size ($47.3\times$ smaller) and has
    a latency of 4.52 ms ($5.31\times$ faster), while retaining an accuracy of $84.18\%$.
    It is possible to do this for other pairs of points on the curves.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Grow-Improve-Shrink for Quality-Sensitive Models: Assuming our goal is to deploy
    a model that has footprint metrics comparable to the model with $w=0.25$ (392.49
    KB model size, 0.93 ms on a high-end Galaxy S10 device), but an accuracy better
    than the baseline $80.6\%$ (refer to Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better")). In this case, we
    can choose to first grow our model to $w=0.5$. This instantly blows up the model
    size to 1.37 MB ($3.49\times$ bigger), and latency to 1.33 ms ($1.43\times$ slower).
    However, we ignore that for a bit and improve our model’s quality to $88.03\%$
    with Data Augmentation & Distillation. Then using Quantization for shrinking (refer
    to Table [6](#S4.T6 "Table 6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide
    to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better")), we can get a model that is 359.31 KB in size (32
    KB smaller) and has a 0.58 ms latency on Galaxy S10 ($1.6\times$ faster), with
    an accuracy of $87.86\%$, an absolute 7.10% increase in accuracy while keeping
    the model size approximately same and making it $1.6\times$ faster. It is also
    possible to apply this strategy to other pairs of models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we’ve verified that the above two strategies can work both ways, whether
    your goal is to optimize for quality metrics or footprint metrics. We were also
    able to visually inspect through Figures [26](#S4.F26 "Figure 26 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") and [27](#S4.F27
    "Figure 27 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    that efficiency techniques can improve on the pareto frontiers constructed through
    manual tuning. To contain the scope of experimentation, we selected two sets of
    efficiency techniques (Compression Techniques (Quantization), and Learning Techniques
    (Data Augmentation & Distillation). Hence, it would be useful to explore other
    techniques as well such as Automation (for Hyper-Parameter Tuning to further improve
    on results), and Efficient Layers & Models (Separable Convolution as illustrated
    in MobileNet (Sandler et al., [2018](#bib.bib134)) could be used in place of larger
    convolutional layers). Finally, we would also like to emphasize paying attention
    to performance of Deep Learning models (optimized or not) on underrepresented
    classes and out-of-distribution data to ensure model fairness, since quality metrics
    alone might not be sufficient for discovering deeper issues with models (Hooker
    et al., [2020](#bib.bib77)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we started with demonstrating the rapid growth in Deep Learning
    models, and motivating the fact that someone training and deploying models today
    has to make either implicit or explicit decisions about efficiency. However, the
    landscape of model efficiency is vast.
  prefs: []
  type: TYPE_NORMAL
- en: To help with this, we laid out a mental model for the readers to wrap their
    heads around the multiple focus areas of model efficiency and optimization. The
    surveys of the core model optimization techniques give the reader an opportunity
    to understand the state-of-the-art, apply these techniques in the modelling process,
    and/or use them as a starting point for exploration. The infrastructure section
    also lays out the software libraries and hardware which make training and inference
    of efficient models possible.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we presented a section of explicit and actionable insights supplemented
    by code, for a practitioner to use as a guide in this space. This section will
    hopefully give concrete and actionable takeaways, as well as tradeoffs to think
    about when optimizing a model for training and deployment. To conclude, we feel
    that with this survey we have equipped the reader with the necessary understanding
    to break-down the steps required to go from a sub-optimal model to one that meets
    their constraints for both quality as well as footprint.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would like to thank the Learn2Compress team at Google Research for their
    support with this work. We would also like to thank Akanksha Saran and Aditya
    Sarawgi for their help with proof-reading and suggestions for improving the content.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abadi et al. (2016) Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
    Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
    Isard, et al. 2016. Tensorflow: A system for large-scale machine learning. In
    *12th $\{$USENIX$\}$ symposium on operating systems design and implementation
    ($\{$OSDI$\}$ 16)*. 265–283.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agnihotri and Batra (2020) Apoorv Agnihotri and Nipun Batra. 2020. Exploring
    bayesian optimization. *Distill* 5, 5 (2020), e26.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alammar (2021) Jay Alammar. 2021. The Illustrated Transformer. [https://jalammar.github.io/illustrated-transformer](https://jalammar.github.io/illustrated-transformer)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Android Developers (2021) Android Developers. 2021. Neural Networks API $|$
    Android NDK $|$ Android Developers. [https://developer.android.com/ndk/guides/neuralnetworks](https://developer.android.com/ndk/guides/neuralnetworks)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anwar et al. (2017) Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. 2017. Structured
    pruning of deep convolutional neural networks. *ACM Journal on Emerging Technologies
    in Computing Systems (JETC)* 13, 3 (2017), 1–18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apple Authors (2021) Apple Authors. 2021. Accelerate $|$ Apple Developer Documentation.
    [https://developer.apple.com/documentation/accelerate](https://developer.apple.com/documentation/accelerate)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021a) PyTorch Authors. 2021a. Automatic Mixed Precision examples —
    PyTorch 1.8.1 documentation. [https://pytorch.org/docs/stable/notes/amp_examples.html](https://pytorch.org/docs/stable/notes/amp_examples.html)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021b) PyTorch Authors. 2021b. Performance Tuning Guide — PyTorch Tutorials
    1.8.1+cu102 documentation. [https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021c) PyTorch Authors. 2021c. PyTorch Mobile. [https://pytorch.org/mobile/home](https://pytorch.org/mobile/home)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021d) PyTorch Authors. 2021d. Quantization Recipe — PyTorch Tutorials
    1.8.1+cu102 documentation. [https://pytorch.org/tutorials/recipes/quantization.html](https://pytorch.org/tutorials/recipes/quantization.html)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021e) PyTorch Authors. 2021e. torch.jit.script — PyTorch 1.8.1 documentation.
    [https://pytorch.org/docs/stable/generated/torch.jit.script.html](https://pytorch.org/docs/stable/generated/torch.jit.script.html)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2020) Tensorflow Authors. 2020. TensorFlow Model Optimization. [https://www.tensorflow.org/model_optimization](https://www.tensorflow.org/model_optimization)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021f) Tensorflow Authors. 2021f. Post-training quantization $|$ TensorFlow
    Lite. [https://www.tensorflow.org/lite/performance/post_training_quantization](https://www.tensorflow.org/lite/performance/post_training_quantization)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021g) Tensorflow Authors. 2021g. TensorFlow. [https://www.tensorflow.org](https://www.tensorflow.org)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021h) Tensorflow Authors. 2021h. TensorFlow Lite converter. [https://www.tensorflow.org/lite/convert](https://www.tensorflow.org/lite/convert)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021i) Tensorflow Authors. 2021i. TensorFlow Lite $|$ ML for Mobile
    and Edge Devices. [https://www.tensorflow.org/lite](https://www.tensorflow.org/lite)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Authors (2021j) Tensorflow Authors. 2021j. XLA: Optimizing Compiler for Machine
    Learning $|$ TensorFlow. [https://www.tensorflow.org/xla](https://www.tensorflow.org/xla)
    [Online; accessed 3\. Jun. 2021].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021k) XNNPACK Authors. 2021k. XNNPACK. [https://github.com/google/XNNPACK](https://github.com/google/XNNPACK)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021l) XNNPACK Authors. 2021l. XNNPACK backend for TensorFlow Lite.
    [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md/#sparse-inference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md/#sparse-inference)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baevski et al. (2020) Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and
    Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech
    representations. *arXiv preprint arXiv:2006.11477* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. (2014) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.
    Neural machine translation by jointly learning to align and translate. *arXiv
    preprint arXiv:1409.0473* (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. (2013) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013.
    Estimating or propagating gradients through stochastic neurons for conditional
    computation. *arXiv preprint arXiv:1308.3432* (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bergstra and Bengio (2012) James Bergstra and Yoshua Bengio. 2012. Random search
    for hyper-parameter optimization. *Journal of machine learning research* 13, 2
    (2012).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blum and Mitchell (1998) Avrim Blum and Tom Mitchell. 1998. Combining labeled
    and unlabeled data with co-training. In *Proceedings of the eleventh annual conference
    on Computational learning theory*. 92–100.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bradbury et al. (2016) James Bradbury, Stephen Merity, Caiming Xiong, and Richard
    Socher. 2016. Quasi-recurrent neural networks. *arXiv preprint arXiv:1611.01576*
    (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020. Language models are few-shot learners. *arXiv preprint
    arXiv:2005.14165* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Buciluǎ et al. (2006) Cristian Buciluǎ, Rich Caruana, and Alexandru Niculescu-Mizil.
    2006. Model compression. In *Proceedings of the 12th ACM SIGKDD international
    conference on Knowledge discovery and data mining*. 535–541.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Charikar (2002) Moses S Charikar. 2002. Similarity estimation techniques from
    rounding algorithms. In *Proceedings of the thiry-fourth annual ACM symposium
    on Theory of computing*. 380–388.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaudhari et al. (2019) Sneha Chaudhari, Varun Mithal, Gungor Polatkan, and
    Rohan Ramanath. 2019. An attentive survey of attention models. *arXiv preprint
    arXiv:1904.02874* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chawla et al. (2002) Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and
    W Philip Kegelmeyer. 2002. SMOTE: synthetic minority over-sampling technique.
    *Journal of artificial intelligence research* 16 (2002), 321–357.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen (2021) Dihao Chen. 2021. advisor. [https://github.com/tobegit3hub/advisor](https://github.com/tobegit3hub/advisor)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020a) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey
    Hinton. 2020a. A simple framework for contrastive learning of visual representations.
    In *International conference on machine learning*. PMLR, 1597–1607.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020b) Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi,
    and Geoffrey Hinton. 2020b. Big self-supervised models are strong semi-supervised
    learners. *arXiv preprint arXiv:2006.10029* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chollet (2017) François Chollet. 2017. Xception: Deep learning with depthwise
    separable convolutions. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*. 1251–1258.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chollet (2020) Francois Chollet. 2020. The Keras Blog. [https://blog.keras.io](https://blog.keras.io)
    [Online; accessed 4\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cireşan et al. (2011) Dan C Cireşan, Ueli Meier, Jonathan Masci, Luca M Gambardella,
    and Jürgen Schmidhuber. 2011. High-performance neural networks for visual object
    classification. *arXiv preprint arXiv:1102.0183* (2011).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributors to Wikimedia projects (2021a) Contributors to Wikimedia projects.
    2021a. AVX-512 - Wikipedia. [https://en.wikipedia.org/w/index.php?title=AVX-512&oldid=1025044245](https://en.wikipedia.org/w/index.php?title=AVX-512&oldid=1025044245)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributors to Wikimedia projects (2021b) Contributors to Wikimedia projects.
    2021b. CUDA - Wikipedia. [https://en.wikipedia.org/w/index.php?title=CUDA&oldid=1025500257](https://en.wikipedia.org/w/index.php?title=CUDA&oldid=1025500257)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributors to Wikimedia projects (2021c) Contributors to Wikimedia projects.
    2021c. Hyperparameter optimization - Wikipedia. [https://en.wikipedia.org/w/index.php?title=Hyperparameter_optimization&oldid=1022309479](https://en.wikipedia.org/w/index.php?title=Hyperparameter_optimization&oldid=1022309479)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributors to Wikimedia projects (2021d) Contributors to Wikimedia projects.
    2021d. Multiply–accumulate operation - Wikipedia. [https://en.wikipedia.org/w/index.php?title=Multiply-accumulate_operation&oldid=1026461481](https://en.wikipedia.org/w/index.php?title=Multiply-accumulate_operation&oldid=1026461481)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributors to Wikimedia projects (2021e) Contributors to Wikimedia projects.
    2021e. SSE4 - Wikipedia. [https://en.wikipedia.org/w/index.php?title=SSE4&oldid=1023092035](https://en.wikipedia.org/w/index.php?title=SSE4&oldid=1023092035)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributors to Wikimedia projects (2021f) Contributors to Wikimedia projects.
    2021f. WebGL - Wikipedia. [https://en.wikipedia.org/w/index.php?title=WebGL&oldid=1026775533](https://en.wikipedia.org/w/index.php?title=WebGL&oldid=1026775533)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cubuk et al. (2019) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan,
    and Quoc V Le. 2019. Autoaugment: Learning augmentation strategies from data.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    113–123.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cubuk et al. (2020) Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
    Le. 2020. Randaugment: Practical automated data augmentation with a reduced search
    space. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Workshops*. 702–703.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In *2009
    IEEE Conference on Computer Vision and Pattern Recognition*. 248–255. [https://doi.org/10.1109/CVPR.2009.5206848](https://doi.org/10.1109/CVPR.2009.5206848)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers and Zettlemoyer (2019) Tim Dettmers and Luke Zettlemoyer. 2019. Sparse
    networks from scratch: Faster training without losing performance. *arXiv preprint
    arXiv:1907.04840* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dietterich (2000) Thomas G Dietterich. 2000. Ensemble methods in machine learning.
    In *International workshop on multiple classifier systems*. Springer, 1–15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doersch et al. (2015) Carl Doersch, Abhinav Gupta, and Alexei A Efros. 2015.
    Unsupervised visual representation learning by context prediction. In *Proceedings
    of the IEEE international conference on computer vision*. 1422–1430.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2017) Xin Dong, Shangyu Chen, and Sinno Jialin Pan. 2017. Learning
    to prune deep neural networks via layer-wise optimal brain surgeon. *arXiv preprint
    arXiv:1705.07565* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dukhan et al. (2020) Marat Dukhan, Yiming Wu Wu, and Hao Lu. 2020. QNNPACK:
    Open source library for optimized mobile deep learning - Facebook Engineering.
    [https://engineering.fb.com/2018/10/29/ml-applications/qnnpack](https://engineering.fb.com/2018/10/29/ml-applications/qnnpack)
    [Online; accessed 3\. Jun. 2021].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elsen et al. (2020) Erich Elsen, Marat Dukhan, Trevor Gale, and Karen Simonyan.
    2020. Fast sparse convnets. In *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition*. 14629–14638.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elsken et al. (2019) Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, et al.
    2019. Neural architecture search: A survey. *J. Mach. Learn. Res.* 20, 55 (2019),
    1–21.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro,
    and Erich Elsen. 2020. Rigging the lottery: Making all tickets winners. In *International
    Conference on Machine Learning*. PMLR, 2943–2952.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020) Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave,
    Rémi Gribonval, Hervé Jégou, and Armand Joulin. 2020. Training with quantization
    noise for extreme model compression. *arXiv e-prints* (2020), arXiv–2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fawzi et al. (2016) Alhussein Fawzi, Horst Samulowitz, Deepak Turaga, and Pascal
    Frossard. 2016. Adaptive data augmentation for image classification. In *2016
    IEEE international conference on image processing (ICIP)*. Ieee, 3688–3692.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frankle and Carbin (2018) Jonathan Frankle and Michael Carbin. 2018. The lottery
    ticket hypothesis: Training pruned neural networks. *arXiv preprint arXiv:1803.03635*
    2 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gale et al. (2019) Trevor Gale, Erich Elsen, and Sara Hooker. 2019. The State
    of Sparsity in Deep Neural Networks. *CoRR* abs/1902.09574 (2019). arXiv:1902.09574
    [http://arxiv.org/abs/1902.09574](http://arxiv.org/abs/1902.09574)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gidaris et al. (2018) Spyros Gidaris, Praveer Singh, and Nikos Komodakis. 2018.
    Unsupervised representation learning by predicting image rotations. *arXiv preprint
    arXiv:1803.07728* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glass (2012) James Glass. 2012. Towards unsupervised speech processing. In *2012
    11th International Conference on Information Science, Signal Processing and their
    Applications (ISSPA)*. IEEE, 1–4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Golovin et al. (2017) Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg
    Kochanski, John Karro, and D Sculley. 2017. Google vizier: A service for black-box
    optimization. In *Proceedings of the 23rd ACM SIGKDD international conference
    on knowledge discovery and data mining*. 1487–1495.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google (2021a) Google. 2021a. Cloud TPU $|$ Google Cloud. [https://cloud.google.com/tpu](https://cloud.google.com/tpu)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google (2021b) Google. 2021b. Coral. [https://coral.ai](https://coral.ai) [Online;
    accessed 4\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google (2021c) Google. 2021c. Edge TPU performance benchmarks $|$ Coral. [https://coral.ai/docs/edgetpu/benchmarks](https://coral.ai/docs/edgetpu/benchmarks)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google (2021d) Google. 2021d. TensorFlow models on the Edge TPU $|$ Coral. [https://coral.ai/docs/edgetpu/models-intro/#supported-operations](https://coral.ai/docs/edgetpu/models-intro/#supported-operations)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: google research (2021) google research. 2021. google-research. [https://github.com/google-research/google-research/tree/master/fastconvnets](https://github.com/google-research/google-research/tree/master/fastconvnets)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gopalan et al. (2021) Arjun Gopalan, Da-Cheng Juan, Cesar Ilharco Magalhaes,
    Chun-Sung Ferng, Allan Heydon, Chun-Ta Lu, Philip Pham, George Yu, Yicheng Fan,
    and Yueqi Wang. 2021. Neural structured learning: training neural networks with
    structured signals. In *Proceedings of the 14th ACM International Conference on
    Web Search and Data Mining*. 1150–1153.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2015a) Song Han, Huizi Mao, and William J Dally. 2015a. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. *arXiv preprint arXiv:1510.00149* (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2015b) Song Han, Jeff Pool, John Tran, and William J Dally. 2015b.
    Learning both weights and connections for efficient neural networks. *arXiv preprint
    arXiv:1506.02626* (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hannun et al. (2014) Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro,
    Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam
    Coates, et al. 2014. Deep speech: Scaling up end-to-end speech recognition. *arXiv
    preprint arXiv:1412.5567* (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hansen and Salamon (1990) Lars Kai Hansen and Peter Salamon. 1990. Neural network
    ensembles. *IEEE transactions on pattern analysis and machine intelligence* 12,
    10 (1990), 993–1001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hassibi et al. (1993) Babak Hassibi, David G Stork, and Gregory J Wolff. 1993.
    Optimal brain surgeon and general network pruning. In *IEEE international conference
    on neural networks*. IEEE, 293–299.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2018) Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and
    Song Han. 2018. Amc: Automl for model compression and acceleration on mobile devices.
    In *Proceedings of the European Conference on Computer Vision (ECCV)*. 784–800.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531* (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hooker et al. (2020) Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio,
    and Emily Denton. 2020. Characterising bias in compressed models. *arXiv preprint
    arXiv:2010.03058* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Howard et al. (2019) Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen,
    Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan,
    et al. 2019. Searching for mobilenetv3\. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 1314–1324.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Howard and Ruder (2018) Jeremy Howard and Sebastian Ruder. 2018. Universal language
    model fine-tuning for text classification. *arXiv preprint arXiv:1801.06146* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hsu et al. (2018) Chi-Hung Hsu, Shu-Huan Chang, Jhao-Hong Liang, Hsin-Ping
    Chou, Chun-Hao Liu, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, and Da-Cheng
    Juan. 2018. Monas: Multi-objective neural architecture search using reinforcement
    learning. *arXiv preprint arXiv:1806.10332* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hubara et al. (2016) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv,
    and Yoshua Bengio. 2016. Binarized neural networks. In *Proceedings of the 30th
    International Conference on Neural Information Processing Systems*. 4114–4122.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inoue (2018) Hiroshi Inoue. 2018. Data augmentation by pairing samples for images
    classification. *arXiv preprint arXiv:1801.02929* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacob et al. (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.
    2704–2713.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaderberg et al. (2017) Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M
    Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen
    Simonyan, et al. 2017. Population based training of neural networks. *arXiv preprint
    arXiv:1711.09846* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jamieson and Talwalkar (2016) Kevin Jamieson and Ameet Talwalkar. 2016. Non-stochastic
    best arm identification and hyperparameter optimization. In *Artificial Intelligence
    and Statistics*. PMLR, 240–248.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeremy Jordan (2020) Jeremy Jordan. 2020. Setting the learning rate of your
    neural network. *Jeremy Jordan* (Aug 2020). [https://www.jeremyjordan.me/nn-learning-rate](https://www.jeremyjordan.me/nn-learning-rate)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jouppi et al. (2017) Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson,
    Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers,
    et al. 2017. In-datacenter performance analysis of a tensor processing unit. In
    *Proceedings of the 44th annual international symposium on computer architecture*.
    1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaliamoorthi et al. (2019) Prabhu Kaliamoorthi, Sujith Ravi, and Zornitsa Kozareva.
    2019. PRADO: Projection Attention Networks for Document Classification On-Device.
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*. 5012–5021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaliamoorthi et al. (2021) Prabhu Kaliamoorthi, Aditya Siddhant, Edward Li,
    and Melvin Johnson. 2021. Distilling Large Language Models into Tiny and Effective
    Students using pQRNN. *arXiv preprint arXiv:2101.08890* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kanwar et al. (2021) Pankaj Kanwar, Peter Brandt, and Zongwei Zhou. 2021. TensorFlow
    2 MLPerf submissions demonstrate best-in-class performance on Google Cloud. [https://blog.tensorflow.org/2020/07/tensorflow-2-mlperf-submissions.html](https://blog.tensorflow.org/2020/07/tensorflow-2-mlperf-submissions.html)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krishnamoorthi (2018) Raghuraman Krishnamoorthi. 2018. Quantizing deep convolutional
    networks for efficient inference: A whitepaper. *arXiv* (Jun 2018). arXiv:1806.08342
    [https://arxiv.org/abs/1806.08342v1](https://arxiv.org/abs/1806.08342v1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning
    multiple layers of features from tiny images. (2009).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. Imagenet classification with deep convolutional neural networks. *Advances
    in neural information processing systems* 25 (2012), 1097–1105.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krogh and Vedelsby (1994) Anders Krogh and Jesper Vedelsby. 1994. Neural network
    ensembles, cross validation and active learning. In *NIPS’94: Proceedings of the
    7th International Conference on Neural Information Processing Systems*. MIT Press,
    Cambridge, MA, USA, 231–238. [https://doi.org/10.5555/2998687.2998716](https://doi.org/10.5555/2998687.2998716)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kung and Leiserson (1980) HT Kung and CE Leiserson. 1980. Introduction to VLSI
    systems. *Mead, C. A_, and Conway, L.,(Eds), Addison-Wesley, Reading, MA* (1980),
    271–292.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kung (1982) Hsiang-Tsung Kung. 1982. Why systolic architectures? *IEEE computer*
    15, 1 (1982), 37–46.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LeCun (2018) Yann LeCun. 2018. Yann LeCun @EPFL - "Self-supervised learning:
    could machines learn like humans?". [https://www.youtube.com/watch?v=7I0Qt7GALVk&t=316s](https://www.youtube.com/watch?v=7I0Qt7GALVk&t=316s)
    [Online; accessed 3\. Jun. 2021].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lecun et al. (1998) Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner.
    1998. Gradient-based learning applied to document recognition. *Proc. IEEE* 86,
    11 (Nov 1998), 2278–2324. [https://doi.org/10.1109/5.726791](https://doi.org/10.1109/5.726791)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1990) Yann LeCun, John S Denker, and Sara A Solla. 1990. Optimal
    brain damage. In *Advances in neural information processing systems*. 598–605.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016) Fengfu Li, Bo Zhang, and Bin Liu. 2016. Ternary weight networks.
    *arXiv preprint arXiv:1605.04711* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016) Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter
    Graf. 2016. Pruning Filters for Efficient ConvNets. In *ICLR (Poster)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017) Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh,
    and Ameet Talwalkar. 2017. Hyperband: A novel bandit-based approach to hyperparameter
    optimization. *The Journal of Machine Learning Research* 18, 1 (2017), 6765–6816.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li (2020) Sharon Y. Li. 2020. Automating Data Augmentation: Practice, Theory
    and New Direction. *SAIL Blog* (Apr 2020). [http://ai.stanford.edu/blog/data-augmentation](http://ai.stanford.edu/blog/data-augmentation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liaw et al. (2018) Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz,
    Joseph E Gonzalez, and Ion Stoica. 2018. Tune: A research platform for distributed
    model selection and training. *arXiv preprint arXiv:1807.05118* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018c) Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens,
    Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.
    2018c. Progressive neural architecture search. In *Proceedings of the European
    conference on computer vision (ECCV)*. 19–34.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018a) Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018a. Darts:
    Differentiable architecture search. *arXiv preprint arXiv:1806.09055* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019) Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang,
    Kwang-Ting Cheng, and Jian Sun. 2019. Metapruning: Meta learning for automatic
    neural network channel pruning. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 3296–3305.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018b) Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor
    Darrell. 2018b. Rethinking the Value of Network Pruning. *CoRR* abs/1810.05270
    (2018). arXiv:1810.05270 [http://arxiv.org/abs/1810.05270](http://arxiv.org/abs/1810.05270)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ltd. (2021) Arm Ltd. 2021. SIMD ISAs $|$ Neon – Arm Developer. [https://developer.arm.com/architectures/instruction-sets/simd-isas/neon](https://developer.arm.com/architectures/instruction-sets/simd-isas/neon)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Menghani and Ravi (2019) Gaurav Menghani and Sujith Ravi. 2019. Learning from
    a Teacher using Unlabeled Data. *arXiv preprint arXiv:1911.05275* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2017) Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian
    Puhrsch, and Armand Joulin. 2017. Advances in pre-training distributed word representations.
    *arXiv preprint arXiv:1712.09405* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Močkus (1975) Jonas Močkus. 1975. On Bayesian methods for seeking the extremum.
    In *Optimization techniques IFIP technical conference*. Springer, 400–404.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Molchanov et al. (2016) Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
    and Jan Kautz. 2016. Pruning Convolutional Neural Networks for Resource Efficient
    Transfer Learning. *CoRR* abs/1611.06440 (2016). arXiv:1611.06440 [http://arxiv.org/abs/1611.06440](http://arxiv.org/abs/1611.06440)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node.js Authors (2021) Node.js Authors. 2021. Node.js. [https://nodejs.org/en](https://nodejs.org/en)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NVIDIA (2020a) NVIDIA. 2020a. GTC 2020: Accelerating Sparsity in the NVIDIA
    Ampere Architecture. [https://developer.nvidia.com/gtc/2020/video/s22085-vid](https://developer.nvidia.com/gtc/2020/video/s22085-vid)
    [Online; accessed 3\. Jun. 2021].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NVIDIA (2020b) NVIDIA. 2020b. Inside Volta: The World’s Most Advanced Data
    Center GPU $|$ NVIDIA Developer Blog. [https://developer.nvidia.com/blog/inside-volta](https://developer.nvidia.com/blog/inside-volta)
    [Online; accessed 3\. Jun. 2021].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA (2021) NVIDIA. 2021. NVIDIA Embedded Systems for Next-Gen Autonomous
    Machines. [https://www.nvidia.com/en-us/autonomous-machines/embedded-systems](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems)
    [Online; accessed 4\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Panigrahy (2021) Rina Panigrahy. 2021. Matrix Compression Operator. [https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html](https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html)
    [Online; accessed 5\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PapersWithCode.com (2021) PapersWithCode.com. 2021. Papers with Code - The latest
    in Machine Learning. [https://paperswithcode.com](https://paperswithcode.com)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning
    library. *arXiv preprint arXiv:1912.01703* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patrick et al. (2020) Mandela Patrick, Yuki M Asano, Polina Kuznetsova, Ruth
    Fong, João F Henriques, Geoffrey Zweig, and Andrea Vedaldi. 2020. Multi-modal
    self-supervision from generalized data transformations. *arXiv preprint arXiv:2003.04298*
    (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher D
    Manning. 2014. Glove: Global vectors for word representation. In *Proceedings
    of the 2014 conference on empirical methods in natural language processing (EMNLP)*.
    1532–1543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perrone et al. (2020) Valerio Perrone, Huibin Shen, Aida Zolic, Iaroslav Shcherbatyi,
    Amr Ahmed, Tanya Bansal, Michele Donini, Fela Winkelmolen, Rodolphe Jenatton,
    Jean Baptiste Faddoul, et al. 2020. Amazon SageMaker Automatic Model Tuning: Scalable
    Black-box Optimization. *arXiv preprint arXiv:2012.08489* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pham et al. (2018) Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean.
    2018. Efficient neural architecture search via parameters sharing. In *International
    Conference on Machine Learning*. PMLR, 4095–4104.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polino et al. (2018) Antonio Polino, Razvan Pascanu, and Dan Alistarh. 2018.
    Model compression via distillation and quantization. *arXiv preprint arXiv:1802.05668*
    (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raina et al. (2009) Rajat Raina, Anand Madhavan, and Andrew Y Ng. 2009. Large-scale
    deep unsupervised learning using graphics processors. In *Proceedings of the 26th
    annual international conference on machine learning*. 873–880.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rakowski (2019) Brian Rakowski. 2019. Pixel 4 is here to help. *Google* (Oct
    2019). [https://blog.google/products/pixel/pixel-4](https://blog.google/products/pixel/pixel-4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rastegari et al. (2016) Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
    and Ali Farhadi. 2016. Xnor-net: Imagenet classification using binary convolutional
    neural networks. In *European conference on computer vision*. Springer, 525–542.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ravi (2017) Sujith Ravi. 2017. Projectionnet: Learning efficient on-device
    deep networks using neural projections. *arXiv preprint arXiv:1708.00630* (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ravi and Kozareva (2018) Sujith Ravi and Zornitsa Kozareva. 2018. Self-governing
    neural networks for on-device short text classification. In *Proceedings of the
    2018 Conference on Empirical Methods in Natural Language Processing*. 887–893.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real et al. (2019) Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le.
    2019. Regularized evolution for image classifier architecture search. In *Proceedings
    of the aaai conference on artificial intelligence*, Vol. 33. 4780–4789.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Research (2019) Microsoft Research. 2019. Neural Network Intelligence - Microsoft
    Research. [https://www.microsoft.com/en-us/research/project/neural-network-intelligence](https://www.microsoft.com/en-us/research/project/neural-network-intelligence)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rotem et al. (2018) Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret Catron,
    Summer Deng, Roman Dzhabarov, Nick Gibson, James Hegeman, Meghan Lele, Roman Levenstein,
    et al. 2018. Glow: Graph lowering compiler techniques for neural networks. *arXiv
    preprint arXiv:1805.00907* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sandler et al. (2018) Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
    and Liang-Chieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    4510–4520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper
    and lighter. *arXiv preprint arXiv:1910.01108* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sankar et al. (2019) Chinnadhurai Sankar, Sujith Ravi, and Zornitsa Kozareva.
    2019. Transferable neural projection representations. *arXiv preprint arXiv:1906.01605*
    (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sankar et al. (2020) Chinnadhurai Sankar, Sujith Ravi, and Zornitsa Kozareva.
    2020. ProFormer: Towards On-Device LSH Projection Based Transformers. *arXiv preprint
    arXiv:2004.05801* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sato (2021) Kaz Sato. 2021. What makes TPUs fine-tuned for deep learning? $|$
    Google Cloud Blog. [https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning](https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schrittwieser et al. (2020) Julian Schrittwieser, Ioannis Antonoglou, Thomas
    Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart,
    Demis Hassabis, Thore Graepel, et al. 2020. Mastering atari, go, chess and shogi
    by planning with a learned model. *Nature* 588, 7839 (2020), 604–609.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.
    Edinburgh neural machine translation systems for wmt 16. *arXiv preprint arXiv:1606.02891*
    (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simard et al. (2003) Patrice Y Simard, David Steinkraus, John C Platt, et al.
    2003. Best practices for convolutional neural networks applied to visual document
    analysis.. In *Icdar*, Vol. 3\. Citeseer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman (2014) Karen Simonyan and Andrew Zisserman. 2014. Very
    deep convolutional networks for large-scale image recognition. *arXiv preprint
    arXiv:1409.1556* (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stosic (2020) Dusan Stosic. 2020. Training Neural Networks with Tensor Cores
    - Dusan Stosic, NVIDIA. [https://www.youtube.com/watch?v=jF4-_ZK_tyc](https://www.youtube.com/watch?v=jF4-_ZK_tyc)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2017) Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav
    Gupta. 2017. Revisiting unreasonable effectiveness of data in deep learning era.
    In *Proceedings of the IEEE international conference on computer vision*. 843–852.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2020) Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming
    Yang, and Denny Zhou. 2020. Mobilebert: a compact task-agnostic bert for resource-limited
    devices. *arXiv preprint arXiv:2004.02984* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
    Sequence to sequence learning with neural networks. *arXiv preprint arXiv:1409.3215*
    (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
    2015. Going deeper with convolutions. In *Proceedings of the IEEE conference on
    computer vision and pattern recognition*. 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. (2019) Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark
    Sandler, Andrew Howard, and Quoc V Le. 2019. Mnasnet: Platform-aware neural architecture
    search for mobile. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*. 2820–2828.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tay et al. (2020) Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
    2020. Efficient transformers: A survey. *arXiv preprint arXiv:2009.06732* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow (2019) TensorFlow. 2019. TensorFlow Model Optimization Toolkit —
    Post-Training Integer Quantization. *Medium* (Nov 2019). [https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow (2021) TensorFlow. 2021. Model optimization $|$ TensorFlow Lite.
    [https://www.tensorflow.org/lite/performance/model_optimization](https://www.tensorflow.org/lite/performance/model_optimization)
    [Online; accessed 3\. Jun. 2021].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tsang (2019) Sik-Ho Tsang. 2019. Review: Xception — With Depthwise Separable
    Convolution, Better Than Inception-v3 (Image Classification). *Medium* (Mar 2019).
    [https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568](https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Urban et al. (2016) Gregor Urban, Krzysztof J Geras, Samira Ebrahimi Kahou,
    Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose,
    and Matt Richardson. 2016. Do deep convolutional nets really need to be deep and
    convolutional? *arXiv preprint arXiv:1603.05691* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vanhoucke et al. (2011) Vincent Vanhoucke, Andrew Senior, and Mark Z Mao. 2011.
    Improving the speed of neural networks on CPUs. (2011).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vasilache et al. (2018) Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis,
    Priya Goyal, Zachary DeVito, William S Moses, Sven Verdoolaege, Andrew Adams,
    and Albert Cohen. 2018. Tensor comprehensions: Framework-agnostic high-performance
    machine learning abstractions. *arXiv preprint arXiv:1802.04730* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *arXiv preprint arXiv:1706.03762* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. 2020.
    Towards Accurate Post-training Network Quantization via Bit-Split and Stitching.
    In *International Conference on Machine Learning*. PMLR, 9847–9856. [http://proceedings.mlr.press/v119/wang20c.html](http://proceedings.mlr.press/v119/wang20c.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Kanwar (2021) Shibo Wang and Pankaj Kanwar. 2021. BFloat16: The secret
    to high performance on Cloud TPUs $|$ Google Cloud Blog. [https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)
    [Online; accessed 3\. Jun. 2021].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018.
    Switchout: an efficient data augmentation algorithm for neural machine translation.
    *arXiv preprint arXiv:1808.07512* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Warden and Situnayake (2019) Pete Warden and Daniel Situnayake. 2019. *Tinyml:
    Machine learning with tensorflow lite on arduino and ultra-low-power microcontrollers*.
    " O’Reilly Media, Inc.".'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019) Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei
    Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. 2019.
    Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture
    search. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*. 10734–10742.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2020) Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. 2020.
    Self-training with noisy student improves imagenet classification. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 10687–10698.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yalniz et al. (2019) I Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and
    Dhruv Mahajan. 2019. Billion-scale semi-supervised learning for image classification.
    *arXiv preprint arXiv:1905.00546* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018) Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai
    Chen, Mohammad Norouzi, and Quoc V Le. 2018. Qanet: Combining local convolution
    with global self-attention for reading comprehension. *arXiv preprint arXiv:1804.09541*
    (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu and Zhu (2020) Tong Yu and Hong Zhu. 2020. Hyper-parameter optimization:
    A review of algorithms and applications. *arXiv preprint arXiv:2003.05689* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zagoruyko and Komodakis (2016) Sergey Zagoruyko and Nikos Komodakis. 2016.
    Paying more attention to attention: Improving the performance of convolutional
    neural networks via attention transfer. *arXiv preprint arXiv:1612.03928* (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017) Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David
    Lopez-Paz. 2017. mixup: Beyond empirical risk minimization. *arXiv preprint arXiv:1710.09412*
    (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu and Gupta (2018) Michael Zhu and Suyog Gupta. 2018. To Prune, or Not to
    Prune: Exploring the Efficacy of Pruning for Model Compression. In *6th International
    Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
    30 - May 3, 2018, Workshop Track Proceedings*. OpenReview.net. [https://openreview.net/forum?id=Sy1iIDkPM](https://openreview.net/forum?id=Sy1iIDkPM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoph and Le (2016) Barret Zoph and Quoc V Le. 2016. Neural architecture search
    with reinforcement learning. *arXiv preprint arXiv:1611.01578* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoph et al. (2018) Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V
    Le. 2018. Learning transferable architectures for scalable image recognition.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    8697–8710.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
