- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:54:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:54:03
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2106.08962] Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2106.08962] 高效深度学习：关于使深度学习模型更小、更快、更好的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2106.08962](https://ar5iv.labs.arxiv.org/html/2106.08962)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2106.08962](https://ar5iv.labs.arxiv.org/html/2106.08962)
- en: 'Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster,
    and Better'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效深度学习：关于使深度学习模型更小、更快、更好的综述
- en: Gaurav Menghani [gmenghani@google.com](mailto:gmenghani@google.com) [0000-0003-2912-2522](https://orcid.org/0000-0003-2912-2522
    "ORCID identifier") Google ResearchMountain ViewCaliforniaUSA95054
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Gaurav Menghani [gmenghani@google.com](mailto:gmenghani@google.com) [0000-0003-2912-2522](https://orcid.org/0000-0003-2912-2522
    "ORCID identifier") Google ResearchMountain ViewCaliforniaUSA95054
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Deep Learning has revolutionized the fields of computer vision, natural language
    understanding, speech recognition, information retrieval and more. However, with
    the progressive improvements in deep learning models, their number of parameters,
    latency, resources required to train, etc. have all have increased significantly.
    Consequently, it has become important to pay attention to these footprint metrics
    of a model as well, not just its quality. We present and motivate the problem
    of efficiency in deep learning, followed by a thorough survey of the five core
    areas of model efficiency (spanning modeling techniques, infrastructure, and hardware)
    and the seminal work there. We also present an experiment-based guide along with
    code, for practitioners to optimize their model training and deployment. We believe
    this is the first comprehensive survey in the efficient deep learning space that
    covers the landscape of model efficiency from modeling techniques to hardware
    support. Our hope is that this survey would provide the reader with the mental
    model and the necessary understanding of the field to apply generic efficiency
    techniques to immediately get significant improvements, and also equip them with
    ideas for further research and experimentation to achieve additional gains.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已经彻底改变了计算机视觉、自然语言理解、语音识别、信息检索等领域。然而，随着深度学习模型的不断改进，它们的参数数量、延迟、训练所需资源等都显著增加。因此，关注模型的这些足迹指标变得同样重要，而不仅仅是模型的质量。我们提出并阐明了深度学习中效率的问题，接着对模型效率的五个核心领域（涵盖建模技术、基础设施和硬件）以及相关开创性工作进行了详细调查。我们还提供了一个基于实验的指南和代码，供从业者优化模型训练和部署。我们相信，这是首个全面调查高效深度学习领域的综述，涵盖了从建模技术到硬件支持的模型效率全景。我们希望这项综述能为读者提供心智模型和必要的领域理解，以便应用通用效率技术立即获得显著改进，并为进一步研究和实验提供想法，以实现更多的收益。
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Deep Learning with neural networks has been the dominant methodology of training
    new machine learning models for the past decade. Its rise to prominence is often
    attributed to the ImageNet competition (Deng et al., [2009](#bib.bib46)) in 2012\.
    That year, a University of Toronto team submitted a deep convolutional network
    (AlexNet (Krizhevsky et al., [2012](#bib.bib93)), named after the lead developer
    Alex Krizhevsky), performed 41% better than the next best submission. As a result
    of this trailblazing work, there was a race to create deeper networks with an
    ever increasing number of parameters and complexity. Several model architectures
    such as VGGNet (Simonyan and Zisserman, [2014](#bib.bib142)), Inception (Szegedy
    et al., [2015](#bib.bib147)), ResNet (He et al., [2016](#bib.bib74)) etc. successively
    beat previous records at ImageNet competitions in the subsequent years, while
    also increasing in their footprint (model size, latency, etc.)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习与神经网络已经成为过去十年训练新机器学习模型的主导方法。它的崛起通常归因于2012年的ImageNet竞赛（Deng et al., [2009](#bib.bib46)）。那一年，多伦多大学的一个团队提交了一个深度卷积网络（AlexNet
    (Krizhevsky et al., [2012](#bib.bib93)），以首席开发者Alex Krizhevsky命名），其表现比下一个最佳提交好41%。由于这项开创性工作，出现了创造更深网络、参数数量和复杂度不断增加的竞赛。诸如VGGNet
    (Simonyan and Zisserman, [2014](#bib.bib142))、Inception (Szegedy et al., [2015](#bib.bib147))、ResNet
    (He et al., [2016](#bib.bib74))等模型架构在随后的几年中相继打破了ImageNet竞赛的记录，同时其足迹（模型大小、延迟等）也在不断扩大。
- en: '![Refer to caption](img/5666049517ceb7f464c21a388a64e46b.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5666049517ceb7f464c21a388a64e46b.png)'
- en: (a) Computer Vision Models
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 计算机视觉模型
- en: '![Refer to caption](img/fb1cc800839b0f6963270ad17905fed0.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fb1cc800839b0f6963270ad17905fed0.png)'
- en: (b) Natural Language Models
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 自然语言模型
- en: Figure 1\. Growth in the number of parameters in Computer Vision models over
    time. (PapersWithCode.com, [2021](#bib.bib119))
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. 计算机视觉模型中参数数量随时间增长的情况。（PapersWithCode.com，[2021](#bib.bib119)）
- en: 'This effect has also been noted in natural language understanding (NLU), where
    the Transformer (Vaswani et al., [2017](#bib.bib156)) architecture based on primarily
    Attention layers, spurred the development of general purpose language encoders
    like BERT (Devlin et al., [2018](#bib.bib48)), GPT-3 (Brown et al., [2020](#bib.bib27)),
    etc. BERT specifically beat 11 NLU benchmarks when it was published. GPT-3 has
    also been used in several places in the industry via its API. The common aspect
    amongst these domains is the rapid growth in the model footprint (Refer to Figure
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Efficient Deep Learning: A Survey on
    Making Deep Learning Models Smaller, Faster, and Better")), and the cost associated
    with training and deploying them.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '这种现象在自然语言理解（NLU）中也有体现，其中基于主要关注层的Transformer（Vaswani等，[2017](#bib.bib156)）架构推动了通用语言编码器的开发，如BERT（Devlin等，[2018](#bib.bib48)）、GPT-3（Brown等，[2020](#bib.bib27)）等。BERT在发布时特别打破了11个NLU基准。GPT-3也通过其API在多个行业中得到了应用。这些领域的共同点在于模型足迹的快速增长（参见图
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Efficient Deep Learning: A Survey on
    Making Deep Learning Models Smaller, Faster, and Better)），以及与训练和部署相关的成本。'
- en: Since deep learning research has been focused on improving the state of the
    art, progressive improvements on benchmarks like image classification, text classification,
    etc. have been correlated with an increase in the network complexity, number of
    parameters, the amount of training resources required to train the network, prediction
    latency, etc. For instance, GPT-3 comprises of 175 billion parameters, and costs
    millions of dollars to train just one iteration ((Brown et al., [2020](#bib.bib27))).
    This excludes the cost of experimentation / trying combinations of different hyper-parameters,
    which is also computationally expensive.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习研究一直集中在提升技术水平上，对图像分类、文本分类等基准的渐进改进通常与网络复杂性、参数数量、训练所需资源、预测延迟等的增加相关。例如，GPT-3包含了1750亿个参数，训练一次迭代的成本达到数百万美元（（Brown等，[2020](#bib.bib27)））。这不包括实验费用/尝试不同超参数组合的成本，这也是计算上昂贵的。
- en: While these models perform well on the tasks they are trained on, they might
    not necessarily be efficient enough for direct deployment in the real world. A
    deep learning practitioner might face the following challenges when training or
    deploying a model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些模型在它们所训练的任务上表现良好，但它们可能并不一定足够高效，无法直接在现实世界中部署。当训练或部署模型时，深度学习从业者可能会面临以下挑战。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sustainable Server-Side Scaling: Training and deploying large deep learning
    models is costly. While training could be a one-time cost (or could be free if
    one is using a pre-trained model), deploying and letting inference run for over
    a long period of time could still turn out to be expensive in terms of consumption
    of server-side RAM, CPU, etc.. There is also a very real concern around the carbon
    footprint of datacenters even for organizations like Google, Facebook, Amazon,
    etc. which spend several billion dollars each per year in capital expenditure
    on their data-centers.'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可持续的服务器端扩展：训练和部署大型深度学习模型成本高昂。虽然训练可能是一次性成本（如果使用预训练模型则可能是免费的），但部署并长时间运行推理仍可能在服务器端RAM、CPU等方面变得昂贵。即便是像Google、Facebook、Amazon等组织，也对数据中心的碳足迹非常关注，这些组织每年在数据中心的资本支出上花费数十亿美元。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Enabling On-Device Deployment: Certain deep learning applications need to run
    realtime on IoT and smart devices (where the model inference happens directly
    on the device), for a multitude of reasons (privacy, connectivity, responsiveness).
    Thus, it becomes imperative to optimize the models for the target devices.'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 启用设备端部署：某些深度学习应用需要在物联网和智能设备上实时运行（模型推理直接在设备上进行），原因多种多样（隐私、连接性、响应速度）。因此，优化模型以适应目标设备变得至关重要。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Privacy & Data Sensitivity: Being able to use as little data as possible for
    training is critical when the user-data might be sensitive. Hence, efficiently
    training models with a fraction of the data means lesser data-collection required.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐私与数据敏感性：当用户数据可能是敏感时，尽可能少地使用数据进行训练至关重要。因此，高效地使用部分数据进行训练意味着需要收集的数据更少。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'New Applications: Certain new applications offer new constraints (around model
    quality or footprint) that existing off-the-shelf models might not be able to
    address.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新应用：某些新应用提出了新的约束（关于模型质量或足迹），现有的现成模型可能无法解决这些问题。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Explosion of Models: While a singular model might work well, training and/or
    deploying multiple models on the same infrastructure (colocation) for different
    applications might end up exhausting the available resources.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型爆炸：虽然单一模型可能表现良好，但在同一基础设施上（共置）训练和/或部署多个模型以满足不同应用的需求，可能会耗尽可用资源。
- en: 1.1\. Efficient Deep Learning
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 高效的深度学习
- en: 'The common theme around the above challenges is *efficiency*. We can break
    it down further as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 上述挑战的共同主题是*效率*。我们可以进一步将其分解如下：
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Inference Efficiency: This primarily deals with questions that someone deploying
    a model for inference (computing the model outputs for a given input), would ask.
    Is the model small? Is it fast, etc.? More concretely, how many parameters does
    the model have, what is the disk size, RAM consumption during inference, inference
    latency, etc.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理效率：这主要涉及到在部署模型时（计算模型对于给定输入的输出）可能会问的问题。模型是否较小？是否快速等？更具体地说，模型有多少个参数，磁盘大小是多少，推理期间的RAM消耗，推理延迟等。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Training Efficiency: This involves questions someone training a model would
    ask, such as How long does the model take to train? How many devices? Can the
    model fit in memory?, etc. It might also include questions like, how much data
    would the model need to achieve the desired performance on the given task?'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练效率：这涉及到训练模型时可能会问的问题，比如模型训练需要多长时间？需要多少设备？模型是否可以适应内存？等。它也可能包括类似的问题，比如模型需要多少数据才能在给定任务上达到所需的性能？
- en: If we were to be given two models, performing equally well on a given task,
    we might want to choose a model which does better in either one, or ideally both
    of the above aspects. If one were to be deploying a model on devices where inference
    is constrained (such as mobile and embedded devices), or expensive (cloud servers),
    it might be worth paying attention to inference efficiency. Similarly, if one
    is training a large model from scratch on either with limited or costly training
    resources, developing models that are designed for training efficiency would help.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有两个模型，在特定任务上表现同样出色，我们可能会选择在上述一个或两个方面表现更好的模型。如果在推理受限（如移动设备和嵌入式设备）或成本高昂（云服务器）的设备上部署模型，可能值得关注推理效率。类似地，如果在有限或昂贵的训练资源下从头开始训练一个大型模型，开发旨在提高训练效率的模型将有所帮助。
- en: '![Refer to caption](img/c8f0b15164247b11b0dd00dbb5d1c784.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c8f0b15164247b11b0dd00dbb5d1c784.png)'
- en: 'Figure 2\. Pareto Optimality: Green dots represent pareto-optimal models (together
    forming the pareto-frontier), where none of the other models (red dots) get better
    accuracy with the same inference latency, or the other way around.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 帕累托最优性：绿色点代表帕累托最优模型（共同形成帕累托前沿），在这些模型中，没有其他模型（红色点）能够在相同的推理延迟下获得更好的准确率，反之亦然。
- en: 'Regardless of what one might be optimizing for, we want to achieve *pareto-optimality*.
    This implies that any model that we choose is the best for the tradeoffs that
    we care about. As an example in Figure [2](#S1.F2 "Figure 2 ‣ 1.1\. Efficient
    Deep Learning ‣ 1\. Introduction ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better"), the green dots represent pareto-optimal
    models, where none of the other models (red dots) get better accuracy with the
    same inference latency, or the other way around. Together, the pareto-optimal
    models (green dots) form our *pareto-frontier*. The models in the pareto-frontier
    are by definition more efficient than the other models, since they perform the
    best for their given tradeoff. Hence, when we seek efficiency, we should be thinking
    about discovering and improving on the pareto-frontier.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '无论我们优化什么，我们都希望实现*帕累托最优性*。这意味着我们选择的任何模型都是在我们关心的权衡下表现最好的。例如，在图[2](#S1.F2 "Figure
    2 ‣ 1.1\. Efficient Deep Learning ‣ 1\. Introduction ‣ Efficient Deep Learning:
    A Survey on Making Deep Learning Models Smaller, Faster, and Better")中，绿色点代表帕累托最优模型，这些模型中没有其他模型（红色点）能在相同的推理延迟下获得更好的准确率，反之亦然。帕累托最优模型（绿色点）共同形成了我们的*帕累托前沿*。定义上，帕累托前沿中的模型比其他模型更高效，因为它们在给定的权衡下表现最佳。因此，当我们寻求效率时，我们应当考虑发现并改进帕累托前沿上的模型。'
- en: To achieve this goal, we propose turning towards a collection of algorithms,
    techniques, tools, and infrastructure that work together to allow users to train
    and deploy *pareto-optimal* models with respect to model quality and its footprint.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这个目标，我们建议转向一系列算法、技术、工具和基础设施，这些都协同工作，以便用户能够训练和部署与模型质量及其足迹相关的*帕累托最优*模型。
- en: 2\. A Mental Model
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 思维模型
- en: In this section we present the mental model to think about the collection of
    algorithms, techniques, and tools related to efficient deep learning. We propose
    to structure them in five major areas, with the first four focused on modeling,
    and the final one around infrastructure and tools.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一个思维模型，用于考虑与高效深度学习相关的算法、技术和工具的集合。我们建议将其结构化为五个主要领域，其中前四个集中在建模上，最后一个则涉及基础设施和工具。
- en: '![Refer to caption](img/fabdaeea4f34ffafb788f2dc3d56a587.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fabdaeea4f34ffafb788f2dc3d56a587.png)'
- en: Figure 3\. A mental model for thinking about algorithms, techniques, and tools
    related to efficiency in Deep Learning.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 一个关于算法、技术和与深度学习效率相关的工具的思维模型。
- en: (1)
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Compression Techniques: These are general techniques and algorithms that look
    at optimizing the model’s architecture, typically by compressing its layers. A
    classical example is quantization (Jacob et al., [2018](#bib.bib83)), which tries
    to compress the weight matrices of a layer, by reducing its precision (eg., from
    32-bit floating point values to 8-bit unsigned integers), with minimal loss in
    quality.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 压缩技术：这些是通用技术和算法，旨在优化模型的架构，通常通过压缩其层来实现。一个经典的例子是量化（Jacob 等，[2018](#bib.bib83)），它试图通过减少精度（例如，从32位浮点值到8位无符号整数）来压缩层的权重矩阵，且质量损失最小。
- en: (2)
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Learning Techniques: These are algorithms which focus on training the model
    differently (to make fewer prediction errors, require less data, converge faster,
    etc.). The improved quality can then be exchanged for a smaller footprint / a
    more efficient model by trimming the number of parameters if needed. An example
    of a learning technique is distillation (Hinton et al., [2015](#bib.bib76)), which
    allows improving the accuracy of a smaller model by learning to mimic a larger
    model.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习技术：这些算法专注于以不同的方式训练模型（例如减少预测错误、需要更少的数据、更快地收敛等）。通过修剪参数的数量，如果需要的话，可以将改进的质量转化为更小的足迹/更高效的模型。一个学习技术的例子是蒸馏（Hinton
    等，[2015](#bib.bib76)），它通过学习模拟更大模型来提高较小模型的准确性。
- en: (3)
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Automation: These are tools for improving the core metrics of the given model
    using automation. An example is hyper-parameter optimization (HPO) (Golovin et al.,
    [2017](#bib.bib62)) where optimizing the hyper-parameters helps increase the accuracy,
    which could then be then exchanged for a model with lesser parameters. Similarly,
    architecture search (Zoph and Le, [2016](#bib.bib169)) falls in this category
    too, where the architecture itself is tuned and the search helps find a model
    that optimizes both the loss / accuracy, and some other metric such as model latency,
    model size, etc.'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动化：这些是用于通过自动化提高给定模型核心指标的工具。一个例子是超参数优化（HPO）（Golovin 等，[2017](#bib.bib62)），通过优化超参数来提高准确性，然后可以将其转化为参数更少的模型。类似地，架构搜索（Zoph
    和 Le，[2016](#bib.bib169)）也属于这一类别，在这里，架构本身被调整，搜索帮助找到一个优化损失/准确性以及其他指标（如模型延迟、模型大小等）的模型。
- en: (4)
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: 'Efficient Architectures: These are fundamental blocks that were designed from
    scratch (convolutional layers, attention, etc.), that are a significant leap over
    the baseline methods used before them (fully connected layers, and RNNs respectively).
    As an example, convolutional layers introduced parameter sharing for use in image
    classification, which avoids having to learn separate weights for each input pixel,
    and also makes them robust to overfitting. Similarly, attention layers (Bahdanau
    et al., [2014](#bib.bib22)) solved the problem of Information Bottleneck in Seq2Seq
    models. These architectures can be used directly for efficiency gains.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高效架构：这些是从头设计的基本模块（卷积层、注意力机制等），相比之前使用的基线方法（全连接层和RNN）具有显著的进步。例如，卷积层引入了参数共享用于图像分类，避免了为每个输入像素学习单独的权重，并使其对过拟合具有鲁棒性。类似地，注意力层（Bahdanau
    等，[2014](#bib.bib22)）解决了Seq2Seq模型中的信息瓶颈问题。这些架构可以直接用于提高效率。
- en: (5)
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (5)
- en: 'Infrastructure: Finally, we also need a foundation of infrastructure and tools
    that help us build and leverage efficient models. This includes the model training
    framework, such as Tensorflow (Abadi et al., [2016](#bib.bib2)), PyTorch (Paszke
    et al., [2019](#bib.bib120)), etc. (along with the tools required specifically
    for deploying efficient models such as Tensorflow Lite (TFLite), PyTorch Mobile,
    etc.). We depend on the infrastructure and tooling to leverage gains from efficient
    models. For example, to get both size and latency improvements with quantized
    models, we need the inference platform to support common neural network layers
    in quantized mode.'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基础设施：最后，我们还需要一个基础设施和工具的基础，以帮助我们构建和利用高效的模型。这包括模型训练框架，如 Tensorflow (Abadi et al.,
    [2016](#bib.bib2))、PyTorch (Paszke et al., [2019](#bib.bib120)) 等（以及专门用于部署高效模型的工具，如
    Tensorflow Lite (TFLite)、PyTorch Mobile 等）。我们依赖基础设施和工具来利用高效模型的优势。例如，要在量化模型中获得大小和延迟的改进，我们需要推理平台支持量化模式下的常见神经网络层。
- en: We will survey each of these areas in depth in the following section.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的部分中深入探讨这些领域。
- en: 3\. Landscape of Efficient Deep Learning
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 高效深度学习的全景
- en: 3.1\. Compression Techniques
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 压缩技术
- en: Compression techniques as mentioned earlier, are usually generic techniques
    for achieving a more efficient representation of one or more layers in a neural
    network, with a possible quality trade off. The efficiency goal could be to optimize
    the model for one or more of the footprint metrics, such as model size, inference
    latency, training time required for convergence, etc. in exchange for as little
    quality loss as possible. In some cases if the model is over-parameterized, these
    techniques can improve model generalization.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，压缩技术通常是实现神经网络中一个或多个层更高效表示的通用技术，可能会有质量的权衡。效率目标可能是优化模型以满足一个或多个占用指标，例如模型大小、推理延迟、收敛所需的训练时间等，以换取尽可能少的质量损失。在某些情况下，如果模型过度参数化，这些技术可以提高模型的泛化能力。
- en: 3.1.1\. Pruning
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 剪枝
- en: Given a neural network $f(X,W)$, where $X$ is the input and $W$ is the set of
    parameters (or weights), pruning is a technique for coming up with a minimal subset
    $W^{\prime}$ such that the rest of the parameters of $W$ are pruned (or set to
    0), while ensuring that the quality of the model remains above the desired threshold.
    After pruning, we can say the network has been made *sparse*, where the sparsity
    can be quantified as the ratio of the number of parameters that were pruned to
    the number of parameters in the original network ($s=(1-\frac{|W^{\prime}|}{|W|})$).
    The higher the sparsity, the lesser the number of non-zero parameters in the pruned
    networks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个神经网络 $f(X,W)$，其中 $X$ 是输入，$W$ 是参数（或权重）集合，剪枝是一种技术，用于得到一个最小的子集 $W^{\prime}$，使得其余的参数
    $W$ 被剪枝（或设置为0），同时确保模型的质量保持在期望的阈值以上。剪枝后，我们可以说网络变得 *稀疏*，稀疏度可以量化为剪枝参数的数量与原网络中参数数量的比例
    ($s=(1-\frac{|W^{\prime}|}{|W|})$)。稀疏度越高，剪枝网络中的非零参数越少。
- en: '![Refer to caption](img/5a9065a48b4901e83bf765de85dcb91e.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5a9065a48b4901e83bf765de85dcb91e.png)'
- en: Figure 4\. A simplified illustration of pruning weights (connections) and neurons
    (nodes) in a neural network comprising of fully connected layers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 简化示意图，展示了在包含全连接层的神经网络中剪枝权重（连接）和神经元（节点）的过程。
- en: 'Some of the classical works in this area are Optimal Brain Damage (OBD) by
    LeCun et al. (LeCun et al., [1990](#bib.bib99)), and Optimal Brain Surgeon paper
    (OBD) by Hassibi et al. (Hassibi et al., [1993](#bib.bib73)). These methods usually
    take a network that has been pre-trained to a reasonable quality and then iteratively
    prune the parameters which have the lowest ‘saliency’ score, such that the impact
    on the validation loss is minimized. Once pruning concludes, the network is fine-tuned
    with the remaining parameters. This process is repeated a number of times until
    the desired number of original parameters are pruned (Algorithm [1](#algorithm1
    "In 3.1.1\. Pruning ‣ 3.1\. Compression Techniques ‣ 3\. Landscape of Efficient
    Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better")).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域的一些经典工作包括 LeCun 等人的 Optimal Brain Damage (OBD) (LeCun 等，[1990](#bib.bib99))
    和 Hassibi 等人的 Optimal Brain Surgeon 论文 (OBD) (Hassibi 等，[1993](#bib.bib73))。这些方法通常采用一个已经预训练到合理质量的网络，然后迭代剪枝具有最低“显著性”分数的参数，以尽量减少对验证损失的影响。剪枝完成后，网络会使用剩余的参数进行微调。这个过程会重复多次，直到剪除所需数量的原始参数
    (算法 [1](#algorithm1 "在 3.1.1\. 剪枝 ‣ 3.1\. 压缩技术 ‣ 3\. 高效深度学习的全景 ‣ 高效深度学习：关于如何让深度学习模型更小、更快、更好的一项调查"))。
- en: 'Data: Pre-trained dense network with weights $W$, inputs $X$, number of pruning
    rounds $N$, fraction of parameters to prune per round $p$.Result: Pruned network
    with weights $W^{\prime}$.1  $W^{\prime}\leftarrow W$;2  for *$i\leftarrow 1$
    to $N$* do3        $S\leftarrow\texttt{compute\_saliency\_scores}(W^{\prime})$;4      5      $W^{\prime}\leftarrow
    W^{\prime}-\texttt{select\_min\_k}\large(S,\frac{|W^{\prime}|}{p}\large)$;6        $W^{\prime}\leftarrow$fine_tune($X$,
    $W^{\prime}$)7 end forreturn $W^{\prime}$'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据：预训练的密集网络，权重 $W$，输入 $X$，剪枝轮次 $N$，每轮剪枝的参数比例 $p$。结果：剪枝后的网络，权重为 $W^{\prime}$。1
    $W^{\prime}\leftarrow W$; 2 对于 *$i\leftarrow 1$ 到 $N$* 执行 3 $S\leftarrow\texttt{compute\_saliency\_scores}(W^{\prime})$;
    4 5 $W^{\prime}\leftarrow W^{\prime}-\texttt{select\_min\_k}\large(S,\frac{|W^{\prime}|}{p}\large)$;
    6 $W^{\prime}\leftarrow$fine_tune($X$, $W^{\prime}$) 7 结束 返回 $W^{\prime}$
- en: Algorithm 1 Standard Network Pruning with Fine-Tuning
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 标准网络剪枝与微调
- en: OBD approximates the saliency score by using a second-derivative of the parameters
    ($\large\frac{\partial^{2}L}{\partial w_{i}^{2}}$), where $L$ is the loss function,
    and $w_{i}$ is the candidate parameter for removal. The intuition is that the
    higher this value for a given parameter, the larger the change in the loss function’s
    gradient if it were to be pruned.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: OBD 通过使用参数的二阶导数 ($\large\frac{\partial^{2}L}{\partial w_{i}^{2}}$) 来近似显著性分数，其中
    $L$ 是损失函数，$w_{i}$ 是待移除的候选参数。直观上，对于给定的参数，这个值越高，剪枝时损失函数梯度的变化就越大。
- en: For the purpose of speeding up the computation of the second-derivatives, OBD
    ignores cross-interaction between the weights ($\large\frac{\partial^{2}L}{\partial
    w_{i}\partial w_{j}}$), and hence computes only the diagonal elements of the Hessian
    matrix. Otherwise, computing the full Hessian matrix is unwieldy for even a reasonable
    number of weights (with $n=10^{4}$, the size of the matrix is $10^{4}\times 10^{4}=10^{8}$).
    In terms of results, LeCun et al. demonstrate that pruning reduced the parameters
    in a well-trained neural net by   8x (combination of both automatic and manual
    removal) without a drop in classification accuracy.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速二阶导数的计算，OBD 忽略了权重之间的交互 ($\large\frac{\partial^{2}L}{\partial w_{i}\partial
    w_{j}}$)，因此仅计算 Hessian 矩阵的对角元素。否则，即使对于合理数量的权重，计算完整的 Hessian 矩阵也会非常繁琐 (对于 $n=10^{4}$，矩阵的大小是
    $10^{4}\times 10^{4}=10^{8}$)。在结果方面，LeCun 等人展示了剪枝将一个经过良好训练的神经网络的参数减少了 8 倍（自动和手动移除的组合），且分类准确率没有下降。
- en: Across different pruning strategies, the core algorithm could remain similar,
    with changes in the following aspects.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的剪枝策略中，核心算法可以保持相似，只是在以下方面有所变化。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Saliency: While (LeCun et al., [1990](#bib.bib99); Hassibi et al., [1993](#bib.bib73))
    use second-order derivatives, other methods rely on simpler magnitude based pruning
    (Han et al., [2015b](#bib.bib70), [a](#bib.bib69)), or momentum based pruning
    (Dettmers and Zettlemoyer, [2019](#bib.bib47)) etc. to determine the saliency
    score.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 显著性：虽然 (LeCun 等，[1990](#bib.bib99); Hassibi 等，[1993](#bib.bib73)) 使用二阶导数，其他方法则依赖于基于简单幅度的剪枝
    (Han 等，[2015b](#bib.bib70), [a](#bib.bib69)) 或基于动量的剪枝 (Dettmers 和 Zettlemoyer，[2019](#bib.bib47))
    等来确定显著性分数。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Structured v/s Unstructured: The most flexible way of pruning is unstructured
    (or random) pruning, where all given parameters are treated equally. In structured
    pruning, parameters are pruned in blocks (such as pruning row-wise in a weight
    matrix, or pruning channel-wise in a convolutional filter (Li et al., [2016](#bib.bib101);
    Anwar et al., [2017](#bib.bib6); Molchanov et al., [2016](#bib.bib113); Liu et al.,
    [2019](#bib.bib107)), etc.). The latter allows easier leveraging of inference-time
    gains in size and latency, since these blocks of pruned parameters can be intelligently
    skipped for storage and inference. Note that unstructured pruning can also be
    viewed as structured pruning with block size = 1.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结构化与非结构化：最灵活的修剪方式是非结构化（或随机）修剪，其中所有给定的参数都被平等对待。在结构化修剪中，参数按块修剪（例如，在权重矩阵中按行修剪，或在卷积滤波器中按通道修剪（Li
    et al., [2016](#bib.bib101); Anwar et al., [2017](#bib.bib6); Molchanov et al.,
    [2016](#bib.bib113); Liu et al., [2019](#bib.bib107)）等）。后者允许更容易利用推理时的大小和延迟的增益，因为这些修剪后的参数块可以智能地跳过存储和推理。请注意，非结构化修剪也可以看作是块大小为
    1 的结构化修剪。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Distribution: The decision about how to distribute the sparsity budget (number
    of parameters to be pruned), could be made either by pooling in all the parameters
    from the network and then deciding which parameters to prune, or by smartly selecting
    how much to prune in each layer individually (Dong et al., [2017](#bib.bib51);
    He et al., [2018](#bib.bib75)). (Elsen et al., [2020](#bib.bib53); google research,
    [2021](#bib.bib67)) have found that some architectures like MobileNetV2, EfficientNet
    (Tan et al., [2019](#bib.bib148)) have thin first layers that do not contribute
    significantly to the number of parameters and pruning them leads to an accuracy
    drop without much gain. Hence, intuitively it would be helpful to allocate sparsity
    on a per-layer basis.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分布：关于如何分配稀疏预算（需要修剪的参数数量）的决定，可以通过将网络中的所有参数汇总在一起，然后决定修剪哪些参数来做，或者通过智能地选择每层需要修剪多少参数来做（Dong
    et al., [2017](#bib.bib51); He et al., [2018](#bib.bib75)）。(Elsen et al., [2020](#bib.bib53);
    google research, [2021](#bib.bib67)) 发现一些架构如 MobileNetV2、EfficientNet (Tan et
    al., [2019](#bib.bib148)) 具有较薄的第一层，这些层对参数数量没有显著贡献，修剪它们会导致准确率下降，而收益不大。因此，直观上，将稀疏度分配到每层可能会更有帮助。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scheduling: Another question is how much to prune, and when? Should we prune
    an equal number of parameters every round (LeCun et al., [1990](#bib.bib99); Hassibi
    et al., [1993](#bib.bib73); Han et al., [2015b](#bib.bib70)), or should we prune
    at a higher pace in the beginning and gradually decrease (Zhu and Gupta, [2018](#bib.bib168);
    Dettmers and Zettlemoyer, [2019](#bib.bib47)).'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调度：另一个问题是修剪的数量和时机。我们是否应该每轮修剪相同数量的参数（LeCun et al., [1990](#bib.bib99); Hassibi
    et al., [1993](#bib.bib73); Han et al., [2015b](#bib.bib70)），或者在开始时以较高的速度修剪，然后逐渐减少（Zhu
    and Gupta, [2018](#bib.bib168); Dettmers and Zettlemoyer, [2019](#bib.bib47)）。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Regrowth: Some methods allow regrowing pruned connections (Evci et al., [2020](#bib.bib55);
    Dettmers and Zettlemoyer, [2019](#bib.bib47)) to keep the same level of sparsity
    through constant cycles of prune-redistribute-regrow. Dettmers et al. (Dettmers
    and Zettlemoyer, [2019](#bib.bib47)) estimate training time speedups between 2.7x
    - 5.6x by starting and operating with a sparse model throughout. However there
    is a gap in terms of implementation of sparse operations on CPU, GPU, and other
    hardware.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再生：一些方法允许再生修剪的连接（Evci et al., [2020](#bib.bib55); Dettmers and Zettlemoyer,
    [2019](#bib.bib47)），通过不断的修剪-重新分配-再生周期保持相同水平的稀疏度。Dettmers et al. (Dettmers and
    Zettlemoyer, [2019](#bib.bib47)) 估计在整个过程中使用稀疏模型可以将训练时间加速 2.7 倍到 5.6 倍。然而，在 CPU、GPU
    和其他硬件上实现稀疏操作存在差距。
- en: '| Model Architecture | Sparsity Type | Sparsity % | FLOPs | Top-1 Accuracy
    % | Source |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 模型架构 | 稀疏类型 | 稀疏百分比 | FLOPs | Top-1 准确率 % | 来源 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| MobileNet v2 - 1.0 | Dense (Baseline) | 0% | 1x | 72.0% | Sandler et al.
    (Sandler et al., [2018](#bib.bib134)) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet v2 - 1.0 | 密集（基线） | 0% | 1x | 72.0% | Sandler et al. (Sandler et
    al., [2018](#bib.bib134)) |'
- en: '| Unstructured | 75% | 0.27x | 67.7% | Zhu et al. (Zhu and Gupta, [2018](#bib.bib168))
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 非结构化 | 75% | 0.27x | 67.7% | Zhu et al. (Zhu and Gupta, [2018](#bib.bib168))
    |'
- en: '| Unstructured | 75% | 0.52x | 71.9% | Evci et al. (Evci et al., [2020](#bib.bib55))
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 非结构化 | 75% | 0.52x | 71.9% | Evci et al. (Evci et al., [2020](#bib.bib55))
    |'
- en: '| Structured (block-wise) | 85% | 0.11x | 69.7% | Elsen et al. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 结构化（按块） | 85% | 0.11x | 69.7% | Elsen et al. |'
- en: '| Unstructured | 90% | 0.12x | 61.8% | Zhu et al. (Zhu and Gupta, [2018](#bib.bib168))
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 非结构化 | 90% | 0.12x | 61.8% | Zhu et al. (Zhu and Gupta, [2018](#bib.bib168))
    |'
- en: '| Unstructured | 90% | 0.12x | 69.7% | Evci et al. (Evci et al., [2020](#bib.bib55))
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 无结构 | 90% | 0.12x | 69.7% | Evci 等（Evci 等，[2020](#bib.bib55)） |'
- en: Table 1\. A sample of various sparsity results on the MobileNet v2 architecture
    with depth multiplier = 1.0.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 在 MobileNet v2 架构中，深度倍增器 = 1.0 的各种稀疏性结果示例。
- en: 'Beyond Model Optimization: Frankle et al.’s (Frankle and Carbin, [2018](#bib.bib58))
    work on the Lottery Ticket Hypothesis took a different look at pruning, and postulated
    that within every large network lies a smaller network, which can be extracted
    with the original initialization of its parameters, and retrained on its own to
    match or exceed the performance of the larger network. The authors demonstrated
    these results on multiple datasets, but others such as (Gale et al., [2019](#bib.bib59);
    Liu et al., [2018b](#bib.bib108)) were not able to replicate this on larger datasets
    such as ImageNet (Deng et al., [2009](#bib.bib46)). Rather Liu et al. (Liu et al.,
    [2018b](#bib.bib108)) demonstrate that the pruned architecture with random initialization
    does no worse than the pruned architecture with the trained weights.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 超越模型优化：Frankle 等人（Frankle 和 Carbin，[2018](#bib.bib58)）关于彩票票据假设的研究从不同的角度看待了剪枝，并假设在每个大型网络中存在一个较小的网络，这个网络可以通过原始参数初始化进行提取，并重新训练以匹配或超过大型网络的性能。作者在多个数据集上展示了这些结果，但其他人（如
    Gale 等，[2019](#bib.bib59)；Liu 等，[2018b](#bib.bib108)）未能在较大的数据集（如 ImageNet（Deng
    等，[2009](#bib.bib46)））上复制这些结果。相反，Liu 等（Liu 等，[2018b](#bib.bib108)）展示了具有随机初始化的剪枝架构不逊色于具有训练权重的剪枝架构。
- en: 'Discussion: There is a significant body of work that demonstrates impressive
    theoretical reduction in the model size (via number of parameters), or estimates
    the savings in FLOPs (Table [1](#S3.T1 "Table 1 ‣ 3.1.1\. Pruning ‣ 3.1\. Compression
    Techniques ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning:
    A Survey on Making Deep Learning Models Smaller, Faster, and Better")). However,
    a large fraction of the results are on *unstructured* pruning, where it is not
    currently clear how these improvements can lead to reduction in footprint metrics
    (apart from using standard file compression tools like GZip).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '讨论：大量的研究表明，模型大小（通过参数数量）在理论上显著减少，或估算了 FLOPs 的节省（表 [1](#S3.T1 "Table 1 ‣ 3.1.1\.
    Pruning ‣ 3.1\. Compression Techniques ‣ 3\. Landscape of Efficient Deep Learning
    ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster,
    and Better")）。然而，大部分结果是在*无结构*剪枝上，尚不清楚这些改进如何导致占用指标的减少（除了使用标准文件压缩工具如 GZip）。'
- en: On the other hand, structured pruning with a meaningful block size is conducive
    to latency improvements. Elsen et al. (Elsen et al., [2020](#bib.bib53); google
    research, [2021](#bib.bib67)) construct sparse convolutional networks that outperform
    their dense counterparts by $1.3$ - $2.4\times$ with $\approx$ 66% of the parameters,
    while retaining the same Top-1 accuracy. They do this via their library to convert
    from the NHWC (channels-last) standard dense representation to a special NCHW
    (channels-first) ‘Block Compressed Sparse Row’ (BCSR) representation which is
    suitable for fast inference using their fast kernels on ARM devices, WebAssembly
    etc. (Authors, [2021k](#bib.bib19)). Although they also introduce some constraints
    on the kinds of sparse networks that can be accelerated (Authors, [2021l](#bib.bib20)).
    Overall, this is a promising step towards practical improvements in footprint
    metrics with pruned networks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，具有有意义的块大小的结构化剪枝有利于延迟改进。Elsen 等（Elsen 等，[2020](#bib.bib53)；谷歌研究，[2021](#bib.bib67)）构建了稀疏卷积网络，比其密集的对应物提高了
    $1.3$ - $2.4\times$，参数约为 66%，同时保持相同的 Top-1 精度。他们通过他们的库将 NHWC（通道最后）标准密集表示转换为适合于
    ARM 设备、WebAssembly 等快速内核的 NCHW（通道优先）‘块压缩稀疏行’（BCSR）表示来实现这一点（作者，[2021k](#bib.bib19)）。尽管他们也对可以加速的稀疏网络类型提出了一些限制（作者，[2021l](#bib.bib20)）。总体而言，这一步朝着通过剪枝网络实现占用指标的实际改进迈出了令人鼓舞的一步。
- en: 3.1.2\. Quantization
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 量化
- en: 'Almost all the weights and activations of a typical network are in 32-bit floating-point
    values. One of the ideas of reducing model footprint is to reduce the precision
    for the weights and activations by *quantizing* to a lower-precision datatype
    (often 8-bit fixed-point integers). There are two kinds of gains that we can get
    from quantization: (a) lower model size, and (b) lower inference latency. Often,
    only the model size is a constraint, and in this case we can employ a technique
    called weight quantization and get model size improvements (Authors, [2021f](#bib.bib14)),
    where only the model weights are in reduced precision. In order to get latency
    improvements, the activations need to be in fixed-point as well (Activation Quantization
    (Vanhoucke et al., [2011](#bib.bib154); Jacob et al., [2018](#bib.bib83)), such
    that all the operations in the quantized graph are happening in fixed-point math
    as well.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有典型网络的权重和激活值都是 32 位浮点值。减少模型占用空间的一个想法是通过*量化*到较低精度的数据类型（通常是 8 位定点整数）来降低权重和激活的精度。量化可以带来两种收益：（a）降低模型大小，以及（b）降低推理延迟。通常，模型大小是一个限制因素，在这种情况下，我们可以采用一种称为权重量化的技术来获得模型大小的改进（Authors,
    [2021f](#bib.bib14)），其中只有模型权重是以降低的精度表示。为了获得延迟改进，激活值也需要以定点形式存在（激活量化 (Vanhoucke
    等, [2011](#bib.bib154); Jacob 等, [2018](#bib.bib83)），以确保量化图中的所有操作也在定点数学中进行。
- en: 'Weight Quantization: A simple *scheme* for quantizing weights to get model
    size improvements (similar to (Krishnamoorthi, [2018](#bib.bib91))) is as follows.
    Given a 32-bit floating-point weight matrix in a model, we can map the minimum
    weight value ($x_{min}$) in that matrix to $0$, and the maximum value ($x_{max}$)
    to $2^{b}-1$ (where $b$ is the number of bits of precision, and $b<32$). Then
    we can linearly extrapolate all values between them to an integer value in [$0,2^{b}-1$]
    (Figure  [5](#S3.F5 "Figure 5 ‣ 3.1.2\. Quantization ‣ 3.1\. Compression Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better")). Thus, we are able
    to map each floating point value to a fixed-point value where the latter requires
    a lesser number of bits than the floating-point representation. This process can
    also be done for signed $b$-bit fixed-point integers, where the output values
    will be in the range [-$2^{\frac{b}{2}}-1$, $2^{\frac{b}{2}}-1$]. One of the reasonable
    values of $b$ is $8$, since this would lead to a $32/8=4\times$ reduction in space,
    and also because of the near-universal support for uint8_t and int8_t datatypes.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 权重量化：一种简单的*方案*用于量化权重以获得模型大小的改进（类似于（Krishnamoorthi, [2018](#bib.bib91)））如下。给定模型中的一个
    32 位浮点权重矩阵，我们可以将该矩阵中的最小权重值 ($x_{min}$) 映射到 $0$，将最大值 ($x_{max}$) 映射到 $2^{b}-1$（其中
    $b$ 是精度的位数，$b<32$）。然后我们可以线性地将它们之间的所有值外推到 [$0,2^{b}-1$] 的整数值（图 [5](#S3.F5 "图 5
    ‣ 3.1.2\. 量化 ‣ 3.1\. 压缩技术 ‣ 3\. 高效深度学习的全景 ‣ 高效深度学习：在缩小、加速和改善深度学习模型方面的调查")）。因此，我们能够将每个浮点值映射到一个定点值，其中后者所需的位数少于浮点表示。这一过程也可以应用于有符号
    $b$ 位定点整数，其中输出值将位于 [-$2^{\frac{b}{2}}-1$, $2^{\frac{b}{2}}-1$] 范围内。$b$ 的一个合理值是
    $8$，因为这将导致 $32/8=4\times$ 空间减少，并且由于对 uint8_t 和 int8_t 数据类型的几乎普遍支持。
- en: During inference, we go in the reverse direction where we recover a lossy estimate
    of the original floating point value (*dequantization*) using just the $x_{min}$
    and $x_{max}$. This estimate is lossy since we lost $32-b$ bits of information
    when did the rounding (another way to look at it is that a range of floating point
    values map to the same quantized value).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，我们将朝相反的方向前进，其中使用 $x_{min}$ 和 $x_{max}$ 恢复原始浮点值的有损估计（*反量化*）。这种估计是有损的，因为在进行舍入时我们丢失了
    $32-b$ 位的信息（另一种看法是，一系列浮点值映射到相同的量化值）。
- en: '![Refer to caption](img/a238d6417900d87f3700f271423324e8.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a238d6417900d87f3700f271423324e8.png)'
- en: Figure 5\. Quantizing floating-point continuous values to discrete fixed-point
    values. The continuous values are clamped to the range $x_{min}$ to $x_{max}$,
    and are mapped to discrete values in [$0$, $2^{b}-1$] (in the above figure, $b=3$,
    hence the quantized values are in the range [$0,7$].
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 将浮点连续值量化为离散定点值。连续值被夹紧到 $x_{min}$ 到 $x_{max}$ 的范围，并被映射到 [$0$, $2^{b}-1$]
    的离散值（在上述图中，$b=3$，因此量化后的值在 [$0,7$] 范围内）。
- en: '(Jacob et al., [2018](#bib.bib83); Krishnamoorthi, [2018](#bib.bib91)) formalize
    the quantization scheme with the following two constraints:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (Jacob 等，[2018](#bib.bib83)；Krishnamoorthi，[2018](#bib.bib91)) 通过以下两个约束来形式化量化方案：
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The quantization scheme should be linear (affine transformation), so that the
    precision bits are linearly distributed.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化方案应为线性（仿射变换），以使精度位线性分布。
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $0.0$ should map exactly to a fixed-point value $x_{q_{0}}$, such that dequantizing
    $x_{q_{0}}$ gives us $0.0$. This is an implementation constraint, since $0$ is
    also used for padding to signify missing elements in tensors, and if dequantizing
    $x_{q_{0}}$ leads to a non-zero value, then it might be interpreted incorrectly
    as a valid element at that index.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $0.0$ 应精确映射到定点值 $x_{q_{0}}$，使得反量化 $x_{q_{0}}$ 能得到 $0.0$。这是一个实现约束，因为 $0$ 也用于填充以表示张量中的缺失元素，如果反量化
    $x_{q_{0}}$ 导致非零值，则可能会被误解为该索引处的有效元素。
- en: 'The second constraint described above requires that $0$ be a part of the quantization
    range, which in turn requires updating $x_{min}$ and $x_{max}$, followed by clamping
    $x$ to lie in $[x_{min},x_{max}]$. Following this, we can quantize $x$ by constructing
    a piece-wise linear transformation as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 上述第二个约束要求 $0$ 是量化范围的一部分，这反过来又要求更新 $x_{min}$ 和 $x_{max}$，然后将 $x$ 限制在 $[x_{min},x_{max}]$
    之间。接着，我们可以通过构造分段线性变换来量化 $x$：
- en: '| (1) |  | $\small\textrm{quantize}(x)=x_{q}=\textrm{round}\bigg{(}\frac{x}{s}\bigg{)}+z$
    |  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\small\textrm{quantize}(x)=x_{q}=\textrm{round}\bigg{(}\frac{x}{s}\bigg{)}+z$
    |  |'
- en: '$s$ is the floating-point *scale* value (can be thought of as the inverse of
    the slope, which can be computed using $x_{min}$, $x_{max}$ and the range of the
    fixed-point values). $z$ is an integer *zero-point* value which is the quantized
    value that is assigned to $x=0.0$. This is the terminology followed in literature
    (Jacob et al., [2018](#bib.bib83); Krishnamoorthi, [2018](#bib.bib91)) (Algorithm
    [2](#algorithm2 "In 3.1.2\. Quantization ‣ 3.1\. Compression Techniques ‣ 3\.
    Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better")).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: $s$ 是浮点数 *缩放* 值（可以看作是斜率的倒数，可以通过 $x_{min}$、$x_{max}$ 和定点值范围来计算）。$z$ 是整数 *零点*
    值，它是分配给 $x=0.0$ 的量化值。这是文献中使用的术语（Jacob 等，[2018](#bib.bib83)；Krishnamoorthi，[2018](#bib.bib91)）（算法
    [2](#algorithm2 "在 3.1.2\. 量化 ‣ 3.1\. 压缩技术 ‣ 3\. 高效深度学习的格局 ‣ 高效深度学习：对深度学习模型变小、更快、更好的调查")）。
- en: 'The dequantization step constructs $\hat{x}$, which is a lossy estimate of
    $x$, since we lose precision when quantizing to a lower number of bits. We can
    compute it as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 反量化步骤构造了 $\hat{x}$，这是 $x$ 的有损估计，因为在量化为较少的位数时，我们会失去精度。我们可以按如下方式计算它：
- en: '| (2) |  | $\small\textrm{dequantize}(x_{q})=\hat{x}=s(x_{q}-z)$ |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\small\textrm{dequantize}(x_{q})=\hat{x}=s(x_{q}-z)$ |  |'
- en: 'Since $s$ is in floating-point, $\hat{x}$ is also a floating-point value (Algorithm
    [3](#algorithm3 "In 3.1.2\. Quantization ‣ 3.1\. Compression Techniques ‣ 3\.
    Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better")). Note that the quantization
    and dequantization steps can be performed for signed integers too by appropriately
    changing the value $x_{q_{min}}$ (which is the lowest fixed-point value in $b$-bits)
    in Algorithm [2](#algorithm2 "In 3.1.2\. Quantization ‣ 3.1\. Compression Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better").'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $s$ 是浮点数，$\hat{x}$ 也是浮点数值（算法 [3](#algorithm3 "在 3.1.2\. 量化 ‣ 3.1\. 压缩技术 ‣
    3\. 高效深度学习的格局 ‣ 高效深度学习：对深度学习模型变小、更快、更好的调查")）。注意，量化和反量化步骤也可以通过适当地改变值 $x_{q_{min}}$（即
    $b$ 位中的最低定点值）来对有符号整数进行。
- en: 'Data: Floating-point tensor to compress $\mathbf{X}$, number of precision bits
    $b$ for the fixed-point representation.Result: Quantized tensor $\mathbf{X_{q}}$.1  $\textbf{X}_{min},\textbf{X}_{max}\leftarrow\textrm{min}(\mathbf{X},0),\textrm{max}(\mathbf{X},0)$;2  $\mathbf{X}\leftarrow\textrm{clamp}(\mathbf{X},\textbf{X}_{min},\textbf{X}_{max})$;3  $s\leftarrow\frac{\displaystyle
    x_{max}-x_{min}}{\displaystyle 2^{b}-1}$;4  $z\leftarrow\textrm{round}\bigg{(}x_{q_{min}}-\frac{\displaystyle
    x_{min}}{\displaystyle s}\bigg{)}$;56$\mathbf{X_{q}}\leftarrow\textrm{round}\bigg{(}\frac{\displaystyle\mathbf{X}}{\displaystyle
    s}\bigg{)}+z$;7 return $\mathbf{X_{q}}$;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 数据：用于压缩的浮点张量 $\mathbf{X}$，定点表示的精度位数 $b$。结果：量化张量 $\mathbf{X_{q}}$。1  $\textbf{X}_{min},\textbf{X}_{max}\leftarrow\textrm{min}(\mathbf{X},0),\textrm{max}(\mathbf{X},0)$；2  $\mathbf{X}\leftarrow\textrm{clamp}(\mathbf{X},\textbf{X}_{min},\textbf{X}_{max})$；3  $s\leftarrow\frac{\displaystyle
    x_{max}-x_{min}}{\displaystyle 2^{b}-1}$；4  $z\leftarrow\textrm{round}\bigg{(}x_{q_{min}}-\frac{\displaystyle
    x_{min}}{\displaystyle s}\bigg{)}$；56$\mathbf{X_{q}}\leftarrow\textrm{round}\bigg{(}\frac{\displaystyle\mathbf{X}}{\displaystyle
    s}\bigg{)}+z$；7 返回 $\mathbf{X_{q}}$；
- en: Algorithm 2 Quantizing a given weight matrix $\mathbf{X}$
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 量化给定的权重矩阵 $\mathbf{X}$
- en: 'Data: Fixed-point matrix to dequantize $\mathbf{X_{q}}$, along with the scale
    $s$, and zero-point $z$ values which were calculated during quantization.Result:
    Dequantized floating-point weight matrix $\widehat{\mathbf{X}}$.1  $\widehat{\mathbf{X}}\leftarrow
    s(\mathbf{X_{q}}-z)$;2 return $\widehat{\mathbf{X}}$;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 数据：用于解量化的定点矩阵 $\mathbf{X_{q}}$，以及在量化过程中计算得出的缩放因子 $s$ 和零点 $z$ 值。结果：解量化的浮点权重矩阵
    $\widehat{\mathbf{X}}$。1  $\widehat{\mathbf{X}}\leftarrow s(\mathbf{X_{q}}-z)$；2
    返回 $\widehat{\mathbf{X}}$；
- en: Algorithm 3 Dequantizing a given fixed-point weight matrix $\mathbf{X_{q}}$
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 解量化给定的定点权重矩阵 $\mathbf{X_{q}}$
- en: We can utilize the above two algorithms for quantizing and dequantizing the
    model’s weight matrices. Quantizing a pre-trained model’s weights for reducing
    the size is termed as *post-training quantization* in literature (Authors, [2021f](#bib.bib14)).
    This might be sufficient for the purpose of reducing the model size when there
    is sufficient representational capacity in the model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用上述两个算法来量化和解量化模型的权重矩阵。对预训练模型权重进行量化以减少模型大小在文献中称为 *后训练量化*（Authors, [2021f](#bib.bib14)）。当模型具有足够的表示能力时，这可能足以减少模型大小。
- en: There are other works in literature (Rastegari et al., [2016](#bib.bib128);
    Hubara et al., [2016](#bib.bib81); Li et al., [2016](#bib.bib100)) that demonstrate
    slightly different variants of quantization. XNOR-Net (Rastegari et al., [2016](#bib.bib128)),
    Binarized Neural Networks (Hubara et al., [2016](#bib.bib81)) and others use $b=1$,
    and thus have weight matrices which just have two possible values $0$ or $1$,
    and the quantization function there is simply the $\textrm{sign}(x)$ function
    (assuming the weights are symmetrically distributed around $0$).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中还有其他工作（Rastegari 等人，[2016](#bib.bib128)；Hubara 等人，[2016](#bib.bib81)；Li 等人，[2016](#bib.bib100)）展示了略有不同的量化变体。
    XNOR-Net（Rastegari 等人，[2016](#bib.bib128)）、二值化神经网络（Hubara 等人，[2016](#bib.bib81)）等使用
    $b=1$，因此权重矩阵只有 $0$ 或 $1$ 两个可能的值，量化函数简单地是 $\textrm{sign}(x)$ 函数（假设权重围绕 $0$ 对称分布）。
- en: The promise with such extreme quantization approaches is the theoretical $32/1=32\times$
    reduction in model size without much quality loss. Some of the works claim improvements
    on larger networks like AlexNet (Krizhevsky et al., [2012](#bib.bib93)), VGG (Simonyan
    and Zisserman, [2014](#bib.bib142)), Inception (Szegedy et al., [2015](#bib.bib147))
    etc., which might already be more amenable to compression. A more informative
    task would be to demonstrate extreme quantization on smaller networks like the
    MobileNet family (Sandler et al., [2018](#bib.bib134); Howard et al., [2019](#bib.bib78)).
    Additionally binary quantization (and other quantization schemes like ternary
    (Li et al., [2016](#bib.bib100)), bit-shift based networks (Rastegari et al.,
    [2016](#bib.bib128)), etc.) promise latency-efficient implementations of standard
    operations where multiplications and divisions are replaced by cheaper operations
    like addition, subtraction, etc. These claims need to be verified because even
    if these lead to theoretical reduction in FLOPs, the implementations still need
    support from the underlying hardware. A fair comparison would be using standard
    quantization with $b=8$, where the multiplications and divisions also become cheaper,
    and are supported by the hardware efficiently via SIMD instructions which allow
    for low-level data parallelism (for example, on x86 via the SSE instruction set,
    on ARM via the Neon (Ltd., [2021](#bib.bib109)) intrinsics, and even on specialized
    DSPs like the Qualcomm Hexagon (Authors, [2021l](#bib.bib20))).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这种极端量化方法的承诺是模型大小理论上可以减少 $32/1=32\times$，且几乎没有质量损失。一些研究声称在更大的网络如 AlexNet（Krizhevsky
    et al., [2012](#bib.bib93)）、VGG（Simonyan and Zisserman, [2014](#bib.bib142)）、Inception（Szegedy
    et al., [2015](#bib.bib147)）等上取得了改进，这些网络可能已经更适合压缩。一个更具信息量的任务是演示在更小的网络如 MobileNet
    系列（Sandler et al., [2018](#bib.bib134); Howard et al., [2019](#bib.bib78)）上的极端量化。此外，二值量化（以及其他量化方案如三值量化（Li
    et al., [2016](#bib.bib100)）、基于位移的网络（Rastegari et al., [2016](#bib.bib128)）等）承诺在标准操作中实现低延迟，其中乘法和除法被加法、减法等更便宜的操作替代。这些声明需要验证，因为即使这些方法在理论上减少了
    FLOPs，实际实现仍需得到底层硬件的支持。一个公平的比较是使用标准量化 $b=8$，其中乘法和除法也变得更便宜，并通过 SIMD 指令得到硬件的高效支持，这允许低级数据并行处理（例如，通过
    SSE 指令集在 x86 上，通过 Neon（Ltd., [2021](#bib.bib109)）指令在 ARM 上，甚至通过 Qualcomm Hexagon（Authors,
    [2021l](#bib.bib20)）等专用 DSP）。
- en: 'Activation Quantization: To be able to get *latency improvements* with quantized
    networks, the math operations have to be done in fixed-point representations too.
    This means all intermediate layer inputs and outputs are also in fixed-point,
    and there is no need to dequantize the weight-matrices since they can be used
    directly along with the inputs.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 激活量化：为了在量化网络中获得*延迟改进*，数学运算也必须以固定点表示进行。这意味着所有中间层的输入和输出也都是固定点的，并且不需要对权重矩阵进行反量化，因为它们可以直接与输入一起使用。
- en: Vanhoucke et al. (Vanhoucke et al., [2011](#bib.bib154)) demonstrated a $3\times$
    inference speedup using a fully fixed-point model on an x86 CPU, when compared
    to a floating-point model on the same CPU, without sacrificing accuracy. The weights
    are still quantized similar to post-training quantization, however all layer inputs
    (except the first layer) and the activations are fixed-point. In terms of performance,
    the primary driver for this improvement was the availability of fixed-point SIMD
    instructions in Intel’s SSE4 instruction set (Contributors to Wikimedia projects,
    [2021e](#bib.bib42)), where commonly used building-block operations like the Multiply-Accumulate
    (MAC) (Contributors to Wikimedia projects, [2021d](#bib.bib41)) can be parallelized.
    Since the paper was published, Intel has released two more iterations of these
    instruction sets (Contributors to Wikimedia projects, [2021a](#bib.bib38)) which
    might further improve the speedups.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Vanhoucke 等人（Vanhoucke et al., [2011](#bib.bib154)）在 x86 CPU 上使用完全固定点模型实现了 $3\times$
    的推理加速，相较于同一 CPU 上的浮点模型，且没有牺牲准确性。权重仍然像训练后量化一样被量化，但所有层的输入（除了第一层）和激活值都是固定点的。在性能方面，这一改进的主要驱动力是
    Intel SSE4 指令集中的固定点 SIMD 指令（Contributors to Wikimedia projects, [2021e](#bib.bib42)），其中常用的构建块操作如乘加（MAC）（Contributors
    to Wikimedia projects, [2021d](#bib.bib41)）可以并行处理。自论文发布以来，Intel 发布了这类指令集的两个新版本（Contributors
    to Wikimedia projects, [2021a](#bib.bib38)），这可能进一步提高加速效果。
- en: 'Quantization-Aware Training (QAT): The network that Vanhoucke et al. mention
    was a 5 layer feed-forward network that was post-training quantized. However post-training
    quantization can lead to quality loss during inference as highlighted in (Krishnamoorthi,
    [2018](#bib.bib91); Jacob et al., [2018](#bib.bib83); Wang et al., [2020](#bib.bib157))
    as the networks become more complex. These could be because of: (a) outlier weights
    that skew the computation of the quantized values for the entire input range towards
    the outliers, leading to less number of bits being allocated to the bulk of the
    range, or (b) Different distribution of weights within the weight matrix, for
    eg. within a convolutional layer the distribution of weights between each filter
    might be different, but they are quantized the same way. These effects might be
    more pronounced at low-bit widths due to an even worse loss of precision. Wang
    et al. (Wang et al., [2020](#bib.bib157)) try to retain the post-training quantization
    but with new heuristics to allocate the precision bits in a learned fashion. Tools
    like the TFLite Converter (TensorFlow, [2019](#bib.bib150)) augment post-training
    quantization with a representative dataset provided by the user, to actively correct
    for errors at different points in the model by comparing the error between the
    activations of the quantized and unquantized graphs.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 量化感知训练（QAT）：Vanhoucke 等提到的网络是一个5层前馈网络，经过训练后进行了量化。然而，如 Krishnamoorthi（Krishnamoorthi,
    [2018](#bib.bib91)）、Jacob 等（Jacob et al., [2018](#bib.bib83)）和 Wang 等（Wang et
    al., [2020](#bib.bib157)）所强调，后训练量化可能会导致推理过程中质量的损失，尤其是当网络变得更复杂时。这可能是由于：（a）异常值权重使得整个输入范围的量化值计算偏向异常值，从而导致大部分范围分配的位数减少；或者（b）权重矩阵内权重的不同分布，例如，在卷积层中，每个滤波器之间的权重分布可能不同，但它们的量化方式相同。这些效应在低位宽下可能会更加明显，因为精度损失更严重。Wang
    等（Wang et al., [2020](#bib.bib157)）尝试保留后训练量化，但通过新的启发式方法以学习的方式分配精度位。像 TFLite 转换器（TensorFlow,
    [2019](#bib.bib150)）这样的工具，通过用户提供的代表性数据集增强了后训练量化，通过比较量化和未量化图的激活之间的误差，主动修正模型中不同点的错误。
- en: Jacob et al. (Jacob et al., [2018](#bib.bib83)) propose (and further detailed
    by Krishnamoorthi et al. (Krishnamoorthi, [2018](#bib.bib91))) a training regime
    which is *quantization-aware*. In this setting, the training happens in floating-point
    but the forward-pass simulates the quantization behavior during inference. Both
    weights and activations are passed through a function that simulates this quantization
    behavior (*fake-quantized* is the term used by many works (Jacob et al., [2018](#bib.bib83);
    Krishnamoorthi, [2018](#bib.bib91))).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Jacob 等（Jacob et al., [2018](#bib.bib83)）提出了一个*量化感知*的训练机制，并由 Krishnamoorthi
    等（Krishnamoorthi, [2018](#bib.bib91)）进一步详细说明。在这种设置下，训练是在浮点数中进行的，但前向传播会模拟推理过程中量化的行为。权重和激活都通过一个函数来模拟这种量化行为（许多文献使用“*伪量化*”这一术语（Jacob
    et al., [2018](#bib.bib83); Krishnamoorthi, [2018](#bib.bib91)））。
- en: 'Assuming $\mathbf{X}$ is the tensor to be fake-quantized, Jacob et al. (Jacob
    et al., [2018](#bib.bib83)) propose adding special quantization nodes in the training
    graph that collect the statistics (moving averages of $x_{min}$ and $x_{max}$)
    related to the weights and activations to be quantized (see Figure [6b](#S3.F6.sf2
    "In Figure 6 ‣ 3.1.2\. Quantization ‣ 3.1\. Compression Techniques ‣ 3\. Landscape
    of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep
    Learning Models Smaller, Faster, and Better")(a) for an illustration). Once we
    have these values for each $\mathbf{X}$, we can derive the respective $\widehat{\mathbf{X}}$
    using equations ([1](#S3.E1 "In 3.1.2\. Quantization ‣ 3.1\. Compression Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") and [2](#S3.E2 "In
    3.1.2\. Quantization ‣ 3.1\. Compression Techniques ‣ 3\. Landscape of Efficient
    Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better")) as follows.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 $\mathbf{X}$ 是待伪量化的张量，Jacob 等人（Jacob et al.，[2018](#bib.bib83)）建议在训练图中添加特殊的量化节点，这些节点收集与待量化的权重和激活相关的统计信息（$x_{min}$
    和 $x_{max}$ 的移动平均）（参见图 [6b](#S3.F6.sf2 "在图 6 ‣ 3.1.2\. 量化 ‣ 3.1\. 压缩技术 ‣ 3\. 高效深度学习的全景
    ‣ 高效深度学习：在使深度学习模型更小、更快、更好方面的调查")(a) 获取说明）。一旦我们获得了每个 $\mathbf{X}$ 的这些值，就可以使用方程式（[1](#S3.E1
    "在 3.1.2\. 量化 ‣ 3.1\. 压缩技术 ‣ 3\. 高效深度学习的全景 ‣ 高效深度学习：在使深度学习模型更小、更快、更好方面的调查") 和
    [2](#S3.E2 "在 3.1.2\. 量化 ‣ 3.1\. 压缩技术 ‣ 3\. 高效深度学习的全景 ‣ 高效深度学习：在使深度学习模型更小、更快、更好方面的调查")）推导出相应的
    $\widehat{\mathbf{X}}$。
- en: '| (3) |  | <math   alttext="\small\begin{split}\widehat{\mathbf{X}}{}&amp;=\textrm{FakeQuant}(\mathbf{X})\\
    &amp;=\textrm{Dequantize}(\textrm{Quantize}(\mathbf{X}))\\'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '| (3) |  | <math   alttext="\small\begin{split}\widehat{\mathbf{X}}{}&amp;=\textrm{FakeQuant}(\mathbf{X})\\
    &amp;=\textrm{Dequantize}(\textrm{Quantize}(\mathbf{X}))\\'
- en: '&amp;=s((\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}+z)-z)\\'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=s((\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}+z)-z)\\'
- en: '&amp;=s\bigg{(}\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}\bigg{)}\\'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=s\bigg{(}\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}\bigg{)}\\'
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd columnalign="right"  ><mover accent="true"  ><mi
    mathsize="90%" >𝐗</mi><mo mathsize="90%" >^</mo></mover></mtd><mtd columnalign="left"  ><mrow
    ><mo mathsize="90%" >=</mo><mrow  ><mtext mathsize="90%"  >FakeQuant</mtext><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="90%" minsize="90%" >(</mo><mi
    mathsize="90%" >𝐗</mi><mo maxsize="90%" minsize="90%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo mathsize="90%"  >=</mo><mrow ><mtext mathsize="90%"
    >Dequantize</mtext><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="90%"
    minsize="90%"  >(</mo><mrow ><mtext mathsize="90%" >Quantize</mtext><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo maxsize="90%" minsize="90%"  >(</mo><mi mathsize="90%"  >𝐗</mi><mo
    maxsize="90%" minsize="90%"  >)</mo></mrow></mrow><mo maxsize="90%" minsize="90%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo mathsize="90%"  >=</mo><mrow ><mi mathsize="90%"
    >s</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="90%" minsize="90%"  >(</mo><mrow
    ><mrow ><mo maxsize="90%" minsize="90%" >(</mo><mrow ><mrow ><mtext mathsize="90%"
    >round</mtext><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo maxsize="210%" minsize="210%"  >(</mo><mfrac
    ><mrow ><mtext mathsize="90%" >clamp</mtext><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo maxsize="90%" minsize="90%"  >(</mo><mi mathsize="90%"  >𝐗</mi><mo mathsize="90%"  >,</mo><msub
    ><mi mathsize="90%"  >x</mi><mrow ><mi mathsize="90%"  >m</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathsize="90%"  >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="90%"  >n</mi></mrow></msub><mo mathsize="90%"  >,</mo><msub ><mi mathsize="90%"  >x</mi><mrow
    ><mi mathsize="90%"  >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >a</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >x</mi></mrow></msub><mo
    maxsize="90%" minsize="90%"  >)</mo></mrow></mrow><mi mathsize="90%"  >s</mi></mfrac><mo
    maxsize="210%" minsize="210%"  >)</mo></mrow></mrow><mo mathsize="90%"  >+</mo><mi
    mathsize="90%"  >z</mi></mrow><mo maxsize="90%" minsize="90%"  >)</mo></mrow><mo
    mathsize="90%"  >−</mo><mi mathsize="90%"  >z</mi></mrow><mo maxsize="90%" minsize="90%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo mathsize="90%"  >=</mo><mrow ><mi mathsize="90%"
    >s</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="210%" minsize="210%"  >(</mo><mrow
    ><mtext mathsize="90%" >round</mtext><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo maxsize="210%" minsize="210%"  >(</mo><mfrac ><mrow ><mtext mathsize="90%"
    >clamp</mtext><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo maxsize="90%" minsize="90%"  >(</mo><mi
    mathsize="90%"  >𝐗</mi><mo mathsize="90%"  >,</mo><msub ><mi mathsize="90%"  >x</mi><mrow
    ><mi mathsize="90%"  >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >n</mi></mrow></msub><mo
    mathsize="90%"  >,</mo><msub ><mi mathsize="90%"  >x</mi><mrow ><mi mathsize="90%"  >m</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >a</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathsize="90%"  >x</mi></mrow></msub><mo maxsize="90%"
    minsize="90%"  >)</mo></mrow></mrow><mi mathsize="90%"  >s</mi></mfrac><mo maxsize="210%"
    minsize="210%"  >)</mo></mrow></mrow><mo maxsize="210%" minsize="210%"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><ci >^</ci><ci  >𝐗</ci></apply><apply
    ><ci ><mtext mathsize="90%" >FakeQuant</mtext></ci><ci >𝐗</ci></apply></apply><apply
    ><apply  ><ci ><mtext mathsize="90%" >Dequantize</mtext></ci><apply ><ci ><mtext
    mathsize="90%" >Quantize</mtext></ci><ci >𝐗</ci></apply></apply></apply><apply
    ><apply  ><ci >𝑠</ci><apply ><apply ><apply  ><ci ><mtext mathsize="90%" >round</mtext></ci><apply
    ><apply ><ci  ><mtext mathsize="90%"  >clamp</mtext></ci><vector ><ci >𝐗</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><apply ><ci >𝑚</ci><ci
    >𝑖</ci><ci >𝑛</ci></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><apply ><ci >𝑚</ci><ci >𝑎</ci><ci >𝑥</ci></apply></apply></vector></apply><ci
    >𝑠</ci></apply></apply><ci >𝑧</ci></apply><ci >𝑧</ci></apply></apply></apply><apply
    ><apply ><ci  >𝑠</ci><apply ><ci ><mtext mathsize="90%" >round</mtext></ci><apply
    ><apply  ><ci ><mtext mathsize="90%"  >clamp</mtext></ci><vector ><ci >𝐗</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><apply ><ci >𝑚</ci><ci
    >𝑖</ci><ci >𝑛</ci></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><apply ><ci >𝑚</ci><ci >𝑎</ci><ci >𝑥</ci></apply></apply></vector></apply><ci
    >𝑠</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >\small\begin{split}\widehat{\mathbf{X}}{}&=\textrm{FakeQuant}(\mathbf{X})\\ &=\textrm{Dequantize}(\textrm{Quantize}(\mathbf{X}))\\
    &=s((\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}+z)-z)\\ &=s\bigg{(}\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}\bigg{)}\\ \end{split}</annotation></semantics></math> |  |
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd columnalign="right"  ><mover accent="true"  ><mi
    mathsize="90%" >𝐗</mi><mo mathsize="90%" >^</mo></mover></mtd><mtd columnalign="left"  ><mrow
    ><mo mathsize="90%" >=</mo><mrow  ><mtext mathsize="90%"  >伪量化</mtext><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo maxsize="90%" minsize="90%" >(</mo><mi mathsize="90%"
    >𝐗</mi><mo maxsize="90%" minsize="90%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo mathsize="90%"  >=</mo><mrow ><mtext mathsize="90%"
    >去量化</mtext><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="90%" minsize="90%"  >(</mo><mrow
    ><mtext mathsize="90%" >量化</mtext><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo maxsize="90%" minsize="90%"  >(</mo><mi mathsize="90%"  >𝐗</mi><mo maxsize="90%"
    minsize="90%"  >)</mo></mrow></mrow><mo maxsize="90%" minsize="90%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo mathsize="90%"  >=</mo><mrow ><mi mathsize="90%"
    >s</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="90%" minsize="90%"  >(</mo><mrow
    ><mrow ><mo maxsize="90%" minsize="90%" >(</mo><mrow ><mrow ><mtext mathsize="90%"
    >四舍五入</mtext><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo maxsize="210%" minsize="210%"  >(</mo><mfrac
    ><mrow ><mtext mathsize="90%" >限制</mtext><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo maxsize="90%" minsize="90%"  >(</mo><mi mathsize="90%"  >𝐗</mi><mo mathsize="90%"  >,</mo><msub
    ><mi mathsize="90%"  >x</mi><mrow ><mi mathsize="90%"  >m</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathsize="90%"  >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="90%"  >n</mi></mrow></msub><mo mathsize="90%"  >,</mo><msub ><mi mathsize="90%"  >x</mi><mrow
    ><mi mathsize="90%"  >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >a</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >x</mi></mrow></msub><mo
    maxsize="90%" minsize="90%"  >)</mo></mrow></mrow><mi mathsize="90%"  >s</mi></mfrac><mo
    maxsize="210%" minsize="210%"  >)</mo></mrow></mrow><mo mathsize="90%"  >+</mo><mi
    mathsize="90%"  >z</mi></mrow><mo maxsize="90%" minsize="90%"  >)</mo></mrow><mo
    mathsize="90%"  >−</mo><mi mathsize="90%"  >z</mi></mrow><mo maxsize="90%" minsize="90%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo mathsize="90%"  >=</mo><mrow ><mi mathsize="90%"
    >s</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="210%" minsize="210%"  >(</mo><mrow
    ><mtext mathsize="90%" >四舍五入</mtext><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo maxsize="210%" minsize="210%"  >(</mo><mfrac ><mrow ><mtext mathsize="90%"
    >限制</mtext><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo maxsize="90%" minsize="90%"  >(</mo><mi
    mathsize="90%"  >𝐗</mi><mo mathsize="90%"  >,</mo><msub ><mi mathsize="90%"  >x</mi><mrow
    ><mi mathsize="90%"  >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >n</mi></mrow></msub><mo
    mathsize="90%"  >,</mo><msub ><mi mathsize="90%"  >x</mi><mrow ><mi mathsize="90%"  >m</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >a</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathsize="90%"  >x</mi></mrow></msub><mo maxsize="90%"
    minsize="90%"  >)</mo></mrow></mrow><mi mathsize="90%"  >s</mi></mfrac><mo maxsize="210%"
    minsize="210%"  >)</mo></mrow></mrow><mo maxsize="210%" minsize="210%"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><ci >^</ci><ci  >𝐗</ci></apply><apply
    ><ci ><mtext mathsize="90%" >伪量化</mtext></ci><ci >𝐗</ci></apply></apply><apply
    ><apply  ><ci ><mtext mathsize="90%" >去量化</mtext></ci><apply ><ci ><mtext mathsize="90%"
    >量化</mtext></ci><ci >𝐗</ci></apply></apply></apply><apply ><apply  ><ci >𝑠</ci><apply
    ><apply ><apply  ><ci ><mtext mathsize="90%" >四舍五入</mtext></ci><apply ><apply
    ><ci  ><mtext mathsize="90%"  >限制</mtext></ci><vector ><ci >𝐗</ci><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><apply ><ci >𝑚</ci><ci >𝑖</ci><ci
    >𝑛</ci></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><apply ><ci >𝑚</ci><ci >𝑎</ci><ci >𝑥</ci></apply></apply></vector></apply><ci
    >𝑠</ci></apply></apply><ci >𝑧</ci></apply><ci >𝑧</ci></apply></apply></apply><apply
    ><apply ><ci  >𝑠</ci><apply ><ci ><mtext mathsize="90%" >四舍五入</mtext></ci><apply
    ><apply  ><ci ><mtext mathsize="90%"  >限制</mtext></ci><vector ><ci >𝐗</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><apply ><ci >𝑚</ci><ci
    >𝑖</ci><ci >𝑛</ci></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><apply ><ci >𝑚</ci><ci >𝑎</ci><
- en: Since the above equation is not directly differentiable because of the rounding
    behavior, to optimize a loss function $L$ w.r.t. $\mathbf{X}$, we can compute
    $\frac{\displaystyle\partial{L}}{\displaystyle\partial{\mathbf{X}}}$ by chain-rule
    using the Straight-Through Estimator (STE) (Bengio et al., [2013](#bib.bib23)).
    This allows us to make the staircase function differentiable with a linear approximation
    (See (Krishnamoorthi, [2018](#bib.bib91)) for details).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述方程因舍入行为而不可直接微分，为了优化相对于$\mathbf{X}$的损失函数$L$，我们可以使用直通估计器（STE）（Bengio等，[2013](#bib.bib23)）通过链式法则计算$\frac{\displaystyle\partial{L}}{\displaystyle\partial{\mathbf{X}}}$。这使我们可以通过线性近似使阶梯函数可微分（有关详细信息，请参见
    (Krishnamoorthi, [2018](#bib.bib91) )）。
- en: Quantization-Aware Training allows the network to adapt to tolerate the noise
    introduced by the clamping and rounding behavior during inference. Once the network
    is trained, tools such as the TFLite Model Converter (Authors, [2021h](#bib.bib16))
    can generate the appropriate fixed-point inference model from a network annotated
    with the quantization nodes.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 量化感知训练使网络能够适应推理过程中由钳制和舍入行为引入的噪声。一旦网络训练完成，工具如TFLite模型转换器（作者，[2021h](#bib.bib16)）可以从标注了量化节点的网络生成适当的定点推理模型。
- en: 'Other Notable Works: Polino et al. (Polino et al., [2018](#bib.bib125)) allow
    non-uniform distribution of precision with learning a vector of quantization-points
    $p$, along with using distillation to further reduce loss of accuracy. The results
    for simpler datasets like CIFAR-10 are comparable to (Krishnamoorthi, [2018](#bib.bib91);
    Jacob et al., [2018](#bib.bib83)). However, when working with ResNet architecture
    on the ImageNet dataset, they achieve lower model size and faster inference by
    using shallower student networks. This is not a fair comparison, since other works
    do not mix distillation along with quantization. Fan et al. (Fan et al., [2020](#bib.bib56))
    demonstrate accuracy improvement on top of standard QAT ((Jacob et al., [2018](#bib.bib83)))
    with $b<8$. They hypothesize that the networks will learn better if the fake-quantization
    is not applied to the complete tensor at the same time to allow unbiased gradients
    to flow (instead of the STE approximation). Instead, they apply the fake-quantization
    operation stochastically in a block-wise manner on the given tensor. They also
    demonstrate improvements over QAT on 4-bit quantized Transformer and EfficientNet
    (Tan et al., [2019](#bib.bib148)) networks.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 其他显著工作：Polino等（Polino等，[2018](#bib.bib125)）允许通过学习量化点向量$p$来实现精度的非均匀分布，并使用蒸馏进一步减少精度损失。对于CIFAR-10等简单数据集的结果与(Krishnamoorthi,
    [2018](#bib.bib91); Jacob等，[2018](#bib.bib83)) 相当。然而，当在ImageNet数据集上使用ResNet架构时，他们通过使用较浅的学生网络来实现更低的模型大小和更快的推理。这并不是一个公平的比较，因为其他工作没有将蒸馏与量化混合在一起。Fan等（Fan等，[2020](#bib.bib56)）展示了在标准QAT（（Jacob等，[2018](#bib.bib83)））基础上的准确度提升，且$b<8$。他们假设如果假量化不是同时应用于整个张量，而是允许无偏的梯度流动（而不是STE近似），网络将会学习得更好。相反，他们在给定张量上以块状方式随机应用假量化操作。他们还展示了在4位量化的Transformer和EfficientNet（Tan等，[2019](#bib.bib148)）网络上对QAT的改进。
- en: 'Results: Refer to Table [2](#S3.T2 "Table 2 ‣ 3.1.2\. Quantization ‣ 3.1\.
    Compression Techniques ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    for a comparison between the baseline floating-point model, post-training quantized,
    and quantization-aware trained models (Authors, [2021f](#bib.bib14)). The model
    with post-training quantization gets close to the baseline, but there is still
    a significant accuracy difference. The model size is $4\times$ smaller, however
    the latency is slightly higher due to the need to dequantize the weights during
    inference. The model with 8-bit Quantization-Aware Training (QAT) gets quite close
    to the baseline floating point model while requiring $4\times$ less disk space
    and being $1.64\times$ faster.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '结果：请参阅表[2](#S3.T2 "Table 2 ‣ 3.1.2\. Quantization ‣ 3.1\. Compression Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") 比较基准浮点模型、后训练量化模型和量化感知训练模型（作者，[2021f](#bib.bib14)）。后训练量化模型接近基准模型，但仍存在显著的准确度差异。模型大小减少了$4\times$，但由于推理过程中需要对权重进行去量化，延迟略有增加。具有8位量化感知训练（QAT）的模型在所需磁盘空间上减少了$4\times$，且比基准浮点模型快$1.64\times$。'
- en: '| Model Architecture | Quantization Type | Top-1 Accuracy | Size (MB) | Latency
    (ms, Pixel2) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 模型架构 | 量化类型 | Top-1 准确度 | 大小 (MB) | 延迟 (ms, Pixel2) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| MobileNet v2-1.0 (224) | Baseline | 71.9% | 14 | 89 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet v2-1.0 (224) | 基线 | 71.9% | 14 | 89 |'
- en: '| Post-Training Quantization | 63.7% | 3.6 | 98 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 后训练量化 | 63.7% | 3.6 | 98 |'
- en: '| Quantization-Aware Training | 70.9% | 3.6 | 54 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 量化感知训练 | 70.9% | 3.6 | 54 |'
- en: Table 2\. A sample of various quantization results on the MobileNet v2 architecture
    for 8-bit quantization (TensorFlow, [2021](#bib.bib151)). We picked results on
    8-bit, since from they can be readily used with hardware and software that exists
    today.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. MobileNet v2架构上8位量化的各种量化结果示例（TensorFlow, [2021](#bib.bib151)）。我们选择8位结果，因为它们可以与现有的硬件和软件直接使用。
- en: '![Refer to caption](img/2d9889535c7e3fe8c6c2e8f54c07c036.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2d9889535c7e3fe8c6c2e8f54c07c036.png)'
- en: (a) Quantization-Aware Training
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 量化感知训练
- en: '![Refer to caption](img/292b3ab51f8eec3f51e4b46e10985788.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/292b3ab51f8eec3f51e4b46e10985788.png)'
- en: (b) Final fixed-point inference graph
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 最终固定点推理图
- en: Figure 6\. (a) shows the injection of fake-quantization nodes to simulate quantization
    effect and collecting tensor statistics, for exporting a fully fixed-point inference
    graph. (b) shows the inference graph derived from the same graph as (a). Inputs
    and weights are in uint8, and results of common operations are in uint32. Biases
    are kept in uint32 (Jacob et al., [2018](#bib.bib83); Krishnamoorthi, [2018](#bib.bib91))
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. (a) 显示了注入伪量化节点以模拟量化效果并收集张量统计数据，以导出一个完全固定点推理图。(b) 显示了从与(a)相同的图中推导出的推理图。输入和权重为uint8，常见操作的结果为uint32。偏置保留为uint32
    (Jacob et al., [2018](#bib.bib83); Krishnamoorthi, [2018](#bib.bib91))
- en: .
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: 'Discussion:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：
- en: •
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Quantization is a well-studied technique for model optimization and can help
    with very significant reduction in model size (often $4\times$ when using 8-bit
    quantization) and inference latency.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化是一种研究充分的模型优化技术，可以显著减少模型大小（使用8位量化时通常为$4\times$）和推理延迟。
- en: •
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Weight quantization is straight-forward enough that it can be implemented by
    itself for reducing model size. Activation quantization should be strongly considered
    because it enables both latency reduction, as well as lower working memory required
    for intermediate computations in the model (which is essential for devices with
    low memory availability)
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 权重量化足够直接，可以单独实施以减少模型大小。应强烈考虑激活量化，因为它不仅能降低延迟，还能减少模型中间计算所需的工作内存（这对于内存有限的设备至关重要）
- en: •
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When possible, Quantization-Aware Training should be used. It has been shown
    to dominate post-training quantization in terms of accuracy.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在可能的情况下，应使用量化感知训练。研究表明，它在准确性方面优于后训练量化。
- en: •
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: However, tools like Tensorflow Lite have made it easy to rely on post-training
    quantization. (TensorFlow, [2019](#bib.bib150)) shows that often there is minimal
    loss when using post-training quantization, and with the help of a representative
    dataset this is further shrunk down. Wherever there is an opportunity for switching
    to fixed-point operations, the infrastructure allows using them.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，像Tensorflow Lite这样的工具使得依赖后训练量化变得容易。（TensorFlow, [2019](#bib.bib150)）显示，使用后训练量化时通常损失最小，并且在代表性数据集的帮助下，这种损失会进一步缩减。在有机会切换到固定点操作的地方，基础设施允许使用它们。
- en: •
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For performance reasons, it is best to consider the common operations that follow
    a typical layer such as Batch-Norm, Activation, etc. and ‘fold’ them in the quantization
    operations.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 出于性能原因，最好考虑跟随典型层的常见操作，如批量归一化、激活等，并将它们“折叠”到量化操作中。
- en: 3.1.3\. Other Compression Techniques
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. 其他压缩技术
- en: There are other compression techniques like Low-Rank Matrix Factorization, K-Means
    Clustering, Weight-Sharing etc. which are also actively being used for model compression
    (Panigrahy, [2021](#bib.bib118)) and might be suitable for further compressing
    hotspots in a model.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他压缩技术，如低秩矩阵分解、K均值聚类、权重共享等，这些技术也在积极用于模型压缩（Panigrahy, [2021](#bib.bib118)），并可能适合进一步压缩模型中的热点。
- en: 3.2\. Learning Techniques
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 学习技巧
- en: Learning techniques try to train a model differently in order to obtain better
    quality metrics (accuracy, F1 score, precision, recall, etc.) while allowing supplementing,
    or in some cases replacing the traditional supervised learning. The improvement
    in quality can sometimes be traded off for a smaller footprint by reducing the
    number of parameters / layers in the model and achieving the same baseline quality
    with a smaller model. An incentive of paying attention to learning techniques
    is that they are applied only on the training, without impacting the inference.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 学习技术试图以不同的方式训练模型，以获得更好的质量指标（准确率、F1 分数、精度、召回率等），同时允许补充或在某些情况下替代传统的监督学习。通过减少模型中的参数/层数，质量的改善有时可以换取更小的模型，从而在更小的模型中达到相同的基准质量。关注学习技术的一个动机是它们仅在训练阶段应用，而不会影响推断。
- en: 3.2.1\. Distillation
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 蒸馏
- en: Ensembles are well known to help with generalization (Krogh and Vedelsby, [1994](#bib.bib94);
    Hansen and Salamon, [1990](#bib.bib72)). The intuition is that this enables learning
    multiple independent hypotheses, which are likely to be better than learning a
    single hypothesis. (Dietterich, [2000](#bib.bib49)) goes over some of the standard
    ensembling methods such as bagging (learning models that are trained on non-overlapping
    data and then ensembling them), boosting (learning models that are trained to
    fix the classification errors of other models in the ensemble), averaging (voting
    by all the ensemble models), etc.Bucila et al. (Buciluǎ et al., [2006](#bib.bib28))
    used large ensembles to label synthetic data that they generated using various
    schemes. A smaller neural net is then trained to learn not just from the labeled
    data but also from this weakly labeled synthetic data. They found that single
    neural nets were able to mimic the performance of larger ensembles, while being
    $1000\times$ smaller and faster. This demonstrated that it is possible to transfer
    the cumulative knowledge of ensembles to a single small model. Though it might
    not be sufficient to rely on just the existing labeled data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法广为人知，有助于提高泛化能力（Krogh 和 Vedelsby，[1994](#bib.bib94)；Hansen 和 Salamon，[1990](#bib.bib72)）。直觉上，这使得学习多个独立的假设成为可能，这些假设可能比学习单一假设更为有效。（Dietterich，[2000](#bib.bib49)）介绍了一些标准的集成方法，如
    bagging（训练在非重叠数据上的模型然后进行集成），boosting（训练修正集成中其他模型分类错误的模型），平均（所有集成模型投票）等。Bucila
    等（Buciluǎ 等，[2006](#bib.bib28)）使用大型集成标记他们用各种方案生成的合成数据。然后训练一个较小的神经网络，不仅从标记数据中学习，还从这些弱标记的合成数据中学习。他们发现，单个神经网络能够模仿大型集成的性能，同时体积小了$1000\times$，速度更快。这证明了将集成的累积知识转移到一个小模型是可能的。尽管如此，单靠现有标记数据可能还不够。
- en: Hinton et al. (Hinton et al., [2015](#bib.bib76)), in their seminal work explored
    how smaller networks (students) can be taught to extract ‘dark knowledge’ from
    larger models / ensembles of larger models (teachers) in a slightly different
    manner. Instead of having to generate synthetic-data, they use the larger teacher
    model to generate *soft-labels* on existing labeled data. The soft-labels assign
    a probability to each class, instead of hard binary values in the original data.
    The intuition is that these soft-labels capture the relationship between the different
    classes which the model can learn from. For example, a truck is more similar to
    a car than to an apple, which the model might not be able to learn directly from
    hard labels.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Hinton 等（Hinton 等，[2015](#bib.bib76)）在他们的开创性工作中探索了如何以稍微不同的方式让较小的网络（学生）从较大的模型/模型集成（教师）中提取“暗知识”。他们不再需要生成合成数据，而是使用较大的教师模型在现有标记数据上生成*软标签*。软标签为每个类别分配一个概率，而不是原始数据中的硬二值。直觉上，这些软标签捕捉了不同类别之间的关系，模型可以从中学习。例如，卡车比苹果更像汽车，模型可能无法直接从硬标签中学习到这一点。
- en: 'The student network learns to minimize the cross-entropy loss on these soft
    labels, along with the original ground-truth hard labels. Since the probabilities
    of the incorrect classes might be very small, the logits are scaled down by a
    ‘temperature’ value $\geq 1.0$, so that the distribution is ‘softened’. If the
    input vector is $\mathbf{X}$, and the teacher model’s logits are $\mathbf{Z^{(t)}}$,
    the teacher model’s softened probabilities with temperature $T$ can be calculated
    as follows using the familiar softmax function:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 学生网络学习在这些软标签上最小化交叉熵损失，同时还包括原始的硬标签。由于错误类别的概率可能非常小，logits 会通过一个 $\geq 1.0$ 的“温度”值进行缩放，以使分布“软化”。如果输入向量是
    $\mathbf{X}$，教师模型的 logits 是 $\mathbf{Z^{(t)}}$，那么使用熟悉的 softmax 函数可以计算出教师模型在温度
    $T$ 下的软化概率：
- en: '| (4) |  | $\small\mathbf{Y}_{i}^{(t)}=\frac{\displaystyle\exp(\mathbf{Z_{i}^{(t)}}/T)}{\displaystyle\sum_{j=1}^{n}\exp(\mathbf{Z_{j}^{(t)}}/T)}$
    |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\small\mathbf{Y}_{i}^{(t)}=\frac{\displaystyle\exp(\mathbf{Z_{i}^{(t)}}/T)}{\displaystyle\sum_{j=1}^{n}\exp(\mathbf{Z_{j}^{(t)}}/T)}$
    |  |'
- en: Note that as $T$ increases, the relative differences between the various elements
    of $Y^{(t)}$ decreases. This happens because if all elements are divided by the
    same constant, the softmax function would lead to a larger drop for the bigger
    values. Hence, as the temperature $T$ increases, we see the distribution of $Y^{(t)}$
    ‘soften’ further.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，随着 $T$ 的增加，$Y^{(t)}$ 各个元素之间的相对差异减小。这是因为如果所有元素都被相同的常数除，softmax 函数会导致较大值的下降更大。因此，随着温度
    $T$ 的增加，我们看到 $Y^{(t)}$ 的分布进一步“软化”。
- en: 'When training along with labeled data ($\mathbf{X}$, $\mathbf{Y}$), and the
    student model’s output ($\mathbf{Y^{(s)}}$), we can describe the loss function
    as:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当与标记数据 ($\mathbf{X}$, $\mathbf{Y}$) 和学生模型的输出 ($\mathbf{Y^{(s)}}$) 一起训练时，我们可以描述损失函数为：
- en: '| (5) |  | $\small\begin{split}L&amp;=\lambda_{1}\cdot L_{\rm ground-truth}+\lambda_{2}\cdot
    L_{\rm distillation}\\ &amp;=\lambda_{1}\cdot\textrm{CrossEntropy}(\mathbf{Y},\mathbf{Y^{(s)}};\theta)+\lambda_{2}\cdot\textrm{CrossEntropy}(\mathbf{Y^{(t)}},\mathbf{Y^{(s)}};\theta)\end{split}$
    |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\small\begin{split}L&amp;=\lambda_{1}\cdot L_{\rm ground-truth}+\lambda_{2}\cdot
    L_{\rm distillation}\\ &amp;=\lambda_{1}\cdot\textrm{CrossEntropy}(\mathbf{Y},\mathbf{Y^{(s)}};\theta)+\lambda_{2}\cdot\textrm{CrossEntropy}(\mathbf{Y^{(t)}},\mathbf{Y^{(s)}};\theta)\end{split}$
    |  |'
- en: 'CrossEntropy is the cross-entropy loss function, which takes in the labels
    and the output. For the first loss term, we pass along the ground truth labels,
    and for the second loss term we pass the corresponding soft labels from the teacher
    model for the same input. $\lambda_{1}$ and $\lambda_{2}$ control the relative
    importance of the standard ground truth loss and the distillation loss respectively.
    When $\lambda_{1}=0$, the student model is trained with just the distillation
    loss. Similarly, when $\lambda_{2}=0$, it is equivalent to training with just
    the ground-truth labels. Usually, the teacher network is pre-trained and frozen
    during this process, and only the student network is updated. Refer to Figure
    [7](#S3.F7 "Figure 7 ‣ 3.2.1\. Distillation ‣ 3.2\. Learning Techniques ‣ 3\.
    Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better") for an illustration of this
    process.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 'CrossEntropy 是交叉熵损失函数，它接受标签和输出。对于第一个损失项，我们传递真实标签，对于第二个损失项，我们传递相应的来自教师模型的软标签。
    $\lambda_{1}$ 和 $\lambda_{2}$ 分别控制标准真实损失和蒸馏损失的相对重要性。当 $\lambda_{1}=0$ 时，学生模型仅用蒸馏损失进行训练。类似地，当
    $\lambda_{2}=0$ 时，这等同于仅用真实标签进行训练。通常，教师网络在此过程中是预训练并被冻结的，只有学生网络会被更新。有关此过程的示意图，请参见图
    [7](#S3.F7 "Figure 7 ‣ 3.2.1\. Distillation ‣ 3.2\. Learning Techniques ‣ 3\.
    Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better")。'
- en: '![Refer to caption](img/88ff33534ccce386d9eb2316a948b12a.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/88ff33534ccce386d9eb2316a948b12a.png)'
- en: Figure 7\. Distillation of a smaller student model from a larger pre-trained
    teacher model. Both the teacher and student models receive the same input. The
    teacher is used to generate ‘soft-labels’ for the student, which gives the student
    more information than just hard binary labels. The student is trained using the
    regular cross-entropy loss with the hard labels, as well as using the distillation
    loss function which uses the soft labels from the teacher. In this setting, the
    teacher is frozen, and only the student receives the gradient updates.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 从较大的预训练教师模型中蒸馏出较小的学生模型。教师和学生模型接收相同的输入。教师用于生成学生的“软标签”，这为学生提供了比仅使用硬二进制标签更多的信息。学生使用常规的交叉熵损失进行训练，同时使用来自教师的软标签的蒸馏损失函数。在这种设置下，教师被冻结，只有学生接收梯度更新。
- en: In the paper, Hinton et al. (Hinton et al., [2015](#bib.bib76)) were able to
    closely match the accuracy of a 10 model ensemble for a speech recognition task
    with a single distilled model. Urban et al. (Urban et al., [2016](#bib.bib153))
    did a comprehensive study demonstrating that distillation significantly improves
    performance of shallow student networks as small as an MLP with one hidden layer
    on tasks like CIFAR-10\. Sanh et al. (Sanh et al., [2019](#bib.bib135)) use the
    distillation loss for compressing a BERT (Devlin et al., [2018](#bib.bib48)) model
    (along with a cosine loss that minimizes the cosine distance between two internal
    vector representation of the input as seen by the teacher and student models).
    Their model retains 97% of the performance of BERT-Base while being 40% smaller
    and 60% faster on CPU.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，Hinton 等人（Hinton 等人, [2015](#bib.bib76)）能够使一个单一的蒸馏模型在语音识别任务中准确度接近 10 模型集成的准确度。Urban
    等人（Urban 等人, [2016](#bib.bib153)）进行了全面的研究，证明蒸馏显著提高了浅层学生网络的性能，甚至对于像 CIFAR-10 这样的任务仅一个隐藏层的
    MLP。Sanh 等人（Sanh 等人, [2019](#bib.bib135)）使用蒸馏损失压缩 BERT（Devlin 等人, [2018](#bib.bib48)）模型（以及最小化教师和学生模型所见输入的两个内部向量表示之间的余弦距离的余弦损失）。他们的模型保留了
    BERT-Base 97% 的性能，同时在 CPU 上体积缩小 40% 和速度提升 60%。
- en: It is possible to adapt the general idea of distillation to work on intermediate
    outputs of teachers and students. Zagoruyko et al. (Zagoruyko and Komodakis, [2016](#bib.bib166))
    transfer intermediate ‘attention maps’ between teacher and student convolutional
    networks. The intuition is to make the student focus on the parts of the image
    where the teacher is paying attention to. MobileBERT (Sun et al., [2020](#bib.bib145))
    uses a progressive-knowledge transfer strategy where they do layer-wise distillation
    between the BERT student and teacher models, but they do so in stages, where the
    first $l$ layers are distilled in the $l$-th stage. Along with other architecture
    improvements, they obtain a 4.3$\times$ smaller and 5.5$\times$ faster BERT with
    small losses in quality.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将蒸馏的总体思路应用于教师和学生的中间输出。Zagoruyko 等人（Zagoruyko 和 Komodakis, [2016](#bib.bib166)）在教师和学生卷积网络之间传递中间的“注意力图”。直观上，目的是让学生关注教师关注的图像部分。MobileBERT（Sun
    等人, [2020](#bib.bib145)）使用了一种渐进式知识传递策略，其中在 BERT 学生和教师模型之间进行逐层蒸馏，但这是分阶段进行的，首先 $l$
    层在第 $l$ 阶段进行蒸馏。结合其他架构改进，他们获得了一个体积缩小 4.3$\times$ 和速度提升 5.5$\times$ 的 BERT，且质量损失很小。
- en: Another idea that has been well explored is exploiting a model trained in a
    supervised training to label unlabeled data. Blum et al. (Blum and Mitchell, [1998](#bib.bib25))
    in their paper from 1998, report halving the error rate of their classifiers by
    retraining on a subset of pseudo-labels generated using the previous classifiers.
    This has been extended through distillation to use the teacher model to label
    a large corpus of unlabeled data, which can then be used to improve the quality
    of the student model (Menghani and Ravi, [2019](#bib.bib110); Xie et al., [2020](#bib.bib162);
    Yalniz et al., [2019](#bib.bib163)).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个已被充分探索的想法是利用在监督训练中训练的模型对未标记数据进行标记。Blum 等人（Blum 和 Mitchell, [1998](#bib.bib25)）在他们1998年的论文中报告，通过在使用先前分类器生成的伪标签子集上重新训练，将分类器的错误率降低了一半。这种方法通过蒸馏得到了扩展，利用教师模型对大量未标记的数据进行标记，然后可以用来提高学生模型的质量（Menghani
    和 Ravi, [2019](#bib.bib110)；Xie 等人, [2020](#bib.bib162)；Yalniz 等人, [2019](#bib.bib163)）。
- en: Overall, distillation has been empirically shown to improve both the accuracy
    as well as the speed of convergence of student models across many domains. Hence,
    it enables training smaller models which might otherwise not be have an acceptable
    quality for deployment.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，蒸馏在许多领域的学生模型的准确性和收敛速度上都有实证证明可以提高。因此，它使得训练较小的模型成为可能，而这些模型可能在部署时的质量无法接受。
- en: 'Discussion:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：
- en: •
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Distillation is an adaptable technique that needs minimal changes in the training
    infrastructure to be used. Even if the teacher model cannot be executed at the
    same time as the student model, the teacher model’s predictions can be collected
    offline and treated as another source of labels.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 蒸馏是一种适应性强的技术，使用时对训练基础设施的修改最小。即使教师模型无法与学生模型同时执行，也可以离线收集教师模型的预测，并将其视为另一种标签来源。
- en: •
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When there is sufficient label data, there is ample evidence that distillation
    is likely to improve the student model’s predictions. If there is a large corpus
    of unlabeled data, the teacher model can be used to generate pseudo-labels on
    the unlabeled data, which can further improve the student model’s accuracy.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当有足够的标签数据时，有充分证据表明蒸馏可能会改善学生模型的预测。如果有大量未标记的数据，教师模型可以用来在未标记数据上生成伪标签，这可以进一步提高学生模型的准确性。
- en: •
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Strategies for intermediate-layer distillation have also shown to be effective
    in the case of complex networks. In such scenarios, a new loss term minimizing
    the difference between the outputs of the two networks at some semantically identical
    intermediate point(s) needs to be added.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于复杂网络，层间蒸馏策略也已被证明有效。在这种情况下，需要添加一个新的损失项，以最小化两个网络在某些语义上相同的中间点的输出差异。
- en: 3.2.2\. Data Augmentation
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 数据增强
- en: When training large models for complex tasks in a supervised learning regime,
    the size of the training data corpus correlates with improvement in generalization.
    (Sun et al., [2017](#bib.bib144)) demonstrates logarithmic increase in the prediction
    accuracy with increase in the number of labeled examples. However, getting high-quality
    labeled data often requires a human in the loop and could be expensive.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习环境中训练大型模型时，训练数据集的大小与泛化能力的提升相关联。（Sun 等人，[2017](#bib.bib144)）展示了随着标记样本数量的增加，预测准确性以对数方式增长。然而，获取高质量的标记数据通常需要人类参与，并且可能成本高昂。
- en: Data Augmentation is a nifty way of addressing the scarcity of labeled data,
    by synthetically inflating the existing dataset through some *augmentation methods*.
    These augmentation methods are transformations that can be applied cheaply on
    the given examples, such that the new label of the augmented example does not
    change, or can be cheaply inferred. As an example, consider the classical image
    classification task of labeling a given image to be a cat or a dog. Given an image
    of a dog, translating the image horizontally / vertically by a small number of
    pixels, rotating it by a small angle, etc. would not materially change the image,
    so the transformed image should still be labeled as ‘dog’ by the classifier. This
    forces the classifier to learn a robust representation of the image that generalizes
    better across these transformations.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是一种巧妙的方法，用于通过一些*增强方法*合成地扩充现有数据集，以解决标记数据的稀缺问题。这些增强方法是可以廉价地应用于给定示例的变换，使得增强示例的新标签不会改变，或可以廉价地推断。例如，考虑给定图像被标记为猫或狗的经典图像分类任务。给定一张狗的图片，通过水平/垂直平移少量像素、旋转小角度等方式转换图像，不会实质性地改变图像，因此转换后的图像仍应被分类器标记为“狗”。这迫使分类器学习一个对这些变换具有更好泛化能力的稳健表示。
- en: The transformations as described above have long been demonstrated to improve
    accuracy of convolutional networks (Simard et al., [2003](#bib.bib141); Cireşan
    et al., [2011](#bib.bib37)). They have also been a core part of seminal works
    in Image Classification. A prime example is AlexNet (Krizhevsky et al., [2012](#bib.bib93)),
    where such transformations were used to increase the effective size of the training
    dataset by 2048 $\times$, which won the ImageNet competition in 2012\. Since then
    it has became common to use such transformations for Image Classification models
    (Inception (Szegedy et al., [2015](#bib.bib147)), XCeption (Chollet, [2017](#bib.bib35)),
    ResNet (He et al., [2016](#bib.bib74)), etc.).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 上述变换已被长期证明能够提高卷积网络的准确性（Simard et al., [2003](#bib.bib141); Cireşan et al., [2011](#bib.bib37)）。它们也是图像分类领域开创性工作的核心部分。一个典型的例子是
    AlexNet（Krizhevsky et al., [2012](#bib.bib93)），在这个模型中，这些变换被用来将训练数据集的有效大小增加了 2048
    $\times$，并赢得了 2012 年的 ImageNet 比赛。从那时起，使用这些变换进行图像分类模型（如 Inception（Szegedy et al.,
    [2015](#bib.bib147)）、XCeption（Chollet, [2017](#bib.bib35)）、ResNet（He et al., [2016](#bib.bib74)）等）变得很常见。
- en: 'We can categorize data-augmentation methods as follows (also refer to Figure
    [8](#S3.F8 "Figure 8 ‣ 3.2.2\. Data Augmentation ‣ 3.2\. Learning Techniques ‣
    3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on
    Making Deep Learning Models Smaller, Faster, and Better")):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以将数据增强方法分类如下（另见图 [8](#S3.F8 "Figure 8 ‣ 3.2.2\. Data Augmentation ‣ 3.2\.
    Learning Techniques ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep
    Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")）：'
- en: •
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Label-Invariant Transformations: These are some of the most common transformations,
    where the transformed example retains the original label. These can include simple
    geometric transformations such as translation, flipping, cropping, rotation, distortion,
    scaling, shearing, etc. However the user has to verify the label-invariance property
    with each transformation for the specific task at hand.'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标签不变性变换：这些是最常见的变换之一，其中变换后的样本保留了原始标签。这些变换包括简单的几何变换，如平移、翻转、裁剪、旋转、扭曲、缩放、剪切等。然而，用户需要根据特定任务验证每种变换的标签不变性特性。
- en: •
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Label-Mixing Transformations: Transformations such as Mixup (Zhang et al.,
    [2017](#bib.bib167)), mix inputs from two different classes in a weighted manner
    and treat the label to be a correspondingly weighted combination of the two classes
    (in the same ratio). The intuition is that the model should be able to extract
    out features that are relevant for both the classes. Other transformations like
    Sample Pairing also seem to help (Inoue, [2018](#bib.bib82)).'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标签混合变换：像 Mixup（Zhang et al., [2017](#bib.bib167)）这样的变换，通过加权的方式混合来自两个不同类别的输入，并将标签视为这两个类别的对应加权组合（按照相同比例）。直观上，模型应该能够提取出对两个类别都相关的特征。其他变换，如样本配对，也似乎有帮助（Inoue,
    [2018](#bib.bib82)）。
- en: •
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data-Dependent Transformations: In this case, transformations are chosen such
    that they maximize the loss for that example (Fawzi et al., [2016](#bib.bib57)),
    or are adversarially chosen so as to fool the classifier (Gopalan et al., [2021](#bib.bib68)).'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据依赖性变换：在这种情况下，变换的选择旨在最大化该样本的损失（Fawzi et al., [2016](#bib.bib57)），或被对抗性选择以迷惑分类器（Gopalan
    et al., [2021](#bib.bib68)）。
- en: •
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Synthetic Sampling: These methods synthetically create new training examples.
    Algorithms like SMOTE (Chawla et al., [2002](#bib.bib31)) allow re-balancing the
    dataset to make up for skew in the datasets, and GANs can be used to synthetically
    create new samples (Zhu and Gupta, [2018](#bib.bib168)) to improve model accuracy.'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 合成采样：这些方法合成地创建新的训练样本。像 SMOTE（Chawla et al., [2002](#bib.bib31)）这样的算法可以重新平衡数据集，以弥补数据集中的偏斜，而
    GANs 可以用来合成地创建新样本（Zhu and Gupta, [2018](#bib.bib168)），以提高模型准确性。
- en: •
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Composition of Transformations: These are transformations that are themselves
    composed of other transformations, and the labels are computed depending on the
    nature of transformations that stacked.'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 变换组合：这些是由其他变换组成的变换，标签的计算取决于堆叠的变换的性质。
- en: '![Refer to caption](img/4eeb88bff5b03fad44b18c8bbfc30034.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/4eeb88bff5b03fad44b18c8bbfc30034.png)'
- en: 'Figure 8\. Some common types of data augmentations. Source: (Li, [2020](#bib.bib103))'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 一些常见的数据增强类型。来源：(Li, [2020](#bib.bib103))
- en: '| Transformation | Validation Accuracy Improvement (%) |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Transformation | Validation Accuracy Improvement (%) |'
- en: '| --- | --- |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| rotate | 1.3 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| rotate | 1.3 |'
- en: '| shear-x | 0.9 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| shear-x | 0.9 |'
- en: '| shear-y | 0.9 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| shear-y | 0.9 |'
- en: '| translate-x | 0.4 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| translate-x | 0.4 |'
- en: '| translate-y | 0.4 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| translate-y | 0.4 |'
- en: '| sharpness | 0.1 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| sharpness | 0.1 |'
- en: '| autoContrast | 0.1 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| autoContrast | 0.1 |'
- en: 'Table 3\. A breakdown of the contribution of various transformations on the
    validation accuracy of a model trained on the CIFAR-10 dataset. Source: (Cubuk
    et al., [2020](#bib.bib45)).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 在 CIFAR-10 数据集上训练的模型的验证准确率中，各种变换的贡献的细分。来源：(Cubuk et al., [2020](#bib.bib45))。
- en: 'Discussion: Apart from Computer Vision, Data-Augmentation has also been used
    in NLP, and Speech. In NLP, a common idea that has been used is ‘back-translation’
    (Yu et al., [2018](#bib.bib164)) where augmented examples are created by training
    two translation models, one going from the source language to the target language,
    and the other going back from the target language to the original source language.
    Since the back-translation is not exact, this process is able to generate augmented
    samples for the given input. Other methods like WordDropout (Sennrich et al.,
    [2016](#bib.bib140)) stochastically set embeddings of certain words to zero. SwitchOut
    (Wang et al., [2018](#bib.bib159)) introduces a similarity measure to disallow
    augmentations that are too dissimilar to the original input. In Speech (Hannun
    et al., [2014](#bib.bib71)), the input audio samples are translated to the left
    / right before being passed to the decoder.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：除了计算机视觉，数据增强还被应用于 NLP 和语音处理。在 NLP 中，一个常见的想法是“回译” (Yu et al., [2018](#bib.bib164))，通过训练两个翻译模型生成增强样本，一个模型从源语言翻译到目标语言，另一个模型则从目标语言翻译回原始源语言。由于回译不是完全准确的，这一过程能够为给定的输入生成增强样本。其他方法如
    WordDropout (Sennrich et al., [2016](#bib.bib140)) 随机将某些词的嵌入设置为零。SwitchOut (Wang
    et al., [2018](#bib.bib159)) 引入了相似性度量，禁止那些与原始输入过于不同的增强。在语音处理中 (Hannun et al.,
    [2014](#bib.bib71))，输入的音频样本在传递到解码器之前被向左/右移动。
- en: While the augmentation policies are usually hand-tuned, there are also methods
    such as AutoAugment (Cubuk et al., [2019](#bib.bib44)) where the augmentation
    policy is learned through a Reinforcement-Learning (RL) based search, searching
    for the transformations to be applied, as well as their respective hyper-parameters.
    Though this is shown to improve accuracy, it is also complicated and expensive
    to setup a separate search for augmentation, taking as many as 15000 GPU hours
    to learn the optimal policy on ImageNet. The RandAugment (Cubuk et al., [2020](#bib.bib45))
    paper demonstrated that it is possible to achieve similar results while reducing
    the search space to just two hyper-parameters (number of augmentation methods,
    and the strength of the distortion) for a given model and dataset.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然增强策略通常是手工调整的，但也有像 AutoAugment (Cubuk et al., [2019](#bib.bib44)) 这样的算法，其中增强策略通过基于强化学习
    (RL) 的搜索来学习，搜索要应用的变换以及它们各自的超参数。尽管这已被证明可以提高准确性，但设置一个单独的搜索来进行增强也很复杂且昂贵，在 ImageNet
    上学习最佳策略可能需要多达 15000 GPU 小时。RandAugment (Cubuk et al., [2020](#bib.bib45)) 论文展示了在将搜索空间缩小到仅两个超参数（增强方法的数量和失真强度）的情况下，仍然可以实现类似的结果。
- en: Overall, we see that data-augmentation leads to better generalization of the
    given models. Some techniques can be specific for their domains RandAugment (Vision),
    BackTranslation and SwitchOut (NLP), etc. However, the core principles behind
    them make it likely that similar methods can be derived for other domains too
    (refer to our categorization of data-augmentation methods above).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来看，我们发现数据增强有助于提高所给模型的泛化能力。一些技术可能是针对特定领域的，如 RandAugment（视觉）、BackTranslation
    和 SwitchOut（NLP）等。然而，这些技术背后的核心原理使得类似的方法也可能在其他领域中得到应用（参见我们上面的数据增强方法分类）。
- en: 3.2.3\. Self-Supervised Learning
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 自监督学习
- en: The Supervised-Learning paradigm relies heavily on labeled data. As mentioned
    earlier, it requires human intervention, and is expensive as well. To achieve
    reasonable quality on a non-trivial task, the amount of labeled data requires
    is large too. While techniques like Data-Augmentation, Distillation etc., help,
    they too rely on the presence of some labeled data to achieve a baseline performance.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督学习范式高度依赖标记数据。如前所述，这需要人工干预，并且也很昂贵。为了在一个复杂的任务上达到合理的质量，所需的标记数据量也很大。尽管数据增强、蒸馏等技术有所帮助，但它们也依赖于一些标记数据来实现基准性能。
- en: 'Self-Supervised learning (SSL) avoids the need for labeled data to learn generalized
    representations, by aiming to extract more supervisory bits from each example.
    Since it focuses on learning robust representations of the example itself, it
    does not need to focus narrowly on the label. This is typically done by solving
    a *pretext task* where the model pretends that a part / structure of the input
    is missing and learns to predict it (Refer to Figure [9](#S3.F9 "Figure 9 ‣ 3.2.3\.
    Self-Supervised Learning ‣ 3.2\. Learning Techniques ‣ 3\. Landscape of Efficient
    Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better") for examples). Since unlabeled data is vast in many
    domains (Books, Wikipedia, and other text for NLP, Web Images & Videos for Computer
    Vision, etc.), the model would not be bottlenecked by data for learning to solve
    these pretext tasks.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '自监督学习（SSL）通过从每个示例中提取更多的监督信息，避免了对标记数据的需求，以学习泛化的表示。由于它专注于学习示例本身的鲁棒表示，因此不需要专注于标签。通常通过解决*前置任务*来完成，其中模型假装输入的某部分/结构缺失并学习预测它（参见图
    [9](#S3.F9 "Figure 9 ‣ 3.2.3\. Self-Supervised Learning ‣ 3.2\. Learning Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better")的示例）。由于许多领域的未标记数据非常丰富（书籍、维基百科和其他文本用于
    NLP，网页图像和视频用于计算机视觉等），模型在学习解决这些前置任务时不会受到数据瓶颈的限制。'
- en: '![Refer to caption](img/99ebb4d4aa1ba09a1925a9e674e1a23d.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/99ebb4d4aa1ba09a1925a9e674e1a23d.png)'
- en: 'Figure 9\. General theme of pretext tasks. Source: (LeCun, [2018](#bib.bib97))'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 前置任务的一般主题。来源：（LeCun，[2018](#bib.bib97)）
- en: Once the models learn generic representations that transfer well across tasks,
    they can be adapted to solve the target task by adding some layers that project
    the representation to the label space, and fine-tuning the model with the labeled
    data. Since the labeled data is not being used for learning rudimentary features,
    but rather how to map the high-level representations into the label space, the
    quantum of labeled data is going to be a fraction of what would have been required
    for training the model from scratch. From this lens, fine-tuning models pre-trained
    with Self-Supervised learning are *data-efficient* (they converge faster, attain
    better quality for the same amount of labeled data when compared to training from
    scratch, etc.) ((Howard and Ruder, [2018](#bib.bib79); Devlin et al., [2018](#bib.bib48))).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型学习到在任务间迁移良好的通用表示，它们可以通过添加一些层将表示投射到标签空间并使用标记数据对模型进行微调，从而适应目标任务。由于标记数据不是用于学习基础特征，而是用于将高层表示映射到标签空间，因此标记数据的数量将是从头开始训练模型所需的数量的一小部分。从这个角度来看，使用自监督学习进行预训练的模型是*数据高效*的（与从头开始训练相比，它们收敛更快，在相同数量的标记数据下达到更好的质量等）((Howard
    和 Ruder，[2018](#bib.bib79)；Devlin 等，[2018](#bib.bib48))）。
- en: '![Refer to caption](img/e5578f99c309d06d7e9ba218bdb137f2.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e5578f99c309d06d7e9ba218bdb137f2.png)'
- en: 'Figure 10\. Validation Error w.r.t. number of training examples for different
    training methods on IMDb (from scratch, ULMFiT Supervised: pre-training with WikiText-103
    and fine-tuning using labeled data, ULMFit Semi-Supervised: Pre-Training with
    WikiText-103 as well as unlabeled data from the target dataset and fine-tuning
    with labeled data). Source: (Howard and Ruder, [2018](#bib.bib79))'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 验证误差与 IMDb 上不同训练方法的训练样本数量的关系（从头开始，ULMFiT 有监督：使用 WikiText-103 进行预训练，并使用标记数据进行微调，ULMFit
    半监督：使用 WikiText-103 和目标数据集中的未标记数据进行预训练，并使用标记数据进行微调）。来源：（Howard 和 Ruder，[2018](#bib.bib79)）
- en: 'An example of this two step process of pre-training on unlabeled data and fine-tuning
    on labeled data has gained rapid acceptance in the NLP community. ULMFiT (Howard
    and Ruder, [2018](#bib.bib79)) pioneered the idea of training a general purpose
    language model, where the model learns to solve the pretext task of predicting
    the next word in a given sentence, without the neeof an associated label. The
    authors found that using a large corpus of preprocessed unlabeled data such as
    the WikiText-103 dataset (derived from English Wikipedia pages) was a good choice
    for the pre-training step. This was sufficient for the model to learn general
    properties about the language, and the authors found that fine-tuning such a pre-trained
    model for a binary classification problem (IMDb dataset) required only 100 labeled
    examples ($\approx 10\times$ less labeled examples otherwise). Refer to Figure
    [10](#S3.F10 "Figure 10 ‣ 3.2.3\. Self-Supervised Learning ‣ 3.2\. Learning Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better"). If we add a middle-step
    of pre-training using unlabeled data from the same target dataset, the authors
    report needing $\approx 20\times$ fewer labeled examples.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在未标记数据上进行预训练并在标记数据上进行微调的两步过程的示例已经在 NLP 社区得到了快速接受。ULMFiT（Howard 和 Ruder，[2018](#bib.bib79)）开创了训练通用语言模型的理念，其中模型学习解决预测给定句子中下一个单词的预任务，而无需关联标签。作者发现，使用大量预处理的未标记数据（如
    WikiText-103 数据集，来源于英文维基百科页面）是预训练步骤的一个不错选择。这对于模型学习语言的一般属性是足够的，作者发现，将这种预训练模型微调到二分类问题（IMDb
    数据集）仅需 100 个标记示例（大约少了 10 倍的标记示例）。参见图 [10](#S3.F10 "图 10 ‣ 3.2.3. 自监督学习 ‣ 3.2.
    学习技术 ‣ 3. 深度学习的现状 ‣ 高效深度学习：关于使深度学习模型更小、更快、更好的调查")。如果我们增加一个中间步骤，使用来自相同目标数据集的未标记数据进行预训练，作者报告说需要大约
    20 倍更少的标记示例。
- en: This idea of pre-training followed by fine-tuning is also used in BERT (Devlin
    et al., [2018](#bib.bib48)) (and other related models like GPT, RoBERTa, T5, etc.)
    where the pre-training steps involves learning to solve two tasks. Firstly, the
    Masked Language Model where about 15% of the tokens in the given sentence are
    masked and the model needs to predict the masked token. The second task is, given
    two sentences $A$ and $B$, predict if $B$ follows $A$. The pre-training loss is
    the mean of the losses for the two tasks. Once pre-trained the model can then
    be used for classification or seq2seq tasks by adding additional layers on top
    of the last hidden layer. When it was published, BERT beat the State-of-the-Art
    on eleven NLP tasks.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这种先预训练后微调的思想也被应用于 BERT（Devlin 等人，[2018](#bib.bib48)）（以及其他相关模型如 GPT、RoBERTa、T5
    等），其中预训练步骤包括学习解决两个任务。首先是掩码语言模型（Masked Language Model），在给定句子的约 15% 的标记被掩盖，模型需要预测被掩盖的标记。第二个任务是，给定两个句子
    $A$ 和 $B$，预测 $B$ 是否跟随 $A$。预训练损失是两个任务损失的均值。一旦预训练完成，模型就可以通过在最后一个隐藏层上添加额外层来用于分类或
    seq2seq 任务。发布时，BERT 在十一项 NLP 任务上超越了最先进的技术。
- en: 'Similar to NLP, the pretext tasks in Vision have been used to train models
    that learn general representations. (Doersch et al., [2015](#bib.bib50)) extracts
    two patches from a training example and then trains the model to predict their
    relative position in the image (Refer to Figure [11](#S3.F11 "Figure 11 ‣ 3.2.3\.
    Self-Supervised Learning ‣ 3.2\. Learning Techniques ‣ 3\. Landscape of Efficient
    Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better")(a)). They demonstrate that using a network pre-trained
    in this fashion improves the quality of the final object detection task, as compared
    to randomly initializing the network. Similarly, another task is to predict the
    degree of rotation for a given rotated image (Gidaris et al., [2018](#bib.bib60)).
    The authors report that the network trained in a self-supervised manner this way
    can be fine-tuned to perform nearly as well as a fully supervised network.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 与 NLP 类似，视觉中的预任务也被用来训练学习一般表示的模型。（Doersch 等人，[2015](#bib.bib50)）从训练示例中提取两个补丁，然后训练模型预测它们在图像中的相对位置（参见图
    [11](#S3.F11 "图 11 ‣ 3.2.3. 自监督学习 ‣ 3.2. 学习技术 ‣ 3. 深度学习的现状 ‣ 高效深度学习：关于使深度学习模型更小、更快、更好的调查")（a））。他们证明，以这种方式预训练的网络比随机初始化的网络在最终目标检测任务中的质量更高。类似地，另一个任务是预测给定旋转图像的旋转角度（Gidaris
    等人，[2018](#bib.bib60)）。作者报告称，以这种自监督方式训练的网络可以微调到与完全监督的网络几乎一样的效果。
- en: '![Refer to caption](img/cb75dc500ca9d5a594d1ee592301c566.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/cb75dc500ca9d5a594d1ee592301c566.png)'
- en: '(a) Detecting relative order of patches. Source: (Doersch et al., [2015](#bib.bib50)).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 检测补丁的相对顺序。来源：(Doersch 等, [2015](#bib.bib50))。
- en: '![Refer to caption](img/5133d594f68ecb7b1e044476c74a2b8c.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5133d594f68ecb7b1e044476c74a2b8c.png)'
- en: (b) Predicting the degree of rotation of a given image.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 预测给定图像的旋转角度。
- en: Figure 11\. Pretext tasks for vision problems.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 视觉问题的前置任务。
- en: Another common theme is Contrastive Learning, where the model is trained to
    distinguish between similar and dissimilar inputs. Frameworks such as SimCLR (Chen
    et al., [2020a](#bib.bib33); Chen et al., [2020b](#bib.bib34)), try to learn representations
    $h_{i}$ and $h_{j}$ for two given inputs $\tilde{x_{i}}$ and $\tilde{x_{j}}$,
    where the latter two are differently augmented views of the same input, such that
    the cosine similarity of the projections of $h_{i}$ and $h_{j}$, $z_{i}$ and $z_{j}$
    (using a separate function $g(.)$) can be maximized. Similarly, for dissimilar
    inputs the cosine similarity of $z_{i}$ and $z_{j}$ should be minimized. The authors
    report a Top-1 accuracy of $73.9\%$ on ImageNet with only 1% labels (13 labels
    per class), and outperform the ResNet-50 supervised baseline with only 10% labels.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见主题是对比学习，其中模型被训练来区分相似和不相似的输入。像 SimCLR (Chen 等, [2020a](#bib.bib33); Chen
    等, [2020b](#bib.bib34)) 这样的框架，试图学习两个给定输入 $\tilde{x_{i}}$ 和 $\tilde{x_{j}}$ 的表示
    $h_{i}$ 和 $h_{j}$，其中后者两个是同一输入的不同增强视图，使得 $h_{i}$ 和 $h_{j}$ 的投影 $z_{i}$ 和 $z_{j}$
    (使用单独的函数 $g(.)$) 的余弦相似度最大化。类似地，对于不相似的输入，$z_{i}$ 和 $z_{j}$ 的余弦相似度应最小化。作者报告在 ImageNet
    上仅用 1% 标签（每类 13 个标签）取得了 $73.9\%$ 的 Top-1 准确率，并且在仅用 10% 标签的情况下超越了 ResNet-50 监督基线。
- en: '![Refer to caption](img/8a2e40f8d451474d2e3ac2bc15a97f43.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8a2e40f8d451474d2e3ac2bc15a97f43.png)'
- en: 'Figure 12\. SimCLR framework for learning visual representations. Source: (Chen
    et al., [2020a](#bib.bib33))'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12\. 用于学习视觉表示的 SimCLR 框架。来源：(Chen 等, [2020a](#bib.bib33))
- en: 'Discussion: Self-Supervised Learning (SSL) has demonstrated significant success
    in the general representational learning with unlabeled data, followed by fine-tuning
    to adapt the model to the target task with a modest number of labeled examples.
    Yann LeCun has likened Self-Supervision as the cake, and Supervised Learning as
    the icing on top (LeCun, [2018](#bib.bib97)), implying that SSL will be the primary
    way of training high-quality models in the future as we move beyond tasks where
    labeled data is abundant.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：自监督学习 (SSL) 在未标记数据的通用表征学习中取得了显著成功，然后通过微调将模型适应目标任务，使用少量标记样本。Yann LeCun 比喻自监督学习为蛋糕，而监督学习为上面的糖霜
    (LeCun, [2018](#bib.bib97))，这意味着 SSL 将是未来训练高质量模型的主要方式，尤其是当我们超越标记数据丰富的任务时。
- en: With unlabeled data being practically limitless, SSL’s success is dependent
    on creating useful pretext tasks for the domain of interest. As demonstrated across
    NLP (Howard and Ruder, [2018](#bib.bib79); Devlin et al., [2018](#bib.bib48)),
    Vision (Chen et al., [2020a](#bib.bib33); Patrick et al., [2020](#bib.bib121)),
    Speech (Glass, [2012](#bib.bib61)), etc., Self-Supervision is indeed not just
    helpful in speeding and improving convergence, but also enabling achieving high
    quality in tasks where it was intractable to get enough labeled samples.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 由于未标记数据几乎是无限的，SSL 的成功取决于为感兴趣的领域创建有用的前置任务。如在 NLP (Howard 和 Ruder, [2018](#bib.bib79);
    Devlin 等, [2018](#bib.bib48))、视觉 (Chen 等, [2020a](#bib.bib33); Patrick 等, [2020](#bib.bib121))、语音
    (Glass, [2012](#bib.bib61)) 等领域所示，自监督学习确实不仅有助于加速和改善收敛，还能在获取足够标记样本困难的任务中实现高质量。
- en: Practically, for someone training Deep Learning models on a custom task (say
    a speech recognition model for a remote African dialect), having a pre-trained
    checkpoint of a model trained in a self-supervised fashion (such as wav2vec (Baevski
    et al., [2020](#bib.bib21)), which pre-trained in a similar way to BERT (Devlin
    et al., [2018](#bib.bib48))), enables them to only spend an extremely tiny fraction
    of resources on both data labeling, as well as training to fine-tune to a good
    enough quality. In some cases, such as SimCLR (Chen et al., [2020b](#bib.bib34)),
    SSL approaches have actually beaten previous supervised baselines with sophisticated
    models like ResNet-50\. Hence, we are hopeful that SSL methods will be crucial
    for ML practitioners for training high-quality models cheaply.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，对于在自定义任务（例如，远程非洲方言的语音识别模型）上训练深度学习模型的人来说，拥有一个以自监督方式训练的模型的预训练检查点（如 wav2vec
    (Baevski et al., [2020](#bib.bib21))，其训练方式类似于 BERT (Devlin et al., [2018](#bib.bib48)))，使他们只需在数据标注和训练上花费极少的资源来微调到足够好的质量。在某些情况下，如
    SimCLR (Chen et al., [2020b](#bib.bib34))，SSL 方法实际上已经超越了以前的监督基准，例如 ResNet-50。因此，我们希望
    SSL 方法对 ML 从业者训练高质量模型的成本效益至关重要。
- en: 3.3\. Automation
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 自动化
- en: It is possible to delegate some of the work around efficiency to automation,
    and letting automated approaches search for ways of training more efficient models.
    Apart from reducing work for humans, it also lowers the bias that manual decisions
    might introduce in model training, apart from systematically and automatically
    looking for optimal solutions. The trade-off is that these methods might require
    large computational resources, and hence have to be carefully applied.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将部分关于效率的工作委托给自动化，让自动化方法寻找训练更高效模型的方法。除了减少人工工作外，它还降低了人工决策可能在模型训练中引入的偏差，同时系统地和自动地寻找最佳解决方案。权衡是，这些方法可能需要大量计算资源，因此必须小心应用。
- en: 3.3.1\. Hyper-Parameter Optimization (HPO)
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 超参数优化（HPO）
- en: One of the commonly used methods that fall under this category is Hyper-Parameter
    Optimization (HPO) (Yu and Zhu, [2020](#bib.bib165)). Hyper-parameters such as
    initial learning rate, weight decay, etc. have to be carefully tuned for faster
    convergence (Jeremy Jordan, [2020](#bib.bib86)). They can also decide the network
    architecture such as the number of fully connected layers, number of filters in
    a convolutional layer, etc.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别中常用的方法之一是超参数优化（HPO）(Yu and Zhu, [2020](#bib.bib165))。初始学习率、权重衰减等超参数必须经过仔细调整以实现更快的收敛（Jeremy
    Jordan, [2020](#bib.bib86)）。它们还可以决定网络架构，例如全连接层的数量、卷积层中的过滤器数量等。
- en: Experimentation can help us build an intuition for the *range* in which these
    parameters might lie, but finding the best values requires a search for the exact
    values that optimize the given objective function (typically the loss value on
    the validation set). Manually searching for these quickly becomes tedious with
    the growth in the number of hyper-parameters and/or their possible values. Hence,
    let us explore possible algorithms for automating the search. To formalize this,
    let us assume without the loss of generalization, that we are optimizing the loss
    value on the given dataset’s validation split. Then, let $\mathcal{L}$ be the
    loss function, $f$ be the model function that is learnt with the set of hyper-parameters
    ($\lambda$), $x$ be the input, and $\theta$ be the model parameters. With the
    search, we are trying to find $\lambda^{*}$ such that,
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 实验可以帮助我们建立对这些参数可能范围的*直觉*，但找到最佳值需要搜索优化给定目标函数（通常是验证集上的损失值）的确切值。随着超参数及其可能值的增加，手动搜索很快变得乏味。因此，让我们探索自动化搜索的可能算法。为了形式化这个问题，假设我们在优化给定数据集验证分割上的损失值。设
    $\mathcal{L}$ 为损失函数，$f$ 为通过超参数集（$\lambda$）学习得到的模型函数，$x$ 为输入，$\theta$ 为模型参数。通过搜索，我们试图找到
    $\lambda^{*}$，使得，
- en: '| (6) |  | $\small\lambda^{*}=\operatorname*{argmin}_{\lambda\in\Lambda}\mathcal{L}(f_{\lambda}(x;\theta),y)$
    |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\small\lambda^{*}=\operatorname*{argmin}_{\lambda\in\Lambda}\mathcal{L}(f_{\lambda}(x;\theta),y)$
    |  |'
- en: '$\Lambda$ is the set of all possible hyper-parameters. In practice, the $\Lambda$
    can be a very large set containing all possible combinations of the hyper-parameters,
    which would often be intractable since hyper-parameters like learning rate are
    real-valued. A common strategy is to approximate $\Lambda$ by picking a finite
    set of *trials*, $S=\{\lambda^{(1)},\lambda^{(2)},...,\lambda^{(n)}\}$, such that
    $S\in\Lambda$, and then we can approximate Equation ([6](#S3.E6 "In 3.3.1\. Hyper-Parameter
    Optimization (HPO) ‣ 3.3\. Automation ‣ 3\. Landscape of Efficient Deep Learning
    ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster,
    and Better")) with:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: $\Lambda$ 是所有可能的超参数的集合。在实际应用中，$\Lambda$ 可以是一个非常大的集合，包含所有可能的超参数组合，这通常是不可处理的，因为像学习率这样的超参数是实值的。一种常见的策略是通过选择一个有限的*试验*集合
    $S=\{\lambda^{(1)},\lambda^{(2)},...,\lambda^{(n)}\}$ 来近似 $\Lambda$，使得 $S\in\Lambda$，然后我们可以用：
- en: '| (7) |  | $\small\lambda^{*}\approx\operatorname*{argmin}_{\lambda\in\{\lambda^{(1)},...,\lambda^{(n)}\}}\mathcal{L}(f_{\lambda}(x;\theta),y)$
    |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\small\lambda^{*}\approx\operatorname*{argmin}_{\lambda\in\{\lambda^{(1)},...,\lambda^{(n)}\}}\mathcal{L}(f_{\lambda}(x;\theta),y)$
    |  |'
- en: As we see, the choice of $S$ is crucial for the approximation to work. The user
    has to construct a range of reasonable values for each hyper-parameter $\lambda_{i}\in\lambda$.
    This can be based on prior experience with those hyper-parameters.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，$S$ 的选择对近似效果至关重要。用户必须为每个超参数 $\lambda_{i}\in\lambda$ 构建一个合理值的范围。这可以基于对这些超参数的先前经验。
- en: A simple algorithm for automating HPO is Grid Search (also referred to as Parameter
    Sweep), where $S$ consists of all the distinct and valid combinations of the given
    hyper-parameters based on their specified ranges. Each trial can then be run in
    parallel since each trial is independent of the others, and the optimal combination
    of the hyper-parameters is found once all the trials have completed. Since this
    approach tries all possible combinations, it suffers from the *curse of dimensionality*,
    where the total number of trials grow very quickly.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化HPO的一个简单算法是网格搜索（也称为参数扫描），其中 $S$ 包含所有根据指定范围的超参数的不同有效组合。由于每个试验是独立的，因此可以并行运行每个试验，一旦所有试验完成，就可以找到超参数的最佳组合。由于这种方法尝试所有可能的组合，它受到了*维度诅咒*的影响，即试验总数增长非常快。
- en: 'Another approach is Random Search where trials are sampled randomly from the
    search space (Bergstra and Bengio, [2012](#bib.bib24)). Since each trial is independent
    of the others, it can still be executed randomly. However, there are few critical
    benefits of Random Search:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是随机搜索，其中试验是从搜索空间中随机抽取的（Bergstra 和 Bengio, [2012](#bib.bib24)）。由于每次试验是独立的，因此仍然可以随机执行。然而，随机搜索有一些关键的好处：
- en: (1)
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Since the trials are i.i.d. (not the case for Grid Search), the resolution of
    the search can be changed on-the-fly (if the computational budget has changed,
    or certain trials have failed).
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于试验是独立同分布的（网格搜索则不是），因此可以在运行时调整搜索的分辨率（如果计算预算发生了变化，或某些试验失败）。
- en: (2)
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Likelihood of finding the optimal $\lambda^{*}$ increases with the number of
    trials, which is not the case with Grid Search.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 找到最佳 $\lambda^{*}$ 的可能性随着试验次数的增加而增加，这与网格搜索不同。
- en: (3)
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: If there are $K$ real-valued hyper-parameters, and $N$ total trials, grid search
    would pick $N^{\frac{1}{K}}$ for each hyper-parameter. However, not all hyper-parameters
    might be important. Random Search picks a random value for each hyper-parameter
    per trial. Hence, in cases with low effective dimensionality of the search space,
    Random Search performs better than Grid Search.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果有 $K$ 个实值超参数和 $N$ 次总试验，网格搜索将为每个超参数选择 $N^{\frac{1}{K}}$。然而，并非所有的超参数都可能重要。随机搜索为每次试验中的每个超参数选择一个随机值。因此，在搜索空间的有效维度较低的情况下，随机搜索的表现优于网格搜索。
- en: '![Refer to caption](img/9032f6169c971fb06443a5b21db78c71.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9032f6169c971fb06443a5b21db78c71.png)'
- en: (a) Grid Search
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 网格搜索
- en: '![Refer to caption](img/324cf46f9f81b4d3202e401f115c8c94.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/324cf46f9f81b4d3202e401f115c8c94.png)'
- en: (b) Random Search
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 随机搜索
- en: '![Refer to caption](img/320d5fe8693b77c73055ab4017fb0126.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/320d5fe8693b77c73055ab4017fb0126.png)'
- en: (c) Bayesian Optimization
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 贝叶斯优化
- en: 'Figure 13\. Hyper-Parameter Search algorithms. Source: (Contributors to Wikimedia
    projects, [2021c](#bib.bib40))'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13\. 超参数搜索算法。来源: (Wikimedia 项目的贡献者, [2021c](#bib.bib40))'
- en: Bayesian Optimization (BO) based search (Močkus, [1975](#bib.bib112); Agnihotri
    and Batra, [2020](#bib.bib3)) is a *model-based* sequential approach where the
    search is guided by actively estimating the value of the objective function at
    different points in the search space, and then spawning trials based on the information
    gathered so far. The estimation of the objective function is done using a *surrogate
    function* that starts off with a prior estimate. The trials are created using
    an *acquisition function* which picks the next trial using the surrogate function,
    the likelihood of improving on the optimum so far, whether to explore / exploit
    etc. As the trials complete, both these functions will refine their estimates.
    Since the method keeps an internal model of how the objective function looks and
    plans the next trials based on that knowledge, it is model-based. Also, since
    the selection of trials depends on the results of the past trials, this method
    is sequential. BO improves over Random Search in that the search is guided rather
    than random, thus fewer trials are required to reach the optimum. However, it
    also makes the search sequential (though it is possible to run multiple trials
    in parallel, overall it will lead to some wasted trials).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 基于贝叶斯优化（BO）的搜索（Močkus，[1975](#bib.bib112)；Agnihotri 和 Batra，[2020](#bib.bib3)）是一种*模型基*的序列方法，其中搜索通过积极估计搜索空间中不同点的目标函数值来引导，然后根据迄今为止收集的信息生成试验。目标函数的估计是使用一个*代理函数*完成的，该函数从一个先前的估计开始。试验的创建是使用*获取函数*进行的，该函数使用代理函数来选择下一个试验，考虑到是否可能改进迄今为止的最优值、是否探索/利用等。随着试验的完成，这两个函数都会细化其估计。由于该方法保持了目标函数的内部模型并根据该知识规划下一个试验，因此它是基于模型的。此外，由于试验的选择依赖于过去试验的结果，因此该方法是序列的。BO
    相较于随机搜索的改进在于搜索是被引导的而非随机的，因此达到最优值所需的试验更少。然而，这也使得搜索变得序列化（虽然可以并行运行多个试验，但总体上会导致一些试验的浪费）。
- en: One of the strategies to save training resources with the above search algorithms
    is the Early Stopping of trials that are not promising. Google’s Vizier (Golovin
    et al., [2017](#bib.bib62)) uses Median Stopping Rule for early stopping, where
    a trial is terminated if it’s performance at a time step $t$ is below the the
    median performance of all trials run till that point of time.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 节省上述搜索算法训练资源的一种策略是提前停止那些不具备潜力的试验。Google 的 Vizier（Golovin 等，[2017](#bib.bib62)）使用中位数停止规则进行提前停止，其中如果一个试验在时间步
    $t$ 的表现低于截至该时间点所有试验的中位数表现，则终止该试验。
- en: 'Other algorithms for HPO include:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 其他用于超参数优化的算法包括：
- en: (1)
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （1）
- en: 'Population Based Training (PBT) (Jaderberg et al., [2017](#bib.bib84)): This
    method is similar to evolutionary approaches like genetic algorithms, where a
    fixed number of trials (referred to as the population) are spawned and trained
    to convergence. Each trial starts with a random set of hyper-parameters, and trained
    to a pre-determined number of steps. At this point, all trials are paused, and
    every trial’s weights and parameters might be replaced by the weights and parameters
    from the ‘best’ trial in the population so far. This is the *exploitation* part
    of the search. For *exploration*, these hyper-parameters are perturbed from their
    original values. This process repeats till convergence. It combines both the search
    and training in a fixed number of trials that run in parallel. It also only works
    with adaptive hyper-parameters like learning rate, weight-decay, etc. but cannot
    be used where hyper-parameters change the model structure. Note that the criteria
    for picking the ‘best’ trial does not have to be differentiable.'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于种群的训练（PBT）（Jaderberg 等，[2017](#bib.bib84)）：这种方法类似于遗传算法等进化方法，其中生成和训练一个固定数量的试验（称为种群）直至收敛。每个试验从一组随机的超参数开始，并训练到预定的步骤数量。在此时，所有试验都会暂停，每个试验的权重和参数可能会被到目前为止种群中“最佳”试验的权重和参数所替代。这是搜索的*利用*部分。为了*探索*，这些超参数会从其原始值中扰动。这个过程会重复直到收敛。它结合了搜索和训练在一个固定数量的并行试验中。它也仅适用于如学习率、权重衰减等自适应超参数，但不能用于超参数改变模型结构的情况。请注意，选择“最佳”试验的标准不一定需要是可微分的。
- en: (2)
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （2）
- en: 'Multi-Armed Bandit Algorithms: Methods like Successive Halving (SHA) (Jamieson
    and Talwalkar, [2016](#bib.bib85)) and Hyper-Band (Li et al., [2017](#bib.bib102))
    are similar to random search, but they allocate more resources to the trials which
    are performing well. Both these methods need the user to specify the total computational
    budget $B$ for the search (can be the total number of epochs of training, for
    instance). They then spawn and train a fixed number of trials with randomly sampled
    hyper-parameters while allocating the training budget. Once the budget is exhausted,
    the worse performing fraction ($\frac{\eta-1}{\eta}$) of the trials are eliminated,
    and the remaining trials’ new budget is multiplied by $\eta$. In the case of SHA,
    $\eta$ is 2, so the bottom $\frac{1}{2}$ of the trials are dropped, and the training
    budget for the remaining trials is doubled. For Hyper-Band $\eta$ is 3 or 4\.
    Hyper-Band differs from SHA in that the user does not need to specify the maximum
    number of parallel trials, which introduces a trade-off between the total budget
    and the per-trial allocation.'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多臂老虎机算法：像逐次缩减（SHA）（Jamieson和Talwalkar，[2016](#bib.bib85)）和超带（Li等，[2017](#bib.bib102)）的方法类似于随机搜索，但它们会将更多资源分配给表现良好的试验。这些方法都需要用户指定搜索的总计算预算$B$（例如，可以是训练的总轮次）。然后，它们会生成并训练固定数量的试验，使用随机采样的超参数，同时分配训练预算。预算用尽后，表现最差的部分（$\frac{\eta-1}{\eta}$）的试验会被淘汰，剩余试验的新预算会乘以$\eta$。在SHA的情况下，$\eta$为2，因此底部$\frac{1}{2}$的试验被淘汰，剩余试验的训练预算翻倍。对于Hyper-Band，$\eta$为3或4。Hyper-Band不同于SHA，因为用户不需要指定最大并行试验数量，这在总预算与每次试验分配之间引入了权衡。
- en: 'HPO Toolkits: There are several software toolkits that incorporate HPO algorithms
    as well as an easy to use interface (UI, as well as a way to specify the hyper-parameters
    and their ranges). Vizier (Golovin et al., [2017](#bib.bib62)) (an internal Google
    tool, also available via Google Cloud for blackbox tuning). Amazon offers Sagemaker
    (Perrone et al., [2020](#bib.bib123)) which is functionally similar and can also
    be accessed as an AWS service. NNI (Research, [2019](#bib.bib132)), Tune (Liaw
    et al., [2018](#bib.bib104)), Advisor (Chen, [2021](#bib.bib32)) are other open-source
    HPO software packages that can be used locally.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: HPO工具包：有几个软件工具包集成了HPO算法以及易于使用的界面（UI，以及指定超参数及其范围的方法）。Vizier（Golovin等，[2017](#bib.bib62)）（一个内部Google工具，也可以通过Google
    Cloud进行黑箱调优）。Amazon提供了Sagemaker（Perrone等，[2020](#bib.bib123)），功能类似，也可以作为AWS服务访问。NNI（Research，[2019](#bib.bib132)），Tune（Liaw等，[2018](#bib.bib104)），Advisor（Chen，[2021](#bib.bib32)）是其他可以在本地使用的开源HPO软件包。
- en: 3.3.2\. Neural Architecture Search (NAS)
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 神经架构搜索（NAS）
- en: Neural Architecture Search can be thought of an extension of Hyper-Parameter
    Optimization wherein we are searching for parameters that change the network architecture
    itself.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构搜索可以被认为是超参数优化的扩展，其中我们寻找的是改变网络架构本身的参数。
- en: 'We find that there is consensus in the literature (Elsken et al., [2019](#bib.bib54))
    around categorizing NAS as a system comprising of the following parts:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现文献中存在共识（Elsken等，[2019](#bib.bib54)），将NAS归类为一个包含以下部分的系统：
- en: (1)
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Search Space: These are the operations that are allowed in the graph (Convolution
    ($1\times 1,3\times 3,5\times 5$), Fully Connected, Pooling, etc.), as well as
    the semantics of how these operations and their outputs connect to other parts
    of the network.'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 搜索空间：这些是在图中允许的操作（卷积（$1\times 1,3\times 3,5\times 5$），全连接，池化等），以及这些操作及其输出如何连接到网络的其他部分的语义。
- en: (2)
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Search Algorithm & State: This is the algorithm that controls the architecture
    search itself. Typically the standard algorithms that apply in HPO (Grid Search,
    Random Search, Bayesian Optimization, Evolutionary Algorithms), can be used for
    NAS as well. However, using Reinforcement Learning (RL) (Zoph and Le, [2016](#bib.bib169)),
    and Gradient Descent (Liu et al., [2018a](#bib.bib106)) are popular alternatives
    too.'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 搜索算法与状态：这是控制架构搜索本身的算法。通常，适用于HPO（网格搜索、随机搜索、贝叶斯优化、进化算法）的标准算法，也可以用于NAS。然而，使用强化学习（RL）（Zoph和Le，[2016](#bib.bib169)）和梯度下降（Liu等，[2018a](#bib.bib106)）也是流行的替代方案。
- en: (3)
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Evaluation Strategy: This defines how we evaluate a model for fitness. It can
    simply be a conventional metric like validation loss, accuracy, etc. Or it can
    also be a compound metric, as in the case of MNasNet (Tan et al., [2019](#bib.bib148))
    which creates a single custom metric based on accuracy as well as latency.'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估策略：这定义了我们如何评估模型的适应性。它可以是一个简单的传统指标，如验证损失、准确率等。或者，它也可以是一个复合指标，例如 MNasNet (Tan
    et al., [2019](#bib.bib148)) 的情况，它基于准确率和延迟创建一个自定义的单一指标。
- en: '![Refer to caption](img/2f90de7a225110a02b92535c81cb47e2.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2f90de7a225110a02b92535c81cb47e2.png)'
- en: 'Figure 14\. Neural Architecture Search: The controller can be thought of as
    a unit that encodes the search space, the search algorithm itself, and the state
    it maintains (typically the model that helps generate the candidates). The algorithm
    generates candidate models in the search space $S$, and receives an evaluation
    feedback. This feedback is used to update the state, and generate better candidate
    models.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14\. 神经架构搜索：控制器可以被认为是一个编码搜索空间、搜索算法本身以及它所维护的状态（通常是帮助生成候选模型的模型）的单元。算法在搜索空间 $S$
    中生成候选模型，并接收评估反馈。这个反馈用于更新状态，并生成更好的候选模型。
- en: 'The user is supposed to either explicitly or implicitly encode the search space.
    Together with the search algorithm, we can view this as a ‘controller’ which generates
    sample candidate networks (Refer to Figure [14](#S3.F14 "Figure 14 ‣ 3.3.2\. Neural
    Architecture Search (NAS) ‣ 3.3\. Automation ‣ 3\. Landscape of Efficient Deep
    Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller,
    Faster, and Better")). The evaluation stage will then train and evaluate these
    candidates for fitness. This fitness value is then passed as feedback to the search
    algorithm, which will use it for generating better candidates. While the implementation
    of each of these blocks vary, this structure is common across the seminal work
    in this area.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 用户应当明确或隐含地编码搜索空间。结合搜索算法，我们可以将其视为一个‘控制器’，它生成样本候选网络（参见图 [14](#S3.F14 "图 14 ‣ 3.3.2\.
    神经架构搜索 (NAS) ‣ 3.3\. 自动化 ‣ 3\. 高效深度学习的全景 ‣ 高效深度学习：关于使深度学习模型更小、更快、更好的调查")）。评估阶段将训练并评估这些候选模型的适应性。这个适应性值然后作为反馈传递给搜索算法，算法将利用它生成更好的候选模型。虽然这些模块的实现各不相同，但这种结构在该领域的开创性工作中是共同的。
- en: 'Zoph et. al’s paper from 2016 (Zoph and Le, [2016](#bib.bib169)), demonstrated
    that end-to-end neural network architectures can be generated using Reinforcement
    Learning. In this case, the controller is a Recurrent Neural Network, which generates
    the architectural hyper-parameters of a feed-forward network one layer at a time,
    for example, number of filters, stride, filter size, etc. They also support adding
    skip connections (refer Figure [15](#S3.F15 "Figure 15 ‣ 3.3.2\. Neural Architecture
    Search (NAS) ‣ 3.3\. Automation ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")).
    The network semantics are baked into the controller, so generating a network that
    behaves differently requires changing the controller. Also, training the controller
    itself is expensive (taking 22,400 GPU hours (Zoph et al., [2018](#bib.bib170))),
    since the entire candidate network has to be trained from scratch for a single
    gradient update to happen. In a follow up paper (Zoph et al., [2018](#bib.bib170)),
    they come up with a refined search space where instead of searching for the end-to-end
    architecture, they search for *cells*: A ‘Normal Cell’ that takes in an input,
    processes it, and returns an output of the same spatial dimensions. And a ‘Reduction
    Cell’ that process its input, and returns an output whose spatial dimensions are
    scaled down by a factor of 2\. Each cell is a combination of $B$ blocks. The controller’s
    RNN generates one block at a time, where it picks outputs of two blocks in the
    past, the respective operations to apply on them, and how to combine them into
    a single output. The Normal and Reduction cells are stacked in alternating fashion
    ($N$ Normal cells followed by 1 Reduction cell, where $N$ is tunable) to construct
    an end-to-end network for CIFAR-10 and ImageNet. Learning these cells individually
    rather than learning the entire network seems to improve the search time by 7$\times$,
    when compared to the end-to-end network search in (Zoph and Le, [2016](#bib.bib169)),
    while beating the state-of-the-art in CIFAR-10 at that time.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zoph等人在2016年的论文（Zoph and Le, [2016](#bib.bib169)）中展示了可以使用强化学习生成端到端的神经网络架构。在这种情况下，控制器是一个递归神经网络，它一次生成一个前馈网络的架构超参数，例如滤波器数量、步幅、滤波器大小等。他们还支持添加跳跃连接（参见图
    [15](#S3.F15 "Figure 15 ‣ 3.3.2\. Neural Architecture Search (NAS) ‣ 3.3\. Automation
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better")）。网络语义被嵌入到控制器中，因此生成一个行为不同的网络需要更改控制器。此外，训练控制器本身很昂贵（需要22,400
    GPU小时（Zoph et al., [2018](#bib.bib170)）），因为必须从头开始训练整个候选网络以进行单次梯度更新。在后续论文（Zoph
    et al., [2018](#bib.bib170)）中，他们提出了一个改进的搜索空间，其中不再搜索端到端的架构，而是搜索*单元*：一个“正常单元”接收输入，处理它，并返回相同空间维度的输出；一个“缩减单元”处理其输入，并返回空间维度缩小一倍的输出。每个单元由$B$块组成。控制器的RNN一次生成一个块，它选择过去两个块的输出、应用于它们的操作以及如何将它们组合成一个输出。正常单元和缩减单元以交替方式堆叠（$N$个正常单元后跟1个缩减单元，其中$N$是可调的）以构建一个端到端的网络，用于CIFAR-10和ImageNet。与（Zoph
    and Le, [2016](#bib.bib169)）中的端到端网络搜索相比，单独学习这些单元似乎将搜索时间提高了7$\times$，同时在CIFAR-10上超过了当时的最先进水平。'
- en: '![Refer to caption](img/5a6c2fa92561ad575a124d509c7cd571.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5a6c2fa92561ad575a124d509c7cd571.png)'
- en: 'Figure 15\. A NASNet controller generating the architecture, recursively making
    one decision at a time and generating a single block in the image (making a total
    of 5 decisions). Source: (Zoph et al., [2018](#bib.bib170)).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图15\. 一个NASNet控制器生成架构，递归地一次做出一个决定，并在图像中生成一个单独的块（共做出5个决定）。来源：（Zoph et al., [2018](#bib.bib170)）。
- en: Other approaches such as evolutionary techniques (Real et al., [2019](#bib.bib131)),
    differentiable architecture search (Liu et al., [2018a](#bib.bib106)), progressive
    search (Liu et al., [2018c](#bib.bib105)), parameter sharing (Pham et al., [2018](#bib.bib124)),
    etc. try to reduce the cost of architecture search (in some cases reducing the
    compute cost to a couple of GPU days instead of thousands of GPU days). These
    are covered in detail in (Elsken et al., [2019](#bib.bib54)).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法，如进化技术（Real et al., [2019](#bib.bib131)），可微分架构搜索（Liu et al., [2018a](#bib.bib106)），渐进搜索（Liu
    et al., [2018c](#bib.bib105)），参数共享（Pham et al., [2018](#bib.bib124)）等，试图降低架构搜索的成本（在某些情况下，将计算成本降低到几天的GPU时间，而不是数千天的GPU时间）。这些在（Elsken
    et al., [2019](#bib.bib54)）中有详细介绍。
- en: 'While most of the early papers focused on finding the architectures that performed
    best on quality metrics like accuracy, unconstrained by the footprint metrics.
    However, when focusing on efficiency, we are often interested in specific tradeoffs
    between quality and footprint. Architecture Search can help with multi-objective
    searches that optimize for both quality and footprint. MNasNet (Tan et al., [2019](#bib.bib148))
    is one such work. It incorporates the model’s latency on the target device into
    the objective function directly, as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然早期的大多数论文集中在寻找在准确度等质量指标上表现最好的架构，而不受足迹指标的限制。但是，当关注效率时，我们通常对质量和足迹之间的具体权衡感兴趣。架构搜索可以帮助进行多目标搜索，以优化质量和足迹。MNasNet（Tan
    et al., [2019](#bib.bib148)）就是其中之一。它将模型在目标设备上的延迟直接纳入目标函数，如下所示：
- en: '| (8) |  | $\small\underset{m}{\operatorname{maximize}}\quad ACC(m)\times\left[\frac{LAT(m)}{T}\right]^{w}$
    |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\small\underset{m}{\operatorname{maximize}}\quad ACC(m)\times\left[\frac{LAT(m)}{T}\right]^{w}$
    |  |'
- en: Where $m$ is the candidate model, $ACC$ is the accuracy metric, and $LAT$ is
    the latency of the given model on the desired device. $T$ is the target latency.
    $w$ is recommended to be $-0.07$. FBNet (Wu et al., [2019](#bib.bib161)) uses
    a similar approach with a compound reward function that has a weighted combination
    of the loss value on the validation set and the latency. However instead of measuring
    the latency of the candidate model on device, they use a pre-computed lookup table
    to approximate the latency to speed up the search process. They achieve networks
    that are upto $2.4\times$ smaller and $1.5\times$ faster than MobileNet, while
    finishing the search in 216 GPU Hours. Other works such as MONAS (Hsu et al.,
    [2018](#bib.bib80)) use Reinforcement Learning to incorporate power consumption
    into the reward function along with hard constraints on the number of MAC operations
    in the model, and discover pareto-frontiers under the given constraints.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m$ 是候选模型，$ACC$ 是准确度指标，$LAT$ 是给定模型在目标设备上的延迟。$T$ 是目标延迟。$w$ 推荐为 $-0.07$。FBNet（Wu
    et al., [2019](#bib.bib161)）使用类似的方法，其复合奖励函数对验证集上的损失值和延迟进行加权组合。然而，它们不是测量候选模型在设备上的延迟，而是使用预计算的查找表来近似延迟，以加速搜索过程。他们实现的网络比
    MobileNet 小 $2.4\times$，速度快 $1.5\times$，同时在 216 GPU 小时内完成了搜索。其他工作如 MONAS（Hsu et
    al., [2018](#bib.bib80)）使用强化学习将功耗纳入奖励函数，同时对模型中的 MAC 操作数量施加硬约束，并在给定约束下发现帕累托前沿。
- en: 'Discussion: Automation plays a critical role in model efficiency. Hyper-Parameter
    Optimization (HPO) is now a natural step in training models and can extract significant
    quality improvements, while minimizing human involvement. In case the cost HPO
    becomes large, algorithms like Bayesian Optimization, Hyper-Band etc. with early
    stopping techniques can be used. HPO is also available in ready-to-use software
    packages like Tune (Liaw et al., [2018](#bib.bib104)), Vizier via Google Cloud
    (Golovin et al., [2017](#bib.bib62)), NNI (Research, [2019](#bib.bib132)), etc.
    Similarly, recent advances in Neural Architecture Search (NAS) also make it feasible
    to construct architectures in a learned manner, while having constraints on both
    quality and footprint (Tan et al., [2019](#bib.bib148)). Assuming several hundred
    GPU hours worth of compute required for the NAS run to finish, and an approx cost
    of $3 GPU / hour on leading cloud computing services, this makes using NAS methods
    financially feasible and not similar in cost to manual experimentation with model
    architecture when optimizing for multiple objectives.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：自动化在模型效率中发挥了关键作用。超参数优化（HPO）现在是训练模型的自然步骤，并且可以显著提高质量，同时最小化人工干预。如果 HPO 的成本变得很大，可以使用类似贝叶斯优化、Hyper-Band
    等带有早期停止技术的算法。HPO 也可以在像 Tune（Liaw et al., [2018](#bib.bib104)）、Google Cloud 的 Vizier（Golovin
    et al., [2017](#bib.bib62)）、NNI（Research, [2019](#bib.bib132)）等现成的软件包中找到。类似地，最近在神经架构搜索（NAS）方面的进展也使得以学习的方式构建架构成为可能，同时对质量和足迹都有约束（Tan
    et al., [2019](#bib.bib148)）。假设 NAS 运行完成需要几百个 GPU 小时的计算，并且在领先的云计算服务上大约为 $3 GPU
    / hour 的成本，这使得使用 NAS 方法在财务上是可行的，与针对多个目标优化的手动实验成本并不相似。
- en: 3.4\. Efficient Architectures
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 高效架构
- en: Another common theme for tackling efficiency problems is to go back to the drawing
    board, and design layers and models that are efficient by design to replace the
    baseline. They are typically designed with some insight which might lead to a
    design that is better in general, or it might be better suited for the specific
    task. In this section, we lay out an examples of such efficient layers and models
    to illustrate this idea.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 处理效率问题的另一个常见方法是回到设计阶段，设计具有高效性的层和模型来替代基线模型。它们通常设计有一定的洞察力，这可能导致设计在一般情况下更好，或者更适合特定任务。在本节中，我们列出了这种高效层和模型的示例，以阐明这一思想。
- en: 3.4.1\. Vision
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1\. 视觉
- en: 'One of the classical example of efficient layers in the Vision domain are the
    Convolutional layers, which improved over Fully Connected (FC) layers in Vision
    models. FC layers suffer from two primary issues:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉领域中，高效层的经典例子之一是卷积层，它们在视觉模型中比全连接（FC）层更有优势。全连接层存在两个主要问题：
- en: (1)
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: FC layers ignore the spatial information of the input pixels. Intuitively, it
    is hard to build an understanding of the given input by looking at individual
    pixel values in isolation. They also ignore the spatial locality in nearby regions.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 全连接层忽略了输入像素的空间信息。直观来说，通过孤立地查看单个像素值，很难建立对给定输入的理解。它们还忽略了附近区域的空间局部性。
- en: (2)
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Secondly, using FC layers also leads to an explosion in the number of parameters
    when working with even moderately sized inputs. A $100\times 100$ RGB image with
    3 channels, would lead to each neuron in the first layer having $3\times 10^{4}$
    connections. This makes the network susceptible to overfitting also.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其次，当处理即使是中等大小的输入时，使用全连接（FC）层也会导致参数数量爆炸。例如，一个$100\times 100$ RGB图像具有3个通道，这将导致第一层中的每个神经元具有$3\times
    10^{4}$个连接。这也使得网络容易过拟合。
- en: Convolutional layers avoid this by learning ‘filters’, where each filter is
    a 3D weight matrix of a fixed size ($3\times 3$, $5\times 5$, etc.), with the
    third dimension being the same as the number of channels in the input. Each filter
    is convolved over the input to generate a feature map for that given filter. These
    filters learn to detect specific features, and convolving them with a particular
    input patch results in a single scalar value that is higher if the feature is
    present in that input patch.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层通过学习‘滤波器’来避免这一问题，其中每个滤波器是一个固定大小的3D权重矩阵（$3\times 3$、$5\times 5$等），第三维与输入中的通道数相同。每个滤波器在输入上进行卷积以生成该滤波器的特征图。这些滤波器学习检测特定特征，将其与特定输入补丁进行卷积会产生一个标量值，如果该特征在输入补丁中存在，则该值较高。
- en: These learned features are simpler in lower layers (such as edges (horizontal,
    vertical, diagonal, etc.)), and more complex in subsequent layers (texture, shapes,
    etc.). This happens because the subsequent layers use the feature maps generated
    by previous layers, and each pixel in the input feature map of the $i$-th layer,
    depends on the past $i-1$ layers. This increases the *receptive field* of the
    said pixel as $i$ increases, progressively increasing the complexity of the features
    that can be encoded in a filter.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在较低层中（例如边缘（水平、垂直、对角线等）），学到的特征较简单，而在后续层中（纹理、形状等）则更复杂。这是因为后续层使用前面层生成的特征图，并且第$i$层输入特征图中的每个像素依赖于过去的$i-1$层。随着$i$的增加，这增加了该像素的*感受野*，逐渐增加了可以在滤波器中编码的特征的复杂性。
- en: The core idea behind the efficiency of Convolutional Layers is that the same
    filter is used everywhere in the image, regardless of where the filter is applied.
    Hence, enforcing spatial invariance while sharing the parameters. Going back to
    the example of a $100\times 100$ RGB image with 3 channels, a $5\times 5$ filter
    would imply a total of $75$ ($5\times 5\times 3$) parameters. Each layer can learn
    multiple unique filters, and still be within a very reasonable parameter budget.
    This also has a regularizing effect, wherein a dramatically reduced number of
    parameters allow for easier optimization, and reducing the likelihood of overfitting.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层效率的核心思想是相同的滤波器在图像中的所有位置都使用，而不论滤波器应用的位置。因此，在共享参数的同时，强制执行空间不变性。以一个$100\times
    100$ RGB图像为例，一个$5\times 5$的滤波器意味着总共有$75$个参数（$5\times 5\times 3$）。每层可以学习多个独特的滤波器，同时仍在非常合理的参数预算范围内。这也具有正则化效果，即大幅减少的参数数量使优化变得更容易，降低了过拟合的可能性。
- en: Convolutional Layers are usually coupled with Pooling Layers, which allow dimensionality
    reduction by subsampling the input (aggregating a sliding 2-D window of pixels,
    using functions like max, avg, etc.). Pooling would lead to smaller feature maps
    for the next layer to process, which makes it faster to process. LeNet5 (Lecun
    et al., [1998](#bib.bib98)) was the first Convolutional Network which included
    convolutional layers, pooling, etc. Subsequently, many iterations of these networks
    have been proposed with various improvements. AlexNet (Krizhevsky et al., [2012](#bib.bib93)),
    Inception (Szegedy et al., [2015](#bib.bib147)), ResNet (He et al., [2016](#bib.bib74)),
    etc. have all made significant improvements over time on known image classification
    benchmarks using Convolutional Layers.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层通常与池化层配合使用，池化层通过子采样输入（聚合像素的滑动二维窗口，使用最大值、平均值等函数）来实现降维。池化操作会导致下一层处理的特征图变小，从而加快处理速度。LeNet5（Lecun
    等人，[1998](#bib.bib98)）是第一个包含卷积层、池化等的卷积网络。随后，许多这些网络的迭代版本被提出，并进行了各种改进。AlexNet（Krizhevsky
    等人，[2012](#bib.bib93)）、Inception（Szegedy 等人，[2015](#bib.bib147)）、ResNet（He 等人，[2016](#bib.bib74)）等在使用卷积层进行已知图像分类基准测试时都取得了显著改进。
- en: 'Depth-Separable Convolutional Layers: In the convolution operation, each filter
    is used to convolve over the two spatial dimensions and the third channel dimension.
    As a result, the size of each filter is $s_{x}\times s_{y}\times$ input_channels,
    where $s_{x}$ and $s_{y}$ are typically equal. This is done for each filter, resulting
    in the convolution operation happening both spatially in the $x$ and $y$ dimensions,
    and depthwise in the $z$ dimension.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 深度可分离卷积层：在卷积操作中，每个滤波器用于在两个空间维度和第三个通道维度上进行卷积。因此，每个滤波器的大小为$s_{x}\times s_{y}\times$input_channels，其中$s_{x}$和$s_{y}$通常相等。对每个滤波器执行此操作，导致卷积操作在$x$和$y$维度上空间上进行，同时在$z$维度上进行深度卷积。
- en: '![Refer to caption](img/c2bbc39b0e4bc690ded3bb25e4211b40.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c2bbc39b0e4bc690ded3bb25e4211b40.png)'
- en: 'Figure 16\. Depth-Separable Convolution. Source: (Tsang, [2019](#bib.bib152)).'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16\. 深度可分离卷积。来源：（Tsang，[2019](#bib.bib152)）。
- en: 'Depth-separable convolution breaks this into two steps (Refer to Figure [16](#S3.F16
    "Figure 16 ‣ 3.4.1\. Vision ‣ 3.4\. Efficient Architectures ‣ 3\. Landscape of
    Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better")):'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '深度可分离卷积将其拆分为两个步骤（参见图 [16](#S3.F16 "Figure 16 ‣ 3.4.1\. Vision ‣ 3.4\. Efficient
    Architectures ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning:
    A Survey on Making Deep Learning Models Smaller, Faster, and Better")）：'
- en: (1)
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Doing a point-wise convolution with $1\times 1$ filters, such that the resulting
    feature map now has a depth of output_channels.
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用$1\times 1$滤波器进行点卷积，结果特征图现在具有output_channels的深度。
- en: (2)
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Doing a spatial convolution with $s_{x}\times s_{y}$ filters in the $x$ and
    $y$ dimensions.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在$x$和$y$维度上使用$s_{x}\times s_{y}$滤波器进行空间卷积。
- en: These two operations stacked together (without any intermediate non-linear activation)
    results in an output of the same shape as a regular convolution, with much fewer
    parameters ($1\times 1\times$input_channels$\times$ output_channels$)+(s_{x}\times
    s_{y}\times$ output_channels$)$, v/s $s_{x}\times s_{y}\times$ input_channels
    $\times$ output_channels for the regular convolution). Similarly there is an order
    of magnitude less computation since the point-wise convolution is much cheaper
    for convolving with each input channel depth-wise (for more calculations refer
    to (Sandler et al., [2018](#bib.bib134))). The Xception model architecture (Chollet,
    [2017](#bib.bib35)) demonstrated that using depth-wise separable convolutions
    in the Inception architecture, allowed reaching convergence sooner in terms of
    steps and a higher accuracy on the ImageNet dataset while keeping the number of
    parameters the same.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种操作叠加在一起（没有任何中间的非线性激活）会产生与常规卷积相同形状的输出，但参数更少（$1\times 1\times$input_channels$\times$output_channels$)+(s_{x}\times
    s_{y}\times$output_channels$)$，相比之下常规卷积的参数为$s_{x}\times s_{y}\times$input_channels
    $\times$output_channels）。由于点卷积在对每个输入通道进行深度卷积时成本更低，因此计算量也减少了一个数量级（有关更多计算，请参阅（Sandler
    等人，[2018](#bib.bib134)））。Xception模型架构（Chollet，[2017](#bib.bib35)）证明了在Inception架构中使用深度可分离卷积可以更快地达到收敛，并在ImageNet数据集上获得更高的准确性，同时保持相同数量的参数。
- en: The MobileNet model architecture (Sandler et al., [2018](#bib.bib134)) which
    was designed for mobile and embedded devices, also uses the depth-wise separable
    layers instead of the regular convolutional layers. This helps them reduce the
    number of parameters as well as the number of multiply-add operations by $7-10\times$
    and allows deployment on Mobile for Computer Vision tasks. Users can expect a
    latency between 10-100ms depending on the model. MobileNet also provides a knob
    via the depth-multiplier for scaling the network to allow the user to trade-off
    between accuracy and latency.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet模型架构 (Sandler 等，[2018](#bib.bib134)) 旨在移动和嵌入式设备上使用，采用了深度可分离层而非常规卷积层。这有助于将参数数量以及乘加操作数量减少$7-10\times$，并允许在移动设备上部署计算机视觉任务。用户可以根据模型预期的延迟在10-100ms之间变化。MobileNet还通过深度乘子提供了一个调节器，用于缩放网络，以便用户在精度和延迟之间进行权衡。
- en: 3.4.2\. Natural Language Understanding
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2\. 自然语言理解
- en: 'Attention Mechanism & Transformer Family: One of the issues plaguing classical
    Sequence-to-Sequence (Seq2Seq) models for solving tasks such as Machine Translation
    (MT), was that of the information-bottleneck. Seq2Seq models typically have one
    or more encoder layers which encode the given input sequence ($\mathbf{x}=(x_{1},x_{2},...,x_{T})$)
    into a fixed length vector(s) (also referred to as the context, $\mathbf{c}$),
    and one or more decoder layers which generate another sequence using this context.
    In the case of MT, the input sequence can be a sentence in the source language,
    and the output sequence can be the sentence in the target language.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制与Transformer家族：困扰经典序列到序列（Seq2Seq）模型的一个问题是信息瓶颈。Seq2Seq模型通常有一个或多个编码器层，这些层将给定的输入序列($\mathbf{x}=(x_{1},x_{2},...,x_{T})$)编码为一个固定长度的向量（也称为上下文，$\mathbf{c}$），以及一个或多个解码器层，这些层使用这个上下文生成另一个序列。在机器翻译（MT）的情况下，输入序列可以是源语言中的一句话，而输出序列可以是目标语言中的句子。
- en: 'However, in classical Seq2Seq models such as (Sutskever et al., [2014](#bib.bib146))
    the decoder layers could only see the hidden state of the final encoder step ($c=h_{T}$).
    This is a *bottleneck* because the encoder block has to squash all the information
    about the sequence in a single context vector for all the decoding steps, and
    the decoder block has to somehow infer the entire encoded sequence from it (Refer
    to Figure [17](#S3.F17 "Figure 17 ‣ 3.4.2\. Natural Language Understanding ‣ 3.4\.
    Efficient Architectures ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")).
    It is possible to increase the size of the context vector, but it would lead to
    an increase in the hidden state of all the intermediate steps, and make the model
    larger and slower.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在经典Seq2Seq模型中，如(Sutskever 等，[2014](#bib.bib146))，解码器层只能看到最终编码步骤的隐藏状态($c=h_{T}$)。这是一个*bottleneck*，因为编码器块必须将有关序列的所有信息压缩到一个单一的上下文向量中供所有解码步骤使用，解码器块必须以某种方式从中推断出整个编码序列（参见图[17](#S3.F17
    "图17 ‣ 3.4.2\. 自然语言理解 ‣ 3.4\. 高效架构 ‣ 3\. 高效深度学习的景观 ‣ 高效深度学习：对使深度学习模型更小、更快、更好的调查")）。增加上下文向量的大小是可能的，但这会导致所有中间步骤的隐藏状态增加，从而使模型变得更大、更慢。
- en: '![Refer to caption](img/c07896369b09a5d2acadddef0edfce3b.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c07896369b09a5d2acadddef0edfce3b.png)'
- en: Figure 17\. Information Bottleneck in a Seq2Seq model for translating from English
    to Hindi. The context vector $c$ that the decoder has access to is fixed, and
    is typically the last hidden state ($h_{T}$).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图17\. Seq2Seq模型中的信息瓶颈，用于将英语翻译为印地语。解码器可以访问的上下文向量$c$是固定的，通常是最后一个隐藏状态($h_{T}$)。
- en: '![Refer to caption](img/ad6d03cd0582600920ae7aaefc36c76e.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ad6d03cd0582600920ae7aaefc36c76e.png)'
- en: 'Figure 18\. Attention module learning a weighted context vector for each output
    token from the hidden states. Source: (Bahdanau et al., [2014](#bib.bib22)).'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图18\. 注意力模块为每个输出标记从隐藏状态中学习一个加权的上下文向量。来源： (Bahdanau 等，[2014](#bib.bib22))。
- en: The Attention mechanism was introduced in Bahdanau et al. (Bahdanau et al.,
    [2014](#bib.bib22)) to be able to create a custom context vector for each output
    token, by allowing all hidden states to be visible to the decoder and then creating
    a weighted context vector, based on the output token’s alignment with each input
    token. Essentially, the new weighted context vector is $c_{i}=\sum_{j}^{T}\alpha_{ij}.h_{j}$,
    where $\alpha_{ij}$ is the learned alignment (attention weight) between the decoder
    hidden state $s_{i-1}$ and the hidden state for the $j$-th token ($h_{j}$). $\alpha_{ij}$
    could be viewed as how much attention should the $i$-th input token be given when
    processing the $j$-th input token. This model is generalized in some cases by
    having explicit Query ($Q$), Key ($K$), and Value ($V$) vectors. Where we seek
    to learn the attention weight distribution ($\mathbf{\alpha}$) between $Q$ and
    $K$, and use it to compute the weighted context vector ($\mathbf{c}$) over $V$.
    In the above encoder-decoder architecture, $Q$ is the decoder hidden state $s_{i-1}$,
    and $K=V$ is the encoder hidden state $h_{j}$. Attention has been used to solve
    a variety of NLU tasks (MT, Question Answering, Text Classification, Sentiment
    Analysis), as well as Vision, Multi-Modal Tasks etc. (Chaudhari et al., [2019](#bib.bib30)).
    We refer the reader to (Chaudhari et al., [2019](#bib.bib30)) for further details
    on the taxonomy of attention models.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在 Bahdanau 等人（Bahdanau 等人，[2014](#bib.bib22)）的研究中引入，通过允许所有隐藏状态对解码器可见，并基于输出标记与每个输入标记的对齐关系创建加权上下文向量。实际上，新的加权上下文向量为
    $c_{i}=\sum_{j}^{T}\alpha_{ij}.h_{j}$，其中 $\alpha_{ij}$ 是解码器隐藏状态 $s_{i-1}$ 和第 $j$
    个标记的隐藏状态 ($h_{j}$) 之间学习到的对齐（注意力权重）。$\alpha_{ij}$ 可以视为在处理第 $j$ 个输入标记时，第 $i$ 个输入标记应给予多少注意力。在某些情况下，这个模型通过显式的查询
    ($Q$)、键 ($K$) 和值 ($V$) 向量进行了推广。我们试图学习 $Q$ 和 $K$ 之间的注意力权重分布 ($\mathbf{\alpha}$)，并用它来计算
    $V$ 上的加权上下文向量 ($\mathbf{c}$)。在上述编码器-解码器架构中，$Q$ 是解码器隐藏状态 $s_{i-1}$，$K=V$ 是编码器隐藏状态
    $h_{j}$。注意力机制已被用于解决各种 NLU 任务（机器翻译、问答、文本分类、情感分析），以及视觉、多模态任务等（Chaudhari 等人，[2019](#bib.bib30)）。我们建议读者参考（Chaudhari
    等人，[2019](#bib.bib30)）以获取更多关于注意力模型分类的细节。
- en: '![Refer to caption](img/1ae24c1cc1da5c3815470e059f74cbbe.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1ae24c1cc1da5c3815470e059f74cbbe.png)'
- en: 'Figure 19\. Transformer with its Encoder and Decoder blocks. Source: (Alammar,
    [2021](#bib.bib4)).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19\. Transformer 及其编码器和解码器模块。来源：（Alammar，[2021](#bib.bib4)）。
- en: The Transformer architecture (Vaswani et al., [2017](#bib.bib156)) was proposed
    in 2017, which introduced using Self-Attention layers for both the Encoder and
    the Decoder. They demonstrated that Attention layers could be used to replace
    traditional RNN based Seq2Seq models. The Self-Attention layer the query, key,
    and value vectors are all derived from the same sequence by using different projection
    matrices.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构（Vaswani 等人，[2017](#bib.bib156)）于 2017 年提出，该架构引入了用于编码器和解码器的自注意力层。他们展示了注意力层可以替代传统的基于
    RNN 的 Seq2Seq 模型。在自注意力层中，查询、键和值向量都通过使用不同的投影矩阵从同一序列中导出。
- en: Self-Attention also allows parallelizing the process of deriving relationships
    between the tokens in the input sequences. RNNs inherently force the process to
    occur one step at a time, i.e., learning long range dependencies is $O(n)$, where
    $n$ is the number of tokens. With Self-Attention, all tokens are processed together
    and pairwise relationships can be learnt in $O(1)$ (Vaswani et al., [2017](#bib.bib156)).
    This makes it easier to leverage optimized training devices like GPUs and TPUs.
    The authors reported up to $300\times$ less training FLOPs as required to converge
    to a similar quality when compared to other recurrent and convolutional models.
    Tay et al. (Tay et al., [2020](#bib.bib149)) discuss the computation and memory
    efficiency of several Transformer variants and their underlying self-attention
    mechanisms in detail.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制还允许并行化处理输入序列中标记之间关系的过程。RNNs 本质上强制过程一步一步进行，即学习长程依赖关系的复杂度为 $O(n)$，其中 $n$
    是标记的数量。使用自注意力机制时，所有标记可以一起处理，并且成对关系可以在 $O(1)$ 内学习（Vaswani 等人，[2017](#bib.bib156)）。这使得利用优化的训练设备如
    GPUs 和 TPUs 更加容易。作者报告称，与其他递归和卷积模型相比，训练 FLOPs 减少了多达 $300\times$，以达到类似的质量。Tay 等人（Tay
    等人，[2020](#bib.bib149)）详细讨论了几种 Transformer 变体及其底层自注意力机制的计算和内存效率。
- en: As introduced earlier, the BERT model architecture (Devlin et al., [2018](#bib.bib48))
    beat the state-of-the-art in several NLU benchmarks. BERT is a stack of Transformer
    encoder layers that are pre-trained using a bi-directional masked language model
    training objective. It can also be used as a general purpose encoder which can
    then be used for other tasks. Other similar models like the GPT family (Brown
    et al., [2020](#bib.bib27)) have also been used for solving many NLU tasks.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，BERT 模型架构（Devlin 等，[2018](#bib.bib48)）在多个 NLU 基准测试中超越了最先进的技术。BERT 是一组 Transformer
    编码器层，通过双向掩蔽语言模型训练目标进行预训练。它还可以用作通用编码器，然后用于其他任务。其他类似的模型如 GPT 系列（Brown 等，[2020](#bib.bib27)）也被用于解决许多
    NLU 任务。
- en: Random Projection Layers & Models Pre-trained token representations such as
    word2vec (Mikolov et al., [2017](#bib.bib111)), GLoVE (Pennington et al., [2014](#bib.bib122)),
    etc. are common for NLU tasks. However, since they require a $d$-dimensional vector
    for storing each token, the total size consumed by the table quickly grows very
    large if the vocabulary size $V$ is substantial ($O(V.d)$).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 随机投影层与模型 预训练的词表示（如 word2vec（Mikolov 等，[2017](#bib.bib111)），GLoVE（Pennington
    等，[2014](#bib.bib122)）等）在 NLU 任务中很常见。然而，由于它们需要一个 $d$-维向量来存储每个词元，如果词汇表大小 $V$ 很大，则表所消耗的总大小会迅速增加（$O(V.d)$）。
- en: If model size is a constraint for deployment, we can either rely on compression
    techniques (as illustrated earlier) to help with Embedding Table compression,
    or evaluate layers and models that can work around the need for embedding tables.
    Random Projection based methods (Ravi, [2017](#bib.bib129); Ravi and Kozareva,
    [2018](#bib.bib130); Kaliamoorthi et al., [2019](#bib.bib88); Kaliamoorthi et al.,
    [2021](#bib.bib89)) are one such family of models that do so. They propose replacing
    the embedding table and lookup by mapping the input feature $x$ (unicode token
    / word token, etc.), into a lower dimensional space. This is done using the random
    projection operator $\mathbb{P}$, such that $\mathbb{P}(x)\in\{0,1\}^{T.r}$, which
    can be decomposed into $T$ individual projection operations each generating an
    $r$-bit representation ($\mathbb{P}(x)=[\mathbb{P}_{1}(x),...,\mathbb{P}_{T}(x)]$,
    where $\mathbb{P}_{i}(x)\in{0,1}^{r}$). $T$ and $r$ can be manually chosen.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型大小是部署的限制条件，我们可以依赖于压缩技术（如前面所述）来帮助嵌入表压缩，或者评估可以绕过嵌入表需求的层和模型。基于随机投影的方法（Ravi，[2017](#bib.bib129)；Ravi
    和 Kozareva，[2018](#bib.bib130)；Kaliamoorthi 等，[2019](#bib.bib88)；Kaliamoorthi
    等，[2021](#bib.bib89)）就是这样的一类模型。它们提议通过将输入特征 $x$（unicode 词元/词元等）映射到低维空间来替代嵌入表和查找。这是通过随机投影算子
    $\mathbb{P}$ 完成的，使得 $\mathbb{P}(x)\in\{0,1\}^{T.r}$，可以分解为 $T$ 个单独的投影操作，每个生成一个
    $r$-位表示（$\mathbb{P}(x)=[\mathbb{P}_{1}(x),...,\mathbb{P}_{T}(x)]$，其中 $\mathbb{P}_{i}(x)\in{0,1}^{r}$）。$T$
    和 $r$ 可以手动选择。
- en: Each random projection operation $\mathbb{P}_{i}$ is implemented using Locality
    Sensitive Hashing (LSH) (Charikar, [2002](#bib.bib29); Ravi and Kozareva, [2018](#bib.bib130)),
    each using a different hash function (via different seeds). For theoretical guarantees
    about the Random Projection operation, refer to (Charikar, [2002](#bib.bib29)),
    which demonstrates that the operation preserves the similarity between two points
    in the lower-dimensional space it maps these points to (this is crucial for the
    model to be learn the semantics about the inputs). If this relationship holds
    in the lower-dimensional space, the projection operation can be used to learn
    discriminative features for the given input. The core-benefit of the projection
    operation when compared to embedding tables is $O(T)$ space required instead of
    $O(V.d)$ ($T$ seeds required for $T$ hash functions). On the other hand, random-projection
    computation is $O(T)$ too v/s $O(1)$ for embedding table lookup. Hence, the projection
    layer is clearly useful when model size is the primary focus of optimization.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 每次随机投影操作 $\mathbb{P}_{i}$ 是通过局部敏感哈希（LSH）实现的（Charikar，[2002](#bib.bib29)；Ravi
    和 Kozareva，[2018](#bib.bib130)），每次使用不同的哈希函数（通过不同的种子）。有关随机投影操作的理论保证，请参见（Charikar，[2002](#bib.bib29)），该文献展示了该操作在将这些点映射到的低维空间中保持两点之间的相似性（这对于模型学习输入的语义至关重要）。如果这种关系在低维空间中成立，那么投影操作可以用于学习给定输入的判别特征。与嵌入表相比，投影操作的核心优势是所需的空间为
    $O(T)$，而不是 $O(V.d)$（$T$ 个种子用于 $T$ 个哈希函数）。另一方面，随机投影计算也是 $O(T)$，而嵌入表查找是 $O(1)$。因此，当模型大小是优化的主要关注点时，投影层显然是有用的。
- en: Across the various papers in the projection model family, there are subtle differences
    in implementation (computing complex features before ((Ravi and Kozareva, [2018](#bib.bib130)))
    v/s after the projection operation ((Kaliamoorthi et al., [2019](#bib.bib88);
    Sankar et al., [2019](#bib.bib136))), generating a ternary representation instead
    of binary ((Kaliamoorthi et al., [2019](#bib.bib88); Kaliamoorthi et al., [2021](#bib.bib89))),
    applying complex layers and networks on top like Attention ((Kaliamoorthi et al.,
    [2019](#bib.bib88))), QRNN ((Kaliamoorthi et al., [2021](#bib.bib89)))), etc.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在投影模型家族的各种论文中，存在实现上的细微差异（在投影操作之前 ((Ravi 和 Kozareva，[2018](#bib.bib130))) 与之后
    ((Kaliamoorthi 等，[2019](#bib.bib88); Sankar 等，[2019](#bib.bib136)))，生成三元表示而非二元表示
    ((Kaliamoorthi 等，[2019](#bib.bib88); Kaliamoorthi 等，[2021](#bib.bib89)))，在其上应用复杂的层和网络，例如注意力
    ((Kaliamoorthi 等，[2019](#bib.bib88)))，QRNN ((Kaliamoorthi 等，[2021](#bib.bib89)))
    等。
- en: '![Refer to caption](img/f9d2a0056c8713d046cc9a58b29ce7fb.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f9d2a0056c8713d046cc9a58b29ce7fb.png)'
- en: '(a) PRADO Model. Source: (Kaliamoorthi et al., [2019](#bib.bib88)).'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: (a) PRADO 模型。来源：(Kaliamoorthi 等，[2019](#bib.bib88))。
- en: '![Refer to caption](img/2d5a716162080dfe45626291fc3dc837.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2d5a716162080dfe45626291fc3dc837.png)'
- en: '(b) PQRNN Model. Source: (Kaliamoorthi et al., [2021](#bib.bib89))'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: (b) PQRNN 模型。来源：(Kaliamoorthi 等，[2021](#bib.bib89))
- en: '![Refer to caption](img/a832eab0563cbc1932b8146fe044ce99.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a832eab0563cbc1932b8146fe044ce99.png)'
- en: '(c) Proformer Model. Source: (Sankar et al., [2020](#bib.bib137)).'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Proformer 模型。来源：(Sankar 等，[2020](#bib.bib137))。
- en: Figure 20\. Collection of notable Random-Projection based models.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20\. 具有显著性的随机投影模型集合。
- en: 'Some of the Projection-based models (refer to Figure [20](#S3.F20 "Figure 20
    ‣ 3.4.2\. Natural Language Understanding ‣ 3.4\. Efficient Architectures ‣ 3\.
    Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better")) have demonstrated impressive
    results on NLU tasks. PRADO ((Kaliamoorthi et al., [2019](#bib.bib88))) generates
    n-gram features from the projected inputs, followed by having a Multi-Headed Attention
    layer on top. It achieved accuracies comparable to standard LSTM models, while
    being  $100\times$ smaller, and taking 20-40 ms for inference on a Nexus 5X device.
    PQRNN (Kaliamoorthi et al., [2021](#bib.bib89)), another Projection-based model
    that additionally uses a fast RNN implementation (QRNN) (Bradbury et al., [2016](#bib.bib26))
    on top of the projected features. They report outperforming LSTMs while being
    $140\times$ smaller, and achieving $97.1\%$ of the quality of a BERT-like model
    while being $350\times$ smaller.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 一些基于投影的模型（参见图 [20](#S3.F20 "图 20 ‣ 3.4.2\. 自然语言理解 ‣ 3.4\. 高效架构 ‣ 3\. 高效深度学习的全景
    ‣ 高效深度学习：关于让深度学习模型更小、更快、更好的一项调查")）在自然语言理解任务中展示了令人印象深刻的结果。PRADO ((Kaliamoorthi
    等，[2019](#bib.bib88))) 从投影输入中生成 n-gram 特征，然后在其上有一个多头注意力层。它的准确性与标准 LSTM 模型相当，同时体积比
    LSTM 小 $100\times$，在 Nexus 5X 设备上推理需要 20-40 毫秒。PQRNN (Kaliamoorthi 等，[2021](#bib.bib89))，另一种基于投影的模型，额外使用了快速
    RNN 实现 (QRNN) (Bradbury 等，[2016](#bib.bib26)) 在投影特征之上。他们报告说在体积比 LSTM 小 $140\times$
    的情况下超越了 LSTM，并且在体积比 BERT-like 模型小 $350\times$ 的情况下达到了 BERT-like 模型质量的 $97.1\%$。
- en: Proformer (Sankar et al., [2020](#bib.bib137)) introduces a Local Projected
    Attention (LPA) Layer, which combines the Projection operation with localized
    attention. They demonstrate reaching $\approx$ 97.2% BERT-base’s performance while
    occupying only 13% of BERT-base’s memory. ProFormer also had 14.4 million parameters,
    compared to 110 million parameters of BERT-base.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Proformer (Sankar 等，[2020](#bib.bib137)) 引入了一个本地投影注意力 (LPA) 层，它将投影操作与局部注意力相结合。他们展示了在仅占用
    BERT-base 内存的 13% 的情况下，达到约 97.2% 的 BERT-base 性能。ProFormer 也有 1440 万个参数，相比之下，BERT-base
    有 1.1 亿个参数。
- en: 3.5\. Infrastructure
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 基础设施
- en: 'In order to be able to train and run inference efficiently, there has to be
    a robust software and hardware infrastructure foundation. In this section we go
    over both these aspects. Refer to Figure [21](#S3.F21 "Figure 21 ‣ 3.5\. Infrastructure
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") for a mental model
    of the software and hardware infrastructure, and how they interact with each other.
    In this section we provide a non-exhaustive but comprehensive survey of leading
    software and hardware infrastructure components that are critical to model efficiency.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够高效地训练和运行推理，必须有一个强大的软件和硬件基础设施基础。在这一部分，我们将讨论这两个方面。请参阅图 [21](#S3.F21 "图 21
    ‣ 3.5\. 基础设施 ‣ 3\. 高效深度学习的格局 ‣ 高效深度学习：对使深度学习模型更小、更快、更好的调查") 以获取关于软件和硬件基础设施及其相互作用的心理模型。在这一部分，我们提供了一个非详尽但全面的领先软件和硬件基础设施组件的调查，这些组件对模型效率至关重要。
- en: '![Refer to caption](img/26638b7b8bbe071083117579c80d76d2.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/26638b7b8bbe071083117579c80d76d2.png)'
- en: Figure 21\. A visualization of the hardware and software infrastructure with
    emphasis on efficiency. On the left hand side is the model-training phase, which
    generates a trained model checkpoint. This model is then used on the inference
    side, which could either be server-side (conventional machines in cloud or on-prem),
    or on-device (mobile phones, IoT, edge devices, etc.).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21\. 这是一个硬件和软件基础设施的可视化图，重点关注效率。左侧是模型训练阶段，它生成一个训练好的模型检查点。然后在推理阶段使用这个模型，这个阶段可以是在服务器端（云端或本地机器），也可以是在设备端（手机、物联网设备、边缘设备等）。
- en: 3.5.1\. Tensorflow Ecosystem
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1\. Tensorflow 生态系统
- en: Tensorflow (TF) (Abadi et al., [2016](#bib.bib2); Authors, [2021g](#bib.bib15))
    is a popular machine learning framework, that has been used in production by many
    large enterprises. It has some of the most extensive software support for model
    efficiency.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow (TF) (Abadi et al., [2016](#bib.bib2); Authors, [2021g](#bib.bib15))
    是一个流行的机器学习框架，已经被许多大型企业用于生产环境。它具有一些最广泛的软件支持以提高模型效率。
- en: 'Tensorflow Lite for On-Device Usecases: Tensorflow Lite (TFLite) (Authors,
    [2021i](#bib.bib17)) is a collection of tools and libraries designed for inference
    in low-resource environments. At a high-level we can break down the TFLite project
    into two core parts:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 用于设备端使用场景的 Tensorflow Lite：Tensorflow Lite (TFLite) (Authors, [2021i](#bib.bib17))
    是一组用于低资源环境下推理的工具和库。从高层次来看，我们可以将 TFLite 项目分为两个核心部分：
- en: •
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Interpreter and Op Kernels: TFLite provides an interpreter for running specialized
    TFLite models, along with implementations of common neural net operations (Fully
    Connected, Convolution, Max Pooling, ReLu, Softmax, etc. each of which as an *Op*).
    The implementation of such an operation is known as an *Op Kernel*. Both the interpreter
    and Op Kernels are primarily optimized for inference on ARM-based processors as
    of the time of writing this paper. They can also leverage smartphone DSPs such
    as Qualcomm’s Hexagon (Authors, [2021l](#bib.bib20)) for faster execution. The
    interpreter also allows the user to set multiple threads for execution.'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解释器和操作内核：TFLite 提供了一个解释器来运行专门的 TFLite 模型，以及常见神经网络操作的实现（全连接、卷积、最大池化、ReLu、Softmax
    等，每一个都是一个 *Op*）。这种操作的实现被称为 *Op Kernel*。解释器和操作内核主要针对 ARM 架构处理器进行了优化，至撰写本文时，它们也可以利用智能手机
    DSP，如 Qualcomm 的 Hexagon (Authors, [2021l](#bib.bib20)) 来加速执行。解释器还允许用户设置多个线程进行执行。
- en: •
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Converter: The TFLite converter as the name suggests is useful for converting
    the given TF model into a single flatbuffer file for inference by the interpreter.
    Apart from the conversion itself, it handles a lot of internal details like getting
    a graph ready for quantized inference, fusing operations, adding other metadata
    to the model, etc. With respect to quantization, it also allows post-training
    quantization as mentioned earlier with an optional representative dataset to improve
    accuracy.'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转换器：正如名称所示，TFLite 转换器用于将给定的 TF 模型转换成一个用于解释器推理的单一 flatbuffer 文件。除了转换本身，它还处理许多内部细节，如准备用于量化推理的图，融合操作，添加其他元数据等。关于量化，它还允许如前所述的后训练量化，并提供一个可选的代表性数据集以提高准确性。
- en: 'Other Tools for On-Device Inference: TF Micro (Warden and Situnayake, [2019](#bib.bib160))
    goes further, and consists of a slimmed down interpreter, and a smaller set of
    ops for inference on very low resource microcontrollers. TF Model Optimization
    toolkit (Authors, [2020](#bib.bib13)) is a Tensorflow library for applying common
    compression techniques like quantization, pruning, clustering etc. TensorflowJS
    (TF.JS) is a library within the TF ecosystem that can be used to train and run
    neural networks within the browser or using Node.js (Node.js Authors, [2021](#bib.bib114)).
    These models can also accelerated through GPUs via the WebGL interface (Contributors
    to Wikimedia projects, [2021f](#bib.bib43)). It supports both, importing models
    trained in TF, as well as creating new models from scratch in TF.JS.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 用于设备端推断的其他工具：TF Micro (Warden 和 Situnayake, [2019](#bib.bib160)) 更进一步，由一个精简的解释器和一组较小的推断操作组成，适用于非常低资源的微控制器。TF
    Model Optimization toolkit (Authors, [2020](#bib.bib13)) 是一个 Tensorflow 库，用于应用常见的压缩技术，如量化、剪枝、聚类等。TensorflowJS
    (TF.JS) 是 TF 生态系统中的一个库，可用于在浏览器中或使用 Node.js (Node.js Authors, [2021](#bib.bib114))
    训练和运行神经网络。这些模型也可以通过 WebGL 接口 (Contributors to Wikimedia projects, [2021f](#bib.bib43))
    通过 GPU 加速。它支持导入在 TF 中训练的模型以及从头创建新的 TF.JS 模型。
- en: 'XLA for Server-Side Acceleration: Typically a TF model graph is executed by
    TF’s executor process and it uses standard optimized kernels for running it on
    CPU, GPU, etc. XLA (Authors, [2021j](#bib.bib18)) is a graph compiler that can
    optimize linear algebra computations in a model, by generating new kernels that
    are customized for the graph. These kernels are optimized for the model graph
    in question. For example, certain operations which can be fused together are combined
    in a single composite op. This avoids having to do multiple costly writes to RAM,
    when the operands can directly be operated on while they are still in cheaper
    caches. Kanwar et al. (Kanwar et al., [2021](#bib.bib90)) report a 7$\times$ increase
    in training throughput, and 5$\times$ increase in the maximum batch size that
    can be used for BERT training. This allows training a BERT model for $32 on Google
    Cloud.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: XLA 用于服务器端加速：通常，TF 模型图由 TF 的执行器进程执行，并使用标准优化内核在 CPU、GPU 等上运行。XLA (Authors, [2021j](#bib.bib18))
    是一个图编译器，可以通过生成为图定制的新内核来优化模型中的线性代数计算。这些内核针对特定的模型图进行优化。例如，某些可以融合在一起的操作会合并成一个复合操作。这避免了在操作数仍在更便宜的缓存中时进行多次昂贵的
    RAM 写入。Kanwar 等人 (Kanwar et al., [2021](#bib.bib90)) 报告了训练吞吐量提高了 7$\times$，以及用于
    BERT 训练的最大批量大小增加了 5$\times$。这允许在 Google Cloud 上训练一个 $32 的 BERT 模型。
- en: 3.5.2\. PyTorch Ecosystem
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2. PyTorch 生态系统
- en: PyTorch (Paszke et al., [2019](#bib.bib120)) is another popular machine-learning
    platform actively used by both academia and industry. It is often compared with
    Tensorflow in terms of usability and features.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch (Paszke et al., [2019](#bib.bib120)) 是另一个在学术界和工业界广泛使用的流行机器学习平台。它常与 Tensorflow
    在可用性和功能方面进行比较。
- en: 'On-Device Usecases: PyTorch also has a light-weight interpreter that enables
    running PyTorch models on Mobile (Authors, [2021c](#bib.bib10)), with native runtimes
    for Android and iOS. This is analogous to the TFLite interpreter and runtime as
    introduced earlier. Similar to TFLite, PyTorch offers post-training quantization
    (Authors, [2021d](#bib.bib11)), and other graph optimization steps such as constant
    folding, fusing certain operations together, putting the channels last (NHWC)
    format for optimizing convolutional layers.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 设备端使用案例：PyTorch 还具有一个轻量级解释器，使得可以在移动设备上运行 PyTorch 模型 (Authors, [2021c](#bib.bib10))，并提供了适用于
    Android 和 iOS 的原生运行时。这类似于之前介绍的 TFLite 解释器和运行时。类似于 TFLite，PyTorch 提供了训练后量化 (Authors,
    [2021d](#bib.bib11)) 以及其他图优化步骤，如常量折叠、融合某些操作、将通道格式设为最后 (NHWC) 以优化卷积层。
- en: 'General Model Optimization: PyTorch also offers the Just-in-Time (JIT) compilation
    facility (Authors, [2021e](#bib.bib12)), which might seem similar to Tensorflow’s
    XLA, but is actually a mechanism for generating a serializable intermediate representation
    (high-level IR, per (Li, [2020](#bib.bib103))) of the model from the code in TorchScript
    (Authors, [2021e](#bib.bib12)), which is a subset of Python. TorchScript adds
    constraints on the code that it can convert, such as type-checks, which allows
    it to sidestep some pitfalls of typical Python programming, while being Python
    compatible. It allows creating a bridge between the flexible PyTorch code for
    research and development, to a representation that can be deployed for inference
    in production. For example, exporting to TorchScript is a requirement to run on
    mobile devices (Authors, [2021c](#bib.bib10)). This representation is analogous
    to the static inference mode graphs generated by TensorFlow. The alternative for
    XLA in the PyTorch world seem to be the Glow (Rotem et al., [2018](#bib.bib133))
    and TensorComprehension (Vasilache et al., [2018](#bib.bib155)) compilers. They
    help in generating the lower-level intermediate representation that is derived
    from the higher-level IR (TorchScript, TF Graph). These low-level deep learning
    compilers are compared in detail in (Li, [2020](#bib.bib103)).'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 通用模型优化：PyTorch 还提供了即时编译（JIT）功能（作者，[2021e](#bib.bib12)），这可能类似于 Tensorflow 的 XLA，但实际上是一个生成序列化中间表示（高层
    IR，如（Li，[2020](#bib.bib103)））的机制，源自 TorchScript（作者，[2021e](#bib.bib12)），这是 Python
    的一个子集。TorchScript 对其能转换的代码施加约束，如类型检查，这使得它能够绕过典型 Python 编程的一些陷阱，同时保持 Python 兼容性。它允许在研究和开发的灵活
    PyTorch 代码与可以用于生产推理的表示之间创建桥梁。例如，导出到 TorchScript 是在移动设备上运行的要求（作者，[2021c](#bib.bib10)）。这种表示类似于
    TensorFlow 生成的静态推理模式图。在 PyTorch 世界中，XLA 的替代方案似乎是 Glow（Rotem 等，[2018](#bib.bib133)）和
    TensorComprehension（Vasilache 等，[2018](#bib.bib155)）编译器。它们有助于生成从高级 IR（TorchScript，TF
    图）派生的低级中间表示。这些低级深度学习编译器在（Li，[2020](#bib.bib103)）中详细比较。
- en: 'PyTorch offers a model tuning guide (Authors, [2021b](#bib.bib9)), which details
    various options that ML practitioners have at their disposal. Some of the core
    ideas in there are:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了一个模型调优指南（作者，[2021b](#bib.bib9)），详细说明了 ML 从业者可以使用的各种选项。指南中的一些核心思想包括：
- en: •
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Turn on mixed-precision training (Authors, [2021a](#bib.bib8)) when using NVIDIA
    GPUs. This is described further in detail in the GPU sub-section in 3.5.4.
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用 NVIDIA GPUs 时，启用混合精度训练（作者，[2021a](#bib.bib8)）。这在 3.5.4 的 GPU 子章节中有更详细的描述。
- en: •
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fusion of pointwise-operations (add, subtract, multiply, divide, etc.) using
    PyTorch JIT. Even though this should happen automatically, but adding the torch.jit.script
    decorator to methods which are completely composed of pointwise operations can
    force the TorchScript compiler to fuse them.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 PyTorch JIT 融合逐点操作（加、减、乘、除等）。尽管这应该会自动发生，但将 torch.jit.script 装饰器添加到完全由逐点操作组成的方法中，可以强制
    TorchScript 编译器进行融合。
- en: •
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Enabling buffer checkpointing allows keeping the outputs of only certain layers
    in memory, and computing the rest during the backward pass. This specifically
    helps with cheap to compute layers with large outputs like activations. A reduced
    memory usage can be exchanged for a larger batch size which improves utilization
    of the training platform (CPU, GPU, TPU, etc.).
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 启用缓冲区检查点可以仅将某些层的输出保留在内存中，并在反向传递过程中计算其余部分。这特别有助于计算成本较低但输出较大的层，如激活层。减少的内存使用可以换取更大的批量大小，从而提高训练平台（CPU、GPU、TPU
    等）的利用率。
- en: •
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Enabling device-specific optimizations, such as the cuDNN library, and Mixed
    Precision Training with NVIDIA GPUs (explained in the GPU subsection).
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 启用设备特定的优化，如 cuDNN 库，并与 NVIDIA GPUs 进行混合精度训练（在 GPU 子章节中说明）。
- en: •
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Train with Distributed Data Parallel Training, which is suitable when there
    is a large amount of data and multiple GPUs are available for training. Each GPU
    gets its own copy of the model and optimizer, and operates on its own subset of
    the data. Each replicas gradients are periodically accumulated and then averaged.
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用分布式数据并行训练，这适用于数据量大且有多个 GPU 可用于训练的情况。每个 GPU 拥有自己的模型和优化器，并在其数据子集上进行操作。每个副本的梯度会定期累积并平均。
- en: 3.5.3\. Hardware-Optimized Libraries
  id: totrans-357
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3\. 硬件优化库
- en: We can further extract efficiency by optimizing for the hardware the neural
    networks run on. A prime deployment target is ARM’s Cortex-family of processors.
    Cortex supports SIMD (Single-Instruction Multiple Data) instructions via the Neon
    (Ltd., [2021](#bib.bib109)) architecture extension. SIMD instructions are useful
    for operating upon registers with vectors of data, which are essential for speeding
    up linear algebra operations through vectorization of these operations. QNNPACK
    (Dukhan et al., [2020](#bib.bib52)) and XNNPACK (Authors, [2021k](#bib.bib19))
    libraries are optimized for ARM Neon for mobile and embedded devices, and for
    x86 SSE2, AVX architectures, etc. QNNPACK supports several common ops in quantized
    inference mode for PyTorch. XNNPACK supports 32-bit floating point models and
    16-bit floating point for TFLite. If a certain operation isn’t supported in XNNPACK,
    it falls back to the default implementation in TFLite.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过优化神经网络运行的硬件来进一步提高效率。一个主要的部署目标是ARM的Cortex系列处理器。Cortex通过Neon（Ltd., [2021](#bib.bib109)）架构扩展支持SIMD（单指令多数据）指令。SIMD指令对操作包含数据向量的寄存器非常有用，这对于通过向量化这些操作来加速线性代数操作至关重要。QNNPACK（Dukhan
    et al., [2020](#bib.bib52)）和XNNPACK（作者，[2021k](#bib.bib19)）库为ARM Neon优化了移动和嵌入式设备，也为x86
    SSE2、AVX架构等进行了优化。QNNPACK支持PyTorch中量化推理模式下的几种常见操作。XNNPACK支持TFLite中的32位浮点模型和16位浮点。如果XNNPACK不支持某个操作，则会回退到TFLite中的默认实现。
- en: Similarly, there are other low-level libraries like Accelerate for iOS (Apple
    Authors, [2021](#bib.bib7)), and NNAPI for Android (Android Developers, [2021](#bib.bib5))
    that try to abstract away the hardware-level acceleration decision from higher
    level ML frameworks.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，还有其他低级库如iOS的Accelerate（Apple Authors, [2021](#bib.bib7)）和Android的NNAPI（Android
    Developers, [2021](#bib.bib5)），它们试图将硬件级加速决策抽象到更高层的ML框架中。
- en: 3.5.4\. Hardware
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.4 硬件
- en: 'GPU: Graphics Processing Units (GPUs) were originally designed for acclerating
    computer graphics, but began to be used for general-purpose usecases with the
    availability of the CUDA library (Contributors to Wikimedia projects, [2021b](#bib.bib39))
    in 2007, and libraries like like cuBLAS for speeding up linear algebra operations.
    In 2009, Raina et al. (Raina et al., [2009](#bib.bib126)) demonstrated that GPUs
    can be used to accelerate deep learning models. In 2012, following the AlexNet
    model’s (Krizhevsky et al., [2012](#bib.bib93)) substantial improvement over the
    next entrant in the ImageNet competition further standardized the use of GPUs
    for deep learning models. Since then Nvidia has released several iterations of
    its GPU microarchitectures with increasing focus on deep learning performance.
    It has also introduced Tensor Cores (NVIDIA, [2020b](#bib.bib116); Stosic, [2020](#bib.bib143))
    which are dedicated execution units in their GPUs, which are specialized for Deep
    Learning applications. TensorCores support training and inference in a range of
    precisions (fp32, TensorFloat32, fp16, bfloat16, int8, int4). As demonstrated
    earlier in quantization, switching to a lower precision is not always a significant
    trade-off, since the difference in model quality might often be minimal.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: GPU：图形处理单元（GPUs）最初是为了加速计算机图形而设计的，但自2007年CUDA库（Wikimedia项目贡献者，[2021b](#bib.bib39)）可用以来，开始用于通用用途，还有cuBLAS等库用于加速线性代数操作。2009年，Raina等人（Raina
    et al., [2009](#bib.bib126)）展示了GPU可以用于加速深度学习模型。2012年，随着AlexNet模型（Krizhevsky et
    al., [2012](#bib.bib93)）在ImageNet竞赛中的显著改进，进一步规范了GPU在深度学习模型中的使用。自那时起，Nvidia发布了几代GPU微架构，越来越注重深度学习性能。它还引入了Tensor
    Cores（NVIDIA, [2020b](#bib.bib116)；Stosic, [2020](#bib.bib143)），这些是其GPU中的专用执行单元，专门用于深度学习应用。TensorCores支持一系列精度（fp32,
    TensorFloat32, fp16, bfloat16, int8, int4）的训练和推理。正如前面量化部分所示，切换到更低的精度并不总是显著的折衷，因为模型质量的差异往往很小。
- en: '![Refer to caption](img/c17aa85e7f05d74e51e614c19bf6333e.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c17aa85e7f05d74e51e614c19bf6333e.png)'
- en: 'Figure 22\. Reduced Precision Multiply-Accumulate (MAC) operation: An illustration
    of the $\mathbf{A}=(\mathbf{B}\times\mathbf{C})+\mathbf{D}$ operation. $\mathbf{B}$
    and $\mathbf{C}$ are in a reduced precision (fp16, bfloat16, TensorFloat32 etc.),
    while $\mathbf{A}$ and $\mathbf{D}$ are in fp32\. The speedup comes from doing
    the expensive matrix-multiplication with a reduced precision format.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图22. 降低精度的乘加（MAC）操作：$\mathbf{A}=(\mathbf{B}\times\mathbf{C})+\mathbf{D}$操作的示意图。$\mathbf{B}$和$\mathbf{C}$是降低精度（fp16,
    bfloat16, TensorFloat32等），而$\mathbf{A}$和$\mathbf{D}$是fp32。加速来自于以降低精度格式进行昂贵的矩阵乘法。
- en: Tensor Cores optimize the standard Multiply-and-Accumulate (MAC) operation (Contributors
    to Wikimedia projects, [2021d](#bib.bib41)), $\mathbf{A}=(\mathbf{B}\times\mathbf{C})+\mathbf{D}$.
    Where, $\mathbf{B}$ and $\mathbf{C}$ are in a reduced precision (fp16, bfloat16,
    TensorFloat32), while $\mathbf{A}$ and $\mathbf{D}$ are in fp32\. The core speedup
    comes from doing the expensive matrix-multiplication in a lower-precision. The
    result of the multiplication is in fp32, which can be relatively cheaply added
    with $\mathbf{D}.$ When training with reduced-precision, NVIDIA reports between
    1$\times$ to 15$\times$ training speedup depending on the model architecture and
    the GPU chosen (Stosic, [2020](#bib.bib143)). Tensor Cores in NVidia’s latest
    Ampere architecture GPUs also support faster inference with sparsity (specifically,
    structured sparsity in the ratio 2:4, where 2 elements out of a block of 4 elements
    are sparse) (NVIDIA, [2020a](#bib.bib115)). They demonstrate an up to 1.5$\times$
    speed up in inference time, and up to 1.8$\times$ speedup in individual layers.
    NVIDIA also offers the cuDNN libary (NVIDIA, [2020a](#bib.bib115)) that contains
    optimized versions of standard neural network operations such as fully-connected,
    convolution, batch-norm, activation, etc.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: Tensor Cores 优化了标准的乘加（MAC）操作（Contributors to Wikimedia projects，[2021d](#bib.bib41)），$\mathbf{A}=(\mathbf{B}\times\mathbf{C})+\mathbf{D}$。其中，$\mathbf{B}$
    和 $\mathbf{C}$ 为降低精度（fp16, bfloat16, TensorFloat32），而 $\mathbf{A}$ 和 $\mathbf{D}$
    为 fp32。核心加速来自于以较低精度进行昂贵的矩阵乘法。乘法结果为 fp32，可以相对便宜地与 $\mathbf{D}$ 相加。在使用降低精度进行训练时，NVIDIA
    报告的训练加速比为 1$\times$ 至 15$\times$，具体取决于模型架构和所选 GPU（Stosic，[2020](#bib.bib143)）。NVIDIA
    最新的 Ampere 架构 GPU 中的 Tensor Cores 还支持带稀疏性的更快推理（特别是比率为 2:4 的结构化稀疏，其中 4 元素块中的 2
    个元素为稀疏）(NVIDIA，[2020a](#bib.bib115))。它们在推理时间上显示出最高 1.5$\times$ 的加速，在单个层上则最高 1.8$\times$
    的加速。NVIDIA 还提供了 cuDNN 库（NVIDIA，[2020a](#bib.bib115)），其中包含了标准神经网络操作的优化版本，如全连接、卷积、批归一化、激活等。
- en: '![Refer to caption](img/f15154751507a175bc63217988432b1e.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f15154751507a175bc63217988432b1e.png)'
- en: 'Figure 23\. Common floating point format used in Training & Inference: fp32
    is the standard 32-bit floating point number from IEEE-754 standard (Wang and
    Kanwar, [2021](#bib.bib158)). One bit is allocated for storing the sign. The exponent
    controls the range of the floating point value that can be expressed with that
    format, and the mantissa controls the precision. Note that fp16 reduces the precision
    as well as range. The bfloat16 format is a reasonable compromise because it keeps
    the same range as fp32 while trading of precision to take up a total of 16 bits.
    NVidia GPUs also support Tensor Float 32 format that allocates 3 more bits to
    the mantissa than bfloat16 to achieve better precision. However, it takes up a
    total of 19 bits which does not make it a trivially portable format.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23\. 训练和推理中常用的浮点格式：fp32 是 IEEE-754 标准中的标准 32 位浮点数（Wang 和 Kanwar，[2021](#bib.bib158)）。其中一位用于存储符号。指数控制该格式所能表示的浮点值的范围，而尾数控制精度。请注意，fp16
    降低了精度和范围。bfloat16 格式是一种合理的折衷，因为它保持了与 fp32 相同的范围，同时牺牲了精度，总共占用 16 位。NVidia GPU 还支持
    Tensor Float 32 格式，该格式比 bfloat16 多分配了 3 位尾数，以实现更好的精度。然而，它总共占用 19 位，这使得它不是一个容易移植的格式。
- en: 'TPU: TPUs are proprietary application-specific integrated circuits (ASICs)
    that Google has designed to accelerate deep learning applications with Tensorflow.
    Because they are not general purpose devices, they need not cater for any non-ML
    applications (which most GPUs have had to), hence they are finely tuned for parallelizing
    and accelerating linear algebra operations. The first iteration of the TPU was
    designed for inference with 8-bit integers, and was being used in Google for a
    year prior to their announcement in 2016 (Jouppi et al., [2017](#bib.bib87)).
    Subsequent iterations of the TPU architectures enabled both training and inference
    with TPUs in floating point too. Google also opened up access to these TPUs via
    their Google Cloud service in 2018 (Google, [2021a](#bib.bib63)).'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: TPU：TPU 是 Google 设计的专有应用特定集成电路（ASICs），用于加速与 Tensorflow 相关的深度学习应用。由于它们不是通用设备，因此不需要考虑任何非机器学习应用（大多数
    GPU 必须考虑），因此它们在并行化和加速线性代数操作方面进行了精细调整。TPU 的第一次迭代是为使用 8 位整数进行推理而设计的，并在 2016 年发布之前在
    Google 使用了一年（Jouppi 等，[2017](#bib.bib87)）。后续的 TPU 架构迭代支持浮点训练和推理。Google 还于 2018
    年通过其 Google Cloud 服务开放了对这些 TPU 的访问（Google，[2021a](#bib.bib63)）。
- en: '![Refer to caption](img/9c7047448530aba2f3e565dd6bf4f3cf.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9c7047448530aba2f3e565dd6bf4f3cf.png)'
- en: (a) A Systolic Array Cell implementing a Multiply-Accumulate (MAC) operation.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 一个实现乘法累加（MAC）操作的同步阵列单元。
- en: '![Refer to caption](img/237c6a26043b4f432471d9bdf1452fe8.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/237c6a26043b4f432471d9bdf1452fe8.png)'
- en: (b) 4x4 Matrix Multiplication using Systolic Array
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 使用同步阵列进行 4x4 矩阵乘法
- en: 'Figure 24\. Systolic Arrays in TPUs: Figure (a) shows a Systolic Array implementing
    a MAC operation, where the variables $A$ and $B$ are received by the cell, and
    $C$ is the resident memory. $A$ is passed to the horizontally adjacent cell on
    the right, and $B$ is passed to the vertically adjacent cell below on the next
    clock tick. Figure (b) demonstrates how two 4$\times$4 matrices are multiplied
    using Systolic Arrays which is a mesh of cells constructed in Figure (a). The
    $i$-th row of array is fed the $i$-th column of $A$ (preceded by $i-1$ 0s, which
    act as a delay). Similarly, the $i$-th column of the array is fed the $i$-th column
    of $B$ (preceded by $i-1$ 0s). The corresponding $a_{ij}$ and $b_{jk}$ are passed
    to the neighboring cells on the next clock tick.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图 24\. TPU 中的同步阵列：图 (a) 显示了一个实现 MAC 操作的同步阵列，其中变量 $A$ 和 $B$ 被接收到单元中，而 $C$ 是驻留的内存。$A$
    被传递到右边的水平相邻单元，而 $B$ 被传递到下方的垂直相邻单元，在下一个时钟周期。图 (b) 展示了如何使用同步阵列对两个 4$\times$4 矩阵进行乘法运算，同步阵列是图
    (a) 中构造的单元网格。阵列的第 $i$ 行被馈入 $A$ 的第 $i$ 列（前面加上 $i-1$ 个 0 作为延迟）。类似地，阵列的第 $i$ 列被馈入
    $B$ 的第 $i$ 列（前面加上 $i-1$ 个 0）。相应的 $a_{ij}$ 和 $b_{jk}$ 被传递到下一个时钟周期的相邻单元。
- en: 'The core architecture of the TPU chips leverages the Systolic Array design
    (Kung and Leiserson, [1980](#bib.bib95); Kung, [1982](#bib.bib96)) (refer to Figure
    [24](#S3.F24 "Figure 24 ‣ 3.5.4\. Hardware ‣ 3.5\. Infrastructure ‣ 3\. Landscape
    of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep
    Learning Models Smaller, Faster, and Better")), where a large computation is split
    across a mesh-like topology, where each cell computes a partial result and passes
    it on to the next cell in the order, every clock-step (in a rhythmic manner analogous
    to the systolic cardiac rhythm). Since there is no need to access registers for
    the intermediate results, once the required data is fetched the computation is
    not memory bound. Each TPU chip has two Tensor Cores (not to be confused with
    NVidia’s Tensor Cores), each of which has a mesh of systolic arrays. There are
    4 inter-connected TPU chips on a single TPU board. To further scale training and
    inference, a larger number of TPU boards can be connected in a mesh topology to
    form a ’pod’. As per publicly released numbers, each TPU chip (v3) can achieve
    420 teraflops, and a TPU pod can reach 100+ petaflops (Sato, [2021](#bib.bib138)).'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: TPU 芯片的核心架构利用了同步阵列设计（Kung 和 Leiserson，[1980](#bib.bib95)；Kung，[1982](#bib.bib96)）（参见图
    [24](#S3.F24 "图 24 ‣ 3.5.4\. 硬件 ‣ 3.5\. 基础设施 ‣ 3\. 高效深度学习的全景 ‣ 高效深度学习：关于让深度学习模型更小、更快、更好的调查")），其中一个大的计算被拆分成网格状拓扑，在其中每个单元计算一个部分结果，并将其传递给下一个单元，每个时钟周期（以类似于心脏收缩节律的有节奏方式）。由于不需要访问寄存器来获取中间结果，一旦所需数据被提取，计算就不会受制于内存。每个
    TPU 芯片有两个 Tensor Cores（不要与 NVIDIA 的 Tensor Cores 混淆），每个 Tensor Core 都有一个同步阵列的网格。在一个
    TPU 板上有 4 个互连的 TPU 芯片。为了进一步扩展训练和推理，可以将更多的 TPU 板以网格拓扑连接起来形成一个‘pod’。根据公开发布的数据，每个
    TPU 芯片（v3）可以达到 420 teraflops，一个 TPU pod 可以达到 100+ petaflops（Sato，[2021](#bib.bib138)）。
- en: TPUs have been used inside Google for applications like training models for
    Google Search, general purpose BERT models (Devlin et al., [2018](#bib.bib48)),
    for applications like DeepMind’s world beating AlphaGo and AlphaZero models (Schrittwieser
    et al., [2020](#bib.bib139)), and many other research applications (Tan et al.,
    [2019](#bib.bib148)). They have also set model training time records in the MLPerf
    benchmarks. Similar to the GPUs, TPUs support the bfloat16 data-type (Wang and
    Kanwar, [2021](#bib.bib158)) which is a reduced-precision alternative to training
    in full floating point 32-bit precision. XLA support allows transparently switching
    to bfloat16 without any model changes.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: TPU 在 Google 内部被用于如 Google 搜索模型的训练、通用 BERT 模型（Devlin 等，[2018](#bib.bib48)）、DeepMind
    的世界顶级 AlphaGo 和 AlphaZero 模型（Schrittwieser 等，[2020](#bib.bib139)）等应用，以及许多其他研究应用（Tan
    等，[2019](#bib.bib148)）。它们在 MLPerf 基准测试中也创下了模型训练时间的纪录。类似于 GPU，TPU 支持 bfloat16 数据类型（Wang
    和 Kanwar，[2021](#bib.bib158)），这是一种减少精度的替代方案，用于代替全浮点 32 位精度的训练。XLA 支持使得可以在不改变模型的情况下透明地切换到
    bfloat16。
- en: 'EdgeTPU: EdgeTPU is a custom ASIC chip designed by Google for running inference
    on edge devices, with low power requirements (4 Tera Ops / sec (TOPS) using 2
    watts of power (Google, [2021c](#bib.bib65))). Like the TPU, it is specialized
    for accelerating linear algebra operations, but only for inference and with a
    much lower compute budget. It is further limited to only a subset of operations
    (Google, [2021d](#bib.bib66)), and works only with int8 quantized Tensorflow Lite
    models. Google releases the EdgeTPU using the Coral platform in various form-factors,
    ranging from a Raspberry-Pi like Dev Board to independent solderable modules (Google,
    [2021b](#bib.bib64)). It has also been released with the Pixel 4 smartphones as
    the Pixel Neural Core (Rakowski, [2019](#bib.bib127)), for accelerating on-device
    deep learning applications. The EdgeTPU chip itself is smaller than a US penny,
    making it amenable for deployment in many kinds of IoT devices.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 'EdgeTPU: EdgeTPU 是 Google 设计的一款定制 ASIC 芯片，用于在边缘设备上进行推理，具有低功耗需求（4 Tera Ops /
    sec (TOPS) 使用 2 瓦特的功率 (Google, [2021c](#bib.bib65))）。与 TPU 类似，它专门用于加速线性代数操作，但仅用于推理，并且计算预算要低得多。它还限制在仅支持部分操作
    (Google, [2021d](#bib.bib66))，并且仅与 int8 量化的 Tensorflow Lite 模型兼容。Google 通过 Coral
    平台以多种形式发布 EdgeTPU，从类似 Raspberry-Pi 的开发板到独立的可焊接模块 (Google, [2021b](#bib.bib64))。它还与
    Pixel 4 智能手机一起发布，作为 Pixel Neural Core (Rakowski, [2019](#bib.bib127))，用于加速设备上的深度学习应用。EdgeTPU
    芯片本身比美分硬币还小，使其适合部署在多种 IoT 设备中。'
- en: 'Jetson: Jetson (NVIDIA, [2021](#bib.bib117)) is a family of accelerators by
    Nvidia to enable deep learning applications for embedded and IoT devices. It comprises
    of the Nano, which is a low-powered "system on a module" (SoM) designed for lightweight
    deployments, as well as the more powerful Xavier and TX variants, which are based
    on the NVidia Volta and Pascal GPU architectures. As expected, the difference
    within the Jetson family is primarily the type and number of GPU cores on the
    accelerators. This makes the Nano suited for applications like home automation,
    and the rest for more compute intensive applications like industrial robotics.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jetson: Jetson (NVIDIA, [2021](#bib.bib117)) 是 Nvidia 推出的加速器系列，旨在为嵌入式和 IoT
    设备提供深度学习应用。它包括 Nano，这是一款低功耗的“模块化系统” (SoM)，设计用于轻量级部署，以及更强大的 Xavier 和 TX 变体，基于 NVidia
    Volta 和 Pascal GPU 架构。正如预期，Jetson 系列的差异主要在于加速器上 GPU 核心的类型和数量。这使得 Nano 适合家庭自动化等应用，其余型号则适用于更计算密集型的应用，如工业机器人。'
- en: 4\. A Practitioner’s Guide to Efficiency
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 从业者的效率指南
- en: '![Refer to caption](img/1850d890bed1a157a8ec2244253459f1.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1850d890bed1a157a8ec2244253459f1.png)'
- en: 'Figure 25\. Trade off between Model Quality and Footprint: There exists a trade-off
    between model quality and model footprint. Model quality can be improved with
    techniques like distillation, data-augmentation, hyper-param tuning etc. Compression
    techniques can in turn help trade off some model quality for a better model footprint.
    Some / all of the improvement in footprint metrics can also be traded for better
    quality by simply adding more model capacity.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 图 25\. 模型质量与足迹之间的权衡：模型质量与模型足迹之间存在权衡。可以通过诸如蒸馏、数据增强、超参数调优等技术来提高模型质量。压缩技术则可以帮助在某种程度上牺牲模型质量，以获得更好的模型足迹。通过简单地增加更多模型容量，一些/所有的足迹指标改进也可以被用来换取更好的质量。
- en: So far, we presented a broad set of tools and techniques in the Efficient Deep
    Learning landscape. In this section, we present a practical guide for practitioners
    to use, and how these tools and techniques work with each other. As mentioned
    earlier, what we seek are *pareto-optimal* models, where we would like to achieve
    the best possible result in one dimension, while holding the other dimensions
    constant. Typically, one of these dimensions is Quality, and the other is Footprint.
    Quality related metrics could included Accuracy, F1, Precision, Recall, AUC, etc.
    While Footprint related metrics can include Model Size, Latency, RAM, etc.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，我们在高效深度学习领域介绍了一系列广泛的工具和技术。在这一部分，我们提供了一个实际的指南，供从业者使用，并说明这些工具和技术如何相互配合。如前所述，我们追求的是*帕累托最优*模型，我们希望在一个维度上取得最佳结果，同时保持其他维度不变。通常，其中一个维度是质量，另一个是足迹。质量相关指标可能包括准确度、F1、精度、召回率、AUC
    等。而足迹相关指标可以包括模型大小、延迟、RAM 等。
- en: 'Naturally, there exists a trade-off between Quality and Footprint metrics.
    A higher-capacity / deeper model is more likely to achieve a better accuracy,
    but at the cost of model size, latency, etc. On the other hand a model with lesser
    capcity / shallower, while possibly suitable for deployment, is also likely to
    be worse in accuracy. As illustrated in Figure [25](#S4.F25 "Figure 25 ‣ 4\. A
    Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better"), we can traverse from a model
    with better quality metrics, and exchange some of the quality for better footprint
    by naively compressing the model / reducing the model capacity (Shrink). Similarly
    it is possible to naively improve quality by adding more capacity to the model
    (Grow). Growing can be addressed by the author of the model via appropriately
    increasing model capacity and tweaking other hyper-parameters to improve model
    quality. Shrinking can be achieved via Compression Techniques (Quantization, Pruning,
    Low-Rank Approximation, etc.), Efficient Layers & Models, Architecture Search
    via Automation, etc. In addition, we can also Improve the quality metrics, while
    keeping the footprint same through Learning Techniques (Distillation, Data Augmentation,
    Self-Supervised Tuning), Hyper-Parameter Tuning, etc. (See Table [4](#S4.T4 "Table
    4 ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") for more examples.)'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '自然地，质量与占用空间之间存在权衡。更高容量/更深层次的模型更可能实现更好的准确性，但代价是模型大小、延迟等。另一方面，容量较小/较浅的模型虽然可能适合部署，但准确性也可能较差。如图[25](#S4.F25
    "Figure 25 ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning:
    A Survey on Making Deep Learning Models Smaller, Faster, and Better")所示，我们可以从一个具有更好质量指标的模型开始，通过简单地压缩模型/减少模型容量（缩小）来交换部分质量以获得更好的占用空间。同样，通过向模型中添加更多容量（增长）可以简单地提高质量。增长可以通过适当增加模型容量并调整其他超参数来解决。缩小可以通过压缩技术（量化、剪枝、低秩近似等）、高效层与模型、自动化架构搜索等实现。此外，我们还可以通过学习技术（蒸馏、数据增强、自监督调优）、超参数调优等在保持占用空间不变的情况下提高质量指标。（更多例子见表[4](#S4.T4
    "Table 4 ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning:
    A Survey on Making Deep Learning Models Smaller, Faster, and Better")。）'
- en: '|'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Grow &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 增长 &#124;'
- en: '&#124; (Model Capacity) &#124;'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (模型容量) &#124;'
- en: '|'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Shrink &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 缩小 &#124;'
- en: '&#124; (Footprint) &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (占用空间) &#124;'
- en: '|'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Improve &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 改进 &#124;'
- en: '&#124; (Quality) &#124;'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （质量）&#124;'
- en: '|'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Add layers, width, etc. either manually or using width / depth / compound
    scaling multipliers |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 手动或使用宽度/深度/复合缩放因子添加层、宽度等 |'
- en: '&#124; Reduce layers, width, etc. &#124;'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 减少层、宽度等 &#124;'
- en: '&#124; either manually or using &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 手动或使用&#124;'
- en: '&#124; width / depth / compound &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 宽度/深度/复合 &#124;'
- en: '&#124; scaling multipliers &#124;'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 缩放因子 &#124;'
- en: '|'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Manual Tuning (Architecture / &#124;'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 手动调优（架构/ &#124;'
- en: '&#124; Hyper-Parameters / &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 超参数/ &#124;'
- en: '&#124; Features, etc.) &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征等）&#124;'
- en: '|'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Compression Techniques: &#124;'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 压缩技术: &#124;'
- en: '&#124; Quantization, Pruning, &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 量化，剪枝，&#124;'
- en: '&#124; Low-Rank Factorization, etc. &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低秩分解等 &#124;'
- en: '|'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Learning Techniques: &#124;'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习技术: &#124;'
- en: '&#124; Data-Augmentation, Distillation, &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据增强、蒸馏，&#124;'
- en: '&#124; Unsupervised Learning, etc. &#124;'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无监督学习等 &#124;'
- en: '|'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Automation: &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自动化: &#124;'
- en: '&#124; Hyper-Param Optimization, &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 超参数优化，&#124;'
- en: '&#124; Architecture Search, etc. &#124;'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 架构搜索等 &#124;'
- en: '|'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Automation: &#124;'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自动化: &#124;'
- en: '&#124; Hyper-Param Optimization, &#124;'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 超参数优化，&#124;'
- en: '&#124; Architecture Search, etc. &#124;'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 架构搜索等 &#124;'
- en: '|'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Efficient Layers & Models: &#124;'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高效层与模型: &#124;'
- en: '&#124; Projection, PQRNN, (NLP), &#124;'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 投影、PQRNN（NLP），&#124;'
- en: '&#124; Separable Convolution (Vision), &#124;'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可分离卷积（视觉），&#124;'
- en: '&#124; etc. &#124;'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 等等 &#124;'
- en: '|'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Efficient Layers & Models: &#124;'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高效层与模型: &#124;'
- en: '&#124; Transformers (NLP), &#124;'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Transformer（NLP），&#124;'
- en: '&#124; Vi-T (Vision), etc. &#124;'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Vi-T（视觉）等 &#124;'
- en: '|'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 4\. Examples of techniques to use in the Grow, Shrink, and Improve phases.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 表4. 在增长、缩小和改进阶段使用的技术示例。
- en: 'Combining these three phases, we propose two strategies towards achieving pareto-optimal
    models:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '结合这三个阶段，我们提出了两种策略以实现帕累托最优模型:'
- en: (1)
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Shrink-and-Improve for Footprint-Sensitive Models: If as a practitioner, you
    want to reduce your footprint, while keeping the quality the same, this could
    be a useful strategy for on-device deployments and server-side model optimization.
    Shrinking should ideally be minimally lossy in terms of quality (can be achieved
    via learned compression techniques, architecture search etc.), but in some cases
    even naively reducing capacity can also be compensated by the Improve phase. It
    is also possible to do the Improve phase before the Shrink phase.'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 足迹敏感模型的**缩减-改进**策略：如果作为从业者，你希望减少占用空间，同时保持相同的质量，这对于设备上的部署和服务器端模型优化可能是一种有用的策略。缩减应理想地在质量上尽可能少地损失（可以通过学习压缩技术、架构搜索等实现），但在某些情况下，即使是简单地减少容量，也可以通过改进阶段来弥补。也可以在缩减阶段之前进行改进阶段。
- en: (2)
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Grow-Improve-and-Shrink for Quality-Sensitive Models: When you want to deploy
    models that have better quality while keeping the same footprint, it might make
    sense to follow this strategy. Here, the capacity is first added by growing the
    model as illustrated earlier. The model is then improved using via learning techniques,
    automation, etc. and then shrunk back either naively or in a learned manner. Alternatively,
    the model could be shrunk back either in a learned manner directly after growing
    the model too.'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 质量敏感模型的**增长-改进-缩减**策略：当你希望在保持相同占用空间的情况下部署更高质量的模型时，采用这种策略可能是合理的。这里，首先通过增长模型来增加容量，如前面所述。然后，利用学习技术、自动化等手段来改进模型，最后将模型缩减回原始大小，可能是简单的方式，也可能是学习的方式。或者，模型在增长后也可以直接以学习的方式进行缩减。
- en: We consider both these strategies as a way of going from a potentially non pareto-optimal
    model to another one that lies on the pareto-frontier with the trade-off that
    is appropriate for the user. Each efficiency technique individually helps move
    us closer to that target model.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为这两种策略都是从一个潜在的非帕累托最优模型转变为另一个位于帕累托前沿的模型的方式，用户可以根据需要选择合适的权衡。每种效率技术单独都能帮助我们更接近那个目标模型。
- en: 4.1\. Experiments
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 实验
- en: 'In order to demonstrate what we proposed above, we undertook the task of going
    through the exercise of making a given Deep Learning model efficient. Concretely,
    we had the following goals with this exercise:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们上面提出的内容，我们进行了使给定深度学习模型高效的任务。具体而言，我们在此练习中设定了以下目标：
- en: (1)
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Achieve a new pareto-frontier using the efficiency techniques. Hence, demonstrating
    that these techniques can be used in isolation as well as in combination with
    other techniques, in the real-world by ML Practitioners.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用效率技术实现新的帕累托前沿。由此展示这些技术可以在实际应用中被单独使用，也可以与其他技术结合使用，来证明这些技术的有效性。
- en: (2)
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: With various combinations of efficiency techniques and model scaling, demonstrate
    the tradeoffs for both ‘Shrink-and-Improve’, and ‘Grow-Improve-and-Shrink’ strategies
    for discovering and traversing the pareto-frontier. In other words, provide empirical
    evidence that it is possible for practitioners to either reduce model capacity
    to bring down the footprint (shrink) and then recover the model quality that they
    traded off (improve), or increase the model capacity to improve quality (growing)
    followed by model compression (shrinking) to improve model footprint.
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过各种效率技术和模型缩放的组合，展示‘缩减-改进’和‘增长-改进-缩减’策略在发现和遍历帕累托前沿时的权衡。换句话说，提供实证证据，证明从业者可以减少模型容量以降低占用空间（缩减），然后恢复他们所牺牲的模型质量（改进），或者增加模型容量以提高质量（增长），然后进行模型压缩（缩减）以改善模型占用空间。
- en: We picked the problem of classifying images in the CIFAR-10 dataset (Krizhevsky
    et al., [2009](#bib.bib92)) on compute constrained devices such as smartphones,
    IoT devices etc. We designed a deep convolutional architecture where we could
    scale the model capacity up or down, by increasing or decreasing the ‘width multiplier’
    ($w$) value. In the implementation, $w$ scales the number of filters for the convolutional
    layers (except the first two). Hence, using different values of $w$ in $[0.1,0.25,0.5,0.75,1.0]$
    we obtain a family of models with different quality and footprint tradeoffs. We
    trained these models with some manual tuning to achieve a baseline of quality
    v/s footprint metrics. In this case, we measured quality through accuracy, and
    footprint through number of parameters, model size, and latency. In terms of techniques,
    we used Quantization for Shrinking, and Data Augmentation and Distillation for
    Improving. Many other techniques could be used to further drive the point home
    (Automation such as Hyper-Parameter Tuning, Efficient Layers such as Separable
    Convolutions), but were skipped to keep the interpretation of the results simpler.
    We used the Tensorflow-backed Keras APIs (Chollet, [2020](#bib.bib36)) for training,
    and the TFLite (Authors, [2021i](#bib.bib17)) framework for inference. The latencies
    were measured on three kinds of devices, low-end (Oppo A5), mid-end (Pixel 3XL),
    and high-end (Galaxy S10), in order of their increasing CPU compute power. The
    model size numbers reported are the sizes of the generated TFLite models, and
    the latency numbers are the average single-threaded CPU latency after warmup on
    the target device. The code for the experiments is available via an IPython notebook
    [here](https://github.com/reddragon/efficient-dl-survey-paper/blob/main/CIFAR_10_End_to_End.ipynb).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了在计算资源受限的设备（如智能手机、物联网设备等）上对 CIFAR-10 数据集（Krizhevsky 等，[2009](#bib.bib92)）中的图像进行分类的问题。我们设计了一个深度卷积架构，通过增加或减少“宽度乘数”（$w$）值来调整模型容量。在实现中，$w$
    调整卷积层（前两个层除外）的过滤器数量。因此，使用不同的 $w$ 值在 $[0.1,0.25,0.5,0.75,1.0]$ 范围内，我们得到了一个具有不同质量和占用空间折中模型的家族。我们对这些模型进行了手动调优，以达到质量与占用空间指标的基线。在这种情况下，我们通过准确率来衡量质量，通过参数数量、模型大小和延迟来衡量占用空间。在技术方面，我们使用了量化来缩小模型规模，数据增强和蒸馏来提高性能。许多其他技术（如超参数调优的自动化，分离卷积等高效层）也可以用来进一步阐明问题，但为了简化结果解释，这些技术被略过。我们使用了基于
    Tensorflow 的 Keras API（Chollet，[2020](#bib.bib36)）进行训练，并使用 TFLite（Authors，[2021i](#bib.bib17)）框架进行推理。延迟是在三种设备上测量的，分别是低端（Oppo
    A5）、中端（Pixel 3XL）和高端（Galaxy S10），按其 CPU 计算能力的递增顺序。报告的模型大小是生成的 TFLite 模型的大小，延迟数据是目标设备上经过预热后的平均单线程
    CPU 延迟。实验代码可通过 IPython notebook [这里](https://github.com/reddragon/efficient-dl-survey-paper/blob/main/CIFAR_10_End_to_End.ipynb)
    获取。
- en: 'Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide
    to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better") compiles the results for 6 width-multipliers in
    increasing order, ranging from $0.05$ to $1.0$. Between the smallest to the largest
    models, the number of params grows by $\approx 91.4\times$, and the model size
    grows by $\approx 80.2\times$. The latency numbers also grow between $3.5-10\times$
    based on the device. Within the same row, footprint metrics will not change since
    we are not changing the model architecture. In Table [5](#S4.T5 "Table 5 ‣ 4.1\.
    Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning:
    A Survey on Making Deep Learning Models Smaller, Faster, and Better") we purely
    work with techniques that will improve the model quality (Data Augmentation and
    Distillation). Table [6](#S4.T6 "Table 6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s
    Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better") reports the numbers for the Quantized versions
    of the corresponding models in Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣
    4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey on
    Making Deep Learning Models Smaller, Faster, and Better"). We use Quantization
    for the Shrink phase, to reduce model size by $\approx 4\times$, and reduce the
    average latency by $1.5-2.65\times$. Figures [26](#S4.F26 "Figure 26 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") and [27](#S4.F27
    "Figure 27 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    plot the notable results from Tables [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣
    4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey on
    Making Deep Learning Models Smaller, Faster, and Better") and [6](#S4.T6 "Table
    6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep
    Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better").'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency
    ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster,
    and Better") 汇总了 6 个宽度乘数的结果，范围从 $0.05$ 到 $1.0$。从最小到最大模型，参数数量增长了 $\approx 91.4\times$，模型大小增长了
    $\approx 80.2\times$。根据设备，延迟数字也在 $3.5-10\times$ 之间增长。在同一行内，足迹指标不会改变，因为我们没有改变模型架构。在表
    [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency
    ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster,
    and Better") 中，我们纯粹使用提升模型质量的技术（数据增强和蒸馏）。表 [6](#S4.T6 "Table 6 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") 报告了表 [5](#S4.T5 "Table
    5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep
    Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    中相应模型的量化版本的数据。我们使用量化来缩小阶段，将模型大小减少 $\approx 4\times$，并将平均延迟减少 $1.5-2.65\times$。图
    [26](#S4.F26 "Figure 26 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency
    ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster,
    and Better") 和 [27](#S4.F27 "Figure 27 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s
    Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better") 绘制了表 [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") 和 [6](#S4.T6 "Table
    6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep
    Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    中的显著结果。'
- en: '| Width Multiplier | # Params (K) | Model Size (KB) |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 宽度乘数 | 参数数量 (K) | 模型大小 (KB) |'
- en: '&#124; Accuracy &#124;'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确率 &#124;'
- en: '&#124; (%) &#124;'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (%) &#124;'
- en: '| Average Latency (ms) |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| 平均延迟 (ms) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Baseline | Augmentation |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 增强 |'
- en: '&#124; Augmentation &#124;'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 增强 &#124;'
- en: '&#124; + Distillation &#124;'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + 蒸馏 &#124;'
- en: '|'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Oppo &#124;'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Oppo &#124;'
- en: '&#124; A5 &#124;'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A5 &#124;'
- en: '|'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Pixel &#124;'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 像素 &#124;'
- en: '&#124; 3XL &#124;'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3XL &#124;'
- en: '|'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Galaxy &#124;'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Galaxy &#124;'
- en: '&#124; S10 &#124;'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S10 &#124;'
- en: '|'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0.05 | 14.7 | 65.45 | 70.17 | 71.71 | 72.89 | 6.72 | 0.6 | 0.78 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| 0.05 | 14.7 | 65.45 | 70.17 | 71.71 | 72.89 | 6.72 | 0.6 | 0.78 |'
- en: '| 0.1 | 26 | 109.61 | 75.93 | 78.22 | 78.93 | 6.85 | 1.7 | 0.85 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| 0.1 | 26 | 109.61 | 75.93 | 78.22 | 78.93 | 6.85 | 1.7 | 0.85 |'
- en: '| 0.25 | 98.57 | 392.49 | 80.6 | 84.14 | 84.51 | 8.15 | 2.02 | 0.93 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| 0.25 | 98.57 | 392.49 | 80.6 | 84.14 | 84.51 | 8.15 | 2.02 | 0.93 |'
- en: '| 0.5 | 350.05 | 1374.11 | 83.04 | 87.47 | 88.03 | 11.46 | 2.8 | 1.33 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 350.05 | 1374.11 | 83.04 | 87.47 | 88.03 | 11.46 | 2.8 | 1.33 |'
- en: '| 0.75 | 764.87 | 2993.71 | 83.79 | 89.06 | 89.51 | 16.7 | 4.09 | 1.92 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| 0.75 | 764.87 | 2993.71 | 83.79 | 89.06 | 89.51 | 16.7 | 4.09 | 1.92 |'
- en: '| 1 | 1343.01 | 5251.34 | 84.42 | 89.41 | 89.92 | 24 | 5.99 | 2.68 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1343.01 | 5251.34 | 84.42 | 89.41 | 89.92 | 24 | 5.99 | 2.68 |'
- en: Table 5\. Quality and Footprint metrics for Floating-Point models for the CIFAR-10
    dataset.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 表5\. CIFAR-10数据集上浮点模型的质量和占用指标。
- en: '| Width Multiplier | # Params (K) | Model Size (KB) |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 宽度倍增器 | 参数数量 (K) | 模型大小 (KB) |'
- en: '&#124; Accuracy &#124;'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确率 &#124;'
- en: '&#124; (%) &#124;'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (%) &#124;'
- en: '| Average Latency (ms) |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| 平均延迟 (ms) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Baseline | Augmentation |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 增强 |'
- en: '&#124; Augmentation &#124;'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 增强 &#124;'
- en: '&#124; + Distillation &#124;'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + 蒸馏 &#124;'
- en: '|'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Oppo &#124;'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Oppo &#124;'
- en: '&#124; A5 &#124;'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A5 &#124;'
- en: '|'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Pixel &#124;'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 像素 &#124;'
- en: '&#124; 3XL &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3XL &#124;'
- en: '|'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Galaxy &#124;'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 银河 &#124;'
- en: '&#124; S10 &#124;'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S10 &#124;'
- en: '|'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0.05 | 14.7 | 26.87 | 69.9 | 71.72 | 72.7 | 4.06 | 0.49 | 0.43 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| 0.05 | 14.7 | 26.87 | 69.9 | 71.72 | 72.7 | 4.06 | 0.49 | 0.43 |'
- en: '| 0.1 | 26 | 38.55 | 75.98 | 78.19 | 78.55 | 4.5 | 1.25 | 0.47 |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| 0.1 | 26 | 38.55 | 75.98 | 78.19 | 78.55 | 4.5 | 1.25 | 0.47 |'
- en: '| 0.25 | 98.57 | 111 | 80.76 | 83.98 | 84.18 | 4.52 | 1.31 | 0.48 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| 0.25 | 98.57 | 111 | 80.76 | 83.98 | 84.18 | 4.52 | 1.31 | 0.48 |'
- en: '| 0.5 | 350.05 | 359.31 | 83 | 87.32 | 87.86 | 6.32 | 1.73 | 0.58 |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 350.05 | 359.31 | 83 | 87.32 | 87.86 | 6.32 | 1.73 | 0.58 |'
- en: '| 0.75 | 764.87 | 767.09 | 83.6 | 88.57 | 89.29 | 8.53 | 2.36 | 0.77 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| 0.75 | 764.87 | 767.09 | 83.6 | 88.57 | 89.29 | 8.53 | 2.36 | 0.77 |'
- en: '| 1 | 1343.01 | 1334.41 | 84.52 | 89.28 | 89.91 | 11.73 | 3.27 | 1.01 |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1343.01 | 1334.41 | 84.52 | 89.28 | 89.91 | 11.73 | 3.27 | 1.01 |'
- en: 'Table 6\. Quality and Footprint metrics for *Quantized* models for the CIFAR-10
    dataset. Each model is the quantized equivalent of the corresponding model in
    Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to
    Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better").'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 表6\. CIFAR-10数据集上*量化*模型的质量和占用指标。每个模型都是表[5](#S4.T5 "表 5 ‣ 4.1\. 实验 ‣ 4\. 实践者效率指南
    ‣ 高效深度学习：关于如何使深度学习模型更小、更快、更好的调查")中对应模型的量化等效模型。
- en: '![Refer to caption](img/7619ccc3cb5f7598bc8a351b1aa1f6af.png)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7619ccc3cb5f7598bc8a351b1aa1f6af.png)'
- en: (a) Number of Params v/s Accuracy
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 参数数量与准确率
- en: '![Refer to caption](img/007f6cf4a51d3ec99de87dbb4ab1ae39.png)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/007f6cf4a51d3ec99de87dbb4ab1ae39.png)'
- en: (b) Model Size v/s Accuracy
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 模型大小与准确率
- en: 'Figure 26\. Change in Accuracy with respect to Number of Params and Model Size.
    Each point on a curve is a model from Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") in figure (a) and
    from Table [6](#S4.T6 "Table 6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide
    to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better") in figure (b).'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 图26\. 准确率相对于参数数量和模型大小的变化。曲线上的每一点都是图（a）中表[5](#S4.T5 "表 5 ‣ 4.1\. 实验 ‣ 4\. 实践者效率指南
    ‣ 高效深度学习：关于如何使深度学习模型更小、更快、更好的调查")中的一个模型，图（b）中来自表[6](#S4.T6 "表 6 ‣ 4.1\. 实验 ‣ 4\.
    实践者效率指南 ‣ 高效深度学习：关于如何使深度学习模型更小、更快、更好的调查")中的一个模型。
- en: '![Refer to caption](img/f55914e6f499a13e2c51e8c240c864db.png)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f55914e6f499a13e2c51e8c240c864db.png)'
- en: (a) Low-End Device Latency
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 低端设备延迟
- en: '![Refer to caption](img/aa9f166d8e5ba3c57c2e4c2d8f3d5652.png)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/aa9f166d8e5ba3c57c2e4c2d8f3d5652.png)'
- en: (b) Mid-Tier Device Latency
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 中端设备延迟
- en: '![Refer to caption](img/08c4064b319de0a020d218bf14c00f89.png)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/08c4064b319de0a020d218bf14c00f89.png)'
- en: (c) High-End Device Latency
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 高端设备延迟
- en: Figure 27\. Average latency of models on different devices (low-, mid-, and
    high-end smartphones). The orange curve denotes the quantized models in addition
    to being trained with distillation and data augmentation.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 图27\. 不同设备（低端、中端和高端智能手机）上的模型平均延迟。橙色曲线表示量化模型，并且这些模型经过了蒸馏和数据增强的训练。
- en: 4.2\. Discussion
  id: totrans-510
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 讨论
- en: Let us try to interpret the above data to validate if our strategies can be
    used practically.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试解释上述数据，以验证我们的策略是否可以实际应用。
- en: 'Shrink-and-Improve for Footprint-Sensitive Models: Refer to Table [5](#S4.T5
    "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    and Figure [26](#S4.F26 "Figure 26 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s
    Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better"). If our goal was to deploy the model with
    Width Multiplier ($w$) = $1.0$ and accuracy $84.42\%$, but the bottleneck was
    the model size (5.25 MB) and latency on a low-end device (24 ms on Oppo A5). This
    is the classic case of the footprint metrics not meeting the bar, hence we could
    apply the Shrink-and-Improve strategy, by first naively scaling our model down
    to a Width Multiplier ($w$) of $0.25$. This smaller model when manually tuned,
    as seen in Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s
    Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better"), achieves an accuracy of $80.76\%$. However,
    when we use a combination of Data Augmentation & Distillation from a separately
    trained larger teacher model with an accuracy of $90.86\%$, the accuracy of the
    smaller model improves to $84.18\%$, very close to the target model that we want
    to deploy. The size of this smaller model is 392.49 KB, which is $13.8\times$
    smaller, and the latency is 8.15 ms, which is $2.94\times$ faster at a comparable
    accuracy. It is possible to further compress this model by using Quantization
    for some additional shrinking. The same smaller model ($w=0.25$) when Quantized
    in Table [6](#S4.T6 "Table 6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide
    to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better"), is 111 KB in size ($47.3\times$ smaller) and has
    a latency of 4.52 ms ($5.31\times$ faster), while retaining an accuracy of $84.18\%$.
    It is possible to do this for other pairs of points on the curves.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '足迹敏感模型的压缩与改进：请参阅表格[5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s
    Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better")和图[26](#S4.F26 "Figure 26 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better")。如果我们的目标是部署一个宽度乘数($w$)为$1.0$且准确率为$84.42\%$的模型，但瓶颈是模型大小（5.25
    MB）和在低端设备上的延迟（Oppo A5上的24 ms）。这是一个经典的足迹指标未达标的情况，因此我们可以应用压缩与改进策略，首先将我们的模型简单地缩小到宽度乘数($w$)为$0.25$。这个较小的模型经过手动调优，如表[5](#S4.T5
    "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")所示，达到了$80.76\%$的准确率。然而，当我们使用从一个准确率为$90.86\%$的独立训练的大模型中进行的数据增强与蒸馏组合时，这个较小模型的准确率提高到$84.18\%$，非常接近我们希望部署的目标模型。这个较小模型的大小为392.49
    KB，比原模型小$13.8\times$，延迟为8.15 ms，比原模型快$2.94\times$，在相似的准确率下。通过量化进一步压缩该模型是可能的。经过量化的相同较小模型（$w=0.25$）在表[6](#S4.T6
    "Table 6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")中，大小为111
    KB（比原模型小$47.3\times$），延迟为4.52 ms（比原模型快$5.31\times$），同时保持$84.18\%$的准确率。其他曲线上的点对也可以进行类似的处理。'
- en: 'Grow-Improve-Shrink for Quality-Sensitive Models: Assuming our goal is to deploy
    a model that has footprint metrics comparable to the model with $w=0.25$ (392.49
    KB model size, 0.93 ms on a high-end Galaxy S10 device), but an accuracy better
    than the baseline $80.6\%$ (refer to Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better")). In this case, we
    can choose to first grow our model to $w=0.5$. This instantly blows up the model
    size to 1.37 MB ($3.49\times$ bigger), and latency to 1.33 ms ($1.43\times$ slower).
    However, we ignore that for a bit and improve our model’s quality to $88.03\%$
    with Data Augmentation & Distillation. Then using Quantization for shrinking (refer
    to Table [6](#S4.T6 "Table 6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide
    to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better")), we can get a model that is 359.31 KB in size (32
    KB smaller) and has a 0.58 ms latency on Galaxy S10 ($1.6\times$ faster), with
    an accuracy of $87.86\%$, an absolute 7.10% increase in accuracy while keeping
    the model size approximately same and making it $1.6\times$ faster. It is also
    possible to apply this strategy to other pairs of models.'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '对于质量敏感模型的增长-改进-收缩：假设我们的目标是部署一个模型，其足迹指标与 $w=0.25$ 的模型（392.49 KB 模型大小，在高端 Galaxy
    S10 设备上的延迟为 0.93 ms）相当，但准确率优于基线的 $80.6\%$（参见表 [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better")）。在这种情况下，我们可以选择首先将模型增长到
    $w=0.5$。这会立即将模型大小扩大到 1.37 MB（$3.49\times$ 更大），延迟增加到 1.33 ms（$1.43\times$ 更慢）。然而，我们暂时忽略这一点，并通过数据增强与蒸馏将模型质量提高到
    $88.03\%$。然后，使用量化进行收缩（参见表 [6](#S4.T6 "Table 6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s
    Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better")），我们可以得到一个大小为 359.31 KB（比原模型小 32 KB）的模型，在
    Galaxy S10 上的延迟为 0.58 ms（$1.6\times$ 更快），准确率为 $87.86\%$，在保持模型大小基本不变的同时，准确率绝对提高了
    7.10%，并使其速度提升了 $1.6\times$。这种策略也可以应用于其他模型对。'
- en: 'Thus, we’ve verified that the above two strategies can work both ways, whether
    your goal is to optimize for quality metrics or footprint metrics. We were also
    able to visually inspect through Figures [26](#S4.F26 "Figure 26 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") and [27](#S4.F27
    "Figure 27 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    that efficiency techniques can improve on the pareto frontiers constructed through
    manual tuning. To contain the scope of experimentation, we selected two sets of
    efficiency techniques (Compression Techniques (Quantization), and Learning Techniques
    (Data Augmentation & Distillation). Hence, it would be useful to explore other
    techniques as well such as Automation (for Hyper-Parameter Tuning to further improve
    on results), and Efficient Layers & Models (Separable Convolution as illustrated
    in MobileNet (Sandler et al., [2018](#bib.bib134)) could be used in place of larger
    convolutional layers). Finally, we would also like to emphasize paying attention
    to performance of Deep Learning models (optimized or not) on underrepresented
    classes and out-of-distribution data to ensure model fairness, since quality metrics
    alone might not be sufficient for discovering deeper issues with models (Hooker
    et al., [2020](#bib.bib77)).'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，我们已经验证了上述两种策略可以同时有效，无论你的目标是优化质量指标还是足迹指标。我们还通过图 [26](#S4.F26 "Figure 26 ‣
    4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep
    Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    和 [27](#S4.F27 "Figure 27 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to
    Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better") 直观地检查了效率技术可以改进通过手动调节构建的帕累托前沿。为了限定实验范围，我们选择了两组效率技术（压缩技术（量化）和学习技术（数据增强与蒸馏））。因此，探索其他技术也很有用，比如自动化（用于超参数调优以进一步改善结果）和高效层与模型（如在
    MobileNet 中说明的可分离卷积（Sandler et al., [2018](#bib.bib134)）可以替代更大的卷积层）。最后，我们还希望强调关注深度学习模型（无论是否优化）在代表性不足的类别和分布外数据上的表现，以确保模型公平性，因为单靠质量指标可能不足以发现模型的深层问题（Hooker
    et al., [2020](#bib.bib77)）。'
- en: 5\. Conclusion
  id: totrans-515
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: In this paper, we started with demonstrating the rapid growth in Deep Learning
    models, and motivating the fact that someone training and deploying models today
    has to make either implicit or explicit decisions about efficiency. However, the
    landscape of model efficiency is vast.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们首先展示了深度学习模型的快速增长，并强调了如今训练和部署模型的人必须对效率做出隐式或显式的决策。然而，模型效率的领域非常广泛。
- en: To help with this, we laid out a mental model for the readers to wrap their
    heads around the multiple focus areas of model efficiency and optimization. The
    surveys of the core model optimization techniques give the reader an opportunity
    to understand the state-of-the-art, apply these techniques in the modelling process,
    and/or use them as a starting point for exploration. The infrastructure section
    also lays out the software libraries and hardware which make training and inference
    of efficient models possible.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助读者理解模型效率和优化的多重关注领域，我们提供了一个心理模型。核心模型优化技术的调研为读者提供了了解最新技术的机会，能够在建模过程中应用这些技术，或将其作为探索的起点。基础设施部分还介绍了使高效模型的训练和推理成为可能的软件库和硬件。
- en: Finally, we presented a section of explicit and actionable insights supplemented
    by code, for a practitioner to use as a guide in this space. This section will
    hopefully give concrete and actionable takeaways, as well as tradeoffs to think
    about when optimizing a model for training and deployment. To conclude, we feel
    that with this survey we have equipped the reader with the necessary understanding
    to break-down the steps required to go from a sub-optimal model to one that meets
    their constraints for both quality as well as footprint.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们提供了一个明确且可行的见解部分，并附有代码，供从业人员用作指南。这一部分将为问题解决者提供具体且可行的解决方案，同时也考虑到了在训练和部署模型时需要思考的权衡。总结一下，我们认为通过这个调研，我们已经为读者提供了必要的理解，可以将从次优模型到满足其质量和占据空间约束的模型的步骤分解开来。
- en: 6\. Acknowledgements
  id: totrans-519
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 致谢
- en: We would like to thank the Learn2Compress team at Google Research for their
    support with this work. We would also like to thank Akanksha Saran and Aditya
    Sarawgi for their help with proof-reading and suggestions for improving the content.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢 Google 研究团队的 Learn2Compress 团队对本文的支持。我们还要感谢 Akanksha Saran 和 Aditya Sarawgi
    在校对和改进内容方面的帮助。
- en: References
  id: totrans-521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Abadi et al. (2016) Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
    Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
    Isard, et al. 2016. Tensorflow: A system for large-scale machine learning. In
    *12th $\{$USENIX$\}$ symposium on operating systems design and implementation
    ($\{$OSDI$\}$ 16)*. 265–283.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abadi et al. (2016) Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy
    Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
    Isard, et al. 2016. Tensorflow：大规模机器学习的系统。 在《第 12 届 USENIX 操作系统设计与实现（OSDI 16）》中
    (pp. 265-283)。
- en: Agnihotri and Batra (2020) Apoorv Agnihotri and Nipun Batra. 2020. Exploring
    bayesian optimization. *Distill* 5, 5 (2020), e26.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agnihotri 和 Batra (2020) Apoorv Agnihotri 和 Nipun Batra. 2020. 探索贝叶斯优化。《蒸馏》（Distill），5（5），e26。
- en: Alammar (2021) Jay Alammar. 2021. The Illustrated Transformer. [https://jalammar.github.io/illustrated-transformer](https://jalammar.github.io/illustrated-transformer)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alammar (2021) Jay Alammar. 2021. **图解 Transformer**。[https://jalammar.github.io/illustrated-transformer](https://jalammar.github.io/illustrated-transformer)
    [在线; 2021年6月3日访问]。
- en: Android Developers (2021) Android Developers. 2021. Neural Networks API $|$
    Android NDK $|$ Android Developers. [https://developer.android.com/ndk/guides/neuralnetworks](https://developer.android.com/ndk/guides/neuralnetworks)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Android 开发者（2021）Android 开发者。2021. 神经网络 API $|$ Android NDK $|$ Android 开发者。[https://developer.android.com/ndk/guides/neuralnetworks](https://developer.android.com/ndk/guides/neuralnetworks)
    [在线; 2021年6月3日访问]。
- en: Anwar et al. (2017) Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. 2017. Structured
    pruning of deep convolutional neural networks. *ACM Journal on Emerging Technologies
    in Computing Systems (JETC)* 13, 3 (2017), 1–18.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anwar et al. (2017) Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. 2017. 深度卷积神经网络的结构剪枝。《ACM
    清华大学出版社计算机学报（JETC）》（第 13 卷，第 3 期，第 1-18 页）。
- en: Apple Authors (2021) Apple Authors. 2021. Accelerate $|$ Apple Developer Documentation.
    [https://developer.apple.com/documentation/accelerate](https://developer.apple.com/documentation/accelerate)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apple 作者（2021）Apple 作者。2021. 加速 $|$ Apple 开发者文档。[https://developer.apple.com/documentation/accelerate](https://developer.apple.com/documentation/accelerate)
    [在线; 2021年6月3日访问]。
- en: Authors (2021a) PyTorch Authors. 2021a. Automatic Mixed Precision examples —
    PyTorch 1.8.1 documentation. [https://pytorch.org/docs/stable/notes/amp_examples.html](https://pytorch.org/docs/stable/notes/amp_examples.html)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们 (2021a) PyTorch 作者们. 2021a. 自动混合精度示例 — PyTorch 1.8.1 文档. [https://pytorch.org/docs/stable/notes/amp_examples.html](https://pytorch.org/docs/stable/notes/amp_examples.html)
    [在线访问; 访问日期 2021年6月3日]。
- en: Authors (2021b) PyTorch Authors. 2021b. Performance Tuning Guide — PyTorch Tutorials
    1.8.1+cu102 documentation. [https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们 (2021b) PyTorch 作者们. 2021b. 性能调优指南 — PyTorch Tutorials 1.8.1+cu102 文档. [https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
    [在线访问; 访问日期 2021年6月3日]。
- en: Authors (2021c) PyTorch Authors. 2021c. PyTorch Mobile. [https://pytorch.org/mobile/home](https://pytorch.org/mobile/home)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们 (2021c) PyTorch 作者们. 2021c. PyTorch Mobile. [https://pytorch.org/mobile/home](https://pytorch.org/mobile/home)
    [在线访问; 访问日期 2021年6月3日]。
- en: Authors (2021d) PyTorch Authors. 2021d. Quantization Recipe — PyTorch Tutorials
    1.8.1+cu102 documentation. [https://pytorch.org/tutorials/recipes/quantization.html](https://pytorch.org/tutorials/recipes/quantization.html)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们 (2021d) PyTorch 作者们. 2021d. 量化配方 — PyTorch Tutorials 1.8.1+cu102 文档. [https://pytorch.org/tutorials/recipes/quantization.html](https://pytorch.org/tutorials/recipes/quantization.html)
    [在线访问; 访问日期 2021年6月3日]。
- en: Authors (2021e) PyTorch Authors. 2021e. torch.jit.script — PyTorch 1.8.1 documentation.
    [https://pytorch.org/docs/stable/generated/torch.jit.script.html](https://pytorch.org/docs/stable/generated/torch.jit.script.html)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们 (2021e) PyTorch 作者们. 2021e. torch.jit.script — PyTorch 1.8.1 文档. [https://pytorch.org/docs/stable/generated/torch.jit.script.html](https://pytorch.org/docs/stable/generated/torch.jit.script.html)
    [在线访问; 访问日期 2021年6月3日]。
- en: Authors (2020) Tensorflow Authors. 2020. TensorFlow Model Optimization. [https://www.tensorflow.org/model_optimization](https://www.tensorflow.org/model_optimization)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们 (2020) Tensorflow 作者们. 2020. TensorFlow 模型优化. [https://www.tensorflow.org/model_optimization](https://www.tensorflow.org/model_optimization)
    [在线访问; 访问日期 2021年6月3日]。
- en: Authors (2021f) Tensorflow Authors. 2021f. Post-training quantization $|$ TensorFlow
    Lite. [https://www.tensorflow.org/lite/performance/post_training_quantization](https://www.tensorflow.org/lite/performance/post_training_quantization)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们 (2021f) Tensorflow 作者们. 2021f. 训练后量化 $|$ TensorFlow Lite. [https://www.tensorflow.org/lite/performance/post_training_quantization](https://www.tensorflow.org/lite/performance/post_training_quantization)
    [在线访问; 访问日期 2021年6月3日]。
- en: Authors (2021g) Tensorflow Authors. 2021g. TensorFlow. [https://www.tensorflow.org](https://www.tensorflow.org)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们 (2021g) Tensorflow 作者们. 2021g. TensorFlow. [https://www.tensorflow.org](https://www.tensorflow.org)
    [在线访问; 访问日期 2021年6月3日]。
- en: Authors (2021h) Tensorflow Authors. 2021h. TensorFlow Lite converter. [https://www.tensorflow.org/lite/convert](https://www.tensorflow.org/lite/convert)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们 (2021h) Tensorflow 作者们. 2021h. TensorFlow Lite 转换器. [https://www.tensorflow.org/lite/convert](https://www.tensorflow.org/lite/convert)
    [在线访问; 访问日期 2021年6月3日]。
- en: Authors (2021i) Tensorflow Authors. 2021i. TensorFlow Lite $|$ ML for Mobile
    and Edge Devices. [https://www.tensorflow.org/lite](https://www.tensorflow.org/lite)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们 (2021i) Tensorflow 作者们. 2021i. TensorFlow Lite $|$ 移动端和边缘设备的 ML. [https://www.tensorflow.org/lite](https://www.tensorflow.org/lite)
    [在线访问; 访问日期 2021年6月3日]。
- en: 'Authors (2021j) Tensorflow Authors. 2021j. XLA: Optimizing Compiler for Machine
    Learning $|$ TensorFlow. [https://www.tensorflow.org/xla](https://www.tensorflow.org/xla)
    [Online; accessed 3\. Jun. 2021].'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '作者们 (2021j) Tensorflow 作者们. 2021j. XLA: 机器学习优化编译器 $|$ TensorFlow. [https://www.tensorflow.org/xla](https://www.tensorflow.org/xla)
    [在线访问; 访问日期 2021年6月3日]。'
- en: Authors (2021k) XNNPACK Authors. 2021k. XNNPACK. [https://github.com/google/XNNPACK](https://github.com/google/XNNPACK)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们 (2021k) XNNPACK 作者们. 2021k. XNNPACK. [https://github.com/google/XNNPACK](https://github.com/google/XNNPACK)
    [在线访问; 访问日期 2021年6月3日]。
- en: Authors (2021l) XNNPACK Authors. 2021l. XNNPACK backend for TensorFlow Lite.
    [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md/#sparse-inference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md/#sparse-inference)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者们 (2021l) XNNPACK 作者们. 2021l. XNNPACK 后端用于 TensorFlow Lite. [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md/#sparse-inference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md/#sparse-inference)
    [在线访问; 访问日期 2021年6月3日]。
- en: 'Baevski et al. (2020) Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and
    Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech
    representations. *arXiv preprint arXiv:2006.11477* (2020).'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Baevski 等（2020）**Alexei Baevski**、**Henry Zhou**、**Abdelrahman Mohamed** 和
    **Michael Auli**。2020。wav2vec 2.0: 自监督学习语音表示的框架。*arXiv 预印本 arXiv:2006.11477*（2020年）。'
- en: Bahdanau et al. (2014) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.
    Neural machine translation by jointly learning to align and translate. *arXiv
    preprint arXiv:1409.0473* (2014).
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 等（2014）**Dzmitry Bahdanau**、**Kyunghyun Cho** 和 **Yoshua Bengio**。2014。通过联合学习对齐和翻译进行神经机器翻译。*arXiv
    预印本 arXiv:1409.0473*（2014年）。
- en: Bengio et al. (2013) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013.
    Estimating or propagating gradients through stochastic neurons for conditional
    computation. *arXiv preprint arXiv:1308.3432* (2013).
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等（2013）**Yoshua Bengio**、**Nicholas Léonard** 和 **Aaron Courville**。2013。通过随机神经元估计或传播梯度以进行条件计算。*arXiv
    预印本 arXiv:1308.3432*（2013年）。
- en: Bergstra and Bengio (2012) James Bergstra and Yoshua Bengio. 2012. Random search
    for hyper-parameter optimization. *Journal of machine learning research* 13, 2
    (2012).
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bergstra 和 Bengio（2012）**James Bergstra** 和 **Yoshua Bengio**。2012。超参数优化的随机搜索。*机器学习研究杂志*
    13, 2（2012年）。
- en: Blum and Mitchell (1998) Avrim Blum and Tom Mitchell. 1998. Combining labeled
    and unlabeled data with co-training. In *Proceedings of the eleventh annual conference
    on Computational learning theory*. 92–100.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blum 和 Mitchell（1998）**Avrim Blum** 和 **Tom Mitchell**。1998。通过协同训练结合标记数据和未标记数据。在*第十一届计算学习理论年会论文集*。92–100。
- en: Bradbury et al. (2016) James Bradbury, Stephen Merity, Caiming Xiong, and Richard
    Socher. 2016. Quasi-recurrent neural networks. *arXiv preprint arXiv:1611.01576*
    (2016).
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bradbury 等（2016）**James Bradbury**、**Stephen Merity**、**Caiming Xiong** 和 **Richard
    Socher**。2016。准递归神经网络。*arXiv 预印本 arXiv:1611.01576*（2016年）。
- en: Brown et al. (2020) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020. Language models are few-shot learners. *arXiv preprint
    arXiv:2005.14165* (2020).
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）**Tom B Brown**、**Benjamin Mann**、**Nick Ryder**、**Melanie Subbiah**、**Jared
    Kaplan**、**Prafulla Dhariwal**、**Arvind Neelakantan**、**Pranav Shyam**、**Girish
    Sastry**、**Amanda Askell** 等。2020。语言模型是少样本学习者。*arXiv 预印本 arXiv:2005.14165*（2020年）。
- en: Buciluǎ et al. (2006) Cristian Buciluǎ, Rich Caruana, and Alexandru Niculescu-Mizil.
    2006. Model compression. In *Proceedings of the 12th ACM SIGKDD international
    conference on Knowledge discovery and data mining*. 535–541.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buciluǎ 等（2006）**Cristian Buciluǎ**、**Rich Caruana** 和 **Alexandru Niculescu-Mizil**。2006。模型压缩。在*第12届
    ACM SIGKDD 国际知识发现与数据挖掘会议论文集*。535–541。
- en: Charikar (2002) Moses S Charikar. 2002. Similarity estimation techniques from
    rounding algorithms. In *Proceedings of the thiry-fourth annual ACM symposium
    on Theory of computing*. 380–388.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Charikar（2002）**Moses S Charikar**。2002。从舍入算法中估计相似性。在*第三十四届 ACM 计算理论年会论文集*。380–388。
- en: Chaudhari et al. (2019) Sneha Chaudhari, Varun Mithal, Gungor Polatkan, and
    Rohan Ramanath. 2019. An attentive survey of attention models. *arXiv preprint
    arXiv:1904.02874* (2019).
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chaudhari 等（2019）**Sneha Chaudhari**、**Varun Mithal**、**Gungor Polatkan** 和
    **Rohan Ramanath**。2019。注意力模型的全面调查。*arXiv 预印本 arXiv:1904.02874*（2019年）。
- en: 'Chawla et al. (2002) Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and
    W Philip Kegelmeyer. 2002. SMOTE: synthetic minority over-sampling technique.
    *Journal of artificial intelligence research* 16 (2002), 321–357.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chawla 等（2002）**Nitesh V Chawla**、**Kevin W Bowyer**、**Lawrence O Hall** 和
    **W Philip Kegelmeyer**。2002。SMOTE: 合成少数过采样技术。*人工智能研究杂志* 16（2002年），321–357。'
- en: Chen (2021) Dihao Chen. 2021. advisor. [https://github.com/tobegit3hub/advisor](https://github.com/tobegit3hub/advisor)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen（2021）**Dihao Chen**。2021。advisor。 [https://github.com/tobegit3hub/advisor](https://github.com/tobegit3hub/advisor)
    [在线；访问于2021年6月3日]。
- en: Chen et al. (2020a) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey
    Hinton. 2020a. A simple framework for contrastive learning of visual representations.
    In *International conference on machine learning*. PMLR, 1597–1607.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2020a）**Ting Chen**、**Simon Kornblith**、**Mohammad Norouzi** 和 **Geoffrey
    Hinton**。2020a。视觉表示的对比学习简单框架。在*国际机器学习会议*。PMLR, 1597–1607。
- en: Chen et al. (2020b) Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi,
    and Geoffrey Hinton. 2020b. Big self-supervised models are strong semi-supervised
    learners. *arXiv preprint arXiv:2006.10029* (2020).
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2020b）**Ting Chen**、**Simon Kornblith**、**Kevin Swersky**、**Mohammad
    Norouzi** 和 **Geoffrey Hinton**。2020b。大型自监督模型是强大的半监督学习者。*arXiv 预印本 arXiv:2006.10029*（2020年）。
- en: 'Chollet (2017) François Chollet. 2017. Xception: Deep learning with depthwise
    separable convolutions. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*. 1251–1258.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chollet (2017) François Chollet。2017。Xception: 深度学习与深度可分离卷积。在 *IEEE 计算机视觉与模式识别会议论文集*。1251–1258页。'
- en: Chollet (2020) Francois Chollet. 2020. The Keras Blog. [https://blog.keras.io](https://blog.keras.io)
    [Online; accessed 4\. Jun. 2021].
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet (2020) François Chollet。2020。Keras 博客。 [https://blog.keras.io](https://blog.keras.io)
    [在线；访问日期：2021年6月4日]。
- en: Cireşan et al. (2011) Dan C Cireşan, Ueli Meier, Jonathan Masci, Luca M Gambardella,
    and Jürgen Schmidhuber. 2011. High-performance neural networks for visual object
    classification. *arXiv preprint arXiv:1102.0183* (2011).
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cireşan 等 (2011) Dan C Cireşan, Ueli Meier, Jonathan Masci, Luca M Gambardella,
    和 Jürgen Schmidhuber。2011。用于视觉对象分类的高性能神经网络。*arXiv 预印本 arXiv:1102.0183* (2011)。
- en: Contributors to Wikimedia projects (2021a) Contributors to Wikimedia projects.
    2021a. AVX-512 - Wikipedia. [https://en.wikipedia.org/w/index.php?title=AVX-512&oldid=1025044245](https://en.wikipedia.org/w/index.php?title=AVX-512&oldid=1025044245)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基媒体项目贡献者 (2021a) 维基媒体项目贡献者。2021a。AVX-512 - 维基百科。 [https://en.wikipedia.org/w/index.php?title=AVX-512&oldid=1025044245](https://en.wikipedia.org/w/index.php?title=AVX-512&oldid=1025044245)
    [在线；访问日期：2021年6月3日]。
- en: Contributors to Wikimedia projects (2021b) Contributors to Wikimedia projects.
    2021b. CUDA - Wikipedia. [https://en.wikipedia.org/w/index.php?title=CUDA&oldid=1025500257](https://en.wikipedia.org/w/index.php?title=CUDA&oldid=1025500257)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基媒体项目贡献者 (2021b) 维基媒体项目贡献者。2021b。CUDA - 维基百科。 [https://en.wikipedia.org/w/index.php?title=CUDA&oldid=1025500257](https://en.wikipedia.org/w/index.php?title=CUDA&oldid=1025500257)
    [在线；访问日期：2021年6月3日]。
- en: Contributors to Wikimedia projects (2021c) Contributors to Wikimedia projects.
    2021c. Hyperparameter optimization - Wikipedia. [https://en.wikipedia.org/w/index.php?title=Hyperparameter_optimization&oldid=1022309479](https://en.wikipedia.org/w/index.php?title=Hyperparameter_optimization&oldid=1022309479)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基媒体项目贡献者 (2021c) 维基媒体项目贡献者。2021c。超参数优化 - 维基百科。 [https://en.wikipedia.org/w/index.php?title=Hyperparameter_optimization&oldid=1022309479](https://en.wikipedia.org/w/index.php?title=Hyperparameter_optimization&oldid=1022309479)
    [在线；访问日期：2021年6月3日]。
- en: Contributors to Wikimedia projects (2021d) Contributors to Wikimedia projects.
    2021d. Multiply–accumulate operation - Wikipedia. [https://en.wikipedia.org/w/index.php?title=Multiply-accumulate_operation&oldid=1026461481](https://en.wikipedia.org/w/index.php?title=Multiply-accumulate_operation&oldid=1026461481)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基媒体项目贡献者 (2021d) 维基媒体项目贡献者。2021d。乘加操作 - 维基百科。 [https://en.wikipedia.org/w/index.php?title=Multiply-accumulate_operation&oldid=1026461481](https://en.wikipedia.org/w/index.php?title=Multiply-accumulate_operation&oldid=1026461481)
    [在线；访问日期：2021年6月3日]。
- en: Contributors to Wikimedia projects (2021e) Contributors to Wikimedia projects.
    2021e. SSE4 - Wikipedia. [https://en.wikipedia.org/w/index.php?title=SSE4&oldid=1023092035](https://en.wikipedia.org/w/index.php?title=SSE4&oldid=1023092035)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基媒体项目贡献者 (2021e) 维基媒体项目贡献者。2021e。SSE4 - 维基百科。 [https://en.wikipedia.org/w/index.php?title=SSE4&oldid=1023092035](https://en.wikipedia.org/w/index.php?title=SSE4&oldid=1023092035)
    [在线；访问日期：2021年6月3日]。
- en: Contributors to Wikimedia projects (2021f) Contributors to Wikimedia projects.
    2021f. WebGL - Wikipedia. [https://en.wikipedia.org/w/index.php?title=WebGL&oldid=1026775533](https://en.wikipedia.org/w/index.php?title=WebGL&oldid=1026775533)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基媒体项目贡献者 (2021f) 维基媒体项目贡献者。2021f。WebGL - 维基百科。 [https://en.wikipedia.org/w/index.php?title=WebGL&oldid=1026775533](https://en.wikipedia.org/w/index.php?title=WebGL&oldid=1026775533)
    [在线；访问日期：2021年6月3日]。
- en: 'Cubuk et al. (2019) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan,
    and Quoc V Le. 2019. Autoaugment: Learning augmentation strategies from data.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    113–123.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cubuk 等 (2019) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan,
    和 Quoc V Le。2019。Autoaugment: 从数据中学习增强策略。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*。113–123页。'
- en: 'Cubuk et al. (2020) Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
    Le. 2020. Randaugment: Practical automated data augmentation with a reduced search
    space. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Workshops*. 702–703.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cubuk 等 (2020) Ekin D Cubuk, Barret Zoph, Jonathon Shlens, 和 Quoc V Le。2020。Randaugment:
    具有简化搜索空间的实用自动数据增强。在 *IEEE/CVF 计算机视觉与模式识别会议研讨会论文集中*。702–703页。'
- en: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In *2009
    IEEE Conference on Computer Vision and Pattern Recognition*. 248–255. [https://doi.org/10.1109/CVPR.2009.5206848](https://doi.org/10.1109/CVPR.2009.5206848)'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. 2009. ImageNet: 一个大规模的层次图像数据库。发表于*2009 IEEE计算机视觉与模式识别会议*，248–255。[https://doi.org/10.1109/CVPR.2009.5206848](https://doi.org/10.1109/CVPR.2009.5206848)'
- en: 'Dettmers and Zettlemoyer (2019) Tim Dettmers and Luke Zettlemoyer. 2019. Sparse
    networks from scratch: Faster training without losing performance. *arXiv preprint
    arXiv:1907.04840* (2019).'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers and Zettlemoyer (2019) Tim Dettmers 和 Luke Zettlemoyer. 2019. 从零开始的稀疏网络：更快的训练而不损失性能。*arXiv预印本
    arXiv:1907.04840* (2019)。
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2018. Bert：用于语言理解的深度双向变换器的预训练。*arXiv预印本 arXiv:1810.04805* (2018)。
- en: Dietterich (2000) Thomas G Dietterich. 2000. Ensemble methods in machine learning.
    In *International workshop on multiple classifier systems*. Springer, 1–15.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dietterich (2000) Thomas G Dietterich. 2000. 机器学习中的集成方法。发表于*多分类器系统国际研讨会*，Springer，1–15。
- en: Doersch et al. (2015) Carl Doersch, Abhinav Gupta, and Alexei A Efros. 2015.
    Unsupervised visual representation learning by context prediction. In *Proceedings
    of the IEEE international conference on computer vision*. 1422–1430.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doersch et al. (2015) Carl Doersch, Abhinav Gupta, 和 Alexei A Efros. 2015. 通过上下文预测进行无监督视觉表示学习。发表于*IEEE国际计算机视觉会议论文集*，1422–1430。
- en: Dong et al. (2017) Xin Dong, Shangyu Chen, and Sinno Jialin Pan. 2017. Learning
    to prune deep neural networks via layer-wise optimal brain surgeon. *arXiv preprint
    arXiv:1705.07565* (2017).
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2017) Xin Dong, Shangyu Chen, 和 Sinno Jialin Pan. 2017. 通过逐层最优脑外科医生学习修剪深度神经网络。*arXiv预印本
    arXiv:1705.07565* (2017)。
- en: 'Dukhan et al. (2020) Marat Dukhan, Yiming Wu Wu, and Hao Lu. 2020. QNNPACK:
    Open source library for optimized mobile deep learning - Facebook Engineering.
    [https://engineering.fb.com/2018/10/29/ml-applications/qnnpack](https://engineering.fb.com/2018/10/29/ml-applications/qnnpack)
    [Online; accessed 3\. Jun. 2021].'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dukhan et al. (2020) Marat Dukhan, Yiming Wu Wu, 和 Hao Lu. 2020. QNNPACK：用于优化移动深度学习的开源库
    - Facebook工程。 [https://engineering.fb.com/2018/10/29/ml-applications/qnnpack](https://engineering.fb.com/2018/10/29/ml-applications/qnnpack)
    [在线；访问日期：2021年6月3日]。
- en: Elsen et al. (2020) Erich Elsen, Marat Dukhan, Trevor Gale, and Karen Simonyan.
    2020. Fast sparse convnets. In *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition*. 14629–14638.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elsen et al. (2020) Erich Elsen, Marat Dukhan, Trevor Gale, 和 Karen Simonyan.
    2020. 快速稀疏卷积网络。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，14629–14638。
- en: 'Elsken et al. (2019) Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, et al.
    2019. Neural architecture search: A survey. *J. Mach. Learn. Res.* 20, 55 (2019),
    1–21.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elsken et al. (2019) Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, 等. 2019.
    神经架构搜索：综述。*J. Mach. Learn. Res.* 20, 55 (2019)，1–21。
- en: 'Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro,
    and Erich Elsen. 2020. Rigging the lottery: Making all tickets winners. In *International
    Conference on Machine Learning*. PMLR, 2943–2952.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro,
    和 Erich Elsen. 2020. 操控彩票：让所有票据都成为赢家。发表于*国际机器学习大会*，PMLR，2943–2952。
- en: Fan et al. (2020) Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave,
    Rémi Gribonval, Hervé Jégou, and Armand Joulin. 2020. Training with quantization
    noise for extreme model compression. *arXiv e-prints* (2020), arXiv–2004.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan et al. (2020) Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave,
    Rémi Gribonval, Hervé Jégou, 和 Armand Joulin. 2020. 通过量化噪声训练以实现极端模型压缩。*arXiv e-prints*
    (2020)，arXiv–2004。
- en: Fawzi et al. (2016) Alhussein Fawzi, Horst Samulowitz, Deepak Turaga, and Pascal
    Frossard. 2016. Adaptive data augmentation for image classification. In *2016
    IEEE international conference on image processing (ICIP)*. Ieee, 3688–3692.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fawzi et al. (2016) Alhussein Fawzi, Horst Samulowitz, Deepak Turaga, 和 Pascal
    Frossard. 2016. 用于图像分类的自适应数据增强。发表于*2016 IEEE国际图像处理会议 (ICIP)*，IEEE，3688–3692。
- en: 'Frankle and Carbin (2018) Jonathan Frankle and Michael Carbin. 2018. The lottery
    ticket hypothesis: Training pruned neural networks. *arXiv preprint arXiv:1803.03635*
    2 (2018).'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle and Carbin (2018) Jonathan Frankle 和 Michael Carbin. 2018. 彩票假设：训练修剪的神经网络。*arXiv预印本
    arXiv:1803.03635* 2 (2018)。
- en: Gale et al. (2019) Trevor Gale, Erich Elsen, and Sara Hooker. 2019. The State
    of Sparsity in Deep Neural Networks. *CoRR* abs/1902.09574 (2019). arXiv:1902.09574
    [http://arxiv.org/abs/1902.09574](http://arxiv.org/abs/1902.09574)
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gale 等 (2019) Trevor Gale、Erich Elsen 和 Sara Hooker。2019。深度神经网络中的稀疏性现状。*CoRR*
    abs/1902.09574 (2019)。arXiv:1902.09574 [http://arxiv.org/abs/1902.09574](http://arxiv.org/abs/1902.09574)
- en: Gidaris et al. (2018) Spyros Gidaris, Praveer Singh, and Nikos Komodakis. 2018.
    Unsupervised representation learning by predicting image rotations. *arXiv preprint
    arXiv:1803.07728* (2018).
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gidaris 等 (2018) Spyros Gidaris、Praveer Singh 和 Nikos Komodakis。2018。通过预测图像旋转进行无监督表示学习。*arXiv预印本
    arXiv:1803.07728* (2018)。
- en: Glass (2012) James Glass. 2012. Towards unsupervised speech processing. In *2012
    11th International Conference on Information Science, Signal Processing and their
    Applications (ISSPA)*. IEEE, 1–4.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glass (2012) James Glass。2012。迈向无监督语音处理。在 *2012年第11届国际信息科学、信号处理及其应用会议 (ISSPA)*。IEEE，1–4。
- en: 'Golovin et al. (2017) Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg
    Kochanski, John Karro, and D Sculley. 2017. Google vizier: A service for black-box
    optimization. In *Proceedings of the 23rd ACM SIGKDD international conference
    on knowledge discovery and data mining*. 1487–1495.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Golovin 等 (2017) Daniel Golovin、Benjamin Solnik、Subhodeep Moitra、Greg Kochanski、John
    Karro 和 D Sculley。2017。Google Vizier：一个用于黑箱优化的服务。在 *第23届ACM SIGKDD国际知识发现与数据挖掘大会论文集*。1487–1495。
- en: Google (2021a) Google. 2021a. Cloud TPU $|$ Google Cloud. [https://cloud.google.com/tpu](https://cloud.google.com/tpu)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google (2021a) Google。2021a。Cloud TPU $|$ Google Cloud。 [https://cloud.google.com/tpu](https://cloud.google.com/tpu)
    [在线; 访问于 2021年6月3日]。
- en: Google (2021b) Google. 2021b. Coral. [https://coral.ai](https://coral.ai) [Online;
    accessed 4\. Jun. 2021].
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google (2021b) Google。2021b。Coral。 [https://coral.ai](https://coral.ai) [在线;
    访问于 2021年6月4日]。
- en: Google (2021c) Google. 2021c. Edge TPU performance benchmarks $|$ Coral. [https://coral.ai/docs/edgetpu/benchmarks](https://coral.ai/docs/edgetpu/benchmarks)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google (2021c) Google。2021c。Edge TPU 性能基准 $|$ Coral。 [https://coral.ai/docs/edgetpu/benchmarks](https://coral.ai/docs/edgetpu/benchmarks)
    [在线; 访问于 2021年6月3日]。
- en: Google (2021d) Google. 2021d. TensorFlow models on the Edge TPU $|$ Coral. [https://coral.ai/docs/edgetpu/models-intro/#supported-operations](https://coral.ai/docs/edgetpu/models-intro/#supported-operations)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google (2021d) Google。2021d。Edge TPU 上的 TensorFlow 模型 $|$ Coral。 [https://coral.ai/docs/edgetpu/models-intro/#supported-operations](https://coral.ai/docs/edgetpu/models-intro/#supported-operations)
    [在线; 访问于 2021年6月3日]。
- en: google research (2021) google research. 2021. google-research. [https://github.com/google-research/google-research/tree/master/fastconvnets](https://github.com/google-research/google-research/tree/master/fastconvnets)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: google research (2021) google research。2021。google-research。 [https://github.com/google-research/google-research/tree/master/fastconvnets](https://github.com/google-research/google-research/tree/master/fastconvnets)
    [在线; 访问于 2021年6月3日]。
- en: 'Gopalan et al. (2021) Arjun Gopalan, Da-Cheng Juan, Cesar Ilharco Magalhaes,
    Chun-Sung Ferng, Allan Heydon, Chun-Ta Lu, Philip Pham, George Yu, Yicheng Fan,
    and Yueqi Wang. 2021. Neural structured learning: training neural networks with
    structured signals. In *Proceedings of the 14th ACM International Conference on
    Web Search and Data Mining*. 1150–1153.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gopalan 等 (2021) Arjun Gopalan、Da-Cheng Juan、Cesar Ilharco Magalhaes、Chun-Sung
    Ferng、Allan Heydon、Chun-Ta Lu、Philip Pham、George Yu、Yicheng Fan 和 Yueqi Wang。2021。神经结构学习：用结构化信号训练神经网络。在
    *第14届ACM国际网络搜索与数据挖掘会议论文集*。1150–1153。
- en: 'Han et al. (2015a) Song Han, Huizi Mao, and William J Dally. 2015a. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. *arXiv preprint arXiv:1510.00149* (2015).'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2015a) Song Han、Huizi Mao 和 William J Dally。2015a。深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。*arXiv预印本
    arXiv:1510.00149* (2015)。
- en: Han et al. (2015b) Song Han, Jeff Pool, John Tran, and William J Dally. 2015b.
    Learning both weights and connections for efficient neural networks. *arXiv preprint
    arXiv:1506.02626* (2015).
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2015b) Song Han、Jeff Pool、John Tran 和 William J Dally。2015b。为高效神经网络学习权重和连接。*arXiv预印本
    arXiv:1506.02626* (2015)。
- en: 'Hannun et al. (2014) Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro,
    Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam
    Coates, et al. 2014. Deep speech: Scaling up end-to-end speech recognition. *arXiv
    preprint arXiv:1412.5567* (2014).'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hannun 等 (2014) Awni Hannun、Carl Case、Jared Casper、Bryan Catanzaro、Greg Diamos、Erich
    Elsen、Ryan Prenger、Sanjeev Satheesh、Shubho Sengupta、Adam Coates 等。2014。深度语音：扩展端到端语音识别。*arXiv预印本
    arXiv:1412.5567* (2014)。
- en: Hansen and Salamon (1990) Lars Kai Hansen and Peter Salamon. 1990. Neural network
    ensembles. *IEEE transactions on pattern analysis and machine intelligence* 12,
    10 (1990), 993–1001.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hansen 和 Salamon（1990）Lars Kai Hansen 和 Peter Salamon。1990。神经网络集成。*IEEE 模式分析与机器智能期刊*
    12, 10（1990），993–1001。
- en: Hassibi et al. (1993) Babak Hassibi, David G Stork, and Gregory J Wolff. 1993.
    Optimal brain surgeon and general network pruning. In *IEEE international conference
    on neural networks*. IEEE, 293–299.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi 等（1993）Babak Hassibi, David G Stork 和 Gregory J Wolff。1993。最优脑外科医生和通用网络剪枝。在
    *IEEE 国际神经网络大会*。IEEE，293–299。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2016）Kaiming He, Xiangyu Zhang, Shaoqing Ren 和 Jian Sun。2016。用于图像识别的深度残差学习。在
    *IEEE 计算机视觉与模式识别会议论文集*。770–778。
- en: 'He et al. (2018) Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and
    Song Han. 2018. Amc: Automl for model compression and acceleration on mobile devices.
    In *Proceedings of the European Conference on Computer Vision (ECCV)*. 784–800.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2018）Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li 和 Song Han。2018。Amc：用于移动设备的模型压缩和加速的
    Automl。在 *欧洲计算机视觉会议（ECCV）论文集*。784–800。
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531* (2015).
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等（2015）Geoffrey Hinton, Oriol Vinyals 和 Jeff Dean。2015。提炼神经网络中的知识。*arXiv
    预印本 arXiv:1503.02531*（2015）。
- en: Hooker et al. (2020) Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio,
    and Emily Denton. 2020. Characterising bias in compressed models. *arXiv preprint
    arXiv:2010.03058* (2020).
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hooker 等（2020）Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio 和 Emily
    Denton。2020。压缩模型中的偏差特征。*arXiv 预印本 arXiv:2010.03058*（2020）。
- en: Howard et al. (2019) Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen,
    Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan,
    et al. 2019. Searching for mobilenetv3\. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 1314–1324.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard 等（2019）Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen,
    Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan 等。2019。寻找
    mobilenetv3。在 *IEEE/CVF 国际计算机视觉会议论文集*。1314–1324。
- en: Howard and Ruder (2018) Jeremy Howard and Sebastian Ruder. 2018. Universal language
    model fine-tuning for text classification. *arXiv preprint arXiv:1801.06146* (2018).
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard 和 Ruder（2018）Jeremy Howard 和 Sebastian Ruder。2018。用于文本分类的通用语言模型微调。*arXiv
    预印本 arXiv:1801.06146*（2018）。
- en: 'Hsu et al. (2018) Chi-Hung Hsu, Shu-Huan Chang, Jhao-Hong Liang, Hsin-Ping
    Chou, Chun-Hao Liu, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, and Da-Cheng
    Juan. 2018. Monas: Multi-objective neural architecture search using reinforcement
    learning. *arXiv preprint arXiv:1806.10332* (2018).'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsu 等（2018）Chi-Hung Hsu, Shu-Huan Chang, Jhao-Hong Liang, Hsin-Ping Chou, Chun-Hao
    Liu, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei 和 Da-Cheng Juan。2018。Monas：使用强化学习的多目标神经架构搜索。*arXiv
    预印本 arXiv:1806.10332*（2018）。
- en: Hubara et al. (2016) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv,
    and Yoshua Bengio. 2016. Binarized neural networks. In *Proceedings of the 30th
    International Conference on Neural Information Processing Systems*. 4114–4122.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara 等（2016）Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv
    和 Yoshua Bengio。2016。二值化神经网络。在 *第30届国际神经信息处理系统大会论文集*。4114–4122。
- en: Inoue (2018) Hiroshi Inoue. 2018. Data augmentation by pairing samples for images
    classification. *arXiv preprint arXiv:1801.02929* (2018).
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inoue（2018）Hiroshi Inoue。2018。通过配对样本进行数据增强以进行图像分类。*arXiv 预印本 arXiv:1801.02929*（2018）。
- en: Jacob et al. (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.
    2704–2713.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob 等（2018）Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew
    Tang, Andrew Howard, Hartwig Adam 和 Dmitry Kalenichenko。2018。量化和训练神经网络以实现高效的整数算术推理。在
    *IEEE 计算机视觉与模式识别会议论文集*。2704–2713。
- en: Jaderberg et al. (2017) Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M
    Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen
    Simonyan, et al. 2017. Population based training of neural networks. *arXiv preprint
    arXiv:1711.09846* (2017).
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg 等（2017）Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech
    M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning,
    Karen Simonyan 等。2017。基于人群的神经网络训练。*arXiv 预印本 arXiv:1711.09846*（2017）。
- en: Jamieson and Talwalkar (2016) Kevin Jamieson and Ameet Talwalkar. 2016. Non-stochastic
    best arm identification and hyperparameter optimization. In *Artificial Intelligence
    and Statistics*. PMLR, 240–248.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jamieson 和 Talwalkar (2016) Kevin Jamieson 和 Ameet Talwalkar. 2016. 非随机最佳臂识别与超参数优化。见
    *人工智能与统计*。PMLR, 240–248。
- en: Jeremy Jordan (2020) Jeremy Jordan. 2020. Setting the learning rate of your
    neural network. *Jeremy Jordan* (Aug 2020). [https://www.jeremyjordan.me/nn-learning-rate](https://www.jeremyjordan.me/nn-learning-rate)
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeremy Jordan (2020) Jeremy Jordan. 2020. 设置神经网络的学习率。*Jeremy Jordan* (2020年8月)。
    [https://www.jeremyjordan.me/nn-learning-rate](https://www.jeremyjordan.me/nn-learning-rate)
- en: Jouppi et al. (2017) Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson,
    Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers,
    et al. 2017. In-datacenter performance analysis of a tensor processing unit. In
    *Proceedings of the 44th annual international symposium on computer architecture*.
    1–12.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jouppi 等 (2017) Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson,
    Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers
    等. 2017. 张量处理单元的机房内性能分析。见 *第 44 届国际计算机架构年会论文集*。1–12。
- en: 'Kaliamoorthi et al. (2019) Prabhu Kaliamoorthi, Sujith Ravi, and Zornitsa Kozareva.
    2019. PRADO: Projection Attention Networks for Document Classification On-Device.
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*. 5012–5021.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaliamoorthi 等 (2019) Prabhu Kaliamoorthi, Sujith Ravi 和 Zornitsa Kozareva.
    2019. PRADO：用于设备上的文档分类的投影注意力网络。见 *2019 年自然语言处理经验方法会议暨第九届国际自然语言处理联合会议论文集 (EMNLP-IJCNLP)*。5012–5021。
- en: Kaliamoorthi et al. (2021) Prabhu Kaliamoorthi, Aditya Siddhant, Edward Li,
    and Melvin Johnson. 2021. Distilling Large Language Models into Tiny and Effective
    Students using pQRNN. *arXiv preprint arXiv:2101.08890* (2021).
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaliamoorthi 等 (2021) Prabhu Kaliamoorthi, Aditya Siddhant, Edward Li 和 Melvin
    Johnson. 2021. 使用 pQRNN 将大型语言模型蒸馏为小型有效学生。*arXiv 预印本 arXiv:2101.08890* (2021)。
- en: Kanwar et al. (2021) Pankaj Kanwar, Peter Brandt, and Zongwei Zhou. 2021. TensorFlow
    2 MLPerf submissions demonstrate best-in-class performance on Google Cloud. [https://blog.tensorflow.org/2020/07/tensorflow-2-mlperf-submissions.html](https://blog.tensorflow.org/2020/07/tensorflow-2-mlperf-submissions.html)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanwar 等 (2021) Pankaj Kanwar, Peter Brandt 和 Zongwei Zhou. 2021. TensorFlow
    2 MLPerf 提交在 Google Cloud 上展示了行业领先的性能。 [https://blog.tensorflow.org/2020/07/tensorflow-2-mlperf-submissions.html](https://blog.tensorflow.org/2020/07/tensorflow-2-mlperf-submissions.html)
    [在线访问；2021年6月3日]。
- en: 'Krishnamoorthi (2018) Raghuraman Krishnamoorthi. 2018. Quantizing deep convolutional
    networks for efficient inference: A whitepaper. *arXiv* (Jun 2018). arXiv:1806.08342
    [https://arxiv.org/abs/1806.08342v1](https://arxiv.org/abs/1806.08342v1)'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishnamoorthi (2018) Raghuraman Krishnamoorthi. 2018. 为高效推理量化深度卷积网络：白皮书。*arXiv*
    (2018年6月)。arXiv:1806.08342 [https://arxiv.org/abs/1806.08342v1](https://arxiv.org/abs/1806.08342v1)
- en: Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning
    multiple layers of features from tiny images. (2009).
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等 (2009) Alex Krizhevsky, Geoffrey Hinton 等. 2009. 从小图像中学习多个特征层。
    (2009)。
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. Imagenet classification with deep convolutional neural networks. *Advances
    in neural information processing systems* 25 (2012), 1097–1105.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等 (2012) Alex Krizhevsky, Ilya Sutskever 和 Geoffrey E Hinton. 2012.
    使用深度卷积神经网络进行 Imagenet 分类。*神经信息处理系统进展* 25 (2012), 1097–1105。
- en: 'Krogh and Vedelsby (1994) Anders Krogh and Jesper Vedelsby. 1994. Neural network
    ensembles, cross validation and active learning. In *NIPS’94: Proceedings of the
    7th International Conference on Neural Information Processing Systems*. MIT Press,
    Cambridge, MA, USA, 231–238. [https://doi.org/10.5555/2998687.2998716](https://doi.org/10.5555/2998687.2998716)'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Krogh 和 Vedelsby (1994) Anders Krogh 和 Jesper Vedelsby. 1994. 神经网络集成、交叉验证和主动学习。见
    *NIPS’94: 第七届国际神经信息处理系统大会论文集*。MIT Press, Cambridge, MA, USA, 231–238。 [https://doi.org/10.5555/2998687.2998716](https://doi.org/10.5555/2998687.2998716)'
- en: Kung and Leiserson (1980) HT Kung and CE Leiserson. 1980. Introduction to VLSI
    systems. *Mead, C. A_, and Conway, L.,(Eds), Addison-Wesley, Reading, MA* (1980),
    271–292.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kung 和 Leiserson (1980) HT Kung 和 CE Leiserson. 1980. 《VLSI 系统导论》。*Mead, C.
    A_ 和 Conway, L., (编), Addison-Wesley, Reading, MA* (1980), 271–292。
- en: Kung (1982) Hsiang-Tsung Kung. 1982. Why systolic architectures? *IEEE computer*
    15, 1 (1982), 37–46.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kung (1982) Hsiang-Tsung Kung. 1982. 为什么选择流线型架构？ *IEEE 计算机* 15, 1 (1982), 37–46。
- en: 'LeCun (2018) Yann LeCun. 2018. Yann LeCun @EPFL - "Self-supervised learning:
    could machines learn like humans?". [https://www.youtube.com/watch?v=7I0Qt7GALVk&t=316s](https://www.youtube.com/watch?v=7I0Qt7GALVk&t=316s)
    [Online; accessed 3\. Jun. 2021].'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun（2018）Yann LeCun. 2018. Yann LeCun @EPFL - “自监督学习：机器能像人类一样学习吗？”。[https://www.youtube.com/watch?v=7I0Qt7GALVk&t=316s](https://www.youtube.com/watch?v=7I0Qt7GALVk&t=316s)
    [在线；访问时间 2021年6月3日]。
- en: Lecun et al. (1998) Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner.
    1998. Gradient-based learning applied to document recognition. *Proc. IEEE* 86,
    11 (Nov 1998), 2278–2324. [https://doi.org/10.1109/5.726791](https://doi.org/10.1109/5.726791)
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lecun 等（1998）Yann Lecun, Leon Bottou, Yoshua Bengio 和 Patrick Haffner. 1998.
    应用于文档识别的梯度基础学习。*IEEE 会议论文集* 86, 11（1998年11月），2278–2324。 [https://doi.org/10.1109/5.726791](https://doi.org/10.1109/5.726791)
- en: LeCun et al. (1990) Yann LeCun, John S Denker, and Sara A Solla. 1990. Optimal
    brain damage. In *Advances in neural information processing systems*. 598–605.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等（1990）Yann LeCun, John S Denker 和 Sara A Solla. 1990. 最优脑损伤。在 *神经信息处理系统进展*
    中。598–605。
- en: Li et al. (2016) Fengfu Li, Bo Zhang, and Bin Liu. 2016. Ternary weight networks.
    *arXiv preprint arXiv:1605.04711* (2016).
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li（2016）Fengfu Li, Bo Zhang 和 Bin Liu. 2016. 三元权重网络。*arXiv 预印本 arXiv:1605.04711*（2016）。
- en: Li et al. (2016) Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter
    Graf. 2016. Pruning Filters for Efficient ConvNets. In *ICLR (Poster)*.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2016）Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet 和 Hans Peter Graf.
    2016. 用于高效 ConvNets 的滤波器修剪。在 *ICLR（海报）* 中。
- en: 'Li et al. (2017) Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh,
    and Ameet Talwalkar. 2017. Hyperband: A novel bandit-based approach to hyperparameter
    optimization. *The Journal of Machine Learning Research* 18, 1 (2017), 6765–6816.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等（2017）Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh 和 Ameet
    Talwalkar. 2017. Hyperband: 一种基于赌博的超参数优化新方法。*机器学习研究杂志* 18, 1（2017），6765–6816。'
- en: 'Li (2020) Sharon Y. Li. 2020. Automating Data Augmentation: Practice, Theory
    and New Direction. *SAIL Blog* (Apr 2020). [http://ai.stanford.edu/blog/data-augmentation](http://ai.stanford.edu/blog/data-augmentation)'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li（2020）Sharon Y. Li. 2020. 自动化数据增强：实践、理论和新方向。*SAIL 博客*（2020年4月）。[http://ai.stanford.edu/blog/data-augmentation](http://ai.stanford.edu/blog/data-augmentation)
- en: 'Liaw et al. (2018) Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz,
    Joseph E Gonzalez, and Ion Stoica. 2018. Tune: A research platform for distributed
    model selection and training. *arXiv preprint arXiv:1807.05118* (2018).'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liaw 等（2018）Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph
    E Gonzalez 和 Ion Stoica. 2018. Tune: 一个用于分布式模型选择和训练的研究平台。*arXiv 预印本 arXiv:1807.05118*（2018）。'
- en: Liu et al. (2018c) Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens,
    Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.
    2018c. Progressive neural architecture search. In *Proceedings of the European
    conference on computer vision (ECCV)*. 19–34.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2018c）Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua,
    Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang 和 Kevin Murphy. 2018c. 逐步神经架构搜索。在
    *欧洲计算机视觉会议（ECCV）论文集* 中。19–34。
- en: 'Liu et al. (2018a) Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018a. Darts:
    Differentiable architecture search. *arXiv preprint arXiv:1806.09055* (2018).'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2018a）Hanxiao Liu, Karen Simonyan 和 Yiming Yang. 2018a. Darts: 可微分架构搜索。*arXiv
    预印本 arXiv:1806.09055*（2018）。'
- en: 'Liu et al. (2019) Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang,
    Kwang-Ting Cheng, and Jian Sun. 2019. Metapruning: Meta learning for automatic
    neural network channel pruning. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 3296–3305.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2019）Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting
    Cheng 和 Jian Sun. 2019. Metapruning: 自动神经网络通道修剪的元学习。在 *IEEE/CVF 国际计算机视觉大会论文集*
    中。3296–3305。'
- en: Liu et al. (2018b) Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor
    Darrell. 2018b. Rethinking the Value of Network Pruning. *CoRR* abs/1810.05270
    (2018). arXiv:1810.05270 [http://arxiv.org/abs/1810.05270](http://arxiv.org/abs/1810.05270)
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2018b）Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang 和 Trevor Darrell.
    2018b. 重新思考网络修剪的价值。*CoRR* abs/1810.05270（2018）。arXiv:1810.05270 [http://arxiv.org/abs/1810.05270](http://arxiv.org/abs/1810.05270)
- en: Ltd. (2021) Arm Ltd. 2021. SIMD ISAs $|$ Neon – Arm Developer. [https://developer.arm.com/architectures/instruction-sets/simd-isas/neon](https://developer.arm.com/architectures/instruction-sets/simd-isas/neon)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ltd.（2021）Arm Ltd. 2021. SIMD ISAs $|$ Neon – Arm 开发者。 [https://developer.arm.com/architectures/instruction-sets/simd-isas/neon](https://developer.arm.com/architectures/instruction-sets/simd-isas/neon)
    [在线；访问时间 2021年6月3日]。
- en: Menghani and Ravi (2019) Gaurav Menghani and Sujith Ravi. 2019. Learning from
    a Teacher using Unlabeled Data. *arXiv preprint arXiv:1911.05275* (2019).
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Menghani 和 Ravi (2019) Gaurav Menghani 和 Sujith Ravi. 2019. 使用未标记数据从教师那里学习。*arXiv
    预印本 arXiv:1911.05275* (2019)。
- en: Mikolov et al. (2017) Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian
    Puhrsch, and Armand Joulin. 2017. Advances in pre-training distributed word representations.
    *arXiv preprint arXiv:1712.09405* (2017).
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等 (2017) Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch
    和 Armand Joulin. 2017. 预训练分布式词表示的进展。*arXiv 预印本 arXiv:1712.09405* (2017)。
- en: Močkus (1975) Jonas Močkus. 1975. On Bayesian methods for seeking the extremum.
    In *Optimization techniques IFIP technical conference*. Springer, 400–404.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Močkus (1975) Jonas Močkus. 1975. 关于寻找极值的贝叶斯方法。见 *优化技术 IFIP 技术会议*。Springer,
    400–404。
- en: Molchanov et al. (2016) Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
    and Jan Kautz. 2016. Pruning Convolutional Neural Networks for Resource Efficient
    Transfer Learning. *CoRR* abs/1611.06440 (2016). arXiv:1611.06440 [http://arxiv.org/abs/1611.06440](http://arxiv.org/abs/1611.06440)
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molchanov 等 (2016) Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila 和
    Jan Kautz. 2016. 为资源高效的迁移学习修剪卷积神经网络。*CoRR* abs/1611.06440 (2016). arXiv:1611.06440
    [http://arxiv.org/abs/1611.06440](http://arxiv.org/abs/1611.06440)
- en: Node.js Authors (2021) Node.js Authors. 2021. Node.js. [https://nodejs.org/en](https://nodejs.org/en)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Node.js Authors (2021) Node.js Authors. 2021. Node.js. [https://nodejs.org/en](https://nodejs.org/en)
    [在线; 访问时间 2021年6月3日]。
- en: 'NVIDIA (2020a) NVIDIA. 2020a. GTC 2020: Accelerating Sparsity in the NVIDIA
    Ampere Architecture. [https://developer.nvidia.com/gtc/2020/video/s22085-vid](https://developer.nvidia.com/gtc/2020/video/s22085-vid)
    [Online; accessed 3\. Jun. 2021].'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'NVIDIA (2020a) NVIDIA. 2020a. GTC 2020: 加速 NVIDIA Ampere 架构中的稀疏性。 [https://developer.nvidia.com/gtc/2020/video/s22085-vid](https://developer.nvidia.com/gtc/2020/video/s22085-vid)
    [在线; 访问时间 2021年6月3日]。'
- en: 'NVIDIA (2020b) NVIDIA. 2020b. Inside Volta: The World’s Most Advanced Data
    Center GPU $|$ NVIDIA Developer Blog. [https://developer.nvidia.com/blog/inside-volta](https://developer.nvidia.com/blog/inside-volta)
    [Online; accessed 3\. Jun. 2021].'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'NVIDIA (2020b) NVIDIA. 2020b. Inside Volta: 世界上最先进的数据中心 GPU $|$ NVIDIA 开发者博客。
    [https://developer.nvidia.com/blog/inside-volta](https://developer.nvidia.com/blog/inside-volta)
    [在线; 访问时间 2021年6月3日]。'
- en: NVIDIA (2021) NVIDIA. 2021. NVIDIA Embedded Systems for Next-Gen Autonomous
    Machines. [https://www.nvidia.com/en-us/autonomous-machines/embedded-systems](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems)
    [Online; accessed 4\. Jun. 2021].
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA (2021) NVIDIA. 2021. NVIDIA 下一代自主机器的嵌入式系统。 [https://www.nvidia.com/en-us/autonomous-machines/embedded-systems](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems)
    [在线; 访问时间 2021年6月4日]。
- en: Panigrahy (2021) Rina Panigrahy. 2021. Matrix Compression Operator. [https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html](https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html)
    [Online; accessed 5\. Jun. 2021].
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Panigrahy (2021) Rina Panigrahy. 2021. 矩阵压缩操作符。 [https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html](https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html)
    [在线; 访问时间 2021年6月5日]。
- en: PapersWithCode.com (2021) PapersWithCode.com. 2021. Papers with Code - The latest
    in Machine Learning. [https://paperswithcode.com](https://paperswithcode.com)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PapersWithCode.com (2021) PapersWithCode.com. 2021. Papers with Code - 机器学习的最新进展。
    [https://paperswithcode.com](https://paperswithcode.com) [在线; 访问时间 2021年6月3日]。
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning
    library. *arXiv preprint arXiv:1912.01703* (2019).'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paszke 等 (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga 等. 2019. Pytorch: 一种命令式风格的高性能深度学习库。*arXiv 预印本 arXiv:1912.01703* (2019)。'
- en: Patrick et al. (2020) Mandela Patrick, Yuki M Asano, Polina Kuznetsova, Ruth
    Fong, João F Henriques, Geoffrey Zweig, and Andrea Vedaldi. 2020. Multi-modal
    self-supervision from generalized data transformations. *arXiv preprint arXiv:2003.04298*
    (2020).
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patrick 等 (2020) Mandela Patrick, Yuki M Asano, Polina Kuznetsova, Ruth Fong,
    João F Henriques, Geoffrey Zweig 和 Andrea Vedaldi. 2020. 从广义数据转换中进行多模态自监督。*arXiv
    预印本 arXiv:2003.04298* (2020)。
- en: 'Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher D
    Manning. 2014. Glove: Global vectors for word representation. In *Proceedings
    of the 2014 conference on empirical methods in natural language processing (EMNLP)*.
    1532–1543.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pennington 等 (2014) Jeffrey Pennington, Richard Socher 和 Christopher D Manning.
    2014. Glove: 用于词表示的全局向量。见 *2014年自然语言处理经验方法会议（EMNLP）论文集*。1532–1543。'
- en: 'Perrone et al. (2020) Valerio Perrone, Huibin Shen, Aida Zolic, Iaroslav Shcherbatyi,
    Amr Ahmed, Tanya Bansal, Michele Donini, Fela Winkelmolen, Rodolphe Jenatton,
    Jean Baptiste Faddoul, et al. 2020. Amazon SageMaker Automatic Model Tuning: Scalable
    Black-box Optimization. *arXiv preprint arXiv:2012.08489* (2020).'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perrone et al. (2020) Valerio Perrone, Huibin Shen, Aida Zolic, Iaroslav Shcherbatyi,
    Amr Ahmed, Tanya Bansal, Michele Donini, Fela Winkelmolen, Rodolphe Jenatton,
    Jean Baptiste Faddoul 等人。2020. Amazon SageMaker 自动模型调优：可扩展的黑箱优化。*arXiv 预印本 arXiv:2012.08489*
    (2020)。
- en: Pham et al. (2018) Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean.
    2018. Efficient neural architecture search via parameters sharing. In *International
    Conference on Machine Learning*. PMLR, 4095–4104.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pham et al. (2018) Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, 和 Jeff Dean。2018.
    通过参数共享的高效神经架构搜索。见于 *国际机器学习会议*。PMLR，4095–4104。
- en: Polino et al. (2018) Antonio Polino, Razvan Pascanu, and Dan Alistarh. 2018.
    Model compression via distillation and quantization. *arXiv preprint arXiv:1802.05668*
    (2018).
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polino et al. (2018) Antonio Polino, Razvan Pascanu, 和 Dan Alistarh。2018. 通过蒸馏和量化进行模型压缩。*arXiv
    预印本 arXiv:1802.05668* (2018)。
- en: Raina et al. (2009) Rajat Raina, Anand Madhavan, and Andrew Y Ng. 2009. Large-scale
    deep unsupervised learning using graphics processors. In *Proceedings of the 26th
    annual international conference on machine learning*. 873–880.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raina et al. (2009) Rajat Raina, Anand Madhavan, 和 Andrew Y Ng。2009. 使用图形处理器的大规模深度无监督学习。见于
    *第26届国际机器学习大会论文集*。873–880。
- en: Rakowski (2019) Brian Rakowski. 2019. Pixel 4 is here to help. *Google* (Oct
    2019). [https://blog.google/products/pixel/pixel-4](https://blog.google/products/pixel/pixel-4)
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rakowski (2019) Brian Rakowski。2019. Pixel 4 已经来帮助你了。*Google*（2019年10月）。[https://blog.google/products/pixel/pixel-4](https://blog.google/products/pixel/pixel-4)
- en: 'Rastegari et al. (2016) Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
    and Ali Farhadi. 2016. Xnor-net: Imagenet classification using binary convolutional
    neural networks. In *European conference on computer vision*. Springer, 525–542.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rastegari et al. (2016) Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
    和 Ali Farhadi. 2016. Xnor-net: 使用二值卷积神经网络进行 ImageNet 分类。见于 *欧洲计算机视觉会议*。Springer，525–542。'
- en: 'Ravi (2017) Sujith Ravi. 2017. Projectionnet: Learning efficient on-device
    deep networks using neural projections. *arXiv preprint arXiv:1708.00630* (2017).'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ravi (2017) Sujith Ravi。2017. Projectionnet: 使用神经投影学习高效的设备端深度网络。*arXiv 预印本
    arXiv:1708.00630* (2017)。'
- en: Ravi and Kozareva (2018) Sujith Ravi and Zornitsa Kozareva. 2018. Self-governing
    neural networks for on-device short text classification. In *Proceedings of the
    2018 Conference on Empirical Methods in Natural Language Processing*. 887–893.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravi and Kozareva (2018) Sujith Ravi 和 Zornitsa Kozareva。2018. 自主管理的神经网络用于设备端短文本分类。见于
    *2018年自然语言处理经验方法会议论文集*。887–893。
- en: Real et al. (2019) Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le.
    2019. Regularized evolution for image classifier architecture search. In *Proceedings
    of the aaai conference on artificial intelligence*, Vol. 33. 4780–4789.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Real et al. (2019) Esteban Real, Alok Aggarwal, Yanping Huang, 和 Quoc V Le。2019.
    图像分类器架构搜索的正则化进化。见于 *AAAI 人工智能会议论文集*，第33卷。4780–4789。
- en: Research (2019) Microsoft Research. 2019. Neural Network Intelligence - Microsoft
    Research. [https://www.microsoft.com/en-us/research/project/neural-network-intelligence](https://www.microsoft.com/en-us/research/project/neural-network-intelligence)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Research (2019) 微软研究院。2019. 神经网络智能 - 微软研究院。[https://www.microsoft.com/en-us/research/project/neural-network-intelligence](https://www.microsoft.com/en-us/research/project/neural-network-intelligence)
    [在线; 访问日期：2021年6月3日]。
- en: 'Rotem et al. (2018) Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret Catron,
    Summer Deng, Roman Dzhabarov, Nick Gibson, James Hegeman, Meghan Lele, Roman Levenstein,
    et al. 2018. Glow: Graph lowering compiler techniques for neural networks. *arXiv
    preprint arXiv:1805.00907* (2018).'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rotem et al. (2018) Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret Catron,
    Summer Deng, Roman Dzhabarov, Nick Gibson, James Hegeman, Meghan Lele, Roman Levenstein
    等人。2018. Glow: 神经网络的图形降级编译技术。*arXiv 预印本 arXiv:1805.00907* (2018)。'
- en: 'Sandler et al. (2018) Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
    and Liang-Chieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    4510–4520.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sandler et al. (2018) Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
    和 Liang-Chieh Chen。2018. Mobilenetv2: 反向残差和线性瓶颈。见于 *IEEE 计算机视觉与模式识别会议论文集*。4510–4520。'
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper
    and lighter. *arXiv preprint arXiv:1910.01108* (2019).'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等（2019）Victor Sanh, Lysandre Debut, Julien Chaumond 和 Thomas Wolf. 2019.
    DistilBERT，BERT 的精简版：更小、更快、更便宜、更轻便。*arXiv 预印本 arXiv:1910.01108* (2019)。
- en: Sankar et al. (2019) Chinnadhurai Sankar, Sujith Ravi, and Zornitsa Kozareva.
    2019. Transferable neural projection representations. *arXiv preprint arXiv:1906.01605*
    (2019).
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sankar 等（2019）Chinnadhurai Sankar, Sujith Ravi 和 Zornitsa Kozareva. 2019. 可迁移的神经投影表示。*arXiv
    预印本 arXiv:1906.01605* (2019)。
- en: 'Sankar et al. (2020) Chinnadhurai Sankar, Sujith Ravi, and Zornitsa Kozareva.
    2020. ProFormer: Towards On-Device LSH Projection Based Transformers. *arXiv preprint
    arXiv:2004.05801* (2020).'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sankar 等（2020）Chinnadhurai Sankar, Sujith Ravi 和 Zornitsa Kozareva. 2020. ProFormer：面向设备的
    LSH 投影基础变换器。*arXiv 预印本 arXiv:2004.05801* (2020)。
- en: Sato (2021) Kaz Sato. 2021. What makes TPUs fine-tuned for deep learning? $|$
    Google Cloud Blog. [https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning](https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sato（2021）Kaz Sato. 2021. 什么使 TPUs 为深度学习进行微调？ $|$ Google Cloud Blog. [https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning](https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning)
    [在线; 访问日期：2021年6月3日]。
- en: Schrittwieser et al. (2020) Julian Schrittwieser, Ioannis Antonoglou, Thomas
    Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart,
    Demis Hassabis, Thore Graepel, et al. 2020. Mastering atari, go, chess and shogi
    by planning with a learned model. *Nature* 588, 7839 (2020), 604–609.
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schrittwieser 等（2020）Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert,
    Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis
    Hassabis, Thore Graepel 等. 2020. 通过计划与学习模型掌握 Atari、围棋、国际象棋和将棋。*自然* 588, 7839 (2020),
    604–609。
- en: Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.
    Edinburgh neural machine translation systems for wmt 16. *arXiv preprint arXiv:1606.02891*
    (2016).
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sennrich 等（2016）Rico Sennrich, Barry Haddow 和 Alexandra Birch. 2016. 爱丁堡神经机器翻译系统，适用于
    wmt 16。*arXiv 预印本 arXiv:1606.02891* (2016)。
- en: Simard et al. (2003) Patrice Y Simard, David Steinkraus, John C Platt, et al.
    2003. Best practices for convolutional neural networks applied to visual document
    analysis.. In *Icdar*, Vol. 3\. Citeseer.
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simard 等（2003）Patrice Y Simard, David Steinkraus, John C Platt 等. 2003. 应用于视觉文档分析的卷积神经网络最佳实践。发表于*Icdar*，第3卷。Citeseer。
- en: Simonyan and Zisserman (2014) Karen Simonyan and Andrew Zisserman. 2014. Very
    deep convolutional networks for large-scale image recognition. *arXiv preprint
    arXiv:1409.1556* (2014).
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman（2014）Karen Simonyan 和 Andrew Zisserman. 2014. 用于大规模图像识别的非常深的卷积网络。*arXiv
    预印本 arXiv:1409.1556* (2014)。
- en: Stosic (2020) Dusan Stosic. 2020. Training Neural Networks with Tensor Cores
    - Dusan Stosic, NVIDIA. [https://www.youtube.com/watch?v=jF4-_ZK_tyc](https://www.youtube.com/watch?v=jF4-_ZK_tyc)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stosic（2020）Dusan Stosic. 2020. 使用 Tensor 核心训练神经网络 - Dusan Stosic，NVIDIA。 [https://www.youtube.com/watch?v=jF4-_ZK_tyc](https://www.youtube.com/watch?v=jF4-_ZK_tyc)
    [在线; 访问日期：2021年6月3日]。
- en: Sun et al. (2017) Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav
    Gupta. 2017. Revisiting unreasonable effectiveness of data in deep learning era.
    In *Proceedings of the IEEE international conference on computer vision*. 843–852.
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2017）Chen Sun, Abhinav Shrivastava, Saurabh Singh 和 Abhinav Gupta. 2017.
    重新审视深度学习时代数据的非凡有效性。发表于*IEEE国际计算机视觉会议论文集*。843–852。
- en: 'Sun et al. (2020) Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming
    Yang, and Denny Zhou. 2020. Mobilebert: a compact task-agnostic bert for resource-limited
    devices. *arXiv preprint arXiv:2004.02984* (2020).'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2020）Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang 和
    Denny Zhou. 2020. Mobilebert：一种用于资源有限设备的紧凑型任务无关 BERT。*arXiv 预印本 arXiv:2004.02984*
    (2020)。
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
    Sequence to sequence learning with neural networks. *arXiv preprint arXiv:1409.3215*
    (2014).
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever 等（2014）Ilya Sutskever, Oriol Vinyals 和 Quoc V Le. 2014. 使用神经网络的序列到序列学习。*arXiv
    预印本 arXiv:1409.3215* (2014)。
- en: Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
    2015. Going deeper with convolutions. In *Proceedings of the IEEE conference on
    computer vision and pattern recognition*. 1–9.
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等（2015）Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott
    Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke 和 Andrew Rabinovich.
    2015. 通过卷积深入研究。发表于*IEEE计算机视觉与模式识别会议论文集*。1–9。
- en: 'Tan et al. (2019) Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark
    Sandler, Andrew Howard, and Quoc V Le. 2019. Mnasnet: Platform-aware neural architecture
    search for mobile. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*. 2820–2828.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tay et al. (2020) Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
    2020. Efficient transformers: A survey. *arXiv preprint arXiv:2009.06732* (2020).'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow (2019) TensorFlow. 2019. TensorFlow Model Optimization Toolkit —
    Post-Training Integer Quantization. *Medium* (Nov 2019). [https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba)
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow (2021) TensorFlow. 2021. Model optimization $|$ TensorFlow Lite.
    [https://www.tensorflow.org/lite/performance/model_optimization](https://www.tensorflow.org/lite/performance/model_optimization)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tsang (2019) Sik-Ho Tsang. 2019. Review: Xception — With Depthwise Separable
    Convolution, Better Than Inception-v3 (Image Classification). *Medium* (Mar 2019).
    [https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568](https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568)'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Urban et al. (2016) Gregor Urban, Krzysztof J Geras, Samira Ebrahimi Kahou,
    Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose,
    and Matt Richardson. 2016. Do deep convolutional nets really need to be deep and
    convolutional? *arXiv preprint arXiv:1603.05691* (2016).
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vanhoucke et al. (2011) Vincent Vanhoucke, Andrew Senior, and Mark Z Mao. 2011.
    Improving the speed of neural networks on CPUs. (2011).
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vasilache et al. (2018) Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis,
    Priya Goyal, Zachary DeVito, William S Moses, Sven Verdoolaege, Andrew Adams,
    and Albert Cohen. 2018. Tensor comprehensions: Framework-agnostic high-performance
    machine learning abstractions. *arXiv preprint arXiv:1802.04730* (2018).'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *arXiv preprint arXiv:1706.03762* (2017).
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. 2020.
    Towards Accurate Post-training Network Quantization via Bit-Split and Stitching.
    In *International Conference on Machine Learning*. PMLR, 9847–9856. [http://proceedings.mlr.press/v119/wang20c.html](http://proceedings.mlr.press/v119/wang20c.html)
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Kanwar (2021) Shibo Wang and Pankaj Kanwar. 2021. BFloat16: The secret
    to high performance on Cloud TPUs $|$ Google Cloud Blog. [https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)
    [Online; accessed 3\. Jun. 2021].'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018.
    Switchout: an efficient data augmentation algorithm for neural machine translation.
    *arXiv preprint arXiv:1808.07512* (2018).'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Warden and Situnayake (2019) Pete Warden and Daniel Situnayake. 2019. *Tinyml:
    Machine learning with tensorflow lite on arduino and ultra-low-power microcontrollers*.
    " O’Reilly Media, Inc.".'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019) Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei
    Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. 2019.
    Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture
    search. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*. 10734–10742.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2020) Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. 2020.
    Self-training with noisy student improves imagenet classification. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 10687–10698.
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yalniz et al. (2019) I Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and
    Dhruv Mahajan. 2019. Billion-scale semi-supervised learning for image classification.
    *arXiv preprint arXiv:1905.00546* (2019).
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018) Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai
    Chen, Mohammad Norouzi, and Quoc V Le. 2018. Qanet: Combining local convolution
    with global self-attention for reading comprehension. *arXiv preprint arXiv:1804.09541*
    (2018).'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu and Zhu (2020) Tong Yu and Hong Zhu. 2020. Hyper-parameter optimization:
    A review of algorithms and applications. *arXiv preprint arXiv:2003.05689* (2020).'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zagoruyko and Komodakis (2016) Sergey Zagoruyko and Nikos Komodakis. 2016.
    Paying more attention to attention: Improving the performance of convolutional
    neural networks via attention transfer. *arXiv preprint arXiv:1612.03928* (2016).'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017) Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David
    Lopez-Paz. 2017. mixup: Beyond empirical risk minimization. *arXiv preprint arXiv:1710.09412*
    (2017).'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu and Gupta (2018) Michael Zhu and Suyog Gupta. 2018. To Prune, or Not to
    Prune: Exploring the Efficacy of Pruning for Model Compression. In *6th International
    Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
    30 - May 3, 2018, Workshop Track Proceedings*. OpenReview.net. [https://openreview.net/forum?id=Sy1iIDkPM](https://openreview.net/forum?id=Sy1iIDkPM)'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoph and Le (2016) Barret Zoph and Quoc V Le. 2016. Neural architecture search
    with reinforcement learning. *arXiv preprint arXiv:1611.01578* (2016).
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoph et al. (2018) Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V
    Le. 2018. Learning transferable architectures for scalable image recognition.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    8697–8710.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zoph 等人（2018）巴雷特·佐夫、维贾伊·瓦苏德万、乔纳森·施伦斯和阮国维。2018年。《用于可扩展图像识别的可迁移架构学习》。发表于*IEEE计算机视觉与模式识别会议论文集*。8697–8710。
