- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:54:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[2106.08962] Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2106.08962](https://ar5iv.labs.arxiv.org/html/2106.08962)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster,
    and Better'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gaurav Menghani [gmenghani@google.com](mailto:gmenghani@google.com) [0000-0003-2912-2522](https://orcid.org/0000-0003-2912-2522
    "ORCID identifier") Google ResearchMountain ViewCaliforniaUSA95054
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep Learning has revolutionized the fields of computer vision, natural language
    understanding, speech recognition, information retrieval and more. However, with
    the progressive improvements in deep learning models, their number of parameters,
    latency, resources required to train, etc. have all have increased significantly.
    Consequently, it has become important to pay attention to these footprint metrics
    of a model as well, not just its quality. We present and motivate the problem
    of efficiency in deep learning, followed by a thorough survey of the five core
    areas of model efficiency (spanning modeling techniques, infrastructure, and hardware)
    and the seminal work there. We also present an experiment-based guide along with
    code, for practitioners to optimize their model training and deployment. We believe
    this is the first comprehensive survey in the efficient deep learning space that
    covers the landscape of model efficiency from modeling techniques to hardware
    support. Our hope is that this survey would provide the reader with the mental
    model and the necessary understanding of the field to apply generic efficiency
    techniques to immediately get significant improvements, and also equip them with
    ideas for further research and experimentation to achieve additional gains.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep Learning with neural networks has been the dominant methodology of training
    new machine learning models for the past decade. Its rise to prominence is often
    attributed to the ImageNet competition (Deng et al., [2009](#bib.bib46)) in 2012\.
    That year, a University of Toronto team submitted a deep convolutional network
    (AlexNet (Krizhevsky et al., [2012](#bib.bib93)), named after the lead developer
    Alex Krizhevsky), performed 41% better than the next best submission. As a result
    of this trailblazing work, there was a race to create deeper networks with an
    ever increasing number of parameters and complexity. Several model architectures
    such as VGGNet (Simonyan and Zisserman, [2014](#bib.bib142)), Inception (Szegedy
    et al., [2015](#bib.bib147)), ResNet (He et al., [2016](#bib.bib74)) etc. successively
    beat previous records at ImageNet competitions in the subsequent years, while
    also increasing in their footprint (model size, latency, etc.)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5666049517ceb7f464c21a388a64e46b.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: (a) Computer Vision Models
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb1cc800839b0f6963270ad17905fed0.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: (b) Natural Language Models
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. Growth in the number of parameters in Computer Vision models over
    time. (PapersWithCode.com, [2021](#bib.bib119))
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'This effect has also been noted in natural language understanding (NLU), where
    the Transformer (Vaswani et al., [2017](#bib.bib156)) architecture based on primarily
    Attention layers, spurred the development of general purpose language encoders
    like BERT (Devlin et al., [2018](#bib.bib48)), GPT-3 (Brown et al., [2020](#bib.bib27)),
    etc. BERT specifically beat 11 NLU benchmarks when it was published. GPT-3 has
    also been used in several places in the industry via its API. The common aspect
    amongst these domains is the rapid growth in the model footprint (Refer to Figure
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Efficient Deep Learning: A Survey on
    Making Deep Learning Models Smaller, Faster, and Better")), and the cost associated
    with training and deploying them.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Since deep learning research has been focused on improving the state of the
    art, progressive improvements on benchmarks like image classification, text classification,
    etc. have been correlated with an increase in the network complexity, number of
    parameters, the amount of training resources required to train the network, prediction
    latency, etc. For instance, GPT-3 comprises of 175 billion parameters, and costs
    millions of dollars to train just one iteration ((Brown et al., [2020](#bib.bib27))).
    This excludes the cost of experimentation / trying combinations of different hyper-parameters,
    which is also computationally expensive.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: While these models perform well on the tasks they are trained on, they might
    not necessarily be efficient enough for direct deployment in the real world. A
    deep learning practitioner might face the following challenges when training or
    deploying a model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sustainable Server-Side Scaling: Training and deploying large deep learning
    models is costly. While training could be a one-time cost (or could be free if
    one is using a pre-trained model), deploying and letting inference run for over
    a long period of time could still turn out to be expensive in terms of consumption
    of server-side RAM, CPU, etc.. There is also a very real concern around the carbon
    footprint of datacenters even for organizations like Google, Facebook, Amazon,
    etc. which spend several billion dollars each per year in capital expenditure
    on their data-centers.'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Enabling On-Device Deployment: Certain deep learning applications need to run
    realtime on IoT and smart devices (where the model inference happens directly
    on the device), for a multitude of reasons (privacy, connectivity, responsiveness).
    Thus, it becomes imperative to optimize the models for the target devices.'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Privacy & Data Sensitivity: Being able to use as little data as possible for
    training is critical when the user-data might be sensitive. Hence, efficiently
    training models with a fraction of the data means lesser data-collection required.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'New Applications: Certain new applications offer new constraints (around model
    quality or footprint) that existing off-the-shelf models might not be able to
    address.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新应用：某些新应用提出了新的约束（关于模型质量或足迹），现有的现成模型可能无法解决这些问题。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Explosion of Models: While a singular model might work well, training and/or
    deploying multiple models on the same infrastructure (colocation) for different
    applications might end up exhausting the available resources.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型爆炸：虽然单一模型可能表现良好，但在同一基础设施上（共置）训练和/或部署多个模型以满足不同应用的需求，可能会耗尽可用资源。
- en: 1.1\. Efficient Deep Learning
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 高效的深度学习
- en: 'The common theme around the above challenges is *efficiency*. We can break
    it down further as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 上述挑战的共同主题是*效率*。我们可以进一步将其分解如下：
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Inference Efficiency: This primarily deals with questions that someone deploying
    a model for inference (computing the model outputs for a given input), would ask.
    Is the model small? Is it fast, etc.? More concretely, how many parameters does
    the model have, what is the disk size, RAM consumption during inference, inference
    latency, etc.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理效率：这主要涉及到在部署模型时（计算模型对于给定输入的输出）可能会问的问题。模型是否较小？是否快速等？更具体地说，模型有多少个参数，磁盘大小是多少，推理期间的RAM消耗，推理延迟等。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Training Efficiency: This involves questions someone training a model would
    ask, such as How long does the model take to train? How many devices? Can the
    model fit in memory?, etc. It might also include questions like, how much data
    would the model need to achieve the desired performance on the given task?'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练效率：这涉及到训练模型时可能会问的问题，比如模型训练需要多长时间？需要多少设备？模型是否可以适应内存？等。它也可能包括类似的问题，比如模型需要多少数据才能在给定任务上达到所需的性能？
- en: If we were to be given two models, performing equally well on a given task,
    we might want to choose a model which does better in either one, or ideally both
    of the above aspects. If one were to be deploying a model on devices where inference
    is constrained (such as mobile and embedded devices), or expensive (cloud servers),
    it might be worth paying attention to inference efficiency. Similarly, if one
    is training a large model from scratch on either with limited or costly training
    resources, developing models that are designed for training efficiency would help.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有两个模型，在特定任务上表现同样出色，我们可能会选择在上述一个或两个方面表现更好的模型。如果在推理受限（如移动设备和嵌入式设备）或成本高昂（云服务器）的设备上部署模型，可能值得关注推理效率。类似地，如果在有限或昂贵的训练资源下从头开始训练一个大型模型，开发旨在提高训练效率的模型将有所帮助。
- en: '![Refer to caption](img/c8f0b15164247b11b0dd00dbb5d1c784.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c8f0b15164247b11b0dd00dbb5d1c784.png)'
- en: 'Figure 2\. Pareto Optimality: Green dots represent pareto-optimal models (together
    forming the pareto-frontier), where none of the other models (red dots) get better
    accuracy with the same inference latency, or the other way around.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 帕累托最优性：绿色点代表帕累托最优模型（共同形成帕累托前沿），在这些模型中，没有其他模型（红色点）能够在相同的推理延迟下获得更好的准确率，反之亦然。
- en: 'Regardless of what one might be optimizing for, we want to achieve *pareto-optimality*.
    This implies that any model that we choose is the best for the tradeoffs that
    we care about. As an example in Figure [2](#S1.F2 "Figure 2 ‣ 1.1\. Efficient
    Deep Learning ‣ 1\. Introduction ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better"), the green dots represent pareto-optimal
    models, where none of the other models (red dots) get better accuracy with the
    same inference latency, or the other way around. Together, the pareto-optimal
    models (green dots) form our *pareto-frontier*. The models in the pareto-frontier
    are by definition more efficient than the other models, since they perform the
    best for their given tradeoff. Hence, when we seek efficiency, we should be thinking
    about discovering and improving on the pareto-frontier.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '无论我们优化什么，我们都希望实现*帕累托最优性*。这意味着我们选择的任何模型都是在我们关心的权衡下表现最好的。例如，在图[2](#S1.F2 "Figure
    2 ‣ 1.1\. Efficient Deep Learning ‣ 1\. Introduction ‣ Efficient Deep Learning:
    A Survey on Making Deep Learning Models Smaller, Faster, and Better")中，绿色点代表帕累托最优模型，这些模型中没有其他模型（红色点）能在相同的推理延迟下获得更好的准确率，反之亦然。帕累托最优模型（绿色点）共同形成了我们的*帕累托前沿*。定义上，帕累托前沿中的模型比其他模型更高效，因为它们在给定的权衡下表现最佳。因此，当我们寻求效率时，我们应当考虑发现并改进帕累托前沿上的模型。'
- en: To achieve this goal, we propose turning towards a collection of algorithms,
    techniques, tools, and infrastructure that work together to allow users to train
    and deploy *pareto-optimal* models with respect to model quality and its footprint.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 2\. A Mental Model
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we present the mental model to think about the collection of
    algorithms, techniques, and tools related to efficient deep learning. We propose
    to structure them in five major areas, with the first four focused on modeling,
    and the final one around infrastructure and tools.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fabdaeea4f34ffafb788f2dc3d56a587.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. A mental model for thinking about algorithms, techniques, and tools
    related to efficiency in Deep Learning.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compression Techniques: These are general techniques and algorithms that look
    at optimizing the model’s architecture, typically by compressing its layers. A
    classical example is quantization (Jacob et al., [2018](#bib.bib83)), which tries
    to compress the weight matrices of a layer, by reducing its precision (eg., from
    32-bit floating point values to 8-bit unsigned integers), with minimal loss in
    quality.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Learning Techniques: These are algorithms which focus on training the model
    differently (to make fewer prediction errors, require less data, converge faster,
    etc.). The improved quality can then be exchanged for a smaller footprint / a
    more efficient model by trimming the number of parameters if needed. An example
    of a learning technique is distillation (Hinton et al., [2015](#bib.bib76)), which
    allows improving the accuracy of a smaller model by learning to mimic a larger
    model.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Automation: These are tools for improving the core metrics of the given model
    using automation. An example is hyper-parameter optimization (HPO) (Golovin et al.,
    [2017](#bib.bib62)) where optimizing the hyper-parameters helps increase the accuracy,
    which could then be then exchanged for a model with lesser parameters. Similarly,
    architecture search (Zoph and Le, [2016](#bib.bib169)) falls in this category
    too, where the architecture itself is tuned and the search helps find a model
    that optimizes both the loss / accuracy, and some other metric such as model latency,
    model size, etc.'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Efficient Architectures: These are fundamental blocks that were designed from
    scratch (convolutional layers, attention, etc.), that are a significant leap over
    the baseline methods used before them (fully connected layers, and RNNs respectively).
    As an example, convolutional layers introduced parameter sharing for use in image
    classification, which avoids having to learn separate weights for each input pixel,
    and also makes them robust to overfitting. Similarly, attention layers (Bahdanau
    et al., [2014](#bib.bib22)) solved the problem of Information Bottleneck in Seq2Seq
    models. These architectures can be used directly for efficiency gains.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (5)
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Infrastructure: Finally, we also need a foundation of infrastructure and tools
    that help us build and leverage efficient models. This includes the model training
    framework, such as Tensorflow (Abadi et al., [2016](#bib.bib2)), PyTorch (Paszke
    et al., [2019](#bib.bib120)), etc. (along with the tools required specifically
    for deploying efficient models such as Tensorflow Lite (TFLite), PyTorch Mobile,
    etc.). We depend on the infrastructure and tooling to leverage gains from efficient
    models. For example, to get both size and latency improvements with quantized
    models, we need the inference platform to support common neural network layers
    in quantized mode.'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基础设施：最后，我们还需要一个基础设施和工具的基础，以帮助我们构建和利用高效的模型。这包括模型训练框架，如 Tensorflow (Abadi et al.,
    [2016](#bib.bib2))、PyTorch (Paszke et al., [2019](#bib.bib120)) 等（以及专门用于部署高效模型的工具，如
    Tensorflow Lite (TFLite)、PyTorch Mobile 等）。我们依赖基础设施和工具来利用高效模型的优势。例如，要在量化模型中获得大小和延迟的改进，我们需要推理平台支持量化模式下的常见神经网络层。
- en: We will survey each of these areas in depth in the following section.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的部分中深入探讨这些领域。
- en: 3\. Landscape of Efficient Deep Learning
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 高效深度学习的全景
- en: 3.1\. Compression Techniques
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 压缩技术
- en: Compression techniques as mentioned earlier, are usually generic techniques
    for achieving a more efficient representation of one or more layers in a neural
    network, with a possible quality trade off. The efficiency goal could be to optimize
    the model for one or more of the footprint metrics, such as model size, inference
    latency, training time required for convergence, etc. in exchange for as little
    quality loss as possible. In some cases if the model is over-parameterized, these
    techniques can improve model generalization.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，压缩技术通常是实现神经网络中一个或多个层更高效表示的通用技术，可能会有质量的权衡。效率目标可能是优化模型以满足一个或多个占用指标，例如模型大小、推理延迟、收敛所需的训练时间等，以换取尽可能少的质量损失。在某些情况下，如果模型过度参数化，这些技术可以提高模型的泛化能力。
- en: 3.1.1\. Pruning
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 剪枝
- en: Given a neural network $f(X,W)$, where $X$ is the input and $W$ is the set of
    parameters (or weights), pruning is a technique for coming up with a minimal subset
    $W^{\prime}$ such that the rest of the parameters of $W$ are pruned (or set to
    0), while ensuring that the quality of the model remains above the desired threshold.
    After pruning, we can say the network has been made *sparse*, where the sparsity
    can be quantified as the ratio of the number of parameters that were pruned to
    the number of parameters in the original network ($s=(1-\frac{|W^{\prime}|}{|W|})$).
    The higher the sparsity, the lesser the number of non-zero parameters in the pruned
    networks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个神经网络 $f(X,W)$，其中 $X$ 是输入，$W$ 是参数（或权重）集合，剪枝是一种技术，用于得到一个最小的子集 $W^{\prime}$，使得其余的参数
    $W$ 被剪枝（或设置为0），同时确保模型的质量保持在期望的阈值以上。剪枝后，我们可以说网络变得 *稀疏*，稀疏度可以量化为剪枝参数的数量与原网络中参数数量的比例
    ($s=(1-\frac{|W^{\prime}|}{|W|})$)。稀疏度越高，剪枝网络中的非零参数越少。
- en: '![Refer to caption](img/5a9065a48b4901e83bf765de85dcb91e.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5a9065a48b4901e83bf765de85dcb91e.png)'
- en: Figure 4\. A simplified illustration of pruning weights (connections) and neurons
    (nodes) in a neural network comprising of fully connected layers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 简化示意图，展示了在包含全连接层的神经网络中剪枝权重（连接）和神经元（节点）的过程。
- en: 'Some of the classical works in this area are Optimal Brain Damage (OBD) by
    LeCun et al. (LeCun et al., [1990](#bib.bib99)), and Optimal Brain Surgeon paper
    (OBD) by Hassibi et al. (Hassibi et al., [1993](#bib.bib73)). These methods usually
    take a network that has been pre-trained to a reasonable quality and then iteratively
    prune the parameters which have the lowest ‘saliency’ score, such that the impact
    on the validation loss is minimized. Once pruning concludes, the network is fine-tuned
    with the remaining parameters. This process is repeated a number of times until
    the desired number of original parameters are pruned (Algorithm [1](#algorithm1
    "In 3.1.1\. Pruning ‣ 3.1\. Compression Techniques ‣ 3\. Landscape of Efficient
    Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better")).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Data: Pre-trained dense network with weights $W$, inputs $X$, number of pruning
    rounds $N$, fraction of parameters to prune per round $p$.Result: Pruned network
    with weights $W^{\prime}$.1  $W^{\prime}\leftarrow W$;2  for *$i\leftarrow 1$
    to $N$* do3        $S\leftarrow\texttt{compute\_saliency\_scores}(W^{\prime})$;4      5      $W^{\prime}\leftarrow
    W^{\prime}-\texttt{select\_min\_k}\large(S,\frac{|W^{\prime}|}{p}\large)$;6        $W^{\prime}\leftarrow$fine_tune($X$,
    $W^{\prime}$)7 end forreturn $W^{\prime}$'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Standard Network Pruning with Fine-Tuning
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: OBD approximates the saliency score by using a second-derivative of the parameters
    ($\large\frac{\partial^{2}L}{\partial w_{i}^{2}}$), where $L$ is the loss function,
    and $w_{i}$ is the candidate parameter for removal. The intuition is that the
    higher this value for a given parameter, the larger the change in the loss function’s
    gradient if it were to be pruned.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: For the purpose of speeding up the computation of the second-derivatives, OBD
    ignores cross-interaction between the weights ($\large\frac{\partial^{2}L}{\partial
    w_{i}\partial w_{j}}$), and hence computes only the diagonal elements of the Hessian
    matrix. Otherwise, computing the full Hessian matrix is unwieldy for even a reasonable
    number of weights (with $n=10^{4}$, the size of the matrix is $10^{4}\times 10^{4}=10^{8}$).
    In terms of results, LeCun et al. demonstrate that pruning reduced the parameters
    in a well-trained neural net by   8x (combination of both automatic and manual
    removal) without a drop in classification accuracy.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Across different pruning strategies, the core algorithm could remain similar,
    with changes in the following aspects.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saliency: While (LeCun et al., [1990](#bib.bib99); Hassibi et al., [1993](#bib.bib73))
    use second-order derivatives, other methods rely on simpler magnitude based pruning
    (Han et al., [2015b](#bib.bib70), [a](#bib.bib69)), or momentum based pruning
    (Dettmers and Zettlemoyer, [2019](#bib.bib47)) etc. to determine the saliency
    score.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Structured v/s Unstructured: The most flexible way of pruning is unstructured
    (or random) pruning, where all given parameters are treated equally. In structured
    pruning, parameters are pruned in blocks (such as pruning row-wise in a weight
    matrix, or pruning channel-wise in a convolutional filter (Li et al., [2016](#bib.bib101);
    Anwar et al., [2017](#bib.bib6); Molchanov et al., [2016](#bib.bib113); Liu et al.,
    [2019](#bib.bib107)), etc.). The latter allows easier leveraging of inference-time
    gains in size and latency, since these blocks of pruned parameters can be intelligently
    skipped for storage and inference. Note that unstructured pruning can also be
    viewed as structured pruning with block size = 1.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结构化与非结构化：最灵活的修剪方式是非结构化（或随机）修剪，其中所有给定的参数都被平等对待。在结构化修剪中，参数按块修剪（例如，在权重矩阵中按行修剪，或在卷积滤波器中按通道修剪（Li
    et al., [2016](#bib.bib101); Anwar et al., [2017](#bib.bib6); Molchanov et al.,
    [2016](#bib.bib113); Liu et al., [2019](#bib.bib107)）等）。后者允许更容易利用推理时的大小和延迟的增益，因为这些修剪后的参数块可以智能地跳过存储和推理。请注意，非结构化修剪也可以看作是块大小为
    1 的结构化修剪。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Distribution: The decision about how to distribute the sparsity budget (number
    of parameters to be pruned), could be made either by pooling in all the parameters
    from the network and then deciding which parameters to prune, or by smartly selecting
    how much to prune in each layer individually (Dong et al., [2017](#bib.bib51);
    He et al., [2018](#bib.bib75)). (Elsen et al., [2020](#bib.bib53); google research,
    [2021](#bib.bib67)) have found that some architectures like MobileNetV2, EfficientNet
    (Tan et al., [2019](#bib.bib148)) have thin first layers that do not contribute
    significantly to the number of parameters and pruning them leads to an accuracy
    drop without much gain. Hence, intuitively it would be helpful to allocate sparsity
    on a per-layer basis.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分布：关于如何分配稀疏预算（需要修剪的参数数量）的决定，可以通过将网络中的所有参数汇总在一起，然后决定修剪哪些参数来做，或者通过智能地选择每层需要修剪多少参数来做（Dong
    et al., [2017](#bib.bib51); He et al., [2018](#bib.bib75)）。(Elsen et al., [2020](#bib.bib53);
    google research, [2021](#bib.bib67)) 发现一些架构如 MobileNetV2、EfficientNet (Tan et
    al., [2019](#bib.bib148)) 具有较薄的第一层，这些层对参数数量没有显著贡献，修剪它们会导致准确率下降，而收益不大。因此，直观上，将稀疏度分配到每层可能会更有帮助。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scheduling: Another question is how much to prune, and when? Should we prune
    an equal number of parameters every round (LeCun et al., [1990](#bib.bib99); Hassibi
    et al., [1993](#bib.bib73); Han et al., [2015b](#bib.bib70)), or should we prune
    at a higher pace in the beginning and gradually decrease (Zhu and Gupta, [2018](#bib.bib168);
    Dettmers and Zettlemoyer, [2019](#bib.bib47)).'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调度：另一个问题是修剪的数量和时机。我们是否应该每轮修剪相同数量的参数（LeCun et al., [1990](#bib.bib99); Hassibi
    et al., [1993](#bib.bib73); Han et al., [2015b](#bib.bib70)），或者在开始时以较高的速度修剪，然后逐渐减少（Zhu
    and Gupta, [2018](#bib.bib168); Dettmers and Zettlemoyer, [2019](#bib.bib47)）。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Regrowth: Some methods allow regrowing pruned connections (Evci et al., [2020](#bib.bib55);
    Dettmers and Zettlemoyer, [2019](#bib.bib47)) to keep the same level of sparsity
    through constant cycles of prune-redistribute-regrow. Dettmers et al. (Dettmers
    and Zettlemoyer, [2019](#bib.bib47)) estimate training time speedups between 2.7x
    - 5.6x by starting and operating with a sparse model throughout. However there
    is a gap in terms of implementation of sparse operations on CPU, GPU, and other
    hardware.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再生：一些方法允许再生修剪的连接（Evci et al., [2020](#bib.bib55); Dettmers and Zettlemoyer,
    [2019](#bib.bib47)），通过不断的修剪-重新分配-再生周期保持相同水平的稀疏度。Dettmers et al. (Dettmers and
    Zettlemoyer, [2019](#bib.bib47)) 估计在整个过程中使用稀疏模型可以将训练时间加速 2.7 倍到 5.6 倍。然而，在 CPU、GPU
    和其他硬件上实现稀疏操作存在差距。
- en: '| Model Architecture | Sparsity Type | Sparsity % | FLOPs | Top-1 Accuracy
    % | Source |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 模型架构 | 稀疏类型 | 稀疏百分比 | FLOPs | Top-1 准确率 % | 来源 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| MobileNet v2 - 1.0 | Dense (Baseline) | 0% | 1x | 72.0% | Sandler et al.
    (Sandler et al., [2018](#bib.bib134)) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet v2 - 1.0 | 密集（基线） | 0% | 1x | 72.0% | Sandler et al. (Sandler et
    al., [2018](#bib.bib134)) |'
- en: '| Unstructured | 75% | 0.27x | 67.7% | Zhu et al. (Zhu and Gupta, [2018](#bib.bib168))
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 非结构化 | 75% | 0.27x | 67.7% | Zhu et al. (Zhu and Gupta, [2018](#bib.bib168))
    |'
- en: '| Unstructured | 75% | 0.52x | 71.9% | Evci et al. (Evci et al., [2020](#bib.bib55))
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 非结构化 | 75% | 0.52x | 71.9% | Evci et al. (Evci et al., [2020](#bib.bib55))
    |'
- en: '| Structured (block-wise) | 85% | 0.11x | 69.7% | Elsen et al. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 结构化（按块） | 85% | 0.11x | 69.7% | Elsen et al. |'
- en: '| Unstructured | 90% | 0.12x | 61.8% | Zhu et al. (Zhu and Gupta, [2018](#bib.bib168))
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 非结构化 | 90% | 0.12x | 61.8% | Zhu et al. (Zhu and Gupta, [2018](#bib.bib168))
    |'
- en: '| Unstructured | 90% | 0.12x | 69.7% | Evci et al. (Evci et al., [2020](#bib.bib55))
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 无结构 | 90% | 0.12x | 69.7% | Evci 等（Evci 等，[2020](#bib.bib55)） |'
- en: Table 1\. A sample of various sparsity results on the MobileNet v2 architecture
    with depth multiplier = 1.0.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 在 MobileNet v2 架构中，深度倍增器 = 1.0 的各种稀疏性结果示例。
- en: 'Beyond Model Optimization: Frankle et al.’s (Frankle and Carbin, [2018](#bib.bib58))
    work on the Lottery Ticket Hypothesis took a different look at pruning, and postulated
    that within every large network lies a smaller network, which can be extracted
    with the original initialization of its parameters, and retrained on its own to
    match or exceed the performance of the larger network. The authors demonstrated
    these results on multiple datasets, but others such as (Gale et al., [2019](#bib.bib59);
    Liu et al., [2018b](#bib.bib108)) were not able to replicate this on larger datasets
    such as ImageNet (Deng et al., [2009](#bib.bib46)). Rather Liu et al. (Liu et al.,
    [2018b](#bib.bib108)) demonstrate that the pruned architecture with random initialization
    does no worse than the pruned architecture with the trained weights.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 超越模型优化：Frankle 等人（Frankle 和 Carbin，[2018](#bib.bib58)）关于彩票票据假设的研究从不同的角度看待了剪枝，并假设在每个大型网络中存在一个较小的网络，这个网络可以通过原始参数初始化进行提取，并重新训练以匹配或超过大型网络的性能。作者在多个数据集上展示了这些结果，但其他人（如
    Gale 等，[2019](#bib.bib59)；Liu 等，[2018b](#bib.bib108)）未能在较大的数据集（如 ImageNet（Deng
    等，[2009](#bib.bib46)））上复制这些结果。相反，Liu 等（Liu 等，[2018b](#bib.bib108)）展示了具有随机初始化的剪枝架构不逊色于具有训练权重的剪枝架构。
- en: 'Discussion: There is a significant body of work that demonstrates impressive
    theoretical reduction in the model size (via number of parameters), or estimates
    the savings in FLOPs (Table [1](#S3.T1 "Table 1 ‣ 3.1.1\. Pruning ‣ 3.1\. Compression
    Techniques ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning:
    A Survey on Making Deep Learning Models Smaller, Faster, and Better")). However,
    a large fraction of the results are on *unstructured* pruning, where it is not
    currently clear how these improvements can lead to reduction in footprint metrics
    (apart from using standard file compression tools like GZip).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '讨论：大量的研究表明，模型大小（通过参数数量）在理论上显著减少，或估算了 FLOPs 的节省（表 [1](#S3.T1 "Table 1 ‣ 3.1.1\.
    Pruning ‣ 3.1\. Compression Techniques ‣ 3\. Landscape of Efficient Deep Learning
    ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster,
    and Better")）。然而，大部分结果是在*无结构*剪枝上，尚不清楚这些改进如何导致占用指标的减少（除了使用标准文件压缩工具如 GZip）。'
- en: On the other hand, structured pruning with a meaningful block size is conducive
    to latency improvements. Elsen et al. (Elsen et al., [2020](#bib.bib53); google
    research, [2021](#bib.bib67)) construct sparse convolutional networks that outperform
    their dense counterparts by $1.3$ - $2.4\times$ with $\approx$ 66% of the parameters,
    while retaining the same Top-1 accuracy. They do this via their library to convert
    from the NHWC (channels-last) standard dense representation to a special NCHW
    (channels-first) ‘Block Compressed Sparse Row’ (BCSR) representation which is
    suitable for fast inference using their fast kernels on ARM devices, WebAssembly
    etc. (Authors, [2021k](#bib.bib19)). Although they also introduce some constraints
    on the kinds of sparse networks that can be accelerated (Authors, [2021l](#bib.bib20)).
    Overall, this is a promising step towards practical improvements in footprint
    metrics with pruned networks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，具有有意义的块大小的结构化剪枝有利于延迟改进。Elsen 等（Elsen 等，[2020](#bib.bib53)；谷歌研究，[2021](#bib.bib67)）构建了稀疏卷积网络，比其密集的对应物提高了
    $1.3$ - $2.4\times$，参数约为 66%，同时保持相同的 Top-1 精度。他们通过他们的库将 NHWC（通道最后）标准密集表示转换为适合于
    ARM 设备、WebAssembly 等快速内核的 NCHW（通道优先）‘块压缩稀疏行’（BCSR）表示来实现这一点（作者，[2021k](#bib.bib19)）。尽管他们也对可以加速的稀疏网络类型提出了一些限制（作者，[2021l](#bib.bib20)）。总体而言，这一步朝着通过剪枝网络实现占用指标的实际改进迈出了令人鼓舞的一步。
- en: 3.1.2\. Quantization
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 量化
- en: 'Almost all the weights and activations of a typical network are in 32-bit floating-point
    values. One of the ideas of reducing model footprint is to reduce the precision
    for the weights and activations by *quantizing* to a lower-precision datatype
    (often 8-bit fixed-point integers). There are two kinds of gains that we can get
    from quantization: (a) lower model size, and (b) lower inference latency. Often,
    only the model size is a constraint, and in this case we can employ a technique
    called weight quantization and get model size improvements (Authors, [2021f](#bib.bib14)),
    where only the model weights are in reduced precision. In order to get latency
    improvements, the activations need to be in fixed-point as well (Activation Quantization
    (Vanhoucke et al., [2011](#bib.bib154); Jacob et al., [2018](#bib.bib83)), such
    that all the operations in the quantized graph are happening in fixed-point math
    as well.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Weight Quantization: A simple *scheme* for quantizing weights to get model
    size improvements (similar to (Krishnamoorthi, [2018](#bib.bib91))) is as follows.
    Given a 32-bit floating-point weight matrix in a model, we can map the minimum
    weight value ($x_{min}$) in that matrix to $0$, and the maximum value ($x_{max}$)
    to $2^{b}-1$ (where $b$ is the number of bits of precision, and $b<32$). Then
    we can linearly extrapolate all values between them to an integer value in [$0,2^{b}-1$]
    (Figure  [5](#S3.F5 "Figure 5 ‣ 3.1.2\. Quantization ‣ 3.1\. Compression Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better")). Thus, we are able
    to map each floating point value to a fixed-point value where the latter requires
    a lesser number of bits than the floating-point representation. This process can
    also be done for signed $b$-bit fixed-point integers, where the output values
    will be in the range [-$2^{\frac{b}{2}}-1$, $2^{\frac{b}{2}}-1$]. One of the reasonable
    values of $b$ is $8$, since this would lead to a $32/8=4\times$ reduction in space,
    and also because of the near-universal support for uint8_t and int8_t datatypes.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: During inference, we go in the reverse direction where we recover a lossy estimate
    of the original floating point value (*dequantization*) using just the $x_{min}$
    and $x_{max}$. This estimate is lossy since we lost $32-b$ bits of information
    when did the rounding (another way to look at it is that a range of floating point
    values map to the same quantized value).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a238d6417900d87f3700f271423324e8.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Quantizing floating-point continuous values to discrete fixed-point
    values. The continuous values are clamped to the range $x_{min}$ to $x_{max}$,
    and are mapped to discrete values in [$0$, $2^{b}-1$] (in the above figure, $b=3$,
    hence the quantized values are in the range [$0,7$].
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '(Jacob et al., [2018](#bib.bib83); Krishnamoorthi, [2018](#bib.bib91)) formalize
    the quantization scheme with the following two constraints:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (Jacob 等，[2018](#bib.bib83)；Krishnamoorthi，[2018](#bib.bib91)) 通过以下两个约束来形式化量化方案：
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The quantization scheme should be linear (affine transformation), so that the
    precision bits are linearly distributed.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化方案应为线性（仿射变换），以使精度位线性分布。
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $0.0$ should map exactly to a fixed-point value $x_{q_{0}}$, such that dequantizing
    $x_{q_{0}}$ gives us $0.0$. This is an implementation constraint, since $0$ is
    also used for padding to signify missing elements in tensors, and if dequantizing
    $x_{q_{0}}$ leads to a non-zero value, then it might be interpreted incorrectly
    as a valid element at that index.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $0.0$ 应精确映射到定点值 $x_{q_{0}}$，使得反量化 $x_{q_{0}}$ 能得到 $0.0$。这是一个实现约束，因为 $0$ 也用于填充以表示张量中的缺失元素，如果反量化
    $x_{q_{0}}$ 导致非零值，则可能会被误解为该索引处的有效元素。
- en: 'The second constraint described above requires that $0$ be a part of the quantization
    range, which in turn requires updating $x_{min}$ and $x_{max}$, followed by clamping
    $x$ to lie in $[x_{min},x_{max}]$. Following this, we can quantize $x$ by constructing
    a piece-wise linear transformation as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 上述第二个约束要求 $0$ 是量化范围的一部分，这反过来又要求更新 $x_{min}$ 和 $x_{max}$，然后将 $x$ 限制在 $[x_{min},x_{max}]$
    之间。接着，我们可以通过构造分段线性变换来量化 $x$：
- en: '| (1) |  | $\small\textrm{quantize}(x)=x_{q}=\textrm{round}\bigg{(}\frac{x}{s}\bigg{)}+z$
    |  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\small\textrm{quantize}(x)=x_{q}=\textrm{round}\bigg{(}\frac{x}{s}\bigg{)}+z$
    |  |'
- en: '$s$ is the floating-point *scale* value (can be thought of as the inverse of
    the slope, which can be computed using $x_{min}$, $x_{max}$ and the range of the
    fixed-point values). $z$ is an integer *zero-point* value which is the quantized
    value that is assigned to $x=0.0$. This is the terminology followed in literature
    (Jacob et al., [2018](#bib.bib83); Krishnamoorthi, [2018](#bib.bib91)) (Algorithm
    [2](#algorithm2 "In 3.1.2\. Quantization ‣ 3.1\. Compression Techniques ‣ 3\.
    Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better")).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: $s$ 是浮点数 *缩放* 值（可以看作是斜率的倒数，可以通过 $x_{min}$、$x_{max}$ 和定点值范围来计算）。$z$ 是整数 *零点*
    值，它是分配给 $x=0.0$ 的量化值。这是文献中使用的术语（Jacob 等，[2018](#bib.bib83)；Krishnamoorthi，[2018](#bib.bib91)）（算法
    [2](#algorithm2 "在 3.1.2\. 量化 ‣ 3.1\. 压缩技术 ‣ 3\. 高效深度学习的格局 ‣ 高效深度学习：对深度学习模型变小、更快、更好的调查")）。
- en: 'The dequantization step constructs $\hat{x}$, which is a lossy estimate of
    $x$, since we lose precision when quantizing to a lower number of bits. We can
    compute it as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 反量化步骤构造了 $\hat{x}$，这是 $x$ 的有损估计，因为在量化为较少的位数时，我们会失去精度。我们可以按如下方式计算它：
- en: '| (2) |  | $\small\textrm{dequantize}(x_{q})=\hat{x}=s(x_{q}-z)$ |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\small\textrm{dequantize}(x_{q})=\hat{x}=s(x_{q}-z)$ |  |'
- en: 'Since $s$ is in floating-point, $\hat{x}$ is also a floating-point value (Algorithm
    [3](#algorithm3 "In 3.1.2\. Quantization ‣ 3.1\. Compression Techniques ‣ 3\.
    Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better")). Note that the quantization
    and dequantization steps can be performed for signed integers too by appropriately
    changing the value $x_{q_{min}}$ (which is the lowest fixed-point value in $b$-bits)
    in Algorithm [2](#algorithm2 "In 3.1.2\. Quantization ‣ 3.1\. Compression Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better").'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $s$ 是浮点数，$\hat{x}$ 也是浮点数值（算法 [3](#algorithm3 "在 3.1.2\. 量化 ‣ 3.1\. 压缩技术 ‣
    3\. 高效深度学习的格局 ‣ 高效深度学习：对深度学习模型变小、更快、更好的调查")）。注意，量化和反量化步骤也可以通过适当地改变值 $x_{q_{min}}$（即
    $b$ 位中的最低定点值）来对有符号整数进行。
- en: 'Data: Floating-point tensor to compress $\mathbf{X}$, number of precision bits
    $b$ for the fixed-point representation.Result: Quantized tensor $\mathbf{X_{q}}$.1  $\textbf{X}_{min},\textbf{X}_{max}\leftarrow\textrm{min}(\mathbf{X},0),\textrm{max}(\mathbf{X},0)$;2  $\mathbf{X}\leftarrow\textrm{clamp}(\mathbf{X},\textbf{X}_{min},\textbf{X}_{max})$;3  $s\leftarrow\frac{\displaystyle
    x_{max}-x_{min}}{\displaystyle 2^{b}-1}$;4  $z\leftarrow\textrm{round}\bigg{(}x_{q_{min}}-\frac{\displaystyle
    x_{min}}{\displaystyle s}\bigg{)}$;56$\mathbf{X_{q}}\leftarrow\textrm{round}\bigg{(}\frac{\displaystyle\mathbf{X}}{\displaystyle
    s}\bigg{)}+z$;7 return $\mathbf{X_{q}}$;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 数据：用于压缩的浮点张量 $\mathbf{X}$，定点表示的精度位数 $b$。结果：量化张量 $\mathbf{X_{q}}$。1  $\textbf{X}_{min},\textbf{X}_{max}\leftarrow\textrm{min}(\mathbf{X},0),\textrm{max}(\mathbf{X},0)$；2  $\mathbf{X}\leftarrow\textrm{clamp}(\mathbf{X},\textbf{X}_{min},\textbf{X}_{max})$；3  $s\leftarrow\frac{\displaystyle
    x_{max}-x_{min}}{\displaystyle 2^{b}-1}$；4  $z\leftarrow\textrm{round}\bigg{(}x_{q_{min}}-\frac{\displaystyle
    x_{min}}{\displaystyle s}\bigg{)}$；56$\mathbf{X_{q}}\leftarrow\textrm{round}\bigg{(}\frac{\displaystyle\mathbf{X}}{\displaystyle
    s}\bigg{)}+z$；7 返回 $\mathbf{X_{q}}$；
- en: Algorithm 2 Quantizing a given weight matrix $\mathbf{X}$
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 量化给定的权重矩阵 $\mathbf{X}$
- en: 'Data: Fixed-point matrix to dequantize $\mathbf{X_{q}}$, along with the scale
    $s$, and zero-point $z$ values which were calculated during quantization.Result:
    Dequantized floating-point weight matrix $\widehat{\mathbf{X}}$.1  $\widehat{\mathbf{X}}\leftarrow
    s(\mathbf{X_{q}}-z)$;2 return $\widehat{\mathbf{X}}$;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 数据：用于解量化的定点矩阵 $\mathbf{X_{q}}$，以及在量化过程中计算得出的缩放因子 $s$ 和零点 $z$ 值。结果：解量化的浮点权重矩阵
    $\widehat{\mathbf{X}}$。1  $\widehat{\mathbf{X}}\leftarrow s(\mathbf{X_{q}}-z)$；2
    返回 $\widehat{\mathbf{X}}$；
- en: Algorithm 3 Dequantizing a given fixed-point weight matrix $\mathbf{X_{q}}$
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 解量化给定的定点权重矩阵 $\mathbf{X_{q}}$
- en: We can utilize the above two algorithms for quantizing and dequantizing the
    model’s weight matrices. Quantizing a pre-trained model’s weights for reducing
    the size is termed as *post-training quantization* in literature (Authors, [2021f](#bib.bib14)).
    This might be sufficient for the purpose of reducing the model size when there
    is sufficient representational capacity in the model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用上述两个算法来量化和解量化模型的权重矩阵。对预训练模型权重进行量化以减少模型大小在文献中称为 *后训练量化*（Authors, [2021f](#bib.bib14)）。当模型具有足够的表示能力时，这可能足以减少模型大小。
- en: There are other works in literature (Rastegari et al., [2016](#bib.bib128);
    Hubara et al., [2016](#bib.bib81); Li et al., [2016](#bib.bib100)) that demonstrate
    slightly different variants of quantization. XNOR-Net (Rastegari et al., [2016](#bib.bib128)),
    Binarized Neural Networks (Hubara et al., [2016](#bib.bib81)) and others use $b=1$,
    and thus have weight matrices which just have two possible values $0$ or $1$,
    and the quantization function there is simply the $\textrm{sign}(x)$ function
    (assuming the weights are symmetrically distributed around $0$).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中还有其他工作（Rastegari 等人，[2016](#bib.bib128)；Hubara 等人，[2016](#bib.bib81)；Li 等人，[2016](#bib.bib100)）展示了略有不同的量化变体。
    XNOR-Net（Rastegari 等人，[2016](#bib.bib128)）、二值化神经网络（Hubara 等人，[2016](#bib.bib81)）等使用
    $b=1$，因此权重矩阵只有 $0$ 或 $1$ 两个可能的值，量化函数简单地是 $\textrm{sign}(x)$ 函数（假设权重围绕 $0$ 对称分布）。
- en: The promise with such extreme quantization approaches is the theoretical $32/1=32\times$
    reduction in model size without much quality loss. Some of the works claim improvements
    on larger networks like AlexNet (Krizhevsky et al., [2012](#bib.bib93)), VGG (Simonyan
    and Zisserman, [2014](#bib.bib142)), Inception (Szegedy et al., [2015](#bib.bib147))
    etc., which might already be more amenable to compression. A more informative
    task would be to demonstrate extreme quantization on smaller networks like the
    MobileNet family (Sandler et al., [2018](#bib.bib134); Howard et al., [2019](#bib.bib78)).
    Additionally binary quantization (and other quantization schemes like ternary
    (Li et al., [2016](#bib.bib100)), bit-shift based networks (Rastegari et al.,
    [2016](#bib.bib128)), etc.) promise latency-efficient implementations of standard
    operations where multiplications and divisions are replaced by cheaper operations
    like addition, subtraction, etc. These claims need to be verified because even
    if these lead to theoretical reduction in FLOPs, the implementations still need
    support from the underlying hardware. A fair comparison would be using standard
    quantization with $b=8$, where the multiplications and divisions also become cheaper,
    and are supported by the hardware efficiently via SIMD instructions which allow
    for low-level data parallelism (for example, on x86 via the SSE instruction set,
    on ARM via the Neon (Ltd., [2021](#bib.bib109)) intrinsics, and even on specialized
    DSPs like the Qualcomm Hexagon (Authors, [2021l](#bib.bib20))).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这种极端量化方法的承诺是模型大小理论上可以减少 $32/1=32\times$，且几乎没有质量损失。一些研究声称在更大的网络如 AlexNet（Krizhevsky
    et al., [2012](#bib.bib93)）、VGG（Simonyan and Zisserman, [2014](#bib.bib142)）、Inception（Szegedy
    et al., [2015](#bib.bib147)）等上取得了改进，这些网络可能已经更适合压缩。一个更具信息量的任务是演示在更小的网络如 MobileNet
    系列（Sandler et al., [2018](#bib.bib134); Howard et al., [2019](#bib.bib78)）上的极端量化。此外，二值量化（以及其他量化方案如三值量化（Li
    et al., [2016](#bib.bib100)）、基于位移的网络（Rastegari et al., [2016](#bib.bib128)）等）承诺在标准操作中实现低延迟，其中乘法和除法被加法、减法等更便宜的操作替代。这些声明需要验证，因为即使这些方法在理论上减少了
    FLOPs，实际实现仍需得到底层硬件的支持。一个公平的比较是使用标准量化 $b=8$，其中乘法和除法也变得更便宜，并通过 SIMD 指令得到硬件的高效支持，这允许低级数据并行处理（例如，通过
    SSE 指令集在 x86 上，通过 Neon（Ltd., [2021](#bib.bib109)）指令在 ARM 上，甚至通过 Qualcomm Hexagon（Authors,
    [2021l](#bib.bib20)）等专用 DSP）。
- en: 'Activation Quantization: To be able to get *latency improvements* with quantized
    networks, the math operations have to be done in fixed-point representations too.
    This means all intermediate layer inputs and outputs are also in fixed-point,
    and there is no need to dequantize the weight-matrices since they can be used
    directly along with the inputs.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 激活量化：为了在量化网络中获得*延迟改进*，数学运算也必须以固定点表示进行。这意味着所有中间层的输入和输出也都是固定点的，并且不需要对权重矩阵进行反量化，因为它们可以直接与输入一起使用。
- en: Vanhoucke et al. (Vanhoucke et al., [2011](#bib.bib154)) demonstrated a $3\times$
    inference speedup using a fully fixed-point model on an x86 CPU, when compared
    to a floating-point model on the same CPU, without sacrificing accuracy. The weights
    are still quantized similar to post-training quantization, however all layer inputs
    (except the first layer) and the activations are fixed-point. In terms of performance,
    the primary driver for this improvement was the availability of fixed-point SIMD
    instructions in Intel’s SSE4 instruction set (Contributors to Wikimedia projects,
    [2021e](#bib.bib42)), where commonly used building-block operations like the Multiply-Accumulate
    (MAC) (Contributors to Wikimedia projects, [2021d](#bib.bib41)) can be parallelized.
    Since the paper was published, Intel has released two more iterations of these
    instruction sets (Contributors to Wikimedia projects, [2021a](#bib.bib38)) which
    might further improve the speedups.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Vanhoucke 等人（Vanhoucke et al., [2011](#bib.bib154)）在 x86 CPU 上使用完全固定点模型实现了 $3\times$
    的推理加速，相较于同一 CPU 上的浮点模型，且没有牺牲准确性。权重仍然像训练后量化一样被量化，但所有层的输入（除了第一层）和激活值都是固定点的。在性能方面，这一改进的主要驱动力是
    Intel SSE4 指令集中的固定点 SIMD 指令（Contributors to Wikimedia projects, [2021e](#bib.bib42)），其中常用的构建块操作如乘加（MAC）（Contributors
    to Wikimedia projects, [2021d](#bib.bib41)）可以并行处理。自论文发布以来，Intel 发布了这类指令集的两个新版本（Contributors
    to Wikimedia projects, [2021a](#bib.bib38)），这可能进一步提高加速效果。
- en: 'Quantization-Aware Training (QAT): The network that Vanhoucke et al. mention
    was a 5 layer feed-forward network that was post-training quantized. However post-training
    quantization can lead to quality loss during inference as highlighted in (Krishnamoorthi,
    [2018](#bib.bib91); Jacob et al., [2018](#bib.bib83); Wang et al., [2020](#bib.bib157))
    as the networks become more complex. These could be because of: (a) outlier weights
    that skew the computation of the quantized values for the entire input range towards
    the outliers, leading to less number of bits being allocated to the bulk of the
    range, or (b) Different distribution of weights within the weight matrix, for
    eg. within a convolutional layer the distribution of weights between each filter
    might be different, but they are quantized the same way. These effects might be
    more pronounced at low-bit widths due to an even worse loss of precision. Wang
    et al. (Wang et al., [2020](#bib.bib157)) try to retain the post-training quantization
    but with new heuristics to allocate the precision bits in a learned fashion. Tools
    like the TFLite Converter (TensorFlow, [2019](#bib.bib150)) augment post-training
    quantization with a representative dataset provided by the user, to actively correct
    for errors at different points in the model by comparing the error between the
    activations of the quantized and unquantized graphs.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 量化感知训练（QAT）：Vanhoucke 等提到的网络是一个5层前馈网络，经过训练后进行了量化。然而，如 Krishnamoorthi（Krishnamoorthi,
    [2018](#bib.bib91)）、Jacob 等（Jacob et al., [2018](#bib.bib83)）和 Wang 等（Wang et
    al., [2020](#bib.bib157)）所强调，后训练量化可能会导致推理过程中质量的损失，尤其是当网络变得更复杂时。这可能是由于：（a）异常值权重使得整个输入范围的量化值计算偏向异常值，从而导致大部分范围分配的位数减少；或者（b）权重矩阵内权重的不同分布，例如，在卷积层中，每个滤波器之间的权重分布可能不同，但它们的量化方式相同。这些效应在低位宽下可能会更加明显，因为精度损失更严重。Wang
    等（Wang et al., [2020](#bib.bib157)）尝试保留后训练量化，但通过新的启发式方法以学习的方式分配精度位。像 TFLite 转换器（TensorFlow,
    [2019](#bib.bib150)）这样的工具，通过用户提供的代表性数据集增强了后训练量化，通过比较量化和未量化图的激活之间的误差，主动修正模型中不同点的错误。
- en: Jacob et al. (Jacob et al., [2018](#bib.bib83)) propose (and further detailed
    by Krishnamoorthi et al. (Krishnamoorthi, [2018](#bib.bib91))) a training regime
    which is *quantization-aware*. In this setting, the training happens in floating-point
    but the forward-pass simulates the quantization behavior during inference. Both
    weights and activations are passed through a function that simulates this quantization
    behavior (*fake-quantized* is the term used by many works (Jacob et al., [2018](#bib.bib83);
    Krishnamoorthi, [2018](#bib.bib91))).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Jacob 等（Jacob et al., [2018](#bib.bib83)）提出了一个*量化感知*的训练机制，并由 Krishnamoorthi
    等（Krishnamoorthi, [2018](#bib.bib91)）进一步详细说明。在这种设置下，训练是在浮点数中进行的，但前向传播会模拟推理过程中量化的行为。权重和激活都通过一个函数来模拟这种量化行为（许多文献使用“*伪量化*”这一术语（Jacob
    et al., [2018](#bib.bib83); Krishnamoorthi, [2018](#bib.bib91)））。
- en: 'Assuming $\mathbf{X}$ is the tensor to be fake-quantized, Jacob et al. (Jacob
    et al., [2018](#bib.bib83)) propose adding special quantization nodes in the training
    graph that collect the statistics (moving averages of $x_{min}$ and $x_{max}$)
    related to the weights and activations to be quantized (see Figure [6b](#S3.F6.sf2
    "In Figure 6 ‣ 3.1.2\. Quantization ‣ 3.1\. Compression Techniques ‣ 3\. Landscape
    of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep
    Learning Models Smaller, Faster, and Better")(a) for an illustration). Once we
    have these values for each $\mathbf{X}$, we can derive the respective $\widehat{\mathbf{X}}$
    using equations ([1](#S3.E1 "In 3.1.2\. Quantization ‣ 3.1\. Compression Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") and [2](#S3.E2 "In
    3.1.2\. Quantization ‣ 3.1\. Compression Techniques ‣ 3\. Landscape of Efficient
    Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better")) as follows.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 $\mathbf{X}$ 是待伪量化的张量，Jacob 等人（Jacob et al.，[2018](#bib.bib83)）建议在训练图中添加特殊的量化节点，这些节点收集与待量化的权重和激活相关的统计信息（$x_{min}$
    和 $x_{max}$ 的移动平均）（参见图 [6b](#S3.F6.sf2 "在图 6 ‣ 3.1.2\. 量化 ‣ 3.1\. 压缩技术 ‣ 3\. 高效深度学习的全景
    ‣ 高效深度学习：在使深度学习模型更小、更快、更好方面的调查")(a) 获取说明）。一旦我们获得了每个 $\mathbf{X}$ 的这些值，就可以使用方程式（[1](#S3.E1
    "在 3.1.2\. 量化 ‣ 3.1\. 压缩技术 ‣ 3\. 高效深度学习的全景 ‣ 高效深度学习：在使深度学习模型更小、更快、更好方面的调查") 和
    [2](#S3.E2 "在 3.1.2\. 量化 ‣ 3.1\. 压缩技术 ‣ 3\. 高效深度学习的全景 ‣ 高效深度学习：在使深度学习模型更小、更快、更好方面的调查")）推导出相应的
    $\widehat{\mathbf{X}}$。
- en: '| (3) |  | <math   alttext="\small\begin{split}\widehat{\mathbf{X}}{}&amp;=\textrm{FakeQuant}(\mathbf{X})\\
    &amp;=\textrm{Dequantize}(\textrm{Quantize}(\mathbf{X}))\\'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '| (3) |  | <math   alttext="\small\begin{split}\widehat{\mathbf{X}}{}&amp;=\textrm{FakeQuant}(\mathbf{X})\\
    &amp;=\textrm{Dequantize}(\textrm{Quantize}(\mathbf{X}))\\'
- en: '&amp;=s((\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}+z)-z)\\'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=s((\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}+z)-z)\\'
- en: '&amp;=s\bigg{(}\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}\bigg{)}\\'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=s\bigg{(}\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}\bigg{)}\\'
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd columnalign="right"  ><mover accent="true"  ><mi
    mathsize="90%" >𝐗</mi><mo mathsize="90%" >^</mo></mover></mtd><mtd columnalign="left"  ><mrow
    ><mo mathsize="90%" >=</mo><mrow  ><mtext mathsize="90%"  >FakeQuant</mtext><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="90%" minsize="90%" >(</mo><mi
    mathsize="90%" >𝐗</mi><mo maxsize="90%" minsize="90%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo mathsize="90%"  >=</mo><mrow ><mtext mathsize="90%"
    >Dequantize</mtext><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="90%"
    minsize="90%"  >(</mo><mrow ><mtext mathsize="90%" >Quantize</mtext><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo maxsize="90%" minsize="90%"  >(</mo><mi mathsize="90%"  >𝐗</mi><mo
    maxsize="90%" minsize="90%"  >)</mo></mrow></mrow><mo maxsize="90%" minsize="90%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo mathsize="90%"  >=</mo><mrow ><mi mathsize="90%"
    >s</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="90%" minsize="90%"  >(</mo><mrow
    ><mrow ><mo maxsize="90%" minsize="90%" >(</mo><mrow ><mrow ><mtext mathsize="90%"
    >round</mtext><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo maxsize="210%" minsize="210%"  >(</mo><mfrac
    ><mrow ><mtext mathsize="90%" >clamp</mtext><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo maxsize="90%" minsize="90%"  >(</mo><mi mathsize="90%"  >𝐗</mi><mo mathsize="90%"  >,</mo><msub
    ><mi mathsize="90%"  >x</mi><mrow ><mi mathsize="90%"  >m</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathsize="90%"  >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathsize="90%"  >n</mi></mrow></msub><mo mathsize="90%"  >,</mo><msub ><mi mathsize="90%"  >x</mi><mrow
    ><mi mathsize="90%"  >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >a</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >x</mi></mrow></msub><mo
    maxsize="90%" minsize="90%"  >)</mo></mrow></mrow><mi mathsize="90%"  >s</mi></mfrac><mo
    maxsize="210%" minsize="210%"  >)</mo></mrow></mrow><mo mathsize="90%"  >+</mo><mi
    mathsize="90%"  >z</mi></mrow><mo maxsize="90%" minsize="90%"  >)</mo></mrow><mo
    mathsize="90%"  >−</mo><mi mathsize="90%"  >z</mi></mrow><mo maxsize="90%" minsize="90%"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo mathsize="90%"  >=</mo><mrow ><mi mathsize="90%"
    >s</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo maxsize="210%" minsize="210%"  >(</mo><mrow
    ><mtext mathsize="90%" >round</mtext><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo maxsize="210%" minsize="210%"  >(</mo><mfrac ><mrow ><mtext mathsize="90%"
    >clamp</mtext><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo maxsize="90%" minsize="90%"  >(</mo><mi
    mathsize="90%"  >𝐗</mi><mo mathsize="90%"  >,</mo><msub ><mi mathsize="90%"  >x</mi><mrow
    ><mi mathsize="90%"  >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >n</mi></mrow></msub><mo
    mathsize="90%"  >,</mo><msub ><mi mathsize="90%"  >x</mi><mrow ><mi mathsize="90%"  >m</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"  >a</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathsize="90%"  >x</mi></mrow></msub><mo maxsize="90%"
    minsize="90%"  >)</mo></mrow></mrow><mi mathsize="90%"  >s</mi></mfrac><mo maxsize="210%"
    minsize="210%"  >)</mo></mrow></mrow><mo maxsize="210%" minsize="210%"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><ci >^</ci><ci  >𝐗</ci></apply><apply
    ><ci ><mtext mathsize="90%" >FakeQuant</mtext></ci><ci >𝐗</ci></apply></apply><apply
    ><apply  ><ci ><mtext mathsize="90%" >Dequantize</mtext></ci><apply ><ci ><mtext
    mathsize="90%" >Quantize</mtext></ci><ci >𝐗</ci></apply></apply></apply><apply
    ><apply  ><ci >𝑠</ci><apply ><apply ><apply  ><ci ><mtext mathsize="90%" >round</mtext></ci><apply
    ><apply ><ci  ><mtext mathsize="90%"  >clamp</mtext></ci><vector ><ci >𝐗</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><apply ><ci >𝑚</ci><ci
    >𝑖</ci><ci >𝑛</ci></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><apply ><ci >𝑚</ci><ci >𝑎</ci><ci >𝑥</ci></apply></apply></vector></apply><ci
    >𝑠</ci></apply></apply><ci >𝑧</ci></apply><ci >𝑧</ci></apply></apply></apply><apply
    ><apply ><ci  >𝑠</ci><apply ><ci ><mtext mathsize="90%" >round</mtext></ci><apply
    ><apply  ><ci ><mtext mathsize="90%"  >clamp</mtext></ci><vector ><ci >𝐗</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><apply ><ci >𝑚</ci><ci
    >𝑖</ci><ci >𝑛</ci></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><apply ><ci >𝑚</ci><ci >𝑎</ci><ci >𝑥</ci></apply></apply></vector></apply><ci
    >𝑠</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >\small\begin{split}\widehat{\mathbf{X}}{}&=\textrm{FakeQuant}(\mathbf{X})\\ &=\textrm{Dequantize}(\textrm{Quantize}(\mathbf{X}))\\
    &=s((\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}+z)-z)\\ &=s\bigg{(}\textrm{round}\bigg{(}\frac{\displaystyle\textrm{clamp}(\mathbf{X},x_{min},x_{max})}{\displaystyle
    s}\bigg{)}\bigg{)}\\ \end{split}</annotation></semantics></math> |  |
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Since the above equation is not directly differentiable because of the rounding
    behavior, to optimize a loss function $L$ w.r.t. $\mathbf{X}$, we can compute
    $\frac{\displaystyle\partial{L}}{\displaystyle\partial{\mathbf{X}}}$ by chain-rule
    using the Straight-Through Estimator (STE) (Bengio et al., [2013](#bib.bib23)).
    This allows us to make the staircase function differentiable with a linear approximation
    (See (Krishnamoorthi, [2018](#bib.bib91)) for details).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述方程因舍入行为而不可直接微分，为了优化相对于$\mathbf{X}$的损失函数$L$，我们可以使用直通估计器（STE）（Bengio等，[2013](#bib.bib23)）通过链式法则计算$\frac{\displaystyle\partial{L}}{\displaystyle\partial{\mathbf{X}}}$。这使我们可以通过线性近似使阶梯函数可微分（有关详细信息，请参见
    (Krishnamoorthi, [2018](#bib.bib91) )）。
- en: Quantization-Aware Training allows the network to adapt to tolerate the noise
    introduced by the clamping and rounding behavior during inference. Once the network
    is trained, tools such as the TFLite Model Converter (Authors, [2021h](#bib.bib16))
    can generate the appropriate fixed-point inference model from a network annotated
    with the quantization nodes.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 量化感知训练使网络能够适应推理过程中由钳制和舍入行为引入的噪声。一旦网络训练完成，工具如TFLite模型转换器（作者，[2021h](#bib.bib16)）可以从标注了量化节点的网络生成适当的定点推理模型。
- en: 'Other Notable Works: Polino et al. (Polino et al., [2018](#bib.bib125)) allow
    non-uniform distribution of precision with learning a vector of quantization-points
    $p$, along with using distillation to further reduce loss of accuracy. The results
    for simpler datasets like CIFAR-10 are comparable to (Krishnamoorthi, [2018](#bib.bib91);
    Jacob et al., [2018](#bib.bib83)). However, when working with ResNet architecture
    on the ImageNet dataset, they achieve lower model size and faster inference by
    using shallower student networks. This is not a fair comparison, since other works
    do not mix distillation along with quantization. Fan et al. (Fan et al., [2020](#bib.bib56))
    demonstrate accuracy improvement on top of standard QAT ((Jacob et al., [2018](#bib.bib83)))
    with $b<8$. They hypothesize that the networks will learn better if the fake-quantization
    is not applied to the complete tensor at the same time to allow unbiased gradients
    to flow (instead of the STE approximation). Instead, they apply the fake-quantization
    operation stochastically in a block-wise manner on the given tensor. They also
    demonstrate improvements over QAT on 4-bit quantized Transformer and EfficientNet
    (Tan et al., [2019](#bib.bib148)) networks.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 其他显著工作：Polino等（Polino等，[2018](#bib.bib125)）允许通过学习量化点向量$p$来实现精度的非均匀分布，并使用蒸馏进一步减少精度损失。对于CIFAR-10等简单数据集的结果与(Krishnamoorthi,
    [2018](#bib.bib91); Jacob等，[2018](#bib.bib83)) 相当。然而，当在ImageNet数据集上使用ResNet架构时，他们通过使用较浅的学生网络来实现更低的模型大小和更快的推理。这并不是一个公平的比较，因为其他工作没有将蒸馏与量化混合在一起。Fan等（Fan等，[2020](#bib.bib56)）展示了在标准QAT（（Jacob等，[2018](#bib.bib83)））基础上的准确度提升，且$b<8$。他们假设如果假量化不是同时应用于整个张量，而是允许无偏的梯度流动（而不是STE近似），网络将会学习得更好。相反，他们在给定张量上以块状方式随机应用假量化操作。他们还展示了在4位量化的Transformer和EfficientNet（Tan等，[2019](#bib.bib148)）网络上对QAT的改进。
- en: 'Results: Refer to Table [2](#S3.T2 "Table 2 ‣ 3.1.2\. Quantization ‣ 3.1\.
    Compression Techniques ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    for a comparison between the baseline floating-point model, post-training quantized,
    and quantization-aware trained models (Authors, [2021f](#bib.bib14)). The model
    with post-training quantization gets close to the baseline, but there is still
    a significant accuracy difference. The model size is $4\times$ smaller, however
    the latency is slightly higher due to the need to dequantize the weights during
    inference. The model with 8-bit Quantization-Aware Training (QAT) gets quite close
    to the baseline floating point model while requiring $4\times$ less disk space
    and being $1.64\times$ faster.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '结果：请参阅表[2](#S3.T2 "Table 2 ‣ 3.1.2\. Quantization ‣ 3.1\. Compression Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") 比较基准浮点模型、后训练量化模型和量化感知训练模型（作者，[2021f](#bib.bib14)）。后训练量化模型接近基准模型，但仍存在显著的准确度差异。模型大小减少了$4\times$，但由于推理过程中需要对权重进行去量化，延迟略有增加。具有8位量化感知训练（QAT）的模型在所需磁盘空间上减少了$4\times$，且比基准浮点模型快$1.64\times$。'
- en: '| Model Architecture | Quantization Type | Top-1 Accuracy | Size (MB) | Latency
    (ms, Pixel2) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 模型架构 | 量化类型 | Top-1 准确度 | 大小 (MB) | 延迟 (ms, Pixel2) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: '| MobileNet v2-1.0 (224) | Baseline | 71.9% | 14 | 89 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '| Post-Training Quantization | 63.7% | 3.6 | 98 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| Quantization-Aware Training | 70.9% | 3.6 | 54 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: Table 2\. A sample of various quantization results on the MobileNet v2 architecture
    for 8-bit quantization (TensorFlow, [2021](#bib.bib151)). We picked results on
    8-bit, since from they can be readily used with hardware and software that exists
    today.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2d9889535c7e3fe8c6c2e8f54c07c036.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: (a) Quantization-Aware Training
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/292b3ab51f8eec3f51e4b46e10985788.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: (b) Final fixed-point inference graph
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6\. (a) shows the injection of fake-quantization nodes to simulate quantization
    effect and collecting tensor statistics, for exporting a fully fixed-point inference
    graph. (b) shows the inference graph derived from the same graph as (a). Inputs
    and weights are in uint8, and results of common operations are in uint32. Biases
    are kept in uint32 (Jacob et al., [2018](#bib.bib83); Krishnamoorthi, [2018](#bib.bib91))
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussion:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantization is a well-studied technique for model optimization and can help
    with very significant reduction in model size (often $4\times$ when using 8-bit
    quantization) and inference latency.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight quantization is straight-forward enough that it can be implemented by
    itself for reducing model size. Activation quantization should be strongly considered
    because it enables both latency reduction, as well as lower working memory required
    for intermediate computations in the model (which is essential for devices with
    low memory availability)
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When possible, Quantization-Aware Training should be used. It has been shown
    to dominate post-training quantization in terms of accuracy.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, tools like Tensorflow Lite have made it easy to rely on post-training
    quantization. (TensorFlow, [2019](#bib.bib150)) shows that often there is minimal
    loss when using post-training quantization, and with the help of a representative
    dataset this is further shrunk down. Wherever there is an opportunity for switching
    to fixed-point operations, the infrastructure allows using them.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For performance reasons, it is best to consider the common operations that follow
    a typical layer such as Batch-Norm, Activation, etc. and ‘fold’ them in the quantization
    operations.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.1.3\. Other Compression Techniques
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are other compression techniques like Low-Rank Matrix Factorization, K-Means
    Clustering, Weight-Sharing etc. which are also actively being used for model compression
    (Panigrahy, [2021](#bib.bib118)) and might be suitable for further compressing
    hotspots in a model.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Learning Techniques
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning techniques try to train a model differently in order to obtain better
    quality metrics (accuracy, F1 score, precision, recall, etc.) while allowing supplementing,
    or in some cases replacing the traditional supervised learning. The improvement
    in quality can sometimes be traded off for a smaller footprint by reducing the
    number of parameters / layers in the model and achieving the same baseline quality
    with a smaller model. An incentive of paying attention to learning techniques
    is that they are applied only on the training, without impacting the inference.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. Distillation
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ensembles are well known to help with generalization (Krogh and Vedelsby, [1994](#bib.bib94);
    Hansen and Salamon, [1990](#bib.bib72)). The intuition is that this enables learning
    multiple independent hypotheses, which are likely to be better than learning a
    single hypothesis. (Dietterich, [2000](#bib.bib49)) goes over some of the standard
    ensembling methods such as bagging (learning models that are trained on non-overlapping
    data and then ensembling them), boosting (learning models that are trained to
    fix the classification errors of other models in the ensemble), averaging (voting
    by all the ensemble models), etc.Bucila et al. (Buciluǎ et al., [2006](#bib.bib28))
    used large ensembles to label synthetic data that they generated using various
    schemes. A smaller neural net is then trained to learn not just from the labeled
    data but also from this weakly labeled synthetic data. They found that single
    neural nets were able to mimic the performance of larger ensembles, while being
    $1000\times$ smaller and faster. This demonstrated that it is possible to transfer
    the cumulative knowledge of ensembles to a single small model. Though it might
    not be sufficient to rely on just the existing labeled data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Hinton et al. (Hinton et al., [2015](#bib.bib76)), in their seminal work explored
    how smaller networks (students) can be taught to extract ‘dark knowledge’ from
    larger models / ensembles of larger models (teachers) in a slightly different
    manner. Instead of having to generate synthetic-data, they use the larger teacher
    model to generate *soft-labels* on existing labeled data. The soft-labels assign
    a probability to each class, instead of hard binary values in the original data.
    The intuition is that these soft-labels capture the relationship between the different
    classes which the model can learn from. For example, a truck is more similar to
    a car than to an apple, which the model might not be able to learn directly from
    hard labels.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'The student network learns to minimize the cross-entropy loss on these soft
    labels, along with the original ground-truth hard labels. Since the probabilities
    of the incorrect classes might be very small, the logits are scaled down by a
    ‘temperature’ value $\geq 1.0$, so that the distribution is ‘softened’. If the
    input vector is $\mathbf{X}$, and the teacher model’s logits are $\mathbf{Z^{(t)}}$,
    the teacher model’s softened probabilities with temperature $T$ can be calculated
    as follows using the familiar softmax function:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\small\mathbf{Y}_{i}^{(t)}=\frac{\displaystyle\exp(\mathbf{Z_{i}^{(t)}}/T)}{\displaystyle\sum_{j=1}^{n}\exp(\mathbf{Z_{j}^{(t)}}/T)}$
    |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: Note that as $T$ increases, the relative differences between the various elements
    of $Y^{(t)}$ decreases. This happens because if all elements are divided by the
    same constant, the softmax function would lead to a larger drop for the bigger
    values. Hence, as the temperature $T$ increases, we see the distribution of $Y^{(t)}$
    ‘soften’ further.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'When training along with labeled data ($\mathbf{X}$, $\mathbf{Y}$), and the
    student model’s output ($\mathbf{Y^{(s)}}$), we can describe the loss function
    as:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $\small\begin{split}L&amp;=\lambda_{1}\cdot L_{\rm ground-truth}+\lambda_{2}\cdot
    L_{\rm distillation}\\ &amp;=\lambda_{1}\cdot\textrm{CrossEntropy}(\mathbf{Y},\mathbf{Y^{(s)}};\theta)+\lambda_{2}\cdot\textrm{CrossEntropy}(\mathbf{Y^{(t)}},\mathbf{Y^{(s)}};\theta)\end{split}$
    |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: 'CrossEntropy is the cross-entropy loss function, which takes in the labels
    and the output. For the first loss term, we pass along the ground truth labels,
    and for the second loss term we pass the corresponding soft labels from the teacher
    model for the same input. $\lambda_{1}$ and $\lambda_{2}$ control the relative
    importance of the standard ground truth loss and the distillation loss respectively.
    When $\lambda_{1}=0$, the student model is trained with just the distillation
    loss. Similarly, when $\lambda_{2}=0$, it is equivalent to training with just
    the ground-truth labels. Usually, the teacher network is pre-trained and frozen
    during this process, and only the student network is updated. Refer to Figure
    [7](#S3.F7 "Figure 7 ‣ 3.2.1\. Distillation ‣ 3.2\. Learning Techniques ‣ 3\.
    Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better") for an illustration of this
    process.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/88ff33534ccce386d9eb2316a948b12a.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. Distillation of a smaller student model from a larger pre-trained
    teacher model. Both the teacher and student models receive the same input. The
    teacher is used to generate ‘soft-labels’ for the student, which gives the student
    more information than just hard binary labels. The student is trained using the
    regular cross-entropy loss with the hard labels, as well as using the distillation
    loss function which uses the soft labels from the teacher. In this setting, the
    teacher is frozen, and only the student receives the gradient updates.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, Hinton et al. (Hinton et al., [2015](#bib.bib76)) were able to
    closely match the accuracy of a 10 model ensemble for a speech recognition task
    with a single distilled model. Urban et al. (Urban et al., [2016](#bib.bib153))
    did a comprehensive study demonstrating that distillation significantly improves
    performance of shallow student networks as small as an MLP with one hidden layer
    on tasks like CIFAR-10\. Sanh et al. (Sanh et al., [2019](#bib.bib135)) use the
    distillation loss for compressing a BERT (Devlin et al., [2018](#bib.bib48)) model
    (along with a cosine loss that minimizes the cosine distance between two internal
    vector representation of the input as seen by the teacher and student models).
    Their model retains 97% of the performance of BERT-Base while being 40% smaller
    and 60% faster on CPU.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to adapt the general idea of distillation to work on intermediate
    outputs of teachers and students. Zagoruyko et al. (Zagoruyko and Komodakis, [2016](#bib.bib166))
    transfer intermediate ‘attention maps’ between teacher and student convolutional
    networks. The intuition is to make the student focus on the parts of the image
    where the teacher is paying attention to. MobileBERT (Sun et al., [2020](#bib.bib145))
    uses a progressive-knowledge transfer strategy where they do layer-wise distillation
    between the BERT student and teacher models, but they do so in stages, where the
    first $l$ layers are distilled in the $l$-th stage. Along with other architecture
    improvements, they obtain a 4.3$\times$ smaller and 5.5$\times$ faster BERT with
    small losses in quality.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Another idea that has been well explored is exploiting a model trained in a
    supervised training to label unlabeled data. Blum et al. (Blum and Mitchell, [1998](#bib.bib25))
    in their paper from 1998, report halving the error rate of their classifiers by
    retraining on a subset of pseudo-labels generated using the previous classifiers.
    This has been extended through distillation to use the teacher model to label
    a large corpus of unlabeled data, which can then be used to improve the quality
    of the student model (Menghani and Ravi, [2019](#bib.bib110); Xie et al., [2020](#bib.bib162);
    Yalniz et al., [2019](#bib.bib163)).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Overall, distillation has been empirically shown to improve both the accuracy
    as well as the speed of convergence of student models across many domains. Hence,
    it enables training smaller models which might otherwise not be have an acceptable
    quality for deployment.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussion:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distillation is an adaptable technique that needs minimal changes in the training
    infrastructure to be used. Even if the teacher model cannot be executed at the
    same time as the student model, the teacher model’s predictions can be collected
    offline and treated as another source of labels.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When there is sufficient label data, there is ample evidence that distillation
    is likely to improve the student model’s predictions. If there is a large corpus
    of unlabeled data, the teacher model can be used to generate pseudo-labels on
    the unlabeled data, which can further improve the student model’s accuracy.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strategies for intermediate-layer distillation have also shown to be effective
    in the case of complex networks. In such scenarios, a new loss term minimizing
    the difference between the outputs of the two networks at some semantically identical
    intermediate point(s) needs to be added.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2.2\. Data Augmentation
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When training large models for complex tasks in a supervised learning regime,
    the size of the training data corpus correlates with improvement in generalization.
    (Sun et al., [2017](#bib.bib144)) demonstrates logarithmic increase in the prediction
    accuracy with increase in the number of labeled examples. However, getting high-quality
    labeled data often requires a human in the loop and could be expensive.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Data Augmentation is a nifty way of addressing the scarcity of labeled data,
    by synthetically inflating the existing dataset through some *augmentation methods*.
    These augmentation methods are transformations that can be applied cheaply on
    the given examples, such that the new label of the augmented example does not
    change, or can be cheaply inferred. As an example, consider the classical image
    classification task of labeling a given image to be a cat or a dog. Given an image
    of a dog, translating the image horizontally / vertically by a small number of
    pixels, rotating it by a small angle, etc. would not materially change the image,
    so the transformed image should still be labeled as ‘dog’ by the classifier. This
    forces the classifier to learn a robust representation of the image that generalizes
    better across these transformations.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: The transformations as described above have long been demonstrated to improve
    accuracy of convolutional networks (Simard et al., [2003](#bib.bib141); Cireşan
    et al., [2011](#bib.bib37)). They have also been a core part of seminal works
    in Image Classification. A prime example is AlexNet (Krizhevsky et al., [2012](#bib.bib93)),
    where such transformations were used to increase the effective size of the training
    dataset by 2048 $\times$, which won the ImageNet competition in 2012\. Since then
    it has became common to use such transformations for Image Classification models
    (Inception (Szegedy et al., [2015](#bib.bib147)), XCeption (Chollet, [2017](#bib.bib35)),
    ResNet (He et al., [2016](#bib.bib74)), etc.).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'We can categorize data-augmentation methods as follows (also refer to Figure
    [8](#S3.F8 "Figure 8 ‣ 3.2.2\. Data Augmentation ‣ 3.2\. Learning Techniques ‣
    3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on
    Making Deep Learning Models Smaller, Faster, and Better")):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Label-Invariant Transformations: These are some of the most common transformations,
    where the transformed example retains the original label. These can include simple
    geometric transformations such as translation, flipping, cropping, rotation, distortion,
    scaling, shearing, etc. However the user has to verify the label-invariance property
    with each transformation for the specific task at hand.'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Label-Mixing Transformations: Transformations such as Mixup (Zhang et al.,
    [2017](#bib.bib167)), mix inputs from two different classes in a weighted manner
    and treat the label to be a correspondingly weighted combination of the two classes
    (in the same ratio). The intuition is that the model should be able to extract
    out features that are relevant for both the classes. Other transformations like
    Sample Pairing also seem to help (Inoue, [2018](#bib.bib82)).'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data-Dependent Transformations: In this case, transformations are chosen such
    that they maximize the loss for that example (Fawzi et al., [2016](#bib.bib57)),
    or are adversarially chosen so as to fool the classifier (Gopalan et al., [2021](#bib.bib68)).'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Synthetic Sampling: These methods synthetically create new training examples.
    Algorithms like SMOTE (Chawla et al., [2002](#bib.bib31)) allow re-balancing the
    dataset to make up for skew in the datasets, and GANs can be used to synthetically
    create new samples (Zhu and Gupta, [2018](#bib.bib168)) to improve model accuracy.'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Composition of Transformations: These are transformations that are themselves
    composed of other transformations, and the labels are computed depending on the
    nature of transformations that stacked.'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4eeb88bff5b03fad44b18c8bbfc30034.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8\. Some common types of data augmentations. Source: (Li, [2020](#bib.bib103))'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '| Transformation | Validation Accuracy Improvement (%) |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| rotate | 1.3 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| shear-x | 0.9 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| shear-y | 0.9 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| translate-x | 0.4 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| translate-y | 0.4 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| sharpness | 0.1 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| autoContrast | 0.1 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: 'Table 3\. A breakdown of the contribution of various transformations on the
    validation accuracy of a model trained on the CIFAR-10 dataset. Source: (Cubuk
    et al., [2020](#bib.bib45)).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussion: Apart from Computer Vision, Data-Augmentation has also been used
    in NLP, and Speech. In NLP, a common idea that has been used is ‘back-translation’
    (Yu et al., [2018](#bib.bib164)) where augmented examples are created by training
    two translation models, one going from the source language to the target language,
    and the other going back from the target language to the original source language.
    Since the back-translation is not exact, this process is able to generate augmented
    samples for the given input. Other methods like WordDropout (Sennrich et al.,
    [2016](#bib.bib140)) stochastically set embeddings of certain words to zero. SwitchOut
    (Wang et al., [2018](#bib.bib159)) introduces a similarity measure to disallow
    augmentations that are too dissimilar to the original input. In Speech (Hannun
    et al., [2014](#bib.bib71)), the input audio samples are translated to the left
    / right before being passed to the decoder.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: While the augmentation policies are usually hand-tuned, there are also methods
    such as AutoAugment (Cubuk et al., [2019](#bib.bib44)) where the augmentation
    policy is learned through a Reinforcement-Learning (RL) based search, searching
    for the transformations to be applied, as well as their respective hyper-parameters.
    Though this is shown to improve accuracy, it is also complicated and expensive
    to setup a separate search for augmentation, taking as many as 15000 GPU hours
    to learn the optimal policy on ImageNet. The RandAugment (Cubuk et al., [2020](#bib.bib45))
    paper demonstrated that it is possible to achieve similar results while reducing
    the search space to just two hyper-parameters (number of augmentation methods,
    and the strength of the distortion) for a given model and dataset.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we see that data-augmentation leads to better generalization of the
    given models. Some techniques can be specific for their domains RandAugment (Vision),
    BackTranslation and SwitchOut (NLP), etc. However, the core principles behind
    them make it likely that similar methods can be derived for other domains too
    (refer to our categorization of data-augmentation methods above).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3\. Self-Supervised Learning
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Supervised-Learning paradigm relies heavily on labeled data. As mentioned
    earlier, it requires human intervention, and is expensive as well. To achieve
    reasonable quality on a non-trivial task, the amount of labeled data requires
    is large too. While techniques like Data-Augmentation, Distillation etc., help,
    they too rely on the presence of some labeled data to achieve a baseline performance.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-Supervised learning (SSL) avoids the need for labeled data to learn generalized
    representations, by aiming to extract more supervisory bits from each example.
    Since it focuses on learning robust representations of the example itself, it
    does not need to focus narrowly on the label. This is typically done by solving
    a *pretext task* where the model pretends that a part / structure of the input
    is missing and learns to predict it (Refer to Figure [9](#S3.F9 "Figure 9 ‣ 3.2.3\.
    Self-Supervised Learning ‣ 3.2\. Learning Techniques ‣ 3\. Landscape of Efficient
    Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better") for examples). Since unlabeled data is vast in many
    domains (Books, Wikipedia, and other text for NLP, Web Images & Videos for Computer
    Vision, etc.), the model would not be bottlenecked by data for learning to solve
    these pretext tasks.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/99ebb4d4aa1ba09a1925a9e674e1a23d.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9\. General theme of pretext tasks. Source: (LeCun, [2018](#bib.bib97))'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Once the models learn generic representations that transfer well across tasks,
    they can be adapted to solve the target task by adding some layers that project
    the representation to the label space, and fine-tuning the model with the labeled
    data. Since the labeled data is not being used for learning rudimentary features,
    but rather how to map the high-level representations into the label space, the
    quantum of labeled data is going to be a fraction of what would have been required
    for training the model from scratch. From this lens, fine-tuning models pre-trained
    with Self-Supervised learning are *data-efficient* (they converge faster, attain
    better quality for the same amount of labeled data when compared to training from
    scratch, etc.) ((Howard and Ruder, [2018](#bib.bib79); Devlin et al., [2018](#bib.bib48))).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e5578f99c309d06d7e9ba218bdb137f2.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10\. Validation Error w.r.t. number of training examples for different
    training methods on IMDb (from scratch, ULMFiT Supervised: pre-training with WikiText-103
    and fine-tuning using labeled data, ULMFit Semi-Supervised: Pre-Training with
    WikiText-103 as well as unlabeled data from the target dataset and fine-tuning
    with labeled data). Source: (Howard and Ruder, [2018](#bib.bib79))'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of this two step process of pre-training on unlabeled data and fine-tuning
    on labeled data has gained rapid acceptance in the NLP community. ULMFiT (Howard
    and Ruder, [2018](#bib.bib79)) pioneered the idea of training a general purpose
    language model, where the model learns to solve the pretext task of predicting
    the next word in a given sentence, without the neeof an associated label. The
    authors found that using a large corpus of preprocessed unlabeled data such as
    the WikiText-103 dataset (derived from English Wikipedia pages) was a good choice
    for the pre-training step. This was sufficient for the model to learn general
    properties about the language, and the authors found that fine-tuning such a pre-trained
    model for a binary classification problem (IMDb dataset) required only 100 labeled
    examples ($\approx 10\times$ less labeled examples otherwise). Refer to Figure
    [10](#S3.F10 "Figure 10 ‣ 3.2.3\. Self-Supervised Learning ‣ 3.2\. Learning Techniques
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better"). If we add a middle-step
    of pre-training using unlabeled data from the same target dataset, the authors
    report needing $\approx 20\times$ fewer labeled examples.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: This idea of pre-training followed by fine-tuning is also used in BERT (Devlin
    et al., [2018](#bib.bib48)) (and other related models like GPT, RoBERTa, T5, etc.)
    where the pre-training steps involves learning to solve two tasks. Firstly, the
    Masked Language Model where about 15% of the tokens in the given sentence are
    masked and the model needs to predict the masked token. The second task is, given
    two sentences $A$ and $B$, predict if $B$ follows $A$. The pre-training loss is
    the mean of the losses for the two tasks. Once pre-trained the model can then
    be used for classification or seq2seq tasks by adding additional layers on top
    of the last hidden layer. When it was published, BERT beat the State-of-the-Art
    on eleven NLP tasks.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to NLP, the pretext tasks in Vision have been used to train models
    that learn general representations. (Doersch et al., [2015](#bib.bib50)) extracts
    two patches from a training example and then trains the model to predict their
    relative position in the image (Refer to Figure [11](#S3.F11 "Figure 11 ‣ 3.2.3\.
    Self-Supervised Learning ‣ 3.2\. Learning Techniques ‣ 3\. Landscape of Efficient
    Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better")(a)). They demonstrate that using a network pre-trained
    in this fashion improves the quality of the final object detection task, as compared
    to randomly initializing the network. Similarly, another task is to predict the
    degree of rotation for a given rotated image (Gidaris et al., [2018](#bib.bib60)).
    The authors report that the network trained in a self-supervised manner this way
    can be fine-tuned to perform nearly as well as a fully supervised network.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb75dc500ca9d5a594d1ee592301c566.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: '(a) Detecting relative order of patches. Source: (Doersch et al., [2015](#bib.bib50)).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5133d594f68ecb7b1e044476c74a2b8c.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: (b) Predicting the degree of rotation of a given image.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11\. Pretext tasks for vision problems.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Another common theme is Contrastive Learning, where the model is trained to
    distinguish between similar and dissimilar inputs. Frameworks such as SimCLR (Chen
    et al., [2020a](#bib.bib33); Chen et al., [2020b](#bib.bib34)), try to learn representations
    $h_{i}$ and $h_{j}$ for two given inputs $\tilde{x_{i}}$ and $\tilde{x_{j}}$,
    where the latter two are differently augmented views of the same input, such that
    the cosine similarity of the projections of $h_{i}$ and $h_{j}$, $z_{i}$ and $z_{j}$
    (using a separate function $g(.)$) can be maximized. Similarly, for dissimilar
    inputs the cosine similarity of $z_{i}$ and $z_{j}$ should be minimized. The authors
    report a Top-1 accuracy of $73.9\%$ on ImageNet with only 1% labels (13 labels
    per class), and outperform the ResNet-50 supervised baseline with only 10% labels.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8a2e40f8d451474d2e3ac2bc15a97f43.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12\. SimCLR framework for learning visual representations. Source: (Chen
    et al., [2020a](#bib.bib33))'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussion: Self-Supervised Learning (SSL) has demonstrated significant success
    in the general representational learning with unlabeled data, followed by fine-tuning
    to adapt the model to the target task with a modest number of labeled examples.
    Yann LeCun has likened Self-Supervision as the cake, and Supervised Learning as
    the icing on top (LeCun, [2018](#bib.bib97)), implying that SSL will be the primary
    way of training high-quality models in the future as we move beyond tasks where
    labeled data is abundant.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: With unlabeled data being practically limitless, SSL’s success is dependent
    on creating useful pretext tasks for the domain of interest. As demonstrated across
    NLP (Howard and Ruder, [2018](#bib.bib79); Devlin et al., [2018](#bib.bib48)),
    Vision (Chen et al., [2020a](#bib.bib33); Patrick et al., [2020](#bib.bib121)),
    Speech (Glass, [2012](#bib.bib61)), etc., Self-Supervision is indeed not just
    helpful in speeding and improving convergence, but also enabling achieving high
    quality in tasks where it was intractable to get enough labeled samples.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Practically, for someone training Deep Learning models on a custom task (say
    a speech recognition model for a remote African dialect), having a pre-trained
    checkpoint of a model trained in a self-supervised fashion (such as wav2vec (Baevski
    et al., [2020](#bib.bib21)), which pre-trained in a similar way to BERT (Devlin
    et al., [2018](#bib.bib48))), enables them to only spend an extremely tiny fraction
    of resources on both data labeling, as well as training to fine-tune to a good
    enough quality. In some cases, such as SimCLR (Chen et al., [2020b](#bib.bib34)),
    SSL approaches have actually beaten previous supervised baselines with sophisticated
    models like ResNet-50\. Hence, we are hopeful that SSL methods will be crucial
    for ML practitioners for training high-quality models cheaply.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Automation
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is possible to delegate some of the work around efficiency to automation,
    and letting automated approaches search for ways of training more efficient models.
    Apart from reducing work for humans, it also lowers the bias that manual decisions
    might introduce in model training, apart from systematically and automatically
    looking for optimal solutions. The trade-off is that these methods might require
    large computational resources, and hence have to be carefully applied.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1\. Hyper-Parameter Optimization (HPO)
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the commonly used methods that fall under this category is Hyper-Parameter
    Optimization (HPO) (Yu and Zhu, [2020](#bib.bib165)). Hyper-parameters such as
    initial learning rate, weight decay, etc. have to be carefully tuned for faster
    convergence (Jeremy Jordan, [2020](#bib.bib86)). They can also decide the network
    architecture such as the number of fully connected layers, number of filters in
    a convolutional layer, etc.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Experimentation can help us build an intuition for the *range* in which these
    parameters might lie, but finding the best values requires a search for the exact
    values that optimize the given objective function (typically the loss value on
    the validation set). Manually searching for these quickly becomes tedious with
    the growth in the number of hyper-parameters and/or their possible values. Hence,
    let us explore possible algorithms for automating the search. To formalize this,
    let us assume without the loss of generalization, that we are optimizing the loss
    value on the given dataset’s validation split. Then, let $\mathcal{L}$ be the
    loss function, $f$ be the model function that is learnt with the set of hyper-parameters
    ($\lambda$), $x$ be the input, and $\theta$ be the model parameters. With the
    search, we are trying to find $\lambda^{*}$ such that,
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\small\lambda^{*}=\operatorname*{argmin}_{\lambda\in\Lambda}\mathcal{L}(f_{\lambda}(x;\theta),y)$
    |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '$\Lambda$ is the set of all possible hyper-parameters. In practice, the $\Lambda$
    can be a very large set containing all possible combinations of the hyper-parameters,
    which would often be intractable since hyper-parameters like learning rate are
    real-valued. A common strategy is to approximate $\Lambda$ by picking a finite
    set of *trials*, $S=\{\lambda^{(1)},\lambda^{(2)},...,\lambda^{(n)}\}$, such that
    $S\in\Lambda$, and then we can approximate Equation ([6](#S3.E6 "In 3.3.1\. Hyper-Parameter
    Optimization (HPO) ‣ 3.3\. Automation ‣ 3\. Landscape of Efficient Deep Learning
    ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster,
    and Better")) with:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $\small\lambda^{*}\approx\operatorname*{argmin}_{\lambda\in\{\lambda^{(1)},...,\lambda^{(n)}\}}\mathcal{L}(f_{\lambda}(x;\theta),y)$
    |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: As we see, the choice of $S$ is crucial for the approximation to work. The user
    has to construct a range of reasonable values for each hyper-parameter $\lambda_{i}\in\lambda$.
    This can be based on prior experience with those hyper-parameters.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: A simple algorithm for automating HPO is Grid Search (also referred to as Parameter
    Sweep), where $S$ consists of all the distinct and valid combinations of the given
    hyper-parameters based on their specified ranges. Each trial can then be run in
    parallel since each trial is independent of the others, and the optimal combination
    of the hyper-parameters is found once all the trials have completed. Since this
    approach tries all possible combinations, it suffers from the *curse of dimensionality*,
    where the total number of trials grow very quickly.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach is Random Search where trials are sampled randomly from the
    search space (Bergstra and Bengio, [2012](#bib.bib24)). Since each trial is independent
    of the others, it can still be executed randomly. However, there are few critical
    benefits of Random Search:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the trials are i.i.d. (not the case for Grid Search), the resolution of
    the search can be changed on-the-fly (if the computational budget has changed,
    or certain trials have failed).
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Likelihood of finding the optimal $\lambda^{*}$ increases with the number of
    trials, which is not the case with Grid Search.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there are $K$ real-valued hyper-parameters, and $N$ total trials, grid search
    would pick $N^{\frac{1}{K}}$ for each hyper-parameter. However, not all hyper-parameters
    might be important. Random Search picks a random value for each hyper-parameter
    per trial. Hence, in cases with low effective dimensionality of the search space,
    Random Search performs better than Grid Search.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9032f6169c971fb06443a5b21db78c71.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: (a) Grid Search
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/324cf46f9f81b4d3202e401f115c8c94.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: (b) Random Search
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/320d5fe8693b77c73055ab4017fb0126.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: (c) Bayesian Optimization
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13\. Hyper-Parameter Search algorithms. Source: (Contributors to Wikimedia
    projects, [2021c](#bib.bib40))'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Optimization (BO) based search (Močkus, [1975](#bib.bib112); Agnihotri
    and Batra, [2020](#bib.bib3)) is a *model-based* sequential approach where the
    search is guided by actively estimating the value of the objective function at
    different points in the search space, and then spawning trials based on the information
    gathered so far. The estimation of the objective function is done using a *surrogate
    function* that starts off with a prior estimate. The trials are created using
    an *acquisition function* which picks the next trial using the surrogate function,
    the likelihood of improving on the optimum so far, whether to explore / exploit
    etc. As the trials complete, both these functions will refine their estimates.
    Since the method keeps an internal model of how the objective function looks and
    plans the next trials based on that knowledge, it is model-based. Also, since
    the selection of trials depends on the results of the past trials, this method
    is sequential. BO improves over Random Search in that the search is guided rather
    than random, thus fewer trials are required to reach the optimum. However, it
    also makes the search sequential (though it is possible to run multiple trials
    in parallel, overall it will lead to some wasted trials).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: One of the strategies to save training resources with the above search algorithms
    is the Early Stopping of trials that are not promising. Google’s Vizier (Golovin
    et al., [2017](#bib.bib62)) uses Median Stopping Rule for early stopping, where
    a trial is terminated if it’s performance at a time step $t$ is below the the
    median performance of all trials run till that point of time.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'Other algorithms for HPO include:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Population Based Training (PBT) (Jaderberg et al., [2017](#bib.bib84)): This
    method is similar to evolutionary approaches like genetic algorithms, where a
    fixed number of trials (referred to as the population) are spawned and trained
    to convergence. Each trial starts with a random set of hyper-parameters, and trained
    to a pre-determined number of steps. At this point, all trials are paused, and
    every trial’s weights and parameters might be replaced by the weights and parameters
    from the ‘best’ trial in the population so far. This is the *exploitation* part
    of the search. For *exploration*, these hyper-parameters are perturbed from their
    original values. This process repeats till convergence. It combines both the search
    and training in a fixed number of trials that run in parallel. It also only works
    with adaptive hyper-parameters like learning rate, weight-decay, etc. but cannot
    be used where hyper-parameters change the model structure. Note that the criteria
    for picking the ‘best’ trial does not have to be differentiable.'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Multi-Armed Bandit Algorithms: Methods like Successive Halving (SHA) (Jamieson
    and Talwalkar, [2016](#bib.bib85)) and Hyper-Band (Li et al., [2017](#bib.bib102))
    are similar to random search, but they allocate more resources to the trials which
    are performing well. Both these methods need the user to specify the total computational
    budget $B$ for the search (can be the total number of epochs of training, for
    instance). They then spawn and train a fixed number of trials with randomly sampled
    hyper-parameters while allocating the training budget. Once the budget is exhausted,
    the worse performing fraction ($\frac{\eta-1}{\eta}$) of the trials are eliminated,
    and the remaining trials’ new budget is multiplied by $\eta$. In the case of SHA,
    $\eta$ is 2, so the bottom $\frac{1}{2}$ of the trials are dropped, and the training
    budget for the remaining trials is doubled. For Hyper-Band $\eta$ is 3 or 4\.
    Hyper-Band differs from SHA in that the user does not need to specify the maximum
    number of parallel trials, which introduces a trade-off between the total budget
    and the per-trial allocation.'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'HPO Toolkits: There are several software toolkits that incorporate HPO algorithms
    as well as an easy to use interface (UI, as well as a way to specify the hyper-parameters
    and their ranges). Vizier (Golovin et al., [2017](#bib.bib62)) (an internal Google
    tool, also available via Google Cloud for blackbox tuning). Amazon offers Sagemaker
    (Perrone et al., [2020](#bib.bib123)) which is functionally similar and can also
    be accessed as an AWS service. NNI (Research, [2019](#bib.bib132)), Tune (Liaw
    et al., [2018](#bib.bib104)), Advisor (Chen, [2021](#bib.bib32)) are other open-source
    HPO software packages that can be used locally.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Neural Architecture Search (NAS)
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Neural Architecture Search can be thought of an extension of Hyper-Parameter
    Optimization wherein we are searching for parameters that change the network architecture
    itself.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'We find that there is consensus in the literature (Elsken et al., [2019](#bib.bib54))
    around categorizing NAS as a system comprising of the following parts:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Search Space: These are the operations that are allowed in the graph (Convolution
    ($1\times 1,3\times 3,5\times 5$), Fully Connected, Pooling, etc.), as well as
    the semantics of how these operations and their outputs connect to other parts
    of the network.'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Search Algorithm & State: This is the algorithm that controls the architecture
    search itself. Typically the standard algorithms that apply in HPO (Grid Search,
    Random Search, Bayesian Optimization, Evolutionary Algorithms), can be used for
    NAS as well. However, using Reinforcement Learning (RL) (Zoph and Le, [2016](#bib.bib169)),
    and Gradient Descent (Liu et al., [2018a](#bib.bib106)) are popular alternatives
    too.'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Evaluation Strategy: This defines how we evaluate a model for fitness. It can
    simply be a conventional metric like validation loss, accuracy, etc. Or it can
    also be a compound metric, as in the case of MNasNet (Tan et al., [2019](#bib.bib148))
    which creates a single custom metric based on accuracy as well as latency.'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2f90de7a225110a02b92535c81cb47e2.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14\. Neural Architecture Search: The controller can be thought of as
    a unit that encodes the search space, the search algorithm itself, and the state
    it maintains (typically the model that helps generate the candidates). The algorithm
    generates candidate models in the search space $S$, and receives an evaluation
    feedback. This feedback is used to update the state, and generate better candidate
    models.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'The user is supposed to either explicitly or implicitly encode the search space.
    Together with the search algorithm, we can view this as a ‘controller’ which generates
    sample candidate networks (Refer to Figure [14](#S3.F14 "Figure 14 ‣ 3.3.2\. Neural
    Architecture Search (NAS) ‣ 3.3\. Automation ‣ 3\. Landscape of Efficient Deep
    Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller,
    Faster, and Better")). The evaluation stage will then train and evaluate these
    candidates for fitness. This fitness value is then passed as feedback to the search
    algorithm, which will use it for generating better candidates. While the implementation
    of each of these blocks vary, this structure is common across the seminal work
    in this area.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'Zoph et. al’s paper from 2016 (Zoph and Le, [2016](#bib.bib169)), demonstrated
    that end-to-end neural network architectures can be generated using Reinforcement
    Learning. In this case, the controller is a Recurrent Neural Network, which generates
    the architectural hyper-parameters of a feed-forward network one layer at a time,
    for example, number of filters, stride, filter size, etc. They also support adding
    skip connections (refer Figure [15](#S3.F15 "Figure 15 ‣ 3.3.2\. Neural Architecture
    Search (NAS) ‣ 3.3\. Automation ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")).
    The network semantics are baked into the controller, so generating a network that
    behaves differently requires changing the controller. Also, training the controller
    itself is expensive (taking 22,400 GPU hours (Zoph et al., [2018](#bib.bib170))),
    since the entire candidate network has to be trained from scratch for a single
    gradient update to happen. In a follow up paper (Zoph et al., [2018](#bib.bib170)),
    they come up with a refined search space where instead of searching for the end-to-end
    architecture, they search for *cells*: A ‘Normal Cell’ that takes in an input,
    processes it, and returns an output of the same spatial dimensions. And a ‘Reduction
    Cell’ that process its input, and returns an output whose spatial dimensions are
    scaled down by a factor of 2\. Each cell is a combination of $B$ blocks. The controller’s
    RNN generates one block at a time, where it picks outputs of two blocks in the
    past, the respective operations to apply on them, and how to combine them into
    a single output. The Normal and Reduction cells are stacked in alternating fashion
    ($N$ Normal cells followed by 1 Reduction cell, where $N$ is tunable) to construct
    an end-to-end network for CIFAR-10 and ImageNet. Learning these cells individually
    rather than learning the entire network seems to improve the search time by 7$\times$,
    when compared to the end-to-end network search in (Zoph and Le, [2016](#bib.bib169)),
    while beating the state-of-the-art in CIFAR-10 at that time.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a6c2fa92561ad575a124d509c7cd571.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15\. A NASNet controller generating the architecture, recursively making
    one decision at a time and generating a single block in the image (making a total
    of 5 decisions). Source: (Zoph et al., [2018](#bib.bib170)).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Other approaches such as evolutionary techniques (Real et al., [2019](#bib.bib131)),
    differentiable architecture search (Liu et al., [2018a](#bib.bib106)), progressive
    search (Liu et al., [2018c](#bib.bib105)), parameter sharing (Pham et al., [2018](#bib.bib124)),
    etc. try to reduce the cost of architecture search (in some cases reducing the
    compute cost to a couple of GPU days instead of thousands of GPU days). These
    are covered in detail in (Elsken et al., [2019](#bib.bib54)).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'While most of the early papers focused on finding the architectures that performed
    best on quality metrics like accuracy, unconstrained by the footprint metrics.
    However, when focusing on efficiency, we are often interested in specific tradeoffs
    between quality and footprint. Architecture Search can help with multi-objective
    searches that optimize for both quality and footprint. MNasNet (Tan et al., [2019](#bib.bib148))
    is one such work. It incorporates the model’s latency on the target device into
    the objective function directly, as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $\small\underset{m}{\operatorname{maximize}}\quad ACC(m)\times\left[\frac{LAT(m)}{T}\right]^{w}$
    |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: Where $m$ is the candidate model, $ACC$ is the accuracy metric, and $LAT$ is
    the latency of the given model on the desired device. $T$ is the target latency.
    $w$ is recommended to be $-0.07$. FBNet (Wu et al., [2019](#bib.bib161)) uses
    a similar approach with a compound reward function that has a weighted combination
    of the loss value on the validation set and the latency. However instead of measuring
    the latency of the candidate model on device, they use a pre-computed lookup table
    to approximate the latency to speed up the search process. They achieve networks
    that are upto $2.4\times$ smaller and $1.5\times$ faster than MobileNet, while
    finishing the search in 216 GPU Hours. Other works such as MONAS (Hsu et al.,
    [2018](#bib.bib80)) use Reinforcement Learning to incorporate power consumption
    into the reward function along with hard constraints on the number of MAC operations
    in the model, and discover pareto-frontiers under the given constraints.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussion: Automation plays a critical role in model efficiency. Hyper-Parameter
    Optimization (HPO) is now a natural step in training models and can extract significant
    quality improvements, while minimizing human involvement. In case the cost HPO
    becomes large, algorithms like Bayesian Optimization, Hyper-Band etc. with early
    stopping techniques can be used. HPO is also available in ready-to-use software
    packages like Tune (Liaw et al., [2018](#bib.bib104)), Vizier via Google Cloud
    (Golovin et al., [2017](#bib.bib62)), NNI (Research, [2019](#bib.bib132)), etc.
    Similarly, recent advances in Neural Architecture Search (NAS) also make it feasible
    to construct architectures in a learned manner, while having constraints on both
    quality and footprint (Tan et al., [2019](#bib.bib148)). Assuming several hundred
    GPU hours worth of compute required for the NAS run to finish, and an approx cost
    of $3 GPU / hour on leading cloud computing services, this makes using NAS methods
    financially feasible and not similar in cost to manual experimentation with model
    architecture when optimizing for multiple objectives.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Efficient Architectures
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another common theme for tackling efficiency problems is to go back to the drawing
    board, and design layers and models that are efficient by design to replace the
    baseline. They are typically designed with some insight which might lead to a
    design that is better in general, or it might be better suited for the specific
    task. In this section, we lay out an examples of such efficient layers and models
    to illustrate this idea.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1\. Vision
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the classical example of efficient layers in the Vision domain are the
    Convolutional layers, which improved over Fully Connected (FC) layers in Vision
    models. FC layers suffer from two primary issues:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: FC layers ignore the spatial information of the input pixels. Intuitively, it
    is hard to build an understanding of the given input by looking at individual
    pixel values in isolation. They also ignore the spatial locality in nearby regions.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Secondly, using FC layers also leads to an explosion in the number of parameters
    when working with even moderately sized inputs. A $100\times 100$ RGB image with
    3 channels, would lead to each neuron in the first layer having $3\times 10^{4}$
    connections. This makes the network susceptible to overfitting also.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Convolutional layers avoid this by learning ‘filters’, where each filter is
    a 3D weight matrix of a fixed size ($3\times 3$, $5\times 5$, etc.), with the
    third dimension being the same as the number of channels in the input. Each filter
    is convolved over the input to generate a feature map for that given filter. These
    filters learn to detect specific features, and convolving them with a particular
    input patch results in a single scalar value that is higher if the feature is
    present in that input patch.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: These learned features are simpler in lower layers (such as edges (horizontal,
    vertical, diagonal, etc.)), and more complex in subsequent layers (texture, shapes,
    etc.). This happens because the subsequent layers use the feature maps generated
    by previous layers, and each pixel in the input feature map of the $i$-th layer,
    depends on the past $i-1$ layers. This increases the *receptive field* of the
    said pixel as $i$ increases, progressively increasing the complexity of the features
    that can be encoded in a filter.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: The core idea behind the efficiency of Convolutional Layers is that the same
    filter is used everywhere in the image, regardless of where the filter is applied.
    Hence, enforcing spatial invariance while sharing the parameters. Going back to
    the example of a $100\times 100$ RGB image with 3 channels, a $5\times 5$ filter
    would imply a total of $75$ ($5\times 5\times 3$) parameters. Each layer can learn
    multiple unique filters, and still be within a very reasonable parameter budget.
    This also has a regularizing effect, wherein a dramatically reduced number of
    parameters allow for easier optimization, and reducing the likelihood of overfitting.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Layers are usually coupled with Pooling Layers, which allow dimensionality
    reduction by subsampling the input (aggregating a sliding 2-D window of pixels,
    using functions like max, avg, etc.). Pooling would lead to smaller feature maps
    for the next layer to process, which makes it faster to process. LeNet5 (Lecun
    et al., [1998](#bib.bib98)) was the first Convolutional Network which included
    convolutional layers, pooling, etc. Subsequently, many iterations of these networks
    have been proposed with various improvements. AlexNet (Krizhevsky et al., [2012](#bib.bib93)),
    Inception (Szegedy et al., [2015](#bib.bib147)), ResNet (He et al., [2016](#bib.bib74)),
    etc. have all made significant improvements over time on known image classification
    benchmarks using Convolutional Layers.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Depth-Separable Convolutional Layers: In the convolution operation, each filter
    is used to convolve over the two spatial dimensions and the third channel dimension.
    As a result, the size of each filter is $s_{x}\times s_{y}\times$ input_channels,
    where $s_{x}$ and $s_{y}$ are typically equal. This is done for each filter, resulting
    in the convolution operation happening both spatially in the $x$ and $y$ dimensions,
    and depthwise in the $z$ dimension.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c2bbc39b0e4bc690ded3bb25e4211b40.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16\. Depth-Separable Convolution. Source: (Tsang, [2019](#bib.bib152)).'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'Depth-separable convolution breaks this into two steps (Refer to Figure [16](#S3.F16
    "Figure 16 ‣ 3.4.1\. Vision ‣ 3.4\. Efficient Architectures ‣ 3\. Landscape of
    Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better")):'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Doing a point-wise convolution with $1\times 1$ filters, such that the resulting
    feature map now has a depth of output_channels.
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Doing a spatial convolution with $s_{x}\times s_{y}$ filters in the $x$ and
    $y$ dimensions.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These two operations stacked together (without any intermediate non-linear activation)
    results in an output of the same shape as a regular convolution, with much fewer
    parameters ($1\times 1\times$input_channels$\times$ output_channels$)+(s_{x}\times
    s_{y}\times$ output_channels$)$, v/s $s_{x}\times s_{y}\times$ input_channels
    $\times$ output_channels for the regular convolution). Similarly there is an order
    of magnitude less computation since the point-wise convolution is much cheaper
    for convolving with each input channel depth-wise (for more calculations refer
    to (Sandler et al., [2018](#bib.bib134))). The Xception model architecture (Chollet,
    [2017](#bib.bib35)) demonstrated that using depth-wise separable convolutions
    in the Inception architecture, allowed reaching convergence sooner in terms of
    steps and a higher accuracy on the ImageNet dataset while keeping the number of
    parameters the same.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: The MobileNet model architecture (Sandler et al., [2018](#bib.bib134)) which
    was designed for mobile and embedded devices, also uses the depth-wise separable
    layers instead of the regular convolutional layers. This helps them reduce the
    number of parameters as well as the number of multiply-add operations by $7-10\times$
    and allows deployment on Mobile for Computer Vision tasks. Users can expect a
    latency between 10-100ms depending on the model. MobileNet also provides a knob
    via the depth-multiplier for scaling the network to allow the user to trade-off
    between accuracy and latency.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2\. Natural Language Understanding
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Attention Mechanism & Transformer Family: One of the issues plaguing classical
    Sequence-to-Sequence (Seq2Seq) models for solving tasks such as Machine Translation
    (MT), was that of the information-bottleneck. Seq2Seq models typically have one
    or more encoder layers which encode the given input sequence ($\mathbf{x}=(x_{1},x_{2},...,x_{T})$)
    into a fixed length vector(s) (also referred to as the context, $\mathbf{c}$),
    and one or more decoder layers which generate another sequence using this context.
    In the case of MT, the input sequence can be a sentence in the source language,
    and the output sequence can be the sentence in the target language.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in classical Seq2Seq models such as (Sutskever et al., [2014](#bib.bib146))
    the decoder layers could only see the hidden state of the final encoder step ($c=h_{T}$).
    This is a *bottleneck* because the encoder block has to squash all the information
    about the sequence in a single context vector for all the decoding steps, and
    the decoder block has to somehow infer the entire encoded sequence from it (Refer
    to Figure [17](#S3.F17 "Figure 17 ‣ 3.4.2\. Natural Language Understanding ‣ 3.4\.
    Efficient Architectures ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")).
    It is possible to increase the size of the context vector, but it would lead to
    an increase in the hidden state of all the intermediate steps, and make the model
    larger and slower.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c07896369b09a5d2acadddef0edfce3b.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
- en: Figure 17\. Information Bottleneck in a Seq2Seq model for translating from English
    to Hindi. The context vector $c$ that the decoder has access to is fixed, and
    is typically the last hidden state ($h_{T}$).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad6d03cd0582600920ae7aaefc36c76e.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18\. Attention module learning a weighted context vector for each output
    token from the hidden states. Source: (Bahdanau et al., [2014](#bib.bib22)).'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: The Attention mechanism was introduced in Bahdanau et al. (Bahdanau et al.,
    [2014](#bib.bib22)) to be able to create a custom context vector for each output
    token, by allowing all hidden states to be visible to the decoder and then creating
    a weighted context vector, based on the output token’s alignment with each input
    token. Essentially, the new weighted context vector is $c_{i}=\sum_{j}^{T}\alpha_{ij}.h_{j}$,
    where $\alpha_{ij}$ is the learned alignment (attention weight) between the decoder
    hidden state $s_{i-1}$ and the hidden state for the $j$-th token ($h_{j}$). $\alpha_{ij}$
    could be viewed as how much attention should the $i$-th input token be given when
    processing the $j$-th input token. This model is generalized in some cases by
    having explicit Query ($Q$), Key ($K$), and Value ($V$) vectors. Where we seek
    to learn the attention weight distribution ($\mathbf{\alpha}$) between $Q$ and
    $K$, and use it to compute the weighted context vector ($\mathbf{c}$) over $V$.
    In the above encoder-decoder architecture, $Q$ is the decoder hidden state $s_{i-1}$,
    and $K=V$ is the encoder hidden state $h_{j}$. Attention has been used to solve
    a variety of NLU tasks (MT, Question Answering, Text Classification, Sentiment
    Analysis), as well as Vision, Multi-Modal Tasks etc. (Chaudhari et al., [2019](#bib.bib30)).
    We refer the reader to (Chaudhari et al., [2019](#bib.bib30)) for further details
    on the taxonomy of attention models.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1ae24c1cc1da5c3815470e059f74cbbe.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19\. Transformer with its Encoder and Decoder blocks. Source: (Alammar,
    [2021](#bib.bib4)).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer architecture (Vaswani et al., [2017](#bib.bib156)) was proposed
    in 2017, which introduced using Self-Attention layers for both the Encoder and
    the Decoder. They demonstrated that Attention layers could be used to replace
    traditional RNN based Seq2Seq models. The Self-Attention layer the query, key,
    and value vectors are all derived from the same sequence by using different projection
    matrices.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Self-Attention also allows parallelizing the process of deriving relationships
    between the tokens in the input sequences. RNNs inherently force the process to
    occur one step at a time, i.e., learning long range dependencies is $O(n)$, where
    $n$ is the number of tokens. With Self-Attention, all tokens are processed together
    and pairwise relationships can be learnt in $O(1)$ (Vaswani et al., [2017](#bib.bib156)).
    This makes it easier to leverage optimized training devices like GPUs and TPUs.
    The authors reported up to $300\times$ less training FLOPs as required to converge
    to a similar quality when compared to other recurrent and convolutional models.
    Tay et al. (Tay et al., [2020](#bib.bib149)) discuss the computation and memory
    efficiency of several Transformer variants and their underlying self-attention
    mechanisms in detail.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: As introduced earlier, the BERT model architecture (Devlin et al., [2018](#bib.bib48))
    beat the state-of-the-art in several NLU benchmarks. BERT is a stack of Transformer
    encoder layers that are pre-trained using a bi-directional masked language model
    training objective. It can also be used as a general purpose encoder which can
    then be used for other tasks. Other similar models like the GPT family (Brown
    et al., [2020](#bib.bib27)) have also been used for solving many NLU tasks.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Random Projection Layers & Models Pre-trained token representations such as
    word2vec (Mikolov et al., [2017](#bib.bib111)), GLoVE (Pennington et al., [2014](#bib.bib122)),
    etc. are common for NLU tasks. However, since they require a $d$-dimensional vector
    for storing each token, the total size consumed by the table quickly grows very
    large if the vocabulary size $V$ is substantial ($O(V.d)$).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: If model size is a constraint for deployment, we can either rely on compression
    techniques (as illustrated earlier) to help with Embedding Table compression,
    or evaluate layers and models that can work around the need for embedding tables.
    Random Projection based methods (Ravi, [2017](#bib.bib129); Ravi and Kozareva,
    [2018](#bib.bib130); Kaliamoorthi et al., [2019](#bib.bib88); Kaliamoorthi et al.,
    [2021](#bib.bib89)) are one such family of models that do so. They propose replacing
    the embedding table and lookup by mapping the input feature $x$ (unicode token
    / word token, etc.), into a lower dimensional space. This is done using the random
    projection operator $\mathbb{P}$, such that $\mathbb{P}(x)\in\{0,1\}^{T.r}$, which
    can be decomposed into $T$ individual projection operations each generating an
    $r$-bit representation ($\mathbb{P}(x)=[\mathbb{P}_{1}(x),...,\mathbb{P}_{T}(x)]$,
    where $\mathbb{P}_{i}(x)\in{0,1}^{r}$). $T$ and $r$ can be manually chosen.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Each random projection operation $\mathbb{P}_{i}$ is implemented using Locality
    Sensitive Hashing (LSH) (Charikar, [2002](#bib.bib29); Ravi and Kozareva, [2018](#bib.bib130)),
    each using a different hash function (via different seeds). For theoretical guarantees
    about the Random Projection operation, refer to (Charikar, [2002](#bib.bib29)),
    which demonstrates that the operation preserves the similarity between two points
    in the lower-dimensional space it maps these points to (this is crucial for the
    model to be learn the semantics about the inputs). If this relationship holds
    in the lower-dimensional space, the projection operation can be used to learn
    discriminative features for the given input. The core-benefit of the projection
    operation when compared to embedding tables is $O(T)$ space required instead of
    $O(V.d)$ ($T$ seeds required for $T$ hash functions). On the other hand, random-projection
    computation is $O(T)$ too v/s $O(1)$ for embedding table lookup. Hence, the projection
    layer is clearly useful when model size is the primary focus of optimization.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Across the various papers in the projection model family, there are subtle differences
    in implementation (computing complex features before ((Ravi and Kozareva, [2018](#bib.bib130)))
    v/s after the projection operation ((Kaliamoorthi et al., [2019](#bib.bib88);
    Sankar et al., [2019](#bib.bib136))), generating a ternary representation instead
    of binary ((Kaliamoorthi et al., [2019](#bib.bib88); Kaliamoorthi et al., [2021](#bib.bib89))),
    applying complex layers and networks on top like Attention ((Kaliamoorthi et al.,
    [2019](#bib.bib88))), QRNN ((Kaliamoorthi et al., [2021](#bib.bib89)))), etc.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f9d2a0056c8713d046cc9a58b29ce7fb.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: '(a) PRADO Model. Source: (Kaliamoorthi et al., [2019](#bib.bib88)).'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2d5a716162080dfe45626291fc3dc837.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: '(b) PQRNN Model. Source: (Kaliamoorthi et al., [2021](#bib.bib89))'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a832eab0563cbc1932b8146fe044ce99.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
- en: '(c) Proformer Model. Source: (Sankar et al., [2020](#bib.bib137)).'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Figure 20\. Collection of notable Random-Projection based models.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the Projection-based models (refer to Figure [20](#S3.F20 "Figure 20
    ‣ 3.4.2\. Natural Language Understanding ‣ 3.4\. Efficient Architectures ‣ 3\.
    Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better")) have demonstrated impressive
    results on NLU tasks. PRADO ((Kaliamoorthi et al., [2019](#bib.bib88))) generates
    n-gram features from the projected inputs, followed by having a Multi-Headed Attention
    layer on top. It achieved accuracies comparable to standard LSTM models, while
    being  $100\times$ smaller, and taking 20-40 ms for inference on a Nexus 5X device.
    PQRNN (Kaliamoorthi et al., [2021](#bib.bib89)), another Projection-based model
    that additionally uses a fast RNN implementation (QRNN) (Bradbury et al., [2016](#bib.bib26))
    on top of the projected features. They report outperforming LSTMs while being
    $140\times$ smaller, and achieving $97.1\%$ of the quality of a BERT-like model
    while being $350\times$ smaller.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Proformer (Sankar et al., [2020](#bib.bib137)) introduces a Local Projected
    Attention (LPA) Layer, which combines the Projection operation with localized
    attention. They demonstrate reaching $\approx$ 97.2% BERT-base’s performance while
    occupying only 13% of BERT-base’s memory. ProFormer also had 14.4 million parameters,
    compared to 110 million parameters of BERT-base.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Infrastructure
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to be able to train and run inference efficiently, there has to be
    a robust software and hardware infrastructure foundation. In this section we go
    over both these aspects. Refer to Figure [21](#S3.F21 "Figure 21 ‣ 3.5\. Infrastructure
    ‣ 3\. Landscape of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") for a mental model
    of the software and hardware infrastructure, and how they interact with each other.
    In this section we provide a non-exhaustive but comprehensive survey of leading
    software and hardware infrastructure components that are critical to model efficiency.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/26638b7b8bbe071083117579c80d76d2.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
- en: Figure 21\. A visualization of the hardware and software infrastructure with
    emphasis on efficiency. On the left hand side is the model-training phase, which
    generates a trained model checkpoint. This model is then used on the inference
    side, which could either be server-side (conventional machines in cloud or on-prem),
    or on-device (mobile phones, IoT, edge devices, etc.).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1\. Tensorflow Ecosystem
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tensorflow (TF) (Abadi et al., [2016](#bib.bib2); Authors, [2021g](#bib.bib15))
    is a popular machine learning framework, that has been used in production by many
    large enterprises. It has some of the most extensive software support for model
    efficiency.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensorflow Lite for On-Device Usecases: Tensorflow Lite (TFLite) (Authors,
    [2021i](#bib.bib17)) is a collection of tools and libraries designed for inference
    in low-resource environments. At a high-level we can break down the TFLite project
    into two core parts:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interpreter and Op Kernels: TFLite provides an interpreter for running specialized
    TFLite models, along with implementations of common neural net operations (Fully
    Connected, Convolution, Max Pooling, ReLu, Softmax, etc. each of which as an *Op*).
    The implementation of such an operation is known as an *Op Kernel*. Both the interpreter
    and Op Kernels are primarily optimized for inference on ARM-based processors as
    of the time of writing this paper. They can also leverage smartphone DSPs such
    as Qualcomm’s Hexagon (Authors, [2021l](#bib.bib20)) for faster execution. The
    interpreter also allows the user to set multiple threads for execution.'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Converter: The TFLite converter as the name suggests is useful for converting
    the given TF model into a single flatbuffer file for inference by the interpreter.
    Apart from the conversion itself, it handles a lot of internal details like getting
    a graph ready for quantized inference, fusing operations, adding other metadata
    to the model, etc. With respect to quantization, it also allows post-training
    quantization as mentioned earlier with an optional representative dataset to improve
    accuracy.'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Other Tools for On-Device Inference: TF Micro (Warden and Situnayake, [2019](#bib.bib160))
    goes further, and consists of a slimmed down interpreter, and a smaller set of
    ops for inference on very low resource microcontrollers. TF Model Optimization
    toolkit (Authors, [2020](#bib.bib13)) is a Tensorflow library for applying common
    compression techniques like quantization, pruning, clustering etc. TensorflowJS
    (TF.JS) is a library within the TF ecosystem that can be used to train and run
    neural networks within the browser or using Node.js (Node.js Authors, [2021](#bib.bib114)).
    These models can also accelerated through GPUs via the WebGL interface (Contributors
    to Wikimedia projects, [2021f](#bib.bib43)). It supports both, importing models
    trained in TF, as well as creating new models from scratch in TF.JS.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'XLA for Server-Side Acceleration: Typically a TF model graph is executed by
    TF’s executor process and it uses standard optimized kernels for running it on
    CPU, GPU, etc. XLA (Authors, [2021j](#bib.bib18)) is a graph compiler that can
    optimize linear algebra computations in a model, by generating new kernels that
    are customized for the graph. These kernels are optimized for the model graph
    in question. For example, certain operations which can be fused together are combined
    in a single composite op. This avoids having to do multiple costly writes to RAM,
    when the operands can directly be operated on while they are still in cheaper
    caches. Kanwar et al. (Kanwar et al., [2021](#bib.bib90)) report a 7$\times$ increase
    in training throughput, and 5$\times$ increase in the maximum batch size that
    can be used for BERT training. This allows training a BERT model for $32 on Google
    Cloud.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2\. PyTorch Ecosystem
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PyTorch (Paszke et al., [2019](#bib.bib120)) is another popular machine-learning
    platform actively used by both academia and industry. It is often compared with
    Tensorflow in terms of usability and features.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'On-Device Usecases: PyTorch also has a light-weight interpreter that enables
    running PyTorch models on Mobile (Authors, [2021c](#bib.bib10)), with native runtimes
    for Android and iOS. This is analogous to the TFLite interpreter and runtime as
    introduced earlier. Similar to TFLite, PyTorch offers post-training quantization
    (Authors, [2021d](#bib.bib11)), and other graph optimization steps such as constant
    folding, fusing certain operations together, putting the channels last (NHWC)
    format for optimizing convolutional layers.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'General Model Optimization: PyTorch also offers the Just-in-Time (JIT) compilation
    facility (Authors, [2021e](#bib.bib12)), which might seem similar to Tensorflow’s
    XLA, but is actually a mechanism for generating a serializable intermediate representation
    (high-level IR, per (Li, [2020](#bib.bib103))) of the model from the code in TorchScript
    (Authors, [2021e](#bib.bib12)), which is a subset of Python. TorchScript adds
    constraints on the code that it can convert, such as type-checks, which allows
    it to sidestep some pitfalls of typical Python programming, while being Python
    compatible. It allows creating a bridge between the flexible PyTorch code for
    research and development, to a representation that can be deployed for inference
    in production. For example, exporting to TorchScript is a requirement to run on
    mobile devices (Authors, [2021c](#bib.bib10)). This representation is analogous
    to the static inference mode graphs generated by TensorFlow. The alternative for
    XLA in the PyTorch world seem to be the Glow (Rotem et al., [2018](#bib.bib133))
    and TensorComprehension (Vasilache et al., [2018](#bib.bib155)) compilers. They
    help in generating the lower-level intermediate representation that is derived
    from the higher-level IR (TorchScript, TF Graph). These low-level deep learning
    compilers are compared in detail in (Li, [2020](#bib.bib103)).'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch offers a model tuning guide (Authors, [2021b](#bib.bib9)), which details
    various options that ML practitioners have at their disposal. Some of the core
    ideas in there are:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turn on mixed-precision training (Authors, [2021a](#bib.bib8)) when using NVIDIA
    GPUs. This is described further in detail in the GPU sub-section in 3.5.4.
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fusion of pointwise-operations (add, subtract, multiply, divide, etc.) using
    PyTorch JIT. Even though this should happen automatically, but adding the torch.jit.script
    decorator to methods which are completely composed of pointwise operations can
    force the TorchScript compiler to fuse them.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling buffer checkpointing allows keeping the outputs of only certain layers
    in memory, and computing the rest during the backward pass. This specifically
    helps with cheap to compute layers with large outputs like activations. A reduced
    memory usage can be exchanged for a larger batch size which improves utilization
    of the training platform (CPU, GPU, TPU, etc.).
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling device-specific optimizations, such as the cuDNN library, and Mixed
    Precision Training with NVIDIA GPUs (explained in the GPU subsection).
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train with Distributed Data Parallel Training, which is suitable when there
    is a large amount of data and multiple GPUs are available for training. Each GPU
    gets its own copy of the model and optimizer, and operates on its own subset of
    the data. Each replicas gradients are periodically accumulated and then averaged.
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.5.3\. Hardware-Optimized Libraries
  id: totrans-357
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can further extract efficiency by optimizing for the hardware the neural
    networks run on. A prime deployment target is ARM’s Cortex-family of processors.
    Cortex supports SIMD (Single-Instruction Multiple Data) instructions via the Neon
    (Ltd., [2021](#bib.bib109)) architecture extension. SIMD instructions are useful
    for operating upon registers with vectors of data, which are essential for speeding
    up linear algebra operations through vectorization of these operations. QNNPACK
    (Dukhan et al., [2020](#bib.bib52)) and XNNPACK (Authors, [2021k](#bib.bib19))
    libraries are optimized for ARM Neon for mobile and embedded devices, and for
    x86 SSE2, AVX architectures, etc. QNNPACK supports several common ops in quantized
    inference mode for PyTorch. XNNPACK supports 32-bit floating point models and
    16-bit floating point for TFLite. If a certain operation isn’t supported in XNNPACK,
    it falls back to the default implementation in TFLite.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, there are other low-level libraries like Accelerate for iOS (Apple
    Authors, [2021](#bib.bib7)), and NNAPI for Android (Android Developers, [2021](#bib.bib5))
    that try to abstract away the hardware-level acceleration decision from higher
    level ML frameworks.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.4\. Hardware
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'GPU: Graphics Processing Units (GPUs) were originally designed for acclerating
    computer graphics, but began to be used for general-purpose usecases with the
    availability of the CUDA library (Contributors to Wikimedia projects, [2021b](#bib.bib39))
    in 2007, and libraries like like cuBLAS for speeding up linear algebra operations.
    In 2009, Raina et al. (Raina et al., [2009](#bib.bib126)) demonstrated that GPUs
    can be used to accelerate deep learning models. In 2012, following the AlexNet
    model’s (Krizhevsky et al., [2012](#bib.bib93)) substantial improvement over the
    next entrant in the ImageNet competition further standardized the use of GPUs
    for deep learning models. Since then Nvidia has released several iterations of
    its GPU microarchitectures with increasing focus on deep learning performance.
    It has also introduced Tensor Cores (NVIDIA, [2020b](#bib.bib116); Stosic, [2020](#bib.bib143))
    which are dedicated execution units in their GPUs, which are specialized for Deep
    Learning applications. TensorCores support training and inference in a range of
    precisions (fp32, TensorFloat32, fp16, bfloat16, int8, int4). As demonstrated
    earlier in quantization, switching to a lower precision is not always a significant
    trade-off, since the difference in model quality might often be minimal.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c17aa85e7f05d74e51e614c19bf6333e.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22\. Reduced Precision Multiply-Accumulate (MAC) operation: An illustration
    of the $\mathbf{A}=(\mathbf{B}\times\mathbf{C})+\mathbf{D}$ operation. $\mathbf{B}$
    and $\mathbf{C}$ are in a reduced precision (fp16, bfloat16, TensorFloat32 etc.),
    while $\mathbf{A}$ and $\mathbf{D}$ are in fp32\. The speedup comes from doing
    the expensive matrix-multiplication with a reduced precision format.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Cores optimize the standard Multiply-and-Accumulate (MAC) operation (Contributors
    to Wikimedia projects, [2021d](#bib.bib41)), $\mathbf{A}=(\mathbf{B}\times\mathbf{C})+\mathbf{D}$.
    Where, $\mathbf{B}$ and $\mathbf{C}$ are in a reduced precision (fp16, bfloat16,
    TensorFloat32), while $\mathbf{A}$ and $\mathbf{D}$ are in fp32\. The core speedup
    comes from doing the expensive matrix-multiplication in a lower-precision. The
    result of the multiplication is in fp32, which can be relatively cheaply added
    with $\mathbf{D}.$ When training with reduced-precision, NVIDIA reports between
    1$\times$ to 15$\times$ training speedup depending on the model architecture and
    the GPU chosen (Stosic, [2020](#bib.bib143)). Tensor Cores in NVidia’s latest
    Ampere architecture GPUs also support faster inference with sparsity (specifically,
    structured sparsity in the ratio 2:4, where 2 elements out of a block of 4 elements
    are sparse) (NVIDIA, [2020a](#bib.bib115)). They demonstrate an up to 1.5$\times$
    speed up in inference time, and up to 1.8$\times$ speedup in individual layers.
    NVIDIA also offers the cuDNN libary (NVIDIA, [2020a](#bib.bib115)) that contains
    optimized versions of standard neural network operations such as fully-connected,
    convolution, batch-norm, activation, etc.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f15154751507a175bc63217988432b1e.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23\. Common floating point format used in Training & Inference: fp32
    is the standard 32-bit floating point number from IEEE-754 standard (Wang and
    Kanwar, [2021](#bib.bib158)). One bit is allocated for storing the sign. The exponent
    controls the range of the floating point value that can be expressed with that
    format, and the mantissa controls the precision. Note that fp16 reduces the precision
    as well as range. The bfloat16 format is a reasonable compromise because it keeps
    the same range as fp32 while trading of precision to take up a total of 16 bits.
    NVidia GPUs also support Tensor Float 32 format that allocates 3 more bits to
    the mantissa than bfloat16 to achieve better precision. However, it takes up a
    total of 19 bits which does not make it a trivially portable format.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'TPU: TPUs are proprietary application-specific integrated circuits (ASICs)
    that Google has designed to accelerate deep learning applications with Tensorflow.
    Because they are not general purpose devices, they need not cater for any non-ML
    applications (which most GPUs have had to), hence they are finely tuned for parallelizing
    and accelerating linear algebra operations. The first iteration of the TPU was
    designed for inference with 8-bit integers, and was being used in Google for a
    year prior to their announcement in 2016 (Jouppi et al., [2017](#bib.bib87)).
    Subsequent iterations of the TPU architectures enabled both training and inference
    with TPUs in floating point too. Google also opened up access to these TPUs via
    their Google Cloud service in 2018 (Google, [2021a](#bib.bib63)).'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9c7047448530aba2f3e565dd6bf4f3cf.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
- en: (a) A Systolic Array Cell implementing a Multiply-Accumulate (MAC) operation.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/237c6a26043b4f432471d9bdf1452fe8.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
- en: (b) 4x4 Matrix Multiplication using Systolic Array
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 24\. Systolic Arrays in TPUs: Figure (a) shows a Systolic Array implementing
    a MAC operation, where the variables $A$ and $B$ are received by the cell, and
    $C$ is the resident memory. $A$ is passed to the horizontally adjacent cell on
    the right, and $B$ is passed to the vertically adjacent cell below on the next
    clock tick. Figure (b) demonstrates how two 4$\times$4 matrices are multiplied
    using Systolic Arrays which is a mesh of cells constructed in Figure (a). The
    $i$-th row of array is fed the $i$-th column of $A$ (preceded by $i-1$ 0s, which
    act as a delay). Similarly, the $i$-th column of the array is fed the $i$-th column
    of $B$ (preceded by $i-1$ 0s). The corresponding $a_{ij}$ and $b_{jk}$ are passed
    to the neighboring cells on the next clock tick.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'The core architecture of the TPU chips leverages the Systolic Array design
    (Kung and Leiserson, [1980](#bib.bib95); Kung, [1982](#bib.bib96)) (refer to Figure
    [24](#S3.F24 "Figure 24 ‣ 3.5.4\. Hardware ‣ 3.5\. Infrastructure ‣ 3\. Landscape
    of Efficient Deep Learning ‣ Efficient Deep Learning: A Survey on Making Deep
    Learning Models Smaller, Faster, and Better")), where a large computation is split
    across a mesh-like topology, where each cell computes a partial result and passes
    it on to the next cell in the order, every clock-step (in a rhythmic manner analogous
    to the systolic cardiac rhythm). Since there is no need to access registers for
    the intermediate results, once the required data is fetched the computation is
    not memory bound. Each TPU chip has two Tensor Cores (not to be confused with
    NVidia’s Tensor Cores), each of which has a mesh of systolic arrays. There are
    4 inter-connected TPU chips on a single TPU board. To further scale training and
    inference, a larger number of TPU boards can be connected in a mesh topology to
    form a ’pod’. As per publicly released numbers, each TPU chip (v3) can achieve
    420 teraflops, and a TPU pod can reach 100+ petaflops (Sato, [2021](#bib.bib138)).'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: TPUs have been used inside Google for applications like training models for
    Google Search, general purpose BERT models (Devlin et al., [2018](#bib.bib48)),
    for applications like DeepMind’s world beating AlphaGo and AlphaZero models (Schrittwieser
    et al., [2020](#bib.bib139)), and many other research applications (Tan et al.,
    [2019](#bib.bib148)). They have also set model training time records in the MLPerf
    benchmarks. Similar to the GPUs, TPUs support the bfloat16 data-type (Wang and
    Kanwar, [2021](#bib.bib158)) which is a reduced-precision alternative to training
    in full floating point 32-bit precision. XLA support allows transparently switching
    to bfloat16 without any model changes.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'EdgeTPU: EdgeTPU is a custom ASIC chip designed by Google for running inference
    on edge devices, with low power requirements (4 Tera Ops / sec (TOPS) using 2
    watts of power (Google, [2021c](#bib.bib65))). Like the TPU, it is specialized
    for accelerating linear algebra operations, but only for inference and with a
    much lower compute budget. It is further limited to only a subset of operations
    (Google, [2021d](#bib.bib66)), and works only with int8 quantized Tensorflow Lite
    models. Google releases the EdgeTPU using the Coral platform in various form-factors,
    ranging from a Raspberry-Pi like Dev Board to independent solderable modules (Google,
    [2021b](#bib.bib64)). It has also been released with the Pixel 4 smartphones as
    the Pixel Neural Core (Rakowski, [2019](#bib.bib127)), for accelerating on-device
    deep learning applications. The EdgeTPU chip itself is smaller than a US penny,
    making it amenable for deployment in many kinds of IoT devices.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'Jetson: Jetson (NVIDIA, [2021](#bib.bib117)) is a family of accelerators by
    Nvidia to enable deep learning applications for embedded and IoT devices. It comprises
    of the Nano, which is a low-powered "system on a module" (SoM) designed for lightweight
    deployments, as well as the more powerful Xavier and TX variants, which are based
    on the NVidia Volta and Pascal GPU architectures. As expected, the difference
    within the Jetson family is primarily the type and number of GPU cores on the
    accelerators. This makes the Nano suited for applications like home automation,
    and the rest for more compute intensive applications like industrial robotics.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 4\. A Practitioner’s Guide to Efficiency
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1850d890bed1a157a8ec2244253459f1.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25\. Trade off between Model Quality and Footprint: There exists a trade-off
    between model quality and model footprint. Model quality can be improved with
    techniques like distillation, data-augmentation, hyper-param tuning etc. Compression
    techniques can in turn help trade off some model quality for a better model footprint.
    Some / all of the improvement in footprint metrics can also be traded for better
    quality by simply adding more model capacity.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: So far, we presented a broad set of tools and techniques in the Efficient Deep
    Learning landscape. In this section, we present a practical guide for practitioners
    to use, and how these tools and techniques work with each other. As mentioned
    earlier, what we seek are *pareto-optimal* models, where we would like to achieve
    the best possible result in one dimension, while holding the other dimensions
    constant. Typically, one of these dimensions is Quality, and the other is Footprint.
    Quality related metrics could included Accuracy, F1, Precision, Recall, AUC, etc.
    While Footprint related metrics can include Model Size, Latency, RAM, etc.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, there exists a trade-off between Quality and Footprint metrics.
    A higher-capacity / deeper model is more likely to achieve a better accuracy,
    but at the cost of model size, latency, etc. On the other hand a model with lesser
    capcity / shallower, while possibly suitable for deployment, is also likely to
    be worse in accuracy. As illustrated in Figure [25](#S4.F25 "Figure 25 ‣ 4\. A
    Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making
    Deep Learning Models Smaller, Faster, and Better"), we can traverse from a model
    with better quality metrics, and exchange some of the quality for better footprint
    by naively compressing the model / reducing the model capacity (Shrink). Similarly
    it is possible to naively improve quality by adding more capacity to the model
    (Grow). Growing can be addressed by the author of the model via appropriately
    increasing model capacity and tweaking other hyper-parameters to improve model
    quality. Shrinking can be achieved via Compression Techniques (Quantization, Pruning,
    Low-Rank Approximation, etc.), Efficient Layers & Models, Architecture Search
    via Automation, etc. In addition, we can also Improve the quality metrics, while
    keeping the footprint same through Learning Techniques (Distillation, Data Augmentation,
    Self-Supervised Tuning), Hyper-Parameter Tuning, etc. (See Table [4](#S4.T4 "Table
    4 ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") for more examples.)'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Grow &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Model Capacity) &#124;'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Shrink &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Footprint) &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Improve &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Quality) &#124;'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '| Add layers, width, etc. either manually or using width / depth / compound
    scaling multipliers |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '&#124; Reduce layers, width, etc. &#124;'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; either manually or using &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; width / depth / compound &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; scaling multipliers &#124;'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Manual Tuning (Architecture / &#124;'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hyper-Parameters / &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Features, etc.) &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Compression Techniques: &#124;'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Quantization, Pruning, &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Low-Rank Factorization, etc. &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Learning Techniques: &#124;'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Data-Augmentation, Distillation, &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Unsupervised Learning, etc. &#124;'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Automation: &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hyper-Param Optimization, &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Architecture Search, etc. &#124;'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Automation: &#124;'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hyper-Param Optimization, &#124;'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Architecture Search, etc. &#124;'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Efficient Layers & Models: &#124;'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Projection, PQRNN, (NLP), &#124;'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Separable Convolution (Vision), &#124;'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; etc. &#124;'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Efficient Layers & Models: &#124;'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Transformers (NLP), &#124;'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Vi-T (Vision), etc. &#124;'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. Examples of techniques to use in the Grow, Shrink, and Improve phases.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: 'Combining these three phases, we propose two strategies towards achieving pareto-optimal
    models:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Shrink-and-Improve for Footprint-Sensitive Models: If as a practitioner, you
    want to reduce your footprint, while keeping the quality the same, this could
    be a useful strategy for on-device deployments and server-side model optimization.
    Shrinking should ideally be minimally lossy in terms of quality (can be achieved
    via learned compression techniques, architecture search etc.), but in some cases
    even naively reducing capacity can also be compensated by the Improve phase. It
    is also possible to do the Improve phase before the Shrink phase.'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Grow-Improve-and-Shrink for Quality-Sensitive Models: When you want to deploy
    models that have better quality while keeping the same footprint, it might make
    sense to follow this strategy. Here, the capacity is first added by growing the
    model as illustrated earlier. The model is then improved using via learning techniques,
    automation, etc. and then shrunk back either naively or in a learned manner. Alternatively,
    the model could be shrunk back either in a learned manner directly after growing
    the model too.'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We consider both these strategies as a way of going from a potentially non pareto-optimal
    model to another one that lies on the pareto-frontier with the trade-off that
    is appropriate for the user. Each efficiency technique individually helps move
    us closer to that target model.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Experiments
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to demonstrate what we proposed above, we undertook the task of going
    through the exercise of making a given Deep Learning model efficient. Concretely,
    we had the following goals with this exercise:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Achieve a new pareto-frontier using the efficiency techniques. Hence, demonstrating
    that these techniques can be used in isolation as well as in combination with
    other techniques, in the real-world by ML Practitioners.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With various combinations of efficiency techniques and model scaling, demonstrate
    the tradeoffs for both ‘Shrink-and-Improve’, and ‘Grow-Improve-and-Shrink’ strategies
    for discovering and traversing the pareto-frontier. In other words, provide empirical
    evidence that it is possible for practitioners to either reduce model capacity
    to bring down the footprint (shrink) and then recover the model quality that they
    traded off (improve), or increase the model capacity to improve quality (growing)
    followed by model compression (shrinking) to improve model footprint.
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We picked the problem of classifying images in the CIFAR-10 dataset (Krizhevsky
    et al., [2009](#bib.bib92)) on compute constrained devices such as smartphones,
    IoT devices etc. We designed a deep convolutional architecture where we could
    scale the model capacity up or down, by increasing or decreasing the ‘width multiplier’
    ($w$) value. In the implementation, $w$ scales the number of filters for the convolutional
    layers (except the first two). Hence, using different values of $w$ in $[0.1,0.25,0.5,0.75,1.0]$
    we obtain a family of models with different quality and footprint tradeoffs. We
    trained these models with some manual tuning to achieve a baseline of quality
    v/s footprint metrics. In this case, we measured quality through accuracy, and
    footprint through number of parameters, model size, and latency. In terms of techniques,
    we used Quantization for Shrinking, and Data Augmentation and Distillation for
    Improving. Many other techniques could be used to further drive the point home
    (Automation such as Hyper-Parameter Tuning, Efficient Layers such as Separable
    Convolutions), but were skipped to keep the interpretation of the results simpler.
    We used the Tensorflow-backed Keras APIs (Chollet, [2020](#bib.bib36)) for training,
    and the TFLite (Authors, [2021i](#bib.bib17)) framework for inference. The latencies
    were measured on three kinds of devices, low-end (Oppo A5), mid-end (Pixel 3XL),
    and high-end (Galaxy S10), in order of their increasing CPU compute power. The
    model size numbers reported are the sizes of the generated TFLite models, and
    the latency numbers are the average single-threaded CPU latency after warmup on
    the target device. The code for the experiments is available via an IPython notebook
    [here](https://github.com/reddragon/efficient-dl-survey-paper/blob/main/CIFAR_10_End_to_End.ipynb).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide
    to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better") compiles the results for 6 width-multipliers in
    increasing order, ranging from $0.05$ to $1.0$. Between the smallest to the largest
    models, the number of params grows by $\approx 91.4\times$, and the model size
    grows by $\approx 80.2\times$. The latency numbers also grow between $3.5-10\times$
    based on the device. Within the same row, footprint metrics will not change since
    we are not changing the model architecture. In Table [5](#S4.T5 "Table 5 ‣ 4.1\.
    Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning:
    A Survey on Making Deep Learning Models Smaller, Faster, and Better") we purely
    work with techniques that will improve the model quality (Data Augmentation and
    Distillation). Table [6](#S4.T6 "Table 6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s
    Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better") reports the numbers for the Quantized versions
    of the corresponding models in Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣
    4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey on
    Making Deep Learning Models Smaller, Faster, and Better"). We use Quantization
    for the Shrink phase, to reduce model size by $\approx 4\times$, and reduce the
    average latency by $1.5-2.65\times$. Figures [26](#S4.F26 "Figure 26 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") and [27](#S4.F27
    "Figure 27 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    plot the notable results from Tables [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣
    4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey on
    Making Deep Learning Models Smaller, Faster, and Better") and [6](#S4.T6 "Table
    6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep
    Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better").'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '| Width Multiplier | # Params (K) | Model Size (KB) |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
- en: '&#124; Accuracy &#124;'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (%) &#124;'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '| Average Latency (ms) |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
- en: '| Baseline | Augmentation |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
- en: '&#124; Augmentation &#124;'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; + Distillation &#124;'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Oppo &#124;'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; A5 &#124;'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pixel &#124;'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3XL &#124;'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Galaxy &#124;'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S10 &#124;'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
- en: '| 0.05 | 14.7 | 65.45 | 70.17 | 71.71 | 72.89 | 6.72 | 0.6 | 0.78 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 26 | 109.61 | 75.93 | 78.22 | 78.93 | 6.85 | 1.7 | 0.85 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
- en: '| 0.25 | 98.57 | 392.49 | 80.6 | 84.14 | 84.51 | 8.15 | 2.02 | 0.93 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 350.05 | 1374.11 | 83.04 | 87.47 | 88.03 | 11.46 | 2.8 | 1.33 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
- en: '| 0.75 | 764.87 | 2993.71 | 83.79 | 89.06 | 89.51 | 16.7 | 4.09 | 1.92 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1343.01 | 5251.34 | 84.42 | 89.41 | 89.92 | 24 | 5.99 | 2.68 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
- en: Table 5\. Quality and Footprint metrics for Floating-Point models for the CIFAR-10
    dataset.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '| Width Multiplier | # Params (K) | Model Size (KB) |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
- en: '&#124; Accuracy &#124;'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (%) &#124;'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '| Average Latency (ms) |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
- en: '| Baseline | Augmentation |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
- en: '&#124; Augmentation &#124;'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; + Distillation &#124;'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Oppo &#124;'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; A5 &#124;'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pixel &#124;'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3XL &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Galaxy &#124;'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S10 &#124;'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
- en: '| 0.05 | 14.7 | 26.87 | 69.9 | 71.72 | 72.7 | 4.06 | 0.49 | 0.43 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 26 | 38.55 | 75.98 | 78.19 | 78.55 | 4.5 | 1.25 | 0.47 |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
- en: '| 0.25 | 98.57 | 111 | 80.76 | 83.98 | 84.18 | 4.52 | 1.31 | 0.48 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 350.05 | 359.31 | 83 | 87.32 | 87.86 | 6.32 | 1.73 | 0.58 |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
- en: '| 0.75 | 764.87 | 767.09 | 83.6 | 88.57 | 89.29 | 8.53 | 2.36 | 0.77 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1343.01 | 1334.41 | 84.52 | 89.28 | 89.91 | 11.73 | 3.27 | 1.01 |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: 'Table 6\. Quality and Footprint metrics for *Quantized* models for the CIFAR-10
    dataset. Each model is the quantized equivalent of the corresponding model in
    Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to
    Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better").'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7619ccc3cb5f7598bc8a351b1aa1f6af.png)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
- en: (a) Number of Params v/s Accuracy
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/007f6cf4a51d3ec99de87dbb4ab1ae39.png)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
- en: (b) Model Size v/s Accuracy
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 26\. Change in Accuracy with respect to Number of Params and Model Size.
    Each point on a curve is a model from Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") in figure (a) and
    from Table [6](#S4.T6 "Table 6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide
    to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better") in figure (b).'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f55914e6f499a13e2c51e8c240c864db.png)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
- en: (a) Low-End Device Latency
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/aa9f166d8e5ba3c57c2e4c2d8f3d5652.png)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
- en: (b) Mid-Tier Device Latency
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/08c4064b319de0a020d218bf14c00f89.png)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
- en: (c) High-End Device Latency
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: Figure 27\. Average latency of models on different devices (low-, mid-, and
    high-end smartphones). The orange curve denotes the quantized models in addition
    to being trained with distillation and data augmentation.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Discussion
  id: totrans-510
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let us try to interpret the above data to validate if our strategies can be
    used practically.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: 'Shrink-and-Improve for Footprint-Sensitive Models: Refer to Table [5](#S4.T5
    "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    and Figure [26](#S4.F26 "Figure 26 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s
    Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better"). If our goal was to deploy the model with
    Width Multiplier ($w$) = $1.0$ and accuracy $84.42\%$, but the bottleneck was
    the model size (5.25 MB) and latency on a low-end device (24 ms on Oppo A5). This
    is the classic case of the footprint metrics not meeting the bar, hence we could
    apply the Shrink-and-Improve strategy, by first naively scaling our model down
    to a Width Multiplier ($w$) of $0.25$. This smaller model when manually tuned,
    as seen in Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s
    Guide to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning
    Models Smaller, Faster, and Better"), achieves an accuracy of $80.76\%$. However,
    when we use a combination of Data Augmentation & Distillation from a separately
    trained larger teacher model with an accuracy of $90.86\%$, the accuracy of the
    smaller model improves to $84.18\%$, very close to the target model that we want
    to deploy. The size of this smaller model is 392.49 KB, which is $13.8\times$
    smaller, and the latency is 8.15 ms, which is $2.94\times$ faster at a comparable
    accuracy. It is possible to further compress this model by using Quantization
    for some additional shrinking. The same smaller model ($w=0.25$) when Quantized
    in Table [6](#S4.T6 "Table 6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide
    to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better"), is 111 KB in size ($47.3\times$ smaller) and has
    a latency of 4.52 ms ($5.31\times$ faster), while retaining an accuracy of $84.18\%$.
    It is possible to do this for other pairs of points on the curves.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: 'Grow-Improve-Shrink for Quality-Sensitive Models: Assuming our goal is to deploy
    a model that has footprint metrics comparable to the model with $w=0.25$ (392.49
    KB model size, 0.93 ms on a high-end Galaxy S10 device), but an accuracy better
    than the baseline $80.6\%$ (refer to Table [5](#S4.T5 "Table 5 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better")). In this case, we
    can choose to first grow our model to $w=0.5$. This instantly blows up the model
    size to 1.37 MB ($3.49\times$ bigger), and latency to 1.33 ms ($1.43\times$ slower).
    However, we ignore that for a bit and improve our model’s quality to $88.03\%$
    with Data Augmentation & Distillation. Then using Quantization for shrinking (refer
    to Table [6](#S4.T6 "Table 6 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide
    to Efficiency ‣ Efficient Deep Learning: A Survey on Making Deep Learning Models
    Smaller, Faster, and Better")), we can get a model that is 359.31 KB in size (32
    KB smaller) and has a 0.58 ms latency on Galaxy S10 ($1.6\times$ faster), with
    an accuracy of $87.86\%$, an absolute 7.10% increase in accuracy while keeping
    the model size approximately same and making it $1.6\times$ faster. It is also
    possible to apply this strategy to other pairs of models.'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we’ve verified that the above two strategies can work both ways, whether
    your goal is to optimize for quality metrics or footprint metrics. We were also
    able to visually inspect through Figures [26](#S4.F26 "Figure 26 ‣ 4.1\. Experiments
    ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient Deep Learning: A Survey
    on Making Deep Learning Models Smaller, Faster, and Better") and [27](#S4.F27
    "Figure 27 ‣ 4.1\. Experiments ‣ 4\. A Practitioner’s Guide to Efficiency ‣ Efficient
    Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better")
    that efficiency techniques can improve on the pareto frontiers constructed through
    manual tuning. To contain the scope of experimentation, we selected two sets of
    efficiency techniques (Compression Techniques (Quantization), and Learning Techniques
    (Data Augmentation & Distillation). Hence, it would be useful to explore other
    techniques as well such as Automation (for Hyper-Parameter Tuning to further improve
    on results), and Efficient Layers & Models (Separable Convolution as illustrated
    in MobileNet (Sandler et al., [2018](#bib.bib134)) could be used in place of larger
    convolutional layers). Finally, we would also like to emphasize paying attention
    to performance of Deep Learning models (optimized or not) on underrepresented
    classes and out-of-distribution data to ensure model fairness, since quality metrics
    alone might not be sufficient for discovering deeper issues with models (Hooker
    et al., [2020](#bib.bib77)).'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusion
  id: totrans-515
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we started with demonstrating the rapid growth in Deep Learning
    models, and motivating the fact that someone training and deploying models today
    has to make either implicit or explicit decisions about efficiency. However, the
    landscape of model efficiency is vast.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: To help with this, we laid out a mental model for the readers to wrap their
    heads around the multiple focus areas of model efficiency and optimization. The
    surveys of the core model optimization techniques give the reader an opportunity
    to understand the state-of-the-art, apply these techniques in the modelling process,
    and/or use them as a starting point for exploration. The infrastructure section
    also lays out the software libraries and hardware which make training and inference
    of efficient models possible.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we presented a section of explicit and actionable insights supplemented
    by code, for a practitioner to use as a guide in this space. This section will
    hopefully give concrete and actionable takeaways, as well as tradeoffs to think
    about when optimizing a model for training and deployment. To conclude, we feel
    that with this survey we have equipped the reader with the necessary understanding
    to break-down the steps required to go from a sub-optimal model to one that meets
    their constraints for both quality as well as footprint.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Acknowledgements
  id: totrans-519
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would like to thank the Learn2Compress team at Google Research for their
    support with this work. We would also like to thank Akanksha Saran and Aditya
    Sarawgi for their help with proof-reading and suggestions for improving the content.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abadi et al. (2016) Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
    Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
    Isard, et al. 2016. Tensorflow: A system for large-scale machine learning. In
    *12th $\{$USENIX$\}$ symposium on operating systems design and implementation
    ($\{$OSDI$\}$ 16)*. 265–283.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agnihotri and Batra (2020) Apoorv Agnihotri and Nipun Batra. 2020. Exploring
    bayesian optimization. *Distill* 5, 5 (2020), e26.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alammar (2021) Jay Alammar. 2021. The Illustrated Transformer. [https://jalammar.github.io/illustrated-transformer](https://jalammar.github.io/illustrated-transformer)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Android Developers (2021) Android Developers. 2021. Neural Networks API $|$
    Android NDK $|$ Android Developers. [https://developer.android.com/ndk/guides/neuralnetworks](https://developer.android.com/ndk/guides/neuralnetworks)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anwar et al. (2017) Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. 2017. Structured
    pruning of deep convolutional neural networks. *ACM Journal on Emerging Technologies
    in Computing Systems (JETC)* 13, 3 (2017), 1–18.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apple Authors (2021) Apple Authors. 2021. Accelerate $|$ Apple Developer Documentation.
    [https://developer.apple.com/documentation/accelerate](https://developer.apple.com/documentation/accelerate)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021a) PyTorch Authors. 2021a. Automatic Mixed Precision examples —
    PyTorch 1.8.1 documentation. [https://pytorch.org/docs/stable/notes/amp_examples.html](https://pytorch.org/docs/stable/notes/amp_examples.html)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021b) PyTorch Authors. 2021b. Performance Tuning Guide — PyTorch Tutorials
    1.8.1+cu102 documentation. [https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021c) PyTorch Authors. 2021c. PyTorch Mobile. [https://pytorch.org/mobile/home](https://pytorch.org/mobile/home)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021d) PyTorch Authors. 2021d. Quantization Recipe — PyTorch Tutorials
    1.8.1+cu102 documentation. [https://pytorch.org/tutorials/recipes/quantization.html](https://pytorch.org/tutorials/recipes/quantization.html)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021e) PyTorch Authors. 2021e. torch.jit.script — PyTorch 1.8.1 documentation.
    [https://pytorch.org/docs/stable/generated/torch.jit.script.html](https://pytorch.org/docs/stable/generated/torch.jit.script.html)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2020) Tensorflow Authors. 2020. TensorFlow Model Optimization. [https://www.tensorflow.org/model_optimization](https://www.tensorflow.org/model_optimization)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021f) Tensorflow Authors. 2021f. Post-training quantization $|$ TensorFlow
    Lite. [https://www.tensorflow.org/lite/performance/post_training_quantization](https://www.tensorflow.org/lite/performance/post_training_quantization)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021g) Tensorflow Authors. 2021g. TensorFlow. [https://www.tensorflow.org](https://www.tensorflow.org)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021h) Tensorflow Authors. 2021h. TensorFlow Lite converter. [https://www.tensorflow.org/lite/convert](https://www.tensorflow.org/lite/convert)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021i) Tensorflow Authors. 2021i. TensorFlow Lite $|$ ML for Mobile
    and Edge Devices. [https://www.tensorflow.org/lite](https://www.tensorflow.org/lite)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Authors (2021j) Tensorflow Authors. 2021j. XLA: Optimizing Compiler for Machine
    Learning $|$ TensorFlow. [https://www.tensorflow.org/xla](https://www.tensorflow.org/xla)
    [Online; accessed 3\. Jun. 2021].'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021k) XNNPACK Authors. 2021k. XNNPACK. [https://github.com/google/XNNPACK](https://github.com/google/XNNPACK)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authors (2021l) XNNPACK Authors. 2021l. XNNPACK backend for TensorFlow Lite.
    [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md/#sparse-inference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md/#sparse-inference)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baevski et al. (2020) Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and
    Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech
    representations. *arXiv preprint arXiv:2006.11477* (2020).'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. (2014) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.
    Neural machine translation by jointly learning to align and translate. *arXiv
    preprint arXiv:1409.0473* (2014).
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. (2013) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013.
    Estimating or propagating gradients through stochastic neurons for conditional
    computation. *arXiv preprint arXiv:1308.3432* (2013).
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bergstra and Bengio (2012) James Bergstra and Yoshua Bengio. 2012. Random search
    for hyper-parameter optimization. *Journal of machine learning research* 13, 2
    (2012).
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blum and Mitchell (1998) Avrim Blum and Tom Mitchell. 1998. Combining labeled
    and unlabeled data with co-training. In *Proceedings of the eleventh annual conference
    on Computational learning theory*. 92–100.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bradbury et al. (2016) James Bradbury, Stephen Merity, Caiming Xiong, and Richard
    Socher. 2016. Quasi-recurrent neural networks. *arXiv preprint arXiv:1611.01576*
    (2016).
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020. Language models are few-shot learners. *arXiv preprint
    arXiv:2005.14165* (2020).
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Buciluǎ et al. (2006) Cristian Buciluǎ, Rich Caruana, and Alexandru Niculescu-Mizil.
    2006. Model compression. In *Proceedings of the 12th ACM SIGKDD international
    conference on Knowledge discovery and data mining*. 535–541.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Charikar (2002) Moses S Charikar. 2002. Similarity estimation techniques from
    rounding algorithms. In *Proceedings of the thiry-fourth annual ACM symposium
    on Theory of computing*. 380–388.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaudhari et al. (2019) Sneha Chaudhari, Varun Mithal, Gungor Polatkan, and
    Rohan Ramanath. 2019. An attentive survey of attention models. *arXiv preprint
    arXiv:1904.02874* (2019).
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chawla et al. (2002) Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and
    W Philip Kegelmeyer. 2002. SMOTE: synthetic minority over-sampling technique.
    *Journal of artificial intelligence research* 16 (2002), 321–357.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen (2021) Dihao Chen. 2021. advisor. [https://github.com/tobegit3hub/advisor](https://github.com/tobegit3hub/advisor)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020a) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey
    Hinton. 2020a. A simple framework for contrastive learning of visual representations.
    In *International conference on machine learning*. PMLR, 1597–1607.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020b) Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi,
    and Geoffrey Hinton. 2020b. Big self-supervised models are strong semi-supervised
    learners. *arXiv preprint arXiv:2006.10029* (2020).
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chollet (2017) François Chollet. 2017. Xception: Deep learning with depthwise
    separable convolutions. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*. 1251–1258.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chollet (2020) Francois Chollet. 2020. The Keras Blog. [https://blog.keras.io](https://blog.keras.io)
    [Online; accessed 4\. Jun. 2021].
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cireşan et al. (2011) Dan C Cireşan, Ueli Meier, Jonathan Masci, Luca M Gambardella,
    and Jürgen Schmidhuber. 2011. High-performance neural networks for visual object
    classification. *arXiv preprint arXiv:1102.0183* (2011).
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributors to Wikimedia projects (2021a) Contributors to Wikimedia projects.
    2021a. AVX-512 - Wikipedia. [https://en.wikipedia.org/w/index.php?title=AVX-512&oldid=1025044245](https://en.wikipedia.org/w/index.php?title=AVX-512&oldid=1025044245)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributors to Wikimedia projects (2021b) Contributors to Wikimedia projects.
    2021b. CUDA - Wikipedia. [https://en.wikipedia.org/w/index.php?title=CUDA&oldid=1025500257](https://en.wikipedia.org/w/index.php?title=CUDA&oldid=1025500257)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributors to Wikimedia projects (2021c) Contributors to Wikimedia projects.
    2021c. Hyperparameter optimization - Wikipedia. [https://en.wikipedia.org/w/index.php?title=Hyperparameter_optimization&oldid=1022309479](https://en.wikipedia.org/w/index.php?title=Hyperparameter_optimization&oldid=1022309479)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributors to Wikimedia projects (2021d) Contributors to Wikimedia projects.
    2021d. Multiply–accumulate operation - Wikipedia. [https://en.wikipedia.org/w/index.php?title=Multiply-accumulate_operation&oldid=1026461481](https://en.wikipedia.org/w/index.php?title=Multiply-accumulate_operation&oldid=1026461481)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributors to Wikimedia projects (2021e) Contributors to Wikimedia projects.
    2021e. SSE4 - Wikipedia. [https://en.wikipedia.org/w/index.php?title=SSE4&oldid=1023092035](https://en.wikipedia.org/w/index.php?title=SSE4&oldid=1023092035)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributors to Wikimedia projects (2021f) Contributors to Wikimedia projects.
    2021f. WebGL - Wikipedia. [https://en.wikipedia.org/w/index.php?title=WebGL&oldid=1026775533](https://en.wikipedia.org/w/index.php?title=WebGL&oldid=1026775533)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cubuk et al. (2019) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan,
    and Quoc V Le. 2019. Autoaugment: Learning augmentation strategies from data.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    113–123.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cubuk et al. (2020) Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
    Le. 2020. Randaugment: Practical automated data augmentation with a reduced search
    space. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Workshops*. 702–703.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In *2009
    IEEE Conference on Computer Vision and Pattern Recognition*. 248–255. [https://doi.org/10.1109/CVPR.2009.5206848](https://doi.org/10.1109/CVPR.2009.5206848)'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers and Zettlemoyer (2019) Tim Dettmers and Luke Zettlemoyer. 2019. Sparse
    networks from scratch: Faster training without losing performance. *arXiv preprint
    arXiv:1907.04840* (2019).'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dietterich (2000) Thomas G Dietterich. 2000. Ensemble methods in machine learning.
    In *International workshop on multiple classifier systems*. Springer, 1–15.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doersch et al. (2015) Carl Doersch, Abhinav Gupta, and Alexei A Efros. 2015.
    Unsupervised visual representation learning by context prediction. In *Proceedings
    of the IEEE international conference on computer vision*. 1422–1430.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2017) Xin Dong, Shangyu Chen, and Sinno Jialin Pan. 2017. Learning
    to prune deep neural networks via layer-wise optimal brain surgeon. *arXiv preprint
    arXiv:1705.07565* (2017).
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dukhan et al. (2020) Marat Dukhan, Yiming Wu Wu, and Hao Lu. 2020. QNNPACK:
    Open source library for optimized mobile deep learning - Facebook Engineering.
    [https://engineering.fb.com/2018/10/29/ml-applications/qnnpack](https://engineering.fb.com/2018/10/29/ml-applications/qnnpack)
    [Online; accessed 3\. Jun. 2021].'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elsen et al. (2020) Erich Elsen, Marat Dukhan, Trevor Gale, and Karen Simonyan.
    2020. Fast sparse convnets. In *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition*. 14629–14638.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elsken et al. (2019) Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, et al.
    2019. Neural architecture search: A survey. *J. Mach. Learn. Res.* 20, 55 (2019),
    1–21.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro,
    and Erich Elsen. 2020. Rigging the lottery: Making all tickets winners. In *International
    Conference on Machine Learning*. PMLR, 2943–2952.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020) Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave,
    Rémi Gribonval, Hervé Jégou, and Armand Joulin. 2020. Training with quantization
    noise for extreme model compression. *arXiv e-prints* (2020), arXiv–2004.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fawzi et al. (2016) Alhussein Fawzi, Horst Samulowitz, Deepak Turaga, and Pascal
    Frossard. 2016. Adaptive data augmentation for image classification. In *2016
    IEEE international conference on image processing (ICIP)*. Ieee, 3688–3692.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frankle and Carbin (2018) Jonathan Frankle and Michael Carbin. 2018. The lottery
    ticket hypothesis: Training pruned neural networks. *arXiv preprint arXiv:1803.03635*
    2 (2018).'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gale et al. (2019) Trevor Gale, Erich Elsen, and Sara Hooker. 2019. The State
    of Sparsity in Deep Neural Networks. *CoRR* abs/1902.09574 (2019). arXiv:1902.09574
    [http://arxiv.org/abs/1902.09574](http://arxiv.org/abs/1902.09574)
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gidaris et al. (2018) Spyros Gidaris, Praveer Singh, and Nikos Komodakis. 2018.
    Unsupervised representation learning by predicting image rotations. *arXiv preprint
    arXiv:1803.07728* (2018).
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glass (2012) James Glass. 2012. Towards unsupervised speech processing. In *2012
    11th International Conference on Information Science, Signal Processing and their
    Applications (ISSPA)*. IEEE, 1–4.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Golovin et al. (2017) Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg
    Kochanski, John Karro, and D Sculley. 2017. Google vizier: A service for black-box
    optimization. In *Proceedings of the 23rd ACM SIGKDD international conference
    on knowledge discovery and data mining*. 1487–1495.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google (2021a) Google. 2021a. Cloud TPU $|$ Google Cloud. [https://cloud.google.com/tpu](https://cloud.google.com/tpu)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google (2021b) Google. 2021b. Coral. [https://coral.ai](https://coral.ai) [Online;
    accessed 4\. Jun. 2021].
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google (2021c) Google. 2021c. Edge TPU performance benchmarks $|$ Coral. [https://coral.ai/docs/edgetpu/benchmarks](https://coral.ai/docs/edgetpu/benchmarks)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google (2021d) Google. 2021d. TensorFlow models on the Edge TPU $|$ Coral. [https://coral.ai/docs/edgetpu/models-intro/#supported-operations](https://coral.ai/docs/edgetpu/models-intro/#supported-operations)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: google research (2021) google research. 2021. google-research. [https://github.com/google-research/google-research/tree/master/fastconvnets](https://github.com/google-research/google-research/tree/master/fastconvnets)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gopalan et al. (2021) Arjun Gopalan, Da-Cheng Juan, Cesar Ilharco Magalhaes,
    Chun-Sung Ferng, Allan Heydon, Chun-Ta Lu, Philip Pham, George Yu, Yicheng Fan,
    and Yueqi Wang. 2021. Neural structured learning: training neural networks with
    structured signals. In *Proceedings of the 14th ACM International Conference on
    Web Search and Data Mining*. 1150–1153.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2015a) Song Han, Huizi Mao, and William J Dally. 2015a. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. *arXiv preprint arXiv:1510.00149* (2015).'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2015b) Song Han, Jeff Pool, John Tran, and William J Dally. 2015b.
    Learning both weights and connections for efficient neural networks. *arXiv preprint
    arXiv:1506.02626* (2015).
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hannun et al. (2014) Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro,
    Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam
    Coates, et al. 2014. Deep speech: Scaling up end-to-end speech recognition. *arXiv
    preprint arXiv:1412.5567* (2014).'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hansen and Salamon (1990) Lars Kai Hansen and Peter Salamon. 1990. Neural network
    ensembles. *IEEE transactions on pattern analysis and machine intelligence* 12,
    10 (1990), 993–1001.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hassibi et al. (1993) Babak Hassibi, David G Stork, and Gregory J Wolff. 1993.
    Optimal brain surgeon and general network pruning. In *IEEE international conference
    on neural networks*. IEEE, 293–299.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2018) Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and
    Song Han. 2018. Amc: Automl for model compression and acceleration on mobile devices.
    In *Proceedings of the European Conference on Computer Vision (ECCV)*. 784–800.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531* (2015).
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hooker et al. (2020) Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio,
    and Emily Denton. 2020. Characterising bias in compressed models. *arXiv preprint
    arXiv:2010.03058* (2020).
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Howard et al. (2019) Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen,
    Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan,
    et al. 2019. Searching for mobilenetv3\. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 1314–1324.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Howard and Ruder (2018) Jeremy Howard and Sebastian Ruder. 2018. Universal language
    model fine-tuning for text classification. *arXiv preprint arXiv:1801.06146* (2018).
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hsu et al. (2018) Chi-Hung Hsu, Shu-Huan Chang, Jhao-Hong Liang, Hsin-Ping
    Chou, Chun-Hao Liu, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, and Da-Cheng
    Juan. 2018. Monas: Multi-objective neural architecture search using reinforcement
    learning. *arXiv preprint arXiv:1806.10332* (2018).'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hubara et al. (2016) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv,
    and Yoshua Bengio. 2016. Binarized neural networks. In *Proceedings of the 30th
    International Conference on Neural Information Processing Systems*. 4114–4122.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inoue (2018) Hiroshi Inoue. 2018. Data augmentation by pairing samples for images
    classification. *arXiv preprint arXiv:1801.02929* (2018).
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacob et al. (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.
    2704–2713.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaderberg et al. (2017) Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M
    Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen
    Simonyan, et al. 2017. Population based training of neural networks. *arXiv preprint
    arXiv:1711.09846* (2017).
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jamieson and Talwalkar (2016) Kevin Jamieson and Ameet Talwalkar. 2016. Non-stochastic
    best arm identification and hyperparameter optimization. In *Artificial Intelligence
    and Statistics*. PMLR, 240–248.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeremy Jordan (2020) Jeremy Jordan. 2020. Setting the learning rate of your
    neural network. *Jeremy Jordan* (Aug 2020). [https://www.jeremyjordan.me/nn-learning-rate](https://www.jeremyjordan.me/nn-learning-rate)
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jouppi et al. (2017) Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson,
    Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers,
    et al. 2017. In-datacenter performance analysis of a tensor processing unit. In
    *Proceedings of the 44th annual international symposium on computer architecture*.
    1–12.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaliamoorthi et al. (2019) Prabhu Kaliamoorthi, Sujith Ravi, and Zornitsa Kozareva.
    2019. PRADO: Projection Attention Networks for Document Classification On-Device.
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*. 5012–5021.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaliamoorthi et al. (2021) Prabhu Kaliamoorthi, Aditya Siddhant, Edward Li,
    and Melvin Johnson. 2021. Distilling Large Language Models into Tiny and Effective
    Students using pQRNN. *arXiv preprint arXiv:2101.08890* (2021).
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kanwar et al. (2021) Pankaj Kanwar, Peter Brandt, and Zongwei Zhou. 2021. TensorFlow
    2 MLPerf submissions demonstrate best-in-class performance on Google Cloud. [https://blog.tensorflow.org/2020/07/tensorflow-2-mlperf-submissions.html](https://blog.tensorflow.org/2020/07/tensorflow-2-mlperf-submissions.html)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krishnamoorthi (2018) Raghuraman Krishnamoorthi. 2018. Quantizing deep convolutional
    networks for efficient inference: A whitepaper. *arXiv* (Jun 2018). arXiv:1806.08342
    [https://arxiv.org/abs/1806.08342v1](https://arxiv.org/abs/1806.08342v1)'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning
    multiple layers of features from tiny images. (2009).
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. Imagenet classification with deep convolutional neural networks. *Advances
    in neural information processing systems* 25 (2012), 1097–1105.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krogh and Vedelsby (1994) Anders Krogh and Jesper Vedelsby. 1994. Neural network
    ensembles, cross validation and active learning. In *NIPS’94: Proceedings of the
    7th International Conference on Neural Information Processing Systems*. MIT Press,
    Cambridge, MA, USA, 231–238. [https://doi.org/10.5555/2998687.2998716](https://doi.org/10.5555/2998687.2998716)'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kung and Leiserson (1980) HT Kung and CE Leiserson. 1980. Introduction to VLSI
    systems. *Mead, C. A_, and Conway, L.,(Eds), Addison-Wesley, Reading, MA* (1980),
    271–292.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kung (1982) Hsiang-Tsung Kung. 1982. Why systolic architectures? *IEEE computer*
    15, 1 (1982), 37–46.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LeCun (2018) Yann LeCun. 2018. Yann LeCun @EPFL - "Self-supervised learning:
    could machines learn like humans?". [https://www.youtube.com/watch?v=7I0Qt7GALVk&t=316s](https://www.youtube.com/watch?v=7I0Qt7GALVk&t=316s)
    [Online; accessed 3\. Jun. 2021].'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lecun et al. (1998) Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner.
    1998. Gradient-based learning applied to document recognition. *Proc. IEEE* 86,
    11 (Nov 1998), 2278–2324. [https://doi.org/10.1109/5.726791](https://doi.org/10.1109/5.726791)
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1990) Yann LeCun, John S Denker, and Sara A Solla. 1990. Optimal
    brain damage. In *Advances in neural information processing systems*. 598–605.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016) Fengfu Li, Bo Zhang, and Bin Liu. 2016. Ternary weight networks.
    *arXiv preprint arXiv:1605.04711* (2016).
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016) Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter
    Graf. 2016. Pruning Filters for Efficient ConvNets. In *ICLR (Poster)*.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017) Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh,
    and Ameet Talwalkar. 2017. Hyperband: A novel bandit-based approach to hyperparameter
    optimization. *The Journal of Machine Learning Research* 18, 1 (2017), 6765–6816.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li (2020) Sharon Y. Li. 2020. Automating Data Augmentation: Practice, Theory
    and New Direction. *SAIL Blog* (Apr 2020). [http://ai.stanford.edu/blog/data-augmentation](http://ai.stanford.edu/blog/data-augmentation)'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liaw et al. (2018) Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz,
    Joseph E Gonzalez, and Ion Stoica. 2018. Tune: A research platform for distributed
    model selection and training. *arXiv preprint arXiv:1807.05118* (2018).'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018c) Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens,
    Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.
    2018c. Progressive neural architecture search. In *Proceedings of the European
    conference on computer vision (ECCV)*. 19–34.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018a) Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018a. Darts:
    Differentiable architecture search. *arXiv preprint arXiv:1806.09055* (2018).'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019) Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang,
    Kwang-Ting Cheng, and Jian Sun. 2019. Metapruning: Meta learning for automatic
    neural network channel pruning. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 3296–3305.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018b) Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor
    Darrell. 2018b. Rethinking the Value of Network Pruning. *CoRR* abs/1810.05270
    (2018). arXiv:1810.05270 [http://arxiv.org/abs/1810.05270](http://arxiv.org/abs/1810.05270)
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ltd. (2021) Arm Ltd. 2021. SIMD ISAs $|$ Neon – Arm Developer. [https://developer.arm.com/architectures/instruction-sets/simd-isas/neon](https://developer.arm.com/architectures/instruction-sets/simd-isas/neon)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Menghani and Ravi (2019) Gaurav Menghani and Sujith Ravi. 2019. Learning from
    a Teacher using Unlabeled Data. *arXiv preprint arXiv:1911.05275* (2019).
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2017) Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian
    Puhrsch, and Armand Joulin. 2017. Advances in pre-training distributed word representations.
    *arXiv preprint arXiv:1712.09405* (2017).
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Močkus (1975) Jonas Močkus. 1975. On Bayesian methods for seeking the extremum.
    In *Optimization techniques IFIP technical conference*. Springer, 400–404.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Molchanov et al. (2016) Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
    and Jan Kautz. 2016. Pruning Convolutional Neural Networks for Resource Efficient
    Transfer Learning. *CoRR* abs/1611.06440 (2016). arXiv:1611.06440 [http://arxiv.org/abs/1611.06440](http://arxiv.org/abs/1611.06440)
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node.js Authors (2021) Node.js Authors. 2021. Node.js. [https://nodejs.org/en](https://nodejs.org/en)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NVIDIA (2020a) NVIDIA. 2020a. GTC 2020: Accelerating Sparsity in the NVIDIA
    Ampere Architecture. [https://developer.nvidia.com/gtc/2020/video/s22085-vid](https://developer.nvidia.com/gtc/2020/video/s22085-vid)
    [Online; accessed 3\. Jun. 2021].'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NVIDIA (2020b) NVIDIA. 2020b. Inside Volta: The World’s Most Advanced Data
    Center GPU $|$ NVIDIA Developer Blog. [https://developer.nvidia.com/blog/inside-volta](https://developer.nvidia.com/blog/inside-volta)
    [Online; accessed 3\. Jun. 2021].'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA (2021) NVIDIA. 2021. NVIDIA Embedded Systems for Next-Gen Autonomous
    Machines. [https://www.nvidia.com/en-us/autonomous-machines/embedded-systems](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems)
    [Online; accessed 4\. Jun. 2021].
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Panigrahy (2021) Rina Panigrahy. 2021. Matrix Compression Operator. [https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html](https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html)
    [Online; accessed 5\. Jun. 2021].
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PapersWithCode.com (2021) PapersWithCode.com. 2021. Papers with Code - The latest
    in Machine Learning. [https://paperswithcode.com](https://paperswithcode.com)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning
    library. *arXiv preprint arXiv:1912.01703* (2019).'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patrick et al. (2020) Mandela Patrick, Yuki M Asano, Polina Kuznetsova, Ruth
    Fong, João F Henriques, Geoffrey Zweig, and Andrea Vedaldi. 2020. Multi-modal
    self-supervision from generalized data transformations. *arXiv preprint arXiv:2003.04298*
    (2020).
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher D
    Manning. 2014. Glove: Global vectors for word representation. In *Proceedings
    of the 2014 conference on empirical methods in natural language processing (EMNLP)*.
    1532–1543.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perrone et al. (2020) Valerio Perrone, Huibin Shen, Aida Zolic, Iaroslav Shcherbatyi,
    Amr Ahmed, Tanya Bansal, Michele Donini, Fela Winkelmolen, Rodolphe Jenatton,
    Jean Baptiste Faddoul, et al. 2020. Amazon SageMaker Automatic Model Tuning: Scalable
    Black-box Optimization. *arXiv preprint arXiv:2012.08489* (2020).'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pham et al. (2018) Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean.
    2018. Efficient neural architecture search via parameters sharing. In *International
    Conference on Machine Learning*. PMLR, 4095–4104.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polino et al. (2018) Antonio Polino, Razvan Pascanu, and Dan Alistarh. 2018.
    Model compression via distillation and quantization. *arXiv preprint arXiv:1802.05668*
    (2018).
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raina et al. (2009) Rajat Raina, Anand Madhavan, and Andrew Y Ng. 2009. Large-scale
    deep unsupervised learning using graphics processors. In *Proceedings of the 26th
    annual international conference on machine learning*. 873–880.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rakowski (2019) Brian Rakowski. 2019. Pixel 4 is here to help. *Google* (Oct
    2019). [https://blog.google/products/pixel/pixel-4](https://blog.google/products/pixel/pixel-4)
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rastegari et al. (2016) Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
    and Ali Farhadi. 2016. Xnor-net: Imagenet classification using binary convolutional
    neural networks. In *European conference on computer vision*. Springer, 525–542.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ravi (2017) Sujith Ravi. 2017. Projectionnet: Learning efficient on-device
    deep networks using neural projections. *arXiv preprint arXiv:1708.00630* (2017).'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ravi and Kozareva (2018) Sujith Ravi and Zornitsa Kozareva. 2018. Self-governing
    neural networks for on-device short text classification. In *Proceedings of the
    2018 Conference on Empirical Methods in Natural Language Processing*. 887–893.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real et al. (2019) Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le.
    2019. Regularized evolution for image classifier architecture search. In *Proceedings
    of the aaai conference on artificial intelligence*, Vol. 33. 4780–4789.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Research (2019) Microsoft Research. 2019. Neural Network Intelligence - Microsoft
    Research. [https://www.microsoft.com/en-us/research/project/neural-network-intelligence](https://www.microsoft.com/en-us/research/project/neural-network-intelligence)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rotem et al. (2018) Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret Catron,
    Summer Deng, Roman Dzhabarov, Nick Gibson, James Hegeman, Meghan Lele, Roman Levenstein,
    et al. 2018. Glow: Graph lowering compiler techniques for neural networks. *arXiv
    preprint arXiv:1805.00907* (2018).'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sandler et al. (2018) Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
    and Liang-Chieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    4510–4520.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper
    and lighter. *arXiv preprint arXiv:1910.01108* (2019).'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sankar et al. (2019) Chinnadhurai Sankar, Sujith Ravi, and Zornitsa Kozareva.
    2019. Transferable neural projection representations. *arXiv preprint arXiv:1906.01605*
    (2019).
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sankar et al. (2020) Chinnadhurai Sankar, Sujith Ravi, and Zornitsa Kozareva.
    2020. ProFormer: Towards On-Device LSH Projection Based Transformers. *arXiv preprint
    arXiv:2004.05801* (2020).'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sato (2021) Kaz Sato. 2021. What makes TPUs fine-tuned for deep learning? $|$
    Google Cloud Blog. [https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning](https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schrittwieser et al. (2020) Julian Schrittwieser, Ioannis Antonoglou, Thomas
    Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart,
    Demis Hassabis, Thore Graepel, et al. 2020. Mastering atari, go, chess and shogi
    by planning with a learned model. *Nature* 588, 7839 (2020), 604–609.
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.
    Edinburgh neural machine translation systems for wmt 16. *arXiv preprint arXiv:1606.02891*
    (2016).
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simard et al. (2003) Patrice Y Simard, David Steinkraus, John C Platt, et al.
    2003. Best practices for convolutional neural networks applied to visual document
    analysis.. In *Icdar*, Vol. 3\. Citeseer.
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman (2014) Karen Simonyan and Andrew Zisserman. 2014. Very
    deep convolutional networks for large-scale image recognition. *arXiv preprint
    arXiv:1409.1556* (2014).
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stosic (2020) Dusan Stosic. 2020. Training Neural Networks with Tensor Cores
    - Dusan Stosic, NVIDIA. [https://www.youtube.com/watch?v=jF4-_ZK_tyc](https://www.youtube.com/watch?v=jF4-_ZK_tyc)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2017) Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav
    Gupta. 2017. Revisiting unreasonable effectiveness of data in deep learning era.
    In *Proceedings of the IEEE international conference on computer vision*. 843–852.
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2020) Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming
    Yang, and Denny Zhou. 2020. Mobilebert: a compact task-agnostic bert for resource-limited
    devices. *arXiv preprint arXiv:2004.02984* (2020).'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
    Sequence to sequence learning with neural networks. *arXiv preprint arXiv:1409.3215*
    (2014).
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
    2015. Going deeper with convolutions. In *Proceedings of the IEEE conference on
    computer vision and pattern recognition*. 1–9.
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. (2019) Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark
    Sandler, Andrew Howard, and Quoc V Le. 2019. Mnasnet: Platform-aware neural architecture
    search for mobile. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*. 2820–2828.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tay et al. (2020) Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
    2020. Efficient transformers: A survey. *arXiv preprint arXiv:2009.06732* (2020).'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow (2019) TensorFlow. 2019. TensorFlow Model Optimization Toolkit —
    Post-Training Integer Quantization. *Medium* (Nov 2019). [https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba)
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow (2021) TensorFlow. 2021. Model optimization $|$ TensorFlow Lite.
    [https://www.tensorflow.org/lite/performance/model_optimization](https://www.tensorflow.org/lite/performance/model_optimization)
    [Online; accessed 3\. Jun. 2021].
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tsang (2019) Sik-Ho Tsang. 2019. Review: Xception — With Depthwise Separable
    Convolution, Better Than Inception-v3 (Image Classification). *Medium* (Mar 2019).
    [https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568](https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568)'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Urban et al. (2016) Gregor Urban, Krzysztof J Geras, Samira Ebrahimi Kahou,
    Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose,
    and Matt Richardson. 2016. Do deep convolutional nets really need to be deep and
    convolutional? *arXiv preprint arXiv:1603.05691* (2016).
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vanhoucke et al. (2011) Vincent Vanhoucke, Andrew Senior, and Mark Z Mao. 2011.
    Improving the speed of neural networks on CPUs. (2011).
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vasilache et al. (2018) Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis,
    Priya Goyal, Zachary DeVito, William S Moses, Sven Verdoolaege, Andrew Adams,
    and Albert Cohen. 2018. Tensor comprehensions: Framework-agnostic high-performance
    machine learning abstractions. *arXiv preprint arXiv:1802.04730* (2018).'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *arXiv preprint arXiv:1706.03762* (2017).
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. 2020.
    Towards Accurate Post-training Network Quantization via Bit-Split and Stitching.
    In *International Conference on Machine Learning*. PMLR, 9847–9856. [http://proceedings.mlr.press/v119/wang20c.html](http://proceedings.mlr.press/v119/wang20c.html)
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Kanwar (2021) Shibo Wang and Pankaj Kanwar. 2021. BFloat16: The secret
    to high performance on Cloud TPUs $|$ Google Cloud Blog. [https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)
    [Online; accessed 3\. Jun. 2021].'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018.
    Switchout: an efficient data augmentation algorithm for neural machine translation.
    *arXiv preprint arXiv:1808.07512* (2018).'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Warden and Situnayake (2019) Pete Warden and Daniel Situnayake. 2019. *Tinyml:
    Machine learning with tensorflow lite on arduino and ultra-low-power microcontrollers*.
    " O’Reilly Media, Inc.".'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019) Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei
    Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. 2019.
    Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture
    search. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*. 10734–10742.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2020) Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. 2020.
    Self-training with noisy student improves imagenet classification. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 10687–10698.
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yalniz et al. (2019) I Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and
    Dhruv Mahajan. 2019. Billion-scale semi-supervised learning for image classification.
    *arXiv preprint arXiv:1905.00546* (2019).
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018) Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai
    Chen, Mohammad Norouzi, and Quoc V Le. 2018. Qanet: Combining local convolution
    with global self-attention for reading comprehension. *arXiv preprint arXiv:1804.09541*
    (2018).'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu and Zhu (2020) Tong Yu and Hong Zhu. 2020. Hyper-parameter optimization:
    A review of algorithms and applications. *arXiv preprint arXiv:2003.05689* (2020).'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zagoruyko and Komodakis (2016) Sergey Zagoruyko and Nikos Komodakis. 2016.
    Paying more attention to attention: Improving the performance of convolutional
    neural networks via attention transfer. *arXiv preprint arXiv:1612.03928* (2016).'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017) Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David
    Lopez-Paz. 2017. mixup: Beyond empirical risk minimization. *arXiv preprint arXiv:1710.09412*
    (2017).'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu and Gupta (2018) Michael Zhu and Suyog Gupta. 2018. To Prune, or Not to
    Prune: Exploring the Efficacy of Pruning for Model Compression. In *6th International
    Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
    30 - May 3, 2018, Workshop Track Proceedings*. OpenReview.net. [https://openreview.net/forum?id=Sy1iIDkPM](https://openreview.net/forum?id=Sy1iIDkPM)'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoph and Le (2016) Barret Zoph and Quoc V Le. 2016. Neural architecture search
    with reinforcement learning. *arXiv preprint arXiv:1611.01578* (2016).
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoph et al. (2018) Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V
    Le. 2018. Learning transferable architectures for scalable image recognition.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    8697–8710.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
