- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:58:44'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2011.00416] Deep Learning for Text Style Transfer: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2011.00416](https://ar5iv.labs.arxiv.org/html/2011.00416)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \historydates
  prefs: []
  type: TYPE_NORMAL
- en: 'Submission received: 25 April 2021, Revised version received: 30 August 2021,
    Accepted for publication: 4 December 2021. yy2021 \dochead'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning for Text Style Transfer:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Survey
  prefs: []
  type: TYPE_NORMAL
- en: Di Jin Equal contribution. MIT CSAIL
  prefs: []
  type: TYPE_NORMAL
- en: jindi15@mit.edu    Zhijing Jin^* Max Planck Institute & ETH Zürich
  prefs: []
  type: TYPE_NORMAL
- en: zjin@tue.mpg.de    Zhiting Hu UC San Diego
  prefs: []
  type: TYPE_NORMAL
- en: zhh019@ucsd.edu    Olga Vechtomova University of Waterloo
  prefs: []
  type: TYPE_NORMAL
- en: ovechtom@uwaterloo.ca    Rada Mihalcea University of Michigan
  prefs: []
  type: TYPE_NORMAL
- en: mihalcea@umich.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Text style transfer is an important task in natural language generation, which
    aims to control certain attributes in the generated text, such as politeness,
    emotion, humor, and many others. It has a long history in the field of natural
    language processing, and recently has re-gained significant attention thanks to
    the promising performance brought by deep neural models. In this paper, we present
    a systematic survey of the research on neural text style transfer, spanning over
    100 representative articles since the first neural text style transfer work in
    2017\. We discuss the task formulation, existing datasets and subtasks, evaluation,
    as well as the rich methodologies in the presence of parallel and non-parallel
    data. We also provide discussions on a variety of important topics regarding the
    future development of this task.¹¹1Our curated paper list is at [https://github.com/zhijing-jin/Text_Style_Transfer_Survey](https://github.com/zhijing-jin/Text_Style_Transfer_Survey).
  prefs: []
  type: TYPE_NORMAL
- en: '^†^†issue: xx'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Language is situational. Every utterance fits in a specific time, place, and
    scenario, conveys specific characteristics of the speaker, and typically has a
    well-defined intent. For example, someone who is uncertain is more likely to use
    tag questions (e.g., “This is true, isn’t it?”) than declarative sentences (e.g.,
    “This is definitely true.”). Similarly, a professional setting is more likely
    to include formal statements (e.g., “Please consider taking a seat.”) as compared
    to an informal situation (e.g., “Come and sit!”). For artificial intelligence
    systems to accurately understand and generate language, it is necessary to model
    language with style/attribute,²²2Note that we interchangeably use the terms style
    and attribute in this survey. Attribute is a broader terminology that can include
    content preferences, e.g., sentiment, topic, and so on. This survey uses style
    in the same broad way, following the common practice in recent papers (see Section [2.1](#S2.SS1
    "2.1 How to Define Style? ‣ 2 What Is Text Style Transfer? ‣ Deep Learning for
    Text Style Transfer: A Survey")). which goes beyond merely verbalizing the semantics
    in a non-stylized way. The values of the attributes can be drawn from a wide range
    of choices depending on pragmatics, such as the extent of formality, politeness,
    simplicity, personality, emotion, partner effect (e.g., reader awareness), genre
    of writing (e.g., fiction or non-fiction), and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of text style transfer (TST) is to automatically control the style
    attributes of text while preserving the content. TST has a wide range of applications,
    as outlined by McDonald and Pustejovsky ([1985](#bib.bib127)) and Hovy ([1987](#bib.bib69)).
    The style of language is crucial because it makes natural language processing
    more user-centered. TST has many immediate applications. For instance, one such
    application is intelligent bots for which users prefer distinct and consistent
    persona (e.g., empathetic) instead of emotionless or inconsistent persona. Another
    application is the development of intelligent writing assistants; for example,
    non-expert writers often need to polish their writings to better fit their purpose,
    e.g., more professional, polite, objective, humorous, or other advanced writing
    requirements, which may take years of experience to master. Other applications
    include automatic text simplification (where the target style is “simple”), debiasing
    online text (where the target style is “objective”), fighting against offensive
    language (where the target style is “non-offensive”), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'To formally define text style transfer, let us denote the target utterance
    as $\bm{x}^{\prime}$ and the target discourse style attribute as $a^{\prime}$.
    TST aims to model $p(\bm{x}^{\prime}|a,\bm{x})$, where $\bm{x}$ is a given text
    carrying a source attribute value $a$. Consider the previous example of text expressed
    by two different extents of formality:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Source sentence $\bm{x}$: | “Come and sit!” | Source attribute $a$: | Informal
    |'
  prefs: []
  type: TYPE_TB
- en: '| Target sentence $\bm{x}^{\prime}$: | “Please consider taking a seat.” | Target
    attribute $a^{\prime}$: | Formal |'
  prefs: []
  type: TYPE_TB
- en: In this case, a TST model should be able to modify the formality and generate
    the formal sentence $\bm{x}^{\prime}=$“Please consider taking a seat.” given the
    informal input $\bm{x}=$“Come and sit!”. Note that the key difference of TST from
    another NLP task, style-conditioned language modeling, is that the latter is conditioned
    on only a style token, whereas TST takes as input both the target style attribute
    $a^{\prime}$ and a source sentence $\bm{x}$ that constrains the content.
  prefs: []
  type: TYPE_NORMAL
- en: Crucial to the definition of style transfer is the distinction of “style” and
    “content,” for which there are two common practices. The first one is by linguistic
    definition, where non-functional linguistic features are classified into the style
    (e.g., formality), and the semantics are classified into the content. In contrast,
    the second practice is data-driven, – given two corpora (e.g., a positive review
    set and a negative review set), the invariance between the two corpora is the
    content, whereas the variance is the style (e.g., sentiment, topic) Mou and Vechtomova
    ([2020](#bib.bib131)).
  prefs: []
  type: TYPE_NORMAL
- en: Driven by the growing needs for TST, active research in this field has emerged,
    from the traditional linguistic approaches, to the more recent neural network-based
    approaches. Traditional approaches rely on term replacement and templates. For
    example, early work in NLG for weather forecasts builds domain-specific templates
    to express different types of weather with different levels of uncertainty for
    different users Sripada et al. ([2004](#bib.bib187)); Reiter et al. ([2005](#bib.bib160));
    Belz ([2008](#bib.bib11)); Gkatzia, Lemon, and Rieser ([2017](#bib.bib49)). Research
    that more explicitly focuses on TST starts from the frame language-based systems
    McDonald and Pustejovsky ([1985](#bib.bib127)), and schema-based NLG systems Hovy
    ([1987](#bib.bib69), [1990](#bib.bib70)) which generate text with pragmatic constraints
    such as formality under small-scale well-defined schema. Most of this earlier
    work required domain-specific templates, hand-featured phrase sets that express
    a certain attribute (e.g., friendly), and sometimes a look-up table of expressions
    with the same meaning but multiple different attributes Bateman and Paris ([1989](#bib.bib9));
    Stamatatos et al. ([1997](#bib.bib188)); Power, Scott, and Bouayad-Agha ([2003](#bib.bib148));
    Reiter, Robertson, and Osman ([2003](#bib.bib159)); Sheikha and Inkpen ([2011](#bib.bib181));
    Mairesse and Walker ([2011](#bib.bib121)).
  prefs: []
  type: TYPE_NORMAL
- en: 'With the success of deep learning in the last decade, a variety of neural methods
    have been recently proposed for TST. If parallel data are provided, standard sequence-to-sequence
    models are often directly applied Rao and Tetreault ([2018](#bib.bib156)) (see
    Section [4](#S4 "4 Methods on Parallel Data ‣ Deep Learning for Text Style Transfer:
    A Survey")). However, most use cases do not have parallel data, so TST on non-parallel
    corpora has become a prolific research area (see Section [5](#S5 "5 Methods on
    Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey")). The first
    line of approaches disentangle text into its content and attribute in the latent
    space, and apply generative modeling Hu et al. ([2017](#bib.bib72)); Shen et al.
    ([2017](#bib.bib182)). This trend was then joined by another distinctive line
    of approach, prototype editing Li et al. ([2018](#bib.bib107)) which extracts
    a sentence template and its attribute markers to generate the text. Another paradigm
    soon followed, i.e., pseudo-parallel corpus construction to train the model as
    if in a supervised way with the pseudo-parallel data Zhang et al. ([2018d](#bib.bib234));
    Jin et al. ([2019](#bib.bib82)). These three directions, (1) disentanglement,
    (2) prototype editing, and (3) pseudo-parallel corpus construction, are further
    advanced with the emergence of Transformer-based models Sudhakar, Upadhyay, and
    Maheswaran ([2019](#bib.bib190)); Malmi, Severyn, and Rothe ([2020a](#bib.bib122)).'
  prefs: []
  type: TYPE_NORMAL
- en: Given the advances in TST methodologies, it now starts to expand its impact
    to downstream applications, such as persona-based dialog generation Niu and Bansal
    ([2018](#bib.bib135)); Huang et al. ([2018](#bib.bib73)), stylistic summarization
    Jin et al. ([2020a](#bib.bib80)), stylized language modeling to imitate specific
    authors Syed et al. ([2020](#bib.bib192)), online text debiasing Pryzant et al.
    ([2020](#bib.bib150)); Ma et al. ([2020](#bib.bib118)), simile generation Chakrabarty,
    Muresan, and Peng ([2020](#bib.bib28)), and many others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Overview of the survey.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Motivation | Data | Method | Extended Applications |'
  prefs: []
  type: TYPE_TB
- en: '| • Artistic writing • Communication • Mitigating social issues | Tasks • Formality
    • Politeness • Gender • Humor • Romance • Biasedness Key Properties • Parallel
    vs. non-parallel • Uni- vs. bi-directional • Dataset size • Large vs. small word
    overlap | • Toxicity • Authorship • Simplicity • Sentiment • Topic • Political
    slant | On Parallel Data • Multi-tasking • Inference techniques • Data augmentation
    On Non-Parallel Data • Disentanglement • Prototype editing • Pseudo data construction
    | Helping Other NLP Tasks • Paraphrasing • Data augmentation • Adversarial robustness
    • Persona-consistent dialog • Anonymization • Summarization • Style-specific MT
    |'
  prefs: []
  type: TYPE_TB
- en: 1.0.0.0.1 Motivation of a Survey on TST.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The increasing interest in modeling the style of text can be regarded as a
    trend reflecting the fact that NLP researchers start to focus more on user-centeredness
    and personalization. However, despite the growing interest in TST, the existing
    literature shows a large diversity in the selection of benchmark datasets, methodological
    frameworks, and evaluation metrics. Thus, the aim of this survey is to provide
    summaries and potential standardizations on some important aspects of TST, such
    as the terminology, problem definition, benchmark datasets, and evaluation metrics.
    We also aim to provide different perspectives on the methodology of TST, and suggest
    some potential cross-cutting research questions for our proposed research agenda
    of the field. As shown in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Deep Learning
    for Text Style Transfer: A Survey"), the key contributions targeted by this survey
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We conduct the first comprehensive review that covers most existing works (more
    than 100 papers) on deep learning-based TST.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We provide an overview of the task setting, terminology definition, benchmark
    datasets (Section [2](#S2 "2 What Is Text Style Transfer? ‣ Deep Learning for
    Text Style Transfer: A Survey")), and evaluation metrics for which we proposed
    standard practices that can be helpful for future works (Section [3](#S3 "3 How
    to Evaluate Style Transfer? ‣ Deep Learning for Text Style Transfer: A Survey")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We categorize the existing approaches on parallel data (Section [4](#S4 "4
    Methods on Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey"))
    and non-parallel data (Section [5](#S5 "5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey")) for which we distill some unified
    methodological frameworks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We discuss a potential research agenda for TST (Section [6](#S6 "6 Research
    Agenda ‣ Deep Learning for Text Style Transfer: A Survey")), including expanding
    the scope of styles, improving the methodology, loosening dataset assumptions,
    and improving evaluation metrics.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We provide a vision for how to broaden the impact of TST (Section [7](#S7 "7
    Expanding the Impact of TST ‣ Deep Learning for Text Style Transfer: A Survey")),
    including connecting to more NLP tasks, and more specialized downstream applications,
    as well as considering some important ethical impacts.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1.0.0.0.2 Paper Selection.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The neural TST papers reviewed in this survey are mainly from top conferences
    in NLP and artificial intelligence (AI), including ACL, EMNLP, NAACL, COLING,
    CoNLL, NeurIPS, ICML, ICLR, AAAI, and IJCAI. Other than conference papers, we
    also include some non-peer-reviewed preprint papers that can offer some insightful
    information about the field. The major factors for selecting non-peer-reviewed
    preprint papers include novelty and completeness, among others.
  prefs: []
  type: TYPE_NORMAL
- en: 2 What Is Text Style Transfer?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section provides an overview of the style transfer task. Section [2.1](#S2.SS1
    "2.1 How to Define Style? ‣ 2 What Is Text Style Transfer? ‣ Deep Learning for
    Text Style Transfer: A Survey") goes through the definition of styles and the
    scope of this survey. Section [2.2](#S2.SS2 "2.2 Task Formulation ‣ 2 What Is
    Text Style Transfer? ‣ Deep Learning for Text Style Transfer: A Survey") gives
    a task formulation and introduces the notations that will be used across the survey.
    Finally, Section [2.3](#S2.SS3 "2.3 Existing Subtasks with Datasets ‣ 2 What Is
    Text Style Transfer? ‣ Deep Learning for Text Style Transfer: A Survey") lists
    all the common subtasks for neural text style transfer which can save the literature
    review efforts for future researchers.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 How to Define Style?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.1.0.0.1 Linguistic Definition of Style.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An intuitive notion of style refers to the manner in which the semantics is
    expressed McDonald and Pustejovsky ([1985](#bib.bib127)). Just as everyone has
    their own signatures, style originates as the characteristics inherent to every
    person’s utterance, which can be expressed through the use of certain stylistic
    devices such as metaphors, as well as choice of words, syntactic structures, and
    so on. Style can also go beyond the sentence level to the discourse level, such
    as the stylistic structure of the entire piece of the work, e.g., stream of consciousness,
    or flashbacks.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the intrinsic personal styles, for pragmatic uses, style further becomes
    a protocol to regularize the manner of communication. For example, for academic
    writing, the protocol requires formality and professionalism. Hovy ([1987](#bib.bib69))
    defines style by its pragmatic aspects, including both personal (e.g., personality,
    gender) and interpersonal (e.g., humor, romance) aspects. Most existing literature
    also takes these well-defined categories of styles.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/302b20cf72aabaa1b6d63c604cf0e78e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Venn diagram of the linguistic definition of style and data-driven
    definition of style.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.0.0.2 Data-Driven Definition of Style as the Scope of this Survey.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This survey aims to provide an overview of existing neural text style transfer
    approaches. To be concise, we will limit the scope to the most common settings
    of existing literature. Specifically, most deep learning work on TST adopts a
    data-driven definition of style, and the scope of this survey covers the styles
    in currently available TST datasets. The data-driven definition of style is different
    from the linguistic or rule-based definition of style, which theoretically constrains
    what constitutes a style and what not, such as a style guide (e.g., American Psychological
    Association, [1983](#bib.bib1)) that requires that formal text not include any
    contraction, e.g., “isn’t.” The distinction of the two defintions of style is
    shown in Figure [1](#S2.F1 "Figure 1 ‣ 2.1.0.0.1 Linguistic Definition of Style.
    ‣ 2.1 How to Define Style? ‣ 2 What Is Text Style Transfer? ‣ Deep Learning for
    Text Style Transfer: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: With the rise of deep learning methods of TST, the data-driven definition of
    style extends the linguistic style to a broader concept – the general attributes
    in text. It regards “style” as the attributes that vary across datasets, as opposed
    to the characteristics that stay invariant Mou and Vechtomova ([2020](#bib.bib131)).
    The reason is that deep learning models, which are the focus of this survey, need
    large corpora to learn the style from, but not all styles have well-matched large
    corpora. Therefore, apart from the very few manually-annotated datasets with linguistic
    style definitions, such as formality Rao and Tetreault ([2018](#bib.bib156)) and
    humor & romance Gan et al. ([2017](#bib.bib42)), many recent dataset collection
    works automatically look for meta-information to link a corpus to a certain attribute.
    A typical example is the widely used Yelp review dataset Shen et al. ([2017](#bib.bib182)),
    where reviews with low ratings are put into the negative corpus, and reviews with
    high ratings are put into the positive corpus, although the negative vs. positive
    opinion is not a style that belongs to the linguistic definition, but more of
    a content-related attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most methods mentioned in this survey can be applied to scenarios that follow
    this data-driven definition of style. As a double-sided sword, the prerequisite
    for most methods is that there exist style-specific corpora for each style of
    interest, either parallel or non-parallel. Note that there can be future works
    that do not take such an assumption, which will be discussed in Section [6.3](#S6.SS3
    "6.3 Loosening the Style-Specific Dataset Assumptions ‣ 6 Research Agenda ‣ Deep
    Learning for Text Style Transfer: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.0.0.3 Comparison of the Two Definitions.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are two phenomena rising from the data-driven definition of style as opposed
    to the linguistic style. One is that the data-driven definition of style can include
    a broader range of attributes including content and topic preferences of the text.
    The other is that data-driven styles, if collected through automatic classification
    by meta-information such as ratings, user information, and source of text, can
    be more ambiguous than the linguistically defined styles. As shown in Jin et al.
    ([2019](#bib.bib82), Section 4.1.1), some automatically collected datasets have
    a concerningly high undecideable rate and inter-annotator disagreement rate when
    the annotators are asked to associate the dataset with human-defined styles such
    as political slant and gender-specific tones.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of the data-driven style is that it can marry well with deep
    learning methods because most neural models learn the concept of style by learning
    to distinguish the multiple style corpora. For the (non-data-driven) linguistic
    style, although it is under-explored in the existing deep learning works of TST,
    we provide in Section [6.3](#S6.SS3 "6.3 Loosening the Style-Specific Dataset
    Assumptions ‣ 6 Research Agenda ‣ Deep Learning for Text Style Transfer: A Survey")
    a discussion of how potential future works can learn TST of linguistics styles
    with no matched data.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Task Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We define the main notations used in this survey in Table [2](#S2.T2 "Table
    2 ‣ 2.2 Task Formulation ‣ 2 What Is Text Style Transfer? ‣ Deep Learning for
    Text Style Transfer: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Notation of each variable and its corresponding meaning.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Notation | Meaning |'
  prefs: []
  type: TYPE_TB
- en: '| Attribute | $a$ | An attribute value, e.g., the formal style |'
  prefs: []
  type: TYPE_TB
- en: '| ${a}^{\prime}$ | An attribute value different from $a$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbb{A}$ | A predefined set of attribute values |'
  prefs: []
  type: TYPE_TB
- en: '| $a_{i}$ | The $i$-th attribute value in $\mathbb{A}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Sentence | $\bm{x}$ | A sentence with attribute value $a$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\bm{x}^{\prime}$ | A sentence with attribute value ${a}^{\prime}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\bm{X}_{i}$ | A corpus of sentences with attribute value $a_{i}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\bm{x}_{i}$ | A sentence from the corpus $\bm{X}_{i}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\widehat{\bm{x}^{\prime}}$ | Attribute-transferred sentence of $\bm{x}$
    learned by the model |'
  prefs: []
  type: TYPE_TB
- en: '| Model | $E$ | Encoder of a TST model |'
  prefs: []
  type: TYPE_TB
- en: '| $G$ | Generator of a TST model |'
  prefs: []
  type: TYPE_TB
- en: '| $f_{c}$ | Attribute classifier |'
  prefs: []
  type: TYPE_TB
- en: '| $\bm{\theta}_{\mathrm{E}}$ | Parameters of the encoder |'
  prefs: []
  type: TYPE_TB
- en: '| $\bm{\theta}_{\mathrm{G}}$ | Parameters of the generator |'
  prefs: []
  type: TYPE_TB
- en: '| $\bm{\theta}_{f_{c}}$ | Parameters of the attribute classifier |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding | $\bm{z}$ | Latent representation of text, i.e., $\bm{z}\overset{\Delta}{=}E(\bm{x})$
    |'
  prefs: []
  type: TYPE_TB
- en: '| $\bm{a}$ | Latent representation of the attribute value in text |'
  prefs: []
  type: TYPE_TB
- en: 'As mentioned previously in Section [2.1.0.0.2](#S2.SS1.SSS0.P2 "2.1.0.0.2 Data-Driven
    Definition of Style as the Scope of this Survey. ‣ 2.1 How to Define Style? ‣
    2 What Is Text Style Transfer? ‣ Deep Learning for Text Style Transfer: A Survey"),
    most neural approaches assume a given set of attribute values $\mathbb{A}$, and
    each attribute value has its own corpus. For example, if the task is about formality
    transfer, then for the attribute of text formality, there are two attribute values,
    $a$ = “formal” and $a^{\prime}$ = “informal,” corresponding to a corpus $\bm{X}_{1}$
    of formal sentences and another corpus $\bm{X}_{2}$ of informal sentences. The
    style corpora can be parallel or non-parallel. Parallel data means that each sentence
    with the attribute $a$ is paired with a counterpart sentence with another attribute
    ${a}^{\prime}$. In contrast, non-parallel data only assumes mono-style corpora.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Existing Subtasks with Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 3: List of common subtasks of TST and their corresponding attribute values
    and datasets. For datasets with multiple attribute-specific corpora, we report
    their sizes by the number of sentences of the smallest of all corpora. We also
    report whether the dataset is parallel (Pa?).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Attribute Values | Datasets | Size | Pa? |'
  prefs: []
  type: TYPE_TB
- en: '| Style Features |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Formality | Informal$\leftrightarrow$Formal | GYAFC³³3GYAFC data: [https://github.com/raosudha89/GYAFC-corpus](https://github.com/raosudha89/GYAFC-corpus)  (Rao
    and Tetreault, [2018](#bib.bib156)) | 50K | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| XFORMAL⁴⁴4GYAFC data: [https://github.com/Elbria/xformal-FoST](https://github.com/Elbria/xformal-FoST)  (Briakou
    et al., [2021b](#bib.bib18)) | 1K | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Politeness | Impolite$\rightarrow$Polite | Politeness⁵⁵5Politeness data:
    [https://github.com/tag-and-generate/politeness-dataset](https://github.com/tag-and-generate/politeness-dataset)  (Madaan
    et al., [2020](#bib.bib119)) | 1M | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Gender | Masculine$\leftrightarrow$Feminine | Yelp Gender⁶⁶6The Yelp Gender
    dataset is from the Yelp Challenge [https://www.yelp.com/dataset](https://www.yelp.com/dataset)
    and its preprocessing needs to follow Prabhumoye et al. ([2018](#bib.bib149)).  (Prabhumoye
    et al., [2018](#bib.bib149)) | 2.5M | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Humor& Romance | Factual$\leftrightarrow$Humorous$\leftrightarrow$ Romantic
    | FlickrStyle⁷⁷7FlickrStyle data: [https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/imagecaption](https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/imagecaption)  (Gan
    et al., [2017](#bib.bib42)) | 5K | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Biasedness | Biased$\rightarrow$Neutral | Wiki Neutrality⁸⁸8Wiki Neutrality
    data: [http://bit.ly/bias-corpus](http://bit.ly/bias-corpus)  (Pryzant et al.,
    [2020](#bib.bib150)) | 181K | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Toxicity | Offensive$\rightarrow$Non-offensive | Twitter dos Santos, Melnyk,
    and Padhi ([2018](#bib.bib172)) | 58K | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Reddit dos Santos, Melnyk, and Padhi ([2018](#bib.bib172)) | 224K |'
  prefs: []
  type: TYPE_TB
- en: '| Reddit Politics Tran, Zhang, and Soleymani ([2020](#bib.bib198)) | 350K |'
  prefs: []
  type: TYPE_TB
- en: '| Authorship | Shakespearean$\leftrightarrow$Modern | Shakespeare Xu et al.
    ([2012](#bib.bib221)) | 18K | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Different Bible translators | Bible⁹⁹9Bible data: [https://github.com/keithecarlson/StyleTransferBibleData](https://github.com/keithecarlson/StyleTransferBibleData)  (Carlson,
    Riddell, and Rockmore, [2018](#bib.bib25)) | 28M |  |'
  prefs: []
  type: TYPE_TB
- en: '| Simplicity | Complicated$\rightarrow$Simple | PWKP Zhu, Bernhard, and Gurevych
    ([2010](#bib.bib238)) | 108K | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Expert (den Bercken, Sips, and Lofi, [2019](#bib.bib13)) | 2.2K | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| MIMIC-III^(10)^(10)10MIMIC-III data: Request access at [https://mimic.physionet.org/gettingstarted/access/](https://mimic.physionet.org/gettingstarted/access/)
    and follow the preprocessing of Weng, Chung, and Szolovits ([2019](#bib.bib209))  Weng,
    Chung, and Szolovits ([2019](#bib.bib209)) | 59K | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| MSD^(11)^(11)11MSD data: [https://srhthu.github.io/expertise-style-transfer/](https://srhthu.github.io/expertise-style-transfer/)  (Cao
    et al., [2020](#bib.bib23)) | 114K | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Engagingness | Plain$\rightarrow$Attractive | Math^(12)^(12)12Math data:
    [https://gitlab.cs.washington.edu/kedzior/Rewriter/](https://gitlab.cs.washington.edu/kedzior/Rewriter/)  Koncel-Kedziorski
    et al. ([2016](#bib.bib93)) | $<$1K | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| TitleStylist^(13)^(13)13TitleStylist data: [https://github.com/jind11/TitleStylist](https://github.com/jind11/TitleStylist)  (Jin
    et al., [2020a](#bib.bib80)) | 146K | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Content Preferences |  |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment | Positive$\leftrightarrow$Negative | Yelp^(14)^(14)14Yelp data:
    [https://github.com/shentianxiao/language-style-transfer](https://github.com/shentianxiao/language-style-transfer)  (Shen
    et al., [2017](#bib.bib182)) | 250K | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon^(15)^(15)15Amazon data: [https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/amazon](https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/amazon)  (He
    and McAuley, [2016](#bib.bib63)) | 277K |  |'
  prefs: []
  type: TYPE_TB
- en: '| Topic | Entertainment$\leftrightarrow$Politics | Yahoo! Answers^(16)^(16)16Yahoo!
    Answers data: [https://webscope.sandbox.yahoo.com/catalog.php?datatype=l&did=11](https://webscope.sandbox.yahoo.com/catalog.php?datatype=l&did=11)  Huang
    et al. ([2020](#bib.bib74)) | 153K | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Politics | Democratic$\leftrightarrow$Republican | Political^(17)^(17)17Political
    data: [https://nlp.stanford.edu/robvoigt/rtgender/](https://nlp.stanford.edu/robvoigt/rtgender/)  (Voigt
    et al., [2018](#bib.bib204)) | 540K | ✗ |'
  prefs: []
  type: TYPE_TB
- en: 'We list the common subtasks and corresponding datasets for neural TST in Table [3](#S2.T3
    "Table 3 ‣ 2.3 Existing Subtasks with Datasets ‣ 2 What Is Text Style Transfer?
    ‣ Deep Learning for Text Style Transfer: A Survey"). The attributes of interest
    vary from style features (e.g., formality and politeness) to content preferences
    (e.g., sentiment and topics). Each task of which will be elaborated below.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.0.0.1 Formality.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Adjusting the extent of formality in text was first proposed by Hovy ([1987](#bib.bib69)).
    It is one of the most distinctive stylistic aspects that can be observed through
    many linguistic phenomena, such as more full names (e.g., “television”) instead
    of abbreviations (e.g., “TV”), and more nouns (e.g., “solicitation”) instead of
    verbs (e.g., “request”). The formality dataset, Grammarly’s Yahoo Answers Formality
    Corpus (GYAFC) Rao and Tetreault ([2018](#bib.bib156)), contains 50K formal-informal
    pairs retrieved by first getting 50K informal sentences from the Yahoo Answers
    corpus, and then recruiting crowdsource workers to rewrite them in a formal way.
    Briakou et al. ([2021b](#bib.bib18)) extend the formality dataset to a multilingual
    version with three more languages, Brazilian Portuguese, French, and Italian.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.0.0.2 Politeness.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Politeness transfer Madaan et al. ([2020](#bib.bib119)) aims to control the
    politeness in text. For example, “Could you please send me the data?” is a more
    polite expression than “send me the data!”. Madaan et al. ([2020](#bib.bib119))
    compiled a dataset of 1.39 million automatically labeled instances from the raw
    Enron corpus Shetty and Adibi ([2004](#bib.bib183)). As politeness is culture-dependent,
    this dataset mainly focuses on politeness in North American English.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.0.0.3 Gender.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Linguistic phenomena related to gender is a heated research area Trudgill ([1972](#bib.bib199));
    Lakoff ([1973](#bib.bib99)); Tannen ([1990](#bib.bib194)); Argamon et al. ([2003](#bib.bib3));
    Boulis and Ostendorf ([2005](#bib.bib16)). The gender-related TST dataset is proposed
    by Prabhumoye et al. ([2018](#bib.bib149)) who compiled 2.5M reviews from Yelp
    Dataset Challenge that are labeled with the gender of the user.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.0.0.4 Humor&Romance.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Humor and romance are some artistic attributes that can provide readers with
    joy. Li et al. ([2018](#bib.bib107)) first propose to borrow the FlickrStyle stylized
    caption dataset Gan et al. ([2017](#bib.bib42)) from the computer vision domain.
    In the FlickrStyle image caption dataset, each image has three captions, with
    a factual, a humorous, and a romantic style, respectively. By keeping only the
    captions of the three styles, Li et al. ([2018](#bib.bib107)) created a subset
    of the FlickrStyle dataset of 5K parallel (factual, humorous, romantic) triplets.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.0.0.5 Biasedness.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Wiki Neutrality Corpus Pryzant et al. ([2020](#bib.bib150)) is the first corpus
    of biased and neutralized sentence pairs. It is collected from Wikipedia revisions
    that adjusted the tone of existing sentences to a more neutral voice. The types
    of bias in the biased corpus include framing bias, epistemological bias, and demographic
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.0.0.6 Toxicity.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Another important use of TST is to fight against offensive language. Tran, Zhang,
    and Soleymani ([2020](#bib.bib198)) collect 350K offensive sentences and 7M non-offensive
    sentences by crawling sentences from Reddits using a list of restricted words.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.0.0.7 Authorship.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Changing the tone of the author is an artistic use of text style transfer. Xu
    et al. ([2012](#bib.bib221)) created an aligned corpus of 18K pairs of Shakespearean
    English and its modern English translation. Carlson, Riddell, and Rockmore ([2018](#bib.bib25))
    collected 28M parallel data from English versions of the Bible by different translators.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.0.0.8 Simplicity.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Another important use of TST is to lower the language barrier for readers, such
    as translating legalese, medical jargon, or other professional text into simple
    English, to avoid discrepancies between expert wordings and laymen’s understanding
    Tan and Goonawardene ([2017](#bib.bib193)). Common tasks include converting standard
    English Wikipedia into Simple Wikipedia whose dataset contains 108K samples Zhu,
    Bernhard, and Gurevych ([2010](#bib.bib238)). Another task is to simplify medical
    descriptions to patient-friendly text, including a dataset with 2.2K samples (den
    Bercken, Sips, and Lofi, [2019](#bib.bib13)), another non-parallel dataset with
    59K free-text discharge summaries compiled from MIMIC-III Weng, Chung, and Szolovits
    ([2019](#bib.bib209)), and a more recent parallel dataset with 114K samples compiled
    from from the health reference Merck Manuals (MSD) where discussions on each medical
    topic has one version for professionals, and the other for consumers Cao et al.
    ([2020](#bib.bib23)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.0.0.9 Sentiment.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Sentiment modification is the most popular task in previous work on TST. It
    aims to change the sentiment polarity in reviews, for example from a negative
    review to a positive review, or vice versa. There is also work on transferring
    sentiments on fine-grained review ratings, e.g., 1-5 scores. Commonly used datasets
    include Yelp reviews Shen et al. ([2017](#bib.bib182)), and Amazon product reviews
    He and McAuley ([2016](#bib.bib63)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.0.0.10 Topic.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are a few works that cover topic transfer. For example, Huang et al. ([2020](#bib.bib74))
    form a two-topic corpus by compiling Yahoo! Answers under two topics, entertainment
    and politics, respectively. There is also a recent dataset with 21 text styles
    such as Sciences, Sport, Politics, and others Zeng, Shoeybi, and Liu ([2020](#bib.bib227)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.0.0.11 Political Slant.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Political slant transfer proposed by Prabhumoye et al. ([2018](#bib.bib149))
    aims to transfer the political view in text. For example, a republican’s comment
    can be “defund all illegal immigrants,” while democrats are more likely to support
    humanistic actions towards immigrants. The political slant dataset Voigt et al.
    ([2018](#bib.bib204)) is collected from comments on Facebook posts of the United
    States Senate and House members. The dataset uses top-level comments directly
    responding to the posts of a democratic or republican Congressperson. There are
    540K training, 4K development, and 56K test instances in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.0.0.12 Combined Attributes.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Lample et al. ([2019](#bib.bib102)) propose a more challenging setting of text
    attribute transfer – multi-attribute transfer. For example, the source sentence
    can be a positive review on an Asian restaurant written by a male reviewer, and
    the target sentence is a negative review on an American restaurant written by
    a female. Each of their datasets has 1-3 independent categories of attributes.
    Their first dataset is FYelp, which is compiled from the Yelp Dataset Challenge,
    labeled with sentiment (positive or negative), gender (male or female), and eatery
    category (American, Asian, bar, dessert, or Mexican). Their second dataset, Amazon,
    which is based on the Amazon product review dataset Li et al. ([2018](#bib.bib107)),
    contains the following attributes: sentiment (positive or negative), and product
    category (book, clothing, electronics, movies, or music). Their third dataset,
    Social Media Content dataset, collected from internal Facebook data which is private,
    contains gender (male or female), age group (18-24 or 65+), and writer-annotated
    feeling (relaxed or annoyed).'
  prefs: []
  type: TYPE_NORMAL
- en: 3 How to Evaluate Style Transfer?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A successful style-transferred output not only needs to demonstrate the correct
    target style, but also, due to the uncontrollability of neural networks, we need
    to verify that it preserves the original semantics, and maintains natural language
    fluency. Therefore, the commonly used practice of evaluation considers the following
    three criteria: (1) transferred style strength, (2) semantic preservation, and
    (3) fluency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first introduce the practice of automatic evaluation on the three criteria,
    discuss the benefits and caveats of automatic evaluation, and then introduce human
    evaluation as a remedy for some of the intrinsic weaknesses of automatic evaluation.
    Finally, we will suggest some standard practice of TST evaluation for future work.
    The overview of evaluation methods regarding each criterion is listed in Table [4](#S3.T4
    "Table 4 ‣ 3 How to Evaluate Style Transfer? ‣ Deep Learning for Text Style Transfer:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Overview of evaluation methods for each criterion.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Criterion | Automatic Evaluation | Human Evaluation |'
  prefs: []
  type: TYPE_TB
- en: '| Overall | BLEU with gold references | Rating or ranking |'
  prefs: []
  type: TYPE_TB
- en: '|    - Transferred Style Strength | Accuracy by a separately trained style
    classifier | Rating or ranking |'
  prefs: []
  type: TYPE_TB
- en: '|    - Semantic Preservation | BLEU/ROUGE/etc. with (modified) inputs | Rating
    or ranking |'
  prefs: []
  type: TYPE_TB
- en: '|    - Fluency | Perplexity by a separately trained LM | Rating or ranking
    |'
  prefs: []
  type: TYPE_TB
- en: 3.1 Automatic Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatic evaluation provides an economic, reproducible, and scalable way to
    assess the quality of generation results. However, due to the complexities of
    natural language, each metric introduced below can address certain aspects, but
    also has intrinsic blind spots.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.0.0.1 BLEU with Gold References.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Similar to many text generation tasks, text style transfer also has human-written
    references on several datasets (e.g., Yelp, Captions, etc.), so it is common to
    use the BLEU score Papineni et al. ([2002](#bib.bib142)) between the gold references
    and model outputs. Using BLEU to evaluate TST models has been seen across pre-deep
    learning works Xu et al. ([2012](#bib.bib221)); Jhamtani et al. ([2017](#bib.bib78))
    and deep learning approaches Rao and Tetreault ([2018](#bib.bib156)); Li et al.
    ([2018](#bib.bib107)); Jin et al. ([2019](#bib.bib82)).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three problems with using BLEU between the gold references and model
    outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: Problem 1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It mainly evaluates content and simply copying the input can result in high
    BLEU scores
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Problem 2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BLEU is shown to have low correlation with human evaluation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Problem 3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some datasets do not have human-written references
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Problem [1](#S3.I1.i1 "item 1 ‣ 3.1.0.0.1 BLEU with Gold References. ‣ 3.1
    Automatic Evaluation ‣ 3 How to Evaluate Style Transfer? ‣ Deep Learning for Text
    Style Transfer: A Survey"): Different from machine translation, where using BLEU
    only is sufficient, TST has to consider the caveat that simply copying the input
    sentence can achieve high BLEU scores with the gold references on many datasets
    (e.g., $\sim$40 on Yelp, $\sim$20 on Humor&Romance, $\sim$50 for informal-to-formal
    style transfer, and $\sim$30 for formal-to-informal style transfer). It is because
    most text rewrites have a large extent of n-gram overlap with the source sentence.
    In contrast, machine translation does not have this concern, because the vocabulary
    of its input and output are different, and copying the input sequence does not
    give high BLEU scores. A possible fix to consider is to combine BLEU with PINC
    Chen and Dolan ([2011](#bib.bib29)) as in paraphrasing Xu et al. ([2012](#bib.bib221));
    Jhamtani et al. ([2017](#bib.bib78)). By using PINC and BLEU as a 2-dimensional
    metric, we can minimize the n-gram overlap with the source sentence but maximize
    the n-gram overlap with the reference sentences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem [2](#S3.I1.i2 "item 2 ‣ 3.1.0.0.1 BLEU with Gold References. ‣ 3.1
    Automatic Evaluation ‣ 3 How to Evaluate Style Transfer? ‣ Deep Learning for Text
    Style Transfer: A Survey")&[3](#S3.I1.i3 "item 3 ‣ 3.1.0.0.1 BLEU with Gold References.
    ‣ 3.1 Automatic Evaluation ‣ 3 How to Evaluate Style Transfer? ‣ Deep Learning
    for Text Style Transfer: A Survey"): Other problems include insufficient correlation
    of BLEU with human evaluations (e.g., $\leq$0.30 w.r.t. human-rated grammaticality
    shown in Li et al. ([2018](#bib.bib107)) and $\leq$0.45 w.r.t. human evaluations
    shown in Mir et al. ([2019](#bib.bib130))), and the unavailability of human-written
    references for some datasets (e.g., gender and political datasets Prabhumoye et al.
    ([2018](#bib.bib149)), and the politeness dataset Madaan et al. ([2020](#bib.bib119))).
    A commonly used fix is to make the evaluation more fine-grained using three different
    independent aspects, namely transferred style strength, semantic preservation,
    and fluency, which will be detailed below.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.0.0.2 Transferred Style Strength.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To automatically evaluate the transferred style strength, most works separately
    train a style classifier to distinguish the attributes Hu et al. ([2017](#bib.bib72));
    Shen et al. ([2017](#bib.bib182)); Fu et al. ([2018](#bib.bib41)); Li et al. ([2018](#bib.bib107));
    Prabhumoye et al. ([2018](#bib.bib149)).^(18)^(18)18Note that this style classifier
    usually report 80+% or 90+% accuracy, and we will discuss the the problem of false
    positives and false negatives in last paragraph of automatic evaluation. This
    classifier is used to judge whether each sample generated by the model conforms
    to the target attribute. The transferred style strength is calculated as $\frac{\text{\#
    test samples correctly classified}}{\text{\# all test samples}}$. Li et al. ([2018](#bib.bib107))
    shows that the attribute classifier correlates well with human evaluation on some
    datasets (e.g., Yelp and Captions), but has almost no correlation with others
    (e.g., Amazon). The reason is that some product genres has a dominant number of
    positive or negative reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.0.0.3 Semantic Preservation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Many metrics can be applied to measure the similarity between the input and
    output sentence pairs, including BLEU (Papineni et al., [2002](#bib.bib142)),
    ROUGE (Lin and Och, [2004](#bib.bib112)), METEOR (Banerjee and Lavie, [2005](#bib.bib7)),
    chrF (Popović, [2015](#bib.bib146)), Word Mover Distance (WMD) (Kusner et al.,
    [2015](#bib.bib97)). Recently, some additional deep-learning-based metrics are
    proposed, such as cosine similarity based on sentence embeddings (Fu et al., [2018](#bib.bib41)),
    and BERTScore (Zhang et al., [2020](#bib.bib230)). There are also evaluation metrics
    that are specific for TST such as the Part-of-Speech distance (Tian, Hu, and Yu,
    [2018](#bib.bib195)). Another newly proposed metric is to first delete all attribute-related
    expressions in the text, and then apply the above similarity evaluations Mir et al.
    ([2019](#bib.bib130)). Among all the metrics, Mir et al. ([2019](#bib.bib130));
    Yamshchikov et al. ([2021](#bib.bib222)) showed that METEOR and WMD have better
    correlation with human evaluation than BLEU, although, in practice, BLEU is the
    most widely used metric to evaluate the semantic similarity between the source
    sentence and style-transferred output Yang et al. ([2018](#bib.bib224)); Madaan
    et al. ([2020](#bib.bib119)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.0.0.4 Fluency.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Fluency is a basic requirement for natural language outputs. To automate this
    evaluation, perplexity is calculated via a language model (LM) pretrained on the
    training data of all attributes (Yang et al., [2018](#bib.bib224)). However, the
    effectiveness of perplexity remains debateable, as Pang and Gimpel ([2019](#bib.bib141))
    showed its high correlation with human ratings of fluency, whereas Mir et al.
    ([2019](#bib.bib130)) suggested no significant correlation between perplexity
    and human scores. We note that perplexity by LM can suffer from the following
    undesired properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Biased towards shorter sentences than longer sentences.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the same meaning, less frequent words will have worse perplexity (e.g.,
    agreeable) than frequent words (e.g., good).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A sentence’s own perplexity will change if the sentence prior to it changes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LMs are not good enough yet.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LMs do not necessarily handle well the domain shift between their training corpus
    and the style-transferred text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perplexity scores produced by LMs are sensitive to the training corpora, LM
    architecture and configuration, as well as optimization configuration. Therefore,
    different models’ outputs must be evaluated by exactly the same LM for fair comparison,
    which adds more difficulty to benchmarking.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Such properties will bias against certain models, which is not desired for an
    evaluation metric. As a potential remedy, future researchers can try grammaticality
    checker to score the generated text.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.0.0.5 Task-Specific Criteria.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As TST can serve as a component for other downstream applications, some task-specific
    criteria are also proposed to evaluate the quality of generated text. For example,
    Reiter, Robertson, and Osman ([2003](#bib.bib159)) evaluated the effect of their
    tailored text on reducing smokers’ intent to smoke through clinical trials. Jin
    et al. ([2020a](#bib.bib80)) applied TST to generate eye-catchy headlines so they
    have an attractive score, and future works in this direction can also test the
    click-through rates. Hu et al. ([2017](#bib.bib72)) evaluated how the generated
    text as augmented data can improve the downstream attribute classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.0.0.6 Tips for Automatic Metrics.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For the evaluation metrics that rely on the pretrained models, namely the style
    classifier and LM, we need to beware of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The pretrained models for automatic evaluation should be separate from the proposed
    TST model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Machine learning models can be imperfect, so we should be aware of the potential
    false positives and false negatives
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The pretrained models are imperfect in the sense that they will favor towards
    a certain type of methods
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the first point, it is important to not use the same style classifier or
    LM in the proposed TST approach, otherwise it can overfit or hack the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: For the second point, we need to understand what can be the false positives
    and false negatives of the generated outputs. An illustrative example is that
    if the style classifier only reports 80+% performance (e.g., on the gender dataset
    Prabhumoye et al. ([2018](#bib.bib149)) and Amazon dataset Li et al. ([2018](#bib.bib107))),
    even perfect style rewrites can only score 80+%, but maybe an imperfect model
    can score 90% because it can resemble the imperfect style classification model
    more and makes advantage of the false positives. Other reasons for false positives
    can be adversarial attacks. Jin et al. ([2020b](#bib.bib81)) showed that merely
    paraphrasing by synonyms can drop the performance of high-accuracy classification
    models from TextCNN Kim ([2014](#bib.bib91)) to BERT Devlin et al. ([2019](#bib.bib36))
    by 90+%. Therefore, higher scores by the style classifier does not necessarily
    indicate more successful transfer. Moreover, the style classifier can produce
    false negatives if there is a distribution shift between the training data and
    style-transferred outputs. For example, in the training corpus, a product may
    appear often with the positive attribute, and in the style-transferred outputs,
    this product co-occurs with the opposite, negative attribute. Such false negatives
    are observed on the Amazon product review dataset Li et al. ([2018](#bib.bib107)).
    On the other hand, the biases of the LM correlate with sentence length, synonym
    replacement, and prior context.
  prefs: []
  type: TYPE_NORMAL
- en: The third point is a direct result implied by the second point, so in practice,
    we need to keep in mind and check whether the proposed model takes advantage of
    the evaluation metrics or makes improvements that are generalizable.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Human Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to the pros and cons of the automatic evaluation metrics mentioned
    above, human evaluation stands out for its flexibility and comprehensiveness.
    For example, when asking humans to evaluate the fluency, we do not need to worry
    for the bias towards shorter sentences as in the LM. We can also design criteria
    that are not computationally easy such as comparing and ranking the outputs of
    multiple models. There are several ways to conduct human evaluation. In terms
    of evaluation types, there are pointwise scoring, namely asking humans to provide
    absolute scores of the model outputs, and pairwise comparison, namely asking humans
    to judge which of the two outputs is better, or providing a ranking for multiple
    outputs. In terms of the criteria, humans can provide overall evaluation, or separate
    scores for transferred style strength, semantic preservation, and fluency.
  prefs: []
  type: TYPE_NORMAL
- en: However, the well-known limitations of human evaluation are cost and irreproducibility.
    Performing human evaluations can be time consuming, which may result in significant
    time and financial costs. Moreover, the human evaluation results in two studies
    are often not directly comparable, because human evaluation results tend to be
    subjective and not easily irreproducible Belz et al. ([2020](#bib.bib12)). Moreover,
    some styles are very difficult to evaluate without expertise and extensive reading
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: As a remedy, we encourage future researchers to report inter-rater agreement
    scores such as the Cohen’s kappa Cohen ([1960](#bib.bib33)) and Krippendorff’s
    alpha Krippendorff ([2018](#bib.bib95)). Briakou et al. ([2021a](#bib.bib17))
    also recommends to standardize and describe evaluation protocols (e.g., linguistic
    background of the annotators, compensation, detailed annotation instructions for
    each evaluation aspect), and release annotations.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.0.0.1 Tips for Human Evaluation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As common practice, most works use 100 outputs for each style transfer direction
    (e.g., 100 outputs for formal$\rightarrow$informal, and 100 outputs for informal$\rightarrow$formal),
    and two human annotators for each task (Shen et al., [2017](#bib.bib182); Fu et al.,
    [2018](#bib.bib41); Li et al., [2018](#bib.bib107)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Suggested Evaluation Settings for Future Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, the experiments of various TST works do not adopt the same setting,
    making it difficult to do head-to-head comparison among the empirical results
    of multiple works. Although it is reasonable to customize the experimental settings
    according to the needs of a certain work, it is suggested to at least use the
    standard setting in at least one of the many reported experiments, to make it
    easy to compare with previous and future works. For example, at least (1) experiment
    on at least one commonly used dataset, (2) list up-to-date best-performing previous
    models as baselines, (3) report on a superset of the most commonly used metrics,
    and (4) release system outputs.
  prefs: []
  type: TYPE_NORMAL
- en: For (1), we suggest that future works use at least one of the most commonly
    used benchmark datasets, such as the Yelp data prepreocessed by Shen et al. ([2017](#bib.bib182))
    and its five human references provided by Jin et al. ([2019](#bib.bib82)), Amazon
    data preprocessed by Li et al. ([2018](#bib.bib107)), and formality data provided
    by Rao and Tetreault ([2018](#bib.bib156)).
  prefs: []
  type: TYPE_NORMAL
- en: For (2), we suggest that future works actively check the latest style transfer
    papers curated at [https://github.com/fuzhenxin/Style-Transfer-in-Text](https://github.com/fuzhenxin/Style-Transfer-in-Text)
    and our repository [https://github.com/zhijing-jin/Text_Style_Transfer_Survey](https://github.com/zhijing-jin/Text_Style_Transfer_Survey),
    and compare with the state-of-the-art performances instead of older ones. We also
    call for more reproducibility in the community, including source codes and evaluation
    codes, because, for example, there are several different scripts to evaluate the
    BLEU scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'For (3), since no single evaluation metric is perfect and comprehensive enough
    for TST, it is strongly suggested to use both human and automatic evaluation on
    three criteria. In evaluation, apart from customized use of metrics, we suggest
    that most future works to include at least the following evaluation practices:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Human evaluation: rate at least two state-of-the-art models according to the
    curated paper lists'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Automatic evaluation: at least report the BLEU score with all available references
    if there exist human-written references (e.g., the five references for the Yelp
    dataset provided by Jin et al. ([2019](#bib.bib82))), and BLEU with the input
    only when there are no human-written references.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For (4), it will also be very helpful to provide system outputs for each TST
    paper, so that future works can better reproduce both human and automatic evaluation
    results. Note that releasing system outputs can help future works’ comparison
    of automatic evaluation results, because there can be different scripts to evaluate
    the BLEU scores, as well as different style classifiers and LM. It will be a great
    addition to the TST community if future work can establish an online leaderboard,
    let existing works upload their output files, and automatically evaluate the model
    outputs using a standard set of automatic evaluation scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Methods on Parallel Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Over the last several years, various methods have been proposed for text style
    transfer. In general, they can be categorized based on whether the dataset has
    parallel text with different styles or several non-parallel mono-style corpora.
    The rightmost column “Pa?” in Table [3](#S2.T3 "Table 3 ‣ 2.3 Existing Subtasks
    with Datasets ‣ 2 What Is Text Style Transfer? ‣ Deep Learning for Text Style
    Transfer: A Survey") shows whether there exist parallel data for each TST subtask.
    In this section, we will cover TST methods on parallel datasets, and in Section [5](#S5
    "5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey")
    we will detail the approaches on non-parallel datasets. To ease the understanding
    for the readers, we will in most cases explain TST on one attribute between two
    values, such as transferring the formality between informal and formal tones,
    which can potentially be extended to multiple attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most methods adopt the standard neural sequence-to-sequence (seq2seq) model
    with the encoder-decoder architecture, which was initially developed for neural
    machine translation (NMT) (Sutskever, Vinyals, and Le, [2014](#bib.bib191); Bahdanau,
    Cho, and Bengio, [2015](#bib.bib6); Cho et al., [2014](#bib.bib32)) and extensively
    seen on text generation tasks such as summarization Rush, Chopra, and Weston ([2015](#bib.bib169))
    and many others Song et al. ([2019](#bib.bib186)). The encoder-decoder seq2seq
    model can be implemented by either LSTM as in  Rao and Tetreault ([2018](#bib.bib156));
    Shang et al. ([2019](#bib.bib178)) or Transformer Vaswani et al. ([2017](#bib.bib202))
    as in Xu, Ge, and Wei ([2019](#bib.bib220)). Copy mechanism Gülçehre et al. ([2016](#bib.bib57));
    See, Liu, and Manning ([2017](#bib.bib175)) is also added to better handle stretches
    of text which should not be changed (e.g., some proper nouns and rare words) (Gu
    et al., [2016](#bib.bib55); Merity et al., [2017](#bib.bib129)). Based on this
    architecture, recent works have developed multiple directions of improvement:
    multi-tasking, inference techniques, and data augmentation, which will be introduced
    below.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.0.0.0.1 Multi-Tasking.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In addition to the seq2seq learning on paired attributed-text, Xu, Ge, and
    Wei ([2019](#bib.bib220)) propose adding three other loss functions: (1) classifier-guided
    loss, which is calculated using a well-trained attribute classifier and encourages
    the model to generate sentences conforming to the target attribute, (2) self-reconstruction
    loss, which encourages the seq2seq model to reconstruct the input itself by specifying
    the desired style the same as the input style, and (3) cycle loss, which first
    transfers the input sentence to the target attribute and then transfers the output
    back to its original attribute. Each of the three losses can gain performance
    improvement of 1-5 BLEU points with the human references Xu, Ge, and Wei ([2019](#bib.bib220)).
    Another type of multi-tasking is to jointly learn TST and machine translation
    from French to English, which improves the performance by 1 BLEU score with human-written
    references Niu, Rao, and Carpuat ([2018](#bib.bib137)). Specific for formality
    transfer, Zhang, Ge, and Sun ([2020](#bib.bib232)) multi-task TST and grammar
    error correction (GEC) so that knowledge from GEC data can be transferred to the
    informal-to-formal style transfer task.'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the additional loss designs, using the pretrained language model
    GPT-2 (Radford et al., [2019](#bib.bib154)) can lead to improvement by at least
    7 BLEU scores with human references Wang et al. ([2019](#bib.bib207)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.0.0.0.2 Inference Techniques.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To avoid the model copying too many parts of the input sentence and not performing
    sufficient edits to flip the attribute, Kajiwara ([2019](#bib.bib84)) first identify
    words in the source sentence requiring replacement, and then change the words
    by negative lexically constrained decoding (Post and Vilar, [2018](#bib.bib147))
    that avoids naive copying. Since this method only changes the beam search process
    for model inference, it can be applied to any text style transfer model without
    model re-training.
  prefs: []
  type: TYPE_NORMAL
- en: 4.0.0.0.3 Data Augmentation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Since style transfer data is expensive to annotate, there are not as many parallel
    datasets as in machine translation. Hence, various methods have been proposed
    for data augmentation to enrich the data. For example, Rao and Tetreault ([2018](#bib.bib156))
    first train a phrase-based machine translation (PBMT) model on a given parallel
    dataset and then use back-translation (Sennrich, Haddow, and Birch, [2016b](#bib.bib177))
    to construct a pseudo-parallel dataset as additional training data, which leads
    to an improvement of around 9.7 BLEU scores with respect to human written references.
  prefs: []
  type: TYPE_NORMAL
- en: Most recently, Zhang, Ge, and Sun ([2020](#bib.bib232)) use a data augmentation
    technique by making use of largely available online text. They scrape informal
    text from online forums and generate back-translations, i.e., informal English
    $\rightarrow$ a pivot language such as French $\rightarrow$ formal English, where
    the formality of the back-translated English text is ensured with a formality
    classifier that is used to only keep text that is classified as formal text.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Methods on Non-Parallel Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Parallel data for TST is difficult to obtain, and for some styles impossible
    to crowd-source (e.g., Mark Twain novels rewritten in Hemmingway’s style). Hence,
    the majority TST methods assume only non-parallel mono-style corpora, and investigate
    how to build deep learning models based on this constraint. In this section, we
    will introduce three main branches of TST methods: disentanglement (Section [5.1](#S5.SS1
    "5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey")), prototype editing (Section [5.2](#S5.SS2 "5.2 Prototype
    Editing ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer:
    A Survey")), and pseudo-parallel corpus construction (Section [5.3](#S5.SS3 "5.3
    Pseudo-Parallel Corpus Construction ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning
    for Text Style Transfer: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Disentanglement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Disentanglement-based models usually perform the following three actions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encode the text $\bm{x}$ with attribute $a$ into a latent representation $\bm{z}$
    (i.e., $\bm{x}\rightarrow\bm{z}$)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulate the latent representation $\bm{z}$ to remove the source attribute
    (i.e., $\bm{z}\rightarrow\bm{z}^{\prime}$)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decode into text $\bm{x}^{\prime}$ with the target attribute $a^{\prime}$ (i.e.,
    $\bm{z}^{\prime}\rightarrow\bm{x}^{\prime}$)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To build such models, the common workflow in disentanglement papers consists
    of the following three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Step 1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select a model as the backbone for the encoder-decoder learning (Section [5.1.1](#S5.SS1.SSS1
    "5.1.1 Encoder-Decoder Training Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel
    Data ‣ Deep Learning for Text Style Transfer: A Survey"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Step 2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select a manipulation method of the latent representation (Section [5.1.2](#S5.SS1.SSS2
    "5.1.2 Latent Representation Manipulation ‣ 5.1 Disentanglement ‣ 5 Methods on
    Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Step 3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the manipulation method chosen above, select (multiple) appropriate loss
    functions (Section [5.1.3](#S5.SS1.SSS3 "5.1.3 Training Objectives ‣ 5.1 Disentanglement
    ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The organization of this section starts with Section [5.1.1](#S5.SS1.SSS1 "5.1.1
    Encoder-Decoder Training Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel
    Data ‣ Deep Learning for Text Style Transfer: A Survey") which introduces the
    encoder-decoder training objectives that is used for Step [1](#S5.I2.i1 "item
    1 ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey"). Next, Section [5.1.2](#S5.SS1.SSS2 "5.1.2 Latent Representation
    Manipulation ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning
    for Text Style Transfer: A Survey") overviews three main approaches to manipulate
    the latent representation for Step [2](#S5.I2.i2 "item 2 ‣ 5.1 Disentanglement
    ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey"),
    and Section [5.1.3](#S5.SS1.SSS3 "5.1.3 Training Objectives ‣ 5.1 Disentanglement
    ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey")
    goes through a plethora of training objectives for Step [3](#S5.I2.i3 "item 3
    ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey"). Table [5](#S5.T5 "Table 5 ‣ 5.1 Disentanglement ‣
    5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey")
    provides an overview of existing models and their corresponding configurations.
    To give a rough idea of the effectiveness of each model, we show their performance
    on the Yelp dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Summary of existing disentanglement-based methods and the setting
    they adopted, with a reference of their performance on the Yelp dataset. For the
    settings, we include the encoder-decoder training method (Enc-Dec) in Section [5.1.1](#S5.SS1.SSS1
    "5.1.1 Encoder-Decoder Training Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel
    Data ‣ Deep Learning for Text Style Transfer: A Survey"), the disentanglement
    method (Disen.) in Section [5.1.2](#S5.SS1.SSS2 "5.1.2 Latent Representation Manipulation
    ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey"), and the loss types used to control style (Style Control)
    and content (Content Control) in Section [5.1.3](#S5.SS1.SSS3 "5.1.3 Training
    Objectives ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning
    for Text Style Transfer: A Survey"). For the model performance, we report automatic
    evaluation scores including BLEU with the one human reference (BL-Ref) provided
    by Li et al. ([2018](#bib.bib107)), accuracy (Acc.), BLEU with the input (BL-Inp)
    and perplexity (PPL). ^∗ marks numbers reported by Liu et al. ([2020](#bib.bib113)).
    Readers can refer to Hu, Lee, and Aggarwal ([2020](#bib.bib71)) for more complete
    performance results on Yelp.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Settings | Performance on Yelp |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Enc-Dec | Disen. | Style Control | Content Control | BL-Ref | Acc. (%)
    | BL-Inp | PPL$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Mueller, Gifford, and Jaakkola ([2017](#bib.bib132)) | VAE | LRE | – | –
    | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Hu et al. ([2017](#bib.bib72)) | VAE | ACC | ACO | – | 22.3 | 86.7 | 58.4
    | – |'
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. ([2017](#bib.bib182)) | AE&GAN | ACC | AdvR$\parallel$AdvO |
    – | 7.8 | 73.9 | 20.7 | 72^∗ |'
  prefs: []
  type: TYPE_TB
- en: '| Fu et al. ([2018](#bib.bib41)) | AE | ACC | AdvR | – | 12.9 | 46.9 | 40.1
    | 166.5^∗ |'
  prefs: []
  type: TYPE_TB
- en: '| Prabhumoye et al. ([2018](#bib.bib149)) | AE | ACC | ACO | – | 6.8 | 87.2
    | – | 32.8^∗ |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. ([2018](#bib.bib235)) | GAN | ACC | AdvR | – | – | 73.4 | 31.2
    | 29.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. ([2018](#bib.bib224)) | AE | ACC | LMO | – | – | 91.2 | 57.8
    | 47.0&60.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Logeswaran, Lee, and Bengio ([2018](#bib.bib116)) | AE | ACC | AdvO | Cycle
    | – | 90.5 | – | 133 |'
  prefs: []
  type: TYPE_TB
- en: '| Tian, Hu, and Yu ([2018](#bib.bib195)) | AE | ACC | AdvO | Noun | 24.9 |
    92.7 | 63.3 | – |'
  prefs: []
  type: TYPE_TB
- en: '| Liao et al. ([2018](#bib.bib111)) | VAE | LRE | – | – | – | 88.3 | – | –
    |'
  prefs: []
  type: TYPE_TB
- en: '| Romanov et al. ([2019](#bib.bib167)) | AE | LRS | ACR&AdvR | – | – | – |
    – | – |'
  prefs: []
  type: TYPE_TB
- en: '| John et al. ([2019](#bib.bib83)) | AE&VAE | LRS | ACR&AdvR | BoW&AdvBoW |
    – | 93.4 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Bao et al. ([2019](#bib.bib8)) | VAE | LRS | ACR&AdvR | BoW&AdvBoW | – |
    – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. ([2019](#bib.bib34)) | AE | ACC | ACO | Cycle | 20.3 | 87.7 |
    54.9 | 73 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang, Hua, and Wan ([2019](#bib.bib206)) | AE | LRE | – | – | 24.6 | 95.4
    | – | 46.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. ([2020](#bib.bib110)) | GAN | ACC | ACO&AdvR | – | – | 95.5 | 53.3
    | – |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([2020](#bib.bib113)) | VAE | LRE | – | – | 18.8 | 92.3 | – |
    18.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Yi et al. ([2020](#bib.bib225)) | VAE | ACC | ACO | Cycle | 26.0 | 90.8 |
    – | 109 |'
  prefs: []
  type: TYPE_TB
- en: '| Jin et al. ([2020a](#bib.bib80)) | AE | LRE | – | – | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: 5.1.1 Encoder-Decoder Training Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are three model choices to obtain the latent representation $\bm{z}$
    from the discrete text $\bm{x}$ and then decode it into the new text $\bm{x}^{\prime}$
    via reconstruction training: auto-encoder (AE), variational auto-encoder (VAE),
    and generative adversarial networks (GANs).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1.0.1 Auto-Encoder (AE).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Auto-encoding is a commonly used method to learn the latent representation $\bm{z}$,
    which first encodes the input sentence $\bm{x}$ into a latent vector $\bm{z}$
    and then reconstructs a sentence as similar to the input sentence as possible.
    AE is used in many TST works (e.g., Shen et al., [2017](#bib.bib182); Hu et al.,
    [2017](#bib.bib72); Fu et al., [2018](#bib.bib41); Zhao et al., [2018](#bib.bib235);
    Prabhumoye et al., [2018](#bib.bib149); Yang et al., [2018](#bib.bib224)). To
    avoid auto-encoding from blindly copying all the elements from the input, Hill,
    Cho, and Korhonen ([2016](#bib.bib65)) adopt denoising auto-encoding (DAE) Vincent
    et al. ([2010](#bib.bib203)) to replace AE in NLP tasks. Specifically, DAE first
    passes the input sentence $\bm{x}$ through a noise model to randomly drop, shuffle,
    or mask some words, and then reconstructs the original sentence from this corrupted
    sentence. This idea is used in later TST works, e.g., Lample et al. ([2019](#bib.bib102));
    Jin et al. ([2020a](#bib.bib80)). As pre-trained models became prevalent in recent
    years, the DAE training method has increased in popularity relative to its counterparts
    such as GAN and VAE, because pre-training over large corpora can grant models
    better performance in terms of semantic preservation and fluency (Lai, Toral,
    and Nissim, [2021](#bib.bib98); Riley et al., [2021b](#bib.bib165)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1.0.2 Variational Auto-Encoder (VAE).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Instead of reconstructing data based on the deterministic latent representations
    by AE, a variational auto-encoder (VAE) (Kingma and Welling, [2014](#bib.bib92);
    Rezende, Mohamed, and Wierstra, [2014](#bib.bib162)) reconstructs data based on
    the sampled latent vector from its posterior, and use the regularization by Kullback–Leibler
    divergence. VAE is also commonly used in TST works Mueller, Gifford, and Jaakkola
    ([2017](#bib.bib132)); Hu et al. ([2017](#bib.bib72)); Liu et al. ([2020](#bib.bib113));
    Liao et al. ([2018](#bib.bib111)); Yi et al. ([2020](#bib.bib225)); Tikhonov et al.
    ([2019](#bib.bib196)). The VAE loss is formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathcal{L}_{\mathrm{VAE}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}})=-\mathbb{E}_{q_{\mathrm{E}}(\bm{z}&#124;\bm{x})}\log
    p_{\mathrm{G}}(\bm{x}&#124;\bm{z})+\lambda\mathrm{KL}\Big{[}q_{\mathrm{E}}(\bm{z}&#124;\bm{x})&#124;&#124;p(\bm{z})\Big{]},\end{split}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda$ is the hyper-parameter to balance the reconstruction loss and
    the KL term, $p(\bm{z})$ is the prior drawn from the standard normal distribution
    of $\mathcal{N}(\bm{0},\bm{I})$, and $q_{\mathrm{E}}(\bm{z}|\bm{x})$ is the posterior
    in the form of $\mathcal{N}(\bm{\mu},\bm{\sigma})$, where $\bm{\mu}$ and $\bm{\sigma}$
    are predicted by the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/57fc4a457226570cf3be64e93189883d.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Latent Representation Editing
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d1ac029aaefbf8792d06e1b352d85a0c.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Attribute Code Control
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/af47a039b056cb9015eac611b468a721.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Latent Representation Splitting
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Three methods to manipulate the latent space based on disentanglement
    for text style transfer.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1.0.3 Generative Adversarial Networks (GANs).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'GANs (Goodfellow et al., [2014](#bib.bib51)) can also be applied to TST Shen
    et al. ([2017](#bib.bib182)); Zhao et al. ([2018](#bib.bib235)); Li et al. ([2020](#bib.bib110)).
    The way GANs work is to first approximate the samples drawn from a true distribution
    $\bm{z}$ by employing a noise sample $\bm{s}$ and a generator function $G$ to
    produce $\widehat{\bm{z}}=G(\bm{s})$. Next, a critic/discriminator $f_{c}(\bm{z})$
    is used to distinguish real data and generated samples. The critic is trained
    to distinguish the real samples from generated samples, and the generator is trained
    to fool the critic. Formally, the training process is expressed as a min-max game
    played among the encoder $E$, generator $G$, and the critic $f_{c}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{c}\min_{E,G}\mathcal{L}_{\mathrm{GAN}}=-\mathbb{E}_{p(\bm{z})}\log
    p_{\mathrm{G}}(\bm{x}&#124;\bm{z})+\mathbb{E}_{p(\bm{z})}f_{c}(\bm{z})-\mathbb{E}_{p(\widehat{\bm{z}})}f_{c}(\widehat{\bm{z}}).$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 5.1.2 Latent Representation Manipulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on the general encoder and decoder training method, the core element
    of disentanglement is the manipulation of latent representation $\bm{z}$. Figure
    [2](#S5.F2 "Figure 2 ‣ 5.1.1.0.2 Variational Auto-Encoder (VAE). ‣ 5.1.1 Encoder-Decoder
    Training Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey") illustrates three main methods: latent
    representation editing, attribute code control, and latent representation splitting.
    In addition, the “Disen.” column of Table [5](#S5.T5 "Table 5 ‣ 5.1 Disentanglement
    ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey")
    shows the type of latent representation manipulation for each work in disentanglement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first approach, Latent Representation Editing (LRE), shown in Figure [2(a)](#S5.F2.sf1
    "In Figure 2 ‣ 5.1.1.0.2 Variational Auto-Encoder (VAE). ‣ 5.1.1 Encoder-Decoder
    Training Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey"), is achieved by ensuring two properties
    of the latent representation $\bm{z}$. The first property is that $\bm{z}$ should
    be able to serve as the latent representation for auto-encoding, namely aligning
    $f_{c}(\bm{z})$ with the input $\bm{x}$, where $\bm{z}\overset{\Delta}{=}E(\bm{x})$.
    The second property is that $\bm{z}$ should be learned such that it incorporates
    the new attribute value of interest $a^{\prime}$. To achieve this, the common
    practice is to first learn an attribute classifier $f_{c}$, e.g., a multilayer
    perceptron (MLP) that takes the latent representation $\bm{z}$ as input, and then
    iteratively update $\bm{z}$ within the constrained space by the first property
    and in the same time maximize the prediction confidence score regarding $a^{\prime}$
    by this attribute classifier  (Mueller, Gifford, and Jaakkola, [2017](#bib.bib132);
    Liao et al., [2018](#bib.bib111); Wang, Hua, and Wan, [2019](#bib.bib206); Liu
    et al., [2020](#bib.bib113)). An alternative way to achieve the second property
    is to multi-task by another auto-encoding task on the corpus with the attribute
    $a^{\prime}$ and share most layers of the transformer except the query transformation
    and layer normalization layers Jin et al. ([2020a](#bib.bib80)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second approach, Attribute Code Control (ACC), as shown in Figure [2(b)](#S5.F2.sf2
    "In Figure 2 ‣ 5.1.1.0.2 Variational Auto-Encoder (VAE). ‣ 5.1.1 Encoder-Decoder
    Training Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey"), first enforces the latent representation
    $\bm{z}$ of the sentence $\bm{x}$ to contain all information except its attribute
    value $a$ via adversarial learning, and then the transferred output is decoded
    based on the combination of $\bm{z}$ and a structured attribute code $\bm{a}$
    corresponding to the attribute value $a$. During the decoding process, the attribute
    code vector $\bm{a}$ controls the attribute of generated text by acting as either
    the initial state (Shen et al., [2017](#bib.bib182); Yi et al., [2020](#bib.bib225))
    or the embedding (Fu et al., [2018](#bib.bib41); Dai et al., [2019](#bib.bib34)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The third approach, Latent Representation Splitting (LRS), as illustrated in
    Figure [2(c)](#S5.F2.sf3 "In Figure 2 ‣ 5.1.1.0.2 Variational Auto-Encoder (VAE).
    ‣ 5.1.1 Encoder-Decoder Training Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel
    Data ‣ Deep Learning for Text Style Transfer: A Survey"), first disentangles the
    input text into two parts: the latent attribute representation $\bm{a}$, and semantic
    representation $\bm{z}$ that captures attribute-independent information. We then
    replace the source attribute $\bm{a}$ with the target attribute $\bm{a}^{\prime}$,
    and the final transferred text is generated using the combination of $\bm{z}$
    and ${a}^{\prime}$ John et al. ([2019](#bib.bib83)); Romanov et al. ([2019](#bib.bib167)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 Training Objectives
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When disentangling the attribute information $a$ and the attribute-independent
    semantic information $\bm{z}$, we need to achieve two aims:'
  prefs: []
  type: TYPE_NORMAL
- en: Aim 1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The target attribute is fully and exclusively controlled by $\bm{a}$ (and not
    $\bm{z}$). We typically use style-oriented losses to achieve this aim (Section [5.1.3](#S5.SS1.SSS3
    "5.1.3 Training Objectives ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data
    ‣ Deep Learning for Text Style Transfer: A Survey")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Aim 2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The attribute-independent information is fully and exclusively captured by
    $\bm{z}$ (and not $\bm{a}$). Content-oriented losses are more often used for this
    aim (Section [5.1.3.0.5](#S5.SS1.SSS3.P5 "5.1.3.0.5 Language Modeling on Outputs
    (LMO). ‣ 5.1.3 Training Objectives ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel
    Data ‣ Deep Learning for Text Style Transfer: A Survey")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We describe the various style-oriented and content-oriented losses below. \subsubsubsectionStyle-Oriented
    Losses
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve [Aim 1](#S5.I3.i1 "item Aim 1 ‣ 5.1.3 Training Objectives ‣ 5.1
    Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style
    Transfer: A Survey"), many different style-oriented losses have been proposed,
    to nudge the model to learn a more clearly disentangled $\bm{a}$ and exclude the
    attribute information from $\bm{z}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3.0.1 Attribute Classifier on Outputs (ACO).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'ACO aims to make sentences generated by the generator ${G}$ carry the target
    attribute $a^{\prime}$ according to a pre-trained attribute classifier $f_{c}$ (Hu
    et al., [2017](#bib.bib72); Prabhumoye et al., [2018](#bib.bib149); Yamshchikov
    et al., [2019](#bib.bib223)). The generator $G$ takes as input the learned attribute
    vector $\widehat{\bm{a}^{\prime}}$, which can be either an attribute code vector
    trained from scratch (as in the ACC approach) or the attribute representation
    disentangled from text (by the LRS approach). We denote the generation process
    to obtain the transferred sentence $\widehat{\bm{x}^{\prime}}$ as $\widehat{\bm{x}^{\prime}}\overset{\Delta}{=}G(E(\bm{x});\bm{a}^{\prime})$.
    Correspondingly, ACO minimizes the following learning objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\mathrm{ACO}}(\bm{\theta}_{\mathrm{G}},\bm{a}^{\prime})=-\mathbb{E}_{p(\bm{x})}\log
    f_{c}(\bm{x}^{\prime})~{}.$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'In training, ACO can be trained in two ways: either a normal loss function
    trained by Gumbel-softmax distribution to approximate the discrete training (Jang,
    Gu, and Poole, [2017](#bib.bib77)), or a negative reward for reinforcement learning
    by policy gradient training Williams ([1992](#bib.bib211)) as in Luo et al. ([2019](#bib.bib117)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3.0.2 Attribute Classifier on Representations (ACR).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Different from the previous ACO objective whose training signal is from the
    the output sentence $\widehat{\bm{x}^{\prime}}$, ACR directly enforces the disentangled
    attribute representation $\bm{a}$ to be correctly classified by the attribute
    classifier, by the following objective John et al. ([2019](#bib.bib83)); Romanov
    et al. ([2019](#bib.bib167)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\mathrm{ACR}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{f_{c}})=-\mathbb{E}_{p(\bm{a})}\log
    f_{c}(\bm{a})~{}.$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 5.1.3.0.3 Adversarial Learning on Representations (AdvR).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As the previous ACR explicitly requires the latent $\bm{a}$ to be classified
    by $f_{c}$, AdvR trains from another perspective – enforcing that no attribute-related
    information is contained in $\bm{z}$ Fu et al. ([2018](#bib.bib41)); Zhao et al.
    ([2018](#bib.bib235)); Romanov et al. ([2019](#bib.bib167)); John et al. ([2019](#bib.bib83));
    Tikhonov et al. ([2019](#bib.bib196)); Li et al. ([2020](#bib.bib110)). Note that
    by combining ACR and AdvR, we can make attribute information captured fully and
    exclusively in $\bm{a}$. To achieve AdvR, the encoder $E$ is trained to generate
    the latent representation $\bm{z}\overset{\Delta}{=}E(\bm{x})$ so that $\bm{z}$
    cannot be discriminated by the attribute classifier $f_{c}$, which is expressed
    by the following learning objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{E}\min_{f_{c}}\mathcal{L}_{\mathrm{AdvR}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{f_{c}})=-\mathbb{E}_{p(\bm{x})}\log
    f_{c}(E(\bm{x}))~{}.$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'Since AdvR can be imbalanced if the number of samples of each attribute value
    differs largely, an extension of AdvR is to treat different attribute values with
    equal weight Shen et al. ([2017](#bib.bib182)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\max_{E}\min_{f_{c}}\mathcal{L}_{\mathrm{AAE}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{f_{c}})&amp;=-\mathbb{E}_{p(\bm{x})}\Big{[}\log
    f_{c}(E(\bm{x}))\Big{]}\\ &amp;-\mathbb{E}_{p(\bm{x}^{\prime})}\Big{[}\log(1-f_{c}(E(\bm{x}^{\prime})))\Big{]}~{}.\end{split}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Note that $p(\bm{x})$ is the distribution of sentences of one attribute, and
    $p(\bm{x}^{\prime})$ is the distribution of sentences of the other attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3.0.4 Adversarial Learning on Outputs (AdvO).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Apart from AdvR that adversarially learn the latent representations, we can
    also use AdvO to perform adversarial training on the outputs, to make them undistinguishable
    from the real data Shen et al. ([2017](#bib.bib182)); Logeswaran, Lee, and Bengio
    ([2018](#bib.bib116)); Tian, Hu, and Yu ([2018](#bib.bib195)). Specifically, for
    each attribute $a_{i}$, we train a classifier $f_{c}^{(i)}$ to distinguish between
    true $\bm{x}_{i}$ from the mono-style corpus of attribute $a_{i}$, and the generated
    sentence $\widehat{\bm{x}_{i}}\overset{\Delta}{=}G(E(\bm{x}_{k});\bm{a}_{i})$,
    where $k\neq i$, which aims to have the attribute $a_{i}$. The loss function is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\max_{E,G}\min_{f_{c}^{(i)}}\mathcal{L}_{\mathrm{AdvO}}^{(i)}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}},\bm{\theta}_{f_{c}^{(i)}})&amp;=-\mathbb{E}_{p(\bm{x}_{i})}\Big{[}\log
    f_{c}^{(i)}(\bm{x}_{i})\Big{]}\\ &amp;-\mathbb{E}_{p(\bm{x}_{k})}\Big{[}\log(1-f_{c}^{(i)}(G(E(\bm{x}_{k});a_{i})))\Big{]}~{}.\end{split}$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'In the training process, usually we first optimize all attribute classifiers
    $f_{c}^{(i)}$, and then train the encoder, generator, and the attribute classifiers
    together by optimizing the sum the all AdvO training losses:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{E,G}\sum_{i}^{&#124;\mathbb{A}&#124;}\min_{f_{c}^{(i)}}\mathcal{L}_{\mathrm{AdvO}}^{(i)}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}},\bm{\theta}_{f_{c}^{(i)}})~{}.$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: Note that in order to propagate the gradients, it is feasible to use the sequence
    of hidden states in the generator instead of discrete text for $G(E(\bm{x}_{k});a_{i})$
    Shen et al. ([2017](#bib.bib182)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3.0.5 Language Modeling on Outputs (LMO).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The above AdvO learns classifiers to distinguish between true samples and generated
    samples. Such discriminative classification can be alternatively achieved by generative
    language modeling, namely $\mathrm{LM}_{i}$ for each mono-style corpus with the
    attribute $a_{i}$ (Yang et al., [2018](#bib.bib224)). Specifically, the training
    objective for each attribute is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathcal{L}_{\mathrm{LMO}}^{(i)}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}},\bm{\theta}_{\mathrm{LM}_{i}})=-\mathbb{E}_{p(\bm{x_{i}})}\Big{[}\log
    p_{\mathrm{LM}_{i}}(\bm{x}_{i})\Big{]}+\gamma\mathbb{E}_{p(\bm{z}_{k})}\Big{[}\log
    p_{\mathrm{LM}_{i}}(G(E(\bm{x}_{k});a_{i}))\Big{]}~{},\end{split}$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\gamma$ is a hyperparameter to weight the two terms. The total training
    objective sums over the losses of all attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{E,G}\sum_{i}^{&#124;\mathbb{A}&#124;}\min_{\mathrm{LM}^{(i)}}\mathcal{L}_{\mathrm{LMO}}^{(i)}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}},\bm{\theta}_{\mathrm{LM}_{i}})~{}.$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: \subsubsubsection
  prefs: []
  type: TYPE_NORMAL
- en: 'Content-Oriented Losses The style-oriented losses introduced above ensures
    the attribute information to be contained in $\bm{a}$, but not necessarily putting
    constraints on the style-independent semantics $\bm{z}$. To learn the attribute-independent
    information fully and exclusively in $\bm{z}$, the following content-oriented
    losses are proposed:'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3.0.6 Cycle Reconstruction (Cycle).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The cycle reconstruction loss  (dos Santos, Melnyk, and Padhi, [2018](#bib.bib172);
    Logeswaran, Lee, and Bengio, [2018](#bib.bib116); Luo et al., [2019](#bib.bib117);
    Dai et al., [2019](#bib.bib34); Yi et al., [2020](#bib.bib225); Huang et al.,
    [2020](#bib.bib74)) first encodes a sentence $\bm{x}$ to its latent representation
    $\bm{z}\overset{\Delta}{=}E(\bm{x})$, and then feed $\bm{z}$ to the generator
    $G$ to obtain the generated sentence $G(\bm{z})$. Since the alignment of the input
    and the generated sentence is to preserve attribute-independent semantic information,
    the generator can be conditioned on any attribute, namely $\bm{a}$ or $\bm{a}^{\prime}$.
    The cycle loss constrains the output $\widehat{\bm{x}^{\prime}}$ to align with
    the input $\bm{x}$ (and, similarly, the output $\widehat{\bm{x}}$ to align with
    the input $\bm{x}^{\prime}$) so that the content information can be preserved:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\mathrm{Cycle}}(\bm{\theta}_{E},\bm{\theta}_{G})=-\mathbb{E}_{p(\bm{x})}\Big{[}\log
    p_{\mathrm{G}}(\bm{x}&#124;E(\bm{x}))\Big{]}-\mathbb{E}_{p(\bm{x}^{\prime})}\Big{[}\log
    p_{\mathrm{G}}(\bm{x}^{\prime}&#124;E(\bm{x}^{\prime}))\Big{]}~{}.$ |  | (11)
    |'
  prefs: []
  type: TYPE_TB
- en: One way to train the above cycle loss is by reinforcement learning as done by
    Luo et al. ([2019](#bib.bib117)) who use the loss function as a negative for content
    preservation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3.0.7 Bag-of-Words Overlap (BoW).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To approximately measure content preservation, bag-of-words (BoW) features are
    used by John et al. ([2019](#bib.bib83)); Bao et al. ([2019](#bib.bib8)). To focus
    on content information only, John et al. ([2019](#bib.bib83)) exclude stopwords
    and style-specific words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us denote the vocabulary set as $\mathbb{V}$. We first predict the distribution
    of BoW features $q_{\mathrm{BoW}}(\bm{z})$ of the latent representation $\bm{z}$
    using softmax on the $1\times|\mathbb{V}|$ BoW features. We then calculate the
    cross entropy loss of this BoW distribution $q_{\mathrm{BoW}}(\bm{z})$ against
    the ground-truth BoW distribution $p_{\mathrm{BoW}}(\bm{x})$ in the input sentence
    $\bm{x}$. The BoW loss is formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\mathrm{BoW}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{q_{\mathrm{BoW}}})=-p_{\mathrm{BoW}}(\bm{x})\log
    q_{\mathrm{BoW}}(\bm{z})~{}.$ |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: 5.1.3.0.8 Adversarial BoW Overlap (AdvBoW).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: BoW ensures the content to be fully captured in $\bm{z}$. As a further step,
    we want to ensure that the content information is exclusively captured in $\bm{z}$,
    namely not contained in $\bm{a}$ at all, via the following AdvBow loss on $\bm{a}$
    John et al. ([2019](#bib.bib83)); Bao et al. ([2019](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: When disentangling $\bm{z}$ and $\bm{a}$ in the LRS framework, we train an adversarial
    classifier $q_{\mathrm{BoW}}(\bm{a})$ to predict the BoW features given $\bm{a}$
    by aligning it with the ground-truth BoW distribution $p_{\mathrm{BoW}}(\bm{x})$,
    namely minimizing
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\mathrm{AdvBoW}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{q_{\mathrm{BoW}}})=-p_{\mathrm{BoW}}(\bm{x})\log
    q_{\mathrm{BoW}}(\bm{z})~{}.$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: The final min-max objective is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{E}\min_{q_{\mathrm{BoW}}}\mathcal{L}_{\mathrm{AdvBoW}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{q_{\mathrm{BoW}}}).$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: 5.1.3.0.9 Other Losses/Rewards.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are also other losses/rewards in recent work such as the noun overlap
    loss (Noun) Tian, Hu, and Yu ([2018](#bib.bib195)), as well as rewards for semantics
    and fluency (Xu et al., [2018](#bib.bib217); Gong et al., [2019](#bib.bib50);
    Sancheti et al., [2020](#bib.bib171)). We do not discuss them in much detail because
    they do not directly operate on the disentanglement of latent representations.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Prototype Editing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite a plethora of models that use end-to-end training of neural networks,
    the prototype-based text editing approach still attracts lots of attention, since
    the proposal of a pipeline method called delete, retrieve, and generate Li et al.
    ([2018](#bib.bib107)).
  prefs: []
  type: TYPE_NORMAL
- en: Prototype editing is reminiscent of early word replacement methods used for
    TST, such as synonym matching using a style dictionary Sheikha and Inkpen ([2011](#bib.bib181)),
    WordNet Khosmood and Levinson ([2010](#bib.bib89)); Mansoorizadeh et al. ([2016](#bib.bib125)),
    hand-crafted rules Khosmood and Levinson ([2008](#bib.bib90)); Castro, Ortega,
    and Muñoz ([2017](#bib.bib26)), or using hypernyms and definitions to replace
    the style-carrying words Karadzhov et al. ([2017](#bib.bib87)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Featuring more controllability and interpretability, prototype editing builds
    an explicit pipeline for text style transfer from $\bm{x}$ with attribute $a$
    to its counterpart $\bm{x}^{\prime}$ with attribute ${a}^{\prime}$:'
  prefs: []
  type: TYPE_NORMAL
- en: Step 1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Detect attribute markers of $a$ in the input sentence $\bm{x}$, and delete
    them, resulting in a content-only sentence (Section [5.2.1](#S5.SS2.SSS1 "5.2.1
    Attribute Marker Detection ‣ 5.2 Prototype Editing ‣ 5 Methods on Non-Parallel
    Data ‣ Deep Learning for Text Style Transfer: A Survey"));'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Step 2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Retrieve candidate attribute markers carrying the desired attribute ${a}^{\prime}$
    (Section [5.2.2](#S5.SS2.SSS2 "5.2.2 Target Attribute Retriever ‣ 5.2 Prototype
    Editing ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer:
    A Survey"));'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Step 3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Infill the sentence by adding new attribute markers and make sure the generated
    sentence is fluent (Section [5.2.3](#S5.SS2.SSS3 "5.2.3 Generation from Prototypes
    ‣ 5.2 Prototype Editing ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.2.1 Attribute Marker Detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Extracting attribute markers is a non-trivial NLP task. Traditional ways to
    do it involve first using tagging, parsing and morphological analysis to select
    features, and then filtering by mutual information and Chi-square testing. In
    recent deep learning pipelines, there are three major types of approaches to identify
    attribute markers: frequency-ratio methods, attention-based methods, and fusion
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Frequency-ratio methods calculate some statistics for each n-gram in the corpora.
    For example, Li et al. ([2018](#bib.bib107)) detect the attribute markers by calculating
    its relative frequency of co-occurrence with attribute $a$ versus ${a}^{\prime}$,
    and those with frequencies higher than a threshold are considered the markers
    of $a$. Using a similar approach, Madaan et al. ([2020](#bib.bib119)) first calculate
    the ratio of mean TF-IDF between the two attribute corpora for each n-gram, then
    normalize this ratio across all possible n-grams, and finally mark those n-grams
    with a normalized ratio $p$ higher than a pre-set threshold as attribute markers.
  prefs: []
  type: TYPE_NORMAL
- en: Attention-based methods train an attribute classifier using the attention mechanism
    Bahdanau, Cho, and Bengio ([2015](#bib.bib6)), and consider words with attention
    weights higher than average as markers Xu et al. ([2018](#bib.bib217)). For the
    architecture of the classifier, Zhang et al. ([2018c](#bib.bib233)) use LSTM,
    and Sudhakar, Upadhyay, and Maheswaran ([2019](#bib.bib190)) use a BERT classifier,
    where the BERT classifier has shown higher detection accuracy for the attribute
    markers.
  prefs: []
  type: TYPE_NORMAL
- en: Fusion methods combine the advantages of the above two methods. For example,
    Wu et al. ([2019](#bib.bib214)) prioritize the attribute markers predicted by
    frequency-ratio methods, and use attention-based methods as an auxiliary back
    up. One use case is when frequency-ratio methods fail to identify any attribute
    markers in a given sentence, they will use the attention-based methods as a secondary
    choice to generate attribute markers. Another case is to reduce false positives.
    To reduce the number of attribute markers that are wrongly recognized, Wu et al.
    ([2019](#bib.bib214)) set a threshold to filter out low-quality attribute markers
    by frequency-ratio methods, and in cases where all attribute markers are deleted,
    they use the markers predicted by attention-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: There are still remaining limitations of the previous methods, such as imperfect
    accuracy of the attribute classifier, and unclear relation between attribute and
    attention scores. Hence, Lee ([2020](#bib.bib103)) propose word importance scoring,
    similar to what is used by Jin et al. ([2020b](#bib.bib81)) for adversarial paraphrasing,
    to measure how important a token is to the attribute by the difference in the
    attribute probability of the original sentence and that after deleting a token.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Target Attribute Retriever
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After deleting the attribute markers $\mathrm{Marker}_{a}(\bm{x})$ of the sentence
    $\bm{x}$ with attribute $a$, we need to find a counterpart attribute marker $\mathrm{Marker}_{a^{\prime}}(\bm{x}^{\prime})$
    from another sentence $\bm{x}^{\prime}$ carrying a different attribute ${a}^{\prime}$.
    Denote the sentence template with all attribute markers deleted as $\mathrm{Template}(\bm{x})\overset{\Delta}{=}\bm{x}\backslash\mathrm{Marker}_{a}(\bm{x})$.
    Similarly, the template of the sentence $\bm{x}^{\prime}$ is $\mathrm{Template}(\bm{x}^{\prime})\overset{\Delta}{=}\bm{x}^{\prime}\backslash\mathrm{Marker}_{a^{\prime}}(\bm{x}^{\prime})$.
    A common approach is to find the counterpart attribute marker by its context,
    because the templates of the original attribute and its counter attribute marker
    should be similar. Specifically, we first match a template $\mathrm{Template}(\bm{x})$
    with the most similar template $\mathrm{Template}(\bm{x}^{\prime})$ in the opposite
    attribute corpus, and then identify the attribute markers $\mathrm{Marker}_{a}(\bm{x})$
    and $\mathrm{Marker}_{a^{\prime}}(\bm{x}^{\prime})$ as counterparts of each other.
    To match templates with their counterparts, most previous works find the nearest
    neighbors by the cosine similarity of sentence embeddings. Commonly used sentence
    embeddings include TF-IDF as used in Li et al. ([2018](#bib.bib107)); Sudhakar,
    Upadhyay, and Maheswaran ([2019](#bib.bib190)), averaged GloVe embedding distance
    used in Li et al. ([2018](#bib.bib107)); Sudhakar, Upadhyay, and Maheswaran ([2019](#bib.bib190)),
    and Universal Sentence Encoder Cer et al. ([2018](#bib.bib27)) used in Sudhakar,
    Upadhyay, and Maheswaran ([2019](#bib.bib190)). Apart from sentence embeddings,
    Tran, Zhang, and Soleymani ([2020](#bib.bib198)) use Part-of-Speech templates
    to match several candidates in the opposite corpus, and conduct an exhaustive
    search to fill parts of the candidate sentences into the masked positions of the
    original attribute markers.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Generation from Prototypes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Li et al. ([2018](#bib.bib107)) and Sudhakar, Upadhyay, and Maheswaran ([2019](#bib.bib190))
    feed the content-only sentence template and new attribute markers into a pretrained
    language model that rearranges them into a natural sentence. This infilling process
    can naturally be achieved by a masked language model (MLM) (Malmi, Severyn, and
    Rothe, [2020b](#bib.bib123)). For example, Wu et al. ([2019](#bib.bib214)) use
    MLM of the template conditioned on the target attribute, and this MLM is trained
    on an additional attribute classification loss using the model output and a fixed
    pre-trained attribute classifier. Since these generation practices are complicated,
    Madaan et al. ([2020](#bib.bib119)) propose a simpler way. They skip Step 2 that
    explicitly retrieves attribute candidates, and, instead, directly learn a generation
    model that only takes attribute-masked sentences as inputs. This generation model
    is trained on data where the attribute-carrying sentences $\bm{x}$ are paired
    with their templates $\mathrm{Template}(\bm{x})$. Training on the pairs of $(\mathrm{Template}(\bm{x}),\bm{x})$
    constructed in the above way can make the model learn how to fill the masked sentence
    template with the target attribute $a$.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Pseudo-Parallel Corpus Construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To provide more signals for training, it is also helpful to generate pseudo-parallel
    data for TST. Two major approaches are retrieval-based and generation-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.0.0.1 Retrieval-Based Corpora Construction.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One common way to construct pseudo-parallel data is through retrieval, namely
    extracting aligned sentence pairs from two mono-style corpora. Jin et al. ([2019](#bib.bib82))
    empirically observe that semantically similar sentences in the two mono-style
    corpora tend to be the attribute-transferred counterparts of each other. Hence,
    they construct the initial pseudo corpora by matching sentence pairs in the two
    attributed corpora according to the cosine similarity of pretrained sentence embeddings.
    Formally, for each sentence $\bm{x}$, its pseudo counterpart $\widehat{\bm{x}^{\prime}}$
    is its most similar sentence in the other attribute corpus $\bm{X}^{\prime}$,
    namely $\widehat{\bm{x}^{\prime}}=\operatorname*{argmax}_{\bm{x}^{\prime}\in\bm{X}^{\prime}}\mathrm{Similarity}(\bm{x},\bm{x}^{\prime})$.
    This approach is extended by Nikolov and Hahnloser ([2019](#bib.bib134)) who use
    large-scale hierarchical alignment to extract pseudo-parallel style transfer pairs.
    Such retrieval-based pseudo-parallel data construction is also useful for machine
    translation Munteanu and Marcu ([2005](#bib.bib133)); Uszkoreit et al. ([2010](#bib.bib201));
    Marie and Fujita ([2017](#bib.bib126)); Grégoire and Langlais ([2018](#bib.bib53));
    Ren et al. ([2020](#bib.bib161)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.0.0.2 Generation-Based Corpora Construction.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Another way is through generation, such as iterative back-translation (IBT)
    Hoang et al. ([2018](#bib.bib66)). IBT is a widely used method in machine translation
    Artetxe et al. ([2018](#bib.bib4)); Lample et al. ([2018a](#bib.bib100), [b](#bib.bib101));
    Dou, Anastasopoulos, and Neubig ([2020](#bib.bib37)) which adopts an iterative
    process to generate pseudo-parallel corpora.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting the iterative process, IBT needs to first initialize two style
    transfer models, $M_{a\rightarrow a^{\prime}}$ which transfers from the attribute
    $a$ to the other attribute $a^{\prime}$ and $M_{a^{\prime}\rightarrow a}$ which
    transfers from $a^{\prime}$ to $a$. Then, in each iteration, it executes the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Step 1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the models to generate pseudo-parallel corpora. Specifically, $M_{a\rightarrow
    a^{\prime}}(\bm{x})$ generates pseudo pairs $(\bm{x},\widehat{\bm{x}^{\prime}})$
    for all $\bm{x}\in\bm{X}$, and $M_{a^{\prime}\rightarrow a}(\bm{x}^{\prime})$
    generates pairs of $(\widehat{\bm{x}},\bm{x}^{\prime})$ for all $\bm{x}^{\prime}\in\bm{X}^{\prime}$;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Step 2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Re-train these two style transfer models on the datasets generated by [1](#S5.I5.i1
    "item 1 ‣ 5.3.0.0.2 Generation-Based Corpora Construction. ‣ 5.3 Pseudo-Parallel
    Corpus Construction ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey"), i.e., re-train $M_{a\rightarrow a^{\prime}}(\bm{x})$
    on $(\widehat{\bm{x}},\bm{x}^{\prime})$ pairs and $M_{a^{\prime}\rightarrow a}(\bm{x}^{\prime})$
    on $(\widehat{\bm{x}^{\prime}},\bm{x})$ pairs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For Step [1](#S5.I5.i1 "item 1 ‣ 5.3.0.0.2 Generation-Based Corpora Construction.
    ‣ 5.3 Pseudo-Parallel Corpus Construction ‣ 5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey"), in order to generate the initial
    pseudo-parallel corpora, a simple baseline is to randomly initialize the two models
    $M_{a\rightarrow a^{\prime}}$ and $M_{a^{\prime}\rightarrow a}$, and use them
    to translate the attribute of each sentence in $\bm{x}\in\bm{X}$ and $\bm{x}^{\prime}\in\bm{X}^{\prime}$.
    However, this simple initialization is subject to randomness and may not bootstrap
    well. Another way adopted by Zhang et al. ([2018d](#bib.bib234)) borrows the idea
    from unsupervised machine translation Lample et al. ([2018a](#bib.bib100)) that
    first learns an unsupervised word-to-word translation table between attribute
    $a$ and $a^{\prime}$, and uses it to generate an initial pseudo-parallel corpora.
    Based on such initial corpora, they train initial style transfer models and bootstrap
    the IBT process. Another model, Iterative Matching and Translation (IMaT) Jin
    et al. ([2019](#bib.bib82)), does not learn the word translation table, and instead
    trains the initial style transfer models on a retrieval-based pseudo-parallel
    corpora introduced in the retrieval-based corpora construction above.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For Step [2](#S5.I5.i2 "item 2 ‣ 5.3.0.0.2 Generation-Based Corpora Construction.
    ‣ 5.3 Pseudo-Parallel Corpus Construction ‣ 5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey"), during the iterative process, it
    is possible to encounter divergence, as there is no constraint to ensure that
    each iteration will produce better pseudo-parallel corpora than the previous iteration.
    One way to enhance the convergence of IBT is to add additional losses. For example,
    Zhang et al. ([2018d](#bib.bib234)) use the attribute classification loss ACO,
    as in Eq. ([3](#S5.E3 "In 5.1.3.0.1 Attribute Classifier on Outputs (ACO). ‣ 5.1.3
    Training Objectives ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey")), to check whether the generated
    sentence by back-translation fits the desired attribute according to a pre-trained
    style classifier. Alternatively, IMaT Jin et al. ([2019](#bib.bib82)) uses a checking
    mechanism instead of additional losses. At the end of each iteration, IMaT looks
    at all candidate pseudo-pairs of an original sentence, and uses Word Mover Distance
    Kusner et al. ([2015](#bib.bib97)) to select the sentence that has the desired
    attribute and is the closest to the original sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Research Agenda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will propose some potential directions for future TST research,
    including expanding the scope of styles (Section [6.1](#S6.SS1 "6.1 Expanding
    the Scope of Styles ‣ 6 Research Agenda ‣ Deep Learning for Text Style Transfer:
    A Survey")), improving the methodology (Section [6.2](#S6.SS2 "6.2 Improving the
    Methodology on Non-Parallel Data ‣ 6 Research Agenda ‣ Deep Learning for Text
    Style Transfer: A Survey")), loosening the style-specific data assumptions (Section [6.3](#S6.SS3
    "6.3 Loosening the Style-Specific Dataset Assumptions ‣ 6 Research Agenda ‣ Deep
    Learning for Text Style Transfer: A Survey")), and improving evaluation metrics
    (Section [6.4](#S6.SS4 "6.4 Improving Evaluation Metrics ‣ 6 Research Agenda ‣
    Deep Learning for Text Style Transfer: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Expanding the Scope of Styles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.1.0.0.1 More Styles.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Extending the list of styles for TST is one popular research direction. Existing
    research originally focused on styles such as simplification Zhu, Bernhard, and
    Gurevych ([2010](#bib.bib238)), formality Sheikha and Inkpen ([2011](#bib.bib181)),
    and sentiment transfer Shen et al. ([2017](#bib.bib182)), while the recent two
    years have seen a richer set of styles such as politeness Madaan et al. ([2020](#bib.bib119)),
    biasedness Pryzant et al. ([2020](#bib.bib150)), medical text simplification Cao
    et al. ([2020](#bib.bib23)), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such extension of styles is driven by the advancement of TST methods, and also
    various downstream needs, such as persona-based dialog generation, customized
    text rewriting applications, and moderation of online text. Apart from the styles
    that have been researched as listed in Table [3](#S2.T3 "Table 3 ‣ 2.3 Existing
    Subtasks with Datasets ‣ 2 What Is Text Style Transfer? ‣ Deep Learning for Text
    Style Transfer: A Survey"), there are also many other new styles that can be interesting
    to conduct new research on, including but not limited to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factual-to-empathetic transfer, to improve counseling dialogs (after the first
    version of this survey in 2020, we gladly found that this direction has now a
    preliminary exploration by Sharma et al. ([2021](#bib.bib180)));
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-native-to-native transfer (i.e., reformulating grammatical error correction
    with TST);
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentence disambiguation, to resolve nuance in text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6.1.0.0.2 More Difficult Forms of Style.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Another direction is to explore more complicated forms of styles. As covered
    by this survey, the early work on deep learning-based TST explores relatively
    simple styles, such as verb tenses Hu et al. ([2017](#bib.bib72)) and positive-vs-negative
    Yelp reviews Shen et al. ([2017](#bib.bib182)). In these tasks, each data point
    is one sentence with a clear, categorized style, and the entire dataset is in
    the same domain. Moreover, the existing datasets can decouple style and style-independent
    contents relatively well.
  prefs: []
  type: TYPE_NORMAL
- en: 'We propose that TST can potentially be extended into the following settings:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aspect-based style transfer (e.g., transferring the sentiment on one aspect
    but not the other aspects on aspect-based sentiment analysis data)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authorship transfer (which has tightly coupled style and content)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document-level style transfer (which includes discourse planning)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain adaptive style transfer (which is preceded by Li et al. ([2019](#bib.bib105)))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6.1.0.0.3 Style Interwoven with Semantics.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In some cases, it can be difficult or impossible to separate attributes from
    meaning, namely the subject matter or the argument that the author wants to convey.
    One reason is that the subject that the author is going to write about can influence
    the choice of writing style. For example, science fiction writing can use the
    first person voice and fancy, flowery tone when describing a place. Another reason
    is that many stylistic devices such as allusion depend on content words.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, it is a simplification of the problem setting to limit it to scenarios
    where the attribute and semantics can be approximately separated. For evaluation,
    so far researchers have allowed the human judges decide the scores of transferred
    style strength and the content preservation.
  prefs: []
  type: TYPE_NORMAL
- en: In future work, it will be an interesting direction to address the more challenging
    scenarios where the style and semantics are interwoven.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Improving the Methodology on Non-Parallel Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the majority of TST research focuses on non-parallel data, we discuss
    below its strengths and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Understanding the Strengths and Limitations of Existing Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To come up with improvement directions for TST methods, it is important to
    first investigate the strengths and limitations of existing methods. We analyze
    the three major streams of approaches for unsupervised TST in Table [6](#S6.T6
    "Table 6 ‣ 6.2.1 Understanding the Strengths and Limitations of Existing Methods
    ‣ 6.2 Improving the Methodology on Non-Parallel Data ‣ 6 Research Agenda ‣ Deep
    Learning for Text Style Transfer: A Survey"), including their strengths, weaknesses,
    and future directions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: The strengths ($+$), weaknesses ($-$), and improvement directions
    ($?$) of the three mainstreams of TST methods on non-parallel data.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Strengths & Weaknesses |'
  prefs: []
  type: TYPE_TB
- en: '| Disentanglement | $+$ More profound in theoretical analysis, e.g., disentangled
    representation learning |'
  prefs: []
  type: TYPE_TB
- en: '| $-$ Difficulties of training deep generative models (VAEs, GANs) for text
    |'
  prefs: []
  type: TYPE_TB
- en: '| $-$ Hard to represent all styles as latent code |'
  prefs: []
  type: TYPE_TB
- en: '| $-$ Computational cost rises with the number of styles to model |'
  prefs: []
  type: TYPE_TB
- en: '| Prototype Editing | $+$ High BLEU scores due to large word preservation |'
  prefs: []
  type: TYPE_TB
- en: '| $-$ Attribute marker detection step can fail if the style and semantics are
    confounded |'
  prefs: []
  type: TYPE_TB
- en: '| $-$ The step target attribute retrieval by templates can fail if there are
    large rewrites for styles, e.g., Shakespearean English vs. modern English |'
  prefs: []
  type: TYPE_TB
- en: '| $-$ Target attribute retrieval step has large complexity (quadratic to the
    number of sentences) |'
  prefs: []
  type: TYPE_TB
- en: '| $-$ Large computational cost if there are many styles, each of which needs
    a pre-trained LM for the generation step |'
  prefs: []
  type: TYPE_TB
- en: '| $?$ Future work can enable matchings for syntactic variation |'
  prefs: []
  type: TYPE_TB
- en: '| $?$ Future work can use grammatical error correction to post-edit the output
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pseudo-Parallel Corpus Construction | $+$ Performance can approximate supervised
    model performance, if the pseudo-parallel data are of good quality |'
  prefs: []
  type: TYPE_TB
- en: '| $-$ May fail for small corpora |'
  prefs: []
  type: TYPE_TB
- en: '| $-$ May fail if the mono-style corpora do not have many samples with similar
    contents |'
  prefs: []
  type: TYPE_TB
- en: '| $-$ For IBT, divergence is possible, and sometimes needs special designs
    to prevent it |'
  prefs: []
  type: TYPE_TB
- en: '| $-$ For IBT, time complexity is high (due to iterative pseudo data generation)
    |'
  prefs: []
  type: TYPE_TB
- en: '| $?$ Improve the convergence of the IBT |'
  prefs: []
  type: TYPE_TB
- en: 6.2.1.0.1 Challenges for Disentanglement.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Theoretically, although disentanglement is impossible without inductive biases
    or other forms of supervision Locatello et al. ([2019](#bib.bib114)), disentanglement
    is achievable with some weak signals, such as only knowing how many factors have
    changed, but not which ones Locatello et al. ([2020](#bib.bib115)).
  prefs: []
  type: TYPE_NORMAL
- en: In practice, some big challenges for disentanglement-based methods include,
    for example, the difficulty to train deep text generative models such as VAEs
    and GANs. Also, it is not easy to represent all styles as latent code. Moreover,
    if targeting multiple styles, the computational complexity linearly increases
    with the number of styles to model.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1.0.2 Challenges for Prototype Editing.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Prototype-editing approaches usually result in relatively high BLEU scores,
    partly because the output text largely overlaps with the input text. This line
    of methods is likely to perform well on tasks such as sentiment modification,
    for which it is easy to identify “attribute markers,” and the input and output
    sentences share an attribute-independent template.
  prefs: []
  type: TYPE_NORMAL
- en: However, prototype editing cannot be applied to all types of style transfer
    tasks. The first step, attribute marker retrieval, might not work if the datasets
    have confounded style and contents, because they may lead to wrong extraction
    of attribute markers, such as some content words or artifacts which can also be
    used to distinguish the style-specific data.
  prefs: []
  type: TYPE_NORMAL
- en: The second step, target attribute retrieval by templates, will fail if there
    is too little word overlap between a sentence and its counterpart carrying another
    style. An example is the TST task to “Shakespearize” modern English. There is
    little lexical overlap between a Shakespearean sentence written in early modern
    English and its corresponding modern English expression. In such cases, the retrieval
    step is likely to fail, because there is a large number of rewrites between the
    two styles, and the template might be almost hollow. Moreover, this step is also
    computationally expensive, if there are a large number of sentences in the data
    (e.g., all Wikipedia text), since this step needs to calculate the pair-wise similarity
    among all available sentences across style-specific corpora.
  prefs: []
  type: TYPE_NORMAL
- en: The third step, generation from prototype, requires a separate pretrained LM
    for each style corpus. When there are multiple styles of interest (e.g., multiple
    persona), this will induce a large computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last limitation of prototype editing is that it amplifies the intrinsic
    problem of using BLEU to evaluate TST (Problem [1](#S3.I1.i1 "item 1 ‣ 3.1.0.0.1
    BLEU with Gold References. ‣ 3.1 Automatic Evaluation ‣ 3 How to Evaluate Style
    Transfer? ‣ Deep Learning for Text Style Transfer: A Survey"), namely the fact
    that simply copying the input can result in a high BLEU score) as introduced in
    Section [3.1](#S3.SS1 "3.1 Automatic Evaluation ‣ 3 How to Evaluate Style Transfer?
    ‣ Deep Learning for Text Style Transfer: A Survey")). For the retrieval-based
    method, some can argue that there is some performance gain because this method
    in practice copies more expressions in the input sentence than other lines of
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: As future study, there can be many interesting directions to explore, for example,
    investigating the performance of existing prototype editing models under a challenging
    dataset that reveals the above shortcomings, proposing new models to improve this
    line of approaches, and better evaluation methods for prototype editing models.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1.0.3 Challenges for Pseudo-Parallel Corpus Construction.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The method to construct pseudo-parallel data can be effective, especially when
    the pseudo-parallel corpora resemble supervised data. The challenge is that this
    approach may not work if the non-parallel corpora do not have enough samples that
    can be matched to create the pseudo-parallel corpora, or when the IBT cannot bootstrap
    well or fails to converge. The time complexity for training IBT is also very high
    because it needs to iteratively generate pseudo-parallel corpus and re-train models.
    Interesting future directions can be reducing the computational cost, designing
    more effective bootstrapping, and improving the convergence of IBT.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Understanding the Evolution from Traditional NLG to Deep Learning Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Despite the exciting methodological revolution led by deep learning recently,
    we are also interested in the merging point of traditional computational linguistics
    and the deep learning techniques Henderson ([2020](#bib.bib64)). Specific to the
    context of TST, we will introduce the traditional NLG framework, and its impact
    on the current TST approaches, especially the prototype editing method.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2.0.1 Traditional NLG Framework.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The traditional NLG framework stages sentence generation into the following
    steps Reiter and Dale ([1997](#bib.bib158)):'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Content determination (not applicable)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Discourse planning (not applicable)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sentence aggregation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lexicalization
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Referring expression generation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linguistic realization
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The first two steps, content determination and discourse planning are not applicable
    to most datasets because the current focus of TST is sentence-level and not discourse-level.
  prefs: []
  type: TYPE_NORMAL
- en: Among Steps 3 to 6, sentence aggregation groups necessary information into a
    single sentence, lexicalization chooses the right word to express the concepts
    generated by sentence aggregation, referring expression generation produces surface
    linguistic forms for domain entities, and linguistic realization edits the text
    so that it conforms to grammar, including syntax, morphology, and orthography.
    This framework is widely applied to NLG tasks (e.g., Zue and Glass, [2000](#bib.bib239);
    Mani, [2001](#bib.bib124); McTear, [2002](#bib.bib128); Gatt and Reiter, [2009](#bib.bib46);
    Androutsopoulos and Malakasiotis, [2010](#bib.bib2)).
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2.0.2 Re-Viewing Prototype-Based TST.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Among the approaches introduced so far, the most relevant for the traditional
    NLG is the prototype-based text editing, which has been introduced in Section [5.2](#S5.SS2
    "5.2 Prototype Editing ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Using the language of the traditional NLG framework, the prototype-based techniques
    can be viewed as a combination of sentence aggregation, lexicalization, and linguistic
    realization. Specifically, prototype-based techniques first prepare an attribute-free
    sentence template, and supply it with candidate attribute markers that carry the
    desired attribute, both of which are sentence aggregation. Then, using language
    models to infill the prototype with the correct expressions corresponds to lexicalization
    and linguistic realization. Note that the existing TST systems do not explicitly
    deal with referring expression generation (e.g., generating co-references), leaving
    it to be handled by language models.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2.0.3 Meeting Point of Traditional and New Methods.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Viewing prototype-based editing as a merging point where traditional, controllable
    framework meets deep learning models, we can see that it takes advantage of the
    powerful deep learning models and the interpretable pipeline of the traditional
    NLG. There are several advantages in merging the traditional NLG with the deep
    learning models. First, sentence planning-like steps make the generated contents
    more controllable. For example, the template of the original sentence is saved,
    and the counterpart attributes can also be explicitly retrieved, as a preparation
    for the final rewriting. Such a controllable, white-box approach can be easy to
    tune, debug, and improve. The accuracy of attribute marker extraction, for example,
    is constantly improving across literature Sudhakar, Upadhyay, and Maheswaran ([2019](#bib.bib190))
    and different ways to extract attribute markers can be easily fused Wu et al.
    ([2019](#bib.bib214)). Second, sentence planning-like steps ensure the truthfulness
    of information. As most content words are kept and no additional information is
    hallucinated by the black-box neural networks, we can better ensure that the information
    of the attribute-transferred output is consistent with the original input.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Inspiration from Tasks with Similar Nature
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An additional perspective that can inspire new methodological innovation is
    insights from other tasks that share a similar nature as TST. We will introduce
    in this section several closely-related tasks, including machine translation,
    image style transfer, style-conditioned language modeling, counterfactual story
    rewriting, contrastive text generation, and prototype-based text editing.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3.0.1 Machine Translation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The problem settings of machine translation and text style transfer share much
    in common: the source and target language in machine translation is analogous
    to the original and desired attribute, $a$ and $a^{\prime}$, respectively. The
    major difference is that in NMT, the source and target corpora are in completely
    different languages, which have almost disjoint word vocabulary, whereas in text
    style transfer, the input and output are in the same language, and the model is
    usually encouraged to copy most content words from input such as the BoW loss
    introduced in Section [5.1.3.0.5](#S5.SS1.SSS3.P5 "5.1.3.0.5 Language Modeling
    on Outputs (LMO). ‣ 5.1.3 Training Objectives ‣ 5.1 Disentanglement ‣ 5 Methods
    on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey"). Some
    TST works have been inspired by MT, such as the pseudo-parallel construction Nikolov
    and Hahnloser ([2019](#bib.bib134)); Zhang et al. ([2018d](#bib.bib234)), and
    in the future there may be more interesting intersections.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3.0.2 Data-to-Text Generation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Data-to-text generation is another potential domain that can draw inspiration
    from and to TST. The data-to-text generation task is to generate textual descriptions
    from structured data such as tables Wiseman, Shieber, and Rush ([2017](#bib.bib212));
    Parikh et al. ([2020](#bib.bib143)), meaning representations Novikova, Dusek,
    and Rieser ([2017](#bib.bib138)), or Resource Description Framework (RDF) triples
    Gardent et al. ([2017](#bib.bib44)); Ferreira et al. ([2020](#bib.bib38)). With
    the recent rise of pretrained seq2seq models for transfer learning Raffel et al.
    ([2020](#bib.bib155)), it is common to formulate data-to-text as a seq2seq task
    by serializing the structured data into a sequence Kale and Rastogi ([2020](#bib.bib85));
    Ribeiro et al. ([2020](#bib.bib163)); Guo et al. ([2020](#bib.bib58)). Then data-to-text
    generation can be seen as a special form of TST from structured information to
    text. This potential connection has not yet been investigated but worth exploring.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3.0.3 Neural Style Transfer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Neural style transfer first originates in image style transfer Gatys, Ecker,
    and Bethge ([2016](#bib.bib47)), and its disentanglement ideas inspired some early
    TST researcg Shen et al. ([2017](#bib.bib182)). The difference between image style
    transfer and TST is that, for images, it is feasible to disentangle the explicit
    representation of the image texture as the gram matrix of image neural feature
    vectors, but for text, styles do not have such an explicit representation, but
    more abstract attributes. Besides this difference, many other aspects of style
    transfer research can have shared nature. Note that there are style transfer works
    across different modalities, including images Gatys, Ecker, and Bethge ([2016](#bib.bib47));
    Zhu et al. ([2017](#bib.bib237)); Chen et al. ([2017b](#bib.bib31)), text, voice
    Gao, Singh, and Raj ([2018](#bib.bib43)); Qian et al. ([2019](#bib.bib151)); Yuan
    et al. ([2021](#bib.bib226)), handwriting Azadi et al. ([2018](#bib.bib5)); Zhang
    and Liu ([2013](#bib.bib231)), and videos Ruder, Dosovitskiy, and Brox ([2016](#bib.bib168));
    Chen et al. ([2017a](#bib.bib30)). Many new advances in one style transfer field
    can inspire another style transfer field. For example, image style transfer has
    been used as a way for data augmentation Zheng et al. ([2019](#bib.bib236)); Jackson
    et al. ([2019](#bib.bib75)) and adversarial attack Xu et al. ([2020](#bib.bib219)),
    but TST has not yet been applied for such usage.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3.0.4 Style-Conditioned Language Modeling.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Different from language modeling that learns how to generate general natural
    language text, conditional language modeling learns how to generate text given
    a condition, such as some context, or a control code Pfaff ([1979](#bib.bib144));
    Poplack ([2000](#bib.bib145)). Recent advances of conditional language models
    Keskar et al. ([2019](#bib.bib88)); Dathathri et al. ([2020](#bib.bib35)) also
    include text generation conditioned on a style token, such as positive or negative.
    Possible conditions include author style Syed et al. ([2020](#bib.bib192)), speaker
    identity, persona and emotion Li et al. ([2016](#bib.bib106)), genre, attributes
    derived from text, topics, and sentiment Ficler and Goldberg ([2017](#bib.bib39)).
    They are currently limited to a small set of pre-defined “condition” tokens and
    can only generate from scratch a sentence, but not yet able to be conditioned
    on an original sentence for style rewriting. The interesting finding in this research
    direction is that it can make good use of a pretrained LM and just do some light-weight
    inference techniques to generate style-conditioned text, so perhaps such approaches
    can inspire future TST methods and reduce the carbon footprints of training TST
    models from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3.0.5 Counterfactual Story Rewriting.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Counterfactual story rewriting aims to learn a new event sequence in the presence
    of a perturbation of a previous event (i.e., counterfactual condition) Goodman
    ([1947](#bib.bib52)); Starr ([2019](#bib.bib189)). Qin et al. ([2019](#bib.bib153))
    propose the first dataset, each sample of which takes an originally five-sentence
    story, and changes the event in the second sentence to a new, counterfactual event.
    The task is to generate the last three sentences of the story based on the newly
    altered second sentence that initiates the story. The criteria of the counterfactual
    story rewriting include relevance with the first two sentences, and minimal edits
    from the original story ending. This line of research is relatively difficult
    to directly apply to TST, because its motivation and dataset nature is different
    from the general text style transfer, and more importantly, this task is not conditioned
    on a predefined categorized style token, but the free-form textual story beginning.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3.0.6 Contrastive Text Generation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As neural network-based NLP models more easily learn spurious statistical correlations
    in the data rather than achieve robust understanding Jia and Liang ([2017](#bib.bib79)),
    there are recent works to construct auxillary datasets composed of near-misses
    of the original data. For example, Gardner et al. ([2020](#bib.bib45)) ask crowdsource
    workers to rewrite the input of the task with minimal changes but matching a different
    target label. To alleviate expensive human labor, Xing et al. ([2020](#bib.bib216))
    develop an automatic text editing approach to generate contrast set for aspect-based
    sentiment analysis. The difference between contrastive text generation and text
    style transfer is that the former does not require content preservation but mainly
    aims to construct a slightly textually different input that can result in a change
    of the ground-truth output, to test the model robustness. So the two tasks are
    not completely the same, although they have some intersections that might inspire
    future work, such as aspect-based style transfer suggested in Section [6.1](#S6.SS1
    "6.1 Expanding the Scope of Styles ‣ 6 Research Agenda ‣ Deep Learning for Text
    Style Transfer: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3.0.7 Prototype-Based Text Editing.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Prototype editing is not unique in TST, but also widely used in other NLP tasks.
    Knowing the new advances in prototype editing for other tasks can potentially
    inspire new method innovations in TST. Guu et al. ([2018](#bib.bib60)) first proposes
    the protype editing approach to improve LM by first sampling a lexically similar
    sentence prototype and then editing it using variational encoder and decoders.
    This prototype-and-then-edit approach can also be seen in summarization Wang,
    Quan, and Wang ([2019](#bib.bib205)), machine translation Cao and Xiong ([2018](#bib.bib22));
    Wu, Wang, and Wang ([2019](#bib.bib213)); Gu et al. ([2018](#bib.bib56)); Zhang
    et al. ([2018a](#bib.bib228)); Bulté and Tezcan ([2019](#bib.bib20)), conversation
    generation Weston, Dinan, and Miller ([2018](#bib.bib210)); Cai et al. ([2019](#bib.bib21)),
    code generation Hashimoto et al. ([2018](#bib.bib62)), and question answering
    Lewis et al. ([2020](#bib.bib104)). As an extension to the retrieve and edit steps,
    Hossain, Ghazvininejad, and Zettlemoyer ([2020](#bib.bib67)) use an ensemble approach
    to retrieve a set of relevant prototypes, edit, and finally rerank to pick the
    best output for machine translation. Such extension can also be potentially applied
    to text style transfer.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Loosening the Style-Specific Dataset Assumptions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A common assumption for most deep learning-based TST works, as mentioned in
    Section [2.1.0.0.2](#S2.SS1.SSS0.P2 "2.1.0.0.2 Data-Driven Definition of Style
    as the Scope of this Survey. ‣ 2.1 How to Define Style? ‣ 2 What Is Text Style
    Transfer? ‣ Deep Learning for Text Style Transfer: A Survey"), is the availability
    of style-specific corpora for each style of interest, either parallel or non-parallel.
    This assumption can potentially be loosened in two ways.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.0.0.1 Linguistic Styles with No Matched Data.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Since there are various concerns raised by the data-driven definition of style
    as described in Section [2.1](#S2.SS1 "2.1 How to Define Style? ‣ 2 What Is Text
    Style Transfer? ‣ Deep Learning for Text Style Transfer: A Survey"), a potentially
    good research direction is to bring back the linguistic definition of style, and
    thus remove some of the concerns associated with large datasets. Several methods
    can be a potential fit for this: prompt design Li and Liang ([2021](#bib.bib109));
    Qin and Eisner ([2021](#bib.bib152)); Scao and Rush ([2021](#bib.bib173)) that
    passes a prompt to GPT Radford et al. ([2019](#bib.bib154)); Brown et al. ([2020](#bib.bib19))
    to obtain a style-transferred text; style-specific template design; or use templates
    to first generate synthetic data and make models learn from the synthetic data.
    Prompt design is not yet investigated as a direction for TST research, but it
    is an interesting direction to explore.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.0.0.2 Distinguishing Styles from a Mixed Corpus.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It might also be possible to distinguish styles direction from a mixed corpus
    with no style labels. For example, Riley et al. ([2021a](#bib.bib164)) learn a
    style vector space from text; Xu, Cheung, and Cao ([2020](#bib.bib218)) use unsupervised
    representation learning to separate the style and contents from a mixed corpus
    of unspecified styles; Guo et al. ([2021](#bib.bib59)) use cycle training with
    a conditional variational auto-encoder to unsupervisedly learn to express the
    same semantics through different styles. Theoretically, although disentanglement
    is impossible without inductive biases or other forms of supervision Locatello
    et al. ([2019](#bib.bib114)), disentanglement is achievable with some weak signals,
    such as only knowing how many factors have changed, but not which ones Locatello
    et al. ([2020](#bib.bib115)). A more advanced direction can be emergent styles
    Kang, Wang, and de Melo ([2020](#bib.bib86)), since styles can be evolving, for
    example across dialog turns.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Improving Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There has been a lot of attention to the problems of evaluation metrics of TST
    and potential improvements Pang and Gimpel ([2019](#bib.bib141)); Tikhonov and
    Yamshchikov ([2018](#bib.bib197)); Mir et al. ([2019](#bib.bib130)); Fu et al.
    ([2019](#bib.bib40)); Pang ([2019](#bib.bib140)); Yamshchikov et al. ([2021](#bib.bib222));
    Jafaritazehjani et al. ([2020](#bib.bib76)). Recently, Gehrmann et al. ([2021](#bib.bib48))
    has proposed a new framework which is a live environment to evaluate NLG in a
    principled and reproducible manner. Apart from the existing scoring methods, future
    works can also make use of linguistic rules such as a checklist to evaluate what
    capabilities the TST model has achieved. For example, there can be a checklist
    for formality transfer according to existing style guidelines such as the APA
    style guide American Psychological Association ([1983](#bib.bib1)). Such a checklist-based
    evaluation can make the performance of black-box deep learning models more interpretable,
    and also allow for more insightful error analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Expanding the Impact of TST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this last section of this survey, we highlight several directions to expand
    the impact of TST. First, TST can be used to help other NLP tasks such as paraphrasing,
    data augmentation, and adversarial robustness probing (Section [7.1](#S7.SS1 "7.1
    Connecting TST to More NLP Tasks ‣ 7 Expanding the Impact of TST ‣ Deep Learning
    for Text Style Transfer: A Survey")). Moreover, many specialized downstream tasks
    can be achieved with the help of TST, such as persona-consistent dialog generation,
    attractive headline generation, style-specific machine translation, and anonymization
    (Section [7.2](#S7.SS2 "7.2 Connecting TST to More Specialized Applications ‣
    7 Expanding the Impact of TST ‣ Deep Learning for Text Style Transfer: A Survey")).
    Last but not the least, we overview the ethical impacts that are important to
    take into consideration for future development of TST (Section [7.3](#S7.SS3 "7.3
    Considering Ethical Impacts of TST ‣ 7 Expanding the Impact of TST ‣ Deep Learning
    for Text Style Transfer: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Connecting TST to More NLP Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Text style transfer can be applied to other important NLP tasks, such as paraphrase
    generation, data augmentation, and adversarial robustness probing.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.0.0.1 Paraphrase Generation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Paraphrase generation is to express the same information in alternative ways
    Madnani and Dorr ([2010](#bib.bib120)). The nature of paraphrasing shares a lot
    in common with TST, which is to transfer the style of text while preserving the
    content. One of the common ways of paraphrasing is syntactic variation, such as
    “X wrote Y.”, “Y was written by X.” and “X is the writer of Y.” Androutsopoulos
    and Malakasiotis ([2010](#bib.bib2)). Besides syntactic variation, it also makes
    sense to include stylistic variation as a form of paraphrases, which means that
    the linguistic style transfer (not the content preference transfer in Table [3](#S2.T3
    "Table 3 ‣ 2.3 Existing Subtasks with Datasets ‣ 2 What Is Text Style Transfer?
    ‣ Deep Learning for Text Style Transfer: A Survey")) can be regarded as a subset
    of paraphrasing. The caution here is that if the paraphrasing is for a downstream
    task, researchers should first check if the downstream task is compatible with
    the used styles. For example, dialog generation may be sensitive to all linguistic
    styles, whereas summarization can allow linguistic style-varied paraphrases in
    the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: There are three implications of this connection of TST and paraphrase generation.
    First, many trained TST models can be borrowed for paraphrasing, such as formality
    transfer and simplification. A second connection is that the method innovations
    proposed in the two fields can inspire each other. For example, Krishna, Wieting,
    and Iyyer ([2020](#bib.bib96)) formulate style transfer as a paraphrasing task.
    Thirdly, the evaluation metrics of the two tasks can also inspire each other.
    For example, Yamshchikov et al. ([2021](#bib.bib222)) associate the semantic similarity
    metrics for two tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.0.0.2 Data Augmentation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Data augmentation generates text similar to the existing training data so that
    the model can have larger training data. TST is a good method for data augmentation
    because TST can produce text with different styles but the same meaning. Image
    style transfer has already been used for data augmentation Zheng et al. ([2019](#bib.bib236));
    Jackson et al. ([2019](#bib.bib75)), so it can be interesting to see future works
    to also apply text style transfer for data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.0.0.3 Adversarial Robustness Probing.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Another use of style transferred text is adversarial robustness probing. For
    example, styles that are task-agnostic can be used for general adversarial attack
    (e.g., politeness transfer to probe sentiment classification robustness) Jin et al.
    ([2020b](#bib.bib81)), while the styles that can change the task output can be
    used to construct contrast sets (e.g., sentiment transfer to probe sentiment classification
    robustness) Xing et al. ([2020](#bib.bib216)). Xu et al. ([2020](#bib.bib219))
    applies image style transfer to adversarial attack, and future research can also
    explore the use of TST in the two ways suggested above.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Connecting TST to More Specialized Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TST can be applied not only to other NLP tasks as introduced in the previous
    section, but also be helpful for specialized downstream applications. In practice,
    when applying NLP models, it is important to customize for some specific needs,
    such as generating dialog with a consistent persona, writing headlines that are
    attractive and engaging, making machine translation models adapt to different
    styles, and anonymizing the user identity by obfuscating the style.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.0.0.1 Persona-Consistent Dialog Generation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A useful downstream application of TST is persona-consistent dialog generation
    Li et al. ([2016](#bib.bib106)); Zhang et al. ([2018b](#bib.bib229)); Shuster
    et al. ([2020](#bib.bib185)). Since conversational agents directly interact with
    users, there is a strong demand for human-like dialog generation. Previously,
    this is done by encoding speaker traits into a vector and the conversation is
    then conditioned on this vector Li et al. ([2016](#bib.bib106)). As future work,
    text style transfer can also be used as part of the pipeline of persona-based
    dialog generation, where the persona can be categorized into distinctive style
    types, and then the generated text can be post-processed by a style transfer model.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.0.0.2 Attractive Headline Generation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In journalism writing, it is crucial to generate engaging headlines. Jin et al.
    ([2020a](#bib.bib80)) first use TST to generate eye-catchy headlines with three
    different styles, humorous, romantic, and clickbaity styles. Li et al. ([2021](#bib.bib108))
    follow this direction and propose a disentanglement-based model to generate attractive
    headlines for Chinese news.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.0.0.3 Style-Specific Machine Translation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In machine translation, it is useful to have an additional control of the style
    for the translated text. Commonly used styles for TST in machine translation are
    politeness Sennrich, Haddow, and Birch ([2016a](#bib.bib176)) and formality Niu,
    Martindale, and Carpuat ([2017](#bib.bib136)); Wu, Wang, and Liu ([2020](#bib.bib215)).
    For example, Wu, Wang, and Liu ([2020](#bib.bib215)) translates from informal
    Chinese to formal English.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.0.0.4 Anonymization.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: TST can also be used for anonymization, which is an important way to protect
    user privacy, especially since there are ongoing heated discussions of ethics
    in the artificial intelligence community. Many concerns have been raised towards
    the discriminative task of author profiling, which can mine the demographic identities
    of the author of a writing, even including privacy-invading properties such as
    gender and age Schler et al. ([2006](#bib.bib174)). As a potential solution, TST
    can be applied to alter the text and obfuscate the real identity of the users
    Reddy and Knight ([2016](#bib.bib157)); Gröndahl and Asokan ([2020](#bib.bib54)).
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Considering Ethical Impacts of TST
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recently, there is more and more attention being paid to the ethical concerns
    associated with AI research. We discuss in the following two ethical considerations:
    (1) social impact of TST applications, and (2) data privacy problem of text style
    transfer.'
  prefs: []
  type: TYPE_NORMAL
- en: Fields that involve human subjects or direct application to humans work under
    a set of core principles and guidelines Beauchamp, Childress et al. ([2001](#bib.bib10)).
    Before initiating a research project, responsible research bodies use these principles
    as a ruler to judge whether the research is ethically correct to start. NLP reserch
    and applications, including TST, that directly involve human users, is regulated
    under a central regulatory board, Institutional Review Board (IRB). We also provide
    several guidelines below to avoid ethical misconduct in future publications on
    text style transfer.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1 Social Impact of TST Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Technologies can have unintended negative consequences Hovy and Spruit ([2016](#bib.bib68)).
    For example, TST can facilitate the automation of intelligent assistants with
    designed attributes, but can also be used to create fake text or fraud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, inventors of a technology should beware how other people very probably
    adopt this technology for their own incentives. For TST, since it has a wide range
    of subtasks and applications, we examine each of them with the following two questions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who will benefit from such a technology?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who will be harmed by such a technology?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Although many ethical issues are debatable, we try to categorize the text attribute
    tasks into three ethical levels: beneficial, neutral, and tasks that can be obvious
    double-sided swords.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1.0.1 Beneficial.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An important direction of NLP for social good is to fight against abusive online
    text. Text style transfer can serve as a very helpful tool as it can be used to
    transfer malicious text to normal language. Shades of abusive language include
    hate speech, offensive language, sexist and racist language, aggression, profanity,
    cyberbullying, harassment, trolling, and toxic language Waseem et al. ([2017](#bib.bib208)).
    There are also other negative text such as propaganda Bernays ([2005](#bib.bib14));
    Carey ([1997](#bib.bib24)), and others. It is widely known that malicious text
    is harmful to people. For example, research shows that cyberbullying victims tend
    to have more stress and suicidal ideation Kowalski et al. ([2014](#bib.bib94)),
    and also detachment from family and offline victimization Oksanen et al. ([2014](#bib.bib139)).
    There are more and more efforts put into combating toxic language, such as 30K
    content moderators that Facebook and Instagram employ Harrison ([2019](#bib.bib61)).
    Therefore, the automatic malicious-to-normal language transfer can be a helpful
    intelligent assistant to address such needs. Apart from purifying malicious text
    on social media, it can also be used on social chatbots to make sure there are
    no bad contents in language they generate (Roller et al., [2021](#bib.bib166)).
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1.0.2 Neutral.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Most text style transfer tasks are neutral. For example, informal-to-formal
    transfer can be used as a writing assistant to help make writings more professional,
    and formal-to-informal transfer can tune the tone of bots to be more casual. Most
    applications to customized the persona of bots are also neutral with regard to
    their societal impact.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1.0.3 Double-Sided Sword.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Besides positive and neutral applications, there are, unfortunately, several
    text style transfer tasks that are double-sided swords. For example, one of the
    most popular TST tasks, sentiment modification, although it can be used to change
    intelligent assistants or robots from a negative to positive mood (which is unlikely
    to harm any parties), the vast majority of papers applies this technology to manipulate
    the polarity of reviews, such as Yelp Shen et al. ([2017](#bib.bib182)) and Amazon
    reviews He and McAuley ([2016](#bib.bib63)). This leads to a setting where a negative
    restaurant review is changed to a positive comment, or vice versa, with debatable
    ethics. Such a technique can be used as a cheating method for the commercial body
    to polish its reviews, or harm the reputation of their competitors. Once this
    technology is used, it will automatically manipulate the online text to contain
    polarity that the model owner desires. Hence, we suggest the research community
    raise serious concern against the review sentiment modification task.
  prefs: []
  type: TYPE_NORMAL
- en: Another task, political slant transfer, may induce concerns within some specific
    context. For example, social bots (i.e., autonomous bots on social media, such
    as Twitter bots and Facebook bots) are a big problem in the U.S., even playing
    a significant role in the 2016 United States presidential election Bessi and Ferrara
    ([2016](#bib.bib15)); Shao et al. ([2018](#bib.bib179)). It is reported that at
    least 400,000 bots were responsible for about 19% of the total Tweets. Social
    bots usually target to advocate certain ideas, supporting campaigns, or aggregating
    other sources either by acting as a "follower" and/or gathering followers itself.
    So the political slant transfer task, which transfers the tone and content between
    republican comments and democratic ones, are highly sensitive and may face the
    risk of being used on social bots to manipulate political views of the mass.
  prefs: []
  type: TYPE_NORMAL
- en: Some more arguable ones are male-to-female tone transfer, which can be potentially
    used for identity deception. The cheater can create an online account and pretend
    to be an attractive young lady. There is also the reversed direction (female-to-male
    tone transfer) which can be used for applications such as authorship obfuscation
    Shetty, Schiele, and Fritz ([2018](#bib.bib184)) which anonymizes the author attributes
    by hiding the gender of a female author by re-synthesizing the text to use male.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2 Data Privacy Issues for TST
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another ethical concern is the use of data in the research practice. Researchers
    should not overmine user data, such as the demographic identities. Such data privacy
    widely exists in the data science community as a whole, and there have been many
    ethical discussions Tse et al. ([2015](#bib.bib200)); Russell, Dewey, and Tegmark
    ([2015](#bib.bib170)).
  prefs: []
  type: TYPE_NORMAL
- en: Although the TST task needs data containing some attributes along with the text
    content. While it is acceptable to use ratings of reviews that are classified
    as positive or negative, but the user attributes are sensitive, including the
    gender of the user’s account Prabhumoye et al. ([2018](#bib.bib149)), and age
    Lample et al. ([2019](#bib.bib102)). The collection and potential use of such
    sensitive user attributes can have implications that need to be carefully considered.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presented a comprehensive review of text style transfer with deep
    learning methods. We have surveyed recent research efforts in TST and developed
    schemes to categorize and distill the existing literature. This survey has covered
    the task formulation, evaluation metrics, and methods on parallel and non-parallel
    data. We also discussed several important topics in the research agenda of TST,
    and how to expand the impact of TST to other tasks and applications, including
    ethical considerations. This survey provides a reference for future researchers
    working on TST.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank Qipeng Guo for his insightful discussions and the anonymous reviewers
    for their constructive suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: \starttwocolumn
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: American Psychological Association (1983) American Psychological Association.
    1983. *Publication manual*. American Psychological Association Washington, DC.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Androutsopoulos and Malakasiotis (2010) Androutsopoulos, Ion and Prodromos Malakasiotis.
    2010. A survey of paraphrasing and textual entailment methods. *Journal of Artificial
    Intelligence Research*, 38:135–187.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Argamon et al. (2003) Argamon, Shlomo, Moshe Koppel, Jonathan Fine, and Anat Rachel
    Shimoni. 2003. Gender, genre, and writing style in formal written texts. *Text
    & Talk*, 23(3):321–346.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artetxe et al. (2018) Artetxe, Mikel, Gorka Labaka, Eneko Agirre, and Kyunghyun
    Cho. 2018. Unsupervised neural machine translation. In *6th International Conference
    on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May
    3, 2018, Conference Track Proceedings*, OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azadi et al. (2018) Azadi, Samaneh, Matthew Fisher, Vladimir G. Kim, Zhaowen
    Wang, Eli Shechtman, and Trevor Darrell. 2018. Multi-content GAN for few-shot
    font style transfer. In *2018 IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*, pages 7564–7573, IEEE Computer
    Society.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau, Cho, and Bengio (2015) Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua
    Bengio. 2015. Neural machine translation by jointly learning to align and translate.
    In *3rd International Conference on Learning Representations, ICLR 2015, San Diego,
    CA, USA, May 7-9, 2015, Conference Track Proceedings*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banerjee and Lavie (2005) Banerjee, Satanjeev and Alon Lavie. 2005. METEOR:
    An automatic metric for MT evaluation with improved correlation with human judgments.
    In *Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures
    for Machine Translation and/or Summarization*, pages 65–72, Association for Computational
    Linguistics, Ann Arbor, Michigan.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bao et al. (2019) Bao, Yu, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova,
    Xin-yu Dai, and Jiajun Chen. 2019. Generating sentences from disentangled syntactic
    and semantic spaces. In *Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics*, pages 6008–6019, Association for Computational
    Linguistics, Florence, Italy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bateman and Paris (1989) Bateman, John A and Cecile Paris. 1989. Phrasing a
    text in terms the user can understand. In *IJCAI*, pages 1511–1517.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beauchamp, Childress et al. (2001) Beauchamp, Tom L, James F Childress, et al.
    2001. *Principles of biomedical ethics*. Oxford University Press, USA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Belz (2008) Belz, Anja. 2008. Automatic generation of weather forecast texts
    using comprehensive probabilistic generation-space models. *Natural Language Engineering*,
    14(4):431–455.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Belz et al. (2020) Belz, Anya, Shubham Agarwal, Anastasia Shimorina, and Ehud
    Reiter. 2020. ReproGen: Proposal for a shared task on reproducibility of human
    evaluations in NLG. In *Proceedings of the 13th International Conference on Natural
    Language Generation*, pages 232–236, Association for Computational Linguistics,
    Dublin, Ireland.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: den Bercken, Sips, and Lofi (2019) den Bercken, Laurens Van, Robert-Jan Sips,
    and Christoph Lofi. 2019. Evaluating neural text simplification in the medical
    domain. In *The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May
    13-17, 2019*, pages 3286–3292, ACM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bernays (2005) Bernays, Edward L. 2005. *Propaganda*. Ig publishing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bessi and Ferrara (2016) Bessi, Alessandro and Emilio Ferrara. 2016. Social
    bots distort the 2016 us presidential election online discussion. *First Monday*,
    21(11-7).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boulis and Ostendorf (2005) Boulis, Constantinos and Mari Ostendorf. 2005. A
    quantitative analysis of lexical differences between genders in telephone conversations.
    In *ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics,
    Proceedings of the Conference, 25-30 June 2005, University of Michigan, USA*,
    pages 435–442, The Association for Computer Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Briakou et al. (2021a) Briakou, Eleftheria, Sweta Agrawal, Ke Zhang, Joel Tetreault,
    and Marine Carpuat. 2021a. A review of human evaluation for style transfer. In
    *Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and
    Metrics (GEM 2021)*, pages 58–67, Association for Computational Linguistics, Online.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Briakou et al. (2021b) Briakou, Eleftheria, Di Lu, Ke Zhang, and Joel Tetreault.
    2021b. Olá, bonjour, salve! XFORMAL: A benchmark for multilingual formality style
    transfer. In *Proceedings of the 2021 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies*,
    pages 3199–3216, Association for Computational Linguistics, Online.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brown et al. (2020) Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. Language models are few-shot learners. In *Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bulté and Tezcan (2019) Bulté, Bram and Arda Tezcan. 2019. Neural fuzzy repair:
    Integrating fuzzy matches into neural machine translation. In *Proceedings of
    the 57th Conference of the Association for Computational Linguistics, ACL 2019,
    Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers*, pages 1800–1809,
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2019) Cai, Deng, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu,
    Wai Lam, and Shuming Shi. 2019. Skeleton-to-response: Dialogue generation guided
    by retrieval memory. In *Proceedings of the 2019 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short
    Papers)*, pages 1219–1228, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao and Xiong (2018) Cao, Qian and Deyi Xiong. 2018. Encoding gated translation
    memory into neural machine translation. In *Proceedings of the 2018 Conference
    on Empirical Methods in Natural Language Processing, Brussels, Belgium, October
    31 - November 4, 2018*, pages 3042–3047, Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2020) Cao, Yixin, Ruihao Shui, Liangming Pan, Min-Yen Kan, Zhiyuan
    Liu, and Tat-Seng Chua. 2020. Expertise style transfer: A new task towards better
    communication between experts and laymen. In *Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics*, pages 1061–1071, Association
    for Computational Linguistics, Online.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carey (1997) Carey, Alex. 1997. *Taking the risk out of democracy: Corporate
    propaganda versus freedom and liberty*. University of Illinois Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlson, Riddell, and Rockmore (2018) Carlson, Keith, Allen Riddell, and Daniel
    Rockmore. 2018. Evaluating prose style transfer with the bible. *Royal Society
    open science*, 5(10):171920.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Castro, Ortega, and Muñoz (2017) Castro, Daniel, Reynier Ortega, and Rafael
    Muñoz. 2017. Author masking by sentence transformation—notebook for pan at clef
    2017. In *CLEF 2017 Evaluation Labs and Workshop–Working Notes Papers*, pages
    11–14.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cer et al. (2018) Cer, Daniel, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole
    Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan,
    Chris Tar, Brian Strope, and Ray Kurzweil. 2018. Universal sentence encoder for
    english. In *Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October
    31 - November 4, 2018*, pages 169–174, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chakrabarty, Muresan, and Peng (2020) Chakrabarty, Tuhin, Smaranda Muresan,
    and Nanyun Peng. 2020. Generating similes effortlessly like a pro: A style transfer
    approach for simile generation. In *Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020*,
    pages 6455–6469, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen and Dolan (2011) Chen, David L. and William B. Dolan. 2011. Collecting
    highly parallel data for paraphrase evaluation. In *The 49th Annual Meeting of
    the Association for Computational Linguistics: Human Language Technologies, Proceedings
    of the Conference, 19-24 June, 2011, Portland, Oregon, USA*, pages 190–200, The
    Association for Computer Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2017a) Chen, Dongdong, Jing Liao, Lu Yuan, Nenghai Yu, and Gang
    Hua. 2017a. Coherent online video style transfer. In *IEEE International Conference
    on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017*, pages 1114–1123,
    IEEE Computer Society.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2017b) Chen, Dongdong, Lu Yuan, Jing Liao, Nenghai Yu, and Gang
    Hua. 2017b. StyleBank: An explicit representation for neural image style transfer.
    In *2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017,
    Honolulu, HI, USA, July 21-26, 2017*, pages 2770–2779, IEEE Computer Society.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cho et al. (2014) Cho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and
    Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder
    approaches. In *Proceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics
    and Structure in Statistical Translation, Doha, Qatar, 25 October 2014*, pages
    103–111, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohen (1960) Cohen, Jacob. 1960. A coefficient of agreement for nominal scales.
    *Educational and psychological measurement*, 20(1):37–46.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2019) Dai, Ning, Jianze Liang, Xipeng Qiu, and Xuanjing Huang.
    2019. Style transformer: Unpaired text style transfer without disentangled latent
    representation. In *Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics*, pages 5997–6007, Association for Computational
    Linguistics, Florence, Italy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dathathri et al. (2020) Dathathri, Sumanth, Andrea Madotto, Janice Lan, Jane
    Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and
    play language models: A simple approach to controlled text generation. In *8th
    International Conference on Learning Representations, ICLR 2020, Addis Ababa,
    Ethiopia, April 26-30, 2020*, OpenReview.net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short
    Papers)*, pages 4171–4186, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dou, Anastasopoulos, and Neubig (2020) Dou, Zi-Yi, Antonios Anastasopoulos,
    and Graham Neubig. 2020. Dynamic data selection and weighting for iterative back-translation.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2020, Online, November 16-20, 2020*, pages 5894–5904, Association
    for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ferreira et al. (2020) Ferreira, Thiago, Claire Gardent, Nikolai Ilinykh, Chris
    van der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina. 2020. The
    2020 bilingual, bi-directional WebNLG+ shared task overview and evaluation results
    (WebNLG+ 2020). In *Proceedings of the 3rd International Workshop on Natural Language
    Generation from the Semantic Web (WebNLG+)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ficler and Goldberg (2017) Ficler, Jessica and Yoav Goldberg. 2017. Controlling
    linguistic style aspects in neural language generation. *CoRR*, abs/1707.02633.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2019) Fu, Yao, Hao Zhou, Jiaze Chen, and Lei Li. 2019. Rethinking
    text attribute transfer: A lexical analysis. In *Proceedings of the 12th International
    Conference on Natural Language Generation, INLG 2019, Tokyo, Japan, October 29
    - November 1, 2019*, pages 24–33, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2018) Fu, Zhenxin, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui
    Yan. 2018. Style transfer in text: Exploration and evaluation. In *Proceedings
    of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the
    30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th
    AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New
    Orleans, Louisiana, USA, February 2-7, 2018*, pages 663–670, AAAI Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gan et al. (2017) Gan, Chuang, Zhe Gan, Xiaodong He, Jianfeng Gao, and Li Deng.
    2017. StyleNet: Generating attractive visual captions with styles. In *2017 IEEE
    Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI,
    USA, July 21-26, 2017*, pages 955–964, IEEE Computer Society.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao, Singh, and Raj (2018) Gao, Yang, Rita Singh, and Bhiksha Raj. 2018. Voice
    impersonation using generative adversarial networks. In *2018 IEEE International
    Conference on Acoustics, Speech and Signal Processing, ICASSP 2018, Calgary, AB,
    Canada, April 15-20, 2018*, pages 2506–2510, IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gardent et al. (2017) Gardent, Claire, Anastasia Shimorina, Shashi Narayan,
    and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from
    RDF data. In *Proceedings of the 10th International Conference on Natural Language
    Generation, INLG 2017, Santiago de Compostela, Spain, September 4-7, 2017*, pages
    124–133, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gardner et al. (2020) Gardner, Matt, Yoav Artzi, Victoria Basmova, Jonathan
    Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth
    Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi,
    Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh,
    Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and
    Ben Zhou. 2020. Evaluating models’ local decision boundaries via contrast sets.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: Findings, EMNLP 2020, Online Event, 16-20 November 2020*, pages 1307–1323,
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gatt and Reiter (2009) Gatt, Albert and Ehud Reiter. 2009. SimpleNLG: A realisation
    engine for practical applications. In *ENLG 2009 - Proceedings of the 12th European
    Workshop on Natural Language Generation, March 30-31, 2009, Athens, Greece*, pages
    90–93, The Association for Computer Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gatys, Ecker, and Bethge (2016) Gatys, Leon A., Alexander S. Ecker, and Matthias
    Bethge. 2016. Image style transfer using convolutional neural networks. In *2016
    IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas,
    NV, USA, June 27-30, 2016*, pages 2414–2423, IEEE Computer Society.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gehrmann et al. (2021) Gehrmann, Sebastian, Tosin P. Adewumi, Karmanya Aggarwal,
    Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Khyathi Raghavi
    Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh D. Dhole, Wanyu Du, Esin
    Durmus, Ondrej Dusek, Chris Emezue, Varun Gangal, Cristina Garbacea, Tatsunori
    Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly,
    Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad
    Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major,
    Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev,
    Rubungo Andre Niyongabo, Salomey Osei, Ankur P. Parikh, Laura Perez-Beltrachini,
    Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João
    Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla
    Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola,
    and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation
    and metrics. *CoRR*, abs/2102.01672.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gkatzia, Lemon, and Rieser (2017) Gkatzia, Dimitra, Oliver Lemon, and Verena
    Rieser. 2017. Data-to-text generation improves decision-making under uncertainty.
    *IEEE Computational Intelligence Magazine*, 12(3):10–17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gong et al. (2019) Gong, Hongyu, Suma Bhat, Lingfei Wu, JinJun Xiong, and Wen-mei
    Hwu. 2019. Reinforcement learning based text style transfer without parallel training
    corpus. In *Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)*, pages 3168–3180, Association for Computational Linguistics,
    Minneapolis, Minnesota.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Goodfellow, Ian J., Jean Pouget-Abadie, M. Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio.
    2014. Generative adversarial nets. In *NIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodman (1947) Goodman, Nelson. 1947. The problem of counterfactual conditionals.
    *The Journal of Philosophy*, 44(5):113–128.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grégoire and Langlais (2018) Grégoire, Francis and Philippe Langlais. 2018.
    Extracting parallel sentences with bidirectional recurrent neural networks to
    improve machine translation. In *Proceedings of the 27th International Conference
    on Computational Linguistics, COLING 2018, Santa Fe, New Mexico, USA, August 20-26,
    2018*, pages 1442–1453, Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gröndahl and Asokan (2020) Gröndahl, Tommi and N. Asokan. 2020. Effective writing
    style transfer via combinatorial paraphrasing. *Proceedings on Privacy Enhancing
    Technologies*, 2020(4):175–195.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2016) Gu, Jiatao, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016.
    Incorporating copying mechanism in sequence-to-sequence learning. In *Proceedings
    of the 54th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 1631–1640, Association for Computational Linguistics,
    Berlin, Germany.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2018) Gu, Jiatao, Yong Wang, Kyunghyun Cho, and Victor O. K. Li.
    2018. Search engine guided neural machine translation. In *Proceedings of the
    Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th
    innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI
    Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans,
    Louisiana, USA, February 2-7, 2018*, pages 5133–5140, AAAI Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gülçehre et al. (2016) Gülçehre, Çaglar, Sungjin Ahn, Ramesh Nallapati, Bowen
    Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. In *Proceedings of
    the 54th Annual Meeting of the Association for Computational Linguistics, ACL
    2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers*, The Association
    for Computer Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2020) Guo, Qipeng, Zhijing Jin, Ning Dai, Xipeng Qiu, Xiangyang
    Xue, David Wipf, and Zheng Zhang. 2020. P2: A plan-and-pretrain approach for knowledge
    graph-to-text generation. In *Proceedings of the 3rd WebNLG Workshop on Natural
    Language Generation from the Semantic Web (WebNLG+ 2020)*, Association for Computational
    Linguistics, Dublin, Ireland (Virtual).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2021) Guo, Qipeng, Zhijing Jin, Ziyu Wang, Xipeng Qiu, Weinan Zhang,
    Jun Zhu, Zheng Zhang, and David Wipf. 2021. Fork or fail: Cycle-consistent training
    with many-to-one mappings. In *The 24th International Conference on Artificial
    Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event*,
    volume 130 of *Proceedings of Machine Learning Research*, pages 1828–1836, PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guu et al. (2018) Guu, Kelvin, Tatsunori B. Hashimoto, Yonatan Oren, and Percy
    Liang. 2018. Generating sentences by editing prototypes. *Transactions of the
    Association for Computational Linguistics*, 6:437–450.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harrison (2019) Harrison, Sara. 2019. Twitter and instagram unveil new ways
    to combat hate–again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hashimoto et al. (2018) Hashimoto, Tatsunori B., Kelvin Guu, Yonatan Oren,
    and Percy Liang. 2018. A retrieve-and-edit framework for predicting structured
    outputs. In *Advances in Neural Information Processing Systems 31: Annual Conference
    on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018,
    Montréal, Canada*, pages 10073–10083.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He and McAuley (2016) He, Ruining and Julian J. McAuley. 2016. Ups and downs:
    Modeling the visual evolution of fashion trends with one-class collaborative filtering.
    In *Proceedings of the 25th International Conference on World Wide Web, WWW 2016,
    Montreal, Canada, April 11 - 15, 2016*, pages 507–517, ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Henderson (2020) Henderson, James. 2020. The unstoppable rise of computational
    linguistics in deep learning. In *Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020*,
    pages 6294–6306, Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hill, Cho, and Korhonen (2016) Hill, Felix, Kyunghyun Cho, and Anna Korhonen.
    2016. Learning distributed representations of sentences from unlabelled data.
    In *NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, San Diego California,
    USA, June 12-17, 2016*, pages 1367–1377, The Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoang et al. (2018) Hoang, Vu Cong Duy, Philipp Koehn, Gholamreza Haffari, and
    Trevor Cohn. 2018. Iterative back-translation for neural machine translation.
    In *Proceedings of the 2nd Workshop on Neural Machine Translation and Generation*,
    pages 18–24, Association for Computational Linguistics, Melbourne, Australia.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hossain, Ghazvininejad, and Zettlemoyer (2020) Hossain, Nabil, Marjan Ghazvininejad,
    and Luke Zettlemoyer. 2020. Simple and effective retrieve-edit-rerank text generation.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, ACL 2020, Online, July 5-10, 2020*, pages 2532–2538, Association
    for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hovy and Spruit (2016) Hovy, Dirk and Shannon L. Spruit. 2016. The social impact
    of natural language processing. In *Proceedings of the 54th Annual Meeting of
    the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin,
    Germany, Volume 2: Short Papers*, The Association for Computer Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hovy (1987) Hovy, Eduard. 1987. Generating natural language under pragmatic
    constraints. *Journal of Pragmatics*, 11(6):689–719.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hovy (1990) Hovy, Eduard H. 1990. Pragmatics and natural language generation.
    *Artificial Intelligence*, 43(2):153–197.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu, Lee, and Aggarwal (2020) Hu, Zhiqiang, R. K. Lee, and C. Aggarwal. 2020.
    Text style transfer: A review and experiment evaluation. *ArXiv*, abs/2010.12742.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2017) Hu, Zhiting, Zichao Yang, Xiaodan Liang, R. Salakhutdinov,
    and E. Xing. 2017. Toward controlled generation of text. In *ICML*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2018) Huang, Chenyang, Osmar Zaïane, Amine Trabelsi, and Nouha
    Dziri. 2018. Automatic dialogue generation with expressed emotions. In *Proceedings
    of the 2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 2 (Short Papers)*, pages 49–54,
    Association for Computational Linguistics, New Orleans, Louisiana.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2020) Huang, Yufang, Wentao Zhu, Deyi Xiong, Yiye Zhang, Changjian
    Hu, and Feiyu Xu. 2020. Cycle-consistent adversarial autoencoders for unsupervised
    text style transfer. In *Proceedings of the 28th International Conference on Computational
    Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020*, pages
    2213–2223, International Committee on Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jackson et al. (2019) Jackson, Philip T. G., Amir Atapour Abarghouei, Stephen
    Bonner, Toby P. Breckon, and Boguslaw Obara. 2019. Style augmentation: Data augmentation
    via style randomization. In *IEEE Conference on Computer Vision and Pattern Recognition
    Workshops, CVPR Workshops 2019, Long Beach, CA, USA, June 16-20, 2019*, pages
    83–92, Computer Vision Foundation / IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jafaritazehjani et al. (2020) Jafaritazehjani, Somayeh, Gwénolé Lecorvé, Damien
    Lolive, and John Kelleher. 2020. Style versus content: A distinction without a
    (learnable) difference? In *Proceedings of the 28th International Conference on
    Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13,
    2020*, pages 2169–2180, International Committee on Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jang, Gu, and Poole (2017) Jang, Eric, Shixiang Gu, and Ben Poole. 2017. Categorical
    reparameterization with gumbel-softmax. In *5th International Conference on Learning
    Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
    Proceedings*, OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jhamtani et al. (2017) Jhamtani, Harsh, Varun Gangal, Eduard H. Hovy, and Eric
    Nyberg. 2017. Shakespearizing modern language using copy-enriched sequence-to-sequence
    models. *CoRR*, abs/1707.01161.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia and Liang (2017) Jia, Robin and Percy Liang. 2017. Adversarial examples
    for evaluating reading comprehension systems. In *Proceedings of the 2017 Conference
    on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark,
    September 9-11, 2017*, pages 2021–2031, Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. (2020a) Jin, Di, Zhijing Jin, Joey Tianyi Zhou, Lisa Orii, and Peter
    Szolovits. 2020a. Hooks in the headline: Learning to generate headlines with controlled
    styles. In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 5082–5093, Association for Computational Linguistics, Online.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2020b) Jin, Di, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits.
    2020b. Is BERT really robust? A strong baseline for natural language attack on
    text classification and entailment. In *The Thirty-Fourth AAAI Conference on Artificial
    Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial
    Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances
    in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020*,
    pages 8018–8025, AAAI Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. (2019) Jin, Zhijing, Di Jin, Jonas Mueller, Nicholas Matthews, and
    Enrico Santus. 2019. IMaT: Unsupervised text attribute transfer via iterative
    matching and translation. In *Proceedings of the 2019 Conference on Empirical
    Methods in Natural Language Processing and the 9th International Joint Conference
    on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November
    3-7, 2019*, pages 3095–3107, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: John et al. (2019) John, Vineet, Lili Mou, Hareesh Bahuleyan, and Olga Vechtomova.
    2019. Disentangled representation learning for non-parallel text style transfer.
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 424–434, Association for Computational Linguistics, Florence,
    Italy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kajiwara (2019) Kajiwara, Tomoyuki. 2019. Negative lexically constrained decoding
    for paraphrase generation. In *Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics*, pages 6047–6052, Association for Computational
    Linguistics, Florence, Italy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kale and Rastogi (2020) Kale, Mihir and Abhinav Rastogi. 2020. Text-to-text
    pre-training for data-to-text tasks. In *Proceedings of the 13th International
    Conference on Natural Language Generation, INLG 2020, Dublin, Ireland, December
    15-18, 2020*, pages 97–102, Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang, Wang, and de Melo (2020) Kang, Yipeng, Tonghan Wang, and Gerard de Melo.
    2020. Incorporating pragmatic reasoning communication into emergent language.
    In *Advances in Neural Information Processing Systems 33: Annual Conference on
    Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
    virtual*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karadzhov et al. (2017) Karadzhov, Georgi, Tsvetomila Mihaylova, Yasen Kiprov,
    Georgi Georgiev, Ivan Koychev, and Preslav Nakov. 2017. The case for being average:
    A mediocrity approach to style masking and author obfuscation - (best of the labs
    track at CLEF-2017). In *Experimental IR Meets Multilinguality, Multimodality,
    and Interaction - 8th International Conference of the CLEF Association, CLEF 2017,
    Dublin, Ireland, September 11-14, 2017, Proceedings*, volume 10456 of *Lecture
    Notes in Computer Science*, pages 173–185, Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keskar et al. (2019) Keskar, Nitish Shirish, Bryan McCann, Lav R. Varshney,
    Caiming Xiong, and Richard Socher. 2019. CTRL: A conditional transformer language
    model for controllable generation. *CoRR*, abs/1909.05858.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khosmood and Levinson (2010) Khosmood, Foaad and Robert Levinson. 2010. Automatic
    synonym and phrase replacement show promise for style transformation. In *The
    Ninth International Conference on Machine Learning and Applications, ICMLA 2010,
    Washington, DC, USA, 12-14 December 2010*, pages 958–961, IEEE Computer Society.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khosmood and Levinson (2008) Khosmood, Foaad and Robert A Levinson. 2008. Automatic
    natural language style classification and transformation. In *BCS-IRSG Workshop
    on Corpus Profiling*, pages 1–11.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim (2014) Kim, Yoon. 2014. Convolutional neural networks for sentence classification.
    In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT,
    a Special Interest Group of the ACL*, pages 1746–1751, ACL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling (2014) Kingma, Diederik P. and M. Welling. 2014. Auto-encoding
    variational bayes. *CoRR*, abs/1312.6114.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koncel-Kedziorski et al. (2016) Koncel-Kedziorski, Rik, Ioannis Konstas, Luke
    Zettlemoyer, and Hannaneh Hajishirzi. 2016. A theme-rewriting approach for generating
    algebra word problems. In *Proceedings of the 2016 Conference on Empirical Methods
    in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4,
    2016*, pages 1617–1628, The Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kowalski et al. (2014) Kowalski, Robin M, Gary W Giumetti, Amber N Schroeder,
    and Micah R Lattanner. 2014. Bullying in the digital age: A critical review and
    meta-analysis of cyberbullying research among youth. *Psychological bulletin*,
    140(4):1073.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krippendorff (2018) Krippendorff, Klaus. 2018. *Content analysis: An introduction
    to its methodology*. Sage publications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krishna, Wieting, and Iyyer (2020) Krishna, Kalpesh, John Wieting, and Mohit
    Iyyer. 2020. Reformulating unsupervised style transfer as paraphrase generation.
    *CoRR*, abs/2010.05700.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kusner et al. (2015) Kusner, Matt, Yu Sun, Nicholas Kolkin, and Kilian Weinberger.
    2015. From word embeddings to document distances. In *International Conference
    on Machine Learning*, pages 957–966.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lai, Toral, and Nissim (2021) Lai, Huiyuan, Antonio Toral, and M. Nissim. 2021.
    Thank you bart! rewarding pre-trained models improves formality style transfer.
    In *ACL/IJCNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lakoff (1973) Lakoff, Robin. 1973. Language and woman’s place. *Language in
    society*, 2(1):45–79.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lample et al. (2018a) Lample, Guillaume, Alexis Conneau, Ludovic Denoyer, and
    Marc’Aurelio Ranzato. 2018a. Unsupervised machine translation using monolingual
    corpora only. In *6th International Conference on Learning Representations, ICLR
    2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*,
    OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lample et al. (2018b) Lample, Guillaume, Myle Ott, Alexis Conneau, Ludovic Denoyer,
    and Marc’Aurelio Ranzato. 2018b. Phrase-based & neural unsupervised machine translation.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing, Brussels, Belgium, October 31 - November 4, 2018*, pages 5039–5049,
    Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lample et al. (2019) Lample, Guillaume, Sandeep Subramanian, Eric Michael Smith,
    Ludovic Denoyer, Marc’Aurelio Ranzato, and Y-Lan Boureau. 2019. Multiple-attribute
    text rewriting. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee (2020) Lee, Joosung. 2020. Stable style transformer: Delete and generate
    approach with encoder-decoder for text style transfer. *CoRR*, abs/2005.12086.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis et al. (2020) Lewis, Patrick S. H., Ethan Perez, Aleksandra Piktus, Fabio
    Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau
    Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented
    generation for knowledge-intensive NLP tasks. *CoRR*, abs/2005.11401.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019) Li, Dianqi, Yizhe Zhang, Zhe Gan, Yu Cheng, Chris Brockett,
    Bill Dolan, and Ming-Ting Sun. 2019. Domain adaptive text style transfer. In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP
    2019, Hong Kong, China, November 3-7, 2019*, pages 3302–3311, Association for
    Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2016) Li, Jiwei, Michel Galley, Chris Brockett, Georgios P. Spithourakis,
    Jianfeng Gao, and William B. Dolan. 2016. A persona-based neural conversation
    model. In *Proceedings of the 54th Annual Meeting of the Association for Computational
    Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers*,
    The Association for Computer Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018) Li, Juncen, Robin Jia, He He, and Percy Liang. 2018. Delete,
    retrieve, generate: A simple approach to sentiment and style transfer. In *Proceedings
    of the 2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 1865–1874,
    Association for Computational Linguistics, New Orleans, Louisiana.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) Li, Mingzhe, Xiuying Chen, Min Yang, Shen Gao, Dongyan Zhao,
    and Rui Yan. 2021. The style-content duality of attractiveness: Learning to write
    eye-catching headlines via disentanglement. In *Thirty-Fifth AAAI Conference on
    Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications
    of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances
    in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021*, pages
    13252–13260, AAAI Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Liang (2021) Li, Xiang Lisa and Percy Liang. 2021. Prefix-tuning: Optimizing
    continuous prompts for generation. In *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers),
    Virtual Event, August 1-6, 2021*, pages 4582–4597, Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Li, Yuan, Chunyuan Li, Yizhe Zhang, Xiujun Li, Guoqing Zheng,
    Lawrence Carin, and Jianfeng Gao. 2020. Complementary auxiliary classifiers for
    label-conditional text generation. In *The Thirty-Fourth AAAI Conference on Artificial
    Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial
    Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances
    in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020*,
    pages 8303–8310, AAAI Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao et al. (2018) Liao, Yi, Lidong Bing, Piji Li, Shuming Shi, Wai Lam, and
    Tong Zhang. 2018. QuaSE: Sequence editing under quantifiable guidance. In *Proceedings
    of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages
    3855–3864, Association for Computational Linguistics, Brussels, Belgium.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin and Och (2004) Lin, Chin-Yew and Franz Josef Och. 2004. Automatic evaluation
    of machine translation quality using longest common subsequence and skip-bigram
    statistics. In *Proceedings of the 42nd Annual Meeting of the Association for
    Computational Linguistics (ACL-04)*, pages 605–612, Barcelona, Spain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2020) Liu, Dayiheng, Jie Fu, Yidan Zhang, Chris Pal, and Jiancheng
    Lv. 2020. Revision in continuous space: Unsupervised text style transfer without
    adversarial learning. In *The Thirty-Fourth AAAI Conference on Artificial Intelligence,
    AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
    Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial
    Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020*, pages 8376–8383,
    AAAI Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locatello et al. (2019) Locatello, Francesco, Stefan Bauer, Mario Lucic, Gunnar
    Rätsch, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. 2019. Challenging
    common assumptions in the unsupervised learning of disentangled representations.
    In *Proceedings of the 36th International Conference on Machine Learning, ICML
    2019, 9-15 June 2019, Long Beach, California, USA*, volume 97 of *Proceedings
    of Machine Learning Research*, pages 4114–4124, PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locatello et al. (2020) Locatello, Francesco, Ben Poole, Gunnar Rätsch, Bernhard
    Schölkopf, Olivier Bachem, and Michael Tschannen. 2020. Weakly-supervised disentanglement
    without compromises. In *Proceedings of the 37th International Conference on Machine
    Learning, ICML 2020, 13-18 July 2020, Virtual Event*, volume 119 of *Proceedings
    of Machine Learning Research*, pages 6348–6359, PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logeswaran, Lee, and Bengio (2018) Logeswaran, Lajanugen, Honglak Lee, and
    Samy Bengio. 2018. Content preserving text generation with attribute controls.
    In *Advances in Neural Information Processing Systems 31: Annual Conference on
    Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
    Montréal, Canada*, pages 5108–5118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2019) Luo, Fuli, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang,
    Zhifang Sui, and Xu Sun. 2019. A dual reinforcement learning framework for unsupervised
    text style transfer. In *Proceedings of the 28th International Joint Conference
    on Artificial Intelligence, IJCAI 2019*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2020) Ma, Xinyao, Maarten Sap, Hannah Rashkin, and Yejin Choi. 2020.
    PowerTransformer: Unsupervised controllable revision for biased language correction.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2020, Online, November 16-20, 2020*, pages 7426–7441, Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al. (2020) Madaan, Aman, Amrith Setlur, Tanmay Parekh, Barnabás Póczos,
    Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan W. Black, and Shrimai Prabhumoye.
    2020. Politeness transfer: A tag and generate approach. In *Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,
    Online, July 5-10, 2020*, pages 1869–1881, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madnani and Dorr (2010) Madnani, Nitin and Bonnie J. Dorr. 2010. Generating
    phrasal and sentential paraphrases: A survey of data-driven methods. *Compututational
    Linguistics*, 36(3):341–387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mairesse and Walker (2011) Mairesse, François and Marilyn A. Walker. 2011.
    Controlling user perceptions of linguistic style: Trainable generation of personality
    traits. *Computational Linguistics*, 37(3):455–488.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malmi, Severyn, and Rothe (2020a) Malmi, Eric, Aliaksei Severyn, and Sascha
    Rothe. 2020a. Unsupervised text style transfer with padded masked language models.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2020, Online, November 16-20, 2020*, pages 8671–8680, Association
    for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malmi, Severyn, and Rothe (2020b) Malmi, Eric, Aliaksei Severyn, and Sascha
    Rothe. 2020b. Unsupervised text style transfer with padded masked language models.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 8671–8680, Association for Computational Linguistics,
    Online.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mani (2001) Mani, Inderjeet. 2001. *Automatic summarization*, volume 3. John
    Benjamins Publishing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mansoorizadeh et al. (2016) Mansoorizadeh, Muharram, Taher Rahgooy, Mohammad
    Aminiyan, and Mahdy Eskandari. 2016. Author obfuscation using wordnet and language
    models—notebook for pan at clef 2016. In *CLEF 2016 Evaluation Labs and Workshop–Working
    Notes Papers*, pages 5–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marie and Fujita (2017) Marie, Benjamin and Atsushi Fujita. 2017. Efficient
    extraction of pseudo-parallel sentences from raw monolingual data using word embeddings.
    In *Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 2: Short
    Papers*, pages 392–398, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McDonald and Pustejovsky (1985) McDonald, David D. and James Pustejovsky. 1985.
    A computational theory of prose style for natural language generation. In *EACL
    1985, 2nd Conference of the European Chapter of the Association for Computational
    Linguistics, March 27-29, 1985, University of Geneva, Geneva, Switzerland*, pages
    187–193, The Association for Computer Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McTear (2002) McTear, Michael F. 2002. Spoken dialogue technology: Enabling
    the conversational user interface. *ACM Computing Surveys*, 34(1):90–169.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. (2017) Merity, Stephen, Caiming Xiong, James Bradbury, and R. Socher.
    2017. Pointer sentinel mixture models. *ArXiv*, abs/1609.07843.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mir et al. (2019) Mir, Remi, Bjarke Felbo, Nick Obradovich, and Iyad Rahwan.
    2019. Evaluating style transfer for text. In *Proceedings of the 2019 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 495–504,
    Association for Computational Linguistics, Minneapolis, Minnesota.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mou and Vechtomova (2020) Mou, Lili and Olga Vechtomova. 2020. Stylized text
    generation: Approaches and applications. In *Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics: Tutorial Abstracts*, pages 19–22,
    Association for Computational Linguistics, Online.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mueller, Gifford, and Jaakkola (2017) Mueller, Jonas, David K. Gifford, and
    Tommi S. Jaakkola. 2017. Sequence to better sequence: Continuous revision of combinatorial
    structures. In *Proceedings of the 34th International Conference on Machine Learning,
    ICML 2017, Sydney, NSW, Australia, 6-11 August 2017*, volume 70 of *Proceedings
    of Machine Learning Research*, pages 2536–2544, PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Munteanu and Marcu (2005) Munteanu, Dragos Stefan and Daniel Marcu. 2005. Improving
    machine translation performance by exploiting non-parallel corpora. *Computational
    Linguistics*, 31(4):477–504.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nikolov and Hahnloser (2019) Nikolov, Nikola I. and Richard H. R. Hahnloser.
    2019. Large-scale hierarchical alignment for data-driven text rewriting. In *Proceedings
    of the International Conference on Recent Advances in Natural Language Processing,
    RANLP 2019, Varna, Bulgaria, September 2-4, 2019*, pages 844–853, INCOMA Ltd.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Niu and Bansal (2018) Niu, Tong and Mohit Bansal. 2018. Polite dialogue generation
    without parallel data. *Transactions of the Association for Computational Linguistics*,
    6:373–389.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Niu, Martindale, and Carpuat (2017) Niu, Xing, Marianna J. Martindale, and
    Marine Carpuat. 2017. A study of style in machine translation: Controlling the
    formality of machine translation output. In *Proceedings of the 2017 Conference
    on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark,
    September 9-11, 2017*, pages 2814–2819, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Niu, Rao, and Carpuat (2018) Niu, Xing, Sudha Rao, and Marine Carpuat. 2018.
    Multi-task neural models for translating between styles within and across languages.
    In *Proceedings of the 27th International Conference on Computational Linguistics*,
    pages 1008–1021, Association for Computational Linguistics, Santa Fe, New Mexico,
    USA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Novikova, Dusek, and Rieser (2017) Novikova, Jekaterina, Ondrej Dusek, and
    Verena Rieser. 2017. The E2E dataset: New challenges for end-to-end generation.
    In *Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue,
    Saarbrücken, Germany, August 15-17, 2017*, pages 201–206, Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oksanen et al. (2014) Oksanen, Atte, James Hawdon, Emma Holkeri, Matti Näsi,
    and Pekka Räsänen. 2014. Exposure to online hate among young social media users.
    In *Soul of society: A focus on the lives of children & youth*. Emerald Group
    Publishing Limited.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pang (2019) Pang, Richard Yuanzhe. 2019. The daunting task of real-world textual
    style transfer auto-evaluation. *CoRR*, abs/1910.03747.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pang and Gimpel (2019) Pang, Richard Yuanzhe and Kevin Gimpel. 2019. Unsupervised
    evaluation metrics and learning criteria for non-parallel textual transfer. In
    *Proceedings of the 3rd Workshop on Neural Generation and Translation@EMNLP-IJCNLP
    2019, Hong Kong, November 4, 2019*, pages 138–147, Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In
    *Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics*,
    pages 311–318, Association for Computational Linguistics, Philadelphia, Pennsylvania,
    USA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parikh et al. (2020) Parikh, Ankur P., Xuezhi Wang, Sebastian Gehrmann, Manaal
    Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: A controlled
    table-to-text generation dataset. In *Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020*,
    pages 1173–1186, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pfaff (1979) Pfaff, Carol W. 1979. Constraints on language mixing: Intrasentential
    code-switching and borrowing in Spanish/English. *Language*, pages 291–318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Poplack (2000) Poplack, Shana. 2000. Sometimes I’ll start a sentence in Spanish
    y termino en español: Toward a typology of code-switching. *The bilingualism reader*,
    18(2):221–256.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Popović (2015) Popović, Maja. 2015. chrF: Character n-gram F-score for automatic
    MT evaluation. In *Proceedings of the Tenth Workshop on Statistical Machine Translation*,
    pages 392–395, Association for Computational Linguistics, Lisbon, Portugal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Post and Vilar (2018) Post, Matt and David Vilar. 2018. Fast lexically constrained
    decoding with dynamic beam allocation for neural machine translation. In *Proceedings
    of the 2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 1314–1324,
    Association for Computational Linguistics, New Orleans, Louisiana.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Power, Scott, and Bouayad-Agha (2003) Power, Richard, Donia Scott, and Nadjet
    Bouayad-Agha. 2003. Generating texts with style. In *International Conference
    on Intelligent Text Processing and Computational Linguistics*, pages 444–452,
    Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prabhumoye et al. (2018) Prabhumoye, Shrimai, Yulia Tsvetkov, Ruslan Salakhutdinov,
    and Alan W Black. 2018. Style transfer through back-translation. In *Proceedings
    of the 56th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 866–876, Association for Computational Linguistics, Melbourne,
    Australia.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pryzant et al. (2020) Pryzant, Reid, Richard Diehl Martinez, Nathan Dass, Sadao
    Kurohashi, Dan Jurafsky, and Diyi Yang. 2020. Automatically neutralizing subjective
    bias in text. In *The Thirty-Fourth AAAI Conference on Artificial Intelligence,
    AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
    Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial
    Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020*, pages 480–489,
    AAAI Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qian et al. (2019) Qian, Kaizhi, Yang Zhang, Shiyu Chang, Xuesong Yang, and
    Mark Hasegawa-Johnson. 2019. AutoVC: Zero-shot voice style transfer with only
    autoencoder loss. In *Proceedings of the 36th International Conference on Machine
    Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA*, volume 97 of
    *Proceedings of Machine Learning Research*, pages 5210–5219, PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin and Eisner (2021) Qin, Guanghui and Jason Eisner. 2021. Learning how to
    ask: Querying LMs with mixtures of soft prompts. In *Proceedings of the 2021 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021*, pages 5203–5212,
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2019) Qin, Lianhui, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula,
    Elizabeth Clark, and Yejin Choi. 2019. Counterfactual story reasoning and generation.
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing,
    EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019*, pages 5042–5052, Association
    for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.
    *OpenAI Blog*, 1(8):9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of Machine Learning Research*, 21:140:1–140:67.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rao and Tetreault (2018) Rao, Sudha and Joel Tetreault. 2018. Dear sir or madam,
    may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality
    style transfer. In *Proceedings of the 2018 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)*, pages 129–140, Association for Computational Linguistics,
    New Orleans, Louisiana.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reddy and Knight (2016) Reddy, Sravana and Kevin Knight. 2016. Obfuscating gender
    in social media writing. In *Proceedings of the First Workshop on NLP and Computational
    Social Science, NLP+CSS@EMNLP 2016, Austin, TX, USA, November 5, 2016*, pages
    17–26, Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reiter and Dale (1997) Reiter, Ehud and Robert Dale. 1997. Building applied
    natural language generation systems. *Natural Language Engineering*, 3(1):57–87.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reiter, Robertson, and Osman (2003) Reiter, Ehud, Roma Robertson, and Liesl M
    Osman. 2003. Lessons from a failure: Generating tailored smoking cessation letters.
    *Artificial Intelligence*, 144(1-2):41–58.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reiter et al. (2005) Reiter, Ehud, Somayajulu Sripada, Jim Hunter, Jin Yu, and
    Ian Davy. 2005. Choosing words in computer-generated weather forecasts. *Artificial
    Intelligence*, 167(1-2):137–169.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2020) Ren, Shuo, Yu Wu, Shujie Liu, Ming Zhou, and Shuai Ma. 2020.
    A retrieve-and-rewrite initialization method for unsupervised machine translation.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, ACL 2020, Online, July 5-10, 2020*, pages 3498–3504, Association
    for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rezende, Mohamed, and Wierstra (2014) Rezende, Danilo Jimenez, Shakir Mohamed,
    and Daan Wierstra. 2014. Stochastic backpropagation and approximate inference
    in deep generative models. In *Proceedings of the 31th International Conference
    on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014*, volume 32 of
    *JMLR Workshop and Conference Proceedings*, pages 1278–1286, JMLR.org.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ribeiro et al. (2020) Ribeiro, Leonardo F. R., Martin Schmitt, Hinrich Schütze,
    and Iryna Gurevych. 2020. Investigating pretrained language models for graph-to-text
    generation. *CoRR*, abs/2007.08426.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Riley et al. (2021a) Riley, Parker, Noah Constant, Mandy Guo, Girish Kumar,
    David Uthus, and Zarana Parekh. 2021a. TextSETTR: Few-shot text style extraction
    and tunable targeted restyling. In *Proceedings of the 59th Annual Meeting of
    the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers)*, pages 3786–3800,
    Association for Computational Linguistics, Online.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Riley et al. (2021b) Riley, Parker, Noah Constant, Mandy Guo, Girish Kumar,
    David C. Uthus, and Zarana Parekh. 2021b. TextSettr: Few-shot text style extraction
    and tunable targeted restyling. In *ACL/IJCNLP*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roller et al. (2021) Roller, Stephen, Emily Dinan, Naman Goyal, Da Ju, Mary
    Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau,
    and Jason Weston. 2021. Recipes for building an open-domain chatbot. In *Proceedings
    of the 16th Conference of the European Chapter of the Association for Computational
    Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021*, pages 300–325,
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Romanov et al. (2019) Romanov, Alexey, Anna Rumshisky, Anna Rogers, and David
    Donahue. 2019. Adversarial decomposition of text representation. In *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    815–825, Association for Computational Linguistics, Minneapolis, Minnesota.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruder, Dosovitskiy, and Brox (2016) Ruder, Manuel, Alexey Dosovitskiy, and Thomas
    Brox. 2016. Artistic style transfer for videos. In *Pattern Recognition - 38th
    German Conference, GCPR 2016, Hannover, Germany, September 12-15, 2016, Proceedings*,
    volume 9796 of *Lecture Notes in Computer Science*, pages 26–36, Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rush, Chopra, and Weston (2015) Rush, Alexander M., Sumit Chopra, and Jason
    Weston. 2015. A neural attention model for abstractive sentence summarization.
    In *Proceedings of the 2015 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015*, pages 379–389,
    The Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Russell, Dewey, and Tegmark (2015) Russell, Stuart J., Daniel Dewey, and Max
    Tegmark. 2015. Research priorities for robust and beneficial artificial intelligence.
    *AI Magazine*, 36(4):105–114.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sancheti et al. (2020) Sancheti, Abhilasha, Kundan Krishna, Balaji Vasan Srinivasan,
    and Anandhavelu Natarajan. 2020. Reinforced rewards framework for text style transfer.
    In *European Conference on Information Retrieval*, pages 545–560, Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'dos Santos, Melnyk, and Padhi (2018) dos Santos, Cícero Nogueira, Igor Melnyk,
    and Inkit Padhi. 2018. Fighting offensive language on social media with unsupervised
    text style transfer. In *Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018,
    Volume 2: Short Papers*, pages 189–194, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scao and Rush (2021) Scao, Teven Le and Alexander M. Rush. 2021. How many data
    points is a prompt worth? In *Proceedings of the 2021 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies, NAACL-HLT 2021, Online, June 6-11, 2021*, pages 2627–2636, Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schler et al. (2006) Schler, Jonathan, Moshe Koppel, Shlomo Argamon, and James W.
    Pennebaker. 2006. Effects of age and gender on blogging. In *Computational Approaches
    to Analyzing Weblogs, Papers from the 2006 AAAI Spring Symposium, Technical Report
    SS-06-03, Stanford, California, USA, March 27-29, 2006*, pages 199–205, AAAI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See, Liu, and Manning (2017) See, Abigail, Peter J. Liu, and Christopher D.
    Manning. 2017. Get to the point: Summarization with pointer-generator networks.
    In *Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers*,
    pages 1073–1083, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sennrich, Haddow, and Birch (2016a) Sennrich, Rico, Barry Haddow, and Alexandra
    Birch. 2016a. Controlling politeness in neural machine translation via side constraints.
    In *NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, San Diego California,
    USA, June 12-17, 2016*, pages 35–40, The Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sennrich, Haddow, and Birch (2016b) Sennrich, Rico, Barry Haddow, and Alexandra
    Birch. 2016b. Edinburgh neural machine translation systems for WMT 16. In *Proceedings
    of the First Conference on Machine Translation: Volume 2, Shared Task Papers*,
    pages 371–376, Association for Computational Linguistics, Berlin, Germany.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shang et al. (2019) Shang, Mingyue, Piji Li, Zhenxin Fu, Lidong Bing, Dongyan
    Zhao, Shuming Shi, and Rui Yan. 2019. Semi-supervised text style transfer: Cross
    projection in latent space. In *Proceedings of the 2019 Conference on Empirical
    Methods in Natural Language Processing and the 9th International Joint Conference
    on Natural Language Processing (EMNLP-IJCNLP)*, pages 4937–4946, Association for
    Computational Linguistics, Hong Kong, China.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shao et al. (2018) Shao, Chengcheng, Giovanni Luca Ciampaglia, Onur Varol, Kai-Cheng
    Yang, Alessandro Flammini, and Filippo Menczer. 2018. The spread of low-credibility
    content by social bots. *Nature communications*, 9(1):1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharma et al. (2021) Sharma, Ashish, Inna W. Lin, Adam S. Miner, David C. Atkins,
    and Tim Althoff. 2021. Towards facilitating empathic conversations in online mental
    health support: A reinforcement learning approach. *CoRR*, abs/2101.07714.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheikha and Inkpen (2011) Sheikha, Fadi Abu and Diana Inkpen. 2011. Generation
    of formal and informal sentences. In *ENLG 2011 - Proceedings of the 13th European
    Workshop on Natural Language Generation, 28-30 September 2011, Nancy, France*,
    pages 187–193, The Association for Computer Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2017) Shen, Tianxiao, Tao Lei, Regina Barzilay, and Tommi Jaakkola.
    2017. Style transfer from non-parallel text by cross-alignment. In *Advances in
    neural information processing systems*, pages 6830–6841.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shetty and Adibi (2004) Shetty, Jitesh and Jafar Adibi. 2004. The enron email
    dataset database schema and brief statistical report. *Information sciences institute
    technical report, University of Southern California*, 4(1):120–128.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shetty, Schiele, and Fritz (2018) Shetty, Rakshith, Bernt Schiele, and Mario
    Fritz. 2018. A4NT: author attribute anonymity by adversarial training of neural
    machine translation. In *27th USENIX Security Symposium, USENIX Security 2018,
    Baltimore, MD, USA, August 15-17, 2018*, pages 1633–1650, USENIX Association.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shuster et al. (2020) Shuster, Kurt, Samuel Humeau, Antoine Bordes, and Jason
    Weston. 2020. Image-Chat: Engaging grounded conversations. In *Proceedings of
    the 58th Annual Meeting of the Association for Computational Linguistics, ACL
    2020, Online, July 5-10, 2020*, pages 2414–2429, Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2019) Song, Kaitao, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan
    Liu. 2019. MASS: masked sequence to sequence pre-training for language generation.
    In *Proceedings of the 36th International Conference on Machine Learning, ICML
    2019, 9-15 June 2019, Long Beach, California, USA*, volume 97 of *Proceedings
    of Machine Learning Research*, pages 5926–5936, PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sripada et al. (2004) Sripada, Somayajulu, Ehud Reiter, Ian Davy, and Kristian
    Nilssen. 2004. Lessons from deploying NLG technology for marine weather forecast
    text generation. In *Proceedings of the 16th Eureopean Conference on Artificial
    Intelligence, ECAI’2004, including Prestigious Applicants of Intelligent Systems,
    PAIS 2004, Valencia, Spain, August 22-27, 2004*, pages 760–764, IOS Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stamatatos et al. (1997) Stamatatos, Efstathios, S Michos, Nikos Fakotakis,
    and George Kokkinakis. 1997. A user-assisted business letter generator dealing
    with text’s stylistic variations. In *Proceedings Ninth IEEE International Conference
    on Tools with Artificial Intelligence*, pages 182–189, IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starr (2019) Starr, William. 2019. Counterfactuals. *The Stanford Encyclopedia
    of Philosophy*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sudhakar, Upadhyay, and Maheswaran (2019) Sudhakar, Akhilesh, Bhargav Upadhyay,
    and Arjun Maheswaran. 2019. "transforming" delete, retrieve, generate approach
    for controlled text style transfer. In *Proceedings of the 2019 Conference on
    Empirical Methods in Natural Language Processing and the 9th International Joint
    Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China,
    November 3-7, 2019*, pages 3267–3277, Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever, Vinyals, and Le (2014) Sutskever, Ilya, Oriol Vinyals, and Quoc V
    Le. 2014. Sequence to sequence learning with neural networks. In *Advances in
    neural information processing systems*, pages 3104–3112.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Syed et al. (2020) Syed, Bakhtiyar, Gaurav Verma, Balaji Vasan Srinivasan, Anandhavelu
    Natarajan, and Vasudeva Varma. 2020. Adapting language models for non-parallel
    author-stylized rewriting. In *AAAI*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan and Goonawardene (2017) Tan, Sharon Swee-Lin and Nadee Goonawardene. 2017.
    Internet health information seeking and the patient-physician relationship: A
    systematic review. *Journal of medical Internet research*, 19(1):e9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tannen (1990) Tannen, Deborah. 1990. Gender differences in topical coherence:
    Creating involvement in best friends’ talk. *Discourse Processes*, 13(1):73–90.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian, Hu, and Yu (2018) Tian, Youzhi, Zhiting Hu, and Zhou Yu. 2018. Structured
    content preservation for unsupervised text style transfer. *CoRR*, abs/1810.06526.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tikhonov et al. (2019) Tikhonov, Alexey, Viacheslav Shibaev, Aleksander Nagaev,
    Aigul Nugmanova, and Ivan P. Yamshchikov. 2019. Style transfer for texts: Retrain,
    report errors, compare with rewrites. In *Proceedings of the 2019 Conference on
    Empirical Methods in Natural Language Processing and the 9th International Joint
    Conference on Natural Language Processing (EMNLP-IJCNLP)*, pages 3936–3945, Association
    for Computational Linguistics, Hong Kong, China.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tikhonov and Yamshchikov (2018) Tikhonov, Alexey and Ivan P. Yamshchikov. 2018.
    What is wrong with style transfer for texts? *CoRR*, abs/1808.04365.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tran, Zhang, and Soleymani (2020) Tran, Minh, Yipeng Zhang, and Mohammad Soleymani.
    2020. Towards a friendly online community: An unsupervised style transfer framework
    for profanity redaction. *CoRR*, abs/2011.00403.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trudgill (1972) Trudgill, Peter. 1972. Sex, covert prestige and linguistic change
    in the urban british english of norwich. *Language in society*, 1(2):179–195.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tse et al. (2015) Tse, Jonathan, Dawn E. Schrader, Dipayan P. Ghosh, Tony C.
    Liao, and David Lundie. 2015. A bibliometric analysis of privacy and ethics in
    *IEEE Security and Privacy*. *Ethics and Information Technology*, 17(2):153–163.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uszkoreit et al. (2010) Uszkoreit, Jakob, Jay Ponte, Ashok C. Popat, and Moshe
    Dubiner. 2010. Large scale parallel document mining for machine translation. In
    *COLING 2010, 23rd International Conference on Computational Linguistics, Proceedings
    of the Conference, 23-27 August 2010, Beijing, China*, pages 1101–1109, Tsinghua
    University Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vaswani et al. (2017) Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *Advances in Neural Information Processing Systems 30: Annual
    Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
    Long Beach, CA, USA*, pages 5998–6008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vincent et al. (2010) Vincent, Pascal, Hugo Larochelle, Isabelle Lajoie, Yoshua
    Bengio, and Pierre-Antoine Manzagol. 2010. Stacked denoising autoencoders: Learning
    useful representations in a deep network with a local denoising criterion. *Journal
    of Machine Learning Research*, 11:3371–3408.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voigt et al. (2018) Voigt, Rob, David Jurgens, Vinodkumar Prabhakaran, Dan
    Jurafsky, and Yulia Tsvetkov. 2018. RtGender: A corpus for studying differential
    responses to gender. In *Proceedings of the Eleventh International Conference
    on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018*,
    European Language Resources Association (ELRA).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang, Quan, and Wang (2019) Wang, Kai, Xiaojun Quan, and Rui Wang. 2019. BiSET:
    Bi-directional selective encoding with template for abstractive summarization.
    In *Proceedings of the 57th Conference of the Association for Computational Linguistics,
    ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers*, pages
    2153–2162, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang, Hua, and Wan (2019) Wang, Ke, Hang Hua, and Xiaojun Wan. 2019. Controllable
    unsupervised text attribute transfer via editing entangled latent representation.
    In *Advances in Neural Information Processing Systems 32: Annual Conference on
    Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
    Vancouver, BC, Canada*, pages 11034–11044.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Wang, Yunli, Yu Wu, Lili Mou, Zhoujun Li, and Wenhan Chao.
    2019. Harnessing pre-trained neural networks with rules for formality style transfer.
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 3573–3578, Association for Computational Linguistics, Hong
    Kong, China.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Waseem et al. (2017) Waseem, Zeerak, Thomas Davidson, Dana Warmsley, and Ingmar
    Weber. 2017. Understanding abuse: A typology of abusive language detection subtasks.
    In *Proceedings of the First Workshop on Abusive Language Online, ALW@ACL 2017,
    Vancouver, BC, Canada, August 4, 2017*, pages 78–84, Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weng, Chung, and Szolovits (2019) Weng, Wei-Hung, Yu-An Chung, and Peter Szolovits.
    2019. Unsupervised clinical language translation. In *Proceedings of the 25th
    ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD
    2019, Anchorage, AK, USA, August 4-8, 2019*, pages 3121–3131, ACM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weston, Dinan, and Miller (2018) Weston, Jason, Emily Dinan, and Alexander H.
    Miller. 2018. Retrieve and refine: Improved sequence generation models for dialogue.
    In *Proceedings of the 2nd International Workshop on Search-Oriented Conversational
    AI, SCAI@EMNLP 2018, Brussels, Belgium, October 31, 2018*, pages 87–92, Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams (1992) Williams, Ronald J. 1992. Simple statistical gradient-following
    algorithms for connectionist reinforcement learning. *Machine learning*, 8(3-4):229–256.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wiseman, Shieber, and Rush (2017) Wiseman, Sam, Stuart M. Shieber, and Alexander M.
    Rush. 2017. Challenges in data-to-document generation. In *Proceedings of the
    2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017,
    Copenhagen, Denmark, September 9-11, 2017*, pages 2253–2263, Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu, Wang, and Wang (2019) Wu, Jiawei, Xin Wang, and William Yang Wang. 2019.
    Extract and edit: An alternative to back-translation for unsupervised neural machine
    translation. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short
    Papers)*, pages 1173–1183, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019) Wu, Xing, Tao Zhang, Liangjun Zang, Jizhong Han, and Songlin
    Hu. 2019. "mask and infill" : Applying masked language model to sentiment transfer.
    *CoRR*, abs/1908.08039.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu, Wang, and Liu (2020) Wu, Yu, Yunli Wang, and Shujie Liu. 2020. A dataset
    for low-resource stylized sequence-to-sequence generation. In *The Thirty-Fourth
    AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative
    Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI
    Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York,
    NY, USA, February 7-12, 2020*, pages 9290–9297, AAAI Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xing et al. (2020) Xing, Xiaoyu, Zhijing Jin, Di Jin, Bingning Wang, Qi Zhang,
    and Xuanjing Huang. 2020. Tasty burgers, soggy fries: Probing aspect robustness
    in aspect-based sentiment analysis. In *Proceedings of the 2020 Conference on
    Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November
    16-20, 2020*, pages 3594–3605, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2018) Xu, Jingjing, Xu Sun, Qi Zeng, Xiaodong Zhang, Xuancheng Ren,
    Houfeng Wang, and Wenjie Li. 2018. Unpaired sentiment-to-sentiment translation:
    A cycled reinforcement learning approach. In *Proceedings of the 56th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages
    979–988, Association for Computational Linguistics, Melbourne, Australia.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu, Cheung, and Cao (2020) Xu, Peng, Jackie Chi Kit Cheung, and Yanshuai Cao.
    2020. On variational learning of controllable representations for text without
    supervision. In *Proceedings of the 37th International Conference on Machine Learning,
    ICML 2020, 13-18 July 2020, Virtual Event*, volume 119 of *Proceedings of Machine
    Learning Research*, pages 10534–10543, PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2020) Xu, Qiuling, Guanhong Tao, Siyuan Cheng, Lin Tan, and Xiangyu
    Zhang. 2020. Towards feature space adversarial attack. *CoRR*, abs/2004.12385.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu, Ge, and Wei (2019) Xu, Ruochen, Tao Ge, and Furu Wei. 2019. Formality style
    transfer with hybrid textual annotations. *CoRR*, abs/1903.06353.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2012) Xu, Wei, Alan Ritter, Bill Dolan, Ralph Grishman, and Colin
    Cherry. 2012. Paraphrasing for style. In *COLING 2012, 24th International Conference
    on Computational Linguistics, Proceedings of the Conference: Technical Papers,
    8-15 December 2012, Mumbai, India*, pages 2899–2914, Indian Institute of Technology
    Bombay.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yamshchikov et al. (2021) Yamshchikov, Ivan P., Viacheslav Shibaev, Nikolay
    Khlebnikov, and Alexey Tikhonov. 2021. Style-transfer and paraphrase: Looking
    for a sensible semantic similarity metric. In *Thirty-Fifth AAAI Conference on
    Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications
    of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances
    in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021*, pages
    14213–14220, AAAI Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yamshchikov et al. (2019) Yamshchikov, Ivan P., Viacheslav Shibaev, Aleksander
    Nagaev, Jürgen Jost, and Alexey Tikhonov. 2019. Decomposing textual information
    for style transfer. In *Proceedings of the 3rd Workshop on Neural Generation and
    Translation*, pages 128–137, Association for Computational Linguistics, Hong Kong.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2018) Yang, Zichao, Zhiting Hu, Chris Dyer, Eric P. Xing, and
    Taylor Berg-Kirkpatrick. 2018. Unsupervised text style transfer using language
    models as discriminators. In *Advances in Neural Information Processing Systems
    31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
    3-8 December 2018, Montréal, Canada*, pages 7298–7309.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. (2020) Yi, Xiaoyuan, Zhenghao Liu, Wenhao Li, and Maosong Sun. 2020.
    Text style transfer via learning style instance supported latent space. In *Proceedings
    of the Twenty-Ninth International Joint Conference on Artificial Intelligence,
    IJCAI 2020*, pages 3801–3807, ijcai.org.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2021) Yuan, Siyang, Pengyu Cheng, Ruiyi Zhang, Weituo Hao, Zhe
    Gan, and Lawrence Carin. 2021. Improving zero-shot voice style transfer via disentangled
    representation learning. *CoRR*, abs/2103.09420.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng, Shoeybi, and Liu (2020) Zeng, Kuo-Hao, Mohammad Shoeybi, and Ming-Yu Liu.
    2020. Style example-guided text generation using generative adversarial transformers.
    *CoRR*, abs/2003.00674.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018a) Zhang, Jingyi, Masao Utiyama, Eiichiro Sumita, Graham
    Neubig, and Satoshi Nakamura. 2018a. Guiding neural machine translation with retrieved
    translation pieces. In *Proceedings of the 2018 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)*,
    pages 1325–1335, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018b) Zhang, Saizheng, Emily Dinan, Jack Urbanek, Arthur Szlam,
    Douwe Kiela, and Jason Weston. 2018b. Personalizing dialogue agents: I have a
    dog, do you have pets too? In *Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018,
    Volume 1: Long Papers*, pages 2204–2213, Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Zhang, Tianyi, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020. BERTScore: Evaluating text generation with BERT. In *8th
    International Conference on Learning Representations, ICLR 2020, Addis Ababa,
    Ethiopia, April 26-30, 2020*, OpenReview.net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Liu (2013) Zhang, Xu-Yao and Cheng-Lin Liu. 2013. Writer adaptation
    with style transfer mapping. *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, 35(7):1773–1787.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang, Ge, and Sun (2020) Zhang, Yi, Tao Ge, and Xu Sun. 2020. Parallel data
    augmentation for formality style transfer. In *Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics*, pages 3221–3228, Association
    for Computational Linguistics, Online.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018c) Zhang, Yi, Jingjing Xu, Pengcheng Yang, and Xu Sun. 2018c.
    Learning sentiment memories for sentiment modification without parallel data.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing, Brussels, Belgium, October 31 - November 4, 2018*, pages 1103–1108,
    Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018d) Zhang, Zhirui, Shuo Ren, Shujie Liu, Jianyong Wang, Peng
    Chen, Mu Li, Ming Zhou, and Enhong Chen. 2018d. Style transfer as unsupervised
    machine translation. *CoRR*, abs/1808.07894.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2018) Zhao, Junbo Jake, Yoon Kim, Kelly Zhang, Alexander M. Rush,
    and Yann LeCun. 2018. Adversarially regularized autoencoders. In *Proceedings
    of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan,
    Stockholm, Sweden, July 10-15, 2018*, volume 80 of *Proceedings of Machine Learning
    Research*, pages 5897–5906, PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2019) Zheng, Xu, Tejo Chalasani, Koustav Ghosal, Sebastian Lutz,
    and Aljosa Smolic. 2019. STaDA: Style transfer as data augmentation. In *Proceedings
    of the 14th International Joint Conference on Computer Vision, Imaging and Computer
    Graphics Theory and Applications, VISIGRAPP 2019, Volume 4: VISAPP, Prague, Czech
    Republic, February 25-27, 2019*, pages 107–114, SciTePress.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017) Zhu, Jun-Yan, Taesung Park, Phillip Isola, and Alexei A. Efros.
    2017. Unpaired image-to-image translation using cycle-consistent adversarial networks.
    In *IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy,
    October 22-29, 2017*, pages 2242–2251, IEEE Computer Society.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu, Bernhard, and Gurevych (2010) Zhu, Zhemin, Delphine Bernhard, and Iryna
    Gurevych. 2010. A monolingual tree-based translation model for sentence simplification.
    In *COLING 2010, 23rd International Conference on Computational Linguistics, Proceedings
    of the Conference, 23-27 August 2010, Beijing, China*, pages 1353–1361, Tsinghua
    University Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zue and Glass (2000) Zue, Victor W and James R Glass. 2000. Conversational
    interfaces: Advances and challenges. *Proceedings of the IEEE*, 88(8):1166–1180.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
