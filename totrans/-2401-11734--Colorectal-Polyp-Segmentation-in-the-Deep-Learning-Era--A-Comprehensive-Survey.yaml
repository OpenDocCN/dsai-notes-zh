- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:34:54'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2401.11734] Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.11734](https://ar5iv.labs.arxiv.org/html/2401.11734)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Colorectal Polyp Segmentation in the
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep Learning Era: A Comprehensive Survey'
  prefs: []
  type: TYPE_NORMAL
- en: Zhenyu Wu, Fengmao Lv, Chenglizhao Chen, Aimin Hao, Shuo Li Zhenyu Wu and Fengmao
    Lv are with the Southwest Jiaotong University. Chenglizhao Chen is with China
    University of Petroleum (Qingdao). Aimin Hao is with Beihang University Shuo Li
    is with Case Western Reserve University
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Colorectal polyp segmentation (CPS), an essential problem in medical image analysis,
    has garnered growing research attention. Recently, the deep learning-based model
    completely overwhelmed traditional methods in the field of CPS, and more and more
    deep CPS methods have emerged, bringing the CPS into the deep learning era. To
    help the researchers quickly grasp the main techniques, datasets, evaluation metrics,
    challenges, and trending of deep CPS, this paper presents a systematic and comprehensive
    review of deep-learning-based CPS methods from 2014 to 2023, a total of 115 technical
    papers. In particular, we first provide a comprehensive review of the current
    deep CPS with a novel taxonomy, including network architectures, level of supervision,
    and learning paradigm. More specifically, network architectures include eight
    subcategories, the level of supervision comprises six subcategories, and the learning
    paradigm encompasses 12 subcategories, totaling 26 subcategories. Then, we provided
    a comprehensive analysis the characteristics of each dataset, including the number
    of datasets, annotation types, image resolution, polyp size, contrast values,
    and polyp location. Following that, we summarized CPS’s commonly used evaluation
    metrics and conducted a detailed analysis of 40 deep SOTA models, including out-of-distribution
    generalization and attribute-based performance analysis. Finally, we discussed
    deep learning-based CPS methods’ main challenges and opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Colorectal Polyp Segmentation, Deep Learning, Colorectal Cancer.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Colorectal cancer (CRC) has become the second leading cause of cancer-related
    death worldwide, causing millions of incidence cases and deaths every year. Research
    shows that nearly 95% of colorectal cancers originate from polyps, undergoing
    a progression from normal mucosa to adenomatous polyps, then precancerous polyps,
    adenocarcinoma, and finally transforming into cancer [[1](#bib.bib1)], as shown
    in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey"). The survival rate of CRC patients
    exceeds 90% in the initial stage but drastically drops to less than 5% in the
    last stage [[2](#bib.bib2)]. Thus, the survival rate can be improved by early
    diagnosis and treatment of precancerous polyps via colonoscopy screening, such
    as colonoscopy and capsule endoscopy. Although colonoscopy provides information
    on the appearance and localization of polyps, it demands expensive labor resources
    and exhibits a high miss rate due to the following challenges: 1) Diagnostic accuracy
    heavily relies on the doctor’s experience.2) Polyps exhibit significant variations
    in color, size, and shape. 3) The boundaries between polyps and surrounding tissues
    are unclear. For instance, sessile polyps, also known as flat polyps, is no significant
    elevation compared to the surrounding normal tissue, making them inconspicuous
    and challenging to detect. Therefore, it is essential to develop an automated
    and accurate colorectal polyp segmentation (CPS) system, minimizing the occurrence
    of misdiagnoses to the greatest extent.'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional polyp segmentation methods primarily focus on learning low-level
    features, such as texture, shape, or color distribution, failing to deal with
    complex scenarios. With advanced deep learning, plenty of CNN/Transformer-based
    methods have been proposed for polyp segmentation. In recent years, UNet [[3](#bib.bib3)]
    based deep learning methods such as UNet++ [[4](#bib.bib4)], ResUNet++ [[5](#bib.bib5)],
    and PraNet [[6](#bib.bib6)] have dominated the field. Recently, transformer-based
    models [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)] have also been proposed
    for polyp segmentation and achieve state-of-the-art (SOTA) performance. Despite
    significant progress made by these deep learning models, it still lacks a comprehensive
    overview for the polyp segmentation task to date.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e5ecac1034023529afde2b716b2dd1db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Colorectal cancers originate from polyps, undergoing a progression
    from normal mucosa to adenomatous polyps, then pre-cancerous polyps, adenocarcinoma,
    and finally transforming into cancer. The survival rate of CRC patients exceeds
    90% in the first stage but drastically drops to less than 5% in the last stage.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/33e66fdf3b19afb13885b1e7bac20a4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A brief history of CPS. The first seminal work [[10](#bib.bib10)]
    on CPS can be traced back to 2003\. The first deep learning-based CPS method [[3](#bib.bib3)]
    emerged in 2015\. The methods mentioned above serve as milestones, which are typically
    highly cited. See Section [1.1.1](#S1.SS1.SSS1 "1.1.1 A brife history of polyp
    segmentation ‣ 1.1 History and Scope ‣ 1 Introduction ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey") for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: This paper presents a systematic and comprehensive review of deep-learning-based
    CPS methods. We first provide a comprehensive review of the current deep CPS with
    a novel taxonomy, including network architectures, level of supervision, and their
    learning paradigm. Then, we also provide a comprehensive analysis of the characteristics
    of each dataset, including the number of datasets, annotation types, image resolution,
    polyp size, contrast values, and polyp location. We conducted a comprehensive
    summary and analysis of 40 deep SOTA models, building an open and standardized
    evaluation benchmarking. Finally, we discussed the main challenges and opportunities
    of deep learning-based CPS methods. We hope that this survey can assist researchers
    in quickly grasping the development history of polyp segmentation and attracting
    more researchers to join the field.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 History and Scope
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1.1.1 A brife history of polyp segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey"), we briefly outline the history
    of colorectal polyp segmentation. To our best knowledge, the first work of CPS
    can be traced back to the seminal works of [[10](#bib.bib10)] published in 2003,
    which employs the canny edge detector to segment images of polyp candidates for
    computed tomography (CT) colonography. Subsequently, Yao et al. [[11](#bib.bib11)]
    proposed an automatic method to segment colonic polyps in CT colonography, which
    is based on a combination of knowledge-guided intensity adjustment and fuzzy c-mean
    clustering. Gross et al. [[12](#bib.bib12)] presented the first automatic polyp
    segmentation algorithm for colonoscopic narrow-band images. Hwang et al. [[13](#bib.bib13)]
    presented an unsupervised method for the detection of polyps in wireless capsule
    endoscopy videos, which adopts watershed segmentation with a novel initial marker
    selection method based on Gabor texture features and K-means clustering. These
    non-deep learning-based polyp segmentation methods [[14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16)] typically depend on manually extracted low-level features and
    traditional segmentation algorithms. However, manually extracting low-level features
    for polyp segmentation is inadequate for handling complex scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: With deep learning technology completely overwhelms traditional methods in the
    field of computer vision, more and more deep CPS methods [[17](#bib.bib17), [4](#bib.bib4),
    [2](#bib.bib2)] have emerged since 2015, bringing the polyp segmentation task
    into the deep learning era. Ronneberger et al. [[3](#bib.bib3)] proposed a UNet
    shape network for biomedical image segmentation and won the ISBI cell tracking
    challenge 2015 by a large margin. Later, Fang et al. [[18](#bib.bib18)] proposed
    a selective feature aggregation network with the area and boundary constraints
    for CPS. Fan et al. [[6](#bib.bib6)] propose a parallel reverse attention network
    for accurate polyp segmentation in colonoscopy images. Zhang et al. [[7](#bib.bib7)]
    propose a transformer-based parallel-in-branch architecture, which combines Transformers
    and CNNs in a parallel style. Recently, Ling et al. [[9](#bib.bib9)] proposed
    a Gaussian-Probabilistic guided semantic fusion method that progressively fuses
    the probability information of polyp positions with the decoder supervised by
    binary masks.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.2 Scope of this survey.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Clinical colorectal polyp screening can be divided at the task level into three
    classical tasks, i.e., polyp classification, polyp detection, and polyp segmentation.
    This work focuses on the polyp segmentation task and aims to systematically summarize
    deep polyp segmentation methods, commonly used polyp segmentation datasets, evaluation
    metrics, and model performance. Although the development of the deep polyp segmentation
    model is only eight years, it has spawned hundreds of papers, making it impractical
    to review all of them. In this survey, we mainly focus on these influential papers
    published in mainstream journals and conferences of medical image analysis from
    2019 to 2023\. Besides, for completeness and better readability, some early relevant
    works ranging from 2014 to 2018 have also been mentioned briefly. Due to space
    limitations and the extent of our knowledge, we apologize to those authors whose
    works could not be included in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Related Surveys
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [I](#S1.T1 "TABLE I ‣ 1.2 Related Surveys ‣ 1 Introduction ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey") lists existing
    surveys related to this work. Among them, Prasath et al. [[19](#bib.bib19)] reviewed
    “capsule endoscopy” colorectal polyp detection and segmentation methods based
    on handcrafted features and discussed the existing challenges. Later, Taha et
    al. [[20](#bib.bib20)] reviewed endoscopy colorectal polyp detection models, which
    divide these models into shape, texture, and fusion features. Sanchez-Peralta
    et al. [[21](#bib.bib21)] mainly focuses on the deep learning-based colorectal
    polyp detection, localization, and segmentation approaches. Besides, they also
    summarize six widely used polyp detection datasets for training or benchmarking
    and 19 commonly used metrics for model evaluation. Finally, a more recently published
    survey [[22](#bib.bib22)] summarized the commonly adopted network architectures,
    introduced the benchmark datasets and evaluation metrics, and discussed the current
    challenges. Although existing reviews provide insights from different perspectives,
    they also have some shortcomings: 1) Insufficient in the number of literature.
    In the above-mentioned surveys, the most comprehensive survey [[19](#bib.bib19)]
    contains only 37 papers. 2) Limited in time span. For example, surveys [[19](#bib.bib19)]
    and [[20](#bib.bib20)] focuse on polyp segmentation methods before 2016 while
    [[21](#bib.bib21)] review polyp segmentation methods from 2015 to 2018\. [[22](#bib.bib22)]
    reviews 20 papers, but 18 out of 20 were published in 2022\. 3) Lacking comprehensive
    evaluation and analysis for existing models and datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Title | Year | Description |'
  prefs: []
  type: TYPE_TB
- en: '| Polyp detection and segmentation from video capsule endoscopy: A review [[19](#bib.bib19)]
    | 2016 | This paper reviews traditional based capsule endoscopy polyps detection
    and segmentation methods before 2016 with only 37 papers. |'
  prefs: []
  type: TYPE_TB
- en: '| Automatic polyp detection in endoscopy videos: A survey [[20](#bib.bib20)]
    | 2017 | This paper reviews handcrafted features based polyps detection methods
    before 2016 with only 28 papers. |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning to find colorectal polyps in colonoscopy:A systematic literature
    review [[21](#bib.bib21)] | 2020 | This paper reviews deep learning-based polyps
    detection, localization, and segmentation methods from 2015 to 2018 with only
    35 papers. |'
  prefs: []
  type: TYPE_TB
- en: '| Detection of colorectal polyps from colonoscopy using machine learning: A
    survey on modern techniques [[22](#bib.bib22)] | 2023 | This paper reviews deep
    learning-based polyps detection with only 20 papers, of which 18 out of 20 were
    published in 2022. |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE I: Summary of previous surveys on CPS. For each survey, the title, publication
    year, and a summary of the content are provided. More discussion can be found
    in Section [1.2](#S1.SS2 "1.2 Related Surveys ‣ 1 Introduction ‣ Colorectal Polyp
    Segmentation in the Deep Learning Era: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Mei et al. [[23](#bib.bib23)] also conducted a polyp segmentation
    survey and released their paper on the Arxiv platform. Compared to this contemporary
    work and previous surveys, our work has several advantages: 1) A well-organized
    taxonomy of deep CPS models. As shown in Fig. [3](#S1.F3 "Figure 3 ‣ 1.3 Our Contributions
    ‣ 1 Introduction ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey"), our survey categorized polyp segmentation methods into three main groups:
    network architectures, level of supervision, and learning paradigm. More specifically,
    network architectures include eight subcategories, the level of supervision comprises
    six subcategories, and the learning paradigm encompasses 12 subcategories, totaling
    26 subcategories. In contrast, Mei’s survey divided existing polyp segmentation
    methods into six categories. 2) A systematic study of current deep CPS methods.
    As shown in Table [II](#S2.T2 "TABLE II ‣ 2.1.2 CNN-based Method ‣ 2.1 Network
    Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the
    Deep Learning Era: A Comprehensive Survey") and Table [III](#S2.T3 "TABLE III
    ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey)
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey"),
    our survey contains 115 papers from 2014 to 2023, while Mei’s survey only covered
    45 papers from 2019 to 2023\. The number of papers in our survey is nearly three
    times that of theirs. Besides we also provided a summary of the architecture,
    key technologies, and training datasets utilized by the models proposed in each
    paper, aiming to enhance readers’ understanding. 3) A comprehensive analysis of
    current CPS datasets. As shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1 Image-level
    Datasets ‣ 3 Polyp Segmentation Datasets ‣ Colorectal Polyp Segmentation in the
    Deep Learning Era: A Comprehensive Survey"), Fig. [5](#S3.F5 "Figure 5 ‣ 3.1 Image-level
    Datasets ‣ 3 Polyp Segmentation Datasets ‣ Colorectal Polyp Segmentation in the
    Deep Learning Era: A Comprehensive Survey") and Table [IV](#S2.T4 "TABLE IV ‣
    2.3.4 Domain Adapation ‣ 2.3 Learning Paradigm ‣ 2 Methodology (Survey) ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey"), we provide
    a comprehensive analysis of the characteristics of each dataset, including the
    number of datasets, annotations types, image resolution, polyp size, contrast
    values, number of polyps and polyp location. 4) An extensive performance comparison
    and analysis of deep CPS approaches. As shown in Table [V](#S4.T5 "TABLE V ‣ 4
    Evaluation Metrics ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A
    Comprehensive Survey"), our survey comprehensively summarized and analyzed 40
    models from 2015 to 2023, while Mei’s survey included only 21 models. In addition,
    as shown in Table [VI](#S4.T6 "TABLE VI ‣ 4 Evaluation Metrics ‣ Colorectal Polyp
    Segmentation in the Deep Learning Era: A Comprehensive Survey") and Table [VIII](#S5.T8
    "TABLE VIII ‣ 5.3 Attribute-based Performance Analysis ‣ 5 Performance Benchmarking
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey"),
    We also assessed the model’s generalization performance on the out-of-distribution
    dataset (i.e., PolypGen [[24](#bib.bib24)]) and its attribute-based performance
    on SUN-SEG[[2](#bib.bib2)] datasets for better uncovering the nuanced strengths
    and weaknesses of deep CPS models. 5) A deeper look into the challenges of CPS
    and providing some promising directions. We extensively summarized the challenges
    encountered in polyp segmentation, including the deep model’s interpretability,
    generalization ability, robustness to adversarial attacks, data privacy, domain
    shift, etc. We also highlighted several promising directions for CPS, such as
    combining CPS with unsupervised anomaly localization, large visual models and
    large language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Our Contributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In summary, this survey systematically studies deep polyp segmentation methods,
    datasets, and metrics from 2014 to 2023, including 115 technical papers, 14 polyp
    segmentation datasets, and 12 commonly used evaluation metrics. We aim to provide
    an intuitive understanding and high-level insights into polyp segmentation so
    that researchers can choose the proper directions to explore. The contributions
    of this work are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A systematic study of deep CPS methods from 2014 to 2023, a total of 115 technical
    papers. We propose a novel taxonomy of polyp segmentation approaches and categorize
    these approaches into three main categories: network architectures, level of supervision,
    and learning paradigm, which can be further divided into 26 subcategories. The
    introduced taxonomy was designed to help researchers gain a deeper insight into
    the essential characteristics of deep CPS models.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comprehensive analysis on current CPS datasets. We thoroughly analyze each
    CPS dataset, including the number of datasets, annotation types, image resolution,
    polyp size, contrast values, number of polyps, and polyp location. Furthermore,
    we analyzed the challenges and difficulties in the existing CPS dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An extensive performance comparison and analysis of deep CPS models. We conducted
    a comprehensive summary and analysis of 40 models from 2015 to 2023 for building
    an open and standardized evaluation benchmarking. Additionally, we evaluated the
    model’s generalization performance on the multi-center dataset (PolypGen) and
    its attribute-based performance on SUN-SEG datasets to better understand the strengths
    and weaknesses of deep CPS models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An in-depth delving into the challenges of deep CPS models and providing promising
    directions. We summarized the challenges of current deep CPS models, including
    deep model’s interpretability, generalization ability, robustness to adversarial
    attacks, data privacy, domain shift, etc. We also present several promising directions
    for CPS, such as combining CPS with unsupervised anomaly localization, large visual
    models, and large language models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of this survey is organized as follows: Section [2](#S2 "2 Methodology
    (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey") presents a novel taxonomy and reviews some representative state-of-the-art
    deep learning-based polyp segmentation models for each category. Section [3](#S3
    "3 Polyp Segmentation Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey") overviews some of the most popular polyp segmentation
    datasets and their characteristics. Section [4](#S4 "4 Evaluation Metrics ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey") lists popular
    metrics for evaluating deep CPS models. Section [5](#S5 "5 Performance Benchmarking
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")
    benchmarks 40 deep CPS models and provides in-depth analyses. Section [6](#S6
    "6 Challenges and Future Directions ‣ Colorectal Polyp Segmentation in the Deep
    Learning Era: A Comprehensive Survey") discusses the main challenges and opportunities
    of deep learning-based CPS methods. Section [7](#S7 "7 Conlusion ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey") is the conclusion.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fe9042f1e5b4af05a9b176f9cf0fc427.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An overview of our proposed taxonomy. We categorize existing polyp
    segmentation approaches into three branches from different perspectives: network
    architectures, level of supervision, and learning paradigm. Specifically, network
    architectures include eight subcategories, the level of supervision comprises
    six subcategories, and the learning paradigm encompasses 12 subcategories, totaling
    26 subcategories.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Methodology (Survey)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section provides a comprehensive overview of prominent deep CPS methods
    from novel taxonomy, including network architectures (Sec. [2.1](#S2.SS1 "2.1
    Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")), level of supervision (Sec.
    [2.2](#S2.SS2 "2.2 Level of Supervision ‣ 2 Methodology (Survey) ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey"))and learning
    paradigm (Sec. [2.3](#S2.SS3 "2.3 Learning Paradigm ‣ 2 Methodology (Survey) ‣
    Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")),
    as shown in Fig. [3](#S1.F3 "Figure 3 ‣ 1.3 Our Contributions ‣ 1 Introduction
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey").
    Table [II](#S2.T2 "TABLE II ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures
    ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey") and Table [III](#S2.T3 "TABLE III ‣ 2.1.2 CNN-based
    Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp
    Segmentation in the Deep Learning Era: A Comprehensive Survey") summarizes the
    recent proposed deep CPS models and some representative traditional CPS methods.
    Due to limited space, we only selected a few representative methods for each category.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Network Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the adopted backbone architectures, we further classify these deep
    CPS models into four categories: multilayer perceptron (MLP) based models (Sec.
    [2.1.1](#S2.SS1.SSS1 "2.1.1 MLP-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology
    (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey")), convolutional neural network (CNN) based models (Sec. [2.1.2](#S2.SS1.SSS2
    "2.1.2 CNN-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣
    Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")),
    transformer-based models (Sec. [2.1.3](#S2.SS1.SSS3 "2.1.3 Transformer-based Method
    ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")), and hybrid-network based
    models (Sec. [2.1.4](#S2.SS1.SSS4 "2.1.4 Hybrid-network Based Method ‣ 2.1 Network
    Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the
    Deep Learning Era: A Comprehensive Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 MLP-based Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.2 CNN-based Method ‣ 2.1 Network
    Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the
    Deep Learning Era: A Comprehensive Survey")(a), MLP is a type of artificial neural
    network architecture composed of multiple layers of nodes, including an input
    layer, one or more hidden layers, and an output layer. Shi et al. [[25](#bib.bib25)]
    proposed a novel work to investigate the MLP-based architecture in polyp segmentation,
    which uses CycleMLP [[26](#bib.bib26)] as the encoder to overcome the fixed input
    scale issue. Yuan et al. [[17](#bib.bib17)] introduce an MLP-based sparse autoencoder
    to learn high-level superpixel characterization from the hand-crafted features.
    Although MLP-based CPS methods exhibit superior performance compared to non-deep
    learning methods, MLP-based models are limited in leveraging the image’s spatial
    information. Moreover, these methods are time-consuming, as they require processing
    in a multi-stage manner.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 CNN-based Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To overcome the limitations of MLP-based methods, recent CPS approaches [[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)] adopted the CNN architecture,
    which enables polyp representation and segmentation in a end-to-end manner. The
    CNN-based methods have become dominant in the CPS field and can be further divided
    into FCN-based networks, UNet-based networks, multi-branch networks, and deep
    supervision networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'FCN-based Network. Long et al. [[36](#bib.bib36)] introduced Fully Convolutional
    Networks (FCN) for semantic segmentation, As shown in Fig. [4](#S2.F4 "Figure
    4 ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey)
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")(b).
    Since then, FCN has become the foundational framework for image segmentation,
    and subsequent CPS methods [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41)] largely evolving from it. Yin et al. [[42](#bib.bib42)]
    proposed two parallel attention-based modules, which can be incorporated into
    any encoder-decoder architecture. Xu et al. [[43](#bib.bib43)] proposed a temporal
    correlation network for video polyp segmentation, in which the temporal correlation
    is unprecedentedly modeled based on the relationship between the original video
    and the captured frames to be adaptable for video polyp segmentation. Feng et
    al. [[44](#bib.bib44)] proposed a novel stair-shape network for real-time polyp
    segmentation in colonoscopy images. Akbari et al. [[45](#bib.bib45)] proposed
    a polyp segmentation method based on fully the convolutional neural network. Wichakam
    et al. [[46](#bib.bib46)] proposed a compressed fully convolutional network by
    modifying the FCN-8s network to detect and segment polyp in real-time. Wu et al.
    [[47](#bib.bib47)] proposed a novel ConvNet to accurately segment polyps from
    colonoscopy videos in a bottom-up/top-down manner. Dong et al. [[48](#bib.bib48)]
    present asymmetric attention upsampling, which utilizes the information of low-level
    feature maps to rescale the high-level feature maps smartly through spatial pooling
    and attention mechanisms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'UNet-based Network. Ronneberger et al. [[3](#bib.bib3)] proposed the UNet architecture
    designed for biomedical image segmentation tasks. As shown in Fig. [4](#S2.F4
    "Figure 4 ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology
    (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey")(c), it comprises a contracting path to capture context and a symmetric
    expanding path, making it suitable for CPS tasks. For instance, Zhang et al. [[49](#bib.bib49)]
    designed a typical UNet structure, followed by a bottom-up feature extraction
    and top-down feature fusion strategy to obtain more comprehensive and semantically
    rich feature representations. Zhao et al. [[50](#bib.bib50)] a ResUnet based framework
    to fuse the proximity frame information at different layers and capture contextual
    information. Srivastava et al. [[51](#bib.bib51)] propose a novel framework for
    medical image segmentation that consists of an encoder block, a shape stream block,
    and a decoder block. Lin et al. [[52](#bib.bib52)] proposed a UNet-based network
    to effectively suppress noises in feature maps and simultaneously improve the
    ability of feature expression at different levels. Song et al. [[53](#bib.bib53)]
    designed a new neural network structure based on the currently popular encoding-decoding
    network architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1726a84b56203874e6b8e39bf8923b78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Classification of deep CPS models according to the network architectures.
    (a) MLP-based methods, (b) FCN-based models, (c) UNet-based approaches, (d) Deep
    supervision network, (e) Transformer-based network, and (f) Multi-branch networks,
    and (g) Hybrid-network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-branch Network. As shown in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.2 CNN-based
    Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp
    Segmentation in the Deep Learning Era: A Comprehensive Survey")(f), multi-branch-based
    methods typically consist of multiple encoders/decoders to extract multi-scale
    features for polyp segmentation explicitly. Li et al. [[54](#bib.bib54)] proposed
    a multi-branch framework, consisting of a segmentation branch and a propagation
    branch, respectively. Huy et al. [[55](#bib.bib55)] proposed an adversarial contrastive
    Fourier method, learning contrastive loss with supervision from the content encoder
    and the style encoder. Ji et al. [[2](#bib.bib2)] design a simple but efficient
    multistream network (named PNS+) for video polyp segmentation, which consists
    of a global encoder and a local encoder. The global and local encoders extract
    long- and short-term spatial-temporal representations from the first anchor frame
    and multiple successive frames. Qiu et al. [[56](#bib.bib56)] design a two-branch
    network; one branch is used to generate a boundary distribution map, and another
    is used to predict the polyp segmentation maps. Chen et al. [[57](#bib.bib57)]
    proposed a novel two-branched network for polyp segmentation of single-modality
    endoscopic images, which consists of an image-to-image translation branch and
    an image segmentation branch. Tomar et al. [[58](#bib.bib58)] propose a novel
    deep learning architecture called dual decoder attention network for automatic
    polyp segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Supervision Network. As shown in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.2 CNN-based
    Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp
    Segmentation in the Deep Learning Era: A Comprehensive Survey")(d), the deep supervision
    learning strategy [[59](#bib.bib59), [4](#bib.bib4), [60](#bib.bib60), [61](#bib.bib61)]
    is proposed to utilize the groundtruth to supervise at each layer of the network,
    facilitating rapid convergence of the model. Patel et al. [[62](#bib.bib62)] proposed
    a novel fuzzy network to focus more on the difficult pixels, where deep supervision
    is applied at the end of the output of each fuzzy attention module and partial
    decoder. Du et al. [[63](#bib.bib63)] proposed an integration context-based reverse-contour
    guidance network with multi-level deep supervision. Nguyen et al. [[64](#bib.bib64)]
    proposed an encoder-decoder-based architecture and each layer was supervised by
    the resized ground truth. Shen et al. [[65](#bib.bib65)] proposed a novel hard
    region enhancement network for polyp segmentation supervised multi-level groundtruth.
    [[7](#bib.bib7)] uses deep supervision to improve the gradient flow by additionally
    supervising the transformer branch and the first fusion branch. Kim et al. [[66](#bib.bib66)]
    propose uncertainty augmented context attention network using extra supervision
    in each layer. Zhang et al. [[67](#bib.bib67)] propose the adaptive context selection
    network supervised by the down-sampled groundtruth. Fan et al. [[6](#bib.bib6)]
    propose a parallel reverse attention network for the polyp segmentation task,
    adopting deep supervision for the last three side outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Year | Methods | Publication | Architecture | Backbone | Supervision |
    Learning Paradigm | Training Dataset | Number | Available |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | PETNet [[9](#bib.bib9)] | MICCAI | Transformer | PVTv2-B2 [[68](#bib.bib68)]
    | Fully-Sup. | Gaussian-Probabilistic, Ensemble | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| RPFA [[71](#bib.bib71)] | MICCAI | Transformer+UNet | PVT [[72](#bib.bib72)]
    | Fully-Sup. | Feature Propagation/Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| S2ME [[73](#bib.bib73)] | MICCAI | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | Scribble-Sup. | Ensemble Learning, Mutual Teaching | SUN-SEG[[2](#bib.bib2)]
    | 6677 | [Open](https://github.com/lofrienger/S2ME) |'
  prefs: []
  type: TYPE_TB
- en: '| WeakPolpy [[75](#bib.bib75)] | MICCAI | Transformer/FCN | Res2Net50[[76](#bib.bib76)]/PVTv2-B2
    [[68](#bib.bib68)] | Box-Sup. | Transformation, Scale Consistency | SUN-SEG[[2](#bib.bib2)]/POLYP-SEG[[75](#bib.bib75)]
    | 19,544/15,916 | [Open](https://github.com/weijun88/WeakPolyp) |'
  prefs: []
  type: TYPE_TB
- en: '| FSFM [[41](#bib.bib41)] | ISBI | CNN+FPN | VAN [[77](#bib.bib77)] | Fully-Sup.
    | Feature Fusion, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| EPSG [[78](#bib.bib78)] | ISBI | CNN+UNet | ResNet50 [[74](#bib.bib74)] |
    Fully-Sup. | Image Transformation, Variation | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 490+800 | No |'
  prefs: []
  type: TYPE_TB
- en: '| RealSeg [[79](#bib.bib79)] | ISBI | FPN | VAN [[77](#bib.bib77)] | Fully-Sup.
    | Real-time, Feature Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| PSTNet [[80](#bib.bib80)] | ISBI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Domain Adaptation, Translation | PICCOLO [[81](#bib.bib81)] | 3,546
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| EMTSNet [[82](#bib.bib82)] | JBHI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Multi-Task, Activation Map | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| GAN-PSNet [[83](#bib.bib83)] | JBHI | GAN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Attention, Generative Adversarial | Kvasir-SEG [[70](#bib.bib70)]
    | 800 | No |'
  prefs: []
  type: TYPE_TB
- en: '| FEGNet [[60](#bib.bib60)] | JBHI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Feedback Mechanism, Boundary | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+800 | No |'
  prefs: []
  type: TYPE_TB
- en: '| PMSACL [[84](#bib.bib84)] | MIA | CNN+UNet | ResNet18[[74](#bib.bib74)] |
    Self/Un-Sup. | Data augmentation, Anomaly | HyperKvasir [[85](#bib.bib85)] | 2100
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| XBFormer [[86](#bib.bib86)] | TMI | Transformer | PVT [[72](#bib.bib72)]
    | Fully-Sup. | Cross-scale, Boundary Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+800 | [Open](https://github.com/jcwang123/xboundformer)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ColnNet [[87](#bib.bib87)] | TMI | CNN+UNet | DenseNet121 [[88](#bib.bib88)]
    | Fully-Sup. | Relationship of Feature, Boundary | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+800 | No |'
  prefs: []
  type: TYPE_TB
- en: '| RPANet [[89](#bib.bib89)] | IPMI | CNN+UNet | ResNet101 [[74](#bib.bib74)]
    | Fully-Sup. | Domain Adaptation, Self-Supervision | Private Dataset [[89](#bib.bib89)]
    | 5,175 | No |'
  prefs: []
  type: TYPE_TB
- en: '| TransNetR [[90](#bib.bib90)] | MIDL | Transformer+CNN | ResNet50 [[74](#bib.bib74)]
    | Fully-Sup. | Out-of-distribution, Generalization | Kvasir-SEG [[70](#bib.bib70)]
    | 900 | [Open](https://github.com/DebeshJha/TransNetR) |'
  prefs: []
  type: TYPE_TB
- en: '| CFANet [[91](#bib.bib91)] | PR | CNN+UNet | Res2Net50[[76](#bib.bib76)] |
    Fully-Sup. | Boundary-aware, Feature Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | [Open](https://github.com/taozh2017/CFANet) |'
  prefs: []
  type: TYPE_TB
- en: '| GSAL [[92](#bib.bib92)] | PR | CNN+GAN | VGG16 [[93](#bib.bib93)] | Fully-Sup.
    | Adversarial Learning | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | [Open](https://github.com/DLWK/GSAL) |'
  prefs: []
  type: TYPE_TB
- en: '| Polyp-Mixer [[25](#bib.bib25)] | TCSVT | MLP | CycleMLP-B1 [[26](#bib.bib26)]
    | Fully-Sup. | Context-Aware, MLP | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| PTMFNet [[94](#bib.bib94)] | ICIP | Transformer | PVTv2 [[68](#bib.bib68)]
    | Fully-Sup. | Pyramid Transformer, Multibranch | - | - | No |'
  prefs: []
  type: TYPE_TB
- en: '| PLCUT-Seg [[95](#bib.bib95)] | IJCNN | CNN | HarDNet68 [[96](#bib.bib96)]
    | Self/Semi-Sup. | Synthetic Data, Self/Semi-Supervised | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| PolypSeg+ [[97](#bib.bib97)] | TCYB | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | Fully-Sup. | Context-aware, Feature Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '|  | CASCADE [[98](#bib.bib98)] | WACV | Transformer+UNet | PVTv2 [[68](#bib.bib68)]
    | Fully-Sup. | Cascaded Attention, Feature Fusion | [Synapse](https://www.synapse.org/#!Synapse:syn3193805/wiki/217789)+ACDC[[99](#bib.bib99)]
    + ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 2212+1930 +548+900
    | [Open](https://github.com/SLDGroup/CASCADE) |'
  prefs: []
  type: TYPE_TB
- en: '|  | PolypPVT [[100](#bib.bib100)] | CAAI AIR | Transformer+UNet | PVT[[72](#bib.bib72)]
    | Fully-Sup. | Cascaded Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [Open](https://github.com/DengPingFan/Polyp-PVT) |'
  prefs: []
  type: TYPE_TB
- en: '|  | PEFNet [[101](#bib.bib101)] | MMM | CNN+UNet | EfficientNetV2-L [[102](#bib.bib102)]
    | Fully-Sup. | Positional Embedding | Kvasir-SEG [[70](#bib.bib70)] | 600 | [Open](https://github.com/huyquoctrinh/PEFNet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | TRFR-Net [[27](#bib.bib27)] | MICCAI | CNN+GAN | ResNet34 [[74](#bib.bib74)]
    | Fully-Sup. | Domain Adaptation, Adversarial Learning | Kvasir-SEG [[70](#bib.bib70)]+ETIS-Larib
    [[103](#bib.bib103)] + CVC-ColonDB [[104](#bib.bib104)] | 700+137+210 | No |'
  prefs: []
  type: TYPE_TB
- en: '| TGANet [[28](#bib.bib28)] | MICCAI | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | Fully-Sup. | Attention, Multi-scale | Kvasir-SEG [[70](#bib.bib70)] | 880 |
    [Open](https://github.com/nikhilroxtomar/tganet) |'
  prefs: []
  type: TYPE_TB
- en: '| LDNet [[49](#bib.bib49)] | MICCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Dynamci Kernel, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+800 | [Open](https://github.com/ReaFly/LDNet) |'
  prefs: []
  type: TYPE_TB
- en: '| SSFormer [[8](#bib.bib8)] | MICCAI | Transformer+UNet | PVTv2 [[68](#bib.bib68)]
    | Fully-Sup. | Local,Global Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | [Open](https://github.com/Qiming-Huang/ssformer) |'
  prefs: []
  type: TYPE_TB
- en: '| BoxPolyp [[29](#bib.bib29)] | MICCAI | Transformer/CNN | PVTv2 [[68](#bib.bib68)]/Res2Net50[[76](#bib.bib76)]
    | Box-Sup. | Fusion, Consistency Loss | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | [Open](https://github.com/weijun88/BoxPolyp) |'
  prefs: []
  type: TYPE_TB
- en: '| PPFormer [[105](#bib.bib105)] | MICCAI | Transformer+CNN | CvT[[106](#bib.bib106)]+VGG16[[93](#bib.bib93)]
    | Fully-Sup. | Self-attention, Local2Global Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| SSTAN [[50](#bib.bib50)] | MICCAI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | Semi-Sup. | Context Attention | LDPolypVideo [[107](#bib.bib107)] | 693+14704
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| NIP [[30](#bib.bib30)] | MedIA | CNN+UNet | ResNet101[[74](#bib.bib74)] |
    Fully-Sup. | Data Augmentation, Resampling | EndoScene [[108](#bib.bib108)] |
    547 | No |'
  prefs: []
  type: TYPE_TB
- en: '| FSM [[31](#bib.bib31)] | MedIA | CNN+UNet | ResNet101[[74](#bib.bib74)] |
    Fully-Sup. | Domain Adaptation, Resampling | EndoScene [[108](#bib.bib108)] |
    547 | [Open](https://github.com/CityU-AIM-Group/SFDA-FSM) |'
  prefs: []
  type: TYPE_TB
- en: '| MSRF-Net [[51](#bib.bib51)] | JBHI | CNN+UNet | SENet [[109](#bib.bib109)]
    | Fully-Sup. | Multi-scale Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 490+800 | [Open](https://github.com/NoviceMAn-prog/MSRF-Net) |'
  prefs: []
  type: TYPE_TB
- en: '| BCNet [[32](#bib.bib32)] | JBHI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Cross-Feature Fusion, Boundary | Kvasir-SEG [[70](#bib.bib70)]
    | 800 | [Open](https://github.com/NoviceMAn-prog/MSRF-Net) |'
  prefs: []
  type: TYPE_TB
- en: '| PNS+ [[2](#bib.bib2)] | MIR | CNN+UNet | Res2Net50[[76](#bib.bib76)] | Fully-Sup.
    | Feature Aggregation, Attention | SUN-SEG [[2](#bib.bib2)] | 19,544 | [Open](https://github.com/GewelsJI/VPS)
    |'
  prefs: []
  type: TYPE_TB
- en: '| FCBFormer [[110](#bib.bib110)] | MIUA | Transformer+CNN | PVTv2-B3 [[68](#bib.bib68)]+ResNet34[[74](#bib.bib74)]
    | Fully-Sup. | Two-stream Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 490+800 | [Open](https://github.com/ESandML/FCBFormer) |'
  prefs: []
  type: TYPE_TB
- en: '| BDGNet [[56](#bib.bib56)] | SPIE MI | CNN+UNet | EfficientNet-B5 [[111](#bib.bib111)]
    | Fully-Sup. | Multi-Scale Fusion, Boundary | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/zihuanqiu/BDG-Net) |'
  prefs: []
  type: TYPE_TB
- en: '| TCNet [[43](#bib.bib43)] | BIBM | FCN | Res2Net50[[76](#bib.bib76)] | Fully-Sup.
    | Temporal Correlation Modeling | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+800 | No |'
  prefs: []
  type: TYPE_TB
- en: '| TransMixer [[61](#bib.bib61)] | BIBM | Transformer+CNN | PVTv2 [[68](#bib.bib68)]+SENet
    [[109](#bib.bib109)] | Fully-Sup. | Interaction Fusion, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| ICBNet [[112](#bib.bib112)] | BIBM | Transformer+UNet | PVT [[72](#bib.bib72)]
    | Fully-Sup. | Iterative Fusion, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| CLDNet [[113](#bib.bib113)] | BIBM | Transformer+UNet | PVTv2 [[68](#bib.bib68)]
    | Fully-Sup. | Multi-Scale Fusion, Boundary | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 490+800 | No |'
  prefs: []
  type: TYPE_TB
- en: '| TASNet [[57](#bib.bib57)] | BIBM | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Color Reversal, Two-Branched Network | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| FuzzyNet [[62](#bib.bib62)] | NeurIPS | CNN/Transformer | Res2Net50[[76](#bib.bib76)]/PVT
    [[72](#bib.bib72)] | Fully-Sup. | Fuzzy Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/krushi1992/FuzzyNet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SwinPA-Net [[114](#bib.bib114)] | TNNLS | Transformer+UNet | Swin-B [[115](#bib.bib115)]
    | Fully-Sup. | Pyramid Attention, Multiscale Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| BSCA-Net [[52](#bib.bib52)] | PR | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Multipath, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| HSNet [[116](#bib.bib116)] | CBM | CNN+Transformer | PVTv2 [[68](#bib.bib68)]+
    Res2Net50[[76](#bib.bib76)] | Fully-Sup. | Cross Attention, Semantic Complementary
    | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 548+900 | [Open](https://github.com/baiboat/HSNet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MSRAformer [[59](#bib.bib59)] | CBM | Transformer+UNet | Swin-B [[115](#bib.bib115)]
    | Fully-Sup. | Channel/Reverse Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 490+800 | [Open](https://github.com/ChengLong1222/MSRAformer-main)
    |'
  prefs: []
  type: TYPE_TB
- en: '| AMNet [[53](#bib.bib53)] | CBM | CNN+UNet | Res2Net50[[76](#bib.bib76)] |
    Fully-Sup. | Multiscale Fusion, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 500+800 | No |'
  prefs: []
  type: TYPE_TB
- en: '| DBMF [[117](#bib.bib117)] | CBM | Transformer+CNN | Swin-B [[115](#bib.bib115)]+EfficientNet[[111](#bib.bib111)]
    | Fully-Sup. | Multiscale Fusion, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| ICGNet [[63](#bib.bib63)] | IJCAI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | Fully-Sup. | Local-Global Fusion, Boundary Supervision | EndoScene [[108](#bib.bib108)]+Kvasir-SEG
    [[70](#bib.bib70)] | 547+600 | No |'
  prefs: []
  type: TYPE_TB
- en: '| TCCNet [[54](#bib.bib54)] | IJCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Semi-Sup. | Context-Free Loss, Reverse Attention | ClinicDB [[69](#bib.bib69)]+CVC-ColonDB
    [[104](#bib.bib104)] | 367+180 | [Open](https://github.com/wener-yung/TCCNet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CoFo [[55](#bib.bib55)] | ISBI | CNN+UNet | ResNet18[[74](#bib.bib74)] |
    Fully-Sup. | Domain Adapation, Adversarial Learning | EndoScene [[108](#bib.bib108)]+Kvasir-SEG
    [[70](#bib.bib70)] | 547+600 | [Open](https://github.com/tadeephuy/CoFo) |'
  prefs: []
  type: TYPE_TB
- en: '| DCRNet [[42](#bib.bib42)] | ISBI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | Fully-Sup. | Contextual-Relation Learning | EndoScene [[108](#bib.bib108)]+Kvasir-SEG
    [[70](#bib.bib70)] + Piccolo[[118](#bib.bib118)] | 547+600 +2203 | [Open](https://github.com/tadeephuy/CoFo)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | MSNet [[33](#bib.bib33)] | MICCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Multiscale Fusion, Loss Network | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/Xiaoqi-Zhao-DLUT/MSNet-M2SNet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CCBANet [[64](#bib.bib64)] | MICCAI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | Fully-Sup. | Cascading Context, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/ntcongvn/CCBANet) |'
  prefs: []
  type: TYPE_TB
- en: '| HRNet [[65](#bib.bib65)] | MICCAI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | Fully-Sup. | Feature Aggregation, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| LODNet [[119](#bib.bib119)] | MICCAI | CNN | ResNet50[[74](#bib.bib74)] |
    Fully-Sup. | Oriented, Sensitive Loss | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/midsdsy/LOD-Net) |'
  prefs: []
  type: TYPE_TB
- en: '| SANet [[34](#bib.bib34)] | MICCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Color Exchange, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/weijun88/SANet) |'
  prefs: []
  type: TYPE_TB
- en: '| Transfuse [[7](#bib.bib7)] | MICCAI | Transformer+CNN | Res2Net34[[76](#bib.bib76)]+DeiT-S[[120](#bib.bib120)]
    | Fully-Sup. | Feature Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [Open](https://github.com/Rayicer/TransFuse) |'
  prefs: []
  type: TYPE_TB
- en: '| CCD [[121](#bib.bib121)] | MICCAI | CNN+UNet | ResNet18[[74](#bib.bib74)]
    | Self/Un-Sup. | Contrastive Learning, Anomaly Localisation | HyperKvasir [[85](#bib.bib85)]
    | 2100 | [Open](https://github.com/tianyu0207/CCD) |'
  prefs: []
  type: TYPE_TB
- en: '| Polyformer [[122](#bib.bib122)] | MICCAI | Transformer+UNet | Segtran[[123](#bib.bib123)]
    | Fully-Sup. | Few-Shot Learning, Domain Adaptation | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/askerlee/segtran) |'
  prefs: []
  type: TYPE_TB
- en: '| PNet [[124](#bib.bib124)] | MICCAI | CNN | Res2Net50[[76](#bib.bib76)] |
    Fully-Sup. | Feature Aggregation, Graph Convolution | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/DengPingFan/PraNet) |'
  prefs: []
  type: TYPE_TB
- en: '| HieraSeg [[125](#bib.bib125)] | MIA | CNN+FCN | Deeplabv3+[[126](#bib.bib126)]
    | Fully-Sup. | Hierarchical, Dynamic Weighting | EndoScene[[108](#bib.bib108)]
    | 547 | [Open](https://github.com/CityU-AIM-Group/DW-HieraSeg) |'
  prefs: []
  type: TYPE_TB
- en: '| ThresholdNet [[127](#bib.bib127)] | TMI | CNN+FCN | Deeplabv3+[[126](#bib.bib126)]
    | Fully-Sup. | Mixup, Threshold Loss | EndoScene[[108](#bib.bib108)] | 547 | [Open](https://github.com/Guo-Xiaoqing/ThresholdNetv)
    |'
  prefs: []
  type: TYPE_TB
- en: '| FANet [[35](#bib.bib35)] | TNNLS | CNN+UNet | N/A | Fully-Sup. | Iterative
    Refining, Feedback Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [Open](https://github.com/nikhilroxtomar/FANet) |'
  prefs: []
  type: TYPE_TB
- en: '| MPA-DA [[128](#bib.bib128)] | JBHI | CNN+UNet | ResNet101[[74](#bib.bib74)]
    | Fully-Sup. | Domain Adaptation, Self-training | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+1000 | [Open](https://github.com/CityU-AIM-Group/MPA-DA)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ResUNet++[[129](#bib.bib129)] | JBHI | CNN+UNet | ResUNet[[130](#bib.bib130)]
    | Fully-Sup. | Test Time Augmentation | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/DebeshJha/ResUNetPlusPlus-with-CRF-and-TTA)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SCRNet [[47](#bib.bib47)] | AAAI | CNN+UNet | FPN[[131](#bib.bib131)] | Fully-Sup.
    | Semantic Calibration/Refinement | Kvasir-SEG [[70](#bib.bib70)] | 700 | No |'
  prefs: []
  type: TYPE_TB
- en: '| CAFD [[132](#bib.bib132)] | ICCV | CNN+UNet | ResNet50[[74](#bib.bib74)]
    | Semi-Sup. | Adversarial Learning, Collaborative Learning | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 306+500 | No |'
  prefs: []
  type: TYPE_TB
- en: '| UACANet [[66](#bib.bib66)] | ACM MM | CNN+UNet | Res2Net[[76](#bib.bib76)]
    | Fully-Sup. | Uncertainty, Self-attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/plemeri/UACANet) |'
  prefs: []
  type: TYPE_TB
- en: '| Segtran[[123](#bib.bib123)] | IJCAI | Transformer+CNN | ResNet101[[74](#bib.bib74)]
    | Fully-Sup. | Squeezed Attention, Positional Encoding | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 489+800 | [Open](https://github.com/askerlee/segtran) |'
  prefs: []
  type: TYPE_TB
- en: '| DenseUNet [[133](#bib.bib133)] | Sensors | CNN+UNet | DenseNet[[88](#bib.bib88)]
    | Fully-Sup. | Dense Connections, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 490+800 | No |'
  prefs: []
  type: TYPE_TB
- en: '| EUNet [[134](#bib.bib134)] | CVR | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | Fully-Sup. | Semantic Enhancement, Global Context | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/rucv/Enhanced-U-Net)
    |'
  prefs: []
  type: TYPE_TB
- en: '| DDANet [[58](#bib.bib58)] | ICPR | CNN+UNet | ResUNet[[130](#bib.bib130)]
    | Fully-Sup. | Dual Decoder | Kvasir-SEG [[70](#bib.bib70)] | 880 | [Open](https://github.com/nikhilroxtomar/DDANet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| FUNet [[135](#bib.bib135)] | CBM | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | Fully-Sup. | Dual Attention, Gate | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | No |'
  prefs: []
  type: TYPE_TB
- en: '| T-UNet [[136](#bib.bib136)] | Healthcare | CNN+UNet | ResNet50[[74](#bib.bib74)]
    | Fully-Sup. | Dilated Convolution, Feature Fusion | ClinicDB [[69](#bib.bib69)]
    | 489 | No |'
  prefs: []
  type: TYPE_TB
- en: '| DNets[[137](#bib.bib137)] | ISBI | CNN+FCN | FPN[[131](#bib.bib131)]+Deeplabv3+[[126](#bib.bib126)]
    | Fully-Sup. | Ensemble Learning | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 489+800 | [Open](https://github.com/vlbthambawita/divergent-nets) |'
  prefs: []
  type: TYPE_TB
- en: '|  | AAU [[48](#bib.bib48)] | ISBI | CNN+FCN | ResNet34[[74](#bib.bib74)] |
    Fully-Sup. | Asymmetric Attention, Upsampling | Kvasir-SEG [[70](#bib.bib70)]
    | 800 | No |'
  prefs: []
  type: TYPE_TB
- en: '|  | BI-GCN [[138](#bib.bib138)] | BMVC | GCN | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Feature Aggregation, Graph Convolution | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/smallmax00/BI-GConv)
    |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE II: Summary of popular deep CPS methods from 2021 to 2023, including
    network architecture, backbone, level of supervision, learning paradigm, training
    dataset and the source code. Section [2](#S2 "2 Methodology (Survey) ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey") gives more
    detailed descriptions.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Year | Methods | Publication | Architecture | Backbone | Supervision |
    Learning Paradigm | Training Dataset | Number | Available |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | ACSNet [[67](#bib.bib67)] | MICCAI | CNN+UNet | ResNet34 [[74](#bib.bib74)]
    | Fully-Sup. | Adaptive Selection, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 547+600 | [Open](https://github.com/ReaFly/ACSNet) |'
  prefs: []
  type: TYPE_TB
- en: '| MI²GAN [[139](#bib.bib139)] | MICCAI | GAN | CycleGAN [[140](#bib.bib140)]
    | Fully-Sup. | Domain Adaptation, Adversarial Learning | ClinicDB [[69](#bib.bib69)]
    | 490 | No |'
  prefs: []
  type: TYPE_TB
- en: '| PraNet [[6](#bib.bib6)] | MICCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Feature Aggregating, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 547+800 | [Open](https://github.com/DengPingFan/PraNet) |'
  prefs: []
  type: TYPE_TB
- en: '| Polypseg [[141](#bib.bib141)] | MICCAI | CNN+UNet | UNet [[3](#bib.bib3)]
    | Fully-Sup. | Context-aware, Semantic | Kvasir-SEG [[70](#bib.bib70)] | 600 |
    No |'
  prefs: []
  type: TYPE_TB
- en: '| ABCNet [[142](#bib.bib142)] | Sensors | CNN+FCN | ResNeXt50 [[143](#bib.bib143)]
    | Fully-Sup. | Boundary Loss-aware, Two-stream | Kvasir-SEG [[70](#bib.bib70)]+EndoScene[[108](#bib.bib108)]
    | 800+547 | No |'
  prefs: []
  type: TYPE_TB
- en: '| UINet [[144](#bib.bib144)] | MIA | CNN | Res2Net50[[76](#bib.bib76)] | Fully-Sup.
    | Uncertainty, Interpretability | EndoScene[[108](#bib.bib108)] | 547 | No |'
  prefs: []
  type: TYPE_TB
- en: '| MCNet [[145](#bib.bib145)] | JBHI | CNN+UNet | VGG16[[93](#bib.bib93)] |
    Fully-Sup. | Global Semantic, Local Detail | ClinicDB [[69](#bib.bib69)]+[EndoVC](https://endovissub-abnormal.grand-challenge.org/)
    | 412+465 | No |'
  prefs: []
  type: TYPE_TB
- en: '| SSN [[44](#bib.bib44)] | ISBI | CNN+FCN | ResNet [[74](#bib.bib74)] | Fully-Sup.
    | Multi-scale Fusion, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 547+800 | No |'
  prefs: []
  type: TYPE_TB
- en: '| RCIS [[146](#bib.bib146)] | ICARM | CNN+FCN | ResNet [[74](#bib.bib74)] |
    Fully-Sup. | Knowledge Distillation | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 547+800 | No |'
  prefs: []
  type: TYPE_TB
- en: '| Double-UNet [[147](#bib.bib147)] | CBMS | CNN+UNet | VGG19[[93](#bib.bib93)]
    | Fully-Sup. | Multi-branch, Feature Fusion | ClinicDB [[69](#bib.bib69)]+ETIS-Larib
    [[103](#bib.bib103)] | 490+157 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | SFANet [[18](#bib.bib18)] | MICCAI | CNN+UNet | ResNet34 [[74](#bib.bib74)]
    | Fully-Sup. | Selective Aggregation, Boundary Loss | EndoScene[[108](#bib.bib108)]
    | 730 | No |'
  prefs: []
  type: TYPE_TB
- en: '| TDE [[148](#bib.bib148)] | CBMS | CNN+UNet | ResNet34 [[74](#bib.bib74)]
    | Fully-Sup. | Data Augmentation | ClinicDB [[69](#bib.bib69)] | 612 | No |'
  prefs: []
  type: TYPE_TB
- en: '| PsiNet [[149](#bib.bib149)] | EMBC | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | Fully-Sup. | Boundary, Multi-task Learning | EndoScene[[108](#bib.bib108)] |
    638 | [Open](https://github.com/Bala93/Multi-task-deep-network) |'
  prefs: []
  type: TYPE_TB
- en: '| PSGAN [[150](#bib.bib150)] | EMBC | GAN | ResNet50 [[74](#bib.bib74)] | Fully-Sup.
    | Generative Adversarial Network | ClinicDB [[69](#bib.bib69)] | 488 | No |'
  prefs: []
  type: TYPE_TB
- en: '| CPSUNet[[151](#bib.bib151)] | ICMLA | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | Fully-Sup. | Dilated Convolution, Morphological | ClinicDB [[69](#bib.bib69)]+
    GIANA[[152](#bib.bib152)] | 300+56 | No |'
  prefs: []
  type: TYPE_TB
- en: '| ResUNet++ [[5](#bib.bib5)] | ISM | CNN+UNet | ResUNet [[130](#bib.bib130)]
    | Fully-Sup. | Squeeze-Excitation, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 547+800 | [Open](https://github.com/DebeshJha/ResUNetPlusPlus-with-CRF-and-TTA)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PDS [[153](#bib.bib153)] | ISMICT | CNN | Maks R-CNN [[154](#bib.bib154)]
    | Fully-Sup. | Feature Extractor, Ensemble | ClinicDB [[69](#bib.bib69)]+ ETIS-Larib[[103](#bib.bib103)]
    | 547+157 | No |'
  prefs: []
  type: TYPE_TB
- en: '| GIANA [[155](#bib.bib155)] | IJCCV | CNN+FCN | ResNet50 [[74](#bib.bib74)]
    | Fully-Sup. | Dilated Convolution, Squeeze-Excitation | ClinicDB [[69](#bib.bib69)]+
    GIANA[[152](#bib.bib152)] | 300+56 | No |'
  prefs: []
  type: TYPE_TB
- en: '| CCS [[156](#bib.bib156)] | EMBC | CNN | LinkNet [[157](#bib.bib157)] | Fully-Sup.
    | Color Space Transformations | CVC-ColonDB [[104](#bib.bib104)] | 284 | No |'
  prefs: []
  type: TYPE_TB
- en: '| APS [[158](#bib.bib158)] | Med Phys | CNN+UNet | VGG16 [[93](#bib.bib93)]
    | Fully-Sup. | Data Augmentation, Ensemble Learning | ClinicDB [[69](#bib.bib69)]+
    ETIS-Larib[[103](#bib.bib103)] | 547+157 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 2014-2018 | PSFCN [[45](#bib.bib45)] | EMBC | CNN+FCN | FCN-8S [[36](#bib.bib36)]
    | Fully-Sup. | Patch Selection, Data Augmentation | CVC-ColonDB [[104](#bib.bib104)]
    | 200 | No |'
  prefs: []
  type: TYPE_TB
- en: '| ACPS [[159](#bib.bib159)] | CBM | N/A | N/A | Un-Sup. | Image Preprocessing,
    Edge Detection | N/A | N/A | No |'
  prefs: []
  type: TYPE_TB
- en: '| UMI [[160](#bib.bib160)] | MLSP | CNN+FCN | SegNet [[161](#bib.bib161)] |
    Fully-Sup. | Uncertainty Modeling, Interpretability | EndoScene[[108](#bib.bib108)]
    | 547 | No |'
  prefs: []
  type: TYPE_TB
- en: '| UNet++ [[4](#bib.bib4)] | DLMIA | CNN+UNet | UNet [[3](#bib.bib3)] | Fully-Sup.
    | Skip Pathways, Deep Supervision | ASU-Mayo [[104](#bib.bib104)] | 7,379 | [Open](https://github.com/MrGiovanni/UNetPlusPlus)
    |'
  prefs: []
  type: TYPE_TB
- en: '| RTPS [[46](#bib.bib46)] | MMM | CNN+FCN | FCN-8S [[36](#bib.bib36)] | Fully-Sup.
    | Real-time, Compression | EndoScene[[108](#bib.bib108)] | 547 | No |'
  prefs: []
  type: TYPE_TB
- en: '| MED [[37](#bib.bib37)] | AIKE | CNN+FCN | Deeplabv3 [[126](#bib.bib126)]
    | Fully-Sup. | Database Augmentation, Multimodal | ClinicDB [[69](#bib.bib69)]
    | 547 | No |'
  prefs: []
  type: TYPE_TB
- en: '| FCNet [[40](#bib.bib40)] | SPIE MI | CNN+FCN | VGG16 [[93](#bib.bib93)] |
    Fully-Sup. | Fully Convolution Networks | ClinicDB [[69](#bib.bib69)] | 612 |
    No |'
  prefs: []
  type: TYPE_TB
- en: '| CPFCNet [[39](#bib.bib39)] | BMEI | CNN+FCN | FCN[[36](#bib.bib36)] | Fully-Sup.
    | Fully Convolution Networks | ClinicDB [[69](#bib.bib69)] | 428 | No |'
  prefs: []
  type: TYPE_TB
- en: '| APSNet [[38](#bib.bib38)] | MIUA | CNN+FCN | FCN-8s [[36](#bib.bib36)] |
    Fully-Sup. | Region Proposals, Textons | CVC-ColonDB [[104](#bib.bib104)] | 200
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| SuperSeg [[162](#bib.bib162)] | SPMB | N/A | N/A | Un-Sup. | Superpixel Segmentation,
    SVM | N/A | N/A | No |'
  prefs: []
  type: TYPE_TB
- en: '| APD [[17](#bib.bib17)] | JBHI | MLP | N/A | Un-Sup. | Superpixel, Autoencoder,
    Saliency | N/A | N/A | No |'
  prefs: []
  type: TYPE_TB
- en: '| WM-DOVA [[69](#bib.bib69)] | CMIG | N/A | N/A | Un-Sup. | Boundary Constraints,
    Saliency | N/A | N/A | No |'
  prefs: []
  type: TYPE_TB
- en: '| PAD-WCE [[163](#bib.bib163)] | ROBIO | N/A | N/A | Un-Sup. | K-means Clustering,
    Contour | N/A | N/A | No |'
  prefs: []
  type: TYPE_TB
- en: '| MSA-DOVA [[16](#bib.bib16)] | TRMI | N/A | N/A | Un-Sup. | Valley Fnformation,
    Energy Maps | N/A | N/A | No |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: Summary of popular deep CPS methods from 2014 to 2020\. For the
    completeness, some early relevant works ranging from 2014 to 2018 have also been
    stated briefly.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 Transformer-based Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The transformer architecture was first introduced by [[164](#bib.bib164)] published
    in 2017\. Compared to CNN, transformer architecture can capture long-range dependencies
    in sequences, making it suitable for understanding global context. Recently, Ling
    et al. [[9](#bib.bib9)] proposed a novel Gaussian-probabilistic guided semantic
    fusion method for polyp segmentation. Wang et al. [[8](#bib.bib8)] introduce a
    pyramid transformer architecture for the polyp segmentation task to increase the
    generalization ability of the neural network. Xiao et al. [[112](#bib.bib112)]
    proposed a transformer-based network for robust and accurate polyp segmentation
    by mimicking the preliminary-to-refined working paradigm of doctors. Chen et al.
    [[113](#bib.bib113)] proposes a novel complement local transformer-based network
    architecture for medical small object segmentation, which can complement local
    detailed information when up-sampling global features. Du et al. [[114](#bib.bib114)]
    introduce a novel method called the swin pyramid aggregation network, which introduces
    two designed modules into the network with a swin transformer as the backbone
    to learn more powerful and robust features. Wu et al. [[59](#bib.bib59)] presents
    a multiscale spatial reverse attention network that adopts the Swin Transformer
    encoder with a pyramid structure to extract the features of four different stages.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.4 Hybrid-network Based Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Very recently, as shown in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.2 CNN-based Method
    ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")(g), the researcher attempted
    to combine CNN with transformer for polyp segmentation, aiming at separately capturing
    local and long-term features from CNN and transformer, respectively. Jha et al.
    [[90](#bib.bib90)] proposed a transformer-based residual network for colon polyp
    segmentation and evaluated its diagnostic performance. Cai et al. [[105](#bib.bib105)]
    proposed a hybrid framework for the polyp segmentation task, in which the encoder
    consists of a deep Transformer branch and a shallow CNN branch to extract rich
    features. Sanderson et al. [[110](#bib.bib110)] proposed a new architecture for
    polyp segmentation in colonoscopy images, which combines FCNs and transformers
    to achieve state-of-the-art results. Huang et al. [[61](#bib.bib61)] presented
    TransMixer, a hybrid interaction fusion architecture of the Transformer branch
    and the CNN branch, which can enhance the local details of global representations
    and the global context awareness of local features. Zhang et al. [[116](#bib.bib116)]
    introduced a hybrid semantic network by combining CNN and Transformer, aiming
    at separately capturing local and long-term features in the polyp. Liu et al.
    [[117](#bib.bib117)] proposed a dual branch multiscale feature fusion network
    for Polyp Segmentation, which uses CNN and Transformer in parallel to extract
    multiscale local information and global contextual information, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Level of Supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to the level of supervision, existing deep CPS methods can be categorized
    into fully-supervised (Sec. [2.2.1](#S2.SS2.SSS1 "2.2.1 Fully-Supervised Methods
    ‣ 2.2 Level of Supervision ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")), semi-supervised (Sec. [2.2.2](#S2.SS2.SSS2
    "2.2.2 Semi-Supervised Methods ‣ 2.2 Level of Supervision ‣ 2 Methodology (Survey)
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")),
    weakly-supervised (Sec. [2.2.3](#S2.SS2.SSS3 "2.2.3 Weakly-Supervised Methods
    ‣ 2.2 Level of Supervision ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")), and self-/un-supervised approaches
    (Sec. [2.2.4](#S2.SS2.SSS4 "2.2.4 Self-/Un-Supervised Methods ‣ 2.2 Level of Supervision
    ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Fully-Supervised Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a fully supervised learning paradigm, a deep model trained on a dataset with
    pixel-level annotations, which heavily relies on the extensive manual annotation
    to ensure remarkable performance, and most current deep CPS methods [[27](#bib.bib27),
    [28](#bib.bib28), [49](#bib.bib49), [8](#bib.bib8), [105](#bib.bib105), [30](#bib.bib30),
    [31](#bib.bib31), [51](#bib.bib51), [110](#bib.bib110), [56](#bib.bib56), [61](#bib.bib61),
    [33](#bib.bib33)] are based on fully supervised learning. Yin et al. [[42](#bib.bib42)]
    proposed a duplex contextual relation network to simultaneously capture the contextual
    relations across images and within individual images, with 3350 pixel-wise annotations.
    Zhang et al. [[49](#bib.bib49)] designed a lesion-aware dynamic network for the
    polyp segmentation task with 1348 pixel-wise labels. Wang et al. [[8](#bib.bib8)]
    uses a pyramid Transformer encoder to improve the generalization ability of models.
    Cai et al. [[105](#bib.bib105)] propose a PPFormer for accurate polyp segmentation,
    which takes advantage of the Transformer’s long-range modeling ability and CNN’s
    local feature extraction. Yue et al. [[32](#bib.bib32)] proposed a novel boundary
    constraint network where the polyp segmentation task is completed in a fully supervised
    manner. Ji et al. [[2](#bib.bib2)] design a multistream network for video polyp
    segmentation, with 19,554 SUN-SEG data for training.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Semi-Supervised Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although a fully-supervised model can achieve high performance, it heavily relies
    on pixel-wise labeled large-scale datasets. Therefore, more and more researchers
    are focusing on semi-supervised learning [[54](#bib.bib54)], which can achieve
    good performance even with a limited amount of labeled and unlabeled data. For
    instance, Li et al. proposed a semi-supervised learning paradigm for video polyp
    segmentation using only 547 pixel-level labeled images, containing a co-training
    scheme to supervise the predictions of unlabeled images. Zhao et al. [[50](#bib.bib50)]
    proposed an accurate and novel network for semi-supervised polyp video segmentation
    task, which exploits the spatial and temporal information from the proximity frames
    in endoscope videos Wu et al. [[132](#bib.bib132)] propose a novel semi-supervised
    polyp segmentation method called collaborative and adversarial learning of focused
    and dispersive representations learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Weakly-Supervised Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On the other hand, to alleviate the fully-supervised methods relying on pixel-wise
    annotation, some researchers have adopted semi-supervised learning methods, including
    point supervision, semantic label supervision, scribble supervision, and bounding-box
    supervision. Wang et al. [[73](#bib.bib73)] proposed a framework of spatial-spectral
    mutual teaching and ensemble learning for scribble-supervised polyp segmentation.
    Wei et al. [[75](#bib.bib75)] introduced a model completely based on bounding
    box annotations, reducing the labeling cost and achieving comparable performance
    to full supervision. Wei et al. [[29](#bib.bib29)] proposed a model to make full
    use of both accurate mask and extra 40,266 frames box annotation from LDPolypVideo
    [[107](#bib.bib107)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4 Self-/Un-Supervised Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Furthermore, to thoroughly address the dependence on annotated datasets, some
    researchers have adopted self-supervised methods to pre-train models and transfer
    them to downstream tasks. Wang et al. [[89](#bib.bib89)] proposed to use pseudo
    labels predicted from the source pre-trained model to perform contrastive learning
    in a supervised way. Yang et al. [[128](#bib.bib128)] propose a progressive self-training
    module, which selects reliable pseudo labels through a novel uncertainty-guided
    self-training loss to obtain accurate prototypes in the target domain. Tian et
    al. [[121](#bib.bib121)] propose a novel self-supervised representation learning
    method for polyp segmentation, which learns fine-grained feature representations
    by simultaneously predicting the distribution of augmented data and image contexts
    using contrastive learning with pretext constraints. It is worth noting that traditional
    polyp segmentation methods [[159](#bib.bib159), [17](#bib.bib17), [162](#bib.bib162),
    [69](#bib.bib69), [163](#bib.bib163)] are typically implemented in an unsupervised
    manner. For instance, Jia et al. [[163](#bib.bib163)] proposed a feasible method
    using K-means clustering and localizing region-based active contour segmentation
    for CPS.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Learning Paradigm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From the perspective of the learning paradigm, deep CPS models can be divided
    into 12 subcategories, such as attention mechanism, multi-scale feature fusion,
    boundary awareness, domain adaptation, multi-task learning, adversarial learning,
    data augmentation, designing loss function, multi-stage refinement, ensemble learning,
    interpretability, and knowledge distillation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Attention Mechanism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The attention mechanism was first proposed in [[165](#bib.bib165)] of machine
    translation, which allows the model to focus on relevant parts of the input when
    making predictions. Subsequently, it was introduced to the polyp segmentation
    task. For instance, Cai et al. [[105](#bib.bib105)] present the PP-guided self-attention
    to enhance the model’s perception of polyp boundary. Zhao et al. [[50](#bib.bib50)]
    propose a novel spatial-temporal attention network composed of temporal local
    context attention module and proximity frame time-space attention module. Ji et
    al. [[2](#bib.bib2)] propose a normalized self-attention block, which is motivated
    by the fact that dynamically updating the receptive field is important for self-attention-based
    networks. Huang et al. [[61](#bib.bib61)] proposed a hierarchical attention module
    to encourage the collection of polyp semantic information from high-level features
    to gradually guide the recovery of polyp spatial information in low-level features.
    Patel et al. [[62](#bib.bib62)] designed a novel attention module to focus more
    on the difficult pixels that usually lie near the boundary region. Du et al. [[114](#bib.bib114)]
    proposed a local pyramid attention module to aggregate attention cues in different
    scales and guide the network to enhance semantic features and emphasize the target
    area.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Multiscale Feature Fusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The goal of multi-scale feature fusion is to improve the model’s ability to
    deal with varying sizes of objects, leading to more robust and accurate segmentation.
    Tomar et al. [[28](#bib.bib28)] propose a multi-scale feature aggregation to capture
    features learned by different decoder blocks. Wang et al. [[8](#bib.bib8)] propose
    a novel multi-stage feature aggregation decoder, which consists of the local emphasis
    module and the stepwise feature aggregation module. Srivastava et al. [[51](#bib.bib51)]
    proposed a novel dual-scale dense fusion block that performs dual-scale feature
    exchange and a sub-network that exchanges multi-scale features. Yue et al. [[32](#bib.bib32)]
    propose a novel cross-layer feature integration strategy that consists of an attention-driven
    cross-layer feature interaction module and a global feature integration module.
    Ji et al. [[2](#bib.bib2)] propose a novel global-to-local learning paradigm,
    which realizes both long-term and short-term spatial-temporal propagation at an
    arbitrary temporal distance. Qiu et al. [[56](#bib.bib56)] design a boundary distribution
    guided decoder to fuse multi-scale features to improve polyp segmentation on different
    sizes. Huang et al. [[61](#bib.bib61)] present the interaction fusion module to
    bridge the semantic gap between the Transformer branch and the CNN branch, thereby
    fully capturing the global contextual information and local detailed information
    of polyps.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3 Boundary-aware
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The boundary-aware-based method is designed to address the issue of fuzzy segmentation
    boundaries in existing polyp segmentation methods. For example, Jin et al. [[60](#bib.bib60)]
    introduces an edge extractor to obtain the edge information for auxiliary supervision
    to use low-level features better. Yue et al. [[32](#bib.bib32)] propose a novel
    deep network (termed BCNet) by focusing on cross-layer feature integration and
    boundary extraction in consideration of the challenging characteristics of polyps.
    With the assistance of high-level location features and boundary constraints,
    BCNet explores the polyp and non-polyp information of the shallow layer collaboratively
    and yields a better segmentation performance. Qiu et al. [[56](#bib.bib56)] design
    a boundary distribution guided network for accurate polyp segmentation, which
    consists of a boundary distribution generate module and a boundary distribution
    guided decoder. Xiao et al. [[112](#bib.bib112)] proposed an iterative feedback
    prediction module to consider contextual and boundary-aware information. Chen
    et al. [[113](#bib.bib113)] proposes a local edge feature extraction block to
    progressively generate a sequence of high-resolution local edge features. Du et
    al. [[63](#bib.bib63)] proposed the reverse-contour guidance module to receive
    contour detail and reverse information to highlight the boundary.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.4 Domain Adapation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Domain adaptation addresses the model’s performance degradation when there is
    a distributional shift between the training and testing data. Wang et al. [[89](#bib.bib89)]
    a practical problem of source-free domain adaptation, which eliminates the reliance
    on annotated source data. Xiong et al. [[80](#bib.bib80)] propose an image-to-image
    translation network for domain adaptation. Shen et al. [[27](#bib.bib27)] propose
    a task-relevant feature replenishment-based network to tackle existing problems
    in unsupervised domain adaptation. Huy et al. [[55](#bib.bib55)] an unsupervised
    domain adaptation that transfers the style between the domains using Fourier transform
    and adversarial training. Yang et al. [[31](#bib.bib31)] devise a novel source-free
    domain adaptation framework with Fourier style mining, where only a well-trained
    source segmentation model is available to adapt to the target domain. Yang et
    al. [[128](#bib.bib128)] propose a mutual-prototype adaptation network to eliminate
    domain shifts in multi-center and multidevices colonoscopy images. Li et al. [[122](#bib.bib122)]
    propose a polymorphic transformer, which can be incorporated into any DNN backbones
    for few-shot domain adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Datasets | Year | Publication | Number | Annotation Type | Resolution
    | Polpy Size ($\%$) | Contrast Values | #Obj. | Availability |'
  prefs: []
  type: TYPE_TB
- en: '| Image-level | ETIS-Larib [[103](#bib.bib103)] | 2014 | IJCARS | 196 | Pixel-wise
    | 1225$\times$966 | 0.10 $\sim$ 29.1 | 0.25 $\sim$ 0.87 | 1 $\sim$ 3 | [Open](https://drive.google.com/drive/folders/10QXjxBJqCf7PAXqbDvoceWmZ-qF07tFi)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CVC-ClinicDB [[69](#bib.bib69)] | 2015 | CMIG | 612 | Pixel-wise | 384$\times$288
    | 0.33 $\sim$ 48.9 | 0.42 $\sim$ 0.96 | 1 $\sim$ 3 | [Open](https://polyp.grand-challenge.org/CVCClinicDB/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CVC-ColonDB [[104](#bib.bib104)] | 2015 | TMI | 300 | Pixel-wise | 574$\times$500
    | 0.29 $\sim$ 63.1 | 0.28 $\sim$ 0.93 | 1 | [Open](https://drive.google.com/drive/folders/1-gZUo1dgsdcWxSdXV9OAPmtGEbwZMfDY)
    |'
  prefs: []
  type: TYPE_TB
- en: '| EndoScene [[108](#bib.bib108)] | 2017 | JHE | 912 | Pixel-wise | 384$\times$288
    to 574$\times$500 | 0.29 $\sim$ 63.2 | 0.27 $\sim$ 0.96 | 1 $\sim$ 3 | [Open](http://pages.cvc.uab.es/CVC-Colon/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Kvasir-SEG [[70](#bib.bib70)] | 2020 | MMM | 1,000 | Pixel-wise | 332$\times$487
    to 1920$\times$1072 | 0.51 $\sim$ 81.4 | 0.35 $\sim$ 0.87 | 1 $\sim$ 3 | [Open](https://datasets.simula.no/kvasir-seg/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| HyperKvasir [[85](#bib.bib85)] | 2020 | SD | 110,079 | Pixel-wise, Bounding-box
    | 332$\times$487 to 1920$\times$1072 | 0.57 $\sim$ 76.6 | 0.35 $\sim$ 0.88 | 1
    $\sim$ 10 | [Open](https://osf.io/mh9sj/) |'
  prefs: []
  type: TYPE_TB
- en: '| Piccolo [[118](#bib.bib118)] | 2020 | AS | 3,433 | Pixel-wise | 854$\times$480
    to 1920$\times$1080 | 0.0005 $\sim$ 65.5 | 0.40 $\sim$ 0.72 | 1 $\sim$ 5 | [Request](https://www.biobancovasco.bioef.eus/en/Sample-and-data-catalog/Databases/PD178-PICCOLO-EN.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Kvasir-sessile [[129](#bib.bib129)] | 2021 | JBHI | 196 | Pixel-wise | 401$\times$415
    to 1348$\times$1070 | 0.54 $\sim$ 58.4 | 0.39 $\sim$ 0.86 | 1 $\sim$ 3 | [Open](https://datasets.simula.no/kvasir-seg/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| BKAI-IGH [[166](#bib.bib166)] | 2021 | AVC | 1,200 | Pixel-wise | 1280$\times$959
    | 0.14 $\sim$ 19.4 | 0.29 $\sim$ 0.88 | 1 $\sim$ 18 | [Open](https://www.kaggle.com/competitions/bkai-igh-neopolyp/data)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PolypGen [[24](#bib.bib24)] | 2023 | SD | 8,037 | Pixel-wise, Bounding-box
    | 384$\times$288 to 1920$\times$1080 | 0.001 $\sim$ 74.1 | 0.29 $\sim$1.0 | 1
    $\sim$ 17 | [Open](https://www.kaggle.com/competitions/bkai-igh-neopolyp/data)
    |'
  prefs: []
  type: TYPE_TB
- en: '|   Video-level | ASU-Mayo [[104](#bib.bib104)] | 2016 | TMI | 36,458 | Pixel-wise
    | 688$\times$550 | - | - | 1 | [Request](https://polyp.grand-challenge.org/AsuMayo/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LDPolyVideo [[107](#bib.bib107)] | 2021 | MICCAI | 901,626 | Bounding-box
    | 560$\times$480 | - | 0.10 $\sim$ 0.99 | 1 | [Open](https://github.com/dashishi/LDPolypVideo-Benchmark)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ToPV [[167](#bib.bib167)] | 2022 | ISBI | 11,953 | Video-level | 384$\times$352
    | - | 0.08 $\sim$ 1.02 | 1 | [Request](http://www.pami.sjtu.edu.cn/En/Show/74/163)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SUN-SEG [[2](#bib.bib2)] | 2022 | MIR | 158,690 | Diverse-Annotations | 1158$\times$1008
    to 1240$\times$1080 | 0.06 $\sim$ 30.0 | 0.20 $\sim$ 0.89 | 1 | [Request](https://github.com/GewelsJI/VPS)
    |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: Statistics of popular polyp segmentation datasets, including the
    number of images, annotation type, image resolution, ratio of the object size
    and contrast value in mages. Please refer to Section. [3](#S3 "3 Polyp Segmentation
    Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey") for more detailed descriptions.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.5 Multi-task Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multi-task learning enhances the model’s learning capabilities by jointly training
    on multiple tasks, exploiting complementary information between different tasks
    to achieve better performance. Wang et al. [[82](#bib.bib82)] propose an efficient
    multi-task synergetic network for concurrent polyp segmentation and classification.
    Tomar et al. [[28](#bib.bib28)] propose a text-guided attention mechanism for
    polyp segmentation using a simple byte-pair encoding. Yue et al. [[32](#bib.bib32)]
    proposed a multi-task learning strategy that simultaneously supervises the boundary
    prediction and polyp prediction with the boundary mask and polyp mask. Xiao et
    al. [[112](#bib.bib112)] presents an iterative context-boundary feedback network
    to iterative feedback the preliminary predictions of both segmentation and boundary
    into different encoder levels. Murugesan et al. [[149](#bib.bib149)] proposed
    a novel multi-task network and mask prediction is the primary task, while contour
    detection and distance map estimation are auxiliary tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.6 Adversarial Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Adversarial learning in image segmentation can enhance models’ robustness and
    generalization capability, making them more suitable for challenging scenarios
    in the real world. Mazumdar et al. [[83](#bib.bib83)] a novel generative adversarial
    network (GAN) based approach for polyp segmentation, integrating a standard generator
    and an attention-based discriminator. Shen et al. [[27](#bib.bib27)] propose polyp-aware
    adversarial learning is employed to bridge the domain gap by aligning features
    in output space. Chen et al. [[57](#bib.bib57)] designed a random color reversal
    synthesis module under the GAN framework, which synthesizes images in which some
    regions are highlighted randomly while the center area of the polyps is always
    highlighted. Yang et al. [[128](#bib.bib128)] propose a novel foreground consistency
    loss with adversarial learning to optimize the reconstruction module. Wu et al.
    [[132](#bib.bib132)] proposed auxiliary adversarial learning to improve the quality
    of segmentation predictions from unlabeled images in the semi-supervised training
    stage. Xie et al. [[139](#bib.bib139)] propose a novel MI²GAN to maintain the
    contents of the medical images during image-to-image domain adaptation. Poorneshwaran
    et al. [[150](#bib.bib150)] propose a conditional generative convolutional framework
    for the task of polyp segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.7 Data Augmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data augmentation enhances the model’s generalization and prevents overfitting
    by applying transformations to the training data, such as rotation, scaling, flipping,
    etc. Moreu et al. [[95](#bib.bib95)] an end-to-end model for integrating synthetic
    and real data under different levels of supervision. Haithami et al. [[78](#bib.bib78)]
    propose a deep learning framework to train an image transformation model with
    a segmentation model jointly. Guo et al. [[30](#bib.bib30)] propose a novel meta-learning
    mixup data augmentation method and a confidence-aware resampling strategy for
    polyp segmentation. Chen et al. [[57](#bib.bib57)] presented a random color reversal
    synthesis module to synthesize polyp images as data augmentation to enhance model
    performance. Wei et al. [[34](#bib.bib34)] propose the color exchange operation
    to decouple the contents and colors, which reduces the overfitting issue. Guo
    et al. [[127](#bib.bib127)] propose a novel ThresholdNet with a confidence-guided
    manifold mixup data augmentation method to alleviate the limited training dataset
    and the class imbalance problems. Thomaz et al. [[148](#bib.bib148)] proposes
    a novel method to increase the quantity and variability of training images from
    a publicly available colonoscopy dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.8 Designing Loss Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the context of deep CPS, the binary cross-entropy (BCE) is the commonly used
    loss function. In practice, the BCE loss always fails to learn accurate object
    boundaries due to the high similarity between polyps and the background tissue.
    Thus, researchers are dedicated to designing various effective loss functions
    to address the shortcomings of BCE. Wei et al. [[75](#bib.bib75)] propose the
    scale consistency loss to improve the robustness of the model against the variability
    of the predictions. Li et al. [[54](#bib.bib54)] proposed a context-free loss
    to mitigate the impact of varying contexts within successive frames. Wei et al.
    [[29](#bib.bib29)] propose the image consistency loss, which mines supervisory
    information from the relationship between images. Zhao et al. [[33](#bib.bib33)]
    build a general training-free loss network to implement the structure supervision
    in the feature levels, which provides an essential supplement to the loss design
    based on the prediction itself. Shen et al. [[65](#bib.bib65)] design an edge
    and structure consistency aware deep loss to improve the segmentation performance
    further. Guo et al. [[127](#bib.bib127)] devised a mixup feature map consistency
    loss and a mixup confidence map consistency loss to exploit the consistent constraints
    in the training of the augmented mixup data. Fang et al. [[142](#bib.bib142)]
    proposed a boundary-sensitive loss designed to leverage the area-boundary constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.9 Others Techniques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to the mainstream techniques mentioned above, there are some less
    commonly used techniques, such as multi-stage refinement, ensemble learning, interpretability,
    and knowledge distillation. Xiao et al. [[112](#bib.bib112)] propose a multi-stage
    refinement model iteratively via feedbacking contextual and boundary-aware detail
    from the preliminary segmentation and boundary predictions. Guo et al. [[158](#bib.bib158)]
    proposed an ensemble model for automated polyp segmentation by ensembling the
    UNet, SegNet [[161](#bib.bib161)], and PSPNet [[168](#bib.bib168)]. Wickstrom
    et al. [[144](#bib.bib144)] develop and evaluate recent advances in uncertainty
    estimation and model interpretability for CPS. Huang et al. [[146](#bib.bib146)]
    proposed the first knowledge distillation to compress a colonoscopy image segmentation
    model to achieve real-time segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Polyp Segmentation Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the rapid development of CPS, many datasets have been introduced. Table
    [IV](#S2.T4 "TABLE IV ‣ 2.3.4 Domain Adapation ‣ 2.3 Learning Paradigm ‣ 2 Methodology
    (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey") provides a systematic summary of 14 commonly used CPS datasets, which
    can be divided into image-level (Sec. [3.1](#S3.SS1 "3.1 Image-level Datasets
    ‣ 3 Polyp Segmentation Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")) and video-level datasets (Sec. [3.2](#S3.SS2 "3.2
    Video-level Datasets ‣ 3 Polyp Segmentation Datasets ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")). Besides, as shown in Fig.
    [5](#S3.F5 "Figure 5 ‣ 3.1 Image-level Datasets ‣ 3 Polyp Segmentation Datasets
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")
    and Fig. [6](#S3.F6 "Figure 6 ‣ 3.1 Image-level Datasets ‣ 3 Polyp Segmentation
    Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey"), we also statistics the polyp size, color contrast, and polyp location
    distribution, providing a better understanding of the characteristics of existing
    polyp datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Image-level Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ETIS-Larib [[103](#bib.bib103)] dataset was introduced in 2014, which contains
    196 polyp images and their corresponding pixel-wise groundtruth, with a resolution
    of 1255$\times$966 pixels. The distribution of image contrast of ETIS-Larib ranges
    from 0.25 to 0.87, with the majority of images having contrast values between
    0.45 and 0.7\. The proportion of polyp sizes relative to the entire image varies
    from 0.1% to 29%, with most images having polyp sizes falling between 0.1% to
    10%. The distribution of polyp locations in the images is scattered.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/87ab63e086bff78f084959ae5cffc836.png)![Refer to caption](img/850c0bc43908462c9921482f8b645e5c.png)![Refer
    to caption](img/702ba63e8b4ec99835250a0863bc7f76.png)![Refer to caption](img/d8463f68def0ae8b22dfedf52da6c8c6.png)![Refer
    to caption](img/04157d8d6c2f10672248757d3c14be2f.png)![Refer to caption](img/2c570f7be509f9270289681dfb1e1e45.png)![Refer
    to caption](img/0bfa66ebccb19ca930840219ac0a14ed.png)![Refer to caption](img/0a88c9342b84fa53b88f8e7e47a389a4.png)![Refer
    to caption](img/764375076495f3bb064559af6bfb000d.png)![Refer to caption](img/7f1d52315c45082b37dcdf88b57d103d.png)![Refer
    to caption](img/55f8cdd045c1ccea844e576c31c83fdb.png)![Refer to caption](img/0fce34f5532328309968d14f93b9960e.png)![Refer
    to caption](img/eebc86a04dd31ac712a071f1180cd086.png)![Refer to caption](img/27f843f7ef466bc0e236eb84923b0f5c.png)![Refer
    to caption](img/39f01b79a4012bb7ee5d78c7b1f23ec5.png)![Refer to caption](img/4ba542f7da0fe041b63d1aa0acc7064f.png)![Refer
    to caption](img/be1ff6b13277fc7c91d303efd9379258.png)![Refer to caption](img/ba0ab400db93ade7b7217aede5272366.png)![Refer
    to caption](img/f445425c930f6d949c19f01a0a281428.png)![Refer to caption](img/ff559b5489e7bb5433ea8cbf81ba9a3b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Statistics of polyp size (first two rows) and contrast value (last
    two rows) of current CPS datasets. From the perspective of polyp size, the BKAI-IGH
    is the most challenging dataset because it has the smallest average polyp size,
    while from the perspective of contrast, the SUN-SEG is the most challenging one
    because its contrast value is only 0.51.'
  prefs: []
  type: TYPE_NORMAL
- en: CVC-ClinicDB [[69](#bib.bib69)] was released in 2015, containing 612 polyp images
    and the corresponding pixel-wise annotations, with a resolution of 384$\times$288
    pixels. The distribution of image contrast of CVC-ClinicDB ranges from 0.42 to
    0.96, with most images having contrast values between 0.53 and 0.9, and the average
    contrast is 0.73\. The proportion of polyp sizes relative to the entire image
    varies from 0.33% to 48.9%, with the majority of images having polyp sizes falling
    between 0.1% to 10% and the mean sizes being 9.3%. The distribution of polyp locations
    in the images is relatively close to the center of the images.
  prefs: []
  type: TYPE_NORMAL
- en: CVC-ColonDB [[104](#bib.bib104)] was released in 2015, including 300 polyp images
    and their corresponding pixel-level labels, with a resolution of 574$\times$500
    pixels. The distribution of image contrast of CVC-ColonDB ranges from 0.28 to
    0.93, with most images having contrast values between 0.4 and 0.75, and the average
    contrast is 0.59\. The proportion of polyp sizes relative to the entire image
    varies from 0.29% to 63.1%, with the majority of images having polyp sizes falling
    between 0.3% to 10% and the mean sizes being 7.4%.
  prefs: []
  type: TYPE_NORMAL
- en: EndoScene [[108](#bib.bib108)] combines CVC-ColonDB and CVC-ClinicDB into a
    new dataset (EndoScene) composed of 912 images obtained from 44 video sequences
    acquired from 36 patients, with different resolutions ranging from 384$\times$288
    to 574$\times$500 pixels. The distribution of image contrast of EndoScene ranges
    from 0.27 to 0.96, with the majority of images having contrast values between
    0.5 and 0.8, and the average contrast is 0.67\. The proportion of polyp sizes
    relative to the entire image varies from 0.29% to 63.2%, with most images having
    polyp sizes falling between 0.3% to 15% and the mean sizes being 8.6%.
  prefs: []
  type: TYPE_NORMAL
- en: Kvasir-SEG [[70](#bib.bib70)] contains 1000 polyp images and their corresponding
    groundtruth, with different resolutions ranging from 332$\times$487 to 1920$\times$1072
    pixels. The contrast of Kvasir-SEG ranges from 0.35 to 0.87, with most images
    having contrast values between 0.5 and 0.7, and the average contrast is 0.59\.
    The polyp sizes vary from 0.51% to 81.4%, with the majority of images falling
    between 0.5% to 20%, and the mean size being 15.6%. The distribution of polyp
    locations in the images is close to the center of the images.
  prefs: []
  type: TYPE_NORMAL
- en: HyperKvasir [[85](#bib.bib85)] was collected during real gastro- and colonoscopy
    examinations at a hospital in Norway and partly labeled by experienced gastrointestinal
    endoscopists. The HyperKvasir contains 110,079 images but only with 348 pixel-wise
    annotations. The contrast of HyperKvasir ranges from 0.35 to 0.88, and the average
    contrast is 0.59\. The polyp sizes vary from 0.57% to 76.6%, with the majority
    of images falling between 0.5% to 16%, and the mean size is 15.1%.
  prefs: []
  type: TYPE_NORMAL
- en: Piccolo [[118](#bib.bib118)] contains 3433 manually annotated images (2131 White-Light
    images and 1302 Narrow-Band images), which is 2203 images for the training set,
    897 images for the validation set, and 333 images for the test set, with a resolution
    of 854$\times$480 to 1920$\times$1080 pixels. Its polyps size ranges from 0.0005%
    to 65.5%, its contrast varies from 0.40 to 0.72, and each image contains 1$\sim$5
    polyps.
  prefs: []
  type: TYPE_NORMAL
- en: Kvasir-sessile [[129](#bib.bib129)] selected from the Kvasir-SEG [[70](#bib.bib70)],
    contains 196 flat or sessile polyps and the corresponding pixel-wise annotations,
    with a resolution of 401$\times$415 to 1348$\times$1070 pixels. Its polyps size
    ranges from 0.54% to 58.4%, its contrast varies from 0.39 to 0.86, and each image
    contains 1$\sim$3 polyps.
  prefs: []
  type: TYPE_NORMAL
- en: BKAI-IGH [[166](#bib.bib166)] contains 1200 images, with a resolution of 1280$\times$959
    pixels. The training set consists of 1000 images with instance-level pixel-wise
    annotations, and the test set consists of 200 images without annotations. All
    polyps are also classified into neoplastic or non-neoplastic classes denoted by
    red and green colors. Its polyps size ranges from 0.14% to 19.4%, its contrast
    varies from 0.29 to 0.88, and each image contains 1$\sim$18 polyps.
  prefs: []
  type: TYPE_NORMAL
- en: PolypGen [[24](#bib.bib24)] is composed of a total of 8037 frames, including
    both single and sequence frames, consisting of 3762 positive sample frames collected
    from six centers and 4275 negative sample frames collected from four different
    hospitals. Its polyps size ranges from 0.001% to 74.1%, its contrast varies from
    0.29 to 1.0, and each image contains 1$\sim$17 polyps.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e9eb802033ecdc05bb4c32993625e4ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Polyps location distributions of some popular CPS datasets. From
    the perspective of location distribution, the HyperKvasir and Kvasir-SEG datasets
    are relatively easier. In contrast, the CVC-ColonDB and ETIS-Larib datasets are
    more challenging because their polyp location distribution is more dispersed.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Video-level Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ASU-Mayo [[104](#bib.bib104)] is composed of a total of 36,458 frames, which
    is the first and a constantly growing set of short and long colonoscopy videos,
    collected and de-identified at the Department of Gastroenterology at Mayo Clinic
    in Arizona. Each frame has a groundtruth or binary mask indicating the polyp region.
    However, the problem is that it is not publicly available, and as of our submission,
    the authors have not responded to our requests for the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: ToPV [[167](#bib.bib167)] contains 360 short videos collected during real colonoscopy
    examination. Two hundred four videos contain one polyp labeled by experienced
    endoscopic physicians and cropped to a size of 384×352\. The maximum duration
    of video clips in our dataset is 58.4 seconds, and the minimal duration is 5.88
    seconds, with a median duration of 15.76 seconds and an average duration of 16.78
    seconds. All videos are under white-light endoscopy observation.
  prefs: []
  type: TYPE_NORMAL
- en: LDPolyVideo [[107](#bib.bib107)] consists of 160 videos with 40,266 frames and
    the corresponding bounding box annotations, with a resolution of 560$\times$480\.
    33,884 frames contain at least one polyp, and in total, 200 labeled polyps. Besides
    we also provide 103 videos, including 861,400 frames without full annotations.
    Each video has a label indicating whether it contains polyps.
  prefs: []
  type: TYPE_NORMAL
- en: SUN-SEG [[2](#bib.bib2)] contains positive cases with 49,136 polyp frames and
    negative cases with 109,554 non-polyp frames. The SUN-SEG dataset contains diversified
    annotations, including pixel-wise object mask, boundary, scribble, and polygon
    annotations. The SUN-SEG dataset consists of 19,544 frames for training and 29,592
    frames for testing. The SUN-SEG is the most well-annotated and high-quality dataset
    for polyp segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Analysis and Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in Table [IV](#S2.T4 "TABLE IV ‣ 2.3.4 Domain Adapation ‣ 2.3 Learning
    Paradigm ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep
    Learning Era: A Comprehensive Survey"), HyperKvasir is the largest dataset for
    image polyp segmentation in terms of quantity but only with 348 pixel-wise annotations,
    while LDPolyVideo is the largest dataset for video polyp segmentation with only
    bounding box annotations. From the perspective of quality and quantity of annotated
    data, SUN-SEG is currently the best dataset, with 49,136 well-annotated images,
    including pixel-wise object mask, boundary, scribble, and polygon annotations.
    As demonstrated in Fig. [5](#S3.F5 "Figure 5 ‣ 3.1 Image-level Datasets ‣ 3 Polyp
    Segmentation Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning Era:
    A Comprehensive Survey"), the BKAI-IGH is the most challenging dataset in terms
    of polyp size because it has the smallest average polyp size, while from the perspective
    of contrast, the SUN-SEG is more challenging one because its contrast value is
    only 0.51\. As shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1 Image-level Datasets ‣
    3 Polyp Segmentation Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey"), from the perspective of location distribution,
    the HyperKvasir and Kvasir-SEG datasets are relatively easier, while the CVC-ColonDB
    and ETIS-Larib datasets are more challenging because their polyp location distribution
    is more dispersed. Besides, polyps exhibit diverse variations in color, size,
    and quantity, making it very challenging to accurately segment them, as shown
    in Fig. [7](#S4.F7 "Figure 7 ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section reviews commonly used CPS evaluation metrics, including Dice, IoU,
    Precision, Recall, Specificity, PR curves, F-measure [[169](#bib.bib169)], MAE
    [[170](#bib.bib170)], Weighted F-measure [[171](#bib.bib171)], S-measure [[172](#bib.bib172)],
    E-measure [[173](#bib.bib173)] and FPS.
  prefs: []
  type: TYPE_NORMAL
- en: Dice coefficient, also known as the F1 score, is a measure of the overlap between
    two sets, with a range of 0 to 1\. A value of 1 indicates a perfect overlap, while
    0 indicates no overlap.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Dice=\frac{2TP}{2TP+FP+FN}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: IoU (Intersection over Union) measures the overlap between two sets but is expressed
    as a ratio of the size of the intersection to the size of the union of the sets.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $IoU=\frac{TP}{TP+FP+FN}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/e3ad44a121131a71fda8382c4944f6fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Illustration of polyp diverse variations in color, size, and quantity,
    making it very challenging to accurately segment polyps.'
  prefs: []
  type: TYPE_NORMAL
- en: Precision is a measure of the positive predictive value of a classifier or the
    proportion of true positive predictions among all positive predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Precision=\frac{TP}{TP+FP}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Recall, also known as sensitivity or true positive rate, is an evaluation metric
    used in binary and multiclass classification to measure the ability of a model
    to identify positive instances correctly. It quantifies the proportion of actual
    positive instances that the model correctly predicted.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Recall=\frac{TP}{TP+FN}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Accuracy is the overall correct classification rate or the proportion of correct
    predictions made by the classifier out of all predictions made.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Acc=\frac{TP+TN}{TP+TN+FP+FN}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|    Method | Publication | CVC-Clinic. [[69](#bib.bib69)] | Kvasir-SEG [[70](#bib.bib70)]
    | CVC-ColonDB [[104](#bib.bib104)] | ETIS-Larib [[103](#bib.bib103)] | EndoScene
    [[108](#bib.bib108)] |'
  prefs: []
  type: TYPE_TB
- en: '| mDice | mIoU | mDice | mIoU | mDice | mIoU | mDice | mIoU | mDice | mIoU
    |'
  prefs: []
  type: TYPE_TB
- en: '| RPFA [[71](#bib.bib71)] | MICCAI 2023 | 0.931 | 0.885 | 0.929 | 0.880 | 0.837
    | 0.759 | 0.822 | 0.746 | 0.905 | 0.839 |'
  prefs: []
  type: TYPE_TB
- en: '| XBFormer [[86](#bib.bib86)] | TMI 2023 | 0.923 | 0.875 | 0.926 | 0.871 |
    0.808 | 0.724 | 0.738 | 0.650 | 0.868 | 0.791 |'
  prefs: []
  type: TYPE_TB
- en: '| ColnNet [[87](#bib.bib87)] | TMI 2023 | 0.930 | 0.887 | 0.926 | 0.872 | 0.797
    | 0.729 | 0.759 | 0.690 | 0.909 | 0.863 |'
  prefs: []
  type: TYPE_TB
- en: '| FSFM [[41](#bib.bib41)] | ISBI 2023 | 0.934 | 0.884 | 0.913 | 0.861 | 0.786
    | 0.709 | 0.778 | 0.702 | 0.910 | 0.846 |'
  prefs: []
  type: TYPE_TB
- en: '| CASCADE [[98](#bib.bib98)] | WACV 2023 | 0.943 | 0.899 | 0.926 | 0.878 |
    0.825 | 0.745 | 0.801 | 0.728 | 0.905 | 0.838 |'
  prefs: []
  type: TYPE_TB
- en: '| PolypPVT [[100](#bib.bib100)] | CAAI AIR 2023 | 0.937 | 0.889 | 0.917 | 0.864
    | 0.808 | 0.727 | 0.787 | 0.706 | 0.900 | 0.833 |'
  prefs: []
  type: TYPE_TB
- en: '| RealSeg [[79](#bib.bib79)] | ISBI 2023 | 0.923 | 0.873 | 0.913 | 0.863 |
    0.785 | 0.710 | 0.777 | 0.698 | 0.909 | 0.844 |'
  prefs: []
  type: TYPE_TB
- en: '| Polyp-Mixer [[25](#bib.bib25)] | TCSVT 2023 | 0.908 | 0.856 | 0.916 | 0.864
    | 0.791 | 0.706 | 0.759 | 0.676 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CFANet [[91](#bib.bib91)] | PR 2023 | 0.933 | 0.883 | 0.915 | 0.861 | 0.743
    | 0.665 | 0.732 | 0.655 | 0.893 | 0.827 |'
  prefs: []
  type: TYPE_TB
- en: '| M2SNet  [[174](#bib.bib174)] | Arxiv 2023 | 0.922 | 0.880 | 0.912 | 0.861
    | 0.758 | 0.685 | 0.749 | 0.678 | 0.869 | 0.807 |'
  prefs: []
  type: TYPE_TB
- en: '| EMTSNet [[82](#bib.bib82)] | JBHI 2023 | 0.935 | 0.885 | 0.919 | 0.869 |
    0.788 | 0.708 | 0.780 | 0.702 | 0.900 | 0.833 |'
  prefs: []
  type: TYPE_TB
- en: '| PPFormer [[105](#bib.bib105)] | MICCAI 2022 | 0.946 | 0.902 | 0.930 | 0.879
    | 0.823 | 0.756 | 0.791 | 0.706 | 0.919 | 0.857 |'
  prefs: []
  type: TYPE_TB
- en: '| SSFormer-L [[8](#bib.bib8)] | MICCAI 2022 | 0.906 | 0.855 | 0.917 | 0.864
    | 0.802 | 0.721 | 0.796 | 0.720 | 0.895 | 0.827 |'
  prefs: []
  type: TYPE_TB
- en: '| SSFormer-S [[8](#bib.bib8)] | MICCAI 2022 | 0.916 | 0.873 | 0.925 | 0.878
    | 0.772 | 0.697 | 0.767 | 0.698 | 0.887 | 0.821 |'
  prefs: []
  type: TYPE_TB
- en: '| LDNet [[49](#bib.bib49)] | MICCAI 2022 | 0.923 | 0.872 | 0.912 | 0.855 |
    0.794 | 0.715 | 0.778 | 0.707 | 0.893 | 0.826 |'
  prefs: []
  type: TYPE_TB
- en: '| TGANet [[28](#bib.bib28)] | MICCAI 2022 | 0.863 | 0.805 | 0.886 | 0.822 |
    0.695 | 0.609 | 0.574 | 0.488 | 0.822 | 0.733 |'
  prefs: []
  type: TYPE_TB
- en: '| TransMixer [[61](#bib.bib61)] | BIBM 2022 | 0.945 | 0.900 | 0.923 | 0.876
    | 0.823 | 0.745 | 0.795 | 0.719 | 0.910 | 0.844 |'
  prefs: []
  type: TYPE_TB
- en: '| ICBNet [[112](#bib.bib112)] | BIBM 2022 | 0.938 | 0.892 | 0.928 | 0.883 |
    0.812 | 0.738 | 0.800 | 0.727 | 0.898 | 0.833 |'
  prefs: []
  type: TYPE_TB
- en: '| TASNet [[57](#bib.bib57)] | BIBM 2022 | 0.930 | 0.884 | 0.913 | 0.863 | 0.799
    | 0.719 | 0.797 | 0.720 | 0.894 | 0.825 |'
  prefs: []
  type: TYPE_TB
- en: '| DCRNet [[42](#bib.bib42)] | ISBI 2022 | 0.896 | 0.844 | 0.886 | 0.825 | 0.704
    | 0.631 | 0.556 | 0.496 | 0.856 | 0.788 |'
  prefs: []
  type: TYPE_TB
- en: '| BDGNet [[56](#bib.bib56)] | SPIE MI 2022 | 0.905 | 0.857 | 0.915 | 0.863
    | 0.797 | 0.723 | 0.752 | 0.681 | 0.899 | 0.831 |'
  prefs: []
  type: TYPE_TB
- en: '| Conv-MLP [[175](#bib.bib175)] | VisCom 2022 | 0.924 | 0.870 | 0.920 | 0.869
    | 0.793 | 0.717 | 0.753 | 0.676 | 0.893 | 0.822 |'
  prefs: []
  type: TYPE_TB
- en: '| GLFRNet [[176](#bib.bib176)] | TMI 2022 | 0.941 | 0.895 | 0.894 | 0.837 |
    0.729 | 0.659 | 0.674 | 0.595 | 0.898 | 0.827 |'
  prefs: []
  type: TYPE_TB
- en: '| FANet [[35](#bib.bib35)] | TNNLS 2022 | 0.823 | 0.756 | 0.852 | 0.791 | 0.558
    | 0.486 | 0.415 | 0.361 | 0.668 | 0.600 |'
  prefs: []
  type: TYPE_TB
- en: '| FNet-Res2Net [[62](#bib.bib62)] | NeurIPS 2022 | 0.919 | 0.867 | 0.889 |
    0.830 | 0.739 | 0.662 | 0.731 | 0.658 | 0.894 | 0.825 |'
  prefs: []
  type: TYPE_TB
- en: '| FNet-PVT [[62](#bib.bib62)] | NeurIPS 2022 | 0.937 | 0.889 | 0.913 | 0.864
    | 0.811 | 0.728 | 0.791 | 0.702 | 0.891 | 0.818 |'
  prefs: []
  type: TYPE_TB
- en: '| CaraNet[[177](#bib.bib177)] | SPIE MI 2022 | 0.921 | 0.876 | 0.913 | 0.859
    | 0.775 | 0.700 | 0.740 | 0.660 | 0.902 | 0.836 |'
  prefs: []
  type: TYPE_TB
- en: '| Transfuse [[7](#bib.bib7)] | MICCAI 2021 | 0.908 | 0.857 | 0.915 | 0.860
    | 0.790 | 0.710 | 0.748 | 0.657 | 0.893 | 0.825 |'
  prefs: []
  type: TYPE_TB
- en: '| MSNet [[33](#bib.bib33)] | MICCAI 2021 | 0.915 | 0.866 | 0.902 | 0.847 |
    0.747 | 0.668 | 0.720 | 0.650 | 0.862 | 0.796 |'
  prefs: []
  type: TYPE_TB
- en: '| SANet [[34](#bib.bib34)] | MICCAI 2021 | 0.916 | 0.859 | 0.904 | 0.847 |
    0.752 | 0.669 | 0.750 | 0.654 | 0.888 | 0.815 |'
  prefs: []
  type: TYPE_TB
- en: '| UACANet-L [[66](#bib.bib66)] | ACM MM 2021 | 0.926 | 0.880 | 0.912 | 0.859
    | 0.751 | 0.678 | 0.766 | 0.689 | 0.909 | 0.844 |'
  prefs: []
  type: TYPE_TB
- en: '| UACANet-S [[66](#bib.bib66)] | ACM MM 2021 | 0.916 | 0.870 | 0.905 | 0.852
    | 0.783 | 0.704 | 0.694 | 0.615 | 0.902 | 0.837 |'
  prefs: []
  type: TYPE_TB
- en: '| EUNet [[134](#bib.bib134)] | CVR 2021 | 0.902 | 0.846 | 0.908 | 0.854 | 0.756
    | 0.681 | 0.687 | 0.609 | 0.837 | 0.765 |'
  prefs: []
  type: TYPE_TB
- en: '| EMSNet [[178](#bib.bib178)] | EMBC 2021 | 0.923 | 0.874 | 0.897 | 0.842 |
    0.715 | 0.642 | 0.682 | 0.611 | 0.900 | 0.834 |'
  prefs: []
  type: TYPE_TB
- en: '| MSEG[[179](#bib.bib179)] | Arxiv 2021 | 0.909 | 0.864 | 0.897 | 0.839 | 0.735
    | 0.666 | 0.700 | 0.630 | 0.874 | 0.804 |'
  prefs: []
  type: TYPE_TB
- en: '| PraNet [[6](#bib.bib6)] | MICCAI 2020 | 0.899 | 0.849 | 0.898 | 0.840 | 0.709
    | 0.640 | 0.628 | 0.567 | 0.871 | 0.797 |'
  prefs: []
  type: TYPE_TB
- en: '| ACSNet [[67](#bib.bib67)] | MICCAI 2020 | 0.882 | 0.826 | 0.898 | 0.838 |
    0.716 | 0.649 | 0.578 | 0.509 | 0.863 | 0.787 |'
  prefs: []
  type: TYPE_TB
- en: '| SFANet [[18](#bib.bib18)] | MICCAI 2019 | 0.700 | 0.607 | 0.723 | 0.611 |
    0.469 | 0.347 | 0.297 | 0.217 | 0.467 | 0.329 |'
  prefs: []
  type: TYPE_TB
- en: '| ResUNet++ [[5](#bib.bib5)] | ISM 2019 | 0.846 | 0.786 | 0.807 | 0.727 | 0.588
    | 0.497 | 0.337 | 0.275 | 0.687 | 0.598 |'
  prefs: []
  type: TYPE_TB
- en: '| UNet [[3](#bib.bib3)] | MICCAI 2015 | 0.823 | 0.755 | 0.818 | 0.746 | 0.512
    | 0.444 | 0.398 | 0.335 | 0.710 | 0.627 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: Benchmarking performance of 40 state-of-the-art deep CPS models on
    five commonly used datasets in terms of mDice and mIoU. The top 2 methods are
    marked in Red and Blue, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Specificity, also known as True Negative Rate, is an evaluation metric used
    in binary and multiclass classification to measure the ability of a model to identify
    negative instances correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Specificity=\frac{TN}{TN+FP}$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: PR curves provides a visual representation of the performance of a classification
    model across various levels of precision and recall by plotting precision against
    recall at different classification thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: F-measure [[169](#bib.bib169)] is a harmonic mean of average precision and average
    recall. We compute the F-measure as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $F_{\beta}=\frac{(1+\beta^{2})\times{\rm Precision}\times{\rm Recall}}{\beta^{2}\times{\rm
    Precision+Recall}}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where we set $\beta^{2}$ to be 0.3 to weigh precision more than recall.
  prefs: []
  type: TYPE_NORMAL
- en: MAE (Mean Absolute Error) [[170](#bib.bib170)] is calculated as the average
    pixel-wise absolute difference between the binary ${GT}$ and the saliency map
    ${S}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $MAE=\frac{1}{W\times H}\sum\limits_{x=1}^{W}\sum\limits_{y=1}^{H}\Big{&#124;}{S}(x,y)-{GT}(x,y)\Big{&#124;}$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $W$ and $H$ are width and height of the saliency map $S$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Weighted F-measure. Weighted F-measure [[171](#bib.bib171)] define weighted
    precision, which is a measure of exactness, and weighted recall, which is a measure
    of completeness:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $F_{\beta}^{w}=\frac{(1+\beta^{2})\times{\rm Precision}^{w}\times{\rm
    Recall}^{w}}{\beta^{2}\times{\rm Precision}^{w}+{\rm Recall}^{w}}$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'S-measure  [[172](#bib.bib172)] simultaneously evaluates region-aware $S_{r}$
    and object-aware $S_{o}$ structural similarity between the saliency map and ground
    truth. It can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $S_{m}=\alpha\times S_{o}+(1-\alpha)\times S_{r}$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$ is set to 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: E-measure [[173](#bib.bib173)] combines local pixel values with the image-level
    mean value to jointly evaluate the similarity between the prediction and the ground
    truth.
  prefs: []
  type: TYPE_NORMAL
- en: FPS (Frame Per Second) is commonly used to assess the efficiency of model inference.
    FPS is a crucial indicator for evaluating a model’s inference speed and real-time
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: '|    Method | PolypGen-C1 | PolypGen-C2 |'
  prefs: []
  type: TYPE_TB
- en: '| mIoU | mDice | Rec. | Prec. | mIoU | mDice | Rec. | Prec. |'
  prefs: []
  type: TYPE_TB
- en: '| UNet [[3](#bib.bib3)] | .577 | .647 | .678 | .846 | .570 | .634 | .735 |
    .737 |'
  prefs: []
  type: TYPE_TB
- en: '| UNet++ [[180](#bib.bib180)] | .586 | .661 | .695 | .825 | .561 | .624 | .719
    | .763 |'
  prefs: []
  type: TYPE_TB
- en: '| ResUNet++ [[5](#bib.bib5)] | .420 | .524 | .639 | .579 | .278 | .343 | .500
    | .420 |'
  prefs: []
  type: TYPE_TB
- en: '| MSEG [[179](#bib.bib179)] | .626 | .712 | .780 | .793 | .567 | .631 | .727
    | .715 |'
  prefs: []
  type: TYPE_TB
- en: '| LDNet [[49](#bib.bib49)] | .639 | .719 | .755 | .848 | .609 | .689 | .854
    | .687 |'
  prefs: []
  type: TYPE_TB
- en: '| TGANet [[28](#bib.bib28)] | .448 | .539 | .642 | .691 | .378 | .458 | .637
    | .524 |'
  prefs: []
  type: TYPE_TB
- en: '| XBFormer [[86](#bib.bib86)] | .654 | .720 | .744 | .878 | .661 | .723 | .807
    | .810 |'
  prefs: []
  type: TYPE_TB
- en: '|   | PolypGen-C3 | PolypGen-C4 |'
  prefs: []
  type: TYPE_TB
- en: '| UNet [[3](#bib.bib3)] | .677 | .748 | .764 | .879 | .370 | .415 | .655 |
    .598 |'
  prefs: []
  type: TYPE_TB
- en: '| UNet++ [[180](#bib.bib180)] | .653 | .725 | .753 | .857 | .381 | .420 | .634
    | .610 |'
  prefs: []
  type: TYPE_TB
- en: '| ResUNet++ [[5](#bib.bib5)] | .410 | .511 | .646 | .548 | .169 | .227 | .634
    | .282 |'
  prefs: []
  type: TYPE_TB
- en: '| MSEG [[179](#bib.bib179)] | .662 | .744 | .795 | .818 | .352 | .394 | .676
    | .553 |'
  prefs: []
  type: TYPE_TB
- en: '| LDNet [[49](#bib.bib49)] | .707 | .787 | .795 | .889 | .427 | .483 | .737
    | .630 |'
  prefs: []
  type: TYPE_TB
- en: '| TGANet [[28](#bib.bib28)] | .465 | .553 | .626 | .687 | .226 | .276 | .665
    | .355 |'
  prefs: []
  type: TYPE_TB
- en: '| XBFormer [[86](#bib.bib86)] | .722 | .787 | .790 | .913 | .460 | .504 | .687
    | .714 |'
  prefs: []
  type: TYPE_TB
- en: '|   | PolypGen-C5 | PolypGen-C6 |'
  prefs: []
  type: TYPE_TB
- en: '| UNet [[3](#bib.bib3)] | .296 | .361 | .458 | .550 | .538 | .613 | .705 |
    .751 |'
  prefs: []
  type: TYPE_TB
- en: '| UNet++ [[180](#bib.bib180)] | .314 | .377 | .447 | .603 | .536 | .616 | .734
    | .723 |'
  prefs: []
  type: TYPE_TB
- en: '| ResUNet++ [[5](#bib.bib5)] | .204 | .275 | .464 | .303 | .282 | .368 | .622
    | .353 |'
  prefs: []
  type: TYPE_TB
- en: '| MSEG [[179](#bib.bib179)] | .309 | .377 | .459 | .525 | .555 | .634 | .720
    | .772 |'
  prefs: []
  type: TYPE_TB
- en: '| LDNet [[49](#bib.bib49)] | .326 | .403 | .494 | .562 | .604 | .675 | .770
    | .767 |'
  prefs: []
  type: TYPE_TB
- en: '| TGANet [[28](#bib.bib28)] | .253 | .329 | .465 | .419 | .374 | .454 | .602
    | .505 |'
  prefs: []
  type: TYPE_TB
- en: '| XBFormer [[86](#bib.bib86)] | .360 | .421 | .451 | .777 | .634 | .692 | .678
    | .943 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VI: Results of the models trained on Kvasir-SEG and tested on multi-centre
    colonoscopy dataset PolypGen.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Performance Benchmarking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section presents empirical analyses to illustrate essential challenges
    and progress in the CPS field. Firstly, we evaluate the current state-of-the-art
    (SOTA) polyp segmentation models and report their performance on five commonly
    used benchmark datasets (Sec. [5.1](#S5.SS1 "5.1 SOTA Performance Comparisons
    ‣ 5 Performance Benchmarking ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")). Secondly, we assess SOTA polyp segmentation models’
    generalizability on out-of-distribution datasets belonging to different medical
    centers (Sec. [5.2](#S5.SS2 "5.2 Out-of-Distribution Generalization ‣ 5 Performance
    Benchmarking ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey")). Subsequently, we undertake an attribute-based study to understand better
    the inherent strengths and weaknesses in current models (Sec. [5.3](#S5.SS3 "5.3
    Attribute-based Performance Analysis ‣ 5 Performance Benchmarking ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 SOTA Performance Comparisons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [V](#S4.T5 "TABLE V ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey") illustrates the performance
    of 40 cutting-edge deep CPS models across five widely used benchmark datasets,
    and the methodologies proposed over the last three years are emphasized. These
    SOTA models were measured by two commonly adopted metrics, i.e., mDice and mIoU.
    To ensure a fair comparison, we follow the same setting as PraNet [[6](#bib.bib6)],
    which includes 900 and 548 images from ClinicDB and Kvasir-Seg datasets as the
    train set, and the remaining 64 and 100 images are used as the test set. For clarity,
    we highlight the best and second performances using red and blue colors. We aspire
    that our performance benchmarking will contribute to establishing an open and
    standardized evaluation system in the CPS community.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [V](#S4.T5 "TABLE V ‣ 4 Evaluation Metrics ‣ Colorectal Polyp
    Segmentation in the Deep Learning Era: A Comprehensive Survey"), RPFA [[71](#bib.bib71)]
    and PPFormer [[105](#bib.bib105)] achieved the Top 2 performance on CVC-ClinicDB,
    Kvasir-SEG, CVC-ColonDB, ETIS-Larib and EndoScene datasets. In the CVC-ColonDB
    and On ETIS-Larib datasets, RPFA demonstrated its superior performance, outperforming
    PPFormer by 1.4% and 3.1% in terms of mDice, respectively. Conversely, for the
    CVC-ClinicDB and EndoScene datasets, PPFormer achieves the best performance by
    surpassing the RPFA model by 1.5% and 1.4%. As anticipated, the overall learning
    capabilities of these deep learning-based models continue to improvement over
    time. For instance, the UNet [[3](#bib.bib3)] segmentation model, introduced in
    2015, initially achieved only 82.3% in terms of mDice on the CVC-ClinicDB dataset.
    Presently, the recently proposed PPFormer achieves an impressive 94.6%, signaling
    an annual improvement in segmentation accuracy of nearly 1.4%.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Out-of-Distribution Generalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generalization measures how effectively a well-trained model can be applied
    to out-of-distribution data. A model with strong generalization can make accurate
    predictions on unseen data, which is essential for deploying deep learning-based
    poppy segmentation models in real-world clinical scenarios. To assess the SOTA
    model’s generalization ability, we first employ three datasets not encountered
    during training: ETIS, CVC-ColonDB, and EndoScene. These datasets comprise a combined
    total of 196, 300, and 912 images, respectively. Unsurprisingly, as shown in Table
    [V](#S4.T5 "TABLE V ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation in
    the Deep Learning Era: A Comprehensive Survey"), the model exhibits considerably
    lower performance on ETIS, ColonDB, and EndoScene than on the training datasets
    (CVC-ClinicDB and Kvasir-SEG). For instance, the performance of RPFA [[71](#bib.bib71)]
    on the CVC-ColonDB dataset is approximately 10% lower than on the CVC-ClinicDB
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we explore the generalization ability of these SOTA models on
    the PolypGen dataset [[24](#bib.bib24)], collected from six different centers
    representing diverse populations. Consequently, validating these SOTA models on
    PolypGen enhances the comprehensiveness of the study and brings it closer to real-world
    scenarios. As shown in Table [VI](#S4.T6 "TABLE VI ‣ 4 Evaluation Metrics ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey"), compared
    to the ETIS, CVC-ColonDB, and EndoScene dataset, these SOTA models exhibit a more
    pronounced performance degradation on the PolypGen dataset. In particular, XBFormer
    [[86](#bib.bib86)] achieves 87.5% in terms of mIoU on CVC-ColinicDB while only
    36% on PolypGen-C5\. This performance drop is more substantial compared to ETIS,
    CVC-ColonDB, and EndoScene datasets. Therefore, we can easily conclude that when
    the distribution of the test dataset is inconsistent with the training data set,
    the model performance will decrease significantly, and the degree of model performance
    degradation is proportional to the degree of inconsistency.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Attr. | Description |'
  prefs: []
  type: TYPE_TB
- en: '| SI | Surgical Instruments. The endoscopic surgical procedures involve the
    positioning of instruments, such as snares, forceps, knives, and electrodes. |'
  prefs: []
  type: TYPE_TB
- en: '| IB | Indefinable Boundaries. The foreground and background areas around the
    object have similar color. |'
  prefs: []
  type: TYPE_TB
- en: '| HO | Heterogeneous Object. Object regions have distinct colors. |'
  prefs: []
  type: TYPE_TB
- en: '| GH | Ghosting. Object has anomaly RGB-colored boundary due to fast moving
    or insufficient refresh rate. |'
  prefs: []
  type: TYPE_TB
- en: '| FM | Fast-motion. The average per-frame object motion in a clip, computed
    as the Euclidean distance of polyp centroids between consecutive frames, is larger
    than 20 pixels |'
  prefs: []
  type: TYPE_TB
- en: '| SO | Small Object. The average ratio between the object size and the image
    area in a clip is smaller than 0.05. |'
  prefs: []
  type: TYPE_TB
- en: '| LO | Large Object. The average ratio between the object size and the image
    area in a clip is larger than 0.15. |'
  prefs: []
  type: TYPE_TB
- en: '| OC | Occlusion. Polyp object becomes partially or fully occluded. |'
  prefs: []
  type: TYPE_TB
- en: '| OV | Out-of-view. Polyp object is partially clipped by the image boundaries.
    |'
  prefs: []
  type: TYPE_TB
- en: '| SV | Scale-variation. The average area ratio among any pair of bounding boxes
    enclosing the target object in a clip is smaller than 0.5. |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VII: Visual attributes and descriptions of SUN-SEG dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Attribute-based Performance Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|    Method | SUN-SEG Easy | SUN-SEG Hard |'
  prefs: []
  type: TYPE_TB
- en: '| SI | IB | HO | GH | FM | SO | LO | OC | OV | SV | SI | IB | HO | GH | FM
    | SO | LO | OC | OV | SV |'
  prefs: []
  type: TYPE_TB
- en: '| UNet [[3](#bib.bib3)] | .675 | .548 | .768 | .715 | .633 | .593 | .648 |
    .670 | .643 | .620 | .618 | .619 | .663 | .676 | .713 | .689 | .633 | .658 | .659
    | .658 |'
  prefs: []
  type: TYPE_TB
- en: '| UNet++ [[180](#bib.bib180)] | .701 | .542 | .782 | .739 | .647 | .591 | .678
    | .683 | .665 | .617 | .654 | .604 | .665 | .696 | .714 | .681 | .660 | .676 |
    .677 | .678 |'
  prefs: []
  type: TYPE_TB
- en: '| COSNet [[181](#bib.bib181)] | .663 | .531 | .786 | .684 | .610 | .549 | .637
    | .648 | .613 | .617 | .641 | .593 | .727 | .668 | .690 | .637 | .694 | .707 |
    .666 | .625 |'
  prefs: []
  type: TYPE_TB
- en: '| ACSNet [[67](#bib.bib67)] | .789 | .612 | .896 | .820 | .704 | .663 | .787
    | .770 | .759 | .705 | .770 | .681 | .828 | .795 | .817 | .738 | .810 | .828 |
    .806 | .759 |'
  prefs: []
  type: TYPE_TB
- en: '| PraNet [[6](#bib.bib6)] | .745 | .585 | .821 | .772 | .673 | .611 | .722
    | .722 | .703 | .653 | .673 | .635 | .725 | .720 | .755 | .691 | .666 | .714 |
    .708 | .703 |'
  prefs: []
  type: TYPE_TB
- en: '| SANet [[34](#bib.bib34)] | .724 | .582 | .854 | .760 | .676 | .615 | .703
    | .701 | .711 | .680 | .658 | .565 | .738 | .709 | .760 | .692 | .733 | .729 |
    .727 | .693 |'
  prefs: []
  type: TYPE_TB
- en: '| MAT [[182](#bib.bib182)] | .772 | .664 | .873 | .789 | .706 | .691 | .755
    | .738 | .746 | .715 | .772 | .701 | .801 | .776 | .782 | .780 | .791 | .795 |
    .789 | .750 |'
  prefs: []
  type: TYPE_TB
- en: '| PCSA [[183](#bib.bib183)] | .676 | .563 | .759 | .708 | .628 | .610 | .634
    | .662 | .656 | .616 | .656 | .591 | .692 | .683 | .706 | .671 | .612 | .677 |
    .665 | .663 |'
  prefs: []
  type: TYPE_TB
- en: '| AMD [[184](#bib.bib184)] | .476 | .461 | .471 | .481 | .484 | .466 | .447
    | .467 | .442 | .498 | .471 | .468 | .447 | .473 | .468 | .469 | .453 | .487 |
    .462 | .481 |'
  prefs: []
  type: TYPE_TB
- en: '| DCF [[185](#bib.bib185)] | .465 | .485 | .479 | .505 | .541 | .495 | .362
    | .484 | .492 | .495 | .441 | .508 | .422 | .498 | .587 | .556 | .351 | .470 |
    .494 | .540 |'
  prefs: []
  type: TYPE_TB
- en: '| FSNet [[186](#bib.bib186)] | .719 | .603 | .810 | .752 | .694 | .632 | .686
    | .711 | .691 | .665 | .662 | .648 | .743 | .713 | .774 | .723 | .701 | .728 |
    .728 | .694 |'
  prefs: []
  type: TYPE_TB
- en: '| PNSNet [[124](#bib.bib124)] | .789 | .592 | .871 | .820 | .723 | .619 | .768
    | .749 | .751 | .705 | .746 | .631 | .803 | .780 | .778 | .743 | .805 | .790 |
    .794 | .758 |'
  prefs: []
  type: TYPE_TB
- en: '| HBNet [[187](#bib.bib187)] | .809 | .625 | .899 | .835 | .728 | .667 | .820
    | .783 | .778 | .719 | .768 | .662 | .865 | .784 | .797 | .737 | .853 | .827 |
    .808 | .765 |'
  prefs: []
  type: TYPE_TB
- en: '| PNS+ [[2](#bib.bib2)] | .819 | .667 | .883 | .844 | .738 | .690 | .796 |
    .782 | .798 | .734 | .770 | .703 | .817 | .801 | .823 | .793 | .792 | .808 | .807
    | .795 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VIII: Visual attributes-based performance comparisons on SUN-SEG-Easy/Hard
    using structure measure.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/04403e957f92d306faf322ccef75ddd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Qualitative visualization of the selected representative deep CPS
    models on SUN-SEG dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the community has observed significant progress in deep CPS models, there
    remains a lack of clarity regarding the specific conditions under which these
    models perform well. Given the multitude of factors influencing CPS algorithm
    performance, including scene category and occlusion, it becomes imperative to
    assess their effectiveness across diverse scenarios. This evaluation is essential
    for uncovering deep CPS models’ nuanced strengths and weaknesses, identifying
    unresolved challenges, and pinpointing prospective research avenues to develop
    more robust algorithms. This paper employs the recently proposed SUN-SEG dataset
    [[2](#bib.bib2)], containing 158 690 frames selected from the SUNdatabase [[188](#bib.bib188)].
    Ji et al. [[2](#bib.bib2)] divided the SUN-SEG dataset into SUN-SEG-Easy with
    119 clips (17, 070 frames) and SUN-SEG-Hard with 54 clips (12, 522 frames) according
    to difficulty levels in each pathological category, and further provide ten visual
    attributes according to the visual characteristics of images, as detailed in Table
    [VII](#S5.T7 "TABLE VII ‣ 5.2 Out-of-Distribution Generalization ‣ 5 Performance
    Benchmarking ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [VIII](#S5.T8 "TABLE VIII ‣ 5.3 Attribute-based Performance Analysis
    ‣ 5 Performance Benchmarking ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey"), we report the performance of specific attribute-based
    subsets on the SUN-SEG dataset. For clarity, we highlight the best and second
    performances using red and blue colors. Besides, as shown in Fig. [8](#S5.F8 "Figure
    8 ‣ 5.3 Attribute-based Performance Analysis ‣ 5 Performance Benchmarking ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey"), we also
    present visual results on these attribute-based scenarios. Below are some important
    observations drawn from these results.'
  prefs: []
  type: TYPE_NORMAL
- en: 1) Not winner takes all. Among the compared methods, PNS+ [[2](#bib.bib2)] stands
    out as the best from an overall perspective; however, it falls short of achieving
    optimal performance in every attribute. Similarly, while the ACSNet [[67](#bib.bib67)]
    does not secure a spot in the top three rankings, it excels in achieving the best
    performance for specific attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 2) Easy and hard scenes. Unexpectedly, methods tested on the SUN-SEG easy dataset
    achieved better performance than those tested on the SUN-SEG hard dataset regarding
    SI, HO, and GH attributes. Typically, the model’s performance on the SUN-SEG easy
    dataset should surpass that on the SUN-SEG hard dataset, indicating potential
    errors in the SUN-SEG dataset partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: 3) Easy and hard attribution. For both SUN-SEG easy and SUN-SEG hard datasets,
    attribute HO is identified as the least challenging, with HBNet [[187](#bib.bib187)]
    achieving a performance score of 0.899\. Conversely, attribute IB is considered
    the most challenging, and the PNS+’s [[2](#bib.bib2)] peak performance on attribute
    IB only reaches 0.667.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Challenges and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Undoubtedly, deep learning has substantially advanced polyp segmentation; however,
    there are numerous challenges that need to be addressed. In the following sections,
    we will explore promising research directions that we believe will help in further
    advancing polyp segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Interpretable Deep CPS Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The developments of deep neural networks have revolutionized the fields of artificial
    intelligence, and have achieved promising performance in polyp segmentation tasks.
    Nevertheless, the majority of deep models are designed without prioritizing interpretability,
    making them as black-box systems. The absence of a clear understanding of the
    underlying mechanisms behind predictions raises concerns about the trustworthiness
    of these deep models. This lack of transparency impedes their application in real-world
    clinical scenarios. For instance, why does the model identify the image as a polyp?
    What are the criteria for this decision? To ensure the secure and trustworthy
    deployment of deep models, it becomes imperative to deliver not only accurate
    predictions but also human-intelligible explanations, particularly for users in
    medical diagnosis. These factors mentioned above underscore the necessity for
    the development of novel techniques to interpret deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Federated Learning for Data Privacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Current deep learning-based models are data-hungry, which means that the greater
    the volume of training data, the higher the model’s performance. However, the
    reality is that medical data is often scattered across different hospitals, safeguarded
    by privacy restrictions. For example, data from different hospitals are isolated
    and become ”data islands.” Given the constraints in size and distribution within
    each data island, a single hospital may struggle to train a high-performance model.
    In an ideal scenario, hospitals can benefit more if we can collaboratively train
    a deep learning model on the union of their data. However, the challenge is that
    we cannot straightforwardly share data among hospitals due to data privacy and
    diverse policies. The recently emerged federated learning [[189](#bib.bib189),
    [190](#bib.bib190), [191](#bib.bib191), [192](#bib.bib192)]can handle these challenges,
    which can collaboratively train machine learning models without collecting their
    local data, making it particularly suitable for scenarios where data cannot be
    easily centralized due to privacy concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Domain Adaptation for Domain Shift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The existing deep learning-based CPS model typically assumes that the training
    dataset (source/reference domain) and the test dataset (target domain) share the
    same data distribution. Unfortunately, this assumption is overly restrictive and
    may not be true in real-world scenarios. Owing to various factors such as illumination
    and image quality, a distribution shift commonly occurs between the training and
    testing datasets that can degrade the performance drastically. As demonstrated
    in Table [V](#S4.T5 "TABLE V ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey") and Table [VI](#S4.T6 "TABLE
    VI ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey"), when the distribution of the test dataset is inconsistent
    with the training data set, these SOTA model’s performance decreased significantly.
    Thus, handling domain shift is crucial to effectively applying deep learning methods
    to medical image analysis. As a promising solution to tackle the distribution
    shift among medical image datasets, domain adaptation [[193](#bib.bib193), [194](#bib.bib194),
    [195](#bib.bib195)] has attracted increasing attention in many tasks, aiming to
    minimize the distribution gap among different but related domains.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Defending Against Adversarial Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recent research revealed that deep models are susceptible to adversarial attacks,
    which may be invisible to the human eye but can lead the model to misclassify
    the output. Szegedy et al. [[196](#bib.bib196)] was the first to demonstrate that
    high-performing deep neural networks can also fall prey to adversarial attacks.
    Su et al. [[197](#bib.bib197)] claimed successful fooling of three different network
    models on the tested images by changing only one pixel per image. Additionally,
    they observed that the average confidence of the networks in assigning incorrect
    labels was 97.47%. Such an adversarial attack poses a significant challenge to
    deploying deep CPS models in real-world clinical applications. In particular,
    the model’s accurate and reliable diagnoses are extremely important in healthcare
    because misdiagnosis may cause severe consequences, including patient mortality.
    Hence, it is extremely urgent to improve the robustness of the deep CPS model
    against various adversaries.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Weakly/Un-supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in Table [II](#S2.T2 "TABLE II ‣ 2.1.2 CNN-based Method ‣ 2.1 Network
    Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the
    Deep Learning Era: A Comprehensive Survey") and Table [III](#S2.T3 "TABLE III
    ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey)
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey"),
    existing deep CPS models are commonly trained in a fully-supervised manner, relying
    on numerous meticulously annotated pixel-level groundtruths. However, constructing
    such a well-annotated pixel-level dataset is resource-intensive and time-consuming.
    Recently, there have been some approaches to training a CPS model with weak supervision,
    such as bounding-box [[75](#bib.bib75)] and scribble level [[73](#bib.bib73)]
    annotations, but performance disparity remains compared to a fully-supervised
    model. Another noteworthy direction is self-supervised learning, which has gained
    considerable attention across various tasks. Misra et al. [[198](#bib.bib198)]
    have proved that a self-supervised model can capture intricate image details,
    facilitating the training of segmentation models. With the multitude of algorithmic
    breakthroughs witnessed in recent years, we anticipate a surge of innovation in
    this promising direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.6 Lightweight Model for Real-world Application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Advanced deep CPS models are intricately designed to enhance learning capacity
    and the model’s performance. However, there is a growing requirement for more
    innovative and lightweight architectures to meet the demands of mobile and embedded
    devices. Dollar et al. [[199](#bib.bib199)] demonstrated that simply scaling the
    model capacity significantly incurs performance degradation in terms of accuracy
    and generalization. To facilitate the practical implementation of deep CPS models
    in clinical settings, we recommend developing a lightweight model to maintain
    a good balance between performance and efficiency. Another recommended direction
    is employing model compression [[200](#bib.bib200)] or knowledge distillation
    [[201](#bib.bib201)] to condense these heavy and high-performance models, which
    can ensure minimal performance degradation and obtain a lightweight model.
  prefs: []
  type: TYPE_NORMAL
- en: 6.7 Connecting CPS with Anomaly Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Anomaly detection [[202](#bib.bib202)], also known as outlier detection, aims
    to identify patterns or instances that deviate significantly from the norm or
    expected behavior within a dataset. The goal is to identify data points that are
    considered rare, unusual, or suspicious compared to the majority of the data.
    In the polyp dataset, a significant portion of images does not contain polyps,
    with only a subset containing polyps. Thus, polyps in an image can be considered
    typical instances of anomaly detection, which means that we can segment polyps
    by using unsupervised anomaly localization techniques without large-scale pixel-wise
    annotations. Recently, some researchers have trained the polyp segmentation model
    with the help of unsupervised anomaly detection, i.e., CCD [[121](#bib.bib121)]
    and PMSACL [[84](#bib.bib84)]. We believe there is still much room for improvement
    in this direction, and the prospects for unsupervised polyp localization are promising.
  prefs: []
  type: TYPE_NORMAL
- en: 6.8 Combining CPS with Large Segmentation Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very recently, the segmentation anything model (SAM) [[203](#bib.bib203)] has
    gained massive attention due to its impressive performance in many segmentation
    tasks, which has been trained on the largest segmentation dataset with more than
    1 billion image-mask pairs, surpassing existing segmentation datasets by a factor
    of 400\. Finetuning the SAM for downstream tasks, such as camouflaged object detection
    [[204](#bib.bib204)], skin cancer segmentation [[205](#bib.bib205)] and image
    style transfer [[206](#bib.bib206)], has become a hot research area. Zhou et al.
    [[207](#bib.bib207)] show that directly applying SAM to the polyp segmentation
    cannot achieve satisfactory performance for these unseen medical images. Thus,
    one promising direction is to finetune the SAM model using training polyp datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 6.9 Combining CPS with Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Combining large language models (LLMs) and computer vision has recently become
    a hot research area, driving significant advancements in the many tasks [[208](#bib.bib208),
    [209](#bib.bib209)]. The large language models, initially created to understand
    human language, are gradually expanding to encompass visual tasks and begin to
    combine text data with visual data. This fusion of LLMs and CV leads to an era
    where AI systems can see the world, understand it, and communicate with it just
    like humans do. In this way, the deep model detects lesions and explains them
    clearly, helping doctors understand and trust the prediction results. On the other
    hand, combining LLMs and polyp segmentation could lead to better diagnosis and
    treatment by joining visual information with a wide range of medical knowledge.
    In summary, this is a promising direction and has the potential to break the current
    learning paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conlusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents a comprehensive review of deep learning-based CPS models.
    Firstly, we provide innovative taxonomy for categorizing deep CPS models from
    perspectives of network architecture, levels of supervision, and learning paradigms.
    Subsequently, we delve into contemporary literature on popular CPS datasets and
    evaluation metrics and conduct a thorough performance benchmarking of major CPS
    methods. In particular, we reveal the strengths and weaknesses of CPS datasets
    by comparing the number of datasets available, types of annotations, image resolutions,
    polyp sizes, contrast values, and polyp location. Furthermore, we evaluate the
    model’s generalization performance on out-of-distribution datasets and its attribute-based
    performance on SUN-SEG datasets, providing a nuanced understanding of the strengths
    and weaknesses of deep CPS models. Finally, we look deeper into the challenges
    of current deep learning-based CPS models, providing insightful discussions and
    several potentially promising directions. We hope our survey will help researchers
    gain a deeper understanding of the developmental history of CPS and inspire new
    works to advance this field.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] J. Bernal, J. Sánchez, and F. Vilarino, “Towards automatic polyp detection
    with a polyp appearance model,” *Pattern Recognition (PR)*, vol. 45, no. 9, pp.
    3166–3182, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] G.-P. Ji, G. Xiao, Y.-C. Chou, D.-P. Fan, K. Zhao, G. Chen, and L. Van Gool,
    “Video polyp segmentation: A deep learning perspective,” *Machine Intelligence
    Research*, vol. 19, no. 6, pp. 531–549, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2015,
    pp. 234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++: A
    nested u-net architecture for medical image segmentation,” in *Deep Learning in
    Medical Image Analysis and Multimodal Learning for Clinical Decision Support,Workshop
    (DLMIA ML-CDS)*, 2018, pp. 3–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] D. Jha, P. H. Smedsrud, M. A. Riegler, D. Johansen, T. De Lange, P. Halvorsen,
    and H. D. Johansen, “Resunet++: An advanced architecture for medical image segmentation,”
    in *Proceedings of the IEEE International Symposium on Multimedia (ISM)*, 2019,
    pp. 225–2255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, and L. Shao, “Pranet:
    Parallel reverse attention network for polyp segmentation,” in *Proceedings of
    the International Conference on Medical Image Computing and Computer-Assisted
    Intervention (MICCAI)*, 2020, pp. 263–273.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Y. Zhang, H. Liu, and Q. Hu, “Transfuse: Fusing transformers and cnns for
    medical image segmentation,” in *Proceedings of the International Conference on
    Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2021, pp.
    14–24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Wang, Q. Huang, F. Tang, J. Meng, J. Su, and S. Song, “Stepwise feature
    fusion: Local guides global,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2022,
    pp. 110–120.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] T. Ling, C. Wu, H. Yu, T. Cai, D. Wang, Y. Zhou, M. Chen, and K. Ding,
    “Probabilistic modeling ensemble vision transformer improves complex polyp segmentation,”
    in *Proceedings of the International Conference on Medical Image Computing and
    Computer-Assisted Intervention (MICCAI)*, 2023, pp. 572–581.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] A. K. Jerebko, S. Teerlink, M. Franaszek, and R. M. Summers, “Polyp segmentation
    method for ct colonography computer-aided detection,” in *SPIE Medical imaging:
    Physiology and Function: Methods, Systems, and Applications*, vol. 5031, 2003,
    pp. 359–369.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Yao, M. Miller, M. Franaszek, and R. M. Summers, “Colonic polyp segmentation
    in ct colonography-based on fuzzy clustering and deformable models,” *IEEE Transactions
    on Medical Imaging (TMI)*, vol. 23, no. 11, pp. 1344–1352, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] S. Gross, M. Kennel, T. Stehle, J. Wulff, J. Tischendorf, C. Trautwein,
    and T. Aach, “Polyp segmentation in nbi colonoscopy,” in *Bildverarbeitung für
    die Medizin 2009: Algorithmen—Systeme—Anwendungen*, 2009, pp. 252–256.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] S. Hwang and M. E. Celebi, “Polyp detection in wireless capsule endoscopy
    videos based on image segmentation and geometric feature,” in *Proceedings of
    the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2010, pp. 678–681.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] L. Lu, A. Barbu, M. Wolf, J. Liang, M. Salganicoff, and D. Comaniciu,
    “Accurate polyp segmentation for 3d ct colongraphy using multi-staged probabilistic
    binary learning and compositional model,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2008, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] M. Ganz, X. Yang, and G. Slabaugh, “Automatic segmentation of polyps in
    colonoscopic narrow-band imaging data,” *IEEE Transactions on Biomedical Engineering
    (TBE)*, vol. 59, no. 8, pp. 2144–2151, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. Bernal, J. M. Núñez, F. J. Sánchez, and F. Vilariño, “Polyp segmentation
    method in colonoscopy videos by means of msa-dova energy maps calculation,” in
    *Clinical Image-Based Procedures, Translational Research in Medical Imaging: Third
    International Workshop (CLIP)*, 2014, pp. 41–49.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Y. Yuan, D. Li, and M. Q.-H. Meng, “Automatic polyp detection via a novel
    unified bottom-up and top-down saliency approach,” *IEEE Journal of Biomedical
    and Health Informatics (JBHI)*, vol. 22, no. 4, pp. 1250–1260, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Y. Fang, C. Chen, Y. Yuan, and K.-y. Tong, “Selective feature aggregation
    network with area-boundary constraints for polyp segmentation,” in *Proceedings
    of the International Conference on Medical Image Computing and Computer-Assisted
    Intervention (MICCAI)*, 2019, pp. 302–310.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] V. S. Prasath, “Polyp detection and segmentation from video capsule endoscopy:
    A review,” *Journal of Imaging*, vol. 3, no. 1, p. 1, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] B. Taha, N. Werghi, and J. Dias, “Automatic polyp detection in endoscopy
    videos: A survey,” in *Proceedings of the International Conference on Biomedical
    Engineering (BioMed)*, 2017, pp. 233–240.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] L. F. Sanchez-Peralta, L. Bote-Curiel, A. Picon, F. M. Sanchez-Margallo,
    and J. B. Pagador, “Deep learning to find colorectal polyps in colonoscopy: A
    systematic literature review,” *Artificial Intelligence in Medicine*, vol. 108,
    p. 101923, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] K. ELKarazle, V. Raman, P. Then, and C. Chua, “Detection of colorectal
    polyps from colonoscopy using machine learning: A survey on modern techniques,”
    *Sensors*, vol. 23, no. 3, p. 1225, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J. Mei, T. Zhou, K. Huang, Y. Zhang, Y. Zhou, Y. Wu, and H. Fu, “A survey
    on deep learning for polyp segmentation: Techniques, challenges and future trends,”
    *arXiv preprint arXiv:2304.07583*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] S. Ali, D. Jha, N. Ghatwary, S. Realdon, R. Cannizzaro, O. E. Salem, D. Lamarque,
    C. Daul, M. A. Riegler, K. V. Anonsen *et al.*, “A multi-centre polyp detection
    and segmentation dataset for generalisability assessment,” *Scientific Data*,
    vol. 10, no. 1, p. 75, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] J.-H. Shi, Q. Zhang, Y.-H. Tang, and Z.-Q. Zhang, “Polyp-mixer: An efficient
    context-aware mlp-based paradigm for polyp segmentation,” *IEEE Transactions on
    Circuits and Systems for Video Technology (TCSVT)*, vol. 33, no. 1, pp. 30–42,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Chen, E. Xie, C. GE, and P. Luo, “Cyclemlp: A mlp-like architecture
    for dense prediction,” in *Proceedings of the International Conference on Learning
    Representation (ICLR)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Shen, Y. Lu, X. Jia, F. Bai, and M. Q.-H. Meng, “Task-relevant feature
    replenishment for cross-centre polyp segmentation,” in *Proceedings of the International
    Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*,
    2022, pp. 599–608.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] N. K. Tomar, D. Jha, U. Bagci, and S. Ali, “Tganet: Text-guided attention
    for improved polyp segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2022,
    pp. 151–160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] J. Wei, Y. Hu, G. Li, S. Cui, S. Kevin Zhou, and Z. Li, “Boxpolyp: Boost
    generalized polyp segmentation using extra coarse bounding box annotations,” in
    *Proceedings of the International Conference on Medical Image Computing and Computer-Assisted
    Intervention (MICCAI)*, 2022, pp. 67–77.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] X. Guo, Z. Chen, J. Liu, and Y. Yuan, “Non-equivalent images and pixels:
    Confidence-aware resampling with meta-learning mixup for polyp segmentation,”
    *Medical Image Analysis (MedIA)*, vol. 78, p. 102394, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] C. Yang, X. Guo, Z. Chen, and Y. Yuan, “Source free domain adaptation
    for medical image segmentation with fourier style mining,” *Medical Image Analysis
    (MedIA)*, vol. 79, p. 102457, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] G. Yue, W. Han, B. Jiang, T. Zhou, R. Cong, and T. Wang, “Boundary constraint
    network with cross layer feature integration for polyp segmentation,” *IEEE Journal
    of Biomedical and Health Informatics (JBHI)*, vol. 26, no. 8, pp. 4090–4099, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] X. Zhao, L. Zhang, and H. Lu, “Automatic polyp segmentation via multi-scale
    subtraction network,” in *Proceedings of the International Conference on Medical
    Image Computing and Computer Assisted Intervention (MICCAI)*, 2021, pp. 120–130.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] J. Wei, Y. Hu, R. Zhang, Z. Li, S. K. Zhou, and S. Cui, “Shallow attention
    network for polyp segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer Assisted Intervention (MICCAI)*, 2021,
    pp. 699–708.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] N. K. Tomar, D. Jha, M. A. Riegler, H. D. Johansen, D. Johansen, J. Rittscher,
    P. Halvorsen, and S. Ali, “Fanet: A feedback attention network for improved biomedical
    image segmentation,” *IEEE Transactions on Neural Networks and Learning Systems
    (TNNLS)*, vol. 34, no. 11, pp. 9375–9388, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, 2015, pp. 3431–3440.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Q. Nguyen and S.-W. Lee, “Colorectal segmentation using multiple encoder-decoder
    network in colonoscopy images,” in *Proceedings of the IEEE International Conference
    on Artificial Intelligence and Knowledge Engineering (AIKE)*, 2018, pp. 208–211.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] L. Zhang, S. Dolwani, and X. Ye, “Automated polyp segmentation in colonoscopy
    frames using fully convolutional neural network and textons,” in *Proceedings
    of the Medical Image Understanding and Analysis (MIUA)*, 2017, pp. 707–717.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Q. Li, G. Yang, Z. Chen, B. Huang, L. Chen, D. Xu, X. Zhou, S. Zhong,
    H. Zhang, and T. Wang, “Colorectal polyp segmentation using a fully convolutional
    neural network,” in *Proceedings of the IEEE International Congress on Image and
    Signal Processing, Biomedical Engineering and Informatics (CISP-BMEI)*, 2017,
    pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] P. Brandao, E. Mazomenos, G. Ciuti, R. Caliò, F. Bianchi, A. Menciassi,
    P. Dario, A. Koulaouzidis, A. Arezzo, and D. Stoyanov, “Fully convolutional neural
    networks for polyp segmentation in colonoscopy,” in *SPIE Medical Imaging: Computer-Aided
    Diagnosis*, vol. 10134, 2017, pp. 101–107.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Y. Su, Q. Xie, J. Ye, J. He, and J. Cheng, “An accurate polyp segmentation
    framework via feature secondary fusion,” in *Proceedings of the International
    Symposium on Biomedical Imaging (ISBI)*, 2023, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Z. Yin, K. Liang, Z. Ma, and J. Guo, “Duplex contextual relation network
    for polyp segmentation,” in *Proceedings of the IEEE International Symposium on
    Biomedical Imaging (ISBI)*, 2022, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Z. Xu, D. Qiu, S. Lin, X. Zhang, S. Shi, S. Zhu, F. Zhang, and X. Wan,
    “Temporal correlation network for video polyp segmentation,” in *Proceedings of
    the IEEE International Conference on Bioinformatics and Biomedicine (BIBM)*, 2022,
    pp. 1317–1322.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] R. Feng, B. Lei, W. Wang, T. Chen, J. Chen, D. Z. Chen, and J. Wu, “Ssn:
    A stair-shape network for real-time polyp segmentation in colonoscopy images,”
    in *Proceedings of the IEEE 17th International Symposium on Biomedical Imaging
    (ISBI)*, 2020, pp. 225–229.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] M. Akbari, M. Mohrekesh, E. Nasr-Esfahani, S. R. Soroushmehr, N. Karimi,
    S. Samavi, and K. Najarian, “Polyp segmentation in colonoscopy images using fully
    convolutional network,” in *Proceedings of the IEEE Engineering in Medicine and
    Biology Society (EMBC)*, 2018, pp. 69–72.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] I. Wichakam, T. Panboonyuen, C. Udomcharoenchaikit, and P. Vateekul, “Real-time
    polyps segmentation for colonoscopy video frames using compressed fully convolutional
    network,” in *Proceedings of the International Conference on Multimedia Modeling
    (MMM)*, 2018, pp. 393–404.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] H. Wu, J. Zhong, W. Wang, Z. Wen, and J. Qin, “Precise yet efficient semantic
    calibration and refinement in convnets for real-time polyp segmentation from colonoscopy
    videos,” in *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*,
    vol. 35, no. 4, 2021, pp. 2916–2924.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] C. Dong, Q. Zhao, K. Chen, and X. Huang, “Asymmetric attention upsampling:
    Rethinking upsampling for biological image segmentation,” in *Proceedings of the
    IEEE 18th International Symposium on Biomedical Imaging (ISBI)*, 2021, pp. 645–649.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] R. Zhang, P. Lai, X. Wan, D.-J. Fan, F. Gao, X.-J. Wu, and G. Li, “Lesion-aware
    dynamic kernel for polyp segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2022,
    pp. 99–109.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] X. Zhao, Z. Wu, S. Tan, D.-J. Fan, Z. Li, X. Wan, and G. Li, “Semi-supervised
    spatial temporal attention network for video polyp segmentation,” in *Proceedings
    of the International Conference on Medical Image Computing and Computer-Assisted
    Intervention (MICCAI)*, 2022, pp. 456–466.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] A. Srivastava, D. Jha, S. Chanda, U. Pal, H. D. Johansen, D. Johansen,
    M. A. Riegler, S. Ali, and P. Halvorsen, “Msrf-net: a multi-scale residual fusion
    network for biomedical image segmentation,” *IEEE Journal of Biomedical and Health
    Informatics (JBHI)*, vol. 26, no. 5, pp. 2252–2263, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Y. Lin, J. Wu, G. Xiao, J. Guo, G. Chen, and J. Ma, “Bsca-net: Bit slicing
    context attention network for polyp segmentation,” *Pattern Recognition (PR)*,
    vol. 132, p. 108917, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] P. Song, J. Li, and H. Fan, “Attention based multi-scale parallel network
    for polyp segmentation,” *Computers in Biology and Medicine (CBM)*, vol. 146,
    p. 105476, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] X. Li, J. Xu, Y. Zhang, R. Feng, R.-W. Zhao, T. Zhang, X. Lu, and S. Gao,
    “Tccnet: Temporally consistent context-free network for semi-supervised video
    polyp segmentation,” in *Proceedings of the International Joint Conference on
    Artificial Intelligence (IJCAI)*, 2022, pp. 1109–1115.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] T. D. Huy, H. C. Huyen, C. D. Nguyen, S. T. Duong, T. Bui, and S. Q. Truong,
    “Adversarial contrastive fourier domain adaptation for polyp segmentation,” in
    *Proceedings of the IEEE International Symposium on Biomedical Imaging (ISBI)*,
    2022, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Z. Qiu, Z. Wang, M. Zhang, Z. Xu, J. Fan, and L. Xu, “Bdg-net: boundary
    distribution guided network for accurate polyp segmentation,” in *SPIE Medical
    Imaging: Image Processing*, vol. 12032, 2022, pp. 792–799.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] M. Chen, X. Li, J. Xu, R. Yuan, Y. Zhang, R. Feng, T. Zhang, and S. Gao,
    “Single-modality endoscopic polyp segmentation via random color reversal synthesis
    and two-branched learning,” in *Proceedings of the IEEE International Conference
    on Bioinformatics and Biomedicine (BIBM)*, 2022, pp. 1501–1504.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] N. K. Tomar, D. Jha, S. Ali, H. D. Johansen, D. Johansen, M. A. Riegler,
    and P. Halvorsen, “Ddanet: Dual decoder attention network for automatic polyp
    segmentation,” in *Proceedings of the International Conference on Pattern Recognition
    Workshops (ICPRW)*, 2021, pp. 307–314.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] C. Wu, C. Long, S. Li, J. Yang, F. Jiang, and R. Zhou, “Msraformer: Multiscale
    spatial reverse attention network for polyp segmentation,” *Computers in Biology
    and Medicine (CBM)*, vol. 151, p. 106274, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Q. Jin, H. Hou, G. Zhang, and Z. Li, “Fegnet: A feedback enhancement gate
    network for automatic polyp segmentation,” *IEEE Journal of Biomedical and Health
    Informatics (JBHI)*, vol. 27, no. 7, pp. 3420–3430, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Y. Huang, D. Tan, Y. Zhang, X. Li, and K. Hu, “Transmixer: A hybrid transformer
    and cnn architecture for polyp segmentation,” in *Proceedings of the IEEE International
    Conference on Bioinformatics and Biomedicine (BIBM)*, 2022, pp. 1558–1561.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] K. B. Patel, F. Li, and G. Wang, “Fuzzynet: A fuzzy attention module for
    polyp segmentation,” in *Proceedings of the Conference on Neural Information Processing
    Systems Workshop (NeurIPS)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] X. Du, X. Xu, and K. Ma, “Icgnet: Integration context-based reverse-contour
    guidance network for polyp segmentation,” in *Proceedings of the International
    Joint Conferences on Artificial Intelligence (IJCAI)*, 2022, pp. 877–883.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] T.-C. Nguyen, T.-P. Nguyen, G.-H. Diep, A.-H. Tran-Dinh, T. V. Nguyen,
    and M.-T. Tran, “Ccbanet: cascading context and balancing attention for polyp
    segmentation,” in *Proceedings of the International Conference on Medical Image
    Computing and Computer Assisted Intervention (MICCAI)*, 2021, pp. 633–643.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Y. Shen, X. Jia, and M. Q.-H. Meng, “Hrenet: A hard region enhancement
    network for polyp segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer Assisted Intervention (MICCAI)*, 2021,
    pp. 559–568.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] T. Kim, H. Lee, and D. Kim, “Uacanet: Uncertainty augmented context attention
    for polyp segmentation,” in *Proceedings of the 29th ACM International Conference
    on Multimedia (ACM MM)*, 2021, pp. 2167–2175.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] R. Zhang, G. Li, Z. Li, S. Cui, D. Qian, and Y. Yu, “Adaptive context
    selection for polyp segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2020,
    pp. 253–262.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and
    L. Shao, “Pvt v2: Improved baselines with pyramid vision transformer,” *Computational
    Visual Media (CVM)*, vol. 8, no. 3, pp. 415–424, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] J. Bernal, F. J. Sánchez, G. Fernández-Esparrach, D. Gil, C. Rodríguez,
    and F. Vilariño, “Wm-dova maps for accurate polyp highlighting in colonoscopy:
    Validation vs. saliency maps from physicians,” *Computerized Medical Imaging and
    Graphics*, vol. 43, pp. 99–111, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. de Lange, D. Johansen,
    and H. D. Johansen, “Kvasir-seg: A segmented polyp dataset,” in *Proceedings of
    the International Conference on Multimedia Modeling (MMM)*, 2020, pp. 451–462.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Y. Su, Y. Shen, J. Ye, J. He, and J. Cheng, “Revisiting feature propagation
    and aggregation in polyp segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2023,
    pp. 632–641.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and
    L. Shao, “Pyramid vision transformer: A versatile backbone for dense prediction
    without convolutions,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision (ICCV)*, 2021, pp. 568–578.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] A. Wang, M. Xu, Y. Zhang, M. Islam, and H. Ren, “S2me: Spatial-spectral
    mutual teaching and ensemble learning for scribble-supervised polyp segmentation,”
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] J. Wei, Y. Hu, S. Cui, S. K. Zhou, and Z. Li, “Weakpolyp: You only look
    bounding box for polyp segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2023,
    pp. 757–766.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] S.-H. Gao, M.-M. Cheng, K. Zhao, X.-Y. Zhang, M.-H. Yang, and P. Torr,
    “Res2net: A new multi-scale backbone architecture,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence (TPAMI)*, vol. 43, no. 2, pp. 652–662, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] M.-H. Guo, C.-Z. Lu, Z.-N. Liu, M.-M. Cheng, and S.-M. Hu, “Visual attention
    network,” *Computational Visual Media (CVM)*, vol. 9, no. 4, pp. 733–752, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] M. Haithami, A. Ahmed, I. Y. Liao, and H. Jalab, “Enhancing polyp segmentation
    generalizability by minimizing images’ total variation,” in *Proceedings of the
    International Symposium on Biomedical Imaging (ISBI)*, 2023, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Y. Su, C. Deng, Z. Deng, J. Ye, J. He, and J. Cheng, “Go to the right:
    A real-time and accurate polyp segmentation model for practical use,” in *Proceedings
    of the International Symposium on Biomedical Imaging (ISBI)*, 2023, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] X. Xiong, S. Li, and G. Li, “Unpaired image-to-image translation based
    domain adaptation for polyp segmentation,” in *Proceedings of the International
    Symposium on Biomedical Imaging (ISBI)*, 2023, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] L. F. Snchez-Peralta, J. B. Pagador, A. Picón, F. Caldern, N. Andraka,
    R. Bilbao, B. Glover, C. L. Saratxaga, and F. M. Snchez-Margallo, “Piccolo white-light
    and narrow-band imaging colonoscopic dataset: A performance comparative of models
    and datasets,” *Applied Sciences*, vol. 10, no. 23, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] M. Wang, X. An, Z. Pei, N. Li, L. Zhang, G. Liu, and D. Ming, “An efficient
    multi-task synergetic network for polyp segmentation and classification,” *IEEE
    Journal of Biomedical and Health Informatics (JBHI)*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] H. Mazumdar, C. Chakraborty, M. Sathvik, P. Jayakumar, and A. Kaushik,
    “Optimizing pix2pix gan with attention mechanisms for ai-driven polyp segmentation
    in iomt-enabled smart healthcare,” *IEEE Journal of Biomedical and Health Informatics
    (JBHI)*, pp. 1–8, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Y. Tian, F. Liu, G. Pang, Y. Chen, Y. Liu, J. W. Verjans, R. Singh, and
    G. Carneiro, “Self-supervised pseudo multi-class pre-training for unsupervised
    anomaly detection and segmentation in medical images,” *Medical Image Analysis
    (MedIA)*, vol. 90, p. 102930, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] H. Borgli, V. Thambawita, P. H. Smedsrud, S. Hicks, D. Jha, S. L. Eskeland,
    K. R. Randel, K. Pogorelov, M. Lux, D. T. D. Nguyen, D. Johansen, C. Griwodz,
    H. K. Stensland, E. Garcia-Ceja, P. T. Schmidt, H. L. Hammer, M. A. Riegler, P. Halvorsen,
    and T. de Lange, “HyperKvasir, a comprehensive multi-class image and video dataset
    for gastrointestinal endoscopy,” *Scientific Data*, vol. 7, no. 1, p. 283, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] J. Wang, F. Chen, Y. Ma, L. Wang, Z. Fei, J. Shuai, X. Tang, Q. Zhou,
    and J. Qin, “Xbound-former: Toward cross-scale boundary modeling in transformers,”
    *IEEE Transactions on Medical Imaging (TMI)*, vol. 42, no. 6, pp. 1735–1745, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] S. Jain, R. Atale, A. Gupta, U. Mishra, A. Seal, A. Ojha, J. Kuncewicz,
    and O. Krejcar, “Coinnet: A convolution-involution network with a novel statistical
    attention for automatic polyp segmentation,” *IEEE Transactions on Medical Imaging
    (TMI)*, vol. 42, no. 12, pp. 3987–4000, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, 2017, pp. 4700–4708.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] J. Wang and C. Chen, “Unsupervised adaptation of polyp segmentation models
    via coarse-to-fine self-supervision,” in *Proceedings of the International Conference
    on Information Processing in Medical Imaging (IPMI)*, 2023, pp. 250–262.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] D. Jha, N. K. Tomar, V. Sharma, and U. Bagci, “Transnetr: Transformer-based
    residual network for polyp segmentation with multi-center out-of-distribution
    testing,” in *Medical Imaging with Deep Learning (MIDL)*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] T. Zhou, Y. Zhou, K. He, C. Gong, J. Yang, H. Fu, and D. Shen, “Cross-level
    feature aggregation network for polyp segmentation,” *Pattern Recognition (PR)*,
    vol. 140, p. 109555, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] K. Wang, X. Zhang, Y. Lu, W. Zhang, S. Huang, and D. Yang, “Gsal: Geometric
    structure adversarial learning for robust medical image segmentation,” *Pattern
    Recognition (PR)*, vol. 140, p. 109596, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *Proceedings of the International Conference on Learning
    Representations (ICLR)*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] A. Wang, M. Wu, H. Qi, H. Shi, J. Chen, Y. Chen, and X. Luo, “Pyramid
    transformer driven multibranch fusion for polyp segmentation in colonoscopic video
    images,” in *Proceedings of the IEEE International Conference on Image Processing
    (ICIP)*, 2023, pp. 2350–2354.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] E. Moreu, E. Arazo, K. McGuinness, and N. E. O’Connor, “Self-supervised
    and semi-supervised polyp segmentation using synthetic data,” in *Proceedings
    of the IEEE International Joint Conference on Neural Networks (IJCNN)*, 2023,
    pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] P. Chao, C.-Y. Kao, Y.-S. Ruan, C.-H. Huang, and Y.-L. Lin, “Hardnet:
    A low memory traffic network,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision (ICCV)*, 2019, pp. 3552–3561.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] H. Wu, Z. Zhao, J. Zhong, W. Wang, Z. Wen, and J. Qin, “Polypseg+: A lightweight
    context-aware network for real-time polyp segmentation,” *IEEE Transactions on
    Cybernetics*, vol. 53, no. 4, pp. 2610–2621, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] M. M. Rahman and R. Marculescu, “Medical image segmentation via cascaded
    attention decoding,” in *Proceedings of the IEEE/CVF Winter Conference on Applications
    of Computer Vision (WACV)*, 2023, pp. 6222–6231.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng,
    I. Cetin, K. Lekadir, O. Camara, M. A. G. Ballester *et al.*, “Deep learning techniques
    for automatic mri cardiac multi-structures segmentation and diagnosis: is the
    problem solved?” *IEEE Transactions on Medical Imaging (TMI)*, vol. 37, no. 11,
    pp. 2514–2525, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] D. Bo, W. Wenhai, F. Deng-Ping, L. Jinpeng, F. Huazhu, and S. Ling, “Polyp-pvt:
    Polyp segmentation with pyramid vision transformers,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] T.-H. Nguyen-Mau, Q.-H. Trinh, N.-T. Bui, P.-T. V. Thi, M.-V. Nguyen,
    X.-N. Cao, M.-T. Tran, and H.-D. Nguyen, “Pefnet: Positional embedding feature
    for polyp segmentation,” in *Proceedings of the International Conference on Multimedia
    Modeling (MMM)*, 2023, pp. 240–251.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] M. Tan and Q. Le, “Efficientnetv2: Smaller models and faster training,”
    in *Proceedings of the International Conference on Machine Learning (ICML)*, 2021,
    pp. 10 096–10 106.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] J. Silva, A. Histace, O. Romain, X. Dray, and B. Granado, “Toward embedded
    detection of polyps in wce images for early diagnosis of colorectal cancer,” *International
    Journal of Computer Assisted Radiology and Surgery*, vol. 9, pp. 283–293, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] N. Tajbakhsh, S. R. Gurudu, and J. Liang, “Automated polyp detection
    in colonoscopy videos using shape and context information,” *IEEE Transactions
    on Medical Imaging (TMI)*, vol. 35, no. 2, pp. 630–644, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] L. Cai, M. Wu, L. Chen, W. Bai, M. Yang, S. Lyu, and Q. Zhao, “Using
    guided self-attention with local information for polyp segmentation,” in *Proceedings
    of the International Conference on Medical Image Computing and Computer-Assisted
    Intervention (MICCAI)*, 2022, pp. 629–638.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang, “Cvt:
    Introducing convolutions to vision transformers,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision (ICCV)*, 2021, pp. 22–31.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Y. Ma, X. Chen, K. Cheng, Y. Li, and B. Sun, “Ldpolypvideo benchmark:
    a large-scale colonoscopy video dataset of diverse polyps,” in *Proceedings of
    the International Conference on Medical Image Computing and Computer Assisted
    Intervention (MICCAI)*, 2021, pp. 387–396.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] D. Vázquez, J. Bernal, F. J. Sánchez, G. Fernández-Esparrach, A. M. López,
    A. Romero, M. Drozdzal, A. Courville *et al.*, “A benchmark for endoluminal scene
    segmentation of colonoscopy images,” *Journal of Healthcare Engineering*, vol.
    2017, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2018,
    pp. 7132–7141.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] E. Sanderson and B. J. Matuszewski, “Fcn-transformer feature fusion for
    polyp segmentation,” in *Proceedings of the Conference on Medical Image Understanding
    and Analysis (MIUA)*, 2022, pp. 892–907.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for convolutional
    neural networks,” in *Proceedings of the International Conference on Machine Learning
    (ICML)*, 2019, pp. 6105–6114.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Y. Xiao, Z. Chen, L. Wan, L. Yu, and L. Zhu, “Icbnet: Iterative context-boundary
    feedback network for polyp segmentation,” in *Proceedings of the IEEE International
    Conference on Bioinformatics and Biomedicine (BIBM)*, 2022, pp. 1297–1304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] R. Chen, X. Wang, B. Jin, J. Tu, F. Zhu, and Y. Li, “Cld-net: Complement
    local detail for medical small-object segmentation,” in *Proceedings of the IEEE
    International Conference on Bioinformatics and Biomedicine (BIBM)*, 2022, pp.
    942–947.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] H. Du, J. Wang, M. Liu, Y. Wang, and E. Meijering, “Swinpa-net: Swin
    transformer-based multiscale feature pyramid aggregation network for medical image
    segmentation,” *IEEE Transactions on Neural Networks and Learning Systems (TNNLS)*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,
    “Swin transformer: Hierarchical vision transformer using shifted windows,” in
    *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*,
    2021, pp. 10 012–10 022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] W. Zhang, C. Fu, Y. Zheng, F. Zhang, Y. Zhao, and C.-W. Sham, “Hsnet:
    A hybrid semantic network for polyp segmentation,” *Computers in Biology and Medicine
    (CBM)*, vol. 150, p. 106173, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] F. Liu, Z. Hua, J. Li, and L. Fan, “Dbmf: Dual branch multiscale feature
    fusion network for polyp segmentation,” *Computers in Biology and Medicine (CBM)*,
    vol. 151, p. 106304, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] L. F. Sánchez-Peralta, J. B. Pagador, A. Picón, Á. J. Calderón, F. Polo,
    N. Andraka, R. Bilbao, B. Glover, C. L. Saratxaga, and F. M. Sánchez-Margallo,
    “Piccolo white-light and narrow-band imaging colonoscopic dataset: A performance
    comparative of models and datasets,” *Applied Sciences*, vol. 10, no. 23, p. 8501,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] M. Cheng, Z. Kong, G. Song, Y. Tian, Y. Liang, and J. Chen, “Learnable
    oriented-derivative network for polyp segmentation,” in *Proceedings of the International
    Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)*.   Springer,
    2021, pp. 720–730.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,
    “Training data-efficient image transformers & distillation through attention,”
    in *Proceedings of the International Conference on Machine Learning (ICML)*, 2021,
    pp. 10 347–10 357.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Y. Tian, G. Pang, F. Liu, Y. Chen, S. H. Shin, J. W. Verjans, R. Singh,
    and G. Carneiro, “Constrained contrastive distribution learning for unsupervised
    anomaly detection and localisation in medical images,” in *Proceedings of the
    International Conference on Medical Image Computing and Computer Assisted Intervention
    (MICCAI)*, 2021, pp. 128–140.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] S. Li, X. Sui, J. Fu, H. Fu, X. Luo, Y. Feng, X. Xu, Y. Liu, D. S. Ting,
    and R. S. M. Goh, “Few-shot domain adaptation with polymorphic transformers,”
    in *Proceedings of the International Conference on Medical Image Computing and
    Computer Assisted Intervention (MICCAI)*, 2021, pp. 330–340.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] S. Li, X. Sui, X. Luo, X. Xu, Y. Liu, and R. Goh, “Medical image segmentation
    using squeeze-and-expansion transformers,” *Proceedings of the International Joint
    Conference on Artificial Intelligence (IJCAI)*, vol. 576, p. 576.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] G.-P. Ji, Y.-C. Chou, D.-P. Fan, G. Chen, H. Fu, D. Jha, and L. Shao,
    “Progressively normalized self-attention network for video polyp segmentation,”
    in *Proceedings of the International Conference on Medical Image Computing and
    Computer-Assisted Intervention (MICCAI)*, 2021, pp. 142–152.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] X. Guo, C. Yang, and Y. Yuan, “Dynamic-weighting hierarchical segmentation
    network for medical images,” *Medical Image Analysis (MedIA)*, vol. 73, p. 102196,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous
    convolution for semantic image segmentation,” *arXiv preprint arXiv:1706.05587*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] X. Guo, C. Yang, Y. Liu, and Y. Yuan, “Learn to threshold: Thresholdnet
    with confidence-guided manifold mixup for polyp segmentation,” *IEEE Transactions
    on Medical Imaging (TMI)*, vol. 40, no. 4, pp. 1134–1146, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] C. Yang, X. Guo, M. Zhu, B. Ibragimov, and Y. Yuan, “Mutual-prototype
    adaptation for cross-domain polyp segmentation,” *IEEE Journal of Biomedical and
    Health Informatics (JBHI)*, vol. 25, no. 10, pp. 3886–3897, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] D. Jha, P. H. Smedsrud, D. Johansen, T. de Lange, H. D. Johansen, P. Halvorsen,
    and M. A. Riegler, “A comprehensive study on colorectal polyp segmentation with
    resunet++, conditional random field and test-time augmentation,” *IEEE Journal
    of Biomedical and Health Informatics (JBHI)*, vol. 25, no. 6, pp. 2029–2040, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Z. Zhang, Q. Liu, and Y. Wang, “Road extraction by deep residual u-net,”
    *IEEE Transactions on Geoscience and Remote Sensing (TGRS)*, vol. 15, no. 5, pp.
    749–753, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2017, pp. 2117–2125.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] H. Wu, G. Chen, Z. Wen, and J. Qin, “Collaborative and adversarial learning
    of focused and dispersive representations for semi-supervised polyp segmentation,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*,
    2021, pp. 3489–3498.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] S. Safarov and T. K. Whangbo, “A-denseunet: Adaptive densely connected
    unet for polyp segmentation in colonoscopy images with atrous convolution,” *Sensors*,
    vol. 21, no. 4, p. 1441, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] K. Patel, A. M. Bur, and G. Wang, “Enhanced u-net: A feature enhancement
    network for polyp segmentation,” in *Proceedings of the IEEE 18th Conference on
    Robots and Vision (CRV)*, 2021, pp. 181–188.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] M. Yeung, E. Sala, C.-B. Schönlieb, and L. Rundo, “Focus u-net: A novel
    dual attention-gated cnn for polyp segmentation during colonoscopy,” *Computers
    in Biology and Medicine (CBM)*, vol. 137, p. 104815, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] S.-T. Tran, C.-H. Cheng, T.-T. Nguyen, M.-H. Le, and D.-G. Liu, “Tmd-unet:
    Triple-unet with multi-scale input features and dense skip connection for medical
    image segmentation,” in *Healthcare*, vol. 9, no. 1, 2021, p. 54.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] V. Thambawita, S. A. Hicks, P. Halvorsen, and M. A. Riegler, “Divergentnets:
    Medical image segmentation by network ensemble,” in *Proceedings of the IEEE International
    Symposium on Biomedical Imaging Workshops (ISBIW)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Y. Meng, H. Zhang, D. Gao, Y. Zhao, X. Yang, X. Qian, X. Huang, Y. Zheng,
    A. Remark, and U. London, “Bi-gcn: Boundary-aware input-dependent graph convolution
    network for biomedical image segmentation,” *Proceedings of the British Machine
    Vision Conference (BMVC)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] X. Xie, J. Chen, Y. Li, L. Shen, K. Ma, and Y. Zheng, “Mi2gan: Generative
    adversarial network for medical image domain adaptation using mutual information
    constraint,” in *Proceedings of the International Conference on Medical Image
    Computing and Computer-Assisted Intervention (MICCAI)*, 2020, pp. 516–525.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” in *Proceedings of the
    IEEE International Conference on Computer Vision (ICCV)*, 2017, pp. 2223–2232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] J. Zhong, W. Wang, H. Wu, Z. Wen, and J. Qin, “Polypseg: An efficient
    context-aware network for polyp segmentation from colonoscopy videos,” in *Proceedings
    of the International Conference on Medical Image Computing and Computer-Assisted
    Intervention (MICCAI)*, 2020, pp. 285–294.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Y. Fang, D. Zhu, J. Yao, Y. Yuan, and K.-Y. Tong, “Abc-net: Area-boundary
    constraint network with dynamical feature selection for colorectal polyp segmentation,”
    *IEEE Sensors Journal*, vol. 21, no. 10, pp. 11 799–11 809, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
    transformations for deep neural networks,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2017, pp. 1492–1500.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] K. Wickstrøm, M. Kampffmeyer, and R. Jenssen, “Uncertainty and interpretability
    in convolutional neural networks for semantic segmentation of colorectal polyps,”
    *Medical Image Analysis (MedIA)*, vol. 60, p. 101619, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] S. Wang, Y. Cong, H. Zhu, X. Chen, L. Qu, H. Fan, Q. Zhang, and M. Liu,
    “Multi-scale context-guided deep network for automated lesion segmentation with
    endoscopy images of gastrointestinal tract,” *IEEE Journal of Biomedical and Health
    Informatics (JBHI)*, vol. 25, no. 2, pp. 514–525, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Z. Huang, Z. Wang, J. Chen, Z. Zhu, and J. Li, “Real-time colonoscopy
    image segmentation based on ensemble knowledge distillation,” in *Proceedings
    of the IEEE International Conference on Advanced Robotics and Mechatronics (ICARM)*,
    2020, pp. 454–459.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] D. Jha, M. A. Riegler, D. Johansen, P. Halvorsen, and H. D. Johansen,
    “Doubleu-net: A deep convolutional neural network for medical image segmentation,”
    in *Proceedings of the IEEE International Symposium on Computer-based Medical
    Systems (CBMS)*, 2020, pp. 558–564.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] V. de Almeida Thomaz, C. A. Sierra-Franco, and A. B. Raposo, “Training
    data enhancements for robust polyp segmentation in colonoscopy images,” in *Proceedings
    of the IEEE International Symposium on Computer-Based Medical Systems (CBMS)*,
    2019, pp. 192–197.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] B. Murugesan, K. Sarveswaran, S. M. Shankaranarayana, K. Ram, J. Joseph,
    and M. Sivaprakasam, “Psi-net: Shape and boundary aware joint multi-task deep
    network for medical image segmentation,” in *Proceedings of the IEEE Engineering
    in Medicine and Biology Society (EMBC)*, 2019, pp. 7223–7226.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] J. Poorneshwaran, S. S. Kumar, K. Ram, J. Joseph, and M. Sivaprakasam,
    “Polyp segmentation using generative adversarial network,” in *Proceedings of
    the IEEE Engineering in Medicine and Biology Society (EMBC)*, 2019, pp. 7201–7204.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] X. Sun, P. Zhang, D. Wang, Y. Cao, and B. Liu, “Colorectal polyp segmentation
    by u-net with dilation convolution,” in *Proceedings of the IEEE International
    Conference on Machine Learning and Applications (ICMLA)*, 2019, pp. 851–858.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] J. Bernal, N. Tajkbaksh, F. J. Sanchez, B. J. Matuszewski, H. Chen, L. Yu,
    Q. Angermann, O. Romain, B. Rustad, I. Balasingham *et al.*, “Comparative validation
    of polyp detection methods in video colonoscopy: results from the miccai 2015
    endoscopic vision challenge,” *IEEE Transactions on Medical Imaging*, vol. 36,
    no. 6, pp. 1231–1249, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] H. A. Qadir, Y. Shin, J. Solhusvik, J. Bergsland, L. Aabakken, and I. Balasingham,
    “Polyp detection and segmentation using mask r-cnn: Does a deeper feature extractor
    cnn always perform better?” in *Proceedings of the IEEE International Symposium
    on Medical Information and Communication Technology (ISMICT)*, 2019, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in *Proceedings
    of the IEEE International Conference on Computer Vision (ICCV)*, 2017, pp. 2961–2969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Y. B. Guo and B. Matuszewski, “Giana polyp segmentation with fully convolutional
    dilation neural networks,” in *Proceedings of the International Joint Conference
    on Computer Vision, Imaging and Computer Graphics Theory and Applications*, 2019,
    pp. 632–641.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] M. Bagheri, M. Mohrekesh, M. Tehrani, K. Najarian, N. Karimi, S. Samavi,
    and S. R. Soroushmehr, “Deep neural network based polyp segmentation in colonoscopy
    images using a combination of color spaces,” in *Proceedings of the IEEE Engineering
    in Medicine and Biology Society (EMBC)*, 2019, pp. 6742–6745.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] A. Chaurasia and E. Culurciello, “Linknet: Exploiting encoder representations
    for efficient semantic segmentation,” in *Proceedings of the IEEE Visual Communications
    and Image Processing (VCIP)*, 2017, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] X. Guo, N. Zhang, J. Guo, H. Zhang, Y. Hao, and J. Hang, “Automated polyp
    segmentation for colonoscopy images: A method based on convolutional neural networks
    and ensemble learning,” *Medical Physics*, vol. 46, no. 12, pp. 5666–5676, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] A. Sánchez-González, B. García-Zapirain, D. Sierra-Sosa, and A. Elmaghraby,
    “Automatized colon polyp segmentation via contour region analysis,” *Computers
    in Biology and Medicine (CBM)*, vol. 100, pp. 152–164, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] K. Wickstrøm, M. Kampffmeyer, and R. Jenssen, “Uncertainty modeling and
    interpretability in convolutional neural networks for polyp segmentation,” in
    *Proceedings of the IEEE International Workshop on Machine Learning for Signal
    Processing (MLSP)*, 2018, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional
    encoder-decoder architecture for image segmentation,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence (TPAMI)*, vol. 39, no. 12, pp. 2481–2495, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] O. H. Maghsoudi, “Superpixel based segmentation and classification of
    polyps in wireless capsule endoscopy,” in *Proceedings of the IEEE Signal Processing
    in Medicine and Biology Symposium (SPMB)*, 2017, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Y. Jia, “Polyps auto-detection in wireless capsule endoscopy images using
    improved method based on image segmentation,” in *Proceedings of the IEEE International
    Conference on Robotics and Biomimetics (ROBIO)*, 2015, pp. 1631–1636.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Proceedings of the
    Advances in Neural Information Processing Systems (NeurIPS)*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” *arXiv preprint arXiv:1409.0473*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] P. Ngoc Lan, N. S. An, D. V. Hang, D. V. Long, T. Q. Trung, N. T. Thuy,
    and D. V. Sang, “Neounet: Towards accurate colon polyp segmentation and neoplasm
    detection,” in *Advances in Visual Computing: 16th International Symposium (ISVC)*,
    2021, pp. 15–28.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] L. Yao, F. He, X. Wang, L. Zhou, H. Peng, and X. Huang, “Scheme and dataset
    for evaluating computer-aided polyp detection system in colonoscopy,” in *Proceedings
    of the IEEE International Symposium on Biomedical Imaging (ISBI)*, 2022, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2017, pp. 2881–2890.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk, “Frequency-tuned
    salient region detection,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2009, pp. 1597–1604.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] F. Perazzi, P. Krähenbühl, Y. Pritch, and A. Hornung, “Saliency filters:
    Contrast based filtering for salient region detection,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2012, pp.
    733–740.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] M. Ran, Z.-M. Lihi, and T. Ayellet, “How to evaluate foreground maps?”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2014, pp. 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] D.-P. Fan, M.-M. Cheng, Y. Liu, T. Li, and A. Borji, “Structure-measure:
    A new way to evaluate foreground maps.” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2017, pp. 4548–4557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] D.-P. Fan, C. Gong, Y. Cao, B. Ren, M.-M. Cheng, and A. Borji, “Enhanced-alignment
    measure for binary foreground map evaluation,” in *Proceedings of the 27th International
    Joint Conference on Artificial Intelligence (IJCAI)*, 2018, pp. 698–704.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] X. Zhao, H. Jia, Y. Pang, L. Lv, F. Tian, L. Zhang, W. Sun, and H. Lu,
    “M2snet: Multi-scale in multi-scale subtraction network for medical image segmentation,”
    *arXiv preprint arXiv:2303.10894*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Y. Jin, Y. Hu, Z. Jiang, and Q. Zheng, “Polyp segmentation with convolutional
    mlp,” *The Visual Computer*, vol. 39, no. 10, pp. 4819–4837, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] J. Song, X. Chen, Q. Zhu, F. Shi, D. Xiang, Z. Chen, Y. Fan, L. Pan,
    and W. Zhu, “Global and local feature reconstruction for medical image segmentation,”
    *IEEE Transactions on Medical Imaging (TMI)*, vol. 41, no. 9, pp. 2273–2284, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] A. Lou, S. Guan, H. Ko, and M. H. Loew, “Caranet: Context axial reverse
    attention network for segmentation of small medical objects,” in *SPIE Medical
    Imaging: Image Processing*, vol. 12032, 2022, pp. 81–92.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] M. Wang, X. An, Y. Li, N. Li, W. Hang, and G. Liu, “Ems-net: Enhanced
    multi-scale network for polyp segmentation,” in *Proceedings of the IEEE Engineering
    in Medicine and Biology Society (EMBC)*, 2021, pp. 2936–2939.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] C.-H. Huang, H.-Y. Wu, and Y.-L. Lin, “Hardnet-mseg: A simple encoder-decoder
    polyp segmentation neural network that achieves over 0.9 mean dice and 86 fps,”
    *arXiv e-prints*, pp. arXiv–2101, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++: Redesigning
    skip connections to exploit multiscale features in image segmentation,” *IEEE
    Transactions on Medical Imaging (TMI)*, vol. 39, no. 6, pp. 1856–1867, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] X. Lu, W. Wang, C. Ma, J. Shen, L. Shao, and F. Porikli, “See more, know
    more: Unsupervised video object segmentation with co-attention siamese networks,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2019, pp. 3623–3632.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] T. Zhou, J. Li, S. Wang, R. Tao, and J. Shen, “Matnet: Motion-attentive
    transition network for zero-shot video object segmentation,” *IEEE Transactions
    on Image Processing (TIP)*, vol. 29, pp. 8326–8338, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Y. Gu, L. Wang, Z. Wang, Y. Liu, M.-M. Cheng, and S.-P. Lu, “Pyramid
    constrained self-attention network for fast video salient object detection,” in
    *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*, vol. 34,
    no. 07, 2020, pp. 10 869–10 876.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] R. Liu, Z. Wu, S. Yu, and S. Lin, “The emergence of objectness: Learning
    zero-shot segmentation from videos,” *Proceedings of the Advances in Neural Information
    Processing Systems (NeurIPS)*, vol. 34, pp. 13 137–13 152, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] M. Zhang, J. Liu, Y. Wang, Y. Piao, S. Yao, W. Ji, J. Li, H. Lu, and
    Z. Luo, “Dynamic context-sensitive filtering network for video salient object
    detection,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision (ICCV)*, 2021, pp. 1553–1563.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] G.-P. Ji, K. Fu, Z. Wu, D.-P. Fan, J. Shen, and L. Shao, “Full-duplex
    strategy for video object segmentation,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*, 2021, pp. 4922–4933.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] J. G.-B. Puyal, K. K. Bhatia, P. Brandao, O. F. Ahmad, D. Toth, R. Kader,
    L. Lovat, P. Mountney, and D. Stoyanov, “Endoscopic polyp segmentation using a
    hybrid 2d/3d cnn,” in *Proceedings of the International Conference on Medical
    Image Computing and Computer Assisted Intervention (MICCAI)*, 2020, pp. 295–305.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] M. Misawa, S.-e. Kudo, Y. Mori, K. Hotta, K. Ohtsuka, T. Matsuda, S. Saito,
    T. Kudo, T. Baba, F. Ishida *et al.*, “Development of a computer-aided detection
    system for colonoscopy and a publicly accessible large colonoscopy video database
    (with video),” *Gastrointestinal Endoscopy*, vol. 93, no. 4, pp. 960–967, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
    K. Bonawitz, Z. Charles, G. Cormode, R. Cummings *et al.*, “Advances and open
    problems in federated learning,” *Foundations and Trends® in Machine Learning
    (FTML)*, vol. 14, no. 1–2, pp. 1–210, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] S. Hong and J. Chae, “Communication-efficient randomized algorithm for
    multi-kernel online federated learning,” *IEEE Transactions on Pattern Analysis
    and Machine Intelligence (TPAMI)*, vol. 44, no. 12, pp. 9872–9886, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] S. Zhou and G. Y. Li, “Federated learning via inexact admm,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence (TPAMI)*, vol. 45, no. 8, pp. 9699–9708,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] L. Liu, X. Jiang, F. Zheng, H. Chen, G.-J. Qi, H. Huang, and L. Shao,
    “A bayesian federated learning framework with online laplace approximation,” *IEEE
    Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*, pp. 1–16,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] W. M. Kouw and M. Loog, “A review of domain adaptation without target
    labels,” *IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*,
    vol. 43, no. 3, pp. 766–785, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] J. Dong, Y. Cong, G. Sun, Z. Fang, and Z. Ding, “Where and how to transfer:
    knowledge aggregation-induced transferability perception for unsupervised domain
    adaptation,” *IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] P. Oza, V. A. Sindagi, V. V. Sharmini, and V. M. Patel, “Unsupervised
    domain adaptation of object detectors: A survey,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence (TPAMI)*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
    and R. Fergus, “Intriguing properties of neural networks,” in *Proceedings of
    the International Conference on Learning Representations (ICLR)*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling deep
    neural networks,” *IEEE Transactions on Evolutionary Computation (TEC)*, vol. 23,
    no. 5, pp. 828–841, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] I. Misra and L. v. d. Maaten, “Self-supervised learning of pretext-invariant
    representations,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, 2020, pp. 6707–6717.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] P. Dollar, M. Singh, and R. Girshick, “Fast and accurate model scaling,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2021, pp. 924–932.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] T. Choudhary, V. Mishra, A. Goswami, and J. Sarangapani, “A comprehensive
    survey on model compression and acceleration,” *Artificial Intelligence Review*,
    vol. 53, pp. 5113–5155, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] T. Li, J. Li, Z. Liu, and C. Zhang, “Few sample knowledge distillation
    for efficient network compression,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2020, pp. 14 639–14 647.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] X. Xia, X. Pan, N. Li, X. He, L. Ma, X. Zhang, and N. Ding, “Gan-based
    anomaly detection: A review,” *Neurocomputing*, vol. 493, pp. 497–535, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,
    S. Whitehead, A. C. Berg, W.-Y. Lo *et al.*, “Segment anything,” *arXiv preprint
    arXiv:2304.02643*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] L. Tang, H. Xiao, and B. Li, “Can sam segment anything? when sam meets
    camouflaged object detection,” *arXiv preprint arXiv:2304.04709*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] M. Hu, Y. Li, and X. Yang, “Skinsam: Empowering skin cancer segmentation
    with segment anything model,” *arXiv preprint arXiv:2304.13973*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] T. Yu, R. Feng, R. Feng, J. Liu, X. Jin, W. Zeng, and Z. Chen, “Inpaint
    anything: Segment anything meets image inpainting,” *arXiv preprint arXiv:2304.06790*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] T. Zhou, Y. Zhang, Y. Zhou, Y. Wu, and C. Gong, “Can sam segment polyps?”
    *arXiv preprint arXiv:2304.07583*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] S. Menon and C. Vondrick, “Visual classification via description from
    large language models,” in *Proceedings of the International Conference on Learning
    Representations (ICLR)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] S. Parisot, Y. Yang, and S. McDonagh, “Learning to name classes for vision
    and language models,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, June 2023, pp. 23 477–23 486.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
