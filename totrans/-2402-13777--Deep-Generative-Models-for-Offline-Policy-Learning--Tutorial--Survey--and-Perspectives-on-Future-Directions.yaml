- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:34:32'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2402.13777] Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.13777](https://ar5iv.labs.arxiv.org/html/2402.13777)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Generative Models for Offline Policy Learning:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tutorial, Survey, and Perspectives on Future Directions
  prefs: []
  type: TYPE_NORMAL
- en: Jiayu Chen chen3686@purdue.edu
  prefs: []
  type: TYPE_NORMAL
- en: Purdue University
  prefs: []
  type: TYPE_NORMAL
- en: West Lafayette, IN 47907 Bhargav Ganguly bganguly@purdue.edu
  prefs: []
  type: TYPE_NORMAL
- en: Purdue University
  prefs: []
  type: TYPE_NORMAL
- en: West Lafayette, IN 47907 Yang Xu xu1720@purdue.edu
  prefs: []
  type: TYPE_NORMAL
- en: Purdue University
  prefs: []
  type: TYPE_NORMAL
- en: West Lafayette, IN 47907 Yongsheng Mei ysmei@gwu.edu
  prefs: []
  type: TYPE_NORMAL
- en: The George Washington University
  prefs: []
  type: TYPE_NORMAL
- en: Washington, DC 20052 Tian Lan tlan@gwu.edu
  prefs: []
  type: TYPE_NORMAL
- en: The George Washington University
  prefs: []
  type: TYPE_NORMAL
- en: Washington, DC 20052 Vaneet Aggarwal vaneet@purdue.edu
  prefs: []
  type: TYPE_NORMAL
- en: Purdue University
  prefs: []
  type: TYPE_NORMAL
- en: West Lafayette, IN 47907
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep generative models (DGMs) have demonstrated great success across various
    domains, particularly in generating texts, images, and videos using models trained
    from offline data. Similarly, data-driven decision-making and robotic control
    also necessitate learning a generator function from the offline data to serve
    as the strategy or policy. In this case, applying deep generative models in offline
    policy learning exhibits great potential, and numerous studies have explored in
    this direction. However, this field still lacks a comprehensive review and so
    developments of different branches are relatively independent. Thus, we provide
    the first systematic review on the applications of deep generative models for
    offline policy learning. In particular, we cover five mainstream deep generative
    models, including Variational Auto-Encoders, Generative Adversarial Networks,
    Normalizing Flows, Transformers, and Diffusion Models, and their applications
    in both offline reinforcement learning (offline RL) and imitation learning (IL).
    Offline RL and IL are two main branches of offline policy learning and are widely-adopted
    techniques for sequential decision-making. Specifically, for each type of DGM-based
    offline policy learning, we distill its fundamental scheme, categorize related
    works based on the usage of the DGM, and sort out the development process of algorithms
    in that field. Subsequent to the main content, we provide in-depth discussions
    on deep generative models and offline policy learning as a summary, based on which
    we present our perspectives on future research directions. This work offers a
    hands-on reference for the research progress in deep generative models for offline
    policy learning, and aims to inspire improved DGM-based offline RL or IL algorithms.
    For convenience, we maintain a paper list on [https://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning](https://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning).
    ¹¹1The contributions of each author for this paper are detailed in the Acknowledgement.
  prefs: []
  type: TYPE_NORMAL
- en: Contents
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[1 Introduction](#S1 "In Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2 Background on Deep Generative Models](#S2 "In Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.1 Variational Auto-Encoders](#S2.SS1 "In 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.2 Generative Adversarial Networks](#S2.SS2 "In 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.3 Normalizing Flows](#S2.SS3 "In 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.4 Transformers](#S2.SS4 "In 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.5 Diffusion Models](#S2.SS5 "In 2 Background on Deep Generative Models ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3 Variational Auto-Encoders in Offline Policy Learning](#S3 "In Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1 Offline Reinforcement Learning](#S3.SS1 "In 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning](#S3.SS1.SSS1
    "In 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.2 An Overview of Applying VAEs in Offline Reinforcement Learning](#S3.SS1.SSS2
    "In 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.3 Addressing the Issue of Out-of-Distribution Actions Using VAEs](#S3.SS1.SSS3
    "In 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.4 Data Augmentation and Transformation with VAEs](#S3.SS1.SSS4 "In 3.1
    Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.5 Offline Multi-task/Hierarchical RL based on VAEs](#S3.SS1.SSS5 "In 3.1
    Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.2 Imitation Learning](#S3.SS2 "In 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.2.1 Core Schemes of VAE-based Imitation Learning](#S3.SS2.SSS1 "In 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.2.2 Improving Data Efficiency in Imitation Learning with VAEs](#S3.SS2.SSS2
    "In 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.2.3 Managing Multi-Modal Inputs in Imitation Learning via VAEs](#S3.SS2.SSS3
    "In 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.2.4 Skill Acquisition and Hierarchical Imitation Learning through VAEs](#S3.SS2.SSS4
    "In 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.2.5 Tackling Causal Confusion in Imitation Learning with VAEs](#S3.SS2.SSS5
    "In 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4 Generative Adversarial Networks in Offline Policy Learning](#S4 "In Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.1 Imitation Learning](#S4.SS1 "In 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.1.1 Fundamental GAN-Based Imitation Learning Algorithms: GAIL and AIRL](#S4.SS1.SSS1
    "In 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.1.2 Extensions of GAIL](#S4.SS1.SSS2 "In 4.1 Imitation Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.1.3 Extensions of AIRL](#S4.SS1.SSS3 "In 4.1 Imitation Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.1.4 Integrating VAEs and GANs for Imitation Learning: A Spotlight](#S4.SS1.SSS4
    "In 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2 Offline Reinforcement Learning](#S4.SS2 "In 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2.1 Background on Model-based Offline Reinforcement Learning](#S4.SS2.SSS1
    "In 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2.2 Policy Approximation Using GANs](#S4.SS2.SSS2 "In 4.2 Offline Reinforcement
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2.3 World Model Representation through GANs](#S4.SS2.SSS3 "In 4.2 Offline
    Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5 Normalizing Flows in Offline Policy Learning](#S5 "In Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.1 Imitation Learning](#S5.SS1 "In 5 Normalizing Flows in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.1.1 Exact Density Estimation in Imitation Learning Using Normalizing Flows](#S5.SS1.SSS1
    "In 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.1.2 Policy Modeling in Imitation Learning with Normalizing Flows](#S5.SS1.SSS2
    "In 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.2 Reinforcement Learning with Offline Data](#S5.SS2 "In 5 Normalizing Flows
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.2.1 Adopting Normalizing Flows in Offline Reinforcement Learning](#S5.SS2.SSS1
    "In 5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.2.2 Adopting Normalizing Flows in Online Reinforcement Learning with Offline
    Data](#S5.SS2.SSS2 "In 5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing
    Flows in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6 Transformers in Offline Policy Learning](#S6 "In Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6.1 Offline Reinforcement Learning](#S6.SS1 "In 6 Transformers in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6.1.1 Background on Trajectory-Optimization-based Offline Reinforcement Learning](#S6.SS1.SSS1
    "In 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6.1.2 Balancing Model Capacity with Training Data](#S6.SS1.SSS2 "In 6.1 Offline
    Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6.1.3 Mitigating Impacts from Environmental Stochasticity](#S6.SS1.SSS3 "In
    6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6.1.4 Transformers in Extended Offline Reinforcement Learning Setups](#S6.SS1.SSS4
    "In 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6.1.5 Reflections on Transformer-based Offline Reinforcement Learning](#S6.SS1.SSS5
    "In 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6.2 Imitation Learning](#S6.SS2 "In 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6.2.1 A Paradigm of Transformer-based Imitation Learning](#S6.SS2.SSS1 "In
    6.2 Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6.2.2 Adopting Transformers as the Policy Backbone for Imitation Learning](#S6.SS2.SSS2
    "In 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6.2.3 Developing Generalist Imitation Learning Agents with Transformers](#S6.SS2.SSS3
    "In 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[7 Diffusion Models in Offline Policy Learning](#S7 "In Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[7.1 Imitation Learning](#S7.SS1 "In 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[7.1.1 Improving Policy Expressiveness in Imitation Learning with Diffusion
    Models](#S7.SS1.SSS1 "In 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[7.1.2 Addressing Common Issues of Imitation Learning with Diffusion Models](#S7.SS1.SSS2
    "In 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[7.2 Offline Reinforcement Learning](#S7.SS2 "In 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[7.2.1 Diffusion Models as Policies](#S7.SS2.SSS1 "In 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[7.2.2 Diffusion Models as Planners](#S7.SS2.SSS2 "In 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[7.2.3 Diffusion Models as Data Synthesizers](#S7.SS2.SSS3 "In 7.2 Offline
    Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[7.2.4 Diffusion Models in Extended Offline Reinforcement Learning Setups](#S7.SS2.SSS4
    "In 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[8 Discussions and Open Problems](#S8 "In Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[8.1 Discussions on Deep Generative Models and Offline Policy Learning](#S8.SS1
    "In 8 Discussions and Open Problems ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[8.2 Perspectives on Future Directions](#S8.SS2 "In 8 Discussions and Open
    Problems ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[8.2.1 Future Works on Data-centric Research](#S8.SS2.SSS1 "In 8.2 Perspectives
    on Future Directions ‣ 8 Discussions and Open Problems ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[8.2.2 Future Works on Benchmarking](#S8.SS2.SSS2 "In 8.2 Perspectives on Future
    Directions ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[8.2.3 Future Works on Theories](#S8.SS2.SSS3 "In 8.2 Perspectives on Future
    Directions ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[8.2.4 Future Works on Algorithm Designs](#S8.SS2.SSS4 "In 8.2 Perspectives
    on Future Directions ‣ 8 Discussions and Open Problems ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[9 Conclusion](#S9 "In Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Offline Policy Learning is a machine learning discipline that leverages pre-existing,
    static datasets to learn effective policies for (robotic) control or decision-making.
    This paper delves into its two primary branches: Offline Reinforcement Learning
    (Offline RL) and Imitation Learning (IL). Offline RL utilizes a pre-compiled batch
    of experience data collected from other policies or human operators, typically
    comprising a series of state-action-reward-next state tuples. The principal objective
    of offline RL is to develop a policy that maximizes the cumulative rewards, which
    may necessitate deviating from the behavior patterns observed in the training
    data. On the other hand, IL trains a policy by mimicking an expert’s behaviors.
    The data used for IL should be demonstrated trajectories from experts. These trajectories,
    highlighting the expert’s responses to various scenarios, usually consist of a
    sequence of state-action pairs (Learning from Demonstrations, LfD) or state-next
    state pairs (Learning from Observations, LfO).'
  prefs: []
  type: TYPE_NORMAL
- en: Generative Models are widely used in many subfields of AI and machine learning.
    Recent advances in parameterizing these models using deep neural networks, combined
    with progress in stochastic optimization methods, have enabled scalable modeling
    of complex, high-dimensional data including images, texts, and speech, with Deep
    Generative Models (DGMs). In particular, we notice the great success of applying
    DGMs to Computer Vision (CV) and Natural Language Processing (NLP). Examples include
    text-to-image generation using Diffusion Models (Stable Diffusion (Rombach et al.
    ([2022](#bib.bib278)))), text-to-video generation with Diffusion Models and Transformers
    (Sora (Brooks et al. ([2024](#bib.bib33)))), large language models based on Transformers
    (ChatGPT (OpenAI ([2023](#bib.bib236)))), and so on. Offline policy learning bears
    similarities to CV and NLP, as all these domains involve learning from a set of
    offline data. While CV or NLP models generate images or texts based on given contexts,
    offline RL/IL models produce actions or trajectories conditioned on task scenarios.
    This similarity suggests that applying DGMs to offline policy learning could potentially
    replicate the success witnessed in CV and NLP, leveraging their capability to
    model and generate complex data patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Great advances have been made in both DGMs and offline policy learning, and
    there are numerous works on applying the progress of DGMs to aid the development
    of offline policy learning. This paper provides a comprehensive review of DGMs
    in offline policy learning. Specifically, the main content is categorized by the
    type of DGMs, and we cover nearly all mainstream DGMs in this paper, including
    Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows,
    Transformers, and Diffusion Models, where Transformers and Diffusion Models are
    representatives of autoregressive and score-based generative models, respectively.
    For each category, we present related works in both offline RL and IL as two subcategories.
    In each subcategory (e.g., VAE-based IL in Section [3.2](#S3.SS2 "3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), we abstract the core schemes of applying that DGM in IL or offline
    RL, categorize related works by how the DGM is utilized, and provide a summary
    table outlining the representative works with their key novelties and evaluation
    benchmarks. It is noteworthy that our paper is more than a survey. For the introduction
    of each work, we try to include its key insights and objective design, to help
    readers get the knowledge without referring to the original papers. Further, for
    each group of works (e.g., Section [3.1.5](#S3.SS1.SSS5 "3.1.5 Offline Multi-task/Hierarchical
    RL based on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") and [7.2.2](#S7.SS2.SSS2
    "7.2.2 Diffusion Models as Planners ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")), algorithms
    are introduced in different levels of details, where the most representative ones
    are emphasized as a tutorial on that specific usage of DGMs for offline policy
    learning and then other extension works are presented for a thorough review. Specifically,
    research works in the same group are presented with unified notations, aiming
    to elucidate the relationships between their objectives and the evolution of their
    algorithm designs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the order of Variational Auto-Encoders, Generative Adversarial Networks,
    Normalizing Flows, Transformers, and Diffusion Models, we present the background
    on DGMs in Section [2](#S2 "2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") and the applications of DGMs for offline policy learning in Section
    [3](#S3 "3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") - [7](#S7 "7 Diffusion Models in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"). For the background of each DGM, we introduce its mathematical basics
    and model variants that are utilized in offline policy learning. Sections related
    to the same DGM (e.g., Section [2.2](#S2.SS2 "2.2 Generative Adversarial Networks
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") and
    [4](#S4 "4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) share the same set of notations such that necessary background can
    be conveniently looked up. Regarding the background on offline RL and IL, we notice
    that different DGMs adopt distinct base offline RL/IL algorithms, so we distribute
    the background introductions to corresponding DGM sections. For instance, three
    main branches of offline RL: dynamic-programming-based, model-based, and trajectory-optimization-based
    offline RL, are adopted and they are respectively introduced in Section [3.1.1](#S3.SS1.SSS1
    "3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), [4.2.1](#S4.SS2.SSS1 "4.2.1 Background
    on Model-based Offline Reinforcement Learning ‣ 4.2 Offline Reinforcement Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), [6.1.1](#S6.SS1.SSS1 "6.1.1 Background on Trajectory-Optimization-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"). Following the main
    content, in Section [8](#S8 "8 Discussions and Open Problems ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), we present in-depth discussions on the main topic of this paper
    and provide perspectives on future research directions. For the discussions (Section
    [8.1](#S8.SS1 "8.1 Discussions on Deep Generative Models and Offline Policy Learning
    ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")), we analyze
    both the common and unique usages of different DGMs in offline policy learning,
    summarize the seminal works in each category and the issues/extensions of offline
    policy learning that have been targeted by DGM-based algorithms. For the future
    works (Section [8.2](#S8.SS2 "8.2 Perspectives on Future Directions ‣ 8 Discussions
    and Open Problems ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")), we offer our perspectives on
    potential research directions across four aspects: data, benchmarking, theories,
    and algorithm designs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of this paper are as follows: (1) This is the first
    review paper on Deep Generative Models for Offline Policy Learning. (2) This paper
    covers a wide array of topics, including five mainstream Deep Generative Models
    and their applications in both Offline Reinforcement Learning and Imitation Learning.
    (3) Throughout this paper, we distill key algorithmic schemes/paradigms and selectively
    highlight seminal research works, as tutorials on respective topics. (4) Our work
    showcases the evolution of DGM-based offline policy learning in parallel with
    the progress of generative models themselves. The summary provided at the end
    of this paper offers valuable insights and directions for future developments
    in this field.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background on Deep Generative Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we introduce five widely-adopted and cutting-edge deep generative
    models: Variational Auto-Encoders, Generative Adversarial Networks, Normalizing
    Flows, Transformers, and Diffusion Models, as the background of their applications
    in offline policy learning. For each of these generative models, we delve into
    their mathematical foundations and provide an overview of their significant variants,
    with a particular focus on those employed in offline policy learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Considering the extensive scope of content addressed in this work, notations
    utilized for different generative models are relatively independent. Within the
    context of the same generative model, including its background and applications
    in offline policy learning, notations are consistent and can be cross-referenced.
    However, notations used across different generative models may not be directly
    comparable or interchangeable, but consistently, we use $x\sim P_{X}(\cdot)$ to
    represent data points in the offline dataset and $z\sim P_{Z}(\cdot)$ to represent
    their latent representations or variations.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Variational Auto-Encoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Variational Auto-Encoders (VAEs, Kingma & Welling ([2014](#bib.bib165); [2019](#bib.bib166)))
    assume that the given data distribution $P_{X}(x)$ can be generated with a deep
    latent-variable model $P_{\theta^{*}}(x)=\int P_{\theta^{*}}(z)P_{\theta^{*}}(x|z)dz$,
    i.e., sampling a continuous latent variable $z$ from the prior $P_{\theta^{*}}(z)$
    and then generating $x$ from the conditional (generative) model $P_{\theta^{*}}(x|z)$.
    It’s also assumed that $P_{\theta^{*}}(z)$ ($P_{\theta^{*}}(x|z)$) comes from
    a parametric family of distributions $P_{\theta}(z)$ ($P_{\theta}(x|z)$). Intuitively,
    such $P_{\theta}(x)$ can be seen as an infinite mixture of some base probability
    distributions $P_{\theta}(x|z)$ (e.g., the Gaussian distribution) across a range
    of possible latent variables $z$, thus it can be quite flexible and powerful for
    modelling complex data distributions $P_{X}(x)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn the mapping from $z$ to $x$, an extra model that can infer the latent
    variables $z$ from the training data $x$ is required, as $z$’s are not provided
    with the offline data. However, according to the Bayes rule, the true posterior
    (inference) model $P_{\theta}(z|x)=P_{\theta}(z)P_{\theta}(x|z)/P_{\theta}(x)$
    is intractable in most cases. For example, if $P_{\theta}(x|z)$ is implemented
    as a neural network, we do not have the analytical form for $P_{\theta}(x)$ and
    so not for $P_{\theta}(z|x)$. Therefore, in VAEs, a generative model $P_{\theta}(x|z)$
    and corresponding inference model $P_{\phi}(z|x)$ (for approximating the true
    posterior) are jointly learned. The most notable contributions of VAEs are two-fold:
    firstly, they propose a variational lower bound as a practical learning objective,
    and secondly, they introduce the reparameterization trick, which facilitates end-to-end
    training of both the generative and inference models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective function of VAEs is a variational lower bound (a.k.a., evidence
    lower bound (ELBO)) of the log likelihood $\log P_{\theta}(x)$, and its derivation
    process is widely adopted in algorithm designs related to latent-variable models,
    so we show the process here:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\log P_{\theta}(x)$ | $\displaystyle=\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x)}\left[\log
    P_{\theta}(x)\right]=\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x)}\left[\log\left[\frac{P_{\theta}(x,z)}{P_{\theta}(z&#124;x)}\right]\right]=\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;x)}\left[\log\left[\frac{P_{\theta}(x,z)}{P_{\phi}(z&#124;x)}\frac{P_{\phi}(z&#124;x)}{P_{\theta}(z&#124;x)}\right]\right]$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x)}\left[\log\left[\frac{P_{\theta}(x,z)}{P_{\phi}(z&#124;x)}\right]+\log\left[\frac{P_{\phi}(z&#124;x)}{P_{\theta}(z&#124;x)}\right]\right]=\mathcal{L}_{\theta,\phi}(x)+D_{KL}(P_{\phi}(z&#124;x)&#124;&#124;P_{\theta}(z&#124;x))\geq\mathcal{L}_{\theta,\phi}(x)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathcal{L}_{\theta,\phi}(x)$ | $\displaystyle=\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;x)}\left[\log P_{\theta}(x,z)-\log P_{\phi}(z&#124;x)\right]=\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;x)}\left[\log P_{\theta}(x&#124;z)-\log\left[\frac{P_{\phi}(z&#124;x)}{P_{\theta}(z)}\right]\right]$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x)}\left[\log
    P_{\theta}(x&#124;z)\right]-D_{KL}(P_{\phi}(z&#124;x)&#124;&#124;P_{\theta}(z))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The inequality in Eq. ([1](#S2.E1 "In 2.1 Variational Auto-Encoders ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) uses the fact that
    KL divergence (i.e., $D_{KL}(\cdot||\cdot)$) is non-negative (Csiszár & Shields
    ([2004](#bib.bib64))). $P_{\phi}$ and $P_{\theta}$ are learned by maximizing $\mathbb{E}_{x\sim
    P_{X}(\cdot)}\mathcal{L}_{\theta,\phi}(x)$, which in turns maximizes $\mathbb{E}_{x\sim
    P_{X}(\cdot)}\log P_{\theta}(x)$. When using neural networks for both the inference
    model $P_{\phi}(z|x)$ and generative model $P_{\theta}(x|z)$, $\mathcal{L}_{\theta,\phi}(x)$
    trains a specific type of auto-encoder, where $P_{\phi}(z|x)$ and $P_{\theta}(x|z)$
    function as the encoder and decoder respectively. Intuitively, this auto-encoder
    is trained by maximizing the data reconstruction accuracy (i.e., the first term
    in $\mathcal{L}_{\theta,\phi}(x)$), while regularizing the distribution of $z$
    from the encoder to be close to the prior distribution of $z$. This regularization
    is necessary, since $z$ is sampled from the encoder for data generation (i.e.,
    $z\sim P_{\phi}(\cdot|x),\hat{x}\sim P_{\theta}(\cdot|z)$) when training but,
    during evaluation, $z$ is sampled from the prior distribution (i.e., $z\sim P_{\theta}(\cdot),\hat{x}\sim
    P_{\theta}(\cdot|z)$).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, VAEs assume that $P_{\theta}(z)=\mathcal{N}(z;0,I)$ and $P_{\phi}(z|x)=\mathcal{N}(z;\mu(x;\phi),\text{diag}(\sigma^{2}(x;\phi)))$.
    Here, assuming $z\in\mathbb{R}^{d}$, $I$ is a $d\times d$ identity matrix, $\mu(x;\phi)\in\mathbb{R}^{d}$
    is a mean vector, and $\text{diag}(\sigma^{2}(x;\phi))\in\mathbb{R}^{d\times d}$
    is a diagonal covariance matrix with $\sigma^{2}_{1:d}(x;\phi)$ as the diagonal
    elements. In this case, the analytical form of $D_{KL}(P_{\phi}(z|x)||P_{\theta}(z))$
    exists and we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\theta,\phi}(x)=\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x)}\left[\log
    P_{\theta}(x&#124;z)\right]+\frac{1}{2}\sum_{i=1}^{d}\left[1+\log\sigma_{i}^{2}(x;\phi)-\mu_{i}^{2}(x;\phi)-\sigma_{i}^{2}(x;\phi)\right]=\mathcal{L}_{\theta,\phi}^{1}(x)+\mathcal{L}_{\phi}^{2}(x)$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: $\nabla_{\phi}\mathcal{L}_{\phi}^{2}(x)$ and $\nabla_{\theta}\mathcal{L}_{\theta,\phi}^{1}(x)$
    can be easily estimated with gradient backpropagation. However, when calculating
    $\nabla_{\phi}\mathcal{L}_{\theta,\phi}^{1}(x)$, the gradient $\nabla_{z}\log
    P_{\theta}(x|z)$ cannot be backpropagated to $\phi$, since $z$ are samples from
    the encoder rather than a function of $\phi$. One solution is the reparameterization
    trick. Specifically, $z$ can be reparameterized as a deterministic function of
    $\phi$ by externalizing the randomness from the output to the input of the encoder,
    i.e., $z=P_{\phi}(x,\epsilon)=\mu(x;\phi)+\sigma(x;\phi)\odot\epsilon,\ \epsilon\sim\mathcal{N}(0,I)$.
    ($\epsilon\in\mathbb{R}^{d}$, and $\odot$ denotes the element-wise product.) In
    this way, the generative models and corresponding inference models can be jointly
    trained using straightforward learning techniques (e.g., stochastic gradient descent
    (Amari ([1993](#bib.bib10)))).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we introduce some representative variants of VAEs, which are utilized
    in Section [3](#S3 "3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions").'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$\beta$-VAE: Assuming that the data $x\sim P_{X}(\cdot)$ is generated by a
    groundtruth simulation process with a number of independent generative factors,
    $\beta$-VAE (Higgins et al. ([2017](#bib.bib131))) is proposed to learn latent
    variables $z$ that encode each generative factor in separate dimensions. For example,
    a simulator might sample independent factors corresponding to the object shape,
    colour, and size to generate an image of a small green apple; ideally, in the
    latent variable of the image, there should be separate dimensions for each of
    these factors. To learn such an encoder $P_{\phi}(z|x)$, the authors propose to
    constrain the KL divergence between $P_{\phi}(z|x)$ and an isotropic unit Gaussian
    distribution $P_{\theta}(z)=\mathcal{N}(z;0,I)$ to encourage the dimensions of
    $z$ to be conditionally independent, resulting in the objective:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\max_{\theta,\phi}\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x)}\left[\log
    P_{\theta}(x&#124;z)\right]\ s.t.\ D_{KL}(P_{\phi}(z&#124;x)&#124;&#124;P_{\theta}(z))<\epsilon$
    |  | (4) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Introducing a Lagrangian multiplier $\beta>0$, the equation above can be reformulated
    into an unconstrained optimization problem: $\max_{\theta,\phi,\beta}\mathbb{E}_{z\sim
    P_{\phi}(\cdot|x)}\left[\log P_{\theta}(x|z)\right]-\beta\left[D_{KL}(P_{\phi}(z|x)||P_{\theta}(z))-\epsilon\right]$.
    They opt to fine-tune $\beta$ as a hyperparameter instead of learning it, leading
    to the final objective:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\max_{\theta,\phi}\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x)}\left[\log
    P_{\theta}(x&#124;z)\right]-\beta D_{KL}(P_{\phi}(z&#124;x)&#124;&#124;P_{\theta}(z))$
    |  | (5) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'When $\beta=1$, this recovers the original VAE formulation (i.e., Eq. ([2](#S2.E2
    "In 2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))). $\beta$ controls the tradeoff between limiting the latent
    channel capacity (via the second term) and preserving the information for reconstructing
    the data sample (via the first term). Empirically, they find that it’s important
    to set $\beta>1$ in order to learn the required disentangled latent variables.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gated VAE: As mentioned in $\beta$-VAE, disentangled latent variables can be
    viewed as data representations in which each element relates to an independent
    and semantically meaningful generator factor of the data. $\beta$-VAE provides
    an unsupervised manner to learn such disentangled representations (through constraining
    the KL divergence). However, consistent disentanglement has recently been demonstrated
    to be impossible without inductive bias or subjective validation (Locatello et al.
    ([2019](#bib.bib198))). Thus, Gated VAE (Vowels et al. ([2020](#bib.bib334)))
    is proposed to incorporate domain knowledge as weak supervision to encourage disentanglement.
    Specifically, as a form of weak supervision, data can be clustered such that each
    cluster comprises data sharing certain generative factors. Then, the latent variable
    $z$ can be split into several partitions, each of which capture the generative
    factors of a specific cluster. Suppose that $x_{1}$ and $x_{2}$ are from the same
    cluster which is assigned to the $i$-th partition of the latent variable, i.e.,
    $z_{i}$, the encoder and decoder can then be trained with a modified ELBO: (which
    is a lower bound of $\log P_{\theta}(x_{2})$)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\theta,\phi}(x_{1},x_{2})=\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x_{1})}\left[\log
    P_{\theta}(x_{2}&#124;z)\right]-D_{KL}(P_{\phi}(z&#124;x_{1})&#124;&#124;P_{\theta}(z))$
    |  | (6) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Note that the entire $z$ is utilized to generate $x_{2}$ from $x_{1}$, but only
    gradients corresponding to $z_{i}$ are backpropagated to the inference model $P_{\phi}$.
    The rationale is that $x_{2}$ is intended to reconstruct only the shared generative
    factors with $x_{1}$, and these factors should exclusively be captured by $z_{i}$.
    Gated VAE requires domain knowledge as supervision, which may limit its general
    applicability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CVAE: Conditional VAE (CVAE, Sohn et al. ([2015](#bib.bib301))) is a model
    for conditional generation. Given a set of data $x$ and their corresponding conditional
    variables $c$ (e.g., classification labels or desired attributes of the data),
    CVAE learns a conditional prior $P_{\theta}(z|c)$ and a conditional generative
    model $P_{\theta}(x|z,c)$, so that a data sample $x$ that satisfies a certain
    condition $c$ can be generated via $z\sim P_{\theta}(\cdot|c),x\sim P_{\theta}(\cdot|z,c)$.
    CVAE is particularly useful in tasks requiring controlled data generation, like
    scenarios where manipulating specific attributes of the generated data is necessary.
    As in VAEs, an inference model $P_{\phi}(z|x,c)$ is introduced for the learning
    process, which gives the ELBO:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\theta,\phi}(x,c)=\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x,c)}\left[\log
    P_{\theta}(x&#124;z,c)\right]-D_{KL}(P_{\phi}(z&#124;x,c)&#124;&#124;P_{\theta}(z&#124;c))$
    |  | (7) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Compared with the original VAE formulation, a conditional variable $c$ is introduced
    to each function, i.e., the prior, inference, and generative model. Following
    a derivation process similar with Eq. ([1](#S2.E1 "In 2.1 Variational Auto-Encoders
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), it
    can be shown that Eq. ([7](#S2.E7 "In 3rd item ‣ 2.1 Variational Auto-Encoders
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) is
    a variational lower bound for $\log P_{\theta}(x|c)$, i.e., the conditional generation
    likelihood. Further, they note that, during evaluation, latent variables are sampled
    from $P_{\theta}(z|c)$ rather than $P_{\phi}(z|x,c)$, as only the conditions $c$
    are available. Thus, to enhance consistency of the data generation process during
    training and evaluation, they introduce an additional objective term to Eq. ([7](#S2.E7
    "In 3rd item ‣ 2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\theta,\phi}^{\text{hybrid}}(x,c)=\alpha\mathcal{L}_{\theta,\phi}(x,c)+(1-\alpha)\mathbb{E}_{z\sim
    P_{\theta}(\cdot&#124;c)}\left[\log P_{\theta}(x&#124;z,c)\right]$ |  | (8) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VQ-VAE: Unlike aforementioned VAE variants where $z$ is continuous, VQ-VAE
    (van den Oord et al. ([2017](#bib.bib326))) is an auto-encoder framework utilizing
    discrete latent variables. In particular, they assume the latent variable space
    is a codebook $[e_{1}^{T},\cdots,e_{k}^{T}]\in\mathbb{R}^{k\times d}$, where $k$
    is number of categories and $d$ is the dimension of the latent vector for each
    category (i.e., $e_{i}$). Such discrete representations are usually easier to
    model than continuous ones and can be particularly advantageous in some situations.
    For example, in a dataset of images with several categories, categorized latent
    representations are more appropriate, where each category is assigned a code from
    the codebook. In particular, the forward process of VQ-VAE is: $z_{E}=P_{\phi}(x),z_{D}=e_{i}\
    (i=\arg\min_{j}||z_{E}-e_{j}||_{2}),\hat{x}\sim P_{\theta}(\cdot|z_{D})$. Here,
    the latent variable from the encoder, i.e., $z_{E}$, is mapped to its nearest
    neighbour in the codebook, i.e., $z_{D}$, which is then input to the decoder for
    data generation. The learnable parameters of VQ-VAE include $\theta$, $\phi$,
    and $e_{1:k}$, for which the objective is as below: ($z_{D}$ and $e_{i}$ are defined
    as above; sg denotes the stop-gradient operator.)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\theta,\phi,e_{1:k}}(x)=\log P_{\theta}(x&#124;z_{D})-&#124;&#124;\text{sg}\left[P_{\phi}(x)\right]-e_{i}&#124;&#124;_{2}^{2}-\beta&#124;&#124;P_{\phi}(x)-\text{sg}\left[e_{i}\right]&#124;&#124;_{2}^{2}$
    |  | (9) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'The generative model is trained to reconstruct the data $x$, as in other VAE
    variants, via the first term of Eq. ([9](#S2.E9 "In 4th item ‣ 2.1 Variational
    Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")).
    The codebook parameters are trained to align with the encoded latent representations
    $z_{E}=P_{\phi}(x)$ through the second term. As for $P_{\phi}$, it is trained
    by both the first and third terms. Specifically, they assume $\frac{\partial\log
    P_{\theta}(x|z_{D})}{\partial z_{E}}=\frac{\partial\log P_{\theta}(x|z_{D})}{\partial
    z_{D}}$, such that the gradient of the first term on $z_{D}$ can be backpropagated
    to $\phi$. The third term, on the other hand, is to make sure that the encoder
    commits to an embedding from the codebook. While empirical evidence demonstrates
    VQ-VAE’s effectiveness, its objective design largely relies on intuition, so additional
    theoretical underpinning would be beneficial.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VRNN: Chung et al. ([2015](#bib.bib58)) introduce a recurrent version of the
    VAE for the purpose of modeling sequences of data (i.e., $x_{\leq T}\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}[x_{0},\cdots,x_{T}]$).
    At each time step $t$, the historical information $(x_{<t},z_{<t})$ is involved
    for sequential decision making, where $z_{<t}$ are latent variables corresponding
    to $x_{<t}$. Instead of directly concatenating the sequence of variables, i.e.,
    $(x_{<t},z_{<t})$, as the history, a recurrent neural network (RNN) is adopted
    to embed the historical information recursively: $h_{t}=\text{RNN}_{\omega}(x_{t},z_{t},h_{t-1}),\
    (t=0,\cdots,T)$ ²²2According to (Chung et al. ([2015](#bib.bib58))), additional
    embedding layers $\text{emb}_{X}$ and $\text{emb}_{Z}$ can be used to extract
    features from the data $x_{t}$ and latent variables $z_{t}$, respectively.. To
    involve historical information in decision-making, the prior, generative, and
    inference model are conditioned on the history embedding as: $P_{\theta}(z_{t}|h_{t-1})$,
    $P_{\theta}(x_{t}|z_{t},h_{t-1})$, and $P_{\phi}(z_{t}|x_{t},h_{t-1})$, where
    $h_{t-1}$ embeds the history $(x_{<t},z_{<t})$. The overall objective (to maximize)
    for these three models and $\text{RNN}_{\omega}$ is as below:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}_{P_{\phi,\omega}(z\leq T&#124;x\leq T)}\left[\sum_{t=1}^{T}\left(\log
    P_{\theta}(x_{t}&#124;z_{t},h_{t-1})-D_{KL}(P_{\phi}(z_{t}&#124;x_{t},h_{t-1})&#124;&#124;P_{\theta}(z_{t}&#124;h_{t-1}))\right)\right]$
    |  | (10) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $P_{\phi,\omega}(z\leq T|x\leq T)=\prod_{t=0}^{T}P_{\phi}(z_{t}|x_{t},h_{t-1})$
    with $h_{t}$ defined as above. This objective is a variational lower bound of
    the log likelihood of the data sequence, i.e., $\log P_{\theta}(x_{\leq T})$,
    as shown in Appendix A of (Chung et al. ([2015](#bib.bib58))). The introduction
    of historical information and explicit use of the Hidden Markov Model (Rabiner
    & Juang ([1986](#bib.bib264))) (e.g., for defining $P_{\phi,\omega}(z\leq T|x\leq
    T)$) significantly improve the representational capability of VRNN for sequential
    data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.2 Generative Adversarial Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Generative Adversarial Network (GAN, Goodfellow et al. ([2014a](#bib.bib106)))
    is a generative model based on the minimax optimization. It consists of a generator
    $G$ and discriminator $D$ that are trained simultaneously to compete against each
    other. The generator tries to capture the distribution of real data $P_{X}(x)$
    and generate new data examples $x$ from initial noise $z\sim P_{Z}(\cdot)$. The
    discriminator is usually a binary classifier used to discriminate generated examples
    from real data as accurately as possible. Formally, the generator can be modeled
    as a differentiable function that maps noise $z$ following a certain prior distribution
    $P_{Z}(\cdot)$ to samples $x$ in the data space $X$: $z\sim P_{Z}(\cdot),x=G(z)\sim
    P_{G}(\cdot)$. While, the discriminator’s output $D(x)$ estimates the probability
    of a data point $x$ being sampled from the true data distribution $P_{X}(\cdot)$
    rather than generated by the generator. The training objective of GAN is formulated
    as a minimax game between $G$ and $D$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{G}\max_{D}\mathbb{E}_{x\sim P_{X}(\cdot)}[\log D(x)]+\mathbb{E}_{z\sim
    P_{Z}(\cdot)}[\log(1-D(G(z)))]$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $D$ is trained to maximize the probability of assigning correct labels
    to both real data and fake ones from the generator: $D(x)\rightarrow 1,D(G(z))\rightarrow
    0$. For a certain $G$, the optimal discriminator is given by: $D_{G}^{*}(x)=\frac{P_{X}(x)}{P_{X}(x)+P_{G}(x)}$.
    Plugging this back in Eq. ([11](#S2.E11 "In 2.2 Generative Adversarial Networks
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), the
    minimax problem can be converted to an optimization problem with regard to the
    generator $G$: $\min_{G}JS(P_{X}(\cdot)||P_{G}(\cdot))$, where $JS(\cdot)$ denotes
    the Jensen-Shannon divergence (Menéndez et al. ([1997](#bib.bib220))). Please
    refer to (Goodfellow et al. ([2014a](#bib.bib106))) for the detailed derivation.
    Thus, in essence, $G$ is trained to generate samples that match the real data
    distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we introduce several representative variants of GAN, as the necessary
    background for Section [4](#S4 "4 Generative Adversarial Networks in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conditional GAN (CGAN) & InfoGAN: CGAN (Mirza & Osindero ([2014](#bib.bib223)))
    trains its generator and discriminator together with extra information $y$ as
    conditions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\min_{G}\max_{D}\mathbb{E}_{x\sim P_{X}(\cdot)}[\log D(x&#124;y)]+\mathbb{E}_{z\sim
    P_{Z}(\cdot)}[\log(1-D(G(z&#124;y)))]$ |  | (12) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Similarly, this objective can be converted to $\min_{G}JS(P_{X}(\cdot|y)||P_{G}(\cdot|y))$,
    i.e., matching the conditional distribution of the real data. With these conditional
    models, CGAN has the advantage of handling not only unimodal datasets but also
    multimodal ones like Flickr (Plummer et al. ([2015](#bib.bib255))), which consists
    of diverse labeled image data. With the class label or text as the condition $y$,
    CGAN can be used for conditional generation. However, when the dataset contains
    multiple modalities but the labels $y$ are not given, InfoGAN (Chen et al. ([2016](#bib.bib50)))
    proposes to introduce a latent code $c$ (following an assumed distribution $P_{C}(\cdot)$)
    to the generator. In this way, the data generation process is modified into $c\sim
    P_{C}(\cdot),z\sim P_{Z}(\cdot),x=G(z,c)\sim P_{G}(\cdot|c)$, where $c$ is for
    capturing distinct modes in the dataset and $z$ targets the unstructured noise
    shared across the modes. The generator $G$ is trained not only with $V(D,G)$ (i.e.,
    Eq. ([11](#S2.E11 "In 2.2 Generative Adversarial Networks ‣ 2 Background on Deep
    Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"))) but also to maximize the mutual
    information between the mode variable $C$ and the respective samples $X|_{C}=G(Z,C)$:
    $\min_{G}\max_{D}V(D,G)-\lambda I(C,X|_{C}))$, such that $G(Z,C)$ would not degenerate
    to a unimodal model. By replacing the mutual information with its practical lower
    bound, we have the objective for InfoGAN as below:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\min_{G,Q}\max_{D}V(D,G)-\lambda\left[\mathbb{E}_{c\sim P_{C}(\cdot),z\sim
    P_{Z}(\cdot),x=G(z,c)}\left[\log Q(c&#124;x)\right]+H(C)\right]$ |  | (13) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Here, $Q(x|c)$ is a variational posterior neural network trained to approximate
    the real but intractable posterior distribution $P(c|x)$, and $H(C)$ denotes the
    entropy of the mode variable $C$ ³³3$I(C,X|_{C})=H(C)-H(C|X|_{C})=-\int P_{C}(c)\log
    P_{C}(c)\,dc+\int P_{C}(c)P_{G}(x|c)\log P(c|x)\,dc\,dx\geq-\int P_{C}(c)\log
    P_{C}(c)\,dc+\int P_{C}(c)P_{G}(x|c)\log Q(c|x)\,dc\,dx.$ The same trick as in
    Eq. ([1](#S2.E1 "In 2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) is adopted here to get this inequality..'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$f$-GAN: A large class of different divergences, including the KL divergence
    and JS divergence, are the so-called $f$-divergences (Csiszár & Shields ([2004](#bib.bib64))).
    Specifically, given two continuous distributions $P(\cdot)$ and $G(\cdot)$, the
    $f$-divergence between them is defined as $D_{f}(P(\cdot)||Q(\cdot))=\int Q(x)f\left(\frac{P(x)}{Q(x)}\right)dx$,
    where $f:\mathbb{R}^{+}\rightarrow\mathbb{R}$ should be a convex, lower-semicontinuous
    function satisfying $f(1)=0$ ⁴⁴4As an instance, when $f(a)=a\log a$, $D_{f}$ recovers
    the KL divergence.. Given a data distribution $P_{X}(\cdot)$, a corresponding
    generative model $G$ can be learned through minimizing $D_{f}(P_{X}(\cdot)||P_{G}(\cdot))$.
    As proposed in $f$-GAN (Nowozin et al. ([2016](#bib.bib235))), this can be approximately
    solved via a minimax optimization problem:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\min_{G}\max_{V}\mathbb{E}_{x\sim P_{X}(\cdot)}\left[g_{f}(V(x))\right]+\mathbb{E}_{x\sim
    P_{G}(\cdot)}\left[-f^{*}(g_{f}(V(x)))\right]$ |  | (14) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Here, $V$ is a differentiable function without any range constraints on the
    output, $g_{f}$ is an output activation function specific to the $f$-divergence
    used, and $f^{*}$ is the convex conjugate function of $f$ (Hiriart-Urruty & Lemaréchal
    ([2004](#bib.bib132))). With specific choices of $f$, $f^{*}$ and $g_{f}$, various
    types of $f$-GAN can be achieved (see Table 6 of (Nowozin et al. ([2016](#bib.bib235)))).
    For example, the original GAN can be recovered by setting $f^{*}(t)=-\log(1-\exp(t))$
    and $g_{f}(v)=-\log(1+\exp(-v))$ ⁵⁵5The discriminator in the original GAN $D(x)$
    would be a function of $V(x)$, i.e., $D(x)=1/(1+\exp(-V(x)))$. $V(x)\in\mathbb{R}$
    and thus $D(x)\in(0,1)$.. In this way, $f$-GAN provides a generalization of multiple
    variants of GANs, including the original GAN, LSGAN, and WGAN.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Least Squares GAN (LSGAN): During the early training stage, the generated examples
    would substantially differ from the real data, but the original GAN objective
    provides only very small penalties for updating $G$, as shown in Figure 16 of
    (Goodfellow ([2017](#bib.bib105))). To overcome this vanishing gradient problem,
    LSGAN (Mao et al. ([2017](#bib.bib212))) replaces the cross-entropy loss used
    in the original GAN objective with least-square losses:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\min_{D}\mathbb{E}_{x\sim P_{X}(\cdot)}\left[(D(x)-j)^{2}\right]+\mathbb{E}_{z\sim
    P_{Z}(\cdot)}\left[(D(G(z))-i)^{2}\right],\ \min_{G}\mathbb{E}_{z\sim P_{Z}(\cdot)}\left[(D(G(z))-k)^{2}\right]$
    |  | (15) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Here, $i,j$ are the labels for the generated and real examples, respectively;
    $k$ is the value that $G$ hopes for $D$ to believe for generated examples. As
    shown in (Mao et al. ([2017](#bib.bib212))), when $j-i=2$ and $j-k=1$, Eq. ([15](#S2.E15
    "In 3rd item ‣ 2.2 Generative Adversarial Networks ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) is equivalent to minimizing the Pearson
    $\chi^{2}$ divergence, which is a type of $f$-divergence, between the real and
    generated data distribution, i.e., $P_{X}(x)$ and $P_{G}(x)$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wasserstein GAN (WGAN): As mentioned earlier, for the original GAN, the optimization
    with respect to $G$ is equivalent to minimizing the JS divergence $JS(P_{X}(\cdot)||P_{G}(\cdot))$.
    WGAN (Arjovsky et al. ([2017](#bib.bib13))) proposes to minimize the Wasserstein
    distance instead, which is defined as $\sup_{||f||_{L}\leq K}\mathbb{E}_{x\sim
    P_{X}(\cdot)}\left[f(x)\right]-\mathbb{E}_{x\sim P_{G}(\cdot)}\left[f(x)\right]$.
    $||f||_{L}\leq K$ denotes the set of $K$-Lipschitz functions. In this case, the
    objective function of WGAN is as below:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\min_{G}\max_{D}\mathbb{E}_{x\sim P_{X}(\cdot)}\left[D(x)\right]-\mathbb{E}_{x\sim
    P_{G}(\cdot)}\left[D(x)\right]$ |  | (16) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Here, $D$ is used to estimate the Wasserstein distance, hence its output is
    not limited to $[0,1]$ as in the original GAN. Also, the neural network $D$ is
    applied with weight clipping, ensuring that $D$ is $K$-Lipschitz. Compared with
    the original GAN, WGAN improves the learning stability and provides meaningful
    learning curves for parameter fine-tuning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Triple-GAN: To realize data classification and conditional generation in the
    meantime, Triple-GAN (Li et al. ([2017a](#bib.bib182))) proposes to introduce
    a classifier $C(y|x)$ to the original GAN framework. Here, $x$ and $y$ denote
    the data and label, respectively. Given a dataset of $(x,y)$, empirical joint
    and marginal distributions of the real data and labels can be acquired as $P_{X,Y}(x,y)$,
    $P_{X}(x)$, and $P_{Y}(y)$. $P_{X,Y}(x,y)$ can be approximated as either $P_{X}(x)P_{C}(y|x)$
    or $P_{Y}(y)P_{G}(x|y)$, corresponding to the classification with $C$ and conditional
    generation with $G$, respectively. Thus, $C$ and $G$ can be trained to match the
    joint distribution of $(x,y)$. Here is the objective:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{C,G}\max_{D}$ | $\displaystyle\mathbb{E}_{(x,y)\sim
    P_{X,Y}(\cdot)}\left[\log D(x,y)\right]+\lambda_{G}\mathbb{E}_{y\sim P_{Y}(\cdot),x\sim
    P_{G}(\cdot&#124;y)}\left[\log(1-D(x,y))\right]+$ |  | (17) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle(1-\lambda_{G})\mathbb{E}_{x\sim P_{X}(\cdot),y\sim P_{C}(\cdot&#124;x)}\left[\log(1-D(x,y))\right]+\mathbb{E}_{(x,y)\sim
    P_{X,Y}(\cdot)}\left[-\log P_{C}(y&#124;x)\right]+$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle\lambda_{C}\mathbb{E}_{y\sim P_{Y}(\cdot),x\sim P_{G}(\cdot&#124;y)}\left[-\log
    P_{C}(y&#124;x)\right]$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'The first three terms resemble the GAN framework, where $C$ and $G$ are trained
    to fool the discriminator $D$ by generating $(x,y)$ close to the real data. The
    last two (cross entropy) terms provide extra supervision for $C$ using samples
    from the real and generated distributions. Note that the last objective term is
    only utilized for training $C$. It is shown in (Li et al. ([2017a](#bib.bib182)))
    that $P_{X,Y}(x,y)=P_{X}(x)P_{C}(y|x)=P_{Y}(y)P_{G}(x|y)$ if and only if the equilibrium
    among the three players ($C,G,D$) is achieved in Eq. ([17](#S2.E17 "In 5th item
    ‣ 2.2 Generative Adversarial Networks ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.3 Normalizing Flows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Normalizing Flows (NFs) represent a class of generative models that offer an
    elegant approach for density estimation and the generation of data samples from
    complex distributions (Kobyzev et al. ([2021](#bib.bib169))). NFs allow for exact
    likelihood evaluation, which is crucial in many probabilistic modeling tasks.
    Formally, NFs transform a simple, known probability distribution on the latent
    variable (i.e., $z\sim P_{Z}(\cdot)$) into a complex, desired distribution on
    data (i.e., $x\sim P_{X}(\cdot)$) through a sequence of invertible and differentiable
    transformations. The mathematical representation of the transformation from $x$
    to $z$ is given by a composition of a series of bijections as $F=F_{N}\circ F_{N-1}\circ\cdots\circ
    F_{1}$. The data flow $[x_{0},x_{1},\cdots,x_{N}]$ ($x_{0}=x,\ x_{N}=z$) within
    NFs adheres to the following: for all $1\leq i\leq N,\ x_{i}=F_{i}(x_{i-1}),\
    x_{i-1}=F^{-1}_{i}(x_{i})$. The core principle behind the transformation is the
    change of variable formula. Specifically, the probability density functions of
    $x$ and $z$ are related as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P_{Z}(z)=P_{X}(F^{-1}(z))&#124;\det D(F^{-1}(z))&#124;,\ P_{X}(x)=P_{Z}(F(x))&#124;\det
    D(F(x))&#124;$ |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\det D(F(x))$ denotes the determinant of the Jacobian matrix of $F(x)$.
    Given the transformation function $F$ and the basic distribution $P_{Z}$, the
    density of a data sample $x$ can be exactly acquired as $P_{X}(x)$ defined in
    Eq. ([18](#S2.E18 "In 2.3 Normalizing Flows ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")). Conversely, the data generation can
    be done by first sampling a latent $z\sim P_{Z}(\cdot)$ and then apply the generator
    function $G=F^{-1}$. As for training, it is usually through maximizing the log-likelihood
    of target data distribution (Dinh et al. ([2017](#bib.bib73))):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}_{X}[\log(P_{X}(x))],\ \log(P_{X}(x))=\log(P_{Z}(F(x)))+\log(&#124;\det
    D(F(x))&#124;)$ |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: 'For NFs, the generator $G$ should be sufficiently expressive to model the distribution
    of interest, while the transformation $F$ should be invertible and the computation
    of the two directions mentioned above, especially regarding the determinant calculation,
    should be efficient. Based on these requirements, various types of flows have
    been developed. Here, we present a brief description of the representative categories.
    As mentioned above, $G$ is composed of a series of generator modules $G_{i}=F_{i}^{-1}$.
    For each of the following categories, we introduce its specific design on $G_{i}$,
    and we use $m$ and $n$ to denote the input and output of $G_{i}$, respectively.
    Note that we will not delve into the complex mathematical details regarding calculations
    or objective designs of specific variants of NFs, as they are not essential for
    understanding the applications of NFs in offline policy learning, as introduced
    in Section [5](#S5 "5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions").'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coupling Flows enable highly expressive transformations via a coupling method.
    In particular, the input $m\in\mathbb{R}^{D}$ is partitioned into two subspaces:
    $(m^{A},m^{B})\in\mathbb{R}^{d}\times\mathbb{R}^{D-d}$, and the generator module
    $G_{i}$ is defined in the format: $G_{i}(m)=(H(m^{A};\Theta(m^{B})),m^{B})$. Here,
    $H(\cdot;\theta):\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}$ is a bijection, and
    $\Theta$ is the parameter function. $H$ is also called the coupling function.
    In this case, the Jacobian of $G_{i}$ is simply the Jacobian of $H$, and for efficient
    computation of determinants, $H$ is usually designed to be an element-wise bijection,
    i.e., $H(m^{A};\theta)=(H_{1}(m^{A}_{1};\theta_{1}),\cdots,H_{d}(m^{A}_{d};\theta_{d}))$
    where each $H_{i}(\cdot;\theta_{i}):\mathbb{R}\rightarrow\mathbb{R}$ is a scalar
    bijection. The expressiveness of a coupling flow lies in the complexity of its
    parameter function $\Theta$, which, in practical applications, is typically modeled
    using neural networks. Algorithms in this category include NICE (Dinh et al. ([2015](#bib.bib72))),
    RealNVP (Dinh et al. ([2017](#bib.bib73))), Glow (Kingma & Dhariwal ([2018](#bib.bib164))),
    NSF (Durkan et al. ([2019](#bib.bib84))), Flow++ (Ho et al. ([2019](#bib.bib135))),
    etc.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Autoregressive Flows utilize autoregressive models for generation. To be specific,
    each entry of $n=G_{i}(m)$ conditions on the previous entries of the input: $n_{t}=H(m_{t};\Theta_{t}(m_{1:t-1}))$,
    where $H(\cdot;\theta):\mathbb{R}\rightarrow\mathbb{R}$ is a scalar bijection
    and $\Theta_{t}$ is a parameter function. Thus, each $n_{t}$ relies solely on
    $m_{1:t}$, resulting in a triangular Jacobian maxtrix of $G$. This structure allows
    for efficient computation of the determinant and parallel computation of each
    entry in $n$. However, the inverse process, represented as $m_{t}=H^{-1}(n_{t};\Theta_{t}(m_{1:t-1}))$,
    must be performed sequentially, since the parameter for the subsequent step (i.e.,
    $\Theta_{t+1}(m_{1:t})$) uses the output of the current step, i.e., $m_{t}$, as
    input. Algorithms like IAF (Kingma et al. ([2016](#bib.bib167))), MAF (Papamakarios
    et al. ([2017](#bib.bib239))), and NAF (Huang et al. ([2018](#bib.bib142))), fall
    in this category.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Residual Flows employ residual networks (He et al. ([2016](#bib.bib126))) as
    the generator component, characterized by $G_{i}(m)=m+R(m)$. Here, $R(\cdot)$
    symbolizes the residual block, which can be implemented as any variant of neural
    network that is invertible. Several studies (Gomez et al. ([2017](#bib.bib104));
    Chang et al. ([2018](#bib.bib36)); Jacobsen et al. ([2018](#bib.bib145))) have
    aimed to develop invertible network architectures suitable for use as residual
    blocks. Nonetheless, these architectures present a significant challenge in efficiently
    calculating their Jacobian determinants. Research works such as iResNet (Behrmann
    et al. ([2019](#bib.bib21))) and Residual Flow (Chen et al. ([2019](#bib.bib48)))
    propose an invertible $G_{i}$ by limiting the Lipschitz constant of the residual
    block to be less than 1 and compute its inverse through fixed-point iterations.
    However, controlling the Lipschitz constant of a neural network is also quite
    challenging.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ODE-based Flows aim to learn a continuous dynamic system shown as a first-order
    ordinary differential equation (ODE): $\frac{d}{dt}m(t)=R(m(t);\theta(t)),\ t\in[0,1]$.
    It’s worthy noting that the residual flows mentioned above can be interpreted
    as a discretization of this ODE. Assuming the uniform Lipschitz continuity in
    $m$ and setting the initial condition $m(0)=z$, the solution of that ODE at each
    time point $\Phi^{t}(z)$ exists and is unique (Arnold ([1992](#bib.bib14))). As
    proposed in NODE (Chen et al. ([2018b](#bib.bib47))), the map at time one $\Phi^{1}(z)$
    can work as the generator, i.e., $x=\Phi^{1}(z)$, which can be considered as an
    “infinitely deep” neural network with stacked weights $\theta(t),t\in[0,1]$. Thus,
    typically, continuous ODE-type flows require fewer parameters to match the performance
    of discrete ones (Grathwohl et al. ([2018](#bib.bib108))). As for the invertibility
    of $\Phi^{1}$, it is naturally ensured by the theorem of existence and uniqueness
    pertaining to the solution of the ODE. However, $\Phi^{1}$ must be orientation
    preserving - that is, its Jacobian determinant must be positive, which may constrain
    its representational capacity. In order to give freedom for the Jacobian determinant
    to remain positive, the authors of ANODE (Dupont et al. ([2019](#bib.bib83)))
    propose augmenting the original ODE with supplementary variables $\hat{m}(t)$,
    resulting in $\frac{d}{dt}[m(t)||\hat{m}(t)]=\hat{R}(m(t)||\hat{m}(t);\theta(t)),\
    t\in[0,1]$, with $||$ signifying concatenation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There are other types of flows which are not widely used in practice and so
    not introduced here, such as Planar and Radial Flows (Rezende & Mohamed ([2015](#bib.bib276));
    van den Berg et al. ([2018](#bib.bib325))) and Langevin Flows (continuous and
    SDE-based) (Welling & Teh ([2011](#bib.bib351)); Chen et al. ([2018a](#bib.bib40))).
    Coupling Flows and Autoregressive Flows facilitate straightforward invertible
    transformations and efficient determinant computation, yet lag in expressiveness
    compared to Residual Flows which are computationally costly. ODE-based Flows provide
    a potentially elegant and parsimonious representation with fewer parameters. However,
    they necessitate the resolution of ODEs during training, a process that can be
    computationally demanding and sensitive to the choice of numerical solver settings.
    Compared with VAEs and GANs, NFs allow for exact density estimation of a generated
    sample and can avoid training issues like mode collapse, vanishing gradients,
    etc (Salimans et al. ([2016](#bib.bib282))). However, its requirements for bijection
    functions and determinant calculations can restrict its capacity to effectively
    model complex data distributions (Cornish et al. ([2020](#bib.bib61))).
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Transformer (Vaswani et al. ([2017](#bib.bib327))) is an important foundation
    model that has shown exceptional capability across various areas, such as natural
    language processing (Kalyan et al. ([2021](#bib.bib155))), computer vision (Han
    et al. ([2022](#bib.bib121))), time series analysis (Wen et al. ([2023](#bib.bib353))),
    and so on. Recently, there has been a growing trend in developing IL and offline
    RL algorithms based on transformers, with the hope to achieve sequential decision-making
    based on next-token predictions as in natural language processing. As shown in
    Figure [1](#S2.F1 "Figure 1 ‣ 2.4 Transformers ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"), the transformer follows the widely-adopted
    encoder-decoder structure. The encoder maps the input $(x_{1},\cdots,x_{n})$ to
    a sequence of embeddings $(z_{1},\cdots,z_{n})$, and the decoder generates the
    output sequence auto-regressively. That is, the decoder predicts one token $y_{m+1}$
    at a time, based on the input embeddings $(z_{1},\cdots,z_{n})$ and previously-generated
    outputs $(y_{1},\cdots,y_{m})$. Both the encoder and decoder are composed of a
    stack of $L$ identical modules. For clarity, we present each layer in the module
    sequentially following the data flow.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Positional Encoding: After the token embedding layer, which is shared by $(x_{1},\cdots,x_{n})$
    and $(y_{1},\cdots,y_{m})$, each element is converted to a $d$-dim vector. To
    enable the model to make use of the order of tokens, information about the relative
    or absolute positions of tokens within the sequence is injected through a positional
    encoding. For the $i$-th token, its positional encoding is also a $d$-dim vector,
    of which the $j$-th dimension is $\sin(\frac{i}{10000^{j/d}})$ if $j$ is even
    and $\cos(\frac{i}{10000^{(j-1)/d}})$ otherwise. The positional encoding and token
    embedding are then combined through summation. Per Vaswani et al. ([2017](#bib.bib327)),
    this (periodic function) positional encoding design embeds relative position information
    and helps the model to manage sequences longer than those experienced in training.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Self Multi-Head Attention (MHA): This component utilizes the attention mechanism.
    Given $s$ queries and $t$ key-value pairs, the attention function maps each query
    to a weighted-sum of the values, where the weight assigned to each value is computed
    as the compatibility of the query with the key paired with that value. The queries,
    keys, and values can be represented as $Q\in\mathbb{R}^{s\times d_{q}}$, $K\in\mathbb{R}^{t\times
    d_{k}}$, and $V\in\mathbb{R}^{t\times d_{v}}$, respectively, and the outputs for
    all queries (i.e., $O$) can be computed in parallel as: (For conformability of
    matrix multiplication, $d_{k}=d_{q}$.)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $O=\text{Attention}(Q,K,V)=\text{SoftMax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V,\
    O_{i}=\text{SoftMax}\left(\frac{\langle Q_{i},K_{1}\rangle}{\sqrt{d_{k}}},\cdots,\frac{\langle
    Q_{i},K_{t}\rangle}{\sqrt{d_{k}}}\right)V$ |  | (20) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Here, SoftMax is a row operator, and the output for query $i$, i.e., $O_{i}$,
    is a weighted sum of the rows of $V$, where the weight for row $j$ is propotional
    to the similarity of $Q_{i}$ (i.e., the $i$-th row of $Q$) and $K_{j}$ measured
    by their inner product $\langle Q_{i},K_{j}\rangle$. The dot products grow large
    with $d_{k}$, which would push the SoftMax function to regions with vanished gradients,
    so the factor $1/\sqrt{d_{k}}$ is introduced. The input embeddings $H_{x}$ (in
    Figure [1](#S2.F1 "Figure 1 ‣ 2.4 Transformers ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) can potentially be used as the matrices
    $Q$, $K$, and $V$ for self attention. However, to enable the model to jointly
    attend to information from different representation subspaces, a multi-head attention
    mechanism is adopted:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\text{MHA}(Q,K,V)=\text{Concat}(\text{head}_{1},\cdots,\text{head}_{h})W^{O},\
    \text{head}_{i}=\text{Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$ |  | (21)
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $Q=K=V=H_{x}\in\mathbb{R}^{n\times d}$, $W_{i}^{Q}\in\mathbb{R}^{d\times
    d^{\prime}}$, $W_{i}^{K}\in\mathbb{R}^{d\times d^{\prime}}$, $W_{i}^{V}\in\mathbb{R}^{d\times
    d^{\prime}}$, $W_{i}^{O}\in\mathbb{R}^{d\times d}$, $d^{\prime}=d/h$, and Concat
    represents the concatenation operation. $W_{i}^{Q},W_{i}^{K},W_{i}^{V}$ convert
    $H_{x}$ to matrices in the $i$-th attention head, providing a distict representation
    subspace. It’s worthy noting that the output of MHA belongs to $\mathbb{R}^{n\times
    d}$, that is, the same in shape as its input.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add & Normalize: A residual connection (He et al. ([2016](#bib.bib126))) is
    employed around each attention layer, followed by a layer normalization (Ba et al.
    ([2016](#bib.bib18))). As common practice, these two operations are adopted to
    stabilize training of (very) deep networks (e.g., by alleviating ill-posed gradients
    and model degeneration). Suppose the previous layer is $F$, which can be an MHA
    or Feed-forward Network as shown in Figure [1](#S2.F1 "Figure 1 ‣ 2.4 Transformers
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), and
    its input is $X$, then the calculation of this Add & Normalize layer can be denoted
    as $\text{LayerNorm}(F(X)+X)$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Point-wise Feed-forward Network (FFN): FFN layers are important for a Transformer
    to achieve good performance. Dong et al. ([2021](#bib.bib75)) observe that simply
    stacking MHA modules causes a rank collapse problem (e.g., leading to token-uniformity),
    and that the FFN is one of the important building blocks to mitigate this issue.
    Specifically, a two-layer fully-connected network with a ReLU activation function
    in the middle is applied to each of the $n$ token embeddings separately, leading
    to $n$ new $d$-dim embeddings as output.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Masked Self MHA & Cross MHA: For the decoder, $H_{x}$ is replaced by $H_{y}\in\mathbb{R}^{m\times
    d}$, i.e., embeddings of previously-generated outputs. For rationality, the query
    (of the Masked Self MHA) at each position is only allowed to attend to positions
    up to and including that position, as the other queries correspond to outputs
    yet to generate. This is realized within the Masked Self MHA by masking out corresponding
    compatibility values, i.e., $\langle Q_{i},K_{j}\rangle=-\infty,\ \forall j>i$
    (in Eq. ([20](#S2.E20 "In 2nd item ‣ 2.4 Transformers ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"))). As for the Cross MHA, its queries come
    from the previous decoder layer and key-value pairs are from the output of the
    encoder, i.e., $H_{z}\in\mathbb{R}^{n\times d}$ in Figure [1](#S2.F1 "Figure 1
    ‣ 2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"). Note that these matrices will be embedded with $W_{i}^{Q},W_{i}^{K},W_{i}^{V}$
    to get multiple representation subspaces, as Eq. ([21](#S2.E21 "In 2nd item ‣
    2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")).
    In this way, every query for predicting the next token can attend over all positions
    in the input sequence (via the Cross MHA) and all previously-generated tokens
    (via the Masked Self MHA).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/189dda9b3fb8fd286086e52f2ac90a1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The Transformer Architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each encoder module outputs an $n\times d$ matrix, and each decoder module
    outputs an $m\times d$ matrix. Here, $n$ and $m$ denote the counts of elements
    in the input and output sequences, respectively, while $d$ represents the embedding
    dimension. This uniformity in the input and output dimensions enables the stacking
    of multiple encoder (or decoder) modules to create a deep model. Each encoder
    (or decoder) module will take the output from the previous encoder (or decoder)
    module. Generally, the transformer architecture can be used in three different
    ways: encoder-only, decoder-only, or encoder-decoder. When using only the encoder,
    its output serves as a representation of the input sequence, suitable for natural
    language understanding tasks such as text classification. While, if only the decoder
    is employed, the Cross MHA modules are removed and this architecture is suitable
    for sequence generation tasks like language modeling. Applications of transformers
    in IL or offline RL usually adopt this decoder-only way. The overall encoder–decoder
    architecture is equipped with the ability to perform both natural language understanding
    and generation, typically used in sequence-to-sequence modeling (e.g., neural
    machine translation).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared with other types of neural networks, the transformer architecture
    has significant advantages. Compared with fully-connected layers, the transformer
    is more flexible in handling variable-length inputs (i.e., with different $n$)
    and more parameter-efficient, since the amount of network parameters in a transformer
    is irrelevant to the sequence length $n$ ⁶⁶6The FFN layers are applied to each
    position rather than the whole sequence, and the MHA layers only require updating
    the weights: $W_{i}^{Q},W_{i}^{K},W_{i}^{V},W^{O}$, the sizes of which are irrelevant
    with $n$.. The convolutional layer uses a convolution kernel of size $k<n$ to
    connect pairs of tokens, so, to link distant token pairs, a stack of convolutional
    layers is required. However, with the attention mechanism, each pair of tokens
    can be connected in one MHA layer and the time complexity for matching their corresponding
    query and key is constant (i.e., irrelevant to $n$). This makes the transformer
    excel at capturing long-range dependencies within a sequence. Recurrent layers
    produce a sequence of hidden states $h_{i}$, each dependent on the previous hidden
    state $h_{i-1}$ and the input at position $i$. This sequential nature prevents
    parallel computation. However, in transformers, computations at each position
    are independent, allowing for parallel execution that are essential for processing
    long sequences. Last, both convolutional and recurrent models make structural
    assumptions over the inputs, suitable for image-like and time series data, respectively.
    However, the transformer has few prior assumptions on the data structure and thus
    is a more universal model. Empirical results have shown that the transformer has
    superior performance on a wide-range of tasks and a larger capacity than CNNs
    and RNNs to handle a huge amount of training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As listed above, the transformer contains multiple types of layers. Numerous
    studies have explored modifications or replacements of layers in the standard
    transformer for improvements. For a comprehensive review, please refer to (Lin
    et al. ([2022c](#bib.bib194))). On the other hand, the computation and memory
    complexity of MHA modules are quadratic to the length of the input sequence (i.e.,
    $n$), leading to inefficiency at processing extremely long sequences. Various
    extensions have been developed to enhance either the computational or memory efficiency.
    Additionally, due to the minimal structural assumptions made by the transformer
    about the input data, transformers are prone to overfitting when trained on small-scale
    datasets. Attempts like pretraining the transformer on large-scale unlabeled data
    (Brown et al. ([2020](#bib.bib34))), introducing sparsity assumptions (Child et al.
    ([2019](#bib.bib55))) such as limiting the number and positions of key-value pairs
    that each query can attend to, have been made to solve these issues. Finally,
    extensions to adapt the transformer for particular downstream applications are
    possible. Studies detailed in Section [6](#S6 "6 Transformers in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions") can be viewed as examples of this type
    of extensions.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Diffusion Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Diffusion models have demonstrated superior performance across multiple domains,
    such as computer vision (Amit et al., [2021](#bib.bib11); Baranchuk et al., [2021](#bib.bib20)),
    natural language processing (Austin et al., [2021b](#bib.bib16); Hoogeboom et al.,
    [2021](#bib.bib137)), and multi-modal learning (Avrahami et al., [2022](#bib.bib17);
    Ramesh et al., [2022](#bib.bib269)), showcasing their impressive capabilities
    of generating detailed and diverse instances. Diffusion models contain two interconnected
    processes: the forward diffusion process and the backward denoising process (Yang
    et al. ([2022a](#bib.bib368))). Specifically, the forward process is predefined
    and transforms the data with a certain (but unknown) distribution, i.e., $x\sim
    P_{X}(\cdot)$, into a (standard Gaussian) random noise $z$. This process progressively
    corrupts the input data by adding a varying scale of noise at each diffusion step.
    Correspondingly, the reverse process uses a neural network as the denoising function
    to gradually undo the forward transformation to reconstruct the data from the
    random noise. There exist three main formulations of diffusion models: Score-based
    Generative Models (SGM) (Song & Ermon, [2019](#bib.bib303); [2020](#bib.bib304)),
    Denoised Diffusion Probabilistic Models (DDPM) (Sohl-Dickstein et al., [2015](#bib.bib299);
    Ho et al., [2020](#bib.bib136); Nichol & Dhariwal, [2021](#bib.bib232)), and Stochastic
    Differential Equations (Score SDE) (Song et al., [2020](#bib.bib306); [2021](#bib.bib307)).
    Next, we introduce these formulations by illustrating their forward/backward processes
    and learning objectives, while discussing their connections with each other.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SGM learns (Stein) score functions (Hyvärinen ([2005](#bib.bib144))) for samples
    at each diffusion step, i.e., $S_{\theta}(x_{t},t),\ t\in[1,\cdots,T]$. As defined,
    the Stein score function of $x$ is the gradient of its log likelihood $\nabla_{x}\log
    P(x)$, with which data samples $x\sim P(\cdot)$ can be generated via various efficient
    score-based sampling schemes such as (Song & Ermon ([2019](#bib.bib303)); Jolicoeur-Martineau
    et al. ([2021](#bib.bib154))). Specifically, the forward process of SGM starts
    from $x_{0}\sim P_{X}(\cdot)$ and injects intensifying Gaussian noise to generate
    $x_{1:T}$: ($\beta_{1:T}$ is a predefined schedule of variance levels.)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $x_{t}\sim F(\cdot&#124;x_{0})={\mathcal{N}}(x_{0},\beta_{t}I),\ 0<\beta_{1}<\beta_{2}<\cdots<\beta_{T}$
    |  | (22) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'The score functions for each step $S_{\theta}(x_{t},t)$ are trained to approximate
    $\nabla_{x_{t}}\log P(x_{t})$, where $P(x_{t})=\int F(x_{t}|x_{0})P_{X}(x_{0})dx_{0}$.
    In particular, the training objective is as below:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta}\mathbb{E}_{t\sim{\mathcal{U}}[1,T],x_{0}\sim P_{X}(\cdot),\epsilon\sim{\mathcal{N}}(0,I)}\left[\lambda(t)&#124;&#124;\epsilon+\sqrt{\beta_{t}}S_{\theta}(x_{t},t)&#124;&#124;^{2}\right]$
    |  | (23) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Here, ${\mathcal{U}}[1,T]$ is a uniform distribution on $[1,\cdots,T]$, $\lambda(t)$’s
    are positive weighting functions. This objective is derived based on the definition
    of the forward process, i.e., $x_{t}=x_{0}+\sqrt{\beta_{t}}\epsilon,\ \epsilon\sim{\mathcal{N}}(0,I)$.
    For detailed derivations and the definition of $\lambda(t)$, please refer to (Song
    & Ermon ([2019](#bib.bib303))). Regarding the backward generation process, the
    score functions $S_{\theta}(x_{t},t),\ t\in[T,\cdots,1],$ are sequentially used
    as denoising functions to generate $x_{t-1}$ from $x_{t}$. Finally, the required
    data samples $x_{0}\sim P_{X}(\cdot)$ can be acquired. As mentioned, multiple
    score-based sampling schemes can be adopted in this process. We take the annealed
    Langevin dynamics sampling scheme (Song & Ermon ([2019](#bib.bib303))) as an example:
    ($x_{T}=x_{T}^{0},\ x_{0}=x_{0}^{0}$)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $x_{t}^{i+1}=x_{t}^{i}+\frac{1}{2}s_{t}S_{\theta}(x_{t}^{i},t)+\sqrt{s_{t}}\epsilon,\
    (i=0,\cdots,N-1),\ x_{t-1}^{0}=x_{t}^{N}$ |  | (24) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: $N$ and $s_{t}$ are hyperparameters, denoting the number of iterations and step
    size for denoising $x_{t}^{0}$ to $x_{t-1}^{0}$ ($t=T\rightarrow 1$).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DDPM gradually adds random noise to the data over a series of time steps $(x_{0},\cdots,x_{T})$
    in the forward process, where $x_{0}=x,\ x_{T}=z$. In particular, the sample at
    each time step is drawn from a Gaussian distribution conditioned on the sample
    from the previous time step: ($\beta_{1:T}$ are predefined.)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $x_{t}\sim F(\cdot&#124;x_{t-1})={\mathcal{N}}(\sqrt{1-\beta_{t}}x_{t-1},\beta_{t}I)$
    |  | (25) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'With Eq. ([25](#S2.E25 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")), the sample at each step $t$
    can be expressed as a function of $x_{0}$: $x_{t}=\sqrt{\alpha_{t}}x_{0}+\sqrt{1-\alpha_{t}}\epsilon$,
    where $\alpha_{t}=\prod_{s=0}^{t}(1-\beta_{s}),\ \epsilon\sim{\mathcal{N}}(0,I)$
    (Sohl-Dickstein et al. ([2015](#bib.bib299))). $\alpha_{T}$ is designed to be
    close to 0, so that $z=x_{T}$ approximately follows ${\mathcal{N}}(0,I)$. Conversely,
    through the reverse denoising process, $x_{T}$ is converted back to $x_{0}$ step
    by step: ($t=T\rightarrow 1$)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $x_{t-1}\sim G_{\theta}(\cdot&#124;x_{t})={\mathcal{N}}(\mu_{\theta}(x_{t},t),\Sigma_{\theta}(x_{t},t))$
    |  | (26) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'where $\mu_{\theta}$ and $\Sigma_{\theta}$ can be implemented as neural networks.
    The objective for learning this denoising function is to match the joint distributions
    of $x_{0:T}$ in the forward and backward processes, i.e., $\min_{\theta}D_{KL}(F(x_{0},\cdots,x_{T})||G_{\theta}(x_{0},\cdots,x_{T}))$,
    which is equivalent to: (Sohl-Dickstein et al., [2015](#bib.bib299))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta}\mathbb{E}_{x_{0}\sim P_{X}(\cdot),x_{1:T}\sim F(\cdot&#124;x_{0})}\left[-\log
    P(x_{T})-\sum_{t=1}^{T}\log\frac{G_{\theta}(x_{t-1}&#124;x_{t})}{F(x_{t}&#124;x_{t-1})}\right]$
    |  | (27) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Note that $P(x_{T})$ and $F(x_{t}|x_{t-1})$ have analytical forms and this
    objective is an upper bound for the negative log-likelihood $\mathbb{E}_{x_{0}\sim
    P_{X}(\cdot)}\left[-\log P_{\theta}(x_{0})\right]$. Although this objective can
    be directly optimized through Monte Carlo sampling, Ho et al. ([2020](#bib.bib136))
    propose a reformulation of it for variance reduction. Since all (forward or backward)
    transformations are based on Gaussian distributions, by specifying the variance
    schedule $\beta_{1:T}$ and fixing the backward variance $\Sigma_{\theta}(x_{t},t)$
    to be $\beta_{t}I$, Eq. ([27](#S2.E27 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2
    Background on Deep Generative Models ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) can be converted
    to: (Ho et al. ([2020](#bib.bib136)))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta}\mathbb{E}_{t\sim{\mathcal{U}}[1,T],x_{0}\sim P_{X}(\cdot),\epsilon\sim{\mathcal{N}}(0,I)}\left[\lambda(t)&#124;&#124;\epsilon-\epsilon_{\theta}(\sqrt{\alpha_{t}}x_{0}+\sqrt{1-\alpha_{t}}\epsilon,t)&#124;&#124;^{2}\right]$
    |  | (28) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Here, $\lambda(t)$ is a positive weighting function which has a closed-form,
    but in practice, $\lambda(t)$ is set as 1 for all $t$ to improve sample quality.
    Intuitively, the denoising function $\epsilon_{\theta}$ is trained to predict
    the noise injected to samples at each step. Also, by setting $\epsilon_{\theta}(x,t)=-\sqrt{\beta_{t}}S_{\theta}(x,t)$,
    the objective forms of SGM (Eq. ([23](#S2.E23 "In 1st item ‣ 2.5 Diffusion Models
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))) and
    DDPM (Eq. ([28](#S2.E28 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"))) can be unified. Finally, with
    the learned $\epsilon_{\theta}$, the sampling process $x_{t-1}\sim G_{\theta}(\cdot|x_{t})$
    is equivalent to: $x_{t-1}=\frac{1}{\sqrt{1-\beta_{t}}}(x_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(x_{t},t))+\sqrt{\beta_{t}}\epsilon,\
    \epsilon\sim{\mathcal{N}}(0,I)$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Score SDE extends the discrete-time schemes of the previous two methods to
    a unified continuous-time framework, building upon stochastic differential equations
    (SDE). In the forward process, it perturbs data to noise with a diffusion process
    governed by the following differential equation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $dx=f(x,t)\>dt+g(t)\>dw,\ t\in[0,T]$ |  | (29) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'where $w$ is the standard Wiener process (Ricciardi ([1976](#bib.bib277))),
    $f(x,t),\ g(t)$ are the diffusion and drift functions, respectively. In particular,
    the forward process of DDPM and SGM can be described by the following two equations
    (Song et al. ([2020](#bib.bib306))), respectively, with specified $f(x,t)$ and
    $g(t)$:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $dx=-\frac{1}{2}\beta(t)x\>dt+\sqrt{\beta(t)}\>dw,\ dx=\sqrt{\frac{d\beta(t)}{dt}}\>dw$
    |  | (30) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'The reverse process can be realized via solving the reverse-time SDE (Anderson,
    [1982](#bib.bib12)):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $dx=[f(x,t)-g^{2}(t)\nabla_{x}\log P_{t}(x)]\>dt+g(t)\>d\bar{w}$ |  |
    (31) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $\bar{w}$ denotes the standard Wiener process when time flows backwards,
    $P_{t}(x)$ denotes the distribution of $x$ at time $t$ in the forward process.
    Similarly with SGM, the score function at each time $\nabla_{x_{t}}\log P_{t}(x_{t})$
    can be estimated with an NN-based function $S_{\theta}(x_{t},t)$ through various
    score matching techniques (Vincent ([2011](#bib.bib332)); Song et al. ([2019](#bib.bib305))).
    Further, in (Song et al. ([2020](#bib.bib306))), they propose to get rid of the
    random noise injection $g(t)\>d\bar{w}$ and prove that the resulting equation
    is a probability flow ordinary differential equation (ODE) which shares the same
    marginal densities as those of the reverse-time SDE, and both equations allow
    sampling from the required data distribution, i.e., $P_{X}(x)$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3 Variational Auto-Encoders in Offline Policy Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present a comprehensive overview of the applications of
    VAEs in offline policy learning, encompassing both offline RL (Section [3.1](#S3.SS1
    "3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) and IL (Section [3.2](#S3.SS2 "3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")). Within each subsection, we categorize related research works based
    on how VAEs are utilized. Additionally, we provide necessary background and a
    summary on the algorithm design paradigm as a brief tutorial. At the end of each
    subsection, we provide a table to summarize the representative algorithms with
    their key novelties and evaluation tasks, serving as a reference for future research
    in algorithm design and evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sections focusing on other generative models, i.e., Sections [4](#S4 "4
    Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    - [7](#S7 "7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    follow a structure similar to this section. As mentioned in the beginning of Section
    [2](#S2 "2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), notations
    of functions or variables are not shared across different generative models. However,
    all content is grounded in the fundamental Markov Decision Process (Puterman ([2014](#bib.bib258))),
    denoted as $\mathcal{M}=(\mathcal{S},\mathcal{A},\mathcal{T},r,\rho_{0}(s),\gamma)$.
    $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{T}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]$
    is the transition function, $\rho_{0}:\mathcal{S}\rightarrow[0,1]$ is the distribution
    of the initial state, $r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ is
    the reward function, and $\mathcal{\gamma}\in(0,1]$ is the discount factor.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Offline Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Interestingly, our findings indicate that VAEs, Normalizing Flows, and Diffusion
    Models are typically integrated with dynamic-programming-based offline RL, whereas
    GANs are employed to enhance model-based offline RL, and Transformers are utilized
    in trajectory-optimization-based offline RL. Therefore, we split the background
    introduction on offline RL into three parts, categorized under the corresponding
    generative models: Section [3.1](#S3.SS1 "3.1 Offline Reinforcement Learning ‣
    3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    Section [4.2](#S4.SS2 "4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"), and Section
    [6.1](#S6.SS1 "6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"). Based on the background on model-free
    offline RL (Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), we
    delve into the specific applications of VAEs for offline RL in Sections [3.1.3](#S3.SS1.SSS3
    "3.1.3 Addressing the Issue of Out-of-Distribution Actions Using VAEs ‣ 3.1 Offline
    Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") -[3.1.5](#S3.SS1.SSS5 "3.1.5 Offline Multi-task/Hierarchical
    RL based on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), preceded by an overview
    in Section [3.1.2](#S3.SS1.SSS2 "3.1.2 An Overview of Applying VAEs in Offline
    Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Dynamic-programming-based offline RL is a main branch of model-free offline
    RL, as detailed in (Levine et al. ([2020](#bib.bib181))). Given an offline dataset
    $D_{\mu}$ collected by the behavior policy $\mu(a|s)$, dynamic-programming-based
    offline RL usually would adopt a constrained policy iteration process as below:
    ($k$ denotes the index of the learning iteration.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\quad Q_{k+1}^{\pi}=\arg\min_{Q}\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[Q(s,a)-\left(r+\gamma\mathbb{E}_{a^{\prime}\sim\pi_{k}(\cdot&#124;s^{\prime})}Q_{k}^{\pi}(s^{\prime},a^{\prime})\right)\right]^{2};$
    |  | (32) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\pi_{k+1}=\arg\max_{\pi}\mathbb{E}_{s\sim D_{\mu}}\left[\mathbb{E}_{a\sim\pi(\cdot&#124;s)}Q_{k+1}^{\pi}(s,a)\right]\
    s.t.\ \mathbb{E}_{s\in D_{\mu}}\left[D(\pi(\cdot&#124;s),\mu(\cdot&#124;s))\right]\leq\epsilon.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Here, the first and second equations are referred to as the policy evaluation
    and policy improvement steps, respectively. When updating the Q function, $(s,a,r,s^{\prime})$
    are sampled from $D_{\mu}$ but the target action $a^{\prime}$ is sampled by the
    being-learned policy $\pi_{k}$. If $\pi_{k}(a^{\prime}|s^{\prime})$ differs substantially
    from $\mu(a^{\prime}|s^{\prime})$, out-of-distribution (OOD) actions, which have
    not been explored by $\mu$, can be sampled. Further, the Q-function trained on
    $D_{\mu}$, i.e., $Q_{k+1}^{\pi}$, may erroneously produce over-optimistic values
    for these OOD actions, leading the policy $\pi_{k+1}$ to generate unpredictable
    OOD behaviors. In online RL, such issues are naturally corrected when the agent
    interacts with the environment, attempting the actions it (erroneously) believes
    to be good and observing that in fact they are not. In offline RL, interactions
    with the environment are not accessible, but, alternatively, the over-optimism
    can be controlled by limiting the discrepancy between $\pi$ and $\mu$, as shown
    in Eq. ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based Offline
    Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")). The discrepancy measure
    $D(\cdot||\cdot)$ has multiple candidates. For a comprehensive review, please
    refer to (Levine et al. ([2020](#bib.bib181))). Built upon this basic paradigm
    (i.e., Eq. ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based Offline
    Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))), we introduce some
    practical and representative offline RL algorithms that focus on addressing the
    issue of OOD actions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Policy constraint methods, such as AWR (Peng et al. ([2019b](#bib.bib251)))
    and AWAC (Nair et al. ([2020](#bib.bib227))), choose the KL-divergence as the
    discrepancy measure, for which the policy improvement step in Eq. ([32](#S3.E32
    "In 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) has a closed-form solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\pi_{k+1}=\arg\max_{\pi}\mathbb{E}_{s\sim D_{\mu}}\left[\mathbb{E}_{a\sim\pi(\cdot&#124;s)}Q_{k+1}^{\pi}(s,a)-\lambda(D_{KL}(\pi(\cdot&#124;s)&#124;&#124;\mu(\cdot&#124;s))-\epsilon)\right]$
    |  | (33) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\Rightarrow$ | $\displaystyle\pi_{k+1}(a&#124;s)=\mu(a&#124;s)\exp\left(Q_{k+1}^{\pi}(s,a)/\lambda\right)/Z(s)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'In this expression, $1/Z(s)$ serves as the normalization factor across all
    possible action choices at state $s$; $\lambda$ is the Lagrangian multiplier to
    convert the original constrained optimization problem to an unconstrained one,
    typically set as a constant rather than being optimized. In practice, such an
    policy can be acquired through weighted supervised learning from $D_{\mu}$, where
    $\exp\left(Q_{k+1}^{\pi}(s,a)\right)$ serves as the weight for $(s,a)$. Offline
    RL algorithms based on diffusion models typically adhere to policy constraint
    methods. Therefore, we will provide further details on policy constraint methods
    in Section [7.2](#S7.SS2 "7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Policy penalty methods, such as BRAC (Wu et al. ([2019](#bib.bib356))), utilize
    approximated KL-divergence $\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))=\widehat{\mathbb{E}}_{a\sim\pi(\cdot|s)}\left[\log\pi(a|s)-\log\hat{\mu}(a|s)\right]$,
    where $\hat{\mu}$ is learned via Behavioral Cloning from $D_{\mu}$ to estimate
    $\mu$ and $\widehat{\mathbb{E}}$ denotes the estimated expectation via Monte Carlo
    sampling, and modify the reward function as $\tilde{r}(s,a)=r(s,a)-\lambda\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$
    ($\lambda>0$). In this way, the deviation from $\mu$ is implemented as a penalty
    term in the reward function for the policy learning to avoid deviating from $\mu$
    not just in the current step but also in future steps. This method is usually
    implemented in its equivalent form (Wu et al. ([2019](#bib.bib356))) as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle Q_{k+1}^{\pi}=\arg\min_{Q}\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[Q(s,a)-\left[r+\gamma\left(\mathbb{E}_{a^{\prime}\sim\pi_{k}(\cdot&#124;s^{\prime})}Q_{k}^{\pi}(s^{\prime},a^{\prime})-\lambda\widehat{D}_{KL}(\pi(\cdot&#124;s^{\prime})&#124;&#124;\mu(\cdot&#124;s^{\prime}))\right)\right]\right]^{2};$
    |  | (34) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\qquad\qquad\qquad\pi_{k+1}=\arg\max_{\pi}\mathbb{E}_{s\sim
    D_{\mu}}\left[\mathbb{E}_{a\sim\pi(\cdot&#124;s)}Q_{k+1}^{\pi}(s,a)-\lambda\widehat{D}_{KL}(\pi(\cdot&#124;s)&#124;&#124;\mu(\cdot&#124;s))\right].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: One significant disadvantage of this approach is that it requires explicit estimation
    of the behavior policy, i.e., $\hat{\mu}$ and the estimation error could hurt
    the overall performance of this approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Support constraint methods, such as BCQ (Fujimoto et al. ([2019](#bib.bib100)))
    and BEAR (Kumar et al. ([2019](#bib.bib175))), propose to confine the support
    of the learned policy within that of the behavior policy to avoid OOD actions,
    because constraining the learned policy to remain close in distribution to the
    behavior policy, as in the previous two methods, can negatively impact the policy
    performance, especially when the behavior policy is substantially suboptimal.
    As an example, BEAR proposes to replace the discrepancy constraint in Eq. ([32](#S3.E32
    "In 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) with a support constraint: $\pi\in\{\pi^{\prime}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\
    |\ \pi^{\prime}(a|s)=0\ \text{whenever}\ \mu(a|s)<\epsilon\}$ ⁷⁷7In practice,
    they use a sampled maximum mean discrepancy (MMD, Gretton et al. ([2012](#bib.bib109)))
    between the being-learned policy and behavior policy to implement such a support
    constraint, of which the effectiveness is empirically justified.. With such a
    support constraint, the target actions $a^{\prime}$ used in policy evaluation
    (i.e., the first equation in Eq. ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))),
    would all satisfy $\mu(a|s)\geq\epsilon$ and so are in-distribution actions, that
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Q_{k+1}^{\pi}=\arg\min_{Q}\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[Q(s,a)-\left(r+\gamma\mathbb{E}_{a^{\prime}\sim\pi_{k}(\cdot&#124;s^{\prime})\
    s.t.\ \mu(a&#124;s)\geq\epsilon}\ Q_{k}^{\pi}(s^{\prime},a^{\prime})\right)\right]^{2}$
    |  | (35) |'
  prefs: []
  type: TYPE_TB
- en: 'Pessimistic value methods regularize Q-function directly to avoid overly optimistic
    values for OOD actions, as an alternative to imposing (support or distributional)
    constraints on the policy. As a representative, CQL (Kumar et al. ([2020](#bib.bib176)))
    removes the policy constraint in Eq. ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) and
    modifies the policy evaluation process as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Q_{k+1}^{\pi}=\arg\min_{Q}$ | $\displaystyle\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[Q(s,a)-\left(r+\gamma\mathbb{E}_{a^{\prime}\sim\pi_{k}(\cdot&#124;s^{\prime})}Q_{k}^{\pi}(s^{\prime},a^{\prime})\right)\right]^{2}+$
    |  | (36) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\lambda\left[\mathbb{E}_{s\sim D_{\mu},a\sim\pi_{k}(\cdot&#124;s)}Q(s,a)-\mathbb{E}_{(s,a)\sim
    D_{\mu}}Q(s,a)\right]$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Here, the second term (with $\lambda>0$) can be viewed as a regularizer for
    the policy evaluation process, which minimizes Q-values under the being-learned
    policy distribution, i.e., $\pi_{k}(\cdot|s)$, and maximizes the Q-values for
    $(s,a)$ within $D_{\mu}$. Intuitively, this ensures that high Q-values are only
    assigned to in-distribution actions. Suppose $\widehat{Q}^{\pi}=\lim_{k\rightarrow\infty}Q^{\pi}_{k}$
    and define $Q^{\pi}$ as the true Q-function for $\pi$, they theoretically prove
    that $\mathbb{E}_{a\sim\pi(\cdot|s)}\left[\widehat{Q}^{\pi}(s,a)\right]\leq\mathbb{E}_{a\sim\pi(\cdot|s)}\left[Q^{\pi}(s,a)\right],\
    \forall s\in D_{\mu}$, with a high probability, when $\lambda$ is large enough.
    Thus, this algorithm mitigates the overestimation issue with a theoretical guarantee.
    However, it tends to learn an overly conservative Q function ⁸⁸8According to Theorem
    3.2 in (Kumar et al. ([2020](#bib.bib176))), the minimum value of $\lambda$ is
    linked to $\max_{s\in D_{\mu}}\frac{1}{\sqrt{|D_{\mu}(s)|}}$, where $|D_{\mu}(s)|$
    denotes the frequency of state $s$ in $D_{\mu}$. Given that $D_{\mu}$’s coverage
    could be limited and certain states would have low visitation frequencies, this
    can result in a high value of $\lambda$. However, as shown in Eq. ([36](#S3.E36
    "In 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")), this same $\lambda$ value is
    used in the Q-update for every state, potentially leading to underestimation for
    other states in $D_{\mu}$..'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 An Overview of Applying VAEs in Offline Reinforcement Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A major use of VAEs for offline RL is to estimate the behavior policy from
    the offline data $D_{\mu}$. Specifically, a CVAE (see Eq. ([7](#S2.E7 "In 3rd
    item ‣ 2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) in Section [2.1](#S2.SS1 "2.1 Variational Auto-Encoders
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) is
    adopted for this estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\theta,\phi}\mathbb{E}_{(s,a)\sim D_{\mu}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;a,s)}\left[\log P_{\theta}(a&#124;z,s)\right]-D_{KL}(P_{\phi}(z&#124;a,s)&#124;&#124;P_{\theta}(z&#124;s))\right]$
    |  | (37) |'
  prefs: []
  type: TYPE_TB
- en: 'Corresponding to Eq. ([7](#S2.E7 "In 3rd item ‣ 2.1 Variational Auto-Encoders
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), $s$
    and $a$ here work as the condition variable $c$ and data sample $x$, respectively.
    As introduced in Section [2.1](#S2.SS1 "2.1 Variational Auto-Encoders ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), the objective above
    constitutes a lower bound for $\mathbb{E}_{(s,a)\sim D_{\mu}}\left[\log P_{\theta}(a|s)\right]$,
    i.e., the typical supervised learning objective. After training, actions at state
    $s$ can be sampled from the estimated behavior policy $\hat{\mu}(\cdot|s)$ as:
    $z\sim P_{\theta}(\cdot|s),\hat{a}\sim P_{\theta}(\cdot|s,z)$. In practice, the
    reconstruction term $\log P_{\theta}(a|z,s)$ can be replaced with $-||a-\hat{a}||_{2}^{2}$
    for continuous action spaces, and the prior $P_{\theta}(z|s)$ can simply be chosen
    as $\mathcal{N}(z|0,I)$. Following the $\beta$-VAE (i.e., Eq. ([5](#S2.E5 "In
    1st item ‣ 2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))), a factor $\beta>0$ is usually introduced as the weight
    of the KL term in Eq. ([37](#S3.E37 "In 3.1.2 An Overview of Applying VAEs in
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) to
    balance the two objective terms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the estimated behavior policy $\hat{\mu}$, the policy penalty and support
    constraint offline RL methods introduced in Section [3.1.1](#S3.SS1.SSS1 "3.1.1
    Background on Dynamic-Programming-based Offline Reinforcement Learning ‣ 3.1 Offline
    Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") can be naturally applied. As a representative of support
    constraint methods, BCQ (Fujimoto et al. ([2019](#bib.bib100))) trains such $\hat{\mu}$
    and defines the policy to learn based on $\hat{\mu}$ as $\pi(\cdot|s)=\hat{a}+\xi(\cdot|s,\hat{a})$,
    where $\hat{a}$ is a sample from $\hat{\mu}(\cdot|s)$ and $\xi(\cdot|s,\hat{a})$
    is a bounded and learnable residual term. $\pi$ is trained to maximize the approximated
    Q-values as in standard RL ⁹⁹9For BCQ, multiple Q functions are learned simultaneously,
    and the target value for policy evaluation is specifically designed based on these
    Q functions, which is though not our focus. Please refer to (Fujimoto et al. ([2019](#bib.bib100)))
    for the details.. In this way, the action support of $\pi$ is constrained to be
    close to the behavior policy’s. Furthermore, inspired by (Shamir ([2018](#bib.bib290))),
    AQL (Wei et al. ([2021](#bib.bib348))) proposes to improve the estimation for
    the behavior policy $\mu$ using a residual generative model $W_{1}(W_{2}G(s)+\hat{a})$,
    where $W_{1,2}$ and $G(\cdot)$ constitute a residual network and $\hat{a}\sim\hat{\mu}(\cdot|s)$
    is used as a residul term. They claim that such a residual structure could effectively
    reduce the estimation error (compared with using $\hat{a}$) for the behavior policy
    $\mu$.'
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the benefits of using a VAE as the policy network, compared to feedforward
    networks composed solely of fully-connected layers, VAEs excel at capturing the
    multiple modalities present in $D_{\mu}$, which could be collected by a diverse
    set of policies, by utilizing the latent variable $z$. Also, the action generation
    process, $z\sim\mathcal{N}(0,I)$ and $a\sim P_{\theta}(\cdot|s,z)$, allows for
    stochastic action sampling. Compared to other deep generative models, VAEs may
    be less expressive but are more lightweight than normalizing flows and diffusion
    models and can provide more stable training than GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Based on these background knowledge, we provide a review of VAE-based offline
    RL algorithms in the following subsections, with a particular focus on works whose
    primary novelty lies in the use of VAEs. One category of such algorithms seeks
    to enhance aforementioned offline RL algorithms via the use of VAEs. They either
    modify the learning objective to further mitigate the issue of OOD actions, or
    apply augmentation/conversion to the offline data for improved learning. The other
    category concentrates on extended offline RL setups, such as hierarchical or multi-task
    offline RL, leveraging the fact that the latent variable $z$ can be learned as
    embeddings for tasks or subgoals/subtasks within a task. Consequently, $P_{\theta}(a|s,z)$
    can be interpreted as a task-conditioned policy (for multi-task RL) or a subtask-conditioned
    policy within a hierarchical policy structure (for hierarchical RL).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Addressing the Issue of Out-of-Distribution Actions Using VAEs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As introduced in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), there
    are four categories of algorithms for mitigating the issue of OOD actions. VAEs
    have been used to improve three categories among them, which are detailed as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying support constraints: PLAS (Zhou et al. ([2020](#bib.bib393))) proposes
    that the support constraint can be simply applied to the latent space of a VAE.
    In particular, they first estimate the behavior policy as a CVAE $\hat{\mu}$ with
    Eq. ([37](#S3.E37 "In 3.1.2 An Overview of Applying VAEs in Offline Reinforcement
    Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")). Ideally, after training,
    for latent variables $z$ which have high probabilities under $P_{\theta}(z|s)$,
    the corresponding decoder $P_{\theta}(a|s,z)$ should output high-probability actions
    under the behavior policy distribution (i.e., in-distribution actions), since
    $\mu(a|s)$ is estimated as $\int P_{\theta}(z|s)P_{\theta}(a|s,z)\ dz$. In this
    case, they learn a latent space policy $\pi(z|s)$ and use it in conjunction with
    the pretrained (and fixed) decoder $P_{\theta}(a|s,z)$ as the mapping from $s$
    to $a$. The output of $\pi$ is constrained to $[-\sigma,\sigma]$, i.e., the high
    probability area of the prior distribution $\mathcal{N}(z;0,I)$, to ensure that
    $(P_{\theta}\circ\pi)(a|s)$ outputs in-distribution actions. As introduced in
    Section [5.2](#S5.SS2 "5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing
    Flows in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), NF-based offline RL
    algorithms adopt the same algorithm idea. SPOT (Wu et al. ([2022](#bib.bib355)))
    suggests that the constraint in BEAR (equivalently for a deterministic policy,
    $\log\mu(\pi(s)|s)\geq\epsilon,\forall s\in D_{\mu}$) can be relaxed to $\mathbb{E}_{s\sim
    D_{\mu}}\left[\log\mu(\pi(s)|s)\right]\geq\epsilon^{\prime}$ for practicality.
    Then, the constrained policy improvement step (i.e., the second equation in Eq.
    ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement
    Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))) can be converted to:
    (We use $\pi$ and $Q$ to represent the policy and Q function from now on, for
    simplicity.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi}\mathbb{E}_{s\sim D_{\mu}}\left[Q(s,\pi(s))+\lambda(\log\mu(\pi(s)&#124;s)-\epsilon^{\prime})\right]$
    |  | (38) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $\lambda>0$ is the Lagrangian multiplier. Again, they explicitly model
    $\mu$ as a CVAE $\hat{\mu}=(P_{\phi},P_{\theta})$ and propose to approximate $\log\mu(a|s)$
    as below: ($z^{(l)}\sim P_{\phi}(\cdot|s,a),\ l=1,\cdots,L$.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\log\mu(a&#124;s)\approx\log P_{\theta}(a&#124;s)=\log\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;s,a)}\left[\frac{P_{\theta}(a&#124;s,z)P_{\theta}(z&#124;s)}{P_{\phi}(z&#124;s,a)}\right]\approx\log\left[\frac{1}{L}\sum_{l=1}^{L}\frac{P_{\theta}(a&#124;s,z^{(l)})P_{\theta}(z^{(l)}&#124;s)}{P_{\phi}(z^{(l)}&#124;s,a)}\right]$
    |  | (39) |'
  prefs: []
  type: TYPE_TB
- en: 'As introduced in Section [2.3](#S2.SS3 "2.3 Normalizing Flows ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), normalizing flows enable
    exact density estimation, which can potentially eliminate the need for sample-based
    approximations as in Eq. ([39](#S3.E39 "In 3.1.3 Addressing the Issue of Out-of-Distribution
    Actions Using VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying policy penalty: TD3-CVAE (Rezaeifar et al. ([2022](#bib.bib275)))
    proposes to replace the penalty term in Eq. (LABEL:brac_obj) (i.e, $\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$)
    with a prediction error $b(s,a)=||a-P_{\theta}\circ P_{\phi}(s,a)||$, where $\hat{\mu}=(P_{\phi},P_{\theta})$
    is the pretrained CVAE for estimating $\mu$. Intuitively, if an action $a$ from
    the being-learned policy $\pi(\cdot|s)$ corresponds to a high prediction error
    under $\hat{\mu}(\cdot|s)$, $a$ is probably an OOD action and so $(s,a)$ should
    be assigned with a high penalty. Further, they theoretically show the equivalence
    (under certain conditions) of $b(s,a)$ and a KL-divergence penalty term $D_{KL}(\pi(\cdot|s)||\pi_{b}(\cdot|s))$,
    where $\pi_{b}(\cdot|s)=\text{SoftMax}(-b(s,\cdot)/\tau)$ and $\tau>0$ is a temperature
    parameter. Compared with $\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$, $b(s,a)$
    is easier to approximate and brings superior empirical performance. BRAC+ (Zhang
    et al. ([2021a](#bib.bib384))) points out that estimating the penalty term $D_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$
    as $\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$ in BRAC requires generating
    a large number of samples to reduce the estimation variance. Therefore, they propose
    an upper bound for $D_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$, which has an analytical
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle D_{KL}(\pi(\cdot&#124;s)&#124;&#124;\mu(\cdot&#124;s))\approx\mathbb{E}_{a\sim\pi(\cdot&#124;s)}\left[\log\pi(a&#124;s)-\log\hat{\mu}(a&#124;s)\right]$
    |  | (40) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\mathbb{E}_{a\sim\pi(\cdot&#124;s)}\left[\log\pi(a&#124;s)\right]-\mathbb{E}_{a\sim\pi(\cdot&#124;s)}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;s,a)}\left[\log P_{\theta}(a&#124;s,z)\right]-D_{KL}(P_{\phi}(z&#124;s,a)&#124;&#124;P_{\theta}(z&#124;s))\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{a\sim\pi(\cdot&#124;s),z\sim P_{\phi}(\cdot&#124;s,a)}\left[\log\pi(a&#124;s)-\log
    P_{\theta}(a&#124;s,z)\right]-\mathbb{E}_{a\sim\pi(\cdot&#124;s)}\left[D_{KL}(P_{\phi}(z&#124;s,a)&#124;&#124;P_{\theta}(z&#124;s))\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Here, the inequality is based on the fact that a CVAE $\hat{\mu}=(P_{\phi},P_{\theta})$
    is used to estimate $\mu$ and $\mathbb{E}_{z\in P_{\phi}(\cdot|s,a)}\left[\log
    P_{\theta}(a|s,z)\right]-D_{KL}(P_{\phi}(z|s,a)||P_{\theta}(z|s))$ constitutes
    a variational lower bound for $\log\hat{\mu}(a|s)$, as shown in Eq. ([7](#S2.E7
    "In 3rd item ‣ 2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")). Given that $P_{\theta}(a|s,z)$, $P_{\theta}(z|s)$,
    and $P_{\phi}(z|s,a)$ all have Gaussian outputs and suppose that the policy $\pi(a|s)$
    is also Gaussian, then both $\log\pi(a|s)-\log P_{\theta}(a|s,z)$ and $D_{KL}(P_{\phi}(z|s,a)||P_{\theta}(z|s))$
    have analytical forms, which can reduce the sample variance. However, sampling
    for $a$ and $z$ is still required.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying pessimistic values: CPQ (Xu et al. ([2022a](#bib.bib361))) extends
    the idea of CQL to safe RL, where the policy is trained to maximize its Q-values
    while minimizing the accumulative cost $Q_{c}(s,a)=c(s,a)+\mathbb{E}_{s^{\prime},a^{\prime}\sim\pi(\cdot|s^{\prime})}[Q_{c}(s^{\prime},a^{\prime})]$.
    In particular, they train the cost Q-function $Q_{c}$ to assign high costs to
    OOD actions, such that, by constraining $Q_{c}$ as in standard safe RL algorithms,
    OOD actions can be avoided at the same time. To realize this, the objective for
    $Q_{c}$ is designed as below, which is similar in form with Eq. ([36](#S3.E36
    "In 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{Q_{c}}\mathbb{E}_{(s,a,c,s^{\prime})\sim D_{\mu}}\left[Q_{c}(s,a)-\left(c+\gamma\mathbb{E}_{a^{\prime}\sim\pi(\cdot&#124;s^{\prime})}Q_{c}(s^{\prime},a^{\prime})\right)\right]^{2}-\lambda\mathbb{E}_{s\sim
    D_{\mu},a\sim\tilde{\mu}(\cdot&#124;s)}Q_{c}(s,a)$ |  | (41) |'
  prefs: []
  type: TYPE_TB
- en: Here, $\tilde{\mu}$ is defined based on $\hat{\mu}=(P_{\phi},P_{\theta})$. In
    particular, $\forall a\sim\tilde{\mu}(\cdot|s),\ D_{KL}(P_{\phi}(z|s,a)||P_{\theta}(z|s))\geq
    d$, where $P_{\theta}(z|s)=\mathcal{N}(z;0,I)$ and $d$ is a predefined threshold.
    Intuitively, $\tilde{\mu}(\cdot|s)$ produces OOD actions $a$ of which the corresponding
    posterior distribution $P_{\phi}(z|s,a)$ deviates significantly with its training
    target $P_{\theta}(z|s)$. MCQ (Lyu et al. ([2022](#bib.bib208))) explores mild
    but enough conservatism for offline RL to mitigate the underestimation issue of
    CQL. Their algorithm’s design does not hinge on VAEs; instead, the VAE is employed
    solely for estimating $\mu$ as in previous works, so we do not provide details
    here. For similar reasons, we skip introductions of UAC (Guan et al. ([2023](#bib.bib113)))
    and O-RAAC (Urpí et al. ([2021](#bib.bib324))), both of which can be considered
    as extensions of BCQ, a support constraint method.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 Data Augmentation and Transformation with VAEs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data augmentation: The provided offline data $D_{\mu}$ may have limited coverage
    of the state-action space or lack diversity in behavioral patterns. Consequently,
    constraint- or pessimism-based algorithms may learn sub-optimal policies with
    limited generalization capabilities in the entire environment. In this case, VAEs
    have been used for data augmentation, aiming at improving the coverage or diversity
    of the offline data. (1) ROMI (Wang et al. ([2021](#bib.bib336))) is a model-based
    data augmentation strategy utilizing reverse rollouts. They first learn the backward
    dynamic model $\widehat{\mathcal{T}}_{\text{rev}}(s|s^{\prime},a)$, reward model
    $\hat{r}(s,a)$, and reverse policy $\hat{\mu}_{\text{rev}}(a|s^{\prime})$ from
    $D_{\mu}$ via simple supervised learning, where $s^{\prime}$ denotes the next
    state. Specifically, $\hat{\mu}_{\text{rev}}(a|s^{\prime})$ is modeled with a
    CVAE $(P_{\phi},P_{\theta})$, and its training objective is the same as Eq. ([37](#S3.E37
    "In 3.1.2 An Overview of Applying VAEs in Offline Reinforcement Learning ‣ 3.1
    Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) but to replace $s$ with $s^{\prime}$.
    With a random sample $s_{t+1}$ from $D_{\mu}$, a reverse rollout (of length $h$)
    can be generated as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left[(s_{t-i},a_{t-i},r_{t-i},s_{t+1-i})\ &#124;\ a_{t-i}\sim\hat{\mu}_{\text{rev}}(\cdot&#124;s_{t+1-i}),s_{t-i}\sim\widehat{\mathcal{T}}_{\text{rev}}(\cdot&#124;s_{t+1-i},a_{t-i}),r_{t-i}\sim\hat{r}(s_{t-i},a_{t-i})\right]_{i=0}^{h-1}$
    |  | (42) |'
  prefs: []
  type: TYPE_TB
- en: 'Unlike the forward generation process, such a reverse manner prevents rollout
    trajectories that end in OOD states. Also, the CVAE makes it possible for stochastic
    inference: $z\sim\mathcal{N}(0,I),a\sim P_{\theta}(\cdot|s^{\prime},z)$, which
    improves the diversity. These generated rollouts are then combined with $D_{\mu}$
    for the use of offline RL. This work is closely related to offline model-based
    RL, which is another important branch of offline RL and mainly introduced in Section
    [4.2](#S4.SS2 "4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"). (2) To enable the learned
    policy to generalize to OOD states, SDC (Zhang et al. ([2022](#bib.bib385))) suggests
    training the policy on perturbed states and motivating it to revert to in-distribution
    states from any state deviations. In particular, a forward dynamic model $\widehat{\mathcal{T}}(s^{\prime}|s,a)$
    and a CVAE-based state transition model $\widehat{U}(s^{\prime}|s)$ is learned
    from $D_{\mu}$ through supervised learning. At a state $\tilde{s}$ perturbed from
    $s$, the policy $\pi$ is trained to minimize $\text{MMD}\left(\widehat{\mathcal{T}}(\cdot|\tilde{s},\pi(\cdot|\tilde{s}))||\widehat{U}(\cdot|s)\right)$,
    i.e., to produce actions that can lead it back to the next state $s^{\prime}$
    in the original trajectory from the perturbation $\tilde{s}$. MMD denotes the
    maximum mean discrepancy. (3) Han & Kim ([2022](#bib.bib120)) point out that the
    latent space of a VAE pretrained on $D_{\mu}$ can capture the data distribution
    in $D_{\mu}$. Based on that, they suggest selectively augmenting the data region
    that is sparse in the original dataset through data generation with the VAE. However,
    that paper does not provide details on training the VAE or measuring sparsity
    through the latent space. (4) KFC (Weissenbacher et al. ([2022](#bib.bib350)))
    suggests inferring symmetries (Hambidge ([1967](#bib.bib119))) of the underlying
    dynamics of an environment using a VAE forward prediction model, and applying
    such symmetry transformations to generate new data points as data augmentation.
    This work requires extensive knowledge of control theory, so it will not be discussed
    in depth here.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data conversion: VAEs have been used to transform the states or actions in
    the offline dataset to simplify the learning task. Here, we present two notable
    works in this direction. When the state space is very high-dimensional (e.g.,
    images), directly applying offline RL to the raw data may be challenging. Thus,
    Rafailov et al. ([2021](#bib.bib266)) propose using VAEs to get compact representations
    $z$ of high-dimensional states $s$ to improve the learning efficiency. $z$ effectively
    represents $s$, since the VAE is trained to reconstruct $s$ from $z$ while adhering
    to variational regulations (i.e., the KL term in Eq. ([2](#S2.E2 "In 2.1 Variational
    Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))).
    However, this data conversion manner is not specific to VAEs, and other generative
    models with an encoder-decoder structure, such as the normalizing flows and transformers,
    could potentially be employed for this purpose. On the other hand, SAQ (Luo et al.
    ([2023](#bib.bib205))) proposes to convert continuous actions to discrete ones,
    to make it significantly simpler to implement constraint/regulation-based offline
    RL methods. To realize this, they train a VQ-VAE (introduced in Section [2.1](#S2.SS1
    "2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) on $D_{\mu}$, which can map a continuous action $a$ at
    a given state $s$ to a discrete variable $\tilde{a}$ (through its encoder) for
    training and map a given discrete variable back to the original action space (with
    its decoder) for evaluation. This state-conditioned action discretization scheme
    is learned by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\theta,\phi,e_{1:k}}\mathbb{E}_{(s,a)\sim D_{\mu}}\left[\log P_{\theta}(a&#124;e_{i})-&#124;&#124;\text{sg}\left[P_{\phi}(s,a)\right]-e_{i}&#124;&#124;_{2}^{2}-\beta&#124;&#124;P_{\phi}(s,a)-\text{sg}\left[e_{i}\right]&#124;&#124;_{2}^{2}\right]$
    |  | (43) |'
  prefs: []
  type: TYPE_TB
- en: Here, $e_{1:k}$ is the codebook and represents the $k$ discretized actions;
    $i=\arg\min_{j}||P_{\phi}(s,a)-e_{j}||_{2}$ is the index of the nearest action
    latent for the embedding $P_{\phi}(s,a)$. Applying the pretrained VQ-VAE encoder
    $P_{\phi}$ on $D_{\mu}$ leads to discretized actions, i.e., $(s,a)\rightarrow(s,e_{i})$.
    For discrete action spaces, the estimation of the constraint terms in offline
    RL, such as the approximated behavior policy $\hat{\mu}(\tilde{a}|s)$ and KL divergence
    $\widehat{D}_{KL}(\pi(\tilde{a}|s)||\mu(\tilde{a}|s))$, could be easier and more
    accurate.
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | VAE Type | VAE Usage | Evaluation Task |'
  prefs: []
  type: TYPE_TB
- en: '| BCQ | CVAE | Estimating $\mu$ (Support Constraint) | MuJoCo |'
  prefs: []
  type: TYPE_TB
- en: '| AQL | CVAE | Estimating $\mu$ (Improved Estimation for $\mu$) | D4RL (L)
    |'
  prefs: []
  type: TYPE_TB
- en: '| PLAS | CVAE | Estimating $\mu$ (Support Constraint) | D4RL (L, A, K), Real
    Robot |'
  prefs: []
  type: TYPE_TB
- en: '| SPOT | CVAE | Estimating $\mu$ (Support Constraint) | D4RL (L, M) |'
  prefs: []
  type: TYPE_TB
- en: '| TD3-CVAE | CVAE | Providing prediction errors (Policy Penalty) | D4RL (L,
    A) |'
  prefs: []
  type: TYPE_TB
- en: '| BRAC+ | CVAE | Estimating $\mu$ and $D_{KL}(\pi(a&#124;s)&#124;&#124;\mu(a&#124;s))$
    (Policy Penalty) | D4RL (L) |'
  prefs: []
  type: TYPE_TB
- en: '| CPQ | $\beta$-CVAE | Estimating $\mu$ and using the latent space for OOD
    detection (Pessimistic Value) | MuJoCo |'
  prefs: []
  type: TYPE_TB
- en: '| MCQ | CVAE | Estimating $\mu$ (Pessimistic Value) | D4RL (L) |'
  prefs: []
  type: TYPE_TB
- en: '| UAC | CVAE | Estimating $\mu$ (Support Constraint) | D4RL (L, M, A) |'
  prefs: []
  type: TYPE_TB
- en: '| O-RAAC | $\beta$-CVAE | Estimating $\mu$ (Support Constraint) | D4RL (L)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ROMI | CVAE | Estimating the reverse behavior policy $\mu_{\text{rev}}(a&#124;s^{\prime})$
    (Data Augmentation) | D4RL (L, M, M2d) |'
  prefs: []
  type: TYPE_TB
- en: '| SDC | CVAE | Estimating the state transition model $U(s^{\prime}&#124;s)$
    of $D_{\mu}$ (Data Augmentation) | GridWorld, D4RL (L) |'
  prefs: []
  type: TYPE_TB
- en: '| KFC | VAE | Modelling $U(s^{\prime}&#124;s)$ for inference of dynamic symmetries
    (Data Transformation) | D4RL (L, M, A, K), MetaWorld, RoboSuite |'
  prefs: []
  type: TYPE_TB
- en: '| SAQ | VQ-VAE | Discretizing the action space to simplify the learning (Data
    Conversation) | D4RL (L, M, A, K), Robomimic |'
  prefs: []
  type: TYPE_TB
- en: '| BOReL | CVAE (T) | Embedding MDPs for Multi-task RL | GridWorld, Meta-MuJoCo
    |'
  prefs: []
  type: TYPE_TB
- en: '| OPAL | $\beta$-CVAE (T) | Embedding skills for Hierarchical RL | D4RL (M,
    K) |'
  prefs: []
  type: TYPE_TB
- en: '| TACO-RL | CVAE (T) | Embedding skills for Hierarchical RL | CALVIN, Real
    Robot |'
  prefs: []
  type: TYPE_TB
- en: '| HiGoC | CVAE | Generating subgoals (Hierarchical RL) | D4RL (M), CARLA |'
  prefs: []
  type: TYPE_TB
- en: '| FLAP | CVAE | Generating subgoals (Hierarchical RL) | Real Robot |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Summary of VAE-based offline RL algorithms. In Column 1, we list representative
    (but not all) algorithms in this section. These algorithms are grouped by their
    categories. Regarding the VAE types, $\beta$-CVAE refers to an integration of
    CVAE and $\beta$-VAE, where a weight $\beta$ is added to the KL term in CVAE.
    The annotation (T) means that the VAE is implemented on trajectories rather than
    individual state transitions. The evaluation tasks are listed in Column 4\. Most
    works are evaluated on D4RL (Fu et al. ([2020](#bib.bib99))), which provides offline
    datasets for various tasks, including Locomotion (L), AntMaze (M), Adroit (A),
    Kitchen (K), Maze2d (M2d), etc. MuJoCo (Todorov et al. ([2012](#bib.bib320)))
    and CARLA (Dosovitskiy et al. ([2017](#bib.bib78))) are commonly-used simulators
    for robotic and self-driving tasks, respectively. Meta-MuJoCo (Dorfman et al.
    ([2021](#bib.bib77))) is a multi-task version of MuJoCo. By Real Robot, we mean
    evaluations on real robotic platforms, which vary from one study to another. For
    other benchmarks, we provide their references here: GridWorld (Zintgraf et al.
    ([2020](#bib.bib400))), MetaWorld (Yu et al. ([2019c](#bib.bib379))), RoboSuite
    (Zhu et al. ([2020](#bib.bib396))), Robomimic (Mandlekar et al. ([2021a](#bib.bib209))),
    CALVIN (Mees et al. ([2022b](#bib.bib219))).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.5 Offline Multi-task/Hierarchical RL based on VAEs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multi-task RL and hierarchical RL extend the basic RL setting. In particular,
    multi-task RL (Sodhani et al. ([2021](#bib.bib298))) aims at learning a policy
    that can be directly applied to or quickly adapted to a distribution of tasks.
    Hierarchical RL, on the other hand, learns a hierarchical (two-level) policy for
    complex, long-horizon tasks which can usually be decomposed into a sequence of
    subtasks. In this case, low-level policies can be used to accomplish each subtask,
    while the high-level policy coordinates the subtasks and the use of low-level
    policies. For example, in goal-achieving tasks, a goal-conditioned policy can
    be considered a multi-task policy, as it can reach multiple goals in an environment
    by changing the goal condition. However, if the goal is distant, the entire path
    might be divided into several subgoals by the high-level policy, and to reach
    each subgoal, a corresponding low-level (subgoal-conditioned) policy can be employed.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we formally introduce these two setups — multi-task RL and hierarchical
    RL — and explore how VAEs can be utilized to enhance them. By providing detailed
    introductions of the most notable research in each category, such as BOReL for
    multi-task RL and OPAL & HiGoC for hierarchical RL, our aim is to present the
    fundamental paradigms as a brief tutorial for these specific research directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-task RL: Given a multi-task offline dataset $D_{\mu}^{M}=[[\tau^{i,j}=(s_{0}^{i,j},a_{0}^{i,j},r_{0}^{i,j},\cdots,s_{T}^{i,j})]_{i=1}^{N}]_{j=1}^{M}$,
    where $i$ and $j$ are the indexes for the trajectory and task respectively, offline
    multi-task RL aims at learning a multi-task policy that can be adapted to unseen
    tasks with zero- or few-shot training. The testing tasks are required to be in
    the same distribution as the training ones. As a representative, BOReL (Dorfman
    et al. ([2021](#bib.bib77))) is proposed for the case where the reward and dynamic
    function $(r_{j},\mathcal{T}_{j})$ vary with the task. To train a multi-task policy,
    a straightforward manner is to condition that policy on the task information $(r_{j},\mathcal{T}_{j})$.
    In particular, they adopt the latent variable $z$ ^(10)^(10)10Actually, they adopt
    the mean and variance of the latent variable (i.e., the output of the encoder)
    as the policy conditioner. of a CVAE as a representation of the reward and dynamic
    function, and learns a latent-conditioned policy $\pi(a|s,z)$ as the multi-task
    policy. To this end, the CVAE is trained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\max_{\theta,\phi}\sum_{t=0}^{T-1}\text{ELBO}_{t},\ \text{ELBO}_{t}=\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;\tau_{0:t})}\left[\log P_{\theta}(s_{0:T},r_{0:T-1}&#124;z,a_{0:T-1})-D_{KL}(P_{\phi}(z&#124;\tau_{0:t})&#124;&#124;P_{\theta}(z))\right],$
    |  | (44) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\log P_{\theta}(s_{0:T},r_{0:T-1}&#124;z,a_{0:T-1})=\log
    P_{\theta}(s_{0}&#124;z)+\sum_{t=0}^{T-1}\left[\log P_{\theta}(s_{t+1}&#124;s_{t},a_{t},z)+\log
    P_{\theta}(r_{t}&#124;s_{t},a_{t},s_{t+1},z)\right]$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'In this context, $\tau_{0:T}=(s_{0:T},r_{0:T-1},a_{0:T-1})$; $a_{0:T-1}$ and
    $(s_{0:T},r_{0:T-1})$ can be viewed as $c$ and $x$ in the CVAE framework. As shown
    in (Zintgraf et al. ([2020](#bib.bib400))), $\text{ELBO}_{t}$ constitutes a variational
    lower bound for $\log P_{\theta}(s_{0:T},r_{0:T-1}|a_{0:T-1})$. The second equation
    in Eq. ([44](#S3.E44 "In 3.1.5 Offline Multi-task/Hierarchical RL based on VAEs
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) is derived based on the Markov
    assumption (Puterman ([2014](#bib.bib258))), and it can be observed from this
    equation that $z$ is trained to embed information regarding the initial state
    distribution, reward and dynamic functions, so as to reconstruct the whole task-specific
    trajectory $\tau_{0:T}$. Subsequently, the pretrained $P_{\phi}(z|\tau_{0:t})$,
    which is implemented as a recurrent neural network as in VRNN (see Section [2.1](#S2.SS1
    "2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")), can be applied to $D_{\mu}$ to infer the task embedding
    $z_{t}$ at each time step, leading to a dataset of transitions in the form $((s_{t},z_{t}),a_{t},r_{t})$.
    Note that $(s_{t},z_{t})$ can be viewed as an extended state $\tilde{s}_{t}$,
    and thus standard offline RL algorithms can then be directly applied to this dataset
    to learn a latent-conditioned policy $\pi(a_{t}|(s_{t},z_{t}))$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hierarchical RL: This category of algorithms try to learn a hierarchical policy
    $(\pi_{\text{high}}(z|s),\pi_{\text{low}}(a|s,z))$ from the offline dataset. Intuitively,
    the agent would segment the whole task into a sequence of subtasks or subgoals,
    each of which is denoted by a (continuous or discrete) variable $z$ and accomplished
    by a corresponding subpolicy/skill $\pi_{\text{low}}(a|s,z)$. This hierarchical
    scheme is especially beneficial for complex tasks with long horizons. OPAL (Ajay
    et al. ([2021](#bib.bib4))) is a representive algorithm in this direction. They
    define the horizon of each skill to be $h$ and organize the offline data as a
    set of trajectory segments $D_{\mu}=[\tau^{i}=[s_{0:h-1}^{i},a_{0:h-1}^{i}]]_{i=1}^{N}$.
    Then, they learn the low-level policy $\pi_{\text{low}}$ for different subtasks
    $z$ as the decoder of a CVAE:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{\theta,\phi}\mathbb{E}_{\tau\sim D_{\mu}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;\tau)}\left[\sum_{t=0}^{h-1}\log\pi_{\theta}(a_{t}&#124;s_{t},z)\right]-\beta
    D_{KL}(P_{\phi}(z&#124;\tau)&#124;&#124;P_{\theta}(z&#124;s_{0}))\right],\ \pi_{\text{low}}\triangleq\pi_{\theta}$
    |  | (45) |'
  prefs: []
  type: TYPE_TB
- en: 'This objective is equivalent to a ($\beta$-)CVAE ELBO, where $s_{0}$ and $\tau$
    work as the conditioner $c$ and data $x$ respectively ^(11)^(11)11For a standard
    CVAE, the reconstruction term should be $\log P_{\theta}(\tau|s_{0},z)=\sum_{t=0}^{h-1}\log\pi_{\theta}(a_{t}|s_{t},z)+\sum_{t=0}^{h-2}\log\mathcal{T}(s_{t+1}|s_{t},a_{t})$.
    However, the transition function $\mathcal{T}$ is not trainable and so would not
    influence the gradient calculation.. However, unlike Eq. ([37](#S3.E37 "In 3.1.2
    An Overview of Applying VAEs in Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), here $\pi_{\theta}$ is trained on sets of trajectory segments rather
    than single-step transitions. This is because, as a skill, $\pi_{\theta}$ is expected
    to extend temporally, for example, lasting for $h$ time steps after being selected.
    Also, the prior $P_{\theta}(z|s)$ is not fixed but implemented as a neural network
    that takes $s_{0}$ as input, to make sure the skill choice $z$ is predictable
    (by the high-level policy $\pi_{\text{high}}(z|s)$) given only the initial state
    $s_{0}$. Applying the pretrained $P_{\phi}(z|\tau)$ on $D_{\mu}$ and introducing
    the reward signals, a new dataset $D_{\mu}^{\text{high}}=(s_{0}^{i},z^{i}\sim
    P_{\phi}(\cdot|\tau^{i}),\sum_{t=0}^{h-2}r_{t}^{i},s_{h-1}^{i})_{i=1}^{N}$ can
    be obtained for training $\pi_{\text{high}}(z|s)$ with any offline RL methods.
    With this hierarchical policy $(\pi_{\text{high}},\pi_{\text{low}})$, the decision
    horizon of offline RL is effectively shorten (by a factor of $h$) and so OOD actions
    caused by the accumulated distribution shift can be mitigated. Rosete-Beas et al.
    ([2022](#bib.bib280)) propose TACO-RL, which is a very similar algorithm with
    OPAL but specifically tailored for goal-achieving tasks. HiGoC (Li et al. ([2022](#bib.bib183)))
    is also a hierarchical framework for goal-achieving tasks, where the high-level
    part is a model-based planner ^(12)^(12)12Please refer to (Li et al. ([2022](#bib.bib183)))
    for further details. for generating the subgoal list and the low-level part is
    a goal-conditioned policy trained by offline RL to reach each subgoal sequentially.
    The subgoals are not labeled in the dataset, so the low-level policy is trained
    in an unsupervised manner. Specifically, for $\pi_{\text{low}}(a_{t_{1}}|s_{t_{1}},s_{t_{2}})$,
    $s_{t_{2}}$ ($t_{2}>t_{1}$) is the subgoal and randomly sampled from states after
    $s_{t_{1}}$ in the same trajectory. To be robust to possible OOD subgoals during
    evaluation, a CVAE $m(s_{t}|s_{t-h})=(P_{\phi}(z|s_{t},s_{t-h}),P_{\theta}(s_{t}|z,s_{t-h}))$
    is pretrained on $D_{\mu}$ for generating the subgoal $s_{t}$ conditioned on the
    previous subgoal $s_{t-h}$, where $h$ is a predefined time interval for subgoal
    selections. With this CVAE, a perturbed subgoal can be generated based on the
    sampled one (i.e., $s_{t_{2}}$) as $P_{\theta}(P_{\phi}(s_{t_{2}},s_{t_{2}-h})+\epsilon,s_{t_{2}-h})$
    ($\epsilon$ is a noise vector), which can replace $s_{t_{2}}$ as the subgoal of
    $\pi_{\text{low}}$ for robustness. In cases of high-dimensional states, such as
    images, adding noise to a well-defined low-dimensional embedding space (from VAEs)
    is a more effective and reasonable approach. Further, it’s noteworthy that the
    pretrained decoder $P_{\theta}(s_{t}|z,s_{t-h})$ can also be used for high-level
    planning. Specifically, a subgoal list can be generated from the initial state
    $s_{0}$ by specifying a list of latent variables $z_{1:k}$: $s_{t_{i}}\sim P_{\theta}(\cdot|z_{i},s_{t_{i}-h}),\
    i\in[1,\cdots,k]$. Such a list can then be evaluated by task-relevant objectives.
    In this case, searching for an optimal list $z_{1:k}$ is literally a model predictive
    planning problem and is more efficient than directly searching on the high-dimensional
    state space, i.e., the list of subgoals. FLAP (Fang et al. ([2022](#bib.bib88)))
    adopts a quite similar protocol with HiGoC, where the CAVE $m$ is referred to
    as the affordance model.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Imitation Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we offer an overview of VAE-based IL algorithms. First, we
    provide a tutorial-like overview of the four schemes of VAE-based IL. Based on
    this, we introduce a categorization of all related works based on how VAEs are
    utilized. The use of VAEs focuses on enhancing Behavioral Cloning, either from
    a data or algorithmic perspective. To conclude, we include a table summarizing
    the representative algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Core Schemes of VAE-based Imitation Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imitation Learning (IL) aims at recovering the expert policy $\pi(a|s)$ from
    a set of demonstrations $D_{E}$. Behavioral Cloning (BC) is a straightforward
    and widely-used IL framework (Pomerleau ([1991](#bib.bib256))), which implements
    imitation by supervised learning: $\max_{\pi}\mathbb{E}_{(s,a)\sim D_{E}}\log\pi(a|s)$.
    With its special encoder-decoder structure, the VAE has been utilized to improve
    BC from multiple perspectives. As a summary, here we provide the four schemes
    of VAE-based IL.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scheme (1): Similar with the major use of VAE for offline RL (as introduced
    in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), a
    CVAE conditioned on the state can be directly used to model the expert policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\theta,\phi}\mathbb{E}_{(s,a)\sim D_{E}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;s,a)}\log P_{\theta}(a&#124;s,z)-D_{KL}(P_{\phi}(z&#124;s,a)&#124;&#124;P_{\theta}(z&#124;s))\right]$
    |  | (46) |'
  prefs: []
  type: TYPE_TB
- en: 'As introduced in Section [2.1](#S2.SS1 "2.1 Variational Auto-Encoders ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), this objective constitutes
    a variational lower bound for $\mathbb{E}_{(s,a)\sim D_{E}}\log\pi(a|s)$, i.e.,
    the BC objective. After training, $z\sim P_{\theta}(\cdot|s),a\sim P_{\theta}(\cdot|s,z)$,
    where $P_{\theta}(z|s)$ is a predefined prior and usually set as $\mathcal{N}(z;0,I)$,
    can be used as the policy $a\sim\pi(\cdot|s)$. With the latent variable, the VAE-based
    policy can model stochastic behaviors with diverse modes. Theoretically, $\pi(a|s)=\int
    P_{\theta}(a|s,z)P_{\theta}(z|s)dz$. Rather than using a fixed prior distribution
    $P_{\theta}(z|s)$, Ren et al. ([2020](#bib.bib273)) propose an algorithm to fine-tune
    the prior online for specific tasks by optimizing a generalization bound from
    the PAC-Bayes theory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scheme (2): The second scheme is based on representation learning (RepL), where
    the latent variable $z$ is adopted as a representation of the state $s$ and usually
    can be learned via state reconstructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{\theta,\phi}\mathbb{E}_{s\sim D_{E}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;s)}\text{Rec}_{\theta}(s,z)-D_{KL}(P_{\phi}(z&#124;s)&#124;&#124;P_{\theta}(z))\right],\
    \max_{\pi}\mathbb{E}_{(s,a)\sim D_{E},z\sim P_{\phi}(\cdot&#124;s)}\log\pi(a&#124;z)$
    |  | (47) |'
  prefs: []
  type: TYPE_TB
- en: 'where the reconstruction objective $\text{Rec}_{\theta}(s,z)=\log P_{\theta}(s|z)\
    \text{or}\ -||s-P_{\theta}(z)||_{2}^{2}$. Note that the first objective in Eq.
    ([47](#S3.E47 "In 3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) is usually coupled with extra regularization terms to encourage
    the disentanglement of the learned representation, or enable the fusion of state
    inputs $s$ from multiple modalities, etc. Based on the representation learning,
    a policy $\pi(a|z)$ conditioned on the compact representation $z$ can be learned
    with any IL algorithm. Compared with $s$, $z$ is in a lower dimension and is less
    noisy for decision making. As a result, RepL can effectively reduce the need of
    IL for large amounts of training data. In Eq. ([47](#S3.E47 "In 3.2.1 Core Schemes
    of VAE-based Imitation Learning ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), there are in total
    three functions (besides $P_{\theta}(z)$ which may not be learnable): $P_{\phi}(z|s)$,
    $P_{\theta}(s|z)$, and $\pi(a|z)$. One manner is to pretrain $P_{\phi}(z|s)$ and
    $P_{\theta}(s|z)$ within the VAE framework and then train $\pi(a|z)$ with IL as
    shown in the second term of Eq. ([47](#S3.E47 "In 3.2.1 Core Schemes of VAE-based
    Imitation Learning ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")). When training $\pi$, the parameters
    of $P_{\phi}$ can either be frozen or not. The other manner is to jointly train
    the three functions through an integrated objective, i.e., adding the two objectives
    in Eq. ([47](#S3.E47 "In 3.2.1 Core Schemes of VAE-based Imitation Learning ‣
    3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) together with an adjustable weight $\lambda$. In this
    case, the state reconstruction with a VAE can be viewed as an auxiliary task for
    IL. EIRLI (Chen et al. ([2021a](#bib.bib41))) provides a systematic empirical
    investigation of representation learning for imitation. They find that RepL using
    VAEs can effectively improve the performance of vision-based IL. However, they
    also mention that the relative impact of adding representation learning tends
    to be lower than the impact of adding or removing data augmentations. This may
    be because usual RepL techniques (for images) tend to capture the most visually
    salient axes of variation (e.g., the color, background, or objects) but the action
    choices are often determined by more fine-grained, local cues in the environment,
    which calls for RepL that is more specific to decision making. Additional examples
    in this category include (Lee et al. ([2019](#bib.bib179)); Rahmatizadeh et al.
    ([2018](#bib.bib267))).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scheme (3): VAEs can also be directly applied to expert trajectories $\tau=(s_{0},a_{0},\cdots,s_{T})$
    with the following objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\quad\quad\max_{\theta,\phi}\mathbb{E}_{\tau\sim D_{E}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;\tau)}\left[\log P_{\theta}(\tau&#124;z)\right]-D_{KL}(P_{\phi}(z&#124;\tau)&#124;&#124;P_{\theta}(z))\right],$
    |  | (48) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\log P_{\theta}(\tau&#124;z)=\log P_{\theta}(s_{0}&#124;z)+\sum_{t=0}^{T-1}\left[\log
    P_{\theta}(a_{t}&#124;s_{t},z)+\log P_{\theta}(s_{t+1}&#124;s_{t},a_{t},z)\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The decomposition of $P_{\theta}(\tau|z)$ makes use of the MDP model, where
    the three terms correspond to the initial state distribution, policy distribution,
    and transition dynamic, respectively. $P_{\theta}(s_{0}|z)$ and $P_{\theta}(s_{t+1}|s_{t},a_{t},z)$,
    which may be independent of $z$, are usually defined within the simulator and
    not required to model, so $\log P_{\theta}(\tau|z)$ can be replaced with $\sum_{t=0}^{T-1}\log
    P_{\theta}(a_{t}|s_{t},z)$ in the original objective. It’s worthy noting that
    $P_{\theta}(\tau|z)$ has multiple alternative decomposition forms. For example,
    T-VAE (Lu et al. ([2019](#bib.bib203))) proposes to model it as $\log P_{\theta}(s_{0:T}|z)+\log
    P_{\theta}(a_{0:T-1}|z,s_{0:T})$. Specifically, two RNNs are adopted to generate
    the state and action sequences respectively, leading to a new definition of $\log
    P_{\theta}(\tau|z)$: (The state sequence $\hat{s}_{0:T}$ is generated before the
    action sequence $\hat{a}_{0:T-1}$; the generation of $\hat{s}_{t}$/$\hat{a}_{t}$
    is conditioned on $h_{t-1}^{\hat{s}}$/$h_{t-1}^{\hat{s},\hat{a}}$ which embeds
    the history $\hat{s}_{1:t-1}$/$(\hat{s}_{1:t-1},\hat{a}_{1:t-1})$.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\log P_{\theta}(\tau&#124;z)=\sum_{t=0}^{T}\log P_{\theta}(s_{t}&#124;z,h_{t-1}^{\hat{s}})+\sum_{t=0}^{T-1}\log
    P_{\theta}(a_{t}&#124;z,h_{t-1}^{\hat{s},\hat{a}})$ |  | (49) |'
  prefs: []
  type: TYPE_TB
- en: 'In this way, the whole trajectory can be predicted without interacting with
    the environment/simulator, which is necessary in certain scenarios. In this scheme,
    the embedding $z$ is inferred from the entire trajectory $\tau$, necessitating
    that the encoder $P_{\phi}$ be implemented with specialized architectures, such
    as encoder-only transformers (see Section [2.4](#S2.SS4 "2.4 Transformers ‣ 2
    Background on Deep Generative Models ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) or RNNs (like
    VRNN in Section [2.1](#S2.SS1 "2.1 Variational Auto-Encoders ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")). Different from aforementioned
    two schemes, such a trajectory-conditioned encoder can output embeddings $z$ that
    encapsulate information at the task or subtask level. Thus, the learned $P_{\theta}(a_{t}|s_{t},z)$
    can be viewed as a multi-task policy which varies with the task embedding $z$.
    As a side note, if replacing the decoder $P_{\theta}(\tau|z)$ with $P_{\theta}(\tau^{\prime}|z)$,
    where $(\tau,\tau^{\prime})\sim D_{E}$ are (state-only) expert trajectories and
    $\tau^{\prime}$ is a future trajectory segment of $\tau$, Eq. ([48](#S3.E48 "In
    3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation Learning ‣
    3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))
    can be used for training a trajectory forecasting model, as detailed in RC-VAE
    (Qi et al. ([2020](#bib.bib261))). By using such a temporal target, i.e, the future
    segment, the representation $z$ is forced to contain predictive information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scheme (4): The last scheme is based on the Variational Information Bottleneck
    (VIB) framework (Alemi et al. ([2017](#bib.bib8))), of which the original objective
    function is as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{f:X\rightarrow Y}I(Z,Y;f)\ s.t.\ I(X,Z;f)\leq I_{c}\Rightarrow\max_{f:X\rightarrow
    Y}I(Z,Y;f)-\beta I(X,Z;f)$ |  | (50) |'
  prefs: []
  type: TYPE_TB
- en: 'where $I(\cdot)$ denotes the mutual information, $I_{c}$ is the information
    constraint, and $\beta>0$ is the Lagrangian multiplier. The goal of this framework
    is to learn an encoding $Z$ that is maximally expressive about the target $Y$
    while being maximally compressive about the input $X$. Substituting $X,Y$ with
    $s,a$, the function to learn, i.e., $f$, is then the expert policy $\pi(a|s)$.
    VIB-based IL can be practically solved by the following objective, which is a
    lower bound of Eq. ([50](#S3.E50 "In 3.2.1 Core Schemes of VAE-based Imitation
    Learning ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) (see (Alemi et al. ([2017](#bib.bib8)))):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\phi,\theta,\omega}\mathbb{E}_{(s,a)\sim D_{E}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;s)}\log P_{\theta}(a&#124;z)-\beta D_{KL}(P_{\phi}(z&#124;s)&#124;&#124;P_{\omega}(z))\right]$
    |  | (51) |'
  prefs: []
  type: TYPE_TB
- en: 'This objective is similar with the VAE ELBO (i.e., Eq. ([2](#S2.E2 "In 2.1
    Variational Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"))) in form but with two main differences. First, the decoder $P_{\theta}$
    here can predict variables different from the input of the encoder, whereas in
    VAEs, it is typically trained to reconstruct the input variable. Second, $P_{\omega}(z)$
    is not an assumed prior distribution of $z$ as in VAEs but a variational approximation
    of the marginal distribution of $z$, i.e., $P_{Z}(z)=\int P_{X}(x)P_{\phi}(z|x)dx$,
    and it is usually learned as a neural network. Both Scheme (2) and Scheme (4)
    can be regarded as forms of imitation learning that incorporate effective representation
    learning. After the training, to sample an action $a\sim\pi(\cdot|s)$, the policy
    encoder first compresses the state to a representation, i.e., $z\sim P_{\phi}(\cdot|s)$,
    and then its decoder predicts the action based on $z$, i.e., $a\sim P_{\theta}(\cdot|z)$.
    When dealing with high-dimensional and noisy observations $s$, using an information
    bottleneck can filter out redundant information while retaining essential knowledge
    for action predictions in $z$. A higher compression rate on the input often results
    in better generalization of the learned policy across tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we provide an overview of the applications of VAEs in IL. Most works in
    this direction focus on extending BC. From a data perspective, VAEs can increase
    BC’s data efficiency in scenarios where original, task-specific data is limited,
    or enable the use of multi-modal input data (e.g., from various sensor types).
    Regarding algorithmic advancements, VAEs have been utilized for skill discovery
    to enable hierarchical IL, and to address the causal misidentification issue inherent
    in BC.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Improving Data Efficiency in Imitation Learning with VAEs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: IL, especially BC, requires a large amount of training data to cover possible
    task scenarios for robust performance. However, expert-level demonstrations for
    a specific task, i.e., $D_{E}$, are usually costly to acquire. Therefore, efficient
    data usage in IL is crucial. This can be achieved by leveraging the inherent structures
    within the data, or by gathering and processing task-related data from alternative
    sources for training. The capability of VAEs to extract latent representations
    plays a key role in facilitating these processes. Here, we present several notable
    works in this direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Behavior Retrieval (Du et al. ([2023](#bib.bib79))) is proposed for the case
    where vast task-unlabelled demonstrations are available, i.e., $D_{\text{prior}}$.
    Specifically, $D_{\text{prior}}$ may contain demonstrations for a range of different
    but related tasks (following the same task distribution) or suboptimal behaviors
    for the current task. To this end, they propose to adopt a $\beta$-VAE (i.e.,
    $(P_{\phi},P_{\theta})$ in Eq. ([5](#S2.E5 "In 1st item ‣ 2.1 Variational Auto-Encoders
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))) to
    learn the latent representations of $(s,a)\sim D_{\text{prior}}$. Then, new training
    data that is similar with the ones in $D_{E}$ can be retrieved from $D_{\text{prior}}$
    as the augmentation $D_{\text{ret}}$. The similarity between data points can be
    simply measured based on their latents from the pretrained encoder $P_{\phi}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f((s_{1},a_{1}),(s_{2},a_{2}))=-\&#124;z_{1}-z_{2}\&#124;_{2},~{}(s_{1},a_{1})\in
    D_{\text{prior}},~{}(s_{2},a_{2})\in D_{E},~{}z_{i}\sim P_{\phi}(\cdot&#124;(s_{i},a_{i})).$
    |  | (52) |'
  prefs: []
  type: TYPE_TB
- en: They claim that this method would effectively filter out sub-optimal or task-irrelevant
    data. As a final step, a policy $\pi(a|s)$ can be learned on $D_{E}\cup D_{\text{ret}}$
    through BC. This whole protocol resembles aforementioned Scheme (2).
  prefs: []
  type: TYPE_NORMAL
- en: 'For robotic control tasks, state-only demonstrations from a third-person view
    (e.g., videos from a demonstrator) are usually more available than the ones in
    the first-person view. State observations from the first- and third-person views,
    i.e., $s^{F}$ and $s^{T}$, correspond to the same true environment state $s$.
    Given a set of demonstrations synchronized across different viewpoints $[(s_{i}^{F},s_{i}^{T_{1}},\cdots,s_{i}^{T_{k}})]_{i=1}^{N}$,
    where $T_{1:k}$ denotes $k$ different third-person viewpoints, a common viewpoint-agnostic
    representation is required to make full use of these data. To realize this, VAE-TPIL
    (Shang & Ryoo ([2021](#bib.bib291))) proposes to learn two VAEs for the first-
    and third-person view data respectively, i.e., $(P_{\phi^{F}},P_{\theta^{F}})$
    and $(P_{\phi^{T}},P_{\theta^{T}})$, and to disentangle the latent variable to
    a viewpoint embedding $v$ and a viewpoint-agnostic state embedding $h$. To realize
    this, besides usual reconstruction objective terms, auxiliary objectives are involved:
    ($i\neq j,\ (h,v)\sim P_{\phi}(s)$)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{v}=\mathbb{E}_{s_{i}^{T}}\left[&#124;&#124;P_{\theta}(h_{i}^{T},v_{j}^{T})-s_{i}^{T}&#124;&#124;_{2}\right],\
    \mathcal{L}_{h}=\mathbb{E}_{s^{T_{i}}}\left[&#124;&#124;P_{\theta}(h^{T_{j}},v^{T_{i}})-s^{T_{i}}&#124;&#124;_{2}\right],\
    \mathcal{L}_{F}=\mathbb{E}_{s^{T},s^{F}}\left[&#124;&#124;h^{T}-\text{sg}(h^{F})&#124;&#124;_{2}\right]$
    |  | (53) |'
  prefs: []
  type: TYPE_TB
- en: 'The intuition behind these terms are: (a) The view embeddings $v^{T}_{i}$ and
    $v^{T}_{j}$ of states in the same view $s_{i}^{T}$ and $s_{j}^{T}$ should be semantically
    equivalent and so $P_{\theta}(h_{i}^{T},v_{j}^{T})$ should still recover $s_{i}^{T}$.
    (b) The embeddings $h^{T_{i}}$ and $h^{T_{j}}$ corresponding to the same state
    from different views $T_{i}$ and $T_{j}$ should be interchangeable for reconstructing
    $s^{T_{i}}$, since $h$ is expected to be view-agnostic. (c) For the same reason,
    $h^{T}$ and $h^{F}$ should be similar as they correspond to the same true environment
    state. With the pretrained $P_{\phi^{F}}$ and $P_{\phi^{T}}$, demonstrations across
    various viewpoints can be converted to viewpoint-agnostic latent representations
    (i.e., $h$). Then, a representation-based policy $\pi(a|h)$ can be learned with
    any IL method.'
  prefs: []
  type: TYPE_NORMAL
- en: GIRIL (Yu et al. ([2020b](#bib.bib381))) suggests modelling the underlying transition
    from $(s_{t},a_{t})$ to $s_{t+1}$ in $D_{E}$ using a CVAE, where $(s_{t},a_{t})$
    and $s_{t+1}$ work as the condition $c$ and data sample $x$ respectively, besides
    imitating the policy $\pi(a_{t}|s_{t})$. Then, an intrinsic reward can be defined
    as $r_{t}=||\hat{s}_{t+1}-s_{t+1}||_{2}^{2}$, where $\hat{s}_{t+1}\sim P_{\theta}(\cdot|s_{t},a_{t})$.
    (Offline) RL can be applied to the extended demonstrations $[(s_{t},a_{t},r_{t},s_{t+1})]$
    to further improve the imitator learned via BC, i.e., $\pi(a_{t}|s_{t})$. Intuitively,
    this process encourages the agent to explore states with high prediction errors
    more to reduce the state uncertainty, making it possible to achieve better-than-expert
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Managing Multi-Modal Inputs in Imitation Learning via VAEs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In real-world scenarios, decision-making inputs often come from multiple modalities,
    including RGB images, depth images, 3D point clouds, language instructions, and
    more. As a motivating example, consider the task of navigating a drone in an outdoor
    environment, where multiple modalities emerge due to visual information captured
    by various sensors and factors like the drone’s current pose and location. Coherently
    managing the multi-modal input is essential for the real-life applications of
    IL. In this context, VAEs have proven effective for fusing or unifying inputs
    from multiple modalities based on the use of latent embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'RGBD-VIB (Du et al. ([2022](#bib.bib80))) proposes to fuse multi-modal inputs
    based on their uncertainty for evaluation, ensuring that inputs with higher certainty
    are given more weight in decision-making. To this end, a VAE $(P_{\phi^{i}}(z|s),P_{\theta^{i}}(a|z),P_{\omega^{i}}(z))$
    is trained for each modality $i\in[1,\cdots,N]$ using the VIB framework (i.e.,
    Eq. ([50](#S3.E50 "In 3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2
    Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))). Note that these VAEs share the same action space but
    vary in their state spaces. The uncertainty measure can then be acquired based
    on the pretrained VAEs as $u^{i}(s)=D_{KL}(P_{\phi^{i}}(z|s)||P_{\omega^{i}}(z))$.
    Intuitively, a large uncertainty $u^{i}(s)$ indicates that the current state in
    modality $i$ may be out of the distribution of the training dataset. Finally,
    the action is a weighted sum of the output from each VAE, where the weight assigned
    to each modality is proportional to $\sum_{j=1}^{N}u^{j}(s)-u^{i}(s)$, inversely
    related to the uncertainty.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Demonstrations may come with contexts $c$, such as the task id, language instructions,
    goal images, etc. In this case, a conditional policy $\pi(a|s,c)$ can be learned
    for specific task goals or instructions. MCIL (Lynch & Sermanet ([2021](#bib.bib206)))
    is proposed for the scenario where multiple related contexts $c_{i},i\in[1,\cdots,N]$
    are provided for a task. For a coherent use of these contexts, MCIL learns an
    encoder for each context, i.e., $(P_{\phi^{1}}(z|c_{1}),\cdots,P_{\phi^{N}}(z|c_{N}))$,
    to embed different contexts into a unified latent space, and a decoder $P_{\theta}(a|s,z)$
    as the latent-conditioned policy. The decoder and encoders can be trained end-to-end
    by $\max_{\theta,\phi^{1:N}}\mathbb{E}_{(s,a,c_{i})\sim D_{E},z\sim P_{\phi^{i}}(\cdot|c_{i})}\left[\log
    P_{\theta}(a|s,z)\right]$ ^(13)^(13)13This objective ensembles the VIB framework,
    i.e., Eq. ([50](#S3.E50 "In 3.2.1 Core Schemes of VAE-based Imitation Learning
    ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")), but overlooks the regulation on $z$. Also, training a
    separate encoder for each context is necessary, since dealing with different modalities
    requires different foundation models.. This leads to a policy that generalizes
    over multi-modal contexts: $z\sim P_{\phi^{i}}(\cdot|c_{i}),a\sim P_{\theta}(\cdot|s,z),i\in[1,\cdots,N]$.
    As an example, language instructions are usually costly to annotate, but other
    contexts like the goal images are relatively easy to obtain (from sensors). Through
    such a unified latent space, MCIL enables the use of (large-amount) demonstrations
    from easier sources to aid the learning of a language-conditioned policy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly with MCIL, CM-VAE-BC (Bonatti et al. ([2020](#bib.bib28))) aims at
    learning a joint low-dimensional embedding for multiple data modalities. It utilizes
    CM-VAE (Spurr et al. ([2018](#bib.bib309))), where a pair of encoder-decoder $(P_{\phi^{i}}(z|s),P_{\theta^{i}}(s|z))$
    is trained for each modality $i\in[1,\cdots,N]$ via cross-modality reconstructions
    of which the objective is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\phi^{i},\theta^{j}}\mathbb{E}_{z\sim P_{\phi^{i}}(\cdot&#124;s^{i})}\left[\log
    P_{\theta^{j}}(s^{j}&#124;z)\right]-D_{KL}(P_{\phi^{i}}(z&#124;s^{i})&#124;&#124;P_{\theta^{i}}(z))$
    |  | (54) |'
  prefs: []
  type: TYPE_TB
- en: 'Suppose $N=2$, then $(i,j)$ in the equation above is an enumeration over the
    set $\{(1,1),(1,2),(2,1),(2,2)\}$. Note that the equation above is a variational
    lower bound of $\log P_{\theta^{j}}(s^{j})$ and $s^{i}\ \&amp;\ s^{j}$ corresponds
    to the same true environment state. This cross-modal training regime results in
    a single latent space that allows embedding and reconstructing multiple data modalities.
    After the representation learning, a policy conditioned on the latent variable
    $\pi(a|z)$ can be learned with any IL method (e.g., BC), following aforementioned
    Scheme (2), i.e., Eq. ([47](#S3.E47 "In 3.2.1 Core Schemes of VAE-based Imitation
    Learning ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Skill Acquisition and Hierarchical Imitation Learning through VAEs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the context of IL, skills are meaningful subsequences/patterns within the
    expert trajectories, which can be potentially utilized in multiple tasks. As an
    example, consider IL for cooking tasks, where the tasks can be diverse but often
    have overlapping patterns, such as, slicing, chopping, etc. As introduced in OPAL
    (see Section [3.1.5](#S3.SS1.SSS5 "3.1.5 Offline Multi-task/Hierarchical RL based
    on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), VAEs can be used to
    extract such patterns via learnt latent embeddings (i.e., using Eq. ([45](#S3.E45
    "In 3.1.5 Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) which is a special case of aforementioned Scheme (3)). As in OPAL,
    after training, the decoder $P_{\theta}(a|s,z)$ can be viewed as the policy for
    skill $z$ (i.e., $\pi_{\text{low}}(a|s,z)$), and the encoder $P_{\phi}(z|\tau)$
    can be used to parse the expert data into trajectories of $(s,z)$ pairs for training
    the high-level policy $\pi_{\text{high}}(z|s)$ with any IL method. Then, the hierarchical
    policy can be executed in a call-and-return manner: sampling a skill $z\sim\pi_{\text{high}}(\cdot|s)$,
    executing the corresponding policy $a\sim\pi_{\text{low}}(\cdot|s,z)$ for $h$
    time steps, sampling the next skill, and so on. This whole process can be considered
    as a hierarchical extension of BC. Beyond this straightforward use of VAEs, some
    works have proposed to impose additional structures or properties, such as disentanglement,
    to the latent space for improved skill acquisition (e.g., in interpretability).
    As mentioned in Section [2.1](#S2.SS1 "2.1 Variational Auto-Encoders ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), being disentangled
    means that each dimension/partition of the latent variable is independent and
    semantically meaningful. One way to realize this is to directly incorporate an
    auxiliary loss term to the standard VAE ELBO as regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{\phi,\theta,\omega}\mathcal{L}^{\text{ELBO}}_{\theta,\phi}(D_{E})-\lambda\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E}),$
    |  | (55) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{L}^{\text{ELBO}}_{\theta,\phi}(D_{E})$ usually takes the form
    of Eq. ([45](#S3.E45 "In 3.1.5 Offline Multi-task/Hierarchical RL based on VAEs
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")); $\omega$ is the parameter of
    an auxiliary network; $\lambda>0$ controls the tradeoff between the two loss terms.'
  prefs: []
  type: TYPE_NORMAL
- en: SAILOR (Nasiriany et al. ([2022](#bib.bib228))) defines $\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E})=\mathbb{E}_{(\tau_{1},\tau_{2})\sim
    D_{E}}\left[f_{\omega}\left(\mu(P_{\phi}(\cdot|\tau_{1}),\mu(P_{\phi}(\cdot|\tau_{2})))\right)-t\right]^{2}$,
    where $\tau_{1}$ and $\tau_{2}$ belong to the same trajectory and are separated
    by $t$ time steps, $\mu(P_{\phi}(\cdot|\tau_{1}))$ denotes the mean of the encoder
    output. Such a term can encourage the learned embedding to be predictable (for
    the temporal difference between skills) and consistent, which is important for
    downstream policy learning with these skills. Note that the gradient from $\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E})$
    can be backpropagated to the encoder, allowing this term to shape the learned
    skill embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both OPAL and SAILOR would partition trajectories into segments of length $h$
    (i.e., the assumed skill horizon) and then train a CVAE on these independent segments
    for skill acquisition (i.e., with Eq. ([45](#S3.E45 "In 3.1.5 Offline Multi-task/Hierarchical
    RL based on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))). CKA (Pasula ([2020](#bib.bib244))),
    on the other hand, proposes a regularization term on the entire trajectory. In
    particular, if a trajectory $\tau$ can be decomposed into $m$ skills in a sequence,
    the learned skill embeddings $z_{1:m}$ can be regularized by a mutual information
    term $\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E})=-\mathbb{E}_{\tau\sim
    D_{E},z_{i}\sim P_{\phi}(\cdot|\tau_{i})}I\left(\sum_{i=1}^{m}z_{i};\tau\right)$,
    where $\tau_{i}$ is the $i$-th segment of $\tau$. The intuition is that, as a
    complex behavior, $\tau$ should be a combination of these skills. By maximizing
    the mutual information, the interpretability of the learned embeddings for the
    behavior $\tau$ can be enhanced. The reason to assume a summation structure $\sum_{i=1}^{m}z_{i}$
    as the combination form is to enable a practical estimation of the mutual information
    term (see (Pasula ([2020](#bib.bib244)))). Aforementioned methods either assume
    a fixed skill horizon of $h$ or require that the skill segmentation of the trajectory
    is readily available, thus only learning skill embeddings for each segment. CompILE
    (Kipf et al. ([2019](#bib.bib168))) suggests a method to concurrently infer both
    the skill boundaries and skill embeddings directly from the trajectory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A major benefit of skill acquisition lies in the potential for skills to be
    transferred across multiple related tasks. This transfer can facilitate the learning
    of a multi-task policy, $\pi(a|s,z,c)$, where $z$ and $c$ represent the skill
    and task, respectively. In this case, $(z,c)$ constitutes the policy condition
    and disentanglement between $z$ and $c$ are required for interpretable and controlled
    behavior generation. To this end, TC-VAE (Noseworthy et al. ([2019](#bib.bib234)))
    is proposed for the scenario where the task variables $c$ are provided and task-irrelevant
    skill embeddings $z$ need to be learned. A regularization term $\max_{\phi}\min_{\omega}\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E})=\max_{\phi}\min_{\omega}\mathbb{E}_{(\tau,c)\sim
    D_{E}}||f_{\omega}(\mu(P_{\phi}(\cdot|\tau)))-c||_{1}$ is adopted, where $f_{\omega}$
    is trained to predict the task from the mean of the encoder $\mu(P_{\phi}(\cdot|\tau))$.
    Intuitively, maximizing this objective over $\phi$ will encourage the learned
    latent space $z$ to be non-informative about the task $c$. SKILL-IL (Bian et al.
    ([2022](#bib.bib23))), on the other hand, learns $z$ and $c$ in the meantime as
    the latent variable of a VAE encoded from a set of multi-task demonstrations.
    To encourage the disentanglement, a Gated VAE (see Section [2.1](#S2.SS1 "2.1
    Variational Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) is adopted, where the latent variable is partitioned into two subdomains
    for $z$ and $c$ respectively. Parameters related to subdomain $z$ are updated
    on data comprising different skills but within the same task. Similarly, parameters
    related to subdomain $c$ are trained with data corresponding to the same skill
    but from different tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | VAE Type | VAE Usage | Evaluation Task |'
  prefs: []
  type: TYPE_TB
- en: '| EIRLI | VAE | Representation Learning (2) | dm_control, Procgen, MAGICAL
    |'
  prefs: []
  type: TYPE_TB
- en: '| T-VAE | VAE | Trajectory modeling (3) | 2D Navigation, 2D Circle, Minecraft
    |'
  prefs: []
  type: TYPE_TB
- en: '| RC-VAE | VRNN | Trajectory forecasting (3) | Basketball Tracking, PEMS-SF
    Traffic, Billiard Ball Trajectory |'
  prefs: []
  type: TYPE_TB
- en: '| Behavior Retrieval | $\beta$-VAE | Data augmentation (2) | RoboSuite, PyBullet,
    Real Robot |'
  prefs: []
  type: TYPE_TB
- en: '| VAE-TPIL | VAE | Data augmentation (2) | Minecraft, PyBullet |'
  prefs: []
  type: TYPE_TB
- en: '| GIRIL | CVAE | Data augmentation | OpenAI Atari, Pybullet |'
  prefs: []
  type: TYPE_TB
- en: '| RGBD-VIB | VIB | Handling multi-modal input (4) | Real Robot |'
  prefs: []
  type: TYPE_TB
- en: '| MCIL | VAE | Handling multi-modal input (1) | 3D Playroom |'
  prefs: []
  type: TYPE_TB
- en: '| CM-VAE-BC | CM-VAE | Handling multi-modal input (2) | AirSim, Real Robot
    |'
  prefs: []
  type: TYPE_TB
- en: '| SAILOR | $\beta$-VAE | Skill acquisition (3) | Franka Kitchen, CALVIN |'
  prefs: []
  type: TYPE_TB
- en: '| CKA | VAE | Skill acquisition (3) | PyBullet |'
  prefs: []
  type: TYPE_TB
- en: '| CompILE | VAE | Skill acquisition (3) | GridWorld, dm_control |'
  prefs: []
  type: TYPE_TB
- en: '| TC-VAE | $\beta$-VAE | Skill acquisition (3) | Synthetic Arcs, MIME Pouring
    |'
  prefs: []
  type: TYPE_TB
- en: '| SKILL-IL | Gated VAE | Skill acquisition (2)(3) | Craftworld, Real-world
    Navigation |'
  prefs: []
  type: TYPE_TB
- en: '| RCM-IL | $\beta$-VAE | Addressing causal confusion (2) | OpenAI Gym/MuJoCo/Atari
    |'
  prefs: []
  type: TYPE_TB
- en: '| Masked | $\beta$-VAE | Addressing causal confusion (2) | OpenAI Gym/MuJoCo
    |'
  prefs: []
  type: TYPE_TB
- en: '| OREO | VQ-VAE | Addressing causal confusion (2) | OpenAI Atari, CARLA |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Summary of representative VAE-based IL algorithms. For each algorithm,
    we detail the type of VAE used, how the VAE is applied, and the tasks on which
    it was evaluated. In Column 3, (1)-(4) refer to the 4 schemes of VAE-based IL
    introduced in Section [3.2.1](#S3.SS2.SSS1 "3.2.1 Core Schemes of VAE-based Imitation
    Learning ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"). As for the evaluation tasks, IL algorithms
    has relatively more diverse choices than offline RL algorithms (as shown in Table
    [1](#S3.T1 "Table 1 ‣ 3.1.4 Data Augmentation and Transformation with VAEs ‣ 3.1
    Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")). By Real Robot or Real-world Navigation,
    we mean evaluations on real-world platforms, which vary from one study to another.
    We provide the references of these benchmarks here: dm_control (Tassa et al. ([2018](#bib.bib317))),
    Procgen (Cobbe et al. ([2020](#bib.bib59))), MAGICAL (Toyer et al. ([2020](#bib.bib321))),
    2D Navigation/Circle (Lu et al. ([2019](#bib.bib203))), Minecraft (Guss et al.
    ([2019](#bib.bib116))), Basketball Tracking (Felsen et al. ([2018](#bib.bib90))),
    PEMS-SF Traffic (Dua et al. ([2019](#bib.bib81))), Billiard Ball Trajectory (Fragkiadaki
    et al. ([2016](#bib.bib94))), RoboSuite (Zhu et al. ([2020](#bib.bib396))), PyBullet
    (Coumans & Bai ([2016–2021](#bib.bib63))), OpenAI Gym/Atari/MuJoCo (Brockman et al.
    ([2016](#bib.bib32))), 3D Playroom (Lynch et al. ([2019](#bib.bib207))), AirSim
    (Shah et al. ([2017](#bib.bib289))), Franka Kitchen (Gupta et al. ([2019a](#bib.bib114))),
    CALVIN (Mees et al. ([2022b](#bib.bib219))), GridWorld (Zintgraf et al. ([2020](#bib.bib400))),
    Synthetic Arcs (Noseworthy et al. ([2019](#bib.bib234))), MIME Pouring (Sharma
    et al. ([2018](#bib.bib293))), Craftworld (Devin ([2024](#bib.bib69))), CARLA
    (Dosovitskiy et al. ([2017](#bib.bib78))).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.5 Tackling Causal Confusion in Imitation Learning with VAEs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Causal confusion/misidentification happens when learnt imitator policies become
    strongly correlated on certain nuisance variables present in the task environment,
    rather than identifying the causal variables that expert demonstrators actually
    exploited in decision-making. Thus, more information in the environment could
    yield worse IL performance as there would be more nuisance factors. For instance,
    in a driving task, the driver must brake when encountering obstacles or pedestrians.
    If the demonstration images include brake light indicators on the dashboard, imitators
    might incorrectly associate the braking behavior with these indicators. However,
    evaluation scenarios may not always feature these indicators, leading to potentially
    catastrophic outcomes (such as hitting a pedestrian). Therefore, to be maximally
    robust to distribution shifts (between the training and evaluation scenarios),
    a policy must rely solely on true causes of expert actions, thereby avoiding causal
    misidentification.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a seminal work, RCM-IL (De Haan et al. ([2019](#bib.bib68))) resolves causal
    misidentification in IL by learning a disentangled representation $z\in\mathbb{R}^{d}$
    of the state $s$, each dimension of which represents a disentangled factor of
    variation, and then differentiating between nuisance and actual causal factors
    in $z$. In particular, a $\beta$-VAE is adopted to learn such a disentangled representation
    $z$ following Eq. ([5](#S2.E5 "In 1st item ‣ 2.1 Variational Auto-Encoders ‣ 2
    Background on Deep Generative Models ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) (with $x$
    replaced by $s$). After pretraining the $\beta$-VAE on $D_{E}$, states $s$ in
    $D_{E}$ can be replaced by their corresponding latent embeddings $z$. Each of
    the $d$ dimensions of $z$ could represent either a causal or nuisance factor,
    and it is necessary to mask out the nuisance factors. The problem is then to find
    the correct mask vector $m\in\{0,1\}^{d}$. With $m$, the policy can be defined
    on the masked representations as $\pi(a|m\odot z)$, where $\odot$ denotes element-wise
    multiplication and $m\odot z$ keeps only actual causal factors. De Haan et al.
    ([2019](#bib.bib68)) propose methods for finding the correct $m$ by querying experts
    or interacting with the environment, which does not rely on the use of VAEs and
    so is not detailed here. Masked (Pfrommer et al. ([2023](#bib.bib253))) adopts
    the same protocol as RCM-IL but improves the algorithm for finding the mask vector.
    Specifically, it uses a carefully-designed statistical hypothesis testing algorithm
    to efficiently mask out nuisance variables among the latent dimensions produced
    by the $\beta$-VAE, which does not require expensive queries to the expert or
    environment for intervention. Under certain assumptions, this algorithm is guaranteed
    not to incorrectly mask factors that causally influence the expert. Please see
    (Pfrommer et al. ([2023](#bib.bib253))) for details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, Park et al. ([2021](#bib.bib242)) present OREO to ameliorate
    causal misidentification in IL, of which the main idea is to encourage the policy
    to uniformly attend to all semantic factors in the state to prevent it from strongly
    exploiting certain nuisance factors. In particular, a VQ-VAE is trained on states
    from demonstrations using Eq. ([9](#S2.E9 "In 4th item ‣ 2.1 Variational Auto-Encoders
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), where
    $x$ is replaced with $s\sim D_{E}$. Since there could be multiple (say $n$) semantic
    factors within $s$, the latent representations from the VQ-VAE, i.e., $z_{E}$
    and $z_{D}$, are set to be 2D matrices in $\mathbb{R}^{n\times d}$, where $d$
    is the dimension of the representation for each single factor. After pretraining,
    given a state $s$, its multi-factor representation $z_{D}$ can be acquired by
    querying the closet code for each row of $z_{E}$: ($e_{1:k}$ is the codebook,
    $e_{j}\in\mathbb{R}^{d}$, and $z_{E,i}\in\mathbb{R}^{d}$ is the $i$-th row of
    $z_{E}$.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle z_{E}\sim P_{\phi}(\cdot&#124;s),\ z_{D}=[e_{q(1)}^{T},\cdots,e_{q(n)}^{T}],\
    q(i)=\underset{j\in[1,\cdots,k]}{\operatorname*{arg\,min}}~{}\&#124;z_{E,i}-e_{j}\&#124;_{2}.$
    |  | (56) |'
  prefs: []
  type: TYPE_TB
- en: 'The next step is to learn a policy through BC based on the multi-factor representation
    while reducing the influence of nuisance factors. Specifically, at each state
    $s$, a sample of $k$ binary random variables $M_{i}\in\{0,1\},i\in[1,\cdots,k]$
    are iid sampled from a Bernoulli distribution, and then a mask vector $m$ corresponding
    to $s$ can be created as: $m=[M_{q(1)},\cdots,M_{q(n)}]$, where $q(1),\cdots,q(n)$
    are defined as above. Finally, the IL objective is: $\max_{\pi,f}\mathbb{E}_{(s,a)\sim
    D_{E},m}\log\pi(a|m\odot f(s))$, where $f$ is initialized with the pretrained
    encoder $P_{\phi}$ and updated during the policy training. Suppose the $q(i)$-th
    factor is a nuisance factor that is strongly correlated with decision-making,
    then it would occur frequently in the latent representations, i.e., $e^{T}_{q(i)}$
    in $z_{D}$. By randomly setting the corresponding mask $M_{q(i)}$ to 0, this nuisance
    factor can be effectively dropped out, thereby mitigating the issue of causal
    misidentification. Randomized dropping provides a simple yet effective way to
    regularize the uniform treatment of each generative factor, i.e., either a causal
    variable that explains expert actions or a nuisance variable that just shows strong
    correlation with the demonstrations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The representative VAE-based offline RL and IL algorithms are summarized in
    Table [1](#S3.T1 "Table 1 ‣ 3.1.4 Data Augmentation and Transformation with VAEs
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") and [2](#S3.T2 "Table 2 ‣ 3.2.4
    Skill Acquisition and Hierarchical Imitation Learning through VAEs ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), respectively. We notice that most applications of VAEs in offline
    policy learning are based on the learned embedding $z$. For example, it can be
    used to identify OOD actions (Section [3.1.3](#S3.SS1.SSS3 "3.1.3 Addressing the
    Issue of Out-of-Distribution Actions Using VAEs ‣ 3.1 Offline Reinforcement Learning
    ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    transform high-dimensional states or continuous actions for improved learning
    efficiency (Section [3.1.4](#S3.SS1.SSS4 "3.1.4 Data Augmentation and Transformation
    with VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), represent tasks or
    skills for multi-task or hierarchical learning (Section [3.1.5](#S3.SS1.SSS5 "3.1.5
    Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement Learning
    ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    fuse input from multiple modalities (Section [3.2.3](#S3.SS2.SSS3 "3.2.3 Managing
    Multi-Modal Inputs in Imitation Learning via VAEs ‣ 3.2 Imitation Learning ‣ 3
    Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    provide disentangled representations of the states for tackling causal confusion
    (Section [3.2.5](#S3.SS2.SSS5 "3.2.5 Tackling Causal Confusion in Imitation Learning
    with VAEs ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")), and so on. On the other hand, we note
    that some directions of VAE-based offline policy learning can be further explored.
    For instance, as introduced in Scheme (2) of Section [3.2.1](#S3.SS2.SSS1 "3.2.1
    Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), the
    VAE can be used to extract compact representations of the states, which can then
    be used for learning a representation-based policy $\pi(a|z)$. However, EIRLI
    (Chen et al. ([2021a](#bib.bib41))) suggests that reward- and value-prediction
    would benefit more (than policy learning) from a representation that captures
    mostly coarse-grained visual differences. Thus, it’s worth investigating whether
    VAE-based representation learning would improve offline RL/IL that requires reward-
    or value-predictions. Moreover, RCM-IL and Masked in Section [3.2.5](#S3.SS2.SSS5
    "3.2.5 Tackling Causal Confusion in Imitation Learning with VAEs ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") assume that $\beta$-VAEs can extract well-disentangled representations
    of the states as the basis of mitigating causal confusion. However, as noted in
    Gated VAE (Section [2.1](#S2.SS1 "2.1 Variational Auto-Encoders ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), consistent disentanglement
    with $\beta$-VAEs, such an unsupervised manner, has been demonstrated to be impossible.
    Thus, further improvements can be made regarding tackling causal confusion.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Generative Adversarial Networks in Offline Policy Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to the previous section, we divide the content here into two parts:
    the applications of GANs in IL (Section [4.1](#S4.SS1 "4.1 Imitation Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) and offline RL (Section [4.2](#S4.SS2 "4.2 Offline Reinforcement
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")). There are relatively more GAN-based IL algorithms compared
    to GAN-based offline RL algorithms. Interestingly, GAN-based IL primarily extends
    two fundamental algorithms: GAIL and AIRL, while GAN-based offline RL focuses
    on expanding the model-based offline RL.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Imitation Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we starts with a detailed introduction of the fundamental
    algorithms in this direction: GAIL and AIRL, as a tutorial, and then present extensions
    for each of them. Finally, we provide a spotlight on the integrated use of GANs
    and VAEs for IL, as examples of synthesizing the use of different generative models
    for offline policy learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.1.1 Fundamental GAN-Based Imitation Learning Algorithms: GAIL and AIRL'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Nearly all works regarding applying GANs for IL can be viewed as extensions
    of two fundamental works in this area: GAIL (Ho & Ermon ([2016](#bib.bib133)))
    and AIRL (Fu et al. ([2017](#bib.bib98))). Both works are based on the maximum
    causal entropy inverse RL (MaxEntIRL) framework (Ziebart et al. ([2010](#bib.bib399))),
    of which the objective is as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{c}\left[\min_{\pi}-H(\pi)+\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[c(s,a)]\right]-\mathbb{E}_{(s,a)\sim\rho_{\pi_{E}}(\cdot)}[c(s,a)]$
    |  | (57) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $\rho_{\pi}(s,a)=\pi(a|s)\sum_{t=0}^{\infty}\gamma^{t}P(s_{t}=s|\pi)$
    is the discounted occupancy measure (i.e., visit frequency) of $(s,a)$ when taking
    $\pi$, and $H(\pi)=\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[-\log\pi(a|s)]$ denotes
    the policy entropy. Intuitively, this framework looks for a cost function $c$
    that assigns low costs to the expert policy $\pi_{E}$ and high costs to any other
    policy, thereby allowing the expert policy to be learned via minimizing the expected
    cost, i.e., through the inner RL process shown in Eq. ([57](#S4.E57 "In 4.1.1
    Fundamental GAN-Based Imitation Learning Algorithms: GAIL and AIRL ‣ 4.1 Imitation
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")). $\pi_{E}$ is not directly accessible and usually represented
    as a set of expert demonstrations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GAIL proposes that, to ensure the expressiveness of $c$ while avoiding overfit
    on the set of demonstrations, $c$ can be any real function (i.e., $\mathbb{R}^{\mathcal{S}\times\mathcal{A}}$)
    but an extra regularizer $\psi:\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\rightarrow\mathbb{R}$
    on $c$ should be introduced to Eq. ([57](#S4.E57 "In 4.1.1 Fundamental GAN-Based
    Imitation Learning Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")). This
    $\psi$-regularized MaxEntIRL problem is equivalent to an occupancy measure matching
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\pi}-H(\pi)+\psi^{*}(\rho_{\pi}-\rho_{\pi_{E}}),\ \psi^{*}(\rho_{\pi}-\rho_{\pi_{E}})=\sup_{c\in\mathbb{R}^{\mathcal{S}\times\mathcal{A}}}(\rho_{\pi}-\rho_{\pi_{E}})^{T}c-\psi(c)$
    |  | (58) |'
  prefs: []
  type: TYPE_TB
- en: 'That is, $\psi$-regularized MaxEntIRL implicitly seeks a policy, whose occupancy
    measure is close to the expert’s, as measured by the convex conjugate of the regularizer,
    i.e., $\psi^{*}$. As introduced in Section [2.2](#S2.SS2 "2.2 Generative Adversarial
    Networks ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    GANs provide a scalable manner for distribution matching. GAIL proposes that,
    with a certain design of $\psi$ (i.e., Eq. (13) in (Ho & Ermon ([2016](#bib.bib133)))),
    Eq. ([58](#S4.E58 "In 4.1.1 Fundamental GAN-Based Imitation Learning Algorithms:
    GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) can be converted to
    a GAN objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\pi}\max_{D\in(0,1)^{\mathcal{S}\times\mathcal{A}}}-H(\pi)+\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[\log(1-D(s,a))]+\mathbb{E}_{(s,a)\sim\rho_{\pi_{E}}(\cdot)}[\log
    D(s,a)]$ |  | (59) |'
  prefs: []
  type: TYPE_TB
- en: 'Compared with Eq. ([57](#S4.E57 "In 4.1.1 Fundamental GAN-Based Imitation Learning
    Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")), we can see
    that $c(s,a)$ is defined with the discriminator as $\log(1-D(s,a))$ ^(14)^(14)14We
    equivalently substitute all $D(s,a)$ in (Ho & Ermon ([2016](#bib.bib133))) as
    $1-D(s,a)$ to be in line with GAN, where the true and fake data should be labeled
    as 1 and 0, respectively, by the discriminator $D$.. Also, the policy $\pi$ works
    as the generator $G$, so $D$ and $\pi$ can be alternatively trained as in GANs
    to optimize Eq. ([59](#S4.E59 "In 4.1.1 Fundamental GAN-Based Imitation Learning
    Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")). Specifically,
    in each training episode, $\pi$ is trained for several iterations to maximize
    the expected return $Q(s_{0},a_{0})=\sum_{t=0}^{\infty}\gamma^{t}(-\log(1-D(s_{t},a_{t}))-\lambda\log\pi(a_{t}|s_{t}))$,
    $\forall(s_{0},a_{0})$, with a widely-adopted RL algorithm – TRPO (Schulman et al.
    ([2015](#bib.bib287))) ^(15)^(15)15$\max_{\pi}Q(s_{0},a_{0})=\sum_{t=0}^{\infty}\gamma^{t}(-\log(1-D(s_{t},a_{t}))-\lambda\log\pi(a_{t}|s_{t}))$
    is equivalent to $\min_{\pi}-\lambda H(\pi)+\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[\log(1-D(s,a))]$.
    $\lambda>0$ is introduced here to balance the two objective terms., and then $D$
    is trained for iterations to correctly classify samples from $\pi$ or $\pi_{E}$
    with Eq. ([59](#S4.E59 "In 4.1.1 Fundamental GAN-Based Imitation Learning Algorithms:
    GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")). Compared with the
    original MaxEntIRL, which requires solving a complete RL problem as the inner
    loop for each update of $c$, GAIL ensures both expressiveness and scalability,
    especially useful for tasks in high-dimensional state/action spaces. Further,
    GAIL provides solid theoretical results on the connection between IRL and GANs.
    As for the global optimality and convergence rate of GAIL, Zhang et al. ([2020b](#bib.bib390))
    provide an analysis under the condition of using two-layer neural networks (with
    ReLU between the two layers) as function estimators and applying natural policy
    gradient for policy updates, when the underlying MDP belongs to the class of linear
    MDPs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AIRL also adopts a GAN-like framework as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\max_{D}\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[\log(1-D(s,a))]+\mathbb{E}_{(s,a)\sim\rho_{\pi_{E}}(\cdot)}[\log
    D(s,a)],$ |  | (60) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\qquad\quad\min_{\pi}\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[\log(1-D(s,a))-\log
    D(s,a)]$ |  |'
  prefs: []
  type: TYPE_TB
- en: However, instead of applying a sigmoid function (i.e., $D(s,a)=\frac{\exp(f(s,a))}{\exp(f(s,a))+1}$)
    as the activation function on the last layer output of the discriminator, i.e.,
    $f(s,a)$, as in GAN and GAIL, AIRL adopts $D(s,a)=\frac{\exp(f(s,a))}{\exp(f(s,a))+\pi(a|s)}$.
    They claim that, with this design, AIRL is equivalent to MaxEntIRL, under the
    assumption that trajectories $\tau=\{s_{0},a_{0},\cdots,s_{T},a_{T},s_{T+1}\}$
    follow the Boltzmann distribution (i.e., $P(\tau)\propto\rho_{0}(s_{0})\prod_{t=0}^{T-1}\mathcal{T}(s_{t+1}|s_{t},a_{t})\exp(\gamma^{t}r(s_{t},a_{t}))$).
    However, the derivation provided in (Fu et al. ([2017](#bib.bib98))) is not rigorous
    ^(16)^(16)16In Appendix A.1 of (Fu et al. ([2017](#bib.bib98))), the step $\frac{\partial}{\partial\theta}\log
    Z_{\theta}=\mathbb{E}_{p_{\theta}}\left[\sum_{t=0}^{T}\frac{\partial}{\partial\theta}r_{\theta}(s_{t},a_{t})\right]$
    is questionable. Also, in its Appendix A.2, they erroneously mix up the use of
    $\mu_{t}$ and $\hat{\mu}_{t}$.. Further, they propose to replace $D(s,a)$ with
    $D(s,a,s^{\prime})=\frac{\exp(f(s,a,s^{\prime}))}{\exp(f(s,a,s^{\prime}))+\pi(a|s)}$,
    where $f(s,a,s^{\prime})=g(s)+\gamma h(s^{\prime})-h(s)$. They state that, at
    optimality, $g(s)$ and $h(s)$ can recover the reward and value function, respectively,
    if the real reward function is a function of $s$ only and the environment is deterministic
    ^(17)^(17)17In Appendix A.4 of (Fu et al. ([2017](#bib.bib98))), they claim that
    the global minimum of the discriminator objective is reached when $\pi=\pi_{E}$,
    which is backed up by the original GAN paper (Goodfellow et al. ([2014b](#bib.bib107))).
    However, the objective design of AIRL is significantly distinct from the original
    GAN. Moreover, this claim forms the basis of the proof presented in its Appendix
    C on $g(s)$ and $h(s)$ recovering the reward and value functions respectively.
    . While its theoretical foundation is not robust, AIRL can be considered as a
    GAN-based IL algorithm, drawing inspiration from MaxEntIRL and well substantiated
    by empirical results.
  prefs: []
  type: TYPE_NORMAL
- en: There is an extensive amount of works developed based on GAIL or AIRL. Instead
    of providing details on each of them, we put an emphasis on the representative
    ones by presenting their objective designs and key novelties in tables, and briefly
    enumerate other works for comprehensiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Extensions of GAIL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Table [3](#S4.T3 "Table 3 ‣ 4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), we provide an overview of representative works that extend GAIL.
    They either substitute the original GAN framework with more advanced GAN variants
    (as introduced in Section [2.2](#S2.SS2 "2.2 Generative Adversarial Networks ‣
    2 Background on Deep Generative Models ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) or expand
    the use of GAIL to other IL settings, including the multi-agent, multi-task, hierarchical,
    and model-based scenarios. Moreover, to provide a thorough review, we briefly
    enumerate other related research works as follows, which are further categorized
    by their topics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GAIL: Extensions of GAIL concentrate on enhancing its learning efficiency,
    tackling key challenges such as increasing the sample efficiency for policy learning,
    ensuring balanced training of the discriminator and policy, etc. (1) DGAIL (Zuo
    et al. ([2020](#bib.bib402))) and SAM (Blondé & Kalousis ([2019](#bib.bib24)))
    replace the stochastic policy $a\sim\pi(\cdot|s)$ in GAIL with a deterministic
    one $a=\pi(s)$ and, correspondingly, proposes a DDPG-based (Lillicrap et al. ([2016](#bib.bib191)))
    updating rule for $\pi$. As an off-policy algorithm, DDPG is more sample efficient
    than TRPO (used in the original GAN). BGAIL (Jeon et al. ([2018](#bib.bib151)))
    improves the sample efficiency by approximating the discriminator parameters’
    posterior distribution in the ideal case where correct labels are assigned to
    both demonstrated and generated behaviors. A more accurate estimation of the cost
    function can then be acquired through sampling from that distribution, consequently
    improving the policy training. (2) VAIL (Peng et al. ([2019a](#bib.bib250))) suggests
    limiting information flow in the discriminator of GAIL (or AIRL) using the Variational
    Information Bottleneck framework (Tishby & Zaslavsky ([2015](#bib.bib319))), to
    prevent the discriminator from converging too quickly and supplying uninformative
    gradients to the generator. TRAIL (Zolna et al. ([2020](#bib.bib401))) proposes
    a different manner to regularize the discriminator (through an auxiliary task),
    with the aim to discourage the agent to exploit spurious patterns in observations
    which are associated with the expert label but task-irrelevant. (3) For applications
    demanding reliability and robustness, RS-GAIL (Lacotte et al. ([2019](#bib.bib177)))
    presents a risk-sensitive version of GAIL by introducing a constraint to the original
    GAIL objective, such that the conditional value-at-risk (Duffie & Pan ([1997](#bib.bib82)))
    of the learned policy is at least as well as that of the expert. This algorithm
    is supported by rigorous theoretical validation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-agent/Hierarchical GAIL: IGASIL (Hao et al. ([2019](#bib.bib123))) extends
    GAIL to fully-collaborative multi-agent scenarios. It adopts a similar objective
    design with MAGAIL’s (listed in Table [3](#S4.T3 "Table 3 ‣ 4.1.2 Extensions of
    GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) but suggests two major improvements for
    learning efficiency: training the policy with DDPG for enhanced sample efficiency
    and adopting high-return trajectories from the policy as demonstrations for self-imitation
    learning. Like Option-GAIL introduced in Table [3](#S4.T3 "Table 3 ‣ 4.1.2 Extensions
    of GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), OptionGAN and Directed-Info GAIL
    extend GAIL to the hierarchical learning setting, taking advantage of the Mixture-of-Experts
    framework (Masoudnia & Ebrahimpour ([2014](#bib.bib214))) and directed information
    maximization (Massey et al. ([1990](#bib.bib215))), respectively. However, these
    two algorithms are not suitable for learning from demonstrations segmented by
    sub-tasks. Also, Directed-Info GAIL trains $\pi_{H}$ and $\pi_{L}$ in two separate
    stages, leading to suboptimality of the hierarchical policy $\pi=(\pi_{H},\pi_{L})$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'InfoGAIL: Hausman et al. ([2017](#bib.bib124)) and Peng et al. ([2022](#bib.bib249))
    address the same challenge as InfoGAIL, i.e., IL from multi-modal but unstructured
    demonstrations. Hausman et al. ([2017](#bib.bib124)) achieve the same objective
    function as InfoGAIL but through a distinct perspective; Peng et al. ([2022](#bib.bib249))
    proposes an alternative manner to update $Q$ (in InfoGAIL) through a VAE framework,
    where $Q$ and $\pi$ serve as the encoder and decoder, respectively. Ess-InfoGAIL
    (Fu et al. ([2023](#bib.bib97))) extends InfoGAIL to manage demonstrations from
    a mix of experts, wherein the quantity of demonstrations from each expert is imbalanced.
    Burn-InfoGAIL (Kuefler & Kochenderfer ([2018](#bib.bib174))) improves InfoGAIL
    to reproduce expert behaviors over extended time horizons.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algo (GAN Type) | Objective | Key Novelty |'
  prefs: []
  type: TYPE_TB
- en: '| CGAIL (CGAN) (Zhang et al. ([2019](#bib.bib387))) | $\min_{\pi}\max_{D}-\lambda
    H(\pi(\cdot&#124;c))+\mathbb{E}_{\rho_{\pi_{E}}(\cdot&#124;c)}[\log D(s,a&#124;c)]+$
    $\mathbb{E}_{\rho_{\pi}(\cdot&#124;c)}[\log(1-D(s,a&#124;c))]+$ $\mathbb{E}_{\rho_{\pi_{E}}(\cdot&#124;c)}[\log(1-D(s,a&#124;c^{\prime})]$
    | Recovering a policy $\pi(a&#124;s,c)$ for each condition $c$; Demonstrations
    are shareable among similar task conditions, so the data efficiency can be high;
    $\max_{D}\mathbb{E}_{\rho_{\pi_{E}}(\cdot&#124;c)}[\log(1-D(s,a&#124;c^{\prime})]$
    discourages the mismatch between $c^{\prime}$ and $(s,a)$ for consistency. |'
  prefs: []
  type: TYPE_TB
- en: '| InfoGAIL (InfoGAN) (Li et al. ([2017b](#bib.bib186))) | $\min_{\pi,Q}\max_{D}-\lambda_{1}H(\pi)+\mathbb{E}_{\rho_{\pi}}[\log(1-D(s,a))]+$
    $\mathbb{E}_{\rho_{\pi_{E}}}[\log D(s,a)]-\lambda_{2}L_{I}(\pi,Q)$, $L_{I}(\pi,Q)=\mathbb{E}_{c\sim
    P_{C}(\cdot),a\sim\pi(\cdot&#124;s,c)}\left[\log Q(c&#124;s,a)\right]+H(C)$, $P_{C}(\cdot)$
    is an assumed prior distribution. | Demonstrations are multi-modal but labels
    $c$ are not provided, unlike CGAIL; $L_{I}(\pi,Q)$ is a lower bound of the mutual
    information $I(c;(s,a))$, by maximizing which we can disentangle trajectories
    corresponding to different $c$; Policies for each latent modality $\pi(a&#124;s,c)$
    can then be recovered. |'
  prefs: []
  type: TYPE_TB
- en: '| $f$-GAIL ($f$-GAN) (Zhang et al. ([2020a](#bib.bib388))) | $\min_{\pi}\max_{f^{*}\in\mathcal{F}^{*},T}\mathbb{E}_{\rho_{\pi_{E}}}[T(s,a)]-\mathbb{E}_{\rho_{\pi}}[f^{*}(T(s,a))]$
    $-H(\pi)$, $f^{*}$ is the convex conjugate of $f$ (which defines a certain $f$-divergence)
    and satisfies: convexity and $\inf_{u\in\text{dom}_{f^{*}}}\{f^{*}(u)-u\}=0$.
    Like $\pi$ and $T$, $f^{*}$ is modeled as a neural network but specially designed
    to fulfill its two constraints. | GAIL is a special case of $f$-GAIL, when $T(x)=\log
    D(x),\ f^{*}(x)=-\log(1-e^{x})$; Maximizing over $f^{*}\in\mathcal{F}^{*}$ aims
    to find the largest $f$-divergence between $\rho_{\pi}$ and $\rho_{\pi_{E}}$,
    which can better guide $\pi$ to $\pi_{E}$. |'
  prefs: []
  type: TYPE_TB
- en: '| Triple-GAIL (Triple-GAN) (Fei et al. ([2020](#bib.bib89))) | $\min_{\pi,C}\max_{D}-\lambda_{1}H(\pi)+\mathbb{E}_{\rho_{\pi_{E}}}[\log
    D(s,a,c)]+$ $\lambda_{2}\mathbb{E}_{\rho_{\pi}}[\log(1-D(s,a,c))]+$ $(1-\lambda_{2})\mathbb{E}_{\rho_{C}}[\log(1-D(s,a,c))]+$
    $\lambda_{4}\mathbb{E}_{\rho_{\pi_{E}}}[-\log C(c&#124;s,a)]+\lambda_{5}\mathbb{E}_{\rho_{\pi}}[-\log
    C(c&#124;s,a)]$; $c$ can be viewed as labels for $(s,a)$, provided in demonstrations;
    $C(c&#124;s,a)$ is utilized to classify $(s,a)$, while $\pi(a&#124;s,c)$ is the
    policy for the class $c$. | The development and theoretical validation of Triple-GAIL
    exactly follow Triple-GAN (see Section [2.2](#S2.SS2 "2.2 Generative Adversarial
    Networks ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")).
    $C$ and $\pi$ are trained by matching the approximated joint occupancy measures
    of $(s,a,c)$: $\rho_{\pi}(\cdot)$ and $\rho_{C}(\cdot)$, with $\rho_{\pi_{E}}(\cdot)$.
    The last two objective terms serve as extra supervision for $C$. Viewing $c$ as
    options/skills, $C$ and $\pi$ then constitute a hierarchical policy. |'
  prefs: []
  type: TYPE_TB
- en: '| MGAIL (GAN) (Baram et al. ([2017](#bib.bib19))) | The objective of $D$ is
    the same as the one of GAIL. $\pi(a&#124;s;\theta)$ is trained by maximizing the
    return $J(s_{0},a_{0};\theta)$, where the policy gradient, i.e., $J^{0}_{\theta}$,
    can be approximated recursively as: ($t=T\rightarrow 0$) $J_{\theta}^{t}=R_{a}\pi_{\theta}+\gamma(J_{\theta}^{t+1}+J_{s^{\prime}}^{t+1}f_{a}\pi_{\theta})$,
    $J_{s}^{t}=R_{s}+R_{a}\pi_{s}+\gamma J^{t+1}_{s^{\prime}}(f_{s}+f_{a}\pi_{s})$;
    $f$ is the learned dynamics: $s^{\prime}=f(s,a)$, $R(s,a)$ is the reward defined
    with $D$, $R_{s}\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}\frac{\partial
    R}{\partial s}$ and so on. | This is a model-based version of GAIL. A dynamic
    model $f$ is learned, such that the agent can look one-step ahead to additionally
    adopt the gradient from $f$, i.e., $\gamma J_{s^{\prime}}^{t+1}f_{a}\pi_{\theta}$,
    as part of $J_{\theta}^{t}$. (Typically, $J_{\theta}^{t}=R_{a}\pi_{\theta}+\gamma
    J_{\theta}^{t+1}$.) Thus, fewer expert data and policy samples would be required,
    compared with GAIL. |'
  prefs: []
  type: TYPE_TB
- en: '| MAGAIL (GAN) (Song et al. ([2018](#bib.bib302))) | $\min_{\pi}\max_{D}\mathbb{E}_{\rho_{\pi}}\left[\sum_{i=1}^{N}\log(1-D_{i}(s,a_{i}))\right]+$
    $\mathbb{E}_{\rho_{\pi_{E}}}\left[\sum_{i=1}^{N}\log D_{i}(s,a_{i})\right]$, $\pi=(\pi_{1},\cdots,\pi_{N})$,
    $\pi_{E}=(\pi_{E}^{1},\cdots,\pi_{E}^{N})$. All agents share the state $s$ but
    choose their own action $a_{i}\sim\pi_{i}(\cdot&#124;s)$. The other agents $-i$
    can be viewed as part of the environment for agent $i$. However, if $\pi_{-i}$
    is unknown, the training environment would be non-stationary for $i$, as $\pi_{-i}$
    are also being updated. | A multi-agent extension of GAIL, which learns a $D_{i}$
    and $\pi_{i}$ for each agent. As agents interact with each other, $D_{i}$ and
    $\pi_{i}$ are not learned independently. The paper’s theoretical results assume
    $\pi_{E}^{-i}$ is known, enabling the learning for each agent $i$ to be treated
    separately. Yet, in practice, better management for the non-stationarity than
    MAGAIL is required. |'
  prefs: []
  type: TYPE_TB
- en: '| Option-GAIL (GAN) (Jing et al. ([2021](#bib.bib153))) | $\min_{\pi}\max_{D}-\lambda
    H(\pi)+\mathbb{E}_{\rho_{\pi}}\left[\log(1-D(o^{\prime},s,o,a))\right]$ $+\mathbb{E}_{\rho_{\pi_{E}}}\left[\log
    D(o^{\prime},s,o,a)\right]$, $\pi=(\pi_{H},\pi_{L})$, $o\sim\pi_{H}(\cdot&#124;s,o^{\prime})$,
    $a\sim\pi_{L}(\cdot&#124;s,o)$. The agent decides on its current option (a.k.a.,
    skill) $o$ with $\pi_{H}$ based on $s$ and $o^{\prime}$ (i.e., the previous option),
    and then samples actions $a$ with $\pi_{L}$ subject to $o$. | A hierarchical learning
    version of GAIL. Long-horizon tasks can be segmented to a sequence of sub-tasks,
    each of which can be done with an option $o$ (i.e., sub-policy). This algorithm
    can recover a hierarchical policy, useful for long-horizon tasks, from the expert
    data through matching the occupancy measure of $(o^{\prime},s,o,a)$ between $\pi$
    and $\pi_{E}$, which follows GAIL. An EM version of Option-GAIL is proposed, in
    case the expert’s options are not labeled. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Summary of representative works following GAIL. In Column 1, we list
    the algorithms and the type of GANs they utilize. Their key objectives and novelties
    are listed in Column 2 and 3, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algo (GAN Type) | Objective | Key Novelty |'
  prefs: []
  type: TYPE_TB
- en: '| $f$-IRL (GAN) (Ni et al. ([2020](#bib.bib231))) | $\max_{D}\mathbb{E}_{s\sim\rho_{\pi_{E}}(\cdot)}\left[\log
    D(s)\right]+\mathbb{E}_{s\sim\rho_{\theta}(\cdot)}\left[\log(1-D(s))\right]$,
    $\min_{\theta}L_{f}(\theta)$, $L_{f}(\theta)=D_{f}(\rho_{\pi_{E}}(s)&#124;&#124;\rho_{\theta}(s))$,
    $\rho_{\theta}(s)\propto$ $\int\rho_{0}(s_{0})\prod_{t=0}^{T-1}\mathcal{T}(s_{t+1}&#124;s_{t},a_{t})\exp(\frac{r_{\theta}(s_{t+1})}{\alpha})\eta_{\tau}(s)d\tau$,
    $\eta_{\tau}(s)=\sum_{t=1}^{T}\mathbbm{1}(s_{t}=s)$; The training alternates between
    $\max_{D}$ and $\min_{\theta}$; With the learned reward function $r_{\theta}$,
    $\pi$ is trained by RL. $\min_{\theta}L_{f}(\theta)$ is realized by gradient descents:
    $\nabla_{\theta}L_{f}(\theta)=$ $\frac{1}{\alpha T}\text{cov}_{\tau\sim\rho_{\theta}(\cdot)}\left(\sum_{t=1}^{T}f(u_{t})-f^{\prime}(u_{t})u_{t},\sum_{t=1}^{T}\nabla_{\theta}r_{\theta}(s_{t})\right)$,
    $u_{t}=\frac{\rho_{\pi_{E}}(s_{t})}{\rho_{\theta}(s_{t})}\approx\frac{D(s_{t})}{1-D(s_{t})}$,
    $\text{cov}(\cdot)$ denotes covariance. | This work provides a generalization
    of AIRL by applying the the general $f$-divergence. The reward $r_{\theta}(s)$
    is trained by state marginal matching, i.e., $\min_{\theta}D_{f}(\rho_{\pi_{E}}(s)&#124;&#124;\rho_{\theta}(s))$.
    $D$ is used to estimate $u_{t}$ in $\nabla_{\theta}L_{f}(\theta)$, based on the
    fact: $D^{*}(s)=\rho_{\pi_{E}}(s)/(\rho_{\pi_{E}}(s)+\rho_{\theta}(s))$. So, each
    update of $\theta$ requires a near- optimal $D^{*}$, which is time-consuming.
    |'
  prefs: []
  type: TYPE_TB
- en: '| MA-AIRL (GAN) (Yu et al. ([2019a](#bib.bib377))) | $\max_{D}\sum_{i=1}^{N}\mathbb{E}_{\rho_{\pi_{E}}}\left[\log
    D_{i}(s,a)\right]+\mathbb{E}_{\rho_{\pi}}\left[\log(1-D_{i}(s,a))\right]$, $\min_{\pi_{i}}\mathbb{E}_{\rho_{\pi}}\left[\log(1-D_{i}(s,a))-\log
    D_{i}(s,a)\right],i=1,\cdots,N$; $D_{i}(s,a)=\exp(f(s,a))/(\exp(f(s,a))+\pi_{i}(a_{i}&#124;s))$,
    $a=(a_{1},\cdots,a_{N})$, $\pi=(\pi_{1},\cdots,\pi_{N})$. All agents share a state
    $s$ and have access to action decisions from the other agents, i.e., $a_{-i}$,
    during the training process. | This is a multi-agent version of AIRL, which trains
    $D_{i}$, $\pi_{i}$ and applies AIRL to each agent. States and joint actions are
    shared among agents. Theoretical results are based on Markov games and logistic
    stochastic best response equilibirum. However, viewing $a_{-i}$ as part of the
    environment, the problem and derivations degenerate to the single-agent AIRL case.
    |'
  prefs: []
  type: TYPE_TB
- en: '| MH-AIRL (GAN) (Chen et al. ([2023d](#bib.bib45))) | $\max_{D}\mathbb{E}_{\pi_{E}}\sum_{t=0}^{T-1}\log
    D(\tilde{s}_{t},\tilde{a}_{t}&#124;c)+\mathbb{E}_{\pi}\sum_{t=0}^{T-1}$ $\log(1-D(\tilde{s}_{t},\tilde{a}_{t}&#124;c))$,
    $(\tilde{s}_{t},\tilde{a}_{t})=((o_{t-1},s_{t}),(o_{t},a_{t}))$; $\min_{\pi}\mathbb{E}_{\pi}\left[\sum_{t=0}^{T-1}\log(1-D_{t}^{c})-\log
    D_{t}^{c}\right]-$ $\lambda_{1}I(\tau_{\pi};C)-\lambda_{2}I(\tau_{\pi}\rightarrow
    O_{0:T}&#124;C)$, $\tau_{\pi}=\{(s_{t},a_{t})\}_{t=0:T-1}$, $D_{t}^{c}=\frac{\exp(f(\tilde{s}_{t},\tilde{a}_{t}&#124;c))}{\exp(f(\tilde{s}_{t},\tilde{a}_{t}&#124;c))+\pi(\tilde{a}_{t}&#124;\tilde{s}_{t},c)}$,
    $\pi=(\pi_{H},\pi_{L})$ and $\pi(\tilde{a}_{t}&#124;\tilde{s}_{t},c)=\pi_{H}(o_{t}&#124;s_{t},o_{t-1})\pi_{L}(a_{t}&#124;s_{t},o_{t})$.
    $c$ and $o$ denote the task and option variable, respectively. MH-AIRL can also
    be adopted, when $c$ and $o$ are not labeled in demonstartions, via an Expectation–Maximization
    (EM) design. | This algorithm integrates multi-task learning, hierarchical learning,
    and AIRL. $\pi$ is a hierarchical policy that can be applied to multiple tasks
    by conditioning on corresponding $c$. The objective design can be viewed as AIRL
    on the extended state-action space $(\tilde{s}_{t},\tilde{a}_{t})$. As regularization,
    $\pi$ is trained to maximize the mutual /directed information $I(\tau_{\pi};C)$
    /$I(\tau_{\pi}\rightarrow O_{0:T}&#124;C)$ to build the causal relationship between
    $\pi$ and the task /option variables. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Summary of representative works following AIRL.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Extensions of AIRL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Extensions of AIRL are fewer than those of GAIL, but we follow the content
    structure in Section [4.1.2](#S4.SS1.SSS2 "4.1.2 Extensions of GAIL ‣ 4.1 Imitation
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), providing a summarization table as Table [4](#S4.T4 "Table
    4 ‣ 4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions") and offering
    a comprehensive review as below.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AIRL adopts the function design $f(s,a,s^{\prime})=g(s)+\gamma h(s^{\prime})-h(s)$,
    where $g(s)$ is assumed to recover the environment reward function and $h(s)$
    can be viewed as the potential function for reward shaping (Ng et al. ([1999](#bib.bib229))).
    EAIRL (Qureshi et al. ([2019](#bib.bib263))) proposes to implement $h(s)$ as the
    empowerment function (Mohamed & Rezende ([2015](#bib.bib225))), maximizing which
    would induce an intrinsic motivation for the agent to seek the states that have
    the highest number of future reachable states. However, as claimed in AIRL, $h(s)$
    is designed to recover the value function at optimality, which is different from
    the empowerment function in definition. AIRL has been extended to various IL setups,
    including model-based, off-policy, multi-task, and hierarchical IL. MAIRL (Sun
    et al. ([2021](#bib.bib313))) simply replaces the GAIL objective in MGAIL (introduced
    in Table [3](#S4.T3 "Table 3 ‣ 4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) with the AIRL’s and gets a model-based version of AIRL. Following
    the same intuition as DGAIL and SAM (introduced in Section [4.1.2](#S4.SS1.SSS2
    "4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")), Off-policy-AIRL
    suggests adopting off-policy RL algorithms (specially, SAC (Haarnoja et al. ([2018](#bib.bib117))))
    to train the generator $\pi$ in AIRL for improved sample efficiency. Table [4](#S4.T4
    "Table 4 ‣ 4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions") introduces
    MH-AIRL, which extends AIRL to support both multi-task and hierarchical learning.
    As related works, PEMIRL (Yu et al. ([2019b](#bib.bib378))) and SMILe (Ghasemipour
    et al. ([2019a](#bib.bib102))) are proposed for multi-task AIRL, while oIRL (Venuto
    et al. ([2020](#bib.bib330))) and H-AIRL (Chen et al. ([2023c](#bib.bib44))) focus
    on hierarchical AIRL.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.1.4 Integrating VAEs and GANs for Imitation Learning: A Spotlight'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Lastly, we spotlight some papers that utilize VAEs to enhance GAIL, serving
    as examples of integrating different generative models for advanced imitation
    learning. GANs are recognized for their capacity to generate sharp image samples,
    as opposed to the blurrier samples from VAE models. However, GANs, unlike VAEs,
    are susceptible to the mode collapse problem, meaning that they may only capture
    partial modes in a multi-modal dataset. Therefore, the capabilities of GANs and
    VAEs are highly complementary. As mentioned in Table [3](#S4.T3 "Table 3 ‣ 4.1.2
    Extensions of GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), one approach, such
    as CGAIL and InfoGAIL, for solving the mode collapse issue is to train a discriminator
    and policy for each mode by conditioning them on the mode embedding. Such embeddings
    could be acquired from a pretrained VAE. In particular, a VAE $(P_{\phi},P_{\theta})$
    can be trained to imitate expert trajectories $\tau=(s_{0},a_{0},\cdots,s_{T})\sim
    D_{E}$ through an ELBO objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\theta,\phi}\mathbb{E}_{\tau\sim D_{E}}\left[\mathbb{E}_{c\sim
    P_{\phi}(\cdot&#124;\tau)}\left[\log P_{\theta}(\tau&#124;c)\right]-D_{KL}(P_{\phi}(c&#124;\tau)&#124;&#124;P_{C}(c))\right]$
    |  | (61) |'
  prefs: []
  type: TYPE_TB
- en: 'where $P_{C}(c)$ is the predefined prior distribution, $P_{\phi}(c|\tau)$ and
    $P_{\theta}(\tau|c)$ work as the encoder and decoder of the VAE. $\log P_{\theta}(\tau|c)$
    can be decomposed as $\log\rho_{\theta}(s_{0}|c)+\sum_{t=0}^{T-1}\left[\log\pi_{\theta}(a_{t}|s_{t},c)+\log\mathcal{T}_{\theta}(s_{t+1}|s_{t},a_{t},c)\right]$
    based on the MDP model. Through this unsupervised training process, the VAE is
    expected to capture the multiple modes in the dataset and its encoder $P_{\phi}(c|\tau)$
    can be employed to provide mode embeddings for demonstration trajectories. Then,
    the mode-conditioned discriminator and policy can be trained via an objective
    similar with CGAIL:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\pi}\max_{D}\mathbb{E}_{\tau\sim D_{E},c\sim P_{\phi}(\cdot&#124;\tau)}\left[\frac{1}{T}\sum_{t=0}^{T-1}\log
    D(s_{t},a_{t}&#124;c)]+\mathbb{E}_{\rho_{\pi}(\cdot&#124;c)}[\log(1-D(s,a&#124;c))]-\lambda
    H(\pi(\cdot&#124;c))\right]$ |  | (62) |'
  prefs: []
  type: TYPE_TB
- en: 'The pretrained VAE policy $\pi_{\theta}(a|s,c)$ can be used to initialize the
    GAIL policy $\pi(a|s,c)$. Through further training with GAIL (i.e., Eq. ([62](#S4.E62
    "In 4.1.4 Integrating VAEs and GANs for Imitation Learning: A Spotlight ‣ 4.1
    Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))), the VAE policy can be refined by addressing the tendency
    to produce blurry imitations. Diverse GAIL (Wang et al. ([2017](#bib.bib345)))
    and VAE-ADAIL (Lu & Tompson ([2020](#bib.bib204))) are notable examples of this
    paradigm. A more straightforward manner to integrate the VAE and GAN for IL is
    to add a regularizer $\min_{\pi}\mathbb{E}_{s\sim\rho_{\phi}(\cdot)}D_{KL}(\pi(a|s)||\pi_{\text{VAE}}(a|s))$
    to the GAIL objective, where $\pi_{\text{VAE}}(a|s)$ is the VAE-based policy pretrained
    on $D_{E}$ (see Section [3.2.1](#S3.SS2.SSS1 "3.2.1 Core Schemes of VAE-based
    Imitation Learning ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")). In this way, the GAIL policy
    $\pi$ is encouraged to be close to the potentially multi-modal VAE policy. SAIL
    (Liu et al. ([2020](#bib.bib195))) explores in this direction. Another significant
    advantage of VAEs over GANs is that they can provide semantically meaningful latent
    embeddings for the input data. As mentioned in Section [3.1.4](#S3.SS1.SSS4 "3.1.4
    Data Augmentation and Transformation with VAEs ‣ 3.1 Offline Reinforcement Learning
    ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    such embeddings can be used as transformed training data to lower the difficulty
    of GAIL training. As an example, LAPAL (Wang et al. ([2022b](#bib.bib340))) proposes
    to adopt a CVAE to transform high-dimensional actions into low-dimensional latent
    vectors and apply GAIL on the latent space instead, in order to stabilize and
    accelerate the learning process.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Offline Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we starts with an introduction on model-based offline RL,
    for which both a world model of the environment and policy need to be learned
    from the offline dataset. GANs have been used to improve both aspects, as detailed
    in Section [4.2.2](#S4.SS2.SSS2 "4.2.2 Policy Approximation Using GANs ‣ 4.2 Offline
    Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") and [4.2.3](#S4.SS2.SSS3 "4.2.3 World Model Representation
    through GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Background on Model-based Offline Reinforcement Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Most research works in this category are based on Offline Model-based RL (Janner
    et al. ([2019](#bib.bib148)); Yu et al. ([2020a](#bib.bib380))). Different from
    the model-free case, a parametric dynamic model $\widehat{\mathcal{T}}(s^{\prime}|s,a)$
    and reward model $\hat{r}(s,a)$ need to be learned through supervised learning:
    ($D_{\mu}$: offline data collected by the behavior policy $\mu$.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\widehat{\mathcal{T}}}\mathbb{E}_{(s,a,s^{\prime})\sim D_{\mu}}\left[\log\widehat{\mathcal{T}}(s^{\prime}&#124;s,a)\right],\
    \min_{\hat{r}}\mathbb{E}_{(s,a,r)\sim D_{\mu}}\left[(\hat{r}(s,a)-r)^{2}\right]$
    |  | (63) |'
  prefs: []
  type: TYPE_TB
- en: 'The approximated MDP $\widehat{\mathcal{M}}=(\mathcal{S},\mathcal{A},\widehat{\mathcal{T}},\hat{r},\hat{\rho}_{0}(s),\gamma)$,
    where $\hat{\rho}_{0}(s)$ is the empirical distribution of initial states in $D_{\mu}$,
    can be used as proxies of the real environment, and (short-horizon) trajectories
    can be collected by interacting with $\widehat{\mathcal{M}}$ using the being-learned
    policy $\pi_{\theta}$, forming another dataset $D_{\pi}$. Note that, to mitigate
    the impact from the model bias of $\widehat{\mathcal{M}}$, an estimation of the
    uncertainty in dynamics, i.e., $u(s,a)$, is utilized as a penalty reward term
    to discourage the agent to visit uncertain regions in the state-action space,
    so the reward at $(s,a)$ is calculated as $\tilde{r}(s,a)=\hat{r}(s,a)-\lambda_{r}u(s,a)$,
    where $\lambda_{r}>0$ is a hyperparameter. This conservative way to avoid OOD
    behaviors resemble the policy penalty method in dynamic-programming-based offline
    RL, as introduced in Section [3.1](#S3.SS1 "3.1 Offline Reinforcement Learning
    ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions").
    Typical choices of the uncertainty measure include variance or disagreement of
    estimations from an ensemble of Q-functions or approximated world models (Prudencio
    et al. ([2023](#bib.bib257))). Specifically, $D_{\pi}$ is generated as follows:
    $s_{0}\sim\hat{\rho}_{0}(\cdot),a_{0}\sim\pi_{\theta}(\cdot|s_{0}),\tilde{r}_{0}=\hat{r}(s_{0},a_{0})-\lambda_{r}u(s_{0},a_{0}),s_{1}\sim\mathcal{T}(\cdot|s_{0},a_{0})$,
    and so on. As a final step, offline/off-policy RL algorithms can be applied to
    train the policy $\pi_{\theta}$ on an augmented dataset $D_{\text{aug}}=\lambda_{D}D_{\mu}+(1-\lambda_{D})D_{\pi}$.
    Here, $\lambda_{D}\in[0,1]$ is another hyperparameter. A typical (off-policy)
    actor-critic framework for this process is as below (Levine et al. ([2020](#bib.bib181))):
    ($Q_{\bar{\phi}}$ is the target Q-function.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\theta}\mathbb{E}_{s\sim D_{\text{aug}},a\sim\pi_{\theta}(\cdot&#124;s)}\left[Q_{\phi}(s,a)\right],\
    \min_{\phi}\mathbb{E}_{(s,a,\tilde{r},s^{\prime})\sim D_{\text{aug}}}\left[(Q_{\phi}(s,a)-(\tilde{r}(s,a)+\gamma\mathbb{E}_{a^{\prime}\sim\pi_{\theta}(\cdot&#124;s^{\prime})}Q_{\bar{\phi}}(s^{\prime},a^{\prime}))\right]$
    |  | (64) |'
  prefs: []
  type: TYPE_TB
- en: 'As introduced in Section [2.2](#S2.SS2 "2.2 Generative Adversarial Networks
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), the
    GAN framework consists of a generator $G$ and discriminator $D$ that are trained
    simultaneously to compete against each other. For offline model-based RL, the
    generator $G$ can be trained to approximate the policy or environment models,
    which is further discussed in the following subsections.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Policy Approximation Using GANs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned in Section [3.1](#S3.SS1 "3.1 Offline Reinforcement Learning ‣
    3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    to avoid OOD states and actions, the learned policy $\pi_{\theta}$ should be close
    to the underlying behavior policy $\mu$, which is typically realized through introducing
    a regularization term to the objective for training $\pi_{\theta}$. Thus, the
    first objective in Eq. ([64](#S4.E64 "In 4.2.1 Background on Model-based Offline
    Reinforcement Learning ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) can be improved
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\theta}\lambda_{Q}\mathbb{E}_{s\sim D_{\text{aug}},a\sim\pi_{\theta}(\cdot&#124;s)}\left[Q_{\phi}(s,a)\right]-f(\pi_{\theta},\mu)$
    |  | (65) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda_{Q}>0$ is the regularization coefficient. SGBCQ (Dong et al.
    ([2023](#bib.bib74))) proposes to directly constrain the action distribution of
    $\pi_{\theta}$ at each state to be close to the one of $\mu$ by implementing $f(\pi_{\theta},\mu)$
    as $\mathbb{E}_{s\sim D_{\mu}}\left[d(\pi_{\theta}(\cdot|s)||\mu(\cdot|s))\right]$,
    where $d(\cdot)$ denotes a statistical divergence, resembling the policy constraint
    methods (i.e., Eq. ([33](#S3.E33 "In 3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))).
    As introduced in Section [2.2](#S2.SS2 "2.2 Generative Adversarial Networks ‣
    2 Background on Deep Generative Models ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"), GANs are
    be used to model and minimize the JS divergence between two data distributions.
    Thus, SGBCQ gives out a practical framework to solve Eq. ([65](#S4.E65 "In 4.2.2
    Policy Approximation Using GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) based
    on a CGAN as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\qquad\qquad\max_{D}\mathbb{E}_{(s,a)\sim D_{\mu}}[\log
    D(a&#124;s)+\mathbb{E}_{z\sim P_{Z}(\cdot)}\log(1-D(G(z&#124;s)))],$ |  | (66)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\min_{G}-\lambda_{Q}\mathbb{E}_{s\sim D_{\text{aug}},z\sim
    P_{Z}(\cdot)}\left[Q_{\phi}(s,G(z&#124;s))\right]+\mathbb{E}_{s\sim D_{\mu},z\sim
    P_{Z}(\cdot)}[\log(1-D(G(z&#124;s))]$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Compared with the CGAN objective (i.e., Eq. ([12](#S2.E12 "In 1st item ‣ 2.2
    Generative Adversarial Networks ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))), $s$ and $a$ here work as the conditional information
    $y$ and data point $x$, respectively. Intuitively, the stochastic policy $a\sim\pi_{\theta}(\cdot|s)$
    is implemented as a generator $a=G(z|s),z\sim P_{Z}(\cdot)$ and the generator
    is trained to maximize the expected Q-values and fool the discriminator $D$ at
    the same time. Involving such a GAN training process encourages the policy to
    generate behaviors close to the demonstrated ones in distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SDM-GAN (Yang et al. ([2022b](#bib.bib370))) follows a similar protocol but
    chooses to regularize the stationary $(s,a)$ distribution of $\pi_{\theta}$ towards
    the offline dataset to avoid OOD cases. Specifically, they define $f(\pi_{\theta},\mu)$
    in Eq. ([66](#S4.E66 "In 4.2.2 Policy Approximation Using GANs ‣ 4.2 Offline Reinforcement
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) as $d(\rho^{\mathcal{T}}_{\mu}(\cdot)||\rho^{\mathcal{T}}_{\pi_{\theta}}(\cdot))$,
    where $\rho^{\mathcal{T}}_{\mu}(s,a)$ can be approximated as the empirical distribution
    of $(s,a)$ in $D_{\mu}$, $\rho^{\mathcal{T}}_{\pi_{\theta}}(s,a)=\lim_{T\rightarrow\infty}\frac{1}{T+1}\sum_{t=0}^{T}P(s_{t}=s,a_{t}=a|s_{0}\sim\rho_{0}(\cdot),a_{t}\sim\pi_{\theta}(\cdot|s_{t}),s_{t+1}\sim\mathcal{T}(\cdot|s_{t},a_{t}))$
    is the stationary $(s,a)$ distribution of $\pi_{\theta}$ under the real dynamic
    $\mathcal{T}$. Assuming the dynamic function $\widehat{\mathcal{T}}$ is well-fitted,
    $d(\rho^{\mathcal{T}}_{\mu}(\cdot)||\rho^{\mathcal{T}}_{\pi_{\theta}}(\cdot))$
    is upper bounded by: (Please refer to (Yang et al. ([2022b](#bib.bib370))) for
    the derivations and definitions of the function classes $\mathcal{G}_{1,2}$.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sup_{g\sim\mathcal{G}_{1}}&#124;\mathbb{E}_{(s,a)\sim\rho^{\mathcal{T}}_{\mu}(\cdot)}[g(s,a)-\mathbb{E}_{s^{\prime}\sim\widehat{\mathcal{T}}(\cdot&#124;s,a),a^{\prime}\sim\pi_{\theta}(\cdot&#124;s^{\prime})}g(s^{\prime},a^{\prime})]&#124;+\sup_{g\sim\mathcal{G}_{2}}&#124;\mathbb{E}_{(s,a)\sim\rho^{\mathcal{T}}_{\mu}(\cdot)}[g(s,a)]-\mathbb{E}_{s\sim\rho^{\mathcal{T}}_{\mu}(\cdot),a\sim\pi_{\theta}(\cdot&#124;s)}g(s,a)]&#124;$
    |  | (67) |'
  prefs: []
  type: TYPE_TB
- en: 'The equation above is in a similar form with the Wasserstein distance (defined
    in Section [2.2](#S2.SS2 "2.2 Generative Adversarial Networks ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) between samples stored in $D_{\mu}$
    and generated by $\pi_{\theta}$, which inspires the use of GANs for the estimation.
    In particular, the objective is as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\qquad\ \ \max_{D}\mathbb{E}_{(s,a)\sim D_{\mu}}[\log
    D(s,a)]+\mathbb{E}_{(s,a)\sim D_{G}}[\log(1-D(s,a))],$ |  | (68) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\min_{G}-\lambda_{Q}\mathbb{E}_{s\sim D_{\text{aug}},z\sim
    P_{Z}(\cdot)}\left[Q_{\phi}(s,G(z&#124;s))\right]+\mathbb{E}_{(s,a)\sim D_{G}}[\log(1-D(s,a)]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Here, inspired by Eq. ([67](#S4.E67 "In 4.2.2 Policy Approximation Using GANs
    ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")), $D_{G}$ contains $(s,a)/(s^{\prime},a^{\prime})$
    pairs collected in this way: $s\sim D_{\mu},a\sim\pi_{\theta}(\cdot|s),s^{\prime}\sim\widehat{\mathcal{T}}(\cdot|s,a),a^{\prime}\sim\pi_{\theta}(\cdot|s^{\prime})$,
    where $a\sim\pi_{\theta}(\cdot|s)$ is implemented as $a=G(z|s),z\sim P_{Z}(\cdot)$.
    The only difference between Eq. ([66](#S4.E66 "In 4.2.2 Policy Approximation Using
    GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) and ([68](#S4.E68 "In
    4.2.2 Policy Approximation Using GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4
    Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))
    is thus the way to generate samples. For Eq. ([66](#S4.E66 "In 4.2.2 Policy Approximation
    Using GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), the $(s,a)$ pairs
    are simply generated by $s\sim D_{\mu},a\sim\pi_{\theta}(\cdot|s)$. The same authors
    have also proposed a model-free variant of this algorithm (Yang et al. ([2022c](#bib.bib371))),
    sharing similar motivations and algorithm designs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AMPL (Yang et al. ([2022d](#bib.bib372))) adopts the same objective (i.e.,
    Eq. ([68](#S4.E68 "In 4.2.2 Policy Approximation Using GANs ‣ 4.2 Offline Reinforcement
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))) for training $\pi_{\theta}$, but proposes that the environment
    models $\widehat{\mathcal{T}}$ and $\hat{r}$ can be periodically updated with
    $\pi_{\theta}$ by collecting new data with $\pi_{\theta}$ and applying supervised
    learning (e.g., with Eq. ([63](#S4.E63 "In 4.2.1 Background on Model-based Offline
    Reinforcement Learning ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"))) to update
    $\widehat{\mathcal{T}}$ and $\hat{r}$. Interestingly, they theoretically show
    that the objective for $\pi_{\theta}$, which combines the Q-function and distribution
    discrepancy, approximates an upper bound for $-J(\pi_{\theta},\mathcal{M})$, i.e,
    the negative expected return of the policy on the real MDP. On the other hand,
    DASCO (Vuong et al. ([2022](#bib.bib335))) proposes that the two terms in the
    objective of $\pi_{\theta}$ may conflict with each other, since fooling the discriminator
    requires mimicking all in-distribution actions, which can be suboptimal, but maximizing
    the Q function would mean avoiding low-return behaviors. Thus, they propose to
    introduce an auxiliary generator $G_{\text{aux}}$ to generate low-return samples
    and modify the objective as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{G,G_{\text{aux}}}\max_{D}\mathbb{E}_{(s,a)\sim D_{\mu}}[\log D(s,a)]+\mathbb{E}_{(s,a)\sim
    D_{G,G_{\text{aux}}}}[\log(1-D(s,a))]-\lambda_{Q}\mathbb{E}_{s\sim D_{\text{aug}},z\sim
    P_{Z}(\cdot)}\left[Q_{\phi}(s,G(z&#124;s))\right]$ |  | (69) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, samples in $D_{G,G_{\text{aux}}}$ are generated as: $s\sim D_{\mu},z\sim
    P_{Z}(\cdot),a=(G(z|s)+G_{\text{aux}}(z|s))/2$. Both $G$ and $G_{\text{aux}}$
    are adopted to mimic the demonstrations, but only the primary generator $G$ is
    trained to maximize the Q-values. Theoretically, they prove that the optimal solution
    for the primary generator $G$ will maximize the probability mass of in-support
    samples that maximize the Q-function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All aforementioned algorithms utilize a very similar objective design, which
    can be viewed as a paradigm of using GANs for policy approximations in offline
    RL. GOPlan (Wang et al. ([2023a](#bib.bib338))) proposes an advantage-weighted
    CGAN objective, which is different in form from previous ones, for capturing the
    multi-modal action distribution in goal-conditioned offline planning:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{G}\max_{D}\mathbb{E}_{g\sim D_{\mu}}\left[\mathbb{E}_{(s,a)\sim
    D_{\mu}^{g}}[w(s,a,g)\log D(s,a&#124;g)]+\mathbb{E}_{s\sim D_{\mu}^{g},z\sim P_{Z}(\cdot)}[\log(1-D(s,G(z&#124;s)&#124;g))]\right]$
    |  | (70) |'
  prefs: []
  type: TYPE_TB
- en: Here, $D_{\mu}^{g}$ denotes the partition of $D_{\mu}$ that takes $g$ as the
    goal; $w(s,a,g)=\exp(A^{\mu}(s,a,g))$ denotes the weight function, where $A^{\mu}(s,a,g)$
    is a separately trained advantage function. This mechanism encourages the policy
    to produce actions that closely resemble high-quality (i.e., high-advantage) actions
    from the offline dataset, but no theoretical guarantee is provided.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 World Model Representation through GANs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Beyond policy generation, GANs can also be used to approximate the environment
    models $\widehat{\mathcal{M}}$, as illustrated in MOAN (Yang et al. ([2023a](#bib.bib367))),
    TS (Hepburn & Montana ([2022](#bib.bib130))), S2P (Cho et al. ([2022](#bib.bib56))),
    which is expected to outperform traditional supervised learning methods (i.e.,
    using Eq. ([63](#S4.E63 "In 4.2.1 Background on Model-based Offline Reinforcement
    Learning ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))). To be specific, MOAN
    proposes the following objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\quad\max_{D}\mathbb{E}_{(s,a,r,s^{\prime})\sim D_{\mu}}[\log
    D(s,a,r,s^{\prime})]+\mathbb{E}_{(s,a)\sim D_{\mu},(\hat{r},\hat{s}^{\prime})\sim
    G(\cdot&#124;s,a)}[\log(1-D(s,a,\hat{r},\hat{s}^{\prime}))],$ |  | (71) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\min_{G}-\lambda_{N}\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[-\log G(r,s^{\prime}&#124;s,a)\right]+\mathbb{E}_{(s,a)\sim D_{\mu},(\hat{r},\hat{s}^{\prime})\sim
    G(\cdot&#124;s,a)}[\log(1-D(s,a,\hat{r},\hat{s}^{\prime}))]$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'This objective design is similar with Eq. ([68](#S4.E68 "In 4.2.2 Policy Approximation
    Using GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) in form, but replaces
    the Q-function term with a negative log-likelihood (i.e., $-\log G(r,s^{\prime}|s,a)$)
    that is commonly used for environment model learning. In this CGAN framework,
    the conditional generator $G(\cdot|s,a)$ is learned to predict $s^{\prime}$ and
    $r$, working as the approximated reward $\hat{r}(s,a)$ and transition function
    $\mathcal{T}(s^{\prime}|s,a)$. In particular, the generator $G(\cdot|s,a)=\mathcal{N}(\text{mean}(s,a),\text{std}(s,a))$
    is implemented as a stochastic Gaussian model, and the standard deviation of the
    predictions, i.e., $\text{std}(s,a)$, can be used as uncertainty measure (i.e.,
    $u(s,a)$ in Section [4.2.1](#S4.SS2.SSS1 "4.2.1 Background on Model-based Offline
    Reinforcement Learning ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")). MOAN proposes
    to further include the confidence level given by the discriminator, i.e., $u(s,a)=\text{std}(s,a)+\sqrt{2(1-D_{(\hat{r},\hat{s}^{\prime})\sim
    G(\cdot|s,a)}(s,a,\hat{r},\hat{s}^{\prime}))}$. Intuitively, if the variance of
    the estimation is high or the discriminator classifies the prediction as generated
    data, i.e., $D\rightarrow 0$, the uncertainty level at this point $(s,a)$ should
    also be high. However, this objective design is not backed up by theoretical analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Different from MOAN, TS and S2P are not limited to model-based offline RL.
    TS proposes to stitch high-value segments from different trajectories together
    to form higher-quality trajectories for offline RL. To realize this, they model
    the distribution $P(a,r,s^{\prime}|s)=P(s^{\prime}|s)P(a|s^{\prime},s)P(r|a,s^{\prime},s)$,
    that is, searching for a potential next state $s^{\prime}$ from the neighbourhood
    of $s$, which has a higher probability $P(s^{\prime}|s)$ and value $V(s^{\prime})$
    than its original next state, and then identifying the most probable intermidiate
    $a,\ r$. In particular, they learn $P(s^{\prime}|s),\ P(a|s^{\prime},s),\ P(r|a,s^{\prime},s)$
    with the traditional supervised learning, CVAE, and WGAN (see Section [2.2](#S2.SS2
    "2.2 Generative Adversarial Networks ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")), respectively. The WGAN objective is: ($G(z|s,a,s^{\prime})$
    works as the approximation of $P(r|a,s^{\prime},s)$.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{G}\max_{D}\mathbb{E}_{(s,a,s^{\prime},r)\sim D_{\mu}}[D(s,a,s^{\prime},r)]-\mathbb{E}_{(s,a,s^{\prime})\sim
    D_{\mu},z\sim P_{Z}(\cdot)}[D(s,a,s^{\prime},G(z&#124;s,a,s^{\prime}))]$ |  |
    (72) |'
  prefs: []
  type: TYPE_TB
- en: 'This algorithm can be viewed as a model-based data augmentation method for
    offline RL using generative models. S2P is a model learning approach for vision-based
    offline RL, where they first learn the approximated transition and reward model
    (i.e., $\widehat{T}(s^{\prime}|s,a),\ \hat{r}(s,a)$) for the underlying states
    through traditional supervised learning and then a generator to synthesize the
    image $I/I^{\prime}$ that perfectly represents the corresponding state $s/s^{\prime}$.
    The generator is conditioned on both the current state and previous image, i.e.,
    $I^{\prime}=G(z|s,I)$, and trained within a WGAN framework. Finally, we briefly
    introduce an interesting work, i.e., IOM (Qi et al. ([2022](#bib.bib260))), on
    offline model-based optimization, which cannot be categorized as either policy
    or world model approximation. They propose to address the distributional shift
    in offline decision-making by enforcing invariance between the learned representations
    in the source and target domains. Specially, they adopt an LSGAN (introduced in
    Section [2.2](#S2.SS2 "2.2 Generative Adversarial Networks ‣ 2 Background on Deep
    Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")), where the generator works as
    the representation function of the data and the discriminator tries to discriminate
    between representations from the source and target domains. In this way, the discrepancy
    between distributions of representations under the source and target domains can
    be minimized.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | GAN Type | GAN Usage | Evaluation Task |'
  prefs: []
  type: TYPE_TB
- en: '| SGBCQ | CGAN | Policy | D4RL (L) |'
  prefs: []
  type: TYPE_TB
- en: '| SDM-GAN | GAN | Policy | D4RL (L, M, A) |'
  prefs: []
  type: TYPE_TB
- en: '| AMPL | GAN | Policy | D4RL (L, M, A) |'
  prefs: []
  type: TYPE_TB
- en: '| DASCO | GAN (auxiliary generator) | Policy | D4RL (L, M) |'
  prefs: []
  type: TYPE_TB
- en: '| GOPlan | CGAN (advantage-weighted) | Policy | MuJoCo Robotic Manipulation
    |'
  prefs: []
  type: TYPE_TB
- en: '| MOAN | GAN | Environment Model | D4RL (L) |'
  prefs: []
  type: TYPE_TB
- en: '| TS | WGAN | Environment Model | D4RL (L) |'
  prefs: []
  type: TYPE_TB
- en: '| S2P | WGAN | Environment Model | dm_control |'
  prefs: []
  type: TYPE_TB
- en: '| IOM | LSGAN | Representation Function | Design-Bench |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Summary of GAN-based offline RL algorithms. Most algorithms in this
    category have been evaluated on D4RL (Fu et al. ([2020](#bib.bib99))), which provides
    multiple datasets for data-driven RL tasks, including Locomotion (L), AntMaze
    (M), Adroit (A), etc. MuJoCo Robotic Manipulation (Yang et al. ([2023b](#bib.bib369)))
    provides offline datasets for a series of goal-conditioned robotic manipulation
    tasks, several of which can be utilized to assess the OOD generalization capabilities
    of the policy. dm_control (Tassa et al. ([2018](#bib.bib317))) includes 6 environments,
    which are typically used for vision-based RL benchmarks. Design-Bench (Trabucco
    et al. ([2022](#bib.bib322))) includes tasks specific for data-driven offline
    model-based optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [5](#S4.T5 "Table 5 ‣ 4.2.3 World Model Representation through
    GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), we provide a summary
    of GAN-based offline RL algorithms. GANs can be employed for both policy and world
    model approximations in model-based offline RL, primarily due to its use in minimizing
    distributional discrepancy.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Normalizing Flows in Offline Policy Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The applications of Normalizing Flows (NFs) in offline policy learning are
    less frequent compared to other deep generative models, particularly for NF-based
    offline RL methods. Thus, we slightly broaden the scope of the RL part to include
    online RL methods that utilize offline datasets in Section [5.2](#S5.SS2 "5.2
    Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Imitation Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned in Section [2.3](#S2.SS3 "2.3 Normalizing Flows ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), NFs can convert a simple
    prior distribution $P_{Z}(z)$ on the latent space into a potentially intricate
    target distribution $P_{X}(x)$ through a sequence of bijections. NFs allow efficient
    sampling (i.e., via the generation direction $z\rightarrow x$) and exact density
    estimation (i.e., via the normalizing direction $x\rightarrow z$). In particular,
    for data generation, a sample $z\sim P_{Z}(\cdot)$ is drawn from the latent space,
    and then the data sample can be generated as $x=F^{-1}(z)=G(z)$. For the other
    direction, computing the density of a point $x$ is accomplished by computing the
    density of its corresponding latent variable $z=F(x)$ and multiplying the associated
    Jacobian determinant $|\det D(F(x))|$, i.e., $P_{X}(x)=P_{Z}(F(x))|\det D(F(x))|$
    where $P_{Z}(\cdot)$ is known.'
  prefs: []
  type: TYPE_NORMAL
- en: NF-based IL works either employ NFs for exact density estimation or leverage
    their proficiency in modelling complex policies to manage challenging task scenarios.
    Next, we present a comprehensive review of works from both categories.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Exact Density Estimation in Imitation Learning Using Normalizing Flows
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As a paradigm, NF-based IL methods that adopt NFs as density estimators usually
    start with standard IL objectives, and optimize them as RL problems wherein the
    rewards necessitate estimation of specific distributions. In this case, NFs are
    introduced to model those distributions and provide exact density inference for
    reward calculation, which greatly enhance the training stability and performance.
    To be specific, we present some representative works here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As proposed in Ghasemipour et al. ([2019b](#bib.bib103)), most IL methods can
    be viewed as matching the agent’s state-action distribution with the expert’s,
    by minimizing some f-divergence $D_{f}$. CFIL (Freund et al. ([2023](#bib.bib96)))
    realizes IL by minimizing the KL divergence between the agent’s and expert’s state-action
    occupancy measure (as defined in Section [4.1.1](#S4.SS1.SSS1 "4.1.1 Fundamental
    GAN-Based Imitation Learning Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\operatorname*{arg\,min}_{\pi}D_{KL}(\rho_{\pi}(s,a)&#124;&#124;\rho_{E}(s,a))=\operatorname*{arg\,max}_{\pi}\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}\left[\log\frac{\rho_{E}(s,a)}{\rho_{\pi}(s,a)}\right]=\operatorname*{arg\,max}_{\pi}J(\pi,r=\log\frac{\rho_{E}}{\rho_{\pi}})$
    |  | (73) |'
  prefs: []
  type: TYPE_TB
- en: 'where $J(\pi,r)$ denotes the expected return (Sutton & Barto ([2018](#bib.bib315)))
    of the policy $\pi$ under the reward function $r$. MAF (Papamakarios et al. ([2017](#bib.bib239)))
    is used to model the distributions $\rho_{E}(s,a)$ and $\rho_{\pi}(s,a)$ based
    on which the rewards can be acquired. Specifically, $(s,a)$ can be viewed as $x$
    in the NF framework and the densities at $(s,a)$, i.e., $\rho_{E}(s,a)$ and $\rho_{\pi}(s,a)$,
    can be estimated using Eq. ([18](#S2.E18 "In 2.3 Normalizing Flows ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")). However, rather than
    optimizing these two NFs independently with corresponding MLE objectives (i.e.,
    Eq. ([19](#S2.E19 "In 2.3 Normalizing Flows ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"))), they couple the modelling of these
    two distributions based on the optimality point of the Donsker-Varadhan form of
    the KL divergence Donsker & Varadhan ([1976](#bib.bib76)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $D_{KL}(\rho_{\pi}(s,a)&#124;&#124;\rho_{E}(s,a))=\sup_{f:S\times A\rightarrow\mathbb{R}}\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}\left[f(s,a)\right]-\log\mathbb{E}_{(s,a)\sim\rho_{E}(\cdot)}\left[e^{f(s,a)}\right]$
    |  | (74) |'
  prefs: []
  type: TYPE_TB
- en: 'In the equation above, optimality occurs with $f^{*}(s,a)=\log\frac{\rho_{\pi}(s,a)}{\rho_{E}(s,a)}+C$
    where $C\in\mathbb{R}$. Thus, through maximizing the right-hand side of the equation
    above, the log ratio used as the reward function can be recovered. Specifically,
    they model $f$ as $f_{\psi,\phi}(s,a)=\log\rho^{\psi}_{\pi}(s,a)-\log\rho^{\phi}_{E}(s,a)$
    with the two flows $\rho^{\psi}_{\pi}$ and $\rho^{\phi}_{E}$. Given this improved
    estimator, the overall IL objective can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\operatorname*{arg\,max}_{\pi}\min_{\rho^{\psi}_{\pi},\rho^{\phi}_{E}}\log\mathbb{E}_{(s,a)\sim
    D_{E}(\cdot)}\left[\frac{\rho_{\pi}^{\psi}(s,a)}{\rho_{E}^{\phi}(s,a)}\right]-\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}\left[\log\frac{\rho_{\pi}^{\psi}(s,a)}{\rho_{E}^{\phi}(s,a)}\right]$
    |  | (75) |'
  prefs: []
  type: TYPE_TB
- en: In this case, the learning alternates between $\max_{\pi}$ and $\min_{\rho_{\pi}^{\phi},\rho_{E}^{\psi}}$.
    $\max_{\pi}$ is realized with an RL process using $\log\frac{\rho_{E}^{\psi}(s,a)}{\rho_{\pi}^{\phi}(s,a)}$
    as the reward function. While, $\rho_{E}^{\psi}(s,a)$ and $\rho_{\pi}^{\phi}(s,a)$
    are updated with expert data (i.e., $(s,a)\sim D_{E}$) and rollout data collected
    by $\pi$ (i.e., $(s,a)\sim\rho_{\pi}(\cdot)$).
  prefs: []
  type: TYPE_NORMAL
- en: 'IL-flOw (Chang et al. ([2022](#bib.bib37))) focuses on Learning from Observations
    (LfO), where the agent only gets access to a dataset of state sequences. This
    work also starts with the KL divergence but processes to decouple the policy optimization
    from the reward learning to improve the training stability. In particular, the
    learning objective for the policy $\pi$ is to maximize $-D_{KL}(P_{\pi}(s^{\prime}|s)||P_{E}(s^{\prime}|s))$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}_{s_{0:T}\sim P_{\pi}}\left[\sum_{t=0}^{T-1}\left(\log P_{E}(s_{t+1}&#124;s_{t})-\log
    P_{\pi}(s_{t+1}&#124;s_{t})\right)\right]=\mathbb{E}_{s_{0:T}\sim P_{\pi}}\left[\sum_{t=0}^{T-1}(\log
    P_{E}(s_{t+1}&#124;s_{t})+H(\pi(\cdot&#124;s_{t})))\right]$ |  | (76) |'
  prefs: []
  type: TYPE_TB
- en: 'The equality holds when environment dynamics are deterministic and invertible.
    This can be optimized with RL by setting the reward as $r_{t}=\log P_{E}(s_{t+1}|s_{t})$
    while maximizing the entropy of the policy, i.e., $H(\pi(\cdot|s_{t}))$. As a
    separate stage, before the RL training, $P_{E}(s^{\prime}|s)$ is modeled with
    a conditional NF (i.e., conditioning the transformation function $F$ on a fixed
    state variable $s$: $s^{\prime}=F(z|s),\ z\sim P_{Z}(\cdot)$) based on a dataset
    of demonstrations $D_{E}$. Specifically, they select NSF (Durkan et al. ([2019](#bib.bib84)))
    as the density estimator.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SOIL-TDM (Boborzi et al. ([2022](#bib.bib27); [2021b](#bib.bib26))) also focuses
    on LfO and starts with the same objective as IL-flOw. However, SOIL-TDM gets rid
    of the requirements for deterministic and invertible dynamics by estimating $P_{\pi}(s_{t+1}|s_{t})$
    as $\mathcal{T}_{\pi}(s_{t+1}|s_{t},a_{t})\pi(a_{t}|s_{t})/\pi^{\prime}(a_{t}|s_{t+1},s_{t})$,
    based on the Bayes Theorem. With this new definition, the objective for $\pi$
    is converted into:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi}\mathbb{E}_{(s_{0:T},a_{0:T})\sim P_{\pi}}\sum_{t=0}^{T-1}\left[r(s_{t},a_{t})+H(\pi(\cdot&#124;s_{t}))\right]$
    |  | (77) |'
  prefs: []
  type: TYPE_TB
- en: where $r(s_{t},a_{t})=\mathbb{E}_{s_{t+1}\sim\mathcal{T}_{\pi}(\cdot|s_{t},a_{t})}\left[-\log\mathcal{T}_{\pi}(s_{t+1}|s_{t},a_{t})+\log\pi^{{}^{\prime}}(a_{t}|s_{t+1},s_{t})+\log
    P_{E}(s_{t+1}|s_{t})\right]$. Still, they separate the reward learning from the
    policy optimization. In this case, besides the expert state transition model $P_{E}(s_{t+1}|s_{t})$,
    they adopt conditional NFs to model the agent’s transition dynamics $\mathcal{T}_{\pi}(s_{t+1}|s_{t},a_{t})$
    and the posterior distribution associated with $\pi$, i.e., $\pi^{\prime}(a_{t}|s_{t+1},s_{t})$,
    for which they choose Real NVP (Dinh et al. ([2017](#bib.bib73))). Note that the
    approximation of $P_{E}(s_{t+1}|s_{t})$ is trained on $D_{E}$, while approximations
    of $\mathcal{T}_{\pi}(s_{t+1}|s_{t},a_{t})$ and $\pi^{\prime}(a_{t}|s_{t+1},s_{t})$
    are trained on rollout data collected by $\pi$ during the RL process.
  prefs: []
  type: TYPE_NORMAL
- en: 'SLIL (Zhang et al. ([2021b](#bib.bib389))) is based on the Likelihood-based
    Imitation Learning (LIL) framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log P_{\pi}(s,a)\right]\
    s.t.\ P_{\pi}=\operatorname*{arg\,max}_{P}\mathbb{E}_{(s,a)\sim\rho_{\pi}}\left[\log
    P(s,a)\right]$ |  | (78) |'
  prefs: []
  type: TYPE_TB
- en: 'Intuitively, $P_{\pi}(s,a)$ represents the probability of observing an expert
    state-action pair $(s,a)\in D_{E}$ when executing the learner policy $\pi$. Directly
    solving this bilevel optimization problem requires jointly learning $\pi$ and
    $P_{\pi}$, which brings training instability. Thus, they propose to maximize its
    tight lower bound:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log\pi(a&#124;s)\right]+\mathbb{E}_{s\sim\rho_{\pi}}\left[\log
    P_{E}(s)\right]\ s.t.\ P_{E}=\operatorname*{arg\,max}_{P}\mathbb{E}_{s\sim D_{E}}\left[\log
    P(s)\right]$ |  | (79) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\rho_{\pi}(s)$ denotes the occupancy measure of $s$ using $\pi$. The
    training process can then be divided into two stages for stability. At stage 1,
    they train an NF to model the expert state distribution $P_{E}(s)$ based on the
    demonstration set $D_{E}$, which is, at stage 2, adopted to optimize the policy
    $\pi(a|s)$ using Eq. ([79](#S5.E79 "In 5.1.1 Exact Density Estimation in Imitation
    Learning Using Normalizing Flows ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")). This objective combines
    Behavioral Cloning (i.e., $\max_{\pi}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log\pi(a|s)\right]$)
    and RL with $\log P_{E}(s)$ as the reward (i.e., $\max_{\pi}\mathbb{E}_{s\sim\rho_{\pi}}\left[\log
    P_{E}(s)\right]$). It is worthy noting that they utilize a Continuous Normalizing
    Flow – DCNF (Zhang et al. ([2021b](#bib.bib389))), rather than discrete ones like
    other works, as density estimators. This choice is due to the enhanced modeling
    capabilities and fewer model restrictions that Continuous Normalizing Flows offer
    Grathwohl et al. ([2018](#bib.bib108)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPRIL (Schroecker et al. ([2019](#bib.bib286))) also utilizes the LIL framework
    but optimizes it in a different manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi_{\theta}}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log P_{\pi_{\theta}}(s,a)\right]=\max_{\pi_{\theta}}\mathbb{E}_{(s,a)\sim
    D_{E}}\left[\log\pi_{\theta}(a&#124;s)+\log P_{\pi_{\theta}}(s)\right]$ |  | (80)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Here, the first term of the right-hand side is simply Behavioral Cloning. While,
    for the second term, according to (Ross & Bagnell ([2010](#bib.bib281))), the
    gradient from it can be estimated as $\nabla_{\theta}\log P_{\pi_{\theta}}(s)\propto\mathbb{E}_{(s^{\prime\prime},a^{\prime\prime})\sim\mathcal{B}_{\pi_{\theta}}(\cdot|s)}\left[\nabla_{\theta}\log\pi_{\theta}(a^{\prime\prime}|s^{\prime\prime})\right]$,
    where $\mathcal{B}_{\pi_{\theta}}$ corresponds to the long-term predecessor distribution
    modelling the distribution of states and actions that, under the current policy
    $\pi_{\theta}$, will eventually lead to the given target state $s$: $\mathcal{B}_{\pi_{\theta}}(s^{\prime\prime},a^{\prime\prime}|s)=(1-\gamma)\sum_{j=0}^{\infty}\gamma^{j}P_{\pi_{\theta}}(s_{t}=s^{\prime\prime},a_{t}=a^{\prime\prime}|s_{t+j+1}=s)$.
    In GPRIL, $\mathcal{B}_{\pi_{\theta}}(s^{\prime\prime},a^{\prime\prime}|s)$ is
    modeled as $\mathcal{B}_{\pi_{\theta}}^{\omega_{1}}(s^{\prime\prime}|s)\cdot\mathcal{B}_{\pi_{\theta}}^{\omega_{2}}(a^{\prime\prime}|s,s^{\prime\prime})$,
    i.e., the product of two density functions modelled by conditional MAF (Papamakarios
    et al. ([2017](#bib.bib239))). To train these models, they collect training data
    using self-supervised roll-outs. In particular, they sample states, actions and
    target-states (i.e., $(s^{\prime\prime},a^{\prime\prime},s)$) where the separation
    in time between the $s^{\prime\prime}$ and $s$ is selected randomly from a geometric
    distribution parameterized by $\gamma$. In this way, the triplets collected theoretically
    obey: $(s^{\prime\prime},a^{\prime\prime})\sim\mathcal{B}_{\pi_{\theta}}(\cdot|s)$.
    As a subsequent research, the work Schroecker & Jr. ([2020](#bib.bib285)) utilizes
    the same objective function, but introduces an alternative method for estimating
    $\nabla_{\theta}\log P_{\pi_{\theta}}(s)$ from a goal-conditioned RL perspective.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Policy Modeling in Imitation Learning with Normalizing Flows
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As a potent generative model, Normalizing Flows can be directly adopted as
    policy generators for tackling challenging tasks. In this regard, recent works
    in off-policy RL have demonstrated that NFs can be used to model continuous policies
    (Haarnoja et al. ([2018](#bib.bib117)); Ward et al. ([2019](#bib.bib347)); Mazoure
    et al. ([2019](#bib.bib217))), which can lead to faster convergence and higher
    rewards by enhancing exploration and supporting multi-modal action distributions.
    Compared with commonly employed diagonal Gaussian policies, where each action
    dimension is independent of the others, those based on NFs offer greater expressiveness.
    In the context of IL, Flow DAC (Boborzi et al. ([2021a](#bib.bib25))) extends
    DAC (an off-policy IL algorithm Kostrikov et al. ([2019](#bib.bib171))) by adopting
    a conditional version of Real NVP as the policy network. Specifically, the action
    $a$ at state $s$ is generated by: $z\sim P_{Z}(\cdot),\ a=G(z|s)$ where $G$ denotes
    the NF generator and $s$ is the conditioner. The stochasticity of this policy
    comes from the latent distribution $P_{Z}(\cdot)$, and the action output of $G$
    can be in any complex distribution. Similarly, FlowPlan (Agarwal et al. ([2020](#bib.bib3)))
    adopts NFs as a trajectory generator through sequentially predicting the next
    state embedding based on historical states. Specifically, NAF is adopted as the
    generative model, which is trained by minimizing the KL divergence between the
    expert’s and generated trajectories.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | CFIL | IL-flOw | SOIL-TDM | SLIL | GPRIL | Flow DAC | FlowPlan
    |'
  prefs: []
  type: TYPE_TB
- en: '| Flow | MAF | NSF | Real NVP | DCNF | MAF | Real NVP | NAF |'
  prefs: []
  type: TYPE_TB
- en: '| Flow Type | AR | CP | CP | ODE | AR | CP | AR |'
  prefs: []
  type: TYPE_TB
- en: '| Flow Usage | DE ($\rho_{E}(s,a)$, $\rho_{\pi}(s,a)$) | DE ($P_{E}(s^{\prime}&#124;s)$)
    | DE ($P_{E}(s^{\prime}&#124;s)$, $\mathcal{T}_{\pi}(s^{\prime}&#124;s,a)$, $\pi^{\prime}(a&#124;s^{\prime},s)$)
    | DE ($P_{E}(s)$) | DE $\mathcal{B}_{\pi}(s^{\prime\prime},a^{\prime\prime}&#124;s)$
    | G (Policy Network) | G (Traj Planner) |'
  prefs: []
  type: TYPE_TB
- en: '| Objective Type | KL | KL | KL | LIL | LIL | AIL | KL |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation Task | MuJoCo | MuJoCo | MuJoCo | MuJoCo | Robotic Insertion |
    High-D & MuJoCo | HES-4D |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Summary of NF-based IL algorithms. AR, CP, and ODE represent autoregressive,
    coupling, and ODE-based continuous flows, respectively. DE and G represent the
    two manners that NFs can be used, corresponding to the density estimator and generator,
    respectively. For DE, we specifically point out the densities estimated, for which
    the definitions are available in Section [5.1.1](#S5.SS1.SSS1 "5.1.1 Exact Density
    Estimation in Imitation Learning Using Normalizing Flows ‣ 5.1 Imitation Learning
    ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions").
    Regarding the types of IL objectives, KL, LIL, and AIL denote the KL divergence,
    Likelihood-based IL, and Adversarial IL, respectively. Examples for AIL include
    GAIL and AIRL, which are introduced in Section [4.1.1](#S4.SS1.SSS1 "4.1.1 Fundamental
    GAN-Based Imitation Learning Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"). For the benchmarks, MuJoCo (Todorov et al. ([2012](#bib.bib320)))
    provides a series of robotic locomotion tasks; Robotic Insertion (Vecerík et al.
    ([2017](#bib.bib328))) is a specific type of robotic manipulation task built on
    the MuJoCo engine; High-D (Krajewski et al. ([2018](#bib.bib173))) and HES-4D
    (Meyer et al. ([2019](#bib.bib222))) are two real-word driving dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [6](#S5.T6 "Table 6 ‣ 5.1.2 Policy Modeling in Imitation Learning
    with Normalizing Flows ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), we provide a summary of NF-based
    IL algorithms. For each algorithm, we provide key information including what type
    of and how the NF is utilized, its underlying IL framework, and evaluation tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Reinforcement Learning with Offline Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned in the beginning of Section [5](#S5 "5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), we broaden the scope of RL-related
    works to include both offline RL (Section [5.2.1](#S5.SS2.SSS1 "5.2.1 Adopting
    Normalizing Flows in Offline Reinforcement Learning ‣ 5.2 Reinforcement Learning
    with Offline Data ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) and online RL with offline data (Section [5.2.2](#S5.SS2.SSS2 "5.2.2
    Adopting Normalizing Flows in Online Reinforcement Learning with Offline Data
    ‣ 5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")). Similar with NF-based IL methods,
    algorithms in Section [5.2.1](#S5.SS2.SSS1 "5.2.1 Adopting Normalizing Flows in
    Offline Reinforcement Learning ‣ 5.2 Reinforcement Learning with Offline Data
    ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    and [5.2.2](#S5.SS2.SSS2 "5.2.2 Adopting Normalizing Flows in Online Reinforcement
    Learning with Offline Data ‣ 5.2 Reinforcement Learning with Offline Data ‣ 5
    Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") can
    be categorized based on the usage of NFs, i.e., working as either a density estimator
    or function generator.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Adopting Normalizing Flows in Offline Reinforcement Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Offline reinforcement learning aims to train a policy on a pre-recorded dataset,
    collected by the behavior policy $\mu(a|s)$, without any additional environment
    interactions. As introduced in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Background
    on Dynamic-Programming-based Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), major challenges in this setting include (1) extrapolation error
    caused by approximating the value of state-action pairs not well-covered by the
    training data and (2) distributional shift between behavior and inference policies.
    The common practice to tackle these problems is to induce conservatism, through
    either keeping the learned policies closer to the behavioral ones or constructing
    pessimistic value functions. However, this can lead to over-conservative, yet
    sub-optimal policies. By introducing Normalizing Flows, an agent can more effectively
    utilize offline data and circumvent out-of-distribution (OOD) actions through
    its generative (Akimov et al. ([2022](#bib.bib6)); Yang et al. ([2023d](#bib.bib376)))
    or density estimation capacity (Zhang et al. ([2023](#bib.bib386))).'
  prefs: []
  type: TYPE_NORMAL
- en: 'CNF (Akimov et al. ([2022](#bib.bib6))) and LPD (Yang et al. ([2023d](#bib.bib376)))
    employ a similar algorithm idea – using NFs to extract latent variables underlying
    primitive actions in the offline dataset. Specifically, they train a Normalizing
    Flow $G$ conditioned on $s$ to model the mapping from $z$ to $a$, following $z\sim
    P_{Z}(\cdot),\ a=G(z|s)$, through supervised learning (i.e., Eq. ([19](#S2.E19
    "In 2.3 Normalizing Flows ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"))). After training, $G$ can work as an action decoder to convert a
    latent variable $z$ (following a simple base distribution) to its corresponding
    action. In this case, only a high-level policy $z\sim\pi_{\text{high}}(\cdot|s)$
    needs to be learned with offline RL, and the function composition $G\circ\pi_{\text{high}}$
    can work as a hierarchical policy mapping from $s$ to $a$. In particular, CNF
    and LPD are driven by different motivations. CNF pretrains a Real NVP conditioned
    on $s$ as the action decoder. To avoid OOD actions caused by the long tail effect,
    the latent space (i.e., the support of $P_{Z}(\cdot)$), which is also the output
    space of the high-level policy, is designed to be bounded, specifically an $n$-dim
    interval $(-1,1)^{n}$. As a result, the actor model (i.e., $G\circ\pi_{\text{high}}$)
    should be unable to generate OOD actions, even without clipping the high-level
    policy’s output (as in PLAS introduced in Section [3.1.3](#S3.SS1.SSS3 "3.1.3
    Addressing the Issue of Out-of-Distribution Actions Using VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), which avoids possible sub-optimality. LPD, instead, focuses on
    improving the offline RL performance in challenging, long-horizon tasks, by adopting
    NFs to model the transformation from $z$ to a fixed-length sequence of actions
    starting from $s$, i.e., a skill. Provable benefits can be gained when the learned
    skills are expressive enough to recover the original policy space (Yang et al.
    ([2023d](#bib.bib376))), and Normalizing Flows can facilitate the learning of
    such skills. With this pretrained transformation, the dataset can be relabeled
    in terms of skills as $D_{\text{high}}=[(s_{0}^{i},z^{i},\sum_{t=0}^{h-1}\gamma^{t}r^{i}_{t},s_{h}^{i})]_{i=1}^{N}$,
    where $h$ is the skill length and $z^{i}$ can be acquired as $G^{-1}(a^{i}_{0:h-1}|s_{0}^{i})=F(a^{i}_{0:h-1}|s_{0}^{i})$
    (i.e., via the normalizing direction of the NF). Then, offline RL can be adopted
    to learn $\pi_{\text{high}(z|s)}$ from $D_{\text{high}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'APAC (Zhang et al. ([2023](#bib.bib386))) adopts NFs (specifically Flow-GAN
    (Grover et al. ([2018](#bib.bib111)))) as density estimators, rather than action
    decoders as in aforementioned algorithms, to filter out OOD actions while avoiding
    being over-conservative. To be specific, the density of the point $(s,a)$ can
    be approximated as $P_{Z}(G^{-1}(a|s))$, where $G$ is the NF: $a=G(z|s)$. Points
    with higher density are more likely to be generated by the behavior policy and
    so can be viewed as “safe” in-distribution points for training use. To be specific,
    they modify the policy improvement step in Eq. ([32](#S3.E32 "In 3.1.1 Background
    on Dynamic-Programming-based Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\pi_{k+1}=\arg\max_{\pi}\mathbb{E}_{s\sim D_{\mu}}\left[\mathbb{E}_{a\sim\pi(\cdot&#124;s)}Q_{k+1}^{\pi}(s,a)\right]\
    s.t.\ P_{Z}(G^{-1}(a&#124;s))>\epsilon(s,a).$ |  | (81) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\epsilon(s,a)$ is a prefixed threshold. In this way, samples for updating
    the policy are restricted within the safe area, which could contain both observed
    and unobserved points in the offline dataset. Thus, the policy can potentially
    perform exploration out of the given dataset and avoid visiting OOD actions, simultaneously.
    Note that this constraint on $\pi$ is more relaxed and practical compared with
    the one of BEAR (introduced in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Background
    on Dynamic-Programming-based Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")): $\pi\in\{\pi^{\prime}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\
    |\ \pi^{\prime}(a|s)=0\ \text{whenever}\ \mu(a|s)<\epsilon\}$, to avoid being
    over-conservative.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Adopting Normalizing Flows in Online Reinforcement Learning with Offline
    Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To deepen the understanding of applying NFs for RL, we incorporate another line
    of work that merges online RL with offline data. Still, these works can be categorized
    as utilizing NFs as potent generators (Mazoure et al. ([2019](#bib.bib217)); Yan
    et al. ([2022](#bib.bib366)); Slack et al. ([2022](#bib.bib297))) or density estimators
    (Wu et al. ([2021](#bib.bib357))). From another perspective, these studies either
    extract useful information from the offline data and subsequently apply online
    RL atop it (Yan et al. ([2022](#bib.bib366)); Slack et al. ([2022](#bib.bib297));
    Wu et al. ([2021](#bib.bib357))), or they alternate between these two processes
    till the end of training (e.g., conducting off-policy RL) (Mazoure et al. ([2019](#bib.bib217))).
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | CNF | LPD | APAC | SAC-NF | CEIP | SAFER | NF Shaping |'
  prefs: []
  type: TYPE_TB
- en: '| Flow | Real NVP | Real NVP | Flow-GAN | IAF | Real NVP | Real NVP | MAF |'
  prefs: []
  type: TYPE_TB
- en: '| Flow Type | CP | CP | CP | AR | CP | CP | AR |'
  prefs: []
  type: TYPE_TB
- en: '| Flow Usage | G (Action Decoder) | G (Skill Decoder) | DE ($P_{G}(s,a)$) |
    G (Policy Network) | G (Action Decoder) | G (Action Decoder) | DE ($P_{G}(s,a)$)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation Task | D4RL (L, M) | D4RL (M, A, K) | D4RL (L, M) | MuJoCo | Kitchen,
    Office, FetchReach | Operation | Robotic Insertion, Pick Place |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Summary of NF-based (Offline) RL algorithms. AR and CP represent autoregressive
    and coupling flows, respectively. DE and G represent the two manners that NFs
    can be used, corresponding to the density estimator and generator, respectively.
    Specifically, $P_{G}(s,a)$ denotes the state-action pair distribution in the offline
    dataset. For the benchmarks, all the evaluation tasks shown in this table are
    built on the MuJoCo (Todorov et al. ([2012](#bib.bib320))) engine; D4RL (Fu et al.
    ([2020](#bib.bib99))) provides a bunch of challenging (robotic) tasks specifically
    for offline RL, including Locomotion (L), AntMaze (M), Adroit (A), Kitchen (K);
    Office (Pertsch et al. ([2021](#bib.bib252))), FetchReach (Plappert et al. ([2018](#bib.bib254))),
    Kitchen (from D4RL), Robotic Insertion, Pick Place (Vecerík et al. ([2017](#bib.bib328))),
    and Operation (Slack et al. ([2022](#bib.bib297))) all involve the manipulation
    of a robot arm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the same insights as Flow DAC (introduced in Section [5.1](#S5.SS1 "5.1
    Imitation Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), SAC-NF (Mazoure et al. ([2019](#bib.bib217))) extends SAC, an off-policy
    RL algorithm (Haarnoja et al. ([2018](#bib.bib117))), by adopting NFs (IAF (Kingma
    et al. ([2016](#bib.bib167)))) as the policy network, which could potentially
    improve exploration and support multi-modal action distributions. On the other
    hand, CEIP (Yan et al. ([2022](#bib.bib366))) and SAFER (Slack et al. ([2022](#bib.bib297)))
    leverage the offline data by first learning a transformation from latent variables
    $z$ to actions $a$ using NFs as an action decoder and then training a high-level
    policy $\pi_{\text{high}}(z|s)$ atop the transformation function with (online)
    RL, akin to CNF. Yet, in comparison to CNF, they extend the action decoder learning
    from different perspectives. In CEIP, the generator is conditioned on a concatenation
    of the current and next states, i.e., $u=[s,s^{\prime}]$, instead of the state
    alone, which can better inform the agent of the state it should try to achieve
    with its current action. Moreover, they propose a manner to utilize data from
    various yet related tasks, which are more accessible than task-specific ones.
    Specifically, they first train a flow for each task $i$ with corresponding demonstrations.
    The generator is defined as $a=G_{i}(z|u)=\exp\{c_{i}(u)\}\odot z+d_{i}(u)$ where
    $c_{i}$ and $d_{i}$ are trainable deep nets, $\odot$ refers to the Hadamard product.
    This structure design follows Real NVP (Dinh et al. ([2017](#bib.bib73))). Subsequently,
    they train a combination flow on demonstrations of the target task, with the generator
    defined as $G(z|u)=\left(\sum_{i=1}^{n}\mu_{i}(u)\exp\{c_{i}(u)\}\right)\odot
    z+\left(\sum_{i=1}^{n}\lambda_{i}(u)d_{i}(u)\right)$. At this stage, they fix
    $c_{i}$ and $d_{i}$ ($i=1,\cdots,n$) and train the weighting functions $\mu_{i}$
    and $\lambda_{i}$ to optimize the combination flow for the designated task via
    supervised learning (i.e., Eq. ([19](#S2.E19 "In 2.3 Normalizing Flows ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))). In this way, they
    efficiently utilize demonstrations from related tasks. While, SAFER emphasizes
    the safety aspect of the learned transformation. Specifically, the transformation
    should prevent selecting unsafe actions $a_{\text{unsafe}}$ in the environment,
    which are labeled in the training dataset. To achieve this, the authors propose
    conditioning the flow on both the state $s$ and a safety context $c$. It’s important
    to note that $c$ is not provided directly but must be inferred from the information
    available in the environment. Thus, SAFER simultaneously learns (1) an inference
    network $P_{\phi}(c|\Lambda)$ to determine the current safety context $c$ based
    on a sliding window of states $\Lambda$ and (2) a generator $a=G_{\theta}(z|s,c)$
    (Real NVP) conditioned on $s$ and $c$, with the following objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\theta,\phi}\mathbb{E}_{s,a,a_{\text{unsafe}}\sim D_{\mu},\ c\sim
    P_{\phi}(\cdot&#124;\Lambda)}\left[\log P_{\theta}(a&#124;s,c)-\lambda\log P_{\theta}(a_{\text{unsafe}}&#124;s,c)\right]-D_{KL}(P_{\phi}(c&#124;\Lambda)&#124;&#124;P_{C}(c))$
    |  | (82) |'
  prefs: []
  type: TYPE_TB
- en: Here, $D_{\mu}$ is the training dataset, $P_{\theta}(a|s,c)=P_{Z}(G^{-1}_{\theta}(a|s,c))$,
    $P_{C}(c)$ denotes the assumed prior distribution of $c$. Intuitively, the first
    two terms encourage safe actions while deterring unsafe ones, and, in conjunction
    with the final term, the variable $c$ is compelled to encapsulate useful information
    regarding safety. This algorithm serves as an illustration of how NFs and VAEs
    can be integrated. In this setup, $P_{\phi}$ and $P_{\theta}$ function as the
    encoder and decoder of the VAE, respectively, with $P_{\theta}$ being implemented
    as a Normalizing Flow.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike SAC-NF, CEIP, and SAFER, NF Shaping (Wu et al. ([2021](#bib.bib357)))
    adopts NFs as density estimators. This is a method that combines (online) reinforcement
    and imitation learning by shaping the reward function with a state-and-action-dependent
    potential learned from demonstrations. To be specific, they first train an NF
    (specifically MAF Papamakarios et al. ([2017](#bib.bib239))) to estimate the density
    of state-action pairs in the demonstration dataset, i.e., $P_{G}(s,a)=P_{Z}(G^{-1}(a|s))$.
    Then, they shape the reward function as $\widetilde{r}_{t}=r(s_{t},a_{t},s_{t+1})+\gamma\Phi(s_{t+1},a_{t+1})-\Phi(s_{t},a_{t})$,
    where $\Phi(s,a)=\beta\log(P_{G}(s,a)+\epsilon)$, $\beta>0$ adjusts the weight,
    and $\epsilon>0$ is a small constant to prevent numerical issues. According to
    (Ng et al. ([1999](#bib.bib229)); Wiewiora ([2003](#bib.bib354))), reward shaping
    can intensify the reward and greatly improve the learning efficiency. Intuitively,
    this potential term highlights regions where the demonstrated policy visits more
    frequently, signaling areas that should be explored more.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, it’s noteworthy that among all the aforementioned works, the algorithms
    proposed in (Akimov et al. ([2022](#bib.bib6)); Yang et al. ([2023d](#bib.bib376));
    Yan et al. ([2022](#bib.bib366)); Slack et al. ([2022](#bib.bib297))) adopt similar
    ideas. They first learn an NF-based transformation from a latent space to the
    real action space based on the offline data. Subsequently, they train a high-level
    policy on the latent variables, rather than primitive actions, with online or
    offline RL. The prelearned transformation constitutes a mapping from a simpler,
    more controllable distribution to the target distribution, which is usually complex
    and unknown. Moreover, we can see that Normalizing Flows are primarily used as
    exact density estimators in IL (Section [5.1](#S5.SS1 "5.1 Imitation Learning
    ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    while mainly functioning as expressive generators in (Offline) RL (Section [5.2](#S5.SS2
    "5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")). We further provide Table [6](#S5.T6
    "Table 6 ‣ 5.1.2 Policy Modeling in Imitation Learning with Normalizing Flows
    ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") and [7](#S5.T7 "Table 7 ‣ 5.2.2 Adopting Normalizing Flows
    in Online Reinforcement Learning with Offline Data ‣ 5.2 Reinforcement Learning
    with Offline Data ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") as summaries for NF-based IL and RL algorithms. It can be observed
    that most works adopt autoregressive or coupling flows, which is probably due
    to their computation efficiency. While almost all algorithms are evaluated on
    robotic benchmarks such as MuJoCo or D4RL, there is a notable absence of comparative
    analysis among these algorithms. Given the diverse perspectives from which these
    algorithms are developed, it is challenging to compare them at the algorithmic
    level, thus comprehensive and fair empirical evaluations are essential for practical
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Transformers in Offline Policy Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Transformers can be used for offline RL (Section [6.1](#S6.SS1 "6.1 Offline
    Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) or IL (Section [6.2](#S6.SS2 "6.2 Imitation Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) by modeling them as
    a problem of predicting the next token based on historical data. Specifically,
    most works in this section follow a similar algorithm design with Decision Transformer
    – a seminal work of trajectory-optimization-based offline RL. Thus, we start with
    an introduction of the background on trajectory-optimization-based offline RL,
    and then provide a comprehensive review on transformer-based offline RL and IL
    algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Offline Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The content of this section is arranged as follows. First, we highlight pioneering
    studies on transformer-based offline RL in Section [6.1.1](#S6.SS1.SSS1 "6.1.1
    Background on Trajectory-Optimization-based Offline Reinforcement Learning ‣ 6.1
    Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), along with a series of follow-up works for improvements
    (i.e., Section [6.1.2](#S6.SS1.SSS2 "6.1.2 Balancing Model Capacity with Training
    Data ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") and [6.1.3](#S6.SS1.SSS3 "6.1.3 Mitigating Impacts from
    Environmental Stochasticity ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")). Subsequently, we explore
    the use of transformers in broader problem scenarios in Section [6.1.4](#S6.SS1.SSS4
    "6.1.4 Transformers in Extended Offline Reinforcement Learning Setups ‣ 6.1 Offline
    Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), such as multi-agent and multi-task RL. Specifically, for multi-task
    settings, we focus on the development of generalist agents based on pretraining
    and fine-tuning. Last, we reflect on existing problems in this field and discuss
    potential future research directions in Section [6.1.5](#S6.SS1.SSS5 "6.1.5 Reflections
    on Transformer-based Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement
    Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions").'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Background on Trajectory-Optimization-based Offline Reinforcement Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Inspired by the great success of high-capacity sequence generation models (such
    as the transformer) in natural language processing (NLP), Chen et al. ([2021b](#bib.bib46))
    proposes to view offline RL as a sequence modeling problem and solve it with a
    Decision Transformer (DT). This algorithm exemplifies trajectory-optimization-based
    offline RL, so we provide a detailed introduction of it, along with two other
    seminal works, as a brief tutorial on this particular branch of offline RL methods.
    In particular, given offline trajectories $\{\tau=\left(s_{0},a_{0},R_{0},\cdots,s_{T},a_{T},R_{T}\right)\}$,
    where $R_{t}=\sum_{i=t}^{T}r_{i}$ denotes the return-to-go (RTG), the DT is trained
    to predict the next action based on the previous $k+1$ transitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\pi}\mathbb{E}_{\tau}\left[\sum_{t=0}^{T}(a_{t}-\pi(\tau_{t-k:t}))^{2}\right],\
    \text{or}\ \min_{\pi}\mathbb{E}_{\tau}\left[\sum_{t=1}^{T}-\log\pi(a_{t}&#124;\tau_{t-k:t})\right]$
    |  | (83) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\tau_{t-k:t}=(s_{j},a_{j},R_{j},\cdots,s_{t},R_{t})$ ($j=\min(t-k,0)$)
    is the input sequence; the two terms above are for tasks with continuous and discrete
    actions, respectively. DT adopts a GPT-like architecture (Radford et al. ([2018](#bib.bib265))),
    which is a decoder-only transformer as introduced in Section [2.4](#S2.SS4 "2.4
    Transformers ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions").
    During evaluation rollouts, an initial target return $R_{0}$ must be specified,
    which can be the highest achievable return for the task. The inference trajectory
    is then generated autoregressively using the DT as follows: $s_{0}\sim\rho_{0}(\cdot)$,
    $a_{0}\sim\pi(\cdot|s_{0},R_{0})$, $s_{1}\sim\mathcal{T}(\cdot|s_{0},a_{0})$,
    $r_{0}=r(s_{0},a_{0},s_{1}),\ \cdots$, $R_{t}=R_{t-1}-r_{t-1}$, $a_{t}\sim\pi(\cdot|\tau_{t-k:t}),\
    \cdots$, where $(\rho_{0},\mathcal{T},r)$ are components of the MDP. DT greatly
    simplifies offline RL by eliminating the necessity to fit value functions through
    dynamic programming or to compute policy gradients as detailed in Section [3.1.1](#S3.SS1.SSS1
    "3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"). Using such a (supervised-learning
    based) sequence modeling objective makes it less prone to selecting OOD actions,
    because multiple state and action anchors throughout the trajectory prevent the
    learned policy from deviating too far from the behavior policy $\mu(a|s)$. Additionally,
    as a high-capacity model, the transformer has the potential to enhance both the
    scalability and robustness of the learned policy, similar to its applications
    in NLP. Per Chen et al. ([2021b](#bib.bib46)), DT shows competitive performance
    compared with prior offline RL methods (such as Conservative Q-Learning (CQL,
    Kumar et al. ([2020](#bib.bib176)))) especially for tasks with sparse and delayed
    reward functions. However, Emmons et al. ([2022](#bib.bib85)) investigate what
    is essential for offline RL via supervised learning (RvS) through extensive experiments.
    Their findings indicate that a two-layer MLP model, utilizing a simpler input
    format (i.e., $(s_{t},R_{t})$ instead of $\tau_{t-k:t}$) and the same objective,
    exhibits competitive, and in some cases superior, performance compared to DT,
    and DT underperform prior offline RL algorithms on most benchmark tasks. This
    raises the questions whether it is necessary to adopt such a high-capacity model,
    i.e., the transformer, as the policy network, and under what scenarios RvS methods
    might outperform dynamic-programming-based offline RL methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Trajectory Transformer (TT, Janner et al. ([2021](#bib.bib149))) follows DT
    but utilizes more techniques from NLP, including tokenization, discretization,
    and beam search. In particular, they treat each dimension of the state and action
    as a token and discretize them independently. Suppose the state and action have
    $N$ and $M$ dimensions respectively, the objective is $\min_{\pi}\mathbb{E}_{\tau}\left[\mathcal{L}(\tau)\right]$,
    where $\mathcal{L}(\tau)$ is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sum_{t=0}^{T}\left[\sum_{i=1}^{N}\log\pi(s_{t}^{i}&#124;s_{t}^{<i},\tau_{<t})+\sum_{j=1}^{M}\log\pi(a_{t}^{j}&#124;a_{t}^{<j},s_{t},\tau_{<t})+\log\pi(r_{t}&#124;a_{t},s_{t},\tau_{<t})+\log\pi(R_{t}&#124;r_{t},a_{t},s_{t},\tau_{<t})\right]$
    |  | (84) |'
  prefs: []
  type: TYPE_TB
- en: This objective involves supervision on each dimension of the state/action, reward,
    and return-to-go. As for the inference process, a beam search technique (Freitag
    & Al-Onaizan ([2017](#bib.bib95))) is utilized. Specifically, $B$ most-likely
    samples are kept when sampling each of $a_{t}^{j},R_{t}$ sequentially. Then, samples
    $(a_{t}^{1:M},R_{t})$ with the highest cumulative reward plus return-to-go (i.e,
    the estimated trajectory return $\sum_{i=0}^{t-1}r_{i}+R_{t}$) is selected for
    execution. Note that, during the evaluation, $s_{t}^{1:N}$ and $r_{t}$ can be
    acquired from the simulator. TT shows superior performance than DT in some benchmarks
    as reported in (Janner et al. ([2021](#bib.bib149))), but treating each dimension
    separately would introduce learning and sample inefficiencies, especially for
    high-dimensional tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of modeling the policy, Q-Transformer (Chebotar et al. ([2023](#bib.bib38)))
    proposes to learn a transformer-based Q-network. Similar with TT, each dimension
    of the action is discretized and treated as a separate time step. When training,
    they update the Q-function in a temporal difference (TD) manner, i.e., minimizing
    the disagreement between the predicted Q-value $Q(s_{t-k:t},a_{t}^{1:i})$ and
    target Q-value $\widehat{Q}(s_{t-k:t},a_{t}^{1:i})$. If $i=M$ (i.e., the action
    dimension), $\widehat{Q}(s_{t-k:t},a_{t}^{1:i})=r_{t}+\max_{a^{1}_{t+1}}Q(s_{t-k+1:t+1},a^{1}_{t+1})$;
    otherwise, it is $\max_{a^{i+1}_{t}}Q(s_{t-k:t},a_{t}^{1:i},a^{i+1}_{t})$. Note
    that for the Bellman optimality (Puterman ([2014](#bib.bib258))) of this per-dimension
    updating rule to hold, it requires the assumption that the choice of $a_{t}^{1:i}$
    does not influence $\max_{a^{i+1}_{t}}Q(s_{t-k:t},a_{t}^{1:i},a^{i+1}_{t})$. However,
    this is probably not true since dimensions of an action are likely to be correlated.
    During evaluation, the action is also generated dimension by dimension, based
    on the trained Q-function.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have introduced some fundamental algorithm designs to cast offline
    RL as sequence modelling problems based on transformers. Next, we will present
    follow-up improvements of these methods. As illustrated in (Emmons et al. ([2022](#bib.bib85))),
    carefully aligning the model structure/capacity with the training data to prevent
    overfitting or underfitting and choosing which information to condition the policy
    on are critical for RvS performance. Thus, we will categorize the following works
    based on these two aspects.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Balancing Model Capacity with Training Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To better align the capacity of transformer-based models with the amount of
    training data, a series of studies have been proposed focusing on augmenting the
    offline dataset to avoid overfitting. These algorithms provide data augmentation
    strategies for different task scenarios. (1) Based on TT, Bootstrapped Transformer
    (Wang et al. ([2022a](#bib.bib337))) proposes to generate trajectories from $\pi$,
    i.e., the model being learned, and adopt them as additional training data to expand
    the amount and coverage of the offline dataset. The self-generated trajectories
    are filtered by their log probability under the policy $\pi$, which indicates
    the quality and reliability of the training data. (2) SS-DT (Zheng et al. ([2023](#bib.bib392)))
    introduces a data augmentation method for semi-supervised settings, where most
    trajectories lack action labels and are in the format $(s_{0},r_{0},\cdots,s_{T},r_{T})$.
    Their approach simply involves training an inverse dynamics model, i.e., $\mathcal{T}_{\text{inv}}(a_{t}|s_{t},s_{t+1})$,
    on trajectories with action labels and then applying this model to predict actions
    for the unlabeled trajectories. (3) Given only sub-optimal trajectories, the trajectory-level
    supervised learning would fail. In this case, the agent needs to learn to stitch
    segments from different trajectories for an optimal policy. Value-based RL does
    not have the same issue as it pools information for each state across trajectories
    through the Bellman backup. In this case, QDT (Yamagata et al. ([2023](#bib.bib365)))
    suggests learning a Q-function via CQL and replacing the RTG values (i.e., $R_{t}$)
    in the offline dataset with the learned Q-values for DT training. (4) CDT (Liu
    et al. ([2023b](#bib.bib197))) proposes a data augmentation method for safe RL.
    Specifically, in a Constrained MDP (Altman ([1998](#bib.bib9))), the DT conditioned
    on both $R(\tau)$ and $C(\tau)=\sum_{i=0}^{T}c_{t}$ (i.e., the constraint-violation
    cost of the trajectory) is expected to achieve the target trajectory return while
    ensuring that the accumulated cost remains lower than $C(\tau)$. However, during
    inference, unachievable $(R(\tau),C(\tau))$ pairs may be given as conditions,
    which are not represented in the offline dataset. Thus, as an augmentation, they
    suggest using the trajectories from the offline dataset that achieve the highest
    return without violating $C(\tau)$ as the corresponding trajectories of $(R(\tau),C(\tau))$
    for offline training. (5) For tasks with sparse and delayed rewards, DT suffers
    from model degradation, since the RTG does not change within a trajectory. DTRD
    (Zhu et al. ([2023a](#bib.bib394))) proposes to learn a reward shaping function
    $r_{\phi}(s,a)$ to redistribute the delayed reward to each time step. This is
    achieved by solving a bi-level optimization problem as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\phi}\mathcal{L}_{\text{val}}(\theta^{*}(\phi),\phi)\ s.t.\ R(\tau)=\sum_{t=0}^{T}r_{\phi}(s_{t},a_{t}),\
    \forall\tau,\ \text{and}\ \theta^{*}(\phi)=\arg\min_{\theta}\mathcal{L}_{\text{train}}(\theta,\phi)$
    |  | (85) |'
  prefs: []
  type: TYPE_TB
- en: Here, $\theta$ is the parameter of the policy $\pi$; $\mathcal{L}_{\text{train}}(\theta,\phi)$
    denotes the DT objective on the training dataset, which is augmented by replacing
    the original sparse rewards $r_{t}$ with $r_{\phi}(s_{t},a_{t})$; $\mathcal{L}_{\text{val}}(\theta^{*}(\phi),\phi)$
    is the DT objective on the augmented validation dataset. Intuitively, $r_{\phi}$
    is updated so that the policy learned from the dataset augmented with the dense
    rewards is optimal. They propose a practical alternative training framework for
    $\theta$ and $\phi$, which however lacks convergence guarantee in theory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another line of works in this category focus on modifying the DT model to explicitly
    make use of the structural patterns within the training trajectories. As mentioned
    in Section [2.4](#S2.SS4 "2.4 Transformers ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), introducing strutual assumptions/priors to the input data
    can greatly improve the learning efficiency and mitigate the overfitting issue
    of learning on small-scale datasets. (1) Based on the observation that each trajectory
    is a sequence of state-action-reward triplets, StARformer (Shang et al. ([2022](#bib.bib292)))
    proposes to first extract representations $z_{t}$ from each $(r_{t-1},a_{t-1},s_{t})$,
    and then apply DT on $(z_{1},\cdots,z_{t})$ to predict $a_{t}$. Note that they
    adopt $r_{t}$ rather than $R_{t}$, which is a counterintuitive design, as RL decision-making
    typically requires information on the future return. (2) GDT (Hu et al. ([2023b](#bib.bib140)))
    follows the same motivation as StARformer, recognizing that naively attending
    to all previous tokens, as in DT, can overlook the causal relationships among
    certain types of tokens and thus hurt the performance. In particular, they introduce
    the relation embeddings to the compatibility calculation (i.e., Eq. ([20](#S2.E20
    "In 2nd item ‣ 2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))) for each pair of inputs $(x_{i},x_{j})$. With this modification,
    the compatibility between the query $Q_{i}$ and key $K_{j}$ can be calculated
    as $\langle Q_{i}+re_{i\rightarrow j},K_{j}+re_{j\rightarrow i}\rangle$, where
    $re$ is an embedding of the adjacency matrix that represents the causal relationships
    among elements in the input sequence. For instance, $a_{t}$ is directly influenced
    by $s_{t}$ and $R_{t}$, so there should be edges from $s_{t}$ to $a_{t}$ and from
    $R_{t}$ to $a_{t}$ in the casual graph. In this way, they incorporate the Markovian
    relationship into the DT model. (3) DTd (Wang et al. ([2023c](#bib.bib341))) views
    the state, action, and RTG as distinct modalities. By analyzing attention values
    from a trained DT, they rank the importance of token interactions (for decision-making)
    within or cross modalities. Specifically, $s-s$, $a-a$, $R-R$ < $s-R$, $a-R$ <
    $s-a$, where $s-s$ denotes interactions (i.e., attending via the MHA) between
    states, and ‘<’ denotes being less important. Consequently, they suggest a hierarchical
    DT structure, processing less crucial interactions before more significant ones
    to prevent important interactions from being distracted. The resulting structure
    is large in scale and outperforms DT, but its overall performance is close to
    traditional offline RL methods like CQL.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 Mitigating Impacts from Environmental Stochasticity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another category of research works propose to change the conditional information
    of the DT to enhance its performance in stochastic environments. (1) In the absence
    of determinism, a high-return offline trajectory could be the outcome of uncontrollable
    environmental randomness, rather than the result of the agent’s actions. Thus,
    goals that are independent from the environmental stochasticity are the only conditions
    that the learned agent can reliably achieve. ESPER (Paster et al. ([2022](#bib.bib243)))
    and DOC (Yang et al. ([2023c](#bib.bib373))) propose methods to learn such conditions
    automatically from the offline dataset. They share a common intuition: to ensure
    the conditional variable to contain sufficient information on action predictions
    but no information regarding the environment dynamics (e.g., on predicting future
    rewards or state transitions). Both algorithms use a contrastive learning framework,
    resulting in similar objectives. However, these methods require to learn at least
    five networks (so we choose not to show the objectives here), which is more complicated
    than traditional offline RL approaches. (2) SPLT (Villaflor et al. ([2022](#bib.bib331)))
    proposes to model the policy as $\pi(\hat{a}_{t}|(s_{t-k},a_{t-k},\cdots,s_{t}),z^{\pi}_{t})$,
    where the condition $z^{\pi}_{t}$ is sampled from an encoder that takes $(s_{t-k},a_{t-k},\cdots,s_{t},a_{t})$
    as input. Both the encoder and $\pi$ have DT-like architectures and are trained
    jointly in a CVAE framework, where $\pi$ works as the decoder, $(s_{t-k},a_{t-k},\cdots,s_{t})$
    and $a_{t}$ work as the condition $c$ and data point $x$ respectively. $z^{\pi}_{t}$
    is a $d_{\pi}$-dim discrete vector, each dimension of which has $c$ categories.
    Thus, extracted by the CVAE, each specific $z^{\pi}_{t}$ can be viewed as a mode
    that is contained in the offline data and can be assigned to $\pi$ as a generation
    condition. In the same manner, they learn a prediction model $\omega(\hat{r}_{t},\hat{s}_{t+1},\widehat{R}_{t+1}|(s_{t-k},a_{t-k},\cdots,s_{t},a_{t}),z^{\omega}_{t})$.
    During inference, with $(\pi,\omega)$ and by enumerating $(z^{\pi}_{t},z^{\omega}_{t})$,
    $c^{d_{\pi}\times d_{\omega}}$ predictions on future $h$-length trajectories $\hat{\tau}_{t}^{h}$
    can be obtained, and the next action is selected from the mode: $\arg\max_{z^{\pi}_{t}}\min_{z^{\omega}_{t}}\widehat{R}(\hat{\tau}_{t}^{h})$
    for robustness, where $\widehat{R}(\widehat{\tau}_{t}^{h})=\sum_{i=0}^{h-1}\hat{r}_{t+i}+\widehat{R}_{t+h}$.
    Through enumeration, the influence brought by the environmental randomness can
    be mitigated. (3) CGDT (Wang et al. ([2023d](#bib.bib342))) suggests training
    an additional Q-network to guide the learning of DT. The rationale behind this
    method is that the Q-value, representing the expected return for each state-action
    pair, can help mitigate the impact of environmental stochasticity that might be
    captured by single-trajectory RTG values.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.4 Transformers in Extended Offline Reinforcement Learning Setups
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DT has been extended to various offline RL setups, including model-based, hierarchical,
    multi-agent, and multi-task offline RL. Unlike the previous three subsections,
    extensions in this part primarily expand on the problem setup rather than the
    algorithmic aspect with respect to DT. Thus, this part is not our primary focus
    and we only provide a taxonomy and brief introduction of the related works, serving
    as a reference for readers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model-based/Hierarchical/Multi-agent Offline RL: (1) Wang et al. ([2023b](#bib.bib339))
    propose Environment Transformer to model the transition dynamics and reward function
    $P(s_{t+1},r_{t}|s_{t},a_{t})$ from the offline dataset, which can be viewed as
    simply replacing the output of DT, i.e., $a_{t}$, with $(s_{t+1},r_{t})$ for the
    learning process. The learned environment model can then be utilized as a simulator
    for policy learning with any RL algorithm. TransDreamer (Chen et al. ([2022](#bib.bib39)))
    suggests replacing the RNN module within Dreamer (Hafner et al. ([2020](#bib.bib118))),
    which is a state-of-the-art (SOTA) model-based RL algorithm, with a transformer,
    for improved performance on tasks requiring modeling long-term temporal dependency.
    (2) HDT (Correia & Alexandre ([2022](#bib.bib62))) employs two DTs for hierarchical
    decision making. The high-level DT models the distribution $\pi_{\text{high}}(g_{t}|s_{t-k},g_{t-k},\cdots,s_{t})$
    for selecting a subgoal, while the low-level DT captures the corresponding action
    distribution $\pi_{\text{low}}(a_{t}|s_{t-k},g_{t-k},a_{t-k},\cdots,s_{t},g_{t})$.
    Moreover, they propose a heuristic approach for extracting subgoals $g_{1:T}$
    from the offline trajectories, when these subgoals are not labeled. Skill DT (Sudhakaran
    & Risi ([2023](#bib.bib311))) adopts a similar hierarchical framework, but proposes
    to use a VQ-VAE (introduced in Section [2.1](#S2.SS1 "2.1 Variational Auto-Encoders
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) as
    the high-level policy to extract the skill choices for the low-level policy to
    condition on. (3) MADT+Distillation (Tseng et al. ([2022](#bib.bib323))) is proposed
    to reformulate offline MARL as a sequence modelling problem. First, a teacher
    (joint) policy $\pi(a^{1:n}_{t}|o^{1:n}_{t-k},R^{1:n}_{t-k},a^{1:n}_{t-k},\cdots,o^{1:n}_{t},R^{1:n}_{t})$
    is learned, as a manner of centralized training. The learning process is the same
    as DT, viewing the concatenation of observations (RTGs/actions) from $n$ agents
    as a single joint observation (RTG/action). Then, $\pi$ is distilled to $n$ student
    (individual) policies $\pi^{i}(a^{i}_{t}|o^{i}_{t-k},R^{i}_{t-k},a^{i}_{t-k},\cdots,o^{i}_{t},R^{i}_{t})$,
    for decentralized execution. The policy distillation process is designed for $\pi^{1:n}$
    to imitate $\pi$ while maintaining the structural relation among these agents
    as in the joint policy $\pi$, which though does not rely on transformers. There
    are some other explorations for the multi-agent scenario: MADT (Meng et al. ([2021](#bib.bib221)))
    investigates combining offline pretraining using DT and online adaptation with
    gradient-based MARL algorithms; SCT (Li et al. ([2023a](#bib.bib184))) proposes
    approaches for handling non-cooperative MARL scenarios. However, these two works
    are still in early stages of development.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-task Offline RL: As a foundation model, the transformer has demonstrated
    exceptional generalization capabilities in NLP and CV tasks. Thus, there is a
    branch of research works on enhancing DT to enable policies pretrained on source
    tasks to be effectively adapted to target tasks through fine-tuning. The target
    task may be the same as the source task, differ but still fall within the same
    task distribution with the source task, or belong to entirely different modalities
    from the source task. We provide a brief review of these three categories as follows,
    each progressively demanding greater generalization capabilities. (1) Further
    adaption on the same task may be required when the provided offline data is highly
    sub-optimal or only covers a limited part of the state space, for which ODT (Zheng
    et al. ([2022](#bib.bib391))) introduces a technique to fine-tune the pretrained
    DT through further online interactions with the environment. Another possible
    scenario is when the high-quality and labelled offline data is scarce while there
    are abundant unlabeled (i.e., reward-free) and sub-optimal trajectories. In this
    case, PDT (independently proposed by Cang et al. ([2022](#bib.bib35)) and Xie
    et al. ([2023](#bib.bib360))) can be used for unsupervised pretraining on the
    unlabeled data, followed by fine-tuning using the limited high-quality data. (2)
    Adaptation to unseen tasks within the same task distribution is a primary focus
    in multi-task/meta (offline) RL. For extending DT in this realm, some studies
    suggest conditioning the DT on task-specific information $z\sim P_{Z}(\cdot)$
    and training the DT across a range of tasks, each associated with unique embeddings
    $z_{\text{train}}\sim P_{Z}(\cdot)$. Consequently, when encountering new tasks
    sampled from $P_{Z}(\cdot)$ and conditioning on the corresponding $z_{\text{test}}$,
    the policy is expected to be effective in these previously unseen tasks after
    few/zero-shot fine-tuning. $z$ can be a task id (Lin et al. ([2022a](#bib.bib192))),
    segment of a task trajectory (Xu et al. ([2022b](#bib.bib362))), or trajectory
    embedding from an encoder (Lin et al. ([2022b](#bib.bib193))). Alternatively,
    Boustati et al. ([2021](#bib.bib29)) suggests running the source policy in similar
    but counterfactual environments to gather augmented data for DT training, to enhance
    the robustness of the policy on unseen tasks. (3) So far, the transformer has
    been successfully adopted in NLP, CV, and RL, which makes it possible to build
    a transformer-based generalist agent across these three modalities. Gato (Reed
    et al. ([2022](#bib.bib271))) shows that a single transformer with the same set
    of weights can handle 604 distinct tasks with varying modalities. Considering
    that texts, images, and decision trajectories can all be tokenized and embedded,
    the transformer can learn to generate all modalities within a unified training
    framework – predicting the next token based on previously generated ones. Specifically,
    the training objective aligns with Eq. ([83](#S6.E83 "In 6.1.1 Background on Trajectory-Optimization-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), but substitutes the
    training data with sequences of words for text generation and sequences of image
    patches for image understanding. During training, each batch of training data
    incorporates sequences from various modalities and each sequence is concatenated
    with a demonstration for the same task, serving as a prompt to assist the agent
    in differentiating between modalities. Gato utilizes a decoder-only transformer
    with 1.2 billion parameters and is trained on a dataset containing 1.76 trillion
    tokens. The empirical results show that Gato achieves performance exceeding 50%
    of the expert score threshold in 450 of the 604 evaluation tasks. Gato represents
    a significant attempt in developing generalist agents, yet its efficacy heavily
    depends on the quantity and quality of the training data. Especially, when learning
    control policies, it doesn’t utilize reward signals, so its performance would
    be capped by the provided demonstrations. Multi-Game DT (Lee et al. ([2022](#bib.bib178)))
    adopts a similar protocol with Gato and suggests how to utilize reward signals
    in both the training and inference process. Another critical question regrading
    multi-modal learning is whether the learning in distinct modalities can enhance
    each other. Reid et al. ([2022](#bib.bib272)) empirically show that pretrained
    language models can be effectively adapted to offline RL tasks with boosted performance.
    Following this work, Takagi ([2022](#bib.bib316)) provides an excellent in-depth
    empirical study on the effect of cross-modality pretraining for DT. They find
    that parameters of the language-pretrained model do not change that much as a
    randomly-initialized model after the fine-tuning, and these stable parameters
    preserves context-equivalent information such that the agent can make decisions
    even in the absence of contexts (i.e., without using the previous $k$ transitions
    as input). Further, the way to efficiently utilize these context-equivalent information
    is also preserved by those stable parameters. As a result, they conclude that
    pretrained context-like information could help the DT training by enabling the
    model to predict action more accurately. In contrast, a transformer pretrained
    on images performs significantly worse than a standard DT, partly due to the significant
    differences in data characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Key Novelty | Evaluation Task |'
  prefs: []
  type: TYPE_TB
- en: '| DT | Solving offline RL as sequence modeling | Atari, D4RL (L) |'
  prefs: []
  type: TYPE_TB
- en: '| TT | Tokenization, discretization and beam search | D4RL (L, M) |'
  prefs: []
  type: TYPE_TB
- en: '| Q-Transformer | Transformer-based Q learning | Real Robot |'
  prefs: []
  type: TYPE_TB
- en: '| Boot Transformer | Data augmentation with self-generated data | D4RL (L,
    A) |'
  prefs: []
  type: TYPE_TB
- en: '| SS-DT | Data augmentation via an inverse dynamic model | D4RL (L, M2d) |'
  prefs: []
  type: TYPE_TB
- en: '| QDT | Replacing RTGs with Q-values to enable stitching | D4RL (L, M2d) |'
  prefs: []
  type: TYPE_TB
- en: '| CDT | Data augmentation for safe RL in constrained MDP | Bullet-safety-gym
    |'
  prefs: []
  type: TYPE_TB
- en: '| DTRD | Data augmentation for sparse, delayed reward setups | Atari, MiniGrid,
    D4RL (L, K, M2d) |'
  prefs: []
  type: TYPE_TB
- en: '| StARformer | Processing $(r_{t-1},a_{t-1},s_{t})$ as a group | Atari, dm_control
    |'
  prefs: []
  type: TYPE_TB
- en: '| GDT | Embedding the causal relation among $s,a,R$ | Atari, D4RL (L) |'
  prefs: []
  type: TYPE_TB
- en: '| DTd | Processing $s,a,R$ as three distinct modalities | D4RL (L, M) |'
  prefs: []
  type: TYPE_TB
- en: '| ESPER | Replacing RTGs with a stochasticity-free conditioner | Stochastic
    Benchmark |'
  prefs: []
  type: TYPE_TB
- en: '| DOC | Replacing RTGs with a stochasticity-free conditioner | MuJoCo, D4RL
    (M), FrozenLake |'
  prefs: []
  type: TYPE_TB
- en: '| SPLT | Enumerating future trajs to mitigate stochasticity | CARLA |'
  prefs: []
  type: TYPE_TB
- en: '| CGDT | Critic-guided DT training | D4RL (L, M) |'
  prefs: []
  type: TYPE_TB
- en: '| Env Trans (MB) | Modeling the dynamic and reward functions | D4RL (L) |'
  prefs: []
  type: TYPE_TB
- en: '| TransDreamer (MB) | Integrating Dreamer with a transformer architecture |
    Hidden Order Discovery, dm_control, Atari |'
  prefs: []
  type: TYPE_TB
- en: '| HDT (HRL) | Subgoal-conditioned decision-making | D4RL (L, A, K) |'
  prefs: []
  type: TYPE_TB
- en: '| Skill DT (HRL) | Skill extraction via a VQ-VAE | D4RL (L, M) |'
  prefs: []
  type: TYPE_TB
- en: '| MADT+Dist (MA) | A centralized training with decentralized execution (CTDE)
    framework for MARL based on policy distillation | Fill-In, Equal Space, SMAC,
    Grid-World, Highway |'
  prefs: []
  type: TYPE_TB
- en: '| MADT (MA) | Applying a shared DT to each agent’s sequence | SMAC |'
  prefs: []
  type: TYPE_TB
- en: '| SCT (MA) | A DT-based method for non-cooperative MARL | simple-tag, simple-world
    |'
  prefs: []
  type: TYPE_TB
- en: '| ODT (PT) | An online fine-tuning method for DT | D4RL (L, M) |'
  prefs: []
  type: TYPE_TB
- en: '| PDT (PT) | Pretraining on large-scale reward-free trajectories | D4RL (L)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MG DT (MT) | A single transformer agent for 46 Atari games | Atari |'
  prefs: []
  type: TYPE_TB
- en: '| Gato (MT) | A generalist agent for 604 cross-modality tasks | See Reed et al.
    ([2022](#bib.bib271)) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Summary of transformer-based offline RL algorithms. In Column 1, we
    list representative (but not all) algorithms in this section, where Boot Transformer,
    Env Trans, MADT+Dist, and MG DT correspond to Bootstrapped Transformer, Environment
    Transformer, MADT+Distillation, and Multi-Game DT, respectively. These algorithms
    are grouped by their categories, with abbreviations MB, HRL, MA, PT, and MT denoting
    model-based, hierarchical, multi-agent, pretraining-based, and multi-task offline
    RL, respectively. The evaluation tasks are listed in Column 3\. Most works are
    evaluated on D4RL (Fu et al. ([2020](#bib.bib99))), which provides offline datasets
    for various tasks, including Locomotion (L), AntMaze (M), Adroit (A), Kitchen
    (K), Maze2d (M2d). Regarding the other benchmarks, we provide their references
    here: Atari (Bellemare et al. ([2013](#bib.bib22))), Real Robot (Chebotar et al.
    ([2023](#bib.bib38))), Bullet-safety-gym (Gronauer ([2022](#bib.bib110))), MiniGrid
    (Chevalier-Boisvert et al. ([2018](#bib.bib52))), dm_control (Tassa et al. ([2018](#bib.bib317))),
    Stochastic Benchmark (Paster et al. ([2022](#bib.bib243))), MuJoCo (Todorov et al.
    ([2012](#bib.bib320))), FrozenLake (Foundation ([2023](#bib.bib93))), CARLA (Dosovitskiy
    et al. ([2017](#bib.bib78))), Hidden Order Discovery (Chen et al. ([2022](#bib.bib39))),
    SMAC (Samvelyan et al. ([2019](#bib.bib283))), Fill-In & Equal Space & Grid-World
    & Highway (Meng et al. ([2021](#bib.bib221))), simple-tag & simple-world (Li et al.
    ([2023a](#bib.bib184))).'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.5 Reflections on Transformer-based Offline Reinforcement Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We notice that there is a series of works on applying transformers to online
    partially-observable RL (Yuan et al. ([2023a](#bib.bib382))). They adopt the transformer
    as the policy network in gradient-based or actor-critic RL algorithms, with the
    hope to harness its ability to process long-horizon historical information for
    decision making. These works focus on architectural modifications of the standard
    transformer, which are specially tailored for RL. For example, GTrXL (Parisotto
    et al. ([2020](#bib.bib241))) and Catformer (Davis et al. ([2021](#bib.bib67)))
    suggest adjustments to the MHA module to facilitate a more stable RL training
    process; ALD (Parisotto & Salakhutdinov ([2021](#bib.bib240))) adopts model distillation
    to improve the computation efficiency in transformer-based distributed RL; STT
    (Yang et al. ([2022f](#bib.bib375))) and WMG (Loynd et al. ([2020](#bib.bib200)))
    propose adaptions for the transformer to process spatiotemporal coupling observarions
    and factored observations, respectively, for enhanced sample efficiency. These
    architectural modifications, or potentially new ones, could be integrated with
    policy-gradient-based offline RL algorithms or DT-like return-conditioned supervised
    learning (RCSL) algorithms for performance improvement. Conducting an (empirical)
    comparison between these two categories (i.e., RCSL and policy gradient offline
    RL), both utilizing the same transformer architecture, would be an intriguing
    study. There is also a growing body of research on using the transformer as the
    policy/value network in other online RL settings. For multi-agent RL, notable
    examples include MAT (Wen et al. ([2022](#bib.bib352))), UPDET (Hu et al. ([2021](#bib.bib141))),
    ATM (Yang et al. ([2022e](#bib.bib374))), and TransMix (Khan et al. ([2022](#bib.bib159)));
    while in multi-task RL, there is DCRL (Dance et al. ([2021](#bib.bib65))). These
    advancements can be potentially adapted to offline settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we reflect on these DT-like RCSL methods. Although a series of improvements
    have been made, there are still fundamental problems or limitations regarding
    these algorithms. According to (Brandfonbrener et al. ([2022](#bib.bib30))), RCSL
    returns near-optimal policy under a set of assumptions that are stronger than
    those needed for traditional RL algorithms, and RCSL alone is unlikely to be a
    general solution for offline RL problems. In particular, RCSL offers guarantees
    only when the environment dynamics (including the state transition and reward
    functions) are nearly deterministic, a priori knowledge of the optimal conditioning
    function (i.e., the RTG values) is available, and the return distribution of the
    provided offline trajectories can cover the possible values of the conditioning
    function. These findings inspire several directions for future research. First,
    adapting RCSL algorithms to stochastic environments, ideally backed by theoretical
    optimality guarantees, is a clear necessity. Second, developing effective strategies
    for selecting RTG values as the conditions during inference, or alternatively,
    exploring new conditioning functions, is crucial. Last, a significant challenge
    lies in addressing out-of-distribution scenarios in case that the offline dataset
    lacks adequate coverage of the task scenarios. Another fundamental problem regarding
    RCSL is whether it is necessary to learn low-return behaviors from the offline
    dataset and to require strong alignment between the target return (i.e., the condition
    of the DT) and realized return. As indicated in Eq. ([83](#S6.E83 "In 6.1.1 Background
    on Trajectory-Optimization-based Offline Reinforcement Learning ‣ 6.1 Offline
    Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), the policy training involves imitating trajectories across a range
    of returns. However, only high-return policy is required during inference. This
    raises the possibility of developing a more efficient mechanism that strategically
    utilizes low-return samples to enhance the learning of a high-return policy.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Imitation Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similarly with DT, transformer-based IL utilizes a supervised learning paradigm
    (see Section [6.2.1](#S6.SS2.SSS1 "6.2.1 A Paradigm of Transformer-based Imitation
    Learning ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")), where the transformer is adopted as the policy backbone
    (see Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Adopting Transformers as the Policy Backbone
    for Imitation Learning ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")). We differentiate works for IL and offline
    RL simply by if the reward or return is used as a condition of the policy. A significant
    advantage of the transformer is its capability to process large quantities of
    training data across multiple modalities, which enables the development of generalist
    agents, as shown in Section [6.2.3](#S6.SS2.SSS3 "6.2.3 Developing Generalist
    Imitation Learning Agents with Transformers ‣ 6.2 Imitation Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions").'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 A Paradigm of Transformer-based Imitation Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given expert demonstrations $D_{E}=\{\tau_{E}=(s_{0},a_{0},\cdots,s_{T},a_{T})\}$,
    transformer-based IL algorithms are designed to learn a policy $\pi$ to replicate
    expert behaviors. The objective function is typically formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\pi}\mathbb{E}_{D_{E}}\left[(a_{t}-\log\pi(s_{t},x_{t-1}\cdots,x_{t-k}))^{2}\right],\
    \text{or}\ \min_{\pi}\mathbb{E}_{D_{E}}\left[-\log\pi(a_{t}&#124;s_{t},x_{t-1},\cdots,x_{t-k})\right]$
    |  | (86) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $x_{t}\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}(s_{t},a_{t})\
    \text{or}\ s_{t}$, and $\pi$ is implemented as a decoder-only transformer (as
    introduced in Section [2.4](#S2.SS4 "2.4 Transformers ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")). This objective is similar in form with
    the one of DT (i.e., Eq. ([83](#S6.E83 "In 6.1.1 Background on Trajectory-Optimization-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))) but replaces $(s_{t},a_{t},R_{t})$
    with $x_{t}$. IL algorithms do not require reward signals but place an emphasis
    on the quality of demonstrations, which ideally should come from experts, as the
    learning process relies solely on imitation. Also, to make full use of demonstrations,
    auxiliary supervision objectives, such as prediction errors on the next state
    (i.e., the forward model loss) or the intermediate action between two consecutive
    states (i.e., the inverse model loss), are often employed. These objectives complement
    the action prediction error (i.e., Eq. ([86](#S6.E86 "In 6.2.1 A Paradigm of Transformer-based
    Imitation Learning ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"))) to aid the policy learning process.
    As a representative, Behavior Transformer (BeT, Shafiullah et al. ([2022](#bib.bib288)))
    empirically shows that a standard transformer architecture (specifically minGPT
    Brown et al. ([2020](#bib.bib34))) can significantly outperform commonly-used
    policy networks, like MLP and LSTM, in learning from large-scale, human-generated
    demonstrations, which typically exhibit high variance and multiple modalities.
    In particular, $x_{t}$ is defined as $s_{t}$. To cover the multiple modes in expert
    behaviors, the authors suggest dividing the action $a$ into two components: its
    corresponding action center $\lfloor a\rfloor$ and the residual action $\langle
    a\rangle$, such that $a=\lfloor a\rfloor+\langle a\rangle$. The set of action
    centers can be acquired by applying K-means clustering to the expert actions.
    Correspondingly, they apply a policy network $\pi$ with two prediction heads,
    one for the action center and the other for the residual action. Explicitly employing
    clustering to identify the various modes in expert actions and utilizing dual-head
    predictions to reason the mode of each instance significantly aids in modelling
    multi-modal actions. However, this approach does not model the multi-modality
    that may exist in the joint distribution of $(s,a)$.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Adopting Transformers as the Policy Backbone for Imitation Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A series of studies (Kim et al. ([2021](#bib.bib160)); Pan et al. ([2022](#bib.bib237));
    Kim et al. ([2022](#bib.bib161)); Zhu et al. ([2022](#bib.bib395)); Chen et al.
    ([2023a](#bib.bib42)); Kim et al. ([2023](#bib.bib162)); Liang et al. ([2023a](#bib.bib189)))
    have adopted transformers as the policy backbone for imitation learning in complex
    control tasks, such as vision-based robotic manipulation and end-to-end self-driving.
    As introduced in Section [2.4](#S2.SS4 "2.4 Transformers ‣ 2 Background on Deep
    Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), the transformer has various advantages
    over CNNs and RNNs. Firstly, the transformer is adept at processing time-series
    data (Kim et al. ([2022](#bib.bib161))) by capturing the long-term temporal dependencies
    between inputs across different time steps. By using the current state as the
    query and historical data as keys and values, the transformer-based agent can
    pinpoint vital information in the sequence for the current decision-making through
    the attention mechanism. Secondly, the transformer can concurrently process various
    types of input data, such as texts and images (Kamath et al. ([2023](#bib.bib156))),
    texts and voxels (Shridhar et al. ([2022](#bib.bib296))), point clouds (Pan et al.
    ([2022](#bib.bib237))), enabling it to be the backbone of an end-to-end deep learning
    agent. Each type of data can be embedded to vectors by (pretrained) domain-specific
    neural networks. Then, instead of simply concatenating all these embeddings for
    subsequent processing, the transformer treats each embedding as distinct tokens.
    As detailed in Section [2.4](#S2.SS4 "2.4 Transformers ‣ 2 Background on Deep
    Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), each token possesses its own
    query, key, and value, and can attend to the other tokens for more informative
    aggregation. Thirdly, the transformer is efficient in processing input that contains
    multiple entities by modeling their interrelations. For example, TSE (Liang et al.
    ([2023a](#bib.bib189))) takes the state of the ego vehicle, the status of surrounding
    vehicles, and lane information as input, explicitly reasoning the relationship
    between the ego vehicle and its surrounding environment for self-driving decision-making;
    Silver-Bullet-3D (Pan et al. ([2022](#bib.bib237))) models the relationship of
    embeddings corresponding to different parts of the manipulated objects and robotic
    arms through the transformer for complex manipulation tasks; VIOLA (Zhu et al.
    ([2022](#bib.bib395))) extracts a series of object-centric representations from
    the visual observation, introduces an extra action token (whose corresponding
    output is the action prediction), and applies a transformer on top of these tokens,
    so that the action token can learn to attend to and focus on task-relevant objects
    for improved performance in action prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Developing Generalist Imitation Learning Agents with Transformers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another research focus in this field is language-conditioned IL, aiming at
    training (robot) agents to follow human instructions. A language instruction,
    comprising a sequence of words, can be embedded into a sequence of tokens $(l_{1},\cdots,l_{m})$
    through a pretrained language model or a predefined embedding table. (1) A straightforward
    method for language-conditioned IL, as shown in TDT (Putterman et al. ([2022](#bib.bib259))),
    is to incorporate language tokens into the policy input, i.e., $\pi(a_{t}|s_{t},x_{t-1},\cdots,x_{t-k},l_{1},\cdots,l_{m})$.
    Still, $\pi$ is implemented as a decoder-only transformer, which, as previously
    mentioned, is capable of processing multiple types of input concurrently. Perceiver-Actor
    (Shridhar et al. ([2022](#bib.bib296))) adopts a similar design, but suggests
    using the Perceiver Transformer (Jaegle et al. ([2022](#bib.bib146))) as the policy
    backbone to manage extra long sequence of input (e.g., a sequence of image/voxel
    patches from the vision input and word tokens from the language input). (2) MARVAL
    (Kamath et al. ([2023](#bib.bib156))) uses an encoder-only transformer to process
    inputs comprising four modalities: instruction texts $(l_{1},\cdots,l_{m})$, historical
    states and actions $(x_{t-k},\cdots,x_{t-1})$, the current state $s_{t}$, and
    the current action candidates $\mathcal{A}_{t}$. As discussed in Section [2.4](#S2.SS4
    "2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    without the Masked MHA component as in the transformer decoder, each input token
    can attend to every other token for more informative aggregation. However, each
    forward pass predicts only a single action $a_{t}$, making it less efficient in
    training than the decoder-only transformer. In decoder-only transformers, the
    output token corresponding to the state input $s_{i}$ is used to predict $a_{i}$
    ($i=0,\cdots,t$). In contrast, encoder-only transformers require a special action
    token, i.e., CLS, as an input to capture the fused representation of the entire
    sequence via the attention mechanism, and its corresponding output token is utilized
    to predict the current action $a_{t}$. Additionally, MARVAL introduces auxiliary
    tasks, including predicting the masked language tokens and predicting the proportion
    of the trajectory that has been completed, to increase the amount of supervision
    for more efficient use of the demonstrations. (3) Instead of using instruction
    texts as conditions, Lang (Hejna et al. ([2023](#bib.bib128))) proposes to predict
    corresponding instructions from the state sequence as an auxiliary task, to realize
    language-guided IL. Specifically, each demonstration trajectory is divided into
    $m$ segments, with each segment $(s_{T_{i}},\cdots,s_{T_{t+1}-1})$ associated
    with a subtask instruction $l^{(i)}=(l_{1}^{(i)},\cdots,l^{(i)}_{b_{i}})$. A transformer
    encoder is applied to extract representations $(z_{1},\cdots,z_{t})$ from the
    state sequence $(s_{1},\cdots,s_{t})$. These representations are expected to encompass
    information essential for predicting both actions and instructions, achieved by
    minimizing the following equation: $-\sum_{t=1}^{T}\log\pi(a_{t}|z_{1},\cdots,z_{t})-\lambda\sum_{i=1}^{m}\sum_{j=1}^{b_{i}}\log
    P(l_{j}^{(i)}|l_{1}^{(i)},\cdots,l_{j-1}^{(i)},z_{1},\cdots,z_{T_{i}-1})$. This
    approach offers an alternative way to incorporate language instructions into action
    predictions and has proven effective when training with a limited amount of demonstrations.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Key Novelty | Evaluation Task |'
  prefs: []
  type: TYPE_TB
- en: '| BeT | Dual-head predictions for the action center and residual action, seperately.
    | CARLA, Block Push, Franka Kitchen |'
  prefs: []
  type: TYPE_TB
- en: '| TSE | A transformer-based policy for end-to-end self-driving. | SMARTS |'
  prefs: []
  type: TYPE_TB
- en: '| Silver-Bullet -3D | A transformer-based control policy for complex manipulation
    skills with a robotic arm. | ManiSkill |'
  prefs: []
  type: TYPE_TB
- en: '| VIOLA | Adopting the transformer to identify task-relevant objects in a multi-object
    environment. | Sorting, Stacking, BUDS-Kitchen, real-world tasks |'
  prefs: []
  type: TYPE_TB
- en: '| TDT | Language-conditioned IL with the transformer. | Atari Frostbite |'
  prefs: []
  type: TYPE_TB
- en: '| Perceiver-Actor | Applying the Perceiver Transformer to manage long input
    sequences in Language-conditioned IL. | RLBench, real-world tasks |'
  prefs: []
  type: TYPE_TB
- en: '| MARVAL | Segmenting the input into 4 modalities and applying various auxiliary
    tasks for more efficient use of the demonstrations. | Matterport, Gibson |'
  prefs: []
  type: TYPE_TB
- en: '| Lang | Improving long-horizon imitation by representation learning based
    on instruction predictions. | BabyAI, Crafting, ALFREAD |'
  prefs: []
  type: TYPE_TB
- en: '| Transformer Adapter | Introducing Adapters to pretrained transformers for
    lightweight task-specific fine-tuning. | MetaWorld |'
  prefs: []
  type: TYPE_TB
- en: '| DualMind | A dual-phase training framework for generalist agents. | MetaWorld,
    Habitat |'
  prefs: []
  type: TYPE_TB
- en: '| MIA | Training an interactive agent by imitation of human-human interactions
    and self-supervision via modality matching. | 3D Playhouse |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Summary of transformer-based IL algorithms. Representative (but not
    all) algorithms in this section with their key novelties and evaluation tasks
    are listed in this table. These algorithms are all grounded in the Behavioral
    Cloning framework, hence the algorithmic and theoretical advancements in this
    section are relatively modest. However, the learned transformer-based agents are
    evaluated on much more diverse, realistic, and challenging tasks. First, there
    are some commonly-used benchmarks. CARLA (Dosovitskiy et al. ([2017](#bib.bib78)))
    is a simulated environment for self-driving tasks. Block Push (Florence et al.
    ([2021](#bib.bib91))), Franka Kitchen (Gupta et al. ([2019a](#bib.bib114))), RLBench
    (James et al. ([2020](#bib.bib147))), MetaWorld (Yu et al. ([2019c](#bib.bib379)))
    provide a range of robotic manipulation tasks, featuring various types of robots
    and input modalities. Habitat (Ramakrishnan et al. ([2021](#bib.bib268))) provides
    navigation tasks for embodied AI in large scale 3D environments. BabyAI (Chevalier-Boisvert
    et al. ([2019](#bib.bib53))), Crafting (Chen et al. ([2021c](#bib.bib49))), ALFREAD
    (Shridhar et al. ([2020](#bib.bib295))) offer instruction-conditioned tasks and
    datasets. Second, some algorithms have shown superior performance on competitions.
    In the table, SMARTS refers to the NeurIPS 2022 Driving SMARTS Competition (Rasouli
    et al. ([2022](#bib.bib270))), and ManiSkill refers to SAPIEN ManiSkill Challenge
    2021 (Mu et al. ([2021](#bib.bib226))). Third, in the table, VIOLA, TDT, MARVAL,
    and MIA each develop their own environments or create large-scale training datasets
    tailored to their specific purposes. These efforts can be regarded as significant
    contributions alongside their algorithm designs. Readers are encouraged to read
    the respective papers for details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Language-conditioned IL can be viewed as an instance of multi-task IL, where
    language texts serve as task contexts. (1) Generally speaking, $(l_{1},\cdots,l_{m})$
    could be substituted with any form of task context (Furuta et al. ([2022](#bib.bib101))).
    For example, Dasari & Gupta ([2020](#bib.bib66)) suggest using demonstration videos
    from human operators, denoted as $v$, as contexts for corresponding tasks. They
    train the policy $\pi(a_{t}|s_{t},\cdots,s_{t-k},v)$ using Eq. ([86](#S6.E86 "In
    6.2.1 A Paradigm of Transformer-based Imitation Learning ‣ 6.2 Imitation Learning
    ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), supplemented
    by an inverse model auxiliary loss term. (2) Transformer Adapter (Liang et al.
    ([2022](#bib.bib188))) presents an alternative approach for multi-task IL through
    pretraining and fine-tuning. Initially, a transformer policy $\pi(a_{t}|s_{t-k},a_{t-k},\cdots,s_{t-1},a_{t-1},s_{t})$,
    without task contexts, is pretrained on extensive multi-task demonstrations using
    Eq. ([86](#S6.E86 "In 6.2.1 A Paradigm of Transformer-based Imitation Learning
    ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")). It is then fine-tuned with a limited amount of task-specific demonstrations
    before application. For efficient fine-tuning, the pretrained model’s parameters
    are kept unchanged. Instead, lightweight adapters (Houlsby et al. ([2019](#bib.bib138)))
    are introduced between the pretrained model’s layers, with only these adapters
    being updated using the task-specific demonstrations. The structure of the adapter
    can be $\text{Adapter}(X)=X+W^{A}_{2}(\text{GeLU}(W^{A}_{1}(X)))$, where GeLU
    (Hendrycks & Gimpel ([2023](#bib.bib129))) is the activation function and $W^{A}_{1,2}$
    are the weight matrices of the adapter, and it is inserted between the point-wise
    FFN layer and Add & Normalization layer of the transformer (as shown in Fig. ([1](#S2.F1
    "Figure 1 ‣ 2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"))). The intuition behind this paradigm is that agents can acquire
    a diverse set of behavioral priors through large-scale task-agnostic pretraining,
    which then enables them to be efficiently fine-tuned for specific tasks. (3) As
    introduced in Section [6.1](#S6.SS1 "6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), the target of multi-task
    learning is a generalist agent, i.e., one model with a single set of weights that
    can be directly applied to a wide range of tasks. Different from Gato, which is
    directly trained on batches of prompt-conditioned demonstrations, DualMind (Wei
    et al. ([2023](#bib.bib349))) introduces a dual-phase method. Specifically, in
    Phase I, the transformer policy $\pi(a_{t}|s_{t-k},a_{t-k},\cdots,s_{t-1},a_{t-1},s_{t})$
    is trained with self-supervised learning objectives to capture generic information
    of state-action transitions, where the self-supervision involves predicting the
    next state, predicting the current action, and reconstructing masked actions based
    on the other elements. In Phase II, a prompt $p$, which can be either language
    or image tokens, is added as a condition to the policy, i.e., $\pi(a_{t}|p,s_{t-k},a_{t-k},\cdots,s_{t-1},a_{t-1},s_{t})$.
    To achieve this, extra Cross Attention layers are incorporated into the transformer
    decoder, enabling state/action tokens to attend to the prompt tokens. A small
    fraction of the transformer, including the Cross Attention layers, would then
    be trained via the prompt-conditioned IL, which is similar with TDT, using expert
    trajectories with associated prompts. This method emulates how humans learn to
    act in the world, i.e., acquiring skills in a task-agnostic manner and then learning
    specific tasks based on the acquired knowledge, and achieves superior performance
    in a series of robotic manipulation and navigation tasks. Abramson et al. ([2021](#bib.bib1))
    propose MIA, a multi-modal interactive agent that can naturally interact and communicate
    with humans. The agent can be simply trained by imitation learning of human-human
    interactions and self-supervised learning through an auxiliary task. Specifically,
    a multi-modal transformer is used to extract representations from the vision and
    language input tokens (i.e., $s^{V}_{t},s^{L}_{t}$) and is trained through the
    following objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi,f,D}\mathbb{E}_{D_{E}}\left[\sum_{t=0}^{T}\log\pi(a_{t}&#124;f(s^{V}_{t},s^{L}_{t}),\cdots,f(s^{V}_{0},s^{L}_{0}))+\lambda\sum_{t=0}^{T}\left[\log
    D(f(s^{V}_{t},s^{L}_{t}))+\log(1-D(f(s^{V}_{t},\tilde{s}^{L}_{t})))\right]\right]$
    |  | (87) |'
  prefs: []
  type: TYPE_TB
- en: Here, $f$ denotes the transformer to extract representations, $\pi$ is the overall
    policy which comprises multiple components besides $f$, $D$ is a discriminator
    (as in GANs) which is introduced to encourage $f$ to generate distinct representations
    for matched vision and text data (i.e., $(s^{V}_{t},s^{L}_{t})$) and unmatched
    ones (i.e., $(s^{V}_{t},\tilde{s}^{L}_{t})$). This modality matching auxiliary
    task, also used in (Mees et al. ([2022a](#bib.bib218))), has shown to significantly
    improve the agent’s performance beyond imitation learning alone.
  prefs: []
  type: TYPE_NORMAL
- en: 'We summarize the representative algorithms discussed in this section in Table
    [9](#S6.T9 "Table 9 ‣ 6.2.3 Developing Generalist Imitation Learning Agents with
    Transformers ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), highlighting their key novelties and evaluation tasks.
    As mentioned in the beginning of this section, transformer-based IL algorithms
    fundamentally rely on the straightforward BC framework. However, these algorithms
    have demonstrated promising outcomes in complex robotic manipulation tasks, both
    simulated and real-world. This leads to an inspiring paradigm for robust robotic
    learning, that is, using a potent foundation model trained by imitation learning
    from extensive, high-quality demonstrations. Future research directions may include
    developing methods to synthesize high-quality training data at lower costs or
    enhancing/replacing the foundational algorithm BC with more advanced alternatives.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Diffusion Models in Offline Policy Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present applications of Diffusion Models (DM) in IL (i.e.,
    Section [7.1](#S7.SS1 "7.1 Imitation Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) and offline RL (i.e., Section
    [7.2](#S7.SS2 "7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")). In particular, there is an emerging
    body of researches on DM-based offline RL algorithms, while more extensions/explorations
    could be made for DM-based IL.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Imitation Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most works regarding applying Diffusion Models in IL are based on the Behavioral
    Cloning (BC) framework (Pomerleau ([1991](#bib.bib256))). To be specific, the
    objective of BC is to train a policy $\pi_{\theta}$ with which the likelihood
    of the expert demonstrations can be maximized:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\theta}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log\pi_{\theta}(a&#124;s)\right]$
    |  | (88) |'
  prefs: []
  type: TYPE_TB
- en: 'As mentioned in Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), diffusion models have
    shown superior performance in modeling and generating data following a certain
    distribution, i.e., $P_{X}(x)$. Thus, many recent works have proposed to improve
    BC by implementing the policy network as a diffusion model to model the conditional
    distribution $P_{A|S}(a|s)$ within the expert data $D_{E}$. Further, as shown
    in Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"), the objective for training DM (i.e.,
    Eq. ([27](#S2.E27 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"))) provides an upper bound for the negative
    log-likelihood, which naturally connects DM with BC. Next, we introduce these
    works in details, which are designed to solve drawbacks of the original BC algorithm
    from multiple perspectives.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.1 Improving Policy Expressiveness in Imitation Learning with Diffusion Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The first group of works (Pearce et al. ([2023](#bib.bib246)); Chi et al. ([2023](#bib.bib54));
    Reuss et al. ([2023](#bib.bib274))) try to improve the expressiveness of the learned
    policy. Through minimizing the Mean Square Error (MSE) shown as Eq. ([88](#S7.E88
    "In 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")), the BC policy $\pi_{\theta}$ is trained to give out point
    estimates ^(18)^(18)18Each state-action pair $(s,a)$ is a data point., which precludes
    it from capturing the multi-modality of the state-action pairs in $D_{E}$. Moreover,
    $\pi_{\theta}$ is encouraged to learn an average distribution, as the objective
    is to maximize an expectation, which would result in bias towards more frequently
    occurring actions. Additionally, $\pi_{\theta}(a|s)$ is usually implemented as
    a diagonal Gaussian policy, thus the prediction of each action dimension is independent,
    potentially leading to uncoordinated behaviours in high-dimensional action spaces.
    On the other hand, DM has shown great potential in generating high-dimensional
    samples that adhere to complex, multi-modal distributions, while ensuring the
    precision and diversity. In this case, Diffusion BC (Pearce et al. ([2023](#bib.bib246)))
    is proposed to utilize the denoising (score) function $\epsilon_{\theta}$ in DDPM
    (Ho et al. ([2020](#bib.bib136))) to generate actions $a$ at given states $s$.
    Similarly with Eq. ([28](#S2.E28 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), the IL objective is
    as follows: (Please refer to Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") for notation definitions.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta}\mathbb{E}_{t\sim{\mathcal{U}}[1,T],(s,a)\sim D_{E},\epsilon\sim{\mathcal{N}}(0,I)}\left[&#124;&#124;\epsilon-\epsilon_{\theta}(a_{t}=\sqrt{\alpha_{t}}a+\sqrt{1-\alpha_{t}}\epsilon,t;s)&#124;&#124;^{2}\right]$
    |  | (89) |'
  prefs: []
  type: TYPE_TB
- en: 'Note that $s$ is the conditioner which is not interrupted in the diffusion
    process. With the learned $\epsilon_{\theta}$, the estimated expert action $a=a_{0}$
    can be generated following the denoising process in DDPM:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $a_{t-1}=\frac{1}{\sqrt{1-\beta_{t}}}(a_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(a_{t},t;s))+\sqrt{\beta_{t}}\epsilon,\
    t=T,\cdots,1,\ a_{T}\sim{\mathcal{N}}(0,I)$ |  | (90) |'
  prefs: []
  type: TYPE_TB
- en: 'Further, Diffusion Policy (Chi et al. ([2023](#bib.bib54))) is proposed to
    utilize DDPM for closed-loop action-sequence prediction. In particular, it models
    the distribution $P_{\vec{A}|\vec{S}}(\vec{a}|\vec{s})$ in $D_{E}$, where $\vec{s}$
    and $\vec{a}$ denote the previous $T_{s}$ states and future $T_{p}$ actions, respectively.
    $T_{a}$ out of the $T_{p}$ actions are executed without replanning. This framework
    allows long-horizon planning, encourages temporal consistency of the action sequence,
    and remains responsive to the changing environment through receding horizon planning.
    In order to learn the joint distribution of the $T_{p}$ actions conditioned on
    the historical states – a task that is inherently high-dimensional – Chi et al.
    ([2023](#bib.bib54)) suggest modeling it as $\epsilon_{\theta}(\vec{a}_{t},t|\vec{s}_{t})$,
    which is trained and sampled with a DDPM framework as delineated in Eq. ([89](#S7.E89
    "In 7.1.1 Improving Policy Expressiveness in Imitation Learning with Diffusion
    Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) and ([90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness
    in Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Last, Reuss et al. ([2023](#bib.bib274)) propose BESO for goal-conditioned
    IL. Specifically, one or more future states within the same trajectory as $(s,a)$
    are used as the goal $g$. The objective is now to get a goal-conditioned policy
    $\pi_{\theta}(a|s,g)$, which is learned as a conditional score function $S_{\theta}(a_{t},t;s,g)$
    in a Score SDE framework (Karras et al. ([2022](#bib.bib158))). With $S_{\theta}(a_{t},t;s,g)$,
    the action at $s$ targeting $g$ can be generated with a Probability Flow ODE,
    as introduced in Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"). Goal-conditioned policies distill
    useful, goal-oriented behaviors, which can be integrated with a high-level planner
    for various downstream tasks, as in Hierarchical RL. However, incorporating the
    goal conditioner amplifies the multimodal nature of the demonstrations, since
    the same goal might be achieved via various distinct trajectories, underscoring
    the necessity of employing DM. It’s worthy noting that all three works suggest
    using transformers as the denoising function (i.e., $\epsilon_{\theta},S_{\theta}$)
    for sample quality, rather than the commonly-used U-Nets (Ronneberger et al. ([2015](#bib.bib279)))
    for DM.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.2 Addressing Common Issues of Imitation Learning with Diffusion Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Beyond enhancing policy expressiveness, numerous studies have explored the
    use of DM to address other challenges in IL, including spurious correlations (Saxena
    et al. ([2023](#bib.bib284))) and noisy (suboptimal) demonstrations (Wang et al.
    ([2023e](#bib.bib343)); Yuan et al. ([2023b](#bib.bib383))). To be specific, due
    to spurious correlations, expressive models, such as DM, may focus on distractors
    that are irrelevant to action prediction and thus fragile in real-world deployment,
    which resembles the causal misidentification issue introduced in Section [3.2.5](#S3.SS2.SSS5
    "3.2.5 Tackling Causal Confusion in Imitation Learning with VAEs ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"). Thus, Saxena et al. ([2023](#bib.bib284)) propose C3DM to improve
    the denoising process of Diffusion BC as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s_{t}=C(s;\text{pos}(a_{t}),t),\ a_{t-1}=\frac{1}{\sqrt{1-\beta_{t}}}(a_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(a_{t},t;s_{t}))+\sqrt{\beta_{t}}\epsilon,\
    t=T,\cdots,1,\ a_{T}\sim{\mathcal{N}}(0,I)$ |  | (91) |'
  prefs: []
  type: TYPE_TB
- en: 'Compared with Eq. ([90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness in
    Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")), the only
    difference is to replace the conditioner $s$ with $s_{t}$ which is updated with
    the denoising process. In particular, at each denosing iteration $t$, the image
    state $s$ is zoomed into the region around the intermediate action, i.e., $\text{pos}(a_{t})$,
    to acquire more details for decision-making while ignoring distractors in other
    regions. Note that this work assume the access to a transformation between the
    action and state spaces to determine $\text{pos}(a_{t})$. Accordingly, they modify
    the training process in Eq. ([89](#S7.E89 "In 7.1.1 Improving Policy Expressiveness
    in Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) by replacing
    $s$ with $C(s;\text{pos}(a),t)$. Further, considering that expert demonstrations
    in real-world scenarios are often noisy, Wang et al. ([2023e](#bib.bib343)) propose
    DP-IL to adopt DM for purifying/denoising the demonstrations. Sequentially, any
    IL algorithm can be applied on these purified data. To be specific, based on the
    DDPM framework, they view $(s,a)$ as the data point $x$ (in Eq. ([28](#S2.E28
    "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on Deep Generative Models ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))) and learn a denoising function $\epsilon_{\theta}(x_{t},t)$
    to recover the original state-action demonstrations (i.e., $x_{0}$) from samples
    interrupted with Gaussian noise (i.e., $x_{t}$). Note that this training process
    is based on a set of “clean” demonstartions. Then, for imperfect data points $\tilde{x}$,
    the learned DM can be used to purify them by first adding random noise to $\tilde{x}$
    to get $\tilde{x}_{t}$ (i.e., via the forward process) and then using $\epsilon_{\theta}$
    in the reverse denoising process to get the purified data $\tilde{x}_{0}$. Theoretically,
    they prove that $\tilde{x}_{t}$ and $x_{t}$ can be arbitrarily closed in distribution
    as $t$ increases, and so $\epsilon_{\theta}$ trained on $x_{t}$ can be used to
    convert $\tilde{x}_{t}$ into purified demonstrations as well. Different from the
    two-stage framework in DP-IL, SMILE (Yuan et al. ([2023b](#bib.bib383))) is proposed
    to jointly perform the automatic filtering of noisy demonstrations and learning
    of the expert policy. It is based on Diffusion BC (i.e., Eq. ([89](#S7.E89 "In
    7.1.1 Improving Policy Expressiveness in Imitation Learning with Diffusion Models
    ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) and ([90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness
    in Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"))), but changes
    the forward diffusion process of DDPM as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $a_{t}\sim\pi^{t}(\cdot&#124;s),\ \pi^{t}(a_{t}&#124;s)=\int\pi^{0}(a_{0}&#124;s){\mathcal{N}}(a_{t}&#124;a_{0},\sigma_{t}^{2}I)\
    da_{0},\ \sigma_{t}^{2}=\sum_{k=1}^{t}\beta_{k}^{2}$ |  | (92) |'
  prefs: []
  type: TYPE_TB
- en: Here, $\pi^{0}$ denotes the underlying policy of the provided demonstrations.
    Through this equation, the diffusion process of SMILE is conducted on a policy
    level rather than on data points, and so the learned denoising function $\epsilon_{\theta}$
    can be used to compare the suboptimality among behavior policies underlying different
    trajectories (i.e., with Eq. (13) in (Yuan et al. ([2023b](#bib.bib383)))). Trajectories
    generated using policies noisier than the currently learned one are then filtered
    out from the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Diffusion BC | Diffusion Policy | BESO | C3DM | DP-IL | SMILE
    |'
  prefs: []
  type: TYPE_TB
- en: '| DM Type | DDPM | DDPM | Score SDE | DDPM | DDPM | DDPM |'
  prefs: []
  type: TYPE_TB
- en: '| DM Usage | Policy Generator | Policy Generator | Policy Generator | Policy
    Generator | Data Denoiser | Policy Generator |'
  prefs: []
  type: TYPE_TB
- en: '| IL Framework | BC | BC | BC | BC | Not Limited | BC |'
  prefs: []
  type: TYPE_TB
- en: '| Targeted IL Issues | Multimodal distribution | Multimodal distribution &
    HD data | Multimodal distribution | Spurious correlation | Noisy data | Noisy
    data |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation Task | Kitchen, CSGO | Kitchen, Push-T, Block-Push, Robomimic
    | Kitchen, Block-Push, CALVIN | Autodesk, Real Robot | MuJoCo | MuJoCo |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Summary of DM-based IL algorithms. Specifically, “HD” is short for
    “high-dimensional”. Regarding the evaluation tasks, Kitchen (Gupta et al., [2019b](#bib.bib115)),
    Block-Push & Push-T (Florence et al., [2022](#bib.bib92)), Robomimic (Mandlekar
    et al., [2021b](#bib.bib210)), CALVIN (Mees et al., [2022b](#bib.bib219)) are
    robotic manipulation tasks in different scenarios; CSGO (Pearce & Zhu, [2022](#bib.bib245))
    is a 3D First-person Shooter video game; Autodesk (Koga et al., [2022](#bib.bib170))
    and Real Robot (Saxena et al., [2023](#bib.bib284)) represent robotic manipulation
    tasks built on the Autodesk simulator and real robot platforms, respectively;
    MuJoCo (Todorov et al. ([2012](#bib.bib320))) provides a series of robotic locomotion
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: While not exclusively designed for DM-based IL, the studies by Shi et al. ([2023](#bib.bib294))
    and Sridhar et al. ([2023](#bib.bib310)) introduce techniques — namely, AWE and
    MCNN — that further enhance the performance of Diffusion Policy and Diffusion
    BC, respectively. Both techniques focus on mitigating the compounding errors of
    IL, i.e., prediction errors compounded over the decision horizon. AWE can be applied
    to automatically extract waypoints within an expert trajectory. The waypoint sequences
    are notably shorter in horizon and can then be used for waypoint-only imitation.
    On the other hand, MCNN doesn’t modify the training data but enhances the policy
    for reduced errors. During evaluation, it identifies the nearest neighbor state
    of the current state in the expert dataset, and then blends the corresponding
    expert action with the output of the learned policy network to formulate an error-constrained
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [10](#S7.T10 "Table 10 ‣ 7.1.2 Addressing Common Issues of Imitation
    Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), we present a summary
    of DM-based IL methods. This includes their base IL algorithms, the specific IL
    challenges they aim to address, and how DM is used in resolving these issues.
    There are several promising future research directions in this field. Firstly,
    while most current works rely on DDPM for its robust performance, future studies
    could explore more advanced DM, particularly those with efficient sampling schemes
    which are crucial for sequential decision-making ^(19)^(19)19At each time step,
    to sample $a\sim\pi_{\theta}(\cdot|s)$, the entire denoising process (with $T$
    iterations) shown as Eq. ([90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness
    in Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) needs to
    be executed, thus the sampling efficiency of DM is essential.. Secondly, given
    that these works target various IL issues, the development of a unified (DM-based)
    algorithm capable of simultaneously resolving multiple issues is also a promising
    direction. Finally, integrating DM with more advanced IL frameworks, as listed
    in Table [6](#S5.T6 "Table 6 ‣ 5.1.2 Policy Modeling in Imitation Learning with
    Normalizing Flows ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"), could offer solutions to fundamental
    issues in BC.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Offline Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recently, there is an emerging body of advancements in applying Diffusion Models
    to offline RL (Zhu et al. ([2023c](#bib.bib398))). In particular, Diffusion Models
    can be adopted as the policy, planner, or data synthesizer in the context of offline
    RL, as introduced in Section [7.2.1](#S7.SS2.SSS1 "7.2.1 Diffusion Models as Policies
    ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") - [7.2.3](#S7.SS2.SSS3 "7.2.3 Diffusion Models as Data
    Synthesizers ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"). Additionally, we present the
    applications of DM for extended offline RL setups in Section [7.2.4](#S7.SS2.SSS4
    "7.2.4 Diffusion Models in Extended Offline Reinforcement Learning Setups ‣ 7.2
    Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"). For each category, we introduce the seminal works in details
    as a tutorial on its paradigm, followed by a brief review of the extensions with
    a focus on their key novelties.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 Diffusion Models as Policies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As introduced in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), in
    order to alleviate the value overestimation issue caused by OOD actions, four
    offline RL schemes are developed. Among them, DM-based offline RL usually follows
    the policy constraint method, that is, constraining the learned policy $\pi_{\theta}$
    to the behavior policy $\mu$ while maximizing the Q-function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\theta}\mathbb{E}_{s\sim D_{\mu}}\left[\mathbb{E}_{a\sim\pi_{\theta(\cdot&#124;s)}}Q_{\phi}(s,a)-\lambda
    D_{KL}(\pi_{\theta}(\cdot&#124;s)&#124;&#124;\mu(\cdot&#124;s))\right]$ |  | (93)
    |'
  prefs: []
  type: TYPE_TB
- en: 'where $Q_{\phi}$ is a learned Q-function for the current policy $\pi_{\theta}$,
    and $\lambda>0$ is the Lagrangian multiplier ^(20)^(20)20As a common practice,
    $\lambda$ can be fine-tuned as a hyperparameter, controlling the tradeoff between
    the two objective terms.. Such an optimization problem has a closed-form solution,
    i.e., $\pi^{*}(a|s)=\frac{1}{Z(s)}\mu(a|s)\exp(\frac{1}{\lambda}Q_{\phi}(s,a))$,
    where $Z(s)$ is the partition function for normalization. Suppose that $\pi^{*}$
    can be represented as a parametric function, then the optimal policy can be learned
    through weighted regression (Wang et al. ([2020](#bib.bib346))) as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\theta}\mathbb{E}_{(s,a)\sim D_{\mu}}\left[\frac{1}{Z(s)}\log\pi_{\theta}(a&#124;s)\exp(\frac{1}{\lambda}Q_{\phi}(s,a))\right]$
    |  | (94) |'
  prefs: []
  type: TYPE_TB
- en: Therefore, the parametric policy $\pi_{\theta}$ should be expressive enough
    to recover the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, DM can be employed to model the policy $\pi_{\theta}$. (1) SfBC
    (Chen et al. ([2023b](#bib.bib43))) provides a straightforward manner to realize
    this. It first imitates the behavior policy $\mu(a|s)$ with a DM $\pi_{\theta}(a|s)$
    (specifically, Score SDE) in an IL scheme as introduced in Section [7.1.1](#S7.SS1.SSS1
    "7.1.1 Improving Policy Expressiveness in Imitation Learning with Diffusion Models
    ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"). Then, for any state $s$, $N$ actions are sampled with
    $\pi_{\theta}(\cdot|s)$ as candidates, and one action is resampled from these
    candidates with $\exp(\frac{1}{\lambda}Q_{\phi}(s,a))$ being the sampling weights.
    In this way, $a\sim\pi^{*}(\cdot|s)$ is approximated via importance sampling.
    Note that $Q_{\phi}(s,a)$ can be learned with any offline RL protocol. IDQL (Hansen-Estruch
    et al. ([2023](#bib.bib122))) adopts the same resampling scheme but imitates $\mu(a|s)$
    with DDPM. Additionally, rather than using $\exp(\frac{1}{\lambda}Q_{\phi}(s,a))$,
    they design a sampling weight based on the Implicit Q-Learning framework (Kostrikov
    et al. ([2022](#bib.bib172))), which is a SOTA offline RL algorithm. (2) Alternatively,
    the Q-function $Q_{\phi}(s,a)$ can be directly involved in training the DM policy
    $\pi_{\theta}(a|s)$. In particular, Diffusion-QL (Wang et al. ([2023f](#bib.bib344)))
    learns a DDPM-based policy through the following objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta}\mathcal{L}_{DM}(\theta)-\frac{\eta}{\mathbb{E}_{(s,a)\sim
    D_{\mu}}\left[&#124;Q_{\phi}(s,a)&#124;\right]}\mathbb{E}_{s\sim D_{\mu},a_{0}\sim\pi_{\theta}(\cdot&#124;s)}\left[Q_{\phi}(s,a_{0})\right]$
    |  | (95) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $\mathcal{L}_{DM}(\theta)$ is defined as Eq. ([89](#S7.E89 "In 7.1.1
    Improving Policy Expressiveness in Imitation Learning with Diffusion Models ‣
    7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) (i.e., an IL objective) and can be replaced by the corresponding
    training objective if a different DM is employed, $\pi_{\theta}$ is implied by
    the denoising function $\epsilon_{\theta}$, $a_{0}$ is the action sample after
    being denoised for $T$ iterations (with Eq. ([90](#S7.E90 "In 7.1.1 Improving
    Policy Expressiveness in Imitation Learning with Diffusion Models ‣ 7.1 Imitation
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))),
    $\mathbb{E}_{(s,a)\sim D_{\mu}}\left[|Q_{\phi}(s,a)|\right]$ is incorporated for
    adaption to Q-functions with different scales. Intuitively, the Q-function acts
    as guidance in the reverse generation process of the DM. Also, Eq. ([95](#S7.E95
    "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement Learning ‣
    7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) mirrors
    Eq. ([93](#S7.E93 "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    since both equations encourage the learned policy to maximize Q-values while being
    closed to the behavior policy, and it’s noteworthy that DiffCPS (He et al. ([2023b](#bib.bib127)))
    provides a theoretical connection between Eq. ([95](#S7.E95 "In 7.2.1 Diffusion
    Models as Policies ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) and Eq. ([93](#S7.E93
    "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement Learning ‣
    7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following Diffusion-QL, several algorithms adopting a similar objective design
    with Eq. ([95](#S7.E95 "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))
    have been proposed, including SRDP (Ada et al. ([2023](#bib.bib2))), EDP (Kang
    et al. ([2023](#bib.bib157))), and Consistency-AC (Ding & Jin ([2023](#bib.bib71))).
    To be specific, SRDP additionally introduces a state-reconstruction loss term
    in Eq. ([95](#S7.E95 "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))
    to enable generalization for OOD states through self-supervised learning (Liu
    et al. ([2023a](#bib.bib196))). EDP improves the sampling efficiency of Diffusion-QL
    by modifying the sampling process $a_{0}\sim\pi_{\theta}(\cdot|s)$ in Eq. ([93](#S7.E93
    "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement Learning ‣
    7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")). Instead
    of iterative sampling with Eq. ([90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness
    in Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")), it first
    samples $a_{t}$ via the forward process and then approximates $a_{0}$ as $\frac{1}{\sqrt{\alpha_{t}}}a_{t}-\frac{\sqrt{1-\alpha_{t}}}{\sqrt{\alpha_{t}}}\epsilon_{\theta}(a_{t},t;s)$,
    since $a_{t}=\sqrt{\alpha_{t}}a_{0}+\sqrt{1-\alpha_{t}}\epsilon$ and $\epsilon_{\theta}$
    is trained to estimate $\epsilon$. Similarly, as a new variant of DM, Consistency
    Models (Song et al. ([2023](#bib.bib308))) aims to learn a consistency function
    $f_{\theta}$ that can map the noisy sample at any iteration $t$ to its original
    sample in just one iteration. Consistency-AC thus replaces DDPM in Diffusion-QL
    with a Consistency Model to improve the sampling efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At last, there are two works: QGPO (Lu et al. ([2023](#bib.bib202))) and CPQL
    (Chen et al. ([2023e](#bib.bib51))) aiming to directly learn the score function
    of the optimal policy: (This is derived from the definition of $\pi^{*}$, where
    $\alpha=1/\lambda$.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\nabla_{a_{t}}\log\pi^{*}(a_{t}&#124;s)\propto\nabla_{a_{t}}\log\mu(a_{t}&#124;s)+\nabla_{a_{t}}\alpha
    Q_{\phi}(s,a_{t})$ |  | (96) |'
  prefs: []
  type: TYPE_TB
- en: 'As mentioned in Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), with the score function
    $\nabla_{a_{t}}\log\pi^{*}(a_{t}|s)$, samples $a_{t}\sim\pi^{*}(\cdot|s)$ can
    be generated via various efficient score-based sampling schemes ^(21)^(21)21Note
    that $a_{t}$ represents the sample at the $t$-th diffusion iteration rather than
    the action at time step $t$.. Here, the score function $\nabla_{a_{t}}\log\mu(a_{t}|s)$
    can be learned with Score-based DM from $D_{\mu}$, as introduced in Section [2.5](#S2.SS5
    "2.5 Diffusion Models ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), but $\nabla_{a_{t}}\alpha Q_{\phi}(s,a_{t})$ relies on the Q-functions
    over intermediate samples $a_{t}$, which is intractable. As potential solutions,
    QGPO propose a contrastive learning framework to estimate $\alpha Q_{\phi}(s,a_{t}),\
    \forall t$. While, CPQL employs Consistency Models as the policy, reframes the
    score function estimation in Eq. ([96](#S7.E96 "In 7.2.1 Diffusion Models as Policies
    ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) into an objective similar to Eq. ([95](#S7.E95 "In 7.2.1
    Diffusion Models as Policies ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")), and theoretically
    establishes their equivalence when the weights for the two terms in Eq. ([95](#S7.E95
    "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement Learning ‣
    7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) are
    properly assigned.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [11](#S7.T11 "Table 11 ‣ 7.2.3 Diffusion Models as Data Synthesizers
    ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), we provide a summary of these algorithms. All the algorithms
    in this category have been evaluated on the D4RL benchmark. Readers can refer
    to corresponding papers for the numeric evaluation results to compare their performance.
    In particular, we notice that three schemes of using diffusion models as policies
    for offline RL have been developed: SfBC & IDQL, QGPO & CPQL, and the other works
    that follow Diffusion-QL. Among them, QGPO & CPQL choose to directly learn the
    score function for the policy, which is a quite challenging but promising direction
    for future works.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2 Diffusion Models as Planners
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For model-based offline RL, a dynamic function $\mathcal{T}_{\psi}(s^{k+1}|s^{k},a^{k})$
    needs to be approximated from the dataset first, and then planning over the optimal
    action sequence $(a^{1}_{*},\cdots,a^{K}_{*})$ can be done by maximizing the return
    while avoiding OOD actions: (For clarity, we use superscript $k$ to indicate the
    time step and subscript $t$ to denote the iteration of the diffusion process.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{a^{1:K}}\sum_{k=1}^{K}r(s^{k},a^{k})-\lambda d_{\mu}(s^{k},a^{k})\
    s.t.\ s^{k+1}=\mathcal{T}_{\psi^{*}}(s^{k},a^{k})$ |  | (97) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $\psi^{*}=\arg\min_{\psi}\mathbb{E}_{D_{\mu}}\left[||s^{k+1}-\mathcal{T}_{\psi}(s^{k},a^{k})||^{2}\right]$,
    $d_{\mu}(s^{k},a^{k})$ represents the uncertainty assessing whether the data point
    $(s^{k},a^{k})$ conforms to the data distribution in $D_{\mu}$. SGP (Suh et al.
    ([2023](#bib.bib312))) proposes to implement $d_{\mu}(s,a)$ as the negative log-likelihood
    of $(s,a)$ under a perturbed distribution, i.e., $-\log P_{\sigma}((s,a);D_{\mu})$
    where $P_{\sigma}(x;D_{\mu})=\frac{1}{|D_{\mu}|}\sum_{x_{i}\in D_{\mu}}\mathcal{N}(x;x_{i},\sigma^{2}I)$.
    Intuitively, this likelihood measures the distance of $(s,a)$ to the dataset $D_{\mu}$
    and a large distance indicates a high uncertainty of the point $(s,a)$ as it may
    be OOD. Then, to optimize Eq. ([97](#S7.E97 "In 7.2.2 Diffusion Models as Planners
    ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) for trajectory planning, the score function $\nabla_{a}\log
    P_{\sigma}((s,a);D_{\mu})$ needs to be estimated. As detailed in (Suh et al. ([2023](#bib.bib312))),
    this estimation can be obtained using a denoising function in DM (specifically,
    SGM).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A more widely-adopted manner for planning with DM, which is firstly proposed
    in Diffuser (Janner et al. ([2022](#bib.bib150))), is to fold the two processes
    mentioned above: transition dynamic modeling and trajectory optimization regarding
    $a^{1:K}$, as a trajectory modeling process using DM. Viewing trajectories $\tau=((s^{1},a^{1}),\cdots,(s^{K},a^{K}))$
    as data points (i.e., $x$ in Eq. ([28](#S2.E28 "In 2nd item ‣ 2.5 Diffusion Models
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))),
    DDPM is used to model the distribution of $\tau$ in $D_{\mu}$, through Eq. ([28](#S2.E28
    "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on Deep Generative Models ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) where a denoising function for trajectory generation $\epsilon_{\theta}(\tau_{t},t)$
    is learned. Then, the planning can be done by sampling trajectories starting from
    the current state using the learned DM. The sampling process is similar with Eq.
    ([90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness in Imitation Learning
    with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")). However, for optimality, this
    process is guided by a (separately-trained) return function of the trajectory
    samples, i.e., $J_{\phi}(\hat{\tau})$ ^(22)^(22)22$J_{\phi}(\hat{\tau})$ is trained
    to estimate the return of the original trajectory, i.e., $J(\tau)=\sum_{k=1}^{K}r(s^{k},a^{k})$,
    where $\hat{\tau}$ can be the trajectory sample at any diffusion iteration, i.e.,
    $\tau_{t}$.: ($\beta_{t}$, $\alpha_{t}$, and $\Sigma_{t}$ are hyperparameters
    and defined in Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background on Deep
    Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions").)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\tau_{t-1}\sim G_{\theta}(\cdot&#124;\tau_{t},g)={\mathcal{N}}(\mu_{t}+\Sigma_{t}g,\Sigma_{t}),\
    \mu_{t}=\frac{1}{\sqrt{1-\beta_{t}}}(\tau_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(\tau_{t},t)),\
    g=\nabla_{\hat{\tau}}J_{\phi}(\hat{\tau})&#124;_{\hat{\tau}=\mu_{t}}$ |  | (98)
    |'
  prefs: []
  type: TYPE_TB
- en: 'This process is also known as classifier-guided sampling (CG (Dhariwal & Nichol
    ([2021](#bib.bib70)))), which is widely adopted for conditional generations with
    diffusion models. Intuitively, the generation is guided by the gradient $\nabla_{\tau}J(\tau)$
    along which the expected return $J(\tau)$ would be maximized, aligning it with
    RL. In (Janner et al. ([2022](#bib.bib150))), the authors connect this guided
    sampling design with the control-as-inference framework of RL (Levine ([2018](#bib.bib180)))
    and claim that trajectories from such a generation process follows the distribution
    $P(\tau|\mathcal{O}_{1:K}=1)$, where $\mathcal{O}_{k}=1$ indicates the optimality
    of the time step $k$. During evaluation, the learned DM can applied as follows:
    ($\mathcal{T}$ is the real dynamic.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $[\tau_{t}(s^{1})\leftarrow s^{k},\ \tau_{t-1}\sim G_{\theta}(\cdot&#124;\tau_{t},g)]_{t=T}^{1},\
    a^{k}\leftarrow\tau_{0}(a^{1}),\ s^{k+1}\sim\mathcal{T}(\cdot&#124;s^{k},a^{k}),\
    k=1,\cdots,K$ |  | (99) |'
  prefs: []
  type: TYPE_TB
- en: 'To determine $a^{k}$, trajectory samples at each iteration are forced to start
    with the current state, i.e., $\tau_{t}(s^{1})\leftarrow s^{k}$ ($t=1,\cdots,T$).
    This follows the idea for solving inpainting problems (Sohl-Dickstein et al. ([2015](#bib.bib300))),
    where the generation for the unobserved part is in a manner consistent with the
    observed constraints. Moreover, only the first action in the generated plan, i.e.,
    $\tau_{0}(a^{1})$, is executed without replanning, which aligns with the receding
    horizon control (Mayne & Michalska ([1988](#bib.bib216))). Decision Diffuser (Ajay
    et al. ([2023](#bib.bib5))) adopts similar designs with two key modifications.
    First, instead of training a return function for classifier-guided sampling, they
    adopt classifier-free guided sampling (CFG (Ho & Salimans ([2022](#bib.bib134)))),
    for which a conditional and an unconditional denoising function, i.e., $\epsilon_{\theta}(\tau_{t},t;J(\tau))$
    and $\epsilon_{\theta}(\tau_{t},t;\emptyset)$, are jointly trained (with Eq. ([28](#S2.E28
    "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on Deep Generative Models ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))) by randomly dropping out the condition $J(\tau)$. The
    trajectory generation process is the same as Eq. ([90](#S7.E90 "In 7.1.1 Improving
    Policy Expressiveness in Imitation Learning with Diffusion Models ‣ 7.1 Imitation
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    but replaces the standard denoising function $\epsilon_{\theta}(\tau_{t},t)$ with
    $\omega\epsilon_{\theta}(\tau_{t},t;J(\tau))+(1-\omega)\epsilon_{\theta}(\tau_{t},t;\emptyset)$.
    Increasing the value of $\omega$ would decrease the diversity of samples but aligns
    the trajectory distribution more closely with $P(\tau|\mathcal{O}_{1:K}=1)$. Notably,
    both CG and CFG can be utilized for generating samples that satisfy specific conditions
    $y$. In the context of RL, $y$ can be the desired return $J(\tau)$, constraints
    to obey, task-specific information (for multi-task RL), subgoals/subtasks (for
    hierarchical RL), and so on. Second, only state sequences are modelled and predicted
    with the DM, i.e, $\tau=(s^{1},\cdots,s^{K})$ and actions are predicted with a
    separate inverse dynamic model, i.e., $a^{k}=\mathcal{T}_{\text{inv}}(s^{k},s^{k+1})$.
    This is because sequences over actions tend to be more high-frequency and less
    smooth, making them much harder to predict and model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following Diffuser and Decision Diffuser, improvements have been made in multiple
    aspects such as the sampling process (SafeDiffuser, Discrete Diffuser), network
    structure (EDGI), training objective (PlanCP), and conditioners (TCD). We present
    a brief overview of these advancements in comparison to Diffuser and Decision
    Diffuser as follows. Notably, only TCP follows Decision Diffuser and the others
    follow Diffuser. SafeDiffuser (Xiao et al. ([2023](#bib.bib358))) aims to ensure
    the safe generation of data in diffusion. For each iteration $t$, the diffusion
    dynamic $u_{t}=\frac{\tau_{t-1}-\tau_{t}}{\Delta t}$ is re-optimized to satisfy
    certain safety constraints and the resulting dynamic $u^{*}_{t}$ is used to update
    $\tau_{t-1}$ as $\tau_{t}+\Delta t*u^{*}_{t}$. $\Delta t$ is the diffusion time
    interval which should be small enough. They theoretically show that $\tau_{0}$
    generated in this manner fulfills the safety constraints with probability almost
    1\. Discrete Diffuser (Coleman et al. ([2023](#bib.bib60))) proposes three diffusion-guidance
    sampling techniques for generation in discrete state and action spaces, where
    Gaussian-based DM cannot be applied. These guided sampling methods can work with
    discrete DM such as D3PM (Austin et al. ([2021a](#bib.bib15))). EDGI (Brehmer
    et al. ([2023](#bib.bib31))) proposes an equivariant network architecture for
    the scenario where an embodied agent operates $n$ objects in a 3D environment.
    This network design takes geometric structures into account and ensures equal
    likelihood of a trajectory and its counterpart for which specific spatial/temporal
    translations or permutations over objects are applied. Better generalization across
    scenarios, where such translations or permutations happen, can thus be realized.
    PlanCP (Sun et al. ([2023](#bib.bib314))) proposes to quantify the uncertainty
    of DM using Conformal Prediction (Vovk et al. ([2005](#bib.bib333))). In particular,
    $M$ trajectories are generated from DM corresponding to $M$ trajectories in $D_{\mu}$,
    each pair of which has a prediction error $e_{i}$. The conformity among $\{e_{i}\}$
    inversely reflects the uncertainty of the DM. Introducing this conformity term
    to the DDPM training objective (i.e., Eq. ([28](#S2.E28 "In 2nd item ‣ 2.5 Diffusion
    Models ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))) as
    an auxiliary term can potentially reduce the uncertainty of sampling with DM.
    Compared with Decision Diffuser, TCD (Hu et al. ([2023a](#bib.bib139))) utilizes
    more temporal information as the conditioner of the denoising function $\epsilon_{\theta}$
    to guide the sampling process. Specifically, following the insight of Decision
    Transformer (introduced in Section [6.1.1](#S6.SS1.SSS1 "6.1.1 Background on Trajectory-Optimization-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), the return-to-go and
    reward at the current time step are involved as the prospective and immediate
    conditions, respectively, to reason about the sampling progress while focusing
    more closely on the current generation step.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.3 Diffusion Models as Data Synthesizers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The performance of offline RL is limited by the provided static dataset $D_{\mu}$.
    Once trained with $D_{\mu}$, the DM can potentially generate a variety of high-quality
    trajectory data. This newly generated data can then be employed to augment $D_{\mu}$,
    thereby creating a feedback loop to further improve the DM. Moreover, in an unseen
    but related task, the DM’s inherent generality allows for generation of well-performing
    trajectories for fine-tuning the model in this novel environment. Several works
    have been developed in this direction following either Diffuser (Hwang et al.
    ([2023](#bib.bib143)); Liang et al. ([2023b](#bib.bib190))) or Decision Diffuser
    (He et al. ([2023a](#bib.bib125))). Specifically, AdaptDiffuser (Liang et al.
    ([2023b](#bib.bib190))) suggests a rule-based method for filtering and enhancing
    generated trajectories $\tau=((s^{1},a^{1}),\cdots,(s^{K},a^{K}))$ from Diffuser,
    based on the return values and dynamic consistency. Starting with $k=1$, a revised
    action $\tilde{a}^{k}$ is determined using a well-trained or defined inverse dynamic
    model $\mathcal{T}_{\text{inv}}(\tilde{s}^{k},s^{k+1})$ (with $\tilde{s}^{1}=s^{1}$).
    Subsequently, the next state is adjusted according to the (learned) environment
    dynamic function as $\tilde{s}^{k+1}=\mathcal{T}(\tilde{s}^{k},\tilde{a}^{k})$.
    Trajectories with states $s^{k}$ significantly deviating from their counterparts
    $\tilde{s}^{k}$ are filtered out. The remaining trajectories $\tilde{\tau}$ are
    further filtered based on their returns $J_{\phi}(\tilde{\tau})=\sum_{k=1}^{K}r_{\phi}(s^{k},a^{k})$.
    Finally, the chosen trajectories are of high quality and can be used for effective
    data augmentation. In a more straightforward approach, MTDiff (He et al. ([2023a](#bib.bib125)))
    models trajectories $\tau=((s^{1},a^{1},r^{1}),\cdots,(s^{K},a^{K},r^{K}))$ conditioned
    on expert demonstrations from the same environment, denoted as $y$. Then, trajectory
    samples from the learned conditional denoising function $\epsilon_{\theta}(\tau_{t},t;y)$
    are directly used for data enhancement. For unseen tasks, a small amount of demonstrations
    can be used as prompts, i.e., $y$, for $\epsilon_{\theta}(\tau_{t},t;y)$ to generate
    high-quality training data. These data can then be employed to adapt the DM policy
    to novel tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | DM Type | DM Usage | Evaluation Task |'
  prefs: []
  type: TYPE_TB
- en: '| SfBC | Score SDE | Policy | D4RL (L, M, K) |'
  prefs: []
  type: TYPE_TB
- en: '| IDQL | DDPM | Policy | D4RL (L, M) |'
  prefs: []
  type: TYPE_TB
- en: '| Diffusion-QL | DDPM | Policy | D4RL (L, M, A, K) |'
  prefs: []
  type: TYPE_TB
- en: '| SRDP | DDPM | Policy | D4RL (L, M) |'
  prefs: []
  type: TYPE_TB
- en: '| DiffCPS | DDPM | Policy | D4RL (L, M) |'
  prefs: []
  type: TYPE_TB
- en: '| EDP | DDPM | Policy | D4RL (L, M, A, K) |'
  prefs: []
  type: TYPE_TB
- en: '| Consistency-AC | Consistency Model | Policy | D4RL (L, M, A, K) |'
  prefs: []
  type: TYPE_TB
- en: '| QGPO | Score-SDE | Policy | D4RL (L, M) |'
  prefs: []
  type: TYPE_TB
- en: '| CPQL | Consistency Model | Policy | D4RL (L, A), dm_control |'
  prefs: []
  type: TYPE_TB
- en: '| DOM2 (MARL) | Score-SDE | Policy | MPE, MAMuJoCO |'
  prefs: []
  type: TYPE_TB
- en: '| LDCQ (HRL) | DDPM | Policy | D4RL (L, M, A, K, C) |'
  prefs: []
  type: TYPE_TB
- en: '| SGP | SGM | Planner | CartPole, D4RL (L), Pixel-Based Single Integrator,
    Box-Pushing |'
  prefs: []
  type: TYPE_TB
- en: '| Diffuser | DDPM | Planner | D4RL (L, M), Kuka Robot |'
  prefs: []
  type: TYPE_TB
- en: '| Decision Diffuser | DDPM | Planner | D4RL (L, K), Kuka Robot, Unitree-go-running
    |'
  prefs: []
  type: TYPE_TB
- en: '| SafeDiffuser | DDPM | Planner (D) | D4RL (L, M) |'
  prefs: []
  type: TYPE_TB
- en: '| Discrete Diffuser | D3PM | Planner (D) | D4RL (L) |'
  prefs: []
  type: TYPE_TB
- en: '| EDGI | DDPM | Planner (D) | Kuka Robot |'
  prefs: []
  type: TYPE_TB
- en: '| PlanCP | DDPM | Planner (D) | D4RL (L, M) |'
  prefs: []
  type: TYPE_TB
- en: '| TCD | DDPM | Planner (DD) | D4RL (L) |'
  prefs: []
  type: TYPE_TB
- en: '| MetaDiffuser (MTRL) | DDPM | Planner (D) | Multi-task MuJoCo |'
  prefs: []
  type: TYPE_TB
- en: '| MADiff (MARL) | DDPM | Planner (DD) | MPE, SMAC, MATP |'
  prefs: []
  type: TYPE_TB
- en: '| HDMI (HRL) | DDPM | Planner (DD) | D4RL (L, M), NeoRL |'
  prefs: []
  type: TYPE_TB
- en: '| AdaptDiffuser | DDPM | Planner & Synthesizer (D) | D4RL (L, M), Kuka Robot
    |'
  prefs: []
  type: TYPE_TB
- en: '| MTDiff (MTRL) | DDPM | Planner & Synthesizer (DD) | Meta-World-V2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Summary of DM-based offline RL algorithms. Some algorithms in this
    table can be categorized as multi-agent RL (MARL), multi-task RL (MTRL), or hierarchical
    RL (HRL). When DM are used as planners, most algorithms can be viewed as extensions
    of Diffuser (D) or Decision Diffuser (DD). Regarding the benchmarks, nearly all
    algorithms in this category have been evaluated on D4RL (Fu et al. ([2020](#bib.bib99))),
    which provides offline datasets for various data-driven RL tasks, including Locomotion
    (L), AntMaze (M), Adroit (A), Kitchen (K), and CARLA Autonomous Driving (C). dm_control
    (Tassa et al. ([2018](#bib.bib317))), Kuka Robot (Janner et al. ([2022](#bib.bib150))),
    Unitree-go-running Margolis & Agrawal ([2022](#bib.bib213)), CartPole (Tedrake
    ([2023](#bib.bib318))), Box-Pushing (Manuelli et al. ([2020](#bib.bib211))) are
    a series of continuous (robotic) control tasks. Pixel-Based Single Integrator
    (Chou & Tedrake ([2023](#bib.bib57))) requires dynamic learning and control over
    the pixel space. NeoRL (Qin et al. ([2022](#bib.bib262))) is an industrial benchmark
    on financial decision making. Lastly, Multi-task MuJoCo (Mitchell et al. ([2021](#bib.bib224)))
    and Meta-World-V2 (Yu et al. ([2019c](#bib.bib379))) are widely-used benchmarks
    for multi-task/meta RL. MPE (2D control Lowe et al. ([2017](#bib.bib199))), MAMuJoCo
    (robotic locomotion Peng et al. ([2021](#bib.bib248))), SMAC (video gaming Samvelyan
    et al. ([2019](#bib.bib283))), and MATP (trajectory prediction Alcorn & Nguyen
    ([2021](#bib.bib7))) provide diverse evaluation tasks for multi-agent RL.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.4 Diffusion Models in Extended Offline Reinforcement Learning Setups
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In addition to standard offline RL, diffusion models have been applied in multi-task,
    multi-agent, and hierarchical offline RL setups. Still, these methods are built
    on the foundational algorithms mentioned above, such as Diffusion-QL, Diffuser,
    and Decision Diffuser. MTDiff (He et al. ([2023a](#bib.bib125))) and MetaDiffuser
    (Ni et al. ([2023](#bib.bib230))) aim to learn task-conditioned planners to solve
    a distribution of tasks. They utilize offline data categorized for each task,
    i.e., $\cup_{i}D_{\mu_{i}}$, where $\mu_{i}$ denotes the behavior policy corresponding
    to task $i$. Following Decision Diffuser, MTDiff incorporates an expert trajectory
    from task $i$, denoted as $y_{i}$, as an extra condition for the planner specific
    to that task, represented as $\epsilon_{\theta}(\tau_{t},t;J(\tau),y_{i})$. Through
    training, the agent is expected to implicitly capture the transition model and
    reward function stored in the prompt trajectory, and perform task-specific planning
    based on these internalized knowledge. When being evaluated in a new but related
    task, the denoising function conditioned on a corresponding demonstration $y_{i}$
    can be used for planning. In contrast, MetaDiffuser uses an explicit approach
    by learning an encoder, denoted as $E_{\eta}$, to encode trajectories into compact
    representations through self-supervision on the reward and dynamic functions (Zintgraf
    et al. ([2020](#bib.bib400))), resembling BoReL introduced in Section [3.1.5](#S3.SS1.SSS5
    "3.1.5 Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\eta,\psi,\phi}\mathbb{E}_{\tau\sim\cup_{i}D_{\mu_{i}},y=E_{\eta}(\tau),(s^{k},a^{k},r^{k},s^{k+1})\sim\tau}\left[\log\mathcal{T}_{\psi}(s^{k+1}&#124;s^{k},a^{k},y)+\log
    r_{\phi}(r^{k}&#124;s^{k},a^{k},y)\right]$ |  | (100) |'
  prefs: []
  type: TYPE_TB
- en: 'When planning for a specific task $i$, the representation $y=E_{\eta}(\tau_{i}),\
    \tau_{i}\sim D_{\mu_{i}}$ can be used as an extra condition of the planner. MetaDiffuser
    is based on Diffuser and it adopts the learned reward and dynamic model to define
    the sampling guidance $g$ in Eq. ([98](#S7.E98 "In 7.2.2 Diffusion Models as Planners
    ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) to enhance the dynamics consistency of the generated trajectories
    while encouraging a high return, detailed as Eq. (7) in (Ni et al. ([2023](#bib.bib230))).'
  prefs: []
  type: TYPE_NORMAL
- en: 'DOM2 (Li et al. ([2023c](#bib.bib187))) and MADiff (Zhu et al. ([2023b](#bib.bib397)))
    target at fully cooperative, offline multi-agent RL. Built upon Diffusion-QL,
    DOM2 learns a Q-function and DM-based policy for each agent. This policy is tailored
    to map the individual observation of each agent to its own action, which is trained
    with Eq. ([95](#S7.E95 "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")).
    Following Decision Diffuser, MADiff is developed for joint trajectory planning
    of all agents. Each agent $i$ has a separate denoising function $\epsilon_{\theta}^{i}$
    for planning its corresponding trajectory segment. To encourage global information
    interchange for better coordination, the denoising function contains an attention
    layer to aggregate trajectory embeddings from other agents. Clearly, more extensions
    can be developed regarding DM-based MARL, such as MARL algorithms for fully competitive
    or mixed (partially cooperative/competitive) task scenarios, and integration of
    DM with SOTA CTDE MARL methods, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LDCQ (Venkatraman et al. ([2023](#bib.bib329))) and HDMI (Li et al. ([2023b](#bib.bib185)))
    are proposed to learn a hierarchical policy/planner from an offline dataset, which
    can be especially beneficial for long-horizon decision-making. LDCQ learns a high-level
    policy $\pi_{\text{high}}(z|s^{1})$ to map an initial state to a skill $z$, and
    low-level policies for each skill $\pi_{\text{low}}(a|s,z)$. In particular, they
    first employ a $\beta$-VAE: (This objective is the same as the one for OPAL introduced
    in Section [3.1.5](#S3.SS1.SSS5 "3.1.5 Offline Multi-task/Hierarchical RL based
    on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions").)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\phi,\theta}\mathbb{E}_{\tau\sim D_{\mu},z\sim P_{\phi}(\cdot&#124;\tau)}\left[\sum_{k=1}^{K}\log
    P_{\theta}(a^{k}&#124;s^{k},z)-\beta D_{KL}(P_{\phi}(z&#124;\tau)&#124;&#124;\text{prior}(z&#124;s^{1}))\right]$
    |  | (101) |'
  prefs: []
  type: TYPE_TB
- en: 'Similar with OPAL, after training, $P_{\theta}$ is used as $\pi_{\text{low}}$
    and $P_{\phi}$ is adopted to create $(s,z)$ pairs for training $\pi_{\text{high}}$.
    $\pi_{\text{high}}$ is modeled as DM and trained in a similar manner with SfBC
    (introduced in Section [7.2.1](#S7.SS2.SSS1 "7.2.1 Diffusion Models as Policies
    ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")). Alternatively, HDMI extracts the skill list corresponding
    to a trajectory as a series of subgoals within it, following a heuristic method
    (Eysenbach et al. ([2019](#bib.bib87))). Then, based on Decision Diffuser, HDMI
    learns a high-level planner to generate subgoal list $\tau_{z}$ guided by the
    trajectory return, i.e., $\epsilon_{\text{high}}(\tau_{z,t},t;J(\tau))$, and a
    low-level planner to give out the state sequence corresponding to each certain
    subgoal $z$, i.e., $\epsilon_{\text{low}}(\tau_{s,t},t;z)$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s worthy noting that fast sampling is essential for the application of DM
    for offline RL or IL. Besides adopting Consistency Models, for DDPM-based methods,
    they usually limit the number of iterations for the reverse generation process
    to a relatively small value and adopt a carefully-designed variance schedule,
    i.e., $\beta_{0:T}$, as proposed in (Xiao et al. ([2022](#bib.bib359))). For SDE-based
    methods, they would solve a probability flow ODE for the reverse generation process
    (as introduced in Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) with a fast solver: DPM-solver
    (Lu et al. ([2022](#bib.bib201))). All algorithms mentioned in this section have
    been summarized in Table [11](#S7.T11 "Table 11 ‣ 7.2.3 Diffusion Models as Data
    Synthesizers ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") and categorized based on the DM
    type and usage. When applying DM as planners, most algorithms are built upon DDPM
    and follow the design of either Diffuser (D) or Decision Diffuser (DD). Also,
    applying DM for offline RL has been an emerging research field and multiple extensions,
    including the multi-task, multi-agent, and hierarchical setups, have been explored.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Discussions and Open Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we provide in-depth discussions on deep generative models
    (DGMs) in offline policy learning and our perspectives on future research directions
    of this area, based on the main text in Section [3](#S3 "3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") - [7](#S7 "7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"). The discussions
    are centered around how DGMs have been used in offline policy learning, which
    provide a comprehensive summary of this paper and insightful ideas for future
    works. Note that, for simplicity, we use abbreviations of the DGMs: VAE - Variational
    Auto-Encoder, GAN - Generative Adversarial Network, NF - Normalizing Flow, DM
    - Diffusion Model.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Discussions on Deep Generative Models and Offline Policy Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A common usage of DGMs in offline policy learning: All DGMs can be utilized
    as policy functions in offline policy learning. In the context of IL, they can
    be the student policy learned by imitating the expert, while, in offline RL, they
    can either be the approximated behavior policy or the RL policy. To be specific,
    we list their mathematical forms as follows, in the order they were introduced
    ^(23)^(23)23$P_{\theta}(z|s)$ and $P_{\theta}(a|s,z)$ denote the prior and decoder
    of the VAE, respectively. $P_{Z}(z)$ is the assumed prior distribution of the
    latent variable $z$. $G(z|s)$ denotes the generator in the GAN or NF, and it is
    composed of a series of invertible and differentiable functions, i.e, $G_{1}\circ\cdots\circ
    G_{N}$, in the NF. $\pi(\cdot|\tau_{t-k:t})$ represents a transformer-based policy,
    where $\tau_{t-k:t}$ can be $(s_{t-k},a_{t-k},\cdots,s_{t})$ in IL and $(s_{t-k},a_{t-k},R_{t-k},\cdots,s_{t},R_{t})$
    in offline RL. $G_{\theta}(a_{t-1}|a_{t},s)$ represents the denosing process of
    DM, and the subscript $t$ denotes the denoising iteration.:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle z\sim P_{\theta}(\cdot&#124;s),\ a\sim P_{\theta}(\cdot&#124;s,z);\
    z\sim P_{Z}(\cdot),\ a=G(z&#124;s);\ z\sim P_{Z}(\cdot),\ a=G(z&#124;s),\ G=G_{1}\circ\cdots\circ
    G_{N};$ |  | (102) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\qquad\qquad\quad a_{t}\sim\pi(\cdot&#124;\tau_{t-k:t});\
    a_{T}\sim\mathcal{N}(0,I),\ a_{t-1}\sim G_{\theta}(\cdot&#124;a_{t},s),\ (t=T,\cdots,1),\
    a=a_{0}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'All these DGM-based policies show superior expressiveness compared with the
    one using only feed-forward neural networks, and each of them has unique advantages/disadvantages:
    (1) VAEs might be less expressive than the others, but offers a lightweight model
    choice and relatively stable training process; (2) As mentioned in Section [4.1.4](#S4.SS1.SSS4
    "4.1.4 Integrating VAEs and GANs for Imitation Learning: A Spotlight ‣ 4.1 Imitation
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), GAN-based polices can generate sharp data samples but
    may suffer from the mode collapse issue, meaning that they may not be able to
    cover the multiple modes in the $(s,a)$ distribution of the dataset like the other
    DGMs; (3) With its special architecture (e.g., the attention mechanism), the transformer
    can process time series or data encompassing multiple entities, as mentioned in
    Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Adopting Transformers as the Policy Backbone
    for Imitation Learning ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"); (4) As for NF and DM, they excel in modelling/generating
    high-dimensional and complex behaviors, due to their multi-component or multi-iteration
    designs shown as Eq. ([102](#S8.E102 "In 8.1 Discussions on Deep Generative Models
    and Offline Policy Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")). As a spotlight, we note that both VAE- and transformer-based policies
    have been used to make decisions on multi-modal input (e.g., information from
    different sensors or in different formats like images and languages), but they
    adopt different strategies. In particular, VAEs try to embed input from different
    modalities into a unified latent space, and then a single latent policy (coupled
    with encoders for each modality) can be used to predict actions based on multiple
    types of input, as introduced in Section [3.2.3](#S3.SS2.SSS3 "3.2.3 Managing
    Multi-Modal Inputs in Imitation Learning via VAEs ‣ 3.2 Imitation Learning ‣ 3
    Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions").
    Transformers, as a universal foundation model, do not make assumptions on the
    structure of the input data, and so can be used in various task scenarios such
    as CV and NLP, as introduced in Section [2.4](#S2.SS4 "2.4 Transformers ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"). Thus, a transformer
    can directly take a data sequence containing different modalities as input, treat
    each modality as a separate token, and integrate them via the attention mechanism,
    as shown in Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Adopting Transformers as the Policy
    Backbone for Imitation Learning ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The unique usages of each DGM for offline policy learning: We notice that each
    DGM has its own unique usage for offline policy learning. (1) VAEs can learn latent
    representations of the data samples. These representations, which are potentially
    disentangled, can be used as data transformation for learning efficiency (Section
    [3.1.4](#S3.SS1.SSS4 "3.1.4 Data Augmentation and Transformation with VAEs ‣ 3.1
    Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")), task or subtask representations for
    multi-task or hierarchical learning (Section [3.1.5](#S3.SS1.SSS5 "3.1.5 Offline
    Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement Learning
    ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    similarity measure of data samples for data augmentation (Section [3.2.2](#S3.SS2.SSS2
    "3.2.2 Improving Data Efficiency in Imitation Learning with VAEs ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), unified representations of multi-modal input (Section [3.2.3](#S3.SS2.SSS3
    "3.2.3 Managing Multi-Modal Inputs in Imitation Learning via VAEs ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), or disentangled representations of the states for mitigating causal
    confusion (Section [3.2.5](#S3.SS2.SSS5 "3.2.5 Tackling Causal Confusion in Imitation
    Learning with VAEs ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")), etc. (2) GANs utilize an adversarial
    learning framework to practically realize distribution matching. Specifically,
    GANs can be used to minimize the discrepancy between the learned policy and expert
    policy, i.e., $d(\rho_{\pi}(s,a)||\rho_{\pi_{E}}(s,a))$, for IL, or the discrepancy
    between the learned policy and behavior policy, i.e., $d(\rho_{\pi}(s,a)||\rho_{\mu}(s,a))$,
    to mitigate the OOD issue for offline RL, as introduced in Section [4.1.1](#S4.SS1.SSS1
    "4.1.1 Fundamental GAN-Based Imitation Learning Algorithms: GAIL and AIRL ‣ 4.1
    Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") and [4.2.2](#S4.SS2.SSS2 "4.2.2 Policy Approximation Using
    GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), respectively. (3) NFs
    can provide exact density estimation due to its special architecture design (i.e.,
    invertiable and differentiable components), enabling them to be incorporated with
    various advanced IL frameworks, as detailed in Section [5.1.1](#S5.SS1.SSS1 "5.1.1
    Exact Density Estimation in Imitation Learning Using Normalizing Flows ‣ 5.1 Imitation
    Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions").
    We highlight that whenever there is a requirement for exact density estimation,
    NFs can be utilized. For example, SOPT, as introduced in Section [3.1.3](#S3.SS1.SSS3
    "3.1.3 Addressing the Issue of Out-of-Distribution Actions Using VAEs ‣ 3.1 Offline
    Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), proposes to approximate the behavior policy’s density
    function, i.e., $\mu(a|s)$, using a VAE to apply support-constraint offline RL.
    However, the NF is a better choice for this purpose since it can provide more
    exact estimations. (4) Due to its ability for sequence modelling, transformers
    enable trajectory-optimization-based offline RL/IL, as introduced in Section [6.1.1](#S6.SS1.SSS1
    "6.1.1 Background on Trajectory-Optimization-based Offline Reinforcement Learning
    ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") and [6.2.1](#S6.SS2.SSS1 "6.2.1 A Paradigm of Transformer-based
    Imitation Learning ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"). Also, as shown in Section [6.2.3](#S6.SS2.SSS3
    "6.2.3 Developing Generalist Imitation Learning Agents with Transformers ‣ 6.2
    Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), owing to its high capacity and generalization ability, it can be
    used as the core component of a generalist agent to handle a range of tasks with
    a single set of parameters, and it allows for pretraining and fine-tuning as in
    CV and NLP. (5) Finally, DM can be used to purify noisy training data with its
    special denoising process, as illustrated in Section [7.1.2](#S7.SS1.SSS2 "7.1.2
    Addressing Common Issues of Imitation Learning with Diffusion Models ‣ 7.1 Imitation
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    and solve model-based offline RL as trajectory modelling, as shown in Section
    [7.2.2](#S7.SS2.SSS2 "7.2.2 Diffusion Models as Planners ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    due to its efficiency in generating high-dimensional data and its special conditional
    sampling mechanisms (i.e., classifier guided or classifier-free guided sampling).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Integrated use of different DGMs for offline policy learning: The unique usages/advantages
    of each DGM form the basis of an integrated use of different DGMs in offline policy
    learning, and there are already some works exploring in this direction. Here,
    we list some (but not all) examples introduced in this paper. (1) In Section [4.1.4](#S4.SS1.SSS4
    "4.1.4 Integrating VAEs and GANs for Imitation Learning: A Spotlight ‣ 4.1 Imitation
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), we present a spotlight on integrating VAEs and GANs for
    IL to synthesize their advantages, i.e., to mitigate the mode collapse issue of
    GANs and the blurry sample issue of VAEs. (2) In Section [5.2](#S5.SS2 "5.2 Reinforcement
    Learning with Offline Data ‣ 5 Normalizing Flows in Offline Policy Learning ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), APAC adopts a Flow-GAN to model the distribution of $(s,a)$
    in $D_{\mu}$, which can be viewed as an integration of NFs and GANs; SAFER adopts
    a VAE to extract the safety context from the state sequence, which is then used
    as a condition of an NF-based policy for safe generation. (3) In Section [6.1.3](#S6.SS1.SSS3
    "6.1.3 Mitigating Impacts from Environmental Stochasticity ‣ 6.1 Offline Reinforcement
    Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    SPLT shows a coherent use of VAEs and transformers, where the VAE is used to capture
    the multiple modes/possibilities within the data as latent variables, which are
    then used as conditioners of a Decision Transformer to mitigate the impact from
    environmental stochasticity. (4) MIA, as introduced in Section [6.2.3](#S6.SS2.SSS3
    "6.2.3 Developing Generalist Imitation Learning Agents with Transformers ‣ 6.2
    Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), adopts a GAN-based auxiliary loss term to improve the data usage
    in transformer-based IL. (5) In Section [7.1.1](#S7.SS1.SSS1 "7.1.1 Improving
    Policy Expressiveness in Imitation Learning with Diffusion Models ‣ 7.1 Imitation
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    all three DM-based IL works implement their denoising functions as transformers,
    rather than commonly-used U-nets, for sample quality and policy expressiveness.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DGM | VAE | GAN | Normalizing Flow | Transformer | Diffusion Model |'
  prefs: []
  type: TYPE_TB
- en: '| IL | BC (ELBO, VIB) | MaxEntIRL (AIL) | KL, LIL, AIL | BC (self-supervision)
    | BC |'
  prefs: []
  type: TYPE_TB
- en: '| Offline RL | DP-based offline RL | Model-based offline RL | DP-based offline
    RL | Trajectory optimization | DP/Model-based offline RL, Trajectory modelling
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: A summary of the base offline RL/IL algorithms for DGM-based offline
    policy learning. MaxEntIRL (Section [4.1.1](#S4.SS1.SSS1 "4.1.1 Fundamental GAN-Based
    Imitation Learning Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) is
    short for maximum causal entropy inverse RL. KL, LIL, and AIL (Section [5.1.1](#S5.SS1.SSS1
    "5.1.1 Exact Density Estimation in Imitation Learning Using Normalizing Flows
    ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) denote the KL-divergence-based, Likelihood-based, and
    Adversarial IL framework, respectively. DP-based offline RL (Section [3.1.1](#S3.SS1.SSS1
    "3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) refers to the Dynamic-Programming-based
    offline RL.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A summary of the base IL/offline RL algorithms for each DGM: We provide this
    summary as Table [12](#S8.T12 "Table 12 ‣ 8.1 Discussions on Deep Generative Models
    and Offline Policy Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"). (1) Regarding IL, most DGMs, including VAEs, GANs, and DMs, select
    BC as the base algorithm, of which the objective is simply to maximize a log likelihood
    as shown in Eq. ([88](#S7.E88 "In 7.1 Imitation Learning ‣ 7 Diffusion Models
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")). However, there are
    differences in the realization. Specifically, as introduced in Section [3.2.1](#S3.SS2.SSS1
    "3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation Learning ‣
    3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    VAE-based IL algorithms either adopt a VAE ELBO, which is a lower bound of the
    BC objective, or a VIB-based framework, which is closed in form with the ELBO.
    For transformer-based Il, besides the BC term, to make full use of demonstrations,
    auxiliary supervision objectives, such as prediction errors on the next state
    (i.e., the forward model loss) or the intermediate action between two consecutive
    states (i.e., the inverse model loss), are often employed. As for DM-based IL,
    the objective for training DM (i.e., Eq. ([27](#S2.E27 "In 2nd item ‣ 2.5 Diffusion
    Models ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))) provides
    a lower bound of the log-likelihood, which naturally connects DM with BC, as mentioned
    in the beginning of Section [7.1](#S7.SS1 "7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"). On the other
    hand, GANs and NFs are integrated with more advanced IL frameworks. In particular,
    as introduced in Section [4.1.1](#S4.SS1.SSS1 "4.1.1 Fundamental GAN-Based Imitation
    Learning Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"), the fundamental
    GAN-based Il algorithms are derived from MaxEntIRL and practically implemented
    as Adversarial IL (AIL) frameworks; NFs, as exact density estimators, have the
    flexibility to be adopted in various IL frameworks such as KL, LIL, and AIL, as
    illustrated in Section [5.1.1](#S5.SS1.SSS1 "5.1.1 Exact Density Estimation in
    Imitation Learning Using Normalizing Flows ‣ 5.1 Imitation Learning ‣ 5 Normalizing
    Flows in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"). (2) As for offline
    RL, we have introduced multiple categories within it throughout this paper, including
    dynamic-programming-based, model-based, and trajectory-optimization-based offline
    RL, as shown in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), [4.2.1](#S4.SS2.SSS1
    "4.2.1 Background on Model-based Offline Reinforcement Learning ‣ 4.2 Offline
    Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), and [6.1.1](#S6.SS1.SSS1 "6.1.1 Background on Trajectory-Optimization-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), respectively. The corresponding
    categories for each DGM are listed in Table [12](#S8.T12 "Table 12 ‣ 8.1 Discussions
    on Deep Generative Models and Offline Policy Learning ‣ 8 Discussions and Open
    Problems ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"). More specifically, we note that VAE-based
    offline RL mainly follows policy penalty, support constraint, and pessimistic
    value methods, which are subcategories of dynamic-programming-based offline RL,
    as introduced in Section [3.1.3](#S3.SS1.SSS3 "3.1.3 Addressing the Issue of Out-of-Distribution
    Actions Using VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"); while, DM-based offline
    RL mainly follows another subcategory of dynamic-programming-based offline RL
    – policy constraint methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Seminal works of DGM-based offline policy learning: Regarding the applications
    in offline policy learning, developments for different DGMs are quite unbalanced,
    and even for the same type of DGM, the applications in IL may be significantly
    more than the ones in offline RL, or vice versa. One main factor is whether there
    are seminal works in that category, since many research works would follow the
    seminal ones for extensions. Here, we highlight the seminal works introduced in
    this paper: VAE - offline RL - BCQ (Fujimoto et al. ([2019](#bib.bib100))), GAN
    - IL - GAIL (Ho & Ermon ([2016](#bib.bib133))) and AIRL (Fu et al. ([2017](#bib.bib98))),
    Transformer - offline RL - Decision Transformer (Chen et al. ([2021b](#bib.bib46)))
    and Trajectory Transformer (Janner et al. ([2021](#bib.bib149))), Diffusion Model
    - offline RL - Diffuser (Janner et al. ([2022](#bib.bib150))) and Decision Diffuser
    (Ajay et al. ([2023](#bib.bib5))). There are still no seminal works for NF-based
    offline policy learning, which explains why there are relatively fewer works in
    Section [5](#S5 "5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"). Notably, some seminal works pioneer new paradigms for offline policy
    learning. For example, GAIL converts IL/IRL to a distribution matching problem
    ^(24)^(24)24Although GAIL- or AIRL-based algorithms are used for imitation learning
    from offline expert data, these algorithms also rely on simulators in their learning
    process, because there is an inner (online) RL process in their algorithm designs.
    Similarly, most algorithms within Section [5.1.1](#S5.SS1.SSS1 "5.1.1 Exact Density
    Estimation in Imitation Learning Using Normalizing Flows ‣ 5.1 Imitation Learning
    ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    require simulators, as they also realize imitation learning via distribution matching
    (like GAIL and AIRL), i.e., $\min_{\pi}d(\rho_{\pi}(s,a)||\rho_{\pi_{E}}(s,a))$,
    and RL is a natural choice for optimization regarding the occupancy measure of
    the policy, i.e., $\rho_{\pi}(s,a)$.; Decision Transformer realizes offline RL
    via trajectory optimization, which eliminates the necessity to fit value functions
    through dynamic programming or to compute policy gradients; Diffuser folds the
    two processes in model-based offline RL: transition dynamic modeling and trajectory
    optimization, into a trajectory modeling process. These paradigm shifts are closely
    related to corresponding DGMs and make full use of their unique advantages, which
    open up promising directions for offline policy learning. On the other hand, simply
    using DGMs as function estimators in traditional offline RL or IL methods can
    be less attractive, unless the applications of DGMs brings superior scalability
    or generalization ability.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DGM | VAE | GAN | Normalizing Flow | Transformer | Diffusion Model |'
  prefs: []
  type: TYPE_TB
- en: '| IL issues | Insufficient demonstrations, Causal confusion, Multi-modal input
    | Insufficient demonstrations, Compounding error, Multi-modal demonstrations |
    Exact density estimation, Learning from observations, Policy expressiveness |
    Sequential/ multi-modal/ multi-entity input, Multi-modal actions, Insufficient
    demonstrations | High-dim/ multi-modal state-action, Spurious correlations, Insufficient/
    low-quality demonstrations, Compounding error |'
  prefs: []
  type: TYPE_TB
- en: '| IL extensions | Hier | MT, MA, Hier, MB, Safety | N/A | MT, Generalization
    | Hier |'
  prefs: []
  type: TYPE_TB
- en: '| Offline RL issues | OOD actions, Low-quality training data, High-dim states,
    Continuous actions | OOD actions, (Vision-based) world model estimation, Segment
    stitching | OOD actions, Over conservatism, Insufficient training data, Sparse
    reward | OOD actions, Learning difficulty, Insufficient training data, Segment
    stitching, Sparse reward, Partially observable | OOD actions, Insufficient training
    data |'
  prefs: []
  type: TYPE_TB
- en: '| Offline RL extensions | MT, Hier | Hier, MB | Hier, Safety | MT, MA, Hier,
    MB, Safety, Generalization | MT, MA, Hier, MB, Safety |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13: A summary of the issues and extensions of offline policy learning
    targeted by the DGMs. MT, MA, Hier, MB represents multi-task, multi-agent, hierarchical,
    model-based learning, respectively. Safety and generalization refer to if the
    learned policy can be safely applied to risky environments or be generalizable
    to unseen environments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A summary of the issues and extensions of offline policy learning that have
    been targeted by the DGMs: In Table [13](#S8.T13 "Table 13 ‣ 8.1 Discussions on
    Deep Generative Models and Offline Policy Learning ‣ 8 Discussions and Open Problems
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), we enumerate the IL/offline RL issues that each DGM has
    tried to solve, as well as the extended IL/offline RL setups that have been explored
    by each DGM. Specifically, there are in total six setup extensions, including
    multi-task (MT), multi-agent (MA), hierarchical (Hier), model-based (MB) learning,
    policy safety, and policy generalization. Readers can easily find the research
    works targeting at specific issues/extensions in corresponding sections. However,
    we note that the listed issues/extensions should not be considered as completely
    resolved by the DGMs, and future works can focus on the unsolved or under-explored
    issues/extensions as detailed in Section [8.2.4](#S8.SS2.SSS4 "8.2.4 Future Works
    on Algorithm Designs ‣ 8.2 Perspectives on Future Directions ‣ 8 Discussions and
    Open Problems ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"). Here, we provide some remarks
    on this table. (1) Low-quality training data is characterized by its limited coverage
    of the state-action space, a lack of diversity in behavior patterns, or the inclusion
    of suboptimal/noisy demonstrations. (2) Multi-modal demonstrations/actions refer
    to demonstrations/actions containing multiple distributional modes, while by multi-modal
    input, we mean input from multiple sensors or in various formats (e.g., images
    and texts). (3) Learning from observations refer to imitating from sequences of
    states only, which is notably more challenging than usual imitation learning.
    (4) Compared with BC, GAN-based IL can mitigate the issues of insufficient demonstrations
    and compounding errors. This is because GAN-based IL methods do more than just
    mimicking observed behaviors – they reason about the underlying reward function
    from demonstrations and utilizes it for further RL training, which complements
    the static demonstrations and enables the agent to act reasonably at unseen states.
    However, GAN-based IL methods rely on simulators for RL training and suffer from
    issues regarding sampling efficiency and training stability, as introduced in
    Section [4.1.2](#S4.SS1.SSS2 "4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"). (5) Decision Transformer has lower learning difficulty and can avoid
    OOD actions for offline RL, because it is based on trajectory optimization, which
    eliminates the necessity to fit value functions or compute policy gradients, and
    multiple state and action anchors throughout the trajectory prevent the learned
    policy from deviating too far from the behavior policy. However, there are still
    pending issues for Decision Transformer, such as the impacts from environmental
    stochasticity (Section [6.1.3](#S6.SS1.SSS3 "6.1.3 Mitigating Impacts from Environmental
    Stochasticity ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) and its theoretical shortcomings
    as listed in Section [6.1.5](#S6.SS1.SSS5 "6.1.5 Reflections on Transformer-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Foundation model & Algorithm & Data: Among all the applications of DGMs, transformer-based
    IL/offline RL have shown some of the most promising and exciting evaluation results:
    (1) As listed in Table [9](#S6.T9 "Table 9 ‣ 6.2.3 Developing Generalist Imitation
    Learning Agents with Transformers ‣ 6.2 Imitation Learning ‣ 6 Transformers in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), most algorithms have
    seen success in challenging robotic simulators, real-world tasks, or large scale
    competitions; (2) As shown in Section [6.2.3](#S6.SS2.SSS3 "6.2.3 Developing Generalist
    Imitation Learning Agents with Transformers ‣ 6.2 Imitation Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), the use of transformer
    enables the process of input encompassing (extra) long time sequences or multiple
    modalities/entities, and (robotic) agents that can follow language instructions
    or interact with humans can be developed solely by imitation and self-supervision;
    (3) As introduced in Section [6.1.4](#S6.SS1.SSS4 "6.1.4 Transformers in Extended
    Offline Reinforcement Learning Setups ‣ 6.1 Offline Reinforcement Learning ‣ 6
    Transformers in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"), Gato (Reed
    et al. ([2022](#bib.bib271))), as a single transformer with the same set of weights,
    can handle 604 distinct tasks with varying modalities (i.e., images, texts, robotic
    control, and video gaming); further, Reid et al. ([2022](#bib.bib272)) and Takagi
    ([2022](#bib.bib316)) explore pretraining the transformer on language tasks and
    fine-tuning it on RL tasks, and demonstrate promising cross-modality pretraining
    effects. All these advancements are based on simple supervised learning objectives
    and are not related to any theoretical breakthrough. However, they do rely on
    a high-capacity and computation-efficient foundation model – transformer, and
    a large amount of diverse training data from multiple modalities. This leads to
    an inspiring paradigm for developing generalist agents, that is, using a potent
    foundation model trained by (self-) supervised learning from extensive, high-quality
    demonstrations. On the other hand, it also raises the question whether the future
    development should be data-driven or algorithm-driven, but there is no doubt that
    the development of new foundation models or algorithms should put more emphasis
    on the scalability and computation efficiency, like the transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Perspectives on Future Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the discussions above and the main content in Section [3](#S3 "3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") - [7](#S7
    "7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), we
    present some perspectives on future research directions here, and we categorize
    the content in this section as four aspects: data, benchmarking, theories, and
    algorithms. We believe more open problems can be gleaned from the detailed introductions
    in the main content, and we have included comments on future works specific to
    certain DGMs in corresponding sections, including the last paragraphs of Section
    [3](#S3 "3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), [5](#S5 "5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), [6.2](#S6.SS2 "6.2 Imitation Learning ‣ 6 Transformers in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), [7.1](#S7.SS1 "7.1 Imitation
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    [7.2.1](#S7.SS2.SSS1 "7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    and Section [6.1.5](#S6.SS1.SSS5 "6.1.5 Reflections on Transformer-based Offline
    Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions").'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Future Works on Data-centric Research
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: How would the offline policy learning agent evolve with the quality and quantity
    of the training data? The performance of offline policy learning is closely related
    to the quality and quantity of the provided offline data. Therefore, one potential
    research direction would be investigating how the dataset’s characteristics would
    impact the learned policy, as instructions for building datasets. (1) First, the
    static dataset may contain noise (i.e., disturbed or misleading information) or
    suboptimal behaviors ^(25)^(25)25This would be an issue for IL, since IL requires
    expert-level demonstartions. However, for offline RL, the dataset could contain
    suboptimal behaviors as long as their rewards are correctly labeled. If there
    are only suboptimal trajectories in the dataset for offline RL, the learning difficulty
    would be high, since the agent needs to learn to stitch segments from various
    trajectories to form an optimal strategy.. In this case, open questions include
    how to measure the noise or suboptimality of the provided data and how robust
    can the policy learning be to these data imperfections. The answers to these questions
    would vary with the used algorithms, DGMs, or evaluation tasks, but the imperfection
    measure and robustness evaluation protocol can be general and beneficial for algorithm
    development. These questions are important because perfect datasets are costly
    to build in scale and the tolerance for imperfection can greatly reduce the burden
    for data collection and possessing. (2) Second, the performance of an offline
    policy learning agent on evaluation tasks relies heavily on the coverage and diversity
    of the training data. An effective set of behaviors for policy learning should
    cover the possible task scenarios and provide various behavioral patterns/skills
    for the agent to learn. How to measure the coverage and diversity of the dataset
    and how these properties would influence the generalization of the learned policy
    on evaluation tasks would be interesting questions. (3) One key factor contributing
    to the success of certain DGMs in CV and NLP is their ability to leverage extensive
    training datasets. Notably, their training performance would continue to improve
    as the quantity of the training data increases. Thus, for each variant of DGM-based
    offline policy learning, it is essential to study their scalability, that is,
    how the learned agent would evolve with the quantity of the training data. Algorithms
    with superior scalability would be more promising in challenging, real-life applications.
  prefs: []
  type: TYPE_NORMAL
- en: How to construct a training dataset for effective offline policy learning? With
    efforts for the open problems mentioned above, we could establish measurements
    for noise, suboptimality, diversity, and coverage of the dataset and get some
    insights on the relationship between these measurements and the performance of
    the learned policy, which can guide the construction of training datasets for
    effective offline policy learning. However, there are some other problems that
    need to be explored. First, the original state and action spaces may be high-dimensional
    and continuous, which would require exponentially more data for policy learning.
    Utilizing compact representations, which can be obtained from DGMs like VAEs and
    encoder-only transformers, or discretization could help improve the learning efficiency,
    but also would introduce inaccuracy. Second, for offline RL datasets, sparse rewards
    can bring training difficulty but dense rewards are costly to annotate. Thus,
    a tradeoff needs to be achieved between them and strategically annotating part
    of the dataset with rewards could be a promising direction. Similarly, although
    there are algorithms for learning multi-task, hierarchical, or safe policies from
    data without the task, skill, or safe action labels, having these labels either
    fully or partially annotated can significantly aid in the policy learning process.
    However, given that real-life datasets can be enormous in scale, the labeling
    should be only for critical points or regions in the state-action space, which
    requires techniques for identification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Augmentation & Data Synthesis. VAEs, transformers, and DMs have been used
    for data augmentation or synthesis, as shown in Section [3.1.4](#S3.SS1.SSS4 "3.1.4
    Data Augmentation and Transformation with VAEs ‣ 3.1 Offline Reinforcement Learning
    ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    [6.1.2](#S6.SS1.SSS2 "6.1.2 Balancing Model Capacity with Training Data ‣ 6.1
    Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), and [7.2.3](#S7.SS2.SSS3 "7.2.3 Diffusion Models as Data
    Synthesizers ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), respectively, of which the purpose
    is simply to involve more data for training. The data used for augmentation may
    already exist but require transformation or supplementation to be effectively
    utilized, or could be synthesized using DGMs. DGMs exhibit remarkable capability
    in synthesizing new data that shares statistical properties with the real data.
    Thus, this approach has the potential to bridge the gap between the demand for
    large datasets and the feasibility of acquiring them. Multiple research directions
    in this domain could be developed. First, based on the coverage or diversity indicators
    of the dataset, targeted data synthesis to supplement the under-covered area or
    to avoid repeated behavioral patterns for better diversity could be developed.
    Second, the synthesized data usually cannot be directly employed without filtering.
    In this case, necessary quality measure, such as the similarity of the synthesized
    data with the real (optimal) data, should be established. Also, ideally, there
    should be (theoretical) study on the relationship of the suboptimality of the
    learned policy with the quality/quantity of the synthesized data. All in all,
    it would be quite exciting to see the creation of a positive feedback loop: enhanced
    policies emerging from newly synthesized data, which in turn can be leveraged
    to generate even higher quality data for training.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Future Works on Benchmarking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Development of more realistic, challenging benchmarks. We notice that most
    DGM-based offline RL algorithms are evaluated on D4RL, as shown in the tables
    of Section [3](#S3 "3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") - [7](#S7 "7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), which is a standard benchmark for offline RL. However,
    widely-used benchmarks like D4RL, cannot fully show or evaluate the advantages
    of DGM-based offline policy learning algorithms, as we have seen agents, which
    are implemented as two-layer neural networks and trained with a moderate amount
    of data, can already achieve excellent performance on these benchmarks. To fully
    explore the potential and guide the development of DGM-based offline policy learning,
    more challenging benchmarks are required. (1) First, the benchmark dataset should
    be large in size to evaluate the scalability of the DGM-based algorithms. This
    includes evaluating the trend of performance improvement relative to the volume
    of training data and determining whether DGM-based algorithms can significantly
    outperform the others when provided with extensive training data. (2) Second,
    the dataset should contain multiple modalities. For example, state inputs could
    be collected from various sources, such as language instructions, visual information
    from different sensors, or even videos; moreover, the state-action space should
    feature multiple distributional modes, such as containing distinct trajectories
    for achieving the same goal. DGM-based algorithms are expected to effectively
    harness information from diverse modalities and learn policies that cover the
    multiple modes within the policy space. Further, benchmarks should not be limited
    to control or decision-making tasks but also include assignments from other modalities,
    such as CV and NLP. These cross-modal tasks could serve as auxiliary tasks or
    be utilized to evaluate the capabilities of generalist agents. (3) The task scenarios
    should be more realistic. A main reason that CV and NLP techniques, which are
    usually DGM-based, can be widely adopted in real life is that these algorithms
    are directly evaluated on real-life tasks/datasets rather than simplified simulators.
    Thus, realistic offline dataset in a large scale could greatly advance the development
    of offline policy learning and showcase the superiority of DGM-based algorithms.
    (4) The benchmark should evaluate more than just the policy return. It should
    also assess aspects such as the generalization, safety of the learned policy and
    the robustness of policy learning in noisy or sparse-reward environments.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparisons among different DGMs. Although there are numerous offline RL or
    IL algorithms for each type of DGMs and they have shared benchmarks, comparisons
    among them are quite rare. It would be insightful to see fair and thorough comparisons
    among different DGMs with the same usage (e.g., as policy backbones) on the same
    set of tasks. Also, as task complexity or dataset size escalates, analyzing the
    trends in policy performance and computational cost across different DGMs can
    provide useful insights. Such comparisons provide suggestions on the choice of
    DGMs, conditioning on the task difficulty and dataset scale. Moreover, as mentioned
    in the main text of this paper, algorithms that fall in the same category, such
    as NF-based offline RL algorithms, are also rarely compared with each other. Comparisons
    within the same category can be valuable, as they can indicate which usage of
    a certain DGM is more effective.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3 Future Works on Theories
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most works on applying DGMs in offline policy learning focus on algorithm designs
    rather than theoretical analysis, so more theoretical research works could be
    done for this field. One promising direction is derivations of the convergence
    rate or performance guarantee for the seminal works in DGM-based offline policy
    learning, such as (Zhang et al. ([2020b](#bib.bib390))) for GAIL and (Brandfonbrener
    et al. ([2022](#bib.bib30))) for return-conditioned supervised learning like Decision
    Transformer, based on the theoretical results from either DGMs or offline RL/IL.
    Moreover, we notice that there have been efforts on theoretically unifying the
    objectives of different DGMs for a deeper understanding of the relationship between
    DGMs and improved learning performance, such as (Nielsen et al. ([2020](#bib.bib233));
    Kingma & Gao ([2023](#bib.bib163))). This group of works can greatly inspire development
    of new DGM-based offline policy learning algorithms or unified analysis of existing
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we outline some specific theoretical problems in this field. (1) First,
    regarding GAN-based IL (i.e., Section [4.1](#S4.SS1 "4.1 Imitation Learning ‣
    4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), so far, no approach has been developed that is both computationally
    efficient and can theoretically guarantee the recovery of the reward function.
    Also, many works propose to replace the on-policy RL within GAIL/AIRL (i.e., TRPO)
    with off-policy RL algorithms (e.g., DDPG) for improved sample efficiency. However,
    this improvement lacks theoretical backup. With off-policy RL, the policy training
    at each iteration is based on samples from multiple past iterations, during which
    the reward function keeps changing. As a result, the RL training is conducted
    in an unstationary MDP where typical RL algorithms would theoretically fail. (2)
    Second, as mentioned in Section [6.1.5](#S6.SS1.SSS5 "6.1.5 Reflections on Transformer-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), return-conditioned
    supervised learning (e.g., Decision Transformer) alone is unlikely to be a general
    solution for offline RL problems, and it offers guarantees only when the environment
    dynamics are nearly deterministic. Thus, adapting these algorithms to stochastic
    environments, ideally backed by theoretical optimality guarantees, is a clear
    necessity. Also, Decision Transformer would imitate both high-return and low-return
    trajectories, and claims that the learning performance in this manner is better
    than imitating high-return trajectories only. A theoretical explanation on how
    the low-return behavior learning would benefit the overall performance can be
    insightful. (3) Third, as introduced in Section [7.2.2](#S7.SS2.SSS2 "7.2.2 Diffusion
    Models as Planners ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), the algorithm design
    of Diffuser is mainly based on intuitions from similar problems, such as the inpainting
    problem and receding horizon control, but lacks theoretical support. To be specific,
    in Eq. ([99](#S7.E99 "In 7.2.2 Diffusion Models as Planners ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    to decide on the action choice $a$ at state $s$, the trajectory generation is
    hardcoded to start with $s$, and then the action $\hat{a}$ that is right after
    $s$ in the generated trajectory is adopted as $a$. The problem here is how to
    ensure the causal relationship between $s$ and $a$, which is crucial for (RL)
    sequential decision-making, and whether it is necessary to generate a trajectory
    segment in case that only the first action prediction is utilized.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.4 Future Works on Algorithm Designs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'From the discussions in Section [8.1](#S8.SS1 "8.1 Discussions on Deep Generative
    Models and Offline Policy Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") and the main content in Section [3](#S3 "3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") - [7](#S7 "7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"), we can identify
    many potential future directions regarding the algorithm design. Here, we list
    some of them as examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under-explored categories in the main text. For existing categories of algorithms,
    some of them are still under-explored, such as GAN-based IL algorithms that do
    not rely on simulators (Section [4.1](#S4.SS1 "4.1 Imitation Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), NF-based
    IL where the NF works as the policy (Section [5.1.2](#S5.SS1.SSS2 "5.1.2 Policy
    Modeling in Imitation Learning with Normalizing Flows ‣ 5.1 Imitation Learning
    ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    NF-based offline RL algorithms (Section [5.2.1](#S5.SS2.SSS1 "5.2.1 Adopting Normalizing
    Flows in Offline Reinforcement Learning ‣ 5.2 Reinforcement Learning with Offline
    Data ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    efficient algorithms on mitigating impacts from environmental stochasticity for
    transformer-based offline RL (Section [6.1.3](#S6.SS1.SSS3 "6.1.3 Mitigating Impacts
    from Environmental Stochasticity ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), transformer-based
    offline RL which integrates traditional offline RL methods with transformer architectures
    (Section [6.1.5](#S6.SS1.SSS5 "6.1.5 Reflections on Transformer-based Offline
    Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), DM-based offline policy
    learning where DMs work as data synthesizers (Section [7.2.3](#S7.SS2.SSS3 "7.2.3
    Diffusion Models as Data Synthesizers ‣ 7.2 Offline Reinforcement Learning ‣ 7
    Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), and
    so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Integrated use of various DGMs for offline policy learning. As mentioned in
    Section [8.1](#S8.SS1 "8.1 Discussions on Deep Generative Models and Offline Policy
    Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), each
    DGM has distinct usages due to their special design and there have been a few
    algorithms managing to integrate two DGMs together for improved offline policy
    learning performance. More in-depth exploration can be done in this direction,
    especially for involving more DGMs in a unified offline policy learning framework.
    For example, we observe from Table [12](#S8.T12 "Table 12 ‣ 8.1 Discussions on
    Deep Generative Models and Offline Policy Learning ‣ 8 Discussions and Open Problems
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") that most DGM-based IL algorithms are based on BC, which
    is a relatively basic IL algorithm and has fundamental issues ^(26)^(26)26BC is
    implemented as supervised learning, of which the objective is shown as Eq. ([88](#S7.E88
    "In 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")). However, predictions made by $\pi$, which is learned
    with BC and used for sequential decision-making, can impact future observations,
    thus breaching a fundamental assumption of supervised learning (Ross & Bagnell
    ([2010](#bib.bib281))): training inputs should be drawn from an independent and
    identically distributed population. Consequently, errors and deviations from the
    demonstrated behavior tend to accumulate over time, as minor mistakes lead the
    agent into areas of the observation space that the expert has not ventured into,
    known as the compounding error. Additionally, BC learns policies merely through
    imitation, without engaging in reasoning, which restricts the generalization capability
    of the learned policy. In this case, extending the base IL framework for transformer-
    or DM-based IL can be essential future directions., while NFs are integrated with
    various advanced IL frameworks due to its ability for exact density estimations.
    In this case, NFs can be integrated with transformer-based DMs, where NFs extend
    the base IL framework and transformer-based DMs work as a core component of a
    generalist agent that can deal with multi-modal input and high-dimensional generations.
    Further, they can be integrated with VAEs, which can be used to extract task or
    subtask representations for multi-task or hierarchical learning, as introduced
    in Section [3.1.5](#S3.SS1.SSS5 "3.1.5 Offline Multi-task/Hierarchical RL based
    on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"). In addition, we notice
    that there are some research working on integrating the advantages of various
    DGMs for computer vision tasks, such as VQ-GAN (VAE+GAN+Transformer, Esser et al.
    ([2021](#bib.bib86))), TransGAN (GAN+Transformer, Jiang et al. ([2021](#bib.bib152))),
    DiffuseVAE (VAE+DM, Pandey et al. ([2022](#bib.bib238))), DiTs (Transformer+DM,
    Peebles & Xie ([2023](#bib.bib247))), which can be potentially utilized for offline
    policy learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsolved issues and extensions of DGM-based offline policy learning. Table
    [13](#S8.T13 "Table 13 ‣ 8.1 Discussions on Deep Generative Models and Offline
    Policy Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    provides a summary of the issues and extensions of IL and offline RL that have
    been explored by DGM-based algorithms, which provides indications for future works.
    (1) By checking the outlined issues, we can pinpoint unresolved or inadequately
    addressed issues by existing methods as future directions. For example, although
    there have been GAN-based or DM-based algorithms targeting at the compounding
    error issue, the GAN-based ones rely on simulators and DM-based ones (i.e., AWE
    and MCNN in Section [7.1.2](#S7.SS1.SSS2 "7.1.2 Addressing Common Issues of Imitation
    Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) are still in the early
    stages of development. Similarly, in offline RL, reward sparsity is a significant
    issue, for which the explorations made by NFs (i.e., NF Shaping in Section [5.2.2](#S5.SS2.SSS2
    "5.2.2 Adopting Normalizing Flows in Online Reinforcement Learning with Offline
    Data ‣ 5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) and transformers (i.e., DTRD
    in Section [6.1.2](#S6.SS1.SSS2 "6.1.2 Balancing Model Capacity with Training
    Data ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) have tried reward shaping and reward function learning,
    which are both interesting directions awaiting further development. (2) Regarding
    the extensions, there can be in total six (or more) setup extensions: multi-task,
    multi-agent, hierarchical, model-based, safety, and generalization. Table [13](#S8.T13
    "Table 13 ‣ 8.1 Discussions on Deep Generative Models and Offline Policy Learning
    ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions") illustrates
    the extensions that are awaiting exploration for each DGM. Further, we note that
    the extension method can be potentially transferred among DGMs. For instance,
    as shown in Section [7.2.4](#S7.SS2.SSS4 "7.2.4 Diffusion Models in Extended Offline
    Reinforcement Learning Setups ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"), DM-based
    multi-task and hierarchical offline RL algorithms adopt the same algorithm ideas
    as BOReL and OPAL (i.e., the VAE-based methods introduced in Section [3.1.5](#S3.SS1.SSS5
    "3.1.5 Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")). In particular, we note that multi-agent extensions for DGM-based
    offline policy learning require further development, and existing explorations
    (such as MAGAIL, MA-AIRL in Section [4.1](#S4.SS1 "4.1 Imitation Learning ‣ 4
    Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    and DOM2, MADiff in Section [7.2.4](#S7.SS2.SSS4 "7.2.4 Diffusion Models in Extended
    Offline Reinforcement Learning Setups ‣ 7.2 Offline Reinforcement Learning ‣ 7
    Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) are
    still quite immature. Specifically, developments of MARL algorithms for fully
    competitive or mixed (partially cooperative/competitive) task scenarios based
    on game theory, and integrations of DGMs with state-of-the-art CTDE ^(27)^(27)27CTDE
    refers to a scheme of multi-agent RL (MARL) that employs centralized training
    and decentralized execution. MARL methods are essential future directions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Development of generalist agents. One of the most exciting future directions
    for DGM-based offline policy learning is developing a decision-making agent that
    can continually evolve with the amount of training data and the size of the DGM-based
    policy network and can handle a range of tasks, including unseen ones. CV and
    NLP have made great progress in this direction, owing to the use of DGMs. To realize
    this for offline policy learning, several things should be done. (1) First, the
    base IL or offline RL algorithm should be compatible with deep and broad neural
    networks, so that its performance can grow with the size of the foundation model.
    This is also the reason why mainstream transformer-based and DM-based offline
    policy learning methods (e.g., Decision Transformer and Diffuser) are based on
    supervised learning as in CV and NLP. Future works can work on developing deep
    foundation models that are compatible with policy-gradient methods or dynamic
    programming. (2) Second, besides constructing large-scale, high-quality training
    datasets as mentioned in Section [8.2.1](#S8.SS2.SSS1 "8.2.1 Future Works on Data-centric
    Research ‣ 8.2 Perspectives on Future Directions ‣ 8 Discussions and Open Problems
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), some algorithm-driven efforts can be done. For example,
    algorithms that are robust to suboptimal training data or capable of processing
    multi-modal inputs should be developed. Specifically, learning from demonstration
    videos (in third-person views) is a quite challenging but promising research direction,
    as large-scale video datasets are more accessible and videos usually contain more
    learnable information than images or vectorized data. Agents capable of learning
    from videos have the potential to continuously evolve by observing their surrounding
    environments during deployment. (3) Third, to enhance generalization, employing
    advanced meta-learning or multi-task learning techniques is advisable. Further,
    adopting the "pretraining + fine-tuning" approach, similar to that used in large
    language and vision models, is also promising, since it enables the accumulation
    of training efforts. Notably, the more significant the difference between the
    source and target tasks, the more challenging it becomes to make pretraining effective,
    yet the more data can be utilized for training the agent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Future works in related areas. Finally, there are some related areas that can
    be used as future research directions. First, reviews on the applications of DGMs
    in Inverse Reinforcement Learning (IRL) or (Online) Reinforcement Learning have
    not yet been developed. IRL and RL are important approaches for sequential decision-making,
    but they both require online interactions with the environment. Second, as shown
    in Table [12](#S8.T12 "Table 12 ‣ 8.1 Discussions on Deep Generative Models and
    Offline Policy Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    integrations with dynamic-programming-based, model-based, and trajectory-optimization-based
    offline RL have been explored. There are some other branches in offline RL (Levine
    et al. ([2020](#bib.bib181))), such as importance-sampling-based and uncertainty-estimation-based
    offline RL, which have the potential to be enhanced with DGMs. Third, we notice
    that new foundation models and DGMs are continually emerging, such as Mamba (Gu
    & Dao ([2023](#bib.bib112))) and Poisson Flows (Xu et al. ([2022c](#bib.bib363);
    [2023](#bib.bib364))). Applying these SOTA models in offline policy learning is
    certainly a promising research direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we show a systematic review on the applications of DGMs in offline
    policy learning. In Section [2](#S2 "2 Background on Deep Generative Models ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), we provide necessary background on several mainstream
    DGMs, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing
    Flows, Transformers, and Diffusion Models. Then, in Section [3](#S3 "3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") - [7](#S7
    "7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), we
    introduce the applications of each DGM in both offline RL and IL, i.e., the two
    primary branches of offline policy learning. Each section includes both a tutorial
    and a survey on the respective topic. Following these main content, we provide
    a summary on existing works and our perspectives on future research directions
    in Section [8](#S8 "8 Discussions and Open Problems ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions").
    As the first review paper of this field, we wish this work to be a hands-on reference
    for better understanding the current research progress on DGMs in offline policy
    learning and help researchers to further improve DGM-based offline RL/IL algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Author contributions: Bhargav Ganguly completes Section [3.2](#S3.SS2 "3.2
    Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), and creates a GitHub repository to maintain the list of
    related works. Yang Xu completes Section [4.2](#S4.SS2 "4.2 Offline Reinforcement
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"). Yongsheng Mei completes Section [2.5](#S2.SS5 "2.5 Diffusion
    Models ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"). Jiayu
    Chen polishes these three sections by adding Subsection [3.2.1](#S3.SS2.SSS1 "3.2.1
    Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") and
    [4.2.1](#S4.SS2.SSS1 "4.2.1 Background on Model-based Offline Reinforcement Learning
    ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), and completes all the other sections.
    Tian Lan and Vaneet Aggarwal supervise this project.'
  prefs: []
  type: TYPE_NORMAL
- en: We thank Hanhan Zhou for participating in discussions and assisting in material
    collections.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abramson et al. (2021) Josh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale,
    Mary Cassin, Felix Fischer, Petko Georgiev, Alex Goldin, Tim Harley, Felix Hill,
    Peter C. Humphreys, Alden Hung, Jessica Landon, Timothy P. Lillicrap, Hamza Merzic,
    Alistair Muldal, Adam Santoro, Guy Scully, Tamara von Glehn, Greg Wayne, Nathaniel
    Wong, Chen Yan, and Rui Zhu. Creating multimodal interactive agents with imitation
    and self-supervised learning. *CoRR*, abs/2112.03763, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ada et al. (2023) Suzan Ece Ada, Erhan Öztop, and Emre Ugur. Diffusion policies
    for out-of-distribution generalization in offline reinforcement learning. *CoRR*,
    abs/2307.04726, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agarwal et al. (2020) Shubhankar Agarwal, Harshit Sikchi, Cole Gulino, and Eric
    Wilkinson. Imitative planning using conditional normalizing flow. *CoRR*, abs/2007.16162,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ajay et al. (2021) Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine,
    and Ofir Nachum. OPAL: offline primitive discovery for accelerating offline reinforcement
    learning. In *Proceedings of the 9th International Conference on Learning Representations*.
    OpenReview.net, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ajay et al. (2023) Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, Tommi S.
    Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need
    for decision making? In *Proceedings of the 11th International Conference on Learning
    Representations*. OpenReview.net, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Akimov et al. (2022) Dmitry Akimov, Vladislav Kurenkov, Alexander Nikulin,
    Denis Tarasov, and Sergey Kolesnikov. Let offline rl flow: Training conservative
    agents in the latent space of normalizing flows. In *NeurIPS Offline RL Workshop*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alcorn & Nguyen (2021) Michael A. Alcorn and Anh Nguyen. baller2vec++: A look-ahead
    multi-entity transformer for modeling coordinated agents. *CoRR*, abs/2104.11980,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alemi et al. (2017) Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin
    Murphy. Deep variational information bottleneck. In *Proceedings of the 5th International
    Conference on Learning Representations*. OpenReview.net, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Altman (1998) Eitan Altman. Constrained markov decision processes with total
    cost criteria: Lagrangian approach and dual linear program. *Mathematical methods
    of operations research*, 48(3):387–417, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amari (1993) Shun-ichi Amari. Backpropagation and stochastic gradient descent
    method. *Neurocomputing*, 5(4-5):185–196, 1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amit et al. (2021) Tomer Amit, Tal Shaharbany, Eliya Nachmani, and Lior Wolf.
    Segdiff: Image segmentation with diffusion probabilistic models. *arXiv preprint
    arXiv:2112.00390*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anderson (1982) Brian DO Anderson. Reverse-time diffusion equation models. *Stochastic
    Processes and their Applications*, 12(3):313–326, 1982.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arjovsky et al. (2017) Martín Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein
    GAN. *CoRR*, abs/1701.07875, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arnold (1992) Vladimir I Arnold. *Ordinary differential equations*. Springer
    Science & Business Media, 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Austin et al. (2021a) Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow,
    and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces.
    In *Advances in Neural Information Processing Systems 34*, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Austin et al. (2021b) Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow,
    and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces.
    *Advances in Neural Information Processing Systems*, 34:17981–17993, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avrahami et al. (2022) Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
    diffusion for text-driven editing of natural images. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pp.  18208–18218, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ba et al. (2016) Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer
    normalization. *CoRR*, abs/1607.06450, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baram et al. (2017) Nir Baram, Oron Anschel, Itai Caspi, and Shie Mannor. End-to-end
    differentiable adversarial imitation learning. In *Proceedings of the 34th International
    Conference on Machine Learning*, volume 70, pp.  390–399, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baranchuk et al. (2021) Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin
    Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion
    models. *arXiv preprint arXiv:2112.03126*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Behrmann et al. (2019) Jens Behrmann, Will Grathwohl, Ricky T. Q. Chen, David
    Duvenaud, and Jörn-Henrik Jacobsen. Invertible residual networks. In *Proceedings
    of the 36th International Conference on Machine Learning*, volume 97 of *Proceedings
    of Machine Learning Research*, pp.  573–582\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bellemare et al. (2013) Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael
    Bowling. The arcade learning environment: An evaluation platform for general agents.
    *Journal of Artificial Intelligence Research*, 47:253–279, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bian et al. (2022) Xihan Bian, Oscar Mendez Maldonado, and Simon Hadfield.
    SKILL-IL: disentangling skill and knowledge in multitask imitation learning. In
    *IEEE/RSJ International Conference on Intelligent Robots and Systems*, pp.  7060–7065\.
    IEEE, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blondé & Kalousis (2019) Lionel Blondé and Alexandros Kalousis. Sample-efficient
    imitation learning via generative adversarial nets. In *Proceedings of the 22nd
    International Conference on Artificial Intelligence and Statistics*, volume 89,
    pp.  3138–3148, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boborzi et al. (2021a) Damian Boborzi, Christoph-Nikolas Straehle, Jens S. Buchner,
    and Lars Mikelsons. Learning normalizing flow policies based on highway demonstrations.
    In *Proceedings of the 24th IEEE International Intelligent Transportation Systems
    Conference*, pp.  22–29\. IEEE, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boborzi et al. (2021b) Damian Boborzi, Christoph-Nikolas Straehle, Jens Stefan
    Buchner, and Lars Mikelsons. State-only imitation learning by trajectory distribution
    matching. In *Submission to the 10th International Conference on Learning Representations*,
    2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boborzi et al. (2022) Damian Boborzi, Christoph-Nikolas Straehle, Jens S. Buchner,
    and Lars Mikelsons. Imitation learning by state-only distribution matching. *CoRR*,
    abs/2202.04332, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bonatti et al. (2020) Rogerio Bonatti, Ratnesh Madaan, Vibhav Vineet, Sebastian A.
    Scherer, and Ashish Kapoor. Learning visuomotor policies for aerial navigation
    using cross-modal representations. In *IEEE/RSJ International Conference on Intelligent
    Robots and Systems*, pp.  1637–1644\. IEEE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boustati et al. (2021) Ayman Boustati, Hana Chockler, and Daniel C. McNamee.
    Transfer learning with causal counterfactual reasoning in decision transformers.
    *CoRR*, abs/2110.14355, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brandfonbrener et al. (2022) David Brandfonbrener, Alberto Bietti, Jacob Buckman,
    Romain Laroche, and Joan Bruna. When does return-conditioned supervised learning
    work for offline reinforcement learning? In *Advances in Neural Information Processing
    Systems 35*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brehmer et al. (2023) Johann Brehmer, Joey Bose, Pim de Haan, and Taco Cohen.
    EDGI: equivariant diffusion for planning with embodied agents. *CoRR*, abs/2303.12410,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brockman et al. (2016) Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas
    Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brooks et al. (2024) Tim Brooks, Bill Peebles, Connor Homes, Will DePue, Yufei
    Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Wing Yin
    Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators,
    2024. URL [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In *Advances in Neural Information
    Processing Systems 33*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cang et al. (2022) Catherine Cang, Kourosh Hakhamaneshi, Ryan Rudes, Igor Mordatch,
    Aravind Rajeswaran, Pieter Abbeel, and Michael Laskin. Semi-supervised offline
    reinforcement learning with pre-trained decision transformers, 2022. URL [https://openreview.net/forum?id=fwJWhOxuzV9](https://openreview.net/forum?id=fwJWhOxuzV9).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang et al. (2018) Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert,
    and Elliot Holtham. Reversible architectures for arbitrarily deep residual neural
    networks. In *Proceedings of the 32nd AAAI Conference on Artificial Intelligence*,
    pp.  2811–2818\. AAAI Press, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chang et al. (2022) Wei-Di Chang, Juan Camilo Gamboa Higuera, Scott Fujimoto,
    David Meger, and Gregory Dudek. Il-flow: Imitation learning from observation using
    normalizing flows. *arXiv preprint arXiv:2205.09251*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chebotar et al. (2023) Yevgen Chebotar, Quan Vuong, Karol Hausman, Fei Xia,
    Yao Lu, Alex Irpan, Aviral Kumar, Tianhe Yu, Alexander Herzog, Karl Pertsch, et al.
    Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions.
    In *Conference on Robot Learning*, pp.  3909–3928\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Transdreamer:
    Reinforcement learning with transformer world models. *CoRR*, abs/2202.09481,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018a) Changyou Chen, Chunyuan Li, Liquan Chen, Wenlin Wang, Yunchen
    Pu, and Lawrence Carin. Continuous-time flows for efficient inference and density
    estimation. In *Proceedings of the 35th International Conference on Machine Learning*,
    volume 80, pp.  823–832\. PMLR, 2018a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021a) Cynthia Chen, Xin Chen, Sam Toyer, Cody Wild, Scott Emmons,
    Ian Fischer, Kuang-Huei Lee, Neel Alex, Steven H. Wang, Ping Luo, Stuart Russell,
    Pieter Abbeel, and Rohin Shah. An empirical investigation of representation learning
    for imitation. In *Proceedings of the Neural Information Processing Systems Track
    on Datasets and Benchmarks 1*, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023a) Daoming Chen, Ning Wang, Feng Chen, and Tony Pipe. Detrive:
    Imitation learning with transformer detection for end-to-end autonomous driving.
    *CoRR*, abs/2310.14224, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023b) Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu.
    Offline reinforcement learning via high-fidelity generative behavior modeling.
    In *Proceedings of the 11th International Conference on Learning Representations*.
    OpenReview.net, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023c) Jiayu Chen, Tian Lan, and Vaneet Aggarwal. Hierarchical
    adversarial inverse reinforcement learning. *IEEE Transactions on Neural Networks
    and Learning Systems*, 2023c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023d) Jiayu Chen, Dipesh Tamboli, Tian Lan, and Vaneet Aggarwal.
    Multi-task hierarchical adversarial inverse reinforcement learning. In *Proceedings
    of the 40th International Conference on Machine Learning*, volume 202, pp.  4895–4920,
    2023d.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021b) Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya
    Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision
    transformer: Reinforcement learning via sequence modeling. *Advances in neural
    information processing systems*, 34:15084–15097, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018b) Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David
    Duvenaud. Neural ordinary differential equations. In *Advances in Neural Information
    Processing Systems 31*, pp.  6572–6583, 2018b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019) Tian Qi Chen, Jens Behrmann, David Duvenaud, and Jörn-Henrik
    Jacobsen. Residual flows for invertible generative modeling. In *Advances in Neural
    Information Processing Systems 32*, pp.  9913–9923, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021c) Valerie Chen, Abhinav Gupta, and Kenneth Marino. Ask your
    humans: Using human instructions to improve generalization in reinforcement learning.
    In *Proceedings of the 9th International Conference on Learning Representations*,
    2021c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2016) Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever,
    and Pieter Abbeel. Infogan: Interpretable representation learning by information
    maximizing generative adversarial nets. *Advances in neural information processing
    systems*, 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023e) Yuhui Chen, Haoran Li, and Dongbin Zhao. Boosting continuous
    control with consistency policy. *CoRR*, abs/2310.06343, 2023e.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chevalier-Boisvert et al. (2018) Maxime Chevalier-Boisvert, Lucas Willems, and
    Suman Pal. Minimalistic gridworld environment for openai gym. [https://github.com/maximecb/gym-minigrid](https://github.com/maximecb/gym-minigrid),
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chevalier-Boisvert et al. (2019) Maxime Chevalier-Boisvert, Dzmitry Bahdanau,
    Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio.
    Babyai: First steps towards grounded language learning with a human in the loop.
    In *International Conference on Learning Representations*, volume 105, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chi et al. (2023) Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau,
    Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning
    via action diffusion. In *Robotics: Science and Systems*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    Generating long sequences with sparse transformers. *CoRR*, abs/1904.10509, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cho et al. (2022) Daesol Cho, Dongseok Shim, and H. Jin Kim. S2P: state-conditioned
    image synthesis for data augmentation in offline reinforcement learning. In *Advances
    in Neural Information Processing Systems 35*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chou & Tedrake (2023) Glen Chou and Russ Tedrake. Synthesizing stable reduced-order
    visuomotor policies for nonlinear systems via sums-of-squares optimization. *CoRR*,
    abs/2304.12405, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2015) Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel,
    Aaron C. Courville, and Yoshua Bengio. A recurrent latent variable model for sequential
    data. In *Advances in Neural Information Processing Systems 28*, pp.  2980–2988,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2020) Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman.
    Leveraging procedural generation to benchmark reinforcement learning. In *Proceedings
    of the 37th International Conference on Machine Learning*, volume 119, pp.  2048–2056,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coleman et al. (2023) Matthew Coleman, Olga Russakovsky, Christine Allen-Blanchette,
    and Ye Zhu. Discrete diffusion reward guidance methods for offline reinforcement
    learning. In *ICML 2023 Workshop: Sampling and Optimization in Discrete Space*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cornish et al. (2020) Robert Cornish, Anthony L. Caterini, George Deligiannidis,
    and Arnaud Doucet. Relaxing bijectivity constraints with continuously indexed
    normalising flows. In *Proceedings of the 37th International Conference on Machine
    Learning*, volume 119, pp.  2133–2143\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correia & Alexandre (2022) André Correia and Luís A. Alexandre. Hierarchical
    decision transformer. *CoRR*, abs/2209.10447, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coumans & Bai (2016–2021) Erwin Coumans and Yunfei Bai. Pybullet, a python module
    for physics simulation for games, robotics and machine learning. [http://pybullet.org](http://pybullet.org),
    2016–2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Csiszár & Shields (2004) Imre Csiszár and Paul C. Shields. Information theory
    and statistics: A tutorial. *Foundations and Trends in Communications and Information
    Theory*, 1(4), 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dance et al. (2021) Christopher R. Dance, Julien Perez, and Théo Cachet. Demonstration-conditioned
    reinforcement learning for few-shot imitation. In *Proceedings of the 38th International
    Conference on Machine Learning*, volume 139, pp.  2376–2387, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dasari & Gupta (2020) Sudeep Dasari and Abhinav Gupta. Transformers for one-shot
    visual imitation. In *Proceedings of the 4th Conference on Robot Learning*, volume
    155, pp.  2071–2084, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Davis et al. (2021) Jared Quincy Davis, Albert Gu, Krzysztof Choromanski, Tri
    Dao, Christopher Ré, Chelsea Finn, and Percy Liang. Catformer: Designing stable
    transformers via sensitivity analysis. In *Proceedings of the 38th International
    Conference on Machine Learning*, volume 139, pp.  2489–2499, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: De Haan et al. (2019) Pim De Haan, Dinesh Jayaraman, and Sergey Levine. Causal
    confusion in imitation learning. *Advances in Neural Information Processing Systems*,
    32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devin (2024) Coline Devin. craftingworld. [https://github.com/cdevin/craftingworld](https://github.com/cdevin/craftingworld),
    2024. Accessed: 2024-02-05.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dhariwal & Nichol (2021) Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion
    models beat gans on image synthesis. In *Advances in Neural Information Processing
    Systems 34*, pp.  8780–8794, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding & Jin (2023) Zihan Ding and Chi Jin. Consistency models as a rich and efficient
    policy class for reinforcement learning. *CoRR*, abs/2309.16984, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dinh et al. (2015) Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear
    independent components estimation. In *3rd International Conference on Learning
    Representations, Workshop Track Proceedings*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dinh et al. (2017) Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density
    estimation using real NVP. In *Proceedings of the 5th International Conference
    on Learning Representations*. OpenReview.net, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2023) Wenbo Dong, Shaofan Liu, and Shiliang Sun. Safe batch constrained
    deep reinforcement learning with generative adversarial network. *Information
    Science*, 634:259–270, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2021) Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
    Attention is not all you need: pure attention loses rank doubly exponentially
    with depth. In *Proceedings of the 38th International Conference on Machine Learning*,
    volume 139, pp.  2793–2803, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Donsker & Varadhan (1976) Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic
    evaluation of certain markov process expectations for large time—iii. *Communications
    on pure and applied Mathematics*, 29(4):389–461, 1976.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dorfman et al. (2021) Ron Dorfman, Idan Shenfeld, and Aviv Tamar. Offline meta
    reinforcement learning - identifiability challenges and effective data collection
    strategies. In *Advances in Neural Information Processing Systems 34*, pp.  4607–4618,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2017) Alexey Dosovitskiy, Germán Ros, Felipe Codevilla,
    Antonio M. López, and Vladlen Koltun. CARLA: an open urban driving simulator.
    In *Proceedings of the 1st Annual Conference on Robot Learning*, volume 78, pp. 
    1–16, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2023) Maximilian Du, Suraj Nair, Dorsa Sadigh, and Chelsea Finn.
    Behavior retrieval: Few-shot imitation learning by querying unlabeled datasets.
    In *Robotics: Science and Systems XIX*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2022) Yuqing Du, Daniel Ho, Alex Alemi, Eric Jang, and Mohi Khansari.
    Bayesian imitation learning for end-to-end mobile manipulation. In *Proceedings
    of the 39th International Conference on Machine Learning*, volume 162 of *Proceedings
    of Machine Learning Research*, pp.  5531–5546\. PMLR, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dua et al. (2019) Dheeru Dua, Casey Graff, et al. Uci machine learning repository,
    2017. *URL http://archive. ics. uci. edu/ml*, 7(1), 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duffie & Pan (1997) Darrell Duffie and Jun Pan. An overview of value at risk.
    *Journal of derivatives*, 4(3):7–49, 1997.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dupont et al. (2019) Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented
    neural odes. In *Advances in Neural Information Processing Systems 32*, pp.  3134–3144,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Durkan et al. (2019) Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios.
    Neural spline flows. *Advances in neural information processing systems*, 32,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Emmons et al. (2022) Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and
    Sergey Levine. Rvs: What is essential for offline RL via supervised learning?
    In *Proceedings of the 10th International Conference on Learning Representations*.
    OpenReview.net, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Esser et al. (2021) Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers
    for high-resolution image synthesis. In *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*, pp.  12873–12883, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eysenbach et al. (2019) Ben Eysenbach, Ruslan Salakhutdinov, and Sergey Levine.
    Search on the replay buffer: Bridging planning and reinforcement learning. In
    *Advances in Neural Information Processing Systems 32*, pp.  15220–15231, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang et al. (2022) Kuan Fang, Patrick Yin, Ashvin Nair, Homer Walke, Gengchen
    Yan, and Sergey Levine. Generalization with lossy affordances: Leveraging broad
    offline data for learning visuomotor tasks. In *Conference on Robot Learning*,
    volume 205 of *Proceedings of Machine Learning Research*, pp.  106–117\. PMLR,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fei et al. (2020) Cong Fei, Bin Wang, Yuzheng Zhuang, Zongzhang Zhang, Jianye
    Hao, Hongbo Zhang, Xuewu Ji, and Wulong Liu. Triple-gail: A multi-modal imitation
    learning framework with generative adversarial nets. In *Proceedings of the 29th
    International Joint Conference on Artificial Intelligence*, pp.  2929–2935, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Felsen et al. (2018) Panna Felsen, Patrick Lucey, and Sujoy Ganguly. Where will
    they go? predicting fine-grained adversarial multi-agent motion using conditional
    variational autoencoders. In *Proceedings of the 15th European Conference on Computer
    Vision*, volume 11215, pp.  761–776\. Springer, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Florence et al. (2021) Pete Florence, Corey Lynch, Andy Zeng, Oscar A. Ramirez,
    Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan
    Tompson. Implicit behavioral cloning. In *Conference on Robot Learning*, volume
    164, pp.  158–168, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Florence et al. (2022) Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez,
    Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan
    Tompson. Implicit behavioral cloning. In *Conference on Robot Learning*, pp. 
    158–168\. PMLR, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foundation (2023) Farama Foundation. Frozen lake environment. [https://gymnasium.farama.org/environments/toy_text/frozen_lake/](https://gymnasium.farama.org/environments/toy_text/frozen_lake/),
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fragkiadaki et al. (2016) Katerina Fragkiadaki, Pulkit Agrawal, Sergey Levine,
    and Jitendra Malik. Learning visual predictive models of physics for playing billiards.
    In *proceedings of the 4th International Conference on Learning Representations*,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Freitag & Al-Onaizan (2017) Markus Freitag and Yaser Al-Onaizan. Beam search
    strategies for neural machine translation. *arXiv preprint arXiv:1702.01806*,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Freund et al. (2023) Gideon Joseph Freund, Elad Sarafian, and Sarit Kraus. A
    coupled flow approach to imitation learning. In *Proceedings of the 40th International
    Conference on Machine Learning*, pp.  10357–10372\. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2023) Huiqiao Fu, Kaiqiang Tang, Yuanyang Lu, Yiming Qi, Guizhou
    Deng, Flood Sung, and Chunlin Chen. Ess-infogail: Semi-supervised imitation learning
    from imbalanced demonstrations. In *Advances in Neural Information Processing
    Systems 36*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2017) Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards
    with adversarial inverse reinforcement learning. *CoRR*, abs/1710.11248, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2020) Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey
    Levine. D4RL: datasets for deep data-driven reinforcement learning. *CoRR*, abs/2004.07219,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fujimoto et al. (2019) Scott Fujimoto, David Meger, and Doina Precup. Off-policy
    deep reinforcement learning without exploration. In *Proceedings of the 36th International
    Conference on Machine Learning*, volume 97, pp.  2052–2062\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furuta et al. (2022) Hiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized
    decision transformer for offline hindsight information matching. In *Proceedings
    of the 10th International Conference on Learning Representations*. OpenReview.net,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghasemipour et al. (2019a) Seyed Kamyar Seyed Ghasemipour, Shixiang Gu, and
    Richard S. Zemel. Smile: Scalable meta inverse reinforcement learning through
    context-conditional policies. In *Advances in Neural Information Processing Systems
    32*, pp.  7879–7889, 2019a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghasemipour et al. (2019b) Seyed Kamyar Seyed Ghasemipour, Richard S. Zemel,
    and Shixiang Gu. A divergence minimization perspective on imitation learning methods.
    In *Proceedings of the 3rd Annual Conference on Robot Learning*, volume 100, pp. 
    1259–1277\. PMLR, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gomez et al. (2017) Aidan N. Gomez, Mengye Ren, Raquel Urtasun, and Roger B.
    Grosse. The reversible residual network: Backpropagation without storing activations.
    In *Advances in Neural Information Processing Systems 30*, pp.  2214–2224, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow (2017) Ian Goodfellow. Nips 2016 tutorial: Generative adversarial
    networks, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014a) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    adversarial nets. *Advances in neural information processing systems*, 27, 2014a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014b) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio.
    Generative adversarial nets. In *Advances in Neural Information Processing Systems
    27*, pp.  2672–2680, 2014b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grathwohl et al. (2018) Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya
    Sutskever, and David Duvenaud. Ffjord: Free-form continuous dynamics for scalable
    reversible generative models. *arXiv preprint arXiv:1810.01367*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gretton et al. (2012) Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch,
    Bernhard Schölkopf, and Alexander J. Smola. A kernel two-sample test. *Journal
    of Machine Learning Research*, 13:723–773, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gronauer (2022) Sven Gronauer. Bullet-safety-gym: A framework for constrained
    reinforcement learning. Technical report, mediaTUM, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grover et al. (2018) Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-gan:
    Combining maximum likelihood and adversarial learning in generative models. In
    *Proceedings of the 32nd AAAI Conference on Artificial Intelligence*, pp.  3069–3076\.
    AAAI Press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu & Dao (2023) Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling
    with selective state spaces. *CoRR*, abs/2312.00752, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guan et al. (2023) Jiayi Guan, Shangding Gu, Zhijun Li, Jing Hou, Yiqin Yang,
    Guang Chen, and Changjun Jiang. Uac: Offline reinforcement learning with uncertain
    action constraint. *IEEE Transactions on Cognitive and Developmental Systems*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. (2019a) Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine,
    and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation
    and reinforcement learning. In *Conference on Robot Learning*, volume 100, pp. 
    1025–1037, 2019a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. (2019b) Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine,
    and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation
    and reinforcement learning. *arXiv preprint arXiv:1910.11956*, 2019b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guss et al. (2019) William H. Guss, Brandon Houghton, Nicholay Topin, Phillip
    Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl: A large-scale
    dataset of minecraft demonstrations. In *Proceedings of the 28th International
    Joint Conference on Artificial Intelligence*, pp.  2442–2448\. ijcai.org, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haarnoja et al. (2018) Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey
    Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning
    with a stochastic actor. In *Proceedings of the 35th International Conference
    on Machine Learning*, volume 80 of *Proceedings of Machine Learning Research*,
    pp.  1856–1865\. PMLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hafner et al. (2020) Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad
    Norouzi. Dream to control: Learning behaviors by latent imagination. In *Proceedings
    of the 8th International Conference on Learning Representations*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hambidge (1967) Jay Hambidge. *The elements of dynamic symmetry*. Courier Corporation,
    1967.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han & Kim (2022) Jungwoo Han and Jinwhan Kim. Selective data augmentation for
    improving the performance of offline reinforcement learning. In *International
    Conference on Control, Automation and Systems (ICCAS)*, pp.  222–226\. IEEE, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2022) Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan
    Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. A survey
    on vision transformer. *IEEE transactions on pattern analysis and machine intelligence*,
    45(1):87–110, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hansen-Estruch et al. (2023) Philippe Hansen-Estruch, Ilya Kostrikov, Michael
    Janner, Jakub Grudzien Kuba, and Sergey Levine. IDQL: implicit q-learning as an
    actor-critic method with diffusion policies. *CoRR*, abs/2304.10573, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hao et al. (2019) Xiaotian Hao, Weixun Wang, Jianye Hao, and Yaodong Yang. Independent
    generative adversarial self-imitation learning in cooperative multiagent systems.
    In *Proceedings of the 18th International Conference on Autonomous Agents and
    MultiAgent Systems*, pp.  1315–1323, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hausman et al. (2017) Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav S.
    Sukhatme, and Joseph J. Lim. Multi-modal imitation learning from unstructured
    demonstrations using generative adversarial nets. In *Advances in Neural Information
    Processing Systems 30*, pp.  1235–1245, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2023a) Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang,
    Dong Wang, Bin Zhao, and Xuelong Li. Diffusion model is an effective planner and
    data synthesizer for multi-task reinforcement learning. *CoRR*, abs/2305.18459,
    2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *2016 IEEE Conference on Computer
    Vision and Pattern Recognition*, pp.  770–778\. IEEE Computer Society, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2023b) Longxiang He, Linrui Zhang, Junbo Tan, and Xueqian Wang.
    Diffcps: Diffusion model based constrained policy search for offline reinforcement
    learning. *CoRR*, abs/2310.05333, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hejna et al. (2023) Joey Hejna, Pieter Abbeel, and Lerrel Pinto. Improving long-horizon
    imitation through instruction prediction. In *Proceedings of the 37th AAAI Conference
    on Artificial Intelligence*, pp.  7857–7865, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks & Gimpel (2023) Dan Hendrycks and Kevin Gimpel. Gaussian error linear
    units (gelus), 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hepburn & Montana (2022) Charles A. Hepburn and Giovanni Montana. Model-based
    trajectory stitching for improved offline reinforcement learning. *CoRR*, abs/2211.11603,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Higgins et al. (2017) Irina Higgins, Loïc Matthey, Arka Pal, Christopher P.
    Burgess, Xavier Glorot, Matthew M. Botvinick, Shakir Mohamed, and Alexander Lerchner.
    beta-vae: Learning basic visual concepts with a constrained variational framework.
    In *Proceedings of the 5th International Conference on Learning Representations*.
    OpenReview.net, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hiriart-Urruty & Lemaréchal (2004) Jean-Baptiste Hiriart-Urruty and Claude Lemaréchal.
    *Fundamentals of convex analysis*. Springer Science & Business Media, 2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho & Ermon (2016) Jonathan Ho and Stefano Ermon. Generative adversarial imitation
    learning. In *Advances in Neural Information Processing Systems 29*, pp.  4565–4573,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho & Salimans (2022) Jonathan Ho and Tim Salimans. Classifier-free diffusion
    guidance. *CoRR*, abs/2207.12598, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ho et al. (2019) Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter
    Abbeel. Flow++: Improving flow-based generative models with variational dequantization
    and architecture design. In *Proceedings of the 36th International Conference
    on Machine Learning*, volume 97, pp.  2722–2730\. PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion
    probabilistic models. *Advances in neural information processing systems*, 33:6840–6851,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hoogeboom et al. (2021) Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick
    Forré, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical
    distributions. *Advances in Neural Information Processing Systems*, 34:12454–12465,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for NLP. In *Proceedings of the 36th
    International Conference on Machine Learning*, volume 97, pp.  2790–2799, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2023a) Jifeng Hu, Yanchao Sun, Sili Huang, Siyuan Guo, Hechang Chen,
    Li Shen, Lichao Sun, Yi Chang, and Dacheng Tao. Instructed diffuser with temporal
    condition guidance for offline reinforcement learning. *CoRR*, abs/2306.04875,
    2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2023b) Shengchao Hu, Li Shen, Ya Zhang, and Dacheng Tao. Graph decision
    transformer. *CoRR*, abs/2303.03747, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Siyi Hu, Fengda Zhu, Xiaojun Chang, and Xiaodan Liang. Updet:
    Universal multi-agent rl via policy decoupling with transformers. In *Proceedings
    of the 9th International Conference on Learning Representations*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2018) Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron C.
    Courville. Neural autoregressive flows. In *Proceedings of the 35th International
    Conference on Machine Learning*, volume 80 of *Proceedings of Machine Learning
    Research*, pp.  2083–2092\. PMLR, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hwang et al. (2023) Dongyoon Hwang, Minho Park, and Jiyoung Lee. Sample generations
    for reinforcement learning via diffusion models. *OpenReview.net*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyvärinen (2005) Aapo Hyvärinen. Estimation of non-normalized statistical models
    by score matching. *Journal of Machine Learning Research*, 6:695–709, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jacobsen et al. (2018) Jörn-Henrik Jacobsen, Arnold W. M. Smeulders, and Edouard
    Oyallon. i-revnet: Deep invertible networks. In *Proceedings of the 6th International
    Conference on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaegle et al. (2022) Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac,
    Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew
    Brock, Evan Shelhamer, Olivier J. Hénaff, Matthew M. Botvinick, Andrew Zisserman,
    Oriol Vinyals, and João Carreira. Perceiver IO: A general architecture for structured
    inputs & outputs. In *Proceedings of the 10th International Conference on Learning
    Representations*. OpenReview.net, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'James et al. (2020) Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J.
    Davison. Rlbench: The robot learning benchmark & learning environment. *IEEE Robotics
    Automation Letters*, 5(2):3019–3026, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Janner et al. (2019) Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.
    When to trust your model: Model-based policy optimization. In *Advances in Neural
    Information Processing Systems 32*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Janner et al. (2021) Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement
    learning as one big sequence modeling problem. In *Advances in Neural Information
    Processing Systems 34*, pp.  1273–1286, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Janner et al. (2022) Michael Janner, Yilun Du, Joshua B. Tenenbaum, and Sergey
    Levine. Planning with diffusion for flexible behavior synthesis. In *Proceedings
    of the 39th International Conference on Machine Learning*, volume 162, pp.  9902–9915\.
    PMLR, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeon et al. (2018) Wonseok Jeon, Seokin Seo, and Kee-Eung Kim. A bayesian approach
    to generative adversarial imitation learning. In *Advances in Neural Information
    Processing Systems 31*, pp.  7440–7450, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2021) Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan:
    Two pure transformers can make one strong gan, and that can scale up. *Advances
    in Neural Information Processing Systems*, 34:14745–14758, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jing et al. (2021) Mingxuan Jing, Wenbing Huang, Fuchun Sun, Xiaojian Ma, Tao
    Kong, Chuang Gan, and Lei Li. Adversarial option-aware hierarchical imitation
    learning. In *Proceedings of the 38th International Conference on Machine Learning*,
    volume 139, pp.  5097–5106\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jolicoeur-Martineau et al. (2021) Alexia Jolicoeur-Martineau, Rémi Piché-Taillefer,
    Ioannis Mitliagkas, and Remi Tachet des Combes. Adversarial score matching and
    improved sampling for image generation. In *Proceedings of the 9th International
    Conference on Learning Representations*. OpenReview.net, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kalyan et al. (2021) Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, and
    Sivanesan Sangeetha. AMMUS : A survey of transformer-based pretrained models in
    natural language processing. *CoRR*, abs/2108.05542, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kamath et al. (2023) Aishwarya Kamath, Peter Anderson, Su Wang, Jing Yu Koh,
    Alexander Ku, Austin Waters, Yinfei Yang, Jason Baldridge, and Zarana Parekh.
    A new path: Scaling vision-and-language navigation with synthetic instructions
    and imitation learning. In *IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pp.  10813–10823, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang et al. (2023) Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng
    Yan. Efficient diffusion policies for offline reinforcement learning. *CoRR*,
    abs/2305.20081, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karras et al. (2022) Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
    Elucidating the design space of diffusion-based generative models. In *Advances
    in neural information processing systems 35*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khan et al. (2022) Muhammad Junaid Khan, Syed Hammad Ahmed, and Gita Sukthankar.
    Transformer-based value function decomposition for cooperative multi-agent reinforcement
    learning in starcraft. In *Proceedings of the 18th AAAI Conference on Artificial
    Intelligence and Interactive Digital Entertainment*, pp.  113–119\. AAAI Press,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2021) Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi. Transformer-based
    deep imitation learning for dual-arm robot manipulation. In *IEEE/RSJ International
    Conference on Intelligent Robots and Systems*, pp.  8965–8972, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2022) Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi. Memory-based
    gaze prediction in deep imitation learning for robot manipulation. In *International
    Conference on Robotics and Automation*, pp.  2427–2433\. IEEE, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2023) Heecheol Kim, Yoshiyuki Ohmura, Akihiko Nagakubo, and Yasuo
    Kuniyoshi. Training robots without robots: Deep imitation learning for master-to-robot
    policy transfer. *IEEE Robotics Automation Letters*, 8(5):2906–2913, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma & Gao (2023) Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives
    as the elbo with simple data augmentation. *Advances in Neural Information Processing
    Systems 36*, 36, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma & Dhariwal (2018) Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative
    flow with invertible 1x1 convolutions. In *Advances in Neural Information Processing
    Systems 31*, pp.  10236–10245, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma & Welling (2014) Diederik P. Kingma and Max Welling. Auto-encoding variational
    bayes. In *Proceedings of the 2nd International Conference on Learning Representations*,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma & Welling (2019) Diederik P. Kingma and Max Welling. An introduction
    to variational autoencoders. *Foundations and Trends® in Machine Learning*, 12(4):307–392,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma et al. (2016) Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen,
    Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive
    flow. *Advances in neural information processing systems*, 29, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kipf et al. (2019) Thomas Kipf, Yujia Li, Hanjun Dai, Vinícius Flores Zambaldi,
    Alvaro Sanchez-Gonzalez, Edward Grefenstette, Pushmeet Kohli, and Peter W. Battaglia.
    Compile: Compositional imitation learning and execution. In *Proceedings of the
    36th International Conference on Machine Learning*, volume 97 of *Proceedings
    of Machine Learning Research*, pp.  3418–3428\. PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kobyzev et al. (2021) Ivan Kobyzev, Simon J. D. Prince, and Marcus A. Brubaker.
    Normalizing flows: An introduction and review of current methods. *IEEE TRANSACTIONS
    ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE*, 43(11):3964–3979, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koga et al. (2022) Yotto Koga, Heather Kerrick, and Sachin Chitta. On CAD informed
    adaptive robotic assembly. In *IEEE/RSJ International Conference on Intelligent
    Robots and Systems*, pp.  10207–10214\. IEEE, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kostrikov et al. (2019) Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi,
    Sergey Levine, and Jonathan Tompson. Discriminator-actor-critic: Addressing sample
    inefficiency and reward bias in adversarial imitation learning. In *Proceedings
    of the 7th International Conference on Learning Representations*. OpenReview.net,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kostrikov et al. (2022) Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline
    reinforcement learning with implicit q-learning. In *Proceedings of the 10th International
    Conference on Learning Representations*. OpenReview.net, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krajewski et al. (2018) Robert Krajewski, Julian Bock, Laurent Kloeker, and
    Lutz Eckstein. The highd dataset: A drone dataset of naturalistic vehicle trajectories
    on german highways for validation of highly automated driving systems. In *Proceedings
    of the 21st International Conference on Intelligent Transportation Systems*, pp. 
    2118–2125\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kuefler & Kochenderfer (2018) Alex Kuefler and Mykel J. Kochenderfer. Burn-in
    demonstrations for multi-modal imitation learning. In *Proceedings of the 17th
    International Conference on Autonomous Agents and MultiAgent Systems*, pp.  1071–1078,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. (2019) Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and
    Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction.
    In *Advances in Neural Information Processing Systems 32*, pp.  11761–11771, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. (2020) Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
    Conservative q-learning for offline reinforcement learning. In *Advances in Neural
    Information Processing Systems 33*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lacotte et al. (2019) Jonathan Lacotte, Mohammad Ghavamzadeh, Yinlam Chow, and
    Marco Pavone. Risk-sensitive generative adversarial imitation learning. In *Proceedings
    of the 22nd International Conference on Artificial Intelligence and Statistics*,
    volume 89, pp.  2154–2163, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2022) Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel
    Freeman, Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski,
    and Igor Mordatch. Multi-game decision transformers. In *Advances in Neural Information
    Processing Systems 35*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2019) Su-Jin Lee, Tae Yoon Chun, Hyoung Woo Lim, and Sang-Ho Lee.
    Path tracking control using imitation learning with variational auto-encoder.
    In *Proceedings of the 19th International Conference on Control, Automation and
    Systems*, pp.  501–505\. IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levine (2018) Sergey Levine. Reinforcement learning and control as probabilistic
    inference: Tutorial and review. *arXiv preprint arXiv:1805.00909*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin
    Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open
    problems. *CoRR*, abs/2005.01643, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017a) Chongxuan Li, Taufik Xu, Jun Zhu, and Bo Zhang. Triple generative
    adversarial nets. In *Advances in Neural Information Processing Systems 30*, pp. 
    4088–4098, 2017a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) Jinning Li, Chen Tang, Masayoshi Tomizuka, and Wei Zhan. Hierarchical
    planning through goal-conditioned offline reinforcement learning. *IEEE Robotics
    Automation Letters*, 7(4):10216–10223, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023a) Tao Li, Juan Guevara, Xinghong Xie, and Quanyan Zhu. Self-confirming
    transformer for locally consistent online adaptation in multi-agent reinforcement
    learning. *CoRR*, abs/2310.04579, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023b) Wenhao Li, Xiangfeng Wang, Bo Jin, and Hongyuan Zha. Hierarchical
    diffusion for offline decision making. In *Proceedings of the 40th International
    Conference on Machine Learning*, volume 202, pp.  20035–20064, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017b) Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable
    imitation learning from visual demonstrations. In *Advances in Neural Information
    Processing Systems 30*, pp.  3812–3822, 2017b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023c) Zhuoran Li, Ling Pan, and Longbo Huang. Beyond conservatism:
    Diffusion policies in offline multi-agent reinforcement learning. *CoRR*, abs/2307.01472,
    2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2022) Anthony Liang, Ishika Singh, Karl Pertsch, and Jesse Thomason.
    Transformer adapters for robot learning. In *CoRL 2022 Workshop on Pre-training
    Robot Learning*, 2022. URL [https://openreview.net/forum?id=H--wvRYBmF](https://openreview.net/forum?id=H--wvRYBmF).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2023a) Hebin Liang, Zibin Dong, Yi Ma, Xiaotian Hao, Yan Zheng,
    and Jianye Hao. A hierarchical imitation learning-based decision framework for
    autonomous driving. In *Proceedings of the 32nd ACM International Conference on
    Information and Knowledge Management*, pp.  4695–4701\. ACM, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2023b) Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi
    Tomizuka, and Ping Luo. Adaptdiffuser: Diffusion models as adaptive self-evolving
    planners. In *Proceedings of the 40th International Conference on Machine Learning*,
    volume 202, pp.  20725–20745, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lillicrap et al. (2016) Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel,
    Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous
    control with deep reinforcement learning. In *Proceedings of the 4th International
    Conference on Learning Representations*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2022a) Qinjie Lin, Han Liu, and Biswa Sengupta. Switch trajectory
    transformer with distributional value approximation for multi-task reinforcement
    learning. *CoRR*, abs/2203.07413, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2022b) Runji Lin, Ye Li, Xidong Feng, Zhaowei Zhang, Xian Hong Wu
    Fung, Haifeng Zhang, Jun Wang, Yali Du, and Yaodong Yang. Contextual transformer
    for offline meta reinforcement learning. *CoRR*, abs/2211.08016, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2022c) Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu.
    A survey of transformers. *AI Open*, 2022c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020) Fangchen Liu, Zhan Ling, Tongzhou Mu, and Hao Su. State alignment-based
    imitation learning. In *Proceedings of the 8th International Conference on Learning
    Representations*. OpenReview.net, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang,
    Jing Zhang, and Jie Tang. Self-supervised learning: Generative or contrastive.
    *IEEE Transactions on Knowledge and Data Engineering*, 35(1):857–876, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023b) Zuxin Liu, Zijian Guo, Yihang Yao, Zhepeng Cen, Wenhao Yu,
    Tingnan Zhang, and Ding Zhao. Constrained decision transformer for offline safe
    reinforcement learning. In *Proceedings of the 40th International Conference on
    Machine Learning*, volume 202, pp.  21611–21630, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locatello et al. (2019) Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar
    Rätsch, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. Challenging common
    assumptions in the unsupervised learning of disentangled representations. In *Proceedings
    of the 36th International Conference on Machine Learning*, volume 97, pp.  4114–4124,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lowe et al. (2017) Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and
    Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments.
    In *Advances in Neural Information Processing Systems 30*, pp.  6379–6390, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loynd et al. (2020) Ricky Loynd, Roland Fernandez, Asli Celikyilmaz, Adith Swaminathan,
    and Matthew J. Hausknecht. Working memory graphs. In *Proceedings of the 37th
    International Conference on Machine Learning*, volume 119, pp.  6404–6414, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2022) Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li,
    and Jun Zhu. Dpm-solver: A fast ODE solver for diffusion probabilistic model sampling
    in around 10 steps. In *Advances in Neural Information Processing Systems 35*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. (2023) Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li,
    and Jun Zhu. Contrastive energy prediction for exact energy-guided diffusion sampling
    in offline reinforcement learning. In *Proceedings of the 40th International Conference
    on Machine Learning*, volume 202, pp.  22825–22855\. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. (2019) Xiaoyu Lu, Jan Stuehmer, and Katja Hofmann. Trajectory VAE
    for multi-modal imitation, 2019. URL [https://openreview.net/forum?id=Byx1VnR9K7](https://openreview.net/forum?id=Byx1VnR9K7).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu & Tompson (2020) Yiren Lu and Jonathan Tompson. ADAIL: adaptive adversarial
    imitation learning. *CoRR*, abs/2008.12647, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2023) Jianlan Luo, Perry Dong, Jeffrey Wu, Aviral Kumar, Xinyang
    Geng, and Sergey Levine. Action-quantized offline reinforcement learning for robotic
    skill learning. In *Conference on Robot Learning*, pp.  1348–1361\. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lynch & Sermanet (2021) Corey Lynch and Pierre Sermanet. Language conditioned
    imitation learning over unstructured data. In *Robotics: Science and Systems XVII,
    Virtual Event, July 12-16, 2021*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lynch et al. (2019) Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan
    Tompson, Sergey Levine, and Pierre Sermanet. Learning latent plans from play.
    In *Proceedings of the 3rd Annual Conference on Robot Learning*, volume 100, pp. 
    1113–1132, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lyu et al. (2022) Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative
    q-learning for offline reinforcement learning. In *Advances in Neural Information
    Processing Systems 35*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mandlekar et al. (2021a) Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany,
    Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto
    Martín-Martín. What matters in learning from offline human demonstrations for
    robot manipulation. In *Conference on Robot Learning*, volume 164, pp.  1678–1690,
    2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mandlekar et al. (2021b) Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany,
    Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto
    Martín-Martín. What matters in learning from offline human demonstrations for
    robot manipulation. *arXiv preprint arXiv:2108.03298*, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manuelli et al. (2020) Lucas Manuelli, Yunzhu Li, Peter R. Florence, and Russ
    Tedrake. Keypoints into the future: Self-supervised correspondence in model-based
    reinforcement learning. In *Proceedings of the 4th Conference on Robot Learning*,
    volume 155, pp.  693–710, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mao et al. (2017) Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, Zhen Wang,
    and Stephen Paul Smolley. Least squares generative adversarial networks. In *IEEE
    International Conference on Computer Vision*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Margolis & Agrawal (2022) Gabriel Margolis and Pulkit Agrawal. Walk these ways:
    Gait-conditioned policies yield diversified quadrupedal agility. In *Conference
    on Robot Learning*, volume 1, pp.  2, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Masoudnia & Ebrahimpour (2014) Saeed Masoudnia and Reza Ebrahimpour. Mixture
    of experts: a literature survey. *Artificial Intelligence Review*, 42:275–293,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Massey et al. (1990) James Massey et al. Causality, feedback and directed information.
    In *The International Symposium on Information Theory and Its Applications*, pp. 
    303–305, 1990.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mayne & Michalska (1988) David Q Mayne and Hannah Michalska. Receding horizon
    control of nonlinear systems. In *Proceedings of the 27th IEEE Conference on Decision
    and Control*, pp.  464–465\. IEEE, 1988.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mazoure et al. (2019) Bogdan Mazoure, Thang Doan, Audrey Durand, Joelle Pineau,
    and R. Devon Hjelm. Leveraging exploration in off-policy algorithms via normalizing
    flows. In *Proceedings of the 3rd Annual Conference on Robot Learning*, volume
    100 of *Proceedings of Machine Learning Research*, pp.  430–444\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mees et al. (2022a) Oier Mees, Lukás Hermann, and Wolfram Burgard. What matters
    in language conditioned robotic imitation learning over unstructured data. *IEEE
    Robotics Automation Letters*, 7(4):11205–11212, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mees et al. (2022b) Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram
    Burgard. Calvin: A benchmark for language-conditioned policy learning for long-horizon
    robot manipulation tasks. *IEEE Robotics and Automation Letters*, 7(3):7327–7334,
    2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Menéndez et al. (1997) ML Menéndez, JA Pardo, L Pardo, and MC Pardo. The jensen-shannon
    divergence. *Journal of the Franklin Institute*, 334(2):307–318, 1997.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meng et al. (2021) Linghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun
    Li, Weinan Zhang, Ying Wen, Haifeng Zhang, Jun Wang, and Bo Xu. Offline pre-trained
    multi-agent decision transformer: One big sequence model tackles all SMAC tasks.
    *CoRR*, abs/2112.02845, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meyer et al. (2019) Gregory P Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-Gonzalez,
    and Carl K Wellington. Lasernet: An efficient probabilistic 3d object detector
    for autonomous driving. In *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition*, pp.  12677–12686, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirza & Osindero (2014) Mehdi Mirza and Simon Osindero. Conditional generative
    adversarial nets. *arXiv preprint arXiv:1411.1784*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitchell et al. (2021) Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey
    Levine, and Chelsea Finn. Offline meta-reinforcement learning with advantage weighting.
    In *Proceedings of the 38th International Conference on Machine Learning*, volume
    139, pp.  7780–7791, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mohamed & Rezende (2015) Shakir Mohamed and Danilo Jimenez Rezende. Variational
    information maximisation for intrinsically motivated reinforcement learning. In
    *Advances in Neural Information Processing Systems 28*, pp.  2125–2133, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mu et al. (2021) Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Yang, Xuanlin Li,
    Stone Tao, Zhiao Huang, Zhiwei Jia, and Hao Su. Maniskill: Generalizable manipulation
    skill benchmark with large-scale demonstrations. In *Proceedings of the Neural
    Information Processing Systems Track on Datasets and Benchmarks 1*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nair et al. (2020) Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
    Accelerating online reinforcement learning with offline datasets. *CoRR*, abs/2006.09359,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nasiriany et al. (2022) Soroush Nasiriany, Tian Gao, Ajay Mandlekar, and Yuke
    Zhu. Learning and retrieval from prior data for skill-based imitation learning.
    In *Proceedings of the 6th Annual Conference on Robot Learning*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ng et al. (1999) Andrew Y. Ng, Daishi Harada, and Stuart Russell. Policy invariance
    under reward transformations: Theory and application to reward shaping. In *Proceedings
    of the 16th International Conference on Machine Learning*, pp.  278–287, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ni et al. (2023) Fei Ni, Jianye Hao, Yao Mu, Yifu Yuan, Yan Zheng, Bin Wang,
    and Zhixuan Liang. Metadiffuser: Diffusion model as conditional planner for offline
    meta-rl. In *Proceedings of the 40th International Conference on Machine Learning*,
    volume 202, pp.  26087–26105, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ni et al. (2020) Tianwei Ni, Harshit S. Sikchi, Yufei Wang, Tejus Gupta, Lisa
    Lee, and Ben Eysenbach. f-irl: Inverse reinforcement learning via state marginal
    matching. In *Proceedings of the 4th Conference on Robot Learning*, volume 155,
    pp.  529–551, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nichol & Dhariwal (2021) Alexander Quinn Nichol and Prafulla Dhariwal. Improved
    denoising diffusion probabilistic models. In *Proceedings of the 38th International
    Conference on Machine Learning*, volume 139, pp.  8162–8171\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nielsen et al. (2020) Didrik Nielsen, Priyank Jaini, Emiel Hoogeboom, Ole Winther,
    and Max Welling. Survae flows: Surjections to bridge the gap between vaes and
    flows. In *Advances in Neural Information Processing Systems 33*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noseworthy et al. (2019) Michael D. Noseworthy, Rohan Paul, Subhro Roy, Daehyung
    Park, and Nicholas Roy. Task-conditioned variational autoencoders for learning
    movement primitives. In *Proceedings of the 3rd Annual Conference on Robot Learning*,
    volume 100 of *Proceedings of Machine Learning Research*, pp.  933–944\. PMLR,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nowozin et al. (2016) Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan:
    Training generative neural samplers using variational divergence minimization.
    In *Advances in Neural Information Processing Systems 29*, pp.  271–279, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023. URL [https://arxiv.org/pdf/2303.08774.pdf](https://arxiv.org/pdf/2303.08774.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. (2022) Yingwei Pan, Yehao Li, Yiheng Zhang, Qi Cai, Fuchen Long,
    Zhaofan Qiu, Ting Yao, and Tao Mei. Silver-bullet-3d at maniskill 2021: Learning-from-demonstrations
    and heuristic rule-based methods for object manipulation. In *ICLR 2022 Workshop
    on Generalizable Policy Learning in Physical World*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pandey et al. (2022) Kushagra Pandey, Avideep Mukherjee, Piyush Rai, and Abhishek
    Kumar. DiffuseVAE: Efficient, controllable and high-fidelity generation from low-dimensional
    latents. *Transactions on Machine Learning Research*, 2022. ISSN 2835-8856. URL
    [https://openreview.net/forum?id=ygoNPRiLxw](https://openreview.net/forum?id=ygoNPRiLxw).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papamakarios et al. (2017) George Papamakarios, Iain Murray, and Theo Pavlakou.
    Masked autoregressive flow for density estimation. In *Advances in Neural Information
    Processing Systems 30*, pp.  2338–2347, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parisotto & Salakhutdinov (2021) Emilio Parisotto and Ruslan Salakhutdinov.
    Efficient transformers in reinforcement learning using actor-learner distillation.
    In *Proceedings of the 9th International Conference on Learning Representations*.
    OpenReview.net, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parisotto et al. (2020) Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan
    Pascanu, Çaglar Gülçehre, Siddhant M. Jayakumar, Max Jaderberg, Raphaël Lopez
    Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia
    Hadsell. Stabilizing transformers for reinforcement learning. In *Proceedings
    of the 37th International Conference on Machine Learning*, volume 119, pp.  7487–7498\.
    PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2021) Jongjin Park, Younggyo Seo, Chang Liu, Li Zhao, Tao Qin,
    Jinwoo Shin, and Tie-Yan Liu. Object-aware regularization for addressing causal
    confusion in imitation learning. *Advances in Neural Information Processing Systems*,
    34:3029–3042, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paster et al. (2022) Keiran Paster, Sheila A. McIlraith, and Jimmy Ba. You
    can’t count on luck: Why decision transformers and rvs fail in stochastic environments.
    In *Advances in Neural Information Processing Systems 35*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pasula (2020) Pranay Pasula. Complex skill acquisition through simple skill
    adversarial imitation learning. *CoRR*, abs/2007.10281, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pearce & Zhu (2022) Tim Pearce and Jun Zhu. Counter-strike deathmatch with large-scale
    behavioural cloning. In *2022 IEEE Conference on Games (CoG)*, pp.  104–111\.
    IEEE, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pearce et al. (2023) Tim Pearce, Tabish Rashid, Anssi Kanervisto, David Bignell,
    Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad,
    Katja Hofmann, and Sam Devlin. Imitating human behaviour with diffusion models.
    In *Proceedings of the 11th International Conference on Learning Representations*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peebles & Xie (2023) William Peebles and Saining Xie. Scalable diffusion models
    with transformers. In *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, pp.  4195–4205, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2021) Bei Peng, Tabish Rashid, Christian Schröder de Witt, Pierre-Alexandre
    Kamienny, Philip H. S. Torr, Wendelin Boehmer, and Shimon Whiteson. FACMAC: factored
    multi-agent centralised policy gradients. In *Advances in Neural Information Processing
    Systems 34*, pp.  12208–12221, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2022) Jian-Wei Peng, Min-Chun Hu, and Wei-Ta Chu. An imitation
    learning framework for generating multi-modal trajectories from unstructured demonstrations.
    *Neurocomputing*, 500:712–723, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2019a) Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel,
    and Sergey Levine. Variational discriminator bottleneck: Improving imitation learning,
    inverse rl, and gans by constraining information flow. In *Proceedings of the
    7th International Conference on Learning Representations*, 2019a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2019b) Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
    Advantage-weighted regression: Simple and scalable off-policy reinforcement learning.
    *CoRR*, abs/1910.00177, 2019b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pertsch et al. (2021) Karl Pertsch, Youngwoon Lee, Yue Wu, and Joseph J. Lim.
    Demonstration-guided reinforcement learning with learned skills. In *Conference
    on Robot Learning*, volume 164 of *Proceedings of Machine Learning Research*,
    pp.  729–739\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pfrommer et al. (2023) Samuel Pfrommer, Yatong Bai, Hyunin Lee, and Somayeh
    Sojoudi. Initial state interventions for deconfounded imitation learning. In *Proceedings
    of the 62nd IEEE Conference on Decision and Control*, pp.  2312–2319\. IEEE, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Plappert et al. (2018) Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob
    McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej,
    Peter Welinder, Vikash Kumar, and Wojciech Zaremba. Multi-goal reinforcement learning:
    Challenging robotics environments and request for research. *CoRR*, abs/1802.09464,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Plummer et al. (2015) Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C.
    Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting
    region-to-phrase correspondences for richer image-to-sentence models. *CoRR*,
    abs/1505.04870, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pomerleau (1991) Dean Pomerleau. Efficient training of artificial neural networks
    for autonomous navigation. *Neural Computation*, 3(1):88–97, 1991.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prudencio et al. (2023) Rafael Figueiredo Prudencio, Marcos ROA Maximo, and
    Esther Luna Colombini. A survey on offline reinforcement learning: Taxonomy, review,
    and open problems. *IEEE Transactions on Neural Networks and Learning Systems*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Puterman (2014) Martin L Puterman. *Markov decision processes: discrete stochastic
    dynamic programming*. John Wiley & Sons, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putterman et al. (2022) Aaron L Putterman, Kevin Lu, Igor Mordatch, and Pieter
    Abbeel. Pretraining for language conditioned imitation with transformers, 2022.
    URL [https://openreview.net/forum?id=eCPCn25gat](https://openreview.net/forum?id=eCPCn25gat).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. (2022) Han Qi, Yi Su, Aviral Kumar, and Sergey Levine. Data-driven
    offline decision-making via invariant representation learning. In *Advances in
    Neural Information Processing Systems 35*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. (2020) Mengshi Qi, Jie Qin, Yu Wu, and Yi Yang. Imitative non-autoregressive
    modeling for trajectory forecasting and imputation. In *IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, pp.  12733–12742\. Computer Vision Foundation
    / IEEE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2022) Rongjun Qin, Xingyuan Zhang, Songyi Gao, Xiong-Hui Chen,
    Zewen Li, Weinan Zhang, and Yang Yu. Neorl: A near real-world benchmark for offline
    reinforcement learning. In *Advances in Neural Information Processing Systems
    35*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qureshi et al. (2019) Ahmed Hussain Qureshi, Byron Boots, and Michael C. Yip.
    Adversarial imitation via variational inverse reinforcement learning. In *Proceedings
    of the 7th International Conference on Learning Representations*. OpenReview.net,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rabiner & Juang (1986) Lawrence Rabiner and Biinghwang Juang. An introduction
    to hidden markov models. *IEEE ASSP Magazine*, 3(1):4–16, 1986.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training. *mikecaptain.com*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rafailov et al. (2021) Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea
    Finn. Offline reinforcement learning from images with latent space models. In
    *Proceedings of the 3rd Annual Conference on Learning for Dynamics and Control*,
    volume 144, pp.  1154–1168, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rahmatizadeh et al. (2018) Rouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau
    Bölöni, and Sergey Levine. Vision-based multi-task manipulation for inexpensive
    robots using end-to-end learning from demonstration. In *IEEE International Conference
    on Robotics and Automation*, pp.  3758–3765\. IEEE, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramakrishnan et al. (2021) Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik
    Wijmans, Oleksandr Maksymets, Alexander Clegg, John M. Turner, Eric Undersander,
    Wojciech Galuba, Andrew Westbury, Angel X. Chang, Manolis Savva, Yili Zhao, and
    Dhruv Batra. Habitat-matterport 3d dataset (HM3D): 1000 large-scale 3d environments
    for embodied AI. In *Proceedings of the Neural Information Processing Systems
    Track on Datasets and Benchmarks 1*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
    and Mark Chen. Hierarchical text-conditional image generation with clip latents.
    *arXiv preprint arXiv:2204.06125*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rasouli et al. (2022) Amir Rasouli, Randy Goebel, Matthew E. Taylor, Iuliia
    Kotseruba, Soheil Alizadeh, Tianpei Yang, Montgomery Alban, Florian Shkurti, Yuzheng
    Zhuang, Adam Scibior, Kasra Rezaee, Animesh Garg, David Meger, Jun Luo, Liam Paull,
    Weinan Zhang, Xinyu Wang, and Xi Chen. Neurips 2022 competition: Driving smarts,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reed et al. (2022) Scott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez
    Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky,
    Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley
    Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar,
    and Nando de Freitas. A generalist agent. *Transactions on Machine Learning Research*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reid et al. (2022) Machel Reid, Yutaro Yamada, and Shixiang Shane Gu. Can wikipedia
    help offline reinforcement learning? *CoRR*, abs/2201.12122, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2020) Allen Z. Ren, Sushant Veer, and Anirudha Majumdar. Generalization
    guarantees for imitation learning. In *Proceedings of the 4th Conference on Robot
    Learning*, volume 155, pp.  1426–1442\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reuss et al. (2023) Moritz Reuss, Maximilian Li, Xiaogang Jia, and Rudolf Lioutikov.
    Goal-conditioned imitation learning using score-based diffusion policies. In *Robotics:
    Science and Systems*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rezaeifar et al. (2022) Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, Léonard
    Hussenot, Olivier Bachem, Olivier Pietquin, and Matthieu Geist. Offline reinforcement
    learning as anti-exploration. In *Thirty-Sixth AAAI Conference on Artificial Intelligence*,
    pp.  8106–8114\. AAAI Press, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rezende & Mohamed (2015) Danilo Jimenez Rezende and Shakir Mohamed. Variational
    inference with normalizing flows. In *Proceedings of the 32nd International Conference
    on Machine Learning*, volume 37, pp.  1530–1538, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ricciardi (1976) Luigi M Ricciardi. On the transformation of diffusion processes
    into the wiener process. *Journal of Mathematical Analysis and Applications*,
    54(1):185–199, 1976.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion
    models. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 
    10674–10685\. IEEE, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    U-net: Convolutional networks for biomedical image segmentation. In *Medical Image
    Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,
    Munich, Germany, October 5-9, 2015, Proceedings, Part III 18*, pp.  234–241\.
    Springer, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rosete-Beas et al. (2022) Erick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka
    Boedecker, and Wolfram Burgard. Latent plans for task-agnostic offline reinforcement
    learning. In *Conference on Robot Learning*, volume 205 of *Proceedings of Machine
    Learning Research*, pp.  1838–1849\. PMLR, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ross & Bagnell (2010) Stéphane Ross and Drew Bagnell. Efficient reductions for
    imitation learning. In *Proceedings of the 13th International Conference on Artificial
    Intelligence and Statistics*, pp.  661–668\. JMLR Workshop and Conference Proceedings,
    2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salimans et al. (2016) Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki
    Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In *Advances
    in Neural Information Processing Systems 29*, pp.  2226–2234, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samvelyan et al. (2019) Mikayel Samvelyan, Tabish Rashid, Christian Schröder
    de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philip
    H. S. Torr, Jakob N. Foerster, and Shimon Whiteson. The starcraft multi-agent
    challenge. *CoRR*, abs/1902.04043, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saxena et al. (2023) Vaibhav Saxena, Yotto Koga, and Danfei Xu. Constrained-context
    conditional diffusion models for imitation learning. *CoRR*, abs/2311.01419, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schroecker & Jr. (2020) Yannick Schroecker and Charles L. Isbell Jr. Universal
    value density estimation for imitation learning and goal-conditioned reinforcement
    learning. *CoRR*, abs/2002.06473, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schroecker et al. (2019) Yannick Schroecker, Mel Vecerík, and Jonathan Scholz.
    Generative predecessor models for sample-efficient imitation learning. In *Proceedings
    of the 7th International Conference on Learning Representations*. OpenReview.net,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. (2015) John Schulman, Sergey Levine, Pieter Abbeel, Michael I.
    Jordan, and Philipp Moritz. Trust region policy optimization. In *Proceedings
    of the 32nd International Conference on Machine Learning*, volume 37, pp.  1889–1897,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shafiullah et al. (2022) Nur Muhammad Shafiullah, Zichen Jeff Cui, Ariuntuya
    Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning $k$ modes with one
    stone. In *Advances in Neural Information Processing Systems 35*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shah et al. (2017) Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor.
    Airsim: High-fidelity visual and physical simulation for autonomous vehicles.
    In *Field and Service Robotics, Results of the 11th International Conference*,
    volume 5 of *Springer Proceedings in Advanced Robotics*, pp.  621–635\. Springer,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shamir (2018) Ohad Shamir. Are resnets provably better than linear predictors?
    In *Advances in Neural Information Processing Systems 31*, pp.  505–514, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shang & Ryoo (2021) Jinghuan Shang and Michael S. Ryoo. Self-supervised disentangled
    representation learning for third-person imitation learning. In *IEEE/RSJ International
    Conference on Intelligent Robots and Systems*, pp.  214–221\. IEEE, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shang et al. (2022) Jinghuan Shang, Kumara Kahatapitiya, Xiang Li, and Michael S.
    Ryoo. Starformer: Transformer with state-action-reward representations for visual
    reinforcement learning. In *Proceedings of the 17th European Conference on Computer
    Vision*, volume 13699, pp.  462–479, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharma et al. (2018) Pratyusha Sharma, Lekha Mohan, Lerrel Pinto, and Abhinav
    Gupta. Multiple interactions made easy (MIME): large scale demonstrations data
    for imitation. In *Proceedings of the 2nd Annual Conference on Robot Learning*,
    volume 87, pp.  906–915, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2023) Lucy Xiaoyang Shi, Archit Sharma, Tony Z. Zhao, and Chelsea
    Finn. Waypoint-based imitation learning for robotic manipulation. *CoRR*, abs/2307.14326,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shridhar et al. (2020) Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan
    Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED:
    A benchmark for interpreting grounded instructions for everyday tasks. In *2020
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.  10737–10746,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shridhar et al. (2022) Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor:
    A multi-task transformer for robotic manipulation. In *Conference on Robot Learning,
    CoRL 2022, 14-18 December 2022, Auckland, New Zealand*, volume 205 of *Proceedings
    of Machine Learning Research*, pp.  785–799\. PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Slack et al. (2022) Dylan Z Slack, Yinlam Chow, Bo Dai, and Nevan Wichers.
    Safer: Data-efficient and safe reinforcement learning via skill acquisition. In
    *ICML Decision Awareness in Reinforcement Learning Workshop*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sodhani et al. (2021) Shagun Sodhani, Amy Zhang, and Joelle Pineau. Multi-task
    reinforcement learning with context-based representations. In *Proceedings of
    the 38th International Conference on Machine Learning*, volume 139 of *Proceedings
    of Machine Learning Research*, pp.  9767–9779\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
    and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics.
    In *International conference on machine learning*, pp.  2256–2265\. PMLR, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan,
    and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics.
    In *Proceedings of the 32nd International Conference on Machine Learning*, volume 37,
    pp.  2256–2265, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohn et al. (2015) Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured
    output representation using deep conditional generative models. In *Advances in
    Neural Information Processing Systems 28*, pp.  3483–3491, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2018) Jiaming Song, Hongyu Ren, Dorsa Sadigh, and Stefano Ermon.
    Multi-agent generative adversarial imitation learning. In *Advances in Neural
    Information Processing Systems 31*, pp.  7472–7483, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song & Ermon (2019) Yang Song and Stefano Ermon. Generative modeling by estimating
    gradients of the data distribution. *Advances in neural information processing
    systems*, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song & Ermon (2020) Yang Song and Stefano Ermon. Improved techniques for training
    score-based generative models. In *Advances in Neural Information Processing Systems
    33*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2019) Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced
    score matching: A scalable approach to density and score estimation. In *Proceedings
    of the 35th Conference on Uncertainty in Artificial Intelligence*, volume 115,
    pp.  574–584, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2020) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek
    Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic
    differential equations. *arXiv preprint arXiv:2011.13456*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2021) Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.
    Maximum likelihood training of score-based diffusion models. In *Advances in Neural
    Information Processing Systems 34*, pp.  1415–1428, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2023) Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
    Consistency models. In *Proceedings of the 40th International Conference on Machine
    Learning*, volume 202, pp.  32211–32252\. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spurr et al. (2018) Adrian Spurr, Jie Song, Seonwook Park, and Otmar Hilliges.
    Cross-modal deep variational hand pose estimation. In *IEEE Conference on Computer
    Vision and Pattern Recognition*, pp.  89–98\. Computer Vision Foundation / IEEE
    Computer Society, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sridhar et al. (2023) Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, James
    Weimer, and Insup Lee. Memory-consistent neural networks for imitation learning.
    *CoRR*, abs/2310.06171, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sudhakaran & Risi (2023) Shyam Sudhakaran and Sebastian Risi. Skill decision
    transformer. *CoRR*, abs/2301.13573, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suh et al. (2023) H.J. Terry Suh, Glen Chou, Hongkai Dai, Lujie Yang, Abhishek
    Gupta, and Russ Tedrake. Fighting uncertainty with gradients: Offline reinforcement
    learning via diffusion score matching. In *7th Annual Conference on Robot Learning*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2021) Jiankai Sun, Lantao Yu, Pinqian Dong, Bo Lu, and Bolei Zhou.
    Adversarial inverse reinforcement learning with self-attention dynamics model.
    *IEEE Robotics and Automation Letters*, 6(2):1880–1886, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Jiankai Sun, Yiqi Jiang, Jianing Qiu, Parth Talpur Nobel,
    Mykel Kochenderfer, and Mac Schwager. Conformal prediction for uncertainty-aware
    planning with diffusion dynamics model. In *Advances in Neural Information Processing
    Systems 36*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton & Barto (2018) Richard S Sutton and Andrew G Barto. *Reinforcement learning:
    An introduction*. MIT press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takagi (2022) Shiro Takagi. On the effect of pre-training for transformer in
    different modality on offline reinforcement learning. In *Advances in Neural Information
    Processing Systems 35*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tassa et al. (2018) Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe
    Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq,
    Timothy P. Lillicrap, and Martin A. Riedmiller. Deepmind control suite. *CoRR*,
    abs/1801.00690, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tedrake (2023) Russ Tedrake. *Underactuated Robotics*. Course Notes for MIT
    6.832, 2023. URL [https://underactuated.csail.mit.edu](https://underactuated.csail.mit.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tishby & Zaslavsky (2015) Naftali Tishby and Noga Zaslavsky. Deep learning and
    the information bottleneck principle. In *IEEE Information Theory Workshop*, pp. 
    1–5\. IEEE, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Todorov et al. (2012) Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A
    physics engine for model-based control. In *IEEE/RSJ International Conference
    on Intelligent Robots and Systems*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toyer et al. (2020) Sam Toyer, Rohin Shah, Andrew Critch, and Stuart Russell.
    The MAGICAL benchmark for robust imitation. In *Advances in Neural Information
    Processing Systems 33*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trabucco et al. (2022) Brandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey
    Levine. Design-bench: Benchmarks for data-driven offline model-based optimization.
    In *Proceedings of the 39th International Conference on Machine Learning*, volume
    162, pp.  21658–21676, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tseng et al. (2022) Wei-Cheng Tseng, Tsun-Hsuan Johnson Wang, Yen-Chen Lin,
    and Phillip Isola. Offline multi-agent reinforcement learning with knowledge distillation.
    In *Advances in Neural Information Processing Systems 35*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Urpí et al. (2021) Núria Armengol Urpí, Sebastian Curi, and Andreas Krause.
    Risk-averse offline reinforcement learning. In *Proceedings of the 9th International
    Conference on Learning Representations*. OpenReview.net, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van den Berg et al. (2018) Rianne van den Berg, Leonard Hasenclever, Jakub M.
    Tomczak, and Max Welling. Sylvester normalizing flows for variational inference.
    In *Proceedings of the 34th Conference on Uncertainty in Artificial*, pp.  393–402,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van den Oord et al. (2017) Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu.
    Neural discrete representation learning. In *Advances in Neural Information Processing
    Systems 30*, pp.  6306–6315, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. *Advances in neural information processing systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vecerík et al. (2017) Matej Vecerík, Todd Hester, Jonathan Scholz, Fumin Wang,
    Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Rothörl, Thomas Lampe, and
    Martin A. Riedmiller. Leveraging demonstrations for deep reinforcement learning
    on robotics problems with sparse rewards. *CoRR*, abs/1707.08817, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Venkatraman et al. (2023) Siddarth Venkatraman, Shivesh Khaitan, Ravi Tej Akella,
    John M. Dolan, Jeff G. Schneider, and Glen Berseth. Reasoning with latent diffusion
    in offline reinforcement learning. *CoRR*, abs/2309.06599, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Venuto et al. (2020) David Venuto, Jhelum Chakravorty, Léonard Boussioux, Junhao
    Wang, Gavin McCracken, and Doina Precup. oirl: Robust adversarial inverse reinforcement
    learning with temporally extended actions. *CoRR*, abs/2002.09043, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Villaflor et al. (2022) Adam R. Villaflor, Zhe Huang, Swapnil Pande, John M.
    Dolan, and Jeff Schneider. Addressing optimism bias in sequence modeling for reinforcement
    learning. In *Proceedings of the 39th International Conference on Machine Learning*,
    volume 162, pp.  22270–22283, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vincent (2011) Pascal Vincent. A connection between score matching and denoising
    autoencoders. *Neural Computation*, 23(7):1661–1674, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vovk et al. (2005) Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. *Algorithmic
    learning in a random world*, volume 29. Springer, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vowels et al. (2020) Matthew J. Vowels, Necati Cihan Camgöz, and Richard Bowden.
    Gated variational autoencoders: Incorporating weak supervision to encourage disentanglement.
    In *Proceedings of the 15th IEEE International Conference on Automatic Face and
    Gesture Recognition*, pp.  125–132\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vuong et al. (2022) Quan Vuong, Aviral Kumar, Sergey Levine, and Yevgen Chebotar.
    DASCO: dual-generator adversarial support constrained offline reinforcement learning.
    In *Advances in Neural Information Processing Systems 35*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021) Jianhao Wang, Wenzhe Li, Haozhe Jiang, Guangxiang Zhu, Siyuan
    Li, and Chongjie Zhang. Offline reinforcement learning with reverse model-based
    imagination. In *Advances in Neural Information Processing Systems 34*, pp.  29420–29432,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022a) Kerong Wang, Hanye Zhao, Xufang Luo, Kan Ren, Weinan Zhang,
    and Dongsheng Li. Bootstrapped transformer for offline reinforcement learning.
    In *Advances in Neural Information Processing Systems 35*, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Mianchu Wang, Rui Yang, Xi Chen, and Meng Fang. Goplan:
    Goal-conditioned offline reinforcement learning by planning with learned models.
    *CoRR*, abs/2310.20025, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023b) Pengqin Wang, Meixin Zhu, and Shaojie Shen. Environment
    transformer and policy optimization for model-based offline reinforcement learning,
    2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022b) Tianyu Wang, Nikhil Karnwal, and Nikolay Atanasov. Latent
    policies for adversarial imitation learning. *CoRR*, abs/2206.11299, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023c) Yiqi Wang, Mengdi Xu, Laixi Shi, and Yuejie Chi. A trajectory
    is worth three sentences: multimodal transformer for offline reinforcement learning.
    In *Uncertainty in Artificial Intelligence*, volume 216, pp.  2226–2236, 2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023d) Yuanfu Wang, Chao Yang, Ying Wen, Yu Liu, and Yu Qiao. Critic-guided
    decision transformer for offline reinforcement learning. *arXiv preprint arXiv:2312.13716*,
    2023d.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023e) Yunke Wang, Minjing Dong, Bo Du, and Chang Xu. Imitation
    learning from purified demonstration. *arXiv preprint arXiv:2310.07143*, 2023e.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023f) Zhendong Wang, Jonathan J. Hunt, and Mingyuan Zhou. Diffusion
    policies as an expressive policy class for offline reinforcement learning. In
    *Proceedings of the 11th International Conference on Learning Representations*.
    OpenReview.net, 2023f.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Ziyu Wang, Josh Merel, Scott E. Reed, Nando de Freitas, Gregory
    Wayne, and Nicolas Heess. Robust imitation of diverse behaviors. In *Advances
    in Neural Information Processing Systems 30, Long Beach, CA, USA*, pp.  5320–5329,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh Merel, Jost Tobias
    Springenberg, Scott E. Reed, Bobak Shahriari, Noah Y. Siegel, Çaglar Gülçehre,
    Nicolas Heess, and Nando de Freitas. Critic regularized regression. In *Advances
    in Neural Information Processing Systems 33*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ward et al. (2019) Patrick Nadeem Ward, Ariella Smofsky, and Avishek Joey Bose.
    Improving exploration in soft-actor-critic with normalizing flows policies. *CoRR*,
    abs/1906.02771, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2021) Hua Wei, Deheng Ye, Zhao Liu, Hao Wu, Bo Yuan, Qiang Fu, Wei
    Yang, and Zhenhui Li. Boosting offline reinforcement learning with residual generative
    modeling. In *Proceedings of the 30th International Joint Conference on Artificial
    Intelligence*, pp.  3574–3580\. ijcai.org, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2023) Yao Wei, Yanchao Sun, Ruijie Zheng, Sai Vemprala, Rogerio
    Bonatti, Shuhang Chen, Ratnesh Madaan, Zhongjie Ba, Ashish Kapoor, and Shuang
    Ma. Is imitation all you need? generalized decision-making with dual-phase training.
    In *IEEE/CVF International Conference on Computer Vision*, pp.  16221–16231, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weissenbacher et al. (2022) Matthias Weissenbacher, Samarth Sinha, Animesh
    Garg, and Yoshinobu Kawahara. Koopman q-learning: Offline reinforcement learning
    via symmetries of dynamics. In *Proceedings of the 39th International Conference
    on Machine Learning*, volume 162, pp.  23645–23667, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Welling & Teh (2011) Max Welling and Yee Whye Teh. Bayesian learning via stochastic
    gradient langevin dynamics. In *Proceedings of the 28th International Conference
    on Machine Learning*, pp.  681–688, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2022) Muning Wen, Jakub Grudzien Kuba, Runji Lin, Weinan Zhang,
    Ying Wen, Jun Wang, and Yaodong Yang. Multi-agent reinforcement learning is a
    sequence modeling problem. In *Advances in Neural Information Processing Systems
    35*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wen et al. (2023) Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing
    Ma, Junchi Yan, and Liang Sun. Transformers in time series: A survey. In *Proceedings
    of the 32nd International Joint Conference on Artificial Intelligence*, pp.  6778–6786,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wiewiora (2003) Eric Wiewiora. Potential-based shaping and q-value initialization
    are equivalent. *Journal of Artificial Intelligence Research*, 19:205–208, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2022) Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng
    Long. Supported policy optimization for offline reinforcement learning. In *Advances
    in Neural Information Processing Systems 35*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2019) Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized
    offline reinforcement learning. *CoRR*, abs/1911.11361, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2021) Yuchen Wu, Melissa Mozifian, and Florian Shkurti. Shaping rewards
    for reinforcement learning with imperfect demonstrations using generative models.
    In *IEEE International Conference on Robotics and Automation*, pp.  6628–6634\.
    IEEE, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Wei Xiao, Tsun-Hsuan Wang, Chuang Gan, and Daniela Rus.
    Safediffuser: Safe planning with diffusion probabilistic models. *CoRR*, abs/2306.00148,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2022) Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling
    the generative learning trilemma with denoising diffusion gans. In *Proceedings
    of the 10th International Conference on Learning Representations*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2023) Zhihui Xie, Zichuan Lin, Deheng Ye, Qiang Fu, Yang Wei, and
    Shuai Li. Future-conditioned unsupervised pretraining for decision transformer.
    In *Proceedings of the 40th International Conference on Machine Learning*, volume
    202, pp.  38187–38203, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2022a) Haoran Xu, Xianyuan Zhan, and Xiangyu Zhu. Constraints penalized
    q-learning for safe offline reinforcement learning. In *Thirty-Sixth AAAI Conference
    on Artificial Intelligence*, pp.  8753–8760\. AAAI Press, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2022b) Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao,
    Joshua B. Tenenbaum, and Chuang Gan. Prompting decision transformer for few-shot
    policy generalization. In *Proceedings of the 39th International Conference on
    Machine Learning*, volume 162, pp.  24631–24645, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2022c) Yilun Xu, Ziming Liu, Max Tegmark, and Tommi S. Jaakkola.
    Poisson flow generative models. In *Advances in Neural Information Processing
    Systems 35*, 2022c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong, Max Tegmark,
    and Tommi S. Jaakkola. PFGM++: unlocking the potential of physics-inspired generative
    models. In *proceedings of the 40th International Conference on Machine Learning*,
    volume 202 of *Proceedings of Machine Learning Research*, pp.  38566–38591\. PMLR,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yamagata et al. (2023) Taku Yamagata, Ahmed Khalil, and Raúl Santos-Rodríguez.
    Q-learning decision transformer: Leveraging dynamic programming for conditional
    sequence modelling in offline RL. In *Proceedings of the 40th International Conference
    on Machine Learning*, volume 202, pp.  38989–39007, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2022) Kai Yan, Alexander G. Schwing, and Yu-Xiong Wang. CEIP: combining
    explicit and implicit priors for reinforcement learning with demonstrations. In
    *Advances in Neural Information Processing Systems 35*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023a) Junming Yang, Xingguo Chen, Shengyuan Wang, and Bolei Zhang.
    Model-based offline policy optimization with adversarial network. In *Proceedings
    of the 26th European Conference on Artificial Intelligence*, volume 372, pp. 
    2850–2857, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2022a) Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng
    Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Ming-Hsuan Yang, and Bin Cui. Diffusion
    models: A comprehensive survey of methods and applications. *CoRR*, abs/2209.00796,
    2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023b) Rui Yang, Lin Yong, Xiaoteng Ma, Hao Hu, Chongjie Zhang,
    and Tong Zhang. What is essential for unseen goal generalization of offline goal-conditioned
    rl? In *Proceedings of the 40th International Conference on Machine Learning*,
    volume 202, pp.  39543–39571, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022b) Shentao Yang, Yihao Feng, Shujian Zhang, and Mingyuan Zhou.
    Regularizing a model-based policy stationary distribution to stabilize offline
    reinforcement learning. In *Proceedings of the 39th International Conference on
    Machine Learning*, volume 162, pp.  24980–25006, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022c) Shentao Yang, Zhendong Wang, Huangjie Zheng, Yihao Feng,
    and Mingyuan Zhou. A behavior regularized implicit policy for offline reinforcement
    learning. *arXiv preprint arXiv:2202.09673*, 2022c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022d) Shentao Yang, Shujian Zhang, Yihao Feng, and Mingyuan Zhou.
    A unified framework for alternating offline model training and policy learning.
    In *Advances in Neural Information Processing Systems 35*, 2022d.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023c) Sherry Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum.
    Dichotomy of control: Separating what you can control from what you cannot. In
    *Proceedings of the 11th International Conference on Learning Representations*.
    OpenReview.net, 2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022e) Yaodong Yang, Guangyong Chen, Weixun Wang, Xiaotian Hao,
    Jianye Hao, and Pheng-Ann Heng. Transformer-based working memory for multiagent
    reinforcement learning with action parsing. In *Advances in Neural Information
    Processing Systems 35*, 2022e.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022f) Yiming Yang, Dengpeng Xing, and Bo Xu. Efficient spatiotemporal
    transformer for robotic reinforcement learning. *IEEE Robotics Automation Letters*,
    7(3):7982–7989, 2022f.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023d) Yiqin Yang, Hao Hu, Wenzhe Li, Siyuan Li, Jun Yang, Qianchuan
    Zhao, and Chongjie Zhang. Flow to control: Offline reinforcement learning with
    lossless primitive discovery. In *Proceedings of the 37th AAAI Conference on Artificial
    Intelligence*, pp.  10843–10851\. AAAI Press, 2023d.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2019a) Lantao Yu, Jiaming Song, and Stefano Ermon. Multi-agent adversarial
    inverse reinforcement learning. In *Proceedings of the 36th International Conference
    on Machine Learning*, volume 97, pp.  7194–7201, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2019b) Lantao Yu, Tianhe Yu, Chelsea Finn, and Stefano Ermon. Meta-inverse
    reinforcement learning with probabilistic context variables. In *Advances in Neural
    Information Processing Systems 32*, pp.  11749–11760, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2019c) Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol
    Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation
    for multi-task and meta reinforcement learning. In *Proceedings of the 3rd Annual
    Conference on Robot Learning*, volume 100, pp.  1094–1100, 2019c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2020a) Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y.
    Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. MOPO: model-based offline policy
    optimization. In *Advances in Neural Information Processing Systems 33*, 2020a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2020b) Xingrui Yu, Yueming Lyu, and Ivor W. Tsang. Intrinsic reward
    driven imitation learning via generative model. In *Proceedings of the 37th International
    Conference on Machine Learning*, volume 119 of *Proceedings of Machine Learning
    Research*, pp.  10925–10935\. PMLR, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2023a) William Yuan, Jiaxing Chen, Shaofei Chen, Lina Lu, Zhenzhen
    Hu, Peng Li, Dawei Feng, Furong Liu, and Jing Chen. Transformer in reinforcement
    learning for decision-making: A survey. *TechRxiv*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2023b) Ye Yuan, Xin Li, Yong Heng, Leiji Zhang, and Mingzhong
    Wang. Good better best: Self-motivated imitation learning for noisy demonstrations.
    *CoRR*, abs/2310.15815, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021a) Chi Zhang, Sanmukh R. Kuppannagari, and Viktor K. Prasanna.
    BRAC+: improved behavior regularized actor critic for offline reinforcement learning.
    In *Asian Conference on Machine Learning*, volume 157 of *Proceedings of Machine
    Learning Research*, pp.  204–219\. PMLR, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022) Hongchang Zhang, Jianzhun Shao, Yuhang Jiang, Shuncheng
    He, Guanwen Zhang, and Xiangyang Ji. State deviation correction for offline reinforcement
    learning. In *Thirty-Sixth AAAI Conference on Artificial Intelligence*, pp.  9022–9030\.
    AAAI Press, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Jing Zhang, Chi Zhang, Wenjia Wang, and Bing-Yi Jing. APAC:
    authorized probability-controlled actor-critic for offline reinforcement learning.
    *CoRR*, abs/2301.12130, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019) Xin Zhang, Yanhua Li, Xun Zhou, and Jun Luo. Unveiling
    taxi drivers’ strategies via cgail: Conditional generative adversarial imitation
    learning. In *IEEE International Conference on Data Mining*, pp.  1480–1485\.
    IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020a) Xin Zhang, Yanhua Li, Ziming Zhang, and Zhi-Li Zhang.
    f-gail: Learning f-divergence for generative adversarial imitation learning. In
    *Advances in Neural Information Processing Systems 33*, 2020a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021b) Xin Zhang, Yanhua Li, Ziming Zhang, Christopher Brinton,
    Zhenming Liu, Zhi-Li Zhang, Hui Lu, and Zhihong Tian. Stabilized likelihood-based
    imitation learning via denoising continuous normalizing flow. In *Submission to
    the 10th International Conference on Learning Representations*, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020b) Yufeng Zhang, Qi Cai, Zhuoran Yang, and Zhaoran Wang.
    Generative adversarial imitation learning with neural network parameterization:
    Global optimality and convergence rate. In *Proceedings of the 40th International
    Conference on Machine Learning*, pp.  11044–11054\. PMLR, 2020b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2022) Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision
    transformer. In *Proceedings of the 39th International Conference on Machine Learning*,
    volume 162, pp.  27042–27059, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Qinqing Zheng, Mikael Henaff, Brandon Amos, and Aditya Grover.
    Semi-supervised offline reinforcement learning with action-free trajectories.
    In *Proceedings of the 40th International Conference on Machine Learning*, volume
    202, pp.  42339–42362, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2020) Wenxuan Zhou, Sujay Bajracharya, and David Held. PLAS: latent
    action space for offline reinforcement learning. In *Proceedings of the 4th Conference
    on Robot Learning*, volume 155 of *Proceedings of Machine Learning Research*,
    pp.  1719–1735\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023a) Tianchen Zhu, Yue Qiu, Haoyi Zhou, and Jianxin Li. Towards
    long-delayed sparsity: Learning a better transformer through reward redistribution.
    In *Proceedings of the 32nd International Joint Conference on Artificial Intelligence*,
    pp.  4693–4701, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2022) Yifeng Zhu, Abhishek Joshi, Peter Stone, and Yuke Zhu. VIOLA:
    imitation learning for vision-based manipulation with object proposal priors.
    *CoRR*, abs/2210.11339, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2020) Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Martín-Martín.
    robosuite: A modular simulation framework and benchmark for robot learning. *CoRR*,
    abs/2009.12293, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023b) Zhengbang Zhu, Minghuan Liu, Liyuan Mao, Bingyi Kang, Minkai
    Xu, Yong Yu, Stefano Ermon, and Weinan Zhang. Madiff: Offline multi-agent learning
    with diffusion models. *CoRR*, abs/2305.17330, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023c) Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu
    Zhang, Yong Yu, and Weinan Zhang. Diffusion models for reinforcement learning:
    A survey. *CoRR*, abs/2311.01223, 2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ziebart et al. (2010) Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey.
    Modeling interaction via the principle of maximum causal entropy. In *Proceedings
    of the 27th International Conference on Machine Learning*, pp.  1255–1262, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zintgraf et al. (2020) Luisa M. Zintgraf, Kyriacos Shiarlis, Maximilian Igl,
    Sebastian Schulze, Yarin Gal, Katja Hofmann, and Shimon Whiteson. Varibad: A very
    good method for bayes-adaptive deep RL via meta-learning. In *Proceedings of the
    8th International Conference on Learning Representations*. OpenReview.net, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zolna et al. (2020) Konrad Zolna, Scott E. Reed, Alexander Novikov, Sergio Gómez
    Colmenarejo, David Budden, Serkan Cabi, Misha Denil, Nando de Freitas, and Ziyu
    Wang. Task-relevant adversarial imitation learning. In *Proceedings of the 4th
    Conference on Robot Learning*, volume 155, pp.  247–263\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zuo et al. (2020) Guoyu Zuo, Kexin Chen, Jiahao Lu, and Xiangsheng Huang. Deterministic
    generative adversarial imitation learning. *Neurocomputing*, 388:60–69, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
