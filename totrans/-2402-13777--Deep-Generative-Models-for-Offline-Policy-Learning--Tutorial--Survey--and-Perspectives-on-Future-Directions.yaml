- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:34:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:34:32
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2402.13777] Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2402.13777] 离线策略学习的深度生成模型：教程、综述与未来方向的展望'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.13777](https://ar5iv.labs.arxiv.org/html/2402.13777)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.13777](https://ar5iv.labs.arxiv.org/html/2402.13777)
- en: 'Deep Generative Models for Offline Policy Learning:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离线策略学习的深度生成模型：
- en: Tutorial, Survey, and Perspectives on Future Directions
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 教程、综述与未来方向的展望
- en: Jiayu Chen chen3686@purdue.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 陈家宇 chen3686@purdue.edu
- en: Purdue University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 普渡大学
- en: West Lafayette, IN 47907 Bhargav Ganguly bganguly@purdue.edu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 西拉法叶，IN 47907 巴尔戈夫·甘古利 bganguly@purdue.edu
- en: Purdue University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 普渡大学
- en: West Lafayette, IN 47907 Yang Xu xu1720@purdue.edu
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 西拉法叶，IN 47907 杨旭 xu1720@purdue.edu
- en: Purdue University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 普渡大学
- en: West Lafayette, IN 47907 Yongsheng Mei ysmei@gwu.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 西拉法叶，IN 47907 梅永生 ysmei@gwu.edu
- en: The George Washington University
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 乔治·华盛顿大学
- en: Washington, DC 20052 Tian Lan tlan@gwu.edu
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 华盛顿，DC 20052 兰田 tlan@gwu.edu
- en: The George Washington University
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 乔治·华盛顿大学
- en: Washington, DC 20052 Vaneet Aggarwal vaneet@purdue.edu
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 华盛顿，DC 20052 瓦尼特·阿格瓦尔 vaneet@purdue.edu
- en: Purdue University
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 普渡大学
- en: West Lafayette, IN 47907
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 西拉法叶，IN 47907
- en: Abstract
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep generative models (DGMs) have demonstrated great success across various
    domains, particularly in generating texts, images, and videos using models trained
    from offline data. Similarly, data-driven decision-making and robotic control
    also necessitate learning a generator function from the offline data to serve
    as the strategy or policy. In this case, applying deep generative models in offline
    policy learning exhibits great potential, and numerous studies have explored in
    this direction. However, this field still lacks a comprehensive review and so
    developments of different branches are relatively independent. Thus, we provide
    the first systematic review on the applications of deep generative models for
    offline policy learning. In particular, we cover five mainstream deep generative
    models, including Variational Auto-Encoders, Generative Adversarial Networks,
    Normalizing Flows, Transformers, and Diffusion Models, and their applications
    in both offline reinforcement learning (offline RL) and imitation learning (IL).
    Offline RL and IL are two main branches of offline policy learning and are widely-adopted
    techniques for sequential decision-making. Specifically, for each type of DGM-based
    offline policy learning, we distill its fundamental scheme, categorize related
    works based on the usage of the DGM, and sort out the development process of algorithms
    in that field. Subsequent to the main content, we provide in-depth discussions
    on deep generative models and offline policy learning as a summary, based on which
    we present our perspectives on future research directions. This work offers a
    hands-on reference for the research progress in deep generative models for offline
    policy learning, and aims to inspire improved DGM-based offline RL or IL algorithms.
    For convenience, we maintain a paper list on [https://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning](https://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning).
    ¹¹1The contributions of each author for this paper are detailed in the Acknowledgement.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 深度生成模型（DGM）在多个领域取得了巨大成功，特别是在使用从离线数据训练的模型生成文本、图像和视频方面。类似地，数据驱动的决策制定和机器人控制也需要从离线数据中学习生成函数，以作为策略或政策。在这种情况下，应用深度生成模型进行离线策略学习显示出巨大的潜力，许多研究也在这一方向上进行了探索。然而，这一领域仍缺乏全面的综述，因此不同分支的发展相对独立。因此，我们提供了关于深度生成模型在离线策略学习中的应用的首个系统性综述。特别地，我们涵盖了五种主流的深度生成模型，包括变分自编码器、生成对抗网络、归一化流、变换器和扩散模型，以及它们在离线强化学习（离线
    RL）和模仿学习（IL）中的应用。离线 RL 和 IL 是离线策略学习的两个主要分支，是序列决策制定的广泛采用技术。具体来说，对于每种基于 DGM 的离线策略学习类型，我们提炼其基本方案，根据
    DGM 的使用情况对相关工作进行分类，并梳理该领域算法的发展过程。在主要内容之后，我们基于对深度生成模型和离线策略学习的深入讨论，提出了我们对未来研究方向的展望。这项工作提供了一个关于深度生成模型在离线策略学习中研究进展的实用参考，并旨在激发改进基于
    DGM 的离线 RL 或 IL 算法。为了方便，我们在[https://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning](https://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning)上维护了一份论文列表。¹¹1每位作者在本文中的贡献详见致谢部分。
- en: Contents
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 目录
- en: '[1 Introduction](#S1 "In Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 介绍](#S1 "在深度生成模型的离线策略学习中：教程、调查和未来方向的展望")'
- en: '[2 Background on Deep Generative Models](#S2 "In Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 深度生成模型背景](#S2 "在深度生成模型的离线策略学习中：教程、调查和未来方向的展望")'
- en: '[2.1 Variational Auto-Encoders](#S2.SS1 "In 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.1 变分自编码器](#S2.SS1 "在深度生成模型背景 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向的展望")'
- en: '[2.2 Generative Adversarial Networks](#S2.SS2 "In 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2 生成对抗网络](#S2.SS2 "在深度生成模型背景 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向的展望")'
- en: '[2.3 Normalizing Flows](#S2.SS3 "In 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.3 归一化流](#S2.SS3 "在深度生成模型背景 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向的展望")'
- en: '[2.4 Transformers](#S2.SS4 "In 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.4 Transformers](#S2.SS4 "在深度生成模型背景 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向的展望")'
- en: '[2.5 Diffusion Models](#S2.SS5 "In 2 Background on Deep Generative Models ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.5 扩散模型](#S2.SS5 "在深度生成模型背景 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向的展望")'
- en: '[3 Variational Auto-Encoders in Offline Policy Learning](#S3 "In Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[离线策略学习中的变分自编码器](#S3 "在深度生成模型的离线策略学习中：教程、调查和未来方向的展望")'
- en: '[3.1 Offline Reinforcement Learning](#S3.SS1 "In 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1 离线强化学习](#S3.SS1 "在离线策略学习中的变分自编码器 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向的展望")'
- en: '[3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning](#S3.SS1.SSS1
    "In 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.1 基于动态规划的离线强化学习背景](#S3.SS1.SSS1 "在离线强化学习 ‣ 离线策略学习中的变分自编码器 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向的展望")'
- en: '[3.1.2 An Overview of Applying VAEs in Offline Reinforcement Learning](#S3.SS1.SSS2
    "In 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.2 离线强化学习中应用 VAEs 的概述](#S3.SS1.SSS2 "在离线强化学习 ‣ 离线策略学习中的变分自编码器 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向的展望")'
- en: '[3.1.3 Addressing the Issue of Out-of-Distribution Actions Using VAEs](#S3.SS1.SSS3
    "In 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.3 解决使用 VAEs 的分布外动作问题](#S3.SS1.SSS3 "在离线强化学习 ‣ 离线策略学习中的变分自编码器 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向的展望")'
- en: '[3.1.4 Data Augmentation and Transformation with VAEs](#S3.SS1.SSS4 "In 3.1
    Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.4 使用 VAEs 进行数据增强和转换](#S3.SS1.SSS4 "在离线强化学习 ‣ 离线策略学习中的变分自编码器 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向的展望")'
- en: '[3.1.5 Offline Multi-task/Hierarchical RL based on VAEs](#S3.SS1.SSS5 "In 3.1
    Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.5 基于变分自编码器的离线多任务/层次强化学习](#S3.SS1.SSS5 "在3.1 离线强化学习 ‣ 3 离线策略学习中的变分自编码器
    ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[3.2 Imitation Learning](#S3.SS2 "In 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2 模仿学习](#S3.SS2 "在3 离线策略学习中的变分自编码器 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[3.2.1 Core Schemes of VAE-based Imitation Learning](#S3.SS2.SSS1 "In 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2.1 基于VAE的模仿学习核心方案](#S3.SS2.SSS1 "在3.2 模仿学习 ‣ 3 离线策略学习中的变分自编码器 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[3.2.2 Improving Data Efficiency in Imitation Learning with VAEs](#S3.SS2.SSS2
    "In 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2.2 使用变分自编码器提高模仿学习的数据效率](#S3.SS2.SSS2 "在3.2 模仿学习 ‣ 3 离线策略学习中的变分自编码器 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[3.2.3 Managing Multi-Modal Inputs in Imitation Learning via VAEs](#S3.SS2.SSS3
    "In 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2.3 通过变分自编码器管理模仿学习中的多模态输入](#S3.SS2.SSS3 "在3.2 模仿学习 ‣ 3 离线策略学习中的变分自编码器 ‣
    离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[3.2.4 Skill Acquisition and Hierarchical Imitation Learning through VAEs](#S3.SS2.SSS4
    "In 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2.4 通过变分自编码器的技能获取和层次模仿学习](#S3.SS2.SSS4 "在3.2 模仿学习 ‣ 3 离线策略学习中的变分自编码器 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[3.2.5 Tackling Causal Confusion in Imitation Learning with VAEs](#S3.SS2.SSS5
    "In 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2.5 使用变分自编码器应对模仿学习中的因果混淆](#S3.SS2.SSS5 "在3.2 模仿学习 ‣ 3 离线策略学习中的变分自编码器 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[4 Generative Adversarial Networks in Offline Policy Learning](#S4 "In Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 离线策略学习中的生成对抗网络](#S4 "在离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[4.1 Imitation Learning](#S4.SS1 "In 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1 模仿学习](#S4.SS1 "在4 离线策略学习中的生成对抗网络 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[4.1.1 Fundamental GAN-Based Imitation Learning Algorithms: GAIL and AIRL](#S4.SS1.SSS1
    "In 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1.1 基本的GAN基模仿学习算法：GAIL和AIRL](#S4.SS1.SSS1 "在4.1 模仿学习 ‣ 4 离线策略学习中的生成对抗网络
    ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[4.1.2 Extensions of GAIL](#S4.SS1.SSS2 "In 4.1 Imitation Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1.2 GAIL的扩展](#S4.SS1.SSS2 "在4.1 模仿学习 ‣ 4 离线策略学习中的生成对抗网络 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[4.1.3 Extensions of AIRL](#S4.SS1.SSS3 "In 4.1 Imitation Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1.3 AIRL 的扩展](#S4.SS1.SSS3 "在《4 模仿学习 ‣ 4 离线策略学习中的生成对抗网络 ‣ 离线策略学习中的深度生成模型：教程、调查与未来方向展望》中")'
- en: '[4.1.4 Integrating VAEs and GANs for Imitation Learning: A Spotlight](#S4.SS1.SSS4
    "In 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1.4 将 VAEs 和 GANs 融合用于模仿学习：聚焦](#S4.SS1.SSS4 "在《4 模仿学习 ‣ 4 离线策略学习中的生成对抗网络
    ‣ 离线策略学习中的深度生成模型：教程、调查与未来方向展望》中")'
- en: '[4.2 Offline Reinforcement Learning](#S4.SS2 "In 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2 离线强化学习](#S4.SS2 "在《4 离线策略学习中的生成对抗网络 ‣ 离线策略学习中的深度生成模型：教程、调查与未来方向展望》中")'
- en: '[4.2.1 Background on Model-based Offline Reinforcement Learning](#S4.SS2.SSS1
    "In 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2.1 基于模型的离线强化学习背景](#S4.SS2.SSS1 "在《4 离线强化学习 ‣ 4 离线策略学习中的生成对抗网络 ‣ 离线策略学习中的深度生成模型：教程、调查与未来方向展望》中")'
- en: '[4.2.2 Policy Approximation Using GANs](#S4.SS2.SSS2 "In 4.2 Offline Reinforcement
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2.2 使用 GANs 进行策略近似](#S4.SS2.SSS2 "在《4 离线强化学习 ‣ 4 离线策略学习中的生成对抗网络 ‣ 离线策略学习中的深度生成模型：教程、调查与未来方向展望》中")'
- en: '[4.2.3 World Model Representation through GANs](#S4.SS2.SSS3 "In 4.2 Offline
    Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2.3 通过 GANs 表示世界模型](#S4.SS2.SSS3 "在《4 离线强化学习 ‣ 4 离线策略学习中的生成对抗网络 ‣ 离线策略学习中的深度生成模型：教程、调查与未来方向展望》中")'
- en: '[5 Normalizing Flows in Offline Policy Learning](#S5 "In Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 离线策略学习中的归一化流](#S5 "在《离线策略学习中的深度生成模型：教程、调查与未来方向展望》中")'
- en: '[5.1 Imitation Learning](#S5.SS1 "In 5 Normalizing Flows in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1 模仿学习](#S5.SS1 "在《5 离线策略学习中的归一化流 ‣ 离线策略学习中的深度生成模型：教程、调查与未来方向展望》中")'
- en: '[5.1.1 Exact Density Estimation in Imitation Learning Using Normalizing Flows](#S5.SS1.SSS1
    "In 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1.1 使用归一化流进行模仿学习的精确密度估计](#S5.SS1.SSS1 "在《5.1 模仿学习 ‣ 5 离线策略学习中的归一化流 ‣ 离线策略学习中的深度生成模型：教程、调查与未来方向展望》中")'
- en: '[5.1.2 Policy Modeling in Imitation Learning with Normalizing Flows](#S5.SS1.SSS2
    "In 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1.2 在模仿学习中使用归一化流进行策略建模](#S5.SS1.SSS2 "在《5.1 模仿学习 ‣ 5 离线策略学习中的归一化流 ‣ 离线策略学习中的深度生成模型：教程、调查与未来方向展望》中")'
- en: '[5.2 Reinforcement Learning with Offline Data](#S5.SS2 "In 5 Normalizing Flows
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2 使用离线数据的强化学习](#S5.SS2 "在《5 离线策略学习中的归一化流 ‣ 离线策略学习中的深度生成模型：教程、调查与未来方向展望》中")'
- en: '[5.2.1 Adopting Normalizing Flows in Offline Reinforcement Learning](#S5.SS2.SSS1
    "In 5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2.1 在离线强化学习中采用归一化流](#S5.SS2.SSS1 "在 5.2 使用离线数据的强化学习 ‣ 5 离线策略学习中的归一化流 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[5.2.2 Adopting Normalizing Flows in Online Reinforcement Learning with Offline
    Data](#S5.SS2.SSS2 "In 5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing
    Flows in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2.2 在使用离线数据的在线强化学习中采用归一化流](#S5.SS2.SSS2 "在 5.2 使用离线数据的强化学习 ‣ 5 离线策略学习中的归一化流
    ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[6 Transformers in Offline Policy Learning](#S6 "In Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6 离线策略学习中的变换器](#S6 "在离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[6.1 Offline Reinforcement Learning](#S6.SS1 "In 6 Transformers in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1 离线强化学习](#S6.SS1 "在 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[6.1.1 Background on Trajectory-Optimization-based Offline Reinforcement Learning](#S6.SS1.SSS1
    "In 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1.1 基于轨迹优化的离线强化学习背景](#S6.SS1.SSS1 "在 6.1 离线强化学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[6.1.2 Balancing Model Capacity with Training Data](#S6.SS1.SSS2 "In 6.1 Offline
    Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1.2 平衡模型容量与训练数据](#S6.SS1.SSS2 "在 6.1 离线强化学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[6.1.3 Mitigating Impacts from Environmental Stochasticity](#S6.SS1.SSS3 "In
    6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1.3 缓解环境随机性带来的影响](#S6.SS1.SSS3 "在 6.1 离线强化学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[6.1.4 Transformers in Extended Offline Reinforcement Learning Setups](#S6.SS1.SSS4
    "In 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1.4 在扩展离线强化学习设置中使用变换器](#S6.SS1.SSS4 "在 6.1 离线强化学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[6.1.5 Reflections on Transformer-based Offline Reinforcement Learning](#S6.SS1.SSS5
    "In 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1.5 对基于变换器的离线强化学习的反思](#S6.SS1.SSS5 "在 6.1 离线强化学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[6.2 Imitation Learning](#S6.SS2 "In 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2 模仿学习](#S6.SS2 "在 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[6.2.1 A Paradigm of Transformer-based Imitation Learning](#S6.SS2.SSS1 "In
    6.2 Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2.1 基于变换器的模仿学习范式](#S6.SS2.SSS1 "在 6.2 模仿学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")'
- en: '[6.2.2 Adopting Transformers as the Policy Backbone for Imitation Learning](#S6.SS2.SSS2
    "In 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2.2 将 Transformers 作为模仿学习的策略骨干](#S6.SS2.SSS2 "在 6.2 模仿学习 ‣ 6 离线策略学习中的 Transformers
    ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的展望")'
- en: '[6.2.3 Developing Generalist Imitation Learning Agents with Transformers](#S6.SS2.SSS3
    "In 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2.3 使用 Transformers 开发通用的模仿学习代理](#S6.SS2.SSS3 "在 6.2 模仿学习 ‣ 6 离线策略学习中的 Transformers
    ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的展望")'
- en: '[7 Diffusion Models in Offline Policy Learning](#S7 "In Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7 离线策略学习中的扩散模型](#S7 "在 离线策略学习的深度生成模型：教程、调查与未来方向的展望")'
- en: '[7.1 Imitation Learning](#S7.SS1 "In 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.1 模仿学习](#S7.SS1 "在 7 离线策略学习中的扩散模型 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的展望")'
- en: '[7.1.1 Improving Policy Expressiveness in Imitation Learning with Diffusion
    Models](#S7.SS1.SSS1 "In 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.1.1 使用扩散模型提高模仿学习中的策略表达力](#S7.SS1.SSS1 "在 7.1 模仿学习 ‣ 7 离线策略学习中的扩散模型 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的展望")'
- en: '[7.1.2 Addressing Common Issues of Imitation Learning with Diffusion Models](#S7.SS1.SSS2
    "In 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.1.2 使用扩散模型解决模仿学习中的常见问题](#S7.SS1.SSS2 "在 7.1 模仿学习 ‣ 7 离线策略学习中的扩散模型 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的展望")'
- en: '[7.2 Offline Reinforcement Learning](#S7.SS2 "In 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.2 离线强化学习](#S7.SS2 "在 7 离线策略学习中的扩散模型 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的展望")'
- en: '[7.2.1 Diffusion Models as Policies](#S7.SS2.SSS1 "In 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.2.1 扩散模型作为策略](#S7.SS2.SSS1 "在 7.2 离线强化学习 ‣ 7 离线策略学习中的扩散模型 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的展望")'
- en: '[7.2.2 Diffusion Models as Planners](#S7.SS2.SSS2 "In 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.2.2 扩散模型作为规划器](#S7.SS2.SSS2 "在 7.2 离线强化学习 ‣ 7 离线策略学习中的扩散模型 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的展望")'
- en: '[7.2.3 Diffusion Models as Data Synthesizers](#S7.SS2.SSS3 "In 7.2 Offline
    Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.2.3 扩散模型作为数据合成器](#S7.SS2.SSS3 "在 7.2 离线强化学习 ‣ 7 离线策略学习中的扩散模型 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的展望")'
- en: '[7.2.4 Diffusion Models in Extended Offline Reinforcement Learning Setups](#S7.SS2.SSS4
    "In 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.2.4 扩散模型在扩展离线强化学习设置中的应用](#S7.SS2.SSS4 "在 7.2 离线强化学习 ‣ 7 离线策略学习中的扩散模型 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的展望")'
- en: '[8 Discussions and Open Problems](#S8 "In Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8 讨论与开放问题](#S8 "在 离线策略学习的深度生成模型：教程、调查与未来方向的展望")'
- en: '[8.1 Discussions on Deep Generative Models and Offline Policy Learning](#S8.SS1
    "In 8 Discussions and Open Problems ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.1 深度生成模型与离线策略学习的讨论](#S8.SS1 "在 8 讨论和开放问题 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")'
- en: '[8.2 Perspectives on Future Directions](#S8.SS2 "In 8 Discussions and Open
    Problems ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.2 对未来方向的展望](#S8.SS2 "在 8 讨论和开放问题 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")'
- en: '[8.2.1 Future Works on Data-centric Research](#S8.SS2.SSS1 "In 8.2 Perspectives
    on Future Directions ‣ 8 Discussions and Open Problems ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.2.1 数据中心研究的未来工作](#S8.SS2.SSS1 "在 8.2 对未来方向的展望 ‣ 8 讨论和开放问题 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")'
- en: '[8.2.2 Future Works on Benchmarking](#S8.SS2.SSS2 "In 8.2 Perspectives on Future
    Directions ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.2.2 基准测试的未来工作](#S8.SS2.SSS2 "在 8.2 对未来方向的展望 ‣ 8 讨论和开放问题 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")'
- en: '[8.2.3 Future Works on Theories](#S8.SS2.SSS3 "In 8.2 Perspectives on Future
    Directions ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.2.3 理论的未来工作](#S8.SS2.SSS3 "在 8.2 对未来方向的展望 ‣ 8 讨论和开放问题 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")'
- en: '[8.2.4 Future Works on Algorithm Designs](#S8.SS2.SSS4 "In 8.2 Perspectives
    on Future Directions ‣ 8 Discussions and Open Problems ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.2.4 算法设计的未来工作](#S8.SS2.SSS4 "在 8.2 对未来方向的展望 ‣ 8 讨论和开放问题 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")'
- en: '[9 Conclusion](#S9 "In Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9 结论](#S9 "在离线策略学习的深度生成模型：教程、调查和未来方向的展望")'
- en: 1 Introduction
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Offline Policy Learning is a machine learning discipline that leverages pre-existing,
    static datasets to learn effective policies for (robotic) control or decision-making.
    This paper delves into its two primary branches: Offline Reinforcement Learning
    (Offline RL) and Imitation Learning (IL). Offline RL utilizes a pre-compiled batch
    of experience data collected from other policies or human operators, typically
    comprising a series of state-action-reward-next state tuples. The principal objective
    of offline RL is to develop a policy that maximizes the cumulative rewards, which
    may necessitate deviating from the behavior patterns observed in the training
    data. On the other hand, IL trains a policy by mimicking an expert’s behaviors.
    The data used for IL should be demonstrated trajectories from experts. These trajectories,
    highlighting the expert’s responses to various scenarios, usually consist of a
    sequence of state-action pairs (Learning from Demonstrations, LfD) or state-next
    state pairs (Learning from Observations, LfO).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 离线策略学习是一个利用现有静态数据集来学习有效（机器人）控制或决策的机器学习学科。本文*深入探讨*了其两个主要分支：离线强化学习（Offline RL）和模仿学习（IL）。离线强化学习利用从其他策略或人类操作员那里收集的预编译经验数据批次，通常包括一系列状态-动作-奖励-下一个状态的元组。离线强化学习的主要目标是开发一个最大化累积奖励的策略，这可能需要偏离训练数据中观察到的行为模式。另一方面，模仿学习通过模仿专家的行为来训练策略。用于模仿学习的数据应来自专家的演示轨迹。这些轨迹突出了专家对各种情境的反应，通常由一系列状态-动作对（学习自演示，LfD）或状态-下一个状态对（学习自观察，LfO）组成。
- en: Generative Models are widely used in many subfields of AI and machine learning.
    Recent advances in parameterizing these models using deep neural networks, combined
    with progress in stochastic optimization methods, have enabled scalable modeling
    of complex, high-dimensional data including images, texts, and speech, with Deep
    Generative Models (DGMs). In particular, we notice the great success of applying
    DGMs to Computer Vision (CV) and Natural Language Processing (NLP). Examples include
    text-to-image generation using Diffusion Models (Stable Diffusion (Rombach et al.
    ([2022](#bib.bib278)))), text-to-video generation with Diffusion Models and Transformers
    (Sora (Brooks et al. ([2024](#bib.bib33)))), large language models based on Transformers
    (ChatGPT (OpenAI ([2023](#bib.bib236)))), and so on. Offline policy learning bears
    similarities to CV and NLP, as all these domains involve learning from a set of
    offline data. While CV or NLP models generate images or texts based on given contexts,
    offline RL/IL models produce actions or trajectories conditioned on task scenarios.
    This similarity suggests that applying DGMs to offline policy learning could potentially
    replicate the success witnessed in CV and NLP, leveraging their capability to
    model and generate complex data patterns.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型在人工智能和机器学习的许多子领域中被广泛使用。最近，通过深度神经网络对这些模型进行参数化的进展，加上随机优化方法的进展，使得使用深度生成模型（DGMs）可以对复杂的高维数据（包括图像、文本和语音）进行可扩展建模。特别是，我们注意到将DGMs应用于计算机视觉（CV）和自然语言处理（NLP）方面取得了巨大的成功。例如，使用扩散模型（Stable
    Diffusion (Rombach et al. ([2022](#bib.bib278)))) 进行文本到图像生成，使用扩散模型和变换器（Sora (Brooks
    et al. ([2024](#bib.bib33)))) 进行文本到视频生成，基于变换器的大型语言模型（ChatGPT (OpenAI ([2023](#bib.bib236))))，等等。离线策略学习与CV和NLP有相似之处，因为所有这些领域都涉及从一组离线数据中学习。尽管CV或NLP模型根据给定的上下文生成图像或文本，但离线RL/IL模型则根据任务场景生成动作或轨迹。这种相似性表明，将DGMs应用于离线策略学习可能会复制在CV和NLP中取得的成功，利用其建模和生成复杂数据模式的能力。
- en: 'Great advances have been made in both DGMs and offline policy learning, and
    there are numerous works on applying the progress of DGMs to aid the development
    of offline policy learning. This paper provides a comprehensive review of DGMs
    in offline policy learning. Specifically, the main content is categorized by the
    type of DGMs, and we cover nearly all mainstream DGMs in this paper, including
    Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows,
    Transformers, and Diffusion Models, where Transformers and Diffusion Models are
    representatives of autoregressive and score-based generative models, respectively.
    For each category, we present related works in both offline RL and IL as two subcategories.
    In each subcategory (e.g., VAE-based IL in Section [3.2](#S3.SS2 "3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), we abstract the core schemes of applying that DGM in IL or offline
    RL, categorize related works by how the DGM is utilized, and provide a summary
    table outlining the representative works with their key novelties and evaluation
    benchmarks. It is noteworthy that our paper is more than a survey. For the introduction
    of each work, we try to include its key insights and objective design, to help
    readers get the knowledge without referring to the original papers. Further, for
    each group of works (e.g., Section [3.1.5](#S3.SS1.SSS5 "3.1.5 Offline Multi-task/Hierarchical
    RL based on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") and [7.2.2](#S7.SS2.SSS2
    "7.2.2 Diffusion Models as Planners ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")), algorithms
    are introduced in different levels of details, where the most representative ones
    are emphasized as a tutorial on that specific usage of DGMs for offline policy
    learning and then other extension works are presented for a thorough review. Specifically,
    research works in the same group are presented with unified notations, aiming
    to elucidate the relationships between their objectives and the evolution of their
    algorithm designs.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '在DGMs和离线策略学习方面已经取得了重大进展，并且有大量研究将DGMs的进展应用于促进离线策略学习的发展。本文提供了关于DGMs在离线策略学习中的综合评审。具体来说，主要内容按照DGM的类型进行分类，我们在本文中涵盖了几乎所有主流的DGMs，包括变分自编码器（Variational
    Auto-Encoders）、生成对抗网络（Generative Adversarial Networks）、归一化流（Normalizing Flows）、变换器（Transformers）和扩散模型（Diffusion
    Models），其中变换器和扩散模型分别代表了自回归和基于评分的生成模型。对于每一类，我们将相关工作分为离线强化学习（RL）和模仿学习（IL）两个子类别。在每个子类别（例如，第[3.2节](#S3.SS2
    "3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")），我们提炼了应用该DGM于IL或离线RL的核心方案，按照DGM的利用方式对相关工作进行分类，并提供了一个总结表，概述了代表性工作及其关键创新和评估基准。值得注意的是，我们的论文不仅仅是一个综述。对于每项工作的介绍，我们尽力包含其关键见解和客观设计，以帮助读者在不参考原文的情况下获取知识。此外，对于每组工作（例如，第[3.1.5节](#S3.SS1.SSS5
    "3.1.5 Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")和[7.2.2节](#S7.SS2.SSS2 "7.2.2 Diffusion Models as Planners ‣ 7.2 Offline
    Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")），算法在不同的细节层次上介绍，其中最具代表性的被强调为离线策略学习的DGM特定用法的教程，然后其他扩展工作被呈现以进行全面审查。具体来说，相同组别的研究工作以统一符号表示，旨在阐明其目标之间的关系以及算法设计的演变。'
- en: 'In the order of Variational Auto-Encoders, Generative Adversarial Networks,
    Normalizing Flows, Transformers, and Diffusion Models, we present the background
    on DGMs in Section [2](#S2 "2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") and the applications of DGMs for offline policy learning in Section
    [3](#S3 "3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") - [7](#S7 "7 Diffusion Models in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"). For the background of each DGM, we introduce its mathematical basics
    and model variants that are utilized in offline policy learning. Sections related
    to the same DGM (e.g., Section [2.2](#S2.SS2 "2.2 Generative Adversarial Networks
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") and
    [4](#S4 "4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) share the same set of notations such that necessary background can
    be conveniently looked up. Regarding the background on offline RL and IL, we notice
    that different DGMs adopt distinct base offline RL/IL algorithms, so we distribute
    the background introductions to corresponding DGM sections. For instance, three
    main branches of offline RL: dynamic-programming-based, model-based, and trajectory-optimization-based
    offline RL, are adopted and they are respectively introduced in Section [3.1.1](#S3.SS1.SSS1
    "3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), [4.2.1](#S4.SS2.SSS1 "4.2.1 Background
    on Model-based Offline Reinforcement Learning ‣ 4.2 Offline Reinforcement Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), [6.1.1](#S6.SS1.SSS1 "6.1.1 Background on Trajectory-Optimization-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"). Following the main
    content, in Section [8](#S8 "8 Discussions and Open Problems ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), we present in-depth discussions on the main topic of this paper
    and provide perspectives on future research directions. For the discussions (Section
    [8.1](#S8.SS1 "8.1 Discussions on Deep Generative Models and Offline Policy Learning
    ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")), we analyze
    both the common and unique usages of different DGMs in offline policy learning,
    summarize the seminal works in each category and the issues/extensions of offline
    policy learning that have been targeted by DGM-based algorithms. For the future
    works (Section [8.2](#S8.SS2 "8.2 Perspectives on Future Directions ‣ 8 Discussions
    and Open Problems ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")), we offer our perspectives on
    potential research directions across four aspects: data, benchmarking, theories,
    and algorithm designs.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '按照变分自编码器、生成对抗网络、归一化流、变换器和扩散模型的顺序，我们在第[2](#S2 "2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")节中介绍了深度生成模型（DGM）的背景，并在第[3](#S3 "3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")节中介绍了DGM在离线策略学习中的应用
    - [7](#S7 "7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")。关于每种DGM的背景，我们介绍了其数学基础和在离线策略学习中使用的模型变体。相关的DGM章节（例如，第[2.2](#S2.SS2
    "2.2 Generative Adversarial Networks ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")节和[4](#S4 "4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")节）共享相同的符号，以便于查找必要的背景信息。关于离线强化学习（RL）和离线学习（IL）的背景，我们注意到不同的DGM采用了不同的基础离线RL/IL算法，因此我们将背景介绍分配到相应的DGM章节。例如，离线RL的三个主要分支：基于动态规划的、基于模型的和基于轨迹优化的离线RL，分别在第[3.1.1](#S3.SS1.SSS1
    "3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")节、第[4.2.1](#S4.SS2.SSS1 "4.2.1
    Background on Model-based Offline Reinforcement Learning ‣ 4.2 Offline Reinforcement
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")节、第[6.1.1](#S6.SS1.SSS1 "6.1.1 Background on Trajectory-Optimization-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")节中介绍。紧接着，在第[8](#S8 "8
    Discussions and Open Problems ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")节中，我们深入讨论了本文的主要话题，并提供了对未来研究方向的展望。在讨论部分（第[8.1](#S8.SS1
    "8.1 Discussions on Deep Generative Models and Offline Policy Learning ‣ 8 Discussions
    and Open Problems ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")节），我们分析了不同DGM在离线策略学习中的共同和独特用途，总结了每个类别中的开创性工作以及DGM算法针对离线策略学习所关注的问题/扩展。对于未来工作（第[8.2](#S8.SS2
    "8.2 Perspectives on Future Directions ‣ 8 Discussions and Open Problems ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")节），我们提供了在数据、基准测试、理论和算法设计四个方面的潜在研究方向的观点。'
- en: 'The main contributions of this paper are as follows: (1) This is the first
    review paper on Deep Generative Models for Offline Policy Learning. (2) This paper
    covers a wide array of topics, including five mainstream Deep Generative Models
    and their applications in both Offline Reinforcement Learning and Imitation Learning.
    (3) Throughout this paper, we distill key algorithmic schemes/paradigms and selectively
    highlight seminal research works, as tutorials on respective topics. (4) Our work
    showcases the evolution of DGM-based offline policy learning in parallel with
    the progress of generative models themselves. The summary provided at the end
    of this paper offers valuable insights and directions for future developments
    in this field.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献如下：（1）这是首篇关于离线策略学习的深度生成模型的综述论文。（2）本文涵盖了广泛的主题，包括五种主流的深度生成模型及其在离线强化学习和模仿学习中的应用。（3）在本文中，我们提炼了关键的算法方案/范式，并有选择地突出了一些开创性的研究工作，作为相关主题的教程。（4）我们的工作展示了基于
    DGM 的离线策略学习的发展与生成模型本身的进展并行。本文最后提供的总结为该领域的未来发展提供了有价值的见解和方向。
- en: 2 Background on Deep Generative Models
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度生成模型背景
- en: 'In this section, we introduce five widely-adopted and cutting-edge deep generative
    models: Variational Auto-Encoders, Generative Adversarial Networks, Normalizing
    Flows, Transformers, and Diffusion Models, as the background of their applications
    in offline policy learning. For each of these generative models, we delve into
    their mathematical foundations and provide an overview of their significant variants,
    with a particular focus on those employed in offline policy learning.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了五种广泛采用的前沿深度生成模型：变分自编码器、生成对抗网络、正则流、变换器和扩散模型，作为其在离线策略学习中的应用背景。对于每种生成模型，我们将深入探讨其数学基础，并概述其显著的变体，特别关注那些在离线策略学习中应用的变体。
- en: Considering the extensive scope of content addressed in this work, notations
    utilized for different generative models are relatively independent. Within the
    context of the same generative model, including its background and applications
    in offline policy learning, notations are consistent and can be cross-referenced.
    However, notations used across different generative models may not be directly
    comparable or interchangeable, but consistently, we use $x\sim P_{X}(\cdot)$ to
    represent data points in the offline dataset and $z\sim P_{Z}(\cdot)$ to represent
    their latent representations or variations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到本研究涵盖的内容范围广泛，不同生成模型所使用的符号相对独立。在同一生成模型的背景和在离线策略学习中的应用中，符号是一致的并且可以交叉引用。然而，不同生成模型之间使用的符号可能不能直接比较或互换，但我们始终使用
    $x\sim P_{X}(\cdot)$ 来表示离线数据集中的数据点，使用 $z\sim P_{Z}(\cdot)$ 来表示它们的潜在表示或变体。
- en: 2.1 Variational Auto-Encoders
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 变分自编码器
- en: Variational Auto-Encoders (VAEs, Kingma & Welling ([2014](#bib.bib165); [2019](#bib.bib166)))
    assume that the given data distribution $P_{X}(x)$ can be generated with a deep
    latent-variable model $P_{\theta^{*}}(x)=\int P_{\theta^{*}}(z)P_{\theta^{*}}(x|z)dz$,
    i.e., sampling a continuous latent variable $z$ from the prior $P_{\theta^{*}}(z)$
    and then generating $x$ from the conditional (generative) model $P_{\theta^{*}}(x|z)$.
    It’s also assumed that $P_{\theta^{*}}(z)$ ($P_{\theta^{*}}(x|z)$) comes from
    a parametric family of distributions $P_{\theta}(z)$ ($P_{\theta}(x|z)$). Intuitively,
    such $P_{\theta}(x)$ can be seen as an infinite mixture of some base probability
    distributions $P_{\theta}(x|z)$ (e.g., the Gaussian distribution) across a range
    of possible latent variables $z$, thus it can be quite flexible and powerful for
    modelling complex data distributions $P_{X}(x)$.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAEs，Kingma & Welling ([2014](#bib.bib165); [2019](#bib.bib166)））假设给定的数据分布
    $P_{X}(x)$ 可以通过深度潜变量模型 $P_{\theta^{*}}(x)=\int P_{\theta^{*}}(z)P_{\theta^{*}}(x|z)dz$
    来生成，即从先验 $P_{\theta^{*}}(z)$ 中采样一个连续的潜变量 $z$，然后通过条件（生成）模型 $P_{\theta^{*}}(x|z)$
    来生成 $x$。还假设 $P_{\theta^{*}}(z)$ ($P_{\theta^{*}}(x|z)$) 来自一个参数化的分布族 $P_{\theta}(z)$
    ($P_{\theta}(x|z)$)。直观上，这样的 $P_{\theta}(x)$ 可以看作是在一系列可能的潜变量 $z$ 上的某些基础概率分布 $P_{\theta}(x|z)$（例如高斯分布）的无限混合，因此它在建模复杂数据分布
    $P_{X}(x)$ 时非常灵活和强大。
- en: 'To learn the mapping from $z$ to $x$, an extra model that can infer the latent
    variables $z$ from the training data $x$ is required, as $z$’s are not provided
    with the offline data. However, according to the Bayes rule, the true posterior
    (inference) model $P_{\theta}(z|x)=P_{\theta}(z)P_{\theta}(x|z)/P_{\theta}(x)$
    is intractable in most cases. For example, if $P_{\theta}(x|z)$ is implemented
    as a neural network, we do not have the analytical form for $P_{\theta}(x)$ and
    so not for $P_{\theta}(z|x)$. Therefore, in VAEs, a generative model $P_{\theta}(x|z)$
    and corresponding inference model $P_{\phi}(z|x)$ (for approximating the true
    posterior) are jointly learned. The most notable contributions of VAEs are two-fold:
    firstly, they propose a variational lower bound as a practical learning objective,
    and secondly, they introduce the reparameterization trick, which facilitates end-to-end
    training of both the generative and inference models.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要学习从 $z$ 到 $x$ 的映射，需要一个额外的模型，该模型能够从训练数据 $x$ 中推断潜在变量 $z$，因为 $z$ 在离线数据中没有提供。然而，根据贝叶斯规则，真实的后验（推断）模型
    $P_{\theta}(z|x)=P_{\theta}(z)P_{\theta}(x|z)/P_{\theta}(x)$ 在大多数情况下是难以处理的。例如，如果
    $P_{\theta}(x|z)$ 是通过神经网络实现的，我们没有 $P_{\theta}(x)$ 的解析形式，因此也没有 $P_{\theta}(z|x)$
    的解析形式。因此，在变分自编码器（VAEs）中，生成模型 $P_{\theta}(x|z)$ 和相应的推断模型 $P_{\phi}(z|x)$（用于近似真实的后验）是联合学习的。VAEs
    最显著的贡献有两个方面：首先，它们提出了变分下界作为实际的学习目标，其次，它们引入了重新参数化技巧，这有助于生成模型和推断模型的端到端训练。
- en: 'The objective function of VAEs is a variational lower bound (a.k.a., evidence
    lower bound (ELBO)) of the log likelihood $\log P_{\theta}(x)$, and its derivation
    process is widely adopted in algorithm designs related to latent-variable models,
    so we show the process here:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs 的目标函数是对数似然 $\log P_{\theta}(x)$ 的变分下界（即证据下界（ELBO）），其推导过程在与潜在变量模型相关的算法设计中被广泛采用，因此我们在这里展示这一过程：
- en: '|  | $\displaystyle\log P_{\theta}(x)$ | $\displaystyle=\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x)}\left[\log
    P_{\theta}(x)\right]=\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x)}\left[\log\left[\frac{P_{\theta}(x,z)}{P_{\theta}(z&#124;x)}\right]\right]=\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;x)}\left[\log\left[\frac{P_{\theta}(x,z)}{P_{\phi}(z&#124;x)}\frac{P_{\phi}(z&#124;x)}{P_{\theta}(z&#124;x)}\right]\right]$
    |  | (1) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\log P_{\theta}(x)$ | $\displaystyle=\mathbb{E}_{z\sim P_{\phi}(\cdot|x)}\left[\log
    P_{\theta}(x)\right]=\mathbb{E}_{z\sim P_{\phi}(\cdot|x)}\left[\log\left[\frac{P_{\theta}(x,z)}{P_{\theta}(z|x)}\right]\right]=\mathbb{E}_{z\sim
    P_{\phi}(\cdot|x)}\left[\log\left[\frac{P_{\theta}(x,z)}{P_{\phi}(z|x)}\frac{P_{\phi}(z|x)}{P_{\theta}(z|x)}\right]\right]$
    |  | (1) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x)}\left[\log\left[\frac{P_{\theta}(x,z)}{P_{\phi}(z&#124;x)}\right]+\log\left[\frac{P_{\phi}(z&#124;x)}{P_{\theta}(z&#124;x)}\right]\right]=\mathcal{L}_{\theta,\phi}(x)+D_{KL}(P_{\phi}(z&#124;x)&#124;&#124;P_{\theta}(z&#124;x))\geq\mathcal{L}_{\theta,\phi}(x)$
    |  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{z\sim P_{\phi}(\cdot|x)}\left[\log\left[\frac{P_{\theta}(x,z)}{P_{\phi}(z|x)}\right]+\log\left[\frac{P_{\phi}(z|x)}{P_{\theta}(z|x)}\right]\right]=\mathcal{L}_{\theta,\phi}(x)+D_{KL}(P_{\phi}(z|x)||P_{\theta}(z|x))\geq\mathcal{L}_{\theta,\phi}(x)$
    |  |'
- en: '|  | $\displaystyle\mathcal{L}_{\theta,\phi}(x)$ | $\displaystyle=\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;x)}\left[\log P_{\theta}(x,z)-\log P_{\phi}(z&#124;x)\right]=\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;x)}\left[\log P_{\theta}(x&#124;z)-\log\left[\frac{P_{\phi}(z&#124;x)}{P_{\theta}(z)}\right]\right]$
    |  | (2) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\theta,\phi}(x)$ | $\displaystyle=\mathbb{E}_{z\sim
    P_{\phi}(\cdot|x)}\left[\log P_{\theta}(x,z)-\log P_{\phi}(z|x)\right]=\mathbb{E}_{z\sim
    P_{\phi}(\cdot|x)}\left[\log P_{\theta}(x|z)-\log\left[\frac{P_{\phi}(z|x)}{P_{\theta}(z)}\right]\right]$
    |  | (2) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x)}\left[\log
    P_{\theta}(x&#124;z)\right]-D_{KL}(P_{\phi}(z&#124;x)&#124;&#124;P_{\theta}(z))$
    |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{z\sim P_{\phi}(\cdot|x)}\left[\log P_{\theta}(x|z)\right]-D_{KL}(P_{\phi}(z|x)||P_{\theta}(z))$
    |  |'
- en: 'The inequality in Eq. ([1](#S2.E1 "In 2.1 Variational Auto-Encoders ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) uses the fact that
    KL divergence (i.e., $D_{KL}(\cdot||\cdot)$) is non-negative (Csiszár & Shields
    ([2004](#bib.bib64))). $P_{\phi}$ and $P_{\theta}$ are learned by maximizing $\mathbb{E}_{x\sim
    P_{X}(\cdot)}\mathcal{L}_{\theta,\phi}(x)$, which in turns maximizes $\mathbb{E}_{x\sim
    P_{X}(\cdot)}\log P_{\theta}(x)$. When using neural networks for both the inference
    model $P_{\phi}(z|x)$ and generative model $P_{\theta}(x|z)$, $\mathcal{L}_{\theta,\phi}(x)$
    trains a specific type of auto-encoder, where $P_{\phi}(z|x)$ and $P_{\theta}(x|z)$
    function as the encoder and decoder respectively. Intuitively, this auto-encoder
    is trained by maximizing the data reconstruction accuracy (i.e., the first term
    in $\mathcal{L}_{\theta,\phi}(x)$), while regularizing the distribution of $z$
    from the encoder to be close to the prior distribution of $z$. This regularization
    is necessary, since $z$ is sampled from the encoder for data generation (i.e.,
    $z\sim P_{\phi}(\cdot|x),\hat{x}\sim P_{\theta}(\cdot|z)$) when training but,
    during evaluation, $z$ is sampled from the prior distribution (i.e., $z\sim P_{\theta}(\cdot),\hat{x}\sim
    P_{\theta}(\cdot|z)$).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 方程中的不等式 ([1](#S2.E1 "在 2.1 变分自编码器 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望"))
    利用了KL散度（即 $D_{KL}(\cdot||\cdot)$）非负的事实（Csiszár & Shields ([2004](#bib.bib64)））。通过最大化
    $\mathbb{E}_{x\sim P_{X}(\cdot)}\mathcal{L}_{\theta,\phi}(x)$ 来学习 $P_{\phi}$ 和
    $P_{\theta}$，这反过来又最大化了 $\mathbb{E}_{x\sim P_{X}(\cdot)}\log P_{\theta}(x)$。当对推断模型
    $P_{\phi}(z|x)$ 和生成模型 $P_{\theta}(x|z)$ 使用神经网络时，$\mathcal{L}_{\theta,\phi}(x)$
    训练一种特定类型的自编码器，其中 $P_{\phi}(z|x)$ 和 $P_{\theta}(x|z)$ 分别作为编码器和解码器。直观地说，这种自编码器通过最大化数据重建准确性（即
    $\mathcal{L}_{\theta,\phi}(x)$ 中的第一项）来进行训练，同时对 $z$ 的分布进行正则化，使其从编码器到 $z$ 的先验分布接近。这种正则化是必要的，因为
    $z$ 在训练时是从编码器中采样用于数据生成的（即 $z\sim P_{\phi}(\cdot|x),\hat{x}\sim P_{\theta}(\cdot|z)$），而在评估过程中，$z$
    是从先验分布中采样的（即 $z\sim P_{\theta}(\cdot),\hat{x}\sim P_{\theta}(\cdot|z)$）。
- en: 'Usually, VAEs assume that $P_{\theta}(z)=\mathcal{N}(z;0,I)$ and $P_{\phi}(z|x)=\mathcal{N}(z;\mu(x;\phi),\text{diag}(\sigma^{2}(x;\phi)))$.
    Here, assuming $z\in\mathbb{R}^{d}$, $I$ is a $d\times d$ identity matrix, $\mu(x;\phi)\in\mathbb{R}^{d}$
    is a mean vector, and $\text{diag}(\sigma^{2}(x;\phi))\in\mathbb{R}^{d\times d}$
    is a diagonal covariance matrix with $\sigma^{2}_{1:d}(x;\phi)$ as the diagonal
    elements. In this case, the analytical form of $D_{KL}(P_{\phi}(z|x)||P_{\theta}(z))$
    exists and we have:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，变分自编码器（VAEs）假设 $P_{\theta}(z)=\mathcal{N}(z;0,I)$ 和 $P_{\phi}(z|x)=\mathcal{N}(z;\mu(x;\phi),\text{diag}(\sigma^{2}(x;\phi)))$。这里，假设
    $z\in\mathbb{R}^{d}$，$I$ 是一个 $d\times d$ 的单位矩阵，$\mu(x;\phi)\in\mathbb{R}^{d}$
    是均值向量，而 $\text{diag}(\sigma^{2}(x;\phi))\in\mathbb{R}^{d\times d}$ 是一个对角协方差矩阵，其中
    $\sigma^{2}_{1:d}(x;\phi)$ 是对角元素。在这种情况下，$D_{KL}(P_{\phi}(z|x)||P_{\theta}(z))$
    的解析形式存在，我们有：
- en: '|  | $\mathcal{L}_{\theta,\phi}(x)=\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x)}\left[\log
    P_{\theta}(x&#124;z)\right]+\frac{1}{2}\sum_{i=1}^{d}\left[1+\log\sigma_{i}^{2}(x;\phi)-\mu_{i}^{2}(x;\phi)-\sigma_{i}^{2}(x;\phi)\right]=\mathcal{L}_{\theta,\phi}^{1}(x)+\mathcal{L}_{\phi}^{2}(x)$
    |  | (3) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\theta,\phi}(x)=\mathbb{E}_{z\sim P_{\phi}(\cdot\mid x)}\left[\log
    P_{\theta}(x\mid z)\right]+\frac{1}{2}\sum_{i=1}^{d}\left[1+\log\sigma_{i}^{2}(x;\phi)-\mu_{i}^{2}(x;\phi)-\sigma_{i}^{2}(x;\phi)\right]=\mathcal{L}_{\theta,\phi}^{1}(x)+\mathcal{L}_{\phi}^{2}(x)$
    |  | (3) |'
- en: $\nabla_{\phi}\mathcal{L}_{\phi}^{2}(x)$ and $\nabla_{\theta}\mathcal{L}_{\theta,\phi}^{1}(x)$
    can be easily estimated with gradient backpropagation. However, when calculating
    $\nabla_{\phi}\mathcal{L}_{\theta,\phi}^{1}(x)$, the gradient $\nabla_{z}\log
    P_{\theta}(x|z)$ cannot be backpropagated to $\phi$, since $z$ are samples from
    the encoder rather than a function of $\phi$. One solution is the reparameterization
    trick. Specifically, $z$ can be reparameterized as a deterministic function of
    $\phi$ by externalizing the randomness from the output to the input of the encoder,
    i.e., $z=P_{\phi}(x,\epsilon)=\mu(x;\phi)+\sigma(x;\phi)\odot\epsilon,\ \epsilon\sim\mathcal{N}(0,I)$.
    ($\epsilon\in\mathbb{R}^{d}$, and $\odot$ denotes the element-wise product.) In
    this way, the generative models and corresponding inference models can be jointly
    trained using straightforward learning techniques (e.g., stochastic gradient descent
    (Amari ([1993](#bib.bib10)))).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: $\nabla_{\phi}\mathcal{L}_{\phi}^{2}(x)$ 和 $\nabla_{\theta}\mathcal{L}_{\theta,\phi}^{1}(x)$
    可以通过梯度反向传播轻松估计。然而，在计算 $\nabla_{\phi}\mathcal{L}_{\theta,\phi}^{1}(x)$ 时，梯度 $\nabla_{z}\log
    P_{\theta}(x|z)$ 无法反向传播到 $\phi$，因为 $z$ 是从编码器中采样得到的，而不是 $\phi$ 的函数。一种解决方案是重参数化技巧。具体来说，$z$
    可以通过将随机性从编码器的输出转移到输入来重新参数化为 $\phi$ 的确定性函数，即 $z=P_{\phi}(x,\epsilon)=\mu(x;\phi)+\sigma(x;\phi)\odot\epsilon,\
    \epsilon\sim\mathcal{N}(0,I)$。($\epsilon\in\mathbb{R}^{d}$，并且 $\odot$ 表示逐元素乘积。)
    这样，生成模型和对应的推断模型可以使用简单的学习技术（例如，随机梯度下降（Amari ([1993](#bib.bib10)))) 进行联合训练。
- en: 'Next, we introduce some representative variants of VAEs, which are utilized
    in Section [3](#S3 "3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions").'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们介绍一些变分自编码器（VAE）的代表性变体，这些变体在第 [3](#S3 "3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") 节中进行了讨论。'
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\beta$-VAE: Assuming that the data $x\sim P_{X}(\cdot)$ is generated by a
    groundtruth simulation process with a number of independent generative factors,
    $\beta$-VAE (Higgins et al. ([2017](#bib.bib131))) is proposed to learn latent
    variables $z$ that encode each generative factor in separate dimensions. For example,
    a simulator might sample independent factors corresponding to the object shape,
    colour, and size to generate an image of a small green apple; ideally, in the
    latent variable of the image, there should be separate dimensions for each of
    these factors. To learn such an encoder $P_{\phi}(z|x)$, the authors propose to
    constrain the KL divergence between $P_{\phi}(z|x)$ and an isotropic unit Gaussian
    distribution $P_{\theta}(z)=\mathcal{N}(z;0,I)$ to encourage the dimensions of
    $z$ to be conditionally independent, resulting in the objective:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\beta$-VAE：假设数据 $x\sim P_{X}(\cdot)$ 是由具有多个独立生成因子的真实模拟过程生成的，$\beta$-VAE（Higgins
    et al. ([2017](#bib.bib131))) 被提出用于学习潜在变量 $z$，使每个生成因子在不同维度上编码。例如，模拟器可能会采样与对象形状、颜色和大小对应的独立因子，以生成一个小绿色苹果的图像；理想情况下，在图像的潜在变量中，应该为这些因子中的每一个提供单独的维度。为了学习这样的编码器
    $P_{\phi}(z|x)$，作者建议约束 $P_{\phi}(z|x)$ 和各向同性单位高斯分布 $P_{\theta}(z)=\mathcal{N}(z;0,I)$
    之间的 KL 散度，以鼓励 $z$ 的维度条件独立，从而得到目标：
- en: '|  | $\max_{\theta,\phi}\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x)}\left[\log
    P_{\theta}(x&#124;z)\right]\ s.t.\ D_{KL}(P_{\phi}(z&#124;x)&#124;&#124;P_{\theta}(z))<\epsilon$
    |  | (4) |'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\max_{\theta,\phi}\mathbb{E}_{z\sim P_{\phi}(\cdot|x)}\left[\log P_{\theta}(x|z)\right]\
    s.t.\ D_{KL}(P_{\phi}(z|x)||P_{\theta}(z))<\epsilon$ |  | (4) |'
- en: 'Introducing a Lagrangian multiplier $\beta>0$, the equation above can be reformulated
    into an unconstrained optimization problem: $\max_{\theta,\phi,\beta}\mathbb{E}_{z\sim
    P_{\phi}(\cdot|x)}\left[\log P_{\theta}(x|z)\right]-\beta\left[D_{KL}(P_{\phi}(z|x)||P_{\theta}(z))-\epsilon\right]$.
    They opt to fine-tune $\beta$ as a hyperparameter instead of learning it, leading
    to the final objective:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 引入拉格朗日乘子 $\beta>0$，上述方程可以重新表述为一个无约束优化问题：$\max_{\theta,\phi,\beta}\mathbb{E}_{z\sim
    P_{\phi}(\cdot|x)}\left[\log P_{\theta}(x|z)\right]-\beta\left[D_{KL}(P_{\phi}(z|x)||P_{\theta}(z))-\epsilon\right]$。他们选择将
    $\beta$ 调整为超参数，而不是学习它，从而得到最终目标：
- en: '|  | $\max_{\theta,\phi}\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x)}\left[\log
    P_{\theta}(x&#124;z)\right]-\beta D_{KL}(P_{\phi}(z&#124;x)&#124;&#124;P_{\theta}(z))$
    |  | (5) |'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\max_{\theta,\phi}\mathbb{E}_{z\sim P_{\phi}(\cdot|x)}\left[\log P_{\theta}(x|z)\right]-\beta
    D_{KL}(P_{\phi}(z|x)||P_{\theta}(z))$ |  | (5) |'
- en: 'When $\beta=1$, this recovers the original VAE formulation (i.e., Eq. ([2](#S2.E2
    "In 2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))). $\beta$ controls the tradeoff between limiting the latent
    channel capacity (via the second term) and preserving the information for reconstructing
    the data sample (via the first term). Empirically, they find that it’s important
    to set $\beta>1$ in order to learn the required disentangled latent variables.'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当$\beta=1$时，这恢复了原始的VAE公式（即公式 ([2](#S2.E2 "在2.1变分自编码器 ‣ 2 深度生成模型背景 ‣ 用于离线策略学习的深度生成模型：教程、调查以及对未来方向的展望"))）。$\beta$控制着限制潜在通道容量（通过第二项）与保留重建数据样本信息（通过第一项）之间的权衡。经验上，他们发现为了学习所需的解耦潜在变量，设置$\beta>1$是很重要的。
- en: •
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Gated VAE: As mentioned in $\beta$-VAE, disentangled latent variables can be
    viewed as data representations in which each element relates to an independent
    and semantically meaningful generator factor of the data. $\beta$-VAE provides
    an unsupervised manner to learn such disentangled representations (through constraining
    the KL divergence). However, consistent disentanglement has recently been demonstrated
    to be impossible without inductive bias or subjective validation (Locatello et al.
    ([2019](#bib.bib198))). Thus, Gated VAE (Vowels et al. ([2020](#bib.bib334)))
    is proposed to incorporate domain knowledge as weak supervision to encourage disentanglement.
    Specifically, as a form of weak supervision, data can be clustered such that each
    cluster comprises data sharing certain generative factors. Then, the latent variable
    $z$ can be split into several partitions, each of which capture the generative
    factors of a specific cluster. Suppose that $x_{1}$ and $x_{2}$ are from the same
    cluster which is assigned to the $i$-th partition of the latent variable, i.e.,
    $z_{i}$, the encoder and decoder can then be trained with a modified ELBO: (which
    is a lower bound of $\log P_{\theta}(x_{2})$)'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 门控变分自编码器（Gated VAE）：如同在$\beta$-VAE中提到的，解耦的潜在变量可以被视为数据的表示形式，其中每个元素与数据的独立且具有语义意义的生成因子相关。$\beta$-VAE提供了一种无监督的方法来学习这种解耦表示（通过限制KL散度）。然而，最近的研究表明，缺乏归纳偏置或主观验证的情况下，始终如一的解耦是不可能实现的（Locatello
    et al. ([2019](#bib.bib198)))。因此，门控变分自编码器（Vowels et al. ([2020](#bib.bib334)))被提出以结合领域知识作为弱监督来鼓励解耦。具体来说，作为一种弱监督的形式，数据可以被聚类，使得每个簇包含具有某些生成因子的共享数据。然后，潜在变量$z$可以被划分为几个部分，每个部分捕捉特定簇的生成因子。假设$x_{1}$和$x_{2}$来自同一簇，该簇被分配到潜在变量的第$i$个部分，即$z_{i}$，编码器和解码器可以通过修改的ELBO进行训练：（这是$\log
    P_{\theta}(x_{2})$的下界）
- en: '|  | $\mathcal{L}_{\theta,\phi}(x_{1},x_{2})=\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x_{1})}\left[\log
    P_{\theta}(x_{2}&#124;z)\right]-D_{KL}(P_{\phi}(z&#124;x_{1})&#124;&#124;P_{\theta}(z))$
    |  | (6) |'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\theta,\phi}(x_{1},x_{2})=\mathbb{E}_{z\sim P_{\phi}(\cdot\vert
    x_{1})}\left[\log P_{\theta}(x_{2}\vert z)\right]-D_{KL}(P_{\phi}(z\vert x_{1})\Vert
    P_{\theta}(z))$ |  | (6) |'
- en: Note that the entire $z$ is utilized to generate $x_{2}$ from $x_{1}$, but only
    gradients corresponding to $z_{i}$ are backpropagated to the inference model $P_{\phi}$.
    The rationale is that $x_{2}$ is intended to reconstruct only the shared generative
    factors with $x_{1}$, and these factors should exclusively be captured by $z_{i}$.
    Gated VAE requires domain knowledge as supervision, which may limit its general
    applicability.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，整个$z$都被用于从$x_{1}$生成$x_{2}$，但只有与$z_{i}$对应的梯度被反向传播到推断模型$P_{\phi}$。其理由是$x_{2}$旨在仅重建与$x_{1}$共享的生成因子，而这些因子应由$z_{i}$独占捕获。门控变分自编码器需要领域知识作为监督，这可能会限制其通用性。
- en: •
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CVAE: Conditional VAE (CVAE, Sohn et al. ([2015](#bib.bib301))) is a model
    for conditional generation. Given a set of data $x$ and their corresponding conditional
    variables $c$ (e.g., classification labels or desired attributes of the data),
    CVAE learns a conditional prior $P_{\theta}(z|c)$ and a conditional generative
    model $P_{\theta}(x|z,c)$, so that a data sample $x$ that satisfies a certain
    condition $c$ can be generated via $z\sim P_{\theta}(\cdot|c),x\sim P_{\theta}(\cdot|z,c)$.
    CVAE is particularly useful in tasks requiring controlled data generation, like
    scenarios where manipulating specific attributes of the generated data is necessary.
    As in VAEs, an inference model $P_{\phi}(z|x,c)$ is introduced for the learning
    process, which gives the ELBO:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CVAE：条件变分自编码器（CVAE, Sohn et al. ([2015](#bib.bib301))) 是一种用于条件生成的模型。给定一组数据 $x$
    及其相应的条件变量 $c$（例如分类标签或数据的期望属性），CVAE 学习条件先验 $P_{\theta}(z|c)$ 和条件生成模型 $P_{\theta}(x|z,c)$，以便通过
    $z\sim P_{\theta}(\cdot|c),x\sim P_{\theta}(\cdot|z,c)$ 生成满足特定条件 $c$ 的数据样本 $x$。CVAE
    在需要受控数据生成的任务中特别有用，例如在需要操控生成数据的特定属性的场景中。与 VAEs 类似，引入了一个推断模型 $P_{\phi}(z|x,c)$ 用于学习过程，其给出
    ELBO：
- en: '|  | $\mathcal{L}_{\theta,\phi}(x,c)=\mathbb{E}_{z\sim P_{\phi}(\cdot&#124;x,c)}\left[\log
    P_{\theta}(x&#124;z,c)\right]-D_{KL}(P_{\phi}(z&#124;x,c)&#124;&#124;P_{\theta}(z&#124;c))$
    |  | (7) |'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\theta,\phi}(x,c)=\mathbb{E}_{z\sim P_{\phi}(\cdot|x,c)}\left[\log
    P_{\theta}(x|z,c)\right]-D_{KL}(P_{\phi}(z|x,c)||P_{\theta}(z|c))$ |  | (7) |'
- en: 'Compared with the original VAE formulation, a conditional variable $c$ is introduced
    to each function, i.e., the prior, inference, and generative model. Following
    a derivation process similar with Eq. ([1](#S2.E1 "In 2.1 Variational Auto-Encoders
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), it
    can be shown that Eq. ([7](#S2.E7 "In 3rd item ‣ 2.1 Variational Auto-Encoders
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) is
    a variational lower bound for $\log P_{\theta}(x|c)$, i.e., the conditional generation
    likelihood. Further, they note that, during evaluation, latent variables are sampled
    from $P_{\theta}(z|c)$ rather than $P_{\phi}(z|x,c)$, as only the conditions $c$
    are available. Thus, to enhance consistency of the data generation process during
    training and evaluation, they introduce an additional objective term to Eq. ([7](#S2.E7
    "In 3rd item ‣ 2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")):'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '与原始的 VAE 公式相比，在每个函数中引入了一个条件变量 $c$，即先验、推断和生成模型。按照类似于公式 ([1](#S2.E1 "In 2.1 Variational
    Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))
    的推导过程，可以证明公式 ([7](#S2.E7 "In 3rd item ‣ 2.1 Variational Auto-Encoders ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) 是 $\log P_{\theta}(x|c)$
    的变分下界，即条件生成似然。进一步，他们指出，在评估过程中，潜在变量是从 $P_{\theta}(z|c)$ 中采样的，而不是 $P_{\phi}(z|x,c)$，因为只有条件
    $c$ 是可用的。因此，为了提高训练和评估期间数据生成过程的一致性，他们在公式 ([7](#S2.E7 "In 3rd item ‣ 2.1 Variational
    Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))
    中引入了一个额外的目标项：'
- en: '|  | $\mathcal{L}_{\theta,\phi}^{\text{hybrid}}(x,c)=\alpha\mathcal{L}_{\theta,\phi}(x,c)+(1-\alpha)\mathbb{E}_{z\sim
    P_{\theta}(\cdot&#124;c)}\left[\log P_{\theta}(x&#124;z,c)\right]$ |  | (8) |'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\theta,\phi}^{\text{hybrid}}(x,c)=\alpha\mathcal{L}_{\theta,\phi}(x,c)+(1-\alpha)\mathbb{E}_{z\sim
    P_{\theta}(\cdot|c)}\left[\log P_{\theta}(x|z,c)\right]$ |  | (8) |'
- en: •
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VQ-VAE: Unlike aforementioned VAE variants where $z$ is continuous, VQ-VAE
    (van den Oord et al. ([2017](#bib.bib326))) is an auto-encoder framework utilizing
    discrete latent variables. In particular, they assume the latent variable space
    is a codebook $[e_{1}^{T},\cdots,e_{k}^{T}]\in\mathbb{R}^{k\times d}$, where $k$
    is number of categories and $d$ is the dimension of the latent vector for each
    category (i.e., $e_{i}$). Such discrete representations are usually easier to
    model than continuous ones and can be particularly advantageous in some situations.
    For example, in a dataset of images with several categories, categorized latent
    representations are more appropriate, where each category is assigned a code from
    the codebook. In particular, the forward process of VQ-VAE is: $z_{E}=P_{\phi}(x),z_{D}=e_{i}\
    (i=\arg\min_{j}||z_{E}-e_{j}||_{2}),\hat{x}\sim P_{\theta}(\cdot|z_{D})$. Here,
    the latent variable from the encoder, i.e., $z_{E}$, is mapped to its nearest
    neighbour in the codebook, i.e., $z_{D}$, which is then input to the decoder for
    data generation. The learnable parameters of VQ-VAE include $\theta$, $\phi$,
    and $e_{1:k}$, for which the objective is as below: ($z_{D}$ and $e_{i}$ are defined
    as above; sg denotes the stop-gradient operator.)'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VQ-VAE：与前述的VAE变体中$z$是连续的不同，VQ-VAE（van den Oord等人 ([2017](#bib.bib326))) 是一个利用离散潜在变量的自编码器框架。具体来说，他们假设潜在变量空间是一个代码本$[e_{1}^{T},\cdots,e_{k}^{T}]\in\mathbb{R}^{k\times
    d}$，其中$k$是类别数，$d$是每个类别的潜在向量的维度（即$e_{i}$）。这种离散表示通常比连续表示更易于建模，在某些情况下特别有利。例如，在具有多个类别的图像数据集中，按类别划分的潜在表示更为合适，其中每个类别都分配了一个来自代码本的代码。具体来说，VQ-VAE的前向过程是：$z_{E}=P_{\phi}(x),z_{D}=e_{i}\
    (i=\arg\min_{j}||z_{E}-e_{j}||_{2}),\hat{x}\sim P_{\theta}(\cdot|z_{D})$。这里，来自编码器的潜在变量，即$z_{E}$，被映射到代码本中的最近邻，即$z_{D}$，然后输入解码器以生成数据。VQ-VAE的可学习参数包括$\theta$、$\phi$和$e_{1:k}$，其目标如下：（$z_{D}$和$e_{i}$如上定义；sg表示停止梯度操作符。）
- en: '|  | $\mathcal{L}_{\theta,\phi,e_{1:k}}(x)=\log P_{\theta}(x&#124;z_{D})-&#124;&#124;\text{sg}\left[P_{\phi}(x)\right]-e_{i}&#124;&#124;_{2}^{2}-\beta&#124;&#124;P_{\phi}(x)-\text{sg}\left[e_{i}\right]&#124;&#124;_{2}^{2}$
    |  | (9) |'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\theta,\phi,e_{1:k}}(x)=\log P_{\theta}(x&#124;z_{D})-&#124;&#124;\text{sg}\left[P_{\phi}(x)\right]-e_{i}&#124;&#124;_{2}^{2}-\beta&#124;&#124;P_{\phi}(x)-\text{sg}\left[e_{i}\right]&#124;&#124;_{2}^{2}$
    |  | (9) |'
- en: 'The generative model is trained to reconstruct the data $x$, as in other VAE
    variants, via the first term of Eq. ([9](#S2.E9 "In 4th item ‣ 2.1 Variational
    Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")).
    The codebook parameters are trained to align with the encoded latent representations
    $z_{E}=P_{\phi}(x)$ through the second term. As for $P_{\phi}$, it is trained
    by both the first and third terms. Specifically, they assume $\frac{\partial\log
    P_{\theta}(x|z_{D})}{\partial z_{E}}=\frac{\partial\log P_{\theta}(x|z_{D})}{\partial
    z_{D}}$, such that the gradient of the first term on $z_{D}$ can be backpropagated
    to $\phi$. The third term, on the other hand, is to make sure that the encoder
    commits to an embedding from the codebook. While empirical evidence demonstrates
    VQ-VAE’s effectiveness, its objective design largely relies on intuition, so additional
    theoretical underpinning would be beneficial.'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成模型通过公式 ([9](#S2.E9 "在第4项 ‣ 2.1 变分自编码器 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向的展望"))
    中的第一项来训练重建数据$x$，这与其他VAE变体相同。代码本参数通过第二项与编码的潜在表示$z_{E}=P_{\phi}(x)$对齐。至于$P_{\phi}$，它通过第一项和第三项进行训练。具体而言，他们假设$\frac{\partial\log
    P_{\theta}(x|z_{D})}{\partial z_{E}}=\frac{\partial\log P_{\theta}(x|z_{D})}{\partial
    z_{D}}$，这样第一项在$z_{D}$上的梯度可以反向传播到$\phi$。另一方面，第三项是为了确保编码器坚持从代码本中进行嵌入。虽然实证证据证明了VQ-VAE的有效性，但其目标设计在很大程度上依赖于直觉，因此额外的理论基础将是有益的。
- en: •
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VRNN: Chung et al. ([2015](#bib.bib58)) introduce a recurrent version of the
    VAE for the purpose of modeling sequences of data (i.e., $x_{\leq T}\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}[x_{0},\cdots,x_{T}]$).
    At each time step $t$, the historical information $(x_{<t},z_{<t})$ is involved
    for sequential decision making, where $z_{<t}$ are latent variables corresponding
    to $x_{<t}$. Instead of directly concatenating the sequence of variables, i.e.,
    $(x_{<t},z_{<t})$, as the history, a recurrent neural network (RNN) is adopted
    to embed the historical information recursively: $h_{t}=\text{RNN}_{\omega}(x_{t},z_{t},h_{t-1}),\
    (t=0,\cdots,T)$ ²²2According to (Chung et al. ([2015](#bib.bib58))), additional
    embedding layers $\text{emb}_{X}$ and $\text{emb}_{Z}$ can be used to extract
    features from the data $x_{t}$ and latent variables $z_{t}$, respectively.. To
    involve historical information in decision-making, the prior, generative, and
    inference model are conditioned on the history embedding as: $P_{\theta}(z_{t}|h_{t-1})$,
    $P_{\theta}(x_{t}|z_{t},h_{t-1})$, and $P_{\phi}(z_{t}|x_{t},h_{t-1})$, where
    $h_{t-1}$ embeds the history $(x_{<t},z_{<t})$. The overall objective (to maximize)
    for these three models and $\text{RNN}_{\omega}$ is as below:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VRNN：Chung et al. ([2015](#bib.bib58)) 引入了 VAE 的递归版本，用于建模数据序列（即 $x_{\leq T}\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}[x_{0},\cdots,x_{T}]$）。在每个时间步
    $t$，历史信息 $(x_{<t},z_{<t})$ 被用于序列决策，其中 $z_{<t}$ 是与 $x_{<t}$ 对应的潜在变量。不是直接将变量序列，即
    $(x_{<t},z_{<t})$，作为历史记录，而是采用递归神经网络（RNN）递归地嵌入历史信息：$h_{t}=\text{RNN}_{\omega}(x_{t},z_{t},h_{t-1}),\
    (t=0,\cdots,T)$²²2 根据 (Chung et al. ([2015](#bib.bib58)))，可以使用额外的嵌入层 $\text{emb}_{X}$
    和 $\text{emb}_{Z}$ 分别从数据 $x_{t}$ 和潜在变量 $z_{t}$ 中提取特征。为了在决策中涉及历史信息，先验、生成和推断模型都以历史嵌入为条件，形式为：$P_{\theta}(z_{t}|h_{t-1})$，$P_{\theta}(x_{t}|z_{t},h_{t-1})$
    和 $P_{\phi}(z_{t}|x_{t},h_{t-1})$，其中 $h_{t-1}$ 嵌入了历史 $(x_{<t},z_{<t})$。这三个模型和
    $\text{RNN}_{\omega}$ 的总体目标（最大化）如下：
- en: '|  | $\mathbb{E}_{P_{\phi,\omega}(z\leq T&#124;x\leq T)}\left[\sum_{t=1}^{T}\left(\log
    P_{\theta}(x_{t}&#124;z_{t},h_{t-1})-D_{KL}(P_{\phi}(z_{t}&#124;x_{t},h_{t-1})&#124;&#124;P_{\theta}(z_{t}&#124;h_{t-1}))\right)\right]$
    |  | (10) |'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{P_{\phi,\omega}(z\leq T&#124;x\leq T)}\left[\sum_{t=1}^{T}\left(\log
    P_{\theta}(x_{t}&#124;z_{t},h_{t-1})-D_{KL}(P_{\phi}(z_{t}&#124;x_{t},h_{t-1})&#124;&#124;P_{\theta}(z_{t}&#124;h_{t-1}))\right)\right]$
    |  | (10) |'
- en: where $P_{\phi,\omega}(z\leq T|x\leq T)=\prod_{t=0}^{T}P_{\phi}(z_{t}|x_{t},h_{t-1})$
    with $h_{t}$ defined as above. This objective is a variational lower bound of
    the log likelihood of the data sequence, i.e., $\log P_{\theta}(x_{\leq T})$,
    as shown in Appendix A of (Chung et al. ([2015](#bib.bib58))). The introduction
    of historical information and explicit use of the Hidden Markov Model (Rabiner
    & Juang ([1986](#bib.bib264))) (e.g., for defining $P_{\phi,\omega}(z\leq T|x\leq
    T)$) significantly improve the representational capability of VRNN for sequential
    data.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $P_{\phi,\omega}(z\leq T|x\leq T)=\prod_{t=0}^{T}P_{\phi}(z_{t}|x_{t},h_{t-1})$，$h_{t}$
    如上所定义。这个目标是数据序列对数似然的变分下界，即 $\log P_{\theta}(x_{\leq T})$，如 (Chung et al. ([2015](#bib.bib58)))
    附录 A 所示。引入历史信息并显式使用隐马尔可夫模型 (Rabiner & Juang ([1986](#bib.bib264)))（例如，用于定义 $P_{\phi,\omega}(z\leq
    T|x\leq T)$）显著提高了 VRNN 对序列数据的表征能力。
- en: 2.2 Generative Adversarial Networks
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 生成对抗网络
- en: 'The Generative Adversarial Network (GAN, Goodfellow et al. ([2014a](#bib.bib106)))
    is a generative model based on the minimax optimization. It consists of a generator
    $G$ and discriminator $D$ that are trained simultaneously to compete against each
    other. The generator tries to capture the distribution of real data $P_{X}(x)$
    and generate new data examples $x$ from initial noise $z\sim P_{Z}(\cdot)$. The
    discriminator is usually a binary classifier used to discriminate generated examples
    from real data as accurately as possible. Formally, the generator can be modeled
    as a differentiable function that maps noise $z$ following a certain prior distribution
    $P_{Z}(\cdot)$ to samples $x$ in the data space $X$: $z\sim P_{Z}(\cdot),x=G(z)\sim
    P_{G}(\cdot)$. While, the discriminator’s output $D(x)$ estimates the probability
    of a data point $x$ being sampled from the true data distribution $P_{X}(\cdot)$
    rather than generated by the generator. The training objective of GAN is formulated
    as a minimax game between $G$ and $D$:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN, Goodfellow et al. ([2014a](#bib.bib106)））是一种基于最小最大优化的生成模型。它由一个生成器
    $G$ 和一个判别器 $D$ 组成，两者同时训练，相互竞争。生成器试图捕捉真实数据的分布 $P_{X}(x)$，并从初始噪声 $z\sim P_{Z}(\cdot)$
    生成新的数据样本 $x$。判别器通常是一个二分类器，用于尽可能准确地区分生成的样本与真实数据。形式上，生成器可以建模为一个可微分的函数，将噪声 $z$ 映射到数据空间
    $X$ 中的样本 $x$：$z\sim P_{Z}(\cdot),x=G(z)\sim P_{G}(\cdot)$。而判别器的输出 $D(x)$ 估计数据点
    $x$ 从真实数据分布 $P_{X}(\cdot)$ 采样的概率，而非由生成器生成。GAN 的训练目标被表述为 $G$ 和 $D$ 之间的最小最大博弈：
- en: '|  | $\min_{G}\max_{D}\mathbb{E}_{x\sim P_{X}(\cdot)}[\log D(x)]+\mathbb{E}_{z\sim
    P_{Z}(\cdot)}[\log(1-D(G(z)))]$ |  | (11) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{G}\max_{D}\mathbb{E}_{x\sim P_{X}(\cdot)}[\log D(x)]+\mathbb{E}_{z\sim
    P_{Z}(\cdot)}[\log(1-D(G(z)))]$ |  | (11) |'
- en: 'Here, $D$ is trained to maximize the probability of assigning correct labels
    to both real data and fake ones from the generator: $D(x)\rightarrow 1,D(G(z))\rightarrow
    0$. For a certain $G$, the optimal discriminator is given by: $D_{G}^{*}(x)=\frac{P_{X}(x)}{P_{X}(x)+P_{G}(x)}$.
    Plugging this back in Eq. ([11](#S2.E11 "In 2.2 Generative Adversarial Networks
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), the
    minimax problem can be converted to an optimization problem with regard to the
    generator $G$: $\min_{G}JS(P_{X}(\cdot)||P_{G}(\cdot))$, where $JS(\cdot)$ denotes
    the Jensen-Shannon divergence (Menéndez et al. ([1997](#bib.bib220))). Please
    refer to (Goodfellow et al. ([2014a](#bib.bib106))) for the detailed derivation.
    Thus, in essence, $G$ is trained to generate samples that match the real data
    distribution.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，$D$ 被训练以最大化对真实数据和生成器生成的伪造数据正确标签的分配概率：$D(x)\rightarrow 1, D(G(z))\rightarrow
    0$。对于某个特定的 $G$，最佳判别器为：$D_{G}^{*}(x)=\frac{P_{X}(x)}{P_{X}(x)+P_{G}(x)}$。将其代入公式（[11](#S2.E11
    "In 2.2 Generative Adversarial Networks ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")），该最小最大问题可以转化为关于生成器 $G$ 的优化问题：$\min_{G}JS(P_{X}(\cdot)||P_{G}(\cdot))$，其中
    $JS(\cdot)$ 表示 Jensen-Shannon 散度（Menéndez et al. ([1997](#bib.bib220)））。有关详细推导，请参见
    (Goodfellow et al. ([2014a](#bib.bib106)))。因此，实质上，$G$ 被训练生成与真实数据分布匹配的样本。'
- en: 'Next, we introduce several representative variants of GAN, as the necessary
    background for Section [4](#S4 "4 Generative Adversarial Networks in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们介绍几个代表性的 GAN 变体，以作为第 [4](#S4 "4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") 节的必要背景。'
- en: •
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Conditional GAN (CGAN) & InfoGAN: CGAN (Mirza & Osindero ([2014](#bib.bib223)))
    trains its generator and discriminator together with extra information $y$ as
    conditions:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 条件生成对抗网络（CGAN）与信息生成对抗网络（InfoGAN）：CGAN（Mirza & Osindero ([2014](#bib.bib223)））在训练生成器和判别器时，将额外的信息
    $y$ 作为条件。
- en: '|  | $\min_{G}\max_{D}\mathbb{E}_{x\sim P_{X}(\cdot)}[\log D(x&#124;y)]+\mathbb{E}_{z\sim
    P_{Z}(\cdot)}[\log(1-D(G(z&#124;y)))]$ |  | (12) |'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\min_{G}\max_{D}\mathbb{E}_{x\sim P_{X}(\cdot)}[\log D(x&#124;y)]+\mathbb{E}_{z\sim
    P_{Z}(\cdot)}[\log(1-D(G(z&#124;y)))]$ |  | (12) |'
- en: 'Similarly, this objective can be converted to $\min_{G}JS(P_{X}(\cdot|y)||P_{G}(\cdot|y))$,
    i.e., matching the conditional distribution of the real data. With these conditional
    models, CGAN has the advantage of handling not only unimodal datasets but also
    multimodal ones like Flickr (Plummer et al. ([2015](#bib.bib255))), which consists
    of diverse labeled image data. With the class label or text as the condition $y$,
    CGAN can be used for conditional generation. However, when the dataset contains
    multiple modalities but the labels $y$ are not given, InfoGAN (Chen et al. ([2016](#bib.bib50)))
    proposes to introduce a latent code $c$ (following an assumed distribution $P_{C}(\cdot)$)
    to the generator. In this way, the data generation process is modified into $c\sim
    P_{C}(\cdot),z\sim P_{Z}(\cdot),x=G(z,c)\sim P_{G}(\cdot|c)$, where $c$ is for
    capturing distinct modes in the dataset and $z$ targets the unstructured noise
    shared across the modes. The generator $G$ is trained not only with $V(D,G)$ (i.e.,
    Eq. ([11](#S2.E11 "In 2.2 Generative Adversarial Networks ‣ 2 Background on Deep
    Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"))) but also to maximize the mutual
    information between the mode variable $C$ and the respective samples $X|_{C}=G(Z,C)$:
    $\min_{G}\max_{D}V(D,G)-\lambda I(C,X|_{C}))$, such that $G(Z,C)$ would not degenerate
    to a unimodal model. By replacing the mutual information with its practical lower
    bound, we have the objective for InfoGAN as below:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类似地，这个目标可以转换为 $\min_{G}JS(P_{X}(\cdot|y)||P_{G}(\cdot|y))$，即匹配真实数据的条件分布。利用这些条件模型，CGAN不仅可以处理单模态数据集，还可以处理多模态数据集，例如Flickr（Plummer等人
    ([2015](#bib.bib255)）），该数据集由多样的标记图像数据组成。通过将类别标签或文本作为条件$y$，CGAN可以用于条件生成。然而，当数据集中包含多种模态但标签$y$未提供时，InfoGAN（Chen等人
    ([2016](#bib.bib50)））提出将潜在代码$c$（遵循假定分布$P_{C}(\cdot)$）引入生成器。这样，数据生成过程被修改为$c\sim
    P_{C}(\cdot),z\sim P_{Z}(\cdot),x=G(z,c)\sim P_{G}(\cdot|c)$，其中$c$用于捕捉数据集中的不同模式，而$z$针对的是跨模式共享的无结构噪声。生成器$G$不仅通过$V(D,G)$（即，等式([11](#S2.E11
    "在2.2 生成对抗网络 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的观点"))）进行训练，还要最大化模式变量$C$与相应样本$X|_{C}=G(Z,C)$之间的互信息：$\min_{G}\max_{D}V(D,G)-\lambda
    I(C,X|_{C})$，使得$G(Z,C)$不会退化为单模态模型。通过用其实际下界替代互信息，我们得到了InfoGAN的目标如下：
- en: '|  | $\min_{G,Q}\max_{D}V(D,G)-\lambda\left[\mathbb{E}_{c\sim P_{C}(\cdot),z\sim
    P_{Z}(\cdot),x=G(z,c)}\left[\log Q(c&#124;x)\right]+H(C)\right]$ |  | (13) |'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\min_{G,Q}\max_{D}V(D,G)-\lambda\left[\mathbb{E}_{c\sim P_{C}(\cdot),z\sim
    P_{Z}(\cdot),x=G(z,c)}\left[\log Q(c|x)\right]+H(C)\right]$ |  | (13) |'
- en: 'Here, $Q(x|c)$ is a variational posterior neural network trained to approximate
    the real but intractable posterior distribution $P(c|x)$, and $H(C)$ denotes the
    entropy of the mode variable $C$ ³³3$I(C,X|_{C})=H(C)-H(C|X|_{C})=-\int P_{C}(c)\log
    P_{C}(c)\,dc+\int P_{C}(c)P_{G}(x|c)\log P(c|x)\,dc\,dx\geq-\int P_{C}(c)\log
    P_{C}(c)\,dc+\int P_{C}(c)P_{G}(x|c)\log Q(c|x)\,dc\,dx.$ The same trick as in
    Eq. ([1](#S2.E1 "In 2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) is adopted here to get this inequality..'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，$Q(x|c)$是一个变分后验神经网络，旨在近似真实但不可处理的后验分布$P(c|x)$，$H(C)$表示模式变量$C$的熵。$I(C,X|_{C})=H(C)-H(C|X|_{C})=-\int
    P_{C}(c)\log P_{C}(c)\,dc+\int P_{C}(c)P_{G}(x|c)\log P(c|x)\,dc\,dx\geq-\int
    P_{C}(c)\log P_{C}(c)\,dc+\int P_{C}(c)P_{G}(x|c)\log Q(c|x)\,dc\,dx.$ 这里采用了与等式([1](#S2.E1
    "在2.1 变分自编码器 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的观点"))中相同的技巧来得到这一不等式。
- en: •
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$f$-GAN: A large class of different divergences, including the KL divergence
    and JS divergence, are the so-called $f$-divergences (Csiszár & Shields ([2004](#bib.bib64))).
    Specifically, given two continuous distributions $P(\cdot)$ and $G(\cdot)$, the
    $f$-divergence between them is defined as $D_{f}(P(\cdot)||Q(\cdot))=\int Q(x)f\left(\frac{P(x)}{Q(x)}\right)dx$,
    where $f:\mathbb{R}^{+}\rightarrow\mathbb{R}$ should be a convex, lower-semicontinuous
    function satisfying $f(1)=0$ ⁴⁴4As an instance, when $f(a)=a\log a$, $D_{f}$ recovers
    the KL divergence.. Given a data distribution $P_{X}(\cdot)$, a corresponding
    generative model $G$ can be learned through minimizing $D_{f}(P_{X}(\cdot)||P_{G}(\cdot))$.
    As proposed in $f$-GAN (Nowozin et al. ([2016](#bib.bib235))), this can be approximately
    solved via a minimax optimization problem:'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $f$-GAN：一大类不同的散度，包括KL散度和JS散度，被称为$f$-散度（Csiszár & Shields ([2004](#bib.bib64))）。具体来说，给定两个连续分布$P(\cdot)$和$G(\cdot)$，它们之间的$f$-散度定义为$D_{f}(P(\cdot)||Q(\cdot))=\int
    Q(x)f\left(\frac{P(x)}{Q(x)}\right)dx$，其中$f:\mathbb{R}^{+}\rightarrow\mathbb{R}$应为一个凸的、下半连续的函数，满足$f(1)=0$。例如，当$f(a)=a\log
    a$时，$D_{f}$恢复了KL散度。给定一个数据分布$P_{X}(\cdot)$，可以通过最小化$D_{f}(P_{X}(\cdot)||P_{G}(\cdot))$来学习一个对应的生成模型$G$。如$f$-GAN（Nowozin
    et al. ([2016](#bib.bib235)))中提出的，这可以通过一个极小极大优化问题近似求解：
- en: '|  | $\min_{G}\max_{V}\mathbb{E}_{x\sim P_{X}(\cdot)}\left[g_{f}(V(x))\right]+\mathbb{E}_{x\sim
    P_{G}(\cdot)}\left[-f^{*}(g_{f}(V(x)))\right]$ |  | (14) |'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\min_{G}\max_{V}\mathbb{E}_{x\sim P_{X}(\cdot)}\left[g_{f}(V(x))\right]+\mathbb{E}_{x\sim
    P_{G}(\cdot)}\left[-f^{*}(g_{f}(V(x)))\right]$ |  | (14) |'
- en: Here, $V$ is a differentiable function without any range constraints on the
    output, $g_{f}$ is an output activation function specific to the $f$-divergence
    used, and $f^{*}$ is the convex conjugate function of $f$ (Hiriart-Urruty & Lemaréchal
    ([2004](#bib.bib132))). With specific choices of $f$, $f^{*}$ and $g_{f}$, various
    types of $f$-GAN can be achieved (see Table 6 of (Nowozin et al. ([2016](#bib.bib235)))).
    For example, the original GAN can be recovered by setting $f^{*}(t)=-\log(1-\exp(t))$
    and $g_{f}(v)=-\log(1+\exp(-v))$ ⁵⁵5The discriminator in the original GAN $D(x)$
    would be a function of $V(x)$, i.e., $D(x)=1/(1+\exp(-V(x)))$. $V(x)\in\mathbb{R}$
    and thus $D(x)\in(0,1)$.. In this way, $f$-GAN provides a generalization of multiple
    variants of GANs, including the original GAN, LSGAN, and WGAN.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，$V$是一个没有输出范围约束的可微函数，$g_{f}$是特定于所用$f$-散度的输出激活函数，而$f^{*}$是$f$的凸共轭函数（Hiriart-Urruty
    & Lemaréchal ([2004](#bib.bib132)))。通过对$f$、$f^{*}$和$g_{f}$的具体选择，可以实现各种类型的$f$-GAN（见（Nowozin
    et al. ([2016](#bib.bib235)))中的表6）。例如，通过设置$f^{*}(t)=-\log(1-\exp(t))$和$g_{f}(v)=-\log(1+\exp(-v))$，可以恢复原始GAN。这样，$f$-GAN提供了对多种GAN变体的推广，包括原始GAN、LSGAN和WGAN。
- en: •
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Least Squares GAN (LSGAN): During the early training stage, the generated examples
    would substantially differ from the real data, but the original GAN objective
    provides only very small penalties for updating $G$, as shown in Figure 16 of
    (Goodfellow ([2017](#bib.bib105))). To overcome this vanishing gradient problem,
    LSGAN (Mao et al. ([2017](#bib.bib212))) replaces the cross-entropy loss used
    in the original GAN objective with least-square losses:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最小二乘GAN（LSGAN）：在早期训练阶段，生成的样本与真实数据差异较大，但原始GAN目标仅对更新$G$提供非常小的惩罚，如（Goodfellow ([2017](#bib.bib105))）中的图16所示。为了克服这一梯度消失问题，LSGAN（Mao
    et al. ([2017](#bib.bib212)))将原始GAN目标中的交叉熵损失替换为最小二乘损失：
- en: '|  | $\min_{D}\mathbb{E}_{x\sim P_{X}(\cdot)}\left[(D(x)-j)^{2}\right]+\mathbb{E}_{z\sim
    P_{Z}(\cdot)}\left[(D(G(z))-i)^{2}\right],\ \min_{G}\mathbb{E}_{z\sim P_{Z}(\cdot)}\left[(D(G(z))-k)^{2}\right]$
    |  | (15) |'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\min_{D}\mathbb{E}_{x\sim P_{X}(\cdot)}\left[(D(x)-j)^{2}\right]+\mathbb{E}_{z\sim
    P_{Z}(\cdot)}\left[(D(G(z))-i)^{2}\right],\ \min_{G}\mathbb{E}_{z\sim P_{Z}(\cdot)}\left[(D(G(z))-k)^{2}\right]$
    |  | (15) |'
- en: 'Here, $i,j$ are the labels for the generated and real examples, respectively;
    $k$ is the value that $G$ hopes for $D$ to believe for generated examples. As
    shown in (Mao et al. ([2017](#bib.bib212))), when $j-i=2$ and $j-k=1$, Eq. ([15](#S2.E15
    "In 3rd item ‣ 2.2 Generative Adversarial Networks ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) is equivalent to minimizing the Pearson
    $\chi^{2}$ divergence, which is a type of $f$-divergence, between the real and
    generated data distribution, i.e., $P_{X}(x)$ and $P_{G}(x)$.'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '这里，$i,j$ 分别是生成样本和真实样本的标签；$k$ 是 $G$ 希望 $D$ 对生成样本的信任值。如 (Mao 等人（[2017](#bib.bib212)））所示，当
    $j-i=2$ 和 $j-k=1$ 时，式（[15](#S2.E15 "In 3rd item ‣ 2.2 Generative Adversarial Networks
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")）等同于最小化
    Pearson $\chi^{2}$ 散度，即真实数据分布 $P_{X}(x)$ 和生成数据分布 $P_{G}(x)$ 之间的 $f$-散度。'
- en: •
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Wasserstein GAN (WGAN): As mentioned earlier, for the original GAN, the optimization
    with respect to $G$ is equivalent to minimizing the JS divergence $JS(P_{X}(\cdot)||P_{G}(\cdot))$.
    WGAN (Arjovsky et al. ([2017](#bib.bib13))) proposes to minimize the Wasserstein
    distance instead, which is defined as $\sup_{||f||_{L}\leq K}\mathbb{E}_{x\sim
    P_{X}(\cdot)}\left[f(x)\right]-\mathbb{E}_{x\sim P_{G}(\cdot)}\left[f(x)\right]$.
    $||f||_{L}\leq K$ denotes the set of $K$-Lipschitz functions. In this case, the
    objective function of WGAN is as below:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Wasserstein GAN（WGAN）：如前所述，对于原始 GAN，关于 $G$ 的优化等同于最小化 JS 散度 $JS(P_{X}(\cdot)||P_{G}(\cdot))$。WGAN（Arjovsky
    等人（[2017](#bib.bib13)））建议改为最小化 Wasserstein 距离，其定义为 $\sup_{||f||_{L}\leq K}\mathbb{E}_{x\sim
    P_{X}(\cdot)}\left[f(x)\right]-\mathbb{E}_{x\sim P_{G}(\cdot)}\left[f(x)\right]$。$||f||_{L}\leq
    K$ 表示 $K$-Lipschitz 函数的集合。在这种情况下，WGAN 的目标函数如下：
- en: '|  | $\min_{G}\max_{D}\mathbb{E}_{x\sim P_{X}(\cdot)}\left[D(x)\right]-\mathbb{E}_{x\sim
    P_{G}(\cdot)}\left[D(x)\right]$ |  | (16) |'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\min_{G}\max_{D}\mathbb{E}_{x\sim P_{X}(\cdot)}\left[D(x)\right]-\mathbb{E}_{x\sim
    P_{G}(\cdot)}\left[D(x)\right]$ |  | (16) |'
- en: Here, $D$ is used to estimate the Wasserstein distance, hence its output is
    not limited to $[0,1]$ as in the original GAN. Also, the neural network $D$ is
    applied with weight clipping, ensuring that $D$ is $K$-Lipschitz. Compared with
    the original GAN, WGAN improves the learning stability and provides meaningful
    learning curves for parameter fine-tuning.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，$D$ 用于估计 Wasserstein 距离，因此其输出不像原始 GAN 那样限制在 $[0,1]$ 内。此外，神经网络 $D$ 采用权重裁剪，确保
    $D$ 是 $K$-Lipschitz 的。与原始 GAN 相比，WGAN 改善了学习的稳定性，并为参数微调提供了有意义的学习曲线。
- en: •
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Triple-GAN: To realize data classification and conditional generation in the
    meantime, Triple-GAN (Li et al. ([2017a](#bib.bib182))) proposes to introduce
    a classifier $C(y|x)$ to the original GAN framework. Here, $x$ and $y$ denote
    the data and label, respectively. Given a dataset of $(x,y)$, empirical joint
    and marginal distributions of the real data and labels can be acquired as $P_{X,Y}(x,y)$,
    $P_{X}(x)$, and $P_{Y}(y)$. $P_{X,Y}(x,y)$ can be approximated as either $P_{X}(x)P_{C}(y|x)$
    or $P_{Y}(y)P_{G}(x|y)$, corresponding to the classification with $C$ and conditional
    generation with $G$, respectively. Thus, $C$ and $G$ can be trained to match the
    joint distribution of $(x,y)$. Here is the objective:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Triple-GAN：为了同时实现数据分类和条件生成，Triple-GAN（Li 等人（[2017a](#bib.bib182)））建议将分类器 $C(y|x)$
    引入原始 GAN 框架。在这里，$x$ 和 $y$ 分别表示数据和标签。给定一个 $(x,y)$ 数据集，可以获得真实数据和标签的经验联合分布和边际分布，分别为
    $P_{X,Y}(x,y)$、$P_{X}(x)$ 和 $P_{Y}(y)$。$P_{X,Y}(x,y)$ 可以近似为 $P_{X}(x)P_{C}(y|x)$
    或 $P_{Y}(y)P_{G}(x|y)$，分别对应于使用 $C$ 的分类和使用 $G$ 的条件生成。因此，可以训练 $C$ 和 $G$ 以匹配 $(x,y)$
    的联合分布。目标如下：
- en: '|  | $\displaystyle\min_{C,G}\max_{D}$ | $\displaystyle\mathbb{E}_{(x,y)\sim
    P_{X,Y}(\cdot)}\left[\log D(x,y)\right]+\lambda_{G}\mathbb{E}_{y\sim P_{Y}(\cdot),x\sim
    P_{G}(\cdot&#124;y)}\left[\log(1-D(x,y))\right]+$ |  | (17) |'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{C,G}\max_{D}$ | $\displaystyle\mathbb{E}_{(x,y)\sim
    P_{X,Y}(\cdot)}\left[\log D(x,y)\right]+\lambda_{G}\mathbb{E}_{y\sim P_{Y}(\cdot),x\sim
    P_{G}(\cdot|y)}\left[\log(1-D(x,y))\right]+$ |  | (17) |'
- en: '|  |  | $\displaystyle(1-\lambda_{G})\mathbb{E}_{x\sim P_{X}(\cdot),y\sim P_{C}(\cdot&#124;x)}\left[\log(1-D(x,y))\right]+\mathbb{E}_{(x,y)\sim
    P_{X,Y}(\cdot)}\left[-\log P_{C}(y&#124;x)\right]+$ |  |'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle(1-\lambda_{G})\mathbb{E}_{x\sim P_{X}(\cdot),y\sim P_{C}(\cdot|x)}\left[\log(1-D(x,y))\right]+\mathbb{E}_{(x,y)\sim
    P_{X,Y}(\cdot)}\left[-\log P_{C}(y|x)\right]+$ |  |'
- en: '|  |  | $\displaystyle\lambda_{C}\mathbb{E}_{y\sim P_{Y}(\cdot),x\sim P_{G}(\cdot&#124;y)}\left[-\log
    P_{C}(y&#124;x)\right]$ |  |'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\lambda_{C}\mathbb{E}_{y\sim P_{Y}(\cdot),x\sim P_{G}(\cdot|y)}\left[-\log
    P_{C}(y|x)\right]$ |  |'
- en: 'The first three terms resemble the GAN framework, where $C$ and $G$ are trained
    to fool the discriminator $D$ by generating $(x,y)$ close to the real data. The
    last two (cross entropy) terms provide extra supervision for $C$ using samples
    from the real and generated distributions. Note that the last objective term is
    only utilized for training $C$. It is shown in (Li et al. ([2017a](#bib.bib182)))
    that $P_{X,Y}(x,y)=P_{X}(x)P_{C}(y|x)=P_{Y}(y)P_{G}(x|y)$ if and only if the equilibrium
    among the three players ($C,G,D$) is achieved in Eq. ([17](#S2.E17 "In 5th item
    ‣ 2.2 Generative Adversarial Networks ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")).'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '前三个项类似于 GAN 框架，其中 $C$ 和 $G$ 被训练来欺骗鉴别器 $D$，通过生成接近真实数据的 $(x,y)$。最后两个（交叉熵）项通过使用来自真实和生成分布的样本来为
    $C$ 提供额外的监督。请注意，最后的目标项仅用于训练 $C$。在 (Li et al. ([2017a](#bib.bib182))) 中显示，当且仅当
    Eq. ([17](#S2.E17 "In 5th item ‣ 2.2 Generative Adversarial Networks ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) 中三方（$C,G,D$）达到平衡时，$P_{X,Y}(x,y)=P_{X}(x)P_{C}(y|x)=P_{Y}(y)P_{G}(x|y)$。'
- en: 2.3 Normalizing Flows
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 归一化流
- en: 'Normalizing Flows (NFs) represent a class of generative models that offer an
    elegant approach for density estimation and the generation of data samples from
    complex distributions (Kobyzev et al. ([2021](#bib.bib169))). NFs allow for exact
    likelihood evaluation, which is crucial in many probabilistic modeling tasks.
    Formally, NFs transform a simple, known probability distribution on the latent
    variable (i.e., $z\sim P_{Z}(\cdot)$) into a complex, desired distribution on
    data (i.e., $x\sim P_{X}(\cdot)$) through a sequence of invertible and differentiable
    transformations. The mathematical representation of the transformation from $x$
    to $z$ is given by a composition of a series of bijections as $F=F_{N}\circ F_{N-1}\circ\cdots\circ
    F_{1}$. The data flow $[x_{0},x_{1},\cdots,x_{N}]$ ($x_{0}=x,\ x_{N}=z$) within
    NFs adheres to the following: for all $1\leq i\leq N,\ x_{i}=F_{i}(x_{i-1}),\
    x_{i-1}=F^{-1}_{i}(x_{i})$. The core principle behind the transformation is the
    change of variable formula. Specifically, the probability density functions of
    $x$ and $z$ are related as:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化流（NFs）表示一种生成模型，它为密度估计和从复杂分布生成数据样本提供了一种优雅的方法（Kobyzev et al. ([2021](#bib.bib169)))。NFs
    允许精确的似然评估，这在许多概率建模任务中至关重要。形式上，NFs 通过一系列可逆且可微的变换将潜在变量（即 $z\sim P_{Z}(\cdot)$）上的简单已知概率分布转换为数据（即
    $x\sim P_{X}(\cdot)$）上的复杂期望分布。从 $x$ 到 $z$ 的变换的数学表示由一系列双射的组合给出，即 $F=F_{N}\circ
    F_{N-1}\circ\cdots\circ F_{1}$。在 NFs 中，数据流 $[x_{0},x_{1},\cdots,x_{N}]$（$x_{0}=x,\
    x_{N}=z$）遵循以下规律：对于所有 $1\leq i\leq N,\ x_{i}=F_{i}(x_{i-1}),\ x_{i-1}=F^{-1}_{i}(x_{i})$。变换的核心原理是变量变换公式。具体来说，$x$
    和 $z$ 的概率密度函数的关系为：
- en: '|  | $P_{Z}(z)=P_{X}(F^{-1}(z))&#124;\det D(F^{-1}(z))&#124;,\ P_{X}(x)=P_{Z}(F(x))&#124;\det
    D(F(x))&#124;$ |  | (18) |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $P_{Z}(z)=P_{X}(F^{-1}(z))\ |\det D(F^{-1}(z))\ |,\ P_{X}(x)=P_{Z}(F(x))\
    |\det D(F(x))\ |$ |  | (18) |'
- en: 'where $\det D(F(x))$ denotes the determinant of the Jacobian matrix of $F(x)$.
    Given the transformation function $F$ and the basic distribution $P_{Z}$, the
    density of a data sample $x$ can be exactly acquired as $P_{X}(x)$ defined in
    Eq. ([18](#S2.E18 "In 2.3 Normalizing Flows ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")). Conversely, the data generation can
    be done by first sampling a latent $z\sim P_{Z}(\cdot)$ and then apply the generator
    function $G=F^{-1}$. As for training, it is usually through maximizing the log-likelihood
    of target data distribution (Dinh et al. ([2017](#bib.bib73))):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\det D(F(x))$ 表示 $F(x)$ 的雅可比矩阵的行列式。给定变换函数 $F$ 和基本分布 $P_{Z}$，数据样本 $x$ 的密度可以精确地获得为
    Eq. ([18](#S2.E18 "In 2.3 Normalizing Flows ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) 中定义的 $P_{X}(x)$。反之，数据生成可以通过首先从 $P_{Z}(\cdot)$
    中采样一个潜在的 $z$ 然后应用生成器函数 $G=F^{-1}$ 来完成。至于训练，通常通过最大化目标数据分布的对数似然（Dinh et al. ([2017](#bib.bib73)))
    来进行：'
- en: '|  | $\mathbb{E}_{X}[\log(P_{X}(x))],\ \log(P_{X}(x))=\log(P_{Z}(F(x)))+\log(&#124;\det
    D(F(x))&#124;)$ |  | (19) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{X}[\log(P_{X}(x))],\ \log(P_{X}(x))=\log(P_{Z}(F(x)))+\log(\
    |\det D(F(x))\ |)$ |  | (19) |'
- en: 'For NFs, the generator $G$ should be sufficiently expressive to model the distribution
    of interest, while the transformation $F$ should be invertible and the computation
    of the two directions mentioned above, especially regarding the determinant calculation,
    should be efficient. Based on these requirements, various types of flows have
    been developed. Here, we present a brief description of the representative categories.
    As mentioned above, $G$ is composed of a series of generator modules $G_{i}=F_{i}^{-1}$.
    For each of the following categories, we introduce its specific design on $G_{i}$,
    and we use $m$ and $n$ to denote the input and output of $G_{i}$, respectively.
    Note that we will not delve into the complex mathematical details regarding calculations
    or objective designs of specific variants of NFs, as they are not essential for
    understanding the applications of NFs in offline policy learning, as introduced
    in Section [5](#S5 "5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions").'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 NFs，生成器 $G$ 应该具有足够的表达能力以建模感兴趣的分布，而变换 $F$ 应该是可逆的，并且上述两个方向的计算，尤其是行列式计算，应当是高效的。基于这些要求，开发了各种类型的流。在这里，我们简要介绍几种代表性的类别。如前所述，$G$
    由一系列生成器模块 $G_{i}=F_{i}^{-1}$ 组成。对于以下每种类别，我们将介绍其在 $G_{i}$ 上的具体设计，并使用 $m$ 和 $n$
    分别表示 $G_{i}$ 的输入和输出。注意，我们不会深入探讨复杂的数学细节，如计算或特定变体的目标设计，因为这些对于理解 NFs 在离线策略学习中的应用并非必要，如第
    [5](#S5 "5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    节所介绍的。 '
- en: •
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Coupling Flows enable highly expressive transformations via a coupling method.
    In particular, the input $m\in\mathbb{R}^{D}$ is partitioned into two subspaces:
    $(m^{A},m^{B})\in\mathbb{R}^{d}\times\mathbb{R}^{D-d}$, and the generator module
    $G_{i}$ is defined in the format: $G_{i}(m)=(H(m^{A};\Theta(m^{B})),m^{B})$. Here,
    $H(\cdot;\theta):\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}$ is a bijection, and
    $\Theta$ is the parameter function. $H$ is also called the coupling function.
    In this case, the Jacobian of $G_{i}$ is simply the Jacobian of $H$, and for efficient
    computation of determinants, $H$ is usually designed to be an element-wise bijection,
    i.e., $H(m^{A};\theta)=(H_{1}(m^{A}_{1};\theta_{1}),\cdots,H_{d}(m^{A}_{d};\theta_{d}))$
    where each $H_{i}(\cdot;\theta_{i}):\mathbb{R}\rightarrow\mathbb{R}$ is a scalar
    bijection. The expressiveness of a coupling flow lies in the complexity of its
    parameter function $\Theta$, which, in practical applications, is typically modeled
    using neural networks. Algorithms in this category include NICE (Dinh et al. ([2015](#bib.bib72))),
    RealNVP (Dinh et al. ([2017](#bib.bib73))), Glow (Kingma & Dhariwal ([2018](#bib.bib164))),
    NSF (Durkan et al. ([2019](#bib.bib84))), Flow++ (Ho et al. ([2019](#bib.bib135))),
    etc.'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Coupling Flows 通过耦合方法实现了高度表达性的变换。特别地，输入 $m\in\mathbb{R}^{D}$ 被划分为两个子空间：$(m^{A},m^{B})\in\mathbb{R}^{d}\times\mathbb{R}^{D-d}$，生成器模块
    $G_{i}$ 定义为：$G_{i}(m)=(H(m^{A};\Theta(m^{B})),m^{B})$。在这里，$H(\cdot;\theta):\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}$
    是一个双射，$\Theta$ 是参数函数。$H$ 也称为耦合函数。在这种情况下，$G_{i}$ 的雅可比矩阵只是 $H$ 的雅可比矩阵，为了高效计算行列式，$H$
    通常被设计为逐元素的双射，即 $H(m^{A};\theta)=(H_{1}(m^{A}_{1};\theta_{1}),\cdots,H_{d}(m^{A}_{d};\theta_{d}))$，其中每个
    $H_{i}(\cdot;\theta_{i}):\mathbb{R}\rightarrow\mathbb{R}$ 是一个标量双射。耦合流的表达能力在于其参数函数
    $\Theta$ 的复杂性，实际应用中，$\Theta$ 通常用神经网络建模。此类别的算法包括 NICE (Dinh et al. ([2015](#bib.bib72)))，RealNVP
    (Dinh et al. ([2017](#bib.bib73)))，Glow (Kingma & Dhariwal ([2018](#bib.bib164)))，NSF
    (Durkan et al. ([2019](#bib.bib84)))，Flow++ (Ho et al. ([2019](#bib.bib135)))
    等。
- en: •
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Autoregressive Flows utilize autoregressive models for generation. To be specific,
    each entry of $n=G_{i}(m)$ conditions on the previous entries of the input: $n_{t}=H(m_{t};\Theta_{t}(m_{1:t-1}))$,
    where $H(\cdot;\theta):\mathbb{R}\rightarrow\mathbb{R}$ is a scalar bijection
    and $\Theta_{t}$ is a parameter function. Thus, each $n_{t}$ relies solely on
    $m_{1:t}$, resulting in a triangular Jacobian maxtrix of $G$. This structure allows
    for efficient computation of the determinant and parallel computation of each
    entry in $n$. However, the inverse process, represented as $m_{t}=H^{-1}(n_{t};\Theta_{t}(m_{1:t-1}))$,
    must be performed sequentially, since the parameter for the subsequent step (i.e.,
    $\Theta_{t+1}(m_{1:t})$) uses the output of the current step, i.e., $m_{t}$, as
    input. Algorithms like IAF (Kingma et al. ([2016](#bib.bib167))), MAF (Papamakarios
    et al. ([2017](#bib.bib239))), and NAF (Huang et al. ([2018](#bib.bib142))), fall
    in this category.'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自回归流利用自回归模型进行生成。具体来说，$n=G_{i}(m)$ 的每个条目都依赖于输入的前一个条目：$n_{t}=H(m_{t};\Theta_{t}(m_{1:t-1}))$，其中
    $H(\cdot;\theta):\mathbb{R}\rightarrow\mathbb{R}$ 是一个标量双射，$\Theta_{t}$ 是一个参数函数。因此，每个
    $n_{t}$ 仅依赖于 $m_{1:t}$，导致 $G$ 的雅可比矩阵是一个三角形矩阵。这种结构允许高效地计算行列式并并行计算 $n$ 中的每个条目。然而，逆过程，表示为
    $m_{t}=H^{-1}(n_{t};\Theta_{t}(m_{1:t-1}))$，必须顺序执行，因为后续步骤的参数（即 $\Theta_{t+1}(m_{1:t})$）使用当前步骤的输出，即
    $m_{t}$，作为输入。类似 IAF（Kingma et al. ([2016](#bib.bib167)))、MAF（Papamakarios et al.
    ([2017](#bib.bib239))) 和 NAF（Huang et al. ([2018](#bib.bib142))) 的算法都属于这一类别。
- en: •
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Residual Flows employ residual networks (He et al. ([2016](#bib.bib126))) as
    the generator component, characterized by $G_{i}(m)=m+R(m)$. Here, $R(\cdot)$
    symbolizes the residual block, which can be implemented as any variant of neural
    network that is invertible. Several studies (Gomez et al. ([2017](#bib.bib104));
    Chang et al. ([2018](#bib.bib36)); Jacobsen et al. ([2018](#bib.bib145))) have
    aimed to develop invertible network architectures suitable for use as residual
    blocks. Nonetheless, these architectures present a significant challenge in efficiently
    calculating their Jacobian determinants. Research works such as iResNet (Behrmann
    et al. ([2019](#bib.bib21))) and Residual Flow (Chen et al. ([2019](#bib.bib48)))
    propose an invertible $G_{i}$ by limiting the Lipschitz constant of the residual
    block to be less than 1 and compute its inverse through fixed-point iterations.
    However, controlling the Lipschitz constant of a neural network is also quite
    challenging.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 残差流使用残差网络（He et al. ([2016](#bib.bib126))) 作为生成器组件，其特点是 $G_{i}(m)=m+R(m)$。这里，$R(\cdot)$
    表示残差块，可以实现为任何可逆的神经网络变体。一些研究（Gomez et al. ([2017](#bib.bib104)); Chang et al. ([2018](#bib.bib36));
    Jacobsen et al. ([2018](#bib.bib145))) 旨在开发适合作为残差块使用的可逆网络架构。然而，这些架构在高效计算其雅可比行列式方面存在显著挑战。诸如
    iResNet（Behrmann et al. ([2019](#bib.bib21))) 和 Residual Flow（Chen et al. ([2019](#bib.bib48)))
    的研究工作通过将残差块的 Lipschitz 常数限制在 1 以下，并通过固定点迭代计算其逆，从而提出了可逆的 $G_{i}$。然而，控制神经网络的 Lipschitz
    常数也是相当具有挑战性的。
- en: •
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ODE-based Flows aim to learn a continuous dynamic system shown as a first-order
    ordinary differential equation (ODE): $\frac{d}{dt}m(t)=R(m(t);\theta(t)),\ t\in[0,1]$.
    It’s worthy noting that the residual flows mentioned above can be interpreted
    as a discretization of this ODE. Assuming the uniform Lipschitz continuity in
    $m$ and setting the initial condition $m(0)=z$, the solution of that ODE at each
    time point $\Phi^{t}(z)$ exists and is unique (Arnold ([1992](#bib.bib14))). As
    proposed in NODE (Chen et al. ([2018b](#bib.bib47))), the map at time one $\Phi^{1}(z)$
    can work as the generator, i.e., $x=\Phi^{1}(z)$, which can be considered as an
    “infinitely deep” neural network with stacked weights $\theta(t),t\in[0,1]$. Thus,
    typically, continuous ODE-type flows require fewer parameters to match the performance
    of discrete ones (Grathwohl et al. ([2018](#bib.bib108))). As for the invertibility
    of $\Phi^{1}$, it is naturally ensured by the theorem of existence and uniqueness
    pertaining to the solution of the ODE. However, $\Phi^{1}$ must be orientation
    preserving - that is, its Jacobian determinant must be positive, which may constrain
    its representational capacity. In order to give freedom for the Jacobian determinant
    to remain positive, the authors of ANODE (Dupont et al. ([2019](#bib.bib83)))
    propose augmenting the original ODE with supplementary variables $\hat{m}(t)$,
    resulting in $\frac{d}{dt}[m(t)||\hat{m}(t)]=\hat{R}(m(t)||\hat{m}(t);\theta(t)),\
    t\in[0,1]$, with $||$ signifying concatenation.'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于ODE的流旨在学习一个连续的动态系统，表现为一阶常微分方程（ODE）：$\frac{d}{dt}m(t)=R(m(t);\theta(t)),\ t\in[0,1]$。值得注意的是，上述提到的残差流可以被解释为该ODE的离散化。假设在$m$中具有一致的Lipschitz连续性，并设置初始条件$m(0)=z$，那么该ODE在每个时间点$\Phi^{t}(z)$的解是存在且唯一的（Arnold
    ([1992](#bib.bib14)))。如NODE中所提出（Chen et al. ([2018b](#bib.bib47)))，在时间点一的映射$\Phi^{1}(z)$可以作为生成器，即$x=\Phi^{1}(z)$，可以视为一个具有堆叠权重$\theta(t),t\in[0,1]$的“无限深”神经网络。因此，通常情况下，连续的ODE类型流需要更少的参数来匹配离散类型流的性能（Grathwohl
    et al. ([2018](#bib.bib108)))。至于$\Phi^{1}$的可逆性，它由ODE解的存在性和唯一性定理自然确保。然而，$\Phi^{1}$必须保持方向不变——也就是说，它的Jacobian行列式必须为正，这可能限制其表示能力。为了使Jacobian行列式保持为正，ANODE的作者（Dupont
    et al. ([2019](#bib.bib83)))建议用补充变量$\hat{m}(t)$来扩充原始ODE，得到$\frac{d}{dt}[m(t)||\hat{m}(t)]=\hat{R}(m(t)||\hat{m}(t);\theta(t)),\
    t\in[0,1]$，其中$||$表示连接。
- en: There are other types of flows which are not widely used in practice and so
    not introduced here, such as Planar and Radial Flows (Rezende & Mohamed ([2015](#bib.bib276));
    van den Berg et al. ([2018](#bib.bib325))) and Langevin Flows (continuous and
    SDE-based) (Welling & Teh ([2011](#bib.bib351)); Chen et al. ([2018a](#bib.bib40))).
    Coupling Flows and Autoregressive Flows facilitate straightforward invertible
    transformations and efficient determinant computation, yet lag in expressiveness
    compared to Residual Flows which are computationally costly. ODE-based Flows provide
    a potentially elegant and parsimonious representation with fewer parameters. However,
    they necessitate the resolution of ODEs during training, a process that can be
    computationally demanding and sensitive to the choice of numerical solver settings.
    Compared with VAEs and GANs, NFs allow for exact density estimation of a generated
    sample and can avoid training issues like mode collapse, vanishing gradients,
    etc (Salimans et al. ([2016](#bib.bib282))). However, its requirements for bijection
    functions and determinant calculations can restrict its capacity to effectively
    model complex data distributions (Cornish et al. ([2020](#bib.bib61))).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些类型的流在实践中不广泛使用，因此在这里没有介绍，例如平面流和径向流（Rezende & Mohamed ([2015](#bib.bib276));
    van den Berg et al. ([2018](#bib.bib325)))以及朗之万流（基于连续和SDE的）（Welling & Teh ([2011](#bib.bib351));
    Chen et al. ([2018a](#bib.bib40)))。耦合流和自回归流便于进行直接的可逆变换和高效的行列式计算，但与计算代价高的残差流相比，它们在表达能力上有所欠缺。基于ODE的流提供了一个可能优雅且简洁的表示，参数更少。然而，它们需要在训练过程中解决ODE，这一过程可能计算开销大且对数值解算器设置的选择敏感。与VAEs和GANs相比，NFs允许对生成样本进行精确的密度估计，并且可以避免训练问题如模式崩溃、梯度消失等（Salimans
    et al. ([2016](#bib.bib282)))。然而，对双射函数和行列式计算的要求可能限制了它在有效建模复杂数据分布方面的能力（Cornish
    et al. ([2020](#bib.bib61))）。
- en: 2.4 Transformers
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 Transformers
- en: 'Transformer (Vaswani et al. ([2017](#bib.bib327))) is an important foundation
    model that has shown exceptional capability across various areas, such as natural
    language processing (Kalyan et al. ([2021](#bib.bib155))), computer vision (Han
    et al. ([2022](#bib.bib121))), time series analysis (Wen et al. ([2023](#bib.bib353))),
    and so on. Recently, there has been a growing trend in developing IL and offline
    RL algorithms based on transformers, with the hope to achieve sequential decision-making
    based on next-token predictions as in natural language processing. As shown in
    Figure [1](#S2.F1 "Figure 1 ‣ 2.4 Transformers ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"), the transformer follows the widely-adopted
    encoder-decoder structure. The encoder maps the input $(x_{1},\cdots,x_{n})$ to
    a sequence of embeddings $(z_{1},\cdots,z_{n})$, and the decoder generates the
    output sequence auto-regressively. That is, the decoder predicts one token $y_{m+1}$
    at a time, based on the input embeddings $(z_{1},\cdots,z_{n})$ and previously-generated
    outputs $(y_{1},\cdots,y_{m})$. Both the encoder and decoder are composed of a
    stack of $L$ identical modules. For clarity, we present each layer in the module
    sequentially following the data flow.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 'Transformer（Vaswani 等（[2017](#bib.bib327)））是一个重要的基础模型，已在自然语言处理（Kalyan 等（[2021](#bib.bib155)））、计算机视觉（Han
    等（[2022](#bib.bib121)））、时间序列分析（Wen 等（[2023](#bib.bib353)））等多个领域展现了卓越的能力。最近，基于
    transformer 的 IL 和离线 RL 算法的发展趋势日益增长，希望实现基于下一个标记预测的顺序决策，如同自然语言处理中的应用。正如图 [1](#S2.F1
    "Figure 1 ‣ 2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") 所示，transformer 遵循广泛采用的编码器-解码器结构。编码器将输入 $(x_{1},\cdots,x_{n})$ 映射为一系列嵌入
    $(z_{1},\cdots,z_{n})$，解码器则自回归地生成输出序列。即，解码器一次预测一个标记 $y_{m+1}$，基于输入嵌入 $(z_{1},\cdots,z_{n})$
    和之前生成的输出 $(y_{1},\cdots,y_{m})$。编码器和解码器都由 $L$ 个相同的模块堆叠而成。为了清晰起见，我们依次展示模块中的每一层，跟随数据流动。'
- en: •
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Positional Encoding: After the token embedding layer, which is shared by $(x_{1},\cdots,x_{n})$
    and $(y_{1},\cdots,y_{m})$, each element is converted to a $d$-dim vector. To
    enable the model to make use of the order of tokens, information about the relative
    or absolute positions of tokens within the sequence is injected through a positional
    encoding. For the $i$-th token, its positional encoding is also a $d$-dim vector,
    of which the $j$-th dimension is $\sin(\frac{i}{10000^{j/d}})$ if $j$ is even
    and $\cos(\frac{i}{10000^{(j-1)/d}})$ otherwise. The positional encoding and token
    embedding are then combined through summation. Per Vaswani et al. ([2017](#bib.bib327)),
    this (periodic function) positional encoding design embeds relative position information
    and helps the model to manage sequences longer than those experienced in training.'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 位置编码：在标记嵌入层之后，该层在 $(x_{1},\cdots,x_{n})$ 和 $(y_{1},\cdots,y_{m})$ 中共享，每个元素被转换为一个
    $d$ 维向量。为了使模型能够利用标记的顺序，通过位置编码注入有关标记在序列中的相对或绝对位置的信息。对于第 $i$ 个标记，其位置编码也是一个 $d$ 维向量，其中第
    $j$ 维是 $\sin(\frac{i}{10000^{j/d}})$ 如果 $j$ 是偶数，反之则是 $\cos(\frac{i}{10000^{(j-1)/d}})$。然后，位置编码和标记嵌入通过加和结合。根据
    Vaswani 等（[2017](#bib.bib327)），这种（周期函数）位置编码设计嵌入了相对位置的信息，帮助模型处理比训练中遇到的序列更长的序列。
- en: •
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Self Multi-Head Attention (MHA): This component utilizes the attention mechanism.
    Given $s$ queries and $t$ key-value pairs, the attention function maps each query
    to a weighted-sum of the values, where the weight assigned to each value is computed
    as the compatibility of the query with the key paired with that value. The queries,
    keys, and values can be represented as $Q\in\mathbb{R}^{s\times d_{q}}$, $K\in\mathbb{R}^{t\times
    d_{k}}$, and $V\in\mathbb{R}^{t\times d_{v}}$, respectively, and the outputs for
    all queries (i.e., $O$) can be computed in parallel as: (For conformability of
    matrix multiplication, $d_{k}=d_{q}$.)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自注意力机制（Self Multi-Head Attention，MHA）：该组件利用注意力机制。给定 $s$ 个查询和 $t$ 个键值对，注意力函数将每个查询映射到值的加权和，其中分配给每个值的权重是计算查询与该值配对的键的兼容性。查询、键和值可以分别表示为
    $Q\in\mathbb{R}^{s\times d_{q}}$、$K\in\mathbb{R}^{t\times d_{k}}$ 和 $V\in\mathbb{R}^{t\times
    d_{v}}$，所有查询的输出（即 $O$）可以并行计算：（为了矩阵乘法的一致性，$d_{k}=d_{q}$。）
- en: '|  | $O=\text{Attention}(Q,K,V)=\text{SoftMax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V,\
    O_{i}=\text{SoftMax}\left(\frac{\langle Q_{i},K_{1}\rangle}{\sqrt{d_{k}}},\cdots,\frac{\langle
    Q_{i},K_{t}\rangle}{\sqrt{d_{k}}}\right)V$ |  | (20) |'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $O=\text{Attention}(Q,K,V)=\text{SoftMax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V,\
    O_{i}=\text{SoftMax}\left(\frac{\langle Q_{i},K_{1}\rangle}{\sqrt{d_{k}}},\cdots,\frac{\langle
    Q_{i},K_{t}\rangle}{\sqrt{d_{k}}}\right)V$ |  | (20) |'
- en: 'Here, SoftMax is a row operator, and the output for query $i$, i.e., $O_{i}$,
    is a weighted sum of the rows of $V$, where the weight for row $j$ is propotional
    to the similarity of $Q_{i}$ (i.e., the $i$-th row of $Q$) and $K_{j}$ measured
    by their inner product $\langle Q_{i},K_{j}\rangle$. The dot products grow large
    with $d_{k}$, which would push the SoftMax function to regions with vanished gradients,
    so the factor $1/\sqrt{d_{k}}$ is introduced. The input embeddings $H_{x}$ (in
    Figure [1](#S2.F1 "Figure 1 ‣ 2.4 Transformers ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) can potentially be used as the matrices
    $Q$, $K$, and $V$ for self attention. However, to enable the model to jointly
    attend to information from different representation subspaces, a multi-head attention
    mechanism is adopted:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '在这里，SoftMax 是一个行操作符，查询 $i$ 的输出，即 $O_{i}$，是 $V$ 行的加权和，其中行 $j$ 的权重与 $Q_{i}$（即
    $Q$ 的第 $i$ 行）和 $K_{j}$ 通过它们的内积 $\langle Q_{i},K_{j}\rangle$ 测量的相似性成正比。点积随着 $d_{k}$
    增大，这会使 SoftMax 函数推向梯度消失的区域，因此引入了因子 $1/\sqrt{d_{k}}$。输入嵌入 $H_{x}$（见图 [1](#S2.F1
    "Figure 1 ‣ 2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")）可以潜在地用作自注意力的矩阵 $Q$、$K$ 和 $V$。然而，为了使模型能够同时关注来自不同表示子空间的信息，采用了多头注意力机制。'
- en: '|  | $\text{MHA}(Q,K,V)=\text{Concat}(\text{head}_{1},\cdots,\text{head}_{h})W^{O},\
    \text{head}_{i}=\text{Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$ |  | (21)
    |'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\text{MHA}(Q,K,V)=\text{Concat}(\text{head}_{1},\cdots,\text{head}_{h})W^{O},\
    \text{head}_{i}=\text{Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$ |  | (21)
    |'
- en: where $Q=K=V=H_{x}\in\mathbb{R}^{n\times d}$, $W_{i}^{Q}\in\mathbb{R}^{d\times
    d^{\prime}}$, $W_{i}^{K}\in\mathbb{R}^{d\times d^{\prime}}$, $W_{i}^{V}\in\mathbb{R}^{d\times
    d^{\prime}}$, $W_{i}^{O}\in\mathbb{R}^{d\times d}$, $d^{\prime}=d/h$, and Concat
    represents the concatenation operation. $W_{i}^{Q},W_{i}^{K},W_{i}^{V}$ convert
    $H_{x}$ to matrices in the $i$-th attention head, providing a distict representation
    subspace. It’s worthy noting that the output of MHA belongs to $\mathbb{R}^{n\times
    d}$, that is, the same in shape as its input.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $Q=K=V=H_{x}\in\mathbb{R}^{n\times d}$，$W_{i}^{Q}\in\mathbb{R}^{d\times d^{\prime}}$，$W_{i}^{K}\in\mathbb{R}^{d\times
    d^{\prime}}$，$W_{i}^{V}\in\mathbb{R}^{d\times d^{\prime}}$，$W_{i}^{O}\in\mathbb{R}^{d\times
    d}$，$d^{\prime}=d/h$，Concat 表示连接操作。$W_{i}^{Q},W_{i}^{K},W_{i}^{V}$ 将 $H_{x}$ 转换为第
    $i$ 个注意力头的矩阵，提供了一个独特的表示子空间。值得注意的是，MHA 的输出属于 $\mathbb{R}^{n\times d}$，即与其输入形状相同。
- en: •
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Add & Normalize: A residual connection (He et al. ([2016](#bib.bib126))) is
    employed around each attention layer, followed by a layer normalization (Ba et al.
    ([2016](#bib.bib18))). As common practice, these two operations are adopted to
    stabilize training of (very) deep networks (e.g., by alleviating ill-posed gradients
    and model degeneration). Suppose the previous layer is $F$, which can be an MHA
    or Feed-forward Network as shown in Figure [1](#S2.F1 "Figure 1 ‣ 2.4 Transformers
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), and
    its input is $X$, then the calculation of this Add & Normalize layer can be denoted
    as $\text{LayerNorm}(F(X)+X)$.'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Add & Normalize：在每个注意力层周围使用了残差连接（He 等人（[2016](#bib.bib126)）），随后进行层归一化（Ba 等人（[2016](#bib.bib18)））。按照常规做法，这两个操作用于稳定（非常）深层网络的训练（例如，通过减轻梯度不适定和模型退化）。假设前一层是
    $F$，它可以是 MHA 或前馈网络，如图 [1](#S2.F1 "Figure 1 ‣ 2.4 Transformers ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") 所示，且其输入为 $X$，那么此 Add & Normalize
    层的计算可以表示为 $\text{LayerNorm}(F(X)+X)$。'
- en: •
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Point-wise Feed-forward Network (FFN): FFN layers are important for a Transformer
    to achieve good performance. Dong et al. ([2021](#bib.bib75)) observe that simply
    stacking MHA modules causes a rank collapse problem (e.g., leading to token-uniformity),
    and that the FFN is one of the important building blocks to mitigate this issue.
    Specifically, a two-layer fully-connected network with a ReLU activation function
    in the middle is applied to each of the $n$ token embeddings separately, leading
    to $n$ new $d$-dim embeddings as output.'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 点-wise 前馈网络（FFN）：FFN 层对于 Transformer 实现良好性能至关重要。Dong 等人 ([2021](#bib.bib75))
    观察到，简单堆叠 MHA 模块会导致秩崩溃问题（例如，导致 token 一致性），而 FFN 是缓解这一问题的重要组成部分。具体而言，一个中间具有 ReLU
    激活函数的两层全连接网络被应用于每个 $n$ 个 token 嵌入，产生 $n$ 个新的 $d$-dim 嵌入作为输出。
- en: •
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Masked Self MHA & Cross MHA: For the decoder, $H_{x}$ is replaced by $H_{y}\in\mathbb{R}^{m\times
    d}$, i.e., embeddings of previously-generated outputs. For rationality, the query
    (of the Masked Self MHA) at each position is only allowed to attend to positions
    up to and including that position, as the other queries correspond to outputs
    yet to generate. This is realized within the Masked Self MHA by masking out corresponding
    compatibility values, i.e., $\langle Q_{i},K_{j}\rangle=-\infty,\ \forall j>i$
    (in Eq. ([20](#S2.E20 "In 2nd item ‣ 2.4 Transformers ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"))). As for the Cross MHA, its queries come
    from the previous decoder layer and key-value pairs are from the output of the
    encoder, i.e., $H_{z}\in\mathbb{R}^{n\times d}$ in Figure [1](#S2.F1 "Figure 1
    ‣ 2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"). Note that these matrices will be embedded with $W_{i}^{Q},W_{i}^{K},W_{i}^{V}$
    to get multiple representation subspaces, as Eq. ([21](#S2.E21 "In 2nd item ‣
    2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")).
    In this way, every query for predicting the next token can attend over all positions
    in the input sequence (via the Cross MHA) and all previously-generated tokens
    (via the Masked Self MHA).'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '掩蔽自注意力 MHA & 交叉 MHA：对于解码器，$H_{x}$ 被 $H_{y}\in\mathbb{R}^{m\times d}$ 替代，即之前生成的输出的嵌入。为了合理性，掩蔽自注意力
    MHA 中的每个位置的查询只能关注到该位置及之前的位置，因为其他查询对应的是尚未生成的输出。这通过在掩蔽自注意力 MHA 中掩蔽相应的兼容性值实现，即 $\langle
    Q_{i},K_{j}\rangle=-\infty,\ \forall j>i$（在公式 ([20](#S2.E20 "In 2nd item ‣ 2.4
    Transformers ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))）。至于交叉
    MHA，它的查询来自于前一层解码器，而键值对来自编码器的输出，即图 [1](#S2.F1 "Figure 1 ‣ 2.4 Transformers ‣ 2
    Background on Deep Generative Models ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions") 中的 $H_{z}\in\mathbb{R}^{n\times
    d}$。注意，这些矩阵将通过 $W_{i}^{Q},W_{i}^{K},W_{i}^{V}$ 嵌入以获得多个表示子空间，如公式 ([21](#S2.E21
    "In 2nd item ‣ 2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))。通过这种方式，每个用于预测下一个 token 的查询可以在输入序列的所有位置（通过交叉 MHA）以及所有之前生成的
    token（通过掩蔽自注意力 MHA）之间进行注意力操作。'
- en: '![Refer to caption](img/189dda9b3fb8fd286086e52f2ac90a1a.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/189dda9b3fb8fd286086e52f2ac90a1a.png)'
- en: 'Figure 1: The Transformer Architecture'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：Transformer 架构
- en: 'Each encoder module outputs an $n\times d$ matrix, and each decoder module
    outputs an $m\times d$ matrix. Here, $n$ and $m$ denote the counts of elements
    in the input and output sequences, respectively, while $d$ represents the embedding
    dimension. This uniformity in the input and output dimensions enables the stacking
    of multiple encoder (or decoder) modules to create a deep model. Each encoder
    (or decoder) module will take the output from the previous encoder (or decoder)
    module. Generally, the transformer architecture can be used in three different
    ways: encoder-only, decoder-only, or encoder-decoder. When using only the encoder,
    its output serves as a representation of the input sequence, suitable for natural
    language understanding tasks such as text classification. While, if only the decoder
    is employed, the Cross MHA modules are removed and this architecture is suitable
    for sequence generation tasks like language modeling. Applications of transformers
    in IL or offline RL usually adopt this decoder-only way. The overall encoder–decoder
    architecture is equipped with the ability to perform both natural language understanding
    and generation, typically used in sequence-to-sequence modeling (e.g., neural
    machine translation).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 每个编码器模块输出一个 $n\times d$ 矩阵，每个解码器模块输出一个 $m\times d$ 矩阵。这里，$n$ 和 $m$ 分别表示输入和输出序列中的元素数量，而
    $d$ 代表嵌入维度。这种输入和输出维度的统一性使得可以堆叠多个编码器（或解码器）模块来创建一个深度模型。每个编码器（或解码器）模块将采用来自前一个编码器（或解码器）模块的输出。通常，变换器架构可以有三种不同的使用方式：仅编码器、仅解码器或编码器-解码器。当仅使用编码器时，它的输出作为输入序列的表示，适用于自然语言理解任务，如文本分类。而如果仅使用解码器，则会移除
    Cross MHA 模块，这种架构适合于序列生成任务，如语言建模。变换器在 IL 或离线 RL 中的应用通常采用这种仅解码器的方式。整体编码器-解码器架构具备同时进行自然语言理解和生成的能力，通常用于序列到序列建模（例如，神经机器翻译）。
- en: 'Compared with other types of neural networks, the transformer architecture
    has significant advantages. Compared with fully-connected layers, the transformer
    is more flexible in handling variable-length inputs (i.e., with different $n$)
    and more parameter-efficient, since the amount of network parameters in a transformer
    is irrelevant to the sequence length $n$ ⁶⁶6The FFN layers are applied to each
    position rather than the whole sequence, and the MHA layers only require updating
    the weights: $W_{i}^{Q},W_{i}^{K},W_{i}^{V},W^{O}$, the sizes of which are irrelevant
    with $n$.. The convolutional layer uses a convolution kernel of size $k<n$ to
    connect pairs of tokens, so, to link distant token pairs, a stack of convolutional
    layers is required. However, with the attention mechanism, each pair of tokens
    can be connected in one MHA layer and the time complexity for matching their corresponding
    query and key is constant (i.e., irrelevant to $n$). This makes the transformer
    excel at capturing long-range dependencies within a sequence. Recurrent layers
    produce a sequence of hidden states $h_{i}$, each dependent on the previous hidden
    state $h_{i-1}$ and the input at position $i$. This sequential nature prevents
    parallel computation. However, in transformers, computations at each position
    are independent, allowing for parallel execution that are essential for processing
    long sequences. Last, both convolutional and recurrent models make structural
    assumptions over the inputs, suitable for image-like and time series data, respectively.
    However, the transformer has few prior assumptions on the data structure and thus
    is a more universal model. Empirical results have shown that the transformer has
    superior performance on a wide-range of tasks and a larger capacity than CNNs
    and RNNs to handle a huge amount of training data.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他类型的神经网络相比，transformer 架构具有显著的优势。与全连接层相比，transformer 在处理变长输入（即具有不同 $n$ 的输入）时更具灵活性，并且参数效率更高，因为
    transformer 中的网络参数数量与序列长度 $n$ 无关 ⁶⁶6FFN 层应用于每个位置而不是整个序列，而 MHA 层仅需要更新权重：$W_{i}^{Q},W_{i}^{K},W_{i}^{V},W^{O}$，其大小与
    $n$ 无关。卷积层使用大小为 $k<n$ 的卷积核来连接成对的标记，因此，要连接远距离的标记对，需要一系列卷积层。然而，借助注意力机制，每对标记可以在一个
    MHA 层中连接，并且匹配它们相应的查询和键的时间复杂度是常数（即与 $n$ 无关）。这使得 transformer 在捕捉序列中的长程依赖关系方面表现出色。递归层产生一系列隐藏状态
    $h_{i}$，每个状态依赖于前一个隐藏状态 $h_{i-1}$ 和位置 $i$ 的输入。这种顺序特性阻碍了并行计算。然而，在 transformers 中，每个位置的计算是独立的，允许进行并行执行，这对于处理长序列至关重要。最后，卷积模型和递归模型对输入做出结构假设，分别适用于图像类和时间序列数据。然而，transformer
    对数据结构的假设较少，因此是一种更为通用的模型。实证结果显示，transformer 在广泛的任务上表现优越，其处理大量训练数据的能力超过了 CNN 和 RNN。
- en: 'As listed above, the transformer contains multiple types of layers. Numerous
    studies have explored modifications or replacements of layers in the standard
    transformer for improvements. For a comprehensive review, please refer to (Lin
    et al. ([2022c](#bib.bib194))). On the other hand, the computation and memory
    complexity of MHA modules are quadratic to the length of the input sequence (i.e.,
    $n$), leading to inefficiency at processing extremely long sequences. Various
    extensions have been developed to enhance either the computational or memory efficiency.
    Additionally, due to the minimal structural assumptions made by the transformer
    about the input data, transformers are prone to overfitting when trained on small-scale
    datasets. Attempts like pretraining the transformer on large-scale unlabeled data
    (Brown et al. ([2020](#bib.bib34))), introducing sparsity assumptions (Child et al.
    ([2019](#bib.bib55))) such as limiting the number and positions of key-value pairs
    that each query can attend to, have been made to solve these issues. Finally,
    extensions to adapt the transformer for particular downstream applications are
    possible. Studies detailed in Section [6](#S6 "6 Transformers in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions") can be viewed as examples of this type
    of extensions.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '如上所述，变压器包含多种类型的层。大量研究探讨了对标准变压器层的修改或替换以进行改进。有关全面的回顾，请参阅（Lin et al. ([2022c](#bib.bib194)))。另一方面，MHA模块的计算和内存复杂度与输入序列的长度（即$n$）呈二次方关系，这在处理极长序列时导致了低效。为了提高计算或内存效率，已经开发了各种扩展。此外，由于变压器对输入数据的结构假设非常少，当在小规模数据集上训练时，变压器容易出现过拟合。为了解决这些问题，已经尝试了诸如在大规模未标记数据上预训练变压器（Brown
    et al. ([2020](#bib.bib34)))、引入稀疏性假设（Child et al. ([2019](#bib.bib55)))，例如限制每个查询可以关注的键值对的数量和位置。最后，可以扩展变压器以适应特定的下游应用。第[6](#S6
    "6 Transformers in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")节中详细描述的研究可以视为这类扩展的示例。'
- en: 2.5 Diffusion Models
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 扩散模型
- en: 'Diffusion models have demonstrated superior performance across multiple domains,
    such as computer vision (Amit et al., [2021](#bib.bib11); Baranchuk et al., [2021](#bib.bib20)),
    natural language processing (Austin et al., [2021b](#bib.bib16); Hoogeboom et al.,
    [2021](#bib.bib137)), and multi-modal learning (Avrahami et al., [2022](#bib.bib17);
    Ramesh et al., [2022](#bib.bib269)), showcasing their impressive capabilities
    of generating detailed and diverse instances. Diffusion models contain two interconnected
    processes: the forward diffusion process and the backward denoising process (Yang
    et al. ([2022a](#bib.bib368))). Specifically, the forward process is predefined
    and transforms the data with a certain (but unknown) distribution, i.e., $x\sim
    P_{X}(\cdot)$, into a (standard Gaussian) random noise $z$. This process progressively
    corrupts the input data by adding a varying scale of noise at each diffusion step.
    Correspondingly, the reverse process uses a neural network as the denoising function
    to gradually undo the forward transformation to reconstruct the data from the
    random noise. There exist three main formulations of diffusion models: Score-based
    Generative Models (SGM) (Song & Ermon, [2019](#bib.bib303); [2020](#bib.bib304)),
    Denoised Diffusion Probabilistic Models (DDPM) (Sohl-Dickstein et al., [2015](#bib.bib299);
    Ho et al., [2020](#bib.bib136); Nichol & Dhariwal, [2021](#bib.bib232)), and Stochastic
    Differential Equations (Score SDE) (Song et al., [2020](#bib.bib306); [2021](#bib.bib307)).
    Next, we introduce these formulations by illustrating their forward/backward processes
    and learning objectives, while discussing their connections with each other.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型在多个领域中表现出卓越的性能，例如计算机视觉（Amit et al., [2021](#bib.bib11)；Baranchuk et al.,
    [2021](#bib.bib20)）、自然语言处理（Austin et al., [2021b](#bib.bib16)；Hoogeboom et al.,
    [2021](#bib.bib137)）和多模态学习（Avrahami et al., [2022](#bib.bib17)；Ramesh et al.,
    [2022](#bib.bib269)），展示了其生成详细且多样化实例的令人印象深刻的能力。扩散模型包含两个相互关联的过程：前向扩散过程和后向去噪过程（Yang
    et al. ([2022a](#bib.bib368)））。具体而言，前向过程是预定义的，它将具有某种（但未知）分布的数据，即 $x\sim P_{X}(\cdot)$，转换为（标准高斯）随机噪声
    $z$。该过程通过在每个扩散步骤中添加不同尺度的噪声来逐步破坏输入数据。相应地，逆过程使用神经网络作为去噪函数，逐渐撤销前向变换，从随机噪声中重建数据。扩散模型主要有三种公式化形式：基于评分的生成模型（SGM）（Song
    & Ermon, [2019](#bib.bib303)；[2020](#bib.bib304)）、去噪扩散概率模型（DDPM）（Sohl-Dickstein
    et al., [2015](#bib.bib299)；Ho et al., [2020](#bib.bib136)；Nichol & Dhariwal,
    [2021](#bib.bib232)）和随机微分方程（评分 SDE）（Song et al., [2020](#bib.bib306)；[2021](#bib.bib307)）。接下来，我们通过阐述这些公式化形式的前向/后向过程和学习目标来介绍它们，同时讨论它们之间的关系。
- en: •
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SGM learns (Stein) score functions (Hyvärinen ([2005](#bib.bib144))) for samples
    at each diffusion step, i.e., $S_{\theta}(x_{t},t),\ t\in[1,\cdots,T]$. As defined,
    the Stein score function of $x$ is the gradient of its log likelihood $\nabla_{x}\log
    P(x)$, with which data samples $x\sim P(\cdot)$ can be generated via various efficient
    score-based sampling schemes such as (Song & Ermon ([2019](#bib.bib303)); Jolicoeur-Martineau
    et al. ([2021](#bib.bib154))). Specifically, the forward process of SGM starts
    from $x_{0}\sim P_{X}(\cdot)$ and injects intensifying Gaussian noise to generate
    $x_{1:T}$: ($\beta_{1:T}$ is a predefined schedule of variance levels.)'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SGM 学习（Stein）评分函数（Hyvärinen ([2005](#bib.bib144)））用于每个扩散步骤的样本，即 $S_{\theta}(x_{t},t),\
    t\in[1,\cdots,T]$。如定义所示，$x$ 的 Stein 评分函数是其对数似然 $\nabla_{x}\log P(x)$ 的梯度，通过此评分函数，可以使用各种高效的基于评分的采样方案生成数据样本
    $x\sim P(\cdot)$，例如（Song & Ermon ([2019](#bib.bib303))；Jolicoeur-Martineau et
    al. ([2021](#bib.bib154))）。具体而言，SGM 的前向过程从 $x_{0}\sim P_{X}(\cdot)$ 开始，并注入逐渐增强的高斯噪声以生成
    $x_{1:T}$：（$\beta_{1:T}$ 是预定义的方差水平调度。）
- en: '|  | $x_{t}\sim F(\cdot&#124;x_{0})={\mathcal{N}}(x_{0},\beta_{t}I),\ 0<\beta_{1}<\beta_{2}<\cdots<\beta_{T}$
    |  | (22) |'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $x_{t}\sim F(\cdot&#124;x_{0})={\mathcal{N}}(x_{0},\beta_{t}I),\ 0<\beta_{1}<\beta_{2}<\cdots<\beta_{T}$
    |  | (22) |'
- en: 'The score functions for each step $S_{\theta}(x_{t},t)$ are trained to approximate
    $\nabla_{x_{t}}\log P(x_{t})$, where $P(x_{t})=\int F(x_{t}|x_{0})P_{X}(x_{0})dx_{0}$.
    In particular, the training objective is as below:'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每一步的评分函数 $S_{\theta}(x_{t},t)$ 经过训练以逼近 $\nabla_{x_{t}}\log P(x_{t})$，其中 $P(x_{t})=\int
    F(x_{t}|x_{0})P_{X}(x_{0})dx_{0}$。特别地，训练目标如下：
- en: '|  | $\min_{\theta}\mathbb{E}_{t\sim{\mathcal{U}}[1,T],x_{0}\sim P_{X}(\cdot),\epsilon\sim{\mathcal{N}}(0,I)}\left[\lambda(t)&#124;&#124;\epsilon+\sqrt{\beta_{t}}S_{\theta}(x_{t},t)&#124;&#124;^{2}\right]$
    |  | (23) |'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\min_{\theta}\mathbb{E}_{t\sim{\mathcal{U}}[1,T],x_{0}\sim P_{X}(\cdot),\epsilon\sim{\mathcal{N}}(0,I)}\left[\lambda(t)&#124;&#124;\epsilon+\sqrt{\beta_{t}}S_{\theta}(x_{t},t)&#124;&#124;^{2}\right]$
    |  | (23) |'
- en: 'Here, ${\mathcal{U}}[1,T]$ is a uniform distribution on $[1,\cdots,T]$, $\lambda(t)$’s
    are positive weighting functions. This objective is derived based on the definition
    of the forward process, i.e., $x_{t}=x_{0}+\sqrt{\beta_{t}}\epsilon,\ \epsilon\sim{\mathcal{N}}(0,I)$.
    For detailed derivations and the definition of $\lambda(t)$, please refer to (Song
    & Ermon ([2019](#bib.bib303))). Regarding the backward generation process, the
    score functions $S_{\theta}(x_{t},t),\ t\in[T,\cdots,1],$ are sequentially used
    as denoising functions to generate $x_{t-1}$ from $x_{t}$. Finally, the required
    data samples $x_{0}\sim P_{X}(\cdot)$ can be acquired. As mentioned, multiple
    score-based sampling schemes can be adopted in this process. We take the annealed
    Langevin dynamics sampling scheme (Song & Ermon ([2019](#bib.bib303))) as an example:
    ($x_{T}=x_{T}^{0},\ x_{0}=x_{0}^{0}$)'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，${\mathcal{U}}[1,T]$ 是 $[1,\cdots,T]$ 上的均匀分布，$\lambda(t)$ 是正的加权函数。这个目标是基于前向过程的定义导出的，即
    $x_{t}=x_{0}+\sqrt{\beta_{t}}\epsilon,\ \epsilon\sim{\mathcal{N}}(0,I)$。有关详细推导和
    $\lambda(t)$ 的定义，请参见 (Song & Ermon ([2019](#bib.bib303)))。关于后向生成过程，评分函数 $S_{\theta}(x_{t},t),\
    t\in[T,\cdots,1],$ 被依次用作去噪函数，以从 $x_{t}$ 生成 $x_{t-1}$。最后，可以获得所需的数据样本 $x_{0}\sim
    P_{X}(\cdot)$。正如提到的，这个过程中可以采用多种基于评分的采样方案。我们以退火 Langevin 动力学采样方案 (Song & Ermon
    ([2019](#bib.bib303))) 为例：（$x_{T}=x_{T}^{0},\ x_{0}=x_{0}^{0}$）
- en: '|  | $x_{t}^{i+1}=x_{t}^{i}+\frac{1}{2}s_{t}S_{\theta}(x_{t}^{i},t)+\sqrt{s_{t}}\epsilon,\
    (i=0,\cdots,N-1),\ x_{t-1}^{0}=x_{t}^{N}$ |  | (24) |'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $x_{t}^{i+1}=x_{t}^{i}+\frac{1}{2}s_{t}S_{\theta}(x_{t}^{i},t)+\sqrt{s_{t}}\epsilon,\
    (i=0,\cdots,N-1),\ x_{t-1}^{0}=x_{t}^{N}$ |  | (24) |'
- en: $N$ and $s_{t}$ are hyperparameters, denoting the number of iterations and step
    size for denoising $x_{t}^{0}$ to $x_{t-1}^{0}$ ($t=T\rightarrow 1$).
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $N$ 和 $s_{t}$ 是超参数，表示去噪 $x_{t}^{0}$ 到 $x_{t-1}^{0}$ 的迭代次数和步长（$t=T\rightarrow
    1$）。
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DDPM gradually adds random noise to the data over a series of time steps $(x_{0},\cdots,x_{T})$
    in the forward process, where $x_{0}=x,\ x_{T}=z$. In particular, the sample at
    each time step is drawn from a Gaussian distribution conditioned on the sample
    from the previous time step: ($\beta_{1:T}$ are predefined.)'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DDPM 在前向过程中逐步向数据添加随机噪声，时间步为 $(x_{0},\cdots,x_{T})$，其中 $x_{0}=x,\ x_{T}=z$。特别地，每个时间步的样本是从条件于前一个时间步样本的高斯分布中抽取的：（$\beta_{1:T}$
    是预定义的。）
- en: '|  | $x_{t}\sim F(\cdot&#124;x_{t-1})={\mathcal{N}}(\sqrt{1-\beta_{t}}x_{t-1},\beta_{t}I)$
    |  | (25) |'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $x_{t}\sim F(\cdot&#124;x_{t-1})={\mathcal{N}}(\sqrt{1-\beta_{t}}x_{t-1},\beta_{t}I)$
    |  | (25) |'
- en: 'With Eq. ([25](#S2.E25 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")), the sample at each step $t$
    can be expressed as a function of $x_{0}$: $x_{t}=\sqrt{\alpha_{t}}x_{0}+\sqrt{1-\alpha_{t}}\epsilon$,
    where $\alpha_{t}=\prod_{s=0}^{t}(1-\beta_{s}),\ \epsilon\sim{\mathcal{N}}(0,I)$
    (Sohl-Dickstein et al. ([2015](#bib.bib299))). $\alpha_{T}$ is designed to be
    close to 0, so that $z=x_{T}$ approximately follows ${\mathcal{N}}(0,I)$. Conversely,
    through the reverse denoising process, $x_{T}$ is converted back to $x_{0}$ step
    by step: ($t=T\rightarrow 1$)'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '根据 Eq. ([25](#S2.E25 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"))，每一步 $t$ 的样本可以表示为 $x_{0}$ 的函数：$x_{t}=\sqrt{\alpha_{t}}x_{0}+\sqrt{1-\alpha_{t}}\epsilon$，其中
    $\alpha_{t}=\prod_{s=0}^{t}(1-\beta_{s}),\ \epsilon\sim{\mathcal{N}}(0,I)$ (Sohl-Dickstein
    等 ([2015](#bib.bib299)))。$\alpha_{T}$ 被设计为接近 0，因此 $z=x_{T}$ 大致遵循 ${\mathcal{N}}(0,I)$。相反，通过反向去噪过程，$x_{T}$
    逐步转换回 $x_{0}$： （$t=T\rightarrow 1$）'
- en: '|  | $x_{t-1}\sim G_{\theta}(\cdot&#124;x_{t})={\mathcal{N}}(\mu_{\theta}(x_{t},t),\Sigma_{\theta}(x_{t},t))$
    |  | (26) |'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $x_{t-1}\sim G_{\theta}(\cdot&#124;x_{t})={\mathcal{N}}(\mu_{\theta}(x_{t},t),\Sigma_{\theta}(x_{t},t))$
    |  | (26) |'
- en: 'where $\mu_{\theta}$ and $\Sigma_{\theta}$ can be implemented as neural networks.
    The objective for learning this denoising function is to match the joint distributions
    of $x_{0:T}$ in the forward and backward processes, i.e., $\min_{\theta}D_{KL}(F(x_{0},\cdots,x_{T})||G_{\theta}(x_{0},\cdots,x_{T}))$,
    which is equivalent to: (Sohl-Dickstein et al., [2015](#bib.bib299))'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\mu_{\theta}$ 和 $\Sigma_{\theta}$ 可以实现为神经网络。学习这个去噪函数的目标是匹配前向和后向过程的 $x_{0:T}$
    的联合分布，即 $\min_{\theta}D_{KL}(F(x_{0},\cdots,x_{T})||G_{\theta}(x_{0},\cdots,x_{T}))$，这等价于：（Sohl-Dickstein
    等，[2015](#bib.bib299)）
- en: '|  | $\min_{\theta}\mathbb{E}_{x_{0}\sim P_{X}(\cdot),x_{1:T}\sim F(\cdot&#124;x_{0})}\left[-\log
    P(x_{T})-\sum_{t=1}^{T}\log\frac{G_{\theta}(x_{t-1}&#124;x_{t})}{F(x_{t}&#124;x_{t-1})}\right]$
    |  | (27) |'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\min_{\theta}\mathbb{E}_{x_{0}\sim P_{X}(\cdot),x_{1:T}\sim F(\cdot&#124;x_{0})}\left[-\log
    P(x_{T})-\sum_{t=1}^{T}\log\frac{G_{\theta}(x_{t-1}&#124;x_{t})}{F(x_{t}&#124;x_{t-1})}\right]$
    |  | (27) |'
- en: 'Note that $P(x_{T})$ and $F(x_{t}|x_{t-1})$ have analytical forms and this
    objective is an upper bound for the negative log-likelihood $\mathbb{E}_{x_{0}\sim
    P_{X}(\cdot)}\left[-\log P_{\theta}(x_{0})\right]$. Although this objective can
    be directly optimized through Monte Carlo sampling, Ho et al. ([2020](#bib.bib136))
    propose a reformulation of it for variance reduction. Since all (forward or backward)
    transformations are based on Gaussian distributions, by specifying the variance
    schedule $\beta_{1:T}$ and fixing the backward variance $\Sigma_{\theta}(x_{t},t)$
    to be $\beta_{t}I$, Eq. ([27](#S2.E27 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2
    Background on Deep Generative Models ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) can be converted
    to: (Ho et al. ([2020](#bib.bib136)))'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '注意到 $P(x_{T})$ 和 $F(x_{t}|x_{t-1})$ 具有解析形式，这一目标是负对数似然 $\mathbb{E}_{x_{0}\sim
    P_{X}(\cdot)}\left[-\log P_{\theta}(x_{0})\right]$ 的上界。虽然可以通过蒙特卡洛采样直接优化这一目标，但
    Ho et al. ([2020](#bib.bib136)) 提出了一个用于方差减少的重构。由于所有（前向或后向）变换都是基于高斯分布的，通过指定方差调度
    $\beta_{1:T}$ 并将后向方差 $\Sigma_{\theta}(x_{t},t)$ 固定为 $\beta_{t}I$，公式 ([27](#S2.E27
    "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on Deep Generative Models ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) 可以转换为：（Ho et al. ([2020](#bib.bib136)))'
- en: '|  | $\min_{\theta}\mathbb{E}_{t\sim{\mathcal{U}}[1,T],x_{0}\sim P_{X}(\cdot),\epsilon\sim{\mathcal{N}}(0,I)}\left[\lambda(t)&#124;&#124;\epsilon-\epsilon_{\theta}(\sqrt{\alpha_{t}}x_{0}+\sqrt{1-\alpha_{t}}\epsilon,t)&#124;&#124;^{2}\right]$
    |  | (28) |'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\min_{\theta}\mathbb{E}_{t\sim{\mathcal{U}}[1,T],x_{0}\sim P_{X}(\cdot),\epsilon\sim{\mathcal{N}}(0,I)}\left[\lambda(t)&#124;&#124;\epsilon-\epsilon_{\theta}(\sqrt{\alpha_{t}}x_{0}+\sqrt{1-\alpha_{t}}\epsilon,t)&#124;&#124;^{2}\right]$
    |  | (28) |'
- en: 'Here, $\lambda(t)$ is a positive weighting function which has a closed-form,
    but in practice, $\lambda(t)$ is set as 1 for all $t$ to improve sample quality.
    Intuitively, the denoising function $\epsilon_{\theta}$ is trained to predict
    the noise injected to samples at each step. Also, by setting $\epsilon_{\theta}(x,t)=-\sqrt{\beta_{t}}S_{\theta}(x,t)$,
    the objective forms of SGM (Eq. ([23](#S2.E23 "In 1st item ‣ 2.5 Diffusion Models
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))) and
    DDPM (Eq. ([28](#S2.E28 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"))) can be unified. Finally, with
    the learned $\epsilon_{\theta}$, the sampling process $x_{t-1}\sim G_{\theta}(\cdot|x_{t})$
    is equivalent to: $x_{t-1}=\frac{1}{\sqrt{1-\beta_{t}}}(x_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(x_{t},t))+\sqrt{\beta_{t}}\epsilon,\
    \epsilon\sim{\mathcal{N}}(0,I)$.'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '在这里，$\lambda(t)$ 是一个正的加权函数，其有封闭形式，但实际上，$\lambda(t)$ 被设置为所有 $t$ 值为 1 以提高样本质量。直观地，去噪函数
    $\epsilon_{\theta}$ 被训练以预测在每一步注入样本的噪声。此外，通过设置 $\epsilon_{\theta}(x,t)=-\sqrt{\beta_{t}}S_{\theta}(x,t)$，SGM
    (公式 ([23](#S2.E23 "In 1st item ‣ 2.5 Diffusion Models ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"))) 和 DDPM (公式 ([28](#S2.E28 "In 2nd item
    ‣ 2.5 Diffusion Models ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"))) 的目标形式可以统一。最后，利用学到的 $\epsilon_{\theta}$，采样过程 $x_{t-1}\sim G_{\theta}(\cdot|x_{t})$
    等价于：$x_{t-1}=\frac{1}{\sqrt{1-\beta_{t}}}(x_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(x_{t},t))+\sqrt{\beta_{t}}\epsilon,\
    \epsilon\sim{\mathcal{N}}(0,I)$。'
- en: •
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Score SDE extends the discrete-time schemes of the previous two methods to
    a unified continuous-time framework, building upon stochastic differential equations
    (SDE). In the forward process, it perturbs data to noise with a diffusion process
    governed by the following differential equation:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Score SDE 将前两种方法的离散时间方案扩展到统一的连续时间框架，基于随机微分方程（SDE）。在前向过程中，它通过以下微分方程将数据扰动成噪声：
- en: '|  | $dx=f(x,t)\>dt+g(t)\>dw,\ t\in[0,T]$ |  | (29) |'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $dx=f(x,t)\>dt+g(t)\>dw,\ t\in[0,T]$ |  | (29) |'
- en: 'where $w$ is the standard Wiener process (Ricciardi ([1976](#bib.bib277))),
    $f(x,t),\ g(t)$ are the diffusion and drift functions, respectively. In particular,
    the forward process of DDPM and SGM can be described by the following two equations
    (Song et al. ([2020](#bib.bib306))), respectively, with specified $f(x,t)$ and
    $g(t)$:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $w$ 是标准的维纳过程（Ricciardi ([1976](#bib.bib277)))，$f(x,t),\ g(t)$ 分别是扩散函数和漂移函数。特别地，DDPM
    和 SGM 的前向过程可以通过以下两个方程描述（Song et al. ([2020](#bib.bib306)))，其中指定了 $f(x,t)$ 和 $g(t)$：
- en: '|  | $dx=-\frac{1}{2}\beta(t)x\>dt+\sqrt{\beta(t)}\>dw,\ dx=\sqrt{\frac{d\beta(t)}{dt}}\>dw$
    |  | (30) |'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $dx=-\frac{1}{2}\beta(t)x\>dt+\sqrt{\beta(t)}\>dw,\ dx=\sqrt{\frac{d\beta(t)}{dt}}\>dw$
    |  | (30) |'
- en: 'The reverse process can be realized via solving the reverse-time SDE (Anderson,
    [1982](#bib.bib12)):'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 逆向过程可以通过求解逆时间SDE来实现（Anderson, [1982](#bib.bib12))：
- en: '|  | $dx=[f(x,t)-g^{2}(t)\nabla_{x}\log P_{t}(x)]\>dt+g(t)\>d\bar{w}$ |  |
    (31) |'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $dx=[f(x,t)-g^{2}(t)\nabla_{x}\log P_{t}(x)]\>dt+g(t)\>d\bar{w}$ |  |
    (31) |'
- en: where $\bar{w}$ denotes the standard Wiener process when time flows backwards,
    $P_{t}(x)$ denotes the distribution of $x$ at time $t$ in the forward process.
    Similarly with SGM, the score function at each time $\nabla_{x_{t}}\log P_{t}(x_{t})$
    can be estimated with an NN-based function $S_{\theta}(x_{t},t)$ through various
    score matching techniques (Vincent ([2011](#bib.bib332)); Song et al. ([2019](#bib.bib305))).
    Further, in (Song et al. ([2020](#bib.bib306))), they propose to get rid of the
    random noise injection $g(t)\>d\bar{w}$ and prove that the resulting equation
    is a probability flow ordinary differential equation (ODE) which shares the same
    marginal densities as those of the reverse-time SDE, and both equations allow
    sampling from the required data distribution, i.e., $P_{X}(x)$.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中，$\bar{w}$表示时间逆流时的标准Wiener过程，$P_{t}(x)$表示正向过程中时间$t$时$x$的分布。与SGM类似，每个时间点的得分函数$\nabla_{x_{t}}\log
    P_{t}(x_{t})$可以通过基于神经网络的函数$S_{\theta}(x_{t},t)$和各种得分匹配技术（Vincent ([2011](#bib.bib332));
    Song et al. ([2019](#bib.bib305)))来估计。此外，在(Song et al. ([2020](#bib.bib306)))中，他们提出了去除随机噪声注入$g(t)\>d\bar{w}$，并证明了所得方程是一个概率流常微分方程（ODE），其边际密度与反向时间SDE的边际密度相同，这两个方程都允许从所需的数据分布中采样，即$P_{X}(x)$。
- en: 3 Variational Auto-Encoders in Offline Policy Learning
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离线策略学习中的3种变分自编码器
- en: 'In this section, we present a comprehensive overview of the applications of
    VAEs in offline policy learning, encompassing both offline RL (Section [3.1](#S3.SS1
    "3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) and IL (Section [3.2](#S3.SS2 "3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")). Within each subsection, we categorize related research works based
    on how VAEs are utilized. Additionally, we provide necessary background and a
    summary on the algorithm design paradigm as a brief tutorial. At the end of each
    subsection, we provide a table to summarize the representative algorithms with
    their key novelties and evaluation tasks, serving as a reference for future research
    in algorithm design and evaluation.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们全面概述了变分自编码器在离线策略学习中的应用，包括离线强化学习（第[3.1节](#S3.SS1 "3.1 离线强化学习 ‣ 3 离线策略学习中的变分自编码器
    ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")）和模仿学习（第[3.2节](#S3.SS2 "3.2 模仿学习 ‣ 3 离线策略学习中的变分自编码器
    ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")）。在每个小节中，我们根据变分自编码器的利用方式对相关研究工作进行分类。此外，我们提供了必要的背景和算法设计范式的总结，作为简要教程。在每个小节的末尾，我们提供了一个表格，总结了具有关键创新和评估任务的代表性算法，为未来的算法设计和评估研究提供参考。
- en: 'The sections focusing on other generative models, i.e., Sections [4](#S4 "4
    Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    - [7](#S7 "7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    follow a structure similar to this section. As mentioned in the beginning of Section
    [2](#S2 "2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), notations
    of functions or variables are not shared across different generative models. However,
    all content is grounded in the fundamental Markov Decision Process (Puterman ([2014](#bib.bib258))),
    denoted as $\mathcal{M}=(\mathcal{S},\mathcal{A},\mathcal{T},r,\rho_{0}(s),\gamma)$.
    $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{T}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]$
    is the transition function, $\rho_{0}:\mathcal{S}\rightarrow[0,1]$ is the distribution
    of the initial state, $r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ is
    the reward function, and $\mathcal{\gamma}\in(0,1]$ is the discount factor.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 关注其他生成模型的部分，即 [4](#S4 "4 生成对抗网络在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查以及未来方向的展望")
    - [7](#S7 "7 扩散模型在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查以及未来方向的展望")，采用了类似于本节的结构。如 [2](#S2
    "2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查以及未来方向的展望") 节开头所提到的，函数或变量的符号在不同的生成模型中并不共享。然而，所有内容都基于基本的马尔可夫决策过程
    (Puterman ([2014](#bib.bib258)))，记作 $\mathcal{M}=(\mathcal{S},\mathcal{A},\mathcal{T},r,\rho_{0}(s),\gamma)$。其中，$\mathcal{S}$
    是状态空间，$\mathcal{A}$ 是动作空间，$\mathcal{T}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]$
    是转移函数，$\rho_{0}:\mathcal{S}\rightarrow[0,1]$ 是初始状态的分布，$r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$
    是奖励函数，$\mathcal{\gamma}\in(0,1]$ 是折扣因子。
- en: 3.1 Offline Reinforcement Learning
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 离线强化学习
- en: 'Interestingly, our findings indicate that VAEs, Normalizing Flows, and Diffusion
    Models are typically integrated with dynamic-programming-based offline RL, whereas
    GANs are employed to enhance model-based offline RL, and Transformers are utilized
    in trajectory-optimization-based offline RL. Therefore, we split the background
    introduction on offline RL into three parts, categorized under the corresponding
    generative models: Section [3.1](#S3.SS1 "3.1 Offline Reinforcement Learning ‣
    3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    Section [4.2](#S4.SS2 "4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"), and Section
    [6.1](#S6.SS1 "6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"). Based on the background on model-free
    offline RL (Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), we
    delve into the specific applications of VAEs for offline RL in Sections [3.1.3](#S3.SS1.SSS3
    "3.1.3 Addressing the Issue of Out-of-Distribution Actions Using VAEs ‣ 3.1 Offline
    Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") -[3.1.5](#S3.SS1.SSS5 "3.1.5 Offline Multi-task/Hierarchical
    RL based on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), preceded by an overview
    in Section [3.1.2](#S3.SS1.SSS2 "3.1.2 An Overview of Applying VAEs in Offline
    Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions").'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们的发现表明，VAEs、规范流和扩散模型通常与动态规划基础的离线强化学习结合使用，而GANs用于增强基于模型的离线强化学习，Transformers则在轨迹优化基础的离线强化学习中得到应用。因此，我们将离线强化学习的背景介绍分为三个部分，分别对应于相关的生成模型：第[3.1](#S3.SS1
    "3.1 离线强化学习 ‣ 3 变分自编码器在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向的展望")节、第[4.2](#S4.SS2
    "4.2 离线强化学习 ‣ 4 生成对抗网络在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向的展望")节和第[6.1](#S6.SS1
    "6.1 离线强化学习 ‣ 6 Transformers在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向的展望")节。基于无模型离线强化学习的背景（第[3.1.1](#S3.SS1.SSS1
    "3.1.1 动态规划基础的离线强化学习背景 ‣ 3.1 离线强化学习 ‣ 3 变分自编码器在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向的展望")节），我们*深入探讨*了VAEs在离线强化学习中的具体应用，见第[3.1.3](#S3.SS1.SSS3
    "3.1.3 使用VAEs解决分布外动作问题 ‣ 3.1 离线强化学习 ‣ 3 变分自编码器在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向的展望")节至第[3.1.5](#S3.SS1.SSS5
    "3.1.5 基于VAEs的离线多任务/层次强化学习 ‣ 3.1 离线强化学习 ‣ 3 变分自编码器在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向的展望")节，前面是第[3.1.2](#S3.SS1.SSS2
    "3.1.2 VAEs在离线强化学习中的应用概述 ‣ 3.1 离线强化学习 ‣ 3 变分自编码器在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向的展望")节的概述。
- en: 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 动态规划基础的离线强化学习背景
- en: 'Dynamic-programming-based offline RL is a main branch of model-free offline
    RL, as detailed in (Levine et al. ([2020](#bib.bib181))). Given an offline dataset
    $D_{\mu}$ collected by the behavior policy $\mu(a|s)$, dynamic-programming-based
    offline RL usually would adopt a constrained policy iteration process as below:
    ($k$ denotes the index of the learning iteration.)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划基础的离线强化学习是无模型离线强化学习的主要分支，如（Levine等人（[2020](#bib.bib181)）所述）。给定由行为策略$\mu(a|s)$收集的离线数据集$D_{\mu}$，动态规划基础的离线强化学习通常采用如下的约束策略迭代过程：（$k$表示学习迭代的索引。）
- en: '|  |  | $\displaystyle\quad Q_{k+1}^{\pi}=\arg\min_{Q}\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[Q(s,a)-\left(r+\gamma\mathbb{E}_{a^{\prime}\sim\pi_{k}(\cdot&#124;s^{\prime})}Q_{k}^{\pi}(s^{\prime},a^{\prime})\right)\right]^{2};$
    |  | (32) |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad Q_{k+1}^{\pi}=\arg\min_{Q}\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[Q(s,a)-\left(r+\gamma\mathbb{E}_{a^{\prime}\sim\pi_{k}(\cdot\mid
    s^{\prime})}Q_{k}^{\pi}(s^{\prime},a^{\prime})\right)\right]^{2};$ |  | (32) |'
- en: '|  |  | $\displaystyle\pi_{k+1}=\arg\max_{\pi}\mathbb{E}_{s\sim D_{\mu}}\left[\mathbb{E}_{a\sim\pi(\cdot&#124;s)}Q_{k+1}^{\pi}(s,a)\right]\
    s.t.\ \mathbb{E}_{s\in D_{\mu}}\left[D(\pi(\cdot&#124;s),\mu(\cdot&#124;s))\right]\leq\epsilon.$
    |  |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\pi_{k+1}=\arg\max_{\pi}\mathbb{E}_{s\sim D_{\mu}}\left[\mathbb{E}_{a\sim\pi(\cdot\mid
    s)}Q_{k+1}^{\pi}(s,a)\right]\ \text{s.t.}\ \mathbb{E}_{s\in D_{\mu}}\left[D(\pi(\cdot\mid
    s),\mu(\cdot\mid s))\right]\leq\epsilon.$ |  |'
- en: 'Here, the first and second equations are referred to as the policy evaluation
    and policy improvement steps, respectively. When updating the Q function, $(s,a,r,s^{\prime})$
    are sampled from $D_{\mu}$ but the target action $a^{\prime}$ is sampled by the
    being-learned policy $\pi_{k}$. If $\pi_{k}(a^{\prime}|s^{\prime})$ differs substantially
    from $\mu(a^{\prime}|s^{\prime})$, out-of-distribution (OOD) actions, which have
    not been explored by $\mu$, can be sampled. Further, the Q-function trained on
    $D_{\mu}$, i.e., $Q_{k+1}^{\pi}$, may erroneously produce over-optimistic values
    for these OOD actions, leading the policy $\pi_{k+1}$ to generate unpredictable
    OOD behaviors. In online RL, such issues are naturally corrected when the agent
    interacts with the environment, attempting the actions it (erroneously) believes
    to be good and observing that in fact they are not. In offline RL, interactions
    with the environment are not accessible, but, alternatively, the over-optimism
    can be controlled by limiting the discrepancy between $\pi$ and $\mu$, as shown
    in Eq. ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based Offline
    Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")). The discrepancy measure
    $D(\cdot||\cdot)$ has multiple candidates. For a comprehensive review, please
    refer to (Levine et al. ([2020](#bib.bib181))). Built upon this basic paradigm
    (i.e., Eq. ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based Offline
    Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))), we introduce some
    practical and representative offline RL algorithms that focus on addressing the
    issue of OOD actions.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，第一个和第二个方程分别被称为策略评估和策略改进步骤。在更新Q函数时，$(s,a,r,s^{\prime})$ 是从 $D_{\mu}$ 中采样的，但目标动作
    $a^{\prime}$ 是由正在学习的策略 $\pi_{k}$ 采样的。如果 $\pi_{k}(a^{\prime}|s^{\prime})$ 与 $\mu(a^{\prime}|s^{\prime})$
    存在显著差异，则可能会采样到 $\mu$ 未探索过的分布外（OOD）动作。此外，基于 $D_{\mu}$ 训练的 Q 函数，即 $Q_{k+1}^{\pi}$，可能会对这些
    OOD 动作产生过于乐观的值，导致策略 $\pi_{k+1}$ 生成不可预测的 OOD 行为。在在线强化学习中，当代理与环境互动时，这些问题会自然得到纠正，代理尝试其（错误地）认为好的动作，并观察到实际情况并非如此。在离线强化学习中，无法与环境进行互动，但可以通过限制
    $\pi$ 和 $\mu$ 之间的差异来控制过度乐观的问题，如方程 ([32](#S3.E32 "在 3.1.1 动态规划基础的离线强化学习背景 ‣ 3.1
    离线强化学习 ‣ 3 离线策略学习中的变分自编码器 ‣ 深度生成模型在离线策略学习中的应用：教程、综述与未来方向")) 所示。差异度量 $D(\cdot||\cdot)$
    有多个候选项。有关详细综述，请参见 (Levine et al. ([2020](#bib.bib181)))。在此基本范式（即方程 ([32](#S3.E32
    "在 3.1.1 动态规划基础的离线强化学习背景 ‣ 3.1 离线强化学习 ‣ 3 离线策略学习中的变分自编码器 ‣ 深度生成模型在离线策略学习中的应用：教程、综述与未来方向"))）的基础上，我们介绍了一些实际和具有代表性的离线强化学习算法，这些算法重点解决
    OOD 动作的问题。
- en: 'Policy constraint methods, such as AWR (Peng et al. ([2019b](#bib.bib251)))
    and AWAC (Nair et al. ([2020](#bib.bib227))), choose the KL-divergence as the
    discrepancy measure, for which the policy improvement step in Eq. ([32](#S3.E32
    "In 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) has a closed-form solution:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '策略约束方法，如 AWR（Peng 等人 ([2019b](#bib.bib251))) 和 AWAC（Nair 等人 ([2020](#bib.bib227)))，选择
    KL 散度作为差异度量，对于这种方法，式 ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) 中的策略改进步骤有一个封闭形式的解：'
- en: '|  |  | $\displaystyle\pi_{k+1}=\arg\max_{\pi}\mathbb{E}_{s\sim D_{\mu}}\left[\mathbb{E}_{a\sim\pi(\cdot&#124;s)}Q_{k+1}^{\pi}(s,a)-\lambda(D_{KL}(\pi(\cdot&#124;s)&#124;&#124;\mu(\cdot&#124;s))-\epsilon)\right]$
    |  | (33) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\pi_{k+1}=\arg\max_{\pi}\mathbb{E}_{s\sim D_{\mu}}\left[\mathbb{E}_{a\sim\pi(\cdot|s)}Q_{k+1}^{\pi}(s,a)-\lambda(D_{KL}(\pi(\cdot|s)||\mu(\cdot|s))-\epsilon)\right]$
    |  | (33) |'
- en: '|  | $\displaystyle\Rightarrow$ | $\displaystyle\pi_{k+1}(a&#124;s)=\mu(a&#124;s)\exp\left(Q_{k+1}^{\pi}(s,a)/\lambda\right)/Z(s)$
    |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Rightarrow$ | $\displaystyle\pi_{k+1}(a|s)=\mu(a|s)\exp\left(Q_{k+1}^{\pi}(s,a)/\lambda\right)/Z(s)$
    |  |'
- en: 'In this expression, $1/Z(s)$ serves as the normalization factor across all
    possible action choices at state $s$; $\lambda$ is the Lagrangian multiplier to
    convert the original constrained optimization problem to an unconstrained one,
    typically set as a constant rather than being optimized. In practice, such an
    policy can be acquired through weighted supervised learning from $D_{\mu}$, where
    $\exp\left(Q_{k+1}^{\pi}(s,a)\right)$ serves as the weight for $(s,a)$. Offline
    RL algorithms based on diffusion models typically adhere to policy constraint
    methods. Therefore, we will provide further details on policy constraint methods
    in Section [7.2](#S7.SS2 "7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions").'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个表达式中，$1/Z(s)$ 作为状态 $s$ 下所有可能动作选择的归一化因子；$\lambda$ 是拉格朗日乘子，用于将原始的约束优化问题转化为无约束问题，通常设置为常数而不是优化。在实际操作中，这样的策略可以通过从
    $D_{\mu}$ 中加权的监督学习来获得，其中 $\exp\left(Q_{k+1}^{\pi}(s,a)\right)$ 作为 $(s,a)$ 的权重。基于扩散模型的离线
    RL 算法通常遵循策略约束方法。因此，我们将在第 [7.2](#S7.SS2 "7.2 Offline Reinforcement Learning ‣ 7
    Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") 节中提供关于策略约束方法的进一步细节。'
- en: 'Policy penalty methods, such as BRAC (Wu et al. ([2019](#bib.bib356))), utilize
    approximated KL-divergence $\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))=\widehat{\mathbb{E}}_{a\sim\pi(\cdot|s)}\left[\log\pi(a|s)-\log\hat{\mu}(a|s)\right]$,
    where $\hat{\mu}$ is learned via Behavioral Cloning from $D_{\mu}$ to estimate
    $\mu$ and $\widehat{\mathbb{E}}$ denotes the estimated expectation via Monte Carlo
    sampling, and modify the reward function as $\tilde{r}(s,a)=r(s,a)-\lambda\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$
    ($\lambda>0$). In this way, the deviation from $\mu$ is implemented as a penalty
    term in the reward function for the policy learning to avoid deviating from $\mu$
    not just in the current step but also in future steps. This method is usually
    implemented in its equivalent form (Wu et al. ([2019](#bib.bib356))) as below:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 策略惩罚方法，如 BRAC（Wu 等人 ([2019](#bib.bib356)))，利用近似的 KL 散度 $\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))=\widehat{\mathbb{E}}_{a\sim\pi(\cdot|s)}\left[\log\pi(a|s)-\log\hat{\mu}(a|s)\right]$，其中
    $\hat{\mu}$ 通过从 $D_{\mu}$ 中的行为克隆学习来估计 $\mu$，而 $\widehat{\mathbb{E}}$ 表示通过蒙特卡罗采样得到的估计期望，并将奖励函数修改为
    $\tilde{r}(s,a)=r(s,a)-\lambda\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$ ($\lambda>0$)。通过这种方式，偏离
    $\mu$ 的行为在奖励函数中实现为惩罚项，以避免在当前步骤以及未来步骤中偏离 $\mu$。该方法通常以等效形式实现（Wu 等人 ([2019](#bib.bib356)))，如下所示：
- en: '|  |  | $\displaystyle Q_{k+1}^{\pi}=\arg\min_{Q}\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[Q(s,a)-\left[r+\gamma\left(\mathbb{E}_{a^{\prime}\sim\pi_{k}(\cdot&#124;s^{\prime})}Q_{k}^{\pi}(s^{\prime},a^{\prime})-\lambda\widehat{D}_{KL}(\pi(\cdot&#124;s^{\prime})&#124;&#124;\mu(\cdot&#124;s^{\prime}))\right)\right]\right]^{2};$
    |  | (34) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle Q_{k+1}^{\pi}=\arg\min_{Q}\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[Q(s,a)-\left[r+\gamma\left(\mathbb{E}_{a^{\prime}\sim\pi_{k}(\cdot|s^{\prime})}Q_{k}^{\pi}(s^{\prime},a^{\prime})-\lambda\widehat{D}_{KL}(\pi(\cdot|s^{\prime})||\mu(\cdot|s^{\prime}))\right)\right]\right]^{2};$
    |  | (34) |'
- en: '|  |  | $\displaystyle\qquad\qquad\qquad\pi_{k+1}=\arg\max_{\pi}\mathbb{E}_{s\sim
    D_{\mu}}\left[\mathbb{E}_{a\sim\pi(\cdot&#124;s)}Q_{k+1}^{\pi}(s,a)-\lambda\widehat{D}_{KL}(\pi(\cdot&#124;s)&#124;&#124;\mu(\cdot&#124;s))\right].$
    |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\qquad\qquad\qquad\pi_{k+1}=\arg\max_{\pi}\mathbb{E}_{s\sim
    D_{\mu}}\left[\mathbb{E}_{a\sim\pi(\cdot|s)}Q_{k+1}^{\pi}(s,a)-\lambda\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))\right].$
    |  |'
- en: One significant disadvantage of this approach is that it requires explicit estimation
    of the behavior policy, i.e., $\hat{\mu}$ and the estimation error could hurt
    the overall performance of this approach.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个显著缺点是它需要对行为策略进行显式估计，即 $\hat{\mu}$，而估计误差可能会影响该方法的整体性能。
- en: 'Support constraint methods, such as BCQ (Fujimoto et al. ([2019](#bib.bib100)))
    and BEAR (Kumar et al. ([2019](#bib.bib175))), propose to confine the support
    of the learned policy within that of the behavior policy to avoid OOD actions,
    because constraining the learned policy to remain close in distribution to the
    behavior policy, as in the previous two methods, can negatively impact the policy
    performance, especially when the behavior policy is substantially suboptimal.
    As an example, BEAR proposes to replace the discrepancy constraint in Eq. ([32](#S3.E32
    "In 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) with a support constraint: $\pi\in\{\pi^{\prime}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\
    |\ \pi^{\prime}(a|s)=0\ \text{whenever}\ \mu(a|s)<\epsilon\}$ ⁷⁷7In practice,
    they use a sampled maximum mean discrepancy (MMD, Gretton et al. ([2012](#bib.bib109)))
    between the being-learned policy and behavior policy to implement such a support
    constraint, of which the effectiveness is empirically justified.. With such a
    support constraint, the target actions $a^{\prime}$ used in policy evaluation
    (i.e., the first equation in Eq. ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))),
    would all satisfy $\mu(a|s)\geq\epsilon$ and so are in-distribution actions, that
    is:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '支持约束方法，如 BCQ（Fujimoto 等人 ([2019](#bib.bib100))) 和 BEAR（Kumar 等人 ([2019](#bib.bib175)))，建议将学习策略的支持限制在行为策略的范围内，以避免
    OOD（超出分布）动作，因为将学习策略限制在与行为策略的分布接近的范围内，如前两种方法所示，可能会对策略性能产生负面影响，特别是当行为策略显著低于最优时。举例来说，BEAR
    提议将等式 ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based Offline
    Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) 中的差异约束替换为支持约束：$\pi\in\{\pi^{\prime}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\
    |\ \pi^{\prime}(a|s)=0\ \text{whenever}\ \mu(a|s)<\epsilon\}$ ⁷⁷7实际上，他们使用被学习策略和行为策略之间的采样最大均值差异（MMD，Gretton
    等人 ([2012](#bib.bib109))) 来实现这种支持约束，其有效性在经验上得到了验证。通过这种支持约束，策略评估中使用的目标动作 $a^{\prime}$（即等式
    ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement
    Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))）将满足 $\mu(a|s)\geq\epsilon$，因此都是在分布内的动作，即：'
- en: '|  | $\displaystyle Q_{k+1}^{\pi}=\arg\min_{Q}\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[Q(s,a)-\left(r+\gamma\mathbb{E}_{a^{\prime}\sim\pi_{k}(\cdot&#124;s^{\prime})\
    s.t.\ \mu(a&#124;s)\geq\epsilon}\ Q_{k}^{\pi}(s^{\prime},a^{\prime})\right)\right]^{2}$
    |  | (35) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q_{k+1}^{\pi}=\arg\min_{Q}\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[Q(s,a)-\left(r+\gamma\mathbb{E}_{a^{\prime}\sim\pi_{k}(\cdot|s^{\prime})\
    \text{且}\ \mu(a|s)\geq\epsilon}\ Q_{k}^{\pi}(s^{\prime},a^{\prime})\right)\right]^{2}$
    |  | (35) |'
- en: 'Pessimistic value methods regularize Q-function directly to avoid overly optimistic
    values for OOD actions, as an alternative to imposing (support or distributional)
    constraints on the policy. As a representative, CQL (Kumar et al. ([2020](#bib.bib176)))
    removes the policy constraint in Eq. ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) and
    modifies the policy evaluation process as below:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '悲观值方法直接对Q函数进行正则化，以避免对OOD（超出分布）动作的过于乐观的值，作为对策略施加（支持或分布）约束的一种替代方案。作为一个代表，CQL（Kumar等人
    ([2020](#bib.bib176))) 在公式 ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) 中移除了策略约束，并如下修改了策略评估过程：'
- en: '|  | $\displaystyle Q_{k+1}^{\pi}=\arg\min_{Q}$ | $\displaystyle\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[Q(s,a)-\left(r+\gamma\mathbb{E}_{a^{\prime}\sim\pi_{k}(\cdot&#124;s^{\prime})}Q_{k}^{\pi}(s^{\prime},a^{\prime})\right)\right]^{2}+$
    |  | (36) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q_{k+1}^{\pi}=\arg\min_{Q}$ | $\displaystyle\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[Q(s,a)-\left(r+\gamma\mathbb{E}_{a^{\prime}\sim\pi_{k}(\cdot|s^{\prime})}Q_{k}^{\pi}(s^{\prime},a^{\prime})\right)\right]^{2}+$
    |  | (36) |'
- en: '|  |  | $\displaystyle\lambda\left[\mathbb{E}_{s\sim D_{\mu},a\sim\pi_{k}(\cdot&#124;s)}Q(s,a)-\mathbb{E}_{(s,a)\sim
    D_{\mu}}Q(s,a)\right]$ |  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\lambda\left[\mathbb{E}_{s\sim D_{\mu},a\sim\pi_{k}(\cdot|s)}Q(s,a)-\mathbb{E}_{(s,a)\sim
    D_{\mu}}Q(s,a)\right]$ |  |'
- en: 'Here, the second term (with $\lambda>0$) can be viewed as a regularizer for
    the policy evaluation process, which minimizes Q-values under the being-learned
    policy distribution, i.e., $\pi_{k}(\cdot|s)$, and maximizes the Q-values for
    $(s,a)$ within $D_{\mu}$. Intuitively, this ensures that high Q-values are only
    assigned to in-distribution actions. Suppose $\widehat{Q}^{\pi}=\lim_{k\rightarrow\infty}Q^{\pi}_{k}$
    and define $Q^{\pi}$ as the true Q-function for $\pi$, they theoretically prove
    that $\mathbb{E}_{a\sim\pi(\cdot|s)}\left[\widehat{Q}^{\pi}(s,a)\right]\leq\mathbb{E}_{a\sim\pi(\cdot|s)}\left[Q^{\pi}(s,a)\right],\
    \forall s\in D_{\mu}$, with a high probability, when $\lambda$ is large enough.
    Thus, this algorithm mitigates the overestimation issue with a theoretical guarantee.
    However, it tends to learn an overly conservative Q function ⁸⁸8According to Theorem
    3.2 in (Kumar et al. ([2020](#bib.bib176))), the minimum value of $\lambda$ is
    linked to $\max_{s\in D_{\mu}}\frac{1}{\sqrt{|D_{\mu}(s)|}}$, where $|D_{\mu}(s)|$
    denotes the frequency of state $s$ in $D_{\mu}$. Given that $D_{\mu}$’s coverage
    could be limited and certain states would have low visitation frequencies, this
    can result in a high value of $\lambda$. However, as shown in Eq. ([36](#S3.E36
    "In 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")), this same $\lambda$ value is
    used in the Q-update for every state, potentially leading to underestimation for
    other states in $D_{\mu}$..'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，第二项（其中$\lambda>0$）可以被视为策略评估过程的正则化项，它最小化在正在学习的策略分布下的Q值，即$\pi_{k}(\cdot|s)$，并最大化在$D_{\mu}$中的$(s,a)$的Q值。直观地，这确保了高Q值仅分配给在分布内的动作。假设$\widehat{Q}^{\pi}=\lim_{k\rightarrow\infty}Q^{\pi}_{k}$，并定义$Q^{\pi}$为$\pi$的真实Q函数，他们理论上证明了$\mathbb{E}_{a\sim\pi(\cdot|s)}\left[\widehat{Q}^{\pi}(s,a)\right]\leq\mathbb{E}_{a\sim\pi(\cdot|s)}\left[Q^{\pi}(s,a)\right],\
    \forall s\in D_{\mu}$，当$\lambda$足够大时，以高概率成立。因此，这种算法在理论上保证了缓解过估计问题。然而，它倾向于学习一个过于保守的Q函数
    ⁸⁸8根据（Kumar等人 ([2020](#bib.bib176))) 的定理3.2，$\lambda$的最小值与$\max_{s\in D_{\mu}}\frac{1}{\sqrt{|D_{\mu}(s)|}}$有关，其中$|D_{\mu}(s)|$表示状态$s$在$D_{\mu}$中的频率。由于$D_{\mu}$的覆盖范围可能有限，某些状态的访问频率较低，这可能导致$\lambda$值很高。然而，如公式
    ([36](#S3.E36 "In 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement
    Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) 所示，相同的$\lambda$值在每个状态的Q更新中使用，这可能导致$D_{\mu}$中其他状态的低估。'
- en: 3.1.2 An Overview of Applying VAEs in Offline Reinforcement Learning
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 应用变分自编码器（VAE）于离线强化学习的概述
- en: 'A major use of VAEs for offline RL is to estimate the behavior policy from
    the offline data $D_{\mu}$. Specifically, a CVAE (see Eq. ([7](#S2.E7 "In 3rd
    item ‣ 2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) in Section [2.1](#S2.SS1 "2.1 Variational Auto-Encoders
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) is
    adopted for this estimation:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs 在离线强化学习中的一个主要用途是从离线数据 $D_{\mu}$ 中估计行为策略。具体来说，一个 CVAE（见 Eq. ([7](#S2.E7
    "第3项 ‣ 2.1 变分自编码器 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")）在 Section [2.1](#S2.SS1
    "2.1 变分自编码器 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")）被采用来进行这种估计：
- en: '|  | $\max_{\theta,\phi}\mathbb{E}_{(s,a)\sim D_{\mu}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;a,s)}\left[\log P_{\theta}(a&#124;z,s)\right]-D_{KL}(P_{\phi}(z&#124;a,s)&#124;&#124;P_{\theta}(z&#124;s))\right]$
    |  | (37) |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\theta,\phi}\mathbb{E}_{(s,a)\sim D_{\mu}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot|a,s)}\left[\log P_{\theta}(a|z,s)\right]-D_{KL}(P_{\phi}(z|a,s)||P_{\theta}(z|s))\right]$
    |  | (37) |'
- en: 'Corresponding to Eq. ([7](#S2.E7 "In 3rd item ‣ 2.1 Variational Auto-Encoders
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), $s$
    and $a$ here work as the condition variable $c$ and data sample $x$, respectively.
    As introduced in Section [2.1](#S2.SS1 "2.1 Variational Auto-Encoders ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), the objective above
    constitutes a lower bound for $\mathbb{E}_{(s,a)\sim D_{\mu}}\left[\log P_{\theta}(a|s)\right]$,
    i.e., the typical supervised learning objective. After training, actions at state
    $s$ can be sampled from the estimated behavior policy $\hat{\mu}(\cdot|s)$ as:
    $z\sim P_{\theta}(\cdot|s),\hat{a}\sim P_{\theta}(\cdot|s,z)$. In practice, the
    reconstruction term $\log P_{\theta}(a|z,s)$ can be replaced with $-||a-\hat{a}||_{2}^{2}$
    for continuous action spaces, and the prior $P_{\theta}(z|s)$ can simply be chosen
    as $\mathcal{N}(z|0,I)$. Following the $\beta$-VAE (i.e., Eq. ([5](#S2.E5 "In
    1st item ‣ 2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))), a factor $\beta>0$ is usually introduced as the weight
    of the KL term in Eq. ([37](#S3.E37 "In 3.1.2 An Overview of Applying VAEs in
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) to
    balance the two objective terms.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 对应于 Eq. ([7](#S2.E7 "第3项 ‣ 2.1 变分自编码器 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望"))，这里的
    $s$ 和 $a$ 分别作为条件变量 $c$ 和数据样本 $x$。如 Section [2.1](#S2.SS1 "2.1 变分自编码器 ‣ 2 深度生成模型背景
    ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望") 中介绍的，上述目标构成了 $\mathbb{E}_{(s,a)\sim D_{\mu}}\left[\log
    P_{\theta}(a|s)\right]$ 的下界，即典型的监督学习目标。经过训练后，可以从估计的行为策略 $\hat{\mu}(\cdot|s)$ 中采样在状态
    $s$ 下的动作：$z\sim P_{\theta}(\cdot|s),\hat{a}\sim P_{\theta}(\cdot|s,z)$。在实践中，对于连续动作空间，重构项
    $\log P_{\theta}(a|z,s)$ 可以用 $-||a-\hat{a}||_{2}^{2}$ 代替，先验 $P_{\theta}(z|s)$
    可以简单地选择为 $\mathcal{N}(z|0,I)$。继承 $\beta$-VAE（即 Eq. ([5](#S2.E5 "第1项 ‣ 2.1 变分自编码器
    ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")）），通常会引入一个因子 $\beta>0$ 作为 Eq. ([37](#S3.E37
    "3.1.2 在离线强化学习中的 VAE 应用概述 ‣ 3.1 离线强化学习 ‣ 3 变分自编码器在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望"))
    中 KL 项的权重，以平衡两个目标项。
- en: 'With the estimated behavior policy $\hat{\mu}$, the policy penalty and support
    constraint offline RL methods introduced in Section [3.1.1](#S3.SS1.SSS1 "3.1.1
    Background on Dynamic-Programming-based Offline Reinforcement Learning ‣ 3.1 Offline
    Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") can be naturally applied. As a representative of support
    constraint methods, BCQ (Fujimoto et al. ([2019](#bib.bib100))) trains such $\hat{\mu}$
    and defines the policy to learn based on $\hat{\mu}$ as $\pi(\cdot|s)=\hat{a}+\xi(\cdot|s,\hat{a})$,
    where $\hat{a}$ is a sample from $\hat{\mu}(\cdot|s)$ and $\xi(\cdot|s,\hat{a})$
    is a bounded and learnable residual term. $\pi$ is trained to maximize the approximated
    Q-values as in standard RL ⁹⁹9For BCQ, multiple Q functions are learned simultaneously,
    and the target value for policy evaluation is specifically designed based on these
    Q functions, which is though not our focus. Please refer to (Fujimoto et al. ([2019](#bib.bib100)))
    for the details.. In this way, the action support of $\pi$ is constrained to be
    close to the behavior policy’s. Furthermore, inspired by (Shamir ([2018](#bib.bib290))),
    AQL (Wei et al. ([2021](#bib.bib348))) proposes to improve the estimation for
    the behavior policy $\mu$ using a residual generative model $W_{1}(W_{2}G(s)+\hat{a})$,
    where $W_{1,2}$ and $G(\cdot)$ constitute a residual network and $\hat{a}\sim\hat{\mu}(\cdot|s)$
    is used as a residul term. They claim that such a residual structure could effectively
    reduce the estimation error (compared with using $\hat{a}$) for the behavior policy
    $\mu$.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '使用估计行为策略 $\hat{\mu}$，第 [3.1.1](#S3.SS1.SSS1 "3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") 节中介绍的策略惩罚和支持约束离线
    RL 方法可以自然地应用。作为支持约束方法的代表，BCQ（Fujimoto et al. ([2019](#bib.bib100))) 训练这种 $\hat{\mu}$
    并基于 $\hat{\mu}$ 定义要学习的策略为 $\pi(\cdot|s)=\hat{a}+\xi(\cdot|s,\hat{a})$，其中 $\hat{a}$
    是从 $\hat{\mu}(\cdot|s)$ 中采样得到的，$\xi(\cdot|s,\hat{a})$ 是一个有界且可学习的残差项。$\pi$ 被训练以最大化近似的
    Q 值，如标准 RL 中所示⁹⁹9 对于 BCQ，多个 Q 函数会同时学习，策略评估的目标值是基于这些 Q 函数专门设计的，这虽然不是我们的重点。详细信息请参见（Fujimoto
    et al. ([2019](#bib.bib100)))。通过这种方式，$\pi$ 的动作支持被约束在接近行为策略的范围内。此外，受到（Shamir ([2018](#bib.bib290)))
    的启发，AQL（Wei et al. ([2021](#bib.bib348))) 提出了使用残差生成模型 $W_{1}(W_{2}G(s)+\hat{a})$
    来改进对行为策略 $\mu$ 的估计，其中 $W_{1,2}$ 和 $G(\cdot)$ 组成了一个残差网络，$\hat{a}\sim\hat{\mu}(\cdot|s)$
    被用作残差项。他们声称这种残差结构可以有效减少对行为策略 $\mu$ 的估计误差（与使用 $\hat{a}$ 相比）。'
- en: Regarding the benefits of using a VAE as the policy network, compared to feedforward
    networks composed solely of fully-connected layers, VAEs excel at capturing the
    multiple modalities present in $D_{\mu}$, which could be collected by a diverse
    set of policies, by utilizing the latent variable $z$. Also, the action generation
    process, $z\sim\mathcal{N}(0,I)$ and $a\sim P_{\theta}(\cdot|s,z)$, allows for
    stochastic action sampling. Compared to other deep generative models, VAEs may
    be less expressive but are more lightweight than normalizing flows and diffusion
    models and can provide more stable training than GANs.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 关于使用 VAE 作为策略网络的好处，相比于仅由全连接层组成的前馈网络，VAE 在捕捉存在于 $D_{\mu}$ 中的多种模式方面表现优异，这些模式可以由多样的策略集合来收集，通过利用潜在变量
    $z$。此外，动作生成过程 $z\sim\mathcal{N}(0,I)$ 和 $a\sim P_{\theta}(\cdot|s,z)$ 允许进行随机动作采样。与其他深度生成模型相比，VAE
    可能表达能力较弱，但比标准化流和扩散模型更轻量，并且比 GAN 提供更稳定的训练。
- en: Based on these background knowledge, we provide a review of VAE-based offline
    RL algorithms in the following subsections, with a particular focus on works whose
    primary novelty lies in the use of VAEs. One category of such algorithms seeks
    to enhance aforementioned offline RL algorithms via the use of VAEs. They either
    modify the learning objective to further mitigate the issue of OOD actions, or
    apply augmentation/conversion to the offline data for improved learning. The other
    category concentrates on extended offline RL setups, such as hierarchical or multi-task
    offline RL, leveraging the fact that the latent variable $z$ can be learned as
    embeddings for tasks or subgoals/subtasks within a task. Consequently, $P_{\theta}(a|s,z)$
    can be interpreted as a task-conditioned policy (for multi-task RL) or a subtask-conditioned
    policy within a hierarchical policy structure (for hierarchical RL).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些背景知识，我们在以下小节中提供了基于VAE的离线RL算法的综述，特别关注那些主要创新在于使用VAE的研究。此类算法的一类旨在通过使用VAE来增强上述离线RL算法。它们要么修改学习目标以进一步缓解OOD动作问题，要么对离线数据进行增强/转换以提高学习效果。另一类算法则集中在扩展的离线RL设置上，例如层次化或多任务离线RL，利用潜变量$z$可以作为任务或任务内子目标/子任务的嵌入进行学习。因此，$P_{\theta}(a|s,z)$可以被解释为任务条件的策略（用于多任务RL）或在层次策略结构中的子任务条件策略（用于层次化RL）。
- en: 3.1.3 Addressing the Issue of Out-of-Distribution Actions Using VAEs
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 使用VAE解决分布外动作问题
- en: 'As introduced in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), there
    are four categories of algorithms for mitigating the issue of OOD actions. VAEs
    have been used to improve three categories among them, which are detailed as follows.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '正如[3.1.1](#S3.SS1.SSS1 "3.1.1 Background on Dynamic-Programming-based Offline
    Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")节中介绍的那样，有四类算法用于缓解OOD动作问题。VAE已被用于改进其中的三类，具体如下。'
- en: 'Applying support constraints: PLAS (Zhou et al. ([2020](#bib.bib393))) proposes
    that the support constraint can be simply applied to the latent space of a VAE.
    In particular, they first estimate the behavior policy as a CVAE $\hat{\mu}$ with
    Eq. ([37](#S3.E37 "In 3.1.2 An Overview of Applying VAEs in Offline Reinforcement
    Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")). Ideally, after training,
    for latent variables $z$ which have high probabilities under $P_{\theta}(z|s)$,
    the corresponding decoder $P_{\theta}(a|s,z)$ should output high-probability actions
    under the behavior policy distribution (i.e., in-distribution actions), since
    $\mu(a|s)$ is estimated as $\int P_{\theta}(z|s)P_{\theta}(a|s,z)\ dz$. In this
    case, they learn a latent space policy $\pi(z|s)$ and use it in conjunction with
    the pretrained (and fixed) decoder $P_{\theta}(a|s,z)$ as the mapping from $s$
    to $a$. The output of $\pi$ is constrained to $[-\sigma,\sigma]$, i.e., the high
    probability area of the prior distribution $\mathcal{N}(z;0,I)$, to ensure that
    $(P_{\theta}\circ\pi)(a|s)$ outputs in-distribution actions. As introduced in
    Section [5.2](#S5.SS2 "5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing
    Flows in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), NF-based offline RL
    algorithms adopt the same algorithm idea. SPOT (Wu et al. ([2022](#bib.bib355)))
    suggests that the constraint in BEAR (equivalently for a deterministic policy,
    $\log\mu(\pi(s)|s)\geq\epsilon,\forall s\in D_{\mu}$) can be relaxed to $\mathbb{E}_{s\sim
    D_{\mu}}\left[\log\mu(\pi(s)|s)\right]\geq\epsilon^{\prime}$ for practicality.
    Then, the constrained policy improvement step (i.e., the second equation in Eq.
    ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement
    Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))) can be converted to:
    (We use $\pi$ and $Q$ to represent the policy and Q function from now on, for
    simplicity.)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 应用支持约束：PLAS（Zhou 等人 ([2020](#bib.bib393))) 提出了可以简单地将支持约束应用于 VAE 的潜在空间。具体而言，他们首先估计行为策略为一个
    CVAE $\hat{\mu}$，如方程 ([37](#S3.E37 "在 3.1.2 VAE 在离线强化学习中的应用概述 ‣ 3.1 离线强化学习 ‣ 3
    潜在变量自编码器在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、综述与未来方向展望")) 所示。理想情况下，在训练后，对于在 $P_{\theta}(z|s)$
    下具有高概率的潜在变量 $z$，对应的解码器 $P_{\theta}(a|s,z)$ 应该在行为策略分布下输出高概率动作（即，分布内动作），因为 $\mu(a|s)$
    被估计为 $\int P_{\theta}(z|s)P_{\theta}(a|s,z)\ dz$。在这种情况下，他们学习一个潜在空间策略 $\pi(z|s)$
    并将其与预训练（且固定）的解码器 $P_{\theta}(a|s,z)$ 一起使用，作为从 $s$ 到 $a$ 的映射。$\pi$ 的输出被限制在 $[-\sigma,\sigma]$
    范围内，即先验分布 $\mathcal{N}(z;0,I)$ 的高概率区域，以确保 $(P_{\theta}\circ\pi)(a|s)$ 输出分布内动作。如在第
    [5.2](#S5.SS2 "5.2 基于离线数据的强化学习 ‣ 5 离线策略学习中的归一化流 ‣ 离线策略学习的深度生成模型：教程、综述与未来方向展望")
    节介绍，基于 NF 的离线 RL 算法采用了相同的算法思想。SPOT（Wu 等人 ([2022](#bib.bib355))) 建议，为了实用性，可以将 BEAR
    中的约束（等效于确定性策略，$\log\mu(\pi(s)|s)\geq\epsilon,\forall s\in D_{\mu}$）放宽为 $\mathbb{E}_{s\sim
    D_{\mu}}\left[\log\mu(\pi(s)|s)\right]\geq\epsilon^{\prime}$。然后，约束策略改进步骤（即方程 ([32](#S3.E32
    "在 3.1.1 动态规划基础的离线强化学习背景 ‣ 3.1 离线强化学习 ‣ 3 潜在变量自编码器在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、综述与未来方向展望"))
    中的第二个方程）可以转换为：（从现在开始，我们用 $\pi$ 和 $Q$ 来表示策略和 Q 函数，以简化表达。）
- en: '|  | $\max_{\pi}\mathbb{E}_{s\sim D_{\mu}}\left[Q(s,\pi(s))+\lambda(\log\mu(\pi(s)&#124;s)-\epsilon^{\prime})\right]$
    |  | (38) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{s\sim D_{\mu}}\left[Q(s,\pi(s))+\lambda(\log\mu(\pi(s)|s)-\epsilon^{\prime})\right]$
    |  | (38) |'
- en: 'Here, $\lambda>0$ is the Lagrangian multiplier. Again, they explicitly model
    $\mu$ as a CVAE $\hat{\mu}=(P_{\phi},P_{\theta})$ and propose to approximate $\log\mu(a|s)$
    as below: ($z^{(l)}\sim P_{\phi}(\cdot|s,a),\ l=1,\cdots,L$.)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\lambda>0$ 是拉格朗日乘子。再次，他们显式地将 $\mu$ 建模为一个 CVAE $\hat{\mu}=(P_{\phi},P_{\theta})$，并提出将
    $\log\mu(a|s)$ 近似为以下形式：($z^{(l)}\sim P_{\phi}(\cdot|s,a),\ l=1,\cdots,L$.)
- en: '|  | $\log\mu(a&#124;s)\approx\log P_{\theta}(a&#124;s)=\log\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;s,a)}\left[\frac{P_{\theta}(a&#124;s,z)P_{\theta}(z&#124;s)}{P_{\phi}(z&#124;s,a)}\right]\approx\log\left[\frac{1}{L}\sum_{l=1}^{L}\frac{P_{\theta}(a&#124;s,z^{(l)})P_{\theta}(z^{(l)}&#124;s)}{P_{\phi}(z^{(l)}&#124;s,a)}\right]$
    |  | (39) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | $\log\mu(a|s)\approx\log P_{\theta}(a|s)=\log\mathbb{E}_{z\sim P_{\phi}(\cdot|s,a)}\left[\frac{P_{\theta}(a|s,z)P_{\theta}(z|s)}{P_{\phi}(z|s,a)}\right]\approx\log\left[\frac{1}{L}\sum_{l=1}^{L}\frac{P_{\theta}(a|s,z^{(l)})P_{\theta}(z^{(l)}|s)}{P_{\phi}(z^{(l)}|s,a)}\right]$
    |  | (39) |'
- en: 'As introduced in Section [2.3](#S2.SS3 "2.3 Normalizing Flows ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), normalizing flows enable
    exact density estimation, which can potentially eliminate the need for sample-based
    approximations as in Eq. ([39](#S3.E39 "In 3.1.3 Addressing the Issue of Out-of-Distribution
    Actions Using VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '如[2.3节](#S2.SS3 "2.3 Normalizing Flows ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")中介绍的，归一化流使得精确的密度估计成为可能，这可能消除对基于样本的近似（如 Eq. ([39](#S3.E39
    "In 3.1.3 Addressing the Issue of Out-of-Distribution Actions Using VAEs ‣ 3.1
    Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")）的需求。'
- en: 'Applying policy penalty: TD3-CVAE (Rezaeifar et al. ([2022](#bib.bib275)))
    proposes to replace the penalty term in Eq. (LABEL:brac_obj) (i.e, $\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$)
    with a prediction error $b(s,a)=||a-P_{\theta}\circ P_{\phi}(s,a)||$, where $\hat{\mu}=(P_{\phi},P_{\theta})$
    is the pretrained CVAE for estimating $\mu$. Intuitively, if an action $a$ from
    the being-learned policy $\pi(\cdot|s)$ corresponds to a high prediction error
    under $\hat{\mu}(\cdot|s)$, $a$ is probably an OOD action and so $(s,a)$ should
    be assigned with a high penalty. Further, they theoretically show the equivalence
    (under certain conditions) of $b(s,a)$ and a KL-divergence penalty term $D_{KL}(\pi(\cdot|s)||\pi_{b}(\cdot|s))$,
    where $\pi_{b}(\cdot|s)=\text{SoftMax}(-b(s,\cdot)/\tau)$ and $\tau>0$ is a temperature
    parameter. Compared with $\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$, $b(s,a)$
    is easier to approximate and brings superior empirical performance. BRAC+ (Zhang
    et al. ([2021a](#bib.bib384))) points out that estimating the penalty term $D_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$
    as $\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$ in BRAC requires generating
    a large number of samples to reduce the estimation variance. Therefore, they propose
    an upper bound for $D_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$, which has an analytical
    form:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 应用策略惩罚：TD3-CVAE (Rezaeifar 等人 ([2022](#bib.bib275))) 提出了用预测误差 $b(s,a)=||a-P_{\theta}\circ
    P_{\phi}(s,a)||$ 替换 Eq. (LABEL:brac_obj) 中的惩罚项（即 $\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$），其中
    $\hat{\mu}=(P_{\phi},P_{\theta})$ 是用于估计 $\mu$ 的预训练 CVAE。直观上，如果来自被学习的策略 $\pi(\cdot|s)$
    的动作 $a$ 在 $\hat{\mu}(\cdot|s)$ 下对应高预测误差，则 $a$ 可能是一个 OOD 动作，因此 $(s,a)$ 应该分配一个高惩罚。此外，他们从理论上展示了在某些条件下
    $b(s,a)$ 和 KL 散度惩罚项 $D_{KL}(\pi(\cdot|s)||\pi_{b}(\cdot|s))$ 的等价性，其中 $\pi_{b}(\cdot|s)=\text{SoftMax}(-b(s,\cdot)/\tau)$
    且 $\tau>0$ 是温度参数。与 $\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$ 相比，$b(s,a)$
    更易于近似，并且带来更优的经验表现。BRAC+ (Zhang 等人 ([2021a](#bib.bib384))) 指出，在 BRAC 中，将惩罚项 $D_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$
    估计为 $\widehat{D}_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$ 需要生成大量样本以减少估计方差。因此，他们提出了 $D_{KL}(\pi(\cdot|s)||\mu(\cdot|s))$
    的一个上界，其具有解析形式：
- en: '|  |  | $\displaystyle D_{KL}(\pi(\cdot&#124;s)&#124;&#124;\mu(\cdot&#124;s))\approx\mathbb{E}_{a\sim\pi(\cdot&#124;s)}\left[\log\pi(a&#124;s)-\log\hat{\mu}(a&#124;s)\right]$
    |  | (40) |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle D_{KL}(\pi(\cdot|s)||\mu(\cdot|s))\approx\mathbb{E}_{a\sim\pi(\cdot|s)}\left[\log\pi(a|s)-\log\hat{\mu}(a|s)\right]$
    |  | (40) |'
- en: '|  |  | $\displaystyle\leq\mathbb{E}_{a\sim\pi(\cdot&#124;s)}\left[\log\pi(a&#124;s)\right]-\mathbb{E}_{a\sim\pi(\cdot&#124;s)}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;s,a)}\left[\log P_{\theta}(a&#124;s,z)\right]-D_{KL}(P_{\phi}(z&#124;s,a)&#124;&#124;P_{\theta}(z&#124;s))\right]$
    |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\mathbb{E}_{a\sim\pi(\cdot|s)}\left[\log\pi(a|s)\right]-\mathbb{E}_{a\sim\pi(\cdot|s)}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot|s,a)}\left[\log P_{\theta}(a|s,z)\right]-D_{KL}(P_{\phi}(z|s,a)||P_{\theta}(z|s))\right]$
    |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{a\sim\pi(\cdot&#124;s),z\sim P_{\phi}(\cdot&#124;s,a)}\left[\log\pi(a&#124;s)-\log
    P_{\theta}(a&#124;s,z)\right]-\mathbb{E}_{a\sim\pi(\cdot&#124;s)}\left[D_{KL}(P_{\phi}(z&#124;s,a)&#124;&#124;P_{\theta}(z&#124;s))\right]$
    |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{a\sim\pi(\cdot|s),z\sim P_{\phi}(\cdot|s,a)}\left[\log\pi(a|s)-\log
    P_{\theta}(a|s,z)\right]-\mathbb{E}_{a\sim\pi(\cdot|s)}\left[D_{KL}(P_{\phi}(z|s,a)||P_{\theta}(z|s))\right]$
    |  |'
- en: 'Here, the inequality is based on the fact that a CVAE $\hat{\mu}=(P_{\phi},P_{\theta})$
    is used to estimate $\mu$ and $\mathbb{E}_{z\in P_{\phi}(\cdot|s,a)}\left[\log
    P_{\theta}(a|s,z)\right]-D_{KL}(P_{\phi}(z|s,a)||P_{\theta}(z|s))$ constitutes
    a variational lower bound for $\log\hat{\mu}(a|s)$, as shown in Eq. ([7](#S2.E7
    "In 3rd item ‣ 2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")). Given that $P_{\theta}(a|s,z)$, $P_{\theta}(z|s)$,
    and $P_{\phi}(z|s,a)$ all have Gaussian outputs and suppose that the policy $\pi(a|s)$
    is also Gaussian, then both $\log\pi(a|s)-\log P_{\theta}(a|s,z)$ and $D_{KL}(P_{\phi}(z|s,a)||P_{\theta}(z|s))$
    have analytical forms, which can reduce the sample variance. However, sampling
    for $a$ and $z$ is still required.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，不等式基于一个事实，即使用 CVAE $\hat{\mu}=(P_{\phi},P_{\theta})$ 来估计 $\mu$，并且 $\mathbb{E}_{z\in
    P_{\phi}(\cdot|s,a)}\left[\log P_{\theta}(a|s,z)\right]-D_{KL}(P_{\phi}(z|s,a)||P_{\theta}(z|s))$
    构成了 $\log\hat{\mu}(a|s)$ 的变分下界，如方程 ([7](#S2.E7 "In 3rd item ‣ 2.1 Variational
    Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))
    所示。鉴于 $P_{\theta}(a|s,z)$、$P_{\theta}(z|s)$ 和 $P_{\phi}(z|s,a)$ 都具有高斯输出，并且假设策略
    $\pi(a|s)$ 也是高斯的，那么 $\log\pi(a|s)-\log P_{\theta}(a|s,z)$ 和 $D_{KL}(P_{\phi}(z|s,a)||P_{\theta}(z|s))$
    都具有解析形式，这可以减少样本方差。然而，仍然需要对 $a$ 和 $z$ 进行采样。'
- en: 'Applying pessimistic values: CPQ (Xu et al. ([2022a](#bib.bib361))) extends
    the idea of CQL to safe RL, where the policy is trained to maximize its Q-values
    while minimizing the accumulative cost $Q_{c}(s,a)=c(s,a)+\mathbb{E}_{s^{\prime},a^{\prime}\sim\pi(\cdot|s^{\prime})}[Q_{c}(s^{\prime},a^{\prime})]$.
    In particular, they train the cost Q-function $Q_{c}$ to assign high costs to
    OOD actions, such that, by constraining $Q_{c}$ as in standard safe RL algorithms,
    OOD actions can be avoided at the same time. To realize this, the objective for
    $Q_{c}$ is designed as below, which is similar in form with Eq. ([36](#S3.E36
    "In 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")):'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '应用悲观值：CPQ（Xu 等人 ([2022a](#bib.bib361))）将 CQL 的思想扩展到安全 RL，其中策略被训练以最大化其 Q 值，同时最小化累积成本
    $Q_{c}(s,a)=c(s,a)+\mathbb{E}_{s^{\prime},a^{\prime}\sim\pi(\cdot|s^{\prime})}[Q_{c}(s^{\prime},a^{\prime})]$。特别地，他们训练成本
    Q 函数 $Q_{c}$，以对 OOD 动作分配高成本，从而通过限制 $Q_{c}$，像在标准安全 RL 算法中一样，同时避免 OOD 动作。为实现这一目标，$Q_{c}$
    的目标函数设计如下，其形式类似于方程 ([36](#S3.E36 "In 3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))：'
- en: '|  | $\min_{Q_{c}}\mathbb{E}_{(s,a,c,s^{\prime})\sim D_{\mu}}\left[Q_{c}(s,a)-\left(c+\gamma\mathbb{E}_{a^{\prime}\sim\pi(\cdot&#124;s^{\prime})}Q_{c}(s^{\prime},a^{\prime})\right)\right]^{2}-\lambda\mathbb{E}_{s\sim
    D_{\mu},a\sim\tilde{\mu}(\cdot&#124;s)}Q_{c}(s,a)$ |  | (41) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{Q_{c}}\mathbb{E}_{(s,a,c,s^{\prime})\sim D_{\mu}}\left[Q_{c}(s,a)-\left(c+\gamma\mathbb{E}_{a^{\prime}\sim\pi(\cdot|s^{\prime})}Q_{c}(s^{\prime},a^{\prime})\right)\right]^{2}-\lambda\mathbb{E}_{s\sim
    D_{\mu},a\sim\tilde{\mu}(\cdot|s)}Q_{c}(s,a)$ |  | (41) |'
- en: Here, $\tilde{\mu}$ is defined based on $\hat{\mu}=(P_{\phi},P_{\theta})$. In
    particular, $\forall a\sim\tilde{\mu}(\cdot|s),\ D_{KL}(P_{\phi}(z|s,a)||P_{\theta}(z|s))\geq
    d$, where $P_{\theta}(z|s)=\mathcal{N}(z;0,I)$ and $d$ is a predefined threshold.
    Intuitively, $\tilde{\mu}(\cdot|s)$ produces OOD actions $a$ of which the corresponding
    posterior distribution $P_{\phi}(z|s,a)$ deviates significantly with its training
    target $P_{\theta}(z|s)$. MCQ (Lyu et al. ([2022](#bib.bib208))) explores mild
    but enough conservatism for offline RL to mitigate the underestimation issue of
    CQL. Their algorithm’s design does not hinge on VAEs; instead, the VAE is employed
    solely for estimating $\mu$ as in previous works, so we do not provide details
    here. For similar reasons, we skip introductions of UAC (Guan et al. ([2023](#bib.bib113)))
    and O-RAAC (Urpí et al. ([2021](#bib.bib324))), both of which can be considered
    as extensions of BCQ, a support constraint method.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\tilde{\mu}$ 是基于 $\hat{\mu}=(P_{\phi},P_{\theta})$ 定义的。特别地，$\forall a\sim\tilde{\mu}(\cdot|s),\
    D_{KL}(P_{\phi}(z|s,a)||P_{\theta}(z|s))\geq d$，其中 $P_{\theta}(z|s)=\mathcal{N}(z;0,I)$，$d$
    是预定义的阈值。直观地，$\tilde{\mu}(\cdot|s)$ 生成的 OOD 动作 $a$ 使得对应的后验分布 $P_{\phi}(z|s,a)$
    与其训练目标 $P_{\theta}(z|s)$ 显著偏离。MCQ（Lyu et al. ([2022](#bib.bib208))) 探索了适度但足够的保守性，以减轻
    CQL 的低估问题。他们算法的设计并不依赖于 VAEs；相反，VAE 仅用于像之前的工作一样估计 $\mu$，因此我们在这里不提供详细信息。出于类似的原因，我们跳过了
    UAC（Guan et al. ([2023](#bib.bib113))) 和 O-RAAC（Urpí et al. ([2021](#bib.bib324)))
    的介绍，它们都可以被视为 BCQ 的扩展，一种支持约束方法。
- en: 3.1.4 Data Augmentation and Transformation with VAEs
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 使用变分自编码器（VAEs）进行数据增强和转换
- en: 'Data augmentation: The provided offline data $D_{\mu}$ may have limited coverage
    of the state-action space or lack diversity in behavioral patterns. Consequently,
    constraint- or pessimism-based algorithms may learn sub-optimal policies with
    limited generalization capabilities in the entire environment. In this case, VAEs
    have been used for data augmentation, aiming at improving the coverage or diversity
    of the offline data. (1) ROMI (Wang et al. ([2021](#bib.bib336))) is a model-based
    data augmentation strategy utilizing reverse rollouts. They first learn the backward
    dynamic model $\widehat{\mathcal{T}}_{\text{rev}}(s|s^{\prime},a)$, reward model
    $\hat{r}(s,a)$, and reverse policy $\hat{\mu}_{\text{rev}}(a|s^{\prime})$ from
    $D_{\mu}$ via simple supervised learning, where $s^{\prime}$ denotes the next
    state. Specifically, $\hat{\mu}_{\text{rev}}(a|s^{\prime})$ is modeled with a
    CVAE $(P_{\phi},P_{\theta})$, and its training objective is the same as Eq. ([37](#S3.E37
    "In 3.1.2 An Overview of Applying VAEs in Offline Reinforcement Learning ‣ 3.1
    Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) but to replace $s$ with $s^{\prime}$.
    With a random sample $s_{t+1}$ from $D_{\mu}$, a reverse rollout (of length $h$)
    can be generated as below:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强：提供的离线数据 $D_{\mu}$ 可能对状态-动作空间的覆盖有限，或在行为模式上缺乏多样性。因此，基于约束或悲观算法可能学习到具有有限泛化能力的次优策略。在这种情况下，VAEs
    被用于数据增强，旨在提高离线数据的覆盖或多样性。 (1) ROMI（Wang et al. ([2021](#bib.bib336))) 是一种基于模型的数据增强策略，利用逆向回滚。他们首先通过简单的监督学习从
    $D_{\mu}$ 学习反向动态模型 $\widehat{\mathcal{T}}_{\text{rev}}(s|s^{\prime},a)$、奖励模型 $\hat{r}(s,a)$
    和反向策略 $\hat{\mu}_{\text{rev}}(a|s^{\prime})$，其中 $s^{\prime}$ 表示下一个状态。具体来说，$\hat{\mu}_{\text{rev}}(a|s^{\prime})$
    使用 CVAE $(P_{\phi},P_{\theta})$ 进行建模，其训练目标与 Eq. ([37](#S3.E37 "在 3.1.2 变分自编码器在离线强化学习中的应用概述
    ‣ 3.1 离线强化学习 ‣ 3 变分自编码器在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向")) 相同，只是将 $s$ 替换为
    $s^{\prime}$。通过从 $D_{\mu}$ 中随机抽取样本 $s_{t+1}$，可以生成如下的逆向回滚（长度为 $h$）：
- en: '|  | $\left[(s_{t-i},a_{t-i},r_{t-i},s_{t+1-i})\ &#124;\ a_{t-i}\sim\hat{\mu}_{\text{rev}}(\cdot&#124;s_{t+1-i}),s_{t-i}\sim\widehat{\mathcal{T}}_{\text{rev}}(\cdot&#124;s_{t+1-i},a_{t-i}),r_{t-i}\sim\hat{r}(s_{t-i},a_{t-i})\right]_{i=0}^{h-1}$
    |  | (42) |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left[(s_{t-i},a_{t-i},r_{t-i},s_{t+1-i})\ &#124;\ a_{t-i}\sim\hat{\mu}_{\text{rev}}(\cdot&#124;s_{t+1-i}),s_{t-i}\sim\widehat{\mathcal{T}}_{\text{rev}}(\cdot&#124;s_{t+1-i},a_{t-i}),r_{t-i}\sim\hat{r}(s_{t-i},a_{t-i})\right]_{i=0}^{h-1}$
    |  | (42) |'
- en: 'Unlike the forward generation process, such a reverse manner prevents rollout
    trajectories that end in OOD states. Also, the CVAE makes it possible for stochastic
    inference: $z\sim\mathcal{N}(0,I),a\sim P_{\theta}(\cdot|s^{\prime},z)$, which
    improves the diversity. These generated rollouts are then combined with $D_{\mu}$
    for the use of offline RL. This work is closely related to offline model-based
    RL, which is another important branch of offline RL and mainly introduced in Section
    [4.2](#S4.SS2 "4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"). (2) To enable the learned
    policy to generalize to OOD states, SDC (Zhang et al. ([2022](#bib.bib385))) suggests
    training the policy on perturbed states and motivating it to revert to in-distribution
    states from any state deviations. In particular, a forward dynamic model $\widehat{\mathcal{T}}(s^{\prime}|s,a)$
    and a CVAE-based state transition model $\widehat{U}(s^{\prime}|s)$ is learned
    from $D_{\mu}$ through supervised learning. At a state $\tilde{s}$ perturbed from
    $s$, the policy $\pi$ is trained to minimize $\text{MMD}\left(\widehat{\mathcal{T}}(\cdot|\tilde{s},\pi(\cdot|\tilde{s}))||\widehat{U}(\cdot|s)\right)$,
    i.e., to produce actions that can lead it back to the next state $s^{\prime}$
    in the original trajectory from the perturbation $\tilde{s}$. MMD denotes the
    maximum mean discrepancy. (3) Han & Kim ([2022](#bib.bib120)) point out that the
    latent space of a VAE pretrained on $D_{\mu}$ can capture the data distribution
    in $D_{\mu}$. Based on that, they suggest selectively augmenting the data region
    that is sparse in the original dataset through data generation with the VAE. However,
    that paper does not provide details on training the VAE or measuring sparsity
    through the latent space. (4) KFC (Weissenbacher et al. ([2022](#bib.bib350)))
    suggests inferring symmetries (Hambidge ([1967](#bib.bib119))) of the underlying
    dynamics of an environment using a VAE forward prediction model, and applying
    such symmetry transformations to generate new data points as data augmentation.
    This work requires extensive knowledge of control theory, so it will not be discussed
    in depth here.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '与前向生成过程不同，这种反向方式可以防止生成以OOD状态结束的滚动轨迹。此外，CVAE使得随机推断成为可能：$z\sim\mathcal{N}(0,I),a\sim
    P_{\theta}(\cdot|s^{\prime},z)$，这提高了多样性。这些生成的滚动轨迹随后与$D_{\mu}$结合，用于离线RL。这项工作与离线基于模型的RL密切相关，这是离线RL的另一个重要分支，主要介绍在第[4.2](#S4.SS2
    "4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")节。(2) 为了使学习到的策略能够推广到OOD状态，SDC（Zhang
    et al. ([2022](#bib.bib385)))建议在扰动状态下训练策略，并促使其从任何状态偏差中恢复到分布内状态。特别地，利用监督学习从$D_{\mu}$中学习一个前向动态模型$\widehat{\mathcal{T}}(s^{\prime}|s,a)$和一个基于CVAE的状态转移模型$\widehat{U}(s^{\prime}|s)$。在从$s$扰动得到的状态$\tilde{s}$下，策略$\pi$被训练以最小化$\text{MMD}\left(\widehat{\mathcal{T}}(\cdot|\tilde{s},\pi(\cdot|\tilde{s}))||\widehat{U}(\cdot|s)\right)$，即产生可以将其从扰动$\tilde{s}$带回原始轨迹中的下一个状态$s^{\prime}$的动作。MMD表示最大均值差异。(3)
    Han & Kim ([2022](#bib.bib120))指出，在$D_{\mu}$上预训练的VAE的潜在空间可以捕捉$D_{\mu}$中的数据分布。基于此，他们建议通过使用VAE进行数据生成，选择性地扩展原始数据集中稀疏的数据区域。然而，该论文没有提供有关VAE训练或通过潜在空间测量稀疏性的细节。(4)
    KFC（Weissenbacher et al. ([2022](#bib.bib350)))建议使用VAE前向预测模型推断环境动态的对称性（Hambidge
    ([1967](#bib.bib119)))，并应用这些对称性变换生成新的数据点作为数据增强。这项工作需要对控制理论有深入了解，因此在此不作详细讨论。'
- en: 'Data conversion: VAEs have been used to transform the states or actions in
    the offline dataset to simplify the learning task. Here, we present two notable
    works in this direction. When the state space is very high-dimensional (e.g.,
    images), directly applying offline RL to the raw data may be challenging. Thus,
    Rafailov et al. ([2021](#bib.bib266)) propose using VAEs to get compact representations
    $z$ of high-dimensional states $s$ to improve the learning efficiency. $z$ effectively
    represents $s$, since the VAE is trained to reconstruct $s$ from $z$ while adhering
    to variational regulations (i.e., the KL term in Eq. ([2](#S2.E2 "In 2.1 Variational
    Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))).
    However, this data conversion manner is not specific to VAEs, and other generative
    models with an encoder-decoder structure, such as the normalizing flows and transformers,
    could potentially be employed for this purpose. On the other hand, SAQ (Luo et al.
    ([2023](#bib.bib205))) proposes to convert continuous actions to discrete ones,
    to make it significantly simpler to implement constraint/regulation-based offline
    RL methods. To realize this, they train a VQ-VAE (introduced in Section [2.1](#S2.SS1
    "2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) on $D_{\mu}$, which can map a continuous action $a$ at
    a given state $s$ to a discrete variable $\tilde{a}$ (through its encoder) for
    training and map a given discrete variable back to the original action space (with
    its decoder) for evaluation. This state-conditioned action discretization scheme
    is learned by:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换：VAEs 已被用于将离线数据集中的状态或动作转换，以简化学习任务。在这里，我们介绍两个在这方面的显著工作。当状态空间非常高维（例如，图像）时，直接将离线
    RL 应用于原始数据可能具有挑战性。因此，Rafailov 等人（[2021](#bib.bib266)）建议使用 VAEs 获取高维状态 $s$ 的紧凑表示
    $z$，以提高学习效率。$z$ 有效地表示 $s$，因为 VAE 被训练来从 $z$ 重建 $s$，同时遵循变分规则（即方程 ([2](#S2.E2 "在
    2.1 变分自编码器 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")) 中的 KL 项）。然而，这种数据转换方式并不特定于
    VAEs，其他具有编码器-解码器结构的生成模型，如正规化流和变换器，也可能用于此目的。另一方面，SAQ（Luo 等人（[2023](#bib.bib205)））建议将连续动作转换为离散动作，以显著简化约束/调节型离线
    RL 方法的实现。为实现这一点，他们训练了一个 VQ-VAE（在第 [2.1](#S2.SS1 "2.1 变分自编码器 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")
    节中介绍），它可以将给定状态 $s$ 下的连续动作 $a$ 映射到离散变量 $\tilde{a}$（通过其编码器）进行训练，并将给定离散变量映射回原始动作空间（通过其解码器）进行评估。这种状态条件下的动作离散化方案通过以下方式学习：
- en: '|  | $\max_{\theta,\phi,e_{1:k}}\mathbb{E}_{(s,a)\sim D_{\mu}}\left[\log P_{\theta}(a&#124;e_{i})-&#124;&#124;\text{sg}\left[P_{\phi}(s,a)\right]-e_{i}&#124;&#124;_{2}^{2}-\beta&#124;&#124;P_{\phi}(s,a)-\text{sg}\left[e_{i}\right]&#124;&#124;_{2}^{2}\right]$
    |  | (43) |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\theta,\phi,e_{1:k}}\mathbb{E}_{(s,a)\sim D_{\mu}}\left[\log P_{\theta}(a|e_{i})-|
    \text{sg}\left[P_{\phi}(s,a)\right]-e_{i}||_{2}^{2}-\beta||P_{\phi}(s,a)-\text{sg}\left[e_{i}\right]||_{2}^{2}\right]$
    |  | (43) |'
- en: Here, $e_{1:k}$ is the codebook and represents the $k$ discretized actions;
    $i=\arg\min_{j}||P_{\phi}(s,a)-e_{j}||_{2}$ is the index of the nearest action
    latent for the embedding $P_{\phi}(s,a)$. Applying the pretrained VQ-VAE encoder
    $P_{\phi}$ on $D_{\mu}$ leads to discretized actions, i.e., $(s,a)\rightarrow(s,e_{i})$.
    For discrete action spaces, the estimation of the constraint terms in offline
    RL, such as the approximated behavior policy $\hat{\mu}(\tilde{a}|s)$ and KL divergence
    $\widehat{D}_{KL}(\pi(\tilde{a}|s)||\mu(\tilde{a}|s))$, could be easier and more
    accurate.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$e_{1:k}$ 是代码本，表示 $k$ 个离散化的动作；$i=\arg\min_{j}||P_{\phi}(s,a)-e_{j}||_{2}$
    是与嵌入 $P_{\phi}(s,a)$ 最近的动作潜变量的索引。将预训练的 VQ-VAE 编码器 $P_{\phi}$ 应用于 $D_{\mu}$ 会得到离散化的动作，即
    $(s,a)\rightarrow(s,e_{i})$。对于离散动作空间，离线 RL 中约束项的估计，例如近似行为策略 $\hat{\mu}(\tilde{a}|s)$
    和 KL 散度 $\widehat{D}_{KL}(\pi(\tilde{a}|s)||\mu(\tilde{a}|s))$，可能会更容易和更准确。
- en: '| Algorithm | VAE Type | VAE Usage | Evaluation Task |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | VAE 类型 | VAE 用法 | 评估任务 |'
- en: '| BCQ | CVAE | Estimating $\mu$ (Support Constraint) | MuJoCo |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| BCQ | CVAE | 估计 $\mu$（支持约束） | MuJoCo |'
- en: '| AQL | CVAE | Estimating $\mu$ (Improved Estimation for $\mu$) | D4RL (L)
    |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| AQL | CVAE | 估计 $\mu$（改进的 $\mu$ 估计） | D4RL (L) |'
- en: '| PLAS | CVAE | Estimating $\mu$ (Support Constraint) | D4RL (L, A, K), Real
    Robot |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| PLAS | CVAE | 估计 $\mu$（支持约束） | D4RL (L, A, K), 实际机器人 |'
- en: '| SPOT | CVAE | Estimating $\mu$ (Support Constraint) | D4RL (L, M) |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| SPOT | CVAE | 估计 $\mu$（支持约束） | D4RL（L, M） |'
- en: '| TD3-CVAE | CVAE | Providing prediction errors (Policy Penalty) | D4RL (L,
    A) |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| TD3-CVAE | CVAE | 提供预测误差（策略惩罚） | D4RL（L, A） |'
- en: '| BRAC+ | CVAE | Estimating $\mu$ and $D_{KL}(\pi(a&#124;s)&#124;&#124;\mu(a&#124;s))$
    (Policy Penalty) | D4RL (L) |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| BRAC+ | CVAE | 估计 $\mu$ 和 $D_{KL}(\pi(a|s)||\mu(a|s))$（策略惩罚） | D4RL（L） |'
- en: '| CPQ | $\beta$-CVAE | Estimating $\mu$ and using the latent space for OOD
    detection (Pessimistic Value) | MuJoCo |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| CPQ | $\beta$-CVAE | 估计 $\mu$ 并使用潜在空间进行OOD检测（悲观值） | MuJoCo |'
- en: '| MCQ | CVAE | Estimating $\mu$ (Pessimistic Value) | D4RL (L) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| MCQ | CVAE | 估计 $\mu$（悲观值） | D4RL（L）'
- en: '| UAC | CVAE | Estimating $\mu$ (Support Constraint) | D4RL (L, M, A) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| UAC | CVAE | 估计 $\mu$（支持约束） | D4RL（L, M, A） |'
- en: '| O-RAAC | $\beta$-CVAE | Estimating $\mu$ (Support Constraint) | D4RL (L)
    |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| O-RAAC | $\beta$-CVAE | 估计 $\mu$（支持约束） | D4RL（L） |'
- en: '| ROMI | CVAE | Estimating the reverse behavior policy $\mu_{\text{rev}}(a&#124;s^{\prime})$
    (Data Augmentation) | D4RL (L, M, M2d) |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| ROMI | CVAE | 估计逆行为策略 $\mu_{\text{rev}}(a|s^{\prime})$（数据增强） | D4RL（L, M,
    M2d） |'
- en: '| SDC | CVAE | Estimating the state transition model $U(s^{\prime}&#124;s)$
    of $D_{\mu}$ (Data Augmentation) | GridWorld, D4RL (L) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| SDC | CVAE | 估计 $D_{\mu}$ 的状态转移模型 $U(s^{\prime}|s)$（数据增强） | GridWorld，D4RL（L）
    |'
- en: '| KFC | VAE | Modelling $U(s^{\prime}&#124;s)$ for inference of dynamic symmetries
    (Data Transformation) | D4RL (L, M, A, K), MetaWorld, RoboSuite |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| KFC | VAE | 建模 $U(s^{\prime}|s)$ 以推断动态对称性（数据转换） | D4RL（L, M, A, K），MetaWorld，RoboSuite
    |'
- en: '| SAQ | VQ-VAE | Discretizing the action space to simplify the learning (Data
    Conversation) | D4RL (L, M, A, K), Robomimic |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| SAQ | VQ-VAE | 离散化动作空间以简化学习（数据转换） | D4RL（L, M, A, K），Robomimic |'
- en: '| BOReL | CVAE (T) | Embedding MDPs for Multi-task RL | GridWorld, Meta-MuJoCo
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| BOReL | CVAE（T） | 嵌入MDP用于多任务强化学习 | GridWorld，Meta-MuJoCo |'
- en: '| OPAL | $\beta$-CVAE (T) | Embedding skills for Hierarchical RL | D4RL (M,
    K) |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| OPAL | $\beta$-CVAE（T） | 嵌入技能用于层次化强化学习 | D4RL（M, K） |'
- en: '| TACO-RL | CVAE (T) | Embedding skills for Hierarchical RL | CALVIN, Real
    Robot |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| TACO-RL | CVAE（T） | 嵌入技能用于层次化强化学习 | CALVIN，真实机器人 |'
- en: '| HiGoC | CVAE | Generating subgoals (Hierarchical RL) | D4RL (M), CARLA |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| HiGoC | CVAE | 生成子目标（层次化强化学习） | D4RL（M），CARLA |'
- en: '| FLAP | CVAE | Generating subgoals (Hierarchical RL) | Real Robot |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| FLAP | CVAE | 生成子目标（层次化强化学习） | 真实机器人 |'
- en: 'Table 1: Summary of VAE-based offline RL algorithms. In Column 1, we list representative
    (but not all) algorithms in this section. These algorithms are grouped by their
    categories. Regarding the VAE types, $\beta$-CVAE refers to an integration of
    CVAE and $\beta$-VAE, where a weight $\beta$ is added to the KL term in CVAE.
    The annotation (T) means that the VAE is implemented on trajectories rather than
    individual state transitions. The evaluation tasks are listed in Column 4\. Most
    works are evaluated on D4RL (Fu et al. ([2020](#bib.bib99))), which provides offline
    datasets for various tasks, including Locomotion (L), AntMaze (M), Adroit (A),
    Kitchen (K), Maze2d (M2d), etc. MuJoCo (Todorov et al. ([2012](#bib.bib320)))
    and CARLA (Dosovitskiy et al. ([2017](#bib.bib78))) are commonly-used simulators
    for robotic and self-driving tasks, respectively. Meta-MuJoCo (Dorfman et al.
    ([2021](#bib.bib77))) is a multi-task version of MuJoCo. By Real Robot, we mean
    evaluations on real robotic platforms, which vary from one study to another. For
    other benchmarks, we provide their references here: GridWorld (Zintgraf et al.
    ([2020](#bib.bib400))), MetaWorld (Yu et al. ([2019c](#bib.bib379))), RoboSuite
    (Zhu et al. ([2020](#bib.bib396))), Robomimic (Mandlekar et al. ([2021a](#bib.bib209))),
    CALVIN (Mees et al. ([2022b](#bib.bib219))).'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：基于VAE的离线强化学习算法总结。在第1列中，我们列出了本节中的代表性（但非全部）算法。这些算法按类别分组。关于VAE类型，$\beta$-CVAE指的是CVAE和$\beta$-VAE的集成，其中在CVAE的KL项中添加了一个权重$\beta$。注释（T）表示VAE在轨迹上实现，而不是在单独的状态转移上。第4列列出了评估任务。大多数工作在D4RL（Fu等人（[2020](#bib.bib99)））上进行评估，它提供了各种任务的离线数据集，包括Locomotion（L）、AntMaze（M）、Adroit（A）、Kitchen（K）、Maze2d（M2d）等。MuJoCo（Todorov等人（[2012](#bib.bib320)））和CARLA（Dosovitskiy等人（[2017](#bib.bib78)））是常用的机器人和自动驾驶任务模拟器。Meta-MuJoCo（Dorfman等人（[2021](#bib.bib77)））是MuJoCo的多任务版本。真实机器人是指在真实机器人平台上的评估，每项研究的具体平台有所不同。对于其他基准，我们在此提供其参考文献：GridWorld（Zintgraf等人（[2020](#bib.bib400)）），MetaWorld（Yu等人（[2019c](#bib.bib379)）），RoboSuite（Zhu等人（[2020](#bib.bib396)）），Robomimic（Mandlekar等人（[2021a](#bib.bib209)）），CALVIN（Mees等人（[2022b](#bib.bib219)））。
- en: 3.1.5 Offline Multi-task/Hierarchical RL based on VAEs
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5 基于VAE的离线多任务/层次化强化学习
- en: Multi-task RL and hierarchical RL extend the basic RL setting. In particular,
    multi-task RL (Sodhani et al. ([2021](#bib.bib298))) aims at learning a policy
    that can be directly applied to or quickly adapted to a distribution of tasks.
    Hierarchical RL, on the other hand, learns a hierarchical (two-level) policy for
    complex, long-horizon tasks which can usually be decomposed into a sequence of
    subtasks. In this case, low-level policies can be used to accomplish each subtask,
    while the high-level policy coordinates the subtasks and the use of low-level
    policies. For example, in goal-achieving tasks, a goal-conditioned policy can
    be considered a multi-task policy, as it can reach multiple goals in an environment
    by changing the goal condition. However, if the goal is distant, the entire path
    might be divided into several subgoals by the high-level policy, and to reach
    each subgoal, a corresponding low-level (subgoal-conditioned) policy can be employed.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务强化学习（Multi-task RL）和层次化强化学习（hierarchical RL）扩展了基本的强化学习设置。特别是，多任务强化学习（Sodhani
    et al. ([2021](#bib.bib298)))旨在学习一种可以直接应用或快速适应于任务分布的策略。另一方面，层次化强化学习则学习一种用于复杂、长时程任务的层次（两级）策略，这些任务通常可以分解为一系列子任务。在这种情况下，低级策略可以用来完成每个子任务，而高级策略则协调子任务和低级策略的使用。例如，在目标达成任务中，一个目标条件策略可以被视为一种多任务策略，因为它可以通过改变目标条件在环境中达到多个目标。然而，如果目标很远，高级策略可能会将整个路径划分为几个子目标，并且为了达到每个子目标，可以采用相应的低级（子目标条件）策略。
- en: Next, we formally introduce these two setups — multi-task RL and hierarchical
    RL — and explore how VAEs can be utilized to enhance them. By providing detailed
    introductions of the most notable research in each category, such as BOReL for
    multi-task RL and OPAL & HiGoC for hierarchical RL, our aim is to present the
    fundamental paradigms as a brief tutorial for these specific research directions.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将正式介绍这两种设置——多任务强化学习和层次化强化学习——并探索如何利用变分自编码器（VAEs）来增强它们。通过详细介绍每个类别中最著名的研究，如多任务强化学习中的BOReL和层次化强化学习中的OPAL与HiGoC，我们的目标是为这些特定研究方向提供基本范式的简要教程。
- en: 'Multi-task RL: Given a multi-task offline dataset $D_{\mu}^{M}=[[\tau^{i,j}=(s_{0}^{i,j},a_{0}^{i,j},r_{0}^{i,j},\cdots,s_{T}^{i,j})]_{i=1}^{N}]_{j=1}^{M}$,
    where $i$ and $j$ are the indexes for the trajectory and task respectively, offline
    multi-task RL aims at learning a multi-task policy that can be adapted to unseen
    tasks with zero- or few-shot training. The testing tasks are required to be in
    the same distribution as the training ones. As a representative, BOReL (Dorfman
    et al. ([2021](#bib.bib77))) is proposed for the case where the reward and dynamic
    function $(r_{j},\mathcal{T}_{j})$ vary with the task. To train a multi-task policy,
    a straightforward manner is to condition that policy on the task information $(r_{j},\mathcal{T}_{j})$.
    In particular, they adopt the latent variable $z$ ^(10)^(10)10Actually, they adopt
    the mean and variance of the latent variable (i.e., the output of the encoder)
    as the policy conditioner. of a CVAE as a representation of the reward and dynamic
    function, and learns a latent-conditioned policy $\pi(a|s,z)$ as the multi-task
    policy. To this end, the CVAE is trained as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务强化学习（Multi-task RL）：给定一个多任务离线数据集 $D_{\mu}^{M}=[[\tau^{i,j}=(s_{0}^{i,j},a_{0}^{i,j},r_{0}^{i,j},\cdots,s_{T}^{i,j})]_{i=1}^{N}]_{j=1}^{M}$，其中
    $i$ 和 $j$ 分别是轨迹和任务的索引，离线多任务强化学习旨在学习一种多任务策略，该策略可以通过零样本或少量样本训练适应未见过的任务。测试任务要求与训练任务在同一分布中。作为代表，BOReL（Dorfman
    et al. ([2021](#bib.bib77))) 被提出用于奖励和动态函数 $(r_{j},\mathcal{T}_{j})$ 随任务变化的情况。为了训练多任务策略，一种简单的方法是将该策略条件化于任务信息
    $(r_{j},\mathcal{T}_{j})$。具体来说，他们采用了CVAE的潜变量 $z$ ^(10)^(10)实际上，他们采用了潜变量（即编码器的输出）的均值和方差作为策略条件。作为奖励和动态函数的表示，并学习了一个潜变量条件策略
    $\pi(a|s,z)$ 作为多任务策略。为此，CVAE的训练如下：
- en: '|  |  | $\displaystyle\max_{\theta,\phi}\sum_{t=0}^{T-1}\text{ELBO}_{t},\ \text{ELBO}_{t}=\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;\tau_{0:t})}\left[\log P_{\theta}(s_{0:T},r_{0:T-1}&#124;z,a_{0:T-1})-D_{KL}(P_{\phi}(z&#124;\tau_{0:t})&#124;&#124;P_{\theta}(z))\right],$
    |  | (44) |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\max_{\theta,\phi}\sum_{t=0}^{T-1}\text{ELBO}_{t},\ \text{ELBO}_{t}=\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;\tau_{0:t})}\left[\log P_{\theta}(s_{0:T},r_{0:T-1}&#124;z,a_{0:T-1})-D_{KL}(P_{\phi}(z&#124;\tau_{0:t})&#124;&#124;P_{\theta}(z))\right],$
    |  | (44) |'
- en: '|  |  | $\displaystyle\log P_{\theta}(s_{0:T},r_{0:T-1}&#124;z,a_{0:T-1})=\log
    P_{\theta}(s_{0}&#124;z)+\sum_{t=0}^{T-1}\left[\log P_{\theta}(s_{t+1}&#124;s_{t},a_{t},z)+\log
    P_{\theta}(r_{t}&#124;s_{t},a_{t},s_{t+1},z)\right]$ |  |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\log P_{\theta}(s_{0:T},r_{0:T-1}|z,a_{0:T-1})=\log P_{\theta}(s_{0}|z)+\sum_{t=0}^{T-1}\left[\log
    P_{\theta}(s_{t+1}|s_{t},a_{t},z)+\log P_{\theta}(r_{t}|s_{t},a_{t},s_{t+1},z)\right]$
    |  |'
- en: 'In this context, $\tau_{0:T}=(s_{0:T},r_{0:T-1},a_{0:T-1})$; $a_{0:T-1}$ and
    $(s_{0:T},r_{0:T-1})$ can be viewed as $c$ and $x$ in the CVAE framework. As shown
    in (Zintgraf et al. ([2020](#bib.bib400))), $\text{ELBO}_{t}$ constitutes a variational
    lower bound for $\log P_{\theta}(s_{0:T},r_{0:T-1}|a_{0:T-1})$. The second equation
    in Eq. ([44](#S3.E44 "In 3.1.5 Offline Multi-task/Hierarchical RL based on VAEs
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) is derived based on the Markov
    assumption (Puterman ([2014](#bib.bib258))), and it can be observed from this
    equation that $z$ is trained to embed information regarding the initial state
    distribution, reward and dynamic functions, so as to reconstruct the whole task-specific
    trajectory $\tau_{0:T}$. Subsequently, the pretrained $P_{\phi}(z|\tau_{0:t})$,
    which is implemented as a recurrent neural network as in VRNN (see Section [2.1](#S2.SS1
    "2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")), can be applied to $D_{\mu}$ to infer the task embedding
    $z_{t}$ at each time step, leading to a dataset of transitions in the form $((s_{t},z_{t}),a_{t},r_{t})$.
    Note that $(s_{t},z_{t})$ can be viewed as an extended state $\tilde{s}_{t}$,
    and thus standard offline RL algorithms can then be directly applied to this dataset
    to learn a latent-conditioned policy $\pi(a_{t}|(s_{t},z_{t}))$.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个上下文中，$\tau_{0:T}=(s_{0:T},r_{0:T-1},a_{0:T-1})$；$a_{0:T-1}$ 和 $(s_{0:T},r_{0:T-1})$
    可以视为CVAE框架中的$c$和$x$。如（Zintgraf 等 ([2020](#bib.bib400))) 所示，$\text{ELBO}_{t}$ 构成了$\log
    P_{\theta}(s_{0:T},r_{0:T-1}|a_{0:T-1})$ 的变分下界。方程中第二个方程（见 Eq. ([44](#S3.E44 "In
    3.1.5 Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"))是基于马尔可夫假设（Puterman ([2014](#bib.bib258))) 推导出来的，从这个方程中可以观察到，$z$ 被训练来嵌入关于初始状态分布、奖励和动态函数的信息，以重构整个任务特定的轨迹
    $\tau_{0:T}$。随后，预训练的$P_{\phi}(z|\tau_{0:t})$，它实现为如VRNN中的递归神经网络（见第 [2.1](#S2.SS1
    "2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")节），可以应用于$D_{\mu}$以推断每个时间步骤的任务嵌入 $z_{t}$，从而形成形式为$((s_{t},z_{t}),a_{t},r_{t})$的过渡数据集。注意
    $(s_{t},z_{t})$ 可以视为扩展状态 $\tilde{s}_{t}$，因此标准离线RL算法可以直接应用于该数据集以学习潜在条件策略 $\pi(a_{t}|(s_{t},z_{t}))$。'
- en: 'Hierarchical RL: This category of algorithms try to learn a hierarchical policy
    $(\pi_{\text{high}}(z|s),\pi_{\text{low}}(a|s,z))$ from the offline dataset. Intuitively,
    the agent would segment the whole task into a sequence of subtasks or subgoals,
    each of which is denoted by a (continuous or discrete) variable $z$ and accomplished
    by a corresponding subpolicy/skill $\pi_{\text{low}}(a|s,z)$. This hierarchical
    scheme is especially beneficial for complex tasks with long horizons. OPAL (Ajay
    et al. ([2021](#bib.bib4))) is a representive algorithm in this direction. They
    define the horizon of each skill to be $h$ and organize the offline data as a
    set of trajectory segments $D_{\mu}=[\tau^{i}=[s_{0:h-1}^{i},a_{0:h-1}^{i}]]_{i=1}^{N}$.
    Then, they learn the low-level policy $\pi_{\text{low}}$ for different subtasks
    $z$ as the decoder of a CVAE:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 层次RL：这类算法尝试从离线数据集中学习层次策略 $(\pi_{\text{high}}(z|s),\pi_{\text{low}}(a|s,z))$。直观地，智能体会将整个任务分解为一系列子任务或子目标，每个子任务由一个（连续或离散）变量
    $z$ 表示，并由相应的子策略/技能 $\pi_{\text{low}}(a|s,z)$ 完成。这种层次方案对具有长时间范围的复杂任务特别有利。OPAL (Ajay
    等 ([2021](#bib.bib4))) 是这一方向上的一个代表性算法。他们将每个技能的范围定义为 $h$ 并将离线数据组织为一组轨迹段 $D_{\mu}=[\tau^{i}=[s_{0:h-1}^{i},a_{0:h-1}^{i}]]_{i=1}^{N}$。然后，他们将不同子任务
    $z$ 的低级策略 $\pi_{\text{low}}$ 作为CVAE的解码器进行学习。
- en: '|  | $\displaystyle\max_{\theta,\phi}\mathbb{E}_{\tau\sim D_{\mu}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;\tau)}\left[\sum_{t=0}^{h-1}\log\pi_{\theta}(a_{t}&#124;s_{t},z)\right]-\beta
    D_{KL}(P_{\phi}(z&#124;\tau)&#124;&#124;P_{\theta}(z&#124;s_{0}))\right],\ \pi_{\text{low}}\triangleq\pi_{\theta}$
    |  | (45) |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\theta,\phi}\mathbb{E}_{\tau\sim D_{\mu}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot\mid\tau)}\left[\sum_{t=0}^{h-1}\log\pi_{\theta}(a_{t}\mid s_{t},z)\right]-\beta
    D_{KL}(P_{\phi}(z\mid\tau)\mid\mid P_{\theta}(z\mid s_{0}))\right],\ \pi_{\text{low}}\triangleq\pi_{\theta}$
    |  | (45) |'
- en: 'This objective is equivalent to a ($\beta$-)CVAE ELBO, where $s_{0}$ and $\tau$
    work as the conditioner $c$ and data $x$ respectively ^(11)^(11)11For a standard
    CVAE, the reconstruction term should be $\log P_{\theta}(\tau|s_{0},z)=\sum_{t=0}^{h-1}\log\pi_{\theta}(a_{t}|s_{t},z)+\sum_{t=0}^{h-2}\log\mathcal{T}(s_{t+1}|s_{t},a_{t})$.
    However, the transition function $\mathcal{T}$ is not trainable and so would not
    influence the gradient calculation.. However, unlike Eq. ([37](#S3.E37 "In 3.1.2
    An Overview of Applying VAEs in Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), here $\pi_{\theta}$ is trained on sets of trajectory segments rather
    than single-step transitions. This is because, as a skill, $\pi_{\theta}$ is expected
    to extend temporally, for example, lasting for $h$ time steps after being selected.
    Also, the prior $P_{\theta}(z|s)$ is not fixed but implemented as a neural network
    that takes $s_{0}$ as input, to make sure the skill choice $z$ is predictable
    (by the high-level policy $\pi_{\text{high}}(z|s)$) given only the initial state
    $s_{0}$. Applying the pretrained $P_{\phi}(z|\tau)$ on $D_{\mu}$ and introducing
    the reward signals, a new dataset $D_{\mu}^{\text{high}}=(s_{0}^{i},z^{i}\sim
    P_{\phi}(\cdot|\tau^{i}),\sum_{t=0}^{h-2}r_{t}^{i},s_{h-1}^{i})_{i=1}^{N}$ can
    be obtained for training $\pi_{\text{high}}(z|s)$ with any offline RL methods.
    With this hierarchical policy $(\pi_{\text{high}},\pi_{\text{low}})$, the decision
    horizon of offline RL is effectively shorten (by a factor of $h$) and so OOD actions
    caused by the accumulated distribution shift can be mitigated. Rosete-Beas et al.
    ([2022](#bib.bib280)) propose TACO-RL, which is a very similar algorithm with
    OPAL but specifically tailored for goal-achieving tasks. HiGoC (Li et al. ([2022](#bib.bib183)))
    is also a hierarchical framework for goal-achieving tasks, where the high-level
    part is a model-based planner ^(12)^(12)12Please refer to (Li et al. ([2022](#bib.bib183)))
    for further details. for generating the subgoal list and the low-level part is
    a goal-conditioned policy trained by offline RL to reach each subgoal sequentially.
    The subgoals are not labeled in the dataset, so the low-level policy is trained
    in an unsupervised manner. Specifically, for $\pi_{\text{low}}(a_{t_{1}}|s_{t_{1}},s_{t_{2}})$,
    $s_{t_{2}}$ ($t_{2}>t_{1}$) is the subgoal and randomly sampled from states after
    $s_{t_{1}}$ in the same trajectory. To be robust to possible OOD subgoals during
    evaluation, a CVAE $m(s_{t}|s_{t-h})=(P_{\phi}(z|s_{t},s_{t-h}),P_{\theta}(s_{t}|z,s_{t-h}))$
    is pretrained on $D_{\mu}$ for generating the subgoal $s_{t}$ conditioned on the
    previous subgoal $s_{t-h}$, where $h$ is a predefined time interval for subgoal
    selections. With this CVAE, a perturbed subgoal can be generated based on the
    sampled one (i.e., $s_{t_{2}}$) as $P_{\theta}(P_{\phi}(s_{t_{2}},s_{t_{2}-h})+\epsilon,s_{t_{2}-h})$
    ($\epsilon$ is a noise vector), which can replace $s_{t_{2}}$ as the subgoal of
    $\pi_{\text{low}}$ for robustness. In cases of high-dimensional states, such as
    images, adding noise to a well-defined low-dimensional embedding space (from VAEs)
    is a more effective and reasonable approach. Further, it’s noteworthy that the
    pretrained decoder $P_{\theta}(s_{t}|z,s_{t-h})$ can also be used for high-level
    planning. Specifically, a subgoal list can be generated from the initial state
    $s_{0}$ by specifying a list of latent variables $z_{1:k}$: $s_{t_{i}}\sim P_{\theta}(\cdot|z_{i},s_{t_{i}-h}),\
    i\in[1,\cdots,k]$. Such a list can then be evaluated by task-relevant objectives.
    In this case, searching for an optimal list $z_{1:k}$ is literally a model predictive
    planning problem and is more efficient than directly searching on the high-dimensional
    state space, i.e., the list of subgoals. FLAP (Fang et al. ([2022](#bib.bib88)))
    adopts a quite similar protocol with HiGoC, where the CAVE $m$ is referred to
    as the affordance model.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目标等同于一个($\beta$-)CVAE ELBO，其中 $s_{0}$ 和 $\tau$ 分别作为条件 $c$ 和数据 $x$ 使用 ^(11)^(11)11对于标准的
    CVAE，重建项应为 $\log P_{\theta}(\tau|s_{0},z)=\sum_{t=0}^{h-1}\log\pi_{\theta}(a_{t}|s_{t},z)+\sum_{t=0}^{h-2}\log\mathcal{T}(s_{t+1}|s_{t},a_{t})$。然而，转移函数
    $\mathcal{T}$ 不是可训练的，因此不会影响梯度计算。尽管如此，与 Eq. ([37](#S3.E37 "在 3.1.2 章节中应用 VAEs 于离线强化学习的概述
    ‣ 3.1 离线强化学习 ‣ 3 变分自编码器在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向展望")) 不同，这里 $\pi_{\theta}$
    是在轨迹段集合上训练的，而不是单步转移。这是因为作为一种技能，$\pi_{\theta}$ 被期望在时间上扩展，例如，在选择后持续 $h$ 时间步。另一个前提
    $P_{\theta}(z|s)$ 不是固定的，而是实现为一个神经网络，以 $s_{0}$ 为输入，确保技能选择 $z$ 由高级策略 $\pi_{\text{high}}(z|s)$
    预测（仅根据初始状态 $s_{0}$）。通过将预训练的 $P_{\phi}(z|\tau)$ 应用到 $D_{\mu}$ 上并引入奖励信号，可以获得一个新的数据集
    $D_{\mu}^{\text{high}}=(s_{0}^{i},z^{i}\sim P_{\phi}(\cdot|\tau^{i}),\sum_{t=0}^{h-2}r_{t}^{i},s_{h-1}^{i})_{i=1}^{N}$，用于用任何离线
    RL 方法训练 $\pi_{\text{high}}(z|s)$。有了这个层次策略 $(\pi_{\text{high}},\pi_{\text{low}})$，离线
    RL 的决策范围有效缩短（缩短了 $h$ 倍），因此可以缓解由累积分布偏移引起的 OOD 动作。Rosete-Beas 等 ([2022](#bib.bib280))
    提出了 TACO-RL，这是一种与 OPAL 非常相似但专门为目标实现任务量身定制的算法。HiGoC（Li 等 ([2022](#bib.bib183)))
    也是一个用于目标实现任务的层次框架，其中高层部分是基于模型的规划器 ^(12)^(12)12有关详细信息，请参阅 (Li 等 ([2022](#bib.bib183)))，用于生成子目标列表，低层部分是通过离线
    RL 训练的目标条件策略，用于依次达到每个子目标。子目标在数据集中没有标记，因此低层策略以无监督的方式进行训练。具体来说，对于 $\pi_{\text{low}}(a_{t_{1}}|s_{t_{1}},s_{t_{2}})$，$s_{t_{2}}$
    ($t_{2}>t_{1}$) 是子目标，并从同一轨迹中 $s_{t_{1}}$ 之后的状态中随机采样。为了在评估期间对可能的 OOD 子目标具有鲁棒性，预训练了
    CVAE $m(s_{t}|s_{t-h})=(P_{\phi}(z|s_{t},s_{t-h}),P_{\theta}(s_{t}|z,s_{t-h}))$，用于生成条件于先前子目标
    $s_{t-h}$ 的子目标 $s_{t}$，其中 $h$ 是选择子目标的预定义时间间隔。利用这个 CVAE，可以基于采样的子目标（即 $s_{t_{2}}$）生成一个扰动的子目标作为
    $P_{\theta}(P_{\phi}(s_{t_{2}},s_{t_{2}-h})+\epsilon,s_{t_{2}-h})$（$\epsilon$
    是一个噪声向量），这可以替代 $s_{t_{2}}$ 作为 $\pi_{\text{low}}$ 的子目标以增强鲁棒性。在高维状态的情况下，例如图像，将噪声添加到定义良好的低维嵌入空间（来自
    VAEs）是一种更有效和合理的方法。此外，值得注意的是，预训练解码器 $P_{\theta}(s_{t}|z,s_{t-h})$ 也可以用于高层规划。具体来说，可以通过指定潜在变量列表
    $z_{1:k}$ 从初始状态 $s_{0}$ 生成子目标列表：$s_{t_{i}}\sim P_{\theta}(\cdot|z_{i},s_{t_{i}-h}),\
    i\in[1,\cdots,k]$。然后可以通过与任务相关的目标来评估这个列表。在这种情况下，寻找一个最优列表 $z_{1:k}$ 实质上是一个模型预测规划问题，比直接在高维状态空间（即子目标列表）中搜索要高效。FLAP（Fang
    等 ([2022](#bib.bib88))) 采用了与 HiGoC 非常相似的协议，其中 CAVE $m$ 被称为适应模型。
- en: 3.2 Imitation Learning
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 模仿学习
- en: In this section, we offer an overview of VAE-based IL algorithms. First, we
    provide a tutorial-like overview of the four schemes of VAE-based IL. Based on
    this, we introduce a categorization of all related works based on how VAEs are
    utilized. The use of VAEs focuses on enhancing Behavioral Cloning, either from
    a data or algorithmic perspective. To conclude, we include a table summarizing
    the representative algorithms.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们概述了基于 VAE 的 IL 算法。首先，我们提供了四种基于 VAE 的 IL 方案的教程式概述。在此基础上，我们介绍了所有相关工作的分类方法，基于
    VAE 的使用方式。VAE 的使用集中于增强行为克隆，从数据或算法的角度来看。最后，我们包括了一张总结性表格，概述了代表性算法。
- en: 3.2.1 Core Schemes of VAE-based Imitation Learning
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 基于 VAE 的模仿学习核心方案
- en: 'Imitation Learning (IL) aims at recovering the expert policy $\pi(a|s)$ from
    a set of demonstrations $D_{E}$. Behavioral Cloning (BC) is a straightforward
    and widely-used IL framework (Pomerleau ([1991](#bib.bib256))), which implements
    imitation by supervised learning: $\max_{\pi}\mathbb{E}_{(s,a)\sim D_{E}}\log\pi(a|s)$.
    With its special encoder-decoder structure, the VAE has been utilized to improve
    BC from multiple perspectives. As a summary, here we provide the four schemes
    of VAE-based IL.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习（IL）的目标是从一组示例 $D_{E}$ 中恢复专家策略 $\pi(a|s)$。行为克隆（BC）是一个直接且广泛使用的 IL 框架（Pomerleau（[1991](#bib.bib256)）），通过监督学习实现模仿：$\max_{\pi}\mathbb{E}_{(s,a)\sim
    D_{E}}\log\pi(a|s)$。凭借其特殊的编码器-解码器结构，VAE 已被用于从多个角度改进 BC。作为总结，这里我们提供了四种基于 VAE 的
    IL 方案。
- en: 'Scheme (1): Similar with the major use of VAE for offline RL (as introduced
    in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), a
    CVAE conditioned on the state can be directly used to model the expert policy:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 方案 (1)：类似于 VAE 在离线 RL 中的主要用途（如第 [3.1.1](#S3.SS1.SSS1 "3.1.1 动态规划基础的离线强化学习背景
    ‣ 3.1 离线强化学习 ‣ 3 基于变分自编码器的离线策略学习 ‣ 深度生成模型用于离线策略学习：教程、调查和未来方向展望")节中介绍的），条件于状态的
    CVAE 可以直接用于建模专家策略：
- en: '|  | $\max_{\theta,\phi}\mathbb{E}_{(s,a)\sim D_{E}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;s,a)}\log P_{\theta}(a&#124;s,z)-D_{KL}(P_{\phi}(z&#124;s,a)&#124;&#124;P_{\theta}(z&#124;s))\right]$
    |  | (46) |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\theta,\phi}\mathbb{E}_{(s,a)\sim D_{E}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot\mid s,a)}\log P_{\theta}(a\mid s,z)-D_{KL}(P_{\phi}(z\mid s,a)\mid\mid
    P_{\theta}(z\mid s))\right]$ |  | (46) |'
- en: 'As introduced in Section [2.1](#S2.SS1 "2.1 Variational Auto-Encoders ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), this objective constitutes
    a variational lower bound for $\mathbb{E}_{(s,a)\sim D_{E}}\log\pi(a|s)$, i.e.,
    the BC objective. After training, $z\sim P_{\theta}(\cdot|s),a\sim P_{\theta}(\cdot|s,z)$,
    where $P_{\theta}(z|s)$ is a predefined prior and usually set as $\mathcal{N}(z;0,I)$,
    can be used as the policy $a\sim\pi(\cdot|s)$. With the latent variable, the VAE-based
    policy can model stochastic behaviors with diverse modes. Theoretically, $\pi(a|s)=\int
    P_{\theta}(a|s,z)P_{\theta}(z|s)dz$. Rather than using a fixed prior distribution
    $P_{\theta}(z|s)$, Ren et al. ([2020](#bib.bib273)) propose an algorithm to fine-tune
    the prior online for specific tasks by optimizing a generalization bound from
    the PAC-Bayes theory.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在第 [2.1](#S2.SS1 "2.1 变分自编码器 ‣ 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向展望")节中介绍的，这个目标构成了
    $\mathbb{E}_{(s,a)\sim D_{E}}\log\pi(a|s)$ 的变分下界，即 BC 目标。训练后，$z\sim P_{\theta}(\cdot|s),a\sim
    P_{\theta}(\cdot|s,z)$，其中 $P_{\theta}(z|s)$ 是一个预定义的先验，通常设置为 $\mathcal{N}(z;0,I)$，可以用作策略
    $a\sim\pi(\cdot|s)$。通过潜在变量，基于 VAE 的策略可以模拟具有多种模式的随机行为。理论上，$\pi(a|s)=\int P_{\theta}(a|s,z)P_{\theta}(z|s)dz$。Ren
    等人（[2020](#bib.bib273)）提出了一种算法，通过优化 PAC-Bayes 理论中的泛化界限，在线微调先验以适应特定任务。
- en: 'Scheme (2): The second scheme is based on representation learning (RepL), where
    the latent variable $z$ is adopted as a representation of the state $s$ and usually
    can be learned via state reconstructions:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 方案 (2)：第二种方案基于表示学习（RepL），其中潜在变量 $z$ 被用作状态 $s$ 的表示，通常可以通过状态重构来学习：
- en: '|  | $\displaystyle\max_{\theta,\phi}\mathbb{E}_{s\sim D_{E}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;s)}\text{Rec}_{\theta}(s,z)-D_{KL}(P_{\phi}(z&#124;s)&#124;&#124;P_{\theta}(z))\right],\
    \max_{\pi}\mathbb{E}_{(s,a)\sim D_{E},z\sim P_{\phi}(\cdot&#124;s)}\log\pi(a&#124;z)$
    |  | (47) |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\theta,\phi}\mathbb{E}_{s\sim D_{E}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot|s)}\text{Rec}_{\theta}(s,z)-D_{KL}(P_{\phi}(z|s)||P_{\theta}(z))\right],\
    \max_{\pi}\mathbb{E}_{(s,a)\sim D_{E},z\sim P_{\phi}(\cdot|s)}\log\pi(a|z)$ |  |
    (47) |'
- en: 'where the reconstruction objective $\text{Rec}_{\theta}(s,z)=\log P_{\theta}(s|z)\
    \text{or}\ -||s-P_{\theta}(z)||_{2}^{2}$. Note that the first objective in Eq.
    ([47](#S3.E47 "In 3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) is usually coupled with extra regularization terms to encourage
    the disentanglement of the learned representation, or enable the fusion of state
    inputs $s$ from multiple modalities, etc. Based on the representation learning,
    a policy $\pi(a|z)$ conditioned on the compact representation $z$ can be learned
    with any IL algorithm. Compared with $s$, $z$ is in a lower dimension and is less
    noisy for decision making. As a result, RepL can effectively reduce the need of
    IL for large amounts of training data. In Eq. ([47](#S3.E47 "In 3.2.1 Core Schemes
    of VAE-based Imitation Learning ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), there are in total
    three functions (besides $P_{\theta}(z)$ which may not be learnable): $P_{\phi}(z|s)$,
    $P_{\theta}(s|z)$, and $\pi(a|z)$. One manner is to pretrain $P_{\phi}(z|s)$ and
    $P_{\theta}(s|z)$ within the VAE framework and then train $\pi(a|z)$ with IL as
    shown in the second term of Eq. ([47](#S3.E47 "In 3.2.1 Core Schemes of VAE-based
    Imitation Learning ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")). When training $\pi$, the parameters
    of $P_{\phi}$ can either be frozen or not. The other manner is to jointly train
    the three functions through an integrated objective, i.e., adding the two objectives
    in Eq. ([47](#S3.E47 "In 3.2.1 Core Schemes of VAE-based Imitation Learning ‣
    3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) together with an adjustable weight $\lambda$. In this
    case, the state reconstruction with a VAE can be viewed as an auxiliary task for
    IL. EIRLI (Chen et al. ([2021a](#bib.bib41))) provides a systematic empirical
    investigation of representation learning for imitation. They find that RepL using
    VAEs can effectively improve the performance of vision-based IL. However, they
    also mention that the relative impact of adding representation learning tends
    to be lower than the impact of adding or removing data augmentations. This may
    be because usual RepL techniques (for images) tend to capture the most visually
    salient axes of variation (e.g., the color, background, or objects) but the action
    choices are often determined by more fine-grained, local cues in the environment,
    which calls for RepL that is more specific to decision making. Additional examples
    in this category include (Lee et al. ([2019](#bib.bib179)); Rahmatizadeh et al.
    ([2018](#bib.bib267))).'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '重构目标是 $\text{Rec}_{\theta}(s,z)=\log P_{\theta}(s|z)\ \text{or}\ -||s-P_{\theta}(z)||_{2}^{2}$。请注意，公式中的第一个目标
    ([47](#S3.E47 "In 3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) 通常会与额外的正则化项结合使用，以促进学习表示的解耦，或实现来自多个模态的状态输入 $s$ 的融合等。基于表示学习，可以使用任何
    IL 算法学习条件于紧凑表示 $z$ 的策略 $\pi(a|z)$。与 $s$ 相比，$z$ 维度更低，对决策的噪声更小。因此，RepL 可以有效减少 IL
    对大量训练数据的需求。在公式 ([47](#S3.E47 "In 3.2.1 Core Schemes of VAE-based Imitation Learning
    ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) 中，总共有三个函数（除了可能不可学习的 $P_{\theta}(z)$）：$P_{\phi}(z|s)$、$P_{\theta}(s|z)$
    和 $\pi(a|z)$。一种方法是先在 VAE 框架中预训练 $P_{\phi}(z|s)$ 和 $P_{\theta}(s|z)$，然后用 IL 训练
    $\pi(a|z)$，如公式 ([47](#S3.E47 "In 3.2.1 Core Schemes of VAE-based Imitation Learning
    ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) 中的第二项所示。在训练 $\pi$ 时，$P_{\phi}$ 的参数可以选择冻结或不冻结。另一种方法是通过一个集成目标共同训练这三个函数，即将公式
    ([47](#S3.E47 "In 3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) 中的两个目标加上一个可调权重 $\lambda$。在这种情况下，VAE 的状态重建可以视为 IL 的一个辅助任务。EIRLI（Chen
    等人 ([2021a](#bib.bib41))) 对模仿学习中的表示学习进行了系统的实证研究。他们发现使用 VAEs 的 RepL 可以有效提高基于视觉的
    IL 的性能。然而，他们也提到，添加表示学习的相对影响通常低于添加或去除数据增强的影响。这可能是因为常见的 RepL 技术（对于图像）往往捕捉到视觉上最显著的变化轴（例如颜色、背景或物体），但动作选择通常由环境中更细粒度的局部线索决定，这需要更特定于决策的
    RepL。此类别的其他示例包括（Lee 等人 ([2019](#bib.bib179))；Rahmatizadeh 等人 ([2018](#bib.bib267))）。'
- en: 'Scheme (3): VAEs can also be directly applied to expert trajectories $\tau=(s_{0},a_{0},\cdots,s_{T})$
    with the following objective:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 方案 (3)：VAE也可以直接应用于专家轨迹$\tau=(s_{0},a_{0},\cdots,s_{T})$，目标如下：
- en: '|  |  | $\displaystyle\quad\quad\max_{\theta,\phi}\mathbb{E}_{\tau\sim D_{E}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;\tau)}\left[\log P_{\theta}(\tau&#124;z)\right]-D_{KL}(P_{\phi}(z&#124;\tau)&#124;&#124;P_{\theta}(z))\right],$
    |  | (48) |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad\quad\max_{\theta,\phi}\mathbb{E}_{\tau\sim D_{E}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot| \tau)}\left[\log P_{\theta}(\tau|z)\right]-D_{KL}(P_{\phi}(z|
    \tau)||P_{\theta}(z))\right],$ |  | (48) |'
- en: '|  |  | $\displaystyle\log P_{\theta}(\tau&#124;z)=\log P_{\theta}(s_{0}&#124;z)+\sum_{t=0}^{T-1}\left[\log
    P_{\theta}(a_{t}&#124;s_{t},z)+\log P_{\theta}(s_{t+1}&#124;s_{t},a_{t},z)\right]$
    |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\log P_{\theta}(\tau|z)=\log P_{\theta}(s_{0}|z)+\sum_{t=0}^{T-1}\left[\log
    P_{\theta}(a_{t}|s_{t},z)+\log P_{\theta}(s_{t+1}|s_{t},a_{t},z)\right]$ |  |'
- en: 'The decomposition of $P_{\theta}(\tau|z)$ makes use of the MDP model, where
    the three terms correspond to the initial state distribution, policy distribution,
    and transition dynamic, respectively. $P_{\theta}(s_{0}|z)$ and $P_{\theta}(s_{t+1}|s_{t},a_{t},z)$,
    which may be independent of $z$, are usually defined within the simulator and
    not required to model, so $\log P_{\theta}(\tau|z)$ can be replaced with $\sum_{t=0}^{T-1}\log
    P_{\theta}(a_{t}|s_{t},z)$ in the original objective. It’s worthy noting that
    $P_{\theta}(\tau|z)$ has multiple alternative decomposition forms. For example,
    T-VAE (Lu et al. ([2019](#bib.bib203))) proposes to model it as $\log P_{\theta}(s_{0:T}|z)+\log
    P_{\theta}(a_{0:T-1}|z,s_{0:T})$. Specifically, two RNNs are adopted to generate
    the state and action sequences respectively, leading to a new definition of $\log
    P_{\theta}(\tau|z)$: (The state sequence $\hat{s}_{0:T}$ is generated before the
    action sequence $\hat{a}_{0:T-1}$; the generation of $\hat{s}_{t}$/$\hat{a}_{t}$
    is conditioned on $h_{t-1}^{\hat{s}}$/$h_{t-1}^{\hat{s},\hat{a}}$ which embeds
    the history $\hat{s}_{1:t-1}$/$(\hat{s}_{1:t-1},\hat{a}_{1:t-1})$.)'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: $P_{\theta}(\tau|z)$的分解利用了MDP模型，其中三个项分别对应于初始状态分布、策略分布和转移动态。$P_{\theta}(s_{0}|z)$和$P_{\theta}(s_{t+1}|s_{t},a_{t},z)$可能与$z$无关，通常在模拟器中定义，而不是模型的一部分，因此$\log
    P_{\theta}(\tau|z)$可以在原始目标中替换为$\sum_{t=0}^{T-1}\log P_{\theta}(a_{t}|s_{t},z)$。值得注意的是，$P_{\theta}(\tau|z)$有多种替代分解形式。例如，T-VAE
    (Lu et al. ([2019](#bib.bib203))) 提出将其建模为 $\log P_{\theta}(s_{0:T}|z)+\log P_{\theta}(a_{0:T-1}|z,s_{0:T})$。具体而言，采用两个RNN分别生成状态序列和动作序列，从而得到了$\log
    P_{\theta}(\tau|z)$的新定义：（状态序列$\hat{s}_{0:T}$在动作序列$\hat{a}_{0:T-1}$之前生成；$\hat{s}_{t}$/$\hat{a}_{t}$的生成以$h_{t-1}^{\hat{s}}$/$h_{t-1}^{\hat{s},\hat{a}}$为条件，这些嵌入了历史$\hat{s}_{1:t-1}$/$(\hat{s}_{1:t-1},\hat{a}_{1:t-1})$。）
- en: '|  | $\log P_{\theta}(\tau&#124;z)=\sum_{t=0}^{T}\log P_{\theta}(s_{t}&#124;z,h_{t-1}^{\hat{s}})+\sum_{t=0}^{T-1}\log
    P_{\theta}(a_{t}&#124;z,h_{t-1}^{\hat{s},\hat{a}})$ |  | (49) |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  | $\log P_{\theta}(\tau|z)=\sum_{t=0}^{T}\log P_{\theta}(s_{t}|z,h_{t-1}^{\hat{s}})+\sum_{t=0}^{T-1}\log
    P_{\theta}(a_{t}|z,h_{t-1}^{\hat{s},\hat{a}})$ |  | (49) |'
- en: 'In this way, the whole trajectory can be predicted without interacting with
    the environment/simulator, which is necessary in certain scenarios. In this scheme,
    the embedding $z$ is inferred from the entire trajectory $\tau$, necessitating
    that the encoder $P_{\phi}$ be implemented with specialized architectures, such
    as encoder-only transformers (see Section [2.4](#S2.SS4 "2.4 Transformers ‣ 2
    Background on Deep Generative Models ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) or RNNs (like
    VRNN in Section [2.1](#S2.SS1 "2.1 Variational Auto-Encoders ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")). Different from aforementioned
    two schemes, such a trajectory-conditioned encoder can output embeddings $z$ that
    encapsulate information at the task or subtask level. Thus, the learned $P_{\theta}(a_{t}|s_{t},z)$
    can be viewed as a multi-task policy which varies with the task embedding $z$.
    As a side note, if replacing the decoder $P_{\theta}(\tau|z)$ with $P_{\theta}(\tau^{\prime}|z)$,
    where $(\tau,\tau^{\prime})\sim D_{E}$ are (state-only) expert trajectories and
    $\tau^{\prime}$ is a future trajectory segment of $\tau$, Eq. ([48](#S3.E48 "In
    3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation Learning ‣
    3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))
    can be used for training a trajectory forecasting model, as detailed in RC-VAE
    (Qi et al. ([2020](#bib.bib261))). By using such a temporal target, i.e, the future
    segment, the representation $z$ is forced to contain predictive information.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '通过这种方式，可以在不与环境/模拟器交互的情况下预测整个轨迹，这在某些场景下是必要的。在该方案中，嵌入 $z$ 是从整个轨迹 $\tau$ 中推断得出的，这要求编码器
    $P_{\phi}$ 必须使用专门的架构，例如仅编码器变换器（参见第 [2.4](#S2.SS4 "2.4 Transformers ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") 节）或 RNN（如第 [2.1](#S2.SS1
    "2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 节中的 VRNN）。与上述两个方案不同，这种条件于轨迹的编码器可以输出封装任务或子任务级信息的嵌入 $z$。因此，学习到的
    $P_{\theta}(a_{t}|s_{t},z)$ 可以视为一个多任务策略，其随着任务嵌入 $z$ 的变化而变化。附带说明，如果将解码器 $P_{\theta}(\tau|z)$
    替换为 $P_{\theta}(\tau^{\prime}|z)$，其中 $(\tau,\tau^{\prime})\sim D_{E}$ 是（仅状态）专家轨迹且
    $\tau^{\prime}$ 是 $\tau$ 的未来轨迹段，则可以使用公式 ([48](#S3.E48 "In 3.2.1 Core Schemes of
    VAE-based Imitation Learning ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) 来训练轨迹预测模型，详细信息见 RC-VAE（Qi
    等人（[2020](#bib.bib261)））。通过使用这样的时间目标，即未来段，表示 $z$ 被迫包含预测信息。'
- en: 'Scheme (4): The last scheme is based on the Variational Information Bottleneck
    (VIB) framework (Alemi et al. ([2017](#bib.bib8))), of which the original objective
    function is as below:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 方案（4）：最后的方案基于变分信息瓶颈（VIB）框架（Alemi 等人（[2017](#bib.bib8)）），其原始目标函数如下：
- en: '|  | $\max_{f:X\rightarrow Y}I(Z,Y;f)\ s.t.\ I(X,Z;f)\leq I_{c}\Rightarrow\max_{f:X\rightarrow
    Y}I(Z,Y;f)-\beta I(X,Z;f)$ |  | (50) |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{f:X\rightarrow Y}I(Z,Y;f)\ s.t.\ I(X,Z;f)\leq I_{c}\Rightarrow\max_{f:X\rightarrow
    Y}I(Z,Y;f)-\beta I(X,Z;f)$ |  | (50) |'
- en: 'where $I(\cdot)$ denotes the mutual information, $I_{c}$ is the information
    constraint, and $\beta>0$ is the Lagrangian multiplier. The goal of this framework
    is to learn an encoding $Z$ that is maximally expressive about the target $Y$
    while being maximally compressive about the input $X$. Substituting $X,Y$ with
    $s,a$, the function to learn, i.e., $f$, is then the expert policy $\pi(a|s)$.
    VIB-based IL can be practically solved by the following objective, which is a
    lower bound of Eq. ([50](#S3.E50 "In 3.2.1 Core Schemes of VAE-based Imitation
    Learning ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) (see (Alemi et al. ([2017](#bib.bib8)))):'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $I(\cdot)$ 表示互信息，$I_{c}$ 是信息约束，$\beta>0$ 是拉格朗日乘子。该框架的目标是学习一个在最大程度上对目标 $Y$
    表达性强，同时对输入 $X$ 最大程度上压缩的编码 $Z$。将 $X,Y$ 替换为 $s,a$，则要学习的函数，即 $f$，是专家策略 $\pi(a|s)$。基于
    VIB 的 IL 可以通过以下目标实用地解决，这个目标是公式 ([50](#S3.E50 "In 3.2.1 Core Schemes of VAE-based
    Imitation Learning ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) 的下界（参见（Alemi 等人（[2017](#bib.bib8)）））。'
- en: '|  | $\max_{\phi,\theta,\omega}\mathbb{E}_{(s,a)\sim D_{E}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot&#124;s)}\log P_{\theta}(a&#124;z)-\beta D_{KL}(P_{\phi}(z&#124;s)&#124;&#124;P_{\omega}(z))\right]$
    |  | (51) |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\phi,\theta,\omega}\mathbb{E}_{(s,a)\sim D_{E}}\left[\mathbb{E}_{z\sim
    P_{\phi}(\cdot|s)}\log P_{\theta}(a|z)-\beta D_{KL}(P_{\phi}(z|s)||P_{\omega}(z))\right]$
    |  | (51) |'
- en: 'This objective is similar with the VAE ELBO (i.e., Eq. ([2](#S2.E2 "In 2.1
    Variational Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"))) in form but with two main differences. First, the decoder $P_{\theta}$
    here can predict variables different from the input of the encoder, whereas in
    VAEs, it is typically trained to reconstruct the input variable. Second, $P_{\omega}(z)$
    is not an assumed prior distribution of $z$ as in VAEs but a variational approximation
    of the marginal distribution of $z$, i.e., $P_{Z}(z)=\int P_{X}(x)P_{\phi}(z|x)dx$,
    and it is usually learned as a neural network. Both Scheme (2) and Scheme (4)
    can be regarded as forms of imitation learning that incorporate effective representation
    learning. After the training, to sample an action $a\sim\pi(\cdot|s)$, the policy
    encoder first compresses the state to a representation, i.e., $z\sim P_{\phi}(\cdot|s)$,
    and then its decoder predicts the action based on $z$, i.e., $a\sim P_{\theta}(\cdot|z)$.
    When dealing with high-dimensional and noisy observations $s$, using an information
    bottleneck can filter out redundant information while retaining essential knowledge
    for action predictions in $z$. A higher compression rate on the input often results
    in better generalization of the learned policy across tasks.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目标在形式上类似于 VAE ELBO（即，方程式 ([2](#S2.E2 "在 2.1 变分自编码器 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")))，但有两个主要区别。首先，这里的解码器
    $P_{\theta}$ 可以预测与编码器输入不同的变量，而在 VAEs 中，它通常被训练以重建输入变量。其次，$P_{\omega}(z)$ 不是 VAEs
    中假定的 $z$ 的先验分布，而是 $z$ 的边际分布的变分近似，即 $P_{Z}(z)=\int P_{X}(x)P_{\phi}(z|x)dx$，通常作为神经网络来学习。方案（2）和方案（4）都可以被视为融合有效表示学习的模仿学习形式。在训练之后，为了采样一个动作
    $a\sim\pi(\cdot|s)$，策略编码器首先将状态压缩为一个表示，即 $z\sim P_{\phi}(\cdot|s)$，然后其解码器根据 $z$
    预测动作，即 $a\sim P_{\theta}(\cdot|z)$。在处理高维和嘈杂的观察 $s$ 时，使用信息瓶颈可以过滤掉冗余信息，同时保留用于动作预测的
    $z$ 的关键知识。对输入的更高压缩率通常会导致学习到的策略在任务间的更好泛化。
- en: Next, we provide an overview of the applications of VAEs in IL. Most works in
    this direction focus on extending BC. From a data perspective, VAEs can increase
    BC’s data efficiency in scenarios where original, task-specific data is limited,
    or enable the use of multi-modal input data (e.g., from various sensor types).
    Regarding algorithmic advancements, VAEs have been utilized for skill discovery
    to enable hierarchical IL, and to address the causal misidentification issue inherent
    in BC.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们概述了 VAEs 在 IL 中的应用。这个方向的大多数工作都集中在扩展 BC 上。从数据的角度来看，VAEs 可以在原始任务特定数据有限的情况下提高
    BC 的数据效率，或者启用使用多模态输入数据（例如，来自各种传感器类型）。关于算法进展，VAEs 已被用于技能发现，以实现层次化 IL，并解决 BC 中固有的因果误识别问题。
- en: 3.2.2 Improving Data Efficiency in Imitation Learning with VAEs
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 使用 VAEs 改善模仿学习中的数据效率
- en: IL, especially BC, requires a large amount of training data to cover possible
    task scenarios for robust performance. However, expert-level demonstrations for
    a specific task, i.e., $D_{E}$, are usually costly to acquire. Therefore, efficient
    data usage in IL is crucial. This can be achieved by leveraging the inherent structures
    within the data, or by gathering and processing task-related data from alternative
    sources for training. The capability of VAEs to extract latent representations
    plays a key role in facilitating these processes. Here, we present several notable
    works in this direction.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习（IL），特别是行为克隆（BC），需要大量的训练数据来覆盖可能的任务场景，以实现稳健的性能。然而，特定任务的专家级示范，即 $D_{E}$，通常很难获取。因此，高效的数据使用在
    IL 中至关重要。这可以通过利用数据中固有的结构来实现，或者通过从其他来源收集和处理任务相关数据来进行训练。 VAEs 提取潜在表示的能力在促进这些过程方面起着关键作用。在这里，我们介绍几个在这方面的显著工作。
- en: 'Behavior Retrieval (Du et al. ([2023](#bib.bib79))) is proposed for the case
    where vast task-unlabelled demonstrations are available, i.e., $D_{\text{prior}}$.
    Specifically, $D_{\text{prior}}$ may contain demonstrations for a range of different
    but related tasks (following the same task distribution) or suboptimal behaviors
    for the current task. To this end, they propose to adopt a $\beta$-VAE (i.e.,
    $(P_{\phi},P_{\theta})$ in Eq. ([5](#S2.E5 "In 1st item ‣ 2.1 Variational Auto-Encoders
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))) to
    learn the latent representations of $(s,a)\sim D_{\text{prior}}$. Then, new training
    data that is similar with the ones in $D_{E}$ can be retrieved from $D_{\text{prior}}$
    as the augmentation $D_{\text{ret}}$. The similarity between data points can be
    simply measured based on their latents from the pretrained encoder $P_{\phi}$:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '行为检索（Du et al. ([2023](#bib.bib79))) 适用于有大量任务未标记演示的情况，即 $D_{\text{prior}}$。具体来说，$D_{\text{prior}}$
    可能包含针对一系列不同但相关任务（遵循相同任务分布）的演示或当前任务的次优行为。为此，他们建议采用 $\beta$-VAE（即 Eq. ([5](#S2.E5
    "In 1st item ‣ 2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"))) 来学习 $(s,a)\sim D_{\text{prior}}$ 的潜在表示。然后，可以从
    $D_{\text{prior}}$ 中检索与 $D_{E}$ 中数据相似的新训练数据，作为扩增数据 $D_{\text{ret}}$。数据点之间的相似性可以简单地根据它们从预训练编码器
    $P_{\phi}$ 中得到的潜在值来衡量：'
- en: '|  | $\displaystyle f((s_{1},a_{1}),(s_{2},a_{2}))=-\&#124;z_{1}-z_{2}\&#124;_{2},~{}(s_{1},a_{1})\in
    D_{\text{prior}},~{}(s_{2},a_{2})\in D_{E},~{}z_{i}\sim P_{\phi}(\cdot&#124;(s_{i},a_{i})).$
    |  | (52) |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f((s_{1},a_{1}),(s_{2},a_{2}))=-\&#124;z_{1}-z_{2}\&#124;_{2},~{}(s_{1},a_{1})\in
    D_{\text{prior}},~{}(s_{2},a_{2})\in D_{E},~{}z_{i}\sim P_{\phi}(\cdot&#124;(s_{i},a_{i})).$
    |  | (52) |'
- en: They claim that this method would effectively filter out sub-optimal or task-irrelevant
    data. As a final step, a policy $\pi(a|s)$ can be learned on $D_{E}\cup D_{\text{ret}}$
    through BC. This whole protocol resembles aforementioned Scheme (2).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 他们声称这种方法可以有效过滤掉次优或与任务无关的数据。最后一步，可以在 $D_{E}\cup D_{\text{ret}}$ 上通过 BC 学习一个策略
    $\pi(a|s)$。整个协议类似于前述的方案 (2)。
- en: 'For robotic control tasks, state-only demonstrations from a third-person view
    (e.g., videos from a demonstrator) are usually more available than the ones in
    the first-person view. State observations from the first- and third-person views,
    i.e., $s^{F}$ and $s^{T}$, correspond to the same true environment state $s$.
    Given a set of demonstrations synchronized across different viewpoints $[(s_{i}^{F},s_{i}^{T_{1}},\cdots,s_{i}^{T_{k}})]_{i=1}^{N}$,
    where $T_{1:k}$ denotes $k$ different third-person viewpoints, a common viewpoint-agnostic
    representation is required to make full use of these data. To realize this, VAE-TPIL
    (Shang & Ryoo ([2021](#bib.bib291))) proposes to learn two VAEs for the first-
    and third-person view data respectively, i.e., $(P_{\phi^{F}},P_{\theta^{F}})$
    and $(P_{\phi^{T}},P_{\theta^{T}})$, and to disentangle the latent variable to
    a viewpoint embedding $v$ and a viewpoint-agnostic state embedding $h$. To realize
    this, besides usual reconstruction objective terms, auxiliary objectives are involved:
    ($i\neq j,\ (h,v)\sim P_{\phi}(s)$)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器人控制任务，从第三人称视角（例如，演示者的视频）的状态-only 演示通常比从第一人称视角获得的演示更为常见。第一人称和第三人称视角下的状态观测，即
    $s^{F}$ 和 $s^{T}$，对应于相同的真实环境状态 $s$。给定一组在不同视角下同步的演示 $[(s_{i}^{F},s_{i}^{T_{1}},\cdots,s_{i}^{T_{k}})]_{i=1}^{N}$，其中
    $T_{1:k}$ 表示 $k$ 个不同的第三人称视角，需要一个通用的视角无关表示来充分利用这些数据。为实现这一点，VAE-TPIL（Shang & Ryoo
    ([2021](#bib.bib291))) 提出了分别为第一人称和第三人称视角数据学习两个 VAE，即 $(P_{\phi^{F}},P_{\theta^{F}})$
    和 $(P_{\phi^{T}},P_{\theta^{T}})$，并将潜在变量解耦为视角嵌入 $v$ 和视角无关状态嵌入 $h$。为实现这一点，除了常规的重建目标项外，还涉及辅助目标：($i\neq
    j,\ (h,v)\sim P_{\phi}(s)$)
- en: '|  | $\mathcal{L}_{v}=\mathbb{E}_{s_{i}^{T}}\left[&#124;&#124;P_{\theta}(h_{i}^{T},v_{j}^{T})-s_{i}^{T}&#124;&#124;_{2}\right],\
    \mathcal{L}_{h}=\mathbb{E}_{s^{T_{i}}}\left[&#124;&#124;P_{\theta}(h^{T_{j}},v^{T_{i}})-s^{T_{i}}&#124;&#124;_{2}\right],\
    \mathcal{L}_{F}=\mathbb{E}_{s^{T},s^{F}}\left[&#124;&#124;h^{T}-\text{sg}(h^{F})&#124;&#124;_{2}\right]$
    |  | (53) |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{v}=\mathbb{E}_{s_{i}^{T}}\left[&#124;&#124;P_{\theta}(h_{i}^{T},v_{j}^{T})-s_{i}^{T}&#124;&#124;_{2}\right],\
    \mathcal{L}_{h}=\mathbb{E}_{s^{T_{i}}}\left[&#124;&#124;P_{\theta}(h^{T_{j}},v^{T_{i}})-s^{T_{i}}&#124;&#124;_{2}\right],\
    \mathcal{L}_{F}=\mathbb{E}_{s^{T},s^{F}}\left[&#124;&#124;h^{T}-\text{sg}(h^{F})&#124;&#124;_{2}\right]$
    |  | (53) |'
- en: 'The intuition behind these terms are: (a) The view embeddings $v^{T}_{i}$ and
    $v^{T}_{j}$ of states in the same view $s_{i}^{T}$ and $s_{j}^{T}$ should be semantically
    equivalent and so $P_{\theta}(h_{i}^{T},v_{j}^{T})$ should still recover $s_{i}^{T}$.
    (b) The embeddings $h^{T_{i}}$ and $h^{T_{j}}$ corresponding to the same state
    from different views $T_{i}$ and $T_{j}$ should be interchangeable for reconstructing
    $s^{T_{i}}$, since $h$ is expected to be view-agnostic. (c) For the same reason,
    $h^{T}$ and $h^{F}$ should be similar as they correspond to the same true environment
    state. With the pretrained $P_{\phi^{F}}$ and $P_{\phi^{T}}$, demonstrations across
    various viewpoints can be converted to viewpoint-agnostic latent representations
    (i.e., $h$). Then, a representation-based policy $\pi(a|h)$ can be learned with
    any IL method.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这些术语背后的直觉是：(a) 同一视角下的状态的视角嵌入 $v^{T}_{i}$ 和 $v^{T}_{j}$ 应该在语义上是等价的，因此 $P_{\theta}(h_{i}^{T},v_{j}^{T})$
    应该仍然能恢复 $s_{i}^{T}$。(b) 对于来自不同视角 $T_{i}$ 和 $T_{j}$ 的同一状态，嵌入 $h^{T_{i}}$ 和 $h^{T_{j}}$
    应该是可以互换的，用于重建 $s^{T_{i}}$，因为 $h$ 期望是视角无关的。(c) 出于同样的原因，$h^{T}$ 和 $h^{F}$ 应该是相似的，因为它们对应于同一真实环境状态。通过预训练的
    $P_{\phi^{F}}$ 和 $P_{\phi^{T}}$，可以将各种视角下的演示转换为视角无关的潜在表示（即 $h$）。然后，可以使用任何模仿学习方法来学习基于表示的策略
    $\pi(a|h)$。
- en: GIRIL (Yu et al. ([2020b](#bib.bib381))) suggests modelling the underlying transition
    from $(s_{t},a_{t})$ to $s_{t+1}$ in $D_{E}$ using a CVAE, where $(s_{t},a_{t})$
    and $s_{t+1}$ work as the condition $c$ and data sample $x$ respectively, besides
    imitating the policy $\pi(a_{t}|s_{t})$. Then, an intrinsic reward can be defined
    as $r_{t}=||\hat{s}_{t+1}-s_{t+1}||_{2}^{2}$, where $\hat{s}_{t+1}\sim P_{\theta}(\cdot|s_{t},a_{t})$.
    (Offline) RL can be applied to the extended demonstrations $[(s_{t},a_{t},r_{t},s_{t+1})]$
    to further improve the imitator learned via BC, i.e., $\pi(a_{t}|s_{t})$. Intuitively,
    this process encourages the agent to explore states with high prediction errors
    more to reduce the state uncertainty, making it possible to achieve better-than-expert
    performance.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: GIRIL（Yu et al. ([2020b](#bib.bib381))) 建议使用CVAE对从 $(s_{t},a_{t})$ 到 $s_{t+1}$
    的底层过渡进行建模，其中 $(s_{t},a_{t})$ 和 $s_{t+1}$ 分别作为条件 $c$ 和数据样本 $x$，除此之外，还模仿策略 $\pi(a_{t}|s_{t})$。然后，可以定义内在奖励为
    $r_{t}=||\hat{s}_{t+1}-s_{t+1}||_{2}^{2}$，其中 $\hat{s}_{t+1}\sim P_{\theta}(\cdot|s_{t},a_{t})$。可以对扩展的演示
    $[(s_{t},a_{t},r_{t},s_{t+1})]$ 应用（离线）强化学习，以进一步改进通过行为克隆（BC）学习的模仿者，即 $\pi(a_{t}|s_{t})$。直观上，这个过程鼓励代理更多地探索具有高预测误差的状态，以减少状态的不确定性，从而有可能实现超过专家的表现。
- en: 3.2.3 Managing Multi-Modal Inputs in Imitation Learning via VAEs
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 通过变分自编码器（VAEs）管理模仿学习中的多模态输入
- en: In real-world scenarios, decision-making inputs often come from multiple modalities,
    including RGB images, depth images, 3D point clouds, language instructions, and
    more. As a motivating example, consider the task of navigating a drone in an outdoor
    environment, where multiple modalities emerge due to visual information captured
    by various sensors and factors like the drone’s current pose and location. Coherently
    managing the multi-modal input is essential for the real-life applications of
    IL. In this context, VAEs have proven effective for fusing or unifying inputs
    from multiple modalities based on the use of latent embeddings.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界场景中，决策输入通常来自多种模式，包括RGB图像、深度图像、3D点云、语言指令等。作为一个激励性的例子，考虑在户外环境中操控无人机的任务，其中由于各种传感器捕捉的视觉信息以及无人机当前的姿态和位置等因素，会出现多种模式。有效管理这些多模态输入对于模仿学习的实际应用至关重要。在这种情况下，变分自编码器（VAEs）在基于潜在嵌入融合或统一来自多个模态的输入方面表现出色。
- en: 'RGBD-VIB (Du et al. ([2022](#bib.bib80))) proposes to fuse multi-modal inputs
    based on their uncertainty for evaluation, ensuring that inputs with higher certainty
    are given more weight in decision-making. To this end, a VAE $(P_{\phi^{i}}(z|s),P_{\theta^{i}}(a|z),P_{\omega^{i}}(z))$
    is trained for each modality $i\in[1,\cdots,N]$ using the VIB framework (i.e.,
    Eq. ([50](#S3.E50 "In 3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2
    Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))). Note that these VAEs share the same action space but
    vary in their state spaces. The uncertainty measure can then be acquired based
    on the pretrained VAEs as $u^{i}(s)=D_{KL}(P_{\phi^{i}}(z|s)||P_{\omega^{i}}(z))$.
    Intuitively, a large uncertainty $u^{i}(s)$ indicates that the current state in
    modality $i$ may be out of the distribution of the training dataset. Finally,
    the action is a weighted sum of the output from each VAE, where the weight assigned
    to each modality is proportional to $\sum_{j=1}^{N}u^{j}(s)-u^{i}(s)$, inversely
    related to the uncertainty.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 'RGBD-VIB（Du et al. ([2022](#bib.bib80))) 提出基于不确定性融合多模态输入以进行评估，确保在决策中对更高确定性的输入赋予更大权重。为此，为每种模态$i\in[1,\cdots,N]$使用VIB框架（即方程
    ([50](#S3.E50 "In 3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"))）训练了一个VAE$(P_{\phi^{i}}(z|s),P_{\theta^{i}}(a|z),P_{\omega^{i}}(z))$。请注意，这些VAEs共享相同的动作空间，但状态空间各异。然后，可以基于预训练的VAEs获得不确定性度量$u^{i}(s)=D_{KL}(P_{\phi^{i}}(z|s)||P_{\omega^{i}}(z))$。直观上，大的不确定性$u^{i}(s)$表示模态$i$中的当前状态可能超出了训练数据集的分布。最后，动作是来自每个VAE输出的加权和，其中分配给每种模态的权重与$\sum_{j=1}^{N}u^{j}(s)-u^{i}(s)$成正比，与不确定性成反比。'
- en: 'Demonstrations may come with contexts $c$, such as the task id, language instructions,
    goal images, etc. In this case, a conditional policy $\pi(a|s,c)$ can be learned
    for specific task goals or instructions. MCIL (Lynch & Sermanet ([2021](#bib.bib206)))
    is proposed for the scenario where multiple related contexts $c_{i},i\in[1,\cdots,N]$
    are provided for a task. For a coherent use of these contexts, MCIL learns an
    encoder for each context, i.e., $(P_{\phi^{1}}(z|c_{1}),\cdots,P_{\phi^{N}}(z|c_{N}))$,
    to embed different contexts into a unified latent space, and a decoder $P_{\theta}(a|s,z)$
    as the latent-conditioned policy. The decoder and encoders can be trained end-to-end
    by $\max_{\theta,\phi^{1:N}}\mathbb{E}_{(s,a,c_{i})\sim D_{E},z\sim P_{\phi^{i}}(\cdot|c_{i})}\left[\log
    P_{\theta}(a|s,z)\right]$ ^(13)^(13)13This objective ensembles the VIB framework,
    i.e., Eq. ([50](#S3.E50 "In 3.2.1 Core Schemes of VAE-based Imitation Learning
    ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")), but overlooks the regulation on $z$. Also, training a
    separate encoder for each context is necessary, since dealing with different modalities
    requires different foundation models.. This leads to a policy that generalizes
    over multi-modal contexts: $z\sim P_{\phi^{i}}(\cdot|c_{i}),a\sim P_{\theta}(\cdot|s,z),i\in[1,\cdots,N]$.
    As an example, language instructions are usually costly to annotate, but other
    contexts like the goal images are relatively easy to obtain (from sensors). Through
    such a unified latent space, MCIL enables the use of (large-amount) demonstrations
    from easier sources to aid the learning of a language-conditioned policy.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '演示可能会附带上下文$c$，如任务ID、语言指令、目标图像等。在这种情况下，可以针对特定任务目标或指令学习条件策略$\pi(a|s,c)$。MCIL（Lynch
    & Sermanet ([2021](#bib.bib206))) 被提出用于提供多个相关上下文$c_{i},i\in[1,\cdots,N]$的任务场景。为了协调使用这些上下文，MCIL
    为每个上下文学习一个编码器，即$(P_{\phi^{1}}(z|c_{1}),\cdots,P_{\phi^{N}}(z|c_{N}))$，将不同的上下文嵌入到统一的潜在空间中，以及作为潜在条件策略的解码器$P_{\theta}(a|s,z)$。解码器和编码器可以通过$\max_{\theta,\phi^{1:N}}\mathbb{E}_{(s,a,c_{i})\sim
    D_{E},z\sim P_{\phi^{i}}(\cdot|c_{i})}\left[\log P_{\theta}(a|s,z)\right]$进行端到端的训练。这一目标集成了VIB框架，即方程
    ([50](#S3.E50 "In 3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"))，但忽略了对$z$的调节。此外，为每个上下文训练一个单独的编码器是必要的，因为处理不同的模态需要不同的基础模型。这导致了一个在多模态上下文上进行泛化的策略：$z\sim
    P_{\phi^{i}}(\cdot|c_{i}),a\sim P_{\theta}(\cdot|s,z),i\in[1,\cdots,N]$。例如，语言指令通常标注成本高，但其他上下文如目标图像相对容易获得（来自传感器）。通过这样的统一潜在空间，MCIL
    能够利用来自更容易源的大量演示来帮助学习语言条件策略。'
- en: 'Similarly with MCIL, CM-VAE-BC (Bonatti et al. ([2020](#bib.bib28))) aims at
    learning a joint low-dimensional embedding for multiple data modalities. It utilizes
    CM-VAE (Spurr et al. ([2018](#bib.bib309))), where a pair of encoder-decoder $(P_{\phi^{i}}(z|s),P_{\theta^{i}}(s|z))$
    is trained for each modality $i\in[1,\cdots,N]$ via cross-modality reconstructions
    of which the objective is:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 与 MCIL 类似，CM-VAE-BC（Bonatti 等人 ([2020](#bib.bib28))）旨在为多种数据模态学习一个联合的低维嵌入。它利用
    CM-VAE（Spurr 等人 ([2018](#bib.bib309))），其中为每种模态 $i\in[1,\cdots,N]$ 训练一对编码器-解码器
    $(P_{\phi^{i}}(z|s),P_{\theta^{i}}(s|z))$，其目标是通过跨模态重建来实现：
- en: '|  | $\max_{\phi^{i},\theta^{j}}\mathbb{E}_{z\sim P_{\phi^{i}}(\cdot&#124;s^{i})}\left[\log
    P_{\theta^{j}}(s^{j}&#124;z)\right]-D_{KL}(P_{\phi^{i}}(z&#124;s^{i})&#124;&#124;P_{\theta^{i}}(z))$
    |  | (54) |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\phi^{i},\theta^{j}}\mathbb{E}_{z\sim P_{\phi^{i}}(\cdot|s^{i})}\left[\log
    P_{\theta^{j}}(s^{j}|z)\right]-D_{KL}(P_{\phi^{i}}(z|s^{i})||P_{\theta^{i}}(z))$
    |  | (54) |'
- en: 'Suppose $N=2$, then $(i,j)$ in the equation above is an enumeration over the
    set $\{(1,1),(1,2),(2,1),(2,2)\}$. Note that the equation above is a variational
    lower bound of $\log P_{\theta^{j}}(s^{j})$ and $s^{i}\ \&amp;\ s^{j}$ corresponds
    to the same true environment state. This cross-modal training regime results in
    a single latent space that allows embedding and reconstructing multiple data modalities.
    After the representation learning, a policy conditioned on the latent variable
    $\pi(a|z)$ can be learned with any IL method (e.g., BC), following aforementioned
    Scheme (2), i.e., Eq. ([47](#S3.E47 "In 3.2.1 Core Schemes of VAE-based Imitation
    Learning ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 $N=2$，则上式中的 $(i,j)$ 是集合 $\{(1,1),(1,2),(2,1),(2,2)\}$ 的枚举。注意，上述方程是 $\log
    P_{\theta^{j}}(s^{j})$ 的变分下界，并且 $s^{i}\ \&\ s^{j}$ 对应于相同的真实环境状态。这种跨模态训练机制会导致一个单一的潜在空间，从而允许嵌入和重建多种数据模态。在表征学习之后，可以使用任何
    IL 方法（例如，BC）学习一个以潜在变量为条件的策略 $\pi(a|z)$，按照上述方案 (2)，即 Eq. ([47](#S3.E47 "在 3.2.1
    VAE 基础模仿学习核心方案 ‣ 3.2 模仿学习 ‣ 3 VAE 在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、综述及未来方向的展望"))。
- en: 3.2.4 Skill Acquisition and Hierarchical Imitation Learning through VAEs
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 技能获取与通过 VAE 的层级模仿学习
- en: 'In the context of IL, skills are meaningful subsequences/patterns within the
    expert trajectories, which can be potentially utilized in multiple tasks. As an
    example, consider IL for cooking tasks, where the tasks can be diverse but often
    have overlapping patterns, such as, slicing, chopping, etc. As introduced in OPAL
    (see Section [3.1.5](#S3.SS1.SSS5 "3.1.5 Offline Multi-task/Hierarchical RL based
    on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), VAEs can be used to
    extract such patterns via learnt latent embeddings (i.e., using Eq. ([45](#S3.E45
    "In 3.1.5 Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) which is a special case of aforementioned Scheme (3)). As in OPAL,
    after training, the decoder $P_{\theta}(a|s,z)$ can be viewed as the policy for
    skill $z$ (i.e., $\pi_{\text{low}}(a|s,z)$), and the encoder $P_{\phi}(z|\tau)$
    can be used to parse the expert data into trajectories of $(s,z)$ pairs for training
    the high-level policy $\pi_{\text{high}}(z|s)$ with any IL method. Then, the hierarchical
    policy can be executed in a call-and-return manner: sampling a skill $z\sim\pi_{\text{high}}(\cdot|s)$,
    executing the corresponding policy $a\sim\pi_{\text{low}}(\cdot|s,z)$ for $h$
    time steps, sampling the next skill, and so on. This whole process can be considered
    as a hierarchical extension of BC. Beyond this straightforward use of VAEs, some
    works have proposed to impose additional structures or properties, such as disentanglement,
    to the latent space for improved skill acquisition (e.g., in interpretability).
    As mentioned in Section [2.1](#S2.SS1 "2.1 Variational Auto-Encoders ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), being disentangled
    means that each dimension/partition of the latent variable is independent and
    semantically meaningful. One way to realize this is to directly incorporate an
    auxiliary loss term to the standard VAE ELBO as regularization:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '在强化学习（IL）的背景下，技能是专家轨迹中的有意义的子序列/模式，这些模式可以在多个任务中潜在地被利用。例如，考虑烹饪任务的IL，这些任务可能各异但通常具有重叠的模式，例如切片、剁碎等。如在OPAL中介绍的（见第[3.1.5节](#S3.SS1.SSS5
    "3.1.5 Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")），可以通过学习的潜在嵌入（即使用公式 ([45](#S3.E45 "In 3.1.5 Offline Multi-task/Hierarchical
    RL based on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")））提取这些模式，这是前述方案（3）的特例。如在OPAL中，经过训练后，解码器
    $P_{\theta}(a|s,z)$ 可以视为技能 $z$ 的策略（即 $\pi_{\text{low}}(a|s,z)$），编码器 $P_{\phi}(z|\tau)$
    可用于将专家数据解析为 $(s,z)$ 对的轨迹，以训练任何IL方法的高层策略 $\pi_{\text{high}}(z|s)$。然后，分层策略可以以调用和返回的方式执行：首先采样技能
    $z\sim\pi_{\text{high}}(\cdot|s)$，执行对应的策略 $a\sim\pi_{\text{low}}(\cdot|s,z)$ 达到
    $h$ 个时间步，然后采样下一个技能，依此类推。这个过程可以被视为BC的分层扩展。除了这种VAEs的直接使用外，一些研究提出在潜在空间中施加额外的结构或属性，如解耦，以改善技能获取（例如，在可解释性方面）。如第[2.1节](#S2.SS1
    "2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")中提到的，解耦意味着潜在变量的每个维度/分区都是独立且具有语义意义的。实现这一点的一种方法是将辅助损失项直接加入标准VAE
    ELBO中作为正则化项：'
- en: '|  | $\displaystyle\max_{\phi,\theta,\omega}\mathcal{L}^{\text{ELBO}}_{\theta,\phi}(D_{E})-\lambda\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E}),$
    |  | (55) |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\phi,\theta,\omega}\mathcal{L}^{\text{ELBO}}_{\theta,\phi}(D_{E})-\lambda\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E}),$
    |  | (55) |'
- en: 'where $\mathcal{L}^{\text{ELBO}}_{\theta,\phi}(D_{E})$ usually takes the form
    of Eq. ([45](#S3.E45 "In 3.1.5 Offline Multi-task/Hierarchical RL based on VAEs
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")); $\omega$ is the parameter of
    an auxiliary network; $\lambda>0$ controls the tradeoff between the two loss terms.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathcal{L}^{\text{ELBO}}_{\theta,\phi}(D_{E})$ 通常呈现为 Eq. ([45](#S3.E45
    "In 3.1.5 Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) 的形式；$\omega$ 是辅助网络的参数；$\lambda>0$ 控制两个损失项之间的权衡。'
- en: SAILOR (Nasiriany et al. ([2022](#bib.bib228))) defines $\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E})=\mathbb{E}_{(\tau_{1},\tau_{2})\sim
    D_{E}}\left[f_{\omega}\left(\mu(P_{\phi}(\cdot|\tau_{1}),\mu(P_{\phi}(\cdot|\tau_{2})))\right)-t\right]^{2}$,
    where $\tau_{1}$ and $\tau_{2}$ belong to the same trajectory and are separated
    by $t$ time steps, $\mu(P_{\phi}(\cdot|\tau_{1}))$ denotes the mean of the encoder
    output. Such a term can encourage the learned embedding to be predictable (for
    the temporal difference between skills) and consistent, which is important for
    downstream policy learning with these skills. Note that the gradient from $\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E})$
    can be backpropagated to the encoder, allowing this term to shape the learned
    skill embeddings.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: SAILOR (Nasiriany 等人 ([2022](#bib.bib228))) 定义了 $\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E})=\mathbb{E}_{(\tau_{1},\tau_{2})\sim
    D_{E}}\left[f_{\omega}\left(\mu(P_{\phi}(\cdot|\tau_{1}),\mu(P_{\phi}(\cdot|\tau_{2})))\right)-t\right]^{2}$，其中
    $\tau_{1}$ 和 $\tau_{2}$ 属于同一轨迹且相隔 $t$ 个时间步，$\mu(P_{\phi}(\cdot|\tau_{1}))$ 表示编码器输出的均值。这样的术语可以鼓励学习到的嵌入具有可预测性（对于技能间的时间差）和一致性，这对于下游的策略学习至关重要。请注意，来自
    $\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E})$ 的梯度可以反向传播到编码器，从而使得这个术语可以塑造学习到的技能嵌入。
- en: 'Both OPAL and SAILOR would partition trajectories into segments of length $h$
    (i.e., the assumed skill horizon) and then train a CVAE on these independent segments
    for skill acquisition (i.e., with Eq. ([45](#S3.E45 "In 3.1.5 Offline Multi-task/Hierarchical
    RL based on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))). CKA (Pasula ([2020](#bib.bib244))),
    on the other hand, proposes a regularization term on the entire trajectory. In
    particular, if a trajectory $\tau$ can be decomposed into $m$ skills in a sequence,
    the learned skill embeddings $z_{1:m}$ can be regularized by a mutual information
    term $\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E})=-\mathbb{E}_{\tau\sim
    D_{E},z_{i}\sim P_{\phi}(\cdot|\tau_{i})}I\left(\sum_{i=1}^{m}z_{i};\tau\right)$,
    where $\tau_{i}$ is the $i$-th segment of $\tau$. The intuition is that, as a
    complex behavior, $\tau$ should be a combination of these skills. By maximizing
    the mutual information, the interpretability of the learned embeddings for the
    behavior $\tau$ can be enhanced. The reason to assume a summation structure $\sum_{i=1}^{m}z_{i}$
    as the combination form is to enable a practical estimation of the mutual information
    term (see (Pasula ([2020](#bib.bib244)))). Aforementioned methods either assume
    a fixed skill horizon of $h$ or require that the skill segmentation of the trajectory
    is readily available, thus only learning skill embeddings for each segment. CompILE
    (Kipf et al. ([2019](#bib.bib168))) suggests a method to concurrently infer both
    the skill boundaries and skill embeddings directly from the trajectory.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: OPAL和SAILOR会将轨迹划分为长度为$h$（即假定的技能范围）的段，然后在这些独立段上训练CVAE以获取技能（即参见Eq. ([45](#S3.E45
    "在3.1.5离线多任务/层次RL基于VAEs ‣ 3.1 离线强化学习 ‣ 3 变分自编码器在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望"))）。另一方面，CKA
    (Pasula ([2020](#bib.bib244))) 提出了对整个轨迹的正则化项。特别地，如果一个轨迹$\tau$可以分解为一个顺序中的$m$个技能，则学习到的技能嵌入$z_{1:m}$可以通过互信息项$\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E})=-\mathbb{E}_{\tau\sim
    D_{E},z_{i}\sim P_{\phi}(\cdot|\tau_{i})}I\left(\sum_{i=1}^{m}z_{i};\tau\right)$进行正则化，其中$\tau_{i}$是$\tau$的第$i$段。直观上，作为一种复杂行为，$\tau$应该是这些技能的组合。通过最大化互信息，可以增强对行为$\tau$的学习嵌入的可解释性。假设求和结构$\sum_{i=1}^{m}z_{i}$作为组合形式的原因是为了实现互信息项的实用估计（见Pasula
    ([2020](#bib.bib244))）。上述方法要么假定固定的技能范围$h$，要么要求轨迹的技能分割已经准备好，从而仅为每个段学习技能嵌入。CompILE
    (Kipf et al. ([2019](#bib.bib168))) 提出了从轨迹中同时推断技能边界和技能嵌入的方法。
- en: 'A major benefit of skill acquisition lies in the potential for skills to be
    transferred across multiple related tasks. This transfer can facilitate the learning
    of a multi-task policy, $\pi(a|s,z,c)$, where $z$ and $c$ represent the skill
    and task, respectively. In this case, $(z,c)$ constitutes the policy condition
    and disentanglement between $z$ and $c$ are required for interpretable and controlled
    behavior generation. To this end, TC-VAE (Noseworthy et al. ([2019](#bib.bib234)))
    is proposed for the scenario where the task variables $c$ are provided and task-irrelevant
    skill embeddings $z$ need to be learned. A regularization term $\max_{\phi}\min_{\omega}\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E})=\max_{\phi}\min_{\omega}\mathbb{E}_{(\tau,c)\sim
    D_{E}}||f_{\omega}(\mu(P_{\phi}(\cdot|\tau)))-c||_{1}$ is adopted, where $f_{\omega}$
    is trained to predict the task from the mean of the encoder $\mu(P_{\phi}(\cdot|\tau))$.
    Intuitively, maximizing this objective over $\phi$ will encourage the learned
    latent space $z$ to be non-informative about the task $c$. SKILL-IL (Bian et al.
    ([2022](#bib.bib23))), on the other hand, learns $z$ and $c$ in the meantime as
    the latent variable of a VAE encoded from a set of multi-task demonstrations.
    To encourage the disentanglement, a Gated VAE (see Section [2.1](#S2.SS1 "2.1
    Variational Auto-Encoders ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) is adopted, where the latent variable is partitioned into two subdomains
    for $z$ and $c$ respectively. Parameters related to subdomain $z$ are updated
    on data comprising different skills but within the same task. Similarly, parameters
    related to subdomain $c$ are trained with data corresponding to the same skill
    but from different tasks.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '技能获取的一个主要好处在于技能可以在多个相关任务之间进行转移。这种转移可以促进多任务策略 $\pi(a|s,z,c)$ 的学习，其中 $z$ 和 $c$
    分别表示技能和任务。在这种情况下，$(z,c)$ 构成了策略条件，需要对 $z$ 和 $c$ 进行解耦，以便生成可解释和可控的行为。为此，提出了 TC-VAE（Noseworthy
    等人 ([2019](#bib.bib234)))，用于任务变量 $c$ 已知且需要学习与任务无关的技能嵌入 $z$ 的情境。采用了正则化项 $\max_{\phi}\min_{\omega}\mathcal{L}^{\text{reg}}_{\omega,\theta,\phi}(D_{E})=\max_{\phi}\min_{\omega}\mathbb{E}_{(\tau,c)\sim
    D_{E}}||f_{\omega}(\mu(P_{\phi}(\cdot|\tau)))-c||_{1}$，其中 $f_{\omega}$ 被训练以从编码器的均值
    $\mu(P_{\phi}(\cdot|\tau))$ 预测任务。直观地，最大化这个目标函数将鼓励学习到的潜在空间 $z$ 对任务 $c$ 不具有信息性。另一方面，SKILL-IL（Bian
    等人 ([2022](#bib.bib23))) 在同时学习 $z$ 和 $c$，作为从一组多任务演示中编码的 VAE 的潜在变量。为了鼓励解耦，采用了 Gated
    VAE（见 [2.1](#S2.SS1 "2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")），其中潜在变量被分为 $z$ 和 $c$ 两个子域。与子域 $z$ 相关的参数在不同技能但相同任务的数据上更新。类似地，与子域
    $c$ 相关的参数在相同技能但来自不同任务的数据上训练。'
- en: '| Algorithm | VAE Type | VAE Usage | Evaluation Task |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | VAE 类型 | VAE 用途 | 评估任务 |'
- en: '| EIRLI | VAE | Representation Learning (2) | dm_control, Procgen, MAGICAL
    |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| EIRLI | VAE | 表示学习 (2) | dm_control, Procgen, MAGICAL |'
- en: '| T-VAE | VAE | Trajectory modeling (3) | 2D Navigation, 2D Circle, Minecraft
    |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| T-VAE | VAE | 轨迹建模 (3) | 2D 导航, 2D 圆形, Minecraft |'
- en: '| RC-VAE | VRNN | Trajectory forecasting (3) | Basketball Tracking, PEMS-SF
    Traffic, Billiard Ball Trajectory |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| RC-VAE | VRNN | 轨迹预测 (3) | 篮球跟踪, PEMS-SF 交通, 台球轨迹 |'
- en: '| Behavior Retrieval | $\beta$-VAE | Data augmentation (2) | RoboSuite, PyBullet,
    Real Robot |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 行为检索 | $\beta$-VAE | 数据增强 (2) | RoboSuite, PyBullet, 真实机器人 |'
- en: '| VAE-TPIL | VAE | Data augmentation (2) | Minecraft, PyBullet |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| VAE-TPIL | VAE | 数据增强 (2) | Minecraft, PyBullet |'
- en: '| GIRIL | CVAE | Data augmentation | OpenAI Atari, Pybullet |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| GIRIL | CVAE | 数据增强 | OpenAI Atari, Pybullet |'
- en: '| RGBD-VIB | VIB | Handling multi-modal input (4) | Real Robot |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| RGBD-VIB | VIB | 处理多模态输入 (4) | 真实机器人 |'
- en: '| MCIL | VAE | Handling multi-modal input (1) | 3D Playroom |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| MCIL | VAE | 处理多模态输入 (1) | 3D 游乐场 |'
- en: '| CM-VAE-BC | CM-VAE | Handling multi-modal input (2) | AirSim, Real Robot
    |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| CM-VAE-BC | CM-VAE | 处理多模态输入 (2) | AirSim, 真实机器人 |'
- en: '| SAILOR | $\beta$-VAE | Skill acquisition (3) | Franka Kitchen, CALVIN |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| SAILOR | $\beta$-VAE | 技能获取 (3) | Franka Kitchen, CALVIN |'
- en: '| CKA | VAE | Skill acquisition (3) | PyBullet |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| CKA | VAE | 技能获取 (3) | PyBullet |'
- en: '| CompILE | VAE | Skill acquisition (3) | GridWorld, dm_control |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| CompILE | VAE | 技能获取 (3) | GridWorld, dm_control |'
- en: '| TC-VAE | $\beta$-VAE | Skill acquisition (3) | Synthetic Arcs, MIME Pouring
    |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| TC-VAE | $\beta$-VAE | 技能获取 (3) | Synthetic Arcs, MIME 倒酒 |'
- en: '| SKILL-IL | Gated VAE | Skill acquisition (2)(3) | Craftworld, Real-world
    Navigation |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| SKILL-IL | Gated VAE | 技能获取 (2)(3) | Craftworld, 真实世界导航 |'
- en: '| RCM-IL | $\beta$-VAE | Addressing causal confusion (2) | OpenAI Gym/MuJoCo/Atari
    |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| RCM-IL | $\beta$-VAE | 解决因果混淆问题（2） | OpenAI Gym/MuJoCo/Atari |'
- en: '| Masked | $\beta$-VAE | Addressing causal confusion (2) | OpenAI Gym/MuJoCo
    |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| Masked | $\beta$-VAE | 解决因果混淆问题（2） | OpenAI Gym/MuJoCo |'
- en: '| OREO | VQ-VAE | Addressing causal confusion (2) | OpenAI Atari, CARLA |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| OREO | VQ-VAE | 解决因果混淆问题（2） | OpenAI Atari，CARLA |'
- en: 'Table 2: Summary of representative VAE-based IL algorithms. For each algorithm,
    we detail the type of VAE used, how the VAE is applied, and the tasks on which
    it was evaluated. In Column 3, (1)-(4) refer to the 4 schemes of VAE-based IL
    introduced in Section [3.2.1](#S3.SS2.SSS1 "3.2.1 Core Schemes of VAE-based Imitation
    Learning ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"). As for the evaluation tasks, IL algorithms
    has relatively more diverse choices than offline RL algorithms (as shown in Table
    [1](#S3.T1 "Table 1 ‣ 3.1.4 Data Augmentation and Transformation with VAEs ‣ 3.1
    Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")). By Real Robot or Real-world Navigation,
    we mean evaluations on real-world platforms, which vary from one study to another.
    We provide the references of these benchmarks here: dm_control (Tassa et al. ([2018](#bib.bib317))),
    Procgen (Cobbe et al. ([2020](#bib.bib59))), MAGICAL (Toyer et al. ([2020](#bib.bib321))),
    2D Navigation/Circle (Lu et al. ([2019](#bib.bib203))), Minecraft (Guss et al.
    ([2019](#bib.bib116))), Basketball Tracking (Felsen et al. ([2018](#bib.bib90))),
    PEMS-SF Traffic (Dua et al. ([2019](#bib.bib81))), Billiard Ball Trajectory (Fragkiadaki
    et al. ([2016](#bib.bib94))), RoboSuite (Zhu et al. ([2020](#bib.bib396))), PyBullet
    (Coumans & Bai ([2016–2021](#bib.bib63))), OpenAI Gym/Atari/MuJoCo (Brockman et al.
    ([2016](#bib.bib32))), 3D Playroom (Lynch et al. ([2019](#bib.bib207))), AirSim
    (Shah et al. ([2017](#bib.bib289))), Franka Kitchen (Gupta et al. ([2019a](#bib.bib114))),
    CALVIN (Mees et al. ([2022b](#bib.bib219))), GridWorld (Zintgraf et al. ([2020](#bib.bib400))),
    Synthetic Arcs (Noseworthy et al. ([2019](#bib.bib234))), MIME Pouring (Sharma
    et al. ([2018](#bib.bib293))), Craftworld (Devin ([2024](#bib.bib69))), CARLA
    (Dosovitskiy et al. ([2017](#bib.bib78))).'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：基于VAE的IL算法的代表性总结。对于每种算法，我们详细说明了使用的VAE类型、VAE的应用方式以及评估的任务。在第3列中，（1）-（4）指的是第[3.2.1](#S3.SS2.SSS1
    "3.2.1 VAE基础的模仿学习核心方案 ‣ 3.2 模仿学习 ‣ 3 离线策略学习中的变分自编码器 ‣ 深度生成模型在离线策略学习中的教程、调查和未来方向的展望")节中介绍的4种VAE基础的IL方案。至于评估任务，IL算法相比离线RL算法有相对更多的选择（如表[1](#S3.T1
    "表1 ‣ 3.1.4 使用VAE的数据增强和变换 ‣ 3.1 离线强化学习 ‣ 3 离线策略学习中的变分自编码器 ‣ 深度生成模型在离线策略学习中的教程、调查和未来方向的展望")所示）。Real
    Robot或Real-world Navigation指的是在实际平台上的评估，这些平台因研究而异。我们在这里提供这些基准的参考文献：dm_control（Tassa等人（[2018](#bib.bib317)）），Procgen（Cobbe等人（[2020](#bib.bib59)）），MAGICAL（Toyer等人（[2020](#bib.bib321)）），2D
    Navigation/Circle（Lu等人（[2019](#bib.bib203)）），Minecraft（Guss等人（[2019](#bib.bib116)）），Basketball
    Tracking（Felsen等人（[2018](#bib.bib90)）），PEMS-SF Traffic（Dua等人（[2019](#bib.bib81)）），Billiard
    Ball Trajectory（Fragkiadaki等人（[2016](#bib.bib94)）），RoboSuite（Zhu等人（[2020](#bib.bib396)）），PyBullet（Coumans
    & Bai（[2016–2021](#bib.bib63)）），OpenAI Gym/Atari/MuJoCo（Brockman等人（[2016](#bib.bib32)）），3D
    Playroom（Lynch等人（[2019](#bib.bib207)）），AirSim（Shah等人（[2017](#bib.bib289)）），Franka
    Kitchen（Gupta等人（[2019a](#bib.bib114)）），CALVIN（Mees等人（[2022b](#bib.bib219)）），GridWorld（Zintgraf等人（[2020](#bib.bib400)）），Synthetic
    Arcs（Noseworthy等人（[2019](#bib.bib234)）），MIME Pouring（Sharma等人（[2018](#bib.bib293)）），Craftworld（Devin（[2024](#bib.bib69)）），CARLA（Dosovitskiy等人（[2017](#bib.bib78)））。
- en: 3.2.5 Tackling Causal Confusion in Imitation Learning with VAEs
  id: totrans-375
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.5 处理模仿学习中的因果混淆问题，使用变分自编码器（VAEs）
- en: Causal confusion/misidentification happens when learnt imitator policies become
    strongly correlated on certain nuisance variables present in the task environment,
    rather than identifying the causal variables that expert demonstrators actually
    exploited in decision-making. Thus, more information in the environment could
    yield worse IL performance as there would be more nuisance factors. For instance,
    in a driving task, the driver must brake when encountering obstacles or pedestrians.
    If the demonstration images include brake light indicators on the dashboard, imitators
    might incorrectly associate the braking behavior with these indicators. However,
    evaluation scenarios may not always feature these indicators, leading to potentially
    catastrophic outcomes (such as hitting a pedestrian). Therefore, to be maximally
    robust to distribution shifts (between the training and evaluation scenarios),
    a policy must rely solely on true causes of expert actions, thereby avoiding causal
    misidentification.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 因果混淆/误识别发生在当学习到的模仿策略在任务环境中的某些干扰变量上变得强相关时，而不是识别专家演示者在决策中实际利用的因果变量。因此，环境中的更多信息可能会导致更糟的IL性能，因为会有更多的干扰因素。例如，在驾驶任务中，当遇到障碍物或行人时，驾驶员必须刹车。如果演示图像包括仪表盘上的刹车灯指示器，模仿者可能会错误地将刹车行为与这些指示器关联。然而，评估场景可能不总是出现这些指示器，从而导致潜在的灾难性结果（例如撞到行人）。因此，为了在训练和评估场景之间的分布变化中保持最大稳健，策略必须仅依赖于专家行动的真实因果因素，从而避免因果误识别。
- en: 'As a seminal work, RCM-IL (De Haan et al. ([2019](#bib.bib68))) resolves causal
    misidentification in IL by learning a disentangled representation $z\in\mathbb{R}^{d}$
    of the state $s$, each dimension of which represents a disentangled factor of
    variation, and then differentiating between nuisance and actual causal factors
    in $z$. In particular, a $\beta$-VAE is adopted to learn such a disentangled representation
    $z$ following Eq. ([5](#S2.E5 "In 1st item ‣ 2.1 Variational Auto-Encoders ‣ 2
    Background on Deep Generative Models ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) (with $x$
    replaced by $s$). After pretraining the $\beta$-VAE on $D_{E}$, states $s$ in
    $D_{E}$ can be replaced by their corresponding latent embeddings $z$. Each of
    the $d$ dimensions of $z$ could represent either a causal or nuisance factor,
    and it is necessary to mask out the nuisance factors. The problem is then to find
    the correct mask vector $m\in\{0,1\}^{d}$. With $m$, the policy can be defined
    on the masked representations as $\pi(a|m\odot z)$, where $\odot$ denotes element-wise
    multiplication and $m\odot z$ keeps only actual causal factors. De Haan et al.
    ([2019](#bib.bib68)) propose methods for finding the correct $m$ by querying experts
    or interacting with the environment, which does not rely on the use of VAEs and
    so is not detailed here. Masked (Pfrommer et al. ([2023](#bib.bib253))) adopts
    the same protocol as RCM-IL but improves the algorithm for finding the mask vector.
    Specifically, it uses a carefully-designed statistical hypothesis testing algorithm
    to efficiently mask out nuisance variables among the latent dimensions produced
    by the $\beta$-VAE, which does not require expensive queries to the expert or
    environment for intervention. Under certain assumptions, this algorithm is guaranteed
    not to incorrectly mask factors that causally influence the expert. Please see
    (Pfrommer et al. ([2023](#bib.bib253))) for details.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一部开创性的工作，RCM-IL（De Haan等人 ([2019](#bib.bib68)）通过学习状态$s$的解耦表示$z\in\mathbb{R}^{d}$解决了IL中的因果误识别问题，其中$z$的每个维度代表一个解耦的变化因子，然后在$z$中区分干扰因子和实际因果因子。特别是，采用了$\beta$-VAE来学习这种解耦表示$z$，遵循公式
    ([5](#S2.E5 "在第1项 ‣ 2.1 变分自编码器 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望"))（将$x$替换为$s$）。在$D_{E}$上预训练$\beta$-VAE后，$D_{E}$中的状态$s$可以被其对应的潜在嵌入$z$替代。$z$的每个$d$维度可能代表因果因子或干扰因子，因此需要屏蔽掉干扰因子。问题是找到正确的掩码向量$m\in\{0,1\}^{d}$。有了$m$，策略可以在掩码表示上定义为$\pi(a|m\odot
    z)$，其中$\odot$表示逐元素乘法，$m\odot z$仅保留实际的因果因子。De Haan等人 ([2019](#bib.bib68)) 提出了通过查询专家或与环境交互来找到正确的$m$的方法，这些方法不依赖于VAE，因此这里没有详细介绍。Masked（Pfrommer等人
    ([2023](#bib.bib253))) 采用了与RCM-IL相同的协议，但改进了寻找掩码向量的算法。具体来说，它使用了精心设计的统计假设检验算法来高效地屏蔽$\beta$-VAE产生的潜在维度中的干扰变量，这不需要对专家或环境进行昂贵的干预查询。在某些假设下，该算法保证不会错误地屏蔽因果影响专家的因子。有关详细信息，请参见（Pfrommer等人
    ([2023](#bib.bib253))）。
- en: 'Alternatively, Park et al. ([2021](#bib.bib242)) present OREO to ameliorate
    causal misidentification in IL, of which the main idea is to encourage the policy
    to uniformly attend to all semantic factors in the state to prevent it from strongly
    exploiting certain nuisance factors. In particular, a VQ-VAE is trained on states
    from demonstrations using Eq. ([9](#S2.E9 "In 4th item ‣ 2.1 Variational Auto-Encoders
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), where
    $x$ is replaced with $s\sim D_{E}$. Since there could be multiple (say $n$) semantic
    factors within $s$, the latent representations from the VQ-VAE, i.e., $z_{E}$
    and $z_{D}$, are set to be 2D matrices in $\mathbb{R}^{n\times d}$, where $d$
    is the dimension of the representation for each single factor. After pretraining,
    given a state $s$, its multi-factor representation $z_{D}$ can be acquired by
    querying the closet code for each row of $z_{E}$: ($e_{1:k}$ is the codebook,
    $e_{j}\in\mathbb{R}^{d}$, and $z_{E,i}\in\mathbb{R}^{d}$ is the $i$-th row of
    $z_{E}$.)'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，Park 等人（[2021](#bib.bib242)）提出了 OREO 来改善 IL 中的因果误识别，其主要思想是鼓励策略均匀地关注状态中的所有语义因素，以防止策略强烈利用某些干扰因素。具体而言，通过使用公式
    ([9](#S2.E9 "在第4项 ‣ 2.1 变分自编码器 ‣ 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")) 在示例状态上训练
    VQ-VAE，其中 $x$ 替换为 $s\sim D_{E}$。由于 $s$ 中可能包含多个（假设为 $n$）语义因素，因此 VQ-VAE 的潜在表示，即
    $z_{E}$ 和 $z_{D}$，被设定为 $\mathbb{R}^{n\times d}$ 中的 2D 矩阵，其中 $d$ 是每个单一因素表示的维度。经过预训练后，给定一个状态
    $s$，其多因素表示 $z_{D}$ 可以通过查询 $z_{E}$ 每一行的最近编码获得：($e_{1:k}$ 是编码本，$e_{j}\in\mathbb{R}^{d}$，$z_{E,i}\in\mathbb{R}^{d}$
    是 $z_{E}$ 的第 $i$ 行。)
- en: '|  | $\displaystyle z_{E}\sim P_{\phi}(\cdot&#124;s),\ z_{D}=[e_{q(1)}^{T},\cdots,e_{q(n)}^{T}],\
    q(i)=\underset{j\in[1,\cdots,k]}{\operatorname*{arg\,min}}~{}\&#124;z_{E,i}-e_{j}\&#124;_{2}.$
    |  | (56) |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z_{E}\sim P_{\phi}(\cdot|s),\ z_{D}=[e_{q(1)}^{T},\cdots,e_{q(n)}^{T}],\
    q(i)=\underset{j\in[1,\cdots,k]}{\operatorname*{arg\,min}}~{}\|z_{E,i}-e_{j}\|_{2}.$
    |  | (56) |'
- en: 'The next step is to learn a policy through BC based on the multi-factor representation
    while reducing the influence of nuisance factors. Specifically, at each state
    $s$, a sample of $k$ binary random variables $M_{i}\in\{0,1\},i\in[1,\cdots,k]$
    are iid sampled from a Bernoulli distribution, and then a mask vector $m$ corresponding
    to $s$ can be created as: $m=[M_{q(1)},\cdots,M_{q(n)}]$, where $q(1),\cdots,q(n)$
    are defined as above. Finally, the IL objective is: $\max_{\pi,f}\mathbb{E}_{(s,a)\sim
    D_{E},m}\log\pi(a|m\odot f(s))$, where $f$ is initialized with the pretrained
    encoder $P_{\phi}$ and updated during the policy training. Suppose the $q(i)$-th
    factor is a nuisance factor that is strongly correlated with decision-making,
    then it would occur frequently in the latent representations, i.e., $e^{T}_{q(i)}$
    in $z_{D}$. By randomly setting the corresponding mask $M_{q(i)}$ to 0, this nuisance
    factor can be effectively dropped out, thereby mitigating the issue of causal
    misidentification. Randomized dropping provides a simple yet effective way to
    regularize the uniform treatment of each generative factor, i.e., either a causal
    variable that explains expert actions or a nuisance variable that just shows strong
    correlation with the demonstrations.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是通过 BC 基于多因素表示学习策略，同时减少干扰因素的影响。具体而言，在每个状态 $s$ 下，从伯努利分布中独立地抽样 $k$ 个二元随机变量
    $M_{i}\in\{0,1\},i\in[1,\cdots,k]$，然后可以创建一个与 $s$ 对应的掩码向量 $m$：$m=[M_{q(1)},\cdots,M_{q(n)}]$，其中
    $q(1),\cdots,q(n)$ 如上所定义。最后，IL 目标是：$\max_{\pi,f}\mathbb{E}_{(s,a)\sim D_{E},m}\log\pi(a|m\odot
    f(s))$，其中 $f$ 以预训练的编码器 $P_{\phi}$ 初始化，并在策略训练过程中更新。假设 $q(i)$-th 因素是与决策密切相关的干扰因素，那么它在潜在表示中会频繁出现，即
    $z_{D}$ 中的 $e^{T}_{q(i)}$。通过将相应的掩码 $M_{q(i)}$ 随机设置为 0，这个干扰因素可以有效地被去除，从而缓解因果误识别问题。随机丢弃提供了一种简单而有效的方法来正则化对每个生成因素的均匀处理，即，解释专家行为的因果变量或仅与示例有强相关性的干扰变量。
- en: 'The representative VAE-based offline RL and IL algorithms are summarized in
    Table [1](#S3.T1 "Table 1 ‣ 3.1.4 Data Augmentation and Transformation with VAEs
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") and [2](#S3.T2 "Table 2 ‣ 3.2.4
    Skill Acquisition and Hierarchical Imitation Learning through VAEs ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), respectively. We notice that most applications of VAEs in offline
    policy learning are based on the learned embedding $z$. For example, it can be
    used to identify OOD actions (Section [3.1.3](#S3.SS1.SSS3 "3.1.3 Addressing the
    Issue of Out-of-Distribution Actions Using VAEs ‣ 3.1 Offline Reinforcement Learning
    ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    transform high-dimensional states or continuous actions for improved learning
    efficiency (Section [3.1.4](#S3.SS1.SSS4 "3.1.4 Data Augmentation and Transformation
    with VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), represent tasks or
    skills for multi-task or hierarchical learning (Section [3.1.5](#S3.SS1.SSS5 "3.1.5
    Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement Learning
    ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    fuse input from multiple modalities (Section [3.2.3](#S3.SS2.SSS3 "3.2.3 Managing
    Multi-Modal Inputs in Imitation Learning via VAEs ‣ 3.2 Imitation Learning ‣ 3
    Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    provide disentangled representations of the states for tackling causal confusion
    (Section [3.2.5](#S3.SS2.SSS5 "3.2.5 Tackling Causal Confusion in Imitation Learning
    with VAEs ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")), and so on. On the other hand, we note
    that some directions of VAE-based offline policy learning can be further explored.
    For instance, as introduced in Scheme (2) of Section [3.2.1](#S3.SS2.SSS1 "3.2.1
    Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), the
    VAE can be used to extract compact representations of the states, which can then
    be used for learning a representation-based policy $\pi(a|z)$. However, EIRLI
    (Chen et al. ([2021a](#bib.bib41))) suggests that reward- and value-prediction
    would benefit more (than policy learning) from a representation that captures
    mostly coarse-grained visual differences. Thus, it’s worth investigating whether
    VAE-based representation learning would improve offline RL/IL that requires reward-
    or value-predictions. Moreover, RCM-IL and Masked in Section [3.2.5](#S3.SS2.SSS5
    "3.2.5 Tackling Causal Confusion in Imitation Learning with VAEs ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") assume that $\beta$-VAEs can extract well-disentangled representations
    of the states as the basis of mitigating causal confusion. However, as noted in
    Gated VAE (Section [2.1](#S2.SS1 "2.1 Variational Auto-Encoders ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), consistent disentanglement
    with $\beta$-VAEs, such an unsupervised manner, has been demonstrated to be impossible.
    Thus, further improvements can be made regarding tackling causal confusion.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [1](#S3.T1 "Table 1 ‣ 3.1.4 Data Augmentation and Transformation with VAEs
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") 和 [2](#S3.T2 "Table 2 ‣ 3.2.4
    Skill Acquisition and Hierarchical Imitation Learning through VAEs ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") 总结了代表性的基于变分自编码器（VAE）的离线强化学习（RL）和模仿学习（IL）算法。我们注意到，大多数VAE在离线策略学习中的应用都是基于学习到的嵌入
    $z$。例如，它可以用于识别OOD动作（第 [3.1.3](#S3.SS1.SSS3 "3.1.3 Addressing the Issue of Out-of-Distribution
    Actions Using VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") 节），转换高维状态或连续动作以提高学习效率（第
    [3.1.4](#S3.SS1.SSS4 "3.1.4 Data Augmentation and Transformation with VAEs ‣ 3.1
    Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions") 节），表示任务或技能以进行多任务或层次学习（第 [3.1.5](#S3.SS1.SSS5
    "3.1.5 Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") 节），融合来自多个模态的输入（第 [3.2.3](#S3.SS2.SSS3 "3.2.3 Managing Multi-Modal
    Inputs in Imitation Learning via VAEs ‣ 3.2 Imitation Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") 节），提供状态的解耦表示以解决因果混淆（第
    [3.2.5](#S3.SS2.SSS5 "3.2.5 Tackling Causal Confusion in Imitation Learning with
    VAEs ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions") 节）等。另一方面，我们注意到基于VAE的离线策略学习的一些方向还有待进一步探索。例如，如第
    [3.2.1](#S3.SS2.SSS1 "3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2
    Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 节中方案（2）所介绍的，VAE可以用于提取状态的紧凑表示，然后可以用来学习基于表示的策略 $\pi(a|z)$。然而，EIRLI（Chen
    et al. ([2021a](#bib.bib41)））指出，相较于策略学习，奖励和价值预测更有利于从主要捕捉粗粒度视觉差异的表示中获益。因此，值得研究基于VAE的表示学习是否会改善需要奖励或价值预测的离线RL/IL。此外，第
    [3.2.5](#S3.SS2.SSS5 "3.2.5 Tackling Causal Confusion in Imitation Learning with
    VAEs ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions") 节中的RCM-IL和Masked假设 $\beta$-VAE 可以提取良好的解耦状态表示，以缓解因果混淆。然而，如第
    [2.1](#S2.SS1 "2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions") 节中所述，$\beta$-VAE 的一致性解耦以这种无监督的方式已被证明是不可能的。因此，可以进一步改进以解决因果混淆的问题。'
- en: 4 Generative Adversarial Networks in Offline Policy Learning
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 离线策略学习中的生成对抗网络
- en: 'Similar to the previous section, we divide the content here into two parts:
    the applications of GANs in IL (Section [4.1](#S4.SS1 "4.1 Imitation Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) and offline RL (Section [4.2](#S4.SS2 "4.2 Offline Reinforcement
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")). There are relatively more GAN-based IL algorithms compared
    to GAN-based offline RL algorithms. Interestingly, GAN-based IL primarily extends
    two fundamental algorithms: GAIL and AIRL, while GAN-based offline RL focuses
    on expanding the model-based offline RL.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一节类似，我们将这里的内容分为两部分：GAN 在模仿学习中的应用（第 [4.1](#S4.SS1 "4.1 模仿学习 ‣ 4 离线策略学习中的生成对抗网络
    ‣ 离线策略学习中的深度生成模型：教程、综述和未来方向的展望") 节）和离线强化学习（第 [4.2](#S4.SS2 "4.2 离线强化学习 ‣ 4 离线策略学习中的生成对抗网络
    ‣ 离线策略学习中的深度生成模型：教程、综述和未来方向的展望") 节）。相比于基于 GAN 的离线强化学习算法，基于 GAN 的模仿学习算法相对更多。有趣的是，基于
    GAN 的模仿学习主要扩展了两个基础算法：GAIL 和 AIRL，而基于 GAN 的离线强化学习则集中在扩展基于模型的离线强化学习上。
- en: 4.1 Imitation Learning
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 模仿学习
- en: 'In this section, we starts with a detailed introduction of the fundamental
    algorithms in this direction: GAIL and AIRL, as a tutorial, and then present extensions
    for each of them. Finally, we provide a spotlight on the integrated use of GANs
    and VAEs for IL, as examples of synthesizing the use of different generative models
    for offline policy learning.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们首先详细介绍这一方向的基础算法：GAIL 和 AIRL，作为教程，然后介绍它们的扩展。最后，我们重点介绍 GAN 和 VAE 在模仿学习中的综合使用，作为不同生成模型在离线策略学习中的应用实例。
- en: '4.1.1 Fundamental GAN-Based Imitation Learning Algorithms: GAIL and AIRL'
  id: totrans-386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 基础 GAN 模仿学习算法：GAIL 和 AIRL
- en: 'Nearly all works regarding applying GANs for IL can be viewed as extensions
    of two fundamental works in this area: GAIL (Ho & Ermon ([2016](#bib.bib133)))
    and AIRL (Fu et al. ([2017](#bib.bib98))). Both works are based on the maximum
    causal entropy inverse RL (MaxEntIRL) framework (Ziebart et al. ([2010](#bib.bib399))),
    of which the objective is as below:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有关于将生成对抗网络（GANs）应用于模仿学习（IL）的研究都可以看作是这一领域两个基础工作的扩展：GAIL（Ho & Ermon ([2016](#bib.bib133)))
    和 AIRL（Fu et al. ([2017](#bib.bib98)))。这两个工作都基于最大因果熵逆强化学习（MaxEntIRL）框架（Ziebart
    et al. ([2010](#bib.bib399)))，其目标如下：
- en: '|  | $\max_{c}\left[\min_{\pi}-H(\pi)+\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[c(s,a)]\right]-\mathbb{E}_{(s,a)\sim\rho_{\pi_{E}}(\cdot)}[c(s,a)]$
    |  | (57) |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{c}\left[\min_{\pi}-H(\pi)+\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[c(s,a)]\right]-\mathbb{E}_{(s,a)\sim\rho_{\pi_{E}}(\cdot)}[c(s,a)]$
    |  | (57) |'
- en: 'Here, $\rho_{\pi}(s,a)=\pi(a|s)\sum_{t=0}^{\infty}\gamma^{t}P(s_{t}=s|\pi)$
    is the discounted occupancy measure (i.e., visit frequency) of $(s,a)$ when taking
    $\pi$, and $H(\pi)=\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[-\log\pi(a|s)]$ denotes
    the policy entropy. Intuitively, this framework looks for a cost function $c$
    that assigns low costs to the expert policy $\pi_{E}$ and high costs to any other
    policy, thereby allowing the expert policy to be learned via minimizing the expected
    cost, i.e., through the inner RL process shown in Eq. ([57](#S4.E57 "In 4.1.1
    Fundamental GAN-Based Imitation Learning Algorithms: GAIL and AIRL ‣ 4.1 Imitation
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")). $\pi_{E}$ is not directly accessible and usually represented
    as a set of expert demonstrations.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\rho_{\pi}(s,a)=\pi(a|s)\sum_{t=0}^{\infty}\gamma^{t}P(s_{t}=s|\pi)$ 是在采用
    $\pi$ 时 $(s,a)$ 的折扣占用度量（即访问频率），$H(\pi)=\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[-\log\pi(a|s)]$
    表示策略熵。直观地，这一框架寻找一个成本函数 $c$，该函数对专家策略 $\pi_{E}$ 分配低成本，对其他任何策略分配高成本，从而通过最小化期望成本来学习专家策略，即通过
    Eq. ([57](#S4.E57 "在 4.1.1 基础 GAN 模仿学习算法：GAIL 和 AIRL ‣ 4.1 模仿学习 ‣ 4 离线策略学习中的生成对抗网络
    ‣ 离线策略学习中的深度生成模型：教程、综述和未来方向的展望")) 中所示的内部强化学习过程。$\pi_{E}$ 通常不可直接访问，通常表示为一组专家演示。
- en: 'GAIL proposes that, to ensure the expressiveness of $c$ while avoiding overfit
    on the set of demonstrations, $c$ can be any real function (i.e., $\mathbb{R}^{\mathcal{S}\times\mathcal{A}}$)
    but an extra regularizer $\psi:\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\rightarrow\mathbb{R}$
    on $c$ should be introduced to Eq. ([57](#S4.E57 "In 4.1.1 Fundamental GAN-Based
    Imitation Learning Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")). This
    $\psi$-regularized MaxEntIRL problem is equivalent to an occupancy measure matching
    problem:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 'GAIL 提出了，为了确保 $c$ 的表现力，同时避免在示例集上过拟合，$c$ 可以是任何实函数（即，$\mathbb{R}^{\mathcal{S}\times\mathcal{A}}$），但需要在
    $c$ 上引入额外的正则化器 $\psi:\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\rightarrow\mathbb{R}$，并将其加入到方程
    ([57](#S4.E57 "In 4.1.1 Fundamental GAN-Based Imitation Learning Algorithms: GAIL
    and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) 中。这种 $\psi$-正则化的 MaxEntIRL 问题等价于一个占用度量匹配问题：'
- en: '|  | $\min_{\pi}-H(\pi)+\psi^{*}(\rho_{\pi}-\rho_{\pi_{E}}),\ \psi^{*}(\rho_{\pi}-\rho_{\pi_{E}})=\sup_{c\in\mathbb{R}^{\mathcal{S}\times\mathcal{A}}}(\rho_{\pi}-\rho_{\pi_{E}})^{T}c-\psi(c)$
    |  | (58) |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\pi}-H(\pi)+\psi^{*}(\rho_{\pi}-\rho_{\pi_{E}}),\ \psi^{*}(\rho_{\pi}-\rho_{\pi_{E}})=\sup_{c\in\mathbb{R}^{\mathcal{S}\times\mathcal{A}}}(\rho_{\pi}-\rho_{\pi_{E}})^{T}c-\psi(c)$
    |  | (58) |'
- en: 'That is, $\psi$-regularized MaxEntIRL implicitly seeks a policy, whose occupancy
    measure is close to the expert’s, as measured by the convex conjugate of the regularizer,
    i.e., $\psi^{*}$. As introduced in Section [2.2](#S2.SS2 "2.2 Generative Adversarial
    Networks ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    GANs provide a scalable manner for distribution matching. GAIL proposes that,
    with a certain design of $\psi$ (i.e., Eq. (13) in (Ho & Ermon ([2016](#bib.bib133)))),
    Eq. ([58](#S4.E58 "In 4.1.1 Fundamental GAN-Based Imitation Learning Algorithms:
    GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) can be converted to
    a GAN objective:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '也就是说，$\psi$-正则化的 MaxEntIRL 隐式地寻找一个策略，其占用度量接近专家的，占用度量通过正则化器的凸共轭来衡量，即 $\psi^{*}$。如在第
    [2.2](#S2.SS2 "2.2 Generative Adversarial Networks ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions") 节中介绍，GAN 提供了一种可扩展的分布匹配方式。GAIL 提出，通过对 $\psi$
    进行特定设计（即，(Ho & Ermon ([2016](#bib.bib133))) 中的方程 (13)），方程 ([58](#S4.E58 "In 4.1.1
    Fundamental GAN-Based Imitation Learning Algorithms: GAIL and AIRL ‣ 4.1 Imitation
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) 可以转化为 GAN 目标：'
- en: '|  | $\min_{\pi}\max_{D\in(0,1)^{\mathcal{S}\times\mathcal{A}}}-H(\pi)+\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[\log(1-D(s,a))]+\mathbb{E}_{(s,a)\sim\rho_{\pi_{E}}(\cdot)}[\log
    D(s,a)]$ |  | (59) |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\pi}\max_{D\in(0,1)^{\mathcal{S}\times\mathcal{A}}}-H(\pi)+\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[\log(1-D(s,a))]+\mathbb{E}_{(s,a)\sim\rho_{\pi_{E}}(\cdot)}[\log
    D(s,a)]$ |  | (59) |'
- en: 'Compared with Eq. ([57](#S4.E57 "In 4.1.1 Fundamental GAN-Based Imitation Learning
    Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")), we can see
    that $c(s,a)$ is defined with the discriminator as $\log(1-D(s,a))$ ^(14)^(14)14We
    equivalently substitute all $D(s,a)$ in (Ho & Ermon ([2016](#bib.bib133))) as
    $1-D(s,a)$ to be in line with GAN, where the true and fake data should be labeled
    as 1 and 0, respectively, by the discriminator $D$.. Also, the policy $\pi$ works
    as the generator $G$, so $D$ and $\pi$ can be alternatively trained as in GANs
    to optimize Eq. ([59](#S4.E59 "In 4.1.1 Fundamental GAN-Based Imitation Learning
    Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")). Specifically,
    in each training episode, $\pi$ is trained for several iterations to maximize
    the expected return $Q(s_{0},a_{0})=\sum_{t=0}^{\infty}\gamma^{t}(-\log(1-D(s_{t},a_{t}))-\lambda\log\pi(a_{t}|s_{t}))$,
    $\forall(s_{0},a_{0})$, with a widely-adopted RL algorithm – TRPO (Schulman et al.
    ([2015](#bib.bib287))) ^(15)^(15)15$\max_{\pi}Q(s_{0},a_{0})=\sum_{t=0}^{\infty}\gamma^{t}(-\log(1-D(s_{t},a_{t}))-\lambda\log\pi(a_{t}|s_{t}))$
    is equivalent to $\min_{\pi}-\lambda H(\pi)+\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[\log(1-D(s,a))]$.
    $\lambda>0$ is introduced here to balance the two objective terms., and then $D$
    is trained for iterations to correctly classify samples from $\pi$ or $\pi_{E}$
    with Eq. ([59](#S4.E59 "In 4.1.1 Fundamental GAN-Based Imitation Learning Algorithms:
    GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")). Compared with the
    original MaxEntIRL, which requires solving a complete RL problem as the inner
    loop for each update of $c$, GAIL ensures both expressiveness and scalability,
    especially useful for tasks in high-dimensional state/action spaces. Further,
    GAIL provides solid theoretical results on the connection between IRL and GANs.
    As for the global optimality and convergence rate of GAIL, Zhang et al. ([2020b](#bib.bib390))
    provide an analysis under the condition of using two-layer neural networks (with
    ReLU between the two layers) as function estimators and applying natural policy
    gradient for policy updates, when the underlying MDP belongs to the class of linear
    MDPs.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 与公式 ([57](#S4.E57 "在 4.1.1 基于 GAN 的模仿学习算法：GAIL 和 AIRL ‣ 4.1 模仿学习 ‣ 4 离线策略学习中的生成对抗网络
    ‣ 离线策略学习的深度生成模型：教程、调查及未来方向")) 相比，我们可以看到 $c(s,a)$ 是用判别器定义的为 $\log(1-D(s,a))$ ^(14)^(14)14我们等效地将（Ho
    & Ermon ([2016](#bib.bib133))) 中的所有 $D(s,a)$ 替换为 $1-D(s,a)$，以与 GAN 保持一致，其中真实和虚假的数据应分别被判别器
    $D$ 标记为 1 和 0.. 此外，策略 $\pi$ 作为生成器 $G$，因此 $D$ 和 $\pi$ 可以像 GAN 一样交替训练以优化公式 ([59](#S4.E59
    "在 4.1.1 基于 GAN 的模仿学习算法：GAIL 和 AIRL ‣ 4.1 模仿学习 ‣ 4 离线策略学习中的生成对抗网络 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向"))。具体来说，在每个训练周期中，$\pi$
    会被训练若干次以最大化期望回报 $Q(s_{0},a_{0})=\sum_{t=0}^{\infty}\gamma^{t}(-\log(1-D(s_{t},a_{t}))-\lambda\log\pi(a_{t}|s_{t}))$，$\forall(s_{0},a_{0})$，使用广泛采用的
    RL 算法 – TRPO (Schulman et al. ([2015](#bib.bib287))) ^(15)^(15)15$\max_{\pi}Q(s_{0},a_{0})=\sum_{t=0}^{\infty}\gamma^{t}(-\log(1-D(s_{t},a_{t}))-\lambda\log\pi(a_{t}|s_{t}))$
    等同于 $\min_{\pi}-\lambda H(\pi)+\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[\log(1-D(s,a))]$。这里引入了
    $\lambda>0$ 以平衡两个目标项。然后，$D$ 会被训练若干次以正确分类来自 $\pi$ 或 $\pi_{E}$ 的样本，使用公式 ([59](#S4.E59
    "在 4.1.1 基于 GAN 的模仿学习算法：GAIL 和 AIRL ‣ 4.1 模仿学习 ‣ 4 离线策略学习中的生成对抗网络 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向"))。与原始的
    MaxEntIRL 相比，后者需要为每次 $c$ 的更新解决完整的 RL 问题，GAIL 确保了表达能力和可扩展性，特别适用于高维状态/动作空间的任务。此外，GAIL
    提供了关于 IRL 和 GAN 之间关系的可靠理论结果。至于 GAIL 的全局最优性和收敛速度，Zhang et al. ([2020b](#bib.bib390))
    在使用两层神经网络（两层之间使用 ReLU）作为函数估计器并应用自然策略梯度进行策略更新的条件下进行了分析，当底层 MDP 属于线性 MDP 类时。
- en: 'AIRL also adopts a GAN-like framework as follows:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: AIRL 也采用了如下的类似 GAN 的框架：
- en: '|  |  | $\displaystyle\max_{D}\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[\log(1-D(s,a))]+\mathbb{E}_{(s,a)\sim\rho_{\pi_{E}}(\cdot)}[\log
    D(s,a)],$ |  | (60) |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\max_{D}\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[\log(1-D(s,a))]+\mathbb{E}_{(s,a)\sim\rho_{\pi_{E}}(\cdot)}[\log
    D(s,a)],$ |  | (60) |'
- en: '|  |  | $\displaystyle\qquad\quad\min_{\pi}\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[\log(1-D(s,a))-\log
    D(s,a)]$ |  |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\qquad\quad\min_{\pi}\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}[\log(1-D(s,a))-\log
    D(s,a)]$ |  |'
- en: However, instead of applying a sigmoid function (i.e., $D(s,a)=\frac{\exp(f(s,a))}{\exp(f(s,a))+1}$)
    as the activation function on the last layer output of the discriminator, i.e.,
    $f(s,a)$, as in GAN and GAIL, AIRL adopts $D(s,a)=\frac{\exp(f(s,a))}{\exp(f(s,a))+\pi(a|s)}$.
    They claim that, with this design, AIRL is equivalent to MaxEntIRL, under the
    assumption that trajectories $\tau=\{s_{0},a_{0},\cdots,s_{T},a_{T},s_{T+1}\}$
    follow the Boltzmann distribution (i.e., $P(\tau)\propto\rho_{0}(s_{0})\prod_{t=0}^{T-1}\mathcal{T}(s_{t+1}|s_{t},a_{t})\exp(\gamma^{t}r(s_{t},a_{t}))$).
    However, the derivation provided in (Fu et al. ([2017](#bib.bib98))) is not rigorous
    ^(16)^(16)16In Appendix A.1 of (Fu et al. ([2017](#bib.bib98))), the step $\frac{\partial}{\partial\theta}\log
    Z_{\theta}=\mathbb{E}_{p_{\theta}}\left[\sum_{t=0}^{T}\frac{\partial}{\partial\theta}r_{\theta}(s_{t},a_{t})\right]$
    is questionable. Also, in its Appendix A.2, they erroneously mix up the use of
    $\mu_{t}$ and $\hat{\mu}_{t}$.. Further, they propose to replace $D(s,a)$ with
    $D(s,a,s^{\prime})=\frac{\exp(f(s,a,s^{\prime}))}{\exp(f(s,a,s^{\prime}))+\pi(a|s)}$,
    where $f(s,a,s^{\prime})=g(s)+\gamma h(s^{\prime})-h(s)$. They state that, at
    optimality, $g(s)$ and $h(s)$ can recover the reward and value function, respectively,
    if the real reward function is a function of $s$ only and the environment is deterministic
    ^(17)^(17)17In Appendix A.4 of (Fu et al. ([2017](#bib.bib98))), they claim that
    the global minimum of the discriminator objective is reached when $\pi=\pi_{E}$,
    which is backed up by the original GAN paper (Goodfellow et al. ([2014b](#bib.bib107))).
    However, the objective design of AIRL is significantly distinct from the original
    GAN. Moreover, this claim forms the basis of the proof presented in its Appendix
    C on $g(s)$ and $h(s)$ recovering the reward and value functions respectively.
    . While its theoretical foundation is not robust, AIRL can be considered as a
    GAN-based IL algorithm, drawing inspiration from MaxEntIRL and well substantiated
    by empirical results.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与 GAN 和 GAIL 中的做法不同，AIRL 在判别器的最后一层输出 $f(s,a)$ 上应用的不是 sigmoid 函数（即 $D(s,a)=\frac{\exp(f(s,a))}{\exp(f(s,a))+1}$）作为激活函数，而是采用了
    $D(s,a)=\frac{\exp(f(s,a))}{\exp(f(s,a))+\pi(a|s)}$。他们声称，通过这种设计，AIRL 在假设轨迹 $\tau=\{s_{0},a_{0},\cdots,s_{T},a_{T},s_{T+1}\}$
    遵循玻尔兹曼分布（即 $P(\tau)\propto\rho_{0}(s_{0})\prod_{t=0}^{T-1}\mathcal{T}(s_{t+1}|s_{t},a_{t})\exp(\gamma^{t}r(s_{t},a_{t}))$）的前提下，AIRL
    与 MaxEntIRL 等价。然而，(Fu et al. ([2017](#bib.bib98))) 中提供的推导并不严谨 ^(16)^(16)16 在 (Fu
    et al. ([2017](#bib.bib98))) 的附录 A.1 中，$\frac{\partial}{\partial\theta}\log Z_{\theta}=\mathbb{E}_{p_{\theta}}\left[\sum_{t=0}^{T}\frac{\partial}{\partial\theta}r_{\theta}(s_{t},a_{t})\right]$
    的步骤值得怀疑。此外，在附录 A.2 中，他们错误地混淆了 $\mu_{t}$ 和 $\hat{\mu}_{t}$ 的使用。进一步地，他们提议用 $D(s,a,s^{\prime})=\frac{\exp(f(s,a,s^{\prime}))}{\exp(f(s,a,s^{\prime}))+\pi(a|s)}$
    替换 $D(s,a)$，其中 $f(s,a,s^{\prime})=g(s)+\gamma h(s^{\prime})-h(s)$。他们指出，在最优情况下，如果实际奖励函数仅是
    $s$ 的函数且环境是确定性的 ^(17)^(17)17 在 (Fu et al. ([2017](#bib.bib98))) 的附录 A.4 中，他们声称当
    $\pi=\pi_{E}$ 时，判别器目标的全局最小值被达到，这一说法得到了原始 GAN 论文 (Goodfellow et al. ([2014b](#bib.bib107)))
    的支持。然而，AIRL 的目标设计与原始 GAN 有显著不同。此外，这一声明构成了其附录 C 中关于 $g(s)$ 和 $h(s)$ 分别恢复奖励和价值函数证明的基础。虽然其理论基础不够稳健，但
    AIRL 可以被视为一种基于 GAN 的 IL 算法，受到 MaxEntIRL 的启发，并且在经验结果上得到了良好的验证。
- en: There is an extensive amount of works developed based on GAIL or AIRL. Instead
    of providing details on each of them, we put an emphasis on the representative
    ones by presenting their objective designs and key novelties in tables, and briefly
    enumerate other works for comprehensiveness.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 GAIL 或 AIRL 开发了大量的工作。我们没有详细介绍每一项工作，而是通过在表格中展示它们的目标设计和关键创新，重点介绍具有代表性的工作，并简要列举其他工作以求全面。
- en: 4.1.2 Extensions of GAIL
  id: totrans-400
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 GAIL 的扩展
- en: 'In Table [3](#S4.T3 "Table 3 ‣ 4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), we provide an overview of representative works that extend GAIL.
    They either substitute the original GAN framework with more advanced GAN variants
    (as introduced in Section [2.2](#S2.SS2 "2.2 Generative Adversarial Networks ‣
    2 Background on Deep Generative Models ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) or expand
    the use of GAIL to other IL settings, including the multi-agent, multi-task, hierarchical,
    and model-based scenarios. Moreover, to provide a thorough review, we briefly
    enumerate other related research works as follows, which are further categorized
    by their topics.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [3](#S4.T3 "Table 3 ‣ 4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") 中，我们提供了对扩展GAIL的代表性工作的概述。这些工作要么用更先进的GAN变体替代了原始的GAN框架（如第 [2.2](#S2.SS2
    "2.2 Generative Adversarial Networks ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 节中介绍），要么扩展了GAIL在其他IL设置中的应用，包括多智能体、多任务、层次化和基于模型的场景。此外，为了提供全面的回顾，我们简要列举了其他相关研究工作，并按主题进一步分类。'
- en: 'GAIL: Extensions of GAIL concentrate on enhancing its learning efficiency,
    tackling key challenges such as increasing the sample efficiency for policy learning,
    ensuring balanced training of the discriminator and policy, etc. (1) DGAIL (Zuo
    et al. ([2020](#bib.bib402))) and SAM (Blondé & Kalousis ([2019](#bib.bib24)))
    replace the stochastic policy $a\sim\pi(\cdot|s)$ in GAIL with a deterministic
    one $a=\pi(s)$ and, correspondingly, proposes a DDPG-based (Lillicrap et al. ([2016](#bib.bib191)))
    updating rule for $\pi$. As an off-policy algorithm, DDPG is more sample efficient
    than TRPO (used in the original GAN). BGAIL (Jeon et al. ([2018](#bib.bib151)))
    improves the sample efficiency by approximating the discriminator parameters’
    posterior distribution in the ideal case where correct labels are assigned to
    both demonstrated and generated behaviors. A more accurate estimation of the cost
    function can then be acquired through sampling from that distribution, consequently
    improving the policy training. (2) VAIL (Peng et al. ([2019a](#bib.bib250))) suggests
    limiting information flow in the discriminator of GAIL (or AIRL) using the Variational
    Information Bottleneck framework (Tishby & Zaslavsky ([2015](#bib.bib319))), to
    prevent the discriminator from converging too quickly and supplying uninformative
    gradients to the generator. TRAIL (Zolna et al. ([2020](#bib.bib401))) proposes
    a different manner to regularize the discriminator (through an auxiliary task),
    with the aim to discourage the agent to exploit spurious patterns in observations
    which are associated with the expert label but task-irrelevant. (3) For applications
    demanding reliability and robustness, RS-GAIL (Lacotte et al. ([2019](#bib.bib177)))
    presents a risk-sensitive version of GAIL by introducing a constraint to the original
    GAIL objective, such that the conditional value-at-risk (Duffie & Pan ([1997](#bib.bib82)))
    of the learned policy is at least as well as that of the expert. This algorithm
    is supported by rigorous theoretical validation.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 'GAIL: GAIL的扩展集中在提高学习效率，解决关键挑战，例如提高策略学习的样本效率、确保判别器和策略的平衡训练等。(1) DGAIL (Zuo et
    al. ([2020](#bib.bib402))) 和 SAM (Blondé & Kalousis ([2019](#bib.bib24))) 将GAIL中的随机策略
    $a\sim\pi(\cdot|s)$ 替换为确定性策略 $a=\pi(s)$，并相应地提出了基于DDPG (Lillicrap et al. ([2016](#bib.bib191)))
    的更新规则。作为一种离策略算法，DDPG比TRPO（用于原始GAN）具有更高的样本效率。BGAIL (Jeon et al. ([2018](#bib.bib151)))
    通过在理想情况下近似判别器参数的后验分布（即正确标签被分配给展示和生成的行为）来提高样本效率。然后可以通过从该分布中采样获得更准确的成本函数估计，从而改善策略训练。(2)
    VAIL (Peng et al. ([2019a](#bib.bib250))) 建议使用变分信息瓶颈框架 (Tishby & Zaslavsky ([2015](#bib.bib319)))
    限制GAIL（或AIRL）中判别器的信息流，以防止判别器过快收敛并向生成器提供无信息的梯度。TRAIL (Zolna et al. ([2020](#bib.bib401)))
    提出了通过辅助任务对判别器进行正则化的不同方法，旨在阻止代理利用与专家标签相关但与任务无关的观察中出现的虚假模式。(3) 对于要求可靠性和鲁棒性的应用，RS-GAIL
    (Lacotte et al. ([2019](#bib.bib177))) 通过对原始GAIL目标引入约束，提出了一种风险敏感的GAIL版本，以确保学习到的策略的条件价值风险
    (Duffie & Pan ([1997](#bib.bib82))) 至少与专家策略相当。该算法得到了严格的理论验证支持。'
- en: 'Multi-agent/Hierarchical GAIL: IGASIL (Hao et al. ([2019](#bib.bib123))) extends
    GAIL to fully-collaborative multi-agent scenarios. It adopts a similar objective
    design with MAGAIL’s (listed in Table [3](#S4.T3 "Table 3 ‣ 4.1.2 Extensions of
    GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) but suggests two major improvements for
    learning efficiency: training the policy with DDPG for enhanced sample efficiency
    and adopting high-return trajectories from the policy as demonstrations for self-imitation
    learning. Like Option-GAIL introduced in Table [3](#S4.T3 "Table 3 ‣ 4.1.2 Extensions
    of GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), OptionGAN and Directed-Info GAIL
    extend GAIL to the hierarchical learning setting, taking advantage of the Mixture-of-Experts
    framework (Masoudnia & Ebrahimpour ([2014](#bib.bib214))) and directed information
    maximization (Massey et al. ([1990](#bib.bib215))), respectively. However, these
    two algorithms are not suitable for learning from demonstrations segmented by
    sub-tasks. Also, Directed-Info GAIL trains $\pi_{H}$ and $\pi_{L}$ in two separate
    stages, leading to suboptimality of the hierarchical policy $\pi=(\pi_{H},\pi_{L})$.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体/层次化 GAIL：IGASIL (Hao 等 ([2019](#bib.bib123))) 将 GAIL 扩展到完全协作的多智能体场景。它采用类似
    MAGAIL 的目标设计（列在表 [3](#S4.T3 "表 3 ‣ 4.1.2 GAIL 扩展 ‣ 4.1 模仿学习 ‣ 4 离线策略学习中的生成对抗网络
    ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")），但提出了两项主要改进以提高学习效率：使用 DDPG 训练策略以增强样本效率，并采用来自策略的高回报轨迹作为自我模仿学习的演示。与表
    [3](#S4.T3 "表 3 ‣ 4.1.2 GAIL 扩展 ‣ 4.1 模仿学习 ‣ 4 离线策略学习中的生成对抗网络 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")
    中介绍的 Option-GAIL 类似，OptionGAN 和 Directed-Info GAIL 将 GAIL 扩展到层次化学习设置，分别利用专家混合框架
    (Masoudnia & Ebrahimpour ([2014](#bib.bib214))) 和定向信息最大化 (Massey 等 ([1990](#bib.bib215)))。然而，这两种算法不适合从由子任务分段的演示中学习。此外，Directed-Info
    GAIL 在两个独立阶段训练$\pi_{H}$和$\pi_{L}$，导致层次化策略$\pi=(\pi_{H},\pi_{L})$的次优性。 |
- en: 'InfoGAIL: Hausman et al. ([2017](#bib.bib124)) and Peng et al. ([2022](#bib.bib249))
    address the same challenge as InfoGAIL, i.e., IL from multi-modal but unstructured
    demonstrations. Hausman et al. ([2017](#bib.bib124)) achieve the same objective
    function as InfoGAIL but through a distinct perspective; Peng et al. ([2022](#bib.bib249))
    proposes an alternative manner to update $Q$ (in InfoGAIL) through a VAE framework,
    where $Q$ and $\pi$ serve as the encoder and decoder, respectively. Ess-InfoGAIL
    (Fu et al. ([2023](#bib.bib97))) extends InfoGAIL to manage demonstrations from
    a mix of experts, wherein the quantity of demonstrations from each expert is imbalanced.
    Burn-InfoGAIL (Kuefler & Kochenderfer ([2018](#bib.bib174))) improves InfoGAIL
    to reproduce expert behaviors over extended time horizons.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: InfoGAIL：Hausman 等 ([2017](#bib.bib124)) 和 Peng 等 ([2022](#bib.bib249)) 解决了与
    InfoGAIL 相同的挑战，即从多模态但未结构化的演示中进行模仿学习。Hausman 等 ([2017](#bib.bib124)) 实现了与 InfoGAIL
    相同的目标函数，但从不同的角度出发；Peng 等 ([2022](#bib.bib249)) 提出了通过 VAE 框架更新 $Q$（在 InfoGAIL 中）的替代方法，其中
    $Q$ 和 $\pi$ 分别作为编码器和解码器。Ess-InfoGAIL (Fu 等 ([2023](#bib.bib97))) 扩展了 InfoGAIL，以处理来自不同专家的演示，其中每位专家的演示数量不平衡。Burn-InfoGAIL
    (Kuefler & Kochenderfer ([2018](#bib.bib174))) 改进了 InfoGAIL，以在较长时间跨度内重现专家行为。
- en: '| Algo (GAN Type) | Objective | Key Novelty |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 算法 (GAN 类型) | 目标 | 关键创新 |'
- en: '| CGAIL (CGAN) (Zhang et al. ([2019](#bib.bib387))) | $\min_{\pi}\max_{D}-\lambda
    H(\pi(\cdot&#124;c))+\mathbb{E}_{\rho_{\pi_{E}}(\cdot&#124;c)}[\log D(s,a&#124;c)]+$
    $\mathbb{E}_{\rho_{\pi}(\cdot&#124;c)}[\log(1-D(s,a&#124;c))]+$ $\mathbb{E}_{\rho_{\pi_{E}}(\cdot&#124;c)}[\log(1-D(s,a&#124;c^{\prime})]$
    | Recovering a policy $\pi(a&#124;s,c)$ for each condition $c$; Demonstrations
    are shareable among similar task conditions, so the data efficiency can be high;
    $\max_{D}\mathbb{E}_{\rho_{\pi_{E}}(\cdot&#124;c)}[\log(1-D(s,a&#124;c^{\prime})]$
    discourages the mismatch between $c^{\prime}$ and $(s,a)$ for consistency. |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| CGAIL (CGAN) (Zhang 等 ([2019](#bib.bib387))) | $\min_{\pi}\max_{D}-\lambda
    H(\pi(\cdot|c))+\mathbb{E}_{\rho_{\pi_{E}}(\cdot|c)}[\log D(s,a|c)]+$ $\mathbb{E}_{\rho_{\pi}(\cdot|c)}[\log(1-D(s,a|c))]+$
    $\mathbb{E}_{\rho_{\pi_{E}}(\cdot|c)}[\log(1-D(s,a|c^{\prime})]$ | 针对每个条件$c$恢复策略$\pi(a|s,c)$；演示可以在相似任务条件下共享，因此数据效率较高；$\max_{D}\mathbb{E}_{\rho_{\pi_{E}}(\cdot|c)}[\log(1-D(s,a|c^{\prime})]$
    旨在防止$c^{\prime}$与$(s,a)$之间的不匹配，以保持一致性。 |'
- en: '| InfoGAIL (InfoGAN) (Li et al. ([2017b](#bib.bib186))) | $\min_{\pi,Q}\max_{D}-\lambda_{1}H(\pi)+\mathbb{E}_{\rho_{\pi}}[\log(1-D(s,a))]+$
    $\mathbb{E}_{\rho_{\pi_{E}}}[\log D(s,a)]-\lambda_{2}L_{I}(\pi,Q)$, $L_{I}(\pi,Q)=\mathbb{E}_{c\sim
    P_{C}(\cdot),a\sim\pi(\cdot&#124;s,c)}\left[\log Q(c&#124;s,a)\right]+H(C)$, $P_{C}(\cdot)$
    is an assumed prior distribution. | Demonstrations are multi-modal but labels
    $c$ are not provided, unlike CGAIL; $L_{I}(\pi,Q)$ is a lower bound of the mutual
    information $I(c;(s,a))$, by maximizing which we can disentangle trajectories
    corresponding to different $c$; Policies for each latent modality $\pi(a&#124;s,c)$
    can then be recovered. |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| InfoGAIL (InfoGAN) (Li et al. ([2017b](#bib.bib186))) | $\min_{\pi,Q}\max_{D}-\lambda_{1}H(\pi)+\mathbb{E}_{\rho_{\pi}}[\log(1-D(s,a))]+$
    $\mathbb{E}_{\rho_{\pi_{E}}}[\log D(s,a)]-\lambda_{2}L_{I}(\pi,Q)$，$L_{I}(\pi,Q)=\mathbb{E}_{c\sim
    P_{C}(\cdot),a\sim\pi(\cdot&#124;s,c)}\left[\log Q(c&#124;s,a)\right]+H(C)$，$P_{C}(\cdot)$
    是一个假定的先验分布。 | 演示是多模态的，但与 CGAIL 不同的是没有提供标签 $c$；$L_{I}(\pi,Q)$ 是互信息 $I(c;(s,a))$
    的下界，通过最大化它，我们可以解开对应于不同 $c$ 的轨迹；然后可以恢复每个潜在模态的策略 $\pi(a&#124;s,c)$。 |'
- en: '| $f$-GAIL ($f$-GAN) (Zhang et al. ([2020a](#bib.bib388))) | $\min_{\pi}\max_{f^{*}\in\mathcal{F}^{*},T}\mathbb{E}_{\rho_{\pi_{E}}}[T(s,a)]-\mathbb{E}_{\rho_{\pi}}[f^{*}(T(s,a))]$
    $-H(\pi)$, $f^{*}$ is the convex conjugate of $f$ (which defines a certain $f$-divergence)
    and satisfies: convexity and $\inf_{u\in\text{dom}_{f^{*}}}\{f^{*}(u)-u\}=0$.
    Like $\pi$ and $T$, $f^{*}$ is modeled as a neural network but specially designed
    to fulfill its two constraints. | GAIL is a special case of $f$-GAIL, when $T(x)=\log
    D(x),\ f^{*}(x)=-\log(1-e^{x})$; Maximizing over $f^{*}\in\mathcal{F}^{*}$ aims
    to find the largest $f$-divergence between $\rho_{\pi}$ and $\rho_{\pi_{E}}$,
    which can better guide $\pi$ to $\pi_{E}$. |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| $f$-GAIL ($f$-GAN) (Zhang et al. ([2020a](#bib.bib388))) | $\min_{\pi}\max_{f^{*}\in\mathcal{F}^{*},T}\mathbb{E}_{\rho_{\pi_{E}}}[T(s,a)]-\mathbb{E}_{\rho_{\pi}}[f^{*}(T(s,a))]$
    $-H(\pi)$，$f^{*}$ 是 $f$ 的凸共轭（定义了某种 $f$-散度），并满足：凸性和 $\inf_{u\in\text{dom}_{f^{*}}}\{f^{*}(u)-u\}=0$。与
    $\pi$ 和 $T$ 类似，$f^{*}$ 被建模为神经网络，但特别设计以满足其两个约束条件。 | GAIL 是 $f$-GAIL 的一个特例，当 $T(x)=\log
    D(x),\ f^{*}(x)=-\log(1-e^{x})$；在 $f^{*}\in\mathcal{F}^{*}$ 上最大化旨在找到 $\rho_{\pi}$
    和 $\rho_{\pi_{E}}$ 之间最大的 $f$-散度，这可以更好地引导 $\pi$ 到 $\pi_{E}$。 |'
- en: '| Triple-GAIL (Triple-GAN) (Fei et al. ([2020](#bib.bib89))) | $\min_{\pi,C}\max_{D}-\lambda_{1}H(\pi)+\mathbb{E}_{\rho_{\pi_{E}}}[\log
    D(s,a,c)]+$ $\lambda_{2}\mathbb{E}_{\rho_{\pi}}[\log(1-D(s,a,c))]+$ $(1-\lambda_{2})\mathbb{E}_{\rho_{C}}[\log(1-D(s,a,c))]+$
    $\lambda_{4}\mathbb{E}_{\rho_{\pi_{E}}}[-\log C(c&#124;s,a)]+\lambda_{5}\mathbb{E}_{\rho_{\pi}}[-\log
    C(c&#124;s,a)]$; $c$ can be viewed as labels for $(s,a)$, provided in demonstrations;
    $C(c&#124;s,a)$ is utilized to classify $(s,a)$, while $\pi(a&#124;s,c)$ is the
    policy for the class $c$. | The development and theoretical validation of Triple-GAIL
    exactly follow Triple-GAN (see Section [2.2](#S2.SS2 "2.2 Generative Adversarial
    Networks ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")).
    $C$ and $\pi$ are trained by matching the approximated joint occupancy measures
    of $(s,a,c)$: $\rho_{\pi}(\cdot)$ and $\rho_{C}(\cdot)$, with $\rho_{\pi_{E}}(\cdot)$.
    The last two objective terms serve as extra supervision for $C$. Viewing $c$ as
    options/skills, $C$ and $\pi$ then constitute a hierarchical policy. |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| Triple-GAIL (Triple-GAN) (Fei et al. ([2020](#bib.bib89))) | $\min_{\pi,C}\max_{D}-\lambda_{1}H(\pi)+\mathbb{E}_{\rho_{\pi_{E}}}[\log
    D(s,a,c)]+$ $\lambda_{2}\mathbb{E}_{\rho_{\pi}}[\log(1-D(s,a,c))]+$ $(1-\lambda_{2})\mathbb{E}_{\rho_{C}}[\log(1-D(s,a,c))]+$
    $\lambda_{4}\mathbb{E}_{\rho_{\pi_{E}}}[-\log C(c&#124;s,a)]+\lambda_{5}\mathbb{E}_{\rho_{\pi}}[-\log
    C(c&#124;s,a)]$；$c$ 可以视为 $(s,a)$ 的标签，在演示中提供；$C(c&#124;s,a)$ 用于对 $(s,a)$ 进行分类，而
    $\pi(a&#124;s,c)$ 是类 $c$ 的策略。 | Triple-GAIL 的发展和理论验证完全遵循 Triple-GAN（见第 [2.2](#S2.SS2
    "2.2 Generative Adversarial Networks ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 节）。$C$ 和 $\pi$ 通过匹配 $(s,a,c)$ 的近似联合占用度量来训练：$\rho_{\pi}(\cdot)$
    和 $\rho_{C}(\cdot)$，与 $\rho_{\pi_{E}}(\cdot)$。最后两个目标项作为对 $C$ 的额外监督。将 $c$ 视为选项/技能，$C$
    和 $\pi$ 形成了一个层次化的策略。 |'
- en: '| MGAIL (GAN) (Baram et al. ([2017](#bib.bib19))) | The objective of $D$ is
    the same as the one of GAIL. $\pi(a&#124;s;\theta)$ is trained by maximizing the
    return $J(s_{0},a_{0};\theta)$, where the policy gradient, i.e., $J^{0}_{\theta}$,
    can be approximated recursively as: ($t=T\rightarrow 0$) $J_{\theta}^{t}=R_{a}\pi_{\theta}+\gamma(J_{\theta}^{t+1}+J_{s^{\prime}}^{t+1}f_{a}\pi_{\theta})$,
    $J_{s}^{t}=R_{s}+R_{a}\pi_{s}+\gamma J^{t+1}_{s^{\prime}}(f_{s}+f_{a}\pi_{s})$;
    $f$ is the learned dynamics: $s^{\prime}=f(s,a)$, $R(s,a)$ is the reward defined
    with $D$, $R_{s}\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}\frac{\partial
    R}{\partial s}$ and so on. | This is a model-based version of GAIL. A dynamic
    model $f$ is learned, such that the agent can look one-step ahead to additionally
    adopt the gradient from $f$, i.e., $\gamma J_{s^{\prime}}^{t+1}f_{a}\pi_{\theta}$,
    as part of $J_{\theta}^{t}$. (Typically, $J_{\theta}^{t}=R_{a}\pi_{\theta}+\gamma
    J_{\theta}^{t+1}$.) Thus, fewer expert data and policy samples would be required,
    compared with GAIL. |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| MGAIL (GAN) (Baram et al. ([2017](#bib.bib19))) | $D$的目标与GAIL相同。通过最大化返回$J(s_{0},a_{0};\theta)$来训练$\pi(a|s;\theta)$，其中策略梯度，即$J^{0}_{\theta}$，可以递归地近似为：（$t=T\rightarrow
    0$）$J_{\theta}^{t}=R_{a}\pi_{\theta}+\gamma(J_{\theta}^{t+1}+J_{s^{\prime}}^{t+1}f_{a}\pi_{\theta})$，$J_{s}^{t}=R_{s}+R_{a}\pi_{s}+\gamma
    J^{t+1}_{s^{\prime}}(f_{s}+f_{a}\pi_{s})$；$f$是学习得到的动态：$s^{\prime}=f(s,a)$，$R(s,a)$是用$D$定义的奖励，$R_{s}\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}\frac{\partial
    R}{\partial s}$等。 | 这是GAIL的基于模型的版本。学习一个动态模型$f$，使得智能体可以向前看一步，以额外采用来自$f$的梯度，即$\gamma
    J_{s^{\prime}}^{t+1}f_{a}\pi_{\theta}$，作为$J_{\theta}^{t}$的一部分。（通常，$J_{\theta}^{t}=R_{a}\pi_{\theta}+\gamma
    J_{\theta}^{t+1}$。）因此，与GAIL相比，需要的专家数据和策略样本更少。'
- en: '| MAGAIL (GAN) (Song et al. ([2018](#bib.bib302))) | $\min_{\pi}\max_{D}\mathbb{E}_{\rho_{\pi}}\left[\sum_{i=1}^{N}\log(1-D_{i}(s,a_{i}))\right]+$
    $\mathbb{E}_{\rho_{\pi_{E}}}\left[\sum_{i=1}^{N}\log D_{i}(s,a_{i})\right]$, $\pi=(\pi_{1},\cdots,\pi_{N})$,
    $\pi_{E}=(\pi_{E}^{1},\cdots,\pi_{E}^{N})$. All agents share the state $s$ but
    choose their own action $a_{i}\sim\pi_{i}(\cdot&#124;s)$. The other agents $-i$
    can be viewed as part of the environment for agent $i$. However, if $\pi_{-i}$
    is unknown, the training environment would be non-stationary for $i$, as $\pi_{-i}$
    are also being updated. | A multi-agent extension of GAIL, which learns a $D_{i}$
    and $\pi_{i}$ for each agent. As agents interact with each other, $D_{i}$ and
    $\pi_{i}$ are not learned independently. The paper’s theoretical results assume
    $\pi_{E}^{-i}$ is known, enabling the learning for each agent $i$ to be treated
    separately. Yet, in practice, better management for the non-stationarity than
    MAGAIL is required. |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| MAGAIL (GAN) (Song et al. ([2018](#bib.bib302))) | $\min_{\pi}\max_{D}\mathbb{E}_{\rho_{\pi}}\left[\sum_{i=1}^{N}\log(1-D_{i}(s,a_{i}))\right]+$
    $\mathbb{E}_{\rho_{\pi_{E}}}\left[\sum_{i=1}^{N}\log D_{i}(s,a_{i})\right]$, $\pi=(\pi_{1},\cdots,\pi_{N})$,
    $\pi_{E}=(\pi_{E}^{1},\cdots,\pi_{E}^{N})$。所有智能体共享状态$s$，但选择各自的动作$a_{i}\sim\pi_{i}(\cdot|s)$。其他智能体$-i$可以视作智能体$i$的环境的一部分。然而，如果$\pi_{-i}$未知，那么训练环境对于$i$来说是非平稳的，因为$\pi_{-i}$也在更新中。
    | MAGAIL是GAIL的多智能体扩展，为每个智能体学习一个$D_{i}$和$\pi_{i}$。由于智能体之间相互作用，$D_{i}$和$\pi_{i}$并不是独立学习的。论文的理论结果假设$\pi_{E}^{-i}$已知，这使得每个智能体$i$的学习可以被单独处理。然而，在实际操作中，比MAGAIL更好的非平稳性管理是必要的。
    |'
- en: '| Option-GAIL (GAN) (Jing et al. ([2021](#bib.bib153))) | $\min_{\pi}\max_{D}-\lambda
    H(\pi)+\mathbb{E}_{\rho_{\pi}}\left[\log(1-D(o^{\prime},s,o,a))\right]$ $+\mathbb{E}_{\rho_{\pi_{E}}}\left[\log
    D(o^{\prime},s,o,a)\right]$, $\pi=(\pi_{H},\pi_{L})$, $o\sim\pi_{H}(\cdot&#124;s,o^{\prime})$,
    $a\sim\pi_{L}(\cdot&#124;s,o)$. The agent decides on its current option (a.k.a.,
    skill) $o$ with $\pi_{H}$ based on $s$ and $o^{\prime}$ (i.e., the previous option),
    and then samples actions $a$ with $\pi_{L}$ subject to $o$. | A hierarchical learning
    version of GAIL. Long-horizon tasks can be segmented to a sequence of sub-tasks,
    each of which can be done with an option $o$ (i.e., sub-policy). This algorithm
    can recover a hierarchical policy, useful for long-horizon tasks, from the expert
    data through matching the occupancy measure of $(o^{\prime},s,o,a)$ between $\pi$
    and $\pi_{E}$, which follows GAIL. An EM version of Option-GAIL is proposed, in
    case the expert’s options are not labeled. |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| Option-GAIL（GAN）（Jing 等人 ([2021](#bib.bib153))) | $\min_{\pi}\max_{D}-\lambda
    H(\pi)+\mathbb{E}_{\rho_{\pi}}\left[\log(1-D(o^{\prime},s,o,a))\right]$ $+\mathbb{E}_{\rho_{\pi_{E}}}\left[\log
    D(o^{\prime},s,o,a)\right]$, $\pi=(\pi_{H},\pi_{L})$，$o\sim\pi_{H}(\cdot&#124;s,o^{\prime})$，$a\sim\pi_{L}(\cdot&#124;s,o)$。代理通过
    $\pi_{H}$ 根据 $s$ 和 $o^{\prime}$（即，之前的选项）决定其当前选项（即技能）$o$，然后通过 $\pi_{L}$ 在 $o$ 的约束下采样动作
    $a$。 | GAIL 的层次学习版本。长时域任务可以被分割为一系列子任务，每个子任务可以通过选项 $o$（即子策略）完成。该算法可以从专家数据中恢复一个层次策略，这对于长时域任务非常有用，通过匹配
    $(o^{\prime},s,o,a)$ 的占用度量在 $\pi$ 和 $\pi_{E}$ 之间，这遵循了 GAIL。提出了一种 Option-GAIL 的
    EM 版本，以防专家的选项没有标记。'
- en: 'Table 3: Summary of representative works following GAIL. In Column 1, we list
    the algorithms and the type of GANs they utilize. Their key objectives and novelties
    are listed in Column 2 and 3, respectively.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：遵循 GAIL 的代表性工作的总结。在第 1 列中，我们列出了算法及其所使用的 GAN 类型。它们的关键目标和创新分别列在第 2 列和第 3 列中。
- en: '| Algo (GAN Type) | Objective | Key Novelty |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 算法（GAN 类型） | 目标 | 关键创新 |'
- en: '| $f$-IRL (GAN) (Ni et al. ([2020](#bib.bib231))) | $\max_{D}\mathbb{E}_{s\sim\rho_{\pi_{E}}(\cdot)}\left[\log
    D(s)\right]+\mathbb{E}_{s\sim\rho_{\theta}(\cdot)}\left[\log(1-D(s))\right]$,
    $\min_{\theta}L_{f}(\theta)$, $L_{f}(\theta)=D_{f}(\rho_{\pi_{E}}(s)&#124;&#124;\rho_{\theta}(s))$,
    $\rho_{\theta}(s)\propto$ $\int\rho_{0}(s_{0})\prod_{t=0}^{T-1}\mathcal{T}(s_{t+1}&#124;s_{t},a_{t})\exp(\frac{r_{\theta}(s_{t+1})}{\alpha})\eta_{\tau}(s)d\tau$,
    $\eta_{\tau}(s)=\sum_{t=1}^{T}\mathbbm{1}(s_{t}=s)$; The training alternates between
    $\max_{D}$ and $\min_{\theta}$; With the learned reward function $r_{\theta}$,
    $\pi$ is trained by RL. $\min_{\theta}L_{f}(\theta)$ is realized by gradient descents:
    $\nabla_{\theta}L_{f}(\theta)=$ $\frac{1}{\alpha T}\text{cov}_{\tau\sim\rho_{\theta}(\cdot)}\left(\sum_{t=1}^{T}f(u_{t})-f^{\prime}(u_{t})u_{t},\sum_{t=1}^{T}\nabla_{\theta}r_{\theta}(s_{t})\right)$,
    $u_{t}=\frac{\rho_{\pi_{E}}(s_{t})}{\rho_{\theta}(s_{t})}\approx\frac{D(s_{t})}{1-D(s_{t})}$,
    $\text{cov}(\cdot)$ denotes covariance. | This work provides a generalization
    of AIRL by applying the the general $f$-divergence. The reward $r_{\theta}(s)$
    is trained by state marginal matching, i.e., $\min_{\theta}D_{f}(\rho_{\pi_{E}}(s)&#124;&#124;\rho_{\theta}(s))$.
    $D$ is used to estimate $u_{t}$ in $\nabla_{\theta}L_{f}(\theta)$, based on the
    fact: $D^{*}(s)=\rho_{\pi_{E}}(s)/(\rho_{\pi_{E}}(s)+\rho_{\theta}(s))$. So, each
    update of $\theta$ requires a near- optimal $D^{*}$, which is time-consuming.
    |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| $f$-IRL（GAN）（Ni 等人 ([2020](#bib.bib231))) | $\max_{D}\mathbb{E}_{s\sim\rho_{\pi_{E}}(\cdot)}\left[\log
    D(s)\right]+\mathbb{E}_{s\sim\rho_{\theta}(\cdot)}\left[\log(1-D(s))\right]$,
    $\min_{\theta}L_{f}(\theta)$, $L_{f}(\theta)=D_{f}(\rho_{\pi_{E}}(s)&#124;&#124;\rho_{\theta}(s))$,
    $\rho_{\theta}(s)\propto$ $\int\rho_{0}(s_{0})\prod_{t=0}^{T-1}\mathcal{T}(s_{t+1}&#124;s_{t},a_{t})\exp(\frac{r_{\theta}(s_{t+1})}{\alpha})\eta_{\tau}(s)d\tau$,
    $\eta_{\tau}(s)=\sum_{t=1}^{T}\mathbbm{1}(s_{t}=s)$；训练过程在 $\max_{D}$ 和 $\min_{\theta}$
    之间交替进行；利用学习到的奖励函数 $r_{\theta}$，通过强化学习（RL）训练 $\pi$。$\min_{\theta}L_{f}(\theta)$
    通过梯度下降实现：$\nabla_{\theta}L_{f}(\theta)=$ $\frac{1}{\alpha T}\text{cov}_{\tau\sim\rho_{\theta}(\cdot)}\left(\sum_{t=1}^{T}f(u_{t})-f^{\prime}(u_{t})u_{t},\sum_{t=1}^{T}\nabla_{\theta}r_{\theta}(s_{t})\right)$,
    $u_{t}=\frac{\rho_{\pi_{E}}(s_{t})}{\rho_{\theta}(s_{t})}\approx\frac{D(s_{t})}{1-D(s_{t})}$，$\text{cov}(\cdot)$
    表示协方差。 | 本工作通过应用一般的 $f$-散度提供了 AIRL 的推广。奖励 $r_{\theta}(s)$ 通过状态边际匹配进行训练，即 $\min_{\theta}D_{f}(\rho_{\pi_{E}}(s)&#124;&#124;\rho_{\theta}(s))$。$D$
    被用来估计 $\nabla_{\theta}L_{f}(\theta)$ 中的 $u_{t}$，基于以下事实：$D^{*}(s)=\rho_{\pi_{E}}(s)/(\rho_{\pi_{E}}(s)+\rho_{\theta}(s))$。因此，每次
    $\theta$ 的更新都需要一个接近最优的 $D^{*}$，这是非常耗时的。 |'
- en: '| MA-AIRL (GAN) (Yu et al. ([2019a](#bib.bib377))) | $\max_{D}\sum_{i=1}^{N}\mathbb{E}_{\rho_{\pi_{E}}}\left[\log
    D_{i}(s,a)\right]+\mathbb{E}_{\rho_{\pi}}\left[\log(1-D_{i}(s,a))\right]$, $\min_{\pi_{i}}\mathbb{E}_{\rho_{\pi}}\left[\log(1-D_{i}(s,a))-\log
    D_{i}(s,a)\right],i=1,\cdots,N$; $D_{i}(s,a)=\exp(f(s,a))/(\exp(f(s,a))+\pi_{i}(a_{i}&#124;s))$,
    $a=(a_{1},\cdots,a_{N})$, $\pi=(\pi_{1},\cdots,\pi_{N})$. All agents share a state
    $s$ and have access to action decisions from the other agents, i.e., $a_{-i}$,
    during the training process. | This is a multi-agent version of AIRL, which trains
    $D_{i}$, $\pi_{i}$ and applies AIRL to each agent. States and joint actions are
    shared among agents. Theoretical results are based on Markov games and logistic
    stochastic best response equilibirum. However, viewing $a_{-i}$ as part of the
    environment, the problem and derivations degenerate to the single-agent AIRL case.
    |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| MA-AIRL (GAN) (Yu 等 ([2019a](#bib.bib377))) | $\max_{D}\sum_{i=1}^{N}\mathbb{E}_{\rho_{\pi_{E}}}\left[\log
    D_{i}(s,a)\right]+\mathbb{E}_{\rho_{\pi}}\left[\log(1-D_{i}(s,a))\right]$, $\min_{\pi_{i}}\mathbb{E}_{\rho_{\pi}}\left[\log(1-D_{i}(s,a))-\log
    D_{i}(s,a)\right],i=1,\cdots,N$; $D_{i}(s,a)=\exp(f(s,a))/(\exp(f(s,a))+\pi_{i}(a_{i}&#124;s))$,
    $a=(a_{1},\cdots,a_{N})$, $\pi=(\pi_{1},\cdots,\pi_{N})$。所有智能体共享一个状态 $s$ 并在训练过程中可以访问其他智能体的动作决策，即
    $a_{-i}$。 | 这是 AIRL 的多智能体版本，它训练 $D_{i}$、$\pi_{i}$ 并将 AIRL 应用于每个智能体。状态和联合动作在智能体之间共享。理论结果基于马尔可夫博弈和逻辑随机最佳响应均衡。然而，将
    $a_{-i}$ 视为环境的一部分时，问题和推导退化为单智能体 AIRL 的情况。'
- en: '| MH-AIRL (GAN) (Chen et al. ([2023d](#bib.bib45))) | $\max_{D}\mathbb{E}_{\pi_{E}}\sum_{t=0}^{T-1}\log
    D(\tilde{s}_{t},\tilde{a}_{t}&#124;c)+\mathbb{E}_{\pi}\sum_{t=0}^{T-1}$ $\log(1-D(\tilde{s}_{t},\tilde{a}_{t}&#124;c))$,
    $(\tilde{s}_{t},\tilde{a}_{t})=((o_{t-1},s_{t}),(o_{t},a_{t}))$; $\min_{\pi}\mathbb{E}_{\pi}\left[\sum_{t=0}^{T-1}\log(1-D_{t}^{c})-\log
    D_{t}^{c}\right]-$ $\lambda_{1}I(\tau_{\pi};C)-\lambda_{2}I(\tau_{\pi}\rightarrow
    O_{0:T}&#124;C)$, $\tau_{\pi}=\{(s_{t},a_{t})\}_{t=0:T-1}$, $D_{t}^{c}=\frac{\exp(f(\tilde{s}_{t},\tilde{a}_{t}&#124;c))}{\exp(f(\tilde{s}_{t},\tilde{a}_{t}&#124;c))+\pi(\tilde{a}_{t}&#124;\tilde{s}_{t},c)}$,
    $\pi=(\pi_{H},\pi_{L})$ and $\pi(\tilde{a}_{t}&#124;\tilde{s}_{t},c)=\pi_{H}(o_{t}&#124;s_{t},o_{t-1})\pi_{L}(a_{t}&#124;s_{t},o_{t})$.
    $c$ and $o$ denote the task and option variable, respectively. MH-AIRL can also
    be adopted, when $c$ and $o$ are not labeled in demonstartions, via an Expectation–Maximization
    (EM) design. | This algorithm integrates multi-task learning, hierarchical learning,
    and AIRL. $\pi$ is a hierarchical policy that can be applied to multiple tasks
    by conditioning on corresponding $c$. The objective design can be viewed as AIRL
    on the extended state-action space $(\tilde{s}_{t},\tilde{a}_{t})$. As regularization,
    $\pi$ is trained to maximize the mutual /directed information $I(\tau_{\pi};C)$
    /$I(\tau_{\pi}\rightarrow O_{0:T}&#124;C)$ to build the causal relationship between
    $\pi$ and the task /option variables. |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| MH-AIRL (GAN) (Chen 等 ([2023d](#bib.bib45))) | $\max_{D}\mathbb{E}_{\pi_{E}}\sum_{t=0}^{T-1}\log
    D(\tilde{s}_{t},\tilde{a}_{t}&#124;c)+\mathbb{E}_{\pi}\sum_{t=0}^{T-1}$ $\log(1-D(\tilde{s}_{t},\tilde{a}_{t}&#124;c))$,
    $(\tilde{s}_{t},\tilde{a}_{t})=((o_{t-1},s_{t}),(o_{t},a_{t}))$; $\min_{\pi}\mathbb{E}_{\pi}\left[\sum_{t=0}^{T-1}\log(1-D_{t}^{c})-\log
    D_{t}^{c}\right]-$ $\lambda_{1}I(\tau_{\pi};C)-\lambda_{2}I(\tau_{\pi}\rightarrow
    O_{0:T}&#124;C)$, $\tau_{\pi}=\{(s_{t},a_{t})\}_{t=0:T-1}$, $D_{t}^{c}=\frac{\exp(f(\tilde{s}_{t},\tilde{a}_{t}&#124;c))}{\exp(f(\tilde{s}_{t},\tilde{a}_{t}&#124;c))+\pi(\tilde{a}_{t}&#124;\tilde{s}_{t},c)}$,
    $\pi=(\pi_{H},\pi_{L})$ 和 $\pi(\tilde{a}_{t}&#124;\tilde{s}_{t},c)=\pi_{H}(o_{t}&#124;s_{t},o_{t-1})\pi_{L}(a_{t}&#124;s_{t},o_{t})$。$c$
    和 $o$ 分别表示任务和选项变量。当 $c$ 和 $o$ 在演示中未被标记时，MH-AIRL 也可以通过期望最大化（EM）设计来采用。 | 该算法集成了多任务学习、层次学习和
    AIRL。 $\pi$ 是一个层次化策略，通过条件化于相应的 $c$，可以应用于多个任务。目标设计可以被视为在扩展的状态-动作空间 $(\tilde{s}_{t},\tilde{a}_{t})$
    上进行的 AIRL。作为正则化，$\pi$ 被训练以最大化互信息 /定向信息 $I(\tau_{\pi};C)$ /$I(\tau_{\pi}\rightarrow
    O_{0:T}&#124;C)$，以建立 $\pi$ 和任务 /选项变量之间的因果关系。'
- en: 'Table 4: Summary of representative works following AIRL.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：基于 AIRL 的代表性工作的总结。
- en: 4.1.3 Extensions of AIRL
  id: totrans-419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 AIRL 的扩展
- en: 'Extensions of AIRL are fewer than those of GAIL, but we follow the content
    structure in Section [4.1.2](#S4.SS1.SSS2 "4.1.2 Extensions of GAIL ‣ 4.1 Imitation
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), providing a summarization table as Table [4](#S4.T4 "Table
    4 ‣ 4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions") and offering
    a comprehensive review as below.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 'AIRL 的扩展比 GAIL 少，但我们遵循第 [4.1.2](#S4.SS1.SSS2 "4.1.2 Extensions of GAIL ‣ 4.1
    Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 节的内容结构，提供了如表 [4](#S4.T4 "Table 4 ‣ 4.1.2 Extensions of
    GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions") 所示的总结表，并提供了如下的全面综述。'
- en: 'AIRL adopts the function design $f(s,a,s^{\prime})=g(s)+\gamma h(s^{\prime})-h(s)$,
    where $g(s)$ is assumed to recover the environment reward function and $h(s)$
    can be viewed as the potential function for reward shaping (Ng et al. ([1999](#bib.bib229))).
    EAIRL (Qureshi et al. ([2019](#bib.bib263))) proposes to implement $h(s)$ as the
    empowerment function (Mohamed & Rezende ([2015](#bib.bib225))), maximizing which
    would induce an intrinsic motivation for the agent to seek the states that have
    the highest number of future reachable states. However, as claimed in AIRL, $h(s)$
    is designed to recover the value function at optimality, which is different from
    the empowerment function in definition. AIRL has been extended to various IL setups,
    including model-based, off-policy, multi-task, and hierarchical IL. MAIRL (Sun
    et al. ([2021](#bib.bib313))) simply replaces the GAIL objective in MGAIL (introduced
    in Table [3](#S4.T3 "Table 3 ‣ 4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) with the AIRL’s and gets a model-based version of AIRL. Following
    the same intuition as DGAIL and SAM (introduced in Section [4.1.2](#S4.SS1.SSS2
    "4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")), Off-policy-AIRL
    suggests adopting off-policy RL algorithms (specially, SAC (Haarnoja et al. ([2018](#bib.bib117))))
    to train the generator $\pi$ in AIRL for improved sample efficiency. Table [4](#S4.T4
    "Table 4 ‣ 4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions") introduces
    MH-AIRL, which extends AIRL to support both multi-task and hierarchical learning.
    As related works, PEMIRL (Yu et al. ([2019b](#bib.bib378))) and SMILe (Ghasemipour
    et al. ([2019a](#bib.bib102))) are proposed for multi-task AIRL, while oIRL (Venuto
    et al. ([2020](#bib.bib330))) and H-AIRL (Chen et al. ([2023c](#bib.bib44))) focus
    on hierarchical AIRL.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 'AIRL 采用函数设计 $f(s,a,s^{\prime})=g(s)+\gamma h(s^{\prime})-h(s)$，其中 $g(s)$ 被假设为恢复环境奖励函数，而
    $h(s)$ 可以视为奖励塑形的潜在函数（Ng 等人 ([1999](#bib.bib229)））。EAIRL（Qureshi 等人 ([2019](#bib.bib263)））提出将
    $h(s)$ 实现为赋权函数（Mohamed & Rezende ([2015](#bib.bib225)）），最大化该函数将引导代理寻求未来可达状态数量最多的状态，从而激发内在动机。然而，正如
    AIRL 所声称的，$h(s)$ 被设计为在最优性下恢复价值函数，这在定义上不同于赋权函数。AIRL 已被扩展到各种模仿学习设置，包括基于模型的、离策略的、多任务的和层次化的模仿学习。MAIRL（Sun
    等人 ([2021](#bib.bib313)））简单地用 AIRL 的目标替换了 MGAIL 中的 GAIL 目标（在表 [3](#S4.T3 "Table
    3 ‣ 4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions") 介绍），并得到了一个基于模型的
    AIRL 版本。按照 DGAIL 和 SAM 的相同直觉（在第 [4.1.2 节](#S4.SS1.SSS2 "4.1.2 Extensions of GAIL
    ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions") 介绍），离策略-AIRL 建议采用离策略 RL 算法（特别是 SAC（Haarnoja
    等人 ([2018](#bib.bib117)）））来训练 AIRL 中的生成器 $\pi$ 以提高样本效率。表 [4](#S4.T4 "Table 4 ‣
    4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") 介绍了 MH-AIRL，它将 AIRL
    扩展以支持多任务和层次化学习。作为相关工作，PEMIRL（Yu 等人 ([2019b](#bib.bib378)））和 SMILe（Ghasemipour
    等人 ([2019a](#bib.bib102)））被提出用于多任务 AIRL，而 oIRL（Venuto 等人 ([2020](#bib.bib330)））和
    H-AIRL（Chen 等人 ([2023c](#bib.bib44)））则专注于层次化 AIRL。'
- en: '4.1.4 Integrating VAEs and GANs for Imitation Learning: A Spotlight'
  id: totrans-422
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 将变分自编码器（VAEs）和生成对抗网络（GANs）结合用于模仿学习：聚焦点
- en: 'Lastly, we spotlight some papers that utilize VAEs to enhance GAIL, serving
    as examples of integrating different generative models for advanced imitation
    learning. GANs are recognized for their capacity to generate sharp image samples,
    as opposed to the blurrier samples from VAE models. However, GANs, unlike VAEs,
    are susceptible to the mode collapse problem, meaning that they may only capture
    partial modes in a multi-modal dataset. Therefore, the capabilities of GANs and
    VAEs are highly complementary. As mentioned in Table [3](#S4.T3 "Table 3 ‣ 4.1.2
    Extensions of GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), one approach, such
    as CGAIL and InfoGAIL, for solving the mode collapse issue is to train a discriminator
    and policy for each mode by conditioning them on the mode embedding. Such embeddings
    could be acquired from a pretrained VAE. In particular, a VAE $(P_{\phi},P_{\theta})$
    can be trained to imitate expert trajectories $\tau=(s_{0},a_{0},\cdots,s_{T})\sim
    D_{E}$ through an ELBO objective:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们重点介绍了一些利用VAE来增强GAIL的论文，作为整合不同生成模型以实现先进模仿学习的示例。GAN以其生成清晰图像样本的能力而受到认可，而VAE模型则生成较模糊的样本。然而，与VAE不同，GAN易受到模式崩溃问题的影响，这意味着它们可能只能捕捉多模态数据集中部分模式。因此，GAN和VAE的能力是高度互补的。如表[3](#S4.T3
    "Table 3 ‣ 4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")所述，解决模式崩溃问题的一种方法是通过对模式嵌入进行条件训练来训练每个模式的鉴别器和策略。这些嵌入可以从预训练的VAE中获取。特别是，可以通过ELBO目标来训练VAE
    $(P_{\phi},P_{\theta})$以模仿专家轨迹 $\tau=(s_{0},a_{0},\cdots,s_{T})\sim D_{E}$：'
- en: '|  | $\max_{\theta,\phi}\mathbb{E}_{\tau\sim D_{E}}\left[\mathbb{E}_{c\sim
    P_{\phi}(\cdot&#124;\tau)}\left[\log P_{\theta}(\tau&#124;c)\right]-D_{KL}(P_{\phi}(c&#124;\tau)&#124;&#124;P_{C}(c))\right]$
    |  | (61) |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\theta,\phi}\mathbb{E}_{\tau\sim D_{E}}\left[\mathbb{E}_{c\sim
    P_{\phi}(\cdot&#124;\tau)}\left[\log P_{\theta}(\tau&#124;c)\right]-D_{KL}(P_{\phi}(c&#124;\tau)&#124;&#124;P_{C}(c))\right]$
    |  | (61) |'
- en: 'where $P_{C}(c)$ is the predefined prior distribution, $P_{\phi}(c|\tau)$ and
    $P_{\theta}(\tau|c)$ work as the encoder and decoder of the VAE. $\log P_{\theta}(\tau|c)$
    can be decomposed as $\log\rho_{\theta}(s_{0}|c)+\sum_{t=0}^{T-1}\left[\log\pi_{\theta}(a_{t}|s_{t},c)+\log\mathcal{T}_{\theta}(s_{t+1}|s_{t},a_{t},c)\right]$
    based on the MDP model. Through this unsupervised training process, the VAE is
    expected to capture the multiple modes in the dataset and its encoder $P_{\phi}(c|\tau)$
    can be employed to provide mode embeddings for demonstration trajectories. Then,
    the mode-conditioned discriminator and policy can be trained via an objective
    similar with CGAIL:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P_{C}(c)$ 是预定义的先验分布，$P_{\phi}(c|\tau)$ 和 $P_{\theta}(\tau|c)$ 分别作为VAE的编码器和解码器。$\log
    P_{\theta}(\tau|c)$ 可以根据MDP模型分解为 $\log\rho_{\theta}(s_{0}|c)+\sum_{t=0}^{T-1}\left[\log\pi_{\theta}(a_{t}|s_{t},c)+\log\mathcal{T}_{\theta}(s_{t+1}|s_{t},a_{t},c)\right]$。通过这一无监督训练过程，VAE预计能够捕捉数据集中的多个模式，其编码器
    $P_{\phi}(c|\tau)$ 可以用于为演示轨迹提供模式嵌入。然后，模式条件鉴别器和策略可以通过类似CGAIL的目标进行训练。
- en: '|  | $\min_{\pi}\max_{D}\mathbb{E}_{\tau\sim D_{E},c\sim P_{\phi}(\cdot&#124;\tau)}\left[\frac{1}{T}\sum_{t=0}^{T-1}\log
    D(s_{t},a_{t}&#124;c)]+\mathbb{E}_{\rho_{\pi}(\cdot&#124;c)}[\log(1-D(s,a&#124;c))]-\lambda
    H(\pi(\cdot&#124;c))\right]$ |  | (62) |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\pi}\max_{D}\mathbb{E}_{\tau\sim D_{E},c\sim P_{\phi}(\cdot&#124;\tau)}\left[\frac{1}{T}\sum_{t=0}^{T-1}\log
    D(s_{t},a_{t}&#124;c)]+\mathbb{E}_{\rho_{\pi}(\cdot&#124;c)}[\log(1-D(s,a&#124;c))]-\lambda
    H(\pi(\cdot&#124;c))\right]$ |  | (62) |'
- en: 'The pretrained VAE policy $\pi_{\theta}(a|s,c)$ can be used to initialize the
    GAIL policy $\pi(a|s,c)$. Through further training with GAIL (i.e., Eq. ([62](#S4.E62
    "In 4.1.4 Integrating VAEs and GANs for Imitation Learning: A Spotlight ‣ 4.1
    Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))), the VAE policy can be refined by addressing the tendency
    to produce blurry imitations. Diverse GAIL (Wang et al. ([2017](#bib.bib345)))
    and VAE-ADAIL (Lu & Tompson ([2020](#bib.bib204))) are notable examples of this
    paradigm. A more straightforward manner to integrate the VAE and GAN for IL is
    to add a regularizer $\min_{\pi}\mathbb{E}_{s\sim\rho_{\phi}(\cdot)}D_{KL}(\pi(a|s)||\pi_{\text{VAE}}(a|s))$
    to the GAIL objective, where $\pi_{\text{VAE}}(a|s)$ is the VAE-based policy pretrained
    on $D_{E}$ (see Section [3.2.1](#S3.SS2.SSS1 "3.2.1 Core Schemes of VAE-based
    Imitation Learning ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")). In this way, the GAIL policy
    $\pi$ is encouraged to be close to the potentially multi-modal VAE policy. SAIL
    (Liu et al. ([2020](#bib.bib195))) explores in this direction. Another significant
    advantage of VAEs over GANs is that they can provide semantically meaningful latent
    embeddings for the input data. As mentioned in Section [3.1.4](#S3.SS1.SSS4 "3.1.4
    Data Augmentation and Transformation with VAEs ‣ 3.1 Offline Reinforcement Learning
    ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    such embeddings can be used as transformed training data to lower the difficulty
    of GAIL training. As an example, LAPAL (Wang et al. ([2022b](#bib.bib340))) proposes
    to adopt a CVAE to transform high-dimensional actions into low-dimensional latent
    vectors and apply GAIL on the latent space instead, in order to stabilize and
    accelerate the learning process.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的 VAE 策略 $\pi_{\theta}(a|s,c)$ 可以用来初始化 GAIL 策略 $\pi(a|s,c)$。通过进一步的 GAIL 训练（即
    Eq. ([62](#S4.E62 "在 4.1.4 部分集成 VAEs 和 GANs 进行模仿学习：聚焦 ‣ 4.1 模仿学习 ‣ 4 离线策略学习中的生成对抗网络
    ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向展望"))），可以通过解决产生模糊模仿的倾向来优化 VAE 策略。多样的 GAIL（Wang et al.
    ([2017](#bib.bib345))) 和 VAE-ADAIL（Lu & Tompson ([2020](#bib.bib204))) 是这一范式的显著例子。将
    VAE 和 GAN 集成到 IL 的一种更直接的方法是向 GAIL 目标中添加正则化项 $\min_{\pi}\mathbb{E}_{s\sim\rho_{\phi}(\cdot)}D_{KL}(\pi(a|s)||\pi_{\text{VAE}}(a|s))$，其中
    $\pi_{\text{VAE}}(a|s)$ 是在 $D_{E}$ 上预训练的基于 VAE 的策略（参见第 [3.2.1](#S3.SS2.SSS1 "3.2.1
    基于 VAE 的模仿学习的核心方案 ‣ 3.2 模仿学习 ‣ 3 离线策略学习中的变分自编码器 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向展望")
    节）。这样，GAIL 策略 $\pi$ 就会被鼓励接近潜在的多模态 VAE 策略。SAIL（Liu et al. ([2020](#bib.bib195)))
    在这一方向上进行了探索。VAEs 相较于 GANs 的另一个显著优势是它们可以为输入数据提供语义上有意义的潜在嵌入。正如第 [3.1.4](#S3.SS1.SSS4
    "3.1.4 使用 VAEs 进行数据扩充和转换 ‣ 3.1 离线强化学习 ‣ 3 离线策略学习中的变分自编码器 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向展望")
    节所述，这些嵌入可以作为变换后的训练数据，用于降低 GAIL 训练的难度。例如，LAPAL（Wang et al. ([2022b](#bib.bib340)))
    提出了采用 CVAE 将高维动作转换为低维潜在向量，并在潜在空间上应用 GAIL，从而稳定和加速学习过程。
- en: 4.2 Offline Reinforcement Learning
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 离线强化学习
- en: 'In this section, we starts with an introduction on model-based offline RL,
    for which both a world model of the environment and policy need to be learned
    from the offline dataset. GANs have been used to improve both aspects, as detailed
    in Section [4.2.2](#S4.SS2.SSS2 "4.2.2 Policy Approximation Using GANs ‣ 4.2 Offline
    Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") and [4.2.3](#S4.SS2.SSS3 "4.2.3 World Model Representation
    through GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions").'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍基于模型的离线强化学习，这要求从离线数据集中学习环境的世界模型和策略。GANs 已被用于改进这两个方面，具体细节见第 [4.2.2](#S4.SS2.SSS2
    "4.2.2 使用 GAN 进行策略近似 ‣ 4.2 离线强化学习 ‣ 4 生成对抗网络在离线策略学习中的应用 ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")
    和 [4.2.3](#S4.SS2.SSS3 "4.2.3 通过 GAN 进行世界模型表示 ‣ 4.2 离线强化学习 ‣ 4 生成对抗网络在离线策略学习中的应用
    ‣ 离线策略学习中的深度生成模型：教程、调查和未来方向的展望")。
- en: 4.2.1 Background on Model-based Offline Reinforcement Learning
  id: totrans-430
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 基于模型的离线强化学习背景
- en: 'Most research works in this category are based on Offline Model-based RL (Janner
    et al. ([2019](#bib.bib148)); Yu et al. ([2020a](#bib.bib380))). Different from
    the model-free case, a parametric dynamic model $\widehat{\mathcal{T}}(s^{\prime}|s,a)$
    and reward model $\hat{r}(s,a)$ need to be learned through supervised learning:
    ($D_{\mu}$: offline data collected by the behavior policy $\mu$.)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '这一类别的大多数研究工作基于离线模型强化学习（Janner et al. ([2019](#bib.bib148)); Yu et al. ([2020a](#bib.bib380)))。与无模型的方法不同，需要通过监督学习来学习一个参数化的动态模型
    $\widehat{\mathcal{T}}(s^{\prime}|s,a)$ 和奖励模型 $\hat{r}(s,a)$: （$D_{\mu}$: 行为策略
    $\mu$ 收集的离线数据。）'
- en: '|  | $\max_{\widehat{\mathcal{T}}}\mathbb{E}_{(s,a,s^{\prime})\sim D_{\mu}}\left[\log\widehat{\mathcal{T}}(s^{\prime}&#124;s,a)\right],\
    \min_{\hat{r}}\mathbb{E}_{(s,a,r)\sim D_{\mu}}\left[(\hat{r}(s,a)-r)^{2}\right]$
    |  | (63) |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\widehat{\mathcal{T}}}\mathbb{E}_{(s,a,s^{\prime})\sim D_{\mu}}\left[\log\widehat{\mathcal{T}}(s^{\prime}|s,a)\right],\
    \min_{\hat{r}}\mathbb{E}_{(s,a,r)\sim D_{\mu}}\left[(\hat{r}(s,a)-r)^{2}\right]$
    |  | (63) |'
- en: 'The approximated MDP $\widehat{\mathcal{M}}=(\mathcal{S},\mathcal{A},\widehat{\mathcal{T}},\hat{r},\hat{\rho}_{0}(s),\gamma)$,
    where $\hat{\rho}_{0}(s)$ is the empirical distribution of initial states in $D_{\mu}$,
    can be used as proxies of the real environment, and (short-horizon) trajectories
    can be collected by interacting with $\widehat{\mathcal{M}}$ using the being-learned
    policy $\pi_{\theta}$, forming another dataset $D_{\pi}$. Note that, to mitigate
    the impact from the model bias of $\widehat{\mathcal{M}}$, an estimation of the
    uncertainty in dynamics, i.e., $u(s,a)$, is utilized as a penalty reward term
    to discourage the agent to visit uncertain regions in the state-action space,
    so the reward at $(s,a)$ is calculated as $\tilde{r}(s,a)=\hat{r}(s,a)-\lambda_{r}u(s,a)$,
    where $\lambda_{r}>0$ is a hyperparameter. This conservative way to avoid OOD
    behaviors resemble the policy penalty method in dynamic-programming-based offline
    RL, as introduced in Section [3.1](#S3.SS1 "3.1 Offline Reinforcement Learning
    ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions").
    Typical choices of the uncertainty measure include variance or disagreement of
    estimations from an ensemble of Q-functions or approximated world models (Prudencio
    et al. ([2023](#bib.bib257))). Specifically, $D_{\pi}$ is generated as follows:
    $s_{0}\sim\hat{\rho}_{0}(\cdot),a_{0}\sim\pi_{\theta}(\cdot|s_{0}),\tilde{r}_{0}=\hat{r}(s_{0},a_{0})-\lambda_{r}u(s_{0},a_{0}),s_{1}\sim\mathcal{T}(\cdot|s_{0},a_{0})$,
    and so on. As a final step, offline/off-policy RL algorithms can be applied to
    train the policy $\pi_{\theta}$ on an augmented dataset $D_{\text{aug}}=\lambda_{D}D_{\mu}+(1-\lambda_{D})D_{\pi}$.
    Here, $\lambda_{D}\in[0,1]$ is another hyperparameter. A typical (off-policy)
    actor-critic framework for this process is as below (Levine et al. ([2020](#bib.bib181))):
    ($Q_{\bar{\phi}}$ is the target Q-function.)'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 逼近的 MDP $\widehat{\mathcal{M}}=(\mathcal{S},\mathcal{A},\widehat{\mathcal{T}},\hat{r},\hat{\rho}_{0}(s),\gamma)$，其中
    $\hat{\rho}_{0}(s)$ 是 $D_{\mu}$ 中初始状态的经验分布，可以作为真实环境的代理，并且可以通过使用正在学习的策略 $\pi_{\theta}$
    与 $\widehat{\mathcal{M}}$ 进行交互来收集（短期）轨迹，形成另一个数据集 $D_{\pi}$。请注意，为了减轻 $\widehat{\mathcal{M}}$
    模型偏差的影响，利用动态的不确定性估计，即 $u(s,a)$，作为惩罚奖励项以阻止智能体访问状态-动作空间中的不确定区域，因此 $(s,a)$ 的奖励计算为
    $\tilde{r}(s,a)=\hat{r}(s,a)-\lambda_{r}u(s,a)$，其中 $\lambda_{r}>0$ 是一个超参数。这种保守的避免
    OOD 行为的方式类似于动态规划基础的离线强化学习中的策略惩罚方法，如在第[3.1节](#S3.SS1 "3.1 离线强化学习 ‣ 3 离线策略学习中的变分自编码器
    ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的观点")中介绍的。典型的不确定性度量选择包括方差或来自 Q-函数或逼近世界模型的估计不一致（Prudencio
    等人 ([2023](#bib.bib257)））。具体地，$D_{\pi}$ 的生成方式如下：$s_{0}\sim\hat{\rho}_{0}(\cdot),a_{0}\sim\pi_{\theta}(\cdot|s_{0}),\tilde{r}_{0}=\hat{r}(s_{0},a_{0})-\lambda_{r}u(s_{0},a_{0}),s_{1}\sim\mathcal{T}(\cdot|s_{0},a_{0})$，依此类推。最后一步是应用离线/离策略
    RL 算法在扩展数据集 $D_{\text{aug}}=\lambda_{D}D_{\mu}+(1-\lambda_{D})D_{\pi}$ 上训练策略 $\pi_{\theta}$。这里，$\lambda_{D}\in[0,1]$
    是另一个超参数。该过程的典型（离策略）演员-评论家框架如下（Levine 等人 ([2020](#bib.bib181))）：($Q_{\bar{\phi}}$
    是目标 Q 函数。)
- en: '|  | $\max_{\theta}\mathbb{E}_{s\sim D_{\text{aug}},a\sim\pi_{\theta}(\cdot&#124;s)}\left[Q_{\phi}(s,a)\right],\
    \min_{\phi}\mathbb{E}_{(s,a,\tilde{r},s^{\prime})\sim D_{\text{aug}}}\left[(Q_{\phi}(s,a)-(\tilde{r}(s,a)+\gamma\mathbb{E}_{a^{\prime}\sim\pi_{\theta}(\cdot&#124;s^{\prime})}Q_{\bar{\phi}}(s^{\prime},a^{\prime}))\right]$
    |  | (64) |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\theta}\mathbb{E}_{s\sim D_{\text{aug}},a\sim\pi_{\theta}(\cdot|s)}\left[Q_{\phi}(s,a)\right],\
    \min_{\phi}\mathbb{E}_{(s,a,\tilde{r},s^{\prime})\sim D_{\text{aug}}}\left[(Q_{\phi}(s,a)-(\tilde{r}(s,a)+\gamma\mathbb{E}_{a^{\prime}\sim\pi_{\theta}(\cdot|s^{\prime})}Q_{\bar{\phi}}(s^{\prime},a^{\prime}))\right]$
    |  | (64) |'
- en: 'As introduced in Section [2.2](#S2.SS2 "2.2 Generative Adversarial Networks
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), the
    GAN framework consists of a generator $G$ and discriminator $D$ that are trained
    simultaneously to compete against each other. For offline model-based RL, the
    generator $G$ can be trained to approximate the policy or environment models,
    which is further discussed in the following subsections.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[2.2节](#S2.SS2 "2.2 生成对抗网络 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的观点")中介绍的，GAN
    框架由生成器 $G$ 和判别器 $D$ 组成，它们同时训练以相互竞争。对于离线基于模型的强化学习，生成器 $G$ 可以被训练来逼近策略或环境模型，这在以下小节中进一步讨论。
- en: 4.2.2 Policy Approximation Using GANs
  id: totrans-436
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 使用 GAN 进行策略逼近
- en: 'As mentioned in Section [3.1](#S3.SS1 "3.1 Offline Reinforcement Learning ‣
    3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    to avoid OOD states and actions, the learned policy $\pi_{\theta}$ should be close
    to the underlying behavior policy $\mu$, which is typically realized through introducing
    a regularization term to the objective for training $\pi_{\theta}$. Thus, the
    first objective in Eq. ([64](#S4.E64 "In 4.2.1 Background on Model-based Offline
    Reinforcement Learning ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) can be improved
    as:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '如第 [3.1](#S3.SS1 "3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") 节所述，为了避免 OOD 状态和动作，学习到的策略
    $\pi_{\theta}$ 应该接近于潜在的行为策略 $\mu$，这通常通过在训练 $\pi_{\theta}$ 的目标中引入正则化项来实现。因此，Eq.
    ([64](#S4.E64 "In 4.2.1 Background on Model-based Offline Reinforcement Learning
    ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) 中的第一个目标可以改进为：'
- en: '|  | $\max_{\theta}\lambda_{Q}\mathbb{E}_{s\sim D_{\text{aug}},a\sim\pi_{\theta}(\cdot&#124;s)}\left[Q_{\phi}(s,a)\right]-f(\pi_{\theta},\mu)$
    |  | (65) |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\theta}\lambda_{Q}\mathbb{E}_{s\sim D_{\text{aug}},a\sim\pi_{\theta}(\cdot\mid
    s)}\left[Q_{\phi}(s,a)\right]-f(\pi_{\theta},\mu)$ |  | (65) |'
- en: 'where $\lambda_{Q}>0$ is the regularization coefficient. SGBCQ (Dong et al.
    ([2023](#bib.bib74))) proposes to directly constrain the action distribution of
    $\pi_{\theta}$ at each state to be close to the one of $\mu$ by implementing $f(\pi_{\theta},\mu)$
    as $\mathbb{E}_{s\sim D_{\mu}}\left[d(\pi_{\theta}(\cdot|s)||\mu(\cdot|s))\right]$,
    where $d(\cdot)$ denotes a statistical divergence, resembling the policy constraint
    methods (i.e., Eq. ([33](#S3.E33 "In 3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))).
    As introduced in Section [2.2](#S2.SS2 "2.2 Generative Adversarial Networks ‣
    2 Background on Deep Generative Models ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"), GANs are
    be used to model and minimize the JS divergence between two data distributions.
    Thus, SGBCQ gives out a practical framework to solve Eq. ([65](#S4.E65 "In 4.2.2
    Policy Approximation Using GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) based
    on a CGAN as below:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\lambda_{Q}>0$ 是正则化系数。SGBCQ（Dong 等人 ([2023](#bib.bib74)）提出直接约束每个状态下的 $\pi_{\theta}$
    行动分布接近于 $\mu$ 的分布，通过将 $f(\pi_{\theta},\mu)$ 实现为 $\mathbb{E}_{s\sim D_{\mu}}\left[d(\pi_{\theta}(\cdot|s)||\mu(\cdot|s))\right]$，其中
    $d(\cdot)$ 表示统计分歧，类似于策略约束方法（即，Eq. ([33](#S3.E33 "In 3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))）。如第
    [2.2](#S2.SS2 "2.2 Generative Adversarial Networks ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions") 节所述，GANs 被用来建模并最小化两个数据分布之间的JS分歧。因此，SGBCQ
    提出了一个基于 CGAN 的实际框架来解决 Eq. ([65](#S4.E65 "In 4.2.2 Policy Approximation Using GANs
    ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"))，具体如下：'
- en: '|  |  | $\displaystyle\qquad\qquad\max_{D}\mathbb{E}_{(s,a)\sim D_{\mu}}[\log
    D(a&#124;s)+\mathbb{E}_{z\sim P_{Z}(\cdot)}\log(1-D(G(z&#124;s)))],$ |  | (66)
    |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\qquad\qquad\max_{D}\mathbb{E}_{(s,a)\sim D_{\mu}}[\log
    D(a\mid s)+\mathbb{E}_{z\sim P_{Z}(\cdot)}\log(1-D(G(z\mid s)))],$ |  | (66) |'
- en: '|  |  | $\displaystyle\min_{G}-\lambda_{Q}\mathbb{E}_{s\sim D_{\text{aug}},z\sim
    P_{Z}(\cdot)}\left[Q_{\phi}(s,G(z&#124;s))\right]+\mathbb{E}_{s\sim D_{\mu},z\sim
    P_{Z}(\cdot)}[\log(1-D(G(z&#124;s))]$ |  |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\min_{G}-\lambda_{Q}\mathbb{E}_{s\sim D_{\text{aug}},z\sim
    P_{Z}(\cdot)}\left[Q_{\phi}(s,G(z\mid s))\right]+\mathbb{E}_{s\sim D_{\mu},z\sim
    P_{Z}(\cdot)}[\log(1-D(G(z\mid s))]$ |  |'
- en: 'Compared with the CGAN objective (i.e., Eq. ([12](#S2.E12 "In 1st item ‣ 2.2
    Generative Adversarial Networks ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))), $s$ and $a$ here work as the conditional information
    $y$ and data point $x$, respectively. Intuitively, the stochastic policy $a\sim\pi_{\theta}(\cdot|s)$
    is implemented as a generator $a=G(z|s),z\sim P_{Z}(\cdot)$ and the generator
    is trained to maximize the expected Q-values and fool the discriminator $D$ at
    the same time. Involving such a GAN training process encourages the policy to
    generate behaviors close to the demonstrated ones in distribution.'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CGAN 目标（即公式 ([12](#S2.E12 "在第 1 项 ‣ 2.2 生成对抗网络 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向的展望"))）相比，这里的
    $s$ 和 $a$ 分别作为条件信息 $y$ 和数据点 $x$。直观上，随机策略 $a\sim\pi_{\theta}(\cdot|s)$ 实现为生成器 $a=G(z|s),z\sim
    P_{Z}(\cdot)$，并且生成器被训练以最大化期望 Q 值并同时欺骗鉴别器 $D$。涉及这样的 GAN 训练过程鼓励策略生成在分布上接近示范行为的行为。
- en: 'SDM-GAN (Yang et al. ([2022b](#bib.bib370))) follows a similar protocol but
    chooses to regularize the stationary $(s,a)$ distribution of $\pi_{\theta}$ towards
    the offline dataset to avoid OOD cases. Specifically, they define $f(\pi_{\theta},\mu)$
    in Eq. ([66](#S4.E66 "In 4.2.2 Policy Approximation Using GANs ‣ 4.2 Offline Reinforcement
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) as $d(\rho^{\mathcal{T}}_{\mu}(\cdot)||\rho^{\mathcal{T}}_{\pi_{\theta}}(\cdot))$,
    where $\rho^{\mathcal{T}}_{\mu}(s,a)$ can be approximated as the empirical distribution
    of $(s,a)$ in $D_{\mu}$, $\rho^{\mathcal{T}}_{\pi_{\theta}}(s,a)=\lim_{T\rightarrow\infty}\frac{1}{T+1}\sum_{t=0}^{T}P(s_{t}=s,a_{t}=a|s_{0}\sim\rho_{0}(\cdot),a_{t}\sim\pi_{\theta}(\cdot|s_{t}),s_{t+1}\sim\mathcal{T}(\cdot|s_{t},a_{t}))$
    is the stationary $(s,a)$ distribution of $\pi_{\theta}$ under the real dynamic
    $\mathcal{T}$. Assuming the dynamic function $\widehat{\mathcal{T}}$ is well-fitted,
    $d(\rho^{\mathcal{T}}_{\mu}(\cdot)||\rho^{\mathcal{T}}_{\pi_{\theta}}(\cdot))$
    is upper bounded by: (Please refer to (Yang et al. ([2022b](#bib.bib370))) for
    the derivations and definitions of the function classes $\mathcal{G}_{1,2}$.)'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: SDM-GAN (Yang et al. ([2022b](#bib.bib370))) 遵循类似的协议，但选择将 $\pi_{\theta}$ 的静态
    $(s,a)$ 分布规范化为离线数据集，以避免 OOD 情况。具体而言，他们在公式 ([66](#S4.E66 "在 4.2.2 使用 GAN 进行策略近似
    ‣ 4.2 离线强化学习 ‣ 4 生成对抗网络在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向的展望")) 中定义了 $f(\pi_{\theta},\mu)$
    为 $d(\rho^{\mathcal{T}}_{\mu}(\cdot)||\rho^{\mathcal{T}}_{\pi_{\theta}}(\cdot))$，其中
    $\rho^{\mathcal{T}}_{\mu}(s,a)$ 可以近似为 $D_{\mu}$ 中 $(s,a)$ 的经验分布，$\rho^{\mathcal{T}}_{\pi_{\theta}}(s,a)=\lim_{T\rightarrow\infty}\frac{1}{T+1}\sum_{t=0}^{T}P(s_{t}=s,a_{t}=a|s_{0}\sim\rho_{0}(\cdot),a_{t}\sim\pi_{\theta}(\cdot|s_{t}),s_{t+1}\sim\mathcal{T}(\cdot|s_{t},a_{t}))$
    是 $\pi_{\theta}$ 在真实动态 $\mathcal{T}$ 下的静态 $(s,a)$ 分布。假设动态函数 $\widehat{\mathcal{T}}$
    拟合良好，则 $d(\rho^{\mathcal{T}}_{\mu}(\cdot)||\rho^{\mathcal{T}}_{\pi_{\theta}}(\cdot))$
    有上界：（请参考 (Yang et al. ([2022b](#bib.bib370))) 以获取函数类 $\mathcal{G}_{1,2}$ 的推导和定义。）
- en: '|  | $\sup_{g\sim\mathcal{G}_{1}}&#124;\mathbb{E}_{(s,a)\sim\rho^{\mathcal{T}}_{\mu}(\cdot)}[g(s,a)-\mathbb{E}_{s^{\prime}\sim\widehat{\mathcal{T}}(\cdot&#124;s,a),a^{\prime}\sim\pi_{\theta}(\cdot&#124;s^{\prime})}g(s^{\prime},a^{\prime})]&#124;+\sup_{g\sim\mathcal{G}_{2}}&#124;\mathbb{E}_{(s,a)\sim\rho^{\mathcal{T}}_{\mu}(\cdot)}[g(s,a)]-\mathbb{E}_{s\sim\rho^{\mathcal{T}}_{\mu}(\cdot),a\sim\pi_{\theta}(\cdot&#124;s)}g(s,a)]&#124;$
    |  | (67) |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sup_{g\sim\mathcal{G}_{1}}&#124;\mathbb{E}_{(s,a)\sim\rho^{\mathcal{T}}_{\mu}(\cdot)}[g(s,a)-\mathbb{E}_{s^{\prime}\sim\widehat{\mathcal{T}}(\cdot&#124;s,a),a^{\prime}\sim\pi_{\theta}(\cdot&#124;s^{\prime})}g(s^{\prime},a^{\prime})]&#124;+\sup_{g\sim\mathcal{G}_{2}}&#124;\mathbb{E}_{(s,a)\sim\rho^{\mathcal{T}}_{\mu}(\cdot)}[g(s,a)]-\mathbb{E}_{s\sim\rho^{\mathcal{T}}_{\mu}(\cdot),a\sim\pi_{\theta}(\cdot&#124;s)}g(s,a)]&#124;$
    |  | (67) |'
- en: 'The equation above is in a similar form with the Wasserstein distance (defined
    in Section [2.2](#S2.SS2 "2.2 Generative Adversarial Networks ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) between samples stored in $D_{\mu}$
    and generated by $\pi_{\theta}$, which inspires the use of GANs for the estimation.
    In particular, the objective is as below:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程与 $D_{\mu}$ 中存储的样本和 $\pi_{\theta}$ 生成的样本之间的 Wasserstein 距离（在 [2.2](#S2.SS2
    "2.2 生成对抗网络 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查及未来方向的展望") 节中定义）形式相似，这激发了使用 GAN
    进行估计的想法。具体而言，目标如下：
- en: '|  |  | $\displaystyle\qquad\ \ \max_{D}\mathbb{E}_{(s,a)\sim D_{\mu}}[\log
    D(s,a)]+\mathbb{E}_{(s,a)\sim D_{G}}[\log(1-D(s,a))],$ |  | (68) |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\qquad\ \ \max_{D}\mathbb{E}_{(s,a)\sim D_{\mu}}[\log
    D(s,a)]+\mathbb{E}_{(s,a)\sim D_{G}}[\log(1-D(s,a))],$ |  | (68) |'
- en: '|  |  | $\displaystyle\min_{G}-\lambda_{Q}\mathbb{E}_{s\sim D_{\text{aug}},z\sim
    P_{Z}(\cdot)}\left[Q_{\phi}(s,G(z&#124;s))\right]+\mathbb{E}_{(s,a)\sim D_{G}}[\log(1-D(s,a)]$
    |  |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\min_{G}-\lambda_{Q}\mathbb{E}_{s\sim D_{\text{aug}},z\sim
    P_{Z}(\cdot)}\left[Q_{\phi}(s,G(z\mid s))\right]+\mathbb{E}_{(s,a)\sim D_{G}}[\log(1-D(s,a))]$
    |  |'
- en: 'Here, inspired by Eq. ([67](#S4.E67 "In 4.2.2 Policy Approximation Using GANs
    ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")), $D_{G}$ contains $(s,a)/(s^{\prime},a^{\prime})$
    pairs collected in this way: $s\sim D_{\mu},a\sim\pi_{\theta}(\cdot|s),s^{\prime}\sim\widehat{\mathcal{T}}(\cdot|s,a),a^{\prime}\sim\pi_{\theta}(\cdot|s^{\prime})$,
    where $a\sim\pi_{\theta}(\cdot|s)$ is implemented as $a=G(z|s),z\sim P_{Z}(\cdot)$.
    The only difference between Eq. ([66](#S4.E66 "In 4.2.2 Policy Approximation Using
    GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) and ([68](#S4.E68 "In
    4.2.2 Policy Approximation Using GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4
    Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))
    is thus the way to generate samples. For Eq. ([66](#S4.E66 "In 4.2.2 Policy Approximation
    Using GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), the $(s,a)$ pairs
    are simply generated by $s\sim D_{\mu},a\sim\pi_{\theta}(\cdot|s)$. The same authors
    have also proposed a model-free variant of this algorithm (Yang et al. ([2022c](#bib.bib371))),
    sharing similar motivations and algorithm designs.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，灵感来自于公式 ([67](#S4.E67 "In 4.2.2 Policy Approximation Using GANs ‣ 4.2 Offline
    Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))，$D_{G}$ 包含了以以下方式收集的 $(s,a)/(s^{\prime},a^{\prime})$ 对：$s\sim
    D_{\mu},a\sim\pi_{\theta}(\cdot|s),s^{\prime}\sim\widehat{\mathcal{T}}(\cdot|s,a),a^{\prime}\sim\pi_{\theta}(\cdot|s^{\prime})$，其中
    $a\sim\pi_{\theta}(\cdot|s)$ 被实现为 $a=G(z|s),z\sim P_{Z}(\cdot)$。因此，公式 ([66](#S4.E66
    "In 4.2.2 Policy Approximation Using GANs ‣ 4.2 Offline Reinforcement Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) 和 ([68](#S4.E68 "In 4.2.2 Policy Approximation Using GANs ‣ 4.2
    Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) 之间的唯一区别是样本生成的方法。对于公式 ([66](#S4.E66
    "In 4.2.2 Policy Approximation Using GANs ‣ 4.2 Offline Reinforcement Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"))，$(s,a)$ 对是通过 $s\sim D_{\mu},a\sim\pi_{\theta}(\cdot|s)$ 简单生成的。相同的作者还提出了该算法的无模型变体（Yang
    et al. ([2022c](#bib.bib371)))，具有类似的动机和算法设计。'
- en: 'AMPL (Yang et al. ([2022d](#bib.bib372))) adopts the same objective (i.e.,
    Eq. ([68](#S4.E68 "In 4.2.2 Policy Approximation Using GANs ‣ 4.2 Offline Reinforcement
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))) for training $\pi_{\theta}$, but proposes that the environment
    models $\widehat{\mathcal{T}}$ and $\hat{r}$ can be periodically updated with
    $\pi_{\theta}$ by collecting new data with $\pi_{\theta}$ and applying supervised
    learning (e.g., with Eq. ([63](#S4.E63 "In 4.2.1 Background on Model-based Offline
    Reinforcement Learning ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"))) to update
    $\widehat{\mathcal{T}}$ and $\hat{r}$. Interestingly, they theoretically show
    that the objective for $\pi_{\theta}$, which combines the Q-function and distribution
    discrepancy, approximates an upper bound for $-J(\pi_{\theta},\mathcal{M})$, i.e,
    the negative expected return of the policy on the real MDP. On the other hand,
    DASCO (Vuong et al. ([2022](#bib.bib335))) proposes that the two terms in the
    objective of $\pi_{\theta}$ may conflict with each other, since fooling the discriminator
    requires mimicking all in-distribution actions, which can be suboptimal, but maximizing
    the Q function would mean avoiding low-return behaviors. Thus, they propose to
    introduce an auxiliary generator $G_{\text{aux}}$ to generate low-return samples
    and modify the objective as follows:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 'AMPL (Yang et al. ([2022d](#bib.bib372))) 采用了相同的目标（即，方程 ([68](#S4.E68 "In 4.2.2
    Policy Approximation Using GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))) 来训练
    $\pi_{\theta}$，但提出环境模型 $\widehat{\mathcal{T}}$ 和 $\hat{r}$ 可以通过收集 $\pi_{\theta}$
    的新数据并应用监督学习（例如，使用方程 ([63](#S4.E63 "In 4.2.1 Background on Model-based Offline
    Reinforcement Learning ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"))) 来定期更新 $\widehat{\mathcal{T}}$
    和 $\hat{r}$。有趣的是，他们理论上表明，结合 Q 函数和分布差异的 $\pi_{\theta}$ 的目标接近于 $-J(\pi_{\theta},\mathcal{M})$
    的上界，即策略在真实 MDP 上的负期望回报。另一方面，DASCO (Vuong et al. ([2022](#bib.bib335))) 提出了 $\pi_{\theta}$
    的目标中的两个项可能相互冲突，因为欺骗鉴别器需要模拟所有在分布内的动作，这可能是不理想的，但最大化 Q 函数意味着避免低回报行为。因此，他们提出引入一个辅助生成器
    $G_{\text{aux}}$ 以生成低回报样本，并将目标修改如下：'
- en: '|  | $\min_{G,G_{\text{aux}}}\max_{D}\mathbb{E}_{(s,a)\sim D_{\mu}}[\log D(s,a)]+\mathbb{E}_{(s,a)\sim
    D_{G,G_{\text{aux}}}}[\log(1-D(s,a))]-\lambda_{Q}\mathbb{E}_{s\sim D_{\text{aug}},z\sim
    P_{Z}(\cdot)}\left[Q_{\phi}(s,G(z&#124;s))\right]$ |  | (69) |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{G,G_{\text{aux}}}\max_{D}\mathbb{E}_{(s,a)\sim D_{\mu}}[\log D(s,a)]+\mathbb{E}_{(s,a)\sim
    D_{G,G_{\text{aux}}}}[\log(1-D(s,a))]-\lambda_{Q}\mathbb{E}_{s\sim D_{\text{aug}},z\sim
    P_{Z}(\cdot)}\left[Q_{\phi}(s,G(z\mid s))\right]$ |  | (69) |'
- en: 'Here, samples in $D_{G,G_{\text{aux}}}$ are generated as: $s\sim D_{\mu},z\sim
    P_{Z}(\cdot),a=(G(z|s)+G_{\text{aux}}(z|s))/2$. Both $G$ and $G_{\text{aux}}$
    are adopted to mimic the demonstrations, but only the primary generator $G$ is
    trained to maximize the Q-values. Theoretically, they prove that the optimal solution
    for the primary generator $G$ will maximize the probability mass of in-support
    samples that maximize the Q-function.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$D_{G,G_{\text{aux}}}$ 中的样本生成方式为：$s\sim D_{\mu},z\sim P_{Z}(\cdot),a=(G(z\mid
    s)+G_{\text{aux}}(z\mid s))/2$。$G$ 和 $G_{\text{aux}}$ 都被用来模拟示范，但只有主生成器 $G$ 被训练以最大化
    Q 值。理论上，他们证明了主生成器 $G$ 的最优解将最大化支持样本的概率质量，从而最大化 Q 函数。
- en: 'All aforementioned algorithms utilize a very similar objective design, which
    can be viewed as a paradigm of using GANs for policy approximations in offline
    RL. GOPlan (Wang et al. ([2023a](#bib.bib338))) proposes an advantage-weighted
    CGAN objective, which is different in form from previous ones, for capturing the
    multi-modal action distribution in goal-conditioned offline planning:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 所有上述算法都采用了非常相似的目标设计，这可以视为在离线强化学习中使用 GANs 进行策略逼近的一种范式。GOPlan (Wang et al. ([2023a](#bib.bib338)))
    提出了一个与之前形式不同的优势加权 CGAN 目标，用于捕捉目标条件下离线规划中的多模态动作分布：
- en: '|  | $\min_{G}\max_{D}\mathbb{E}_{g\sim D_{\mu}}\left[\mathbb{E}_{(s,a)\sim
    D_{\mu}^{g}}[w(s,a,g)\log D(s,a&#124;g)]+\mathbb{E}_{s\sim D_{\mu}^{g},z\sim P_{Z}(\cdot)}[\log(1-D(s,G(z&#124;s)&#124;g))]\right]$
    |  | (70) |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{G}\max_{D}\mathbb{E}_{g\sim D_{\mu}}\left[\mathbb{E}_{(s,a)\sim
    D_{\mu}^{g}}[w(s,a,g)\log D(s,a\mid g)]+\mathbb{E}_{s\sim D_{\mu}^{g},z\sim P_{Z}(\cdot)}[\log(1-D(s,G(z\mid
    s)\mid g))]\right]$ |  | (70) |'
- en: Here, $D_{\mu}^{g}$ denotes the partition of $D_{\mu}$ that takes $g$ as the
    goal; $w(s,a,g)=\exp(A^{\mu}(s,a,g))$ denotes the weight function, where $A^{\mu}(s,a,g)$
    is a separately trained advantage function. This mechanism encourages the policy
    to produce actions that closely resemble high-quality (i.e., high-advantage) actions
    from the offline dataset, but no theoretical guarantee is provided.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$D_{\mu}^{g}$ 表示将 $g$ 作为目标的 $D_{\mu}$ 的划分；$w(s,a,g)=\exp(A^{\mu}(s,a,g))$
    表示权重函数，其中 $A^{\mu}(s,a,g)$ 是单独训练的优势函数。该机制鼓励策略生成的动作与离线数据集中高质量（即高优势）动作相似，但未提供理论保证。
- en: 4.2.3 World Model Representation through GANs
  id: totrans-455
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 通过 GANs 进行世界模型表示
- en: 'Beyond policy generation, GANs can also be used to approximate the environment
    models $\widehat{\mathcal{M}}$, as illustrated in MOAN (Yang et al. ([2023a](#bib.bib367))),
    TS (Hepburn & Montana ([2022](#bib.bib130))), S2P (Cho et al. ([2022](#bib.bib56))),
    which is expected to outperform traditional supervised learning methods (i.e.,
    using Eq. ([63](#S4.E63 "In 4.2.1 Background on Model-based Offline Reinforcement
    Learning ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))). To be specific, MOAN
    proposes the following objective:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '除了策略生成外，GANs 还可以用于近似环境模型 $\widehat{\mathcal{M}}$，如 MOAN (Yang et al. ([2023a](#bib.bib367)))、TS
    (Hepburn & Montana ([2022](#bib.bib130))) 和 S2P (Cho et al. ([2022](#bib.bib56)))
    所示，预计其性能将优于传统的监督学习方法（即，使用 Eq. ([63](#S4.E63 "In 4.2.1 Background on Model-based
    Offline Reinforcement Learning ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))）。具体来说，MOAN
    提出了以下目标：'
- en: '|  |  | $\displaystyle\quad\max_{D}\mathbb{E}_{(s,a,r,s^{\prime})\sim D_{\mu}}[\log
    D(s,a,r,s^{\prime})]+\mathbb{E}_{(s,a)\sim D_{\mu},(\hat{r},\hat{s}^{\prime})\sim
    G(\cdot&#124;s,a)}[\log(1-D(s,a,\hat{r},\hat{s}^{\prime}))],$ |  | (71) |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad\max_{D}\mathbb{E}_{(s,a,r,s^{\prime})\sim D_{\mu}}[\log
    D(s,a,r,s^{\prime})]+\mathbb{E}_{(s,a)\sim D_{\mu},(\hat{r},\hat{s}^{\prime})\sim
    G(\cdot&#124;s,a)}[\log(1-D(s,a,\hat{r},\hat{s}^{\prime}))],$ |  | (71) |'
- en: '|  |  | $\displaystyle\min_{G}-\lambda_{N}\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[-\log G(r,s^{\prime}&#124;s,a)\right]+\mathbb{E}_{(s,a)\sim D_{\mu},(\hat{r},\hat{s}^{\prime})\sim
    G(\cdot&#124;s,a)}[\log(1-D(s,a,\hat{r},\hat{s}^{\prime}))]$ |  |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\min_{G}-\lambda_{N}\mathbb{E}_{(s,a,r,s^{\prime})\sim
    D_{\mu}}\left[-\log G(r,s^{\prime}&#124;s,a)\right]+\mathbb{E}_{(s,a)\sim D_{\mu},(\hat{r},\hat{s}^{\prime})\sim
    G(\cdot&#124;s,a)}[\log(1-D(s,a,\hat{r},\hat{s}^{\prime}))]$ |  |'
- en: 'This objective design is similar with Eq. ([68](#S4.E68 "In 4.2.2 Policy Approximation
    Using GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) in form, but replaces
    the Q-function term with a negative log-likelihood (i.e., $-\log G(r,s^{\prime}|s,a)$)
    that is commonly used for environment model learning. In this CGAN framework,
    the conditional generator $G(\cdot|s,a)$ is learned to predict $s^{\prime}$ and
    $r$, working as the approximated reward $\hat{r}(s,a)$ and transition function
    $\mathcal{T}(s^{\prime}|s,a)$. In particular, the generator $G(\cdot|s,a)=\mathcal{N}(\text{mean}(s,a),\text{std}(s,a))$
    is implemented as a stochastic Gaussian model, and the standard deviation of the
    predictions, i.e., $\text{std}(s,a)$, can be used as uncertainty measure (i.e.,
    $u(s,a)$ in Section [4.2.1](#S4.SS2.SSS1 "4.2.1 Background on Model-based Offline
    Reinforcement Learning ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")). MOAN proposes
    to further include the confidence level given by the discriminator, i.e., $u(s,a)=\text{std}(s,a)+\sqrt{2(1-D_{(\hat{r},\hat{s}^{\prime})\sim
    G(\cdot|s,a)}(s,a,\hat{r},\hat{s}^{\prime}))}$. Intuitively, if the variance of
    the estimation is high or the discriminator classifies the prediction as generated
    data, i.e., $D\rightarrow 0$, the uncertainty level at this point $(s,a)$ should
    also be high. However, this objective design is not backed up by theoretical analysis.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '这个目标设计在形式上与 Eq. ([68](#S4.E68 "In 4.2.2 Policy Approximation Using GANs ‣ 4.2
    Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) 类似，但用负对数似然（即，$-\log G(r,s^{\prime}|s,a)$）替代了
    Q 函数项，这是在环境模型学习中常用的。在这个 CGAN 框架中，条件生成器 $G(\cdot|s,a)$ 被学习以预测 $s^{\prime}$ 和 $r$，作为近似的奖励
    $\hat{r}(s,a)$ 和转移函数 $\mathcal{T}(s^{\prime}|s,a)$。特别地，生成器 $G(\cdot|s,a)=\mathcal{N}(\text{mean}(s,a),\text{std}(s,a))$
    被实现为一个随机高斯模型，预测的标准差，即 $\text{std}(s,a)$，可以作为不确定性度量（即，$u(s,a)$ 在 Section [4.2.1](#S4.SS2.SSS1
    "4.2.1 Background on Model-based Offline Reinforcement Learning ‣ 4.2 Offline
    Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")）。MOAN 提议进一步包括判别器给出的置信度，即 $u(s,a)=\text{std}(s,a)+\sqrt{2(1-D_{(\hat{r},\hat{s}^{\prime})\sim
    G(\cdot|s,a)}(s,a,\hat{r},\hat{s}^{\prime}))}$。直观上，如果估计的方差较高或判别器将预测分类为生成数据，即 $D\rightarrow
    0$，则此点 $(s,a)$ 的不确定性水平也应该较高。然而，这个目标设计并没有理论分析支持。'
- en: 'Different from MOAN, TS and S2P are not limited to model-based offline RL.
    TS proposes to stitch high-value segments from different trajectories together
    to form higher-quality trajectories for offline RL. To realize this, they model
    the distribution $P(a,r,s^{\prime}|s)=P(s^{\prime}|s)P(a|s^{\prime},s)P(r|a,s^{\prime},s)$,
    that is, searching for a potential next state $s^{\prime}$ from the neighbourhood
    of $s$, which has a higher probability $P(s^{\prime}|s)$ and value $V(s^{\prime})$
    than its original next state, and then identifying the most probable intermidiate
    $a,\ r$. In particular, they learn $P(s^{\prime}|s),\ P(a|s^{\prime},s),\ P(r|a,s^{\prime},s)$
    with the traditional supervised learning, CVAE, and WGAN (see Section [2.2](#S2.SS2
    "2.2 Generative Adversarial Networks ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")), respectively. The WGAN objective is: ($G(z|s,a,s^{\prime})$
    works as the approximation of $P(r|a,s^{\prime},s)$.)'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '与 MOAN 不同，TS 和 S2P 并不限于基于模型的离线 RL。TS 提议将来自不同轨迹的高价值段拼接在一起，形成更高质量的轨迹用于离线 RL。为实现这一点，他们建模了分布
    $P(a,r,s^{\prime}|s)=P(s^{\prime}|s)P(a|s^{\prime},s)P(r|a,s^{\prime},s)$，即从 $s$
    的邻域中寻找一个潜在的下一个状态 $s^{\prime}$，其具有比原始下一个状态更高的概率 $P(s^{\prime}|s)$ 和价值 $V(s^{\prime})$，然后识别最可能的中间
    $a,\ r$。特别地，他们使用传统的监督学习、CVAE 和 WGAN 分别学习 $P(s^{\prime}|s),\ P(a|s^{\prime},s),\
    P(r|a,s^{\prime},s)$（见 Section [2.2](#S2.SS2 "2.2 Generative Adversarial Networks
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")）。WGAN
    的目标是：($G(z|s,a,s^{\prime})$ 作为 $P(r|a,s^{\prime},s)$ 的近似)。'
- en: '|  | $\min_{G}\max_{D}\mathbb{E}_{(s,a,s^{\prime},r)\sim D_{\mu}}[D(s,a,s^{\prime},r)]-\mathbb{E}_{(s,a,s^{\prime})\sim
    D_{\mu},z\sim P_{Z}(\cdot)}[D(s,a,s^{\prime},G(z&#124;s,a,s^{\prime}))]$ |  |
    (72) |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{G}\max_{D}\mathbb{E}_{(s,a,s^{\prime},r)\sim D_{\mu}}[D(s,a,s^{\prime},r)]-\mathbb{E}_{(s,a,s^{\prime})\sim
    D_{\mu},z\sim P_{Z}(\cdot)}[D(s,a,s^{\prime},G(z&#124;s,a,s^{\prime}))]$ |  |
    (72) |'
- en: 'This algorithm can be viewed as a model-based data augmentation method for
    offline RL using generative models. S2P is a model learning approach for vision-based
    offline RL, where they first learn the approximated transition and reward model
    (i.e., $\widehat{T}(s^{\prime}|s,a),\ \hat{r}(s,a)$) for the underlying states
    through traditional supervised learning and then a generator to synthesize the
    image $I/I^{\prime}$ that perfectly represents the corresponding state $s/s^{\prime}$.
    The generator is conditioned on both the current state and previous image, i.e.,
    $I^{\prime}=G(z|s,I)$, and trained within a WGAN framework. Finally, we briefly
    introduce an interesting work, i.e., IOM (Qi et al. ([2022](#bib.bib260))), on
    offline model-based optimization, which cannot be categorized as either policy
    or world model approximation. They propose to address the distributional shift
    in offline decision-making by enforcing invariance between the learned representations
    in the source and target domains. Specially, they adopt an LSGAN (introduced in
    Section [2.2](#S2.SS2 "2.2 Generative Adversarial Networks ‣ 2 Background on Deep
    Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")), where the generator works as
    the representation function of the data and the discriminator tries to discriminate
    between representations from the source and target domains. In this way, the discrepancy
    between distributions of representations under the source and target domains can
    be minimized.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '该算法可以被视为一种基于模型的数据增强方法，用于使用生成模型的离线强化学习。S2P 是一种基于视觉的离线强化学习的模型学习方法，其中他们首先通过传统的监督学习学习近似的转移和奖励模型（即
    $\widehat{T}(s^{\prime}|s,a),\ \hat{r}(s,a)$），然后使用生成器合成完美表示对应状态 $s/s^{\prime}$
    的图像 $I/I^{\prime}$。生成器在当前状态和先前图像的条件下工作，即 $I^{\prime}=G(z|s,I)$，并在 WGAN 框架内训练。最后，我们简要介绍一个有趣的工作，即
    IOM（Qi 等人（[2022](#bib.bib260)）），关于离线基于模型的优化，这不能归类为策略或世界模型逼近。他们提出通过在源领域和目标领域之间强制表示的不可变性来解决离线决策中的分布转移。特别是，他们采用了
    LSGAN（在第 [2.2](#S2.SS2 "2.2 Generative Adversarial Networks ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") 节中介绍），其中生成器作为数据的表示函数，鉴别器试图区分源领域和目标领域的表示。通过这种方式，可以最小化源领域和目标领域表示之间的分布差异。'
- en: '| Algorithm | GAN Type | GAN Usage | Evaluation Task |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | GAN 类型 | GAN 使用 | 评价任务 |'
- en: '| SGBCQ | CGAN | Policy | D4RL (L) |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| SGBCQ | CGAN | 策略 | D4RL (L) |'
- en: '| SDM-GAN | GAN | Policy | D4RL (L, M, A) |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| SDM-GAN | GAN | 策略 | D4RL (L, M, A) |'
- en: '| AMPL | GAN | Policy | D4RL (L, M, A) |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| AMPL | GAN | 策略 | D4RL (L, M, A) |'
- en: '| DASCO | GAN (auxiliary generator) | Policy | D4RL (L, M) |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| DASCO | GAN（辅助生成器） | 策略 | D4RL (L, M) |'
- en: '| GOPlan | CGAN (advantage-weighted) | Policy | MuJoCo Robotic Manipulation
    |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| GOPlan | CGAN（优势加权） | 策略 | MuJoCo 机器人操作 |'
- en: '| MOAN | GAN | Environment Model | D4RL (L) |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| MOAN | GAN | 环境模型 | D4RL (L) |'
- en: '| TS | WGAN | Environment Model | D4RL (L) |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| TS | WGAN | 环境模型 | D4RL (L) |'
- en: '| S2P | WGAN | Environment Model | dm_control |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| S2P | WGAN | 环境模型 | dm_control |'
- en: '| IOM | LSGAN | Representation Function | Design-Bench |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| IOM | LSGAN | 表示函数 | Design-Bench |'
- en: 'Table 5: Summary of GAN-based offline RL algorithms. Most algorithms in this
    category have been evaluated on D4RL (Fu et al. ([2020](#bib.bib99))), which provides
    multiple datasets for data-driven RL tasks, including Locomotion (L), AntMaze
    (M), Adroit (A), etc. MuJoCo Robotic Manipulation (Yang et al. ([2023b](#bib.bib369)))
    provides offline datasets for a series of goal-conditioned robotic manipulation
    tasks, several of which can be utilized to assess the OOD generalization capabilities
    of the policy. dm_control (Tassa et al. ([2018](#bib.bib317))) includes 6 environments,
    which are typically used for vision-based RL benchmarks. Design-Bench (Trabucco
    et al. ([2022](#bib.bib322))) includes tasks specific for data-driven offline
    model-based optimization.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：基于GAN的离线RL算法总结。该类别的大多数算法已经在D4RL（Fu et al. ([2020](#bib.bib99)))上进行了评估，该数据集为数据驱动的RL任务提供了多个数据集，包括Locomotion
    (L)、AntMaze (M)、Adroit (A)等。MuJoCo Robotic Manipulation (Yang et al. ([2023b](#bib.bib369)))提供了一系列目标条件下的机器人操作任务的离线数据集，其中一些可以用于评估策略的OOD泛化能力。dm_control
    (Tassa et al. ([2018](#bib.bib317)))包括6个环境，通常用于基于视觉的RL基准。Design-Bench (Trabucco
    et al. ([2022](#bib.bib322)))包含针对数据驱动离线模型优化的特定任务。
- en: 'As shown in Table [5](#S4.T5 "Table 5 ‣ 4.2.3 World Model Representation through
    GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), we provide a summary
    of GAN-based offline RL algorithms. GANs can be employed for both policy and world
    model approximations in model-based offline RL, primarily due to its use in minimizing
    distributional discrepancy.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[5](#S4.T5 "Table 5 ‣ 4.2.3 World Model Representation through GANs ‣ 4.2
    Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")所示，我们提供了基于GAN的离线RL算法的总结。GAN可以用于模型基础离线RL中的策略和世界模型近似，这主要是由于其在最小化分布差异方面的应用。'
- en: 5 Normalizing Flows in Offline Policy Learning
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 离线策略学习中的归一化流
- en: 'The applications of Normalizing Flows (NFs) in offline policy learning are
    less frequent compared to other deep generative models, particularly for NF-based
    offline RL methods. Thus, we slightly broaden the scope of the RL part to include
    online RL methods that utilize offline datasets in Section [5.2](#S5.SS2 "5.2
    Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions").'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '与其他深度生成模型相比，归一化流（NFs）在离线策略学习中的应用较少，特别是针对NF-based离线RL方法。因此，我们稍微扩大了RL部分的范围，以包括在第[5.2](#S5.SS2
    "5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")节中使用离线数据集的在线RL方法。'
- en: 5.1 Imitation Learning
  id: totrans-477
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 模仿学习
- en: 'As mentioned in Section [2.3](#S2.SS3 "2.3 Normalizing Flows ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), NFs can convert a simple
    prior distribution $P_{Z}(z)$ on the latent space into a potentially intricate
    target distribution $P_{X}(x)$ through a sequence of bijections. NFs allow efficient
    sampling (i.e., via the generation direction $z\rightarrow x$) and exact density
    estimation (i.e., via the normalizing direction $x\rightarrow z$). In particular,
    for data generation, a sample $z\sim P_{Z}(\cdot)$ is drawn from the latent space,
    and then the data sample can be generated as $x=F^{-1}(z)=G(z)$. For the other
    direction, computing the density of a point $x$ is accomplished by computing the
    density of its corresponding latent variable $z=F(x)$ and multiplying the associated
    Jacobian determinant $|\det D(F(x))|$, i.e., $P_{X}(x)=P_{Z}(F(x))|\det D(F(x))|$
    where $P_{Z}(\cdot)$ is known.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[2.3](#S2.SS3 "2.3 Normalizing Flows ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")节中提到的，NFs可以通过一系列双射将潜在空间中的简单先验分布$P_{Z}(z)$转化为可能复杂的目标分布$P_{X}(x)$。NFs允许高效的采样（即，通过生成方向$z\rightarrow
    x$）和精确的密度估计（即，通过归一化方向$x\rightarrow z$）。特别是对于数据生成，从潜在空间中抽取一个样本$z\sim P_{Z}(\cdot)$，然后数据样本可以生成为$x=F^{-1}(z)=G(z)$。对于另一个方向，通过计算其对应的潜在变量$z=F(x)$的密度，并乘以相关的Jacobian行列式$|\det
    D(F(x))|$，即$P_{X}(x)=P_{Z}(F(x))|\det D(F(x))|$来计算点$x$的密度，其中$P_{Z}(\cdot)$是已知的。'
- en: NF-based IL works either employ NFs for exact density estimation or leverage
    their proficiency in modelling complex policies to manage challenging task scenarios.
    Next, we present a comprehensive review of works from both categories.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 基于NF的IL工作要么使用NFs进行精确密度估计，要么利用其建模复杂策略的能力来管理具有挑战性的任务场景。接下来，我们将对这两类工作进行全面回顾。
- en: 5.1.1 Exact Density Estimation in Imitation Learning Using Normalizing Flows
  id: totrans-480
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 使用正规化流的模仿学习中的精确密度估计
- en: 'As a paradigm, NF-based IL methods that adopt NFs as density estimators usually
    start with standard IL objectives, and optimize them as RL problems wherein the
    rewards necessitate estimation of specific distributions. In this case, NFs are
    introduced to model those distributions and provide exact density inference for
    reward calculation, which greatly enhance the training stability and performance.
    To be specific, we present some representative works here:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种范式，采用NFs作为密度估计器的NF-based IL方法通常以标准IL目标开始，并将其优化为RL问题，其中奖励需要估计特定分布。在这种情况下，NFs被引入以建模这些分布，并提供精确的密度推断以计算奖励，这大大增强了训练的稳定性和性能。具体来说，我们在这里介绍一些具有代表性的工作：
- en: 'As proposed in Ghasemipour et al. ([2019b](#bib.bib103)), most IL methods can
    be viewed as matching the agent’s state-action distribution with the expert’s,
    by minimizing some f-divergence $D_{f}$. CFIL (Freund et al. ([2023](#bib.bib96)))
    realizes IL by minimizing the KL divergence between the agent’s and expert’s state-action
    occupancy measure (as defined in Section [4.1.1](#S4.SS1.SSS1 "4.1.1 Fundamental
    GAN-Based Imitation Learning Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")):'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Ghasemipour等人（[2019b](#bib.bib103)）所提出，大多数IL方法可以被视为通过最小化某些f-散度$D_{f}$来使代理的状态-动作分布与专家的分布匹配。CFIL（Freund等人（[2023](#bib.bib96)））通过最小化代理和专家的状态-动作占用测度之间的KL散度来实现IL（如第[4.1.1节](#S4.SS1.SSS1
    "4.1.1 基础GAN-Based模仿学习算法：GAIL和AIRL ‣ 4.1 模仿学习 ‣ 4 离线策略学习中的生成对抗网络 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向的展望")中定义的）：
- en: '|  | $\operatorname*{arg\,min}_{\pi}D_{KL}(\rho_{\pi}(s,a)&#124;&#124;\rho_{E}(s,a))=\operatorname*{arg\,max}_{\pi}\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}\left[\log\frac{\rho_{E}(s,a)}{\rho_{\pi}(s,a)}\right]=\operatorname*{arg\,max}_{\pi}J(\pi,r=\log\frac{\rho_{E}}{\rho_{\pi}})$
    |  | (73) |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname*{arg\,min}_{\pi}D_{KL}(\rho_{\pi}(s,a)&#124;&#124;\rho_{E}(s,a))=\operatorname*{arg\,max}_{\pi}\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}\left[\log\frac{\rho_{E}(s,a)}{\rho_{\pi}(s,a)}\right]=\operatorname*{arg\,max}_{\pi}J(\pi,r=\log\frac{\rho_{E}}{\rho_{\pi}})$
    |  | (73) |'
- en: 'where $J(\pi,r)$ denotes the expected return (Sutton & Barto ([2018](#bib.bib315)))
    of the policy $\pi$ under the reward function $r$. MAF (Papamakarios et al. ([2017](#bib.bib239)))
    is used to model the distributions $\rho_{E}(s,a)$ and $\rho_{\pi}(s,a)$ based
    on which the rewards can be acquired. Specifically, $(s,a)$ can be viewed as $x$
    in the NF framework and the densities at $(s,a)$, i.e., $\rho_{E}(s,a)$ and $\rho_{\pi}(s,a)$,
    can be estimated using Eq. ([18](#S2.E18 "In 2.3 Normalizing Flows ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")). However, rather than
    optimizing these two NFs independently with corresponding MLE objectives (i.e.,
    Eq. ([19](#S2.E19 "In 2.3 Normalizing Flows ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"))), they couple the modelling of these
    two distributions based on the optimality point of the Donsker-Varadhan form of
    the KL divergence Donsker & Varadhan ([1976](#bib.bib76)):'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$J(\pi,r)$表示在奖励函数$r$下策略$\pi$的期望回报（Sutton & Barto（[2018](#bib.bib315)））。MAF（Papamakarios等人（[2017](#bib.bib239)））用于建模分布$\rho_{E}(s,a)$和$\rho_{\pi}(s,a)$，基于这些分布可以获取奖励。具体地，$(s,a)$可以视为NF框架中的$x$，并且在$(s,a)$处的密度，即$\rho_{E}(s,a)$和$\rho_{\pi}(s,a)$，可以使用方程([18](#S2.E18
    "在2.3 正规化流 ‣ 2 深度生成模型背景 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向的展望"))进行估计。然而，与其独立优化这两个NFs及其对应的MLE目标（即方程([19](#S2.E19
    "在2.3 正规化流 ‣ 2 深度生成模型背景 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向的展望"))），它们基于KL散度Donsker-Varadhan形式的最优点耦合这两个分布的建模（Donsker
    & Varadhan（[1976](#bib.bib76)））：
- en: '|  | $D_{KL}(\rho_{\pi}(s,a)&#124;&#124;\rho_{E}(s,a))=\sup_{f:S\times A\rightarrow\mathbb{R}}\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}\left[f(s,a)\right]-\log\mathbb{E}_{(s,a)\sim\rho_{E}(\cdot)}\left[e^{f(s,a)}\right]$
    |  | (74) |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|  | $D_{KL}(\rho_{\pi}(s,a)&#124;&#124;\rho_{E}(s,a))=\sup_{f:S\times A\rightarrow\mathbb{R}}\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}\left[f(s,a)\right]-\log\mathbb{E}_{(s,a)\sim\rho_{E}(\cdot)}\left[e^{f(s,a)}\right]$
    |  | (74) |'
- en: 'In the equation above, optimality occurs with $f^{*}(s,a)=\log\frac{\rho_{\pi}(s,a)}{\rho_{E}(s,a)}+C$
    where $C\in\mathbb{R}$. Thus, through maximizing the right-hand side of the equation
    above, the log ratio used as the reward function can be recovered. Specifically,
    they model $f$ as $f_{\psi,\phi}(s,a)=\log\rho^{\psi}_{\pi}(s,a)-\log\rho^{\phi}_{E}(s,a)$
    with the two flows $\rho^{\psi}_{\pi}$ and $\rho^{\phi}_{E}$. Given this improved
    estimator, the overall IL objective can be written as:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，最优性发生在$f^{*}(s,a)=\log\frac{\rho_{\pi}(s,a)}{\rho_{E}(s,a)}+C$，其中$C\in\mathbb{R}$。因此，通过最大化上述方程的右侧，可以恢复作为奖励函数使用的对数比率。具体来说，他们将$f$建模为$f_{\psi,\phi}(s,a)=\log\rho^{\psi}_{\pi}(s,a)-\log\rho^{\phi}_{E}(s,a)$，其中包括两个流$\rho^{\psi}_{\pi}$和$\rho^{\phi}_{E}$。给定这个改进的估计器，整体IL目标可以写成：
- en: '|  | $\operatorname*{arg\,max}_{\pi}\min_{\rho^{\psi}_{\pi},\rho^{\phi}_{E}}\log\mathbb{E}_{(s,a)\sim
    D_{E}(\cdot)}\left[\frac{\rho_{\pi}^{\psi}(s,a)}{\rho_{E}^{\phi}(s,a)}\right]-\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}\left[\log\frac{\rho_{\pi}^{\psi}(s,a)}{\rho_{E}^{\phi}(s,a)}\right]$
    |  | (75) |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname*{arg\,max}_{\pi}\min_{\rho^{\psi}_{\pi},\rho^{\phi}_{E}}\log\mathbb{E}_{(s,a)\sim
    D_{E}(\cdot)}\left[\frac{\rho_{\pi}^{\psi}(s,a)}{\rho_{E}^{\phi}(s,a)}\right]-\mathbb{E}_{(s,a)\sim\rho_{\pi}(\cdot)}\left[\log\frac{\rho_{\pi}^{\psi}(s,a)}{\rho_{E}^{\phi}(s,a)}\right]$
    |  | (75) |'
- en: In this case, the learning alternates between $\max_{\pi}$ and $\min_{\rho_{\pi}^{\phi},\rho_{E}^{\psi}}$.
    $\max_{\pi}$ is realized with an RL process using $\log\frac{\rho_{E}^{\psi}(s,a)}{\rho_{\pi}^{\phi}(s,a)}$
    as the reward function. While, $\rho_{E}^{\psi}(s,a)$ and $\rho_{\pi}^{\phi}(s,a)$
    are updated with expert data (i.e., $(s,a)\sim D_{E}$) and rollout data collected
    by $\pi$ (i.e., $(s,a)\sim\rho_{\pi}(\cdot)$).
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，学习在$\max_{\pi}$和$\min_{\rho_{\pi}^{\phi},\rho_{E}^{\psi}}$之间交替进行。$\max_{\pi}$通过使用$\log\frac{\rho_{E}^{\psi}(s,a)}{\rho_{\pi}^{\phi}(s,a)}$作为奖励函数的RL过程实现。而$\rho_{E}^{\psi}(s,a)$和$\rho_{\pi}^{\phi}(s,a)$则通过专家数据（即$(s,a)\sim
    D_{E}$）和由$\pi$收集的回滚数据（即$(s,a)\sim\rho_{\pi}(\cdot)$）进行更新。
- en: 'IL-flOw (Chang et al. ([2022](#bib.bib37))) focuses on Learning from Observations
    (LfO), where the agent only gets access to a dataset of state sequences. This
    work also starts with the KL divergence but processes to decouple the policy optimization
    from the reward learning to improve the training stability. In particular, the
    learning objective for the policy $\pi$ is to maximize $-D_{KL}(P_{\pi}(s^{\prime}|s)||P_{E}(s^{\prime}|s))$:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: IL-flOw (Chang et al. ([2022](#bib.bib37))) 专注于从观察中学习（LfO），其中代理只能访问状态序列的数据集。这项工作也从KL散度开始，但处理过程中将策略优化与奖励学习解耦，以提高训练稳定性。具体来说，策略$\pi$的学习目标是最大化$-D_{KL}(P_{\pi}(s^{\prime}|s)||P_{E}(s^{\prime}|s))$：
- en: '|  | $\mathbb{E}_{s_{0:T}\sim P_{\pi}}\left[\sum_{t=0}^{T-1}\left(\log P_{E}(s_{t+1}&#124;s_{t})-\log
    P_{\pi}(s_{t+1}&#124;s_{t})\right)\right]=\mathbb{E}_{s_{0:T}\sim P_{\pi}}\left[\sum_{t=0}^{T-1}(\log
    P_{E}(s_{t+1}&#124;s_{t})+H(\pi(\cdot&#124;s_{t})))\right]$ |  | (76) |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{s_{0:T}\sim P_{\pi}}\left[\sum_{t=0}^{T-1}\left(\log P_{E}(s_{t+1}&#124;s_{t})-\log
    P_{\pi}(s_{t+1}&#124;s_{t})\right)\right]=\mathbb{E}_{s_{0:T}\sim P_{\pi}}\left[\sum_{t=0}^{T-1}(\log
    P_{E}(s_{t+1}&#124;s_{t})+H(\pi(\cdot&#124;s_{t})))\right]$ |  | (76) |'
- en: 'The equality holds when environment dynamics are deterministic and invertible.
    This can be optimized with RL by setting the reward as $r_{t}=\log P_{E}(s_{t+1}|s_{t})$
    while maximizing the entropy of the policy, i.e., $H(\pi(\cdot|s_{t}))$. As a
    separate stage, before the RL training, $P_{E}(s^{\prime}|s)$ is modeled with
    a conditional NF (i.e., conditioning the transformation function $F$ on a fixed
    state variable $s$: $s^{\prime}=F(z|s),\ z\sim P_{Z}(\cdot)$) based on a dataset
    of demonstrations $D_{E}$. Specifically, they select NSF (Durkan et al. ([2019](#bib.bib84)))
    as the density estimator.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '当环境动态是确定性和可逆时，上述等式成立。通过将奖励设置为$r_{t}=\log P_{E}(s_{t+1}|s_{t})$并最大化策略的熵，即$H(\pi(\cdot|s_{t}))$，可以通过强化学习（RL）进行优化。作为一个独立阶段，在RL训练之前，基于演示数据集$D_{E}$，用条件NF建模$P_{E}(s^{\prime}|s)$（即将变换函数$F$条件化为固定状态变量$s$:
    $s^{\prime}=F(z|s),\ z\sim P_{Z}(\cdot)$）。具体而言，他们选择NSF（Durkan et al. ([2019](#bib.bib84)))作为密度估计器。'
- en: 'SOIL-TDM (Boborzi et al. ([2022](#bib.bib27); [2021b](#bib.bib26))) also focuses
    on LfO and starts with the same objective as IL-flOw. However, SOIL-TDM gets rid
    of the requirements for deterministic and invertible dynamics by estimating $P_{\pi}(s_{t+1}|s_{t})$
    as $\mathcal{T}_{\pi}(s_{t+1}|s_{t},a_{t})\pi(a_{t}|s_{t})/\pi^{\prime}(a_{t}|s_{t+1},s_{t})$,
    based on the Bayes Theorem. With this new definition, the objective for $\pi$
    is converted into:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: SOIL-TDM (Boborzi et al. ([2022](#bib.bib27); [2021b](#bib.bib26))) 也关注于 LfO，并从与
    IL-flOw 相同的目标开始。然而，SOIL-TDM 通过基于贝叶斯定理估计 $P_{\pi}(s_{t+1}|s_{t})$ 为 $\mathcal{T}_{\pi}(s_{t+1}|s_{t},a_{t})\pi(a_{t}|s_{t})/\pi^{\prime}(a_{t}|s_{t+1},s_{t})$，从而摆脱了对确定性和可逆动态的要求。通过这个新定义，$\pi$
    的目标被转换为：
- en: '|  | $\max_{\pi}\mathbb{E}_{(s_{0:T},a_{0:T})\sim P_{\pi}}\sum_{t=0}^{T-1}\left[r(s_{t},a_{t})+H(\pi(\cdot&#124;s_{t}))\right]$
    |  | (77) |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{(s_{0:T},a_{0:T})\sim P_{\pi}}\sum_{t=0}^{T-1}\left[r(s_{t},a_{t})+H(\pi(\cdot|s_{t}))\right]$
    |  | (77) |'
- en: where $r(s_{t},a_{t})=\mathbb{E}_{s_{t+1}\sim\mathcal{T}_{\pi}(\cdot|s_{t},a_{t})}\left[-\log\mathcal{T}_{\pi}(s_{t+1}|s_{t},a_{t})+\log\pi^{{}^{\prime}}(a_{t}|s_{t+1},s_{t})+\log
    P_{E}(s_{t+1}|s_{t})\right]$. Still, they separate the reward learning from the
    policy optimization. In this case, besides the expert state transition model $P_{E}(s_{t+1}|s_{t})$,
    they adopt conditional NFs to model the agent’s transition dynamics $\mathcal{T}_{\pi}(s_{t+1}|s_{t},a_{t})$
    and the posterior distribution associated with $\pi$, i.e., $\pi^{\prime}(a_{t}|s_{t+1},s_{t})$,
    for which they choose Real NVP (Dinh et al. ([2017](#bib.bib73))). Note that the
    approximation of $P_{E}(s_{t+1}|s_{t})$ is trained on $D_{E}$, while approximations
    of $\mathcal{T}_{\pi}(s_{t+1}|s_{t},a_{t})$ and $\pi^{\prime}(a_{t}|s_{t+1},s_{t})$
    are trained on rollout data collected by $\pi$ during the RL process.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r(s_{t},a_{t})=\mathbb{E}_{s_{t+1}\sim\mathcal{T}_{\pi}(\cdot|s_{t},a_{t})}\left[-\log\mathcal{T}_{\pi}(s_{t+1}|s_{t},a_{t})+\log\pi^{{}^{\prime}}(a_{t}|s_{t+1},s_{t})+\log
    P_{E}(s_{t+1}|s_{t})\right]$。他们仍然将奖励学习与策略优化分开。在这种情况下，除了专家状态转移模型 $P_{E}(s_{t+1}|s_{t})$
    外，他们采用条件正态流（conditional NFs）来建模智能体的转移动态 $\mathcal{T}_{\pi}(s_{t+1}|s_{t},a_{t})$
    和与 $\pi$ 相关的后验分布，即 $\pi^{\prime}(a_{t}|s_{t+1},s_{t})$，他们选择了 Real NVP (Dinh et
    al. ([2017](#bib.bib73)))。注意，$P_{E}(s_{t+1}|s_{t})$ 的近似是在 $D_{E}$ 上训练的，而 $\mathcal{T}_{\pi}(s_{t+1}|s_{t},a_{t})$
    和 $\pi^{\prime}(a_{t}|s_{t+1},s_{t})$ 的近似是在 $\pi$ 在 RL 过程中收集的滚动数据上训练的。
- en: 'SLIL (Zhang et al. ([2021b](#bib.bib389))) is based on the Likelihood-based
    Imitation Learning (LIL) framework:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: SLIL (Zhang et al. ([2021b](#bib.bib389))) 基于基于似然的模仿学习（LIL）框架：
- en: '|  | $\max_{\pi}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log P_{\pi}(s,a)\right]\
    s.t.\ P_{\pi}=\operatorname*{arg\,max}_{P}\mathbb{E}_{(s,a)\sim\rho_{\pi}}\left[\log
    P(s,a)\right]$ |  | (78) |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log P_{\pi}(s,a)\right]\
    s.t.\ P_{\pi}=\operatorname*{arg\,max}_{P}\mathbb{E}_{(s,a)\sim\rho_{\pi}}\left[\log
    P(s,a)\right]$ |  | (78) |'
- en: 'Intuitively, $P_{\pi}(s,a)$ represents the probability of observing an expert
    state-action pair $(s,a)\in D_{E}$ when executing the learner policy $\pi$. Directly
    solving this bilevel optimization problem requires jointly learning $\pi$ and
    $P_{\pi}$, which brings training instability. Thus, they propose to maximize its
    tight lower bound:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，$P_{\pi}(s,a)$ 代表在执行学习者策略 $\pi$ 时观察到专家状态-动作对 $(s,a)\in D_{E}$ 的概率。直接解决这个双层优化问题需要联合学习
    $\pi$ 和 $P_{\pi}$，这会带来训练的不稳定。因此，他们提出最大化其紧界下限：
- en: '|  | $\max_{\pi}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log\pi(a&#124;s)\right]+\mathbb{E}_{s\sim\rho_{\pi}}\left[\log
    P_{E}(s)\right]\ s.t.\ P_{E}=\operatorname*{arg\,max}_{P}\mathbb{E}_{s\sim D_{E}}\left[\log
    P(s)\right]$ |  | (79) |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log\pi(a|s)\right]+\mathbb{E}_{s\sim\rho_{\pi}}\left[\log
    P_{E}(s)\right]\ s.t.\ P_{E}=\operatorname*{arg\,max}_{P}\mathbb{E}_{s\sim D_{E}}\left[\log
    P(s)\right]$ |  | (79) |'
- en: 'where $\rho_{\pi}(s)$ denotes the occupancy measure of $s$ using $\pi$. The
    training process can then be divided into two stages for stability. At stage 1,
    they train an NF to model the expert state distribution $P_{E}(s)$ based on the
    demonstration set $D_{E}$, which is, at stage 2, adopted to optimize the policy
    $\pi(a|s)$ using Eq. ([79](#S5.E79 "In 5.1.1 Exact Density Estimation in Imitation
    Learning Using Normalizing Flows ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")). This objective combines
    Behavioral Cloning (i.e., $\max_{\pi}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log\pi(a|s)\right]$)
    and RL with $\log P_{E}(s)$ as the reward (i.e., $\max_{\pi}\mathbb{E}_{s\sim\rho_{\pi}}\left[\log
    P_{E}(s)\right]$). It is worthy noting that they utilize a Continuous Normalizing
    Flow – DCNF (Zhang et al. ([2021b](#bib.bib389))), rather than discrete ones like
    other works, as density estimators. This choice is due to the enhanced modeling
    capabilities and fewer model restrictions that Continuous Normalizing Flows offer
    Grathwohl et al. ([2018](#bib.bib108)).'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\rho_{\pi}(s)$ 表示使用 $\pi$ 的 $s$ 的占用测度。训练过程可以被分为两个阶段以保证稳定性。在第1阶段，他们训练一个NF来建模专家状态分布
    $P_{E}(s)$，该分布基于示例集 $D_{E}$，在第2阶段，则采用它来优化策略 $\pi(a|s)$，使用方程 ([79](#S5.E79 "In
    5.1.1 Exact Density Estimation in Imitation Learning Using Normalizing Flows ‣
    5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))。这个目标将行为克隆（即 $\max_{\pi}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log\pi(a|s)\right]$）和强化学习与
    $\log P_{E}(s)$ 作为奖励（即 $\max_{\pi}\mathbb{E}_{s\sim\rho_{\pi}}\left[\log P_{E}(s)\right]$）结合在一起。值得注意的是，他们使用的是连续正则化流
    – DCNF（张等人 ([2021b](#bib.bib389)))，而不是像其他研究那样使用离散的正则化流作为密度估计器。这一选择是由于连续正则化流提供了更强的建模能力和更少的模型限制（Grathwohl
    et al. ([2018](#bib.bib108))）。'
- en: 'GPRIL (Schroecker et al. ([2019](#bib.bib286))) also utilizes the LIL framework
    but optimizes it in a different manner:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: GPRIL（Schroecker et al. ([2019](#bib.bib286))) 也利用了 LIL 框架，但以不同的方式进行优化：
- en: '|  | $\max_{\pi_{\theta}}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log P_{\pi_{\theta}}(s,a)\right]=\max_{\pi_{\theta}}\mathbb{E}_{(s,a)\sim
    D_{E}}\left[\log\pi_{\theta}(a&#124;s)+\log P_{\pi_{\theta}}(s)\right]$ |  | (80)
    |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi_{\theta}}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log P_{\pi_{\theta}}(s,a)\right]=\max_{\pi_{\theta}}\mathbb{E}_{(s,a)\sim
    D_{E}}\left[\log\pi_{\theta}(a\mid s)+\log P_{\pi_{\theta}}(s)\right]$ |  | (80)
    |'
- en: 'Here, the first term of the right-hand side is simply Behavioral Cloning. While,
    for the second term, according to (Ross & Bagnell ([2010](#bib.bib281))), the
    gradient from it can be estimated as $\nabla_{\theta}\log P_{\pi_{\theta}}(s)\propto\mathbb{E}_{(s^{\prime\prime},a^{\prime\prime})\sim\mathcal{B}_{\pi_{\theta}}(\cdot|s)}\left[\nabla_{\theta}\log\pi_{\theta}(a^{\prime\prime}|s^{\prime\prime})\right]$,
    where $\mathcal{B}_{\pi_{\theta}}$ corresponds to the long-term predecessor distribution
    modelling the distribution of states and actions that, under the current policy
    $\pi_{\theta}$, will eventually lead to the given target state $s$: $\mathcal{B}_{\pi_{\theta}}(s^{\prime\prime},a^{\prime\prime}|s)=(1-\gamma)\sum_{j=0}^{\infty}\gamma^{j}P_{\pi_{\theta}}(s_{t}=s^{\prime\prime},a_{t}=a^{\prime\prime}|s_{t+j+1}=s)$.
    In GPRIL, $\mathcal{B}_{\pi_{\theta}}(s^{\prime\prime},a^{\prime\prime}|s)$ is
    modeled as $\mathcal{B}_{\pi_{\theta}}^{\omega_{1}}(s^{\prime\prime}|s)\cdot\mathcal{B}_{\pi_{\theta}}^{\omega_{2}}(a^{\prime\prime}|s,s^{\prime\prime})$,
    i.e., the product of two density functions modelled by conditional MAF (Papamakarios
    et al. ([2017](#bib.bib239))). To train these models, they collect training data
    using self-supervised roll-outs. In particular, they sample states, actions and
    target-states (i.e., $(s^{\prime\prime},a^{\prime\prime},s)$) where the separation
    in time between the $s^{\prime\prime}$ and $s$ is selected randomly from a geometric
    distribution parameterized by $\gamma$. In this way, the triplets collected theoretically
    obey: $(s^{\prime\prime},a^{\prime\prime})\sim\mathcal{B}_{\pi_{\theta}}(\cdot|s)$.
    As a subsequent research, the work Schroecker & Jr. ([2020](#bib.bib285)) utilizes
    the same objective function, but introduces an alternative method for estimating
    $\nabla_{\theta}\log P_{\pi_{\theta}}(s)$ from a goal-conditioned RL perspective.'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，右侧的第一个术语是简单的行为克隆（**Behavioral Cloning**）。而对于第二个术语，根据（Ross & Bagnell ([2010](#bib.bib281)))，它的梯度可以估计为
    $\nabla_{\theta}\log P_{\pi_{\theta}}(s)\propto\mathbb{E}_{(s^{\prime\prime},a^{\prime\prime})\sim\mathcal{B}_{\pi_{\theta}}(\cdot|s)}\left[\nabla_{\theta}\log\pi_{\theta}(a^{\prime\prime}|s^{\prime\prime})\right]$，其中
    $\mathcal{B}_{\pi_{\theta}}$ 对应于长期前驱分布，用于建模在当前策略 $\pi_{\theta}$ 下最终会导致给定目标状态 $s$
    的状态和动作的分布：$\mathcal{B}_{\pi_{\theta}}(s^{\prime\prime},a^{\prime\prime}|s)=(1-\gamma)\sum_{j=0}^{\infty}\gamma^{j}P_{\pi_{\theta}}(s_{t}=s^{\prime\prime},a_{t}=a^{\prime\prime}|s_{t+j+1}=s)$。在
    GPRIL 中，$\mathcal{B}_{\pi_{\theta}}(s^{\prime\prime},a^{\prime\prime}|s)$ 被建模为
    $\mathcal{B}_{\pi_{\theta}}^{\omega_{1}}(s^{\prime\prime}|s)\cdot\mathcal{B}_{\pi_{\theta}}^{\omega_{2}}(a^{\prime\prime}|s,s^{\prime\prime})$，即由条件
    MAF（Papamakarios et al. ([2017](#bib.bib239))) 建模的两个密度函数的乘积。为了训练这些模型，他们使用自监督的回滚收集训练数据。特别地，他们抽取状态、动作和目标状态（即
    $(s^{\prime\prime},a^{\prime\prime},s)$），其中 $s^{\prime\prime}$ 和 $s$ 之间的时间间隔从参数为
    $\gamma$ 的几何分布中随机选择。这样，理论上收集到的三元组满足：$(s^{\prime\prime},a^{\prime\prime})\sim\mathcal{B}_{\pi_{\theta}}(\cdot|s)$。作为后续研究，Schroecker
    & Jr. ([2020](#bib.bib285)) 利用相同的目标函数，但引入了一种从目标条件 RL 视角估计 $\nabla_{\theta}\log
    P_{\pi_{\theta}}(s)$ 的替代方法。
- en: 5.1.2 Policy Modeling in Imitation Learning with Normalizing Flows
  id: totrans-503
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 使用归一化流的模仿学习中的策略建模
- en: 'As a potent generative model, Normalizing Flows can be directly adopted as
    policy generators for tackling challenging tasks. In this regard, recent works
    in off-policy RL have demonstrated that NFs can be used to model continuous policies
    (Haarnoja et al. ([2018](#bib.bib117)); Ward et al. ([2019](#bib.bib347)); Mazoure
    et al. ([2019](#bib.bib217))), which can lead to faster convergence and higher
    rewards by enhancing exploration and supporting multi-modal action distributions.
    Compared with commonly employed diagonal Gaussian policies, where each action
    dimension is independent of the others, those based on NFs offer greater expressiveness.
    In the context of IL, Flow DAC (Boborzi et al. ([2021a](#bib.bib25))) extends
    DAC (an off-policy IL algorithm Kostrikov et al. ([2019](#bib.bib171))) by adopting
    a conditional version of Real NVP as the policy network. Specifically, the action
    $a$ at state $s$ is generated by: $z\sim P_{Z}(\cdot),\ a=G(z|s)$ where $G$ denotes
    the NF generator and $s$ is the conditioner. The stochasticity of this policy
    comes from the latent distribution $P_{Z}(\cdot)$, and the action output of $G$
    can be in any complex distribution. Similarly, FlowPlan (Agarwal et al. ([2020](#bib.bib3)))
    adopts NFs as a trajectory generator through sequentially predicting the next
    state embedding based on historical states. Specifically, NAF is adopted as the
    generative model, which is trained by minimizing the KL divergence between the
    expert’s and generated trajectories.'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种强大的生成模型，规范化流（Normalizing Flows）可以直接作为策略生成器，用于处理具有挑战性的任务。在这方面，近期的离策略强化学习（off-policy
    RL）研究表明，NFs 可以用来建模连续策略（Haarnoja et al. ([2018](#bib.bib117)); Ward et al. ([2019](#bib.bib347));
    Mazoure et al. ([2019](#bib.bib217)))，这可以通过增强探索和支持多模态动作分布来实现更快的收敛和更高的奖励。与常用的对角高斯策略相比，其中每个动作维度相互独立，基于
    NFs 的策略提供了更大的表现力。在 IL 的背景下，Flow DAC（Boborzi et al. ([2021a](#bib.bib25))) 通过采用
    Real NVP 的条件版本作为策略网络，扩展了 DAC（一个离策略 IL 算法 Kostrikov et al. ([2019](#bib.bib171)))。具体而言，状态
    $s$ 下的动作 $a$ 由以下公式生成：$z\sim P_{Z}(\cdot),\ a=G(z|s)$ 其中 $G$ 表示 NF 生成器，$s$ 是条件变量。该策略的随机性来自于潜在分布
    $P_{Z}(\cdot)$，$G$ 的动作输出可以是任何复杂分布。类似地，FlowPlan（Agarwal et al. ([2020](#bib.bib3)))
    通过基于历史状态顺序预测下一个状态嵌入，将 NFs 作为轨迹生成器。具体来说，NAF 被用作生成模型，通过最小化专家轨迹与生成轨迹之间的 KL 散度进行训练。
- en: '| Algorithm | CFIL | IL-flOw | SOIL-TDM | SLIL | GPRIL | Flow DAC | FlowPlan
    |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | CFIL | IL-flOw | SOIL-TDM | SLIL | GPRIL | Flow DAC | FlowPlan |'
- en: '| Flow | MAF | NSF | Real NVP | DCNF | MAF | Real NVP | NAF |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| 流 | MAF | NSF | Real NVP | DCNF | MAF | Real NVP | NAF |'
- en: '| Flow Type | AR | CP | CP | ODE | AR | CP | AR |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| 流类型 | AR | CP | CP | ODE | AR | CP | AR |'
- en: '| Flow Usage | DE ($\rho_{E}(s,a)$, $\rho_{\pi}(s,a)$) | DE ($P_{E}(s^{\prime}&#124;s)$)
    | DE ($P_{E}(s^{\prime}&#124;s)$, $\mathcal{T}_{\pi}(s^{\prime}&#124;s,a)$, $\pi^{\prime}(a&#124;s^{\prime},s)$)
    | DE ($P_{E}(s)$) | DE $\mathcal{B}_{\pi}(s^{\prime\prime},a^{\prime\prime}&#124;s)$
    | G (Policy Network) | G (Traj Planner) |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| 流使用 | DE ($\rho_{E}(s,a)$, $\rho_{\pi}(s,a)$) | DE ($P_{E}(s^{\prime}&#124;s)$)
    | DE ($P_{E}(s^{\prime}&#124;s)$, $\mathcal{T}_{\pi}(s^{\prime}&#124;s,a)$, $\pi^{\prime}(a&#124;s^{\prime},s)$)
    | DE ($P_{E}(s)$) | DE $\mathcal{B}_{\pi}(s^{\prime\prime},a^{\prime\prime}&#124;s)$
    | G (策略网络) | G (轨迹规划器) |'
- en: '| Objective Type | KL | KL | KL | LIL | LIL | AIL | KL |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| 目标类型 | KL | KL | KL | LIL | LIL | AIL | KL |'
- en: '| Evaluation Task | MuJoCo | MuJoCo | MuJoCo | MuJoCo | Robotic Insertion |
    High-D & MuJoCo | HES-4D |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| 评估任务 | MuJoCo | MuJoCo | MuJoCo | MuJoCo | 机器人插入 | High-D & MuJoCo | HES-4D
    |'
- en: 'Table 6: Summary of NF-based IL algorithms. AR, CP, and ODE represent autoregressive,
    coupling, and ODE-based continuous flows, respectively. DE and G represent the
    two manners that NFs can be used, corresponding to the density estimator and generator,
    respectively. For DE, we specifically point out the densities estimated, for which
    the definitions are available in Section [5.1.1](#S5.SS1.SSS1 "5.1.1 Exact Density
    Estimation in Imitation Learning Using Normalizing Flows ‣ 5.1 Imitation Learning
    ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions").
    Regarding the types of IL objectives, KL, LIL, and AIL denote the KL divergence,
    Likelihood-based IL, and Adversarial IL, respectively. Examples for AIL include
    GAIL and AIRL, which are introduced in Section [4.1.1](#S4.SS1.SSS1 "4.1.1 Fundamental
    GAN-Based Imitation Learning Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"). For the benchmarks, MuJoCo (Todorov et al. ([2012](#bib.bib320)))
    provides a series of robotic locomotion tasks; Robotic Insertion (Vecerík et al.
    ([2017](#bib.bib328))) is a specific type of robotic manipulation task built on
    the MuJoCo engine; High-D (Krajewski et al. ([2018](#bib.bib173))) and HES-4D
    (Meyer et al. ([2019](#bib.bib222))) are two real-word driving dataset.'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '表6：基于NF的IL算法总结。AR、CP和ODE分别代表自回归、耦合和基于ODE的连续流。DE和G代表NFs可以使用的两种方式，分别对应密度估计器和生成器。对于DE，我们特别指出了估计的密度，定义见第[5.1.1](#S5.SS1.SSS1
    "5.1.1 Exact Density Estimation in Imitation Learning Using Normalizing Flows
    ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")节。关于IL目标类型，KL、LIL和AIL分别表示KL散度、基于似然的IL和对抗性IL。AIL的示例包括GAIL和AIRL，这些在第[4.1.1](#S4.SS1.SSS1
    "4.1.1 Fundamental GAN-Based Imitation Learning Algorithms: GAIL and AIRL ‣ 4.1
    Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")节中介绍。基准测试中，MuJoCo (Todorov et al. ([2012](#bib.bib320)))
    提供了一系列机器人运动任务；Robotic Insertion (Vecerík et al. ([2017](#bib.bib328))) 是基于MuJoCo引擎构建的特定类型的机器人操作任务；High-D
    (Krajewski et al. ([2018](#bib.bib173))) 和HES-4D (Meyer et al. ([2019](#bib.bib222)))
    是两个真实世界的驾驶数据集。'
- en: 'In Table [6](#S5.T6 "Table 6 ‣ 5.1.2 Policy Modeling in Imitation Learning
    with Normalizing Flows ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), we provide a summary of NF-based
    IL algorithms. For each algorithm, we provide key information including what type
    of and how the NF is utilized, its underlying IL framework, and evaluation tasks.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[6](#S5.T6 "Table 6 ‣ 5.1.2 Policy Modeling in Imitation Learning with Normalizing
    Flows ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")中，我们提供了NF-based IL算法的总结。对于每个算法，我们提供了包括NF的类型及使用方式、其基础IL框架和评估任务等关键信息。'
- en: 5.2 Reinforcement Learning with Offline Data
  id: totrans-513
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 离线数据的强化学习
- en: 'As mentioned in the beginning of Section [5](#S5 "5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), we broaden the scope of RL-related
    works to include both offline RL (Section [5.2.1](#S5.SS2.SSS1 "5.2.1 Adopting
    Normalizing Flows in Offline Reinforcement Learning ‣ 5.2 Reinforcement Learning
    with Offline Data ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) and online RL with offline data (Section [5.2.2](#S5.SS2.SSS2 "5.2.2
    Adopting Normalizing Flows in Online Reinforcement Learning with Offline Data
    ‣ 5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")). Similar with NF-based IL methods,
    algorithms in Section [5.2.1](#S5.SS2.SSS1 "5.2.1 Adopting Normalizing Flows in
    Offline Reinforcement Learning ‣ 5.2 Reinforcement Learning with Offline Data
    ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    and [5.2.2](#S5.SS2.SSS2 "5.2.2 Adopting Normalizing Flows in Online Reinforcement
    Learning with Offline Data ‣ 5.2 Reinforcement Learning with Offline Data ‣ 5
    Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") can
    be categorized based on the usage of NFs, i.e., working as either a density estimator
    or function generator.'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '正如[第5节](#S5 "5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")开始时提到的，我们将RL相关工作的范围扩大到包括离线RL（[5.2.1节](#S5.SS2.SSS1 "5.2.1 Adopting
    Normalizing Flows in Offline Reinforcement Learning ‣ 5.2 Reinforcement Learning
    with Offline Data ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")）和带有离线数据的在线RL（[5.2.2节](#S5.SS2.SSS2 "5.2.2 Adopting Normalizing Flows
    in Online Reinforcement Learning with Offline Data ‣ 5.2 Reinforcement Learning
    with Offline Data ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")）。类似于基于NF的IL方法，[5.2.1节](#S5.SS2.SSS1 "5.2.1 Adopting Normalizing Flows
    in Offline Reinforcement Learning ‣ 5.2 Reinforcement Learning with Offline Data
    ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")和[5.2.2节](#S5.SS2.SSS2
    "5.2.2 Adopting Normalizing Flows in Online Reinforcement Learning with Offline
    Data ‣ 5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")中的算法可以根据NFs的使用情况进行分类，即作为密度估计器或函数生成器。'
- en: 5.2.1 Adopting Normalizing Flows in Offline Reinforcement Learning
  id: totrans-515
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 在离线强化学习中采用归一化流
- en: 'Offline reinforcement learning aims to train a policy on a pre-recorded dataset,
    collected by the behavior policy $\mu(a|s)$, without any additional environment
    interactions. As introduced in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Background
    on Dynamic-Programming-based Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), major challenges in this setting include (1) extrapolation error
    caused by approximating the value of state-action pairs not well-covered by the
    training data and (2) distributional shift between behavior and inference policies.
    The common practice to tackle these problems is to induce conservatism, through
    either keeping the learned policies closer to the behavioral ones or constructing
    pessimistic value functions. However, this can lead to over-conservative, yet
    sub-optimal policies. By introducing Normalizing Flows, an agent can more effectively
    utilize offline data and circumvent out-of-distribution (OOD) actions through
    its generative (Akimov et al. ([2022](#bib.bib6)); Yang et al. ([2023d](#bib.bib376)))
    or density estimation capacity (Zhang et al. ([2023](#bib.bib386))).'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '离线强化学习旨在利用预先记录的数据集训练策略，这些数据集是通过行为策略 $\mu(a|s)$ 收集的，而无需额外的环境交互。如在第[3.1.1节](#S3.SS1.SSS1
    "3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")中介绍的，这种情况下的主要挑战包括（1）由于训练数据未覆盖的状态-动作对的值近似所导致的外推误差，以及（2）行为策略和推断策略之间的分布偏移。解决这些问题的常见做法是引入保守性，通过保持学习的策略接近行为策略或构建悲观的价值函数。然而，这可能会导致过于保守的，但仍然次优的策略。通过引入归一化流，代理可以更有效地利用离线数据，并通过其生成（Akimov
    等人 ([2022](#bib.bib6)); Yang 等人 ([2023d](#bib.bib376))) 或密度估计能力 (Zhang 等人 ([2023](#bib.bib386)))
    来规避分布外（OOD）动作。'
- en: 'CNF (Akimov et al. ([2022](#bib.bib6))) and LPD (Yang et al. ([2023d](#bib.bib376)))
    employ a similar algorithm idea – using NFs to extract latent variables underlying
    primitive actions in the offline dataset. Specifically, they train a Normalizing
    Flow $G$ conditioned on $s$ to model the mapping from $z$ to $a$, following $z\sim
    P_{Z}(\cdot),\ a=G(z|s)$, through supervised learning (i.e., Eq. ([19](#S2.E19
    "In 2.3 Normalizing Flows ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"))). After training, $G$ can work as an action decoder to convert a
    latent variable $z$ (following a simple base distribution) to its corresponding
    action. In this case, only a high-level policy $z\sim\pi_{\text{high}}(\cdot|s)$
    needs to be learned with offline RL, and the function composition $G\circ\pi_{\text{high}}$
    can work as a hierarchical policy mapping from $s$ to $a$. In particular, CNF
    and LPD are driven by different motivations. CNF pretrains a Real NVP conditioned
    on $s$ as the action decoder. To avoid OOD actions caused by the long tail effect,
    the latent space (i.e., the support of $P_{Z}(\cdot)$), which is also the output
    space of the high-level policy, is designed to be bounded, specifically an $n$-dim
    interval $(-1,1)^{n}$. As a result, the actor model (i.e., $G\circ\pi_{\text{high}}$)
    should be unable to generate OOD actions, even without clipping the high-level
    policy’s output (as in PLAS introduced in Section [3.1.3](#S3.SS1.SSS3 "3.1.3
    Addressing the Issue of Out-of-Distribution Actions Using VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), which avoids possible sub-optimality. LPD, instead, focuses on
    improving the offline RL performance in challenging, long-horizon tasks, by adopting
    NFs to model the transformation from $z$ to a fixed-length sequence of actions
    starting from $s$, i.e., a skill. Provable benefits can be gained when the learned
    skills are expressive enough to recover the original policy space (Yang et al.
    ([2023d](#bib.bib376))), and Normalizing Flows can facilitate the learning of
    such skills. With this pretrained transformation, the dataset can be relabeled
    in terms of skills as $D_{\text{high}}=[(s_{0}^{i},z^{i},\sum_{t=0}^{h-1}\gamma^{t}r^{i}_{t},s_{h}^{i})]_{i=1}^{N}$,
    where $h$ is the skill length and $z^{i}$ can be acquired as $G^{-1}(a^{i}_{0:h-1}|s_{0}^{i})=F(a^{i}_{0:h-1}|s_{0}^{i})$
    (i.e., via the normalizing direction of the NF). Then, offline RL can be adopted
    to learn $\pi_{\text{high}(z|s)}$ from $D_{\text{high}}$.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 'CNF (Akimov 等人 ([2022](#bib.bib6))) 和 LPD (Yang 等人 ([2023d](#bib.bib376)))
    使用了类似的算法思想——利用 NFs 从离线数据集中提取潜在变量，这些变量潜藏在原始动作之下。具体而言，他们训练一个条件于 $s$ 的 Normalizing
    Flow $G$ 来建模从 $z$ 到 $a$ 的映射，遵循 $z\sim P_{Z}(\cdot),\ a=G(z|s)$，通过监督学习（即，公式 ([19](#S2.E19
    "In 2.3 Normalizing Flows ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"))）。训练完成后，$G$ 可以作为一个动作解码器，将潜在变量 $z$（遵循简单的基本分布）转换为对应的动作。在这种情况下，只需要通过离线
    RL 学习一个高层策略 $z\sim\pi_{\text{high}}(\cdot|s)$，并且函数组合 $G\circ\pi_{\text{high}}$
    可以作为一个从 $s$ 到 $a$ 的层次策略映射。特别地，CNF 和 LPD 的驱动动机不同。CNF 预训练了一个条件于 $s$ 的 Real NVP 作为动作解码器。为了避免因长尾效应导致的
    OOD 动作，潜在空间（即，$P_{Z}(\cdot)$ 的支持集），也是高层策略的输出空间，设计为有界的，具体来说是一个 $n$-维区间 $(-1,1)^{n}$。因此，演员模型（即，$G\circ\pi_{\text{high}}$）应该无法生成
    OOD 动作，即使没有对高层策略的输出进行剪裁（如在第 [3.1.3](#S3.SS1.SSS3 "3.1.3 Addressing the Issue of
    Out-of-Distribution Actions Using VAEs ‣ 3.1 Offline Reinforcement Learning ‣
    3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    节中介绍的 PLAS），这避免了可能的次优性。相反，LPD 侧重于通过采用 NFs 来建模从 $z$ 到以 $s$ 为起点的固定长度动作序列（即，技能）的转换，从而提高在具有挑战性的长时间任务中的离线
    RL 性能。当学习到的技能足够表达，以恢复原始策略空间时，可以获得可证明的好处（Yang 等人 ([2023d](#bib.bib376)))，而 Normalizing
    Flows 可以促进这种技能的学习。利用这种预训练的转换，数据集可以以技能的形式重新标记为 $D_{\text{high}}=[(s_{0}^{i},z^{i},\sum_{t=0}^{h-1}\gamma^{t}r^{i}_{t},s_{h}^{i})]_{i=1}^{N}$，其中
    $h$ 是技能长度，$z^{i}$ 可以通过 $G^{-1}(a^{i}_{0:h-1}|s_{0}^{i})=F(a^{i}_{0:h-1}|s_{0}^{i})$（即，通过
    NF 的归一化方向）获得。然后，可以采用离线 RL 从 $D_{\text{high}}$ 学习 $\pi_{\text{high}(z|s)}$。'
- en: 'APAC (Zhang et al. ([2023](#bib.bib386))) adopts NFs (specifically Flow-GAN
    (Grover et al. ([2018](#bib.bib111)))) as density estimators, rather than action
    decoders as in aforementioned algorithms, to filter out OOD actions while avoiding
    being over-conservative. To be specific, the density of the point $(s,a)$ can
    be approximated as $P_{Z}(G^{-1}(a|s))$, where $G$ is the NF: $a=G(z|s)$. Points
    with higher density are more likely to be generated by the behavior policy and
    so can be viewed as “safe” in-distribution points for training use. To be specific,
    they modify the policy improvement step in Eq. ([32](#S3.E32 "In 3.1.1 Background
    on Dynamic-Programming-based Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) as:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 'APAC（Zhang et al. ([2023](#bib.bib386))) 采用 NFs（特别是 Flow-GAN（Grover et al.
    ([2018](#bib.bib111))））作为密度估计器，而不是上述算法中的行动解码器，以筛选 OOD 行动，同时避免过于保守。具体而言，点 $(s,a)$
    的密度可以被近似为 $P_{Z}(G^{-1}(a|s))$，其中 $G$ 是 NF：$a=G(z|s)$。具有较高密度的点更可能是由行为策略生成的，因此可以被视为用于训练的“安全”分布内点。具体来说，他们修改了
    Eq. ([32](#S3.E32 "In 3.1.1 Background on Dynamic-Programming-based Offline Reinforcement
    Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) 中的策略改进步骤：'
- en: '|  | $\displaystyle\pi_{k+1}=\arg\max_{\pi}\mathbb{E}_{s\sim D_{\mu}}\left[\mathbb{E}_{a\sim\pi(\cdot&#124;s)}Q_{k+1}^{\pi}(s,a)\right]\
    s.t.\ P_{Z}(G^{-1}(a&#124;s))>\epsilon(s,a).$ |  | (81) |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\pi_{k+1}=\arg\max_{\pi}\mathbb{E}_{s\sim D_{\mu}}\left[\mathbb{E}_{a\sim\pi(\cdot\mid
    s)}Q_{k+1}^{\pi}(s,a)\right]\ \text{s.t.}\ P_{Z}(G^{-1}(a\mid s))>\epsilon(s,a).$
    |  | (81) |'
- en: 'where $\epsilon(s,a)$ is a prefixed threshold. In this way, samples for updating
    the policy are restricted within the safe area, which could contain both observed
    and unobserved points in the offline dataset. Thus, the policy can potentially
    perform exploration out of the given dataset and avoid visiting OOD actions, simultaneously.
    Note that this constraint on $\pi$ is more relaxed and practical compared with
    the one of BEAR (introduced in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Background
    on Dynamic-Programming-based Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")): $\pi\in\{\pi^{\prime}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\
    |\ \pi^{\prime}(a|s)=0\ \text{whenever}\ \mu(a|s)<\epsilon\}$, to avoid being
    over-conservative.'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\epsilon(s,a)$ 是一个预设的阈值。通过这种方式，用于更新策略的样本被限制在安全区域内，该区域可以包含离线数据集中观察到的点和未观察到的点。因此，策略可以在给定的数据集之外进行探索，同时避免访问
    OOD 行动。需要注意的是，与 BEAR 的限制（见第 [3.1.1](#S3.SS1.SSS1 "3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") 节）相比，对
    $\pi$ 的这种约束更加宽松和实际：$\pi\in\{\pi^{\prime}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\
    |\ \pi^{\prime}(a|s)=0\ \text{whenever}\ \mu(a|s)<\epsilon\}$，以避免过于保守。'
- en: 5.2.2 Adopting Normalizing Flows in Online Reinforcement Learning with Offline
    Data
  id: totrans-521
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 在具有离线数据的在线强化学习中采用归一化流
- en: To deepen the understanding of applying NFs for RL, we incorporate another line
    of work that merges online RL with offline data. Still, these works can be categorized
    as utilizing NFs as potent generators (Mazoure et al. ([2019](#bib.bib217)); Yan
    et al. ([2022](#bib.bib366)); Slack et al. ([2022](#bib.bib297))) or density estimators
    (Wu et al. ([2021](#bib.bib357))). From another perspective, these studies either
    extract useful information from the offline data and subsequently apply online
    RL atop it (Yan et al. ([2022](#bib.bib366)); Slack et al. ([2022](#bib.bib297));
    Wu et al. ([2021](#bib.bib357))), or they alternate between these two processes
    till the end of training (e.g., conducting off-policy RL) (Mazoure et al. ([2019](#bib.bib217))).
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深化对将 NFs 应用于 RL 的理解，我们结合了另一项将在线 RL 与离线数据结合的工作。然而，这些工作可以归类为利用 NFs 作为强大的生成器（Mazoure
    et al. ([2019](#bib.bib217)); Yan et al. ([2022](#bib.bib366)); Slack et al. ([2022](#bib.bib297)))
    或密度估计器（Wu et al. ([2021](#bib.bib357)))。从另一个角度来看，这些研究要么从离线数据中提取有用的信息，并随后在其上应用在线
    RL（Yan et al. ([2022](#bib.bib366)); Slack et al. ([2022](#bib.bib297)); Wu et
    al. ([2021](#bib.bib357)))，要么在训练结束之前在这两个过程之间交替进行（例如，进行离策略 RL）（Mazoure et al. ([2019](#bib.bib217)))。
- en: '| Algorithm | CNF | LPD | APAC | SAC-NF | CEIP | SAFER | NF Shaping |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | CNF | LPD | APAC | SAC-NF | CEIP | SAFER | NF Shaping |'
- en: '| Flow | Real NVP | Real NVP | Flow-GAN | IAF | Real NVP | Real NVP | MAF |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| 流 | 实值NVP | 实值NVP | 流-GAN | IAF | 实值NVP | 实值NVP | MAF |'
- en: '| Flow Type | CP | CP | CP | AR | CP | CP | AR |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| 流类型 | CP | CP | CP | AR | CP | CP | AR |'
- en: '| Flow Usage | G (Action Decoder) | G (Skill Decoder) | DE ($P_{G}(s,a)$) |
    G (Policy Network) | G (Action Decoder) | G (Action Decoder) | DE ($P_{G}(s,a)$)
    |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| 流使用 | G (动作解码器) | G (技能解码器) | DE ($P_{G}(s,a)$) | G (策略网络) | G (动作解码器) |
    G (动作解码器) | DE ($P_{G}(s,a)$) |'
- en: '| Evaluation Task | D4RL (L, M) | D4RL (M, A, K) | D4RL (L, M) | MuJoCo | Kitchen,
    Office, FetchReach | Operation | Robotic Insertion, Pick Place |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| 评估任务 | D4RL (L, M) | D4RL (M, A, K) | D4RL (L, M) | MuJoCo | 厨房、办公室、抓取到达
    | 操作 | 机器人插入、拾取放置 |'
- en: 'Table 7: Summary of NF-based (Offline) RL algorithms. AR and CP represent autoregressive
    and coupling flows, respectively. DE and G represent the two manners that NFs
    can be used, corresponding to the density estimator and generator, respectively.
    Specifically, $P_{G}(s,a)$ denotes the state-action pair distribution in the offline
    dataset. For the benchmarks, all the evaluation tasks shown in this table are
    built on the MuJoCo (Todorov et al. ([2012](#bib.bib320))) engine; D4RL (Fu et al.
    ([2020](#bib.bib99))) provides a bunch of challenging (robotic) tasks specifically
    for offline RL, including Locomotion (L), AntMaze (M), Adroit (A), Kitchen (K);
    Office (Pertsch et al. ([2021](#bib.bib252))), FetchReach (Plappert et al. ([2018](#bib.bib254))),
    Kitchen (from D4RL), Robotic Insertion, Pick Place (Vecerík et al. ([2017](#bib.bib328))),
    and Operation (Slack et al. ([2022](#bib.bib297))) all involve the manipulation
    of a robot arm.'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：基于NF的（离线）RL算法总结。AR 和 CP 分别代表自回归和耦合流。DE 和 G 代表了NFs的两种使用方式，分别对应于密度估计器和生成器。具体来说，$P_{G}(s,a)$
    表示离线数据集中状态-动作对的分布。对于基准测试，本表中显示的所有评估任务都建立在MuJoCo（Todorov等人 ([2012](#bib.bib320)))
    引擎上；D4RL（Fu等人 ([2020](#bib.bib99))) 提供了一系列专门针对离线RL的具有挑战性的（机器人）任务，包括步态（L）、蚂蚁迷宫（M）、Adroit（A）、厨房（K）；办公室（Pertsch等人
    ([2021](#bib.bib252)))、抓取到达（Plappert等人 ([2018](#bib.bib254)))、厨房（来自D4RL）、机器人插入、拾取放置（Vecerík等人
    ([2017](#bib.bib328))) 和操作（Slack等人 ([2022](#bib.bib297))) 都涉及机器人臂的操作。
- en: 'With the same insights as Flow DAC (introduced in Section [5.1](#S5.SS1 "5.1
    Imitation Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), SAC-NF (Mazoure et al. ([2019](#bib.bib217))) extends SAC, an off-policy
    RL algorithm (Haarnoja et al. ([2018](#bib.bib117))), by adopting NFs (IAF (Kingma
    et al. ([2016](#bib.bib167)))) as the policy network, which could potentially
    improve exploration and support multi-modal action distributions. On the other
    hand, CEIP (Yan et al. ([2022](#bib.bib366))) and SAFER (Slack et al. ([2022](#bib.bib297)))
    leverage the offline data by first learning a transformation from latent variables
    $z$ to actions $a$ using NFs as an action decoder and then training a high-level
    policy $\pi_{\text{high}}(z|s)$ atop the transformation function with (online)
    RL, akin to CNF. Yet, in comparison to CNF, they extend the action decoder learning
    from different perspectives. In CEIP, the generator is conditioned on a concatenation
    of the current and next states, i.e., $u=[s,s^{\prime}]$, instead of the state
    alone, which can better inform the agent of the state it should try to achieve
    with its current action. Moreover, they propose a manner to utilize data from
    various yet related tasks, which are more accessible than task-specific ones.
    Specifically, they first train a flow for each task $i$ with corresponding demonstrations.
    The generator is defined as $a=G_{i}(z|u)=\exp\{c_{i}(u)\}\odot z+d_{i}(u)$ where
    $c_{i}$ and $d_{i}$ are trainable deep nets, $\odot$ refers to the Hadamard product.
    This structure design follows Real NVP (Dinh et al. ([2017](#bib.bib73))). Subsequently,
    they train a combination flow on demonstrations of the target task, with the generator
    defined as $G(z|u)=\left(\sum_{i=1}^{n}\mu_{i}(u)\exp\{c_{i}(u)\}\right)\odot
    z+\left(\sum_{i=1}^{n}\lambda_{i}(u)d_{i}(u)\right)$. At this stage, they fix
    $c_{i}$ and $d_{i}$ ($i=1,\cdots,n$) and train the weighting functions $\mu_{i}$
    and $\lambda_{i}$ to optimize the combination flow for the designated task via
    supervised learning (i.e., Eq. ([19](#S2.E19 "In 2.3 Normalizing Flows ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))). In this way, they
    efficiently utilize demonstrations from related tasks. While, SAFER emphasizes
    the safety aspect of the learned transformation. Specifically, the transformation
    should prevent selecting unsafe actions $a_{\text{unsafe}}$ in the environment,
    which are labeled in the training dataset. To achieve this, the authors propose
    conditioning the flow on both the state $s$ and a safety context $c$. It’s important
    to note that $c$ is not provided directly but must be inferred from the information
    available in the environment. Thus, SAFER simultaneously learns (1) an inference
    network $P_{\phi}(c|\Lambda)$ to determine the current safety context $c$ based
    on a sliding window of states $\Lambda$ and (2) a generator $a=G_{\theta}(z|s,c)$
    (Real NVP) conditioned on $s$ and $c$, with the following objective:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 与Flow DAC（在第
- en: '|  | $\max_{\theta,\phi}\mathbb{E}_{s,a,a_{\text{unsafe}}\sim D_{\mu},\ c\sim
    P_{\phi}(\cdot&#124;\Lambda)}\left[\log P_{\theta}(a&#124;s,c)-\lambda\log P_{\theta}(a_{\text{unsafe}}&#124;s,c)\right]-D_{KL}(P_{\phi}(c&#124;\Lambda)&#124;&#124;P_{C}(c))$
    |  | (82) |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\theta,\phi}\mathbb{E}_{s,a,a_{\text{unsafe}}\sim D_{\mu},\ c\sim
    P_{\phi}(\cdot| \Lambda)}\left[\log P_{\theta}(a|s,c)-\lambda\log P_{\theta}(a_{\text{unsafe}}|s,c)\right]-D_{KL}(P_{\phi}(c|
    \Lambda) || P_{C}(c))$ |  | (82) |'
- en: Here, $D_{\mu}$ is the training dataset, $P_{\theta}(a|s,c)=P_{Z}(G^{-1}_{\theta}(a|s,c))$,
    $P_{C}(c)$ denotes the assumed prior distribution of $c$. Intuitively, the first
    two terms encourage safe actions while deterring unsafe ones, and, in conjunction
    with the final term, the variable $c$ is compelled to encapsulate useful information
    regarding safety. This algorithm serves as an illustration of how NFs and VAEs
    can be integrated. In this setup, $P_{\phi}$ and $P_{\theta}$ function as the
    encoder and decoder of the VAE, respectively, with $P_{\theta}$ being implemented
    as a Normalizing Flow.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$D_{\mu}$ 是训练数据集，$P_{\theta}(a|s,c)=P_{Z}(G^{-1}_{\theta}(a|s,c))$，$P_{C}(c)$
    表示 $c$ 的假设先验分布。直观上，前两个项鼓励安全行为，同时抑制不安全行为，并且，与最后一个项结合，变量 $c$ 被迫包含有关安全性的有用信息。该算法展示了
    NFs 和 VAEs 如何集成。在这个设置中，$P_{\phi}$ 和 $P_{\theta}$ 分别作为 VAE 的编码器和解码器，其中 $P_{\theta}$
    实现为一个 Normalizing Flow。
- en: Unlike SAC-NF, CEIP, and SAFER, NF Shaping (Wu et al. ([2021](#bib.bib357)))
    adopts NFs as density estimators. This is a method that combines (online) reinforcement
    and imitation learning by shaping the reward function with a state-and-action-dependent
    potential learned from demonstrations. To be specific, they first train an NF
    (specifically MAF Papamakarios et al. ([2017](#bib.bib239))) to estimate the density
    of state-action pairs in the demonstration dataset, i.e., $P_{G}(s,a)=P_{Z}(G^{-1}(a|s))$.
    Then, they shape the reward function as $\widetilde{r}_{t}=r(s_{t},a_{t},s_{t+1})+\gamma\Phi(s_{t+1},a_{t+1})-\Phi(s_{t},a_{t})$,
    where $\Phi(s,a)=\beta\log(P_{G}(s,a)+\epsilon)$, $\beta>0$ adjusts the weight,
    and $\epsilon>0$ is a small constant to prevent numerical issues. According to
    (Ng et al. ([1999](#bib.bib229)); Wiewiora ([2003](#bib.bib354))), reward shaping
    can intensify the reward and greatly improve the learning efficiency. Intuitively,
    this potential term highlights regions where the demonstrated policy visits more
    frequently, signaling areas that should be explored more.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 与 SAC-NF、CEIP 和 SAFER 不同，NF Shaping (Wu et al. ([2021](#bib.bib357))) 采用了 NFs
    作为密度估计器。这是一种通过用从示例中学习到的状态和动作相关的潜力来塑造奖励函数，从而结合（在线）强化学习和模仿学习的方法。具体而言，他们首先训练一个 NF（特别是
    MAF Papamakarios et al. ([2017](#bib.bib239))) 来估计示例数据集中状态-动作对的密度，即 $P_{G}(s,a)=P_{Z}(G^{-1}(a|s))$。然后，他们将奖励函数塑造为
    $\widetilde{r}_{t}=r(s_{t},a_{t},s_{t+1})+\gamma\Phi(s_{t+1},a_{t+1})-\Phi(s_{t},a_{t})$，其中
    $\Phi(s,a)=\beta\log(P_{G}(s,a)+\epsilon)$，$\beta>0$ 调整权重，$\epsilon>0$ 是一个小常数，用于防止数值问题。根据
    (Ng et al. ([1999](#bib.bib229)); Wiewiora ([2003](#bib.bib354)))，奖励塑造可以增强奖励并大大提高学习效率。直观来说，这个潜力项突出了示例策略更频繁访问的区域，标志着需要更多探索的区域。
- en: 'To sum up, it’s noteworthy that among all the aforementioned works, the algorithms
    proposed in (Akimov et al. ([2022](#bib.bib6)); Yang et al. ([2023d](#bib.bib376));
    Yan et al. ([2022](#bib.bib366)); Slack et al. ([2022](#bib.bib297))) adopt similar
    ideas. They first learn an NF-based transformation from a latent space to the
    real action space based on the offline data. Subsequently, they train a high-level
    policy on the latent variables, rather than primitive actions, with online or
    offline RL. The prelearned transformation constitutes a mapping from a simpler,
    more controllable distribution to the target distribution, which is usually complex
    and unknown. Moreover, we can see that Normalizing Flows are primarily used as
    exact density estimators in IL (Section [5.1](#S5.SS1 "5.1 Imitation Learning
    ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    while mainly functioning as expressive generators in (Offline) RL (Section [5.2](#S5.SS2
    "5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")). We further provide Table [6](#S5.T6
    "Table 6 ‣ 5.1.2 Policy Modeling in Imitation Learning with Normalizing Flows
    ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") and [7](#S5.T7 "Table 7 ‣ 5.2.2 Adopting Normalizing Flows
    in Online Reinforcement Learning with Offline Data ‣ 5.2 Reinforcement Learning
    with Offline Data ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") as summaries for NF-based IL and RL algorithms. It can be observed
    that most works adopt autoregressive or coupling flows, which is probably due
    to their computation efficiency. While almost all algorithms are evaluated on
    robotic benchmarks such as MuJoCo or D4RL, there is a notable absence of comparative
    analysis among these algorithms. Given the diverse perspectives from which these
    algorithms are developed, it is challenging to compare them at the algorithmic
    level, thus comprehensive and fair empirical evaluations are essential for practical
    applications.'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，值得注意的是，在所有上述工作中，（Akimov et al. ([2022](#bib.bib6)); Yang et al. ([2023d](#bib.bib376));
    Yan et al. ([2022](#bib.bib366)); Slack et al. ([2022](#bib.bib297))) 提出的算法采用了类似的理念。它们首先从离线数据中学习一种基于NF的从潜在空间到实际动作空间的变换。随后，它们在潜在变量上训练高级策略，而不是原始动作，使用在线或离线RL。预学习的变换构成了从更简单、更可控的分布到目标分布的映射，而目标分布通常是复杂且未知的。此外，我们可以看到，Normalizing
    Flows主要在IL中作为精确的密度估计器使用（第[5.1节](#S5.SS1 "5.1 Imitation Learning ‣ 5 Normalizing
    Flows in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")），而在（离线）RL中主要作为表达性生成器使用（第[5.2节](#S5.SS2
    "5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")）。我们进一步提供了表[6](#S5.T6 "Table 6
    ‣ 5.1.2 Policy Modeling in Imitation Learning with Normalizing Flows ‣ 5.1 Imitation
    Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")和[7](#S5.T7
    "Table 7 ‣ 5.2.2 Adopting Normalizing Flows in Online Reinforcement Learning with
    Offline Data ‣ 5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")，作为基于NF的IL和RL算法的总结。可以观察到，大多数工作采用了自回归或耦合流，这可能是由于它们的计算效率。虽然几乎所有算法都在诸如MuJoCo或D4RL这样的机器人基准上进行评估，但这些算法之间的对比分析明显缺失。鉴于这些算法从不同的角度出发进行开发，算法级别的比较具有挑战性，因此全面而公正的实证评估对于实际应用至关重要。'
- en: 6 Transformers in Offline Policy Learning
  id: totrans-534
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 Transformers在离线策略学习中的应用
- en: 'Transformers can be used for offline RL (Section [6.1](#S6.SS1 "6.1 Offline
    Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")) or IL (Section [6.2](#S6.SS2 "6.2 Imitation Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) by modeling them as
    a problem of predicting the next token based on historical data. Specifically,
    most works in this section follow a similar algorithm design with Decision Transformer
    – a seminal work of trajectory-optimization-based offline RL. Thus, we start with
    an introduction of the background on trajectory-optimization-based offline RL,
    and then provide a comprehensive review on transformer-based offline RL and IL
    algorithms.'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器可以用于离线强化学习（第[6.1](#S6.SS1 "6.1 离线强化学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向展望")节）或模仿学习（第[6.2](#S6.SS2
    "6.2 模仿学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向展望")节），通过将其建模为基于历史数据预测下一个标记的问题。具体来说，本节中的大多数工作遵循与决策变换器类似的算法设计——这是轨迹优化基础上的离线强化学习的开创性工作。因此，我们首先介绍轨迹优化基础上的离线强化学习背景，然后对基于变换器的离线强化学习和模仿学习算法进行全面回顾。
- en: 6.1 Offline Reinforcement Learning
  id: totrans-536
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 离线强化学习
- en: 'The content of this section is arranged as follows. First, we highlight pioneering
    studies on transformer-based offline RL in Section [6.1.1](#S6.SS1.SSS1 "6.1.1
    Background on Trajectory-Optimization-based Offline Reinforcement Learning ‣ 6.1
    Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), along with a series of follow-up works for improvements
    (i.e., Section [6.1.2](#S6.SS1.SSS2 "6.1.2 Balancing Model Capacity with Training
    Data ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") and [6.1.3](#S6.SS1.SSS3 "6.1.3 Mitigating Impacts from
    Environmental Stochasticity ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")). Subsequently, we explore
    the use of transformers in broader problem scenarios in Section [6.1.4](#S6.SS1.SSS4
    "6.1.4 Transformers in Extended Offline Reinforcement Learning Setups ‣ 6.1 Offline
    Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), such as multi-agent and multi-task RL. Specifically, for multi-task
    settings, we focus on the development of generalist agents based on pretraining
    and fine-tuning. Last, we reflect on existing problems in this field and discuss
    potential future research directions in Section [6.1.5](#S6.SS1.SSS5 "6.1.5 Reflections
    on Transformer-based Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement
    Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions").'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容安排如下。首先，在[6.1.1](#S6.SS1.SSS1 "6.1.1 轨迹优化基础上的离线强化学习背景 ‣ 6.1 离线强化学习 ‣ 6 离线策略学习中的变换器
    ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向展望")节中，我们突出了基于变换器的离线强化学习的开创性研究，以及一系列后续改进工作（即[6.1.2](#S6.SS1.SSS2
    "6.1.2 模型容量与训练数据的平衡 ‣ 6.1 离线强化学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向展望")和[6.1.3](#S6.SS1.SSS3
    "6.1.3 减轻环境随机性的影响 ‣ 6.1 离线强化学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向展望")。随后，我们在[6.1.4](#S6.SS1.SSS4
    "6.1.4 扩展离线强化学习设置中的变换器 ‣ 6.1 离线强化学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向展望")节中探讨变换器在更广泛问题场景中的应用，例如多智能体和多任务强化学习。具体来说，对于多任务设置，我们关注基于预训练和微调的通用智能体的发展。最后，我们在[6.1.5](#S6.SS1.SSS5
    "6.1.5 对基于变换器的离线强化学习的反思 ‣ 6.1 离线强化学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向展望")节中反思该领域存在的问题，并讨论潜在的未来研究方向。
- en: 6.1.1 Background on Trajectory-Optimization-based Offline Reinforcement Learning
  id: totrans-538
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 轨迹优化基础的离线强化学习背景
- en: 'Inspired by the great success of high-capacity sequence generation models (such
    as the transformer) in natural language processing (NLP), Chen et al. ([2021b](#bib.bib46))
    proposes to view offline RL as a sequence modeling problem and solve it with a
    Decision Transformer (DT). This algorithm exemplifies trajectory-optimization-based
    offline RL, so we provide a detailed introduction of it, along with two other
    seminal works, as a brief tutorial on this particular branch of offline RL methods.
    In particular, given offline trajectories $\{\tau=\left(s_{0},a_{0},R_{0},\cdots,s_{T},a_{T},R_{T}\right)\}$,
    where $R_{t}=\sum_{i=t}^{T}r_{i}$ denotes the return-to-go (RTG), the DT is trained
    to predict the next action based on the previous $k+1$ transitions:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 受高容量序列生成模型（如 Transformer）在自然语言处理（NLP）中的巨大成功启发，Chen 等人 ([2021b](#bib.bib46))
    提出将离线强化学习视为一个序列建模问题，并使用决策 Transformer（DT）来解决。该算法是基于轨迹优化的离线强化学习的典型例子，因此我们提供了详细的介绍，连同其他两个开创性工作，作为这个离线强化学习方法特定分支的简要教程。特别地，给定离线轨迹
    $\{\tau=\left(s_{0},a_{0},R_{0},\cdots,s_{T},a_{T},R_{T}\right)\}$，其中 $R_{t}=\sum_{i=t}^{T}r_{i}$
    表示未来回报（RTG），DT 被训练来基于之前的 $k+1$ 个过渡预测下一步动作：
- en: '|  | $\min_{\pi}\mathbb{E}_{\tau}\left[\sum_{t=0}^{T}(a_{t}-\pi(\tau_{t-k:t}))^{2}\right],\
    \text{or}\ \min_{\pi}\mathbb{E}_{\tau}\left[\sum_{t=1}^{T}-\log\pi(a_{t}&#124;\tau_{t-k:t})\right]$
    |  | (83) |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\pi}\mathbb{E}_{\tau}\left[\sum_{t=0}^{T}(a_{t}-\pi(\tau_{t-k:t}))^{2}\right],\
    \text{or}\ \min_{\pi}\mathbb{E}_{\tau}\left[\sum_{t=1}^{T}-\log\pi(a_{t}&#124;\tau_{t-k:t})\right]$
    |  | (83) |'
- en: 'where $\tau_{t-k:t}=(s_{j},a_{j},R_{j},\cdots,s_{t},R_{t})$ ($j=\min(t-k,0)$)
    is the input sequence; the two terms above are for tasks with continuous and discrete
    actions, respectively. DT adopts a GPT-like architecture (Radford et al. ([2018](#bib.bib265))),
    which is a decoder-only transformer as introduced in Section [2.4](#S2.SS4 "2.4
    Transformers ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions").
    During evaluation rollouts, an initial target return $R_{0}$ must be specified,
    which can be the highest achievable return for the task. The inference trajectory
    is then generated autoregressively using the DT as follows: $s_{0}\sim\rho_{0}(\cdot)$,
    $a_{0}\sim\pi(\cdot|s_{0},R_{0})$, $s_{1}\sim\mathcal{T}(\cdot|s_{0},a_{0})$,
    $r_{0}=r(s_{0},a_{0},s_{1}),\ \cdots$, $R_{t}=R_{t-1}-r_{t-1}$, $a_{t}\sim\pi(\cdot|\tau_{t-k:t}),\
    \cdots$, where $(\rho_{0},\mathcal{T},r)$ are components of the MDP. DT greatly
    simplifies offline RL by eliminating the necessity to fit value functions through
    dynamic programming or to compute policy gradients as detailed in Section [3.1.1](#S3.SS1.SSS1
    "3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"). Using such a (supervised-learning
    based) sequence modeling objective makes it less prone to selecting OOD actions,
    because multiple state and action anchors throughout the trajectory prevent the
    learned policy from deviating too far from the behavior policy $\mu(a|s)$. Additionally,
    as a high-capacity model, the transformer has the potential to enhance both the
    scalability and robustness of the learned policy, similar to its applications
    in NLP. Per Chen et al. ([2021b](#bib.bib46)), DT shows competitive performance
    compared with prior offline RL methods (such as Conservative Q-Learning (CQL,
    Kumar et al. ([2020](#bib.bib176)))) especially for tasks with sparse and delayed
    reward functions. However, Emmons et al. ([2022](#bib.bib85)) investigate what
    is essential for offline RL via supervised learning (RvS) through extensive experiments.
    Their findings indicate that a two-layer MLP model, utilizing a simpler input
    format (i.e., $(s_{t},R_{t})$ instead of $\tau_{t-k:t}$) and the same objective,
    exhibits competitive, and in some cases superior, performance compared to DT,
    and DT underperform prior offline RL algorithms on most benchmark tasks. This
    raises the questions whether it is necessary to adopt such a high-capacity model,
    i.e., the transformer, as the policy network, and under what scenarios RvS methods
    might outperform dynamic-programming-based offline RL methods.'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\tau_{t-k:t}=(s_{j},a_{j},R_{j},\cdots,s_{t},R_{t})$ ($j=\min(t-k,0)$)
    是输入序列；上述两个术语分别用于连续和离散动作任务。DT 采用 GPT-like 架构 (Radford et al. ([2018](#bib.bib265)))，这是一个仅包含解码器的变换器，如第
    [2.4](#S2.SS4 "2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 节所介绍。在评估回合中，必须指定初始目标回报 $R_{0}$，这可以是任务中可实现的最高回报。然后，使用 DT
    自回归地生成推断轨迹，如下所示：$s_{0}\sim\rho_{0}(\cdot)$, $a_{0}\sim\pi(\cdot|s_{0},R_{0})$,
    $s_{1}\sim\mathcal{T}(\cdot|s_{0},a_{0})$, $r_{0}=r(s_{0},a_{0},s_{1}),\ \cdots$,
    $R_{t}=R_{t-1}-r_{t-1}$, $a_{t}\sim\pi(\cdot|\tau_{t-k:t}),\ \cdots$，其中 $(\rho_{0},\mathcal{T},r)$
    是 MDP 的组件。DT 通过消除通过动态规划拟合价值函数或计算策略梯度的必要性，大大简化了离线 RL，如第 [3.1.1](#S3.SS1.SSS1 "3.1.1
    Background on Dynamic-Programming-based Offline Reinforcement Learning ‣ 3.1 Offline
    Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 节所述。使用这种（基于监督学习的）序列建模目标使其不易选择 OOD 动作，因为整个轨迹中的多个状态和动作锚点防止了学习到的策略偏离行为策略
    $\mu(a|s)$ 太远。此外，作为一个高容量模型，变换器有潜力提升学习到的策略的可扩展性和鲁棒性，类似于其在 NLP 中的应用。根据 Chen et al.
    ([2021b](#bib.bib46))，DT 与之前的离线 RL 方法（如 Conservative Q-Learning (CQL, Kumar et
    al. ([2020](#bib.bib176)))) 相比，表现具有竞争力，特别是在具有稀疏和延迟奖励函数的任务中。然而，Emmons et al. ([2022](#bib.bib85))
    通过大量实验研究了通过监督学习（RvS）进行离线 RL 的关键因素。他们的发现表明，使用更简单输入格式（即 $(s_{t},R_{t})$ 而非 $\tau_{t-k:t}$）和相同目标的两层
    MLP 模型在性能上具有竞争力，在某些情况下甚至优于 DT，并且 DT 在大多数基准任务上表现不如之前的离线 RL 算法。这引发了是否有必要采用如此高容量的模型，即变换器，作为策略网络的问题，以及在什么情况下
    RvS 方法可能会超越基于动态规划的离线 RL 方法。'
- en: 'Trajectory Transformer (TT, Janner et al. ([2021](#bib.bib149))) follows DT
    but utilizes more techniques from NLP, including tokenization, discretization,
    and beam search. In particular, they treat each dimension of the state and action
    as a token and discretize them independently. Suppose the state and action have
    $N$ and $M$ dimensions respectively, the objective is $\min_{\pi}\mathbb{E}_{\tau}\left[\mathcal{L}(\tau)\right]$,
    where $\mathcal{L}(\tau)$ is defined as follows:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹变换器（TT，Janner 等人（[2021](#bib.bib149)））遵循 DT，但利用了更多来自 NLP 的技术，包括分词、离散化和束搜索。特别地，他们将状态和动作的每个维度视为一个标记，并独立离散化它们。假设状态和动作分别具有
    $N$ 和 $M$ 维度，目标是 $\min_{\pi}\mathbb{E}_{\tau}\left[\mathcal{L}(\tau)\right]$，其中
    $\mathcal{L}(\tau)$ 定义如下：
- en: '|  | $\sum_{t=0}^{T}\left[\sum_{i=1}^{N}\log\pi(s_{t}^{i}&#124;s_{t}^{<i},\tau_{<t})+\sum_{j=1}^{M}\log\pi(a_{t}^{j}&#124;a_{t}^{<j},s_{t},\tau_{<t})+\log\pi(r_{t}&#124;a_{t},s_{t},\tau_{<t})+\log\pi(R_{t}&#124;r_{t},a_{t},s_{t},\tau_{<t})\right]$
    |  | (84) |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{t=0}^{T}\left[\sum_{i=1}^{N}\log\pi(s_{t}^{i}&#124;s_{t}^{<i},\tau_{<t})+\sum_{j=1}^{M}\log\pi(a_{t}^{j}&#124;a_{t}^{<j},s_{t},\tau_{<t})+\log\pi(r_{t}&#124;a_{t},s_{t},\tau_{<t})+\log\pi(R_{t}&#124;r_{t},a_{t},s_{t},\tau_{<t})\right]$
    |  | (84) |'
- en: This objective involves supervision on each dimension of the state/action, reward,
    and return-to-go. As for the inference process, a beam search technique (Freitag
    & Al-Onaizan ([2017](#bib.bib95))) is utilized. Specifically, $B$ most-likely
    samples are kept when sampling each of $a_{t}^{j},R_{t}$ sequentially. Then, samples
    $(a_{t}^{1:M},R_{t})$ with the highest cumulative reward plus return-to-go (i.e,
    the estimated trajectory return $\sum_{i=0}^{t-1}r_{i}+R_{t}$) is selected for
    execution. Note that, during the evaluation, $s_{t}^{1:N}$ and $r_{t}$ can be
    acquired from the simulator. TT shows superior performance than DT in some benchmarks
    as reported in (Janner et al. ([2021](#bib.bib149))), but treating each dimension
    separately would introduce learning and sample inefficiencies, especially for
    high-dimensional tasks.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 这一目标涉及对状态/动作、奖励和回报的每个维度的监督。至于推断过程，使用了一种束搜索技术（Freitag & Al-Onaizan（[2017](#bib.bib95)））。具体来说，在顺序采样每个
    $a_{t}^{j},R_{t}$ 时，保留 $B$ 个最可能的样本。然后，选择累积奖励加上回报（即，估计的轨迹回报 $\sum_{i=0}^{t-1}r_{i}+R_{t}$）最高的样本
    $(a_{t}^{1:M},R_{t})$ 进行执行。注意，在评估过程中，$s_{t}^{1:N}$ 和 $r_{t}$ 可以从模拟器中获取。TT 在一些基准测试中显示出优于
    DT 的性能，如（Janner 等人（[2021](#bib.bib149)））所报告的，但将每个维度单独处理会引入学习和样本效率低下，尤其是在高维任务中。
- en: Instead of modeling the policy, Q-Transformer (Chebotar et al. ([2023](#bib.bib38)))
    proposes to learn a transformer-based Q-network. Similar with TT, each dimension
    of the action is discretized and treated as a separate time step. When training,
    they update the Q-function in a temporal difference (TD) manner, i.e., minimizing
    the disagreement between the predicted Q-value $Q(s_{t-k:t},a_{t}^{1:i})$ and
    target Q-value $\widehat{Q}(s_{t-k:t},a_{t}^{1:i})$. If $i=M$ (i.e., the action
    dimension), $\widehat{Q}(s_{t-k:t},a_{t}^{1:i})=r_{t}+\max_{a^{1}_{t+1}}Q(s_{t-k+1:t+1},a^{1}_{t+1})$;
    otherwise, it is $\max_{a^{i+1}_{t}}Q(s_{t-k:t},a_{t}^{1:i},a^{i+1}_{t})$. Note
    that for the Bellman optimality (Puterman ([2014](#bib.bib258))) of this per-dimension
    updating rule to hold, it requires the assumption that the choice of $a_{t}^{1:i}$
    does not influence $\max_{a^{i+1}_{t}}Q(s_{t-k:t},a_{t}^{1:i},a^{i+1}_{t})$. However,
    this is probably not true since dimensions of an action are likely to be correlated.
    During evaluation, the action is also generated dimension by dimension, based
    on the trained Q-function.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: Q-变换器（Chebotar 等人（[2023](#bib.bib38)））提出学习一个基于变换器的 Q 网络，而不是建模策略。与 TT 相似，动作的每个维度被离散化并视为一个单独的时间步骤。在训练时，他们以时间差分（TD）方式更新
    Q 函数，即最小化预测 Q 值 $Q(s_{t-k:t},a_{t}^{1:i})$ 与目标 Q 值 $\widehat{Q}(s_{t-k:t},a_{t}^{1:i})$
    之间的不一致。如果 $i=M$（即动作维度），则 $\widehat{Q}(s_{t-k:t},a_{t}^{1:i})=r_{t}+\max_{a^{1}_{t+1}}Q(s_{t-k+1:t+1},a^{1}_{t+1})$；否则，它是
    $\max_{a^{i+1}_{t}}Q(s_{t-k:t},a_{t}^{1:i},a^{i+1}_{t})$。注意，为了使这种逐维更新规则符合 Bellman
    最优性（Puterman（[2014](#bib.bib258)）），要求 $a_{t}^{1:i}$ 的选择不会影响 $\max_{a^{i+1}_{t}}Q(s_{t-k:t},a_{t}^{1:i},a^{i+1}_{t})$。然而，这可能不成立，因为动作的维度可能相关。在评估过程中，动作也是逐维生成的，基于训练好的
    Q 函数。
- en: So far, we have introduced some fundamental algorithm designs to cast offline
    RL as sequence modelling problems based on transformers. Next, we will present
    follow-up improvements of these methods. As illustrated in (Emmons et al. ([2022](#bib.bib85))),
    carefully aligning the model structure/capacity with the training data to prevent
    overfitting or underfitting and choosing which information to condition the policy
    on are critical for RvS performance. Thus, we will categorize the following works
    based on these two aspects.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了一些基本的算法设计，将离线强化学习（RL）视为基于变换器的序列建模问题。接下来，我们将介绍这些方法的后续改进。如（Emmons
    等人（[2022](#bib.bib85)）所示，**精心调整模型结构/容量以匹配训练数据，以防止过拟合或欠拟合**，以及选择政策条件的信息对于RvS性能至关重要。因此，我们将基于这两个方面对以下工作进行分类。
- en: 6.1.2 Balancing Model Capacity with Training Data
  id: totrans-547
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 平衡模型容量与训练数据
- en: 'To better align the capacity of transformer-based models with the amount of
    training data, a series of studies have been proposed focusing on augmenting the
    offline dataset to avoid overfitting. These algorithms provide data augmentation
    strategies for different task scenarios. (1) Based on TT, Bootstrapped Transformer
    (Wang et al. ([2022a](#bib.bib337))) proposes to generate trajectories from $\pi$,
    i.e., the model being learned, and adopt them as additional training data to expand
    the amount and coverage of the offline dataset. The self-generated trajectories
    are filtered by their log probability under the policy $\pi$, which indicates
    the quality and reliability of the training data. (2) SS-DT (Zheng et al. ([2023](#bib.bib392)))
    introduces a data augmentation method for semi-supervised settings, where most
    trajectories lack action labels and are in the format $(s_{0},r_{0},\cdots,s_{T},r_{T})$.
    Their approach simply involves training an inverse dynamics model, i.e., $\mathcal{T}_{\text{inv}}(a_{t}|s_{t},s_{t+1})$,
    on trajectories with action labels and then applying this model to predict actions
    for the unlabeled trajectories. (3) Given only sub-optimal trajectories, the trajectory-level
    supervised learning would fail. In this case, the agent needs to learn to stitch
    segments from different trajectories for an optimal policy. Value-based RL does
    not have the same issue as it pools information for each state across trajectories
    through the Bellman backup. In this case, QDT (Yamagata et al. ([2023](#bib.bib365)))
    suggests learning a Q-function via CQL and replacing the RTG values (i.e., $R_{t}$)
    in the offline dataset with the learned Q-values for DT training. (4) CDT (Liu
    et al. ([2023b](#bib.bib197))) proposes a data augmentation method for safe RL.
    Specifically, in a Constrained MDP (Altman ([1998](#bib.bib9))), the DT conditioned
    on both $R(\tau)$ and $C(\tau)=\sum_{i=0}^{T}c_{t}$ (i.e., the constraint-violation
    cost of the trajectory) is expected to achieve the target trajectory return while
    ensuring that the accumulated cost remains lower than $C(\tau)$. However, during
    inference, unachievable $(R(\tau),C(\tau))$ pairs may be given as conditions,
    which are not represented in the offline dataset. Thus, as an augmentation, they
    suggest using the trajectories from the offline dataset that achieve the highest
    return without violating $C(\tau)$ as the corresponding trajectories of $(R(\tau),C(\tau))$
    for offline training. (5) For tasks with sparse and delayed rewards, DT suffers
    from model degradation, since the RTG does not change within a trajectory. DTRD
    (Zhu et al. ([2023a](#bib.bib394))) proposes to learn a reward shaping function
    $r_{\phi}(s,a)$ to redistribute the delayed reward to each time step. This is
    achieved by solving a bi-level optimization problem as below:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地将基于变换器的模型容量与训练数据量对齐，已经提出了一系列研究，重点是增强离线数据集以避免过拟合。这些算法为不同任务场景提供了数据增强策略。 (1)
    基于 TT，Bootstrapped Transformer (Wang et al. ([2022a](#bib.bib337))) 提出了从 $\pi$（即正在学习的模型）生成轨迹，并将其作为额外的训练数据来扩展离线数据集的数量和覆盖范围。自生成的轨迹通过其在策略
    $\pi$ 下的对数概率进行过滤，这表示了训练数据的质量和可靠性。 (2) SS-DT (Zheng et al. ([2023](#bib.bib392)))
    引入了一种用于半监督设置的数据增强方法，其中大多数轨迹缺乏动作标签，格式为 $(s_{0},r_{0},\cdots,s_{T},r_{T})$。他们的方法简单地涉及在带有动作标签的轨迹上训练一个逆动态模型，即
    $\mathcal{T}_{\text{inv}}(a_{t}|s_{t},s_{t+1})$，然后将该模型应用于预测未标记轨迹的动作。 (3) 仅给定次优轨迹时，轨迹级别的监督学习会失败。在这种情况下，代理需要学习从不同轨迹中拼接片段以获得最佳策略。基于值的
    RL 没有这个问题，因为它通过 Bellman 备份汇聚每个状态的跨轨迹信息。在这种情况下，QDT (Yamagata et al. ([2023](#bib.bib365)))
    建议通过 CQL 学习 Q 函数，并用学习到的 Q 值替换离线数据集中的 RTG 值（即 $R_{t}$）进行 DT 训练。 (4) CDT (Liu et
    al. ([2023b](#bib.bib197))) 提出了安全 RL 的数据增强方法。具体而言，在受限 MDP (Altman ([1998](#bib.bib9)))
    中，条件在 $R(\tau)$ 和 $C(\tau)=\sum_{i=0}^{T}c_{t}$（即轨迹的约束违反成本）上的 DT 预计可以实现目标轨迹回报，同时确保累计成本保持低于
    $C(\tau)$。然而，在推理过程中，可能会给出不可实现的 $(R(\tau),C(\tau))$ 对作为条件，这些条件在离线数据集中没有表示。因此，作为增强，他们建议使用离线数据集中那些在不违反
    $C(\tau)$ 的情况下实现最高回报的轨迹，作为 $(R(\tau),C(\tau))$ 的对应轨迹用于离线训练。 (5) 对于奖励稀疏和延迟的任务，DT
    遭遇模型退化，因为 RTG 在轨迹中不发生变化。DTRD (Zhu et al. ([2023a](#bib.bib394))) 提出了学习奖励塑形函数 $r_{\phi}(s,a)$
    以将延迟奖励重新分配到每个时间步。这是通过解决如下的双层优化问题实现的：
- en: '|  | $\min_{\phi}\mathcal{L}_{\text{val}}(\theta^{*}(\phi),\phi)\ s.t.\ R(\tau)=\sum_{t=0}^{T}r_{\phi}(s_{t},a_{t}),\
    \forall\tau,\ \text{and}\ \theta^{*}(\phi)=\arg\min_{\theta}\mathcal{L}_{\text{train}}(\theta,\phi)$
    |  | (85) |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\phi}\mathcal{L}_{\text{val}}(\theta^{*}(\phi),\phi)\ s.t.\ R(\tau)=\sum_{t=0}^{T}r_{\phi}(s_{t},a_{t}),\
    \forall\tau,\ \text{and}\ \theta^{*}(\phi)=\arg\min_{\theta}\mathcal{L}_{\text{train}}(\theta,\phi)$
    |  | (85) |'
- en: Here, $\theta$ is the parameter of the policy $\pi$; $\mathcal{L}_{\text{train}}(\theta,\phi)$
    denotes the DT objective on the training dataset, which is augmented by replacing
    the original sparse rewards $r_{t}$ with $r_{\phi}(s_{t},a_{t})$; $\mathcal{L}_{\text{val}}(\theta^{*}(\phi),\phi)$
    is the DT objective on the augmented validation dataset. Intuitively, $r_{\phi}$
    is updated so that the policy learned from the dataset augmented with the dense
    rewards is optimal. They propose a practical alternative training framework for
    $\theta$ and $\phi$, which however lacks convergence guarantee in theory.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\theta$ 是策略 $\pi$ 的参数；$\mathcal{L}_{\text{train}}(\theta,\phi)$ 表示训练数据集上的
    DT 目标，该数据集通过将原始稀疏奖励 $r_{t}$ 替换为 $r_{\phi}(s_{t},a_{t})$ 进行了增强；$\mathcal{L}_{\text{val}}(\theta^{*}(\phi),\phi)$
    是在增强的验证数据集上的 DT 目标。直观地，$r_{\phi}$ 被更新以使得从增强了密集奖励的数据集中学到的策略是最优的。他们提出了一种实际的替代训练框架用于
    $\theta$ 和 $\phi$，但理论上缺乏收敛保证。
- en: 'Another line of works in this category focus on modifying the DT model to explicitly
    make use of the structural patterns within the training trajectories. As mentioned
    in Section [2.4](#S2.SS4 "2.4 Transformers ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), introducing strutual assumptions/priors to the input data
    can greatly improve the learning efficiency and mitigate the overfitting issue
    of learning on small-scale datasets. (1) Based on the observation that each trajectory
    is a sequence of state-action-reward triplets, StARformer (Shang et al. ([2022](#bib.bib292)))
    proposes to first extract representations $z_{t}$ from each $(r_{t-1},a_{t-1},s_{t})$,
    and then apply DT on $(z_{1},\cdots,z_{t})$ to predict $a_{t}$. Note that they
    adopt $r_{t}$ rather than $R_{t}$, which is a counterintuitive design, as RL decision-making
    typically requires information on the future return. (2) GDT (Hu et al. ([2023b](#bib.bib140)))
    follows the same motivation as StARformer, recognizing that naively attending
    to all previous tokens, as in DT, can overlook the causal relationships among
    certain types of tokens and thus hurt the performance. In particular, they introduce
    the relation embeddings to the compatibility calculation (i.e., Eq. ([20](#S2.E20
    "In 2nd item ‣ 2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))) for each pair of inputs $(x_{i},x_{j})$. With this modification,
    the compatibility between the query $Q_{i}$ and key $K_{j}$ can be calculated
    as $\langle Q_{i}+re_{i\rightarrow j},K_{j}+re_{j\rightarrow i}\rangle$, where
    $re$ is an embedding of the adjacency matrix that represents the causal relationships
    among elements in the input sequence. For instance, $a_{t}$ is directly influenced
    by $s_{t}$ and $R_{t}$, so there should be edges from $s_{t}$ to $a_{t}$ and from
    $R_{t}$ to $a_{t}$ in the casual graph. In this way, they incorporate the Markovian
    relationship into the DT model. (3) DTd (Wang et al. ([2023c](#bib.bib341))) views
    the state, action, and RTG as distinct modalities. By analyzing attention values
    from a trained DT, they rank the importance of token interactions (for decision-making)
    within or cross modalities. Specifically, $s-s$, $a-a$, $R-R$ < $s-R$, $a-R$ <
    $s-a$, where $s-s$ denotes interactions (i.e., attending via the MHA) between
    states, and ‘<’ denotes being less important. Consequently, they suggest a hierarchical
    DT structure, processing less crucial interactions before more significant ones
    to prevent important interactions from being distracted. The resulting structure
    is large in scale and outperforms DT, but its overall performance is close to
    traditional offline RL methods like CQL.'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '这一类别的另一类工作侧重于修改DT模型，以显式利用训练轨迹中的结构模式。如[2.4节](#S2.SS4 "2.4 Transformers ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")中提到的，通过对输入数据引入结构假设/先验可以大大提高学习效率，并减轻在小规模数据集上学习的过拟合问题。
    (1) 基于每个轨迹是状态-动作-奖励三元组序列的观察，StARformer (Shang et al. ([2022](#bib.bib292))) 提出了首先从每个
    $(r_{t-1},a_{t-1},s_{t})$ 中提取表示 $z_{t}$，然后在 $(z_{1},\cdots,z_{t})$ 上应用DT来预测 $a_{t}$。请注意，他们采用
    $r_{t}$ 而非 $R_{t}$，这是一种反直觉的设计，因为强化学习决策通常需要关于未来回报的信息。 (2) GDT (Hu et al. ([2023b](#bib.bib140)))
    与 StARformer 有相同的动机，认识到像DT那样天真地关注所有先前的标记，可能会忽略某些类型标记之间的因果关系，从而影响性能。特别地，他们在兼容性计算中引入了关系嵌入
    (即，[20](#S2.E20 "In 2nd item ‣ 2.4 Transformers ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions") 中的公式)，对于每对输入 $(x_{i},x_{j})$。通过这一修改，查询
    $Q_{i}$ 和键 $K_{j}$ 之间的兼容性可以计算为 $\langle Q_{i}+re_{i\rightarrow j},K_{j}+re_{j\rightarrow
    i}\rangle$，其中 $re$ 是一个表示输入序列中元素之间因果关系的邻接矩阵的嵌入。例如，$a_{t}$ 直接受到 $s_{t}$ 和 $R_{t}$
    的影响，因此在因果图中应存在从 $s_{t}$ 到 $a_{t}$ 和从 $R_{t}$ 到 $a_{t}$ 的边。通过这种方式，他们将马尔可夫关系纳入DT模型。
    (3) DTd (Wang et al. ([2023c](#bib.bib341))) 将状态、动作和RTG视为不同的模态。通过分析训练后的DT中的注意力值，他们对模态内部或跨模态的标记交互重要性进行排名。具体而言，$s-s$、$a-a$、$R-R$
    < $s-R$、$a-R$ < $s-a$，其中 $s-s$ 表示状态之间的交互（即，通过MHA的注意），而‘<’表示较不重要。因此，他们建议采用分层DT结构，先处理较不重要的交互，然后再处理更重要的交互，以防止重要交互被干扰。最终的结构规模较大，性能优于DT，但其总体性能接近于传统的离线RL方法，如CQL。'
- en: 6.1.3 Mitigating Impacts from Environmental Stochasticity
  id: totrans-552
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3 减少环境随机性带来的影响
- en: 'Another category of research works propose to change the conditional information
    of the DT to enhance its performance in stochastic environments. (1) In the absence
    of determinism, a high-return offline trajectory could be the outcome of uncontrollable
    environmental randomness, rather than the result of the agent’s actions. Thus,
    goals that are independent from the environmental stochasticity are the only conditions
    that the learned agent can reliably achieve. ESPER (Paster et al. ([2022](#bib.bib243)))
    and DOC (Yang et al. ([2023c](#bib.bib373))) propose methods to learn such conditions
    automatically from the offline dataset. They share a common intuition: to ensure
    the conditional variable to contain sufficient information on action predictions
    but no information regarding the environment dynamics (e.g., on predicting future
    rewards or state transitions). Both algorithms use a contrastive learning framework,
    resulting in similar objectives. However, these methods require to learn at least
    five networks (so we choose not to show the objectives here), which is more complicated
    than traditional offline RL approaches. (2) SPLT (Villaflor et al. ([2022](#bib.bib331)))
    proposes to model the policy as $\pi(\hat{a}_{t}|(s_{t-k},a_{t-k},\cdots,s_{t}),z^{\pi}_{t})$,
    where the condition $z^{\pi}_{t}$ is sampled from an encoder that takes $(s_{t-k},a_{t-k},\cdots,s_{t},a_{t})$
    as input. Both the encoder and $\pi$ have DT-like architectures and are trained
    jointly in a CVAE framework, where $\pi$ works as the decoder, $(s_{t-k},a_{t-k},\cdots,s_{t})$
    and $a_{t}$ work as the condition $c$ and data point $x$ respectively. $z^{\pi}_{t}$
    is a $d_{\pi}$-dim discrete vector, each dimension of which has $c$ categories.
    Thus, extracted by the CVAE, each specific $z^{\pi}_{t}$ can be viewed as a mode
    that is contained in the offline data and can be assigned to $\pi$ as a generation
    condition. In the same manner, they learn a prediction model $\omega(\hat{r}_{t},\hat{s}_{t+1},\widehat{R}_{t+1}|(s_{t-k},a_{t-k},\cdots,s_{t},a_{t}),z^{\omega}_{t})$.
    During inference, with $(\pi,\omega)$ and by enumerating $(z^{\pi}_{t},z^{\omega}_{t})$,
    $c^{d_{\pi}\times d_{\omega}}$ predictions on future $h$-length trajectories $\hat{\tau}_{t}^{h}$
    can be obtained, and the next action is selected from the mode: $\arg\max_{z^{\pi}_{t}}\min_{z^{\omega}_{t}}\widehat{R}(\hat{\tau}_{t}^{h})$
    for robustness, where $\widehat{R}(\widehat{\tau}_{t}^{h})=\sum_{i=0}^{h-1}\hat{r}_{t+i}+\widehat{R}_{t+h}$.
    Through enumeration, the influence brought by the environmental randomness can
    be mitigated. (3) CGDT (Wang et al. ([2023d](#bib.bib342))) suggests training
    an additional Q-network to guide the learning of DT. The rationale behind this
    method is that the Q-value, representing the expected return for each state-action
    pair, can help mitigate the impact of environmental stochasticity that might be
    captured by single-trajectory RTG values.'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类研究工作提出改变决策树（DT）的条件信息，以提高其在随机环境中的性能。(1) 在缺乏确定性的情况下，高回报的离线轨迹可能是不可控的环境随机性造成的结果，而不是代理行为的结果。因此，与环境随机性无关的目标是学习到的代理可以可靠实现的唯一条件。ESPER（Paster
    等人 ([2022](#bib.bib243))) 和 DOC（Yang 等人 ([2023c](#bib.bib373))) 提出了从离线数据集中自动学习这些条件的方法。它们共享一个共同的直觉：确保条件变量包含足够的行动预测信息，但不包含关于环境动态的信息（例如，关于未来奖励或状态转移的预测）。这两种算法都使用对比学习框架，结果目标类似。然而，这些方法需要学习至少五个网络（因此我们选择不在这里展示目标），比传统的离线强化学习方法更复杂。(2)
    SPLT（Villaflor 等人 ([2022](#bib.bib331))) 提出了将策略建模为 $\pi(\hat{a}_{t}|(s_{t-k},a_{t-k},\cdots,s_{t}),z^{\pi}_{t})$，其中条件
    $z^{\pi}_{t}$ 是从一个编码器中采样的，该编码器以 $(s_{t-k},a_{t-k},\cdots,s_{t},a_{t})$ 作为输入。编码器和
    $\pi$ 具有类似 DT 的架构，并在 CVAE 框架中联合训练，其中 $\pi$ 作为解码器，$(s_{t-k},a_{t-k},\cdots,s_{t})$
    和 $a_{t}$ 分别作为条件 $c$ 和数据点 $x$。$z^{\pi}_{t}$ 是一个 $d_{\pi}$-维离散向量，每个维度有 $c$ 类别。因此，通过
    CVAE 提取的每个特定 $z^{\pi}_{t}$ 可以被视为包含在离线数据中的模式，并可以作为生成条件分配给 $\pi$。以同样的方式，它们学习了预测模型
    $\omega(\hat{r}_{t},\hat{s}_{t+1},\widehat{R}_{t+1}|(s_{t-k},a_{t-k},\cdots,s_{t},a_{t}),z^{\omega}_{t})$。在推理过程中，通过
    $(\pi,\omega)$ 并枚举 $(z^{\pi}_{t},z^{\omega}_{t})$，可以获得 $c^{d_{\pi}\times d_{\omega}}$
    个关于未来 $h$ 长度轨迹 $\hat{\tau}_{t}^{h}$ 的预测，并从模式中选择下一个行动：$\arg\max_{z^{\pi}_{t}}\min_{z^{\omega}_{t}}\widehat{R}(\hat{\tau}_{t}^{h})$
    以确保鲁棒性，其中 $\widehat{R}(\widehat{\tau}_{t}^{h})=\sum_{i=0}^{h-1}\hat{r}_{t+i}+\widehat{R}_{t+h}$。通过枚举，可以减轻环境随机性带来的影响。(3)
    CGDT（Wang 等人 ([2023d](#bib.bib342))) 建议训练一个额外的 Q 网络以指导 DT 的学习。该方法的基本原理是，Q 值表示每个状态-行动对的预期回报，可以帮助减轻可能由单一轨迹
    RTG 值捕捉到的环境随机性的影响。
- en: 6.1.4 Transformers in Extended Offline Reinforcement Learning Setups
  id: totrans-554
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.4 扩展离线强化学习设置中的 Transformers
- en: DT has been extended to various offline RL setups, including model-based, hierarchical,
    multi-agent, and multi-task offline RL. Unlike the previous three subsections,
    extensions in this part primarily expand on the problem setup rather than the
    algorithmic aspect with respect to DT. Thus, this part is not our primary focus
    and we only provide a taxonomy and brief introduction of the related works, serving
    as a reference for readers.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: DT 已扩展到各种离线强化学习设置，包括基于模型、层次化、多智能体和多任务离线强化学习。与前面三个小节不同，本部分的扩展主要关注问题设置而非算法方面。因此，本部分不是我们的主要关注点，我们仅提供相关工作的分类和简要介绍，供读者参考。
- en: 'Model-based/Hierarchical/Multi-agent Offline RL: (1) Wang et al. ([2023b](#bib.bib339))
    propose Environment Transformer to model the transition dynamics and reward function
    $P(s_{t+1},r_{t}|s_{t},a_{t})$ from the offline dataset, which can be viewed as
    simply replacing the output of DT, i.e., $a_{t}$, with $(s_{t+1},r_{t})$ for the
    learning process. The learned environment model can then be utilized as a simulator
    for policy learning with any RL algorithm. TransDreamer (Chen et al. ([2022](#bib.bib39)))
    suggests replacing the RNN module within Dreamer (Hafner et al. ([2020](#bib.bib118))),
    which is a state-of-the-art (SOTA) model-based RL algorithm, with a transformer,
    for improved performance on tasks requiring modeling long-term temporal dependency.
    (2) HDT (Correia & Alexandre ([2022](#bib.bib62))) employs two DTs for hierarchical
    decision making. The high-level DT models the distribution $\pi_{\text{high}}(g_{t}|s_{t-k},g_{t-k},\cdots,s_{t})$
    for selecting a subgoal, while the low-level DT captures the corresponding action
    distribution $\pi_{\text{low}}(a_{t}|s_{t-k},g_{t-k},a_{t-k},\cdots,s_{t},g_{t})$.
    Moreover, they propose a heuristic approach for extracting subgoals $g_{1:T}$
    from the offline trajectories, when these subgoals are not labeled. Skill DT (Sudhakaran
    & Risi ([2023](#bib.bib311))) adopts a similar hierarchical framework, but proposes
    to use a VQ-VAE (introduced in Section [2.1](#S2.SS1 "2.1 Variational Auto-Encoders
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) as
    the high-level policy to extract the skill choices for the low-level policy to
    condition on. (3) MADT+Distillation (Tseng et al. ([2022](#bib.bib323))) is proposed
    to reformulate offline MARL as a sequence modelling problem. First, a teacher
    (joint) policy $\pi(a^{1:n}_{t}|o^{1:n}_{t-k},R^{1:n}_{t-k},a^{1:n}_{t-k},\cdots,o^{1:n}_{t},R^{1:n}_{t})$
    is learned, as a manner of centralized training. The learning process is the same
    as DT, viewing the concatenation of observations (RTGs/actions) from $n$ agents
    as a single joint observation (RTG/action). Then, $\pi$ is distilled to $n$ student
    (individual) policies $\pi^{i}(a^{i}_{t}|o^{i}_{t-k},R^{i}_{t-k},a^{i}_{t-k},\cdots,o^{i}_{t},R^{i}_{t})$,
    for decentralized execution. The policy distillation process is designed for $\pi^{1:n}$
    to imitate $\pi$ while maintaining the structural relation among these agents
    as in the joint policy $\pi$, which though does not rely on transformers. There
    are some other explorations for the multi-agent scenario: MADT (Meng et al. ([2021](#bib.bib221)))
    investigates combining offline pretraining using DT and online adaptation with
    gradient-based MARL algorithms; SCT (Li et al. ([2023a](#bib.bib184))) proposes
    approaches for handling non-cooperative MARL scenarios. However, these two works
    are still in early stages of development.'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '基于模型/分层/多智能体离线强化学习： (1) Wang 等人 ([2023b](#bib.bib339)) 提出了环境变换器（Environment
    Transformer），用于从离线数据集中建模转移动态和奖励函数 $P(s_{t+1},r_{t}|s_{t},a_{t})$，这可以看作是简单地将DT的输出，即
    $a_{t}$，替换为 $(s_{t+1},r_{t})$ 进行学习过程。学习到的环境模型可以作为任何强化学习算法的策略学习模拟器。TransDreamer
    (Chen 等人 ([2022](#bib.bib39))) 建议用变换器替换 Dreamer (Hafner 等人 ([2020](#bib.bib118)))
    中的 RNN 模块，以提高对需要建模长期时间依赖性任务的性能。 (2) HDT (Correia & Alexandre ([2022](#bib.bib62)))
    采用两个DT进行分层决策。高层DT建模分布 $\pi_{\text{high}}(g_{t}|s_{t-k},g_{t-k},\cdots,s_{t})$
    用于选择子目标，而低层DT捕获相应的动作分布 $\pi_{\text{low}}(a_{t}|s_{t-k},g_{t-k},a_{t-k},\cdots,s_{t},g_{t})$。此外，当这些子目标未标记时，他们提出了一种启发式方法来从离线轨迹中提取子目标
    $g_{1:T}$。Skill DT (Sudhakaran & Risi ([2023](#bib.bib311))) 采用类似的分层框架，但提议使用 VQ-VAE
    (在第 [2.1](#S2.SS1 "2.1 Variational Auto-Encoders ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions") 节介绍) 作为高层策略来提取低层策略条件的技能选择。 (3) MADT+Distillation
    (Tseng 等人 ([2022](#bib.bib323))) 被提出用以将离线 MARL 重构为序列建模问题。首先，学习一个教师（联合）策略 $\pi(a^{1:n}_{t}|o^{1:n}_{t-k},R^{1:n}_{t-k},a^{1:n}_{t-k},\cdots,o^{1:n}_{t},R^{1:n}_{t})$，作为一种集中训练的方式。学习过程与
    DT 相同，将来自 $n$ 个智能体的观察（RTGs/动作）的串联视为单一的联合观察（RTG/动作）。然后，将 $\pi$ 蒸馏为 $n$ 个学生（个体）策略
    $\pi^{i}(a^{i}_{t}|o^{i}_{t-k},R^{i}_{t-k},a^{i}_{t-k},\cdots,o^{i}_{t},R^{i}_{t})$，用于去中心化执行。策略蒸馏过程旨在使
    $\pi^{1:n}$ 模仿 $\pi$ 的同时保持这些智能体之间在联合策略 $\pi$ 中的结构关系，尽管这不依赖于变换器。还有一些其他的多智能体场景探索：MADT
    (Meng 等人 ([2021](#bib.bib221))) 研究了结合使用DT进行离线预训练和使用基于梯度的MARL算法进行在线适应的方法；SCT (Li
    等人 ([2023a](#bib.bib184))) 提出了处理非合作 MARL 场景的方法。然而，这两项工作仍处于开发的早期阶段。'
- en: 'Multi-task Offline RL: As a foundation model, the transformer has demonstrated
    exceptional generalization capabilities in NLP and CV tasks. Thus, there is a
    branch of research works on enhancing DT to enable policies pretrained on source
    tasks to be effectively adapted to target tasks through fine-tuning. The target
    task may be the same as the source task, differ but still fall within the same
    task distribution with the source task, or belong to entirely different modalities
    from the source task. We provide a brief review of these three categories as follows,
    each progressively demanding greater generalization capabilities. (1) Further
    adaption on the same task may be required when the provided offline data is highly
    sub-optimal or only covers a limited part of the state space, for which ODT (Zheng
    et al. ([2022](#bib.bib391))) introduces a technique to fine-tune the pretrained
    DT through further online interactions with the environment. Another possible
    scenario is when the high-quality and labelled offline data is scarce while there
    are abundant unlabeled (i.e., reward-free) and sub-optimal trajectories. In this
    case, PDT (independently proposed by Cang et al. ([2022](#bib.bib35)) and Xie
    et al. ([2023](#bib.bib360))) can be used for unsupervised pretraining on the
    unlabeled data, followed by fine-tuning using the limited high-quality data. (2)
    Adaptation to unseen tasks within the same task distribution is a primary focus
    in multi-task/meta (offline) RL. For extending DT in this realm, some studies
    suggest conditioning the DT on task-specific information $z\sim P_{Z}(\cdot)$
    and training the DT across a range of tasks, each associated with unique embeddings
    $z_{\text{train}}\sim P_{Z}(\cdot)$. Consequently, when encountering new tasks
    sampled from $P_{Z}(\cdot)$ and conditioning on the corresponding $z_{\text{test}}$,
    the policy is expected to be effective in these previously unseen tasks after
    few/zero-shot fine-tuning. $z$ can be a task id (Lin et al. ([2022a](#bib.bib192))),
    segment of a task trajectory (Xu et al. ([2022b](#bib.bib362))), or trajectory
    embedding from an encoder (Lin et al. ([2022b](#bib.bib193))). Alternatively,
    Boustati et al. ([2021](#bib.bib29)) suggests running the source policy in similar
    but counterfactual environments to gather augmented data for DT training, to enhance
    the robustness of the policy on unseen tasks. (3) So far, the transformer has
    been successfully adopted in NLP, CV, and RL, which makes it possible to build
    a transformer-based generalist agent across these three modalities. Gato (Reed
    et al. ([2022](#bib.bib271))) shows that a single transformer with the same set
    of weights can handle 604 distinct tasks with varying modalities. Considering
    that texts, images, and decision trajectories can all be tokenized and embedded,
    the transformer can learn to generate all modalities within a unified training
    framework – predicting the next token based on previously generated ones. Specifically,
    the training objective aligns with Eq. ([83](#S6.E83 "In 6.1.1 Background on Trajectory-Optimization-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), but substitutes the
    training data with sequences of words for text generation and sequences of image
    patches for image understanding. During training, each batch of training data
    incorporates sequences from various modalities and each sequence is concatenated
    with a demonstration for the same task, serving as a prompt to assist the agent
    in differentiating between modalities. Gato utilizes a decoder-only transformer
    with 1.2 billion parameters and is trained on a dataset containing 1.76 trillion
    tokens. The empirical results show that Gato achieves performance exceeding 50%
    of the expert score threshold in 450 of the 604 evaluation tasks. Gato represents
    a significant attempt in developing generalist agents, yet its efficacy heavily
    depends on the quantity and quality of the training data. Especially, when learning
    control policies, it doesn’t utilize reward signals, so its performance would
    be capped by the provided demonstrations. Multi-Game DT (Lee et al. ([2022](#bib.bib178)))
    adopts a similar protocol with Gato and suggests how to utilize reward signals
    in both the training and inference process. Another critical question regrading
    multi-modal learning is whether the learning in distinct modalities can enhance
    each other. Reid et al. ([2022](#bib.bib272)) empirically show that pretrained
    language models can be effectively adapted to offline RL tasks with boosted performance.
    Following this work, Takagi ([2022](#bib.bib316)) provides an excellent in-depth
    empirical study on the effect of cross-modality pretraining for DT. They find
    that parameters of the language-pretrained model do not change that much as a
    randomly-initialized model after the fine-tuning, and these stable parameters
    preserves context-equivalent information such that the agent can make decisions
    even in the absence of contexts (i.e., without using the previous $k$ transitions
    as input). Further, the way to efficiently utilize these context-equivalent information
    is also preserved by those stable parameters. As a result, they conclude that
    pretrained context-like information could help the DT training by enabling the
    model to predict action more accurately. In contrast, a transformer pretrained
    on images performs significantly worse than a standard DT, partly due to the significant
    differences in data characteristics.'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务离线强化学习：作为基础模型，transformer 在 NLP 和 CV 任务中展示了卓越的泛化能力。因此，有一系列研究工作致力于增强 DT，使得在源任务上预训练的策略能够通过微调有效地适应目标任务。目标任务可能与源任务相同、不同但仍在源任务的任务分布中，或与源任务完全不同。我们简要回顾了这三类情况，每类情况对泛化能力的要求逐渐增加。
    (1) 当提供的离线数据高度次优或仅覆盖状态空间的一部分时，可能需要进一步适应相同的任务，此时 ODT（Zheng 等人（[2022](#bib.bib391)））引入了一种技术，通过与环境的进一步在线交互来微调预训练的
    DT。另一种可能的情况是高质量和标记的离线数据稀缺，而存在大量未标记（即无奖励）和次优轨迹。在这种情况下，PDT（Cang 等人（[2022](#bib.bib35)）和
    Xie 等人（[2023](#bib.bib360)）独立提出）可以用于在未标记数据上进行无监督预训练，然后使用有限的高质量数据进行微调。 (2) 在同一任务分布内适应未见任务是多任务/元学习（离线）强化学习的主要关注点。为了扩展
    DT 在这一领域，一些研究建议将 DT 以任务特定信息 $z\sim P_{Z}(\cdot)$ 进行条件化，并在一系列任务上训练 DT，每个任务都与独特的嵌入
    $z_{\text{train}}\sim P_{Z}(\cdot)$ 相关。因此，当遇到从 $P_{Z}(\cdot)$ 采样的新任务并以相应的 $z_{\text{test}}$
    进行条件化时，预期策略在经过少量/零次微调后能够在这些之前未见的任务中有效。$z$ 可以是任务 ID（Lin 等人（[2022a](#bib.bib192)））、任务轨迹的片段（Xu
    等人（[2022b](#bib.bib362)））或来自编码器的轨迹嵌入（Lin 等人（[2022b](#bib.bib193)））。另外，Boustati
    等人（[2021](#bib.bib29)）建议在类似但反事实的环境中运行源策略，以收集增强数据用于 DT 训练，以增强策略在未见任务上的鲁棒性。 (3)
    到目前为止，transformer 已成功应用于 NLP、CV 和 RL，这使得在这三种模态中构建基于 transformer 的通用体成为可能。Gato（Reed
    等人（[2022](#bib.bib271)））显示，单个 transformer 使用相同的权重集可以处理 604 个具有不同模态的任务。考虑到文本、图像和决策轨迹都可以被标记化和嵌入，transformer
    可以在统一的训练框架内学习生成所有模态——基于先前生成的内容预测下一个标记。具体而言，训练目标与 Eq.（[83](#S6.E83 "在 6.1.1 轨迹优化基础上的离线强化学习
    ‣ 6.1 离线强化学习 ‣ 6 Transformer 在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向")），但用文本生成的词序列和图像理解的图像补丁序列替代训练数据。在训练期间，每批训练数据包含来自各种模态的序列，每个序列与相同任务的演示连接在一起，作为提示帮助智能体区分模态。Gato
    利用一个仅有解码器的 transformer，具有 12 亿个参数，并在包含 1.76 万亿个标记的数据集上进行训练。实证结果显示，Gato 在 604 个评估任务中的
    450 个任务中达到了超过 50% 的专家分数阈值。Gato 代表了开发通用体的重大尝试，但其有效性严重依赖于训练数据的数量和质量。特别是在学习控制策略时，它不利用奖励信号，因此其性能会受到提供的演示的限制。Multi-Game
    DT（Lee 等人（[2022](#bib.bib178)））采用了类似于 Gato 的协议，并建议如何在训练和推理过程中利用奖励信号。另一个关于多模态学习的关键问题是不同模态的学习是否能够相互增强。Reid
    等人（[2022](#bib.bib272)）实证显示，预训练的语言模型可以有效地适应离线 RL 任务，并提升性能。在此工作之后，Takagi（[2022](#bib.bib316)）提供了一项关于跨模态预训练对
    DT 影响的出色深入实证研究。他们发现，经过微调后，语言预训练模型的参数变化不如随机初始化模型那样大，这些稳定的参数保持了上下文等效信息，从而使智能体即使在没有上下文的情况下（即不使用之前的
    $k$ 次转移作为输入）也能做出决策。此外，这些稳定参数还保留了如何有效利用这些上下文等效信息。因此，他们总结认为，预训练的上下文信息可以通过使模型更准确地预测动作来帮助
    DT 训练。相比之下，预训练于图像上的 transformer 性能显著差于标准 DT，这部分是由于数据特征的显著差异。
- en: '| Algorithm | Key Novelty | Evaluation Task |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| Algorithm | 主要创新 | 评估任务 |'
- en: '| DT | Solving offline RL as sequence modeling | Atari, D4RL (L) |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| DT | 将离线 RL 作为序列建模 | Atari, D4RL (L) |'
- en: '| TT | Tokenization, discretization and beam search | D4RL (L, M) |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| TT | 标记化、离散化和束搜索 | D4RL (L, M) |'
- en: '| Q-Transformer | Transformer-based Q learning | Real Robot |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| Q-Transformer | 基于 Transformer 的 Q 学习 | Real Robot |'
- en: '| Boot Transformer | Data augmentation with self-generated data | D4RL (L,
    A) |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| Boot Transformer | 使用自生成数据进行数据增强 | D4RL (L, A) |'
- en: '| SS-DT | Data augmentation via an inverse dynamic model | D4RL (L, M2d) |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| SS-DT | 通过逆动态模型进行数据增强 | D4RL (L, M2d) |'
- en: '| QDT | Replacing RTGs with Q-values to enable stitching | D4RL (L, M2d) |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| QDT | 用 Q 值替代 RTGs 以实现拼接 | D4RL (L, M2d) |'
- en: '| CDT | Data augmentation for safe RL in constrained MDP | Bullet-safety-gym
    |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| CDT | 受限 MDP 中的安全 RL 数据增强 | Bullet-safety-gym |'
- en: '| DTRD | Data augmentation for sparse, delayed reward setups | Atari, MiniGrid,
    D4RL (L, K, M2d) |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| DTRD | 稀疏、延迟奖励设置的数据增强 | Atari, MiniGrid, D4RL (L, K, M2d) |'
- en: '| StARformer | Processing $(r_{t-1},a_{t-1},s_{t})$ as a group | Atari, dm_control
    |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| StARformer | 将 $(r_{t-1},a_{t-1},s_{t})$ 作为一个组处理 | Atari, dm_control |'
- en: '| GDT | Embedding the causal relation among $s,a,R$ | Atari, D4RL (L) |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| GDT | 嵌入 $s,a,R$ 之间的因果关系 | Atari, D4RL (L) |'
- en: '| DTd | Processing $s,a,R$ as three distinct modalities | D4RL (L, M) |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| DTd | 将 $s,a,R$ 处理为三种不同的模态 | D4RL (L, M) |'
- en: '| ESPER | Replacing RTGs with a stochasticity-free conditioner | Stochastic
    Benchmark |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| ESPER | 用无随机性的调节器替代 RTGs | 随机基准测试 |'
- en: '| DOC | Replacing RTGs with a stochasticity-free conditioner | MuJoCo, D4RL
    (M), FrozenLake |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| DOC | 用无随机性的调节器替代 RTGs | MuJoCo, D4RL (M), FrozenLake |'
- en: '| SPLT | Enumerating future trajs to mitigate stochasticity | CARLA |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| SPLT | 枚举未来轨迹以减轻随机性 | CARLA |'
- en: '| CGDT | Critic-guided DT training | D4RL (L, M) |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| CGDT | 由批评者指导的 DT 训练 | D4RL (L, M) |'
- en: '| Env Trans (MB) | Modeling the dynamic and reward functions | D4RL (L) |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| Env Trans (MB) | 建模动态和奖励函数 | D4RL (L) |'
- en: '| TransDreamer (MB) | Integrating Dreamer with a transformer architecture |
    Hidden Order Discovery, dm_control, Atari |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| TransDreamer (MB) | 将 Dreamer 与 Transformer 架构集成 | Hidden Order Discovery,
    dm_control, Atari |'
- en: '| HDT (HRL) | Subgoal-conditioned decision-making | D4RL (L, A, K) |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| HDT (HRL) | 子目标条件决策 | D4RL (L, A, K) |'
- en: '| Skill DT (HRL) | Skill extraction via a VQ-VAE | D4RL (L, M) |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| Skill DT (HRL) | 通过 VQ-VAE 提取技能 | D4RL (L, M) |'
- en: '| MADT+Dist (MA) | A centralized training with decentralized execution (CTDE)
    framework for MARL based on policy distillation | Fill-In, Equal Space, SMAC,
    Grid-World, Highway |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| MADT+Dist (MA) | 基于策略蒸馏的集中训练与分散执行 (CTDE) 框架用于 MARL | Fill-In, Equal Space,
    SMAC, Grid-World, Highway |'
- en: '| MADT (MA) | Applying a shared DT to each agent’s sequence | SMAC |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| MADT (MA) | 将共享 DT 应用于每个代理的序列 | SMAC |'
- en: '| SCT (MA) | A DT-based method for non-cooperative MARL | simple-tag, simple-world
    |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| SCT (MA) | 一种基于 DT 的非合作 MARL 方法 | simple-tag, simple-world |'
- en: '| ODT (PT) | An online fine-tuning method for DT | D4RL (L, M) |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| ODT (PT) | 一种 DT 的在线微调方法 | D4RL (L, M) |'
- en: '| PDT (PT) | Pretraining on large-scale reward-free trajectories | D4RL (L)
    |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| PDT (PT) | 在大规模无奖励轨迹上预训练 | D4RL (L) |'
- en: '| MG DT (MT) | A single transformer agent for 46 Atari games | Atari |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| MG DT (MT) | 一个单一的 Transformer 代理用于 46 个 Atari 游戏 | Atari |'
- en: '| Gato (MT) | A generalist agent for 604 cross-modality tasks | See Reed et al.
    ([2022](#bib.bib271)) |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '| Gato (MT) | 用于 604 个跨模态任务的通用代理 | 参见 Reed 等 ([2022](#bib.bib271)) |'
- en: 'Table 8: Summary of transformer-based offline RL algorithms. In Column 1, we
    list representative (but not all) algorithms in this section, where Boot Transformer,
    Env Trans, MADT+Dist, and MG DT correspond to Bootstrapped Transformer, Environment
    Transformer, MADT+Distillation, and Multi-Game DT, respectively. These algorithms
    are grouped by their categories, with abbreviations MB, HRL, MA, PT, and MT denoting
    model-based, hierarchical, multi-agent, pretraining-based, and multi-task offline
    RL, respectively. The evaluation tasks are listed in Column 3\. Most works are
    evaluated on D4RL (Fu et al. ([2020](#bib.bib99))), which provides offline datasets
    for various tasks, including Locomotion (L), AntMaze (M), Adroit (A), Kitchen
    (K), Maze2d (M2d). Regarding the other benchmarks, we provide their references
    here: Atari (Bellemare et al. ([2013](#bib.bib22))), Real Robot (Chebotar et al.
    ([2023](#bib.bib38))), Bullet-safety-gym (Gronauer ([2022](#bib.bib110))), MiniGrid
    (Chevalier-Boisvert et al. ([2018](#bib.bib52))), dm_control (Tassa et al. ([2018](#bib.bib317))),
    Stochastic Benchmark (Paster et al. ([2022](#bib.bib243))), MuJoCo (Todorov et al.
    ([2012](#bib.bib320))), FrozenLake (Foundation ([2023](#bib.bib93))), CARLA (Dosovitskiy
    et al. ([2017](#bib.bib78))), Hidden Order Discovery (Chen et al. ([2022](#bib.bib39))),
    SMAC (Samvelyan et al. ([2019](#bib.bib283))), Fill-In & Equal Space & Grid-World
    & Highway (Meng et al. ([2021](#bib.bib221))), simple-tag & simple-world (Li et al.
    ([2023a](#bib.bib184))).'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：基于 Transformer 的离线 RL 算法总结。在第 1 列中，我们列出了本节中的代表性（但不是全部）算法，其中 Boot Transformer、Env
    Trans、MADT+Dist 和 MG DT 分别对应于 Bootstrapped Transformer、Environment Transformer、MADT+Distillation
    和 Multi-Game DT。这些算法按类别分组，缩写 MB、HRL、MA、PT 和 MT 分别表示基于模型的、分层的、多智能体的、基于预训练的和多任务的离线
    RL。评估任务列在第 3 列。大多数工作在 D4RL (Fu 等人 ([2020](#bib.bib99))) 上进行评估，该平台提供了各种任务的离线数据集，包括
    Locomotion (L)、AntMaze (M)、Adroit (A)、Kitchen (K)、Maze2d (M2d)。关于其他基准，我们在这里提供它们的参考文献：Atari
    (Bellemare 等人 ([2013](#bib.bib22)))，Real Robot (Chebotar 等人 ([2023](#bib.bib38)))，Bullet-safety-gym
    (Gronauer ([2022](#bib.bib110)))，MiniGrid (Chevalier-Boisvert 等人 ([2018](#bib.bib52)))，dm_control
    (Tassa 等人 ([2018](#bib.bib317)))，Stochastic Benchmark (Paster 等人 ([2022](#bib.bib243)))，MuJoCo
    (Todorov 等人 ([2012](#bib.bib320)))，FrozenLake (Foundation ([2023](#bib.bib93)))，CARLA
    (Dosovitskiy 等人 ([2017](#bib.bib78)))，Hidden Order Discovery (Chen 等人 ([2022](#bib.bib39)))，SMAC
    (Samvelyan 等人 ([2019](#bib.bib283)))，Fill-In & Equal Space & Grid-World & Highway
    (Meng 等人 ([2021](#bib.bib221)))，simple-tag & simple-world (Li 等人 ([2023a](#bib.bib184)))。
- en: 6.1.5 Reflections on Transformer-based Offline Reinforcement Learning
  id: totrans-586
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.5 对基于 Transformer 的离线强化学习的反思
- en: We notice that there is a series of works on applying transformers to online
    partially-observable RL (Yuan et al. ([2023a](#bib.bib382))). They adopt the transformer
    as the policy network in gradient-based or actor-critic RL algorithms, with the
    hope to harness its ability to process long-horizon historical information for
    decision making. These works focus on architectural modifications of the standard
    transformer, which are specially tailored for RL. For example, GTrXL (Parisotto
    et al. ([2020](#bib.bib241))) and Catformer (Davis et al. ([2021](#bib.bib67)))
    suggest adjustments to the MHA module to facilitate a more stable RL training
    process; ALD (Parisotto & Salakhutdinov ([2021](#bib.bib240))) adopts model distillation
    to improve the computation efficiency in transformer-based distributed RL; STT
    (Yang et al. ([2022f](#bib.bib375))) and WMG (Loynd et al. ([2020](#bib.bib200)))
    propose adaptions for the transformer to process spatiotemporal coupling observarions
    and factored observations, respectively, for enhanced sample efficiency. These
    architectural modifications, or potentially new ones, could be integrated with
    policy-gradient-based offline RL algorithms or DT-like return-conditioned supervised
    learning (RCSL) algorithms for performance improvement. Conducting an (empirical)
    comparison between these two categories (i.e., RCSL and policy gradient offline
    RL), both utilizing the same transformer architecture, would be an intriguing
    study. There is also a growing body of research on using the transformer as the
    policy/value network in other online RL settings. For multi-agent RL, notable
    examples include MAT (Wen et al. ([2022](#bib.bib352))), UPDET (Hu et al. ([2021](#bib.bib141))),
    ATM (Yang et al. ([2022e](#bib.bib374))), and TransMix (Khan et al. ([2022](#bib.bib159)));
    while in multi-task RL, there is DCRL (Dance et al. ([2021](#bib.bib65))). These
    advancements can be potentially adapted to offline settings.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到有一系列关于将变换器应用于在线部分可观察RL的研究（Yuan等人（[2023a](#bib.bib382)））。他们将变换器作为基于梯度或演员-评论家RL算法中的策略网络，期望利用其处理长时间范围历史信息的能力进行决策。这些工作集中在标准变换器的架构修改上，这些修改特别针对RL进行定制。例如，GTrXL（Parisotto等人（[2020](#bib.bib241)））和Catformer（Davis等人（[2021](#bib.bib67)））建议对MHA模块进行调整，以促进更稳定的RL训练过程；ALD（Parisotto
    & Salakhutdinov（[2021](#bib.bib240)））采用模型蒸馏来提高基于变换器的分布式RL中的计算效率；STT（Yang等人（[2022f](#bib.bib375)））和WMG（Loynd等人（[2020](#bib.bib200)））分别提出了变换器处理时空耦合观测和分解观测的适配方案，以提高样本效率。这些架构修改，或者潜在的新修改，可以与基于策略梯度的离线RL算法或DT-like回报条件监督学习（RCSL）算法结合，以提高性能。在这两类（即RCSL和基于策略梯度的离线RL）中，利用相同变换器架构进行的（实证）比较将是一个有趣的研究。此外，还有越来越多的研究将变换器作为其他在线RL设置中的策略/价值网络。对于多智能体RL，值得注意的例子包括MAT（Wen等人（[2022](#bib.bib352)））、UPDET（Hu等人（[2021](#bib.bib141)））、ATM（Yang等人（[2022e](#bib.bib374)））和TransMix（Khan等人（[2022](#bib.bib159)））；而在多任务RL中，有DCRL（Dance等人（[2021](#bib.bib65)））。这些进展有可能被改编到离线设置中。
- en: 'Finally, we reflect on these DT-like RCSL methods. Although a series of improvements
    have been made, there are still fundamental problems or limitations regarding
    these algorithms. According to (Brandfonbrener et al. ([2022](#bib.bib30))), RCSL
    returns near-optimal policy under a set of assumptions that are stronger than
    those needed for traditional RL algorithms, and RCSL alone is unlikely to be a
    general solution for offline RL problems. In particular, RCSL offers guarantees
    only when the environment dynamics (including the state transition and reward
    functions) are nearly deterministic, a priori knowledge of the optimal conditioning
    function (i.e., the RTG values) is available, and the return distribution of the
    provided offline trajectories can cover the possible values of the conditioning
    function. These findings inspire several directions for future research. First,
    adapting RCSL algorithms to stochastic environments, ideally backed by theoretical
    optimality guarantees, is a clear necessity. Second, developing effective strategies
    for selecting RTG values as the conditions during inference, or alternatively,
    exploring new conditioning functions, is crucial. Last, a significant challenge
    lies in addressing out-of-distribution scenarios in case that the offline dataset
    lacks adequate coverage of the task scenarios. Another fundamental problem regarding
    RCSL is whether it is necessary to learn low-return behaviors from the offline
    dataset and to require strong alignment between the target return (i.e., the condition
    of the DT) and realized return. As indicated in Eq. ([83](#S6.E83 "In 6.1.1 Background
    on Trajectory-Optimization-based Offline Reinforcement Learning ‣ 6.1 Offline
    Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), the policy training involves imitating trajectories across a range
    of returns. However, only high-return policy is required during inference. This
    raises the possibility of developing a more efficient mechanism that strategically
    utilizes low-return samples to enhance the learning of a high-return policy.'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们反思这些类似DT的RCSL方法。尽管已经进行了一系列改进，但这些算法仍存在一些根本性问题或局限性。根据（Brandfonbrener等人（[2022](#bib.bib30)）），RCSL在一组比传统RL算法所需假设更强的假设下返回近似最优策略，而仅凭RCSL不太可能成为离线RL问题的通用解决方案。特别是，RCSL只有在环境动态（包括状态转移和奖励函数）几乎是确定性的、能够获得关于最优条件函数（即RTG值）的先验知识，并且所提供的离线轨迹的回报分布能够覆盖条件函数的所有可能值时才提供保证。这些发现激发了未来研究的几个方向。首先，将RCSL算法适应于随机环境，理想情况下有理论上的最优性保证，是一个明显的必要性。其次，开发在推理过程中选择RTG值作为条件的有效策略，或探索新的条件函数，是至关重要的。最后，一个重要的挑战在于解决离线数据集在任务场景中覆盖不足的情况下的分布外场景问题。关于RCSL的另一个根本问题是，是否有必要从离线数据集中学习低回报行为，并要求目标回报（即DT的条件）与实际回报之间有强对齐。如公式（[83](#S6.E83
    "在6.1.1基于轨迹优化的离线强化学习背景 ‣ 6.1 离线强化学习 ‣ 6 离线策略学习中的变换器 ‣ 深度生成模型用于离线策略学习：教程、调查及未来方向")）所示，策略训练涉及在一系列回报上模仿轨迹。然而，推理过程中只需要高回报策略。这提出了开发一种更高效的机制的可能性，该机制战略性地利用低回报样本来增强高回报策略的学习。
- en: 6.2 Imitation Learning
  id: totrans-589
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 模仿学习
- en: 'Similarly with DT, transformer-based IL utilizes a supervised learning paradigm
    (see Section [6.2.1](#S6.SS2.SSS1 "6.2.1 A Paradigm of Transformer-based Imitation
    Learning ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")), where the transformer is adopted as the policy backbone
    (see Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Adopting Transformers as the Policy Backbone
    for Imitation Learning ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")). We differentiate works for IL and offline
    RL simply by if the reward or return is used as a condition of the policy. A significant
    advantage of the transformer is its capability to process large quantities of
    training data across multiple modalities, which enables the development of generalist
    agents, as shown in Section [6.2.3](#S6.SS2.SSS3 "6.2.3 Developing Generalist
    Imitation Learning Agents with Transformers ‣ 6.2 Imitation Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions").'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 与DT类似，基于变换器的模仿学习利用了一个监督学习范式（参见第[6.2.1节](#S6.SS2.SSS1 "6.2.1 基于变换器的模仿学习范式 ‣ 6.2
    模仿学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查与未来方向的展望")），其中变换器被作为策略骨干（参见第[6.2.2节](#S6.SS2.SSS2
    "6.2.2 将变换器作为模仿学习的策略骨干 ‣ 6.2 模仿学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查与未来方向的展望")）。我们通过策略是否使用奖励或回报作为条件来区分模仿学习和离线强化学习的工作。变换器的一个显著优势是能够处理大量跨多个模态的训练数据，这使得开发通用代理成为可能，如第[6.2.3节](#S6.SS2.SSS3
    "6.2.3 使用变换器开发通用模仿学习代理 ‣ 6.2 模仿学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习中的深度生成模型：教程、调查与未来方向的展望")所示。
- en: 6.2.1 A Paradigm of Transformer-based Imitation Learning
  id: totrans-591
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 基于变换器的模仿学习范式
- en: 'Given expert demonstrations $D_{E}=\{\tau_{E}=(s_{0},a_{0},\cdots,s_{T},a_{T})\}$,
    transformer-based IL algorithms are designed to learn a policy $\pi$ to replicate
    expert behaviors. The objective function is typically formulated as follows:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 给定专家演示数据 $D_{E}=\{\tau_{E}=(s_{0},a_{0},\cdots,s_{T},a_{T})\}$，基于变换器的模仿学习算法旨在学习一个策略
    $\pi$ 来复制专家行为。目标函数通常被表述如下：
- en: '|  | $\min_{\pi}\mathbb{E}_{D_{E}}\left[(a_{t}-\log\pi(s_{t},x_{t-1}\cdots,x_{t-k}))^{2}\right],\
    \text{or}\ \min_{\pi}\mathbb{E}_{D_{E}}\left[-\log\pi(a_{t}&#124;s_{t},x_{t-1},\cdots,x_{t-k})\right]$
    |  | (86) |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\pi}\mathbb{E}_{D_{E}}\left[(a_{t}-\log\pi(s_{t},x_{t-1}\cdots,x_{t-k}))^{2}\right],\
    \text{or}\ \min_{\pi}\mathbb{E}_{D_{E}}\left[-\log\pi(a_{t}&#124;s_{t},x_{t-1},\cdots,x_{t-k})\right]$
    |  | (86) |'
- en: 'Here, $x_{t}\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}(s_{t},a_{t})\
    \text{or}\ s_{t}$, and $\pi$ is implemented as a decoder-only transformer (as
    introduced in Section [2.4](#S2.SS4 "2.4 Transformers ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")). This objective is similar in form with
    the one of DT (i.e., Eq. ([83](#S6.E83 "In 6.1.1 Background on Trajectory-Optimization-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))) but replaces $(s_{t},a_{t},R_{t})$
    with $x_{t}$. IL algorithms do not require reward signals but place an emphasis
    on the quality of demonstrations, which ideally should come from experts, as the
    learning process relies solely on imitation. Also, to make full use of demonstrations,
    auxiliary supervision objectives, such as prediction errors on the next state
    (i.e., the forward model loss) or the intermediate action between two consecutive
    states (i.e., the inverse model loss), are often employed. These objectives complement
    the action prediction error (i.e., Eq. ([86](#S6.E86 "In 6.2.1 A Paradigm of Transformer-based
    Imitation Learning ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"))) to aid the policy learning process.
    As a representative, Behavior Transformer (BeT, Shafiullah et al. ([2022](#bib.bib288)))
    empirically shows that a standard transformer architecture (specifically minGPT
    Brown et al. ([2020](#bib.bib34))) can significantly outperform commonly-used
    policy networks, like MLP and LSTM, in learning from large-scale, human-generated
    demonstrations, which typically exhibit high variance and multiple modalities.
    In particular, $x_{t}$ is defined as $s_{t}$. To cover the multiple modes in expert
    behaviors, the authors suggest dividing the action $a$ into two components: its
    corresponding action center $\lfloor a\rfloor$ and the residual action $\langle
    a\rangle$, such that $a=\lfloor a\rfloor+\langle a\rangle$. The set of action
    centers can be acquired by applying K-means clustering to the expert actions.
    Correspondingly, they apply a policy network $\pi$ with two prediction heads,
    one for the action center and the other for the residual action. Explicitly employing
    clustering to identify the various modes in expert actions and utilizing dual-head
    predictions to reason the mode of each instance significantly aids in modelling
    multi-modal actions. However, this approach does not model the multi-modality
    that may exist in the joint distribution of $(s,a)$.'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，$x_{t}\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}(s_{t},a_{t})\
    \text{或}\ s_{t}$，而 $\pi$ 被实现为仅解码器的 transformer（如在第 [2.4](#S2.SS4 "2.4 Transformers
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") 节中介绍）。这个目标在形式上类似于
    DT 的目标（即，等式 ([83](#S6.E83 "In 6.1.1 Background on Trajectory-Optimization-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))），但将 $(s_{t},a_{t},R_{t})$
    替换为 $x_{t}$。IL 算法不需要奖励信号，而是强调示范的质量，这些示范理想情况下应来自专家，因为学习过程完全依赖于模仿。此外，为了充分利用示范，通常会采用辅助监督目标，如对下一状态的预测误差（即前向模型损失）或两个连续状态之间的中间动作的预测误差（即逆向模型损失）。这些目标补充了动作预测误差（即，等式
    ([86](#S6.E86 "In 6.2.1 A Paradigm of Transformer-based Imitation Learning ‣ 6.2
    Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"))），以帮助策略学习过程。作为一个代表，Behavior Transformer (BeT, Shafiullah 等 ([2022](#bib.bib288)))
    通过实验证明，标准 transformer 架构（具体是 minGPT Brown 等 ([2020](#bib.bib34))) 可以显著超越常用的策略网络，如
    MLP 和 LSTM，在从大规模人类生成的示范中学习，这些示范通常表现出高方差和多种模态。特别地，$x_{t}$ 被定义为 $s_{t}$。为了覆盖专家行为中的多种模式，作者建议将动作
    $a$ 分为两个组成部分：其对应的动作中心 $\lfloor a\rfloor$ 和残差动作 $\langle a\rangle$，使得 $a=\lfloor
    a\rfloor+\langle a\rangle$。动作中心的集合可以通过对专家动作应用 K-means 聚类来获得。相应地，他们应用具有两个预测头的策略网络
    $\pi$，一个用于动作中心，另一个用于残差动作。明确地使用聚类来识别专家动作中的各种模式，并利用双头预测来推理每个实例的模式，显著有助于建模多模态动作。然而，这种方法并没有建模
    $(s,a)$ 的联合分布中可能存在的多模态性。'
- en: 6.2.2 Adopting Transformers as the Policy Backbone for Imitation Learning
  id: totrans-595
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 采用 Transformers 作为模仿学习的策略骨干
- en: 'A series of studies (Kim et al. ([2021](#bib.bib160)); Pan et al. ([2022](#bib.bib237));
    Kim et al. ([2022](#bib.bib161)); Zhu et al. ([2022](#bib.bib395)); Chen et al.
    ([2023a](#bib.bib42)); Kim et al. ([2023](#bib.bib162)); Liang et al. ([2023a](#bib.bib189)))
    have adopted transformers as the policy backbone for imitation learning in complex
    control tasks, such as vision-based robotic manipulation and end-to-end self-driving.
    As introduced in Section [2.4](#S2.SS4 "2.4 Transformers ‣ 2 Background on Deep
    Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), the transformer has various advantages
    over CNNs and RNNs. Firstly, the transformer is adept at processing time-series
    data (Kim et al. ([2022](#bib.bib161))) by capturing the long-term temporal dependencies
    between inputs across different time steps. By using the current state as the
    query and historical data as keys and values, the transformer-based agent can
    pinpoint vital information in the sequence for the current decision-making through
    the attention mechanism. Secondly, the transformer can concurrently process various
    types of input data, such as texts and images (Kamath et al. ([2023](#bib.bib156))),
    texts and voxels (Shridhar et al. ([2022](#bib.bib296))), point clouds (Pan et al.
    ([2022](#bib.bib237))), enabling it to be the backbone of an end-to-end deep learning
    agent. Each type of data can be embedded to vectors by (pretrained) domain-specific
    neural networks. Then, instead of simply concatenating all these embeddings for
    subsequent processing, the transformer treats each embedding as distinct tokens.
    As detailed in Section [2.4](#S2.SS4 "2.4 Transformers ‣ 2 Background on Deep
    Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), each token possesses its own
    query, key, and value, and can attend to the other tokens for more informative
    aggregation. Thirdly, the transformer is efficient in processing input that contains
    multiple entities by modeling their interrelations. For example, TSE (Liang et al.
    ([2023a](#bib.bib189))) takes the state of the ego vehicle, the status of surrounding
    vehicles, and lane information as input, explicitly reasoning the relationship
    between the ego vehicle and its surrounding environment for self-driving decision-making;
    Silver-Bullet-3D (Pan et al. ([2022](#bib.bib237))) models the relationship of
    embeddings corresponding to different parts of the manipulated objects and robotic
    arms through the transformer for complex manipulation tasks; VIOLA (Zhu et al.
    ([2022](#bib.bib395))) extracts a series of object-centric representations from
    the visual observation, introduces an extra action token (whose corresponding
    output is the action prediction), and applies a transformer on top of these tokens,
    so that the action token can learn to attend to and focus on task-relevant objects
    for improved performance in action prediction.'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '一系列研究（Kim 等人 ([2021](#bib.bib160)); Pan 等人 ([2022](#bib.bib237)); Kim 等人 ([2022](#bib.bib161));
    Zhu 等人 ([2022](#bib.bib395)); Chen 等人 ([2023a](#bib.bib42)); Kim 等人 ([2023](#bib.bib162));
    Liang 等人 ([2023a](#bib.bib189))) 采用了 transformers 作为模仿学习在复杂控制任务中的策略骨干，例如基于视觉的机器人操作和端到端的自动驾驶。如在第
    [2.4](#S2.SS4 "2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 节中介绍，transformer 相对于 CNNs 和 RNNs 具有多种优势。首先，transformer
    擅长处理时间序列数据（Kim 等人 ([2022](#bib.bib161)))，通过捕捉不同时间步输入之间的长期时间依赖关系。通过将当前状态作为查询，将历史数据作为键和值，基于
    transformer 的智能体可以通过注意力机制确定序列中对当前决策至关重要的信息。其次，transformer 能够同时处理各种类型的输入数据，如文本和图像（Kamath
    等人 ([2023](#bib.bib156)))，文本和体素（Shridhar 等人 ([2022](#bib.bib296)))，点云（Pan 等人 ([2022](#bib.bib237)))，使其成为端到端深度学习智能体的骨干。每种类型的数据可以通过（预训练的）领域特定神经网络嵌入到向量中。然后，transformer
    不会简单地将所有这些嵌入连接起来进行后续处理，而是将每个嵌入视为独特的标记。如第 [2.4](#S2.SS4 "2.4 Transformers ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") 节中详细说明，每个标记具有其自己的查询、键和值，并可以关注其他标记以进行更具信息性的聚合。第三，transformer
    在处理包含多个实体的输入时，通过建模它们之间的关系表现出高效。例如，TSE（Liang 等人 ([2023a](#bib.bib189))) 将自车状态、周围车辆状态和车道信息作为输入，明确推理自车与其周围环境之间的关系以进行自动驾驶决策；Silver-Bullet-3D（Pan
    等人 ([2022](#bib.bib237))) 通过 transformer 对操作对象和机器人手臂不同部位的嵌入进行建模，以应对复杂的操作任务；VIOLA（Zhu
    等人 ([2022](#bib.bib395))) 从视觉观察中提取一系列以对象为中心的表示，引入额外的动作标记（其对应的输出是动作预测），并在这些标记上应用
    transformer，使得动作标记能够学习关注并集中于任务相关的对象，从而提高动作预测的性能。'
- en: 6.2.3 Developing Generalist Imitation Learning Agents with Transformers
  id: totrans-597
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3 使用变换器开发通用模仿学习代理
- en: 'Another research focus in this field is language-conditioned IL, aiming at
    training (robot) agents to follow human instructions. A language instruction,
    comprising a sequence of words, can be embedded into a sequence of tokens $(l_{1},\cdots,l_{m})$
    through a pretrained language model or a predefined embedding table. (1) A straightforward
    method for language-conditioned IL, as shown in TDT (Putterman et al. ([2022](#bib.bib259))),
    is to incorporate language tokens into the policy input, i.e., $\pi(a_{t}|s_{t},x_{t-1},\cdots,x_{t-k},l_{1},\cdots,l_{m})$.
    Still, $\pi$ is implemented as a decoder-only transformer, which, as previously
    mentioned, is capable of processing multiple types of input concurrently. Perceiver-Actor
    (Shridhar et al. ([2022](#bib.bib296))) adopts a similar design, but suggests
    using the Perceiver Transformer (Jaegle et al. ([2022](#bib.bib146))) as the policy
    backbone to manage extra long sequence of input (e.g., a sequence of image/voxel
    patches from the vision input and word tokens from the language input). (2) MARVAL
    (Kamath et al. ([2023](#bib.bib156))) uses an encoder-only transformer to process
    inputs comprising four modalities: instruction texts $(l_{1},\cdots,l_{m})$, historical
    states and actions $(x_{t-k},\cdots,x_{t-1})$, the current state $s_{t}$, and
    the current action candidates $\mathcal{A}_{t}$. As discussed in Section [2.4](#S2.SS4
    "2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    without the Masked MHA component as in the transformer decoder, each input token
    can attend to every other token for more informative aggregation. However, each
    forward pass predicts only a single action $a_{t}$, making it less efficient in
    training than the decoder-only transformer. In decoder-only transformers, the
    output token corresponding to the state input $s_{i}$ is used to predict $a_{i}$
    ($i=0,\cdots,t$). In contrast, encoder-only transformers require a special action
    token, i.e., CLS, as an input to capture the fused representation of the entire
    sequence via the attention mechanism, and its corresponding output token is utilized
    to predict the current action $a_{t}$. Additionally, MARVAL introduces auxiliary
    tasks, including predicting the masked language tokens and predicting the proportion
    of the trajectory that has been completed, to increase the amount of supervision
    for more efficient use of the demonstrations. (3) Instead of using instruction
    texts as conditions, Lang (Hejna et al. ([2023](#bib.bib128))) proposes to predict
    corresponding instructions from the state sequence as an auxiliary task, to realize
    language-guided IL. Specifically, each demonstration trajectory is divided into
    $m$ segments, with each segment $(s_{T_{i}},\cdots,s_{T_{t+1}-1})$ associated
    with a subtask instruction $l^{(i)}=(l_{1}^{(i)},\cdots,l^{(i)}_{b_{i}})$. A transformer
    encoder is applied to extract representations $(z_{1},\cdots,z_{t})$ from the
    state sequence $(s_{1},\cdots,s_{t})$. These representations are expected to encompass
    information essential for predicting both actions and instructions, achieved by
    minimizing the following equation: $-\sum_{t=1}^{T}\log\pi(a_{t}|z_{1},\cdots,z_{t})-\lambda\sum_{i=1}^{m}\sum_{j=1}^{b_{i}}\log
    P(l_{j}^{(i)}|l_{1}^{(i)},\cdots,l_{j-1}^{(i)},z_{1},\cdots,z_{T_{i}-1})$. This
    approach offers an alternative way to incorporate language instructions into action
    predictions and has proven effective when training with a limited amount of demonstrations.'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '该领域的另一个研究重点是语言条件的强化学习，旨在训练（机器人）代理以遵循人类指令。语言指令，由一系列单词组成，可以通过预训练的语言模型或预定义的嵌入表嵌入到一系列标记$(l_{1},\cdots,l_{m})$中。
    (1) 一种简单的语言条件强化学习方法，如TDT（Putterman等人（[2022](#bib.bib259)））所示，是将语言标记纳入策略输入，即$\pi(a_{t}|s_{t},x_{t-1},\cdots,x_{t-k},l_{1},\cdots,l_{m})$。尽管如此，$\pi$被实现为仅解码器的变换器，正如前面提到的，它能够并行处理多种类型的输入。Perceiver-Actor（Shridhar等人（[2022](#bib.bib296)））采用类似的设计，但建议使用Perceiver
    Transformer（Jaegle等人（[2022](#bib.bib146)））作为策略骨干，以处理额外长的输入序列（例如，来自视觉输入的图像/体素补丁序列和来自语言输入的单词标记）。
    (2) MARVAL（Kamath等人（[2023](#bib.bib156)））使用仅编码器的变换器来处理包括四种模态的输入：指令文本$(l_{1},\cdots,l_{m})$、历史状态和动作$(x_{t-k},\cdots,x_{t-1})$、当前状态$s_{t}$以及当前动作候选$\mathcal{A}_{t}$。如[2.4](#S2.SS4
    "2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")节中讨论的那样，没有像变换器解码器中的Masked
    MHA组件，每个输入标记可以关注每个其他标记以获得更有信息的聚合。然而，每次前向传递仅预测一个单一的动作$a_{t}$，使得其在训练中不如仅解码器的变换器高效。在仅解码器的变换器中，与状态输入$s_{i}$对应的输出标记用于预测$a_{i}$（$i=0,\cdots,t$）。相比之下，仅编码器的变换器需要一个特殊的动作标记，即CLS，作为输入，通过注意机制捕捉整个序列的融合表示，并利用其对应的输出标记预测当前动作$a_{t}$。此外，MARVAL引入了辅助任务，包括预测被屏蔽的语言标记和预测已完成的轨迹比例，以增加监督量，从而更有效地利用演示。
    (3) Lang（Hejna等人（[2023](#bib.bib128)））提出了一种方法，预测与状态序列对应的指令作为辅助任务，以实现语言指导的强化学习，而不是将指令文本作为条件。具体而言，每个演示轨迹被划分为$m$个段，每个段$(s_{T_{i}},\cdots,s_{T_{t+1}-1})$与一个子任务指令$l^{(i)}=(l_{1}^{(i)},\cdots,l^{(i)}_{b_{i}})$相关联。应用变换器编码器从状态序列$(s_{1},\cdots,s_{t})$中提取表示$(z_{1},\cdots,z_{t})$。这些表示预计包含对预测动作和指令至关重要的信息，通过最小化以下方程来实现：$-\sum_{t=1}^{T}\log\pi(a_{t}|z_{1},\cdots,z_{t})-\lambda\sum_{i=1}^{m}\sum_{j=1}^{b_{i}}\log
    P(l_{j}^{(i)}|l_{1}^{(i)},\cdots,l_{j-1}^{(i)},z_{1},\cdots,z_{T_{i}-1})$。这种方法提供了一种将语言指令融入动作预测的替代方式，并在使用有限的演示进行训练时已证明有效。'
- en: '| Algorithm | Key Novelty | Evaluation Task |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| Algorithm | 关键创新 | 评估任务 |'
- en: '| BeT | Dual-head predictions for the action center and residual action, seperately.
    | CARLA, Block Push, Franka Kitchen |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| BeT | 分别对动作中心和剩余动作进行双头预测。 | CARLA、Block Push、Franka Kitchen |'
- en: '| TSE | A transformer-based policy for end-to-end self-driving. | SMARTS |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| TSE | 一种基于变换器的端到端自动驾驶策略。 | SMARTS |'
- en: '| Silver-Bullet -3D | A transformer-based control policy for complex manipulation
    skills with a robotic arm. | ManiSkill |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| Silver-Bullet -3D | 一种基于变换器的控制策略，用于复杂的机器人臂操作技能。 | ManiSkill |'
- en: '| VIOLA | Adopting the transformer to identify task-relevant objects in a multi-object
    environment. | Sorting, Stacking, BUDS-Kitchen, real-world tasks |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| VIOLA | 采用变换器在多物体环境中识别任务相关物体。 | 排序、堆叠、BUDS-Kitchen、现实世界任务 |'
- en: '| TDT | Language-conditioned IL with the transformer. | Atari Frostbite |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| TDT | 使用变换器进行语言条件下的强化学习。 | Atari Frostbite |'
- en: '| Perceiver-Actor | Applying the Perceiver Transformer to manage long input
    sequences in Language-conditioned IL. | RLBench, real-world tasks |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| Perceiver-Actor | 将Perceiver变换器应用于处理语言条件下的长输入序列。 | RLBench、现实世界任务 |'
- en: '| MARVAL | Segmenting the input into 4 modalities and applying various auxiliary
    tasks for more efficient use of the demonstrations. | Matterport, Gibson |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| MARVAL | 将输入分为4种模态，并应用各种辅助任务以更高效地利用演示。 | Matterport、Gibson |'
- en: '| Lang | Improving long-horizon imitation by representation learning based
    on instruction predictions. | BabyAI, Crafting, ALFREAD |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| Lang | 基于指令预测的表示学习来提升长期模仿能力。 | BabyAI、Crafting、ALFREAD |'
- en: '| Transformer Adapter | Introducing Adapters to pretrained transformers for
    lightweight task-specific fine-tuning. | MetaWorld |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| Transformer Adapter | 向预训练变换器引入适配器以进行轻量级的任务特定微调。 | MetaWorld |'
- en: '| DualMind | A dual-phase training framework for generalist agents. | MetaWorld,
    Habitat |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| DualMind | 一种用于通用智能体的双阶段训练框架。 | MetaWorld、Habitat |'
- en: '| MIA | Training an interactive agent by imitation of human-human interactions
    and self-supervision via modality matching. | 3D Playhouse |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| MIA | 通过模仿人际互动和模态匹配的自我监督来训练交互式代理。 | 3D Playhouse |'
- en: 'Table 9: Summary of transformer-based IL algorithms. Representative (but not
    all) algorithms in this section with their key novelties and evaluation tasks
    are listed in this table. These algorithms are all grounded in the Behavioral
    Cloning framework, hence the algorithmic and theoretical advancements in this
    section are relatively modest. However, the learned transformer-based agents are
    evaluated on much more diverse, realistic, and challenging tasks. First, there
    are some commonly-used benchmarks. CARLA (Dosovitskiy et al. ([2017](#bib.bib78)))
    is a simulated environment for self-driving tasks. Block Push (Florence et al.
    ([2021](#bib.bib91))), Franka Kitchen (Gupta et al. ([2019a](#bib.bib114))), RLBench
    (James et al. ([2020](#bib.bib147))), MetaWorld (Yu et al. ([2019c](#bib.bib379)))
    provide a range of robotic manipulation tasks, featuring various types of robots
    and input modalities. Habitat (Ramakrishnan et al. ([2021](#bib.bib268))) provides
    navigation tasks for embodied AI in large scale 3D environments. BabyAI (Chevalier-Boisvert
    et al. ([2019](#bib.bib53))), Crafting (Chen et al. ([2021c](#bib.bib49))), ALFREAD
    (Shridhar et al. ([2020](#bib.bib295))) offer instruction-conditioned tasks and
    datasets. Second, some algorithms have shown superior performance on competitions.
    In the table, SMARTS refers to the NeurIPS 2022 Driving SMARTS Competition (Rasouli
    et al. ([2022](#bib.bib270))), and ManiSkill refers to SAPIEN ManiSkill Challenge
    2021 (Mu et al. ([2021](#bib.bib226))). Third, in the table, VIOLA, TDT, MARVAL,
    and MIA each develop their own environments or create large-scale training datasets
    tailored to their specific purposes. These efforts can be regarded as significant
    contributions alongside their algorithm designs. Readers are encouraged to read
    the respective papers for details.'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：基于变换器的 IL 算法总结。本节中代表性的（但并非全部）算法及其关键创新和评估任务列在此表中。这些算法均基于行为克隆框架，因此本节中的算法和理论进展相对较小。然而，学习到的基于变换器的智能体在更加多样化、现实且具有挑战性的任务上进行了评估。首先，有一些常用的基准测试。CARLA（Dosovitskiy
    等人（[2017](#bib.bib78)））是一个用于自动驾驶任务的模拟环境。Block Push（Florence 等人（[2021](#bib.bib91)））、Franka
    Kitchen（Gupta 等人（[2019a](#bib.bib114)））、RLBench（James 等人（[2020](#bib.bib147)））、MetaWorld（Yu
    等人（[2019c](#bib.bib379)））提供了一系列机器人操作任务，涵盖了各种类型的机器人和输入模态。Habitat（Ramakrishnan 等人（[2021](#bib.bib268)））提供了大规模
    3D 环境中的导航任务。BabyAI（Chevalier-Boisvert 等人（[2019](#bib.bib53)））、Crafting（Chen 等人（[2021c](#bib.bib49)））、ALFREAD（Shridhar
    等人（[2020](#bib.bib295)））提供了条件指令任务和数据集。其次，一些算法在竞赛中表现出色。在表中，SMARTS 指的是 NeurIPS 2022
    驾驶 SMARTS 竞赛（Rasouli 等人（[2022](#bib.bib270)）），而 ManiSkill 指的是 SAPIEN ManiSkill
    Challenge 2021（Mu 等人（[2021](#bib.bib226)））。第三，在表中，VIOLA、TDT、MARVAL 和 MIA 各自开发了自己的环境或创建了大规模的训练数据集，以适应其特定目的。这些努力可以视为与其算法设计同样重要的贡献。鼓励读者查阅相关论文以获取详细信息。
- en: 'Language-conditioned IL can be viewed as an instance of multi-task IL, where
    language texts serve as task contexts. (1) Generally speaking, $(l_{1},\cdots,l_{m})$
    could be substituted with any form of task context (Furuta et al. ([2022](#bib.bib101))).
    For example, Dasari & Gupta ([2020](#bib.bib66)) suggest using demonstration videos
    from human operators, denoted as $v$, as contexts for corresponding tasks. They
    train the policy $\pi(a_{t}|s_{t},\cdots,s_{t-k},v)$ using Eq. ([86](#S6.E86 "In
    6.2.1 A Paradigm of Transformer-based Imitation Learning ‣ 6.2 Imitation Learning
    ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), supplemented
    by an inverse model auxiliary loss term. (2) Transformer Adapter (Liang et al.
    ([2022](#bib.bib188))) presents an alternative approach for multi-task IL through
    pretraining and fine-tuning. Initially, a transformer policy $\pi(a_{t}|s_{t-k},a_{t-k},\cdots,s_{t-1},a_{t-1},s_{t})$,
    without task contexts, is pretrained on extensive multi-task demonstrations using
    Eq. ([86](#S6.E86 "In 6.2.1 A Paradigm of Transformer-based Imitation Learning
    ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")). It is then fine-tuned with a limited amount of task-specific demonstrations
    before application. For efficient fine-tuning, the pretrained model’s parameters
    are kept unchanged. Instead, lightweight adapters (Houlsby et al. ([2019](#bib.bib138)))
    are introduced between the pretrained model’s layers, with only these adapters
    being updated using the task-specific demonstrations. The structure of the adapter
    can be $\text{Adapter}(X)=X+W^{A}_{2}(\text{GeLU}(W^{A}_{1}(X)))$, where GeLU
    (Hendrycks & Gimpel ([2023](#bib.bib129))) is the activation function and $W^{A}_{1,2}$
    are the weight matrices of the adapter, and it is inserted between the point-wise
    FFN layer and Add & Normalization layer of the transformer (as shown in Fig. ([1](#S2.F1
    "Figure 1 ‣ 2.4 Transformers ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"))). The intuition behind this paradigm is that agents can acquire
    a diverse set of behavioral priors through large-scale task-agnostic pretraining,
    which then enables them to be efficiently fine-tuned for specific tasks. (3) As
    introduced in Section [6.1](#S6.SS1 "6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), the target of multi-task
    learning is a generalist agent, i.e., one model with a single set of weights that
    can be directly applied to a wide range of tasks. Different from Gato, which is
    directly trained on batches of prompt-conditioned demonstrations, DualMind (Wei
    et al. ([2023](#bib.bib349))) introduces a dual-phase method. Specifically, in
    Phase I, the transformer policy $\pi(a_{t}|s_{t-k},a_{t-k},\cdots,s_{t-1},a_{t-1},s_{t})$
    is trained with self-supervised learning objectives to capture generic information
    of state-action transitions, where the self-supervision involves predicting the
    next state, predicting the current action, and reconstructing masked actions based
    on the other elements. In Phase II, a prompt $p$, which can be either language
    or image tokens, is added as a condition to the policy, i.e., $\pi(a_{t}|p,s_{t-k},a_{t-k},\cdots,s_{t-1},a_{t-1},s_{t})$.
    To achieve this, extra Cross Attention layers are incorporated into the transformer
    decoder, enabling state/action tokens to attend to the prompt tokens. A small
    fraction of the transformer, including the Cross Attention layers, would then
    be trained via the prompt-conditioned IL, which is similar with TDT, using expert
    trajectories with associated prompts. This method emulates how humans learn to
    act in the world, i.e., acquiring skills in a task-agnostic manner and then learning
    specific tasks based on the acquired knowledge, and achieves superior performance
    in a series of robotic manipulation and navigation tasks. Abramson et al. ([2021](#bib.bib1))
    propose MIA, a multi-modal interactive agent that can naturally interact and communicate
    with humans. The agent can be simply trained by imitation learning of human-human
    interactions and self-supervised learning through an auxiliary task. Specifically,
    a multi-modal transformer is used to extract representations from the vision and
    language input tokens (i.e., $s^{V}_{t},s^{L}_{t}$) and is trained through the
    following objective:'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '语言条件的强化学习（IL）可以视为多任务IL的一个实例，其中语言文本作为任务上下文。 (1) 通常来说，$(l_{1},\cdots,l_{m})$
    可以被任何形式的任务上下文替代（Furuta et al. ([2022](#bib.bib101))）。例如，Dasari & Gupta ([2020](#bib.bib66))
    建议使用来自人类操作员的演示视频，记作 $v$，作为对应任务的上下文。他们使用公式 ([86](#S6.E86 "In 6.2.1 A Paradigm of
    Transformer-based Imitation Learning ‣ 6.2 Imitation Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) 训练策略 $\pi(a_{t}|s_{t},\cdots,s_{t-k},v)$，并补充了一个逆模型辅助损失项。
    (2) Transformer Adapter (Liang et al. ([2022](#bib.bib188))) 提出了通过预训练和微调的多任务IL的另一种方法。最初，一个没有任务上下文的变压器策略
    $\pi(a_{t}|s_{t-k},a_{t-k},\cdots,s_{t-1},a_{t-1},s_{t})$ 在使用公式 ([86](#S6.E86
    "In 6.2.1 A Paradigm of Transformer-based Imitation Learning ‣ 6.2 Imitation Learning
    ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) 的大规模多任务演示上进行预训练。然后，在应用之前，使用有限数量的任务特定演示对其进行微调。为了高效微调，预训练模型的参数保持不变。相反，轻量级适配器（Houlsby
    et al. ([2019](#bib.bib138))) 被引入到预训练模型的层之间，仅更新这些适配器，使用任务特定演示。适配器的结构可以是 $\text{Adapter}(X)=X+W^{A}_{2}(\text{GeLU}(W^{A}_{1}(X)))$，其中
    GeLU (Hendrycks & Gimpel ([2023](#bib.bib129))) 是激活函数，$W^{A}_{1,2}$ 是适配器的权重矩阵，它被插入到变压器的逐点FFN层和加法与归一化层之间（如图
    ([1](#S2.F1 "Figure 1 ‣ 2.4 Transformers ‣ 2 Background on Deep Generative Models
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) 所示）。这一范式背后的直觉是，代理可以通过大规模的任务无关预训练获得一组多样的行为先验，这使得它们能够高效地微调特定任务。
    (3) 如第 [6.1](#S6.SS1 "6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") 节介绍，多任务学习的目标是一个通用型代理，即一个具有单一权重集的模型，可以直接应用于各种任务。与直接在条件提示演示批次上训练的
    Gato 不同，DualMind (Wei et al. ([2023](#bib.bib349))) 引入了一个双阶段的方法。具体来说，在第一阶段，变压器策略
    $\pi(a_{t}|s_{t-k},a_{t-k},\cdots,s_{t-1},a_{t-1},s_{t})$ 使用自监督学习目标进行训练，以捕捉状态-动作转移的通用信息，其中自监督包括预测下一个状态、预测当前动作和基于其他元素重构被遮蔽的动作。在第二阶段，将提示
    $p$（可以是语言或图像令牌）作为条件添加到策略中，即 $\pi(a_{t}|p,s_{t-k},a_{t-k},\cdots,s_{t-1},a_{t-1},s_{t})$。为此，额外的交叉注意力层被纳入变压器解码器中，使状态/动作令牌能够关注提示令牌。然后，变压器的一个小部分，包括交叉注意力层，将通过条件提示IL进行训练，这与
    TDT 类似，使用与提示相关的专家轨迹。这种方法模拟了人类如何学习在世界中行动，即以任务无关的方式获得技能，然后基于所获得的知识学习特定任务，并在一系列机器人操作和导航任务中取得了优异的表现。Abramson
    et al. ([2021](#bib.bib1)) 提出了 MIA，一个可以自然与人类互动和沟通的多模态交互代理。该代理可以通过模仿人类互动和通过辅助任务进行自监督学习来简单地进行训练。具体来说，使用多模态变压器从视觉和语言输入令牌（即
    $s^{V}_{t},s^{L}_{t}$）中提取表示，并通过以下目标进行训练：'
- en: '|  | $\max_{\pi,f,D}\mathbb{E}_{D_{E}}\left[\sum_{t=0}^{T}\log\pi(a_{t}&#124;f(s^{V}_{t},s^{L}_{t}),\cdots,f(s^{V}_{0},s^{L}_{0}))+\lambda\sum_{t=0}^{T}\left[\log
    D(f(s^{V}_{t},s^{L}_{t}))+\log(1-D(f(s^{V}_{t},\tilde{s}^{L}_{t})))\right]\right]$
    |  | (87) |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi,f,D}\mathbb{E}_{D_{E}}\left[\sum_{t=0}^{T}\log\pi(a_{t}&#124;f(s^{V}_{t},s^{L}_{t}),\cdots,f(s^{V}_{0},s^{L}_{0}))+\lambda\sum_{t=0}^{T}\left[\log
    D(f(s^{V}_{t},s^{L}_{t}))+\log(1-D(f(s^{V}_{t},\tilde{s}^{L}_{t})))\right]\right]$
    |  | (87) |'
- en: Here, $f$ denotes the transformer to extract representations, $\pi$ is the overall
    policy which comprises multiple components besides $f$, $D$ is a discriminator
    (as in GANs) which is introduced to encourage $f$ to generate distinct representations
    for matched vision and text data (i.e., $(s^{V}_{t},s^{L}_{t})$) and unmatched
    ones (i.e., $(s^{V}_{t},\tilde{s}^{L}_{t})$). This modality matching auxiliary
    task, also used in (Mees et al. ([2022a](#bib.bib218))), has shown to significantly
    improve the agent’s performance beyond imitation learning alone.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$f$表示用于提取表示的变换器，$\pi$是包含多个组件的整体策略，除了$f$之外，$D$是一个判别器（如同GANs），用于鼓励$f$为匹配的视觉和文本数据（即$(s^{V}_{t},s^{L}_{t})$）以及不匹配的数据（即$(s^{V}_{t},\tilde{s}^{L}_{t})$）生成不同的表示。这种模态匹配的辅助任务，也用于（Mees
    et al. ([2022a](#bib.bib218)))，已经显示出显著提高了代理的性能，超出了仅仅模仿学习的效果。
- en: 'We summarize the representative algorithms discussed in this section in Table
    [9](#S6.T9 "Table 9 ‣ 6.2.3 Developing Generalist Imitation Learning Agents with
    Transformers ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), highlighting their key novelties and evaluation tasks.
    As mentioned in the beginning of this section, transformer-based IL algorithms
    fundamentally rely on the straightforward BC framework. However, these algorithms
    have demonstrated promising outcomes in complex robotic manipulation tasks, both
    simulated and real-world. This leads to an inspiring paradigm for robust robotic
    learning, that is, using a potent foundation model trained by imitation learning
    from extensive, high-quality demonstrations. Future research directions may include
    developing methods to synthesize high-quality training data at lower costs or
    enhancing/replacing the foundational algorithm BC with more advanced alternatives.'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格[9](#S6.T9 "Table 9 ‣ 6.2.3 Developing Generalist Imitation Learning Agents
    with Transformers ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")中总结了本节讨论的代表性算法，突出了它们的主要创新和评估任务。如本节开始时提到的，基于变换器的IL算法在根本上依赖于简单的BC框架。然而，这些算法在复杂的机器人操作任务中（无论是模拟还是现实世界）都展示了令人鼓舞的成果。这导致了一种激励性的鲁棒机器人学习范式，即使用通过从大量高质量演示中进行模仿学习训练的强大基础模型。未来的研究方向可能包括开发方法以更低的成本合成高质量的训练数据，或提升/替换基础算法BC以采用更先进的替代方案。'
- en: 7 Diffusion Models in Offline Policy Learning
  id: totrans-616
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 离线政策学习中的扩散模型
- en: 'In this section, we present applications of Diffusion Models (DM) in IL (i.e.,
    Section [7.1](#S7.SS1 "7.1 Imitation Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) and offline RL (i.e., Section
    [7.2](#S7.SS2 "7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")). In particular, there is an emerging
    body of researches on DM-based offline RL algorithms, while more extensions/explorations
    could be made for DM-based IL.'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们介绍了扩散模型（DM）在IL（即第[7.1](#S7.SS1 "7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")节）和离线RL（即第[7.2](#S7.SS2
    "7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")节）中的应用。特别是，基于DM的离线RL算法有一系列新兴的研究，而基于DM的IL还可以做更多的扩展/探索。'
- en: 7.1 Imitation Learning
  id: totrans-618
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 模仿学习
- en: 'Most works regarding applying Diffusion Models in IL are based on the Behavioral
    Cloning (BC) framework (Pomerleau ([1991](#bib.bib256))). To be specific, the
    objective of BC is to train a policy $\pi_{\theta}$ with which the likelihood
    of the expert demonstrations can be maximized:'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数关于在IL中应用扩散模型的工作都基于行为克隆（BC）框架（Pomerleau ([1991](#bib.bib256)))。具体来说，BC的目标是训练一个策略$\pi_{\theta}$，以最大化专家演示的可能性：
- en: '|  | $\max_{\theta}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log\pi_{\theta}(a&#124;s)\right]$
    |  | (88) |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\theta}\mathbb{E}_{(s,a)\sim D_{E}}\left[\log\pi_{\theta}(a\mid
    s)\right]$ |  | (88) |'
- en: 'As mentioned in Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), diffusion models have
    shown superior performance in modeling and generating data following a certain
    distribution, i.e., $P_{X}(x)$. Thus, many recent works have proposed to improve
    BC by implementing the policy network as a diffusion model to model the conditional
    distribution $P_{A|S}(a|s)$ within the expert data $D_{E}$. Further, as shown
    in Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"), the objective for training DM (i.e.,
    Eq. ([27](#S2.E27 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"))) provides an upper bound for the negative
    log-likelihood, which naturally connects DM with BC. Next, we introduce these
    works in details, which are designed to solve drawbacks of the original BC algorithm
    from multiple perspectives.'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[2.5节](#S2.SS5 "2.5 扩散模型 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向展望")中所述，扩散模型在模拟和生成遵循特定分布的数据方面表现出优越的性能，即$P_{X}(x)$。因此，许多近期的研究工作建议通过将策略网络实现为扩散模型来改善行为克隆（BC），以模拟专家数据$D_{E}$中的条件分布$P_{A|S}(a|s)$。进一步地，如第[2.5节](#S2.SS5
    "2.5 扩散模型 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向展望")中所示，训练扩散模型（即式 ([27](#S2.E27
    "在第2项 ‣ 2.5 扩散模型 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向展望")））的目标为负对数似然提供了上界，这自然将扩散模型与行为克隆联系起来。接下来，我们将详细介绍这些旨在从多个角度解决原始行为克隆算法缺陷的研究工作。
- en: 7.1.1 Improving Policy Expressiveness in Imitation Learning with Diffusion Models
  id: totrans-622
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1 在模仿学习中利用扩散模型提高策略表达能力
- en: 'The first group of works (Pearce et al. ([2023](#bib.bib246)); Chi et al. ([2023](#bib.bib54));
    Reuss et al. ([2023](#bib.bib274))) try to improve the expressiveness of the learned
    policy. Through minimizing the Mean Square Error (MSE) shown as Eq. ([88](#S7.E88
    "In 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")), the BC policy $\pi_{\theta}$ is trained to give out point
    estimates ^(18)^(18)18Each state-action pair $(s,a)$ is a data point., which precludes
    it from capturing the multi-modality of the state-action pairs in $D_{E}$. Moreover,
    $\pi_{\theta}$ is encouraged to learn an average distribution, as the objective
    is to maximize an expectation, which would result in bias towards more frequently
    occurring actions. Additionally, $\pi_{\theta}(a|s)$ is usually implemented as
    a diagonal Gaussian policy, thus the prediction of each action dimension is independent,
    potentially leading to uncoordinated behaviours in high-dimensional action spaces.
    On the other hand, DM has shown great potential in generating high-dimensional
    samples that adhere to complex, multi-modal distributions, while ensuring the
    precision and diversity. In this case, Diffusion BC (Pearce et al. ([2023](#bib.bib246)))
    is proposed to utilize the denoising (score) function $\epsilon_{\theta}$ in DDPM
    (Ho et al. ([2020](#bib.bib136))) to generate actions $a$ at given states $s$.
    Similarly with Eq. ([28](#S2.E28 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), the IL objective is
    as follows: (Please refer to Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") for notation definitions.)'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '第一组工作（Pearce等人（[2023](#bib.bib246)）；Chi等人（[2023](#bib.bib54)）；Reuss等人（[2023](#bib.bib274)））尝试提高学习到的策略的表现力。通过最小化均方误差（MSE），如等式（[88](#S7.E88
    "In 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")）所示，BC策略$\pi_{\theta}$被训练以给出点估计^（18）^（18）18每个状态-动作对$(s,a)$都是一个数据点。这样就无法捕捉到$D_{E}$中状态-动作对的多模态性。此外，$\pi_{\theta}$被鼓励学习一个平均分布，因为目标是最大化期望，这会导致对更频繁出现的动作产生偏见。此外，$\pi_{\theta}(a|s)$通常实现为对角高斯策略，因此每个动作维度的预测是独立的，这可能导致在高维动作空间中的不协调行为。另一方面，DM在生成符合复杂、多模态分布的高维样本方面显示出巨大潜力，同时确保精度和多样性。在这种情况下，Diffusion
    BC（Pearce等人（[2023](#bib.bib246)））被提出利用DDPM中的去噪（得分）函数$\epsilon_{\theta}$（Ho等人（[2020](#bib.bib136)））在给定状态$s$下生成动作$a$。与等式（[28](#S2.E28
    "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on Deep Generative Models ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")）类似，IL目标如下：（请参阅第[2.5节](#S2.SS5 "2.5 Diffusion Models ‣ 2
    Background on Deep Generative Models ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")以获取符号定义。）'
- en: '|  | $\min_{\theta}\mathbb{E}_{t\sim{\mathcal{U}}[1,T],(s,a)\sim D_{E},\epsilon\sim{\mathcal{N}}(0,I)}\left[&#124;&#124;\epsilon-\epsilon_{\theta}(a_{t}=\sqrt{\alpha_{t}}a+\sqrt{1-\alpha_{t}}\epsilon,t;s)&#124;&#124;^{2}\right]$
    |  | (89) |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta}\mathbb{E}_{t\sim{\mathcal{U}}[1,T],(s,a)\sim D_{E},\epsilon\sim{\mathcal{N}}(0,I)}\left[&#124;&#124;\epsilon-\epsilon_{\theta}(a_{t}=\sqrt{\alpha_{t}}a+\sqrt{1-\alpha_{t}}\epsilon,t;s)&#124;&#124;^{2}\right]$
    |  | (89) |'
- en: 'Note that $s$ is the conditioner which is not interrupted in the diffusion
    process. With the learned $\epsilon_{\theta}$, the estimated expert action $a=a_{0}$
    can be generated following the denoising process in DDPM:'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，$s$是扩散过程中未被打断的条件。利用学习到的$\epsilon_{\theta}$，可以按照DDPM中的去噪过程生成估计的专家动作$a=a_{0}$。
- en: '|  | $a_{t-1}=\frac{1}{\sqrt{1-\beta_{t}}}(a_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(a_{t},t;s))+\sqrt{\beta_{t}}\epsilon,\
    t=T,\cdots,1,\ a_{T}\sim{\mathcal{N}}(0,I)$ |  | (90) |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '|  | $a_{t-1}=\frac{1}{\sqrt{1-\beta_{t}}}(a_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(a_{t},t;s))+\sqrt{\beta_{t}}\epsilon,\
    t=T,\cdots,1,\ a_{T}\sim{\mathcal{N}}(0,I)$ |  | (90) |'
- en: 'Further, Diffusion Policy (Chi et al. ([2023](#bib.bib54))) is proposed to
    utilize DDPM for closed-loop action-sequence prediction. In particular, it models
    the distribution $P_{\vec{A}|\vec{S}}(\vec{a}|\vec{s})$ in $D_{E}$, where $\vec{s}$
    and $\vec{a}$ denote the previous $T_{s}$ states and future $T_{p}$ actions, respectively.
    $T_{a}$ out of the $T_{p}$ actions are executed without replanning. This framework
    allows long-horizon planning, encourages temporal consistency of the action sequence,
    and remains responsive to the changing environment through receding horizon planning.
    In order to learn the joint distribution of the $T_{p}$ actions conditioned on
    the historical states – a task that is inherently high-dimensional – Chi et al.
    ([2023](#bib.bib54)) suggest modeling it as $\epsilon_{\theta}(\vec{a}_{t},t|\vec{s}_{t})$,
    which is trained and sampled with a DDPM framework as delineated in Eq. ([89](#S7.E89
    "In 7.1.1 Improving Policy Expressiveness in Imitation Learning with Diffusion
    Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) and ([90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness
    in Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")).'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Diffusion Policy（Chi等人（[2023](#bib.bib54)））提出利用DDPM进行闭环动作序列预测。特别地，它对$D_{E}$中的分布$P_{\vec{A}|\vec{S}}(\vec{a}|\vec{s})$进行建模，其中$\vec{s}$和$\vec{a}$分别表示前$T_{s}$个状态和未来$T_{p}$个动作。
    $T_{a}$中的$T_{p}$个动作在没有重新规划的情况下执行。该框架允许长时间视野计划，鼓励动作序列的时间一致性，并通过递减视野规划对环境变化保持响应。为了学习$T_{p}$个动作在历史状态条件下的联合分布
    - 这本质上是一个高维任务 - Chi等人（[2023](#bib.bib54)）建议将其建模为$\epsilon_{\theta}(\vec{a}_{t},t|\vec{s}_{t})$，并且根据Eq.
    ([89](#S7.E89 "在7.1.1中通过扩散模型提高模仿学习中的策略表达能力 ‣ 7.1 模仿学习 ‣ 离线策略学习中的扩散模型 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向展望")和([90](#S7.E90
    "在7.1.1中通过扩散模型提高模仿学习中的策略表达能力 ‣ 7.1 模仿学习 ‣ 离线策略学习中的扩散模型 ‣ 深度生成模型的离线策略学习：教程、调查和未来方向")）中训练和采样。
- en: 'Last, Reuss et al. ([2023](#bib.bib274)) propose BESO for goal-conditioned
    IL. Specifically, one or more future states within the same trajectory as $(s,a)$
    are used as the goal $g$. The objective is now to get a goal-conditioned policy
    $\pi_{\theta}(a|s,g)$, which is learned as a conditional score function $S_{\theta}(a_{t},t;s,g)$
    in a Score SDE framework (Karras et al. ([2022](#bib.bib158))). With $S_{\theta}(a_{t},t;s,g)$,
    the action at $s$ targeting $g$ can be generated with a Probability Flow ODE,
    as introduced in Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"). Goal-conditioned policies distill
    useful, goal-oriented behaviors, which can be integrated with a high-level planner
    for various downstream tasks, as in Hierarchical RL. However, incorporating the
    goal conditioner amplifies the multimodal nature of the demonstrations, since
    the same goal might be achieved via various distinct trajectories, underscoring
    the necessity of employing DM. It’s worthy noting that all three works suggest
    using transformers as the denoising function (i.e., $\epsilon_{\theta},S_{\theta}$)
    for sample quality, rather than the commonly-used U-Nets (Ronneberger et al. ([2015](#bib.bib279)))
    for DM.'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Reuss等人（[2023](#bib.bib274)）提出了针对目标条件的IL的BESO。具体而言，作为目标$g$的$(s,a)$轨迹中的一个或多个未来状态被用作目标$g$。目标现在是获得一个目标条件策略$\pi_{\theta}(a|s,g)$，在Score
    SDE框架中学习为条件得分函数$S_{\theta}(a_{t},t;s,g)$（Karras等人（[2022](#bib.bib158)））。通过$S_{\theta}(a_{t},t;s,g)$，可以使用概率流ODE生成针对$g$的$s$处的动作，如在第[2.5](#S2.SS5
    "2.5 扩散模型 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向展望")节中介绍的。目标条件策略提炼有用的、以目标为导向的行为，这些行为可以与高级规划者在各种下游任务中集成，如分层RL。然而，整合目标调节器会增强示范的多模态性质，因为通过各种不同的轨迹可能达到相同的目标，强调了使用DM的必要性。值得注意的是，所有三项工作都建议将变压器用作噪声函数（即$\epsilon_{\theta},S_{\theta}$）以获取样本质量，而不是常用的U-Net（Ronneberger等人（[2015](#bib.bib279)））用于DM。
- en: 7.1.2 Addressing Common Issues of Imitation Learning with Diffusion Models
  id: totrans-629
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2 使用扩散模型解决模仿学习的常见问题
- en: 'Beyond enhancing policy expressiveness, numerous studies have explored the
    use of DM to address other challenges in IL, including spurious correlations (Saxena
    et al. ([2023](#bib.bib284))) and noisy (suboptimal) demonstrations (Wang et al.
    ([2023e](#bib.bib343)); Yuan et al. ([2023b](#bib.bib383))). To be specific, due
    to spurious correlations, expressive models, such as DM, may focus on distractors
    that are irrelevant to action prediction and thus fragile in real-world deployment,
    which resembles the causal misidentification issue introduced in Section [3.2.5](#S3.SS2.SSS5
    "3.2.5 Tackling Causal Confusion in Imitation Learning with VAEs ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"). Thus, Saxena et al. ([2023](#bib.bib284)) propose C3DM to improve
    the denoising process of Diffusion BC as follows:'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '除了增强策略表达能力外，许多研究探讨了使用DM解决IL中其他挑战的方法，包括虚假相关（Saxena et al. ([2023](#bib.bib284)))和噪声（次优）示范（Wang
    et al. ([2023e](#bib.bib343)); Yuan et al. ([2023b](#bib.bib383)))。具体来说，由于虚假相关，表达性模型如DM可能会关注与动作预测无关的干扰因素，从而在实际部署中表现脆弱，这类似于在第[3.2.5](#S3.SS2.SSS5
    "3.2.5 Tackling Causal Confusion in Imitation Learning with VAEs ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")节中引入的因果误识别问题。因此，Saxena et al. ([2023](#bib.bib284))提出了C3DM，以改进Diffusion
    BC的去噪过程如下：'
- en: '|  | $s_{t}=C(s;\text{pos}(a_{t}),t),\ a_{t-1}=\frac{1}{\sqrt{1-\beta_{t}}}(a_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(a_{t},t;s_{t}))+\sqrt{\beta_{t}}\epsilon,\
    t=T,\cdots,1,\ a_{T}\sim{\mathcal{N}}(0,I)$ |  | (91) |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{t}=C(s;\text{pos}(a_{t}),t),\ a_{t-1}=\frac{1}{\sqrt{1-\beta_{t}}}(a_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(a_{t},t;s_{t}))+\sqrt{\beta_{t}}\epsilon,\
    t=T,\cdots,1,\ a_{T}\sim{\mathcal{N}}(0,I)$ |  | (91) |'
- en: 'Compared with Eq. ([90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness in
    Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")), the only
    difference is to replace the conditioner $s$ with $s_{t}$ which is updated with
    the denoising process. In particular, at each denosing iteration $t$, the image
    state $s$ is zoomed into the region around the intermediate action, i.e., $\text{pos}(a_{t})$,
    to acquire more details for decision-making while ignoring distractors in other
    regions. Note that this work assume the access to a transformation between the
    action and state spaces to determine $\text{pos}(a_{t})$. Accordingly, they modify
    the training process in Eq. ([89](#S7.E89 "In 7.1.1 Improving Policy Expressiveness
    in Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) by replacing
    $s$ with $C(s;\text{pos}(a),t)$. Further, considering that expert demonstrations
    in real-world scenarios are often noisy, Wang et al. ([2023e](#bib.bib343)) propose
    DP-IL to adopt DM for purifying/denoising the demonstrations. Sequentially, any
    IL algorithm can be applied on these purified data. To be specific, based on the
    DDPM framework, they view $(s,a)$ as the data point $x$ (in Eq. ([28](#S2.E28
    "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on Deep Generative Models ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))) and learn a denoising function $\epsilon_{\theta}(x_{t},t)$
    to recover the original state-action demonstrations (i.e., $x_{0}$) from samples
    interrupted with Gaussian noise (i.e., $x_{t}$). Note that this training process
    is based on a set of “clean” demonstartions. Then, for imperfect data points $\tilde{x}$,
    the learned DM can be used to purify them by first adding random noise to $\tilde{x}$
    to get $\tilde{x}_{t}$ (i.e., via the forward process) and then using $\epsilon_{\theta}$
    in the reverse denoising process to get the purified data $\tilde{x}_{0}$. Theoretically,
    they prove that $\tilde{x}_{t}$ and $x_{t}$ can be arbitrarily closed in distribution
    as $t$ increases, and so $\epsilon_{\theta}$ trained on $x_{t}$ can be used to
    convert $\tilde{x}_{t}$ into purified demonstrations as well. Different from the
    two-stage framework in DP-IL, SMILE (Yuan et al. ([2023b](#bib.bib383))) is proposed
    to jointly perform the automatic filtering of noisy demonstrations and learning
    of the expert policy. It is based on Diffusion BC (i.e., Eq. ([89](#S7.E89 "In
    7.1.1 Improving Policy Expressiveness in Imitation Learning with Diffusion Models
    ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) and ([90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness
    in Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"))), but changes
    the forward diffusion process of DDPM as follows:'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '与 Eq. ([90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness in Imitation
    Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) 相比，唯一的区别是将调节器 $s$ 替换为通过去噪过程更新的
    $s_{t}$。具体来说，在每次去噪迭代 $t$ 中，图像状态 $s$ 会被放大到中间动作的区域，即 $\text{pos}(a_{t})$，以获取更多决策细节，同时忽略其他区域的干扰因素。请注意，这项工作假设可以获取动作和状态空间之间的转换来确定
    $\text{pos}(a_{t})$。因此，他们通过用 $C(s;\text{pos}(a),t)$ 替换 $s$ 来修改 Eq. ([89](#S7.E89
    "In 7.1.1 Improving Policy Expressiveness in Imitation Learning with Diffusion
    Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) 中的训练过程。此外，考虑到现实世界中的专家演示通常是噪声的，Wang 等人 ([2023e](#bib.bib343))
    提出了 DP-IL，利用 DM 来净化/去噪这些演示。随后，任何 IL 算法都可以应用于这些净化后的数据。具体而言，基于 DDPM 框架，他们将 $(s,a)$
    视为数据点 $x$（在 Eq. ([28](#S2.E28 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))) 并学习一个去噪函数 $\epsilon_{\theta}(x_{t},t)$
    从被高斯噪声打断的样本（即 $x_{t}$）中恢复原始状态-动作演示（即 $x_{0}$）。请注意，这个训练过程是基于一组“干净”的演示。然后，对于不完美的数据点
    $\tilde{x}$，可以通过首先向 $\tilde{x}$ 添加随机噪声得到 $\tilde{x}_{t}$（即通过前向过程），再利用 $\epsilon_{\theta}$
    在反向去噪过程中得到净化后的数据 $\tilde{x}_{0}$。理论上，他们证明了随着 $t$ 的增加，$\tilde{x}_{t}$ 和 $x_{t}$
    在分布上可以任意接近，因此在 $x_{t}$ 上训练的 $\epsilon_{\theta}$ 也可以用于将 $\tilde{x}_{t}$ 转换为净化的演示。与
    DP-IL 的两阶段框架不同，SMILE（Yuan 等人 ([2023b](#bib.bib383))) 被提出用于同时自动过滤噪声演示和学习专家策略。它基于
    Diffusion BC（即 Eq. ([89](#S7.E89 "In 7.1.1 Improving Policy Expressiveness in
    Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) 和 ([90](#S7.E90
    "In 7.1.1 Improving Policy Expressiveness in Imitation Learning with Diffusion
    Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")))，但对 DDPM 的前向扩散过程进行了如下修改：'
- en: '|  | $a_{t}\sim\pi^{t}(\cdot&#124;s),\ \pi^{t}(a_{t}&#124;s)=\int\pi^{0}(a_{0}&#124;s){\mathcal{N}}(a_{t}&#124;a_{0},\sigma_{t}^{2}I)\
    da_{0},\ \sigma_{t}^{2}=\sum_{k=1}^{t}\beta_{k}^{2}$ |  | (92) |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '|  | $a_{t}\sim\pi^{t}(\cdot\mid s),\ \pi^{t}(a_{t}\mid s)=\int\pi^{0}(a_{0}\mid
    s){\mathcal{N}}(a_{t}\mid a_{0},\sigma_{t}^{2}I)\ da_{0},\ \sigma_{t}^{2}=\sum_{k=1}^{t}\beta_{k}^{2}$
    |  | (92) |'
- en: Here, $\pi^{0}$ denotes the underlying policy of the provided demonstrations.
    Through this equation, the diffusion process of SMILE is conducted on a policy
    level rather than on data points, and so the learned denoising function $\epsilon_{\theta}$
    can be used to compare the suboptimality among behavior policies underlying different
    trajectories (i.e., with Eq. (13) in (Yuan et al. ([2023b](#bib.bib383)))). Trajectories
    generated using policies noisier than the currently learned one are then filtered
    out from the training dataset.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\pi^{0}$ 表示提供的示范的基础策略。通过这个方程，SMILE 的扩散过程是在策略层面上进行的，而不是在数据点上，因此学习到的去噪函数
    $\epsilon_{\theta}$ 可以用来比较不同轨迹下行为策略的次优性（即，参见 (Yuan et al. ([2023b](#bib.bib383))）中的公式
    (13)）。使用比当前学习到的策略更嘈杂的策略生成的轨迹会从训练数据集中筛除。
- en: '| Algorithm | Diffusion BC | Diffusion Policy | BESO | C3DM | DP-IL | SMILE
    |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 扩散 BC | 扩散策略 | BESO | C3DM | DP-IL | SMILE |'
- en: '| DM Type | DDPM | DDPM | Score SDE | DDPM | DDPM | DDPM |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| DM 类型 | DDPM | DDPM | 分数 SDE | DDPM | DDPM | DDPM |'
- en: '| DM Usage | Policy Generator | Policy Generator | Policy Generator | Policy
    Generator | Data Denoiser | Policy Generator |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| DM 使用 | 策略生成器 | 策略生成器 | 策略生成器 | 策略生成器 | 数据去噪器 | 策略生成器 |'
- en: '| IL Framework | BC | BC | BC | BC | Not Limited | BC |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| IL 框架 | BC | BC | BC | BC | 不受限制 | BC |'
- en: '| Targeted IL Issues | Multimodal distribution | Multimodal distribution &
    HD data | Multimodal distribution | Spurious correlation | Noisy data | Noisy
    data |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '| 目标 IL 问题 | 多模态分布 | 多模态分布 & HD 数据 | 多模态分布 | 虚假相关 | 噪声数据 | 噪声数据 |'
- en: '| Evaluation Task | Kitchen, CSGO | Kitchen, Push-T, Block-Push, Robomimic
    | Kitchen, Block-Push, CALVIN | Autodesk, Real Robot | MuJoCo | MuJoCo |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
  zh: '| 评估任务 | Kitchen, CSGO | Kitchen, Push-T, Block-Push, Robomimic | Kitchen,
    Block-Push, CALVIN | Autodesk, Real Robot | MuJoCo | MuJoCo |'
- en: 'Table 10: Summary of DM-based IL algorithms. Specifically, “HD” is short for
    “high-dimensional”. Regarding the evaluation tasks, Kitchen (Gupta et al., [2019b](#bib.bib115)),
    Block-Push & Push-T (Florence et al., [2022](#bib.bib92)), Robomimic (Mandlekar
    et al., [2021b](#bib.bib210)), CALVIN (Mees et al., [2022b](#bib.bib219)) are
    robotic manipulation tasks in different scenarios; CSGO (Pearce & Zhu, [2022](#bib.bib245))
    is a 3D First-person Shooter video game; Autodesk (Koga et al., [2022](#bib.bib170))
    and Real Robot (Saxena et al., [2023](#bib.bib284)) represent robotic manipulation
    tasks built on the Autodesk simulator and real robot platforms, respectively;
    MuJoCo (Todorov et al. ([2012](#bib.bib320))) provides a series of robotic locomotion
    tasks.'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：基于 DM 的 IL 算法总结。具体来说，“HD”是“高维”的缩写。关于评估任务，Kitchen (Gupta et al., [2019b](#bib.bib115))、Block-Push
    & Push-T (Florence et al., [2022](#bib.bib92))、Robomimic (Mandlekar et al., [2021b](#bib.bib210))、CALVIN
    (Mees et al., [2022b](#bib.bib219)) 是在不同场景中的机器人操作任务；CSGO (Pearce & Zhu, [2022](#bib.bib245))
    是一款 3D 第一人称射击视频游戏；Autodesk (Koga et al., [2022](#bib.bib170)) 和 Real Robot (Saxena
    et al., [2023](#bib.bib284)) 分别代表基于 Autodesk 模拟器和真实机器人平台的机器人操作任务；MuJoCo (Todorov
    et al. ([2012](#bib.bib320))) 提供了一系列机器人运动任务。
- en: While not exclusively designed for DM-based IL, the studies by Shi et al. ([2023](#bib.bib294))
    and Sridhar et al. ([2023](#bib.bib310)) introduce techniques — namely, AWE and
    MCNN — that further enhance the performance of Diffusion Policy and Diffusion
    BC, respectively. Both techniques focus on mitigating the compounding errors of
    IL, i.e., prediction errors compounded over the decision horizon. AWE can be applied
    to automatically extract waypoints within an expert trajectory. The waypoint sequences
    are notably shorter in horizon and can then be used for waypoint-only imitation.
    On the other hand, MCNN doesn’t modify the training data but enhances the policy
    for reduced errors. During evaluation, it identifies the nearest neighbor state
    of the current state in the expert dataset, and then blends the corresponding
    expert action with the output of the learned policy network to formulate an error-constrained
    policy.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管并非专门为基于DM的IL设计，Shi等人 ([2023](#bib.bib294)) 和Sridhar等人 ([2023](#bib.bib310))
    的研究引入了技术——即AWE和MCNN——进一步提升了扩散策略和扩散BC的性能。这两种技术都侧重于减轻IL的累积误差，即在决策范围内积累的预测误差。AWE可用于自动提取专家轨迹中的路标点。路标点序列明显较短，然后可以用于仅路标点的模仿。另一方面，MCNN不修改训练数据，但通过增强策略以减少误差。在评估过程中，它识别专家数据集中当前状态的最近邻状态，然后将相应的专家动作与学习的策略网络的输出进行混合，以制定误差约束策略。
- en: 'In Table [10](#S7.T10 "Table 10 ‣ 7.1.2 Addressing Common Issues of Imitation
    Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), we present a summary
    of DM-based IL methods. This includes their base IL algorithms, the specific IL
    challenges they aim to address, and how DM is used in resolving these issues.
    There are several promising future research directions in this field. Firstly,
    while most current works rely on DDPM for its robust performance, future studies
    could explore more advanced DM, particularly those with efficient sampling schemes
    which are crucial for sequential decision-making ^(19)^(19)19At each time step,
    to sample $a\sim\pi_{\theta}(\cdot|s)$, the entire denoising process (with $T$
    iterations) shown as Eq. ([90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness
    in Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")) needs to
    be executed, thus the sampling efficiency of DM is essential.. Secondly, given
    that these works target various IL issues, the development of a unified (DM-based)
    algorithm capable of simultaneously resolving multiple issues is also a promising
    direction. Finally, integrating DM with more advanced IL frameworks, as listed
    in Table [6](#S5.T6 "Table 6 ‣ 5.1.2 Policy Modeling in Imitation Learning with
    Normalizing Flows ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"), could offer solutions to fundamental
    issues in BC.'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[10](#S7.T10 "Table 10 ‣ 7.1.2 Addressing Common Issues of Imitation Learning
    with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")中，我们总结了基于DM的IL方法。这包括它们的基础IL算法、它们旨在解决的具体IL挑战，以及DM如何用于解决这些问题。该领域有几个有前途的未来研究方向。首先，尽管目前的大多数工作依赖于DDPM因其强大的性能，未来的研究可以探索更先进的DM，特别是那些具有高效采样方案的DM，这对于序列决策至关重要
    ^(19)^(19)19每一步，为了采样 $a\sim\pi_{\theta}(\cdot|s)$，需要执行整个去噪过程（具有 $T$ 次迭代），如公式([90](#S7.E90
    "In 7.1.1 Improving Policy Expressiveness in Imitation Learning with Diffusion
    Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")）所示，因此DM的采样效率至关重要。其次，由于这些工作针对各种IL问题，开发一种统一的（基于DM的）算法，能够同时解决多个问题也是一个有前途的方向。最后，将DM与更先进的IL框架结合，如表[6](#S5.T6
    "Table 6 ‣ 5.1.2 Policy Modeling in Imitation Learning with Normalizing Flows
    ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")中列出的，可能会为BC中的基本问题提供解决方案。'
- en: 7.2 Offline Reinforcement Learning
  id: totrans-644
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 离线强化学习
- en: 'Recently, there is an emerging body of advancements in applying Diffusion Models
    to offline RL (Zhu et al. ([2023c](#bib.bib398))). In particular, Diffusion Models
    can be adopted as the policy, planner, or data synthesizer in the context of offline
    RL, as introduced in Section [7.2.1](#S7.SS2.SSS1 "7.2.1 Diffusion Models as Policies
    ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") - [7.2.3](#S7.SS2.SSS3 "7.2.3 Diffusion Models as Data
    Synthesizers ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"). Additionally, we present the
    applications of DM for extended offline RL setups in Section [7.2.4](#S7.SS2.SSS4
    "7.2.4 Diffusion Models in Extended Offline Reinforcement Learning Setups ‣ 7.2
    Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"). For each category, we introduce the seminal works in details
    as a tutorial on its paradigm, followed by a brief review of the extensions with
    a focus on their key novelties.'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在将扩散模型应用于离线RL方面出现了一系列进展（Zhu et al. ([2023c](#bib.bib398)））。特别是，扩散模型可以作为策略、规划器或数据合成器应用于离线RL的背景中，正如在第[7.2.1](#S7.SS2.SSS1
    "7.2.1 扩散模型作为策略 ‣ 7.2 离线强化学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 深度生成模型在离线策略学习中的教程、调查和未来方向的展望")节中介绍的
    - [7.2.3](#S7.SS2.SSS3 "7.2.3 扩散模型作为数据合成器 ‣ 7.2 离线强化学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 深度生成模型在离线策略学习中的教程、调查和未来方向的展望")。此外，我们在第[7.2.4](#S7.SS2.SSS4
    "7.2.4 扩散模型在扩展离线强化学习设置中的应用 ‣ 7.2 离线强化学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 深度生成模型在离线策略学习中的教程、调查和未来方向的展望")节中介绍了DM在扩展离线RL设置中的应用。对于每个类别，我们详细介绍了其范式中的开创性工作，然后简要回顾了扩展工作，重点关注其关键创新。
- en: 7.2.1 Diffusion Models as Policies
  id: totrans-646
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1 扩散模型作为策略
- en: 'As introduced in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), in
    order to alleviate the value overestimation issue caused by OOD actions, four
    offline RL schemes are developed. Among them, DM-based offline RL usually follows
    the policy constraint method, that is, constraining the learned policy $\pi_{\theta}$
    to the behavior policy $\mu$ while maximizing the Q-function:'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在第[3.1.1](#S3.SS1.SSS1 "3.1.1 关于基于动态规划的离线强化学习的背景 ‣ 3.1 离线强化学习 ‣ 3 变分自编码器在离线策略学习中的应用
    ‣ 深度生成模型在离线策略学习中的教程、调查和未来方向的展望")节中介绍的，为了缓解由OOD动作引起的价值高估问题，开发了四种离线RL方案。其中，基于DM的离线RL通常遵循策略约束方法，即在最大化Q函数的同时，将学习到的策略$\pi_{\theta}$约束于行为策略$\mu$：
- en: '|  | $\max_{\theta}\mathbb{E}_{s\sim D_{\mu}}\left[\mathbb{E}_{a\sim\pi_{\theta(\cdot&#124;s)}}Q_{\phi}(s,a)-\lambda
    D_{KL}(\pi_{\theta}(\cdot&#124;s)&#124;&#124;\mu(\cdot&#124;s))\right]$ |  | (93)
    |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\theta}\mathbb{E}_{s\sim D_{\mu}}\left[\mathbb{E}_{a\sim\pi_{\theta(\cdot&#124;s)}}Q_{\phi}(s,a)-\lambda
    D_{KL}(\pi_{\theta}(\cdot&#124;s)&#124;&#124;\mu(\cdot&#124;s))\right]$ |  | (93)
    |'
- en: 'where $Q_{\phi}$ is a learned Q-function for the current policy $\pi_{\theta}$,
    and $\lambda>0$ is the Lagrangian multiplier ^(20)^(20)20As a common practice,
    $\lambda$ can be fine-tuned as a hyperparameter, controlling the tradeoff between
    the two objective terms.. Such an optimization problem has a closed-form solution,
    i.e., $\pi^{*}(a|s)=\frac{1}{Z(s)}\mu(a|s)\exp(\frac{1}{\lambda}Q_{\phi}(s,a))$,
    where $Z(s)$ is the partition function for normalization. Suppose that $\pi^{*}$
    can be represented as a parametric function, then the optimal policy can be learned
    through weighted regression (Wang et al. ([2020](#bib.bib346))) as below:'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$Q_{\phi}$是当前策略$\pi_{\theta}$的学习Q函数，$\lambda>0$是拉格朗日乘子^(20)^(20)20作为一种常见做法，$\lambda$可以作为超参数进行微调，以控制两个目标项之间的权衡。这个优化问题有一个闭式解，即$\pi^{*}(a|s)=\frac{1}{Z(s)}\mu(a|s)\exp(\frac{1}{\lambda}Q_{\phi}(s,a))$，其中$Z(s)$是用于归一化的分区函数。假设$\pi^{*}$可以表示为参数函数，那么可以通过加权回归（Wang
    et al. ([2020](#bib.bib346)））来学习最优策略，如下所示：
- en: '|  | $\max_{\theta}\mathbb{E}_{(s,a)\sim D_{\mu}}\left[\frac{1}{Z(s)}\log\pi_{\theta}(a&#124;s)\exp(\frac{1}{\lambda}Q_{\phi}(s,a))\right]$
    |  | (94) |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\theta}\mathbb{E}_{(s,a)\sim D_{\mu}}\left[\frac{1}{Z(s)}\log\pi_{\theta}(a|s)\exp(\frac{1}{\lambda}Q_{\phi}(s,a))\right]$
    |  | (94) |'
- en: Therefore, the parametric policy $\pi_{\theta}$ should be expressive enough
    to recover the optimal policy.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，参数化策略 $\pi_{\theta}$ 应该具有足够的表达能力来恢复最优策略。
- en: 'In this case, DM can be employed to model the policy $\pi_{\theta}$. (1) SfBC
    (Chen et al. ([2023b](#bib.bib43))) provides a straightforward manner to realize
    this. It first imitates the behavior policy $\mu(a|s)$ with a DM $\pi_{\theta}(a|s)$
    (specifically, Score SDE) in an IL scheme as introduced in Section [7.1.1](#S7.SS1.SSS1
    "7.1.1 Improving Policy Expressiveness in Imitation Learning with Diffusion Models
    ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"). Then, for any state $s$, $N$ actions are sampled with
    $\pi_{\theta}(\cdot|s)$ as candidates, and one action is resampled from these
    candidates with $\exp(\frac{1}{\lambda}Q_{\phi}(s,a))$ being the sampling weights.
    In this way, $a\sim\pi^{*}(\cdot|s)$ is approximated via importance sampling.
    Note that $Q_{\phi}(s,a)$ can be learned with any offline RL protocol. IDQL (Hansen-Estruch
    et al. ([2023](#bib.bib122))) adopts the same resampling scheme but imitates $\mu(a|s)$
    with DDPM. Additionally, rather than using $\exp(\frac{1}{\lambda}Q_{\phi}(s,a))$,
    they design a sampling weight based on the Implicit Q-Learning framework (Kostrikov
    et al. ([2022](#bib.bib172))), which is a SOTA offline RL algorithm. (2) Alternatively,
    the Q-function $Q_{\phi}(s,a)$ can be directly involved in training the DM policy
    $\pi_{\theta}(a|s)$. In particular, Diffusion-QL (Wang et al. ([2023f](#bib.bib344)))
    learns a DDPM-based policy through the following objective:'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: '在这种情况下，DM 可以用于建模策略 $\pi_{\theta}$。 (1) SfBC (Chen et al. ([2023b](#bib.bib43)))
    提供了一种实现这一目标的直接方法。它首先在 IL 方案中通过 DM $\pi_{\theta}(a|s)$（具体来说是 Score SDE）模仿行为策略 $\mu(a|s)$，如第
    [7.1.1](#S7.SS1.SSS1 "7.1.1 Improving Policy Expressiveness in Imitation Learning
    with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") 节所介绍。然后，对于任何状态 $s$，从 $\pi_{\theta}(\cdot|s)$
    中采样 $N$ 个动作作为候选，然后从这些候选中重新采样一个动作，其中 $\exp(\frac{1}{\lambda}Q_{\phi}(s,a))$ 作为采样权重。通过这种方式，$a\sim\pi^{*}(\cdot|s)$
    通过重要性采样进行逼近。注意 $Q_{\phi}(s,a)$ 可以通过任何离线 RL 协议进行学习。IDQL (Hansen-Estruch et al.
    ([2023](#bib.bib122))) 采用相同的重新采样方案，但使用 DDPM 来模仿 $\mu(a|s)$。此外，他们设计了一个基于隐式 Q 学习框架
    (Kostrikov et al. ([2022](#bib.bib172))) 的采样权重，而不是使用 $\exp(\frac{1}{\lambda}Q_{\phi}(s,a))$，这是一个
    SOTA 离线 RL 算法。 (2) 另一种方法是直接将 Q 函数 $Q_{\phi}(s,a)$ 纳入 DM 策略 $\pi_{\theta}(a|s)$
    的训练中。特别地，Diffusion-QL (Wang et al. ([2023f](#bib.bib344))) 通过以下目标学习基于 DDPM 的策略：'
- en: '|  | $\min_{\theta}\mathcal{L}_{DM}(\theta)-\frac{\eta}{\mathbb{E}_{(s,a)\sim
    D_{\mu}}\left[&#124;Q_{\phi}(s,a)&#124;\right]}\mathbb{E}_{s\sim D_{\mu},a_{0}\sim\pi_{\theta}(\cdot&#124;s)}\left[Q_{\phi}(s,a_{0})\right]$
    |  | (95) |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta}\mathcal{L}_{DM}(\theta)-\frac{\eta}{\mathbb{E}_{(s,a)\sim
    D_{\mu}}\left[|Q_{\phi}(s,a)|\right]}\mathbb{E}_{s\sim D_{\mu},a_{0}\sim\pi_{\theta}(\cdot|s)}\left[Q_{\phi}(s,a_{0})\right]$
    |  | (95) |'
- en: 'Here, $\mathcal{L}_{DM}(\theta)$ is defined as Eq. ([89](#S7.E89 "In 7.1.1
    Improving Policy Expressiveness in Imitation Learning with Diffusion Models ‣
    7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) (i.e., an IL objective) and can be replaced by the corresponding
    training objective if a different DM is employed, $\pi_{\theta}$ is implied by
    the denoising function $\epsilon_{\theta}$, $a_{0}$ is the action sample after
    being denoised for $T$ iterations (with Eq. ([90](#S7.E90 "In 7.1.1 Improving
    Policy Expressiveness in Imitation Learning with Diffusion Models ‣ 7.1 Imitation
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))),
    $\mathbb{E}_{(s,a)\sim D_{\mu}}\left[|Q_{\phi}(s,a)|\right]$ is incorporated for
    adaption to Q-functions with different scales. Intuitively, the Q-function acts
    as guidance in the reverse generation process of the DM. Also, Eq. ([95](#S7.E95
    "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement Learning ‣
    7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) mirrors
    Eq. ([93](#S7.E93 "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    since both equations encourage the learned policy to maximize Q-values while being
    closed to the behavior policy, and it’s noteworthy that DiffCPS (He et al. ([2023b](#bib.bib127)))
    provides a theoretical connection between Eq. ([95](#S7.E95 "In 7.2.1 Diffusion
    Models as Policies ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) and Eq. ([93](#S7.E93
    "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement Learning ‣
    7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")).'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\mathcal{L}_{DM}(\theta)$ 定义如公式 ([89](#S7.E89 "在 7.1.1 使用扩散模型改善模仿学习中的策略表达性
    ‣ 7.1 模仿学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向"))（即 IL 目标），如果使用不同的
    DM，则可以用相应的训练目标替代，$\pi_{\theta}$ 由去噪函数 $\epsilon_{\theta}$ 隐含，$a_{0}$ 是经过 $T$ 次迭代去噪后的动作样本（见公式
    ([90](#S7.E90 "在 7.1.1 使用扩散模型改善模仿学习中的策略表达性 ‣ 7.1 模仿学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向"))），$\mathbb{E}_{(s,a)\sim
    D_{\mu}}\left[|Q_{\phi}(s,a)|\right]$ 被引入以适应不同尺度的 Q 函数。直观地，Q 函数在 DM 的逆生成过程中起到指导作用。此外，公式
    ([95](#S7.E95 "在 7.2.1 扩散模型作为策略 ‣ 7.2 离线强化学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向"))
    与公式 ([93](#S7.E93 "在 7.2.1 扩散模型作为策略 ‣ 7.2 离线强化学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向"))
    对应，因为这两个方程都鼓励学习到的策略在接近行为策略的同时最大化 Q 值，值得注意的是，DiffCPS（He 等 ([2023b](#bib.bib127)))
    提供了公式 ([95](#S7.E95 "在 7.2.1 扩散模型作为策略 ‣ 7.2 离线强化学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向"))
    和公式 ([93](#S7.E93 "在 7.2.1 扩散模型作为策略 ‣ 7.2 离线强化学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向"))
    之间的理论联系。
- en: 'Following Diffusion-QL, several algorithms adopting a similar objective design
    with Eq. ([95](#S7.E95 "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))
    have been proposed, including SRDP (Ada et al. ([2023](#bib.bib2))), EDP (Kang
    et al. ([2023](#bib.bib157))), and Consistency-AC (Ding & Jin ([2023](#bib.bib71))).
    To be specific, SRDP additionally introduces a state-reconstruction loss term
    in Eq. ([95](#S7.E95 "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))
    to enable generalization for OOD states through self-supervised learning (Liu
    et al. ([2023a](#bib.bib196))). EDP improves the sampling efficiency of Diffusion-QL
    by modifying the sampling process $a_{0}\sim\pi_{\theta}(\cdot|s)$ in Eq. ([93](#S7.E93
    "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement Learning ‣
    7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")). Instead
    of iterative sampling with Eq. ([90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness
    in Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")), it first
    samples $a_{t}$ via the forward process and then approximates $a_{0}$ as $\frac{1}{\sqrt{\alpha_{t}}}a_{t}-\frac{\sqrt{1-\alpha_{t}}}{\sqrt{\alpha_{t}}}\epsilon_{\theta}(a_{t},t;s)$,
    since $a_{t}=\sqrt{\alpha_{t}}a_{0}+\sqrt{1-\alpha_{t}}\epsilon$ and $\epsilon_{\theta}$
    is trained to estimate $\epsilon$. Similarly, as a new variant of DM, Consistency
    Models (Song et al. ([2023](#bib.bib308))) aims to learn a consistency function
    $f_{\theta}$ that can map the noisy sample at any iteration $t$ to its original
    sample in just one iteration. Consistency-AC thus replaces DDPM in Diffusion-QL
    with a Consistency Model to improve the sampling efficiency.'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 继 Diffusion-QL 之后，提出了几个采用与公式 ([95](#S7.E95 "在 7.2.1 扩散模型作为策略 ‣ 7.2 离线强化学习 ‣
    7 扩散模型在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")) 类似目标设计的算法，包括 SRDP（Ada 等人 ([2023](#bib.bib2)))、EDP（Kang
    等人 ([2023](#bib.bib157))) 和 Consistency-AC（Ding & Jin ([2023](#bib.bib71)))。具体而言，SRDP
    在公式 ([95](#S7.E95 "在 7.2.1 扩散模型作为策略 ‣ 7.2 离线强化学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望"))
    中额外引入了状态重构损失项，通过自监督学习（Liu 等人 ([2023a](#bib.bib196))) 实现对 OOD 状态的泛化。EDP 通过修改采样过程
    $a_{0}\sim\pi_{\theta}(\cdot|s)$ 来提高 Diffusion-QL 的采样效率，如公式 ([93](#S7.E93 "在 7.2.1
    扩散模型作为策略 ‣ 7.2 离线强化学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望")) 所示。与公式
    ([90](#S7.E90 "在 7.1.1 通过扩散模型提高模仿学习中的策略表达能力 ‣ 7.1 模仿学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的展望"))
    的迭代采样不同，它首先通过前向过程采样 $a_{t}$，然后近似 $a_{0}$ 为 $\frac{1}{\sqrt{\alpha_{t}}}a_{t}-\frac{\sqrt{1-\alpha_{t}}}{\sqrt{\alpha_{t}}}\epsilon_{\theta}(a_{t},t;s)$，因为
    $a_{t}=\sqrt{\alpha_{t}}a_{0}+\sqrt{1-\alpha_{t}}\epsilon$ 和 $\epsilon_{\theta}$
    被训练来估计 $\epsilon$。类似地，作为 DM 的新变体，Consistency Models（Song 等人 ([2023](#bib.bib308)))
    旨在学习一个一致性函数 $f_{\theta}$，可以将任何迭代 $t$ 的噪声样本映射到其原始样本，只需一次迭代。因此，Consistency-AC 用一个一致性模型替代了
    Diffusion-QL 中的 DDPM，以提高采样效率。
- en: 'At last, there are two works: QGPO (Lu et al. ([2023](#bib.bib202))) and CPQL
    (Chen et al. ([2023e](#bib.bib51))) aiming to directly learn the score function
    of the optimal policy: (This is derived from the definition of $\pi^{*}$, where
    $\alpha=1/\lambda$.)'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有两项工作：QGPO（Lu 等人 ([2023](#bib.bib202))) 和 CPQL（Chen 等人 ([2023e](#bib.bib51)))
    旨在直接学习最优策略的评分函数：（这源自于 $\pi^{*}$ 的定义，其中 $\alpha=1/\lambda$。）
- en: '|  | $\nabla_{a_{t}}\log\pi^{*}(a_{t}&#124;s)\propto\nabla_{a_{t}}\log\mu(a_{t}&#124;s)+\nabla_{a_{t}}\alpha
    Q_{\phi}(s,a_{t})$ |  | (96) |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla_{a_{t}}\log\pi^{*}(a_{t}&#124;s)\propto\nabla_{a_{t}}\log\mu(a_{t}&#124;s)+\nabla_{a_{t}}\alpha
    Q_{\phi}(s,a_{t})$ |  | (96) |'
- en: 'As mentioned in Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), with the score function
    $\nabla_{a_{t}}\log\pi^{*}(a_{t}|s)$, samples $a_{t}\sim\pi^{*}(\cdot|s)$ can
    be generated via various efficient score-based sampling schemes ^(21)^(21)21Note
    that $a_{t}$ represents the sample at the $t$-th diffusion iteration rather than
    the action at time step $t$.. Here, the score function $\nabla_{a_{t}}\log\mu(a_{t}|s)$
    can be learned with Score-based DM from $D_{\mu}$, as introduced in Section [2.5](#S2.SS5
    "2.5 Diffusion Models ‣ 2 Background on Deep Generative Models ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), but $\nabla_{a_{t}}\alpha Q_{\phi}(s,a_{t})$ relies on the Q-functions
    over intermediate samples $a_{t}$, which is intractable. As potential solutions,
    QGPO propose a contrastive learning framework to estimate $\alpha Q_{\phi}(s,a_{t}),\
    \forall t$. While, CPQL employs Consistency Models as the policy, reframes the
    score function estimation in Eq. ([96](#S7.E96 "In 7.2.1 Diffusion Models as Policies
    ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) into an objective similar to Eq. ([95](#S7.E95 "In 7.2.1
    Diffusion Models as Policies ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")), and theoretically
    establishes their equivalence when the weights for the two terms in Eq. ([95](#S7.E95
    "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement Learning ‣
    7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) are
    properly assigned.'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[2.5节](#S2.SS5 "2.5 扩散模型 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的展望")中提到的，通过得分函数$\nabla_{a_{t}}\log\pi^{*}(a_{t}|s)$，可以通过各种高效的得分基础采样方案生成样本$a_{t}\sim\pi^{*}(\cdot|s)$^(21)^(21)21请注意，$a_{t}$表示第$t$次扩散迭代中的样本，而不是时间步$t$上的动作。这里，得分函数$\nabla_{a_{t}}\log\mu(a_{t}|s)$可以通过来自$D_{\mu}$的基于得分的DM进行学习，如第[2.5节](#S2.SS5
    "2.5 扩散模型 ‣ 2 深度生成模型背景 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的展望")中所介绍，但$\nabla_{a_{t}}\alpha
    Q_{\phi}(s,a_{t})$依赖于中间样本$a_{t}$上的Q函数，这是不可解的。作为潜在解决方案，QGPO提出了一种对比学习框架来估计$\alpha
    Q_{\phi}(s,a_{t}),\ \forall t$。而CPQL则将一致性模型作为策略，将公式中得分函数的估计重新定义为类似于公式([96](#S7.E96
    "在7.2.1 扩散模型作为策略 ‣ 7.2 离线强化学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的展望"))的目标，并在理论上确定当公式([95](#S7.E95
    "在7.2.1 扩散模型作为策略 ‣ 7.2 离线强化学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 离线策略学习的深度生成模型：教程、调查与未来方向的展望"))中的两个项的权重得到适当分配时，它们的等价性。
- en: 'In Table [11](#S7.T11 "Table 11 ‣ 7.2.3 Diffusion Models as Data Synthesizers
    ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), we provide a summary of these algorithms. All the algorithms
    in this category have been evaluated on the D4RL benchmark. Readers can refer
    to corresponding papers for the numeric evaluation results to compare their performance.
    In particular, we notice that three schemes of using diffusion models as policies
    for offline RL have been developed: SfBC & IDQL, QGPO & CPQL, and the other works
    that follow Diffusion-QL. Among them, QGPO & CPQL choose to directly learn the
    score function for the policy, which is a quite challenging but promising direction
    for future works.'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[11](#S7.T11 "表 11 ‣ 7.2.3 扩散模型作为数据合成器 ‣ 7.2 离线强化学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣
    离线策略学习的深度生成模型：教程、调查与未来方向的展望")中，我们总结了这些算法。本类别中的所有算法均已在D4RL基准上进行了评估。读者可以参考相关论文以获取数值评估结果，以比较它们的性能。特别地，我们注意到，已经开发了三种将扩散模型用于离线RL的方案：SfBC
    & IDQL、QGPO & CPQL，以及其他跟随Diffusion-QL的工作。其中，QGPO & CPQL选择直接学习用于策略的得分函数，这是一个相当具有挑战性但对未来工作有希望的方向。
- en: 7.2.2 Diffusion Models as Planners
  id: totrans-660
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2 扩散模型作为规划者
- en: 'For model-based offline RL, a dynamic function $\mathcal{T}_{\psi}(s^{k+1}|s^{k},a^{k})$
    needs to be approximated from the dataset first, and then planning over the optimal
    action sequence $(a^{1}_{*},\cdots,a^{K}_{*})$ can be done by maximizing the return
    while avoiding OOD actions: (For clarity, we use superscript $k$ to indicate the
    time step and subscript $t$ to denote the iteration of the diffusion process.)'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于模型的离线强化学习，需要先从数据集中逼近一个动态函数 $\mathcal{T}_{\psi}(s^{k+1}|s^{k},a^{k})$，然后通过最大化回报并避免
    OOD 动作来规划最优动作序列 $(a^{1}_{*},\cdots,a^{K}_{*})$：（为了清晰起见，我们使用上标 $k$ 表示时间步，使用下标 $t$
    表示扩散过程的迭代。）
- en: '|  | $\max_{a^{1:K}}\sum_{k=1}^{K}r(s^{k},a^{k})-\lambda d_{\mu}(s^{k},a^{k})\
    s.t.\ s^{k+1}=\mathcal{T}_{\psi^{*}}(s^{k},a^{k})$ |  | (97) |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{a^{1:K}}\sum_{k=1}^{K}r(s^{k},a^{k})-\lambda d_{\mu}(s^{k},a^{k})\
    s.t.\ s^{k+1}=\mathcal{T}_{\psi^{*}}(s^{k},a^{k})$ |  | (97) |'
- en: 'Here, $\psi^{*}=\arg\min_{\psi}\mathbb{E}_{D_{\mu}}\left[||s^{k+1}-\mathcal{T}_{\psi}(s^{k},a^{k})||^{2}\right]$,
    $d_{\mu}(s^{k},a^{k})$ represents the uncertainty assessing whether the data point
    $(s^{k},a^{k})$ conforms to the data distribution in $D_{\mu}$. SGP (Suh et al.
    ([2023](#bib.bib312))) proposes to implement $d_{\mu}(s,a)$ as the negative log-likelihood
    of $(s,a)$ under a perturbed distribution, i.e., $-\log P_{\sigma}((s,a);D_{\mu})$
    where $P_{\sigma}(x;D_{\mu})=\frac{1}{|D_{\mu}|}\sum_{x_{i}\in D_{\mu}}\mathcal{N}(x;x_{i},\sigma^{2}I)$.
    Intuitively, this likelihood measures the distance of $(s,a)$ to the dataset $D_{\mu}$
    and a large distance indicates a high uncertainty of the point $(s,a)$ as it may
    be OOD. Then, to optimize Eq. ([97](#S7.E97 "In 7.2.2 Diffusion Models as Planners
    ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) for trajectory planning, the score function $\nabla_{a}\log
    P_{\sigma}((s,a);D_{\mu})$ needs to be estimated. As detailed in (Suh et al. ([2023](#bib.bib312))),
    this estimation can be obtained using a denoising function in DM (specifically,
    SGM).'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，$\psi^{*}=\arg\min_{\psi}\mathbb{E}_{D_{\mu}}\left[||s^{k+1}-\mathcal{T}_{\psi}(s^{k},a^{k})||^{2}\right]$，$d_{\mu}(s^{k},a^{k})$
    代表评估数据点 $(s^{k},a^{k})$ 是否符合 $D_{\mu}$ 中的数据分布的不确定性。SGP (Suh et al. ([2023](#bib.bib312)))
    提出将 $d_{\mu}(s,a)$ 实现为在扰动分布下 $(s,a)$ 的负对数似然，即 $-\log P_{\sigma}((s,a);D_{\mu})$，其中
    $P_{\sigma}(x;D_{\mu})=\frac{1}{|D_{\mu}|}\sum_{x_{i}\in D_{\mu}}\mathcal{N}(x;x_{i},\sigma^{2}I)$。直观上，这种似然测量了
    $(s,a)$ 到数据集 $D_{\mu}$ 的距离，较大的距离表明点 $(s,a)$ 的不确定性较高，因为它可能是 OOD 的。然后，为了优化 Eq. ([97](#S7.E97
    "In 7.2.2 Diffusion Models as Planners ‣ 7.2 Offline Reinforcement Learning ‣
    7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) 以进行轨迹规划，需要估计评分函数
    $\nabla_{a}\log P_{\sigma}((s,a);D_{\mu})$。正如 (Suh et al. ([2023](#bib.bib312)))
    中详细描述的，这种估计可以使用 DM 中的去噪函数（特别是 SGM）获得。'
- en: 'A more widely-adopted manner for planning with DM, which is firstly proposed
    in Diffuser (Janner et al. ([2022](#bib.bib150))), is to fold the two processes
    mentioned above: transition dynamic modeling and trajectory optimization regarding
    $a^{1:K}$, as a trajectory modeling process using DM. Viewing trajectories $\tau=((s^{1},a^{1}),\cdots,(s^{K},a^{K}))$
    as data points (i.e., $x$ in Eq. ([28](#S2.E28 "In 2nd item ‣ 2.5 Diffusion Models
    ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))),
    DDPM is used to model the distribution of $\tau$ in $D_{\mu}$, through Eq. ([28](#S2.E28
    "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on Deep Generative Models ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) where a denoising function for trajectory generation $\epsilon_{\theta}(\tau_{t},t)$
    is learned. Then, the planning can be done by sampling trajectories starting from
    the current state using the learned DM. The sampling process is similar with Eq.
    ([90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness in Imitation Learning
    with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")). However, for optimality, this
    process is guided by a (separately-trained) return function of the trajectory
    samples, i.e., $J_{\phi}(\hat{\tau})$ ^(22)^(22)22$J_{\phi}(\hat{\tau})$ is trained
    to estimate the return of the original trajectory, i.e., $J(\tau)=\sum_{k=1}^{K}r(s^{k},a^{k})$,
    where $\hat{\tau}$ can be the trajectory sample at any diffusion iteration, i.e.,
    $\tau_{t}$.: ($\beta_{t}$, $\alpha_{t}$, and $\Sigma_{t}$ are hyperparameters
    and defined in Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background on Deep
    Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions").)'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 DM 进行规划的一种更广泛采用的方法，首先在 Diffuser (Janner et al. ([2022](#bib.bib150))) 中提出，是将上述两个过程：过渡动态建模和关于
    $a^{1:K}$ 的轨迹优化，折叠为一个使用 DM 的轨迹建模过程。将轨迹 $\tau=((s^{1},a^{1}),\cdots,(s^{K},a^{K}))$
    视为数据点（即，$x$ 在 Eq. ([28](#S2.E28 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")))，DDPM 被用来建模 $\tau$
    在 $D_{\mu}$ 中的分布，通过 Eq. ([28](#S2.E28 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2
    Background on Deep Generative Models ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"))，其中一个用于轨迹生成的去噪函数
    $\epsilon_{\theta}(\tau_{t},t)$ 被学习。然后，可以通过使用学习到的 DM 从当前状态开始采样轨迹来进行规划。采样过程类似于
    Eq. ([90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness in Imitation Learning
    with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"))。然而，为了实现最优性，该过程由轨迹样本的（单独训练的）回报函数
    $J_{\phi}(\hat{\tau})$ 指导 ^(22)^(22)22$J_{\phi}(\hat{\tau})$ 被训练以估计原始轨迹的回报，即 $J(\tau)=\sum_{k=1}^{K}r(s^{k},a^{k})$，其中
    $\hat{\tau}$ 可以是任何扩散迭代中的轨迹样本，即 $\tau_{t}$。: ($\beta_{t}$、$\alpha_{t}$ 和 $\Sigma_{t}$
    是超参数，在 Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions") 中定义。)'
- en: '|  | $\tau_{t-1}\sim G_{\theta}(\cdot&#124;\tau_{t},g)={\mathcal{N}}(\mu_{t}+\Sigma_{t}g,\Sigma_{t}),\
    \mu_{t}=\frac{1}{\sqrt{1-\beta_{t}}}(\tau_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(\tau_{t},t)),\
    g=\nabla_{\hat{\tau}}J_{\phi}(\hat{\tau})&#124;_{\hat{\tau}=\mu_{t}}$ |  | (98)
    |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tau_{t-1}\sim G_{\theta}(\cdot| \tau_{t},g)={\mathcal{N}}(\mu_{t}+\Sigma_{t}g,\Sigma_{t}),\
    \mu_{t}=\frac{1}{\sqrt{1-\beta_{t}}}(\tau_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(\tau_{t},t)),\
    g=\nabla_{\hat{\tau}}J_{\phi}(\hat{\tau})|_{\hat{\tau}=\mu_{t}}$ |  | (98) |'
- en: 'This process is also known as classifier-guided sampling (CG (Dhariwal & Nichol
    ([2021](#bib.bib70)))), which is widely adopted for conditional generations with
    diffusion models. Intuitively, the generation is guided by the gradient $\nabla_{\tau}J(\tau)$
    along which the expected return $J(\tau)$ would be maximized, aligning it with
    RL. In (Janner et al. ([2022](#bib.bib150))), the authors connect this guided
    sampling design with the control-as-inference framework of RL (Levine ([2018](#bib.bib180)))
    and claim that trajectories from such a generation process follows the distribution
    $P(\tau|\mathcal{O}_{1:K}=1)$, where $\mathcal{O}_{k}=1$ indicates the optimality
    of the time step $k$. During evaluation, the learned DM can applied as follows:
    ($\mathcal{T}$ is the real dynamic.)'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程也被称为分类器引导采样（CG (Dhariwal & Nichol ([2021](#bib.bib70))），它被广泛应用于扩散模型的条件生成中。直观地说，生成过程是由梯度
    $\nabla_{\tau}J(\tau)$ 引导的，这样期望回报 $J(\tau)$ 会被最大化，从而与强化学习（RL）对齐。在 (Janner et al.
    ([2022](#bib.bib150))) 中，作者将这种引导采样设计与 RL 的控制-推断框架 (Levine ([2018](#bib.bib180)))
    关联起来，并声称这种生成过程中的轨迹遵循分布 $P(\tau|\mathcal{O}_{1:K}=1)$，其中 $\mathcal{O}_{k}=1$ 表示时间步
    $k$ 的最优性。在评估期间，学习到的 DM 可以如下应用：（$\mathcal{T}$ 是实际动态。）
- en: '|  | $[\tau_{t}(s^{1})\leftarrow s^{k},\ \tau_{t-1}\sim G_{\theta}(\cdot&#124;\tau_{t},g)]_{t=T}^{1},\
    a^{k}\leftarrow\tau_{0}(a^{1}),\ s^{k+1}\sim\mathcal{T}(\cdot&#124;s^{k},a^{k}),\
    k=1,\cdots,K$ |  | (99) |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '|  | $[\tau_{t}(s^{1})\leftarrow s^{k},\ \tau_{t-1}\sim G_{\theta}(\cdot\vert\tau_{t},g)]_{t=T}^{1},\
    a^{k}\leftarrow\tau_{0}(a^{1}),\ s^{k+1}\sim\mathcal{T}(\cdot\vert s^{k},a^{k}),\
    k=1,\cdots,K$ |  | (99) |'
- en: 'To determine $a^{k}$, trajectory samples at each iteration are forced to start
    with the current state, i.e., $\tau_{t}(s^{1})\leftarrow s^{k}$ ($t=1,\cdots,T$).
    This follows the idea for solving inpainting problems (Sohl-Dickstein et al. ([2015](#bib.bib300))),
    where the generation for the unobserved part is in a manner consistent with the
    observed constraints. Moreover, only the first action in the generated plan, i.e.,
    $\tau_{0}(a^{1})$, is executed without replanning, which aligns with the receding
    horizon control (Mayne & Michalska ([1988](#bib.bib216))). Decision Diffuser (Ajay
    et al. ([2023](#bib.bib5))) adopts similar designs with two key modifications.
    First, instead of training a return function for classifier-guided sampling, they
    adopt classifier-free guided sampling (CFG (Ho & Salimans ([2022](#bib.bib134)))),
    for which a conditional and an unconditional denoising function, i.e., $\epsilon_{\theta}(\tau_{t},t;J(\tau))$
    and $\epsilon_{\theta}(\tau_{t},t;\emptyset)$, are jointly trained (with Eq. ([28](#S2.E28
    "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on Deep Generative Models ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))) by randomly dropping out the condition $J(\tau)$. The
    trajectory generation process is the same as Eq. ([90](#S7.E90 "In 7.1.1 Improving
    Policy Expressiveness in Imitation Learning with Diffusion Models ‣ 7.1 Imitation
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    but replaces the standard denoising function $\epsilon_{\theta}(\tau_{t},t)$ with
    $\omega\epsilon_{\theta}(\tau_{t},t;J(\tau))+(1-\omega)\epsilon_{\theta}(\tau_{t},t;\emptyset)$.
    Increasing the value of $\omega$ would decrease the diversity of samples but aligns
    the trajectory distribution more closely with $P(\tau|\mathcal{O}_{1:K}=1)$. Notably,
    both CG and CFG can be utilized for generating samples that satisfy specific conditions
    $y$. In the context of RL, $y$ can be the desired return $J(\tau)$, constraints
    to obey, task-specific information (for multi-task RL), subgoals/subtasks (for
    hierarchical RL), and so on. Second, only state sequences are modelled and predicted
    with the DM, i.e, $\tau=(s^{1},\cdots,s^{K})$ and actions are predicted with a
    separate inverse dynamic model, i.e., $a^{k}=\mathcal{T}_{\text{inv}}(s^{k},s^{k+1})$.
    This is because sequences over actions tend to be more high-frequency and less
    smooth, making them much harder to predict and model.'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '要确定$a^{k}$，在每次迭代中，轨迹样本被迫从当前状态开始，即$\tau_{t}(s^{1})\leftarrow s^{k}$（$t=1,\cdots,T$）。这遵循了解决修补问题的思路（Sohl-Dickstein等人（[2015](#bib.bib300)）），其中生成未观察部分的方式与观察到的约束一致。此外，生成计划中的仅第一行动，即$\tau_{0}(a^{1})$，在没有重新规划的情况下执行，这与递减视界控制（Mayne
    & Michalska（[1988](#bib.bib216)））一致。Decision Diffuser（Ajay等人（[2023](#bib.bib5)））采用了类似的设计，但有两个关键修改。首先，他们没有为分类器引导的采样训练返回函数，而是采用了无分类器引导的采样（CFG（Ho
    & Salimans（[2022](#bib.bib134)））），其中一个条件去噪函数和一个无条件去噪函数，即$\epsilon_{\theta}(\tau_{t},t;J(\tau))$和$\epsilon_{\theta}(\tau_{t},t;\emptyset)$，通过随机丢弃条件$J(\tau)$进行联合训练（见Eq.（[28](#S2.E28
    "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background on Deep Generative Models ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")））。轨迹生成过程与Eq.（[90](#S7.E90 "In 7.1.1 Improving Policy Expressiveness
    in Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")）相同，但用$\omega\epsilon_{\theta}(\tau_{t},t;J(\tau))+(1-\omega)\epsilon_{\theta}(\tau_{t},t;\emptyset)$替代了标准的去噪函数$\epsilon_{\theta}(\tau_{t},t)$。增加$\omega$的值将减少样本的多样性，但使轨迹分布更紧密地与$P(\tau|\mathcal{O}_{1:K}=1)$对齐。值得注意的是，CG和CFG都可以用于生成满足特定条件$y$的样本。在RL的背景下，$y$可以是期望的回报$J(\tau)$、需要遵守的约束、任务特定信息（对于多任务RL）、子目标/子任务（对于分层RL）等。其次，仅状态序列被用DM建模和预测，即$\tau=(s^{1},\cdots,s^{K})$，动作则用单独的逆动态模型预测，即$a^{k}=\mathcal{T}_{\text{inv}}(s^{k},s^{k+1})$。这是因为动作序列往往更加高频且不平滑，使得预测和建模更加困难。'
- en: 'Following Diffuser and Decision Diffuser, improvements have been made in multiple
    aspects such as the sampling process (SafeDiffuser, Discrete Diffuser), network
    structure (EDGI), training objective (PlanCP), and conditioners (TCD). We present
    a brief overview of these advancements in comparison to Diffuser and Decision
    Diffuser as follows. Notably, only TCP follows Decision Diffuser and the others
    follow Diffuser. SafeDiffuser (Xiao et al. ([2023](#bib.bib358))) aims to ensure
    the safe generation of data in diffusion. For each iteration $t$, the diffusion
    dynamic $u_{t}=\frac{\tau_{t-1}-\tau_{t}}{\Delta t}$ is re-optimized to satisfy
    certain safety constraints and the resulting dynamic $u^{*}_{t}$ is used to update
    $\tau_{t-1}$ as $\tau_{t}+\Delta t*u^{*}_{t}$. $\Delta t$ is the diffusion time
    interval which should be small enough. They theoretically show that $\tau_{0}$
    generated in this manner fulfills the safety constraints with probability almost
    1\. Discrete Diffuser (Coleman et al. ([2023](#bib.bib60))) proposes three diffusion-guidance
    sampling techniques for generation in discrete state and action spaces, where
    Gaussian-based DM cannot be applied. These guided sampling methods can work with
    discrete DM such as D3PM (Austin et al. ([2021a](#bib.bib15))). EDGI (Brehmer
    et al. ([2023](#bib.bib31))) proposes an equivariant network architecture for
    the scenario where an embodied agent operates $n$ objects in a 3D environment.
    This network design takes geometric structures into account and ensures equal
    likelihood of a trajectory and its counterpart for which specific spatial/temporal
    translations or permutations over objects are applied. Better generalization across
    scenarios, where such translations or permutations happen, can thus be realized.
    PlanCP (Sun et al. ([2023](#bib.bib314))) proposes to quantify the uncertainty
    of DM using Conformal Prediction (Vovk et al. ([2005](#bib.bib333))). In particular,
    $M$ trajectories are generated from DM corresponding to $M$ trajectories in $D_{\mu}$,
    each pair of which has a prediction error $e_{i}$. The conformity among $\{e_{i}\}$
    inversely reflects the uncertainty of the DM. Introducing this conformity term
    to the DDPM training objective (i.e., Eq. ([28](#S2.E28 "In 2nd item ‣ 2.5 Diffusion
    Models ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))) as
    an auxiliary term can potentially reduce the uncertainty of sampling with DM.
    Compared with Decision Diffuser, TCD (Hu et al. ([2023a](#bib.bib139))) utilizes
    more temporal information as the conditioner of the denoising function $\epsilon_{\theta}$
    to guide the sampling process. Specifically, following the insight of Decision
    Transformer (introduced in Section [6.1.1](#S6.SS1.SSS1 "6.1.1 Background on Trajectory-Optimization-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), the return-to-go and
    reward at the current time step are involved as the prospective and immediate
    conditions, respectively, to reason about the sampling progress while focusing
    more closely on the current generation step.'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 在Diffuser和Decision Diffuser之后，多个方面得到了改进，例如采样过程（SafeDiffuser、Discrete Diffuser）、网络结构（EDGI）、训练目标（PlanCP）和条件器（TCD）。我们简要概述了这些进展，相较于Diffuser和Decision
    Diffuser，具体如下。值得注意的是，只有TCP遵循Decision Diffuser，其余都遵循Diffuser。SafeDiffuser（Xiao等，[2023](#bib.bib358)）旨在确保在扩散中安全地生成数据。对于每次迭代$t$，扩散动态$u_{t}=\frac{\tau_{t-1}-\tau_{t}}{\Delta
    t}$被重新优化，以满足特定的安全约束，得到的动态$u^{*}_{t}$用于将$\tau_{t-1}$更新为$\tau_{t}+\Delta t*u^{*}_{t}$。$\Delta
    t$是扩散时间间隔，应该足够小。他们理论上展示了以这种方式生成的$\tau_{0}$几乎以1的概率满足安全约束。Discrete Diffuser（Coleman等，[2023](#bib.bib60)）提出了三种扩散指导采样技术，用于在离散状态和动作空间中生成，其中无法应用基于高斯的DM。这些指导采样方法可以与离散DM（如D3PM（Austin等，[2021a](#bib.bib15)））一起使用。EDGI（Brehmer等，[2023](#bib.bib31)）提出了一种等变网络架构，适用于一个具身代理在3D环境中操作$n$个物体的场景。该网络设计考虑了几何结构，并确保了轨迹及其对应轨迹的等可能性，后者应用了对物体的特定空间/时间转换或排列。因此，可以实现更好的场景泛化，其中发生了这些转换或排列。PlanCP（Sun等，[2023](#bib.bib314)）提出使用保形预测（Vovk等，[2005](#bib.bib333)）量化DM的不确定性。具体来说，$M$条轨迹从DM生成，对应于$D_{\mu}$中的$M$条轨迹，每对轨迹有一个预测误差$e_{i}$。$\{e_{i}\}$之间的一致性反映了DM的不确定性。将这一一致性项引入DDPM训练目标（即，第[28](#S2.E28
    "在第2项 ‣ 2.5 扩散模型 ‣ 2 关于深度生成模型的背景 ‣ 用于离线策略学习的深度生成模型：教程、调查和未来方向的展望")条公式）作为辅助项可以潜在地减少DM采样的不确定性。与Decision
    Diffuser相比，TCD（Hu等，[2023a](#bib.bib139)）利用更多的时间信息作为去噪函数$\epsilon_{\theta}$的条件器，以指导采样过程。具体来说，遵循Decision
    Transformer的见解（介绍于第[6.1.1](#S6.SS1.SSS1 "6.1.1 关于基于轨迹优化的离线强化学习的背景 ‣ 6.1 离线强化学习
    ‣ 6 Transformers在离线策略学习中的应用 ‣ 用于离线策略学习的深度生成模型：教程、调查和未来方向的展望")节），当前时间步的回报和奖励作为前瞻性和即时条件分别参与，以推理采样进度，同时更紧密地关注当前生成步骤。
- en: 7.2.3 Diffusion Models as Data Synthesizers
  id: totrans-670
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.3 扩散模型作为数据合成器
- en: The performance of offline RL is limited by the provided static dataset $D_{\mu}$.
    Once trained with $D_{\mu}$, the DM can potentially generate a variety of high-quality
    trajectory data. This newly generated data can then be employed to augment $D_{\mu}$,
    thereby creating a feedback loop to further improve the DM. Moreover, in an unseen
    but related task, the DM’s inherent generality allows for generation of well-performing
    trajectories for fine-tuning the model in this novel environment. Several works
    have been developed in this direction following either Diffuser (Hwang et al.
    ([2023](#bib.bib143)); Liang et al. ([2023b](#bib.bib190))) or Decision Diffuser
    (He et al. ([2023a](#bib.bib125))). Specifically, AdaptDiffuser (Liang et al.
    ([2023b](#bib.bib190))) suggests a rule-based method for filtering and enhancing
    generated trajectories $\tau=((s^{1},a^{1}),\cdots,(s^{K},a^{K}))$ from Diffuser,
    based on the return values and dynamic consistency. Starting with $k=1$, a revised
    action $\tilde{a}^{k}$ is determined using a well-trained or defined inverse dynamic
    model $\mathcal{T}_{\text{inv}}(\tilde{s}^{k},s^{k+1})$ (with $\tilde{s}^{1}=s^{1}$).
    Subsequently, the next state is adjusted according to the (learned) environment
    dynamic function as $\tilde{s}^{k+1}=\mathcal{T}(\tilde{s}^{k},\tilde{a}^{k})$.
    Trajectories with states $s^{k}$ significantly deviating from their counterparts
    $\tilde{s}^{k}$ are filtered out. The remaining trajectories $\tilde{\tau}$ are
    further filtered based on their returns $J_{\phi}(\tilde{\tau})=\sum_{k=1}^{K}r_{\phi}(s^{k},a^{k})$.
    Finally, the chosen trajectories are of high quality and can be used for effective
    data augmentation. In a more straightforward approach, MTDiff (He et al. ([2023a](#bib.bib125)))
    models trajectories $\tau=((s^{1},a^{1},r^{1}),\cdots,(s^{K},a^{K},r^{K}))$ conditioned
    on expert demonstrations from the same environment, denoted as $y$. Then, trajectory
    samples from the learned conditional denoising function $\epsilon_{\theta}(\tau_{t},t;y)$
    are directly used for data enhancement. For unseen tasks, a small amount of demonstrations
    can be used as prompts, i.e., $y$, for $\epsilon_{\theta}(\tau_{t},t;y)$ to generate
    high-quality training data. These data can then be employed to adapt the DM policy
    to novel tasks.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 离线RL的性能受限于提供的静态数据集$D_{\mu}$。一旦使用$D_{\mu}$进行训练，DM可以生成各种高质量的轨迹数据。然后，这些新生成的数据可以用来增强$D_{\mu}$，从而创建一个反馈循环，以进一步改进DM。此外，在未见过但相关的任务中，DM的固有泛化能力允许生成表现良好的轨迹，以便在这种新环境中微调模型。在这方面已经发展出若干工作，分别跟随Diffuser（Hwang
    et al. ([2023](#bib.bib143)); Liang et al. ([2023b](#bib.bib190))) 或Decision Diffuser（He
    et al. ([2023a](#bib.bib125)))。具体而言，AdaptDiffuser（Liang et al. ([2023b](#bib.bib190)))建议一种基于规则的方法，用于过滤和增强来自Diffuser的生成轨迹$\tau=((s^{1},a^{1}),\cdots,(s^{K},a^{K}))$，基于回报值和动态一致性。从$k=1$开始，使用经过良好训练或定义的逆动态模型$\mathcal{T}_{\text{inv}}(\tilde{s}^{k},s^{k+1})$（其中$\tilde{s}^{1}=s^{1}$）确定修正后的动作$\tilde{a}^{k}$。随后，根据（学习到的）环境动态函数调整下一个状态为$\tilde{s}^{k+1}=\mathcal{T}(\tilde{s}^{k},\tilde{a}^{k})$。将状态$s^{k}$与其对应的$\tilde{s}^{k}$有显著偏差的轨迹过滤掉。剩余的轨迹$\tilde{\tau}$进一步基于其回报$J_{\phi}(\tilde{\tau})=\sum_{k=1}^{K}r_{\phi}(s^{k},a^{k})$进行过滤。最终，所选择的轨迹具有高质量，可以用于有效的数据增强。在一种更直接的方法中，MTDiff（He
    et al. ([2023a](#bib.bib125)))对条件于来自相同环境的专家演示的轨迹$\tau=((s^{1},a^{1},r^{1}),\cdots,(s^{K},a^{K},r^{K}))$进行建模，记作$y$。然后，直接使用来自学习到的条件去噪函数$\epsilon_{\theta}(\tau_{t},t;y)$的轨迹样本进行数据增强。对于未见过的任务，可以使用少量演示作为提示，即$y$，用于$\epsilon_{\theta}(\tau_{t},t;y)$生成高质量的训练数据。这些数据可以用于调整DM策略以应对新任务。
- en: '| Algorithm | DM Type | DM Usage | Evaluation Task |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | DM 类型 | DM 使用 | 评估任务 |'
- en: '| SfBC | Score SDE | Policy | D4RL (L, M, K) |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '| SfBC | Score SDE | 策略 | D4RL (L, M, K) |'
- en: '| IDQL | DDPM | Policy | D4RL (L, M) |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '| IDQL | DDPM | 策略 | D4RL (L, M) |'
- en: '| Diffusion-QL | DDPM | Policy | D4RL (L, M, A, K) |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '| 扩散-QL | DDPM | 策略 | D4RL (L, M, A, K) |'
- en: '| SRDP | DDPM | Policy | D4RL (L, M) |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '| SRDP | DDPM | 策略 | D4RL (L, M) |'
- en: '| DiffCPS | DDPM | Policy | D4RL (L, M) |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| DiffCPS | DDPM | 策略 | D4RL (L, M) |'
- en: '| EDP | DDPM | Policy | D4RL (L, M, A, K) |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| EDP | DDPM | 策略 | D4RL (L, M, A, K) |'
- en: '| Consistency-AC | Consistency Model | Policy | D4RL (L, M, A, K) |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| 一致性-AC | 一致性模型 | 策略 | D4RL (L, M, A, K) |'
- en: '| QGPO | Score-SDE | Policy | D4RL (L, M) |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| QGPO | Score-SDE | 策略 | D4RL (L, M) |'
- en: '| CPQL | Consistency Model | Policy | D4RL (L, A), dm_control |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| CPQL | 一致性模型 | 策略 | D4RL (L, A), dm_control |'
- en: '| DOM2 (MARL) | Score-SDE | Policy | MPE, MAMuJoCO |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '| DOM2 (MARL) | Score-SDE | 策略 | MPE, MAMuJoCO |'
- en: '| LDCQ (HRL) | DDPM | Policy | D4RL (L, M, A, K, C) |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| LDCQ (HRL) | DDPM | 策略 | D4RL (L, M, A, K, C) |'
- en: '| SGP | SGM | Planner | CartPole, D4RL (L), Pixel-Based Single Integrator,
    Box-Pushing |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '| SGP | SGM | 规划器 | CartPole, D4RL (L), Pixel-Based Single Integrator, Box-Pushing
    |'
- en: '| Diffuser | DDPM | Planner | D4RL (L, M), Kuka Robot |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
  zh: '| 扩散器 | DDPM | 规划器 | D4RL (L, M), Kuka 机器人 |'
- en: '| Decision Diffuser | DDPM | Planner | D4RL (L, K), Kuka Robot, Unitree-go-running
    |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '| 决策扩散器 | DDPM | 规划器 | D4RL (L, K), Kuka 机器人, Unitree-go-running |'
- en: '| SafeDiffuser | DDPM | Planner (D) | D4RL (L, M) |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
  zh: '| SafeDiffuser | DDPM | 规划器 (D) | D4RL (L, M) |'
- en: '| Discrete Diffuser | D3PM | Planner (D) | D4RL (L) |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
  zh: '| 离散扩散器 | D3PM | 规划器 (D) | D4RL (L) |'
- en: '| EDGI | DDPM | Planner (D) | Kuka Robot |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '| EDGI | DDPM | 规划器 (D) | Kuka 机器人 |'
- en: '| PlanCP | DDPM | Planner (D) | D4RL (L, M) |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '| PlanCP | DDPM | 规划器 (D) | D4RL (L, M) |'
- en: '| TCD | DDPM | Planner (DD) | D4RL (L) |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| TCD | DDPM | 规划器 (DD) | D4RL (L) |'
- en: '| MetaDiffuser (MTRL) | DDPM | Planner (D) | Multi-task MuJoCo |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '| MetaDiffuser (MTRL) | DDPM | 规划器 (D) | 多任务 MuJoCo |'
- en: '| MADiff (MARL) | DDPM | Planner (DD) | MPE, SMAC, MATP |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '| MADiff (MARL) | DDPM | 规划器 (DD) | MPE, SMAC, MATP |'
- en: '| HDMI (HRL) | DDPM | Planner (DD) | D4RL (L, M), NeoRL |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '| HDMI (HRL) | DDPM | 规划器 (DD) | D4RL (L, M), NeoRL |'
- en: '| AdaptDiffuser | DDPM | Planner & Synthesizer (D) | D4RL (L, M), Kuka Robot
    |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
  zh: '| AdaptDiffuser | DDPM | 规划器 & 合成器 (D) | D4RL (L, M), Kuka 机器人 |'
- en: '| MTDiff (MTRL) | DDPM | Planner & Synthesizer (DD) | Meta-World-V2 |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '| MTDiff (MTRL) | DDPM | 规划器 & 合成器 (DD) | Meta-World-V2 |'
- en: 'Table 11: Summary of DM-based offline RL algorithms. Some algorithms in this
    table can be categorized as multi-agent RL (MARL), multi-task RL (MTRL), or hierarchical
    RL (HRL). When DM are used as planners, most algorithms can be viewed as extensions
    of Diffuser (D) or Decision Diffuser (DD). Regarding the benchmarks, nearly all
    algorithms in this category have been evaluated on D4RL (Fu et al. ([2020](#bib.bib99))),
    which provides offline datasets for various data-driven RL tasks, including Locomotion
    (L), AntMaze (M), Adroit (A), Kitchen (K), and CARLA Autonomous Driving (C). dm_control
    (Tassa et al. ([2018](#bib.bib317))), Kuka Robot (Janner et al. ([2022](#bib.bib150))),
    Unitree-go-running Margolis & Agrawal ([2022](#bib.bib213)), CartPole (Tedrake
    ([2023](#bib.bib318))), Box-Pushing (Manuelli et al. ([2020](#bib.bib211))) are
    a series of continuous (robotic) control tasks. Pixel-Based Single Integrator
    (Chou & Tedrake ([2023](#bib.bib57))) requires dynamic learning and control over
    the pixel space. NeoRL (Qin et al. ([2022](#bib.bib262))) is an industrial benchmark
    on financial decision making. Lastly, Multi-task MuJoCo (Mitchell et al. ([2021](#bib.bib224)))
    and Meta-World-V2 (Yu et al. ([2019c](#bib.bib379))) are widely-used benchmarks
    for multi-task/meta RL. MPE (2D control Lowe et al. ([2017](#bib.bib199))), MAMuJoCo
    (robotic locomotion Peng et al. ([2021](#bib.bib248))), SMAC (video gaming Samvelyan
    et al. ([2019](#bib.bib283))), and MATP (trajectory prediction Alcorn & Nguyen
    ([2021](#bib.bib7))) provide diverse evaluation tasks for multi-agent RL.'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：基于 DM 的离线 RL 算法汇总。此表中的某些算法可以被归类为多智能体 RL (MARL)、多任务 RL (MTRL) 或层次 RL (HRL)。当
    DM 被用作规划器时，大多数算法可以视为扩展自扩散器 (D) 或决策扩散器 (DD)。关于基准，几乎所有此类别的算法都在 D4RL (Fu et al. ([2020](#bib.bib99)))
    上进行了评估，该平台提供了用于各种数据驱动 RL 任务的离线数据集，包括 Locomotion (L)、AntMaze (M)、Adroit (A)、Kitchen
    (K) 和 CARLA 自动驾驶 (C)。dm_control (Tassa et al. ([2018](#bib.bib317)))、Kuka 机器人
    (Janner et al. ([2022](#bib.bib150)))、Unitree-go-running Margolis & Agrawal ([2022](#bib.bib213))、CartPole
    (Tedrake ([2023](#bib.bib318)))、Box-Pushing (Manuelli et al. ([2020](#bib.bib211)))
    是一系列连续 (机器人) 控制任务。Pixel-Based Single Integrator (Chou & Tedrake ([2023](#bib.bib57)))
    需要在像素空间进行动态学习和控制。NeoRL (Qin et al. ([2022](#bib.bib262))) 是一个关于金融决策的工业基准。最后，多任务
    MuJoCo (Mitchell et al. ([2021](#bib.bib224))) 和 Meta-World-V2 (Yu et al. ([2019c](#bib.bib379)))
    是广泛使用的多任务/元 RL 基准。MPE (2D 控制 Lowe et al. ([2017](#bib.bib199)))、MAMuJoCo (机器人运动
    Peng et al. ([2021](#bib.bib248)))、SMAC (视频游戏 Samvelyan et al. ([2019](#bib.bib283)))
    和 MATP (轨迹预测 Alcorn & Nguyen ([2021](#bib.bib7))) 提供了多样的多智能体 RL 评估任务。
- en: 7.2.4 Diffusion Models in Extended Offline Reinforcement Learning Setups
  id: totrans-698
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.4 扩展离线强化学习设置中的扩散模型
- en: 'In addition to standard offline RL, diffusion models have been applied in multi-task,
    multi-agent, and hierarchical offline RL setups. Still, these methods are built
    on the foundational algorithms mentioned above, such as Diffusion-QL, Diffuser,
    and Decision Diffuser. MTDiff (He et al. ([2023a](#bib.bib125))) and MetaDiffuser
    (Ni et al. ([2023](#bib.bib230))) aim to learn task-conditioned planners to solve
    a distribution of tasks. They utilize offline data categorized for each task,
    i.e., $\cup_{i}D_{\mu_{i}}$, where $\mu_{i}$ denotes the behavior policy corresponding
    to task $i$. Following Decision Diffuser, MTDiff incorporates an expert trajectory
    from task $i$, denoted as $y_{i}$, as an extra condition for the planner specific
    to that task, represented as $\epsilon_{\theta}(\tau_{t},t;J(\tau),y_{i})$. Through
    training, the agent is expected to implicitly capture the transition model and
    reward function stored in the prompt trajectory, and perform task-specific planning
    based on these internalized knowledge. When being evaluated in a new but related
    task, the denoising function conditioned on a corresponding demonstration $y_{i}$
    can be used for planning. In contrast, MetaDiffuser uses an explicit approach
    by learning an encoder, denoted as $E_{\eta}$, to encode trajectories into compact
    representations through self-supervision on the reward and dynamic functions (Zintgraf
    et al. ([2020](#bib.bib400))), resembling BoReL introduced in Section [3.1.5](#S3.SS1.SSS5
    "3.1.5 Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"):'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '除了标准的离线 RL，扩散模型还被应用于多任务、多智能体和层次化离线 RL 设置中。不过，这些方法都建立在前述基础算法上，如 Diffusion-QL、Diffuser
    和 Decision Diffuser。MTDiff (He et al. ([2023a](#bib.bib125))) 和 MetaDiffuser (Ni
    et al. ([2023](#bib.bib230))) 旨在学习任务条件的规划者以解决任务分布问题。它们利用每个任务分类的离线数据，即 $\cup_{i}D_{\mu_{i}}$，其中
    $\mu_{i}$ 表示与任务 $i$ 相关的行为策略。继 Decision Diffuser 之后，MTDiff 通过将任务 $i$ 的专家轨迹 $y_{i}$
    作为该任务的额外条件来增强规划者的任务特定性，表示为 $\epsilon_{\theta}(\tau_{t},t;J(\tau),y_{i})$。通过训练，代理被期望能够隐式捕捉到存储在提示轨迹中的转移模型和奖励函数，并基于这些内化的知识执行任务特定的规划。在评估新的但相关的任务时，可以使用以相应演示
    $y_{i}$ 为条件的去噪函数进行规划。相比之下，MetaDiffuser 通过学习一个编码器 $E_{\eta}$ 来显式地将轨迹编码成紧凑的表示，采用在奖励和动态函数上的自监督方法（Zintgraf
    et al. ([2020](#bib.bib400)))，类似于在第 [3.1.5](#S3.SS1.SSS5 "3.1.5 Offline Multi-task/Hierarchical
    RL based on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") 节中介绍的 BoReL。'
- en: '|  | $\max_{\eta,\psi,\phi}\mathbb{E}_{\tau\sim\cup_{i}D_{\mu_{i}},y=E_{\eta}(\tau),(s^{k},a^{k},r^{k},s^{k+1})\sim\tau}\left[\log\mathcal{T}_{\psi}(s^{k+1}&#124;s^{k},a^{k},y)+\log
    r_{\phi}(r^{k}&#124;s^{k},a^{k},y)\right]$ |  | (100) |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\eta,\psi,\phi}\mathbb{E}_{\tau\sim\cup_{i}D_{\mu_{i}},y=E_{\eta}(\tau),(s^{k},a^{k},r^{k},s^{k+1})\sim\tau}\left[\log\mathcal{T}_{\psi}(s^{k+1}&#124;s^{k},a^{k},y)+\log
    r_{\phi}(r^{k}&#124;s^{k},a^{k},y)\right]$ |  | (100) |'
- en: 'When planning for a specific task $i$, the representation $y=E_{\eta}(\tau_{i}),\
    \tau_{i}\sim D_{\mu_{i}}$ can be used as an extra condition of the planner. MetaDiffuser
    is based on Diffuser and it adopts the learned reward and dynamic model to define
    the sampling guidance $g$ in Eq. ([98](#S7.E98 "In 7.2.2 Diffusion Models as Planners
    ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) to enhance the dynamics consistency of the generated trajectories
    while encouraging a high return, detailed as Eq. (7) in (Ni et al. ([2023](#bib.bib230))).'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: '在为特定任务 $i$ 进行规划时，表示为 $y=E_{\eta}(\tau_{i}),\ \tau_{i}\sim D_{\mu_{i}}$ 可以作为规划者的额外条件。MetaDiffuser
    基于 Diffuser，并采用学习到的奖励和动态模型来定义采样指导 $g$，如 Eq. ([98](#S7.E98 "In 7.2.2 Diffusion
    Models as Planners ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))，以增强生成轨迹的动态一致性，同时鼓励高回报，具体如
    Eq. (7) 在 (Ni et al. ([2023](#bib.bib230)))。'
- en: 'DOM2 (Li et al. ([2023c](#bib.bib187))) and MADiff (Zhu et al. ([2023b](#bib.bib397)))
    target at fully cooperative, offline multi-agent RL. Built upon Diffusion-QL,
    DOM2 learns a Q-function and DM-based policy for each agent. This policy is tailored
    to map the individual observation of each agent to its own action, which is trained
    with Eq. ([95](#S7.E95 "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")).
    Following Decision Diffuser, MADiff is developed for joint trajectory planning
    of all agents. Each agent $i$ has a separate denoising function $\epsilon_{\theta}^{i}$
    for planning its corresponding trajectory segment. To encourage global information
    interchange for better coordination, the denoising function contains an attention
    layer to aggregate trajectory embeddings from other agents. Clearly, more extensions
    can be developed regarding DM-based MARL, such as MARL algorithms for fully competitive
    or mixed (partially cooperative/competitive) task scenarios, and integration of
    DM with SOTA CTDE MARL methods, etc.'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 'DOM2（Li 等 ([2023c](#bib.bib187)））和 MADiff（Zhu 等 ([2023b](#bib.bib397)））专注于完全合作的离线多智能体强化学习。在
    Diffusion-QL 的基础上，DOM2 为每个智能体学习一个 Q 函数和基于 DM 的策略。该策略旨在将每个智能体的个体观察映射到其自身的行动，并使用
    Eq. ([95](#S7.E95 "In 7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))
    进行训练。继 Decision Diffuser 之后，MADiff 被开发用于所有智能体的联合轨迹规划。每个智能体 $i$ 有一个单独的去噪函数 $\epsilon_{\theta}^{i}$
    来规划其相应的轨迹段。为了促进全球信息交换以实现更好的协调，去噪函数包含一个注意力层，以聚合来自其他智能体的轨迹嵌入。显然，还可以开发更多关于基于 DM 的
    MARL 的扩展，例如针对完全竞争或混合（部分合作/竞争）任务场景的 MARL 算法，以及 DM 与 SOTA CTDE MARL 方法的集成等。'
- en: 'LDCQ (Venkatraman et al. ([2023](#bib.bib329))) and HDMI (Li et al. ([2023b](#bib.bib185)))
    are proposed to learn a hierarchical policy/planner from an offline dataset, which
    can be especially beneficial for long-horizon decision-making. LDCQ learns a high-level
    policy $\pi_{\text{high}}(z|s^{1})$ to map an initial state to a skill $z$, and
    low-level policies for each skill $\pi_{\text{low}}(a|s,z)$. In particular, they
    first employ a $\beta$-VAE: (This objective is the same as the one for OPAL introduced
    in Section [3.1.5](#S3.SS1.SSS5 "3.1.5 Offline Multi-task/Hierarchical RL based
    on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions").)'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 'LDCQ（Venkatraman 等 ([2023](#bib.bib329)））和 HDMI（Li 等 ([2023b](#bib.bib185)））被提出用于从离线数据集中学习层次化策略/规划器，这对长期决策尤其有益。LDCQ
    学习一个高层策略 $\pi_{\text{high}}(z|s^{1})$，将初始状态映射到技能 $z$，以及每个技能的低层策略 $\pi_{\text{low}}(a|s,z)$。特别地，他们首先使用
    $\beta$-VAE：（这一目标与第 [3.1.5](#S3.SS1.SSS5 "3.1.5 Offline Multi-task/Hierarchical
    RL based on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") 节中介绍的 OPAL 的目标相同。）'
- en: '|  | $\max_{\phi,\theta}\mathbb{E}_{\tau\sim D_{\mu},z\sim P_{\phi}(\cdot&#124;\tau)}\left[\sum_{k=1}^{K}\log
    P_{\theta}(a^{k}&#124;s^{k},z)-\beta D_{KL}(P_{\phi}(z&#124;\tau)&#124;&#124;\text{prior}(z&#124;s^{1}))\right]$
    |  | (101) |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\phi,\theta}\mathbb{E}_{\tau\sim D_{\mu},z\sim P_{\phi}(\cdot\mid\tau)}\left[\sum_{k=1}^{K}\log
    P_{\theta}(a^{k}\mid s^{k},z)-\beta D_{KL}(P_{\phi}(z\mid\tau)\mid\mid\text{prior}(z\mid
    s^{1}))\right]$ |  | (101) |'
- en: 'Similar with OPAL, after training, $P_{\theta}$ is used as $\pi_{\text{low}}$
    and $P_{\phi}$ is adopted to create $(s,z)$ pairs for training $\pi_{\text{high}}$.
    $\pi_{\text{high}}$ is modeled as DM and trained in a similar manner with SfBC
    (introduced in Section [7.2.1](#S7.SS2.SSS1 "7.2.1 Diffusion Models as Policies
    ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")). Alternatively, HDMI extracts the skill list corresponding
    to a trajectory as a series of subgoals within it, following a heuristic method
    (Eysenbach et al. ([2019](#bib.bib87))). Then, based on Decision Diffuser, HDMI
    learns a high-level planner to generate subgoal list $\tau_{z}$ guided by the
    trajectory return, i.e., $\epsilon_{\text{high}}(\tau_{z,t},t;J(\tau))$, and a
    low-level planner to give out the state sequence corresponding to each certain
    subgoal $z$, i.e., $\epsilon_{\text{low}}(\tau_{s,t},t;z)$.'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: '与 OPAL 类似，训练后，$P_{\theta}$ 用作 $\pi_{\text{low}}$，$P_{\phi}$ 被用于生成用于训练 $\pi_{\text{high}}$
    的 $(s,z)$ 对。$\pi_{\text{high}}$ 被建模为 DM 并以类似 SfBC 的方式进行训练（如在第 [7.2.1](#S7.SS2.SSS1
    "7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions") 节中介绍的）。另外，HDMI
    提取与轨迹对应的技能列表，作为其中的一系列子目标，遵循启发式方法 (Eysenbach et al. ([2019](#bib.bib87)))。然后，基于
    Decision Diffuser，HDMI 学习一个高层次的规划器，以生成由轨迹回报指导的子目标列表 $\tau_{z}$，即 $\epsilon_{\text{high}}(\tau_{z,t},t;J(\tau))$，以及一个低层次的规划器，以给出对应于每个特定子目标
    $z$ 的状态序列，即 $\epsilon_{\text{low}}(\tau_{s,t},t;z)$。'
- en: 'It’s worthy noting that fast sampling is essential for the application of DM
    for offline RL or IL. Besides adopting Consistency Models, for DDPM-based methods,
    they usually limit the number of iterations for the reverse generation process
    to a relatively small value and adopt a carefully-designed variance schedule,
    i.e., $\beta_{0:T}$, as proposed in (Xiao et al. ([2022](#bib.bib359))). For SDE-based
    methods, they would solve a probability flow ODE for the reverse generation process
    (as introduced in Section [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background on
    Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) with a fast solver: DPM-solver
    (Lu et al. ([2022](#bib.bib201))). All algorithms mentioned in this section have
    been summarized in Table [11](#S7.T11 "Table 11 ‣ 7.2.3 Diffusion Models as Data
    Synthesizers ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") and categorized based on the DM
    type and usage. When applying DM as planners, most algorithms are built upon DDPM
    and follow the design of either Diffuser (D) or Decision Diffuser (DD). Also,
    applying DM for offline RL has been an emerging research field and multiple extensions,
    including the multi-task, multi-agent, and hierarchical setups, have been explored.'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: '值得注意的是，快速采样对于离线强化学习（RL）或离线模仿学习（IL）中 DM 的应用至关重要。除了采用一致性模型外，对于基于 DDPM 的方法，它们通常将反向生成过程的迭代次数限制在一个相对较小的值，并采用精心设计的方差调度，即
    $\beta_{0:T}$，如 (Xiao et al. ([2022](#bib.bib359))) 提出的那样。对于基于 SDE 的方法，它们会为反向生成过程求解一个概率流
    ODE（如在第 [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions") 节中介绍的）并使用一个快速求解器：DPM-solver (Lu et al.
    ([2022](#bib.bib201)))。本节提到的所有算法已在表 [11](#S7.T11 "Table 11 ‣ 7.2.3 Diffusion Models
    as Data Synthesizers ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") 中总结，并根据 DM 类型和用途进行了分类。在将
    DM 应用于规划器时，大多数算法基于 DDPM 并遵循 Diffuser (D) 或 Decision Diffuser (DD) 的设计。此外，将 DM
    应用于离线 RL 已成为一个新兴的研究领域，已经探索了多任务、多智能体和层次设置等多种扩展。'
- en: 8 Discussions and Open Problems
  id: totrans-707
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 讨论与开放问题
- en: 'In this section, we provide in-depth discussions on deep generative models
    (DGMs) in offline policy learning and our perspectives on future research directions
    of this area, based on the main text in Section [3](#S3 "3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") - [7](#S7 "7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"). The discussions
    are centered around how DGMs have been used in offline policy learning, which
    provide a comprehensive summary of this paper and insightful ideas for future
    works. Note that, for simplicity, we use abbreviations of the DGMs: VAE - Variational
    Auto-Encoder, GAN - Generative Adversarial Network, NF - Normalizing Flow, DM
    - Diffusion Model.'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将深入讨论深度生成模型（DGMs）在离线策略学习中的应用，并基于第 [3](#S3 "3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") - [7](#S7 "7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions") 节的主要内容，提供对未来研究方向的观点。讨论内容集中于
    DGMs 在离线策略学习中的应用，这些讨论全面总结了本文内容，并对未来的工作提出了有见地的想法。需要注意的是，为了简便起见，我们使用了 DGMs 的缩写：VAE
    - 变分自编码器，GAN - 生成对抗网络，NF - 归一化流，DM - 扩散模型。'
- en: 8.1 Discussions on Deep Generative Models and Offline Policy Learning
  id: totrans-709
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 关于深度生成模型和离线策略学习的讨论
- en: 'A common usage of DGMs in offline policy learning: All DGMs can be utilized
    as policy functions in offline policy learning. In the context of IL, they can
    be the student policy learned by imitating the expert, while, in offline RL, they
    can either be the approximated behavior policy or the RL policy. To be specific,
    we list their mathematical forms as follows, in the order they were introduced
    ^(23)^(23)23$P_{\theta}(z|s)$ and $P_{\theta}(a|s,z)$ denote the prior and decoder
    of the VAE, respectively. $P_{Z}(z)$ is the assumed prior distribution of the
    latent variable $z$. $G(z|s)$ denotes the generator in the GAN or NF, and it is
    composed of a series of invertible and differentiable functions, i.e, $G_{1}\circ\cdots\circ
    G_{N}$, in the NF. $\pi(\cdot|\tau_{t-k:t})$ represents a transformer-based policy,
    where $\tau_{t-k:t}$ can be $(s_{t-k},a_{t-k},\cdots,s_{t})$ in IL and $(s_{t-k},a_{t-k},R_{t-k},\cdots,s_{t},R_{t})$
    in offline RL. $G_{\theta}(a_{t-1}|a_{t},s)$ represents the denosing process of
    DM, and the subscript $t$ denotes the denoising iteration.:'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: DGMs 在离线策略学习中的一种常见用法：所有 DGMs 都可以作为离线策略学习中的策略函数使用。在逆向学习（IL）的背景下，它们可以作为通过模仿专家学习的学生策略，而在离线强化学习（RL）中，它们可以是近似的行为策略或
    RL 策略。具体而言，我们列出了它们的数学形式，如下所示，按引入顺序排列。$P_{\theta}(z|s)$ 和 $P_{\theta}(a|s,z)$ 分别表示
    VAE 的先验和解码器。$P_{Z}(z)$ 是潜变量 $z$ 的假设先验分布。$G(z|s)$ 表示 GAN 或 NF 中的生成器，它由一系列可逆和可微分的函数组成，即
    $G_{1}\circ\cdots\circ G_{N}$，在 NF 中。$\pi(\cdot|\tau_{t-k:t})$ 代表基于变换器的策略，其中 $\tau_{t-k:t}$
    可以是 IL 中的 $(s_{t-k},a_{t-k},\cdots,s_{t})$ 和离线 RL 中的 $(s_{t-k},a_{t-k},R_{t-k},\cdots,s_{t},R_{t})$。$G_{\theta}(a_{t-1}|a_{t},s)$
    代表 DM 的去噪过程，子脚标 $t$ 表示去噪迭代。
- en: '|  |  | $\displaystyle z\sim P_{\theta}(\cdot&#124;s),\ a\sim P_{\theta}(\cdot&#124;s,z);\
    z\sim P_{Z}(\cdot),\ a=G(z&#124;s);\ z\sim P_{Z}(\cdot),\ a=G(z&#124;s),\ G=G_{1}\circ\cdots\circ
    G_{N};$ |  | (102) |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle z\sim P_{\theta}(\cdot\mid s),\ a\sim P_{\theta}(\cdot\mid
    s,z);\ z\sim P_{Z}(\cdot),\ a=G(z\mid s);\ z\sim P_{Z}(\cdot),\ a=G(z\mid s),\
    G=G_{1}\circ\cdots\circ G_{N};$ |  | (102) |'
- en: '|  |  | $\displaystyle\qquad\qquad\quad a_{t}\sim\pi(\cdot&#124;\tau_{t-k:t});\
    a_{T}\sim\mathcal{N}(0,I),\ a_{t-1}\sim G_{\theta}(\cdot&#124;a_{t},s),\ (t=T,\cdots,1),\
    a=a_{0}.$ |  |'
  id: totrans-712
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\qquad\qquad\quad a_{t}\sim\pi(\cdot\mid\tau_{t-k:t});\
    a_{T}\sim\mathcal{N}(0,I),\ a_{t-1}\sim G_{\theta}(\cdot\mid a_{t},s),\ (t=T,\cdots,1),\
    a=a_{0}.$ |  |'
- en: 'All these DGM-based policies show superior expressiveness compared with the
    one using only feed-forward neural networks, and each of them has unique advantages/disadvantages:
    (1) VAEs might be less expressive than the others, but offers a lightweight model
    choice and relatively stable training process; (2) As mentioned in Section [4.1.4](#S4.SS1.SSS4
    "4.1.4 Integrating VAEs and GANs for Imitation Learning: A Spotlight ‣ 4.1 Imitation
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), GAN-based polices can generate sharp data samples but
    may suffer from the mode collapse issue, meaning that they may not be able to
    cover the multiple modes in the $(s,a)$ distribution of the dataset like the other
    DGMs; (3) With its special architecture (e.g., the attention mechanism), the transformer
    can process time series or data encompassing multiple entities, as mentioned in
    Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Adopting Transformers as the Policy Backbone
    for Imitation Learning ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"); (4) As for NF and DM, they excel in modelling/generating
    high-dimensional and complex behaviors, due to their multi-component or multi-iteration
    designs shown as Eq. ([102](#S8.E102 "In 8.1 Discussions on Deep Generative Models
    and Offline Policy Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")). As a spotlight, we note that both VAE- and transformer-based policies
    have been used to make decisions on multi-modal input (e.g., information from
    different sensors or in different formats like images and languages), but they
    adopt different strategies. In particular, VAEs try to embed input from different
    modalities into a unified latent space, and then a single latent policy (coupled
    with encoders for each modality) can be used to predict actions based on multiple
    types of input, as introduced in Section [3.2.3](#S3.SS2.SSS3 "3.2.3 Managing
    Multi-Modal Inputs in Imitation Learning via VAEs ‣ 3.2 Imitation Learning ‣ 3
    Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions").
    Transformers, as a universal foundation model, do not make assumptions on the
    structure of the input data, and so can be used in various task scenarios such
    as CV and NLP, as introduced in Section [2.4](#S2.SS4 "2.4 Transformers ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"). Thus, a transformer
    can directly take a data sequence containing different modalities as input, treat
    each modality as a separate token, and integrate them via the attention mechanism,
    as shown in Section [6.2.2](#S6.SS2.SSS2 "6.2.2 Adopting Transformers as the Policy
    Backbone for Imitation Learning ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions").'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些基于DGM（深度生成模型）的方法在表达能力上都优于仅使用前馈神经网络的方法，每种方法都有其独特的优点/缺点：（1）变分自编码器（VAEs）可能不如其他方法那样具有表达力，但它提供了轻量级的模型选择和相对稳定的训练过程；（2）如[4.1.4节](#S4.SS1.SSS4
    "4.1.4 将VAEs与GANs集成用于模仿学习：聚焦 ‣ 4 模仿学习 ‣ 4 离线策略学习中的生成对抗网络 ‣ 深度生成模型在离线策略学习中的教程、调查和未来方向的展望")所述，基于GAN（生成对抗网络）的方法可以生成清晰的数据样本，但可能会遭遇模式崩溃问题，这意味着它们可能无法像其他DGM那样覆盖数据集中$(s,a)$分布中的多种模式；（3）由于其特殊的架构（例如，注意力机制），变压器可以处理时间序列或涵盖多个实体的数据，如[6.2.2节](#S6.SS2.SSS2
    "6.2.2 采用变压器作为模仿学习的策略骨干 ‣ 6 模仿学习 ‣ 6 离线策略学习中的变压器 ‣ 深度生成模型在离线策略学习中的教程、调查和未来方向的展望")中所述；（4）至于NF（流量模型）和DM（深度模型），由于其多组件或多迭代设计，它们在建模/生成高维和复杂行为方面表现出色，如公式
    ([102](#S8.E102 "在8.1 关于深度生成模型和离线策略学习的讨论 ‣ 8 讨论与开放问题 ‣ 深度生成模型在离线策略学习中的教程、调查和未来方向的展望"))所示。值得一提的是，VAE和变压器基于的策略都已用于处理多模态输入（例如，来自不同传感器的信息或不同格式的图像和语言），但它们采用了不同的策略。特别是，VAEs尝试将来自不同模态的输入嵌入到统一的潜在空间中，然后可以使用单一的潜在策略（配合每种模态的编码器）基于多种输入类型预测动作，如[3.2.3节](#S3.SS2.SSS3
    "3.2.3 通过VAEs管理多模态输入的模仿学习 ‣ 3 模仿学习 ‣ 3 离线策略学习中的变分自编码器 ‣ 深度生成模型在离线策略学习中的教程、调查和未来方向的展望")中介绍。作为通用基础模型的变压器不会对输入数据的结构做出假设，因此可以在各种任务场景中使用，如计算机视觉（CV）和自然语言处理（NLP），如[2.4节](#S2.SS4
    "2.4 变压器 ‣ 2 深度生成模型背景 ‣ 深度生成模型在离线策略学习中的教程、调查和未来方向的展望")中介绍。因此，变压器可以直接接收包含不同模态的数据序列作为输入，将每种模态视为单独的标记，并通过注意力机制整合它们，如[6.2.2节](#S6.SS2.SSS2
    "6.2.2 采用变压器作为模仿学习的策略骨干 ‣ 6 模仿学习 ‣ 6 离线策略学习中的变压器 ‣ 深度生成模型在离线策略学习中的教程、调查和未来方向的展望")所示。
- en: 'The unique usages of each DGM for offline policy learning: We notice that each
    DGM has its own unique usage for offline policy learning. (1) VAEs can learn latent
    representations of the data samples. These representations, which are potentially
    disentangled, can be used as data transformation for learning efficiency (Section
    [3.1.4](#S3.SS1.SSS4 "3.1.4 Data Augmentation and Transformation with VAEs ‣ 3.1
    Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")), task or subtask representations for
    multi-task or hierarchical learning (Section [3.1.5](#S3.SS1.SSS5 "3.1.5 Offline
    Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement Learning
    ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    similarity measure of data samples for data augmentation (Section [3.2.2](#S3.SS2.SSS2
    "3.2.2 Improving Data Efficiency in Imitation Learning with VAEs ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), unified representations of multi-modal input (Section [3.2.3](#S3.SS2.SSS3
    "3.2.3 Managing Multi-Modal Inputs in Imitation Learning via VAEs ‣ 3.2 Imitation
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), or disentangled representations of the states for mitigating causal
    confusion (Section [3.2.5](#S3.SS2.SSS5 "3.2.5 Tackling Causal Confusion in Imitation
    Learning with VAEs ‣ 3.2 Imitation Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")), etc. (2) GANs utilize an adversarial
    learning framework to practically realize distribution matching. Specifically,
    GANs can be used to minimize the discrepancy between the learned policy and expert
    policy, i.e., $d(\rho_{\pi}(s,a)||\rho_{\pi_{E}}(s,a))$, for IL, or the discrepancy
    between the learned policy and behavior policy, i.e., $d(\rho_{\pi}(s,a)||\rho_{\mu}(s,a))$,
    to mitigate the OOD issue for offline RL, as introduced in Section [4.1.1](#S4.SS1.SSS1
    "4.1.1 Fundamental GAN-Based Imitation Learning Algorithms: GAIL and AIRL ‣ 4.1
    Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") and [4.2.2](#S4.SS2.SSS2 "4.2.2 Policy Approximation Using
    GANs ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), respectively. (3) NFs
    can provide exact density estimation due to its special architecture design (i.e.,
    invertiable and differentiable components), enabling them to be incorporated with
    various advanced IL frameworks, as detailed in Section [5.1.1](#S5.SS1.SSS1 "5.1.1
    Exact Density Estimation in Imitation Learning Using Normalizing Flows ‣ 5.1 Imitation
    Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions").
    We highlight that whenever there is a requirement for exact density estimation,
    NFs can be utilized. For example, SOPT, as introduced in Section [3.1.3](#S3.SS1.SSS3
    "3.1.3 Addressing the Issue of Out-of-Distribution Actions Using VAEs ‣ 3.1 Offline
    Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), proposes to approximate the behavior policy’s density
    function, i.e., $\mu(a|s)$, using a VAE to apply support-constraint offline RL.
    However, the NF is a better choice for this purpose since it can provide more
    exact estimations. (4) Due to its ability for sequence modelling, transformers
    enable trajectory-optimization-based offline RL/IL, as introduced in Section [6.1.1](#S6.SS1.SSS1
    "6.1.1 Background on Trajectory-Optimization-based Offline Reinforcement Learning
    ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") and [6.2.1](#S6.SS2.SSS1 "6.2.1 A Paradigm of Transformer-based
    Imitation Learning ‣ 6.2 Imitation Learning ‣ 6 Transformers in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"). Also, as shown in Section [6.2.3](#S6.SS2.SSS3
    "6.2.3 Developing Generalist Imitation Learning Agents with Transformers ‣ 6.2
    Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), owing to its high capacity and generalization ability, it can be
    used as the core component of a generalist agent to handle a range of tasks with
    a single set of parameters, and it allows for pretraining and fine-tuning as in
    CV and NLP. (5) Finally, DM can be used to purify noisy training data with its
    special denoising process, as illustrated in Section [7.1.2](#S7.SS1.SSS2 "7.1.2
    Addressing Common Issues of Imitation Learning with Diffusion Models ‣ 7.1 Imitation
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    and solve model-based offline RL as trajectory modelling, as shown in Section
    [7.2.2](#S7.SS2.SSS2 "7.2.2 Diffusion Models as Planners ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    due to its efficiency in generating high-dimensional data and its special conditional
    sampling mechanisms (i.e., classifier guided or classifier-free guided sampling).'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 每种深度生成模型（DGM）在离线策略学习中的独特用途：我们注意到，每种DGM在离线策略学习中都有其独特的用途。 (1) VAEs 可以学习数据样本的潜在表示。这些潜在表示可能是解缠结的，可以用作数据转换以提高学习效率（见
    [3.1.4](#S3.SS1.SSS4 "3.1.4 数据增强与变换使用 VAEs ‣ 3.1 离线强化学习 ‣ 3 变分自编码器在离线策略学习中的应用
    ‣ 深度生成模型在离线策略学习中的教程、调查与未来方向展望")），多任务或层次学习的任务或子任务表示（见 [3.1.5](#S3.SS1.SSS5 "3.1.5
    基于 VAEs 的离线多任务/层次 RL ‣ 3.1 离线强化学习 ‣ 3 变分自编码器在离线策略学习中的应用 ‣ 深度生成模型在离线策略学习中的教程、调查与未来方向展望")），数据样本的相似性度量用于数据增强（见
    [3.2.2](#S3.SS2.SSS2 "3.2.2 使用 VAEs 改善模仿学习的数据效率 ‣ 3.2 模仿学习 ‣ 3 变分自编码器在离线策略学习中的应用
    ‣ 深度生成模型在离线策略学习中的教程、调查与未来方向展望")），多模态输入的统一表示（见 [3.2.3](#S3.SS2.SSS3 "3.2.3 通过 VAEs
    管理多模态输入 ‣ 3.2 模仿学习 ‣ 3 变分自编码器在离线策略学习中的应用 ‣ 深度生成模型在离线策略学习中的教程、调查与未来方向展望")），或状态的解缠结表示以缓解因果混淆（见
    [3.2.5](#S3.SS2.SSS5 "3.2.5 使用 VAEs 解决模仿学习中的因果混淆 ‣ 3.2 模仿学习 ‣ 3 变分自编码器在离线策略学习中的应用
    ‣ 深度生成模型在离线策略学习中的教程、调查与未来方向展望")），等等。 (2) GANs 利用对抗学习框架实际实现分布匹配。具体而言，GANs 可以用于最小化学得策略与专家策略之间的差异，即
    $d(\rho_{\pi}(s,a)||\rho_{\pi_{E}}(s,a))$，用于模仿学习，或学得策略与行为策略之间的差异，即 $d(\rho_{\pi}(s,a)||\rho_{\mu}(s,a))$，以缓解离线强化学习中的OOD问题，分别见于
    [4.1.1](#S4.SS1.SSS1 "4.1.1 基于 GAN 的模仿学习算法：GAIL 和 AIRL ‣ 4.1 模仿学习 ‣ 4 生成对抗网络在离线策略学习中的应用
    ‣ 深度生成模型在离线策略学习中的教程、调查与未来方向展望") 和 [4.2.2](#S4.SS2.SSS2 "4.2.2 使用 GANs 的策略近似 ‣
    4.2 离线强化学习 ‣ 4 生成对抗网络在离线策略学习中的应用 ‣ 深度生成模型在离线策略学习中的教程、调查与未来方向展望")。 (3) NFs 由于其特殊的架构设计（即可逆和可微分的组件）可以提供精确的密度估计，使其能够与各种先进的模仿学习框架结合，详见
    [5.1.1](#S5.SS1.SSS1 "5.1.1 使用归一化流在模仿学习中的精确密度估计 ‣ 5.1 模仿学习 ‣ 5 归一化流在离线策略学习中的应用
    ‣ 深度生成模型在离线策略学习中的教程、调查与未来方向展望")。我们强调，只要需要精确的密度估计，就可以使用 NFs。例如，SOPT，见于 [3.1.3](#S3.SS1.SSS3
    "3.1.3 使用 VAEs 解决分布外动作的问题 ‣ 3.1 离线强化学习 ‣ 3 变分自编码器在离线策略学习中的应用 ‣ 深度生成模型在离线策略学习中的教程、调查与未来方向展望")，建议使用
    VAE 来近似行为策略的密度函数，即 $\mu(a|s)$，以应用支持约束离线 RL。然而，NF 是更好的选择，因为它可以提供更精确的估计。 (4) 由于其序列建模能力，transformers
    使基于轨迹优化的离线 RL/IL 成为可能，见于 [6.1.1](#S6.SS1.SSS1 "6.1.1 基于轨迹优化的离线强化学习背景 ‣ 6.1 离线强化学习
    ‣ 6 Transformers 在离线策略学习中的应用 ‣ 深度生成模型在离线策略学习中的教程、调查与未来方向展望") 和 [6.2.1](#S6.SS2.SSS1
    "6.2.1 基于 Transformer 的模仿学习范式 ‣ 6.2 模仿学习 ‣ 6 Transformers 在离线策略学习中的应用 ‣ 深度生成模型在离线策略学习中的教程、调查与未来方向展望")。此外，见于
    [6.2.3](#S6.SS2.SSS3 "6.2.3 使用 Transformers 开发通用模仿学习代理 ‣ 6.2 模仿学习 ‣ 6 Transformers
    在离线策略学习中的应用 ‣ 深度生成模型在离线策略学习中的教程、调查与未来方向展望")，由于其高容量和泛化能力，它可以作为通用代理的核心组件，用于处理一系列任务，具有单一的参数集，并且允许像计算机视觉和自然语言处理中的预训练和微调。
    (5) 最后，DM 可以利用其特殊的去噪过程来净化噪声训练数据，如 [7.1.2](#S7.SS1.SSS2 "7.1.2 使用扩散模型解决模仿学习中的常见问题
    ‣ 7.1 模仿学习 ‣ 7 扩散模型在离线策略学习中的应用 ‣ 深度生成模型在离线策略学习中的教程、调查与未来方向展望") 所示，并解决模型基础的离线 RL
    作为轨迹建模，如 [7.2.2](#S7.SS2.SSS2 "7.2.2 扩散模型作为规划者 ‣ 7.2 离线强化学习 ‣ 7 扩散模型在离线策略学习中的应用
    ‣ 深度生成模型在离线策略学习中的教程、调查与未来方向展望") 所示，因其在生成高维数据和特殊条件采样机制（即分类器引导或无分类器引导采样）方面的效率。
- en: 'Integrated use of different DGMs for offline policy learning: The unique usages/advantages
    of each DGM form the basis of an integrated use of different DGMs in offline policy
    learning, and there are already some works exploring in this direction. Here,
    we list some (but not all) examples introduced in this paper. (1) In Section [4.1.4](#S4.SS1.SSS4
    "4.1.4 Integrating VAEs and GANs for Imitation Learning: A Spotlight ‣ 4.1 Imitation
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), we present a spotlight on integrating VAEs and GANs for
    IL to synthesize their advantages, i.e., to mitigate the mode collapse issue of
    GANs and the blurry sample issue of VAEs. (2) In Section [5.2](#S5.SS2 "5.2 Reinforcement
    Learning with Offline Data ‣ 5 Normalizing Flows in Offline Policy Learning ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), APAC adopts a Flow-GAN to model the distribution of $(s,a)$
    in $D_{\mu}$, which can be viewed as an integration of NFs and GANs; SAFER adopts
    a VAE to extract the safety context from the state sequence, which is then used
    as a condition of an NF-based policy for safe generation. (3) In Section [6.1.3](#S6.SS1.SSS3
    "6.1.3 Mitigating Impacts from Environmental Stochasticity ‣ 6.1 Offline Reinforcement
    Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    SPLT shows a coherent use of VAEs and transformers, where the VAE is used to capture
    the multiple modes/possibilities within the data as latent variables, which are
    then used as conditioners of a Decision Transformer to mitigate the impact from
    environmental stochasticity. (4) MIA, as introduced in Section [6.2.3](#S6.SS2.SSS3
    "6.2.3 Developing Generalist Imitation Learning Agents with Transformers ‣ 6.2
    Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), adopts a GAN-based auxiliary loss term to improve the data usage
    in transformer-based IL. (5) In Section [7.1.1](#S7.SS1.SSS1 "7.1.1 Improving
    Policy Expressiveness in Imitation Learning with Diffusion Models ‣ 7.1 Imitation
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    all three DM-based IL works implement their denoising functions as transformers,
    rather than commonly-used U-nets, for sample quality and policy expressiveness.'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 不同DGM在离线策略学习中的综合使用：每个DGM的独特用法/优势构成了不同DGM在离线策略学习中综合使用的基础，已经有一些工作在这一方向上进行探索。在此，我们列出了一些（但不是所有）在本文中介绍的示例。
    (1) 在第[4.1.4](#S4.SS1.SSS4 "4.1.4 结合VAE和GAN进行模仿学习：聚焦点 ‣ 4.1 模仿学习 ‣ 4 生成对抗网络在离线策略学习中的应用
    ‣ 深度生成模型在离线策略学习中的教程、调查及未来方向展望")节中，我们展示了结合VAE和GAN进行IL的聚焦点，即合成它们的优势，即缓解GAN的模式崩溃问题和VAE的模糊样本问题。
    (2) 在第[5.2](#S5.SS2 "5.2 使用离线数据进行强化学习 ‣ 5 归一化流在离线策略学习中的应用 ‣ 深度生成模型在离线策略学习中的教程、调查及未来方向展望")节中，APAC采用了Flow-GAN来建模$D_{\mu}$中的$(s,a)$分布，这可以视为NFs和GANs的综合；SAFER采用了VAE来从状态序列中提取安全上下文，然后作为基于NF的策略的条件进行安全生成。
    (3) 在第[6.1.3](#S6.SS1.SSS3 "6.1.3 缓解环境随机性影响 ‣ 6.1 离线强化学习 ‣ 6 Transformer在离线策略学习中的应用
    ‣ 深度生成模型在离线策略学习中的教程、调查及未来方向展望")节中，SPLT展示了VAE和transformers的一致使用，其中VAE用于捕捉数据中的多重模式/可能性作为潜在变量，然后用作决策Transformer的条件，以缓解环境随机性的影响。
    (4) 在第[6.2.3](#S6.SS2.SSS3 "6.2.3 使用Transformers开发通用模仿学习代理 ‣ 6.2 模仿学习 ‣ 6 Transformer在离线策略学习中的应用
    ‣ 深度生成模型在离线策略学习中的教程、调查及未来方向展望")节中介绍的MIA，采用了基于GAN的辅助损失项来改善transformer-based IL中的数据使用。
    (5) 在第[7.1.1](#S7.SS1.SSS1 "7.1.1 使用扩散模型提高模仿学习中的策略表达力 ‣ 7.1 模仿学习 ‣ 7 扩散模型在离线策略学习中的应用
    ‣ 深度生成模型在离线策略学习中的教程、调查及未来方向展望")节中，所有三个基于DM的IL工作都将其去噪功能实现为transformers，而不是常用的U-nets，以提高样本质量和策略表达力。
- en: '| DGM | VAE | GAN | Normalizing Flow | Transformer | Diffusion Model |'
  id: totrans-716
  prefs: []
  type: TYPE_TB
  zh: '| DGM | VAE | GAN | 归一化流 | Transformer | 扩散模型 |'
- en: '| IL | BC (ELBO, VIB) | MaxEntIRL (AIL) | KL, LIL, AIL | BC (self-supervision)
    | BC |'
  id: totrans-717
  prefs: []
  type: TYPE_TB
  zh: '| IL | BC (ELBO, VIB) | MaxEntIRL (AIL) | KL, LIL, AIL | BC (自监督) | BC |'
- en: '| Offline RL | DP-based offline RL | Model-based offline RL | DP-based offline
    RL | Trajectory optimization | DP/Model-based offline RL, Trajectory modelling
    |'
  id: totrans-718
  prefs: []
  type: TYPE_TB
  zh: '| 离线RL | 基于DP的离线RL | 基于模型的离线RL | 基于DP的离线RL | 轨迹优化 | 基于DP/模型的离线RL，轨迹建模 |'
- en: 'Table 12: A summary of the base offline RL/IL algorithms for DGM-based offline
    policy learning. MaxEntIRL (Section [4.1.1](#S4.SS1.SSS1 "4.1.1 Fundamental GAN-Based
    Imitation Learning Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) is
    short for maximum causal entropy inverse RL. KL, LIL, and AIL (Section [5.1.1](#S5.SS1.SSS1
    "5.1.1 Exact Density Estimation in Imitation Learning Using Normalizing Flows
    ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) denote the KL-divergence-based, Likelihood-based, and
    Adversarial IL framework, respectively. DP-based offline RL (Section [3.1.1](#S3.SS1.SSS1
    "3.1.1 Background on Dynamic-Programming-based Offline Reinforcement Learning
    ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) refers to the Dynamic-Programming-based
    offline RL.'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 表12：基于DGM的离线策略学习的基础离线RL/IL算法汇总。MaxEntIRL（第[4.1.1](#S4.SS1.SSS1 "4.1.1 基于GAN的模仿学习算法：GAIL和AIRL
    ‣ 4.1 模仿学习 ‣ 4 离线策略学习中的生成对抗网络 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向的展望")节）是最大因果熵逆RL的简称。KL、LIL和AIL（第[5.1.1](#S5.SS1.SSS1
    "5.1.1 使用归一化流的模仿学习中的精确密度估计 ‣ 5.1 模仿学习 ‣ 5 离线策略学习中的归一化流 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向的展望")节）分别表示基于KL散度、基于似然的和对抗性IL框架。基于DP的离线RL（第[3.1.1](#S3.SS1.SSS1
    "3.1.1 基于动态规划的离线强化学习背景 ‣ 3.1 离线强化学习 ‣ 3 离线策略学习中的变分自编码器 ‣ 离线策略学习中的深度生成模型：教程、调查及未来方向的展望")节）指的是基于动态规划的离线RL。
- en: 'A summary of the base IL/offline RL algorithms for each DGM: We provide this
    summary as Table [12](#S8.T12 "Table 12 ‣ 8.1 Discussions on Deep Generative Models
    and Offline Policy Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"). (1) Regarding IL, most DGMs, including VAEs, GANs, and DMs, select
    BC as the base algorithm, of which the objective is simply to maximize a log likelihood
    as shown in Eq. ([88](#S7.E88 "In 7.1 Imitation Learning ‣ 7 Diffusion Models
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")). However, there are
    differences in the realization. Specifically, as introduced in Section [3.2.1](#S3.SS2.SSS1
    "3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation Learning ‣
    3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    VAE-based IL algorithms either adopt a VAE ELBO, which is a lower bound of the
    BC objective, or a VIB-based framework, which is closed in form with the ELBO.
    For transformer-based Il, besides the BC term, to make full use of demonstrations,
    auxiliary supervision objectives, such as prediction errors on the next state
    (i.e., the forward model loss) or the intermediate action between two consecutive
    states (i.e., the inverse model loss), are often employed. As for DM-based IL,
    the objective for training DM (i.e., Eq. ([27](#S2.E27 "In 2nd item ‣ 2.5 Diffusion
    Models ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))) provides
    a lower bound of the log-likelihood, which naturally connects DM with BC, as mentioned
    in the beginning of Section [7.1](#S7.SS1 "7.1 Imitation Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"). On the other
    hand, GANs and NFs are integrated with more advanced IL frameworks. In particular,
    as introduced in Section [4.1.1](#S4.SS1.SSS1 "4.1.1 Fundamental GAN-Based Imitation
    Learning Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"), the fundamental
    GAN-based Il algorithms are derived from MaxEntIRL and practically implemented
    as Adversarial IL (AIL) frameworks; NFs, as exact density estimators, have the
    flexibility to be adopted in various IL frameworks such as KL, LIL, and AIL, as
    illustrated in Section [5.1.1](#S5.SS1.SSS1 "5.1.1 Exact Density Estimation in
    Imitation Learning Using Normalizing Flows ‣ 5.1 Imitation Learning ‣ 5 Normalizing
    Flows in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"). (2) As for offline
    RL, we have introduced multiple categories within it throughout this paper, including
    dynamic-programming-based, model-based, and trajectory-optimization-based offline
    RL, as shown in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Background on Dynamic-Programming-based
    Offline Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), [4.2.1](#S4.SS2.SSS1
    "4.2.1 Background on Model-based Offline Reinforcement Learning ‣ 4.2 Offline
    Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), and [6.1.1](#S6.SS1.SSS1 "6.1.1 Background on Trajectory-Optimization-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), respectively. The corresponding
    categories for each DGM are listed in Table [12](#S8.T12 "Table 12 ‣ 8.1 Discussions
    on Deep Generative Models and Offline Policy Learning ‣ 8 Discussions and Open
    Problems ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions"). More specifically, we note that VAE-based
    offline RL mainly follows policy penalty, support constraint, and pessimistic
    value methods, which are subcategories of dynamic-programming-based offline RL,
    as introduced in Section [3.1.3](#S3.SS1.SSS3 "3.1.3 Addressing the Issue of Out-of-Distribution
    Actions Using VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"); while, DM-based offline
    RL mainly follows another subcategory of dynamic-programming-based offline RL
    – policy constraint methods.'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '每个 DGM 的基本 IL/离线 RL 算法总结见表格[12](#S8.T12 "Table 12 ‣ 8.1 Discussions on Deep
    Generative Models and Offline Policy Learning ‣ 8 Discussions and Open Problems
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")。 (1) 关于 IL，大多数 DGM，包括 VAEs、GANs 和 DMs，选择 BC 作为基础算法，其目标是简单地最大化对数似然，如公式
    ([88](#S7.E88 "In 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")) 所示。然而，实施上有所不同。具体而言，如[3.2.1](#S3.SS2.SSS1
    "3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation Learning ‣
    3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    节所介绍，基于 VAE 的 IL 算法要么采用 VAE ELBO，这是一种 BC 目标的下界，要么采用 VIB 基础框架，其形式上与 ELBO 一致。对于基于
    transformer 的 IL，除了 BC 项外，为了充分利用演示，通常采用辅助监督目标，如对下一状态的预测误差（即前向模型损失）或两个连续状态之间的中间动作（即逆向模型损失）。至于基于
    DM 的 IL，训练 DM 的目标（即公式 ([27](#S2.E27 "In 2nd item ‣ 2.5 Diffusion Models ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"))) 提供了对数似然的下界，这自然将 DM
    与 BC 联系起来，如[7.1](#S7.SS1 "7.1 Imitation Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") 节开始部分所述。另一方面，GANs 和 NFs 被集成到更先进的
    IL 框架中。特别地，如[4.1.1](#S4.SS1.SSS1 "4.1.1 Fundamental GAN-Based Imitation Learning
    Algorithms: GAIL and AIRL ‣ 4.1 Imitation Learning ‣ 4 Generative Adversarial
    Networks in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions") 节所介绍，基本的 GAN
    基础 IL 算法源于 MaxEntIRL，并实际实现为对抗性 IL (AIL) 框架；NFs 作为精确的密度估计器，具有在各种 IL 框架中采用的灵活性，如
    KL、LIL 和 AIL，具体见[5.1.1](#S5.SS1.SSS1 "5.1.1 Exact Density Estimation in Imitation
    Learning Using Normalizing Flows ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") 节。 (2) 关于离线 RL，我们在本文中介绍了多种类别，包括基于动态规划的、基于模型的和基于轨迹优化的离线
    RL，如[3.1.1](#S3.SS1.SSS1 "3.1.1 Background on Dynamic-Programming-based Offline
    Reinforcement Learning ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")、[4.2.1](#S4.SS2.SSS1
    "4.2.1 Background on Model-based Offline Reinforcement Learning ‣ 4.2 Offline
    Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 和[6.1.1](#S6.SS1.SSS1 "6.1.1 Background on Trajectory-Optimization-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")节。每个 DGM 的相应类别见表格[12](#S8.T12
    "Table 12 ‣ 8.1 Discussions on Deep Generative Models and Offline Policy Learning
    ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")。更具体地说，我们注意到基于
    VAE 的离线 RL 主要遵循策略惩罚、支持约束和悲观值方法，这些是基于动态规划的离线 RL 的子类别，如[3.1.3](#S3.SS1.SSS3 "3.1.3
    Addressing the Issue of Out-of-Distribution Actions Using VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") 节所介绍；而基于 DM 的离线 RL 主要遵循另一种基于动态规划的离线 RL 子类别——策略约束方法。'
- en: 'Seminal works of DGM-based offline policy learning: Regarding the applications
    in offline policy learning, developments for different DGMs are quite unbalanced,
    and even for the same type of DGM, the applications in IL may be significantly
    more than the ones in offline RL, or vice versa. One main factor is whether there
    are seminal works in that category, since many research works would follow the
    seminal ones for extensions. Here, we highlight the seminal works introduced in
    this paper: VAE - offline RL - BCQ (Fujimoto et al. ([2019](#bib.bib100))), GAN
    - IL - GAIL (Ho & Ermon ([2016](#bib.bib133))) and AIRL (Fu et al. ([2017](#bib.bib98))),
    Transformer - offline RL - Decision Transformer (Chen et al. ([2021b](#bib.bib46)))
    and Trajectory Transformer (Janner et al. ([2021](#bib.bib149))), Diffusion Model
    - offline RL - Diffuser (Janner et al. ([2022](#bib.bib150))) and Decision Diffuser
    (Ajay et al. ([2023](#bib.bib5))). There are still no seminal works for NF-based
    offline policy learning, which explains why there are relatively fewer works in
    Section [5](#S5 "5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"). Notably, some seminal works pioneer new paradigms for offline policy
    learning. For example, GAIL converts IL/IRL to a distribution matching problem
    ^(24)^(24)24Although GAIL- or AIRL-based algorithms are used for imitation learning
    from offline expert data, these algorithms also rely on simulators in their learning
    process, because there is an inner (online) RL process in their algorithm designs.
    Similarly, most algorithms within Section [5.1.1](#S5.SS1.SSS1 "5.1.1 Exact Density
    Estimation in Imitation Learning Using Normalizing Flows ‣ 5.1 Imitation Learning
    ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    require simulators, as they also realize imitation learning via distribution matching
    (like GAIL and AIRL), i.e., $\min_{\pi}d(\rho_{\pi}(s,a)||\rho_{\pi_{E}}(s,a))$,
    and RL is a natural choice for optimization regarding the occupancy measure of
    the policy, i.e., $\rho_{\pi}(s,a)$.; Decision Transformer realizes offline RL
    via trajectory optimization, which eliminates the necessity to fit value functions
    through dynamic programming or to compute policy gradients; Diffuser folds the
    two processes in model-based offline RL: transition dynamic modeling and trajectory
    optimization, into a trajectory modeling process. These paradigm shifts are closely
    related to corresponding DGMs and make full use of their unique advantages, which
    open up promising directions for offline policy learning. On the other hand, simply
    using DGMs as function estimators in traditional offline RL or IL methods can
    be less attractive, unless the applications of DGMs brings superior scalability
    or generalization ability.'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 'DGM 基础离线策略学习的开创性工作：关于离线策略学习的应用，不同 DGM 的发展相当不平衡，即使是相同类型的 DGM，在 IL 中的应用也可能明显多于离线
    RL 中的应用，反之亦然。一个主要因素是该类别中是否有开创性工作，因为许多研究工作会跟随开创性工作进行扩展。在这里，我们重点介绍本文中的开创性工作：VAE
    - 离线 RL - BCQ（Fujimoto 等人 ([2019](#bib.bib100)))，GAN - IL - GAIL（Ho & Ermon ([2016](#bib.bib133)))
    和 AIRL（Fu 等人 ([2017](#bib.bib98)))，Transformer - 离线 RL - Decision Transformer（Chen
    等人 ([2021b](#bib.bib46))) 和 Trajectory Transformer（Janner 等人 ([2021](#bib.bib149)))，Diffusion
    Model - 离线 RL - Diffuser（Janner 等人 ([2022](#bib.bib150))) 和 Decision Diffuser（Ajay
    等人 ([2023](#bib.bib5)))。目前尚无 NF 基础离线策略学习的开创性工作，这解释了为什么在第 [5](#S5 "5 Normalizing
    Flows in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") 节中相关工作的相对较少。值得注意的是，一些开创性工作为离线策略学习开辟了新的范式。例如，GAIL
    将 IL/IRL 转换为分布匹配问题 ^(24)^(24)24尽管 GAIL 或 AIRL 基础的算法用于从离线专家数据中进行模仿学习，这些算法在其学习过程中也依赖于模拟器，因为其算法设计中存在内部（在线）RL
    过程。同样，第 [5.1.1](#S5.SS1.SSS1 "5.1.1 Exact Density Estimation in Imitation Learning
    Using Normalizing Flows ‣ 5.1 Imitation Learning ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") 节中的大多数算法也需要模拟器，因为它们通过分布匹配（如 GAIL
    和 AIRL）实现模仿学习，即 $\min_{\pi}d(\rho_{\pi}(s,a)||\rho_{\pi_{E}}(s,a))$，而 RL 是关于策略的占用测度的优化的自然选择，即
    $\rho_{\pi}(s,a)$。Decision Transformer 通过轨迹优化实现离线 RL，从而消除了通过动态规划拟合价值函数或计算策略梯度的必要性；Diffuser
    将模型基础的离线 RL 中的两个过程：过渡动态建模和轨迹优化，折叠为轨迹建模过程。这些范式的转变与相应的 DGM 密切相关，充分利用了它们的独特优势，为离线策略学习开辟了有前景的方向。另一方面，除非
    DGM 的应用带来了卓越的可扩展性或泛化能力，否则仅仅将 DGM 作为传统离线 RL 或 IL 方法中的函数估计器可能不够吸引人。'
- en: '| DGM | VAE | GAN | Normalizing Flow | Transformer | Diffusion Model |'
  id: totrans-722
  prefs: []
  type: TYPE_TB
  zh: '| DGM | 变分自编码器 | 对抗生成网络 | 归一化流 | 转换器 | 扩散模型 |'
- en: '| IL issues | Insufficient demonstrations, Causal confusion, Multi-modal input
    | Insufficient demonstrations, Compounding error, Multi-modal demonstrations |
    Exact density estimation, Learning from observations, Policy expressiveness |
    Sequential/ multi-modal/ multi-entity input, Multi-modal actions, Insufficient
    demonstrations | High-dim/ multi-modal state-action, Spurious correlations, Insufficient/
    low-quality demonstrations, Compounding error |'
  id: totrans-723
  prefs: []
  type: TYPE_TB
  zh: '| IL 问题 | 演示不足, 因果混淆, 多模态输入 | 演示不足, 累积错误, 多模态演示 | 精确密度估计, 从观察中学习, 策略表达性 | 顺序/多模态/多实体输入,
    多模态动作, 演示不足 | 高维/多模态状态-动作, 假相关, 演示不足/低质量, 累积错误 |'
- en: '| IL extensions | Hier | MT, MA, Hier, MB, Safety | N/A | MT, Generalization
    | Hier |'
  id: totrans-724
  prefs: []
  type: TYPE_TB
  zh: '| IL 扩展 | 层次 | 多任务, 多智能体, 层次, 模型基础, 安全性 | 不适用 | 多任务, 泛化 | 层次 |'
- en: '| Offline RL issues | OOD actions, Low-quality training data, High-dim states,
    Continuous actions | OOD actions, (Vision-based) world model estimation, Segment
    stitching | OOD actions, Over conservatism, Insufficient training data, Sparse
    reward | OOD actions, Learning difficulty, Insufficient training data, Segment
    stitching, Sparse reward, Partially observable | OOD actions, Insufficient training
    data |'
  id: totrans-725
  prefs: []
  type: TYPE_TB
  zh: '| 离线强化学习问题 | OOD 动作, 低质量训练数据, 高维状态, 连续动作 | OOD 动作, (基于视觉) 世界模型估计, 段拼接 | OOD
    动作, 过度保守, 不足的训练数据, 稀疏奖励 | OOD 动作, 学习难度, 不足的训练数据, 段拼接, 稀疏奖励, 部分可观察 | OOD 动作, 不足的训练数据
    |'
- en: '| Offline RL extensions | MT, Hier | Hier, MB | Hier, Safety | MT, MA, Hier,
    MB, Safety, Generalization | MT, MA, Hier, MB, Safety |'
  id: totrans-726
  prefs: []
  type: TYPE_TB
  zh: '| 离线强化学习扩展 | 多任务, 层次 | 层次, 模型基础 | 层次, 安全性 | 多任务, 多智能体, 层次, 模型基础, 安全性, 泛化 |
    多任务, 多智能体, 层次, 模型基础, 安全性 |'
- en: 'Table 13: A summary of the issues and extensions of offline policy learning
    targeted by the DGMs. MT, MA, Hier, MB represents multi-task, multi-agent, hierarchical,
    model-based learning, respectively. Safety and generalization refer to if the
    learned policy can be safely applied to risky environments or be generalizable
    to unseen environments.'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：DGMs 针对离线策略学习的问题和扩展的总结。多任务、多智能体、层次、模型基础分别表示多任务学习、多智能体学习、层次学习、模型基础学习。安全性和泛化指的是所学习的策略是否可以安全地应用于风险环境中或是否能够泛化到未见过的环境中。
- en: 'A summary of the issues and extensions of offline policy learning that have
    been targeted by the DGMs: In Table [13](#S8.T13 "Table 13 ‣ 8.1 Discussions on
    Deep Generative Models and Offline Policy Learning ‣ 8 Discussions and Open Problems
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), we enumerate the IL/offline RL issues that each DGM has
    tried to solve, as well as the extended IL/offline RL setups that have been explored
    by each DGM. Specifically, there are in total six setup extensions, including
    multi-task (MT), multi-agent (MA), hierarchical (Hier), model-based (MB) learning,
    policy safety, and policy generalization. Readers can easily find the research
    works targeting at specific issues/extensions in corresponding sections. However,
    we note that the listed issues/extensions should not be considered as completely
    resolved by the DGMs, and future works can focus on the unsolved or under-explored
    issues/extensions as detailed in Section [8.2.4](#S8.SS2.SSS4 "8.2.4 Future Works
    on Algorithm Designs ‣ 8.2 Perspectives on Future Directions ‣ 8 Discussions and
    Open Problems ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"). Here, we provide some remarks
    on this table. (1) Low-quality training data is characterized by its limited coverage
    of the state-action space, a lack of diversity in behavior patterns, or the inclusion
    of suboptimal/noisy demonstrations. (2) Multi-modal demonstrations/actions refer
    to demonstrations/actions containing multiple distributional modes, while by multi-modal
    input, we mean input from multiple sensors or in various formats (e.g., images
    and texts). (3) Learning from observations refer to imitating from sequences of
    states only, which is notably more challenging than usual imitation learning.
    (4) Compared with BC, GAN-based IL can mitigate the issues of insufficient demonstrations
    and compounding errors. This is because GAN-based IL methods do more than just
    mimicking observed behaviors – they reason about the underlying reward function
    from demonstrations and utilizes it for further RL training, which complements
    the static demonstrations and enables the agent to act reasonably at unseen states.
    However, GAN-based IL methods rely on simulators for RL training and suffer from
    issues regarding sampling efficiency and training stability, as introduced in
    Section [4.1.2](#S4.SS1.SSS2 "4.1.2 Extensions of GAIL ‣ 4.1 Imitation Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"). (5) Decision Transformer has lower learning difficulty and can avoid
    OOD actions for offline RL, because it is based on trajectory optimization, which
    eliminates the necessity to fit value functions or compute policy gradients, and
    multiple state and action anchors throughout the trajectory prevent the learned
    policy from deviating too far from the behavior policy. However, there are still
    pending issues for Decision Transformer, such as the impacts from environmental
    stochasticity (Section [6.1.3](#S6.SS1.SSS3 "6.1.3 Mitigating Impacts from Environmental
    Stochasticity ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) and its theoretical shortcomings
    as listed in Section [6.1.5](#S6.SS1.SSS5 "6.1.5 Reflections on Transformer-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions").'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 离线策略学习中 DGMs 关注的问题和扩展的总结：在表格 [13](#S8.T13 "表格 13 ‣ 8.1 深度生成模型与离线策略学习讨论 ‣ 8 讨论和未解问题
    ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的视角") 中，我们列举了每个 DGM 尝试解决的 IL/离线 RL 问题，以及每个 DGM 探索的扩展
    IL/离线 RL 设置。具体来说，总共有六种设置扩展，包括多任务（MT）、多智能体（MA）、层次（Hier）、基于模型（MB）的学习、策略安全性和策略泛化。读者可以在相应的章节中轻松找到针对特定问题/扩展的研究工作。然而，我们指出，列出的这些问题/扩展不应被视为
    DGMs 完全解决的问题，未来的工作可以关注未解决或尚未充分探索的问题/扩展，详见第 [8.2.4](#S8.SS2.SSS4 "8.2.4 算法设计的未来工作
    ‣ 8.2 未来方向的视角 ‣ 8 讨论和未解问题 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的视角") 节。以下是对该表格的一些说明：（1）低质量训练数据的特点是其对状态-动作空间的覆盖有限、行为模式缺乏多样性或包含次优/噪声演示。（2）多模态演示/动作指的是包含多种分布模式的演示/动作，而多模态输入则是指来自多个传感器或以各种格式（例如图像和文本）输入的数据。（3）从观察中学习指的是仅从状态序列中进行模仿，这比通常的模仿学习更具挑战性。（4）与
    BC 相比，基于 GAN 的 IL 可以缓解演示不足和误差积累的问题。这是因为基于 GAN 的 IL 方法不仅仅是模仿观察到的行为——它们还会从演示中推理出潜在的奖励函数，并利用其进行进一步的
    RL 训练，这补充了静态演示，并使代理能够在未见状态下合理地行动。然而，基于 GAN 的 IL 方法依赖于模拟器进行 RL 训练，并在采样效率和训练稳定性方面存在问题，详见第
    [4.1.2](#S4.SS1.SSS2 "4.1.2 GAIL 的扩展 ‣ 4.1 模仿学习 ‣ 4 离线策略学习中的生成对抗网络 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的视角")
    节。（5）决策变换器具有较低的学习难度，并且可以避免离线 RL 中的 OOD 行为，因为它基于轨迹优化，这消除了拟合价值函数或计算策略梯度的必要性，并且轨迹中的多个状态和动作锚点防止了学习的策略偏离行为策略。然而，决策变换器仍然存在待解决的问题，例如来自环境随机性的影响（第
    [6.1.3](#S6.SS1.SSS3 "6.1.3 缓解环境随机性影响 ‣ 6.1 离线强化学习 ‣ 6 离线策略学习中的变换器 ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的视角")
    节）以及第 [6.1.5](#S6.SS1.SSS5 "6.1.5 对基于变换器的离线强化学习的反思 ‣ 6.1 离线强化学习 ‣ 6 离线策略学习中的变换器
    ‣ 离线策略学习的深度生成模型：教程、调查和未来方向的视角") 节中列出的理论缺陷。
- en: 'Foundation model & Algorithm & Data: Among all the applications of DGMs, transformer-based
    IL/offline RL have shown some of the most promising and exciting evaluation results:
    (1) As listed in Table [9](#S6.T9 "Table 9 ‣ 6.2.3 Developing Generalist Imitation
    Learning Agents with Transformers ‣ 6.2 Imitation Learning ‣ 6 Transformers in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), most algorithms have
    seen success in challenging robotic simulators, real-world tasks, or large scale
    competitions; (2) As shown in Section [6.2.3](#S6.SS2.SSS3 "6.2.3 Developing Generalist
    Imitation Learning Agents with Transformers ‣ 6.2 Imitation Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), the use of transformer
    enables the process of input encompassing (extra) long time sequences or multiple
    modalities/entities, and (robotic) agents that can follow language instructions
    or interact with humans can be developed solely by imitation and self-supervision;
    (3) As introduced in Section [6.1.4](#S6.SS1.SSS4 "6.1.4 Transformers in Extended
    Offline Reinforcement Learning Setups ‣ 6.1 Offline Reinforcement Learning ‣ 6
    Transformers in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"), Gato (Reed
    et al. ([2022](#bib.bib271))), as a single transformer with the same set of weights,
    can handle 604 distinct tasks with varying modalities (i.e., images, texts, robotic
    control, and video gaming); further, Reid et al. ([2022](#bib.bib272)) and Takagi
    ([2022](#bib.bib316)) explore pretraining the transformer on language tasks and
    fine-tuning it on RL tasks, and demonstrate promising cross-modality pretraining
    effects. All these advancements are based on simple supervised learning objectives
    and are not related to any theoretical breakthrough. However, they do rely on
    a high-capacity and computation-efficient foundation model – transformer, and
    a large amount of diverse training data from multiple modalities. This leads to
    an inspiring paradigm for developing generalist agents, that is, using a potent
    foundation model trained by (self-) supervised learning from extensive, high-quality
    demonstrations. On the other hand, it also raises the question whether the future
    development should be data-driven or algorithm-driven, but there is no doubt that
    the development of new foundation models or algorithms should put more emphasis
    on the scalability and computation efficiency, like the transformer.'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: '基础模型与算法与数据：在所有DGMs的应用中，基于变换器的IL/离线RL展现了最有前景和令人兴奋的评估结果：(1) 如表[9](#S6.T9 "Table
    9 ‣ 6.2.3 Developing Generalist Imitation Learning Agents with Transformers ‣
    6.2 Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")所列，大多数算法在具有挑战性的机器人模拟器、真实世界任务或大规模竞赛中取得了成功；(2) 如第[6.2.3](#S6.SS2.SSS3
    "6.2.3 Developing Generalist Imitation Learning Agents with Transformers ‣ 6.2
    Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")节所示，变换器的使用使得输入过程能够涵盖（额外的）长时间序列或多种模态/实体，并且可以仅通过模仿和自我监督来开发能够遵循语言指令或与人互动的（机器人）智能体；(3)
    如第[6.1.4](#S6.SS1.SSS4 "6.1.4 Transformers in Extended Offline Reinforcement Learning
    Setups ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")节所介绍，Gato（Reed et al. ([2022](#bib.bib271)))作为一个具有相同权重的单一变换器，能够处理604种具有不同模态的任务（即图像、文本、机器人控制和视频游戏）；此外，Reid
    et al. ([2022](#bib.bib272))和Takagi ([2022](#bib.bib316))探讨了对变换器进行语言任务的预训练，并在RL任务上进行微调，展示了有前景的跨模态预训练效果。所有这些进展都基于简单的监督学习目标，并且与任何理论突破无关。然而，它们确实依赖于一个高容量和计算高效的基础模型——变换器，以及来自多种模态的大量多样化训练数据。这导致了一种激动人心的通用智能体开发范式，即使用通过广泛、高质量演示的（自我）监督学习训练的强大基础模型。另一方面，它也提出了未来的发展应是数据驱动还是算法驱动的问题，但毫无疑问，新基础模型或算法的发展应更加重视可扩展性和计算效率，如变换器所示。'
- en: 8.2 Perspectives on Future Directions
  id: totrans-730
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 未来方向的展望
- en: 'Based on the discussions above and the main content in Section [3](#S3 "3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") - [7](#S7
    "7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), we
    present some perspectives on future research directions here, and we categorize
    the content in this section as four aspects: data, benchmarking, theories, and
    algorithms. We believe more open problems can be gleaned from the detailed introductions
    in the main content, and we have included comments on future works specific to
    certain DGMs in corresponding sections, including the last paragraphs of Section
    [3](#S3 "3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), [5](#S5 "5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions"), [6.2](#S6.SS2 "6.2 Imitation Learning ‣ 6 Transformers in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), [7.1](#S7.SS1 "7.1 Imitation
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    [7.2.1](#S7.SS2.SSS1 "7.2.1 Diffusion Models as Policies ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    and Section [6.1.5](#S6.SS1.SSS5 "6.1.5 Reflections on Transformer-based Offline
    Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions").'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: '基于上述讨论以及第[3](#S3 "3 Variational Auto-Encoders in Offline Policy Learning ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")节和第[7](#S7 "7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")节的主要内容，我们在这里提出了对未来研究方向的一些看法，并将本节内容分为数据、基准测试、理论和算法四个方面。我们相信从主要内容的详细介绍中可以提炼出更多的开放问题，并且我们在相关节中对特定的深度生成模型（DGM）的未来工作进行了评论，包括第[3](#S3
    "3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")节、第[5](#S5
    "5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")节、第[6.2](#S6.SS2
    "6.2 Imitation Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")节、第[7.1](#S7.SS1 "7.1 Imitation Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")节、第[7.2.1](#S7.SS2.SSS1 "7.2.1
    Diffusion Models as Policies ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")节，以及第[6.1.5](#S6.SS1.SSS5
    "6.1.5 Reflections on Transformer-based Offline Reinforcement Learning ‣ 6.1 Offline
    Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")节的最后几段。'
- en: 8.2.1 Future Works on Data-centric Research
  id: totrans-732
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1 数据中心研究的未来工作
- en: How would the offline policy learning agent evolve with the quality and quantity
    of the training data? The performance of offline policy learning is closely related
    to the quality and quantity of the provided offline data. Therefore, one potential
    research direction would be investigating how the dataset’s characteristics would
    impact the learned policy, as instructions for building datasets. (1) First, the
    static dataset may contain noise (i.e., disturbed or misleading information) or
    suboptimal behaviors ^(25)^(25)25This would be an issue for IL, since IL requires
    expert-level demonstartions. However, for offline RL, the dataset could contain
    suboptimal behaviors as long as their rewards are correctly labeled. If there
    are only suboptimal trajectories in the dataset for offline RL, the learning difficulty
    would be high, since the agent needs to learn to stitch segments from various
    trajectories to form an optimal strategy.. In this case, open questions include
    how to measure the noise or suboptimality of the provided data and how robust
    can the policy learning be to these data imperfections. The answers to these questions
    would vary with the used algorithms, DGMs, or evaluation tasks, but the imperfection
    measure and robustness evaluation protocol can be general and beneficial for algorithm
    development. These questions are important because perfect datasets are costly
    to build in scale and the tolerance for imperfection can greatly reduce the burden
    for data collection and possessing. (2) Second, the performance of an offline
    policy learning agent on evaluation tasks relies heavily on the coverage and diversity
    of the training data. An effective set of behaviors for policy learning should
    cover the possible task scenarios and provide various behavioral patterns/skills
    for the agent to learn. How to measure the coverage and diversity of the dataset
    and how these properties would influence the generalization of the learned policy
    on evaluation tasks would be interesting questions. (3) One key factor contributing
    to the success of certain DGMs in CV and NLP is their ability to leverage extensive
    training datasets. Notably, their training performance would continue to improve
    as the quantity of the training data increases. Thus, for each variant of DGM-based
    offline policy learning, it is essential to study their scalability, that is,
    how the learned agent would evolve with the quantity of the training data. Algorithms
    with superior scalability would be more promising in challenging, real-life applications.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 离线策略学习代理如何随着训练数据的质量和数量而演变？离线策略学习的性能与提供的离线数据的质量和数量密切相关。因此，一个潜在的研究方向是调查数据集的特征如何影响学习到的策略，作为构建数据集的指示。
    (1) 首先，静态数据集可能包含噪声（即，干扰或误导信息）或次优行为^(25)^(25)25这将是IL（逆向学习）的一个问题，因为IL需要专家级的示范。然而，对于离线RL（强化学习），只要奖励被正确标记，数据集可以包含次优行为。如果离线RL的数据集中只有次优轨迹，学习难度将很高，因为代理需要学习将不同轨迹的片段拼接起来形成最佳策略。在这种情况下，开放性问题包括如何测量提供数据的噪声或次优性，以及策略学习对这些数据缺陷的鲁棒性如何。对这些问题的答案会因使用的算法、DGM（深生成模型）或评估任务的不同而有所变化，但不完美性测量和鲁棒性评估协议可以是通用的，并对算法开发有益。这些问题很重要，因为完美的数据集在规模上构建成本高，而对不完美性的容忍可以大大减少数据收集和处理的负担。
    (2) 其次，离线策略学习代理在评估任务上的表现很大程度上依赖于训练数据的覆盖面和多样性。用于策略学习的有效行为集应涵盖可能的任务场景，并提供各种行为模式/技能供代理学习。如何测量数据集的覆盖面和多样性，以及这些特性如何影响学习到的策略在评估任务上的泛化，将是有趣的问题。
    (3) 促成某些DGM在计算机视觉（CV）和自然语言处理（NLP）中成功的一个关键因素是它们能够利用大量的训练数据。值得注意的是，随着训练数据数量的增加，它们的训练性能将继续提高。因此，对于每个DGM基础的离线策略学习变体，研究其可扩展性，即学习到的代理如何随着训练数据数量的增加而演变是至关重要的。具有优越可扩展性的算法在具有挑战性的实际应用中会更具前景。
- en: How to construct a training dataset for effective offline policy learning? With
    efforts for the open problems mentioned above, we could establish measurements
    for noise, suboptimality, diversity, and coverage of the dataset and get some
    insights on the relationship between these measurements and the performance of
    the learned policy, which can guide the construction of training datasets for
    effective offline policy learning. However, there are some other problems that
    need to be explored. First, the original state and action spaces may be high-dimensional
    and continuous, which would require exponentially more data for policy learning.
    Utilizing compact representations, which can be obtained from DGMs like VAEs and
    encoder-only transformers, or discretization could help improve the learning efficiency,
    but also would introduce inaccuracy. Second, for offline RL datasets, sparse rewards
    can bring training difficulty but dense rewards are costly to annotate. Thus,
    a tradeoff needs to be achieved between them and strategically annotating part
    of the dataset with rewards could be a promising direction. Similarly, although
    there are algorithms for learning multi-task, hierarchical, or safe policies from
    data without the task, skill, or safe action labels, having these labels either
    fully or partially annotated can significantly aid in the policy learning process.
    However, given that real-life datasets can be enormous in scale, the labeling
    should be only for critical points or regions in the state-action space, which
    requires techniques for identification.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 如何构建有效的离线策略学习训练数据集？通过解决上述开放性问题，我们可以建立数据集的噪声、次优性、多样性和覆盖度的测量标准，并深入了解这些测量标准与所学策略的性能之间的关系，这可以指导有效的离线策略学习训练数据集的构建。然而，还有一些其他问题需要探索。首先，原始的状态和动作空间可能是高维的且连续的，这会需要指数级更多的数据来进行策略学习。利用紧凑的表示方法，如从变分自编码器（VAEs）和仅编码器的变换器（transformers）中获得的表示，或离散化，可以提高学习效率，但也会引入不准确性。其次，对于离线强化学习数据集，稀疏的奖励会带来训练难度，但密集的奖励则注释成本高。因此，需要在二者之间找到权衡，并且战略性地对数据集的部分进行奖励注释可能是一个有前途的方向。同样，尽管存在从没有任务、技能或安全动作标签的数据中学习多任务、层次化或安全策略的算法，但完全或部分标注这些标签可以显著帮助策略学习过程。然而，考虑到现实生活中的数据集可能非常庞大，标注应仅限于状态-动作空间中的关键点或区域，这需要识别技术。
- en: 'Data Augmentation & Data Synthesis. VAEs, transformers, and DMs have been used
    for data augmentation or synthesis, as shown in Section [3.1.4](#S3.SS1.SSS4 "3.1.4
    Data Augmentation and Transformation with VAEs ‣ 3.1 Offline Reinforcement Learning
    ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    [6.1.2](#S6.SS1.SSS2 "6.1.2 Balancing Model Capacity with Training Data ‣ 6.1
    Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), and [7.2.3](#S7.SS2.SSS3 "7.2.3 Diffusion Models as Data
    Synthesizers ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), respectively, of which the purpose
    is simply to involve more data for training. The data used for augmentation may
    already exist but require transformation or supplementation to be effectively
    utilized, or could be synthesized using DGMs. DGMs exhibit remarkable capability
    in synthesizing new data that shares statistical properties with the real data.
    Thus, this approach has the potential to bridge the gap between the demand for
    large datasets and the feasibility of acquiring them. Multiple research directions
    in this domain could be developed. First, based on the coverage or diversity indicators
    of the dataset, targeted data synthesis to supplement the under-covered area or
    to avoid repeated behavioral patterns for better diversity could be developed.
    Second, the synthesized data usually cannot be directly employed without filtering.
    In this case, necessary quality measure, such as the similarity of the synthesized
    data with the real (optimal) data, should be established. Also, ideally, there
    should be (theoretical) study on the relationship of the suboptimality of the
    learned policy with the quality/quantity of the synthesized data. All in all,
    it would be quite exciting to see the creation of a positive feedback loop: enhanced
    policies emerging from newly synthesized data, which in turn can be leveraged
    to generate even higher quality data for training.'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: '数据增强与数据合成。变分自编码器（VAEs）、变换器（transformers）和扩散模型（DMs）已被用于数据增强或合成，如在第 [3.1.4](#S3.SS1.SSS4
    "3.1.4 Data Augmentation and Transformation with VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")、[6.1.2](#S6.SS1.SSS2 "6.1.2 Balancing Model Capacity with Training
    Data ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 和 [7.2.3](#S7.SS2.SSS3 "7.2.3 Diffusion Models as Data
    Synthesizers ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") 中分别提到，其目的仅仅是为了引入更多的数据用于训练。用于增强的数据可能已经存在，但需要变换或补充才能有效利用，或者可以通过深度生成模型（DGMs）进行合成。DGMs
    在合成具有与真实数据相似的统计特性的全新数据方面表现出显著能力。因此，这种方法有可能弥补对大数据集的需求与获取这些数据集的可行性之间的差距。在这个领域可以发展多个研究方向。首先，可以基于数据集的覆盖度或多样性指标，开发针对性的数据合成，以补充覆盖不足的领域或避免重复的行为模式以提高多样性。其次，合成的数据通常不能直接使用，需要进行筛选。在这种情况下，应建立必要的质量度量，例如合成数据与真实（最优）数据的相似度。此外，理想情况下，还应对所学策略的次优性与合成数据的质量/数量之间的关系进行（理论）研究。总的来说，看到从新合成的数据中产生的增强策略，并进一步用于生成更高质量的训练数据，将是相当令人兴奋的。'
- en: 8.2.2 Future Works on Benchmarking
  id: totrans-736
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2 未来的基准测试工作
- en: 'Development of more realistic, challenging benchmarks. We notice that most
    DGM-based offline RL algorithms are evaluated on D4RL, as shown in the tables
    of Section [3](#S3 "3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") - [7](#S7 "7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), which is a standard benchmark for offline RL. However,
    widely-used benchmarks like D4RL, cannot fully show or evaluate the advantages
    of DGM-based offline policy learning algorithms, as we have seen agents, which
    are implemented as two-layer neural networks and trained with a moderate amount
    of data, can already achieve excellent performance on these benchmarks. To fully
    explore the potential and guide the development of DGM-based offline policy learning,
    more challenging benchmarks are required. (1) First, the benchmark dataset should
    be large in size to evaluate the scalability of the DGM-based algorithms. This
    includes evaluating the trend of performance improvement relative to the volume
    of training data and determining whether DGM-based algorithms can significantly
    outperform the others when provided with extensive training data. (2) Second,
    the dataset should contain multiple modalities. For example, state inputs could
    be collected from various sources, such as language instructions, visual information
    from different sensors, or even videos; moreover, the state-action space should
    feature multiple distributional modes, such as containing distinct trajectories
    for achieving the same goal. DGM-based algorithms are expected to effectively
    harness information from diverse modalities and learn policies that cover the
    multiple modes within the policy space. Further, benchmarks should not be limited
    to control or decision-making tasks but also include assignments from other modalities,
    such as CV and NLP. These cross-modal tasks could serve as auxiliary tasks or
    be utilized to evaluate the capabilities of generalist agents. (3) The task scenarios
    should be more realistic. A main reason that CV and NLP techniques, which are
    usually DGM-based, can be widely adopted in real life is that these algorithms
    are directly evaluated on real-life tasks/datasets rather than simplified simulators.
    Thus, realistic offline dataset in a large scale could greatly advance the development
    of offline policy learning and showcase the superiority of DGM-based algorithms.
    (4) The benchmark should evaluate more than just the policy return. It should
    also assess aspects such as the generalization, safety of the learned policy and
    the robustness of policy learning in noisy or sparse-reward environments.'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: '开发更现实、更具挑战性的基准测试。我们注意到，大多数基于 DGM 的离线 RL 算法在 D4RL 上进行评估，如第 [3](#S3 "3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") - [7](#S7
    "7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") 节中的表格所示，D4RL
    是离线 RL 的标准基准测试。然而，像 D4RL 这样的广泛使用的基准测试不能完全展示或评估基于 DGM 的离线策略学习算法的优势，因为我们看到，作为两层神经网络实现并用适量数据训练的代理，已经能够在这些基准上取得优异的表现。为了充分探索潜力并指导基于
    DGM 的离线策略学习的发展，需要更多具有挑战性的基准测试。(1) 首先，基准数据集应该具有较大的规模，以评估基于 DGM 的算法的可扩展性。这包括评估相对于训练数据量的性能改进趋势，并确定在提供大量训练数据时，基于
    DGM 的算法是否能显著超越其他算法。(2) 其次，数据集应包含多种模态。例如，状态输入可以从各种来源收集，如语言指令、来自不同传感器的视觉信息，甚至是视频；此外，状态-动作空间应具有多个分布模式，例如包含实现相同目标的不同轨迹。基于
    DGM 的算法应有效利用来自不同模态的信息，并学习覆盖策略空间中多个模式的策略。此外，基准测试不应局限于控制或决策任务，还应包括来自其他模态的任务，如计算机视觉和自然语言处理。这些跨模态任务可以作为辅助任务或用于评估通用智能体的能力。(3)
    任务场景应更加现实。计算机视觉和自然语言处理技术通常基于 DGM，其能够广泛应用于现实生活中的主要原因是这些算法直接在现实生活任务/数据集上进行评估，而不是在简化的模拟器上。因此，大规模的现实离线数据集可以大大推动离线策略学习的发展，并展示基于
    DGM 的算法的优势。(4) 基准测试应评估的不仅仅是策略回报。它还应评估诸如学习策略的泛化能力、安全性以及在噪声或稀疏奖励环境中的策略学习的稳健性等方面。'
- en: Comparisons among different DGMs. Although there are numerous offline RL or
    IL algorithms for each type of DGMs and they have shared benchmarks, comparisons
    among them are quite rare. It would be insightful to see fair and thorough comparisons
    among different DGMs with the same usage (e.g., as policy backbones) on the same
    set of tasks. Also, as task complexity or dataset size escalates, analyzing the
    trends in policy performance and computational cost across different DGMs can
    provide useful insights. Such comparisons provide suggestions on the choice of
    DGMs, conditioning on the task difficulty and dataset scale. Moreover, as mentioned
    in the main text of this paper, algorithms that fall in the same category, such
    as NF-based offline RL algorithms, are also rarely compared with each other. Comparisons
    within the same category can be valuable, as they can indicate which usage of
    a certain DGM is more effective.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 不同 DGM 之间的比较。尽管每种 DGM 类型都有许多离线 RL 或 IL 算法，并且它们共享基准测试，但它们之间的比较却相当少见。看到不同 DGM
    在相同任务集上（例如，作为策略骨干）进行公平和彻底的比较将是很有见地的。此外，随着任务复杂度或数据集规模的增加，分析不同 DGM 在策略表现和计算成本方面的趋势可以提供有用的见解。这种比较可以为选择
    DGM 提供建议，依据任务难度和数据集规模。此外，正如本文主要内容中提到的，属于同一类别的算法，例如基于 NF 的离线 RL 算法，之间的比较也很少见。同一类别中的比较是有价值的，因为它们可以指示某个
    DGM 的哪种使用方式更有效。
- en: 8.2.3 Future Works on Theories
  id: totrans-739
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.3 未来的理论研究
- en: Most works on applying DGMs in offline policy learning focus on algorithm designs
    rather than theoretical analysis, so more theoretical research works could be
    done for this field. One promising direction is derivations of the convergence
    rate or performance guarantee for the seminal works in DGM-based offline policy
    learning, such as (Zhang et al. ([2020b](#bib.bib390))) for GAIL and (Brandfonbrener
    et al. ([2022](#bib.bib30))) for return-conditioned supervised learning like Decision
    Transformer, based on the theoretical results from either DGMs or offline RL/IL.
    Moreover, we notice that there have been efforts on theoretically unifying the
    objectives of different DGMs for a deeper understanding of the relationship between
    DGMs and improved learning performance, such as (Nielsen et al. ([2020](#bib.bib233));
    Kingma & Gao ([2023](#bib.bib163))). This group of works can greatly inspire development
    of new DGM-based offline policy learning algorithms or unified analysis of existing
    ones.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数关于将 DGM 应用于离线政策学习的工作集中于算法设计，而非理论分析，因此这一领域还可以进行更多的理论研究。一个有前途的方向是推导 DGM 基于离线政策学习的开创性工作（例如，GAIL
    的 (Zhang et al. ([2020b](#bib.bib390))) 和基于回报条件的监督学习如决策变换器的 (Brandfonbrener et
    al. ([2022](#bib.bib30))) 的收敛速率或性能保证，基于 DGM 或离线 RL/IL 的理论结果。此外，我们注意到已经有努力在理论上统一不同
    DGM 的目标，以更深入地理解 DGM 之间的关系和改进学习性能，例如 (Nielsen et al. ([2020](#bib.bib233)); Kingma
    & Gao ([2023](#bib.bib163)))。这一组工作可以极大地激发新 DGM 基于离线政策学习算法的开发或现有算法的统一分析。
- en: 'Next, we outline some specific theoretical problems in this field. (1) First,
    regarding GAN-based IL (i.e., Section [4.1](#S4.SS1 "4.1 Imitation Learning ‣
    4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")), so far, no approach has been developed that is both computationally
    efficient and can theoretically guarantee the recovery of the reward function.
    Also, many works propose to replace the on-policy RL within GAIL/AIRL (i.e., TRPO)
    with off-policy RL algorithms (e.g., DDPG) for improved sample efficiency. However,
    this improvement lacks theoretical backup. With off-policy RL, the policy training
    at each iteration is based on samples from multiple past iterations, during which
    the reward function keeps changing. As a result, the RL training is conducted
    in an unstationary MDP where typical RL algorithms would theoretically fail. (2)
    Second, as mentioned in Section [6.1.5](#S6.SS1.SSS5 "6.1.5 Reflections on Transformer-based
    Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), return-conditioned
    supervised learning (e.g., Decision Transformer) alone is unlikely to be a general
    solution for offline RL problems, and it offers guarantees only when the environment
    dynamics are nearly deterministic. Thus, adapting these algorithms to stochastic
    environments, ideally backed by theoretical optimality guarantees, is a clear
    necessity. Also, Decision Transformer would imitate both high-return and low-return
    trajectories, and claims that the learning performance in this manner is better
    than imitating high-return trajectories only. A theoretical explanation on how
    the low-return behavior learning would benefit the overall performance can be
    insightful. (3) Third, as introduced in Section [7.2.2](#S7.SS2.SSS2 "7.2.2 Diffusion
    Models as Planners ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"), the algorithm design
    of Diffuser is mainly based on intuitions from similar problems, such as the inpainting
    problem and receding horizon control, but lacks theoretical support. To be specific,
    in Eq. ([99](#S7.E99 "In 7.2.2 Diffusion Models as Planners ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    to decide on the action choice $a$ at state $s$, the trajectory generation is
    hardcoded to start with $s$, and then the action $\hat{a}$ that is right after
    $s$ in the generated trajectory is adopted as $a$. The problem here is how to
    ensure the causal relationship between $s$ and $a$, which is crucial for (RL)
    sequential decision-making, and whether it is necessary to generate a trajectory
    segment in case that only the first action prediction is utilized.'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们概述该领域中的一些具体理论问题。 (1) 首先，关于基于 GAN 的 IL（即第 [4.1](#S4.SS1 "4.1 Imitation
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 节），目前尚未开发出既具有计算效率又能理论上保证奖励函数恢复的方法。此外，许多工作提议用离线 RL 算法（例如
    DDPG）替代 GAIL/AIRL（即 TRPO）中的在线 RL，以提高样本效率。然而，这种改进缺乏理论支持。使用离线 RL 时，每次迭代中的策略训练基于来自多个过去迭代的样本，而奖励函数持续变化。因此，RL
    训练在一个非平稳的 MDP 中进行，其中典型的 RL 算法理论上会失败。 (2) 其次，如第 [6.1.5](#S6.SS1.SSS5 "6.1.5 Reflections
    on Transformer-based Offline Reinforcement Learning ‣ 6.1 Offline Reinforcement
    Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    节所述，单独使用回报条件的监督学习（例如 Decision Transformer）不太可能成为离线 RL 问题的通用解决方案，并且只有在环境动态几乎确定的情况下才能提供保证。因此，将这些算法适应于随机环境，并
    ideally 有理论上的最优性保证，是显而易见的必要。此外，Decision Transformer 将模仿高回报和低回报轨迹，并声称这种方式的学习性能优于仅模仿高回报轨迹的性能。关于低回报行为学习如何有利于整体性能的理论解释可能会很有启发。
    (3) 第三，如第 [7.2.2](#S7.SS2.SSS2 "7.2.2 Diffusion Models as Planners ‣ 7.2 Offline
    Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 节所介绍的，Diffuser 的算法设计主要基于类似问题的直觉，如修复问题和递归控制，但缺乏理论支持。具体来说，在
    Eq. ([99](#S7.E99 "In 7.2.2 Diffusion Models as Planners ‣ 7.2 Offline Reinforcement
    Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"))
    中，为了决定在状态 $s$ 下的动作选择 $a$，轨迹生成被硬编码为从 $s$ 开始，然后将生成轨迹中紧跟在 $s$ 之后的动作 $\hat{a}$ 作为
    $a$ 采用。这里的问题是如何确保 $s$ 和 $a$ 之间的因果关系，这对于 (RL) 顺序决策至关重要，以及是否有必要生成一个轨迹片段以防只利用第一个动作预测。'
- en: 8.2.4 Future Works on Algorithm Designs
  id: totrans-742
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.4 算法设计的未来工作
- en: 'From the discussions in Section [8.1](#S8.SS1 "8.1 Discussions on Deep Generative
    Models and Offline Policy Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") and the main content in Section [3](#S3 "3 Variational Auto-Encoders
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") - [7](#S7 "7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"), we can identify
    many potential future directions regarding the algorithm design. Here, we list
    some of them as examples.'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: '从第[8.1](#S8.SS1 "8.1 Discussions on Deep Generative Models and Offline Policy
    Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")节的讨论和第[3](#S3
    "3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")节的主要内容
    - [7](#S7 "7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")，我们可以确定许多潜在的未来方向，涉及算法设计。在这里，我们列出一些作为例子。'
- en: 'Under-explored categories in the main text. For existing categories of algorithms,
    some of them are still under-explored, such as GAN-based IL algorithms that do
    not rely on simulators (Section [4.1](#S4.SS1 "4.1 Imitation Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), NF-based
    IL where the NF works as the policy (Section [5.1.2](#S5.SS1.SSS2 "5.1.2 Policy
    Modeling in Imitation Learning with Normalizing Flows ‣ 5.1 Imitation Learning
    ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    NF-based offline RL algorithms (Section [5.2.1](#S5.SS2.SSS1 "5.2.1 Adopting Normalizing
    Flows in Offline Reinforcement Learning ‣ 5.2 Reinforcement Learning with Offline
    Data ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")),
    efficient algorithms on mitigating impacts from environmental stochasticity for
    transformer-based offline RL (Section [6.1.3](#S6.SS1.SSS3 "6.1.3 Mitigating Impacts
    from Environmental Stochasticity ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), transformer-based
    offline RL which integrates traditional offline RL methods with transformer architectures
    (Section [6.1.5](#S6.SS1.SSS5 "6.1.5 Reflections on Transformer-based Offline
    Reinforcement Learning ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")), DM-based offline policy
    learning where DMs work as data synthesizers (Section [7.2.3](#S7.SS2.SSS3 "7.2.3
    Diffusion Models as Data Synthesizers ‣ 7.2 Offline Reinforcement Learning ‣ 7
    Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")), and
    so on.'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: '在主文中有一些尚未充分探讨的类别。对于现有的算法类别，其中一些仍然未被深入研究，例如不依赖于模拟器的基于GAN的IL算法（第[4.1节](#S4.SS1
    "4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in Offline Policy
    Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")），NF作为策略的NF-based IL（第[5.1.2节](#S5.SS1.SSS2
    "5.1.2 Policy Modeling in Imitation Learning with Normalizing Flows ‣ 5.1 Imitation
    Learning ‣ 5 Normalizing Flows in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")），NF-based离线RL算法（第[5.2.1节](#S5.SS2.SSS1
    "5.2.1 Adopting Normalizing Flows in Offline Reinforcement Learning ‣ 5.2 Reinforcement
    Learning with Offline Data ‣ 5 Normalizing Flows in Offline Policy Learning ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")），减轻基于变换器的离线RL中环境随机性影响的高效算法（第[6.1.3节](#S6.SS1.SSS3 "6.1.3
    Mitigating Impacts from Environmental Stochasticity ‣ 6.1 Offline Reinforcement
    Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")），将传统离线RL方法与变换器架构结合的基于变换器的离线RL（第[6.1.5节](#S6.SS1.SSS5
    "6.1.5 Reflections on Transformer-based Offline Reinforcement Learning ‣ 6.1 Offline
    Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")），将DM作为数据合成器的基于DM的离线策略学习（第[7.2.3节](#S7.SS2.SSS3 "7.2.3 Diffusion Models
    as Data Synthesizers ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion Models
    in Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")），等等。'
- en: 'Integrated use of various DGMs for offline policy learning. As mentioned in
    Section [8.1](#S8.SS1 "8.1 Discussions on Deep Generative Models and Offline Policy
    Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), each
    DGM has distinct usages due to their special design and there have been a few
    algorithms managing to integrate two DGMs together for improved offline policy
    learning performance. More in-depth exploration can be done in this direction,
    especially for involving more DGMs in a unified offline policy learning framework.
    For example, we observe from Table [12](#S8.T12 "Table 12 ‣ 8.1 Discussions on
    Deep Generative Models and Offline Policy Learning ‣ 8 Discussions and Open Problems
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") that most DGM-based IL algorithms are based on BC, which
    is a relatively basic IL algorithm and has fundamental issues ^(26)^(26)26BC is
    implemented as supervised learning, of which the objective is shown as Eq. ([88](#S7.E88
    "In 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")). However, predictions made by $\pi$, which is learned
    with BC and used for sequential decision-making, can impact future observations,
    thus breaching a fundamental assumption of supervised learning (Ross & Bagnell
    ([2010](#bib.bib281))): training inputs should be drawn from an independent and
    identically distributed population. Consequently, errors and deviations from the
    demonstrated behavior tend to accumulate over time, as minor mistakes lead the
    agent into areas of the observation space that the expert has not ventured into,
    known as the compounding error. Additionally, BC learns policies merely through
    imitation, without engaging in reasoning, which restricts the generalization capability
    of the learned policy. In this case, extending the base IL framework for transformer-
    or DM-based IL can be essential future directions., while NFs are integrated with
    various advanced IL frameworks due to its ability for exact density estimations.
    In this case, NFs can be integrated with transformer-based DMs, where NFs extend
    the base IL framework and transformer-based DMs work as a core component of a
    generalist agent that can deal with multi-modal input and high-dimensional generations.
    Further, they can be integrated with VAEs, which can be used to extract task or
    subtask representations for multi-task or hierarchical learning, as introduced
    in Section [3.1.5](#S3.SS1.SSS5 "3.1.5 Offline Multi-task/Hierarchical RL based
    on VAEs ‣ 3.1 Offline Reinforcement Learning ‣ 3 Variational Auto-Encoders in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions"). In addition, we notice
    that there are some research working on integrating the advantages of various
    DGMs for computer vision tasks, such as VQ-GAN (VAE+GAN+Transformer, Esser et al.
    ([2021](#bib.bib86))), TransGAN (GAN+Transformer, Jiang et al. ([2021](#bib.bib152))),
    DiffuseVAE (VAE+DM, Pandey et al. ([2022](#bib.bib238))), DiTs (Transformer+DM,
    Peebles & Xie ([2023](#bib.bib247))), which can be potentially utilized for offline
    policy learning.'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: '各种深度生成模型（DGM）的集成用于离线策略学习。如第[8.1](#S8.SS1 "8.1 Discussions on Deep Generative
    Models and Offline Policy Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")节所述，由于其特殊设计，每种DGM具有不同的用途，目前已有少数算法成功地将两种DGM集成在一起，以提升离线策略学习的性能。可以在这个方向上进行更深入的探索，尤其是将更多的DGM纳入统一的离线策略学习框架。例如，我们从表[12](#S8.T12
    "Table 12 ‣ 8.1 Discussions on Deep Generative Models and Offline Policy Learning
    ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")中观察到，大多数基于DGM的IL算法都基于BC，这是一种相对基础的IL算法，并且存在基本问题^(26)^(26)26BC被实现为监督学习，其目标如公式([88](#S7.E88
    "In 7.1 Imitation Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"))所示。然而，通过BC学习的$\pi$用于顺序决策时，会影响未来的观察，从而违反了监督学习的一个基本假设（Ross
    & Bagnell ([2010](#bib.bib281))）：训练输入应来自于独立且同分布的总体。因此，错误和偏差倾向于随着时间的推移而积累，因为微小的错误将代理引导到专家未曾涉足的观察空间区域，这被称为累积误差。此外，BC仅通过模仿来学习策略，而没有进行推理，这限制了所学习策略的泛化能力。在这种情况下，将基础IL框架扩展到基于变换器或DM的IL可能是重要的未来方向，同时，NFs由于其精确的密度估计能力而被集成到各种先进的IL框架中。在这种情况下，NFs可以与基于变换器的DMs集成，其中NFs扩展了基础IL框架，而基于变换器的DMs则作为通用代理的核心组件，能够处理多模态输入和高维生成。此外，它们还可以与VAEs集成，VAEs可以用于提取任务或子任务表示，以进行多任务或层级学习，如第[3.1.5](#S3.SS1.SSS5
    "3.1.5 Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")节中介绍的那样。此外，我们还注意到有些研究正在整合各种DGM在计算机视觉任务中的优势，如VQ-GAN（VAE+GAN+Transformer,
    Esser et al. ([2021](#bib.bib86)）），TransGAN（GAN+Transformer, Jiang et al. ([2021](#bib.bib152)）），DiffuseVAE（VAE+DM,
    Pandey et al. ([2022](#bib.bib238)）），DiTs（Transformer+DM, Peebles & Xie ([2023](#bib.bib247)）），这些方法有可能被用于离线策略学习。'
- en: 'Unsolved issues and extensions of DGM-based offline policy learning. Table
    [13](#S8.T13 "Table 13 ‣ 8.1 Discussions on Deep Generative Models and Offline
    Policy Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    provides a summary of the issues and extensions of IL and offline RL that have
    been explored by DGM-based algorithms, which provides indications for future works.
    (1) By checking the outlined issues, we can pinpoint unresolved or inadequately
    addressed issues by existing methods as future directions. For example, although
    there have been GAN-based or DM-based algorithms targeting at the compounding
    error issue, the GAN-based ones rely on simulators and DM-based ones (i.e., AWE
    and MCNN in Section [7.1.2](#S7.SS1.SSS2 "7.1.2 Addressing Common Issues of Imitation
    Learning with Diffusion Models ‣ 7.1 Imitation Learning ‣ 7 Diffusion Models in
    Offline Policy Learning ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")) are still in the early
    stages of development. Similarly, in offline RL, reward sparsity is a significant
    issue, for which the explorations made by NFs (i.e., NF Shaping in Section [5.2.2](#S5.SS2.SSS2
    "5.2.2 Adopting Normalizing Flows in Online Reinforcement Learning with Offline
    Data ‣ 5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions")) and transformers (i.e., DTRD
    in Section [6.1.2](#S6.SS1.SSS2 "6.1.2 Balancing Model Capacity with Training
    Data ‣ 6.1 Offline Reinforcement Learning ‣ 6 Transformers in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions")) have tried reward shaping and reward function learning,
    which are both interesting directions awaiting further development. (2) Regarding
    the extensions, there can be in total six (or more) setup extensions: multi-task,
    multi-agent, hierarchical, model-based, safety, and generalization. Table [13](#S8.T13
    "Table 13 ‣ 8.1 Discussions on Deep Generative Models and Offline Policy Learning
    ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions") illustrates
    the extensions that are awaiting exploration for each DGM. Further, we note that
    the extension method can be potentially transferred among DGMs. For instance,
    as shown in Section [7.2.4](#S7.SS2.SSS4 "7.2.4 Diffusion Models in Extended Offline
    Reinforcement Learning Setups ‣ 7.2 Offline Reinforcement Learning ‣ 7 Diffusion
    Models in Offline Policy Learning ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions"), DM-based
    multi-task and hierarchical offline RL algorithms adopt the same algorithm ideas
    as BOReL and OPAL (i.e., the VAE-based methods introduced in Section [3.1.5](#S3.SS1.SSS5
    "3.1.5 Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions")). In particular, we note that multi-agent extensions for DGM-based
    offline policy learning require further development, and existing explorations
    (such as MAGAIL, MA-AIRL in Section [4.1](#S4.SS1 "4.1 Imitation Learning ‣ 4
    Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    and DOM2, MADiff in Section [7.2.4](#S7.SS2.SSS4 "7.2.4 Diffusion Models in Extended
    Offline Reinforcement Learning Setups ‣ 7.2 Offline Reinforcement Learning ‣ 7
    Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")) are
    still quite immature. Specifically, developments of MARL algorithms for fully
    competitive or mixed (partially cooperative/competitive) task scenarios based
    on game theory, and integrations of DGMs with state-of-the-art CTDE ^(27)^(27)27CTDE
    refers to a scheme of multi-agent RL (MARL) that employs centralized training
    and decentralized execution. MARL methods are essential future directions.'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 'DGM 基于的离线策略学习中的未解决问题和扩展。表格 [13](#S8.T13 "Table 13 ‣ 8.1 Discussions on Deep
    Generative Models and Offline Policy Learning ‣ 8 Discussions and Open Problems
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 提供了 DGM 基于算法探索的 IL 和离线 RL 问题及扩展的总结，为未来工作提供了指示。 (1) 通过检查列出的这些问题，我们可以确定现有方法未解决或处理不充分的问题作为未来的方向。例如，尽管已经有针对复合错误问题的
    GAN 基于或 DM 基于算法，但 GAN 基于算法依赖于模拟器，而 DM 基于算法（即第 [7.1.2](#S7.SS1.SSS2 "7.1.2 Addressing
    Common Issues of Imitation Learning with Diffusion Models ‣ 7.1 Imitation Learning
    ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") 节中提到的
    AWE 和 MCNN）仍处于开发的早期阶段。同样，在离线 RL 中，奖励稀疏性是一个重要问题，其中 NFs（即第 [5.2.2](#S5.SS2.SSS2
    "5.2.2 Adopting Normalizing Flows in Online Reinforcement Learning with Offline
    Data ‣ 5.2 Reinforcement Learning with Offline Data ‣ 5 Normalizing Flows in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") 节中的 NF Shaping）和变换器（即第 [6.1.2](#S6.SS1.SSS2
    "6.1.2 Balancing Model Capacity with Training Data ‣ 6.1 Offline Reinforcement
    Learning ‣ 6 Transformers in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    节中的 DTRD）尝试了奖励塑造和奖励函数学习，这些都是值得进一步发展的有趣方向。 (2) 关于扩展，总共有六种（或更多）设置扩展：多任务、多智能体、层次结构、基于模型、安全性和泛化。表格
    [13](#S8.T13 "Table 13 ‣ 8.1 Discussions on Deep Generative Models and Offline
    Policy Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")
    展示了每个 DGM 需要探索的扩展。此外，我们注意到扩展方法可能会在 DGMs 之间转移。例如，如第 [7.2.4](#S7.SS2.SSS4 "7.2.4
    Diffusion Models in Extended Offline Reinforcement Learning Setups ‣ 7.2 Offline
    Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 节所示，DM 基于的多任务和层次结构离线 RL 算法采用了与 BOReL 和 OPAL（即第 [3.1.5](#S3.SS1.SSS5
    "3.1.5 Offline Multi-task/Hierarchical RL based on VAEs ‣ 3.1 Offline Reinforcement
    Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") 节中介绍的基于 VAE 的方法）相同的算法思想。特别是，我们注意到，DGM 基于的离线策略学习的多智能体扩展需要进一步发展，现有的探索（如第
    [4.1](#S4.SS1 "4.1 Imitation Learning ‣ 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions") 节中的 MAGAIL、MA-AIRL 和第 [7.2.4](#S7.SS2.SSS4
    "7.2.4 Diffusion Models in Extended Offline Reinforcement Learning Setups ‣ 7.2
    Offline Reinforcement Learning ‣ 7 Diffusion Models in Offline Policy Learning
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 节中的 DOM2、MADiff）仍然相当不成熟。特别是基于博弈理论的完全竞争或混合（部分合作/竞争）任务场景的
    MARL 算法的发展，以及与最先进的 CTDE (27)^(27)27CTDE 指的是一种采用集中训练和分散执行的多智能体强化学习（MARL）方案。MARL
    方法是未来的重要方向的集成。'
- en: 'Development of generalist agents. One of the most exciting future directions
    for DGM-based offline policy learning is developing a decision-making agent that
    can continually evolve with the amount of training data and the size of the DGM-based
    policy network and can handle a range of tasks, including unseen ones. CV and
    NLP have made great progress in this direction, owing to the use of DGMs. To realize
    this for offline policy learning, several things should be done. (1) First, the
    base IL or offline RL algorithm should be compatible with deep and broad neural
    networks, so that its performance can grow with the size of the foundation model.
    This is also the reason why mainstream transformer-based and DM-based offline
    policy learning methods (e.g., Decision Transformer and Diffuser) are based on
    supervised learning as in CV and NLP. Future works can work on developing deep
    foundation models that are compatible with policy-gradient methods or dynamic
    programming. (2) Second, besides constructing large-scale, high-quality training
    datasets as mentioned in Section [8.2.1](#S8.SS2.SSS1 "8.2.1 Future Works on Data-centric
    Research ‣ 8.2 Perspectives on Future Directions ‣ 8 Discussions and Open Problems
    ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), some algorithm-driven efforts can be done. For example,
    algorithms that are robust to suboptimal training data or capable of processing
    multi-modal inputs should be developed. Specifically, learning from demonstration
    videos (in third-person views) is a quite challenging but promising research direction,
    as large-scale video datasets are more accessible and videos usually contain more
    learnable information than images or vectorized data. Agents capable of learning
    from videos have the potential to continuously evolve by observing their surrounding
    environments during deployment. (3) Third, to enhance generalization, employing
    advanced meta-learning or multi-task learning techniques is advisable. Further,
    adopting the "pretraining + fine-tuning" approach, similar to that used in large
    language and vision models, is also promising, since it enables the accumulation
    of training efforts. Notably, the more significant the difference between the
    source and target tasks, the more challenging it becomes to make pretraining effective,
    yet the more data can be utilized for training the agent.'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: '通用智能体的发展。基于DGM的离线策略学习未来最令人兴奋的方向之一是开发一个可以随着训练数据量和DGM基础策略网络规模不断进化的决策智能体，并且能够处理包括未见任务在内的多种任务。计算机视觉（CV）和自然语言处理（NLP）在这方面取得了巨大进展，这要归功于DGMs的使用。为了实现这一目标，离线策略学习需要做几件事。（1）首先，基础的IL或离线RL算法应该兼容深度和广泛的神经网络，以便其性能能够随着基础模型的规模增长。这也是主流的基于变换器和DM的离线策略学习方法（例如，Decision
    Transformer和Diffuser）之所以采用与CV和NLP类似的监督学习的原因。未来的工作可以致力于开发兼容策略梯度方法或动态规划的深度基础模型。（2）其次，除了构建如第[8.2.1节](#S8.SS2.SSS1
    "8.2.1 Future Works on Data-centric Research ‣ 8.2 Perspectives on Future Directions
    ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")所提到的大规模高质量训练数据集外，还可以进行一些算法驱动的工作。例如，应该开发对次优训练数据具有鲁棒性或能够处理多模态输入的算法。具体来说，从演示视频（第三人称视角）中学习是一个相当具有挑战性但又充满前景的研究方向，因为大规模视频数据集更加容易获取，且视频通常包含比图像或矢量化数据更多的可学习信息。能够从视频中学习的智能体有潜力通过在部署期间观察其周围环境而不断进化。（3）第三，为了增强泛化能力，建议采用先进的元学习或多任务学习技术。此外，采用类似于大型语言和视觉模型中使用的“预训练+微调”方法也是有前途的，因为它能够积累训练成果。值得注意的是，源任务和目标任务之间的差异越大，预训练的效果就越难以实现，但也可以利用更多的数据来训练智能体。'
- en: 'Future works in related areas. Finally, there are some related areas that can
    be used as future research directions. First, reviews on the applications of DGMs
    in Inverse Reinforcement Learning (IRL) or (Online) Reinforcement Learning have
    not yet been developed. IRL and RL are important approaches for sequential decision-making,
    but they both require online interactions with the environment. Second, as shown
    in Table [12](#S8.T12 "Table 12 ‣ 8.1 Discussions on Deep Generative Models and
    Offline Policy Learning ‣ 8 Discussions and Open Problems ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"),
    integrations with dynamic-programming-based, model-based, and trajectory-optimization-based
    offline RL have been explored. There are some other branches in offline RL (Levine
    et al. ([2020](#bib.bib181))), such as importance-sampling-based and uncertainty-estimation-based
    offline RL, which have the potential to be enhanced with DGMs. Third, we notice
    that new foundation models and DGMs are continually emerging, such as Mamba (Gu
    & Dao ([2023](#bib.bib112))) and Poisson Flows (Xu et al. ([2022c](#bib.bib363);
    [2023](#bib.bib364))). Applying these SOTA models in offline policy learning is
    certainly a promising research direction.'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: '相关领域的未来工作。最后，还有一些相关领域可以作为未来的研究方向。首先，尚未开展有关深度生成模型（DGM）在逆向强化学习（IRL）或（在线）强化学习中的应用的综述。IRL和RL是顺序决策的重要方法，但这两者都需要与环境进行在线交互。其次，如表[12](#S8.T12
    "Table 12 ‣ 8.1 Discussions on Deep Generative Models and Offline Policy Learning
    ‣ 8 Discussions and Open Problems ‣ Deep Generative Models for Offline Policy
    Learning: Tutorial, Survey, and Perspectives on Future Directions")所示，已探索了与基于动态规划、基于模型和基于轨迹优化的离线RL的集成。离线RL中还有一些其他分支（Levine
    et al. ([2020](#bib.bib181)))，例如基于重要性采样和不确定性估计的离线RL，这些领域有可能通过DGM得到增强。第三，我们注意到新型基础模型和DGM不断涌现，例如Mamba（Gu
    & Dao ([2023](#bib.bib112)))和Poisson Flows（Xu et al. ([2022c](#bib.bib363); [2023](#bib.bib364)))。将这些SOTA模型应用于离线策略学习无疑是一个有前景的研究方向。'
- en: 9 Conclusion
  id: totrans-749
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this paper, we show a systematic review on the applications of DGMs in offline
    policy learning. In Section [2](#S2 "2 Background on Deep Generative Models ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), we provide necessary background on several mainstream
    DGMs, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing
    Flows, Transformers, and Diffusion Models. Then, in Section [3](#S3 "3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") - [7](#S7
    "7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"), we
    introduce the applications of each DGM in both offline RL and IL, i.e., the two
    primary branches of offline policy learning. Each section includes both a tutorial
    and a survey on the respective topic. Following these main content, we provide
    a summary on existing works and our perspectives on future research directions
    in Section [8](#S8 "8 Discussions and Open Problems ‣ Deep Generative Models for
    Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions").
    As the first review paper of this field, we wish this work to be a hands-on reference
    for better understanding the current research progress on DGMs in offline policy
    learning and help researchers to further improve DGM-based offline RL/IL algorithms.'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们对DGM在离线策略学习中的应用进行了系统性的综述。在第[2](#S2 "2 Background on Deep Generative
    Models ‣ Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
    and Perspectives on Future Directions")节中，我们提供了有关几种主流DGM的必要背景，包括变分自编码器、生成对抗网络、标准化流、变换器和扩散模型。然后，在第[3](#S3
    "3 Variational Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models
    for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")节至第[7](#S7
    "7 Diffusion Models in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions")节中，我们介绍了每种DGM在离线RL和IL中的应用，即离线策略学习的两个主要分支。每节包括有关各自主题的教程和综述。在这些主要内容之后，我们在第[8](#S8
    "8 Discussions and Open Problems ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions")节中提供了对现有工作的总结以及对未来研究方向的展望。作为该领域的第一篇综述论文，我们希望这项工作能成为理解DGM在离线策略学习中当前研究进展的实用参考，并帮助研究人员进一步改进基于DGM的离线RL/IL算法。'
- en: Acknowledgement
  id: totrans-751
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'Author contributions: Bhargav Ganguly completes Section [3.2](#S3.SS2 "3.2
    Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"), and creates a GitHub repository to maintain the list of
    related works. Yang Xu completes Section [4.2](#S4.SS2 "4.2 Offline Reinforcement
    Learning ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep
    Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions"). Yongsheng Mei completes Section [2.5](#S2.SS5 "2.5 Diffusion
    Models ‣ 2 Background on Deep Generative Models ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions"). Jiayu
    Chen polishes these three sections by adding Subsection [3.2.1](#S3.SS2.SSS1 "3.2.1
    Core Schemes of VAE-based Imitation Learning ‣ 3.2 Imitation Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") and
    [4.2.1](#S4.SS2.SSS1 "4.2.1 Background on Model-based Offline Reinforcement Learning
    ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative Adversarial Networks in Offline
    Policy Learning ‣ Deep Generative Models for Offline Policy Learning: Tutorial,
    Survey, and Perspectives on Future Directions"), and completes all the other sections.
    Tian Lan and Vaneet Aggarwal supervise this project.'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: '作者贡献：Bhargav Ganguly 完成了第 [3.2](#S3.SS2 "3.2 Imitation Learning ‣ 3 Variational
    Auto-Encoders in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") 节，并创建了一个
    GitHub 存储库以维护相关工作的列表。Yang Xu 完成了第 [4.2](#S4.SS2 "4.2 Offline Reinforcement Learning
    ‣ 4 Generative Adversarial Networks in Offline Policy Learning ‣ Deep Generative
    Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future
    Directions") 节。Yongsheng Mei 完成了第 [2.5](#S2.SS5 "2.5 Diffusion Models ‣ 2 Background
    on Deep Generative Models ‣ Deep Generative Models for Offline Policy Learning:
    Tutorial, Survey, and Perspectives on Future Directions") 节。Jiayu Chen 通过添加小节
    [3.2.1](#S3.SS2.SSS1 "3.2.1 Core Schemes of VAE-based Imitation Learning ‣ 3.2
    Imitation Learning ‣ 3 Variational Auto-Encoders in Offline Policy Learning ‣
    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives
    on Future Directions") 和 [4.2.1](#S4.SS2.SSS1 "4.2.1 Background on Model-based
    Offline Reinforcement Learning ‣ 4.2 Offline Reinforcement Learning ‣ 4 Generative
    Adversarial Networks in Offline Policy Learning ‣ Deep Generative Models for Offline
    Policy Learning: Tutorial, Survey, and Perspectives on Future Directions") 打磨了这三个部分，并完成了所有其他部分。Tian
    Lan 和 Vaneet Aggarwal 监督了这个项目。'
- en: We thank Hanhan Zhou for participating in discussions and assisting in material
    collections.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢 Hanhan Zhou 参与讨论并协助收集资料。
- en: References
  id: totrans-754
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abramson et al. (2021) Josh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale,
    Mary Cassin, Felix Fischer, Petko Georgiev, Alex Goldin, Tim Harley, Felix Hill,
    Peter C. Humphreys, Alden Hung, Jessica Landon, Timothy P. Lillicrap, Hamza Merzic,
    Alistair Muldal, Adam Santoro, Guy Scully, Tamara von Glehn, Greg Wayne, Nathaniel
    Wong, Chen Yan, and Rui Zhu. Creating multimodal interactive agents with imitation
    and self-supervised learning. *CoRR*, abs/2112.03763, 2021.
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abramson 等（2021）Josh Abramson、Arun Ahuja、Arthur Brussee、Federico Carnevale、Mary
    Cassin、Felix Fischer、Petko Georgiev、Alex Goldin、Tim Harley、Felix Hill、Peter C.
    Humphreys、Alden Hung、Jessica Landon、Timothy P. Lillicrap、Hamza Merzic、Alistair
    Muldal、Adam Santoro、Guy Scully、Tamara von Glehn、Greg Wayne、Nathaniel Wong、Chen
    Yan 和 Rui Zhu。创建具有模仿和自监督学习的多模态互动代理。*CoRR*，abs/2112.03763，2021年。
- en: Ada et al. (2023) Suzan Ece Ada, Erhan Öztop, and Emre Ugur. Diffusion policies
    for out-of-distribution generalization in offline reinforcement learning. *CoRR*,
    abs/2307.04726, 2023.
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ada 等（2023）Suzan Ece Ada、Erhan Öztop 和 Emre Ugur。离线强化学习中的分布外泛化的扩散策略。*CoRR*，abs/2307.04726，2023年。
- en: Agarwal et al. (2020) Shubhankar Agarwal, Harshit Sikchi, Cole Gulino, and Eric
    Wilkinson. Imitative planning using conditional normalizing flow. *CoRR*, abs/2007.16162,
    2020.
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等（2020）Shubhankar Agarwal、Harshit Sikchi、Cole Gulino 和 Eric Wilkinson。使用条件归一化流的模仿规划。*CoRR*，abs/2007.16162，2020年。
- en: 'Ajay et al. (2021) Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine,
    and Ofir Nachum. OPAL: offline primitive discovery for accelerating offline reinforcement
    learning. In *Proceedings of the 9th International Conference on Learning Representations*.
    OpenReview.net, 2021.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ajay 等（2021）Anurag Ajay、Aviral Kumar、Pulkit Agrawal、Sergey Levine 和 Ofir Nachum。OPAL：用于加速离线强化学习的离线原始发现。在
    *第九届国际学习表征会议论文集* 中。OpenReview.net，2021年。
- en: Ajay et al. (2023) Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, Tommi S.
    Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need
    for decision making? In *Proceedings of the 11th International Conference on Learning
    Representations*. OpenReview.net, 2023.
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ajay et al. (2023) Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, Tommi S.
    Jaakkola 和 Pulkit Agrawal. 条件生成建模是否就是你决策所需的一切？发表于 *Proceedings of the 11th International
    Conference on Learning Representations*. OpenReview.net, 2023.
- en: 'Akimov et al. (2022) Dmitry Akimov, Vladislav Kurenkov, Alexander Nikulin,
    Denis Tarasov, and Sergey Kolesnikov. Let offline rl flow: Training conservative
    agents in the latent space of normalizing flows. In *NeurIPS Offline RL Workshop*,
    2022.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Akimov et al. (2022) Dmitry Akimov, Vladislav Kurenkov, Alexander Nikulin,
    Denis Tarasov 和 Sergey Kolesnikov. 让离线 rl 流动起来: 在归一化流的潜在空间中训练保守代理。发表于 *NeurIPS
    Offline RL Workshop*, 2022.'
- en: 'Alcorn & Nguyen (2021) Michael A. Alcorn and Anh Nguyen. baller2vec++: A look-ahead
    multi-entity transformer for modeling coordinated agents. *CoRR*, abs/2104.11980,
    2021.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alcorn & Nguyen (2021) Michael A. Alcorn 和 Anh Nguyen. baller2vec++: 一种前瞻性多实体变换器，用于建模协调代理。*CoRR*,
    abs/2104.11980, 2021.'
- en: Alemi et al. (2017) Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin
    Murphy. Deep variational information bottleneck. In *Proceedings of the 5th International
    Conference on Learning Representations*. OpenReview.net, 2017.
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alemi et al. (2017) Alexander A. Alemi, Ian Fischer, Joshua V. Dillon 和 Kevin
    Murphy. 深度变分信息瓶颈。发表于 *Proceedings of the 5th International Conference on Learning
    Representations*. OpenReview.net, 2017.
- en: 'Altman (1998) Eitan Altman. Constrained markov decision processes with total
    cost criteria: Lagrangian approach and dual linear program. *Mathematical methods
    of operations research*, 48(3):387–417, 1998.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Altman (1998) Eitan Altman. 总成本标准的约束马尔可夫决策过程: 拉格朗日方法和对偶线性规划。*Mathematical methods
    of operations research*, 48(3):387–417, 1998.'
- en: Amari (1993) Shun-ichi Amari. Backpropagation and stochastic gradient descent
    method. *Neurocomputing*, 5(4-5):185–196, 1993.
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amari (1993) Shun-ichi Amari. 反向传播和随机梯度下降方法。*Neurocomputing*, 5(4-5):185–196,
    1993.
- en: 'Amit et al. (2021) Tomer Amit, Tal Shaharbany, Eliya Nachmani, and Lior Wolf.
    Segdiff: Image segmentation with diffusion probabilistic models. *arXiv preprint
    arXiv:2112.00390*, 2021.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Amit et al. (2021) Tomer Amit, Tal Shaharbany, Eliya Nachmani 和 Lior Wolf.
    Segdiff: 使用扩散概率模型进行图像分割。*arXiv preprint arXiv:2112.00390*, 2021.'
- en: Anderson (1982) Brian DO Anderson. Reverse-time diffusion equation models. *Stochastic
    Processes and their Applications*, 12(3):313–326, 1982.
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson (1982) Brian DO Anderson. 反向时间扩散方程模型。*Stochastic Processes and their
    Applications*, 12(3):313–326, 1982.
- en: Arjovsky et al. (2017) Martín Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein
    GAN. *CoRR*, abs/1701.07875, 2017.
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky et al. (2017) Martín Arjovsky, Soumith Chintala 和 Léon Bottou. Wasserstein
    GAN。*CoRR*, abs/1701.07875, 2017.
- en: Arnold (1992) Vladimir I Arnold. *Ordinary differential equations*. Springer
    Science & Business Media, 1992.
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arnold (1992) Vladimir I Arnold. *常微分方程*. Springer Science & Business Media,
    1992.
- en: Austin et al. (2021a) Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow,
    and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces.
    In *Advances in Neural Information Processing Systems 34*, 2021a.
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin et al. (2021a) Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow
    和 Rianne van den Berg. 离散状态空间中的结构化去噪扩散模型。发表于 *Advances in Neural Information Processing
    Systems 34*, 2021a.
- en: Austin et al. (2021b) Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow,
    and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces.
    *Advances in Neural Information Processing Systems*, 34:17981–17993, 2021b.
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin et al. (2021b) Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow
    和 Rianne Van Den Berg. 离散状态空间中的结构化去噪扩散模型。*Advances in Neural Information Processing
    Systems*, 34:17981–17993, 2021b.
- en: Avrahami et al. (2022) Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
    diffusion for text-driven editing of natural images. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pp.  18208–18218, 2022.
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Avrahami et al. (2022) Omri Avrahami, Dani Lischinski 和 Ohad Fried. 用于自然图像文本驱动编辑的混合扩散。发表于
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    页码 18208–18218, 2022.
- en: Ba et al. (2016) Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer
    normalization. *CoRR*, abs/1607.06450, 2016.
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ba et al. (2016) Lei Jimmy Ba, Jamie Ryan Kiros 和 Geoffrey E. Hinton. 层归一化。*CoRR*,
    abs/1607.06450, 2016.
- en: Baram et al. (2017) Nir Baram, Oron Anschel, Itai Caspi, and Shie Mannor. End-to-end
    differentiable adversarial imitation learning. In *Proceedings of the 34th International
    Conference on Machine Learning*, volume 70, pp.  390–399, 2017.
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baram et al. (2017) Nir Baram, Oron Anschel, Itai Caspi 和 Shie Mannor. 端到端可微对抗模仿学习。发表于
    *Proceedings of the 34th International Conference on Machine Learning*, volume 70,
    页码 390–399, 2017.
- en: Baranchuk et al. (2021) Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin
    Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion
    models. *arXiv preprint arXiv:2112.03126*, 2021.
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baranchuk et al. (2021) Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin
    Khrulkov, 和 Artem Babenko. 基于扩散模型的标签高效语义分割。 *arXiv预印本 arXiv:2112.03126*，2021。
- en: Behrmann et al. (2019) Jens Behrmann, Will Grathwohl, Ricky T. Q. Chen, David
    Duvenaud, and Jörn-Henrik Jacobsen. Invertible residual networks. In *Proceedings
    of the 36th International Conference on Machine Learning*, volume 97 of *Proceedings
    of Machine Learning Research*, pp.  573–582\. PMLR, 2019.
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Behrmann et al. (2019) Jens Behrmann, Will Grathwohl, Ricky T. Q. Chen, David
    Duvenaud, 和 Jörn-Henrik Jacobsen. 可逆残差网络。 在 *第36届国际机器学习会议论文集*，第97卷 *机器学习研究论文集*，第573–582页。PMLR，2019。
- en: 'Bellemare et al. (2013) Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael
    Bowling. The arcade learning environment: An evaluation platform for general agents.
    *Journal of Artificial Intelligence Research*, 47:253–279, 2013.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare et al. (2013) Marc G. Bellemare, Yavar Naddaf, Joel Veness, 和 Michael
    Bowling. 《街机学习环境：通用代理的评估平台》。*人工智能研究期刊*，47:253–279, 2013。
- en: 'Bian et al. (2022) Xihan Bian, Oscar Mendez Maldonado, and Simon Hadfield.
    SKILL-IL: disentangling skill and knowledge in multitask imitation learning. In
    *IEEE/RSJ International Conference on Intelligent Robots and Systems*, pp.  7060–7065\.
    IEEE, 2022.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bian et al. (2022) Xihan Bian, Oscar Mendez Maldonado, 和 Simon Hadfield. SKILL-IL：在多任务模仿学习中解开技能与知识。
    在 *IEEE/RSJ国际智能机器人与系统会议*，第7060–7065页。IEEE，2022。
- en: Blondé & Kalousis (2019) Lionel Blondé and Alexandros Kalousis. Sample-efficient
    imitation learning via generative adversarial nets. In *Proceedings of the 22nd
    International Conference on Artificial Intelligence and Statistics*, volume 89,
    pp.  3138–3148, 2019.
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blondé & Kalousis (2019) Lionel Blondé 和 Alexandros Kalousis. 通过生成对抗网络的样本有效模仿学习。
    在 *第22届国际人工智能与统计会议论文集*，第89卷，第3138–3148页，2019。
- en: Boborzi et al. (2021a) Damian Boborzi, Christoph-Nikolas Straehle, Jens S. Buchner,
    and Lars Mikelsons. Learning normalizing flow policies based on highway demonstrations.
    In *Proceedings of the 24th IEEE International Intelligent Transportation Systems
    Conference*, pp.  22–29\. IEEE, 2021a.
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boborzi et al. (2021a) Damian Boborzi, Christoph-Nikolas Straehle, Jens S. Buchner,
    和 Lars Mikelsons. 基于高速公路演示学习标准化流策略。 在 *第24届IEEE国际智能交通系统会议论文集*，第22–29页。IEEE，2021a。
- en: Boborzi et al. (2021b) Damian Boborzi, Christoph-Nikolas Straehle, Jens Stefan
    Buchner, and Lars Mikelsons. State-only imitation learning by trajectory distribution
    matching. In *Submission to the 10th International Conference on Learning Representations*,
    2021b.
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boborzi et al. (2021b) Damian Boborzi, Christoph-Nikolas Straehle, Jens Stefan
    Buchner, 和 Lars Mikelsons. 仅通过状态的轨迹分布匹配的模仿学习。 在 *第10届国际学习表征会议投稿*，2021b。
- en: Boborzi et al. (2022) Damian Boborzi, Christoph-Nikolas Straehle, Jens S. Buchner,
    and Lars Mikelsons. Imitation learning by state-only distribution matching. *CoRR*,
    abs/2202.04332, 2022.
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boborzi et al. (2022) Damian Boborzi, Christoph-Nikolas Straehle, Jens S. Buchner,
    和 Lars Mikelsons. 仅通过状态分布匹配的模仿学习。*CoRR*，abs/2202.04332，2022。
- en: Bonatti et al. (2020) Rogerio Bonatti, Ratnesh Madaan, Vibhav Vineet, Sebastian A.
    Scherer, and Ashish Kapoor. Learning visuomotor policies for aerial navigation
    using cross-modal representations. In *IEEE/RSJ International Conference on Intelligent
    Robots and Systems*, pp.  1637–1644\. IEEE, 2020.
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bonatti et al. (2020) Rogerio Bonatti, Ratnesh Madaan, Vibhav Vineet, Sebastian
    A. Scherer, 和 Ashish Kapoor. 使用跨模态表示学习视觉运动策略进行空中导航。 在 *IEEE/RSJ国际智能机器人与系统会议*，第1637–1644页。IEEE，2020。
- en: Boustati et al. (2021) Ayman Boustati, Hana Chockler, and Daniel C. McNamee.
    Transfer learning with causal counterfactual reasoning in decision transformers.
    *CoRR*, abs/2110.14355, 2021.
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boustati et al. (2021) Ayman Boustati, Hana Chockler, 和 Daniel C. McNamee. 通过因果反事实推理进行决策变换器的迁移学习。*CoRR*，abs/2110.14355，2021。
- en: Brandfonbrener et al. (2022) David Brandfonbrener, Alberto Bietti, Jacob Buckman,
    Romain Laroche, and Joan Bruna. When does return-conditioned supervised learning
    work for offline reinforcement learning? In *Advances in Neural Information Processing
    Systems 35*, 2022.
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brandfonbrener et al. (2022) David Brandfonbrener, Alberto Bietti, Jacob Buckman,
    Romain Laroche, 和 Joan Bruna. 回报条件的监督学习何时适用于离线强化学习？ 在 *神经信息处理系统35*，2022。
- en: 'Brehmer et al. (2023) Johann Brehmer, Joey Bose, Pim de Haan, and Taco Cohen.
    EDGI: equivariant diffusion for planning with embodied agents. *CoRR*, abs/2303.12410,
    2023.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brehmer et al. (2023) Johann Brehmer, Joey Bose, Pim de Haan, 和 Taco Cohen.
    EDGI：用于具身代理规划的等变扩散。*CoRR*，abs/2303.12410，2023。
- en: Brockman et al. (2016) Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas
    Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brockman等（2016）**Greg Brockman**、**Vicki Cheung**、**Ludwig Pettersson**、**Jonas
    Schneider**、**John Schulman**、**Jie Tang** 和 **Wojciech Zaremba**。OpenAI Gym，2016。
- en: Brooks et al. (2024) Tim Brooks, Bill Peebles, Connor Homes, Will DePue, Yufei
    Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Wing Yin
    Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators,
    2024. URL [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators).
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brooks等（2024）**Tim Brooks**、**Bill Peebles**、**Connor Homes**、**Will DePue**、**Yufei
    Guo**、**Li Jing**、**David Schnurr**、**Joe Taylor**、**Troy Luhman**、**Eric Luhman**、**Clarence
    Wing Yin Ng**、**Ricky Wang** 和 **Aditya Ramesh**。视频生成模型作为世界模拟器，2024。URL [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In *Advances in Neural Information
    Processing Systems 33*, 2020.
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等（2020）**Tom B. Brown**、**Benjamin Mann**、**Nick Ryder**、**Melanie Subbiah**、**Jared
    Kaplan**、**Prafulla Dhariwal**、**Arvind Neelakantan**、**Pranav Shyam**、**Girish
    Sastry**、**Amanda Askell**、**Sandhini Agarwal**、**Ariel Herbert-Voss**、**Gretchen
    Krueger**、**Tom Henighan**、**Rewon Child**、**Aditya Ramesh**、**Daniel M. Ziegler**、**Jeffrey
    Wu**、**Clemens Winter**、**Christopher Hesse**、**Mark Chen**、**Eric Sigler**、**Mateusz
    Litwin**、**Scott Gray**、**Benjamin Chess**、**Jack Clark**、**Christopher Berner**、**Sam
    McCandlish**、**Alec Radford**、**Ilya Sutskever** 和 **Dario Amodei**。语言模型是少样本学习者。载于
    *神经信息处理系统进展 33*，2020。
- en: Cang et al. (2022) Catherine Cang, Kourosh Hakhamaneshi, Ryan Rudes, Igor Mordatch,
    Aravind Rajeswaran, Pieter Abbeel, and Michael Laskin. Semi-supervised offline
    reinforcement learning with pre-trained decision transformers, 2022. URL [https://openreview.net/forum?id=fwJWhOxuzV9](https://openreview.net/forum?id=fwJWhOxuzV9).
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cang等（2022）**Catherine Cang**、**Kourosh Hakhamaneshi**、**Ryan Rudes**、**Igor
    Mordatch**、**Aravind Rajeswaran**、**Pieter Abbeel** 和 **Michael Laskin**。使用预训练决策变换器的半监督离线强化学习，2022。URL
    [https://openreview.net/forum?id=fwJWhOxuzV9](https://openreview.net/forum?id=fwJWhOxuzV9)。
- en: Chang et al. (2018) Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert,
    and Elliot Holtham. Reversible architectures for arbitrarily deep residual neural
    networks. In *Proceedings of the 32nd AAAI Conference on Artificial Intelligence*,
    pp.  2811–2818\. AAAI Press, 2018.
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常等（2018）**Bo Chang**、**Lili Meng**、**Eldad Haber**、**Lars Ruthotto**、**David
    Begert** 和 **Elliot Holtham**。任意深度残差神经网络的可逆架构。载于 *第32届AAAI人工智能大会论文集*，页码2811–2818。AAAI
    Press，2018。
- en: 'Chang et al. (2022) Wei-Di Chang, Juan Camilo Gamboa Higuera, Scott Fujimoto,
    David Meger, and Gregory Dudek. Il-flow: Imitation learning from observation using
    normalizing flows. *arXiv preprint arXiv:2205.09251*, 2022.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常等（2022）**Wei-Di Chang**、**Juan Camilo Gamboa Higuera**、**Scott Fujimoto**、**David
    Meger** 和 **Gregory Dudek**。Il-flow：使用归一化流的观察模仿学习。*arXiv预印本 arXiv:2205.09251*，2022。
- en: 'Chebotar et al. (2023) Yevgen Chebotar, Quan Vuong, Karol Hausman, Fei Xia,
    Yao Lu, Alex Irpan, Aviral Kumar, Tianhe Yu, Alexander Herzog, Karl Pertsch, et al.
    Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions.
    In *Conference on Robot Learning*, pp.  3909–3928\. PMLR, 2023.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chebotar等（2023）**Yevgen Chebotar**、**Quan Vuong**、**Karol Hausman**、**Fei Xia**、**Yao
    Lu**、**Alex Irpan**、**Aviral Kumar**、**Tianhe Yu**、**Alexander Herzog**、**Karl
    Pertsch** 等。Q-transformer：通过自回归 Q 函数的可扩展离线强化学习。载于 *机器人学习大会*，页码3909–3928。PMLR，2023。
- en: 'Chen et al. (2022) Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Transdreamer:
    Reinforcement learning with transformer world models. *CoRR*, abs/2202.09481,
    2022.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2022）**Chang Chen**、**Yi-Fu Wu**、**Jaesik Yoon** 和 **Sungjin Ahn**。Transdreamer：使用变换器世界模型的强化学习。*CoRR*，abs/2202.09481，2022。
- en: Chen et al. (2018a) Changyou Chen, Chunyuan Li, Liquan Chen, Wenlin Wang, Yunchen
    Pu, and Lawrence Carin. Continuous-time flows for efficient inference and density
    estimation. In *Proceedings of the 35th International Conference on Machine Learning*,
    volume 80, pp.  823–832\. PMLR, 2018a.
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2018a）**Changyou Chen**、**Chunyuan Li**、**Liquan Chen**、**Wenlin Wang**、**Yunchen
    Pu** 和 **Lawrence Carin**。高效推理和密度估计的连续时间流。载于 *第35届国际机器学习大会论文集*，第80卷，页码823–832。PMLR，2018a。
- en: Chen et al. (2021a) Cynthia Chen, Xin Chen, Sam Toyer, Cody Wild, Scott Emmons,
    Ian Fischer, Kuang-Huei Lee, Neel Alex, Steven H. Wang, Ping Luo, Stuart Russell,
    Pieter Abbeel, and Rohin Shah. An empirical investigation of representation learning
    for imitation. In *Proceedings of the Neural Information Processing Systems Track
    on Datasets and Benchmarks 1*, 2021a.
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021a）辛西娅·陈、辛·陈、萨姆·托耶、科迪·怀尔德、斯科特·埃蒙斯、伊恩·费舍尔、匡辉·李、尼尔·亚历克斯、史蒂文·H·王、平·罗、斯图尔特·拉塞尔、皮特·阿贝尔和罗欣·沙。《对模仿学习表征学习的实证研究》。在*神经信息处理系统数据集与基准追踪会议论文集
    1*，2021a。
- en: 'Chen et al. (2023a) Daoming Chen, Ning Wang, Feng Chen, and Tony Pipe. Detrive:
    Imitation learning with transformer detection for end-to-end autonomous driving.
    *CoRR*, abs/2310.14224, 2023a.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2023a）道明·陈、宁·王、冯·陈和托尼·派普。《Detrive：基于变换器检测的模仿学习用于端到端自动驾驶》。*CoRR*，abs/2310.14224，2023a。
- en: Chen et al. (2023b) Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu.
    Offline reinforcement learning via high-fidelity generative behavior modeling.
    In *Proceedings of the 11th International Conference on Learning Representations*.
    OpenReview.net, 2023b.
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2023b）华宇·陈、程·陆、程阳·英、杭·苏和俊·朱。《通过高保真生成行为建模进行离线强化学习》。在*第11届国际学习表征大会论文集*。OpenReview.net，2023b。
- en: Chen et al. (2023c) Jiayu Chen, Tian Lan, and Vaneet Aggarwal. Hierarchical
    adversarial inverse reinforcement learning. *IEEE Transactions on Neural Networks
    and Learning Systems*, 2023c.
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2023c）贾玉·陈、田岚和瓦尼特·阿格瓦尔。《层次对抗逆向强化学习》。*IEEE神经网络与学习系统汇刊*，2023c。
- en: Chen et al. (2023d) Jiayu Chen, Dipesh Tamboli, Tian Lan, and Vaneet Aggarwal.
    Multi-task hierarchical adversarial inverse reinforcement learning. In *Proceedings
    of the 40th International Conference on Machine Learning*, volume 202, pp.  4895–4920,
    2023d.
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2023d）贾玉·陈、迪佩什·坦博利、田岚和瓦尼特·阿格瓦尔。《多任务层次对抗逆向强化学习》。在*第40届国际机器学习大会论文集*，第202卷，页4895–4920，2023d。
- en: 'Chen et al. (2021b) Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya
    Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision
    transformer: Reinforcement learning via sequence modeling. *Advances in neural
    information processing systems*, 34:15084–15097, 2021b.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021b）莉莉·陈、凯文·陆、阿拉文·拉杰斯瓦兰、基敏·李、阿迪提亚·格罗弗、米莎·拉斯金、皮特·阿贝尔、阿拉文·斯里尼瓦斯和伊戈尔·莫达奇。《决策变换器：通过序列建模进行强化学习》。*神经信息处理系统进展*，34:15084–15097，2021b。
- en: Chen et al. (2018b) Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David
    Duvenaud. Neural ordinary differential equations. In *Advances in Neural Information
    Processing Systems 31*, pp.  6572–6583, 2018b.
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2018b）田齐·陈、尤利亚·鲁巴诺娃、杰西·贝滕考特和戴维·杜文奥德。《神经常微分方程》。在*神经信息处理系统进展 31*，页6572–6583，2018b。
- en: Chen et al. (2019) Tian Qi Chen, Jens Behrmann, David Duvenaud, and Jörn-Henrik
    Jacobsen. Residual flows for invertible generative modeling. In *Advances in Neural
    Information Processing Systems 32*, pp.  9913–9923, 2019.
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2019）田齐·陈、延斯·贝赫曼、戴维·杜文奥德和约恩-亨里克·雅各布森。《用于可逆生成建模的残差流》。在*神经信息处理系统进展 32*，页9913–9923，2019。
- en: 'Chen et al. (2021c) Valerie Chen, Abhinav Gupta, and Kenneth Marino. Ask your
    humans: Using human instructions to improve generalization in reinforcement learning.
    In *Proceedings of the 9th International Conference on Learning Representations*,
    2021c.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021c）瓦莱丽·陈、阿宾纳夫·古普塔和肯尼斯·马里诺。《问你的同行：利用人工指令提高强化学习中的泛化能力》。在*第9届国际学习表征大会论文集*，2021c。
- en: 'Chen et al. (2016) Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever,
    and Pieter Abbeel. Infogan: Interpretable representation learning by information
    maximizing generative adversarial nets. *Advances in neural information processing
    systems*, 29, 2016.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2016）希·陈、燕·段、雷因·豪特胡夫特、约翰·舒尔曼、伊利亚·苏茨克弗和皮特·阿贝尔。《Infogan：通过信息最大化生成对抗网络进行可解释的表征学习》。*神经信息处理系统进展*，29，2016。
- en: Chen et al. (2023e) Yuhui Chen, Haoran Li, and Dongbin Zhao. Boosting continuous
    control with consistency policy. *CoRR*, abs/2310.06343, 2023e.
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2023e）余辉·陈、浩然·李和董斌·赵。《通过一致性策略提升连续控制》。*CoRR*，abs/2310.06343，2023e。
- en: Chevalier-Boisvert et al. (2018) Maxime Chevalier-Boisvert, Lucas Willems, and
    Suman Pal. Minimalistic gridworld environment for openai gym. [https://github.com/maximecb/gym-minigrid](https://github.com/maximecb/gym-minigrid),
    2018.
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谢瓦利耶-博伊斯韦特等（2018）马克西姆·谢瓦利耶-博伊斯韦特、卢卡斯·威廉斯和苏曼·帕尔。《适用于 OpenAI Gym 的极简网格世界环境》。 [https://github.com/maximecb/gym-minigrid](https://github.com/maximecb/gym-minigrid)，2018。
- en: 'Chevalier-Boisvert et al. (2019) Maxime Chevalier-Boisvert, Dzmitry Bahdanau,
    Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio.
    Babyai: First steps towards grounded language learning with a human in the loop.
    In *International Conference on Learning Representations*, volume 105, 2019.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chevalier-Boisvert 等人（2019）Maxime Chevalier-Boisvert、Dzmitry Bahdanau、Salem
    Lahlou、Lucas Willems、Chitwan Saharia、Thien Huu Nguyen 和 Yoshua Bengio。Babyai：迈向以人为环的基础语言学习的第一步。在
    *International Conference on Learning Representations* 中，第 105 卷，2019年。
- en: 'Chi et al. (2023) Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau,
    Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning
    via action diffusion. In *Robotics: Science and Systems*, 2023.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chi 等人（2023）Cheng Chi、Siyuan Feng、Yilun Du、Zhenjia Xu、Eric Cousineau、Benjamin
    Burchfiel 和 Shuran Song。扩散策略：通过动作扩散进行视觉运动策略学习。在 *Robotics: Science and Systems*
    中，2023年。'
- en: Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    Generating long sequences with sparse transformers. *CoRR*, abs/1904.10509, 2019.
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child 等人（2019）Rewon Child、Scott Gray、Alec Radford 和 Ilya Sutskever。使用稀疏变换器生成长序列。*CoRR*，abs/1904.10509，2019年。
- en: 'Cho et al. (2022) Daesol Cho, Dongseok Shim, and H. Jin Kim. S2P: state-conditioned
    image synthesis for data augmentation in offline reinforcement learning. In *Advances
    in Neural Information Processing Systems 35*, 2022.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等人（2022）Daesol Cho、Dongseok Shim 和 H. Jin Kim。S2P：用于离线强化学习的数据增强的状态条件图像合成。在
    *Advances in Neural Information Processing Systems 35* 中，2022年。
- en: Chou & Tedrake (2023) Glen Chou and Russ Tedrake. Synthesizing stable reduced-order
    visuomotor policies for nonlinear systems via sums-of-squares optimization. *CoRR*,
    abs/2304.12405, 2023.
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chou & Tedrake（2023）Glen Chou 和 Russ Tedrake。通过平方和优化合成稳定的降阶视觉运动策略。*CoRR*，abs/2304.12405，2023年。
- en: Chung et al. (2015) Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel,
    Aaron C. Courville, and Yoshua Bengio. A recurrent latent variable model for sequential
    data. In *Advances in Neural Information Processing Systems 28*, pp.  2980–2988,
    2015.
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等人（2015）Junyoung Chung、Kyle Kastner、Laurent Dinh、Kratarth Goel、Aaron C.
    Courville 和 Yoshua Bengio。用于序列数据的递归潜变量模型。在 *Advances in Neural Information Processing
    Systems 28* 中，第 2980–2988 页，2015年。
- en: Cobbe et al. (2020) Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman.
    Leveraging procedural generation to benchmark reinforcement learning. In *Proceedings
    of the 37th International Conference on Machine Learning*, volume 119, pp.  2048–2056,
    2020.
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人（2020）Karl Cobbe、Christopher Hesse、Jacob Hilton 和 John Schulman。利用程序生成来基准化强化学习。在
    *Proceedings of the 37th International Conference on Machine Learning* 中，第 2048–2056
    页，2020年。
- en: 'Coleman et al. (2023) Matthew Coleman, Olga Russakovsky, Christine Allen-Blanchette,
    and Ye Zhu. Discrete diffusion reward guidance methods for offline reinforcement
    learning. In *ICML 2023 Workshop: Sampling and Optimization in Discrete Space*,
    2023.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Coleman 等人（2023）Matthew Coleman、Olga Russakovsky、Christine Allen-Blanchette
    和 Ye Zhu。用于离线强化学习的离散扩散奖励引导方法。在 *ICML 2023 Workshop: Sampling and Optimization
    in Discrete Space* 中，2023年。'
- en: Cornish et al. (2020) Robert Cornish, Anthony L. Caterini, George Deligiannidis,
    and Arnaud Doucet. Relaxing bijectivity constraints with continuously indexed
    normalising flows. In *Proceedings of the 37th International Conference on Machine
    Learning*, volume 119, pp.  2133–2143\. PMLR, 2020.
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cornish 等人（2020）Robert Cornish、Anthony L. Caterini、George Deligiannidis 和 Arnaud
    Doucet。通过连续索引归一化流放宽双射约束。在 *Proceedings of the 37th International Conference on
    Machine Learning* 中，第 2133–2143 页。PMLR，2020年。
- en: Correia & Alexandre (2022) André Correia and Luís A. Alexandre. Hierarchical
    decision transformer. *CoRR*, abs/2209.10447, 2022.
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Correia & Alexandre（2022）André Correia 和 Luís A. Alexandre。分层决策变换器。*CoRR*，abs/2209.10447，2022年。
- en: Coumans & Bai (2016–2021) Erwin Coumans and Yunfei Bai. Pybullet, a python module
    for physics simulation for games, robotics and machine learning. [http://pybullet.org](http://pybullet.org),
    2016–2021.
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coumans & Bai（2016–2021）Erwin Coumans 和 Yunfei Bai。Pybullet，一个用于游戏、机器人和机器学习的物理模拟
    Python 模块。 [http://pybullet.org](http://pybullet.org)，2016–2021年。
- en: 'Csiszár & Shields (2004) Imre Csiszár and Paul C. Shields. Information theory
    and statistics: A tutorial. *Foundations and Trends in Communications and Information
    Theory*, 1(4), 2004.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Csiszár & Shields（2004）Imre Csiszár 和 Paul C. Shields。信息理论与统计学：教程。*Foundations
    and Trends in Communications and Information Theory*，1(4)，2004年。
- en: Dance et al. (2021) Christopher R. Dance, Julien Perez, and Théo Cachet. Demonstration-conditioned
    reinforcement learning for few-shot imitation. In *Proceedings of the 38th International
    Conference on Machine Learning*, volume 139, pp.  2376–2387, 2021.
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dance 等人（2021）Christopher R. Dance、Julien Perez 和 Théo Cachet。演示条件的强化学习用于少样本模仿。在
    *Proceedings of the 38th International Conference on Machine Learning* 中，第 2376–2387
    页，2021年。
- en: Dasari & Gupta (2020) Sudeep Dasari and Abhinav Gupta. Transformers for one-shot
    visual imitation. In *Proceedings of the 4th Conference on Robot Learning*, volume
    155, pp.  2071–2084, 2020.
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasari & Gupta（2020）Sudeep Dasari 和 Abhinav Gupta。用于单次视觉模仿的变压器。见 *第4届机器人学习大会论文集*，第155卷，第2071–2084页，2020年。
- en: 'Davis et al. (2021) Jared Quincy Davis, Albert Gu, Krzysztof Choromanski, Tri
    Dao, Christopher Ré, Chelsea Finn, and Percy Liang. Catformer: Designing stable
    transformers via sensitivity analysis. In *Proceedings of the 38th International
    Conference on Machine Learning*, volume 139, pp.  2489–2499, 2021.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Davis 等（2021）Jared Quincy Davis, Albert Gu, Krzysztof Choromanski, Tri Dao,
    Christopher Ré, Chelsea Finn 和 Percy Liang。Catformer：通过敏感性分析设计稳定的变压器。见 *第38届国际机器学习大会论文集*，第139卷，第2489–2499页，2021年。
- en: De Haan et al. (2019) Pim De Haan, Dinesh Jayaraman, and Sergey Levine. Causal
    confusion in imitation learning. *Advances in Neural Information Processing Systems*,
    32, 2019.
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Haan 等（2019）Pim De Haan, Dinesh Jayaraman 和 Sergey Levine。模仿学习中的因果混淆。*神经信息处理系统进展*，第32卷，2019年。
- en: 'Devin (2024) Coline Devin. craftingworld. [https://github.com/cdevin/craftingworld](https://github.com/cdevin/craftingworld),
    2024. Accessed: 2024-02-05.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devin（2024）Coline Devin。craftingworld。 [https://github.com/cdevin/craftingworld](https://github.com/cdevin/craftingworld)，2024年。访问日期：2024年2月5日。
- en: Dhariwal & Nichol (2021) Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion
    models beat gans on image synthesis. In *Advances in Neural Information Processing
    Systems 34*, pp.  8780–8794, 2021.
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhariwal & Nichol（2021）Prafulla Dhariwal 和 Alexander Quinn Nichol。扩散模型在图像合成中击败
    GANs。见 *神经信息处理系统进展 34*，第8780–8794页，2021年。
- en: Ding & Jin (2023) Zihan Ding and Chi Jin. Consistency models as a rich and efficient
    policy class for reinforcement learning. *CoRR*, abs/2309.16984, 2023.
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding & Jin（2023）Zihan Ding 和 Chi Jin。作为丰富且高效政策类别的连贯性模型。*CoRR*，abs/2309.16984，2023年。
- en: 'Dinh et al. (2015) Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear
    independent components estimation. In *3rd International Conference on Learning
    Representations, Workshop Track Proceedings*, 2015.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dinh 等（2015）Laurent Dinh, David Krueger 和 Yoshua Bengio。NICE：非线性独立成分估计。见 *第3届国际表示学习大会，研讨会论文集*，2015年。
- en: Dinh et al. (2017) Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density
    estimation using real NVP. In *Proceedings of the 5th International Conference
    on Learning Representations*. OpenReview.net, 2017.
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dinh 等（2017）Laurent Dinh, Jascha Sohl-Dickstein 和 Samy Bengio。使用真实 NVP 的密度估计。见
    *第5届国际表示学习大会论文集*。OpenReview.net，2017年。
- en: Dong et al. (2023) Wenbo Dong, Shaofan Liu, and Shiliang Sun. Safe batch constrained
    deep reinforcement learning with generative adversarial network. *Information
    Science*, 634:259–270, 2023.
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2023）Wenbo Dong, Shaofan Liu 和 Shiliang Sun。具有生成对抗网络的安全批量约束深度强化学习。*信息科学*，634：259–270，2023年。
- en: 'Dong et al. (2021) Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
    Attention is not all you need: pure attention loses rank doubly exponentially
    with depth. In *Proceedings of the 38th International Conference on Machine Learning*,
    volume 139, pp.  2793–2803, 2021.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2021）Yihe Dong, Jean-Baptiste Cordonnier 和 Andreas Loukas。注意力不是你需要的一切：纯注意力在深度上以双指数级别丧失秩。见
    *第38届国际机器学习大会论文集*，第139卷，第2793–2803页，2021年。
- en: Donsker & Varadhan (1976) Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic
    evaluation of certain markov process expectations for large time—iii. *Communications
    on pure and applied Mathematics*, 29(4):389–461, 1976.
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Donsker & Varadhan（1976）Monroe D Donsker 和 SR Srinivasa Varadhan。大型时间下某些马尔可夫过程期望的渐近评价—III。*纯数学与应用数学通讯*，第29卷第4期：389–461，1976年。
- en: Dorfman et al. (2021) Ron Dorfman, Idan Shenfeld, and Aviv Tamar. Offline meta
    reinforcement learning - identifiability challenges and effective data collection
    strategies. In *Advances in Neural Information Processing Systems 34*, pp.  4607–4618,
    2021.
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dorfman 等（2021）Ron Dorfman, Idan Shenfeld 和 Aviv Tamar。离线元强化学习——可识别性挑战与有效数据收集策略。见
    *神经信息处理系统进展 34*，第4607–4618页，2021年。
- en: 'Dosovitskiy et al. (2017) Alexey Dosovitskiy, Germán Ros, Felipe Codevilla,
    Antonio M. López, and Vladlen Koltun. CARLA: an open urban driving simulator.
    In *Proceedings of the 1st Annual Conference on Robot Learning*, volume 78, pp. 
    1–16, 2017.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等（2017）Alexey Dosovitskiy, Germán Ros, Felipe Codevilla, Antonio
    M. López 和 Vladlen Koltun。CARLA：一个开放的城市驾驶模拟器。见 *第1届机器人学习年会论文集*，第78卷，第1–16页，2017年。
- en: 'Du et al. (2023) Maximilian Du, Suraj Nair, Dorsa Sadigh, and Chelsea Finn.
    Behavior retrieval: Few-shot imitation learning by querying unlabeled datasets.
    In *Robotics: Science and Systems XIX*, 2023.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等（2023）Maximilian Du, Suraj Nair, Dorsa Sadigh 和 Chelsea Finn。行为检索：通过查询未标记数据集进行少样本模仿学习。见
    *机器人：科学与系统 XIX*，2023年。
- en: Du et al. (2022) Yuqing Du, Daniel Ho, Alex Alemi, Eric Jang, and Mohi Khansari.
    Bayesian imitation learning for end-to-end mobile manipulation. In *Proceedings
    of the 39th International Conference on Machine Learning*, volume 162 of *Proceedings
    of Machine Learning Research*, pp.  5531–5546\. PMLR, 2022.
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. (2022) Yuqing Du、Daniel Ho、Alex Alemi、Eric Jang 和 Mohi Khansari. 基于贝叶斯的模仿学习用于端到端移动操作。发表于*第39届国际机器学习会议论文集*，*机器学习研究论文集*第162卷，第5531–5546页。PMLR，2022年。
- en: Dua et al. (2019) Dheeru Dua, Casey Graff, et al. Uci machine learning repository,
    2017. *URL http://archive. ics. uci. edu/ml*, 7(1), 2019.
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dua et al. (2019) Dheeru Dua、Casey Graff 等。UCI机器学习库，2017年。*网址 http://archive.ics.uci.edu/ml*，7(1)，2019年。
- en: Duffie & Pan (1997) Darrell Duffie and Jun Pan. An overview of value at risk.
    *Journal of derivatives*, 4(3):7–49, 1997.
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duffie & Pan (1997) Darrell Duffie 和 Jun Pan. 风险价值概述。*衍生品期刊*，4(3):7–49，1997年。
- en: Dupont et al. (2019) Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented
    neural odes. In *Advances in Neural Information Processing Systems 32*, pp.  3134–3144,
    2019.
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dupont et al. (2019) Emilien Dupont、Arnaud Doucet 和 Yee Whye Teh. 增强神经常微分方程。发表于*神经信息处理系统进展
    32*，第3134–3144页，2019年。
- en: Durkan et al. (2019) Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios.
    Neural spline flows. *Advances in neural information processing systems*, 32,
    2019.
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Durkan et al. (2019) Conor Durkan、Artur Bekasov、Iain Murray 和 George Papamakarios.
    神经样条流。*神经信息处理系统进展*，32，2019年。
- en: 'Emmons et al. (2022) Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and
    Sergey Levine. Rvs: What is essential for offline RL via supervised learning?
    In *Proceedings of the 10th International Conference on Learning Representations*.
    OpenReview.net, 2022.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Emmons et al. (2022) Scott Emmons、Benjamin Eysenbach、Ilya Kostrikov 和 Sergey
    Levine. Rvs：通过监督学习离线强化学习的关键是什么？发表于*第10届国际学习表征会议论文集*。OpenReview.net，2022年。
- en: Esser et al. (2021) Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers
    for high-resolution image synthesis. In *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*, pp.  12873–12883, 2021.
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esser et al. (2021) Patrick Esser、Robin Rombach 和 Bjorn Ommer. 驯化变换器以实现高分辨率图像合成。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，第12873–12883页，2021年。
- en: 'Eysenbach et al. (2019) Ben Eysenbach, Ruslan Salakhutdinov, and Sergey Levine.
    Search on the replay buffer: Bridging planning and reinforcement learning. In
    *Advances in Neural Information Processing Systems 32*, pp.  15220–15231, 2019.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eysenbach et al. (2019) Ben Eysenbach、Ruslan Salakhutdinov 和 Sergey Levine.
    回放缓冲区的搜索：弥合规划与强化学习。发表于*神经信息处理系统进展 32*，第15220–15231页，2019年。
- en: 'Fang et al. (2022) Kuan Fang, Patrick Yin, Ashvin Nair, Homer Walke, Gengchen
    Yan, and Sergey Levine. Generalization with lossy affordances: Leveraging broad
    offline data for learning visuomotor tasks. In *Conference on Robot Learning*,
    volume 205 of *Proceedings of Machine Learning Research*, pp.  106–117\. PMLR,
    2022.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2022) Kuan Fang、Patrick Yin、Ashvin Nair、Homer Walke、Gengchen Yan
    和 Sergey Levine. 具有损失感知的泛化：利用广泛的离线数据来学习视觉运动任务。发表于*机器人学习会议*，*机器学习研究论文集*第205卷，第106–117页。PMLR，2022年。
- en: 'Fei et al. (2020) Cong Fei, Bin Wang, Yuzheng Zhuang, Zongzhang Zhang, Jianye
    Hao, Hongbo Zhang, Xuewu Ji, and Wulong Liu. Triple-gail: A multi-modal imitation
    learning framework with generative adversarial nets. In *Proceedings of the 29th
    International Joint Conference on Artificial Intelligence*, pp.  2929–2935, 2020.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fei et al. (2020) Cong Fei、Bin Wang、Yuzheng Zhuang、Zongzhang Zhang、Jianye Hao、Hongbo
    Zhang、Xuewu Ji 和 Wulong Liu. Triple-gail：一个带有生成对抗网络的多模态模仿学习框架。发表于*第29届国际人工智能联合会议论文集*，第2929–2935页，2020年。
- en: Felsen et al. (2018) Panna Felsen, Patrick Lucey, and Sujoy Ganguly. Where will
    they go? predicting fine-grained adversarial multi-agent motion using conditional
    variational autoencoders. In *Proceedings of the 15th European Conference on Computer
    Vision*, volume 11215, pp.  761–776\. Springer, 2018.
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Felsen et al. (2018) Panna Felsen、Patrick Lucey 和 Sujoy Ganguly. 他们会去哪儿？使用条件变分自编码器预测精细粒度的对抗性多智能体运动。发表于*第15届欧洲计算机视觉会议论文集*，第11215卷，第761–776页。Springer，2018年。
- en: Florence et al. (2021) Pete Florence, Corey Lynch, Andy Zeng, Oscar A. Ramirez,
    Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan
    Tompson. Implicit behavioral cloning. In *Conference on Robot Learning*, volume
    164, pp.  158–168, 2021.
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Florence et al. (2021) Pete Florence、Corey Lynch、Andy Zeng、Oscar A. Ramirez、Ayzaan
    Wahid、Laura Downs、Adrian Wong、Johnny Lee、Igor Mordatch 和 Jonathan Tompson. 隐式行为克隆。发表于*机器人学习会议*，第164卷，第158–168页，2021年。
- en: Florence et al. (2022) Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez,
    Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan
    Tompson. Implicit behavioral cloning. In *Conference on Robot Learning*, pp. 
    158–168\. PMLR, 2022.
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Florence 等 (2022) Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan
    Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, 和 Jonathan Tompson.
    隐式行为克隆。在 *机器人学习会议* 中, pp. 158–168. PMLR, 2022.
- en: Foundation (2023) Farama Foundation. Frozen lake environment. [https://gymnasium.farama.org/environments/toy_text/frozen_lake/](https://gymnasium.farama.org/environments/toy_text/frozen_lake/),
    2023.
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foundation (2023) Farama Foundation. 冻结湖环境。 [https://gymnasium.farama.org/environments/toy_text/frozen_lake/](https://gymnasium.farama.org/environments/toy_text/frozen_lake/),
    2023.
- en: Fragkiadaki et al. (2016) Katerina Fragkiadaki, Pulkit Agrawal, Sergey Levine,
    and Jitendra Malik. Learning visual predictive models of physics for playing billiards.
    In *proceedings of the 4th International Conference on Learning Representations*,
    2016.
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fragkiadaki 等 (2016) Katerina Fragkiadaki, Pulkit Agrawal, Sergey Levine, 和
    Jitendra Malik. 学习用于打台球的视觉预测模型。在 *第4届国际学习表征会议论文集* 中, 2016.
- en: Freitag & Al-Onaizan (2017) Markus Freitag and Yaser Al-Onaizan. Beam search
    strategies for neural machine translation. *arXiv preprint arXiv:1702.01806*,
    2017.
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freitag & Al-Onaizan (2017) Markus Freitag 和 Yaser Al-Onaizan. 神经机器翻译的束搜索策略。*arXiv
    预印本 arXiv:1702.01806*, 2017.
- en: Freund et al. (2023) Gideon Joseph Freund, Elad Sarafian, and Sarit Kraus. A
    coupled flow approach to imitation learning. In *Proceedings of the 40th International
    Conference on Machine Learning*, pp.  10357–10372\. PMLR, 2023.
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freund 等 (2023) Gideon Joseph Freund, Elad Sarafian, 和 Sarit Kraus. 一种耦合流方法用于模仿学习。在
    *第40届国际机器学习会议论文集* 中, pp. 10357–10372. PMLR, 2023.
- en: 'Fu et al. (2023) Huiqiao Fu, Kaiqiang Tang, Yuanyang Lu, Yiming Qi, Guizhou
    Deng, Flood Sung, and Chunlin Chen. Ess-infogail: Semi-supervised imitation learning
    from imbalanced demonstrations. In *Advances in Neural Information Processing
    Systems 36*, 2023.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu 等 (2023) Huiqiao Fu, Kaiqiang Tang, Yuanyang Lu, Yiming Qi, Guizhou Deng,
    Flood Sung, 和 Chunlin Chen. Ess-infogail: 从不平衡演示中进行半监督模仿学习。在 *神经信息处理系统进展 36* 中,
    2023.'
- en: Fu et al. (2017) Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards
    with adversarial inverse reinforcement learning. *CoRR*, abs/1710.11248, 2017.
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 (2017) Justin Fu, Katie Luo, 和 Sergey Levine. 利用对抗性逆强化学习学习鲁棒奖励。*CoRR*,
    abs/1710.11248, 2017.
- en: 'Fu et al. (2020) Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey
    Levine. D4RL: datasets for deep data-driven reinforcement learning. *CoRR*, abs/2004.07219,
    2020.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu 等 (2020) Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, 和 Sergey Levine.
    D4RL: 深度数据驱动强化学习的数据集。*CoRR*, abs/2004.07219, 2020.'
- en: Fujimoto et al. (2019) Scott Fujimoto, David Meger, and Doina Precup. Off-policy
    deep reinforcement learning without exploration. In *Proceedings of the 36th International
    Conference on Machine Learning*, volume 97, pp.  2052–2062\. PMLR, 2019.
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fujimoto 等 (2019) Scott Fujimoto, David Meger, 和 Doina Precup. 无探索的离政策深度强化学习。在
    *第36届国际机器学习会议论文集* 中, 卷97, pp. 2052–2062. PMLR, 2019.
- en: Furuta et al. (2022) Hiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized
    decision transformer for offline hindsight information matching. In *Proceedings
    of the 10th International Conference on Learning Representations*. OpenReview.net,
    2022.
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Furuta 等 (2022) Hiroki Furuta, Yutaka Matsuo, 和 Shixiang Shane Gu. 针对离线回顾信息匹配的广义决策变换器。在
    *第10届国际学习表征会议论文集* 中。OpenReview.net, 2022.
- en: 'Ghasemipour et al. (2019a) Seyed Kamyar Seyed Ghasemipour, Shixiang Gu, and
    Richard S. Zemel. Smile: Scalable meta inverse reinforcement learning through
    context-conditional policies. In *Advances in Neural Information Processing Systems
    32*, pp.  7879–7889, 2019a.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ghasemipour 等 (2019a) Seyed Kamyar Seyed Ghasemipour, Shixiang Gu, 和 Richard
    S. Zemel. Smile: 通过上下文条件策略的可扩展元逆强化学习。在 *神经信息处理系统进展 32* 中, pp. 7879–7889, 2019a.'
- en: Ghasemipour et al. (2019b) Seyed Kamyar Seyed Ghasemipour, Richard S. Zemel,
    and Shixiang Gu. A divergence minimization perspective on imitation learning methods.
    In *Proceedings of the 3rd Annual Conference on Robot Learning*, volume 100, pp. 
    1259–1277\. PMLR, 2019b.
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghasemipour 等 (2019b) Seyed Kamyar Seyed Ghasemipour, Richard S. Zemel, 和 Shixiang
    Gu. 对模仿学习方法的发散最小化视角。在 *第3届年度机器人学习会议论文集* 中, 卷100, pp. 1259–1277. PMLR, 2019b.
- en: 'Gomez et al. (2017) Aidan N. Gomez, Mengye Ren, Raquel Urtasun, and Roger B.
    Grosse. The reversible residual network: Backpropagation without storing activations.
    In *Advances in Neural Information Processing Systems 30*, pp.  2214–2224, 2017.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gomez 等 (2017) Aidan N. Gomez, Mengye Ren, Raquel Urtasun, 和 Roger B. Grosse.
    可逆残差网络: 无需存储激活值的反向传播。在 *神经信息处理系统进展 30* 中, pp. 2214–2224, 2017.'
- en: 'Goodfellow (2017) Ian Goodfellow. Nips 2016 tutorial: Generative adversarial
    networks, 2017.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goodfellow (2017) Ian Goodfellow。NIPS 2016 讲座: 生成对抗网络，2017。'
- en: Goodfellow et al. (2014a) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    adversarial nets. *Advances in neural information processing systems*, 27, 2014a.
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2014a) Ian Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing
    Xu、David Warde-Farley、Sherjil Ozair、Aaron Courville 和 Yoshua Bengio。生成对抗网络。*神经信息处理系统进展*，27，2014a。
- en: Goodfellow et al. (2014b) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio.
    Generative adversarial nets. In *Advances in Neural Information Processing Systems
    27*, pp.  2672–2680, 2014b.
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2014b) Ian J. Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing
    Xu、David Warde-Farley、Sherjil Ozair、Aaron C. Courville 和 Yoshua Bengio。生成对抗网络。在
    *神经信息处理系统进展 27*，第 2672–2680 页，2014b。
- en: 'Grathwohl et al. (2018) Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya
    Sutskever, and David Duvenaud. Ffjord: Free-form continuous dynamics for scalable
    reversible generative models. *arXiv preprint arXiv:1810.01367*, 2018.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Grathwohl et al. (2018) Will Grathwohl、Ricky TQ Chen、Jesse Bettencourt、Ilya
    Sutskever 和 David Duvenaud。Ffjord: 可扩展可逆生成模型的自由形式连续动态。*arXiv 预印本 arXiv:1810.01367*，2018。'
- en: Gretton et al. (2012) Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch,
    Bernhard Schölkopf, and Alexander J. Smola. A kernel two-sample test. *Journal
    of Machine Learning Research*, 13:723–773, 2012.
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gretton et al. (2012) Arthur Gretton、Karsten M. Borgwardt、Malte J. Rasch、Bernhard
    Schölkopf 和 Alexander J. Smola。核两样本检验。*机器学习研究杂志*，13:723–773，2012。
- en: 'Gronauer (2022) Sven Gronauer. Bullet-safety-gym: A framework for constrained
    reinforcement learning. Technical report, mediaTUM, 2022.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gronauer (2022) Sven Gronauer。Bullet-safety-gym: 一个约束强化学习的框架。技术报告，mediaTUM，2022。'
- en: 'Grover et al. (2018) Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-gan:
    Combining maximum likelihood and adversarial learning in generative models. In
    *Proceedings of the 32nd AAAI Conference on Artificial Intelligence*, pp.  3069–3076\.
    AAAI Press, 2018.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Grover et al. (2018) Aditya Grover、Manik Dhar 和 Stefano Ermon。Flow-gan: 将最大似然与对抗学习结合于生成模型。在
    *第32届 AAAI 人工智能会议论文集*，第 3069–3076 页。AAAI Press，2018。'
- en: 'Gu & Dao (2023) Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling
    with selective state spaces. *CoRR*, abs/2312.00752, 2023.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu & Dao (2023) Albert Gu 和 Tri Dao。Mamba: 线性时间序列建模与选择性状态空间。*CoRR*，abs/2312.00752，2023。'
- en: 'Guan et al. (2023) Jiayi Guan, Shangding Gu, Zhijun Li, Jing Hou, Yiqin Yang,
    Guang Chen, and Changjun Jiang. Uac: Offline reinforcement learning with uncertain
    action constraint. *IEEE Transactions on Cognitive and Developmental Systems*,
    2023.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guan et al. (2023) Jiayi Guan、Shangding Gu、Zhijun Li、Jing Hou、Yiqin Yang、Guang
    Chen 和 Changjun Jiang。Uac: 不确定动作约束的离线强化学习。*IEEE 认知与发展系统汇刊*，2023。'
- en: 'Gupta et al. (2019a) Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine,
    and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation
    and reinforcement learning. In *Conference on Robot Learning*, volume 100, pp. 
    1025–1037, 2019a.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gupta et al. (2019a) Abhishek Gupta、Vikash Kumar、Corey Lynch、Sergey Levine
    和 Karol Hausman。Relay 策略学习: 通过模仿与强化学习解决长期任务。在 *机器人学习会议*，第 100 卷，第 1025–1037 页，2019a。'
- en: 'Gupta et al. (2019b) Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine,
    and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation
    and reinforcement learning. *arXiv preprint arXiv:1910.11956*, 2019b.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gupta et al. (2019b) Abhishek Gupta、Vikash Kumar、Corey Lynch、Sergey Levine
    和 Karol Hausman。Relay 策略学习: 通过模仿与强化学习解决长期任务。*arXiv 预印本 arXiv:1910.11956*，2019b。'
- en: 'Guss et al. (2019) William H. Guss, Brandon Houghton, Nicholay Topin, Phillip
    Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl: A large-scale
    dataset of minecraft demonstrations. In *Proceedings of the 28th International
    Joint Conference on Artificial Intelligence*, pp.  2442–2448\. ijcai.org, 2019.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guss et al. (2019) William H. Guss、Brandon Houghton、Nicholay Topin、Phillip
    Wang、Cayden Codel、Manuela Veloso 和 Ruslan Salakhutdinov。Minerl: 大规模 Minecraft
    演示数据集。在 *第28届国际联合人工智能会议论文集*，第 2442–2448 页。ijcai.org，2019。'
- en: 'Haarnoja et al. (2018) Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey
    Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning
    with a stochastic actor. In *Proceedings of the 35th International Conference
    on Machine Learning*, volume 80 of *Proceedings of Machine Learning Research*,
    pp.  1856–1865\. PMLR, 2018.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Haarnoja et al. (2018) Tuomas Haarnoja、Aurick Zhou、Pieter Abbeel 和 Sergey Levine。Soft
    actor-critic: 基于随机演员的离线最大熵深度强化学习。在 *第35届国际机器学习大会论文集*，*机器学习研究论文集*第 80 卷，第 1856–1865
    页。PMLR，2018。'
- en: 'Hafner et al. (2020) Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad
    Norouzi. Dream to control: Learning behaviors by latent imagination. In *Proceedings
    of the 8th International Conference on Learning Representations*, 2020.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner et al. (2020) 达尼贾尔·哈夫纳、蒂莫西·P·利利克拉普、吉米·巴和穆罕默德·诺鲁齐。梦想控制：通过潜在想象学习行为。在*第8届国际学习表征会议论文集*中，2020。
- en: Hambidge (1967) Jay Hambidge. *The elements of dynamic symmetry*. Courier Corporation,
    1967.
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hambidge (1967) 杰伊·汉比奇。*动态对称元素*。库里尔公司，1967。
- en: Han & Kim (2022) Jungwoo Han and Jinwhan Kim. Selective data augmentation for
    improving the performance of offline reinforcement learning. In *International
    Conference on Control, Automation and Systems (ICCAS)*, pp.  222–226\. IEEE, 2022.
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han & Kim (2022) 韩政宇和金镇焕。选择性数据增强以提高离线强化学习的性能。在*控制、自动化与系统国际会议（ICCAS）*中，第222–226页。IEEE，2022。
- en: Han et al. (2022) Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan
    Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. A survey
    on vision transformer. *IEEE transactions on pattern analysis and machine intelligence*,
    45(1):87–110, 2022.
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2022) 韩凯、王云鹤、陈涵亭、陈兴浩、郭建元、刘振华、唐叶辉、安晓、徐春静、徐一星等。关于视觉变换器的调查。*IEEE模式分析与机器智能学报*，45(1)：87–110，2022。
- en: 'Hansen-Estruch et al. (2023) Philippe Hansen-Estruch, Ilya Kostrikov, Michael
    Janner, Jakub Grudzien Kuba, and Sergey Levine. IDQL: implicit q-learning as an
    actor-critic method with diffusion policies. *CoRR*, abs/2304.10573, 2023.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hansen-Estruch et al. (2023) 菲利普·汉森-埃斯特鲁赫、伊利亚·科斯特里科夫、迈克尔·詹纳、雅库布·格鲁津·库巴和谢尔盖·列维丁。IDQL：隐式Q学习作为一种具有扩散策略的演员-评论家方法。*CoRR*，abs/2304.10573，2023。
- en: Hao et al. (2019) Xiaotian Hao, Weixun Wang, Jianye Hao, and Yaodong Yang. Independent
    generative adversarial self-imitation learning in cooperative multiagent systems.
    In *Proceedings of the 18th International Conference on Autonomous Agents and
    MultiAgent Systems*, pp.  1315–1323, 2019.
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao et al. (2019) 肖天浩、王卫勋、郝建业和杨耀东。在合作多智能体系统中进行独立生成对抗自我模仿学习。在*第18届国际自主代理与多智能体系统会议论文集*中，第1315–1323页，2019。
- en: Hausman et al. (2017) Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav S.
    Sukhatme, and Joseph J. Lim. Multi-modal imitation learning from unstructured
    demonstrations using generative adversarial nets. In *Advances in Neural Information
    Processing Systems 30*, pp.  1235–1245, 2017.
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hausman et al. (2017) 卡罗尔·豪斯曼、叶夫根·切博塔尔、斯特凡·沙尔、古拉夫·S·苏克哈特梅和约瑟夫·J·林。从非结构化演示中通过生成对抗网络进行多模态模仿学习。在*神经信息处理系统进展30*中，第1235–1245页，2017。
- en: He et al. (2023a) Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang,
    Dong Wang, Bin Zhao, and Xuelong Li. Diffusion model is an effective planner and
    data synthesizer for multi-task reinforcement learning. *CoRR*, abs/2305.18459,
    2023a.
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2023a) 何浩然、白晨佳、徐康、杨卓然、张伟南、王东、赵斌和李雪龙。扩散模型是多任务强化学习的有效规划器和数据合成器。*CoRR*，abs/2305.18459，2023a。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *2016 IEEE Conference on Computer
    Vision and Pattern Recognition*, pp.  770–778\. IEEE Computer Society, 2016.
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2016) 何恺明、张翔宇、任少卿和孙剑。深度残差学习用于图像识别。在*2016年IEEE计算机视觉与模式识别大会*中，第770–778页。IEEE计算机学会，2016。
- en: 'He et al. (2023b) Longxiang He, Linrui Zhang, Junbo Tan, and Xueqian Wang.
    Diffcps: Diffusion model based constrained policy search for offline reinforcement
    learning. *CoRR*, abs/2310.05333, 2023b.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2023b) 贺龙祥、张林锐、谭俊博和王雪倩。Diffcps：基于扩散模型的受限策略搜索用于离线强化学习。*CoRR*，abs/2310.05333，2023b。
- en: Hejna et al. (2023) Joey Hejna, Pieter Abbeel, and Lerrel Pinto. Improving long-horizon
    imitation through instruction prediction. In *Proceedings of the 37th AAAI Conference
    on Artificial Intelligence*, pp.  7857–7865, 2023.
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hejna et al. (2023) 乔伊·赫伊纳、皮特·阿贝尔和勒雷尔·平托。通过指令预测改进长期模仿。在*第37届AAAI人工智能会议论文集*中，第7857–7865页，2023。
- en: Hendrycks & Gimpel (2023) Dan Hendrycks and Kevin Gimpel. Gaussian error linear
    units (gelus), 2023.
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks & Gimpel (2023) 丹·亨德里克斯和凯文·金佩尔。高斯误差线性单元（gelus），2023。
- en: Hepburn & Montana (2022) Charles A. Hepburn and Giovanni Montana. Model-based
    trajectory stitching for improved offline reinforcement learning. *CoRR*, abs/2211.11603,
    2022.
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hepburn & Montana (2022) 查尔斯·A·赫本和乔凡尼·蒙塔纳。基于模型的轨迹拼接以改进离线强化学习。*CoRR*，abs/2211.11603，2022。
- en: 'Higgins et al. (2017) Irina Higgins, Loïc Matthey, Arka Pal, Christopher P.
    Burgess, Xavier Glorot, Matthew M. Botvinick, Shakir Mohamed, and Alexander Lerchner.
    beta-vae: Learning basic visual concepts with a constrained variational framework.
    In *Proceedings of the 5th International Conference on Learning Representations*.
    OpenReview.net, 2017.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Higgins 等（2017）Irina Higgins、Loïc Matthey、Arka Pal、Christopher P. Burgess、Xavier
    Glorot、Matthew M. Botvinick、Shakir Mohamed 和 Alexander Lerchner。beta-vae：使用受限变分框架学习基本视觉概念。在*第5届国际学习表征会议论文集*。OpenReview.net，2017。
- en: Hiriart-Urruty & Lemaréchal (2004) Jean-Baptiste Hiriart-Urruty and Claude Lemaréchal.
    *Fundamentals of convex analysis*. Springer Science & Business Media, 2004.
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hiriart-Urruty & Lemaréchal（2004）Jean-Baptiste Hiriart-Urruty 和 Claude Lemaréchal。*凸分析基础*。Springer
    Science & Business Media，2004。
- en: Ho & Ermon (2016) Jonathan Ho and Stefano Ermon. Generative adversarial imitation
    learning. In *Advances in Neural Information Processing Systems 29*, pp.  4565–4573,
    2016.
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho & Ermon（2016）Jonathan Ho 和 Stefano Ermon。生成对抗模仿学习。在*神经信息处理系统进展 29*，页4565–4573，2016。
- en: Ho & Salimans (2022) Jonathan Ho and Tim Salimans. Classifier-free diffusion
    guidance. *CoRR*, abs/2207.12598, 2022.
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho & Salimans（2022）Jonathan Ho 和 Tim Salimans。无分类器扩散指导。*CoRR*，abs/2207.12598，2022。
- en: 'Ho et al. (2019) Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter
    Abbeel. Flow++: Improving flow-based generative models with variational dequantization
    and architecture design. In *Proceedings of the 36th International Conference
    on Machine Learning*, volume 97, pp.  2722–2730\. PMLR, 2019.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等（2019）Jonathan Ho、Xi Chen、Aravind Srinivas、Yan Duan 和 Pieter Abbeel。Flow++：通过变分去量化和架构设计改进基于流的生成模型。在*第36届国际机器学习会议论文集*，第97卷，页2722–2730。PMLR，2019。
- en: Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion
    probabilistic models. *Advances in neural information processing systems*, 33:6840–6851,
    2020.
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等（2020）Jonathan Ho、Ajay Jain 和 Pieter Abbeel。去噪扩散概率模型。*神经信息处理系统进展*，33:6840–6851，2020。
- en: 'Hoogeboom et al. (2021) Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick
    Forré, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical
    distributions. *Advances in Neural Information Processing Systems*, 34:12454–12465,
    2021.'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoogeboom 等（2021）Emiel Hoogeboom、Didrik Nielsen、Priyank Jaini、Patrick Forré
    和 Max Welling。Argmax 流和多项式扩散：学习分类分布。*神经信息处理系统进展*，34:12454–12465，2021。
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for NLP. In *Proceedings of the 36th
    International Conference on Machine Learning*, volume 97, pp.  2790–2799, 2019.
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby 等（2019）Neil Houlsby、Andrei Giurgiu、Stanislaw Jastrzebski、Bruna Morrone、Quentin
    de Laroussilhe、Andrea Gesmundo、Mona Attariyan 和 Sylvain Gelly。用于自然语言处理的参数高效迁移学习。在*第36届国际机器学习会议论文集*，第97卷，页2790–2799，2019。
- en: Hu et al. (2023a) Jifeng Hu, Yanchao Sun, Sili Huang, Siyuan Guo, Hechang Chen,
    Li Shen, Lichao Sun, Yi Chang, and Dacheng Tao. Instructed diffuser with temporal
    condition guidance for offline reinforcement learning. *CoRR*, abs/2306.04875,
    2023a.
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2023a）Jifeng Hu、Yanchao Sun、Sili Huang、Siyuan Guo、Hechang Chen、Li Shen、Lichao
    Sun、Yi Chang 和 Dacheng Tao。带有时间条件指导的指令扩散器用于离线强化学习。*CoRR*，abs/2306.04875，2023a。
- en: Hu et al. (2023b) Shengchao Hu, Li Shen, Ya Zhang, and Dacheng Tao. Graph decision
    transformer. *CoRR*, abs/2303.03747, 2023b.
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2023b）Shengchao Hu、Li Shen、Ya Zhang 和 Dacheng Tao。图决策变换器。*CoRR*，abs/2303.03747，2023b。
- en: 'Hu et al. (2021) Siyi Hu, Fengda Zhu, Xiaojun Chang, and Xiaodan Liang. Updet:
    Universal multi-agent rl via policy decoupling with transformers. In *Proceedings
    of the 9th International Conference on Learning Representations*, 2021.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2021）Siyi Hu、Fengda Zhu、Xiaojun Chang 和 Xiaodan Liang。Updet：通过与变压器解耦的策略进行通用多智能体强化学习。在*第九届国际学习表征会议论文集*，2021。
- en: Huang et al. (2018) Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron C.
    Courville. Neural autoregressive flows. In *Proceedings of the 35th International
    Conference on Machine Learning*, volume 80 of *Proceedings of Machine Learning
    Research*, pp.  2083–2092\. PMLR, 2018.
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2018）Chin-Wei Huang、David Krueger、Alexandre Lacoste 和 Aaron C. Courville。神经自回归流。在*第35届国际机器学习会议论文集*，*机器学习研究论文集*第80卷，页2083–2092。PMLR，2018。
- en: Hwang et al. (2023) Dongyoon Hwang, Minho Park, and Jiyoung Lee. Sample generations
    for reinforcement learning via diffusion models. *OpenReview.net*, 2023.
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hwang 等（2023）Dongyoon Hwang、Minho Park 和 Jiyoung Lee。通过扩散模型进行强化学习的样本生成。*OpenReview.net*，2023。
- en: Hyvärinen (2005) Aapo Hyvärinen. Estimation of non-normalized statistical models
    by score matching. *Journal of Machine Learning Research*, 6:695–709, 2005.
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyvärinen (2005) Aapo Hyvärinen. 通过分数匹配估计非规范化统计模型。*Journal of Machine Learning
    Research*, 6:695–709, 2005。
- en: 'Jacobsen et al. (2018) Jörn-Henrik Jacobsen, Arnold W. M. Smeulders, and Edouard
    Oyallon. i-revnet: Deep invertible networks. In *Proceedings of the 6th International
    Conference on Learning Representations*, 2018.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jacobsen 等 (2018) Jörn-Henrik Jacobsen, Arnold W. M. Smeulders 和 Edouard Oyallon.
    i-revnet: 深度可逆网络。在 *Proceedings of the 6th International Conference on Learning
    Representations*，2018。'
- en: 'Jaegle et al. (2022) Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac,
    Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew
    Brock, Evan Shelhamer, Olivier J. Hénaff, Matthew M. Botvinick, Andrew Zisserman,
    Oriol Vinyals, and João Carreira. Perceiver IO: A general architecture for structured
    inputs & outputs. In *Proceedings of the 10th International Conference on Learning
    Representations*. OpenReview.net, 2022.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jaegle 等 (2022) Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl
    Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock,
    Evan Shelhamer, Olivier J. Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol
    Vinyals 和 João Carreira. Perceiver IO: 一种用于结构化输入和输出的通用架构。在 *Proceedings of the
    10th International Conference on Learning Representations*。OpenReview.net, 2022。'
- en: 'James et al. (2020) Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J.
    Davison. Rlbench: The robot learning benchmark & learning environment. *IEEE Robotics
    Automation Letters*, 5(2):3019–3026, 2020.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'James 等 (2020) Stephen James, Zicong Ma, David Rovick Arrojo 和 Andrew J. Davison.
    Rlbench: 机器人学习基准与学习环境。*IEEE Robotics Automation Letters*, 5(2):3019–3026, 2020。'
- en: 'Janner et al. (2019) Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.
    When to trust your model: Model-based policy optimization. In *Advances in Neural
    Information Processing Systems 32*, 2019.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Janner 等 (2019) Michael Janner, Justin Fu, Marvin Zhang 和 Sergey Levine. 何时信任你的模型：基于模型的策略优化。在
    *Advances in Neural Information Processing Systems 32*，2019。
- en: Janner et al. (2021) Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement
    learning as one big sequence modeling problem. In *Advances in Neural Information
    Processing Systems 34*, pp.  1273–1286, 2021.
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Janner 等 (2021) Michael Janner, Qiyang Li 和 Sergey Levine. 离线强化学习作为一个大的序列建模问题。在
    *Advances in Neural Information Processing Systems 34*，第 1273–1286 页，2021。
- en: Janner et al. (2022) Michael Janner, Yilun Du, Joshua B. Tenenbaum, and Sergey
    Levine. Planning with diffusion for flexible behavior synthesis. In *Proceedings
    of the 39th International Conference on Machine Learning*, volume 162, pp.  9902–9915\.
    PMLR, 2022.
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Janner 等 (2022) Michael Janner, Yilun Du, Joshua B. Tenenbaum 和 Sergey Levine.
    使用扩散规划进行灵活的行为合成。在 *Proceedings of the 39th International Conference on Machine
    Learning*，第 162 卷，第 9902–9915 页。PMLR, 2022。
- en: Jeon et al. (2018) Wonseok Jeon, Seokin Seo, and Kee-Eung Kim. A bayesian approach
    to generative adversarial imitation learning. In *Advances in Neural Information
    Processing Systems 31*, pp.  7440–7450, 2018.
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeon 等 (2018) Wonseok Jeon, Seokin Seo 和 Kee-Eung Kim. 基于贝叶斯的方法进行生成对抗模仿学习。在
    *Advances in Neural Information Processing Systems 31*，第 7440–7450 页，2018。
- en: 'Jiang et al. (2021) Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan:
    Two pure transformers can make one strong gan, and that can scale up. *Advances
    in Neural Information Processing Systems*, 34:14745–14758, 2021.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang 等 (2021) Yifan Jiang, Shiyu Chang 和 Zhangyang Wang. Transgan: 两个纯变换器可以生成一个强大的
    GAN，并且可以扩展。*Advances in Neural Information Processing Systems*, 34:14745–14758,
    2021。'
- en: Jing et al. (2021) Mingxuan Jing, Wenbing Huang, Fuchun Sun, Xiaojian Ma, Tao
    Kong, Chuang Gan, and Lei Li. Adversarial option-aware hierarchical imitation
    learning. In *Proceedings of the 38th International Conference on Machine Learning*,
    volume 139, pp.  5097–5106\. PMLR, 2021.
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jing 等 (2021) Mingxuan Jing, Wenbing Huang, Fuchun Sun, Xiaojian Ma, Tao Kong,
    Chuang Gan 和 Lei Li. 对抗性选项感知层次模仿学习。在 *Proceedings of the 38th International Conference
    on Machine Learning*，第 139 卷，第 5097–5106 页。PMLR, 2021。
- en: Jolicoeur-Martineau et al. (2021) Alexia Jolicoeur-Martineau, Rémi Piché-Taillefer,
    Ioannis Mitliagkas, and Remi Tachet des Combes. Adversarial score matching and
    improved sampling for image generation. In *Proceedings of the 9th International
    Conference on Learning Representations*. OpenReview.net, 2021.
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jolicoeur-Martineau 等 (2021) Alexia Jolicoeur-Martineau, Rémi Piché-Taillefer,
    Ioannis Mitliagkas 和 Remi Tachet des Combes. 对抗性分数匹配和改进的图像生成采样。在 *Proceedings
    of the 9th International Conference on Learning Representations*。OpenReview.net,
    2021。
- en: 'Kalyan et al. (2021) Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, and
    Sivanesan Sangeetha. AMMUS : A survey of transformer-based pretrained models in
    natural language processing. *CoRR*, abs/2108.05542, 2021.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kalyan 等 (2021) Katikapalli Subramanyam Kalyan, Ajit Rajasekharan 和 Sivanesan
    Sangeetha. AMMUS: 基于变换器的预训练模型在自然语言处理中的调查。*CoRR*, abs/2108.05542, 2021。'
- en: 'Kamath et al. (2023) Aishwarya Kamath, Peter Anderson, Su Wang, Jing Yu Koh,
    Alexander Ku, Austin Waters, Yinfei Yang, Jason Baldridge, and Zarana Parekh.
    A new path: Scaling vision-and-language navigation with synthetic instructions
    and imitation learning. In *IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pp.  10813–10823, 2023.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamath et al. (2023) Aishwarya Kamath, Peter Anderson, Su Wang, Jing Yu Koh,
    Alexander Ku, Austin Waters, Yinfei Yang, Jason Baldridge 和 Zarana Parekh. 一条新路径：通过合成指令和模仿学习扩展视觉-语言导航。发表于*IEEE/CVF
    计算机视觉与模式识别会议*，第 10813–10823 页，2023年。
- en: Kang et al. (2023) Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng
    Yan. Efficient diffusion policies for offline reinforcement learning. *CoRR*,
    abs/2305.20081, 2023.
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang et al. (2023) Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang 和 Shuicheng Yan.
    离线强化学习的高效扩散策略。*CoRR*，abs/2305.20081，2023年。
- en: Karras et al. (2022) Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
    Elucidating the design space of diffusion-based generative models. In *Advances
    in neural information processing systems 35*, 2022.
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras et al. (2022) Tero Karras, Miika Aittala, Timo Aila 和 Samuli Laine. 阐明基于扩散的生成模型的设计空间。发表于*神经信息处理系统进展
    35*，2022年。
- en: Khan et al. (2022) Muhammad Junaid Khan, Syed Hammad Ahmed, and Gita Sukthankar.
    Transformer-based value function decomposition for cooperative multi-agent reinforcement
    learning in starcraft. In *Proceedings of the 18th AAAI Conference on Artificial
    Intelligence and Interactive Digital Entertainment*, pp.  113–119\. AAAI Press,
    2022.
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan et al. (2022) Muhammad Junaid Khan, Syed Hammad Ahmed 和 Gita Sukthankar.
    基于 Transformer 的价值函数分解用于《星际争霸》的合作多智能体强化学习。发表于*第18届 AAAI 人工智能与互动数字娱乐会议论文集*，第 113–119
    页，AAAI出版社，2022年。
- en: Kim et al. (2021) Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi. Transformer-based
    deep imitation learning for dual-arm robot manipulation. In *IEEE/RSJ International
    Conference on Intelligent Robots and Systems*, pp.  8965–8972, 2021.
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2021) Heecheol Kim, Yoshiyuki Ohmura 和 Yasuo Kuniyoshi. 基于 Transformer
    的深度模仿学习用于双臂机器人操作。发表于*IEEE/RSJ 国际智能机器人与系统会议*，第 8965–8972 页，2021年。
- en: Kim et al. (2022) Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi. Memory-based
    gaze prediction in deep imitation learning for robot manipulation. In *International
    Conference on Robotics and Automation*, pp.  2427–2433\. IEEE, 2022.
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2022) Heecheol Kim, Yoshiyuki Ohmura 和 Yasuo Kuniyoshi. 基于记忆的注视预测在深度模仿学习中的应用用于机器人操作。发表于*国际机器人与自动化会议*，第
    2427–2433 页，IEEE，2022年。
- en: 'Kim et al. (2023) Heecheol Kim, Yoshiyuki Ohmura, Akihiko Nagakubo, and Yasuo
    Kuniyoshi. Training robots without robots: Deep imitation learning for master-to-robot
    policy transfer. *IEEE Robotics Automation Letters*, 8(5):2906–2913, 2023.'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2023) Heecheol Kim, Yoshiyuki Ohmura, Akihiko Nagakubo 和 Yasuo Kuniyoshi.
    无需机器人训练机器人：用于主到机器人策略转移的深度模仿学习。*IEEE 机器人自动化通讯*，8(5):2906–2913，2023年。
- en: Kingma & Gao (2023) Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives
    as the elbo with simple data augmentation. *Advances in Neural Information Processing
    Systems 36*, 36, 2023.
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma & Gao (2023) Diederik Kingma 和 Ruiqi Gao. 将扩散目标理解为带有简单数据增强的 ELBO。*神经信息处理系统进展
    36*，36，2023年。
- en: 'Kingma & Dhariwal (2018) Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative
    flow with invertible 1x1 convolutions. In *Advances in Neural Information Processing
    Systems 31*, pp.  10236–10245, 2018.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma & Dhariwal (2018) Diederik P. Kingma 和 Prafulla Dhariwal. Glow: 使用可逆
    1x1 卷积的生成流。发表于*神经信息处理系统进展 31*，第 10236–10245 页，2018年。'
- en: Kingma & Welling (2014) Diederik P. Kingma and Max Welling. Auto-encoding variational
    bayes. In *Proceedings of the 2nd International Conference on Learning Representations*,
    2014.
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma & Welling (2014) Diederik P. Kingma 和 Max Welling. 自动编码变分贝叶斯。发表于*第2届国际学习表征会议论文集*，2014年。
- en: Kingma & Welling (2019) Diederik P. Kingma and Max Welling. An introduction
    to variational autoencoders. *Foundations and Trends® in Machine Learning*, 12(4):307–392,
    2019.
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma & Welling (2019) Diederik P. Kingma 和 Max Welling. 变分自编码器简介。*机器学习基础与趋势®*，12(4):307–392，2019年。
- en: Kingma et al. (2016) Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen,
    Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive
    flow. *Advances in neural information processing systems*, 29, 2016.
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma et al. (2016) Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen,
    Ilya Sutskever 和 Max Welling. 使用逆自回归流改进变分推断。*神经信息处理系统进展*，29，2016年。
- en: 'Kipf et al. (2019) Thomas Kipf, Yujia Li, Hanjun Dai, Vinícius Flores Zambaldi,
    Alvaro Sanchez-Gonzalez, Edward Grefenstette, Pushmeet Kohli, and Peter W. Battaglia.
    Compile: Compositional imitation learning and execution. In *Proceedings of the
    36th International Conference on Machine Learning*, volume 97 of *Proceedings
    of Machine Learning Research*, pp.  3418–3428\. PMLR, 2019.'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf 等 (2019) Thomas Kipf, Yujia Li, Hanjun Dai, Vinícius Flores Zambaldi, Alvaro
    Sanchez-Gonzalez, Edward Grefenstette, Pushmeet Kohli, 和 Peter W. Battaglia. Compile：组合模仿学习与执行。载于
    *第36届国际机器学习会议论文集*，第97卷，*机器学习研究论文集*，第3418–3428页。PMLR，2019年。
- en: 'Kobyzev et al. (2021) Ivan Kobyzev, Simon J. D. Prince, and Marcus A. Brubaker.
    Normalizing flows: An introduction and review of current methods. *IEEE TRANSACTIONS
    ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE*, 43(11):3964–3979, 2021.'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kobyzev 等 (2021) Ivan Kobyzev, Simon J. D. Prince, 和 Marcus A. Brubaker. 正则化流：方法介绍与现状回顾。*IEEE模式分析与机器智能汇刊*，43(11)：3964–3979，2021年。
- en: Koga et al. (2022) Yotto Koga, Heather Kerrick, and Sachin Chitta. On CAD informed
    adaptive robotic assembly. In *IEEE/RSJ International Conference on Intelligent
    Robots and Systems*, pp.  10207–10214\. IEEE, 2022.
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koga 等 (2022) Yotto Koga, Heather Kerrick, 和 Sachin Chitta. 基于 CAD 的自适应机器人装配。载于
    *IEEE/RSJ国际智能机器人与系统会议*，第10207–10214页。IEEE，2022年。
- en: 'Kostrikov et al. (2019) Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi,
    Sergey Levine, and Jonathan Tompson. Discriminator-actor-critic: Addressing sample
    inefficiency and reward bias in adversarial imitation learning. In *Proceedings
    of the 7th International Conference on Learning Representations*. OpenReview.net,
    2019.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kostrikov 等 (2019) Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi,
    Sergey Levine, 和 Jonathan Tompson. 鉴别器-演员-评论家：解决对抗模仿学习中的样本低效和奖励偏差问题。载于 *第7届国际学习表征会议论文集*。OpenReview.net，2019年。
- en: Kostrikov et al. (2022) Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline
    reinforcement learning with implicit q-learning. In *Proceedings of the 10th International
    Conference on Learning Representations*. OpenReview.net, 2022.
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kostrikov 等 (2022) Ilya Kostrikov, Ashvin Nair, 和 Sergey Levine. 隐式 Q 学习的离线强化学习。载于
    *第10届国际学习表征会议论文集*。OpenReview.net，2022年。
- en: 'Krajewski et al. (2018) Robert Krajewski, Julian Bock, Laurent Kloeker, and
    Lutz Eckstein. The highd dataset: A drone dataset of naturalistic vehicle trajectories
    on german highways for validation of highly automated driving systems. In *Proceedings
    of the 21st International Conference on Intelligent Transportation Systems*, pp. 
    2118–2125\. IEEE, 2018.'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krajewski 等 (2018) Robert Krajewski, Julian Bock, Laurent Kloeker, 和 Lutz Eckstein.
    highd 数据集：用于验证高度自动驾驶系统的德国高速公路自然车辆轨迹的无人机数据集。载于 *第21届国际智能交通系统会议论文集*，第2118–2125页。IEEE，2018年。
- en: Kuefler & Kochenderfer (2018) Alex Kuefler and Mykel J. Kochenderfer. Burn-in
    demonstrations for multi-modal imitation learning. In *Proceedings of the 17th
    International Conference on Autonomous Agents and MultiAgent Systems*, pp.  1071–1078,
    2018.
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuefler & Kochenderfer (2018) Alex Kuefler 和 Mykel J. Kochenderfer. 多模态模仿学习的预热演示。载于
    *第17届国际自主代理与多代理系统会议论文集*，第1071–1078页，2018年。
- en: Kumar et al. (2019) Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and
    Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction.
    In *Advances in Neural Information Processing Systems 32*, pp.  11761–11771, 2019.
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等 (2019) Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, 和 Sergey
    Levine. 通过引导误差减少来稳定离策略 Q 学习。载于 *神经信息处理系统进展 32*，第11761–11771页，2019年。
- en: Kumar et al. (2020) Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
    Conservative q-learning for offline reinforcement learning. In *Advances in Neural
    Information Processing Systems 33*, 2020.
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等 (2020) Aviral Kumar, Aurick Zhou, George Tucker, 和 Sergey Levine. 离线强化学习中的保守
    Q 学习。载于 *神经信息处理系统进展 33*，2020年。
- en: Lacotte et al. (2019) Jonathan Lacotte, Mohammad Ghavamzadeh, Yinlam Chow, and
    Marco Pavone. Risk-sensitive generative adversarial imitation learning. In *Proceedings
    of the 22nd International Conference on Artificial Intelligence and Statistics*,
    volume 89, pp.  2154–2163, 2019.
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lacotte 等 (2019) Jonathan Lacotte, Mohammad Ghavamzadeh, Yinlam Chow, 和 Marco
    Pavone. 风险敏感生成对抗模仿学习。载于 *第22届国际人工智能与统计学会议论文集*，第89卷，第2154–2163页，2019年。
- en: Lee et al. (2022) Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel
    Freeman, Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski,
    and Igor Mordatch. Multi-game decision transformers. In *Advances in Neural Information
    Processing Systems 35*, 2022.
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 (2022) Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman,
    Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski 和 Igor
    Mordatch。多游戏决策变换器。发表于 *Advances in Neural Information Processing Systems 35*，2022年。
- en: Lee et al. (2019) Su-Jin Lee, Tae Yoon Chun, Hyoung Woo Lim, and Sang-Ho Lee.
    Path tracking control using imitation learning with variational auto-encoder.
    In *Proceedings of the 19th International Conference on Control, Automation and
    Systems*, pp.  501–505\. IEEE, 2019.
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 (2019) Su-Jin Lee, Tae Yoon Chun, Hyoung Woo Lim 和 Sang-Ho Lee。使用变分自编码器的模仿学习路径跟踪控制。发表于
    *Proceedings of the 19th International Conference on Control, Automation and Systems*，第
    501–505 页。IEEE，2019年。
- en: 'Levine (2018) Sergey Levine. Reinforcement learning and control as probabilistic
    inference: Tutorial and review. *arXiv preprint arXiv:1805.00909*, 2018.'
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levine (2018) Sergey Levine。将强化学习和控制视为概率推理：教程和综述。*arXiv 预印本 arXiv:1805.00909*，2018年。
- en: 'Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin
    Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open
    problems. *CoRR*, abs/2005.01643, 2020.'
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levine 等 (2020) Sergey Levine, Aviral Kumar, George Tucker 和 Justin Fu。离线强化学习：教程、综述以及对开放问题的观点。*CoRR*，abs/2005.01643，2020年。
- en: Li et al. (2017a) Chongxuan Li, Taufik Xu, Jun Zhu, and Bo Zhang. Triple generative
    adversarial nets. In *Advances in Neural Information Processing Systems 30*, pp. 
    4088–4098, 2017a.
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2017a) Chongxuan Li, Taufik Xu, Jun Zhu 和 Bo Zhang。三重生成对抗网络。发表于 *Advances
    in Neural Information Processing Systems 30*，第 4088–4098 页，2017年。
- en: Li et al. (2022) Jinning Li, Chen Tang, Masayoshi Tomizuka, and Wei Zhan. Hierarchical
    planning through goal-conditioned offline reinforcement learning. *IEEE Robotics
    Automation Letters*, 7(4):10216–10223, 2022.
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2022) Jinning Li, Chen Tang, Masayoshi Tomizuka 和 Wei Zhan。通过目标条件离线强化学习进行层次规划。*IEEE
    Robotics Automation Letters*，7(4):10216–10223，2022年。
- en: Li et al. (2023a) Tao Li, Juan Guevara, Xinghong Xie, and Quanyan Zhu. Self-confirming
    transformer for locally consistent online adaptation in multi-agent reinforcement
    learning. *CoRR*, abs/2310.04579, 2023a.
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023a) Tao Li, Juan Guevara, Xinghong Xie 和 Quanyan Zhu。用于多智能体强化学习中的局部一致性在线适应的自我确认变换器。*CoRR*，abs/2310.04579，2023年。
- en: Li et al. (2023b) Wenhao Li, Xiangfeng Wang, Bo Jin, and Hongyuan Zha. Hierarchical
    diffusion for offline decision making. In *Proceedings of the 40th International
    Conference on Machine Learning*, volume 202, pp.  20035–20064, 2023b.
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023b) Wenhao Li, Xiangfeng Wang, Bo Jin 和 Hongyuan Zha。用于离线决策制定的层次扩散。发表于
    *Proceedings of the 40th International Conference on Machine Learning*，卷 202，第
    20035–20064 页，2023年。
- en: 'Li et al. (2017b) Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable
    imitation learning from visual demonstrations. In *Advances in Neural Information
    Processing Systems 30*, pp.  3812–3822, 2017b.'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2017b) Yunzhu Li, Jiaming Song 和 Stefano Ermon。Infogail：来自视觉演示的可解释模仿学习。发表于
    *Advances in Neural Information Processing Systems 30*，第 3812–3822 页，2017年。
- en: 'Li et al. (2023c) Zhuoran Li, Ling Pan, and Longbo Huang. Beyond conservatism:
    Diffusion policies in offline multi-agent reinforcement learning. *CoRR*, abs/2307.01472,
    2023c.'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023c) Zhuoran Li, Ling Pan 和 Longbo Huang。超越保守主义：离线多智能体强化学习中的扩散策略。*CoRR*，abs/2307.01472，2023年。
- en: Liang et al. (2022) Anthony Liang, Ishika Singh, Karl Pertsch, and Jesse Thomason.
    Transformer adapters for robot learning. In *CoRL 2022 Workshop on Pre-training
    Robot Learning*, 2022. URL [https://openreview.net/forum?id=H--wvRYBmF](https://openreview.net/forum?id=H--wvRYBmF).
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等 (2022) Anthony Liang, Ishika Singh, Karl Pertsch 和 Jesse Thomason。用于机器人学习的变换器适配器。发表于
    *CoRL 2022 Workshop on Pre-training Robot Learning*，2022年。网址 [https://openreview.net/forum?id=H--wvRYBmF](https://openreview.net/forum?id=H--wvRYBmF)。
- en: Liang et al. (2023a) Hebin Liang, Zibin Dong, Yi Ma, Xiaotian Hao, Yan Zheng,
    and Jianye Hao. A hierarchical imitation learning-based decision framework for
    autonomous driving. In *Proceedings of the 32nd ACM International Conference on
    Information and Knowledge Management*, pp.  4695–4701\. ACM, 2023a.
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等 (2023a) Hebin Liang, Zibin Dong, Yi Ma, Xiaotian Hao, Yan Zheng 和 Jianye
    Hao。基于层次模仿学习的自主驾驶决策框架。发表于 *Proceedings of the 32nd ACM International Conference
    on Information and Knowledge Management*，第 4695–4701 页。ACM，2023年。
- en: 'Liang et al. (2023b) Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi
    Tomizuka, and Ping Luo. Adaptdiffuser: Diffusion models as adaptive self-evolving
    planners. In *Proceedings of the 40th International Conference on Machine Learning*,
    volume 202, pp.  20725–20745, 2023b.'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等 (2023b) Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka
    和 Ping Luo。Adaptdiffuser：将扩散模型作为自适应自我进化规划器。发表于 *Proceedings of the 40th International
    Conference on Machine Learning*，卷 202，第 20725–20745 页，2023年。
- en: Lillicrap et al. (2016) Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel,
    Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous
    control with deep reinforcement learning. In *Proceedings of the 4th International
    Conference on Learning Representations*, 2016.
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lillicrap 等 (2016) Timothy P. Lillicrap、Jonathan J. Hunt、Alexander Pritzel、Nicolas
    Heess、Tom Erez、Yuval Tassa、David Silver 和 Daan Wierstra. 使用深度强化学习进行连续控制。见 *Proceedings
    of the 4th International Conference on Learning Representations*，2016。
- en: Lin et al. (2022a) Qinjie Lin, Han Liu, and Biswa Sengupta. Switch trajectory
    transformer with distributional value approximation for multi-task reinforcement
    learning. *CoRR*, abs/2203.07413, 2022a.
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 (2022a) Qinjie Lin、Han Liu 和 Biswa Sengupta. 带有分布值近似的切换轨迹变压器用于多任务强化学习。*CoRR*，abs/2203.07413，2022a。
- en: Lin et al. (2022b) Runji Lin, Ye Li, Xidong Feng, Zhaowei Zhang, Xian Hong Wu
    Fung, Haifeng Zhang, Jun Wang, Yali Du, and Yaodong Yang. Contextual transformer
    for offline meta reinforcement learning. *CoRR*, abs/2211.08016, 2022b.
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 (2022b) Runji Lin、Ye Li、Xidong Feng、Zhaowei Zhang、Xian Hong Wu Fung、Haifeng
    Zhang、Jun Wang、Yali Du 和 Yaodong Yang. 离线元强化学习的上下文变压器。*CoRR*，abs/2211.08016，2022b。
- en: Lin et al. (2022c) Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu.
    A survey of transformers. *AI Open*, 2022c.
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 (2022c) Tianyang Lin、Yuxin Wang、Xiangyang Liu 和 Xipeng Qiu. 变压器调查。*AI
    Open*，2022c。
- en: Liu et al. (2020) Fangchen Liu, Zhan Ling, Tongzhou Mu, and Hao Su. State alignment-based
    imitation learning. In *Proceedings of the 8th International Conference on Learning
    Representations*. OpenReview.net, 2020.
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2020) Fangchen Liu、Zhan Ling、Tongzhou Mu 和 Hao Su. 基于状态对齐的模仿学习。见 *Proceedings
    of the 8th International Conference on Learning Representations*。OpenReview.net，2020。
- en: 'Liu et al. (2023a) Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang,
    Jing Zhang, and Jie Tang. Self-supervised learning: Generative or contrastive.
    *IEEE Transactions on Knowledge and Data Engineering*, 35(1):857–876, 2023a.'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2023a) Xiao Liu、Fanjin Zhang、Zhenyu Hou、Li Mian、Zhaoyu Wang、Jing Zhang
    和 Jie Tang. 自监督学习：生成性还是对比性。*IEEE Transactions on Knowledge and Data Engineering*，35(1)：857–876，2023a。
- en: Liu et al. (2023b) Zuxin Liu, Zijian Guo, Yihang Yao, Zhepeng Cen, Wenhao Yu,
    Tingnan Zhang, and Ding Zhao. Constrained decision transformer for offline safe
    reinforcement learning. In *Proceedings of the 40th International Conference on
    Machine Learning*, volume 202, pp.  21611–21630, 2023b.
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2023b) Zuxin Liu、Zijian Guo、Yihang Yao、Zhepeng Cen、Wenhao Yu、Tingnan
    Zhang 和 Ding Zhao. 用于离线安全强化学习的受限决策变压器。见 *Proceedings of the 40th International
    Conference on Machine Learning*，卷 202，第 21611–21630 页，2023b。
- en: Locatello et al. (2019) Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar
    Rätsch, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. Challenging common
    assumptions in the unsupervised learning of disentangled representations. In *Proceedings
    of the 36th International Conference on Machine Learning*, volume 97, pp.  4114–4124,
    2019.
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Locatello 等 (2019) Francesco Locatello、Stefan Bauer、Mario Lucic、Gunnar Rätsch、Sylvain
    Gelly、Bernhard Schölkopf 和 Olivier Bachem. 挑战无监督学习中对解缠表示的常见假设。见 *Proceedings of
    the 36th International Conference on Machine Learning*，卷 97，第 4114–4124 页，2019。
- en: Lowe et al. (2017) Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and
    Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments.
    In *Advances in Neural Information Processing Systems 30*, pp.  6379–6390, 2017.
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowe 等 (2017) Ryan Lowe、Yi Wu、Aviv Tamar、Jean Harb、Pieter Abbeel 和 Igor Mordatch.
    多智能体演员-评论家方法用于混合合作-竞争环境。见 *Advances in Neural Information Processing Systems 30*，第
    6379–6390 页，2017。
- en: Loynd et al. (2020) Ricky Loynd, Roland Fernandez, Asli Celikyilmaz, Adith Swaminathan,
    and Matthew J. Hausknecht. Working memory graphs. In *Proceedings of the 37th
    International Conference on Machine Learning*, volume 119, pp.  6404–6414, 2020.
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loynd 等 (2020) Ricky Loynd、Roland Fernandez、Asli Celikyilmaz、Adith Swaminathan
    和 Matthew J. Hausknecht. 工作记忆图。见 *Proceedings of the 37th International Conference
    on Machine Learning*，卷 119，第 6404–6414 页，2020。
- en: 'Lu et al. (2022) Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li,
    and Jun Zhu. Dpm-solver: A fast ODE solver for diffusion probabilistic model sampling
    in around 10 steps. In *Advances in Neural Information Processing Systems 35*,
    2022.'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等 (2022) Cheng Lu、Yuhao Zhou、Fan Bao、Jianfei Chen、Chongxuan Li 和 Jun Zhu.
    Dpm-solver：一种快速的 ODE 求解器，用于在大约 10 步内对扩散概率模型进行采样。见 *Advances in Neural Information
    Processing Systems 35*，2022。
- en: Lu et al. (2023) Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li,
    and Jun Zhu. Contrastive energy prediction for exact energy-guided diffusion sampling
    in offline reinforcement learning. In *Proceedings of the 40th International Conference
    on Machine Learning*, volume 202, pp.  22825–22855\. PMLR, 2023.
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等 (2023) Cheng Lu、Huayu Chen、Jianfei Chen、Hang Su、Chongxuan Li 和 Jun Zhu.
    用于离线强化学习的对比能量预测，以实现精确的能量引导扩散采样。见 *Proceedings of the 40th International Conference
    on Machine Learning*，卷 202，第 22825–22855 页。PMLR，2023。
- en: Lu et al. (2019) Xiaoyu Lu, Jan Stuehmer, and Katja Hofmann. Trajectory VAE
    for multi-modal imitation, 2019. URL [https://openreview.net/forum?id=Byx1VnR9K7](https://openreview.net/forum?id=Byx1VnR9K7).
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. (2019) Xiaoyu Lu、Jan Stuehmer 和 Katja Hofmann. 多模态模仿的轨迹VAE，2019年。网址
    [https://openreview.net/forum?id=Byx1VnR9K7](https://openreview.net/forum?id=Byx1VnR9K7)。
- en: 'Lu & Tompson (2020) Yiren Lu and Jonathan Tompson. ADAIL: adaptive adversarial
    imitation learning. *CoRR*, abs/2008.12647, 2020.'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu & Tompson (2020) Yiren Lu 和 Jonathan Tompson. ADAIL：自适应对抗性模仿学习。*CoRR*，abs/2008.12647，2020年。
- en: Luo et al. (2023) Jianlan Luo, Perry Dong, Jeffrey Wu, Aviral Kumar, Xinyang
    Geng, and Sergey Levine. Action-quantized offline reinforcement learning for robotic
    skill learning. In *Conference on Robot Learning*, pp.  1348–1361\. PMLR, 2023.
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2023) Jianlan Luo、Perry Dong、Jeffrey Wu、Aviral Kumar、Xinyang Geng
    和 Sergey Levine. 动作量化的离线强化学习用于机器人技能学习。见于 *机器人学习会议*，第1348–1361页，PMLR，2023年。
- en: 'Lynch & Sermanet (2021) Corey Lynch and Pierre Sermanet. Language conditioned
    imitation learning over unstructured data. In *Robotics: Science and Systems XVII,
    Virtual Event, July 12-16, 2021*, 2021.'
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lynch & Sermanet (2021) Corey Lynch 和 Pierre Sermanet. 语言条件模仿学习在非结构化数据上的应用。见于
    *机器人：科学与系统 XVII，虚拟会议，2021年7月12-16日*，2021年。
- en: Lynch et al. (2019) Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan
    Tompson, Sergey Levine, and Pierre Sermanet. Learning latent plans from play.
    In *Proceedings of the 3rd Annual Conference on Robot Learning*, volume 100, pp. 
    1113–1132, 2019.
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lynch et al. (2019) Corey Lynch、Mohi Khansari、Ted Xiao、Vikash Kumar、Jonathan
    Tompson、Sergey Levine 和 Pierre Sermanet. 从游戏中学习潜在计划。见于 *第3届年度机器人学习会议论文集*，卷100，第1113–1132页，2019年。
- en: Lyu et al. (2022) Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative
    q-learning for offline reinforcement learning. In *Advances in Neural Information
    Processing Systems 35*, 2022.
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lyu et al. (2022) Jiafei Lyu、Xiaoteng Ma、Xiu Li 和 Zongqing Lu. 温和保守的q学习用于离线强化学习。见于
    *神经信息处理系统进展 35*，2022年。
- en: Mandlekar et al. (2021a) Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany,
    Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto
    Martín-Martín. What matters in learning from offline human demonstrations for
    robot manipulation. In *Conference on Robot Learning*, volume 164, pp.  1678–1690,
    2021a.
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mandlekar et al. (2021a) Ajay Mandlekar、Danfei Xu、Josiah Wong、Soroush Nasiriany、Chen
    Wang、Rohun Kulkarni、Li Fei-Fei、Silvio Savarese、Yuke Zhu 和 Roberto Martín-Martín.
    从离线人类演示中学习的关键因素。见于 *机器人学习会议*，卷164，第1678–1690页，2021年。
- en: Mandlekar et al. (2021b) Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany,
    Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto
    Martín-Martín. What matters in learning from offline human demonstrations for
    robot manipulation. *arXiv preprint arXiv:2108.03298*, 2021b.
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mandlekar et al. (2021b) Ajay Mandlekar、Danfei Xu、Josiah Wong、Soroush Nasiriany、Chen
    Wang、Rohun Kulkarni、Li Fei-Fei、Silvio Savarese、Yuke Zhu 和 Roberto Martín-Martín.
    从离线人类演示中学习的关键因素。*arXiv 预印本 arXiv:2108.03298*，2021年。
- en: 'Manuelli et al. (2020) Lucas Manuelli, Yunzhu Li, Peter R. Florence, and Russ
    Tedrake. Keypoints into the future: Self-supervised correspondence in model-based
    reinforcement learning. In *Proceedings of the 4th Conference on Robot Learning*,
    volume 155, pp.  693–710, 2020.'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Manuelli et al. (2020) Lucas Manuelli、Yunzhu Li、Peter R. Florence 和 Russ Tedrake.
    关键点的未来：模型基强化学习中的自监督对应。见于 *第4届机器人学习会议论文集*，卷155，第693–710页，2020年。
- en: Mao et al. (2017) Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, Zhen Wang,
    and Stephen Paul Smolley. Least squares generative adversarial networks. In *IEEE
    International Conference on Computer Vision*, 2017.
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao et al. (2017) Xudong Mao、Qing Li、Haoran Xie、Raymond Y. K. Lau、Zhen Wang
    和 Stephen Paul Smolley. 最小二乘生成对抗网络。见于 *IEEE国际计算机视觉会议*，2017年。
- en: 'Margolis & Agrawal (2022) Gabriel Margolis and Pulkit Agrawal. Walk these ways:
    Gait-conditioned policies yield diversified quadrupedal agility. In *Conference
    on Robot Learning*, volume 1, pp.  2, 2022.'
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Margolis & Agrawal (2022) Gabriel Margolis 和 Pulkit Agrawal. 沿这些路径行走：步态条件策略产生多样化的四足灵活性。见于
    *机器人学习会议*，卷1，第2页，2022年。
- en: 'Masoudnia & Ebrahimpour (2014) Saeed Masoudnia and Reza Ebrahimpour. Mixture
    of experts: a literature survey. *Artificial Intelligence Review*, 42:275–293,
    2014.'
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Masoudnia & Ebrahimpour (2014) Saeed Masoudnia 和 Reza Ebrahimpour. 专家混合模型：文献综述。*人工智能评论*，42:275–293，2014年。
- en: Massey et al. (1990) James Massey et al. Causality, feedback and directed information.
    In *The International Symposium on Information Theory and Its Applications*, pp. 
    303–305, 1990.
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Massey et al. (1990) James Massey 等人。因果性、反馈与有向信息。见于 *国际信息理论与应用研讨会*，第303–305页，1990年。
- en: Mayne & Michalska (1988) David Q Mayne and Hannah Michalska. Receding horizon
    control of nonlinear systems. In *Proceedings of the 27th IEEE Conference on Decision
    and Control*, pp.  464–465\. IEEE, 1988.
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mayne & Michalska (1988) David Q Mayne 和 Hannah Michalska. 非线性系统的递归视野控制。载于 *第27届IEEE决策与控制会议论文集*，第464–465页。IEEE，1988年。
- en: Mazoure et al. (2019) Bogdan Mazoure, Thang Doan, Audrey Durand, Joelle Pineau,
    and R. Devon Hjelm. Leveraging exploration in off-policy algorithms via normalizing
    flows. In *Proceedings of the 3rd Annual Conference on Robot Learning*, volume
    100 of *Proceedings of Machine Learning Research*, pp.  430–444\. PMLR, 2019.
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mazoure et al. (2019) Bogdan Mazoure、Thang Doan、Audrey Durand、Joelle Pineau
    和 R. Devon Hjelm。通过归一化流利用探索在离线策略算法中的作用。载于 *第3届年度机器人学习会议论文集*，第100卷，*机器学习研究论文集*，第430–444页。PMLR，2019年。
- en: Mees et al. (2022a) Oier Mees, Lukás Hermann, and Wolfram Burgard. What matters
    in language conditioned robotic imitation learning over unstructured data. *IEEE
    Robotics Automation Letters*, 7(4):11205–11212, 2022a.
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mees et al. (2022a) Oier Mees、Lukás Hermann 和 Wolfram Burgard。在非结构化数据上的语言条件机器人模仿学习中重要的因素。*IEEE机器人自动化通讯*，7(4)：11205–11212，2022a年。
- en: 'Mees et al. (2022b) Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram
    Burgard. Calvin: A benchmark for language-conditioned policy learning for long-horizon
    robot manipulation tasks. *IEEE Robotics and Automation Letters*, 7(3):7327–7334,
    2022b.'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mees et al. (2022b) Oier Mees、Lukas Hermann、Erick Rosete-Beas 和 Wolfram Burgard。Calvin：用于长时间机器人操作任务的语言条件政策学习基准。*IEEE机器人与自动化通讯*，7(3)：7327–7334，2022b年。
- en: Menéndez et al. (1997) ML Menéndez, JA Pardo, L Pardo, and MC Pardo. The jensen-shannon
    divergence. *Journal of the Franklin Institute*, 334(2):307–318, 1997.
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Menéndez et al. (1997) ML Menéndez、JA Pardo、L Pardo 和 MC Pardo。詹森-香农散度。*富兰克林学院期刊*，334(2)：307–318，1997年。
- en: 'Meng et al. (2021) Linghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun
    Li, Weinan Zhang, Ying Wen, Haifeng Zhang, Jun Wang, and Bo Xu. Offline pre-trained
    multi-agent decision transformer: One big sequence model tackles all SMAC tasks.
    *CoRR*, abs/2112.02845, 2021.'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng et al. (2021) Linghui Meng、Muning Wen、Yaodong Yang、Chenyang Le、Xiyun Li、Weinan
    Zhang、Ying Wen、Haifeng Zhang、Jun Wang 和 Bo Xu。离线预训练多智能体决策变换器：一个大规模序列模型解决所有SMAC任务。*CoRR*，abs/2112.02845，2021年。
- en: 'Meyer et al. (2019) Gregory P Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-Gonzalez,
    and Carl K Wellington. Lasernet: An efficient probabilistic 3d object detector
    for autonomous driving. In *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition*, pp.  12677–12686, 2019.'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meyer et al. (2019) Gregory P Meyer、Ankit Laddha、Eric Kee、Carlos Vallespi-Gonzalez
    和 Carl K Wellington。Lasernet：一种高效的概率性三维物体检测器，用于自动驾驶。载于 *IEEE/CVF计算机视觉与模式识别会议论文集*，第12677–12686页，2019年。
- en: Mirza & Osindero (2014) Mehdi Mirza and Simon Osindero. Conditional generative
    adversarial nets. *arXiv preprint arXiv:1411.1784*, 2014.
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirza & Osindero (2014) Mehdi Mirza 和 Simon Osindero。条件生成对抗网络。*arXiv预印本 arXiv:1411.1784*，2014年。
- en: Mitchell et al. (2021) Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey
    Levine, and Chelsea Finn. Offline meta-reinforcement learning with advantage weighting.
    In *Proceedings of the 38th International Conference on Machine Learning*, volume
    139, pp.  7780–7791, 2021.
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell et al. (2021) Eric Mitchell、Rafael Rafailov、Xue Bin Peng、Sergey Levine
    和 Chelsea Finn。具有优势加权的离线元强化学习。载于 *第38届国际机器学习大会论文集*，第139卷，第7780–7791页，2021年。
- en: Mohamed & Rezende (2015) Shakir Mohamed and Danilo Jimenez Rezende. Variational
    information maximisation for intrinsically motivated reinforcement learning. In
    *Advances in Neural Information Processing Systems 28*, pp.  2125–2133, 2015.
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohamed & Rezende (2015) Shakir Mohamed 和 Danilo Jimenez Rezende。用于内在动机强化学习的变分信息最大化。载于
    *神经信息处理系统进展28*，第2125–2133页，2015年。
- en: 'Mu et al. (2021) Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Yang, Xuanlin Li,
    Stone Tao, Zhiao Huang, Zhiwei Jia, and Hao Su. Maniskill: Generalizable manipulation
    skill benchmark with large-scale demonstrations. In *Proceedings of the Neural
    Information Processing Systems Track on Datasets and Benchmarks 1*, 2021.'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu et al. (2021) Tongzhou Mu、Zhan Ling、Fanbo Xiang、Derek Yang、Xuanlin Li、Stone
    Tao、Zhiao Huang、Zhiwei Jia 和 Hao Su。Maniskill：具有大规模示例的通用操作技能基准。载于 *神经信息处理系统数据集和基准轨道论文集1*，2021年。
- en: Nair et al. (2020) Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
    Accelerating online reinforcement learning with offline datasets. *CoRR*, abs/2006.09359,
    2020.
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair et al. (2020) Ashvin Nair、Murtaza Dalal、Abhishek Gupta 和 Sergey Levine。利用离线数据集加速在线强化学习。*CoRR*，abs/2006.09359，2020年。
- en: Nasiriany et al. (2022) Soroush Nasiriany, Tian Gao, Ajay Mandlekar, and Yuke
    Zhu. Learning and retrieval from prior data for skill-based imitation learning.
    In *Proceedings of the 6th Annual Conference on Robot Learning*, 2022.
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nasiriany 等人（2022）**Soroush Nasiriany**、**Tian Gao**、**Ajay Mandlekar** 和 **Yuke
    Zhu**。从先前数据中学习和检索用于基于技能的模仿学习。载于 *第6届机器人学习年会论文集*，2022年。
- en: 'Ng et al. (1999) Andrew Y. Ng, Daishi Harada, and Stuart Russell. Policy invariance
    under reward transformations: Theory and application to reward shaping. In *Proceedings
    of the 16th International Conference on Machine Learning*, pp.  278–287, 1999.'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ng 等人（1999）**Andrew Y. Ng**、**Daishi Harada** 和 **Stuart Russell**。奖励变换下的策略不变性：理论及其在奖励塑形中的应用。载于
    *第16届国际机器学习大会论文集*，第 278–287 页，1999年。
- en: 'Ni et al. (2023) Fei Ni, Jianye Hao, Yao Mu, Yifu Yuan, Yan Zheng, Bin Wang,
    and Zhixuan Liang. Metadiffuser: Diffusion model as conditional planner for offline
    meta-rl. In *Proceedings of the 40th International Conference on Machine Learning*,
    volume 202, pp.  26087–26105, 2023.'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ni 等人（2023）**Fei Ni**、**Jianye Hao**、**Yao Mu**、**Yifu Yuan**、**Yan Zheng**、**Bin
    Wang** 和 **Zhixuan Liang**。Metadiffuser：作为条件规划器的扩散模型用于离线元强化学习。载于 *第40届国际机器学习大会论文集*，第202卷，第
    26087–26105 页，2023年。
- en: 'Ni et al. (2020) Tianwei Ni, Harshit S. Sikchi, Yufei Wang, Tejus Gupta, Lisa
    Lee, and Ben Eysenbach. f-irl: Inverse reinforcement learning via state marginal
    matching. In *Proceedings of the 4th Conference on Robot Learning*, volume 155,
    pp.  529–551, 2020.'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ni 等人（2020）**Tianwei Ni**、**Harshit S. Sikchi**、**Yufei Wang**、**Tejus Gupta**、**Lisa
    Lee** 和 **Ben Eysenbach**。f-irl：通过状态边际匹配的逆强化学习。载于 *第4届机器人学习大会论文集*，第155卷，第 529–551
    页，2020年。
- en: Nichol & Dhariwal (2021) Alexander Quinn Nichol and Prafulla Dhariwal. Improved
    denoising diffusion probabilistic models. In *Proceedings of the 38th International
    Conference on Machine Learning*, volume 139, pp.  8162–8171\. PMLR, 2021.
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nichol & Dhariwal（2021）**Alexander Quinn Nichol** 和 **Prafulla Dhariwal**。改进的去噪扩散概率模型。载于
    *第38届国际机器学习大会论文集*，第139卷，第 8162–8171 页。PMLR，2021年。
- en: 'Nielsen et al. (2020) Didrik Nielsen, Priyank Jaini, Emiel Hoogeboom, Ole Winther,
    and Max Welling. Survae flows: Surjections to bridge the gap between vaes and
    flows. In *Advances in Neural Information Processing Systems 33*, 2020.'
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nielsen 等人（2020）**Didrik Nielsen**、**Priyank Jaini**、**Emiel Hoogeboom**、**Ole
    Winther** 和 **Max Welling**。Survae flows：用于弥合 VAE 和 flows 之间差距的映射。载于 *第33届神经信息处理系统进展*，2020年。
- en: Noseworthy et al. (2019) Michael D. Noseworthy, Rohan Paul, Subhro Roy, Daehyung
    Park, and Nicholas Roy. Task-conditioned variational autoencoders for learning
    movement primitives. In *Proceedings of the 3rd Annual Conference on Robot Learning*,
    volume 100 of *Proceedings of Machine Learning Research*, pp.  933–944\. PMLR,
    2019.
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Noseworthy 等人（2019）**Michael D. Noseworthy**、**Rohan Paul**、**Subhro Roy**、**Daehyung
    Park** 和 **Nicholas Roy**。面向任务的变分自编码器用于学习运动原型。载于 *第3届机器人学习年会论文集*，*机器学习研究论文集*第100卷，第
    933–944 页。PMLR，2019年。
- en: 'Nowozin et al. (2016) Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan:
    Training generative neural samplers using variational divergence minimization.
    In *Advances in Neural Information Processing Systems 29*, pp.  271–279, 2016.'
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nowozin 等人（2016）**Sebastian Nowozin**、**Botond Cseke** 和 **Ryota Tomioka**。f-gan：使用变分散度最小化训练生成神经采样器。载于
    *第29届神经信息处理系统进展*，第 271–279 页，2016年。
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023. URL [https://arxiv.org/pdf/2303.08774.pdf](https://arxiv.org/pdf/2303.08774.pdf).
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）**OpenAI**。GPT-4 技术报告，2023年。网址 [https://arxiv.org/pdf/2303.08774.pdf](https://arxiv.org/pdf/2303.08774.pdf)。
- en: 'Pan et al. (2022) Yingwei Pan, Yehao Li, Yiheng Zhang, Qi Cai, Fuchen Long,
    Zhaofan Qiu, Ting Yao, and Tao Mei. Silver-bullet-3d at maniskill 2021: Learning-from-demonstrations
    and heuristic rule-based methods for object manipulation. In *ICLR 2022 Workshop
    on Generalizable Policy Learning in Physical World*, 2022.'
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等人（2022）**Yingwei Pan**、**Yehao Li**、**Yiheng Zhang**、**Qi Cai**、**Fuchen
    Long**、**Zhaofan Qiu**、**Ting Yao** 和 **Tao Mei**。Silver-bullet-3d 在 Maniskill
    2021：基于演示学习和启发式规则的方法用于物体操作。载于 *ICLR 2022 物理世界中可推广政策学习研讨会*，2022年。
- en: 'Pandey et al. (2022) Kushagra Pandey, Avideep Mukherjee, Piyush Rai, and Abhishek
    Kumar. DiffuseVAE: Efficient, controllable and high-fidelity generation from low-dimensional
    latents. *Transactions on Machine Learning Research*, 2022. ISSN 2835-8856. URL
    [https://openreview.net/forum?id=ygoNPRiLxw](https://openreview.net/forum?id=ygoNPRiLxw).'
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandey 等人（2022）**Kushagra Pandey**、**Avideep Mukherjee**、**Piyush Rai** 和 **Abhishek
    Kumar**。DiffuseVAE：高效、可控且高保真度的低维潜在变量生成。*机器学习研究交易*，2022年。ISSN 2835-8856。网址 [https://openreview.net/forum?id=ygoNPRiLxw](https://openreview.net/forum?id=ygoNPRiLxw)。
- en: Papamakarios et al. (2017) George Papamakarios, Iain Murray, and Theo Pavlakou.
    Masked autoregressive flow for density estimation. In *Advances in Neural Information
    Processing Systems 30*, pp.  2338–2347, 2017.
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papamakarios et al. (2017) George Papamakarios, Iain Murray, 和 Theo Pavlakou.
    用于密度估计的掩码自回归流。收录于 *Advances in Neural Information Processing Systems 30*，第2338–2347页，2017年。
- en: Parisotto & Salakhutdinov (2021) Emilio Parisotto and Ruslan Salakhutdinov.
    Efficient transformers in reinforcement learning using actor-learner distillation.
    In *Proceedings of the 9th International Conference on Learning Representations*.
    OpenReview.net, 2021.
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parisotto & Salakhutdinov (2021) Emilio Parisotto 和 Ruslan Salakhutdinov. 使用演员-学习者蒸馏的强化学习高效变换器。收录于
    *Proceedings of the 9th International Conference on Learning Representations*。OpenReview.net，2021年。
- en: Parisotto et al. (2020) Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan
    Pascanu, Çaglar Gülçehre, Siddhant M. Jayakumar, Max Jaderberg, Raphaël Lopez
    Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia
    Hadsell. Stabilizing transformers for reinforcement learning. In *Proceedings
    of the 37th International Conference on Machine Learning*, volume 119, pp.  7487–7498\.
    PMLR, 2020.
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parisotto et al. (2020) Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan
    Pascanu, Çaglar Gülçehre, Siddhant M. Jayakumar, Max Jaderberg, Raphaël Lopez
    Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, 和 Raia Hadsell.
    为强化学习稳定化变换器。收录于 *Proceedings of the 37th International Conference on Machine Learning*，卷119，第7487–7498页。PMLR，2020年。
- en: Park et al. (2021) Jongjin Park, Younggyo Seo, Chang Liu, Li Zhao, Tao Qin,
    Jinwoo Shin, and Tie-Yan Liu. Object-aware regularization for addressing causal
    confusion in imitation learning. *Advances in Neural Information Processing Systems*,
    34:3029–3042, 2021.
  id: totrans-996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. (2021) Jongjin Park, Younggyo Seo, Chang Liu, Li Zhao, Tao Qin,
    Jinwoo Shin, 和 Tie-Yan Liu. 面向对象的正则化以解决模仿学习中的因果混淆。*Advances in Neural Information
    Processing Systems*，34:3029–3042，2021年。
- en: 'Paster et al. (2022) Keiran Paster, Sheila A. McIlraith, and Jimmy Ba. You
    can’t count on luck: Why decision transformers and rvs fail in stochastic environments.
    In *Advances in Neural Information Processing Systems 35*, 2022.'
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paster et al. (2022) Keiran Paster, Sheila A. McIlraith, 和 Jimmy Ba. 你不能依靠运气：为什么决策变换器和RVS在随机环境中失败。收录于
    *Advances in Neural Information Processing Systems 35*，2022年。
- en: Pasula (2020) Pranay Pasula. Complex skill acquisition through simple skill
    adversarial imitation learning. *CoRR*, abs/2007.10281, 2020.
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pasula (2020) Pranay Pasula. 通过简单技能对抗性模仿学习获得复杂技能。*CoRR*，abs/2007.10281，2020年。
- en: Pearce & Zhu (2022) Tim Pearce and Jun Zhu. Counter-strike deathmatch with large-scale
    behavioural cloning. In *2022 IEEE Conference on Games (CoG)*, pp.  104–111\.
    IEEE, 2022.
  id: totrans-999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearce & Zhu (2022) Tim Pearce 和 Jun Zhu. 通过大规模行为克隆的反恐精英死亡竞赛。收录于 *2022 IEEE
    Conference on Games (CoG)*，第104–111页。IEEE，2022年。
- en: Pearce et al. (2023) Tim Pearce, Tabish Rashid, Anssi Kanervisto, David Bignell,
    Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad,
    Katja Hofmann, and Sam Devlin. Imitating human behaviour with diffusion models.
    In *Proceedings of the 11th International Conference on Learning Representations*,
    2023.
  id: totrans-1000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearce et al. (2023) Tim Pearce, Tabish Rashid, Anssi Kanervisto, David Bignell,
    Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad,
    Katja Hofmann, 和 Sam Devlin. 使用扩散模型模仿人类行为。收录于 *Proceedings of the 11th International
    Conference on Learning Representations*，2023年。
- en: Peebles & Xie (2023) William Peebles and Saining Xie. Scalable diffusion models
    with transformers. In *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, pp.  4195–4205, 2023.
  id: totrans-1001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peebles & Xie (2023) William Peebles 和 Saining Xie. 使用变换器的可扩展扩散模型。收录于 *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*，第4195–4205页，2023年。
- en: 'Peng et al. (2021) Bei Peng, Tabish Rashid, Christian Schröder de Witt, Pierre-Alexandre
    Kamienny, Philip H. S. Torr, Wendelin Boehmer, and Shimon Whiteson. FACMAC: factored
    multi-agent centralised policy gradients. In *Advances in Neural Information Processing
    Systems 34*, pp.  12208–12221, 2021.'
  id: totrans-1002
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. (2021) Bei Peng, Tabish Rashid, Christian Schröder de Witt, Pierre-Alexandre
    Kamienny, Philip H. S. Torr, Wendelin Boehmer, 和 Shimon Whiteson. FACMAC：因子化的多智能体集中策略梯度。收录于
    *Advances in Neural Information Processing Systems 34*，第12208–12221页，2021年。
- en: Peng et al. (2022) Jian-Wei Peng, Min-Chun Hu, and Wei-Ta Chu. An imitation
    learning framework for generating multi-modal trajectories from unstructured demonstrations.
    *Neurocomputing*, 500:712–723, 2022.
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. (2022) Jian-Wei Peng, Min-Chun Hu, 和 Wei-Ta Chu. 一种用于从非结构化演示生成多模态轨迹的模仿学习框架。*Neurocomputing*，500:712–723，2022年。
- en: 'Peng et al. (2019a) Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel,
    and Sergey Levine. Variational discriminator bottleneck: Improving imitation learning,
    inverse rl, and gans by constraining information flow. In *Proceedings of the
    7th International Conference on Learning Representations*, 2019a.'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彭等（2019a）雪宾·彭，安久·金泽瓦，萨姆·托耶，皮特·阿贝尔，谢尔盖·列文。变分鉴别器瓶颈：通过限制信息流改进模仿学习、逆向强化学习和生成对抗网络。在*第七届国际学习表征会议论文集*，2019a。
- en: 'Peng et al. (2019b) Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
    Advantage-weighted regression: Simple and scalable off-policy reinforcement learning.
    *CoRR*, abs/1910.00177, 2019b.'
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彭等（2019b）雪宾·彭，阿维拉尔·库马尔，格雷斯·张，和谢尔盖·列文。优势加权回归：简单且可扩展的离策略强化学习。*CoRR*，abs/1910.00177，2019b。
- en: Pertsch et al. (2021) Karl Pertsch, Youngwoon Lee, Yue Wu, and Joseph J. Lim.
    Demonstration-guided reinforcement learning with learned skills. In *Conference
    on Robot Learning*, volume 164 of *Proceedings of Machine Learning Research*,
    pp.  729–739\. PMLR, 2021.
  id: totrans-1006
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 佩尔奇等（2021）卡尔·佩尔奇，年轻人·李，月吴，和约瑟夫·J·林。利用学习技能的演示指导强化学习。在*机器人学习会议*，*机器学习研究论文集*第164卷，第729–739页。PMLR，2021。
- en: Pfrommer et al. (2023) Samuel Pfrommer, Yatong Bai, Hyunin Lee, and Somayeh
    Sojoudi. Initial state interventions for deconfounded imitation learning. In *Proceedings
    of the 62nd IEEE Conference on Decision and Control*, pp.  2312–2319\. IEEE, 2023.
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普罗默等（2023）塞缪尔·普罗默，雅通·白，休宁·李，和索梅耶·索乔迪。去混淆模仿学习的初始状态干预。在*第62届IEEE决策与控制会议论文集*，第2312–2319页。IEEE，2023。
- en: 'Plappert et al. (2018) Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob
    McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej,
    Peter Welinder, Vikash Kumar, and Wojciech Zaremba. Multi-goal reinforcement learning:
    Challenging robotics environments and request for research. *CoRR*, abs/1802.09464,
    2018.'
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普拉普特等（2018）马蒂亚斯·普拉普特，马尔钦·安德里乔维茨，亚历克斯·雷，鲍勃·麦格鲁，鲍文·贝克，格伦·鲍威尔，乔纳斯·施耐德，乔什·托宾，马切克·乔西耶，彼得·韦林德，维卡什·库马尔，和沃伊切赫·扎伦巴。多目标强化学习：具有挑战性的机器人环境和研究请求。*CoRR*，abs/1802.09464，2018。
- en: 'Plummer et al. (2015) Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C.
    Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting
    region-to-phrase correspondences for richer image-to-sentence models. *CoRR*,
    abs/1505.04870, 2015.'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普拉默等（2015）布赖恩·A·普拉默，李伟·王，克里斯·M·塞万提斯，胡安·C·凯塞多，朱莉亚·霍肯迈尔，和斯韦特拉娜·拉泽布尼克。Flickr30k
    实体：为更丰富的图像到句子模型收集区域到短语的对应关系。*CoRR*，abs/1505.04870，2015。
- en: Pomerleau (1991) Dean Pomerleau. Efficient training of artificial neural networks
    for autonomous navigation. *Neural Computation*, 3(1):88–97, 1991.
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 波默劳（1991）迪安·波默劳。用于自主导航的人工神经网络高效训练。*神经计算*，3(1)：88–97，1991。
- en: 'Prudencio et al. (2023) Rafael Figueiredo Prudencio, Marcos ROA Maximo, and
    Esther Luna Colombini. A survey on offline reinforcement learning: Taxonomy, review,
    and open problems. *IEEE Transactions on Neural Networks and Learning Systems*,
    2023.'
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普鲁登西奥等（2023）拉斐尔·菲格雷多·普鲁登西奥，马科斯·ROA·马克西莫，和埃丝特·卢娜·科伦比尼。关于离线强化学习的调查：分类、综述和未解决的问题。*IEEE神经网络与学习系统汇刊*，2023。
- en: 'Puterman (2014) Martin L Puterman. *Markov decision processes: discrete stochastic
    dynamic programming*. John Wiley & Sons, 2014.'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普特曼（2014）马丁·L·普特曼。*马尔可夫决策过程：离散随机动态规划*。约翰·威利 & 兄弟，2014。
- en: Putterman et al. (2022) Aaron L Putterman, Kevin Lu, Igor Mordatch, and Pieter
    Abbeel. Pretraining for language conditioned imitation with transformers, 2022.
    URL [https://openreview.net/forum?id=eCPCn25gat](https://openreview.net/forum?id=eCPCn25gat).
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普特曼等（2022）亚伦·L·普特曼，凯文·陆，伊戈尔·莫达奇，和皮特·阿贝尔。使用变压器进行语言条件模仿的预训练，2022年。网址 [https://openreview.net/forum?id=eCPCn25gat](https://openreview.net/forum?id=eCPCn25gat)。
- en: Qi et al. (2022) Han Qi, Yi Su, Aviral Kumar, and Sergey Levine. Data-driven
    offline decision-making via invariant representation learning. In *Advances in
    Neural Information Processing Systems 35*, 2022.
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 齐等（2022）韩齐，易苏，阿维拉尔·库马尔，和谢尔盖·列文。通过不变表征学习进行数据驱动的离线决策。 在*神经信息处理系统进展 35*，2022年。
- en: Qi et al. (2020) Mengshi Qi, Jie Qin, Yu Wu, and Yi Yang. Imitative non-autoregressive
    modeling for trajectory forecasting and imputation. In *IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, pp.  12733–12742\. Computer Vision Foundation
    / IEEE, 2020.
  id: totrans-1015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 齐等（2020）孟诗·齐，解琴，余吴，和易杨。用于轨迹预测和补全的模仿非自回归建模。在*IEEE/CVF计算机视觉与模式识别会议*，第12733–12742页。计算机视觉基金会
    / IEEE，2020。
- en: 'Qin et al. (2022) Rongjun Qin, Xingyuan Zhang, Songyi Gao, Xiong-Hui Chen,
    Zewen Li, Weinan Zhang, and Yang Yu. Neorl: A near real-world benchmark for offline
    reinforcement learning. In *Advances in Neural Information Processing Systems
    35*, 2022.'
  id: totrans-1016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 秦等（2022）荣俊·秦，兴元·张，松义·高，熊辉·陈，泽文·李，伟南·张，和杨宇。Neorl：一个近现实世界的离线强化学习基准。在*神经信息处理系统进展
    35*，2022年。
- en: Qureshi et al. (2019) Ahmed Hussain Qureshi, Byron Boots, and Michael C. Yip.
    Adversarial imitation via variational inverse reinforcement learning. In *Proceedings
    of the 7th International Conference on Learning Representations*. OpenReview.net,
    2019.
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库雷希等（2019）艾哈迈德·侯赛因·库雷希，拜伦·布茨，和迈克尔·C·叶。通过变分逆强化学习进行对抗模仿。在*第七届国际学习表征会议论文集*。OpenReview.net，2019年。
- en: Rabiner & Juang (1986) Lawrence Rabiner and Biinghwang Juang. An introduction
    to hidden markov models. *IEEE ASSP Magazine*, 3(1):4–16, 1986.
  id: totrans-1018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉宾纳和庄（1986）劳伦斯·拉宾纳和宾煌·庄。隐马尔可夫模型简介。*IEEE ASSP杂志*，3(1)：4–16，1986年。
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training. *mikecaptain.com*,
    2018.
  id: totrans-1019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉德福德等（2018）亚历克·拉德福德，卡尔提克·纳拉西曼，蒂姆·萨利曼斯，伊利亚·苏茨克弗，等。通过生成预训练提高语言理解。*mikecaptain.com*，2018年。
- en: Rafailov et al. (2021) Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea
    Finn. Offline reinforcement learning from images with latent space models. In
    *Proceedings of the 3rd Annual Conference on Learning for Dynamics and Control*,
    volume 144, pp.  1154–1168, 2021.
  id: totrans-1020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉法伊洛夫等（2021）拉斐尔·拉法伊洛夫，天赫·余，阿拉文德·拉杰斯瓦兰，和切尔西·芬恩。使用潜在空间模型从图像中进行离线强化学习。在*第三届动态与控制学习年会论文集*，第144卷，第1154–1168页，2021年。
- en: Rahmatizadeh et al. (2018) Rouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau
    Bölöni, and Sergey Levine. Vision-based multi-task manipulation for inexpensive
    robots using end-to-end learning from demonstration. In *IEEE International Conference
    on Robotics and Automation*, pp.  3758–3765\. IEEE, 2018.
  id: totrans-1021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉赫马蒂扎德等（2018）鲁霍拉·拉赫马蒂扎德，普亚·阿博尔哈塞米，拉迪斯劳·博洛尼，和谢尔盖·莱文。基于视觉的多任务操作用于廉价机器人，通过端到端学习进行演示。在*IEEE国际机器人与自动化会议*，第3758–3765页。IEEE，2018年。
- en: 'Ramakrishnan et al. (2021) Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik
    Wijmans, Oleksandr Maksymets, Alexander Clegg, John M. Turner, Eric Undersander,
    Wojciech Galuba, Andrew Westbury, Angel X. Chang, Manolis Savva, Yili Zhao, and
    Dhruv Batra. Habitat-matterport 3d dataset (HM3D): 1000 large-scale 3d environments
    for embodied AI. In *Proceedings of the Neural Information Processing Systems
    Track on Datasets and Benchmarks 1*, 2021.'
  id: totrans-1022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉马克里希南等（2021）桑托什·库马尔·拉马克里希南，亚伦·戈卡斯兰，埃里克·维贾曼斯，奥列克桑德尔·马克西梅茨，亚历山大·克雷格，约翰·M·特纳，埃里克·安德桑德，沃伊切赫·加鲁巴，安德鲁·韦斯特伯里，安吉尔·X·张，马诺利斯·萨瓦，易力·赵，和德鲁夫·巴特拉。Habitat-matterport
    3d数据集（HM3D）：1000个大规模3D环境用于具身人工智能。在*神经信息处理系统数据集与基准追踪会议论文集 1*，2021年。
- en: Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
    and Mark Chen. Hierarchical text-conditional image generation with clip latents.
    *arXiv preprint arXiv:2204.06125*, 2022.
  id: totrans-1023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉梅什等（2022）阿迪蒂亚·拉梅什，普拉富拉·达里瓦尔，亚历克斯·尼科尔，凯西·朱，和马克·陈。基于CLIP潜在变量的分层文本条件图像生成。*arXiv预印本
    arXiv:2204.06125*，2022年。
- en: 'Rasouli et al. (2022) Amir Rasouli, Randy Goebel, Matthew E. Taylor, Iuliia
    Kotseruba, Soheil Alizadeh, Tianpei Yang, Montgomery Alban, Florian Shkurti, Yuzheng
    Zhuang, Adam Scibior, Kasra Rezaee, Animesh Garg, David Meger, Jun Luo, Liam Paull,
    Weinan Zhang, Xinyu Wang, and Xi Chen. Neurips 2022 competition: Driving smarts,
    2022.'
  id: totrans-1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉苏利等（2022）阿米尔·拉苏利，兰迪·戈贝尔，马修·E·泰勒，尤利亚·科特瑟鲁巴，索赫伊尔·阿利扎德，田沛·杨，蒙哥马利·阿尔班，弗洛里安·施库尔蒂，余峥·庄，亚当·斯比奥尔，卡斯拉·雷扎伊，阿尼梅什·戈格，大卫·梅格，罗俊，利亚姆·保尔，伟南·张，辛宇·王，和席·陈。Neurips
    2022竞赛：驾驶智能，2022年。
- en: Reed et al. (2022) Scott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez
    Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky,
    Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley
    Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar,
    and Nando de Freitas. A generalist agent. *Transactions on Machine Learning Research*,
    2022.
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 里德等（2022）斯科特·E·里德，孔拉德·佐尔纳，埃米利奥·帕里索托，塞尔吉奥·戈麦斯·科尔门纳雷霍，亚历山大·诺维科夫，加布里埃尔·巴特-马龙，麦·吉门内斯，尤里·苏尔斯基，杰基·凯，约斯特·托比亚斯·斯普林贝格，汤姆·埃克尔斯，杰克·布鲁斯，阿里·拉扎维，艾希莉·爱德华兹，尼古拉斯·赫斯，玉田·陈，拉亚·哈德塞尔，奥里奥尔·维尼亚尔斯，马赫亚尔·博德巴尔，和南多·德·弗雷塔斯。通用智能体。*机器学习研究汇刊*，2022年。
- en: Reid et al. (2022) Machel Reid, Yutaro Yamada, and Shixiang Shane Gu. Can wikipedia
    help offline reinforcement learning? *CoRR*, abs/2201.12122, 2022.
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 瑞德等（2022）马歇尔·瑞德，弥助·山田，和石香·肖恩·古。维基百科能帮助离线强化学习吗？*CoRR*，abs/2201.12122，2022年。
- en: Ren et al. (2020) Allen Z. Ren, Sushant Veer, and Anirudha Majumdar. Generalization
    guarantees for imitation learning. In *Proceedings of the 4th Conference on Robot
    Learning*, volume 155, pp.  1426–1442\. PMLR, 2020.
  id: totrans-1027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等 (2020) Allen Z. Ren, Sushant Veer, 和 Anirudha Majumdar. 对模仿学习的泛化保证. 见于
    *第 4 届机器人学习大会论文集*，第 155 卷，第 1426–1442 页。PMLR，2020。
- en: 'Reuss et al. (2023) Moritz Reuss, Maximilian Li, Xiaogang Jia, and Rudolf Lioutikov.
    Goal-conditioned imitation learning using score-based diffusion policies. In *Robotics:
    Science and Systems*, 2023.'
  id: totrans-1028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reuss 等 (2023) Moritz Reuss, Maximilian Li, Xiaogang Jia, 和 Rudolf Lioutikov.
    使用基于评分的扩散策略的目标条件模仿学习. 见于 *机器人: 科学与系统*，2023。'
- en: Rezaeifar et al. (2022) Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, Léonard
    Hussenot, Olivier Bachem, Olivier Pietquin, and Matthieu Geist. Offline reinforcement
    learning as anti-exploration. In *Thirty-Sixth AAAI Conference on Artificial Intelligence*,
    pp.  8106–8114\. AAAI Press, 2022.
  id: totrans-1029
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rezaeifar 等 (2022) Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, Léonard
    Hussenot, Olivier Bachem, Olivier Pietquin, 和 Matthieu Geist. 离线强化学习作为反探索. 见于
    *第 36 届 AAAI 人工智能大会*，第 8106–8114 页。AAAI Press，2022。
- en: Rezende & Mohamed (2015) Danilo Jimenez Rezende and Shakir Mohamed. Variational
    inference with normalizing flows. In *Proceedings of the 32nd International Conference
    on Machine Learning*, volume 37, pp.  1530–1538, 2015.
  id: totrans-1030
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rezende 和 Mohamed (2015) Danilo Jimenez Rezende 和 Shakir Mohamed. 使用归一化流的变分推断.
    见于 *第 32 届国际机器学习会议论文集*，第 37 卷，第 1530–1538 页，2015。
- en: Ricciardi (1976) Luigi M Ricciardi. On the transformation of diffusion processes
    into the wiener process. *Journal of Mathematical Analysis and Applications*,
    54(1):185–199, 1976.
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ricciardi (1976) Luigi M Ricciardi. 扩散过程向维纳过程的转换. *数学分析与应用杂志*，54(1):185–199，1976。
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion
    models. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 
    10674–10685\. IEEE, 2022.
  id: totrans-1032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rombach 等 (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser,
    和 Björn Ommer. 高分辨率图像合成与潜在扩散模型. 见于 *IEEE/CVF 计算机视觉与模式识别大会*，第 10674–10685 页。IEEE，2022。
- en: 'Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    U-net: Convolutional networks for biomedical image segmentation. In *Medical Image
    Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,
    Munich, Germany, October 5-9, 2015, Proceedings, Part III 18*, pp.  234–241\.
    Springer, 2015.'
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ronneberger 等 (2015) Olaf Ronneberger, Philipp Fischer, 和 Thomas Brox. U-net:
    用于生物医学图像分割的卷积网络. 见于 *医学图像计算与计算机辅助干预–MICCAI 2015: 第 18 届国际会议，德国慕尼黑，2015 年 10 月
    5-9 日，论文集，第三部分 18*，第 234–241 页。Springer，2015。'
- en: Rosete-Beas et al. (2022) Erick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka
    Boedecker, and Wolfram Burgard. Latent plans for task-agnostic offline reinforcement
    learning. In *Conference on Robot Learning*, volume 205 of *Proceedings of Machine
    Learning Research*, pp.  1838–1849\. PMLR, 2022.
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosete-Beas 等 (2022) Erick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka
    Boedecker, 和 Wolfram Burgard. 任务无关离线强化学习的潜在计划. 见于 *机器人学习大会*，第 205 卷的 *机器学习研究论文集*，第
    1838–1849 页。PMLR，2022。
- en: Ross & Bagnell (2010) Stéphane Ross and Drew Bagnell. Efficient reductions for
    imitation learning. In *Proceedings of the 13th International Conference on Artificial
    Intelligence and Statistics*, pp.  661–668\. JMLR Workshop and Conference Proceedings,
    2010.
  id: totrans-1035
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross 和 Bagnell (2010) Stéphane Ross 和 Drew Bagnell. 高效的模仿学习降维方法. 见于 *第 13 届国际人工智能与统计会议论文集*，第
    661–668 页。JMLR 工作坊与会议论文集，2010。
- en: Salimans et al. (2016) Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki
    Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In *Advances
    in Neural Information Processing Systems 29*, pp.  2226–2234, 2016.
  id: totrans-1036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salimans 等 (2016) Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung,
    Alec Radford, 和 Xi Chen. 改进的生成对抗网络训练技术. 见于 *神经信息处理系统进展 29*，第 2226–2234 页，2016。
- en: Samvelyan et al. (2019) Mikayel Samvelyan, Tabish Rashid, Christian Schröder
    de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philip
    H. S. Torr, Jakob N. Foerster, and Shimon Whiteson. The starcraft multi-agent
    challenge. *CoRR*, abs/1902.04043, 2019.
  id: totrans-1037
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samvelyan 等 (2019) Mikayel Samvelyan, Tabish Rashid, Christian Schröder de Witt,
    Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philip H.
    S. Torr, Jakob N. Foerster, 和 Shimon Whiteson. 星际争霸多智能体挑战. *CoRR*，abs/1902.04043，2019。
- en: Saxena et al. (2023) Vaibhav Saxena, Yotto Koga, and Danfei Xu. Constrained-context
    conditional diffusion models for imitation learning. *CoRR*, abs/2311.01419, 2023.
  id: totrans-1038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saxena 等 (2023) Vaibhav Saxena, Yotto Koga, 和 Danfei Xu. 用于模仿学习的约束上下文条件扩散模型.
    *CoRR*，abs/2311.01419，2023。
- en: Schroecker & Jr. (2020) Yannick Schroecker and Charles L. Isbell Jr. Universal
    value density estimation for imitation learning and goal-conditioned reinforcement
    learning. *CoRR*, abs/2002.06473, 2020.
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schroecker & Jr. (2020) Yannick Schroecker 和 Charles L. Isbell Jr. 用于模仿学习和目标条件强化学习的通用值密度估计.
    *CoRR*, abs/2002.06473, 2020.
- en: Schroecker et al. (2019) Yannick Schroecker, Mel Vecerík, and Jonathan Scholz.
    Generative predecessor models for sample-efficient imitation learning. In *Proceedings
    of the 7th International Conference on Learning Representations*. OpenReview.net,
    2019.
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schroecker et al. (2019) Yannick Schroecker, Mel Vecerík 和 Jonathan Scholz.
    生成前驱模型用于样本高效的模仿学习. 收录于 *Proceedings of the 7th International Conference on Learning
    Representations*. OpenReview.net, 2019.
- en: Schulman et al. (2015) John Schulman, Sergey Levine, Pieter Abbeel, Michael I.
    Jordan, and Philipp Moritz. Trust region policy optimization. In *Proceedings
    of the 32nd International Conference on Machine Learning*, volume 37, pp.  1889–1897,
    2015.
  id: totrans-1041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman et al. (2015) John Schulman, Sergey Levine, Pieter Abbeel, Michael
    I. Jordan 和 Philipp Moritz. 信任域策略优化. 收录于 *Proceedings of the 32nd International
    Conference on Machine Learning*，第37卷，页码 1889–1897, 2015.
- en: 'Shafiullah et al. (2022) Nur Muhammad Shafiullah, Zichen Jeff Cui, Ariuntuya
    Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning $k$ modes with one
    stone. In *Advances in Neural Information Processing Systems 35*, 2022.'
  id: totrans-1042
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shafiullah et al. (2022) Nur Muhammad Shafiullah, Zichen Jeff Cui, Ariuntuya
    Altanzaya 和 Lerrel Pinto. 行为变换器: 一箭双雕地克隆 $k$ 模式. 收录于 *Advances in Neural Information
    Processing Systems 35*, 2022.'
- en: 'Shah et al. (2017) Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor.
    Airsim: High-fidelity visual and physical simulation for autonomous vehicles.
    In *Field and Service Robotics, Results of the 11th International Conference*,
    volume 5 of *Springer Proceedings in Advanced Robotics*, pp.  621–635\. Springer,
    2017.'
  id: totrans-1043
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shah et al. (2017) Shital Shah, Debadeepta Dey, Chris Lovett 和 Ashish Kapoor.
    Airsim: 高保真视觉和物理仿真用于自动驾驶车辆. 收录于 *Field and Service Robotics, Results of the 11th
    International Conference*，*Springer Proceedings in Advanced Robotics* 第5卷，页码 621–635。Springer,
    2017.'
- en: Shamir (2018) Ohad Shamir. Are resnets provably better than linear predictors?
    In *Advances in Neural Information Processing Systems 31*, pp.  505–514, 2018.
  id: totrans-1044
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shamir (2018) Ohad Shamir. ResNets 是否比线性预测器更具可证明性? 收录于 *Advances in Neural Information
    Processing Systems 31*，页码 505–514, 2018.
- en: Shang & Ryoo (2021) Jinghuan Shang and Michael S. Ryoo. Self-supervised disentangled
    representation learning for third-person imitation learning. In *IEEE/RSJ International
    Conference on Intelligent Robots and Systems*, pp.  214–221\. IEEE, 2021.
  id: totrans-1045
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang & Ryoo (2021) Jinghuan Shang 和 Michael S. Ryoo. 自监督解耦表示学习用于第三人称模仿学习. 收录于
    *IEEE/RSJ International Conference on Intelligent Robots and Systems*，页码 214–221。IEEE,
    2021.
- en: 'Shang et al. (2022) Jinghuan Shang, Kumara Kahatapitiya, Xiang Li, and Michael S.
    Ryoo. Starformer: Transformer with state-action-reward representations for visual
    reinforcement learning. In *Proceedings of the 17th European Conference on Computer
    Vision*, volume 13699, pp.  462–479, 2022.'
  id: totrans-1046
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shang et al. (2022) Jinghuan Shang, Kumara Kahatapitiya, Xiang Li 和 Michael
    S. Ryoo. Starformer: 用于视觉强化学习的状态-动作-奖励表示的 Transformer. 收录于 *Proceedings of the
    17th European Conference on Computer Vision*，第13699卷，页码 462–479, 2022.'
- en: 'Sharma et al. (2018) Pratyusha Sharma, Lekha Mohan, Lerrel Pinto, and Abhinav
    Gupta. Multiple interactions made easy (MIME): large scale demonstrations data
    for imitation. In *Proceedings of the 2nd Annual Conference on Robot Learning*,
    volume 87, pp.  906–915, 2018.'
  id: totrans-1047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sharma et al. (2018) Pratyusha Sharma, Lekha Mohan, Lerrel Pinto 和 Abhinav
    Gupta. 多重交互简化 (MIME): 大规模演示数据用于模仿学习. 收录于 *Proceedings of the 2nd Annual Conference
    on Robot Learning*，第87卷，页码 906–915, 2018.'
- en: Shi et al. (2023) Lucy Xiaoyang Shi, Archit Sharma, Tony Z. Zhao, and Chelsea
    Finn. Waypoint-based imitation learning for robotic manipulation. *CoRR*, abs/2307.14326,
    2023.
  id: totrans-1048
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2023) Lucy Xiaoyang Shi, Archit Sharma, Tony Z. Zhao 和 Chelsea Finn.
    基于路径点的模仿学习用于机器人操作. *CoRR*, abs/2307.14326, 2023.
- en: 'Shridhar et al. (2020) Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan
    Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED:
    A benchmark for interpreting grounded instructions for everyday tasks. In *2020
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.  10737–10746,
    2020.'
  id: totrans-1049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shridhar et al. (2020) Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan
    Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer 和 Dieter Fox. ALFRED: 用于解释日常任务中有根指令的基准.
    收录于 *2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition*，页码 10737–10746,
    2020.'
- en: 'Shridhar et al. (2022) Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor:
    A multi-task transformer for robotic manipulation. In *Conference on Robot Learning,
    CoRL 2022, 14-18 December 2022, Auckland, New Zealand*, volume 205 of *Proceedings
    of Machine Learning Research*, pp.  785–799\. PMLR, 2022.'
  id: totrans-1050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar et al. (2022) 莫希特·施里达、卢卡斯·马努埃利和迪特·福克斯。Perceiver-actor：一种用于机器人操作的多任务变换器。见于*机器人学习会议，CoRL
    2022，2022年12月14-18日，新西兰奥克兰*，第205卷*机器学习研究论文集*，第785–799页。PMLR，2022年。
- en: 'Slack et al. (2022) Dylan Z Slack, Yinlam Chow, Bo Dai, and Nevan Wichers.
    Safer: Data-efficient and safe reinforcement learning via skill acquisition. In
    *ICML Decision Awareness in Reinforcement Learning Workshop*, 2022.'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Slack et al. (2022) 达伦·Z·斯莱克、尹蓝·周、博·戴和内万·维切斯。Safer：通过技能获取实现数据高效和安全的强化学习。见于*ICML决策意识强化学习研讨会*，2022年。
- en: Sodhani et al. (2021) Shagun Sodhani, Amy Zhang, and Joelle Pineau. Multi-task
    reinforcement learning with context-based representations. In *Proceedings of
    the 38th International Conference on Machine Learning*, volume 139 of *Proceedings
    of Machine Learning Research*, pp.  9767–9779\. PMLR, 2021.
  id: totrans-1052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sodhani et al. (2021) 沙根·索达尼、艾米·张和乔厄尔·皮诺。基于上下文的多任务强化学习。见于*第38届国际机器学习会议论文集*，第139卷*机器学习研究论文集*，第9767–9779页。PMLR，2021年。
- en: Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
    and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics.
    In *International conference on machine learning*, pp.  2256–2265\. PMLR, 2015.
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohl-Dickstein et al. (2015) 贾沙·索尔-迪克斯坦、埃里克·韦斯、尼鲁·马赫斯瓦拉纳坦和苏利亚·甘古利。利用非平衡热力学进行深度无监督学习。见于*国际机器学习会议*，第2256–2265页。PMLR，2015年。
- en: Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan,
    and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics.
    In *Proceedings of the 32nd International Conference on Machine Learning*, volume 37,
    pp.  2256–2265, 2015.
  id: totrans-1054
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohl-Dickstein et al. (2015) 贾沙·索尔-迪克斯坦、埃里克·A·韦斯、尼鲁·马赫斯瓦拉纳坦和苏利亚·甘古利。利用非平衡热力学进行深度无监督学习。见于*第32届国际机器学习会议论文集*，第37卷，第2256–2265页，2015年。
- en: Sohn et al. (2015) Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured
    output representation using deep conditional generative models. In *Advances in
    Neural Information Processing Systems 28*, pp.  3483–3491, 2015.
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohn et al. (2015) 宋基赫、李洪烂和闫新辰。使用深度条件生成模型学习结构化输出表示。见于*神经信息处理系统进展 28*，第3483–3491页，2015年。
- en: Song et al. (2018) Jiaming Song, Hongyu Ren, Dorsa Sadigh, and Stefano Ermon.
    Multi-agent generative adversarial imitation learning. In *Advances in Neural
    Information Processing Systems 31*, pp.  7472–7483, 2018.
  id: totrans-1056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2018) 贾明·宋、洪宇·任、杜莎·萨迪赫和斯特凡诺·埃尔蒙。多智能体生成对抗模仿学习。见于*神经信息处理系统进展 31*，第7472–7483页，2018年。
- en: Song & Ermon (2019) Yang Song and Stefano Ermon. Generative modeling by estimating
    gradients of the data distribution. *Advances in neural information processing
    systems*, 32, 2019.
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song & Ermon (2019) 杨·宋和斯特凡诺·埃尔蒙。通过估计数据分布的梯度进行生成建模。*神经信息处理系统进展*，32，2019年。
- en: Song & Ermon (2020) Yang Song and Stefano Ermon. Improved techniques for training
    score-based generative models. In *Advances in Neural Information Processing Systems
    33*, 2020.
  id: totrans-1058
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song & Ermon (2020) 杨·宋和斯特凡诺·埃尔蒙。改进的分数基础生成模型训练技术。见于*神经信息处理系统进展 33*，2020年。
- en: 'Song et al. (2019) Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced
    score matching: A scalable approach to density and score estimation. In *Proceedings
    of the 35th Conference on Uncertainty in Artificial Intelligence*, volume 115,
    pp.  574–584, 2019.'
  id: totrans-1059
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2019) 杨·宋、萨哈杰·戈格、贾欣·施和斯特凡诺·埃尔蒙。切片分数匹配：一种可扩展的密度和分数估计方法。见于*第35届人工智能不确定性会议论文集*，第115卷，第574–584页，2019年。
- en: Song et al. (2020) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek
    Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic
    differential equations. *arXiv preprint arXiv:2011.13456*, 2020.
  id: totrans-1060
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2020) 杨·宋、贾沙·索尔-迪克斯坦、迪德里克·P·金马、阿比舍克·库马尔、斯特凡诺·埃尔蒙和本·普尔。通过随机微分方程进行基于分数的生成建模。*arXiv预印本
    arXiv:2011.13456*，2020年。
- en: Song et al. (2021) Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.
    Maximum likelihood training of score-based diffusion models. In *Advances in Neural
    Information Processing Systems 34*, pp.  1415–1428, 2021.
  id: totrans-1061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2021) 杨·宋、康纳·杜尔肯、伊恩·穆雷和斯特凡诺·埃尔蒙。基于分数的扩散模型的最大似然训练。见于*神经信息处理系统进展
    34*，第1415–1428页，2021年。
- en: Song et al. (2023) Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
    Consistency models. In *Proceedings of the 40th International Conference on Machine
    Learning*, volume 202, pp.  32211–32252\. PMLR, 2023.
  id: totrans-1062
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2023) Yang Song, Prafulla Dhariwal, Mark Chen 和 Ilya Sutskever。一致性模型。在*第40届国际机器学习大会论文集*，第202卷，第32211–32252页。PMLR，2023。
- en: Spurr et al. (2018) Adrian Spurr, Jie Song, Seonwook Park, and Otmar Hilliges.
    Cross-modal deep variational hand pose estimation. In *IEEE Conference on Computer
    Vision and Pattern Recognition*, pp.  89–98\. Computer Vision Foundation / IEEE
    Computer Society, 2018.
  id: totrans-1063
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spurr et al. (2018) Adrian Spurr, Jie Song, Seonwook Park 和 Otmar Hilliges。跨模态深度变分手部姿态估计。在*IEEE计算机视觉与模式识别大会*，第89–98页。计算机视觉基金会/IEEE计算机学会，2018。
- en: Sridhar et al. (2023) Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, James
    Weimer, and Insup Lee. Memory-consistent neural networks for imitation learning.
    *CoRR*, abs/2310.06171, 2023.
  id: totrans-1064
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sridhar et al. (2023) Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, James
    Weimer 和 Insup Lee。用于模仿学习的记忆一致性神经网络。*CoRR*，abs/2310.06171，2023。
- en: Sudhakaran & Risi (2023) Shyam Sudhakaran and Sebastian Risi. Skill decision
    transformer. *CoRR*, abs/2301.13573, 2023.
  id: totrans-1065
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sudhakaran & Risi (2023) Shyam Sudhakaran 和 Sebastian Risi。技能决策变换器。*CoRR*，abs/2301.13573，2023。
- en: 'Suh et al. (2023) H.J. Terry Suh, Glen Chou, Hongkai Dai, Lujie Yang, Abhishek
    Gupta, and Russ Tedrake. Fighting uncertainty with gradients: Offline reinforcement
    learning via diffusion score matching. In *7th Annual Conference on Robot Learning*,
    2023.'
  id: totrans-1066
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suh et al. (2023) H.J. Terry Suh, Glen Chou, Hongkai Dai, Lujie Yang, Abhishek
    Gupta 和 Russ Tedrake。通过梯度对抗不确定性：通过扩散评分匹配的离线强化学习。在*第七届机器人学习年会*，2023。
- en: Sun et al. (2021) Jiankai Sun, Lantao Yu, Pinqian Dong, Bo Lu, and Bolei Zhou.
    Adversarial inverse reinforcement learning with self-attention dynamics model.
    *IEEE Robotics and Automation Letters*, 6(2):1880–1886, 2021.
  id: totrans-1067
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2021) Jiankai Sun, Lantao Yu, Pinqian Dong, Bo Lu 和 Bolei Zhou。具有自注意力动态模型的对抗性逆向强化学习。*IEEE机器人与自动化快报*，6(2):1880–1886，2021。
- en: Sun et al. (2023) Jiankai Sun, Yiqi Jiang, Jianing Qiu, Parth Talpur Nobel,
    Mykel Kochenderfer, and Mac Schwager. Conformal prediction for uncertainty-aware
    planning with diffusion dynamics model. In *Advances in Neural Information Processing
    Systems 36*, 2023.
  id: totrans-1068
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2023) Jiankai Sun, Yiqi Jiang, Jianing Qiu, Parth Talpur Nobel,
    Mykel Kochenderfer 和 Mac Schwager。具有扩散动态模型的考虑不确定性的预测。在*神经信息处理系统进展 36*，2023。
- en: 'Sutton & Barto (2018) Richard S Sutton and Andrew G Barto. *Reinforcement learning:
    An introduction*. MIT press, 2018.'
  id: totrans-1069
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton & Barto (2018) Richard S Sutton 和 Andrew G Barto。*强化学习：导论*。MIT出版社，2018。
- en: Takagi (2022) Shiro Takagi. On the effect of pre-training for transformer in
    different modality on offline reinforcement learning. In *Advances in Neural Information
    Processing Systems 35*, 2022.
  id: totrans-1070
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Takagi (2022) Shiro Takagi。不同模态下变换器的预训练对离线强化学习的影响。在*神经信息处理系统进展 35*，2022。
- en: Tassa et al. (2018) Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe
    Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq,
    Timothy P. Lillicrap, and Martin A. Riedmiller. Deepmind control suite. *CoRR*,
    abs/1801.00690, 2018.
  id: totrans-1071
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tassa et al. (2018) Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe
    Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq,
    Timothy P. Lillicrap 和 Martin A. Riedmiller。Deepmind 控制套件。*CoRR*，abs/1801.00690，2018。
- en: Tedrake (2023) Russ Tedrake. *Underactuated Robotics*. Course Notes for MIT
    6.832, 2023. URL [https://underactuated.csail.mit.edu](https://underactuated.csail.mit.edu).
  id: totrans-1072
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tedrake (2023) Russ Tedrake。*欠驱动机器人学*。MIT 6.832课程笔记，2023。网址 [https://underactuated.csail.mit.edu](https://underactuated.csail.mit.edu)。
- en: Tishby & Zaslavsky (2015) Naftali Tishby and Noga Zaslavsky. Deep learning and
    the information bottleneck principle. In *IEEE Information Theory Workshop*, pp. 
    1–5\. IEEE, 2015.
  id: totrans-1073
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tishby & Zaslavsky (2015) Naftali Tishby 和 Noga Zaslavsky。深度学习和信息瓶颈原理。在*IEEE信息理论研讨会*，第1–5页。IEEE，2015。
- en: 'Todorov et al. (2012) Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A
    physics engine for model-based control. In *IEEE/RSJ International Conference
    on Intelligent Robots and Systems*, 2012.'
  id: totrans-1074
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Todorov et al. (2012) Emanuel Todorov, Tom Erez 和 Yuval Tassa。Mujoco：用于基于模型的控制的物理引擎。在*IEEE/RSJ国际智能机器人与系统会议*，2012。
- en: Toyer et al. (2020) Sam Toyer, Rohin Shah, Andrew Critch, and Stuart Russell.
    The MAGICAL benchmark for robust imitation. In *Advances in Neural Information
    Processing Systems 33*, 2020.
  id: totrans-1075
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Toyer et al. (2020) Sam Toyer, Rohin Shah, Andrew Critch 和 Stuart Russell。稳健模仿的MAGICAL基准。在*神经信息处理系统进展
    33*，2020。
- en: 'Trabucco et al. (2022) Brandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey
    Levine. Design-bench: Benchmarks for data-driven offline model-based optimization.
    In *Proceedings of the 39th International Conference on Machine Learning*, volume
    162, pp.  21658–21676, 2022.'
  id: totrans-1076
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Trabucco 等 (2022) Brandon Trabucco, Xinyang Geng, Aviral Kumar, 和 Sergey Levine.
    Design-bench: 数据驱动的离线模型优化基准。在 *Proceedings of the 39th International Conference
    on Machine Learning* 中，第 162 卷，第 21658–21676 页，2022。'
- en: Tseng et al. (2022) Wei-Cheng Tseng, Tsun-Hsuan Johnson Wang, Yen-Chen Lin,
    and Phillip Isola. Offline multi-agent reinforcement learning with knowledge distillation.
    In *Advances in Neural Information Processing Systems 35*, 2022.
  id: totrans-1077
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tseng 等 (2022) Wei-Cheng Tseng, Tsun-Hsuan Johnson Wang, Yen-Chen Lin, 和 Phillip
    Isola. 利用知识蒸馏的离线多智能体强化学习。在 *Advances in Neural Information Processing Systems
    35* 中，2022。
- en: Urpí et al. (2021) Núria Armengol Urpí, Sebastian Curi, and Andreas Krause.
    Risk-averse offline reinforcement learning. In *Proceedings of the 9th International
    Conference on Learning Representations*. OpenReview.net, 2021.
  id: totrans-1078
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Urpí 等 (2021) Núria Armengol Urpí, Sebastian Curi, 和 Andreas Krause. 风险厌恶的离线强化学习。在
    *Proceedings of the 9th International Conference on Learning Representations*
    中。OpenReview.net，2021。
- en: van den Berg et al. (2018) Rianne van den Berg, Leonard Hasenclever, Jakub M.
    Tomczak, and Max Welling. Sylvester normalizing flows for variational inference.
    In *Proceedings of the 34th Conference on Uncertainty in Artificial*, pp.  393–402,
    2018.
  id: totrans-1079
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van den Berg 等 (2018) Rianne van den Berg, Leonard Hasenclever, Jakub M. Tomczak,
    和 Max Welling. Sylvester 归一化流用于变分推断。在 *Proceedings of the 34th Conference on Uncertainty
    in Artificial* 中，第 393–402 页，2018。
- en: van den Oord et al. (2017) Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu.
    Neural discrete representation learning. In *Advances in Neural Information Processing
    Systems 30*, pp.  6306–6315, 2017.
  id: totrans-1080
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van den Oord 等 (2017) Aäron van den Oord, Oriol Vinyals, 和 Koray Kavukcuoglu.
    神经离散表示学习。在 *Advances in Neural Information Processing Systems 30* 中，第 6306–6315
    页，2017。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. *Advances in neural information processing systems*, 30, 2017.
  id: totrans-1081
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等 (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 注意力机制是你所需要的一切。*Advances
    in neural information processing systems*，30，2017。
- en: Vecerík et al. (2017) Matej Vecerík, Todd Hester, Jonathan Scholz, Fumin Wang,
    Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Rothörl, Thomas Lampe, and
    Martin A. Riedmiller. Leveraging demonstrations for deep reinforcement learning
    on robotics problems with sparse rewards. *CoRR*, abs/1707.08817, 2017.
  id: totrans-1082
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vecerík 等 (2017) Matej Vecerík, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier
    Pietquin, Bilal Piot, Nicolas Heess, Thomas Rothörl, Thomas Lampe, 和 Martin A.
    Riedmiller. 利用演示进行深度强化学习以解决稀疏奖励的机器人问题。*CoRR*，abs/1707.08817，2017。
- en: Venkatraman et al. (2023) Siddarth Venkatraman, Shivesh Khaitan, Ravi Tej Akella,
    John M. Dolan, Jeff G. Schneider, and Glen Berseth. Reasoning with latent diffusion
    in offline reinforcement learning. *CoRR*, abs/2309.06599, 2023.
  id: totrans-1083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Venkatraman 等 (2023) Siddarth Venkatraman, Shivesh Khaitan, Ravi Tej Akella,
    John M. Dolan, Jeff G. Schneider, 和 Glen Berseth. 在离线强化学习中利用潜在扩散进行推理。*CoRR*，abs/2309.06599，2023。
- en: 'Venuto et al. (2020) David Venuto, Jhelum Chakravorty, Léonard Boussioux, Junhao
    Wang, Gavin McCracken, and Doina Precup. oirl: Robust adversarial inverse reinforcement
    learning with temporally extended actions. *CoRR*, abs/2002.09043, 2020.'
  id: totrans-1084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Venuto 等 (2020) David Venuto, Jhelum Chakravorty, Léonard Boussioux, Junhao
    Wang, Gavin McCracken, 和 Doina Precup. oirl: 具有时间扩展动作的鲁棒对抗逆向强化学习。*CoRR*，abs/2002.09043，2020。'
- en: Villaflor et al. (2022) Adam R. Villaflor, Zhe Huang, Swapnil Pande, John M.
    Dolan, and Jeff Schneider. Addressing optimism bias in sequence modeling for reinforcement
    learning. In *Proceedings of the 39th International Conference on Machine Learning*,
    volume 162, pp.  22270–22283, 2022.
  id: totrans-1085
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Villaflor 等 (2022) Adam R. Villaflor, Zhe Huang, Swapnil Pande, John M. Dolan,
    和 Jeff Schneider. 解决强化学习序列建模中的乐观偏差。在 *Proceedings of the 39th International Conference
    on Machine Learning* 中，第 162 卷，第 22270–22283 页，2022。
- en: Vincent (2011) Pascal Vincent. A connection between score matching and denoising
    autoencoders. *Neural Computation*, 23(7):1661–1674, 2011.
  id: totrans-1086
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent (2011) Pascal Vincent. 评分匹配与去噪自编码器之间的联系。*Neural Computation*，23(7):1661–1674，2011。
- en: Vovk et al. (2005) Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. *Algorithmic
    learning in a random world*, volume 29. Springer, 2005.
  id: totrans-1087
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vovk 等 (2005) Vladimir Vovk, Alexander Gammerman, 和 Glenn Shafer. *Algorithmic
    learning in a random world*，第 29 卷。Springer，2005。
- en: 'Vowels et al. (2020) Matthew J. Vowels, Necati Cihan Camgöz, and Richard Bowden.
    Gated variational autoencoders: Incorporating weak supervision to encourage disentanglement.
    In *Proceedings of the 15th IEEE International Conference on Automatic Face and
    Gesture Recognition*, pp.  125–132\. IEEE, 2020.'
  id: totrans-1088
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vowels et al. (2020) Matthew J. Vowels、Necati Cihan Camgöz 和 Richard Bowden.
    门控变分自编码器: 融入弱监督以鼓励解耦. 见于*第15届IEEE国际自动人脸与手势识别会议论文集*，第125–132页，IEEE，2020年。'
- en: 'Vuong et al. (2022) Quan Vuong, Aviral Kumar, Sergey Levine, and Yevgen Chebotar.
    DASCO: dual-generator adversarial support constrained offline reinforcement learning.
    In *Advances in Neural Information Processing Systems 35*, 2022.'
  id: totrans-1089
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vuong et al. (2022) Quan Vuong、Aviral Kumar、Sergey Levine 和 Yevgen Chebotar.
    DASCO: 双生成对抗支持约束离线强化学习. 见于*神经信息处理系统进展 35*，2022年。'
- en: Wang et al. (2021) Jianhao Wang, Wenzhe Li, Haozhe Jiang, Guangxiang Zhu, Siyuan
    Li, and Chongjie Zhang. Offline reinforcement learning with reverse model-based
    imagination. In *Advances in Neural Information Processing Systems 34*, pp.  29420–29432,
    2021.
  id: totrans-1090
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021) Jianhao Wang、Wenzhe Li、Haozhe Jiang、Guangxiang Zhu、Siyuan
    Li 和 Chongjie Zhang. 具有逆模型基础想象的离线强化学习. 见于*神经信息处理系统进展 34*，第29420–29432页，2021年。
- en: Wang et al. (2022a) Kerong Wang, Hanye Zhao, Xufang Luo, Kan Ren, Weinan Zhang,
    and Dongsheng Li. Bootstrapped transformer for offline reinforcement learning.
    In *Advances in Neural Information Processing Systems 35*, 2022a.
  id: totrans-1091
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022a) Kerong Wang、Hanye Zhao、Xufang Luo、Kan Ren、Weinan Zhang 和
    Dongsheng Li. 用于离线强化学习的自举变换器. 见于*神经信息处理系统进展 35*，2022年。
- en: 'Wang et al. (2023a) Mianchu Wang, Rui Yang, Xi Chen, and Meng Fang. Goplan:
    Goal-conditioned offline reinforcement learning by planning with learned models.
    *CoRR*, abs/2310.20025, 2023a.'
  id: totrans-1092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023a) Mianchu Wang、Rui Yang、Xi Chen 和 Meng Fang. Goplan: 通过规划学习模型进行目标条件的离线强化学习.
    *CoRR*，abs/2310.20025，2023年。'
- en: Wang et al. (2023b) Pengqin Wang, Meixin Zhu, and Shaojie Shen. Environment
    transformer and policy optimization for model-based offline reinforcement learning,
    2023b.
  id: totrans-1093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) Pengqin Wang、Meixin Zhu 和 Shaojie Shen. 环境变换器与基于模型的离线强化学习的策略优化，2023年。
- en: Wang et al. (2022b) Tianyu Wang, Nikhil Karnwal, and Nikolay Atanasov. Latent
    policies for adversarial imitation learning. *CoRR*, abs/2206.11299, 2022b.
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022b) Tianyu Wang、Nikhil Karnwal 和 Nikolay Atanasov. 用于对抗性模仿学习的潜在策略.
    *CoRR*，abs/2206.11299，2022年。
- en: 'Wang et al. (2023c) Yiqi Wang, Mengdi Xu, Laixi Shi, and Yuejie Chi. A trajectory
    is worth three sentences: multimodal transformer for offline reinforcement learning.
    In *Uncertainty in Artificial Intelligence*, volume 216, pp.  2226–2236, 2023c.'
  id: totrans-1095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023c) Yiqi Wang、Mengdi Xu、Laixi Shi 和 Yuejie Chi. 一段轨迹胜过三句话:
    离线强化学习的多模态变换器. 见于*人工智能不确定性*，第216卷，第2226–2236页，2023年。'
- en: Wang et al. (2023d) Yuanfu Wang, Chao Yang, Ying Wen, Yu Liu, and Yu Qiao. Critic-guided
    decision transformer for offline reinforcement learning. *arXiv preprint arXiv:2312.13716*,
    2023d.
  id: totrans-1096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023d) Yuanfu Wang、Chao Yang、Ying Wen、Yu Liu 和 Yu Qiao. 基于批评指导的决策变换器用于离线强化学习.
    *arXiv 预印本 arXiv:2312.13716*，2023年。
- en: Wang et al. (2023e) Yunke Wang, Minjing Dong, Bo Du, and Chang Xu. Imitation
    learning from purified demonstration. *arXiv preprint arXiv:2310.07143*, 2023e.
  id: totrans-1097
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023e) Yunke Wang、Minjing Dong、Bo Du 和 Chang Xu. 从纯化演示中进行模仿学习.
    *arXiv 预印本 arXiv:2310.07143*，2023年。
- en: Wang et al. (2023f) Zhendong Wang, Jonathan J. Hunt, and Mingyuan Zhou. Diffusion
    policies as an expressive policy class for offline reinforcement learning. In
    *Proceedings of the 11th International Conference on Learning Representations*.
    OpenReview.net, 2023f.
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023f) Zhendong Wang、Jonathan J. Hunt 和 Mingyuan Zhou. 扩散策略作为离线强化学习的表达性策略类别.
    见于*第11届国际学习表征会议论文集*，OpenReview.net，2023年。
- en: Wang et al. (2017) Ziyu Wang, Josh Merel, Scott E. Reed, Nando de Freitas, Gregory
    Wayne, and Nicolas Heess. Robust imitation of diverse behaviors. In *Advances
    in Neural Information Processing Systems 30, Long Beach, CA, USA*, pp.  5320–5329,
    2017.
  id: totrans-1099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2017) Ziyu Wang、Josh Merel、Scott E. Reed、Nando de Freitas、Gregory
    Wayne 和 Nicolas Heess. 对多样行为的鲁棒模仿. 见于*神经信息处理系统进展 30, 洛杉矶，加利福尼亚州，美国*，第5320–5329页，2017年。
- en: Wang et al. (2020) Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh Merel, Jost Tobias
    Springenberg, Scott E. Reed, Bobak Shahriari, Noah Y. Siegel, Çaglar Gülçehre,
    Nicolas Heess, and Nando de Freitas. Critic regularized regression. In *Advances
    in Neural Information Processing Systems 33*, 2020.
  id: totrans-1100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020) Ziyu Wang、Alexander Novikov、Konrad Zolna、Josh Merel、Jost
    Tobias Springenberg、Scott E. Reed、Bobak Shahriari、Noah Y. Siegel、Çaglar Gülçehre、Nicolas
    Heess 和 Nando de Freitas. 批评家正则化回归. 见于*神经信息处理系统进展 33*，2020年。
- en: Ward et al. (2019) Patrick Nadeem Ward, Ariella Smofsky, and Avishek Joey Bose.
    Improving exploration in soft-actor-critic with normalizing flows policies. *CoRR*,
    abs/1906.02771, 2019.
  id: totrans-1101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ward 等（2019）帕特里克·纳迪姆·沃德、艾瑞拉·斯莫夫斯基和阿维谢克·乔伊·博斯。通过归一化流策略改进软行为者-评论家中的探索。在*CoRR*，abs/1906.02771，2019年。
- en: Wei et al. (2021) Hua Wei, Deheng Ye, Zhao Liu, Hao Wu, Bo Yuan, Qiang Fu, Wei
    Yang, and Zhenhui Li. Boosting offline reinforcement learning with residual generative
    modeling. In *Proceedings of the 30th International Joint Conference on Artificial
    Intelligence*, pp.  3574–3580\. ijcai.org, 2021.
  id: totrans-1102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2021）华伟、叶德恒、刘超、吴浩、元博、傅强、杨伟和李振辉。利用残差生成建模提升离线强化学习。在*第30届国际人工智能联合会议论文集*，第3574–3580页。ijcai.org，2021年。
- en: Wei et al. (2023) Yao Wei, Yanchao Sun, Ruijie Zheng, Sai Vemprala, Rogerio
    Bonatti, Shuhang Chen, Ratnesh Madaan, Zhongjie Ba, Ashish Kapoor, and Shuang
    Ma. Is imitation all you need? generalized decision-making with dual-phase training.
    In *IEEE/CVF International Conference on Computer Vision*, pp.  16221–16231, 2023.
  id: totrans-1103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2023）魏耀、孙彦超、郑瑞杰、韦萨伊·维姆普拉、罗杰里奥·博纳提、陈书航、拉特内什·马丹、巴中杰、阿希什·卡普尔和马爽。模仿是你所需的一切吗？通过双阶段训练进行广义决策。在*IEEE/CVF国际计算机视觉会议*，第16221–16231页，2023年。
- en: 'Weissenbacher et al. (2022) Matthias Weissenbacher, Samarth Sinha, Animesh
    Garg, and Yoshinobu Kawahara. Koopman q-learning: Offline reinforcement learning
    via symmetries of dynamics. In *Proceedings of the 39th International Conference
    on Machine Learning*, volume 162, pp.  23645–23667, 2022.'
  id: totrans-1104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weissenbacher 等（2022）马蒂亚斯·魏森巴赫、萨玛斯·辛哈、安尼梅什·加格和川原义信。库普曼 q 学习：通过动态对称的离线强化学习。在*第39届国际机器学习会议论文集*，第162卷，第23645–23667页，2022年。
- en: Welling & Teh (2011) Max Welling and Yee Whye Teh. Bayesian learning via stochastic
    gradient langevin dynamics. In *Proceedings of the 28th International Conference
    on Machine Learning*, pp.  681–688, 2011.
  id: totrans-1105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Welling & Teh（2011）马克斯·威灵和叶伟辉。通过随机梯度朗之万动力学的贝叶斯学习。在*第28届国际机器学习会议论文集*，第681–688页，2011年。
- en: Wen et al. (2022) Muning Wen, Jakub Grudzien Kuba, Runji Lin, Weinan Zhang,
    Ying Wen, Jun Wang, and Yaodong Yang. Multi-agent reinforcement learning is a
    sequence modeling problem. In *Advances in Neural Information Processing Systems
    35*, 2022.
  id: totrans-1106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等（2022）温慕宁、雅库布·格鲁金·库巴、林润吉、张维南、温英、王军和杨耀东。多智能体强化学习是序列建模问题。在*神经信息处理系统进展 35*，2022年。
- en: 'Wen et al. (2023) Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing
    Ma, Junchi Yan, and Liang Sun. Transformers in time series: A survey. In *Proceedings
    of the 32nd International Joint Conference on Artificial Intelligence*, pp.  6778–6786,
    2023.'
  id: totrans-1107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等（2023）温青松、周天、张超力、陈伟齐、马子青、闫军驰和孙亮。时间序列中的变压器：一项调查。在*第32届国际人工智能联合会议论文集*，第6778–6786页，2023年。
- en: Wiewiora (2003) Eric Wiewiora. Potential-based shaping and q-value initialization
    are equivalent. *Journal of Artificial Intelligence Research*, 19:205–208, 2003.
  id: totrans-1108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wiewiora（2003）埃里克·维沃拉。基于潜力的塑形和 q 值初始化是等效的。在*人工智能研究杂志*，19:205–208，2003年。
- en: Wu et al. (2022) Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng
    Long. Supported policy optimization for offline reinforcement learning. In *Advances
    in Neural Information Processing Systems 35*, 2022.
  id: totrans-1109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2022）吴佳龙、吴海旭、邱子涵、王建敏和龙铭盛。用于离线强化学习的支持策略优化。在*神经信息处理系统进展 35*，2022年。
- en: Wu et al. (2019) Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized
    offline reinforcement learning. *CoRR*, abs/1911.11361, 2019.
  id: totrans-1110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2019）吴毅凡、乔治·塔克和奥菲尔·纳赫姆。行为正则化的离线强化学习。在*CoRR*，abs/1911.11361，2019年。
- en: Wu et al. (2021) Yuchen Wu, Melissa Mozifian, and Florian Shkurti. Shaping rewards
    for reinforcement learning with imperfect demonstrations using generative models.
    In *IEEE International Conference on Robotics and Automation*, pp.  6628–6634\.
    IEEE, 2021.
  id: totrans-1111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2021）吴宇辰、梅丽莎·莫齐菲安和弗洛里安·舒尔提。利用生成模型塑形奖励以进行不完美演示的强化学习。在*IEEE国际机器人与自动化会议*，第6628–6634页。IEEE，2021年。
- en: 'Xiao et al. (2023) Wei Xiao, Tsun-Hsuan Wang, Chuang Gan, and Daniela Rus.
    Safediffuser: Safe planning with diffusion probabilistic models. *CoRR*, abs/2306.00148,
    2023.'
  id: totrans-1112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等（2023）肖伟、王村轩、甘创和丹妮拉·鲁斯。安全扩散器：利用扩散概率模型进行安全规划。在*CoRR*，abs/2306.00148，2023年。
- en: Xiao et al. (2022) Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling
    the generative learning trilemma with denoising diffusion gans. In *Proceedings
    of the 10th International Conference on Learning Representations*, 2022.
  id: totrans-1113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等（2022）肖志胜、卡尔斯滕·克雷斯和阿拉什·瓦赫达特。利用去噪扩散生成对抗网络解决生成学习三难问题。在*第10届国际学习表征会议论文集*，2022年。
- en: Xie et al. (2023) Zhihui Xie, Zichuan Lin, Deheng Ye, Qiang Fu, Yang Wei, and
    Shuai Li. Future-conditioned unsupervised pretraining for decision transformer.
    In *Proceedings of the 40th International Conference on Machine Learning*, volume
    202, pp.  38187–38203, 2023.
  id: totrans-1114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等人 (2023) Zhihui Xie, Zichuan Lin, Deheng Ye, Qiang Fu, Yang Wei, 和 Shuai
    Li. 面向未来的无监督预训练用于决策变换器。发表于*第40届国际机器学习会议论文集*，第202卷，第38187–38203页，2023年。
- en: Xu et al. (2022a) Haoran Xu, Xianyuan Zhan, and Xiangyu Zhu. Constraints penalized
    q-learning for safe offline reinforcement learning. In *Thirty-Sixth AAAI Conference
    on Artificial Intelligence*, pp.  8753–8760\. AAAI Press, 2022a.
  id: totrans-1115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2022a) Haoran Xu, Xianyuan Zhan, 和 Xiangyu Zhu. 约束惩罚 Q-learning 用于安全离线强化学习。发表于*第36届
    AAAI 人工智能会议*，第8753–8760页。AAAI出版社，2022a年。
- en: Xu et al. (2022b) Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao,
    Joshua B. Tenenbaum, and Chuang Gan. Prompting decision transformer for few-shot
    policy generalization. In *Proceedings of the 39th International Conference on
    Machine Learning*, volume 162, pp.  24631–24645, 2022b.
  id: totrans-1116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2022b) Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua
    B. Tenenbaum, 和 Chuang Gan. 提示决策变换器用于少样本策略泛化。发表于*第39届国际机器学习会议论文集*，第162卷，第24631–24645页，2022b年。
- en: Xu et al. (2022c) Yilun Xu, Ziming Liu, Max Tegmark, and Tommi S. Jaakkola.
    Poisson flow generative models. In *Advances in Neural Information Processing
    Systems 35*, 2022c.
  id: totrans-1117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2022c) Yilun Xu, Ziming Liu, Max Tegmark, 和 Tommi S. Jaakkola. 泊松流生成模型。发表于*神经信息处理系统进展
    35*，2022c年。
- en: 'Xu et al. (2023) Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong, Max Tegmark,
    and Tommi S. Jaakkola. PFGM++: unlocking the potential of physics-inspired generative
    models. In *proceedings of the 40th International Conference on Machine Learning*,
    volume 202 of *Proceedings of Machine Learning Research*, pp.  38566–38591\. PMLR,
    2023.'
  id: totrans-1118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2023) Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong, Max Tegmark,
    和 Tommi S. Jaakkola. PFGM++：解锁物理启发生成模型的潜力。发表于*第40届国际机器学习会议论文集*，第202卷，第38566–38591页。PMLR，2023年。
- en: 'Yamagata et al. (2023) Taku Yamagata, Ahmed Khalil, and Raúl Santos-Rodríguez.
    Q-learning decision transformer: Leveraging dynamic programming for conditional
    sequence modelling in offline RL. In *Proceedings of the 40th International Conference
    on Machine Learning*, volume 202, pp.  38989–39007, 2023.'
  id: totrans-1119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yamagata 等人 (2023) Taku Yamagata, Ahmed Khalil, 和 Raúl Santos-Rodríguez. Q-learning
    决策变换器：利用动态规划进行离线强化学习中的条件序列建模。发表于*第40届国际机器学习会议论文集*，第202卷，第38989–39007页，2023年。
- en: 'Yan et al. (2022) Kai Yan, Alexander G. Schwing, and Yu-Xiong Wang. CEIP: combining
    explicit and implicit priors for reinforcement learning with demonstrations. In
    *Advances in Neural Information Processing Systems 35*, 2022.'
  id: totrans-1120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等人 (2022) Kai Yan, Alexander G. Schwing, 和 Yu-Xiong Wang. CEIP：结合显性和隐性先验用于带演示的强化学习。发表于*神经信息处理系统进展
    35*，2022年。
- en: Yang et al. (2023a) Junming Yang, Xingguo Chen, Shengyuan Wang, and Bolei Zhang.
    Model-based offline policy optimization with adversarial network. In *Proceedings
    of the 26th European Conference on Artificial Intelligence*, volume 372, pp. 
    2850–2857, 2023a.
  id: totrans-1121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 (2023a) Junming Yang, Xingguo Chen, Shengyuan Wang, 和 Bolei Zhang. 基于模型的离线策略优化与对抗网络。发表于*第26届欧洲人工智能会议论文集*，第372卷，第2850–2857页，2023a年。
- en: 'Yang et al. (2022a) Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng
    Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Ming-Hsuan Yang, and Bin Cui. Diffusion
    models: A comprehensive survey of methods and applications. *CoRR*, abs/2209.00796,
    2022a.'
  id: totrans-1122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 (2022a) Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu,
    Yue Zhao, Yingxia Shao, Wentao Zhang, Ming-Hsuan Yang, 和 Bin Cui. 扩散模型：方法和应用的综合调查。*CoRR*，abs/2209.00796，2022a年。
- en: Yang et al. (2023b) Rui Yang, Lin Yong, Xiaoteng Ma, Hao Hu, Chongjie Zhang,
    and Tong Zhang. What is essential for unseen goal generalization of offline goal-conditioned
    rl? In *Proceedings of the 40th International Conference on Machine Learning*,
    volume 202, pp.  39543–39571, 2023b.
  id: totrans-1123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 (2023b) Rui Yang, Lin Yong, Xiaoteng Ma, Hao Hu, Chongjie Zhang, 和 Tong
    Zhang. 离线目标条件强化学习中对未见目标泛化的关键因素是什么？发表于*第40届国际机器学习会议论文集*，第202卷，第39543–39571页，2023b年。
- en: Yang et al. (2022b) Shentao Yang, Yihao Feng, Shujian Zhang, and Mingyuan Zhou.
    Regularizing a model-based policy stationary distribution to stabilize offline
    reinforcement learning. In *Proceedings of the 39th International Conference on
    Machine Learning*, volume 162, pp.  24980–25006, 2022b.
  id: totrans-1124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 (2022b) Shentao Yang, Yihao Feng, Shujian Zhang, 和 Mingyuan Zhou. 规范化基于模型的策略静态分布以稳定离线强化学习。发表于*第39届国际机器学习会议论文集*，第162卷，第24980–25006页，2022b年。
- en: Yang et al. (2022c) Shentao Yang, Zhendong Wang, Huangjie Zheng, Yihao Feng,
    and Mingyuan Zhou. A behavior regularized implicit policy for offline reinforcement
    learning. *arXiv preprint arXiv:2202.09673*, 2022c.
  id: totrans-1125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2022c）Shentao Yang, Zhendong Wang, Huangjie Zheng, Yihao Feng, 和 Mingyuan
    Zhou. 一种行为正则化的隐式策略用于离线强化学习。*arXiv预印本 arXiv:2202.09673*，2022c。
- en: Yang et al. (2022d) Shentao Yang, Shujian Zhang, Yihao Feng, and Mingyuan Zhou.
    A unified framework for alternating offline model training and policy learning.
    In *Advances in Neural Information Processing Systems 35*, 2022d.
  id: totrans-1126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2022d）Shentao Yang, Shujian Zhang, Yihao Feng, 和 Mingyuan Zhou. 一个统一框架用于交替的离线模型训练和策略学习。见于*神经信息处理系统进展
    35*，2022d。
- en: 'Yang et al. (2023c) Sherry Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum.
    Dichotomy of control: Separating what you can control from what you cannot. In
    *Proceedings of the 11th International Conference on Learning Representations*.
    OpenReview.net, 2023c.'
  id: totrans-1127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2023c）Sherry Yang, Dale Schuurmans, Pieter Abbeel, 和 Ofir Nachum. 控制的二分法：将你能控制的与不能控制的分开。见于*第11届国际学习表征会议论文集*。OpenReview.net,
    2023c。
- en: Yang et al. (2022e) Yaodong Yang, Guangyong Chen, Weixun Wang, Xiaotian Hao,
    Jianye Hao, and Pheng-Ann Heng. Transformer-based working memory for multiagent
    reinforcement learning with action parsing. In *Advances in Neural Information
    Processing Systems 35*, 2022e.
  id: totrans-1128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2022e）Yaodong Yang, Guangyong Chen, Weixun Wang, Xiaotian Hao, Jianye
    Hao, 和 Pheng-Ann Heng. 基于变换器的多智能体强化学习中的工作记忆与动作解析。见于*神经信息处理系统进展 35*，2022e。
- en: Yang et al. (2022f) Yiming Yang, Dengpeng Xing, and Bo Xu. Efficient spatiotemporal
    transformer for robotic reinforcement learning. *IEEE Robotics Automation Letters*,
    7(3):7982–7989, 2022f.
  id: totrans-1129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2022f）Yiming Yang, Dengpeng Xing, 和 Bo Xu. 用于机器人强化学习的高效时空变换器。*IEEE机器人与自动化快报*,
    7(3):7982–7989, 2022f。
- en: 'Yang et al. (2023d) Yiqin Yang, Hao Hu, Wenzhe Li, Siyuan Li, Jun Yang, Qianchuan
    Zhao, and Chongjie Zhang. Flow to control: Offline reinforcement learning with
    lossless primitive discovery. In *Proceedings of the 37th AAAI Conference on Artificial
    Intelligence*, pp.  10843–10851\. AAAI Press, 2023d.'
  id: totrans-1130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2023d）Yiqin Yang, Hao Hu, Wenzhe Li, Siyuan Li, Jun Yang, Qianchuan Zhao,
    和 Chongjie Zhang. 流动控制：通过无损原语发现进行离线强化学习。见于*第37届AAAI人工智能会议论文集*，第10843–10851页。AAAI出版社,
    2023d。
- en: Yu et al. (2019a) Lantao Yu, Jiaming Song, and Stefano Ermon. Multi-agent adversarial
    inverse reinforcement learning. In *Proceedings of the 36th International Conference
    on Machine Learning*, volume 97, pp.  7194–7201, 2019a.
  id: totrans-1131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等（2019a）Lantao Yu, Jiaming Song, 和 Stefano Ermon. 多智能体对抗性逆强化学习。见于*第36届国际机器学习会议论文集*，第97卷，第7194–7201页，2019a。
- en: Yu et al. (2019b) Lantao Yu, Tianhe Yu, Chelsea Finn, and Stefano Ermon. Meta-inverse
    reinforcement learning with probabilistic context variables. In *Advances in Neural
    Information Processing Systems 32*, pp.  11749–11760, 2019b.
  id: totrans-1132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等（2019b）Lantao Yu, Tianhe Yu, Chelsea Finn, 和 Stefano Ermon. 带有概率上下文变量的元逆强化学习。见于*神经信息处理系统进展
    32*，第11749–11760页，2019b。
- en: 'Yu et al. (2019c) Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol
    Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation
    for multi-task and meta reinforcement learning. In *Proceedings of the 3rd Annual
    Conference on Robot Learning*, volume 100, pp.  1094–1100, 2019c.'
  id: totrans-1133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等（2019c）Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman,
    Chelsea Finn, 和 Sergey Levine. Meta-world：多任务和元强化学习的基准与评估。见于*第3届年度机器人学习会议论文集*，第100卷，第1094–1100页，2019c。
- en: 'Yu et al. (2020a) Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y.
    Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. MOPO: model-based offline policy
    optimization. In *Advances in Neural Information Processing Systems 33*, 2020a.'
  id: totrans-1134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等（2020a）Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou,
    Sergey Levine, Chelsea Finn, 和 Tengyu Ma. MOPO：基于模型的离线策略优化。见于*神经信息处理系统进展 33*，2020a。
- en: Yu et al. (2020b) Xingrui Yu, Yueming Lyu, and Ivor W. Tsang. Intrinsic reward
    driven imitation learning via generative model. In *Proceedings of the 37th International
    Conference on Machine Learning*, volume 119 of *Proceedings of Machine Learning
    Research*, pp.  10925–10935\. PMLR, 2020b.
  id: totrans-1135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等（2020b）Xingrui Yu, Yueming Lyu, 和 Ivor W. Tsang. 通过生成模型驱动的内在奖励模仿学习。见于*第37届国际机器学习会议论文集*，*机器学习研究论文集*第119卷，第10925–10935页。PMLR,
    2020b。
- en: 'Yuan et al. (2023a) William Yuan, Jiaxing Chen, Shaofei Chen, Lina Lu, Zhenzhen
    Hu, Peng Li, Dawei Feng, Furong Liu, and Jing Chen. Transformer in reinforcement
    learning for decision-making: A survey. *TechRxiv*, 2023a.'
  id: totrans-1136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan等（2023a）William Yuan, Jiaxing Chen, Shaofei Chen, Lina Lu, Zhenzhen Hu,
    Peng Li, Dawei Feng, Furong Liu, 和 Jing Chen. 强化学习中的变换器用于决策：一项综述。*TechRxiv*，2023a。
- en: 'Yuan et al. (2023b) Ye Yuan, Xin Li, Yong Heng, Leiji Zhang, and Mingzhong
    Wang. Good better best: Self-motivated imitation learning for noisy demonstrations.
    *CoRR*, abs/2310.15815, 2023b.'
  id: totrans-1137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等人（2023b）Ye Yuan、Xin Li、Yong Heng、Leiji Zhang 和 Mingzhong Wang。好、佳、最佳：自我驱动的噪声演示模仿学习。*CoRR*，abs/2310.15815，2023b。
- en: 'Zhang et al. (2021a) Chi Zhang, Sanmukh R. Kuppannagari, and Viktor K. Prasanna.
    BRAC+: improved behavior regularized actor critic for offline reinforcement learning.
    In *Asian Conference on Machine Learning*, volume 157 of *Proceedings of Machine
    Learning Research*, pp.  204–219\. PMLR, 2021a.'
  id: totrans-1138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人（2021a）Chi Zhang、Sanmukh R. Kuppannagari 和 Viktor K. Prasanna。BRAC+:
    改进的行为正则化演员评论家用于离线强化学习。见于 *亚洲机器学习会议*，*机器学习研究论文集*第157卷，第204–219页。PMLR，2021a。'
- en: Zhang et al. (2022) Hongchang Zhang, Jianzhun Shao, Yuhang Jiang, Shuncheng
    He, Guanwen Zhang, and Xiangyang Ji. State deviation correction for offline reinforcement
    learning. In *Thirty-Sixth AAAI Conference on Artificial Intelligence*, pp.  9022–9030\.
    AAAI Press, 2022.
  id: totrans-1139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2022）Hongchang Zhang、Jianzhun Shao、Yuhang Jiang、Shuncheng He、Guanwen
    Zhang 和 Xiangyang Ji。离线强化学习的状态偏差修正。见于 *第36届 AAAI 人工智能会议*，第9022–9030页。AAAI Press，2022。
- en: 'Zhang et al. (2023) Jing Zhang, Chi Zhang, Wenjia Wang, and Bing-Yi Jing. APAC:
    authorized probability-controlled actor-critic for offline reinforcement learning.
    *CoRR*, abs/2301.12130, 2023.'
  id: totrans-1140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人（2023）Jing Zhang、Chi Zhang、Wenjia Wang 和 Bing-Yi Jing。APAC: 授权概率控制的演员-评论家用于离线强化学习。*CoRR*，abs/2301.12130，2023。'
- en: 'Zhang et al. (2019) Xin Zhang, Yanhua Li, Xun Zhou, and Jun Luo. Unveiling
    taxi drivers’ strategies via cgail: Conditional generative adversarial imitation
    learning. In *IEEE International Conference on Data Mining*, pp.  1480–1485\.
    IEEE, 2019.'
  id: totrans-1141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2019）Xin Zhang、Yanhua Li、Xun Zhou 和 Jun Luo。通过 cgail 揭示出租车司机的策略：条件生成对抗模仿学习。见于
    *IEEE 国际数据挖掘会议*，第1480–1485页。IEEE，2019。
- en: 'Zhang et al. (2020a) Xin Zhang, Yanhua Li, Ziming Zhang, and Zhi-Li Zhang.
    f-gail: Learning f-divergence for generative adversarial imitation learning. In
    *Advances in Neural Information Processing Systems 33*, 2020a.'
  id: totrans-1142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人（2020a）Xin Zhang、Yanhua Li、Ziming Zhang 和 Zhi-Li Zhang。f-gail: 学习 f-散度用于生成对抗模仿学习。见于
    *神经信息处理系统进展 33*，2020a。'
- en: Zhang et al. (2021b) Xin Zhang, Yanhua Li, Ziming Zhang, Christopher Brinton,
    Zhenming Liu, Zhi-Li Zhang, Hui Lu, and Zhihong Tian. Stabilized likelihood-based
    imitation learning via denoising continuous normalizing flow. In *Submission to
    the 10th International Conference on Learning Representations*, 2021b.
  id: totrans-1143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2021b）Xin Zhang、Yanhua Li、Ziming Zhang、Christopher Brinton、Zhenming
    Liu、Zhi-Li Zhang、Hui Lu 和 Zhihong Tian。通过去噪连续归一化流稳定化的基于似然的模仿学习。见于 *第10届国际学习表征会议提交稿*，2021b。
- en: 'Zhang et al. (2020b) Yufeng Zhang, Qi Cai, Zhuoran Yang, and Zhaoran Wang.
    Generative adversarial imitation learning with neural network parameterization:
    Global optimality and convergence rate. In *Proceedings of the 40th International
    Conference on Machine Learning*, pp.  11044–11054\. PMLR, 2020b.'
  id: totrans-1144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2020b）Yufeng Zhang、Qi Cai、Zhuoran Yang 和 Zhaoran Wang。使用神经网络参数化的生成对抗模仿学习：全局最优性和收敛速率。见于
    *第40届国际机器学习会议论文集*，第11044–11054页。PMLR，2020b。
- en: Zheng et al. (2022) Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision
    transformer. In *Proceedings of the 39th International Conference on Machine Learning*,
    volume 162, pp.  27042–27059, 2022.
  id: totrans-1145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人（2022）Qinqing Zheng、Amy Zhang 和 Aditya Grover。在线决策变换器。见于 *第39届国际机器学习会议论文集*，第162卷，第27042–27059页，2022。
- en: Zheng et al. (2023) Qinqing Zheng, Mikael Henaff, Brandon Amos, and Aditya Grover.
    Semi-supervised offline reinforcement learning with action-free trajectories.
    In *Proceedings of the 40th International Conference on Machine Learning*, volume
    202, pp.  42339–42362, 2023.
  id: totrans-1146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人（2023）Qinqing Zheng、Mikael Henaff、Brandon Amos 和 Aditya Grover。具有无行动轨迹的半监督离线强化学习。见于
    *第40届国际机器学习会议论文集*，第202卷，第42339–42362页，2023。
- en: 'Zhou et al. (2020) Wenxuan Zhou, Sujay Bajracharya, and David Held. PLAS: latent
    action space for offline reinforcement learning. In *Proceedings of the 4th Conference
    on Robot Learning*, volume 155 of *Proceedings of Machine Learning Research*,
    pp.  1719–1735\. PMLR, 2020.'
  id: totrans-1147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人（2020）Wenxuan Zhou、Sujay Bajracharya 和 David Held。PLAS: 离线强化学习的潜在行动空间。见于
    *第四届机器人学习会议论文集*，*机器学习研究论文集*第155卷，第1719–1735页。PMLR，2020。'
- en: 'Zhu et al. (2023a) Tianchen Zhu, Yue Qiu, Haoyi Zhou, and Jianxin Li. Towards
    long-delayed sparsity: Learning a better transformer through reward redistribution.
    In *Proceedings of the 32nd International Joint Conference on Artificial Intelligence*,
    pp.  4693–4701, 2023a.'
  id: totrans-1148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等（2023a）朱天辰、丘岳、周浩毅和李剑欣。面向长期延迟稀疏性：通过奖励重新分配学习更好的变换器。发表于*第32届国际人工智能联合会议论文集*，第4693–4701页，2023a。
- en: 'Zhu et al. (2022) Yifeng Zhu, Abhishek Joshi, Peter Stone, and Yuke Zhu. VIOLA:
    imitation learning for vision-based manipulation with object proposal priors.
    *CoRR*, abs/2210.11339, 2022.'
  id: totrans-1149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等（2022）朱一峰、阿比谢克·乔希、彼得·斯通和朱宇克。VIOLA：用于基于视觉的操作的模仿学习与对象提议先验。*CoRR*，abs/2210.11339，2022。
- en: 'Zhu et al. (2020) Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Martín-Martín.
    robosuite: A modular simulation framework and benchmark for robot learning. *CoRR*,
    abs/2009.12293, 2020.'
  id: totrans-1150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等（2020）朱宇克、乔赛亚·黄、阿贾伊·曼德尔卡尔和罗伯托·马丁·马丁。robosuite：一个模块化仿真框架和机器人学习基准。*CoRR*，abs/2009.12293，2020。
- en: 'Zhu et al. (2023b) Zhengbang Zhu, Minghuan Liu, Liyuan Mao, Bingyi Kang, Minkai
    Xu, Yong Yu, Stefano Ermon, and Weinan Zhang. Madiff: Offline multi-agent learning
    with diffusion models. *CoRR*, abs/2305.17330, 2023b.'
  id: totrans-1151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等（2023b）郑邦朱、刘名焕、毛丽媛、康冰怡、徐敏凯、余勇、斯特凡诺·厄尔蒙和张伟南。Madiff：使用扩散模型的离线多智能体学习。*CoRR*，abs/2305.17330，2023b。
- en: 'Zhu et al. (2023c) Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu
    Zhang, Yong Yu, and Weinan Zhang. Diffusion models for reinforcement learning:
    A survey. *CoRR*, abs/2311.01223, 2023c.'
  id: totrans-1152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等（2023c）郑邦朱、赵寒业、何浩然、钟一超、张神宇、余勇和张伟南。强化学习中的扩散模型：综述。*CoRR*，abs/2311.01223，2023c。
- en: Ziebart et al. (2010) Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey.
    Modeling interaction via the principle of maximum causal entropy. In *Proceedings
    of the 27th International Conference on Machine Learning*, pp.  1255–1262, 2010.
  id: totrans-1153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziebart等（2010）布赖恩·D·齐巴特、J·安德鲁·巴格内尔和安尼德·K·戴。通过最大因果熵原理建模互动。发表于*第27届国际机器学习会议论文集*，第1255–1262页，2010。
- en: 'Zintgraf et al. (2020) Luisa M. Zintgraf, Kyriacos Shiarlis, Maximilian Igl,
    Sebastian Schulze, Yarin Gal, Katja Hofmann, and Shimon Whiteson. Varibad: A very
    good method for bayes-adaptive deep RL via meta-learning. In *Proceedings of the
    8th International Conference on Learning Representations*. OpenReview.net, 2020.'
  id: totrans-1154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zintgraf等（2020）路易莎·M·津特格拉夫、基里亚科斯·希亚尔利斯、马克西米连·伊格尔、塞巴斯蒂安·舒尔茨、雅琳·加尔、卡蒂娅·霍夫曼和希蒙·怀特森。Varibad：一种通过元学习进行贝叶斯自适应深度强化学习的非常好的方法。发表于*第8届学习表示国际会议论文集*。OpenReview.net，2020。
- en: Zolna et al. (2020) Konrad Zolna, Scott E. Reed, Alexander Novikov, Sergio Gómez
    Colmenarejo, David Budden, Serkan Cabi, Misha Denil, Nando de Freitas, and Ziyu
    Wang. Task-relevant adversarial imitation learning. In *Proceedings of the 4th
    Conference on Robot Learning*, volume 155, pp.  247–263\. PMLR, 2020.
  id: totrans-1155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zolna等（2020）康拉德·佐尔纳、斯科特·E·里德、亚历山大·诺维科夫、塞尔吉奥·戈麦斯·科尔门纳雷霍、大卫·布登、赛尔坎·卡比、米沙·丹尼尔、南多·德·弗雷塔斯和王紫瑜。任务相关对抗模仿学习。发表于*第4届机器人学习会议论文集*，第155卷，第247–263页。PMLR，2020。
- en: Zuo et al. (2020) Guoyu Zuo, Kexin Chen, Jiahao Lu, and Xiangsheng Huang. Deterministic
    generative adversarial imitation learning. *Neurocomputing*, 388:60–69, 2020.
  id: totrans-1156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zuo等（2020）左国瑜、陈克鑫、卢佳豪和黄翔生。确定性生成对抗模仿学习。*Neurocomputing*，388:60–69，2020。
