- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:36:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:36:07
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2311.00603] Occluded Person Re-Identification with Deep Learning: A Survey
    and Perspectives'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2311.00603] 遮挡行人重识别与深度学习：综述与展望'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.00603](https://ar5iv.labs.arxiv.org/html/2311.00603)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.00603](https://ar5iv.labs.arxiv.org/html/2311.00603)
- en: \fnref
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \fnref
- en: fn1
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: fn1
- en: \fnref
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \fnref
- en: fn1
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: fn1
- en: \fntext
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \fntext
- en: '[fn1]Equal contribution.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[fn1]等贡献。'
- en: \cormark
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: \cormark
- en: '[1] \cormark[1] 1]organization=Institute of Semiconductors, Chinese Academy
    of Sciences, city=Beijing, postcode=100083, country=China 2]organization=Center
    of Materials Science and Optoelectronics Engineering $\&amp;$ School of Microelectronics,
    University of Chinese Academy of Sciences, city=Beijing, postcode=100083, country=China
    3]organization=School of Software, Xinjiang University, city=Xinjiang, postcode=830000,
    country=China 4]organization= School of Information Technology, Halmstad University,
    city=Halmstad, postcode=30118, country=Sweden'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] \cormark[1] 1]机构=中国科学院半导体研究所，城市=北京，邮政编码=100083，国家=中国 2]机构=中国科学院大学材料科学与光电工程中心
    $\&amp;$ 微电子学院，城市=北京，邮政编码=100083，国家=中国 3]机构=新疆大学软件学院，城市=新疆，邮政编码=830000，国家=中国 4]机构=哈尔姆斯塔德大学信息技术学院，城市=哈尔姆斯塔德，邮政编码=30118，国家=瑞典'
- en: \cortext
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \cortext
- en: '[1]Corresponding author.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]通讯作者。'
- en: 'Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遮挡行人重识别与深度学习：综述与展望
- en: Enhao Ning ningenhao@163.com    Changshuo Wang wangchangshuo@semi.ac.cn    Huang
    Zhang zhhh1998@outlook.com    Xin Ning ningxin@semi.ac.cn    Prayag Tiwari prayag.tiwari@ieee.org
    [ [ [ [
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Enhao Ning ningenhao@163.com    Changshuo Wang wangchangshuo@semi.ac.cn    Huang
    Zhang zhhh1998@outlook.com    Xin Ning ningxin@semi.ac.cn    Prayag Tiwari prayag.tiwari@ieee.org
    [ [ [ [
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Person re-identification (Re-ID) technology plays an increasingly crucial role
    in intelligent surveillance systems. Widespread occlusion significantly impacts
    the performance of person Re-ID. Occluded person Re-ID refers to a pedestrian
    matching method that deals with challenges such as pedestrian information loss,
    noise interference, and perspective misalignment. It has garnered extensive attention
    from researchers. Over the past few years, several occlusion-solving person Re-ID
    methods have been proposed, tackling various sub-problems arising from occlusion.
    However, there is a lack of comprehensive studies that compare, summarize, and
    evaluate the potential of occluded person Re-ID methods in detail. In this review,
    we start by providing a detailed overview of the datasets and evaluation scheme
    used for occluded person Re-ID. Next, we scientifically classify and analyze existing
    deep learning-based occluded person Re-ID methods from various perspectives, summarizing
    them concisely. Furthermore, we conduct a systematic comparison among these methods,
    identify the state-of-the-art approaches, and present an outlook on the future
    development of occluded person Re-ID.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 行人重识别（Re-ID）技术在智能监控系统中扮演着越来越重要的角色。广泛的遮挡显著影响了行人重识别的性能。遮挡行人重识别是指处理行人信息丢失、噪声干扰和视角失配等挑战的行人匹配方法。该领域已引起了研究人员的广泛关注。在过去几年中，提出了多种解决遮挡问题的行人重识别方法，解决了由遮挡引起的各种子问题。然而，目前缺乏对遮挡行人重识别方法进行全面比较、总结和详细评估的研究。在本综述中，我们首先提供了关于遮挡行人重识别所使用的数据集和评估方案的详细概述。接下来，我们从不同角度科学分类和分析现有基于深度学习的遮挡行人重识别方法，并简要总结。进一步地，我们对这些方法进行系统比较，识别出最先进的方法，并展望遮挡行人重识别的未来发展。
- en: 'keywords:'
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Occluded Person Re-identification \sepLiterature Survey and Perspectives \sepMultimodal
    Person Re-identification \sep3D Person Re-identification.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 遮挡行人重识别 \sep 文献综述与展望 \sep 多模态行人重识别 \sep 3D行人重识别。
- en: 1 Introduction
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: With the increasing integration and intelligence of surveillance equipment (Bedagkar-Gala
    & Shah,, [2014](#bib.bib1)) in recent years, person re-identification (Re-ID)
    technology has significantly advanced. This technology finds extensive application
    in sensitive and specialized domains, such as medicine, rescue operations, criminal
    investigations, and surveillance. These fields often operate in complex and dynamic
    environments. Consequently, the rapid and accurate localization and identification
    of specific targets in multi-camera occlusion scenarios hold immense practical
    significance.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着监控设备的集成度和智能化程度的提高（Bedagkar-Gala & Shah,, [2014](#bib.bib1)），人员重识别（Re-ID）技术取得了显著进展。这项技术在医疗、救援行动、刑事调查和监控等敏感和专业领域中得到了广泛应用。这些领域通常在复杂和动态的环境中运行。因此，在多摄像头遮挡场景中快速而准确地定位和识别特定目标具有极大的实际意义。
- en: 'Given the complexity and variability of real-life scenes, where people and
    objects move randomly, and surveillance devices typically cover wide areas, the
    likelihood of occluded individuals is high. Occlusion can have a severe impact
    on visual information, rendering the affected features unreliable. Occlusion can
    occur due to object interference, changes in pedestrian pose, clothing, and perspective.
    In early pedestrian representations, researchers primarily relied on basic, local
    visual attributes extracted from images, such as color, texture, edges, and corner
    points. These features capture geometric shapes and pixel distributions in images
    but are highly sensitive to external factors, lacking robustness and generalization.
    The development of deep learning has introduced high-level visual features. Compared
    with low-level visual features, high-level features are more adaptive to occlusions,
    noises and pose changes, and have stronger robustness in complex environments.
    Consequently, numerous researchers have developed a multitude of methods to address
    the prevalent occlusion problem. In general, the occlusion problem is divided
    into three sub-problems: (1) Noise problem. The problem of interference by multiple
    and mixed information from the features in the acquisition of complex scenes.
    (2) Missing problem. The problem of incomplete pedestrian features is due to only
    a part of the pedestrian being captured. (3) Alignment problem. Owing to the change
    in posture, perspective, and position, the features cannot correspond one-to-one,
    which causes distraction, shared location misalignment, and other issues. The
    study of occlusion also involves the separation of humans and backgrounds to extract
    human features as the core. Methods to extract fine-grained, highly discriminative,
    and more essential features with reference and value have also been studied.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于现实场景的复杂性和变化性，人和物体的随机移动，加上监控设备通常覆盖广泛区域，遮挡现象的可能性很高。遮挡会严重影响视觉信息，使受影响的特征变得不可靠。遮挡可能由于物体干扰、行人姿势、衣物和视角的变化而发生。在早期的行人表示中，研究人员主要依赖从图像中提取的基本局部视觉属性，如颜色、纹理、边缘和角点。这些特征捕捉图像中的几何形状和像素分布，但对外部因素非常敏感，缺乏鲁棒性和泛化能力。深度学习的发展引入了高级视觉特征。与低级视觉特征相比，高级特征对遮挡、噪声和姿势变化的适应性更强，在复杂环境中具有更强的鲁棒性。因此，许多研究人员开发了大量方法来解决普遍存在的遮挡问题。一般而言，遮挡问题被分为三个子问题：（1）噪声问题。由于复杂场景中多个和混合信息的干扰而产生的特征问题。（2）缺失问题。由于仅捕捉到部分行人而导致的行人特征不完整问题。（3）对齐问题。由于姿势、视角和位置的变化，特征无法一一对应，导致干扰、共享位置对齐问题等。遮挡研究还涉及人和背景的分离，以提取以人为核心的特征。对提取细粒度、高度区分性和更本质特征的方法也进行了研究。
- en: 'We want to identify the current state-of-the-art and limitations of existing
    methods and discover unexplored areas. Specifically, we present methods for dealing
    with occluded person Re-ID that were submitted in top international journals or
    conferences before 2023\. We classify deep learning-based occluded person Re-ID
    according to the network structure of extracted features (CNN-based, transformer-based,
    and hybrid structure-based), the way features are extracted (uni-modal and multi-modal),
    and the hierarchical structure of features (2d and 3d). ( see Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Occluded Person Re-Identification with Deep Learning:
    A Survey and Perspectives")). First, due to the powerful performance of convolutional
    neural networks (CNNs) in image matching tasks, CNN-based methods have become
    one of the mainstream methods to deal with occlusion problems in person Re-ID.
    Therefore, we treat the cnn-based methods as the first class of methods to deal
    with the occlusion problem. Secondly, based on the success of transformer in the
    field of natural language, in recent years, vit has also been widely used to deal
    with the occlusion problem in pedestrian re-identification with good results.
    Therefore, we treat transformer-based methods as the second category. The third
    class of methods are some composite methods. For example, the complementary nature
    of CNN and vit is exploited to form a hybrid structure. The fourth and fifth class
    of methods are based on 3D and multimodal to deal with the occlusion problem in
    person Re-ID. They deal with more scenarios and are a relatively novel approach.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '我们希望识别现有方法的最新技术水平和局限性，并发现未被探索的领域。具体而言，我们介绍了处理遮挡人物重识别的方法，这些方法在2023年前提交至顶级国际期刊或会议。我们根据特征提取的网络结构（基于CNN的、基于transformer的和混合结构的）、特征提取的方式（单模态和多模态）以及特征的层次结构（2D和3D）对基于深度学习的遮挡人物重识别进行分类。
    （见图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Occluded Person Re-Identification
    with Deep Learning: A Survey and Perspectives")）。首先，由于卷积神经网络（CNN）在图像匹配任务中的强大性能，基于CNN的方法已成为处理人物重识别遮挡问题的主流方法之一。因此，我们将基于CNN的方法视为处理遮挡问题的第一类方法。其次，基于transformer在自然语言处理领域的成功，近年来，vit也被广泛应用于行人重识别中的遮挡问题，并取得了良好的结果。因此，我们将基于transformer的方法视为第二类。第三类方法是一些复合方法。例如，利用CNN和vit的互补性形成混合结构。第四类和第五类方法基于3D和多模态处理人物重识别中的遮挡问题。这些方法处理更多场景，并且是一种相对新颖的方法。'
- en: '![Refer to caption](img/376919f96fa6d88bd2e42f194f5f8ead.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/376919f96fa6d88bd2e42f194f5f8ead.png)'
- en: 'Figure 1: Overall structure of the survey.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：调查的整体结构。
- en: 'Table 1: The summary of person-reported surveys in recent years.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：近年来人物重识别调查的总结。
- en: '| Survey | Venue |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 调查 | 会议 |'
- en: '| A survey of approaches and trends in person Re-ID (Bedagkar-Gala & Shah,,
    [2014](#bib.bib1)) | IVC2014 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 人物重识别方法和趋势调查 (Bedagkar-Gala & Shah,, [2014](#bib.bib1)) | IVC2014 |'
- en: '| Person Re-ID Past, Present and Future (Zheng et al.,, [2016](#bib.bib135))
    | arXiv2016 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 人物重识别的过去、现在与未来 (Zheng et al.,, [2016](#bib.bib135)) | arXiv2016 |'
- en: '| A systematic evaluation and benchmark for person Re-ID: Features, metrics,
    and datasets (Gou et al.,, [2018](#bib.bib19)) | TPAMI2019 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 人物重识别的系统评估与基准：特征、度量标准和数据集 (Gou et al.,, [2018](#bib.bib19)) | TPAMI2019 |'
- en: '| Beyond intra-modality discrepancy: A comprehensive survey of heterogeneous
    person Re-ID ([Wang et al., 2019b,](#bib.bib102) ) | arXiv2019 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 超越内部模态差异：异构人物重识别的全面调查 ([Wang et al., 2019b,](#bib.bib102)) | arXiv2019 |'
- en: '| A Survey of Open-World Person ReIdentification (Leng et al.,, [2019](#bib.bib44))
    | TCSVT2020 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 开放世界人物重识别调查 (Leng et al.,, [2019](#bib.bib44)) | TCSVT2020 |'
- en: '| Survey on Reliable Deep Learning-Based person Re-ID Models: Are We There
    Yet? (Lavi et al.,, [2020](#bib.bib43)) | arXiv2020 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 可靠的基于深度学习的人物重识别模型调查：我们是否达到了目标？ (Lavi et al.,, [2020](#bib.bib43)) | arXiv2020
    |'
- en: '| Deep Learning for Person Reidentification: A Survey and Outlook ([Ye et al.,
    2021b,](#bib.bib120) ) | TPAMI2021 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 基于深度学习的人物重识别：调查与展望 ([Ye et al., 2021b,](#bib.bib120)) | TPAMI2021 |'
- en: '| SSS-PR: A short survey of surveys in person Re-ID (Yaghoubi et al.,, [2021](#bib.bib113))
    | PRL2021 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| SSS-PR：人物重识别调查的简短调查 (Yaghoubi et al.,, [2021](#bib.bib113)) | PRL2021 |'
- en: '| Deep learning-based person Re-ID methods: A survey and outlook of recent
    works (Ming et al.,, [2022](#bib.bib64)) | IVC2022 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 基于深度学习的人物重识别方法：近期工作的调查与展望 (Ming et al.,, [2022](#bib.bib64)) | IVC2022 |'
- en: '| Deep Learning-based Occluded person Re-ID: A Survey (Peng et al.,, [2022](#bib.bib71))
    | arXiv2022 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 基于深度学习的遮挡人物重识别：调查 (Peng et al.,, [2022](#bib.bib71)) | arXiv2022 |'
- en: 'In general, the contributions of this study are as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，本研究的贡献如下：
- en: 1) This study focuses on addressing the occlusion problem in person Re-ID models,
    which is crucial for achieving high accuracy and robustness. We present a scientific
    and comprehensive review of past and current state-of-the-art approaches.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 本研究重点解决行人再识别模型中的遮挡问题，这对实现高精度和鲁棒性至关重要。我们对过去和当前最先进的方法进行了科学和全面的综述。
- en: 2) The current review of person Re-ID methods lacks sufficient coverage of approaches
    based on ViT. Given the excellent performance of ViT in occluded person Re-ID,
    we include a discussion of this method and its hybrid variants in our study, offering
    researchers new ideas and options for addressing the occlusion problem.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 目前对行人再识别方法的综述缺乏对基于ViT方法的充分覆盖。鉴于ViT在遮挡行人再识别中的优秀表现，我们在研究中包括了对这种方法及其混合变体的讨论，为研究人员提供了应对遮挡问题的新思路和选项。
- en: 3) We creatively incorporate 3D person Re-ID and multimodal person Re-ID, which
    have become popular in recent years. These novel methods can better solve the
    occlusion problem by utilizing additional depth or modal information, thus improving
    the performance and reliability of person Re-ID.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 我们创造性地融入了近年来流行的3D行人再识别和多模态行人再识别。这些新方法通过利用额外的深度或模态信息，更好地解决遮挡问题，从而提高行人再识别的性能和可靠性。
- en: 4) We anticipate advancements in occluded person Re-ID and firmly believe that
    continued research and innovation will lead to the development of more effective
    methods and technologies for addressing the occlusion problem. These advancements
    will inspire and drive progress in the field of person Re-ID.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 我们预期在遮挡行人再识别（Re-ID）方面会取得进展，并坚信持续的研究和创新将导致更有效的方法和技术的发展，以解决遮挡问题。这些进展将激发并推动行人再识别领域的进步。
- en: 2 Literature review
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 文献综述
- en: 'In the field of person Re-ID, there is a relative scarcity of specialized reviews
    compared to methodological articles. And they all focus on specific problems.
    These surveys are listed in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Occluded
    Person Re-Identification with Deep Learning: A Survey and Perspectives"). Bedagkar-Gala
    & Shah, ([2014](#bib.bib1)) focuses on the challenges of person Re-ID and divides
    it into open-set Re-ID and closed-set Re-ID based on the fixity of the gallery.
    Zheng et al., ([2016](#bib.bib135)) divides the methods of person Re-ID into methods
    for images and methods for videos based on the matching strategy. Gou et al.,
    ([2018](#bib.bib19)) provides a more detailed study of the features, metrics,
    and datasets of person Re-ID. [Wang et al., 2019b](#bib.bib102) focuses on heterogeneous
    person Re-ID. According to the application scenario, it classify the methods into
    four categories — low-resolution, infrared,sketch, and text. Leng et al., ([2019](#bib.bib44))
    focuses on open-world Re-ID tasks. Lavi et al., ([2020](#bib.bib43)) classifies
    Re-ID into single feature learning based approaches and multi-feature learning
    based approaches based on feature learning strategies. [Ye et al., 2021b](#bib.bib120)
    provides a more detailed explanation of Re-ID for open and closed settings, and
    introduces methods such as transmembrane states, unsupervised. Yaghoubi et al.,
    ([2021](#bib.bib113)) provides a more multidimensional classification of the person
    Re-ID problem. Ming et al., ([2022](#bib.bib64)) classifies the methods of person
    Re-ID into four categories based on metric learning and representation learning,
    and adds the latest methods. Peng et al., ([2022](#bib.bib71)) focuses on image-based
    obscured person Re-ID methods. However, these investigations are inevitably affected
    by a number of inherent limitations. Considering the widespread existence of the
    occlusion problem in pedestrian recognition, research on occluded person Re-ID
    is essential. Therefore, we provide an in-depth summary and comprehensive analysis
    of methods and prospects in occluded person Re-ID to advance future developments.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在行人重识别领域，相较于方法学文章，专门的综述文章相对较少，并且它们都聚焦于具体问题。这些综述列于表[1](#S1.T1 "表 1 ‣ 1 引言 ‣ 基于深度学习的遮挡行人重识别：综述与展望")。Bedagkar-Gala
    & Shah, ([2014](#bib.bib1)) 关注行人重识别的挑战，并根据画廊的固定性将其分为开放集重识别和闭合集重识别。Zheng 等人 ([2016](#bib.bib135))
    根据匹配策略将行人重识别的方法分为图像方法和视频方法。Gou 等人 ([2018](#bib.bib19)) 更详细地研究了行人重识别的特征、度量和数据集。[Wang
    等人, 2019b](#bib.bib102) 关注异质行人重识别。根据应用场景，它将方法分类为四类——低分辨率、红外、草图和文本。Leng 等人 ([2019](#bib.bib44))
    关注开放世界重识别任务。Lavi 等人 ([2020](#bib.bib43)) 根据特征学习策略将重识别分类为单特征学习方法和多特征学习方法。[Ye 等人,
    2021b](#bib.bib120) 对开放和闭合设置下的重识别提供了更详细的解释，并介绍了如跨膜状态、无监督等方法。Yaghoubi 等人 ([2021](#bib.bib113))
    提供了对行人重识别问题的多维分类。Ming 等人 ([2022](#bib.bib64)) 根据度量学习和表示学习将行人重识别方法分为四类，并添加了最新的方法。Peng
    等人 ([2022](#bib.bib71)) 关注基于图像的遮挡行人重识别方法。然而，这些研究不可避免地受到一些固有限制的影响。考虑到遮挡问题在行人识别中的广泛存在，对遮挡行人重识别的研究是至关重要的。因此，我们提供了对遮挡行人重识别方法和前景的深入总结和全面分析，以推动未来的发展。
- en: 3 Datasets and Evaluation Protocols
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数据集和评估协议
- en: 3.1 Datasets
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集
- en: 'Occluded person Re-ID datasets can be divided into two categories: partial
    person and occluded person Re-ID datasets. The pedestrian images of the occluded
    person Re-ID datasets have occlusion information interference and are not cropped.
    The pedestrian image portion of the partial person Re-ID dataset is present and
    artificially cropped. Examples of partial/occluded person Re-ID datasets are shown
    in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Datasets ‣ 3 Datasets and Evaluation Protocols
    ‣ Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 遮挡行人重识别数据集可以分为两类：部分行人和遮挡行人重识别数据集。遮挡行人重识别数据集的行人图像有遮挡信息干扰且未裁剪。部分行人重识别数据集的行人图像部分存在并且经过人工裁剪。部分/遮挡行人重识别数据集的示例如图[2](#S3.F2
    "图 2 ‣ 3.1 数据集 ‣ 3 数据集和评估协议 ‣ 基于深度学习的遮挡行人重识别：综述与展望")所示。
- en: Occluded-DukeMTMC (Miao et al.,, [2019](#bib.bib62)) was collected from DukeMTMC-reID
    (Zheng et al.,, [2017](#bib.bib140)), containing 15,618 training images of 708
    pedestrians, 2,210 query images of 519 pedestrians, and 17,661 gallery images
    of 1,110 pedestrians for testing. Of these images, 9$\%$ of the training set,
    100$\%$ of the query set, and 10$\%$ of the gallery are occluded images. Obstacles
    include cars, bicycles, trees, and other pedestrians, adding complexity to the
    dataset.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Occluded-DukeMTMC（Miao et al., [2019](#bib.bib62)）是从DukeMTMC-reID（Zheng et al.,
    [2017](#bib.bib140)）收集的，包含15,618张708名行人的训练图像，519名行人的2,210张查询图像，以及1,110名行人的17,661张图库图像用于测试。其中，训练集的9$\%$，查询集的100$\%$，以及图库的10$\%$是遮挡图像。障碍物包括汽车、自行车、树木和其他行人，增加了数据集的复杂性。
- en: P-ETHZ ([Zheng et al., 2015b,](#bib.bib138) ) is an image-based occluded person
    Re-ID dataset, modified by ETHZ (Ess et al.,, [2008](#bib.bib13)). It has 3,897
    images containing 85 pedestrian identities with 1 to 30 full-body and occluded
    pedestrian images per identity.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: P-ETHZ（[Zheng et al., 2015b](#bib.bib138)）是一个基于图像的遮挡行人重识别数据集，由ETHZ（Ess et al.,
    [2008](#bib.bib13)）修改而来。它包含3,897张图像，涵盖85个行人身份，每个身份有1到30张全身和遮挡图像。
- en: P-DukeMTMC-reID (Zhuo et al.,, [2018](#bib.bib149)) was modified from DukeMTMC-reID
    (Zheng et al.,, [2017](#bib.bib140)), containing a total of 24,143 images of 1,299
    pedestrians, and each identity has a full-body and occlusion image; the pedestrian
    in the image is occluded by different objects, such as other pedestrians, cars,
    and signage.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: P-DukeMTMC-reID（Zhuo et al., [2018](#bib.bib149)）是从DukeMTMC-reID（Zheng et al.,
    [2017](#bib.bib140)）修改而来的，包含1,299名行人的总计24,143张图片，每个身份有全身图像和遮挡图像；图像中的行人被不同物体遮挡，如其他行人、汽车和标志。
- en: Occluded-REID ([Zheng et al., 2015b,](#bib.bib138) ) has 2,000 images of 200
    pedestrians, each pedestrian corresponding to 5 occlusion and 5 whole body images,
    collected from Sun Yat-sen University. The dataset includes different viewpoints
    and types of severe occlusion, which challenges person Re-ID.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Occluded-REID（[Zheng et al., 2015b](#bib.bib138)）包含2,000张200名行人的图像，每名行人有5张遮挡图像和5张全身图像，数据集来自中山大学。该数据集包括不同的视角和严重遮挡类型，给行人重识别带来了挑战。
- en: Occluded-DukeMTMC-VideoReID (Hou et al.,, [2021](#bib.bib30)) was reorganized
    from the DukeMTMC-VideoReID (Wu et al.,, [2018](#bib.bib108)) dataset. The training
    set contains 1,702 trajectory segments covering 702 pedestrians, the test set
    queries cover 661 pedestrians, and the gallery covers 1,110\. More than 70$\%$
    of the videos are occluded, including different perspectives and a variety of
    obstacles, such as cars, trees, bicycles, and other pedestrians.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Occluded-DukeMTMC-VideoReID（Hou et al., [2021](#bib.bib30)）是从DukeMTMC-VideoReID（Wu
    et al., [2018](#bib.bib108)）数据集中重新组织的。训练集包含1,702个轨迹片段，覆盖702名行人，测试集查询覆盖661名行人，图库覆盖1,110名行人。超过70$\%$的视频是遮挡的，包括不同的视角和各种障碍物，如汽车、树木、自行车和其他行人。
- en: Partial-ReID ([Zheng et al., 2015b,](#bib.bib138) ) has 600 images of 60 pedestrians,
    5 partial and 5 full-body images for each pedestrian. Using the visible parts,
    they are manually cropped to form new partial images. The images are collected
    from different perspectives, backgrounds and occlusions in a university campus.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Partial-ReID（[Zheng et al., 2015b](#bib.bib138)）包含60名行人的600张图片，每名行人有5张部分图像和5张全身图像。使用可见部分，人工裁剪成新的部分图像。这些图像从大学校园中的不同角度、背景和遮挡中收集。
- en: Partial-iLIDS (He et al.,, [2018](#bib.bib24)) was derived from iLIDS (Zheng
    et al.,, [2011](#bib.bib137)) and contains 238 images of 119 pedestrians. Each
    pedestrian corresponds to one manually cropped non-occluded partial image and
    one full-body image. The partial image is used as a query, and the full-body image
    is used as a search library. It was shot by multiple non-overlapping cameras,
    mostly for test sets.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Partial-iLIDS（He et al., [2018](#bib.bib24)）源于iLIDS（Zheng et al., [2011](#bib.bib137)），包含119名行人的238张图片。每名行人对应一张人工裁剪的非遮挡部分图像和一张全身图像。部分图像用作查询，全身图像用作搜索库。它由多个非重叠的摄像机拍摄，主要用于测试集。
- en: Partial-CAVIAR (He et al.,, [2018](#bib.bib24)) was derived from CAVIAR (Cheng
    et al.,, [2011](#bib.bib9)) and contains 142 images of 72 pedestrians. The partial
    map is generated by randomly picking half of the overall image of each pedestrian.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Partial-CAVIAR（He et al., [2018](#bib.bib24)）源于CAVIAR（Cheng et al., [2011](#bib.bib9)），包含72名行人的142张图片。部分图像通过随机选择每名行人整体图像的一半生成。
- en: P-CUHK03 (Kim & Yoo,, [2017](#bib.bib40)) was constructed based on CUHK03 (Li
    et al.,, [2014](#bib.bib45)), with a total of 1,360 pedestrian images, wherein
    15,080 images corresponding to 1,160 pedestrians are used as a training set, and
    the remaining 100 pedestrians are used as a validation and test set. Two of the
    images are selected to generate 10 local body query images with a spatial area
    ratio, and the remaining three images are used as whole body gallery images.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: P-CUHK03（Kim & Yoo，[2017](#bib.bib40)）是基于CUHK03（Li et al.，[2014](#bib.bib45)）构建的，包含1,360张行人图像，其中15,080张图像对应于1,160名行人用于训练集，其余100名行人用于验证和测试集。选择其中两张图像生成10张具有空间区域比的局部身体查询图像，其余三张图像用作全身图库图像。
- en: '![Refer to caption](img/77d33192e282a611ccbf5fcb489b0ec3.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/77d33192e282a611ccbf5fcb489b0ec3.png)'
- en: 'Figure 2: Examples of four commonly used occluded person Re-ID datasets.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：四种常用遮挡行人Re-ID数据集的示例。
- en: 3.2 Evaluation Protocols
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 评估协议
- en: In the field of occluded person Re-ID, the commonly used evaluation metrics
    are Cumulative Matching Characteristic (CMC) curves and mean Average Precision
    (mAP).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在遮挡行人Re-ID领域，常用的评估指标是累积匹配特征（CMC）曲线和平均准确率（mAP）。
- en: CMC curves are based on the principle of ranking the similarity between the
    query image and the image library, and the higher the top image, the higher the
    similarity with the query image. Then, the top-k accuracy ${\rm\emph{ACC}}^{\emph{k}}$
    of the query image is calculated based on this ranking. If the first k samples
    contain the query target, then ${\rm\emph{ACC}}^{\emph{k}}$ is 1, $\emph{k}\in\{1,2,3...\}$.
    Otherwise, ${\rm\emph{ACC}}^{\emph{k}}$ is 0 . Finally, the ${\rm\emph{ACC}}^{\emph{k}}$curves
    for all targets are summed and divided by the total number of targets to obtain
    CMC-k.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: CMC曲线基于查询图像与图像库之间的相似性排名原则，排名越高的图像与查询图像的相似性越高。然后，根据这种排名计算查询图像的top-k准确率${\rm\emph{ACC}}^{\emph{k}}$。如果前k个样本包含查询目标，则${\rm\emph{ACC}}^{\emph{k}}$为1，$\emph{k}\in\{1,2,3...\}$。否则，${\rm\emph{ACC}}^{\emph{k}}$为0。最后，所有目标的${\rm\emph{ACC}}^{\emph{k}}$曲线进行求和并除以目标总数，以获得CMC-k。
- en: mAP better reflects the degree to which all correct target pictures are at the
    top of the sorted list. Compared with the CMC curve, it can more comprehensively
    measure the performance of Re-ID algorithms, where P is the precision rate, which
    refers to the proportion of correct samples among all samples. It reflects the
    accuracy of the correct samples in the output. The AP is the average of all correct
    samples predicted by the model. It reflects how well the model works on a single
    category and is the average of the accuracy of each correct prediction. Since
    there is more than one class in the recognition, the average AP value needs to
    be calculated for all classes, so the average accuracy of each class is added
    and divided by the total number of classes to obtain mAP.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: mAP更好地反映了所有正确目标图像在排序列表顶部的程度。与CMC曲线相比，它可以更全面地衡量Re-ID算法的性能，其中P是精度率，指的是所有样本中正确样本的比例。它反映了输出中正确样本的准确性。AP是模型预测的所有正确样本的平均值。它反映了模型在单一类别上的表现，是每个正确预测的准确率的平均值。由于识别中存在多个类别，需要计算所有类别的平均AP值，因此每个类别的平均准确率相加并除以类别总数以获得mAP。
- en: The CMC curve cannot consider the hits of the samples with lower rankings, while
    mAP takes all samples into account. Therefore, they are important and complementary.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: CMC曲线不能考虑排名较低样本的命中，而mAP考虑了所有样本。因此，它们是重要的且互补的。
- en: 4 Deep Learning Methods
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度学习方法
- en: 4.1 Based on CNN
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基于CNN
- en: Convolutional neural networks (CNNs) have emerged as one of the leading methods
    for learning pedestrian representations from RGB images. By using local perceptual
    fields and learning filters, CNNs can extract powerful features that capture regional
    information about local features of pedestrians. These features are then compressed
    and mapped to higher-level representations. Researchers have refined them to be
    usable for pedestrian matching tasks in complex realistic scenarios. We classify
    it into local feature learning, relational representation, mixing methods, and
    other methods.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）已成为从RGB图像中学习行人表征的领先方法之一。通过使用局部感知域和学习滤波器，CNN能够提取捕捉行人局部特征区域信息的强大特征。这些特征随后被压缩并映射到更高层次的表征。研究人员已将其精炼以适用于复杂现实场景中的行人匹配任务。我们将其分类为本地特征学习、关系表示、混合方法和其他方法。
- en: 4.1.1 Local Feature Learning
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 本地特征学习
- en: 'Local feature-learning performs better at handling regional features, and it
    has unique advantages for occlusion region recognition and location compared with
    global features. According to its implementation of different local feature methods,
    we divide them into human segmentation, pose estimation, human parsing, attribute
    annotation, and hybrid methods ( see Figure [3](#S4.F3 "Figure 3 ‣ 4.1.1 Local
    Feature Learning ‣ 4.1 Based on CNN ‣ 4 Deep Learning Methods ‣ Occluded Person
    Re-Identification with Deep Learning: A Survey and Perspectives") ).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '局部特征学习在处理区域特征时表现更好，相比于全局特征，它在遮挡区域识别和定位上具有独特的优势。根据不同局部特征方法的实现，我们将其分为人体分割、姿态估计、人脸解析、属性注释和混合方法（见图
    [3](#S4.F3 "Figure 3 ‣ 4.1.1 Local Feature Learning ‣ 4.1 Based on CNN ‣ 4 Deep
    Learning Methods ‣ Occluded Person Re-Identification with Deep Learning: A Survey
    and Perspectives")）。'
- en: 'Body Segmentation. By leveraging the characteristic of pedestrians walking
    upright, our method extracts improved local features through the segmentation
    of the original image or feature map. The segmentation results can take the form
    of stripes, fixed regions, or small patches ( see Figure [4](#S4.F4 "Figure 4
    ‣ 4.1.1 Local Feature Learning ‣ 4.1 Based on CNN ‣ 4 Deep Learning Methods ‣
    Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives")
    ).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '身体分割。通过利用行人直立行走的特性，我们的方法通过对原始图像或特征图的分割来提取改进的局部特征。分割结果可以表现为条纹、固定区域或小块（见图 [4](#S4.F4
    "Figure 4 ‣ 4.1.1 Local Feature Learning ‣ 4.1 Based on CNN ‣ 4 Deep Learning
    Methods ‣ Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives")）。'
- en: However, segmentation does not have the process of identifying occlusions, so
    it is sensitive to noise. CBDB-Net (Tan et al.,, [2021](#bib.bib85)) evenly divides
    the strips on the feature map and discards each strip one by one to output multiple
    incomplete feature maps, which forces the model to learn a more robust pedestrian
    representation in an environment with incomplete information. The DPPR (Kim &
    Yoo,, [2017](#bib.bib40)) predefines thirteen bounding boxes for the whole-body
    image, including the whole image, half-body image, and horizontal part image,
    and extracts features from each part. At the same time, an attention-based matching
    mechanism is introduced to make the feature weight of the same body part larger,
    which can alleviate the information loss caused by occlusion. At the same time,
    an attention-based matching mechanism is introduced to make the feature weight
    of the same body part larger, which can alleviate the information loss caused
    by occlusion. OCNet (Kim et al.,, [2022](#bib.bib41)) introduces a relationship-based
    approach to deal with occlusion problems. OCNet (Kim et al.,, [2022](#bib.bib41))
    divides the feature map horizontally into top and bottom features and takes 1/4
    of the middle width as the central feature. Then, it is put into the relational
    adaptive module consisting of two shared layers together with the global feature
    map. The alignment problem between regional features is handled by relations,
    and weights are introduced to suppress noise interference.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，分割没有识别遮挡的过程，因此对噪声敏感。CBDB-Net (Tan et al., [2021](#bib.bib85)) 将特征图上的条带均匀划分，并逐一丢弃每个条带以输出多个不完整的特征图，这迫使模型在信息不完整的环境中学习更鲁棒的人体表示。DPPR
    (Kim & Yoo, [2017](#bib.bib40)) 预定义了十三个用于全身图像的边界框，包括整个图像、半身图像和水平部分图像，并从每个部分提取特征。同时，引入了一种基于注意力的匹配机制，使得相同身体部位的特征权重更大，从而减轻了由遮挡造成的信息丢失。同时，引入了一种基于注意力的匹配机制，使得相同身体部位的特征权重更大，从而减轻了由遮挡造成的信息丢失。OCNet
    (Kim et al., [2022](#bib.bib41)) 引入了一种基于关系的方法来处理遮挡问题。OCNet (Kim et al., [2022](#bib.bib41))
    将特征图水平划分为上部和下部特征，并以中间宽度的 1/4 作为中心特征。然后，它与全局特征图一起输入到由两个共享层组成的关系自适应模块中。通过关系处理区域特征之间的对齐问题，并引入权重以抑制噪声干扰。
- en: '![Refer to caption](img/5d21802d43ea6a9c254dec94b902b811.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5d21802d43ea6a9c254dec94b902b811.png)'
- en: 'Figure 3: Four different local feature learning methods: (a) indicates pose
    estimation. (b) indicates semantic segmentation. (c) indicates attribute annotations.
    (d) indicates the mixing method.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 四种不同的局部特征学习方法：（a）表示姿态估计。（b）表示语义分割。（c）表示属性注释。（d）表示混合方法。'
- en: Pose Estimation. Pose estimation extracts semantic information at the image
    pose level by exploiting the highly structured human skeleton. The interference
    of noise is suppressed in a guided or fused manner.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 姿态估计。姿态估计通过利用高度结构化的人体骨架来提取图像姿态级别的语义信息。噪声干扰以引导或融合的方式得到抑制。
- en: HOReID ([Wang et al., 2020a,](#bib.bib93) ) introduced a learnable relational
    matrix. The human body key points obtained from the pose estimation are regarded
    as nodes in the graph, and finally, a topology graph is formed to suppress noise
    interference. PMFB (Miao et al.,, [2021](#bib.bib63)) uses pose estimation to
    obtain confidence and coordinates of human keypoints. Then, a threshold is set
    to filter the occluded regions. Finally, the visible part is used to constrain
    the feature response at channel level to solve the occlusion problem. PGMANet
    (Zhai et al.,, [2021](#bib.bib121)) generates an attention mask using a human
    heat map. The interference of noise is removed jointly by the dot product of feature
    maps and guidance of higher-order relations.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: HOReID（[Wang et al., 2020a](#bib.bib93)）引入了一种可学习的关系矩阵。从姿态估计中获得的人体关键点被视为图中的节点，最终形成一个拓扑图以抑制噪声干扰。PMFB（Miao
    et al., [2021](#bib.bib63)）使用姿态估计来获取人体关键点的置信度和坐标。然后，设置一个阈值来过滤遮挡区域。最后，使用可见部分来约束通道级别的特征响应，以解决遮挡问题。PGMANet（Zhai
    et al., [2021](#bib.bib121)）使用人体热图生成注意力掩码。通过特征图的点积和高阶关系的引导共同去除噪声干扰。
- en: 'Researchers generally use pose estimation in two directions: 1) to obtain semantic
    features through pose estimation, identify noise points, and better remove noise
    interference, and 2) to localize human regions through pose estimation and thus
    solve the problem of alignment and local feature extraction. AACN (Xu et al.,,
    [2018](#bib.bib111)) uses pose points to locate pedestrian body regions and introduces
    a posture-guided visibility score to separate occlusions. DAReID (Xu et al.,,
    [2021](#bib.bib112)) adopts a dual-branch structure, in which the mask branch
    extracts more discriminative local features based on the spatial attention module
    guided by pose estimation, and the global branch enhances the representation of
    human discriminative information through feature activation. DSA-reID (Zhang et al.,,
    [2019](#bib.bib128)) estimates dense semantic information at the 2D level by a
    pre-trained DensePose (Güler et al.,, [2018](#bib.bib20)) model and maps a set
    of dense 3D semantic alignment components. Local features are extracted by integrating
    neighboring components. Finally, the global and local features are fused into
    the final feature. PGFL-KD (Zheng et al.,, [2021](#bib.bib133)) takes the local
    features of the semantic layer as queries and looks for more prominent foreground
    regions in the feature map to obtain enhanced foreground features. Based on this
    feature, interactive training and knowledge distillation are performed to constrain
    the learning of the backbone network. ACSAP ([He et al., 2021c,](#bib.bib29) )
    uses pose keypoints to guide the adversarial generation network to remove noise
    interference by weakening the spatial relationship between the front and back
    blocks. PDC (Su et al.,, [2017](#bib.bib79)) obtains 6 body regions according
    to 14 key points of pose, rotates and scales each part, and uses an improved PTN
    network to learn the parameters of affine transformation. These local features
    are automatically placed at certain locations in the drawing to resolve alignment
    issues. PVPM ([Gao et al., 2020b,](#bib.bib18) ) trains a visibility predictor
    based on the correspondence between visibility parts. After that, the alignment
    problem is solved by generating part pseudo-labels through graph matching.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员通常在两个方向上使用姿态估计：1）通过姿态估计获得语义特征，识别噪声点，并更好地去除噪声干扰；2）通过姿态估计定位人体区域，从而解决对齐和局部特征提取的问题。AACN（Xu
    等，[2018](#bib.bib111)）使用姿态点定位行人身体区域，并引入姿态引导的可见性评分来分离遮挡。DAReID（Xu 等，[2021](#bib.bib112)）采用双分支结构，其中掩模分支基于姿态估计引导的空间注意模块提取更具辨别性的局部特征，全球分支则通过特征激活增强人体辨别信息的表现。DSA-reID（Zhang
    等，[2019](#bib.bib128)）通过预训练的 DensePose（Güler 等，[2018](#bib.bib20)）模型在2D层面估计密集语义信息，并映射一组密集的3D语义对齐组件。通过整合邻近组件来提取局部特征。最后，将全球和局部特征融合成最终特征。PGFL-KD（Zheng
    等，[2021](#bib.bib133)）将语义层的局部特征作为查询，并在特征图中寻找更突出的前景区域，以获得增强的前景特征。基于该特征，进行交互训练和知识蒸馏以约束主干网络的学习。ACSAP（[He
    et al., 2021c,](#bib.bib29)）使用姿态关键点指导对抗生成网络，通过减弱前后块之间的空间关系来去除噪声干扰。PDC（Su 等，[2017](#bib.bib79)）根据14个姿态关键点获得6个身体区域，对每个部分进行旋转和缩放，并使用改进的PTN网络学习仿射变换的参数。这些局部特征被自动放置在绘图中的某些位置，以解决对齐问题。PVPM（[Gao
    et al., 2020b,](#bib.bib18)）基于可见性部件之间的对应关系训练可见性预测器。之后，通过图匹配生成部件伪标签来解决对齐问题。
- en: '![Refer to caption](img/4d9bfcd3d971b061afb00266149f6a3e.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4d9bfcd3d971b061afb00266149f6a3e.png)'
- en: 'Figure 4: Three common Body Segmentation schematics: (a) indicates stripes.
    (b) indicates fixed areas. (c) indicates small blocks.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：三种常见的身体分割示意图：（a）表示条纹。（b）表示固定区域。（c）表示小块。
- en: Semantic Segmentation. By introducing a human parsing model, the interference
    of noise is identified and removed in the form of segmentation or semantic parsing.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割。通过引入人类解析模型，噪声的干扰被识别并以分割或语义解析的形式去除。
- en: SPReID (Kalayeh et al.,, [2018](#bib.bib37)) generates probability maps associated
    with five different body regions based on the trained pedestrian class semantic
    parsing model Inception-V3 (Szegedy et al.,, [2016](#bib.bib84)), namely, foreground,
    head, upper body, lower body, and shoes. Then, the probability map is fused with
    the semantic region features after bilinear interpolation to activate different
    parts and remove the interference of occlusion. CoAttention (Lin & Wang,, [2021](#bib.bib51))
    takes the parsing mask of the local image of the pedestrian’s body as a query
    and constructs a mapping while introducing a self-attentive mechanism to filter
    occlusions. MMGA (Cai et al.,, [2019](#bib.bib2)) first separates pedestrians
    from images into upper and lower body using JPPNet (Liang et al.,, [2018](#bib.bib50)).
    Then, two attention modules are designed; the first is used to filter the interference
    from the background, and the second generates the corresponding spatial and channel
    attentions to extract different features guided by the whole, upper, and lower
    body masks. Finally, element-level multiplication is performed as the final feature.
    HPNet (Huang et al.,, [2020](#bib.bib31)) uses the COCO (Lin et al.,, [2014](#bib.bib52))
    dataset to train the human body parsing model to obtain labels for the four main
    body parts, based on which the parsing model and overall network are trained in
    a multitasking manner while generating visibility scores to remove occlusions.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: SPReID（Kalayeh et al., [2018](#bib.bib37)）基于训练过的行人类语义解析模型Inception-V3（Szegedy
    et al., [2016](#bib.bib84)），生成与五个不同身体区域相关的概率图，即前景、头部、上半身、下半身和鞋子。然后，概率图与经过双线性插值后的语义区域特征融合，以激活不同部位并去除遮挡干扰。CoAttention（Lin
    & Wang, [2021](#bib.bib51)）将行人身体局部图像的解析掩码作为查询，构建映射，同时引入自注意机制以过滤遮挡。MMGA（Cai et
    al., [2019](#bib.bib2)）首先使用JPPNet（Liang et al., [2018](#bib.bib50)）将图像中的行人分为上半身和下半身。然后，设计了两个注意力模块；第一个用于过滤背景干扰，第二个生成相应的空间和通道注意力，以提取由整体、上半身和下半身掩码引导的不同特征。最后，进行元素级乘法作为最终特征。HPNet（Huang
    et al., [2020](#bib.bib31)）使用COCO（Lin et al., [2014](#bib.bib52)）数据集训练人体解析模型，以获得四个主要身体部位的标签，并基于这些标签以多任务方式训练解析模型和整体网络，同时生成可见性评分以去除遮挡。
- en: Semantic segmentation-based approaches also contribute to enhancing the diversity
    of features. In the case of SORN (Zhang et al.,, [2020](#bib.bib126)), a three-branch
    model composed of a global branch, a local branch, and a semantic branch is designed.
    The global branch aims to obtain global features through normalization and feature
    aggregation. Meanwhile, the local branch leverages prior knowledge of the pedestrian
    body structure to generate pedestrian body parts and derive local features through
    mapping, pooling, and normalization. The semantic branch first uses the DANet
    (Fu et al.,, [2019](#bib.bib16)) model to pre-train the semantic labels of the
    data, trains a semantic segmentation model on the DensePose-COCO dataset (Güler
    et al.,, [2018](#bib.bib20)), introduces label smoothing to optimize the semantic
    labels, and forms a foreground pedestrian body region by aggregating the semantic
    segmentation part to realize the separation of background and pedestrians. SGSFA
    (Ren et al.,, [2020](#bib.bib74)) introduces a semantic alignment branch and spatial
    feature alignment branch. The former achieves semantic alignment through element-level
    multiplication. The latter is based on regional spatial alignment achieved by
    body structure.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 基于语义分割的方法也有助于增强特征的多样性。在SORN（Zhang et al., [2020](#bib.bib126)）的情况下，设计了一个由全局分支、局部分支和语义分支组成的三分支模型。全局分支通过归一化和特征聚合来获取全局特征。同时，局部分支利用行人身体结构的先验知识生成行人体部，并通过映射、池化和归一化来提取局部特征。语义分支首先使用DANet（Fu
    et al., [2019](#bib.bib16)）模型对数据的语义标签进行预训练，在DensePose-COCO数据集（Güler et al., [2018](#bib.bib20)）上训练一个语义分割模型，引入标签平滑以优化语义标签，并通过聚合语义分割部分形成前景行人身体区域，实现背景与行人的分离。SGSFA（Ren
    et al., [2020](#bib.bib74)）引入了语义对齐分支和空间特征对齐分支。前者通过元素级乘法实现语义对齐。后者基于通过身体结构实现的区域空间对齐。
- en: Attribute annotation. The occlusion problem is handled by introducing attribute
    annotation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 属性注释。通过引入属性注释来处理遮挡问题。
- en: ASAN (Jin et al.,, [2021](#bib.bib36)) extracts the visible part of human features
    by combining attribute information and weak supervision. Attribute information
    is a semantic level attribute annotation. Based on the visibility part determination,
    a region visibility matching algorithm is introduced to achieve the effect of
    denoising.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ASAN (Jin et al.,, [2021](#bib.bib36)) 通过结合属性信息和弱监督来提取人类特征的可见部分。属性信息是语义级别的属性注解。根据可见部分的确定，引入了区域可见性匹配算法来实现去噪效果。
- en: Mixing method. Introducing more than two kinds of external information can help
    the model remove the interference of noise in the form of feature interaction
    or co-guidance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 混合方法。引入两种以上的外部信息可以帮助模型消除以特征交互或共同引导形式的噪音干扰。
- en: GASM (He & Liu,, [2020](#bib.bib25)) proposes an architecture for learning salient
    information, which separates pedestrians from background by using semantic information,
    removes occlusion interference by pose estimation, and then fuses the two features
    to guide model learning. SSPReID (Quispe & Pedrini,, [2019](#bib.bib73)) designs
    a joint learning method to combine salient and semantic features. Five different
    semantic features of the human body are obtained by human body parsing. The saliency
    features utilize the regions of highest attention in the graph. These features
    are fused with global features and finally concatenated together to form the final
    features. TSA ([Gao et al., 2020a,](#bib.bib17) ) introduces two kinds of area
    features for the pose change problem, one guided by pose keypoints and one guided
    by partial masks of the human parsing model, after which the two are fused. Using
    interaction can solve the pose change problem, while using pose and segmentation
    can also suppress noise and thus solve the occlusion problem. FGSA ([Zhou et al.,
    2020a,](#bib.bib145) ) proposes a pose resolution network for complex pose changes
    to deal with local locations and the relationships between them. Attribute interaction
    learning is designed for local feature information extraction corresponding to
    pose points, which is achieved by training an intermediate attribute classification
    model that treats attribute recognition as a multi-category labeling problem.
    Finally, a local enhanced alignment model is added in the feature fusion phase,
    that is, less weight is added to the background and more weight is added to the
    local and attribute locations. The backbone network of LKWS (Yang et al.,, [2021](#bib.bib116))
    is based on PCB (Sun et al.,, [2018](#bib.bib83)). In local feature extraction,
    the visibility label of points is generated by pose estimation and a reasonable
    threshold, and then the visibility of thick stripes is obtained by a voting mechanism.
    Based on the visibility of stripes, a visibility discriminator is trained to recognize
    noise interference. PGFA (Miao et al.,, [2019](#bib.bib62)) introduces a two-branch
    structure, where the local branch extracts local features based on horizontal
    segmentation. The global branch is guided by pose, and key points are first obtained
    from pose estimation. Then, a reasonable threshold is set to filter the noise
    points, a proper dot product is performed with the feature map to fuse the features,
    and the features of the partial branches are stitched together into the final
    global features. Similarly, PDVM ([Zhou et al., 2020b,](#bib.bib146) ) extracts
    the de-obscured global features based on the pose heat map. Segmentation guides
    local feature extraction.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: GASM（He & Liu，[2020](#bib.bib25)）提出了一种学习显著信息的架构，该架构通过使用语义信息将行人从背景中分离，通过姿态估计去除遮挡干扰，然后融合这两个特征以指导模型学习。SSPReID（Quispe
    & Pedrini，[2019](#bib.bib73)）设计了一种联合学习方法来结合显著和语义特征。通过人体解析获得了人体的五种不同语义特征。显著性特征利用图中最高关注区域的特征。这些特征与全局特征融合，最后拼接在一起形成最终特征。TSA（[Gao
    et al., 2020a,](#bib.bib17)）为姿态变化问题引入了两种区域特征，一种由姿态关键点指导，另一种由人体解析模型的部分掩码指导，然后将这两者融合。使用交互可以解决姿态变化问题，而使用姿态和分割也可以抑制噪声，从而解决遮挡问题。FGSA（[Zhou
    et al., 2020a,](#bib.bib145)）提出了一种针对复杂姿态变化的姿态解析网络，以处理局部位置及其之间的关系。属性交互学习用于提取对应于姿态点的局部特征信息，这是通过训练一个将属性识别视为多类别标记问题的中间属性分类模型来实现的。最后，在特征融合阶段增加了一个局部增强对齐模型，即对背景添加较少的权重，对局部和属性位置添加更多的权重。LKWS（Yang
    et al., [2021](#bib.bib116)）的骨干网络基于PCB（Sun et al., [2018](#bib.bib83)）。在局部特征提取中，通过姿态估计和合理的阈值生成点的可见性标签，然后通过投票机制获得厚条纹的可见性。基于条纹的可见性，训练了一个可见性判别器以识别噪声干扰。PGFA（Miao
    et al., [2019](#bib.bib62)）引入了一个双分支结构，其中局部分支基于水平分割提取局部特征。全局分支由姿态指导，首先从姿态估计中获得关键点。然后，设置合理的阈值以过滤噪声点，执行适当的点积以融合特征，并将部分分支的特征拼接成最终的全局特征。同样，PDVM（[Zhou
    et al., 2020b,](#bib.bib146)）基于姿态热图提取去遮挡的全局特征。分割指导局部特征提取。
- en: '![Refer to caption](img/7b33ed07324abffb4c6b32f501947405.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7b33ed07324abffb4c6b32f501947405.png)'
- en: 'Figure 5: (a) Schematic diagram showing figure convolution. (b) Schematic representation
    of clustering. (c) Schematic representation ofrepresentation of the shared area.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：（a）显示图卷积的示意图。（b）聚类的示意表示。（c）共享区域的示意表示。
- en: 4.1.2 Relationship Representation
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 关系表示
- en: 'By focusing on feature relationships, occlusions are handled in a suppressed,
    removed, or supervised manner. We divide them into attention, clustering, graph
    convolution, and shared region ( see Figure [5](#S4.F5 "Figure 5 ‣ 4.1.1 Local
    Feature Learning ‣ 4.1 Based on CNN ‣ 4 Deep Learning Methods ‣ Occluded Person
    Re-Identification with Deep Learning: A Survey and Perspectives") ) according
    to their different ways and means of learning relationships'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过关注特征关系，遮挡问题被以抑制、移除或监督的方式处理。我们将其分为注意力、聚类、图卷积和共享区域（见图[5](#S4.F5 "图 5 ‣ 4.1.1
    本地特征学习 ‣ 4.1 基于 CNN ‣ 4 深度学习方法 ‣ 遮挡行人重新识别：深度学习的调查与展望")），根据它们不同的学习关系的方式和手段。
- en: Attention. By introducing the attention mechanism, the model can select the
    highly salient and discriminative regions to suppress the interference of noise.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意。通过引入注意力机制，模型能够选择高度显著和区分性的区域，以抑制噪声的干扰。
- en: To address the issue of missing images in person Re-ID, DPPR (Kim & Yoo,, [2017](#bib.bib40))
    employs an attention mechanism to emphasize the same pedestrian part across different
    images. This approach enhances the representation of individuals and improves
    matching accuracy. Moreover, OCNet (Kim et al.,, [2022](#bib.bib41)) mitigates
    the effect of noise by capturing higher-order relationships among regional features
    and incorporating them with weighted combinations. This method effectively suppresses
    the influence of noisy or irrelevant information, resulting in more robust and
    accurate person Re-ID outcomes. AACN (Xu et al.,, [2018](#bib.bib111)) combines
    pose guided attention maps and partial visibility scores to remove background
    distractions and occlusions before extracting clean pedestrian features. DAReID
    (Xu et al.,, [2021](#bib.bib112)) introduces dual attention recognition. The local
    area visible to the pedestrian is obtained by gesture-guided spatial attention.
    Global features are extracted by feature activation and pose. Both will then be
    used together to guide the representation of features. PISNet (Zhao et al.,, [2020](#bib.bib131))
    is concerned with the overlapping area between people and objects. A module is
    designed to act as a guide feature by querying features, which can attenuate the
    problem of attention distraction caused by multiple pedestrians in the gallery.
    Also, a reverse attention module based on strong activation attention is designed,
    which enables the model to assign more weight to the target region. APN (Huo et al.,,
    [2021](#bib.bib33)) proposes a partial perceptual attention network, which takes
    partial feature maps as query vectors, calculates a similarity mapping M with
    a mapping X of the feature maps, and morphs the features by weighting M by X to
    achieve the purpose of aggregating and extracting refined features. MHSA-Net ([Tan
    et al., 2022a,](#bib.bib86) ) multiplies attention weights with feature maps and
    applies a nonlinear transformation to encourage multi-headed attention mechanisms
    to adaptively capture key local features. CASN (Zheng et al.,, [2019](#bib.bib136))
    takes attention and attention consistency as the criteria for model learning and
    removes occlusion by introducing an attention twin network to focus on more discriminative
    core areas. VPM ([Sun et al., 2019b,](#bib.bib82) ) learns the visibility and
    location of components by introducing a self-supervised component localizer at
    the convolution output and introducing a feature extractor that generates region
    information through weighted pooling. PSE (Sarfraz et al.,, [2018](#bib.bib76))
    solves the occlusion problem caused by view angle using view angle prediction.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决人物重新识别中缺失图像的问题，DPPR（Kim & Yoo，，[2017](#bib.bib40)）采用了一种注意力机制，以强调不同图像中的相同行人部分。这种方法增强了个体的表现，并提高了匹配精度。此外，OCNet（Kim
    et al.，，[2022](#bib.bib41)）通过捕捉区域特征之间的高阶关系并将其与加权组合结合，减轻了噪声的影响。这种方法有效地抑制了噪声或无关信息的影响，结果是更为稳健和准确的人物重新识别结果。AACN（Xu
    et al.，，[2018](#bib.bib111)）结合了姿态引导的注意力图和部分可见性评分，以在提取干净的行人特征之前去除背景干扰和遮挡。DAReID（Xu
    et al.，，[2021](#bib.bib112)）引入了双重注意力识别。通过姿势引导的空间注意力获得行人可见的局部区域。全局特征通过特征激活和姿势提取，然后一起用于指导特征表示。PISNet（Zhao
    et al.，，[2020](#bib.bib131)）关注于人物与物体之间的重叠区域。设计了一个模块作为引导特征，通过查询特征来减轻画廊中多个行人引起的注意力分散问题。此外，还设计了一个基于强激活注意力的反向注意力模块，使模型能够为目标区域分配更多的权重。APN（Huo
    et al.，，[2021](#bib.bib33)）提出了一种部分感知注意力网络，该网络将部分特征图作为查询向量，计算特征图的映射X与相似性映射M，并通过将M加权X来变形特征，从而实现特征的聚合和提取。MHSA-Net（[Tan
    et al., 2022a,](#bib.bib86)）将注意力权重与特征图相乘，并应用非线性变换，以鼓励多头注意力机制自适应捕捉关键局部特征。CASN（Zheng
    et al.，，[2019](#bib.bib136)）以注意力和注意力一致性作为模型学习的标准，并通过引入注意力双胞胎网络去除遮挡，以关注更具辨别性的核心区域。VPM（[Sun
    et al., 2019b,](#bib.bib82)）通过在卷积输出处引入自监督组件定位器和引入生成区域信息的特征提取器，学习组件的可见性和位置。PSE（Sarfraz
    et al.，，[2018](#bib.bib76)）通过视角预测解决了由视角造成的遮挡问题。
- en: Attention-based approaches not only enhance the flexibility of the model, but
    also inject more contextual information into the features. PAFM (Yang et al.,,
    [2022](#bib.bib115)) introduces an improved spatial attention module to discover
    relationships between pixel points while capturing and aggregating pixel points
    with high semantic relevance. Finally, it is multiplied with the feature map containing
    pose information to perform feature fusion. Co-Attention (Lin & Wang,, [2021](#bib.bib51))
    takes the analytic mask of a partial pedestrian image and whole image as the target
    and matches it through a self-attention mechanism (Li et al.,, [2020](#bib.bib47)).
    Finally, noise interference is suppressed by focusing pedestrian features. QPM
    ([Wang et al., 2022b,](#bib.bib96) ) divides the feature map into six parts evenly
    in the vertical direction, and introduces a component quality score to judge the
    visibility. Meanwhile, a two-layer identity-aware module based on an attention
    map is used to deal with pedestrian occlusion in the global branch. Finally, global
    features are adaptively extracted from clean pedestrian regions. DSOP ([Wang et al.,
    2020b,](#bib.bib98) ) divides occlusion into shallow and deep layers. The shallow
    layer learns the feature after occlusion by focusing on the local region, and
    the deep layer gives a large receptive field to learn the global feature, that
    is, the feature before occlusion. After that, the channel and spatial attention
    mechanisms are applied to the two branches for weighted fusion of features (Ning
    et al.,, [2021](#bib.bib67)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力的方法不仅增强了模型的灵活性，还向特征中注入了更多的上下文信息。PAFM（Yang 等人，[2022](#bib.bib115)）引入了改进的空间注意力模块，以发现像素点之间的关系，同时捕捉和聚合具有高语义相关性的像素点。最后，它与包含姿态信息的特征图相乘以进行特征融合。Co-Attention（Lin
    & Wang，[2021](#bib.bib51)）将部分行人图像和整图像的分析掩码作为目标，并通过自注意力机制（Li 等人，[2020](#bib.bib47)）进行匹配。最终，通过关注行人特征来抑制噪声干扰。QPM（[Wang
    等人，2022b](#bib.bib96)）将特征图在垂直方向上均分为六部分，并引入组件质量评分来判断可见性。同时，使用基于注意力图的两层身份感知模块来处理全局分支中的行人遮挡。最后，从干净的行人区域自适应地提取全局特征。DSOP（[Wang
    等人，2020b](#bib.bib98)）将遮挡分为浅层和深层。浅层通过关注局部区域学习遮挡后的特征，而深层则给予较大的感受野以学习全局特征，即遮挡前的特征。之后，通道和空间注意力机制应用于两个分支以加权融合特征（Ning
    等人，[2021](#bib.bib67)）。
- en: Clustering.The interference of noise is solved by finding the inherent distribution
    structure of the data to categorize the pixel points.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类。通过寻找数据的固有分布结构来对像素点进行分类，从而解决噪声干扰问题。
- en: ISP (Zhu et al.,, [2020](#bib.bib147)) assigns a pseudo-label to each pixel
    by tandem clustering. All pixels of the human body image are firstly divided into
    foreground and background, based on the assumption that the foreground is more
    responsive than the background. Secondly, the pixels are clustered into different
    parts and assigned pseudo-labels. Based on the pseudo-labels, different weights
    are assigned to the pixels to extract local features. This not only separates
    occlusions from pedestrians at the pixel level, but also enables automatic alignment.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ISP（Zhu 等人，[2020](#bib.bib147)）通过串联聚类为每个像素分配伪标签。首先，人体图像的所有像素被分为前景和背景，基于前景比背景更具响应性的假设。其次，像素被聚类为不同的部分并分配伪标签。根据伪标签，给像素分配不同的权重以提取局部特征。这不仅在像素级别上将遮挡物与行人分离，还实现了自动对齐。
- en: Graph Convolution. By learning the high-order semantic relationship between
    pixels, the noise interference is suppressed by restricting the information transmission.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图卷积。通过学习像素之间的高阶语义关系，通过限制信息传输来抑制噪声干扰。
- en: HOReID ([Wang et al., 2020a,](#bib.bib93) ) introduces a matrix describing the
    higher order relationships between points and later passes information in this
    relationship matrix to form a topological map. With the help of the constraints
    of the topological map, the transfer of useless information between points is
    suppressed, and the purpose of noise removal is achieved.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: HOReID（[Wang 等人，2020a](#bib.bib93)）引入了一个矩阵，用于描述点之间的高阶关系，并随后将这一关系矩阵中的信息传递以形成拓扑图。在拓扑图的约束帮助下，点之间无用信息的传递被抑制，实现了噪声去除的目的。
- en: Shared Area. The interference of noise is reduced by sensing the same body parts
    of pedestrians in the image pairs to extract shareable features.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 共享区域。通过感知图像对中行人的相同身体部位来提取可共享特征，从而减少噪声干扰。
- en: DPPR (Kim & Yoo,, [2017](#bib.bib40)) gives larger weights to regions containing
    the same body parts to improve the ability of the model to extract core features.
    VPM ([Sun et al., 2019b,](#bib.bib82) ) solves the alignment and denoising problem
    by perceiving the visibility of shared regions. PPCL ([He et al., 2021b,](#bib.bib28)
    ) learns component matching in a self-supervised manner and finally computes image
    similarity based on shared semantic corresponding regions only. KBFM (Han et al.,,
    [2020](#bib.bib22)) focuses on extracting highly visible and shareable pose points,
    which are used as the core area to extract features for matching, and achieve
    the effect of denoising and alignment.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: DPPR（Kim & Yoo, [2017](#bib.bib40)）给予包含相同身体部位的区域更大的权重，以提高模型提取核心特征的能力。VPM（[Sun
    et al., 2019b,](#bib.bib82)）通过感知共享区域的可见性来解决对齐和去噪问题。PPCL（[He et al., 2021b,](#bib.bib28)）以自监督方式学习组件匹配，并最终基于共享语义对应区域计算图像相似度。KBFM（Han
    et al., [2020](#bib.bib22)）专注于提取高可见性和可共享的姿态点，将其作为提取特征以进行匹配的核心区域，实现去噪和对齐的效果。
- en: 4.1.3 Other Methods
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 其他方法
- en: Regional reconfiguration.This is complements obscured or noisy areas by using
    complete pedestrian areas.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 区域重新配置。这是通过使用完整的步行区域来补充被遮挡或噪声干扰的区域。
- en: To solve the problem of information loss caused by occlusion, RFCNet (Hou et al.,,
    [2021](#bib.bib30)) introduces an encoder–decoder that uses non-occlusion remote
    spatial context for feature completion. The encoder is modeled by similarity region
    assignment. The decoder reconstructs the occluded region by establishing the correlation
    between the occluded region and distant non-occluded region through clustering.
    ACSAP ([He et al., 2021c,](#bib.bib29) ) combines attitude and adversarial generation
    networks and designs an attitude-guided spatial generator and spatial discriminator
    to remove noise interference.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决由于遮挡导致的信息丢失问题，RFCNet（Hou et al., [2021](#bib.bib30)）引入了一个编码器-解码器，利用非遮挡的远程空间上下文进行特征补全。编码器通过相似性区域分配进行建模。解码器通过聚类建立遮挡区域与远离的非遮挡区域之间的相关性来重建遮挡区域。ACSAP（[He
    et al., 2021c,](#bib.bib29)）结合了姿态和对抗生成网络，设计了一个姿态引导的空间生成器和空间判别器，以去除噪声干扰。
- en: Data enhancement. The sensitivity of the model to occlusion is improved by incorporating
    transformation of the data.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强。通过数据转换提高模型对遮挡的敏感性。
- en: 'APNet ([Zhong et al., 2020a,](#bib.bib141) ) proposes a method to modify the
    detected bounding box. APNet ([Zhong et al., 2020a,](#bib.bib141) ) designed a
    bounding box aligner that slides over the image in a matching manner. Then, a
    feature extractor with high discriminative power is designed to extract the core
    local features and discard the noisy local features. IGOAS (Zhao et al.,, [2021](#bib.bib129))
    adopts a progressive occlusion module, which randomly generates small uniform
    occlusion on a group of images and generates larger occlusion based on small occlusion
    after model learning. Such a growing occlusion region can improve the recognition
    ability of occlusion and achieve the purpose of removing occlusion noise. OAMN
    ([Chen et al., 2021c,](#bib.bib8) ) adopts a method based on cropping and scaling,
    predefines four corners, randomly selects a training image to be cropped and scaled
    to form patches on four positions, and realizes weighted learning in combination
    with attention to achieve denoising. RE ([Zhong et al., 2020b,](#bib.bib142) )
    introduces the technique of random pixel removal, which replaces the pixel values
    in the region with random values by randomly selecting rectangular regions, thereby
    improving the diversity of data and robustness of the model. SSGR (Yan et al.,,
    [2021](#bib.bib114)) introduces a compound batch erase method, which includes
    two erase operations: one is frequently used random erase, and the other is batch
    constant erase. It first divides the image horizontally into random S and randomly
    selects a strip in each sub-batch to erase. Then, referring to the self-attention
    mechanism and local feature learning, a matching-based disentanglement non-local
    operation is introduced to extract better features from the complete pedestrian
    region. ETNDNet (Dong et al.,, [2023](#bib.bib11)) addresses the occlusion problem
    from an adversarial defence perspective. It deals with incomplete information,
    positional misalignment and noisy information through strategies of randomly erasing
    feature maps, introducing random transformations and perturbing feature maps.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: APNet ([Zhong et al., 2020a,](#bib.bib141)) 提出了修改检测到的边界框的方法。APNet ([Zhong et
    al., 2020a,](#bib.bib141)) 设计了一种在图像上滑动匹配的边界框对齐器。然后，设计了一个具有高区分能力的特征提取器，以提取核心局部特征并丢弃噪声局部特征。IGOAS
    (Zhao et al., [2021](#bib.bib129)) 采用了一个渐进遮挡模块，该模块在一组图像上随机生成小的均匀遮挡，并在模型学习后基于小遮挡生成较大的遮挡。这种逐渐增长的遮挡区域可以提高对遮挡的识别能力，并达到去除遮挡噪声的目的。OAMN
    ([Chen et al., 2021c,](#bib.bib8)) 采用了一种基于裁剪和缩放的方法，预定义四个角，随机选择一张训练图像进行裁剪和缩放以形成四个位置的补丁，并结合注意力实现加权学习以达到去噪的目的。RE
    ([Zhong et al., 2020b,](#bib.bib142)) 引入了随机像素移除的技术，通过随机选择矩形区域，用随机值替代区域内的像素值，从而提高数据的多样性和模型的鲁棒性。SSGR
    (Yan et al., [2021](#bib.bib114)) 引入了一种复合批次擦除方法，包括两个擦除操作：一种是频繁使用的随机擦除，另一种是批次常量擦除。它首先将图像水平划分为随机S，并在每个子批次中随机选择一条带状区域进行擦除。然后，参考自注意力机制和局部特征学习，引入基于匹配的解耦非局部操作，从完整的行人区域中提取更好的特征。ETNDNet
    (Dong et al., [2023](#bib.bib11)) 从对抗防御的角度解决遮挡问题。它通过随机擦除特征图、引入随机变换和扰动特征图的策略来处理不完整信息、位置错位和噪声信息。
- en: Regularization. By imposing penalties and constraints on high-attention areas,
    the model is forced to focus on the full pedestrian area. Pedestrian features
    are extracted using complete information.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化。通过对高关注区域施加惩罚和约束，模型被迫关注整个行人区域。行人特征通过完整的信息提取。
- en: MHSA-Net ([Tan et al., 2022a,](#bib.bib86) ) introduces a feature regularization
    mechanism, which consists of a regularization term based on attention weight embedding
    and a hard triplet loss based on triplet feature units. The regularization term
    can cover local information in many ways and increase the completeness of information
    ([Ning et al., 2020a,](#bib.bib68) ). Hard triplet loss can refine the fusion
    and be better used for pedestrian matching.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: MHSA-Net ([Tan et al., 2022a,](#bib.bib86)) 引入了一种特征正则化机制，该机制由基于注意力权重嵌入的正则化项和基于三元组特征单元的硬三元组损失组成。正则化项可以以多种方式覆盖局部信息，并增加信息的完整性
    ([Ning et al., 2020a,](#bib.bib68))。硬三元组损失可以优化融合，并更好地用于行人匹配。
- en: Teacher-student model. Teacher models assist student models in dealing with
    occlusion problems.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 教师-学生模型。教师模型协助学生模型处理遮挡问题。
- en: HG (Kiran et al.,, [2021](#bib.bib42)) designs an end-to-end unsupervised teacher–student
    framework that lets the teacher network learn the between-class distance by inputting
    different combinations of images, and then the student inherits the network and
    learns the within-class distance by inputting more noisy images of the same class.
    At the same time, the attention embedding method with distance distribution matching
    can help the student network to remove noise interference better and extract more
    discriminative features. AFPB (Zhuo et al.,, [2019](#bib.bib150)) first puts regular
    data and pedestrian volume data simulating occlusion into the teacher network,
    and then it conducts joint training to make the teacher network learn a basic
    model that is robust to occlusion. The student network then inherits the teacher
    network. It learns on more realistic, noisier real-world occluded pedestrian data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: HG (Kiran et al., [2021](#bib.bib42)) 设计了一个端到端的无监督教师–学生框架，让教师网络通过输入不同组合的图像来学习类间距离，然后学生继承该网络，通过输入更多同类的噪声图像来学习类内距离。同时，带有距离分布匹配的注意力嵌入方法可以帮助学生网络更好地去除噪声干扰，并提取更具区分性的特征。AFPB
    (Zhuo et al., [2019](#bib.bib150)) 首先将常规数据和模拟遮挡的行人体积数据输入教师网络，然后进行联合训练，使教师网络学习到一个对遮挡具有鲁棒性的基础模型。学生网络继承该教师网络，并在更现实、更嘈杂的实际遮挡行人数据上进行学习。
- en: Multiscale. The occlusion problem is handled by multi-scale feature representation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度。通过多尺度特征表示处理遮挡问题。
- en: DSR (He et al.,, [2018](#bib.bib24)) focuses on solving problems caused by differences
    in scale. DSR (He et al.,, [2018](#bib.bib24)) first trains a fixed-size fully
    convolutional network with only convolution and pooling on Market-1501 ([Zheng
    et al., 2015a,](#bib.bib134) ) to represent the identity feature map, and it then
    introduces three different scales of blocks to extract features in a sliding manner
    (Li et al.,, [2021](#bib.bib48)). Similarly, FPR (He et al.,, [2019](#bib.bib26))
    also introduces a structure that only has convolution and pooling and proposes
    a pyramid layer composed of multiple different pooling kernels to solve the scale
    problem, an attention-based foreground probability generator to process the background,
    and a small weight to the background to achieve removal of background distractions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: DSR (He et al., [2018](#bib.bib24)) 专注于解决由尺度差异引起的问题。DSR (He et al., [2018](#bib.bib24))
    首先在 Market-1501 ([Zheng et al., 2015a,](#bib.bib134)) 上训练一个固定大小的全卷积网络，仅使用卷积和池化来表示身份特征图，然后引入三种不同尺度的块以滑动方式提取特征
    (Li et al., [2021](#bib.bib48))。类似地，FPR (He et al., [2019](#bib.bib26)) 也引入了一个仅包含卷积和池化的结构，并提出了由多个不同池化核组成的金字塔层来解决尺度问题，一个基于注意力的前景概率生成器用于处理背景，并对背景施加小权重以去除背景干扰。
- en: '![Refer to caption](img/bc172c1801381b6b3f68e0b3b54d81a3.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/bc172c1801381b6b3f68e0b3b54d81a3.png)'
- en: 'Figure 6: (a) Schematic of the transformer-based approach. (b) Schematic representation
    of data augmentation.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: (a) 基于变换器的方法示意图。 (b) 数据增强的示意图。'
- en: 4.2 Based on vit
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于 vit
- en: 'TransReID ([He et al., 2021a,](#bib.bib27) ), proposed in 2021, is the first
    model to apply a vit (Dosovitskiy et al.,, [2020](#bib.bib12)) in Re-ID ( see
    Figure [6](#S4.F6 "Figure 6 ‣ 4.1.3 Other Methods ‣ 4.1 Based on CNN ‣ 4 Deep
    Learning Methods ‣ Occluded Person Re-Identification with Deep Learning: A Survey
    and Perspectives") ), which has two major features compared to ResNet (He et al.,,
    [2016](#bib.bib23)): (1) Multi-headed self-attention is better at capturing long-range
    relationships and driving the model to focus on different body parts (Khan et al.,,
    [2022](#bib.bib39); Shamshad et al.,, [2023](#bib.bib78)). (2) The transformer
    can retain more detailed information in the absence of downsampling computation.
    Based on these characteristics, many variants of transformer have appeared in
    recent years, and researchers have widely used them in occlusion Re-ID.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: TransReID ([He et al., 2021a,](#bib.bib27))，于 2021 年提出，是第一个将 vit (Dosovitskiy
    et al., [2020](#bib.bib12)) 应用于 Re-ID 的模型（见图 [6](#S4.F6 "图 6 ‣ 4.1.3 其他方法 ‣ 4.1
    基于 CNN ‣ 4 深度学习方法 ‣ 遮挡行人重识别的深度学习综述与展望")），与 ResNet (He et al., [2016](#bib.bib23))
    相比，具有两个主要特点：（1）多头自注意力更擅长捕捉长程关系，推动模型关注不同的身体部位 (Khan et al., [2022](#bib.bib39);
    Shamshad et al., [2023](#bib.bib78))。（2）变换器可以在没有下采样计算的情况下保留更多的详细信息。基于这些特性，近年来出现了许多变换器的变体，研究人员广泛应用于遮挡
    Re-ID。
- en: PFD ([Wang et al., 2022d,](#bib.bib99) ) first divides the image into fixed-size
    blocks that can be overlapped and then uses the transformer encoder to capture
    contextual relationships. Next, pose guided feature aggregation and a feature
    matching mechanism are used to display the visible body parts. Finally, the pose
    heat map and decoder are used as keys and values to learn a set of semantic part
    views to enhance the discriminability of body parts. DPM ([Tan et al., 2022b,](#bib.bib87)
    ) introduces subspace selection to deal with the alignment problem. Specifically,
    DPM ([Tan et al., 2022b,](#bib.bib87) ) first aggregates the feature representations
    of the overall prototype map and the occlusion map using hierarchical semantic
    information to enhance the quality of the generated prototype mask. Then, a head
    enrichment module based on normalization and orthogonality constraints is introduced
    to enhance the discriminative representation of features and remove the interference
    of noise. PFT (Zhao et al.,, [2022](#bib.bib132)) introduces a learnable enhancement
    patch to compensate for the problem of local feature extraction (Islam,, [2022](#bib.bib34))
    by transformer. Through feature slicing, merging, and stitching, the patch sequence
    can learn the long-range correlation of regions while ensuring the receptive field
    and paying more attention to local features. FED ([Wang et al., 2022g,](#bib.bib104)
    ) divides occlusion into non-pedestrian and non-target pedestrian occlusion, and
    focuses on the latter. Specifically, an occlusion set is first created to enhance
    the data. Then, a multiplicative occlusion score is introduced to diffuse the
    visible pedestrian parts, which improves the quality of the synthesized non-target
    pedestrians. The joint optimization of feature erasure and feature diffusion modules
    realizes the perception ability of the model to the target pedestrian.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: PFD ([Wang et al., 2022d,](#bib.bib99)) 首先将图像划分为可以重叠的固定大小块，然后使用 transformer
    编码器捕捉上下文关系。接着，使用姿态引导特征聚合和特征匹配机制来显示可见的身体部位。最后，使用姿态热图和解码器作为键和值来学习一组语义部件视图，以增强身体部件的区分能力。DPM
    ([Tan et al., 2022b,](#bib.bib87)) 引入了子空间选择来处理对齐问题。具体而言，DPM ([Tan et al., 2022b,](#bib.bib87))
    首先使用分层语义信息聚合整体原型图和遮挡图的特征表示，以增强生成的原型掩码的质量。然后，引入基于归一化和正交性约束的头部增强模块，以增强特征的区分表示并去除噪声干扰。PFT
    (Zhao et al., [2022](#bib.bib132)) 引入了可学习的增强补丁，以弥补 transformer 的局部特征提取问题 (Islam,
    [2022](#bib.bib34))。通过特征切片、合并和拼接，补丁序列可以学习区域的长程关联，同时确保感受野，并更加关注局部特征。FED ([Wang
    et al., 2022g,](#bib.bib104)) 将遮挡分为非行人遮挡和非目标行人遮挡，并重点关注后者。具体而言，首先创建一个遮挡集以增强数据。然后，引入乘法遮挡评分来扩散可见行人部件，从而提高合成的非目标行人的质量。特征擦除和特征扩散模块的联合优化实现了模型对目标行人的感知能力。
- en: FRT (Xu et al.,, [2022](#bib.bib110)) first classifies the pedestrian into head,
    torso, and legs using a pretrained pose estimation model HRNet ([Sun et al., 2019a,](#bib.bib80)
    ), and extracts the corresponding features. Then the occlusion elimination module
    based on graph matching is introduced to eliminate the interference caused by
    occlusion by learning the similarity of common regions. Finally, FRT (Xu et al.,,
    [2022](#bib.bib110)) recovers query features by aggregating neighbor features
    to solve the problem of information loss caused by occlusion. TL-TransNet ([Wang
    et al., 2022c,](#bib.bib97) ) uses a well-improved Swin transformer (Liu et al.,,
    [2021](#bib.bib56)) as a benchmark model to capture the main part of the person
    and uses DeepLabV3+ (Chen et al.,, [2018](#bib.bib7)) to remove background interference
    in the query and gallery. Finally, a reordering method based on hybrid similarity
    and background adaptation is designed to achieve the fusion of original features
    and removed background features. PADE (Huang et al.,, [2022](#bib.bib32)) first
    obtains two new images through cropping and erasing operations and feeds them
    together with the original image into a multi-branch parameter sharing network
    with a vit (Dosovitskiy et al.,, [2020](#bib.bib12)) as the backbone to enhance
    the global and local features in a manner similar to the self-attention mechanism.
    Finally, they are connected to form the final feature representation.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: FRT (Xu et al., [2022](#bib.bib110)) 首先使用预训练的姿态估计模型 HRNet ([Sun et al., 2019a,](#bib.bib80))
    将行人分类为头部、躯干和腿部，并提取相应的特征。然后引入基于图匹配的遮挡消除模块，通过学习常见区域的相似性来消除遮挡造成的干扰。最后，FRT (Xu et
    al., [2022](#bib.bib110)) 通过聚合邻域特征来恢复查询特征，以解决遮挡造成的信息丢失问题。TL-TransNet ([Wang et
    al., 2022c,](#bib.bib97)) 使用改进版的 Swin transformer (Liu et al., [2021](#bib.bib56))
    作为基准模型来捕捉人体的主要部分，并使用 DeepLabV3+ (Chen et al., [2018](#bib.bib7)) 去除查询和库中的背景干扰。最后，设计了一种基于混合相似性和背景适应的重新排序方法，以实现原始特征与去除背景特征的融合。PADE
    (Huang et al., [2022](#bib.bib32)) 首先通过裁剪和擦除操作获得两张新图像，并将它们与原始图像一起输入一个具有 vit (Dosovitskiy
    et al., [2020](#bib.bib12)) 作为骨干网的多分支参数共享网络，以类似自注意机制的方式增强全局和局部特征。最后，将它们连接以形成最终的特征表示。
- en: 4.3 Composite Method
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 复合方法
- en: By introducing more than two different networks, the occlusion problem is dealt
    with in an interactive or fused manner.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入多个不同的网络，遮挡问题以交互或融合的方式得到解决。
- en: FGMFN ([Zhang et al., 2022a,](#bib.bib123) ) introduces a dual branch network.
    The local feature branches are first passed through an affine transformation based
    on ResNet18\. Then, input into ResNet50 (He et al.,, [2016](#bib.bib23)) to get
    the upper body features and divide it into three regions, while introducing the
    attention module based on the gate mechanism to remove noise interference. The
    global feature branch adopts a feature extraction scheme based on block partition,
    and finally, the features of the two branches are fused proportionally as the
    final feature. Pirt (Ma et al.,, [2021](#bib.bib59)) first pre-trains the HRNet
    ([Sun et al., 2019a,](#bib.bib80) ) pose estimation model on the COCO (Lin et al.,,
    [2014](#bib.bib52)) dataset, from which a two-branch structure is elicited. Intra-part
    features are guided by a carefully modified ResNet50 (He et al.,, [2016](#bib.bib23)),
    inter-part relationships are guided by a transformer, and the effects of occlusion
    are removed by establishing part-aware long-range dependencies. Pirt (Ma et al.,,
    [2021](#bib.bib59)) learns inter- and intra-part relationships in a collaborative
    manner. MAT (Zhou et al.,, [2022](#bib.bib144)) first uses a CNN to extract features
    from the image. Then, it flattens and passes them to a transformer. This method
    uses the methods of feature map and key point embedding to obtain a key point
    heat map and segmentation map. It uses affine transformation to obtain motion
    information of each key point and combines action information to realize fine-grained
    human body segmentation, and extracts representative pedestrian body parts to
    remove interference noise. DRL-Net (Jia et al.,, [2022](#bib.bib35)) first acquires
    obstacles from the training images to construct augmented samples with random
    obstacles. Then, a CNN and Transformer are concatenated to design a query-based
    semantic feature extraction layer. Finally, the semantic bootstrap is used to
    learn positive and negative sample comparisons and remove interference noise.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: FGMFN ([Zhang et al., 2022a,](#bib.bib123)) 引入了一种双分支网络。局部特征分支首先通过基于 ResNet18
    的仿射变换，然后输入到 ResNet50 (He et al., [2016](#bib.bib23)) 中以获取上半身特征，并将其分为三个区域，同时引入基于门机制的注意力模块以去除噪声干扰。全局特征分支采用基于块划分的特征提取方案，最后将两个分支的特征按比例融合为最终特征。Pirt
    (Ma et al., [2021](#bib.bib59)) 首先在 COCO (Lin et al., [2014](#bib.bib52)) 数据集上预训练
    HRNet ([Sun et al., 2019a,](#bib.bib80)) 姿态估计模型，从中引出了一个双分支结构。由精心修改的 ResNet50 (He
    et al., [2016](#bib.bib23)) 引导体内特征，变换器引导体间关系，并通过建立部件感知的长距离依赖来去除遮挡效果。Pirt (Ma et
    al., [2021](#bib.bib59)) 以协作的方式学习体间和体内关系。MAT (Zhou et al., [2022](#bib.bib144))
    首先使用 CNN 从图像中提取特征。然后，将其展平并传递给变换器。该方法使用特征图和关键点嵌入的方法来获得关键点热图和分割图。它使用仿射变换来获取每个关键点的运动信息，并结合动作信息实现细粒度的人体分割，并提取代表性行人身体部位以去除干扰噪声。DRL-Net
    (Jia et al., [2022](#bib.bib35)) 首先从训练图像中获取障碍物，以构建带有随机障碍物的增强样本。然后，将 CNN 和变换器连接以设计基于查询的语义特征提取层。最后，使用语义引导来学习正负样本比较并去除干扰噪声。
- en: 4.4 3D Re-ID
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 3D Re-ID
- en: Compared to 2D, the use of 3D information for occlusion person Re-ID is a relatively
    new approach. The shape and spatial depth features represented by 3D data are
    robust to texture information, and it usually reduces the interference caused
    by occlusion through 3D feature denoising, 3D feature complementation and multi-view
    construction.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 与 2D 相比，使用 3D 信息进行遮挡人员再识别是一种相对较新的方法。3D 数据所表示的形状和空间深度特征对纹理信息具有鲁棒性，并且通常通过 3D 特征去噪、3D
    特征补全和多视角构建来减少遮挡造成的干扰。
- en: PersonX (Sun & Zheng,, [2019](#bib.bib81)) constructs virtual 3D models by scanning
    people and objects in the real world and then maps them back to 2D for re-representation.
    Such manipulation of the data enhances the data representation. ([Wang et al.,
    2022f,](#bib.bib101) ) used UV texture mapping to clone clothing from real-world
    pedestrians to virtual 3D characters, while using a patch-based feature segmentation
    and expansion approach to deal with occlusion. A more common form of 3D information
    is the point cloud (Qi et al.,, [2017](#bib.bib72)), where the depth information
    in the point cloud can be used as an additional channel to the image. OG-Net (Zheng
    et al.,, [2022](#bib.bib139)) uses Skinned Multi-Person Linear ( SMPL (Kanazawa
    et al.,, [2018](#bib.bib38)) ) to generate six channels of point cloud data from
    2D images, providing positional and texture information.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: PersonX (Sun & Zheng, [2019](#bib.bib81)) 通过扫描现实世界中的人物和物体来构建虚拟 3D 模型，然后将其映射回
    2D 进行重新表示。这种数据操作增强了数据表示。([Wang et al., 2022f,](#bib.bib101)) 使用 UV 纹理映射将现实世界行人的服装克隆到虚拟
    3D 角色上，同时使用基于补丁的特征分割和扩展方法来处理遮挡。另一种常见的 3D 信息形式是点云 (Qi et al., [2017](#bib.bib72))，其中点云中的深度信息可以作为图像的额外通道。OG-Net
    (Zheng et al., [2022](#bib.bib139)) 使用 Skinned Multi-Person Linear (SMPL (Kanazawa
    et al., [2018](#bib.bib38))) 从 2D 图像生成六通道点云数据，提供位置和纹理信息。
- en: ASSP ([Chen et al., 2021b,](#bib.bib6) ) uses 3D body reconstruction as an auxiliary
    task for 2D feature extraction. Specifically, texture-insensitive 3D shape information
    is first extracted from 2D images as auxiliary features. After that, a 3D row
    human model is created using SMPL (Loper et al.,, [2015](#bib.bib57)), while an
    adversarial learning and self-supervised projection combination is designed to
    combine 2D and 3D information into a 3D model for reconstruction ([Ning et al.,
    2020b,](#bib.bib69) ). Regularization based on 3D reconstruction forces the model
    to decouple 3D shape information from visual features and remove the interference
    of noise to extract more discriminative features. JGCL ([Chen et al., 2021a,](#bib.bib5)
    ) mitigates the lack of information caused by perspective in an unsupervised mapping.
    Specifically, a corresponding 3D mesh is first generated using a 3D network generator
    HMR (Kanazawa et al.,, [2018](#bib.bib38)). After that, the model is rotated and
    combined with a GAN to generate diverse views of the human body. These generated
    new views are combined with the original images in contrast learning while enhancing
    the view-invariant representation to improve the generated picture quality. 3DT
    ([Zhang et al., 2022c,](#bib.bib125) ) implements group person Re-ID using a 3D
    transformer. Because pedestrians in a group inevitably occlude each other, it
    transforms the 3D space into a discrete space by introducing a spatial layout
    token based on quantization and sampling, and it later assigns layout features
    to each member to reconstruct the spatial relationships between members.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ASSP ([Chen et al., 2021b,](#bib.bib6)) 使用 3D 身体重建作为 2D 特征提取的辅助任务。具体而言，首先从 2D
    图像中提取纹理无关的 3D 形状信息作为辅助特征。之后，使用 SMPL (Loper et al., [2015](#bib.bib57)) 创建一个 3D
    行人模型，同时设计了对抗学习和自监督投影组合，以将 2D 和 3D 信息结合到 3D 模型中进行重建 ([Ning et al., 2020b,](#bib.bib69))。基于
    3D 重建的正则化迫使模型将 3D 形状信息与视觉特征解耦，并去除噪声干扰，以提取更具辨别力的特征。JGCL ([Chen et al., 2021a,](#bib.bib5))
    缓解了无监督映射中由于透视造成的信息不足。具体而言，首先使用 3D 网络生成器 HMR (Kanazawa et al., [2018](#bib.bib38))
    生成一个对应的 3D 网格。之后，模型被旋转并与 GAN 结合生成多样的人体视图。这些生成的新视图与原始图像在对比学习中结合，同时增强视图不变表示，以提高生成图像的质量。3DT
    ([Zhang et al., 2022c,](#bib.bib125)) 使用 3D 变换器实现组人员 Re-ID。由于组中的行人不可避免地互相遮挡，它通过引入基于量化和采样的空间布局标记，将
    3D 空间转换为离散空间，并随后为每个成员分配布局特征，以重建成员之间的空间关系。
- en: However, the research on recognition using point clouds is still limited compared
    to 2D images, which is an important research direction for the future.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与 2D 图像相比，基于点云的识别研究仍然有限，这是未来的重要研究方向。
- en: 4.5 Multi-modal Re-ID
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 多模态 Re-ID
- en: RGB-IR multimodal Re-ID. Both day and night are important scenes of pedestrian
    life, and in the case of insufficient illumination, images can only be collected
    by infrared cameras (Nguyen et al.,, [2017](#bib.bib66); [Wu et al., 2017b,](#bib.bib107)
    ). If there is occlusion in the scene, the infrared image still has occlusion.
    At the same time, the infrared image can be used as a special channel of the RGB
    image, which makes the representation mode of the RGB image more complete and
    can supplement the information well in the case of information loss caused by
    occlusion. The RGB-IR multimodalRe-ID is designed to feed both infrared and conventional
    images into the model. Removal of interference from occlusion is achieved by the
    relationship of different modalities or by combining approaches, such as attention
    mechanisms, in dealing with single modal noise, multi-scale, etc.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: RGB-IR 多模态 Re-ID。白天和黑夜都是行人生活的重要场景，在光照不足的情况下，图像只能通过红外相机收集 (Nguyen 等，[2017](#bib.bib66);
    [Wu 等, 2017b,](#bib.bib107))。如果场景中存在遮挡，红外图像仍会有遮挡。同时，红外图像可以作为 RGB 图像的特殊通道，使 RGB
    图像的表示模式更加完整，并且在因遮挡造成的信息丢失的情况下能很好地补充信息。RGB-IR 多模态 Re-ID 旨在将红外图像和常规图像都输入模型。通过不同模态之间的关系或结合方法（如注意力机制）来去除遮挡干扰，从而处理单一模态噪声、多尺度等问题。
- en: DDAG (Ye et al.,, [2020](#bib.bib119)) proposes a dynamic dual attention cross-modal
    graph structure. First, local attention is generated according to the similarity
    between features. Then, an aggregation representation of part-level relation learning
    is introduced. At the same time, the graph structure is introduced to remove the
    interference of noise based on relation. MSPAC (Zhang et al.,, [2021](#bib.bib122))
    proposes a multi-scale-based component awareness mechanism. By adding an attention
    mechanism at the single scale, more fine-grained features are extracted in channel
    and spatial dimensions. After that, feature aggregation is realized using a cascade
    framework. By adding different scale features, the global structured body information
    is represented uniformly. CM-LSP-GE ([Wang et al., 2022e,](#bib.bib100) ) is a
    framework for global features and local characteristics. The occlusion problem
    is solved by local features, and the alignment problem is solved by image segmentation
    and computing the shortest path of local features in two images.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: DDAG (Ye 等，[2020](#bib.bib119)) 提出了一个动态双重注意力跨模态图结构。首先，根据特征之间的相似性生成局部注意力。接着，引入了部分级关系学习的聚合表示。同时，引入图结构以去除基于关系的噪声干扰。MSPAC
    (Zhang 等，[2021](#bib.bib122)) 提出了一个基于多尺度的组件感知机制。通过在单尺度上添加注意力机制，可以在通道和空间维度上提取更细粒度的特征。之后，使用级联框架实现特征聚合。通过添加不同尺度的特征，全球结构体信息得到了统一表示。CM-LSP-GE
    ([Wang 等, 2022e,](#bib.bib100)) 是一个用于全局特征和局部特征的框架。局部特征解决了遮挡问题，而图像分割和计算两张图像中局部特征的最短路径解决了对齐问题。
- en: To solve the intra-modal problem, HMML ([Zhang et al., 2022b,](#bib.bib124)
    ) introduces a pairing-based intra-modal similarity constraint to enhance the
    features. Similarly, CMC (Wen et al.,, [2022](#bib.bib105)) introduces multi-scale,
    multi-level feature learning to achieve refined feature extraction. To solve the
    problem of pedestrian pose misalignment caused by occlusion in transmembrane state,
    DAPN (Liu et al.,, [2022](#bib.bib55)) proposes an alignment network that can
    learn global and local modal invariance. Specifically, DAPN (Liu et al.,, [2022](#bib.bib55))
    first introduces an adaptive spatial transform module to align images. Second,
    the image is segmented horizontally to extract local features. Meanwhile, to learn
    the similarity representation of different modes, the different modes are embedded
    into a unified feature space (Cai et al.,, [2021](#bib.bib3)). Finally, the global
    and local features are fused as the final features. To solve the alignment problem,
    SPOT (Chen et al.,, [2022](#bib.bib4)) proposes a transformer-based network. First,
    a relational network composed of four convolution layers and two pooling layers
    is used to process the human body key point heat map to obtain the pedestrian
    body structure information. Combining it with the attention mechanism, the pedestrian
    region is highlighted to remove the background interference. Then, the transformer
    is used to mine the relationship between parts’ positions and features to enhance
    the discriminability of local features. The final feature is extracted by adding
    weight combination. VI (Park et al.,, [2021](#bib.bib70)) first extracts features
    of IR and RGB using a dual-stream CNN. The cosine similarity of all corresponding
    feature pairs is calculated simultaneously, and the corresponding matching probability
    is calculated by the SoftMax function. These probabilities are taken as semantic
    similarity features of different modalities. Then, more discriminative features
    are extracted by local feature association. The above features are then aggregated
    to form the final feature. To solve the problem of image internal misalignment,
    DTRM ([Ye et al., 2021a,](#bib.bib118) ) combines attention and partial aggregation,
    and uses the context relationship of the two modalities to improve the global
    features to eliminate the noise effect.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决模态内部的问题，HMML ([Zhang et al., 2022b,](#bib.bib124)) 引入了基于配对的模态内部相似性约束来增强特征。类似地，CMC
    (Wen et al., [2022](#bib.bib105)) 引入了多尺度、多层次特征学习以实现精细的特征提取。为了解决由于遮挡导致的跨膜状态下行人姿态对齐问题，DAPN
    (Liu et al., [2022](#bib.bib55)) 提出了一个可以学习全局和局部模态不变性的对齐网络。具体而言，DAPN (Liu et al.,
    [2022](#bib.bib55)) 首先引入了一个自适应空间变换模块来对齐图像。其次，对图像进行水平分割以提取局部特征。同时，为了学习不同模态的相似性表示，将不同模态嵌入到统一的特征空间
    (Cai et al., [2021](#bib.bib3))。最后，将全局特征和局部特征融合为最终特征。为了解决对齐问题，SPOT (Chen et al.,
    [2022](#bib.bib4)) 提出了一个基于变换器的网络。首先，使用由四层卷积层和两层池化层组成的关系网络处理人体关键点热图，以获取行人体结构信息。结合注意力机制，突出行人区域以去除背景干扰。然后，变换器用于挖掘部件位置和特征之间的关系，以增强局部特征的区分能力。最终特征通过加权组合提取。VI
    (Park et al., [2021](#bib.bib70)) 首先使用双流 CNN 提取 IR 和 RGB 的特征。所有对应特征对的余弦相似度同时计算，并通过
    SoftMax 函数计算相应的匹配概率。这些概率被作为不同模态的语义相似性特征。然后，通过局部特征关联提取更具区分性的特征。上述特征被聚合以形成最终特征。为了解决图像内部对齐问题，DTRM
    ([Ye et al., 2021a,](#bib.bib118)) 结合了注意力和部分聚合，并利用两种模态的上下文关系来改善全局特征以消除噪声效应。
- en: RGB-Depth multimodal Re-ID. Depth ([Wang et al., 2021a,](#bib.bib92) ) images
    are captured through devices such as laser radars, and they provide body shape
    and skeletal information by measuring distance. For images, when information is
    lost owing to obstacles, depth features can supplement the position information
    of texture features to assist in a more complete expression. At the same time,
    it can solve the lighting problem and the problem of changing clothes for pedestrians.
    It can also help solve the problems of obstacle illumination and different changing
    environments for clothing. RGB-Depth multimodal Re-ID aims to input depth map
    and RGB image simultaneously, and remove the interference of noise by the attention
    mechanism and other methods.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: RGB-Depth 多模态 Re-ID。深度 ([Wang et al., 2021a,](#bib.bib92)) 图像通过激光雷达等设备捕获，提供身体形状和骨骼信息，通过测距来实现。对于图像，当由于障碍物导致信息丢失时，深度特征可以补充纹理特征的位置信息，帮助更完整的表达。同时，它可以解决照明问题和行人换衣服的问题。它还可以帮助解决障碍物照明和不同环境中衣物变化的问题。RGB-Depth
    多模态 Re-ID 旨在同时输入深度图和 RGB 图像，并通过注意力机制等方法去除噪声干扰。
- en: CMD (Hafner et al.,, [2022](#bib.bib21)) proposes an approach combining embedding
    representation and feature distillation. It removes noise through a gate-controlled
    attention mechanism. This mechanism uses one modality to dynamically activate
    the more discriminative features in another modality by gating signals.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: CMD (Hafner et al., [2022](#bib.bib21)) 提出了结合嵌入表示和特征蒸馏的方法。它通过门控控制的注意力机制去除噪声。该机制使用一种模态通过门控信号动态激活另一种模态中的更具辨别性的特征。
- en: RGB-Text multimodal Re-ID. RGB-Text multimodal Re-ID aims to introduce text
    data to enhance feature representation by sharing semantic information and attentional
    calibration to eliminate the effect of noise.In daily life, text information is
    one of the most frequently used types of information. When image information is
    missing or cannot be used owing to obstacles, it can be supplemented with text.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: RGB-Text 多模态 Re-ID。RGB-Text 多模态 Re-ID 旨在通过共享语义信息和注意力校准来引入文本数据，从而增强特征表示，消除噪声的影响。在日常生活中，文本信息是最常用的信息类型之一。当图像信息由于障碍物缺失或无法使用时，可以用文本信息进行补充。
- en: AXM-Net (Farooq et al.,, [2022](#bib.bib14)) dynamically exploits multi-scale
    information of text and images, recalibrating each modality according to the shared
    semantics and adding contextual attention to the text branch to supplement the
    information of the convolution block. Furthermore, attention is introduced to
    enhance feature consistency and local information of the visual part. It can learn
    the alignment semantic information of different modalities and automatically remove
    the interference of irrelevant information.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: AXM-Net (Farooq et al., [2022](#bib.bib14)) 动态利用文本和图像的多尺度信息，根据共享语义重新校准每种模态，并在文本分支中添加上下文注意力以补充卷积块的信息。此外，引入注意力机制来增强视觉部分的特征一致性和局部信息。它可以学习不同模态的对齐语义信息，并自动去除无关信息的干扰。
- en: 5 Method Comparison
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 方法比较
- en: 'Table 2: Comparison of experimental results based on local feature learning
    methods. The red numbers indicate the best results. (in $\%$).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：基于本地特征学习方法的实验结果比较。红色数字表示最佳结果。（以 $\%$ 表示）。
- en: '|  |  | Method | Venue | Occluded-Duke | Occluded-REID | Partial-REID | Partial-iLIDS
    | Market1501 | DukeMTMC-reID |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 方法 | 会议 | Occluded-Duke | Occluded-REID | Partial-REID | Partial-iLIDS
    | Market1501 | DukeMTMC-reID |'
- en: '|  |  | Rank-1 | mAP | Rank-1 | mAP | Rank-1 | Rank-3 | Rank-1 | Rank-3 | Rank-1
    | mAP | Rank-1 | mAP |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Rank-1 | mAP | Rank-1 | mAP | Rank-1 | Rank-3 | Rank-1 | Rank-3 | Rank-1
    | mAP | Rank-1 | mAP |'
- en: '| Local Feature Learning | Body Segmentation | CBDB-Net (Tan et al.,, [2021](#bib.bib85))
    | TCSVT2021 | 50.09 | 38.9 | - | - | 66.7 | 78.3 | 68.4 | 81.5 | 94.4 | 85 | 87.7
    | 74.3 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 本地特征学习 | 身体分割 | CBDB-Net (Tan et al., [2021](#bib.bib85)) | TCSVT2021 | 50.09
    | 38.9 | - | - | 66.7 | 78.3 | 68.4 | 81.5 | 94.4 | 85 | 87.7 | 74.3 |'
- en: '| OCNet (Kim et al.,, [2022](#bib.bib41)) | ICASSP2022 | 64.30 | 54.40 | -
    | - | - | - | - | - | 95 | 89.3 | 90.5 | 80.2 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| OCNet (Kim et al., [2022](#bib.bib41)) | ICASSP2022 | 64.30 | 54.40 | - |
    - | - | - | - | - | 95 | 89.3 | 90.5 | 80.2 |'
- en: '| Pose Estimation | AACN (Xu et al.,, [2018](#bib.bib111)) | CVPR2018 | - |
    - | - | - | - | - | - | - | 88.69 | 82.96 | 76.84 | 59.25 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 姿态估计 | AACN (Xu et al., [2018](#bib.bib111)) | CVPR2018 | - | - | - | - |
    - | - | - | - | 88.69 | 82.96 | 76.84 | 59.25 |'
- en: '| ACSAP ([He et al., 2021c,](#bib.bib29) ) | ICIP2021 | - | - | - | - | 77
    | 83.7 | 76.5 | 87.4 | - | - | - | - |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| ACSAP ([He et al., 2021c,](#bib.bib29)) | ICIP2021 | - | - | - | - | 77 |
    83.7 | 76.5 | 87.4 | - | - | - | - |'
- en: '| DAReID (Xu et al.,, [2021](#bib.bib112)) | KBS2021 | 63.4 | - | - | - | 68.1
    | 79.5 | 76.7 | 85.3 | 94.6 | 87 | 88.9 | 78.4 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| DAReID (Xu et al., [2021](#bib.bib112)) | KBS2021 | 63.4 | - | - | - | 68.1
    | 79.5 | 76.7 | 85.3 | 94.6 | 87 | 88.9 | 78.4 |'
- en: '| DSA-reID (Zhang et al.,, [2019](#bib.bib128)) | CVPR2019 | - | - | - | -
    | - | - | - | - | 95.7 | 87.6 | 86.2 | 74.3 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| DSA-reID (Zhang et al., [2019](#bib.bib128)) | CVPR2019 | - | - | - | - |
    - | - | - | - | 95.7 | 87.6 | 86.2 | 74.3 |'
- en: '| HOReID ([Wang et al., 2020a,](#bib.bib93) ) | CVPR2020 | - | - | 80.3 | 70.2
    | 85.3 | 91 | 72.6 | 86.4 | 94.2 | 84.9 | 86.9 | 75.6 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| HOReID ([Wang et al., 2020a,](#bib.bib93)) | CVPR2020 | - | - | 80.3 | 70.2
    | 85.3 | 91 | 72.6 | 86.4 | 94.2 | 84.9 | 86.9 | 75.6 |'
- en: '| PAFM (Yang et al.,, [2022](#bib.bib115)) | NCA2022 | 55.1 | 42.3 | 76.4 |
    68 | 82.5 | - | - | - | 95.6 | 88.5 | 91.2 | 80.1 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| PAFM (Yang et al., [2022](#bib.bib115)) | NCA2022 | 55.1 | 42.3 | 76.4 |
    68 | 82.5 | - | - | - | 95.6 | 88.5 | 91.2 | 80.1 |'
- en: '| PDC (Su et al.,, [2017](#bib.bib79)) | ICCV2017 | - | - | - | - | - | - |
    - | - | 84.14 | 63.41 | - | - |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| PDC (Su et al., [2017](#bib.bib79)) | ICCV2017 | - | - | - | - | - | - |
    - | - | 84.14 | 63.41 | - | - |'
- en: '| PGFL-KD (Zheng et al.,, [2021](#bib.bib133)) | MM2021 | 63 | 54.1 | 80.7
    | 70.3 | 85.1 | 90.8 | 74 | 86.7 | 95.3 | 87.2 | 89.6 | 79.5 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| PGFL-KD (Zheng et al., [2021](#bib.bib133)) | MM2021 | 63 | 54.1 | 80.7 |
    70.3 | 85.1 | 90.8 | 74 | 86.7 | 95.3 | 87.2 | 89.6 | 79.5 |'
- en: '| PGMANet (Zhai et al.,, [2021](#bib.bib121)) | IJCNN2021 | - | - | - | - |
    82.1 | 85.5 | 68.8 | 78.1 | - | - | - | - |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| PGMANet (Zhai et al., [2021](#bib.bib121)) | IJCNN2021 | - | - | - | - |
    82.1 | 85.5 | 68.8 | 78.1 | - | - | - | - |'
- en: '| PMFB (Miao et al.,, [2021](#bib.bib63)) | TNNLS2021 | 56.3 | 43.5 | - | -
    | 72.5 | 83 | 70.6 | 81.3 | 92.7 | 81.3 | 86.2 | 72.6 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| PMFB (Miao et al., [2021](#bib.bib63)) | TNNLS2021 | 56.3 | 43.5 | - | -
    | 72.5 | 83 | 70.6 | 81.3 | 92.7 | 81.3 | 86.2 | 72.6 |'
- en: '| PVPM ([Gao et al., 2020b,](#bib.bib18) ) | CVPR2020 | - | - | 70.4 | 61.2
    | 78.3 | - | - | - | - | - | - | - |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| PVPM ([Gao et al., 2020b,](#bib.bib18)) | CVPR2020 | - | - | 70.4 | 61.2
    | 78.3 | - | - | - | - | - | - | - |'
- en: '| Semantic Segmentation | Co-Attention (Lin & Wang,, [2021](#bib.bib51)) |
    ICIP2021 | - | - | - | - | 83 | 90.3 | 73.1 | 83.2 | - | - | - | - |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 语义分割 | Co-Attention (Lin & Wang, [2021](#bib.bib51)) | ICIP2021 | - | - |
    - | - | 83 | 90.3 | 73.1 | 83.2 | - | - | - | - |'
- en: '| HPNet (Huang et al.,, [2020](#bib.bib31)) | ICME2020 | - | - | 87.3 | 77.4
    | 85.7 | - | 68.9 | 80.7 | - | - | - | - |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| HPNet (Huang et al., [2020](#bib.bib31)) | ICME2020 | - | - | 87.3 | 77.4
    | 85.7 | - | 68.9 | 80.7 | - | - | - | - |'
- en: '| MMGA (Cai et al.,, [2019](#bib.bib2)) | CVPR2019 | - | - | - | - | - | -
    | - | - | 95 | 87.2 | 89.5 | 78.1 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| MMGA (Cai et al., [2019](#bib.bib2)) | CVPR2019 | - | - | - | - | - | - |
    - | - | 95 | 87.2 | 89.5 | 78.1 |'
- en: '| SGSFA (Ren et al.,, [2020](#bib.bib74)) | PMLR2020 | 62.3 | 47.4 | 63.1 |
    53.2 | 68.2 | - | - | - | 92.3 | 80.2 | 84.7 | 70.8 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| SGSFA (Ren et al., [2020](#bib.bib74)) | PMLR2020 | 62.3 | 47.4 | 63.1 |
    53.2 | 68.2 | - | - | - | 92.3 | 80.2 | 84.7 | 70.8 |'
- en: '| SORN (Zhang et al.,, [2020](#bib.bib126)) | TCSVT2020 | 57.6 | 46.3 | - |
    - | 76.7 | 84.3 | 79.8 | 86.6 | 94.8 | 84.5 | 86.9 | 74.1 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| SORN (Zhang et al., [2020](#bib.bib126)) | TCSVT2020 | 57.6 | 46.3 | - |
    - | 76.7 | 84.3 | 79.8 | 86.6 | 94.8 | 84.5 | 86.9 | 74.1 |'
- en: '| SPReID (Kalayeh et al.,, [2018](#bib.bib37)) | CVPR2017 | - | - | - | - |
    - | - | - | - | 94.63 | 90.96 | 88.96 | 84.99 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| SPReID (Kalayeh et al., [2018](#bib.bib37)) | CVPR2017 | - | - | - | - |
    - | - | - | - | 94.63 | 90.96 | 88.96 | 84.99 |'
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Attribute &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 属性 &#124;'
- en: '&#124; Annotation &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注释 &#124;'
- en: '| ASAN (Jin et al.,, [2021](#bib.bib36)) | TCSVT2021 | 55.40 | 43.80 | 82.50
    | 71.80 | 86.80 | 93.50 | 81.70 | 88.30 | 94.60 | 85.30 | 87.50 | 76.30 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| ASAN (Jin et al., [2021](#bib.bib36)) | TCSVT2021 | 55.40 | 43.80 | 82.50
    | 71.80 | 86.80 | 93.50 | 81.70 | 88.30 | 94.60 | 85.30 | 87.50 | 76.30 |'
- en: '| Mixing Method | FGSA ([Zhou et al., 2020a,](#bib.bib145) ) | TIP2020 | -
    | - | - | - | - | - | - | - | 91.50 | 85.40 | 85.90 | 74.10 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 混合方法 | FGSA ([Zhou et al., 2020a,](#bib.bib145)) | TIP2020 | - | - | - |
    - | - | - | - | - | 91.50 | 85.40 | 85.90 | 74.10 |'
- en: '| GASM (He & Liu,, [2020](#bib.bib25)) | ECCV2020 | - | - | 80.30 | 73.10 |
    - | - | - | - | 95.30 | 84.70 | 88.30 | 74.40 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| GASM (He & Liu, [2020](#bib.bib25)) | ECCV2020 | - | - | 80.30 | 73.10 |
    - | - | - | - | 95.30 | 84.70 | 88.30 | 74.40 |'
- en: '| PGFA (Miao et al.,, [2019](#bib.bib62)) | ICCV2019 | - | - | - | - | 68.80
    | 80.00 | 69.10 | 80.90 | 91.20 | 76.80 | 82.60 | 65.50 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| PGFA (Miao et al., [2019](#bib.bib62)) | ICCV2019 | - | - | - | - | 68.80
    | 80.00 | 69.10 | 80.90 | 91.20 | 76.80 | 82.60 | 65.50 |'
- en: '| SSPReID (Quispe & Pedrini,, [2019](#bib.bib73)) | IVC 2019 | - | - | - |
    - | - | - | - | - | 93.70 | 90.80 | 86.40 | 83.70 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| SSPReID (Quispe & Pedrini, [2019](#bib.bib73)) | IVC 2019 | - | - | - | -
    | - | - | - | - | 93.70 | 90.80 | 86.40 | 83.70 |'
- en: '| LKWS (Yang et al.,, [2021](#bib.bib116)) | ICCV2021 | 62.2 | 46.3 | 81 |
    71 | 85.7 | 93.7 | 80.7 | 88.2 | - | - | - | - |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| LKWS (Yang et al., [2021](#bib.bib116)) | ICCV2021 | 62.2 | 46.3 | 81 | 71
    | 85.7 | 93.7 | 80.7 | 88.2 | - | - | - | - |'
- en: '| TSA ([Gao et al., 2020a,](#bib.bib17) ) | ACM MM2020 | - | - | - | - | 72.70
    | 85.20 | 73.90 | 84.70 | - | - | - | - |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| TSA ([Gao et al., 2020a,](#bib.bib17)) | ACM MM2020 | - | - | - | - | 72.70
    | 85.20 | 73.90 | 84.70 | - | - | - | - |'
- en: 'Table 3: Comparison of experimental results based on relationship representation
    methods. The red numbers indicate the best results.(in $\%$).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：基于关系表示方法的实验结果比较。红色数字表示最佳结果。（以百分比表示）
- en: '|  |  |  |  | Occluded-Duke | Occluded-REID | Partial-REID | Partial-iLIDS
    | Market1501 | DukeMTMC-reID |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | 遮挡-Duke | 遮挡-REID | 部分-REID | 部分-iLIDS | Market1501 | DukeMTMC-reID
    |'
- en: '|  |  | Method | Venue | Rank-1 | mAP | Rank-1 | mAP | Rank-1 | Rank-3 | Rank-1
    | Rank-3 | Rank-1 | mAP | Rank-1 | mAP |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 方法 | 会议 | Rank-1 | mAP | Rank-1 | mAP | Rank-1 | Rank-3 | Rank-1 |
    Rank-3 | Rank-1 | mAP | Rank-1 | mAP |'
- en: '| Relationship Representation | Shared Area | KBFM (Han et al.,, [2020](#bib.bib22))
    | ICIP2020 | - | - | - | - | 69.7 | 82.2 | 64.1 | 73.9 | - | - | - | - |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 关系表示 | 共享区域 | KBFM (Han et al., [2020](#bib.bib22)) | ICIP2020 | - | - |
    - | - | 69.7 | 82.2 | 64.1 | 73.9 | - | - | - | - |'
- en: '| PPCL ([He et al., 2021b,](#bib.bib28) ) | CVPR2021 | - | - | - | - | 83.70
    | 88.70 | 71.40 | 85.70 | - | - | - | - |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| PPCL ([He et al., 2021b,](#bib.bib28) ) | CVPR2021 | - | - | - | - | 83.70
    | 88.70 | 71.40 | 85.70 | - | - | - | - |'
- en: '| VPM ([Sun et al., 2019b,](#bib.bib82) ) | CVPR2019 | - | - | - | - | 67.70
    | 81.90 | 65.50 | 74.80 | 90.40 | 75.70 | - | - |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| VPM ([Sun et al., 2019b,](#bib.bib82) ) | CVPR2019 | - | - | - | - | 67.70
    | 81.90 | 65.50 | 74.80 | 90.40 | 75.70 | - | - |'
- en: '| Clustering | ISP (Zhu et al.,, [2020](#bib.bib147)) | ECCV2020 | 62.80 |
    52.30 | - | - | - | - | - | - | 94.63 | 90.69 | 88.96 | 84.99 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 | ISP (Zhu et al., [2020](#bib.bib147)) | ECCV2020 | 62.80 | 52.30 | -
    | - | - | - | - | - | 94.63 | 90.69 | 88.96 | 84.99 |'
- en: '|'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Figure &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图表 &#124;'
- en: '&#124; Convolution &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 卷积 &#124;'
- en: '| HOReID ([Wang et al., 2020a,](#bib.bib93) ) | CVPR2020 | - | - | 80.3 | 70.2
    | 85.3 | 91 | 72.6 | 86.4 | 94.2 | 84.9 | 86.9 | 75.6 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| HOReID ([Wang et al., 2020a,](#bib.bib93) ) | CVPR2020 | - | - | 80.3 | 70.2
    | 85.3 | 91 | 72.6 | 86.4 | 94.2 | 84.9 | 86.9 | 75.6 |'
- en: '| Attention | AACN (Xu et al.,, [2018](#bib.bib111)) | CVPR2018 | - | - | -
    | - | - | - | - | - | 88.69 | 82.96 | 76.84 | 59.25 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 注意力 | AACN (Xu et al., [2018](#bib.bib111)) | CVPR2018 | - | - | - | - |
    - | - | - | - | 88.69 | 82.96 | 76.84 | 59.25 |'
- en: '| APN (Huo et al.,, [2021](#bib.bib33)) | ICPR2021 | - | - | - | - | 71.80
    | 85.50 | 66.40 | 76.50 | 96.00 | 89.00 | 89.50 | 79.20 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| APN (Huo et al., [2021](#bib.bib33)) | ICPR2021 | - | - | - | - | 71.80 |
    85.50 | 66.40 | 76.50 | 96.00 | 89.00 | 89.50 | 79.20 |'
- en: '| CASN (Zheng et al.,, [2019](#bib.bib136)) | CVPR2019 | - | - | - | - | -
    | - | - | - | 94.40 | 82.80 | 87.70 | 73.70 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| CASN (Zheng et al., [2019](#bib.bib136)) | CVPR2019 | - | - | - | - | - |
    - | - | - | 94.40 | 82.80 | 87.70 | 73.70 |'
- en: '| Co-Attention (Lin & Wang,, [2021](#bib.bib51)) | ICIP2021 | - | - | - | -
    | 83.00 | 90.30 | 73.10 | 83.20 | - | - | - | - |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Co-Attention (Lin & Wang, [2021](#bib.bib51)) | ICIP2021 | - | - | - | -
    | 83.00 | 90.30 | 73.10 | 83.20 | - | - | - | - |'
- en: '| DAReID (Xu et al.,, [2021](#bib.bib112)) | KBS2021 | 63.4 | - | - | - | 68.1
    | 79.5 | 76.7 | 85.3 | 94.6 | 87 | 88.9 | 78.4 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| DAReID (Xu et al., [2021](#bib.bib112)) | KBS2021 | 63.4 | - | - | - | 68.1
    | 79.5 | 76.7 | 85.3 | 94.6 | 87 | 88.9 | 78.4 |'
- en: '| DSOP ([Wang et al., 2020b,](#bib.bib98) ) | JPCS2020 | 57.70 | 45.30 | -
    | - | - | - | - | - | 95.40 | 85.90 | 88.20 | 77.00 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| DSOP ([Wang et al., 2020b,](#bib.bib98) ) | JPCS2020 | 57.70 | 45.30 | -
    | - | - | - | - | - | 95.40 | 85.90 | 88.20 | 77.00 |'
- en: '| MHSA-Net ([Tan et al., 2022a,](#bib.bib86) ) | TNNLS2022 | 59.70 | 44.80
    | - | - | 85.70 | 91.30 | 74.90 | 87.20 | 95.50 | 93.00 | 90.70 | 87.20 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| MHSA-Net ([Tan et al., 2022a,](#bib.bib86) ) | TNNLS2022 | 59.70 | 44.80
    | - | - | 85.70 | 91.30 | 74.90 | 87.20 | 95.50 | 93.00 | 90.70 | 87.20 |'
- en: '| OCNet (Kim et al.,, [2022](#bib.bib41)) | ICASSP2022 | 64.30 | 54.40 | -
    | - | - | - | - | - | - | - | - | - |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| OCNet (Kim et al., [2022](#bib.bib41)) | ICASSP2022 | 64.30 | 54.40 | - |
    - | - | - | - | - | - | - | - | - |'
- en: '| PAFM (Yang et al.,, [2022](#bib.bib115)) | NCA2022 | 55.1 | 42.3 | 76.4 |
    68 | 82.5 | - | - | - | 95.6 | 88.5 | 91.2 | 80.1 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| PAFM (Yang et al., [2022](#bib.bib115)) | NCA2022 | 55.1 | 42.3 | 76.4 |
    68 | 82.5 | - | - | - | 95.6 | 88.5 | 91.2 | 80.1 |'
- en: '| PISNet (Zhao et al.,, [2020](#bib.bib131)) | ECCV2020 | - | - | - | - | -
    | - | - | - | 95.60 | 87.10 | 88.80 | 78.70 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| PISNet (Zhao et al., [2020](#bib.bib131)) | ECCV2020 | - | - | - | - | -
    | - | - | - | 95.60 | 87.10 | 88.80 | 78.70 |'
- en: '| PSE (Sarfraz et al.,, [2018](#bib.bib76)) | CVPR2018 | - | - | - | - | -
    | - | - | - | 90.30 | 84.00 | 85.20 | 79.80 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| PSE (Sarfraz et al., [2018](#bib.bib76)) | CVPR2018 | - | - | - | - | - |
    - | - | - | 90.30 | 84.00 | 85.20 | 79.80 |'
- en: '| QPM ([Wang et al., 2022b,](#bib.bib96) ) | ITM2022 | 64.40 | 49.70 | - |
    - | 81.70 | 88.00 | 77.30 | 85.70 | - | - | - | - |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| QPM ([Wang et al., 2022b,](#bib.bib96) ) | ITM2022 | 64.40 | 49.70 | - |
    - | 81.70 | 88.00 | 77.30 | 85.70 | - | - | - | - |'
- en: '| VPM ([Sun et al., 2019b,](#bib.bib82) ) | CVPR2019 | - | - | - | - | 67.70
    | 81.90 | 65.50 | 74.80 | 90.40 | 75.70 | - | - |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| VPM ([Sun et al., 2019b,](#bib.bib82) ) | CVPR2019 | - | - | - | - | 67.70
    | 81.90 | 65.50 | 74.80 | 90.40 | 75.70 | - | - |'
- en: 'Table 4: Comparison of experimental results with other methods. The red numbers
    indicate the best results.(in $\%$).'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '表4: 与其他方法的实验结果比较。红色数字表示最佳结果。（以百分比表示）。'
- en: '|  |  |  | Occluded-Duke | Occluded-REID | Partial-REID | Partial-iLIDS | Market1501
    | DukeMTMC-reID |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 遮挡-Duke | 遮挡-REID | 部分-REID | 部分-iLIDS | Market1501 | DukeMTMC-reID
    |'
- en: '|  | Method | Venue | Rank-1 | mAP | Rank-1 | mAP | Rank-1 | Rank-3 | Rank-1
    | Rank-3 | Rank-1 | mAP | Rank-1 | mAP |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 会议 | Rank-1 | mAP | Rank-1 | mAP | Rank-1 | Rank-3 | Rank-1 | Rank-3
    | Rank-1 | mAP | Rank-1 | mAP |'
- en: '| Transformer | DPM ([Tan et al., 2022b,](#bib.bib87) ) | ACM MM 2022 | 71.40
    | 61.80 | 85.50 | 79.70 | - | - | - | - | 95.50 | 89.70 | 91.00 | 82.60 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | DPM ([Tan et al., 2022b,](#bib.bib87)) | ACM MM 2022 | 71.40
    | 61.80 | 85.50 | 79.70 | - | - | - | - | 95.50 | 89.70 | 91.00 | 82.60 |'
- en: '| FED ([Wang et al., 2022g,](#bib.bib104) ) | CVPR2022 | 68.10 | 56.40 | 86.30
    | 79.30 | 83.10 | - | - | - | 95.00 | 86.30 | 89.40 | 78.00 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| FED ([Wang et al., 2022g,](#bib.bib104)) | CVPR2022 | 68.10 | 56.40 | 86.30
    | 79.30 | 83.10 | - | - | - | 95.00 | 86.30 | 89.40 | 78.00 |'
- en: '| FRT (Xu et al.,, [2022](#bib.bib110)) | TIP2022 | 70.70 | 61.30 | 80.40 |
    71.00 | 88.20 | 93.20 | 73.00 | 87.00 | 95.50 | 88.10 | 90.50 | 81.70 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| FRT (Xu et al., [2022](#bib.bib110)) | TIP2022 | 70.70 | 61.30 | 80.40 |
    71.00 | 88.20 | 93.20 | 73.00 | 87.00 | 95.50 | 88.10 | 90.50 | 81.70 |'
- en: '| TransReID ([He et al., 2021a,](#bib.bib27) ) | ICCV2021 | 66.40 | 59.20 |
    - | - | - | - | - | - | 95.20 | 88.90 | 90.70 | 82.00 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| TransReID ([He et al., 2021a,](#bib.bib27)) | ICCV2021 | 66.40 | 59.20 |
    - | - | - | - | - | - | 95.20 | 88.90 | 90.70 | 82.00 |'
- en: '| PFD ([Wang et al., 2022d,](#bib.bib99) ) | AAAI2022 | 69.50 | 61.80 | 81.50
    | 83.00 | - | - | - | - | 95.50 | 89.70 | 91.20 | 83.20 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| PFD ([Wang et al., 2022d,](#bib.bib99)) | AAAI2022 | 69.50 | 61.80 | 81.50
    | 83.00 | - | - | - | - | 95.50 | 89.70 | 91.20 | 83.20 |'
- en: '| PFT (Zhao et al.,, [2022](#bib.bib132)) | NCA2022 | 69.80 | 60.80 | 83.00
    | 78.30 | 81.30 | 79.90 | 68.10 | 81.50 | 95.30 | 88.80 | 90.70 | 82.10 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| PFT (Zhao et al., [2022](#bib.bib132)) | NCA2022 | 69.80 | 60.80 | 83.00
    | 78.30 | 81.30 | 79.90 | 68.10 | 81.50 | 95.30 | 88.80 | 90.70 | 82.10 |'
- en: '| Mixing Method | DRL-Net (Jia et al.,, [2022](#bib.bib35)) | TMM2021 | 65.80
    | 53.90 | - | - | - | - | - | - | 94.70 | 86.90 | 88.10 | 76.60 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 混合方法 | DRL-Net (Jia et al., [2022](#bib.bib35)) | TMM2021 | 65.80 | 53.90
    | - | - | - | - | - | - | 94.70 | 86.90 | 88.10 | 76.60 |'
- en: '| Pirt (Ma et al.,, [2021](#bib.bib59)) | ACMMM2021 | 60.00 | 50.90 | - | -
    | - | - | - | - | 94.10 | 86.30 | 88.90 | 77.60 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Pirt (Ma et al., [2021](#bib.bib59)) | ACMMM2021 | 60.00 | 50.90 | - | -
    | - | - | - | - | 94.10 | 86.30 | 88.90 | 77.60 |'
- en: '| Multiscale | DSR (He et al.,, [2018](#bib.bib24)) | CVPR2018 | 40.80 | 30.40
    | 72.80 | 62.80 | 50.70 | 70.00 | 58.80 | 67.20 | 83.58 | 64.25 | - | - |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 多尺度 | DSR (He et al., [2018](#bib.bib24)) | CVPR2018 | 40.80 | 30.40 | 72.80
    | 62.80 | 50.70 | 70.00 | 58.80 | 67.20 | 83.58 | 64.25 | - | - |'
- en: '| FPR (He et al.,, [2019](#bib.bib26)) | ICCV2019 | - | - | 78.30 | 68.00 |
    81.00 | - | 68.10 | - | 95.42 | 86.58 | 88.64 | 78.42 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| FPR (He et al., [2019](#bib.bib26)) | ICCV2019 | - | - | 78.30 | 68.00 |
    81.00 | - | 68.10 | - | 95.42 | 86.58 | 88.64 | 78.42 |'
- en: '| Regional Reconfiguration | ACSAP ([He et al., 2021c,](#bib.bib29) ) | ICIP2021
    | - | - | - | - | 77.00 | 83.70 | 76.50 | 87.40 | - | - | - | - |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 区域重构 | ACSAP ([He et al., 2021c,](#bib.bib29)) | ICIP2021 | - | - | - | -
    | 77.00 | 83.70 | 76.50 | 87.40 | - | - | - | - |'
- en: '| RFCNet (Hou et al.,, [2021](#bib.bib30)) | TPAMI2021 | 63.90 | 54.50 | -
    | - | - | - | - | - | 95.20 | 89.20 | 90.70 | 80.70 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| RFCNet (Hou et al., [2021](#bib.bib30)) | TPAMI2021 | 63.90 | 54.50 | - |
    - | - | - | - | - | 95.20 | 89.20 | 90.70 | 80.70 |'
- en: '| Data Enhancement | IGOAS (Zhao et al.,, [2021](#bib.bib129)) | TIP2021 |
    60.10 | 49.40 | 81.10 | - | - | - | - | - | 93.40 | 84.10 | 86.90 | 75.10 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 数据增强 | IGOAS (Zhao et al., [2021](#bib.bib129)) | TIP2021 | 60.10 | 49.40
    | 81.10 | - | - | - | - | - | 93.40 | 84.10 | 86.90 | 75.10 |'
- en: '| OAMN ([Chen et al., 2021c,](#bib.bib8) ) | ICCV2021 | 62.60 | 46.10 | - |
    - | 86.00 | - | 77.30 | - | 93.20 | 79.80 | 86.30 | 72.60 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| OAMN ([Chen et al., 2021c,](#bib.bib8)) | ICCV2021 | 62.60 | 46.10 | - |
    - | 86.00 | - | 77.30 | - | 93.20 | 79.80 | 86.30 | 72.60 |'
- en: '| SSGR (Yan et al.,, [2021](#bib.bib114)) | ICCV2021 | 65.80 | 57.20 | 78.50
    | 72.90 | - | - | - | - | 96.10 | 89.30 | 91.10 | 81.30 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| SSGR (Yan et al., [2021](#bib.bib114)) | ICCV2021 | 65.80 | 57.20 | 78.50
    | 72.90 | - | - | - | - | 96.10 | 89.30 | 91.10 | 81.30 |'
- en: '| Regularization | MHSA-Net ([Tan et al., 2022a,](#bib.bib86) ) | TNNLS2022
    | 59.70 | 44.80 | - | - | 85.70 | 91.30 | 74.90 | 87.20 | 95.50 | 93.00 | 90.70
    | 87.20 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 正则化 | MHSA-Net ([Tan et al., 2022a,](#bib.bib86)) | TNNLS2022 | 59.70 | 44.80
    | - | - | 85.70 | 91.30 | 74.90 | 87.20 | 95.50 | 93.00 | 90.70 | 87.20 |'
- en: 'We statistically evaluate the results of the occluded person Re-ID methods
    on two general datasets (Market1501 ([Zheng et al., 2015a,](#bib.bib134) ), DukeMTMC-reID
    (Ristani et al.,, [2016](#bib.bib75))), two occluded person Re-ID datasets (Occluded-DukeMTMC
    (Miao et al.,, [2019](#bib.bib62)), Occluded-REID ([Zheng et al., 2015b,](#bib.bib138)
    )) and two partial person Re-ID datasets (Partial-ReID ([Zheng et al., 2015b,](#bib.bib138)
    ), Partial-iLIDS (He et al.,, [2018](#bib.bib24))). We simply divide them into
    three categories: The experimental results based on the local feature learning
    method are shown in Table [2](#S5.T2 "Table 2 ‣ 5 Method Comparison ‣ Occluded
    Person Re-Identification with Deep Learning: A Survey and Perspectives"). It contains
    body segmentation, pose estimation, semantic segmentation, attribute annotation,
    and mixing method. The experimental results based on the relationship representation
    method are shown in Table [3](#S5.T3 "Table 3 ‣ 5 Method Comparison ‣ Occluded
    Person Re-Identification with Deep Learning: A Survey and Perspectives"). It contains
    shared area, clustering, figure convolution, and attention. The experimental results
    of other methods are shown in Table [4](#S5.T4 "Table 4 ‣ 5 Method Comparison
    ‣ Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives").
    It contains the transformer, mixing method, multi-scale, regional reconfiguration,
    data enhancement, and regularization. For an introduction to each class of methods
    and details of each study, the reader is referred to Section 4.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对两种通用数据集（Market1501 ([Zheng et al., 2015a,](#bib.bib134) ), DukeMTMC-reID
    (Ristani et al., [2016](#bib.bib75)))、两种遮挡人物 Re-ID 数据集（Occluded-DukeMTMC (Miao
    et al., [2019](#bib.bib62)), Occluded-REID ([Zheng et al., 2015b,](#bib.bib138)
    )）以及两种部分人物 Re-ID 数据集（Partial-ReID ([Zheng et al., 2015b,](#bib.bib138) ), Partial-iLIDS
    (He et al., [2018](#bib.bib24)))的遮挡人物 Re-ID 方法结果进行统计评估。我们将这些方法简单地分为三类：基于局部特征学习方法的实验结果如表
    [2](#S5.T2 "Table 2 ‣ 5 Method Comparison ‣ Occluded Person Re-Identification
    with Deep Learning: A Survey and Perspectives") 所示。它包括身体分割、姿态估计、语义分割、属性标注和混合方法。基于关系表示方法的实验结果如表
    [3](#S5.T3 "Table 3 ‣ 5 Method Comparison ‣ Occluded Person Re-Identification
    with Deep Learning: A Survey and Perspectives") 所示。它包括共享区域、聚类、图卷积和注意力。其他方法的实验结果如表
    [4](#S5.T4 "Table 4 ‣ 5 Method Comparison ‣ Occluded Person Re-Identification
    with Deep Learning: A Survey and Perspectives") 所示。它包括变换器、混合方法、多尺度、区域重配置、数据增强和正则化。有关每类方法的介绍和每项研究的详细信息，请参见第4节。'
- en: 'From the results we can get the following information: (1) From the results
    we can get the following information: OCNet (Kim et al.,, [2022](#bib.bib41))
    based on local feature learning, QPM ([Wang et al., 2022b,](#bib.bib96) ) based
    on relational representation, DPM ([Tan et al., 2022b,](#bib.bib87) ) and PFD
    ([Wang et al., 2022d,](#bib.bib99) ) based on transformer perform better on the
    Occluded-DukeMTMC dataset. On the Occluded-REID dataset, HPNet (Huang et al.,,
    [2020](#bib.bib31)) based on local feature learning, HOReID ([Wang et al., 2020a,](#bib.bib93)
    ) based on relational representation, FED ([Wang et al., 2022g,](#bib.bib104)
    ) and PFD ([Wang et al., 2022d,](#bib.bib99) ) based on transformer perform stably.
    On the Partial-ReID dataset, ASAN (Jin et al.,, [2021](#bib.bib36)) and LKWS (Yang
    et al.,, [2021](#bib.bib116)) based on local feature learning, MHSA-Net ([Tan
    et al., 2022a,](#bib.bib86) ) based on relational representation, and FRT (Xu
    et al.,, [2022](#bib.bib110)) based on transformer achieve better performance.
    On the Partial-iLIDS dataset, ASAN (Jin et al.,, [2021](#bib.bib36)) based on
    local feature learning, MHSA-Net ([Tan et al., 2022a,](#bib.bib86) ) and QPM ([Wang
    et al., 2022b,](#bib.bib96) ) based on relational representation, ACSAP ([He et al.,
    2021c,](#bib.bib29) ) based on region reconstruction, and OAMN ([Chen et al.,
    2021c,](#bib.bib8) ) based on data augmentation achieve very stable performance.
    Therefore, there is no single method that achieves the best performance on all
    datasets. (2) In general, the performance on the occlusion dataset reflects the
    ability of the model to resist noise, the performance on the partial dataset reflects
    the recognition ability of the model under the condition of missing pedestrian
    information, and the performance on the general dataset reflects the comprehensive
    performance of the model. Each of these approaches addresses one or more specific
    problems. (3) Attention and pose estimation are the more mainstream and typical
    of the many pedestrian re-identification methods for dealing with occlusion. Attribute
    annotation-based, clustering-based, figure convolution-based and regularisation-based
    methods, on the other hand, have received less attention.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中我们可以得到以下信息：（1）从结果中我们可以得到以下信息：基于局部特征学习的OCNet（Kim et al., [2022](#bib.bib41)）、基于关系表示的QPM（[Wang
    et al., 2022b](#bib.bib96)）、基于变换器的DPM（[Tan et al., 2022b](#bib.bib87)）和PFD（[Wang
    et al., 2022d](#bib.bib99)）在Occluded-DukeMTMC数据集上的表现更好。在Occluded-REID数据集上，基于局部特征学习的HPNet（Huang
    et al., [2020](#bib.bib31)）、基于关系表示的HOReID（[Wang et al., 2020a](#bib.bib93)）、基于变换器的FED（[Wang
    et al., 2022g](#bib.bib104)）和PFD（[Wang et al., 2022d](#bib.bib99)）表现稳定。在Partial-ReID数据集上，基于局部特征学习的ASAN（Jin
    et al., [2021](#bib.bib36)）和LKWS（Yang et al., [2021](#bib.bib116)）、基于关系表示的MHSA-Net（[Tan
    et al., 2022a](#bib.bib86)）以及基于变换器的FRT（Xu et al., [2022](#bib.bib110)）取得了更好的性能。在Partial-iLIDS数据集上，基于局部特征学习的ASAN（Jin
    et al., [2021](#bib.bib36)）、基于关系表示的MHSA-Net（[Tan et al., 2022a](#bib.bib86)）和QPM（[Wang
    et al., 2022b](#bib.bib96)）、基于区域重建的ACSAP（[He et al., 2021c](#bib.bib29)）以及基于数据增强的OAMN（[Chen
    et al., 2021c](#bib.bib8)）表现非常稳定。因此，没有一种方法在所有数据集上都能取得最佳性能。（2）一般来说，遮挡数据集上的表现反映了模型抗噪声的能力，部分数据集上的表现反映了模型在缺失行人信息的情况下的识别能力，而通用数据集上的表现反映了模型的综合性能。这些方法各自解决了一个或多个特定的问题。（3）注意力机制和姿态估计是许多行人重识别方法中更主流和典型的应对遮挡的方法。另一方面，基于属性注释、聚类、图卷积和正则化的方法则受到的关注较少。
- en: 6 Future directions
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 未来方向
- en: 6.1 Richer, higher quality datasets
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 更丰富、更高质量的数据集
- en: Most models are evaluated on datasets collected in controlled environments.
    The data in real-world scenarios is often uncontrollable, which can significantly
    impact the model’s performance on such datasets.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数模型是在受控环境中收集的数据集上进行评估的。现实世界中的数据往往是不可控的，这可能会显著影响模型在这些数据集上的表现。
- en: For an occluded person Re-ID dataset, it is necessary to incorporate one or
    more modal inputs, including a diverse combination of images, text, infrared maps,
    and depth maps. This enables the model to effectively handle a wider range of
    realistic occlusion scenarios. Moreover, there is a lack of larger datasets encompassing
    a broader range of domains, environments (Gou et al.,, [2018](#bib.bib19)), and
    higher resolutions, which would provide richer content and higher quality for
    research purposes.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对于遮挡人体重新识别数据集，需要结合一个或多个模态输入，包括图像、文本、红外图和深度图的多样组合。这使得模型能够有效处理更广泛的现实遮挡场景。此外，目前缺乏涵盖更广泛领域、环境
    (Gou et al., [2018](#bib.bib19)) 和更高分辨率的大型数据集，这将为研究提供更丰富的内容和更高的质量。
- en: 6.2 More robust and varied feature extraction
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 更强大和多样化的特征提取
- en: 3D Re-ID. Inspired by human three-dimensional cognition, some researchers believe
    that the complete pedestrian representation should be a fusion of 3D and 2D (Zheng
    et al.,, [2022](#bib.bib139)).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 3D重新识别。受到人类三维认知的启发，一些研究者认为完整的行人表示应该是3D和2D的融合 (Zheng et al., [2022](#bib.bib139))。
- en: At present, PointNet (Qi et al.,, [2017](#bib.bib72)), as a representative of
    deep learning methods in point cloud feature extraction ([Wang et al., 2022a,](#bib.bib91)
    ; CS et al.,, [2022](#bib.bib10)), has demonstrated promising results. Point cloud
    completion (Fei et al.,, [2022](#bib.bib15)) and point cloud correction can aid
    in 3D occluded person Re-ID, while 3D pose estimation ([Wang et al., 2021b,](#bib.bib95)
    ) and 3D semantic segmentation (Xie et al.,, [2020](#bib.bib109)) can guide the
    feature extraction process for person Re-ID. However, research in the 3D domain
    for pedestrian recognition (Zhao et al.,, [2013](#bib.bib130); [Sun et al., 2019b,](#bib.bib82)
    ) is still relatively limited compared to the advancements in 2D approaches. Therefore,
    3D occluded person Re-ID represents a significant and promising research direction
    for the future (Tirkolaee et al.,, [2020](#bib.bib88)).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，作为点云特征提取深度学习方法的代表，PointNet (Qi et al., [2017](#bib.bib72)) 已展示出令人鼓舞的结果。点云补全
    (Fei et al., [2022](#bib.bib15)) 和点云修正可以帮助3D遮挡人体重新识别，而3D姿态估计 ([Wang et al., 2021b,](#bib.bib95))
    和3D语义分割 (Xie et al., [2020](#bib.bib109)) 可以指导人体重新识别的特征提取过程。然而，与2D方法的进展相比，行人识别的3D领域研究
    (Zhao et al., [2013](#bib.bib130); [Sun et al., 2019b,](#bib.bib82)) 仍然相对有限。因此，3D遮挡人体重新识别代表了未来一个重要且有前景的研究方向
    (Tirkolaee et al., [2020](#bib.bib88))。
- en: Multimodal Re-ID. The information captured from different modalities demonstrates
    a significant diversity in content representation ([Wu et al., 2017b,](#bib.bib107)
    ; [Wu et al., 2017a,](#bib.bib106) ). Improving the interaction, fusion, and extraction
    of more comprehensive pedestrian features at both the data and feature extraction
    stages represents a crucial research direction for future advancements (Tutsoy
    & Tanrikulu,, [2022](#bib.bib90); Sekhar et al.,, [2017](#bib.bib77)).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态重新识别。从不同模态捕获的信息展示了内容表示的显著多样性 ([Wu et al., 2017b,](#bib.bib107); [Wu et al.,
    2017a,](#bib.bib106))。在数据和特征提取阶段，改进交互、融合和提取更全面的行人特征是未来进展的关键研究方向 (Tutsoy & Tanrikulu,
    [2022](#bib.bib90); Sekhar et al., [2017](#bib.bib77))。
- en: Cross-resolution occluded person Re-ID. Owing to the influence of the distance
    and pixel size of the collection device, the resolution of the collected samples
    is uneven, and the feature space correspondence is also inconsistent (Li et al.,,
    [2019](#bib.bib49)). At the same time, low resolution will lose significant spatial
    and detail information (Mao et al.,, [2019](#bib.bib60)). How to extract pedestrian
    features at different resolutions under occlusion conditions is a problem to be
    solved in the future.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 跨分辨率遮挡人体重新识别。由于采集设备的距离和像素大小的影响，采集样本的分辨率不均匀，特征空间对应关系也不一致 (Li et al., [2019](#bib.bib49))。同时，低分辨率会丧失重要的空间和细节信息
    (Mao et al., [2019](#bib.bib60))。如何在遮挡条件下提取不同分辨率的行人特征是未来需要解决的问题。
- en: Fast response, smaller occlusion person Re-ID model. Constructing smaller (Zhou
    et al.,, [2019](#bib.bib143); Li et al.,, [2018](#bib.bib46)) and faster (Liu
    et al.,, [2017](#bib.bib53)) occluded person Re-ID models with reduced parameters
    is a crucial research direction for future advancements (Tutsoy et al.,, [2018](#bib.bib89)).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 快速响应，更小的遮挡人体重新识别模型。构建更小的 (Zhou et al., [2019](#bib.bib143); Li et al., [2018](#bib.bib46))
    和更快的 (Liu et al., [2017](#bib.bib53)) 遮挡人体重新识别模型，减少参数是未来进展的关键研究方向 (Tutsoy et al.,
    [2018](#bib.bib89))。
- en: Unsupervised and semi-supervised occluded person Re-ID. The complex manual labeling
    process is omitted, and the pedestrian features are learned by using the datasets
    without labels (Zhang & Lu,, [2018](#bib.bib127); Liu et al.,, [2019](#bib.bib54))
    or with a small number of labels ([Wang et al., 2019c,](#bib.bib103) ; [Wang et al.,
    2019a,](#bib.bib94) ; Nagaraju et al.,, [2016](#bib.bib65)).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督和半监督遮挡人物 Re-ID。省略了复杂的手动标注过程，通过使用无标签的数据集 (Zhang & Lu,, [2018](#bib.bib127);
    Liu 等,, [2019](#bib.bib54)) 或少量标签 ([Wang 等, 2019c,](#bib.bib103); [Wang 等, 2019a,](#bib.bib94);
    Nagaraju 等,, [2016](#bib.bib65)) 来学习行人特征。
- en: Currently, the performance of unsupervised and semi-supervised based methods
    on the task of occluded person Re-ID is still some way off compared to supervised
    methods. Supervised methods usually rely on large-scale labelled datasets for
    training and thus can achieve high performance. However, as unsupervised and semi-supervised
    methods are increasingly studied, they show significant potential in improving
    the generalisation of occluded person Re-ID models.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，基于无监督和半监督的方法在遮挡人物 Re-ID 任务中的表现仍然与监督方法有一定差距。监督方法通常依赖于大规模标注数据集进行训练，因此可以实现较高的性能。然而，随着无监督和半监督方法的研究逐渐增多，它们在提高遮挡人物
    Re-ID 模型的泛化能力方面显示出显著潜力。
- en: 6.3 Occluded person Re-ID system
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 遮挡人物 Re-ID 系统
- en: At present, few researchers combine object detection and occluded person Re-ID
    together. The end-to-end person Re-ID systems are lacking, and the integrated
    system has more applications in real life (Martinel et al.,, [2016](#bib.bib61)).
    How to combine the two more effectively and rationally and design a occluded person
    Re-ID system that is more robust to occlusion is an important research direction.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，结合目标检测和遮挡人物 Re-ID 的研究较少。端到端的个人 Re-ID 系统仍然缺乏，而集成系统在现实生活中有更多应用 (Martinel 等,
    [2016](#bib.bib61))。如何更有效和合理地结合这两者，设计出对遮挡更具鲁棒性的遮挡人物 Re-ID 系统是一个重要的研究方向。
- en: 7 Conclusion
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: 'This review offers a comprehensive and integrated analysis and discussion of
    deep learning methods for occluded person Re-ID, addressing both practical and
    research-driven requirements. Firstly, we introduce the classification of occlusion
    problems and the datasets specifically designed for occluded person Re-ID. Secondly,
    we comprehensively classify and introduce the methods proposed in top international
    journals and conferences before 2023 for addressing occluded person Re-ID. Finally,
    the future prospects of occluded person Re-ID are analyzed from data, feature,
    and system perspectives, respectively. In this study, we categorize the most significant
    image feature extraction methods into five major categories: local feature learning,
    relational representation, transformer-based methods, mixing methods, and other
    approaches. This review will assist researchers in comprehending the process and
    objectives of these methods, providing valuable references and contributing to
    the research significance in the advancement of occluded Re-ID.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述提供了对遮挡人物 Re-ID 深度学习方法的全面整合分析和讨论，涉及实际和研究驱动的需求。首先，我们介绍了遮挡问题的分类以及专门为遮挡人物 Re-ID
    设计的数据集。其次，我们全面分类并介绍了在2023年之前的顶级国际期刊和会议上提出的解决遮挡人物 Re-ID 的方法。最后，从数据、特征和系统角度分析了遮挡人物
    Re-ID 的未来前景。在本研究中，我们将最重要的图像特征提取方法分为五大类：局部特征学习、关系表示、基于变换器的方法、混合方法和其他方法。该综述将帮助研究人员理解这些方法的过程和目标，提供有价值的参考，并为推动遮挡
    Re-ID 研究的重要性做出贡献。
- en: References
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bedagkar-Gala & Shah, (2014) Bedagkar-Gala, A. & Shah, S. K. (2014). A survey
    of approaches and trends in person re-identification. Image and vision computing,
    32(4), 270–286.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bedagkar-Gala 和 Shah, (2014) Bedagkar-Gala, A. & Shah, S. K. (2014). 个人重新识别的方法和趋势综述。图像与视觉计算，32(4),
    270–286。
- en: Cai et al., (2019) Cai, H., Wang, Z., & Cheng, J. (2019). Multi-scale body-part
    mask guided attention for person re-identification. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Workshops (pp. 0–0).
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等, (2019) Cai, H., Wang, Z., & Cheng, J. (2019). 多尺度身体部位掩码引导的注意力用于个人重新识别。发表于IEEE/CVF计算机视觉与模式识别会议研讨会论文集
    (pp. 0–0)。
- en: Cai et al., (2021) Cai, X., Liu, L., Zhu, L., & Zhang, H. (2021). Dual-modality
    hard mining triplet-center loss for visible infrared person re-identification.
    Knowledge-Based Systems, 215, 106772.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等, (2021) Cai, X., Liu, L., Zhu, L., & Zhang, H. (2021). 双模态硬挖掘三元组中心损失用于可见红外人物重新识别。基于知识的系统，215,
    106772。
- en: Chen et al., (2022) Chen, C., Ye, M., Qi, M., Wu, J., Jiang, J., & Lin, C.-W.
    (2022). Structure-aware positional transformer for visible-infrared person re-identification.
    IEEE Transactions on Image Processing, 31, 2352–2364.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人，（2022）Chen, C., Ye, M., Qi, M., Wu, J., Jiang, J., & Lin, C.-W.（2022）。用于可见-红外人重新识别的结构感知位置变换器。IEEE
    图像处理汇刊，31，2352–2364。
- en: (5) Chen, H., Wang, Y., Lagadec, B., Dantcheva, A., & Bremond, F. (2021a). Joint
    generative and contrastive learning for unsupervised person re-identification.
    In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
    (pp. 2004–2013).
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （5）Chen, H., Wang, Y., Lagadec, B., Dantcheva, A., & Bremond, F.（2021a）。用于无监督人重新识别的联合生成和对比学习。在
    IEEE/CVF 计算机视觉与模式识别会议论文集（第 2004–2013 页）。
- en: (6) Chen, J., Jiang, X., Wang, F., Zhang, J., Zheng, F., Sun, X., & Zheng, W.-S.
    (2021b). Learning 3d shape feature for texture-insensitive person re-identification.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (pp. 8146–8155).
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （6）Chen, J., Jiang, X., Wang, F., Zhang, J., Zheng, F., Sun, X., & Zheng, W.-S.（2021b）。学习
    3D 形状特征以用于纹理无关的人重新识别。在 IEEE/CVF 计算机视觉与模式识别会议论文集（第 8146–8155 页）。
- en: Chen et al., (2018) Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., & Adam,
    H. (2018). Encoder-decoder with atrous separable convolution for semantic image
    segmentation. In Proceedings of the European conference on computer vision (ECCV)
    (pp. 801–818).
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人，（2018）Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., & Adam, H.（2018）。具有膨胀可分卷积的编码器-解码器用于语义图像分割。在欧洲计算机视觉会议（ECCV）论文集（第
    801–818 页）。
- en: '(8) Chen, P., Liu, W., Dai, P., Liu, J., Ye, Q., Xu, M., Chen, Q., & Ji, R.
    (2021c). Occlude them all: Occlusion-aware attention network for occluded person
    re-id. In Proceedings of the IEEE/CVF international conference on computer vision
    (pp. 11833–11842).'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （8）Chen, P., Liu, W., Dai, P., Liu, J., Ye, Q., Xu, M., Chen, Q., & Ji, R.（2021c）。遮挡所有：用于遮挡人重新识别的遮挡感知注意网络。在
    IEEE/CVF 国际计算机视觉会议论文集（第 11833–11842 页）。
- en: 'Cheng et al., (2011) Cheng, D. S., Cristani, M., Stoppa, M., Bazzani, L., &
    Murino, V. (2011). Custom pictorial structures for re-identification. In Bmvc,
    volume 1 (pp.6̃).: Citeseer.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng 等人，（2011）Cheng, D. S., Cristani, M., Stoppa, M., Bazzani, L., & Murino,
    V.（2011）。用于重新识别的自定义图像结构。在 Bmvc，第 1 卷（第 6 页）。: Citeseer。'
- en: CS et al., (2022) CS, W., H, W., X, N., SW, T., & WJ, L. (2022). 3d point cloud
    classification method based on dynamic coverage of local area. Journal of Software,
    (pp. 0–0).
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CS 等人，（2022）CS, W., H, W., X, N., SW, T., & WJ, L.（2022）。基于局部区域动态覆盖的 3D 点云分类方法。软件学报，（第
    0–0 页）。
- en: Dong et al., (2023) Dong, N., Zhang, L., Yan, S., Tang, H., & Tang, J. (2023).
    Erasing, transforming, and noising defense network for occluded person re-identification.
    arXiv preprint arXiv:2307.07187.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等人，（2023）Dong, N., Zhang, L., Yan, S., Tang, H., & Tang, J.（2023）。用于遮挡人重新识别的擦除、转换和噪声防御网络。arXiv
    预印本 arXiv:2307.07187。
- en: 'Dosovitskiy et al., (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., et al. (2020). An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等人，（2020）Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., 等（2020）。一张图像值 16x16 个词：用于大规模图像识别的变换器。arXiv 预印本 arXiv:2010.11929。
- en: 'Ess et al., (2008) Ess, A., Leibe, B., Schindler, K., & Van Gool, L. (2008).
    A mobile vision system for robust multi-person tracking. In 2008 IEEE Conference
    on Computer Vision and Pattern Recognition (pp. 1–8).: IEEE.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ess 等人，（2008）Ess, A., Leibe, B., Schindler, K., & Van Gool, L.（2008）。用于鲁棒多人跟踪的移动视觉系统。在
    2008 IEEE 计算机视觉与模式识别会议（第 1–8 页）。: IEEE。'
- en: 'Farooq et al., (2022) Farooq, A., Awais, M., Kittler, J., & Khalid, S. S. (2022).
    Axm-net: Implicit cross-modal feature alignment for person re-identification.
    In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36 (pp. 4477–4485).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Farooq 等人，（2022）Farooq, A., Awais, M., Kittler, J., & Khalid, S. S.（2022）。Axm-net：用于人重新识别的隐式跨模态特征对齐。在
    AAAI 人工智能会议论文集，第 36 卷（第 4477–4485 页）。
- en: Fei et al., (2022) Fei, B., Yang, W., Chen, W.-M., Li, Z., Li, Y., Ma, T., Hu,
    X., & Ma, L. (2022). Comprehensive review of deep learning-based 3d point cloud
    completion processing and analysis. IEEE Transactions on Intelligent Transportation
    Systems.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fei 等人，（2022）Fei, B., Yang, W., Chen, W.-M., Li, Z., Li, Y., Ma, T., Hu, X.,
    & Ma, L.（2022）。基于深度学习的 3D 点云补全处理与分析的全面综述。IEEE 智能交通系统汇刊。
- en: Fu et al., (2019) Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., & Lu,
    H. (2019). Dual attention network for scene segmentation. In Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition (pp. 3146–3154).
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., & Lu, H. (2019). 双重注意网络用于场景分割。在IEEE/CVF计算机视觉与模式识别会议论文集（第3146–3154页）。
- en: (17) Gao, L., Zhang, H., Gao, Z., Guan, W., Cheng, Z., & Wang, M. (2020a). Texture
    semantically aligned with visibility-aware for partial person re-identification.
    In Proceedings of the 28th ACM International Conference on Multimedia (pp. 3771–3779).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao, L., Zhang, H., Gao, Z., Guan, W., Cheng, Z., & Wang, M. (2020a). 与可见性感知对齐的纹理用于部分人物重识别。在第28届ACM国际多媒体会议论文集（第3771–3779页）。
- en: (18) Gao, S., Wang, J., Lu, H., & Liu, Z. (2020b). Pose-guided visible part
    matching for occluded person reid. In Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition (pp. 11744–11752).
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao, S., Wang, J., Lu, H., & Liu, Z. (2020b). 姿态引导的可见部分匹配用于遮挡人物重识别。在IEEE/CVF计算机视觉与模式识别会议论文集（第11744–11752页）。
- en: 'Gou et al., (2018) Gou, M., Wu, Z., Rates-Borras, A., Camps, O., Radke, R. J.,
    et al. (2018). A systematic evaluation and benchmark for person re-identification:
    Features, metrics, and datasets. IEEE transactions on pattern analysis and machine
    intelligence, 41(3), 523–536.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gou, M., Wu, Z., Rates-Borras, A., Camps, O., Radke, R. J., 等. (2018). 人物重识别的系统评估与基准：特征、度量和数据集。IEEE模式分析与机器智能汇刊，41(3),
    523–536。
- en: 'Güler et al., (2018) Güler, R. A., Neverova, N., & Kokkinos, I. (2018). Densepose:
    Dense human pose estimation in the wild. In Proceedings of the IEEE conference
    on computer vision and pattern recognition (pp. 7297–7306).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Güler, R. A., Neverova, N., & Kokkinos, I. (2018). Densepose: 野外的密集人类姿态估计。在IEEE计算机视觉与模式识别会议论文集（第7297–7306页）。'
- en: Hafner et al., (2022) Hafner, F. M., Bhuyian, A., Kooij, J. F., & Granger, E.
    (2022). Cross-modal distillation for rgb-depth person re-identification. Computer
    Vision and Image Understanding, 216, 103352.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner, F. M., Bhuyian, A., Kooij, J. F., & Granger, E. (2022). RGB-深度人物重识别的跨模态蒸馏。计算机视觉与图像理解，216,
    103352。
- en: 'Han et al., (2020) Han, C., Gao, C., & Sang, N. (2020). Keypoint-based feature
    matching for partial person re-identification. In 2020 IEEE International Conference
    on Image Processing (ICIP) (pp. 226–230).: IEEE.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han, C., Gao, C., & Sang, N. (2020). 基于关键点的特征匹配用于部分人物重识别。在2020年IEEE国际图像处理会议（ICIP）（第226–230页）。:
    IEEE。'
- en: He et al., (2016) He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual
    learning for image recognition. In Proceedings of the IEEE conference on computer
    vision and pattern recognition (pp. 770–778).
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He, K., Zhang, X., Ren, S., & Sun, J. (2016). 图像识别的深度残差学习。在IEEE计算机视觉与模式识别会议论文集（第770–778页）。
- en: 'He et al., (2018) He, L., Liang, J., Li, H., & Sun, Z. (2018). Deep spatial
    feature reconstruction for partial person re-identification: Alignment-free approach.
    In Proceedings of the IEEE conference on computer vision and pattern recognition
    (pp. 7073–7082).'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He, L., Liang, J., Li, H., & Sun, Z. (2018). 深度空间特征重建用于部分人物重识别：无对齐方法。在IEEE计算机视觉与模式识别会议论文集（第7073–7082页）。
- en: 'He & Liu, (2020) He, L. & Liu, W. (2020). Guided saliency feature learning
    for person re-identification in crowded scenes. In Computer Vision–ECCV 2020:
    16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVIII
    16 (pp. 357–373).: Springer.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He, L. & Liu, W. (2020). 具指导性的显著特征学习用于拥挤场景中的人物重识别。在计算机视觉–ECCV 2020: 第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，会议论文集，第XXVIII部分，第16卷（第357–373页）。:
    施普林格。'
- en: He et al., (2019) He, L., Wang, Y., Liu, W., Zhao, H., Sun, Z., & Feng, J. (2019).
    Foreground-aware pyramid reconstruction for alignment-free occluded person re-identification.
    In Proceedings of the IEEE/CVF international conference on computer vision (pp. 8450–8459).
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He, L., Wang, Y., Liu, W., Zhao, H., Sun, Z., & Feng, J. (2019). 面向前景的金字塔重建用于无对齐遮挡人物重识别。在IEEE/CVF国际计算机视觉会议论文集（第8450–8459页）。
- en: '(27) He, S., Luo, H., Wang, P., Wang, F., Li, H., & Jiang, W. (2021a). Transreid:
    Transformer-based object re-identification. In Proceedings of the IEEE/CVF international
    conference on computer vision (pp. 15013–15022).'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He, S., Luo, H., Wang, P., Wang, F., Li, H., & Jiang, W. (2021a). Transreid:
    基于Transformer的对象重识别。在IEEE/CVF国际计算机视觉会议论文集（第15013–15022页）。'
- en: (28) He, T., Shen, X., Huang, J., Chen, Z., & Hua, X.-S. (2021b). Partial person
    re-identification with part-part correspondence learning. In Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9105–9115).
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (28) He, T., Shen, X., Huang, J., Chen, Z., & Hua, X.-S. (2021b). 通过部件-部件对应学习进行部分行人重识别。收录于IEEE/CVF计算机视觉与模式识别大会论文集（第9105–9115页）。
- en: '(29) He, Y., Yang, H., & Chen, L. (2021c). Adversarial cross-scale alignment
    pursuit for seriously misaligned person re-identification. In 2021 IEEE International
    Conference on Image Processing (ICIP) (pp. 2373–2377).: IEEE.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(29) He, Y., Yang, H., & Chen, L. (2021c). 对严重错位行人重识别的对抗性跨尺度对齐追踪。收录于2021 IEEE国际图像处理大会（ICIP）（第2373–2377页）。:
    IEEE。'
- en: Hou et al., (2021) Hou, R., Ma, B., Chang, H., Gu, X., Shan, S., & Chen, X.
    (2021). Feature completion for occluded person re-identification. IEEE Transactions
    on Pattern Analysis and Machine Intelligence, 44(9), 4894–4912.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou等（2021）Hou, R., Ma, B., Chang, H., Gu, X., Shan, S., & Chen, X. (2021). 遮挡行人重识别的特征补全。IEEE模式分析与机器智能汇刊，44(9)，4894–4912。
- en: 'Huang et al., (2020) Huang, H., Chen, X., & Huang, K. (2020). Human parsing
    based alignment with multi-task learning for occluded person re-identification.
    In 2020 IEEE international conference on multimedia and expo (ICME) (pp. 1–6).:
    IEEE.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang等（2020）Huang, H., Chen, X., & Huang, K. (2020). 基于人类解析的对齐与多任务学习用于遮挡行人重识别。收录于2020
    IEEE国际多媒体与博览会（ICME）（第1–6页）。: IEEE。'
- en: Huang et al., (2022) Huang, H., Zheng, A., Li, C., He, R., et al. (2022). Parallel
    augmentation and dual enhancement for occluded person re-identification. arXiv
    preprint arXiv:2210.05438.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等（2022）Huang, H., Zheng, A., Li, C., He, R., 等（2022）。用于遮挡行人重识别的并行增强和双重增强。arXiv预印本
    arXiv:2210.05438。
- en: 'Huo et al., (2021) Huo, L., Song, C., Liu, Z., & Zhang, Z. (2021). Attentive
    part-aware networks for partial person re-identification. In 2020 25th International
    Conference on Pattern Recognition (ICPR) (pp. 3652–3659).: IEEE.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huo等（2021）Huo, L., Song, C., Liu, Z., & Zhang, Z. (2021). 用于部分行人重识别的注意力部件感知网络。收录于2020年第25届国际模式识别大会（ICPR）（第3652–3659页）。:
    IEEE。'
- en: 'Islam, (2022) Islam, K. (2022). Recent advances in vision transformer: A survey
    and outlook of recent work. arXiv preprint arXiv:2203.01536.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Islam（2022）Islam, K. (2022). 视觉变换器的最新进展：近期工作的调查与展望。arXiv预印本 arXiv:2203.01536。
- en: Jia et al., (2022) Jia, M., Cheng, X., Lu, S., & Zhang, J. (2022). Learning
    disentangled representation implicitly via transformer for occluded person re-identification.
    IEEE Transactions on Multimedia.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia等（2022）Jia, M., Cheng, X., Lu, S., & Zhang, J. (2022). 通过变换器隐式学习解耦表示用于遮挡行人重识别。IEEE多媒体汇刊。
- en: Jin et al., (2021) Jin, H., Lai, S., & Qian, X. (2021). Occlusion-sensitive
    person re-identification via attribute-based shift attention. IEEE Transactions
    on Circuits and Systems for Video Technology, 32(4), 2170–2185.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin等（2021）Jin, H., Lai, S., & Qian, X. (2021). 通过基于属性的偏移注意力进行遮挡敏感行人重识别。IEEE视频技术电路与系统汇刊，32(4)，2170–2185。
- en: Kalayeh et al., (2018) Kalayeh, M. M., Basaran, E., Gökmen, M., Kamasak, M. E.,
    & Shah, M. (2018). Human semantic parsing for person re-identification. In Proceedings
    of the IEEE conference on computer vision and pattern recognition (pp. 1062–1071).
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalayeh等（2018）Kalayeh, M. M., Basaran, E., Gökmen, M., Kamasak, M. E., & Shah,
    M. (2018). 用于行人重识别的人类语义解析。收录于IEEE计算机视觉与模式识别大会论文集（第1062–1071页）。
- en: Kanazawa et al., (2018) Kanazawa, A., Black, M. J., Jacobs, D. W., & Malik,
    J. (2018). End-to-end recovery of human shape and pose. In Proceedings of the
    IEEE conference on computer vision and pattern recognition (pp. 7122–7131).
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanazawa等（2018）Kanazawa, A., Black, M. J., Jacobs, D. W., & Malik, J. (2018).
    人体形状和姿态的端到端恢复。收录于IEEE计算机视觉与模式识别大会论文集（第7122–7131页）。
- en: 'Khan et al., (2022) Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S.,
    & Shah, M. (2022). Transformers in vision: A survey. ACM computing surveys (CSUR),
    54(10s), 1–41.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan等（2022）Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., & Shah,
    M. (2022). 视觉中的变换器：综述。ACM计算机调查（CSUR），54(10s)，1–41。
- en: 'Kim & Yoo, (2017) Kim, J. & Yoo, C. D. (2017). Deep partial person re-identification
    via attention model. In 2017 IEEE International Conference on Image Processing
    (ICIP) (pp. 3425–3429).: IEEE.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim & Yoo（2017）Kim, J. & Yoo, C. D. (2017). 通过注意力模型进行深度部分行人重识别。收录于2017 IEEE国际图像处理大会（ICIP）（第3425–3429页）。:
    IEEE。'
- en: 'Kim et al., (2022) Kim, M., Cho, M., Lee, H., Cho, S., & Lee, S. (2022). Occluded
    person re-identification via relational adaptive feature correction learning.
    In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP) (pp. 2719–2723).: IEEE.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al., (2022) Kim, M., Cho, M., Lee, H., Cho, S., & Lee, S. (2022). 通过关系自适应特征校正学习实现遮挡人物再识别。见于
    ICASSP 2022-2022 IEEE 国际声学、语音与信号处理会议（ICASSP）（第 2719–2723 页）。：IEEE。
- en: Kiran et al., (2021) Kiran, M., Praveen, R. G., Nguyen-Meidine, L. T., Belharbi,
    S., Blais-Morin, L.-A., & Granger, E. (2021). Holistic guidance for occluded person
    re-identification. arXiv preprint arXiv:2104.06524.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiran et al., (2021) Kiran, M., Praveen, R. G., Nguyen-Meidine, L. T., Belharbi,
    S., Blais-Morin, L.-A., & Granger, E. (2021). 用于遮挡人物再识别的整体指导。arXiv 预印本 arXiv:2104.06524。
- en: 'Lavi et al., (2020) Lavi, B., Ullah, I., Fatan, M., & Rocha, A. (2020). Survey
    on reliable deep learning-based person re-identification models: Are we there
    yet? arXiv preprint arXiv:2005.00355.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lavi et al., (2020) Lavi, B., Ullah, I., Fatan, M., & Rocha, A. (2020). 可靠的深度学习基础人物再识别模型综述：我们已经到达了吗？arXiv
    预印本 arXiv:2005.00355。
- en: Leng et al., (2019) Leng, Q., Ye, M., & Tian, Q. (2019). A survey of open-world
    person re-identification. IEEE Transactions on Circuits and Systems for Video
    Technology, 30(4), 1092–1108.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leng et al., (2019) Leng, Q., Ye, M., & Tian, Q. (2019). 开放世界人物再识别综述。IEEE 视频技术电路与系统汇刊，30(4)，1092–1108。
- en: 'Li et al., (2014) Li, W., Zhao, R., Xiao, T., & Wang, X. (2014). Deepreid:
    Deep filter pairing neural network for person re-identification. In Proceedings
    of the IEEE conference on computer vision and pattern recognition (pp. 152–159).'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al., (2014) Li, W., Zhao, R., Xiao, T., & Wang, X. (2014). Deepreid:
    用于人物再识别的深度滤波配对神经网络。见于 IEEE 计算机视觉与模式识别会议论文集（第 152–159 页）。'
- en: Li et al., (2018) Li, W., Zhu, X., & Gong, S. (2018). Harmonious attention network
    for person re-identification. In Proceedings of the IEEE conference on computer
    vision and pattern recognition (pp. 2285–2294).
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al., (2018) Li, W., Zhu, X., & Gong, S. (2018). 和谐注意力网络用于人物再识别。见于 IEEE
    计算机视觉与模式识别会议论文集（第 2285–2294 页）。
- en: Li et al., (2020) Li, Y., Jiang, X., & Hwang, J.-N. (2020). Effective person
    re-identification by self-attention model guided feature learning. Knowledge-Based
    Systems, 187, 104832.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al., (2020) Li, Y., Jiang, X., & Hwang, J.-N. (2020). 通过自注意力模型引导特征学习实现有效的人物再识别。知识基础系统，187，104832。
- en: Li et al., (2021) Li, Y., Liu, L., Zhu, L., & Zhang, H. (2021). Person re-identification
    based on multi-scale feature learning. Knowledge-Based Systems, 228, 107281.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al., (2021) Li, Y., Liu, L., Zhu, L., & Zhang, H. (2021). 基于多尺度特征学习的人物再识别。知识基础系统，228，107281。
- en: 'Li et al., (2019) Li, Y.-J., Chen, Y.-C., Lin, Y.-Y., Du, X., & Wang, Y.-C. F.
    (2019). Recover and identify: A generative dual model for cross-resolution person
    re-identification. In Proceedings of the IEEE/CVF international conference on
    computer vision (pp. 8090–8099).'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al., (2019) Li, Y.-J., Chen, Y.-C., Lin, Y.-Y., Du, X., & Wang, Y.-C.
    F. (2019). 恢复与识别：一种用于跨分辨率人物再识别的生成双模型。见于 IEEE/CVF 国际计算机视觉会议论文集（第 8090–8099 页）。
- en: 'Liang et al., (2018) Liang, X., Gong, K., Shen, X., & Lin, L. (2018). Look
    into person: Joint body parsing & pose estimation network and a new benchmark.
    IEEE transactions on pattern analysis and machine intelligence, 41(4), 871–885.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al., (2018) Liang, X., Gong, K., Shen, X., & Lin, L. (2018). 关注人物：联合体解析与姿态估计网络及新基准。IEEE
    模式分析与机器智能汇刊，41(4)，871–885。
- en: 'Lin & Wang, (2021) Lin, C.-S. & Wang, Y.-C. F. (2021). Self-supervised bodymap-to-appearance
    co-attention for partial person re-identification. In 2021 IEEE International
    Conference on Image Processing (ICIP) (pp. 2299–2303).: IEEE.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin & Wang, (2021) Lin, C.-S. & Wang, Y.-C. F. (2021). 自监督体图与外观协同注意力用于部分人物再识别。见于
    2021 IEEE 国际图像处理会议（ICIP）（第 2299–2303 页）。：IEEE。
- en: 'Lin et al., (2014) Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
    Ramanan, D., Dollár, P., & Zitnick, C. L. (2014). Microsoft coco: Common objects
    in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland,
    September 6-12, 2014, Proceedings, Part V 13 (pp. 740–755).: Springer.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al., (2014) Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
    Ramanan, D., Dollár, P., & Zitnick, C. L. (2014). Microsoft coco: 上下文中的常见物体。见于计算机视觉–ECCV
    2014: 第 13 届欧洲会议，瑞士苏黎世，2014 年 9 月 6-12 日，论文集，第 V 部分 13（第 740–755 页）。：Springer。'
- en: Liu et al., (2017) Liu, H., Feng, J., Jie, Z., Jayashree, K., Zhao, B., Qi,
    M., Jiang, J., & Yan, S. (2017). Neural person search machines. In Proceedings
    of the IEEE International Conference on Computer Vision (pp. 493–501).
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al., (2017) Liu, H., Feng, J., Jie, Z., Jayashree, K., Zhao, B., Qi,
    M., Jiang, J., & Yan, S. (2017). 神经人物搜索机器。见于 IEEE 国际计算机视觉会议论文集（第 493–501 页）。
- en: Liu et al., (2019) Liu, J., Zha, Z.-J., Hong, R., Wang, M., & Zhang, Y. (2019).
    Deep adversarial graph attention convolution network for text-based person search.
    In Proceedings of the 27th ACM International Conference on Multimedia (pp. 665–673).
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2019）Liu, J., Zha, Z.-J., Hong, R., Wang, M., & Zhang, Y.（2019）。用于基于文本的人物搜索的深度对抗图注意卷积网络。收录于第27届ACM国际多媒体会议论文集（第665–673页）。
- en: Liu et al., (2022) Liu, Q., Teng, Q., Chen, H., Li, B., & Qing, L. (2022). Dual
    adaptive alignment and partitioning network for visible and infrared cross-modality
    person re-identification. Applied Intelligence, 52(1), 547–563.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2022）Liu, Q., Teng, Q., Chen, H., Li, B., & Qing, L.（2022）。用于可见光和红外跨模态人体再识别的双重自适应对齐和分区网络。《应用智能》，52（1），547–563。
- en: 'Liu et al., (2021) Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,
    S., & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using
    shifted windows. In Proceedings of the IEEE/CVF international conference on computer
    vision (pp. 10012–10022).'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2021）Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S.,
    & Guo, B.（2021）。Swin 变换器：使用位移窗口的分层视觉变换器。收录于IEEE/CVF国际计算机视觉会议论文集（第10012–10022页）。
- en: 'Loper et al., (2015) Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., & Black,
    M. J. (2015). Smpl: A skinned multi-person linear model. ACM transactions on graphics
    (TOG), 34(6), 1–16.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loper 等人（2015）Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., & Black, M.
    J.（2015）。SMPL：一个皮肤多人体线性模型。《ACM图形学学报》（TOG），34（6），1–16。
- en: Luo et al., (2019) Luo, H., Jiang, W., Gu, Y., Liu, F., Liao, X., Lai, S., &
    Gu, J. (2019). A strong baseline and batch normneuralization neck for deep person
    reidentification. arXiv preprint arXiv:1906.08332.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人（2019）Luo, H., Jiang, W., Gu, Y., Liu, F., Liao, X., Lai, S., & Gu, J.（2019）。用于深度人体再识别的强基线和批量归一化颈部。arXiv预印本
    arXiv:1906.08332。
- en: Ma et al., (2021) Ma, Z., Zhao, Y., & Li, J. (2021). Pose-guided inter-and intra-part
    relational transformer for occluded person re-identification. In Proceedings of
    the 29th ACM International Conference on Multimedia (pp. 1487–1496).
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等人（2021）Ma, Z., Zhao, Y., & Li, J.（2021）。用于遮挡人体再识别的姿态引导的部件间和部件内关系变换器。收录于第29届ACM国际多媒体会议论文集（第1487–1496页）。
- en: Mao et al., (2019) Mao, S., Zhang, S., & Yang, M. (2019). Resolution-invariant
    person re-identification. arXiv preprint arXiv:1906.09748.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等人（2019）Mao, S., Zhang, S., & Yang, M.（2019）。分辨率不变的人体再识别。arXiv预印本 arXiv:1906.09748。
- en: 'Martinel et al., (2016) Martinel, N., Das, A., Micheloni, C., & Roy-Chowdhury,
    A. K. (2016). Temporal model adaptation for person re-identification. In Computer
    Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October
    11–14, 2016, Proceedings, Part IV 14 (pp. 858–877).: Springer.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martinel 等人（2016）Martinel, N., Das, A., Micheloni, C., & Roy-Chowdhury, A. K.（2016）。用于人体再识别的时间模型适配。收录于计算机视觉–ECCV
    2016：第14届欧洲会议，荷兰阿姆斯特丹，2016年10月11–14日，论文集，第IV部分（第858–877页）。：Springer。
- en: Miao et al., (2019) Miao, J., Wu, Y., Liu, P., Ding, Y., & Yang, Y. (2019).
    Pose-guided feature alignment for occluded person re-identification. In Proceedings
    of the IEEE/CVF international conference on computer vision (pp. 542–551).
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miao 等人（2019）Miao, J., Wu, Y., Liu, P., Ding, Y., & Yang, Y.（2019）。用于遮挡人体再识别的姿态引导特征对齐。收录于IEEE/CVF国际计算机视觉会议论文集（第542–551页）。
- en: Miao et al., (2021) Miao, J., Wu, Y., & Yang, Y. (2021). Identifying visible
    parts via pose estimation for occluded person re-identification. IEEE transactions
    on neural networks and learning systems, 33(9), 4624–4634.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miao 等人（2021）Miao, J., Wu, Y., & Yang, Y.（2021）。通过姿态估计识别可见部位用于遮挡人体再识别。《IEEE神经网络与学习系统汇刊》，33（9），4624–4634。
- en: 'Ming et al., (2022) Ming, Z., Zhu, M., Wang, X., Zhu, J., Cheng, J., Gao, C.,
    Yang, Y., & Wei, X. (2022). Deep learning-based person re-identification methods:
    A survey and outlook of recent works. Image and Vision Computing, 119, 104394.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ming 等人（2022）Ming, Z., Zhu, M., Wang, X., Zhu, J., Cheng, J., Gao, C., Yang,
    Y., & Wei, X.（2022）。基于深度学习的人体再识别方法：最近工作的调查与展望。《图像与视觉计算》，119，104394。
- en: 'Nagaraju et al., (2016) Nagaraju, G., Raju, G. S. R., Ko, Y. H., & Yu, J. S.
    (2016). Hierarchical ni–co layered double hydroxide nanosheets entrapped on conductive
    textile fibers: a cost-effective and flexible electrode for high-performance pseudocapacitors.
    Nanoscale, 8(2), 812–825.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagaraju 等人（2016）Nagaraju, G., Raju, G. S. R., Ko, Y. H., & Yu, J. S.（2016）。层状双氢氧化物纳米片被包覆在导电纺织纤维上：一种高性价比且灵活的电极，用于高性能假电容器。《纳米尺度》，8（2），812–825。
- en: Nguyen et al., (2017) Nguyen, D. T., Hong, H. G., Kim, K. W., & Park, K. R.
    (2017). Person recognition system based on a combination of body images from visible
    light and thermal cameras. Sensors, 17(3), 605.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等人，（2017）Nguyen, D. T., Hong, H. G., Kim, K. W., & Park, K. R. (2017).
    基于可见光和热成像相机的身体图像组合的人物识别系统。Sensors, 17(3), 605。
- en: 'Ning et al., (2021) Ning, X., Gong, K., Li, W., & Zhang, L. (2021). Jwsaa:
    joint weak saliency and attention aware for person re-identification. Neurocomputing,
    453, 801–811.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ning 等人，（2021）Ning, X., Gong, K., Li, W., & Zhang, L. (2021). Jwsaa: 结合弱显著性和注意力机制进行人物再识别。Neurocomputing,
    453, 801–811。'
- en: (68) Ning, X., Gong, K., Li, W., Zhang, L., Bai, X., & Tian, S. (2020a). Feature
    refinement and filter network for person re-identification. IEEE Transactions
    on Circuits and Systems for Video Technology, 31(9), 3391–3402.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (68) Ning, X., Gong, K., Li, W., Zhang, L., Bai, X., & Tian, S. (2020a). 特征精炼和滤波网络用于人物再识别。IEEE
    Transactions on Circuits and Systems for Video Technology, 31(9), 3391–3402。
- en: '(69) Ning, X., Nan, F., Xu, S., Yu, L., & Zhang, L. (2020b). Multi-view frontal
    face image generation: a survey. Concurrency and Computation: Practice and Experience,
    (pp. e6147).'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(69) Ning, X., Nan, F., Xu, S., Yu, L., & Zhang, L. (2020b). 多视角正面人脸图像生成：综述。Concurrency
    and Computation: Practice and Experience, （第 e6147 页）。'
- en: 'Park et al., (2021) Park, H., Lee, S., Lee, J., & Ham, B. (2021). Learning
    by aligning: Visible-infrared person re-identification using cross-modal correspondences.
    In Proceedings of the IEEE/CVF international conference on computer vision (pp. 12046–12055).'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人，（2021）Park, H., Lee, S., Lee, J., & Ham, B. (2021). 通过对齐学习：使用跨模态对应的可见-红外人物再识别。收录于IEEE/CVF国际计算机视觉会议（第
    12046–12055 页）。
- en: 'Peng et al., (2022) Peng, Y., Hou, S., Cao, C., Liu, X., Huang, Y., & He, Z.
    (2022). Deep learning-based occluded person re-identification: A survey. arXiv
    preprint arXiv:2207.14452.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人，（2022）Peng, Y., Hou, S., Cao, C., Liu, X., Huang, Y., & He, Z. (2022).
    基于深度学习的遮挡人物再识别：综述。arXiv 预印本 arXiv:2207.14452。
- en: 'Qi et al., (2017) Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). Pointnet:
    Deep learning on point sets for 3d classification and segmentation. In Proceedings
    of the IEEE conference on computer vision and pattern recognition (pp. 652–660).'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qi 等人，（2017）Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). Pointnet: 对点集进行
    3d 分类和分割的深度学习。收录于IEEE计算机视觉与模式识别会议（第 652–660 页）。'
- en: Quispe & Pedrini, (2019) Quispe, R. & Pedrini, H. (2019). Improved person re-identification
    based on saliency and semantic parsing with deep neural network models. Image
    and Vision Computing, 92, 103809.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quispe & Pedrini，（2019）Quispe, R. & Pedrini, H. (2019). 基于显著性和语义解析的改进人物再识别，采用深度神经网络模型。Image
    and Vision Computing, 92, 103809。
- en: 'Ren et al., (2020) Ren, X., Zhang, D., & Bao, X. (2020). Semantic-guided shared
    feature alignment for occluded person re-identification. In Asian Conference on
    Machine Learning (pp. 17–32).: PMLR.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren 等人，（2020）Ren, X., Zhang, D., & Bao, X. (2020). 基于语义引导的共享特征对齐用于遮挡人物再识别。收录于亚洲机器学习会议（第
    17–32 页）。: PMLR。'
- en: 'Ristani et al., (2016) Ristani, E., Solera, F., Zou, R., Cucchiara, R., & Tomasi,
    C. (2016). Performance measures and a data set for multi-target, multi-camera
    tracking. In Computer Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands,
    October 8-10 and 15-16, 2016, Proceedings, Part II (pp. 17–35).: Springer.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ristani 等人，（2016）Ristani, E., Solera, F., Zou, R., Cucchiara, R., & Tomasi,
    C. (2016). 多目标、多摄像头跟踪的性能度量和数据集。收录于计算机视觉–ECCV 2016 工作坊：荷兰阿姆斯特丹，2016年10月8-10日和15-16日，会议录，第
    II 部分（第 17–35 页）。: Springer。'
- en: Sarfraz et al., (2018) Sarfraz, M. S., Schumann, A., Eberle, A., & Stiefelhagen,
    R. (2018). A pose-sensitive embedding for person re-identification with expanded
    cross neighborhood re-ranking. In Proceedings of the IEEE conference on computer
    vision and pattern recognition (pp. 420–429).
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarfraz 等人，（2018）Sarfraz, M. S., Schumann, A., Eberle, A., & Stiefelhagen, R.
    (2018). 一种用于人物再识别的姿态敏感嵌入与扩展的交叉邻域重新排序。收录于IEEE计算机视觉与模式识别会议（第 420–429 页）。
- en: Sekhar et al., (2017) Sekhar, S. C., Nagaraju, G., & Yu, J. S. (2017). Conductive
    silver nanowires-fenced carbon cloth fibers-supported layered double hydroxide
    nanosheets as a flexible and binder-free electrode for high-performance asymmetric
    supercapacitors. Nano Energy, 36, 58–67.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sekhar 等人，（2017）Sekhar, S. C., Nagaraju, G., & Yu, J. S. (2017). 导电银纳米线围绕的碳布纤维支持的层状双氢氧化物纳米片作为高性能非对称超级电容器的柔性无粘合剂电极。Nano
    Energy, 36, 58–67。
- en: 'Shamshad et al., (2023) Shamshad, F., Khan, S., Zamir, S. W., Khan, M. H.,
    Hayat, M., Khan, F. S., & Fu, H. (2023). Transformers in medical imaging: A survey.
    Medical Image Analysis, (pp. 102802).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shamshad 等人，（2023）Shamshad, F., Khan, S., Zamir, S. W., Khan, M. H., Hayat,
    M., Khan, F. S., & Fu, H. (2023). 医学成像中的 Transformers：综述。Medical Image Analysis,
    （第 102802 页）。
- en: Su et al., (2017) Su, C., Li, J., Zhang, S., Xing, J., Gao, W., & Tian, Q. (2017).
    Pose-driven deep convolutional model for person re-identification. In Proceedings
    of the IEEE international conference on computer vision (pp. 3960–3969).
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等（2017）Su, C., Li, J., Zhang, S., Xing, J., Gao, W., & Tian, Q.（2017）。基于姿态驱动的深度卷积模型用于人员再识别。在IEEE国际计算机视觉会议论文集中（第3960–3969页）。
- en: (80) Sun, K., Xiao, B., Liu, D., & Wang, J. (2019a). Deep high-resolution representation
    learning for human pose estimation. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition (pp. 5693–5703).
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (80) Sun, K., Xiao, B., Liu, D., & Wang, J.（2019a）。用于人体姿态估计的深度高分辨率表示学习。在IEEE/CVF计算机视觉与模式识别会议论文集中（第5693–5703页）。
- en: Sun & Zheng, (2019) Sun, X. & Zheng, L. (2019). Dissecting person re-identification
    from the viewpoint of viewpoint. In Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition (pp. 608–617).
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun & Zheng（2019）Sun, X. & Zheng, L.（2019）。从视角的角度剖析人员再识别。在IEEE/CVF计算机视觉与模式识别会议论文集中（第608–617页）。
- en: '(82) Sun, Y., Xu, Q., Li, Y., Zhang, C., Li, Y., Wang, S., & Sun, J. (2019b).
    Perceive where to focus: Learning visibility-aware part-level features for partial
    person re-identification. In Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition (pp. 393–402).'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (82) Sun, Y., Xu, Q., Li, Y., Zhang, C., Li, Y., Wang, S., & Sun, J.（2019b）。感知焦点：学习可见性感知的部件级特征用于部分人员再识别。在IEEE/CVF计算机视觉与模式识别会议论文集中（第393–402页）。
- en: 'Sun et al., (2018) Sun, Y., Zheng, L., Yang, Y., Tian, Q., & Wang, S. (2018).
    Beyond part models: Person retrieval with refined part pooling (and a strong convolutional
    baseline). In Proceedings of the European conference on computer vision (ECCV)
    (pp. 480–496).'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2018）Sun, Y., Zheng, L., Yang, Y., Tian, Q., & Wang, S.（2018）。超越部件模型：通过精细的部件池化进行人员检索（以及强大的卷积基线）。在欧洲计算机视觉会议（ECCV）论文集中（第480–496页）。
- en: Szegedy et al., (2016) Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., &
    Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In
    Proceedings of the IEEE conference on computer vision and pattern recognition
    (pp. 2818–2826).
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等（2016）Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z.（2016）。重新思考计算机视觉中的
    inception 结构。在IEEE计算机视觉与模式识别会议论文集中（第2818–2826页）。
- en: Tan et al., (2021) Tan, H., Liu, X., Bian, Y., Wang, H., & Yin, B. (2021). Incomplete
    descriptor mining with elastic loss for person re-identification. IEEE Transactions
    on Circuits and Systems for Video Technology, 32(1), 160–171.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 等（2021）Tan, H., Liu, X., Bian, Y., Wang, H., & Yin, B.（2021）。基于弹性损失的不完整描述符挖掘用于人员再识别。IEEE
    影视技术电路与系统汇刊，32(1)，160–171。
- en: '(86) Tan, H., Liu, X., Yin, B., & Li, X. (2022a). Mhsa-net: Multihead self-attention
    network for occluded person re-identification. IEEE Transactions on Neural Networks
    and Learning Systems.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (86) Tan, H., Liu, X., Yin, B., & Li, X.（2022a）。Mhsa-net：用于遮挡人员再识别的多头自注意力网络。IEEE
    神经网络与学习系统汇刊。
- en: (87) Tan, L., Dai, P., Ji, R., & Wu, Y. (2022b). Dynamic prototype mask for
    occluded person re-identification. In Proceedings of the 30th ACM International
    Conference on Multimedia (pp. 531–540).
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (87) Tan, L., Dai, P., Ji, R., & Wu, Y.（2022b）。用于遮挡人员再识别的动态原型掩码。在第30届ACM国际多媒体会议论文集中（第531–540页）。
- en: Tirkolaee et al., (2020) Tirkolaee, E. B., Goli, A., & Weber, G.-W. (2020).
    Fuzzy mathematical programming and self-adaptive artificial fish swarm algorithm
    for just-in-time energy-aware flow shop scheduling problem with outsourcing option.
    IEEE transactions on fuzzy systems, 28(11), 2772–2783.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tirkolaee 等（2020）Tirkolaee, E. B., Goli, A., & Weber, G.-W.（2020）。模糊数学编程和自适应人工鱼群算法用于具有外包选项的即时能源感知流车间调度问题。IEEE
    模糊系统汇刊，28(11)，2772–2783。
- en: Tutsoy et al., (2018) Tutsoy, O., Barkana, D. E., & Tugal, H. (2018). Design
    of a completely model free adaptive control in the presence of parametric, non-parametric
    uncertainties and random control signal delay. ISA transactions, 76, 67–77.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tutsoy 等（2018）Tutsoy, O., Barkana, D. E., & Tugal, H.（2018）。在存在参数性、非参数性不确定性和随机控制信号延迟的情况下，设计完全模型自适应控制。ISA
    交易，76，67–77。
- en: 'Tutsoy & Tanrikulu, (2022) Tutsoy, O. & Tanrikulu, M. Y. (2022). Priority and
    age specific vaccination algorithm for the pandemic diseases: a comprehensive
    parametric prediction model. BMC Medical Informatics and Decision Making, 22(1),
    4.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tutsoy & Tanrikulu（2022）Tutsoy, O. & Tanrikulu, M. Y.（2022）。针对流行病的优先级和年龄特定疫苗接种算法：一个全面的参数预测模型。BMC
    医学信息学与决策制定，22(1)，4。
- en: (91) Wang, C., Ning, X., Sun, L., Zhang, L., Li, W., & Bai, X. (2022a). Learning
    discriminative features by covering local geometric space for point cloud analysis.
    IEEE Transactions on Geoscience and Remote Sensing, 60, 1–15.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (91) Wang, C., Ning, X., Sun, L., Zhang, L., Li, W., & Bai, X. (2022a). 通过覆盖局部几何空间学习判别特征用于点云分析。IEEE地球科学与遥感学报,
    60, 1–15.
- en: (92) Wang, C., Wang, C., Li, W., & Wang, H. (2021a). A brief survey on rgb-d
    semantic segmentation using deep learning. Displays, 70, 102080.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (92) Wang, C., Wang, C., Li, W., & Wang, H. (2021a). 关于使用深度学习的RGB-D语义分割的简要调查。《显示器》，70,
    102080.
- en: '(93) Wang, G., Yang, S., Liu, H., Wang, Z., Yang, Y., Wang, S., Yu, G., Zhou,
    E., & Sun, J. (2020a). High-order information matters: Learning relation and topology
    for occluded person re-identification. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition (pp. 6449–6458).'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (93) Wang, G., Yang, S., Liu, H., Wang, Z., Yang, Y., Wang, S., Yu, G., Zhou,
    E., & Sun, J. (2020a). 高阶信息的重要性：学习关系和拓扑用于遮挡行人重识别。见IEEE/CVF计算机视觉与模式识别会议论文集（第6449–6458页）。
- en: (94) Wang, G., Zhang, T., Cheng, J., Liu, S., Yang, Y., & Hou, Z. (2019a). Rgb-infrared
    cross-modality person re-identification via joint pixel and feature alignment.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 3623–3632).
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (94) Wang, G., Zhang, T., Cheng, J., Liu, S., Yang, Y., & Hou, Z. (2019a). RGB-红外跨模态行人重识别通过联合像素和特征对齐。见IEEE/CVF国际计算机视觉会议论文集（第3623–3632页）。
- en: '(95) Wang, J., Tan, S., Zhen, X., Xu, S., Zheng, F., He, Z., & Shao, L. (2021b).
    Deep 3d human pose estimation: A review. Computer Vision and Image Understanding,
    210, 103225.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (95) Wang, J., Tan, S., Zhen, X., Xu, S., Zheng, F., He, Z., & Shao, L. (2021b).
    深度3D人类姿态估计：综述。计算机视觉与图像理解, 210, 103225.
- en: (96) Wang, P., Ding, C., Shao, Z., Hong, Z., Zhang, S., & Tao, D. (2022b). Quality-aware
    part models for occluded person re-identification. IEEE Transactions on Multimedia.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (96) Wang, P., Ding, C., Shao, Z., Hong, Z., Zhang, S., & Tao, D. (2022b). 针对遮挡行人的质量感知部分模型。IEEE多媒体学报。
- en: (97) Wang, Q., Huang, H., Zhong, Y., Min, W., Han, Q., Xu, D., & Xu, C. (2022c).
    Swin transformer based on two-fold loss and background adaptation re-ranking for
    person re-identification. Electronics, 11(13), 1941.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (97) Wang, Q., Huang, H., Zhong, Y., Min, W., Han, Q., Xu, D., & Xu, C. (2022c).
    基于双重损失和背景适应重新排名的Swin变换器用于行人重识别。《电子学》, 11(13), 1941.
- en: '(98) Wang, Q., Qi, M., Jin, K., & Jiang, J. (2020b). Deep-shallow occlusion
    parallelism network for person re-identification. In Journal of Physics: Conference
    Series, volume 1518 (pp. 012026).: IOP Publishing.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(98) Wang, Q., Qi, M., Jin, K., & Jiang, J. (2020b). 用于行人重识别的深浅遮挡并行网络。见《物理学杂志：会议系列》，第1518卷（第012026页）。:
    IOP Publishing.'
- en: (99) Wang, T., Liu, H., Song, P., Guo, T., & Shi, W. (2022d). Pose-guided feature
    disentangling for occluded person re-identification based on transformer. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 36 (pp. 2540–2549).
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (99) Wang, T., Liu, H., Song, P., Guo, T., & Shi, W. (2022d). 基于变换器的姿态引导特征解耦用于遮挡行人重识别。见AAAI人工智能会议论文集，第36卷（第2540–2549页）。
- en: (100) Wang, X., Li, C., & Ma, X. (2022e). Cross-modal local shortest path and
    global enhancement for visible-thermal person re-identification. arXiv preprint
    arXiv:2206.04401.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (100) Wang, X., Li, C., & Ma, X. (2022e). 可见-热成像行人重识别的跨模态局部最短路径和全局增强。arXiv预印本
    arXiv:2206.04401.
- en: (101) Wang, Y., Liang, X., & Liao, S. (2022f). Cloning outfits from real-world
    images to 3d characters for generalizable person re-identification. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4900–4909).
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (101) Wang, Y., Liang, X., & Liao, S. (2022f). 从现实世界图像到3D角色的服装克隆，用于通用行人重识别。见IEEE/CVF计算机视觉与模式识别会议论文集（第4900–4909页）。
- en: '(102) Wang, Z., Wang, Z., Wu, Y., Wang, J., & Satoh, S. (2019b). Beyond intra-modality
    discrepancy: A comprehensive survey of heterogeneous person re-identification.
    arXiv preprint arXiv:1905.10048, 4.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (102) Wang, Z., Wang, Z., Wu, Y., Wang, J., & Satoh, S. (2019b). 超越模态内部差异：异质行人重识别的全面调查。arXiv预印本
    arXiv:1905.10048, 4.
- en: (103) Wang, Z., Wang, Z., Zheng, Y., Chuang, Y.-Y., & Satoh, S. (2019c). Learning
    to reduce dual-level discrepancy for infrared-visible person re-identification.
    In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
    (pp. 618–626).
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (103) Wang, Z., Wang, Z., Zheng, Y., Chuang, Y.-Y., & Satoh, S. (2019c). 学习减少双层差异用于红外-可见行人重识别。见IEEE/CVF计算机视觉与模式识别会议论文集（第618–626页）。
- en: (104) Wang, Z., Zhu, F., Tang, S., Zhao, R., He, L., & Song, J. (2022g). Feature
    erasing and diffusion network for occluded person re-identification. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4754–4763).
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (104) Wang, Z., Zhu, F., Tang, S., Zhao, R., He, L., & Song, J. (2022g). 特征擦除与扩散网络用于遮挡行人重新识别。《IEEE/CVF计算机视觉与模式识别会议论文集》，（第4754–4763页）。
- en: Wen et al., (2022) Wen, X., Feng, X., Li, P., & Chen, W. (2022). Cross-modality
    collaborative learning identified pedestrian. The Visual Computer, (pp. 1–16).
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen等（2022）Wen, X., Feng, X., Li, P., & Chen, W. (2022). 跨模态协作学习识别行人。《视觉计算机》，（第1–16页）。
- en: (106) Wu, A., Zheng, W.-S., & Lai, J.-H. (2017a). Robust depth-based person
    re-identification. IEEE Transactions on Image Processing, 26(6), 2588–2603.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (106) Wu, A., Zheng, W.-S., & Lai, J.-H. (2017a). 基于深度的鲁棒行人重新识别。《IEEE图像处理汇刊》，26(6),
    2588–2603。
- en: (107) Wu, A., Zheng, W.-S., Yu, H.-X., Gong, S., & Lai, J. (2017b). Rgb-infrared
    cross-modality person re-identification. In Proceedings of the IEEE international
    conference on computer vision (pp. 5380–5389).
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (107) Wu, A., Zheng, W.-S., Yu, H.-X., Gong, S., & Lai, J. (2017b). RGB-红外跨模态行人重新识别。《IEEE国际计算机视觉会议论文集》，（第5380–5389页）。
- en: 'Wu et al., (2018) Wu, Y., Lin, Y., Dong, X., Yan, Y., Ouyang, W., & Yang, Y.
    (2018). Exploit the unknown gradually: One-shot video-based person re-identification
    by stepwise learning. In Proceedings of the IEEE conference on computer vision
    and pattern recognition (pp. 5177–5186).'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等（2018）Wu, Y., Lin, Y., Dong, X., Yan, Y., Ouyang, W., & Yang, Y. (2018).
    逐步挖掘未知：基于视频的一次性行人重新识别。《IEEE计算机视觉与模式识别会议论文集》，（第5177–5186页）。
- en: 'Xie et al., (2020) Xie, Y., Tian, J., & Zhu, X. X. (2020). Linking points with
    labels in 3d: A review of point cloud semantic segmentation. IEEE Geoscience and
    remote sensing magazine, 8(4), 38–59.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等（2020）Xie, Y., Tian, J., & Zhu, X. X. (2020). 3D点云语义分割中的标签点链接：点云语义分割综述。《IEEE地球科学与遥感杂志》，8(4),
    38–59。
- en: Xu et al., (2022) Xu, B., He, L., Liang, J., & Sun, Z. (2022). Learning feature
    recovery transformer for occluded person re-identification. IEEE Transactions
    on Image Processing, 31, 4651–4662.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等（2022）Xu, B., He, L., Liang, J., & Sun, Z. (2022). 用于遮挡行人重新识别的特征恢复变换器。《IEEE图像处理汇刊》，31,
    4651–4662。
- en: Xu et al., (2018) Xu, J., Zhao, R., Zhu, F., Wang, H., & Ouyang, W. (2018).
    Attention-aware compositional network for person re-identification. In Proceedings
    of the IEEE conference on computer vision and pattern recognition (pp. 2119–2128).
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等（2018）Xu, J., Zhao, R., Zhu, F., Wang, H., & Ouyang, W. (2018). 注意力感知组合网络用于行人重新识别。《IEEE计算机视觉与模式识别会议论文集》，（第2119–2128页）。
- en: Xu et al., (2021) Xu, Y., Zhao, L., & Qin, F. (2021). Dual attention-based method
    for occluded person re-identification. Knowledge-Based Systems, 212, 106554.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等（2021）Xu, Y., Zhao, L., & Qin, F. (2021). 基于双重注意力的遮挡行人重新识别方法。《知识-based系统》，212,
    106554。
- en: 'Yaghoubi et al., (2021) Yaghoubi, E., Kumar, A., & Proença, H. (2021). Sss-pr:
    A short survey of surveys in person re-identification. Pattern Recognition Letters,
    143, 50–57.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yaghoubi等（2021）Yaghoubi, E., Kumar, A., & Proença, H. (2021). Sss-pr：行人重新识别的短期调查综述。《模式识别信函》，143,
    50–57。
- en: Yan et al., (2021) Yan, C., Pang, G., Jiao, J., Bai, X., Feng, X., & Shen, C.
    (2021). Occluded person re-identification with single-scale global representations.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 11875–11884).
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan等（2021）Yan, C., Pang, G., Jiao, J., Bai, X., Feng, X., & Shen, C. (2021).
    基于单尺度全局表示的遮挡行人重新识别。《IEEE/CVF国际计算机视觉会议论文集》，（第11875–11884页）。
- en: 'Yang et al., (2022) Yang, J., Zhang, C., Tang, Y., & Li, Z. (2022). Pafm: pose-drive
    attention fusion mechanism for occluded person re-identification. Neural Computing
    and Applications, 34(10), 8241–8252.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2022）Yang, J., Zhang, C., Tang, Y., & Li, Z. (2022). Pafm：用于遮挡行人重新识别的姿态驱动注意力融合机制。《神经计算与应用》，34(10),
    8241–8252。
- en: 'Yang et al., (2021) Yang, J., Zhang, J., Yu, F., Jiang, X., Zhang, M., Sun,
    X., Chen, Y.-C., & Zheng, W.-S. (2021). Learning to know where to see: A visibility-aware
    approach for occluded person re-identification. In Proceedings of the IEEE/CVF
    international conference on computer vision (pp. 11885–11894).'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2021）Yang, J., Zhang, J., Yu, F., Jiang, X., Zhang, M., Sun, X., Chen,
    Y.-C., & Zheng, W.-S. (2021). 学习知道何处观察：一种针对遮挡行人重新识别的可见性感知方法。《IEEE/CVF国际计算机视觉会议论文集》，（第11885–11894页）。
- en: Yang et al., (2020) Yang, W., Yan, Y., Chen, S., Zhang, X., & Wang, H. (2020).
    Multi-scale generative adversarial network for person reidentification under occlusion.
    Journal of Software, 31(7), 1943–1958.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2020）Yang, W., Yan, Y., Chen, S., Zhang, X., & Wang, H. (2020). 用于遮挡下行人重新识别的多尺度生成对抗网络。《软件学报》，31(7),
    1943–1958。
- en: (118) Ye, M., Chen, C., Shen, J., & Shao, L. (2021a). Dynamic tri-level relation
    mining with attentive graph for visible infrared re-identification. IEEE Transactions
    on Information Forensics and Security, 17, 386–398.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (118) Ye, M., Chen, C., Shen, J., & Shao, L.（2021a）。基于注意力图的动态三层关系挖掘用于可见-红外重识别。IEEE
    信息取证与安全汇刊，17，386–398。
- en: 'Ye et al., (2020) Ye, M., Shen, J., J. Crandall, D., Shao, L., & Luo, J. (2020).
    Dynamic dual-attentive aggregation learning for visible-infrared person re-identification.
    In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part XVII 16 (pp. 229–247).: Springer.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 等人（2020）叶敏、沈俊、Crandall J、邵雷、罗俊（2020）。动态双重注意力聚合学习用于可见-红外行人重识别。在计算机视觉–ECCV
    2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第XVII部分16（第229–247页）。：Springer。
- en: '(120) Ye, M., Shen, J., Lin, G., Xiang, T., Shao, L., & Hoi, S. C. (2021b).
    Deep learning for person re-identification: A survey and outlook. IEEE transactions
    on pattern analysis and machine intelligence, 44(6), 2872–2893.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (120) Ye, M., Shen, J., Lin, G., Xiang, T., Shao, L., & Hoi, S. C.（2021b）。行人重识别的深度学习：综述与展望。IEEE
    模式分析与机器智能汇刊，44（6），2872–2893。
- en: 'Zhai et al., (2021) Zhai, Y., Han, X., Ma, W., Gou, X., & Xiao, G. (2021).
    Pgmanet: Pose-guided mixed attention network for occluded person re-identification.
    In 2021 International Joint Conference on Neural Networks (IJCNN) (pp. 1–8).:
    IEEE.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai 等人（2021）翟颖、韩旭、马伟、勾旭、肖广（2021）。Pgmanet：用于遮挡行人重识别的姿态引导混合注意力网络。在2021年国际联合神经网络会议（IJCNN）（第1–8页）。IEEE。
- en: 'Zhang et al., (2021) Zhang, C., Liu, H., Guo, W., & Ye, M. (2021). Multi-scale
    cascading network with compact feature learning for rgb-infrared person re-identification.
    In 2020 25th International Conference on Pattern Recognition (ICPR) (pp. 8679–8686).:
    IEEE.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2021）张超、刘辉、郭伟、叶敏（2021）。具有紧凑特征学习的多尺度级联网络用于RGB-红外行人重识别。在2020年第25届国际模式识别大会（ICPR）（第8679–8686页）。IEEE。
- en: (123) Zhang, G., Chen, C., Chen, Y., Zhang, H., & Zheng, Y. (2022a). Fine-grained-based
    multi-feature fusion for occluded person re-identification. Journal of Visual
    Communication and Image Representation, 87, 103581.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (123) Zhang, G., Chen, C., Chen, Y., Zhang, H., & Zheng, Y.（2022a）。基于细粒度的多特征融合用于遮挡行人重识别。视觉通信与图像表示期刊，87，103581。
- en: (124) Zhang, L., Guo, H., Zhu, K., Qiao, H., Huang, G., Zhang, S., Zhang, H.,
    Sun, J., & Wang, J. (2022b). Hybrid modality metric learning for visible-infrared
    person re-identification. ACM Transactions on Multimedia Computing, Communications,
    and Applications (TOMM), 18(1s), 1–15.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (124) Zhang, L., Guo, H., Zhu, K., Qiao, H., Huang, G., Zhang, S., Zhang, H.,
    Sun, J., & Wang, J.（2022b）。用于可见-红外行人重识别的混合模态度量学习。ACM 多媒体计算、通信与应用汇刊（TOMM），18（1s），1–15。
- en: (125) Zhang, Q., Dang, K., Lai, J.-H., Feng, Z., & Xie, X. (2022c). Modeling
    3d layout for group re-identification. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (pp. 7512–7520).
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (125) Zhang, Q., Dang, K., Lai, J.-H., Feng, Z., & Xie, X.（2022c）。建模3D布局用于群体重识别。在
    IEEE/CVF 计算机视觉与模式识别会议论文集中（第7512–7520页）。
- en: Zhang et al., (2020) Zhang, X., Yan, Y., Xue, J.-H., Hua, Y., & Wang, H. (2020).
    Semantic-aware occlusion-robust network for occluded person re-identification.
    IEEE Transactions on Circuits and Systems for Video Technology, 31(7), 2764–2778.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2020）张晓、颜燕、薛君豪、华越、王浩（2020）。语义感知遮挡鲁棒网络用于遮挡行人重识别。IEEE 电路与系统视频技术汇刊，31（7），2764–2778。
- en: Zhang & Lu, (2018) Zhang, Y. & Lu, H. (2018). Deep cross-modal projection learning
    for image-text matching. In Proceedings of the European conference on computer
    vision (ECCV) (pp. 686–701).
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang & Lu（2018）张云、陆辉（2018）。深度跨模态投影学习用于图像-文本匹配。在欧洲计算机视觉会议（ECCV）论文集中（第686–701页）。
- en: Zhang et al., (2019) Zhang, Z., Lan, C., Zeng, W., & Chen, Z. (2019). Densely
    semantically aligned person re-identification. In Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition (pp. 667–676).
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2019）张智、兰晨、曾文、陈泽（2019）。密集语义对齐的行人重识别。在 IEEE/CVF 计算机视觉与模式识别会议论文集中（第667–676页）。
- en: Zhao et al., (2021) Zhao, C., Lv, X., Dou, S., Zhang, S., Wu, J., & Wang, L.
    (2021). Incremental generative occlusion adversarial suppression network for person
    reid. IEEE Transactions on Image Processing, 30, 4212–4224.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2021）赵程、吕欣、窦松、张晟、吴军、王磊（2021）。用于行人重识别的增量生成遮挡对抗抑制网络。IEEE 图像处理汇刊，30，4212–4224。
- en: Zhao et al., (2013) Zhao, R., Ouyang, W., & Wang, X. (2013). Unsupervised salience
    learning for person re-identification. In Proceedings of the IEEE conference on
    computer vision and pattern recognition (pp. 3586–3593).
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等（2013）赵瑞，欧阳威，王鑫。（2013）。**无监督显著性学习用于行人重识别**。在IEEE计算机视觉与模式识别会议论文集中（第3586–3593页）。
- en: 'Zhao et al., (2020) Zhao, S., Gao, C., Zhang, J., Cheng, H., Han, C., Jiang,
    X., Guo, X., Zheng, W.-S., Sang, N., & Sun, X. (2020). Do not disturb me: Person
    re-identification under the interference of other pedestrians. In Computer Vision–ECCV
    2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part VI 16 (pp. 647–663).: Springer.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等（2020）赵晟，高聪，张佳，程辉，韩超，姜轩，郭鑫，郑文生，桑宁，孙轩。（2020）。**请勿打扰我：在其他行人的干扰下的行人重识别**。在计算机视觉–ECCV
    2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，会议录，第VI部分16（第647–663页）。：Springer。
- en: Zhao et al., (2022) Zhao, Y., Zhu, S., Wang, D., & Liang, Z. (2022). Short range
    correlation transformer for occluded person re-identification. Neural computing
    and applications, 34(20), 17633–17645.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等（2022）赵毅，朱晟，王东，梁哲。（2022）。**用于遮挡行人重识别的短程相关变换器**。《神经计算与应用》，34（20），17633–17645。
- en: Zheng et al., (2021) Zheng, K., Lan, C., Zeng, W., Liu, J., Zhang, Z., & Zha,
    Z.-J. (2021). Pose-guided feature learning with knowledge distillation for occluded
    person re-identification. In Proceedings of the 29th ACM International Conference
    on Multimedia (pp. 4537–4545).
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等（2021）郑凯，兰超，曾伟，刘俊，张志，查志杰。（2021）。**基于姿态引导特征学习与知识蒸馏的遮挡行人重识别**。在第29届ACM国际多媒体会议论文集中（第4537–4545页）。
- en: '(134) Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., & Tian, Q. (2015a).
    Scalable person re-identification: A benchmark. In Proceedings of the IEEE international
    conference on computer vision (pp. 1116–1124).'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （134）郑磊，沈亮，田磊，王森，王剑，田权。（2015a）。**可扩展的行人重识别：基准**。在IEEE国际计算机视觉会议论文集中（第1116–1124页）。
- en: 'Zheng et al., (2016) Zheng, L., Yang, Y., & Hauptmann, A. G. (2016). Person
    re-identification: Past, present and future. arXiv preprint arXiv:1610.02984.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等（2016）郑磊，杨扬，霍普特曼（A.G.）。（2016）。**行人重识别：过去、现在和未来**。arXiv预印本arXiv:1610.02984。
- en: Zheng et al., (2019) Zheng, M., Karanam, S., Wu, Z., & Radke, R. J. (2019).
    Re-identification with consistent attentive siamese networks. In Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition (pp. 5735–5744).
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等（2019）郑明，卡拉南，吴哲，拉德克（R.J.）。（2019）。**使用一致的注意力Siamese网络进行重识别**。在IEEE/CVF计算机视觉与模式识别会议论文集中（第5735–5744页）。
- en: 'Zheng et al., (2011) Zheng, W.-S., Gong, S., & Xiang, T. (2011). Person re-identification
    by probabilistic relative distance comparison. In CVPR 2011 (pp. 649–656).: IEEE.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等（2011）郑文生，龚世杰，向涛。（2011）。**通过概率相对距离比较进行行人重识别**。在CVPR 2011（第649–656页）。：IEEE。
- en: (138) Zheng, W.-S., Li, X., Xiang, T., Liao, S., Lai, J., & Gong, S. (2015b).
    Partial person re-identification. In Proceedings of the IEEE international conference
    on computer vision (pp. 4678–4686).
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （138）郑文生，李晓，向涛，廖松，赖劲，龚世杰。（2015b）。**部分行人重识别**。在IEEE国际计算机视觉会议论文集中（第4678–4686页）。
- en: Zheng et al., (2022) Zheng, Z., Wang, X., Zheng, N., & Yang, Y. (2022). Parameter-efficient
    person re-identification in the 3d space. IEEE Transactions on Neural Networks
    and Learning Systems.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等（2022）郑志，王鑫，郑宁，杨扬。（2022）。**在三维空间中高效参数化的行人重识别**。IEEE《神经网络与学习系统》杂志。
- en: Zheng et al., (2017) Zheng, Z., Zheng, L., & Yang, Y. (2017). Unlabeled samples
    generated by gan improve the person re-identification baseline in vitro. In Proceedings
    of the IEEE international conference on computer vision (pp. 3754–3762).
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等（2017）郑志，郑磊，杨扬。（2017）。**GAN生成的未标记样本改善了体内的行人重识别基线**。在IEEE国际计算机视觉会议论文集中（第3754–3762页）。
- en: (141) Zhong, Y., Wang, X., & Zhang, S. (2020a). Robust partial matching for
    person search in the wild. In Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition (pp. 6827–6835).
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （141）钟勇，王鑫，张晟。（2020a）。**野外行人搜索的鲁棒部分匹配**。在IEEE/CVF计算机视觉与模式识别会议论文集中（第6827–6835页）。
- en: (142) Zhong, Z., Zheng, L., Kang, G., Li, S., & Yang, Y. (2020b). Random erasing
    data augmentation. In Proceedings of the AAAI conference on artificial intelligence,
    volume 34 (pp. 13001–13008).
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （142）钟智，郑磊，康光，李硕，杨扬。（2020b）。**随机擦除数据增强**。在AAAI人工智能会议论文集中，第34卷（第13001–13008页）。
- en: Zhou et al., (2019) Zhou, K., Yang, Y., Cavallaro, A., & Xiang, T. (2019). Omni-scale
    feature learning for person re-identification. In Proceedings of the IEEE/CVF
    international conference on computer vision (pp. 3702–3712).
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等人（2019）周凯，杨宇，卡瓦拉罗，向涛（2019）。跨尺度特征学习用于人员重新识别。在计算机视觉国际会议（pp. 3702–3712）的IEEE/CVF国际会议文集中。
- en: Zhou et al., (2022) Zhou, M., Liu, H., Lv, Z., Hong, W., & Chen, X. (2022).
    Motion-aware transformer for occluded person re-identification. arXiv preprint
    arXiv:2202.04243.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等人（2022）周敏，刘航，吕哲，洪文，陈晓（2022）。遮挡人员重新识别的动作感知变压器。arXiv预印本arXiv:2202.04243。
- en: (145) Zhou, Q., Zhong, B., Lan, X., Sun, G., Zhang, Y., Zhang, B., & Ji, R.
    (2020a). Fine-grained spatial alignment model for person re-identification with
    focal triplet loss. IEEE Transactions on Image Processing, 29, 7578–7589.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （145）周强，钟波，兰晓，孙刚，张宇，张波，季睿（2020a）。细粒度空间对齐模型用于具有聚焦三重损失的人员重新识别。图像处理通信，29，7578–7589。
- en: (146) Zhou, S., Wu, J., Zhang, F., & Sehdev, P. (2020b). Depth occlusion perception
    feature analysis for person re-identification. Pattern Recognition Letters, 138,
    617–623.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （146）周苏，吴军，张峰，塞德夫，皮亚仁（2020b）。人员重新识别的深度遮挡感知特征分析。图像处理通信，138，617–623。
- en: 'Zhu et al., (2020) Zhu, K., Guo, H., Liu, Z., Tang, M., & Wang, J. (2020).
    Identity-guided human semantic parsing for person re-identification. In Computer
    Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part III 16 (pp. 346–363).: Springer.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱等人（2020）朱凯，郭宏，刘芝，唐敏，王娟（2020）。人员重新识别的身份引导人类语义解析。在计算机视觉–ECCV 2020年：第16届欧洲会议，英国格拉斯哥，2020年8月23日至28日，第16部分（pp.
    346–363）。：斯普林格。
- en: Zhu et al., (2022) Zhu, X., Zheng, M., Chen, X., Zhang, X., Yuan, C., & Zhang,
    F. (2022). Information disentanglement based cross-modal representation learning
    for visible-infrared person re-identification. Multimedia Tools and Applications,
    (pp. 1–27).
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱等人（2022）朱晓，郑明，陈晓，张鑫，袁超，张飞（2022）。基于信息解缠的可见-红外人员重新识别跨模态表示学习。多媒体工具和应用，（pp. 1–27）。
- en: 'Zhuo et al., (2018) Zhuo, J., Chen, Z., Lai, J., & Wang, G. (2018). Occluded
    person re-identification. In 2018 IEEE International Conference on Multimedia
    and Expo (ICME) (pp. 1–6).: IEEE.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卓等人（2018）卓健，陈泽，赖杰，王刚（2018）。被遮挡的人员重新识别。在2018年IEEE国际多媒体与展览会（ICME）（pp. 1–6）中：IEEE。
- en: Zhuo et al., (2019) Zhuo, J., Lai, J., & Chen, P. (2019). A novel teacher-student
    learning framework for occluded person re-identification. arXiv preprint arXiv:1907.03253.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卓等人（2019）卓健，赖杰，陈鹏（2019）。遮挡人员重新识别的新型师生学习框架。arXiv预印本arXiv:1907.03253。
