- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:36:07'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2311.00603] Occluded Person Re-Identification with Deep Learning: A Survey
    and Perspectives'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.00603](https://ar5iv.labs.arxiv.org/html/2311.00603)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \fnref
  prefs: []
  type: TYPE_NORMAL
- en: fn1
  prefs: []
  type: TYPE_NORMAL
- en: \fnref
  prefs: []
  type: TYPE_NORMAL
- en: fn1
  prefs: []
  type: TYPE_NORMAL
- en: \fntext
  prefs: []
  type: TYPE_NORMAL
- en: '[fn1]Equal contribution.'
  prefs: []
  type: TYPE_NORMAL
- en: \cormark
  prefs: []
  type: TYPE_NORMAL
- en: '[1] \cormark[1] 1]organization=Institute of Semiconductors, Chinese Academy
    of Sciences, city=Beijing, postcode=100083, country=China 2]organization=Center
    of Materials Science and Optoelectronics Engineering $\&amp;$ School of Microelectronics,
    University of Chinese Academy of Sciences, city=Beijing, postcode=100083, country=China
    3]organization=School of Software, Xinjiang University, city=Xinjiang, postcode=830000,
    country=China 4]organization= School of Information Technology, Halmstad University,
    city=Halmstad, postcode=30118, country=Sweden'
  prefs: []
  type: TYPE_NORMAL
- en: \cortext
  prefs: []
  type: TYPE_NORMAL
- en: '[1]Corresponding author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enhao Ning ningenhao@163.com    Changshuo Wang wangchangshuo@semi.ac.cn    Huang
    Zhang zhhh1998@outlook.com    Xin Ning ningxin@semi.ac.cn    Prayag Tiwari prayag.tiwari@ieee.org
    [ [ [ [
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Person re-identification (Re-ID) technology plays an increasingly crucial role
    in intelligent surveillance systems. Widespread occlusion significantly impacts
    the performance of person Re-ID. Occluded person Re-ID refers to a pedestrian
    matching method that deals with challenges such as pedestrian information loss,
    noise interference, and perspective misalignment. It has garnered extensive attention
    from researchers. Over the past few years, several occlusion-solving person Re-ID
    methods have been proposed, tackling various sub-problems arising from occlusion.
    However, there is a lack of comprehensive studies that compare, summarize, and
    evaluate the potential of occluded person Re-ID methods in detail. In this review,
    we start by providing a detailed overview of the datasets and evaluation scheme
    used for occluded person Re-ID. Next, we scientifically classify and analyze existing
    deep learning-based occluded person Re-ID methods from various perspectives, summarizing
    them concisely. Furthermore, we conduct a systematic comparison among these methods,
    identify the state-of-the-art approaches, and present an outlook on the future
    development of occluded person Re-ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Occluded Person Re-identification \sepLiterature Survey and Perspectives \sepMultimodal
    Person Re-identification \sep3D Person Re-identification.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the increasing integration and intelligence of surveillance equipment (Bedagkar-Gala
    & Shah,, [2014](#bib.bib1)) in recent years, person re-identification (Re-ID)
    technology has significantly advanced. This technology finds extensive application
    in sensitive and specialized domains, such as medicine, rescue operations, criminal
    investigations, and surveillance. These fields often operate in complex and dynamic
    environments. Consequently, the rapid and accurate localization and identification
    of specific targets in multi-camera occlusion scenarios hold immense practical
    significance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the complexity and variability of real-life scenes, where people and
    objects move randomly, and surveillance devices typically cover wide areas, the
    likelihood of occluded individuals is high. Occlusion can have a severe impact
    on visual information, rendering the affected features unreliable. Occlusion can
    occur due to object interference, changes in pedestrian pose, clothing, and perspective.
    In early pedestrian representations, researchers primarily relied on basic, local
    visual attributes extracted from images, such as color, texture, edges, and corner
    points. These features capture geometric shapes and pixel distributions in images
    but are highly sensitive to external factors, lacking robustness and generalization.
    The development of deep learning has introduced high-level visual features. Compared
    with low-level visual features, high-level features are more adaptive to occlusions,
    noises and pose changes, and have stronger robustness in complex environments.
    Consequently, numerous researchers have developed a multitude of methods to address
    the prevalent occlusion problem. In general, the occlusion problem is divided
    into three sub-problems: (1) Noise problem. The problem of interference by multiple
    and mixed information from the features in the acquisition of complex scenes.
    (2) Missing problem. The problem of incomplete pedestrian features is due to only
    a part of the pedestrian being captured. (3) Alignment problem. Owing to the change
    in posture, perspective, and position, the features cannot correspond one-to-one,
    which causes distraction, shared location misalignment, and other issues. The
    study of occlusion also involves the separation of humans and backgrounds to extract
    human features as the core. Methods to extract fine-grained, highly discriminative,
    and more essential features with reference and value have also been studied.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to identify the current state-of-the-art and limitations of existing
    methods and discover unexplored areas. Specifically, we present methods for dealing
    with occluded person Re-ID that were submitted in top international journals or
    conferences before 2023\. We classify deep learning-based occluded person Re-ID
    according to the network structure of extracted features (CNN-based, transformer-based,
    and hybrid structure-based), the way features are extracted (uni-modal and multi-modal),
    and the hierarchical structure of features (2d and 3d). ( see Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Occluded Person Re-Identification with Deep Learning:
    A Survey and Perspectives")). First, due to the powerful performance of convolutional
    neural networks (CNNs) in image matching tasks, CNN-based methods have become
    one of the mainstream methods to deal with occlusion problems in person Re-ID.
    Therefore, we treat the cnn-based methods as the first class of methods to deal
    with the occlusion problem. Secondly, based on the success of transformer in the
    field of natural language, in recent years, vit has also been widely used to deal
    with the occlusion problem in pedestrian re-identification with good results.
    Therefore, we treat transformer-based methods as the second category. The third
    class of methods are some composite methods. For example, the complementary nature
    of CNN and vit is exploited to form a hybrid structure. The fourth and fifth class
    of methods are based on 3D and multimodal to deal with the occlusion problem in
    person Re-ID. They deal with more scenarios and are a relatively novel approach.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/376919f96fa6d88bd2e42f194f5f8ead.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overall structure of the survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: The summary of person-reported surveys in recent years.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Survey | Venue |'
  prefs: []
  type: TYPE_TB
- en: '| A survey of approaches and trends in person Re-ID (Bedagkar-Gala & Shah,,
    [2014](#bib.bib1)) | IVC2014 |'
  prefs: []
  type: TYPE_TB
- en: '| Person Re-ID Past, Present and Future (Zheng et al.,, [2016](#bib.bib135))
    | arXiv2016 |'
  prefs: []
  type: TYPE_TB
- en: '| A systematic evaluation and benchmark for person Re-ID: Features, metrics,
    and datasets (Gou et al.,, [2018](#bib.bib19)) | TPAMI2019 |'
  prefs: []
  type: TYPE_TB
- en: '| Beyond intra-modality discrepancy: A comprehensive survey of heterogeneous
    person Re-ID ([Wang et al., 2019b,](#bib.bib102) ) | arXiv2019 |'
  prefs: []
  type: TYPE_TB
- en: '| A Survey of Open-World Person ReIdentification (Leng et al.,, [2019](#bib.bib44))
    | TCSVT2020 |'
  prefs: []
  type: TYPE_TB
- en: '| Survey on Reliable Deep Learning-Based person Re-ID Models: Are We There
    Yet? (Lavi et al.,, [2020](#bib.bib43)) | arXiv2020 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Learning for Person Reidentification: A Survey and Outlook ([Ye et al.,
    2021b,](#bib.bib120) ) | TPAMI2021 |'
  prefs: []
  type: TYPE_TB
- en: '| SSS-PR: A short survey of surveys in person Re-ID (Yaghoubi et al.,, [2021](#bib.bib113))
    | PRL2021 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning-based person Re-ID methods: A survey and outlook of recent
    works (Ming et al.,, [2022](#bib.bib64)) | IVC2022 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Learning-based Occluded person Re-ID: A Survey (Peng et al.,, [2022](#bib.bib71))
    | arXiv2022 |'
  prefs: []
  type: TYPE_TB
- en: 'In general, the contributions of this study are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 1) This study focuses on addressing the occlusion problem in person Re-ID models,
    which is crucial for achieving high accuracy and robustness. We present a scientific
    and comprehensive review of past and current state-of-the-art approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 2) The current review of person Re-ID methods lacks sufficient coverage of approaches
    based on ViT. Given the excellent performance of ViT in occluded person Re-ID,
    we include a discussion of this method and its hybrid variants in our study, offering
    researchers new ideas and options for addressing the occlusion problem.
  prefs: []
  type: TYPE_NORMAL
- en: 3) We creatively incorporate 3D person Re-ID and multimodal person Re-ID, which
    have become popular in recent years. These novel methods can better solve the
    occlusion problem by utilizing additional depth or modal information, thus improving
    the performance and reliability of person Re-ID.
  prefs: []
  type: TYPE_NORMAL
- en: 4) We anticipate advancements in occluded person Re-ID and firmly believe that
    continued research and innovation will lead to the development of more effective
    methods and technologies for addressing the occlusion problem. These advancements
    will inspire and drive progress in the field of person Re-ID.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Literature review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the field of person Re-ID, there is a relative scarcity of specialized reviews
    compared to methodological articles. And they all focus on specific problems.
    These surveys are listed in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Occluded
    Person Re-Identification with Deep Learning: A Survey and Perspectives"). Bedagkar-Gala
    & Shah, ([2014](#bib.bib1)) focuses on the challenges of person Re-ID and divides
    it into open-set Re-ID and closed-set Re-ID based on the fixity of the gallery.
    Zheng et al., ([2016](#bib.bib135)) divides the methods of person Re-ID into methods
    for images and methods for videos based on the matching strategy. Gou et al.,
    ([2018](#bib.bib19)) provides a more detailed study of the features, metrics,
    and datasets of person Re-ID. [Wang et al., 2019b](#bib.bib102) focuses on heterogeneous
    person Re-ID. According to the application scenario, it classify the methods into
    four categories — low-resolution, infrared,sketch, and text. Leng et al., ([2019](#bib.bib44))
    focuses on open-world Re-ID tasks. Lavi et al., ([2020](#bib.bib43)) classifies
    Re-ID into single feature learning based approaches and multi-feature learning
    based approaches based on feature learning strategies. [Ye et al., 2021b](#bib.bib120)
    provides a more detailed explanation of Re-ID for open and closed settings, and
    introduces methods such as transmembrane states, unsupervised. Yaghoubi et al.,
    ([2021](#bib.bib113)) provides a more multidimensional classification of the person
    Re-ID problem. Ming et al., ([2022](#bib.bib64)) classifies the methods of person
    Re-ID into four categories based on metric learning and representation learning,
    and adds the latest methods. Peng et al., ([2022](#bib.bib71)) focuses on image-based
    obscured person Re-ID methods. However, these investigations are inevitably affected
    by a number of inherent limitations. Considering the widespread existence of the
    occlusion problem in pedestrian recognition, research on occluded person Re-ID
    is essential. Therefore, we provide an in-depth summary and comprehensive analysis
    of methods and prospects in occluded person Re-ID to advance future developments.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Datasets and Evaluation Protocols
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Occluded person Re-ID datasets can be divided into two categories: partial
    person and occluded person Re-ID datasets. The pedestrian images of the occluded
    person Re-ID datasets have occlusion information interference and are not cropped.
    The pedestrian image portion of the partial person Re-ID dataset is present and
    artificially cropped. Examples of partial/occluded person Re-ID datasets are shown
    in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Datasets ‣ 3 Datasets and Evaluation Protocols
    ‣ Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives").'
  prefs: []
  type: TYPE_NORMAL
- en: Occluded-DukeMTMC (Miao et al.,, [2019](#bib.bib62)) was collected from DukeMTMC-reID
    (Zheng et al.,, [2017](#bib.bib140)), containing 15,618 training images of 708
    pedestrians, 2,210 query images of 519 pedestrians, and 17,661 gallery images
    of 1,110 pedestrians for testing. Of these images, 9$\%$ of the training set,
    100$\%$ of the query set, and 10$\%$ of the gallery are occluded images. Obstacles
    include cars, bicycles, trees, and other pedestrians, adding complexity to the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: P-ETHZ ([Zheng et al., 2015b,](#bib.bib138) ) is an image-based occluded person
    Re-ID dataset, modified by ETHZ (Ess et al.,, [2008](#bib.bib13)). It has 3,897
    images containing 85 pedestrian identities with 1 to 30 full-body and occluded
    pedestrian images per identity.
  prefs: []
  type: TYPE_NORMAL
- en: P-DukeMTMC-reID (Zhuo et al.,, [2018](#bib.bib149)) was modified from DukeMTMC-reID
    (Zheng et al.,, [2017](#bib.bib140)), containing a total of 24,143 images of 1,299
    pedestrians, and each identity has a full-body and occlusion image; the pedestrian
    in the image is occluded by different objects, such as other pedestrians, cars,
    and signage.
  prefs: []
  type: TYPE_NORMAL
- en: Occluded-REID ([Zheng et al., 2015b,](#bib.bib138) ) has 2,000 images of 200
    pedestrians, each pedestrian corresponding to 5 occlusion and 5 whole body images,
    collected from Sun Yat-sen University. The dataset includes different viewpoints
    and types of severe occlusion, which challenges person Re-ID.
  prefs: []
  type: TYPE_NORMAL
- en: Occluded-DukeMTMC-VideoReID (Hou et al.,, [2021](#bib.bib30)) was reorganized
    from the DukeMTMC-VideoReID (Wu et al.,, [2018](#bib.bib108)) dataset. The training
    set contains 1,702 trajectory segments covering 702 pedestrians, the test set
    queries cover 661 pedestrians, and the gallery covers 1,110\. More than 70$\%$
    of the videos are occluded, including different perspectives and a variety of
    obstacles, such as cars, trees, bicycles, and other pedestrians.
  prefs: []
  type: TYPE_NORMAL
- en: Partial-ReID ([Zheng et al., 2015b,](#bib.bib138) ) has 600 images of 60 pedestrians,
    5 partial and 5 full-body images for each pedestrian. Using the visible parts,
    they are manually cropped to form new partial images. The images are collected
    from different perspectives, backgrounds and occlusions in a university campus.
  prefs: []
  type: TYPE_NORMAL
- en: Partial-iLIDS (He et al.,, [2018](#bib.bib24)) was derived from iLIDS (Zheng
    et al.,, [2011](#bib.bib137)) and contains 238 images of 119 pedestrians. Each
    pedestrian corresponds to one manually cropped non-occluded partial image and
    one full-body image. The partial image is used as a query, and the full-body image
    is used as a search library. It was shot by multiple non-overlapping cameras,
    mostly for test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Partial-CAVIAR (He et al.,, [2018](#bib.bib24)) was derived from CAVIAR (Cheng
    et al.,, [2011](#bib.bib9)) and contains 142 images of 72 pedestrians. The partial
    map is generated by randomly picking half of the overall image of each pedestrian.
  prefs: []
  type: TYPE_NORMAL
- en: P-CUHK03 (Kim & Yoo,, [2017](#bib.bib40)) was constructed based on CUHK03 (Li
    et al.,, [2014](#bib.bib45)), with a total of 1,360 pedestrian images, wherein
    15,080 images corresponding to 1,160 pedestrians are used as a training set, and
    the remaining 100 pedestrians are used as a validation and test set. Two of the
    images are selected to generate 10 local body query images with a spatial area
    ratio, and the remaining three images are used as whole body gallery images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/77d33192e282a611ccbf5fcb489b0ec3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Examples of four commonly used occluded person Re-ID datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Evaluation Protocols
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the field of occluded person Re-ID, the commonly used evaluation metrics
    are Cumulative Matching Characteristic (CMC) curves and mean Average Precision
    (mAP).
  prefs: []
  type: TYPE_NORMAL
- en: CMC curves are based on the principle of ranking the similarity between the
    query image and the image library, and the higher the top image, the higher the
    similarity with the query image. Then, the top-k accuracy ${\rm\emph{ACC}}^{\emph{k}}$
    of the query image is calculated based on this ranking. If the first k samples
    contain the query target, then ${\rm\emph{ACC}}^{\emph{k}}$ is 1, $\emph{k}\in\{1,2,3...\}$.
    Otherwise, ${\rm\emph{ACC}}^{\emph{k}}$ is 0 . Finally, the ${\rm\emph{ACC}}^{\emph{k}}$curves
    for all targets are summed and divided by the total number of targets to obtain
    CMC-k.
  prefs: []
  type: TYPE_NORMAL
- en: mAP better reflects the degree to which all correct target pictures are at the
    top of the sorted list. Compared with the CMC curve, it can more comprehensively
    measure the performance of Re-ID algorithms, where P is the precision rate, which
    refers to the proportion of correct samples among all samples. It reflects the
    accuracy of the correct samples in the output. The AP is the average of all correct
    samples predicted by the model. It reflects how well the model works on a single
    category and is the average of the accuracy of each correct prediction. Since
    there is more than one class in the recognition, the average AP value needs to
    be calculated for all classes, so the average accuracy of each class is added
    and divided by the total number of classes to obtain mAP.
  prefs: []
  type: TYPE_NORMAL
- en: The CMC curve cannot consider the hits of the samples with lower rankings, while
    mAP takes all samples into account. Therefore, they are important and complementary.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Deep Learning Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Based on CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolutional neural networks (CNNs) have emerged as one of the leading methods
    for learning pedestrian representations from RGB images. By using local perceptual
    fields and learning filters, CNNs can extract powerful features that capture regional
    information about local features of pedestrians. These features are then compressed
    and mapped to higher-level representations. Researchers have refined them to be
    usable for pedestrian matching tasks in complex realistic scenarios. We classify
    it into local feature learning, relational representation, mixing methods, and
    other methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Local Feature Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Local feature-learning performs better at handling regional features, and it
    has unique advantages for occlusion region recognition and location compared with
    global features. According to its implementation of different local feature methods,
    we divide them into human segmentation, pose estimation, human parsing, attribute
    annotation, and hybrid methods ( see Figure [3](#S4.F3 "Figure 3 ‣ 4.1.1 Local
    Feature Learning ‣ 4.1 Based on CNN ‣ 4 Deep Learning Methods ‣ Occluded Person
    Re-Identification with Deep Learning: A Survey and Perspectives") ).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Body Segmentation. By leveraging the characteristic of pedestrians walking
    upright, our method extracts improved local features through the segmentation
    of the original image or feature map. The segmentation results can take the form
    of stripes, fixed regions, or small patches ( see Figure [4](#S4.F4 "Figure 4
    ‣ 4.1.1 Local Feature Learning ‣ 4.1 Based on CNN ‣ 4 Deep Learning Methods ‣
    Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives")
    ).'
  prefs: []
  type: TYPE_NORMAL
- en: However, segmentation does not have the process of identifying occlusions, so
    it is sensitive to noise. CBDB-Net (Tan et al.,, [2021](#bib.bib85)) evenly divides
    the strips on the feature map and discards each strip one by one to output multiple
    incomplete feature maps, which forces the model to learn a more robust pedestrian
    representation in an environment with incomplete information. The DPPR (Kim &
    Yoo,, [2017](#bib.bib40)) predefines thirteen bounding boxes for the whole-body
    image, including the whole image, half-body image, and horizontal part image,
    and extracts features from each part. At the same time, an attention-based matching
    mechanism is introduced to make the feature weight of the same body part larger,
    which can alleviate the information loss caused by occlusion. At the same time,
    an attention-based matching mechanism is introduced to make the feature weight
    of the same body part larger, which can alleviate the information loss caused
    by occlusion. OCNet (Kim et al.,, [2022](#bib.bib41)) introduces a relationship-based
    approach to deal with occlusion problems. OCNet (Kim et al.,, [2022](#bib.bib41))
    divides the feature map horizontally into top and bottom features and takes 1/4
    of the middle width as the central feature. Then, it is put into the relational
    adaptive module consisting of two shared layers together with the global feature
    map. The alignment problem between regional features is handled by relations,
    and weights are introduced to suppress noise interference.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5d21802d43ea6a9c254dec94b902b811.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Four different local feature learning methods: (a) indicates pose
    estimation. (b) indicates semantic segmentation. (c) indicates attribute annotations.
    (d) indicates the mixing method.'
  prefs: []
  type: TYPE_NORMAL
- en: Pose Estimation. Pose estimation extracts semantic information at the image
    pose level by exploiting the highly structured human skeleton. The interference
    of noise is suppressed in a guided or fused manner.
  prefs: []
  type: TYPE_NORMAL
- en: HOReID ([Wang et al., 2020a,](#bib.bib93) ) introduced a learnable relational
    matrix. The human body key points obtained from the pose estimation are regarded
    as nodes in the graph, and finally, a topology graph is formed to suppress noise
    interference. PMFB (Miao et al.,, [2021](#bib.bib63)) uses pose estimation to
    obtain confidence and coordinates of human keypoints. Then, a threshold is set
    to filter the occluded regions. Finally, the visible part is used to constrain
    the feature response at channel level to solve the occlusion problem. PGMANet
    (Zhai et al.,, [2021](#bib.bib121)) generates an attention mask using a human
    heat map. The interference of noise is removed jointly by the dot product of feature
    maps and guidance of higher-order relations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Researchers generally use pose estimation in two directions: 1) to obtain semantic
    features through pose estimation, identify noise points, and better remove noise
    interference, and 2) to localize human regions through pose estimation and thus
    solve the problem of alignment and local feature extraction. AACN (Xu et al.,,
    [2018](#bib.bib111)) uses pose points to locate pedestrian body regions and introduces
    a posture-guided visibility score to separate occlusions. DAReID (Xu et al.,,
    [2021](#bib.bib112)) adopts a dual-branch structure, in which the mask branch
    extracts more discriminative local features based on the spatial attention module
    guided by pose estimation, and the global branch enhances the representation of
    human discriminative information through feature activation. DSA-reID (Zhang et al.,,
    [2019](#bib.bib128)) estimates dense semantic information at the 2D level by a
    pre-trained DensePose (Güler et al.,, [2018](#bib.bib20)) model and maps a set
    of dense 3D semantic alignment components. Local features are extracted by integrating
    neighboring components. Finally, the global and local features are fused into
    the final feature. PGFL-KD (Zheng et al.,, [2021](#bib.bib133)) takes the local
    features of the semantic layer as queries and looks for more prominent foreground
    regions in the feature map to obtain enhanced foreground features. Based on this
    feature, interactive training and knowledge distillation are performed to constrain
    the learning of the backbone network. ACSAP ([He et al., 2021c,](#bib.bib29) )
    uses pose keypoints to guide the adversarial generation network to remove noise
    interference by weakening the spatial relationship between the front and back
    blocks. PDC (Su et al.,, [2017](#bib.bib79)) obtains 6 body regions according
    to 14 key points of pose, rotates and scales each part, and uses an improved PTN
    network to learn the parameters of affine transformation. These local features
    are automatically placed at certain locations in the drawing to resolve alignment
    issues. PVPM ([Gao et al., 2020b,](#bib.bib18) ) trains a visibility predictor
    based on the correspondence between visibility parts. After that, the alignment
    problem is solved by generating part pseudo-labels through graph matching.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4d9bfcd3d971b061afb00266149f6a3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Three common Body Segmentation schematics: (a) indicates stripes.
    (b) indicates fixed areas. (c) indicates small blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Segmentation. By introducing a human parsing model, the interference
    of noise is identified and removed in the form of segmentation or semantic parsing.
  prefs: []
  type: TYPE_NORMAL
- en: SPReID (Kalayeh et al.,, [2018](#bib.bib37)) generates probability maps associated
    with five different body regions based on the trained pedestrian class semantic
    parsing model Inception-V3 (Szegedy et al.,, [2016](#bib.bib84)), namely, foreground,
    head, upper body, lower body, and shoes. Then, the probability map is fused with
    the semantic region features after bilinear interpolation to activate different
    parts and remove the interference of occlusion. CoAttention (Lin & Wang,, [2021](#bib.bib51))
    takes the parsing mask of the local image of the pedestrian’s body as a query
    and constructs a mapping while introducing a self-attentive mechanism to filter
    occlusions. MMGA (Cai et al.,, [2019](#bib.bib2)) first separates pedestrians
    from images into upper and lower body using JPPNet (Liang et al.,, [2018](#bib.bib50)).
    Then, two attention modules are designed; the first is used to filter the interference
    from the background, and the second generates the corresponding spatial and channel
    attentions to extract different features guided by the whole, upper, and lower
    body masks. Finally, element-level multiplication is performed as the final feature.
    HPNet (Huang et al.,, [2020](#bib.bib31)) uses the COCO (Lin et al.,, [2014](#bib.bib52))
    dataset to train the human body parsing model to obtain labels for the four main
    body parts, based on which the parsing model and overall network are trained in
    a multitasking manner while generating visibility scores to remove occlusions.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation-based approaches also contribute to enhancing the diversity
    of features. In the case of SORN (Zhang et al.,, [2020](#bib.bib126)), a three-branch
    model composed of a global branch, a local branch, and a semantic branch is designed.
    The global branch aims to obtain global features through normalization and feature
    aggregation. Meanwhile, the local branch leverages prior knowledge of the pedestrian
    body structure to generate pedestrian body parts and derive local features through
    mapping, pooling, and normalization. The semantic branch first uses the DANet
    (Fu et al.,, [2019](#bib.bib16)) model to pre-train the semantic labels of the
    data, trains a semantic segmentation model on the DensePose-COCO dataset (Güler
    et al.,, [2018](#bib.bib20)), introduces label smoothing to optimize the semantic
    labels, and forms a foreground pedestrian body region by aggregating the semantic
    segmentation part to realize the separation of background and pedestrians. SGSFA
    (Ren et al.,, [2020](#bib.bib74)) introduces a semantic alignment branch and spatial
    feature alignment branch. The former achieves semantic alignment through element-level
    multiplication. The latter is based on regional spatial alignment achieved by
    body structure.
  prefs: []
  type: TYPE_NORMAL
- en: Attribute annotation. The occlusion problem is handled by introducing attribute
    annotation.
  prefs: []
  type: TYPE_NORMAL
- en: ASAN (Jin et al.,, [2021](#bib.bib36)) extracts the visible part of human features
    by combining attribute information and weak supervision. Attribute information
    is a semantic level attribute annotation. Based on the visibility part determination,
    a region visibility matching algorithm is introduced to achieve the effect of
    denoising.
  prefs: []
  type: TYPE_NORMAL
- en: Mixing method. Introducing more than two kinds of external information can help
    the model remove the interference of noise in the form of feature interaction
    or co-guidance.
  prefs: []
  type: TYPE_NORMAL
- en: GASM (He & Liu,, [2020](#bib.bib25)) proposes an architecture for learning salient
    information, which separates pedestrians from background by using semantic information,
    removes occlusion interference by pose estimation, and then fuses the two features
    to guide model learning. SSPReID (Quispe & Pedrini,, [2019](#bib.bib73)) designs
    a joint learning method to combine salient and semantic features. Five different
    semantic features of the human body are obtained by human body parsing. The saliency
    features utilize the regions of highest attention in the graph. These features
    are fused with global features and finally concatenated together to form the final
    features. TSA ([Gao et al., 2020a,](#bib.bib17) ) introduces two kinds of area
    features for the pose change problem, one guided by pose keypoints and one guided
    by partial masks of the human parsing model, after which the two are fused. Using
    interaction can solve the pose change problem, while using pose and segmentation
    can also suppress noise and thus solve the occlusion problem. FGSA ([Zhou et al.,
    2020a,](#bib.bib145) ) proposes a pose resolution network for complex pose changes
    to deal with local locations and the relationships between them. Attribute interaction
    learning is designed for local feature information extraction corresponding to
    pose points, which is achieved by training an intermediate attribute classification
    model that treats attribute recognition as a multi-category labeling problem.
    Finally, a local enhanced alignment model is added in the feature fusion phase,
    that is, less weight is added to the background and more weight is added to the
    local and attribute locations. The backbone network of LKWS (Yang et al.,, [2021](#bib.bib116))
    is based on PCB (Sun et al.,, [2018](#bib.bib83)). In local feature extraction,
    the visibility label of points is generated by pose estimation and a reasonable
    threshold, and then the visibility of thick stripes is obtained by a voting mechanism.
    Based on the visibility of stripes, a visibility discriminator is trained to recognize
    noise interference. PGFA (Miao et al.,, [2019](#bib.bib62)) introduces a two-branch
    structure, where the local branch extracts local features based on horizontal
    segmentation. The global branch is guided by pose, and key points are first obtained
    from pose estimation. Then, a reasonable threshold is set to filter the noise
    points, a proper dot product is performed with the feature map to fuse the features,
    and the features of the partial branches are stitched together into the final
    global features. Similarly, PDVM ([Zhou et al., 2020b,](#bib.bib146) ) extracts
    the de-obscured global features based on the pose heat map. Segmentation guides
    local feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7b33ed07324abffb4c6b32f501947405.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: (a) Schematic diagram showing figure convolution. (b) Schematic representation
    of clustering. (c) Schematic representation ofrepresentation of the shared area.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Relationship Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'By focusing on feature relationships, occlusions are handled in a suppressed,
    removed, or supervised manner. We divide them into attention, clustering, graph
    convolution, and shared region ( see Figure [5](#S4.F5 "Figure 5 ‣ 4.1.1 Local
    Feature Learning ‣ 4.1 Based on CNN ‣ 4 Deep Learning Methods ‣ Occluded Person
    Re-Identification with Deep Learning: A Survey and Perspectives") ) according
    to their different ways and means of learning relationships'
  prefs: []
  type: TYPE_NORMAL
- en: Attention. By introducing the attention mechanism, the model can select the
    highly salient and discriminative regions to suppress the interference of noise.
  prefs: []
  type: TYPE_NORMAL
- en: To address the issue of missing images in person Re-ID, DPPR (Kim & Yoo,, [2017](#bib.bib40))
    employs an attention mechanism to emphasize the same pedestrian part across different
    images. This approach enhances the representation of individuals and improves
    matching accuracy. Moreover, OCNet (Kim et al.,, [2022](#bib.bib41)) mitigates
    the effect of noise by capturing higher-order relationships among regional features
    and incorporating them with weighted combinations. This method effectively suppresses
    the influence of noisy or irrelevant information, resulting in more robust and
    accurate person Re-ID outcomes. AACN (Xu et al.,, [2018](#bib.bib111)) combines
    pose guided attention maps and partial visibility scores to remove background
    distractions and occlusions before extracting clean pedestrian features. DAReID
    (Xu et al.,, [2021](#bib.bib112)) introduces dual attention recognition. The local
    area visible to the pedestrian is obtained by gesture-guided spatial attention.
    Global features are extracted by feature activation and pose. Both will then be
    used together to guide the representation of features. PISNet (Zhao et al.,, [2020](#bib.bib131))
    is concerned with the overlapping area between people and objects. A module is
    designed to act as a guide feature by querying features, which can attenuate the
    problem of attention distraction caused by multiple pedestrians in the gallery.
    Also, a reverse attention module based on strong activation attention is designed,
    which enables the model to assign more weight to the target region. APN (Huo et al.,,
    [2021](#bib.bib33)) proposes a partial perceptual attention network, which takes
    partial feature maps as query vectors, calculates a similarity mapping M with
    a mapping X of the feature maps, and morphs the features by weighting M by X to
    achieve the purpose of aggregating and extracting refined features. MHSA-Net ([Tan
    et al., 2022a,](#bib.bib86) ) multiplies attention weights with feature maps and
    applies a nonlinear transformation to encourage multi-headed attention mechanisms
    to adaptively capture key local features. CASN (Zheng et al.,, [2019](#bib.bib136))
    takes attention and attention consistency as the criteria for model learning and
    removes occlusion by introducing an attention twin network to focus on more discriminative
    core areas. VPM ([Sun et al., 2019b,](#bib.bib82) ) learns the visibility and
    location of components by introducing a self-supervised component localizer at
    the convolution output and introducing a feature extractor that generates region
    information through weighted pooling. PSE (Sarfraz et al.,, [2018](#bib.bib76))
    solves the occlusion problem caused by view angle using view angle prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Attention-based approaches not only enhance the flexibility of the model, but
    also inject more contextual information into the features. PAFM (Yang et al.,,
    [2022](#bib.bib115)) introduces an improved spatial attention module to discover
    relationships between pixel points while capturing and aggregating pixel points
    with high semantic relevance. Finally, it is multiplied with the feature map containing
    pose information to perform feature fusion. Co-Attention (Lin & Wang,, [2021](#bib.bib51))
    takes the analytic mask of a partial pedestrian image and whole image as the target
    and matches it through a self-attention mechanism (Li et al.,, [2020](#bib.bib47)).
    Finally, noise interference is suppressed by focusing pedestrian features. QPM
    ([Wang et al., 2022b,](#bib.bib96) ) divides the feature map into six parts evenly
    in the vertical direction, and introduces a component quality score to judge the
    visibility. Meanwhile, a two-layer identity-aware module based on an attention
    map is used to deal with pedestrian occlusion in the global branch. Finally, global
    features are adaptively extracted from clean pedestrian regions. DSOP ([Wang et al.,
    2020b,](#bib.bib98) ) divides occlusion into shallow and deep layers. The shallow
    layer learns the feature after occlusion by focusing on the local region, and
    the deep layer gives a large receptive field to learn the global feature, that
    is, the feature before occlusion. After that, the channel and spatial attention
    mechanisms are applied to the two branches for weighted fusion of features (Ning
    et al.,, [2021](#bib.bib67)).
  prefs: []
  type: TYPE_NORMAL
- en: Clustering.The interference of noise is solved by finding the inherent distribution
    structure of the data to categorize the pixel points.
  prefs: []
  type: TYPE_NORMAL
- en: ISP (Zhu et al.,, [2020](#bib.bib147)) assigns a pseudo-label to each pixel
    by tandem clustering. All pixels of the human body image are firstly divided into
    foreground and background, based on the assumption that the foreground is more
    responsive than the background. Secondly, the pixels are clustered into different
    parts and assigned pseudo-labels. Based on the pseudo-labels, different weights
    are assigned to the pixels to extract local features. This not only separates
    occlusions from pedestrians at the pixel level, but also enables automatic alignment.
  prefs: []
  type: TYPE_NORMAL
- en: Graph Convolution. By learning the high-order semantic relationship between
    pixels, the noise interference is suppressed by restricting the information transmission.
  prefs: []
  type: TYPE_NORMAL
- en: HOReID ([Wang et al., 2020a,](#bib.bib93) ) introduces a matrix describing the
    higher order relationships between points and later passes information in this
    relationship matrix to form a topological map. With the help of the constraints
    of the topological map, the transfer of useless information between points is
    suppressed, and the purpose of noise removal is achieved.
  prefs: []
  type: TYPE_NORMAL
- en: Shared Area. The interference of noise is reduced by sensing the same body parts
    of pedestrians in the image pairs to extract shareable features.
  prefs: []
  type: TYPE_NORMAL
- en: DPPR (Kim & Yoo,, [2017](#bib.bib40)) gives larger weights to regions containing
    the same body parts to improve the ability of the model to extract core features.
    VPM ([Sun et al., 2019b,](#bib.bib82) ) solves the alignment and denoising problem
    by perceiving the visibility of shared regions. PPCL ([He et al., 2021b,](#bib.bib28)
    ) learns component matching in a self-supervised manner and finally computes image
    similarity based on shared semantic corresponding regions only. KBFM (Han et al.,,
    [2020](#bib.bib22)) focuses on extracting highly visible and shareable pose points,
    which are used as the core area to extract features for matching, and achieve
    the effect of denoising and alignment.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Other Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Regional reconfiguration.This is complements obscured or noisy areas by using
    complete pedestrian areas.
  prefs: []
  type: TYPE_NORMAL
- en: To solve the problem of information loss caused by occlusion, RFCNet (Hou et al.,,
    [2021](#bib.bib30)) introduces an encoder–decoder that uses non-occlusion remote
    spatial context for feature completion. The encoder is modeled by similarity region
    assignment. The decoder reconstructs the occluded region by establishing the correlation
    between the occluded region and distant non-occluded region through clustering.
    ACSAP ([He et al., 2021c,](#bib.bib29) ) combines attitude and adversarial generation
    networks and designs an attitude-guided spatial generator and spatial discriminator
    to remove noise interference.
  prefs: []
  type: TYPE_NORMAL
- en: Data enhancement. The sensitivity of the model to occlusion is improved by incorporating
    transformation of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'APNet ([Zhong et al., 2020a,](#bib.bib141) ) proposes a method to modify the
    detected bounding box. APNet ([Zhong et al., 2020a,](#bib.bib141) ) designed a
    bounding box aligner that slides over the image in a matching manner. Then, a
    feature extractor with high discriminative power is designed to extract the core
    local features and discard the noisy local features. IGOAS (Zhao et al.,, [2021](#bib.bib129))
    adopts a progressive occlusion module, which randomly generates small uniform
    occlusion on a group of images and generates larger occlusion based on small occlusion
    after model learning. Such a growing occlusion region can improve the recognition
    ability of occlusion and achieve the purpose of removing occlusion noise. OAMN
    ([Chen et al., 2021c,](#bib.bib8) ) adopts a method based on cropping and scaling,
    predefines four corners, randomly selects a training image to be cropped and scaled
    to form patches on four positions, and realizes weighted learning in combination
    with attention to achieve denoising. RE ([Zhong et al., 2020b,](#bib.bib142) )
    introduces the technique of random pixel removal, which replaces the pixel values
    in the region with random values by randomly selecting rectangular regions, thereby
    improving the diversity of data and robustness of the model. SSGR (Yan et al.,,
    [2021](#bib.bib114)) introduces a compound batch erase method, which includes
    two erase operations: one is frequently used random erase, and the other is batch
    constant erase. It first divides the image horizontally into random S and randomly
    selects a strip in each sub-batch to erase. Then, referring to the self-attention
    mechanism and local feature learning, a matching-based disentanglement non-local
    operation is introduced to extract better features from the complete pedestrian
    region. ETNDNet (Dong et al.,, [2023](#bib.bib11)) addresses the occlusion problem
    from an adversarial defence perspective. It deals with incomplete information,
    positional misalignment and noisy information through strategies of randomly erasing
    feature maps, introducing random transformations and perturbing feature maps.'
  prefs: []
  type: TYPE_NORMAL
- en: Regularization. By imposing penalties and constraints on high-attention areas,
    the model is forced to focus on the full pedestrian area. Pedestrian features
    are extracted using complete information.
  prefs: []
  type: TYPE_NORMAL
- en: MHSA-Net ([Tan et al., 2022a,](#bib.bib86) ) introduces a feature regularization
    mechanism, which consists of a regularization term based on attention weight embedding
    and a hard triplet loss based on triplet feature units. The regularization term
    can cover local information in many ways and increase the completeness of information
    ([Ning et al., 2020a,](#bib.bib68) ). Hard triplet loss can refine the fusion
    and be better used for pedestrian matching.
  prefs: []
  type: TYPE_NORMAL
- en: Teacher-student model. Teacher models assist student models in dealing with
    occlusion problems.
  prefs: []
  type: TYPE_NORMAL
- en: HG (Kiran et al.,, [2021](#bib.bib42)) designs an end-to-end unsupervised teacher–student
    framework that lets the teacher network learn the between-class distance by inputting
    different combinations of images, and then the student inherits the network and
    learns the within-class distance by inputting more noisy images of the same class.
    At the same time, the attention embedding method with distance distribution matching
    can help the student network to remove noise interference better and extract more
    discriminative features. AFPB (Zhuo et al.,, [2019](#bib.bib150)) first puts regular
    data and pedestrian volume data simulating occlusion into the teacher network,
    and then it conducts joint training to make the teacher network learn a basic
    model that is robust to occlusion. The student network then inherits the teacher
    network. It learns on more realistic, noisier real-world occluded pedestrian data.
  prefs: []
  type: TYPE_NORMAL
- en: Multiscale. The occlusion problem is handled by multi-scale feature representation.
  prefs: []
  type: TYPE_NORMAL
- en: DSR (He et al.,, [2018](#bib.bib24)) focuses on solving problems caused by differences
    in scale. DSR (He et al.,, [2018](#bib.bib24)) first trains a fixed-size fully
    convolutional network with only convolution and pooling on Market-1501 ([Zheng
    et al., 2015a,](#bib.bib134) ) to represent the identity feature map, and it then
    introduces three different scales of blocks to extract features in a sliding manner
    (Li et al.,, [2021](#bib.bib48)). Similarly, FPR (He et al.,, [2019](#bib.bib26))
    also introduces a structure that only has convolution and pooling and proposes
    a pyramid layer composed of multiple different pooling kernels to solve the scale
    problem, an attention-based foreground probability generator to process the background,
    and a small weight to the background to achieve removal of background distractions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bc172c1801381b6b3f68e0b3b54d81a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: (a) Schematic of the transformer-based approach. (b) Schematic representation
    of data augmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Based on vit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TransReID ([He et al., 2021a,](#bib.bib27) ), proposed in 2021, is the first
    model to apply a vit (Dosovitskiy et al.,, [2020](#bib.bib12)) in Re-ID ( see
    Figure [6](#S4.F6 "Figure 6 ‣ 4.1.3 Other Methods ‣ 4.1 Based on CNN ‣ 4 Deep
    Learning Methods ‣ Occluded Person Re-Identification with Deep Learning: A Survey
    and Perspectives") ), which has two major features compared to ResNet (He et al.,,
    [2016](#bib.bib23)): (1) Multi-headed self-attention is better at capturing long-range
    relationships and driving the model to focus on different body parts (Khan et al.,,
    [2022](#bib.bib39); Shamshad et al.,, [2023](#bib.bib78)). (2) The transformer
    can retain more detailed information in the absence of downsampling computation.
    Based on these characteristics, many variants of transformer have appeared in
    recent years, and researchers have widely used them in occlusion Re-ID.'
  prefs: []
  type: TYPE_NORMAL
- en: PFD ([Wang et al., 2022d,](#bib.bib99) ) first divides the image into fixed-size
    blocks that can be overlapped and then uses the transformer encoder to capture
    contextual relationships. Next, pose guided feature aggregation and a feature
    matching mechanism are used to display the visible body parts. Finally, the pose
    heat map and decoder are used as keys and values to learn a set of semantic part
    views to enhance the discriminability of body parts. DPM ([Tan et al., 2022b,](#bib.bib87)
    ) introduces subspace selection to deal with the alignment problem. Specifically,
    DPM ([Tan et al., 2022b,](#bib.bib87) ) first aggregates the feature representations
    of the overall prototype map and the occlusion map using hierarchical semantic
    information to enhance the quality of the generated prototype mask. Then, a head
    enrichment module based on normalization and orthogonality constraints is introduced
    to enhance the discriminative representation of features and remove the interference
    of noise. PFT (Zhao et al.,, [2022](#bib.bib132)) introduces a learnable enhancement
    patch to compensate for the problem of local feature extraction (Islam,, [2022](#bib.bib34))
    by transformer. Through feature slicing, merging, and stitching, the patch sequence
    can learn the long-range correlation of regions while ensuring the receptive field
    and paying more attention to local features. FED ([Wang et al., 2022g,](#bib.bib104)
    ) divides occlusion into non-pedestrian and non-target pedestrian occlusion, and
    focuses on the latter. Specifically, an occlusion set is first created to enhance
    the data. Then, a multiplicative occlusion score is introduced to diffuse the
    visible pedestrian parts, which improves the quality of the synthesized non-target
    pedestrians. The joint optimization of feature erasure and feature diffusion modules
    realizes the perception ability of the model to the target pedestrian.
  prefs: []
  type: TYPE_NORMAL
- en: FRT (Xu et al.,, [2022](#bib.bib110)) first classifies the pedestrian into head,
    torso, and legs using a pretrained pose estimation model HRNet ([Sun et al., 2019a,](#bib.bib80)
    ), and extracts the corresponding features. Then the occlusion elimination module
    based on graph matching is introduced to eliminate the interference caused by
    occlusion by learning the similarity of common regions. Finally, FRT (Xu et al.,,
    [2022](#bib.bib110)) recovers query features by aggregating neighbor features
    to solve the problem of information loss caused by occlusion. TL-TransNet ([Wang
    et al., 2022c,](#bib.bib97) ) uses a well-improved Swin transformer (Liu et al.,,
    [2021](#bib.bib56)) as a benchmark model to capture the main part of the person
    and uses DeepLabV3+ (Chen et al.,, [2018](#bib.bib7)) to remove background interference
    in the query and gallery. Finally, a reordering method based on hybrid similarity
    and background adaptation is designed to achieve the fusion of original features
    and removed background features. PADE (Huang et al.,, [2022](#bib.bib32)) first
    obtains two new images through cropping and erasing operations and feeds them
    together with the original image into a multi-branch parameter sharing network
    with a vit (Dosovitskiy et al.,, [2020](#bib.bib12)) as the backbone to enhance
    the global and local features in a manner similar to the self-attention mechanism.
    Finally, they are connected to form the final feature representation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Composite Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By introducing more than two different networks, the occlusion problem is dealt
    with in an interactive or fused manner.
  prefs: []
  type: TYPE_NORMAL
- en: FGMFN ([Zhang et al., 2022a,](#bib.bib123) ) introduces a dual branch network.
    The local feature branches are first passed through an affine transformation based
    on ResNet18\. Then, input into ResNet50 (He et al.,, [2016](#bib.bib23)) to get
    the upper body features and divide it into three regions, while introducing the
    attention module based on the gate mechanism to remove noise interference. The
    global feature branch adopts a feature extraction scheme based on block partition,
    and finally, the features of the two branches are fused proportionally as the
    final feature. Pirt (Ma et al.,, [2021](#bib.bib59)) first pre-trains the HRNet
    ([Sun et al., 2019a,](#bib.bib80) ) pose estimation model on the COCO (Lin et al.,,
    [2014](#bib.bib52)) dataset, from which a two-branch structure is elicited. Intra-part
    features are guided by a carefully modified ResNet50 (He et al.,, [2016](#bib.bib23)),
    inter-part relationships are guided by a transformer, and the effects of occlusion
    are removed by establishing part-aware long-range dependencies. Pirt (Ma et al.,,
    [2021](#bib.bib59)) learns inter- and intra-part relationships in a collaborative
    manner. MAT (Zhou et al.,, [2022](#bib.bib144)) first uses a CNN to extract features
    from the image. Then, it flattens and passes them to a transformer. This method
    uses the methods of feature map and key point embedding to obtain a key point
    heat map and segmentation map. It uses affine transformation to obtain motion
    information of each key point and combines action information to realize fine-grained
    human body segmentation, and extracts representative pedestrian body parts to
    remove interference noise. DRL-Net (Jia et al.,, [2022](#bib.bib35)) first acquires
    obstacles from the training images to construct augmented samples with random
    obstacles. Then, a CNN and Transformer are concatenated to design a query-based
    semantic feature extraction layer. Finally, the semantic bootstrap is used to
    learn positive and negative sample comparisons and remove interference noise.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 3D Re-ID
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to 2D, the use of 3D information for occlusion person Re-ID is a relatively
    new approach. The shape and spatial depth features represented by 3D data are
    robust to texture information, and it usually reduces the interference caused
    by occlusion through 3D feature denoising, 3D feature complementation and multi-view
    construction.
  prefs: []
  type: TYPE_NORMAL
- en: PersonX (Sun & Zheng,, [2019](#bib.bib81)) constructs virtual 3D models by scanning
    people and objects in the real world and then maps them back to 2D for re-representation.
    Such manipulation of the data enhances the data representation. ([Wang et al.,
    2022f,](#bib.bib101) ) used UV texture mapping to clone clothing from real-world
    pedestrians to virtual 3D characters, while using a patch-based feature segmentation
    and expansion approach to deal with occlusion. A more common form of 3D information
    is the point cloud (Qi et al.,, [2017](#bib.bib72)), where the depth information
    in the point cloud can be used as an additional channel to the image. OG-Net (Zheng
    et al.,, [2022](#bib.bib139)) uses Skinned Multi-Person Linear ( SMPL (Kanazawa
    et al.,, [2018](#bib.bib38)) ) to generate six channels of point cloud data from
    2D images, providing positional and texture information.
  prefs: []
  type: TYPE_NORMAL
- en: ASSP ([Chen et al., 2021b,](#bib.bib6) ) uses 3D body reconstruction as an auxiliary
    task for 2D feature extraction. Specifically, texture-insensitive 3D shape information
    is first extracted from 2D images as auxiliary features. After that, a 3D row
    human model is created using SMPL (Loper et al.,, [2015](#bib.bib57)), while an
    adversarial learning and self-supervised projection combination is designed to
    combine 2D and 3D information into a 3D model for reconstruction ([Ning et al.,
    2020b,](#bib.bib69) ). Regularization based on 3D reconstruction forces the model
    to decouple 3D shape information from visual features and remove the interference
    of noise to extract more discriminative features. JGCL ([Chen et al., 2021a,](#bib.bib5)
    ) mitigates the lack of information caused by perspective in an unsupervised mapping.
    Specifically, a corresponding 3D mesh is first generated using a 3D network generator
    HMR (Kanazawa et al.,, [2018](#bib.bib38)). After that, the model is rotated and
    combined with a GAN to generate diverse views of the human body. These generated
    new views are combined with the original images in contrast learning while enhancing
    the view-invariant representation to improve the generated picture quality. 3DT
    ([Zhang et al., 2022c,](#bib.bib125) ) implements group person Re-ID using a 3D
    transformer. Because pedestrians in a group inevitably occlude each other, it
    transforms the 3D space into a discrete space by introducing a spatial layout
    token based on quantization and sampling, and it later assigns layout features
    to each member to reconstruct the spatial relationships between members.
  prefs: []
  type: TYPE_NORMAL
- en: However, the research on recognition using point clouds is still limited compared
    to 2D images, which is an important research direction for the future.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Multi-modal Re-ID
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RGB-IR multimodal Re-ID. Both day and night are important scenes of pedestrian
    life, and in the case of insufficient illumination, images can only be collected
    by infrared cameras (Nguyen et al.,, [2017](#bib.bib66); [Wu et al., 2017b,](#bib.bib107)
    ). If there is occlusion in the scene, the infrared image still has occlusion.
    At the same time, the infrared image can be used as a special channel of the RGB
    image, which makes the representation mode of the RGB image more complete and
    can supplement the information well in the case of information loss caused by
    occlusion. The RGB-IR multimodalRe-ID is designed to feed both infrared and conventional
    images into the model. Removal of interference from occlusion is achieved by the
    relationship of different modalities or by combining approaches, such as attention
    mechanisms, in dealing with single modal noise, multi-scale, etc.
  prefs: []
  type: TYPE_NORMAL
- en: DDAG (Ye et al.,, [2020](#bib.bib119)) proposes a dynamic dual attention cross-modal
    graph structure. First, local attention is generated according to the similarity
    between features. Then, an aggregation representation of part-level relation learning
    is introduced. At the same time, the graph structure is introduced to remove the
    interference of noise based on relation. MSPAC (Zhang et al.,, [2021](#bib.bib122))
    proposes a multi-scale-based component awareness mechanism. By adding an attention
    mechanism at the single scale, more fine-grained features are extracted in channel
    and spatial dimensions. After that, feature aggregation is realized using a cascade
    framework. By adding different scale features, the global structured body information
    is represented uniformly. CM-LSP-GE ([Wang et al., 2022e,](#bib.bib100) ) is a
    framework for global features and local characteristics. The occlusion problem
    is solved by local features, and the alignment problem is solved by image segmentation
    and computing the shortest path of local features in two images.
  prefs: []
  type: TYPE_NORMAL
- en: To solve the intra-modal problem, HMML ([Zhang et al., 2022b,](#bib.bib124)
    ) introduces a pairing-based intra-modal similarity constraint to enhance the
    features. Similarly, CMC (Wen et al.,, [2022](#bib.bib105)) introduces multi-scale,
    multi-level feature learning to achieve refined feature extraction. To solve the
    problem of pedestrian pose misalignment caused by occlusion in transmembrane state,
    DAPN (Liu et al.,, [2022](#bib.bib55)) proposes an alignment network that can
    learn global and local modal invariance. Specifically, DAPN (Liu et al.,, [2022](#bib.bib55))
    first introduces an adaptive spatial transform module to align images. Second,
    the image is segmented horizontally to extract local features. Meanwhile, to learn
    the similarity representation of different modes, the different modes are embedded
    into a unified feature space (Cai et al.,, [2021](#bib.bib3)). Finally, the global
    and local features are fused as the final features. To solve the alignment problem,
    SPOT (Chen et al.,, [2022](#bib.bib4)) proposes a transformer-based network. First,
    a relational network composed of four convolution layers and two pooling layers
    is used to process the human body key point heat map to obtain the pedestrian
    body structure information. Combining it with the attention mechanism, the pedestrian
    region is highlighted to remove the background interference. Then, the transformer
    is used to mine the relationship between parts’ positions and features to enhance
    the discriminability of local features. The final feature is extracted by adding
    weight combination. VI (Park et al.,, [2021](#bib.bib70)) first extracts features
    of IR and RGB using a dual-stream CNN. The cosine similarity of all corresponding
    feature pairs is calculated simultaneously, and the corresponding matching probability
    is calculated by the SoftMax function. These probabilities are taken as semantic
    similarity features of different modalities. Then, more discriminative features
    are extracted by local feature association. The above features are then aggregated
    to form the final feature. To solve the problem of image internal misalignment,
    DTRM ([Ye et al., 2021a,](#bib.bib118) ) combines attention and partial aggregation,
    and uses the context relationship of the two modalities to improve the global
    features to eliminate the noise effect.
  prefs: []
  type: TYPE_NORMAL
- en: RGB-Depth multimodal Re-ID. Depth ([Wang et al., 2021a,](#bib.bib92) ) images
    are captured through devices such as laser radars, and they provide body shape
    and skeletal information by measuring distance. For images, when information is
    lost owing to obstacles, depth features can supplement the position information
    of texture features to assist in a more complete expression. At the same time,
    it can solve the lighting problem and the problem of changing clothes for pedestrians.
    It can also help solve the problems of obstacle illumination and different changing
    environments for clothing. RGB-Depth multimodal Re-ID aims to input depth map
    and RGB image simultaneously, and remove the interference of noise by the attention
    mechanism and other methods.
  prefs: []
  type: TYPE_NORMAL
- en: CMD (Hafner et al.,, [2022](#bib.bib21)) proposes an approach combining embedding
    representation and feature distillation. It removes noise through a gate-controlled
    attention mechanism. This mechanism uses one modality to dynamically activate
    the more discriminative features in another modality by gating signals.
  prefs: []
  type: TYPE_NORMAL
- en: RGB-Text multimodal Re-ID. RGB-Text multimodal Re-ID aims to introduce text
    data to enhance feature representation by sharing semantic information and attentional
    calibration to eliminate the effect of noise.In daily life, text information is
    one of the most frequently used types of information. When image information is
    missing or cannot be used owing to obstacles, it can be supplemented with text.
  prefs: []
  type: TYPE_NORMAL
- en: AXM-Net (Farooq et al.,, [2022](#bib.bib14)) dynamically exploits multi-scale
    information of text and images, recalibrating each modality according to the shared
    semantics and adding contextual attention to the text branch to supplement the
    information of the convolution block. Furthermore, attention is introduced to
    enhance feature consistency and local information of the visual part. It can learn
    the alignment semantic information of different modalities and automatically remove
    the interference of irrelevant information.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Method Comparison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 2: Comparison of experimental results based on local feature learning
    methods. The red numbers indicate the best results. (in $\%$).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Method | Venue | Occluded-Duke | Occluded-REID | Partial-REID | Partial-iLIDS
    | Market1501 | DukeMTMC-reID |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Rank-1 | mAP | Rank-1 | mAP | Rank-1 | Rank-3 | Rank-1 | Rank-3 | Rank-1
    | mAP | Rank-1 | mAP |'
  prefs: []
  type: TYPE_TB
- en: '| Local Feature Learning | Body Segmentation | CBDB-Net (Tan et al.,, [2021](#bib.bib85))
    | TCSVT2021 | 50.09 | 38.9 | - | - | 66.7 | 78.3 | 68.4 | 81.5 | 94.4 | 85 | 87.7
    | 74.3 |'
  prefs: []
  type: TYPE_TB
- en: '| OCNet (Kim et al.,, [2022](#bib.bib41)) | ICASSP2022 | 64.30 | 54.40 | -
    | - | - | - | - | - | 95 | 89.3 | 90.5 | 80.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Pose Estimation | AACN (Xu et al.,, [2018](#bib.bib111)) | CVPR2018 | - |
    - | - | - | - | - | - | - | 88.69 | 82.96 | 76.84 | 59.25 |'
  prefs: []
  type: TYPE_TB
- en: '| ACSAP ([He et al., 2021c,](#bib.bib29) ) | ICIP2021 | - | - | - | - | 77
    | 83.7 | 76.5 | 87.4 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DAReID (Xu et al.,, [2021](#bib.bib112)) | KBS2021 | 63.4 | - | - | - | 68.1
    | 79.5 | 76.7 | 85.3 | 94.6 | 87 | 88.9 | 78.4 |'
  prefs: []
  type: TYPE_TB
- en: '| DSA-reID (Zhang et al.,, [2019](#bib.bib128)) | CVPR2019 | - | - | - | -
    | - | - | - | - | 95.7 | 87.6 | 86.2 | 74.3 |'
  prefs: []
  type: TYPE_TB
- en: '| HOReID ([Wang et al., 2020a,](#bib.bib93) ) | CVPR2020 | - | - | 80.3 | 70.2
    | 85.3 | 91 | 72.6 | 86.4 | 94.2 | 84.9 | 86.9 | 75.6 |'
  prefs: []
  type: TYPE_TB
- en: '| PAFM (Yang et al.,, [2022](#bib.bib115)) | NCA2022 | 55.1 | 42.3 | 76.4 |
    68 | 82.5 | - | - | - | 95.6 | 88.5 | 91.2 | 80.1 |'
  prefs: []
  type: TYPE_TB
- en: '| PDC (Su et al.,, [2017](#bib.bib79)) | ICCV2017 | - | - | - | - | - | - |
    - | - | 84.14 | 63.41 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PGFL-KD (Zheng et al.,, [2021](#bib.bib133)) | MM2021 | 63 | 54.1 | 80.7
    | 70.3 | 85.1 | 90.8 | 74 | 86.7 | 95.3 | 87.2 | 89.6 | 79.5 |'
  prefs: []
  type: TYPE_TB
- en: '| PGMANet (Zhai et al.,, [2021](#bib.bib121)) | IJCNN2021 | - | - | - | - |
    82.1 | 85.5 | 68.8 | 78.1 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PMFB (Miao et al.,, [2021](#bib.bib63)) | TNNLS2021 | 56.3 | 43.5 | - | -
    | 72.5 | 83 | 70.6 | 81.3 | 92.7 | 81.3 | 86.2 | 72.6 |'
  prefs: []
  type: TYPE_TB
- en: '| PVPM ([Gao et al., 2020b,](#bib.bib18) ) | CVPR2020 | - | - | 70.4 | 61.2
    | 78.3 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Semantic Segmentation | Co-Attention (Lin & Wang,, [2021](#bib.bib51)) |
    ICIP2021 | - | - | - | - | 83 | 90.3 | 73.1 | 83.2 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| HPNet (Huang et al.,, [2020](#bib.bib31)) | ICME2020 | - | - | 87.3 | 77.4
    | 85.7 | - | 68.9 | 80.7 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MMGA (Cai et al.,, [2019](#bib.bib2)) | CVPR2019 | - | - | - | - | - | -
    | - | - | 95 | 87.2 | 89.5 | 78.1 |'
  prefs: []
  type: TYPE_TB
- en: '| SGSFA (Ren et al.,, [2020](#bib.bib74)) | PMLR2020 | 62.3 | 47.4 | 63.1 |
    53.2 | 68.2 | - | - | - | 92.3 | 80.2 | 84.7 | 70.8 |'
  prefs: []
  type: TYPE_TB
- en: '| SORN (Zhang et al.,, [2020](#bib.bib126)) | TCSVT2020 | 57.6 | 46.3 | - |
    - | 76.7 | 84.3 | 79.8 | 86.6 | 94.8 | 84.5 | 86.9 | 74.1 |'
  prefs: []
  type: TYPE_TB
- en: '| SPReID (Kalayeh et al.,, [2018](#bib.bib37)) | CVPR2017 | - | - | - | - |
    - | - | - | - | 94.63 | 90.96 | 88.96 | 84.99 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Attribute &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Annotation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ASAN (Jin et al.,, [2021](#bib.bib36)) | TCSVT2021 | 55.40 | 43.80 | 82.50
    | 71.80 | 86.80 | 93.50 | 81.70 | 88.30 | 94.60 | 85.30 | 87.50 | 76.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixing Method | FGSA ([Zhou et al., 2020a,](#bib.bib145) ) | TIP2020 | -
    | - | - | - | - | - | - | - | 91.50 | 85.40 | 85.90 | 74.10 |'
  prefs: []
  type: TYPE_TB
- en: '| GASM (He & Liu,, [2020](#bib.bib25)) | ECCV2020 | - | - | 80.30 | 73.10 |
    - | - | - | - | 95.30 | 84.70 | 88.30 | 74.40 |'
  prefs: []
  type: TYPE_TB
- en: '| PGFA (Miao et al.,, [2019](#bib.bib62)) | ICCV2019 | - | - | - | - | 68.80
    | 80.00 | 69.10 | 80.90 | 91.20 | 76.80 | 82.60 | 65.50 |'
  prefs: []
  type: TYPE_TB
- en: '| SSPReID (Quispe & Pedrini,, [2019](#bib.bib73)) | IVC 2019 | - | - | - |
    - | - | - | - | - | 93.70 | 90.80 | 86.40 | 83.70 |'
  prefs: []
  type: TYPE_TB
- en: '| LKWS (Yang et al.,, [2021](#bib.bib116)) | ICCV2021 | 62.2 | 46.3 | 81 |
    71 | 85.7 | 93.7 | 80.7 | 88.2 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| TSA ([Gao et al., 2020a,](#bib.bib17) ) | ACM MM2020 | - | - | - | - | 72.70
    | 85.20 | 73.90 | 84.70 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Comparison of experimental results based on relationship representation
    methods. The red numbers indicate the best results.(in $\%$).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  | Occluded-Duke | Occluded-REID | Partial-REID | Partial-iLIDS
    | Market1501 | DukeMTMC-reID |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Method | Venue | Rank-1 | mAP | Rank-1 | mAP | Rank-1 | Rank-3 | Rank-1
    | Rank-3 | Rank-1 | mAP | Rank-1 | mAP |'
  prefs: []
  type: TYPE_TB
- en: '| Relationship Representation | Shared Area | KBFM (Han et al.,, [2020](#bib.bib22))
    | ICIP2020 | - | - | - | - | 69.7 | 82.2 | 64.1 | 73.9 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PPCL ([He et al., 2021b,](#bib.bib28) ) | CVPR2021 | - | - | - | - | 83.70
    | 88.70 | 71.40 | 85.70 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| VPM ([Sun et al., 2019b,](#bib.bib82) ) | CVPR2019 | - | - | - | - | 67.70
    | 81.90 | 65.50 | 74.80 | 90.40 | 75.70 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Clustering | ISP (Zhu et al.,, [2020](#bib.bib147)) | ECCV2020 | 62.80 |
    52.30 | - | - | - | - | - | - | 94.63 | 90.69 | 88.96 | 84.99 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Figure &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Convolution &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| HOReID ([Wang et al., 2020a,](#bib.bib93) ) | CVPR2020 | - | - | 80.3 | 70.2
    | 85.3 | 91 | 72.6 | 86.4 | 94.2 | 84.9 | 86.9 | 75.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Attention | AACN (Xu et al.,, [2018](#bib.bib111)) | CVPR2018 | - | - | -
    | - | - | - | - | - | 88.69 | 82.96 | 76.84 | 59.25 |'
  prefs: []
  type: TYPE_TB
- en: '| APN (Huo et al.,, [2021](#bib.bib33)) | ICPR2021 | - | - | - | - | 71.80
    | 85.50 | 66.40 | 76.50 | 96.00 | 89.00 | 89.50 | 79.20 |'
  prefs: []
  type: TYPE_TB
- en: '| CASN (Zheng et al.,, [2019](#bib.bib136)) | CVPR2019 | - | - | - | - | -
    | - | - | - | 94.40 | 82.80 | 87.70 | 73.70 |'
  prefs: []
  type: TYPE_TB
- en: '| Co-Attention (Lin & Wang,, [2021](#bib.bib51)) | ICIP2021 | - | - | - | -
    | 83.00 | 90.30 | 73.10 | 83.20 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DAReID (Xu et al.,, [2021](#bib.bib112)) | KBS2021 | 63.4 | - | - | - | 68.1
    | 79.5 | 76.7 | 85.3 | 94.6 | 87 | 88.9 | 78.4 |'
  prefs: []
  type: TYPE_TB
- en: '| DSOP ([Wang et al., 2020b,](#bib.bib98) ) | JPCS2020 | 57.70 | 45.30 | -
    | - | - | - | - | - | 95.40 | 85.90 | 88.20 | 77.00 |'
  prefs: []
  type: TYPE_TB
- en: '| MHSA-Net ([Tan et al., 2022a,](#bib.bib86) ) | TNNLS2022 | 59.70 | 44.80
    | - | - | 85.70 | 91.30 | 74.90 | 87.20 | 95.50 | 93.00 | 90.70 | 87.20 |'
  prefs: []
  type: TYPE_TB
- en: '| OCNet (Kim et al.,, [2022](#bib.bib41)) | ICASSP2022 | 64.30 | 54.40 | -
    | - | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PAFM (Yang et al.,, [2022](#bib.bib115)) | NCA2022 | 55.1 | 42.3 | 76.4 |
    68 | 82.5 | - | - | - | 95.6 | 88.5 | 91.2 | 80.1 |'
  prefs: []
  type: TYPE_TB
- en: '| PISNet (Zhao et al.,, [2020](#bib.bib131)) | ECCV2020 | - | - | - | - | -
    | - | - | - | 95.60 | 87.10 | 88.80 | 78.70 |'
  prefs: []
  type: TYPE_TB
- en: '| PSE (Sarfraz et al.,, [2018](#bib.bib76)) | CVPR2018 | - | - | - | - | -
    | - | - | - | 90.30 | 84.00 | 85.20 | 79.80 |'
  prefs: []
  type: TYPE_TB
- en: '| QPM ([Wang et al., 2022b,](#bib.bib96) ) | ITM2022 | 64.40 | 49.70 | - |
    - | 81.70 | 88.00 | 77.30 | 85.70 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| VPM ([Sun et al., 2019b,](#bib.bib82) ) | CVPR2019 | - | - | - | - | 67.70
    | 81.90 | 65.50 | 74.80 | 90.40 | 75.70 | - | - |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Comparison of experimental results with other methods. The red numbers
    indicate the best results.(in $\%$).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | Occluded-Duke | Occluded-REID | Partial-REID | Partial-iLIDS | Market1501
    | DukeMTMC-reID |'
  prefs: []
  type: TYPE_TB
- en: '|  | Method | Venue | Rank-1 | mAP | Rank-1 | mAP | Rank-1 | Rank-3 | Rank-1
    | Rank-3 | Rank-1 | mAP | Rank-1 | mAP |'
  prefs: []
  type: TYPE_TB
- en: '| Transformer | DPM ([Tan et al., 2022b,](#bib.bib87) ) | ACM MM 2022 | 71.40
    | 61.80 | 85.50 | 79.70 | - | - | - | - | 95.50 | 89.70 | 91.00 | 82.60 |'
  prefs: []
  type: TYPE_TB
- en: '| FED ([Wang et al., 2022g,](#bib.bib104) ) | CVPR2022 | 68.10 | 56.40 | 86.30
    | 79.30 | 83.10 | - | - | - | 95.00 | 86.30 | 89.40 | 78.00 |'
  prefs: []
  type: TYPE_TB
- en: '| FRT (Xu et al.,, [2022](#bib.bib110)) | TIP2022 | 70.70 | 61.30 | 80.40 |
    71.00 | 88.20 | 93.20 | 73.00 | 87.00 | 95.50 | 88.10 | 90.50 | 81.70 |'
  prefs: []
  type: TYPE_TB
- en: '| TransReID ([He et al., 2021a,](#bib.bib27) ) | ICCV2021 | 66.40 | 59.20 |
    - | - | - | - | - | - | 95.20 | 88.90 | 90.70 | 82.00 |'
  prefs: []
  type: TYPE_TB
- en: '| PFD ([Wang et al., 2022d,](#bib.bib99) ) | AAAI2022 | 69.50 | 61.80 | 81.50
    | 83.00 | - | - | - | - | 95.50 | 89.70 | 91.20 | 83.20 |'
  prefs: []
  type: TYPE_TB
- en: '| PFT (Zhao et al.,, [2022](#bib.bib132)) | NCA2022 | 69.80 | 60.80 | 83.00
    | 78.30 | 81.30 | 79.90 | 68.10 | 81.50 | 95.30 | 88.80 | 90.70 | 82.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixing Method | DRL-Net (Jia et al.,, [2022](#bib.bib35)) | TMM2021 | 65.80
    | 53.90 | - | - | - | - | - | - | 94.70 | 86.90 | 88.10 | 76.60 |'
  prefs: []
  type: TYPE_TB
- en: '| Pirt (Ma et al.,, [2021](#bib.bib59)) | ACMMM2021 | 60.00 | 50.90 | - | -
    | - | - | - | - | 94.10 | 86.30 | 88.90 | 77.60 |'
  prefs: []
  type: TYPE_TB
- en: '| Multiscale | DSR (He et al.,, [2018](#bib.bib24)) | CVPR2018 | 40.80 | 30.40
    | 72.80 | 62.80 | 50.70 | 70.00 | 58.80 | 67.20 | 83.58 | 64.25 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| FPR (He et al.,, [2019](#bib.bib26)) | ICCV2019 | - | - | 78.30 | 68.00 |
    81.00 | - | 68.10 | - | 95.42 | 86.58 | 88.64 | 78.42 |'
  prefs: []
  type: TYPE_TB
- en: '| Regional Reconfiguration | ACSAP ([He et al., 2021c,](#bib.bib29) ) | ICIP2021
    | - | - | - | - | 77.00 | 83.70 | 76.50 | 87.40 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RFCNet (Hou et al.,, [2021](#bib.bib30)) | TPAMI2021 | 63.90 | 54.50 | -
    | - | - | - | - | - | 95.20 | 89.20 | 90.70 | 80.70 |'
  prefs: []
  type: TYPE_TB
- en: '| Data Enhancement | IGOAS (Zhao et al.,, [2021](#bib.bib129)) | TIP2021 |
    60.10 | 49.40 | 81.10 | - | - | - | - | - | 93.40 | 84.10 | 86.90 | 75.10 |'
  prefs: []
  type: TYPE_TB
- en: '| OAMN ([Chen et al., 2021c,](#bib.bib8) ) | ICCV2021 | 62.60 | 46.10 | - |
    - | 86.00 | - | 77.30 | - | 93.20 | 79.80 | 86.30 | 72.60 |'
  prefs: []
  type: TYPE_TB
- en: '| SSGR (Yan et al.,, [2021](#bib.bib114)) | ICCV2021 | 65.80 | 57.20 | 78.50
    | 72.90 | - | - | - | - | 96.10 | 89.30 | 91.10 | 81.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Regularization | MHSA-Net ([Tan et al., 2022a,](#bib.bib86) ) | TNNLS2022
    | 59.70 | 44.80 | - | - | 85.70 | 91.30 | 74.90 | 87.20 | 95.50 | 93.00 | 90.70
    | 87.20 |'
  prefs: []
  type: TYPE_TB
- en: 'We statistically evaluate the results of the occluded person Re-ID methods
    on two general datasets (Market1501 ([Zheng et al., 2015a,](#bib.bib134) ), DukeMTMC-reID
    (Ristani et al.,, [2016](#bib.bib75))), two occluded person Re-ID datasets (Occluded-DukeMTMC
    (Miao et al.,, [2019](#bib.bib62)), Occluded-REID ([Zheng et al., 2015b,](#bib.bib138)
    )) and two partial person Re-ID datasets (Partial-ReID ([Zheng et al., 2015b,](#bib.bib138)
    ), Partial-iLIDS (He et al.,, [2018](#bib.bib24))). We simply divide them into
    three categories: The experimental results based on the local feature learning
    method are shown in Table [2](#S5.T2 "Table 2 ‣ 5 Method Comparison ‣ Occluded
    Person Re-Identification with Deep Learning: A Survey and Perspectives"). It contains
    body segmentation, pose estimation, semantic segmentation, attribute annotation,
    and mixing method. The experimental results based on the relationship representation
    method are shown in Table [3](#S5.T3 "Table 3 ‣ 5 Method Comparison ‣ Occluded
    Person Re-Identification with Deep Learning: A Survey and Perspectives"). It contains
    shared area, clustering, figure convolution, and attention. The experimental results
    of other methods are shown in Table [4](#S5.T4 "Table 4 ‣ 5 Method Comparison
    ‣ Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives").
    It contains the transformer, mixing method, multi-scale, regional reconfiguration,
    data enhancement, and regularization. For an introduction to each class of methods
    and details of each study, the reader is referred to Section 4.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the results we can get the following information: (1) From the results
    we can get the following information: OCNet (Kim et al.,, [2022](#bib.bib41))
    based on local feature learning, QPM ([Wang et al., 2022b,](#bib.bib96) ) based
    on relational representation, DPM ([Tan et al., 2022b,](#bib.bib87) ) and PFD
    ([Wang et al., 2022d,](#bib.bib99) ) based on transformer perform better on the
    Occluded-DukeMTMC dataset. On the Occluded-REID dataset, HPNet (Huang et al.,,
    [2020](#bib.bib31)) based on local feature learning, HOReID ([Wang et al., 2020a,](#bib.bib93)
    ) based on relational representation, FED ([Wang et al., 2022g,](#bib.bib104)
    ) and PFD ([Wang et al., 2022d,](#bib.bib99) ) based on transformer perform stably.
    On the Partial-ReID dataset, ASAN (Jin et al.,, [2021](#bib.bib36)) and LKWS (Yang
    et al.,, [2021](#bib.bib116)) based on local feature learning, MHSA-Net ([Tan
    et al., 2022a,](#bib.bib86) ) based on relational representation, and FRT (Xu
    et al.,, [2022](#bib.bib110)) based on transformer achieve better performance.
    On the Partial-iLIDS dataset, ASAN (Jin et al.,, [2021](#bib.bib36)) based on
    local feature learning, MHSA-Net ([Tan et al., 2022a,](#bib.bib86) ) and QPM ([Wang
    et al., 2022b,](#bib.bib96) ) based on relational representation, ACSAP ([He et al.,
    2021c,](#bib.bib29) ) based on region reconstruction, and OAMN ([Chen et al.,
    2021c,](#bib.bib8) ) based on data augmentation achieve very stable performance.
    Therefore, there is no single method that achieves the best performance on all
    datasets. (2) In general, the performance on the occlusion dataset reflects the
    ability of the model to resist noise, the performance on the partial dataset reflects
    the recognition ability of the model under the condition of missing pedestrian
    information, and the performance on the general dataset reflects the comprehensive
    performance of the model. Each of these approaches addresses one or more specific
    problems. (3) Attention and pose estimation are the more mainstream and typical
    of the many pedestrian re-identification methods for dealing with occlusion. Attribute
    annotation-based, clustering-based, figure convolution-based and regularisation-based
    methods, on the other hand, have received less attention.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Future directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Richer, higher quality datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most models are evaluated on datasets collected in controlled environments.
    The data in real-world scenarios is often uncontrollable, which can significantly
    impact the model’s performance on such datasets.
  prefs: []
  type: TYPE_NORMAL
- en: For an occluded person Re-ID dataset, it is necessary to incorporate one or
    more modal inputs, including a diverse combination of images, text, infrared maps,
    and depth maps. This enables the model to effectively handle a wider range of
    realistic occlusion scenarios. Moreover, there is a lack of larger datasets encompassing
    a broader range of domains, environments (Gou et al.,, [2018](#bib.bib19)), and
    higher resolutions, which would provide richer content and higher quality for
    research purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 More robust and varied feature extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3D Re-ID. Inspired by human three-dimensional cognition, some researchers believe
    that the complete pedestrian representation should be a fusion of 3D and 2D (Zheng
    et al.,, [2022](#bib.bib139)).
  prefs: []
  type: TYPE_NORMAL
- en: At present, PointNet (Qi et al.,, [2017](#bib.bib72)), as a representative of
    deep learning methods in point cloud feature extraction ([Wang et al., 2022a,](#bib.bib91)
    ; CS et al.,, [2022](#bib.bib10)), has demonstrated promising results. Point cloud
    completion (Fei et al.,, [2022](#bib.bib15)) and point cloud correction can aid
    in 3D occluded person Re-ID, while 3D pose estimation ([Wang et al., 2021b,](#bib.bib95)
    ) and 3D semantic segmentation (Xie et al.,, [2020](#bib.bib109)) can guide the
    feature extraction process for person Re-ID. However, research in the 3D domain
    for pedestrian recognition (Zhao et al.,, [2013](#bib.bib130); [Sun et al., 2019b,](#bib.bib82)
    ) is still relatively limited compared to the advancements in 2D approaches. Therefore,
    3D occluded person Re-ID represents a significant and promising research direction
    for the future (Tirkolaee et al.,, [2020](#bib.bib88)).
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal Re-ID. The information captured from different modalities demonstrates
    a significant diversity in content representation ([Wu et al., 2017b,](#bib.bib107)
    ; [Wu et al., 2017a,](#bib.bib106) ). Improving the interaction, fusion, and extraction
    of more comprehensive pedestrian features at both the data and feature extraction
    stages represents a crucial research direction for future advancements (Tutsoy
    & Tanrikulu,, [2022](#bib.bib90); Sekhar et al.,, [2017](#bib.bib77)).
  prefs: []
  type: TYPE_NORMAL
- en: Cross-resolution occluded person Re-ID. Owing to the influence of the distance
    and pixel size of the collection device, the resolution of the collected samples
    is uneven, and the feature space correspondence is also inconsistent (Li et al.,,
    [2019](#bib.bib49)). At the same time, low resolution will lose significant spatial
    and detail information (Mao et al.,, [2019](#bib.bib60)). How to extract pedestrian
    features at different resolutions under occlusion conditions is a problem to be
    solved in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Fast response, smaller occlusion person Re-ID model. Constructing smaller (Zhou
    et al.,, [2019](#bib.bib143); Li et al.,, [2018](#bib.bib46)) and faster (Liu
    et al.,, [2017](#bib.bib53)) occluded person Re-ID models with reduced parameters
    is a crucial research direction for future advancements (Tutsoy et al.,, [2018](#bib.bib89)).
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised and semi-supervised occluded person Re-ID. The complex manual labeling
    process is omitted, and the pedestrian features are learned by using the datasets
    without labels (Zhang & Lu,, [2018](#bib.bib127); Liu et al.,, [2019](#bib.bib54))
    or with a small number of labels ([Wang et al., 2019c,](#bib.bib103) ; [Wang et al.,
    2019a,](#bib.bib94) ; Nagaraju et al.,, [2016](#bib.bib65)).
  prefs: []
  type: TYPE_NORMAL
- en: Currently, the performance of unsupervised and semi-supervised based methods
    on the task of occluded person Re-ID is still some way off compared to supervised
    methods. Supervised methods usually rely on large-scale labelled datasets for
    training and thus can achieve high performance. However, as unsupervised and semi-supervised
    methods are increasingly studied, they show significant potential in improving
    the generalisation of occluded person Re-ID models.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Occluded person Re-ID system
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At present, few researchers combine object detection and occluded person Re-ID
    together. The end-to-end person Re-ID systems are lacking, and the integrated
    system has more applications in real life (Martinel et al.,, [2016](#bib.bib61)).
    How to combine the two more effectively and rationally and design a occluded person
    Re-ID system that is more robust to occlusion is an important research direction.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This review offers a comprehensive and integrated analysis and discussion of
    deep learning methods for occluded person Re-ID, addressing both practical and
    research-driven requirements. Firstly, we introduce the classification of occlusion
    problems and the datasets specifically designed for occluded person Re-ID. Secondly,
    we comprehensively classify and introduce the methods proposed in top international
    journals and conferences before 2023 for addressing occluded person Re-ID. Finally,
    the future prospects of occluded person Re-ID are analyzed from data, feature,
    and system perspectives, respectively. In this study, we categorize the most significant
    image feature extraction methods into five major categories: local feature learning,
    relational representation, transformer-based methods, mixing methods, and other
    approaches. This review will assist researchers in comprehending the process and
    objectives of these methods, providing valuable references and contributing to
    the research significance in the advancement of occluded Re-ID.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bedagkar-Gala & Shah, (2014) Bedagkar-Gala, A. & Shah, S. K. (2014). A survey
    of approaches and trends in person re-identification. Image and vision computing,
    32(4), 270–286.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al., (2019) Cai, H., Wang, Z., & Cheng, J. (2019). Multi-scale body-part
    mask guided attention for person re-identification. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Workshops (pp. 0–0).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al., (2021) Cai, X., Liu, L., Zhu, L., & Zhang, H. (2021). Dual-modality
    hard mining triplet-center loss for visible infrared person re-identification.
    Knowledge-Based Systems, 215, 106772.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al., (2022) Chen, C., Ye, M., Qi, M., Wu, J., Jiang, J., & Lin, C.-W.
    (2022). Structure-aware positional transformer for visible-infrared person re-identification.
    IEEE Transactions on Image Processing, 31, 2352–2364.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (5) Chen, H., Wang, Y., Lagadec, B., Dantcheva, A., & Bremond, F. (2021a). Joint
    generative and contrastive learning for unsupervised person re-identification.
    In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
    (pp. 2004–2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (6) Chen, J., Jiang, X., Wang, F., Zhang, J., Zheng, F., Sun, X., & Zheng, W.-S.
    (2021b). Learning 3d shape feature for texture-insensitive person re-identification.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (pp. 8146–8155).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al., (2018) Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., & Adam,
    H. (2018). Encoder-decoder with atrous separable convolution for semantic image
    segmentation. In Proceedings of the European conference on computer vision (ECCV)
    (pp. 801–818).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(8) Chen, P., Liu, W., Dai, P., Liu, J., Ye, Q., Xu, M., Chen, Q., & Ji, R.
    (2021c). Occlude them all: Occlusion-aware attention network for occluded person
    re-id. In Proceedings of the IEEE/CVF international conference on computer vision
    (pp. 11833–11842).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al., (2011) Cheng, D. S., Cristani, M., Stoppa, M., Bazzani, L., &
    Murino, V. (2011). Custom pictorial structures for re-identification. In Bmvc,
    volume 1 (pp.6̃).: Citeseer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CS et al., (2022) CS, W., H, W., X, N., SW, T., & WJ, L. (2022). 3d point cloud
    classification method based on dynamic coverage of local area. Journal of Software,
    (pp. 0–0).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al., (2023) Dong, N., Zhang, L., Yan, S., Tang, H., & Tang, J. (2023).
    Erasing, transforming, and noising defense network for occluded person re-identification.
    arXiv preprint arXiv:2307.07187.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al., (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., et al. (2020). An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ess et al., (2008) Ess, A., Leibe, B., Schindler, K., & Van Gool, L. (2008).
    A mobile vision system for robust multi-person tracking. In 2008 IEEE Conference
    on Computer Vision and Pattern Recognition (pp. 1–8).: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Farooq et al., (2022) Farooq, A., Awais, M., Kittler, J., & Khalid, S. S. (2022).
    Axm-net: Implicit cross-modal feature alignment for person re-identification.
    In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36 (pp. 4477–4485).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fei et al., (2022) Fei, B., Yang, W., Chen, W.-M., Li, Z., Li, Y., Ma, T., Hu,
    X., & Ma, L. (2022). Comprehensive review of deep learning-based 3d point cloud
    completion processing and analysis. IEEE Transactions on Intelligent Transportation
    Systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al., (2019) Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., & Lu,
    H. (2019). Dual attention network for scene segmentation. In Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition (pp. 3146–3154).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (17) Gao, L., Zhang, H., Gao, Z., Guan, W., Cheng, Z., & Wang, M. (2020a). Texture
    semantically aligned with visibility-aware for partial person re-identification.
    In Proceedings of the 28th ACM International Conference on Multimedia (pp. 3771–3779).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (18) Gao, S., Wang, J., Lu, H., & Liu, Z. (2020b). Pose-guided visible part
    matching for occluded person reid. In Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition (pp. 11744–11752).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gou et al., (2018) Gou, M., Wu, Z., Rates-Borras, A., Camps, O., Radke, R. J.,
    et al. (2018). A systematic evaluation and benchmark for person re-identification:
    Features, metrics, and datasets. IEEE transactions on pattern analysis and machine
    intelligence, 41(3), 523–536.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Güler et al., (2018) Güler, R. A., Neverova, N., & Kokkinos, I. (2018). Densepose:
    Dense human pose estimation in the wild. In Proceedings of the IEEE conference
    on computer vision and pattern recognition (pp. 7297–7306).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hafner et al., (2022) Hafner, F. M., Bhuyian, A., Kooij, J. F., & Granger, E.
    (2022). Cross-modal distillation for rgb-depth person re-identification. Computer
    Vision and Image Understanding, 216, 103352.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al., (2020) Han, C., Gao, C., & Sang, N. (2020). Keypoint-based feature
    matching for partial person re-identification. In 2020 IEEE International Conference
    on Image Processing (ICIP) (pp. 226–230).: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al., (2016) He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual
    learning for image recognition. In Proceedings of the IEEE conference on computer
    vision and pattern recognition (pp. 770–778).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al., (2018) He, L., Liang, J., Li, H., & Sun, Z. (2018). Deep spatial
    feature reconstruction for partial person re-identification: Alignment-free approach.
    In Proceedings of the IEEE conference on computer vision and pattern recognition
    (pp. 7073–7082).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He & Liu, (2020) He, L. & Liu, W. (2020). Guided saliency feature learning
    for person re-identification in crowded scenes. In Computer Vision–ECCV 2020:
    16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVIII
    16 (pp. 357–373).: Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al., (2019) He, L., Wang, Y., Liu, W., Zhao, H., Sun, Z., & Feng, J. (2019).
    Foreground-aware pyramid reconstruction for alignment-free occluded person re-identification.
    In Proceedings of the IEEE/CVF international conference on computer vision (pp. 8450–8459).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(27) He, S., Luo, H., Wang, P., Wang, F., Li, H., & Jiang, W. (2021a). Transreid:
    Transformer-based object re-identification. In Proceedings of the IEEE/CVF international
    conference on computer vision (pp. 15013–15022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (28) He, T., Shen, X., Huang, J., Chen, Z., & Hua, X.-S. (2021b). Partial person
    re-identification with part-part correspondence learning. In Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9105–9115).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(29) He, Y., Yang, H., & Chen, L. (2021c). Adversarial cross-scale alignment
    pursuit for seriously misaligned person re-identification. In 2021 IEEE International
    Conference on Image Processing (ICIP) (pp. 2373–2377).: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou et al., (2021) Hou, R., Ma, B., Chang, H., Gu, X., Shan, S., & Chen, X.
    (2021). Feature completion for occluded person re-identification. IEEE Transactions
    on Pattern Analysis and Machine Intelligence, 44(9), 4894–4912.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al., (2020) Huang, H., Chen, X., & Huang, K. (2020). Human parsing
    based alignment with multi-task learning for occluded person re-identification.
    In 2020 IEEE international conference on multimedia and expo (ICME) (pp. 1–6).:
    IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al., (2022) Huang, H., Zheng, A., Li, C., He, R., et al. (2022). Parallel
    augmentation and dual enhancement for occluded person re-identification. arXiv
    preprint arXiv:2210.05438.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huo et al., (2021) Huo, L., Song, C., Liu, Z., & Zhang, Z. (2021). Attentive
    part-aware networks for partial person re-identification. In 2020 25th International
    Conference on Pattern Recognition (ICPR) (pp. 3652–3659).: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Islam, (2022) Islam, K. (2022). Recent advances in vision transformer: A survey
    and outlook of recent work. arXiv preprint arXiv:2203.01536.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al., (2022) Jia, M., Cheng, X., Lu, S., & Zhang, J. (2022). Learning
    disentangled representation implicitly via transformer for occluded person re-identification.
    IEEE Transactions on Multimedia.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al., (2021) Jin, H., Lai, S., & Qian, X. (2021). Occlusion-sensitive
    person re-identification via attribute-based shift attention. IEEE Transactions
    on Circuits and Systems for Video Technology, 32(4), 2170–2185.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalayeh et al., (2018) Kalayeh, M. M., Basaran, E., Gökmen, M., Kamasak, M. E.,
    & Shah, M. (2018). Human semantic parsing for person re-identification. In Proceedings
    of the IEEE conference on computer vision and pattern recognition (pp. 1062–1071).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kanazawa et al., (2018) Kanazawa, A., Black, M. J., Jacobs, D. W., & Malik,
    J. (2018). End-to-end recovery of human shape and pose. In Proceedings of the
    IEEE conference on computer vision and pattern recognition (pp. 7122–7131).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khan et al., (2022) Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S.,
    & Shah, M. (2022). Transformers in vision: A survey. ACM computing surveys (CSUR),
    54(10s), 1–41.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim & Yoo, (2017) Kim, J. & Yoo, C. D. (2017). Deep partial person re-identification
    via attention model. In 2017 IEEE International Conference on Image Processing
    (ICIP) (pp. 3425–3429).: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al., (2022) Kim, M., Cho, M., Lee, H., Cho, S., & Lee, S. (2022). Occluded
    person re-identification via relational adaptive feature correction learning.
    In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP) (pp. 2719–2723).: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kiran et al., (2021) Kiran, M., Praveen, R. G., Nguyen-Meidine, L. T., Belharbi,
    S., Blais-Morin, L.-A., & Granger, E. (2021). Holistic guidance for occluded person
    re-identification. arXiv preprint arXiv:2104.06524.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lavi et al., (2020) Lavi, B., Ullah, I., Fatan, M., & Rocha, A. (2020). Survey
    on reliable deep learning-based person re-identification models: Are we there
    yet? arXiv preprint arXiv:2005.00355.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leng et al., (2019) Leng, Q., Ye, M., & Tian, Q. (2019). A survey of open-world
    person re-identification. IEEE Transactions on Circuits and Systems for Video
    Technology, 30(4), 1092–1108.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al., (2014) Li, W., Zhao, R., Xiao, T., & Wang, X. (2014). Deepreid:
    Deep filter pairing neural network for person re-identification. In Proceedings
    of the IEEE conference on computer vision and pattern recognition (pp. 152–159).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al., (2018) Li, W., Zhu, X., & Gong, S. (2018). Harmonious attention network
    for person re-identification. In Proceedings of the IEEE conference on computer
    vision and pattern recognition (pp. 2285–2294).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al., (2020) Li, Y., Jiang, X., & Hwang, J.-N. (2020). Effective person
    re-identification by self-attention model guided feature learning. Knowledge-Based
    Systems, 187, 104832.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al., (2021) Li, Y., Liu, L., Zhu, L., & Zhang, H. (2021). Person re-identification
    based on multi-scale feature learning. Knowledge-Based Systems, 228, 107281.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al., (2019) Li, Y.-J., Chen, Y.-C., Lin, Y.-Y., Du, X., & Wang, Y.-C. F.
    (2019). Recover and identify: A generative dual model for cross-resolution person
    re-identification. In Proceedings of the IEEE/CVF international conference on
    computer vision (pp. 8090–8099).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al., (2018) Liang, X., Gong, K., Shen, X., & Lin, L. (2018). Look
    into person: Joint body parsing & pose estimation network and a new benchmark.
    IEEE transactions on pattern analysis and machine intelligence, 41(4), 871–885.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin & Wang, (2021) Lin, C.-S. & Wang, Y.-C. F. (2021). Self-supervised bodymap-to-appearance
    co-attention for partial person re-identification. In 2021 IEEE International
    Conference on Image Processing (ICIP) (pp. 2299–2303).: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al., (2014) Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
    Ramanan, D., Dollár, P., & Zitnick, C. L. (2014). Microsoft coco: Common objects
    in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland,
    September 6-12, 2014, Proceedings, Part V 13 (pp. 740–755).: Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al., (2017) Liu, H., Feng, J., Jie, Z., Jayashree, K., Zhao, B., Qi,
    M., Jiang, J., & Yan, S. (2017). Neural person search machines. In Proceedings
    of the IEEE International Conference on Computer Vision (pp. 493–501).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al., (2019) Liu, J., Zha, Z.-J., Hong, R., Wang, M., & Zhang, Y. (2019).
    Deep adversarial graph attention convolution network for text-based person search.
    In Proceedings of the 27th ACM International Conference on Multimedia (pp. 665–673).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al., (2022) Liu, Q., Teng, Q., Chen, H., Li, B., & Qing, L. (2022). Dual
    adaptive alignment and partitioning network for visible and infrared cross-modality
    person re-identification. Applied Intelligence, 52(1), 547–563.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al., (2021) Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,
    S., & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using
    shifted windows. In Proceedings of the IEEE/CVF international conference on computer
    vision (pp. 10012–10022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loper et al., (2015) Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., & Black,
    M. J. (2015). Smpl: A skinned multi-person linear model. ACM transactions on graphics
    (TOG), 34(6), 1–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al., (2019) Luo, H., Jiang, W., Gu, Y., Liu, F., Liao, X., Lai, S., &
    Gu, J. (2019). A strong baseline and batch normneuralization neck for deep person
    reidentification. arXiv preprint arXiv:1906.08332.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al., (2021) Ma, Z., Zhao, Y., & Li, J. (2021). Pose-guided inter-and intra-part
    relational transformer for occluded person re-identification. In Proceedings of
    the 29th ACM International Conference on Multimedia (pp. 1487–1496).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mao et al., (2019) Mao, S., Zhang, S., & Yang, M. (2019). Resolution-invariant
    person re-identification. arXiv preprint arXiv:1906.09748.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Martinel et al., (2016) Martinel, N., Das, A., Micheloni, C., & Roy-Chowdhury,
    A. K. (2016). Temporal model adaptation for person re-identification. In Computer
    Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October
    11–14, 2016, Proceedings, Part IV 14 (pp. 858–877).: Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miao et al., (2019) Miao, J., Wu, Y., Liu, P., Ding, Y., & Yang, Y. (2019).
    Pose-guided feature alignment for occluded person re-identification. In Proceedings
    of the IEEE/CVF international conference on computer vision (pp. 542–551).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miao et al., (2021) Miao, J., Wu, Y., & Yang, Y. (2021). Identifying visible
    parts via pose estimation for occluded person re-identification. IEEE transactions
    on neural networks and learning systems, 33(9), 4624–4634.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ming et al., (2022) Ming, Z., Zhu, M., Wang, X., Zhu, J., Cheng, J., Gao, C.,
    Yang, Y., & Wei, X. (2022). Deep learning-based person re-identification methods:
    A survey and outlook of recent works. Image and Vision Computing, 119, 104394.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nagaraju et al., (2016) Nagaraju, G., Raju, G. S. R., Ko, Y. H., & Yu, J. S.
    (2016). Hierarchical ni–co layered double hydroxide nanosheets entrapped on conductive
    textile fibers: a cost-effective and flexible electrode for high-performance pseudocapacitors.
    Nanoscale, 8(2), 812–825.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al., (2017) Nguyen, D. T., Hong, H. G., Kim, K. W., & Park, K. R.
    (2017). Person recognition system based on a combination of body images from visible
    light and thermal cameras. Sensors, 17(3), 605.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ning et al., (2021) Ning, X., Gong, K., Li, W., & Zhang, L. (2021). Jwsaa:
    joint weak saliency and attention aware for person re-identification. Neurocomputing,
    453, 801–811.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (68) Ning, X., Gong, K., Li, W., Zhang, L., Bai, X., & Tian, S. (2020a). Feature
    refinement and filter network for person re-identification. IEEE Transactions
    on Circuits and Systems for Video Technology, 31(9), 3391–3402.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(69) Ning, X., Nan, F., Xu, S., Yu, L., & Zhang, L. (2020b). Multi-view frontal
    face image generation: a survey. Concurrency and Computation: Practice and Experience,
    (pp. e6147).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al., (2021) Park, H., Lee, S., Lee, J., & Ham, B. (2021). Learning
    by aligning: Visible-infrared person re-identification using cross-modal correspondences.
    In Proceedings of the IEEE/CVF international conference on computer vision (pp. 12046–12055).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al., (2022) Peng, Y., Hou, S., Cao, C., Liu, X., Huang, Y., & He, Z.
    (2022). Deep learning-based occluded person re-identification: A survey. arXiv
    preprint arXiv:2207.14452.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al., (2017) Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). Pointnet:
    Deep learning on point sets for 3d classification and segmentation. In Proceedings
    of the IEEE conference on computer vision and pattern recognition (pp. 652–660).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quispe & Pedrini, (2019) Quispe, R. & Pedrini, H. (2019). Improved person re-identification
    based on saliency and semantic parsing with deep neural network models. Image
    and Vision Computing, 92, 103809.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al., (2020) Ren, X., Zhang, D., & Bao, X. (2020). Semantic-guided shared
    feature alignment for occluded person re-identification. In Asian Conference on
    Machine Learning (pp. 17–32).: PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ristani et al., (2016) Ristani, E., Solera, F., Zou, R., Cucchiara, R., & Tomasi,
    C. (2016). Performance measures and a data set for multi-target, multi-camera
    tracking. In Computer Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands,
    October 8-10 and 15-16, 2016, Proceedings, Part II (pp. 17–35).: Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sarfraz et al., (2018) Sarfraz, M. S., Schumann, A., Eberle, A., & Stiefelhagen,
    R. (2018). A pose-sensitive embedding for person re-identification with expanded
    cross neighborhood re-ranking. In Proceedings of the IEEE conference on computer
    vision and pattern recognition (pp. 420–429).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sekhar et al., (2017) Sekhar, S. C., Nagaraju, G., & Yu, J. S. (2017). Conductive
    silver nanowires-fenced carbon cloth fibers-supported layered double hydroxide
    nanosheets as a flexible and binder-free electrode for high-performance asymmetric
    supercapacitors. Nano Energy, 36, 58–67.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shamshad et al., (2023) Shamshad, F., Khan, S., Zamir, S. W., Khan, M. H.,
    Hayat, M., Khan, F. S., & Fu, H. (2023). Transformers in medical imaging: A survey.
    Medical Image Analysis, (pp. 102802).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al., (2017) Su, C., Li, J., Zhang, S., Xing, J., Gao, W., & Tian, Q. (2017).
    Pose-driven deep convolutional model for person re-identification. In Proceedings
    of the IEEE international conference on computer vision (pp. 3960–3969).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (80) Sun, K., Xiao, B., Liu, D., & Wang, J. (2019a). Deep high-resolution representation
    learning for human pose estimation. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition (pp. 5693–5703).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun & Zheng, (2019) Sun, X. & Zheng, L. (2019). Dissecting person re-identification
    from the viewpoint of viewpoint. In Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition (pp. 608–617).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(82) Sun, Y., Xu, Q., Li, Y., Zhang, C., Li, Y., Wang, S., & Sun, J. (2019b).
    Perceive where to focus: Learning visibility-aware part-level features for partial
    person re-identification. In Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition (pp. 393–402).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al., (2018) Sun, Y., Zheng, L., Yang, Y., Tian, Q., & Wang, S. (2018).
    Beyond part models: Person retrieval with refined part pooling (and a strong convolutional
    baseline). In Proceedings of the European conference on computer vision (ECCV)
    (pp. 480–496).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al., (2016) Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., &
    Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In
    Proceedings of the IEEE conference on computer vision and pattern recognition
    (pp. 2818–2826).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al., (2021) Tan, H., Liu, X., Bian, Y., Wang, H., & Yin, B. (2021). Incomplete
    descriptor mining with elastic loss for person re-identification. IEEE Transactions
    on Circuits and Systems for Video Technology, 32(1), 160–171.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(86) Tan, H., Liu, X., Yin, B., & Li, X. (2022a). Mhsa-net: Multihead self-attention
    network for occluded person re-identification. IEEE Transactions on Neural Networks
    and Learning Systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (87) Tan, L., Dai, P., Ji, R., & Wu, Y. (2022b). Dynamic prototype mask for
    occluded person re-identification. In Proceedings of the 30th ACM International
    Conference on Multimedia (pp. 531–540).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tirkolaee et al., (2020) Tirkolaee, E. B., Goli, A., & Weber, G.-W. (2020).
    Fuzzy mathematical programming and self-adaptive artificial fish swarm algorithm
    for just-in-time energy-aware flow shop scheduling problem with outsourcing option.
    IEEE transactions on fuzzy systems, 28(11), 2772–2783.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tutsoy et al., (2018) Tutsoy, O., Barkana, D. E., & Tugal, H. (2018). Design
    of a completely model free adaptive control in the presence of parametric, non-parametric
    uncertainties and random control signal delay. ISA transactions, 76, 67–77.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tutsoy & Tanrikulu, (2022) Tutsoy, O. & Tanrikulu, M. Y. (2022). Priority and
    age specific vaccination algorithm for the pandemic diseases: a comprehensive
    parametric prediction model. BMC Medical Informatics and Decision Making, 22(1),
    4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (91) Wang, C., Ning, X., Sun, L., Zhang, L., Li, W., & Bai, X. (2022a). Learning
    discriminative features by covering local geometric space for point cloud analysis.
    IEEE Transactions on Geoscience and Remote Sensing, 60, 1–15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (92) Wang, C., Wang, C., Li, W., & Wang, H. (2021a). A brief survey on rgb-d
    semantic segmentation using deep learning. Displays, 70, 102080.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(93) Wang, G., Yang, S., Liu, H., Wang, Z., Yang, Y., Wang, S., Yu, G., Zhou,
    E., & Sun, J. (2020a). High-order information matters: Learning relation and topology
    for occluded person re-identification. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition (pp. 6449–6458).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (94) Wang, G., Zhang, T., Cheng, J., Liu, S., Yang, Y., & Hou, Z. (2019a). Rgb-infrared
    cross-modality person re-identification via joint pixel and feature alignment.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 3623–3632).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(95) Wang, J., Tan, S., Zhen, X., Xu, S., Zheng, F., He, Z., & Shao, L. (2021b).
    Deep 3d human pose estimation: A review. Computer Vision and Image Understanding,
    210, 103225.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (96) Wang, P., Ding, C., Shao, Z., Hong, Z., Zhang, S., & Tao, D. (2022b). Quality-aware
    part models for occluded person re-identification. IEEE Transactions on Multimedia.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (97) Wang, Q., Huang, H., Zhong, Y., Min, W., Han, Q., Xu, D., & Xu, C. (2022c).
    Swin transformer based on two-fold loss and background adaptation re-ranking for
    person re-identification. Electronics, 11(13), 1941.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(98) Wang, Q., Qi, M., Jin, K., & Jiang, J. (2020b). Deep-shallow occlusion
    parallelism network for person re-identification. In Journal of Physics: Conference
    Series, volume 1518 (pp. 012026).: IOP Publishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (99) Wang, T., Liu, H., Song, P., Guo, T., & Shi, W. (2022d). Pose-guided feature
    disentangling for occluded person re-identification based on transformer. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 36 (pp. 2540–2549).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (100) Wang, X., Li, C., & Ma, X. (2022e). Cross-modal local shortest path and
    global enhancement for visible-thermal person re-identification. arXiv preprint
    arXiv:2206.04401.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (101) Wang, Y., Liang, X., & Liao, S. (2022f). Cloning outfits from real-world
    images to 3d characters for generalizable person re-identification. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4900–4909).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(102) Wang, Z., Wang, Z., Wu, Y., Wang, J., & Satoh, S. (2019b). Beyond intra-modality
    discrepancy: A comprehensive survey of heterogeneous person re-identification.
    arXiv preprint arXiv:1905.10048, 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (103) Wang, Z., Wang, Z., Zheng, Y., Chuang, Y.-Y., & Satoh, S. (2019c). Learning
    to reduce dual-level discrepancy for infrared-visible person re-identification.
    In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
    (pp. 618–626).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (104) Wang, Z., Zhu, F., Tang, S., Zhao, R., He, L., & Song, J. (2022g). Feature
    erasing and diffusion network for occluded person re-identification. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4754–4763).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al., (2022) Wen, X., Feng, X., Li, P., & Chen, W. (2022). Cross-modality
    collaborative learning identified pedestrian. The Visual Computer, (pp. 1–16).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (106) Wu, A., Zheng, W.-S., & Lai, J.-H. (2017a). Robust depth-based person
    re-identification. IEEE Transactions on Image Processing, 26(6), 2588–2603.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (107) Wu, A., Zheng, W.-S., Yu, H.-X., Gong, S., & Lai, J. (2017b). Rgb-infrared
    cross-modality person re-identification. In Proceedings of the IEEE international
    conference on computer vision (pp. 5380–5389).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al., (2018) Wu, Y., Lin, Y., Dong, X., Yan, Y., Ouyang, W., & Yang, Y.
    (2018). Exploit the unknown gradually: One-shot video-based person re-identification
    by stepwise learning. In Proceedings of the IEEE conference on computer vision
    and pattern recognition (pp. 5177–5186).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al., (2020) Xie, Y., Tian, J., & Zhu, X. X. (2020). Linking points with
    labels in 3d: A review of point cloud semantic segmentation. IEEE Geoscience and
    remote sensing magazine, 8(4), 38–59.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al., (2022) Xu, B., He, L., Liang, J., & Sun, Z. (2022). Learning feature
    recovery transformer for occluded person re-identification. IEEE Transactions
    on Image Processing, 31, 4651–4662.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al., (2018) Xu, J., Zhao, R., Zhu, F., Wang, H., & Ouyang, W. (2018).
    Attention-aware compositional network for person re-identification. In Proceedings
    of the IEEE conference on computer vision and pattern recognition (pp. 2119–2128).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al., (2021) Xu, Y., Zhao, L., & Qin, F. (2021). Dual attention-based method
    for occluded person re-identification. Knowledge-Based Systems, 212, 106554.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yaghoubi et al., (2021) Yaghoubi, E., Kumar, A., & Proença, H. (2021). Sss-pr:
    A short survey of surveys in person re-identification. Pattern Recognition Letters,
    143, 50–57.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al., (2021) Yan, C., Pang, G., Jiao, J., Bai, X., Feng, X., & Shen, C.
    (2021). Occluded person re-identification with single-scale global representations.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 11875–11884).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al., (2022) Yang, J., Zhang, C., Tang, Y., & Li, Z. (2022). Pafm: pose-drive
    attention fusion mechanism for occluded person re-identification. Neural Computing
    and Applications, 34(10), 8241–8252.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al., (2021) Yang, J., Zhang, J., Yu, F., Jiang, X., Zhang, M., Sun,
    X., Chen, Y.-C., & Zheng, W.-S. (2021). Learning to know where to see: A visibility-aware
    approach for occluded person re-identification. In Proceedings of the IEEE/CVF
    international conference on computer vision (pp. 11885–11894).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al., (2020) Yang, W., Yan, Y., Chen, S., Zhang, X., & Wang, H. (2020).
    Multi-scale generative adversarial network for person reidentification under occlusion.
    Journal of Software, 31(7), 1943–1958.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (118) Ye, M., Chen, C., Shen, J., & Shao, L. (2021a). Dynamic tri-level relation
    mining with attentive graph for visible infrared re-identification. IEEE Transactions
    on Information Forensics and Security, 17, 386–398.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al., (2020) Ye, M., Shen, J., J. Crandall, D., Shao, L., & Luo, J. (2020).
    Dynamic dual-attentive aggregation learning for visible-infrared person re-identification.
    In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part XVII 16 (pp. 229–247).: Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(120) Ye, M., Shen, J., Lin, G., Xiang, T., Shao, L., & Hoi, S. C. (2021b).
    Deep learning for person re-identification: A survey and outlook. IEEE transactions
    on pattern analysis and machine intelligence, 44(6), 2872–2893.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhai et al., (2021) Zhai, Y., Han, X., Ma, W., Gou, X., & Xiao, G. (2021).
    Pgmanet: Pose-guided mixed attention network for occluded person re-identification.
    In 2021 International Joint Conference on Neural Networks (IJCNN) (pp. 1–8).:
    IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al., (2021) Zhang, C., Liu, H., Guo, W., & Ye, M. (2021). Multi-scale
    cascading network with compact feature learning for rgb-infrared person re-identification.
    In 2020 25th International Conference on Pattern Recognition (ICPR) (pp. 8679–8686).:
    IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (123) Zhang, G., Chen, C., Chen, Y., Zhang, H., & Zheng, Y. (2022a). Fine-grained-based
    multi-feature fusion for occluded person re-identification. Journal of Visual
    Communication and Image Representation, 87, 103581.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (124) Zhang, L., Guo, H., Zhu, K., Qiao, H., Huang, G., Zhang, S., Zhang, H.,
    Sun, J., & Wang, J. (2022b). Hybrid modality metric learning for visible-infrared
    person re-identification. ACM Transactions on Multimedia Computing, Communications,
    and Applications (TOMM), 18(1s), 1–15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (125) Zhang, Q., Dang, K., Lai, J.-H., Feng, Z., & Xie, X. (2022c). Modeling
    3d layout for group re-identification. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (pp. 7512–7520).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al., (2020) Zhang, X., Yan, Y., Xue, J.-H., Hua, Y., & Wang, H. (2020).
    Semantic-aware occlusion-robust network for occluded person re-identification.
    IEEE Transactions on Circuits and Systems for Video Technology, 31(7), 2764–2778.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang & Lu, (2018) Zhang, Y. & Lu, H. (2018). Deep cross-modal projection learning
    for image-text matching. In Proceedings of the European conference on computer
    vision (ECCV) (pp. 686–701).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al., (2019) Zhang, Z., Lan, C., Zeng, W., & Chen, Z. (2019). Densely
    semantically aligned person re-identification. In Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition (pp. 667–676).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al., (2021) Zhao, C., Lv, X., Dou, S., Zhang, S., Wu, J., & Wang, L.
    (2021). Incremental generative occlusion adversarial suppression network for person
    reid. IEEE Transactions on Image Processing, 30, 4212–4224.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al., (2013) Zhao, R., Ouyang, W., & Wang, X. (2013). Unsupervised salience
    learning for person re-identification. In Proceedings of the IEEE conference on
    computer vision and pattern recognition (pp. 3586–3593).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al., (2020) Zhao, S., Gao, C., Zhang, J., Cheng, H., Han, C., Jiang,
    X., Guo, X., Zheng, W.-S., Sang, N., & Sun, X. (2020). Do not disturb me: Person
    re-identification under the interference of other pedestrians. In Computer Vision–ECCV
    2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part VI 16 (pp. 647–663).: Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al., (2022) Zhao, Y., Zhu, S., Wang, D., & Liang, Z. (2022). Short range
    correlation transformer for occluded person re-identification. Neural computing
    and applications, 34(20), 17633–17645.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al., (2021) Zheng, K., Lan, C., Zeng, W., Liu, J., Zhang, Z., & Zha,
    Z.-J. (2021). Pose-guided feature learning with knowledge distillation for occluded
    person re-identification. In Proceedings of the 29th ACM International Conference
    on Multimedia (pp. 4537–4545).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(134) Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., & Tian, Q. (2015a).
    Scalable person re-identification: A benchmark. In Proceedings of the IEEE international
    conference on computer vision (pp. 1116–1124).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al., (2016) Zheng, L., Yang, Y., & Hauptmann, A. G. (2016). Person
    re-identification: Past, present and future. arXiv preprint arXiv:1610.02984.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al., (2019) Zheng, M., Karanam, S., Wu, Z., & Radke, R. J. (2019).
    Re-identification with consistent attentive siamese networks. In Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition (pp. 5735–5744).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al., (2011) Zheng, W.-S., Gong, S., & Xiang, T. (2011). Person re-identification
    by probabilistic relative distance comparison. In CVPR 2011 (pp. 649–656).: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (138) Zheng, W.-S., Li, X., Xiang, T., Liao, S., Lai, J., & Gong, S. (2015b).
    Partial person re-identification. In Proceedings of the IEEE international conference
    on computer vision (pp. 4678–4686).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al., (2022) Zheng, Z., Wang, X., Zheng, N., & Yang, Y. (2022). Parameter-efficient
    person re-identification in the 3d space. IEEE Transactions on Neural Networks
    and Learning Systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al., (2017) Zheng, Z., Zheng, L., & Yang, Y. (2017). Unlabeled samples
    generated by gan improve the person re-identification baseline in vitro. In Proceedings
    of the IEEE international conference on computer vision (pp. 3754–3762).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (141) Zhong, Y., Wang, X., & Zhang, S. (2020a). Robust partial matching for
    person search in the wild. In Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition (pp. 6827–6835).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (142) Zhong, Z., Zheng, L., Kang, G., Li, S., & Yang, Y. (2020b). Random erasing
    data augmentation. In Proceedings of the AAAI conference on artificial intelligence,
    volume 34 (pp. 13001–13008).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al., (2019) Zhou, K., Yang, Y., Cavallaro, A., & Xiang, T. (2019). Omni-scale
    feature learning for person re-identification. In Proceedings of the IEEE/CVF
    international conference on computer vision (pp. 3702–3712).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al., (2022) Zhou, M., Liu, H., Lv, Z., Hong, W., & Chen, X. (2022).
    Motion-aware transformer for occluded person re-identification. arXiv preprint
    arXiv:2202.04243.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (145) Zhou, Q., Zhong, B., Lan, X., Sun, G., Zhang, Y., Zhang, B., & Ji, R.
    (2020a). Fine-grained spatial alignment model for person re-identification with
    focal triplet loss. IEEE Transactions on Image Processing, 29, 7578–7589.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (146) Zhou, S., Wu, J., Zhang, F., & Sehdev, P. (2020b). Depth occlusion perception
    feature analysis for person re-identification. Pattern Recognition Letters, 138,
    617–623.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al., (2020) Zhu, K., Guo, H., Liu, Z., Tang, M., & Wang, J. (2020).
    Identity-guided human semantic parsing for person re-identification. In Computer
    Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part III 16 (pp. 346–363).: Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al., (2022) Zhu, X., Zheng, M., Chen, X., Zhang, X., Yuan, C., & Zhang,
    F. (2022). Information disentanglement based cross-modal representation learning
    for visible-infrared person re-identification. Multimedia Tools and Applications,
    (pp. 1–27).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuo et al., (2018) Zhuo, J., Chen, Z., Lai, J., & Wang, G. (2018). Occluded
    person re-identification. In 2018 IEEE International Conference on Multimedia
    and Expo (ICME) (pp. 1–6).: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuo et al., (2019) Zhuo, J., Lai, J., & Chen, P. (2019). A novel teacher-student
    learning framework for occluded person re-identification. arXiv preprint arXiv:1907.03253.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
