- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:52:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:52:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2108.04097] Deep Learning for Embodied Visual Navigation Research: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2108.04097] 《深度学习在具身视觉导航研究中的应用：综述》'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2108.04097](https://ar5iv.labs.arxiv.org/html/2108.04097)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2108.04097](https://ar5iv.labs.arxiv.org/html/2108.04097)
- en: 'Deep Learning for Embodied Visual Navigation Research: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《深度学习在具身视觉导航研究中的应用：综述》
- en: Fengda Zhu, Yi Zhu, Vincent CS Lee, Xiaodan Liang and Xiaojun Chang Fengda Zhu
    and Vincent CS Lee are with Department of Data Science and AI, Faculty of Information
    Technology, Monash University. Yi Zhu is with University of Chinese Academy of
    Sciences. Xiaodan Liang is with School of Intelligent Systems Engineering, Sun
    Yat-sen University. Xiaojun Chang is with School of Computing Technologies, RMIT
    University. Manuscript received July 8, 2021.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 朱凤达、朱怡、Vincent CS Lee、梁晓丹和常晓军。朱凤达和Vincent CS Lee 供职于蒙纳士大学信息技术学院数据科学与人工智能系。朱怡供职于中国科学院大学。梁晓丹供职于中山大学智能系统工程学院。常晓军供职于RMIT大学计算技术学院。手稿收到日期：2021年7月8日。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: “Embodied visual navigation” problem requires an agent to navigate in a 3D environment
    mainly rely on its first-person observation. This problem has attracted rising
    attention in recent years due to its wide application in vacuum cleaner, and rescue
    robot, etc. A navigation agent is supposed to have various intelligent skills,
    such as visual perceiving, mapping, planning, exploring and reasoning, etc. Building
    such an agent that observes, thinks, and acts is a key to real intelligence. The
    remarkable learning ability of deep learning methods empowered the agents to accomplish
    embodied visual navigation tasks. Despite this, embodied visual navigation is
    still in its infancy since a lot of advanced skills are required, including perceiving
    partially observed visual input, exploring unseen areas, memorizing and modeling
    seen scenarios, understanding cross-modal instructions, and adapting to a new
    environment, etc. Recently, embodied visual navigation has attracted rising attention
    of the community, and numerous works has been proposed to learn these skills.
    This paper attempts to establish an outline of the current works in the field
    of embodied visual navigation by providing a comprehensive literature survey.
    We summarize the benchmarks and metrics, review different methods, analysis the
    challenges, and highlight the state-of-the-art methods. Finally, we discuss unresolved
    challenges in the field of embodied visual navigation and give promising directions
    in pursuing future research.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: “具身视觉导航”问题要求代理在3D环境中主要依赖其第一人称观察进行导航。由于在吸尘器、救援机器人等广泛应用，这一问题近年来引起了越来越多的关注。一个导航代理应该具备各种智能技能，如视觉感知、建图、规划、探索和推理等。构建一个能够观察、思考和行动的代理是实现真正智能的关键。深度学习方法的卓越学习能力使得代理能够完成具身视觉导航任务。尽管如此，由于需要许多高级技能，包括感知部分观察到的视觉输入、探索未见区域、记忆和建模已见场景、理解跨模态指令以及适应新环境等，具身视觉导航仍处于初期阶段。近年来，具身视觉导航引起了社区的广泛关注，许多研究提出了学习这些技能的方法。本文试图通过提供全面的文献综述，建立该领域当前工作的概述。我们总结了基准和指标，回顾了不同的方法，分析了挑战，并突出了最先进的方法。最后，我们讨论了具身视觉导航领域尚未解决的挑战，并提出了未来研究的有前景方向。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: deep learning, embodied environments, embodied visual navigation, cross-modal
    navigation, navigation robotics.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习、具身环境、具身视觉导航、跨模态导航、导航机器人。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/75a8025d7e6db04837d17de88cd5ba51.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/75a8025d7e6db04837d17de88cd5ba51.png)'
- en: 'Figure 1: A taxonomy of deep learning methods for embodied navigation.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：具身导航的深度学习方法分类。
- en: 'Building a robot to accomplish tasks autonomously in place of humans has been
    a topic researched for a long time [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)].
    Some complex applications, such as vacuum cleaning, disabled helping and rescuing,
    require an agent to navigate to finish several sub-tasks in different places in
    a 3D embodied environment. Therefore, navigation is one of the key capabilities
    in building the intelligent navigable robots in the real world. During the process
    of navigation, a robot needs to move around to find the target location by perceiving
    embodied visual inputs, which is named as “embodied visual navigation”. The agent
    that interacts with the environment through its physical entity within that environment
    is named “embodied agent” [[4](#bib.bib4)]. Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Deep Learning for Embodied Visual Navigation Research: A Survey") demonstrates
    a navigation process. An agent firstly receives an instruction “*Put the chair
    in the living room into the second balcony*”. Then it navigates to find the target
    chair. The agent picks up the chair and navigate to the balcony and put it down.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '构建一个可以自主完成任务的机器人已经是一个研究了很长时间的话题[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]。一些复杂的应用，如吸尘、帮助残疾人和救援，需要一个代理在3D环境中导航以完成不同地点的多个子任务。因此，导航是构建现实世界中智能可导航机器人的关键能力之一。在导航过程中，机器人需要通过感知具体的视觉输入来移动以找到目标位置，这被称为“具体视觉导航”。通过其物理实体与环境互动的代理被称为“具体代理”[[4](#bib.bib4)]。图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Deep Learning for Embodied Visual Navigation Research:
    A Survey")展示了一个导航过程。代理首先接收到指令“*把客厅里的椅子放到第二个阳台上*”。然后，它导航找到目标椅子。代理拿起椅子并导航到阳台上并放下椅子。'
- en: Early works on robotics navigation [[5](#bib.bib5), [6](#bib.bib6)] mainly rely
    on hand-crafted features like optical flow and traditional algorithms like Markov
    localization [[7](#bib.bib7)], incremental localization [[8](#bib.bib8)], or landmark
    tracking [[9](#bib.bib9)]. These methods involve lots of hyper-parameters and
    cannot generalize well in unseen environments. Recent developments of deep learning
    reveal its ability to learn a robust model from large-scale data. Vision robots
    trained by end-to-end deep learning methods is more robust, have less hyper-parameters,
    and have better generalization ability in unseen environments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 早期关于机器人导航的工作[[5](#bib.bib5), [6](#bib.bib6)]主要依赖于手工特征，如光流和传统算法，如马尔可夫定位[[7](#bib.bib7)]、增量定位[[8](#bib.bib8)]或地标跟踪[[9](#bib.bib9)]。这些方法涉及大量的超参数，并且在未见过的环境中无法很好地泛化。深度学习的最新发展揭示了其从大规模数据中学习稳健模型的能力。通过端到端深度学习方法训练的视觉机器人更加稳健，超参数更少，并且在未见过的环境中具有更好的泛化能力。
- en: 'However, some challenges are going to be tackled in achieving deep learning
    for embodied visual navigation: 1) collecting data from the real world is expensive;
    2) the model learned from partial observation is unstable; 3) it is difficult
    to learn the skills for long-term navigation such as exploration and memorization;
    4) perceiving natural language instructions is challenging because natural language
    is diverse with flexible formats; 5) the large domain gap between the simulated
    environment and the real-world environment impedes the adaptation of navigation
    policy, etc.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实现具体视觉导航的深度学习过程中，一些挑战需要解决：1) 从现实世界收集数据很昂贵；2) 从部分观察中学习的模型不稳定；3) 学习长期导航技能如探索和记忆困难；4)
    感知自然语言指令具有挑战性，因为自然语言多样且格式灵活；5) 模拟环境与现实世界环境之间的大领域差距阻碍了导航策略的适应等。
- en: 'This paper discusses the related works in robot navigation and give a promising
    direction in building real-world navigation robots. The structure of this paper
    is shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey"). Training and testing in the real-world
    has many disadvantages: 1) The data sampling efficiency in the real world is very
    low since a real robot can only sample a trajectory at once while a simulator
    can efficiently sample trajectories in multi-processing; 2) the complexity and
    the dynamic of the real-world environment hinges the reproductivity; 3) there
    is large domain bias between different environments, etc. The development of 3D
    simulation technology enables researchers to construct a simulated environment [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)] to study
    in building a robust navigation agent within it. Simulators render these 3D assets
    to generate RGB-D images and provide sensors like physical sensors, GPS sensors
    to simulate a realistic embodied robotic environment. Learning to navigation in
    a simulated environment is a broad field with lots of challenges to solve. In
    solving target-driven navigation problem, researchers propose model-free methods [[15](#bib.bib15),
    [10](#bib.bib10), [16](#bib.bib16)], self-supervised methods [[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19)], planning-based methods [[20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)]. Perceiving natural language is a challenging
    task due to its diversity and complexity. It requires the agents not only can
    following a sentence instruction step-by-step [[12](#bib.bib12), [23](#bib.bib23),
    [24](#bib.bib24)], but also understand dialogues [[20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22)] or navigate to answer questions [[20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22)]. In building a real-world navigation robots, some works [[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)] proposed to train an agent in real-world environments
    directly while other works [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]
    propose to introduce transfer learning to transfer the learned navigation policy
    from simulated environments to the real-world environment.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '本文讨论了机器人导航领域的相关工作，并为构建真实世界的导航机器人提供了有前景的方向。本文的结构如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Deep Learning for Embodied Visual Navigation Research: A Survey")所示。在真实世界中进行训练和测试存在许多缺点：1）由于真实机器人一次只能采样一个轨迹，而模拟器可以高效地在多处理环境中采样轨迹，真实世界的数据采样效率非常低；2）真实世界环境的复杂性和动态性限制了可复制性；3）不同环境之间存在较大的领域偏差等。3D仿真技术的发展使研究人员能够构建一个模拟环境[[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)]，以研究在其中构建稳健导航代理。模拟器渲染这些3D资产以生成RGB-D图像，并提供物理传感器、GPS传感器等来模拟真实的具身机器人环境。在模拟环境中学习导航是一个广泛的领域，有许多挑战需要解决。在解决目标驱动导航问题时，研究人员提出了无模型方法[[15](#bib.bib15),
    [10](#bib.bib10), [16](#bib.bib16)]，自监督方法[[17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19)]，基于规划的方法[[20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)]。感知自然语言是一项具有挑战性的任务，因为其多样性和复杂性。这要求代理不仅能够逐步执行句子指令[[12](#bib.bib12),
    [23](#bib.bib23), [24](#bib.bib24)]，还要理解对话[[20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22)]或导航以回答问题[[20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)]。在构建真实世界导航机器人时，一些工作[[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)]建议直接在真实环境中训练代理，而其他工作[[28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30)]则建议引入迁移学习，将从模拟环境中学习到的导航策略转移到真实环境中。'
- en: '![Refer to caption](img/63afb6d1175851d1d69b31ecbfb89eeb.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/63afb6d1175851d1d69b31ecbfb89eeb.png)'
- en: 'Figure 2: A demonstration of a navigation process, in which a robot move to
    several places to accomplish a task.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：导航过程的演示，其中机器人移动到几个地方以完成任务。
- en: 'Compared with previous surveys of robotic navigation [[6](#bib.bib6), [5](#bib.bib5)],
    our paper focuses on deep learning methods that solve the embodied navigation
    problems:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的机器人导航综述[[6](#bib.bib6), [5](#bib.bib5)]相比，我们的论文专注于解决具身导航问题的深度学习方法：
- en: '1.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: To the best of our knowledge, our paper is the first comprehensive study on
    the advances of deep learning methods on embodied navigation tasks.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，我们的论文是首个全面研究深度学习方法在具身导航任务中进展的工作。
- en: '2.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: This paper summarizes and compares their unique insights of recently proposed
    embodied navigation datasets, simulators and tasks.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文总结并比较了最近提出的具身导航数据集、模拟器和任务的独特见解。
- en: '3.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: This paper introduce deep learning methods for embodied visual navigation, including
    their motivations and contributions.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这篇论文介绍了用于具身视觉导航的深度学习方法，包括它们的动机和贡献。
- en: '4.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: This paper classifies the research results in recent years, and gives some promising
    embodied navigation directions.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文对近年来的研究结果进行了分类，并给出了一些有前景的具身导航方向。
- en: 'The paper is organized as follows. Sec. [2](#S2 "2 Embodied Navigation Environments
    ‣ Deep Learning for Embodied Visual Navigation Research: A Survey") discusses
    the current embodied datasets and embodied simulators. Sec. [3](#S3 "3 Embodied
    Navigation Benchmarks ‣ Deep Learning for Embodied Visual Navigation Research:
    A Survey") introduces the embodied navigation benchmarks including the navigation
    tasks and navigation metrics. Sec. [4](#S4 "4 Methods in Simulated Environments
    ‣ Deep Learning for Embodied Visual Navigation Research: A Survey") lists the
    methods to train an agent navigation in a simulated embodied navigation environment,
    where Sec. [4.1](#S4.SS1 "4.1 Target-driven Navigation ‣ 4 Methods in Simulated
    Environments ‣ Deep Learning for Embodied Visual Navigation Research: A Survey")
    lists the methods for target-driven tasks and Sec. [4.2](#S4.SS2 "4.2 Cross-modal
    Navigation ‣ 4 Methods in Simulated Environments ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey") introduces the methods for cross-modal
    tasks. Sec. [5](#S5 "5 Methods in Real-world Environments ‣ Deep Learning for
    Embodied Visual Navigation Research: A Survey") summarizes the works that builds
    a navigation robot. Sec. [6](#S6 "6 Navigation from Simulator to Real-world ‣
    Deep Learning for Embodied Visual Navigation Research: A Survey") illustrates
    the domain gap betweeen simulated environments and the real-world environment,
    and introduce the methods that solves these challenges. In Sec. [7](#S7 "7 Future
    Directions ‣ Deep Learning for Embodied Visual Navigation Research: A Survey"),
    we highlight the recent state-of-the-art works, discuss about the limitations
    of current works, and propose promising directions in building a real-world navigation
    robot.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的组织结构如下。第[2](#S2 "2 Embodied Navigation Environments ‣ Deep Learning for
    Embodied Visual Navigation Research: A Survey)节讨论了当前的具身数据集和具身模拟器。第[3](#S3 "3 Embodied
    Navigation Benchmarks ‣ Deep Learning for Embodied Visual Navigation Research:
    A Survey")节介绍了具身导航基准，包括导航任务和导航指标。第[4](#S4 "4 Methods in Simulated Environments
    ‣ Deep Learning for Embodied Visual Navigation Research: A Survey")节列出了在模拟具身导航环境中训练代理导航的方法，其中第[4.1](#S4.SS1
    "4.1 Target-driven Navigation ‣ 4 Methods in Simulated Environments ‣ Deep Learning
    for Embodied Visual Navigation Research: A Survey")节列出了面向目标的任务的方法，第[4.2](#S4.SS2
    "4.2 Cross-modal Navigation ‣ 4 Methods in Simulated Environments ‣ Deep Learning
    for Embodied Visual Navigation Research: A Survey")节介绍了跨模态任务的方法。第[5](#S5 "5 Methods
    in Real-world Environments ‣ Deep Learning for Embodied Visual Navigation Research:
    A Survey")节总结了建立导航机器人的工作。第[6](#S6 "6 Navigation from Simulator to Real-world ‣
    Deep Learning for Embodied Visual Navigation Research: A Survey")节阐明了模拟环境与现实环境之间的领域差距，并介绍了解决这些挑战的方法。在第[7](#S7
    "7 Future Directions ‣ Deep Learning for Embodied Visual Navigation Research:
    A Survey")节，我们重点介绍了最近的最先进工作，讨论了当前工作的局限性，并提出了在构建现实世界导航机器人方面的有希望的方向。'
- en: '| Dataset | Year | Scenes | Rooms | Object Catagories | RGB | Depth | 2D Semantics
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 | 场景 | 房间 | 对象类别 | RGB | 深度 | 2D 语义 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Stanford Scene* [[31](#bib.bib31)] | 2012 | 130 | 130 | - | synthetic | ✗
    | ✗ |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Stanford Scene* [[31](#bib.bib31)] | 2012 | 130 | 130 | - | 合成 | ✗ | ✗ |'
- en: '| SceneNet* [[32](#bib.bib32)] | 2016 | 57 | 57 | 218 | synthetic | ✗ | ✓ |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| SceneNet* [[32](#bib.bib32)] | 2016 | 57 | 57 | 218 | 合成 | ✗ | ✓ |'
- en: '| 2D-3D-S* [[33](#bib.bib33)] | 2017 | 270 | 270 | 13 | synthetic | ✓ | ✓ |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 2D-3D-S* [[33](#bib.bib33)] | 2017 | 270 | 270 | 13 | 合成 | ✓ | ✓ |'
- en: '| SUNCG [[34](#bib.bib34)] | 2017 | 45,622 | 775,574 | 84 | synthetic | ✓ |
    ✓ |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| SUNCG [[34](#bib.bib34)] | 2017 | 45,622 | 775,574 | 84 | 合成 | ✓ | ✓ |'
- en: '| CHALET [[35](#bib.bib35)] | 2018 | 10 | 58 | 150 | synthetic | ✗ | ✗ |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| CHALET [[35](#bib.bib35)] | 2018 | 10 | 58 | 150 | 合成 | ✗ | ✗ |'
- en: '| Matterport3D [[36](#bib.bib36)] | 2017 | 90 | 2,056 | 40 | realistic | ✓
    | ✓ |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Matterport3D [[36](#bib.bib36)] | 2017 | 90 | 2,056 | 40 | 现实 | ✓ | ✓ |'
- en: '| Gibson [[14](#bib.bib14)] | 2018 | 572 | 8,854 | 84 | realistic | ✓ | ✓ |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| Gibson [[14](#bib.bib14)] | 2018 | 572 | 8,854 | 84 | 现实 | ✓ | ✓ |'
- en: '| Replica [[37](#bib.bib37)] | 2019 | 18 | 35 | 88 | realistic | ✓ | ✓ |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Replica [[37](#bib.bib37)] | 2019 | 18 | 35 | 88 | 现实 | ✓ | ✓ |'
- en: 'TABLE I: Comparison of existing embodied datasets (*: the datasets render only
    a room as scene).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I：现有具身数据集的比较 (*: 数据集仅渲染一个房间作为场景)。'
- en: '![Refer to caption](img/bec46b8e43bcfc24c975b1cbaae140be.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bec46b8e43bcfc24c975b1cbaae140be.png)'
- en: 'Figure 3: The render scenes of each dataset.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：每个数据集的渲染场景。
- en: 2 Embodied Navigation Environments
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 具身导航环境
- en: Here, we discuss the environments used for embodied navigation. We summarize
    the dataset that provides 3D assets and the simulators that render assets and
    provide interactive interfaces for navigation agents.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论用于具身导航的环境。我们总结了提供3D资产的数据集，以及渲染资产并为导航代理提供交互界面的模拟器。
- en: 2.1 Embodied Datasets
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 具身数据集
- en: 'An embodied dataset contains 3D assets like textures and meshes for rendering
    and other configuration data like object location, object category and camera
    pose for high-level tasks. A comparison of the proposed datasets is shown in Tab. [I](#S1.T1
    "TABLE I ‣ 1 Introduction ‣ Deep Learning for Embodied Visual Navigation Research:
    A Survey").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '具身数据集包含用于渲染的3D资产，如纹理和网格，以及用于高层任务的其他配置数据，如物体位置、物体类别和相机姿态。所提数据集的比较见表格 [I](#S1.T1
    "TABLE I ‣ 1 Introduction ‣ Deep Learning for Embodied Visual Navigation Research:
    A Survey")。'
- en: 'Early works focused on rendering composite RGB views [[31](#bib.bib31)]. It
    trains a probabilistic model to generate synthetic data based on hand-created
    scenes. Later, SceneNet [[32](#bib.bib32)] introduces a generator model to annotate
    2D semantics. As depth channel is proved to be helpful for navigation agents [[19](#bib.bib19),
    [38](#bib.bib38)], 2D-3D-S [[33](#bib.bib33)] provides assets with depth information.
    Different from these works that render a single room at once, later works [[34](#bib.bib34),
    [39](#bib.bib39), [40](#bib.bib40)] provides a large number of scenes consist
    of bedrooms, living rooms, bathrooms, kitchens, etc. However, the synthetic view
    used by the aforementioned datasets is quite different from the real world scene,
    which limits the application of the datasets. To this end, Matterport3D [[36](#bib.bib36)]
    provides photo-realistic panoramic views by 3D reconstruction and 2D and 3D semantics
    of these views. Gibson [[14](#bib.bib14)] provides a more diverse dataset with
    572 houses. Replica [[37](#bib.bib37)] proposes a dataset with 18 indoor scene
    consist of dense meshes and high-resolution textures. Some work such as AI2-THOR [[40](#bib.bib40)],
    RoboTHOR [[39](#bib.bib39)] and CHALET [[35](#bib.bib35)] rely on the datasets
    that not currently released. The rendering scenes of some datasets are shown in
    Fig. [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Deep Learning for Embodied Visual
    Navigation Research: A Survey").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '早期工作集中于渲染复合RGB视图 [[31](#bib.bib31)]。它训练一个概率模型，以基于手工创建的场景生成合成数据。后来，SceneNet [[32](#bib.bib32)]
    引入了一个生成模型来注释2D语义。由于深度通道被证明对导航代理有帮助 [[19](#bib.bib19), [38](#bib.bib38)]，2D-3D-S [[33](#bib.bib33)]
    提供了带有深度信息的资产。与这些一次渲染单个房间的工作不同，后来的工作 [[34](#bib.bib34), [39](#bib.bib39), [40](#bib.bib40)]
    提供了大量场景，包括卧室、客厅、浴室、厨房等。然而，上述数据集使用的合成视图与真实世界场景相差较大，这限制了数据集的应用。为此，Matterport3D [[36](#bib.bib36)]
    通过3D重建提供了照片级真实的全景视图，并提供了这些视图的2D和3D语义。Gibson [[14](#bib.bib14)] 提供了一个包含572栋房屋的更多样化的数据集。Replica [[37](#bib.bib37)]
    提出了一个包含18个室内场景的数据集，具有密集网格和高分辨率纹理。一些工作，如AI2-THOR [[40](#bib.bib40)]、RoboTHOR [[39](#bib.bib39)]
    和CHALET [[35](#bib.bib35)]，依赖于当前未发布的数据集。一些数据集的渲染场景见图 [3](#S1.F3 "Figure 3 ‣ 1
    Introduction ‣ Deep Learning for Embodied Visual Navigation Research: A Survey")。'
- en: 2.2 Embodied Simulators
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 具身模拟器
- en: 'An embodied simulator provides an interface for an agent to interact with the
    environment. We compare different features of the existing simulators in Tab. [II](#S2.T2
    "TABLE II ‣ 2.2 Embodied Simulators ‣ 2 Embodied Navigation Environments ‣ Deep
    Learning for Embodied Visual Navigation Research: A Survey"). A simulators is
    equipped with many sensors, such as a RGB sensor, a depth sensor, a physical sensor
    and a position sensor. Early simulators provide low RGB resolution and unrealistic
    imagery due to the limit of 3D rendering technology. The lack of visual detail,
    limits the navigation performance of the agent. Afterwards, to address this, simulations
    such as Matterport3D simulator [[12](#bib.bib12)], Gibson simulator [[14](#bib.bib14)]
    and Habitat [[13](#bib.bib13)] propose high-resolution photo-realistic panoramic
    view to simulate a more realistic environment. Rendering frame rate is also important
    to embodied simulators since it is critical to training efficiency. MINOS [[11](#bib.bib11)]
    runs over 100 frames per second (FPS), which is 10 times faster than its previous
    works. Habitat [[13](#bib.bib13)] over than 1000 FPS on $512\times 512$ RGB-D
    image, making it become the fastest simulator among existing simulators. Discrete
    state space in [[12](#bib.bib12)] simplifies the navigation problem and makes
    the agent easy to learn complex vision-language navigation tasks. However, continuous
    state space is more welcome since it facilitates transferring a learned agent
    to a real-world robot. A customizable simulator is able to generate more diverse
    data by moving the objects, changing the textures of objects and reconfiguring
    the lights. Diverse data has little bias and therefore, enables the deep learning
    to learn a robust navigation policy. Despite of navigating to find the target
    object in a static room, interacting is another key skill for real-world robots.
    Some complex tasks may require a robot to interact with objects, such as picking
    up a cup, moving a chair, or opening a door. AI2-THOR [[40](#bib.bib40)], iGibson [[41](#bib.bib41)]
    and RoboTHOR [[39](#bib.bib39)] provide interactive environments to train such
    a skill. Multi-agent reinforcement learning [[42](#bib.bib42), [43](#bib.bib43)]
    is an emerging problem of cooperation and competition among agents. AI2-THOR and
    iGibson also support multi-agent training in studying cooperative tasks.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '具身模拟器提供了一个界面，使代理能够与环境互动。我们在表 [II](#S2.T2 "TABLE II ‣ 2.2 Embodied Simulators
    ‣ 2 Embodied Navigation Environments ‣ Deep Learning for Embodied Visual Navigation
    Research: A Survey")中比较了现有模拟器的不同特性。模拟器配备了许多传感器，如 RGB 传感器、深度传感器、物理传感器和位置传感器。早期的模拟器由于3D渲染技术的限制，提供了低
    RGB 分辨率和不真实的图像。视觉细节的缺乏限制了代理的导航性能。随后，为了解决这个问题，Matterport3D 模拟器 [[12](#bib.bib12)]、Gibson
    模拟器 [[14](#bib.bib14)] 和 Habitat [[13](#bib.bib13)] 等模拟器提出了高分辨率的照片真实全景视图，以模拟更真实的环境。渲染帧率对于具身模拟器也很重要，因为它对训练效率至关重要。MINOS [[11](#bib.bib11)]
    的帧率超过了每秒 100 帧（FPS），是其前作的 10 倍。Habitat [[13](#bib.bib13)] 在 $512\times 512$ RGB-D
    图像上超过了 1000 FPS，使其成为现有模拟器中速度最快的。[[12](#bib.bib12)]中的离散状态空间简化了导航问题，使代理更容易学习复杂的视觉-语言导航任务。然而，连续状态空间更受欢迎，因为它有助于将已学习的代理转移到现实世界的机器人上。可定制的模拟器能够通过移动物体、改变物体的纹理和重新配置灯光来生成更多样的数据。多样的数据偏差较小，因此使深度学习能够学习到稳健的导航策略。尽管在静态房间中寻找目标物体，但互动是现实世界机器人另一个关键技能。一些复杂的任务可能需要机器人与物体互动，例如捡起杯子、移动椅子或打开门。AI2-THOR [[40](#bib.bib40)]、iGibson [[41](#bib.bib41)]
    和 RoboTHOR [[39](#bib.bib39)] 提供了互动环境来训练这种技能。多智能体强化学习 [[42](#bib.bib42), [43](#bib.bib43)]
    是一个新兴的合作与竞争问题。AI2-THOR 和 iGibson 也支持多智能体训练以研究合作任务。'
- en: '| Simulator | Year | Use Dataset(s) | Resolution | Physics | FPS | Customizable
    | Interactive | Multi-agent |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 模拟器 | 年份 | 使用数据集 | 分辨率 | 物理 | 帧率 | 可定制 | 互动 | 多智能体 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| MINOS [[11](#bib.bib11)] | 2017 | SUNCG, Matterport3D | $84\times 84$ | ✓
    | 100 | ✗ | ✗ | ✗ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| MINOS [[11](#bib.bib11)] | 2017 | SUNCG, Matterport3D | $84\times 84$ | ✓
    | 100 | ✗ | ✗ | ✗ |'
- en: '| AI2-THOR* [[40](#bib.bib40)] | 2017 | - | $300\times 300$ | ✓ | 120 | ✓ |
    ✓ | ✓ |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| AI2-THOR* [[40](#bib.bib40)] | 2017 | - | $300\times 300$ | ✓ | 120 | ✓ |
    ✓ | ✓ |'
- en: '| House3D [[10](#bib.bib10)] | 2018 | SUNCG | $120\times 90$ | ✗ | 600 | ✓
    | ✗ | ✗ |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| House3D [[10](#bib.bib10)] | 2018 | SUNCG | $120\times 90$ | ✗ | 600 | ✓
    | ✗ | ✗ |'
- en: '| CHALET [[35](#bib.bib35)] | 2018 | CHALET | $800\times 600$ | ✗ | 10 | ✗
    | ✗ | ✗ |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| CHALET [[35](#bib.bib35)] | 2018 | CHALET | $800\times 600$ | ✗ | 10 | ✗
    | ✗ | ✗ |'
- en: '| Matterport3D [[12](#bib.bib12)] | 2018 | Matterport3D | $512\times 512$ |
    ✗ | 1,000 | ✗ | ✗ | ✗ |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Matterport3D [[12](#bib.bib12)] | 2018 | Matterport3D | $512\times 512$ |
    ✗ | 1,000 | ✗ | ✗ | ✗ |'
- en: '| Gibson [[14](#bib.bib14)] | 2018 | Gibson, Matterport3D, 2D-3D-S | $512\times
    512$ | ✓ | 400 | ✗ | ✗ | ✗ |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Gibson [[14](#bib.bib14)] | 2018 | Gibson, Matterport3D, 2D-3D-S | $512\times
    512$ | ✓ | 400 | ✗ | ✗ | ✗ |'
- en: '| iGibson [[41](#bib.bib41)] | 2018 | Gibson | $512\times 512$ | ✓ | 400 |
    ✓ | ✓ | ✓ |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| iGibson [[41](#bib.bib41)] | 2018 | Gibson | $512\times 512$ | ✓ | 400 |
    ✓ | ✓ | ✓ |'
- en: '| Habitat [[13](#bib.bib13)] | 2019 | Matterport3D, Gibson, Replica | $512\times
    512$ | ✓ | 10,000 | ✗ | ✗ | ✗ |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Habitat [[13](#bib.bib13)] | 2019 | Matterport3D, Gibson, Replica | $512\times
    512$ | ✓ | 10,000 | ✗ | ✗ | ✗ |'
- en: '| RoboTHOR* [[39](#bib.bib39)] | 2020 | - | $300\times 300$ | ✓ | 1200 | ✓
    | ✓ | ✗ |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| RoboTHOR* [[39](#bib.bib39)] | 2020 | - | $300\times 300$ | ✓ | 1200 | ✓
    | ✓ | ✗ |'
- en: 'TABLE II: Comparison of existing embodied simulators (*: the dataset that the
    simulator uses is not currently released).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II：现有体现模拟器的比较（*: 模拟器使用的数据集目前尚未发布）。'
- en: 3 Embodied Navigation Benchmarks
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 体现导航基准
- en: 'Here, we introduce several tasks to study the embodied visual navigation problem.
    These tasks can be divided into three categories: target-driven navigation task,
    cross-modal navigation task, and interactive navigation task.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍了几个任务以研究体现的视觉导航问题。这些任务可以分为三类：目标驱动导航任务、跨模态导航任务和互动导航任务。
- en: 3.1 Target-driven Navigation Tasks
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 目标驱动导航任务
- en: PointGoal Navigation, firstly defined by Anderson *et al.* [[44](#bib.bib44)],
    is a task where an agent is initialized to a random starting position and orientation
    then asked to navigate to a target position. The target position is indicated
    by its relative coordinates to the starting position. This task requires an agent
    to estimate the cumulative distance from the starting position so that the agent
    knows how far away from the goal. Theoretically, this task is able to be applied
    to all embodied environments.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: PointGoal 导航，首先由安德森 *等人* [[44](#bib.bib44)] 定义，是一个任务，其中代理从随机起始位置和方向初始化，然后要求导航到一个目标位置。目标位置由其相对于起始位置的相对坐标指示。这个任务要求代理估计从起始位置到目标的累计距离，以便代理知道离目标有多远。从理论上讲，这个任务可以应用于所有体现环境。
- en: ObjectGoal Navigation is proposed by Zhu *et al.* [[15](#bib.bib15)]. In this
    task, an agent is initialized to a random starting position and is required to
    find a specific object, such as a desk or a bed. Once the navigation agent find
    the object, it stops. The navigation process is regarded as a *success* if the
    agent is located within a distance to the target object. In addition to the room
    structure, the *ObjectGoal* navigation task needs the object labels and locations.
    Object recognizing and exploring are key skills to the *ObjectGoal* navigation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ObjectGoal 导航是由朱 *等人* [[15](#bib.bib15)] 提出的。在这个任务中，代理被初始化到一个随机起始位置，并要求找到一个特定的物体，例如桌子或床。代理找到物体后停止。如果代理在目标物体附近，则导航过程被视为
    *成功*。除了房间结构，*ObjectGoal* 导航任务还需要物体标签和位置。物体识别和探索是 *ObjectGoal* 导航的关键技能。
- en: RoomGoal Navigation is proposed by Wu *et al.* [[10](#bib.bib10)]. In this task,
    an agent initialized at a random position is asked to navigate to a room (e.g.
    bedroom or kitchen). The navigation process is regarded as a *success* if the
    agent is stopped within the target room. *RoomGoal* navigation requires the room
    annotations. The concept of the room is a high-level semantic. Therefore, a *RoomGoal*
    navigation agent needs to understand the scene based on the visual details, such
    as furniture type and room layout.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: RoomGoal 导航是由吴 *等人* [[10](#bib.bib10)] 提出的。在这个任务中，代理从随机位置初始化，要求导航到一个房间（例如卧室或厨房）。如果代理停在目标房间内，则导航过程被视为
    *成功*。*RoomGoal* 导航需要房间注释。房间的概念是一种高级语义。因此，*RoomGoal* 导航代理需要根据视觉细节，如家具类型和房间布局，来理解场景。
- en: Multi-Object Navigation (MultiON) Recently, more and more researchers are paying
    attention to long-term navigation where an agent memorize all the visited scenes.
    Motivated by this, Wani *et al.* [[45](#bib.bib45)] propose MultiON, a benchmark
    for Multi-Object Navigation. In MultiON, an agent is asked to navigate to multiple
    target objects one-by-one, which makes the navigation trajectory quite long. The
    agent raise a FOUND action when it reaches the instructed target. Perception and
    effective planning under partial observation would be the key to solve this task.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 多目标导航（MultiON）最近，越来越多的研究者关注于长期导航任务，其中一个智能体需要记住所有访问过的场景。受到这一点的启发，Wani *et al.* [[45](#bib.bib45)]
    提出了 MultiON，一个用于多目标导航的基准。在 MultiON 中，智能体被要求逐一导航到多个目标对象，这使得导航轨迹相当长。当智能体到达指定目标时，会发出
    FOUND 动作。部分观察下的感知和有效规划将是解决此任务的关键。
- en: 3.2 Cross-modal Navigation Tasks
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 跨模态导航任务
- en: Vision-and-Language Navigation (VLN) VLN is a task where an agent navigates
    step-by-step following natural language instructions [[12](#bib.bib12)]. Previous
    tasks such as *ObjectGoal* and *RoomGoal* hard-code the object and room semantics
    as a one-hot vector. On the contrary, VLN introduce natural language sentences
    to instruct the navigation process like *“Head upstairs and walk past the piano
    through an archway directly in front. Turn right when the hallway ends at pictures
    and table. Wait by the moose antlers hanging on the wall”*. The VLN task is successfully
    completed if the agent stops close to the intended goal following the instruction.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉-语言导航（VLN）VLN 是一个任务，要求智能体逐步遵循自然语言指令进行导航 [[12](#bib.bib12)]。以前的任务如 *ObjectGoal*
    和 *RoomGoal* 将对象和房间的语义硬编码为一个独热向量。相反，VLN 引入自然语言句子来指导导航过程，如 *“上楼，经过正前方的钢琴和拱门。走廊尽头遇到图片和桌子时右转。停在墙上悬挂的驼鹿角旁等待”*。如果智能体按照指令停在接近预期目标的位置，则
    VLN 任务成功完成。
- en: 'There are several datasets have been proposed for vision-language navigation:
    R2R [[12](#bib.bib12)], R4R [[46](#bib.bib46)], and RxR [[47](#bib.bib47)]. The
    room-to-room (R2R) dataset is proposed in [[12](#bib.bib12)] to study vision-language
    navigation. The R2R dataset contains 21,567 navigation instructions with an average
    length of 29 words. However, the R2R dataset has several shortcomings: 1) the
    referenced paths are direct-to-goal so that R2R instructions lack the capability
    of describing complex paths; 2) the instruction consists of several sentences
    and not fine-grained; 3) the training data is small, and the model is easily overfitting;
    4) the language of instruction is English only, and no other languages are included.
    To address these problems, more advances datasets have been proposed. Jain *et
    al.* [[46](#bib.bib46)] cross-connects the trajectories and instructions in R2R
    and generate a new dataset named R4R. FGR2R [[48](#bib.bib48)] enriches R2R with
    sub-instructions and their corresponding trajectories. RxR [[47](#bib.bib47)]
    is a time-aligned dataset and it relieves the known biases in trajectories and
    elicits more references to visible entities in R2R.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了几个用于视觉-语言导航的数据集：R2R [[12](#bib.bib12)]、R4R [[46](#bib.bib46)] 和 RxR [[47](#bib.bib47)]。房间到房间（R2R）数据集在 [[12](#bib.bib12)]
    中被提出，用于研究视觉-语言导航。R2R 数据集包含 21,567 条导航指令，平均长度为 29 个单词。然而，R2R 数据集存在几个缺点：1) 参考路径是直接到达目标，因此
    R2R 指令缺乏描述复杂路径的能力；2) 指令由几个句子组成且不够细化；3) 训练数据量小，模型容易过拟合；4) 指令语言仅为英语，不包含其他语言。为了解决这些问题，提出了更多先进的数据集。Jain
    *et al.* [[46](#bib.bib46)] 将 R2R 中的轨迹和指令交叉连接，生成了一个新的数据集，名为 R4R。FGR2R [[48](#bib.bib48)]
    通过子指令及其对应的轨迹丰富了 R2R。RxR [[47](#bib.bib47)] 是一个时间对齐的数据集，它缓解了轨迹中的已知偏差，并引出了更多对 R2R
    中可见实体的引用。
- en: Navigation from Dialog History (NDH) When navigating in an unfamiliar environment,
    a human usually asks for assistance and continued navigation according to the
    responses of other humans. However, building an agent that is able to autonomously
    ask natural language questions and react to the answer is still a long-term goal
    in robotic navigation. In NDH [[49](#bib.bib49)], an agent is required to navigate
    according to a dialog history, which consists of several question-answering pairs.
    Studying NDH is fundamental for building a real-world dialog navigation robot.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从对话历史进行导航（NDH） 在陌生环境中导航时，人类通常会寻求帮助并根据其他人的回答继续导航。然而，构建一个能够自主提出自然语言问题并对回答做出反应的代理仍然是机器人导航的长期目标。在NDH[[49](#bib.bib49)]中，要求代理根据对话历史进行导航，对话历史由多个问答对组成。研究NDH对于构建真实世界的对话导航机器人至关重要。
- en: 'Embodied Questioning and Answering (EQA) Visual Question Answering (VQA) [[50](#bib.bib50)]
    is a cross-modal task, in which a system answers a text-based question with a
    given image. VQA soon became one of the most popular computer vision tasks, because
    it revealed the possibility of interaction between human beings and artificial
    intelligence agents in natural language [[51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53)].
    Compared with VQA, a more advanced activity is to answer questions by self-exploration
    in an unseen environment. Embodied Questioning and Answering (EQA) [[54](#bib.bib54)]
    is a task where an agent is spawned at a random location in a 3D environment and
    asked a question. EQA is a challenging task since it requires a wide range of
    AI skills: visual perception, language understanding, target-driven navigation,
    commonsense reasoning, etc. In addition to navigation accuracy in other tasks,
    EQA propose EQA accuracy to measure whether the agent correctly answers the question
    or not.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 具身问答（EQA） 视觉问答（VQA）[[50](#bib.bib50)] 是一种跨模态任务，其中系统根据给定的图像回答基于文本的问题。VQA很快成为最受欢迎的计算机视觉任务之一，因为它揭示了人类与人工智能代理在自然语言中的交互可能性[[51](#bib.bib51),
    [52](#bib.bib52), [53](#bib.bib53)]。与VQA相比，更先进的活动是通过自我探索在未知环境中回答问题。具身问答（EQA）[[54](#bib.bib54)]
    是一种任务，其中一个代理在3D环境中的随机位置生成并被问到一个问题。EQA是一个具有挑战性的任务，因为它需要广泛的AI技能：视觉感知、语言理解、目标驱动导航、常识推理等。除了其他任务中的导航准确性，EQA还提出了EQA准确性来衡量代理是否正确回答了问题。
- en: REVERIE Recently, Qi *et al.* [[55](#bib.bib55)] proposes Remote Embodied Visual
    referring Expression in Real Indoor Environments, named REVERIE in short, to research
    associating natural language instructions and the visual semantics. Different
    from VLN that gives an instruction that describes the trajectory step-by-step
    toward the target, the natural language instruction in REVERIE refers to a remote
    target object. Compared with *ObjectGoal* navigation, REVERIE offers rich language
    descriptions to enable the agent to find a unique target in the house.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: REVERIE 最近，齐*等人*[[55](#bib.bib55)]提出了在真实室内环境中的远程具身视觉参考表达，简称REVERIE，以研究自然语言指令与视觉语义的关联。与VLN不同，VLN给出的指令是描述目标的逐步轨迹，而REVERIE中的自然语言指令指向一个远程目标对象。与*ObjectGoal*导航相比，REVERIE提供了丰富的语言描述，以帮助代理在房屋中找到唯一的目标。
- en: 'Audio-visual Navigation, proposed by Chen *et al.* [[56](#bib.bib56)] introduces
    audio modality for embodied navigation environment. This task requires the agent
    to navigate to a sound object by seeing and hearing. It encourages researchers
    to study the role of audio plays in navigation. This work also offer the SoundSpaces [[56](#bib.bib56)]
    dataset for the Audio-visual Navigation task. The SoundSpaces dataset is built
    upon two simulators, Replica and Matterport3D. It contains 102 natural sounds
    across a wide variety of categories: bell, door opening, music, people speaking,
    telephone, etc.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 音频视觉导航，由陈*等人*[[56](#bib.bib56)]提出，引入了音频模式以用于具身导航环境。该任务要求代理通过视觉和听觉导航到一个声音对象。这鼓励研究人员研究音频在导航中的作用。这项工作还提供了用于音频视觉导航任务的SoundSpaces[[56](#bib.bib56)]数据集。SoundSpaces数据集建立在两个模拟器上，Replica和Matterport3D。它包含了102种自然声音，涵盖了多种类别：铃声、开门声、音乐、人声、电话铃声等。
- en: '| Metric Name | $\uparrow\downarrow$ | Formulation | PS | SP | UO | SI | OS
    | CC |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 指标名称 | $\uparrow\downarrow$ | 公式 | PS | SP | UO | SI | OS | CC |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Path Length (PL) | - | $\sum_{1\leq i<&#124;P&#124;}d(p_{i},p_{i+1})$ | -
    | ✓ | - | ✗ | ✗ | $O(&#124;P&#124;)$ |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 路径长度 (PL) | - | $\sum_{1\leq i<&#124;P&#124;}d(p_{i},p_{i+1})$ | - | ✓ |
    - | ✗ | ✗ | $O(&#124;P&#124;)$ |'
- en: '| Navigation Error (NE) | $\downarrow$ | $d(p_{&#124;P&#124;},r_{&#124;R&#124;})$
    | ✗ | ✓ | ✗ | ✗ | ✗ | $O(1)$ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 导航误差（NE） | $\downarrow$ | $d(p_{&#124;P&#124;},r_{&#124;R&#124;})$ | ✗ |
    ✓ | ✗ | ✗ | ✗ | $O(1)$'
- en: '| Oracle Navigation Error (ONE) | $\downarrow$ | $\min_{p\in P}d(p,r_{&#124;R&#124;})$
    | ✗ | ✓ | ✗ | ✗ | ✗ | $O(&#124;P&#124;)$ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 预测导航误差（ONE） | $\downarrow$ | $\min_{p\in P}d(p,r_{&#124;R&#124;})$ | ✗ |
    ✓ | ✗ | ✗ | ✗ | $O(&#124;P&#124;)$ |'
- en: '| Success Rate (SR) | $\uparrow$ | $\mathbbm{1}[\text{NE}(P,R)\leq d_{th}]$
    | ✗ | ✗ | ✗ | ✓ | ✗ | $O(1)$ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 成功率（SR） | $\uparrow$ | $\mathbbm{1}[\text{NE}(P,R)\leq d_{th}]$ | ✗ | ✗ |
    ✗ | ✓ | ✗ | $O(1)$ |'
- en: '| Oracle Success Rate (OSR) | $\uparrow$ | $\mathbbm{1}[\text{ONE}(P,R)\leq
    d_{th}]$ | ✗ | ✗ | ✗ | ✓ | ✗ | $O(&#124;P&#124;)$ |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 预测成功率（OSR） | $\uparrow$ | $\mathbbm{1}[\text{ONE}(P,R)\leq d_{th}]$ | ✗ |
    ✗ | ✗ | ✓ | ✗ | $O(&#124;P&#124;)$ |'
- en: '| Success weighted by PL (SPL) | $\uparrow$ | $\text{SR}(P,R)\cdot\dfrac{d(p_{1},r_{&#124;R&#124;})}{\max\{\text{PL}(P),d(p_{1},r_{&#124;R&#124;})\}}$
    | ✗ | ✓ | ✓ | ✓ | ✗ | $O(&#124;P&#124;)$ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 按PL加权的成功率（SPL） | $\uparrow$ | $\text{SR}(P,R)\cdot\dfrac{d(p_{1},r_{&#124;R&#124;})}{\max\{\text{PL}(P),d(p_{1},r_{&#124;R&#124;})\}}$
    | ✗ | ✓ | ✓ | ✓ | ✗ | $O(&#124;P&#124;)$ |'
- en: '| Success weighted by Edit Distance (SED) | $\uparrow$ | $\text{SR}(P,R)\left(1-\dfrac{\text{ED}(P,R)}{\max{\{&#124;P&#124;,&#124;R&#124;}\}-1}\right)$
    | ✓ | ✗ | ✓ | ✓ | ✗ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 按编辑距离加权的成功率（SED） | $\uparrow$ | $\text{SR}(P,R)\left(1-\dfrac{\text{ED}(P,R)}{\max{\{&#124;P&#124;,&#124;R&#124;}\}-1}\right)$
    | ✓ | ✗ | ✓ | ✓ | ✗ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
- en: '| Path Coverage (PC) | $\uparrow$ | $\dfrac{1}{&#124;R&#124;}\sum_{r\in R}\exp\left(-\dfrac{d(r,P)}{d_{th}}\right)$
    | ✓ | ✓ | ✓ | ✓ | ✗ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 路径覆盖（PC） | $\uparrow$ | $\dfrac{1}{&#124;R&#124;}\sum_{r\in R}\exp\left(-\dfrac{d(r,P)}{d_{th}}\right)$
    | ✓ | ✓ | ✓ | ✓ | ✗ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
- en: '| Length Score (LS) | - | $\dfrac{1}{1+&#124;1-\frac{\text{PL}(P)}{\text{PC}(P,R)\cdot\text{PL}(R)}&#124;}$
    | ✗ | ✓ | ✗ | ✗ | ✗ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 长度评分（LS） | - | $\dfrac{1}{1+&#124;1-\frac{\text{PL}(P)}{\text{PC}(P,R)\cdot\text{PL}(R)}&#124;}$
    | ✗ | ✓ | ✗ | ✗ | ✗ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
- en: '| Coverage weighted by LS (CLS) | $\uparrow$ | $\text{PC}(P,R)\cdot\text{LS}(P,R)$
    | ✓ | ✓ | ✓ | ✓ | ✗ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 按LS加权覆盖（CLS） | $\uparrow$ | $\text{PC}(P,R)\cdot\text{LS}(P,R)$ | ✓ | ✓ |
    ✓ | ✓ | ✗ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
- en: '| Normalized Dynamic Time Warping (nDTW) | $\uparrow$ | $\exp\left({-\dfrac{\min\limits_{W\in\mathcal{W}}\sum_{(i_{k},j_{k})\in
    W}d(r_{i_{k}},p_{j_{k}})}{&#124;R&#124;\cdot d_{th}}}\right)$ | ✓ | ✓ | ✓ | ✓
    | ✓ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 标准化动态时间规整（nDTW） | $\uparrow$ | $\exp\left({-\dfrac{\min\limits_{W\in\mathcal{W}}\sum_{(i_{k},j_{k})\in
    W}d(r_{i_{k}},p_{j_{k}})}{&#124;R&#124;\cdot d_{th}}}\right)$ | ✓ | ✓ | ✓ | ✓
    | ✓ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
- en: 'TABLE III: We compare the existing metrics from several aspects, including
    performance ($\uparrow$ indicates the higher the better, $\downarrow$ indicates
    the lower the better), Formulation, Path similarity (PS), Soft Penalties (SP),
    Unique Optimum (UO), Scale Invariance (SI), Order Sensitivity (OS), and Computational
    Complexity (CC). Suppose we have a predicted trajectory $P$ and a ground truth
    trajectory $R$. $p_{i}$ and $r_{i}$ are the ith node on trajectory $P$ and $R$.
    $|P|$ and $|R|$ stand for the length of $P$ and $R$ respectively. The Dijkstra
    distance of the house has been preprocessed and the computation complexity of
    any $d(p_{i},r_{j})$ is $O(1)$.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：我们从多个方面比较现有指标，包括性能（$\uparrow$表示越高越好，$\downarrow$表示越低越好）、公式、路径相似度（PS）、软惩罚（SP）、唯一最优（UO）、尺度不变性（SI）、顺序敏感性（OS）和计算复杂度（CC）。假设我们有一个预测轨迹$P$和一个真实轨迹$R$。$p_{i}$和$r_{i}$分别是轨迹$P$和$R$上的第i个节点。$|P|$和$|R|$分别表示$P$和$R$的长度。房子的Dijkstra距离已预处理，任何$d(p_{i},r_{j})$的计算复杂度为$O(1)$。
- en: Multi-Target Embodied Questioning and Answering (MT-EQA) The natural language
    questions in EQA are simple since each of them describes one object and lacks
    attributes and relationships between multiple targets. In MT-EQA [[57](#bib.bib57)],
    the instructions are like *“Is the dresser in the bedroom bigger than the oven
    in the kitchen”*, where the *dresser* and the *oven* locate in different places
    with different attributes. Thus the agent has to navigate to multiple places,
    find all targets, analyze the relationships between them, and answer the question.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 多目标具身问答（MT-EQA）EQA中的自然语言问题很简单，因为每个问题仅描述一个对象，缺少属性和多个目标之间的关系。在MT-EQA [[57](#bib.bib57)]中，指令类似于*“卧室里的梳妆台是否比厨房里的烤箱大”*，其中*梳妆台*和*烤箱*位于不同的地方，具有不同的属性。因此，智能体必须导航到多个地方，找到所有目标，分析它们之间的关系，并回答问题。
- en: 3.3 Interactive Navigation Tasks
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 交互导航任务
- en: Interactive Questioning and Answering (IQA) Building an agent which is able
    to interact with a dynamic environment is a long-standing goal of the AI community.
    Recently proposed interactive simulators [[41](#bib.bib41), [39](#bib.bib39),
    [40](#bib.bib40)] provide basic functions like opening a door or moving a chair,
    which enables researchers to build an interactive navigation agent. Interactive
    Questioning and Answering (IQA) [[58](#bib.bib58)] asks an agent to answer questions
    by interacting with objects in an environment. IQA contains 76,800 training questions
    that include existence questions, counting questions, spatial relationship questions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式问答（IQA）构建一个能够与动态环境互动的代理是AI社区的长期目标。最近提出的交互式模拟器[[41](#bib.bib41), [39](#bib.bib39),
    [40](#bib.bib40)] 提供了基本功能，如开门或移动椅子，这使研究人员能够构建一个交互式导航代理。交互式问答（IQA）[[58](#bib.bib58)]
    让代理通过与环境中的物体互动来回答问题。IQA包含76,800个训练问题，包括存在问题、计数问题、空间关系问题。
- en: “Help, Anna!” (HANNA) HANNA [[59](#bib.bib59)] is an object-finding task that
    allows an agent to request help from Automatic Natural Navigation Assistants (ANNA)
    when it gets lost. Different from NDH that provides a global dialog history as
    the instruction, the HANNA offers an environment where the instructions dynamically
    change by the situation. The environment creates an interface that enables a human
    to help the agent when it gets lost in testing time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: “帮助，安娜！”（HANNA）HANNA [[59](#bib.bib59)] 是一个对象寻找任务，允许代理在迷路时请求自动自然导航助理（ANNA）的帮助。与提供全球对话历史作为指令的NDH不同，HANNA提供了一个环境，在这个环境中，指令会根据情况动态变化。这个环境创建了一个接口，使得人在测试期间能够帮助代理在迷路时。
- en: 3.4 Evaluation Metrics
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 评估指标
- en: 'Many evaluation metrics have been proposed to evaluate how well a navigation
    agent performs. We divide them into two categories: trajectory-insensitive metrics
    and the trajectory-sensitive metrics. Trajectory-insensitive Metrics Zhu *et al.* [[15](#bib.bib15)]
    use the average number of steps (i.e., average trajectory length) it takes to
    reach a target from a random starting point. However, there is a large proportion
    of trajectories fail when the navigation environment becomes more complex and
    the navigation task becomes more challenging. Later works [[10](#bib.bib10), [60](#bib.bib60),
    [11](#bib.bib11)] introduce propose the Success Rate (SR) to measure frequency
    of the agent successfully reach the goal and other works [[60](#bib.bib60), [54](#bib.bib54)]
    report Navigation Error (NE), the mean distance toward the goal when the agent
    finally stops. Oracle Success Rate (OSR) is proposed to evaluate if the agent
    correctly stops following the oracle stopping rule [[61](#bib.bib61), [12](#bib.bib12)].
    These metrics measure the probability of whether the agent completes the task
    or not, however, fail to measure how much proportion it completes the task.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出许多评估指标来评估导航代理的表现。我们将这些指标分为两类：轨迹无关指标和轨迹相关指标。轨迹无关指标Zhu *et al.* [[15](#bib.bib15)]
    使用从随机起点到达目标所需的平均步数（即，平均轨迹长度）。然而，当导航环境变得更复杂且导航任务变得更具挑战性时，大比例的轨迹会失败。后续工作[[10](#bib.bib10),
    [60](#bib.bib60), [11](#bib.bib11)] 引入了成功率（SR）来衡量代理成功到达目标的频率，其他工作[[60](#bib.bib60),
    [54](#bib.bib54)] 报告了导航误差（NE），即代理最终停止时距离目标的平均距离。提出了Oracle Success Rate（OSR）来评估代理是否按照oracle停止规则正确停止[[61](#bib.bib61),
    [12](#bib.bib12)]。这些指标测量代理完成任务的概率，但未能衡量完成任务的比例。
- en: 'Trajectory-sensitive Metrics Success weighted by Path Length (SPL) is the first
    metric that evaluates both the efficiency and efficacy of a navigation agent,
    and it is regarded as the primary metric in VLN. The SPL ignores the turning actions
    and the agent heading. *Success weighted by edit distance* (SED) [[62](#bib.bib62)]
    takes turning actions into consideration and fix this problem. SED is designed
    for instruction compliance in a graph-based environment, where there exists a
    certain correct path. However, in some tasks like R4R [[46](#bib.bib46)] and R6R [[63](#bib.bib63)],
    the instructed paths are not direct-to-goal. Therefore, it is not appropriate
    to evaluate the navigation performance of the SPL. Therefore, *Coverage weighted
    by Length Score* (CLS) [[46](#bib.bib46)] is proposed to measure the fidelity
    of the agent’s behavior to the described path. CLS is the product of two variables:
    path coverage and length fraction. Ilharco *et al.* absorb the idea of Dynamic
    Time Warpping [[64](#bib.bib64)], an approach widely used in various areas [[65](#bib.bib65),
    [66](#bib.bib66), [67](#bib.bib67)], and propose normalized Dynamic Time Warping
    (nDTW) metric [[68](#bib.bib68)] to evaluate the navigation performance. Similar
    to CLS, nDTW evaluates the distance between the predicted path with the ground-truth
    path. Moreover, nDTW is sensitive to the order of the navigation path while CLS
    is order-invariant. nDTW can be implemented in an efficient dynamic programming
    algorithm. The path-sensitive metrics, like CLS and nDTW, perform better when
    they are used as reward functions than target-oriented reward functions in reinforcement
    learning to navigate [[46](#bib.bib46), [68](#bib.bib68)].'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**轨迹敏感度量** **成功按路径长度加权（SPL）** 是第一个评估导航代理的效率和效果的度量，并被视为VLN中的主要度量。SPL忽略了转向动作和代理方向。*按编辑距离加权的成功*（SED）[[62](#bib.bib62)]考虑了转向动作并解决了这个问题。SED设计用于基于图的环境中的指令遵守，其中存在某条正确路径。然而，在一些任务中，如R4R
    [[46](#bib.bib46)] 和 R6R [[63](#bib.bib63)]，指令路径并不是直接到达目标。因此，使用SPL来评估导航性能是不合适的。因此，提出了*按长度加权的覆盖度得分*（CLS）[[46](#bib.bib46)]来测量代理行为对描述路径的一致性。CLS是两个变量的乘积：路径覆盖率和长度分数。Ilharco
    *等* 吸收了广泛应用于各种领域的动态时间规整[[64](#bib.bib64)]的理念[[65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67)]，并提出了标准化动态时间规整（nDTW）度量[[68](#bib.bib68)]来评估导航性能。与CLS类似，nDTW评估预测路径与真实路径之间的距离。此外，nDTW对导航路径的顺序敏感，而CLS对顺序不敏感。nDTW可以通过高效的动态规划算法实现。路径敏感度量，如CLS和nDTW，在作为奖励函数使用时表现优于目标导向奖励函数，用于强化学习中的导航[[46](#bib.bib46),
    [68](#bib.bib68)]。'
- en: 'Measurements of Metrics Each metric has its unique characteristics according
    to their formulation. We compare the formulation and characteristics of existing
    metrics in Tab. [III](#S3.T3 "TABLE III ‣ 3.2 Cross-modal Navigation Tasks ‣ 3
    Embodied Navigation Benchmarks ‣ Deep Learning for Embodied Visual Navigation
    Research: A Survey"). In this part, we introduce measurements to evaluate the
    functions of a metric:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**度量的测量** 每个度量根据其公式具有独特的特性。我们在表格[III](#S3.T3 "TABLE III ‣ 3.2 Cross-modal Navigation
    Tasks ‣ 3 Embodied Navigation Benchmarks ‣ Deep Learning for Embodied Visual Navigation
    Research: A Survey")中比较了现有度量的公式和特性。在这一部分，我们介绍了评估度量函数的测量方法：'
- en: 1) Path Similarity (PS) characterizes a notion of similarity between the $P$
    and the $R$. This implies that metrics should depend on all nodes in $P$ and all
    nodes in $R$. PS penalizes deviations from the ground truth path, even if they
    lead to the same goal. This is not only prudent, as agents might wander around
    undesired terrain if this is not enforced, but also explicitly gauges the fidelity
    of the predictions with respect to the provided language instructions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 1) **路径相似性（PS）**描述了$P$和$R$之间的相似性概念。这意味着度量应依赖于$P$中的所有节点以及$R$中的所有节点。PS惩罚与真实路径的偏离，即使这些偏离最终达到相同的目标。这不仅是谨慎的，因为如果不加以强制，代理可能会在不希望的地形上游荡，而且还明确衡量了预测与所提供语言指令的一致性。
- en: 2) Soft Penalties (SP) penalizes differences from the ground truth path according
    to a soft notion of dissimilarity that depends on distances in the graph. This
    ensures that larger discrepancies are penalized more severely than smaller ones
    and that SP should not rely only on dichotomous views of intersection.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 2) **软惩罚（SP）**根据图中的距离以一种软性不相似的方式惩罚与真实路径的差异。这确保了较大的偏差会比较小的偏差受到更严厉的惩罚，并且SP不应仅依赖于交叉点的二分观点。
- en: '3) Unique Optimum (UO) yields a perfect score if and only if the reference
    and predicted paths are an exact match. This ensures that the perfect score is
    unambiguous: the reference path $R$ is therefore treated as a golden standard.
    No other path should have the same or higher score as the reference path itself.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 唯一最优（UO）仅当参考路径和预测路径完全匹配时才会产生完美得分。这确保了完美得分是明确无误的：参考路径$R$因此被视为黄金标准。没有其他路径应该有与参考路径相同或更高的得分。
- en: 4) Scale Invariance (SI) measures if a metric is independent over different
    datasets. If a metric variants over datasets, such as navigation error, its scores
    across different datasets cannot be directly compared.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 尺度不变性（SI）衡量一个度量在不同数据集上是否独立。如果度量在数据集上变异，例如导航误差，则其在不同数据集上的得分不能直接比较。
- en: 5) Order Sensitive (OS) indicates if a metric is sensitive to the navigation
    order with the same trajectory length, success rate, etc. The navigation order
    reveals some sorts of navigation policy even though it is usually hard to be evaluated.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 5) 顺序敏感性（OS）指示一个度量是否对具有相同轨迹长度、成功率等的导航顺序敏感。导航顺序揭示了一些导航策略，尽管通常很难评估。
- en: 6) Computational Complexity (CC) measures the cost of computing a pair of $(P,R)$.
    It is important to design a fast algorithm to calculate the score for automatic
    validation and testing.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 6) 计算复杂度（CC）衡量计算一对$(P,R)$的成本。设计一个快速算法来计算自动验证和测试的得分是重要的。
- en: 3.5 Summary
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 总结
- en: Embodied navigation benchmarks define the tasks and metrics for different settings.
    The target-oriented tasks like *PointGoal*, *ObjectGoal*, and *RoomGoal* Navigation
    can provide label by the 3D assets and do not require extra human annotation.
    cross-modal navigation tasks like R2R [[12](#bib.bib12)], Visual Dialogue Navigation [[49](#bib.bib49)]
    or REVERIE [[55](#bib.bib55)] require human to label the trajectory and the corresponding
    language description. The interactive interactive tasks [[59](#bib.bib59), [41](#bib.bib41)]
    require the agent to learn to manipulate objects, which attract rising attention
    due to their wide application in real-world scenarios.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 具身导航基准定义了不同设置下的任务和指标。像*PointGoal*、*ObjectGoal*和*RoomGoal*导航这样的目标导向任务可以通过3D资产提供标签，不需要额外的人为注释。跨模态导航任务，如R2R
    [[12](#bib.bib12)]、Visual Dialogue Navigation [[49](#bib.bib49)] 或 REVERIE [[55](#bib.bib55)]
    需要人工标注轨迹和相应的语言描述。交互式任务 [[59](#bib.bib59), [41](#bib.bib41)] 需要代理学习操作对象，由于其在现实场景中的广泛应用，引起了越来越多的关注。
- en: 4 Methods in Simulated Environments
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 模拟环境中的方法
- en: 'In this section, we mainly discuss two problems in the simulated environments:
    target-driven navigation and cross-modal navigation. And we introduce the methods
    to solve these problems.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们主要讨论模拟环境中的两个问题：目标驱动导航和跨模态导航。并介绍了解决这些问题的方法。
- en: 4.1 Target-driven Navigation
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 目标驱动导航
- en: 'Methods for this problem focus on navigating from a random starting position
    to a target. The target may be specified by an RGB image, a vector, or a word.
    The agent predict actions like *turn left*, *turn right*, *move forward* to navigate
    in the embodied environment and predict *stop* indicate the stop action. There
    are diverse methods that try to solve this problem, including: 1) model-free methods;
    2) planning-based methods; and 3) self-supervised methods.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这个问题的方法主要集中在从随机起始位置导航到目标。目标可以由RGB图像、向量或单词指定。代理预测诸如*向左转*、*向右转*、*向前移动*等动作，以在具身环境中导航，并预测*停止*表示停止动作。解决此问题的方法多种多样，包括：1)
    无模型方法；2) 基于规划的方法；和3) 自监督方法。
- en: 4.1.1 Model-free Methods
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 无模型方法
- en: 'The model-free methods learn to navigate end-to-end without modeling the environment,
    as illustrated in the Fig. [4](#S4.F4 "Figure 4 ‣ 4.1.1 Model-free Methods ‣ 4.1
    Target-driven Navigation ‣ 4 Methods in Simulated Environments ‣ Deep Learning
    for Embodied Visual Navigation Research: A Survey"). The learning objective includes
    imitation learning or reinforcement learning. The formulation of the learning
    object is:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 无模型方法通过端到端学习来导航，而不对环境进行建模，如图[4](#S4.F4 "图 4 ‣ 4.1.1 无模型方法 ‣ 4.1 目标驱动导航 ‣ 4 模拟环境中的方法
    ‣ 具身视觉导航深度学习研究综述")所示。学习目标包括模仿学习或强化学习。学习目标的公式为：
- en: '|  | $\mathcal{L}=\sum_{t}-a_{t}^{*}log\left(p_{t}\right)-\sum_{t}a_{t}log\left(p_{t}\right)A_{t},$
    |  | (1) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\sum_{t}-a_{t}^{*}log\left(p_{t}\right)-\sum_{t}a_{t}log\left(p_{t}\right)A_{t},$
    |  | (1) |'
- en: where $a^{*}$ is the ground truth action, $p_{t}$ is the action probability,
    and $A_{t}$ is the advantage in A3C [[69](#bib.bib69)]. Though extensive reinforcement
    learning works [[70](#bib.bib70), [71](#bib.bib71), [61](#bib.bib61)] have long
    studied 2D navigation problem where an agent receives global state for each step,
    the embodied navigation problem with partial observation remains challenging.
    Many robot control works [[72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74),
    [75](#bib.bib75)] focus on obstacle avoidance rather than trajectory planning.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a^{*}$ 是真实的动作，$p_{t}$ 是动作的概率，而 $A_{t}$ 是 A3C 中的优势[[69](#bib.bib69)]。虽然广泛的强化学习研究[[70](#bib.bib70),
    [71](#bib.bib71), [61](#bib.bib61)] 长期以来一直研究代理在每一步接收全局状态的二维导航问题，但具有部分观察的体态导航问题仍然具有挑战性。许多机器人控制的研究[[72](#bib.bib72),
    [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75)] 更关注于避障而非轨迹规划。
- en: '![Refer to caption](img/7f0b7cc4c9ce53ba3da33ee33e8e7135.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7f0b7cc4c9ce53ba3da33ee33e8e7135.png)'
- en: 'Figure 4: An illustration of an model-free visual navigation model. This model
    learned from imitation learning and reinforcement learning. $r_{t}$ is the reward
    and $f(s_{t})$ stands for the labels calculated from the state $s_{t}$. And $a^{\prime}$
    is the label stands for the optimal action.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：无模型视觉导航模型的示意图。该模型通过模仿学习和强化学习进行学习。$r_{t}$ 是奖励，$f(s_{t})$ 表示从状态 $s_{t}$ 计算的标签。而
    $a^{\prime}$ 是代表最佳动作的标签。
- en: Zhu *et al.* [[15](#bib.bib15)] firstly propose to use deep learning for feature
    matching and deep reinforcement learning for policy prediction, which allows the
    agent to better generalize to unseen environments. Afterwards, Successor Representation
    (SR) [[76](#bib.bib76)] is proposed to enable the agent to interact with objects.
    This framework takes the states of objects and a discrete description of the scene
    into consideration. Successor Representation encodes semantic information and
    concatenate it with the visual representation as in [[15](#bib.bib15)]. Different
    from [[15](#bib.bib15)] that only uses reinforcement learning to learn a policy
    predictor, Successor Representation model that bootstraps reinforcement learning
    with imitation learning. Previous models lack of the ability of encoding temporal
    information. By introducing an LSTM layer to encode historical information, Wu
    *et al.* [[10](#bib.bib10)] are able to build an agent that is able to generalize
    to unseen scenarios. In the ablation study, this work proves that A3C [[69](#bib.bib69)]
    outperforms DDPG [[77](#bib.bib77)] in visual navigation task, and the model learned
    from semantic mask outperforms which learned from RGB inputs. Inspite of solving
    visual navigation problem via on-policy deep reinforcement learning algorithms,
    some works adopt other algorithms. Li *et al.* [[78](#bib.bib78)] propose an end-to-end
    model based on Q-learning that learns viewpoint invariant and target on invariant
    visual servoing for local mobile robot navigation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Zhu *等* [[15](#bib.bib15)] 首先提出使用深度学习进行特征匹配和深度强化学习进行策略预测，这使得代理能够更好地在未见过的环境中进行泛化。随后，提出了后继表示
    (SR) [[76](#bib.bib76)] 以使代理能够与对象互动。该框架考虑了对象的状态和场景的离散描述。后继表示编码了语义信息，并将其与视觉表示连接起来，如[[15](#bib.bib15)]
    中所述。与仅使用强化学习来学习策略预测器的[[15](#bib.bib15)]不同，后继表示模型通过模仿学习引导强化学习。以前的模型缺乏编码时间信息的能力。通过引入
    LSTM 层以编码历史信息，Wu *等* [[10](#bib.bib10)] 能够构建一个能够在未见过的场景中进行泛化的代理。在消融研究中，该工作证明了
    A3C [[69](#bib.bib69)] 在视觉导航任务中优于 DDPG [[77](#bib.bib77)]，并且从语义掩码中学到的模型优于从 RGB
    输入中学到的模型。尽管通过策略深度强化学习算法解决视觉导航问题，但一些工作采用了其他算法。Li *等* [[78](#bib.bib78)] 提出了一个基于
    Q 学习的端到端模型，该模型学习视角不变和目标不变的视觉伺服，以进行本地移动机器人导航。
- en: There are lots of works use segmentation masks of objects to augment visual
    inputs. Mousavian *et al.* [[16](#bib.bib16)] exploit the instance features in
    the vision inputs by introducing Faster-RCNN detector trained on MSCOCO dataset [[79](#bib.bib79)]
    and a segmenter defined by [[80](#bib.bib80)] to detect and segment objects. Shen
    *et al.* [[81](#bib.bib81)] improve zero-shot generalization of a navigation agent
    by fusing diverse visual representations, including RGB features, depth features,
    segmentation features, detection features, etc. The different visual representations
    are adaptively weighted for fusing. To further improve the robustness, they propose
    a inter-task affinity regularization that encourages the agent to select more
    complementary and less redundant representations to fuse. Despite the well-performed
    detector and segmenter, learning a robust navigation policy is still challenging.
    For example, to search for mugs, a human would search cabinets near the coffee
    machine and for fruits a human may try the fridge first. To address this, Lv *et
    al.* [[82](#bib.bib82)] integrate 3D knowledge graph and sub-targets into deep
    reinforcement learning framework. To enhance the cross-scene generalization, Wu
    *et al.* [[83](#bib.bib83)] introduce an information theoretic regularization
    term into the RL objective and models the action-observation dynamics by learning
    a variational generative model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究使用对象的分割掩膜来增强视觉输入。Mousavian *et al.* [[16](#bib.bib16)] 通过引入在 MSCOCO 数据集
    [[79](#bib.bib79)] 上训练的 Faster-RCNN 检测器和 [[80](#bib.bib80)] 定义的分割器来利用视觉输入中的实例特征，以检测和分割对象。Shen
    *et al.* [[81](#bib.bib81)] 通过融合多样的视觉表示（包括 RGB 特征、深度特征、分割特征、检测特征等）来提高导航代理的零样本泛化能力。不同的视觉表示被自适应加权以进行融合。为了进一步提高鲁棒性，他们提出了一种任务间亲和性正则化，鼓励代理选择更多互补且冗余较少的表示进行融合。尽管检测器和分割器表现良好，学习一个鲁棒的导航策略仍然具有挑战性。例如，寻找杯子时，人们会在咖啡机附近搜索柜子，而寻找水果时，人们可能会首先检查冰箱。为了解决这个问题，Lv
    *et al.* [[82](#bib.bib82)] 将 3D 知识图谱和子目标集成到深度强化学习框架中。为了增强跨场景泛化，Wu *et al.* [[83](#bib.bib83)]
    将信息论正则化项引入 RL 目标，并通过学习变分生成模型来建模动作-观察动态。
- en: Some works investigate problem settings other than indoor navigation, such as
    street view navigation or combining other modalities. Khosla *et al.* [[84](#bib.bib84)]
    firstly attempt to solve outdoor street navigation task by embodied visual navigation
    method ,where the agent navigate purely based on panoramic street views. DeepNav [[85](#bib.bib85)]
    is build upon a Convolutional Neural Network (CNN) for navigating in large cities
    using locally visible street-view images. These works rely on supervised training
    with the ground truth compass input , however, the compass can sometimes be unavailable
    in real-world. Another work [[86](#bib.bib86)] propose an end-to-end deep reinforcement
    learning framework that uses the street scenes from Google Street View as visual
    input but without the ground truth compass. Recognizing the importance of locale-specific
    knowledge to navigation, they propose a dual pathway architecture that allows
    locale-specific features to be encapsulated. AV-WaN [[87](#bib.bib87)] is proposed
    to tackle the challenges in Audio-visual Navigation. This model learns the audio-visual
    waypoints and dynamically sets intermediate goal locations based on its audio-visual
    observations and partial maps in an end-to-end manner.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究探讨了室外导航以外的问题设置，例如街景导航或结合其他模态。Khosla *et al.* [[84](#bib.bib84)] 首次尝试通过具身视觉导航方法解决户外街道导航任务，其中代理纯粹依靠全景街景进行导航。DeepNav
    [[85](#bib.bib85)] 基于卷积神经网络（CNN），使用局部可见的街景图像在大城市中进行导航。这些研究依赖于带有真实方向输入的监督训练，但在现实世界中，指南针有时可能不可用。另一个研究
    [[86](#bib.bib86)] 提出了一个端到端的深度强化学习框架，使用来自 Google 街景的街景场景作为视觉输入，但没有真实的指南针。认识到地方特定知识对导航的重要性，他们提出了一种双通道架构，允许地方特定特征被封装。AV-WaN
    [[87](#bib.bib87)] 被提出以应对音频-视觉导航中的挑战。该模型以音频-视觉路径点为基础，基于其音频-视觉观察和部分地图动态设置中间目标位置，以端到端的方式进行学习。
- en: '![Refer to caption](img/368ed74b49049d5504093b393c0992f0.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/368ed74b49049d5504093b393c0992f0.png)'
- en: 'Figure 5: An illustration of an end-to-end visual navigation model with self-supervised
    objectives. $r_{t}$ is the reward and $f(s_{t})$ stands for the labels calculated
    from the state $s_{t}$.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：端到端视觉导航模型的自监督目标示意图。$r_{t}$ 是奖励，$f(s_{t})$ 代表从状态 $s_{t}$ 计算出的标签。
- en: 4.1.2 Self-Supervised Methods
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 自监督方法
- en: Self-supervised learning is a long studied topic of exploiting extra training
    signals via various pretext tasks. It enables an agent to learn more knowledge
    without additional human annotations. Various self-supervised tasks have been
    proposed in the field of deep learning, such as context prediction [[88](#bib.bib88)],
    solving jigsaw puzzles [[89](#bib.bib89)], colorization [[90](#bib.bib90)], rotation [[91](#bib.bib91)].
    There is also some auxiliary tasks proposed to improve data efficiency and generalization
    in reinforcement learning. Xie *et al.* [[17](#bib.bib17)] combines self-supervised
    learning with model-based reinforcement learning to solve robotic tasks. Motivated
    by traditional UVFA architecture [[92](#bib.bib92)] which learns a value function
    by means of feature learning, Jaderberg *et al.* [[18](#bib.bib18)] invent auxiliary
    control and reward prediction tasks that dramatically improve both data efficiency
    and robustness.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习是利用各种前置任务进行额外训练信号的长期研究课题。它使得智能体能够在没有额外人工注释的情况下学习更多知识。在深度学习领域，已经提出了各种自监督任务，例如上下文预测 [[88](#bib.bib88)]、解决拼图难题 [[89](#bib.bib89)]、颜色化 [[90](#bib.bib90)]、旋转 [[91](#bib.bib91)]。在强化学习中，也提出了一些辅助任务，以提高数据效率和泛化能力。Xie
    *et al.* [[17](#bib.bib17)] 将自监督学习与基于模型的强化学习相结合，以解决机器人任务。受传统UVFA架构的启发 [[92](#bib.bib92)]，该架构通过特征学习来学习价值函数，Jaderberg
    *et al.* [[18](#bib.bib18)] 发明了辅助控制和奖励预测任务，显著提高了数据效率和鲁棒性。
- en: 'In embodied navigation, the environment contains unstructured semantic information
    that is hard to learn in end-to-end manner. In spite of explicitly modeling the
    environment using SLAM or memory mechanism, self-supervised learning provide another
    feasible way of learning the unstructured knowledge. Mirowski *et al.* [[19](#bib.bib19)]
    propose an online navigation model with two self-supervised auxiliary objectives,
    predicting the current depth view by RGB view and detecting the loop closure.
    Similiar idea [[93](#bib.bib93)] has been applied in game applications [[94](#bib.bib94)]
    for rapid exploration. Auxiliary tasks also can speed up learning. Ye *et al.* [[95](#bib.bib95),
    [96](#bib.bib96)] achieve great success in *PointGoal* and *ObjectGoal* navigation
    by assembling reinforcement learning with various kinds of auxiliary tasks, formulated
    as:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在具身导航中，环境包含了结构不规范的语义信息，这在端到端方式中很难学习。尽管可以使用SLAM或记忆机制显式建模环境，自监督学习提供了另一种学习这些结构不规范知识的可行方法。Mirowski
    *et al.* [[19](#bib.bib19)] 提出了一个在线导航模型，具有两个自监督辅助目标：通过RGB视图预测当前深度视图和检测回环闭合。类似的思想 [[93](#bib.bib93)]
    已经应用于游戏应用 [[94](#bib.bib94)] 中，以实现快速探索。辅助任务还可以加速学习。Ye *et al.* [[95](#bib.bib95),
    [96](#bib.bib96)] 通过将强化学习与各种辅助任务相结合，在 *PointGoal* 和 *ObjectGoal* 导航中取得了巨大的成功，其形式为：
- en: '|  | $L_{total}=L_{RL}+\sum_{i}^{n}\beta_{i}L_{Aux,i}.$ |  | (2) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{total}=L_{RL}+\sum_{i}^{n}\beta_{i}L_{Aux,i}.$ |  | (2) |'
- en: 'Visual perception is critical for visual navigation. But the training signal
    provided by reinforcement learning contain too much noise to train a robust feature
    perception network. An encoder-decoder architecture is proved to be beneficial [[97](#bib.bib97)]
    in visual encoding and segmentation predicting. In addition, an auxiliary task
    is used to penalize the segmentation error, which benefits the learning of feature
    perception. However, this self-supervised auxiliary task only learns the low-level
    dynamic function between two adjacent states and fails to learn the high-level
    semantic information. To guarantee the semantic consistency of actions in a trajectory,
    Liu *et al.* [[98](#bib.bib98)] propose an auxiliary regularization task to penalizes
    the inconsistency of representations. This regularization task encourages the
    policy network to extract salient features from each sensor. Real-world robot
    locomotion is far from deterministic due to the presence of actuation noise, which
    might be caused by wheels slipping, motion sensor error, rebound, etc. To reduce
    the noise, Datta *et al.* [[99](#bib.bib99)] introduce an auxiliary task of localization
    estimation by means of temporal difference. The auxiliary task is used to train
    a CNN network and use the estimated locomotion as an input of the policy network.
    A curiosity-driven self-supervised objective [[100](#bib.bib100)] is applied to
    encourage exploration while penalizing the repeating actions. A stable curiosity-driven
    policy without repeating actions could improve the exploration efficiency. Self-supervised
    auxiliary tasks are also helpful in cross-modal understanding for navigation.
    Dean *et al.* [[101](#bib.bib101)] use audio as an additional modality for self-supervised
    exploration. It includes an curiosity driven intrinsic reward, which encourages
    the agent to explore novel associations between different sensory modalities (audio
    and visual). An overview of the pipeline of self-supervised navigation methods
    is shown in Fig. [5](#S4.F5 "Figure 5 ‣ 4.1.1 Model-free Methods ‣ 4.1 Target-driven
    Navigation ‣ 4 Methods in Simulated Environments ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey"). The agent firstly embeds a visual image
    and an instruction as features. Then the visual feature and the instruction feature
    are fused to predict the action. The auxiliary tasks use the fused feature to
    make a prediction, such as predicting the reward, or reconstructing the input
    visual image.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '视觉感知对于视觉导航至关重要。但强化学习提供的训练信号含有过多噪声，难以训练出一个稳健的特征感知网络。编码器-解码器架构被证明在视觉编码和分割预测中是有益的[[97](#bib.bib97)]。此外，采用辅助任务来惩罚分割误差，这有助于特征感知的学习。然而，这种自监督辅助任务仅学习了两个相邻状态之间的低级动态功能，未能学习高级语义信息。为了保证轨迹中动作的语义一致性，刘*等*[[98](#bib.bib98)]提出了一种辅助正则化任务来惩罚表示的不一致性。这个正则化任务鼓励策略网络从每个传感器中提取显著特征。由于存在驱动噪声（如轮胎打滑、运动传感器误差、反弹等），现实世界的机器人运动远非确定性。为了减少噪声，Datta
    *等*[[99](#bib.bib99)]通过时间差分引入了一种定位估计的辅助任务。该辅助任务用于训练CNN网络，并将估计的运动作为策略网络的输入。一个好奇心驱动的自监督目标[[100](#bib.bib100)]被应用于鼓励探索，同时惩罚重复动作。一个稳定的好奇心驱动的策略在没有重复动作的情况下可以提高探索效率。自监督辅助任务在导航的跨模态理解中也很有帮助。Dean
    *等*[[101](#bib.bib101)]使用音频作为自监督探索的额外模态。它包括一个好奇心驱动的内在奖励，鼓励智能体探索不同感官模态（音频和视觉）之间的新关联。自监督导航方法的流程概述见图[5](#S4.F5
    "Figure 5 ‣ 4.1.1 Model-free Methods ‣ 4.1 Target-driven Navigation ‣ 4 Methods
    in Simulated Environments ‣ Deep Learning for Embodied Visual Navigation Research:
    A Survey")。智能体首先将视觉图像和指令嵌入为特征。然后将视觉特征和指令特征融合以预测动作。辅助任务使用融合特征进行预测，如预测奖励或重建输入的视觉图像。'
- en: '![Refer to caption](img/99fa91ad020b79cc5bec012f2e97493b.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/99fa91ad020b79cc5bec012f2e97493b.png)'
- en: 'Figure 6: An overview of the common practice of the “Neural SLAM”-based model.
    “ST” is the spatial transformation function.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '图6: “神经SLAM”模型的常见实践概述。“ST”是空间变换函数。'
- en: 4.1.3 Planning-based Methods
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 基于规划的方法
- en: The map building problem for an unknown environment while solving the localization
    problem at the same time is known as Simultaneous Localization and Mapping (SLAM) [[102](#bib.bib102),
    [103](#bib.bib103)]. The earlier investigations on visual navigation were carried
    out with a stereo camera [[104](#bib.bib104), [105](#bib.bib105)] and a monocular
    camera, such as MonoSLAM [[106](#bib.bib106)]. Over the past decade, traditional
    geometric-based approaches [[107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109)]
    remains dominating the field. With the development of deep learning, some methods
    like CNN-SLAM [[110](#bib.bib110)], DVO [[111](#bib.bib111)] and D3VO [[112](#bib.bib112)]
    are proposed. Some indoor tasks are proposed to study SLAM, such as KITTI [[113](#bib.bib113)]
    and EuRoC [[114](#bib.bib114)]. However, these tasks are different from embodied
    navigation task. The odometry benchmark is to estimate the location given a sequence
    of visual inputs while the navigation task is to align the instruction with the
    environment semantics.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决定位问题的同时对未知环境进行地图构建的问题被称为**同时定位与地图构建**（SLAM）[[102](#bib.bib102), [103](#bib.bib103)]。早期的视觉导航研究使用了立体摄像机[[104](#bib.bib104),
    [105](#bib.bib105)]和单目摄像机，如MonoSLAM[[106](#bib.bib106)]。在过去的十年里，传统的几何基础方法[[107](#bib.bib107),
    [108](#bib.bib108), [109](#bib.bib109)]仍然主导了这一领域。随着深度学习的发展，提出了一些方法，如CNN-SLAM[[110](#bib.bib110)]、DVO[[111](#bib.bib111)]和D3VO[[112](#bib.bib112)]。一些室内任务被提出用于研究SLAM，例如KITTI[[113](#bib.bib113)]和EuRoC[[114](#bib.bib114)]。然而，这些任务与体现导航任务不同。里程计基准是估计给定视觉输入序列的位置，而导航任务是将指令与环境语义对齐。
- en: 'Recently, researchers find that the ability of localization is important to
    navigation, especially for long-term path planning. Thus, some works introduce
    SLAM methods to model the house and improve the localization ability of the agent.
    Neural Map [[115](#bib.bib115)] generalize this idea for all deep reinforcement
    learning agents rather than navigation only. However, this work assumes the location
    of the agent is always known and does not utilize the 2D structure of this memory.
    Neural SLAM [[116](#bib.bib116)] fixes this problem by embedding SLAM-like procedures
    into the soft-attention [[117](#bib.bib117)]. To avoid spatial blurring associated
    with repeated warping, MapNet [[118](#bib.bib118)] proposes to use a world-centric
    rather than an egocentric map. Different from previous works, MapNet maintains
    a 2.5D representation by a deep neural network module that learns to distill visual
    representations from 3D embodied visual inputs. Gordon *et al.* [[58](#bib.bib58)]
    proposes Hierarchical Interactive Memory Network (HIMN), a framework with hierarchical
    controller for IQA task. The high-level controller is a planner that decides the
    long-term navigation target, and the low-level controller predicts the action,
    interacts with the environment, and answers the question. Gupta *et al.* [[20](#bib.bib20)]
    introduce the Neural-SLAM method in embodied navigation. This work consists of
    two parts: mapping and planning. The mapping mechanism maintains a 2D memory map.
    For each step, it transforms the embodied scene into a 2D feature and update the
    map with the feature. The planning mechanism uses a value function to output a
    policy.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，研究人员发现定位能力对导航非常重要，尤其是在长期路径规划中。因此，一些工作引入SLAM方法来建模房屋并提高代理的定位能力。Neural Map[[115](#bib.bib115)]将这一理念推广到所有深度强化学习代理，而不仅仅是导航。然而，这项工作假设代理的位置始终已知，并且未利用这一记忆的二维结构。Neural
    SLAM[[116](#bib.bib116)]通过将类似SLAM的过程嵌入软注意力[[117](#bib.bib117)]来解决这一问题。为了避免与重复变形相关的空间模糊，MapNet[[118](#bib.bib118)]建议使用以世界为中心而非以自我为中心的地图。与之前的工作不同，MapNet通过一个深度神经网络模块保持2.5D表示，该模块学习从3D体现视觉输入中提炼视觉表示。Gordon
    *et al.*[[58](#bib.bib58)]提出了层次互动记忆网络（HIMN），这是一个用于IQA任务的层次控制器框架。高层控制器是一个规划者，它决定长期导航目标，低层控制器则预测动作，与环境互动并回答问题。Gupta
    *et al.*[[20](#bib.bib20)]在体现导航中引入了Neural-SLAM方法。该工作包括两个部分：地图构建和规划。地图构建机制维护一个2D记忆地图。对于每一步，它将体现场景转换为2D特征，并用该特征更新地图。规划机制使用价值函数来输出策略。
- en: 'Efficient exploration is widely regarded as one of the main challenges in reinforcement
    learning (RL) [[119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121)]. Similarly,
    it is important in navigation since the target does not always visible from the
    starting position and the agent is required to explore the unseen scene and search
    for the target. Recently, exploration based on explicitly modeled semantic memory
    is proven to be efficient. To learn a policy with spatial memory, Chen *et al.* [[122](#bib.bib122)]
    bootstrap the model with imitation learning and finetune it with coverage rewards
    derived purely from on-board sensors. Active Neural SLAM (ANS) [[22](#bib.bib22)]
    is a successful neural SLAM method which achieves the state-of-the-art on the
    CVPR 2019 Habitat *Pointgoal* Navigation Challenge. ANS proposes a hierarchical
    structure for planning. Inspired by the idea of hierarchical RL [[123](#bib.bib123),
    [124](#bib.bib124)], ANS learns the high-level planner by reinforcement learning
    and learns the low-level planner by imitation learning. The mapper is implemented
    by an auxiliary task of predicting a 2D map. The first channel of the map stands
    for if there is an obstacle and the second map stands for whether the position
    has been explored or not. However, the predefined 2D map cannot help long-term
    navigation since the semantic information of the scenes in different viewpoints
    is not encoded in the map. Neural Topological SLAM [[125](#bib.bib125)] propose
    a more advanced way which stores the observed feature representations. This method
    introduce a graph update module to leverage semantics. The graph update module
    maintains a topological feature memory. For each step, the module localize current
    observation into memory nodes. If an observation is not localized in any node
    of the memory, the graph update module will add a new node into the topological
    feature memory. Goal-Oriented Semantic Exploration (SemExp) [[126](#bib.bib126)]
    tackles the object goal navigation task in realistic environments. This method
    first builds a episodic semantic map and uses it to explore the environment based
    on the category of the target object. This approach achieves state-of-the-art
    in Habitat ObjectNav Challenge 2020. An overview of the common practice of the
    ‘Neural SLAM’-based model is shown in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1.2 Self-Supervised
    Methods ‣ 4.1 Target-driven Navigation ‣ 4 Methods in Simulated Environments ‣
    Deep Learning for Embodied Visual Navigation Research: A Survey"). In addition
    to the visual encoder and the instruction encoder as in Fig. [5](#S4.F5 "Figure
    5 ‣ 4.1.1 Model-free Methods ‣ 4.1 Target-driven Navigation ‣ 4 Methods in Simulated
    Environments ‣ Deep Learning for Embodied Visual Navigation Research: A Survey"),
    ‘Neural SLAM’-based model have a unique module to project a embodied visual view
    to feature representation and store it in a 2D top-down map:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '高效的探索被广泛认为是强化学习（RL）中的主要挑战之一[[119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121)]。类似地，它在导航中也很重要，因为目标从起始位置并不总是可见，代理需要探索未见过的场景并寻找目标。最近，基于明确建模的语义记忆的探索被证明是高效的。为了学习具有空间记忆的策略，Chen
    *et al.* [[122](#bib.bib122)] 通过模仿学习引导模型并用完全来自车载传感器的覆盖奖励进行微调。主动神经SLAM（ANS）[[22](#bib.bib22)]
    是一种成功的神经SLAM方法，在CVPR 2019 Habitat *Pointgoal* 导航挑战中达到了最先进的水平。ANS 提出了一个用于规划的层次结构。受到层次化RL[[123](#bib.bib123),
    [124](#bib.bib124)] 思想的启发，ANS 通过强化学习学习高层规划器，通过模仿学习学习低层规划器。映射器通过预测2D地图的辅助任务来实现。地图的第一个通道表示是否存在障碍物，第二个地图表示位置是否已被探索。然而，预定义的2D地图无法帮助长期导航，因为不同视角下场景的语义信息未被编码在地图中。神经拓扑SLAM[[125](#bib.bib125)]
    提出了一种更先进的方法，存储观察到的特征表示。这种方法引入了一个图更新模块以利用语义。图更新模块维护一个拓扑特征记忆。每一步，模块将当前观察定位到记忆节点中。如果观察未被定位到记忆中的任何节点，图更新模块将添加一个新节点到拓扑特征记忆中。面向目标的语义探索（SemExp）[[126](#bib.bib126)]
    解决了现实环境中的目标导航任务。这种方法首先建立一个情景语义地图，并基于目标对象的类别使用它来探索环境。这种方法在Habitat ObjectNav Challenge
    2020中达到了最先进的水平。图[6](#S4.F6 "Figure 6 ‣ 4.1.2 Self-Supervised Methods ‣ 4.1 Target-driven
    Navigation ‣ 4 Methods in Simulated Environments ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey")展示了基于‘神经SLAM’的模型的常见做法。除了图[5](#S4.F5 "Figure
    5 ‣ 4.1.1 Model-free Methods ‣ 4.1 Target-driven Navigation ‣ 4 Methods in Simulated
    Environments ‣ Deep Learning for Embodied Visual Navigation Research: A Survey")中所示的视觉编码器和指令编码器外，‘神经SLAM’模型还有一个独特的模块，将具身视觉视图投影到特征表示中，并将其存储在2D自上而下的地图中。'
- en: '|  | $m_{t},\hat{x_{t}}=f_{SLAM}(s_{t},x_{t-1:t}^{\prime},m_{t-1}&#124;\theta_{S})$
    |  | (3) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $m_{t},\hat{x_{t}}=f_{SLAM}(s_{t},x_{t-1:t}^{\prime},m_{t-1}| \theta_{S})$
    |  | (3) |'
- en: where $x_{t-1:t}^{\prime}$ stands for previous poses, $m_{t-1}$ is previous
    maps, and $\theta_{S}$ stands for parameters. This map models the room structure
    and visual representation of scenes. The projected feature representations are
    fused with the visual feature and the instruction feature to jointly predict an
    action.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{t-1:t}^{\prime}$ 代表之前的姿态，$m_{t-1}$ 是之前的地图，$\theta_{S}$ 代表参数。这个地图模型描述了房间的结构和场景的视觉表现。投影特征表示与视觉特征和指令特征融合，以共同预测一个动作。
- en: 4.1.4 Summary
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 总结
- en: Compared with traditional robotics methods, the model-free methods are able
    to obtain robust navigation models by sampling large scale data with the embodied
    simulator. Some works adopt detection and segmentation approaches to get better
    visual views. In spite of indoor scenarios, model-free methods achieve great success
    in street scene and multi-modal environments. Self-supervised methods are proposed
    to exploit the extra knowledge by auxiliary tasks to improve the learning efficiency
    and generalization ability. Planning-based methods utilize a 2D map or a topological
    memory to model the environment during navigation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的机器人方法相比，无模型方法能够通过使用具身模拟器采样大规模数据来获得稳健的导航模型。一些工作采用检测和分割方法以获得更好的视觉视图。尽管主要应用于室内场景，无模型方法在街道场景和多模态环境中取得了巨大成功。提出了自监督方法，通过辅助任务利用额外的知识以提高学习效率和泛化能力。基于规划的方法利用2D地图或拓扑记忆在导航过程中对环境进行建模。
- en: 4.2 Cross-modal Navigation
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 跨模态导航
- en: 'A navigation robot who understands natural language can accomplish more complex
    tasks, such as “*pick up the cup in the kitchen*” or “*help me find my glass upstairs*”.
    In this section, we introduce three kinds of works that solves cross-modal navigation
    tasks: 1) step-by-step methods; 2) pretraining-based methods; 3) planning based
    methods.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一种理解自然语言的导航机器人可以完成更复杂的任务，例如“*拿起厨房里的杯子*”或“*帮我找找楼上的玻璃杯*”。在这一部分，我们介绍了三种解决跨模态导航任务的方法：1）逐步方法；2）基于预训练的方法；3）基于规划的方法。
- en: 4.2.1 Sequence-to-sequence Navigation
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 序列到序列导航
- en: 'Anderson *et al.* [[12](#bib.bib12)] firstly propose a sequence-to-sequence
    model similar to [[127](#bib.bib127)] to address the vision language navigation
    problem. This model sequentially encodes a language instruction word-by-word,
    concatenates the sentence feature with the visual image feature and decodes the
    action sequence. However, a sequence-to-sequence model is lack of stability and
    generalization since it fails to consider the dynamics in the real-world environments.
    RPA [[23](#bib.bib23)] is proposed to tackle the generalization issue by equipping
    a ‘look-ahead’ module, which learns to predict the future state and reward. To
    improve the generalization ability in instruction-trajectory alignment, Fried
    *et al.* [[24](#bib.bib24)] propose a data augmentation approach named “speaker-follower”
    to improve the model generalization. To generate augmentation data, the speaker
    firstly translates a randomly trajectory into an instruction, and the follower
    secondly translates an instruction into a trajectory:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Anderson *et al.* [[12](#bib.bib12)] 首先提出了一种与 [[127](#bib.bib127)] 类似的序列到序列模型来解决视觉语言导航问题。该模型逐步编码语言指令的每个词，将句子特征与视觉图像特征拼接，并解码动作序列。然而，序列到序列模型由于未考虑现实世界环境中的动态性，缺乏稳定性和泛化性。RPA
    [[23](#bib.bib23)] 通过配备‘前瞻’模块来解决泛化问题，该模块学习预测未来状态和奖励。为提高指令-轨迹对齐的泛化能力，Fried *et
    al.* [[24](#bib.bib24)] 提出了一种名为“speaker-follower”的数据增强方法来提升模型泛化性。为了生成增强数据，发言者首先将随机轨迹翻译成指令，而跟随者则将指令翻译成轨迹：
- en: '|  | $\underset{r\in R(d)}{\mathrm{argmax}}P_{S}(d&#124;r)\cdot P_{F}(r&#124;d)^{(1-\gamma)},$
    |  | (4) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underset{r\in R(d)}{\mathrm{argmax}}P_{S}(d|r)\cdot P_{F}(r|d)^{(1-\gamma)},$
    |  | (4) |'
- en: where $P_{S}$ is the speaker, $P_{F}$ is the follower, $d$ stands for an instruction,
    $r$ stands for a trajectory, and $\gamma$ is a weighting factor. Another contribution
    of this paper [[24](#bib.bib24)] is the definition of a high-level action that
    move forward toward an orientation in a panoramic space in stead of low-level
    actions like *turn left*, *turn right* and *go forward*. Compared with the definition
    of low-level actions, this approach largely reduce the length of the action sequence
    that describes the same trajectory. Navigating by the high-level action space
    requires less prediction times, which makes the model easier to train and more
    robust to test. Howeverr, previous methods learn to navigate by imitation learning
    only with the instruction-trajectory data pairs, which supervises the shortest
    path while ignore the sub-optimal trajectories so that leds to overfitting. To
    tackle this problem, Wang *et al.* [[128](#bib.bib128)] propose to jointly learn
    a navigation agent by imitation learning and reinforcement learning.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P_{S}$ 是说话者，$P_{F}$ 是跟随者，$d$ 代表指令，$r$ 代表轨迹，$\gamma$ 是加权因子。本文的另一个贡献[[24](#bib.bib24)]是定义了一个高层次的动作，该动作在全景空间中向某个方向移动，而不是像*向左转*、*向右转*和*向前走*这样低层次的动作。与低层次动作的定义相比，这种方法大大减少了描述相同轨迹的动作序列长度。通过高层次动作空间进行导航需要的预测次数更少，这使得模型更容易训练，并且在测试时更为稳健。然而，之前的方法仅通过指令-轨迹数据对进行模仿学习来进行导航，这种方法监督了最短路径，却忽略了次优轨迹，从而导致了过拟合。为了解决这个问题，Wang
    *et al.* [[128](#bib.bib128)] 提出了通过模仿学习和强化学习共同学习导航代理的方法。
- en: '![Refer to caption](img/97045fbee2699dcf062da9dcda8c12b5.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/97045fbee2699dcf062da9dcda8c12b5.png)'
- en: 'Figure 7: A comparison of seq-to-seq models in VLN and VQA.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：VLN 和 VQA 中 seq-to-seq 模型的比较。
- en: In addition, this method introduce an LSTM to encode the temporal information
    of visual features and introduce a cross-modal mechanism to achieve better vision-language
    navigation ability. Ma *et al.* [[129](#bib.bib129)] propose a self-monitoring
    agent with a visual-textual co-grounding module and progress monitor. The progress
    monitor use the cross-modal feature from the co-grounding module and estimate
    the completed progress. Since the instruction in vision-language task guides the
    agent to the target step-by-step, the progress information contain rich knowledge
    that help improve the perception of the agent. Ma *et al.* propose in [[130](#bib.bib130)]
    the *Regretful Agent*, with a regretful module which uses the estimated progress
    to indicate if the agent navigates to a wrong place and need to go back. Similar
    to the *Regretful Agent*, Ke *et al.* propose  [[131](#bib.bib131)] a framework
    for using asynchronous search to boost a VLN navigator by enabling explicit backtrack.
    Anderson *et al.* [[132](#bib.bib132)] regard the step-by-step navigation process
    as a visual tracking task. This approach implements the navigation agent within
    the framework of Bayesian state tracking [[3](#bib.bib3)] and formulates an end-to-end
    differentiable histogram filter [[133](#bib.bib133)] with learnable observation
    and motion models. One commonly used method that relieve the visual overfitting
    is to apply an dropout [[134](#bib.bib134)] layer on the visual feature, which
    is extracted by a pretrained network like VGG [[135](#bib.bib135)] or ResNet [[136](#bib.bib136)].
    Tan *et al.* [[137](#bib.bib137)] argue that simply applying a dropout layer on
    the visual feature leads to inconsistency, e.g. a chair in this frame could be
    dropped in the next frame. To solve the problem, They propose a environmental
    dropout layer that randomly dropout some fixed channels during a trajectory. Zhu
    *et al.* [[138](#bib.bib138)] propose AuxRN, a framework that introduce self-supervised
    auxiliary tasks to exploit environmental knowledge from several aspects. In addition
    to introducing the temporal difference auxiliary task that is widely use in other
    embodied visual navigation methods [[96](#bib.bib96), [19](#bib.bib19)], AuxRN
    introduces a trajectory retelling task and instruction-trajectory matching task
    that learn the temporal semantics of a trajectory. Instead of generating the low-quality
    augmented data, Fu *et al.* [[139](#bib.bib139)] introduce the concept of counterfactual
    thinking to sample challenging paths to augment training dataset. They present
    a model-agnostic adversarial path sampler (APS) to pick the difficult trajectories
    and only consider useful counterfactual conditions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种方法引入了一个LSTM来编码视觉特征的时间信息，并引入了一个跨模态机制以实现更好的视觉-语言导航能力。Ma *et al.* [[129](#bib.bib129)]
    提出了一个自我监控代理，配备了视觉-文本共同定位模块和进度监控器。进度监控器利用共同定位模块中的跨模态特征来估计完成进度。由于视觉-语言任务中的指令引导代理逐步到达目标，因此进度信息包含丰富的知识，帮助提高代理的感知。Ma
    *et al.* 在[[130](#bib.bib130)] 中提出了*后悔代理*，它包含一个后悔模块，该模块使用估计的进度来指示代理是否导航到了错误的位置并需要返回。类似于*后悔代理*，Ke
    *et al.* [[131](#bib.bib131)] 提出了一个使用异步搜索来提升VLN导航器的框架，支持显式回溯。Anderson *et al.*
    [[132](#bib.bib132)] 将逐步导航过程视为视觉跟踪任务。这种方法在贝叶斯状态跟踪[[3](#bib.bib3)]框架内实现导航代理，并制定了一个端到端可微分的直方图滤波器[[133](#bib.bib133)]，具有可学习的观察和运动模型。常用的一种缓解视觉过拟合的方法是对视觉特征应用dropout[[134](#bib.bib134)]层，这些特征由预训练网络如VGG[[135](#bib.bib135)]或ResNet[[136](#bib.bib136)]提取。Tan
    *et al.* [[137](#bib.bib137)] 认为，仅仅在视觉特征上应用dropout层会导致不一致，例如在这一帧中的一把椅子可能在下一帧中被丢弃。为了解决这个问题，他们提出了一种环境dropout层，在轨迹中随机丢弃一些固定通道。Zhu
    *et al.* [[138](#bib.bib138)] 提出了AuxRN，一个引入自监督辅助任务的框架，以从多个方面挖掘环境知识。除了引入在其他具身视觉导航方法中广泛使用的时间差异辅助任务[[96](#bib.bib96),
    [19](#bib.bib19)]，AuxRN还引入了轨迹重述任务和指令-轨迹匹配任务，学习轨迹的时间语义。Fu *et al.* [[139](#bib.bib139)]
    提出了逆事实思维的概念，用以采样具有挑战性的路径来增强训练数据集。他们展示了一个与模型无关的对抗路径采样器（APS），用于选择困难的轨迹并仅考虑有用的逆事实条件。
- en: 'Different from the earlier works that based on data augmentation and other
    classical navigation methods, some works discover the importance of natural language
    to VLN. Thomason *et al.* [[140](#bib.bib140)] find the unimodal baseline outperforms
    random baselines and even some of their multimodal counterparts. Thus the work
    advocates that ablating unimodal to evaluate the bias is important to proposing
    a dataset. A study of Huang *et al.* [[141](#bib.bib141)] shows that only a limited
    number of those augmented paths in  [[24](#bib.bib24)] are useful and after using
    60% of the augmented data, the improvement diminishes with additional augmented
    data. To avoid the extensive work in reward engineering, Wang *et al.* [[142](#bib.bib142)]
    propose a Soft Expert Reward Learning model that includes two parts: 1) soft expert
    distillation, which encourages agents to behave like an expert in soft fashion;
    2) self perceiving, which pushes the agent towards the final destination as fast
    as possible. Xia *et al.* [[143](#bib.bib143)] leverages multiple instructions
    as different descriptions for the same trajectory to resolve language ambiguity
    and improve generalization ability. This work indicates that the human annotations
    in VLN are largely biased according to the specific scene and the trajectory.
    The quality of visual features is critical for improving the performance of embodied
    navigation. Previous works extract global visual features from panoramic views
    by a pretrained CNN network like ResNet-101 [[136](#bib.bib136)]. Hong *et al.* [[144](#bib.bib144)]
    introduce Faster-RCNN to detect objects in navigation and build a relationship
    graph between visual and language entities for vision-language alignment. In spite
    of the visual inputs, the structure information also helps navigation. Hu *et
    al.* [[145](#bib.bib145)] discover that the language instructions contain high-level
    semantic information while visual representations are a lower-level modality,
    which makes the vision-language alignment difficult. Motivated by this, they decomposes
    the grounding procedure into a set of expert models with access to different modalities
    and ensemble them at prediction time. To better research what role does language
    understanding play in VLN task, Hong *et al.* [[48](#bib.bib48)] argue that the
    intermediate supervision is important in vision-language alignment. Thus, they
    propose FGR2R, a method which enables navigation processes to be traceable and
    encourage the agent to move at the level of sub-instructions.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期基于数据增强和其他经典导航方法的研究不同，一些研究发现自然语言对视觉语言导航（VLN）的重要性。Thomason *等* [[140](#bib.bib140)]
    发现单模态基线优于随机基线，甚至优于一些多模态对比基线。因此，该研究主张，评估单模态以检测偏差对于提出数据集是重要的。Huang *等* [[141](#bib.bib141)]
    的研究显示，只有在 [[24](#bib.bib24)] 中有限数量的增强路径是有用的，在使用了 60% 的增强数据后，额外的增强数据带来的改进会减小。为了避免在奖励工程中进行大量工作，Wang
    *等* [[142](#bib.bib142)] 提出了一个软专家奖励学习模型，该模型包括两个部分：1）软专家蒸馏，鼓励智能体以软性方式表现得像专家；2）自我感知，推动智能体尽快到达最终目标。Xia
    *等* [[143](#bib.bib143)] 利用多种指令作为同一路径的不同描述，以解决语言歧义并提高泛化能力。这项工作表明，VLN 中的人类标注在很大程度上受特定场景和轨迹的偏见影响。视觉特征的质量对于提升体现导航的表现至关重要。以往的研究通过预训练的
    CNN 网络（如 ResNet-101）从全景视图中提取全局视觉特征 [[136](#bib.bib136)]。Hong *等* [[144](#bib.bib144)]
    引入 Faster-RCNN 来检测导航中的物体，并建立视觉和语言实体之间的关系图，用于视觉语言对齐。尽管有视觉输入，结构信息也有助于导航。Hu *等* [[145](#bib.bib145)]
    发现语言指令包含高级语义信息，而视觉表示则是低级模态，这使得视觉语言对齐变得困难。受到此启发，他们将基础定位过程分解为一组具有不同模态访问的专家模型，并在预测时进行集成。为了更好地研究语言理解在
    VLN 任务中扮演的角色，Hong *等* [[48](#bib.bib48)] 认为中间监督在视觉语言对齐中是重要的。因此，他们提出了 FGR2R 方法，使导航过程可追踪，并鼓励智能体在子指令的层次上移动。
- en: '![Refer to caption](img/945bb9e482d759ee7f7208544728b7c4.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/945bb9e482d759ee7f7208544728b7c4.png)'
- en: 'Figure 8: An example of pretraining-based framework.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：基于预训练框架的示例。
- en: 4.2.2 Pretraining-based Methods
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 基于预训练的方法
- en: 'Several challenges are discovered during the research on the vision-language
    navigation: 1) low training efficiency; 2) large data bias (include both vision
    and language); 3) lack of generalization from seen to unseen scenes. To address
    these challenges, pretraining-based models are proposed to learn from large-scale
    data sets from other sources and fast adapt to unseen scenarios.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉语言导航的研究过程中发现了几个挑战：1) 训练效率低；2) 大数据偏差（包括视觉和语言）；3) 从已见场景到未见场景的泛化不足。为了解决这些挑战，提出了基于预训练的模型来从其他来源的大规模数据集中学习，并快速适应未见的场景。
- en: Low training efficiency The traditional encoder-decoder framework first samples
    the total trajectory by teacher-forcing or student-forcing and then back-propagate
    the gradients. In other deep learning tasks like image classification [[1](#bib.bib1)]
    or text recognition [[146](#bib.bib146)], the model predicts a result directly.
    However, in the vision-language navigation task, the agent predicts a trajectory
    by interacting with the environment in a step-by-step manner, which is so time-consuming
    that reduce the training efficiency.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 训练效率低 传统的编码器-解码器框架首先通过教师强制或学生强制采样整个轨迹，然后反向传播梯度。在其他深度学习任务中，如图像分类 [[1](#bib.bib1)]
    或文本识别 [[146](#bib.bib146)]，模型直接预测结果。然而，在视觉语言导航任务中，代理通过逐步与环境交互来预测轨迹，这样的过程非常耗时，从而降低了训练效率。
- en: '![Refer to caption](img/d30ffc1f1185e4bcaf22cd1dd0a2b1d1.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d30ffc1f1185e4bcaf22cd1dd0a2b1d1.png)'
- en: 'Figure 9: A comparison of three embodied vision-language navigation tasks:
    EQA-v1 [[54](#bib.bib54)], MT-EQA [[57](#bib.bib57)] and VDN [[49](#bib.bib49)].'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：三种具身视觉语言导航任务的比较：EQA-v1 [[54](#bib.bib54)]、MT-EQA [[57](#bib.bib57)] 和 VDN [[49](#bib.bib49)]。
- en: Large data bias The vision-language navigation scenarios are so diverse that
    61 houses in R2R cannot cover all of them. From the aspect of natural language,
    in the R2R task, only 69% of bigrams are shared between training and evaluation.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据偏差 视觉语言导航场景的多样性非常高，以至于 R2R 中的 61 个房屋无法覆盖所有场景。从自然语言的角度来看，在 R2R 任务中，只有 69%
    的双词组在训练和评估之间共享。
- en: Lack of generalization Lacking of diverse training data still largely limits
    the generalization in spite of the proposed augmentation methods like trajectory
    augmentation, visual feature augmentation and natural language augmentation. Thus,
    introducing extra knowledge from other tasks and datasets becomes a promising
    topic.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化不足 尽管提出了诸如轨迹增强、视觉特征增强和自然语言增强等数据增强方法，但缺乏多样化的训练数据仍然在很大程度上限制了泛化。因此，引入来自其他任务和数据集的额外知识成为一个有前途的课题。
- en: 'Pretraining-based methods largely improve the generalization ability of a model
    by learning in large scale of data [[136](#bib.bib136), [51](#bib.bib51)]. Furthermore,
    bert-based methods [[147](#bib.bib147), [148](#bib.bib148)] pretrain a transformer
    network with proxy tasks and achieve great success in vision, language and cross-modal
    tasks. Many researchers consider to solve the vision-language navigation problem
    by pretaining-based methods. Li *et al.* [[149](#bib.bib149)] propose PreSS first
    introduce a pretrained language models to learn instruction representations. And
    they propose a stochastic sampling scheme to reduce the gap between the expert
    actions in training and the sampled actions in testing. Majumdar *et al.* [[150](#bib.bib150)]
    advocate to improve model by leveraging large-scale of web data. However, it is
    hard to transfer the static image data to VLN task. Therefore, they propose VLN-bert,
    a transformer-based model which is pretrained by static images and their captions.
    Prevalent [[151](#bib.bib151)] is self-supervisedly pretrained on large amount
    of image-text-action triplets sampled from an embodied environment with two pretrianing
    objectives, masked language modeling (MLM) and action prediction (AP):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 基于预训练的方法通过在大规模数据上学习，显著提升了模型的泛化能力 [[136](#bib.bib136), [51](#bib.bib51)]。此外，基于
    BERT 的方法 [[147](#bib.bib147), [148](#bib.bib148)] 通过代理任务预训练一个 Transformer 网络，在视觉、语言和跨模态任务中取得了巨大的成功。许多研究者考虑通过基于预训练的方法解决视觉语言导航问题。Li
    *et al.* [[149](#bib.bib149)] 提出了 PreSS，首先引入预训练的语言模型以学习指令表示。他们还提出了一种随机采样方案，以减少训练中的专家动作与测试中采样动作之间的差距。Majumdar
    *et al.* [[150](#bib.bib150)] 倡导通过利用大规模的网络数据来改进模型。然而，将静态图像数据转移到 VLN 任务中是困难的。因此，他们提出了
    VLN-bert，这是一种基于 Transformer 的模型，通过静态图像及其说明进行预训练。Prevalent [[151](#bib.bib151)]
    在从具身环境中采样的大量图像-文本-动作三元组上自监督预训练，具有两个预训练目标：掩码语言建模 (MLM) 和动作预测 (AP)。
- en: '|  | $\displaystyle\begin{split}&amp;L_{MLM}=-\mathbb{E}_{s\sim p(\tau),(\tau,x)\sim
    D_{E}}\mathrm{log}\ p(x_{i}&#124;\textbf{x},s),\\ &amp;L_{AP}=-\mathbb{E}_{(a,s)\sim
    p(\tau),(\tau,x)\sim D_{E}}\mathrm{log}\ p(\textbf{x}&#124;x_{[CLS]},s),\end{split}$
    |  | (5) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\begin{split}&amp;L_{MLM}=-\mathbb{E}_{s\sim p(\tau),(\tau,x)\sim
    D_{E}}\mathrm{log}\ p(x_{i}&#124;\textbf{x},s),\\ &amp;L_{AP}=-\mathbb{E}_{(a,s)\sim
    p(\tau),(\tau,x)\sim D_{E}}\mathrm{log}\ p(\textbf{x}&#124;x_{[CLS]},s),\end{split}$
    |  | (5) |'
- en: where $(s,a)$ a state-action pair. Prevalent is proven to be effective on several
    vision-language navigation datasets, including R2R, CVDN and HANNA. The embodied
    navigation agent receives partial observation rather than global observation,
    which is better to be modeled as a partially observable Markov Decision Process.
    Different from the encoder-decoder model, previous pretraining-based models do
    not memorize previously seen scenes during navigation and utilize temporal knowledge,
    which causes information loss in action prediction. Motivated by this, Hong *et
    al.* [[152](#bib.bib152)] propose a recurrent multi-layer transformer network
    that is time-aware for use in VLN. This method introduce a Transformer which maintains
    a feature vector to represent temporal context.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(s,a)$ 是状态-动作对。经过验证，Prevalent 在多个视觉-语言导航数据集上表现出有效性，包括 R2R、CVDN 和 HANNA。具身导航代理接收到的是部分观察而非全局观察，这更适合被建模为部分可观察的马尔可夫决策过程。与编码器-解码器模型不同，之前基于预训练的模型在导航过程中不会记住先前看到的场景，也不利用时间知识，这会导致动作预测中的信息丢失。受此启发，Hong
    *et al.* [[152](#bib.bib152)] 提出了一个时间感知的递归多层 Transformer 网络，用于 VLN。这种方法引入了一种 Transformer，它保持一个特征向量来表示时间上下文。
- en: 4.2.3 Navigation with Questioning and Answering
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 问答导航
- en: Instead of passively perceive natural language instructions from a human commander,
    Das *et al.* [[57](#bib.bib57)] suggest that an intelligent agent should be able
    to answer a question via navigation. Thus Das *et al.* present a new task named
    EQA (Embodied Questioning and Answering), where an agent is spawned at a random
    location in a 3D environment and asked to answer a question. In order to answer,
    the agent have to first navigate to explore the environment, gather information
    through egocentric vision, and then answer the question. To solve this challenging
    task, Das *et al.* present PACMAN, a CNN-RNN model with Adaptive Computation Time
    (ACT) module [[153](#bib.bib153)] to decide how many times to repeatly execute
    an action [[57](#bib.bib57)]. The PACMAN is bootstrapped by shortest path demonstrations
    and then fine-tuned with RL. However, this method is lack of the ability of high-level
    representation. In a later work [[154](#bib.bib154)], Das *et al.* propose a hierarchical
    policy named Neural Modular Controller (NMC) that operates at multiple timescales,
    where the higher-level master policy proposes sub-goals to be executed by low-level
    sub-policies. Anand *et al.* [[155](#bib.bib155)] find that a blindfold (question-only)
    baseline on EQA and find that the baseline perform previous state-of-the-art models.
    They suggest that previous EQA models are ineffective at leveraging the context
    from the environment and the EQAv1 dataset has lots of noise.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Das *et al.* [[57](#bib.bib57)] 建议，智能代理不仅要被动接收来自人类指挥官的自然语言指令，还应该能够通过导航回答问题。因此，Das
    *et al.* 提出了一个名为 EQA（具身问答）的新任务，其中代理在 3D 环境中的随机位置生成，并被要求回答一个问题。为了回答这个问题，代理必须首先进行导航以探索环境，通过自我中心视觉收集信息，然后回答问题。为了解决这一具有挑战性的任务，Das
    *et al.* 提出了 PACMAN，这是一种具有自适应计算时间（ACT）模块的 CNN-RNN 模型 [[153](#bib.bib153)]，用于决定执行一个动作的重复次数
    [[57](#bib.bib57)]。PACMAN 通过最短路径示例进行初始化，然后通过 RL 进行微调。然而，这种方法缺乏高层次表示能力。在后续工作中 [[154](#bib.bib154)]，Das
    *et al.* 提出了一个名为神经模块控制器（NMC）的层次策略，它在多个时间尺度上运行，其中高层次的主策略提出子目标，由低层次子策略执行。Anand *et
    al.* [[155](#bib.bib155)] 发现了一个盲目（仅问题）基线在 EQA 上，并发现该基线表现超越了之前的最先进模型。他们建议之前的 EQA
    模型在利用环境上下文方面效果不佳，而且 EQAv1 数据集噪声较多。
- en: 'Wu *et al.* [[156](#bib.bib156)] propose a simple supervised learning baseline
    which is competitive to the state-of-the-art EQA methods. To improve EQA performance
    in unseen environment, in this paper, they propose a setting in which allows the
    agent to answer questions for adaptation. Yu *et al.* [[57](#bib.bib57)] argues
    that the EQA task assumes that each question has exactly one target, which limits
    its application. Therefore, Yu *et al.* present Multi-Target EQA (MT-EQA), a generalized
    version of EQA. The question of this task contains multiple targets. And it require
    the agent to perform comparative reasoning over multiple targets rather than simply
    perceive the attributes of one target. Wijmans *et al.* [[157](#bib.bib157)] extend
    the EQA problem to photorealstic environment. In this environment, they discover
    that point cloud representations are more effective for navigation. Luo *et al.* [[158](#bib.bib158)]
    suggest that the visual perception ability limits the performance of the EQA.
    They introduce Flownet2 [[159](#bib.bib159)], a high-speed video segmentation
    framework as a backbone to assist navigation and question answering. Li *et al.* [[160](#bib.bib160)]
    propose a MIND module that model the environment imagery and generate mental images
    that are treated as short-term sub-goals. Tan *et al.* [[161](#bib.bib161)] investigate
    the questioning and answering problems between multiple targets. In this task,
    the agent has to navigate to multiple places, find all targets, analysis the relationships
    between them, and answer the question. Motivated by recent progress in Visual
    Question Answering (VQA) [[50](#bib.bib50)] and Video Question Answering (VideoQA) [[162](#bib.bib162)],
    Cangea *et al.* [[163](#bib.bib163)] propose VideoNavQA, a dataset that contains
    pairs of questions and videos generated in the House3D environment. This dataset
    fills the gap between the VQA and the EQA. The VideoNavQA task represents an alternative
    view of the EQA paradigm: By providing nearly-optimal trajectories to the agent,
    the navigation problem is easier to solve compared with the reasoning problem.
    Deng *et al.* [[164](#bib.bib164)] propose Manipulation Question Answering (MQA)
    where the robot is required to find the answer to the question by actively exploring
    the environment via manipulation. To suggest a promising direction of solving
    MQA, they provide a framework which consists of a QA module (VQA framework) and
    a manipulation model (Q learning framework). Nilsson *et al.* [[165](#bib.bib165)]
    build an agent which explores in a 3D environment and occasionally requests annotation
    during navigation. Similarly, Roman *et al.* [[166](#bib.bib166)] suggest a two-agent
    paradigm for cooperative vision-and-dialogue navigation. Their model learns multiple-skills,
    including navigation, question asking, and questioning-answering components.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 吴*等*[[156](#bib.bib156)]提出了一种简单的监督学习基线，与最先进的EQA方法具有竞争力。为了在未见环境中提高EQA性能，本文提出了一种设置，允许智能体通过适应回答问题。于*等*[[57](#bib.bib57)]认为EQA任务假设每个问题有一个明确的目标，这限制了其应用。因此，于*等*提出了多目标EQA（MT-EQA），这是EQA的一个广义版本。该任务的问题包含多个目标，并且要求智能体对多个目标进行比较推理，而不仅仅是感知一个目标的属性。Wijmans*等*[[157](#bib.bib157)]将EQA问题扩展到逼真的环境中。在这种环境下，他们发现点云表示对导航更为有效。罗*等*[[158](#bib.bib158)]指出视觉感知能力限制了EQA的性能。他们引入了Flownet2[[159](#bib.bib159)]，这是一个高速视频分割框架，作为支持导航和问题回答的骨干。李*等*[[160](#bib.bib160)]提出了一个MIND模块，模拟环境影像并生成作为短期子目标的心理图像。谭*等*[[161](#bib.bib161)]研究了多目标之间的问题问答。在这个任务中，智能体必须导航到多个地点，找到所有目标，分析它们之间的关系，并回答问题。受到最近视觉问答（VQA）[[50](#bib.bib50)]和视频问答（VideoQA）[[162](#bib.bib162)]进展的启发，Cangea*等*[[163](#bib.bib163)]提出了VideoNavQA，一个包含在House3D环境中生成的问题和视频对的数据集。这个数据集填补了VQA和EQA之间的空白。VideoNavQA任务代表了EQA范式的另一种视角：通过为智能体提供近似最优的轨迹，相比推理问题，导航问题更易解决。邓*等*[[164](#bib.bib164)]提出了操作问答（MQA），其中机器人通过操作环境来主动探索并寻找问题的答案。为了建议解决MQA的有前途方向，他们提供了一个包含QA模块（VQA框架）和操作模型（Q学习框架）的框架。Nilsson*等*[[165](#bib.bib165)]建立了一个智能体，该智能体在3D环境中探索，并在导航过程中偶尔请求注释。同样，Roman*等*[[166](#bib.bib166)]建议了一种双智能体范式，用于合作视觉与对话导航。他们的模型学习了多项技能，包括导航、提问和问答组件。
- en: 4.2.4 Navigation with Dialogue
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 带对话的导航
- en: 'There is a long history that human use a dialog to guide a robot [[167](#bib.bib167),
    [168](#bib.bib168)]. In the field of embodied navigation, Banerjee *et al.* [[59](#bib.bib59)]
    propose “Help, Anna!” (HANNA), an interactive photo-realistic simulator in which
    an agent fulfills object-finding tasks by requesting and interpreting natural
    language and vision assistance. Nguyen *et al.* [[169](#bib.bib169)] propose a
    task named VLNA, where an agent is guided via language to find objects. However,
    the language instruction in these two tasks far from real-world problem: the responses
    of HANNA are automatic generated from a trained model while the guidance of VLNA
    are in the form of templated language that encodes gold-standard planner action.
    Vries *et al.* [[170](#bib.bib170)] propose “Talk The Walk” (TtW), where two humans
    communicate to reach a goal location in an outdoor environment. However, in TtW,
    the human uses an abstracted semantic map rather than an egocentric view of the
    environment.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 人类使用对话来指导机器人已有悠久的历史[[167](#bib.bib167), [168](#bib.bib168)]。在具身导航领域，Banerjee
    *et al.* [[59](#bib.bib59)] 提出了“Help, Anna!”（HANNA），这是一个互动式逼真模拟器，其中一个代理通过请求和解释自然语言及视觉辅助来完成物体寻找任务。Nguyen
    *et al.* [[169](#bib.bib169)] 提出了一个名为VLNA的任务，其中一个代理通过语言引导来寻找物体。然而，这两个任务中的语言指令与现实世界问题相距甚远：HANNA的响应是从训练模型中自动生成的，而VLNA的指导则以编码黄金标准规划动作的模板化语言形式出现。Vries
    *et al.* [[170](#bib.bib170)] 提出了“Talk The Walk”（TtW），在这个任务中，两个人类在户外环境中通过交流来达到目标位置。然而，在TtW中，人类使用的是抽象的语义地图，而不是环境的自我中心视图。
- en: 'Thomason *et al.* [[49](#bib.bib49)] propose vision-and-dialog navigation (VDN),
    a scaffold for navigation-centered question asking and question answering tasks
    where an agent navigates following a multi-round dialog history rather than an
    instruction. Compared with the single-round instructions in R2R dataset, VDN provides
    multi-round annotation, in which each round of dialogue describes a sub-trajectory.
    The more fine-grained dialogue annotation facilitate researchers to study the
    problem of navigation with natural language. Zhu *et al.* [[138](#bib.bib138)]
    propose a framework with a cross-modal memory mechanism to capture the hierarchical
    correlation between the dialogue rounds and the sub-trajectories. More generally,
    several methods, such as Prevalent [[151](#bib.bib151)] and BabyWalk [[63](#bib.bib63)],
    validate their navigation ability using both sentence instructions and dialog
    instructions. Unfortunately, these works heavily rely on dialogue annotations
    which is labor-intensive. To alleviate this, Roman *et al.* [[166](#bib.bib166)]
    exploit to generate dialogue questions answers based on visual views. This work
    addresses four challenges in modeling turn-based dialogues, which includes: 1)
    deciding when to ask a question; 2) generating navigator questions; 3) generating
    question-answer pairs for guidance; 4) generating navigator actions. To achieve
    this, Roman *et al.* [[166](#bib.bib166)] introduce a two-agent paradigm, where
    one agent navigates and asks questions while the other guides agent answers. Different
    from previous works that guide navigator with template language, this work initialize
    the oracle model via pretraining on CVDN dialogues to generate natural language.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Thomason *et al.* [[49](#bib.bib49)] 提出了视觉与对话导航（VDN），这是一个用于导航中心问题提问和回答任务的框架，其中代理跟随多轮对话历史进行导航，而不是仅仅依赖指令。与R2R数据集中单轮指令相比，VDN提供了多轮注释，其中每轮对话描述了一个子轨迹。更细粒度的对话注释有助于研究人员研究自然语言中的导航问题。Zhu
    *et al.* [[138](#bib.bib138)] 提出了一个具有跨模态记忆机制的框架，以捕捉对话轮次与子轨迹之间的层次关联。更一般地说，几种方法，如Prevalent
    [[151](#bib.bib151)] 和BabyWalk [[63](#bib.bib63)]，通过句子指令和对话指令验证其导航能力。不幸的是，这些工作严重依赖于劳动密集型的对话注释。为了缓解这一问题，Roman
    *et al.* [[166](#bib.bib166)] 试图基于视觉视图生成对话问题答案。这项工作解决了建模轮转对话中的四个挑战，包括：1) 决定何时提问；2)
    生成导航问题；3) 生成指导用的问答对；4) 生成导航员动作。为此，Roman *et al.* [[166](#bib.bib166)] 引入了一个双代理范式，其中一个代理进行导航并提问，而另一个代理提供回答。与之前使用模板语言指导导航员的工作不同，这项工作通过在CVDN对话上进行预训练来初始化oracle模型，以生成自然语言。
- en: A dialog does not always describe a step-by-step navigation process. Rather,
    the oracle describes the target scene and let the navigator to find it, which
    commonly occurs when someone get lost in a new building. Hahn *et al.* [[171](#bib.bib171)]
    propose a LED task (localizing the observer from dialog history) to realize when
    it get lost. Motivated by this, they present a dataset named Where Are You [[171](#bib.bib171)]
    that consists of 6k dialogues of two humans. Due to the wide application of multi-agent
    communication systems [[172](#bib.bib172), [173](#bib.bib173)] in real-world,
    researchers become interested in implementing dialog navigating in physical environments.
    Marge *et al.* [[174](#bib.bib174)] present MRDwH, a platform that implements
    autonomous dialogue management and navigation of two simulated robots in a large
    outdoor simulated environment. Banerjee *et al.* [[175](#bib.bib175)] propose
    RobotSlang benchmark, a dataset which is gathered by pairing a human “driver”
    controlling a physical robot and asking questions of a human “commander”
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对话并不总是描述一个一步一步的导航过程。相反，oracle描述目标场景，并让导航员去找到它，这种情况通常发生在有人在新建筑物中迷路时。Hahn *et
    al.*[[171](#bib.bib171)] 提出了一个LED任务（根据对话历史定位观察者）来实现当它迷路时。受此启发，他们提出了一个名为“你在哪”的数据集[[171](#bib.bib171)],其中包含6k对话。由于真实世界中多智能体通信系统[[172](#bib.bib172),
    [173](#bib.bib173)]的广泛应用，研究人员对在物理环境中实现对话导航产生了兴趣。Marge *et al.*[[174](#bib.bib174)]提出了MRDwH，这是一个在大型室外模拟环境中实现自主对话管理和两个模拟机器人导航的平台。Banerjee
    *et al.*[[175](#bib.bib175)]提出了RobotSlang基准测试，这是一个通过配对一个控制物理机器人的人类“驾驶员”和向人类“指挥官”提问的数据集。
- en: 'We compare the difference of Embodied Question Answering (EQA) [[54](#bib.bib54)],
    Multi-Target Embodied Question Answering (MT-EQA) [[57](#bib.bib57)] and Vision-and-dialog
    navigation (VDN) [[49](#bib.bib49)] in Fig. [9](#S4.F9 "Figure 9 ‣ 4.2.2 Pretraining-based
    Methods ‣ 4.2 Cross-modal Navigation ‣ 4 Methods in Simulated Environments ‣ Deep
    Learning for Embodied Visual Navigation Research: A Survey"). We demonstrate three
    different dialogues for the same navigation trajectory as an example. Compared
    with EQA, the question in MT-EQA are more complex since it should describe multiple
    targets. The agent have to acquire high-level skills, such as reasoning, comparison
    and multi-object localization, to accomplish MT-EQA. In the EQA and MT-EQA tasks,
    the agent is required to answer question from a human via navigation. However,
    in the VDN task, the agent is the navigator and the questioner which asks a human
    for hints to find the target. The difference of the task setting led to the different
    designs of the navigation model.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图[9](#S4.F9 "Figure 9 ‣ 4.2.2 Pretraining-based Methods ‣ 4.2 Cross-modal
    Navigation ‣ 4 Methods in Simulated Environments ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey")中比较了具有实体问题回答（EQA）[[54](#bib.bib54)]，多目标实体问题回答（MT-EQA）[[57](#bib.bib57)]和视觉与对话导航（VDN）[[49](#bib.bib49)]的差异。我们以一个导航轨迹的三个不同对话为例进行展示。与EQA相比，MT-EQA中的问题更加复杂，因为它应该描述多个目标。Agent必须获得高级技能，如推理，比较和多目标定位，以完成MT-EQA。在EQA和MT-EQA任务中，agent需要通过导航来回答人类的问题。然而，在VDN任务中，agent是导航员和提问者，他向人类寻求提示以找到目标。任务设置的不同导致了导航模型的不同设计。'
- en: 4.2.5 Summary
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5 总结
- en: Natural language provides an interface for a human to interact with a robot.
    A robot with cross-modal understanding is able to accomplish complex tasks such
    as navigating following a natural language instruction or a dialogue, asking the
    oracle for more details, etc. Lots of works have been proposed to research on
    vision-language navigation problem from diverse aspects.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言为人类与机器人交互提供了接口。具有跨模态理解能力的机器人能够完成诸如遵循自然语言指示或对话导航，询问oracle以获取更多细节等复杂任务。已经提出了许多研究面向不同方面的视觉-语言导航问题。
- en: '![Refer to caption](img/d4116bf607dc732e5ac829e483e5bde8.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d4116bf607dc732e5ac829e483e5bde8.png)'
- en: 'Figure 10: A comparison of input space and action space of a simulated environment
    and the real-world environment.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '图10: 一个模拟环境和真实世界环境的输入空间和动作空间的比较。'
- en: 5 Methods in Real-world Environments
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 真实环境中的5种方法
- en: Embodied navigation methods in simulated environments give a promising direction
    of solving real-world navigation problems. In this section, we are going to 1)
    introduce methods for real-world applications; 2) compare them with the methods
    in simulators; 3) discuss the possibility of sim-to-real transferring.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在模拟环境中体现的导航方法为解决现实世界的导航问题提供了有希望的方向。在这一部分，我们将1）介绍现实世界应用的方法；2）与模拟器中的方法进行比较；3）讨论模拟到现实的转移可能性。
- en: 5.1 Real-world Navigation Methods
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 现实世界导航方法
- en: 5.1.1 Indoor Robotic Navigation
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 室内机器人导航
- en: Deep learning plays an important role in indoor navigation for real-world applications.
    LeCun *et al.* [[176](#bib.bib176)] firstly adopt convolutional network for obstacle
    avoidance. Hadsell *et al.* [[177](#bib.bib177)] propose a self-supervised learning
    process that accurately classifies long-range vision semantics via a hierarchical
    deep model. The method is validated on a Learning applied to ground robots (LAGR) [[25](#bib.bib25)].
    Later, more and more real-world robots adopt deep learning to perceive and extract
    distinctive visual features [[178](#bib.bib178)]. Zhang *et al.* [[179](#bib.bib179)]
    research on the problem where a real robot navigates in simple maze-like environments.
    Based on the success of RL algorithms for solving challenging control tasks [[180](#bib.bib180),
    [77](#bib.bib77)], Zhang *et al.* employ successor representation in learning
    to achieve quick adaptation. Morad *et al.* [[26](#bib.bib26)] present an indoor
    object-driven navigation method named NavACL that uses automatic curriculum learning
    and is easily generalized to new environments and targets. Kahn *et al.* [[181](#bib.bib181)]
    adopt multitask learning and off-policy RL learning to learn directly from real-world
    events. This method enables a robot to learn autonomously and be easily deployed
    on multiple real-world tasks without any human provided labels.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在现实世界应用的室内导航中扮演着重要角色。LeCun *et al.* [[176](#bib.bib176)] 首次采用卷积网络进行避障。Hadsell
    *et al.* [[177](#bib.bib177)] 提出了一个自监督学习过程，通过分层深度模型准确分类长距离视觉语义。这一方法在应用于地面机器人（LAGR）上得到验证[[25](#bib.bib25)]。随后，越来越多的现实世界机器人采用深度学习来感知和提取独特的视觉特征[[178](#bib.bib178)]。Zhang
    *et al.* [[179](#bib.bib179)] 研究了现实机器人在简单迷宫环境中导航的问题。基于强化学习算法在解决挑战性控制任务中的成功[[180](#bib.bib180),
    [77](#bib.bib77)]，Zhang *et al.* 运用后继表示进行学习以实现快速适应。Morad *et al.* [[26](#bib.bib26)]
    提出了一个名为NavACL的室内物体驱动导航方法，该方法使用自动课程学习，并能够轻松推广到新环境和目标。Kahn *et al.* [[181](#bib.bib181)]
    采用多任务学习和离策略强化学习，直接从现实世界事件中学习。这一方法使得机器人能够自主学习，并能轻松部署于多个现实世界任务中，无需任何人为提供的标签。
- en: 5.1.2 Outdoor Robotic Navigation
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 户外机器人导航
- en: There has been a long history that human study outdoor navigation robot. Thorpe
    *et al.* [[27](#bib.bib27)] present two algorithms, a RGB-based method for road
    following and a 3D-based method for obstacle detection, for a robot to learn to
    navigate in a campus. Ross *et al.* [[182](#bib.bib182)] combine deep learning
    and reinforcement learning to learn obstacle avoidance for UAVs. Morad *et al.*
    evaluate the performance of NavACL on two simulated environments, Gibson and Habitat.
    And we transfer the navigation to a Turtlebot3 wheeled robot (AGV) and a DJI Tello
    quadrotor (UAV). Both quantitative and qualitative results reveal that the policy
    of NavACL trained in the simualted environment is surprisingly effective in AGV
    and UAV. Manderson *et al.* [[183](#bib.bib183)] use conditional imitation learning
    to train an underwater vehicle to navigate close to sparse geographic waypoints
    without any prior map.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 人类研究户外导航机器人的历史悠久。Thorpe *et al.* [[27](#bib.bib27)] 提出了两种算法：一种基于RGB的道路跟随方法和一种基于3D的障碍物检测方法，用于让机器人在校园内学习导航。Ross
    *et al.* [[182](#bib.bib182)] 结合深度学习和强化学习，学习无人机的避障。Morad *et al.* 评估了NavACL在两个模拟环境Gibson和Habitat中的表现。我们将导航转移到Turtlebot3轮式机器人（AGV）和DJI
    Tello四旋翼无人机（UAV）上。定量和定性结果都表明，在模拟环境中训练的NavACL策略在AGV和UAV中表现出意外的有效性。Manderson *et
    al.* [[183](#bib.bib183)] 使用条件模仿学习来训练水下车辆，在没有任何先验地图的情况下，接近稀疏的地理航点进行导航。
- en: 5.1.3 Long-range Navigation
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 长距离导航
- en: Their model achieves the best performance and shows competitive generalization
    ability on a real robot platform. Borenstein *et al.* [[184](#bib.bib184)] propose
    to maintain a world model [[185](#bib.bib185)] that updated continuously and in
    real-time to avoid obstacles. The world model learns and simulates the real-world
    environment and reduce the cost of data sampling [[185](#bib.bib185)]. Liu *et
    al.* propose Lifelong Federated Reinforcement Learning (LFRL), a learning architecture
    for navigation in cloud robotic systems to address this problem.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的模型在现实机器人平台上表现出最佳性能，并显示出具有竞争力的泛化能力。Borenstein *et al.* [[184](#bib.bib184)]
    提出了一个世界模型 [[185](#bib.bib185)]，该模型不断实时更新以避免障碍。世界模型学习并模拟现实世界环境，减少数据采样的成本 [[185](#bib.bib185)]。Liu
    *et al.* 提出了终身联邦强化学习（LFRL），这是一个用于云机器人系统导航的学习架构，以解决这一问题。
- en: Long-range navigation is challenging for real-world robots. To address this
    proble, Francis *et al.* [[186](#bib.bib186)] present PRM-RL, a hierarchical robot
    navigation method. The PRM-RL model consists of a reinforcement learning agent
    that learns short-range obstacle avoidance from noisy sensors, and a sampling-based
    planner to map the navigation space. Shah *et al.* [[187](#bib.bib187)] propose
    ViNG, a learning-based navigation system for reaching visually indicated goals
    and demonstrating this system on a real mobile robot platform. Unlike prior work,
    ViNG uses purely offline experience and does not require a simulator or online
    data collection, which significantly improves the training efficiency. Mapping [[188](#bib.bib188)]
    and path planning [[189](#bib.bib189)] has also been widely adopted by many real-world
    applications. Davison *et al.* [[190](#bib.bib190)] builds an automatic system,
    which is able to detect, store and track suitable landmark features during goal-directed
    navigation. They show how a robot can use active vision to provide continuous
    and accurate global positioning, thus achieving efficient navigation. Sim *et
    al.* [[191](#bib.bib191)] enable a robot to accurately localize its location by
    employing a hybrid map representation of 3D point landmarks.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 长距离导航对现实世界机器人来说是具有挑战性的。为了解决这个问题，Francis *et al.* [[186](#bib.bib186)] 提出了PRM-RL，一种分层机器人导航方法。PRM-RL模型包括一个强化学习代理，从噪声传感器中学习短距离障碍物避免，以及一个基于采样的规划器来映射导航空间。Shah
    *et al.* [[187](#bib.bib187)] 提出了ViNG，这是一种基于学习的导航系统，用于到达视觉指示的目标，并在实际移动机器人平台上展示了该系统。与以往工作不同，ViNG仅使用离线经验，不需要模拟器或在线数据收集，这显著提高了训练效率。映射 [[188](#bib.bib188)]
    和路径规划 [[189](#bib.bib189)] 也被广泛应用于许多现实世界的应用中。Davison *et al.* [[190](#bib.bib190)]
    构建了一个自动系统，能够在目标导向导航过程中检测、存储和跟踪合适的地标特征。他们展示了机器人如何使用主动视觉提供持续和准确的全球定位，从而实现高效导航。Sim
    *et al.* [[191](#bib.bib191)] 通过采用3D点地标的混合地图表示，使机器人能够准确定位其位置。
- en: 6 Navigation from Simulator to Real-world
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 从模拟器到现实世界的导航
- en: In this section, we first demonstrate the challenges in the real-world navigation
    by comparing the difference between simulated environments and the real-world
    environment. Then we introduce the methods that focus on solving these challenges.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先通过比较模拟环境与现实世界环境的差异来展示现实世界导航的挑战。然后，我们介绍了专注于解决这些挑战的方法。
- en: 6.1 Comparison of Simulated and Real Navigation
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 模拟与现实导航的比较
- en: 'Today, current achievements in the simulated navigation are still far of building
    a real-world navigation robot. Compared with the simulated environments, the real-world
    navigation environment is much more complex and ever changing. An comparison of
    inputs between a simulated environment (Habitat [[13](#bib.bib13)]) and the real-world
    environment is shown in Fig. [10](#S4.F10 "Figure 10 ‣ 4.2.5 Summary ‣ 4.2 Cross-modal
    Navigation ‣ 4 Methods in Simulated Environments ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey").'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '目前，模拟导航的现有成果仍然远未建立一个现实世界的导航机器人。与模拟环境相比，现实世界的导航环境要复杂得多且不断变化。模拟环境（Habitat [[13](#bib.bib13)])与现实世界环境之间的输入比较见图[10](#S4.F10
    "Figure 10 ‣ 4.2.5 Summary ‣ 4.2 Cross-modal Navigation ‣ 4 Methods in Simulated
    Environments ‣ Deep Learning for Embodied Visual Navigation Research: A Survey")。'
- en: 6.1.1 Reasons of Domain Gap
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 领域差距的原因
- en: 'We summarize three aspects cause the sim-real domain gap: 1) observation space;
    2) action space; 3) environmental dynamics.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了导致模拟与现实领域差距的三个方面：1) 观测空间；2) 动作空间；3) 环境动态。
- en: Observation Difference. An observation of the simulated environment can be an
    RGB image, a depth image, or a ground truth map. The quality of the RGB image
    and depth image inputs are high. The environment contains all static object information
    and enable it to provide ground truth information, like room structure, segmentation
    or object labels. The simulated environments provide unreal synthetic images with
    fewer objects where the real-world environments are far more complex with many.
    The sensors in the real-world environment, including RGB, GPS, and the velocity
    sensor, are usually noisy while the sensors in the simulated environment have
    no noise. Although some simulators [[11](#bib.bib11), [13](#bib.bib13), [14](#bib.bib14)]
    provide physical sensors and simulate some physical interactions (such as collision
    and acceleration), the performance of their physics engine is still far from real.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 观察差异。模拟环境中的观察可以是 RGB 图像、深度图像或真实地图。RGB 图像和深度图像输入的质量很高。环境包含所有静态物体信息，并提供真实信息，如房间结构、分割或物体标签。模拟环境提供的虚拟合成图像包含较少的物体，而现实世界环境则复杂得多，物体更多。现实世界环境中的传感器，包括
    RGB、GPS 和速度传感器，通常存在噪声，而模拟环境中的传感器没有噪声。尽管一些模拟器[[11](#bib.bib11), [13](#bib.bib13),
    [14](#bib.bib14)]提供了物理传感器并模拟了一些物理交互（如碰撞和加速度），但它们的物理引擎性能仍远未达到真实水平。
- en: Action Difference. Different from the simple action space consists of ‘turn
    left’, ‘turn right’, and ‘go forward’ in the simulated environment, the action
    space in the real world is more challenging, depending on the structure of the
    robot. Lots of obstacles exist during real-world navigation, which blocks the
    robot from turning or moving forward. Real-world environments are often dynamic
    since the environment is so complex that many factors are changing in the long
    term or short term, such as temperature, moisture, friction, obstacles, and pedestrians.
    Another challenge which is also widely ignored in the simulated environments is
    the complexity and instability of the action space. For example, the results of
    executing the same action are uncertain since the physical condition is evolving,
    such as the wheels are skidding or get stuck.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 行动差异。与模拟环境中仅包含“向左转”，“向右转”和“前进”的简单动作空间不同，现实世界中的动作空间更具挑战性，这取决于机器人结构。在现实世界导航中存在许多障碍物，这些障碍物阻挡了机器人转弯或前进。现实世界环境通常是动态的，因为环境非常复杂，许多因素在长期或短期内发生变化，如温度、湿度、摩擦、障碍物和行人。另一个通常在模拟环境中被忽视的挑战是动作空间的复杂性和不稳定性。例如，由于物理条件的变化，例如轮子打滑或卡住，相同动作的执行结果是不确定的。
- en: Environmental Dynamics. The evolving of environmental conditions, such as temperature,
    humidity or parts wear cause the environmental dynamics. A policy without online
    adaptation ability cannot handle this problem well. Recently, more and more attention
    has been paid to the adaptive policy of learning dynamic environment. Some works [[192](#bib.bib192),
    [193](#bib.bib193), [194](#bib.bib194)] propose simulated robot environments to
    accomplish this, however, the simulation is far simpler than the real-world.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 环境动态。环境条件的变化，如温度、湿度或零件磨损，会导致环境动态。没有在线适应能力的策略无法很好地处理这个问题。最近，越来越多的关注被集中在学习动态环境的自适应策略上。一些工作[[192](#bib.bib192),
    [193](#bib.bib193), [194](#bib.bib194)]提出了模拟机器人环境来实现这一点，但这些模拟仍远比现实世界简单。
- en: '![Refer to caption](img/0673e726984068fcd1337fa0790b99dd.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0673e726984068fcd1337fa0790b99dd.png)'
- en: 'Figure 11: The performances of methods on the Habitat *ObjectGoal* navigation,
    including DD-PPO [[195](#bib.bib195)], Active Exploration [[22](#bib.bib22)],
    SemExp [[126](#bib.bib126)] and 6-Act Tether [[96](#bib.bib96)].'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：方法在 Habitat *ObjectGoal* 导航中的表现，包括 DD-PPO [[195](#bib.bib195)]、主动探索 [[22](#bib.bib22)]、SemExp
    [[126](#bib.bib126)] 和 6-Act Tether [[96](#bib.bib96)]。
- en: '| Methods | R2R Validation Seen | R2R Validation Unseen | R2R Test Unseen |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | R2R 验证见过 | R2R 验证未见过 | R2R 测试未见过 |'
- en: '| TL | NE$\downarrow$ | SR$\uparrow$ | SPL$\uparrow$ | TL | NE$\downarrow$
    | SR$\uparrow$ | SPL$\uparrow$ | TL | NE$\downarrow$ | SR$\uparrow$ | SPL$\uparrow$
    |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| TL | NE$\downarrow$ | SR$\uparrow$ | SPL$\uparrow$ | TL | NE$\downarrow$
    | SR$\uparrow$ | SPL$\uparrow$ | TL | NE$\downarrow$ | SR$\uparrow$ | SPL$\uparrow$
    |'
- en: '| Random | 9.58 | 9.45 | 16 | - | 9.77 | 9.23 | 16 | - | 9.89 | 9.79 | 13 |
    12 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 9.58 | 9.45 | 16 | - | 9.77 | 9.23 | 16 | - | 9.89 | 9.79 | 13 | 12
    |'
- en: '| Human | - | - | - | - | - | - | - | - | 11.85 | 1.61 | 86 | 76 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | - | - | - | - | - | - | - | - | 11.85 | 1.61 | 86 | 76 |'
- en: '| Seq2Seq [[12](#bib.bib12)] | 11.33 | 6.01 | 39 | - | 8.39 | 7.81 | 22 | -
    | 8.13 | 7.85 | 20 | 18 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| Seq2Seq [[12](#bib.bib12)] | 11.33 | 6.01 | 39 | - | 8.39 | 7.81 | 22 | -
    | 8.13 | 7.85 | 20 | 18 |'
- en: '| Speaker-Follower [[24](#bib.bib24)] | - | 3.36 | 66 | - | - | 6.62 | 35 |
    - | 14.82 | 6.62 | 35 | 28 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| Speaker-Follower [[24](#bib.bib24)] | - | 3.36 | 66 | - | - | 6.62 | 35 |
    - | 14.82 | 6.62 | 35 | 28 |'
- en: '| RPA [[23](#bib.bib23)] | 8.46 | 5.56 | 43 | - | 7.22 | 7.65 | 25 | - | 9.15
    | 7.53 | 25 | - |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| RPA [[23](#bib.bib23)] | 8.46 | 5.56 | 43 | - | 7.22 | 7.65 | 25 | - | 9.15
    | 7.53 | 25 | - |'
- en: '| SMNA [[129](#bib.bib129)] | - | 3.22 | 67 | 58 | - | 5.52 | 45 | 32 | 18.04
    | 5.67 | 48 | 35 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| SMNA [[129](#bib.bib129)] | - | 3.22 | 67 | 58 | - | 5.52 | 45 | 32 | 18.04
    | 5.67 | 48 | 35 |'
- en: '| RCM+SIL [[128](#bib.bib128)] | 10.65 | 3.53 | 67 | - | 11.46 | 6.09 | 43
    | - | 11.97 | 6.12 | 43 | 38 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| RCM+SIL [[128](#bib.bib128)] | 10.65 | 3.53 | 67 | - | 11.46 | 6.09 | 43
    | - | 11.97 | 6.12 | 43 | 38 |'
- en: '| Regretful [[130](#bib.bib130)] | - | 3.23 | 69 | 63 | - | 5.32 | 50 | 41
    | 13.69 | 5.69 | 48 | 40 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Regretful [[130](#bib.bib130)] | - | 3.23 | 69 | 63 | - | 5.32 | 50 | 41
    | 13.69 | 5.69 | 48 | 40 |'
- en: '| PRESS* [[149](#bib.bib149)] | 10.57 | 4.39 | 58 | 55 | 10.36 | 5.28 | 49
    | 45 | 10.77 | 5.49 | 49 | 45 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| PRESS* [[149](#bib.bib149)] | 10.57 | 4.39 | 58 | 55 | 10.36 | 5.28 | 49
    | 45 | 10.77 | 5.49 | 49 | 45 |'
- en: '| FAST-Short [[131](#bib.bib131)] | - | - | - | - | 21.17 | 4.97 | 56 | 43
    | 22.08 | 5.14 | 54 | 41 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| FAST-Short [[131](#bib.bib131)] | - | - | - | - | 21.17 | 4.97 | 56 | 43
    | 22.08 | 5.14 | 54 | 41 |'
- en: '| EnvDrop [[137](#bib.bib137)] | 11.00 | 3.99 | 62 | 59 | 10.70 | 5.22 | 52
    | 48 | 11.66 | 5.23 | 51 | 47 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| EnvDrop [[137](#bib.bib137)] | 11.00 | 3.99 | 62 | 59 | 10.70 | 5.22 | 52
    | 48 | 11.66 | 5.23 | 51 | 47 |'
- en: '| AuxRN [[138](#bib.bib138)] | - | 3.33 | 70 | 67 | - | 5.28 | 55 | 50 | -
    | 5.15 | 55 | 51 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| AuxRN [[138](#bib.bib138)] | - | 3.33 | 70 | 67 | - | 5.28 | 55 | 50 | -
    | 5.15 | 55 | 51 |'
- en: '| PREVALENT* [[151](#bib.bib151)] | 10.32 | 3.67 | 69 | 65 | 10.19 | 4.71 |
    58 | 53 | 10.51 | 5.30 | 54 | 51 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| PREVALENT* [[151](#bib.bib151)] | 10.32 | 3.67 | 69 | 65 | 10.19 | 4.71 |
    58 | 53 | 10.51 | 5.30 | 54 | 51 |'
- en: '| Active Exploration [[196](#bib.bib196)] | 19.70 | 3.20 | 70 | 52 | 20.60
    | 4.36 | 58 | 40 | 21.6 | 4.33 | 60 | 41 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Active Exploration [[196](#bib.bib196)] | 19.70 | 3.20 | 70 | 52 | 20.60
    | 4.36 | 58 | 40 | 21.6 | 4.33 | 60 | 41 |'
- en: '| RelGraph [[144](#bib.bib144)] | 10.13 | 3.47 | 67 | 65 | 9.99 | 4.73 | 57
    | 53 | 10.29 | 4.75 | 55 | 52 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| RelGraph [[144](#bib.bib144)] | 10.13 | 3.47 | 67 | 65 | 9.99 | 4.73 | 57
    | 53 | 10.29 | 4.75 | 55 | 52 |'
- en: '| VLN $\circlearrowright$ BERT* [[152](#bib.bib152)] | 11.13 | 2.90 | 72 |
    68 | 12.01 | 3.93 | 63 | 57 | 12.35 | 4.09 | 63 | 57 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| VLN $\circlearrowright$ BERT* [[152](#bib.bib152)] | 11.13 | 2.90 | 72 |
    68 | 12.01 | 3.93 | 63 | 57 | 12.35 | 4.09 | 63 | 57 |'
- en: 'TABLE IV: Comparison of agent performance on R2R in single-run setting. *pretraining-based
    methods.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：在单次运行设置中代理性能的比较。*基于预训练的方法。
- en: 6.1.2 Solutions for the Domain Gap
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 领域差距的解决方案
- en: The domain gap brings critical challenges and researchers put forward methods
    to fill the gap between these two settings. Mobile robot navigation is considered
    a geometric problem, which requires the robot to sense the geometric shape of
    the environment in order to plan collision-free paths to reach the target. Obstacle
    avoidance is one of the most important challenges, and many methods [[27](#bib.bib27),
    [176](#bib.bib176), [186](#bib.bib186)] have been put forward in previous work
    to achieve this. However, robot navigation in simulated tasks are regarded as
    a policy learning problem that learns a robust navigation policy from a starting
    position to the target in a complex environment with many possible routes. SLAM-based
    methods as in [[60](#bib.bib60), [116](#bib.bib116)] contribute a lot to mapping
    and path planning, which is general for both simulated and real navigation. Deep
    learning shows its ability in processing images and learning policies for robotic
    control, which is widely applied in both settings. However, the usages of deep
    learning are different between simulated navigation and real navigation. In real-world
    navigation, the deep neural network is used to perceive RGB inputs [[177](#bib.bib177)],
    predict the future [[197](#bib.bib197)] and learn the navigation policy [[186](#bib.bib186)].
    However, due to the sampling inefficiency and the complex dynamic factors of the
    real-world environment, the policy is not robust enough. Some works [[185](#bib.bib185),
    [184](#bib.bib184)] propose to model the environment and other works [[186](#bib.bib186)]
    adopt handcrafted rules to improve the robustness of the navigation policy. Data
    sampling is much more efficient in simulated environments. Most of the simulators
    render RGB and depth images in more than hundreds of frames per second (FPS),
    in which the fastest simulator, Habitat [[13](#bib.bib13)], achieves 100,000 FPS.
    Fast data sampling enables learning with large batch size. Many works prove that
    a large training batch size leads to robustness in representations [[198](#bib.bib198),
    [45](#bib.bib45)]. In spite of the rendered RGB and depth images, some simulated
    environments are able to provide semantic segmentation masks [[157](#bib.bib157),
    [14](#bib.bib14), [13](#bib.bib13)]. A more accurate simulator with few noise
    to facilitate training. With richer, noise-free data, researchers can apply a
    deeper neural network on navigation agents without worrying about overfitting.
    For example, Transformer [[117](#bib.bib117)] is widely applied in navigation
    works in simulated environments due to its capability of feature representation
    while it is easily overfitting if it is trained on noisy data.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 领域差距带来了关键挑战，研究人员提出了填补这两种设置之间差距的方法。移动机器人导航被视为几何问题，这要求机器人感知环境的几何形状，以规划无碰撞的路径以到达目标。避障是最重要的挑战之一，许多方法 [[27](#bib.bib27),
    [176](#bib.bib176), [186](#bib.bib186)] 在之前的工作中被提出以实现这一目标。然而，机器人在模拟任务中的导航被视为一个策略学习问题，该问题从起始位置到目标位置在复杂环境中学习一个稳健的导航策略，有许多可能的路线。基于SLAM的方法如 [[60](#bib.bib60),
    [116](#bib.bib116)] 对于映射和路径规划贡献很大，这对模拟和真实导航都适用。深度学习在处理图像和学习机器人控制策略方面展示了其能力，这在这两种设置中都被广泛应用。然而，深度学习的应用在模拟导航和真实导航之间是不同的。在真实世界导航中，深度神经网络用于感知RGB输入 [[177](#bib.bib177)]、预测未来 [[197](#bib.bib197)]
    和学习导航策略 [[186](#bib.bib186)]。然而，由于采样效率低和真实世界环境的复杂动态因素，策略的稳健性不足。一些工作 [[185](#bib.bib185),
    [184](#bib.bib184)] 提出了建模环境，而其他工作 [[186](#bib.bib186)] 采用手工规则来提高导航策略的稳健性。数据采样在模拟环境中效率更高。大多数模拟器每秒渲染RGB和深度图像超过数百帧（FPS），其中最快的模拟器Habitat [[13](#bib.bib13)]
    实现了100,000 FPS。快速的数据采样使得使用大批量数据进行学习成为可能。许多工作证明，大的训练批量大小导致了表示的稳健性 [[198](#bib.bib198),
    [45](#bib.bib45)]。尽管渲染的RGB和深度图像，一些模拟环境能够提供语义分割掩码 [[157](#bib.bib157), [14](#bib.bib14),
    [13](#bib.bib13)]。更准确的模拟器具有较少噪声以促进训练。借助更丰富、无噪声的数据，研究人员可以在导航代理上应用更深的神经网络，而不必担心过拟合。例如，Transformer [[117](#bib.bib117)]
    因其特征表示能力而在模拟环境中的导航工作中被广泛应用，但如果在噪声数据上训练，它容易过拟合。
- en: '![Refer to caption](img/32cf244e6e259c1736b0e35693a2ce61.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/32cf244e6e259c1736b0e35693a2ce61.png)'
- en: 'Figure 12: The performances of methods on the Habitat *PointGoal* Challenge,
    including DD-PPO [[195](#bib.bib195)], ego-localization [[99](#bib.bib99)], Occupancy
    Anticipation [[199](#bib.bib199)] and SLAM-net.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：在Habitat *PointGoal* 挑战中方法的表现，包括DD-PPO [[195](#bib.bib195)]、自我定位 [[99](#bib.bib99)]、占用预测 [[199](#bib.bib199)]
    和SLAM-net。
- en: 6.1.3 Learning Efficiency
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3 学习效率
- en: Many researchers focus on learning efficiency since data sampling in the real
    world is slow and expensive. Lobos-Tsunekawa *et al.* [[200](#bib.bib200)] propose
    a map-less visual navigation method for biped humanoid robots. In this method,
    DDPG algorithm [[77](#bib.bib77)] is used to extract information from color images,
    so as to derive motion commands. This method runs 20 ms on a physical robot, allowing
    its use in real-time applications. Bruce *et al.* [[201](#bib.bib201)] present
    a method for learning to navigate to a fixed goal on a mobile robot. By using
    an interactive replay of a single traversal of the environment and stochastic
    environmental augmentation, Bruce *et al.* demonstrates zero-shot transfer under
    real-world environmental variations without fine-tuning. To further improve the
    sampling efficiency, Pfeiffer *et al.* [[202](#bib.bib202)] leverage prior expert
    demonstrations for pre-training so that the training cost could be largely reduced
    in the fine-tuning process.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究者关注学习效率，因为现实世界中的数据采样既慢又昂贵。Lobos-Tsunekawa *等人* [[200](#bib.bib200)] 提出了一个无需地图的视觉导航方法，用于双足类人机器人。在此方法中，使用DDPG算法
    [[77](#bib.bib77)] 从彩色图像中提取信息，以生成运动指令。该方法在物理机器人上运行20毫秒，使其可以用于实时应用。Bruce *等人* [[201](#bib.bib201)]
    提出了一种学习移动机器人在固定目标处导航的方法。通过使用对环境单次遍历的互动回放和随机环境增强，Bruce *等人* 在真实世界环境变化下展示了零样本迁移，无需微调。为了进一步提高采样效率，Pfeiffer
    *等人* [[202](#bib.bib202)] 利用先前专家演示进行预训练，以便在微调过程中大幅降低训练成本。
- en: 6.2 Navigation Transferring
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 导航迁移
- en: 'Transfer learning is attracting rising attention in embodied navigation. The
    researchers are motivated from two aspects: 1) learn a navigation agent that is
    able to perform accurate and efficient navigation in diverse domains and tasks;
    2) deploy an agent trained in a simulated environment in a real-world navigation
    robot.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习在具身导航中受到越来越多的关注。研究者们从两个方面受到激励：1）学习一种能够在不同领域和任务中执行准确高效导航的智能体；2）将训练于模拟环境中的智能体部署到现实世界的导航机器人中。
- en: It is challenging to train a model to learn skills for navigating in different
    domains. Moreover, due to the large domain gap between simulated environments
    and the real-world environment, a well-performed navigation policy trained on
    a simulated environment cannot be easily transferred to the real-world environment.
    A lot of navigation tasks have been proposed to investigate different capabilities
    for navigation in diverse scenarios.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个模型以学习在不同领域中导航的技能具有挑战性。此外，由于模拟环境和现实世界环境之间的巨大领域差距，训练于模拟环境中的高效导航策略不能轻易迁移到现实世界环境中。已经提出了许多导航任务以研究在不同场景下的导航能力。
- en: 'In this section, we discuss the transfer learning in navigation from two different
    levels: 1) task-level transferring; 2) environment-level transferring, including
    sim-to-real transferring. The task-level transferring requires the agent to learn
    a policy that adapts to different input modalities or targets; the environment-level
    transferring requires the model to be invariant to different dynamics and transition
    functions.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从两个不同的层面讨论导航中的迁移学习：1）任务级迁移；2）环境级迁移，包括模拟到现实的迁移。任务级迁移要求智能体学习适应不同输入模态或目标的策略；环境级迁移要求模型对不同的动态和转换函数保持不变。
- en: DisCoRL[[28](#bib.bib28)] introduce a policy distillation method [[203](#bib.bib203)]
    to transfer a 2D navigation policy. In addition to the navigation policy, the
    vision and language embedding layer could also be transferred  [[29](#bib.bib29)].
    Motivated by the success of meta-learning [[204](#bib.bib204)], Dimension-variable
    skill transfer (DVST) [[205](#bib.bib205)] obtains a meta-agent with deep reinforcement
    learning and then transfers the meta-skill to a robot with a different dimensional
    configuration using a method named dimension-variable skill transfer. Similarly,
    Li *et al.* [[206](#bib.bib206)] propose an unsupervised reinforcement learning
    method to learn transferable meta-skills. Zhu *et al.* [[63](#bib.bib63)] decompose
    long navigation instructions into shorter ones, and thus enables the model to
    be easily transferred to navigation tasks with longer trajectories. Chaplot *et
    al.* [[207](#bib.bib207)] propose a multi-task model that jointly learns multi-modal
    tasks, and transfers vision-language knowledge across the tasks. The model adopts
    a Dual-Attention unit to disentangle the vision knowledge and language knowledge
    and align them with each other.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: DisCoRL[[28](#bib.bib28)] 引入了一种策略蒸馏方法 [[203](#bib.bib203)] 来传递二维导航策略。除了导航策略外，视觉和语言嵌入层也可以被转移
    [[29](#bib.bib29)]。受到元学习成功的启发 [[204](#bib.bib204)]，维度变量技能转移 (DVST) [[205](#bib.bib205)]
    通过深度强化学习获得一个元代理，然后使用名为维度变量技能转移的方法将元技能转移到具有不同维度配置的机器人上。类似地，李 *等人* [[206](#bib.bib206)]
    提出了一个无监督强化学习方法来学习可转移的元技能。朱 *等人* [[63](#bib.bib63)] 将长导航指令分解为较短的指令，从而使模型能够轻松转移到具有更长轨迹的导航任务中。Chaplot
    *等人* [[207](#bib.bib207)] 提出了一个多任务模型，该模型共同学习多模态任务，并在任务间转移视觉-语言知识。该模型采用双重注意力单元来解耦视觉知识和语言知识，并使它们相互对齐。
- en: '![Refer to caption](img/e1cefb4e349a3e769d9a78dbfeaa8e71.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e1cefb4e349a3e769d9a78dbfeaa8e71.png)'
- en: 'Figure 13: A summary of future directions for building an advanced robotic
    for real-world navigation.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：构建先进机器人以应对真实世界导航的未来方向总结。
- en: Wang *et al.* [[208](#bib.bib208)] propose to learn environment-agnostic representations
    for the navigation policy enables the model to perform on both Vision-Language
    Navigation (VLN) and Navigation from Dialog History (NDH) tasks. Yan *et al.* [[30](#bib.bib30)]
    propose MVV-IN, a method that acquires transferable meta-skills with multi-modal
    inputs to cope with new tasks. Liu *et al.* [[209](#bib.bib209)] investigate on
    how to make robots fuse and transfer their experience so that they can effectively
    use prior knowledge and quickly adapt to new environments. Gordon *et al.* [[210](#bib.bib210)]
    propose to decouple the visual perception and policy to facilitates transfer to
    new environments and tasks.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 王 *等人* [[208](#bib.bib208)] 提出了学习环境无关表示的方法，以便在视觉-语言导航 (VLN) 和对话历史导航 (NDH) 任务中都能表现良好。燕
    *等人* [[30](#bib.bib30)] 提出了 MVV-IN，这是一种通过多模态输入获得可转移的元技能的方法，以应对新任务。刘 *等人* [[209](#bib.bib209)]
    研究了如何使机器人融合和转移其经验，以便有效地利用先验知识并迅速适应新环境。戈登 *等人* [[210](#bib.bib210)] 提出了将视觉感知和策略解耦的方法，以促进在新环境和任务中的转移。
- en: Sim-real transferring have been well studied in the field of robotic control [[211](#bib.bib211),
    [212](#bib.bib212)]. Sadeghi *et al.* [[213](#bib.bib213)] firstly propose a learning-based
    method, which trains a navigation agent entirely in a simulator and then transfers
    it into real-world environments without finetuning on any real images. Consequently,
    Yuan *et al.* [[214](#bib.bib214)] adopt a sim-real transfer strategy for learning
    navigation controllers using an end-to-end policy that maps raw pixels as visual
    input to control actions without any form of engineered feature extraction. Tai
    *et al.* [[215](#bib.bib215)] train a robot in simulation with Asynchronous DDPG [[77](#bib.bib77)]
    algorithm and directly deployed the learned controller to a real robot for navigation
    transferring. Rusu *et al.* [[212](#bib.bib212)] introduce a progressive network
    to transfer the learned policies from simulation to the real world. Similarly,
    adversarial feature adaptation methods [[216](#bib.bib216)] is also applicable
    in sim-to-real policy transferring [[217](#bib.bib217)]. Sim-to-real transfer
    for deep reinforcement learning policies can be applied to complex navigation
    tasks [[218](#bib.bib218)], including six-legged robots [[219](#bib.bib219)],
    robots for soccer competitions [[220](#bib.bib220)], etc.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 真实与模拟环境的转移在机器人控制领域已经得到了很好的研究[[211](#bib.bib211), [212](#bib.bib212)]。Sadeghi
    *等人*[[213](#bib.bib213)] 首先提出了一种基于学习的方法，该方法在模拟器中完全训练一个导航代理，然后将其转移到现实世界环境中，而无需在任何真实图像上进行微调。因此，Yuan
    *等人*[[214](#bib.bib214)] 采用了一种模拟-现实转移策略，通过端到端策略将原始像素作为视觉输入映射到控制动作，而不进行任何形式的特征工程提取。Tai
    *等人*[[215](#bib.bib215)] 使用异步DDPG[[77](#bib.bib77)] 算法在模拟环境中训练机器人，并将学习到的控制器直接部署到真实机器人上进行导航转移。Rusu
    *等人*[[212](#bib.bib212)] 引入了一种渐进网络，将学习到的策略从模拟环境转移到现实世界。同样，敌对特征适应方法[[216](#bib.bib216)]
    也适用于模拟到现实的策略转移[[217](#bib.bib217)]。深度强化学习策略的模拟到现实转移可以应用于复杂的导航任务[[218](#bib.bib218)]，包括六足机器人[[219](#bib.bib219)]、足球比赛机器人[[220](#bib.bib220)]等。
- en: 6.3 Summary
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 总结
- en: In this section, We firstly compare the difference between simulated environments
    and real-world environments. Then, we reason about the domain gaps that cause
    the domain gaps. Finally, we introduce some transfer learning works in the navigation
    to give a promising direction to solve this problem.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先比较模拟环境和现实世界环境之间的差异。然后，我们推理导致领域差距的原因。最后，我们介绍了一些在导航领域的迁移学习工作，为解决这个问题提供了一个有前景的方向。
- en: 7 Future Directions
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 未来方向
- en: 'Although extensive works have addressed the navigation problem from diverse
    aspects, current research progress is still far from real artificial intelligence.
    Also, current work cannot build a robust robot for real-world navigation. We summarize
    the challenges in solving embodied AI into these aspects: 1) the functions and
    the performance are limited by the embodied environment; 2) the navigation problem
    is not well defined; 3) the performances of embodied AI agents in complex environments
    are still poor; 4) perceiving natural language is difficult to learn; 5) hard
    to deploy a trained navigation policy to the real-world application.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大量工作已经从不同方面解决了导航问题，但目前的研究进展仍远未达到真正的人工智能水平。此外，当前的工作无法为现实世界导航构建一个强健的机器人。我们将解决具身AI的挑战总结为以下几个方面：1)
    功能和性能受限于具身环境；2) 导航问题定义不明确；3) 复杂环境中的具身AI代理的表现仍然较差；4) 学习自然语言感知困难；5) 将训练好的导航策略部署到现实世界应用中困难。
- en: Future Embodied Environments. The advanced functions in the environment help
    the navigation model to obtain high-level abilities. For instance, compared to
    the early embodied environments, the large scene in the Matterport3D [[36](#bib.bib36)]
    firstly requires the navigation model to explore and memorize the complex room
    structure. The vision-language navigation benchmark [[12](#bib.bib12)] enables
    the agents to perceive natural language. The interactive embodied environment
    like AI2-THOR [[40](#bib.bib40)] and iGibson [[41](#bib.bib41)] enable the agent
    to perform interactive actions. The agent learned in an interactive environment
    is able to move an object, put an object and open a door.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的具身环境。环境中的高级功能有助于导航模型获得高级能力。例如，与早期的具身环境相比，Matterport3D[[36](#bib.bib36)] 中的大场景首先要求导航模型探索并记忆复杂的房间结构。视觉-语言导航基准[[12](#bib.bib12)]
    使得代理能够感知自然语言。像AI2-THOR[[40](#bib.bib40)] 和iGibson[[41](#bib.bib41)] 这样的互动具身环境使代理能够执行交互动作。在互动环境中学习的代理能够移动物体、放置物体和开门。
- en: An environment with more functions is the basis of learning a smart agent. An
    agent must be able to handle a dynamic environment when the objects in the rooms
    with evolving conditions. In stead of navigating within the navigable areas like [[12](#bib.bib12),
    [13](#bib.bib13)], we expect an agent to find possible roads within a room that
    has many obstacles. In addition, we need an interactive agent, which can pick
    up and put down objects, move chairs, and interact with human beings. Other modes
    such as walking, running, and climbing also need to be considered if we want to
    build a robust navigator within a complex indoor environment.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有更多功能的环境是学习智能体的基础。一个智能体必须能够处理动态环境中的对象，当房间里的条件不断变化时。在可以导航的区域如[[12](#bib.bib12),
    [13](#bib.bib13)]中导航只是其中之一，我们期望智能体能够在充满障碍物的房间内找到可能的道路。此外，我们需要一个互动性强的智能体，它可以捡起和放下物体、移动椅子，并与人类互动。如果我们想在复杂的室内环境中构建一个强大的导航系统，还需要考虑行走、奔跑和攀爬等其他模式。
- en: 'Define Advanced Navigation Tasks. Even though many embodied navigation tasks
    and navigation metrics have been proposed, what is a good navigation policy remains
    unclear. This problem has two folds: 1) what factors have to be considered; 2)
    how to balance these factors. As we analysed in Sec. [3.4](#S3.SS4 "3.4 Evaluation
    Metrics ‣ 3 Embodied Navigation Benchmarks ‣ Deep Learning for Embodied Visual
    Navigation Research: A Survey"), the accuracy and efficiency are two main factors
    to evaluate the performance of navigation. However, the importance of accuracy
    and efficiency are different among metrics. Optimal navigation policy varies according
    to different metrics. In the interactive navigation task proposed by [[221](#bib.bib221)],
    the performance of the agent is evaluated by a path efficiency score and a effort
    efficiency score. The interactive navigation task varies the score weights in
    evaluation to test if an agent performs well in different settings. However, it
    is still unclear how the weight of the factors affects the test results and what
    setting it is in real-world application.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '定义高级导航任务。尽管已经提出了许多具身导航任务和导航指标，但什么是好的导航策略仍然不清楚。这个问题有两个方面：1) 需要考虑哪些因素；2) 如何平衡这些因素。正如我们在第[3.4节](#S3.SS4
    "3.4 Evaluation Metrics ‣ 3 Embodied Navigation Benchmarks ‣ Deep Learning for
    Embodied Visual Navigation Research: A Survey")分析的那样，准确性和效率是评估导航性能的两个主要因素。然而，准确性和效率在不同指标中的重要性有所不同。最优的导航策略会根据不同的指标有所变化。在[[221](#bib.bib221)]提出的互动导航任务中，智能体的性能通过路径效率分数和努力效率分数来评估。互动导航任务在评估中变化分数权重，以测试智能体在不同设置下的表现。然而，因素权重如何影响测试结果以及在实际应用中的设置仍不清楚。'
- en: Moreover, in the questioning-answering settings like Help Anna [[59](#bib.bib59)],
    or RMM [[166](#bib.bib166)], the frequency of questioning or requesting from the
    agent are take into consideration. Current methods regard that the performance
    is lower if the agent ask for more information from human. Balancing the ‘cost’
    of asking questions and the ‘cost’ of navigation is still a challenging problem.
    We hope that the community could publish more works discussing on how to evaluate
    the advanced navigation behavior or comparing the differences of navigation policies
    between an agent and a human.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在类似Help Anna[[59](#bib.bib59)]或RMM[[166](#bib.bib166)]的问答设置中，需要考虑智能体提问或请求的频率。目前的方法认为，如果智能体向人类请求更多信息，则性能较低。平衡提问的“成本”和导航的“成本”仍然是一个具有挑战性的问题。我们希望社区能发布更多关于如何评估高级导航行为或比较智能体与人类导航策略差异的研究。
- en: 'Improve Navigation Performance. Current navigation agents still perform poorly
    even in easy navigation tasks such as *PointGoal* task and *ObjectGoal* task,
    as shown in Fig. [12](#S6.F12 "Figure 12 ‣ 6.1.2 Solutions for the Domain Gap
    ‣ 6.1 Comparison of Simulated and Real Navigation ‣ 6 Navigation from Simulator
    to Real-world ‣ Deep Learning for Embodied Visual Navigation Research: A Survey")
    and Fig. [11](#S6.F11 "Figure 11 ‣ 6.1.1 Reasons of Domain Gap ‣ 6.1 Comparison
    of Simulated and Real Navigation ‣ 6 Navigation from Simulator to Real-world ‣
    Deep Learning for Embodied Visual Navigation Research: A Survey"). The state-of-the
    art model of the *PointGoal* task performs 64.5% on success rate (SR) and 37.7%
    on SPL, while on the *ObjectGoal* task, the best model performs 21.08% on success
    rate (SR) and 8.38% on SPL. As shown in Fig. [IV](#S6.T4 "TABLE IV ‣ 6.1.1 Reasons
    of Domain Gap ‣ 6.1 Comparison of Simulated and Real Navigation ‣ 6 Navigation
    from Simulator to Real-world ‣ Deep Learning for Embodied Visual Navigation Research:
    A Survey"), the current state-of-the-art model [[152](#bib.bib152)] in the vision-language
    navigation task performs 63% in SR and 57% in SPL while the human performance
    is 86% in SR and 76% in SPL. The navigation performances of current models are
    still far from human performance. Besides, existing models usually perform poorly
    on the challenging task of vision-and-language navigation. As shown in Tab. [IV](#S6.T4
    "TABLE IV ‣ 6.1.1 Reasons of Domain Gap ‣ 6.1 Comparison of Simulated and Real
    Navigation ‣ 6 Navigation from Simulator to Real-world ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey"), there is about 19% performance gap between
    the state-of-the-art model and the human baseline. In the interactive dialog tasks [[59](#bib.bib59),
    [166](#bib.bib166), [222](#bib.bib222)], the natural language used by agents to
    interact with oracle has many errors and is not fluent. The baselines in the recently
    proposed tasks [[221](#bib.bib221)] can hardly complete the task. Moreover, the
    navigation robot in the real world cannot perform as well as in the simulated
    environment.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '提升导航性能。当前的导航代理在*PointGoal*任务和*ObjectGoal*任务等简单导航任务中的表现仍然很差，如图[12](#S6.F12 "Figure
    12 ‣ 6.1.2 Solutions for the Domain Gap ‣ 6.1 Comparison of Simulated and Real
    Navigation ‣ 6 Navigation from Simulator to Real-world ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey")和图[11](#S6.F11 "Figure 11 ‣ 6.1.1 Reasons
    of Domain Gap ‣ 6.1 Comparison of Simulated and Real Navigation ‣ 6 Navigation
    from Simulator to Real-world ‣ Deep Learning for Embodied Visual Navigation Research:
    A Survey")所示。*PointGoal*任务的最先进模型在成功率（SR）上达到64.5%，在SPL上达到37.7%；而在*ObjectGoal*任务中，最佳模型在成功率（SR）上为21.08%，在SPL上为8.38%。如图[IV](#S6.T4
    "TABLE IV ‣ 6.1.1 Reasons of Domain Gap ‣ 6.1 Comparison of Simulated and Real
    Navigation ‣ 6 Navigation from Simulator to Real-world ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey")所示，当前在视觉-语言导航任务中的最先进模型[[152](#bib.bib152)]在SR上表现为63%，在SPL上为57%，而人类的表现则为SR
    86%和SPL 76%。当前模型的导航性能仍然远远落后于人类表现。此外，现有模型在视觉-语言导航这一具有挑战性的任务上通常表现不佳。如表[IV](#S6.T4
    "TABLE IV ‣ 6.1.1 Reasons of Domain Gap ‣ 6.1 Comparison of Simulated and Real
    Navigation ‣ 6 Navigation from Simulator to Real-world ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey")所示，最先进模型与人类基线之间存在约19%的性能差距。在交互对话任务[[59](#bib.bib59),
    [166](#bib.bib166), [222](#bib.bib222)]中，代理用于与oracle互动的自然语言存在许多错误且不够流畅。最近提出的任务中的基线[[221](#bib.bib221)]几乎无法完成任务。此外，现实世界中的导航机器人无法像在模拟环境中那样表现出色。'
- en: Several promising directions, which are motivated by referring recent works,
    could tackle these problems. Transformer [[117](#bib.bib117)] shows its capability
    in feature extraction and cross-modal fusion. Some works [[151](#bib.bib151),
    [152](#bib.bib152)] build navigation models based on Transformer and achieve great
    success in vision-language navigation task, which reveals that Transformer structure
    is beneficial for cross-modal navigation policy. Chaplot *et al.* [[125](#bib.bib125)]
    build a neural SLAM module into a navigation model and train the model via hierarchical
    reinforcement learning. This model is able to learn the structure of the room
    and perform a robust low-level navigation policy in an environment with continuous
    state space. We suggest that model-based methods [[96](#bib.bib96), [138](#bib.bib138)]
    and hierarchical reinforcement learning [[223](#bib.bib223)] are the key to build
    a robust navigation model.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 受最近工作的启发，有几个有前景的方向可以解决这些问题。Transformer [[117](#bib.bib117)] 显示了其在特征提取和跨模态融合方面的能力。一些工作 [[151](#bib.bib151),
    [152](#bib.bib152)] 基于 Transformer 构建了导航模型，并在视觉-语言导航任务中取得了巨大成功，这表明 Transformer
    结构对跨模态导航策略是有利的。Chaplot *et al.* [[125](#bib.bib125)] 将神经 SLAM 模块构建到导航模型中，并通过分层强化学习训练该模型。该模型能够学习房间的结构，并在具有连续状态空间的环境中执行稳健的低级导航策略。我们建议模型驱动的方法 [[96](#bib.bib96),
    [138](#bib.bib138)] 和分层强化学习 [[223](#bib.bib223)] 是构建稳健导航模型的关键。
- en: Smartly Perceiving Natural Language. Natural language is a complex modality
    for a robot to understand due to its diversity and complexity. At this moment,
    however, teach a navigation robot to learn to understand language requires a large
    amount of natural language annotations and each of them describes the semantics
    of a trajectory, a scene or a kind of behavior. The language annotations can be
    a word, a sentence, a question-answer pair or a dialogue, which are pretty expensive
    and labor-intensive.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 智能感知自然语言。自然语言对于机器人来说是一种复杂的模态，因为其多样性和复杂性。然而，目前，教一个导航机器人学习理解语言需要大量的自然语言注释，每一个注释描述了一个路径、一个场景或一种行为的语义。语言注释可以是一个词、一句话、一个问答对或一个对话，这些都相当昂贵且劳动密集。
- en: Even if we have sufficient language annotations for training a navigation robot,
    it is still challenging for the robot to correctly understand language instructions.
    For example, because there are many natural language variants for describing the
    same trajectory or scene, supervising an agent with trajectory-instruction pairs
    may led to severe overfitting. In addition, the skill of perceiving natural language
    needs prior knowledge. For example, “find the forth chair in the living room”
    requires the agent be able to count and “navigate to bathroom safely” requires
    the agent to turn smoothly and do not touch any objects. Some works [[151](#bib.bib151),
    [152](#bib.bib152)] adopt pre-training methods to obtain a better language understanding
    skill with prior knowledge. Based on the success of these works, we believe that
    learning from other large-scale language datasets [[224](#bib.bib224), [225](#bib.bib225),
    [147](#bib.bib147)] and transfer the prior knowledge might be a promising direction
    in solving the challenges in understanding natural language instructions.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们拥有足够的语言注释来训练导航机器人，机器人正确理解语言指令仍然具有挑战性。例如，由于描述同一路径或场景的自然语言变体众多，使用路径-指令对来监督代理可能导致严重的过拟合。此外，感知自然语言的技能需要先验知识。例如，“找到客厅中的第四把椅子”要求代理能够进行计数，而“安全导航到浴室”要求代理平稳转弯并避免碰触任何物体。一些工作 [[151](#bib.bib151),
    [152](#bib.bib152)] 采用预训练方法以获得更好的语言理解技能和先验知识。基于这些工作的成功，我们认为从其他大规模语言数据集 [[224](#bib.bib224),
    [225](#bib.bib225), [147](#bib.bib147)] 学习并转移先验知识可能是解决理解自然语言指令挑战的一个有前景的方向。
- en: 'Deploy Robust Policies on Real World. Even though we have obtained a robust
    navigation policy in a simulated environment, how to deploy this policy to real-world
    still remains challenging. As demonstrated in Sec. [6.1](#S6.SS1 "6.1 Comparison
    of Simulated and Real Navigation ‣ 6 Navigation from Simulator to Real-world ‣
    Deep Learning for Embodied Visual Navigation Research: A Survey"), three major
    differences cause the large sim-real domain gap: 1) observation; 2) action space;
    3) environmental dynamics. Large sim-real domain gap hinders the direct deployment
    of the learned navigation policy to the real world. There are two directions in
    tackling this problem. One way is building a realistic simulator, including a
    realistic visual image rendering mechanism, advanced physical sensors, obstacle
    objects, dynamic simulation, simulation of robot components like wheels and gears,
    etc. However, such a realistic visual simulator is computation costly. Another
    way of solving sim-real deployment problem is achieving online adaptation by transfer
    learning or meta-reinforcement learning [[226](#bib.bib226), [204](#bib.bib204)].
    These methods enable an agent to change its policy to adapt the environment. This
    method not only has high computational efficiency, but also has stronger adaptability
    when accidents happen.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中部署稳健的策略。尽管我们在模拟环境中获得了稳健的导航策略，但如何将该策略部署到现实世界仍然具有挑战性。如第[6.1节](#S6.SS1 "6.1
    模拟与真实导航的比较 ‣ 从模拟器到现实世界的导航 ‣ 具身视觉导航研究的深度学习综述")所示，三个主要差异导致了模拟-现实领域之间的巨大差距：1) 观测；2)
    动作空间；3) 环境动态。模拟-现实领域之间的巨大差距阻碍了所学导航策略的直接部署。有两种解决这一问题的方向。一种方法是构建一个逼真的模拟器，包括逼真的视觉图像渲染机制、先进的物理传感器、障碍物、动态模拟、机器人组件如轮子和齿轮的模拟等。然而，这种逼真的视觉模拟器计算成本高。解决模拟-现实部署问题的另一种方法是通过迁移学习或元强化学习实现在线适应[[226](#bib.bib226),
    [204](#bib.bib204)]。这些方法使代理能够调整其策略以适应环境。这种方法不仅具有高计算效率，而且在发生意外时具有更强的适应能力。
- en: 8 Conclusion
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: This paper presented a comprehensive survey on the embodied navigation scenario
    by summarizing hundreds of works. We thoroughly investigate the environments,
    tasks, and metrics to introduce the problem that the researchers are trying to
    solve. And we introduce hundreds of methods that solve these tasks in the embodied
    environments and compare their differences. Then we introduce the methods in the
    real-world environment and demonstrate how the large domain gap led to the drop
    in navigation performance. At last, we analyze the current problems that exist
    in the embodied navigation and give out four future directions to improve our
    community.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 本文通过总结数百项工作，呈现了对具身导航场景的全面调查。我们深入调查了环境、任务和指标，以介绍研究人员试图解决的问题。我们介绍了数百种在具身环境中解决这些任务的方法，并比较了它们的差异。然后，我们介绍了现实世界环境中的方法，并展示了领域差距如何导致导航性能下降。最后，我们分析了具身导航中存在的当前问题，并提出了四个未来方向以改善我们的社区。
- en: References
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *Communications of The ACM*, vol. 60,
    no. 6, pp. 84–90, 2017.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton，“使用深度卷积神经网络进行Imagenet分类，” *ACM通讯*，第60卷，第6期，第84–90页，2017年。'
- en: '[2] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Y. LeCun, Y. Bengio, 和 G. Hinton，“深度学习，” *自然*，第521卷，第7553期，第436–444页，2015年。'
- en: '[3] S. Thrun, *Probabilistic Robotics*, 2005.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] S. Thrun，*概率机器人学*，2005年。'
- en: '[4] P. Dourish, *Where the Action Is: The Foundations of Embodied Interaction*,
    2001.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] P. Dourish，*行动所在：具身互动的基础*，2001年。'
- en: '[5] T. Kruse, A. K. Pandey, R. Alami, and A. Kirsch, “Human-aware robot navigation:
    A survey,” *Robotics and Autonomous Systems*, vol. 61, no. 12, pp. 1726–1743,
    2013.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] T. Kruse, A. K. Pandey, R. Alami, 和 A. Kirsch，“人类感知的机器人导航：综述，” *机器人与自主系统*，第61卷，第12期，第1726–1743页，2013年。'
- en: '[6] G. N. DeSouza and A. C. Kak, “Vision for mobile robot navigation: A survey,”
    *TPAMI*, vol. 24, no. 2, pp. 237–267, 2002.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] G. N. DeSouza 和 A. C. Kak，“移动机器人导航的视觉：综述，” *TPAMI*，第24卷，第2期，第237–267页，2002年。'
- en: '[7] S. Thrun, “Probabilistic algorithms in robotics,” *Ai Magazine*, vol. 21,
    no. 4, pp. 93–109, 2000.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] S. Thrun，“机器人中的概率算法，” *AI杂志*，第21卷，第4期，第93–109页，2000年。'
- en: '[8] A. Kosaka and A. Kak, “Fast vision-guided mobile robot navigation using
    model-based reasoning and prediction of uncertainties,” in *IROS*, vol. 3, 1992,
    pp. 2177–2186.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Kosaka 和 A. Kak，“使用基于模型的推理和不确定性预测的快速视觉引导移动机器人导航，” 见 *IROS*，第3卷，1992年，第2177–2186页。'
- en: '[9] M. Kabuka and A. Arenas, “Position verification of a mobile robot using
    standard pattern,” *ICRA*, vol. 3, no. 6, pp. 505–516, 1987.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] M. Kabuka 和 A. Arenas，“使用标准模式对移动机器人进行位置验证，”*ICRA*，第3卷，第6期，第505–516页，1987年。'
- en: '[10] Y. Wu, Y. Wu, G. Gkioxari, and Y. Tian, “Building generalizable agents
    with a realistic and rich 3d environment,” in *ICLR (Workshop)*, 2018.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Y. Wu, Y. Wu, G. Gkioxari, 和 Y. Tian，“利用真实且丰富的3D环境构建通用代理，”发表于*ICLR (Workshop)*，2018年。'
- en: '[11] M. Savva, A. X. Chang, A. Dosovitskiy, T. A. Funkhouser, and V. Koltun,
    “Minos: Multimodal indoor simulator for navigation in complex environments.” *arXiv
    preprint arXiv:1712.03931*, 2017.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] M. Savva, A. X. Chang, A. Dosovitskiy, T. A. Funkhouser, 和 V. Koltun，“Minos：用于在复杂环境中导航的多模态室内模拟器。”
    *arXiv预印本 arXiv:1712.03931*，2017年。'
- en: '[12] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sunderhauf, I. Reid,
    S. Gould, and A. van den Hengel, “Vision-and-language navigation: Interpreting
    visually-grounded navigation instructions in real environments,” in *CVPR*, 2018,
    pp. 3674–3683.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sunderhauf, I.
    Reid, S. Gould, 和 A. van den Hengel，“视野与语言导航：在真实环境中解释视觉基础的导航指令，”发表于*CVPR*，2018年，第3674–3683页。'
- en: '[13] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub,
    J. Liu, V. Koltun, J. Malik *et al.*, “Habitat: A platform for embodied ai research,”
    in *ICCV*, 2019, pp. 9339–9347.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub,
    J. Liu, V. Koltun, J. Malik *等*，“Habitat：一个用于体现AI研究的平台，”发表于*ICCV*，2019年，第9339–9347页。'
- en: '[14] F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese, “Gibson
    env: Real-world perception for embodied agents,” in *CVPR*, 2018, pp. 9068–9079.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, 和 S. Savarese，“Gibson env：为体现代理提供现实世界的感知，”发表于*CVPR*，2018年，第9068–9079页。'
- en: '[15] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi,
    “Target-driven visual navigation in indoor scenes using deep reinforcement learning,”
    in *ICRA*, 2017, pp. 3357–3364.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, 和 A. Farhadi，“使用深度强化学习进行室内场景的目标驱动视觉导航，”发表于*ICRA*，2017年，第3357–3364页。'
- en: '[16] A. Mousavian, A. Toshev, M. Fiser, J. Kosecka, A. Wahid, and J. Davidson,
    “Visual representations for semantic target driven navigation,” in *ICRA*, 2019,
    pp. 8846–8852.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] A. Mousavian, A. Toshev, M. Fiser, J. Kosecka, A. Wahid, 和 J. Davidson，“用于语义目标驱动导航的视觉表示，”发表于*ICRA*，2019年，第8846–8852页。'
- en: '[17] C. Xie, S. Patil, T. Moldovan, S. Levine, and P. Abbeel, “Model-based
    reinforcement learning with parametrized physical models and optimism-driven exploration,”
    in *ICRA*, 2016, pp. 504–511.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] C. Xie, S. Patil, T. Moldovan, S. Levine, 和 P. Abbeel，“基于模型的强化学习与参数化物理模型和乐观驱动的探索，”发表于*ICRA*，2016年，第504–511页。'
- en: '[18] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver,
    and K. Kavukcuoglu, “Reinforcement learning with unsupervised auxiliary tasks,”
    in *ICLR 2017 : ICLR 2017*, 2017.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver,
    和 K. Kavukcuoglu，“带有无监督辅助任务的强化学习，”发表于*ICLR 2017 : ICLR 2017*，2017年。'
- en: '[19] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. Ballard, A. Banino, M. Denil,
    R. Goroshin, L. Sifre, K. Kavukcuoglu, D. Kumaran, and R. Hadsell, “Learning to
    navigate in complex environments,” in *ICLR*, 2017.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. Ballard, A. Banino, M.
    Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, D. Kumaran, 和 R. Hadsell，“学习在复杂环境中导航，”发表于*ICLR*，2017年。'
- en: '[20] S. Gupta, V. Tolani, J. Davidson, S. Levine, R. Sukthankar, and J. Malik,
    “Cognitive mapping and planning for visual navigation,” *IJCV*, vol. 128, no. 5,
    pp. 1311–1330, 2020.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] S. Gupta, V. Tolani, J. Davidson, S. Levine, R. Sukthankar, 和 J. Malik，“视觉导航的认知映射与规划，”*IJCV*，第128卷，第5期，第1311–1330页，2020年。'
- en: '[21] W. Qi, R. T. Mullapudi, S. Gupta, and D. Ramanan, “Learning to move with
    affordance maps,” in *ICLR*, 2020.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] W. Qi, R. T. Mullapudi, S. Gupta, 和 D. Ramanan，“利用便利性图进行运动学习，”发表于*ICLR*，2020年。'
- en: '[22] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, “Learning
    to explore using active neural slam,” in *ICLR*, 2020.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, 和 R. Salakhutdinov，“使用主动神经SLAM进行探索学习，”发表于*ICLR*，2020年。'
- en: '[23] X. Wang, W. Xiong, H. Wang, and W. Yang Wang, “Look before you leap: Bridging
    model-free and model-based reinforcement learning for planned-ahead vision-and-language
    navigation,” in *ECCV*, 2018, pp. 37–53.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] X. Wang, W. Xiong, H. Wang, 和 W. Yang Wang，“三思而后行：弥合无模型和基于模型的强化学习以实现计划视野和语言导航，”发表于*ECCV*，2018年，第37–53页。'
- en: '[24] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick,
    K. Saenko, D. Klein, and T. Darrell, “Speaker-follower models for vision-and-language
    navigation,” in *NeurIPS*, 2018, pp. 3314–3325.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T.
    Berg-Kirkpatrick, K. Saenko, D. Klein, 和 T. Darrell，“面向视野和语言导航的讲者-跟随者模型，”发表于*NeurIPS*，2018年，第3314–3325页。'
- en: '[25] L. D. Jackel, E. Krotkov, M. Perschbacher, J. Pippine, and C. Sullivan,
    “The darpa lagr program: Goals, challenges, methodology, and phase i results,”
    *Journal of Field robotics*, vol. 23, no. 11-12, pp. 945–973, 2006.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] L. D. Jackel, E. Krotkov, M. Perschbacher, J. Pippine, 和 C. Sullivan,
    “DARPA LAGR计划：目标、挑战、方法论及第一阶段结果，” *领域机器人学杂志*，第23卷，第11-12期，第945–973页，2006年。'
- en: '[26] S. D. Morad, R. Mecca, R. P. K. Poudel, S. Liwicki, and R. Cipolla, “Embodied
    visual navigation with automatic curriculum learning in real environments,” in
    *IEEE Robotics and Automation Letters*, vol. 6, no. 2, 2021, pp. 683–690.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] S. D. Morad, R. Mecca, R. P. K. Poudel, S. Liwicki, 和 R. Cipolla, “在真实环境中通过自动课程学习进行具身视觉导航，”
    发表在 *IEEE Robotics and Automation Letters*，第6卷，第2期，2021年，第683–690页。'
- en: '[27] C. Thorpe, M. H. Hebert, T. Kanade, and S. A. Shafer, “Vision and navigation
    for the carnegie-mellon navlab,” *TPAMI*, vol. 10, no. 3, pp. 362–373, 1988.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] C. Thorpe, M. H. Hebert, T. Kanade, 和 S. A. Shafer, “卡内基梅隆导航实验室的视觉与导航，”
    *TPAMI*，第10卷，第3期，第362–373页，1988年。'
- en: '[28] R. Traoré, H. Caselles-Dupré, T. Lesort, T. Sun, G. Cai, N. Díaz-Rodríguez,
    and D. Filliat, “Discorl: Continual reinforcement learning via policy distillation,”
    *NeurIPS workshop on Deep Reinforcement Learning*, 2019.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] R. Traoré, H. Caselles-Dupré, T. Lesort, T. Sun, G. Cai, N. Díaz-Rodríguez,
    和 D. Filliat, “Discorl：通过策略蒸馏进行持续强化学习，” *NeurIPS 深度强化学习研讨会*，2019年。'
- en: '[29] H. Huang, V. Jain, H. Mehta, A. Ku, G. Magalhaes, J. Baldridge, and E. Ie,
    “Transferable representation learning in vision-and-language navigation,” in *ICCV*,
    2019, pp. 7404–7413.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] H. Huang, V. Jain, H. Mehta, A. Ku, G. Magalhaes, J. Baldridge, 和 E. Ie,
    “在视觉与语言导航中进行可迁移表示学习，” 发表在 *ICCV*，2019年，第7404–7413页。'
- en: '[30] L. Yan, D. Liu, Y. Song, and C. Yu, “Multimodal aggregation approach for
    memory vision-voice indoor navigation with meta-learning,” in *IROS*, 2020, pp.
    5847–5854.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] L. Yan, D. Liu, Y. Song, 和 C. Yu, “用于记忆视觉-语音室内导航的多模态聚合方法与元学习，” 发表在 *IROS*，2020年，第5847–5854页。'
- en: '[31] M. Fisher, D. Ritchie, M. Savva, T. Funkhouser, and P. Hanrahan, “Example-based
    synthesis of 3d object arrangements,” *international conference on computer graphics
    and interactive techniques*, vol. 31, no. 6, p. 135, 2012.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] M. Fisher, D. Ritchie, M. Savva, T. Funkhouser, 和 P. Hanrahan, “基于示例的3d物体排列合成，”
    发表在 *国际计算机图形学与交互技术会议*，第31卷，第6期，第135页，2012年。'
- en: '[32] A. Handa, V. Patraucean, S. Stent, and R. Cipolla, “Scenenet: An annotated
    model generator for indoor scene understanding,” in *ICRA*, 2016, pp. 5737–5743.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] A. Handa, V. Patraucean, S. Stent, 和 R. Cipolla, “Scenenet：用于室内场景理解的注释模型生成器，”
    发表在 *ICRA*，2016年，第5737–5743页。'
- en: '[33] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, “Joint 2d-3d-semantic
    data for indoor scene understanding.” *arXiv preprint arXiv:1702.01105*, 2017.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] I. Armeni, S. Sax, A. R. Zamir, 和 S. Savarese, “用于室内场景理解的联合2d-3d-语义数据。”
    *arXiv preprint arXiv:1702.01105*，2017年。'
- en: '[34] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser, “Semantic
    scene completion from a single depth image,” in *CVPR*, 2017, pp. 190–198.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, 和 T. Funkhouser, “从单一深度图像完成语义场景，”
    发表在 *CVPR*，2017年，第190–198页。'
- en: '[35] C. Yan, D. K. Misra, A. Bennett, A. Walsman, Y. Bisk, and Y. Artzi, “Chalet:
    Cornell house agent learning environment.” *arXiv preprint arXiv:1801.07357*,
    2018.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] C. Yan, D. K. Misra, A. Bennett, A. Walsman, Y. Bisk, 和 Y. Artzi, “Chalet：康奈尔住宅代理学习环境。”
    *arXiv preprint arXiv:1801.07357*，2018年。'
- en: '[36] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niebner, M. Savva, S. Song,
    A. Zeng, and Y. Zhang, “Matterport3d: Learning from rgb-d data in indoor environments,”
    in *3DV*, 2017, pp. 667–676.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niebner, M. Savva, S. Song,
    A. Zeng, 和 Y. Zhang, “Matterport3d：从室内环境中的rgb-d数据学习，” 发表在 *3DV*，2017年，第667–676页。'
- en: '[37] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel,
    R. Mur-Artal, C. Ren, S. Verma, A. Clarkson, M. Yan, B. Budge, Y. Yan, X. Pan,
    J. Yon, Y. Zou, K. Leon, N. Carter, J. Briales, T. Gillingham, E. Mueggler, L. Pesqueira,
    M. Savva, D. Batra, H. M. Strasdat, R. D. Nardi, M. Goesele, S. Lovegrove, and
    R. A. Newcombe, “The replica dataset: A digital replica of indoor spaces.” *arXiv
    preprint arXiv:1906.05797*, 2019.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel,
    R. Mur-Artal, C. Ren, S. Verma, A. Clarkson, M. Yan, B. Budge, Y. Yan, X. Pan,
    J. Yon, Y. Zou, K. Leon, N. Carter, J. Briales, T. Gillingham, E. Mueggler, L.
    Pesqueira, M. Savva, D. Batra, H. M. Strasdat, R. D. Nardi, M. Goesele, S. Lovegrove,
    和 R. A. Newcombe, “Replica数据集：室内空间的数字副本。” *arXiv preprint arXiv:1906.05797*，2019年。'
- en: '[38] L. Tai and M. Liu, “Towards cognitive exploration through deep reinforcement
    learning for mobile robots,” *arXiv preprint arXiv:1610.01733*, 2016.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] L. Tai 和 M. Liu, “通过深度强化学习实现认知探索以用于移动机器人，” *arXiv preprint arXiv:1610.01733*，2016年。'
- en: '[39] M. Deitke, W. Han, A. Herrasti, A. Kembhavi, E. Kolve, R. Mottaghi, J. Salvador,
    D. Schwenk, E. VanderBilt, M. Wallingford *et al.*, “Robothor: An open simulation-to-real
    embodied ai platform,” in *CVPR*, 2020, pp. 3164–3174.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] M. Deitke, W. Han, A. Herrasti, A. Kembhavi, E. Kolve, R. Mottaghi, J.
    Salvador, D. Schwenk, E. VanderBilt, M. Wallingford *等*，“Robothor：一个开放的模拟到现实的具身人工智能平台，”
    见 *CVPR*，2020年，第3164–3174页。'
- en: '[40] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi, “Ai2-thor:
    An interactive 3d environment for visual ai.” *arXiv preprint arXiv:1712.05474*,
    2017.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, 和 A. Farhadi，“Ai2-thor：一个用于视觉人工智能的互动3D环境。”
    *arXiv预印本 arXiv:1712.05474*，2017年。'
- en: '[41] F. Xia, W. B. Shen, C. Li, P. Kasimbeg, M. E. Tchapmi, A. Toshev, R. Martin-Martin,
    and S. Savarese, “Interactive gibson benchmark: A benchmark for interactive navigation
    in cluttered environments,” *IEEE Robotics and Automation Letters*, vol. 5, no. 2,
    pp. 713–720, 2020.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] F. Xia, W. B. Shen, C. Li, P. Kasimbeg, M. E. Tchapmi, A. Toshev, R. Martin-Martin,
    和 S. Savarese，“互动Gibson基准：一个用于拥挤环境中互动导航的基准，” *IEEE Robotics and Automation Letters*，第5卷，第2期，第713–720页，2020年。'
- en: '[42] M. L. Littman, “Markov games as a framework for multi-agent reinforcement
    learning,” in *ICML*, 1994, pp. 157–163.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] M. L. Littman，“马尔可夫博弈作为多智能体强化学习的框架，” 见 *ICML*，1994年，第157–163页。'
- en: '[43] M. Tan, “Multi-agent reinforcement learning: independent vs. cooperative
    agents,” in *ICML*, 1997, pp. 487–494.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] M. Tan，“多智能体强化学习：独立与合作代理，” 见 *ICML*，1997年，第487–494页。'
- en: '[44] P. Anderson, A. X. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun,
    J. Kosecka, J. Malik, R. Mottaghi, M. Savva, and A. R. Zamir, “On evaluation of
    embodied navigation agents.” *arXiv preprint arXiv:1807.06757*, 2018.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] P. Anderson, A. X. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V.
    Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, 和 A. R. Zamir，“关于具身导航代理的评估。”
    *arXiv预印本 arXiv:1807.06757*，2018年。'
- en: '[45] S. Wani, S. Patel, U. Jain, A. X. Chang, and M. Savva, “Multion: Benchmarking
    semantic map memory using multi-object navigation,” in *NeurIPS*, vol. 33, 2020.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] S. Wani, S. Patel, U. Jain, A. X. Chang, 和 M. Savva，“Multion：使用多物体导航基准测试语义地图记忆，”
    见 *NeurIPS*，第33卷，2020年。'
- en: '[46] V. Jain, G. Magalhaes, A. Ku, A. Vaswani, E. Ie, and J. Baldridge, “Stay
    on the path: Instruction fidelity in vision-and-language navigation,” in *ACL*,
    2019, pp. 1862–1872.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] V. Jain, G. Magalhaes, A. Ku, A. Vaswani, E. Ie, 和 J. Baldridge，“保持在路径上：视觉与语言导航中的指令保真度，”
    见 *ACL*，2019年，第1862–1872页。'
- en: '[47] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge, “Room-across-room:
    Multilingual vision-and-language navigation with dense spatiotemporal grounding,”
    in *EMNLP*, 2020, pp. 4392–4412.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] A. Ku, P. Anderson, R. Patel, E. Ie, 和 J. Baldridge，“跨房间：具有密集时空基础的多语言视觉与语言导航，”
    见 *EMNLP*，2020年，第4392–4412页。'
- en: '[48] Y. Hong, C. Rodriguez, Q. Wu, and S. Gould, “Sub-instruction aware vision-and-language
    navigation,” in *EMNLP*, 2020, pp. 3360–3376.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Y. Hong, C. Rodriguez, Q. Wu, 和 S. Gould，“子指令感知的视觉与语言导航，” 见 *EMNLP*，2020年，第3360–3376页。'
- en: '[49] J. Thomason, M. Murray, M. Cakmak, and L. Zettlemoyer, “Vision-and-dialog
    navigation,” *CoRL*, pp. 394–406, 2019.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] J. Thomason, M. Murray, M. Cakmak, 和 L. Zettlemoyer，“视觉与对话导航，” *CoRL*，第394–406页，2019年。'
- en: '[50] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
    D. Parikh, “Vqa: Visual question answering,” in *ICCV*, 2015, pp. 2425–2433.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, 和 D.
    Parikh，“VQA：视觉问答，” 见 *ICCV*，2015年，第2425–2433页。'
- en: '[51] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang,
    “Bottom-up and top-down attention for image captioning and visual question answering,”
    in *CVPR*, 2018, pp. 6077–6086.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, 和 L. Zhang，“用于图像描述和视觉问答的自下而上和自上而下的注意力机制，”
    见 *CVPR*，2018年，第6077–6086页。'
- en: '[52] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
    Y. Kalantidis, L.-J. Li, D. A. Shamma, M. S. Bernstein, and L. Fei-Fei, “Visual
    genome: Connecting language and vision using crowdsourced dense image annotations,”
    *IJCV*, vol. 123, no. 1, pp. 32–73, 2017.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
    Y. Kalantidis, L.-J. Li, D. A. Shamma, M. S. Bernstein, 和 L. Fei-Fei，“视觉基因组：通过众包密集图像注释连接语言和视觉，”
    *IJCV*，第123卷，第1期，第32–73页，2017年。'
- en: '[53] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked attention networks
    for image question answering,” in *CVPR*, 2016, pp. 21–29.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Z. Yang, X. He, J. Gao, L. Deng, 和 A. Smola，“用于图像问答的堆叠注意力网络，” 见 *CVPR*，2016年，第21–29页。'
- en: '[54] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, “Embodied
    question answering,” in *CVPR Workshops (CVPRW)*, 2018, pp. 1–10.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, 和 D. Batra，“具身问答，” 见
    *CVPR Workshops (CVPRW)*，2018年，第1–10页。'
- en: '[55] Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Y. Wang, C. Shen, and A. van den
    Hengel, “Reverie: Remote embodied visual referring expression in real indoor environments,”
    in *CVPR*, 2020, pp. 9982–9991.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Y. Wang, C. Shen 和 A. van den Hengel，“Reverie:
    在真实室内环境中进行远程具身视觉指代表达”，在*CVPR*，2020年，第9982–9991页。'
- en: '[56] C. Chen, U. Jain, C. Schissler, S. V. A. Gari, Z. Al-Halah, V. K. Ithapu,
    P. Robinson, and K. Grauman, “Soundspaces: Audio-visual navigation in 3d environments,”
    in *ECCV*, 2020, pp. 17–36.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] C. Chen, U. Jain, C. Schissler, S. V. A. Gari, Z. Al-Halah, V. K. Ithapu,
    P. Robinson 和 K. Grauman，“Soundspaces: 3D环境中的音频-视觉导航”，在*ECCV*，2020年，第17–36页。'
- en: '[57] L. Yu, X. Chen, G. Gkioxari, M. Bansal, T. L. Berg, and D. Batra, “Multi-target
    embodied question answering,” in *CVPR*, 2019, pp. 6309–6318.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] L. Yu, X. Chen, G. Gkioxari, M. Bansal, T. L. Berg 和 D. Batra，“多目标具身问答”，在*CVPR*，2019年，第6309–6318页。'
- en: '[58] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and A. Farhadi,
    “Iqa: Visual question answering in interactive environments,” in *CVPR*, 2018,
    pp. 4089–4098.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox 和 A. Farhadi，“Iqa:
    互动环境中的视觉问答”，在*CVPR*，2018年，第4089–4098页。'
- en: '[59] K. Nguyen and H. Daumé, “Help, anna! vision-based navigation with natural
    multimodal assistance via retrospective curiosity-encouraging imitation learning,”
    in *EMNLP*, 2019, pp. 684–695.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] K. Nguyen 和 H. Daumé，“Help, anna! 基于视觉的导航与自然多模态辅助通过回顾性好奇心鼓励模仿学习”，在*EMNLP*，2019年，第684–695页。'
- en: '[60] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik, “Cognitive
    mapping and planning for visual navigation,” in *CVPR*, 2017, pp. 7272–7281.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] S. Gupta, J. Davidson, S. Levine, R. Sukthankar 和 J. Malik，“视觉导航的认知映射与规划”，在*CVPR*，2017年，第7272–7281页。'
- en: '[61] D. K. Misra, J. Langford, and Y. Artzi, “Mapping instructions and visual
    observations to actions with reinforcement learning.” in *EMNLP*, 2017, pp. 1004–1015.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] D. K. Misra, J. Langford 和 Y. Artzi，“将指令和视觉观察映射到行动的强化学习”，在*EMNLP*，2017年，第1004–1015页。'
- en: '[62] H. Chen, A. Suhr, D. Misra, N. Snavely, and Y. Artzi, “Touchdown: Natural
    language navigation and spatial reasoning in visual street environments,” in *CVPR*,
    2019, pp. 12 538–12 547.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] H. Chen, A. Suhr, D. Misra, N. Snavely 和 Y. Artzi，“Touchdown: 在视觉街道环境中的自然语言导航和空间推理”，在*CVPR*，2019年，第12,538–12,547页。'
- en: '[63] W. Zhu, H. Hu, J. Chen, Z. Deng, V. Jain, E. Ie, and F. Sha, “Babywalk:
    Going farther in vision-and-language navigation by taking baby steps,” in *ACL*,
    2020, pp. 2539–2556.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] W. Zhu, H. Hu, J. Chen, Z. Deng, V. Jain, E. Ie 和 F. Sha，“Babywalk: 通过逐步小步走在视觉和语言导航中走得更远”，在*ACL*，2020年，第2539–2556页。'
- en: '[64] D. J. Berndt and J. Clifford, “Using dynamic time warping to find patterns
    in time series,” in *AAAIWS’94 Proceedings of the 3rd International Conference
    on Knowledge Discovery and Data Mining*, 1994, pp. 359–370.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] D. J. Berndt 和 J. Clifford，“使用动态时间规整查找时间序列中的模式”，在*AAAIWS’94 第三届国际知识发现与数据挖掘大会论文集*，1994年，第359–370页。'
- en: '[65] H. Sakoe and S. Chiba, “Dynamic programming algorithm optimization for
    spoken word recognition,” *IEEE transactions on acoustics, speech, and signal
    processing*, vol. 26, no. 1, pp. 43–49, 1978.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] H. Sakoe 和 S. Chiba，“用于语音识别的动态规划算法优化”，*IEEE声学、语音和信号处理学报*，第26卷，第1期，第43–49页，1978年。'
- en: '[66] A. Vakanski, I. Mantegh, A. Irish, and F. Janabi-Sharifi, “Trajectory
    learning for robot programming by demonstration using hidden markov model and
    dynamic time warping,” *IEEE Transactions on Systems, Man, and Cybernetics*, vol. 42,
    no. 4, pp. 1039–1052, 2012.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] A. Vakanski, I. Mantegh, A. Irish 和 F. Janabi-Sharifi，“使用隐马尔可夫模型和动态时间规整进行机器人编程示范的轨迹学习”，*IEEE系统、人与控制学报*，第42卷，第4期，第1039–1052页，2012年。'
- en: '[67] E. J. Keogh and M. J. Pazzani, “Scaling up dynamic time warping for datamining
    applications,” in *KDD*, 2000, pp. 285–289.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] E. J. Keogh 和 M. J. Pazzani，“扩大动态时间规整以适用于数据挖掘应用”，在*KDD*，2000年，第285–289页。'
- en: '[68] G. Ilharco, V. Jain, A. Ku, E. Ie, and J. Baldridge, “General evaluation
    for instruction conditioned navigation using dynamic time warping,” *ViGIL@NeurIPS*,
    2019.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] G. Ilharco, V. Jain, A. Ku, E. Ie 和 J. Baldridge，“使用动态时间规整的指令条件导航的通用评估”，*ViGIL@NeurIPS*，2019年。'
- en: '[69] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Harley, T. P. Lillicrap,
    D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,”
    in *ICML*, 2016, pp. 1928–1937.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Harley, T. P. Lillicrap,
    D. Silver 和 K. Kavukcuoglu，“深度强化学习的异步方法”，在*ICML*，2016年，第1928–1937页。'
- en: '[70] A. Tamar, Y. Wu, G. Thomas, S. Levine, and P. Abbeel, “Value iteration
    networks,” in *IJCAI*, 2017, pp. 4949–4953.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] A. Tamar, Y. Wu, G. Thomas, S. Levine 和 P. Abbeel，“价值迭代网络”，在*IJCAI*，2017年，第4949–4953页。'
- en: '[71] L. Lee, E. Parisotto, D. S. Chaplot, E. P. Xing, and R. Salakhutdinov,
    “Gated path planning networks,” in *ICML*, 2018, pp. 2947–2955.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] L. Lee, E. Parisotto, D. S. Chaplot, E. P. Xing, 和 R. Salakhutdinov，“门控路径规划网络，”发表于*ICML*，2018年，第2947–2955页。'
- en: '[72] S. Lenser and M. Veloso, “Visual sonar: fast obstacle avoidance using
    monocular vision,” in *IROS*, vol. 1, 2003, pp. 886–891.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] S. Lenser 和 M. Veloso，“视觉声呐：使用单目视觉进行快速障碍物避免，”发表于*IROS*，第1卷，2003年，第886–891页。'
- en: '[73] E. Royer, J. Bom, M. Dhome, B. Thuilot, M. Lhuillier, and F. Marmoiton,
    “Outdoor autonomous navigation using monocular vision,” in *IROS*, 2005, pp. 1253–1258.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] E. Royer, J. Bom, M. Dhome, B. Thuilot, M. Lhuillier, 和 F. Marmoiton，“使用单目视觉的户外自主导航，”发表于*IROS*，2005年，第1253–1258页。'
- en: '[74] H. Haddad, M. Khatib, S. Lacroix, and R. Chatila, “Reactive navigation
    in outdoor environments using potential fields,” in *IROS*, vol. 2, 1998, pp.
    1232–1237.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] H. Haddad, M. Khatib, S. Lacroix, 和 R. Chatila，“使用势场进行户外环境的反应式导航，”发表于*IROS*，第2卷，1998年，第1232–1237页。'
- en: '[75] A. Remazeilles, F. Chaumette, and P. Gros, “Robot motion control from
    a visual memory,” in *ICRA*, vol. 5, 2004, pp. 4695–4700.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] A. Remazeilles, F. Chaumette, 和 P. Gros，“基于视觉记忆的机器人运动控制，”发表于*ICRA*，第5卷，2004年，第4695–4700页。'
- en: '[76] Y. Zhu, D. Gordon, E. Kolve, D. Fox, L. Fei-Fei, A. Gupta, R. Mottaghi,
    and A. Farhadi, “Visual semantic planning using deep successor representations,”
    in *ICCV*, 2017, pp. 483–492.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Y. Zhu, D. Gordon, E. Kolve, D. Fox, L. Fei-Fei, A. Gupta, R. Mottaghi,
    和 A. Farhadi，“使用深度继任表示进行视觉语义规划，”发表于*ICCV*，2017年，第483–492页。'
- en: '[77] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    in *ICLR*, 2016.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, 和 D. Wierstra，“使用深度强化学习进行连续控制，”发表于*ICLR*，2016年。'
- en: '[78] Y. Li and J. Kosecka, “Learning view and target invariant visual servoing
    for navigation,” in *ICRA*, 2020, pp. 658–664.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Y. Li 和 J. Kosecka，“学习视图和目标不变的视觉伺服导航，”发表于*ICRA*，2020年，第658–664页。'
- en: '[79] T.-Y. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *ECCV*, 2014,
    pp. 740–755.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] T.-Y. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P.
    Dollár, 和 C. L. Zitnick，“Microsoft coco: 上下文中的常见物体，”发表于*ECCV*，2014年，第740–755页。'
- en: '[80] A. Mousavian, H. Pirsiavash, and J. Kosecka, “Joint semantic segmentation
    and depth estimation with deep convolutional networks,” in *3DV*, 2016, pp. 611–619.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] A. Mousavian, H. Pirsiavash, 和 J. Kosecka，“联合语义分割和深度估计的深度卷积网络，”发表于*3DV*，2016年，第611–619页。'
- en: '[81] W. Shen, D. Xu, Y. Zhu, L. Fei-Fei, L. Guibas, and S. Savarese, “Situational
    fusion of visual representation for visual navigation,” in *ICCV*, 2019, pp. 2881–2890.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] W. Shen, D. Xu, Y. Zhu, L. Fei-Fei, L. Guibas, 和 S. Savarese，“视觉导航的情境融合视觉表示，”发表于*ICCV*，2019年，第2881–2890页。'
- en: '[82] Y. Lv, N. Xie, Y. Shi, Z. Wang, and H. T. Shen, “Improving target-driven
    visual navigation with attention on 3d spatial relationships,” *arXiv preprint
    arXiv:2005.02153*, 2020.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Y. Lv, N. Xie, Y. Shi, Z. Wang, 和 H. T. Shen，“通过关注3D空间关系提高目标驱动的视觉导航，”*arXiv
    preprint arXiv:2005.02153*，2020年。'
- en: '[83] Q. Wu, K. Xu, J. Wang, M. Xu, X. Gong, and D. Manocha, “Reinforcement
    learning-based visual navigation with information-theoretic regularization,” in
    *IEEE Robotics and Automation Letters*, vol. 6, no. 2, 2021, pp. 731–738.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Q. Wu, K. Xu, J. Wang, M. Xu, X. Gong, 和 D. Manocha，“基于强化学习的视觉导航与信息理论正则化，”发表于*IEEE
    Robotics and Automation Letters*，第6卷，第2期，2021年，第731–738页。'
- en: '[84] A. Khosla, B. An, J. J. Lim, and A. Torralba, “Looking beyond the visible
    scene,” in *CVPR*, 2014, pp. 3710–3717.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] A. Khosla, B. An, J. J. Lim, 和 A. Torralba，“超越可见场景的观察，”发表于*CVPR*，2014年，第3710–3717页。'
- en: '[85] S. Brahmbhatt and J. Hays, “Deepnav: Learning to navigate large cities,”
    in *CVPR*, 2017, pp. 3087–3096.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. Brahmbhatt 和 J. Hays，“Deepnav: 学习在大城市中导航，”发表于*CVPR*，2017年，第3087–3096页。'
- en: '[86] P. Mirowski, M. K. Grimes, M. Malinowski, K. M. Hermann, K. Anderson,
    D. Teplyashin, K. Simonyan, K. Kavukcuoglu, A. Zisserman, and R. Hadsell, “Learning
    to navigate in cities without a map,” in *NeurIPS*, vol. 31, 2018, pp. 2419–2430.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] P. Mirowski, M. K. Grimes, M. Malinowski, K. M. Hermann, K. Anderson,
    D. Teplyashin, K. Simonyan, K. Kavukcuoglu, A. Zisserman, 和 R. Hadsell，“在没有地图的情况下学习城市导航，”发表于*NeurIPS*，第31卷，2018年，第2419–2430页。'
- en: '[87] C. Chen, S. Majumder, Z. Al-Halah, R. Gao, S. K. Ramakrishnan, and K. Grauman,
    “Learning to set waypoints for audio-visual navigation,” in *ICLR*, 2021.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] C. Chen, S. Majumder, Z. Al-Halah, R. Gao, S. K. Ramakrishnan, 和 K. Grauman，“学习设置音视频导航的路径点，”发表于*ICLR*，2021年。'
- en: '[88] C. Doersch, A. Gupta, and A. A. Efros, “Unsupervised visual representation
    learning by context prediction,” in *ICCV*, 2015, pp. 1422–1430.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] C. Doersch, A. Gupta, 和 A. A. Efros，“通过上下文预测进行无监督视觉表示学习，”发表于*ICCV*，2015年，第1422–1430页。'
- en: '[89] M. Noroozi and P. Favaro, “Unsupervised learning of visual representations
    by solving jigsaw puzzles,” in *ECCV*, 2016, pp. 69–84.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] M. Noroozi 和 P. Favaro，“通过解决拼图来进行无监督视觉表征学习”，在 *ECCV*，2016年，第69–84页。'
- en: '[90] R. Zhang, P. Isola, and A. A. Efros, “Colorful image colorization,” in
    *ECCV*, 2016, pp. 649–666.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] R. Zhang, P. Isola 和 A. A. Efros，“丰富的图像着色”，在 *ECCV*，2016年，第649–666页。'
- en: '[91] Y. Sun, X. Wang, Z. Liu, J. Miller, A. A. Efros, and M. Hardt, “Test-time
    training with self-supervision for generalization under distribution shifts,”
    in *ICML*, 2020, pp. 9229–9248.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Y. Sun, X. Wang, Z. Liu, J. Miller, A. A. Efros 和 M. Hardt，“自我监督的测试时训练以应对分布变化的泛化”，在
    *ICML*，2020年，第9229–9248页。'
- en: '[92] T. Schaul, D. Horgan, K. Gregor, and D. Silver, “Universal value function
    approximators,” in *ICML*, 2015, pp. 1312–1320.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] T. Schaul, D. Horgan, K. Gregor 和 D. Silver，“通用价值函数逼近器”，在 *ICML*，2015年，第1312–1320页。'
- en: '[93] A. Dosovitskiy and V. Koltun, “Learning to act by predicting the future,”
    in *ICLR*, 2016.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] A. Dosovitskiy 和 V. Koltun，“通过预测未来学习行动”，在 *ICLR*，2016年。'
- en: '[94] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. Jaśkowski, “Vizdoom:
    A doom-based ai research platform for visual reinforcement learning,” in *CIG*.   IEEE,
    2016, pp. 1–8.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] M. Kempka, M. Wydmuch, G. Runc, J. Toczek 和 W. Jaśkowski，“Vizdoom：基于Doom的视觉强化学习AI研究平台”，在
    *CIG*。 IEEE，2016年，第1–8页。'
- en: '[95] J. Ye, D. Batra, E. Wijmans, and A. Das, “Auxiliary tasks speed up learning
    pointgoal navigation.” *arXiv preprint arXiv:2007.04561*, 2020.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] J. Ye, D. Batra, E. Wijmans 和 A. Das，“辅助任务加速点目标导航学习。” *arXiv 预印本 arXiv:2007.04561*，2020年。'
- en: '[96] J. Ye, D. Batra, A. Das, and E. Wijmans, “Auxiliary tasks and exploration
    enable objectnav.” *arXiv preprint arXiv:2104.04112*, 2021.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] J. Ye, D. Batra, A. Das 和 E. Wijmans，“辅助任务和探索使对象导航成为可能。” *arXiv 预印本 arXiv:2104.04112*，2021年。'
- en: '[97] X. Ye, Z. Lin, J.-Y. Lee, J. Zhang, S. Zheng, and Y. Yang, “Gaple: Generalizable
    approaching policy learning for robotic object searching in indoor environment,”
    *IEEE Robotics and Automation Letters*, vol. 4, no. 4, pp. 4003–4010, 2019.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] X. Ye, Z. Lin, J.-Y. Lee, J. Zhang, S. Zheng 和 Y. Yang，“Gaple：通用的接近策略学习用于室内环境中的机器人目标搜索”，*IEEE机器人与自动化通讯*，第4卷，第4期，第4003–4010页，2019年。'
- en: '[98] G.-H. Liu, A. Siravuru, S. P. Selvaraj, M. M. Veloso, and G. Kantor, “Learning
    end-to-end multimodal sensor policies for autonomous navigation.” *CoRL*, pp.
    249–261, 2017.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] G.-H. Liu, A. Siravuru, S. P. Selvaraj, M. M. Veloso 和 G. Kantor，“学习端到端多模态传感器策略以实现自主导航。”
    *CoRL*，第249–261页，2017年。'
- en: '[99] S. Datta, O. Maksymets, J. Hoffman, S. Lee, D. Batra, and D. Parikh, “Integrating
    egocentric localization for more realistic point-goal navigation agents,” *arXiv
    preprint arXiv:2009.03231*, 2020.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] S. Datta, O. Maksymets, J. Hoffman, S. Lee, D. Batra 和 D. Parikh，“整合自我中心定位以实现更现实的点目标导航代理”，*arXiv
    预印本 arXiv:2009.03231*，2020年。'
- en: '[100] R. Bigazzi, F. Landi, M. Cornia, S. Cascianelli, L. Baraldi, and R. Cucchiara,
    “Explore and explain: Self-supervised navigation and recounting,” in *ICPR*, 2021,
    pp. 1152–1159.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] R. Bigazzi, F. Landi, M. Cornia, S. Cascianelli, L. Baraldi 和 R. Cucchiara，“探索与解释：自我监督导航与讲述”，在
    *ICPR*，2021年，第1152–1159页。'
- en: '[101] V. Dean, S. Tulsiani, and A. Gupta, “See, hear, explore: Curiosity via
    audio-visual association,” in *NeurIPS*, vol. 33, 2020.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] V. Dean, S. Tulsiani 和 A. Gupta，“看、听、探索：通过视听关联激发好奇心”，在 *NeurIPS*，第33卷，2020年。'
- en: '[102] H. Durrant-Whyte and T. Bailey, “Simultaneous localization and mapping:
    part i,” *IEEE robotics & automation magazine*, vol. 13, no. 2, pp. 99–110, 2006.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] H. Durrant-Whyte 和 T. Bailey，“同时定位与映射：第一部分”，*IEEE机器人与自动化杂志*，第13卷，第2期，第99–110页，2006年。'
- en: '[103] M. Filipenko and I. Afanasyev, “Comparison of various slam systems for
    mobile robot in an indoor environment,” in *IS*, 2018, pp. 400–407.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] M. Filipenko 和 I. Afanasyev，“室内环境中各种SLAM系统的比较”，在 *IS*，2018年，第400–407页。'
- en: '[104] S. Se, D. G. Lowe, and J. J. Little, “Mobile robot localization and mapping
    with uncertainty using scale-invariant visual landmarks,” *The International Journal
    of Robotics Research*, vol. 21, no. 8, pp. 735–758, 2002.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] S. Se, D. G. Lowe 和 J. J. Little，“使用尺度不变视觉地标的移动机器人定位与映射”，*国际机器人研究期刊*，第21卷，第8期，第735–758页，2002年。'
- en: '[105] C. F. Olson, L. H. Matthies, M. Schoppers, and M. W. Maimone, “Rover
    navigation using stereo ego-motion,” *Robotics and Autonomous Systems*, vol. 43,
    no. 4, pp. 215–229, 2003.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] C. F. Olson, L. H. Matthies, M. Schoppers 和 M. W. Maimone，“使用立体视觉运动的探测器导航”，*机器人与自主系统*，第43卷，第4期，第215–229页，2003年。'
- en: '[106] Davison, “Real-time simultaneous localisation and mapping with a single
    camera,” in *ICCV*, vol. 2, 2003, pp. 1403–1410.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Davison，“使用单台相机的实时同时定位与映射”，在 *ICCV*，第2卷，2003年，第1403–1410页。'
- en: '[107] J. Engel, V. Koltun, and D. Cremers, “Direct sparse odometry,” *TPAMI*,
    vol. 40, no. 3, pp. 611–625, 2017.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] J. Engel, V. Koltun, 和 D. Cremers, “直接稀疏里程计，” *TPAMI*，第40卷，第3期，第611–625页，2017年。'
- en: '[108] J. Engel, T. Schöps, and D. Cremers, “Lsd-slam: Large-scale direct monocular
    slam,” in *ECCV*.   Springer, 2014, pp. 834–849.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] J. Engel, T. Schöps, 和 D. Cremers, “Lsd-slam：大规模直接单目slam，” 发表在 *ECCV*。 Springer,
    2014年，第834–849页。'
- en: '[109] R. Mur-Artal and J. D. Tardós, “Orb-slam2: An open-source slam system
    for monocular, stereo, and rgb-d cameras,” *IEEE Transactions on Robotics*, vol. 33,
    no. 5, pp. 1255–1262, 2017.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] R. Mur-Artal 和 J. D. Tardós, “Orb-slam2：一个开源的单目、立体和rgb-d相机slam系统，” *IEEE机器人学报*，第33卷，第5期，第1255–1262页，2017年。'
- en: '[110] K. Tateno, F. Tombari, I. Laina, and N. Navab, “Cnn-slam: Real-time dense
    monocular slam with learned depth prediction,” in *CVPR*, 2017, pp. 6243–6252.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] K. Tateno, F. Tombari, I. Laina, 和 N. Navab, “Cnn-slam：具有学习深度预测的实时稠密单目slam，”
    发表在 *CVPR*，2017年，第6243–6252页。'
- en: '[111] C. Wang, J. Miguel Buenaposada, R. Zhu, and S. Lucey, “Learning depth
    from monocular videos using direct methods,” in *CVPR*, 2018, pp. 2022–2030.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] C. Wang, J. Miguel Buenaposada, R. Zhu, 和 S. Lucey, “使用直接方法从单目视频中学习深度，”
    发表在 *CVPR*，2018年，第2022–2030页。'
- en: '[112] N. Yang, L. v. Stumberg, R. Wang, and D. Cremers, “D3vo: Deep depth,
    deep pose and deep uncertainty for monocular visual odometry,” in *CVPR*, 2020,
    pp. 1281–1292.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] N. Yang, L. v. Stumberg, R. Wang, 和 D. Cremers, “D3vo：单目视觉里程计的深度深度、深度姿态和深度不确定性，”
    发表在 *CVPR*，2020年，第1281–1292页。'
- en: '[113] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *CVPR*.   IEEE, 2012, pp. 3354–3361.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] A. Geiger, P. Lenz, 和 R. Urtasun, “我们为自动驾驶做好准备了吗？KITTI视觉基准套件，” 发表在 *CVPR*。 IEEE,
    2012年，第3354–3361页。'
- en: '[114] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W.
    Achtelik, and R. Siegwart, “The euroc micro aerial vehicle datasets,” *The International
    Journal of Robotics Research*, vol. 35, no. 10, pp. 1157–1163, 2016.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W.
    Achtelik, 和 R. Siegwart, “EUROC微型飞行器数据集，” *国际机器人研究期刊*，第35卷，第10期，第1157–1163页，2016年。'
- en: '[115] E. Parisotto and R. Salakhutdinov, “Neural map: Structured memory for
    deep reinforcement learning.” in *ICLR*, 2017.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] E. Parisotto 和 R. Salakhutdinov, “神经地图：深度强化学习的结构化记忆。” 发表在 *ICLR*，2017年。'
- en: '[116] J. Zhang, L. Tai, J. Boedecker, W. Burgard, and M. Liu, “Neural slam:
    Learning to explore with external memory,” *arXiv preprint arXiv:1706.09520*,
    2017.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] J. Zhang, L. Tai, J. Boedecker, W. Burgard, 和 M. Liu, “神经slam：学习使用外部记忆进行探索，”
    *arXiv预印本 arXiv:1706.09520*，2017年。'
- en: '[117] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” in *NeurIPS*, vol. 30,
    2017, pp. 5998–6008.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, 和 I. Polosukhin, “注意力机制是你所需的一切，” 发表在 *NeurIPS*，第30卷，2017年，第5998–6008页。'
- en: '[118] J. F. Henriques and A. Vedaldi, “Mapnet: An allocentric spatial memory
    for mapping environments,” in *CVPR*, 2018, pp. 8476–8484.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] J. F. Henriques 和 A. Vedaldi, “Mapnet：一种用于环境映射的分配空间记忆，” 发表在 *CVPR*，2018年，第8476–8484页。'
- en: '[119] T. Jaksch, R. Ortner, and P. Auer, “Near-optimal regret bounds for reinforcement
    learning,” *JMLR*, vol. 11, no. 51, pp. 1563–1600, 2010.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] T. Jaksch, R. Ortner, 和 P. Auer, “强化学习的近似最优遗憾界限，” *JMLR*，第11卷，第51期，第1563–1600页，2010年。'
- en: '[120] C. Jin, Z. Allen-Zhu, S. Bubeck, and M. I. Jordan, “Is q-learning provably
    efficient?” in *NeurIPS*, vol. 31, 2018, pp. 4863–4873.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] C. Jin, Z. Allen-Zhu, S. Bubeck, 和 M. I. Jordan, “Q学习是否可证明高效？” 发表在 *NeurIPS*，第31卷，2018年，第4863–4873页。'
- en: '[121] M. G. Azar, I. Osband, and R. Munos, “Minimax regret bounds for reinforcement
    learning,” in *ICML*, 2017, pp. 263–272.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] M. G. Azar, I. Osband, 和 R. Munos, “强化学习的最小化遗憾界限，” 发表在 *ICML*，2017年，第263–272页。'
- en: '[122] T. Chen, S. Gupta, and A. Gupta, “Learning exploration policies for navigation,”
    in *ICLR*, 2019.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] T. Chen, S. Gupta, 和 A. Gupta, “为导航学习探索策略，” 发表在 *ICLR*，2019年。'
- en: '[123] A. G. Barto and S. Mahadevan, “Recent advances in hierarchical reinforcement
    learning,” *Discrete Event Dynamic Systems*, vol. 13, no. 1, pp. 41–77, 2003.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] A. G. Barto 和 S. Mahadevan, “层次强化学习的最新进展，” *离散事件动态系统*，第13卷，第1期，第41–77页，2003年。'
- en: '[124] P. Dayan and G. E. Hinton, “Feudal reinforcement learning,” in *NeurIPS*,
    vol. 5, 1992, pp. 271–278.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] P. Dayan 和 G. E. Hinton, “封建式强化学习，” 发表在 *NeurIPS*，第5卷，1992年，第271–278页。'
- en: '[125] D. S. Chaplot, R. Salakhutdinov, A. Gupta, and S. Gupta, “Neural topological
    slam for visual navigation,” in *CVPR*, 2020, pp. 12 875–12 884.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] D. S. Chaplot, R. Salakhutdinov, A. Gupta, 和 S. Gupta, “用于视觉导航的神经拓扑slam，”
    发表在 *CVPR*，2020年，第12 875–12 884页。'
- en: '[126] D. S. Chaplot, D. P. Gandhi, A. Gupta, and R. R. Salakhutdinov, “Object
    goal navigation using goal-oriented semantic exploration,” in *NeurIPS*, vol. 33,
    2020.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] D. S. Chaplot, D. P. Gandhi, A. Gupta, 和 R. R. Salakhutdinov, “基于目标导向的语义探索进行物体目标导航，”
    发表在 *NeurIPS*，第33卷，2020年。'
- en: '[127] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” *NeurIPS*, vol. 27, pp. 3104–3112, 2014.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] I. Sutskever, O. Vinyals 和 Q. V. Le，“基于神经网络的序列到序列学习，” *NeurIPS*，第 27
    卷，页码 3104–3112，2014。'
- en: '[128] X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y.-F. Wang, W. Y.
    Wang, and L. Zhang, “Reinforced cross-modal matching and self-supervised imitation
    learning for vision-language navigation.” in *CVPR*, 2018, pp. 6629–6638.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y.-F. Wang, W. Y.
    Wang 和 L. Zhang，“强化的跨模态匹配和自监督模仿学习用于视觉-语言导航。” 见 *CVPR*，2018，页码 6629–6638。'
- en: '[129] C.-Y. Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher, and C. Xiong,
    “Self-monitoring navigation agent via auxiliary progress estimation,” in *ICLR*,
    2019.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] C.-Y. Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher 和 C. Xiong，“通过辅助进度估计的自我监控导航代理，”
    见 *ICLR*，2019。'
- en: '[130] C.-Y. Ma, Z. Wu, G. AlRegib, C. Xiong, and Z. Kira, “The regretful agent:
    Heuristic-aided navigation through progress estimation,” in *CVPR*, 2019, pp.
    6732–6740.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] C.-Y. Ma, Z. Wu, G. AlRegib, C. Xiong 和 Z. Kira，“令人遗憾的代理：通过进度估计的启发式辅助导航，”
    见 *CVPR*，2019，页码 6732–6740。'
- en: '[131] L. Ke, X. Li, Y. Bisk, A. Holtzman, Z. Gan, J. Liu, J. Gao, Y. Choi,
    and S. Srinivasa, “Tactical rewind: Self-correction via backtracking in vision-and-language
    navigation,” in *CVPR*, 2019, pp. 6741–6749.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] L. Ke, X. Li, Y. Bisk, A. Holtzman, Z. Gan, J. Liu, J. Gao, Y. Choi 和
    S. Srinivasa，“战术回溯：通过回溯在视觉和语言导航中的自我纠正，” 见 *CVPR*，2019，页码 6741–6749。'
- en: '[132] P. Anderson, A. Shrivastava, D. Parikh, D. Batra, and S. Lee, “Chasing
    ghosts: Instruction following as bayesian state tracking,” in *NeurIPS*, 2019,
    pp. 371–381.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] P. Anderson, A. Shrivastava, D. Parikh, D. Batra 和 S. Lee，“追踪幽灵：将指令跟随作为贝叶斯状态跟踪，”
    见 *NeurIPS*，2019，页码 371–381。'
- en: '[133] R. Jonschkowski and O. Brock, “End-to-end learnable histogram filters,”
    2016.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] R. Jonschkowski 和 O. Brock，“端到端可学习的直方图滤波器，” 2016。'
- en: '[134] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    “Dropout: a simple way to prevent neural networks from overfitting,” *The JMLR*,
    vol. 15, no. 1, pp. 1929–1958, 2014.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever 和 R. Salakhutdinov，“Dropout：一种防止神经网络过拟合的简单方法，”
    *The JMLR*，第 15 卷，第 1 期，页码 1929–1958，2014。'
- en: '[135] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *ICLR*, 2015.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] K. Simonyan 和 A. Zisserman，“用于大规模图像识别的非常深的卷积网络，” 见 *ICLR*，2015。'
- en: '[136] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016, pp. 770–778.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] K. He, X. Zhang, S. Ren 和 J. Sun， “用于图像识别的深度残差学习，” 见 *CVPR*，2016，页码 770–778。'
- en: '[137] H. Tan, L. Yu, and M. Bansal, “Learning to navigate unseen environments:
    Back translation with environmental dropout,” in *NAACL*, 2019, pp. 2610–2621.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] H. Tan, L. Yu 和 M. Bansal，“学习在未见环境中导航：环境丢失的反向翻译，” 见 *NAACL*，2019，页码 2610–2621。'
- en: '[138] Y. Zhu, F. Zhu, Z. Zhan, B. Lin, J. Jiao, X. Chang, and X. Liang, “Vision-dialog
    navigation by exploring cross-modal memory,” in *CVPR*, 2020, pp. 10 730–10 739.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Y. Zhu, F. Zhu, Z. Zhan, B. Lin, J. Jiao, X. Chang 和 X. Liang，“通过探索跨模态记忆进行视觉对话导航，”
    见 *CVPR*，2020，页码 10,730–10,739。'
- en: '[139] T.-J. Fu, X. E. Wang, M. F. Peterson, S. T. Grafton, M. P. Eckstein,
    and W. Y. Wang, “Counterfactual vision-and-language navigation via adversarial
    path sampling,” in *ECCV*, 2019, pp. 71–86.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] T.-J. Fu, X. E. Wang, M. F. Peterson, S. T. Grafton, M. P. Eckstein 和
    W. Y. Wang，“通过对抗路径采样进行反事实视觉和语言导航，” 见 *ECCV*，2019，页码 71–86。'
- en: '[140] J. Thomason, D. Gordon, and Y. Bisk, “Shifting the baseline: Single modality
    performance on visual navigation & qa,” in *NAACL*, 2019, pp. 1977–1983.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] J. Thomason, D. Gordon 和 Y. Bisk，“改变基线：视觉导航和问答的单模态表现，” 见 *NAACL*，2019，页码
    1977–1983。'
- en: '[141] H. Huang, V. Jain, H. Mehta, J. Baldridge, and E. Ie, “Multi-modal discriminative
    model for vision-and-language navigation,” in *RoboNLP*, 2019, pp. 40–49.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] H. Huang, V. Jain, H. Mehta, J. Baldridge 和 E. Ie，“用于视觉和语言导航的多模态判别模型，”
    见 *RoboNLP*，2019，页码 40–49。'
- en: '[142] H. Wang, Q. Wu, and C. Shen, “Soft expert reward learning for vision-and-language
    navigation,” in *ECCV*, 2020, pp. 126–141.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] H. Wang, Q. Wu 和 C. Shen，“用于视觉和语言导航的软专家奖励学习，” 见 *ECCV*，2020，页码 126–141。'
- en: '[143] Q. Xia, X. Li, C. Li, Y. Bisk, Z. Sui, J. Gao, Y. Choi, and N. A. Smith,
    “Multi-view learning for vision-and-language navigation.” *arXiv preprint arXiv:2003.00857*,
    2020.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Q. Xia, X. Li, C. Li, Y. Bisk, Z. Sui, J. Gao, Y. Choi 和 N. A. Smith，“用于视觉和语言导航的多视图学习。”
    *arXiv 预印本 arXiv:2003.00857*，2020。'
- en: '[144] Y. Hong, C. Rodriguez, Y. Qi, Q. Wu, and S. Gould, “Language and visual
    entity relationship graph for agent navigation,” in *NeurIPS*, vol. 33, 2020.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Y. Hong, C. Rodriguez, Y. Qi, Q. Wu 和 S. Gould，“用于代理导航的语言和视觉实体关系图，” 见
    *NeurIPS*，第 33 卷，2020。'
- en: '[145] R. Hu, D. Fried, A. Rohrbach, D. Klein, T. Darrell, and K. Saenko, “Are
    you looking? grounding to multiple modalities in vision-and-language navigation,”
    in *ACL*, 2019, pp. 6551–6557.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] R. Hu, D. Fried, A. Rohrbach, D. Klein, T. Darrell, 和 K. Saenko, “你在看吗？视觉和语言导航中的多模态基础，”发表于*ACL*，2019年，第6551–6557页。'
- en: '[146] B. Shi, X. Wang, P. Lyu, C. Yao, and X. Bai, “Robust scene text recognition
    with automatic rectification,” in *CVPR*, 2016, pp. 4168–4176.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] B. Shi, X. Wang, P. Lyu, C. Yao, 和 X. Bai, “具有自动矫正的鲁棒场景文本识别，”发表于*CVPR*，2016年，第4168–4176页。'
- en: '[147] J. Devlin, M.-W. Chang, K. Lee, and K. N. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” in *NAACL*, 2018,
    pp. 4171–4186.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] J. Devlin, M.-W. Chang, K. Lee, 和 K. N. Toutanova, “Bert：用于语言理解的深度双向变换器的预训练，”发表于*NAACL*，2018年，第4171–4186页。'
- en: '[148] H. Tan and M. Bansal, “Lxmert: Learning cross-modality encoder representations
    from transformers,” in *EMNLP*, 2019, pp. 5099–5110.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] H. Tan 和 M. Bansal, “Lxmert：从变换器学习跨模态编码器表示，”发表于*EMNLP*，2019年，第5099–5110页。'
- en: '[149] X. Li, C. Li, Q. Xia, Y. Bisk, A. Celikyilmaz, J. Gao, N. A. Smith, and
    Y. Choi, “Robust navigation with language pretraining and stochastic sampling,”
    in *EMNLP*, 2019, pp. 1494–1499.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] X. Li, C. Li, Q. Xia, Y. Bisk, A. Celikyilmaz, J. Gao, N. A. Smith, 和
    Y. Choi, “通过语言预训练和随机采样实现鲁棒导航，”发表于*EMNLP*，2019年，第1494–1499页。'
- en: '[150] A. Majumdar, A. Shrivastava, S. Lee, P. Anderson, D. Parikh, and D. Batra,
    “Improving vision-and-language navigation with image-text pairs from the web,”
    in *ECCV*, 2020, pp. 259–274.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] A. Majumdar, A. Shrivastava, S. Lee, P. Anderson, D. Parikh, 和 D. Batra,
    “通过来自网络的图像-文本对改进视觉和语言导航，”发表于*ECCV*，2020年，第259–274页。'
- en: '[151] W. Hao, C. Li, X. Li, L. Carin, and J. Gao, “Towards learning a generic
    agent for vision-and-language navigation via pre-training,” in *CVPR*, 2020, pp.
    13 137–13 146.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] W. Hao, C. Li, X. Li, L. Carin, 和 J. Gao, “通过预训练学习用于视觉和语言导航的通用代理，”发表于*CVPR*，2020年，第13 137–13 146页。'
- en: '[152] Y. Hong, Q. Wu, Y. Qi, C. Rodriguez-Opazo, and S. Gould, “Vln bert: A
    recurrent vision-and-language bert for navigation,” in *CVPR*, 2021, pp. 1643–1653.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Y. Hong, Q. Wu, Y. Qi, C. Rodriguez-Opazo, 和 S. Gould, “Vln bert：用于导航的递归视觉和语言bert，”发表于*CVPR*，2021年，第1643–1653页。'
- en: '[153] A. Graves, “Adaptive computation time for recurrent neural networks,”
    *arXiv preprint arXiv:1603.08983*, 2016.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] A. Graves, “递归神经网络的自适应计算时间，”*arXiv预印本 arXiv:1603.08983*，2016年。'
- en: '[154] A. Das, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, “Neural Modular
    Control for Embodied Question Answering,” in *CoRL*, 2018.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] A. Das, G. Gkioxari, S. Lee, D. Parikh, 和 D. Batra, “用于具身问答的神经模块控制，”发表于*CoRL*，2018年。'
- en: '[155] A. Anand, E. Belilovsky, K. Kastner, H. Larochelle, and A. C. Courville,
    “Blindfold baselines for embodied qa.” *arXiv: Computer Vision and Pattern Recognition*,
    2018.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] A. Anand, E. Belilovsky, K. Kastner, H. Larochelle, 和 A. C. Courville,
    “具身问答的盲目基线。” *arXiv:计算机视觉和模式识别*，2018年。'
- en: '[156] Y. Wu, L. Jiang, and Y. Yang, “Revisiting embodiedqa: A simple baseline
    and beyond,” *IEEE Transactions on Image Processing*, vol. 29, pp. 3984–3992,
    2020.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Y. Wu, L. Jiang, 和 Y. Yang, “重新审视具身问答：一个简单的基线及其超越，”*IEEE图像处理学报*，第29卷，第3984–3992页，2020年。'
- en: '[157] E. Wijmans, S. Datta, O. Maksymets, A. Das, G. Gkioxari, S. Lee, I. Essa,
    D. Parikh, and D. Batra, “Embodied Question Answering in Photorealistic Environments
    with Point Cloud Perception,” in *CVPR*, 2019.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] E. Wijmans, S. Datta, O. Maksymets, A. Das, G. Gkioxari, S. Lee, I. Essa,
    D. Parikh, 和 D. Batra, “在光照逼真的环境中使用点云感知的具身问答，”发表于*CVPR*，2019年。'
- en: '[158] H. Luo, G. Lin, Z. Liu, F. Liu, Z. Tang, and Y. Yao, “Segeqa: Video segmentation
    based visual attention for embodied question answering,” in *ICCV*, 2019, pp.
    9667–9676.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] H. Luo, G. Lin, Z. Liu, F. Liu, Z. Tang, 和 Y. Yao, “Segeqa：基于视频分割的视觉注意力用于具身问答，”发表于*ICCV*，2019年，第9667–9676页。'
- en: '[159] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox,
    “Flownet 2.0: Evolution of optical flow estimation with deep networks,” in *CVPR*,
    2017, pp. 2462–2470.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, 和 T. Brox, “Flownet
    2.0：通过深度网络演化的光流估计，”发表于*CVPR*，2017年，第2462–2470页。'
- en: '[160] J. Li, S. Tang, F. Wu, and Y. Zhuang, “Walking with mind: Mental imagery
    enhanced embodied qa,” in *ACM Multimedia*, 2019, pp. 1211–1219.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] J. Li, S. Tang, F. Wu, 和 Y. Zhuang, “心灵漫步：增强心理意象的具身问答，”发表于*ACM Multimedia*，2019年，第1211–1219页。'
- en: '[161] S. Tan, W. Xiang, H. Liu, D. Guo, and F. Sun, “Multi-agent embodied question
    answering in interactive environments,” in *ECCV*, 2020, pp. 663–678.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] S. Tan, W. Xiang, H. Liu, D. Guo, 和 F. Sun, “在交互环境中的多智能体具身问答，”发表于*ECCV*，2020年，第663–678页。'
- en: '[162] Y. Ye, Z. Zhao, Y. Li, L. Chen, J. Xiao, and Y. Zhuang, “Video question
    answering via attribute-augmented attention network learning,” in *ACM*, 2017,
    pp. 829–832.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Y. Ye, Z. Zhao, Y. Li, L. Chen, J. Xiao, 和 Y. Zhuang, “通过属性增强注意力网络学习的视频问答，”发表于*ACM*，2017年，第829–832页。'
- en: '[163] C. Cangea, E. Belilovsky, P. Liò, and A. C. Courville, “Videonavqa: Bridging
    the gap between visual and embodied question answering.” in *ViGIL@NeurIPS*, 2019,
    p. 280.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] C. Cangea, E. Belilovsky, P. Liò, 和 A. C. Courville, “Videonavqa：弥合视觉与体现问答之间的差距，”发表于*ViGIL@NeurIPS*，2019年，页码280。'
- en: '[164] Y. Deng, D. Guo, X. Guo, N. Zhang, H. Liu, and F. Sun, “Mqa: Answering
    the question via robotic manipulation,” in *RSS*, vol. 17, 2021.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Y. Deng, D. Guo, X. Guo, N. Zhang, H. Liu, 和 F. Sun, “Mqa：通过机器人操控回答问题，”发表于*RSS*，第17卷，2021年。'
- en: '[165] D. Nilsson, A. Pirinen, E. Gärtner, and C. Sminchisescu, “Embodied visual
    active learning for semantic segmentation.” in *AAAI*, 2020, pp. 2373–2383.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] D. Nilsson, A. Pirinen, E. Gärtner, 和 C. Sminchisescu, “用于语义分割的体现视觉主动学习，”发表于*AAAI*，2020年，页码2373–2383。'
- en: '[166] H. R. Roman, Y. Bisk, J. Thomason, A. Celikyilmaz, and J. Gao, “Rmm:
    A recursive mental model for dialog navigation,” in *EMNLP*, 2020, pp. 1732–1745.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] H. R. Roman, Y. Bisk, J. Thomason, A. Celikyilmaz, 和 J. Gao, “Rmm：对话导航的递归心理模型，”发表于*EMNLP*，2020年，页码1732–1745。'
- en: '[167] J. Thomason, A. Padmakumar, J. Sinapov, N. Walker, Y. Jiang, H. Yedidsion,
    J. Hart, P. Stone, and R. J. Mooney, “Improving grounded natural language understanding
    through human-robot dialog,” in *ICRA*.   IEEE, 2019, pp. 6934–6941.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] J. Thomason, A. Padmakumar, J. Sinapov, N. Walker, Y. Jiang, H. Yedidsion,
    J. Hart, P. Stone, 和 R. J. Mooney, “通过人机对话改善扎根自然语言理解，”发表于*ICRA*。 IEEE，2019年，页码6934–6941。'
- en: '[168] S. Tellex, R. A. Knepper, A. Li, D. Rus, and N. Roy, “Asking for help
    using inverse semantics,” in *Robotics: Science and Systems 2014*, vol. 10, 2014.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] S. Tellex, R. A. Knepper, A. Li, D. Rus, 和 N. Roy, “通过逆语义请求帮助，”发表于*Robotics:
    Science and Systems 2014*，第10卷，2014年。'
- en: '[169] K. Nguyen, D. Dey, C. Brockett, and B. Dolan, “Vision-based navigation
    with language-based assistance via imitation learning with indirect intervention,”
    in *CVPR*, 2019, pp. 12 527–12 537.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] K. Nguyen, D. Dey, C. Brockett, 和 B. Dolan, “基于视觉的导航与语言辅助通过模仿学习与间接干预，”发表于*CVPR*，2019年，页码12 527–12 537。'
- en: '[170] H. de Vries, K. Shuster, D. Batra, D. Parikh, J. Weston, and D. Kiela,
    “Talk the walk: Navigating new york city through grounded dialogue,” *arXiv: Artificial
    Intelligence*, 2018.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] H. de Vries, K. Shuster, D. Batra, D. Parikh, J. Weston, 和 D. Kiela,
    “Talk the walk: 通过扎根对话导航纽约市，”*arXiv: Artificial Intelligence*，2018年。'
- en: '[171] M. Hahn, J. Krantz, D. Batra, D. Parikh, J. M. Rehg, S. Lee, and P. Anderson,
    “Where are you? localization from embodied dialog,” in *EMNLP*, 2020, pp. 806–822.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] M. Hahn, J. Krantz, D. Batra, D. Parikh, J. M. Rehg, S. Lee, 和 P. Anderson,
    “你在哪里？从体现对话中定位，”发表于*EMNLP*，2020年，页码806–822。'
- en: '[172] U. Jain, L. Weihs, E. Kolve, M. Rastegari, S. Lazebnik, A. Farhadi, A. G.
    Schwing, and A. Kembhavi, “Two body problem: Collaborative visual task completion,”
    in *CVPR*, 2019, pp. 6689–6699.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] U. Jain, L. Weihs, E. Kolve, M. Rastegari, S. Lazebnik, A. Farhadi, A.
    G. Schwing, 和 A. Kembhavi, “双体问题：协作视觉任务完成，”发表于*CVPR*，2019年，页码6689–6699。'
- en: '[173] A. Singh, T. Jain, and S. Sukhbaatar, “Learning when to communicate at
    scale in multiagent cooperative and competitive tasks,” in *ICLR*, 2018.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] A. Singh, T. Jain, 和 S. Sukhbaatar, “在多智能体合作与竞争任务中，学习何时进行大规模沟通，”发表于*ICLR*，2018年。'
- en: '[174] M. Marge, S. Nogar, C. J. Hayes, S. M. Lukin, J. Bloecker, E. Holder,
    and C. R. Voss, “A research platform for multi-robot dialogue with humans,” in
    *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics (Demonstrations)*, 2019, pp. 132–137.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] M. Marge, S. Nogar, C. J. Hayes, S. M. Lukin, J. Bloecker, E. Holder,
    和 C. R. Voss, “一个用于多机器人与人类对话的研究平台，”发表于*Proceedings of the 2019 Conference of the
    North American Chapter of the Association for Computational Linguistics (Demonstrations)*，2019年，页码132–137。'
- en: '[175] S. Banerjee, J. Thomason, and J. J. Corso, “The robotslang benchmark:
    Dialog-guided robot localization and navigation.” *arXiv: Robotics*, 2020.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] S. Banerjee, J. Thomason, 和 J. J. Corso, “robotslang基准测试：对话引导下的机器人定位与导航，”*arXiv:
    Robotics*，2020年。'
- en: '[176] U. Muller, J. Ben, E. Cosatto, B. Flepp, and Y. L. Cun, “Off-road obstacle
    avoidance through end-to-end learning,” in *NeurIPS*.   Citeseer, 2006, pp. 739–746.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] U. Muller, J. Ben, E. Cosatto, B. Flepp, 和 Y. L. Cun, “通过端到端学习进行越野障碍物避让，”发表于*NeurIPS*。
    Citeseer，2006年，页码739–746。'
- en: '[177] R. Hadsell, P. Sermanet, J. Ben, A. Erkan, M. Scoffier, K. Kavukcuoglu,
    U. Muller, and Y. LeCun, “Learning long-range vision for autonomous off-road driving,”
    *Journal of Field Robotics*, vol. 26, no. 2, pp. 120–144, 2009.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] R. Hadsell, P. Sermanet, J. Ben, A. Erkan, M. Scoffier, K. Kavukcuoglu,
    U. Muller, 和 Y. LeCun, “为自主越野驾驶学习长距离视觉，”*Journal of Field Robotics*，第26卷，第2期，页码120–144，2009年。'
- en: '[178] W. Zhang, K. Liu, W. Zhang, Y. Zhang, and J. Gu, “Deep neural networks
    for wireless localization in indoor and outdoor environments,” *Neurocomputing*,
    vol. 194, pp. 279–287, 2016.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] W. Zhang, K. Liu, W. Zhang, Y. Zhang, 和 J. Gu, “用于室内外环境无线定位的深度神经网络，”*Neurocomputing*，第194卷，页码279–287，2016年。'
- en: '[179] J. Zhang, J. T. Springenberg, J. Boedecker, and W. Burgard, “Deep reinforcement
    learning with successor features for navigation across similar environments,”
    in *IROS*, 2017, pp. 2371–2378.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] J. Zhang, J. T. Springenberg, J. Boedecker, 和 W. Burgard， “使用后继特征的深度强化学习在相似环境中的导航，”
    发表在*IROS*，2017年，页码2371–2378。'
- en: '[180] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie,
    A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis,
    “Human-level control through deep reinforcement learning,” *Nature*, vol. 518,
    no. 7540, pp. 529–533, 2015.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie,
    A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, 和 D. Hassabis，
    “通过深度强化学习实现人类级别的控制，” *Nature*，第518卷，第7540期，页码529–533，2015年。'
- en: '[181] G. Kahn, A. Villaflor, P. Abbeel, and S. Levine, “Composable action-conditioned
    predictors: Flexible off-policy learning for robot navigation,” in *CoRL*.   PMLR,
    2018, pp. 806–816.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] G. Kahn, A. Villaflor, P. Abbeel, 和 S. Levine， “可组合的动作条件预测器：灵活的离线学习用于机器人导航，”
    发表在*CoRL*。 PMLR，2018年，页码806–816。'
- en: '[182] S. Ross, N. Melik-Barkhudarov, K. S. Shankar, A. Wendel, D. Dey, J. A.
    Bagnell, and M. Hebert, “Learning monocular reactive uav control in cluttered
    natural environments,” in *ICRA*.   IEEE, 2013, pp. 1765–1772.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] S. Ross, N. Melik-Barkhudarov, K. S. Shankar, A. Wendel, D. Dey, J. A.
    Bagnell, 和 M. Hebert， “在杂乱自然环境中学习单目反应无人机控制，” 发表在*ICRA*。 IEEE，2013年，页码1765–1772。'
- en: '[183] T. Manderson, J. C. G. Higuera, S. Wapnick, J.-F. Tremblay, F. Shkurti,
    D. Meger, and G. Dudek, “Vision-based goal-conditioned policies for underwater
    navigation in the presence of obstacles,” in *RSS*, vol. 16, 2020.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] T. Manderson, J. C. G. Higuera, S. Wapnick, J.-F. Tremblay, F. Shkurti,
    D. Meger, 和 G. Dudek， “基于视觉的目标条件策略用于在障碍物存在下的水下导航，” 发表在*RSS*，第16卷，2020年。'
- en: '[184] J. Borenstein and Y. Koren, “Real-time obstacle avoidance for fast mobile
    robots in cluttered environments,” in *ICRA*.   IEEE, 1990, pp. 572–577.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] J. Borenstein 和 Y. Koren， “在杂乱环境中快速移动机器人实时障碍物规避，” 发表在*ICRA*。 IEEE，1990年，页码572–577。'
- en: '[185] D. Ha and J. Schmidhuber, “World models,” *arXiv preprint arXiv:1803.10122*,
    2018.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] D. Ha 和 J. Schmidhuber， “世界模型，” *arXiv 预印本 arXiv:1803.10122*，2018年。'
- en: '[186] A. Francis, A. Faust, H.-T. L. Chiang, J. Hsu, J. C. Kew, M. Fiser, and
    T.-W. E. Lee, “Long-range indoor navigation with prm-rl,” *IEEE Transactions on
    Robotics*, vol. 36, no. 4, pp. 1115–1134, 2020.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] A. Francis, A. Faust, H.-T. L. Chiang, J. Hsu, J. C. Kew, M. Fiser, 和
    T.-W. E. Lee， “使用 PRM-RL 进行长距离室内导航，” *IEEE机器人学报*，第36卷，第4期，页码1115–1134，2020年。'
- en: '[187] D. Shah, B. Eysenbach, G. Kahn, N. Rhinehart, and S. Levine, “Ving: Learning
    open-world navigation with visual goals,” *arXiv preprint arXiv:2012.09812*, 2020.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] D. Shah, B. Eysenbach, G. Kahn, N. Rhinehart, 和 S. Levine， “Ving: 使用视觉目标学习开放世界导航，”
    *arXiv 预印本 arXiv:2012.09812*，2020年。'
- en: '[188] S. Thrun, “Probabilistic robotics,” *Communications of the ACM*, vol. 45,
    no. 3, pp. 52–57, 2002.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] S. Thrun， “概率机器人，” *ACM 通讯*，第45卷，第3期，页码52–57，2002年。'
- en: '[189] S. M. LaValle, *Planning algorithms*.   Cambridge university press, 2006.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] S. M. LaValle， *规划算法*。 剑桥大学出版社，2006年。'
- en: '[190] A. J. Davison and D. W. Murray, “Mobile robot localisation using active
    vision,” in *ECCV*.   Springer, 1998, pp. 809–825.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] A. J. Davison 和 D. W. Murray， “使用主动视觉进行移动机器人定位，” 发表在*ECCV*。 Springer，1998年，页码809–825。'
- en: '[191] R. Sim and J. J. Little, “Autonomous vision-based exploration and mapping
    using hybrid maps and rao-blackwellised particle filters,” in *IROS*.   IEEE,
    2006, pp. 2082–2089.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] R. Sim 和 J. J. Little， “使用混合地图和 Rao-Blackwell 化粒子滤波器进行自主视觉探索和映射，” 发表在*IROS*。
    IEEE，2006年，页码2082–2089。'
- en: '[192] G. Berseth, D. Geng, C. M. Devin, N. Rhinehart, C. Finn, D. Jayaraman,
    and S. Levine, “Smirl: Surprise minimizing reinforcement learning in unstable
    environments,” in *ICLR*, 2021.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] G. Berseth, D. Geng, C. M. Devin, N. Rhinehart, C. Finn, D. Jayaraman,
    和 S. Levine， “SMIRL: 在不稳定环境中最小化惊讶的强化学习，” 发表在*ICLR*，2021年。'
- en: '[193] A. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine,
    and C. Finn, “Learning to adapt in dynamic, real-world environments through meta-reinforcement
    learning,” in *ICLR*, 2018.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] A. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine,
    和 C. Finn， “通过元强化学习在动态现实世界环境中学习适应，” 发表在*ICLR*，2018年。'
- en: '[194] K. Lee, Y. Seo, S. Lee, H. Lee, and J. Shin, “Context-aware dynamics
    model for generalization in model-based reinforcement learning,” in *ICML*, vol. 1,
    2020, pp. 5757–5766.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] K. Lee, Y. Seo, S. Lee, H. Lee, 和 J. Shin， “用于模型基础强化学习的上下文感知动态模型，” 发表在*ICML*，第1卷，2020年，页码5757–5766。'
- en: '[195] E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva,
    and D. Batra, “Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion
    frames,” in *Eighth ICLR*, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva,
    和 D. Batra, “Dd-ppo：从25亿帧中学习近乎完美的点目标导航器”，在 *第八届 ICLR*，2020年。'
- en: '[196] H. Wang, W. Wang, T. Shu, W. Liang, and J. Shen, “Active visual information
    gathering for vision-language navigation,” in *ECCV*, 2020, pp. 307–322.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] H. Wang, W. Wang, T. Shu, W. Liang, 和 J. Shen, “用于视觉语言导航的主动视觉信息收集”，在
    *ECCV*，2020年，页码 307–322。'
- en: '[197] C. Finn and S. Levine, “Deep visual foresight for planning robot motion,”
    in *ICRA*.   IEEE, 2017, pp. 2786–2793.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] C. Finn 和 S. Levine, “用于规划机器人运动的深度视觉预见”，在 *ICRA*。IEEE，2017年，页码 2786–2793。'
- en: '[198] N. Malone and V. Kypraios, “Dd t07 - designing a situational judgment
    test to evaluate lll-structured problem solving in a virtual learning environment,”
    2012.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] N. Malone 和 V. Kypraios, “Dd t07 - 设计用于评估虚拟学习环境中结构化问题解决能力的情境判断测试”，2012年。'
- en: '[199] S. K. Ramakrishnan, Z. Al-Halah, and K. Grauman, “Occupancy anticipation
    for efficient exploration and navigation,” in *ECCV*, 2020, pp. 400–418.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] S. K. Ramakrishnan, Z. Al-Halah, 和 K. Grauman, “高效探索和导航的占用预测”，在 *ECCV*，2020年，页码
    400–418。'
- en: '[200] K. Lobos-Tsunekawa, F. Leiva, and J. Ruiz-del-Solar, “Visual navigation
    for biped humanoid robots using deep reinforcement learning,” in *IEEE Robotics
    and Automation Letters*, vol. 3, no. 4, 2018, pp. 3247–3254.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] K. Lobos-Tsunekawa, F. Leiva, 和 J. Ruiz-del-Solar, “使用深度强化学习的双足类人机器人视觉导航”，在
    *IEEE Robotics and Automation Letters*，第3卷，第4期，2018年，页码 3247–3254。'
- en: '[201] J. Bruce, N. Sünderhauf, P. Mirowski, R. Hadsell, and M. Milford, “One-shot
    reinforcement learning for robot navigation with interactive replay.” *arXiv preprint
    arXiv:1711.10137*, 2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] J. Bruce, N. Sünderhauf, P. Mirowski, R. Hadsell, 和 M. Milford, “用于机器人导航的一次性强化学习与交互回放”，*arXiv
    预印本 arXiv:1711.10137*，2017年。'
- en: '[202] M. Pfeiffer, S. Shukla, M. Turchetta, C. Cadena, A. Krause, R. Siegwart,
    and J. I. Nieto, “Reinforced imitation: Sample efficient deep reinforcement learning
    for mapless navigation by leveraging prior demonstrations,” in *IEEE Robotics
    and Automation Letters*, vol. 3, no. 4, 2018, pp. 4423–4430.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] M. Pfeiffer, S. Shukla, M. Turchetta, C. Cadena, A. Krause, R. Siegwart,
    和 J. I. Nieto, “强化模仿：通过利用先前演示进行样本高效的深度强化学习以实现无地图导航”，在 *IEEE Robotics and Automation
    Letters*，第3卷，第4期，2018年，页码 4423–4430。'
- en: '[203] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick,
    R. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell, “Policy distillation,” in
    *ICLR*, 2016.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick,
    R. Pascanu, V. Mnih, K. Kavukcuoglu, 和 R. Hadsell, “策略蒸馏”，在 *ICLR*，2016年。'
- en: '[204] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” in *ICML*, 2017, pp. 1126–1135.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] C. Finn, P. Abbeel, 和 S. Levine, “模型无关的元学习用于深度网络的快速适应”，在 *ICML*，2017年，页码
    1126–1135。'
- en: '[205] W. Zhang, Y. Zhang, and N. Liu, “Map-less navigation: a single drl-based
    controller for robots with varied dimensions,” *arXiv preprint arXiv:2002.06320*,
    2020.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] W. Zhang, Y. Zhang, 和 N. Liu, “无地图导航：用于不同尺寸机器人单一 DRL 控制器”，*arXiv 预印本
    arXiv:2002.06320*，2020年。'
- en: '[206] J. Li, X. Wang, S. Tang, H. Shi, F. Wu, Y. Zhuang, and W. Y. Wang, “Unsupervised
    reinforcement learning of transferable meta-skills for embodied navigation,” in
    *CVPR*, 2020, pp. 12 123–12 132.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] J. Li, X. Wang, S. Tang, H. Shi, F. Wu, Y. Zhuang, 和 W. Y. Wang, “无监督强化学习用于具身导航的可转移元技能”，在
    *CVPR*，2020年，页码 12 123–12 132。'
- en: '[207] D. S. Chaplot, L. Lee, R. Salakhutdinov, D. Parikh, and D. Batra, “Embodied
    multimodal multitask learning,” in *IJCAI*, vol. 3, 2020, pp. 2442–2448.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] D. S. Chaplot, L. Lee, R. Salakhutdinov, D. Parikh, 和 D. Batra, “具身多模态多任务学习”，在
    *IJCAI*，第3卷，2020年，页码 2442–2448。'
- en: '[208] X. E. Wang, V. Jain, E. Ie, W. Y. Wang, Z. Kozareva, and S. Ravi, “Environment-agnostic
    multitask learning for natural language grounded navigation,” in *ECCV*, 2020,
    pp. 413–430.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] X. E. Wang, V. Jain, E. Ie, W. Y. Wang, Z. Kozareva, 和 S. Ravi, “环境无关的多任务学习用于自然语言基础的导航”，在
    *ECCV*，2020年，页码 413–430。'
- en: '[209] B. Liu, L. Wang, and M. Liu, “Lifelong federated reinforcement learning:
    A learning architecture for navigation in cloud robotic systems,” in *IEEE Robotics
    and Automation Letters*, vol. 4, no. 4, 2019, pp. 4555–4562.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] B. Liu, L. Wang, 和 M. Liu, “终身联邦强化学习：一种用于云机器人系统导航的学习架构”，在 *IEEE Robotics
    and Automation Letters*，第4卷，第4期，2019年，页码 4555–4562。'
- en: '[210] D. Gordon, A. Kadian, D. Parikh, J. Hoffman, and D. Batra, “Splitnet:
    Sim2sim and task2task transfer for embodied visual navigation,” in *ICCV*, 2019,
    pp. 1022–1031.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] D. Gordon, A. Kadian, D. Parikh, J. Hoffman, 和 D. Batra, “Splitnet：用于具身视觉导航的Sim2sim和任务到任务的转移”，在
    *ICCV*，2019年，页码 1022–1031。'
- en: '[211] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, “Domain
    randomization for transferring deep neural networks from simulation to the real
    world,” in *IROS*.   IEEE, 2017, pp. 23–30.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, 和 P. Abbeel，“领域随机化：将深度神经网络从仿真转移到现实世界，”在
    *IROS* 中。 IEEE，2017年，第23–30页。'
- en: '[212] A. A. Rusu, M. Večerík, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell,
    “Sim-to-real robot learning from pixels with progressive nets,” in *CoRL*.   PMLR,
    2017, pp. 262–270.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] A. A. Rusu, M. Večerík, T. Rothörl, N. Heess, R. Pascanu, 和 R. Hadsell，“通过渐进网络从像素中学习的模拟到现实机器人学习，”在
    *CoRL* 中。 PMLR，2017年，第262–270页。'
- en: '[213] F. Sadeghi and S. Levine, “Cad2rl: Real single-image flight without a
    single real image,” in *RSS*, vol. 13, 2017.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] F. Sadeghi 和 S. Levine，“Cad2rl：在没有真实图像的情况下进行真实单图像飞行，”在 *RSS* 中，第13卷，2017年。'
- en: '[214] W. Yuan, K. Hang, D. Kragic, M. Y. Wang, and J. A. Stork, “End-to-end
    nonprehensile rearrangement with deep reinforcement learning and simulation-to-reality
    transfer,” *Robotics and Autonomous Systems*, vol. 119, pp. 119–134, 2019.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] W. Yuan, K. Hang, D. Kragic, M. Y. Wang, 和 J. A. Stork，“通过深度强化学习和仿真到现实转移进行端到端的非抓取重排，”
    *机器人与自主系统*，第119卷，第119–134页，2019年。'
- en: '[215] L. Tai, G. Paolo, and M. Liu, “Virtual-to-real deep reinforcement learning:
    Continuous control of mobile robots for mapless navigation,” in *IROS*.   IEEE,
    2017, pp. 31–36.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] L. Tai, G. Paolo, 和 M. Liu，“虚拟到现实深度强化学习：无地图导航的移动机器人连续控制，”在 *IROS* 中。
    IEEE，2017年，第31–36页。'
- en: '[216] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative
    domain adaptation,” in *CVPR*, 2017, pp. 7167–7176.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] E. Tzeng, J. Hoffman, K. Saenko, 和 T. Darrell，“对抗性判别领域适应，”在 *CVPR* 中，2017年，第7167–7176页。'
- en: '[217] F. Zhu, L. Zhu, and Y. Yang, “Sim-real joint reinforcement transfer for
    3d indoor navigation,” in *CVPR*, 2019, pp. 11 388–11 397.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] F. Zhu, L. Zhu, 和 Y. Yang，“用于3D室内导航的模拟-现实联合强化转移，”在 *CVPR* 中，2019年，第11 388–11 397页。'
- en: '[218] W. Zhao, J. P. Queralta, and T. Westerlund, “Sim-to-real transfer in
    deep reinforcement learning for robotics: a survey,” in *SSCI*, 2020, pp. 737–744.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] W. Zhao, J. P. Queralta, 和 T. Westerlund，“深度强化学习中的模拟到现实转移用于机器人：综述，”在
    *SSCI* 中，2020年，第737–744页。'
- en: '[219] B. Qin, Y. Gao, and Y. Bai, “Sim-to-real: Six-legged robot control with
    deep reinforcement learning and curriculum learning,” in *ICRAE*, 2019.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] B. Qin, Y. Gao, 和 Y. Bai，“从模拟到现实：使用深度强化学习和课程学习的六足机器人控制，”在 *ICRAE* 中，2019年。'
- en: '[220] H. F. Bassani, R. A. Delgado, J. N. de O. Lima Junior, H. R. Medeiros,
    P. H. M. Braga, and A. Tapp, “Learning to play soccer by reinforcement and applying
    sim-to-real to compete in the real world,” *CoRR*, 2020.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] H. F. Bassani, R. A. Delgado, J. N. de O. Lima Junior, H. R. Medeiros,
    P. H. M. Braga, 和 A. Tapp，“通过强化学习学习踢足球并将模拟到现实应用于现实世界竞赛，” *CoRR*，2020年。'
- en: '[221] B. Shen, F. Xia, C. Li, R. Martín-Martín, L. Fan, G. Wang, S. Buch, C. D’Arpino,
    S. Srivastava, L. P. Tchapmi, M. E. Tchapmi, K. Vainio, L. Fei-Fei, and S. Savarese,
    “igibson, a simulation environment for interactive tasks in large realistic scenes.”
    *arXiv preprint arXiv:2012.02924*, 2020.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] B. Shen, F. Xia, C. Li, R. Martín-Martín, L. Fan, G. Wang, S. Buch, C.
    D’Arpino, S. Srivastava, L. P. Tchapmi, M. E. Tchapmi, K. Vainio, L. Fei-Fei,
    和 S. Savarese，“igibson，一个用于大型真实场景交互任务的仿真环境。” *arXiv 预印本 arXiv:2012.02924*，2020年。'
- en: '[222] Y. Zhu, Y. Weng, F. Zhu, X. Liang, Q. Ye, Y. Lu, and J. jiao, “Self-motivated
    communication agent for real-world vision-dialog navigation,” in *ICCV*, 2021.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] Y. Zhu, Y. Weng, F. Zhu, X. Liang, Q. Ye, Y. Lu, 和 J. jiao，“用于现实世界视觉对话导航的自我激励通信代理，”在
    *ICCV* 中，2021年。'
- en: '[223] L. Jeni, Z. Istenes, P. Korondi, and H. Hashimoto, “Hierarchical reinforcement
    learning for robot navigation using the intelligent space concept,” in *2007 11th
    International Conference on Intelligent Engineering Systems*.   IEEE, 2007, pp.
    149–153.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] L. Jeni, Z. Istenes, P. Korondi, 和 H. Hashimoto，“用于机器人导航的分层强化学习及智能空间概念，”在
    *2007年第11届智能工程系统国际会议* 中。 IEEE，2007年，第149–153页。'
- en: '[224] Z. Yang, Z. Dai, Y. Yang, J. G. Carbonell, R. Salakhutdinov, and Q. V.
    Le, “Xlnet: Generalized autoregressive pretraining for language understanding,”
    in *NeurIPS*, vol. 32, 2019, pp. 5753–5763.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] Z. Yang, Z. Dai, Y. Yang, J. G. Carbonell, R. Salakhutdinov, 和 Q. V.
    Le，“Xlnet：用于语言理解的广义自回归预训练，”在 *NeurIPS* 中，第32卷，2019年，第5753–5763页。'
- en: '[225] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, “Biobert:
    a pre-trained biomedical language representation model for biomedical text mining.”
    *Bioinformatics*, vol. 36, no. 4, pp. 1234–1240, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, 和 J. Kang，“Biobert：一个用于生物医学文本挖掘的预训练生物医学语言表示模型。”
    *生物信息学*，第36卷，第4期，第1234–1240页，2019年。'
- en: '[226] S. Levine and P. Abbeel, “Learning neural network policies with guided
    policy search under unknown dynamics,” in *NeurIPS*, vol. 27, 2014, pp. 1071–1079.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] S. Levine 和 P. Abbeel, “在未知动态下通过引导策略搜索学习神经网络策略，” *NeurIPS*, 第27卷, 2014年,
    第1071–1079页。'
- en: '| ![[Uncaptioned image]](img/ac4f7f759354922f4a1911a5471a0a93.png) | Fengda
    Zhu received the bachelor’s degree in School of Software Engineering from Beihang
    University, Beijing, China, in 2017\. He is currently pursuing the Ph.D. degree
    with the Faculty of Information Technology, Monash University under the supervision
    of Prof. Xiaojun Chang. His research interests include machine learning, deep
    learning and reinforcement learning. |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/ac4f7f759354922f4a1911a5471a0a93.png) | Fengda Zhu 于2017年获得北京航空航天大学软件工程学院学士学位。他目前在蒙纳士大学信息技术学院攻读博士学位，导师是**Xiaojun
    Chang** 教授。他的研究兴趣包括机器学习、深度学习和强化学习。 |'
- en: '| ![[Uncaptioned image]](img/9dd2a220972e7f7575653f27538e17a3.png) | Yi Zhu
    received the B.S. degree in software engineering from Sun Yat-sen University,
    Guangzhou, China, in 2013\. Since 2015, she has been a Ph.D student in computer
    science with the School of Electronic, Electrical, and Communication Engineering,
    University of Chinese Academy of Sciences, Beijing, China. Her current research
    interests include object recognition, scene understanding, weakly supervised learning
    and visual reasoning. |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/9dd2a220972e7f7575653f27538e17a3.png) | Yi Zhu 于2013年获得广州中山大学软件工程学士学位。从2015年起，她在中国科学院电子电气与通信工程学院攻读计算机科学博士学位。她目前的研究兴趣包括物体识别、场景理解、弱监督学习和视觉推理。
    |'
- en: '| ![[Uncaptioned image]](img/234615cc3789528ac7f8116446fdc825.png) | Vincent
    Lee is currently an Associate Professor at Machine learning and Deep Learning
    Discipline of the Department of Data Science and artificial Intelligence, Faculty
    of IT, Monash University, Australia. He is a senior member of IEEE USA. He received
    Australia Federal Government scholarship to pursue PhD from 1988 through to 1991
    at The University of New Castle, NSW, in Australia. In 1973 to 1974, he was awarded
    a joint research scholarship by Ministry of Defence (Singapore) and Ministry of
    Defence (UK) for postgraduate study at Royal Air Force College, UK in aircraft
    electrical and instrument systems. He was a visiting academic to Tsinghua University
    in Beijing at the School of Economics and Management from Nov 2006 to March 2007;
    and was also a Visiting Professor to Information Communication Institute of Singapore
    from July 1994 through June 1995. |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/234615cc3789528ac7f8116446fdc825.png) | Vincent Lee 目前是澳大利亚蒙纳士大学信息技术学院数据科学与人工智能系机器学习与深度学习学科的副教授。他是
    IEEE USA 的高级会员。他获得了澳大利亚联邦政府奖学金，于1988年至1991年在澳大利亚新城大学攻读博士学位。在1973年至1974年间，他获得了新加坡国防部和英国国防部联合研究奖学金，前往英国皇家空军学院进行飞机电气和仪表系统的研究生学习。他在2006年11月至2007年3月期间曾是清华大学经济与管理学院的访问学者；同时也是新加坡信息通信研究所的访问教授，从1994年7月到1995年6月。
    |'
- en: '| ![[Uncaptioned image]](img/f2314019aa8bc325d8c9781fa6137750.png) | Xiaodan
    Liang is currently an Associate Professor at Sun Yat-sen University. She was a
    postdoc researcher in the machine learning department at Carnegie Mellon University,
    working with Prof. Eric Xing, from 2016 to 2018\. She received her PhD degree
    from Sun Yat-sen University in 2016, advised by Liang Lin. She has published several
    cutting-edge projects on human-related analysis, including human parsing, pedestrian
    detection and instance segmentation, 2D/3D human pose estimation and activity
    recognition. |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/f2314019aa8bc325d8c9781fa6137750.png) | Xiaodan Liang 目前是中山大学的副教授。她曾在2016年至2018年担任卡内基梅隆大学机器学习部门的博士后研究员，与
    Eric Xing 教授合作。她于2016年在中山大学获得博士学位，导师是梁林。她在与人相关的分析领域发表了多项前沿研究，包括人类解析、行人检测和实例分割、2D/3D
    人体姿态估计和活动识别。 |'
- en: '| ![[Uncaptioned image]](img/0dd571195b6a48400f1f5040616c97bf.png) | Xiaojun
    Chang is currently an Associate Professor with School of Computing Technologies,
    RMIT University, Australia. Before joining RMIT, he was a Senior Lecturer with
    the Faculty of Information Technology, Monash University Clayton Campus, Clayton,
    VIC, Australia. He is also with the Monash University Centre for Data Science.
    He was a Post-Doctoral Research Associate with the School of Computer Science,
    Carnegie Mellon University, Pittsburgh, PA, USA, working with Prof. A. Hauptmann.
    He has spent most of the time working on exploring multiple signals (visual, acoustic,
    and textual) for automatic content analysis in unconstrained or surveillance videos.
    Dr. Chang is an ARC Discovery Early Career Researcher Award (DECRA) Fellow from
    2019 to 2021\. He has achieved top performance in various international competitions,
    such as TRECVID MED, TRECVID SIN, and TRECVID AVS. |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图像]](img/0dd571195b6a48400f1f5040616c97bf.png) | Xiaojun Chang 目前是澳大利亚
    RMIT 大学计算技术学院的副教授。在加入 RMIT 之前，他曾是澳大利亚维多利亚州克莱顿的蒙纳士大学信息技术学院的高级讲师。他还与蒙纳士大学数据科学中心合作。他曾是美国宾夕法尼亚州匹兹堡卡内基梅隆大学计算机科学学院的博士后研究员，与
    A. Hauptmann 教授合作。他主要致力于探索多种信号（视觉、听觉和文本）用于自动内容分析，特别是在非约束或监控视频中。Chang 博士是 2019
    至 2021 年的 ARC Discovery Early Career Researcher Award (DECRA) 奖学金获得者。他在 TRECVID
    MED、TRECVID SIN 和 TRECVID AVS 等各种国际竞赛中取得了优异的成绩。'
