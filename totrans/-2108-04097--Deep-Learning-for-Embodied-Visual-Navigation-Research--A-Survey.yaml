- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:52:25'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2108.04097] Deep Learning for Embodied Visual Navigation Research: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2108.04097](https://ar5iv.labs.arxiv.org/html/2108.04097)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for Embodied Visual Navigation Research: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fengda Zhu, Yi Zhu, Vincent CS Lee, Xiaodan Liang and Xiaojun Chang Fengda Zhu
    and Vincent CS Lee are with Department of Data Science and AI, Faculty of Information
    Technology, Monash University. Yi Zhu is with University of Chinese Academy of
    Sciences. Xiaodan Liang is with School of Intelligent Systems Engineering, Sun
    Yat-sen University. Xiaojun Chang is with School of Computing Technologies, RMIT
    University. Manuscript received July 8, 2021.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: “Embodied visual navigation” problem requires an agent to navigate in a 3D environment
    mainly rely on its first-person observation. This problem has attracted rising
    attention in recent years due to its wide application in vacuum cleaner, and rescue
    robot, etc. A navigation agent is supposed to have various intelligent skills,
    such as visual perceiving, mapping, planning, exploring and reasoning, etc. Building
    such an agent that observes, thinks, and acts is a key to real intelligence. The
    remarkable learning ability of deep learning methods empowered the agents to accomplish
    embodied visual navigation tasks. Despite this, embodied visual navigation is
    still in its infancy since a lot of advanced skills are required, including perceiving
    partially observed visual input, exploring unseen areas, memorizing and modeling
    seen scenarios, understanding cross-modal instructions, and adapting to a new
    environment, etc. Recently, embodied visual navigation has attracted rising attention
    of the community, and numerous works has been proposed to learn these skills.
    This paper attempts to establish an outline of the current works in the field
    of embodied visual navigation by providing a comprehensive literature survey.
    We summarize the benchmarks and metrics, review different methods, analysis the
    challenges, and highlight the state-of-the-art methods. Finally, we discuss unresolved
    challenges in the field of embodied visual navigation and give promising directions
    in pursuing future research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: deep learning, embodied environments, embodied visual navigation, cross-modal
    navigation, navigation robotics.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/75a8025d7e6db04837d17de88cd5ba51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A taxonomy of deep learning methods for embodied navigation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Building a robot to accomplish tasks autonomously in place of humans has been
    a topic researched for a long time [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)].
    Some complex applications, such as vacuum cleaning, disabled helping and rescuing,
    require an agent to navigate to finish several sub-tasks in different places in
    a 3D embodied environment. Therefore, navigation is one of the key capabilities
    in building the intelligent navigable robots in the real world. During the process
    of navigation, a robot needs to move around to find the target location by perceiving
    embodied visual inputs, which is named as “embodied visual navigation”. The agent
    that interacts with the environment through its physical entity within that environment
    is named “embodied agent” [[4](#bib.bib4)]. Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Deep Learning for Embodied Visual Navigation Research: A Survey") demonstrates
    a navigation process. An agent firstly receives an instruction “*Put the chair
    in the living room into the second balcony*”. Then it navigates to find the target
    chair. The agent picks up the chair and navigate to the balcony and put it down.'
  prefs: []
  type: TYPE_NORMAL
- en: Early works on robotics navigation [[5](#bib.bib5), [6](#bib.bib6)] mainly rely
    on hand-crafted features like optical flow and traditional algorithms like Markov
    localization [[7](#bib.bib7)], incremental localization [[8](#bib.bib8)], or landmark
    tracking [[9](#bib.bib9)]. These methods involve lots of hyper-parameters and
    cannot generalize well in unseen environments. Recent developments of deep learning
    reveal its ability to learn a robust model from large-scale data. Vision robots
    trained by end-to-end deep learning methods is more robust, have less hyper-parameters,
    and have better generalization ability in unseen environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, some challenges are going to be tackled in achieving deep learning
    for embodied visual navigation: 1) collecting data from the real world is expensive;
    2) the model learned from partial observation is unstable; 3) it is difficult
    to learn the skills for long-term navigation such as exploration and memorization;
    4) perceiving natural language instructions is challenging because natural language
    is diverse with flexible formats; 5) the large domain gap between the simulated
    environment and the real-world environment impedes the adaptation of navigation
    policy, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper discusses the related works in robot navigation and give a promising
    direction in building real-world navigation robots. The structure of this paper
    is shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey"). Training and testing in the real-world
    has many disadvantages: 1) The data sampling efficiency in the real world is very
    low since a real robot can only sample a trajectory at once while a simulator
    can efficiently sample trajectories in multi-processing; 2) the complexity and
    the dynamic of the real-world environment hinges the reproductivity; 3) there
    is large domain bias between different environments, etc. The development of 3D
    simulation technology enables researchers to construct a simulated environment [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)] to study
    in building a robust navigation agent within it. Simulators render these 3D assets
    to generate RGB-D images and provide sensors like physical sensors, GPS sensors
    to simulate a realistic embodied robotic environment. Learning to navigation in
    a simulated environment is a broad field with lots of challenges to solve. In
    solving target-driven navigation problem, researchers propose model-free methods [[15](#bib.bib15),
    [10](#bib.bib10), [16](#bib.bib16)], self-supervised methods [[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19)], planning-based methods [[20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)]. Perceiving natural language is a challenging
    task due to its diversity and complexity. It requires the agents not only can
    following a sentence instruction step-by-step [[12](#bib.bib12), [23](#bib.bib23),
    [24](#bib.bib24)], but also understand dialogues [[20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22)] or navigate to answer questions [[20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22)]. In building a real-world navigation robots, some works [[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)] proposed to train an agent in real-world environments
    directly while other works [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]
    propose to introduce transfer learning to transfer the learned navigation policy
    from simulated environments to the real-world environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/63afb6d1175851d1d69b31ecbfb89eeb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A demonstration of a navigation process, in which a robot move to
    several places to accomplish a task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared with previous surveys of robotic navigation [[6](#bib.bib6), [5](#bib.bib5)],
    our paper focuses on deep learning methods that solve the embodied navigation
    problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To the best of our knowledge, our paper is the first comprehensive study on
    the advances of deep learning methods on embodied navigation tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This paper summarizes and compares their unique insights of recently proposed
    embodied navigation datasets, simulators and tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This paper introduce deep learning methods for embodied visual navigation, including
    their motivations and contributions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This paper classifies the research results in recent years, and gives some promising
    embodied navigation directions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The paper is organized as follows. Sec. [2](#S2 "2 Embodied Navigation Environments
    ‣ Deep Learning for Embodied Visual Navigation Research: A Survey") discusses
    the current embodied datasets and embodied simulators. Sec. [3](#S3 "3 Embodied
    Navigation Benchmarks ‣ Deep Learning for Embodied Visual Navigation Research:
    A Survey") introduces the embodied navigation benchmarks including the navigation
    tasks and navigation metrics. Sec. [4](#S4 "4 Methods in Simulated Environments
    ‣ Deep Learning for Embodied Visual Navigation Research: A Survey") lists the
    methods to train an agent navigation in a simulated embodied navigation environment,
    where Sec. [4.1](#S4.SS1 "4.1 Target-driven Navigation ‣ 4 Methods in Simulated
    Environments ‣ Deep Learning for Embodied Visual Navigation Research: A Survey")
    lists the methods for target-driven tasks and Sec. [4.2](#S4.SS2 "4.2 Cross-modal
    Navigation ‣ 4 Methods in Simulated Environments ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey") introduces the methods for cross-modal
    tasks. Sec. [5](#S5 "5 Methods in Real-world Environments ‣ Deep Learning for
    Embodied Visual Navigation Research: A Survey") summarizes the works that builds
    a navigation robot. Sec. [6](#S6 "6 Navigation from Simulator to Real-world ‣
    Deep Learning for Embodied Visual Navigation Research: A Survey") illustrates
    the domain gap betweeen simulated environments and the real-world environment,
    and introduce the methods that solves these challenges. In Sec. [7](#S7 "7 Future
    Directions ‣ Deep Learning for Embodied Visual Navigation Research: A Survey"),
    we highlight the recent state-of-the-art works, discuss about the limitations
    of current works, and propose promising directions in building a real-world navigation
    robot.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Year | Scenes | Rooms | Object Catagories | RGB | Depth | 2D Semantics
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Stanford Scene* [[31](#bib.bib31)] | 2012 | 130 | 130 | - | synthetic | ✗
    | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| SceneNet* [[32](#bib.bib32)] | 2016 | 57 | 57 | 218 | synthetic | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2D-3D-S* [[33](#bib.bib33)] | 2017 | 270 | 270 | 13 | synthetic | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| SUNCG [[34](#bib.bib34)] | 2017 | 45,622 | 775,574 | 84 | synthetic | ✓ |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| CHALET [[35](#bib.bib35)] | 2018 | 10 | 58 | 150 | synthetic | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Matterport3D [[36](#bib.bib36)] | 2017 | 90 | 2,056 | 40 | realistic | ✓
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Gibson [[14](#bib.bib14)] | 2018 | 572 | 8,854 | 84 | realistic | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Replica [[37](#bib.bib37)] | 2019 | 18 | 35 | 88 | realistic | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE I: Comparison of existing embodied datasets (*: the datasets render only
    a room as scene).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bec46b8e43bcfc24c975b1cbaae140be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The render scenes of each dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Embodied Navigation Environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we discuss the environments used for embodied navigation. We summarize
    the dataset that provides 3D assets and the simulators that render assets and
    provide interactive interfaces for navigation agents.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Embodied Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An embodied dataset contains 3D assets like textures and meshes for rendering
    and other configuration data like object location, object category and camera
    pose for high-level tasks. A comparison of the proposed datasets is shown in Tab. [I](#S1.T1
    "TABLE I ‣ 1 Introduction ‣ Deep Learning for Embodied Visual Navigation Research:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Early works focused on rendering composite RGB views [[31](#bib.bib31)]. It
    trains a probabilistic model to generate synthetic data based on hand-created
    scenes. Later, SceneNet [[32](#bib.bib32)] introduces a generator model to annotate
    2D semantics. As depth channel is proved to be helpful for navigation agents [[19](#bib.bib19),
    [38](#bib.bib38)], 2D-3D-S [[33](#bib.bib33)] provides assets with depth information.
    Different from these works that render a single room at once, later works [[34](#bib.bib34),
    [39](#bib.bib39), [40](#bib.bib40)] provides a large number of scenes consist
    of bedrooms, living rooms, bathrooms, kitchens, etc. However, the synthetic view
    used by the aforementioned datasets is quite different from the real world scene,
    which limits the application of the datasets. To this end, Matterport3D [[36](#bib.bib36)]
    provides photo-realistic panoramic views by 3D reconstruction and 2D and 3D semantics
    of these views. Gibson [[14](#bib.bib14)] provides a more diverse dataset with
    572 houses. Replica [[37](#bib.bib37)] proposes a dataset with 18 indoor scene
    consist of dense meshes and high-resolution textures. Some work such as AI2-THOR [[40](#bib.bib40)],
    RoboTHOR [[39](#bib.bib39)] and CHALET [[35](#bib.bib35)] rely on the datasets
    that not currently released. The rendering scenes of some datasets are shown in
    Fig. [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Deep Learning for Embodied Visual
    Navigation Research: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Embodied Simulators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An embodied simulator provides an interface for an agent to interact with the
    environment. We compare different features of the existing simulators in Tab. [II](#S2.T2
    "TABLE II ‣ 2.2 Embodied Simulators ‣ 2 Embodied Navigation Environments ‣ Deep
    Learning for Embodied Visual Navigation Research: A Survey"). A simulators is
    equipped with many sensors, such as a RGB sensor, a depth sensor, a physical sensor
    and a position sensor. Early simulators provide low RGB resolution and unrealistic
    imagery due to the limit of 3D rendering technology. The lack of visual detail,
    limits the navigation performance of the agent. Afterwards, to address this, simulations
    such as Matterport3D simulator [[12](#bib.bib12)], Gibson simulator [[14](#bib.bib14)]
    and Habitat [[13](#bib.bib13)] propose high-resolution photo-realistic panoramic
    view to simulate a more realistic environment. Rendering frame rate is also important
    to embodied simulators since it is critical to training efficiency. MINOS [[11](#bib.bib11)]
    runs over 100 frames per second (FPS), which is 10 times faster than its previous
    works. Habitat [[13](#bib.bib13)] over than 1000 FPS on $512\times 512$ RGB-D
    image, making it become the fastest simulator among existing simulators. Discrete
    state space in [[12](#bib.bib12)] simplifies the navigation problem and makes
    the agent easy to learn complex vision-language navigation tasks. However, continuous
    state space is more welcome since it facilitates transferring a learned agent
    to a real-world robot. A customizable simulator is able to generate more diverse
    data by moving the objects, changing the textures of objects and reconfiguring
    the lights. Diverse data has little bias and therefore, enables the deep learning
    to learn a robust navigation policy. Despite of navigating to find the target
    object in a static room, interacting is another key skill for real-world robots.
    Some complex tasks may require a robot to interact with objects, such as picking
    up a cup, moving a chair, or opening a door. AI2-THOR [[40](#bib.bib40)], iGibson [[41](#bib.bib41)]
    and RoboTHOR [[39](#bib.bib39)] provide interactive environments to train such
    a skill. Multi-agent reinforcement learning [[42](#bib.bib42), [43](#bib.bib43)]
    is an emerging problem of cooperation and competition among agents. AI2-THOR and
    iGibson also support multi-agent training in studying cooperative tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Simulator | Year | Use Dataset(s) | Resolution | Physics | FPS | Customizable
    | Interactive | Multi-agent |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MINOS [[11](#bib.bib11)] | 2017 | SUNCG, Matterport3D | $84\times 84$ | ✓
    | 100 | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| AI2-THOR* [[40](#bib.bib40)] | 2017 | - | $300\times 300$ | ✓ | 120 | ✓ |
    ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| House3D [[10](#bib.bib10)] | 2018 | SUNCG | $120\times 90$ | ✗ | 600 | ✓
    | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| CHALET [[35](#bib.bib35)] | 2018 | CHALET | $800\times 600$ | ✗ | 10 | ✗
    | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Matterport3D [[12](#bib.bib12)] | 2018 | Matterport3D | $512\times 512$ |
    ✗ | 1,000 | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Gibson [[14](#bib.bib14)] | 2018 | Gibson, Matterport3D, 2D-3D-S | $512\times
    512$ | ✓ | 400 | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| iGibson [[41](#bib.bib41)] | 2018 | Gibson | $512\times 512$ | ✓ | 400 |
    ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Habitat [[13](#bib.bib13)] | 2019 | Matterport3D, Gibson, Replica | $512\times
    512$ | ✓ | 10,000 | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| RoboTHOR* [[39](#bib.bib39)] | 2020 | - | $300\times 300$ | ✓ | 1200 | ✓
    | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE II: Comparison of existing embodied simulators (*: the dataset that the
    simulator uses is not currently released).'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Embodied Navigation Benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we introduce several tasks to study the embodied visual navigation problem.
    These tasks can be divided into three categories: target-driven navigation task,
    cross-modal navigation task, and interactive navigation task.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Target-driven Navigation Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PointGoal Navigation, firstly defined by Anderson *et al.* [[44](#bib.bib44)],
    is a task where an agent is initialized to a random starting position and orientation
    then asked to navigate to a target position. The target position is indicated
    by its relative coordinates to the starting position. This task requires an agent
    to estimate the cumulative distance from the starting position so that the agent
    knows how far away from the goal. Theoretically, this task is able to be applied
    to all embodied environments.
  prefs: []
  type: TYPE_NORMAL
- en: ObjectGoal Navigation is proposed by Zhu *et al.* [[15](#bib.bib15)]. In this
    task, an agent is initialized to a random starting position and is required to
    find a specific object, such as a desk or a bed. Once the navigation agent find
    the object, it stops. The navigation process is regarded as a *success* if the
    agent is located within a distance to the target object. In addition to the room
    structure, the *ObjectGoal* navigation task needs the object labels and locations.
    Object recognizing and exploring are key skills to the *ObjectGoal* navigation.
  prefs: []
  type: TYPE_NORMAL
- en: RoomGoal Navigation is proposed by Wu *et al.* [[10](#bib.bib10)]. In this task,
    an agent initialized at a random position is asked to navigate to a room (e.g.
    bedroom or kitchen). The navigation process is regarded as a *success* if the
    agent is stopped within the target room. *RoomGoal* navigation requires the room
    annotations. The concept of the room is a high-level semantic. Therefore, a *RoomGoal*
    navigation agent needs to understand the scene based on the visual details, such
    as furniture type and room layout.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Object Navigation (MultiON) Recently, more and more researchers are paying
    attention to long-term navigation where an agent memorize all the visited scenes.
    Motivated by this, Wani *et al.* [[45](#bib.bib45)] propose MultiON, a benchmark
    for Multi-Object Navigation. In MultiON, an agent is asked to navigate to multiple
    target objects one-by-one, which makes the navigation trajectory quite long. The
    agent raise a FOUND action when it reaches the instructed target. Perception and
    effective planning under partial observation would be the key to solve this task.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Cross-modal Navigation Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vision-and-Language Navigation (VLN) VLN is a task where an agent navigates
    step-by-step following natural language instructions [[12](#bib.bib12)]. Previous
    tasks such as *ObjectGoal* and *RoomGoal* hard-code the object and room semantics
    as a one-hot vector. On the contrary, VLN introduce natural language sentences
    to instruct the navigation process like *“Head upstairs and walk past the piano
    through an archway directly in front. Turn right when the hallway ends at pictures
    and table. Wait by the moose antlers hanging on the wall”*. The VLN task is successfully
    completed if the agent stops close to the intended goal following the instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several datasets have been proposed for vision-language navigation:
    R2R [[12](#bib.bib12)], R4R [[46](#bib.bib46)], and RxR [[47](#bib.bib47)]. The
    room-to-room (R2R) dataset is proposed in [[12](#bib.bib12)] to study vision-language
    navigation. The R2R dataset contains 21,567 navigation instructions with an average
    length of 29 words. However, the R2R dataset has several shortcomings: 1) the
    referenced paths are direct-to-goal so that R2R instructions lack the capability
    of describing complex paths; 2) the instruction consists of several sentences
    and not fine-grained; 3) the training data is small, and the model is easily overfitting;
    4) the language of instruction is English only, and no other languages are included.
    To address these problems, more advances datasets have been proposed. Jain *et
    al.* [[46](#bib.bib46)] cross-connects the trajectories and instructions in R2R
    and generate a new dataset named R4R. FGR2R [[48](#bib.bib48)] enriches R2R with
    sub-instructions and their corresponding trajectories. RxR [[47](#bib.bib47)]
    is a time-aligned dataset and it relieves the known biases in trajectories and
    elicits more references to visible entities in R2R.'
  prefs: []
  type: TYPE_NORMAL
- en: Navigation from Dialog History (NDH) When navigating in an unfamiliar environment,
    a human usually asks for assistance and continued navigation according to the
    responses of other humans. However, building an agent that is able to autonomously
    ask natural language questions and react to the answer is still a long-term goal
    in robotic navigation. In NDH [[49](#bib.bib49)], an agent is required to navigate
    according to a dialog history, which consists of several question-answering pairs.
    Studying NDH is fundamental for building a real-world dialog navigation robot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Embodied Questioning and Answering (EQA) Visual Question Answering (VQA) [[50](#bib.bib50)]
    is a cross-modal task, in which a system answers a text-based question with a
    given image. VQA soon became one of the most popular computer vision tasks, because
    it revealed the possibility of interaction between human beings and artificial
    intelligence agents in natural language [[51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53)].
    Compared with VQA, a more advanced activity is to answer questions by self-exploration
    in an unseen environment. Embodied Questioning and Answering (EQA) [[54](#bib.bib54)]
    is a task where an agent is spawned at a random location in a 3D environment and
    asked a question. EQA is a challenging task since it requires a wide range of
    AI skills: visual perception, language understanding, target-driven navigation,
    commonsense reasoning, etc. In addition to navigation accuracy in other tasks,
    EQA propose EQA accuracy to measure whether the agent correctly answers the question
    or not.'
  prefs: []
  type: TYPE_NORMAL
- en: REVERIE Recently, Qi *et al.* [[55](#bib.bib55)] proposes Remote Embodied Visual
    referring Expression in Real Indoor Environments, named REVERIE in short, to research
    associating natural language instructions and the visual semantics. Different
    from VLN that gives an instruction that describes the trajectory step-by-step
    toward the target, the natural language instruction in REVERIE refers to a remote
    target object. Compared with *ObjectGoal* navigation, REVERIE offers rich language
    descriptions to enable the agent to find a unique target in the house.
  prefs: []
  type: TYPE_NORMAL
- en: 'Audio-visual Navigation, proposed by Chen *et al.* [[56](#bib.bib56)] introduces
    audio modality for embodied navigation environment. This task requires the agent
    to navigate to a sound object by seeing and hearing. It encourages researchers
    to study the role of audio plays in navigation. This work also offer the SoundSpaces [[56](#bib.bib56)]
    dataset for the Audio-visual Navigation task. The SoundSpaces dataset is built
    upon two simulators, Replica and Matterport3D. It contains 102 natural sounds
    across a wide variety of categories: bell, door opening, music, people speaking,
    telephone, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric Name | $\uparrow\downarrow$ | Formulation | PS | SP | UO | SI | OS
    | CC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Path Length (PL) | - | $\sum_{1\leq i<&#124;P&#124;}d(p_{i},p_{i+1})$ | -
    | ✓ | - | ✗ | ✗ | $O(&#124;P&#124;)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Navigation Error (NE) | $\downarrow$ | $d(p_{&#124;P&#124;},r_{&#124;R&#124;})$
    | ✗ | ✓ | ✗ | ✗ | ✗ | $O(1)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle Navigation Error (ONE) | $\downarrow$ | $\min_{p\in P}d(p,r_{&#124;R&#124;})$
    | ✗ | ✓ | ✗ | ✗ | ✗ | $O(&#124;P&#124;)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Success Rate (SR) | $\uparrow$ | $\mathbbm{1}[\text{NE}(P,R)\leq d_{th}]$
    | ✗ | ✗ | ✗ | ✓ | ✗ | $O(1)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle Success Rate (OSR) | $\uparrow$ | $\mathbbm{1}[\text{ONE}(P,R)\leq
    d_{th}]$ | ✗ | ✗ | ✗ | ✓ | ✗ | $O(&#124;P&#124;)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Success weighted by PL (SPL) | $\uparrow$ | $\text{SR}(P,R)\cdot\dfrac{d(p_{1},r_{&#124;R&#124;})}{\max\{\text{PL}(P),d(p_{1},r_{&#124;R&#124;})\}}$
    | ✗ | ✓ | ✓ | ✓ | ✗ | $O(&#124;P&#124;)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Success weighted by Edit Distance (SED) | $\uparrow$ | $\text{SR}(P,R)\left(1-\dfrac{\text{ED}(P,R)}{\max{\{&#124;P&#124;,&#124;R&#124;}\}-1}\right)$
    | ✓ | ✗ | ✓ | ✓ | ✗ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Path Coverage (PC) | $\uparrow$ | $\dfrac{1}{&#124;R&#124;}\sum_{r\in R}\exp\left(-\dfrac{d(r,P)}{d_{th}}\right)$
    | ✓ | ✓ | ✓ | ✓ | ✗ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Length Score (LS) | - | $\dfrac{1}{1+&#124;1-\frac{\text{PL}(P)}{\text{PC}(P,R)\cdot\text{PL}(R)}&#124;}$
    | ✗ | ✓ | ✗ | ✗ | ✗ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Coverage weighted by LS (CLS) | $\uparrow$ | $\text{PC}(P,R)\cdot\text{LS}(P,R)$
    | ✓ | ✓ | ✓ | ✓ | ✗ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Normalized Dynamic Time Warping (nDTW) | $\uparrow$ | $\exp\left({-\dfrac{\min\limits_{W\in\mathcal{W}}\sum_{(i_{k},j_{k})\in
    W}d(r_{i_{k}},p_{j_{k}})}{&#124;R&#124;\cdot d_{th}}}\right)$ | ✓ | ✓ | ✓ | ✓
    | ✓ | $O(&#124;R&#124;\cdot&#124;P&#124;)$ |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: We compare the existing metrics from several aspects, including
    performance ($\uparrow$ indicates the higher the better, $\downarrow$ indicates
    the lower the better), Formulation, Path similarity (PS), Soft Penalties (SP),
    Unique Optimum (UO), Scale Invariance (SI), Order Sensitivity (OS), and Computational
    Complexity (CC). Suppose we have a predicted trajectory $P$ and a ground truth
    trajectory $R$. $p_{i}$ and $r_{i}$ are the ith node on trajectory $P$ and $R$.
    $|P|$ and $|R|$ stand for the length of $P$ and $R$ respectively. The Dijkstra
    distance of the house has been preprocessed and the computation complexity of
    any $d(p_{i},r_{j})$ is $O(1)$.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Target Embodied Questioning and Answering (MT-EQA) The natural language
    questions in EQA are simple since each of them describes one object and lacks
    attributes and relationships between multiple targets. In MT-EQA [[57](#bib.bib57)],
    the instructions are like *“Is the dresser in the bedroom bigger than the oven
    in the kitchen”*, where the *dresser* and the *oven* locate in different places
    with different attributes. Thus the agent has to navigate to multiple places,
    find all targets, analyze the relationships between them, and answer the question.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Interactive Navigation Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Interactive Questioning and Answering (IQA) Building an agent which is able
    to interact with a dynamic environment is a long-standing goal of the AI community.
    Recently proposed interactive simulators [[41](#bib.bib41), [39](#bib.bib39),
    [40](#bib.bib40)] provide basic functions like opening a door or moving a chair,
    which enables researchers to build an interactive navigation agent. Interactive
    Questioning and Answering (IQA) [[58](#bib.bib58)] asks an agent to answer questions
    by interacting with objects in an environment. IQA contains 76,800 training questions
    that include existence questions, counting questions, spatial relationship questions.
  prefs: []
  type: TYPE_NORMAL
- en: “Help, Anna!” (HANNA) HANNA [[59](#bib.bib59)] is an object-finding task that
    allows an agent to request help from Automatic Natural Navigation Assistants (ANNA)
    when it gets lost. Different from NDH that provides a global dialog history as
    the instruction, the HANNA offers an environment where the instructions dynamically
    change by the situation. The environment creates an interface that enables a human
    to help the agent when it gets lost in testing time.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many evaluation metrics have been proposed to evaluate how well a navigation
    agent performs. We divide them into two categories: trajectory-insensitive metrics
    and the trajectory-sensitive metrics. Trajectory-insensitive Metrics Zhu *et al.* [[15](#bib.bib15)]
    use the average number of steps (i.e., average trajectory length) it takes to
    reach a target from a random starting point. However, there is a large proportion
    of trajectories fail when the navigation environment becomes more complex and
    the navigation task becomes more challenging. Later works [[10](#bib.bib10), [60](#bib.bib60),
    [11](#bib.bib11)] introduce propose the Success Rate (SR) to measure frequency
    of the agent successfully reach the goal and other works [[60](#bib.bib60), [54](#bib.bib54)]
    report Navigation Error (NE), the mean distance toward the goal when the agent
    finally stops. Oracle Success Rate (OSR) is proposed to evaluate if the agent
    correctly stops following the oracle stopping rule [[61](#bib.bib61), [12](#bib.bib12)].
    These metrics measure the probability of whether the agent completes the task
    or not, however, fail to measure how much proportion it completes the task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Trajectory-sensitive Metrics Success weighted by Path Length (SPL) is the first
    metric that evaluates both the efficiency and efficacy of a navigation agent,
    and it is regarded as the primary metric in VLN. The SPL ignores the turning actions
    and the agent heading. *Success weighted by edit distance* (SED) [[62](#bib.bib62)]
    takes turning actions into consideration and fix this problem. SED is designed
    for instruction compliance in a graph-based environment, where there exists a
    certain correct path. However, in some tasks like R4R [[46](#bib.bib46)] and R6R [[63](#bib.bib63)],
    the instructed paths are not direct-to-goal. Therefore, it is not appropriate
    to evaluate the navigation performance of the SPL. Therefore, *Coverage weighted
    by Length Score* (CLS) [[46](#bib.bib46)] is proposed to measure the fidelity
    of the agent’s behavior to the described path. CLS is the product of two variables:
    path coverage and length fraction. Ilharco *et al.* absorb the idea of Dynamic
    Time Warpping [[64](#bib.bib64)], an approach widely used in various areas [[65](#bib.bib65),
    [66](#bib.bib66), [67](#bib.bib67)], and propose normalized Dynamic Time Warping
    (nDTW) metric [[68](#bib.bib68)] to evaluate the navigation performance. Similar
    to CLS, nDTW evaluates the distance between the predicted path with the ground-truth
    path. Moreover, nDTW is sensitive to the order of the navigation path while CLS
    is order-invariant. nDTW can be implemented in an efficient dynamic programming
    algorithm. The path-sensitive metrics, like CLS and nDTW, perform better when
    they are used as reward functions than target-oriented reward functions in reinforcement
    learning to navigate [[46](#bib.bib46), [68](#bib.bib68)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Measurements of Metrics Each metric has its unique characteristics according
    to their formulation. We compare the formulation and characteristics of existing
    metrics in Tab. [III](#S3.T3 "TABLE III ‣ 3.2 Cross-modal Navigation Tasks ‣ 3
    Embodied Navigation Benchmarks ‣ Deep Learning for Embodied Visual Navigation
    Research: A Survey"). In this part, we introduce measurements to evaluate the
    functions of a metric:'
  prefs: []
  type: TYPE_NORMAL
- en: 1) Path Similarity (PS) characterizes a notion of similarity between the $P$
    and the $R$. This implies that metrics should depend on all nodes in $P$ and all
    nodes in $R$. PS penalizes deviations from the ground truth path, even if they
    lead to the same goal. This is not only prudent, as agents might wander around
    undesired terrain if this is not enforced, but also explicitly gauges the fidelity
    of the predictions with respect to the provided language instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 2) Soft Penalties (SP) penalizes differences from the ground truth path according
    to a soft notion of dissimilarity that depends on distances in the graph. This
    ensures that larger discrepancies are penalized more severely than smaller ones
    and that SP should not rely only on dichotomous views of intersection.
  prefs: []
  type: TYPE_NORMAL
- en: '3) Unique Optimum (UO) yields a perfect score if and only if the reference
    and predicted paths are an exact match. This ensures that the perfect score is
    unambiguous: the reference path $R$ is therefore treated as a golden standard.
    No other path should have the same or higher score as the reference path itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 4) Scale Invariance (SI) measures if a metric is independent over different
    datasets. If a metric variants over datasets, such as navigation error, its scores
    across different datasets cannot be directly compared.
  prefs: []
  type: TYPE_NORMAL
- en: 5) Order Sensitive (OS) indicates if a metric is sensitive to the navigation
    order with the same trajectory length, success rate, etc. The navigation order
    reveals some sorts of navigation policy even though it is usually hard to be evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: 6) Computational Complexity (CC) measures the cost of computing a pair of $(P,R)$.
    It is important to design a fast algorithm to calculate the score for automatic
    validation and testing.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Embodied navigation benchmarks define the tasks and metrics for different settings.
    The target-oriented tasks like *PointGoal*, *ObjectGoal*, and *RoomGoal* Navigation
    can provide label by the 3D assets and do not require extra human annotation.
    cross-modal navigation tasks like R2R [[12](#bib.bib12)], Visual Dialogue Navigation [[49](#bib.bib49)]
    or REVERIE [[55](#bib.bib55)] require human to label the trajectory and the corresponding
    language description. The interactive interactive tasks [[59](#bib.bib59), [41](#bib.bib41)]
    require the agent to learn to manipulate objects, which attract rising attention
    due to their wide application in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Methods in Simulated Environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we mainly discuss two problems in the simulated environments:
    target-driven navigation and cross-modal navigation. And we introduce the methods
    to solve these problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Target-driven Navigation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Methods for this problem focus on navigating from a random starting position
    to a target. The target may be specified by an RGB image, a vector, or a word.
    The agent predict actions like *turn left*, *turn right*, *move forward* to navigate
    in the embodied environment and predict *stop* indicate the stop action. There
    are diverse methods that try to solve this problem, including: 1) model-free methods;
    2) planning-based methods; and 3) self-supervised methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Model-free Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The model-free methods learn to navigate end-to-end without modeling the environment,
    as illustrated in the Fig. [4](#S4.F4 "Figure 4 ‣ 4.1.1 Model-free Methods ‣ 4.1
    Target-driven Navigation ‣ 4 Methods in Simulated Environments ‣ Deep Learning
    for Embodied Visual Navigation Research: A Survey"). The learning objective includes
    imitation learning or reinforcement learning. The formulation of the learning
    object is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\sum_{t}-a_{t}^{*}log\left(p_{t}\right)-\sum_{t}a_{t}log\left(p_{t}\right)A_{t},$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $a^{*}$ is the ground truth action, $p_{t}$ is the action probability,
    and $A_{t}$ is the advantage in A3C [[69](#bib.bib69)]. Though extensive reinforcement
    learning works [[70](#bib.bib70), [71](#bib.bib71), [61](#bib.bib61)] have long
    studied 2D navigation problem where an agent receives global state for each step,
    the embodied navigation problem with partial observation remains challenging.
    Many robot control works [[72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74),
    [75](#bib.bib75)] focus on obstacle avoidance rather than trajectory planning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7f0b7cc4c9ce53ba3da33ee33e8e7135.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: An illustration of an model-free visual navigation model. This model
    learned from imitation learning and reinforcement learning. $r_{t}$ is the reward
    and $f(s_{t})$ stands for the labels calculated from the state $s_{t}$. And $a^{\prime}$
    is the label stands for the optimal action.'
  prefs: []
  type: TYPE_NORMAL
- en: Zhu *et al.* [[15](#bib.bib15)] firstly propose to use deep learning for feature
    matching and deep reinforcement learning for policy prediction, which allows the
    agent to better generalize to unseen environments. Afterwards, Successor Representation
    (SR) [[76](#bib.bib76)] is proposed to enable the agent to interact with objects.
    This framework takes the states of objects and a discrete description of the scene
    into consideration. Successor Representation encodes semantic information and
    concatenate it with the visual representation as in [[15](#bib.bib15)]. Different
    from [[15](#bib.bib15)] that only uses reinforcement learning to learn a policy
    predictor, Successor Representation model that bootstraps reinforcement learning
    with imitation learning. Previous models lack of the ability of encoding temporal
    information. By introducing an LSTM layer to encode historical information, Wu
    *et al.* [[10](#bib.bib10)] are able to build an agent that is able to generalize
    to unseen scenarios. In the ablation study, this work proves that A3C [[69](#bib.bib69)]
    outperforms DDPG [[77](#bib.bib77)] in visual navigation task, and the model learned
    from semantic mask outperforms which learned from RGB inputs. Inspite of solving
    visual navigation problem via on-policy deep reinforcement learning algorithms,
    some works adopt other algorithms. Li *et al.* [[78](#bib.bib78)] propose an end-to-end
    model based on Q-learning that learns viewpoint invariant and target on invariant
    visual servoing for local mobile robot navigation.
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of works use segmentation masks of objects to augment visual
    inputs. Mousavian *et al.* [[16](#bib.bib16)] exploit the instance features in
    the vision inputs by introducing Faster-RCNN detector trained on MSCOCO dataset [[79](#bib.bib79)]
    and a segmenter defined by [[80](#bib.bib80)] to detect and segment objects. Shen
    *et al.* [[81](#bib.bib81)] improve zero-shot generalization of a navigation agent
    by fusing diverse visual representations, including RGB features, depth features,
    segmentation features, detection features, etc. The different visual representations
    are adaptively weighted for fusing. To further improve the robustness, they propose
    a inter-task affinity regularization that encourages the agent to select more
    complementary and less redundant representations to fuse. Despite the well-performed
    detector and segmenter, learning a robust navigation policy is still challenging.
    For example, to search for mugs, a human would search cabinets near the coffee
    machine and for fruits a human may try the fridge first. To address this, Lv *et
    al.* [[82](#bib.bib82)] integrate 3D knowledge graph and sub-targets into deep
    reinforcement learning framework. To enhance the cross-scene generalization, Wu
    *et al.* [[83](#bib.bib83)] introduce an information theoretic regularization
    term into the RL objective and models the action-observation dynamics by learning
    a variational generative model.
  prefs: []
  type: TYPE_NORMAL
- en: Some works investigate problem settings other than indoor navigation, such as
    street view navigation or combining other modalities. Khosla *et al.* [[84](#bib.bib84)]
    firstly attempt to solve outdoor street navigation task by embodied visual navigation
    method ,where the agent navigate purely based on panoramic street views. DeepNav [[85](#bib.bib85)]
    is build upon a Convolutional Neural Network (CNN) for navigating in large cities
    using locally visible street-view images. These works rely on supervised training
    with the ground truth compass input , however, the compass can sometimes be unavailable
    in real-world. Another work [[86](#bib.bib86)] propose an end-to-end deep reinforcement
    learning framework that uses the street scenes from Google Street View as visual
    input but without the ground truth compass. Recognizing the importance of locale-specific
    knowledge to navigation, they propose a dual pathway architecture that allows
    locale-specific features to be encapsulated. AV-WaN [[87](#bib.bib87)] is proposed
    to tackle the challenges in Audio-visual Navigation. This model learns the audio-visual
    waypoints and dynamically sets intermediate goal locations based on its audio-visual
    observations and partial maps in an end-to-end manner.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/368ed74b49049d5504093b393c0992f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: An illustration of an end-to-end visual navigation model with self-supervised
    objectives. $r_{t}$ is the reward and $f(s_{t})$ stands for the labels calculated
    from the state $s_{t}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Self-Supervised Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Self-supervised learning is a long studied topic of exploiting extra training
    signals via various pretext tasks. It enables an agent to learn more knowledge
    without additional human annotations. Various self-supervised tasks have been
    proposed in the field of deep learning, such as context prediction [[88](#bib.bib88)],
    solving jigsaw puzzles [[89](#bib.bib89)], colorization [[90](#bib.bib90)], rotation [[91](#bib.bib91)].
    There is also some auxiliary tasks proposed to improve data efficiency and generalization
    in reinforcement learning. Xie *et al.* [[17](#bib.bib17)] combines self-supervised
    learning with model-based reinforcement learning to solve robotic tasks. Motivated
    by traditional UVFA architecture [[92](#bib.bib92)] which learns a value function
    by means of feature learning, Jaderberg *et al.* [[18](#bib.bib18)] invent auxiliary
    control and reward prediction tasks that dramatically improve both data efficiency
    and robustness.
  prefs: []
  type: TYPE_NORMAL
- en: 'In embodied navigation, the environment contains unstructured semantic information
    that is hard to learn in end-to-end manner. In spite of explicitly modeling the
    environment using SLAM or memory mechanism, self-supervised learning provide another
    feasible way of learning the unstructured knowledge. Mirowski *et al.* [[19](#bib.bib19)]
    propose an online navigation model with two self-supervised auxiliary objectives,
    predicting the current depth view by RGB view and detecting the loop closure.
    Similiar idea [[93](#bib.bib93)] has been applied in game applications [[94](#bib.bib94)]
    for rapid exploration. Auxiliary tasks also can speed up learning. Ye *et al.* [[95](#bib.bib95),
    [96](#bib.bib96)] achieve great success in *PointGoal* and *ObjectGoal* navigation
    by assembling reinforcement learning with various kinds of auxiliary tasks, formulated
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{total}=L_{RL}+\sum_{i}^{n}\beta_{i}L_{Aux,i}.$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'Visual perception is critical for visual navigation. But the training signal
    provided by reinforcement learning contain too much noise to train a robust feature
    perception network. An encoder-decoder architecture is proved to be beneficial [[97](#bib.bib97)]
    in visual encoding and segmentation predicting. In addition, an auxiliary task
    is used to penalize the segmentation error, which benefits the learning of feature
    perception. However, this self-supervised auxiliary task only learns the low-level
    dynamic function between two adjacent states and fails to learn the high-level
    semantic information. To guarantee the semantic consistency of actions in a trajectory,
    Liu *et al.* [[98](#bib.bib98)] propose an auxiliary regularization task to penalizes
    the inconsistency of representations. This regularization task encourages the
    policy network to extract salient features from each sensor. Real-world robot
    locomotion is far from deterministic due to the presence of actuation noise, which
    might be caused by wheels slipping, motion sensor error, rebound, etc. To reduce
    the noise, Datta *et al.* [[99](#bib.bib99)] introduce an auxiliary task of localization
    estimation by means of temporal difference. The auxiliary task is used to train
    a CNN network and use the estimated locomotion as an input of the policy network.
    A curiosity-driven self-supervised objective [[100](#bib.bib100)] is applied to
    encourage exploration while penalizing the repeating actions. A stable curiosity-driven
    policy without repeating actions could improve the exploration efficiency. Self-supervised
    auxiliary tasks are also helpful in cross-modal understanding for navigation.
    Dean *et al.* [[101](#bib.bib101)] use audio as an additional modality for self-supervised
    exploration. It includes an curiosity driven intrinsic reward, which encourages
    the agent to explore novel associations between different sensory modalities (audio
    and visual). An overview of the pipeline of self-supervised navigation methods
    is shown in Fig. [5](#S4.F5 "Figure 5 ‣ 4.1.1 Model-free Methods ‣ 4.1 Target-driven
    Navigation ‣ 4 Methods in Simulated Environments ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey"). The agent firstly embeds a visual image
    and an instruction as features. Then the visual feature and the instruction feature
    are fused to predict the action. The auxiliary tasks use the fused feature to
    make a prediction, such as predicting the reward, or reconstructing the input
    visual image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/99fa91ad020b79cc5bec012f2e97493b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: An overview of the common practice of the “Neural SLAM”-based model.
    “ST” is the spatial transformation function.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Planning-based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The map building problem for an unknown environment while solving the localization
    problem at the same time is known as Simultaneous Localization and Mapping (SLAM) [[102](#bib.bib102),
    [103](#bib.bib103)]. The earlier investigations on visual navigation were carried
    out with a stereo camera [[104](#bib.bib104), [105](#bib.bib105)] and a monocular
    camera, such as MonoSLAM [[106](#bib.bib106)]. Over the past decade, traditional
    geometric-based approaches [[107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109)]
    remains dominating the field. With the development of deep learning, some methods
    like CNN-SLAM [[110](#bib.bib110)], DVO [[111](#bib.bib111)] and D3VO [[112](#bib.bib112)]
    are proposed. Some indoor tasks are proposed to study SLAM, such as KITTI [[113](#bib.bib113)]
    and EuRoC [[114](#bib.bib114)]. However, these tasks are different from embodied
    navigation task. The odometry benchmark is to estimate the location given a sequence
    of visual inputs while the navigation task is to align the instruction with the
    environment semantics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, researchers find that the ability of localization is important to
    navigation, especially for long-term path planning. Thus, some works introduce
    SLAM methods to model the house and improve the localization ability of the agent.
    Neural Map [[115](#bib.bib115)] generalize this idea for all deep reinforcement
    learning agents rather than navigation only. However, this work assumes the location
    of the agent is always known and does not utilize the 2D structure of this memory.
    Neural SLAM [[116](#bib.bib116)] fixes this problem by embedding SLAM-like procedures
    into the soft-attention [[117](#bib.bib117)]. To avoid spatial blurring associated
    with repeated warping, MapNet [[118](#bib.bib118)] proposes to use a world-centric
    rather than an egocentric map. Different from previous works, MapNet maintains
    a 2.5D representation by a deep neural network module that learns to distill visual
    representations from 3D embodied visual inputs. Gordon *et al.* [[58](#bib.bib58)]
    proposes Hierarchical Interactive Memory Network (HIMN), a framework with hierarchical
    controller for IQA task. The high-level controller is a planner that decides the
    long-term navigation target, and the low-level controller predicts the action,
    interacts with the environment, and answers the question. Gupta *et al.* [[20](#bib.bib20)]
    introduce the Neural-SLAM method in embodied navigation. This work consists of
    two parts: mapping and planning. The mapping mechanism maintains a 2D memory map.
    For each step, it transforms the embodied scene into a 2D feature and update the
    map with the feature. The planning mechanism uses a value function to output a
    policy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Efficient exploration is widely regarded as one of the main challenges in reinforcement
    learning (RL) [[119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121)]. Similarly,
    it is important in navigation since the target does not always visible from the
    starting position and the agent is required to explore the unseen scene and search
    for the target. Recently, exploration based on explicitly modeled semantic memory
    is proven to be efficient. To learn a policy with spatial memory, Chen *et al.* [[122](#bib.bib122)]
    bootstrap the model with imitation learning and finetune it with coverage rewards
    derived purely from on-board sensors. Active Neural SLAM (ANS) [[22](#bib.bib22)]
    is a successful neural SLAM method which achieves the state-of-the-art on the
    CVPR 2019 Habitat *Pointgoal* Navigation Challenge. ANS proposes a hierarchical
    structure for planning. Inspired by the idea of hierarchical RL [[123](#bib.bib123),
    [124](#bib.bib124)], ANS learns the high-level planner by reinforcement learning
    and learns the low-level planner by imitation learning. The mapper is implemented
    by an auxiliary task of predicting a 2D map. The first channel of the map stands
    for if there is an obstacle and the second map stands for whether the position
    has been explored or not. However, the predefined 2D map cannot help long-term
    navigation since the semantic information of the scenes in different viewpoints
    is not encoded in the map. Neural Topological SLAM [[125](#bib.bib125)] propose
    a more advanced way which stores the observed feature representations. This method
    introduce a graph update module to leverage semantics. The graph update module
    maintains a topological feature memory. For each step, the module localize current
    observation into memory nodes. If an observation is not localized in any node
    of the memory, the graph update module will add a new node into the topological
    feature memory. Goal-Oriented Semantic Exploration (SemExp) [[126](#bib.bib126)]
    tackles the object goal navigation task in realistic environments. This method
    first builds a episodic semantic map and uses it to explore the environment based
    on the category of the target object. This approach achieves state-of-the-art
    in Habitat ObjectNav Challenge 2020. An overview of the common practice of the
    ‘Neural SLAM’-based model is shown in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1.2 Self-Supervised
    Methods ‣ 4.1 Target-driven Navigation ‣ 4 Methods in Simulated Environments ‣
    Deep Learning for Embodied Visual Navigation Research: A Survey"). In addition
    to the visual encoder and the instruction encoder as in Fig. [5](#S4.F5 "Figure
    5 ‣ 4.1.1 Model-free Methods ‣ 4.1 Target-driven Navigation ‣ 4 Methods in Simulated
    Environments ‣ Deep Learning for Embodied Visual Navigation Research: A Survey"),
    ‘Neural SLAM’-based model have a unique module to project a embodied visual view
    to feature representation and store it in a 2D top-down map:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $m_{t},\hat{x_{t}}=f_{SLAM}(s_{t},x_{t-1:t}^{\prime},m_{t-1}&#124;\theta_{S})$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $x_{t-1:t}^{\prime}$ stands for previous poses, $m_{t-1}$ is previous
    maps, and $\theta_{S}$ stands for parameters. This map models the room structure
    and visual representation of scenes. The projected feature representations are
    fused with the visual feature and the instruction feature to jointly predict an
    action.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compared with traditional robotics methods, the model-free methods are able
    to obtain robust navigation models by sampling large scale data with the embodied
    simulator. Some works adopt detection and segmentation approaches to get better
    visual views. In spite of indoor scenarios, model-free methods achieve great success
    in street scene and multi-modal environments. Self-supervised methods are proposed
    to exploit the extra knowledge by auxiliary tasks to improve the learning efficiency
    and generalization ability. Planning-based methods utilize a 2D map or a topological
    memory to model the environment during navigation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Cross-modal Navigation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A navigation robot who understands natural language can accomplish more complex
    tasks, such as “*pick up the cup in the kitchen*” or “*help me find my glass upstairs*”.
    In this section, we introduce three kinds of works that solves cross-modal navigation
    tasks: 1) step-by-step methods; 2) pretraining-based methods; 3) planning based
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Sequence-to-sequence Navigation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Anderson *et al.* [[12](#bib.bib12)] firstly propose a sequence-to-sequence
    model similar to [[127](#bib.bib127)] to address the vision language navigation
    problem. This model sequentially encodes a language instruction word-by-word,
    concatenates the sentence feature with the visual image feature and decodes the
    action sequence. However, a sequence-to-sequence model is lack of stability and
    generalization since it fails to consider the dynamics in the real-world environments.
    RPA [[23](#bib.bib23)] is proposed to tackle the generalization issue by equipping
    a ‘look-ahead’ module, which learns to predict the future state and reward. To
    improve the generalization ability in instruction-trajectory alignment, Fried
    *et al.* [[24](#bib.bib24)] propose a data augmentation approach named “speaker-follower”
    to improve the model generalization. To generate augmentation data, the speaker
    firstly translates a randomly trajectory into an instruction, and the follower
    secondly translates an instruction into a trajectory:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{r\in R(d)}{\mathrm{argmax}}P_{S}(d&#124;r)\cdot P_{F}(r&#124;d)^{(1-\gamma)},$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $P_{S}$ is the speaker, $P_{F}$ is the follower, $d$ stands for an instruction,
    $r$ stands for a trajectory, and $\gamma$ is a weighting factor. Another contribution
    of this paper [[24](#bib.bib24)] is the definition of a high-level action that
    move forward toward an orientation in a panoramic space in stead of low-level
    actions like *turn left*, *turn right* and *go forward*. Compared with the definition
    of low-level actions, this approach largely reduce the length of the action sequence
    that describes the same trajectory. Navigating by the high-level action space
    requires less prediction times, which makes the model easier to train and more
    robust to test. Howeverr, previous methods learn to navigate by imitation learning
    only with the instruction-trajectory data pairs, which supervises the shortest
    path while ignore the sub-optimal trajectories so that leds to overfitting. To
    tackle this problem, Wang *et al.* [[128](#bib.bib128)] propose to jointly learn
    a navigation agent by imitation learning and reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/97045fbee2699dcf062da9dcda8c12b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A comparison of seq-to-seq models in VLN and VQA.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, this method introduce an LSTM to encode the temporal information
    of visual features and introduce a cross-modal mechanism to achieve better vision-language
    navigation ability. Ma *et al.* [[129](#bib.bib129)] propose a self-monitoring
    agent with a visual-textual co-grounding module and progress monitor. The progress
    monitor use the cross-modal feature from the co-grounding module and estimate
    the completed progress. Since the instruction in vision-language task guides the
    agent to the target step-by-step, the progress information contain rich knowledge
    that help improve the perception of the agent. Ma *et al.* propose in [[130](#bib.bib130)]
    the *Regretful Agent*, with a regretful module which uses the estimated progress
    to indicate if the agent navigates to a wrong place and need to go back. Similar
    to the *Regretful Agent*, Ke *et al.* propose  [[131](#bib.bib131)] a framework
    for using asynchronous search to boost a VLN navigator by enabling explicit backtrack.
    Anderson *et al.* [[132](#bib.bib132)] regard the step-by-step navigation process
    as a visual tracking task. This approach implements the navigation agent within
    the framework of Bayesian state tracking [[3](#bib.bib3)] and formulates an end-to-end
    differentiable histogram filter [[133](#bib.bib133)] with learnable observation
    and motion models. One commonly used method that relieve the visual overfitting
    is to apply an dropout [[134](#bib.bib134)] layer on the visual feature, which
    is extracted by a pretrained network like VGG [[135](#bib.bib135)] or ResNet [[136](#bib.bib136)].
    Tan *et al.* [[137](#bib.bib137)] argue that simply applying a dropout layer on
    the visual feature leads to inconsistency, e.g. a chair in this frame could be
    dropped in the next frame. To solve the problem, They propose a environmental
    dropout layer that randomly dropout some fixed channels during a trajectory. Zhu
    *et al.* [[138](#bib.bib138)] propose AuxRN, a framework that introduce self-supervised
    auxiliary tasks to exploit environmental knowledge from several aspects. In addition
    to introducing the temporal difference auxiliary task that is widely use in other
    embodied visual navigation methods [[96](#bib.bib96), [19](#bib.bib19)], AuxRN
    introduces a trajectory retelling task and instruction-trajectory matching task
    that learn the temporal semantics of a trajectory. Instead of generating the low-quality
    augmented data, Fu *et al.* [[139](#bib.bib139)] introduce the concept of counterfactual
    thinking to sample challenging paths to augment training dataset. They present
    a model-agnostic adversarial path sampler (APS) to pick the difficult trajectories
    and only consider useful counterfactual conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different from the earlier works that based on data augmentation and other
    classical navigation methods, some works discover the importance of natural language
    to VLN. Thomason *et al.* [[140](#bib.bib140)] find the unimodal baseline outperforms
    random baselines and even some of their multimodal counterparts. Thus the work
    advocates that ablating unimodal to evaluate the bias is important to proposing
    a dataset. A study of Huang *et al.* [[141](#bib.bib141)] shows that only a limited
    number of those augmented paths in  [[24](#bib.bib24)] are useful and after using
    60% of the augmented data, the improvement diminishes with additional augmented
    data. To avoid the extensive work in reward engineering, Wang *et al.* [[142](#bib.bib142)]
    propose a Soft Expert Reward Learning model that includes two parts: 1) soft expert
    distillation, which encourages agents to behave like an expert in soft fashion;
    2) self perceiving, which pushes the agent towards the final destination as fast
    as possible. Xia *et al.* [[143](#bib.bib143)] leverages multiple instructions
    as different descriptions for the same trajectory to resolve language ambiguity
    and improve generalization ability. This work indicates that the human annotations
    in VLN are largely biased according to the specific scene and the trajectory.
    The quality of visual features is critical for improving the performance of embodied
    navigation. Previous works extract global visual features from panoramic views
    by a pretrained CNN network like ResNet-101 [[136](#bib.bib136)]. Hong *et al.* [[144](#bib.bib144)]
    introduce Faster-RCNN to detect objects in navigation and build a relationship
    graph between visual and language entities for vision-language alignment. In spite
    of the visual inputs, the structure information also helps navigation. Hu *et
    al.* [[145](#bib.bib145)] discover that the language instructions contain high-level
    semantic information while visual representations are a lower-level modality,
    which makes the vision-language alignment difficult. Motivated by this, they decomposes
    the grounding procedure into a set of expert models with access to different modalities
    and ensemble them at prediction time. To better research what role does language
    understanding play in VLN task, Hong *et al.* [[48](#bib.bib48)] argue that the
    intermediate supervision is important in vision-language alignment. Thus, they
    propose FGR2R, a method which enables navigation processes to be traceable and
    encourage the agent to move at the level of sub-instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/945bb9e482d759ee7f7208544728b7c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: An example of pretraining-based framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Pretraining-based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Several challenges are discovered during the research on the vision-language
    navigation: 1) low training efficiency; 2) large data bias (include both vision
    and language); 3) lack of generalization from seen to unseen scenes. To address
    these challenges, pretraining-based models are proposed to learn from large-scale
    data sets from other sources and fast adapt to unseen scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: Low training efficiency The traditional encoder-decoder framework first samples
    the total trajectory by teacher-forcing or student-forcing and then back-propagate
    the gradients. In other deep learning tasks like image classification [[1](#bib.bib1)]
    or text recognition [[146](#bib.bib146)], the model predicts a result directly.
    However, in the vision-language navigation task, the agent predicts a trajectory
    by interacting with the environment in a step-by-step manner, which is so time-consuming
    that reduce the training efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d30ffc1f1185e4bcaf22cd1dd0a2b1d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: A comparison of three embodied vision-language navigation tasks:
    EQA-v1 [[54](#bib.bib54)], MT-EQA [[57](#bib.bib57)] and VDN [[49](#bib.bib49)].'
  prefs: []
  type: TYPE_NORMAL
- en: Large data bias The vision-language navigation scenarios are so diverse that
    61 houses in R2R cannot cover all of them. From the aspect of natural language,
    in the R2R task, only 69% of bigrams are shared between training and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of generalization Lacking of diverse training data still largely limits
    the generalization in spite of the proposed augmentation methods like trajectory
    augmentation, visual feature augmentation and natural language augmentation. Thus,
    introducing extra knowledge from other tasks and datasets becomes a promising
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pretraining-based methods largely improve the generalization ability of a model
    by learning in large scale of data [[136](#bib.bib136), [51](#bib.bib51)]. Furthermore,
    bert-based methods [[147](#bib.bib147), [148](#bib.bib148)] pretrain a transformer
    network with proxy tasks and achieve great success in vision, language and cross-modal
    tasks. Many researchers consider to solve the vision-language navigation problem
    by pretaining-based methods. Li *et al.* [[149](#bib.bib149)] propose PreSS first
    introduce a pretrained language models to learn instruction representations. And
    they propose a stochastic sampling scheme to reduce the gap between the expert
    actions in training and the sampled actions in testing. Majumdar *et al.* [[150](#bib.bib150)]
    advocate to improve model by leveraging large-scale of web data. However, it is
    hard to transfer the static image data to VLN task. Therefore, they propose VLN-bert,
    a transformer-based model which is pretrained by static images and their captions.
    Prevalent [[151](#bib.bib151)] is self-supervisedly pretrained on large amount
    of image-text-action triplets sampled from an embodied environment with two pretrianing
    objectives, masked language modeling (MLM) and action prediction (AP):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\begin{split}&amp;L_{MLM}=-\mathbb{E}_{s\sim p(\tau),(\tau,x)\sim
    D_{E}}\mathrm{log}\ p(x_{i}&#124;\textbf{x},s),\\ &amp;L_{AP}=-\mathbb{E}_{(a,s)\sim
    p(\tau),(\tau,x)\sim D_{E}}\mathrm{log}\ p(\textbf{x}&#124;x_{[CLS]},s),\end{split}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $(s,a)$ a state-action pair. Prevalent is proven to be effective on several
    vision-language navigation datasets, including R2R, CVDN and HANNA. The embodied
    navigation agent receives partial observation rather than global observation,
    which is better to be modeled as a partially observable Markov Decision Process.
    Different from the encoder-decoder model, previous pretraining-based models do
    not memorize previously seen scenes during navigation and utilize temporal knowledge,
    which causes information loss in action prediction. Motivated by this, Hong *et
    al.* [[152](#bib.bib152)] propose a recurrent multi-layer transformer network
    that is time-aware for use in VLN. This method introduce a Transformer which maintains
    a feature vector to represent temporal context.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Navigation with Questioning and Answering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of passively perceive natural language instructions from a human commander,
    Das *et al.* [[57](#bib.bib57)] suggest that an intelligent agent should be able
    to answer a question via navigation. Thus Das *et al.* present a new task named
    EQA (Embodied Questioning and Answering), where an agent is spawned at a random
    location in a 3D environment and asked to answer a question. In order to answer,
    the agent have to first navigate to explore the environment, gather information
    through egocentric vision, and then answer the question. To solve this challenging
    task, Das *et al.* present PACMAN, a CNN-RNN model with Adaptive Computation Time
    (ACT) module [[153](#bib.bib153)] to decide how many times to repeatly execute
    an action [[57](#bib.bib57)]. The PACMAN is bootstrapped by shortest path demonstrations
    and then fine-tuned with RL. However, this method is lack of the ability of high-level
    representation. In a later work [[154](#bib.bib154)], Das *et al.* propose a hierarchical
    policy named Neural Modular Controller (NMC) that operates at multiple timescales,
    where the higher-level master policy proposes sub-goals to be executed by low-level
    sub-policies. Anand *et al.* [[155](#bib.bib155)] find that a blindfold (question-only)
    baseline on EQA and find that the baseline perform previous state-of-the-art models.
    They suggest that previous EQA models are ineffective at leveraging the context
    from the environment and the EQAv1 dataset has lots of noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Wu *et al.* [[156](#bib.bib156)] propose a simple supervised learning baseline
    which is competitive to the state-of-the-art EQA methods. To improve EQA performance
    in unseen environment, in this paper, they propose a setting in which allows the
    agent to answer questions for adaptation. Yu *et al.* [[57](#bib.bib57)] argues
    that the EQA task assumes that each question has exactly one target, which limits
    its application. Therefore, Yu *et al.* present Multi-Target EQA (MT-EQA), a generalized
    version of EQA. The question of this task contains multiple targets. And it require
    the agent to perform comparative reasoning over multiple targets rather than simply
    perceive the attributes of one target. Wijmans *et al.* [[157](#bib.bib157)] extend
    the EQA problem to photorealstic environment. In this environment, they discover
    that point cloud representations are more effective for navigation. Luo *et al.* [[158](#bib.bib158)]
    suggest that the visual perception ability limits the performance of the EQA.
    They introduce Flownet2 [[159](#bib.bib159)], a high-speed video segmentation
    framework as a backbone to assist navigation and question answering. Li *et al.* [[160](#bib.bib160)]
    propose a MIND module that model the environment imagery and generate mental images
    that are treated as short-term sub-goals. Tan *et al.* [[161](#bib.bib161)] investigate
    the questioning and answering problems between multiple targets. In this task,
    the agent has to navigate to multiple places, find all targets, analysis the relationships
    between them, and answer the question. Motivated by recent progress in Visual
    Question Answering (VQA) [[50](#bib.bib50)] and Video Question Answering (VideoQA) [[162](#bib.bib162)],
    Cangea *et al.* [[163](#bib.bib163)] propose VideoNavQA, a dataset that contains
    pairs of questions and videos generated in the House3D environment. This dataset
    fills the gap between the VQA and the EQA. The VideoNavQA task represents an alternative
    view of the EQA paradigm: By providing nearly-optimal trajectories to the agent,
    the navigation problem is easier to solve compared with the reasoning problem.
    Deng *et al.* [[164](#bib.bib164)] propose Manipulation Question Answering (MQA)
    where the robot is required to find the answer to the question by actively exploring
    the environment via manipulation. To suggest a promising direction of solving
    MQA, they provide a framework which consists of a QA module (VQA framework) and
    a manipulation model (Q learning framework). Nilsson *et al.* [[165](#bib.bib165)]
    build an agent which explores in a 3D environment and occasionally requests annotation
    during navigation. Similarly, Roman *et al.* [[166](#bib.bib166)] suggest a two-agent
    paradigm for cooperative vision-and-dialogue navigation. Their model learns multiple-skills,
    including navigation, question asking, and questioning-answering components.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 Navigation with Dialogue
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There is a long history that human use a dialog to guide a robot [[167](#bib.bib167),
    [168](#bib.bib168)]. In the field of embodied navigation, Banerjee *et al.* [[59](#bib.bib59)]
    propose “Help, Anna!” (HANNA), an interactive photo-realistic simulator in which
    an agent fulfills object-finding tasks by requesting and interpreting natural
    language and vision assistance. Nguyen *et al.* [[169](#bib.bib169)] propose a
    task named VLNA, where an agent is guided via language to find objects. However,
    the language instruction in these two tasks far from real-world problem: the responses
    of HANNA are automatic generated from a trained model while the guidance of VLNA
    are in the form of templated language that encodes gold-standard planner action.
    Vries *et al.* [[170](#bib.bib170)] propose “Talk The Walk” (TtW), where two humans
    communicate to reach a goal location in an outdoor environment. However, in TtW,
    the human uses an abstracted semantic map rather than an egocentric view of the
    environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thomason *et al.* [[49](#bib.bib49)] propose vision-and-dialog navigation (VDN),
    a scaffold for navigation-centered question asking and question answering tasks
    where an agent navigates following a multi-round dialog history rather than an
    instruction. Compared with the single-round instructions in R2R dataset, VDN provides
    multi-round annotation, in which each round of dialogue describes a sub-trajectory.
    The more fine-grained dialogue annotation facilitate researchers to study the
    problem of navigation with natural language. Zhu *et al.* [[138](#bib.bib138)]
    propose a framework with a cross-modal memory mechanism to capture the hierarchical
    correlation between the dialogue rounds and the sub-trajectories. More generally,
    several methods, such as Prevalent [[151](#bib.bib151)] and BabyWalk [[63](#bib.bib63)],
    validate their navigation ability using both sentence instructions and dialog
    instructions. Unfortunately, these works heavily rely on dialogue annotations
    which is labor-intensive. To alleviate this, Roman *et al.* [[166](#bib.bib166)]
    exploit to generate dialogue questions answers based on visual views. This work
    addresses four challenges in modeling turn-based dialogues, which includes: 1)
    deciding when to ask a question; 2) generating navigator questions; 3) generating
    question-answer pairs for guidance; 4) generating navigator actions. To achieve
    this, Roman *et al.* [[166](#bib.bib166)] introduce a two-agent paradigm, where
    one agent navigates and asks questions while the other guides agent answers. Different
    from previous works that guide navigator with template language, this work initialize
    the oracle model via pretraining on CVDN dialogues to generate natural language.'
  prefs: []
  type: TYPE_NORMAL
- en: A dialog does not always describe a step-by-step navigation process. Rather,
    the oracle describes the target scene and let the navigator to find it, which
    commonly occurs when someone get lost in a new building. Hahn *et al.* [[171](#bib.bib171)]
    propose a LED task (localizing the observer from dialog history) to realize when
    it get lost. Motivated by this, they present a dataset named Where Are You [[171](#bib.bib171)]
    that consists of 6k dialogues of two humans. Due to the wide application of multi-agent
    communication systems [[172](#bib.bib172), [173](#bib.bib173)] in real-world,
    researchers become interested in implementing dialog navigating in physical environments.
    Marge *et al.* [[174](#bib.bib174)] present MRDwH, a platform that implements
    autonomous dialogue management and navigation of two simulated robots in a large
    outdoor simulated environment. Banerjee *et al.* [[175](#bib.bib175)] propose
    RobotSlang benchmark, a dataset which is gathered by pairing a human “driver”
    controlling a physical robot and asking questions of a human “commander”
  prefs: []
  type: TYPE_NORMAL
- en: 'We compare the difference of Embodied Question Answering (EQA) [[54](#bib.bib54)],
    Multi-Target Embodied Question Answering (MT-EQA) [[57](#bib.bib57)] and Vision-and-dialog
    navigation (VDN) [[49](#bib.bib49)] in Fig. [9](#S4.F9 "Figure 9 ‣ 4.2.2 Pretraining-based
    Methods ‣ 4.2 Cross-modal Navigation ‣ 4 Methods in Simulated Environments ‣ Deep
    Learning for Embodied Visual Navigation Research: A Survey"). We demonstrate three
    different dialogues for the same navigation trajectory as an example. Compared
    with EQA, the question in MT-EQA are more complex since it should describe multiple
    targets. The agent have to acquire high-level skills, such as reasoning, comparison
    and multi-object localization, to accomplish MT-EQA. In the EQA and MT-EQA tasks,
    the agent is required to answer question from a human via navigation. However,
    in the VDN task, the agent is the navigator and the questioner which asks a human
    for hints to find the target. The difference of the task setting led to the different
    designs of the navigation model.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.5 Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Natural language provides an interface for a human to interact with a robot.
    A robot with cross-modal understanding is able to accomplish complex tasks such
    as navigating following a natural language instruction or a dialogue, asking the
    oracle for more details, etc. Lots of works have been proposed to research on
    vision-language navigation problem from diverse aspects.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d4116bf607dc732e5ac829e483e5bde8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: A comparison of input space and action space of a simulated environment
    and the real-world environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Methods in Real-world Environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embodied navigation methods in simulated environments give a promising direction
    of solving real-world navigation problems. In this section, we are going to 1)
    introduce methods for real-world applications; 2) compare them with the methods
    in simulators; 3) discuss the possibility of sim-to-real transferring.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Real-world Navigation Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.1.1 Indoor Robotic Navigation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Deep learning plays an important role in indoor navigation for real-world applications.
    LeCun *et al.* [[176](#bib.bib176)] firstly adopt convolutional network for obstacle
    avoidance. Hadsell *et al.* [[177](#bib.bib177)] propose a self-supervised learning
    process that accurately classifies long-range vision semantics via a hierarchical
    deep model. The method is validated on a Learning applied to ground robots (LAGR) [[25](#bib.bib25)].
    Later, more and more real-world robots adopt deep learning to perceive and extract
    distinctive visual features [[178](#bib.bib178)]. Zhang *et al.* [[179](#bib.bib179)]
    research on the problem where a real robot navigates in simple maze-like environments.
    Based on the success of RL algorithms for solving challenging control tasks [[180](#bib.bib180),
    [77](#bib.bib77)], Zhang *et al.* employ successor representation in learning
    to achieve quick adaptation. Morad *et al.* [[26](#bib.bib26)] present an indoor
    object-driven navigation method named NavACL that uses automatic curriculum learning
    and is easily generalized to new environments and targets. Kahn *et al.* [[181](#bib.bib181)]
    adopt multitask learning and off-policy RL learning to learn directly from real-world
    events. This method enables a robot to learn autonomously and be easily deployed
    on multiple real-world tasks without any human provided labels.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Outdoor Robotic Navigation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There has been a long history that human study outdoor navigation robot. Thorpe
    *et al.* [[27](#bib.bib27)] present two algorithms, a RGB-based method for road
    following and a 3D-based method for obstacle detection, for a robot to learn to
    navigate in a campus. Ross *et al.* [[182](#bib.bib182)] combine deep learning
    and reinforcement learning to learn obstacle avoidance for UAVs. Morad *et al.*
    evaluate the performance of NavACL on two simulated environments, Gibson and Habitat.
    And we transfer the navigation to a Turtlebot3 wheeled robot (AGV) and a DJI Tello
    quadrotor (UAV). Both quantitative and qualitative results reveal that the policy
    of NavACL trained in the simualted environment is surprisingly effective in AGV
    and UAV. Manderson *et al.* [[183](#bib.bib183)] use conditional imitation learning
    to train an underwater vehicle to navigate close to sparse geographic waypoints
    without any prior map.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 Long-range Navigation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Their model achieves the best performance and shows competitive generalization
    ability on a real robot platform. Borenstein *et al.* [[184](#bib.bib184)] propose
    to maintain a world model [[185](#bib.bib185)] that updated continuously and in
    real-time to avoid obstacles. The world model learns and simulates the real-world
    environment and reduce the cost of data sampling [[185](#bib.bib185)]. Liu *et
    al.* propose Lifelong Federated Reinforcement Learning (LFRL), a learning architecture
    for navigation in cloud robotic systems to address this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Long-range navigation is challenging for real-world robots. To address this
    proble, Francis *et al.* [[186](#bib.bib186)] present PRM-RL, a hierarchical robot
    navigation method. The PRM-RL model consists of a reinforcement learning agent
    that learns short-range obstacle avoidance from noisy sensors, and a sampling-based
    planner to map the navigation space. Shah *et al.* [[187](#bib.bib187)] propose
    ViNG, a learning-based navigation system for reaching visually indicated goals
    and demonstrating this system on a real mobile robot platform. Unlike prior work,
    ViNG uses purely offline experience and does not require a simulator or online
    data collection, which significantly improves the training efficiency. Mapping [[188](#bib.bib188)]
    and path planning [[189](#bib.bib189)] has also been widely adopted by many real-world
    applications. Davison *et al.* [[190](#bib.bib190)] builds an automatic system,
    which is able to detect, store and track suitable landmark features during goal-directed
    navigation. They show how a robot can use active vision to provide continuous
    and accurate global positioning, thus achieving efficient navigation. Sim *et
    al.* [[191](#bib.bib191)] enable a robot to accurately localize its location by
    employing a hybrid map representation of 3D point landmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Navigation from Simulator to Real-world
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first demonstrate the challenges in the real-world navigation
    by comparing the difference between simulated environments and the real-world
    environment. Then we introduce the methods that focus on solving these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Comparison of Simulated and Real Navigation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Today, current achievements in the simulated navigation are still far of building
    a real-world navigation robot. Compared with the simulated environments, the real-world
    navigation environment is much more complex and ever changing. An comparison of
    inputs between a simulated environment (Habitat [[13](#bib.bib13)]) and the real-world
    environment is shown in Fig. [10](#S4.F10 "Figure 10 ‣ 4.2.5 Summary ‣ 4.2 Cross-modal
    Navigation ‣ 4 Methods in Simulated Environments ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Reasons of Domain Gap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We summarize three aspects cause the sim-real domain gap: 1) observation space;
    2) action space; 3) environmental dynamics.'
  prefs: []
  type: TYPE_NORMAL
- en: Observation Difference. An observation of the simulated environment can be an
    RGB image, a depth image, or a ground truth map. The quality of the RGB image
    and depth image inputs are high. The environment contains all static object information
    and enable it to provide ground truth information, like room structure, segmentation
    or object labels. The simulated environments provide unreal synthetic images with
    fewer objects where the real-world environments are far more complex with many.
    The sensors in the real-world environment, including RGB, GPS, and the velocity
    sensor, are usually noisy while the sensors in the simulated environment have
    no noise. Although some simulators [[11](#bib.bib11), [13](#bib.bib13), [14](#bib.bib14)]
    provide physical sensors and simulate some physical interactions (such as collision
    and acceleration), the performance of their physics engine is still far from real.
  prefs: []
  type: TYPE_NORMAL
- en: Action Difference. Different from the simple action space consists of ‘turn
    left’, ‘turn right’, and ‘go forward’ in the simulated environment, the action
    space in the real world is more challenging, depending on the structure of the
    robot. Lots of obstacles exist during real-world navigation, which blocks the
    robot from turning or moving forward. Real-world environments are often dynamic
    since the environment is so complex that many factors are changing in the long
    term or short term, such as temperature, moisture, friction, obstacles, and pedestrians.
    Another challenge which is also widely ignored in the simulated environments is
    the complexity and instability of the action space. For example, the results of
    executing the same action are uncertain since the physical condition is evolving,
    such as the wheels are skidding or get stuck.
  prefs: []
  type: TYPE_NORMAL
- en: Environmental Dynamics. The evolving of environmental conditions, such as temperature,
    humidity or parts wear cause the environmental dynamics. A policy without online
    adaptation ability cannot handle this problem well. Recently, more and more attention
    has been paid to the adaptive policy of learning dynamic environment. Some works [[192](#bib.bib192),
    [193](#bib.bib193), [194](#bib.bib194)] propose simulated robot environments to
    accomplish this, however, the simulation is far simpler than the real-world.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0673e726984068fcd1337fa0790b99dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The performances of methods on the Habitat *ObjectGoal* navigation,
    including DD-PPO [[195](#bib.bib195)], Active Exploration [[22](#bib.bib22)],
    SemExp [[126](#bib.bib126)] and 6-Act Tether [[96](#bib.bib96)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | R2R Validation Seen | R2R Validation Unseen | R2R Test Unseen |'
  prefs: []
  type: TYPE_TB
- en: '| TL | NE$\downarrow$ | SR$\uparrow$ | SPL$\uparrow$ | TL | NE$\downarrow$
    | SR$\uparrow$ | SPL$\uparrow$ | TL | NE$\downarrow$ | SR$\uparrow$ | SPL$\uparrow$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 9.58 | 9.45 | 16 | - | 9.77 | 9.23 | 16 | - | 9.89 | 9.79 | 13 |
    12 |'
  prefs: []
  type: TYPE_TB
- en: '| Human | - | - | - | - | - | - | - | - | 11.85 | 1.61 | 86 | 76 |'
  prefs: []
  type: TYPE_TB
- en: '| Seq2Seq [[12](#bib.bib12)] | 11.33 | 6.01 | 39 | - | 8.39 | 7.81 | 22 | -
    | 8.13 | 7.85 | 20 | 18 |'
  prefs: []
  type: TYPE_TB
- en: '| Speaker-Follower [[24](#bib.bib24)] | - | 3.36 | 66 | - | - | 6.62 | 35 |
    - | 14.82 | 6.62 | 35 | 28 |'
  prefs: []
  type: TYPE_TB
- en: '| RPA [[23](#bib.bib23)] | 8.46 | 5.56 | 43 | - | 7.22 | 7.65 | 25 | - | 9.15
    | 7.53 | 25 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SMNA [[129](#bib.bib129)] | - | 3.22 | 67 | 58 | - | 5.52 | 45 | 32 | 18.04
    | 5.67 | 48 | 35 |'
  prefs: []
  type: TYPE_TB
- en: '| RCM+SIL [[128](#bib.bib128)] | 10.65 | 3.53 | 67 | - | 11.46 | 6.09 | 43
    | - | 11.97 | 6.12 | 43 | 38 |'
  prefs: []
  type: TYPE_TB
- en: '| Regretful [[130](#bib.bib130)] | - | 3.23 | 69 | 63 | - | 5.32 | 50 | 41
    | 13.69 | 5.69 | 48 | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| PRESS* [[149](#bib.bib149)] | 10.57 | 4.39 | 58 | 55 | 10.36 | 5.28 | 49
    | 45 | 10.77 | 5.49 | 49 | 45 |'
  prefs: []
  type: TYPE_TB
- en: '| FAST-Short [[131](#bib.bib131)] | - | - | - | - | 21.17 | 4.97 | 56 | 43
    | 22.08 | 5.14 | 54 | 41 |'
  prefs: []
  type: TYPE_TB
- en: '| EnvDrop [[137](#bib.bib137)] | 11.00 | 3.99 | 62 | 59 | 10.70 | 5.22 | 52
    | 48 | 11.66 | 5.23 | 51 | 47 |'
  prefs: []
  type: TYPE_TB
- en: '| AuxRN [[138](#bib.bib138)] | - | 3.33 | 70 | 67 | - | 5.28 | 55 | 50 | -
    | 5.15 | 55 | 51 |'
  prefs: []
  type: TYPE_TB
- en: '| PREVALENT* [[151](#bib.bib151)] | 10.32 | 3.67 | 69 | 65 | 10.19 | 4.71 |
    58 | 53 | 10.51 | 5.30 | 54 | 51 |'
  prefs: []
  type: TYPE_TB
- en: '| Active Exploration [[196](#bib.bib196)] | 19.70 | 3.20 | 70 | 52 | 20.60
    | 4.36 | 58 | 40 | 21.6 | 4.33 | 60 | 41 |'
  prefs: []
  type: TYPE_TB
- en: '| RelGraph [[144](#bib.bib144)] | 10.13 | 3.47 | 67 | 65 | 9.99 | 4.73 | 57
    | 53 | 10.29 | 4.75 | 55 | 52 |'
  prefs: []
  type: TYPE_TB
- en: '| VLN $\circlearrowright$ BERT* [[152](#bib.bib152)] | 11.13 | 2.90 | 72 |
    68 | 12.01 | 3.93 | 63 | 57 | 12.35 | 4.09 | 63 | 57 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: Comparison of agent performance on R2R in single-run setting. *pretraining-based
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Solutions for the Domain Gap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The domain gap brings critical challenges and researchers put forward methods
    to fill the gap between these two settings. Mobile robot navigation is considered
    a geometric problem, which requires the robot to sense the geometric shape of
    the environment in order to plan collision-free paths to reach the target. Obstacle
    avoidance is one of the most important challenges, and many methods [[27](#bib.bib27),
    [176](#bib.bib176), [186](#bib.bib186)] have been put forward in previous work
    to achieve this. However, robot navigation in simulated tasks are regarded as
    a policy learning problem that learns a robust navigation policy from a starting
    position to the target in a complex environment with many possible routes. SLAM-based
    methods as in [[60](#bib.bib60), [116](#bib.bib116)] contribute a lot to mapping
    and path planning, which is general for both simulated and real navigation. Deep
    learning shows its ability in processing images and learning policies for robotic
    control, which is widely applied in both settings. However, the usages of deep
    learning are different between simulated navigation and real navigation. In real-world
    navigation, the deep neural network is used to perceive RGB inputs [[177](#bib.bib177)],
    predict the future [[197](#bib.bib197)] and learn the navigation policy [[186](#bib.bib186)].
    However, due to the sampling inefficiency and the complex dynamic factors of the
    real-world environment, the policy is not robust enough. Some works [[185](#bib.bib185),
    [184](#bib.bib184)] propose to model the environment and other works [[186](#bib.bib186)]
    adopt handcrafted rules to improve the robustness of the navigation policy. Data
    sampling is much more efficient in simulated environments. Most of the simulators
    render RGB and depth images in more than hundreds of frames per second (FPS),
    in which the fastest simulator, Habitat [[13](#bib.bib13)], achieves 100,000 FPS.
    Fast data sampling enables learning with large batch size. Many works prove that
    a large training batch size leads to robustness in representations [[198](#bib.bib198),
    [45](#bib.bib45)]. In spite of the rendered RGB and depth images, some simulated
    environments are able to provide semantic segmentation masks [[157](#bib.bib157),
    [14](#bib.bib14), [13](#bib.bib13)]. A more accurate simulator with few noise
    to facilitate training. With richer, noise-free data, researchers can apply a
    deeper neural network on navigation agents without worrying about overfitting.
    For example, Transformer [[117](#bib.bib117)] is widely applied in navigation
    works in simulated environments due to its capability of feature representation
    while it is easily overfitting if it is trained on noisy data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/32cf244e6e259c1736b0e35693a2ce61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The performances of methods on the Habitat *PointGoal* Challenge,
    including DD-PPO [[195](#bib.bib195)], ego-localization [[99](#bib.bib99)], Occupancy
    Anticipation [[199](#bib.bib199)] and SLAM-net.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 Learning Efficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many researchers focus on learning efficiency since data sampling in the real
    world is slow and expensive. Lobos-Tsunekawa *et al.* [[200](#bib.bib200)] propose
    a map-less visual navigation method for biped humanoid robots. In this method,
    DDPG algorithm [[77](#bib.bib77)] is used to extract information from color images,
    so as to derive motion commands. This method runs 20 ms on a physical robot, allowing
    its use in real-time applications. Bruce *et al.* [[201](#bib.bib201)] present
    a method for learning to navigate to a fixed goal on a mobile robot. By using
    an interactive replay of a single traversal of the environment and stochastic
    environmental augmentation, Bruce *et al.* demonstrates zero-shot transfer under
    real-world environmental variations without fine-tuning. To further improve the
    sampling efficiency, Pfeiffer *et al.* [[202](#bib.bib202)] leverage prior expert
    demonstrations for pre-training so that the training cost could be largely reduced
    in the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Navigation Transferring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Transfer learning is attracting rising attention in embodied navigation. The
    researchers are motivated from two aspects: 1) learn a navigation agent that is
    able to perform accurate and efficient navigation in diverse domains and tasks;
    2) deploy an agent trained in a simulated environment in a real-world navigation
    robot.'
  prefs: []
  type: TYPE_NORMAL
- en: It is challenging to train a model to learn skills for navigating in different
    domains. Moreover, due to the large domain gap between simulated environments
    and the real-world environment, a well-performed navigation policy trained on
    a simulated environment cannot be easily transferred to the real-world environment.
    A lot of navigation tasks have been proposed to investigate different capabilities
    for navigation in diverse scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we discuss the transfer learning in navigation from two different
    levels: 1) task-level transferring; 2) environment-level transferring, including
    sim-to-real transferring. The task-level transferring requires the agent to learn
    a policy that adapts to different input modalities or targets; the environment-level
    transferring requires the model to be invariant to different dynamics and transition
    functions.'
  prefs: []
  type: TYPE_NORMAL
- en: DisCoRL[[28](#bib.bib28)] introduce a policy distillation method [[203](#bib.bib203)]
    to transfer a 2D navigation policy. In addition to the navigation policy, the
    vision and language embedding layer could also be transferred  [[29](#bib.bib29)].
    Motivated by the success of meta-learning [[204](#bib.bib204)], Dimension-variable
    skill transfer (DVST) [[205](#bib.bib205)] obtains a meta-agent with deep reinforcement
    learning and then transfers the meta-skill to a robot with a different dimensional
    configuration using a method named dimension-variable skill transfer. Similarly,
    Li *et al.* [[206](#bib.bib206)] propose an unsupervised reinforcement learning
    method to learn transferable meta-skills. Zhu *et al.* [[63](#bib.bib63)] decompose
    long navigation instructions into shorter ones, and thus enables the model to
    be easily transferred to navigation tasks with longer trajectories. Chaplot *et
    al.* [[207](#bib.bib207)] propose a multi-task model that jointly learns multi-modal
    tasks, and transfers vision-language knowledge across the tasks. The model adopts
    a Dual-Attention unit to disentangle the vision knowledge and language knowledge
    and align them with each other.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e1cefb4e349a3e769d9a78dbfeaa8e71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: A summary of future directions for building an advanced robotic
    for real-world navigation.'
  prefs: []
  type: TYPE_NORMAL
- en: Wang *et al.* [[208](#bib.bib208)] propose to learn environment-agnostic representations
    for the navigation policy enables the model to perform on both Vision-Language
    Navigation (VLN) and Navigation from Dialog History (NDH) tasks. Yan *et al.* [[30](#bib.bib30)]
    propose MVV-IN, a method that acquires transferable meta-skills with multi-modal
    inputs to cope with new tasks. Liu *et al.* [[209](#bib.bib209)] investigate on
    how to make robots fuse and transfer their experience so that they can effectively
    use prior knowledge and quickly adapt to new environments. Gordon *et al.* [[210](#bib.bib210)]
    propose to decouple the visual perception and policy to facilitates transfer to
    new environments and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Sim-real transferring have been well studied in the field of robotic control [[211](#bib.bib211),
    [212](#bib.bib212)]. Sadeghi *et al.* [[213](#bib.bib213)] firstly propose a learning-based
    method, which trains a navigation agent entirely in a simulator and then transfers
    it into real-world environments without finetuning on any real images. Consequently,
    Yuan *et al.* [[214](#bib.bib214)] adopt a sim-real transfer strategy for learning
    navigation controllers using an end-to-end policy that maps raw pixels as visual
    input to control actions without any form of engineered feature extraction. Tai
    *et al.* [[215](#bib.bib215)] train a robot in simulation with Asynchronous DDPG [[77](#bib.bib77)]
    algorithm and directly deployed the learned controller to a real robot for navigation
    transferring. Rusu *et al.* [[212](#bib.bib212)] introduce a progressive network
    to transfer the learned policies from simulation to the real world. Similarly,
    adversarial feature adaptation methods [[216](#bib.bib216)] is also applicable
    in sim-to-real policy transferring [[217](#bib.bib217)]. Sim-to-real transfer
    for deep reinforcement learning policies can be applied to complex navigation
    tasks [[218](#bib.bib218)], including six-legged robots [[219](#bib.bib219)],
    robots for soccer competitions [[220](#bib.bib220)], etc.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, We firstly compare the difference between simulated environments
    and real-world environments. Then, we reason about the domain gaps that cause
    the domain gaps. Finally, we introduce some transfer learning works in the navigation
    to give a promising direction to solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although extensive works have addressed the navigation problem from diverse
    aspects, current research progress is still far from real artificial intelligence.
    Also, current work cannot build a robust robot for real-world navigation. We summarize
    the challenges in solving embodied AI into these aspects: 1) the functions and
    the performance are limited by the embodied environment; 2) the navigation problem
    is not well defined; 3) the performances of embodied AI agents in complex environments
    are still poor; 4) perceiving natural language is difficult to learn; 5) hard
    to deploy a trained navigation policy to the real-world application.'
  prefs: []
  type: TYPE_NORMAL
- en: Future Embodied Environments. The advanced functions in the environment help
    the navigation model to obtain high-level abilities. For instance, compared to
    the early embodied environments, the large scene in the Matterport3D [[36](#bib.bib36)]
    firstly requires the navigation model to explore and memorize the complex room
    structure. The vision-language navigation benchmark [[12](#bib.bib12)] enables
    the agents to perceive natural language. The interactive embodied environment
    like AI2-THOR [[40](#bib.bib40)] and iGibson [[41](#bib.bib41)] enable the agent
    to perform interactive actions. The agent learned in an interactive environment
    is able to move an object, put an object and open a door.
  prefs: []
  type: TYPE_NORMAL
- en: An environment with more functions is the basis of learning a smart agent. An
    agent must be able to handle a dynamic environment when the objects in the rooms
    with evolving conditions. In stead of navigating within the navigable areas like [[12](#bib.bib12),
    [13](#bib.bib13)], we expect an agent to find possible roads within a room that
    has many obstacles. In addition, we need an interactive agent, which can pick
    up and put down objects, move chairs, and interact with human beings. Other modes
    such as walking, running, and climbing also need to be considered if we want to
    build a robust navigator within a complex indoor environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define Advanced Navigation Tasks. Even though many embodied navigation tasks
    and navigation metrics have been proposed, what is a good navigation policy remains
    unclear. This problem has two folds: 1) what factors have to be considered; 2)
    how to balance these factors. As we analysed in Sec. [3.4](#S3.SS4 "3.4 Evaluation
    Metrics ‣ 3 Embodied Navigation Benchmarks ‣ Deep Learning for Embodied Visual
    Navigation Research: A Survey"), the accuracy and efficiency are two main factors
    to evaluate the performance of navigation. However, the importance of accuracy
    and efficiency are different among metrics. Optimal navigation policy varies according
    to different metrics. In the interactive navigation task proposed by [[221](#bib.bib221)],
    the performance of the agent is evaluated by a path efficiency score and a effort
    efficiency score. The interactive navigation task varies the score weights in
    evaluation to test if an agent performs well in different settings. However, it
    is still unclear how the weight of the factors affects the test results and what
    setting it is in real-world application.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, in the questioning-answering settings like Help Anna [[59](#bib.bib59)],
    or RMM [[166](#bib.bib166)], the frequency of questioning or requesting from the
    agent are take into consideration. Current methods regard that the performance
    is lower if the agent ask for more information from human. Balancing the ‘cost’
    of asking questions and the ‘cost’ of navigation is still a challenging problem.
    We hope that the community could publish more works discussing on how to evaluate
    the advanced navigation behavior or comparing the differences of navigation policies
    between an agent and a human.
  prefs: []
  type: TYPE_NORMAL
- en: 'Improve Navigation Performance. Current navigation agents still perform poorly
    even in easy navigation tasks such as *PointGoal* task and *ObjectGoal* task,
    as shown in Fig. [12](#S6.F12 "Figure 12 ‣ 6.1.2 Solutions for the Domain Gap
    ‣ 6.1 Comparison of Simulated and Real Navigation ‣ 6 Navigation from Simulator
    to Real-world ‣ Deep Learning for Embodied Visual Navigation Research: A Survey")
    and Fig. [11](#S6.F11 "Figure 11 ‣ 6.1.1 Reasons of Domain Gap ‣ 6.1 Comparison
    of Simulated and Real Navigation ‣ 6 Navigation from Simulator to Real-world ‣
    Deep Learning for Embodied Visual Navigation Research: A Survey"). The state-of-the
    art model of the *PointGoal* task performs 64.5% on success rate (SR) and 37.7%
    on SPL, while on the *ObjectGoal* task, the best model performs 21.08% on success
    rate (SR) and 8.38% on SPL. As shown in Fig. [IV](#S6.T4 "TABLE IV ‣ 6.1.1 Reasons
    of Domain Gap ‣ 6.1 Comparison of Simulated and Real Navigation ‣ 6 Navigation
    from Simulator to Real-world ‣ Deep Learning for Embodied Visual Navigation Research:
    A Survey"), the current state-of-the-art model [[152](#bib.bib152)] in the vision-language
    navigation task performs 63% in SR and 57% in SPL while the human performance
    is 86% in SR and 76% in SPL. The navigation performances of current models are
    still far from human performance. Besides, existing models usually perform poorly
    on the challenging task of vision-and-language navigation. As shown in Tab. [IV](#S6.T4
    "TABLE IV ‣ 6.1.1 Reasons of Domain Gap ‣ 6.1 Comparison of Simulated and Real
    Navigation ‣ 6 Navigation from Simulator to Real-world ‣ Deep Learning for Embodied
    Visual Navigation Research: A Survey"), there is about 19% performance gap between
    the state-of-the-art model and the human baseline. In the interactive dialog tasks [[59](#bib.bib59),
    [166](#bib.bib166), [222](#bib.bib222)], the natural language used by agents to
    interact with oracle has many errors and is not fluent. The baselines in the recently
    proposed tasks [[221](#bib.bib221)] can hardly complete the task. Moreover, the
    navigation robot in the real world cannot perform as well as in the simulated
    environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Several promising directions, which are motivated by referring recent works,
    could tackle these problems. Transformer [[117](#bib.bib117)] shows its capability
    in feature extraction and cross-modal fusion. Some works [[151](#bib.bib151),
    [152](#bib.bib152)] build navigation models based on Transformer and achieve great
    success in vision-language navigation task, which reveals that Transformer structure
    is beneficial for cross-modal navigation policy. Chaplot *et al.* [[125](#bib.bib125)]
    build a neural SLAM module into a navigation model and train the model via hierarchical
    reinforcement learning. This model is able to learn the structure of the room
    and perform a robust low-level navigation policy in an environment with continuous
    state space. We suggest that model-based methods [[96](#bib.bib96), [138](#bib.bib138)]
    and hierarchical reinforcement learning [[223](#bib.bib223)] are the key to build
    a robust navigation model.
  prefs: []
  type: TYPE_NORMAL
- en: Smartly Perceiving Natural Language. Natural language is a complex modality
    for a robot to understand due to its diversity and complexity. At this moment,
    however, teach a navigation robot to learn to understand language requires a large
    amount of natural language annotations and each of them describes the semantics
    of a trajectory, a scene or a kind of behavior. The language annotations can be
    a word, a sentence, a question-answer pair or a dialogue, which are pretty expensive
    and labor-intensive.
  prefs: []
  type: TYPE_NORMAL
- en: Even if we have sufficient language annotations for training a navigation robot,
    it is still challenging for the robot to correctly understand language instructions.
    For example, because there are many natural language variants for describing the
    same trajectory or scene, supervising an agent with trajectory-instruction pairs
    may led to severe overfitting. In addition, the skill of perceiving natural language
    needs prior knowledge. For example, “find the forth chair in the living room”
    requires the agent be able to count and “navigate to bathroom safely” requires
    the agent to turn smoothly and do not touch any objects. Some works [[151](#bib.bib151),
    [152](#bib.bib152)] adopt pre-training methods to obtain a better language understanding
    skill with prior knowledge. Based on the success of these works, we believe that
    learning from other large-scale language datasets [[224](#bib.bib224), [225](#bib.bib225),
    [147](#bib.bib147)] and transfer the prior knowledge might be a promising direction
    in solving the challenges in understanding natural language instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy Robust Policies on Real World. Even though we have obtained a robust
    navigation policy in a simulated environment, how to deploy this policy to real-world
    still remains challenging. As demonstrated in Sec. [6.1](#S6.SS1 "6.1 Comparison
    of Simulated and Real Navigation ‣ 6 Navigation from Simulator to Real-world ‣
    Deep Learning for Embodied Visual Navigation Research: A Survey"), three major
    differences cause the large sim-real domain gap: 1) observation; 2) action space;
    3) environmental dynamics. Large sim-real domain gap hinders the direct deployment
    of the learned navigation policy to the real world. There are two directions in
    tackling this problem. One way is building a realistic simulator, including a
    realistic visual image rendering mechanism, advanced physical sensors, obstacle
    objects, dynamic simulation, simulation of robot components like wheels and gears,
    etc. However, such a realistic visual simulator is computation costly. Another
    way of solving sim-real deployment problem is achieving online adaptation by transfer
    learning or meta-reinforcement learning [[226](#bib.bib226), [204](#bib.bib204)].
    These methods enable an agent to change its policy to adapt the environment. This
    method not only has high computational efficiency, but also has stronger adaptability
    when accidents happen.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presented a comprehensive survey on the embodied navigation scenario
    by summarizing hundreds of works. We thoroughly investigate the environments,
    tasks, and metrics to introduce the problem that the researchers are trying to
    solve. And we introduce hundreds of methods that solve these tasks in the embodied
    environments and compare their differences. Then we introduce the methods in the
    real-world environment and demonstrate how the large domain gap led to the drop
    in navigation performance. At last, we analyze the current problems that exist
    in the embodied navigation and give out four future directions to improve our
    community.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *Communications of The ACM*, vol. 60,
    no. 6, pp. 84–90, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] S. Thrun, *Probabilistic Robotics*, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] P. Dourish, *Where the Action Is: The Foundations of Embodied Interaction*,
    2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] T. Kruse, A. K. Pandey, R. Alami, and A. Kirsch, “Human-aware robot navigation:
    A survey,” *Robotics and Autonomous Systems*, vol. 61, no. 12, pp. 1726–1743,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] G. N. DeSouza and A. C. Kak, “Vision for mobile robot navigation: A survey,”
    *TPAMI*, vol. 24, no. 2, pp. 237–267, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] S. Thrun, “Probabilistic algorithms in robotics,” *Ai Magazine*, vol. 21,
    no. 4, pp. 93–109, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. Kosaka and A. Kak, “Fast vision-guided mobile robot navigation using
    model-based reasoning and prediction of uncertainties,” in *IROS*, vol. 3, 1992,
    pp. 2177–2186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] M. Kabuka and A. Arenas, “Position verification of a mobile robot using
    standard pattern,” *ICRA*, vol. 3, no. 6, pp. 505–516, 1987.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. Wu, Y. Wu, G. Gkioxari, and Y. Tian, “Building generalizable agents
    with a realistic and rich 3d environment,” in *ICLR (Workshop)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] M. Savva, A. X. Chang, A. Dosovitskiy, T. A. Funkhouser, and V. Koltun,
    “Minos: Multimodal indoor simulator for navigation in complex environments.” *arXiv
    preprint arXiv:1712.03931*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sunderhauf, I. Reid,
    S. Gould, and A. van den Hengel, “Vision-and-language navigation: Interpreting
    visually-grounded navigation instructions in real environments,” in *CVPR*, 2018,
    pp. 3674–3683.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub,
    J. Liu, V. Koltun, J. Malik *et al.*, “Habitat: A platform for embodied ai research,”
    in *ICCV*, 2019, pp. 9339–9347.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese, “Gibson
    env: Real-world perception for embodied agents,” in *CVPR*, 2018, pp. 9068–9079.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi,
    “Target-driven visual navigation in indoor scenes using deep reinforcement learning,”
    in *ICRA*, 2017, pp. 3357–3364.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] A. Mousavian, A. Toshev, M. Fiser, J. Kosecka, A. Wahid, and J. Davidson,
    “Visual representations for semantic target driven navigation,” in *ICRA*, 2019,
    pp. 8846–8852.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] C. Xie, S. Patil, T. Moldovan, S. Levine, and P. Abbeel, “Model-based
    reinforcement learning with parametrized physical models and optimism-driven exploration,”
    in *ICRA*, 2016, pp. 504–511.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver,
    and K. Kavukcuoglu, “Reinforcement learning with unsupervised auxiliary tasks,”
    in *ICLR 2017 : ICLR 2017*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. Ballard, A. Banino, M. Denil,
    R. Goroshin, L. Sifre, K. Kavukcuoglu, D. Kumaran, and R. Hadsell, “Learning to
    navigate in complex environments,” in *ICLR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] S. Gupta, V. Tolani, J. Davidson, S. Levine, R. Sukthankar, and J. Malik,
    “Cognitive mapping and planning for visual navigation,” *IJCV*, vol. 128, no. 5,
    pp. 1311–1330, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] W. Qi, R. T. Mullapudi, S. Gupta, and D. Ramanan, “Learning to move with
    affordance maps,” in *ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, “Learning
    to explore using active neural slam,” in *ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] X. Wang, W. Xiong, H. Wang, and W. Yang Wang, “Look before you leap: Bridging
    model-free and model-based reinforcement learning for planned-ahead vision-and-language
    navigation,” in *ECCV*, 2018, pp. 37–53.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick,
    K. Saenko, D. Klein, and T. Darrell, “Speaker-follower models for vision-and-language
    navigation,” in *NeurIPS*, 2018, pp. 3314–3325.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] L. D. Jackel, E. Krotkov, M. Perschbacher, J. Pippine, and C. Sullivan,
    “The darpa lagr program: Goals, challenges, methodology, and phase i results,”
    *Journal of Field robotics*, vol. 23, no. 11-12, pp. 945–973, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. D. Morad, R. Mecca, R. P. K. Poudel, S. Liwicki, and R. Cipolla, “Embodied
    visual navigation with automatic curriculum learning in real environments,” in
    *IEEE Robotics and Automation Letters*, vol. 6, no. 2, 2021, pp. 683–690.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] C. Thorpe, M. H. Hebert, T. Kanade, and S. A. Shafer, “Vision and navigation
    for the carnegie-mellon navlab,” *TPAMI*, vol. 10, no. 3, pp. 362–373, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] R. Traoré, H. Caselles-Dupré, T. Lesort, T. Sun, G. Cai, N. Díaz-Rodríguez,
    and D. Filliat, “Discorl: Continual reinforcement learning via policy distillation,”
    *NeurIPS workshop on Deep Reinforcement Learning*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] H. Huang, V. Jain, H. Mehta, A. Ku, G. Magalhaes, J. Baldridge, and E. Ie,
    “Transferable representation learning in vision-and-language navigation,” in *ICCV*,
    2019, pp. 7404–7413.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] L. Yan, D. Liu, Y. Song, and C. Yu, “Multimodal aggregation approach for
    memory vision-voice indoor navigation with meta-learning,” in *IROS*, 2020, pp.
    5847–5854.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] M. Fisher, D. Ritchie, M. Savva, T. Funkhouser, and P. Hanrahan, “Example-based
    synthesis of 3d object arrangements,” *international conference on computer graphics
    and interactive techniques*, vol. 31, no. 6, p. 135, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] A. Handa, V. Patraucean, S. Stent, and R. Cipolla, “Scenenet: An annotated
    model generator for indoor scene understanding,” in *ICRA*, 2016, pp. 5737–5743.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, “Joint 2d-3d-semantic
    data for indoor scene understanding.” *arXiv preprint arXiv:1702.01105*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser, “Semantic
    scene completion from a single depth image,” in *CVPR*, 2017, pp. 190–198.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] C. Yan, D. K. Misra, A. Bennett, A. Walsman, Y. Bisk, and Y. Artzi, “Chalet:
    Cornell house agent learning environment.” *arXiv preprint arXiv:1801.07357*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niebner, M. Savva, S. Song,
    A. Zeng, and Y. Zhang, “Matterport3d: Learning from rgb-d data in indoor environments,”
    in *3DV*, 2017, pp. 667–676.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel,
    R. Mur-Artal, C. Ren, S. Verma, A. Clarkson, M. Yan, B. Budge, Y. Yan, X. Pan,
    J. Yon, Y. Zou, K. Leon, N. Carter, J. Briales, T. Gillingham, E. Mueggler, L. Pesqueira,
    M. Savva, D. Batra, H. M. Strasdat, R. D. Nardi, M. Goesele, S. Lovegrove, and
    R. A. Newcombe, “The replica dataset: A digital replica of indoor spaces.” *arXiv
    preprint arXiv:1906.05797*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] L. Tai and M. Liu, “Towards cognitive exploration through deep reinforcement
    learning for mobile robots,” *arXiv preprint arXiv:1610.01733*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] M. Deitke, W. Han, A. Herrasti, A. Kembhavi, E. Kolve, R. Mottaghi, J. Salvador,
    D. Schwenk, E. VanderBilt, M. Wallingford *et al.*, “Robothor: An open simulation-to-real
    embodied ai platform,” in *CVPR*, 2020, pp. 3164–3174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi, “Ai2-thor:
    An interactive 3d environment for visual ai.” *arXiv preprint arXiv:1712.05474*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] F. Xia, W. B. Shen, C. Li, P. Kasimbeg, M. E. Tchapmi, A. Toshev, R. Martin-Martin,
    and S. Savarese, “Interactive gibson benchmark: A benchmark for interactive navigation
    in cluttered environments,” *IEEE Robotics and Automation Letters*, vol. 5, no. 2,
    pp. 713–720, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] M. L. Littman, “Markov games as a framework for multi-agent reinforcement
    learning,” in *ICML*, 1994, pp. 157–163.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] M. Tan, “Multi-agent reinforcement learning: independent vs. cooperative
    agents,” in *ICML*, 1997, pp. 487–494.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] P. Anderson, A. X. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun,
    J. Kosecka, J. Malik, R. Mottaghi, M. Savva, and A. R. Zamir, “On evaluation of
    embodied navigation agents.” *arXiv preprint arXiv:1807.06757*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] S. Wani, S. Patel, U. Jain, A. X. Chang, and M. Savva, “Multion: Benchmarking
    semantic map memory using multi-object navigation,” in *NeurIPS*, vol. 33, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] V. Jain, G. Magalhaes, A. Ku, A. Vaswani, E. Ie, and J. Baldridge, “Stay
    on the path: Instruction fidelity in vision-and-language navigation,” in *ACL*,
    2019, pp. 1862–1872.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge, “Room-across-room:
    Multilingual vision-and-language navigation with dense spatiotemporal grounding,”
    in *EMNLP*, 2020, pp. 4392–4412.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Y. Hong, C. Rodriguez, Q. Wu, and S. Gould, “Sub-instruction aware vision-and-language
    navigation,” in *EMNLP*, 2020, pp. 3360–3376.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] J. Thomason, M. Murray, M. Cakmak, and L. Zettlemoyer, “Vision-and-dialog
    navigation,” *CoRL*, pp. 394–406, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
    D. Parikh, “Vqa: Visual question answering,” in *ICCV*, 2015, pp. 2425–2433.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang,
    “Bottom-up and top-down attention for image captioning and visual question answering,”
    in *CVPR*, 2018, pp. 6077–6086.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
    Y. Kalantidis, L.-J. Li, D. A. Shamma, M. S. Bernstein, and L. Fei-Fei, “Visual
    genome: Connecting language and vision using crowdsourced dense image annotations,”
    *IJCV*, vol. 123, no. 1, pp. 32–73, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked attention networks
    for image question answering,” in *CVPR*, 2016, pp. 21–29.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, “Embodied
    question answering,” in *CVPR Workshops (CVPRW)*, 2018, pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Y. Wang, C. Shen, and A. van den
    Hengel, “Reverie: Remote embodied visual referring expression in real indoor environments,”
    in *CVPR*, 2020, pp. 9982–9991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] C. Chen, U. Jain, C. Schissler, S. V. A. Gari, Z. Al-Halah, V. K. Ithapu,
    P. Robinson, and K. Grauman, “Soundspaces: Audio-visual navigation in 3d environments,”
    in *ECCV*, 2020, pp. 17–36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] L. Yu, X. Chen, G. Gkioxari, M. Bansal, T. L. Berg, and D. Batra, “Multi-target
    embodied question answering,” in *CVPR*, 2019, pp. 6309–6318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and A. Farhadi,
    “Iqa: Visual question answering in interactive environments,” in *CVPR*, 2018,
    pp. 4089–4098.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] K. Nguyen and H. Daumé, “Help, anna! vision-based navigation with natural
    multimodal assistance via retrospective curiosity-encouraging imitation learning,”
    in *EMNLP*, 2019, pp. 684–695.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik, “Cognitive
    mapping and planning for visual navigation,” in *CVPR*, 2017, pp. 7272–7281.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] D. K. Misra, J. Langford, and Y. Artzi, “Mapping instructions and visual
    observations to actions with reinforcement learning.” in *EMNLP*, 2017, pp. 1004–1015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] H. Chen, A. Suhr, D. Misra, N. Snavely, and Y. Artzi, “Touchdown: Natural
    language navigation and spatial reasoning in visual street environments,” in *CVPR*,
    2019, pp. 12 538–12 547.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] W. Zhu, H. Hu, J. Chen, Z. Deng, V. Jain, E. Ie, and F. Sha, “Babywalk:
    Going farther in vision-and-language navigation by taking baby steps,” in *ACL*,
    2020, pp. 2539–2556.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] D. J. Berndt and J. Clifford, “Using dynamic time warping to find patterns
    in time series,” in *AAAIWS’94 Proceedings of the 3rd International Conference
    on Knowledge Discovery and Data Mining*, 1994, pp. 359–370.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] H. Sakoe and S. Chiba, “Dynamic programming algorithm optimization for
    spoken word recognition,” *IEEE transactions on acoustics, speech, and signal
    processing*, vol. 26, no. 1, pp. 43–49, 1978.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] A. Vakanski, I. Mantegh, A. Irish, and F. Janabi-Sharifi, “Trajectory
    learning for robot programming by demonstration using hidden markov model and
    dynamic time warping,” *IEEE Transactions on Systems, Man, and Cybernetics*, vol. 42,
    no. 4, pp. 1039–1052, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] E. J. Keogh and M. J. Pazzani, “Scaling up dynamic time warping for datamining
    applications,” in *KDD*, 2000, pp. 285–289.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] G. Ilharco, V. Jain, A. Ku, E. Ie, and J. Baldridge, “General evaluation
    for instruction conditioned navigation using dynamic time warping,” *ViGIL@NeurIPS*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Harley, T. P. Lillicrap,
    D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,”
    in *ICML*, 2016, pp. 1928–1937.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] A. Tamar, Y. Wu, G. Thomas, S. Levine, and P. Abbeel, “Value iteration
    networks,” in *IJCAI*, 2017, pp. 4949–4953.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] L. Lee, E. Parisotto, D. S. Chaplot, E. P. Xing, and R. Salakhutdinov,
    “Gated path planning networks,” in *ICML*, 2018, pp. 2947–2955.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] S. Lenser and M. Veloso, “Visual sonar: fast obstacle avoidance using
    monocular vision,” in *IROS*, vol. 1, 2003, pp. 886–891.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] E. Royer, J. Bom, M. Dhome, B. Thuilot, M. Lhuillier, and F. Marmoiton,
    “Outdoor autonomous navigation using monocular vision,” in *IROS*, 2005, pp. 1253–1258.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] H. Haddad, M. Khatib, S. Lacroix, and R. Chatila, “Reactive navigation
    in outdoor environments using potential fields,” in *IROS*, vol. 2, 1998, pp.
    1232–1237.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] A. Remazeilles, F. Chaumette, and P. Gros, “Robot motion control from
    a visual memory,” in *ICRA*, vol. 5, 2004, pp. 4695–4700.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Y. Zhu, D. Gordon, E. Kolve, D. Fox, L. Fei-Fei, A. Gupta, R. Mottaghi,
    and A. Farhadi, “Visual semantic planning using deep successor representations,”
    in *ICCV*, 2017, pp. 483–492.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    in *ICLR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Y. Li and J. Kosecka, “Learning view and target invariant visual servoing
    for navigation,” in *ICRA*, 2020, pp. 658–664.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] T.-Y. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *ECCV*, 2014,
    pp. 740–755.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] A. Mousavian, H. Pirsiavash, and J. Kosecka, “Joint semantic segmentation
    and depth estimation with deep convolutional networks,” in *3DV*, 2016, pp. 611–619.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] W. Shen, D. Xu, Y. Zhu, L. Fei-Fei, L. Guibas, and S. Savarese, “Situational
    fusion of visual representation for visual navigation,” in *ICCV*, 2019, pp. 2881–2890.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Y. Lv, N. Xie, Y. Shi, Z. Wang, and H. T. Shen, “Improving target-driven
    visual navigation with attention on 3d spatial relationships,” *arXiv preprint
    arXiv:2005.02153*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Q. Wu, K. Xu, J. Wang, M. Xu, X. Gong, and D. Manocha, “Reinforcement
    learning-based visual navigation with information-theoretic regularization,” in
    *IEEE Robotics and Automation Letters*, vol. 6, no. 2, 2021, pp. 731–738.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] A. Khosla, B. An, J. J. Lim, and A. Torralba, “Looking beyond the visible
    scene,” in *CVPR*, 2014, pp. 3710–3717.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. Brahmbhatt and J. Hays, “Deepnav: Learning to navigate large cities,”
    in *CVPR*, 2017, pp. 3087–3096.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] P. Mirowski, M. K. Grimes, M. Malinowski, K. M. Hermann, K. Anderson,
    D. Teplyashin, K. Simonyan, K. Kavukcuoglu, A. Zisserman, and R. Hadsell, “Learning
    to navigate in cities without a map,” in *NeurIPS*, vol. 31, 2018, pp. 2419–2430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] C. Chen, S. Majumder, Z. Al-Halah, R. Gao, S. K. Ramakrishnan, and K. Grauman,
    “Learning to set waypoints for audio-visual navigation,” in *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] C. Doersch, A. Gupta, and A. A. Efros, “Unsupervised visual representation
    learning by context prediction,” in *ICCV*, 2015, pp. 1422–1430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] M. Noroozi and P. Favaro, “Unsupervised learning of visual representations
    by solving jigsaw puzzles,” in *ECCV*, 2016, pp. 69–84.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] R. Zhang, P. Isola, and A. A. Efros, “Colorful image colorization,” in
    *ECCV*, 2016, pp. 649–666.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. Sun, X. Wang, Z. Liu, J. Miller, A. A. Efros, and M. Hardt, “Test-time
    training with self-supervision for generalization under distribution shifts,”
    in *ICML*, 2020, pp. 9229–9248.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] T. Schaul, D. Horgan, K. Gregor, and D. Silver, “Universal value function
    approximators,” in *ICML*, 2015, pp. 1312–1320.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] A. Dosovitskiy and V. Koltun, “Learning to act by predicting the future,”
    in *ICLR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. Jaśkowski, “Vizdoom:
    A doom-based ai research platform for visual reinforcement learning,” in *CIG*.   IEEE,
    2016, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] J. Ye, D. Batra, E. Wijmans, and A. Das, “Auxiliary tasks speed up learning
    pointgoal navigation.” *arXiv preprint arXiv:2007.04561*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] J. Ye, D. Batra, A. Das, and E. Wijmans, “Auxiliary tasks and exploration
    enable objectnav.” *arXiv preprint arXiv:2104.04112*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] X. Ye, Z. Lin, J.-Y. Lee, J. Zhang, S. Zheng, and Y. Yang, “Gaple: Generalizable
    approaching policy learning for robotic object searching in indoor environment,”
    *IEEE Robotics and Automation Letters*, vol. 4, no. 4, pp. 4003–4010, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] G.-H. Liu, A. Siravuru, S. P. Selvaraj, M. M. Veloso, and G. Kantor, “Learning
    end-to-end multimodal sensor policies for autonomous navigation.” *CoRL*, pp.
    249–261, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] S. Datta, O. Maksymets, J. Hoffman, S. Lee, D. Batra, and D. Parikh, “Integrating
    egocentric localization for more realistic point-goal navigation agents,” *arXiv
    preprint arXiv:2009.03231*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] R. Bigazzi, F. Landi, M. Cornia, S. Cascianelli, L. Baraldi, and R. Cucchiara,
    “Explore and explain: Self-supervised navigation and recounting,” in *ICPR*, 2021,
    pp. 1152–1159.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] V. Dean, S. Tulsiani, and A. Gupta, “See, hear, explore: Curiosity via
    audio-visual association,” in *NeurIPS*, vol. 33, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] H. Durrant-Whyte and T. Bailey, “Simultaneous localization and mapping:
    part i,” *IEEE robotics & automation magazine*, vol. 13, no. 2, pp. 99–110, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] M. Filipenko and I. Afanasyev, “Comparison of various slam systems for
    mobile robot in an indoor environment,” in *IS*, 2018, pp. 400–407.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] S. Se, D. G. Lowe, and J. J. Little, “Mobile robot localization and mapping
    with uncertainty using scale-invariant visual landmarks,” *The International Journal
    of Robotics Research*, vol. 21, no. 8, pp. 735–758, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] C. F. Olson, L. H. Matthies, M. Schoppers, and M. W. Maimone, “Rover
    navigation using stereo ego-motion,” *Robotics and Autonomous Systems*, vol. 43,
    no. 4, pp. 215–229, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Davison, “Real-time simultaneous localisation and mapping with a single
    camera,” in *ICCV*, vol. 2, 2003, pp. 1403–1410.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] J. Engel, V. Koltun, and D. Cremers, “Direct sparse odometry,” *TPAMI*,
    vol. 40, no. 3, pp. 611–625, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] J. Engel, T. Schöps, and D. Cremers, “Lsd-slam: Large-scale direct monocular
    slam,” in *ECCV*.   Springer, 2014, pp. 834–849.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] R. Mur-Artal and J. D. Tardós, “Orb-slam2: An open-source slam system
    for monocular, stereo, and rgb-d cameras,” *IEEE Transactions on Robotics*, vol. 33,
    no. 5, pp. 1255–1262, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] K. Tateno, F. Tombari, I. Laina, and N. Navab, “Cnn-slam: Real-time dense
    monocular slam with learned depth prediction,” in *CVPR*, 2017, pp. 6243–6252.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] C. Wang, J. Miguel Buenaposada, R. Zhu, and S. Lucey, “Learning depth
    from monocular videos using direct methods,” in *CVPR*, 2018, pp. 2022–2030.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] N. Yang, L. v. Stumberg, R. Wang, and D. Cremers, “D3vo: Deep depth,
    deep pose and deep uncertainty for monocular visual odometry,” in *CVPR*, 2020,
    pp. 1281–1292.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *CVPR*.   IEEE, 2012, pp. 3354–3361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W.
    Achtelik, and R. Siegwart, “The euroc micro aerial vehicle datasets,” *The International
    Journal of Robotics Research*, vol. 35, no. 10, pp. 1157–1163, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] E. Parisotto and R. Salakhutdinov, “Neural map: Structured memory for
    deep reinforcement learning.” in *ICLR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] J. Zhang, L. Tai, J. Boedecker, W. Burgard, and M. Liu, “Neural slam:
    Learning to explore with external memory,” *arXiv preprint arXiv:1706.09520*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” in *NeurIPS*, vol. 30,
    2017, pp. 5998–6008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] J. F. Henriques and A. Vedaldi, “Mapnet: An allocentric spatial memory
    for mapping environments,” in *CVPR*, 2018, pp. 8476–8484.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] T. Jaksch, R. Ortner, and P. Auer, “Near-optimal regret bounds for reinforcement
    learning,” *JMLR*, vol. 11, no. 51, pp. 1563–1600, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] C. Jin, Z. Allen-Zhu, S. Bubeck, and M. I. Jordan, “Is q-learning provably
    efficient?” in *NeurIPS*, vol. 31, 2018, pp. 4863–4873.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] M. G. Azar, I. Osband, and R. Munos, “Minimax regret bounds for reinforcement
    learning,” in *ICML*, 2017, pp. 263–272.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] T. Chen, S. Gupta, and A. Gupta, “Learning exploration policies for navigation,”
    in *ICLR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] A. G. Barto and S. Mahadevan, “Recent advances in hierarchical reinforcement
    learning,” *Discrete Event Dynamic Systems*, vol. 13, no. 1, pp. 41–77, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] P. Dayan and G. E. Hinton, “Feudal reinforcement learning,” in *NeurIPS*,
    vol. 5, 1992, pp. 271–278.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] D. S. Chaplot, R. Salakhutdinov, A. Gupta, and S. Gupta, “Neural topological
    slam for visual navigation,” in *CVPR*, 2020, pp. 12 875–12 884.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] D. S. Chaplot, D. P. Gandhi, A. Gupta, and R. R. Salakhutdinov, “Object
    goal navigation using goal-oriented semantic exploration,” in *NeurIPS*, vol. 33,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” *NeurIPS*, vol. 27, pp. 3104–3112, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y.-F. Wang, W. Y.
    Wang, and L. Zhang, “Reinforced cross-modal matching and self-supervised imitation
    learning for vision-language navigation.” in *CVPR*, 2018, pp. 6629–6638.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] C.-Y. Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher, and C. Xiong,
    “Self-monitoring navigation agent via auxiliary progress estimation,” in *ICLR*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] C.-Y. Ma, Z. Wu, G. AlRegib, C. Xiong, and Z. Kira, “The regretful agent:
    Heuristic-aided navigation through progress estimation,” in *CVPR*, 2019, pp.
    6732–6740.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] L. Ke, X. Li, Y. Bisk, A. Holtzman, Z. Gan, J. Liu, J. Gao, Y. Choi,
    and S. Srinivasa, “Tactical rewind: Self-correction via backtracking in vision-and-language
    navigation,” in *CVPR*, 2019, pp. 6741–6749.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] P. Anderson, A. Shrivastava, D. Parikh, D. Batra, and S. Lee, “Chasing
    ghosts: Instruction following as bayesian state tracking,” in *NeurIPS*, 2019,
    pp. 371–381.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] R. Jonschkowski and O. Brock, “End-to-end learnable histogram filters,”
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    “Dropout: a simple way to prevent neural networks from overfitting,” *The JMLR*,
    vol. 15, no. 1, pp. 1929–1958, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *ICLR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] H. Tan, L. Yu, and M. Bansal, “Learning to navigate unseen environments:
    Back translation with environmental dropout,” in *NAACL*, 2019, pp. 2610–2621.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Y. Zhu, F. Zhu, Z. Zhan, B. Lin, J. Jiao, X. Chang, and X. Liang, “Vision-dialog
    navigation by exploring cross-modal memory,” in *CVPR*, 2020, pp. 10 730–10 739.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] T.-J. Fu, X. E. Wang, M. F. Peterson, S. T. Grafton, M. P. Eckstein,
    and W. Y. Wang, “Counterfactual vision-and-language navigation via adversarial
    path sampling,” in *ECCV*, 2019, pp. 71–86.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] J. Thomason, D. Gordon, and Y. Bisk, “Shifting the baseline: Single modality
    performance on visual navigation & qa,” in *NAACL*, 2019, pp. 1977–1983.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] H. Huang, V. Jain, H. Mehta, J. Baldridge, and E. Ie, “Multi-modal discriminative
    model for vision-and-language navigation,” in *RoboNLP*, 2019, pp. 40–49.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] H. Wang, Q. Wu, and C. Shen, “Soft expert reward learning for vision-and-language
    navigation,” in *ECCV*, 2020, pp. 126–141.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Q. Xia, X. Li, C. Li, Y. Bisk, Z. Sui, J. Gao, Y. Choi, and N. A. Smith,
    “Multi-view learning for vision-and-language navigation.” *arXiv preprint arXiv:2003.00857*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Y. Hong, C. Rodriguez, Y. Qi, Q. Wu, and S. Gould, “Language and visual
    entity relationship graph for agent navigation,” in *NeurIPS*, vol. 33, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] R. Hu, D. Fried, A. Rohrbach, D. Klein, T. Darrell, and K. Saenko, “Are
    you looking? grounding to multiple modalities in vision-and-language navigation,”
    in *ACL*, 2019, pp. 6551–6557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] B. Shi, X. Wang, P. Lyu, C. Yao, and X. Bai, “Robust scene text recognition
    with automatic rectification,” in *CVPR*, 2016, pp. 4168–4176.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] J. Devlin, M.-W. Chang, K. Lee, and K. N. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” in *NAACL*, 2018,
    pp. 4171–4186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] H. Tan and M. Bansal, “Lxmert: Learning cross-modality encoder representations
    from transformers,” in *EMNLP*, 2019, pp. 5099–5110.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] X. Li, C. Li, Q. Xia, Y. Bisk, A. Celikyilmaz, J. Gao, N. A. Smith, and
    Y. Choi, “Robust navigation with language pretraining and stochastic sampling,”
    in *EMNLP*, 2019, pp. 1494–1499.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] A. Majumdar, A. Shrivastava, S. Lee, P. Anderson, D. Parikh, and D. Batra,
    “Improving vision-and-language navigation with image-text pairs from the web,”
    in *ECCV*, 2020, pp. 259–274.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] W. Hao, C. Li, X. Li, L. Carin, and J. Gao, “Towards learning a generic
    agent for vision-and-language navigation via pre-training,” in *CVPR*, 2020, pp.
    13 137–13 146.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Y. Hong, Q. Wu, Y. Qi, C. Rodriguez-Opazo, and S. Gould, “Vln bert: A
    recurrent vision-and-language bert for navigation,” in *CVPR*, 2021, pp. 1643–1653.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] A. Graves, “Adaptive computation time for recurrent neural networks,”
    *arXiv preprint arXiv:1603.08983*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] A. Das, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, “Neural Modular
    Control for Embodied Question Answering,” in *CoRL*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] A. Anand, E. Belilovsky, K. Kastner, H. Larochelle, and A. C. Courville,
    “Blindfold baselines for embodied qa.” *arXiv: Computer Vision and Pattern Recognition*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Y. Wu, L. Jiang, and Y. Yang, “Revisiting embodiedqa: A simple baseline
    and beyond,” *IEEE Transactions on Image Processing*, vol. 29, pp. 3984–3992,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] E. Wijmans, S. Datta, O. Maksymets, A. Das, G. Gkioxari, S. Lee, I. Essa,
    D. Parikh, and D. Batra, “Embodied Question Answering in Photorealistic Environments
    with Point Cloud Perception,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] H. Luo, G. Lin, Z. Liu, F. Liu, Z. Tang, and Y. Yao, “Segeqa: Video segmentation
    based visual attention for embodied question answering,” in *ICCV*, 2019, pp.
    9667–9676.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox,
    “Flownet 2.0: Evolution of optical flow estimation with deep networks,” in *CVPR*,
    2017, pp. 2462–2470.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] J. Li, S. Tang, F. Wu, and Y. Zhuang, “Walking with mind: Mental imagery
    enhanced embodied qa,” in *ACM Multimedia*, 2019, pp. 1211–1219.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] S. Tan, W. Xiang, H. Liu, D. Guo, and F. Sun, “Multi-agent embodied question
    answering in interactive environments,” in *ECCV*, 2020, pp. 663–678.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Y. Ye, Z. Zhao, Y. Li, L. Chen, J. Xiao, and Y. Zhuang, “Video question
    answering via attribute-augmented attention network learning,” in *ACM*, 2017,
    pp. 829–832.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] C. Cangea, E. Belilovsky, P. Liò, and A. C. Courville, “Videonavqa: Bridging
    the gap between visual and embodied question answering.” in *ViGIL@NeurIPS*, 2019,
    p. 280.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Y. Deng, D. Guo, X. Guo, N. Zhang, H. Liu, and F. Sun, “Mqa: Answering
    the question via robotic manipulation,” in *RSS*, vol. 17, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] D. Nilsson, A. Pirinen, E. Gärtner, and C. Sminchisescu, “Embodied visual
    active learning for semantic segmentation.” in *AAAI*, 2020, pp. 2373–2383.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] H. R. Roman, Y. Bisk, J. Thomason, A. Celikyilmaz, and J. Gao, “Rmm:
    A recursive mental model for dialog navigation,” in *EMNLP*, 2020, pp. 1732–1745.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] J. Thomason, A. Padmakumar, J. Sinapov, N. Walker, Y. Jiang, H. Yedidsion,
    J. Hart, P. Stone, and R. J. Mooney, “Improving grounded natural language understanding
    through human-robot dialog,” in *ICRA*.   IEEE, 2019, pp. 6934–6941.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] S. Tellex, R. A. Knepper, A. Li, D. Rus, and N. Roy, “Asking for help
    using inverse semantics,” in *Robotics: Science and Systems 2014*, vol. 10, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] K. Nguyen, D. Dey, C. Brockett, and B. Dolan, “Vision-based navigation
    with language-based assistance via imitation learning with indirect intervention,”
    in *CVPR*, 2019, pp. 12 527–12 537.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] H. de Vries, K. Shuster, D. Batra, D. Parikh, J. Weston, and D. Kiela,
    “Talk the walk: Navigating new york city through grounded dialogue,” *arXiv: Artificial
    Intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] M. Hahn, J. Krantz, D. Batra, D. Parikh, J. M. Rehg, S. Lee, and P. Anderson,
    “Where are you? localization from embodied dialog,” in *EMNLP*, 2020, pp. 806–822.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] U. Jain, L. Weihs, E. Kolve, M. Rastegari, S. Lazebnik, A. Farhadi, A. G.
    Schwing, and A. Kembhavi, “Two body problem: Collaborative visual task completion,”
    in *CVPR*, 2019, pp. 6689–6699.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] A. Singh, T. Jain, and S. Sukhbaatar, “Learning when to communicate at
    scale in multiagent cooperative and competitive tasks,” in *ICLR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] M. Marge, S. Nogar, C. J. Hayes, S. M. Lukin, J. Bloecker, E. Holder,
    and C. R. Voss, “A research platform for multi-robot dialogue with humans,” in
    *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics (Demonstrations)*, 2019, pp. 132–137.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] S. Banerjee, J. Thomason, and J. J. Corso, “The robotslang benchmark:
    Dialog-guided robot localization and navigation.” *arXiv: Robotics*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] U. Muller, J. Ben, E. Cosatto, B. Flepp, and Y. L. Cun, “Off-road obstacle
    avoidance through end-to-end learning,” in *NeurIPS*.   Citeseer, 2006, pp. 739–746.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] R. Hadsell, P. Sermanet, J. Ben, A. Erkan, M. Scoffier, K. Kavukcuoglu,
    U. Muller, and Y. LeCun, “Learning long-range vision for autonomous off-road driving,”
    *Journal of Field Robotics*, vol. 26, no. 2, pp. 120–144, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] W. Zhang, K. Liu, W. Zhang, Y. Zhang, and J. Gu, “Deep neural networks
    for wireless localization in indoor and outdoor environments,” *Neurocomputing*,
    vol. 194, pp. 279–287, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] J. Zhang, J. T. Springenberg, J. Boedecker, and W. Burgard, “Deep reinforcement
    learning with successor features for navigation across similar environments,”
    in *IROS*, 2017, pp. 2371–2378.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie,
    A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis,
    “Human-level control through deep reinforcement learning,” *Nature*, vol. 518,
    no. 7540, pp. 529–533, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] G. Kahn, A. Villaflor, P. Abbeel, and S. Levine, “Composable action-conditioned
    predictors: Flexible off-policy learning for robot navigation,” in *CoRL*.   PMLR,
    2018, pp. 806–816.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] S. Ross, N. Melik-Barkhudarov, K. S. Shankar, A. Wendel, D. Dey, J. A.
    Bagnell, and M. Hebert, “Learning monocular reactive uav control in cluttered
    natural environments,” in *ICRA*.   IEEE, 2013, pp. 1765–1772.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] T. Manderson, J. C. G. Higuera, S. Wapnick, J.-F. Tremblay, F. Shkurti,
    D. Meger, and G. Dudek, “Vision-based goal-conditioned policies for underwater
    navigation in the presence of obstacles,” in *RSS*, vol. 16, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] J. Borenstein and Y. Koren, “Real-time obstacle avoidance for fast mobile
    robots in cluttered environments,” in *ICRA*.   IEEE, 1990, pp. 572–577.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] D. Ha and J. Schmidhuber, “World models,” *arXiv preprint arXiv:1803.10122*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] A. Francis, A. Faust, H.-T. L. Chiang, J. Hsu, J. C. Kew, M. Fiser, and
    T.-W. E. Lee, “Long-range indoor navigation with prm-rl,” *IEEE Transactions on
    Robotics*, vol. 36, no. 4, pp. 1115–1134, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] D. Shah, B. Eysenbach, G. Kahn, N. Rhinehart, and S. Levine, “Ving: Learning
    open-world navigation with visual goals,” *arXiv preprint arXiv:2012.09812*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] S. Thrun, “Probabilistic robotics,” *Communications of the ACM*, vol. 45,
    no. 3, pp. 52–57, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] S. M. LaValle, *Planning algorithms*.   Cambridge university press, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] A. J. Davison and D. W. Murray, “Mobile robot localisation using active
    vision,” in *ECCV*.   Springer, 1998, pp. 809–825.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] R. Sim and J. J. Little, “Autonomous vision-based exploration and mapping
    using hybrid maps and rao-blackwellised particle filters,” in *IROS*.   IEEE,
    2006, pp. 2082–2089.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] G. Berseth, D. Geng, C. M. Devin, N. Rhinehart, C. Finn, D. Jayaraman,
    and S. Levine, “Smirl: Surprise minimizing reinforcement learning in unstable
    environments,” in *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] A. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine,
    and C. Finn, “Learning to adapt in dynamic, real-world environments through meta-reinforcement
    learning,” in *ICLR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] K. Lee, Y. Seo, S. Lee, H. Lee, and J. Shin, “Context-aware dynamics
    model for generalization in model-based reinforcement learning,” in *ICML*, vol. 1,
    2020, pp. 5757–5766.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva,
    and D. Batra, “Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion
    frames,” in *Eighth ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] H. Wang, W. Wang, T. Shu, W. Liang, and J. Shen, “Active visual information
    gathering for vision-language navigation,” in *ECCV*, 2020, pp. 307–322.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] C. Finn and S. Levine, “Deep visual foresight for planning robot motion,”
    in *ICRA*.   IEEE, 2017, pp. 2786–2793.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] N. Malone and V. Kypraios, “Dd t07 - designing a situational judgment
    test to evaluate lll-structured problem solving in a virtual learning environment,”
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] S. K. Ramakrishnan, Z. Al-Halah, and K. Grauman, “Occupancy anticipation
    for efficient exploration and navigation,” in *ECCV*, 2020, pp. 400–418.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] K. Lobos-Tsunekawa, F. Leiva, and J. Ruiz-del-Solar, “Visual navigation
    for biped humanoid robots using deep reinforcement learning,” in *IEEE Robotics
    and Automation Letters*, vol. 3, no. 4, 2018, pp. 3247–3254.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] J. Bruce, N. Sünderhauf, P. Mirowski, R. Hadsell, and M. Milford, “One-shot
    reinforcement learning for robot navigation with interactive replay.” *arXiv preprint
    arXiv:1711.10137*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] M. Pfeiffer, S. Shukla, M. Turchetta, C. Cadena, A. Krause, R. Siegwart,
    and J. I. Nieto, “Reinforced imitation: Sample efficient deep reinforcement learning
    for mapless navigation by leveraging prior demonstrations,” in *IEEE Robotics
    and Automation Letters*, vol. 3, no. 4, 2018, pp. 4423–4430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick,
    R. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell, “Policy distillation,” in
    *ICLR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” in *ICML*, 2017, pp. 1126–1135.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] W. Zhang, Y. Zhang, and N. Liu, “Map-less navigation: a single drl-based
    controller for robots with varied dimensions,” *arXiv preprint arXiv:2002.06320*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] J. Li, X. Wang, S. Tang, H. Shi, F. Wu, Y. Zhuang, and W. Y. Wang, “Unsupervised
    reinforcement learning of transferable meta-skills for embodied navigation,” in
    *CVPR*, 2020, pp. 12 123–12 132.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] D. S. Chaplot, L. Lee, R. Salakhutdinov, D. Parikh, and D. Batra, “Embodied
    multimodal multitask learning,” in *IJCAI*, vol. 3, 2020, pp. 2442–2448.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] X. E. Wang, V. Jain, E. Ie, W. Y. Wang, Z. Kozareva, and S. Ravi, “Environment-agnostic
    multitask learning for natural language grounded navigation,” in *ECCV*, 2020,
    pp. 413–430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] B. Liu, L. Wang, and M. Liu, “Lifelong federated reinforcement learning:
    A learning architecture for navigation in cloud robotic systems,” in *IEEE Robotics
    and Automation Letters*, vol. 4, no. 4, 2019, pp. 4555–4562.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] D. Gordon, A. Kadian, D. Parikh, J. Hoffman, and D. Batra, “Splitnet:
    Sim2sim and task2task transfer for embodied visual navigation,” in *ICCV*, 2019,
    pp. 1022–1031.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, “Domain
    randomization for transferring deep neural networks from simulation to the real
    world,” in *IROS*.   IEEE, 2017, pp. 23–30.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] A. A. Rusu, M. Večerík, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell,
    “Sim-to-real robot learning from pixels with progressive nets,” in *CoRL*.   PMLR,
    2017, pp. 262–270.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] F. Sadeghi and S. Levine, “Cad2rl: Real single-image flight without a
    single real image,” in *RSS*, vol. 13, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] W. Yuan, K. Hang, D. Kragic, M. Y. Wang, and J. A. Stork, “End-to-end
    nonprehensile rearrangement with deep reinforcement learning and simulation-to-reality
    transfer,” *Robotics and Autonomous Systems*, vol. 119, pp. 119–134, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] L. Tai, G. Paolo, and M. Liu, “Virtual-to-real deep reinforcement learning:
    Continuous control of mobile robots for mapless navigation,” in *IROS*.   IEEE,
    2017, pp. 31–36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative
    domain adaptation,” in *CVPR*, 2017, pp. 7167–7176.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] F. Zhu, L. Zhu, and Y. Yang, “Sim-real joint reinforcement transfer for
    3d indoor navigation,” in *CVPR*, 2019, pp. 11 388–11 397.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] W. Zhao, J. P. Queralta, and T. Westerlund, “Sim-to-real transfer in
    deep reinforcement learning for robotics: a survey,” in *SSCI*, 2020, pp. 737–744.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] B. Qin, Y. Gao, and Y. Bai, “Sim-to-real: Six-legged robot control with
    deep reinforcement learning and curriculum learning,” in *ICRAE*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] H. F. Bassani, R. A. Delgado, J. N. de O. Lima Junior, H. R. Medeiros,
    P. H. M. Braga, and A. Tapp, “Learning to play soccer by reinforcement and applying
    sim-to-real to compete in the real world,” *CoRR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] B. Shen, F. Xia, C. Li, R. Martín-Martín, L. Fan, G. Wang, S. Buch, C. D’Arpino,
    S. Srivastava, L. P. Tchapmi, M. E. Tchapmi, K. Vainio, L. Fei-Fei, and S. Savarese,
    “igibson, a simulation environment for interactive tasks in large realistic scenes.”
    *arXiv preprint arXiv:2012.02924*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Y. Zhu, Y. Weng, F. Zhu, X. Liang, Q. Ye, Y. Lu, and J. jiao, “Self-motivated
    communication agent for real-world vision-dialog navigation,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] L. Jeni, Z. Istenes, P. Korondi, and H. Hashimoto, “Hierarchical reinforcement
    learning for robot navigation using the intelligent space concept,” in *2007 11th
    International Conference on Intelligent Engineering Systems*.   IEEE, 2007, pp.
    149–153.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] Z. Yang, Z. Dai, Y. Yang, J. G. Carbonell, R. Salakhutdinov, and Q. V.
    Le, “Xlnet: Generalized autoregressive pretraining for language understanding,”
    in *NeurIPS*, vol. 32, 2019, pp. 5753–5763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, “Biobert:
    a pre-trained biomedical language representation model for biomedical text mining.”
    *Bioinformatics*, vol. 36, no. 4, pp. 1234–1240, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] S. Levine and P. Abbeel, “Learning neural network policies with guided
    policy search under unknown dynamics,” in *NeurIPS*, vol. 27, 2014, pp. 1071–1079.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/ac4f7f759354922f4a1911a5471a0a93.png) | Fengda
    Zhu received the bachelor’s degree in School of Software Engineering from Beihang
    University, Beijing, China, in 2017\. He is currently pursuing the Ph.D. degree
    with the Faculty of Information Technology, Monash University under the supervision
    of Prof. Xiaojun Chang. His research interests include machine learning, deep
    learning and reinforcement learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/9dd2a220972e7f7575653f27538e17a3.png) | Yi Zhu
    received the B.S. degree in software engineering from Sun Yat-sen University,
    Guangzhou, China, in 2013\. Since 2015, she has been a Ph.D student in computer
    science with the School of Electronic, Electrical, and Communication Engineering,
    University of Chinese Academy of Sciences, Beijing, China. Her current research
    interests include object recognition, scene understanding, weakly supervised learning
    and visual reasoning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/234615cc3789528ac7f8116446fdc825.png) | Vincent
    Lee is currently an Associate Professor at Machine learning and Deep Learning
    Discipline of the Department of Data Science and artificial Intelligence, Faculty
    of IT, Monash University, Australia. He is a senior member of IEEE USA. He received
    Australia Federal Government scholarship to pursue PhD from 1988 through to 1991
    at The University of New Castle, NSW, in Australia. In 1973 to 1974, he was awarded
    a joint research scholarship by Ministry of Defence (Singapore) and Ministry of
    Defence (UK) for postgraduate study at Royal Air Force College, UK in aircraft
    electrical and instrument systems. He was a visiting academic to Tsinghua University
    in Beijing at the School of Economics and Management from Nov 2006 to March 2007;
    and was also a Visiting Professor to Information Communication Institute of Singapore
    from July 1994 through June 1995. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/f2314019aa8bc325d8c9781fa6137750.png) | Xiaodan
    Liang is currently an Associate Professor at Sun Yat-sen University. She was a
    postdoc researcher in the machine learning department at Carnegie Mellon University,
    working with Prof. Eric Xing, from 2016 to 2018\. She received her PhD degree
    from Sun Yat-sen University in 2016, advised by Liang Lin. She has published several
    cutting-edge projects on human-related analysis, including human parsing, pedestrian
    detection and instance segmentation, 2D/3D human pose estimation and activity
    recognition. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/0dd571195b6a48400f1f5040616c97bf.png) | Xiaojun
    Chang is currently an Associate Professor with School of Computing Technologies,
    RMIT University, Australia. Before joining RMIT, he was a Senior Lecturer with
    the Faculty of Information Technology, Monash University Clayton Campus, Clayton,
    VIC, Australia. He is also with the Monash University Centre for Data Science.
    He was a Post-Doctoral Research Associate with the School of Computer Science,
    Carnegie Mellon University, Pittsburgh, PA, USA, working with Prof. A. Hauptmann.
    He has spent most of the time working on exploring multiple signals (visual, acoustic,
    and textual) for automatic content analysis in unconstrained or surveillance videos.
    Dr. Chang is an ARC Discovery Early Career Researcher Award (DECRA) Fellow from
    2019 to 2021\. He has achieved top performance in various international competitions,
    such as TRECVID MED, TRECVID SIN, and TRECVID AVS. |'
  prefs: []
  type: TYPE_TB
