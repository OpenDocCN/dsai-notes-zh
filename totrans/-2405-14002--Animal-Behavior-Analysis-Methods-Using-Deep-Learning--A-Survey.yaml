- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:32:10'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2405.14002] Animal Behavior Analysis Methods Using Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14002](https://ar5iv.labs.arxiv.org/html/2405.14002)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Animal Behavior Analysis Methods Using Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Edoardo Fazzari [edoardo.fazzari@santannapisa.it](mailto:edoardo.fazzari@santannapisa.it)
    [0000-0002-4570-4170](https://orcid.org/0000-0002-4570-4170 "ORCID identifier")
    Sant’Anna School of Advanced StudiesPiazza Martiri della LibertàPisaItaly56127
    ,  Donato Romano Sant’Anna School of Advanced StudiesPiazza Martiri della LibertàPisaItaly56127
    [donato.romano@santannapisa.it](mailto:donato.romano@santannapisa.it) ,  Fabrizio
    Falchi Institute of Information Science and Technologies, National Research Council
    of ItalyVia G. MoruzziPisaItaly56124 Sant’Anna School of Advanced StudiesPiazza
    Martiri della LibertàPisaItaly56127 [fabrizio.falchi@cnr.it](mailto:fabrizio.falchi@cnr.it)
     and  Cesare Stefanini Sant’Anna School of Advanced StudiesPiazza Martiri della
    LibertàPisaItaly56127 [cesare.stefanini@santannapisa.it](mailto:cesare.stefanini@santannapisa.it)(2023)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Animal behavior serves as a reliable indicator of the adaptation of organisms
    to their environment and their overall well-being. Through rigorous observation
    of animal actions and interactions, researchers and observers can glean valuable
    insights into diverse facets of their lives, encompassing health, social dynamics,
    ecological relationships, and neuroethological dimensions. Although state-of-the-art
    deep learning models have demonstrated remarkable accuracy in classifying various
    forms of animal data, their adoption in animal behavior studies remains limited.
    This survey article endeavors to comprehensively explore deep learning architectures
    and strategies applied to the identification of animal behavior, spanning auditory,
    visual, and audiovisual methodologies. Furthermore, the manuscript scrutinizes
    extant animal behavior datasets, offering a detailed examination of the principal
    challenges confronting this research domain. The article culminates in a comprehensive
    discussion of key research directions within deep learning that hold potential
    for advancing the field of animal behavior studies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Animal Behavior, Deep Learning, Pose Estimation, Object Detection, Bio-acoustics,
    Machine Learning^†^†copyright: acmcopyright^†^†journalyear: 2023^†^†doi: XXXXXXX.XXXXXXX^†^†ccs:
    Computing methodologies Machine learning^†^†ccs: Applied computing Bioinformatics'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Animal behavior encompasses a spectrum of actions, reactions, and activity patterns
    demonstrated by animals in response to their environment, fellow organisms, and
    internal stimuli (Brown and de Bivort, [2017](#bib.bib30)). This expansive field
    covers a diverse range of behaviors, spanning from innate instincts and simple
    reflexes to intricate social interactions and learned conduct. The study of animal
    behavior involves the observation, description, and comprehension of how animals
    engage with one another and their surroundings. Presently, the landscape of animal
    behavior research is undergoing rapid evolution, propelled by the continual introduction
    of innovative experimental methodologies and the advancement of sophisticated
    behavior detection systems (Wang et al., [2021a](#bib.bib191)). This progression
    holds particular significance in advancing our understanding of neuroethological
    aspects, exemplified by the utilization of mice in exploring diseases like Alzheimer’s (Pedersen
    et al., [2006](#bib.bib150)), and in refining animal welfare practices within
    agriculture (Mishra and Sharma, [2023](#bib.bib129)). The impetus behind this
    surge in progress is the integration of cutting-edge technologies, with deep learning
    standing out as a transformative force that reshapes the approaches researchers
    employ to investigate and interpret animal behaviors(Brown and de Bivort, [2017](#bib.bib30)).
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning has emerged as a pivotal tool in the exploration of animal behavior.
    This advanced branch of artificial intelligence empowers computers to autonomously
    discern patterns and features from extensive datasets. As researchers amass increasingly
    intricate datasets through state-of-the-art monitoring technologies, including
    high-resolution cameras, GPS tracking devices, and sensors, the capability of
    deep learning algorithms to extract meaningful insights becomes indispensable (Benaissa
    et al., [2023](#bib.bib17); Koger et al., [2023](#bib.bib95)). This not only expedites
    the analysis process but also reveals nuanced aspects of animal behavior that
    were previously challenging to decipher. Furthermore, deep learning plays a crucial
    role in the development of sophisticated behavior detection systems. These systems
    can automatically recognize and classify various behaviors, allowing researchers
    to redirect their focus from laborious manual data annotation to the interpretation
    of results (Arablouei et al., [2023b](#bib.bib9)). This acceleration in data processing
    and behavior recognition enhances the scalability and efficiency of animal behavior
    studies, ushering in a new era of discovery and understanding in this dynamic
    field.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1\. Survey structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The structure of our survey is systematically delineated as follows: In the
    initial section, we expound upon the underlying motivation propelling the use
    of deep learning for examining animal behaviors, explaining the inherent limitations
    therein. Concurrently, we articulate the research questions that our survey endeavors
    to address. Subsequently, a comprehensive exposition of the methodological framework
    employed for conducting the survey is presented, which includes a discerning analysis
    of the gathered data to elucidate discernible trends within the research domain.
    The subsequent segmentation of the study into two distinct components is pivotal
    to the overarching architecture of this article. These segments, namely pose estimation
    and non-pose estimation-based methods, constitute the primary constituents of
    our investigation, elucidating the extraction of salient information pertinent
    to animal behavioral analysis and its subsequent application in behavior identification.
    Following this delineation, we furnish a compendium of publicly available datasets.
    In conclusion, we revisit the initially posited research questions, providing
    responses in light of the findings expounded within the two principal segments
    of the article.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. Contributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This survey article makes a threefold contribution to the current understanding
    of the study of animal behavior through deep learning:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide a thorough examination of existing technologies and algorithms employed
    in the analysis of animal behavior. This entails a detailed exploration of methodologies
    and approaches currently prevalent in this domain, providing readers with a nuanced
    understanding of the technological landscape.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We compile and present a comprehensive list of publicly available datasets relevant
    to the research field. This compilation serves as a valuable resource for researchers
    and practitioners, facilitating access to essential data for furthering investigations
    into animal behaviors through data-driven methodologies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We engage in a substantive discussion regarding potential directions for the
    evolution of the field. Emphasizing the integration of deep learning techniques,
    our discourse aims to enhance the quality of existing technologies, thereby advancing
    the understanding of animal behaviors. This forward-looking analysis provides
    insights into potential avenues for improvement and innovation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To the best of our knowledge, this survey is the only examination of the topic
    to date. The comprehensive overview, dataset compilation, and forward-looking
    discussions collectively contribute to a nuanced understanding of current advancements
    and lay the groundwork for future developments in the application of deep learning
    to the study of animal behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Motivation and problem statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we expound upon the foundational motivations that underscore
    the study of animal behavior through deep learning, articulating the diverse advantages
    and objectives inherent to this specialized research domain. Concurrently, we
    meticulously scrutinize the principal limitations that characterize this field,
    recognizing the nuanced challenges that arise from variations in research setups.
    Our intention is to furnish a comprehensive guide for prospective researchers,
    endowing them with a thorough understanding of potential impediments prior to
    initiating investigations within this domain. Concurrently, we underscore the
    myriad opportunities inherent in the study of animal behavior, fostering an appreciation
    for the intricate dimensions of this research frontier.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Summary of the limitations and objectives in studying animal behaviors.
    DL stands for Deep Learning
  prefs: []
  type: TYPE_NORMAL
- en: '| Limitations | Objectives |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sensor-induced stress | Biodiversity conservation |'
  prefs: []
  type: TYPE_TB
- en: '| Battery life | Biodiversity preservation |'
  prefs: []
  type: TYPE_TB
- en: '| Data noise | Ecological insight |'
  prefs: []
  type: TYPE_TB
- en: '| Storage constraints | Health impact |'
  prefs: []
  type: TYPE_TB
- en: '| Labeling economics | Welfare optimization |'
  prefs: []
  type: TYPE_TB
- en: '| Subjective annotation | Pest management |'
  prefs: []
  type: TYPE_TB
- en: '| Computational demands | Social dynamics |'
  prefs: []
  type: TYPE_TB
- en: '| Ethical considerations | Non-invasive (DL related) |'
  prefs: []
  type: TYPE_TB
- en: '| In-field tracking | Real-time applications (DL related) |'
  prefs: []
  type: TYPE_TB
- en: '| Environmental unpredictability |  |'
  prefs: []
  type: TYPE_TB
- en: 2.1\. Limitations in studying animal behaviors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The exploration of animal behavior is confronted by a multitude of challenges
    that intricately shape the effectiveness and practicality of its applications.
    These challenges permeate the methodologies employed for data acquisition, the
    intricacies of data analysis (both in terms of location and computational demands),
    and the nuanced process of data annotation.
  prefs: []
  type: TYPE_NORMAL
- en: An integral aspect of animal behavior studies is the utilization of sensors
    for data collection. However, the attachment of sensors to animals introduces
    a unique set of challenges. Notably, there is a risk of inducing stress responses
    and altering normal behaviors (Zhang et al., [2020](#bib.bib208)). This necessitates
    a profound reflection on the authenticity of the collected data, urging researchers
    to question whether stress-induced behaviors accurately mirror natural patterns.
    Moreover, the task of differentiating genuine behavioral signals from background
    noise in sensor data adds complexity to interpretation, emphasizing the requirement
    for advanced algorithms capable of discerning meaningful patterns amidst the noise (Kavlak
    et al., [2023](#bib.bib89)). Beyond stress responses and noise challenges, sensor
    equipment grapples with limitations in battery life (Mekruksavanich et al., [2022](#bib.bib127)),
    impacting the duration and scope of behavioral studies, especially in scenarios
    requiring continuous data collection over extended periods. Researchers are challenged
    to strike a balance between the need for comprehensive, continuous monitoring
    and the practical constraints imposed by limited battery capacities.
  prefs: []
  type: TYPE_NORMAL
- en: Transitioning into the domain of mobile devices introduces another layer of
    complexity to the challenges encountered in deep learning applications. The implementation
    of models on these devices confronts the persistent issue of storage limitations (Cao
    et al., [2020](#bib.bib34)). Balancing robust object detection with efficient
    data compression becomes a paramount concern, with techniques like Quantized-CNN (Wu
    et al., [2016](#bib.bib197)) attempting to address this challenge. However, the
    ongoing quest is to achieve this balance without compromising precision, a crucial
    consideration for the reliability of behavioral analyses.
  prefs: []
  type: TYPE_NORMAL
- en: A pivotal challenge arises in the realm of labeling and annotation, where economic
    and practical constraints hinder the tagging of large datasets for each animal (Bhattacharya
    and Shahnawaz, [2021](#bib.bib21)). This bottleneck impedes the scalability of
    deep learning models, heavily reliant on labeled datasets for effective training.
    The impracticality of manual labeling raises fundamental concerns about the breadth
    and accuracy of behavioral datasets, impacting the reliability of subsequent analyses.
    Subjectivity compounds these challenges, influencing the accuracy and consistency
    of behavioral annotations. Visual inspection, often subjective, is limited in
    providing objective insights into complex animal behaviors (Bernardes et al.,
    [2021](#bib.bib20)). Manual annotation, while traditional, is labor-intensive
    and susceptible to inter-annotator disagreements (Zhou et al., [2022](#bib.bib210);
    Segalin et al., [2021](#bib.bib173)). The inherent subjectivity introduces variability,
    raising questions about the replicability and reliability of experiments (Hou
    et al., [2020](#bib.bib75); Dell et al., [2014](#bib.bib48)).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, these challenges extend to innovative techniques, such as multi-view
    recordings, which hold promise for providing richer insights into animal behaviors (Jiang
    et al., [2021](#bib.bib82)). However, challenges arise in correlating social behaviors
    from different perspectives due to the lack of correspondence across data sources.
    Effectively coordinating information from multiple viewpoints demands inventive
    approaches to ensure accuracy and reliability, representing a frontier where deep
    learning methodologies can contribute significantly.
  prefs: []
  type: TYPE_NORMAL
- en: A major challenge surfaces when comparing laboratory studies, where challenges
    often revolve around the subjectivity introduced by the controlled environment,
    with ethological studies conducted in the wild presenting a distinct set of challenges,
    notably in-field tracking (Marshall et al., [2022](#bib.bib120)). The diverse
    and unpredictable environments encountered in the wild introduce complexities
    not found in controlled laboratory settings. Bridging the gap between these two
    disciplines necessitates adaptable detection and tracking algorithms that seamlessly
    operate in both environments. Robust algorithms capable of handling varying animal
    sizes, changing appearances, clutter, occlusions, and unpredictable environments
    are vital for extracting meaningful insights (Haalck et al., [2020](#bib.bib66);
    Hou et al., [2020](#bib.bib75); Lauer et al., [2022](#bib.bib99)). These challenges
    underscore the critical need for technological innovation that aligns with the
    demands of both controlled and wild settings.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Objectives in studying animal behaviors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Studying animal behavior offers manifold advantages, enriching our comprehension
    of the natural world and presenting practical applications across diverse domains,
    including neuroscience, pharmacology, medicine, agriculture, ecology, and robotics.
    Six key advantages of studying animal behavior are identified:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Biodiversity Conservation: Understanding animal behavior is crucial for the
    conservation of biodiversity (Hou et al., [2020](#bib.bib75); Nilsson et al.,
    [2020](#bib.bib140); Wijeyakulasuriya et al., [2020](#bib.bib193); Ditria et al.,
    [2020](#bib.bib49); Pillai et al., [2023](#bib.bib157)). Knowledge of behaviors
    such as migration patterns, feeding habits, and reproductive strategies is essential
    for designing effective conservation strategies and protecting endangered species.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ecological Understanding: Animal behavior provides insights into the ecological
    dynamics of ecosystems (Akçay et al., [2020](#bib.bib3); Gotanda et al., [2019](#bib.bib64)).
    Behavioral studies help researchers understand how animals interact with their
    environment, including their roles in nutrient cycling, seed dispersal, and predator-prey
    relationships (Nasiri et al., [2023](#bib.bib135); Chen et al., [2020](#bib.bib39);
    Yamada et al., [2020](#bib.bib203)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Human Health and Medicine: Studying animal behavior can have implications for
    human health and medicine (Shaw and Lahrman, [2023](#bib.bib176); Hart, [2011](#bib.bib69);
    Gnanasekar et al., [2022](#bib.bib61); Manduca et al., [2023](#bib.bib116)). For
    example, research on animal models helps in understanding certain diseases and
    developing potential treatments. Behavioral studies on animals also contribute
    to our understanding of the neurobiology and psychology that underlie human behavior (Mathis
    and Mathis, [2020](#bib.bib124); Coria-Avila et al., [2022](#bib.bib45); Saleh
    et al., [2023](#bib.bib169)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Animal Welfare and Husbandry: Knowledge of animal behavior is essential for
    promoting the welfare of domesticated animals and optimizing their husbandry practices (Jiang
    et al., [2021](#bib.bib82); Bao and Xie, [2022](#bib.bib14)). Understanding how
    animals express natural behaviors can inform the design of environments that support
    their physical and psychological well-being (Tassinari et al., [2021](#bib.bib181);
    Manoharan, [2020](#bib.bib118); Jiang et al., [2020](#bib.bib80)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (5)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pest Control and Agriculture: Although right now it is very limited to pest
    identification, understanding the behavior of pest species can aid in the development
    of effective pest control strategies in agriculture (Coulibaly et al., [2022](#bib.bib46);
    Júnior and Rieder, [2020](#bib.bib84); Mendoza et al., [2023](#bib.bib128)). This
    knowledge helps farmers manage crop damage and reduce the need for harmful pesticides (Teixeira
    et al., [2023](#bib.bib182); Mankin et al., [2021](#bib.bib117)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (6)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Understanding Social Dynamics: Observing social behaviors in animals can provide
    insights into the principles governing social structures and interactions (Xiao
    et al., [2023](#bib.bib198); Papaspyros et al., [2023](#bib.bib147)). This knowledge
    can have applications in fields such as sociology and psychology, contributing
    to our understanding of social dynamics in general (Alameer et al., [2022](#bib.bib6);
    Perez and Toler-Franklin, [2023](#bib.bib154); Landgraf et al., [2021](#bib.bib98)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this context, deep learning plays a pivotal role and emerges as a major technology
    in advancing the field, opening new opportunities. Addressing the limitations
    discussed in the previous section, we will now elaborate on the advantages that
    deep learning and computational technologies bring to the study of animal behavior.
  prefs: []
  type: TYPE_NORMAL
- en: We previously emphasized the integral aspect of employing sensors for data collection
    in animal behavior studies, which may induce stress and high noise levels. The
    use of multiple and diverse sensors for data acquisition, coupled with advanced
    architectures incorporating fusion layers, has been shown to mitigate noise and
    enhance the precision of analysis (Mahmud et al., [2021](#bib.bib115)). However,
    attaching sensors directly to animals may introduce bias, prompting researchers
    in livestock health assessment and neuroscience to adopt computer vision. The
    ability of computer vision to provide real-time, non-invasive, and accurate animal-level
    information through the use of cameras has gained popularity (Oliveira et al.,
    [2021](#bib.bib143)). Nevertheless, this approach is limited to setups within
    the camera frame, except for innovations like Haalck et al.’s (Haalck et al.,
    [2020](#bib.bib66)) moving camera that tracks animals, creating a dynamic map
    of their environment. In larger scenarios, such as meadows where cows graze, sensors
    remain preferable. Nevertheless, collecting and analyzing sensor data from mobile
    devices on animals proves challenging and time-consuming. To address this, Dang
    et al. (Dang et al., [2022](#bib.bib47)) introduced Long Range Area Network (LoRaWAN),
    where sensors attached to cows connect to gateways transmitting information to
    the cloud. This not only overcomes the limitations of computational power associated
    with mobile devices but also ensures continuous, real-time data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The efficacy of deep learning is contingent on annotated data, especially for
    supervised approaches. While manual annotation remains unavoidable, in tasks such
    as pose estimation and classification, labeling can be iterative. This involves
    annotating a small portion of the dataset, training a network, predicting on new
    images, correcting labels, and repeating this process multiple times. This iterative
    approach accelerates the labeling process, as demonstrated by Pereira et al. (Pereira
    et al., [2019](#bib.bib152)). Another approach is to generate artificial labels (Li
    and Lee, [2023](#bib.bib104)).
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 1](#S2.T1 "Table 1 ‣ 2\. Motivation and problem statement ‣ Animal Behavior
    Analysis Methods Using Deep Learning: A Survey") succinctly encapsulates a consolidated
    overview of both primary limitations and advantages, providing a discerning reference
    for researchers navigating the sophisticated landscape of animal behavior studies.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Research questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The survey aims to address the following research questions:'
  prefs: []
  type: TYPE_NORMAL
- en: RQ1 Which animal species are less considered and why?
  prefs: []
  type: TYPE_NORMAL
- en: This research question seeks to investigate overlooked or less studied animal
    species in the context of behavior analysis. To address this question, the survey
    will explore existing literature and research to identify trends and biases in
    the selection of animal subjects. The objective is to understand why certain species
    receive less attention and to gain insights into potential gaps in knowledge,
    aiding the development of more comprehensive and inclusive research strategies.
  prefs: []
  type: TYPE_NORMAL
- en: RQ2 What deep learning methods have been used in the literature for animal behavior
    analysis?
  prefs: []
  type: TYPE_NORMAL
- en: This research question focuses on summarizing and categorizing the existing
    deep learning methods employed in the literature for animal behavior analysis.
    The survey will review a wide range of studies to identify and classify the various
    deep learning techniques applied to analyze animal behavior. The objective is
    to analyze existing methodologies to identify trends, strengths, and limitations
    of current approaches in the field.
  prefs: []
  type: TYPE_NORMAL
- en: RQ3 What are the differences between human and animal behavior analysis?
  prefs: []
  type: TYPE_NORMAL
- en: This research question aims to highlight the distinctions between the analysis
    of human behavior and that of other animal species. The survey will compare methodologies,
    ethical considerations, and challenges specific to studying human and animal behavior.
  prefs: []
  type: TYPE_NORMAL
- en: RQ4 What are the deep learning strategies that are suitable and could enhance
    this task, but are not yet exploited?
  prefs: []
  type: TYPE_NORMAL
- en: This research question looks forward, aiming to identify untapped potential
    in the application of deep learning to animal behavior analysis. The survey will
    involve a comprehensive review of the current literature to identify gaps or areas
    where deep learning strategies have not been extensively explored. This involves
    proposing novel applications of existing techniques or suggesting modifications
    to adapt deep learning methods for more effective analysis of animal behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Method for literature survey
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we clarify the methodology applied in our survey. Our approach
    involved a thorough systematic review to carefully select the pertinent studies
    considered in this article. Following this, we accurately analyzed the gathered
    information, employing statistical methods to derive meaningful insights.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Search and selection strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section delineates the methodologies employed for data collection and
    synthesis. Initially, data acquisition was conducted through systematic searches
    on academic repositories, including Google Scholar, IEEE Xplore, and the Springer
    Database. The formulated search queries were as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: animal behavior AND deep learning
  prefs: []
  type: TYPE_NORMAL
- en: (insect OR wild) AND behavior AND deep learning
  prefs: []
  type: TYPE_NORMAL
- en: The decision to employ distinct queries for insects and wild animals was necessitated
    by the observed paucity of literature in these categories relative to studies
    involving farm animals and neuroethology, commonly focused on mice. Thus, the
    formulation of specific queries was imperative to encompass a broader spectrum
    of animal species.
  prefs: []
  type: TYPE_NORMAL
- en: 'Subsequently, the acquired data underwent systematic tabulation based on features
    explicated in [Table 2](#S3.T2 "Table 2 ‣ 3.1\. Search and selection strategies
    ‣ 3\. Method for literature survey ‣ Animal Behavior Analysis Methods Using Deep
    Learning: A Survey"). These features were derived from discerned patterns identified
    during a comprehensive analysis of the extant literature. Synthesizing the outcomes
    of this analysis, Sections [4](#S4 "4\. Pose estimation-based methods ‣ Animal
    Behavior Analysis Methods Using Deep Learning: A Survey") and [5](#S5 "5\. Non
    pose estimation-based methods ‣ Animal Behavior Analysis Methods Using Deep Learning:
    A Survey") encapsulate the aggregated findings, summarizing the respective papers
    that expound upon solution methodologies grounded in pose estimation and those
    that do not.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a judicious filtering operation was executed to extract only those
    articles germane to the objectives of this survey, resulting in 151 articles.
    Each article within this subset underwent thorough examination, and pertinent
    references therein were scrutinized and subsequently incorporated into our survey
    to enrich its content.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Features Extracted from Papers
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference: Assigned identifier for each retrieved article. |'
  prefs: []
  type: TYPE_TB
- en: '| Year: Publication year of the article. |'
  prefs: []
  type: TYPE_TB
- en: '| Country: Geographical location where the authors are based, as required in
    Section [3.2](#S3.SS2 "3.2\. Comprehensive science mapping analysis ‣ 3\. Method
    for literature survey ‣ Animal Behavior Analysis Methods Using Deep Learning:
    A Survey"). |'
  prefs: []
  type: TYPE_TB
- en: '| Species: The specific species under investigation in the article. |'
  prefs: []
  type: TYPE_TB
- en: '| Pose Estimation: Indicates whether the methodology incorporates pose estimation
    (True or False). |'
  prefs: []
  type: TYPE_TB
- en: '| Behavior Analysis: Indicates whether the analysis considers an association
    between extracted features and observed behaviors (True or False). |'
  prefs: []
  type: TYPE_TB
- en: '| Feature Methodology: The approach employed to extract salient features in
    the study. |'
  prefs: []
  type: TYPE_TB
- en: '| Behavior Methodology: The methodological framework used to correlate features
    with observed behaviors. |'
  prefs: []
  type: TYPE_TB
- en: '| Authors’ research field: Indicates the research fields the authors mainly
    work on. |'
  prefs: []
  type: TYPE_TB
- en: 3.2\. Comprehensive science mapping analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.2.1\. Annual scientific production
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the process of retrieving articles, our attention was exclusively directed
    towards research publications spanning the temporal spectrum from 2020 to 2023\.
    [Figure 1](#S3.F1 "Figure 1 ‣ 3.2.3\. Research field of authors ‣ 3.2\. Comprehensive
    science mapping analysis ‣ 3\. Method for literature survey ‣ Animal Behavior
    Analysis Methods Using Deep Learning: A Survey")a elucidates this distribution
    through a histogram, illustrating the quantitative representation of papers across
    each respective year.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Scientific production based on animal considered
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Figure 1](#S3.F1 "Figure 1 ‣ 3.2.3\. Research field of authors ‣ 3.2\. Comprehensive
    science mapping analysis ‣ 3\. Method for literature survey ‣ Animal Behavior
    Analysis Methods Using Deep Learning: A Survey")b illustrates the distribution
    of percentages pertaining to the various animal species under consideration in
    the selected articles. Evidently, a predominant emphasis is placed on research
    concerning livestock, notably focusing on cows and pigs, as well as studies involving
    mice, related mostly to neuroscience.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3\. Research field of authors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given that animal behavior analysis is an interdisciplinary and multidisciplinary
    field, it becomes imperative to comprehend the research background of individuals
    engaged in this domain. Despite the predominant focus on articles related to deep
    learning technologies, it is noteworthy that a considerable number of non-artificial
    intelligence practitioners are actively entering this field, as illustrated in
    [Figure 1](#S3.F1 "Figure 1 ‣ 3.2.3\. Research field of authors ‣ 3.2\. Comprehensive
    science mapping analysis ‣ 3\. Method for literature survey ‣ Animal Behavior
    Analysis Methods Using Deep Learning: A Survey")c. Interestingly, when combining
    ”computer science” (encompassing computer engineering) and ”artificial intelligence,”
    they constitute only 18% of the scholarly contributions. In contrast, bio-related
    fields, including biology, animal science, agriculture, veterinary, and ecology,
    collectively contribute 30% to the research landscape. Noteworthy is the active
    participation of various engineering fields, even those with a mechanical-electrical
    background, in the exploration of animal behavior. Additionally, a compelling
    correlation is observed in the fields of neuroscience and psychology, where the
    majority of articles are dedicated to the study of mice.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6954f48b7fa90e4c4f298b84f239a1fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. a) illustrates a histogram depicting the distribution of research
    articles per year, focusing exclusively on papers obtained and cataloged during
    the initial scavenging phase. b) presents a pie chart detailing the variety of
    animals utilized in behavioral studies leveraging deep learning techniques. c)
    displays another pie chart showcasing the diverse research fields of the authors.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Pose estimation-based methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pose estimation, the process of identifying and locating the position and orientation
    of objects, is a fundamental technique widely used in the examination of animal
    behaviors alongside object detection, as discussed in [subsection 5.3](#S5.SS3
    "5.3\. Object Detection ‣ 5\. Non pose estimation-based methods ‣ Animal Behavior
    Analysis Methods Using Deep Learning: A Survey"). Originating from Human Pose
    Estimation (HPE), the evolution into Animal Pose Estimation (APE) was spearheaded
    by Mathis et al. through DeepLabCut (Mathis et al., [2018](#bib.bib123)) and Pereira
    et al. via LEAP (Pereira et al., [2019](#bib.bib152)), subsequently evolving into
    SLEAP (Pereira et al., [2022](#bib.bib153)). This section delves into an in-depth
    analysis of these two methodologies juxtaposed with emerging trends within the
    field of research. Given the primary focus of our survey on animal behavior analysis,
    subsequent to the introduction of these predominant approaches, we elucidate the
    utilization of pose estimation outputs for behavior analysis and classification.
    For a more comprehensive understanding of animal pose estimation, we recommend
    perusing the survey conducted by Jiang et al. (Jiang et al., [2022a](#bib.bib79)),
    published in 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LEAP (Pereira et al., [2019](#bib.bib152)) is a single-animal pose estimation
    model employing convolutional layers culminating in confidence maps that delineate
    the probability distribution for each distinct body keypoint. This architectural
    design, depicted in [Figure 2](#S4.F2 "Figure 2 ‣ 4\. Pose estimation-based methods
    ‣ Animal Behavior Analysis Methods Using Deep Learning: A Survey"), is characterized
    by its simplicity, featuring three sets of convolutional layers. The initial two
    sets are terminated by max pooling to alleviate computational complexity. Subsequently,
    transposed convolution is applied to restore the original dimensions of the images,
    yet with a depth corresponding to the number of keypoints, thereby generating
    a confidence map for each. Despite its simplicity, the LEAP model encounters challenges
    in non-laboratory settings due to issues such as occlusion, prompting the introduction
    of T-LEAP (Russello et al., [2022](#bib.bib167)). T-LEAP preserves the architecture
    of LEAP but diverges in its use of 3D convolution instead of 2D convolution. The
    input to T-LEAP comprises four consecutive frames extracted from videos, enhancing
    the model’s robustness. Notably, T-LEAP maintains a focus on single-animal pose
    estimation, as elucidated in [Figure 2](#S4.F2 "Figure 2 ‣ 4\. Pose estimation-based
    methods ‣ Animal Behavior Analysis Methods Using Deep Learning: A Survey"). Subsequently,
    the author of LEAP introduced a refined version known as Social LEAP (SLEAP) (Pereira
    et al., [2022](#bib.bib153)), designed to proficiently address the challenges
    associated with multi-animal pose estimation through the integration of both bottom-up (Papandreou
    et al., [2018](#bib.bib146)) and top-down strategies (Nguyen and Kresovic, [2022](#bib.bib139)).
    In the top-down strategy, SLEAP first identifies individuals and subsequently
    detects their respective body parts. Unlike LEAP, SLEAP seamlessly incorporates
    this approach without the need for an additional object detection architecture.
    On the other hand, the bottom-up strategy in SLEAP involves detecting individual
    body parts and subsequently grouping them into individuals based on their connectivity.
    A key advantage of this dual-strategy framework is its efficiency, requiring only
    a single pass through the neural network. The output of this strategy produces
    multi-part confidence maps and part affinity fields (PAFs) (Cao et al., [2017](#bib.bib35)),
    constituting vector fields that intricately represent spatial relationships between
    pairs of body parts. Additionally, SLEAP undergoes a structural enhancement by
    transitioning from LEAP’s backbone to a more intricate U-Net architecture (Ronneberger
    et al., [2015](#bib.bib165)), thereby significantly improving accuracy in the
    realm of multi-animal pose estimation scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7d36183130e1f948fe954d35f9feb44d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. a) shows the architecture of LEAP (Pereira et al., [2019](#bib.bib152));
    b) the one exploited in T-LEAP (Russello et al., [2022](#bib.bib167))
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, DeepLabCut (DLC) (Mathis et al., [2018](#bib.bib123)) has evolved
    significantly over time. Initially designed as a single-animal pose estimation
    method, it utilizes a pretrained ResNet-50 (He et al., [2016](#bib.bib71)) backbone
    with subsequent deconvolutional layers to generate confidence maps for keypoints.
    This approach, taking advantage of Imagenet pretrained weights, allowed DLC to
    effectively estimate skeletons with minimal data. The model’s capabilities were
    later expanded to include 3D pose estimation through the use of multiple cameras (Nath
    et al., [2019](#bib.bib136)). Each camera view was trained independently, and
    sophisticated camera calibration techniques were employed to derive 3D locations.
    A subsequent milestone in DLC’s development involved addressing the challenges
    of multi-animal pose estimation (Lauer et al., [2022](#bib.bib99)). This evolution
    introduced DLCRNet, a structural modification that replaced the ResNet backbone.
    DLCRNet employs a bottom-up multi-animal pose estimation approach, featuring a
    multi-fusion architecture and a multi-stage decoder. The decoder utilizes multiple
    stages of score maps and PAFs (Cao et al., [2017](#bib.bib35)) to predict keypoints
    for each animal. Further innovation is exemplified by SuperAnimal (Ye et al.,
    [2022](#bib.bib206)), which introduced transformer layers into the model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'While DLC (Mathis et al., [2018](#bib.bib123)) and SLEAP (Pereira et al., [2022](#bib.bib153))
    currently stand as the predominant pose estimation methodologies in behavior analysis
    for animal behavior classification, it is imperative to acknowledge recent advancements
    in animal pose estimation architectures. Several notable methodologies have been
    introduced:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OptiFlex (Liu et al., [2020](#bib.bib111)) is a video-based animal pose estimation
    method that, given a skip ratio $s$ and a frame range $f$, assembles a sequence
    of $2f+1$ images with indices ranging from $t-s\times f$ to $t+s\times f$. This
    sequence is input to a model based on residual blocks with intermediate supervision,
    generating predictions for each image and producing a sequence of heatmap tensors.
    These tensors are then fed into an OpticalFlow model, ultimately yielding the
    final heatmap prediction for index $t$. OptiFlex has demonstrated superior accuracy
    compared to DeepLabCut (Mathis et al., [2018](#bib.bib123)), LEAP (Pereira et al.,
    [2019](#bib.bib152)), and DeepPoseKit (Graving et al., [2019](#bib.bib65)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SemiMultiPose (Blau et al., [2022](#bib.bib24)) introduces a semi-supervised
    multi-animal pose estimation approach, building upon DeepGraphPose (Wu et al.,
    [2020](#bib.bib196)) and DirectPose (Tian et al., [2019](#bib.bib184)). Taking
    both labeled and unlabeled frames as input, the method processes them using a
    ResNet (He et al., [2016](#bib.bib71)) backbone, generating a compact representation
    fed into three branches: one for detecting keypoint heatmaps (B1), one for bounding
    box heatmaps (B2), and a third for keypoint detection (B3). SemiMultiPose aims
    to generate pseudo keypoint coordinates from B2 and B3 for the self-supervised
    branch, contributing to B1\. The network has shown improved accuracy compared
    to SLEAP (Pereira et al., [2022](#bib.bib153)). However, the authors note that
    in cases of abundant labeled data, their method may not significantly outperform
    others, and for single-animal pose estimation with unlabeled frames from a sequential
    video, DeepGraphPose (Wu et al., [2020](#bib.bib196)) might outperform SemiMultiPose (Blau
    et al., [2022](#bib.bib24)), benefiting from the consideration of spatial and
    temporal information.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lightning Pose (Biderman et al., [2023](#bib.bib22)) exploits spatiotemporal
    statistics of unlabeled videos in two ways. Firstly, it introduces unsupervised
    training objectives penalizing the network for predictions violating the smoothness
    of physical motion, multiple-view geometry, or departing from a low-dimensional
    subspace of plausible body configurations. Secondly, it proposes a novel network
    architecture predicting poses for a given frame using temporal context from surrounding
    unlabeled frames. The resulting pose estimation networks exhibit superior performance
    with fewer labels, generalize effectively to unseen videos, and provide smoother
    and more reliable pose trajectories for downstream analysis (e.g., neural decoding
    analyses) compared to previously mentioned approaches.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhattacharya et al. (Bhattacharya and Shahnawaz, [2021](#bib.bib21)) introduced
    a novel model for recognizing the pose of multiple animals from unlabeled data.
    The approach involves the removal of background information from each image and
    the application of an edge detection algorithm to the body of the animal. Subsequently,
    the motion of the edge pixels is tracked, and agglomerative clustering is performed
    to segment body parts. In a departure from previous methods, the end result is
    not specific keypoints but rather the segmentation of body parts. To achieve this,
    the authors utilized contrastive learning to discourage the grouping of distant
    body parts together.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After obtaining the skeletal representation of each animal in every frame, whether
    from videos or images, the subsequent step involves processing the data to discern
    specific behaviors. The trajectories derived from pose estimation can be effectively
    analyzed through statistical methods. Weber et al. (Weber et al., [2022](#bib.bib192))
    utilized DeepLabCut predictions (Mathis et al., [2018](#bib.bib123)) and ANOVA (Kaufmann
    and Schering, [2014](#bib.bib88)) to conduct behavioral profiling of rodents,
    with a focus on studying stroke recovery. In a similar vein, Lee et al. (Lee et al.,
    [2021](#bib.bib102)), employing DLC (Mathis et al., [2018](#bib.bib123)), investigated
    the behavior of non-tethered fruit flies. Their study involved predicting locomotion
    patterns and identifying the centroid of the animals’ legs.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning for analyzing pose estimation trajectories becomes crucial
    when classifying postures and relating them to specific behaviors. One of the
    simplest approaches is to use a Nearest-Neighbor classifier. Saleh et al. (Saleh
    et al., [2023](#bib.bib169)) tested this method to classify mouse behaviors such
    as crossing and rearing in an open-field experiment, achieving 97% accuracy. Other
    machine learning approaches were employed by Fang et al. (Fang et al., [2021](#bib.bib54))
    and Nilsson et al. (Nilsson et al., [2020](#bib.bib140)). The former used a naive
    Bayesian classifier to identify eating, preening, resting, walking, standing,
    and running behaviors for poultry analysis, providing a disease warning system.
    The latter introduced the SimBa toolkit, importing DeepLabCut (Mathis et al.,
    [2018](#bib.bib123)) or DeepPoseKit (Graving et al., [2019](#bib.bib65)) projects
    to create classifiers using RandomForest (Breiman, [2001](#bib.bib29)) and extracting
    features like velocities and total movements. McKenzie-Smith et al. (McKenzie-Smith
    et al., [2023](#bib.bib126)) used trajectories obtained with SLEAP (Pereira et al.,
    [2022](#bib.bib153)) to identify stereotyped behaviors such as grooming, proboscis
    extension, and locomotion in Drosophila melanogaster, using resulting ethograms
    provided by MotionMapper (Berman et al., [2014](#bib.bib19)) to explore how flies’
    behavior varies across time of day and days, finding distinct circadian patterns
    in all stereotyped behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Other authors opted for recurrent and convolutional neural networks, with simple
    approaches such as using Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber,
    [1997](#bib.bib73)) and 1D convolutional neural networks to process trajectories
    for drawing behavioral conclusions. Examples include detecting lameness in horses
     (Alagele and Yildirim, [2022](#bib.bib5)) and determining chemical interactions
    experienced by crickets  (Fazzari et al., [2023](#bib.bib55)). More complex approaches
    include Wittek et al. (Wittek et al., [2023](#bib.bib194))’s use of InceptionTime (Ismail Fawaz
    et al., [2020](#bib.bib77)), an ensemble of deep convolutional neural network
    models, to classify seven distinct behaviors in birds. Some authors simplified
    the classification process by introducing a non-linear clustering phase to improve
    the feature space, followed by classification using Multilayer Perceptrons (MLP),
    demonstrating advantages in classification (Ye et al., [2022](#bib.bib206); Schneider
    et al., [2023](#bib.bib172)).
  prefs: []
  type: TYPE_NORMAL
- en: A recent emerging trend involves the utilization of unsupervised learning techniques
    in the analysis of animal behavior. Luxer et al. (Luxem et al., [2022](#bib.bib114))
    have innovatively proposed a methodology for processing trajectories derived from
    DeepLabCut (Mathis et al., [2018](#bib.bib123)) by employing a Variational Auto-Encoder
    (VAE) (Kingma and Welling, [2013](#bib.bib90)). Subsequently, they apply a Hidden
    Markov Model (HMM) (Rabiner and Juang, [1986](#bib.bib158)) to the new representation
    of trajectories to discern underlying motifs. Following a comprehensive analysis
    of motif usage, the authors iteratively employ HMM, limiting the number of motifs
    to those surpassing a 1% usage threshold in the previous analysis. The refined
    motifs were attributed to specific behavior exhibited by the mice, such as exploration,
    rearing, grooming, pausing, or walking. Notably, this methodological approach
    outperforms conventional techniques, such as Auto-Regressive HMM (AR-HMM) or MotionMapper (Berman
    et al., [2014](#bib.bib19)), when applied directly to the motion sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Motion trajectories extend their utility beyond predicting the behavior of
    individual animals; in multi-animal scenarios, they can also be applied to unravel
    the intricate web of social interactions among them. Segalin et al. (Segalin et al.,
    [2021](#bib.bib173)) introduced the Mouse Action Recognition System (MARS), a
    sophisticated automated pipeline tailored for pose estimation and behavior quantification
    in pairs of freely interacting mice. MARS adeptly discerns three specific social
    behaviors: close investigation, mounting, and attack. On a different note, Zhou
    et al. (Zhou et al., [2022](#bib.bib210)) proposed the Cross-Skeleton Interaction
    Graph Aggregation Network (CS-IGANet), a groundbreaking framework designed to
    capture the diverse dynamics of freely interacting mice. CS-IGANet successfully
    identifies a spectrum of behaviors, including approaching, attacking, chasing,
    copulation, walking away from another mouse, sniffing, and many others.'
  prefs: []
  type: TYPE_NORMAL
- en: Trajectories not only serve as a means to identify specific behaviors but are
    also instrumental in anomaly detection. For instance, Fujimori et al. (Fujimori
    et al., [2020](#bib.bib60)) employed OneClassSVM (Boser et al., [1992](#bib.bib27))
    and IsolationForest (Liu et al., [2008](#bib.bib110)) to detect outlier behaviors
    in domestic cats. Similarly, Gnanasekar et al. (Gnanasekar et al., [2022](#bib.bib61))
    utilized pose estimation data to predict abnormal behavior in mice undergoing
    opioid withdrawal, employing pretrained convolutional neural networks for the
    classification of shaking behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Non pose estimation-based methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we expound upon methodologies employed in the investigation
    of animal behaviors without recourse to pose estimation techniques. To enhance
    clarity and systematic presentation, we have delineated subsections corresponding
    to each methodology.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Sensor based approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sensor-generated data, typically originating from accelerometers or gyroscopes,
    has been extensively explored in the literature, as comprehensively in (Kleanthous
    et al., [2022a](#bib.bib92); Neethirajan, [2020](#bib.bib137)) surveys. These
    surveys delve into the application of classical machine learning methods in modern
    animal farming and the study of animal behavior. More recently, a shift towards
    leveraging deep learning approaches has been observed. Arablouei et al. (Arablouei
    et al., [2023b](#bib.bib9)) utilized a wearable collar tag equipped with an accelerometer
    to collect data from grazing beef cattle. They applied a Multi-Layer Perceptron
    to classify behaviors such as grazing, walking, ruminating, resting, and drinking.
    Similarly, Eerdekens et al. (Eerdekens et al., [2020](#bib.bib50)) employed tri-axial
    accelerometers on horses, strategically positioned at the two front legs’ lateral
    side. They proposed a Convolutional Neural Network to detect behaviors like standing,
    walking, trotting, cantering, rolling, pawing, and flank-watching based on the
    acquired data. Mekruksavanich et al. (Mekruksavanich et al., [2022](#bib.bib127)),
    instead, segmented accelerometer data into 2-second windows and exploited a pre-trained
    ResNet model (He et al., [2016](#bib.bib71)) to perform sheep activity recognition.
    Dang et al. (Dang et al., [2022](#bib.bib47)) introduced the integration of multiple
    sensors, collecting environmental data (e.g., temperature, humidity) alongside
    cow behavior information obtained from accelerometers and gyroscopes. They preprocessed
    this information using a 1D-convolutional neural network and LSTM networks for
    classifying walking, feeding, lying, and standing. In a recent study, Pan et al. (Pan
    et al., [2023](#bib.bib145)) introduced four novel Convolutional Neural Network
    architectures tailored for Animal Action Recognition (AAR). These architectures,
    namely one-channel temporal (OCT), one-channel spatial (OCS), OCT and spatial
    (OCTS), and two-channel temporal and spatial (TCTS) networks, leverage data from
    3D accelerometers and 3D gyroscopes. The core objective of their research was
    to mscrupulously identify behaviors such as movement, drinking, eating, nursing,
    sleeping, and lying in lactating sows.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to accelerometer and gyroscope data, GNSS (Global Navigation Satellite
    System) data emerges as a valuable tool for understanding animal behavior. Arablouei
    et al. (Arablouei et al., [2023a](#bib.bib10)) explored this avenue by employing
    GNSS to extract pertinent information about cattle behavior, including metrics
    like distance from water points, median speed, and median estimated horizontal
    position error. Integrating this GNSS data with accelerometry information, the
    researchers pursued two distinct approaches. The first involved concatenating
    features from both sensor datasets into a comprehensive feature vector, subsequently
    fed into a MLP classifier. Alternatively, the second approach centered on fusing
    the posterior probabilities predicted by two separate MLP classifiers. These methodologies
    enabled the accurate detection of behaviors such as grazing, walking, resting,
    and drinking.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Bioacoustics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While bioacoustics offers a captivating glimpse into animal behavior (Stowell,
    [2022](#bib.bib178)), given the integral role of sound in animal activities such
    as communication, mating, navigation, and territorial defense (Chalmers et al.,
    [2021](#bib.bib36)), the current landscape of published articles predominantly
    emphasizes animal identification (Xu et al., [2020](#bib.bib199); Bravo Sanchez
    et al., [2021](#bib.bib28); Varma et al., [2021](#bib.bib188)) and sound event
    detection (Nolasco et al., [2023](#bib.bib141); Moummad et al., [2023](#bib.bib134)).
    Notably, the existing literature reveals a scarcity of research endeavors combining
    acoustics and deep learning for the identification of animal behaviors. Wang et
    al. (Wang et al., [2021b](#bib.bib190)) stand out as pioneers in this domain,
    as they endeavored to classify sheep behaviors, including chewing, biting, chewing-biting,
    and ruminating sounds. This was accomplished using a recording device positioned
    proximal to the animal’s face, with a placebo class designated as noise. The acquired
    wavelet data were leveraged for classification tasks through both a feed-forward
    neural network and a recurrent neural network. Additionally, the information was
    further processed by transforming it into a log-scaled Mel-spectrogram, serving
    as input for a convolutional neural network. The findings underscore that while
    the recurrent neural network exhibited superior performance, the convolutional
    neural network outperformed the feed-forward approach, attributing its success
    to the enhanced signal representation offered by the Mel-spectrogram.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Object Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In conjunction with pose estimation techniques, object detection stands out
    as a widely employed deep learning methodology for analyzing animal behavior.
    Its prevalence may be attributed to its established utility in animal recognition
    and detection (Chen et al., [2021](#bib.bib38); Teixeira et al., [2023](#bib.bib182);
    Banerjee et al., [2023](#bib.bib13)), prompting researchers to redirect their
    focus toward studying animal welfare and activity.
  prefs: []
  type: TYPE_NORMAL
- en: Among the leading architectures for animal behavior identification, Faster R-CNN (Ren
    et al., [2015](#bib.bib161)) and particularly YOLO (Redmon et al., [2016](#bib.bib160))
    are frequently employed. Nonetheless, alternative architectures have been proposed.
    For instance, Samsudin et al. (Samsudin et al., [2022](#bib.bib170)) utilized
    SSD MobileNetv2 (Sandler et al., [2018](#bib.bib171)) to detect abnormal and normal
    zebrafish larvae behaviors for examining the effects of neurotoxins. McIntosh
    et al. (McIntosh et al., [2020](#bib.bib125)) introduced TempNet, incorporating
    an encoder bridge and residual blocks with a two-staged spatial-temporal encoder,
    to detect startle event in fish.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection serves a dual role, encompassing instantaneous behavior detection
    through image or video frame analysis, as well as the quantification and tracking
    of specific behaviors. The accurate analysis of single frames, counting, and frame-by-frame
    examination enable researchers to quantify both the duration and frequency of
    distinct actions. For instance, the application of YOLO in the study by Alameer
    et al. (Alameer et al., [2022](#bib.bib6)) facilitated the quantification of contact
    frequency among pigs, allowing the identification of peculiar behaviors such as
    rear snorting and tail-biting. In the context of cows and pigs, a crucial aspect
    involves quantifying movement and aggressive behavior (Alameer et al., [2022](#bib.bib6);
    Odo et al., [2023](#bib.bib142)). Furthermore, efforts to discern rank relationships
    based on fighting behavior in animals like cows are of crucial importance (Uchino
    and Ohwada, [2021](#bib.bib186)). Importantly, for tasks demanding prolonged animal
    identification, tracking is conventionally executed using DeepSort (Wojke et al.,
    [2017](#bib.bib195); Evangelista et al., [2022](#bib.bib52)).
  prefs: []
  type: TYPE_NORMAL
- en: Efficient instant detection can be accomplished by conducting a single analysis
    on the animal and directly classifying its behavior through a single image. In
    this context, deep learning object detection models prove instrumental in directly
    identifying behaviors such as positional activities (e.g., mating, standing, feeding,
    spreading, fighting, drinking) for the comprehensive analysis of animal health
    and stress behaviors (Riekert et al., [2020](#bib.bib162); Manoharan, [2020](#bib.bib118);
    Wang et al., [2020](#bib.bib189)). These models also find application in disease
    identification, such as the detection of wryneck (Elbarrany et al., [2023](#bib.bib51)),
    and in studying behavioral adaptations to new environments (Li et al., [2019b](#bib.bib106)).
    Furthermore, object detection models can be extended to operate with thermal and
    infrared images. For example, Xudong et al. (Xudong et al., [2020](#bib.bib202))
    utilized thermal images for the automatic recognition of dairy cow mastitis, introducing
    the EFMYOLOv3 model. Similarly, Lei et al. employed infrared images to discern
    feeding, resting, moving, and socializing behaviors in slow animals (Lei et al.,
    [2022](#bib.bib103)).
  prefs: []
  type: TYPE_NORMAL
- en: Beyond these applications, notable approaches utilizing object detection include
    Fuentes et al. (Fuentes et al., [2020](#bib.bib59)), who integrated YOLO and Optical
    Flow to detect actions in cows. Additionally, some researchers employ object detection
    solely for localizing the animal within the image or video. They subsequently
    crop that region and use it in other models, leveraging 2D pretrained networks
    or introducing 3D convolutional neural networks for video analysis (Feighelstein
    et al., [2023](#bib.bib58); Thanh and Netramai, [2022](#bib.bib183)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. Others
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Several research endeavors have employed unique deep learning methodologies,
    distinct from those discussed in the preceding section. Due to the relative scarcity
    of deep learning strategies for evaluating animal behavior in the existing literature
    using the aforementioned approaches, we endeavored to compile a comprehensive
    assortment of ideas. To achieve this, we have identified and categorized five
    distinct approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional Classification on Raw Data. Alameer et al. (Alameer et al., [2020](#bib.bib7))
    employed a GoogLeNet-like architecture(Szegedy et al., [2015](#bib.bib179)) to
    discern between feeding and non-nutritive visits to a manger in pig recordings.
    In a similar vein, Ayadi et al. (Ayadi et al., [2020](#bib.bib12)) utilized VGG19 (Simonyan
    and Zisserman, [2014](#bib.bib177)) to determine whether cows were ruminating
    or not. This network architecture was also applied to identify various behaviors
    in mice, such as grooming, licking the abdomen, squatting, resting, circling,
    wandering, climbing, and searching (Wang et al., [2021a](#bib.bib191)). Similarly
    leveraging pre-trained networks, Andresen et al. (Andresen et al., [2020](#bib.bib8))
    developed a fully automated system for surveilling post-surgical and post-anesthetic
    effects in mice facial expressions, employing InceptionV3 (Szegedy et al., [2016](#bib.bib180))
    for pain identification. Notably, Bohnslav et al. (Bohnslav et al., [2021](#bib.bib26))
    introduced DeepEthogram, a software tested for predicting mice and flies behaviors.
    The approach involves using a sequence of 11 frames, where the last frame is the
    target for prediction. Optical flow frames are generated using MotionNet (Zhu
    et al., [2019](#bib.bib211)). These frames, along with the target frame, are fed
    into a feature extractor (ResNet architectures (He et al., [2016](#bib.bib71);
    Hara et al., [2018](#bib.bib68))) to extract both flow and spatial features. Subsequently,
    the features are concatenated, and Temporal Guassian Mixture (TGM) model (Piergiovanni
    and Ryoo, [2019](#bib.bib156)) is applied for classification. Han et al. (Han
    et al., [2020](#bib.bib67)) employed a simpler approach, superimposing the frame
    to be predicted with computationally generated optical flow from the subsequent
    frame. The resulting image is then classified using a convolutional neural network
    to categorize behaviors in fish shoals, including normal state, group stimulated,
    individual disturbed, feeding, anoxic, and starvation state.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Segmentation. Xiao et al.(Xiao et al., [2023](#bib.bib198)) employed Mask R-CNN (He
    et al., [2017](#bib.bib70)) to segment birds within a 3D space, facilitating the
    analysis of their interactions based on distinct social actions: approach, stay,
    leave, and sing to. In contrast, other researchers have devised innovative pipelines
    to investigate animal behavior. EthoFlow(Bernardes et al., [2021](#bib.bib20))
    is a software grounded in segmentation, enabling the tracking and behavioral analysis
    of organisms (validated on bee datasets). On a different note, SIPEC (Marks et al.,
    [2022](#bib.bib119)) constitutes a pipeline leveraging an Xception network (Chollet,
    [2017](#bib.bib43)) to extract features from frames. These features are subsequently
    processed over time using a Temporal Convolutional Network (TCN) (Lea et al.,
    [2016](#bib.bib100)) to classify the animal’s behavior in each frame. While SIPEC
    abstains from segmentation in the case of single animal classification, it seamlessly
    incorporates segmentation for multi-animal behavior classification.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-Supervised Learning. Jia et al. (Jia et al., [2022](#bib.bib78)) proposed
    an innovative self-supervised learning approach known as Selfee, designed for
    extracting comprehensive and discriminative features directly from raw video recordings
    of animal behaviors. Selfee utilizes a pair of Siamese convolutional neural networks(Koch
    et al., [2015](#bib.bib94)), trained explicitly to generate discriminative representations
    for live frames.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explainability. To the best of our knowledge, Choi et al. (Choi et al., [2022](#bib.bib41))
    stands as the sole contributor employing Explainable Artificial Intelligence (XAI).
    In their study, they harnessed Grad-CAM (Selvaraju et al., [2016](#bib.bib174))
    to delve into the decision-making process of a neural network designed to distinguish
    between unstable and stable ant swarms. The investigation aimed to ascertain the
    network’s capacity to comprehend intricate behaviors such as dueling and dominance
    biting, shedding light on the explainability of its predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Behavior identification in clips. Li et al. (Li et al., [2020](#bib.bib105))
    undertook the task of categorizing significant pig behaviors, such as feeding,
    lying, motoring, scratching, and mounting. Their approach involved the development
    of Pig’s Multi-Behavior recognition (PMB-SCN), a sophisticated architecture built
    upon the SlowFast framework (Feichtenhofer et al., [2019](#bib.bib57)) and leveraging
    spatio-temporal convolution. PMB-SCN comprised two distinct SlowFast pathways
    with varying temporal speeds. The slow pathway utilized a larger temporal stride
    when processing input frames (e.g., 8, considering a clip with a length of 64
    frames), while the fast pathway employed a smaller temporal stride (e.g., 2).
    The features extracted by these pathways were interconnected through lateral connections (Lin
    et al., [2017](#bib.bib109)), enhancing the model’s ability to capture complex
    spatio-temporal patterns. The final phase of the methodology involved classification,
    where the fused features were utilized to discern and categorize various pig behaviors
    effectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6\. Publicly available datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section meticulously enumerates publicly accessible datasets featured
    or referenced in the articles identified through our systematic search. [Table 3](#S6.T3
    "Table 3 ‣ 6\. Publicly available datasets ‣ Animal Behavior Analysis Methods
    Using Deep Learning: A Survey") presents details on each dataset, including the
    article of introduction, authorship, targeted species, data type (e.g., images,
    videos, audio signals, sensor data), the specific tasks for which they were utilized
    and the content of the datasets. Noteworthy is the incorporation of references
    indicating the dual usage of datasets—initially introduced for a specific task
    and subsequently repurposed, signified by citations in the Application column.
    Regrettably, several articles utilized private datasets, although some authors
    may offer dataset-sharing options. We recommend consulting the corresponding articles
    to explore potential data access avenues.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A number of key considerations can be gleaned from Table [3](#S6.T3 "Table
    3 ‣ 6\. Publicly available datasets ‣ Animal Behavior Analysis Methods Using Deep
    Learning: A Survey"), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of datasets tailored for pose estimation, a salient observation
    is the standardization of skeletal structures when deployed across diverse animal
    species (Cao et al., [2019](#bib.bib33); Yu et al., [2021](#bib.bib207); Ng et al.,
    [2022](#bib.bib138)). This standardization facilitates the training of a singular
    network, avoiding the need for species-specific networks. Conversely, datasets
    exclusive to individual animal species exhibit intricate skeletal configurations
    tailored to the anatomical nuances of that species. For instance, precision in
    detecting the distal and proximal ends of crickets’ antennae may be achievable (Fazzari
    et al., [2022](#bib.bib56)), but such granularity may not translate to not insect
    species like horses.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inclination of animals like goats and birds to move in open fields compels
    researchers to rely on sensor data or alternative methods, such as audio recordings,
    for event and action detection. This approach is significantly more manageable
    than tracking the animals with cameras. However, the utilization of sensors is
    constrained by the availability and affordability of these devices, directly impacting
    the number of individuals involved and the quantity of sequences that can be compiled
    for the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For identifying static positions, such as whether an animal is lying down or
    standing, still images suffice. However, capturing and analyzing videos, or more
    precisely, short video clips, is crucial for recognizing dynamic actions and behaviors.
    These clips are intentionally brief to focus solely on the relevant action event,
    ensuring accurate classification using deep learning techniques. This approach
    effectively eliminates extraneous or unrelated behaviors that may interfere with
    the identification of the specific behavioral instance. This is the rationale
    behind Yang et al.’s decision to utilize 15 frames for each video clip (Yang et al.,
    [2022](#bib.bib205)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, publicly available collective and social behavior prediction
    and analysis datasets are currently limited to mice and fish, even though we discussed
    a study in this survey that utilized explainable artificial intelligence to analyze
    ant behavior. This innovative research methodology relies heavily on video data,
    presenting a computational challenge that demands substantial processing efforts.
    The intricate nature of this approach necessitates a considerable investment of
    time for meticulous frame and event labeling, thereby slightly diminishing its
    overall research appeal and popularity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An essential consideration pertains to the primary focus of many databases,
    which primarily aim at identification, detection, pose estimation, and tracking.
    Despite this orientation, it is crucial to acknowledge that several datasets have
    been instrumental in behavioral analysis, even if not explicitly designed for
    such purposes. Researchers are strongly encouraged not to overlook animal datasets
    merely because they do not pertain to specific behaviors. Valuable insights can
    be gleaned from these datasets, and their broader applicability should be explored
    beyond their initially intended scope.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Table 3\. Datasets Information useful for Animal Behavior Analysis
  prefs: []
  type: TYPE_NORMAL
- en: '| Species | Introduced by | Type | Applications | Dimensions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Birds | Akçay (Akçay et al., [2020](#bib.bib3)) | Images | Detection | 3436
    images |'
  prefs: []
  type: TYPE_TB
- en: '| Birds | Morfi et al. (Morfi et al., [2019](#bib.bib132)) | Audio | Recognition (Bravo Sanchez
    et al., [2021](#bib.bib28)) | 687 recording, 87 classes |'
  prefs: []
  type: TYPE_TB
- en: '| Birds | Knight et al. (Knight and Bayne, [2019](#bib.bib93)) | Audio | Event
    detection (Lostanlen et al., [2019](#bib.bib113)) | 64 recording, 5 classes |'
  prefs: []
  type: TYPE_TB
- en: '| Birds | Shamoun-Baranes et al. (Shamoun-Baranes et al., [2017](#bib.bib175))
    | Sensor data | Movement prediction (Wijeyakulasuriya et al., [2020](#bib.bib193))
    | 19 sequences |'
  prefs: []
  type: TYPE_TB
- en: '| Cattle | Arablouei et al. (Arablouei et al., [2023a](#bib.bib10)) | Sensor
    data | Behavior classification | 11962 labeled datapoints (arm20c), 10879 labeled
    datapoints (arm20e) |'
  prefs: []
  type: TYPE_TB
- en: '| Dogs | Barnard et al. (Barnard et al., [2016](#bib.bib15)) | Images | Pose
    Estimation | 22479 images |'
  prefs: []
  type: TYPE_TB
- en: '| Goats | Kamminga et al. (Kamminga et al., [2018](#bib.bib85)) | Sensor data
    | Action recognition (also  (Bocaj et al., [2020](#bib.bib25))) | 177.8 hours
    of sequence data, 5 individuals |'
  prefs: []
  type: TYPE_TB
- en: '| Goats | Kleanthous et al. (Kleanthous et al., [2022b](#bib.bib91)) | Sensor
    data | Action recognition (also  (Bocaj et al., [2020](#bib.bib25); Mekruksavanich
    et al., [2022](#bib.bib127))) | 2 sequences |'
  prefs: []
  type: TYPE_TB
- en: '| Horses | Kamminga et al. (Kamminga et al., [2019](#bib.bib86)) | Videos |
    Pose estimation, Action recognition (Bocaj et al., [2020](#bib.bib25)) | 8144
    frames |'
  prefs: []
  type: TYPE_TB
- en: '| Horses | Mathis et al. (Mathis et al., [2021](#bib.bib122)) | Images | Pose
    Estimation | 608550 images |'
  prefs: []
  type: TYPE_TB
- en: '| Fish | Mathis et al. (Mathis et al., [2018](#bib.bib123)) | Videos | Pose
    estimation | 100 frames |'
  prefs: []
  type: TYPE_TB
- en: '| Fish | McIntosh et al. (McIntosh et al., [2020](#bib.bib125)) | Videos |
    Behavior classification | 892 clips |'
  prefs: []
  type: TYPE_TB
- en: '| Fish | Papaspyros et al. (Papaspyros et al., [2023](#bib.bib147)) | Videos
    | Collective Behavior Prediction | three 16-hour trajectory datasets |'
  prefs: []
  type: TYPE_TB
- en: '| Fish | Rahman et al. (Rahman et al., [2014](#bib.bib159)) | Videos | Action
    recognition (Gore et al., [2023](#bib.bib63)) | 95 clips |'
  prefs: []
  type: TYPE_TB
- en: '| Fish | Tucker et al. (Tucker Edmister et al., [2022](#bib.bib185)) | Videos
    | Tracking, Chemical response analysis (Gore et al., [2023](#bib.bib63)) | 3 hours
    for each of the 384 individuals |'
  prefs: []
  type: TYPE_TB
- en: '| Insects | Bjerge et al. (Bjerge et al., [2023](#bib.bib23)) | Images | Detection
    | 29960 images |'
  prefs: []
  type: TYPE_TB
- en: '| Insects | Fazzari et al. (Fazzari et al., [2023](#bib.bib55)) | Videos |
    Tracking, Chemical response analysis | 5 minutes clip for 69 individuals |'
  prefs: []
  type: TYPE_TB
- en: '| Insects | Modlmeier et al. (Modlmeier et al., [2019](#bib.bib130)) | Images
    | Movement prediction (Wijeyakulasuriya et al., [2020](#bib.bib193)) | 14400 frames
    (4 hours) |'
  prefs: []
  type: TYPE_TB
- en: '| Insects | Pereira et al. (Pereira et al., [2022](#bib.bib153)) | Videos |
    Pose estimation | 30 videos, 2000 labels |'
  prefs: []
  type: TYPE_TB
- en: '| Insects | Pham et al. (Pham, [2022](#bib.bib155)) | Sequences | Locomotion
    classification | 258 traces |'
  prefs: []
  type: TYPE_TB
- en: '| Insects | Ullah et al. (Ullah et al., [2022](#bib.bib187)) | Images | Recognition
    | 1686 images |'
  prefs: []
  type: TYPE_TB
- en: '| Jellyfish | Martin et al. (Martin-Abadal et al., [2020](#bib.bib121)) | Images
    | Detection | 842 images |'
  prefs: []
  type: TYPE_TB
- en: '| Mice | Burgos et al. (Burgos-Artizzu et al., [2012](#bib.bib32)) | Videos
    | Pose estimation, Social behavior analysis (Jiang et al., [2021](#bib.bib82))
    | 237 videos and over 8M frames |'
  prefs: []
  type: TYPE_TB
- en: '| Mice | Jiang et al. (Jiang et al., [2021](#bib.bib82)) | Videos | Social
    behavior analysis | 12*3 annotated videos, 216,000*3 frames in total |'
  prefs: []
  type: TYPE_TB
- en: '| Mice | Jiang et al. (Jiang et al., [2022b](#bib.bib81)) | Videos | Detection,
    Tracking | 10 videos, each video lasts 3 min, 4000 frames annotated |'
  prefs: []
  type: TYPE_TB
- en: '| Mice | Mathis et al. (Mathis et al., [2018](#bib.bib123)) | Videos | Pose
    estimation | 161 frames |'
  prefs: []
  type: TYPE_TB
- en: '| Mice | Pereira et al. (Pereira et al., [2022](#bib.bib153)) | Videos | Pose
    estimation | 1000 frames, 2950 instances & 1474 frames, 2948 instances |'
  prefs: []
  type: TYPE_TB
- en: '| Mice | Segalin et al. (Segalin et al., [2021](#bib.bib173)) | Videos | Pose
    estimation, Behavior classification | 3.3M labels for pose annotation, 14 hours
    of behavior annotation |'
  prefs: []
  type: TYPE_TB
- en: '| Multiple, 5 | Cao et al. (Cao et al., [2019](#bib.bib33)) | Images | Pose
    Estimation | 4666 images |'
  prefs: []
  type: TYPE_TB
- en: '| Multiple, 30 | Yang et al. (Yang et al., [2022](#bib.bib205)) | Images/Videos
    | Pose Estimation | 2.4K video clips with 15 frames for each video, resulting
    in 36K frames |'
  prefs: []
  type: TYPE_TB
- en: '| Multiple, 54 | Yu et al. (Yu et al., [2021](#bib.bib207)) | Images | Pose
    Estimation | 10015 images |'
  prefs: []
  type: TYPE_TB
- en: '| Multiple, 850 | Ng et al. (Ng et al., [2022](#bib.bib138)) | Images/Videos
    | Pose Estimation, Action recognition | 50 hours of annotated videos, 30K video
    sequences for AAR, 33K frames for APE |'
  prefs: []
  type: TYPE_TB
- en: '| Pigs | Rielert et al. (Riekert et al., [2020](#bib.bib162)) | Images | Position
    classification | 7277 images |'
  prefs: []
  type: TYPE_TB
- en: '| Tigers | Li et al. (Li et al., [2019a](#bib.bib107)) | Images | Pose estimation
    | 8076 images |'
  prefs: []
  type: TYPE_TB
- en: '| Monkeys | Labuguen et al. (Labuguen et al., [2021](#bib.bib97)) | Images
    | Pose estimation | 13083 images |'
  prefs: []
  type: TYPE_TB
- en: 7\. Discussion about research questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In concluding our extensive survey, we undertake the task of responding to
    the research questions outlined in [subsection 2.3](#S2.SS3 "2.3\. Research questions
    ‣ 2\. Motivation and problem statement ‣ Animal Behavior Analysis Methods Using
    Deep Learning: A Survey"), drawing upon the insights gleaned from the studies
    expounded upon earlier. These responses aim to offer readers practical guidance
    in navigating the dynamic trends discerned from the comprehensive examination
    of the state-of-the-art literature. Their purpose is to serve as a compass for
    readers, facilitating a deeper comprehension of emerging patterns and fostering
    the implementation of advancements in the field of animal behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ1 (Which animal species are less considered and why?): In the context of
    animal behavior analysis employing deep learning, farm animals take center stage,
    as illustrated in [Figure 1](#S3.F1 "Figure 1 ‣ 3.2.3\. Research field of authors
    ‣ 3.2\. Comprehensive science mapping analysis ‣ 3\. Method for literature survey
    ‣ Animal Behavior Analysis Methods Using Deep Learning: A Survey"). Pigs and cows
    emerge as prominent subjects, while mice claim a noteworthy position owing to
    their significance in neuroscience research (Bryda, [2013](#bib.bib31)). Despite
    chickens being the most widely farmed animals globally, surpassing even cows,
    sheep, and goats (Robinson et al., [2014](#bib.bib163)), their consideration in
    behavior analysis appears relatively subdued. This discrepancy may be attributed
    to the limited nature of chicken behavior, encompassing mostly activities such
    as sleeping, moving, and eating. They are predominantly analyzed for welfare-related
    assessments (Mohialdin et al., [2023](#bib.bib131); Joo et al., [2022](#bib.bib83)).'
  prefs: []
  type: TYPE_NORMAL
- en: Goats and sheep, less scrutinized in behavioral studies, likely owe their lower
    visibility to their outdoor grazing habits, which hinder the feasibility of employing
    image processing techniques. Unlike pigs, which are often studied within confined
    spaces, the vast open areas in which goats and sheep are typically raised limit
    the practicality of utilizing image processing, even with occasional drone deployment (Al-Thani
    et al., [2020](#bib.bib4)). Birds face a similar challenge, requiring continuous
    tracking or data collection from sensors for comprehensive analysis (Bergen et al.,
    [2022](#bib.bib18)).
  prefs: []
  type: TYPE_NORMAL
- en: For aquatic creatures like fishes, a distinct hurdle arises from the difficulty
    in training neural networks on underwater images. These images often suffer from
    poor quality due to distortion and color/contrast loss in water, necessitating
    an image enhancement phase for meaningful analysis (Saleh et al., [2022](#bib.bib168)).
  prefs: []
  type: TYPE_NORMAL
- en: A notable observation is the limited consideration given to domestic animals
    in this research context (Choi et al., [2021](#bib.bib42); Lecomte et al., [2021](#bib.bib101);
    Chambers et al., [2021](#bib.bib37); Kasnesis et al., [2022](#bib.bib87)). This
    may stem from the scarcity of veterinary professionals engaged in this evolving
    field or ethical concerns surrounding the study of domestic animals.
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ2 (What deep learning methods have been used in the literature for animal
    behavior analysis?): Throughout this survey, we delineate two distinct methodologies
    employed for the analysis of animal behaviors: pose estimation and non-pose estimation
    methods. Pose estimation-based approaches hinge on the analysis of trajectories
    traced by keypoints, with subsequent utilization of various machine and deep learning
    techniques to scrutinize behavior. Predominantly, recurrent neural networks and
    1D convolutional neural networks are the favored deep learning methods, though
    recent applications have also embraced variational auto-encoders for unsupervised
    motif identification (Kingma and Welling, [2013](#bib.bib90); Luxem et al., [2022](#bib.bib114))
    and convolutional graph networks for interaction analysis (Zhou et al., [2022](#bib.bib210)).
    However, a prevalent trend emerges wherein data is often processed through statistical
    analysis or traditional machine learning methods (Saleh et al., [2023](#bib.bib169);
    Fang et al., [2021](#bib.bib54); Nilsson et al., [2020](#bib.bib140); McKenzie-Smith
    et al., [2023](#bib.bib126)). This inclination may stem from the fact that many
    researchers are not inherently engaged in artificial intelligence or data science,
    as mentioned earlier (see [subsection 3.2](#S3.SS2 "3.2\. Comprehensive science
    mapping analysis ‣ 3\. Method for literature survey ‣ Animal Behavior Analysis
    Methods Using Deep Learning: A Survey")), or it could be influenced by the volume
    of available data, given the heightened data requirements of deep learning (Özdaş
    et al., [2023](#bib.bib144)). Despite the migration of classification tasks and
    behavior discovery to deep learning, certain aspects, such as behavior outlier
    detection, persist in employing classical machine learning approaches like OneClassSVM (Boser
    et al., [1992](#bib.bib27)) and IsolationForest (Liu et al., [2008](#bib.bib110)).
    On the other hand, non-pose estimation encompasses diverse applications, categorized
    based on the type of data: sensors, audio and video, and image data. Sensor data
    commonly undergoes processing through (1D)-convolutional or recurrent neural networks (Eerdekens
    et al., [2020](#bib.bib50); Mekruksavanich et al., [2022](#bib.bib127); Pan et al.,
    [2023](#bib.bib145); Dang et al., [2022](#bib.bib47)), and in some cases, multi-layer
    perceptron classifiers (Arablouei et al., [2023b](#bib.bib9)), particularly when
    sensor data is transformed into features like velocity, angles, humidity, location,
    among others (Arablouei et al., [2023a](#bib.bib10)). In the domain of bioacoustics,
    recurrent neural networks or pre-trained convolutional neural networks on spectrogram
    images are frequently applied (Wang et al., [2021b](#bib.bib190)). For images
    and videos, processing methods vary according to the task at hand. They may be
    employed for object detection, where identification extends beyond the animal
    to encompass specific behaviors, typically achieved through frameworks such as
    YOLO (Redmon et al., [2016](#bib.bib160)) and Faster R-CNN (Ren et al., [2015](#bib.bib161)).
    Alternatively, pre-trained neural networks or segmentation techniques, with subsequent
    analysis of the segmentation mask, are utilized (Xiao et al., [2023](#bib.bib198)).
    [Figure 3](#S7.F3 "Figure 3 ‣ 7\. Discussion about research questions ‣ Animal
    Behavior Analysis Methods Using Deep Learning: A Survey") and [Figure 4](#S7.F4
    "Figure 4 ‣ 7\. Discussion about research questions ‣ Animal Behavior Analysis
    Methods Using Deep Learning: A Survey") encapsulate and illustrate the summarized
    pose and non-pose estimation methods for behavior analysis expounded in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/76bcef76a300775f2ef419926aab1f09.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Comprehensive schema illustrating the pose estimation architectures
    covered in this survey, accompanied by detailed methodologies for accurate classification
    of predictions into distinct behavioral classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9106813393802f9bfd0cabc8509a04e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Comprehensive schema illustrating the non-pose estimation architectures
    covered in this survey. To improve readability we divided them into blocks using
    the same structure employed in the survey.
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ3 (What are the differences between human and animal behavior analysis?):
    Deep learning for animal behavior analysis draws inspiration from and translates
    methodologies used in human applications. While similarities exist in predicting
    final classification outputs, notable differences emerge in data handling, particularly
    with sensor data. Moreover, a distinct gap is evident in pose estimation methods.
    In the following list, we highlight these divergences that present opportunities
    for leveraging deep learning in animal behavior analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pose estimation (regression). In animal pose estimation, regression-based methodologies
    have not garnered widespread acclaim as observed in human applications. This divergence
    in popularity can likely be attributed to their utilization predating the ascendancy
    of heatmap-based techniques (Zheng et al., [2023](#bib.bib209)). Conversely, within
    the human context, these regression approaches have demonstrated noteworthy success,
    particularly when seamlessly integrated with multi-task learning techniques (Ruder,
    [2017](#bib.bib166)). By facilitating the exchange of information between interconnected
    tasks, such as pose estimation and pose-based action recognition, models can enhance
    their generalization capabilities. For example, Fan et al. (Fan et al., [2015](#bib.bib53))
    introduced an innovative dual-source convolutional neural network, employing both
    image patches and complete images for two distinct tasks: joint detection, responsible
    for discerning whether a patch contains a body joint, and joint localization,
    aimed at pinpointing the precise location of the joint within the patch. Each
    task is associated with its dedicated loss function, and the synergistic combination
    of these tasks yields notable improvements in overall performance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pose estimation (heatmap). While these approaches find application in the realm
    of animal behavior, we believe that leveraging the following methods employed
    in human pose estimation could significantly augment this field:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using pre-trained an general models based on transformers. Xu et al. (Xu et al.,
    [2023](#bib.bib201)) introduced VitPose++, an extension of the VitPose model (Xu
    et al., [2022](#bib.bib200)), specifically designed for generic body pose estimation.
    This innovative framework utilizes vision transformers and distinguishes itself
    by not only focusing on human subjects but also extending its application to animals.
    The training process incorporates the AP-10K (yu2021ap) and APT36K (Yang et al.,
    [2022](#bib.bib205)) datasets, ensuring a comprehensive understanding of both
    human and animal poses. We believe that fine-tuning pre-trained models of this
    kind may improve animal pose estimation results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using Generative Adversarial Networks (GANs) (Goodfellow et al., [2014](#bib.bib62)).
    The benefit can be twofold. GANs are explored in human pose estimation to generate
    biologically plausible pose configurations and to discriminate the predictions
    with high confidence from those with low confidence, which could infer the poses
    of the occluded body parts (Chen et al., [2017](#bib.bib40); Chou et al., [2018](#bib.bib44)).
    A valuable aspect that could be used also in our task, solving possible occlusion
    issue in open-field research. Another usage is to perform adversarial data augmentation
    treating the pose estimation network as a discriminator and using augmentation
    networks as a generator to perform adversarial augmentations (Peng et al., [2018](#bib.bib151)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smart handling of sensor data. The significance of detecting human behaviors
    through sensor technologies has grown significantly in recent years (Helmi et al.,
    [2023](#bib.bib72); Park et al., [2023a](#bib.bib148); Islam et al., [2023](#bib.bib76)).
    This has prompted researchers to critically evaluate the relevance of various
    sensors employed in behavior classification through deep learning techniques.
    The central question revolves around the necessity of each sensor in contributing
    to accurate behavior identification and whether a contribution significance analysis
    can be effectively employed in this context. Addressing this concern, Li et al. (Li
    et al., [2023](#bib.bib108)) introduced a novel method aimed at optimizing sensor
    selection. Their approach involves assessing the self-information brought by the
    jth sensor concerning the occurrence of a specific activity $A_{i}$ and multiplying
    it by the universality of the same sensor j during instances of the same activity
    $A_{i}$. This innovative strategy proved to be highly effective, resulting in
    improved recognition rates and reduced time consumption, thanks to the elimination
    of redundant and noisy data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quantifying data scarcity. As highlighted in the limitations section (refer
    to [subsection 2.1](#S2.SS1 "2.1\. Limitations in studying animal behaviors ‣
    2\. Motivation and problem statement ‣ Animal Behavior Analysis Methods Using
    Deep Learning: A Survey")), the collection of behavior data is frequently a laborious
    and resource-intensive process. Consequently, estimating dataset size has garnered
    attention in the field of Human Action Recognition (HAR) as an integral component
    of data collection planning. The objective is to mitigate the time and effort
    invested in behavior data collection while ensuring precise estimates of model
    parameters. For example, Hossain et al. (Hossain et al., [2023](#bib.bib74)) introduced
    a method grounded in Uncertainty Quantification (UQ) (Abdar et al., [2021](#bib.bib2))
    to determine the optimal amount of behavior data required for obtaining accurate
    estimates of model parameters when modeling human behaviors as a Markov Decision
    Process (Bellman, [1957](#bib.bib16)). Technologies of this nature have the potential
    to expedite research endeavors by facilitating more efficient workflows.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'RQ4 (What are the deep learning strategies that are suitable and could enhance
    this task, but are not yet exploited?): Exploring animal behavior through deep
    learning is just one facet of the multifaceted study in ethology and neuroscience.
    Beyond mere identification, understanding the decision-making processes and the
    emergence of new behaviors in animals is of paramount importance (Laboratory et al.,
    [2021](#bib.bib96)). This survey focuses on methodologies primarily centered around
    detecting behavioral classes or, in the case of unsupervised learning, identifying
    behavioral motifs. These motifs are then grouped into behavioral classes based
    on similarity, often with the assistance of human experts. However, animals, much
    like humans, exhibit dynamic changes in their behavior over time There is a growing
    need for methods that can efficiently capture these trial-to-trial changes, breaking
    them down into a learning component and a noise component (Ashwood et al., [2020](#bib.bib11)).
    This is where reinforcement learning plays a pivotal role. Not only does it allow
    the perception of shifts in animal behavior and provide examples of biological
    learning algorithms, but it also enables the emulation of animal movements. This
    emulation has given rise to digital twins (Liu et al., [2022](#bib.bib112)), providing
    a valuable tool for a more comprehensive study of animal behavior thanks to the
    ease of data acquisition. The convergence of deep learning strategies and reinforcement
    learning opens up exciting possibilities, paving the way for the development of
    interactive simulacra of animal behavior, akin to advancements already achieved
    in human behavior studies (Park et al., [2023b](#bib.bib149)). This innovative
    approach empowers researchers to simulate how animals might adapt their behavior
    in diverse environments while interacting with various agents, be they other animals,
    humans, or even robots (Yamaguchi et al., [2018](#bib.bib204); Mori et al., [2022](#bib.bib133);
    Romano and Stefanini, [2021](#bib.bib164)). Ultimately, the fusion of deep learning
    and reinforcement learning holds the promise of creating dynamic, interactive
    simulations that significantly deepen our understanding of the nuances of animal
    behavior across a spectrum of contexts.'
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In summary, this survey rigorously examined the manifold benefits associated
    with the application of deep learning methodologies in the identification of animal
    behavior. Commencing with a detailed elucidation of the underlying motivations,
    limitations, objectives, and pertinent research inquiries steering the integration
    of deep learning within this domain, we established a robust contextual framework.
    Subsequently, our scrutiny extended to a thorough review of contemporary techniques,
    systematically categorized into pose and non-pose estimation methodologies. Within
    these delineations, we expounded upon a spectrum of methodologies, encompassing
    sequence processing in pose estimation and biocoustics, direct classification
    utilizing convolutional neural networks, outliers detection, convolutional graph
    neural networks, object detection, segmentation strategies, self-supervised learning,
    explainability, and unsupervised learning. Moreover, we curated a comprehensive
    table of publicly available datasets relevant to animal behavior, thereby augmenting
    the practical utility of deep learning applications. Our discourse on the subject
    and prospective considerations has pinpointed extant challenges within the literature,
    proffering a roadmap for potential research trajectories conducive to the advancement
    of the field. In essence, this survey serves as an invaluable compendium for researchers
    spanning diverse domains, with particular relevance to ethologists and neuroscientists.
    We believe that this survey will function as a guiding beacon, steering forthcoming
    research initiatives and fostering advancements in the intricate domain of animal
    behavior studies using deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abdar et al. (2021) Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan,
    Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra
    Acharya, et al. 2021. A review of uncertainty quantification in deep learning:
    Techniques, applications and challenges. *Information fusion* 76 (2021), 243–297.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Akçay et al. (2020) Hüseyin Gökhan Akçay, Bekir Kabasakal, Duygugül Aksu, Nusret
    Demir, Melih Öz, and Ali Erdoğan. 2020. Automated bird counting with deep learning
    for regional bird distribution mapping. *Animals* 10, 7 (2020), 1207.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Al-Thani et al. (2020) Najla Al-Thani, Alreem Albuainain, Fatima Alnaimi, and
    Nizar Zorba. 2020. Drones for sheep livestock monitoring. In *2020 IEEE 20th Mediterranean
    Electrotechnical Conference (MELECON)*. IEEE, 672–676.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alagele and Yildirim (2022) Mohammed Alagele and Remzi Yildirim. 2022. ANIMAL
    GAIT IDENTIFICATION USING A DEEP LEARNING METHOD. In *2022 International Symposium
    on Multidisciplinary Studies and Innovative Technologies (ISMSIT)*. IEEE, 540–542.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alameer et al. (2022) Ali Alameer, Stephanie Buijs, Niamh O’Connell, Luke Dalton,
    Mona Larsen, Lene Pedersen, and Ilias Kyriazakis. 2022. Automated detection and
    quantification of contact behaviour in pigs using deep learning. *biosystems engineering*
    224 (2022), 118–130.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alameer et al. (2020) Ali Alameer, Ilias Kyriazakis, Hillary A Dalton, Amy L
    Miller, and Jaume Bacardit. 2020. Automatic recognition of feeding and foraging
    behaviour in pigs using deep learning. *Biosystems engineering* 197 (2020), 91–104.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andresen et al. (2020) Niek Andresen, Manuel Wöllhaf, Katharina Hohlbaum, Lars
    Lewejohann, Olaf Hellwich, Christa Thöne-Reineke, and Vitaly Belik. 2020. Towards
    a fully automated surveillance of well-being status in laboratory mice using deep
    learning: Starting with facial expression analysis. *PLoS One* 15, 4 (2020), e0228059.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arablouei et al. (2023b) Reza Arablouei, Liang Wang, Lachlan Currie, Jodan Yates,
    Flavio AP Alvarenga, and Greg J Bishop-Hurley. 2023b. Animal behavior classification
    via deep learning on embedded systems. *Computers and Electronics in Agriculture*
    207 (2023), 107707.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arablouei et al. (2023a) Reza Arablouei, Ziwei Wang, Greg J Bishop-Hurley, and
    Jiajun Liu. 2023a. Multimodal sensor data fusion for in-situ classification of
    animal behavior using accelerometry and GNSS data. *Smart Agricultural Technology*
    4 (2023), 100163.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ashwood et al. (2020) Zoe Ashwood, Nicholas A Roy, Ji Hyun Bak, and Jonathan W
    Pillow. 2020. Inferring learning rules from animal decision-making. *Advances
    in Neural Information Processing Systems* 33 (2020), 3442–3453.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ayadi et al. (2020) Safa Ayadi, Ahmed Ben Said, Rateb Jabbar, Chafik Aloulou,
    Achraf Chabbouh, and Ahmed Ben Achballah. 2020. Dairy cow rumination detection:
    A deep learning approach. In *Distributed Computing for Emerging Smart Networks:
    Second International Workshop, DiCES-N 2020, Bizerte, Tunisia, December 18, 2020,
    Proceedings 2*. Springer, 123–139.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banerjee et al. (2023) Shoubhik Chandan Banerjee, Khursheed Ahmad Khan, and
    Rati Sharma. 2023. Deep-worm-tracker: Deep learning methods for accurate detection
    and tracking for behavioral studies in C. elegans. *Applied Animal Behaviour Science*
    266 (2023), 106024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bao and Xie (2022) Jun Bao and Qiuju Xie. 2022. Artificial intelligence in
    animal farming: A systematic literature review. *Journal of Cleaner Production*
    331 (2022), 129956.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Barnard et al. (2016) Shanis Barnard, Simone Calderara, Simone Pistocchi, Rita
    Cucchiara, Michele Podaliri-Vulpiani, Stefano Messori, and Nicola Ferri. 2016.
    Quick, accurate, smart: 3D computer vision technology helps assessing confined
    animals’ behaviour. *PloS one* 11, 7 (2016), e0158748.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bellman (1957) Richard Bellman. 1957. A Markovian Decision Process. *Journal
    of Mathematics and Mechanics* 6, 5 (1957), 679–684. [http://www.jstor.org/stable/24900506](http://www.jstor.org/stable/24900506)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benaissa et al. (2023) S Benaissa, FAM Tuyttens, D Plets, L Martens, L Vandaele,
    W Joseph, and B Sonck. 2023. Improved cattle behaviour monitoring by combining
    Ultra-Wideband location and accelerometer data. *animal* 17, 4 (2023), 100730.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bergen et al. (2022) Silas Bergen, Manuela M Huso, Adam E Duerr, Melissa A
    Braham, Todd E Katzner, Sara Schmuecker, and Tricia A Miller. 2022. Classifying
    behavior from short-interval biologging data: An example with GPS tracking of
    birds. *Ecology and Evolution* 12, 2 (2022), e08395.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Berman et al. (2014) Gordon J Berman, Daniel M Choi, William Bialek, and Joshua W
    Shaevitz. 2014. Mapping the stereotyped behaviour of freely moving fruit flies.
    *Journal of The Royal Society Interface* 11, 99 (2014), 20140672.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bernardes et al. (2021) Rodrigo Cupertino Bernardes, Maria Augusta Pereira
    Lima, Raul Narciso Carvalho Guedes, Clíssia Barboza da Silva, and Gustavo Ferreira
    Martins. 2021. Ethoflow: computer vision and artificial intelligence-based software
    for automatic behavior analysis. *Sensors* 21, 9 (2021), 3237.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bhattacharya and Shahnawaz (2021) Samayan Bhattacharya and Sk Shahnawaz. 2021.
    Pose recognition in the wild: Animal pose estimation using agglomerative clustering
    and contrastive learning. *arXiv preprint arXiv:2111.08259* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Biderman et al. (2023) Dan Biderman, Matthew R Whiteway, Cole Hurwitz, Nicholas
    Greenspan, Robert S Lee, Ankit Vishnubhotla, Richard Warren, Federico Pedraja,
    Dillon Noone, Michael Schartner, et al. 2023. Lightning Pose: improved animal
    pose estimation via semi-supervised learning, Bayesian ensembling, and cloud-native
    open-source tools. *bioRxiv* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bjerge et al. (2023) Kim Bjerge, Jamie Alison, Mads Dyrmann, Carsten Eie Frigaard,
    Hjalte MR Mann, and Toke Thomas Høye. 2023. Accurate detection and identification
    of insects from camera trap images with deep learning. *PLOS Sustainability and
    Transformation* 2, 3 (2023), e0000051.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blau et al. (2022) Ari Blau, Christoph Gebhardt, Andres Bendesky, Liam Paninski,
    and Anqi Wu. 2022. SemiMultiPose: A Semi-supervised Multi-animal Pose Estimation
    Framework. *arXiv preprint arXiv:2204.07072* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bocaj et al. (2020) Enkeleda Bocaj, Dimitris Uzunidis, Panagiotis Kasnesis,
    and Charalampos Z Patrikakis. 2020. On the benefits of deep convolutional neural
    networks on animal activity recognition. In *2020 International Conference on
    Smart Systems and Technologies (SST)*. IEEE, 83–88.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bohnslav et al. (2021) James P Bohnslav, Nivanthika K Wimalasena, Kelsey J Clausing,
    Yu Y Dai, David A Yarmolinsky, Tomás Cruz, Adam D Kashlan, M Eugenia Chiappe,
    Lauren L Orefice, Clifford J Woolf, et al. 2021. DeepEthogram, a machine learning
    pipeline for supervised behavior classification from raw pixels. *Elife* 10 (2021),
    e63377.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boser et al. (1992) Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik.
    1992. A training algorithm for optimal margin classifiers. In *Proceedings of
    the fifth annual workshop on Computational learning theory*. 144–152.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bravo Sanchez et al. (2021) Francisco J Bravo Sanchez, Md Rahat Hossain, Nathan B
    English, and Steven T Moore. 2021. Bioacoustic classification of avian calls from
    raw sound waveforms with an open-source deep learning architecture. *Scientific
    Reports* 11, 1 (2021), 15733.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breiman (2001) Leo Breiman. 2001. Random forests. *Machine learning* 45 (2001),
    5–32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown and de Bivort (2017) André EX Brown and Benjamin de Bivort. 2017. The
    study of animal behaviour as a physical science. *bioRxiv* (2017). [https://doi.org/10.1101/220855](https://doi.org/10.1101/220855)
    arXiv:https://www.biorxiv.org/content/early/2017/11/17/220855.full.pdf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bryda (2013) Elizabeth C Bryda. 2013. The Mighty Mouse: the impact of rodents
    on advances in biomedical research. *Missouri medicine* 110, 3 (2013), 207.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Burgos-Artizzu et al. (2012) Xavier P Burgos-Artizzu, Piotr Dollár, Dayu Lin,
    David J Anderson, and Pietro Perona. 2012. Social behavior recognition in continuous
    video. In *2012 IEEE conference on computer vision and pattern recognition*. IEEE,
    1322–1329.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2019) Jinkun Cao, Hongyang Tang, Hao-Shu Fang, Xiaoyong Shen, Cewu
    Lu, and Yu-Wing Tai. 2019. Cross-domain adaptation for animal pose estimation.
    In *Proceedings of the IEEE/CVF international conference on computer vision*.
    9498–9507.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2020) Shuo Cao, Dean Zhao, Xiaoyang Liu, and Yueping Sun. 2020.
    Real-time robust detector for underwater live crabs based on deep learning. *Computers
    and Electronics in Agriculture* 172 (2020), 105339.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2017) Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2017.
    Realtime multi-person 2d pose estimation using part affinity fields. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 7291–7299.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chalmers et al. (2021) Carl Chalmers, Paul Fergus, S Wich, and SN Longmore.
    2021. Modelling Animal Biodiversity Using Acoustic Monitoring and Deep Learning.
    In *2021 International Joint Conference on Neural Networks (IJCNN)*. IEEE, 1–7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chambers et al. (2021) Robert D Chambers, Nathanael C Yoder, Aletha B Carson,
    Christian Junge, David E Allen, Laura M Prescott, Sophie Bradley, Garrett Wymore,
    Kevin Lloyd, and Scott Lyle. 2021. Deep learning classification of canine behavior
    using a single collar-mounted accelerometer: Real-world validation. *Animals*
    11, 6 (2021), 1549.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Chen Chen, Weixing Zhu, and Tomas Norton. 2021. Behaviour
    recognition of pigs and cattle: Journey from computer vision to deep learning.
    *Computers and Electronics in Agriculture* 187 (2021), 106255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020) Chen Chen, Weixing Zhu, Juan Steibel, Janice Siegford, Junjie
    Han, and Tomas Norton. 2020. Recognition of feeding behaviour of pigs and determination
    of feeding time of each pig by a video-based deep learning method. *Computers
    and Electronics in Agriculture* 176 (2020), 105642.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2017) Yu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, and Jian
    Yang. 2017. Adversarial posenet: A structure-aware convolutional network for human
    pose estimation. In *Proceedings of the IEEE international conference on computer
    vision*. 1212–1221.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. (2022) Taeyeong Choi, Benjamin Pyenson, Juergen Liebig, and Theodore P
    Pavlic. 2022. Beyond tracking: using deep learning to discover novel interactions
    in biological swarms. *Artificial Life and Robotics* 27, 2 (2022), 393–400.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choi et al. (2021) Yoona Choi, Heechan Chae, Jonguk Lee, Daihee Park, and Yongwha
    Chung. 2021. Cat Monitoring and Disease Diagnosis System based on Deep Learning.
    *Journal of Korea Multimedia Society* 24, 2 (2021), 233–244.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chollet (2017) François Chollet. 2017. Xception: Deep learning with depthwise
    separable convolutions. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*. 1251–1258.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chou et al. (2018) Chia-Jung Chou, Jui-Ting Chien, and Hwann-Tzong Chen. 2018.
    Self adversarial training for human pose estimation. In *2018 Asia-Pacific Signal
    and Information Processing Association Annual Summit and Conference (APSIPA ASC)*.
    IEEE, 17–30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coria-Avila et al. (2022) Genaro A Coria-Avila, James G Pfaus, Agustín Orihuela,
    Adriana Domínguez-Oliva, Nancy José-Pérez, Laura Astrid Hernández, and Daniel
    Mota-Rojas. 2022. The neurobiology of behavior and its applicability for animal
    welfare: A review. *Animals* 12, 7 (2022), 928.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coulibaly et al. (2022) Solemane Coulibaly, Bernard Kamsu-Foguem, Dantouma Kamissoko,
    and Daouda Traore. 2022. Explainable deep convolutional neural networks for insect
    pest recognition. *Journal of Cleaner Production* 371 (2022), 133638.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dang et al. (2022) Thai-Ha Dang, Ngoc-Hai Dang, Viet-Thang Tran, and Wan-Young
    Chung. 2022. A LoRaWAN-Based Smart Sensor Tag for Cow Behavior Monitoring. In
    *2022 IEEE Sensors*. IEEE, 1–4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dell et al. (2014) Anthony I Dell, John A Bender, Kristin Branson, Iain D Couzin,
    Gonzalo G de Polavieja, Lucas PJJ Noldus, Alfonso Pérez-Escudero, Pietro Perona,
    Andrew D Straw, Martin Wikelski, et al. 2014. Automated image-based tracking and
    its application in ecology. *Trends in ecology & evolution* 29, 7 (2014), 417–428.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ditria et al. (2020) Ellen M Ditria, Sebastian Lopez-Marcano, Michael Sievers,
    Eric L Jinks, Christopher J Brown, and Rod M Connolly. 2020. Automating the analysis
    of fish abundance using object detection: optimizing animal ecology with deep
    learning. *Frontiers in Marine Science* (2020), 429.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eerdekens et al. (2020) Anniek Eerdekens, Margot Deruyck, Jaron Fontaine, Luc
    Martens, Eli De Poorter, David Plets, and Wout Joseph. 2020. Resampling and data
    augmentation for equines’ behaviour classification based on wearable sensor accelerometer
    data using a convolutional neural network. In *2020 International Conference on
    Omni-layer Intelligent Systems (COINS)*. IEEE, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elbarrany et al. (2023) Abdullah Magdy Elbarrany, Abdallah Mohialdin, and Ayman
    Atia. 2023. The Use of Pose Estimation for Abnormal Behavior Analysis in Poultry
    Farms. In *2023 5th Novel Intelligent and Leading Emerging Sciences Conference
    (NILES)*. IEEE, 33–36.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evangelista et al. (2022) Ivan Roy S Evangelista, Ronnie Concepcion, Maria Gemel B
    Palconit, Argel A Bandala, and Elmer P Dadios. 2022. YOLOv7 and DeepSORT for Intelligent
    Quail Behavioral Activities Monitoring. In *2022 IEEE 14th International Conference
    on Humanoid, Nanotechnology, Information Technology, Communication and Control,
    Environment, and Management (HNICEM)*. IEEE, 1–5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2015) Xiaochuan Fan, Kang Zheng, Yuewei Lin, and Song Wang. 2015.
    Combining local appearance and holistic view: Dual-source deep neural networks
    for human pose estimation. In *Proceedings of the IEEE conference on computer
    vision and pattern recognition*. 1347–1355.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. (2021) Cheng Fang, Tiemin Zhang, Haikun Zheng, Junduan Huang, and
    Kaixuan Cuan. 2021. Pose estimation and behavior classification of broiler chickens
    based on deep neural networks. *Computers and Electronics in Agriculture* 180
    (2021), 105863.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fazzari et al. (2023) Edoardo Fazzari, Fabio Carrara, Fabrizio Falchi, Cesare
    Stefanini, and Donato Romano. 2023. Using AI to decode the behavioral responses
    of an insect to chemical stimuli: towards machine-animal computational technologies.
    *International Journal of Machine Learning and Cybernetics* (2023). [https://doi.org/10.1007/s13042-023-02009-y](https://doi.org/10.1007/s13042-023-02009-y)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fazzari et al. (2022) Edoardo Fazzari, Fabio Carrara, Fabrizio Falchi, Cesare
    Stefanini, Donato Romano, et al. 2022. A Workflow for Developing Biohybrid Intelligent
    Sensing Systems. (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feichtenhofer et al. (2019) Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik,
    and Kaiming He. 2019. Slowfast networks for video recognition. In *Proceedings
    of the IEEE/CVF international conference on computer vision*. 6202–6211.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feighelstein et al. (2023) Marcelo Feighelstein, Yamit Ehrlich, Li Naftaly,
    Miriam Alpin, Shenhav Nadir, Ilan Shimshoni, Renata H Pinho, Stelio PL Luna, and
    Anna Zamansky. 2023. Deep learning for video-based automated pain recognition
    in rabbits. *Scientific Reports* 13, 1 (2023), 14679.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fuentes et al. (2020) Alvaro Fuentes, Sook Yoon, Jongbin Park, and Dong Sun
    Park. 2020. Deep learning-based hierarchical cattle behavior recognition with
    spatio-temporal information. *Computers and Electronics in Agriculture* 177 (2020),
    105627.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fujimori et al. (2020) Shiori Fujimori, Takaaki Ishikawa, and Hiroshi Watanabe.
    2020. Animal behavior classification using DeepLabCut. In *2020 IEEE 9th Global
    Conference on Consumer Electronics (GCCE)*. IEEE, 254–257.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gnanasekar et al. (2022) Sudarsini Tekkam Gnanasekar, Svetlana Yanushkevich,
    Nynke J Van den Hoogen, and Tuan Trang. 2022. Rodent Tracking and Abnormal Behavior
    Classification in Live Video using Deep Neural Networks. In *2022 IEEE Symposium
    Series on Computational Intelligence (SSCI)*. IEEE, 830–837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.
    Generative adversarial nets. *Advances in neural information processing systems*
    27 (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gore et al. (2023) Sayali V Gore, Rohit Kakodkar, Thaís Del Rosario Hernández,
    Sara Tucker Edmister, and Robbert Creton. 2023. Zebrafish Larvae Position Tracker
    (Z-LaP Tracker): a high-throughput deep-learning behavioral approach for the identification
    of calcineurin pathway-modulating drugs using zebrafish larvae. *Scientific Reports*
    13, 1 (2023), 3174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gotanda et al. (2019) Kiyoko M Gotanda, Damien R Farine, Claudius F Kratochwil,
    Kate L Laskowski, and Pierre-Olivier Montiglio. 2019. Animal behavior facilitates
    eco-evolutionary dynamics. *arXiv preprint arXiv:1912.09505* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graving et al. (2019) Jacob M Graving, Daniel Chae, Hemal Naik, Liang Li, Benjamin
    Koger, Blair R Costelloe, and Iain D Couzin. 2019. DeepPoseKit, a software toolkit
    for fast and robust animal pose estimation using deep learning. *Elife* 8 (2019),
    e47994.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haalck et al. (2020) Lars Haalck, Michael Mangan, Barbara Webb, and Benjamin
    Risse. 2020. Towards image-based animal tracking in natural environments using
    a freely moving camera. *Journal of neuroscience methods* 330 (2020), 108455.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2020) Fangfang Han, Junchao Zhu, Bin Liu, Baofeng Zhang, and Fuhua
    Xie. 2020. Fish shoals behavior detection based on convolutional neural network
    and spatiotemporal information. *IEEE Access* 8 (2020), 126907–126926.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hara et al. (2018) Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. 2018. Can
    spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?. In *Proceedings
    of the IEEE conference on Computer Vision and Pattern Recognition*. 6546–6555.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hart (2011) Benjamin L Hart. 2011. Behavioural defences in animals against
    pathogens and parasites: parallels with the pillars of medicine in humans. *Philosophical
    Transactions of the Royal Society B: Biological Sciences* 366, 1583 (2011), 3406–3417.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2017) Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
    2017. Mask r-cnn. In *Proceedings of the IEEE international conference on computer
    vision*. 2961–2969.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Identity mappings in deep residual networks. In *Computer Vision–ECCV 2016: 14th
    European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings,
    Part IV 14*. Springer, 630–645.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helmi et al. (2023) Ahmed M Helmi, Mohammed AA Al-qaness, Abdelghani Dahou,
    and Mohamed Abd Elaziz. 2023. Human activity recognition using marine predators
    algorithm with deep learning. *Future Generation Computer Systems* 142 (2023),
    340–350.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long short-term memory. *Neural computation* 9, 8 (1997), 1735–1780.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hossain et al. (2023) Tahera Hossain, Wanggang Shen, Anindya Antar, Snehal Prabhudesai,
    Sozo Inoue, Xun Huan, and Nikola Banovic. 2023. A Bayesian approach for quantifying
    data scarcity when modeling human behavior via inverse reinforcement learning.
    *ACM Transactions on Computer-Human Interaction* 30, 1 (2023), 1–27.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. (2020) Jin Hou, Yuxin He, Hongbo Yang, Thomas Connor, Jie Gao, Yujun
    Wang, Yichao Zeng, Jindong Zhang, Jinyan Huang, Bochuan Zheng, et al. 2020. Identification
    of animal individuals using deep learning: A case study of giant panda. *Biological
    Conservation* 242 (2020), 108414.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Islam et al. (2023) Md Milon Islam, Sheikh Nooruddin, Fakhri Karray, and Ghulam
    Muhammad. 2023. Multi-level feature fusion for multimodal human activity recognition
    in Internet of Healthcare Things. *Information Fusion* 94 (2023), 17–31.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ismail Fawaz et al. (2020) Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier,
    Charlotte Pelletier, Daniel F Schmidt, Jonathan Weber, Geoffrey I Webb, Lhassane
    Idoumghar, Pierre-Alain Muller, and François Petitjean. 2020. Inceptiontime: Finding
    alexnet for time series classification. *Data Mining and Knowledge Discovery*
    34, 6 (2020), 1936–1962.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. (2022) Yinjun Jia, Shuaishuai Li, Xuan Guo, Bo Lei, Junqiang Hu,
    Xiao-Hong Xu, and Wei Zhang. 2022. Selfee, self-supervised features extraction
    of animal behaviors. *Elife* 11 (2022), e76218.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2022a) Le Jiang, Caleb Lee, Divyang Teotia, and Sarah Ostadabbas.
    2022a. Animal pose estimation: A closer look at the state-of-the-art, existing
    gaps and opportunities. *Computer Vision and Image Understanding* (2022), 103483.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2020) Min Jiang, Yuan Rao, Jingyao Zhang, and Yiming Shen. 2020.
    Automatic behavior recognition of group-housed goats using deep learning. *Computers
    and Electronics in Agriculture* 177 (2020), 105706.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2022b) Zheheng Jiang, Zhihua Liu, Long Chen, Lei Tong, Xiangrong
    Zhang, Xiangyuan Lan, Danny Crookes, Ming-Hsuan Yang, and Huiyu Zhou. 2022b. Detecting
    and tracking of multiple mice using part proposal networks. *IEEE Transactions
    on Neural Networks and Learning Systems* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2021) Zheheng Jiang, Feixiang Zhou, Aite Zhao, Xin Li, Ling Li,
    Dacheng Tao, Xuelong Li, and Huiyu Zhou. 2021. Multi-view mouse social behaviour
    recognition with deep graphic model. *IEEE Transactions on Image Processing* 30
    (2021), 5490–5504.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joo et al. (2022) Kevin Hyekang Joo, Shiyuan Duan, Shawna L Weimer, and Mohammad Nayeem
    Teli. 2022. Birds’ Eye View: Measuring Behavior and Posture of Chickens as a Metric
    for Their Well-Being. *arXiv preprint arXiv:2205.00069* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Júnior and Rieder (2020) Telmo De Cesaro Júnior and Rafael Rieder. 2020. Automatic
    identification of insects from digital images: A survey. *Computers and Electronics
    in Agriculture* 178 (2020), 105784.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kamminga et al. (2018) Jacob W Kamminga, Duc V Le, Jan Pieter Meijers, Helena
    Bisby, Nirvana Meratnia, and Paul JM Havinga. 2018. Robust sensor-orientation-independent
    feature selection for animal activity recognition on collar tags. *Proceedings
    of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies* 2, 1
    (2018), 1–27.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kamminga et al. (2019) Jacob W Kamminga, Nirvana Meratnia, and Paul JM Havinga.
    2019. Dataset: Horse movement data and analysis of its potential for activity
    recognition. In *Proceedings of the 2nd Workshop on Data Acquisition To Analysis*.
    22–25.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kasnesis et al. (2022) Panagiotis Kasnesis, Vasileios Doulgerakis, Dimitris
    Uzunidis, Dimitris G Kogias, Susana I Funcia, Marta B González, Christos Giannousis,
    and Charalampos Z Patrikakis. 2022. Deep learning empowered wearable-based behavior
    recognition for search and rescue dogs. *Sensors* 22, 3 (2022), 993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaufmann and Schering (2014) Jörg Kaufmann and AG Schering. 2014. *Analysis
    of Variance ANOVA*. John Wiley & Sons, Ltd. [https://doi.org/10.1002/9781118445112.stat06938](https://doi.org/10.1002/9781118445112.stat06938)
    arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat06938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kavlak et al. (2023) AT Kavlak, M Pastell, and P Uimari. 2023. Disease detection
    in pigs based on feeding behaviour traits using machine learning. *biosystems
    engineering* 226 (2023), 132–143.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-encoding
    variational bayes. *arXiv preprint arXiv:1312.6114* (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kleanthous et al. (2022b) Natasa Kleanthous, Abir Hussain, Wasiq Khan, Jennifer
    Sneddon, and Panos Liatsis. 2022b. Deep transfer learning in sheep activity recognition
    using accelerometer data. *Expert Systems with Applications* 207 (2022), 117925.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kleanthous et al. (2022a) Natasa Kleanthous, Abir Jaafar Hussain, Wasiq Khan,
    Jennifer Sneddon, Ahmed Al-Shamma’a, and Panos Liatsis. 2022a. A survey of machine
    learning approaches in animal behaviour. *Neurocomputing* 491 (2022), 442–463.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knight and Bayne (2019) Elly C Knight and Erin M Bayne. 2019. Classification
    threshold and training data affect the quality and utility of focal species data
    processed with automated audio-recognition software. *Bioacoustics* 28, 6 (2019),
    539–554.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koch et al. (2015) Gregory Koch, Richard Zemel, Ruslan Salakhutdinov, et al.
    2015. Siamese neural networks for one-shot image recognition. In *ICML deep learning
    workshop*, Vol. 2\. Lille.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koger et al. (2023) Benjamin Koger, Adwait Deshpande, Jeffrey T Kerby, Jacob M
    Graving, Blair R Costelloe, and Iain D Couzin. 2023. Quantifying the movement,
    behaviour and environmental context of group-living animals using drones and computer
    vision. *Journal of Animal Ecology* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laboratory et al. (2021) International Brain Laboratory, Valeria Aguillon-Rodriguez,
    Dora Angelaki, Hannah Bayer, Niccolò Bonacchi, Matteo Carandini, Fanny Cazettes,
    Gaelle Chapuis, Anne K Churchland, Yang Dan, et al. 2021. Standardized and reproducible
    measurement of decision-making in mice. *Elife* 10 (2021), e63711.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Labuguen et al. (2021) Rollyn Labuguen, Jumpei Matsumoto, Salvador Blanco Negrete,
    Hiroshi Nishimaru, Hisao Nishijo, Masahiko Takada, Yasuhiro Go, Ken-ichi Inoue,
    and Tomohiro Shibata. 2021. MacaquePose: a novel “in the wild” macaque monkey
    pose dataset for markerless motion capture. *Frontiers in behavioral neuroscience*
    14 (2021), 581154.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Landgraf et al. (2021) Tim Landgraf, Gregor HW Gebhardt, David Bierbach, Pawel
    Romanczuk, Lea Musiolek, Verena V Hafner, and Jens Krause. 2021. Animal-in-the-loop:
    using interactive robotic conspecifics to study social behavior in animal groups.
    *Annual Review of Control, Robotics, and Autonomous Systems* 4 (2021), 487–507.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lauer et al. (2022) Jessy Lauer, Mu Zhou, Shaokai Ye, William Menegas, Steffen
    Schneider, Tanmay Nath, Mohammed Mostafizur Rahman, Valentina Di Santo, Daniel
    Soberanes, Guoping Feng, et al. 2022. Multi-animal pose estimation, identification
    and tracking with DeepLabCut. *Nature Methods* 19, 4 (2022), 496–504.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lea et al. (2016) Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager.
    2016. Temporal convolutional networks: A unified approach to action segmentation.
    In *Computer Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10
    and 15-16, 2016, Proceedings, Part III 14*. Springer, 47–54.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lecomte et al. (2021) Charly G Lecomte, Johannie Audet, Jonathan Harnie, and
    Alain Frigon. 2021. A validation of supervised deep learning for gait analysis
    in the cat. *Frontiers in Neuroinformatics* 15 (2021), 712623.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2021) Sanghoon Lee, Brayden Waugh, Garret O’Dell, Xiji Zhao, Wook-Sung
    Yoo, and Dal Hyung Kim. 2021. Predicting Fruit Fly Behaviour using TOLC device
    and DeepLabCut. In *2021 IEEE 21st International Conference on Bioinformatics
    and Bioengineering (BIBE)*. IEEE, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lei et al. (2022) Yujie Lei, Pengmei Dong, Yan Guan, Ying Xiang, Meng Xie,
    Jiong Mu, Yongzhao Wang, and Qingyong Ni. 2022. Postural behavior recognition
    of captive nocturnal animals based on deep learning: a case study of Bengal slow
    loris. *Scientific Reports* 12, 1 (2022), 7738.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Lee (2023) Chen Li and Gim Hee Lee. 2023. ScarceNet: Animal Pose Estimation
    with Scarce Annotations. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 17174–17183.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Dan Li, Kaifeng Zhang, Zhenbo Li, and Yifei Chen. 2020. A spatiotemporal
    convolutional network for multi-behavior recognition of pigs. *Sensors* 20, 8
    (2020), 2381.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019b) Juan Li, Chen Xu, Lingxu Jiang, Ying Xiao, Limiao Deng, and
    Zhongzhi Han. 2019b. Detection and analysis of behavior trajectory for sea cucumbers
    based on deep learning. *Ieee Access* 8 (2019), 18832–18840.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019a) Shuyuan Li, Jianguo Li, Hanlin Tang, Rui Qian, and Weiyao
    Lin. 2019a. ATRW: a benchmark for Amur tiger re-identification in the wild. *arXiv
    preprint arXiv:1906.05586* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Yang Li, Guanci Yang, Zhidong Su, Shaobo Li, and Yang Wang.
    2023. Human activity recognition based on multienvironment sensor data. *Information
    Fusion* 91 (2023), 47–63.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2017) Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath
    Hariharan, and Serge Belongie. 2017. Feature pyramid networks for object detection.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    2117–2125.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2008) Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2008. Isolation
    forest. In *2008 eighth ieee international conference on data mining*. IEEE, 413–422.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2020) XiaoLe Liu, Si-yang Yu, Nico Flierman, Sebastian Loyola,
    Maarten Kamermans, Tycho M Hoogland, and Chris I De Zeeuw. 2020. OptiFlex: video-based
    animal pose estimation using deep learning enhanced by optical flow. *BioRxiv*
    (2020), 2020–04.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022) Yongkui Liu, He Xu, Ding Liu, and Lihui Wang. 2022. A digital
    twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial
    robot grasping. *Robotics and Computer-Integrated Manufacturing* 78 (2022), 102365.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lostanlen et al. (2019) Vincent Lostanlen, Kaitlin Palmer, Elly Knight, Christopher
    Clark, Holger Klinck, Andrew Farnsworth, Tina Wong, Jason Cramer, and Juan Pablo
    Bello. 2019. Long-distance detection of bioacoustic events with per-channel energy
    normalization. *arXiv preprint arXiv:1911.00417* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luxem et al. (2022) Kevin Luxem, Petra Mocellin, Falko Fuhrmann, Johannes Kürsch,
    Stephanie R Miller, Jorge J Palop, Stefan Remy, and Pavol Bauer. 2022. Identifying
    behavioral structure from deep variational embeddings of animal motion. *Communications
    Biology* 5, 1 (2022), 1267.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahmud et al. (2021) Md Sultan Mahmud, Azlan Zahid, Anup Kumar Das, Muhammad
    Muzammil, and Muhammad Usman Khan. 2021. A systematic literature review on deep
    learning applications for precision cattle farming. *Computers and Electronics
    in Agriculture* 187 (2021), 106313.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manduca et al. (2023) Gianluca Manduca, Valeria Zeni, Sara Moccia, Beatrice A
    Milano, Angelo Canale, Giovanni Benelli, Cesare Stefanini, and Donato Romano.
    2023. Learning algorithms estimate pose and detect motor anomalies in flies exposed
    to minimal doses of a toxicant. *Iscience* 26, 12 (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mankin et al. (2021) Richard Mankin, David Hagstrum, Min Guo, Panagiotis Eliopoulos,
    and Anastasia Njoroge. 2021. Automated applications of acoustics for stored product
    insect detection, monitoring, and management. *Insects* 12, 3 (2021), 259.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manoharan (2020) Dr Samuel Manoharan. 2020. Embedded imaging system based behavior
    analysis of dairy cow. *Journal of Electronics and Informatics* 2, 2 (2020), 148–154.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marks et al. (2022) Markus Marks, Qiuhan Jin, Oliver Sturman, Lukas von Ziegler,
    Sepp Kollmorgen, Wolfger von der Behrens, Valerio Mante, Johannes Bohacek, and
    Mehmet Fatih Yanik. 2022. Deep-learning-based identification, tracking, pose estimation
    and behaviour classification of interacting primates and mice in complex environments.
    *Nature machine intelligence* 4, 4 (2022), 331–340.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marshall et al. (2022) Jesse D Marshall, Tianqing Li, Joshua H Wu, and Timothy W
    Dunn. 2022. Leaving flatland: Advances in 3D behavioral measurement. *Current
    Opinion in Neurobiology* 73 (2022), 102522.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Martin-Abadal et al. (2020) Miguel Martin-Abadal, Ana Ruiz-Frau, Hilmar Hinz,
    and Yolanda Gonzalez-Cid. 2020. Jellytoring: real-time jellyfish monitoring based
    on deep learning object detection. *Sensors* 20, 6 (2020), 1708.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathis et al. (2021) Alexander Mathis, Thomas Biasi, Steffen Schneider, Mert
    Yuksekgonul, Byron Rogers, Matthias Bethge, and Mackenzie W Mathis. 2021. Pretraining
    boosts out-of-domain robustness for pose estimation. In *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*. 1859–1868.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mathis et al. (2018) Alexander Mathis, Pranav Mamidanna, Kevin M Cury, Taiga
    Abe, Venkatesh N Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. 2018.
    DeepLabCut: markerless pose estimation of user-defined body parts with deep learning.
    *Nature neuroscience* 21, 9 (2018), 1281–1289.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathis and Mathis (2020) Mackenzie Weygandt Mathis and Alexander Mathis. 2020.
    Deep learning tools for the measurement of animal behavior in neuroscience. *Current
    opinion in neurobiology* 60 (2020), 1–11.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McIntosh et al. (2020) Declan McIntosh, Tunai Porto Marques, Alexandra Branzan
    Albu, Rodney Rountree, and Fabio De Leo. 2020. Movement tracks for the automatic
    detection of fish behavior in videos. *arXiv preprint arXiv:2011.14070* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McKenzie-Smith et al. (2023) Grace C McKenzie-Smith, Scott W Wolf, Julien F
    Ayroles, and Joshua W Shaevitz. 2023. Capturing continuous, long timescale behavioral
    changes in Drosophila melanogaster postural data. *arXiv preprint arXiv:2309.04044*
    (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mekruksavanich et al. (2022) Sakorn Mekruksavanich, Ponnipa Jantawong, and Anuchit
    Jitpattanakul. 2022. ResNet-based Deep Neural Network using Transfer Learning
    for Animal Activity Recognition. In *2022 6th International Conference on Information
    Technology (InCIT)*. IEEE, 445–449.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mendoza et al. (2023) Querriel Arvy Mendoza, Lester Pordesimo, Mitchell Neilsen,
    Paul Armstrong, James Campbell, and Princess Tiffany Mendoza. 2023. Application
    of Machine Learning for Insect Monitoring in Grain Facilities. *AI* 4, 1 (2023),
    348–360.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mishra and Sharma (2023) Shailendra Mishra and Sunil Kumar Sharma. 2023. Advanced
    contribution of IoT in agricultural production for the development of smart livestock
    environments. *Internet of Things* 22 (2023), 100724.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modlmeier et al. (2019) Andreas P Modlmeier, Ewan Colman, Ephraim M Hanks, Ryan
    Bringenberg, Shweta Bansal, and David P Hughes. 2019. Ant colonies maintain social
    homeostasis in the face of decreased density. *Elife* 8 (2019), e38473.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mohialdin et al. (2023) Abdallah Mohamed Mohialdin, Abdullah Magdy Elbarrany,
    and Ayman Atia. 2023. Chicken Behavior Analysis for Surveillance in Poultry Farms.
    (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Morfi et al. (2019) Veronica Morfi, Yves Bas, Hanna Pamuła, Hervé Glotin, and
    Dan Stowell. 2019. NIPS4Bplus: a richly annotated birdsong audio dataset. *PeerJ
    Computer Science* 5 (2019), e223.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mori et al. (2022) Keita Mori, Naohiro Yamauchi, Haoyu Wang, Ken Sato, Yu Toyoshima,
    and Yuichi Iino. 2022. Probabilistic generative modeling and reinforcement learning
    extract the intrinsic features of animal behavior. *Neural Networks* 145 (2022),
    107–120.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moummad et al. (2023) Ilyass Moummad, Romain Serizel, and Nicolas Farrugia.
    2023. Regularized Contrastive Pre-training for Few-shot Bioacoustic Sound Detection.
    *arXiv preprint arXiv:2309.08971* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nasiri et al. (2023) Amin Nasiri, Ahmad Amirivojdan, Yang Zhao, and Hao Gan.
    2023. Estimating the Feeding Time of Individual Broilers via Convolutional Neural
    Network and Image Processing. *Animals* 13, 15 (2023), 2428.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nath et al. (2019) Tanmay Nath, Alexander Mathis, An Chi Chen, Amir Patel, Matthias
    Bethge, and Mackenzie Weygandt Mathis. 2019. Using DeepLabCut for 3D markerless
    pose estimation across species and behaviors. *Nature protocols* 14, 7 (2019),
    2152–2176.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neethirajan (2020) Suresh Neethirajan. 2020. The role of sensors, big data and
    machine learning in modern animal farming. *Sensing and Bio-Sensing Research*
    29 (2020), 100367.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ng et al. (2022) Xun Long Ng, Kian Eng Ong, Qichen Zheng, Yun Ni, Si Yong Yeo,
    and Jun Liu. 2022. Animal kingdom: A large and diverse dataset for animal behavior
    understanding. In *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*. 19023–19034.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen and Kresovic (2022) Thong Duy Nguyen and Milan Kresovic. 2022. A survey
    of top-down approaches for human pose estimation. *arXiv preprint arXiv:2202.02656*
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nilsson et al. (2020) Simon RO Nilsson, Nastacia L Goodwin, Jia Jie Choong,
    Sophia Hwang, Hayden R Wright, Zane C Norville, Xiaoyu Tong, Dayu Lin, Brandon S
    Bentzley, Neir Eshel, et al. 2020. Simple Behavioral Analysis (SimBA)–an open
    source toolkit for computer classification of complex social behaviors in experimental
    animals. *BioRxiv* (2020), 2020–04.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nolasco et al. (2023) Inês Nolasco, Shubhr Singh, Veronica Morfi, Vincent Lostanlen,
    Ariana Strandburg-Peshkin, Ester Vidaña-Vila, Lisa Gill, Hanna Pamuła, Helen Whitehead,
    Ivan Kiskin, et al. 2023. Learning to detect an animal sound from five examples.
    *Ecological Informatics* 77 (2023), 102258.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Odo et al. (2023) Anicetus Odo, Ramon Muns, Laura Boyle, and Ilias Kyriazakis.
    2023. Video Analysis using Deep Learning for Automatic Quantification of Ear Biting
    in Pigs. *IEEE Access* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oliveira et al. (2021) Dario Augusto Borges Oliveira, Luiz Gustavo Ribeiro Pereira,
    Tiago Bresolin, Rafael Ehrich Pontes Ferreira, and Joao Ricardo Reboucas Dorea.
    2021. A review of deep learning algorithms for computer vision systems in livestock.
    *Livestock Science* 253 (2021), 104700.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Özdaş et al. (2023) Mehmet Batuhan Özdaş, Fatih Uysal, and Fırat Hardalaç. 2023.
    Classification of Retinal Diseases in Optical Coherence Tomography Images Using
    Artificial Intelligence and Firefly Algorithm. *Diagnostics* 13, 3 (2023), 433.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan et al. (2023) Zhixin Pan, Huihui Chen, Weizhao Zhong, Aiguo Wang, and Chundi
    Zheng. 2023. A CNN-Based Animal Behavior Recognition Algorithm for Wearable Devices.
    *IEEE Sensors Journal* 23, 5 (2023), 5156–5164.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papandreou et al. (2018) George Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros
    Gidaris, Jonathan Tompson, and Kevin Murphy. 2018. PersonLab: Person Pose Estimation
    and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model.
    In *Proceedings of the European Conference on Computer Vision (ECCV)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papaspyros et al. (2023) Vaios Papaspyros, Ramón Escobedo, Alexandre Alahi,
    Guy Theraulaz, Clément Sire, and Francesco Mondada. 2023. Predicting long-term
    collective animal behavior with deep learning. *bioRxiv* (2023), 2023–02.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2023a) Hyunseo Park, Nakyoung Kim, Gyeong Ho Lee, and Jun Kyun
    Choi. 2023a. MultiCNN-FilterLSTM: Resource-efficient sensor-based human activity
    recognition in IoT applications. *Future Generation Computer Systems* 139 (2023),
    196–209.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2023b) Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. 2023b. Generative agents: Interactive
    simulacra of human behavior. In *Proceedings of the 36th Annual ACM Symposium
    on User Interface Software and Technology*. 1–22.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pedersen et al. (2006) Ward A Pedersen, Pamela J McMillan, J Jacob Kulstad,
    James B Leverenz, Suzanne Craft, and Gleb R Haynatzki. 2006. Rosiglitazone attenuates
    learning and memory deficits in Tg2576 Alzheimer mice. *Experimental neurology*
    199, 2 (2006), 265–273.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2018) Xi Peng, Zhiqiang Tang, Fei Yang, Rogerio S Feris, and Dimitris
    Metaxas. 2018. Jointly optimize data augmentation and network training: Adversarial
    data augmentation in human pose estimation. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 2226–2234.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pereira et al. (2019) Talmo D Pereira, Diego E Aldarondo, Lindsay Willmore,
    Mikhail Kislin, Samuel S-H Wang, Mala Murthy, and Joshua W Shaevitz. 2019. Fast
    animal pose estimation using deep neural networks. *Nature methods* 16, 1 (2019),
    117–125.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pereira et al. (2022) Talmo D Pereira, Nathaniel Tabris, Arie Matsliah, David M
    Turner, Junyu Li, Shruthi Ravindranath, Eleni S Papadoyannis, Edna Normand, David S
    Deutsch, Z Yan Wang, et al. 2022. SLEAP: A deep learning system for multi-animal
    pose tracking. *Nature methods* 19, 4 (2022), 486–495.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perez and Toler-Franklin (2023) Michael Perez and Corey Toler-Franklin. 2023.
    CNN-Based Action Recognition and Pose Estimation for Classifying Animal Behavior
    from Videos: A Survey. *arXiv preprint arXiv:2301.06187* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pham (2022) Tuan D Pham. 2022. Classification of Caenorhabditis Elegans Locomotion
    Behaviors With Eigenfeature-Enhanced Long Short-Term Memory Networks. *IEEE/ACM
    Transactions on Computational Biology and Bioinformatics* 20, 1 (2022), 206–216.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Piergiovanni and Ryoo (2019) AJ Piergiovanni and Michael Ryoo. 2019. Temporal
    gaussian mixture layer for videos. In *International Conference on Machine learning*.
    PMLR, 5152–5161.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pillai et al. (2023) Rudresh Pillai, Rupesh Gupta, Neha Sharma, and Rajesh Kumar
    Bansal. 2023. A Deep Learning Approach for Detection and Classification of Ten
    Species of Monkeys. In *2023 International Conference on Smart Systems for applications
    in Electrical Sciences (ICSSES)*. IEEE, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rabiner and Juang (1986) Lawrence Rabiner and Biinghwang Juang. 1986. An introduction
    to hidden Markov models. *ieee assp magazine* 3, 1 (1986), 4–16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rahman et al. (2014) Shah Atiqur Rahman, Insu Song, M.K.H. Leung, Ickjai Lee,
    and Kyungmi Lee. 2014. Fast action recognition using negative space features.
    *Expert Systems with Applications* 41, 2 (2014), 574–587. [https://doi.org/10.1016/j.eswa.2013.07.082](https://doi.org/10.1016/j.eswa.2013.07.082)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redmon et al. (2016) Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
    Farhadi. 2016. You only look once: Unified, real-time object detection. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 779–788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015.
    Faster r-cnn: Towards real-time object detection with region proposal networks.
    *Advances in neural information processing systems* 28 (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riekert et al. (2020) Martin Riekert, Achim Klein, Felix Adrion, Christa Hoffmann,
    and Eva Gallmann. 2020. Automatically detecting pig position and posture by 2D
    camera imaging and deep learning. *Computers and Electronics in Agriculture* 174
    (2020), 105391. [https://doi.org/10.1016/j.compag.2020.105391](https://doi.org/10.1016/j.compag.2020.105391)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robinson et al. (2014) Timothy P Robinson, GR William Wint, Giulia Conchedda,
    Thomas P Van Boeckel, Valentina Ercoli, Elisa Palamara, Giuseppina Cinardi, Laura
    D’Aietti, Simon I Hay, and Marius Gilbert. 2014. Mapping the global distribution
    of livestock. *PloS one* 9, 5 (2014), e96084.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Romano and Stefanini (2021) Donato Romano and Cesare Stefanini (Eds.). 2021\.
    Biological Cybernetics. 115, issue 6 Animal-robot interaction and biohybrid organisms
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    2015. U-net: Convolutional networks for biomedical image segmentation. In *Medical
    Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International
    Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18*. Springer,
    234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruder (2017) Sebastian Ruder. 2017. An overview of multi-task learning in deep
    neural networks. *arXiv preprint arXiv:1706.05098* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Russello et al. (2022) Helena Russello, Rik van der Tol, and Gert Kootstra.
    2022. T-LEAP: Occlusion-robust pose estimation of walking cows using temporal
    information. *Computers and Electronics in Agriculture* 192 (2022), 106559.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saleh et al. (2022) Alzayat Saleh, Marcus Sheaves, Dean Jerry, and Mostafa Rahimi
    Azghadi. 2022. Adaptive uncertainty distribution in deep learning for unsupervised
    underwater image enhancement. *arXiv preprint arXiv:2212.08983* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saleh et al. (2023) Dema Saleh, Moemen Ahmed, Mai Zaafan, Yasmine Farouk, and
    Ayman Atia. 2023. A Pharmacology Toolkit for Animal Pose Estimation, Tracking
    and Analysis. In *2023 International Mobile, Intelligent, and Ubiquitous Computing
    Conference (MIUCC)*. IEEE, 1–7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samsudin et al. (2022) W Samsudin, MZ Harizan, MZ Ibrahim, RA Karim, and W Ibrahim.
    2022. Zebrafish larvae locomotor activity detection using Convolutional Neural
    Network (CNN). (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sandler et al. (2018) Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
    and Liang-Chieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    4510–4520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schneider et al. (2023) Steffen Schneider, Jin Hwa Lee, and Mackenzie Weygandt
    Mathis. 2023. Learnable latent embeddings for joint behavioural and neural analysis.
    *Nature* (2023), 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segalin et al. (2021) Cristina Segalin, Jalani Williams, Tomomi Karigo, May
    Hui, Moriel Zelikowsky, Jennifer J Sun, Pietro Perona, David J Anderson, and Ann
    Kennedy. 2021. The Mouse Action Recognition System (MARS) software pipeline for
    automated analysis of social behaviors in mice. *Elife* 10 (2021), e63720.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Selvaraju et al. (2016) Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna
    Vedantam, Michael Cogswell, Devi Parikh, and Dhruv Batra. 2016. Grad-CAM: Why
    did you say that? *arXiv preprint arXiv:1611.07450* (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shamoun-Baranes et al. (2017) Judy Shamoun-Baranes, Joseph B Burant, E Emiel
    van Loon, Willem Bouten, and CJ Camphuysen. 2017. Short distance migrants travel
    as far as long distance migrants in lesser black-backed gulls Larus fuscus. *Journal
    of Avian Biology* 48, 1 (2017), 49–57.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shaw and Lahrman (2023) Julie K Shaw and Sarah Lahrman. 2023. The human–animal
    bond–a brief look at its richness and complexities. *Canine and Feline Behavior
    for Veterinary Technicians and Nurses* (2023), 88–105.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman (2014) Karen Simonyan and Andrew Zisserman. 2014. Very
    deep convolutional networks for large-scale image recognition. *arXiv preprint
    arXiv:1409.1556* (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stowell (2022) Dan Stowell. 2022. Computational bioacoustics with deep learning:
    a review and roadmap. *PeerJ* 10 (2022), e13152.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
    2015. Going deeper with convolutions. In *Proceedings of the IEEE conference on
    computer vision and pattern recognition*. 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
    Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer
    vision. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*. 2818–2826.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tassinari et al. (2021) Patrizia Tassinari, Marco Bovo, Stefano Benni, Simone
    Franzoni, Matteo Poggi, Ludovica Maria Eugenia Mammi, Stefano Mattoccia, Luigi
    Di Stefano, Filippo Bonora, Alberto Barbaresi, et al. 2021. A computer vision
    approach based on deep learning for the detection of dairy cows in free stall
    barn. *Computers and Electronics in Agriculture* 182 (2021), 106030.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teixeira et al. (2023) Ana Cláudia Teixeira, José Ribeiro, Raul Morais, Joaquim J
    Sousa, and António Cunha. 2023. A Systematic Review on Automatic Insect Detection
    Using Deep Learning. *Agriculture* 13, 3 (2023), 713.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanh and Netramai (2022) Vu Quang Thanh and Chayakorn Netramai. 2022. Deep
    learning-based monitoring system for distress on mice using behavior analysis.
    In *2022 International Electrical Engineering Congress (iEECON)*. IEEE, 1–4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. (2019) Zhi Tian, Hao Chen, and Chunhua Shen. 2019. Directpose:
    Direct end-to-end multi-person pose estimation. *arXiv preprint arXiv:1911.07451*
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tucker Edmister et al. (2022) Sara Tucker Edmister, Thaís Del Rosario Hernández,
    Rahma Ibrahim, Cameron A Brown, Sayali V Gore, Rohit Kakodkar, Jill A Kreiling,
    and Robbert Creton. 2022. Novel use of FDA-approved drugs identified by cluster
    analysis of behavioral profiles. *Scientific Reports* 12, 1 (2022), 6120.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uchino and Ohwada (2021) Tom Uchino and Hayato Ohwada. 2021. Individual identification
    model and method for estimating social rank among herd of dairy cows using YOLOv5\.
    In *2021 IEEE 20th International Conference on Cognitive Informatics & Cognitive
    Computing (ICCI* CC)*. IEEE, 235–241.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ullah et al. (2022) Naeem Ullah, Javed Ali Khan, Lubna Abdulaziz Alharbi, Asaf
    Raza, Wahab Khan, and Ijaz Ahmad. 2022. An efficient approach for crops pests
    recognition and classification based on novel DeepPestNet deep learning model.
    *IEEE Access* 10 (2022), 73019–73032.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varma et al. (2021) Alluri LSV Siddhartha Varma, Vishal Bateshwar, Anubuthi
    Rathi, and Anurag Singh. 2021. Acoustic Classification of Insects using Signal
    Processing and Deep Learning Approaches. In *2021 8th International Conference
    on Signal Processing and Integrated Networks (SPIN)*. IEEE, 1048–1052.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Juan Wang, Nan Wang, Lihua Li, and Zhenhui Ren. 2020. Real-time
    behavior detection and judgment of egg breeders based on YOLO v3. *Neural Computing
    and Applications* 32 (2020), 5471–5481.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021b) Kui Wang, Pei Wu, Hongmei Cui, Chuanzhong Xuan, and He Su.
    2021b. Identification and classification for sheep foraging behavior based on
    acoustic signal and deep learning. *Computers and Electronics in Agriculture*
    187 (2021), 106275.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021a) Xingqi Wang, Chen Du, Ying Wang, Sheng Hu, and Yuliang Zhao.
    2021a. Behavioral Recognition of Mice Based on a Deep Network. In *2021 IEEE 11th
    Annual International Conference on CYBER Technology in Automation, Control, and
    Intelligent Systems (CYBER)*. IEEE, 840–844.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weber et al. (2022) Rebecca Z Weber, Geertje Mulders, Julia Kaiser, Christian
    Tackenberg, and Ruslan Rust. 2022. Deep learning-based behavioral profiling of
    rodent stroke recovery. *BMC biology* 20, 1 (2022), 1–19.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wijeyakulasuriya et al. (2020) Dhanushi A Wijeyakulasuriya, Elizabeth W Eisenhauer,
    Benjamin A Shaby, and Ephraim M Hanks. 2020. Machine learning for modeling animal
    movement. *Plos one* 15, 7 (2020), e0235750.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wittek et al. (2023) Neslihan Wittek, Kevin Wittek, Christopher Keibel, and
    Onur Güntürkün. 2023. Supervised machine learning aided behavior classification
    in pigeons. *Behavior Research Methods* 55, 4 (2023), 1624–1640.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wojke et al. (2017) Nicolai Wojke, Alex Bewley, and Dietrich Paulus. 2017. Simple
    online and realtime tracking with a deep association metric. In *2017 IEEE international
    conference on image processing (ICIP)*. IEEE, 3645–3649.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2020) Anqi Wu, Estefany Kelly Buchanan, Matthew Whiteway, Michael
    Schartner, Guido Meijer, Jean-Paul Noel, Erica Rodriguez, Claire Everett, Amy
    Norovich, Evan Schaffer, et al. 2020. Deep Graph Pose: a semi-supervised deep
    graphical model for improved animal pose tracking. *Advances in Neural Information
    Processing Systems* 33 (2020), 6040–6052.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2016) Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng.
    2016. Quantized convolutional neural networks for mobile devices. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 4820–4828.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2023) Shiting Xiao, Yufu Wang, Ammon Perkes, Bernd Pfrommer, Marc
    Schmidt, Kostas Daniilidis, and Marc Badger. 2023. Multi-view Tracking, Re-ID,
    and Social Network Analysis of a Flock of Visually Similar Birds in an Outdoor
    Aviary. *International Journal of Computer Vision* 131, 6 (2023), 1532–1549.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2020) Weitao Xu, Xiang Zhang, Lina Yao, Wanli Xue, and Bo Wei. 2020.
    A multi-view CNN-based acoustic classification system for automatic animal species
    identification. *Ad Hoc Networks* 102 (2020), 102115.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2022) Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. 2022.
    Vitpose: Simple vision transformer baselines for human pose estimation. *Advances
    in Neural Information Processing Systems* 35 (2022), 38571–38584.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. 2023.
    ViTPose++: Vision Transformer for Generic Body Pose Estimation. *IEEE Transactions
    on Pattern Analysis and Machine Intelligence* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xudong et al. (2020) Zhang Xudong, Kang Xi, Feng Ningning, and Liu Gang. 2020.
    Automatic recognition of dairy cow mastitis from thermal images by a deep learning
    detector. *Computers and Electronics in Agriculture* 178 (2020), 105754.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yamada et al. (2020) Jun Yamada, John Shawe-Taylor, and Zafeirios Fountas. 2020.
    Evolution of a complex predator-prey ecosystem on large-scale multi-agent deep
    reinforcement learning. In *2020 International Joint Conference on Neural Networks
    (IJCNN)*. IEEE, 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yamaguchi et al. (2018) Shoichiro Yamaguchi, Honda Naoki, Muneki Ikeda, Yuki
    Tsukada, Shunji Nakano, Ikue Mori, and Shin Ishii. 2018. Identification of animal
    behavioral strategies by inverse reinforcement learning. *PLoS computational biology*
    14, 5 (2018), e1006122.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2022) Yuxiang Yang, Junjie Yang, Yufei Xu, Jing Zhang, Long Lan,
    and Dacheng Tao. 2022. Apt-36k: A large-scale benchmark for animal pose estimation
    and tracking. *Advances in Neural Information Processing Systems* 35 (2022), 17301–17313.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2022) Shaokai Ye, Anastasiia Filippova, Jessy Lauer, Maxime Vidal,
    Steffen Schneider, Tian Qiu, Alexander Mathis, and Mackenzie Weygandt Mathis.
    2022. SuperAnimal models pretrained for plug-and-play analysis of animal behavior.
    *arXiv preprint arXiv:2203.07436* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2021) Hang Yu, Yufei Xu, Jing Zhang, Wei Zhao, Ziyu Guan, and Dacheng
    Tao. 2021. AP-10K: A Benchmark for Animal Pose Estimation in the Wild. arXiv:2108.12617 [cs.CV]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020) Kaifeng Zhang, Dan Li, Jiayun Huang, and Yifei Chen. 2020.
    Automated video behavior recognition of pigs using two-stream convolutional networks.
    *Sensors* 20, 4 (2020), 1085.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2023) Ce Zheng, Wenhan Wu, Chen Chen, Taojiannan Yang, Sijie
    Zhu, Ju Shen, Nasser Kehtarnavaz, and Mubarak Shah. 2023. Deep learning-based
    human pose estimation: A survey. *Comput. Surveys* 56, 1 (2023), 1–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Feixiang Zhou, Xinyu Yang, Fang Chen, Long Chen, Zheheng
    Jiang, Hui Zhu, Reiko Heckel, Haikuan Wang, Minrui Fei, and Huiyu Zhou. 2022.
    Cross-Skeleton Interaction Graph Aggregation Network for Representation Learning
    of Mouse Social Behaviour. *arXiv preprint arXiv:2208.03819* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2019) Yi Zhu, Zhenzhong Lan, Shawn Newsam, and Alexander Hauptmann.
    2019. Hidden two-stream convolutional networks for action recognition. In *Computer
    Vision–ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia,
    December 2–6, 2018, Revised Selected Papers, Part III 14*. Springer, 363–378.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
