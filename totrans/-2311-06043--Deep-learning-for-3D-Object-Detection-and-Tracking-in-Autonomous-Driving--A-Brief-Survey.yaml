- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:36:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:36:01
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2311.06043] Deep learning for 3D Object Detection and Tracking in Autonomous
    Driving: A Brief Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2311.06043] 深度学习在自动驾驶中的3D目标检测与跟踪：简要调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.06043](https://ar5iv.labs.arxiv.org/html/2311.06043)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.06043](https://ar5iv.labs.arxiv.org/html/2311.06043)
- en: 'Deep learning for 3D Object Detection and Tracking in Autonomous Driving: A
    Brief Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在自动驾驶中的3D目标检测与跟踪：简要调查
- en: Yang Peng
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 杨鹏
- en: Southern University of Science and Technology
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 南方科技大学
- en: 12032453@mail.sustech.edu.cn
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 12032453@mail.sustech.edu.cn
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Object detection and tracking are vital and fundamental tasks for autonomous
    driving, aiming at identifying and locating objects from those predefined categories
    in a scene. 3D point cloud learning has been attracting more and more attention
    among all other forms of self-driving data. Currently, there are many deep learning
    methods for 3D object detection. However, the tasks of object detection and tracking
    for point clouds still need intensive study due to the unique characteristics
    of point cloud data. To help get a good grasp of the present situation of this
    research, this paper shows recent advances in deep learning methods for 3D object
    detection and tracking.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测和跟踪是自动驾驶中至关重要的基本任务，旨在从场景中的预定义类别中识别和定位对象。3D 点云学习在所有自驾数据形式中越来越受到关注。目前，有许多深度学习方法用于3D目标检测。然而，由于点云数据的独特特性，点云的目标检测和跟踪任务仍需深入研究。为了帮助更好地掌握目前的研究现状，本文展示了深度学习方法在3D目标检测和跟踪中的最新进展。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Object detection occupies an important position in the ﬁeld of computer vision
    all along. As the cornerstone of image understanding, object detection is widely
    applied in large numbers of areas such as autonomous driving and robot vision.
    Object detection enables an autonomous driving system to see those driving environments
    clearly and understand what are they just like human drivers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测在计算机视觉领域一直占据着重要位置。作为图像理解的基石，目标检测广泛应用于自动驾驶、机器人视觉等大量领域。目标检测使自动驾驶系统能够清晰地看清驾驶环境，并理解这些环境中的事物，就像人类司机一样。
- en: With the fast development of deep learning [[1](#bib.bib1)], it becomes easier
    to learn complex, subtle and abstract features for a deep learning model without
    manual feature extraction like the traditional way. In view of the excellent ability
    of deep learning to process data, major progress has been made in the research
    of object detection.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的快速发展[[1](#bib.bib1)]，深度学习模型可以更容易地学习复杂、微妙和抽象的特征，而不需要像传统方法那样进行人工特征提取。鉴于深度学习在处理数据方面的卓越能力，目标检测的研究取得了重大进展。
- en: 'The research for 2D object detection has been well-developed [[2](#bib.bib2)].
    The accuracy and efﬁciency of algorithms proposed for solving 2D object detection
    problems have reached a high level and those methods play an important role in
    engineering practice. Traditional detection strategies based on sliding windows
    like [[3](#bib.bib3), [4](#bib.bib4)] keep the mainstream for a long time before
    methods combining deep learning techniques. In this stage when deep learning hasn’t
    been applied to detection, the pipeline[[5](#bib.bib5)] of object detection methods
    mentioned above usually can be categorized into three parts: i) proposal generation;
    ii) feature vector extraction; iii) region classification [[6](#bib.bib6)]. The
    main task of proposal generation is to search the whole image to ﬁnd those positions
    that might contain expected objects. These positions are called regions of interest
    (ROI). The sliding window technique is an intuitive idea proposed to scan through
    the input image and ﬁnd ROI. In the second stage, algorithms will extract a constant-length
    feature vector from each target position of the image. Histogram of Gradients[[3](#bib.bib3)]
    is one of the most popular feature extraction methods proposed by Navneet Dalal
    and Bill Triggs. In general, linear support vector machines are used together
    with HOG, achieving region classiﬁcation.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于2D物体检测的研究已经得到了很好的发展[[2](#bib.bib2)]。为解决2D物体检测问题而提出的算法的准确性和效率已经达到了很高的水平，这些方法在工程实践中发挥着重要作用。在结合深度学习技术的方法之前，基于滑动窗口的传统检测策略[[3](#bib.bib3),
    [4](#bib.bib4)]在很长一段时间内占据了主流。在深度学习尚未应用于检测的阶段，前述物体检测方法的流程[[5](#bib.bib5)]通常可以分为三个部分：i)
    提议生成；ii) 特征向量提取；iii) 区域分类[[6](#bib.bib6)]。提议生成的主要任务是扫描整个图像，找到可能包含期望物体的位置。这些位置被称为兴趣区域（ROI）。滑动窗口技术是一种直观的思想，用于扫描输入图像并寻找ROI。在第二阶段，算法将从图像的每个目标位置提取一个固定长度的特征向量。梯度直方图[[3](#bib.bib3)]是Navneet
    Dalal和Bill Triggs提出的最流行的特征提取方法之一。一般来说，线性支持向量机与HOG一起使用，完成区域分类。
- en: With these traditional detectors, much success has been achieved in object detection.
    Nevertheless, there are still some limitations here. The huge search space and
    expensive computations of methods based on sliding windows encourage better solutions.
    Moreover, one shortage which should not be overlooked is that the procedure of
    design and optimize a detector is separate [[7](#bib.bib7)]. This may lead to
    local optimal solution for the system.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些传统探测器，已经在物体检测方面取得了很大的成功。然而，这里仍然存在一些限制。基于滑动窗口的方法具有巨大的搜索空间和昂贵的计算成本，这促使了更好的解决方案。此外，不容忽视的一个短板是设计和优化探测器的过程是分开的[[7](#bib.bib7)]。这可能会导致系统的局部最优解。
- en: '![Refer to caption](img/39c3e85c63793beb77ad46dca86ed821.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/39c3e85c63793beb77ad46dca86ed821.png)'
- en: 'Figure 1: Major developments and milestones of object detection techniques.
    In 2012 there occurred a turning point drawing a line between traditional and
    deep learning-based object detection methods.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：物体检测技术的主要发展和里程碑。在2012年，出现了一个转折点，划分了传统方法和基于深度学习的方法之间的界限。
- en: 'Since deep learning emerged in this ﬁeld, it has become more capable of detecting
    objects. These object detection systems using deep learning techniques can handle
    questions with more facility and rapidity than in the past. We observe a turning
    point in 2012 with the development of DCNN for image classiﬁcation by Krizhevsky
    et al.[[8](#bib.bib8)]. Since this time, there has been an incessant springing
    up of approaches based on deep learning. Currently, the mainstream of object detection
    algorithms can be divided into two types: i)two-stage methods, such as Region-based
    CNN (R-CNN) [[9](#bib.bib9)] and its variants[[10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12)]; ii)one-stage methods such as You Only Look Once (YOLO)[[13](#bib.bib13)]
    and its variants[[14](#bib.bib14)]. Two-stage methods are one kind of region proposal-based
    methods, which propose some possible regions that contain objects ﬁrstly and extract
    feature vectors from these regions respectively. One-stage methods predict the
    categories of objects on the location of the feature maps directly to the contrary,
    discarding the region classiﬁcation steps. The different kinds of methods have
    their own strong points. Fast-RCNN[[10](#bib.bib10)] cut down the processing time
    of the previous network. However, a bottleneck bothers people a lot because the
    computation of this network is quite expensive. Until the appearance of Faster-RCNN[[12](#bib.bib12)]
    this problem gets solved. 2D detectors are not the key point in this paper. For
    the integrity of the whole review and introduction of 3D object detection methods,
    we mention about those methods. Here we present the main progress and milestones
    of 2D object detection techniques in Figure[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Deep learning for 3D Object Detection and Tracking in Autonomous Driving: A
    Brief Survey").'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自从深度学习在这一领域出现以来，它在检测物体方面变得更为强大。这些使用深度学习技术的物体检测系统能够比以往更迅速、更高效地处理问题。我们观察到2012年是一个转折点，当时Krizhevsky等人开发了用于图像分类的DCNN[[8](#bib.bib8)]。从那时起，基于深度学习的方法不断涌现。目前，主流的物体检测算法可以分为两类：i)
    两阶段方法，如基于区域的CNN（R-CNN）[[9](#bib.bib9)]及其变体[[10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12)]；ii) 一阶段方法，如“你只看一次”（YOLO）[[13](#bib.bib13)]及其变体[[14](#bib.bib14)]。两阶段方法是一种基于区域提议的方法，它首先提出一些可能包含物体的区域，并从这些区域分别提取特征向量。相反，一阶段方法直接在特征图的位置上预测物体的类别，省略了区域分类步骤。这些不同类型的方法各有优点。Fast-RCNN[[10](#bib.bib10)]减少了之前网络的处理时间。然而，一个瓶颈问题困扰了人们，因为这个网络的计算成本相当高。直到Faster-RCNN[[12](#bib.bib12)]的出现，这个问题才得到解决。2D检测器不是本文的重点。为了完整介绍3D物体检测方法的综述，我们提到了这些方法。这里展示了2D物体检测技术的主要进展和里程碑，如图[1](#S1.F1
    "图1 ‣ 1 介绍 ‣ 深度学习在自动驾驶中的3D物体检测与跟踪：简要调查")。
- en: However, with new application scenarios such as robot vision and autonomous
    driving proposed [[15](#bib.bib15)], the implementation of 2D object detection
    is far from enough. The process of images captured by cameras is projecting 3D
    space into 2D view, which causes the loss of 3D spatial information and is unable
    to satisfy people. More 3D spatial information needs to be considered. With the
    rapid development of many 3D techniques, large amounts of 3D sensors such as LiDARs,
    3D scanners and RGB-D cameras are becoming increasingly available and affordable
    [[16](#bib.bib16)]. In this review, we mainly analyse point clouds obtained by
    using LiDARs. As a common format, each point provides us with useful geometric
    position information and some may contain RGB information. There is an urgent
    demand for 3D object detection and tracking when it comes to autonomous driving.
    In the research of autonomous driving, accurate environmental perception and precise
    location are the keys for an autonomous driving system to achieve reliable navigation,
    information decisions and safe driving in complex and dynamic environments. Compared
    to images whose data quality is influenced by illumination, point cloud is robust
    for different lighting conditions [19]. As a result, point cloud data is widely
    used in this ﬁeld.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着诸如机器人视觉和自动驾驶等新应用场景的提出[[15](#bib.bib15)]，2D目标检测的实施远远不够。相机捕捉的图像是将3D空间投影到2D视图中，这导致了3D空间信息的丢失，无法满足需求。需要考虑更多的3D空间信息。随着众多3D技术的快速发展，大量的3D传感器如LiDAR、3D扫描仪和RGB-D摄像头变得越来越可用且价格亲民[[16](#bib.bib16)]。在这篇综述中，我们主要分析使用LiDAR获得的点云。作为一种常见格式，每个点提供了有用的几何位置信息，有些点还可能包含RGB信息。对于自动驾驶来说，对3D目标检测和跟踪的需求非常迫切。在自动驾驶研究中，准确的环境感知和精确的定位是实现可靠导航、信息决策和安全驾驶的关键。与受照明影响的数据质量的图像相比，点云对于不同的光照条件更具鲁棒性[19]。因此，点云数据在这一领域得到了广泛应用。
- en: 'On the basis of 2D object detection, changes have taken place in the questions
    and requirements of object detection. For the sparsity and the irregularity of
    point clouds, it is impossible to apply 2D object detection methods to 3D point
    clouds. As a result, 2D object detection methods need to be changed to make sure
    those methods can be extended to 3D cases. Similar to the object detection approaches
    applied in images, these 3D object detection methods still can be divided into
    two categories: two-stage methods and one-stage methods. Figure[2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Deep learning for 3D Object Detection and Tracking in Autonomous
    Driving: A Brief Survey") presents a simple classiﬁcation of recent popular 3D
    object detection methods based on deep learning techniques. More details of these
    3D detection methods can be found later Section 3 of this paper.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '在2D目标检测的基础上，目标检测的问题和需求发生了变化。由于点云的稀疏性和不规则性，2D目标检测方法无法直接应用于3D点云。因此，需要对2D目标检测方法进行改进，以确保这些方法可以扩展到3D情况。类似于应用于图像的目标检测方法，这些3D目标检测方法仍然可以分为两类：两阶段方法和一阶段方法。图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Deep learning for 3D Object Detection and Tracking
    in Autonomous Driving: A Brief Survey")展示了基于深度学习技术的近期流行3D目标检测方法的简单分类。这些3D检测方法的更多细节可以在本文的第3节中找到。'
- en: '![Refer to caption](img/5e29a2d7919b3717b00d87c361e9a256.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5e29a2d7919b3717b00d87c361e9a256.png)'
- en: 'Figure 2: Summary of major deep learning-based methods of 3D object detection.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于深度学习的3D目标检测主要方法总结。
- en: For this paper, we mainly focus on these state-of-the-art methods of 3D object
    detection on the basis of point clouds. The remaining parts of this review are
    organized as follows. Related backgrounds, including the problems identiﬁcation,
    the questions description, the key challenges and the motivation of this review
    are stated in Section LABEL:Background. In the third part of this paper, we make
    a detailed interpretation of those current approaches for 3D object detection,
    including their advantages and disadvantages. Section 4 makes a summary of recent
    developments in deep learning methods for point clouds. Furthermore, we point
    out some difﬁcult problems still unsolved on the basis of existing methods and
    provide several possible research directions that might make sense.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本文，我们主要关注基于点云的3D物体检测的最先进方法。本文的其余部分组织如下。相关背景，包括问题识别、问题描述、关键挑战以及本次综述的动机，都在第
    LABEL:背景 节中说明。本文的第三部分详细解释了当前的3D物体检测方法，包括它们的优缺点。第4节总结了点云深度学习方法的最新发展。此外，我们指出了现有方法中仍未解决的一些难题，并提供了若干可能有意义的研究方向。
- en: '![Refer to caption](img/c2e3ab9196474698609a42feadfe1304.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c2e3ab9196474698609a42feadfe1304.png)'
- en: 'Figure 3: Bounding boxes.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：边界框。
- en: 2 Background
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: For the purpose of helping understand how exactly modern deep learning techniques
    can be applied to solve 3D object detection problems, this section describes some
    simple conceptions of the research object and the data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助理解现代深度学习技术如何应用于解决3D物体检测问题，本节描述了一些关于研究对象和数据的简单概念。
- en: 2.1 The Problem
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题
- en: The problem of 3D object detection, which is similar to 2D detection, can be
    formulated as follows. Given some point cloud data, a 3D detector needs to determine
    whether or not there are instances from those predeﬁned categories and if yes,
    locate where they are. One thing we should ﬁrst consider is these predeﬁned categories,
    which are problem-dependent. Considering the case of autonomous driving, the research
    community is more interested in those trafﬁc participants (e.g., bicycles, cars
    and pedestrians). In general, unstructured scenes such as sky and clouds are often
    not our research objects.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 3D物体检测的问题，类似于2D检测，可以表述如下。给定一些点云数据，一个3D检测器需要确定这些预定义类别中是否存在实例，如果存在，则定位它们的位置。我们首先需要考虑的是这些预定义的类别，这些类别是与问题相关的。考虑到自动驾驶的情况，研究界对这些交通参与者（例如，
    bicycles、汽车和行人）更感兴趣。一般来说，天空和云等非结构化场景通常不是我们的研究对象。
- en: 'In general, a 3D bounding box is used to indicate the location of those 3D
    objects. It is a rectangular cuboid placed in 3D spaces and usually has three
    types of representation methods, such as axis-aligned 3D centre offset method[[17](#bib.bib17)],
    8-corners method[[18](#bib.bib18)] and 4-corner-2-height method[[19](#bib.bib19)].
    In general, the ﬁnal result of 3D object detection is using 3D bounding boxes
    to earmark 3D objects, as shown in Figure[3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Deep learning for 3D Object Detection and Tracking in Autonomous Driving: A
    Brief Survey").'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，3D边界框用于指示这些3D物体的位置。它是一个放置在3D空间中的矩形长方体，通常有三种表示方法，如轴对齐的3D中心偏移方法[[17](#bib.bib17)]、8角点方法[[18](#bib.bib18)]和4角2高方法[[19](#bib.bib19)]。通常，3D物体检测的最终结果是使用3D边界框标记3D物体，如图[3](#S1.F3
    "图 3 ‣ 1 介绍 ‣ 用于自动驾驶的3D物体检测和跟踪的深度学习：简要综述")所示。
- en: 2.2 Main Challenges
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 主要挑战
- en: Generic object detection methods aim at recognizing and localizing interested
    object categories. In most computer vision problems, people care about accuracy
    and efﬁciency mostly. Object detection is no exception.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通用物体检测方法旨在识别和定位感兴趣的物体类别。在大多数计算机视觉问题中，人们主要关注准确性和效率。物体检测也不例外。
- en: Those point cloud data acquired from LiDARs, depth cameras and binocular cameras
    can be used in 3D object detection. However, with the increase in the distance
    between objects and cameras, there would be a sudden drop in the density of the
    point clouds, causing a huge change in the density. What’s more, a few portions
    of the objects might be invisible owing to the occlusion, which causes a problem
    that there exists a large lag in the distribution of the point clouds of the same
    object. To sum up, the representations of point clouds are quite different and
    hard for a detector to make fairly accurate detections.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从 LiDAR、深度相机和双目相机获取的点云数据可以用于 3D 物体检测。然而，随着物体与相机之间距离的增加，点云的密度会突然下降，导致密度发生巨大变化。此外，由于遮挡，物体的某些部分可能会不可见，这造成了点云分布存在较大的滞后问题。总而言之，点云的表示差异很大，使得检测器很难进行准确的检测。
- en: Another point we should notice is the sparsity and the irregularity of point
    clouds. The order of those points of the same object is greatly influenced by
    different acquisition devices and different coordinate systems. Those irregular
    cloud points make it really hard for an end-to-end model to handle. In addition,
    compared with the large scale of scenes, the coverage of LiDAR sampling points
    has strong sparsity. It is well-known that with the rapid development of the artiﬁcial
    intelligence, deep neural network is widely used in most tasks of autonomous driving
    because of their high accuracy and strong robustness. The performance of deep
    neural networks [[20](#bib.bib20), [21](#bib.bib21)] used in the ﬁeld of 2D object
    detection is much better than other types of algorithms. Nevertheless, we have
    just mentioned the characteristics of point clouds, which result in the loss of
    efﬁcacy of those deep learning methods. This is why the movement of research in
    deep learning-based 3D object detection is slow. As a result, in the stage of
    data preprocessing, how to represent sparse point cloud data for better use deserves
    to be investigated thoroughly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应注意点云的稀疏性和不规则性。同一物体的这些点的顺序受不同采集设备和坐标系统的影响很大。这些不规则的点云使得端到端模型处理起来非常困难。此外，与大规模场景相比，LiDAR
    采样点的覆盖具有较强的稀疏性。众所周知，随着人工智能的快速发展，深度神经网络因其高准确性和强鲁棒性在大多数自动驾驶任务中被广泛使用。深度神经网络在 2D 物体检测领域的表现[[20](#bib.bib20),
    [21](#bib.bib21)]远远优于其他类型的算法。然而，我们刚刚提到的点云特性导致这些深度学习方法的效能下降。这就是深度学习基于 3D 物体检测的研究进展缓慢的原因。因此，在数据预处理阶段，如何表示稀疏的点云数据以便更好地利用是值得深入研究的问题。
- en: Despite the useful information such as depth and spatial information that point
    clouds have, it seems that utilizing image data at the same time performs better
    sometimes. Therefore a number of methods combining LiDAR point clouds and images
    are well developed. In this paper, we mainly talk about deep learning methods
    for point clouds. For the completeness of this paper, some fusion methods which
    use 2D images as well will also be included.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管点云具有诸如深度和空间信息等有用信息，但有时同时利用图像数据似乎效果更佳。因此，许多结合 LiDAR 点云和图像的方法得到了良好的发展。在本文中，我们主要讨论点云的深度学习方法。为了本文的完整性，使用
    2D 图像的某些融合方法也将被包括在内。
- en: 2.3 Motivation
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 动机
- en: Related methods mentioned in this paper are organized based on their different
    algorithm execution processes. In other words, we mainly focus on whether a detector
    ﬁrst generates proposals. This paper intends to investigate state-of-the-art research
    about 3D object detection for point clouds and categorize those methods, presenting
    a comprehensive summary of recent advances based on deep learning technologies
    for point clouds. It also covers the comparative advantages and disadvantages
    of different methods, by observing those questions still unsolved to inspire more
    future possible research directions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提到的相关方法是根据其不同的算法执行过程进行组织的。换句话说，我们主要关注检测器是否首先生成提议。本文旨在调查有关点云 3D 物体检测的最新研究，并对这些方法进行分类，提供基于深度学习技术的点云研究的全面总结。它还涵盖了不同方法的比较优缺点，通过观察仍未解决的问题以启发未来可能的研究方向。
- en: 3 Methods
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 3.1 Two-Stage Methods
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 两阶段方法
- en: 'Two-stage detectors ﬁrst detect a number of possible regions also called proposals
    which contain objects, then make predictions for extracted features. According
    to the introduction of [[22](#bib.bib22)], we further categorize those two-stage
    methods into three kinds: i) multi-view-based; ii) segmentation-based; iii) frustum-based
    methods.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 两阶段检测器首先检测包含目标的一些可能区域，也称为提案，然后对提取的特征进行预测。根据[[22](#bib.bib22)]的介绍，我们将这两阶段方法进一步分类为三种：i)
    基于多视角的；ii) 基于分割的；iii) 基于截锥的。
- en: 'Multi-View Methods. It is necessary to realize that texture information, which
    is important for class discrimination is not included in point clouds. By contrast,
    monocular images cannot provide us with depth information for accurate 3D localization
    and size estimation. So multi-view methods try to use different modalities to
    improve the performance. A deep fusion combines region-wise features from multiple
    views (e.g., bird’s eye view (BEV), LiDAR front view (FV) and images) and gets
    oriented 3D boxes, as shown in Figure [4(a)](#S3.F4.sf1 "In Figure 4 ‣ 3.1 Two-Stage
    Methods ‣ 3 Methods ‣ Deep learning for 3D Object Detection and Tracking in Autonomous
    Driving: A Brief Survey").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 多视角方法。需要意识到，点云中不包含对类别区分重要的纹理信息。相反，单目图像无法提供准确的 3D 定位和大小估计所需的深度信息。因此，多视角方法尝试使用不同的模态来提高性能。深度融合结合来自多个视角的区域特征（例如，鸟瞰图（BEV）、LiDAR
    前视图（FV）和图像），并获得定向的 3D 框，如图[4(a)](#S3.F4.sf1 "在图 4 ‣ 3.1 两阶段方法 ‣ 3 方法 ‣ 深度学习在自动驾驶中的
    3D 目标检测与跟踪：简要调查")所示。
- en: '![Refer to caption](img/8743cd434874790f8c7462cdba15eb8d.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8743cd434874790f8c7462cdba15eb8d.png)'
- en: (a) MV3D
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MV3D
- en: '![Refer to caption](img/05b10386b0dd12033029ab7a55651b30.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/05b10386b0dd12033029ab7a55651b30.png)'
- en: (b) PointRCNN
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: (b) PointRCNN
- en: '![Refer to caption](img/664c23c1a057d998f81b52b1a0fb0cbb.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/664c23c1a057d998f81b52b1a0fb0cbb.png)'
- en: (c) Frustum PointNet
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Frustum PointNet
- en: 'Figure 4: Typical networks for 3D object detection methods.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：3D 目标检测方法的典型网络。
- en: 'One remarkable contribution made by Chen et al. [[18](#bib.bib18)] is their
    multi-view 3D object detection network (MV3D). In terms of the expensive computations
    of existing LiDAR-based 3D methods, the disadvantage of current image-based methods
    and the limitation of existing multi-modal fusion methods, this work tries to
    propose a method to overcome those disadvantages preventing the development of
    3D object detection. The MV3D is composed of two subnetworks: one is a 3D proposal
    network to generate 3D proposals and another subnetwork is a deep fusion network,
    whose main function is trying to fuse multi-view feature proposals. They use 3D
    proposals and project them to three views. Theoretically, it is of great useful
    that 3D proposals can be projected to any view in 3D space. In their experiments,
    LiDAR-based methods achieve an average precision of 87.65% at an Intersection-over-Union
    (IOU) of 0.5 on the KITTI validation set, which obtains 30% higher over VeloFCN
    [[23](#bib.bib23)]. Besides, the authors illustrate the construction process of
    BEV and state three advantages of BEV, which casts light on their search for a
    bird’s eye view and has a certain reference value for future research. Nevertheless,
    this model is not perfect because the whole procedure for detection is too slow
    to have practical applications. This region proposal network(RPN) is not suitable
    for small object instances in BEV. Small objects occupy a fraction of a pixel
    in a feature map, causing insufﬁcient data to extract features. Subsequently,
    a number of endeavours have been made to improve the MV3D model.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人[[18](#bib.bib18)]做出的一个显著贡献是他们的多视角 3D 目标检测网络（MV3D）。考虑到现有 LiDAR 基于 3D 方法的高昂计算成本、当前图像基方法的缺陷以及现有多模态融合方法的局限性，该工作试图提出一种方法来克服这些阻碍
    3D 目标检测发展的缺点。MV3D 由两个子网络组成：一个是 3D 提案网络，用于生成 3D 提案，另一个子网络是深度融合网络，其主要功能是尝试融合多视角特征提案。他们使用
    3D 提案并将其投影到三个视图中。理论上，将 3D 提案投影到 3D 空间中的任何视图中是非常有用的。在他们的实验中，LiDAR 基于的方法在 KITTI
    验证集上以 0.5 的交并比实现了 87.65% 的平均精度，比 VeloFCN [[23](#bib.bib23)] 高出 30%。此外，作者阐述了 BEV
    的构建过程，并说明了 BEV 的三个优势，这为他们寻找鸟瞰图提供了启示，对未来的研究具有一定的参考价值。然而，这个模型并不完美，因为整个检测过程过于缓慢，无法用于实际应用。该区域提案网络
    (RPN) 不适合 BEV 中的小物体实例。小物体在特征图中占据了一个像素的一部分，导致数据不足以提取特征。随后，已经进行了许多努力来改进 MV3D 模型。
- en: Several attempts are made on the efﬁciency of information fusion by applying
    different modalities. Ku et al. [[19](#bib.bib19)] proposed an aggregation view
    object detection network (AVOD), which uses LIDAR point clouds and RGB images
    as well. Unlike MV3D it extends ROI feature fusion to the stage of proposal generation.
    Their RPN has a novel architecture having the ability to ﬁnish multi-modal feature
    fusion on high-resolution feature maps, thus it is convenient to generate accurate
    region proposals for tiny objects in scenes. Furthermore, they test the performance
    of AVOD on the KITTI Object Detection Benchmark [[24](#bib.bib24)] and the result
    shows that AVOD can run in real-time and with a low memory expense. However, ROI
    feature fusion is conﬁned to high-level feature maps. Furthermore, only those
    features extracted from selected object regions will be fused. ContFuse [[25](#bib.bib25)]
    was developed to overcome those drawbacks. They exploited continuous convolutions
    to fuse feature maps at different resolutions. With the projection of the LIDAR
    points image and BEV spaces can correspond with each other. In other words, in
    the BEV spaces corresponding image features for each point can be extracted and
    projecting image features into the BEV plane can obtain dense BEV feature maps.
    However, extremely sparse point clouds set limits on this kind of fusion. Liang
    et al. [[26](#bib.bib26)] proposed an object detection network for multiple tasks
    (e.g., 3D object detection, ground estimation and depth completion) with multi-sensors.
    This paper utilizes the strong points from both point-wise and ROI-wise feature
    fusion. Speciﬁcally, the implementation of multiple tasks helps whole network
    learn better representations. Consequently, the KITTI and TOR4D datasets are used
    for validation of this approach, which proves to achieve outstanding improvement
    on detection problems and outperforms state-of-the-art approaches in the past.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用不同的模态进行信息融合的效率进行了一些尝试。Ku等人[[19](#bib.bib19)]提出了一种聚合视图物体检测网络（AVOD），同时使用LIDAR点云和RGB图像。与MV3D不同的是，它将ROI特征融合扩展到提议生成阶段。他们的RPN具有一种新颖的架构，能够在高分辨率特征图上完成多模态特征融合，从而方便地生成准确的区域提议用于场景中的小物体。此外，他们在KITTI物体检测基准[[24](#bib.bib24)]上测试了AVOD的性能，结果表明AVOD可以实时运行，并且内存消耗较低。然而，ROI特征融合仅限于高级特征图。此外，仅从选择的物体区域提取的那些特征会被融合。ContFuse[[25](#bib.bib25)]被开发用来克服这些缺点。他们利用连续卷积来融合不同分辨率的特征图。通过LIDAR点云图像和BEV空间的投影可以相互对应。换句话说，在BEV空间中，可以提取每个点的相应图像特征，并将图像特征投影到BEV平面上可以获得密集的BEV特征图。然而，极其稀疏的点云限制了这种融合。Liang等人[[26](#bib.bib26)]提出了一种用于多任务（例如3D物体检测、地面估计和深度补全）的物体检测网络，配备多传感器。本文利用了点级和ROI级特征融合的优势。具体来说，多任务的实施有助于整个网络学习更好的表示。因此，使用KITTI和TOR4D数据集对该方法进行了验证，结果证明在检测问题上取得了显著改进，并超越了过去的最先进方法。
- en: Another direction for improvement is to explore how to extract robust representations
    of the inputs. A novel Spatial-Channel Attention Network (SCANet) [[27](#bib.bib27)]
    was proposed aiming at achieving high accuracy 3D object detection. In this paper,
    they raise a brand-new Spatial-Channel Attention (SCA) module and an Extension
    Spatial Upsample (ESU) module for 3D region proposal use. The former module can
    focus on global and multi-scale contexts in a scene and can capture those discriminative
    features. The latter module combines different scale low-level features and produces
    reliable 3D region proposals. In addition, a way to fuse those features better
    is to apply a new multi-level fusion scheme allowing more interactions between
    them. Finally, the experimental results present that at a speed of 11.1 FPS their
    method is 5 times faster than MV3D.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个改进方向是探讨如何提取输入的鲁棒表示。提出了一种新颖的空间通道注意力网络（SCANet）[[27](#bib.bib27)]，旨在实现高精度的3D物体检测。本文提出了全新的空间通道注意力（SCA）模块和扩展空间上采样（ESU）模块用于3D区域提议。前者模块可以关注场景中的全局和多尺度上下文，并捕捉那些具有区分性的特征。后者模块结合了不同尺度的低级特征，并生成可靠的3D区域提议。此外，一种更好地融合这些特征的方法是应用新的多级融合方案，允许它们之间有更多的互动。最后，实验结果表明，在11.1
    FPS的速度下，他们的方法比MV3D快5倍。
- en: 'Segmentation-based Methods. This kind of method usually has preliminary processing
    of semantic segmentation [[28](#bib.bib28)], which means that using semantic segmentation
    techniques to take out those background points can help generate high-quality
    proposals from foreground points. One typical segmentation-based network is shown
    in Figure [4(b)](#S3.F4.sf2 "In Figure 4 ‣ 3.1 Two-Stage Methods ‣ 3 Methods ‣
    Deep learning for 3D Object Detection and Tracking in Autonomous Driving: A Brief
    Survey"). In addition, compared with those multi-view methods mentioned above,
    segmentation-based methods obtain higher recall scores and can be applied in complex
    scenes with a large number of occlusions and crowded object swell.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '基于分割的方法。这类方法通常在语义分割的初步处理阶段[[28](#bib.bib28)]，即通过使用语义分割技术去除背景点，可以帮助从前景点生成高质量的提议。一个典型的基于分割的网络如图[4(b)](#S3.F4.sf2
    "在图 4 ‣ 3.1 两阶段方法 ‣ 3 方法 ‣ 深度学习在自动驾驶中的3D目标检测与跟踪: 简要调查")所示。此外，与上述多视角方法相比，基于分割的方法获得了更高的召回率，并且可以应用于具有大量遮挡和拥挤物体膨胀的复杂场景中。'
- en: The first method to be introduced is proposed in [[29](#bib.bib29)]. This method
    named IPOD ﬁrst completes semantic segmentation on the images and generates point-based
    proposals. Generating proposals at those positive points remains high ﬁdelity
    as well. Besides, some possible questions such as proposal redundancy and ambiguity
    have also been considered. This paper provided a new criterion named PointsIoU
    to address those problems. Experimental results surely show that this model is
    better than many 3D detection methods, especially for those scenes with high occlusion.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首个介绍的方法是[[29](#bib.bib29)]中提出的。这种方法名为IPOD，首先对图像进行语义分割，并生成基于点的提议。在这些正样本点生成的提议也保持了较高的准确性。此外，还考虑了一些可能的问题，如提议冗余和模糊性。本文提出了一种新的标准，名为PointsIoU，以解决这些问题。实验结果确实表明，这一模型优于许多3D检测方法，特别是在高遮挡场景中表现更佳。
- en: Another classical segmentation-based network in our review to talk about is
    PointRCNN [[30](#bib.bib30)] proposed by Shi et al. This network generates 3D
    proposals via segmentation techniques in the ﬁrst stage and in stage two those
    proposals will be reﬁned to get ﬁnal detection results. Unlike IPOD, PointRCNN
    directly segments the point cloud to generate high-quality proposals rather than
    applying 2D object segmentation. An important module of this network is Bin-based
    3D bounding box generation and those boxes are regressed from the foreground points.
    Instead of using L1 or L2 loss for regression, this module adopts a bin-based
    method. That is to say, they ﬁrst split each foreground point into different bins,
    then regress boxes among each bin. This work achieves an RPN in 3D space. Drawing
    lessons from the RPN stage of PointRCNN, Jesus et al. proposed a graph-based 3D
    detection pipeline named PointRGCN [[31](#bib.bib31)], which takes advantage of
    advances in GCNs, including two subnetworks R-GCN and C-GCN. R-GCN is a residual
    GCN that achieves pre-proposal feature aggregation by using all points in a proposal.
    C-GCN is a contextual GCN whose main function is reﬁning proposals via the shared
    contextual information between different proposals. Sourabh et al. proposed PointPainting
    [[32](#bib.bib32)] which works by projecting lidar points into the output of an
    image-based semantic segmentation network and appending the class scores to each
    point. Those appended points can be fed to any existing lidar-only detector such
    as PointRCNN we have mentioned above. Their work can ﬁll the gap that comprehensive
    information provided by different sensors is beneficial for fusion-based methods,
    but experimental results on the main benchmark datasets show that lidar-only methods
    outperform fusion-based methods in most cases.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的审查中另一个经典的基于分割的网络是Shi等人提出的PointRCNN [[30](#bib.bib30)]。该网络通过分割技术在第一阶段生成3D提议，然后在第二阶段对这些提议进行细化以获得最终检测结果。与IPOD不同，PointRCNN直接对点云进行分割，以生成高质量的提议，而不是应用2D物体分割。该网络的一个重要模块是基于箱体的3D边界框生成，这些箱体是从前景点回归得出的。该模块采用基于箱体的方法，而不是使用L1或L2损失进行回归。也就是说，他们首先将每个前景点拆分到不同的箱体中，然后在每个箱体中回归箱体。这项工作在3D空间中实现了一个RPN。从PointRCNN的RPN阶段中汲取经验，Jesus等人提出了一个基于图的3D检测管道，名为PointRGCN
    [[31](#bib.bib31)]，它利用GCNs的进展，包括两个子网络R-GCN和C-GCN。R-GCN是一个残差GCN，通过使用提议中的所有点来实现预提议特征聚合。C-GCN是一个上下文GCN，其主要功能是通过不同提议之间共享的上下文信息来细化提议。Sourabh等人提出了PointPainting
    [[32](#bib.bib32)]，该方法通过将激光雷达点投影到基于图像的语义分割网络的输出中，并将类别分数附加到每个点上。这些附加的点可以输入到任何现有的仅激光雷达检测器中，如上面提到的PointRCNN。他们的工作填补了不同传感器提供的综合信息对基于融合的方法有益的空白，但在主要基准数据集上的实验结果显示，大多数情况下，仅激光雷达方法的表现优于基于融合的方法。
- en: 'Frustum-based Methods. These methods leverage both mature 2D object detectors
    and advanced 3D deep learning for object localization. They generate 2D object
    region proposals ﬁrst and then generate 3D frustum proposals via lifting a 2D
    bounding box to a frustum that includes a 3D search space for the object. One
    speciﬁc process of the frustum-based method can be seen in Figure [4(c)](#S3.F4.sf3
    "In Figure 4 ‣ 3.1 Two-Stage Methods ‣ 3 Methods ‣ Deep learning for 3D Object
    Detection and Tracking in Autonomous Driving: A Brief Survey"). Certainly, it
    is of great significance to know that although with high efﬁciency to propose
    possible regions of 3D objects, the step-by-step pipeline results in extreme dependency
    on 2D image detectors.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 基于截锥体的方法。这些方法利用成熟的2D物体检测器和先进的3D深度学习进行物体定位。它们首先生成2D物体区域提议，然后通过将2D边界框提升到包括3D搜索空间的截锥体来生成3D截锥体提议。基于截锥体的方法的一个具体过程可以在图[4(c)](#S3.F4.sf3
    "在图4 ‣ 3.1 两阶段方法 ‣ 3 方法 ‣ 深度学习用于自动驾驶中的3D物体检测与跟踪：简要调查")中看到。显然，尽管这种方法在提出可能的3D物体区域时效率很高，但逐步的流程使其极度依赖2D图像检测器。
- en: 'Qi et al. did pioneering work in this direction. They proposed a novel framework
    named Frustum PointNets [[33](#bib.bib33)], based on RGB-D data for 3D object
    detection. In their work, the model ﬁrst feeds RGB images to the convolutional
    neural network to obtain 2D proposals and then combines depth information to project
    region to frustum. This is the process of getting frustum proposals. For those
    points contained in the frustum, 3D instance segmentation would be executed. Based
    on the results of segmentation, a lightweight regression PointNet attempts to
    adjust those points via translation so that their centroid is close to amodal
    box centre. Finally, a 3D box estimation network estimates those 3D amodal bounding
    boxes. It is surprising that F-PointNets have the ability to predict correctly
    posed amodal 3D boxes with a few points. However, there still exists some problems
    such as failure to work in the case of multiple instances from the same category.
    Following the work of F-PointNets, Zhao et al. presented a new network architecture
    called SIFRNet [[34](#bib.bib34)] relying on front view images and frustum point
    clouds to predict 3D detection results. The whole network mainly consists of three
    parts: i)3D instance segmentation network (Point-UNet); ii)T-Net; iii)3D box estimation
    network (Point-SENet). They contribute to the improvement of the performance in
    3D segmentation and the efﬁciency of 3D bounding box prediction. PointSIFT [[35](#bib.bib35)]
    module is integrated into their network, capturing orientation information of
    point clouds and achieving strong robustness to shape scaling. A series of experiments
    show that this method achieves better performance on the KITTI dataset and SUN-RGBD
    dataset [[36](#bib.bib36)] when compared to F-PointNets.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Qi 等人在这个方向上做了开创性的工作。他们提出了一个名为 Frustum PointNets [[33](#bib.bib33)] 的新框架，基于 RGB-D
    数据进行 3D 物体检测。在他们的工作中，模型首先将 RGB 图像输入到卷积神经网络中，以获取 2D 提议，然后结合深度信息将区域投影到视锥体。这是获取视锥体提议的过程。对于那些包含在视锥体中的点，将执行
    3D 实例分割。基于分割结果，轻量级回归 PointNet 尝试通过平移调整这些点，以使它们的质心接近于模态框中心。最后，一个 3D 盒子估计网络估算这些
    3D 模态边界框。令人惊讶的是，F-PointNets 能够用少量点正确预测模态 3D 盒子。然而，仍然存在一些问题，例如在同一类别的多个实例的情况下无法工作。在
    F-PointNets 的工作之后，Zhao 等人提出了一种新的网络架构，称为 SIFRNet [[34](#bib.bib34)]，依赖前视图图像和视锥体点云来预测
    3D 检测结果。整个网络主要由三个部分组成：i) 3D 实例分割网络（Point-UNet）；ii) T-Net；iii) 3D 盒子估计网络（Point-SENet）。它们有助于提高
    3D 分割的性能和 3D 边界框预测的效率。PointSIFT [[35](#bib.bib35)] 模块被集成到他们的网络中，捕捉点云的方向信息，并对形状缩放具有强大的鲁棒性。一系列实验表明，该方法在与
    F-PointNets 相比时，在 KITTI 数据集和 SUN-RGBD 数据集 [[36](#bib.bib36)] 上表现更好。
- en: A generic 3D object detection method called PointFusion [[37](#bib.bib37)] is
    presented by Xu et al. To handle the challenge of the combination of various RGB
    images and point cloud data, previous methods usually transform the form of point
    cloud data such as representing point cloud data by 2D image or voxel. It is convenient
    but runs into the problem of losing some information contained in point clouds.
    Instead, this method directly processes images and 3D point clouds via ResNet
    [[38](#bib.bib38)] and PointNet [[39](#bib.bib39)]. The obtained 2D image region
    and its related frustum points are used to precisely regress 3D boxes. They present
    a global fusion network to directly get the 3D box corner locations. At the same
    time, a new dense fusion network for the purpose of predicting spatial offset
    and choosing the ﬁnal prediction results with the highest score.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Xu 等人提出了一种通用的 3D 物体检测方法，称为 PointFusion [[37](#bib.bib37)]。为了应对各种 RGB 图像和点云数据结合的挑战，以前的方法通常将点云数据转换为
    2D 图像或体素。这种方法虽然方便，但会遇到丢失点云数据中部分信息的问题。相反，该方法通过 ResNet [[38](#bib.bib38)] 和 PointNet
    [[39](#bib.bib39)] 直接处理图像和 3D 点云。获得的 2D 图像区域及其相关的视锥体点用于精确回归 3D 盒子。他们提出了一种全局融合网络，直接获取
    3D 盒子的角点位置。同时，还提出了一种新的密集融合网络，用于预测空间偏移量并选择最终得分最高的预测结果。
- en: It is noteworthy that Wang et al. put forward a novel approach called Frustum
    ConvNet [[40](#bib.bib40)], which ﬁrst generates a strand of frustums for each
    proposal and utilizes obtained frustums to group those points. F-PointNet we have
    mentioned above also works directly on raw point cloud, but it has not been designed
    to be a end-to-end pipeline, owing to its T-Net alignment. This method takes this
    factor into account and is designed to combine several beneﬁts from previous works.
    It has been proved that this novel method F-ConvNet for amodal 3D object detection
    with end-to-end style achieves the state-of-the-art performance on the KITTI dataset
    among 2D detectors and is helpful for a large number of applications such as autonomous
    driving.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，Wang等人提出了一种称为Frustum ConvNet的新方法[[40](#bib.bib40)]，该方法首先为每个提案生成一组截头圆锥体，并利用获得的截头圆锥体对这些点进行分组。我们之前提到的F-PointNet也直接在原始点云上工作，但由于其T-Net对齐，它并未设计为端到端的流水线。这种方法考虑了这一因素，并设计为结合之前工作的多种优点。已证明这种新方法F-ConvNet在KITTI数据集中相较于2D检测器实现了最先进的性能，并对诸如自动驾驶等大量应用具有帮助。
- en: Two-stage methods obtain several advantages including high accuracy for object
    detection. However, because of the process of generating regions containing pre-deﬁned
    objects, the speed of this object detection procedure will be decreased.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 两阶段方法在物体检测中获得了高准确率等多个优点。然而，由于生成包含预定义物体的区域的过程，这种物体检测程序的速度会降低。
- en: '![Refer to caption](img/ae44f425dab27114481054c70ea6c6ee.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ae44f425dab27114481054c70ea6c6ee.png)'
- en: 'Figure 5: A comprehensive view of the proposed 3D object detector from Bird’s
    Eye View (BEV) based on point clouds.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：基于点云的鸟瞰视角（BEV）的3D物体检测器的全面视图。
- en: 3.2 One-Stage Methods
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 单阶段方法
- en: In our discussion, one stage is equal to a single shot in the meaning of predicting
    class probabilities and regressing bounding boxes directly. These methods are
    free of region proposal network (RPN) and post-processing. Consequently, they
    have higher speed than two-stage approaches and are applied to real-time systems
    mostly. We categorize one-stage methods to the form of input data, including BEV-based
    methods, discretization-based methods and point cloud-based methods.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的讨论中，一个阶段等于在预测类别概率和直接回归边界框的意义上的一次单独操作。这些方法不需要区域建议网络（RPN）和后处理。因此，它们的速度比两阶段方法更快，并且主要应用于实时系统。我们将单阶段方法按照输入数据的形式分类，包括基于BEV的方法、基于离散化的方法和基于点云的方法。
- en: BEV-based Methods. Looking into the name of this kind of method, it is obvious
    that BEV-based methods use BEV representation as input. As mentioned in [[18](#bib.bib18)],
    the BEV map has several strong points in 3D object detection compared to the front
    view or image plane. First, in the BEV map objects are of the same physical sizes
    as the original sizes, thus decreasing the size error that the front view/image
    plane has. Second, it is almost impossible to run into occlusion problems because
    objects in the BEV are located in different places. Last, for autonomous driving
    applications, those trafﬁc participants usually lie on the ground plane and in
    the vertical direction there exists a small variance. So the BEV location is of
    great significance when getting 3D bounding boxes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于BEV的方法。根据这种方法的名称，很明显基于BEV的方法使用BEV表示作为输入。如[[18](#bib.bib18)]中提到的，与前视图或图像平面相比，BEV地图在3D物体检测中有几个强项。首先，在BEV地图中物体的实际尺寸与原始尺寸相同，从而减少了前视图/图像平面中的尺寸误差。其次，几乎不可能遇到遮挡问题，因为BEV中的物体位于不同的位置。最后，对于自动驾驶应用，这些交通参与者通常位于地面平面上，并且在垂直方向上存在较小的方差。因此，BEV位置在获取3D边界框时具有重要意义。
- en: 'Yang et al. proposed a proposal-free, one-stage method named PIXOR [[41](#bib.bib41)],
    which represents raw 3D data from the Bird’s Eye View (BEV). We can see the network
    architecture of PIXOR from Figure [5](#S3.F5 "Figure 5 ‣ 3.1 Two-Stage Methods
    ‣ 3 Methods ‣ Deep learning for 3D Object Detection and Tracking in Autonomous
    Driving: A Brief Survey"). Here the BEV representation is chosen because of its
    friendly computation. In the stage of elaborating input representation, they discretize
    3D points contained in those interested scenes with a resolution of $d_{L}$ ×
    $d_{W}$ × $d_{H}$ each cell ﬁrst and then encode the value of each cell to get
    occupancy tensor. Finally, they get a combination of the 2D reﬂectance image and
    the 3D occupancy tensor. When designing network architecture, a fully convolutional
    network is used for dense 3D object detection. A backbone network and a header
    network are composed of PIXOR. The former extracts ordinary representation of
    the input and the latter is used for doing speciﬁc task prediction. Based on experimental
    results, the conclusion has been made that PIXOR outperforms most one-stage methods
    in terms of Average Precision (AP) while running at 10 FPS.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'Yang 等人提出了一种无提案、单阶段的方法，称为PIXOR [[41](#bib.bib41)]，该方法表示来自鸟瞰图（BEV）的原始 3D 数据。我们可以从图
    [5](#S3.F5 "Figure 5 ‣ 3.1 Two-Stage Methods ‣ 3 Methods ‣ Deep learning for 3D
    Object Detection and Tracking in Autonomous Driving: A Brief Survey") 中看到PIXOR的网络架构。选择BEV表示是因为其计算友好。在输入表示的阶段，他们首先将感兴趣场景中的3D点离散化为每个单元分辨率为
    $d_{L}$ × $d_{W}$ × $d_{H}$ 的离散点，然后对每个单元的值进行编码以获得占用张量。最后，他们获得了2D反射图像和3D占用张量的组合。在设计网络架构时，使用了全卷积网络进行密集的3D物体检测。PIXOR由一个主干网络和一个头部网络组成。前者提取输入的普通表示，后者用于执行特定任务预测。根据实验结果，得出的结论是PIXOR在平均精度（AP）方面优于大多数单阶段方法，同时以10
    FPS的速度运行。'
- en: Later on, Yang et al. [[42](#bib.bib42)] showed us that High-Deﬁnition (HD)
    maps possess strong priors, which is beneﬁcial for the improvement of the performance
    and robustness of 3D object detection. To reach their goal, they developed a single-stage
    detector that works in the Bird’s Eye View (BEV) and fuses LiDAR information.
    Speciﬁcally, they get the coordinates of ground points from the HD map and then
    the absolute distance in the BEV representation would be replaced by the distance
    relative to the ground for the purpose of mending translation variance. Considering
    the situation that HD maps are not available everywhere, they put forward a solution
    that is practicable to utilize an online map prediction module and estimate map
    priors on the basis of LiDAR point cloud data. In their experiments, the baseline
    is a PIXOR++ detector without a map. The results illustrate that this HD map-aware
    model outperforms its baseline on the TOR4D and KITTI evidently. Nevertheless,
    a problem still exists for the poor generalization performance to point cloud
    data with diverse densities. Intending to address this problem, a new network
    architecture was proposed by Beltran et al. named BirdNet [[43](#bib.bib43)].
    They developed a new encoding method for BEV, which is invariant to distance and
    differences on LiDAR devices. The proposed density normalization method enables
    training models on those popular high-resolution lase datasets.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，Yang 等人 [[42](#bib.bib42)] 向我们展示了高分辨率（HD）地图具有强大的先验，这有助于提高3D物体检测的性能和鲁棒性。为了实现他们的目标，他们开发了一种在鸟瞰图（BEV）中工作的单阶段检测器，并融合了激光雷达信息。具体而言，他们从HD地图中获取地面点的坐标，然后BEV表示中的绝对距离将被替换为相对于地面的距离，以修正平移方差。考虑到HD地图并非随处可用，他们提出了一种实用的解决方案，即利用在线地图预测模块，并基于激光雷达点云数据估计地图先验。在他们的实验中，基线是没有地图的PIXOR++检测器。结果表明，这种基于HD地图的模型在TOR4D和KITTI上明显优于其基线。然而，仍然存在一个问题，即对具有不同密度的点云数据的泛化性能较差。为了解决这个问题，Beltran
    等人提出了一种新的网络架构，称为BirdNet [[43](#bib.bib43)]。他们开发了一种新的BEV编码方法，该方法对距离和激光雷达设备的差异具有不变性。所提出的密度归一化方法使得能够在流行的高分辨率激光数据集上训练模型。
- en: Discretization-based Methods. It is easy to understand this kind of method via
    its name. These methods usually convert raw point cloud data into a regular discrete
    format (e.g., 2D map), and then use deep neural networks to predict the category
    probabilities and 3D boxes of objects.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于离散化的方法。通过其名称很容易理解这种方法。这些方法通常将原始点云数据转换为规则的离散格式（例如，2D地图），然后使用深度神经网络预测物体的类别概率和3D框。
- en: 'The first method to use a FCN in the purpose of 3D object detection was proposed
    by Li et al.. Owing to the quick development of convolutional network techniques,
    they raised a new idea to transplant the FCN technique to object detection on
    3D data [[23](#bib.bib23)]. In this method, point cloud data are converted into
    a 2D point map and the FCN is used for predicting the conﬁdences of those objects
    and the bounding boxes at the same time. This is the first attempt to introduce
    the FCN techniques into the object detection on range scan data, generating an
    order and end-to-end framework for detection. However, this paper only analyzes
    the method for 3D range scan from Velogyne 64E. As a result, in their later work,
    they discretized the point cloud into a 4D tensor with dimensions of length, width,
    height and channels [[44](#bib.bib44)]. In this work, they also extended the former
    2D detection method based on FCN to 3D domain for 3D object detection. We can
    observe the detection results in Figure [6](#S3.F6 "Figure 6 ‣ 3.2 One-Stage Methods
    ‣ 3 Methods ‣ Deep learning for 3D Object Detection and Tracking in Autonomous
    Driving: A Brief Survey"). Based on their experimental results, they made a comparison
    with FCN for 2D detection work, observing a gain of over 20% in accuracy.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '第一个将FCN用于3D物体检测的方法是由Li等人提出的。由于卷积网络技术的快速发展，他们提出了一种将FCN技术移植到3D数据物体检测中的新想法[[23](#bib.bib23)]。在这种方法中，点云数据被转换为2D点图，然后使用FCN同时预测这些物体的置信度和边界框。这是首次尝试将FCN技术引入范围扫描数据的物体检测，生成了一种顺序和端到端的检测框架。然而，本文仅分析了来自Velodyne
    64E的3D范围扫描的方法。因此，在他们的后续工作中，他们将点云离散化为一个具有长度、宽度、高度和通道维度的4D张量[[44](#bib.bib44)]。在这项工作中，他们还将先前基于FCN的2D检测方法扩展到3D领域进行3D物体检测。我们可以在图[6](#S3.F6
    "Figure 6 ‣ 3.2 One-Stage Methods ‣ 3 Methods ‣ Deep learning for 3D Object Detection
    and Tracking in Autonomous Driving: A Brief Survey")中观察到检测结果。根据他们的实验结果，他们与2D检测工作的FCN进行了比较，观察到准确度提高了超过20%。'
- en: '![Refer to caption](img/be868da7e5c33d533fd9c04a37666d0f.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/be868da7e5c33d533fd9c04a37666d0f.png)'
- en: 'Figure 6: Direct outcomes of the 3D FCN detection process. (a) Aggregating
    bounding boxes in possession of high objectness conﬁdence and plotting them as
    green boxes. Plotting bounding box predictions coming from with green boxes. (b)
    Plotting those bounding boxes with blue original point clouds after clustering.
    (c) Detection in 3D since (a) and (b) are visualized in the bird’s eye view.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：3D FCN检测过程的直接结果。 (a) 聚合具有高物体置信度的边界框，并将其绘制为绿色框。绘制来自绿色框的边界框预测。 (b) 在聚类后，将这些边界框与蓝色原始点云绘制在一起。
    (c) 在3D中进行检测，因为 (a) 和 (b) 以鸟瞰图的形式可视化。
- en: Although 3D FCN-based method makes progress compared to the previous work, there
    are still some possible questions here. Recalling the process, it is easy to ﬁnd
    that this method has a huge expensive computation because of 3D convolutions and
    the sparsity of the data. Engelcke et al. put forward a computationally efﬁcient
    approach named Vote3Deep to do object detection for 3D point clouds [[45](#bib.bib45)].
    Vote3Deep is a feature-centric voting framework for the purpose of improving the
    efﬁciency of computation. For one thing, building an efﬁcient convolutional layer
    via leveraging a voting framework is of great use. For another, to take advantage
    of sparse convolutional layers of the whole CNN stack they utilize modiﬁed linear
    units and sparsity penalty. Li et al. proposed a 3D backbone network learning
    3D features from most raw data [[46](#bib.bib46)]. The network is constructed
    to address the problem of the lack of powerful 3D feature extraction methods.
    As a result, this method obtains rich 3D features and would not introduce a huge
    computational burden.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于3D FCN的方法相比之前的工作有所进展，但仍然存在一些可能的问题。回顾这个过程，我们可以很容易发现，由于3D卷积和数据的稀疏性，这种方法计算开销巨大。Engelcke等人提出了一种名为Vote3Deep的计算高效方法，用于对3D点云进行物体检测[[45](#bib.bib45)]。Vote3Deep是一个以特征为中心的投票框架，旨在提高计算效率。一方面，通过利用投票框架构建高效的卷积层具有很大的用处。另一方面，为了利用整个CNN堆栈中的稀疏卷积层，他们使用了修改过的线性单元和稀疏性惩罚。Li等人提出了一种3D主干网络，从大部分原始数据中学习3D特征[[46](#bib.bib46)]。该网络旨在解决缺乏强大3D特征提取方法的问题。因此，这种方法获得了丰富的3D特征，并且不会引入巨大的计算负担。
- en: 'Zhou et al. proposed an end-to-end network named VoxelNet for 3D object detection
    [[47](#bib.bib47)]. Figure [7](#S3.F7 "Figure 7 ‣ 3.2 One-Stage Methods ‣ 3 Methods
    ‣ Deep learning for 3D Object Detection and Tracking in Autonomous Driving: A
    Brief Survey") shows how VoxelNet works. They projected point clouds into equally
    spaced voxels and transformed a number of points in each voxel into a centralized
    feature representation via the brand-proposed voxel feature encoding (VFE) layer.
    The method using skill to represent point clouds with voxels has great performance.
    Voxel-based methods sometimes are designed with the intention of enhancing the
    retention of information at the stage of processing point cloud data. However,
    the speed of the model process is still very slow because of the sparsity of those
    voxels and 3D convolutional computations. Following Zhou’s work, Yan et al. did
    research on how to increase the speed of training and prediction and proposed
    an improved sparse convolution method [[48](#bib.bib48)]. What’s more, to handle
    the question that it is ambiguous to solve the sine function between 0 and $\pi$,
    they proposed a sine error angle loss. Another attempt has been made by Sindagi
    et al. about a small change on VoxelNet is fusing image and point cloud features
    at a different stage [[49](#bib.bib49)]. Specifically, PointFusion and VoxelFusion
    are two unique techniques mentioned in [[49](#bib.bib49)]. The former works in
    the early stage where point cloud data can be projected to the image plane. The
    latter is used to project those 3D voxels to the image. Compared to VoxelNet,
    this network with new techniques has the ability to exploit multi-modal information
    and can decrease the false positives and negatives.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zhou等人提出了一种名为VoxelNet的端到端网络，用于3D目标检测[[47](#bib.bib47)]。图[7](#S3.F7 "Figure
    7 ‣ 3.2 One-Stage Methods ‣ 3 Methods ‣ Deep learning for 3D Object Detection
    and Tracking in Autonomous Driving: A Brief Survey")展示了VoxelNet的工作原理。他们将点云投影到等距的体素中，并通过品牌提出的体素特征编码（VFE）层将每个体素中的多个点转换为集中化的特征表示。使用体素表示点云的方法表现良好。基于体素的方法有时是为了增强在处理点云数据阶段的信息保留。然而，由于这些体素的稀疏性和3D卷积计算，模型处理的速度仍然非常慢。继Zhou的工作后，Yan等人研究了如何提高训练和预测速度，并提出了一种改进的稀疏卷积方法[[48](#bib.bib48)]。此外，为了解决在$0$和$\pi$之间解决正弦函数的模糊性问题，他们提出了正弦误差角损失。Sindagi等人尝试对VoxelNet进行小幅修改，将图像和点云特征在不同阶段融合[[49](#bib.bib49)]。具体来说，PointFusion和VoxelFusion是[[49](#bib.bib49)]中提到的两种独特技术。前者在点云数据可以投影到图像平面时工作，后者则用于将这些3D体素投影到图像上。与VoxelNet相比，这种具有新技术的网络能够利用多模态信息，并能减少假阳性和假阴性。'
- en: '![Refer to caption](img/73475ad4207c3464cea592da5a064dcd.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/73475ad4207c3464cea592da5a064dcd.png)'
- en: 'Figure 7: VoxelNet directly works on the raw point cloud without feature engineering
    and predicts the 3D detection results using a single end-to-end trainable network.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：VoxelNet直接在原始点云上工作，无需特征工程，并使用单一的端到端可训练网络预测3D检测结果。
- en: Considering the situation that although point clouds contain much spatial information,
    it is inevitable to lose some in the process of downscaled feature maps with existing
    one-shot methods. He et al. proposed a SA-SSD network exploiting ﬁne-grained architecture
    information to improve localization accuracy [[50](#bib.bib50)]. What makes it
    speciﬁcal is that they ﬁrst transfer point cloud data to a tensor and put them
    to a backbone model for the purpose of extracting multiple stages features. What’s
    more, they designed a supplementary network in the possession of point-level supervision,
    which can instruct those features to learn the structure of point clouds. It is
    surprising that their experimental results demonstrate that SA-SSD ranks the ﬁrst
    on the KITTI BEV detection benchmark on the Car class.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到尽管点云包含大量空间信息，但在现有的一次性方法中，缩小特征图的过程中不可避免地会丢失一些信息，He等人提出了一种SA-SSD网络，利用细粒度的架构信息来提高定位精度[[50](#bib.bib50)]。其特殊之处在于，他们首先将点云数据转化为张量，并将其输入到骨干网络中以提取多个阶段的特征。此外，他们设计了一个具有点级监督的补充网络，可以指导这些特征学习点云的结构。令人惊讶的是，他们的实验结果表明，SA-SSD在Car类别的KITTI
    BEV检测基准中排名第一。
- en: '![Refer to caption](img/05bfa393a17a02efbde8d535eaca0de4.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/05bfa393a17a02efbde8d535eaca0de4.png)'
- en: 'Figure 8: Different object detection processes for two-stage and one-stage
    methods.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：两阶段和单阶段方法的不同目标检测过程。
- en: Point-based Methods. These methods don’t transform the format of raw input,
    directly feeding the point cloud to the network. Yang et al. did a pioneering
    work proposing a network named 3DSSD [[51](#bib.bib51)]. This is the ﬁrst lightweight
    and efﬁcient point cloud-based 3D one-stage object detector. In this new model,
    Distance-FPS (D-FPS) and Feature-FPS (F-FPS) combined together are their proposed
    strategies for fusion sampling. In addition, they generated a detailed bounding
    box prediction network to get the best use of the representative points, exploiting
    a candidate generation layer (CG), an anchor-free regression head and a 3D centerness
    assignment strategy. At last, those experimental results demonstrate that 3DSSD
    outperforms the point-based method PointRCNN at a speed of 25 fps.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 基于点的方法。这些方法不对原始输入进行格式转换，而是直接将点云输入网络。杨等人进行了开创性的工作，提出了一种名为3DSSD的网络[[51](#bib.bib51)]。这是首个轻量级、高效的基于点云的3D单阶段物体检测器。在这一新模型中，Distance-FPS（D-FPS）和Feature-FPS（F-FPS）的结合是他们提出的融合采样策略。此外，他们生成了一个详细的边界框预测网络，以最佳利用代表性点，利用了候选生成层（CG）、无锚点回归头和3D中心分配策略。最终，实验结果表明，3DSSD在25
    fps的速度下优于基于点的方法PointRCNN。
- en: Other Methods. Also there are some other single-stage object detection methods
    which haven’t been divided into any types of method in this review. Meyer et al.
    proposed LaserNet, which is an efﬁcient probabilistic 3D object detection model
    [[52](#bib.bib52)]. It is deserving to know that they use a small and dense range
    view data as input rather than Bird’s Eye View data. So their model is more efﬁcient.
    Further, to great knowledge, this is the ﬁrst method to obtain the uncertainty
    of detection in the manner of modelling the distribution of bounding box corners.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法。此外，还有一些其他的单阶段物体检测方法在本综述中未被分为任何类型。迈耶等人提出了LaserNet，这是一种高效的概率性3D物体检测模型[[52](#bib.bib52)]。值得注意的是，他们使用了小型且密集的范围视图数据作为输入，而不是鸟瞰图数据。因此，他们的模型更高效。此外，据我们所知，这是第一种通过建模边界框角点分布来获得检测不确定性的方法。
- en: Compared to two-stage methods, one-stage methods may don’t achieve such high
    detection accuracy. But one-stage methods have the ability to detect objects faster,
    which are suitable for real-time detection.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与两阶段方法相比，一阶段方法可能无法实现如此高的检测准确率。但一阶段方法具有更快的物体检测能力，适用于实时检测。
- en: 4 Summary
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 总结
- en: '3D object detection is an enormous help for computers to understand scenes
    and is the key technique for a number of real-world applications such as autonomous
    driving. In this review, we list some typical state-of-the-art 3D object detection
    methods and categorize them into two-stage methods and one-stage methods. The
    former one needs to generate a series of proposals at ﬁrst and then predict of
    regress those extracted features. The one-stage method skips the procedure of
    proposal generation, directly predicting class probabilities and regressing bounding
    boxes. For the purpose of directly understanding how those two types of methods
    achieve object detection, Figure [8](#S3.F8 "Figure 8 ‣ 3.2 One-Stage Methods
    ‣ 3 Methods ‣ Deep learning for 3D Object Detection and Tracking in Autonomous
    Driving: A Brief Survey") gives a simple description. We also state the advantages
    of point cloud data for object detection and list several acknowledged drawbacks
    of point clouds. At present, current popular methods try to introduce different
    types of input such as LIDAR points and camera data. The images provide us with
    more dense information but with the loss of 3D spatial information. LiDAR point
    cloud is suitable for 3D object detection with its geometric position information
    and depth information. The sparsity and the irregularity of point clouds urge
    people to investigate novel methods to leverage the advantages of both images
    and LiDAR-based data.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '三维物体检测对计算机理解场景具有巨大帮助，是许多现实世界应用如自动驾驶的关键技术。在这篇综述中，我们列举了一些典型的最先进的三维物体检测方法，并将其分类为两阶段方法和一阶段方法。前者需要首先生成一系列提议，然后预测或回归这些提取的特征。一阶段方法跳过了提议生成的过程，直接预测类别概率和回归边界框。为了直接理解这两种方法如何实现物体检测，图[8](#S3.F8
    "Figure 8 ‣ 3.2 One-Stage Methods ‣ 3 Methods ‣ Deep learning for 3D Object Detection
    and Tracking in Autonomous Driving: A Brief Survey")给出了简单描述。我们还阐述了点云数据在物体检测中的优点，并列出了点云的几个已知缺点。目前，流行的方法尝试引入不同类型的输入，如激光雷达点和相机数据。图像提供了更多的密集信息，但损失了三维空间信息。激光雷达点云适合于三维物体检测，因其具有几何位置和深度信息。点云的稀疏性和不规则性促使人们研究新颖的方法，以充分利用图像和激光雷达数据的优点。'
- en: 'According to the analysis of those various kinds of existing methods, the following
    problems require further research:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 根据对各种现有方法的分析，以下问题需要进一步研究：
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: First of all, owing to the regular representation of data, those mature 2D image
    process networks can be applied to projection-based techniques and discretization-based
    techniques greatly. Nevertheless, it is inevitable to lose some useful information
    during the process of projecting 3D data into 2D format, which is a great limitation
    for projection-based methods. For discretization-based methods the exponentially
    increasing computation and huge memory costs caused by the increase of the resolution
    maintain the major bottleneck. Taking the above problems into consideration, building
    a sparse convolutional layer based on indexing architectures may be a feasible
    solution to those questions and it deserves further research.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，由于数据的规则表示，那些成熟的二维图像处理网络可以很好地应用于基于投影的技术和基于离散化的技术。然而，在将三维数据投影到二维格式的过程中，不可避免地会丢失一些有用的信息，这是投影方法的一大限制。对于基于离散化的方法，由于分辨率的提高导致计算量和巨大的内存开销指数增长，依然是主要瓶颈。考虑到上述问题，基于索引架构构建稀疏卷积层可能是一个可行的解决方案，值得进一步研究。
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: At present, point cloud-based models are popular methods that people pay the
    most attention to. However, point representation usually lacks clear neighbouring
    information because of the sparsity and irregularity of point clouds. A great
    many existing point cloud-based methods use expensive nearest neighbour searching
    techniques such as KNN applied in [[53](#bib.bib53)]. The weak efﬁciency of these
    methods calls for more efﬁcient methods. A recent point-voxel combined representation
    method [[54](#bib.bib54)] can be a possible direction for further study.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目前，基于点云的模型是人们最关注的热门方法。然而，由于点云的稀疏性和不规则性，点表示通常缺乏明确的邻域信息。许多现有的点云方法使用昂贵的最近邻搜索技术，如应用于[[53](#bib.bib53)]的KNN。这些方法的低效性呼唤更高效的方法。最近的一种点-体素结合表示方法[[54](#bib.bib54)]可能是进一步研究的方向。
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Most of the existing 3D point cloud object detection methods work on small scale
    of point clouds. However, those point cloud data obtained by LiDARs are extremely
    immense and large-scale because the process of data acquisition is continuous.
    As a result, there is a real thirst for further investigation to solve the problem
    of those large-scale point clouds.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目前大多数现有的 3D 点云目标检测方法适用于小规模的点云。然而，由于数据采集过程是连续的，通过 LiDAR 获得的点云数据极其庞大。因此，迫切需要进一步研究以解决这些大规模点云的问题。
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A large number of researchers [[55](#bib.bib55), [56](#bib.bib56)] have begun
    to learn spatio-temporal information from dynamic point clouds. The spatio-temporal
    information is expected to help improve the performance of many later assignments
    such as 3D object segmentation, object recognition and completion.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大量研究人员 [[55](#bib.bib55), [56](#bib.bib56)] 已经开始从动态点云中学习时空信息。预计这些时空信息将有助于提高许多后续任务的性能，如
    3D 对象分割、对象识别和补全。
- en: References
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] G. Lan, Y. Wu, F. Hu, and Q. Hao, “Vision-based human pose estimation via
    deep learning: A survey,” *IEEE Transactions on Human-Machine Systems*, vol. 53,
    no. 1, pp. 253–268, 2023.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] G. Lan, Y. Wu, F. Hu, 和 Q. Hao, “基于视觉的人体姿态估计：综述,” *IEEE 人机系统汇刊*, 卷 53,
    期 1, 页 253–268, 2023年。'
- en: '[2] G. Lan, L. De Vries, and S. Wang, “Evolving efficient deep neural networks
    for real-time object recognition,” in *2019 IEEE Symposium Series on Computational
    Intelligence (SSCI)*.   IEEE, 2019, pp. 2571–2578.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] G. Lan, L. De Vries, 和 S. Wang, “为实时对象识别进化高效的深度神经网络,” 在 *2019 IEEE 计算智能研讨会系列（SSCI）*。IEEE,
    2019年, 页 2571–2578。'
- en: '[3] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,”
    in *2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
    (CVPR 2005), 20-26 June 2005, San Diego, CA, USA*.   IEEE Computer Society, 2005,
    pp. 886–893.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] N. Dalal 和 B. Triggs, “用于人体检测的定向梯度直方图,” 在 *2005 IEEE 计算机学会计算机视觉与模式识别会议（CVPR
    2005）, 2005年6月20-26日, 圣地亚哥, CA, 美国*。IEEE 计算机学会, 2005年, 页 886–893。'
- en: '[4] P. F. Felzenszwalb, R. B. Girshick, D. A. McAllester, and D. Ramanan, “Object
    detection with discriminatively trained part-based models,” *IEEE Trans. Pattern
    Anal. Mach. Intell.*, vol. 32, no. 9, pp. 1627–1645, 2010.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] P. F. Felzenszwalb, R. B. Girshick, D. A. McAllester, 和 D. Ramanan, “基于部分的判别训练模型进行目标检测,”
    *IEEE 模式分析与机器智能汇刊*, 卷 32, 期 9, 页 1627–1645, 2010年。'
- en: '[5] X. Wu, D. Sahoo, and S. C. H. Hoi, “Recent advances in deep learning for
    object detection,” *CoRR*, vol. abs/1908.03673, 2019\. [Online]. Available: [http://arxiv.org/abs/1908.03673](http://arxiv.org/abs/1908.03673)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] X. Wu, D. Sahoo, 和 S. C. H. Hoi, “深度学习在目标检测中的最新进展,” *CoRR*, 卷 abs/1908.03673,
    2019年。[在线]. 可用: [http://arxiv.org/abs/1908.03673](http://arxiv.org/abs/1908.03673)'
- en: '[6] G. Lan, Z. Gao, L. Tong, and T. Liu, “Class binarization to neuroevolution
    for multiclass classification,” *Neural Computing and Applications*, vol. 34,
    no. 22, pp. 19 845–19 862, 2022.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] G. Lan, Z. Gao, L. Tong, 和 T. Liu, “用于多类分类的神经进化的类二值化,” *神经计算与应用*, 卷 34,
    期 22, 页 19 845–19 862, 2022年。'
- en: '[7] G. Lan, J. M. Tomczak, D. M. Roijers, and A. Eiben, “Time efficiency in
    optimization with a bayesian-evolutionary algorithm,” *Swarm and Evolutionary
    Computation*, vol. 69, p. 100970, 2022.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] G. Lan, J. M. Tomczak, D. M. Roijers, 和 A. Eiben, “贝叶斯进化算法的优化时间效率,” *群体智能与进化计算*,
    卷 69, 页 100970, 2022年。'
- en: '[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in Neural Information Processing
    Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012\.
    Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States*,
    P. L. Bartlett, F. C. N. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger,
    Eds., 2012, pp. 1106–1114.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “使用深度卷积神经网络进行 Imagenet 分类,”
    在 *神经信息处理系统进展 25: 2012年神经信息处理系统年会论文集, 会议于2012年12月3-6日举行, 内华达州湖塔霍, 美国*, P. L. Bartlett,
    F. C. N. Pereira, C. J. C. Burges, L. Bottou, 和 K. Q. Weinberger 编辑, 2012年, 页
    1106–1114。'
- en: '[9] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
    for accurate object detection and semantic segmentation,” in *2014 IEEE Conference
    on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June
    23-28, 2014*.   IEEE Computer Society, 2014, pp. 580–587.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] R. B. Girshick, J. Donahue, T. Darrell, 和 J. Malik, “丰富的特征层次结构用于准确的目标检测和语义分割,”
    在 *2014 IEEE 计算机视觉与模式识别会议, CVPR 2014, 哥伦布, OH, 美国, 2014年6月23-28日*。IEEE 计算机学会,
    2014年, 页 580–587。'
- en: '[10] R. B. Girshick, “Fast R-CNN,” in *2015 IEEE International Conference on
    Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015*.   IEEE Computer
    Society, 2015, pp. 1440–1448.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] R. B. Girshick，“快速R-CNN”，见于*2015 IEEE国际计算机视觉会议，ICCV 2015，圣地亚哥，智利，2015年12月7-13日*。IEEE计算机学会，2015年，第1440-1448页。'
- en: '[11] K. He, G. Gkioxari, P. Dollár, and R. B. Girshick, “Mask R-CNN,” in *IEEE
    International Conference on Computer Vision, ICCV 2017, Venice, Italy, October
    22-29, 2017*.   IEEE Computer Society, 2017, pp. 2980–2988.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] K. He, G. Gkioxari, P. Dollár 和 R. B. Girshick，“Mask R-CNN”，见于*IEEE国际计算机视觉会议，ICCV
    2017，威尼斯，意大利，2017年10月22-29日*。IEEE计算机学会，2017年，第2980-2988页。'
- en: '[12] S. Ren, K. He, R. B. Girshick, and J. Sun, “Faster R-CNN: towards real-time
    object detection with region proposal networks,” in *Advances in Neural Information
    Processing Systems 28: Annual Conference on Neural Information Processing Systems
    2015, December 7-12, 2015, Montreal, Quebec, Canada*, C. Cortes, N. D. Lawrence,
    D. D. Lee, M. Sugiyama, and R. Garnett, Eds., 2015, pp. 91–99.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. Ren, K. He, R. B. Girshick 和 J. Sun，“Faster R-CNN：通过区域提议网络实现实时目标检测”，见于*神经信息处理系统进展28：2015年神经信息处理系统年会，2015年12月7-12日，蒙特利尔，魁北克，加拿大*，C.
    Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama 和 R. Garnett 主编，2015年，第91-99页。'
- en: '[13] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi, “You only look
    once: Unified, real-time object detection,” in *2016 IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016*.   IEEE
    Computer Society, 2016, pp. 779–788.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Redmon, S. K. Divvala, R. B. Girshick 和 A. Farhadi，“你只需看一次：统一的实时目标检测”，见于*2016
    IEEE计算机视觉与模式识别会议，CVPR 2016，拉斯维加斯，内华达州，美国，2016年6月27-30日*。IEEE计算机学会，2016年，第779-788页。'
- en: '[14] J. Redmon and A. Farhadi, “YOLO9000: better, faster, stronger,” in *2017
    IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu,
    HI, USA, July 21-26, 2017*.   IEEE Computer Society, 2017, pp. 6517–6525.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] J. Redmon 和 A. Farhadi，“YOLO9000：更好、更快、更强”，见于*2017 IEEE计算机视觉与模式识别会议，CVPR
    2017，檀香山，夏威夷，美国，2017年7月21-26日*。IEEE计算机学会，2017年，第6517-6525页。'
- en: '[15] G. Lan, J. Benito-Picazo, D. M. Roijers, E. Domínguez, and A. Eiben, “Real-time
    robot vision on low-performance computing hardware,” in *2018 15th international
    conference on control, automation, robotics and vision (ICARCV)*.   IEEE, 2018,
    pp. 1959–1965.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] G. Lan, J. Benito-Picazo, D. M. Roijers, E. Domínguez 和 A. Eiben，“低性能计算硬件上的实时机器人视觉”，见于*2018年第15届控制、自动化、机器人与视觉国际会议（ICARCV）*。IEEE，2018年，第1959-1965页。'
- en: '[16] H. Xu, G. Lan, S. Wu, and Q. Hao, “Online intelligent calibration of cameras
    and lidars for autonomous driving systems,” in *2019 IEEE Intelligent Transportation
    Systems Conference (ITSC)*.   IEEE, 2019, pp. 3913–3920.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] H. Xu, G. Lan, S. Wu 和 Q. Hao，“用于自动驾驶系统的在线智能相机和激光雷达标定”，见于*2019 IEEE智能交通系统会议（ITSC）*。IEEE，2019年，第3913-3920页。'
- en: '[17] S. Song and J. Xiao, “Deep sliding shapes for amodal 3D object detection
    in RGB-D images,” in *2016 IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016*.   IEEE Computer Society, 2016,
    pp. 808–816.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. Song 和 J. Xiao，“用于RGB-D图像中无模态3D目标检测的深度滑动形状”，见于*2016 IEEE计算机视觉与模式识别会议，CVPR
    2016，拉斯维加斯，内华达州，美国，2016年6月27-30日*。IEEE计算机学会，2016年，第808-816页。'
- en: '[18] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3D object detection
    network for autonomous driving,” in *2017 IEEE Conference on Computer Vision and
    Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017*.   IEEE Computer
    Society, 2017, pp. 6526–6534.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] X. Chen, H. Ma, J. Wan, B. Li 和 T. Xia，“用于自动驾驶的多视角3D目标检测网络”，见于*2017 IEEE计算机视觉与模式识别会议，CVPR
    2017，檀香山，夏威夷，美国，2017年7月21-26日*。IEEE计算机学会，2017年，第6526-6534页。'
- en: '[19] J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. L. Waslander, “Joint 3D
    proposal generation and object detection from view aggregation,” in *2018 IEEE/RSJ
    International Conference on Intelligent Robots and Systems, IROS 2018, Madrid,
    Spain, October 1-5, 2018*.   IEEE, 2018, pp. 1–8.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Ku, M. Mozifian, J. Lee, A. Harakeh 和 S. L. Waslander，“从视图聚合中生成联合3D提议和目标检测”，见于*2018
    IEEE/RSJ国际智能机器人与系统会议，IROS 2018，马德里，西班牙，2018年10月1-5日*。IEEE，2018年，第1-8页。'
- en: '[20] G. Lan, M. van Hooft, M. De Carlo, and J. M. Tomczak, “Learning locomotion
    skills in evolvable robots,” *Neurocomputing*, vol. 452, pp. 294–306, 2021.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] G. Lan, M. van Hooft, M. De Carlo 和 J. M. Tomczak，“在可进化机器人中学习运动技能”，*神经计算*，第452卷，第294-306页，2021年。'
- en: '[21] G. Lan, M. De Carlo, F. van Diggelen, J. M. Tomczak, and D. M. Roijers,
    “Learning directed locomotion in modular robots with evolvable morphologies,”
    *Applied Soft Computing*, vol. 111, p. 107688, 2021.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] G. Lan, M. De Carlo, F. van Diggelen, J. M. Tomczak 和 D. M. Roijers，“在具有可进化形态的模块化机器人中学习定向运动”，*应用软计算*，第111卷，第107688页，2021年。'
- en: '[22] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun, “Deep learning
    for 3D point clouds: A survey,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 43,
    no. 12, pp. 4338–4364, 2021.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Y. Guo、H. Wang、Q. Hu、H. Liu、L. Liu 和 M. Bennamoun，“3D 点云的深度学习：一项综述，” *IEEE
    计算机学会模式分析与机器智能汇刊*，卷号 43，第12期，第4338–4364页，2021年。'
- en: '[23] B. Li, T. Zhang, and T. Xia, “Vehicle detection from 3D lidar using fully
    convolutional network,” in *Robotics: Science and Systems XII, University of Michigan,
    Ann Arbor, Michigan, USA, June 18 - June 22, 2016*, D. Hsu, N. M. Amato, S. Berman,
    and S. A. Jacobs, Eds., 2016.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] B. Li、T. Zhang 和 T. Xia，“基于全卷积网络的 3D 激光雷达车辆检测，” 在 *机器人学：第十二届科学与系统会议，密歇根大学，安娜堡，密歇根州，美国，2016年6月18-22日*，D.
    Hsu、N. M. Amato、S. Berman 和 S. A. Jacobs 主编，2016年。'
- en: '[24] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the KITTI vision benchmark suite,” in *2012 IEEE Conference on Computer Vision
    and Pattern Recognition, Providence, RI, USA, June 16-21, 2012*.   IEEE Computer
    Society, 2012, pp. 3354–3361.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] A. Geiger、P. Lenz 和 R. Urtasun，“我们准备好自动驾驶了吗？KITTI 视觉基准套件，” 在 *2012 IEEE
    计算机视觉与模式识别大会，普罗维登斯，RI，美国，2012年6月16-21日*。IEEE计算机学会，2012年，第3354–3361页。'
- en: '[25] M. Liang, B. Yang, S. Wang, and R. Urtasun, “Deep continuous fusion for
    multi-sensor 3D object detection,” in *Computer Vision - ECCV 2018 - 15th European
    Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XVI*, ser.
    Lecture Notes in Computer Science, V. Ferrari, M. Hebert, C. Sminchisescu, and
    Y. Weiss, Eds., vol. 11220.   Springer, 2018, pp. 663–678.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] M. Liang、B. Yang、S. Wang 和 R. Urtasun，“用于多传感器 3D 物体检测的深度连续融合，” 在 *计算机视觉
    - ECCV 2018 - 第15届欧洲会议，慕尼黑，德国，2018年9月8-14日，会议论文集，第十六部分*，系列：计算机科学讲义丛书，V. Ferrari、M.
    Hebert、C. Sminchisescu 和 Y. Weiss 主编，卷号 11220。Springer，2018年，第663–678页。'
- en: '[26] M. Liang, B. Yang, Y. Chen, R. Hu, and R. Urtasun, “Multi-task multi-sensor
    fusion for 3D object detection,” in *IEEE Conference on Computer Vision and Pattern
    Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019*.   Computer Vision
    Foundation / IEEE, 2019, pp. 7345–7353.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] M. Liang、B. Yang、Y. Chen、R. Hu 和 R. Urtasun，“用于 3D 物体检测的多任务多传感器融合，” 在
    *IEEE 计算机视觉与模式识别大会，CVPR 2019，长滩，加州，美国，2019年6月16-20日*。计算机视觉基金会 / IEEE，2019年，第7345–7353页。'
- en: '[27] H. Lu, X. Chen, G. Zhang, Q. Zhou, Y. Ma, and Y. Zhao, “Scanet: Spatial-channel
    attention network for 3D object detection,” in *IEEE International Conference
    on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom,
    May 12-17, 2019*.   IEEE, 2019, pp. 1992–1996.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] H. Lu、X. Chen、G. Zhang、Q. Zhou、Y. Ma 和 Y. Zhao，“Scanet: 用于 3D 物体检测的空间通道注意力网络，”
    在 *IEEE国际声学、语音和信号处理会议，ICASSP 2019，布莱顿，英国，2019年5月12-17日*。IEEE，2019年，第1992–1996页。'
- en: '[28] G. Lan, T. Liu, X. Wang, X. Pan, and Z. Huang, “A semantic web technology
    index,” *Scientific reports*, vol. 12, no. 1, p. 3672, 2022.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] G. Lan、T. Liu、X. Wang、X. Pan 和 Z. Huang，“一种语义网技术索引，” *Scientific Reports*，卷号
    12，第1期，页码 3672，2022年。'
- en: '[29] Z. Yang, Y. Sun, S. Liu, X. Shen, and J. Jia, “IPOD: intensive point-based
    object detector for point cloud,” *CoRR*, vol. abs/1812.05276, 2018. [Online].
    Available: [http://arxiv.org/abs/1812.05276](http://arxiv.org/abs/1812.05276)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Z. Yang、Y. Sun、S. Liu、X. Shen 和 J. Jia，“IPOD: 强化点云对象检测器，” *CoRR*，卷号 abs/1812.05276，2018年。[在线]。可用网址：[http://arxiv.org/abs/1812.05276](http://arxiv.org/abs/1812.05276)'
- en: '[30] S. Shi, X. Wang, and H. Li, “Pointrcnn: 3D object proposal generation
    and detection from point cloud,” in *IEEE Conference on Computer Vision and Pattern
    Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019*.   Computer Vision
    Foundation / IEEE, 2019, pp. 770–779.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] S. Shi、X. Wang 和 H. Li，“Pointrcnn: 从点云生成和检测 3D 物体提议，” 在 *IEEE 计算机视觉与模式识别大会，CVPR
    2019，长滩，加州，美国，2019年6月16-20日*。计算机视觉基金会 / IEEE，2019年，第770–779页。'
- en: '[31] J. Zarzar, S. Giancola, and B. Ghanem, “Pointrgcn: Graph convolution networks
    for 3D vehicles detection refinement,” *CoRR*, vol. abs/1911.12236, 2019\. [Online].
    Available: [http://arxiv.org/abs/1911.12236](http://arxiv.org/abs/1911.12236)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Zarzar、S. Giancola 和 B. Ghanem，“Pointrgcn: 用于 3D 车辆检测精化的图卷积网络，” *CoRR*，卷号
    abs/1911.12236，2019年。[在线]。可用网址：[http://arxiv.org/abs/1911.12236](http://arxiv.org/abs/1911.12236)'
- en: '[32] S. Vora, A. H. Lang, B. Helou, and O. Beijbom, “Pointpainting: Sequential
    fusion for 3D object detection,” in *2020 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020*.   Computer
    Vision Foundation / IEEE, 2020, pp. 4603–4611.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] S. Vora、A. H. Lang、B. Helou 和 O. Beijbom，“Pointpainting: 用于 3D 物体检测的序列融合，”
    在 *2020 IEEE/CVF 计算机视觉与模式识别大会，CVPR 2020，西雅图，WA，美国，2020年6月13-19日*。计算机视觉基金会 / IEEE，2020年，第4603–4611页。'
- en: '[33] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets for
    3D object detection from RGB-D data,” in *2018 IEEE Conference on Computer Vision
    and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*.   Computer
    Vision Foundation / IEEE Computer Society, 2018, pp. 918–927.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] C. R. Qi, W. Liu, C. Wu, H. Su, 和 L. J. Guibas, “Frustum pointnets用于从RGB-D数据中进行3D目标检测,”
    在 *2018 IEEE计算机视觉与模式识别会议, CVPR 2018, 盐湖城, UT, 美国, 2018年6月18-22日*。计算机视觉基金会 / IEEE计算机学会,
    2018, 页码 918–927.'
- en: '[34] X. Zhao, Z. Liu, R. Hu, and K. Huang, “3D object detection using scale
    invariant and feature reweighting networks,” in *The Thirty-Third AAAI Conference
    on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications
    of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on
    Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii,
    USA, January 27 - February 1, 2019*.   AAAI Press, 2019, pp. 9267–9274.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] X. Zhao, Z. Liu, R. Hu, 和 K. Huang, “使用尺度不变和特征重标定网络进行3D目标检测,” 在 *第三十三届AAAI人工智能会议,
    AAAI 2019, 第三十一届人工智能创新应用会议, IAAI 2019, 第九届AAAI教育进展研讨会, EAAI 2019, 檀香山, 夏威夷, 美国,
    2019年1月27日 - 2月1日*。AAAI出版社, 2019, 页码 9267–9274.'
- en: '[35] M. Jiang, Y. Wu, and C. Lu, “Pointsift: A sift-like network module for
    3D point cloud semantic segmentation,” *CoRR*, vol. abs/1807.00652, 2018. [Online].
    Available: [http://arxiv.org/abs/1807.00652](http://arxiv.org/abs/1807.00652)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] M. Jiang, Y. Wu, 和 C. Lu, “Pointsift: 类似于SIFT的3D点云语义分割网络模块,” *CoRR*, vol.
    abs/1807.00652, 2018. [在线]. 可用: [http://arxiv.org/abs/1807.00652](http://arxiv.org/abs/1807.00652)'
- en: '[36] S. Song, S. P. Lichtenberg, and J. Xiao, “SUN RGB-D: A RGB-D scene understanding
    benchmark suite,” in *IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2015, Boston, MA, USA, June 7-12, 2015*.   IEEE Computer Society, 2015, pp.
    567–576.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Song, S. P. Lichtenberg, 和 J. Xiao, “SUN RGB-D: RGB-D场景理解基准套件,” 在 *IEEE计算机视觉与模式识别会议,
    CVPR 2015, 波士顿, MA, 美国, 2015年6月7-12日*。IEEE计算机学会, 2015, 页码 567–576.'
- en: '[37] D. Xu, D. Anguelov, and A. Jain, “Pointfusion: Deep sensor fusion for
    3D bounding box estimation,” in *2018 IEEE Conference on Computer Vision and Pattern
    Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*.   Computer
    Vision Foundation / IEEE Computer Society, 2018, pp. 244–253.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] D. Xu, D. Anguelov, 和 A. Jain, “Pointfusion: 深度传感器融合用于3D边界框估计,” 在 *2018
    IEEE计算机视觉与模式识别会议, CVPR 2018, 盐湖城, UT, 美国, 2018年6月18-22日*。计算机视觉基金会 / IEEE计算机学会,
    2018, 页码 244–253.'
- en: '[38] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *2016 IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016*.   IEEE Computer Society, 2016,
    pp. 770–778.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] K. He, X. Zhang, S. Ren, 和 J. Sun, “深度残差学习用于图像识别,” 在 *2016 IEEE计算机视觉与模式识别会议,
    CVPR 2016, 拉斯维加斯, NV, 美国, 2016年6月27-30日*。IEEE计算机学会, 2016, 页码 770–778.'
- en: '[39] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3D classification and segmentation,” in *2017 IEEE Conference on
    Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26,
    2017*.   IEEE Computer Society, 2017, pp. 77–85.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] C. R. Qi, H. Su, K. Mo, 和 L. J. Guibas, “Pointnet: 深度学习点集用于3D分类和分割,” 在
    *2017 IEEE计算机视觉与模式识别会议, CVPR 2017, 檀香山, HI, 美国, 2017年7月21-26日*。IEEE计算机学会, 2017,
    页码 77–85.'
- en: '[40] Z. Wang and K. Jia, “Frustum convnet: Sliding frustums to aggregate local
    point-wise features for amodal 3D object detection,” *CoRR*, vol. abs/1903.01864,
    2019\. [Online]. Available: [http://arxiv.org/abs/1903.01864](http://arxiv.org/abs/1903.01864)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Z. Wang 和 K. Jia, “Frustum convnet: 滑动视锥体以聚合局部点特征用于模态3D目标检测,” *CoRR*,
    vol. abs/1903.01864, 2019. [在线]. 可用: [http://arxiv.org/abs/1903.01864](http://arxiv.org/abs/1903.01864)'
- en: '[41] B. Yang, W. Luo, and R. Urtasun, “PIXOR: real-time 3D object detection
    from point clouds,” in *2018 IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*.   Computer Vision Foundation
    / IEEE Computer Society, 2018, pp. 7652–7660.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] B. Yang, W. Luo, 和 R. Urtasun, “PIXOR: 实时3D目标检测从点云,” 在 *2018 IEEE计算机视觉与模式识别会议,
    CVPR 2018, 盐湖城, UT, 美国, 2018年6月18-22日*。计算机视觉基金会 / IEEE计算机学会, 2018, 页码 7652–7660.'
- en: '[42] B. Yang, M. Liang, and R. Urtasun, “HDNET: exploiting HD maps for 3D object
    detection,” in *2nd Annual Conference on Robot Learning, CoRL 2018, Zürich, Switzerland,
    29-31 October 2018, Proceedings*, ser. Proceedings of Machine Learning Research,
    vol. 87.   PMLR, 2018, pp. 146–155.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] B. Yang, M. Liang, 和 R. Urtasun, “HDNET: 利用高清地图进行3D目标检测,” 在 *第2届年度机器人学习会议,
    CoRL 2018, 苏黎世, 瑞士, 2018年10月29-31日, 会议论文集*, 机器学习研究系列, vol. 87。PMLR, 2018, 页码 146–155.'
- en: '[43] J. Beltrán, C. Guindel, F. M. Moreno, D. Cruzado, F. García, and A. de la
    Escalera, “Birdnet: A 3D object detection framework from lidar information,” in
    *21st International Conference on Intelligent Transportation Systems, ITSC 2018,
    Maui, HI, USA, November 4-7, 2018*, W. Zhang, A. M. Bayen, J. J. S. Medina, and
    M. J. Barth, Eds.   IEEE, 2018, pp. 3517–3523.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] J. Beltrán, C. Guindel, F. M. Moreno, D. Cruzado, F. García, 和 A. de la
    Escalera, “Birdnet: 基于激光雷达信息的3D物体检测框架，” 在 *第21届国际智能交通系统会议，ITSC 2018, 毛伊岛, HI,
    美国, 2018年11月4-7日*, W. Zhang, A. M. Bayen, J. J. S. Medina, 和 M. J. Barth 编, IEEE,
    2018年, 页码 3517–3523。'
- en: '[44] B. Li, “3D fully convolutional network for vehicle detection in point
    cloud,” in *2017 IEEE/RSJ International Conference on Intelligent Robots and Systems,
    IROS 2017, Vancouver, BC, Canada, September 24-28, 2017*.   IEEE, 2017, pp. 1513–1518.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] B. Li, “用于点云中车辆检测的3D全卷积网络，” 在 *2017 IEEE/RSJ国际智能机器人与系统会议, IROS 2017, 温哥华,
    BC, 加拿大, 2017年9月24-28日*. IEEE, 2017年, 页码 1513–1518。'
- en: '[45] M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong, and I. Posner, “Vote3Deep:
    Fast object detection in 3D point clouds using efficient convolutional neural
    networks,” in *2017 IEEE International Conference on Robotics and Automation,
    ICRA 2017, Singapore, Singapore, May 29 - June 3, 2017*.   IEEE, 2017, pp. 1355–1361.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong, 和 I. Posner, “Vote3Deep:
    使用高效卷积神经网络在3D点云中进行快速物体检测，” 在 *2017 IEEE国际机器人与自动化会议, ICRA 2017, 新加坡, 2017年5月29日
    - 6月3日*. IEEE, 2017年, 页码 1355–1361。'
- en: '[46] X. Li, J. Guivant, N. Kwok, Y. Xu, R. Li, and H. Wu, “Three-dimensional
    backbone network for 3D object detection in traffic scenes,” *arXiv e-prints*,
    2019.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] X. Li, J. Guivant, N. Kwok, Y. Xu, R. Li, 和 H. Wu, “三维骨干网络用于交通场景中的3D物体检测，”
    *arXiv 预印本*, 2019年。'
- en: '[47] Y. Zhou and O. Tuzel, “Voxelnet: End-to-end learning for point cloud based
    3D object detection,” in *2018 IEEE Conference on Computer Vision and Pattern
    Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*.   Computer
    Vision Foundation / IEEE Computer Society, 2018, pp. 4490–4499.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Y. Zhou 和 O. Tuzel, “Voxelnet: 基于点云的3D物体检测的端到端学习，” 在 *2018 IEEE计算机视觉与模式识别会议,
    CVPR 2018, 盐湖城, UT, 美国, 2018年6月18-22日*. 计算机视觉基金会 / IEEE计算机学会, 2018年, 页码 4490–4499。'
- en: '[48] Y. Yan, Y. Mao, and B. Li, “SECOND: sparsely embedded convolutional detection,”
    *Sensors*, vol. 18, no. 10, p. 3337, 2018.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Y. Yan, Y. Mao, 和 B. Li, “SECOND: 稀疏嵌入卷积检测，” *传感器*, 卷18, 期10, 页码 3337,
    2018年。'
- en: '[49] V. A. Sindagi, Y. Zhou, and O. Tuzel, “MVX-Net: Multimodal VoxelNet for
    3D Object Detection,” *arXiv e-prints*, p. arXiv:1904.01649, Apr. 2019.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] V. A. Sindagi, Y. Zhou, 和 O. Tuzel, “MVX-Net: 多模态体素网络用于3D物体检测，” *arXiv
    预印本*, 页码 arXiv:1904.01649, 2019年4月。'
- en: '[50] C. He, H. Zeng, J. Huang, X. Hua, and L. Zhang, “Structure aware single-stage
    3D object detection from point cloud,” in *2020 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020*.   Computer
    Vision Foundation / IEEE, 2020, pp. 11 870–11 879.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] C. He, H. Zeng, J. Huang, X. Hua, 和 L. Zhang, “结构感知单阶段3D物体检测从点云中，” 在 *2020
    IEEE/CVF计算机视觉与模式识别会议, CVPR 2020, 西雅图, WA, 美国, 2020年6月13-19日*. 计算机视觉基金会 / IEEE,
    2020年, 页码 11 870–11 879。'
- en: '[51] Z. Yang, Y. Sun, S. Liu, and J. Jia, “3DSSD: point-based 3D single stage
    object detector,” in *2020 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020*.   Computer Vision
    Foundation / IEEE, 2020, pp. 11 037–11 045.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Z. Yang, Y. Sun, S. Liu, 和 J. Jia, “3DSSD: 基于点的3D单阶段物体检测器，” 在 *2020 IEEE/CVF计算机视觉与模式识别会议,
    CVPR 2020, 西雅图, WA, 美国, 2020年6月13-19日*. 计算机视觉基金会 / IEEE, 2020年, 页码 11 037–11 045。'
- en: '[52] G. P. Meyer, A. Laddha, E. Kee, C. Vallespi-Gonzalez, and C. K. Wellington,
    “Lasernet: An efficient probabilistic 3D object detector for autonomous driving,”
    in *IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long
    Beach, CA, USA, June 16-20, 2019*.   Computer Vision Foundation / IEEE, 2019,
    pp. 12 677–12 686.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] G. P. Meyer, A. Laddha, E. Kee, C. Vallespi-Gonzalez, 和 C. K. Wellington,
    “Lasernet: 高效的概率3D物体检测器用于自动驾驶，” 在 *IEEE计算机视觉与模式识别会议, CVPR 2019, 长滩, CA, 美国, 2019年6月16-20日*.
    计算机视觉基金会 / IEEE, 2019年, 页码 12 677–12 686。'
- en: '[53] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, “Pointcnn: Convolution
    on x-transformed points,” in *Advances in Neural Information Processing Systems
    31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
    December 3-8, 2018, Montréal, Canada*, S. Bengio, H. M. Wallach, H. Larochelle,
    K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., 2018, pp. 828–838.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, 和 B. Chen, “Pointcnn: 在x变换点上的卷积，”
    在 *神经信息处理系统进展 31: 神经信息处理系统年会 2018, NeurIPS 2018, 2018年12月3-8日, 蒙特利尔, 加拿大*, S. Bengio,
    H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, 和 R. Garnett 编, 2018年,
    页码 828–838。'
- en: '[54] Z. Liu, H. Tang, Y. Lin, and S. Han, “Point-voxel CNN for efficient 3D
    deep learning,” in *Advances in Neural Information Processing Systems 32: Annual
    Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December
    8-14, 2019, Vancouver, BC, Canada*, H. M. Wallach, H. Larochelle, A. Beygelzimer,
    F. d’Alché-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 963–973.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Z. Liu， H. Tang， Y. Lin 和 S. Han，“用于高效 3D 深度学习的点-体素 CNN，” 见 *Advances
    in Neural Information Processing Systems 32: Annual Conference on Neural Information
    Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada*，H.
    M. Wallach， H. Larochelle， A. Beygelzimer， F. d’Alché-Buc， E. B. Fox 和 R. Garnett
    主编，2019年，第 963–973 页。'
- en: '[55] H. Fan and Y. Yang, “Pointrnn: Point recurrent neural network for moving
    point cloud processing,” *CoRR*, vol. abs/1910.08287, 2019\. [Online]. Available:
    [http://arxiv.org/abs/1910.08287](http://arxiv.org/abs/1910.08287)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] H. Fan 和 Y. Yang， “Pointrnn: 用于移动点云处理的点递归神经网络，” *CoRR*，卷 abs/1910.08287，2019年。
    [在线] 可用: [http://arxiv.org/abs/1910.08287](http://arxiv.org/abs/1910.08287)'
- en: '[56] X. Liu, M. Yan, and J. Bohg, “Meteornet: Deep learning on dynamic 3D point
    cloud sequences,” in *2019 IEEE/CVF International Conference on Computer Vision,
    ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019*.   IEEE, 2019,
    pp. 9245–9254.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] X. Liu， M. Yan 和 J. Bohg，“Meteornet: 动态 3D 点云序列上的深度学习，” 见 *2019 IEEE/CVF
    International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South),
    October 27 - November 2, 2019*。 IEEE，2019年，第 9245–9254 页。'
