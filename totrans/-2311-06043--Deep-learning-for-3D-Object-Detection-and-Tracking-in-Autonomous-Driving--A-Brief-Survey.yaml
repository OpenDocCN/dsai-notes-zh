- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:36:01'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2311.06043] Deep learning for 3D Object Detection and Tracking in Autonomous
    Driving: A Brief Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.06043](https://ar5iv.labs.arxiv.org/html/2311.06043)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep learning for 3D Object Detection and Tracking in Autonomous Driving: A
    Brief Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yang Peng
  prefs: []
  type: TYPE_NORMAL
- en: Southern University of Science and Technology
  prefs: []
  type: TYPE_NORMAL
- en: 12032453@mail.sustech.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Object detection and tracking are vital and fundamental tasks for autonomous
    driving, aiming at identifying and locating objects from those predefined categories
    in a scene. 3D point cloud learning has been attracting more and more attention
    among all other forms of self-driving data. Currently, there are many deep learning
    methods for 3D object detection. However, the tasks of object detection and tracking
    for point clouds still need intensive study due to the unique characteristics
    of point cloud data. To help get a good grasp of the present situation of this
    research, this paper shows recent advances in deep learning methods for 3D object
    detection and tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object detection occupies an important position in the ﬁeld of computer vision
    all along. As the cornerstone of image understanding, object detection is widely
    applied in large numbers of areas such as autonomous driving and robot vision.
    Object detection enables an autonomous driving system to see those driving environments
    clearly and understand what are they just like human drivers.
  prefs: []
  type: TYPE_NORMAL
- en: With the fast development of deep learning [[1](#bib.bib1)], it becomes easier
    to learn complex, subtle and abstract features for a deep learning model without
    manual feature extraction like the traditional way. In view of the excellent ability
    of deep learning to process data, major progress has been made in the research
    of object detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The research for 2D object detection has been well-developed [[2](#bib.bib2)].
    The accuracy and efﬁciency of algorithms proposed for solving 2D object detection
    problems have reached a high level and those methods play an important role in
    engineering practice. Traditional detection strategies based on sliding windows
    like [[3](#bib.bib3), [4](#bib.bib4)] keep the mainstream for a long time before
    methods combining deep learning techniques. In this stage when deep learning hasn’t
    been applied to detection, the pipeline[[5](#bib.bib5)] of object detection methods
    mentioned above usually can be categorized into three parts: i) proposal generation;
    ii) feature vector extraction; iii) region classification [[6](#bib.bib6)]. The
    main task of proposal generation is to search the whole image to ﬁnd those positions
    that might contain expected objects. These positions are called regions of interest
    (ROI). The sliding window technique is an intuitive idea proposed to scan through
    the input image and ﬁnd ROI. In the second stage, algorithms will extract a constant-length
    feature vector from each target position of the image. Histogram of Gradients[[3](#bib.bib3)]
    is one of the most popular feature extraction methods proposed by Navneet Dalal
    and Bill Triggs. In general, linear support vector machines are used together
    with HOG, achieving region classiﬁcation.'
  prefs: []
  type: TYPE_NORMAL
- en: With these traditional detectors, much success has been achieved in object detection.
    Nevertheless, there are still some limitations here. The huge search space and
    expensive computations of methods based on sliding windows encourage better solutions.
    Moreover, one shortage which should not be overlooked is that the procedure of
    design and optimize a detector is separate [[7](#bib.bib7)]. This may lead to
    local optimal solution for the system.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/39c3e85c63793beb77ad46dca86ed821.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Major developments and milestones of object detection techniques.
    In 2012 there occurred a turning point drawing a line between traditional and
    deep learning-based object detection methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since deep learning emerged in this ﬁeld, it has become more capable of detecting
    objects. These object detection systems using deep learning techniques can handle
    questions with more facility and rapidity than in the past. We observe a turning
    point in 2012 with the development of DCNN for image classiﬁcation by Krizhevsky
    et al.[[8](#bib.bib8)]. Since this time, there has been an incessant springing
    up of approaches based on deep learning. Currently, the mainstream of object detection
    algorithms can be divided into two types: i)two-stage methods, such as Region-based
    CNN (R-CNN) [[9](#bib.bib9)] and its variants[[10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12)]; ii)one-stage methods such as You Only Look Once (YOLO)[[13](#bib.bib13)]
    and its variants[[14](#bib.bib14)]. Two-stage methods are one kind of region proposal-based
    methods, which propose some possible regions that contain objects ﬁrstly and extract
    feature vectors from these regions respectively. One-stage methods predict the
    categories of objects on the location of the feature maps directly to the contrary,
    discarding the region classiﬁcation steps. The different kinds of methods have
    their own strong points. Fast-RCNN[[10](#bib.bib10)] cut down the processing time
    of the previous network. However, a bottleneck bothers people a lot because the
    computation of this network is quite expensive. Until the appearance of Faster-RCNN[[12](#bib.bib12)]
    this problem gets solved. 2D detectors are not the key point in this paper. For
    the integrity of the whole review and introduction of 3D object detection methods,
    we mention about those methods. Here we present the main progress and milestones
    of 2D object detection techniques in Figure[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Deep learning for 3D Object Detection and Tracking in Autonomous Driving: A
    Brief Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: However, with new application scenarios such as robot vision and autonomous
    driving proposed [[15](#bib.bib15)], the implementation of 2D object detection
    is far from enough. The process of images captured by cameras is projecting 3D
    space into 2D view, which causes the loss of 3D spatial information and is unable
    to satisfy people. More 3D spatial information needs to be considered. With the
    rapid development of many 3D techniques, large amounts of 3D sensors such as LiDARs,
    3D scanners and RGB-D cameras are becoming increasingly available and affordable
    [[16](#bib.bib16)]. In this review, we mainly analyse point clouds obtained by
    using LiDARs. As a common format, each point provides us with useful geometric
    position information and some may contain RGB information. There is an urgent
    demand for 3D object detection and tracking when it comes to autonomous driving.
    In the research of autonomous driving, accurate environmental perception and precise
    location are the keys for an autonomous driving system to achieve reliable navigation,
    information decisions and safe driving in complex and dynamic environments. Compared
    to images whose data quality is influenced by illumination, point cloud is robust
    for different lighting conditions [19]. As a result, point cloud data is widely
    used in this ﬁeld.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the basis of 2D object detection, changes have taken place in the questions
    and requirements of object detection. For the sparsity and the irregularity of
    point clouds, it is impossible to apply 2D object detection methods to 3D point
    clouds. As a result, 2D object detection methods need to be changed to make sure
    those methods can be extended to 3D cases. Similar to the object detection approaches
    applied in images, these 3D object detection methods still can be divided into
    two categories: two-stage methods and one-stage methods. Figure[2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Deep learning for 3D Object Detection and Tracking in Autonomous
    Driving: A Brief Survey") presents a simple classiﬁcation of recent popular 3D
    object detection methods based on deep learning techniques. More details of these
    3D detection methods can be found later Section 3 of this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5e29a2d7919b3717b00d87c361e9a256.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Summary of major deep learning-based methods of 3D object detection.'
  prefs: []
  type: TYPE_NORMAL
- en: For this paper, we mainly focus on these state-of-the-art methods of 3D object
    detection on the basis of point clouds. The remaining parts of this review are
    organized as follows. Related backgrounds, including the problems identiﬁcation,
    the questions description, the key challenges and the motivation of this review
    are stated in Section LABEL:Background. In the third part of this paper, we make
    a detailed interpretation of those current approaches for 3D object detection,
    including their advantages and disadvantages. Section 4 makes a summary of recent
    developments in deep learning methods for point clouds. Furthermore, we point
    out some difﬁcult problems still unsolved on the basis of existing methods and
    provide several possible research directions that might make sense.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c2e3ab9196474698609a42feadfe1304.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Bounding boxes.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the purpose of helping understand how exactly modern deep learning techniques
    can be applied to solve 3D object detection problems, this section describes some
    simple conceptions of the research object and the data.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 The Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The problem of 3D object detection, which is similar to 2D detection, can be
    formulated as follows. Given some point cloud data, a 3D detector needs to determine
    whether or not there are instances from those predeﬁned categories and if yes,
    locate where they are. One thing we should ﬁrst consider is these predeﬁned categories,
    which are problem-dependent. Considering the case of autonomous driving, the research
    community is more interested in those trafﬁc participants (e.g., bicycles, cars
    and pedestrians). In general, unstructured scenes such as sky and clouds are often
    not our research objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, a 3D bounding box is used to indicate the location of those 3D
    objects. It is a rectangular cuboid placed in 3D spaces and usually has three
    types of representation methods, such as axis-aligned 3D centre offset method[[17](#bib.bib17)],
    8-corners method[[18](#bib.bib18)] and 4-corner-2-height method[[19](#bib.bib19)].
    In general, the ﬁnal result of 3D object detection is using 3D bounding boxes
    to earmark 3D objects, as shown in Figure[3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Deep learning for 3D Object Detection and Tracking in Autonomous Driving: A
    Brief Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Main Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generic object detection methods aim at recognizing and localizing interested
    object categories. In most computer vision problems, people care about accuracy
    and efﬁciency mostly. Object detection is no exception.
  prefs: []
  type: TYPE_NORMAL
- en: Those point cloud data acquired from LiDARs, depth cameras and binocular cameras
    can be used in 3D object detection. However, with the increase in the distance
    between objects and cameras, there would be a sudden drop in the density of the
    point clouds, causing a huge change in the density. What’s more, a few portions
    of the objects might be invisible owing to the occlusion, which causes a problem
    that there exists a large lag in the distribution of the point clouds of the same
    object. To sum up, the representations of point clouds are quite different and
    hard for a detector to make fairly accurate detections.
  prefs: []
  type: TYPE_NORMAL
- en: Another point we should notice is the sparsity and the irregularity of point
    clouds. The order of those points of the same object is greatly influenced by
    different acquisition devices and different coordinate systems. Those irregular
    cloud points make it really hard for an end-to-end model to handle. In addition,
    compared with the large scale of scenes, the coverage of LiDAR sampling points
    has strong sparsity. It is well-known that with the rapid development of the artiﬁcial
    intelligence, deep neural network is widely used in most tasks of autonomous driving
    because of their high accuracy and strong robustness. The performance of deep
    neural networks [[20](#bib.bib20), [21](#bib.bib21)] used in the ﬁeld of 2D object
    detection is much better than other types of algorithms. Nevertheless, we have
    just mentioned the characteristics of point clouds, which result in the loss of
    efﬁcacy of those deep learning methods. This is why the movement of research in
    deep learning-based 3D object detection is slow. As a result, in the stage of
    data preprocessing, how to represent sparse point cloud data for better use deserves
    to be investigated thoroughly.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the useful information such as depth and spatial information that point
    clouds have, it seems that utilizing image data at the same time performs better
    sometimes. Therefore a number of methods combining LiDAR point clouds and images
    are well developed. In this paper, we mainly talk about deep learning methods
    for point clouds. For the completeness of this paper, some fusion methods which
    use 2D images as well will also be included.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Motivation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Related methods mentioned in this paper are organized based on their different
    algorithm execution processes. In other words, we mainly focus on whether a detector
    ﬁrst generates proposals. This paper intends to investigate state-of-the-art research
    about 3D object detection for point clouds and categorize those methods, presenting
    a comprehensive summary of recent advances based on deep learning technologies
    for point clouds. It also covers the comparative advantages and disadvantages
    of different methods, by observing those questions still unsolved to inspire more
    future possible research directions.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Two-Stage Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Two-stage detectors ﬁrst detect a number of possible regions also called proposals
    which contain objects, then make predictions for extracted features. According
    to the introduction of [[22](#bib.bib22)], we further categorize those two-stage
    methods into three kinds: i) multi-view-based; ii) segmentation-based; iii) frustum-based
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-View Methods. It is necessary to realize that texture information, which
    is important for class discrimination is not included in point clouds. By contrast,
    monocular images cannot provide us with depth information for accurate 3D localization
    and size estimation. So multi-view methods try to use different modalities to
    improve the performance. A deep fusion combines region-wise features from multiple
    views (e.g., bird’s eye view (BEV), LiDAR front view (FV) and images) and gets
    oriented 3D boxes, as shown in Figure [4(a)](#S3.F4.sf1 "In Figure 4 ‣ 3.1 Two-Stage
    Methods ‣ 3 Methods ‣ Deep learning for 3D Object Detection and Tracking in Autonomous
    Driving: A Brief Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8743cd434874790f8c7462cdba15eb8d.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) MV3D
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/05b10386b0dd12033029ab7a55651b30.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) PointRCNN
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/664c23c1a057d998f81b52b1a0fb0cbb.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Frustum PointNet
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Typical networks for 3D object detection methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One remarkable contribution made by Chen et al. [[18](#bib.bib18)] is their
    multi-view 3D object detection network (MV3D). In terms of the expensive computations
    of existing LiDAR-based 3D methods, the disadvantage of current image-based methods
    and the limitation of existing multi-modal fusion methods, this work tries to
    propose a method to overcome those disadvantages preventing the development of
    3D object detection. The MV3D is composed of two subnetworks: one is a 3D proposal
    network to generate 3D proposals and another subnetwork is a deep fusion network,
    whose main function is trying to fuse multi-view feature proposals. They use 3D
    proposals and project them to three views. Theoretically, it is of great useful
    that 3D proposals can be projected to any view in 3D space. In their experiments,
    LiDAR-based methods achieve an average precision of 87.65% at an Intersection-over-Union
    (IOU) of 0.5 on the KITTI validation set, which obtains 30% higher over VeloFCN
    [[23](#bib.bib23)]. Besides, the authors illustrate the construction process of
    BEV and state three advantages of BEV, which casts light on their search for a
    bird’s eye view and has a certain reference value for future research. Nevertheless,
    this model is not perfect because the whole procedure for detection is too slow
    to have practical applications. This region proposal network(RPN) is not suitable
    for small object instances in BEV. Small objects occupy a fraction of a pixel
    in a feature map, causing insufﬁcient data to extract features. Subsequently,
    a number of endeavours have been made to improve the MV3D model.'
  prefs: []
  type: TYPE_NORMAL
- en: Several attempts are made on the efﬁciency of information fusion by applying
    different modalities. Ku et al. [[19](#bib.bib19)] proposed an aggregation view
    object detection network (AVOD), which uses LIDAR point clouds and RGB images
    as well. Unlike MV3D it extends ROI feature fusion to the stage of proposal generation.
    Their RPN has a novel architecture having the ability to ﬁnish multi-modal feature
    fusion on high-resolution feature maps, thus it is convenient to generate accurate
    region proposals for tiny objects in scenes. Furthermore, they test the performance
    of AVOD on the KITTI Object Detection Benchmark [[24](#bib.bib24)] and the result
    shows that AVOD can run in real-time and with a low memory expense. However, ROI
    feature fusion is conﬁned to high-level feature maps. Furthermore, only those
    features extracted from selected object regions will be fused. ContFuse [[25](#bib.bib25)]
    was developed to overcome those drawbacks. They exploited continuous convolutions
    to fuse feature maps at different resolutions. With the projection of the LIDAR
    points image and BEV spaces can correspond with each other. In other words, in
    the BEV spaces corresponding image features for each point can be extracted and
    projecting image features into the BEV plane can obtain dense BEV feature maps.
    However, extremely sparse point clouds set limits on this kind of fusion. Liang
    et al. [[26](#bib.bib26)] proposed an object detection network for multiple tasks
    (e.g., 3D object detection, ground estimation and depth completion) with multi-sensors.
    This paper utilizes the strong points from both point-wise and ROI-wise feature
    fusion. Speciﬁcally, the implementation of multiple tasks helps whole network
    learn better representations. Consequently, the KITTI and TOR4D datasets are used
    for validation of this approach, which proves to achieve outstanding improvement
    on detection problems and outperforms state-of-the-art approaches in the past.
  prefs: []
  type: TYPE_NORMAL
- en: Another direction for improvement is to explore how to extract robust representations
    of the inputs. A novel Spatial-Channel Attention Network (SCANet) [[27](#bib.bib27)]
    was proposed aiming at achieving high accuracy 3D object detection. In this paper,
    they raise a brand-new Spatial-Channel Attention (SCA) module and an Extension
    Spatial Upsample (ESU) module for 3D region proposal use. The former module can
    focus on global and multi-scale contexts in a scene and can capture those discriminative
    features. The latter module combines different scale low-level features and produces
    reliable 3D region proposals. In addition, a way to fuse those features better
    is to apply a new multi-level fusion scheme allowing more interactions between
    them. Finally, the experimental results present that at a speed of 11.1 FPS their
    method is 5 times faster than MV3D.
  prefs: []
  type: TYPE_NORMAL
- en: 'Segmentation-based Methods. This kind of method usually has preliminary processing
    of semantic segmentation [[28](#bib.bib28)], which means that using semantic segmentation
    techniques to take out those background points can help generate high-quality
    proposals from foreground points. One typical segmentation-based network is shown
    in Figure [4(b)](#S3.F4.sf2 "In Figure 4 ‣ 3.1 Two-Stage Methods ‣ 3 Methods ‣
    Deep learning for 3D Object Detection and Tracking in Autonomous Driving: A Brief
    Survey"). In addition, compared with those multi-view methods mentioned above,
    segmentation-based methods obtain higher recall scores and can be applied in complex
    scenes with a large number of occlusions and crowded object swell.'
  prefs: []
  type: TYPE_NORMAL
- en: The first method to be introduced is proposed in [[29](#bib.bib29)]. This method
    named IPOD ﬁrst completes semantic segmentation on the images and generates point-based
    proposals. Generating proposals at those positive points remains high ﬁdelity
    as well. Besides, some possible questions such as proposal redundancy and ambiguity
    have also been considered. This paper provided a new criterion named PointsIoU
    to address those problems. Experimental results surely show that this model is
    better than many 3D detection methods, especially for those scenes with high occlusion.
  prefs: []
  type: TYPE_NORMAL
- en: Another classical segmentation-based network in our review to talk about is
    PointRCNN [[30](#bib.bib30)] proposed by Shi et al. This network generates 3D
    proposals via segmentation techniques in the ﬁrst stage and in stage two those
    proposals will be reﬁned to get ﬁnal detection results. Unlike IPOD, PointRCNN
    directly segments the point cloud to generate high-quality proposals rather than
    applying 2D object segmentation. An important module of this network is Bin-based
    3D bounding box generation and those boxes are regressed from the foreground points.
    Instead of using L1 or L2 loss for regression, this module adopts a bin-based
    method. That is to say, they ﬁrst split each foreground point into different bins,
    then regress boxes among each bin. This work achieves an RPN in 3D space. Drawing
    lessons from the RPN stage of PointRCNN, Jesus et al. proposed a graph-based 3D
    detection pipeline named PointRGCN [[31](#bib.bib31)], which takes advantage of
    advances in GCNs, including two subnetworks R-GCN and C-GCN. R-GCN is a residual
    GCN that achieves pre-proposal feature aggregation by using all points in a proposal.
    C-GCN is a contextual GCN whose main function is reﬁning proposals via the shared
    contextual information between different proposals. Sourabh et al. proposed PointPainting
    [[32](#bib.bib32)] which works by projecting lidar points into the output of an
    image-based semantic segmentation network and appending the class scores to each
    point. Those appended points can be fed to any existing lidar-only detector such
    as PointRCNN we have mentioned above. Their work can ﬁll the gap that comprehensive
    information provided by different sensors is beneficial for fusion-based methods,
    but experimental results on the main benchmark datasets show that lidar-only methods
    outperform fusion-based methods in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Frustum-based Methods. These methods leverage both mature 2D object detectors
    and advanced 3D deep learning for object localization. They generate 2D object
    region proposals ﬁrst and then generate 3D frustum proposals via lifting a 2D
    bounding box to a frustum that includes a 3D search space for the object. One
    speciﬁc process of the frustum-based method can be seen in Figure [4(c)](#S3.F4.sf3
    "In Figure 4 ‣ 3.1 Two-Stage Methods ‣ 3 Methods ‣ Deep learning for 3D Object
    Detection and Tracking in Autonomous Driving: A Brief Survey"). Certainly, it
    is of great significance to know that although with high efﬁciency to propose
    possible regions of 3D objects, the step-by-step pipeline results in extreme dependency
    on 2D image detectors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Qi et al. did pioneering work in this direction. They proposed a novel framework
    named Frustum PointNets [[33](#bib.bib33)], based on RGB-D data for 3D object
    detection. In their work, the model ﬁrst feeds RGB images to the convolutional
    neural network to obtain 2D proposals and then combines depth information to project
    region to frustum. This is the process of getting frustum proposals. For those
    points contained in the frustum, 3D instance segmentation would be executed. Based
    on the results of segmentation, a lightweight regression PointNet attempts to
    adjust those points via translation so that their centroid is close to amodal
    box centre. Finally, a 3D box estimation network estimates those 3D amodal bounding
    boxes. It is surprising that F-PointNets have the ability to predict correctly
    posed amodal 3D boxes with a few points. However, there still exists some problems
    such as failure to work in the case of multiple instances from the same category.
    Following the work of F-PointNets, Zhao et al. presented a new network architecture
    called SIFRNet [[34](#bib.bib34)] relying on front view images and frustum point
    clouds to predict 3D detection results. The whole network mainly consists of three
    parts: i)3D instance segmentation network (Point-UNet); ii)T-Net; iii)3D box estimation
    network (Point-SENet). They contribute to the improvement of the performance in
    3D segmentation and the efﬁciency of 3D bounding box prediction. PointSIFT [[35](#bib.bib35)]
    module is integrated into their network, capturing orientation information of
    point clouds and achieving strong robustness to shape scaling. A series of experiments
    show that this method achieves better performance on the KITTI dataset and SUN-RGBD
    dataset [[36](#bib.bib36)] when compared to F-PointNets.'
  prefs: []
  type: TYPE_NORMAL
- en: A generic 3D object detection method called PointFusion [[37](#bib.bib37)] is
    presented by Xu et al. To handle the challenge of the combination of various RGB
    images and point cloud data, previous methods usually transform the form of point
    cloud data such as representing point cloud data by 2D image or voxel. It is convenient
    but runs into the problem of losing some information contained in point clouds.
    Instead, this method directly processes images and 3D point clouds via ResNet
    [[38](#bib.bib38)] and PointNet [[39](#bib.bib39)]. The obtained 2D image region
    and its related frustum points are used to precisely regress 3D boxes. They present
    a global fusion network to directly get the 3D box corner locations. At the same
    time, a new dense fusion network for the purpose of predicting spatial offset
    and choosing the ﬁnal prediction results with the highest score.
  prefs: []
  type: TYPE_NORMAL
- en: It is noteworthy that Wang et al. put forward a novel approach called Frustum
    ConvNet [[40](#bib.bib40)], which ﬁrst generates a strand of frustums for each
    proposal and utilizes obtained frustums to group those points. F-PointNet we have
    mentioned above also works directly on raw point cloud, but it has not been designed
    to be a end-to-end pipeline, owing to its T-Net alignment. This method takes this
    factor into account and is designed to combine several beneﬁts from previous works.
    It has been proved that this novel method F-ConvNet for amodal 3D object detection
    with end-to-end style achieves the state-of-the-art performance on the KITTI dataset
    among 2D detectors and is helpful for a large number of applications such as autonomous
    driving.
  prefs: []
  type: TYPE_NORMAL
- en: Two-stage methods obtain several advantages including high accuracy for object
    detection. However, because of the process of generating regions containing pre-deﬁned
    objects, the speed of this object detection procedure will be decreased.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ae44f425dab27114481054c70ea6c6ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A comprehensive view of the proposed 3D object detector from Bird’s
    Eye View (BEV) based on point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 One-Stage Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our discussion, one stage is equal to a single shot in the meaning of predicting
    class probabilities and regressing bounding boxes directly. These methods are
    free of region proposal network (RPN) and post-processing. Consequently, they
    have higher speed than two-stage approaches and are applied to real-time systems
    mostly. We categorize one-stage methods to the form of input data, including BEV-based
    methods, discretization-based methods and point cloud-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: BEV-based Methods. Looking into the name of this kind of method, it is obvious
    that BEV-based methods use BEV representation as input. As mentioned in [[18](#bib.bib18)],
    the BEV map has several strong points in 3D object detection compared to the front
    view or image plane. First, in the BEV map objects are of the same physical sizes
    as the original sizes, thus decreasing the size error that the front view/image
    plane has. Second, it is almost impossible to run into occlusion problems because
    objects in the BEV are located in different places. Last, for autonomous driving
    applications, those trafﬁc participants usually lie on the ground plane and in
    the vertical direction there exists a small variance. So the BEV location is of
    great significance when getting 3D bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yang et al. proposed a proposal-free, one-stage method named PIXOR [[41](#bib.bib41)],
    which represents raw 3D data from the Bird’s Eye View (BEV). We can see the network
    architecture of PIXOR from Figure [5](#S3.F5 "Figure 5 ‣ 3.1 Two-Stage Methods
    ‣ 3 Methods ‣ Deep learning for 3D Object Detection and Tracking in Autonomous
    Driving: A Brief Survey"). Here the BEV representation is chosen because of its
    friendly computation. In the stage of elaborating input representation, they discretize
    3D points contained in those interested scenes with a resolution of $d_{L}$ ×
    $d_{W}$ × $d_{H}$ each cell ﬁrst and then encode the value of each cell to get
    occupancy tensor. Finally, they get a combination of the 2D reﬂectance image and
    the 3D occupancy tensor. When designing network architecture, a fully convolutional
    network is used for dense 3D object detection. A backbone network and a header
    network are composed of PIXOR. The former extracts ordinary representation of
    the input and the latter is used for doing speciﬁc task prediction. Based on experimental
    results, the conclusion has been made that PIXOR outperforms most one-stage methods
    in terms of Average Precision (AP) while running at 10 FPS.'
  prefs: []
  type: TYPE_NORMAL
- en: Later on, Yang et al. [[42](#bib.bib42)] showed us that High-Deﬁnition (HD)
    maps possess strong priors, which is beneﬁcial for the improvement of the performance
    and robustness of 3D object detection. To reach their goal, they developed a single-stage
    detector that works in the Bird’s Eye View (BEV) and fuses LiDAR information.
    Speciﬁcally, they get the coordinates of ground points from the HD map and then
    the absolute distance in the BEV representation would be replaced by the distance
    relative to the ground for the purpose of mending translation variance. Considering
    the situation that HD maps are not available everywhere, they put forward a solution
    that is practicable to utilize an online map prediction module and estimate map
    priors on the basis of LiDAR point cloud data. In their experiments, the baseline
    is a PIXOR++ detector without a map. The results illustrate that this HD map-aware
    model outperforms its baseline on the TOR4D and KITTI evidently. Nevertheless,
    a problem still exists for the poor generalization performance to point cloud
    data with diverse densities. Intending to address this problem, a new network
    architecture was proposed by Beltran et al. named BirdNet [[43](#bib.bib43)].
    They developed a new encoding method for BEV, which is invariant to distance and
    differences on LiDAR devices. The proposed density normalization method enables
    training models on those popular high-resolution lase datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Discretization-based Methods. It is easy to understand this kind of method via
    its name. These methods usually convert raw point cloud data into a regular discrete
    format (e.g., 2D map), and then use deep neural networks to predict the category
    probabilities and 3D boxes of objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first method to use a FCN in the purpose of 3D object detection was proposed
    by Li et al.. Owing to the quick development of convolutional network techniques,
    they raised a new idea to transplant the FCN technique to object detection on
    3D data [[23](#bib.bib23)]. In this method, point cloud data are converted into
    a 2D point map and the FCN is used for predicting the conﬁdences of those objects
    and the bounding boxes at the same time. This is the first attempt to introduce
    the FCN techniques into the object detection on range scan data, generating an
    order and end-to-end framework for detection. However, this paper only analyzes
    the method for 3D range scan from Velogyne 64E. As a result, in their later work,
    they discretized the point cloud into a 4D tensor with dimensions of length, width,
    height and channels [[44](#bib.bib44)]. In this work, they also extended the former
    2D detection method based on FCN to 3D domain for 3D object detection. We can
    observe the detection results in Figure [6](#S3.F6 "Figure 6 ‣ 3.2 One-Stage Methods
    ‣ 3 Methods ‣ Deep learning for 3D Object Detection and Tracking in Autonomous
    Driving: A Brief Survey"). Based on their experimental results, they made a comparison
    with FCN for 2D detection work, observing a gain of over 20% in accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/be868da7e5c33d533fd9c04a37666d0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Direct outcomes of the 3D FCN detection process. (a) Aggregating
    bounding boxes in possession of high objectness conﬁdence and plotting them as
    green boxes. Plotting bounding box predictions coming from with green boxes. (b)
    Plotting those bounding boxes with blue original point clouds after clustering.
    (c) Detection in 3D since (a) and (b) are visualized in the bird’s eye view.'
  prefs: []
  type: TYPE_NORMAL
- en: Although 3D FCN-based method makes progress compared to the previous work, there
    are still some possible questions here. Recalling the process, it is easy to ﬁnd
    that this method has a huge expensive computation because of 3D convolutions and
    the sparsity of the data. Engelcke et al. put forward a computationally efﬁcient
    approach named Vote3Deep to do object detection for 3D point clouds [[45](#bib.bib45)].
    Vote3Deep is a feature-centric voting framework for the purpose of improving the
    efﬁciency of computation. For one thing, building an efﬁcient convolutional layer
    via leveraging a voting framework is of great use. For another, to take advantage
    of sparse convolutional layers of the whole CNN stack they utilize modiﬁed linear
    units and sparsity penalty. Li et al. proposed a 3D backbone network learning
    3D features from most raw data [[46](#bib.bib46)]. The network is constructed
    to address the problem of the lack of powerful 3D feature extraction methods.
    As a result, this method obtains rich 3D features and would not introduce a huge
    computational burden.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhou et al. proposed an end-to-end network named VoxelNet for 3D object detection
    [[47](#bib.bib47)]. Figure [7](#S3.F7 "Figure 7 ‣ 3.2 One-Stage Methods ‣ 3 Methods
    ‣ Deep learning for 3D Object Detection and Tracking in Autonomous Driving: A
    Brief Survey") shows how VoxelNet works. They projected point clouds into equally
    spaced voxels and transformed a number of points in each voxel into a centralized
    feature representation via the brand-proposed voxel feature encoding (VFE) layer.
    The method using skill to represent point clouds with voxels has great performance.
    Voxel-based methods sometimes are designed with the intention of enhancing the
    retention of information at the stage of processing point cloud data. However,
    the speed of the model process is still very slow because of the sparsity of those
    voxels and 3D convolutional computations. Following Zhou’s work, Yan et al. did
    research on how to increase the speed of training and prediction and proposed
    an improved sparse convolution method [[48](#bib.bib48)]. What’s more, to handle
    the question that it is ambiguous to solve the sine function between 0 and $\pi$,
    they proposed a sine error angle loss. Another attempt has been made by Sindagi
    et al. about a small change on VoxelNet is fusing image and point cloud features
    at a different stage [[49](#bib.bib49)]. Specifically, PointFusion and VoxelFusion
    are two unique techniques mentioned in [[49](#bib.bib49)]. The former works in
    the early stage where point cloud data can be projected to the image plane. The
    latter is used to project those 3D voxels to the image. Compared to VoxelNet,
    this network with new techniques has the ability to exploit multi-modal information
    and can decrease the false positives and negatives.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/73475ad4207c3464cea592da5a064dcd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: VoxelNet directly works on the raw point cloud without feature engineering
    and predicts the 3D detection results using a single end-to-end trainable network.'
  prefs: []
  type: TYPE_NORMAL
- en: Considering the situation that although point clouds contain much spatial information,
    it is inevitable to lose some in the process of downscaled feature maps with existing
    one-shot methods. He et al. proposed a SA-SSD network exploiting ﬁne-grained architecture
    information to improve localization accuracy [[50](#bib.bib50)]. What makes it
    speciﬁcal is that they ﬁrst transfer point cloud data to a tensor and put them
    to a backbone model for the purpose of extracting multiple stages features. What’s
    more, they designed a supplementary network in the possession of point-level supervision,
    which can instruct those features to learn the structure of point clouds. It is
    surprising that their experimental results demonstrate that SA-SSD ranks the ﬁrst
    on the KITTI BEV detection benchmark on the Car class.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/05bfa393a17a02efbde8d535eaca0de4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Different object detection processes for two-stage and one-stage
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Point-based Methods. These methods don’t transform the format of raw input,
    directly feeding the point cloud to the network. Yang et al. did a pioneering
    work proposing a network named 3DSSD [[51](#bib.bib51)]. This is the ﬁrst lightweight
    and efﬁcient point cloud-based 3D one-stage object detector. In this new model,
    Distance-FPS (D-FPS) and Feature-FPS (F-FPS) combined together are their proposed
    strategies for fusion sampling. In addition, they generated a detailed bounding
    box prediction network to get the best use of the representative points, exploiting
    a candidate generation layer (CG), an anchor-free regression head and a 3D centerness
    assignment strategy. At last, those experimental results demonstrate that 3DSSD
    outperforms the point-based method PointRCNN at a speed of 25 fps.
  prefs: []
  type: TYPE_NORMAL
- en: Other Methods. Also there are some other single-stage object detection methods
    which haven’t been divided into any types of method in this review. Meyer et al.
    proposed LaserNet, which is an efﬁcient probabilistic 3D object detection model
    [[52](#bib.bib52)]. It is deserving to know that they use a small and dense range
    view data as input rather than Bird’s Eye View data. So their model is more efﬁcient.
    Further, to great knowledge, this is the ﬁrst method to obtain the uncertainty
    of detection in the manner of modelling the distribution of bounding box corners.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to two-stage methods, one-stage methods may don’t achieve such high
    detection accuracy. But one-stage methods have the ability to detect objects faster,
    which are suitable for real-time detection.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '3D object detection is an enormous help for computers to understand scenes
    and is the key technique for a number of real-world applications such as autonomous
    driving. In this review, we list some typical state-of-the-art 3D object detection
    methods and categorize them into two-stage methods and one-stage methods. The
    former one needs to generate a series of proposals at ﬁrst and then predict of
    regress those extracted features. The one-stage method skips the procedure of
    proposal generation, directly predicting class probabilities and regressing bounding
    boxes. For the purpose of directly understanding how those two types of methods
    achieve object detection, Figure [8](#S3.F8 "Figure 8 ‣ 3.2 One-Stage Methods
    ‣ 3 Methods ‣ Deep learning for 3D Object Detection and Tracking in Autonomous
    Driving: A Brief Survey") gives a simple description. We also state the advantages
    of point cloud data for object detection and list several acknowledged drawbacks
    of point clouds. At present, current popular methods try to introduce different
    types of input such as LIDAR points and camera data. The images provide us with
    more dense information but with the loss of 3D spatial information. LiDAR point
    cloud is suitable for 3D object detection with its geometric position information
    and depth information. The sparsity and the irregularity of point clouds urge
    people to investigate novel methods to leverage the advantages of both images
    and LiDAR-based data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the analysis of those various kinds of existing methods, the following
    problems require further research:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First of all, owing to the regular representation of data, those mature 2D image
    process networks can be applied to projection-based techniques and discretization-based
    techniques greatly. Nevertheless, it is inevitable to lose some useful information
    during the process of projecting 3D data into 2D format, which is a great limitation
    for projection-based methods. For discretization-based methods the exponentially
    increasing computation and huge memory costs caused by the increase of the resolution
    maintain the major bottleneck. Taking the above problems into consideration, building
    a sparse convolutional layer based on indexing architectures may be a feasible
    solution to those questions and it deserves further research.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At present, point cloud-based models are popular methods that people pay the
    most attention to. However, point representation usually lacks clear neighbouring
    information because of the sparsity and irregularity of point clouds. A great
    many existing point cloud-based methods use expensive nearest neighbour searching
    techniques such as KNN applied in [[53](#bib.bib53)]. The weak efﬁciency of these
    methods calls for more efﬁcient methods. A recent point-voxel combined representation
    method [[54](#bib.bib54)] can be a possible direction for further study.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the existing 3D point cloud object detection methods work on small scale
    of point clouds. However, those point cloud data obtained by LiDARs are extremely
    immense and large-scale because the process of data acquisition is continuous.
    As a result, there is a real thirst for further investigation to solve the problem
    of those large-scale point clouds.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A large number of researchers [[55](#bib.bib55), [56](#bib.bib56)] have begun
    to learn spatio-temporal information from dynamic point clouds. The spatio-temporal
    information is expected to help improve the performance of many later assignments
    such as 3D object segmentation, object recognition and completion.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] G. Lan, Y. Wu, F. Hu, and Q. Hao, “Vision-based human pose estimation via
    deep learning: A survey,” *IEEE Transactions on Human-Machine Systems*, vol. 53,
    no. 1, pp. 253–268, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] G. Lan, L. De Vries, and S. Wang, “Evolving efficient deep neural networks
    for real-time object recognition,” in *2019 IEEE Symposium Series on Computational
    Intelligence (SSCI)*.   IEEE, 2019, pp. 2571–2578.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,”
    in *2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
    (CVPR 2005), 20-26 June 2005, San Diego, CA, USA*.   IEEE Computer Society, 2005,
    pp. 886–893.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] P. F. Felzenszwalb, R. B. Girshick, D. A. McAllester, and D. Ramanan, “Object
    detection with discriminatively trained part-based models,” *IEEE Trans. Pattern
    Anal. Mach. Intell.*, vol. 32, no. 9, pp. 1627–1645, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] X. Wu, D. Sahoo, and S. C. H. Hoi, “Recent advances in deep learning for
    object detection,” *CoRR*, vol. abs/1908.03673, 2019\. [Online]. Available: [http://arxiv.org/abs/1908.03673](http://arxiv.org/abs/1908.03673)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] G. Lan, Z. Gao, L. Tong, and T. Liu, “Class binarization to neuroevolution
    for multiclass classification,” *Neural Computing and Applications*, vol. 34,
    no. 22, pp. 19 845–19 862, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] G. Lan, J. M. Tomczak, D. M. Roijers, and A. Eiben, “Time efficiency in
    optimization with a bayesian-evolutionary algorithm,” *Swarm and Evolutionary
    Computation*, vol. 69, p. 100970, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in Neural Information Processing
    Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012\.
    Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States*,
    P. L. Bartlett, F. C. N. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger,
    Eds., 2012, pp. 1106–1114.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
    for accurate object detection and semantic segmentation,” in *2014 IEEE Conference
    on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June
    23-28, 2014*.   IEEE Computer Society, 2014, pp. 580–587.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] R. B. Girshick, “Fast R-CNN,” in *2015 IEEE International Conference on
    Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015*.   IEEE Computer
    Society, 2015, pp. 1440–1448.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] K. He, G. Gkioxari, P. Dollár, and R. B. Girshick, “Mask R-CNN,” in *IEEE
    International Conference on Computer Vision, ICCV 2017, Venice, Italy, October
    22-29, 2017*.   IEEE Computer Society, 2017, pp. 2980–2988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] S. Ren, K. He, R. B. Girshick, and J. Sun, “Faster R-CNN: towards real-time
    object detection with region proposal networks,” in *Advances in Neural Information
    Processing Systems 28: Annual Conference on Neural Information Processing Systems
    2015, December 7-12, 2015, Montreal, Quebec, Canada*, C. Cortes, N. D. Lawrence,
    D. D. Lee, M. Sugiyama, and R. Garnett, Eds., 2015, pp. 91–99.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi, “You only look
    once: Unified, real-time object detection,” in *2016 IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016*.   IEEE
    Computer Society, 2016, pp. 779–788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] J. Redmon and A. Farhadi, “YOLO9000: better, faster, stronger,” in *2017
    IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu,
    HI, USA, July 21-26, 2017*.   IEEE Computer Society, 2017, pp. 6517–6525.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] G. Lan, J. Benito-Picazo, D. M. Roijers, E. Domínguez, and A. Eiben, “Real-time
    robot vision on low-performance computing hardware,” in *2018 15th international
    conference on control, automation, robotics and vision (ICARCV)*.   IEEE, 2018,
    pp. 1959–1965.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] H. Xu, G. Lan, S. Wu, and Q. Hao, “Online intelligent calibration of cameras
    and lidars for autonomous driving systems,” in *2019 IEEE Intelligent Transportation
    Systems Conference (ITSC)*.   IEEE, 2019, pp. 3913–3920.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] S. Song and J. Xiao, “Deep sliding shapes for amodal 3D object detection
    in RGB-D images,” in *2016 IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016*.   IEEE Computer Society, 2016,
    pp. 808–816.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3D object detection
    network for autonomous driving,” in *2017 IEEE Conference on Computer Vision and
    Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017*.   IEEE Computer
    Society, 2017, pp. 6526–6534.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. L. Waslander, “Joint 3D
    proposal generation and object detection from view aggregation,” in *2018 IEEE/RSJ
    International Conference on Intelligent Robots and Systems, IROS 2018, Madrid,
    Spain, October 1-5, 2018*.   IEEE, 2018, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] G. Lan, M. van Hooft, M. De Carlo, and J. M. Tomczak, “Learning locomotion
    skills in evolvable robots,” *Neurocomputing*, vol. 452, pp. 294–306, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] G. Lan, M. De Carlo, F. van Diggelen, J. M. Tomczak, and D. M. Roijers,
    “Learning directed locomotion in modular robots with evolvable morphologies,”
    *Applied Soft Computing*, vol. 111, p. 107688, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun, “Deep learning
    for 3D point clouds: A survey,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 43,
    no. 12, pp. 4338–4364, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] B. Li, T. Zhang, and T. Xia, “Vehicle detection from 3D lidar using fully
    convolutional network,” in *Robotics: Science and Systems XII, University of Michigan,
    Ann Arbor, Michigan, USA, June 18 - June 22, 2016*, D. Hsu, N. M. Amato, S. Berman,
    and S. A. Jacobs, Eds., 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the KITTI vision benchmark suite,” in *2012 IEEE Conference on Computer Vision
    and Pattern Recognition, Providence, RI, USA, June 16-21, 2012*.   IEEE Computer
    Society, 2012, pp. 3354–3361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] M. Liang, B. Yang, S. Wang, and R. Urtasun, “Deep continuous fusion for
    multi-sensor 3D object detection,” in *Computer Vision - ECCV 2018 - 15th European
    Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XVI*, ser.
    Lecture Notes in Computer Science, V. Ferrari, M. Hebert, C. Sminchisescu, and
    Y. Weiss, Eds., vol. 11220.   Springer, 2018, pp. 663–678.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] M. Liang, B. Yang, Y. Chen, R. Hu, and R. Urtasun, “Multi-task multi-sensor
    fusion for 3D object detection,” in *IEEE Conference on Computer Vision and Pattern
    Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019*.   Computer Vision
    Foundation / IEEE, 2019, pp. 7345–7353.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] H. Lu, X. Chen, G. Zhang, Q. Zhou, Y. Ma, and Y. Zhao, “Scanet: Spatial-channel
    attention network for 3D object detection,” in *IEEE International Conference
    on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom,
    May 12-17, 2019*.   IEEE, 2019, pp. 1992–1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] G. Lan, T. Liu, X. Wang, X. Pan, and Z. Huang, “A semantic web technology
    index,” *Scientific reports*, vol. 12, no. 1, p. 3672, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Z. Yang, Y. Sun, S. Liu, X. Shen, and J. Jia, “IPOD: intensive point-based
    object detector for point cloud,” *CoRR*, vol. abs/1812.05276, 2018. [Online].
    Available: [http://arxiv.org/abs/1812.05276](http://arxiv.org/abs/1812.05276)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] S. Shi, X. Wang, and H. Li, “Pointrcnn: 3D object proposal generation
    and detection from point cloud,” in *IEEE Conference on Computer Vision and Pattern
    Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019*.   Computer Vision
    Foundation / IEEE, 2019, pp. 770–779.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Zarzar, S. Giancola, and B. Ghanem, “Pointrgcn: Graph convolution networks
    for 3D vehicles detection refinement,” *CoRR*, vol. abs/1911.12236, 2019\. [Online].
    Available: [http://arxiv.org/abs/1911.12236](http://arxiv.org/abs/1911.12236)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] S. Vora, A. H. Lang, B. Helou, and O. Beijbom, “Pointpainting: Sequential
    fusion for 3D object detection,” in *2020 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020*.   Computer
    Vision Foundation / IEEE, 2020, pp. 4603–4611.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets for
    3D object detection from RGB-D data,” in *2018 IEEE Conference on Computer Vision
    and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*.   Computer
    Vision Foundation / IEEE Computer Society, 2018, pp. 918–927.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] X. Zhao, Z. Liu, R. Hu, and K. Huang, “3D object detection using scale
    invariant and feature reweighting networks,” in *The Thirty-Third AAAI Conference
    on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications
    of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on
    Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii,
    USA, January 27 - February 1, 2019*.   AAAI Press, 2019, pp. 9267–9274.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] M. Jiang, Y. Wu, and C. Lu, “Pointsift: A sift-like network module for
    3D point cloud semantic segmentation,” *CoRR*, vol. abs/1807.00652, 2018. [Online].
    Available: [http://arxiv.org/abs/1807.00652](http://arxiv.org/abs/1807.00652)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S. Song, S. P. Lichtenberg, and J. Xiao, “SUN RGB-D: A RGB-D scene understanding
    benchmark suite,” in *IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2015, Boston, MA, USA, June 7-12, 2015*.   IEEE Computer Society, 2015, pp.
    567–576.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] D. Xu, D. Anguelov, and A. Jain, “Pointfusion: Deep sensor fusion for
    3D bounding box estimation,” in *2018 IEEE Conference on Computer Vision and Pattern
    Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*.   Computer
    Vision Foundation / IEEE Computer Society, 2018, pp. 244–253.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *2016 IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016*.   IEEE Computer Society, 2016,
    pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3D classification and segmentation,” in *2017 IEEE Conference on
    Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26,
    2017*.   IEEE Computer Society, 2017, pp. 77–85.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Z. Wang and K. Jia, “Frustum convnet: Sliding frustums to aggregate local
    point-wise features for amodal 3D object detection,” *CoRR*, vol. abs/1903.01864,
    2019\. [Online]. Available: [http://arxiv.org/abs/1903.01864](http://arxiv.org/abs/1903.01864)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] B. Yang, W. Luo, and R. Urtasun, “PIXOR: real-time 3D object detection
    from point clouds,” in *2018 IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*.   Computer Vision Foundation
    / IEEE Computer Society, 2018, pp. 7652–7660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] B. Yang, M. Liang, and R. Urtasun, “HDNET: exploiting HD maps for 3D object
    detection,” in *2nd Annual Conference on Robot Learning, CoRL 2018, Zürich, Switzerland,
    29-31 October 2018, Proceedings*, ser. Proceedings of Machine Learning Research,
    vol. 87.   PMLR, 2018, pp. 146–155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] J. Beltrán, C. Guindel, F. M. Moreno, D. Cruzado, F. García, and A. de la
    Escalera, “Birdnet: A 3D object detection framework from lidar information,” in
    *21st International Conference on Intelligent Transportation Systems, ITSC 2018,
    Maui, HI, USA, November 4-7, 2018*, W. Zhang, A. M. Bayen, J. J. S. Medina, and
    M. J. Barth, Eds.   IEEE, 2018, pp. 3517–3523.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] B. Li, “3D fully convolutional network for vehicle detection in point
    cloud,” in *2017 IEEE/RSJ International Conference on Intelligent Robots and Systems,
    IROS 2017, Vancouver, BC, Canada, September 24-28, 2017*.   IEEE, 2017, pp. 1513–1518.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong, and I. Posner, “Vote3Deep:
    Fast object detection in 3D point clouds using efficient convolutional neural
    networks,” in *2017 IEEE International Conference on Robotics and Automation,
    ICRA 2017, Singapore, Singapore, May 29 - June 3, 2017*.   IEEE, 2017, pp. 1355–1361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] X. Li, J. Guivant, N. Kwok, Y. Xu, R. Li, and H. Wu, “Three-dimensional
    backbone network for 3D object detection in traffic scenes,” *arXiv e-prints*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Y. Zhou and O. Tuzel, “Voxelnet: End-to-end learning for point cloud based
    3D object detection,” in *2018 IEEE Conference on Computer Vision and Pattern
    Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*.   Computer
    Vision Foundation / IEEE Computer Society, 2018, pp. 4490–4499.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Y. Yan, Y. Mao, and B. Li, “SECOND: sparsely embedded convolutional detection,”
    *Sensors*, vol. 18, no. 10, p. 3337, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] V. A. Sindagi, Y. Zhou, and O. Tuzel, “MVX-Net: Multimodal VoxelNet for
    3D Object Detection,” *arXiv e-prints*, p. arXiv:1904.01649, Apr. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] C. He, H. Zeng, J. Huang, X. Hua, and L. Zhang, “Structure aware single-stage
    3D object detection from point cloud,” in *2020 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020*.   Computer
    Vision Foundation / IEEE, 2020, pp. 11 870–11 879.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Z. Yang, Y. Sun, S. Liu, and J. Jia, “3DSSD: point-based 3D single stage
    object detector,” in *2020 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020*.   Computer Vision
    Foundation / IEEE, 2020, pp. 11 037–11 045.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] G. P. Meyer, A. Laddha, E. Kee, C. Vallespi-Gonzalez, and C. K. Wellington,
    “Lasernet: An efficient probabilistic 3D object detector for autonomous driving,”
    in *IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long
    Beach, CA, USA, June 16-20, 2019*.   Computer Vision Foundation / IEEE, 2019,
    pp. 12 677–12 686.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, “Pointcnn: Convolution
    on x-transformed points,” in *Advances in Neural Information Processing Systems
    31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
    December 3-8, 2018, Montréal, Canada*, S. Bengio, H. M. Wallach, H. Larochelle,
    K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., 2018, pp. 828–838.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Z. Liu, H. Tang, Y. Lin, and S. Han, “Point-voxel CNN for efficient 3D
    deep learning,” in *Advances in Neural Information Processing Systems 32: Annual
    Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December
    8-14, 2019, Vancouver, BC, Canada*, H. M. Wallach, H. Larochelle, A. Beygelzimer,
    F. d’Alché-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 963–973.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] H. Fan and Y. Yang, “Pointrnn: Point recurrent neural network for moving
    point cloud processing,” *CoRR*, vol. abs/1910.08287, 2019\. [Online]. Available:
    [http://arxiv.org/abs/1910.08287](http://arxiv.org/abs/1910.08287)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] X. Liu, M. Yan, and J. Bohg, “Meteornet: Deep learning on dynamic 3D point
    cloud sequences,” in *2019 IEEE/CVF International Conference on Computer Vision,
    ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019*.   IEEE, 2019,
    pp. 9245–9254.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
