- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:05:08'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 20:05:08'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1909.03354] Deep Weakly-Supervised Learning Methods for Classification and
    Localization in Histology Images: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1909.03354] 深度弱监督学习方法在组织学图像分类与定位中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1909.03354](https://ar5iv.labs.arxiv.org/html/1909.03354)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1909.03354](https://ar5iv.labs.arxiv.org/html/1909.03354)
- en: 'Deep Weakly-Supervised Learning Methods for Classification and Localization
    in Histology Images: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度弱监督学习方法在组织学图像分类与定位中的应用：综述
- en: \nameJérôme Rony \emailjerome.rony.1@etsmtl.net
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \name热罗姆·罗尼 \emailjerome.rony.1@etsmtl.net
- en: \addrLIVIA, Dept. of Systems Engineering, École de technologie supérieure, Montreal,
    Canada \AND\nameSoufiane Belharbi \emailsoufiane.belharbi.1@ens.etsmtl.ca
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \addrLIVIA, 系统工程系，蒙特利尔高等技术学院，加拿大 \AND\name苏菲安·贝尔哈比 \emailsoufiane.belharbi.1@ens.etsmtl.ca
- en: \addrLIVIA, Dept. of Systems Engineering, École de technologie supérieure, Montreal,
    Canada \AND\nameJose Dolz \emailjose.dolz@etsmtl.ca
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \addrLIVIA, 系统工程系，蒙特利尔高等技术学院，加拿大 \AND\name何塞·多尔兹 \emailjose.dolz@etsmtl.ca
- en: \addrLIVIA, Dept. of Software and IT Engineering, École de technologie supérieure,
    Montreal, Canada \AND\nameIsmail Ben Ayed \emailismail.benayed@etsmtl.ca
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \addrLIVIA, 软件与信息技术工程系，蒙特利尔高等技术学院，加拿大 \AND\name伊斯梅尔·本·阿耶德 \emailismail.benayed@etsmtl.ca
- en: \addrLIVIA, Dept. of Systems Engineering, École de technologie supérieure, Montreal,
    Canada \AND\nameLuke McCaffrey \emailluke.mccaffrey@mcgill.ca
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \addrLIVIA, 系统工程系，蒙特利尔高等技术学院，加拿大 \AND\name卢克·麦卡弗雷 \emailluke.mccaffrey@mcgill.ca
- en: \addrGoodman Cancer Research Centre, Dept. of Oncology, McGill University, Montreal,
    Canada \AND\nameEric Granger \emaileric.granger@etsmtl.ca
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: \addr古德曼癌症研究中心，肿瘤学系，麦吉尔大学，蒙特利尔，加拿大 \AND\name埃里克·格兰杰 \emaileric.granger@etsmtl.ca
- en: \addrLIVIA, Dept. of Systems Engineering, École de technologie supérieure, Montreal,
    Canada
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \addrLIVIA, 系统工程系，蒙特利尔高等技术学院，加拿大
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Using state-of-the-art deep learning (DL) models to diagnose cancer from histology
    data presents several challenges related to the nature and availability of labeled
    histology images, including image size, stain variations, and label ambiguity.
    In addition, cancer grading and the localization of regions of interest (ROIs)
    in such images normally rely on both image- and pixel-level labels, with the latter
    requiring a costly annotation process. Deep weakly-supervised object localization
    (WSOL) methods provide different strategies for low-cost training of DL models.
    Given only image-class annotations, these methods can be trained to simultaneously
    classify an image, and yield class activation maps (CAMs) for ROI localization.
    This paper provides a review of deep WSOL methods to identify and locate diseases
    in histology images, without the need for pixel-level annotations. We propose
    a taxonomy in which these methods are divided into bottom-up and top-down methods
    according to the information flow in models. Although the latter have seen only
    limited progress, recent bottom-up methods are currently driving a lot of progress
    with the use of deep WSOL methods. Early works focused on designing different
    spatial pooling functions. However, those methods quickly peaked in term of localization
    accuracy and revealed a major limitation, namely, – the under-activation of CAMs,
    which leads to high false negative localization. Subsequent works aimed to alleviate
    this shortcoming and recover the complete object from the background, using different
    techniques such as perturbation, self-attention, shallow features, pseudo-annotation,
    and task decoupling. In the present paper, representative deep WSOL methods from
    our taxonomy are also evaluated and compared in terms of classification and localization
    accuracy using two challenging public histology datasets – one for colon cancer
    (GlaS), and a second, for breast cancer (CAMELYON16). Overall, the results indicate
    poor localization performance, particularly for generic methods that were initially
    designed to process natural images. Methods designed to address the challenges
    posed by histology data often use priors such as ROI size, or additional pixel-wise
    supervision estimated from a pre-trained classifier, allowing them to achieve
    better results. However, all the methods suffer from high false positive/negative
    localization. Classification performance is mainly affected by the model selection
    process, which uses either the classification or the localization metric. Finally,
    four key challenges are identified in the application of deep WSOL methods in
    histology, namely, – under-/over-activation of CAMs, sensitivity to thresholding,
    and model selection – and research avenues are provided to mitigate them. Our
    code is publicly available at [https://github.com/jeromerony/survey_wsl_histology](https://github.com/jeromerony/survey_wsl_histology).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最先进的深度学习（DL）模型从组织学数据中诊断癌症面临诸多挑战，这些挑战与标记组织学图像的性质和可用性有关，包括图像大小、染色差异和标签模糊性。此外，癌症分级和在这些图像中感兴趣区域（ROIs）的定位通常依赖于图像级和像素级标签，后者需要昂贵的标注过程。深度弱监督对象定位（WSOL）方法提供了低成本训练DL模型的不同策略。仅根据图像分类注释，这些方法可以同时进行图像分类，并生成用于ROI定位的类别激活图（CAMs）。本文回顾了深度WSOL方法，以识别和定位组织学图像中的疾病，而无需像素级注释。我们提出了一个分类法，其中这些方法根据模型中的信息流分为自下而上的方法和自上而下的方法。尽管后者仅取得了有限的进展，但最近的自下而上的方法正在推动深度WSOL方法的许多进展。早期的工作集中在设计不同的空间池化函数。然而，这些方法在定位准确度上很快达到了顶峰，并揭示了一个主要的限制，即–
    CAMs的激活不足，这导致了高假阴性定位。随后的工作旨在缓解这一不足，并使用不同的技术如扰动、自注意力、浅层特征、伪注释和任务解耦，从背景中恢复完整的对象。本文还评估和比较了我们分类法中的代表性深度WSOL方法，在分类和定位准确度方面使用了两个具有挑战性的公共组织学数据集–一个用于结肠癌（GlaS），另一个用于乳腺癌（CAMELYON16）。总体而言，结果表明定位性能较差，特别是对于最初设计用于处理自然图像的通用方法。旨在解决组织学数据挑战的方法通常使用先验信息，如ROI大小，或从预训练分类器估计的附加像素级监督，从而取得更好的结果。然而，所有方法都存在高假阳性/假阴性定位的问题。分类性能主要受到模型选择过程的影响，该过程使用分类或定位度量。最后，识别了应用深度WSOL方法于组织学中的四个关键挑战，即–
    CAMs的激活不足/过度激活、对阈值的敏感性和模型选择，并提供了缓解这些挑战的研究方向。我们的代码公开可用，[https://github.com/jeromerony/survey_wsl_histology](https://github.com/jeromerony/survey_wsl_histology)。
- en: 'Keywords: Medical/Histology Image Analysis, Computer-Aided Diagnosis, Deep
    Learning, Weakly Supervised Object Localization, Weakly Supervised Learning, Image
    Classification.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词：医学/组织学图像分析，计算机辅助诊断，深度学习，弱监督物体定位，弱监督学习，图像分类。
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The advent of Whole Slide Imaging (WSI) scanners opened new possibilities in
    pathology image analysis (He et al., [2012](#bib.bib65); Madabhushi, [2009](#bib.bib93)).
    Histology slides provide more comprehensive views of diseases and of their effect
    on tissue (Hipp et al., [2011](#bib.bib66)), since their preparation preserves
    the underlying tissue structure (He et al., [2012](#bib.bib65)). For instance,
    some disease characteristics (\eg, lymphatic infiltration of cancer) may be predicted
    using only histology images (Gurcan et al., [2009](#bib.bib60)). Histology images
    analysis remains the gold standard in diagnosing several diseases, including most
    types of cancer (Gurcan et al., [2009](#bib.bib60); He et al., [2012](#bib.bib65);
    Veta et al., [2014](#bib.bib148)). Breast cancer, which is the most prevalent
    cancer in women worldwide, relies on medical imaging systems as a primary diagnostic
    tool for its early detection (Daisuke and Shumpei, [2018](#bib.bib34); Veta et al.,
    [2014](#bib.bib148); Xie et al., [2019](#bib.bib157)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 整张幻灯片成像（WSI）扫描仪的出现为病理图像分析开辟了新的可能性（He 等，[2012](#bib.bib65)；Madabhushi，[2009](#bib.bib93)）。组织学切片提供了对疾病及其对组织影响的更全面视角（Hipp
    等，[2011](#bib.bib66)），因为其制备过程保持了底层组织结构（He 等，[2012](#bib.bib65)）。例如，一些疾病特征（例如癌症的淋巴浸润）可能仅通过组织学图像进行预测（Gurcan
    等，[2009](#bib.bib60)）。组织学图像分析仍然是诊断多种疾病的黄金标准，包括大多数类型的癌症（Gurcan 等，[2009](#bib.bib60)；He
    等，[2012](#bib.bib65)；Veta 等，[2014](#bib.bib148)）。乳腺癌是全球女性中最常见的癌症，它依赖医学成像系统作为早期检测的主要诊断工具（Daisuke
    和 Shumpei，[2018](#bib.bib34)；Veta 等，[2014](#bib.bib148)；Xie 等，[2019](#bib.bib157)）。
- en: Cancer is mainly diagnosed by pathologists who analyze WSIs to identify and
    assess epithelial cells organized into ducts, lobules, or malignant clusters,
    and embedded within a heterogeneous stroma. Manual analysis of histology tissues
    depends heavily on the expertise and experience of histopathologists. Such manual
    interpretation is time-consuming and difficult to grade in a reproducible manner.
    Analyzing WSIs from digitized histology slides enables facilitated, and potentially
    automated, Computer-Aided Diagnosis in pathology, where the main goal is to confirm
    the presence or absence of disease and to grade or measure disease progression.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 癌症主要由病理学家诊断，他们分析WSI以识别和评估组织中的上皮细胞，这些细胞被组织在管腔、小叶或恶性簇中，并嵌入在异质基质中。组织学切片的人工分析高度依赖于组织病理学家的专业知识和经验。这样的人工解释既费时又难以以可重复的方式进行评分。分析来自数字化组织学切片的WSI使得在病理学中实现便捷且可能自动化的计算机辅助诊断成为可能，其主要目标是确认疾病的存在或缺失，并对疾病进展进行分级或测量。
- en: Given the large number of digitized exams in use, automated systems have become
    a part of the clinical routines for breast cancer detection (Tang et al., [2009](#bib.bib141)).
    Automated analysis of the spatial structures in histology images can be traced
    back to early works (Bartels et al., [1992](#bib.bib6); Hamilton et al., [1994](#bib.bib62);
    Weind et al., [1998](#bib.bib156)). Various image processing and machine learning
    (ML) techniques have been investigated in a bid to identify discriminative structures
    and classify histology images (He et al., [2012](#bib.bib65)); these include thresholding (Gurcan
    et al., [2006](#bib.bib59); Petushi et al., [2006](#bib.bib110)), active contours (Bamford
    and Lovell, [2001](#bib.bib5)), Bayesian classifiers (Naik et al., [2007](#bib.bib102)),
    graphs used to model spatial structures (Bilgin et al., [2007](#bib.bib17); Tabesh
    et al., [2007](#bib.bib140)), and ensemble methods based on Support Vector Machines
    and Adaboost (Doyle et al., [2006](#bib.bib43); Qureshi et al., [2008](#bib.bib113)).
    An overview of these techniques and their applications is provided in (Gurcan
    et al., [2009](#bib.bib60); He et al., [2012](#bib.bib65); Veta et al., [2014](#bib.bib148)).
    Recently, deep learning (DL) models have attracted a lot of attention in histology
    image analysis (Belharbi et al., [2021](#bib.bib12), [2022a](#bib.bib13), [2019](#bib.bib11),
    [2022b](#bib.bib14); Courtiol et al., [2018](#bib.bib30); Dimitriou et al., [2019](#bib.bib40);
    Iizuka et al., [2020](#bib.bib68); Janowczyk and Madabhushi, [2016](#bib.bib72);
    Li and Ping, [2018](#bib.bib88); Srinidhi et al., [2019](#bib.bib133)). In the
    present paper, we continue in the same vein and focus on the application of DL
    models in histology image analysis.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于大量数字化考试的使用，自动化系统已成为乳腺癌检测的临床常规的一部分（Tang et al., [2009](#bib.bib141)）。对组织学图像空间结构的自动分析可以追溯到早期的研究（Bartels
    et al., [1992](#bib.bib6); Hamilton et al., [1994](#bib.bib62); Weind et al.,
    [1998](#bib.bib156)）。各种图像处理和机器学习（ML）技术已被研究以识别区分性结构并对组织学图像进行分类（He et al., [2012](#bib.bib65)）；这些技术包括阈值处理（Gurcan
    et al., [2006](#bib.bib59); Petushi et al., [2006](#bib.bib110)）、主动轮廓（Bamford
    and Lovell, [2001](#bib.bib5)）、贝叶斯分类器（Naik et al., [2007](#bib.bib102)）、用于建模空间结构的图（Bilgin
    et al., [2007](#bib.bib17); Tabesh et al., [2007](#bib.bib140)）以及基于支持向量机和Adaboost的集成方法（Doyle
    et al., [2006](#bib.bib43); Qureshi et al., [2008](#bib.bib113)）。这些技术及其应用的概述可以在（Gurcan
    et al., [2009](#bib.bib60); He et al., [2012](#bib.bib65); Veta et al., [2014](#bib.bib148)）中找到。最近，深度学习（DL）模型在组织学图像分析中引起了广泛关注（Belharbi
    et al., [2021](#bib.bib12), [2022a](#bib.bib13), [2019](#bib.bib11), [2022b](#bib.bib14);
    Courtiol et al., [2018](#bib.bib30); Dimitriou et al., [2019](#bib.bib40); Iizuka
    et al., [2020](#bib.bib68); Janowczyk and Madabhushi, [2016](#bib.bib72); Li and
    Ping, [2018](#bib.bib88); Srinidhi et al., [2019](#bib.bib133)）。在本文中，我们继续这一方向，重点讨论DL模型在组织学图像分析中的应用。
- en: DL models (Goodfellow et al., [2016](#bib.bib58)), and convolutional neural
    networks (CNNs) in particular, provide state-of-the-art performance in many visual
    recognition applications such as image classification (Krizhevsky et al., [2012](#bib.bib85)),
    object detection (Redmon et al., [2016](#bib.bib115)), and segmentation (Dolz
    et al., [2018](#bib.bib41)). These supervised learning architectures are trained
    end-to-end with large amounts of annotated data. More recently, the potential
    of DL models has begun to be explored in assisted pathology diagnosis (Daisuke
    and Shumpei, [2018](#bib.bib34); Janowczyk and Madabhushi, [2016](#bib.bib72);
    Li and Ping, [2018](#bib.bib88)). Given the growing availability of histology
    slides, DL models have not only been proposed for disease prediction (Hou et al.,
    [2016](#bib.bib67); Li and Ping, [2018](#bib.bib88); Sheikhzadeh et al., [2016](#bib.bib126);
    Spanhol et al., [2016a](#bib.bib131); Xu et al., [2016](#bib.bib158)), but also
    for related tasks such as the detection and segmentation of tumor regions within
    WSI (Kieffer et al., [2017](#bib.bib80); Mungle et al., [2017](#bib.bib96)), scoring
    of immunostaining (Sheikhzadeh et al., [2016](#bib.bib126); Wang et al., [2015](#bib.bib150)),
    cancer staging (Shah et al., [2017](#bib.bib125); Spanhol et al., [2016a](#bib.bib131)),
    mitosis detection (Chen et al., [2016](#bib.bib23); Cireşan et al., [2013](#bib.bib29);
    Roux et al., [2013](#bib.bib119)), gland segmentation (Caie et al., [2014](#bib.bib19);
    Gertych et al., [2015](#bib.bib54); Sirinukunwattana et al., [2017](#bib.bib129)),
    and detection and quantification of vascular invasion (Caicedo et al., [2011](#bib.bib18)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型（Goodfellow 等，[2016](#bib.bib58)），特别是卷积神经网络（CNNs），在许多视觉识别应用中提供了最先进的性能，例如图像分类（Krizhevsky
    等，[2012](#bib.bib85)），目标检测（Redmon 等，[2016](#bib.bib115)）和分割（Dolz 等，[2018](#bib.bib41)）。这些监督学习架构通过大量标注数据进行端到端的训练。最近，深度学习模型在辅助病理诊断中的潜力开始得到探索（Daisuke
    和 Shumpei，[2018](#bib.bib34)；Janowczyk 和 Madabhushi，[2016](#bib.bib72)；Li 和 Ping，[2018](#bib.bib88)）。鉴于组织学切片的日益普及，深度学习模型不仅被提议用于疾病预测（Hou
    等，[2016](#bib.bib67)；Li 和 Ping，[2018](#bib.bib88)；Sheikhzadeh 等，[2016](#bib.bib126)；Spanhol
    等，[2016a](#bib.bib131)；Xu 等，[2016](#bib.bib158)），还用于相关任务，如 WSIs 中肿瘤区域的检测和分割（Kieffer
    等，[2017](#bib.bib80)；Mungle 等，[2017](#bib.bib96)），免疫染色评分（Sheikhzadeh 等，[2016](#bib.bib126)；Wang
    等，[2015](#bib.bib150)），癌症分期（Shah 等，[2017](#bib.bib125)；Spanhol 等，[2016a](#bib.bib131)），有丝分裂检测（Chen
    等，[2016](#bib.bib23)；Cireşan 等，[2013](#bib.bib29)；Roux 等，[2013](#bib.bib119)），腺体分割（Caie
    等，[2014](#bib.bib19)；Gertych 等，[2015](#bib.bib54)；Sirinukunwattana 等，[2017](#bib.bib129)），以及血管侵袭的检测和量化（Caicedo
    等，[2011](#bib.bib18)）。
- en: '![Refer to caption](img/0fd2b02287e3aa47d74f31245754d310.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0fd2b02287e3aa47d74f31245754d310.png)'
- en: (a)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: （a）
- en: '![Refer to caption](img/bf4024278e6892c6d2fe92b9d8f5f549.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bf4024278e6892c6d2fe92b9d8f5f549.png)'
- en: (b)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: （b）
- en: 'Figure 1: Segmentation of two WSIs from the ICIAR 2018 BACH Challenge. Colors
    represent different types of cancerous regions: red for Benign, green for In Situ
    Carcinoma and blue for Invasive Carcinoma. These examples highlight the diversity
    in size and regions (Aresta et al., [2018](#bib.bib3)).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：ICIAR 2018 BACH 挑战中的两个 WSIs 的分割。颜色代表不同类型的癌变区域：红色表示良性，绿色表示原位癌，蓝色表示浸润癌。这些示例突出了大小和区域的多样性（Aresta
    等，[2018](#bib.bib3)）。
- en: '![Refer to caption](img/2b867357f8cc5e39001208aff26d0a56.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2b867357f8cc5e39001208aff26d0a56.png)'
- en: 'Figure 2: Difference in staining for two images both labeled as *In Situ Carcinoma*
    extracted from different WSIs (Aresta et al., [2018](#bib.bib3)).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：从不同的 WSIs 中提取的两个标记为*原位癌*的图像在染色上的差异（Aresta 等，[2018](#bib.bib3)）。
- en: 'Histology images present additional challenges for ML/DL models because of
    their (1) high resolution where, for instance, a single core of prostate biopsy
    tissue digitized at ${40\times}$ magnification is approximately $(15000\times
    15000)$ elements ($\sim 225$ million pixels); (2) heterogeneous nature resulting
    mainly from the variation of the WSI production process; and (3) noisy/ambiguous
    labels (Daisuke and Shumpei, [2018](#bib.bib34)) caused by the annotation process
    that is conducted by assigning the worst stage of cancer to the image. Therefore,
    a WSI that is annotated with a specific grade is also more likely to contain regions
    with lower grades. This leads to imbalanced datasets having fewer images with
    high grades. Noisy/ambiguous labels are an issue for models trained through multi-instance
    learning (Carbonneau et al., [2018](#bib.bib21); Cheplygina et al., [2019](#bib.bib26);
    Wang et al., [2018](#bib.bib152); Zhou, [2004](#bib.bib174)), where the WSI label
    is transferred to sampled image patches and can introduce annotation errors. Such
    label inconsistencies can degrade model performance, and hinder learning (Frenay
    and Verleysen, [2014](#bib.bib51); Sukhbaatar et al., [2014](#bib.bib137); Zhang
    et al., [2017](#bib.bib163)) (see Figs. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Deep Weakly-Supervised Learning Methods for Classification and Localization
    in Histology Images: A Survey") and [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep
    Weakly-Supervised Learning Methods for Classification and Localization in Histology
    Images: A Survey")).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 组织学图像给 ML/DL 模型带来了额外的挑战，因为它们的（1）高分辨率，例如，一个前列腺活检组织的单个核心在 ${40\times}$ 放大倍数下大约包含
    $(15000\times 15000)$ 个元素（约 2.25 亿像素）；（2）异质性主要源于 WSI 生产过程的变化；以及（3）由注释过程造成的噪声/模糊标签（Daisuke
    和 Shumpei，[2018](#bib.bib34)），注释过程中通常会将图像标记为癌症的最严重阶段。因此，被标注为特定等级的 WSI 也更可能包含较低等级的区域。这导致了图像等级较高的图像较少的不平衡数据集。噪声/模糊标签对通过多实例学习训练的模型是个问题（Carbonneau
    等，[2018](#bib.bib21)；Cheplygina 等，[2019](#bib.bib26)；Wang 等，[2018](#bib.bib152)；Zhou，[2004](#bib.bib174)），在这种情况下，WSI
    标签被转移到采样的图像补丁中，可能会引入注释错误。这种标签不一致性可能会降低模型性能，并阻碍学习（Frenay 和 Verleysen，[2014](#bib.bib51)；Sukhbaatar
    等，[2014](#bib.bib137)；Zhang 等，[2017](#bib.bib163)）（参见图 [1](#S1.F1 "图 1 ‣ 1 介绍
    ‣ 组织学图像分类与定位的深度弱监督学习方法综述") 和 [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ 组织学图像分类与定位的深度弱监督学习方法综述")）。
- en: Training accurate DL models to analyze histology images often requires full
    supervision to address key tasks, such as classification, localization, and segmentation
    (Daisuke and Shumpei, [2018](#bib.bib34); Janowczyk and Madabhushi, [2016](#bib.bib72)).
    Learning to accurately localize cancerous regions typically requires a large number
    of images with pixel-wise annotations. Considering the size and complexity of
    such images, dense annotations come at a considerable cost, and require highly
    trained experts. Outsourcing this task to standard workers such as Mechanical
    Turk Workers is not an option. As a result, histology datasets are often composed
    of large images that are coarsely annotated according to the diagnosis. It is
    therefore clear that training powerful DL models to simultaneously predict the
    image class and, localize important image regions linked to a prediction *without*
    dense annotations; is highly desirable for histology image analysis.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 训练准确的 DL 模型以分析组织学图像通常需要全监督，以解决分类、定位和分割等关键任务（Daisuke 和 Shumpei，[2018](#bib.bib34)；Janowczyk
    和 Madabhushi，[2016](#bib.bib72)）。准确学习定位癌变区域通常需要大量带有像素级注释的图像。考虑到这些图像的大小和复杂性，密集注释的成本相当高，并且需要高度培训的专家。将这一任务外包给标准工人，如
    Mechanical Turk Workers，是不现实的。因此，组织学数据集通常由根据诊断粗略注释的大图像组成。因此，显然，训练强大的 DL 模型以在没有密集注释的情况下，同时预测图像类别和定位与预测相关的重要图像区域，对于组织学图像分析是非常理想的。
- en: Despite their intrinsic challenges (Choe et al., [2020](#bib.bib28)), techniques
    for weakly-supervised learning (WSL) (Zhou, [2017](#bib.bib175)) have recently
    emerged to alleviate the need for dense annotation, particularly for computer
    vision applications. These techniques are adapted to different forms of weak supervision,
    including image-tags (image-levels label) (Kim et al., [2017](#bib.bib81); Pathak
    et al., [2015](#bib.bib107); Teh et al., [2016](#bib.bib144); Wei et al., [2017](#bib.bib154)),
    scribbles (Lin et al., [2016](#bib.bib89); Tang et al., [2018](#bib.bib142)),
    points (Bearman et al., [2016](#bib.bib9)), bounding boxes (Dai et al., [2015](#bib.bib33);
    Khoreva et al., [2017](#bib.bib78)), global image statistics, such as the target
    size (Bateson et al., [2019](#bib.bib7); Jia et al., [2017](#bib.bib73); Kervadec
    et al., [2019a](#bib.bib76), [b](#bib.bib77)). The reduced weak supervision requirement
    provides an appealing learning framework. In this paper, we focus on WSL methods
    that allow training a DL model using only image-level annotations for the classification
    of histology images and for the localization of image ROIs linked to class predictions.
    These methods perform the weakly-supervised object localization (WSOL) task, which
    can produce localization under the form of activation maps and bounding boxes (Choe
    et al., [2020](#bib.bib28)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管面临固有的挑战（Choe 等，[2020](#bib.bib28)），弱监督学习（WSL）（Zhou，[2017](#bib.bib175)）技术最近出现，以减轻对密集注释的需求，特别是在计算机视觉应用中。这些技术适应了不同形式的弱监督，包括图像标签（图像级别标签）（Kim
    等，[2017](#bib.bib81)；Pathak 等，[2015](#bib.bib107)；Teh 等，[2016](#bib.bib144)；Wei
    等，[2017](#bib.bib154)）、涂鸦（Lin 等，[2016](#bib.bib89)；Tang 等，[2018](#bib.bib142)）、点（Bearman
    等，[2016](#bib.bib9)）、边界框（Dai 等，[2015](#bib.bib33)；Khoreva 等，[2017](#bib.bib78)）、全局图像统计，例如目标大小（Bateson
    等，[2019](#bib.bib7)；Jia 等，[2017](#bib.bib73)；Kervadec 等，[2019a](#bib.bib76)，[b](#bib.bib77)）。减少的弱监督需求提供了一个有吸引力的学习框架。本文重点关注允许使用仅图像级别注释进行DL模型训练的WSL方法，用于组织学图像分类和与类预测相关的图像ROI定位。这些方法执行弱监督对象定位（WSOL）任务，可以生成激活图和边界框（Choe
    等，[2020](#bib.bib28)）。
- en: Interpretability frameworks (Samek et al., [2019](#bib.bib121); Zhang et al.,
    [2021](#bib.bib171)) have attracted much attention in computer vision (Alber et al.,
    [2019](#bib.bib2); Bau et al., [2017](#bib.bib8); Belharbi et al., [2021](#bib.bib12);
    Dabkowski and Gal, [2017](#bib.bib32); Fong et al., [2019](#bib.bib49); Fong and
    Vedaldi, [2017](#bib.bib50); Goh et al., [2020](#bib.bib56); Murdoch et al., [2019](#bib.bib97);
    Petsiuk et al., [2018](#bib.bib108), [2020](#bib.bib109); Ribeiro et al., [2016](#bib.bib117);
    Samek et al., [2020](#bib.bib122); Zhang et al., [2020b](#bib.bib169)), and medical
    image analysis (Cruz-Roa et al., [2013](#bib.bib31); De La Torre et al., [2020](#bib.bib35);
    Fan et al., [2020](#bib.bib47); Ghosal and Shah, [2020](#bib.bib55); Hägele et al.,
    [2020](#bib.bib61); Hao et al., [2019](#bib.bib63); Korbar et al., [2017](#bib.bib84);
    Saleem et al., [2021](#bib.bib120); Tavolara et al., [2020](#bib.bib143)). They
    are related to WSOL in the sense that it also allows providing a spatial map associated
    with a class prediction decision. However, interpretability methods are often
    evaluated differently, using for instance the pointing game (Zhang et al., [2018b](#bib.bib166)),
    which allows localizing an object via a point. Therefore, we limit the focus of
    this paper to DL models in the literature that were designed and evaluated mainly
    for the localization task.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性框架（Samek 等，[2019](#bib.bib121)；Zhang 等，[2021](#bib.bib171)）在计算机视觉（Alber 等，[2019](#bib.bib2)；Bau
    等，[2017](#bib.bib8)；Belharbi 等，[2021](#bib.bib12)；Dabkowski 和 Gal，[2017](#bib.bib32)；Fong
    等，[2019](#bib.bib49)；Fong 和 Vedaldi，[2017](#bib.bib50)；Goh 等，[2020](#bib.bib56)；Murdoch
    等，[2019](#bib.bib97)；Petsiuk 等，[2018](#bib.bib108)，[2020](#bib.bib109)；Ribeiro
    等，[2016](#bib.bib117)；Samek 等，[2020](#bib.bib122)；Zhang 等，[2020b](#bib.bib169)）以及医学图像分析（Cruz-Roa
    等，[2013](#bib.bib31)；De La Torre 等，[2020](#bib.bib35)；Fan 等，[2020](#bib.bib47)；Ghosal
    和 Shah，[2020](#bib.bib55)；Hägele 等，[2020](#bib.bib61)；Hao 等，[2019](#bib.bib63)；Korbar
    等，[2017](#bib.bib84)；Saleem 等，[2021](#bib.bib120)；Tavolara 等，[2020](#bib.bib143)）中受到了广泛关注。它们与WSOL有关，因为它也允许提供与类预测决策相关的空间图。然而，解释性方法通常有不同的评估方式，例如使用指向游戏（Zhang
    等，[2018b](#bib.bib166)），该方法通过一个点来定位一个对象。因此，我们将本文的重点限制在文献中主要用于定位任务的深度学习模型上。
- en: Currently, Class Activation Mapping (CAM) methods are practically the only technique
    for WSOL (Belharbi et al., [2022c](#bib.bib15)). CAMs are built on top of convolution
    responses over an image, leading to a natural emergence of ROIs. Strong spatial
    activations in CAMs correspond to discriminative ROIs (Zhou et al., [2016](#bib.bib172))
    which allow object localization. Note that localization maps in CAM-based methods
    are part of the model itself. Such methods have been widely studied in the literature
    for the weakly-supervised object localization task. In parallel, other methods
    have emerged for the interpretability, explainability, and visualization of machine
    learning models (Samek et al., [2019](#bib.bib121)). These methods often provide
    visualization tools, such as saliency maps to characterize the response of a pre-trained
    network for the input image. These methods include approaches such as attribution
    methods (Dabkowski and Gal, [2017](#bib.bib32); Fong and Vedaldi, [2017](#bib.bib50);
    Fong et al., [2019](#bib.bib49); Petsiuk et al., [2018](#bib.bib108); Zeiler and
    Fergus, [2014](#bib.bib162)). Different from the CAM, they produce saliency maps
    that are external to the network architecture, and that are often estimated by
    solving an optimization problem. In addition to not being commonly used for object
    localization, these methods have their own evaluation metrics, such as the pointing
    game (Zhang et al., [2018b](#bib.bib166)). Apart from CAM methods,  (Meethal et al.,
    [2020](#bib.bib95)) presents the only work that aims to directly produce a bounding
    box, without using any CAMs, in order to localize objects. In (Zhang et al., [2020a](#bib.bib164)),
    the authors aim to train a regressor to produce bounding boxes, where the target
    boxes are estimated from CAMs. Our review has shown that there very few works
    weakly localize objects without using CAMs, because of the difficulty in producing
    a bounding box using only global labels. CAMs have emerged as a natural response
    of convolution over visible pixels. However, a bounding box is an abstract and
    invisible shape, making it difficult to produce without explicit supervision,
    \ie, bounding box target. This difference between the two approaches explains
    the current state of the literature on WSOL. Generally, the goal of CAM methods
    is to build a model that is able to correctly classify an image where only image-class
    labels are needed. The methods also yield a per-class activation map, \ie, a CAM,
    under the form of a soft-segmentation map allowing the pixel-wise localization
    of objects. This map can also be post-processed to estimate a bounding box. Therefore,
    the scope of this paper is limited to CAM methods.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，类别激活映射（CAM）方法几乎是WSOL的唯一技术（Belharbi et al., [2022c](#bib.bib15)）。CAM建立在图像的卷积响应之上，自然形成了感兴趣区域（ROIs）。CAM中的强空间激活对应于具有区分性的ROIs（Zhou
    et al., [2016](#bib.bib172)），这允许对象定位。请注意，CAM方法中的定位图是模型本身的一部分。这些方法在文献中被广泛研究，用于弱监督对象定位任务。与此同时，其他方法也出现了，用于机器学习模型的可解释性、可解释性和可视化（Samek
    et al., [2019](#bib.bib121)）。这些方法通常提供可视化工具，如显著性图，以描述预训练网络对输入图像的响应。这些方法包括诸如归因方法（Dabkowski
    and Gal, [2017](#bib.bib32); Fong and Vedaldi, [2017](#bib.bib50); Fong et al.,
    [2019](#bib.bib49); Petsiuk et al., [2018](#bib.bib108); Zeiler and Fergus, [2014](#bib.bib162)）等方法。与CAM不同，它们生成的是网络结构之外的显著性图，并且通常通过解决优化问题来估计。除了不常用于对象定位外，这些方法还有自己的评估指标，如指向游戏（Zhang
    et al., [2018b](#bib.bib166)）。除了CAM方法，（Meethal et al., [2020](#bib.bib95)）提出了唯一旨在直接生成边界框的方法，而不使用任何CAMs，以实现对象定位。在（Zhang
    et al., [2020a](#bib.bib164)）中，作者旨在训练一个回归器生成边界框，其中目标框是从CAMs估计的。我们的综述表明，几乎没有研究在不使用CAM的情况下进行弱定位，因为仅使用全局标签生成边界框是困难的。CAMs作为对可见像素进行卷积的自然响应出现。然而，边界框是一种抽象且不可见的形状，没有明确的监督，即边界框目标，很难生成。这两种方法之间的差异解释了目前在WSOL文献中的状态。一般而言，CAM方法的目标是构建一个能够正确分类图像的模型，只需要图像类别标签。这些方法还产生每类激活图，即CAM，形式为软分割图，允许对象的像素级定位。该图还可以进行后处理以估计边界框。因此，本文的范围限于CAM方法。
- en: In this work, we provide a review of state-of-the-art deep WSOL methods proposed
    from 2013 to early 2022\. Most of these reviewed methods have been proposed and
    evaluated on natural image datasets, with only few having been developed and evaluated
    with histology images in mind. The performance of representative methods is compared
    using two public histology datasets for breast and colon cancer, allowing to assess
    their classification and localization performance. While there have been different
    reviews of ML/DL models for medical image analysis, and particularly for the analysis
    of histology WSIs (Daisuke and Shumpei, [2018](#bib.bib34); Janowczyk and Madabhushi,
    [2016](#bib.bib72); Kandemir and Hamprecht, [2015](#bib.bib75); Litjens et al.,
    [2017](#bib.bib91); Sudharshan et al., [2019](#bib.bib136)) and medical video
    analysis (Quellec et al., [2017](#bib.bib112)), these have focused on fully supervised
    tasks, semi-supervised tasks, or a mixture of different learning settings for
    classification and segmentation tasks (Litjens et al., [2017](#bib.bib91); Srinidhi
    et al., [2019](#bib.bib133)). To our knowledge, this paper represents the first
    review focused on deep WSOL models, trained on data with image-class labels for
    the classification of histology images and localization of ROIs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提供了对2013年到2022年初提出的最先进的深度WSOL方法的综述。大多数这些被综述的方法是在自然图像数据集上提出和评估的，仅有少数是针对组织学图像开发和评估的。我们使用两个公共组织学数据集（乳腺癌和结肠癌）对代表性方法的性能进行比较，从而评估它们的分类和定位性能。尽管已经有关于医疗图像分析的ML/DL模型的不同综述，特别是针对组织学WSI的分析（Daisuke
    和 Shumpei, [2018](#bib.bib34); Janowczyk 和 Madabhushi, [2016](#bib.bib72); Kandemir
    和 Hamprecht, [2015](#bib.bib75); Litjens et al., [2017](#bib.bib91); Sudharshan
    et al., [2019](#bib.bib136)）以及医疗视频分析（Quellec et al., [2017](#bib.bib112)），这些综述都集中于完全监督的任务、半监督的任务，或者分类和分割任务的不同学习设置的混合（Litjens
    et al., [2017](#bib.bib91); Srinidhi et al., [2019](#bib.bib133)）。据我们所知，本文是首个集中于深度WSOL模型的综述，专注于在图像类别标签数据上训练，用于组织学图像的分类和ROI的定位。
- en: Deep WSOL methods in the literature are divided into two main categories, based
    on the flow of information in models, namely, bottom-up and top-down methods.
    Our review shows that research in bottom-up methods is more active and dominant
    than is the case with to top-down methods, making the former state-of-the-art
    techniques. To address the shortcomings of CAMs, bottom-up methods have progressed
    from designing simple spatial pooling techniques to performing perturbations and
    self-attention, to using shallow features, and most recently, to exploiting pseudo-annotation
    and separating the training of classification from localization tasks. Recent
    successful WSOL techniques combine the use of shallow features, with pseudo-annotation,
    while decoupling classification and localization tasks. Top-down techniques for
    their part have seen less progress. The methods usually rely either on biologically-inspired
    processes, gradients, or confidence scores to build CAMs. Our comparative results
    study revealed that while deep WSOL methods proposed for histology data can yield
    good results, generic methods initially proposed for natural images nevertheless
    produced poor results. The former methods often rely on priors that aim to reduce
    false positives/negatives related to the ROI size, for example, or use explicit
    pixel-wise guidance collected from pre-trained classifiers. Overall, all WSOL
    methods suffer from high false positive/negative localization. We discuss several
    issues related to the application of such methods to histology data, including
    the under-/over-activation of CAMs, sensitivity to thresholding, and model selection.
    CAM over-activation is a new behavior that may be caused by the visual similarity
    between the foreground and the background.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中的深度 WSOL 方法主要分为两类，基于模型中的信息流，即自下而上和自上而下的方法。我们的综述显示，自下而上的方法的研究比自上而下的方法更为活跃且占据主导地位，使得前者成为了最先进的技术。为了应对
    CAM 的不足，自下而上的方法从设计简单的空间池化技术发展到执行扰动和自注意力，再到使用浅层特征，最近则利用伪标注和将分类训练与定位任务分开。近期成功的 WSOL
    技术结合了浅层特征和伪标注，同时解耦了分类和定位任务。自上而下的方法进展较少。这些方法通常依赖于生物启发的过程、梯度或置信度分数来构建 CAM。我们的比较结果研究揭示，尽管针对组织学数据提出的深度
    WSOL 方法能够产生良好结果，但最初为自然图像提出的通用方法结果却不佳。前者的方法通常依赖于旨在减少与 ROI 大小相关的假阳性/假阴性的先验，或使用从预训练分类器收集的显式像素级指导。总体而言，所有
    WSOL 方法都存在较高的假阳性/假阴性定位问题。我们讨论了这些方法在组织学数据应用中的几个问题，包括 CAM 的欠激活/过激活、对阈值的敏感性以及模型选择。CAM
    过激活是一种新的行为，可能是由前景和背景之间的视觉相似性引起的。
- en: 'In [section 2](#S2 "2 A taxonomy of weakly-supervised object localization methods
    ‣ Deep Weakly-Supervised Learning Methods for Classification and Localization
    in Histology Images: A Survey"), a taxonomy and a review of state-of-the-art deep
    WSOL methods are provided, followed by our experimental methodology ([section 3](#S3
    "3 Experimental methodology ‣ Deep Weakly-Supervised Learning Methods for Classification
    and Localization in Histology Images: A Survey")) and results ([section 4](#S4
    "4 Results and discussion ‣ Deep Weakly-Supervised Learning Methods for Classification
    and Localization in Histology Images: A Survey")). We conclude this work with
    a discussion of the main findings, and key challenges facing the application of
    such WSOL methods in histology, and provide future directions to mitigate these
    challenges and potentially reduce the gap in performance between WSOL and fully
    supervised methods. More experimental details are provided in [section A](#S1a
    "A Hyper-parameter search ‣ Deep Weakly-Supervised Learning Methods for Classification
    and Localization in Histology Images: A Survey") and [section B](#S2a "B CAMELYON16
    protocol for WSL ‣ Deep Weakly-Supervised Learning Methods for Classification
    and Localization in Histology Images: A Survey"). In addition, more visual results
    of localizations are presented in [section C](#S3a "C Visual results ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey").
    Our code is publicly available.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2节](#S2 "2 弱监督目标定位方法的分类 ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述")中，提供了一个分类和对最先进的深度WSOL方法的回顾，接着是我们的实验方法论（[第3节](#S3
    "3 实验方法论 ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述")）和结果（[第4节](#S4 "4 结果与讨论 ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述")）。我们在这项工作中总结了主要发现和在组织学中应用这些WSOL方法所面临的关键挑战，并提供了未来的方向，以缓解这些挑战，并可能缩小WSOL与完全监督方法之间的性能差距。更多实验细节见[第A节](#S1a
    "A 超参数搜索 ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述")和[第B节](#S2a "B CAMELYON16协议用于WSL ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述")。此外，[第C节](#S3a
    "C 视觉结果 ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述")展示了更多的定位视觉结果。我们的代码是公开的。
- en: 2 A taxonomy of weakly-supervised object localization methods
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 弱监督目标定位方法的分类
- en: '![Refer to caption](img/f46d009c2bb2b287790b96abea7e00af.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f46d009c2bb2b287790b96abea7e00af.png)'
- en: 'Figure 3: Overall taxonomy of deep WSOL methods for training on data with global
    image-class annotations, and classification and ROI localization. Methods in each
    category are ordered chronologically: 1\. Bottom-up: relies on forward pass information.
    2\. Top-down: Exploits both forward and backward pass information.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：深度WSOL方法的总体分类，用于训练具有全局图像类别注释的数据，以及分类和ROI定位。每个类别的方法按时间顺序排列：1\. 底向上：依赖于前向传递信息。2\.
    顶向下：利用前向和后向传递信息。
- en: '![Refer to caption](img/2bf2d70ce40f389644e6b7871cd3e4ec.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2bf2d70ce40f389644e6b7871cd3e4ec.png)'
- en: 'Figure 4: Illustration of the main differences between bottom-up (*top*) and
    top-down (*bottom*) methods for deep WSOL. Both approaches provide CAMs. However,
    bottom-up techniques produce them during the forward pass, while top-down techniques
    require a forward, then a backward pass to obtain them. The numbers 1, 2, and
    3 in circles indicate the order of operations. The top-down methods can either
    produce CAMs at the top before the class pooling or the input of the network.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：展示了底向上（*上*）和顶向下（*下*）方法在深度WSOL中的主要区别。两种方法都提供CAMs。然而，底向上技术在前向传递过程中生成它们，而顶向下技术则需要前向和后向传递才能获得它们。圆圈中的数字1、2和3表示操作的顺序。顶向下方法可以在类池化之前或网络的输入处生成CAMs。
- en: In our taxonomy, we focus on deep WSL methods that allow classifying an image
    and *pixel-wise* localize its ROIs via a heat map (soft-segmentation map, CAM)¹¹1In
    practice, these methods can also yield a bounding box after performing an image
    processing procedure over the CAM (Choe et al., [2020](#bib.bib28)). During training,
    only global image-class labels are required for supervision. These methods are
    referred to as weakly-supervised object localization methods (WSOL) (Choe et al.,
    [2020](#bib.bib28)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分类法中，我们关注于深度WSL方法，这些方法允许通过热图（软分割图，CAM）对图像进行分类并*逐像素*定位其ROI¹¹1在实际应用中，这些方法在对CAM进行图像处理程序后也可以生成边界框（Choe等，
    [2020](#bib.bib28)）。在训练过程中，只需要全局图像类别标签进行监督。这些方法被称为弱监督目标定位方法（WSOL）（Choe等， [2020](#bib.bib28)）。
- en: '[Figure 3](#S2.F3 "Figure 3 ‣ 2 A taxonomy of weakly-supervised object localization
    methods ‣ Deep Weakly-Supervised Learning Methods for Classification and Localization
    in Histology Images: A Survey") illustrates the overall taxonomy. Among deep WSOL
    methods, we identify two main categories based on the information flow in the
    network to yield region localization (see [Figure 4](#S2.F4 "Figure 4 ‣ 2 A taxonomy
    of weakly-supervised object localization methods ‣ Deep Weakly-Supervised Learning
    Methods for Classification and Localization in Histology Images: A Survey")):
    (a) bottom-up methods, which are based on the forward pass information within
    a network, and (b) top-down methods, which exploit the backward information in
    addition to a forward pass. Each WSOL aims to stimulate the localization of ROI
    using a different mechanism. Both categories rely on building a spatial attention
    map that has a high magnitude response over ROI and low activations over the background.
    The rest of this section provides the notations used herein, details on the main
    categories and sub-categories, and highlights of the main emerging trends that
    contributed to the progress of the WSOL task.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3](#S2.F3 "图 3 ‣ 2 弱监督目标定位方法的分类 ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述") 展示了整体分类。在深度WSOL方法中，我们根据网络中的信息流识别出两大类，用于区域定位（见[图 4](#S2.F4
    "图 4 ‣ 2 弱监督目标定位方法的分类 ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述")）：（a）自下而上的方法，基于网络中的前向传递信息；（b）自上而下的方法，利用了除了前向传递外的反向信息。每种WSOL方法旨在通过不同机制刺激ROI的定位。这两类方法都依赖于构建一个空间注意力图，该图在ROI上具有高幅度响应，而在背景上激活度低。本节其余部分提供了本文使用的符号、主要类别和子类别的详细信息，以及对WSOL任务进展贡献的主要新兴趋势的重点介绍。'
- en: Notation. To describe the mechanisms behind different methods, we introduce
    the following notation. Let us consider a set of training samples ${\mathbb{D}=\{(\bm{x}^{(t)},y^{(t)})\}}$
    of images ${\bm{x}^{(t)}\in\R^{D\times H^{\text{in}}\times W^{\text{in}}}}$ with
    $H^{\text{in}}$, $W^{\text{in}}$, $D$ being the height, width and depth of the
    input image, respectively; its image-level label (\ie, class) is $y^{(t)}\in\Y$,
    with $C$ possible classes. For simplicity, we refer to a training sample (an input
    and its label) as ${(\bm{x},y)}$.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 符号。为了描述不同方法背后的机制，我们引入以下符号。考虑一组训练样本${\mathbb{D}=\{(\bm{x}^{(t)},y^{(t)})\}}$，其中图像${\bm{x}^{(t)}\in\R^{D\times
    H^{\text{in}}\times W^{\text{in}}}}$的高度为$H^{\text{in}}$，宽度为$W^{\text{in}}$，深度为$D$；其图像级标签（即类别）为$y^{(t)}\in\Y$，共有$C$个可能的类别。为了简化，我们将训练样本（输入及其标签）称为${(\bm{x},y)}$。
- en: Let ${f_{\bm{\theta}}:\R^{D\times H^{\text{in}}\times W^{\text{in}}}\to\Y}$
    be a function that models a neural network, where the input ${\bm{x}}$ has an
    arbitrary height and width and ${\bm{\theta}}$ is the set of model parameters.
    The training procedure aims to optimize parameters ${\bm{\theta}}$ to achieve
    a specific task. In a multi-class scenario, the network typically outputs a vector
    of scores ${\bm{s}\in\R^{C}}$ in response to an input image. This vector is then
    normalized to obtain a posterior probability using a $\mathrm{softmax}$ function,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 设${f_{\bm{\theta}}:\R^{D\times H^{\text{in}}\times W^{\text{in}}}\to\Y}$为一个建模神经网络的函数，其中输入${\bm{x}}$具有任意高度和宽度，${\bm{\theta}}$是模型参数的集合。训练过程旨在优化参数${\bm{\theta}}$以实现特定任务。在多类别场景中，网络通常输出一个分数向量${\bm{s}\in\R^{C}}$，作为对输入图像的响应。然后使用$\mathrm{softmax}$函数将该向量归一化以获得后验概率，
- en: '|  | $\Prob(y=i&#124;\bm{x})=\softmax(\bm{s})_{i}=\frac{\exp(\bm{s}_{i})}{\sum_{j=1}^{C}\exp(\bm{s}_{j})}\;.$
    |  | (1) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Prob(y=i&#124;\bm{x})=\softmax(\bm{s})_{i}=\frac{\exp(\bm{s}_{i})}{\sum_{j=1}^{C}\exp(\bm{s}_{j})}\;.$
    |  | (1) |'
- en: 'The model predicts the class label corresponding to the maximum probability:
    $\argmax\{\Prob(y=i|\bm{x}):i=1,2,...,C\}=\argmax\{\bm{s}_{i}:i=1,2,...,C\}$.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型预测与最大概率对应的类别标签：$\argmax\{\Prob(y=i|\bm{x}):i=1,2,...,C\}=\argmax\{\bm{s}_{i}:i=1,2,...,C\}$。
- en: Besides the classification of the input image, we are also interested in the
    pixel-wise localization of ROIs within the image. Typically, a WSOL method can
    predict a set of $C$ activation maps of height $H$ and width $W$ to indicate the
    location of the regions of each class. We note this set as a tensor of shape $\bm{M}\in\R^{C\times
    H\times W}$, where ${\bm{M}_{c}}$ indicates the $c^{\text{th}}$ map. ${\bm{M}}$
    is commonly referred to as *Class Activation Maps* (CAMs). Due to convolutional
    and downsampling operations, typical CAMs have a low resolution as compared to
    the input image. We note the downscale factor as $S$, such that $H=H^{\text{in}}/S$
    and $W=W^{\text{in}}/S$. Interpolation is often required to yield a CAM of the
    same size as the image.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对输入图像进行分类外，我们还关注图像中感兴趣区域（ROIs）的逐像素定位。通常，WSOL 方法可以预测一组 $C$ 个高度为 $H$、宽度为 $W$
    的激活图，以指示每个类别区域的位置。我们将这一组记为形状为 $\bm{M}\in\R^{C\times H\times W}$ 的张量，其中 ${\bm{M}_{c}}$
    表示第 $c^{\text{th}}$ 个图。${\bm{M}}$ 通常被称为 *类别激活图*（CAMs）。由于卷积和下采样操作，典型的 CAMs 相较于输入图像具有较低的分辨率。我们将下采样因子记为
    $S$，使得 $H=H^{\text{in}}/S$ 和 $W=W^{\text{in}}/S$。通常需要插值来生成与图像相同大小的 CAM。
- en: 2.1 Bottom-up WSOL techniques
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 自下而上的 WSOL 技术
- en: 'In bottom-up methods, the pixel-wise localization is based on the activation
    of the feature maps resulting from the standard flow of information within a network
    from the input layer into the output layer (forward pass, [Figure 4](#S2.F4 "Figure
    4 ‣ 2 A taxonomy of weakly-supervised object localization methods ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey")
    (*top*)). Within this category, we identify two different subcategories of techniques
    to address weakly supervised localization. The first category contains techniques
    that rely mainly on spatial pooling. Different ways were proposed to pool class
    scores while simultaneously stimulating a spatial response in CAM to localize
    ROIs. These methods had limited success. Therefore, another type of method emerged
    and aimed to refine CAMs directly while using simple spatial pooling techniques.
    In the next subsections, we present these methods and their variants.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '在自下而上的方法中，逐像素定位是基于特征图的激活，这些特征图源自网络中从输入层到输出层的标准信息流（前向传播，[图 4](#S2.F4 "Figure
    4 ‣ 2 A taxonomy of weakly-supervised object localization methods ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey")
    (*top*)）。在这一类别中，我们识别出两种不同的技术子类别来处理弱监督定位。第一类包含主要依赖空间池化的技术。提出了不同的方法来池化类别得分，同时在 CAM
    中刺激空间响应以定位 ROIs。这些方法的成功有限。因此，出现了另一种类型的方法，旨在直接精炼 CAMs，同时使用简单的空间池化技术。在接下来的小节中，我们将介绍这些方法及其变体。'
- en: 2.2 Spatial pooling methods
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 空间池化方法
- en: 'This family of techniques aims to design different spatial pooling methods
    to compute per-class scores, which are then used to train the whole network for
    classification using standard cross-entropy. In some cases, the pooling is performed
    to build an image representation, \ie, bag features. Such spatial pooling allows
    building maps (CAMs) to localize ROIs. Each method promotes the emergence of ROIs
    localization differently. This strategy undergirds WSOL, and is considered a pioneering
    mechanism that introduced weakly supervised localization in deep models (Lin et al.,
    [2013](#bib.bib90)). Learning preserving spatial information in CAMs allows ROIs
    *localization* while requiring only global class annotation. Different methods
    have been proposed to compute the class scores from spatial maps, whith each pooling
    strategy having a direct impact on the emerging localization. The challenge is
    to stimulate the emergence of just ROI in the CAM. All techniques usually start
    off the same way: a CNN extracts $K$ feature maps $\bm{F}\in\R^{K\times H\times
    W}$, where $K$ is the number of feature maps, which is architecture-dependent.
    The feature maps $\bm{F}$ are then used to compute a per-class score using a spatial
    pooling technique.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这类技术旨在设计不同的空间池化方法来计算每类的分数，这些分数随后用于训练整个网络以进行分类，使用标准的交叉熵。在某些情况下，池化被执行以构建图像表示，即，袋特征。这种空间池化允许构建映射（CAMs）以定位ROIs。每种方法以不同的方式促进ROIs定位的出现。这一策略支撑了WSOL，并被认为是引入深度模型中的弱监督定位的开创性机制（Lin等，[2013](#bib.bib90)）。学习在CAMs中保持空间信息可以实现ROIs的*定位*，同时只需全局类别注释。已提出不同的方法来计算空间映射中的类别分数，每种池化策略对出现的定位有直接影响。挑战在于刺激CAM中仅出现ROI。所有技术通常以相同的方式开始：CNN提取$K$个特征图$\bm{F}\in\R^{K\times
    H\times W}$，其中$K$是特征图的数量，取决于架构。特征图$\bm{F}$随后用于使用空间池化技术计算每类分数。
- en: The first method is Global Average Pooling (GAP) (Lin et al., [2013](#bib.bib90)).
    It simply averages each feature map in ${\bm{F}\in\R^{K\times H\times W}}$ to
    yield the per-map score in order to build the global representation ${\bm{f}\in\R^{K}}$
    of the input image,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是全局平均池化（GAP）（Lin等，[2013](#bib.bib90)）。它简单地对每个特征图在${\bm{F}\in\R^{K\times
    H\times W}}$中取平均，以产生每图的分数，从而构建输入图像的全局表示${\bm{f}\in\R^{K}}$。
- en: '|  | $\bm{f}_{k}=\frac{1}{H\,W}\sum\limits_{i=1,j=1}^{H,W}\bm{F}_{k,i,j}\;,$
    |  | (2) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{f}_{k}=\frac{1}{H\,W}\sum\limits_{i=1,j=1}^{H,W}\bm{F}_{k,i,j}\;,$
    |  | (2) |'
- en: where $\bm{f}_{k}$ is the $k^{\text{th}}$ feature of the output. The class-specific
    activations are then obtained by a linear combination of the features using the
    weights of the classification layer. Note that in practice, one can directly average
    CAMs, when available, to yield per-class scores instead of using an intermediate
    dense layer. In both cases, this pooling strategy ties the per-class score to
    *all* spatial locations on a map. This means that both ROIs and the background
    participate in the computation of the per-class score. The CAM literature shows
    that this pooling strategy can be used to allow a CNN to perform localization
    using only global labels (Zhou et al., [2016](#bib.bib172)). Typically, in a CNN,
    the last layer which classifies the representation $\bm{f}$ is a fully connected
    layer parameterized by $\bm{W}\in\R^{C\times K}$ such that $\bm{s}=\bm{W}\bm{f}$
    (bias is omitted for simplicity). The CAMs, denoted as ${\bm{M}\in\mathbb{R}^{C\times
    H\times W}}$, are then obtained using a weighted sum of the spatial feature ${\bm{F}}$,
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\bm{f}_{k}$是输出的$k^{\text{th}}$特征。然后通过使用分类层的权重对特征进行线性组合来获得类别特定的激活。请注意，在实践中，当CAMs可用时，可以直接平均CAMs，以产生每类分数，而不是使用中间密集层。在这两种情况下，这种池化策略将每类分数与映射上的*所有*空间位置联系起来。这意味着ROIs和背景都参与了每类分数的计算。CAM文献表明，这种池化策略可以使CNN仅使用全局标签进行定位（Zhou等，[2016](#bib.bib172)）。通常，在CNN中，对表示$\bm{f}$进行分类的最后一层是由$\bm{W}\in\R^{C\times
    K}$参数化的全连接层，使得$\bm{s}=\bm{W}\bm{f}$（为简便起见省略了偏差）。CAMs，记作${\bm{M}\in\mathbb{R}^{C\times
    H\times W}}$，然后通过空间特征${\bm{F}}$的加权和获得。
- en: '|  | $\bm{M}_{c}=\sum\limits_{k=1}^{K}\bm{W}_{c,k}\,\bm{F}_{k}\;.$ |  | (3)
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{M}_{c}=\sum\limits_{k=1}^{K}\bm{W}_{c,k}\,\bm{F}_{k}\;.$ |  | (3)
    |'
- en: This strategy has been widely used for natural scene images as well as for medical
    images (Feng et al., [2017](#bib.bib48); Gondal et al., [2017](#bib.bib57); Izadyyazdanabadi
    et al., [2018](#bib.bib70); Sedai et al., [2018](#bib.bib123)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这一策略已广泛用于自然场景图像以及医学图像（Feng等，[2017](#bib.bib48)；Gondal等，[2017](#bib.bib57)；Izadyyazdanabadi等，[2018](#bib.bib70)；Sedai等，[2018](#bib.bib123)）。
- en: 'An early work on CAM methods (Zhou et al., [2016](#bib.bib172)) revealed a
    fundamental issue, namely, under-activation. CAMs tend to activate only on small
    discriminative regions, and therefore, localizing only a small part of the object
    while missing a large part of it. This leads to high false negatives. Subsequent
    works in WSOL aimed mainly to tackle this issue by pushing activations in CAM
    to cover the entire object. This is done either through a different pooling strategy
    or by explicitly designing a method aiming to recover the full object ([subsubsection 2.2.1](#S2.SS2.SSS1
    "2.2.1 CAM refinement methods ‣ 2.2 Spatial pooling methods ‣ 2 A taxonomy of
    weakly-supervised object localization methods ‣ Deep Weakly-Supervised Learning
    Methods for Classification and Localization in Histology Images: A Survey")).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '对于CAM方法的早期研究（Zhou等，[2016](#bib.bib172)）揭示了一个基本问题，即欠激活。CAM倾向于仅在小的区分区域激活，因此只能局部化物体的一小部分，而忽略了大部分。这导致了较高的假阴性。随后的WSOL工作主要旨在通过推动CAM中的激活以覆盖整个物体来解决此问题。这可以通过不同的池化策略或通过显式设计旨在恢复整个物体的方法来实现（[subsubsection
    2.2.1](#S2.SS2.SSS1 "2.2.1 CAM refinement methods ‣ 2.2 Spatial pooling methods
    ‣ 2 A taxonomy of weakly-supervised object localization methods ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey")）。'
- en: As an alternative to averaging spatial responses, authors in (Oquab et al.,
    [2015](#bib.bib105)) consider using the *maximum* response value on the CAM as
    a per-class score (MAX-Pool). The method therefore avoids including potential
    background regions in the class score, thus reducing false positives. However,
    this pooling technique tends also to focus on small discriminative parts of objects
    since the per-class score is tied only to one pixel of the response map²²2Note
    that one pixel in the CAM corresponds to a large surface in the input image depending
    on the size of the receptive field of the network at the CAM layer.. To alleviate
    this problem  (Pinheiro and Collobert, [2015](#bib.bib111); Sun et al., [2016](#bib.bib138))
    consider using a smoothed approximation of the maximum function to discover larger
    parts of objects of interest using the *log-sum-exp* function (LSE),
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作为空间响应平均的替代方法，作者在（Oquab等，[2015](#bib.bib105)）中考虑了使用CAM上的*最大*响应值作为每类得分（MAX-Pool）。该方法因此避免了将潜在的背景区域纳入类别得分，从而减少了假阳性。然而，这种池化技术也倾向于关注物体的较小区分部分，因为每类得分仅与响应图的一个像素相关²²2注意，CAM中的一个像素对应于输入图像中的大表面，这取决于CAM层网络的感受野大小。为缓解这一问题，（Pinheiro和Collobert，[2015](#bib.bib111)；Sun等，[2016](#bib.bib138)）考虑使用*对数和指数*函数（LSE）来平滑近似最大函数，以发现更大部分的目标物体。
- en: '|  | $\bm{s}_{c}=\frac{1}{q}\log\big{[}\frac{1}{H\,W}\sum_{i=1,j=1}^{H,W}\exp(q\,\bm{M}_{c,i,j})\big{]}\;,$
    |  | (4) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{s}_{c}=\frac{1}{q}\log\big{[}\frac{1}{H\,W}\sum_{i=1,j=1}^{H,W}\exp(q\,\bm{M}_{c,i,j})\big{]}\;,$
    |  | (4) |'
- en: where $q\in\R_{+}^{*}$ controls the smoothness of the approximation. A small
    $q$ value makes the approximation closer to the average function, while a large
    $q$ makes it close to the maximum function. Thus, with small $q$ values, make
    the network consider large regions, while large values consider only small regions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $q\in\R_{+}^{*}$ 控制近似的平滑度。小的 $q$ 值使近似更接近平均函数，而大的 $q$ 值使其接近最大函数。因此，使用小的 $q$
    值可以让网络考虑大区域，而大的 $q$ 值仅考虑小区域。
- en: Instead of considering the maximum of the map (Oquab et al., [2015](#bib.bib105)),
    \ie, a single high response point, authors in (Zhou et al., [2018](#bib.bib173))
    (PRM) propose to use *local maxima*. This amounts to using local peak responses
    which are more likely to cover a larger part of the object than occurs when using
    only the single maximum response,
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与考虑地图的最大值（Oquab等，[2015](#bib.bib105)），即单个高响应点不同，作者在（Zhou等，[2018](#bib.bib173)）（PRM）中提出使用*局部极大值*。这意味着使用局部峰值响应，这些响应更有可能覆盖物体的更大部分，而不仅仅是单个最大响应。
- en: '|  | $\bm{s}_{c}=\bm{M}^{c}*G^{c}=\frac{1}{N^{c}}\sum^{N^{c}}_{k=1}\bm{M}^{c}_{i_{k},j_{k}}\;,$
    |  | (5) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{s}_{c}=\bm{M}^{c}*G^{c}=\frac{1}{N^{c}}\sum^{N^{c}}_{k=1}\bm{M}^{c}_{i_{k},j_{k}}\;,$
    |  | (5) |'
- en: where $G^{c}\in\mathbb{R}^{H\times W}$ is a sampling kernel, $*$ is the convolution
    operation, and $N^{c}$ is the number of local maxima. Depending on the size of
    the kernel, this pooling allows stimulating different distant locations, which
    can help recover adjacent regions with an object. Similarly, it is likely that
    background regions are stimulated. This highlights the challenge faced with transferring
    global labels into local pixels. Note that such a transfer of supervision is known
    to be an ill-posed problem in the field of WSOL (Wan et al., [2018](#bib.bib149)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G^{c}\in\mathbb{R}^{H\times W}$ 是一个采样核，$*$ 是卷积操作，$N^{c}$ 是局部极大值的数量。根据核的大小，这种池化允许激发不同的远处位置，这有助于恢复具有物体的邻近区域。类似地，背景区域也可能被激发。这突出了将全局标签转移到局部像素的挑战。请注意，这种监督转移在弱监督学习（WSOL）领域被认为是一个不适定问题（Wan
    et al., [2018](#bib.bib149)）。
- en: All the pooling methods discussed thus far rely on high responses to yield per-class
    scores. The assumption with CAMs is that strong responses indicate potential ROIs,
    while low responses are more likely to represent backgrounds. This assumption
    is incorporated in the computation of per-class scores, and therefore, has a direct
    impact on the localization in CAMs. Authors in (Durand et al., [2017](#bib.bib45),
    [2016](#bib.bib44)) (WILDCAT) pursue a different strategy by including low activation,
    \ie, negative evidence, in the computation of the per-class score. They argue
    that such pooling plays a regularization role and prevents overfitting, allowing
    better classification performance. However, it remains unclear how tying negative
    regions to class scores improves localization, since the aim is to maximize the
    per-class score of the true label. Nevertheless, the authors provide an architectural
    change of the pooling layer where several *modality maps* per class are considered.
    Hence, these modalities allow to capture several parts of the object, leading
    to better localization. Formally, the pooling is written as,
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止讨论的所有池化方法都依赖于强响应以产生每类评分。CAMs 的假设是，强响应表示潜在的 ROI，而低响应更可能代表背景。这一假设被纳入每类评分的计算中，因此对
    CAMs 的定位有直接影响。Durand et al.（[2017](#bib.bib45), [2016](#bib.bib44)）的作者（WILDCAT）采用了一种不同的策略，通过将低激活，即负证据，纳入每类评分的计算中。他们认为这种池化起到正则化作用，防止过拟合，从而提高分类性能。然而，目前尚不清楚将负区域与类别评分绑定如何改善定位，因为目标是最大化真实标签的每类评分。尽管如此，作者提供了一种池化层的架构变更，其中每个类别考虑了几个*模态图*。因此，这些模态能够捕捉物体的多个部分，从而实现更好的定位。形式上，池化可以写作，
- en: '|  | $\bm{s}_{c}=\frac{Z_{c}^{+}}{n^{+}}+\alpha\frac{Z_{c}^{-}}{n^{-}}\;,$
    |  | (6) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{s}_{c}=\frac{Z_{c}^{+}}{n^{+}}+\alpha\frac{Z_{c}^{-}}{n^{-}}\;,$
    |  | (6) |'
- en: where $Z_{c}^{+}$ and $Z_{c}^{-}$ correspond to the sum of the $n^{+}$ highest
    and $n^{-}$ lowest activations of $\bm{M}_{c}$ respectively, and $\alpha$ is a
    hyper-parameter that controls the importance of the minimum scoring regions. Such
    an operation consists in selecting for each class the ${n^{+}}$ highest activation
    and the ${n^{-}}$ lowest activation within the corresponding map. This method
    has also been used in the medical field for weakly supervised region localization
    and image classification in histology images (Belharbi et al., [2019](#bib.bib11),
    [2022b](#bib.bib14)). In (Courtiol et al., [2018](#bib.bib30)), instead of operating
    on pixels, the authors consider adapting (Durand et al., [2017](#bib.bib45), [2016](#bib.bib44))
    for WSIs to operate on instances (tiles).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Z_{c}^{+}$ 和 $Z_{c}^{-}$ 分别对应于 $\bm{M}_{c}$ 中 $n^{+}$ 个最高激活值和 $n^{-}$ 个最低激活值的总和，$\alpha$
    是一个超参数，用于控制最低评分区域的重要性。这种操作包括为每个类别选择对应图中的 ${n^{+}}$ 个最高激活值和 ${n^{-}}$ 个最低激活值。这种方法也已在医学领域用于弱监督区域定位和组织学图像分类（Belharbi
    et al., [2019](#bib.bib11), [2022b](#bib.bib14)）。在 (Courtiol et al., [2018](#bib.bib30))
    中，作者考虑将 (Durand et al., [2017](#bib.bib45), [2016](#bib.bib44)) 方法适配到 WSIs 上，以对实例（tiles）进行操作，而不是对像素进行操作。
- en: The aforementioned methods build a bag (image) representation, and then compute
    CAMs that hold local localization responses, and finally, pull the per-class scores.
    Authors in (Ilse et al., [2018](#bib.bib69)) (Deep MIL) rely explicitly on a multi-instance
    learning (MIL) framework (Carbonneau et al., [2018](#bib.bib21); Cheplygina et al.,
    [2019](#bib.bib26); Wang et al., [2018](#bib.bib152); Zhou, [2004](#bib.bib174)).
    Here, instance representations are firstly built. Then, using the attention mechanism (Bahdanau
    et al., [2015](#bib.bib4)), a bag representation is computed using a weighted
    average of the instances representations. In this case, it is the attention weights
    that represent the CAM. Strong weights indicate instances with ROIs, while small
    weights indicate background instances. This method requires changes to standard
    CNN models. In addition, it is tied to binary classification only. Adjusting to
    a multi-class context requires further changes to the architecture. Formally,
    given a set of features $\bm{F}\in\R^{K\times H\times W}$ extracted for an image,
    the representation $\bm{f}$ of the image is computed as,
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法构建了一个包（图像）表示，然后计算包含局部定位响应的 CAMs，最后提取每类的分数。作者在 (Ilse et al., [2018](#bib.bib69))（深度
    MIL）显式依赖于多实例学习（MIL）框架 (Carbonneau et al., [2018](#bib.bib21); Cheplygina et al.,
    [2019](#bib.bib26); Wang et al., [2018](#bib.bib152); Zhou, [2004](#bib.bib174))。首先建立实例表示。然后，使用注意力机制 (Bahdanau
    et al., [2015](#bib.bib4))，通过加权平均实例表示计算包表示。在这种情况下，注意力权重代表 CAM。强权重表示具有 ROI 的实例，而小权重表示背景实例。这种方法需要对标准
    CNN 模型进行更改。此外，它仅限于二分类。调整为多类上下文需要对架构进行进一步更改。形式上，给定一个从图像中提取的特征集 $\bm{F}\in\R^{K\times
    H\times W}$，图像的表示 $\bm{f}$ 计算为，
- en: '|  |  | $\displaystyle\bm{f}=\sum\limits_{i=1,j=1}^{H,W}\bm{A}_{i,j}\bm{F}_{i,j}\;,$
    |  | (7) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\bm{f}=\sum\limits_{i=1,j=1}^{H,W}\bm{A}_{i,j}\bm{F}_{i,j}\;,$
    |  | (7) |'
- en: '|  | and | $\displaystyle\bm{A}_{i,j}=\frac{\exp(\psi(\bm{F}_{i,j}))}{\sum_{i=1,j=1}^{H,W}\exp(\psi(\bm{F}_{i,j}))}\;,$
    |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | 和 | $\displaystyle\bm{A}_{i,j}=\frac{\exp(\psi(\bm{F}_{i,j}))}{\sum_{i=1,j=1}^{H,W}\exp(\psi(\bm{F}_{i,j}))}\;,$
    |  |'
- en: where $\bm{F}_{i,j}$ is the feature vector of the location (\ie, instance) indexed
    by $i$ and $j$. ${\psi:\R^{K}\to\R}$ is a scoring function. The resulting representation
    $\bm{f}$ is then classified by a fully connected layer. Two scoring functions
    are considered (Ilse et al., [2018](#bib.bib69)),
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\bm{F}_{i,j}$ 是由 $i$ 和 $j$ 索引的位点（即实例）的特征向量。${\psi:\R^{K}\to\R}$ 是一个评分函数。结果表示
    $\bm{f}$ 然后通过一个全连接层进行分类。考虑了两个评分函数 (Ilse et al., [2018](#bib.bib69))，
- en: '|  | $\displaystyle\psi_{1}(\bm{f})$ | $\displaystyle=\bm{w}\tanh(\bm{V}\bm{f})\;,$
    |  | (8) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\psi_{1}(\bm{f})$ | $\displaystyle=\bm{w}\tanh(\bm{V}\bm{f})\;,$
    |  | (8) |'
- en: '|  | $\displaystyle\psi_{2}(\bm{f})$ | $\displaystyle=\bm{w}\big{[}\tanh(\bm{V}\bm{f})\odot\sigma(\bm{U}\bm{f})\big{]}\;,$
    |  | (9) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\psi_{2}(\bm{f})$ | $\displaystyle=\bm{w}\big{[}\tanh(\bm{V}\bm{f})\odot\sigma(\bm{U}\bm{f})\big{]}\;,$
    |  | (9) |'
- en: where $\bm{w}\in\R^{L}$, $(\bm{V},\bm{U})\in\R^{L\times K}$ are learnable weights,
    and ${\odot}$ is an element-wise multiplication. This approach is designed specifically
    for binary classification and produces a matrix of attention weights ${\bm{A}\in[0,1]^{H\times
    W}}$ with ${\sum\bm{A}=1}$. In the next section, we present a second bottom-up
    category that aims to refine the CAMs directly.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{w}\in\R^{L}$，$(\bm{V},\bm{U})\in\R^{L\times K}$ 是可学习的权重，${\odot}$ 是逐元素乘法。这种方法专门为二分类设计，并生成一个注意力权重矩阵
    ${\bm{A}\in[0,1]^{H\times W}}$，其中 ${\sum\bm{A}=1}$。在下一节中，我们介绍了第二种自下而上的类别，旨在直接精细化
    CAMs。
- en: 2.2.1 CAM refinement methods
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 CAM 精细化方法
- en: 'While spatial pooling methods have helped the emergence of some discriminative
    regions in CAMs, they have limited success when it comes to covering the full
    foreground object. Under-activation of CAMs is still a major ongoing issue in
    the WSOL field, reflecting the difficulty face in to transferring global labels
    to pixel level. Ever since this became clear, research has shifted from improving
    the pooling function to explicitly overcoming the under-activation issue and recovering
    the entire object. Often, this is achieved while using simple pooling functions
    such as the GAP method (Lin et al., [2013](#bib.bib90)), and to this end, different
    strategies have been proposed. We divide these into two main categories: methods
    that use data augmentation to *mine* more discriminative regions, and methods
    that aim to enhance and learn better internal features of a CNN.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然空间池化方法在 CAMs 中帮助出现了一些具有辨别性的区域，但在覆盖整个前景物体方面效果有限。CAMs 的不足激活仍然是 WSOL 领域中的一个主要问题，反映了将全局标签转移到像素级别的困难。自从这一点变得清晰后，研究已经从改进池化函数转向明确克服不足激活问题并恢复整个物体。通常，这是在使用简单的池化函数如
    GAP 方法（Lin 等人，[2013](#bib.bib90)）时实现的，为此提出了不同的策略。我们将这些策略分为两个主要类别：使用数据增强来*挖掘*更多具有辨别性的区域的方法，以及旨在增强和学习
    CNN 更好内部特征的方法。
- en: Data augmentation methods. Data augmentation is a strategy often used in machine
    learning to prevent overfitting and improve performance (Goodfellow et al., [2016](#bib.bib58)).
    It has been similarly been used in the WSOL field to prevent models from overfitting
    one single discriminative region, \ie, from under-activating. This is often achieved
    by pushing the model to seek, \ie, *mine*, other discriminative regions, thereby
    promoting a large coverage of objects in CAM. Data augmentation most commonly
    takes the form as information suppression, \ie, *erasing*, where part of an input
    signal is deleted. This can be performed over input images or intermediate features.
    Conceptually, this can be seen as a *perturbation* process to stimulate the emergence
    of more ROIs. For instance, authors in (Singh and Lee, [2017](#bib.bib128)) propose
    a ’Hide-And-Seek’ (HaS) training strategy, where the input image is divided into
    multiple patches. During the training phase, only some of these patches are randomly
    set to be visible while the rest are hidden. Such data augmentation has already
    been shown to regularize CNN models and improve their classification performance (Devries
    and Taylor, [2017](#bib.bib39)) (cutout). This is similar to applying a dropout (Srivastava
    et al., [2014](#bib.bib134)) over the input image, where the target regions consist
    of a whole patch instead of a single pixel. As a result, the network will not
    overly rely on the most discriminative patches, and will seek other discriminative
    regions. While this is an advantage, it can be counter-productive as the network
    may inadvertently be pushed to consider the background as discriminative, especially
    for small objects that can be easily deleted.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强方法。数据增强是一种在机器学习中常用的策略，旨在防止过拟合并提高性能（Goodfellow 等人，[2016](#bib.bib58)）。在 WSOL
    领域中，它也被用来防止模型过度拟合单一的辨别性区域，即，防止不足激活。这通常是通过推动模型去寻找，即*挖掘*其他辨别性区域，从而促进 CAM 中的物体大范围覆盖来实现的。数据增强最常见的形式是信息抑制，即*擦除*，其中输入信号的一部分被删除。这可以在输入图像或中间特征上进行。从概念上讲，这可以视为一种*扰动*过程，以刺激更多
    ROI 的出现。例如，Singh 和 Lee（[2017](#bib.bib128)）提出了一种‘隐藏与寻找’（HaS）训练策略，其中输入图像被分割成多个补丁。在训练阶段，这些补丁中只有一部分是随机可见的，而其余部分则被隐藏。这种数据增强已经被证明可以正则化
    CNN 模型并提高其分类性能（Devries 和 Taylor，[2017](#bib.bib39)）（cutout）。这类似于在输入图像上应用 dropout（Srivastava
    等人，[2014](#bib.bib134)），其中目标区域是整个补丁而不是单个像素。结果是，网络不会过度依赖最具辨别性的补丁，而是会寻找其他辨别性区域。虽然这是一种优势，但也可能适得其反，因为网络可能会无意中被推向将背景视为辨别性的区域，尤其是对于那些容易被删除的小物体。
- en: 'Other data augmentations have been exploited to improve localization. For instance,
    the MixUp method (Zhang et al., [2018a](#bib.bib165)) was designed to regularize
    neural networks by making them less sensitive to adversarial examples and reducing
    their memorization. This is done by blending two training images to certain degree,
    in which case the label of the augmented image is assigned by the linear combination
    of the labels of the two images. Despite the augmented images looking unnatural
    and locally ambiguous, the method improves classification accuracy. Authors in (Yun
    et al., [2019](#bib.bib161)) (CutMix) adapt this method to improve localization.
    Instead of fully blending images, they propose to randomly cut a patch from an
    image and mix it with a second image. The label is mixed proportionally to the
    size of the patch. In essence, this is similar to cutout (Devries and Taylor,
    [2017](#bib.bib39)) and HaS (Singh and Lee, [2017](#bib.bib128)), but instead
    patches being filled with black or random noise, they are filled with pixels from
    another image. In practice, this has been shown to improve localization performance.
    However, due to the randomness in the source patch selection process, this method
    may select background regions leading to wrong mixed labels, which then leads
    to the classifier learning unexpected feature representations. Similarly, (Kim
    et al., [2020](#bib.bib82)) proposed PuzzleMix, which jointly optimizes two objectives:
    selecting an optimal mask and selecting an optimal mixing plan. Here, the mixing
    of the input images is no longer random, but uses image saliency, which emerges
    from image statistics. The mask tries to reveal the most salient data of the two
    images. Meanwhile, the optimal transport plan aims to maximize the saliency of
    the revealed portion of the data. In the same vein, SaliencyMix (Uddin et al.,
    [2021](#bib.bib147)) exploits image saliency, but uses a bounding box to capture
    a region remix instead of a mask. Note that relying on image saliency is a major
    drawback for less salient images such as those bearing histology data since the
    foreground and background look similar. Authors in (Stegmüller et al., [2022](#bib.bib135))
    (ScoreMix) applied this type of approach to histology data by using proposed regions
    via attention. Mixing region approach is based on classifier attention instead
    of image statistics. Discriminative regions from the sources are cut and mixed
    over non-discriminative regions of the target. Conceptually, this gives a better
    regional mixing. However, since the learned attention can easily hold false positives/negatives,
    the mixing can still be vulnerable. In addition, the obtained results seem relatively
    close to those of the CutMix method (Yun et al., [2019](#bib.bib161)).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数据增强方法已被用于提高定位精度。例如，MixUp 方法（Zhang 等，[2018a](#bib.bib165)）旨在通过减少神经网络对对抗样本的敏感性和降低其记忆能力来正则化神经网络。这是通过将两张训练图像混合到一定程度来完成的，在这种情况下，增强图像的标签由两张图像的标签的线性组合来分配。尽管增强图像看起来不自然且局部模糊，但该方法提高了分类准确性。
    (Yun 等，[2019](#bib.bib161)) 的作者（CutMix）将此方法调整以改善定位。他们提出了随机剪切图像的一部分并与第二张图像混合的方法，而不是完全混合图像。标签按补丁大小的比例混合。实质上，这类似于
    cutout（Devries 和 Taylor，[2017](#bib.bib39)）和 HaS（Singh 和 Lee，[2017](#bib.bib128)），但与填充黑色或随机噪声的补丁不同，这里补丁被填充为来自另一张图像的像素。实际上，这已被证明可以提高定位性能。然而，由于源补丁选择过程中的随机性，这种方法可能选择背景区域，导致错误的混合标签，从而使分类器学习到意外的特征表示。类似地，(Kim
    等，[2020](#bib.bib82)) 提出了 PuzzleMix，联合优化两个目标：选择最佳掩膜和选择最佳混合方案。在这里，输入图像的混合不再是随机的，而是利用从图像统计中出现的图像显著性。掩膜尝试揭示两张图像的最显著数据。同时，最佳传输方案旨在最大化揭示数据部分的显著性。同样，SaliencyMix（Uddin
    等，[2021](#bib.bib147)）利用图像显著性，但使用边界框捕捉区域混合而不是掩膜。请注意，依赖图像显著性是针对像组织数据这样的显著性较低的图像的主要缺陷，因为前景和背景看起来相似。
    (Stegmüller 等，[2022](#bib.bib135))（ScoreMix）通过使用注意力提出的区域将这种方法应用于组织数据。混合区域方法基于分类器注意力而不是图像统计。从源图像中裁剪的判别区域在目标图像的非判别区域上混合。从概念上讲，这提供了更好的区域混合。然而，由于学习的注意力容易包含假阳性/假阴性，因此混合仍然可能脆弱。此外，获得的结果似乎与
    CutMix 方法（Yun 等，[2019](#bib.bib161)）的结果相对接近。
- en: In (Wei et al., [2017](#bib.bib154)) (AE), the authors propose an iterative
    strategy to mine discriminative regions for semantic segmentation. Similarly to
    the HaS method (Singh and Lee, [2017](#bib.bib128)), they erase regions with the
    highest response values through learning epochs of a classifier. This allows the
    emergence of large parts of the model. The emerging segmentation proposals are
    used to train the model for semantic segmentation. Sequential erasing yields a
    computationally expensive process since multiple rounds are required. To improve
    this, ACoL (Zhang et al., [2018c](#bib.bib167)) designed two branch classifiers
    to predict the discriminative region and corresponding complementary area simultaneously.
    The MEIL method (Mai et al., [2020](#bib.bib94)) proceeds in a similar fashion
    by adding multiple output branches that exploit the erasing process within the
    learning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在（Wei et al.，[2017](#bib.bib154)）（AE）中，作者提出了一种迭代策略，用于挖掘语义分割的有区分性区域。与HaS方法（Singh和Lee，[2017](#bib.bib128)）类似，他们通过学习分类器的学习时期擦除具有最高响应值的区域。这允许模型的大部分出现。新出现的分割提议被用于训练模型进行语义分割。顺序擦除会导致计算负担沉重，因为需要多轮。为了改进这一点，ACoL（Zhang等，[2018c](#bib.bib167)）设计了两个分支分类器，同时预测有区分性的区域和相应的互补区域。MEIL方法（Mai等，[2020](#bib.bib94)）以类似的方式进行，通过添加利用学习中的擦除过程的多个输出分支。
- en: Guided Attention Inference Network (GAIN) (Li et al., [2018](#bib.bib87)) method
    uses two sequential networks with a shared backbone to mine ROIs. The first network
    yields an attention map of ROIs, which is used to erase discriminative regions
    in the image. The erased image is then fed into the next network, where its class
    response with respect to the target label is used to ensure that no discriminative
    regions are left in the image after the erasing process. The ROI suppression process
    is expected to push the first model to seek more discriminative regions, hence
    large ROIs are covered by the CAM. Similarly, authors in (Kim et al., [2017](#bib.bib81))
    (Two-Phase) consider two-phase training of two networks. The first network is
    trained until convergence. Then, it is used, with frozen weights, in front of
    a second network to produce a CAM of the target label. The CAM is thresholded
    to localize the most discriminative regions. Instead of masking the input image
    as done in the GAIN method (Li et al., [2018](#bib.bib87)), the authors consider
    masking intermediate feature maps. Once again, results show that this type of
    information hiding at the feature level allows exploring more ROIs to uncover
    complete objects.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 引导注意力推理网络（GAIN）（Li et al.，[2018](#bib.bib87)）方法使用两个顺序网络和一个共享主干来挖掘感兴趣区域（ROIs）。第一个网络产生ROIs的注意力图，用于擦除图像中的有区分性的区域。然后将擦除后的图像输入到下一个网络中，该网络利用其对目标标签的类响应来确保擦除过程后图像中没有有区分性的区域。期望ROI抑制过程促使第一个模型寻找更多有区分性的区域，因此大的ROIs都被CAM覆盖。类似地，Kim等人（[2017](#bib.bib81)）中考虑了两个网络的两阶段训练（Two-Phase）。第一个网络在收敛之前进行训练。然后，它与冻结的权重一起用于第二个网络的前面，以产生目标标签的CAM。CAM被阈值化以定位最有区分性的区域。作者不像GAIN方法（Li
    et al.，[2018](#bib.bib87)）中所做的那样遮盖输入图像，而是考虑遮盖中间特征图。结果再次显示，这种在特征级别的信息隐藏允许探索更多的ROIs以发现完整的对象。
- en: 'The GC-Net method (Lu et al., [2020](#bib.bib92)) considers incorporating Geometry
    Constraints (GC) to train a network to localize objects. Specifically, the authors
    use 3 models: a detector that yields object localization under the form of a box
    or an ellipse; a mask generator, which generates a mask based on the generated
    localization, and a classifier that is evaluated over the ROIs covered by the
    mask and its complement, \ie, background. The detector is trained to produce small
    ROIs in which the classifier has a high score while a low score is achieved over
    the background.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: GC-Net方法（Lu等，[2020](#bib.bib92)）考虑将几何约束（GC）纳入训练网络以定位目标。具体而言，作者使用3个模型：一个探测器，产生盒子或椭圆形的目标本地化；一个掩模生成器，基于生成的定位产生掩模；以及一个在掩模和其补充（即背景）上评估的分类器。探测器被训练以产生分类器具有高分数的小的ROIs，而在背景上达到低分数。
- en: 'Authors in (Belharbi et al., [2019](#bib.bib11)) (RecMin) consider a recursive
    mining algorithm integrated directly into back-propagation, allowing to mine ROIs
    on the fly. All these methods perform mining-erasing of information over the input
    image. The ADL (Choe and Shim, [2019](#bib.bib27)) method builds a self-attention
    map per layer to spot potential ROIs. Then, it stochastically erases locations
    over multiple intermediate feature maps at once during forward propagation through
    simple element-wise multiplication. The erasing is performed by simple dropout
    over the attention mask. Such a procedure allows the enhancement of both classification
    and localization performance. Note that self-attention was already used prior
    to ADL in (Zhu et al., [2017](#bib.bib176)) (SPN) as a layer to yield proposal
    regions that are coupled with feature maps allowing only potential ROIs to pass
    to the next layer, filtering out background/noise. Authors in (Belharbi et al.,
    [2022b](#bib.bib14)) (MAXMIN) use two models: a localizer, followed by a classifier.
    The localizer aims to build a CAM to localize ROIs at the pixel level. The input
    image is masked by the produced CAM, and then fed to the classifier. The authors
    explicitly include the background prior to learning the CAM by constraining it
    to holding both foreground and background regions. This prevents under-/over-activations,
    which in turn reduces false positives/negatives. Using entropy, the target classifier
    scores are constrained to be low over the background and high over the foreground,
    thus ensuring that no ROI is left in the background. A significant downside of
    these erasing/mining-based methods is their inherent risk of over-mining since
    there are no clear criteria to stop mining. While they are efficient at expanding
    and gathering object regions, it is very easy to expand to non-discriminative
    regions, which directly increases false positives.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在(Belharbi 等，[2019](#bib.bib11)) (RecMin)中，作者考虑了将递归挖掘算法直接集成到反向传播中的方法，从而允许即时挖掘感兴趣区域（ROIs）。所有这些方法在输入图像上执行信息的挖掘和擦除。ADL
    (Choe 和 Shim，[2019](#bib.bib27)) 方法在每一层构建自注意力图，以发现潜在的ROIs。然后，它在前向传播过程中，通过简单的逐元素乘法随机擦除多个中间特征图上的位置。这种擦除通过在注意力掩码上进行简单的
    dropout 实现。这种过程可以增强分类和定位的性能。值得注意的是，自注意力在ADL之前已在 (Zhu 等，[2017](#bib.bib176)) (SPN)
    中用作层，以生成与特征图耦合的提议区域，允许只有潜在的ROIs通过到下一层，从而过滤掉背景/噪声。在 (Belharbi 等，[2022b](#bib.bib14))
    (MAXMIN) 中，作者使用了两个模型：一个定位器，随后是一个分类器。定位器旨在构建 CAM 以在像素级别定位 ROIs。输入图像通过生成的 CAM 进行掩码处理，然后传递给分类器。作者明确在学习
    CAM 之前包括背景，通过将其约束为同时包含前景和背景区域。这可以防止过度/不足激活，从而减少假阳性/假阴性。通过熵，目标分类器的分数被约束在背景上较低，在前景上较高，从而确保背景中没有遗漏的ROI。这些擦除/挖掘方法的一个显著缺点是它们固有的过度挖掘风险，因为没有明确的标准来停止挖掘。虽然它们在扩展和收集目标区域方面非常有效，但很容易扩展到非区分性区域，这直接增加了假阳性。
- en: Features enhancement methods. Other methods aim to improve localization by learning
    better features. This is often achieved through architectural changes of standard
    models or by exploiting different levels of features for localization, such as
    shallow features. Additionally, using pseudo-labels to explicitly guide learning
    has emerged as an alternative approach for tackling WSOL tasks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 特征增强方法。其他方法旨在通过学习更好的特征来改善定位。这通常通过对标准模型进行架构更改或利用不同层次的特征进行定位（例如浅层特征）来实现。此外，使用伪标签来显式指导学习已成为解决
    WSOL 任务的一种替代方法。
- en: 'Authors in (Wei et al., [2018](#bib.bib155)) analyze the impact of the object
    scale on predictions and propose to exploit dilated-convolution (Chen et al.,
    [2018](#bib.bib25), [2015](#bib.bib24)). They equip a classifier with a varying
    dilation rate: multi-dilated convolutional (MDC) blocks. This has been shown to
    effectively enlarge the receptive fields of convolutional kernels, and more importantly,
    to transfer the surrounding discriminative information to non-discriminative object
    regions, promoting the emergence of ROI while suppressing the background. Unlike
    most works that pull CAMs from the top layer (high level), authors in (Yang et al.,
    [2020](#bib.bib160)) (NL-CCAM) consider non-local features by combining low- and
    high-level features to promote better localization. In addition, rather than using
    a per-class map as the final CAM, they combine all CAMs using a weighted sum after
    ordering them using their posterior class probabilities. This allows to gather
    several parts of the objects and to suppress background regions in the final localization
    map. The FickleNet method (Lee et al., [2019](#bib.bib86)) randomly selects hidden
    locations over feature maps. During training, for each random selection, a new
    CAM is generated. Therefore, for each input image, multiple CAMs can be generated
    to predict the most discriminative parts. This allows building CAMs that better
    cover the object. This method is related to ADL (Choe and Shim, [2019](#bib.bib27)),
    which uses attention, followed by dropout, to mask features. FickleNet does not
    rely on attention, and simply drops random locations.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: (Wei et al., [2018](#bib.bib155)) 的作者分析了对象尺度对预测的影响，并建议利用膨胀卷积 (Chen et al., [2018](#bib.bib25),
    [2015](#bib.bib24))。他们为分类器配备了一个可变膨胀率的多膨胀卷积 (MDC) 模块。这已被证明能够有效扩大卷积核的感受野，更重要的是，将周围的区分信息转移到非区分的对象区域，促进
    ROI 的出现，同时抑制背景。与大多数将 CAM 从顶层（高层）提取的工作不同，(Yang et al., [2020](#bib.bib160)) 的作者
    (NL-CCAM) 通过结合低级和高级特征来考虑非本地特征，以促进更好的定位。此外，他们不是使用每个类别的图作为最终 CAM，而是通过使用后验类别概率对所有
    CAM 进行加权求和。这允许在最终定位图中汇集对象的多个部分并抑制背景区域。FickleNet 方法 (Lee et al., [2019](#bib.bib86))
    随机选择特征图上的隐藏位置。在训练过程中，对于每个随机选择，会生成一个新的 CAM。因此，对于每个输入图像，可以生成多个 CAM 以预测最具区分性的部分。这允许构建更好地覆盖对象的
    CAM。这种方法与 ADL (Choe and Shim, [2019](#bib.bib27)) 相关，后者使用注意力，然后通过 dropout 遮盖特征。FickleNet
    不依赖于注意力，而是简单地丢弃随机位置。
- en: DANet (Xue et al., [2019](#bib.bib159)) uses multi-branch outputs at different
    layers to yield a CAM with different resolutions. This allows to obtain a hierarchical
    localization. To spread activation over the entire object *without* deteriorating
    the classification performance, the authors consider a joint optimization of two
    different terms. A discrepant divergent activation loss constrains CAMs of the
    same class to cover *different* regions. The authors note that classes with similar
    visual features are typically suppressed in standard CNNs, since the latter are
    not discriminative. To recover these regions, they propose a hierarchical divergent
    activation loss. Meta-classes are created hierarchically to gather previous meta-classes,
    in which the bottom of the hierarchy contains the original classes. At a specific
    level, the classifier is trained to assign the same meta-class for all samples
    assigned to it. This pushes shared similar features to activate within that meta-class,
    hence recovering similar features in original classes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: DANet (Xue et al., [2019](#bib.bib159)) 使用不同层次的多分支输出来生成具有不同分辨率的 CAM。这允许获得层次化的定位。为了在*不*影响分类性能的情况下使激活扩展到整个对象，作者考虑了两种不同项的联合优化。一个差异化的激活损失约束同类的
    CAM 覆盖*不同*区域。作者指出，具有相似视觉特征的类别在标准 CNN 中通常会被抑制，因为后者不具备区分能力。为了恢复这些区域，他们提出了层次化的差异化激活损失。元类别是分层创建的，以汇集之前的元类别，其中层次的底部包含原始类别。在特定层次上，分类器被训练为将所有分配给它的样本分配到相同的元类别中。这推动了共享的相似特征在该元类别内激活，从而恢复原始类别中的相似特征。
- en: In the I²C method (Zhang et al., [2020c](#bib.bib170)), the authors propose
    to leverage pixel-wise similarities at the spatial feature level via Inter-Image
    Communication (I²C) for better localization. Local and global discriminative features
    are pushed to be consistent. A local constraint aims to learn the stochastic feature
    consistency among discriminative pixels, which are randomly sampled from a pair
    of images within a batch. A global constraint is employed, where a global center
    feature per-class is maintained and updated in memory after each mini-batch. Average
    local random features are constrained to be close to the center class features.
    The ICL method (Ki et al., [2020](#bib.bib79)) aims to deal with over-activation
    by preventing CAMs from spiking over the background. An attention contrastive
    loss is proposed. Similar to ADL (Choe and Shim, [2019](#bib.bib27)), an attention
    map is estimated from feature maps. Very high and very low activations are used
    to estimate potential foreground and background regions. The middle activations
    could be either foreground or background. To expand the activation from foreground
    into uncertain region, the contrastive loss aims to push activation with *spatial
    features* similar to foreground features to be foreground while activations with
    similar spatial features to background features are pushed to be background. This
    allows a careful expansion of foreground activation toward background regions.
    In addition, attention at the top layer, which is semantically rich, is used in
    a self-learning setup to align and guide low layer attention, which is often noisy.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在 I²C 方法（Zhang 等人，[2020c](#bib.bib170)）中，作者提出通过 Inter-Image Communication (I²C)
    利用空间特征层面上的像素相似性来进行更好的定位。本文探索了局部和全局的差异判别特征。局部约束旨在学习来自批次内一对图像中随机采样的差异判别像素的随机特征一致性。全局约束使用了全局中心特征以保持并在每个小批量更新后更新类别的记忆中。平均局部随机特征被约束为接近于中心类别特征。ICL
    方法（Ki 等人，[2020](#bib.bib79)）旨在通过防止 CAMs 在背景上激增来处理过度激活问题。本文提出了一种注意力对比损失。类似于 ADL（Choe
    和 Shim，[2019](#bib.bib27)），注意力图是从特征图中估计出来的。非常高和非常低的激活值被用于估计潜在的前景和背景区域。中等的激活值可能是前景，也可能是背景。为了将激活从前景扩展到不确定区域，对比损失的目标是推动具有与前景特征相似的*空间特征*的激活成为前景，而将具有与背景特征相似的空间特征的激活推向背景。这允许谨慎地将前景激活扩展到背景区域。此外，本文利用了语义丰富的顶层注意力来对齐和引导通常嘈杂的底层注意力。
- en: The WSOL task has also benefited from recent advances in architectural design
    in deep learning. Transformers (Dosovitskiy et al., [2021](#bib.bib42)) in particular
    have seen their first use in such a task in the TS-CAM (Gao et al., [2021](#bib.bib53))
    method. A visual transformer (Dosovitskiy et al., [2021](#bib.bib42)) constructs
    a sequence of tokens by splitting an input image into patches with positional
    embedding and applying cascaded transformer blocks to extract a visual representation.
    Visual transformers can learn complex spatial transforms and reflect long-range
    semantic correlations adaptively via self-attention mechanism and multilayer perceptrons.
    This occurs to be crucial for localizing full object. TS-CAM (Gao et al., [2021](#bib.bib53))
    improves patch embeddings by exploiting self-attention. In addition, a class-agnostic
    map is built at each layer. The authors equipped the transformers’ output with
    a CAM module allowing to obtain semantic maps. The Final CAM is an aggregation
    between the CAM yielded by the CAM module and the average class-agnostic maps
    across all layers. This shows to help improve localization. In the CSTN method (Meethal
    et al., [2020](#bib.bib95)), the authors replace standard convolution filters
    with Spatial Transformer Networks (STNs) (Jaderberg et al., [2015](#bib.bib71)).
    In addition to using multi-scale localization, this STN model learns affine transformations,
    which can cover different variations including translation, scale, and rotation,
    allowing to better attend different object variations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: WSOL任务也受益于深度学习领域在架构设计方面的最新进展。特别是**Transformers**（Dosovitskiy 等，[2021](#bib.bib42)）首次应用于TS-CAM（Gao
    等，[2021](#bib.bib53)）方法中。视觉transformer（Dosovitskiy 等，[2021](#bib.bib42)）通过将输入图像拆分为带有位置嵌入的块并应用级联transformer块来构建一个token序列，从而提取视觉表示。视觉transformers可以通过自注意力机制和多层感知器自适应地学习复杂的空间变换并反映长距离语义关联。这对于定位完整对象至关重要。TS-CAM（Gao
    等，[2021](#bib.bib53)）通过利用自注意力来改进patch嵌入。此外，在每一层构建了一个类无关的映射。作者为transformers的输出配备了一个CAM模块，允许获取语义图。最终的CAM是CAM模块产生的CAM和所有层的平均类无关映射之间的聚合。这有助于提高定位精度。在CSTN方法（Meethal
    等，[2020](#bib.bib95)）中，作者用空间变换网络（STNs）（Jaderberg 等，[2015](#bib.bib71)）替代了标准卷积滤波器。除了使用多尺度定位外，这个STN模型还学习了仿射变换，可以涵盖不同的变化，包括平移、缩放和旋转，从而更好地关注不同的对象变异。
- en: Recently, a new trend has emerged in WSOL, in which *pseudo-annotations* are
    exploited. An external model or a WSOL classifier is initially trained using weak
    labels. Then, it is used to collect pseudo-labels which represent a substitute
    for the missing full supervision. They are then used to fine-tune a final model.
    This provides explicit localization guidance for training. However, such methods
    inherit a major drawback in the form of learning with inaccurate/noisy labels,
    which must be dealt with. For instance, in (Rahimi et al., [2020](#bib.bib114))
    (Pair-Sim), the authors use a fully supervised source dataset to train a proposal
    generator (Faster-RCNN (Ren et al., [2015](#bib.bib116))). Then, they apply the
    generator over a target weakly supervised dataset for the WSOL task to yield proposals
    for each sample, \ie, bag. The classical MIL framework (Carbonneau et al., [2018](#bib.bib21);
    Cheplygina et al., [2019](#bib.bib26); Wang et al., [2018](#bib.bib152); Zhou,
    [2004](#bib.bib174)) is applied by splitting the target dataset into 2 subsets
    conditioned on the class; one with positive samples with that class label, and
    the other holding negative samples. The MIL framework is solved such as to yield
    exactly one proposal per positive sample. A unary score regarding the proposal
    abjectness that is learned from the source is used, in addition to a pairwise
    score that measures the compatibility of two proposals conditioned on the bag
    class.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，WSOL 中出现了一种新趋势，即利用*伪标注*。首先，使用弱标注训练一个外部模型或 WSOL 分类器。然后，利用该模型收集伪标签，这些伪标签代表缺失的完整监督的替代品。接下来，这些伪标签被用来微调最终模型。这为训练提供了明确的定位指导。然而，这种方法继承了一个主要缺点，即使用不准确/嘈杂的标签进行学习，这必须加以处理。例如，在
    (Rahimi et al., [2020](#bib.bib114)) (Pair-Sim) 中，作者使用完全监督的源数据集来训练一个提案生成器 (Faster-RCNN
    (Ren et al., [2015](#bib.bib116)))。然后，他们将生成器应用于目标弱监督数据集，以执行 WSOL 任务，为每个样本，即包，生成提案。经典的
    MIL 框架 (Carbonneau et al., [2018](#bib.bib21); Cheplygina et al., [2019](#bib.bib26);
    Wang et al., [2018](#bib.bib152); Zhou, [2004](#bib.bib174)) 通过将目标数据集分成 2 个子集来应用，这些子集根据类别进行条件分割；一个子集包含具有该类别标签的正样本，另一个子集包含负样本。MIL
    框架的解决方案是每个正样本生成一个准确的提案。除了从源数据中学习到的关于提案物体性的单一分数外，还使用一个配对分数来测量两个提案在包类别条件下的兼容性。
- en: Authors in (Zhang et al., [2020a](#bib.bib164)) show that localization and classification
    interfere with each other in WSOL, and that these should be divided into two separate
    tasks. They first train a classifier, which is used to generate Pseudo Supervised
    Object Localization (PSOL). This pseudo-supervision is then used to train a separate
    class-agnostic localizer. In the same vein, the work in (Wei et al., [2021](#bib.bib153))
    demonstrates the benefits of shallow features for localization. The authors exploit
    low level (Shallow) features to yield Pseudo supervised Object Localization (SPOL),
    which is used to guide the training of another network. The F-CAM method (Belharbi
    et al., [2022c](#bib.bib15)) also exploits shallow features by equipping standard
    classifiers with a segmentation decoder to form a U-Net architecture (Ronneberger
    et al., [2015](#bib.bib118)). Such a model builds the final CAM through top and
    low features using skip-connections. The authors show the impact of the CAM size
    on localization performance, with a lower localization performance seen with a
    smaller CAM size. CAMs are often interpolated to have the same size as the input
    image. Since the interpolation algorithm does not take into consideration the
    image statistics, the authors propose to gradually increase the resolution of
    the CAMs via a parametric decoder. A low resolution CAM, image statistics and
    generic size priors are used to train the decoder. The authors propose a *stochastic*
    sampling of local evidence as opposed to common practice in the literature, where
    pseudo-labels are selected and fixed before training. The F-CAM method was further
    adapted for transformer-based methods (Murtaza et al., [2023](#bib.bib99), [2022](#bib.bib98))
    for WASOL in drone-surveillance, and subsequently, for WSOL in videos (Belharbi
    et al., [2023](#bib.bib16)). Following F-CAM architecture, NEGEV (Belharbi et al.,
    [2022a](#bib.bib13)) was proposed for histology data to improve localization and
    classifier interpretability. However, the authors focus mainly on using negative
    evidence collected from a pre-trained classifier, as well as evidence occurring
    naturally in datasets, \ie, fully negative samples. This allows the method to
    achieve state-of-the-art performance in localization. Additional experiments also
    show that the stochastic sampling proposed in (Belharbi et al., [2022c](#bib.bib15))
    outperforms the fixed selection of local evidence by a large margin. The Self-Produced
    Guidance method (SPG) (Zhang et al., [2018d](#bib.bib168)) extracts several attention
    maps from the top- and low-level layers to benefit from global and detailed localization.
    Discrete locations of potential ROIs are collected from the maps using thresholding,
    and are then used to train different layers in a self-supervised way. Note that
    this approach is related to SPN (Zhu et al., [2017](#bib.bib176)) and ADL (Choe
    and Shim, [2019](#bib.bib27)), which exploit attention as a self-guidance mechanism
    to steer the focus toward potential ROIs by masking feature maps. However, the
    SPG method explicitly learns a segmentation mask using discrete pixel-wise information
    collected from attention as supervision.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在(Zhang et al., [2020a](#bib.bib164))中指出，在WSOL中，定位和分类彼此干扰，因此应将这两个任务分开。首先，他们训练一个分类器，用于生成伪监督对象定位（PSOL）。然后，利用这些伪监督训练一个独立的无类别定位器。类似地，Wei
    et al. ([2021](#bib.bib153))的研究展示了浅层特征在定位中的好处。作者利用低层（浅层）特征生成伪监督对象定位（SPOL），用于指导另一个网络的训练。F-CAM方法（Belharbi
    et al., [2022c](#bib.bib15)）也利用浅层特征，通过为标准分类器配备分割解码器来形成U-Net架构（Ronneberger et al.,
    [2015](#bib.bib118)）。这样的模型通过跳跃连接利用顶部和底层特征构建最终的CAM。作者展示了CAM大小对定位性能的影响，较小的CAM大小会导致较低的定位性能。CAM通常会被插值到与输入图像相同的大小。由于插值算法没有考虑图像统计数据，作者提出通过参数解码器逐步提高CAM的分辨率。使用低分辨率CAM、图像统计数据和通用大小先验来训练解码器。作者提出了一种*随机*采样本地证据的方法，这与文献中常见的在训练前选择和固定伪标签的做法不同。F-CAM方法进一步被调整用于基于变换器的方法（Murtaza
    et al., [2023](#bib.bib99), [2022](#bib.bib98)）在无人机监控中的WASOL，随后在视频中的WSOL（Belharbi
    et al., [2023](#bib.bib16)）。继F-CAM架构之后，NEGEV（Belharbi et al., [2022a](#bib.bib13)）被提出用于组织学数据，以改善定位和分类器的可解释性。然而，作者主要关注使用从预训练分类器中收集的负面证据以及数据集中自然出现的证据，即完全负样本。这使得该方法在定位中达到最先进的性能。额外的实验还显示，在(Belharbi
    et al., [2022c](#bib.bib15))中提出的随机采样方法比固定选择本地证据具有更大的优势。Self-Produced Guidance方法（SPG）（Zhang
    et al., [2018d](#bib.bib168)）从顶层和低层提取多个注意力图，以便从全局和详细的定位中受益。通过阈值从图中收集潜在ROI的离散位置，然后以自监督的方式训练不同的层。需要注意的是，这种方法与SPN（Zhu
    et al., [2017](#bib.bib176)）和ADL（Choe and Shim, [2019](#bib.bib27)）相关，它们利用注意力作为自我引导机制，通过掩蔽特征图来引导注意力集中在潜在ROI上。然而，SPG方法明确使用从注意力中收集的离散像素信息来学习分割掩膜作为监督。
- en: 2.3 Top-down WSOL techniques
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 自上而下的弱监督目标定位技术
- en: 'This second main category is based essentially on the backward pass information
    within a network to build an attention map that localizes ROIs with respect to
    a selected target class (backward pass, [Figure 4](#S2.F4 "Figure 4 ‣ 2 A taxonomy
    of weakly-supervised object localization methods ‣ Deep Weakly-Supervised Learning
    Methods for Classification and Localization in Histology Images: A Survey") (*bottom*)).
    We distinguish three main sub-categories which differ in the way the top signal
    is back-traced. The first category exploits a secondary conductive feedback network;
    the second relies on gradient information to aggregate backbone spatial feature
    maps, while the last exploits posterior class scores for aggregation.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '第二大类主要基于网络中的反向传播信息来构建注意力图，从而定位相对于选定目标类别的感兴趣区域（反向传播，[Figure 4](#S2.F4 "Figure
    4 ‣ 2 A taxonomy of weakly-supervised object localization methods ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey")
    (*bottom*)）。我们区分了三种主要的子类别，它们在顶层信号的回溯方式上有所不同。第一类利用次级导电反馈网络；第二类依赖于梯度信息来聚合骨干空间特征图，而最后一类利用后验类别分数进行聚合。'
- en: Biologically-inspired methods. These methods are often inspired by cognitive
    science. For instance, authors in (Cao et al., [2015](#bib.bib20)) argue that
    visual attention in humans is typically dominated by a target, \ie, a ’goal’,
    in a top-down fashion. Biased competition theory (Beck and Kastner, [2009](#bib.bib10);
    Desimone, [1998](#bib.bib37); Desimone and Duncan, [1995](#bib.bib38)) explains
    that the human visual cortex is enhanced by top-down stimuli and irrelevant neurons
    are suppressed in *feedback loops* when searching for objects. The work in (Cao
    et al., [2015](#bib.bib20)) mimics this top-down loop with a *feedback network*
    that is attached to standard feedforward networks and holds binary variables in
    addition to ReLU activations (Nair and Hinton, [2010](#bib.bib103)). These binary
    variables are activated by a top-down message. Given a target class, a standard
    forward step is performed within the feedforward network to maximize the posterior
    score of the target. Then, a backward pass is achieved within the feedback network.
    To promote *selectivity* in the feedback loop (Desimone and Duncan, [1995](#bib.bib38)),
    the L[1] norm is used as a sparsity term over the binary variables of the feedback
    network. The forward/backward process is performed several times in order to optimize
    a loss function composed of the posterior score of a target class, and the sparsity
    term over the binary variables. For a localization task, the backward loop can
    reach the network input layer to yield an attention map that indicates ROIs associated
    with the target class selected at the top of the network. Despite the benefits
    of this method for CNN visualization and ROI localization, its iterative optimization
    process to obtain localization makes it less practical for WSOL tasks. The excitation-backprop (Zhang
    et al., [2018b](#bib.bib166)) method follows a similar top-down scheme. In particular,
    the authors consider a top-down Winner-Take-ALL (WTA) process (Tsotsos et al.,
    [1995](#bib.bib146)) which selects one winning path. To avoid selecting one deterministic
    path, which is less representative and leads to binary maps, the authors propose
    a *probabilistic* WTA downstream process that models all paths. This process integrates
    both bottom-up and top-down information to compute the winning probability for
    each neuron. To improve the localization accuracy of the attention map, particularly
    for images with multi-objects, the authors propose a contrastive top-down attention
    which captures the differential effect between a pair of top-down attention signals.
    This allows the attention map to hold activations only for one target class. While
    both methods (Cao et al., [2015](#bib.bib20); Zhang et al., [2018b](#bib.bib166))
    yield good results, they require substantial changes to standard CNN architectures.
    In addition, the methods are often used for interpretability and explainability
    of deep models (Samek et al., [2019](#bib.bib121)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 生物启发的方法。这些方法通常受到认知科学的启发。例如，作者在 (Cao et al., [2015](#bib.bib20)) 中认为，人类的视觉注意力通常由一个目标，即“目标”，以自上而下的方式主导。偏向竞争理论
    (Beck and Kastner, [2009](#bib.bib10); Desimone, [1998](#bib.bib37); Desimone
    and Duncan, [1995](#bib.bib38)) 解释了人类视觉皮层如何受到自上而下的刺激增强，并且在搜索物体时，相关的神经元在*反馈环路*中被抑制。在
    (Cao et al., [2015](#bib.bib20)) 的工作中，通过一个*反馈网络*模仿这种自上而下的环路，该反馈网络附加在标准的前馈网络上，并且除了
    ReLU 激活外，还包含二元变量 (Nair and Hinton, [2010](#bib.bib103))。这些二元变量由自上而下的信息激活。给定一个目标类别，在前馈网络中执行标准的前向步骤以最大化目标的后验得分。然后，在反馈网络中完成反向传递。为了在反馈环路中促进*选择性*
    (Desimone and Duncan, [1995](#bib.bib38))，使用 L[1] 范数作为反馈网络中二元变量的稀疏性项。前向/反向过程重复多次，以优化由目标类别的后验得分和二元变量的稀疏性项组成的损失函数。对于定位任务，反向环路可以到达网络输入层，生成一个表示与网络顶端选择的目标类别相关的
    ROI 的注意力图。尽管这种方法在 CNN 可视化和 ROI 定位方面有很大好处，但其迭代优化过程使得它在 WSOL 任务中不够实用。激励-反向传播 (Zhang
    et al., [2018b](#bib.bib166)) 方法遵循类似的自上而下的方案。特别是，作者考虑了自上而下的赢家通吃 (WTA) 过程 (Tsotsos
    et al., [1995](#bib.bib146))，该过程选择一个获胜路径。为了避免选择一个确定性路径，这种路径代表性较差且导致二元图，作者提出了一个*概率*
    WTA 下游过程，该过程建模所有路径。该过程整合了自下而上和自上而下的信息，以计算每个神经元的获胜概率。为了提高注意力图的定位准确性，特别是对于具有多个物体的图像，作者提出了一种对比自上而下的注意力，该注意力捕捉一对自上而下注意力信号之间的差异效应。这使得注意力图仅对一个目标类别保持激活。尽管两种方法
    (Cao et al., [2015](#bib.bib20); Zhang et al., [2018b](#bib.bib166)) 都取得了良好结果，但它们需要对标准
    CNN 架构进行 substantial 的更改。此外，这些方法通常用于深度模型的可解释性和解释性 (Samek et al., [2019](#bib.bib121))。
- en: In the next two categories, we describe how intermediate spatial feature maps
    are used to pull discriminative maps to localize ROIs associated with a fixed
    target class. Usually, an aggregation scheme via a weight-per-feature map is performed.
    The key element in these methods is how the weighting coefficients are estimated.
    All these weights are back-streamed from the per-class posterior score at the
    top of the network.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个类别中，我们描述了如何使用中间空间特征图来拉取判别图，以定位与固定目标类别相关的 ROI。通常，通过每特征图的权重汇聚方案被执行。这些方法中的关键要素是权重系数如何估计。所有这些权重都是从网络顶部的每类后验分数中反向流回来的。
- en: Grad-based aggregation. This family of methods relies on the gradient of posterior
    class scores of the true label with respect to the feature maps to determine aggregation
    weights. Such approaches are also used as *visual tools* to explain a network’s
    decision(Samek et al., [2019](#bib.bib121)). In (Selvaraju et al., [2017](#bib.bib124)),
    the authors propose the Grad-CAM method. In order to compute the CAMs, they propose
    to *aggregate* spatial feature maps using gradient coefficients. The coefficient
    of each feature map is computed using the gradient of the score of the selected
    target class with respect to that map. This gradient indicates how much a pixel
    location contributes to the target output. The CAM for the class $c$ is computed
    as,
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的聚合。这类方法依赖于真实标签的后验类别分数相对于特征图的梯度来确定聚合权重。这些方法也被用作*可视化工具*来解释网络的决策（Samek 等，
    [2019](#bib.bib121)）。在（Selvaraju 等， [2017](#bib.bib124)）中，作者提出了 Grad-CAM 方法。为了计算
    CAMs，他们提出使用梯度系数来*汇聚*空间特征图。每个特征图的系数是通过选定目标类别的分数相对于该图的梯度计算的。该梯度表示像素位置对目标输出的贡献程度。类别
    $c$ 的 CAM 计算为，
- en: '|  | $\displaystyle\bm{M}_{c}$ | $\displaystyle=\relu\Big{(}\sum\limits_{k=1}^{K}\bm{A}_{c,k}\;\bm{F}_{k}\Big{)}\;,$
    |  | (10) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{M}_{c}$ | $\displaystyle=\relu\Big{(}\sum\limits_{k=1}^{K}\bm{A}_{c,k}\;\bm{F}_{k}\Big{)}\;,$
    |  | (10) |'
- en: '|  | $\displaystyle\text{where}\quad\bm{A}_{c,k}$ | $\displaystyle=\frac{1}{H\,W}\sum\limits_{i=1,j=1}^{H,W}\frac{\partial\bm{s}_{c}}{\partial\bm{F}_{k,i,j}}\;,$
    |  | (11) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{where}\quad\bm{A}_{c,k}$ | $\displaystyle=\frac{1}{H\,W}\sum\limits_{i=1,j=1}^{H,W}\frac{\partial\bm{s}_{c}}{\partial\bm{F}_{k,i,j}}\;,$
    |  | (11) |'
- en: 'where ${\bm{s}_{c}}$ is the score for the class $c$. This approach is a generalization
    of the CAM method (Zhou et al., [2016](#bib.bib172)), where the derivative of
    the score with respect to the feature map is used instead of learned weights.
    This approach was improved in Grad-CAM++ (Chattopadhyay et al., [2018](#bib.bib22))
    and Smooth-Grad-CAM++ (Omeiza et al., [2019](#bib.bib104)) to obtain better localization
    by covering a complete object, as well as explaining the occurrence of multiple
    instances in a single image. In (Fu et al., [2020](#bib.bib52)), the authors propose
    theoretical grounds for CAM-based methods, in particular Grad-CAM, for a more
    accurate visualization. They include two important axioms: sensitivity and conservation,
    which determine how to better compute the importance weight of each feature map.
    Following these axioms, the authors propose a new gradient-based method, XGrad-CAM,
    which computes the coefficient differently. The coefficients are the solution
    to an optimization problem composed of the two axioms. To date, these methods
    have aggregated the feature maps at the last layer. LayerCAM method (Jiang et al.,
    [2021](#bib.bib74)) exploits top- and low-level layers to extract localization.
    Top layers are commonly known to hold coarse localization, while low-level layers
    hold detailed but noisy localization. This method extracts CAMs from each layer
    using a back-propagated gradient signal. Then, the final CAM is computed by fusing
    all CAMs estimated at each layer. Note that this family of methods is designed
    to *interrogate* a pre-trained model. While they are model-independent, and allow
    inspecting the decision of a trained model, the user cannot control their behavior
    during the training of the model. This ties the localization performance of these
    methods strongly to the localization information provided by the trained model.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\bm{s}_{c}}$ 是类别 $c$ 的分数。这种方法是 CAM 方法（Zhou 等，[2016](#bib.bib172)）的一种推广，其中使用相对于特征图的分数的导数，而不是学习到的权重。这种方法在
    Grad-CAM++（Chattopadhyay 等，[2018](#bib.bib22)）和 Smooth-Grad-CAM++（Omeiza 等，[2019](#bib.bib104)）中得到了改进，以通过覆盖整个对象来获得更好的定位，并解释单张图像中多个实例的出现。在（Fu
    等，[2020](#bib.bib52)）中，作者提出了 CAM 基于方法的理论基础，特别是 Grad-CAM，以实现更准确的可视化。他们包括两个重要的公理：敏感性和保守性，这决定了如何更好地计算每个特征图的重要性权重。根据这些公理，作者提出了一种新的基于梯度的方法
    XGrad-CAM，它以不同的方式计算系数。这些系数是由两个公理组成的优化问题的解。迄今为止，这些方法已经聚合了最后一层的特征图。LayerCAM 方法（Jiang
    等，[2021](#bib.bib74)）利用高层和低层来提取定位。高层通常被认为具有粗略定位，而低层则具有详细但噪声较大的定位。这种方法从每一层提取 CAM，使用反向传播的梯度信号。然后，通过融合在每一层估计的所有
    CAM 来计算最终的 CAM。请注意，这类方法旨在*审问*预训练模型。虽然它们与模型无关，并允许检查训练模型的决策，但用户无法在模型训练期间控制其行为。这使得这些方法的定位性能与训练模型提供的定位信息紧密相关。
- en: Confidence-based aggregation. Several methods exploit raw scores of the target
    class, instead of the gradient, in order to *aggregate* spatial features of a
    backbone. In the Score-CAM method (Wang et al., [2020](#bib.bib151)), each feature
    map is element-wise multiplied with the input image, and is then used to compute
    the target-class score. This allows one to obtain a posterior score for each spatial
    map, which is then compared to the score of the raw image. The difference between
    both scores yields Channel-wise Increase of Confidence (CIC), in which where high
    values indicate the presence of a strong ROI in the feature map. The final CAM
    is a linear weighted sum of all the feature maps, followed by ReLU non-linearity (Nair
    and Hinton, [2010](#bib.bib103)) where CIC coefficients are used as weights,
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 基于置信度的聚合。一些方法利用目标类别的原始分数，而不是梯度，来*聚合*骨干网的空间特征。在 Score-CAM 方法（Wang 等，[2020](#bib.bib151)）中，每个特征图与输入图像逐元素相乘，然后用于计算目标类别分数。这使得可以获得每个空间图的后验分数，然后将其与原始图像的分数进行比较。两个分数之间的差异产生了通道-wise
    置信度增加（CIC），其中高值表示特征图中存在强 ROI。最终的 CAM 是所有特征图的线性加权和，然后是 ReLU 非线性（Nair 和 Hinton，[2010](#bib.bib103)），其中
    CIC 系数被用作权重。
- en: '|  | $\bm{M}^{c}_{Score-CAM}=ReLU\Big{(}\sum_{k}\alpha^{c}_{k}\bm{F}_{k}^{l}\Big{)}\;,$
    |  | (12) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{M}^{c}_{Score-CAM}=ReLU\Big{(}\sum_{k}\alpha^{c}_{k}\bm{F}_{k}^{l}\Big{)}\;,$
    |  | (12) |'
- en: where ${\alpha^{c}_{k}}$ is the CIC of class $c$ for feature map $k$, and $l$
    is a layer. Smoothed-Score-CAM (SS-CAM) (Naidu and Michael, [2020](#bib.bib100))
    improves the Score-CAM (Wang et al., [2020](#bib.bib151)). Instead of computing
    the CIC over a single masked image, SS-CAM averages many perturbed masked images.
    The authors propose either to perturbate the feature map or the input image. This
    yields smoothed aggregation weights. The IS-CAM method (Naidu et al., [2020](#bib.bib101))
    performs a similar process to smooth weights. While these methods yield good localization,
    a recent empirical evaluation showed that they are computationally expensive (Belharbi
    et al., [2022c](#bib.bib15)). For instance, computing a single CAM for an image
    of size ${224\times 224}$ over the ResNet50 (He et al., [2016](#bib.bib64)) model
    takes a few minutes on a decent GPU. This makes training such methods impractical.
    In the Ablation-CAM method (Desai and Ramaswamy, [2020](#bib.bib36)), the authors
    consider a gradient-free method to avoid using gradients due to unreliability
    stemming from gradient saturation (Adebayo et al., [2018](#bib.bib1); Kindermans
    et al., [2019](#bib.bib83)). The importance coefficient of a feature map is computed
    as a slope that measures the difference between the posterior class score and
    the score obtained when turning off that feature map. That difference is then
    normalized.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\alpha^{c}_{k}}$ 是特征图 $k$ 对于类别 $c$ 的CIC（类别重要性系数），$l$ 是一层。平滑分数CAM（SS-CAM）（Naidu和Michael，[2020](#bib.bib100)）改进了分数CAM（Score-CAM）（Wang等，[2020](#bib.bib151)）。SS-CAM不是在单一的掩蔽图像上计算CIC，而是对许多扰动的掩蔽图像进行平均。作者提出可以扰动特征图或输入图像。这会产生平滑的聚合权重。IS-CAM方法（Naidu等，[2020](#bib.bib101)）执行类似的过程以平滑权重。虽然这些方法能够提供良好的定位，但最近的实证评估表明，它们在计算上是昂贵的（Belharbi等，[2022c](#bib.bib15)）。例如，在ResNet50（He等，[2016](#bib.bib64)）模型上计算一张大小为
    ${224\times 224}$ 的图像的单个CAM需要几分钟时间，这在较好的GPU上也是如此。这使得训练这些方法变得不切实际。在Ablation-CAM方法（Desai和Ramaswamy，[2020](#bib.bib36)）中，作者考虑了一种无梯度的方法，以避免使用由于梯度饱和引起的不可靠性（Adebayo等，[2018](#bib.bib1)；Kindermans等，[2019](#bib.bib83)）。特征图的重要性系数被计算为一个斜率，该斜率测量后验类别得分与关闭该特征图时获得的得分之间的差异。然后对该差异进行归一化。
- en: 2.4 Critical Analysis
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 关键分析
- en: Our review of several works on WSOL carried out in recent years showed the emergence
    of different strategies for WSOL tasks. We cite two main families. Top-down methods,
    which aim to back-trace the posterior probability class to find ROIs. These methods
    rely either on biology processes, classifier confidence, or gradients. Gradient-based
    methods, which are the more dominant. They are model-independent and can be used
    for any trained network, in addition to being easy to implement. This family of
    methods are also used for CNN visualization, and interpretability (Samek et al.,
    [2019](#bib.bib121)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对近年来几项WSOL（弱监督对象定位）工作的回顾显示，WSOL任务中出现了不同的策略。我们将这些策略分为两大类。自上而下的方法，旨在通过追踪后验概率类别来找到感兴趣区域（ROIs）。这些方法依赖于生物过程、分类器置信度或梯度。基于梯度的方法是更为主流的。这些方法与模型无关，可以用于任何经过训练的网络，并且实现起来比较容易。这类方法也被用于CNN的可视化和解释性（Samek等，[2019](#bib.bib121)）。
- en: 'While top-down methods have been successful, they have experienced slower progress
    than bottom-up methods, which seem to be the driving core of WSOL. Most successful
    WSOL methods derive from this family. They are easy to implement and follow the
    standard flow of information in feed-forward networks. Early works aimed to design
    different spatial pooling functions to obtain CAMs, but these methods quickly
    hit a fundamental snag in CAMs, in the form of under-activation. This also suggests
    that relying only on spatial pooling to transfer global labels to the pixel-level
    is not enough. Subsequent works have focused on this issue mainly by attempting
    to recover the localization of a complete object. To that end, several strategies
    have been proposed:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自上而下的方法取得了一些成功，但其进展比自下而上的方法要慢，而自下而上的方法似乎是WSOL的核心驱动力。大多数成功的WSOL方法都源于这一类别。它们实现起来较为简单，并且遵循了前馈网络的信息流标准。早期的工作旨在设计不同的空间池化函数来获取CAMs（类激活图），但这些方法很快遇到了CAMs的一个根本问题，即激活不足。这也表明，仅仅依靠空间池化来将全局标签传递到像素级别是不够的。随后的一些工作主要集中在通过尝试恢复完整物体的定位来解决这个问题。为此，提出了几种策略：
- en: '- Perturbation of input images or embeddings, \ie, intermediate features. It
    is often used to mine discriminative regions. The most common perturbation mechanism
    is suppression, in which a part of the signal is deleted stochastically either
    uniformly or via selective attention.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对输入图像或嵌入，\ie，即中间特征的扰动。这种方法常用于挖掘区分性区域。最常见的扰动机制是抑制，其中信号的一部分以随机方式删除，既可以均匀删除，也可以通过选择性注意力删除。'
- en: '- Self-attention and self-learning. Training CNNs to localize objects using
    only global labels is an ill-posed problem (Wan et al., [2018](#bib.bib149)).
    However, thanks to the convolutional filter properties, common patterns can emerge
    within intermediate spatial feature maps. Researchers exploit this property to
    collect self-attention maps, which often focus on objects. This self-attention
    has been successfully used to *guide* intermediate convolution layers to further
    focus on emerging ROI and filter out background and noisy features. In addition,
    most confident regions in a self-attention map have been used as self-supervisory
    signals at the pixel level.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '- 自注意力和自学习。仅使用全局标签训练CNN以定位物体是一个不适定问题（Wan et al., [2018](#bib.bib149)）。然而，得益于卷积滤波器的特性，常见模式可以在中间空间特征图中出现。研究人员利用这一特性收集自注意力图，这些图通常集中在物体上。这种自注意力已经成功地*引导*中间卷积层进一步关注新兴的ROI，并过滤背景和噪声特征。此外，自注意力图中最可信的区域已被用作像素级的自监督信号。'
- en: '- Shallow features have long been known to hold useful but noisy localization
    information in supervised learning, such as in segmentation (Ronneberger et al.,
    [2015](#bib.bib118)). It was not until recently, however, that shallow features
    began to be exploited as well in WSOL tasks, allowing further boost the localization
    performance.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '- 浅层特征长期以来被认为在监督学习中包含有用但嘈杂的定位信息，如在分割任务中（Ronneberger et al., [2015](#bib.bib118)）。然而，直到最近，浅层特征才开始在WSOL任务中被利用，从而进一步提升定位性能。'
- en: '- Pseudo-annotation provides a substitute for full supervision. Using only
    global labels for localization been somewhat very successful. They allow the identification
    of the most discriminative regions but are unable to recover full objects. Partial,
    noisy, and uncertain pseudo-supervision is currently deemed to be very useful
    to boost localization performance. It provides low-cost supervision, and yet,
    should be used with care since it could be very noisy, which could push the model
    in the wrong direction or trap it in local solutions that predict similar pseudo-annotations.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '- 伪标注提供了完整监督的替代方案。仅使用全局标签进行定位已经取得了一些成功。它们可以识别出最具区分性的区域，但无法恢复完整的物体。部分、噪声和不确定的伪监督目前被认为非常有助于提升定位性能。它提供了低成本的监督，但仍应谨慎使用，因为它可能非常嘈杂，这可能会将模型推向错误的方向或陷入预测类似伪标注的局部解决方案。'
- en: '- Decoupling classification and localization tasks. Recent studies in WSOL
    have shown that these tasks are antagonistic: localization task converges during
    the very early epochs, and then later degrades, while the classification task
    converges toward the end of the training. Some works separate them by first training
    a classifier, then a localizer. The aim is to build a final framework that can
    yield the best performance over both tasks. Note that recent successful works
    on WSOL tasks combine shallow features with pseudo-annotations while separating
    classification and localization tasks.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '- 解耦分类和定位任务。最近在WSOL中的研究表明，这些任务是对立的：定位任务在非常早期的阶段收敛，然后在后期退化，而分类任务则在训练结束时收敛。一些工作通过首先训练分类器，然后训练定位器来分离它们。其目的是建立一个最终框架，以在这两个任务上都能获得最佳性能。请注意，最近在WSOL任务上取得成功的工作将浅层特征与伪标注相结合，同时分离分类和定位任务。'
- en: 3 Experimental methodology
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验方法
- en: In this section, we present the experimental procedure used to evaluate the
    performance of deep WSOL models. The aim of our experiments was to assess and
    compare their ability to accurately classify histology images and localize cancerous
    ROIs.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们展示了用于评估深度WSOL模型性能的实验过程。我们实验的目的是评估和比较它们准确分类组织图像和定位癌症ROI的能力。
- en: 'In order to compare the localization performance of WSOL techniques on histology
    data, we selected different representative methods from both categories (bottom-up
    and top-down). From the *bottom-up* category, we consider the following methods:
    GAP (Lin et al., [2013](#bib.bib90)), MAX-Pool (Oquab et al., [2015](#bib.bib105)),
    LSE (Sun et al., [2016](#bib.bib138)), CAM (Zhou et al., [2016](#bib.bib172)),
    HaS (Singh and Lee, [2017](#bib.bib128)), WILDCAT (Durand et al., [2017](#bib.bib45)),
    ACoL (Zhang et al., [2018c](#bib.bib167)), SPG (Zhang et al., [2018d](#bib.bib168)),
    Deep MIL (Ilse et al., [2018](#bib.bib69)), PRM (Zhou et al., [2018](#bib.bib173)),
    ADL (Choe and Shim, [2019](#bib.bib27)), CutMix (Yun et al., [2019](#bib.bib161)),
    TS-CAM (Gao et al., [2021](#bib.bib53)), MAXMIN (Belharbi et al., [2022b](#bib.bib14)),
    NEGEV (Belharbi et al., [2022a](#bib.bib13)); while the following methods are
    considered from the *top-down* category: GradCAM (Selvaraju et al., [2017](#bib.bib124)),
    GradCAM++ (Chattopadhyay et al., [2018](#bib.bib22)), Smooth-GradCAM++ (Omeiza
    et al., [2019](#bib.bib104)), XGradCAM (Fu et al., [2020](#bib.bib52)), LayerCAM (Jiang
    et al., [2021](#bib.bib74)).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较 WSOL 技术在组织学数据上的定位性能，我们从两类方法（自下而上和自上而下）中选择了不同的代表性方法。来自*自下而上*类别的方法包括：GAP (Lin
    et al., [2013](#bib.bib90))，MAX-Pool (Oquab et al., [2015](#bib.bib105))，LSE (Sun
    et al., [2016](#bib.bib138))，CAM (Zhou et al., [2016](#bib.bib172))，HaS (Singh
    and Lee, [2017](#bib.bib128))，WILDCAT (Durand et al., [2017](#bib.bib45))，ACoL (Zhang
    et al., [2018c](#bib.bib167))，SPG (Zhang et al., [2018d](#bib.bib168))，Deep MIL (Ilse
    et al., [2018](#bib.bib69))，PRM (Zhou et al., [2018](#bib.bib173))，ADL (Choe and
    Shim, [2019](#bib.bib27))，CutMix (Yun et al., [2019](#bib.bib161))，TS-CAM (Gao
    et al., [2021](#bib.bib53))，MAXMIN (Belharbi et al., [2022b](#bib.bib14))，NEGEV (Belharbi
    et al., [2022a](#bib.bib13))；而来自*自上而下*类别的方法包括：GradCAM (Selvaraju et al., [2017](#bib.bib124))，GradCAM++ (Chattopadhyay
    et al., [2018](#bib.bib22))，Smooth-GradCAM++ (Omeiza et al., [2019](#bib.bib104))，XGradCAM (Fu
    et al., [2020](#bib.bib52))，LayerCAM (Jiang et al., [2021](#bib.bib74))。
- en: 'Experiments are conducted on two public datasets of histology images, described
    in [subsection 3.3](#S3.SS3 "3.3 Datasets ‣ 3 Experimental methodology ‣ Deep
    Weakly-Supervised Learning Methods for Classification and Localization in Histology
    Images: A Survey"). Most of the public datasets used are collected exclusively
    for classification or segmentation purposes (Daisuke and Shumpei, [2018](#bib.bib34)),
    these include the BreaKHis (Spanhol et al., [2016b](#bib.bib132)) and BACH (Aresta
    et al., [2018](#bib.bib3)) datasets. The only dataset we found that contained
    both image-level and pixel-level annotations was GlaS ([subsubsection 3.3.1](#S3.SS3.SSS1
    "3.3.1 GlaS dataset (GlaS) ‣ 3.3 Datasets ‣ 3 Experimental methodology ‣ Deep
    Weakly-Supervised Learning Methods for Classification and Localization in Histology
    Images: A Survey")). Using a single dataset for evaluation could be insufficient
    to draw meaningful conclusions. Therefore, we created an additional dataset with
    the required annotations by using a protocol to sample image patches from WSIs
    of the CAMELYON16 dataset ([subsubsection 3.3.2](#S3.SS3.SSS2 "3.3.2 Camelyon16
    dataset (CAMELYON16) ‣ 3.3 Datasets ‣ 3 Experimental methodology ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey")).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 实验在两个公开的组织学图像数据集上进行，详见 [3.3 节](#S3.SS3 "3.3 数据集 ‣ 3 实验方法 ‣ 组织学图像中的深度弱监督学习方法：综述")。大多数使用的公开数据集是专门为分类或分割目的收集的 (Daisuke
    and Shumpei, [2018](#bib.bib34))，包括 BreaKHis (Spanhol et al., [2016b](#bib.bib132))
    和 BACH (Aresta et al., [2018](#bib.bib3)) 数据集。我们找到的唯一包含图像级和像素级注释的数据集是 GlaS ([3.3.1
    节](#S3.SS3.SSS1 "3.3.1 GlaS 数据集 (GlaS) ‣ 3.3 数据集 ‣ 3 实验方法 ‣ 组织学图像中的深度弱监督学习方法：综述"))。仅使用一个数据集进行评估可能不足以得出有意义的结论。因此，我们通过使用一种协议从
    CAMELYON16 数据集的 WSI 中采样图像块创建了一个附加的数据集，附带所需的注释 ([3.3.2 节](#S3.SS3.SSS2 "3.3.2 Camelyon16
    数据集 (CAMELYON16) ‣ 3.3 数据集 ‣ 3 实验方法 ‣ 组织学图像中的深度弱监督学习方法：综述"))。
- en: 3.1 Protocol
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 协议
- en: In all our experiments, we follow the same experimental protocol as found in (Choe
    et al., [2020](#bib.bib28)) which defines a clear setup to evaluate ROI localization
    obtained by a weakly supervised classifier. The protocol includes two main elements,
    namely, *model selection*, and an *evaluation metric* at the pixel level.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有实验中，我们遵循了与 (Choe et al., [2020](#bib.bib28))中找到的相同实验协议，该协议定义了评估弱监督分类器获得的
    ROI 定位的明确设置。该协议包括两个主要要素，即*模型选择*和像素级*评估指标*。
- en: 'In a weakly supervised setup, model selection is critical. The learning scenario
    considered in our experiments entails two main tasks: Classification and localization,
    which are shown to be antagonistic tasks (Belharbi et al., [2022c](#bib.bib15);
    Choe et al., [2020](#bib.bib28)). While the localization task converges during
    the very early training epochs, the classification task converges at late epochs.
    Therefore, to yield a better localization model, an adequate model selection protocol
    is required. Following (Choe et al., [2020](#bib.bib28)), and considering a full
    validation set labeled only at the global level, we randomly select a few samples
    to be labeled additionally at the pixel level. In particular, we select a few
    samples per class to yield a balanced set. These samples are used for model selection
    using localization measures. This selection is referred to as a B-LOC selection.
    Model selection using a full validation set employing only classification measures
    with global labels is referred to as B-CL selection. We provide results on both
    selection methods to assess their impact on performance. All results are reported
    using B-LOC selection unless specified otherwise. In the next section, we present
    the evaluation metrics.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在弱监督设置中，模型选择至关重要。我们实验中考虑的学习场景包括两个主要任务：分类和定位，这两个任务被证明是对立的任务 (Belharbi et al.,
    [2022c](#bib.bib15); Choe et al., [2020](#bib.bib28))。虽然定位任务在训练初期就会收敛，但分类任务则在后期才会收敛。因此，为了获得更好的定位模型，需要一个适当的模型选择协议。根据
    (Choe et al., [2020](#bib.bib28))，并考虑到仅在全局层面标记的完整验证集，我们随机选择了一些样本以在像素级别上进行额外标记。具体而言，我们为每个类别选择了一些样本，以生成一个平衡的集合。这些样本用于通过定位指标进行模型选择。此选择称为
    B-LOC 选择。使用仅包含全局标签的完整验证集进行模型选择并仅使用分类指标称为 B-CL 选择。我们提供了这两种选择方法的结果，以评估其对性能的影响。除非另有说明，否则所有结果均使用
    B-LOC 选择。在下一节中，我们将介绍评估指标。
- en: 3.2 Performance measures
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 性能指标
- en: For each task, \ie, classification and localization, we consider their respective
    metric.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个任务，即分类和定位，我们考虑各自的指标。
- en: 3.2.1 Classification task
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 分类任务
- en: We use a standard classification accuracy CL,
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用标准的分类准确率 CL，
- en: '|  | $\texttt{CL}=100\times\;\frac{\text{\#correctly classified samples}}{\text{\#samples}}\;(\%)\;,$
    |  | (13) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $\texttt{CL}=100\times\;\frac{\text{\#正确分类样本数}}{\text{\#样本总数}}\;(\%)\;,$
    |  | (13) |'
- en: where *#correctly classified samples* is the total number of correctly classified
    samples, and *#samples* is the total number of samples.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *#正确分类样本数* 是正确分类样本的总数，而 *#样本总数* 是样本的总数。
- en: 3.2.2 Localization task
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 定位任务
- en: 'The aim of WSOL is to produce a score map that is used to localize an object.
    In order to measure the quality of localization of ROIs, we consider the same
    protocol used in (Choe et al., [2020](#bib.bib28)). Using the class activation
    map ${\bm{S}}$ of a target class, a binary map is obtained through thresholding.
    At pixel location ${(i,j)}$, this map is compared to the ground true mask ${\bm{T}}$.
    Following (Choe et al., [2020](#bib.bib28)), we threshold the score map at $\tau$
    to generate the binary mask $\{(i,j)\mid s_{ij}\geq\tau\}$. We consider the following
    localization metrics:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: WSOL 的目标是生成一个用于定位物体的评分图。为了测量 ROI 的定位质量，我们考虑使用 (Choe et al., [2020](#bib.bib28))
    中使用的相同协议。使用目标类别的类别激活图 ${\bm{S}}$，通过阈值化获得二值图。在像素位置 ${(i,j)}$，将此图与真实掩模 ${\bm{T}}$
    进行比较。根据 (Choe et al., [2020](#bib.bib28))，我们在 $\tau$ 处对评分图进行阈值处理，以生成二值掩模 $\{(i,j)\mid
    s_{ij}\geq\tau\}$。我们考虑以下定位指标：
- en: 'PxAP: We use the PxAP metric, presented in (Choe et al., [2020](#bib.bib28)),
    which measures the pixel-wise precision-recall. At a specific threshold ${\tau}$,
    the pixel precision and recall are defined as,'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: PxAP：我们使用 PxAP 指标，该指标在 (Choe et al., [2020](#bib.bib28)) 中提出，用于测量逐像素的精准率和召回率。在特定的阈值
    ${\tau}$ 下，像素精准率和召回率定义为，
- en: '|  | $\displaystyle\text{{PxPrec}}(\tau)=\frac{&#124;\{s^{(n)}_{ij}\geq\tau\}\cap\{T^{(n)}_{ij}=1\}&#124;}{&#124;\{s^{(n)}_{ij}\geq\tau\}&#124;}\;,$
    |  | (14) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{{PxPrec}}(\tau)=\frac{&#124;\{s^{(n)}_{ij}\geq\tau\}\cap\{T^{(n)}_{ij}=1\}&#124;}{&#124;\{s^{(n)}_{ij}\geq\tau\}&#124;}\;,$
    |  | (14) |'
- en: '|  | $\displaystyle\text{{PxRec}}(\tau)=\frac{&#124;\{s^{(n)}_{ij}\geq\tau\}\cap\{T^{(n)}_{ij}=1\}&#124;}{&#124;\{T^{(n)}_{ij}=1\}&#124;}\;.$
    |  | (15) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{{PxRec}}(\tau)=\frac{&#124;\{s^{(n)}_{ij}\geq\tau\}\cap\{T^{(n)}_{ij}=1\}&#124;}{&#124;\{T^{(n)}_{ij}=1\}&#124;}\;.$
    |  | (15) |'
- en: The PxAP metric marginalizes the threshold ${\tau}$ over a predefined set of
    thresholds³³3In all the experiments, we used ${\tau}\in[0,1]$ with a step of ${0.001}$
    as in (Choe et al., [2020](#bib.bib28)).,
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: PxAP 指标对预定义的阈值集合³³3 在所有实验中，我们使用了 ${\tau}\in[0,1]$，步长为 ${0.001}$，如 (Choe et al.,
    [2020](#bib.bib28))中所示。
- en: '|  | $\text{{PxAP}}:=\sum_{l}\text{{PxPrec}}(\tau_{l})\times(\text{{PxRec}}(\tau_{l})-\text{{PxRec}}(\tau_{l-1}))\;,$
    |  | (16) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{{PxAP}}:=\sum_{l}\text{{PxPrec}}(\tau_{l})\times(\text{{PxRec}}(\tau_{l})-\text{{PxRec}}(\tau_{l-1}))\;,$
    |  | (16) |'
- en: which is the area under the curve of the pixel precision-recall curve.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 即像素精度-召回曲线下的面积。
- en: 'Confusion Matrix: Since we are dealing with a medical application, it is important
    to assess true positives/negatives and false positives/negatives performance at
    the pixel level in order to have real insights into localization accuracy. Such
    information is not explicitly provided via the PxAP metric. Therefore, we consider
    measuring the confusion matrix by marginalizing the threshold ${\tau}$ similarly
    to what is done in the PxAP metric. First, we compute each normalized component
    of the confusion matrix with respect to a fixed threshold as follows,'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵：由于我们处理的是医疗应用，因此在像素级别评估真正例/假负例和假正例/真负例的表现非常重要，以便对定位准确性有真实的洞察。这些信息并未通过 PxAP
    指标显式提供。因此，我们考虑通过对阈值 ${\tau}$ 进行边际化来测量混淆矩阵，这与 PxAP 指标中的做法类似。首先，我们根据固定阈值计算混淆矩阵的每个归一化组件，如下所示，
- en: '|  | $\displaystyle\text{{TP}}(\tau)=\frac{&#124;\{s^{(n)}_{ij}\geq\tau\}\cap\{T^{(n)}_{ij}=1\}&#124;}{&#124;\{T^{(n)}_{ij}=1\}&#124;}\;,$
    |  | (17) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{{TP}}(\tau)=\frac{&#124;\{s^{(n)}_{ij}\geq\tau\}\cap\{T^{(n)}_{ij}=1\}&#124;}{&#124;\{T^{(n)}_{ij}=1\}&#124;}\;,$
    |  | (17) |'
- en: '|  | $\displaystyle\text{{FN}}(\tau)=\frac{&#124;\{s^{(n)}_{ij}<\tau\}\cap\{T^{(n)}_{ij}=1\}&#124;}{&#124;\{T^{(n)}_{ij}=1\}&#124;}\;,$
    |  | (18) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{{FN}}(\tau)=\frac{&#124;\{s^{(n)}_{ij}<\tau\}\cap\{T^{(n)}_{ij}=1\}&#124;}{&#124;\{T^{(n)}_{ij}=1\}&#124;}\;,$
    |  | (18) |'
- en: '|  | $\displaystyle\text{{FP}}(\tau)=\frac{&#124;\{s^{(n)}_{ij}\geq\tau\}\cap\{T^{(n)}_{ij}=0\}&#124;}{&#124;\{T^{(n)}_{ij}=0\}&#124;}\;,$
    |  | (19) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{{FP}}(\tau)=\frac{&#124;\{s^{(n)}_{ij}\geq\tau\}\cap\{T^{(n)}_{ij}=0\}&#124;}{&#124;\{T^{(n)}_{ij}=0\}&#124;}\;,$
    |  | (19) |'
- en: '|  | $\displaystyle\text{{TN}}(\tau)=\frac{&#124;\{s^{(n)}_{ij}<\tau\}\cap\{T^{(n)}_{ij}=0\}&#124;}{&#124;\{T^{(n)}_{ij}=0\}&#124;}\;,$
    |  | (20) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{{TN}}(\tau)=\frac{&#124;\{s^{(n)}_{ij}<\tau\}\cap\{T^{(n)}_{ij}=0\}&#124;}{&#124;\{T^{(n)}_{ij}=0\}&#124;}\;,$
    |  | (20) |'
- en: where ${\texttt{TP},\texttt{FN},\texttt{FP},\texttt{TN}}$ are the true positives,
    false negatives, false positives, and true negatives, respectively. Each component
    can be represented as a graph with the x-axis as the threshold ${\tau}$. Similarly
    to PxAP, we marginalize confusion matrix components over ${\tau}$ by measuring
    the area under each component, which is also the average since the step between
    thresholds is fixed. We report the percentage values of each component,
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\texttt{TP},\texttt{FN},\texttt{FP},\texttt{TN}}$ 分别为真正例、假负例、假正例和真负例。每个组件可以表示为以阈值
    ${\tau}$ 为 x 轴的图形。类似于 PxAP，我们通过测量每个组件下的面积来对混淆矩阵组件进行 ${\tau}$ 边际化，由于阈值之间的步长是固定的，这也就是平均值。我们报告每个组件的百分比值，
- en: '|  | $\displaystyle\text{{TP*}}:=100\times\sum_{l}\text{{TP}}(\tau_{l})\times(\tau_{l}-\tau_{l-1})\;,$
    |  | (21) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{{TP*}}:=100\times\sum_{l}\text{{TP}}(\tau_{l})\times(\tau_{l}-\tau_{l-1})\;,$
    |  | (21) |'
- en: '|  | $\displaystyle\text{{FN*}}:=100\times\sum_{l}\text{{FN}}(\tau_{l})\times(\tau_{l}-\tau_{l-1})\;,$
    |  | (22) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{{FN*}}:=100\times\sum_{l}\text{{FN}}(\tau_{l})\times(\tau_{l}-\tau_{l-1})\;,$
    |  | (22) |'
- en: '|  | $\displaystyle\text{{FP*}}:=100\times\sum_{l}\text{{FP}}(\tau_{l})\times(\tau_{l}-\tau_{l-1})\;,$
    |  | (23) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{{FP*}}:=100\times\sum_{l}\text{{FP}}(\tau_{l})\times(\tau_{l}-\tau_{l-1})\;,$
    |  | (23) |'
- en: '|  | $\displaystyle\text{{TN*}}:=100\times\sum_{l}\text{{TN}}(\tau_{l})\times(\tau_{l}-\tau_{l-1})\;.$
    |  | (24) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{{TN*}}:=100\times\sum_{l}\text{{TN}}(\tau_{l})\times(\tau_{l}-\tau_{l-1})\;.$
    |  | (24) |'
- en: 3.3 Datasets
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 数据集
- en: 'In this section, we describe the two public datasets of histology images used
    in our experiments: GlaS for colon cancer, and CAMELYON16 for breast cancer.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了实验中使用的两个公共组织学图像数据集：用于结肠癌的 GlaS 数据集，以及用于乳腺癌的 CAMELYON16 数据集。
- en: '![Refer to caption](img/5b30a45dd443261eb8d27c353bd752fa.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/5b30a45dd443261eb8d27c353bd752fa.png)'
- en: 'Figure 5: Example of images of different classes with their segmentations from
    GlaS dataset (Credit: (Sirinukunwattana et al., [2017](#bib.bib129))). *Row 1*:
    Benign. *Row 2*: Malignant.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：来自 GlaS 数据集的不同类别图像及其分割示例（致谢： (Sirinukunwattana et al., [2017](#bib.bib129)))。*第
    1 行*：良性。*第 2 行*：恶性。
- en: 3.3.1 GlaS dataset (GlaS)
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 GlaS 数据集 (GlaS)
- en: 'This is a histology dataset for colon cancer diagnosis (Sirinukunwattana et al.,
    [2017](#bib.bib129))⁴⁴4The Gland Segmentation in Colon Histology Contest: [https://warwick.ac.uk/fac/sci/dcs/research/tia/glascontest](https://warwick.ac.uk/fac/sci/dcs/research/tia/glascontest).
    It contains 165 images from 16 Hematoxylin and Eosin (H&E) histology sections
    and their corresponding labels. For each image, both pixel-level and image-level
    annotations for cancer grading (\ie, benign or malign) are provided. The whole
    dataset is split into training (67 samples), validation (18 samples), and testing
    (80 samples) subsets. Among the validation set, 3 samples per class are selected
    to be fully supervised, \ie, 6 samples in total for B-LOC selection.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于结肠癌诊断的组织学数据集（Sirinukunwattana 等人，[2017](#bib.bib129)）⁴⁴4结肠组织学分割竞赛：[https://warwick.ac.uk/fac/sci/dcs/research/tia/glascontest](https://warwick.ac.uk/fac/sci/dcs/research/tia/glascontest)。它包含了来自16个苏木精-伊红（H&E）组织学切片的165张图像及其相应的标签。每张图像提供了癌症分级（即良性或恶性）的像素级和图像级标注。整个数据集分为训练集（67个样本）、验证集（18个样本）和测试集（80个样本）。在验证集中，每类选择了3个样本进行完全监督，即B-LOC选择的总样本数为6个。
- en: 3.3.2 Camelyon16 dataset (CAMELYON16)
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 Camelyon16 数据集（CAMELYON16）
- en: 'This dataset⁵⁵5The Cancer Metastases in Lymph Nodes Challenge 2016 (CAMELLYON16):
    [https://camelyon16.grand-challenge.org/Home](https://camelyon16.grand-challenge.org/Home/)
    is composed of 399 WSI for detection of metastases in H&E stained tissue sections
    of sentinel auxiliary lymph nodes (SNLs) of women with breast cancer (Ehteshami Bejnordi
    et al., [2017](#bib.bib46)). The WSIs are annotated globally as normal or metastases.
    The WSIs with metastases are further annotated at the pixel level to indicate
    regions of tumors. An example of a WSI is provided in [Figure 6](#S3.F6 "Figure
    6 ‣ 3.3.2 Camelyon16 dataset (CAMELYON16) ‣ 3.3 Datasets ‣ 3 Experimental methodology
    ‣ Deep Weakly-Supervised Learning Methods for Classification and Localization
    in Histology Images: A Survey"). Among the 399 WSIs provided, 270 are used for
    training, and 129 for testing⁶⁶6Sample test_114 is discarded since the pixel level
    annotation was not provided. Therefore, the test set is composed of $128$ samples
    with $48$ samples with nodal metastases.. The large size of the images makes their
    use in this survey inconvenient. Therefore, we designed a concise protocol to
    sample small sub-images for WSL with pixel-wise and image-level annotations. In
    summary, we sample sub-images of size ${512\times 512}$ to form train, validation,
    and test sets, respectively (Fig.[7](#S3.F7 "Figure 7 ‣ 3.3.2 Camelyon16 dataset
    (CAMELYON16) ‣ 3.3 Datasets ‣ 3 Experimental methodology ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey")).
    A detailed sampling protocol is provided in [section B](#S2a "B CAMELYON16 protocol
    for WSL ‣ Deep Weakly-Supervised Learning Methods for Classification and Localization
    in Histology Images: A Survey"). This protocol generates a benchmark containing
    a total of 48,870 samples: 24,348 samples for training, 8,858 samples for validation,
    and 15,664 samples for testing. Each sub-set has balanced classes. For B-LOC,
    we randomly select 5 samples per class from the validation set to be fully supervised,
    \ie, 10 samples in total.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集⁵⁵5癌症转移淋巴结挑战赛2016（CAMELYON16）：[https://camelyon16.grand-challenge.org/Home](https://camelyon16.grand-challenge.org/Home/)
    包含399个全切片图像，用于检测女性乳腺癌患者的哨点辅助淋巴结（SNLs）中H&E染色组织切片的转移情况（Ehteshami Bejnordi 等人，[2017](#bib.bib46)）。全切片图像被全球标注为正常或转移。标注为转移的全切片图像在像素级别上进一步标注，以指示肿瘤区域。全切片图像的示例见[图6](#S3.F6
    "图 6 ‣ 3.3.2 Camelyon16 数据集（CAMELYON16） ‣ 3.3 数据集 ‣ 3 实验方法 ‣ 深度弱监督学习方法在组织学图像中的分类和定位：综述")。在提供的399个全切片图像中，270个用于训练，129个用于测试⁶⁶6样本
    test_114 被丢弃，因为未提供像素级标注。因此，测试集包含128个样本，其中48个样本有结节转移。图像的较大尺寸使其在本调查中的使用不方便。因此，我们设计了一个简洁的协议来对小型子图像进行采样，以进行像素级和图像级标注。总之，我们采样了大小为${512\times
    512}$的子图像，分别形成训练集、验证集和测试集（见图[7](#S3.F7 "图 7 ‣ 3.3.2 Camelyon16 数据集（CAMELYON16）
    ‣ 3.3 数据集 ‣ 3 实验方法 ‣ 深度弱监督学习方法在组织学图像中的分类和定位：综述")）。详细的采样协议见[章节B](#S2a "B CAMELYON16
    协议用于 WSL ‣ 深度弱监督学习方法在组织学图像中的分类和定位：综述")。该协议生成了一个基准数据集，总共包含48,870个样本：24,348个样本用于训练，8,858个样本用于验证，15,664个样本用于测试。每个子集的类别是平衡的。对于B-LOC，我们从验证集中随机选择每类5个样本进行完全监督，即总共10个样本。
- en: '![Refer to caption](img/e887c1e1a1e50ac6fe9a5fb52f82516b.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e887c1e1a1e50ac6fe9a5fb52f82516b.png)'
- en: 'Figure 6: Example of metastatic regions in a WSI from CAMELYON16 dataset (Credit: (Sirinukunwattana
    et al., [2017](#bib.bib129))). *Top left*: WSI with tumor. *Top right*: Zoom-in
    of one of the metastatic regions. *Bottom*: Further zoom-in of the frontier between
    normal and metastatic regions.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: CAMELYON16 数据集中 WSI 中的转移性区域示例（致谢: (Sirinukunwattana 等人, [2017](#bib.bib129))）。*左上*:
    带有肿瘤的 WSI。*右上*: 一个转移性区域的放大图。*下*: 正常区域与转移性区域交界处的进一步放大图。'
- en: '![Refer to caption](img/ad653aa5509f4947ac831b02f7ebb5e3.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ad653aa5509f4947ac831b02f7ebb5e3.png)'
- en: 'Figure 7: Examples of test images from metastatic (top) and normal (bottom)
    classes of CAMELYON16 dataset of size ${512\times 512}$. Metastatic regions are
    indicated with a red mask.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: CAMELYON16 数据集中转移性（上）和正常（下）类别的测试图像示例，图像大小为 ${512\times 512}$。转移性区域用红色遮罩标记。'
- en: '|  |  | GlaS |  | CAMELYON16 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GlaS |  | CAMELYON16 |'
- en: '|  |  | VGG | Inception | ResNet | Mean |  | VGG | Inception | ResNet | Mean
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  |  | VGG | Inception | ResNet | 平均 |  | VGG | Inception | ResNet | 平均 |'
- en: '| Methods / Metric |  | PxAP (B-LOC) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 方法 / 指标 |  | PxAP (B-LOC) |'
- en: '| Bottom-up WSOL |  |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 自底向上 WSOL |  |  |'
- en: '| GAP (Lin et al., [2013](#bib.bib90))  *(corr,2013)* |  | $58.5$ | $57.5$
    | $56.2$ | 57.4 |  | $37.5$ | $24.6$ | $43.7$ | 35.2 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| GAP (Lin 等人, [2013](#bib.bib90))  *(corr,2013)* |  | $58.5$ | $57.5$ | $56.2$
    | 57.4 |  | $37.5$ | $24.6$ | $43.7$ | 35.2 |'
- en: '| MAX-Pool (Oquab et al., [2015](#bib.bib105))  *(cvpr,2015)* |  | $58.5$ |
    $57.1$ | $46.2$ | 53.9 |  | $42.1$ | $40.9$ | $20.2$ | 34.4 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| MAX-Pool (Oquab 等人, [2015](#bib.bib105))  *(cvpr,2015)* |  | $58.5$ | $57.1$
    | $46.2$ | 53.9 |  | $42.1$ | $40.9$ | $20.2$ | 34.4 |'
- en: '| LSE (Sun et al., [2016](#bib.bib138))  *(cvpr,2016)* |  | $63.9$ | $62.8$
    | $59.1$ | 61.9 |  | $63.1$ | $29.0$ | $42.1$ | 44.7 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| LSE (Sun 等人, [2016](#bib.bib138))  *(cvpr,2016)* |  | $63.9$ | $62.8$ | $59.1$
    | 61.9 |  | $63.1$ | $29.0$ | $42.1$ | 44.7 |'
- en: '| CAM (Zhou et al., [2016](#bib.bib172))  *(cvpr,2016)* |  | $68.5$ | $50.5$
    | $64.4$ | $61.1$ |  | $25.4$ | $48.7$ | $27.5$ | 33.8 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| CAM (Zhou 等人, [2016](#bib.bib172))  *(cvpr,2016)* |  | $68.5$ | $50.5$ |
    $64.4$ | $61.1$ |  | $25.4$ | $48.7$ | $27.5$ | 33.8 |'
- en: '| HaS (Singh and Lee, [2017](#bib.bib128))  *(iccv,2017)* |  | $65.5$ | $65.4$
    | $63.5$ | 64.8 |  | $25.4$ | $47.1$ | $29.7$ | 34.0 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| HaS (Singh 和 Lee, [2017](#bib.bib128))  *(iccv,2017)* |  | $65.5$ | $65.4$
    | $63.5$ | 64.8 |  | $25.4$ | $47.1$ | $29.7$ | 34.0 |'
- en: '| WILDCAT (Durand et al., [2017](#bib.bib45))  *(cvpr,2017)* |  | $56.1$ |
    $54.9$ | $60.1$ | 57.0 |  | $44.4$ | $31.4$ | $31.0$ | 35.6 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| WILDCAT (Durand 等人, [2017](#bib.bib45))  *(cvpr,2017)* |  | $56.1$ | $54.9$
    | $60.1$ | 57.0 |  | $44.4$ | $31.4$ | $31.0$ | 35.6 |'
- en: '| ACoL (Zhang et al., [2018c](#bib.bib167))  *(cvpr,2018)* |  | $63.7$ | $58.2$
    | $54.2$ | 58.7 |  | $31.3$ | $39.3$ | $31.3$ | 33.9 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| ACoL (Zhang 等人, [2018c](#bib.bib167))  *(cvpr,2018)* |  | $63.7$ | $58.2$
    | $54.2$ | 58.7 |  | $31.3$ | $39.3$ | $31.3$ | 33.9 |'
- en: '| SPG (Zhang et al., [2018d](#bib.bib168))  *(eccv,2018)* |  | $63.6$ | $58.3$
    | $51.4$ | 57.7 |  | $45.4$ | $24.5$ | $22.6$ | 30.8 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| SPG (Zhang 等人, [2018d](#bib.bib168))  *(eccv,2018)* |  | $63.6$ | $58.3$
    | $51.4$ | 57.7 |  | $45.4$ | $24.5$ | $22.6$ | 30.8 |'
- en: '| Deep MIL (Ilse et al., [2018](#bib.bib69))  *(icml,2018)* |  | $66.6$ | $61.8$
    | $64.7$ | 64.3 |  | $53.8$ | $51.1$ | $57.9$ | 54.2 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Deep MIL (Ilse 等人, [2018](#bib.bib69))  *(icml,2018)* |  | $66.6$ | $61.8$
    | $64.7$ | 64.3 |  | $53.8$ | $51.1$ | $57.9$ | 54.2 |'
- en: '| PRM (Zhou et al., [2018](#bib.bib173))  *(cvpr,2018)* |  | $59.8$ | $53.1$
    | $62.3$ | $58.4$ |  | $46.0$ | $41.7$ | $23.2$ | $36.9$ |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| PRM (Zhou 等人, [2018](#bib.bib173))  *(cvpr,2018)* |  | $59.8$ | $53.1$ |
    $62.3$ | $58.4$ |  | $46.0$ | $41.7$ | $23.2$ | $36.9$ |'
- en: '| ADL (Choe and Shim, [2019](#bib.bib27))  *(cvpr,2019)* |  | $65.0$ | $60.6$
    | $54.1$ | 59.9 |  | $19.0$ | $46.0$ | $46.0$ | 37.0 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| ADL (Choe 和 Shim, [2019](#bib.bib27))  *(cvpr,2019)* |  | $65.0$ | $60.6$
    | $54.1$ | 59.9 |  | $19.0$ | $46.0$ | $46.0$ | 37.0 |'
- en: '| CutMix (Yun et al., [2019](#bib.bib161))  *(eccv,2019)* |  | $59.9$ | $50.4$
    | $56.7$ | 55.6 |  | $56.4$ | $44.9$ | $20.7$ | 40.6 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| CutMix (Yun 等人, [2019](#bib.bib161))  *(eccv,2019)* |  | $59.9$ | $50.4$
    | $56.7$ | 55.6 |  | $56.4$ | $44.9$ | $20.7$ | 40.6 |'
- en: '| TS-CAM (Gao et al., [2021](#bib.bib53))  *(corr,2021)* |  | t:$54.5$ | b:$57.8$
    | s:$55.1$ | 52.8 |  | t:$46.3$ | b:$21.6$ | s:$42.2$ | 36.7 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| TS-CAM (Gao 等人, [2021](#bib.bib53))  *(corr,2021)* |  | t:$54.5$ | b:$57.8$
    | s:$55.1$ | 52.8 |  | t:$46.3$ | b:$21.6$ | s:$42.2$ | 36.7 |'
- en: '| MAXMIN (Belharbi et al., [2022b](#bib.bib14))  *(tmi,2022)* |  | $75.0$ |
    $49.1$ | $81.2$ | 68.4 |  | $50.4$ | $\bm{80.8}$ | $\bm{77.7}$ | 69.6 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| MAXMIN (Belharbi 等人, [2022b](#bib.bib14))  *(tmi,2022)* |  | $75.0$ | $49.1$
    | $81.2$ | 68.4 |  | $50.4$ | $\bm{80.8}$ | $\bm{77.7}$ | 69.6 |'
- en: '| NEGEV (Belharbi et al., [2022a](#bib.bib13))  *(midl,2022)* |  | $\bm{81.3}$
    | $\bm{70.1}$ | $\bm{82.0}$ | $\bm{77.8}$ |  | $\bm{70.3}$ | $53.8$ | $52.6$ |
    $58.9$ |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| NEGEV (Belharbi 等人, [2022a](#bib.bib13))  *(midl,2022)* |  | $\bm{81.3}$
    | $\bm{70.1}$ | $\bm{82.0}$ | $\bm{77.8}$ |  | $\bm{70.3}$ | $53.8$ | $52.6$ |
    $58.9$ |'
- en: '| Top-down WSOL |  |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 自顶向下 WSOL |  |  |'
- en: '| GradCAM (Selvaraju et al., [2017](#bib.bib124))  *(iccv,2017)* |  | $75.7$
    | $56.9$ | $70.0$ | 67.5 |  | $40.2$ | $34.4$ | $29.1$ | 34.5 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| GradCAM (Selvaraju 等人, [2017](#bib.bib124))  *(iccv,2017)* |  | $75.7$ |
    $56.9$ | $70.0$ | 67.5 |  | $40.2$ | $34.4$ | $29.1$ | 34.5 |'
- en: '| GradCAM++ (Chattopadhyay et al., [2018](#bib.bib22))  *(wacv,2018)* |  |
    $76.1$ | $65.7$ | $70.7$ | 70.8 |  | $41.3$ | $43.9$ | $25.8$ | 37.0 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| GradCAM++ (查托帕迪亚等人, [2018](#bib.bib22))  *(wacv,2018)* |  | $76.1$ | $65.7$
    | $70.7$ | 70.8 |  | $41.3$ | $43.9$ | $25.8$ | 37.0 |'
- en: '| Smooth-GradCAM++ (Omeiza et al., [2019](#bib.bib104))  *(corr,2019)* |  |
    $71.3$ | $67.6$ | $75.5$ | 71.4 |  | $35.1$ | $31.6$ | $25.1$ | 30.6 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Smooth-GradCAM++ (欧梅扎等人, [2019](#bib.bib104))  *(corr,2019)* |  | $71.3$
    | $67.6$ | $75.5$ | 71.4 |  | $35.1$ | $31.6$ | $25.1$ | 30.6 |'
- en: '| XGradCAM (Fu et al., [2020](#bib.bib52))  *(bmvc,2020)* |  | $73.7$ | $66.4$
    | $62.6$ | 67.5 |  | $40.2$ | $33.0$ | $24.4$ | 32.5 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| XGradCAM (付等人, [2020](#bib.bib52))  *(bmvc,2020)* |  | $73.7$ | $66.4$ |
    $62.6$ | 67.5 |  | $40.2$ | $33.0$ | $24.4$ | 32.5 |'
- en: '| LayerCAM (Jiang et al., [2021](#bib.bib74))  *(ieee,2021)* |  | $67.8$ |
    $66.1$ | $70.9$ | 68.2 |  | $34.1$ | $25.0$ | $29.1$ | 29.4 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| LayerCAM (姜等人, [2021](#bib.bib74))  *(ieee,2021)* |  | $67.8$ | $66.1$ |
    $70.9$ | 68.2 |  | $34.1$ | $25.0$ | $29.1$ | 29.4 |'
- en: '| Fully supervised |  |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Fully supervised |  |  |'
- en: '| U-Net (Ronneberger et al., [2015](#bib.bib118))*(miccai,2015)* |  | $96.8$
    | $95.4$ | $96.4$ | 96.2 |  | $83.0$ | $82.2$ | $83.6$ | 82.9 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| U-Net (罗尼伯格等人, [2015](#bib.bib118))*(miccai,2015)* |  | $96.8$ | $95.4$ |
    $96.4$ | 96.2 |  | $83.0$ | $82.2$ | $83.6$ | 82.9 |'
- en: 'Table 1: PxAP performance over GlaS and CAMELYON16 test sets. Model selection:
    B-LOC.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: GlaS 和 CAMELYON16 测试集上的 PxAP 性能。模型选择：B-LOC。'
- en: '|  |  | VGG |  | Inception |  | ResNet |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  |  | VGG |  | Inception |  | ResNet |'
- en: '| Bottom-up WSOL |  | TP* | FN* | FP* | TN* |  | TP* | FN* | FP* | TN* |  |
    TP* | FN* | FP* | TN* |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Bottom-up WSOL |  | TP* | FN* | FP* | TN* |  | TP* | FN* | FP* | TN* |  |
    TP* | FN* | FP* | TN* |'
- en: '| GAP (Lin et al., [2013](#bib.bib90))  *(corr,2013)* |  | $34.5$ | $65.4$
    | $20.5$ | 79.4 |  | $50.0$ | $49.9$ | $51.1$ | 48.8 |  | $30.6$ | $69.3$ | $21.2$
    | 78.7 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| GAP (林等人, [2013](#bib.bib90))  *(corr,2013)* |  | $34.5$ | $65.4$ | $20.5$
    | 79.4 |  | $50.0$ | $49.9$ | $51.1$ | 48.8 |  | $30.6$ | $69.3$ | $21.2$ | 78.7
    |'
- en: '| MAX-Pool (Oquab et al., [2015](#bib.bib105))  *(cvpr,2015)* |  | $38.7$ |
    $61.2$ | $31.5$ | 68.4 |  | $41.2$ | $58.7$ | $36.3$ | 63.6 |  | $50.1$ | $49.8$
    | $55.7$ | 44.2 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| MAX-Pool (奥夸布等人, [2015](#bib.bib105))  *(cvpr,2015)* |  | $38.7$ | $61.2$
    | $31.5$ | 68.4 |  | $41.2$ | $58.7$ | $36.3$ | 63.6 |  | $50.1$ | $49.8$ | $55.7$
    | 44.2 |'
- en: '| LSE (Sun et al., [2016](#bib.bib138))  *(cvpr,2016)* |  | $52.0$ | $47.9$
    | $41.4$ | 58.5 |  | $\bm{70.9}$ | $\bm{29.0}$ | $63.7$ | 36.2 |  | $51.0$ | $48.9$
    | $44.9$ | 55.0 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| LSE (孙等人, [2016](#bib.bib138))  *(cvpr,2016)* |  | $52.0$ | $47.9$ | $41.4$
    | 58.5 |  | $\bm{70.9}$ | $\bm{29.0}$ | $63.7$ | 36.2 |  | $51.0$ | $48.9$ | $44.9$
    | 55.0 |'
- en: '| CAM (Zhou et al., [2016](#bib.bib172))  *(cvpr,2016)* |  | $34.5$ | $65.4$
    | $20.5$ | 79.4 |  | $50.0$ | $49.9$ | $51.1$ | 48.8 |  | $30.6$ | $69.3$ | $21.2$
    | 78.7 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| CAM (周等人, [2016](#bib.bib172))  *(cvpr,2016)* |  | $34.5$ | $65.4$ | $20.5$
    | 79.4 |  | $50.0$ | $49.9$ | $51.1$ | 48.8 |  | $30.6$ | $69.3$ | $21.2$ | 78.7
    |'
- en: '| HaS (Singh and Lee, [2017](#bib.bib128))  *(iccv,2017)* |  | $42.3$ | $57.6$
    | $31.4$ | 68.5 |  | $26.4$ | $73.5$ | $16.1$ | 83.8 |  | $36.0$ | $63.9$ | $27.5$
    | 72.4 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| HaS (辛格和李, [2017](#bib.bib128))  *(iccv,2017)* |  | $42.3$ | $57.6$ | $31.4$
    | 68.5 |  | $26.4$ | $73.5$ | $16.1$ | 83.8 |  | $36.0$ | $63.9$ | $27.5$ | 72.4
    |'
- en: '| WILDCAT (Durand et al., [2017](#bib.bib45))  *(cvpr,2017)* |  | $37.6$ |
    $62.3$ | $33.2$ | 66.7 |  | $42.3$ | $57.6$ | $35.6$ | 64.3 |  | $\bm{74.3}$ |
    $\bm{25.6}$ | $68.8$ | 31.1 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| WILDCAT (杜朗等人, [2017](#bib.bib45))  *(cvpr,2017)* |  | $37.6$ | $62.3$ |
    $33.2$ | 66.7 |  | $42.3$ | $57.6$ | $35.6$ | 64.3 |  | $\bm{74.3}$ | $\bm{25.6}$
    | $68.8$ | 31.1 |'
- en: '| ACoL (Zhang et al., [2018c](#bib.bib167))  *(cvpr,2018)* |  | $28.3$ | $71.6$
    | $11.1$ | 88.8 |  | $6.6$ | $93.3$ | $4.6$ | 95.3 |  | $16.1$ | $83.8$ | $6.4$
    | 93.5 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| ACoL (张等人, [2018c](#bib.bib167))  *(cvpr,2018)* |  | $28.3$ | $71.6$ | $11.1$
    | 88.8 |  | $6.6$ | $93.3$ | $4.6$ | 95.3 |  | $16.1$ | $83.8$ | $6.4$ | 93.5
    |'
- en: '| SPG (Zhang et al., [2018d](#bib.bib168))  *(eccv,2018)* |  | $62.2$ | $37.7$
    | $50.5$ | 49.4 |  | $58.8$ | $41.1$ | $51.9$ | 48.0 |  | $47.1$ | $52.8$ | $47.0$
    | 52.9 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| SPG (张等人, [2018d](#bib.bib168))  *(eccv,2018)* |  | $62.2$ | $37.7$ | $50.5$
    | 49.4 |  | $58.8$ | $41.1$ | $51.9$ | 48.0 |  | $47.1$ | $52.8$ | $47.0$ | 52.9
    |'
- en: '| Deep MIL (Ilse et al., [2018](#bib.bib69))  *(icml,2018)* |  | $14.7$ | $85.2$
    | $\bm{7.3}$ | 92.6 |  | $8.9$ | $91.0$ | $4.3$ | 95.6 |  | $10.2$ | $89.7$ |
    $\bm{4.0}$ | 95.9 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Deep MIL (伊尔斯等人, [2018](#bib.bib69))  *(icml,2018)* |  | $14.7$ | $85.2$
    | $\bm{7.3}$ | 92.6 |  | $8.9$ | $91.0$ | $4.3$ | 95.6 |  | $10.2$ | $89.7$ |
    $\bm{4.0}$ | 95.9 |'
- en: '| PRM (Zhou et al., [2018](#bib.bib173))  *(cvpr,2018)* |  | $41.8$ | $58.1$
    | $34.7$ | 65.2 |  | $61.1$ | $38.8$ | $59.7$ | 40.2 |  | $37.3$ | $62.6$ | $29.9$
    | 70.0 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| PRM (周等人, [2018](#bib.bib173))  *(cvpr,2018)* |  | $41.8$ | $58.1$ | $34.7$
    | 65.2 |  | $61.1$ | $38.8$ | $59.7$ | 40.2 |  | $37.3$ | $62.6$ | $29.9$ | 70.0
    |'
- en: '| ADL (Choe and Shim, [2019](#bib.bib27))  *(cvpr,2019)* |  | $41.3$ | $58.6$
    | $30.2$ | 69.7 |  | $47.3$ | $52.6$ | $38.5$ | 61.4 |  | $34.4$ | $65.5$ | $31.4$
    | 68.5 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| ADL (崔和沈, [2019](#bib.bib27))  *(cvpr,2019)* |  | $41.3$ | $58.6$ | $30.2$
    | 69.7 |  | $47.3$ | $52.6$ | $38.5$ | 61.4 |  | $34.4$ | $65.5$ | $31.4$ | 68.5
    |'
- en: '| CutMix (Yun et al., [2019](#bib.bib161))  *(eccv,2019)* |  | $41.3$ | $58.6$
    | $33.6$ | 66.3 |  | $38.9$ | $61.0$ | $39.4$ | 60.5 |  | $31.3$ | $68.6$ | $28.1$
    | 71.8 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| CutMix (Yun 等，[2019](#bib.bib161))  *(eccv,2019)* |  | $41.3$ | $58.6$ |
    $33.6$ | 66.3 |  | $38.9$ | $61.0$ | $39.4$ | 60.5 |  | $31.3$ | $68.6$ | $28.1$
    | 71.8 |'
- en: '| TS-CAM (Gao et al., [2021](#bib.bib53))  *(corr,2021)* |  | t:$23.1$ | t:$76.8$
    | t:$20.4$ | t:79.5 |  | b:$25.2$ | b:$74.7$ | b:$20.4$ | b:79.5 |  | s:$30.3$
    | s:$69.6$ | s:$26.5$ | s:73.4 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| TS-CAM (Gao 等，[2021](#bib.bib53))  *(corr,2021)* |  | t:$23.1$ | t:$76.8$
    | t:$20.4$ | t:79.5 |  | b:$25.2$ | b:$74.7$ | b:$20.4$ | b:79.5 |  | s:$30.3$
    | s:$69.6$ | s:$26.5$ | s:73.4 |'
- en: '| MAXMIN (Belharbi et al., [2022b](#bib.bib14))  *(tmi,2022)* |  | $\bm{57.4}$
    | $\bm{42.5}$ | $41.8$ | 58.1 |  | $43.0$ | $56.9$ | $44.3$ | 55.6 |  | $56.0$
    | $43.9$ | $38.6$ | 61.3 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| MAXMIN (Belharbi 等，[2022b](#bib.bib14))  *(tmi,2022)* |  | $\bm{57.4}$ |
    $\bm{42.5}$ | $41.8$ | 58.1 |  | $43.0$ | $56.9$ | $44.3$ | 55.6 |  | $56.0$ |
    $43.9$ | $38.6$ | 61.3 |'
- en: '| NEGEV (Belharbi et al., [2022a](#bib.bib13))  *(midl,2022)* |  | $52.3$ |
    $47.6$ | $42.5$ | 57.4 |  | $54.7$ | $45.2$ | $48.9$ | 51.0 |  | $52.2$ | $47.7$
    | $45.6$ | 54.3 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| NEGEV (Belharbi 等，[2022a](#bib.bib13))  *(midl,2022)* |  | $52.3$ | $47.6$
    | $42.5$ | 57.4 |  | $54.7$ | $45.2$ | $48.9$ | 51.0 |  | $52.2$ | $47.7$ | $45.6$
    | 54.3 |'
- en: '| Top-down WSOL |  |  |  |  |  |  |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 自上而下的 WSOL |  |  |  |  |  |  |  |'
- en: '| GradCAM (Selvaraju et al., [2017](#bib.bib124))  *(iccv,2017)* |  | $28.3$
    | $71.6$ | $11.1$ | 88.8 |  | $6.6$ | $93.3$ | $4.6$ | 95.3 |  | $16.1$ | $83.8$
    | $6.4$ | 93.5 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| GradCAM (Selvaraju 等，[2017](#bib.bib124))  *(iccv,2017)* |  | $28.3$ | $71.6$
    | $11.1$ | 88.8 |  | $6.6$ | $93.3$ | $4.6$ | 95.3 |  | $16.1$ | $83.8$ | $6.4$
    | 93.5 |'
- en: '| GradCAM++ (Chattopadhyay et al., [2018](#bib.bib22))  *(wacv,2018)* |  |
    $30.6$ | $69.3$ | $13.4$ | 86.5 |  | $12.5$ | $87.4$ | $5.1$ | 94.8 |  | $19.0$
    | $80.9$ | $8.6$ | 91.3 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| GradCAM++ (Chattopadhyay 等，[2018](#bib.bib22))  *(wacv,2018)* |  | $30.6$
    | $69.3$ | $13.4$ | 86.5 |  | $12.5$ | $87.4$ | $5.1$ | 94.8 |  | $19.0$ | $80.9$
    | $8.6$ | 91.3 |'
- en: '| Smooth-GradCAM++ (Omeiza et al., [2019](#bib.bib104))  *(corr,2019)* |  |
    $31.3$ | $68.6$ | $17.0$ | 82.9 |  | $15.6$ | $84.3$ | $5.7$ | 94.2 |  | $24.8$
    | $75.1$ | $10.0$ | 89.9 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Smooth-GradCAM++ (Omeiza 等，[2019](#bib.bib104))  *(corr,2019)* |  | $31.3$
    | $68.6$ | $17.0$ | 82.9 |  | $15.6$ | $84.3$ | $5.7$ | 94.2 |  | $24.8$ | $75.1$
    | $10.0$ | 89.9 |'
- en: '| XGradCAM (Fu et al., [2020](#bib.bib52))  *(bmvc,2020)* |  | $31.5$ | $68.4$
    | $14.9$ | 85.0 |  | $13.0$ | $86.9$ | $4.1$ | 95.8 |  | $11.8$ | $88.1$ | $5.7$
    | 94.2 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| XGradCAM (Fu 等，[2020](#bib.bib52))  *(bmvc,2020)* |  | $31.5$ | $68.4$ |
    $14.9$ | 85.0 |  | $13.0$ | $86.9$ | $4.1$ | 95.8 |  | $11.8$ | $88.1$ | $5.7$
    | 94.2 |'
- en: '| LayerCAM (Jiang et al., [2021](#bib.bib74))  *(ieee,2021)* |  | $35.4$ |
    $64.5$ | $21.6$ | 78.3 |  | $11.0$ | $88.9$ | $\bm{3.5}$ | 96.4 |  | $18.2$ |
    $81.7$ | $8.1$ | 91.8 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| LayerCAM (Jiang 等，[2021](#bib.bib74))  *(ieee,2021)* |  | $35.4$ | $64.5$
    | $21.6$ | 78.3 |  | $11.0$ | $88.9$ | $\bm{3.5}$ | 96.4 |  | $18.2$ | $81.7$
    | $8.1$ | 91.8 |'
- en: '| Fully supervised |  |  |  |  |  |  |  |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 全监督 |  |  |  |  |  |  |  |'
- en: '| U-Net (Ronneberger et al., [2015](#bib.bib118))*(miccai,2015)* |  | $89.8$
    | $10.1$ | $11.6$ | 88.3 |  | $86.9$ | $13.0$ | $14.6$ | 85.3 |  | $87.0$ | $12.9$
    | $12.4$ | 87.5 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| U-Net (Ronneberger 等，[2015](#bib.bib118))*(miccai,2015)* |  | $89.8$ | $10.1$
    | $11.6$ | 88.3 |  | $86.9$ | $13.0$ | $14.6$ | 85.3 |  | $87.0$ | $12.9$ | $12.4$
    | 87.5 |'
- en: 'Table 2: Confusion matrix performance over GlaS test set. Model selection:
    B-LOC.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：GlaS 测试集的混淆矩阵性能。模型选择：B-LOC。
- en: '|  |  | VGG |  | Inception |  | ResNet |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  |  | VGG |  | Inception |  | ResNet |'
- en: '| Bottom-up WSOL |  | TP* | FN* | FP* | TN* |  | TP* | FN* | FP* | TN* |  |
    TP* | FN* | FP* | TN* |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 自下而上的 WSOL |  | TP* | FN* | FP* | TN* |  | TP* | FN* | FP* | TN* |  | TP*
    | FN* | FP* | TN* |'
- en: '| GAP (Lin et al., [2013](#bib.bib90))  *(corr,2013)* |  | $54.0$ | $45.9$
    | $52.6$ | 47.3 |  | $95.8$ | $4.1$ | $38.0$ | 61.9 |  | $63.3$ | $36.6$ | $52.6$
    | 47.3 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| GAP (Lin 等，[2013](#bib.bib90))  *(corr,2013)* |  | $54.0$ | $45.9$ | $52.6$
    | 47.3 |  | $95.8$ | $4.1$ | $38.0$ | 61.9 |  | $63.3$ | $36.6$ | $52.6$ | 47.3
    |'
- en: '| MAX-Pool (Oquab et al., [2015](#bib.bib105))  *(cvpr,2015)* |  | $\bm{94.5}$
    | $\bm{5.4}$ | $95.8$ | 4.1 |  | $70.8$ | $29.1$ | $56.5$ | 43.4 |  | $75.7$ |
    $24.2$ | $85.0$ | 14.9 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| MAX-Pool (Oquab 等，[2015](#bib.bib105))  *(cvpr,2015)* |  | $\bm{94.5}$ |
    $\bm{5.4}$ | $95.8$ | 4.1 |  | $70.8$ | $29.1$ | $56.5$ | 43.4 |  | $75.7$ | $24.2$
    | $85.0$ | 14.9 |'
- en: '| LSE (Sun et al., [2016](#bib.bib138))  *(cvpr,2016)* |  | $80.9$ | $19.0$
    | $53.5$ | 46.4 |  | $52.2$ | $47.7$ | $48.8$ | 51.1 |  | $87.4$ | $12.5$ | $76.2$
    | 23.7 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| LSE (Sun 等，[2016](#bib.bib138))  *(cvpr,2016)* |  | $80.9$ | $19.0$ | $53.5$
    | 46.4 |  | $52.2$ | $47.7$ | $48.8$ | 51.1 |  | $87.4$ | $12.5$ | $76.2$ | 23.7
    |'
- en: '| CAM (Zhou et al., [2016](#bib.bib172))  *(cvpr,2016)* |  | $54.0$ | $45.9$
    | $52.6$ | 47.3 |  | $\bm{95.8}$ | $\bm{4.1}$ | $38.0$ | 61.9 |  | $63.3$ | $36.6$
    | $52.6$ | 47.3 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| CAM (Zhou 等，[2016](#bib.bib172))  *(cvpr,2016)* |  | $54.0$ | $45.9$ | $52.6$
    | 47.3 |  | $\bm{95.8}$ | $\bm{4.1}$ | $38.0$ | 61.9 |  | $63.3$ | $36.6$ | $52.6$
    | 47.3 |'
- en: '| HaS (Singh and Lee, [2017](#bib.bib128))  *(iccv,2017)* |  | $54.0$ | $45.9$
    | $52.6$ | 47.3 |  | $90.5$ | $9.4$ | $36.1$ | 63.8 |  | $53.8$ | $46.1$ | $48.6$
    | 51.3 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| HaS (Singh 和 Lee，[2017](#bib.bib128))  *(iccv,2017)* |  | $54.0$ | $45.9$
    | $52.6$ | 47.3 |  | $90.5$ | $9.4$ | $36.1$ | 63.8 |  | $53.8$ | $46.1$ | $48.6$
    | 51.3 |'
- en: '| WILDCAT (Durand et al., [2017](#bib.bib45))  *(cvpr,2017)* |  | $84.0$ |
    $15.9$ | $48.5$ | 51.4 |  | $40.1$ | $59.8$ | $16.2$ | 83.7 |  | $37.4$ | $62.5$
    | $21.4$ | 78.5 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| WILDCAT (Durand et al., [2017](#bib.bib45))  *(cvpr,2017)* |  | $84.0$ |
    $15.9$ | $48.5$ | 51.4 |  | $40.1$ | $59.8$ | $16.2$ | 83.7 |  | $37.4$ | $62.5$
    | $21.4$ | 78.5 |'
- en: '| ACoL (Zhang et al., [2018c](#bib.bib167))  *(cvpr,2018)* |  | $13.4$ | $86.5$
    | $4.6$ | 95.3 |  | $14.1$ | $85.8$ | $7.2$ | 92.7 |  | $7.0$ | $92.9$ | $3.8$
    | 96.1 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| ACoL (Zhang et al., [2018c](#bib.bib167))  *(cvpr,2018)* |  | $13.4$ | $86.5$
    | $4.6$ | 95.3 |  | $14.1$ | $85.8$ | $7.2$ | 92.7 |  | $7.0$ | $92.9$ | $3.8$
    | 96.1 |'
- en: '| SPG (Zhang et al., [2018d](#bib.bib168))  *(eccv,2018)* |  | $79.6$ | $20.3$
    | $55.0$ | 44.9 |  | $47.7$ | $52.2$ | $46.9$ | 53.0 |  | $47.1$ | $52.8$ | $48.1$
    | 51.8 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| SPG (Zhang et al., [2018d](#bib.bib168))  *(eccv,2018)* |  | $79.6$ | $20.3$
    | $55.0$ | 44.9 |  | $47.7$ | $52.2$ | $46.9$ | 53.0 |  | $47.1$ | $52.8$ | $48.1$
    | 51.8 |'
- en: '| Deep MIL (Ilse et al., [2018](#bib.bib69))  *(icml,2018)* |  | $28.5$ | $71.4$
    | $9.7$ | 90.2 |  | $54.4$ | $45.5$ | $25.3$ | 74.6 |  | $25.2$ | $74.7$ | $7.6$
    | 92.3 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Deep MIL (Ilse et al., [2018](#bib.bib69))  *(icml,2018)* |  | $28.5$ | $71.4$
    | $9.7$ | 90.2 |  | $54.4$ | $45.5$ | $25.3$ | 74.6 |  | $25.2$ | $74.7$ | $7.6$
    | 92.3 |'
- en: '| PRM (Zhou et al., [2018](#bib.bib173))  *(cvpr,2018)* |  | $94.4$ | $5.5$
    | $36.5$ | 63.4 |  | $63.5$ | $36.4$ | $40.6$ | 59.3 |  | $00.0$ | $100.0$ | $00.0$
    | 100.0 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| PRM (Zhou et al., [2018](#bib.bib173))  *(cvpr,2018)* |  | $94.4$ | $5.5$
    | $36.5$ | 63.4 |  | $63.5$ | $36.4$ | $40.6$ | 59.3 |  | $00.0$ | $100.0$ | $00.0$
    | 100.0 |'
- en: '| ADL (Choe and Shim, [2019](#bib.bib27))  *(cvpr,2019)* |  | $51.8$ | $48.1$
    | $56.5$ | 43.4 |  | $82.5$ | $17.4$ | $41.7$ | 58.2 |  | $\bm{94.5}$ | $\bm{5.4}$
    | $35.9$ | 64.0 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| ADL (Choe and Shim, [2019](#bib.bib27))  *(cvpr,2019)* |  | $51.8$ | $48.1$
    | $56.5$ | 43.4 |  | $82.5$ | $17.4$ | $41.7$ | 58.2 |  | $\bm{94.5}$ | $\bm{5.4}$
    | $35.9$ | 64.0 |'
- en: '| CutMix (Yun et al., [2019](#bib.bib161))  *(eccv,2019)* |  | $79.3$ | $20.6$
    | $52.5$ | 47.4 |  | $73.7$ | $26.2$ | $49.4$ | 50.5 |  | $1.5$ | $98.4$ | $22.0$
    | 77.9 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| CutMix (Yun et al., [2019](#bib.bib161))  *(eccv,2019)* |  | $79.3$ | $20.6$
    | $52.5$ | 47.4 |  | $73.7$ | $26.2$ | $49.4$ | 50.5 |  | $1.5$ | $98.4$ | $22.0$
    | 77.9 |'
- en: '| TS-CAM (Gao et al., [2021](#bib.bib53))  *(corr,2021)* |  | t:$92.8$ | t:$7.1$
    | t:$38.5$ | t:61.4 |  | b:$31.5$ | b:$68.4$ | b:$34.1$ | b:65.8 |  | s:$87.0$
    | s:$12.9$ | s:$38.5$ | s:61.4 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| TS-CAM (Gao et al., [2021](#bib.bib53))  *(corr,2021)* |  | t:$92.8$ | t:$7.1$
    | t:$38.5$ | t:61.4 |  | b:$31.5$ | b:$68.4$ | b:$34.1$ | b:65.8 |  | s:$87.0$
    | s:$12.9$ | s:$38.5$ | s:61.4 |'
- en: '| MAXMIN (Belharbi et al., [2022b](#bib.bib14))  *(tmi,2022)* |  | $47.1$ |
    $52.8$ | $47.1$ | 52.8 |  | $78.9$ | $21.0$ | $29.6$ | 70.3 |  | $62.1$ | $37.8$
    | $14.5$ | 85.4 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| MAXMIN (Belharbi et al., [2022b](#bib.bib14))  *(tmi,2022)* |  | $47.1$ |
    $52.8$ | $47.1$ | 52.8 |  | $78.9$ | $21.0$ | $29.6$ | 70.3 |  | $62.1$ | $37.8$
    | $14.5$ | 85.4 |'
- en: '| NEGEV (Belharbi et al., [2022a](#bib.bib13))  *(midl,2022)* |  | $21.1$ |
    $78.8$ | $\bm{4.2}$ | 95.7 |  | $22.1$ | $77.8$ | $5.9$ | 94.0 |  | $9.1$ | $90.8$
    | $3.9$ | 96.0 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| NEGEV (Belharbi et al., [2022a](#bib.bib13))  *(midl,2022)* |  | $21.1$ |
    $78.8$ | $\bm{4.2}$ | 95.7 |  | $22.1$ | $77.8$ | $5.9$ | 94.0 |  | $9.1$ | $90.8$
    | $3.9$ | 96.0 |'
- en: '| Top-down WSOL |  |  |  |  |  |  |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 自上而下的 WSOL |  |  |  |  |  |  |'
- en: '| GradCAM (Selvaraju et al., [2017](#bib.bib124))  *(iccv,2017)* |  | $13.4$
    | $86.5$ | $4.6$ | 95.3 |  | $14.1$ | $85.8$ | $7.2$ | 92.7 |  | $7.0$ | $92.9$
    | $3.8$ | 96.1 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| GradCAM (Selvaraju et al., [2017](#bib.bib124))  *(iccv,2017)* |  | $13.4$
    | $86.5$ | $4.6$ | 95.3 |  | $14.1$ | $85.8$ | $7.2$ | 92.7 |  | $7.0$ | $92.9$
    | $3.8$ | 96.1 |'
- en: '| GradCAM++ (Chattopadhyay et al., [2018](#bib.bib22))  *(wacv,2018)* |  |
    $70.6$ | $29.3$ | $43.6$ | 56.3 |  | $16.1$ | $83.8$ | $\bm{5.3}$ | 94.6 |  |
    $4.6$ | $95.3$ | $\bm{2.9}$ | 97.0 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| GradCAM++ (Chattopadhyay et al., [2018](#bib.bib22))  *(wacv,2018)* |  |
    $70.6$ | $29.3$ | $43.6$ | 56.3 |  | $16.1$ | $83.8$ | $\bm{5.3}$ | 94.6 |  |
    $4.6$ | $95.3$ | $\bm{2.9}$ | 97.0 |'
- en: '| Smooth-GradCAM++ (Omeiza et al., [2019](#bib.bib104))  *(corr,2019)* |  |
    $33.7$ | $66.2$ | $22.5$ | 77.4 |  | $14.7$ | $85.2$ | $8.6$ | 91.3 |  | $34.0$
    | $65.9$ | $31.8$ | 68.1 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Smooth-GradCAM++ (Omeiza et al., [2019](#bib.bib104))  *(corr,2019)* |  |
    $33.7$ | $66.2$ | $22.5$ | 77.4 |  | $14.7$ | $85.2$ | $8.6$ | 91.3 |  | $34.0$
    | $65.9$ | $31.8$ | 68.1 |'
- en: '| XGradCAM (Fu et al., [2020](#bib.bib52))  *(bmvc,2020)* |  | $13.4$ | $86.5$
    | $4.6$ | 95.3 |  | $26.4$ | $73.5$ | $16.0$ | 83.9 |  | $15.1$ | $84.8$ | $15.4$
    | 84.5 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| XGradCAM (Fu et al., [2020](#bib.bib52))  *(bmvc,2020)* |  | $13.4$ | $86.5$
    | $4.6$ | 95.3 |  | $26.4$ | $73.5$ | $16.0$ | 83.9 |  | $15.1$ | $84.8$ | $15.4$
    | 84.5 |'
- en: '| LayerCAM (Jiang et al., [2021](#bib.bib74))  *(ieee,2021)* |  | $13.2$ |
    $86.7$ | $6.2$ | 93.7 |  | $25.7$ | $74.2$ | $23.8$ | 76.1 |  | $5.9$ | $94.0$
    | $3.7$ | 96.2 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| LayerCAM (Jiang et al., [2021](#bib.bib74))  *(ieee,2021)* |  | $13.2$ |
    $86.7$ | $6.2$ | 93.7 |  | $25.7$ | $74.2$ | $23.8$ | 76.1 |  | $5.9$ | $94.0$
    | $3.7$ | 96.2 |'
- en: '| Fully supervised |  |  |  |  |  |  |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 完全监督 |  |  |  |  |  |  |  |'
- en: '| U-Net (Ronneberger et al., [2015](#bib.bib118))*(miccai,2015)* |  | $58.9$
    | $41.0$ | $7.1$ | 92.8 |  | $54.6$ | $45.3$ | $5.4$ | 94.5 |  | $58.7$ | $41.2$
    | $8.1$ | 91.8 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| U-Net (Ronneberger et al., [2015](#bib.bib118))*(miccai,2015)* |  | $58.9$
    | $41.0$ | $7.1$ | 92.8 |  | $54.6$ | $45.3$ | $5.4$ | 94.5 |  | $58.7$ | $41.2$
    | $8.1$ | 91.8 |'
- en: 'Table 3: Confusion matrix performance over CAMELYON16 test set. Model selection:
    B-LOC.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：CAMELYON16 测试集上的混淆矩阵性能。模型选择：B-LOC。
- en: 3.3.3 Implementation details
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 实施细节
- en: 'The training of all methods is performed using SGD with 32 batch size (Choe
    et al., [2020](#bib.bib28)), 1000 epochs for GlaS, and 20 epochs for CAMELYON16.
    We use a weight decay of ${10^{-4}}$. Images are resized to 256x256, and patches
    of size 224x224 are randomly sampled for training. Since almost all methods were
    evaluated on natural images, we cannot use the reported best hyper-parameters
    in their original papers. For each method, we perform a search for the best hyper-parameter
    over the validation set, including the learning rate. For the methods (Belharbi
    et al., [2022a](#bib.bib13), [b](#bib.bib14)), we set part of the hyper-parameters
    as described in their papers, since they were evaluated on the same histology
    datasets. For each method, the number of hyper-parameters to tune ranges from
    one to six. We use three different common backbones (Choe et al., [2020](#bib.bib28)):
    VGG16 (Simonyan and Zisserman, [2015](#bib.bib127)), InceptionV3 (Szegedy et al.,
    [2016](#bib.bib139)), and ResNet50 (He et al., [2016](#bib.bib64)). For the TS-CAM
    method (Gao et al., [2021](#bib.bib53)), we use DeiT-based architectures (Touvron
    et al., [2021](#bib.bib145)): DeiT-Ti (t), DeiT-S (s), DeiT-B (b). We use U-Net (Ronneberger
    et al., [2015](#bib.bib118)) with full pixel annotation to yield an upper-bound
    segmentation performance. The weights of all architectures (backbones) are initialized
    using pre-trained models over Image-Net (Krizhevsky et al., [2012](#bib.bib85)).
    Then, ann the weights are trained on the histology data. The U-Net decoder is
    initialized randomly. In [section A](#S1a "A Hyper-parameter search ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey"),
    we provide full details on the hyper-parameters search.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '所有方法的训练使用批量大小为32的SGD（Choe等，[2020](#bib.bib28)），GlaS使用1000个训练周期，CAMELYON16使用20个训练周期。我们使用${10^{-4}}$的权重衰减。图像被调整为256x256的大小，训练时随机抽取224x224的补丁。由于几乎所有方法都是在自然图像上评估的，我们无法使用原始论文中报告的最佳超参数。对于每种方法，我们在验证集上进行超参数搜索，包括学习率。对于方法（Belharbi等，[2022a](#bib.bib13)，[b](#bib.bib14)），我们按照其论文中描述的设置部分超参数，因为它们是在相同的组织学数据集上评估的。每种方法需要调整的超参数数量从一到六不等。我们使用三种不同的常见骨干网络（Choe等，[2020](#bib.bib28)）：VGG16（Simonyan
    和 Zisserman，[2015](#bib.bib127)）、InceptionV3（Szegedy 等，[2016](#bib.bib139)）和ResNet50（He等，[2016](#bib.bib64)）。对于TS-CAM方法（Gao等，[2021](#bib.bib53)），我们使用基于DeiT的架构（Touvron等，[2021](#bib.bib145)）：DeiT-Ti（t）、DeiT-S（s）、DeiT-B（b）。我们使用U-Net（Ronneberger等，[2015](#bib.bib118)）进行全像素标注，以获得上限分割性能。所有架构（骨干网）的权重使用在Image-Net（Krizhevsky等，[2012](#bib.bib85)）上预训练的模型进行初始化。然后，所有权重在组织学数据上进行训练。U-Net解码器随机初始化。在[部分A](#S1a
    "A Hyper-parameter search ‣ Deep Weakly-Supervised Learning Methods for Classification
    and Localization in Histology Images: A Survey")中，我们提供了超参数搜索的详细信息。'
- en: '![Refer to caption](img/aeaafd061b6436a5a7246fe7a7e6534c.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/aeaafd061b6436a5a7246fe7a7e6534c.png)'
- en: 'Figure 8: Localization sensitivity to thresholding: WSOL methods (orange),
    average WSOL methods (blue), fully supervised method (green). Top: GlaS. Bottom:
    CAMELYON16. Best visualized in color.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：阈值敏感性定位：WSOL 方法（橙色）、平均 WSOL 方法（蓝色）、完全监督方法（绿色）。上图：GlaS。下图：CAMELYON16。最佳效果请使用彩色显示。
- en: 4 Results and discussion
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果与讨论
- en: 4.1 Comparison of selected methods
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 选定方法的比较
- en: '[Table 1](#S3.T1 "Table 1 ‣ 3.3.2 Camelyon16 dataset (CAMELYON16) ‣ 3.3 Datasets
    ‣ 3 Experimental methodology ‣ Deep Weakly-Supervised Learning Methods for Classification
    and Localization in Histology Images: A Survey") shows the localization performance
    (PxAP) of all methods over both the GlaS and CAMELYON16 datasets. Overall, we
    observed a discrepancy in performance between different backbones. Across all
    WSOL methods, we obtained an average PxAP localization performance of ${66.86\%}$
    for VGG, followed by ${63.46\%}$ for ResNet50, and finally, ${59.60\%}$ for Inception
    over the GlaS test set ([Table 1](#S3.T1 "Table 1 ‣ 3.3.2 Camelyon16 dataset (CAMELYON16)
    ‣ 3.3 Datasets ‣ 3 Experimental methodology ‣ Deep Weakly-Supervised Learning
    Methods for Classification and Localization in Histology Images: A Survey")).
    Over CAMELYON16, VGG still ranks first with ${42.17\%}$, followed by Inception
    with ${40.61\%}$, and then ResNet50 with ${34.72\%}$. This performance difference
    comes from the basic architectural design difference between these common backbones.
    In addition, the results of the WSOL methods show that the CAMELYON16 dataset,
    with an average localization performance of ${39.01\%}$, is more challenging than
    the GlaS dataset, which has an average localization performance of ${62.75\%}$.
    This reflects the inherited difficulty in the CAMELYON16 dataset. While both datasets
    are challenging, the task in the GlaS dataset boils down to localizing glands
    which often have a relatively distinct but variable shape or texture ([Figure 5](#S3.F5
    "Figure 5 ‣ 3.3 Datasets ‣ 3 Experimental methodology ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey")).
    This makes spotting them relatively easy even for a non-expert. However, ROIs
    in CAMELYON16 have no obvious/common shape or texture ([Figure 7](#S3.F7 "Figure
    7 ‣ 3.3.2 Camelyon16 dataset (CAMELYON16) ‣ 3.3 Datasets ‣ 3 Experimental methodology
    ‣ Deep Weakly-Supervised Learning Methods for Classification and Localization
    in Histology Images: A Survey")). They can seem completely random from a local
    perspective. Spotting these ROIs can be even extremely challenging for non-experts.
    This explains the difference in the localization performance of WSOL methods over
    both datasets. Note that methods that were designed for histology images such
    as MAXMIN (Belharbi et al., [2022b](#bib.bib14)) and NEGEV (Belharbi et al., [2022a](#bib.bib13))
    yield the best localization performance compared to generic methods that were
    designed and evaluated on natural images. Top-down methods, such as GradCAM++ (Chattopadhyay
    et al., [2018](#bib.bib22)) and LayerCAM (Jiang et al., [2021](#bib.bib74)), have
    been shown to be more efficient on GlaS, with an average of ${69.08\%}$, than
    bottom-up methods, with an average of ${60.65\%}$. However, bottom-up methods
    perform better on CAMELYON16, with an average of ${41.09\%}$, as compared to ${32.80\%}$
    for top-down methods. This also can be explained by the aforementioned difference
    between both datasets. Bottom-up methods rely on convolution-responses that allow
    spotting common patterns, which can be better detected in GlaS on CAMELYON16.
    However, top-down methods often rely on gradients that can spot arbitrary shapes
    giving these methods more advantage over CAMELYON16. The deep MIL method (Ilse
    et al., [2018](#bib.bib69)) yielded interesting results on both datasets.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 1](#S3.T1 "表 1 ‣ 3.3.2 Camelyon16 数据集 (CAMELYON16) ‣ 3.3 数据集 ‣ 3 实验方法 ‣
    深度弱监督学习方法在组织学图像分类和定位中的应用：综述") 显示了所有方法在 GlaS 和 CAMELYON16 数据集上的定位性能（PxAP）。总体来看，我们观察到不同骨干网络之间性能存在差异。在所有
    WSOL 方法中，我们在 GlaS 测试集上获得了 VGG 平均 PxAP 定位性能为 ${66.86\%}$，其次是 ResNet50 的 ${63.46\%}$，最后是
    Inception 的 ${59.60\%}$（[表 1](#S3.T1 "表 1 ‣ 3.3.2 Camelyon16 数据集 (CAMELYON16)
    ‣ 3.3 数据集 ‣ 3 实验方法 ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述")）。在 CAMELYON16 上，VGG 仍排名第一，表现为
    ${42.17\%}$，其次是 Inception 的 ${40.61\%}$，最后是 ResNet50 的 ${34.72\%}$。这种性能差异源于这些常见骨干网络之间的基本架构设计差异。此外，WSOL
    方法的结果显示，CAMELYON16 数据集的平均定位性能为 ${39.01\%}$，比 GlaS 数据集的 ${62.75\%}$ 更具挑战性。这反映了
    CAMELYON16 数据集固有的难度。虽然这两个数据集都很具挑战性，但 GlaS 数据集的任务归结为定位腺体，这些腺体通常具有相对明显但变化的形状或纹理（[图 5](#S3.F5
    "图 5 ‣ 3.3 数据集 ‣ 3 实验方法 ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述")）。这使得即使是非专家也相对容易发现它们。然而，CAMELYON16
    中的 ROI 没有明显/常见的形状或纹理（[图 7](#S3.F7 "图 7 ‣ 3.3.2 Camelyon16 数据集 (CAMELYON16) ‣ 3.3
    数据集 ‣ 3 实验方法 ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述")）。从局部视角来看，它们可能显得完全随机。发现这些 ROI 对非专家来说可能极具挑战性。这解释了
    WSOL 方法在这两个数据集上的定位性能差异。请注意，相比于那些专为自然图像设计和评估的通用方法，专为组织学图像设计的方法如 MAXMIN (Belharbi
    et al., [2022b](#bib.bib14)) 和 NEGEV (Belharbi et al., [2022a](#bib.bib13)) 的定位性能最佳。自上而下的方法，如
    GradCAM++ (Chattopadhyay et al., [2018](#bib.bib22)) 和 LayerCAM (Jiang et al.,
    [2021](#bib.bib74))，在 GlaS 上表现更高效，平均为 ${69.08\%}$，相比之下，自下而上的方法平均为 ${60.65\%}$。然而，自下而上的方法在
    CAMELYON16 上表现更好，平均为 ${41.09\%}$，而自上而下的方法为 ${32.80\%}$。这也可以通过前述的数据集差异来解释。自下而上的方法依赖于卷积响应，这些响应可以发现常见的模式，这在
    GlaS 中比在 CAMELYON16 中更容易检测。然而，自上而下的方法通常依赖于梯度来发现任意形状，这使这些方法在 CAMELYON16 上更具优势。深度
    MIL 方法 (Ilse et al., [2018](#bib.bib69)) 在这两个数据集上均产生了有趣的结果。'
- en: Overall, the results also show a large performance gap between learning with
    weak supervision (global label in this case) and the fully-supervised method.
    This highlights the difficulty with histology images as compared to natural images.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，结果还显示了使用弱监督（在这种情况下为全局标签）与完全监督方法之间的巨大性能差距。这突显了组织学图像相较于自然图像的难度。
- en: 'We also look into the confusion matrix to better assess the pixel-wise predictions
    on both the GlaS ([Table 2](#S3.T2 "Table 2 ‣ 3.3.2 Camelyon16 dataset (CAMELYON16)
    ‣ 3.3 Datasets ‣ 3 Experimental methodology ‣ Deep Weakly-Supervised Learning
    Methods for Classification and Localization in Histology Images: A Survey")) and
    CAMELYON16 ([Table 3](#S3.T3 "Table 3 ‣ 3.3.2 Camelyon16 dataset (CAMELYON16)
    ‣ 3.3 Datasets ‣ 3 Experimental methodology ‣ Deep Weakly-Supervised Learning
    Methods for Classification and Localization in Histology Images: A Survey")) datasets.
    The first observation is the high *false negative* rate, with a large part of
    the ROI considered as background. This metric goes up to ${\sim 93\%}$ over GlaS
    and ${100\%}$ over CAMELYON16. This indicates that WSOL methods tend to under-activate
    by highlighting only a small part of the object and missing the rest. Under-activation
    is a common behavior in the WSOL method over natural images (Choe et al., [2020](#bib.bib28)),
    which increases false negatives. We observe a new trend, namely, high *false positive*,
    which is less common in WSOL (Choe et al., [2020](#bib.bib28)). This is caused
    by the over-activation of the entire image, including the ROI and background.
    The visual similarity between foreground/background regions is the source of this
    issue, as the model is unable to discriminate between both regions. On average,
    false positives are much more frequent in CAMELYON16 than in GlaS. However, in
    both datasets, the false negative rate is much higher than the false positive
    rate.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还查看了混淆矩阵，以更好地评估 GlaS ([Table 2](#S3.T2 "Table 2 ‣ 3.3.2 Camelyon16 dataset
    (CAMELYON16) ‣ 3.3 Datasets ‣ 3 Experimental methodology ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey"))
    和 CAMELYON16 ([Table 3](#S3.T3 "Table 3 ‣ 3.3.2 Camelyon16 dataset (CAMELYON16)
    ‣ 3.3 Datasets ‣ 3 Experimental methodology ‣ Deep Weakly-Supervised Learning
    Methods for Classification and Localization in Histology Images: A Survey")) 数据集上的像素级预测。第一个观察结果是高*假阴性*率，大部分
    ROI 被认为是背景。该指标在 GlaS 上高达 ${\sim 93\%}$，在 CAMELYON16 上高达 ${100\%}$。这表明 WSOL 方法倾向于通过仅突出对象的一小部分而遗漏其余部分，从而导致不足激活。不足激活是
    WSOL 方法在自然图像上 (Choe et al., [2020](#bib.bib28)) 中的常见行为，这增加了假阴性。我们观察到一种新的趋势，即高*假阳性*，这是
    WSOL (Choe et al., [2020](#bib.bib28)) 中较少见的。这是由于整个图像的过度激活，包括 ROI 和背景。前景/背景区域之间的视觉相似性是问题的根源，因为模型无法区分这两个区域。总体而言，CAMELYON16
    中的假阳性远比 GlaS 中的频繁。然而，在这两个数据集中，假阴性率都远高于假阳性率。'
- en: 'These results suggest that when dealing with histology images, the WSL method
    can exhibit two behaviors, either under-activate or over-activate, leading to
    high false negatives or positives. Both drawbacks should be considered when designing
    WSOL methods for this type of data. We provide visual results of both behaviors
    in [section C](#S3a "C Visual results ‣ Deep Weakly-Supervised Learning Methods
    for Classification and Localization in Histology Images: A Survey"). In the histology
    literature, two different ways are considered to alleviate these issues. In (Belharbi
    et al., [2022b](#bib.bib14)), the authors consider explicitly adding a background
    prior to prevent over-activation, while simultaneously preventing under-activation
    by promoting large sizes for both the background and the foreground. The authors
    in (Belharbi et al., [2022a](#bib.bib13)) have considered using pixel-level guidance
    from a pre-trained classifier. Empirically, this allowed consistent patterns to
    emerge while avoiding under-/over-activation. However, the main drawback of this
    method is its strong dependence on the quality of pixel-wise evidence collected
    from the pre-trained classifier.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '这些结果表明，在处理组织学图像时，WSL 方法可能表现出两种行为，要么是过度激活，要么是不足激活，导致高假阴性或假阳性。在设计适用于这种数据的 WSOL
    方法时，应考虑这两种缺点。我们在[部分 C](#S3a "C Visual results ‣ Deep Weakly-Supervised Learning
    Methods for Classification and Localization in Histology Images: A Survey")提供了这两种行为的视觉结果。在组织学文献中，有两种不同的方法被考虑用于缓解这些问题。在
    (Belharbi et al., [2022b](#bib.bib14))中，作者考虑了显式添加背景先验以防止过度激活，同时通过促进背景和前景的大尺寸来防止不足激活。在
    (Belharbi et al., [2022a](#bib.bib13))中，作者考虑了使用来自预训练分类器的像素级指导。从经验上看，这允许一致的模式出现，同时避免了不足/过度激活。然而，这种方法的主要缺点是对从预训练分类器收集的像素级证据质量的强依赖。'
- en: 4.2 Localization sensitivity to thresholds
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 阈值对定位的敏感性
- en: 'In WSOL, thresholding is required (Choe et al., [2020](#bib.bib28)) to obtain
    a mask of a specific ROI. All the localization performances reported in this work
    are marginalized over a set of thresholds to allow a fair comparison (Choe et al.,
    [2020](#bib.bib28)). It has been shown that threshold values are critical for
    localization performance (Choe et al., [2020](#bib.bib28)). In practice, for a
    test sample, one needs to threshold the CAM to yield a discrete localization of
    the ROI⁷⁷7Activations of CAMs can also be exploited visually by the user to determine
    ROI and manually inspect them.. Ideally, the value of the threshold should not
    have a major impact on the ROI localization. However, that is not the case for
    WSOL. Evaluations on natural images (Belharbi et al., [2022c](#bib.bib15); Choe
    et al., [2020](#bib.bib28)) have shown that localization performance from CAMs
    obtained in weakly-supervised setups is strongly tied to thresholds. We perform
    a similar analysis to the variation of localization performance with respect to
    the threshold in [Figure 8](#S3.F8 "Figure 8 ‣ 3.3.3 Implementation details ‣
    3.3 Datasets ‣ 3 Experimental methodology ‣ Deep Weakly-Supervised Learning Methods
    for Classification and Localization in Histology Images: A Survey"). We observe
    a steep decline in performance when increasing the threshold, similar to the results
    obtained in (Belharbi et al., [2022c](#bib.bib15); Choe et al., [2020](#bib.bib28)).
    This once again highlights the dependency of localization on the threshold, and
    also indicates that optimal thresholds are concentrated near zero, similarly to
    the observation of (Belharbi et al., [2022c](#bib.bib15); Choe et al., [2020](#bib.bib28)).
    These results also suggest that CAM’s activation distribution has a single mode
    located near zero as demonstrated in (Belharbi et al., [2022c](#bib.bib15); Choe
    et al., [2020](#bib.bib28)). This makes finding an optimal threshold difficult,
    and therefore, makes CAMs sensitive to thresholding, which reflects the uncertainty
    in CAMs. Ideally, the activation distribution is expected to be bimodal: background
    mode near zero, and foreground mode near one. Consequently, separating the foreground
    from the background becomes easy and less sensitive to the threshold, such as
    in a fully supervised method. The vulnerability of CAMs to thresholding is still
    an open issue in WSOL (Belharbi et al., [2022c](#bib.bib15); Choe et al., [2020](#bib.bib28)),
    and this should be considered in future designs of WSOL methods for general purposes,
    including histology data applications.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '在 WSOL 中，需要进行阈值处理 (Choe et al., [2020](#bib.bib28)) 来获取特定 ROI 的掩码。本文报告的所有定位性能都是在一组阈值上边际化处理，以便进行公平比较 (Choe
    et al., [2020](#bib.bib28))。已经证明，阈值对定位性能至关重要 (Choe et al., [2020](#bib.bib28))。在实际操作中，对于一个测试样本，需要对
    CAM 进行阈值处理，以获得 ROI 的离散定位⁷⁷7CAM 的激活也可以通过用户的视觉检查来确定 ROI 并进行手动检查。理想情况下，阈值的值不应对 ROI
    定位产生重大影响。然而，在 WSOL 中情况并非如此。在自然图像上的评估 (Belharbi et al., [2022c](#bib.bib15); Choe
    et al., [2020](#bib.bib28)) 已经表明，从 CAMs 获得的定位性能与阈值密切相关。我们在[图 8](#S3.F8 "Figure
    8 ‣ 3.3.3 Implementation details ‣ 3.3 Datasets ‣ 3 Experimental methodology ‣
    Deep Weakly-Supervised Learning Methods for Classification and Localization in
    Histology Images: A Survey")中对定位性能相对于阈值的变化进行了类似的分析。我们观察到，当增加阈值时，性能急剧下降，这与 (Belharbi
    et al., [2022c](#bib.bib15); Choe et al., [2020](#bib.bib28)) 中获得的结果类似。这再次突出了定位对阈值的依赖性，并且表明最佳阈值集中在接近零的区域，与 (Belharbi
    et al., [2022c](#bib.bib15); Choe et al., [2020](#bib.bib28)) 的观察结果类似。这些结果还表明
    CAM 的激活分布具有一个位于零附近的单峰，如 (Belharbi et al., [2022c](#bib.bib15); Choe et al., [2020](#bib.bib28))
    所示。这使得找到最佳阈值变得困难，因此，使得 CAM 对阈值变化敏感，这反映了 CAM 中的不确定性。理想情况下，激活分布应为双峰的：背景模式接近零，前景模式接近一。因此，将前景与背景分离变得容易且对阈值不敏感，就像在完全监督的方法中一样。CAM
    对阈值处理的脆弱性仍然是 WSOL 中的一个未解问题 (Belharbi et al., [2022c](#bib.bib15); Choe et al.,
    [2020](#bib.bib28))，这一点应在未来的 WSOL 方法设计中考虑，以适用于包括组织学数据应用在内的通用目的。'
- en: 'Results in [Figure 8](#S3.F8 "Figure 8 ‣ 3.3.3 Implementation details ‣ 3.3
    Datasets ‣ 3 Experimental methodology ‣ Deep Weakly-Supervised Learning Methods
    for Classification and Localization in Histology Images: A Survey") also show
    that there is still a large performance gap between WSOL and fully supervised
    methods, with the gap being much larger in the CAMELYON16 dataset.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8](#S3.F8 "Figure 8 ‣ 3.3.3 Implementation details ‣ 3.3 Datasets ‣ 3 Experimental
    methodology ‣ Deep Weakly-Supervised Learning Methods for Classification and Localization
    in Histology Images: A Survey")中的结果还显示WSOL与完全监督方法之间仍存在较大的性能差距，而在CAMELYON16数据集中，差距更大。'
- en: '![Refer to caption](img/ad34b18622c22cd9af3137c5dd02c014.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ad34b18622c22cd9af3137c5dd02c014.png)'
- en: '(a) Localization: Impact of model selection (B-LOC: orange. vs. B-CL: blue)
    over test localization (PxAP) performance. Each point indicates the epoch (x-axis)
    at which the best model is selected and its corresponding localization performance
    (y-axis). Large circles indicate the average over all WSOL methods. Top: GlaS.
    Bottom: CAMELYON16.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 定位：模型选择（B-LOC：橙色 vs. B-CL：蓝色）对测试定位（PxAP）性能的影响。每个点表示选择最佳模型的时期（x轴）及其对应的定位性能（y轴）。大圆圈表示所有WSOL方法的平均值。顶部：GlaS。底部：CAMELYON16。
- en: '![Refer to caption](img/114fc496654fe97d225c67efc13bfaf4.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/114fc496654fe97d225c67efc13bfaf4.png)'
- en: '(b) Classification: Impact of model selection (B-LOC: orange. vs. B-CL: blue)
    over test classification (CL) performance. Each point indicates the epoch (x-axis)
    at which the best model is selected and its corresponding classification performance
    (y-axis). Large circles indicate the average over all WSOL methods. Top: GlaS.
    Bottom: CAMELYON16.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 分类：模型选择（B-LOC：橙色 vs. B-CL：蓝色）对测试分类（CL）性能的影响。每个点表示选择最佳模型的时期（x轴）及其对应的分类性能（y轴）。大圆圈表示所有WSOL方法的平均值。顶部：GlaS。底部：CAMELYON16。
- en: 'Figure 9: Impact of model selection (B-LOC: orange. vs. B-CL: blue) on test
    localization (PxAP) and classification (CL) performance.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：模型选择（B-LOC：橙色 vs. B-CL：蓝色）对测试定位（PxAP）和分类（CL）性能的影响。
- en: 4.3 Importance of model selection
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 模型选择的重要性
- en: Typically, training a model for localization task with image class as weak supervision
    is done without having access to any localization information. This means that
    only classification information can be used to perform model selection using early
    stopping via a validation set, for instance. However, it has been shown that classification
    and localization tasks are antagonistic (Belharbi et al., [2022c](#bib.bib15);
    Choe et al., [2020](#bib.bib28)). This implies that model selection via localization
    (B-LOC) leads to good localization, and more likely than not, to poor classification.
    On the other hand, selection using classification (B-CL) yields good classification,
    and more likely than not, to poor localization, as observed empirically in (Belharbi
    et al., [2022c](#bib.bib15); Choe et al., [2020](#bib.bib28)).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用图像分类作为弱监督来训练定位任务的模型时，是在没有访问任何定位信息的情况下进行的。这意味着仅能使用分类信息来通过验证集执行模型选择，例如，通过早期停止。然而，已有研究表明，分类和定位任务是对立的（Belharbi
    et al., [2022c](#bib.bib15); Choe et al., [2020](#bib.bib28)）。这意味着通过定位（B-LOC）进行的模型选择会导致良好的定位，而分类则往往较差。另一方面，使用分类（B-CL）进行选择则会产生良好的分类，而定位往往较差，这一点在（Belharbi
    et al., [2022c](#bib.bib15); Choe et al., [2020](#bib.bib28)）中已被实证观察到。
- en: The authors in (Choe et al., [2020](#bib.bib28)) suggested a standard protocol
    for WSOL, in which a few samples in the validation set are fully labeled, \ie,
    they bear localization annotations. Such subset is used for model selection using
    localization (B-LOC) performance. While unrealistic⁸⁸8Since in a real weakly supervised
    application, we do not have any localization information, this protocol allows
    a fair comparison between methods by removing user bias from the selection.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: （Choe et al., [2020](#bib.bib28)）的作者建议了一种WSOL的标准协议，其中验证集中的一些样本被完全标注，即具有定位注释。该子集用于通过定位（B-LOC）性能进行模型选择。虽然在现实的弱监督应用中没有任何定位信息，这种协议通过去除选择中的用户偏见，允许对方法进行公平比较。
- en: 'In [9a](#S4.F9.sf1 "9a ‣ Figure 9 ‣ 4.2 Localization sensitivity to thresholds
    ‣ 4 Results and discussion ‣ Deep Weakly-Supervised Learning Methods for Classification
    and Localization in Histology Images: A Survey"), we show the impact of model
    selection on localization performance. We observe two main trends. First, localization
    performance converges during the very early epochs, while classification converges
    toward the end. This indicates that localization performance reaches its peak
    early on, and then degrades with long training epochs. The opposite can be said
    about classification. This result is consistent with the findings in (Belharbi
    et al., [2022c](#bib.bib15); Choe et al., [2020](#bib.bib28)) over natural images.
    A second important observation is that, on average, using localization information
    for model selection yields slightly better localization performance as compared
    to when using classification measures. Note that the total average gap varies
    on both datasets with ${\sim 2\%}$ for GlaS, and ${\sim 8\%}$ for CAMELYON16.
    This suggests that difficult datasets may benefit more from localization selection.
    Details of the differences in localization performance are reported in [Table 6](#S4.T6
    "Table 6 ‣ 4.4 Computational complexity ‣ 4 Results and discussion ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey").'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在[9a](#S4.F9.sf1 "9a ‣ 图 9 ‣ 4.2 对阈值的定位灵敏度 ‣ 4 结果与讨论 ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述")中，我们展示了模型选择对定位性能的影响。我们观察到两个主要趋势。首先，定位性能在非常早期的训练阶段就趋于稳定，而分类性能则在训练结束时趋于稳定。这表明定位性能在早期达到峰值，然后在较长的训练周期中下降。分类则恰恰相反。这个结果与自然图像中的研究发现一致（Belharbi
    等，[2022c](#bib.bib15); Choe 等，[2020](#bib.bib28)）。第二个重要的观察是，平均而言，使用定位信息进行模型选择相比于使用分类指标能略微提高定位性能。注意，总体平均差距在两个数据集中有所不同，GlaS
    数据集为${\sim 2\%}$，CAMELYON16 数据集为${\sim 8\%}$。这表明困难的数据集可能更能从定位选择中获益。定位性能差异的详细信息报告在[表
    6](#S4.T6 "表 6 ‣ 4.4 计算复杂度 ‣ 4 结果与讨论 ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述")中。
- en: 'In parallel, we inspected the impact of model selection on classification performance
    [9b](#S4.F9.sf2 "9b ‣ Figure 9 ‣ 4.2 Localization sensitivity to thresholds ‣
    4 Results and discussion ‣ Deep Weakly-Supervised Learning Methods for Classification
    and Localization in Histology Images: A Survey"). In most cases, we found that
    selecting a model based on its classification performance over a validation set
    consistently yields a largely better model in terms of classification. In contrast,
    model selection using localization performance yields poor classification performance.
    From [7](#S4.T7 "Table 7 ‣ 4.4 Computational complexity ‣ 4 Results and discussion
    ‣ Deep Weakly-Supervised Learning Methods for Classification and Localization
    in Histology Images: A Survey"), [9b](#S4.F9.sf2 "9b ‣ Figure 9 ‣ 4.2 Localization
    sensitivity to thresholds ‣ 4 Results and discussion ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey"),
    in ${70.39\%}$ of the cases, the classification selection outperforms the localization
    selection, with a performance gap of: [min: ${0.10\%}$, max: ${54.3\%}$, average:
    ${15.08\%}$]. In addition, the reverse is true for ${11.18\%}$ of the cases with
    a performance gap of: [min: ${0.29\%}$, max: ${11.3\%}$ avg: ${4.45\%}$]. In ${18.42\%}$
    of the cases, where both strategies yield the same classification performance.
    Therefore, based on these statistics, a better classifier is more likely to be
    selected using classification accuracy.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '与此同时，我们检查了模型选择对分类性能的影响[9b](#S4.F9.sf2 "9b ‣ 图 9 ‣ 4.2 对阈值的定位灵敏度 ‣ 4 结果与讨论 ‣
    深度弱监督学习方法在组织学图像分类和定位中的应用：综述")。在大多数情况下，我们发现基于验证集上的分类性能选择模型，通常能得到一个在分类方面表现更好的模型。相反，使用定位性能进行模型选择则导致分类性能较差。从[7](#S4.T7
    "表 7 ‣ 4.4 计算复杂度 ‣ 4 结果与讨论 ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述")，[9b](#S4.F9.sf2 "9b
    ‣ 图 9 ‣ 4.2 对阈值的定位灵敏度 ‣ 4 结果与讨论 ‣ 深度弱监督学习方法在组织学图像分类和定位中的应用：综述")中，在${70.39\%}$的情况下，分类选择优于定位选择，性能差距为：[最小:
    ${0.10\%}$，最大: ${54.3\%}$，平均: ${15.08\%}$]。此外，${11.18\%}$的情况下则相反，性能差距为：[最小: ${0.29\%}$，最大:
    ${11.3\%}$，平均: ${4.45\%}$]。在${18.42\%}$的情况下，两种策略的分类性能相同。因此，根据这些统计数据，使用分类准确率更有可能选择出更好的分类器。'
- en: All these results mimic what is observed in (Choe et al., [2020](#bib.bib28)),
    once again confirming that classification and localization under a weakly-supervised
    setup are more likely to be antagonistic. This is another challenge to consider
    when designing WSL methods.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些结果都模仿了在（Choe et al., [2020](#bib.bib28)）中观察到的情况，再次确认在弱监督设置下，分类和定位更可能是对立的。这是设计WSL方法时需要考虑的另一个挑战。
- en: The issue of model selection in WSOL was highlighted in (Choe et al., [2020](#bib.bib28)),
    including its impact on localization and classification performance on natural
    images. A similar behavior is observed here on the histology dataset, with no
    clear solution. Most research works on WSOL aim primarily to improve the state-of-the-art
    localization perfromance. Works in (Belharbi et al., [2022a](#bib.bib13), [c](#bib.bib15);
    Zhang et al., [2020a](#bib.bib164)) propose to separate localization from the
    classification task. First, they train a classifier to get the best classification
    performance. Then, using information from the classifier, a localizer is trained.
    This strategy allows to obtain a final framework that yields the best classification
    and localization performance.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: WSOL中的模型选择问题在（Choe et al., [2020](#bib.bib28)）中被强调，包括其对自然图像上的定位和分类性能的影响。这里在组织学数据集上观察到类似的行为，没有明确的解决方案。大多数关于WSOL的研究主要旨在提高最先进的定位性能。文献（Belharbi
    et al., [2022a](#bib.bib13), [c](#bib.bib15); Zhang et al., [2020a](#bib.bib164)）提出将定位任务与分类任务分开。首先，他们训练一个分类器以获得最佳分类性能。然后，利用分类器的信息来训练一个定位器。这种策略使得最终框架能够获得最佳的分类和定位性能。
- en: 4.4 Computational complexity
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 计算复杂性
- en: 'Inference time is an important consideration for systems deployed in real-world
    applications. [Table 4](#S4.T4 "Table 4 ‣ 4.4 Computational complexity ‣ 4 Results
    and discussion ‣ Deep Weakly-Supervised Learning Methods for Classification and
    Localization in Histology Images: A Survey") shows that WSOL methods require run
    times that are suitable for daily clinical routines. However, these statistics
    show that bottom-up methods are relatively faster than top-down methods. Note
    that bottom-up methods simply require a forward pass to yield the classification
    and localization. However, top-down methods require an additional backward pass
    to perform localization which, adds to the processing time. Among top-down methods,
    confidence-based aggregation methods have proven to be very slow since they need
    a series of forward passes to compute multiple scores. In addition, we observe
    that methods behave differently depending on the backbone architecture, with ResNet50
    appearing to yield a slower performance than other backbones. [Table 5](#S4.T5
    "Table 5 ‣ 4.4 Computational complexity ‣ 4 Results and discussion ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey")
    presents more details about the memory resources required for backbones, including
    the number of parameters.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '推理时间是部署在现实世界应用中的系统的一个重要考虑因素。[表4](#S4.T4 "Table 4 ‣ 4.4 Computational complexity
    ‣ 4 Results and discussion ‣ Deep Weakly-Supervised Learning Methods for Classification
    and Localization in Histology Images: A Survey") 显示WSOL方法所需的运行时间适合日常临床工作。然而，这些统计数据显示，自下而上的方法相对比自上而下的方法更快。注意，自下而上的方法只需一次前向传递即可得到分类和定位。然而，自上而下的方法需要额外的后向传递来进行定位，从而增加了处理时间。在自上而下的方法中，基于置信度的聚合方法被证明非常缓慢，因为它们需要一系列前向传递来计算多个分数。此外，我们观察到不同的方法在不同的主干架构下表现不同，ResNet50似乎比其他主干架构的性能更慢。[表5](#S4.T5
    "Table 5 ‣ 4.4 Computational complexity ‣ 4 Results and discussion ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey")
    提供了有关主干所需内存资源的更多细节，包括参数数量。'
- en: '|  |  | CNN Backbones |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CNN 主干 |'
- en: '| Methods |  | VGG16 |  | Inception |  | ResNet50 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |  | VGG16 |  | Inception |  | ResNet50 |'
- en: '| CAM (Zhou et al., [2016](#bib.bib172))  *(cvpr,2016)* |  | 0.2ms |  | 0.2ms
    |  | 0.3ms |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| CAM （Zhou et al., [2016](#bib.bib172)） *（cvpr,2016）* |  | 0.2ms |  | 0.2ms
    |  | 0.3ms |'
- en: '| ACoL (Zhang et al., [2018c](#bib.bib167))  *(cvpr,2018)* |  | 12.0ms |  |
    19.2ms |  | 24.9ms |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| ACoL （Zhang et al., [2018c](#bib.bib167)） *（cvpr,2018）* |  | 12.0ms |  |
    19.2ms |  | 24.9ms |'
- en: '| SPG (Zhang et al., [2018d](#bib.bib168))  *(eccv,2016)* |  | 11.0ms |  |
    18.0ms |  | 23.9ms |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| SPG （Zhang et al., [2018d](#bib.bib168)） *（eccv,2016）* |  | 11.0ms |  | 18.0ms
    |  | 23.9ms |'
- en: '| ADL (Choe and Shim, [2019](#bib.bib27))  *(cvpr,2019)* |  | 6.4ms |  | 16.0ms
    |  | 14.4ms |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| ADL （Choe and Shim, [2019](#bib.bib27)） *（cvpr,2019）* |  | 6.4ms |  | 16.0ms
    |  | 14.4ms |'
- en: '| NEGEV  (Belharbi et al., [2022a](#bib.bib13))  *(midl,2022)* |  | 6.2ms |  |
    25.5ms |  | 18.5ms |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| NEGEV （Belharbi et al., [2022a](#bib.bib13)） *（midl,2022）* |  | 6.2ms |  |
    25.5ms |  | 18.5ms |'
- en: '| GradCAM (Selvaraju et al., [2017](#bib.bib124))  *(iccv,2017)* |  | 7.7ms
    |  | 21.1ms |  | 27.8ms |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| GradCAM (塞尔瓦拉朱等, [2017](#bib.bib124))  *(iccv,2017)* |  | 7.7ms |  | 21.1ms
    |  | 27.8ms |'
- en: '| GradCAM++ (Chattopadhyay et al., [2018](#bib.bib22))  *(wacv,2018)* |  |
    23.5ms |  | 23.7ms |  | 28.0ms |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| GradCAM++ (查托帕迪亚等, [2018](#bib.bib22))  *(wacv,2018)* |  | 23.5ms |  | 23.7ms
    |  | 28.0ms |'
- en: '| Smooth-GradCAM (Omeiza et al., [2019](#bib.bib104))  *(corr,2019)* |  | 62.0ms
    |  | 150.7ms |  | 136.2ms |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Smooth-GradCAM (奥梅扎等, [2019](#bib.bib104))  *(corr,2019)* |  | 62.0ms |  |
    150.7ms |  | 136.2ms |'
- en: '| XGradCAM (Fu et al., [2020](#bib.bib52))  *(bmvc,2020)* |  | 2.9ms |  | 19.2ms
    |  | 14.2ms |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| XGradCAM (傅等, [2020](#bib.bib52))  *(bmvc,2020)* |  | 2.9ms |  | 19.2ms |  |
    14.2ms |'
- en: '| LayerCAM (Jiang et al., [2021](#bib.bib74))  *(ieee,2021)* |  | 3.2ms |  |
    18.2ms |  | 17.9ms |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| LayerCAM (姜等, [2021](#bib.bib74))  *(ieee,2021)* |  | 3.2ms |  | 18.2ms |  |
    17.9ms |'
- en: '| ScoreCAM (Wang et al., [2020](#bib.bib151))  *(cvpr,2020)* |  | 1.9s |  |
    3.4s |  | 9.3s |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| ScoreCAM (王等, [2020](#bib.bib151))  *(cvpr,2020)* |  | 1.9s |  | 3.4s |  |
    9.3s |'
- en: '| SSCAM (Naidu and Michael, [2020](#bib.bib100))  *(corr,2020)* |  | 105s |  |
    136s |  | 349s |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| SSCAM (奈杜和迈克尔, [2020](#bib.bib100))  *(corr,2020)* |  | 105s |  | 136s |  |
    349s |'
- en: '| IS-CAM (Naidu et al., [2020](#bib.bib101))  *(corr,2020)* |  | 30s |  | 39s
    |  | 99s |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| IS-CAM (奈杜等, [2020](#bib.bib101))  *(corr,2020)* |  | 30s |  | 39s |  | 99s
    |'
- en: 'Table 4: Inference time required to produce CAMs using different WSOL methods
    with standard classifiers = encoder (VGG16, Inception, ResNet50) + global average
    pooling. The time needed to build a full-size CAM is estimated using an idle Tesla
    P100 GPU for one random RGB image of size ${224\times 224}$ with ${200}$ classes.
    SSCAM (Naidu and Michael, [2020](#bib.bib100)) (${N=35,\sigma=2}$), IS-CAM (Naidu
    et al., [2020](#bib.bib101)) (${N=10}$), IS-CAM (Naidu et al., [2020](#bib.bib101))
    (${N=10}$) methods are evaluated with a batch size of 32 with their original hyper-parameters
    (${N,\text{ and }\sigma}$).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 使用不同的 WSOL 方法与标准分类器（=编码器（VGG16, Inception, ResNet50）+全局平均池化）生成 CAM 的推理时间。构建全尺寸
    CAM 的时间是使用空闲的 Tesla P100 GPU 对一个随机 RGB 图像（大小为 ${224\times 224}$，有 ${200}$ 类别）进行估算的。SSCAM (奈杜和迈克尔,
    [2020](#bib.bib100)) (${N=35,\sigma=2}$), IS-CAM (奈杜等, [2020](#bib.bib101)) (${N=10}$),
    IS-CAM (奈杜等, [2020](#bib.bib101)) (${N=10}$) 方法在批量大小为 32 和其原始超参数 (${N,\text{ and
    }\sigma}$) 下进行评估。'
- en: '|  |  | CNN Backbones |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CNN 主干网络 |'
- en: '| Measures |  | VGG16 |  | Inception |  | ResNet50 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 测量 |  | VGG16 |  | Inception |  | ResNet50 |'
- en: '| # parameters |  | $\approx$19.6M |  | $\approx$25.6M |  | $\approx$23.9M
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| # 参数 |  | $\approx$19.6M |  | $\approx$25.6M |  | $\approx$23.9M |'
- en: '| # feature maps |  | 1024 |  | 1024 |  | 2048 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| # 特征图 |  | 1024 |  | 1024 |  | 2048 |'
- en: 'Table 5: The number of parameters per backbone, and the number of feature maps
    at the top layer. The size of feature maps at the top layer is ${28\times 28}$.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 每个主干网络的参数数量，以及顶层的特征图数量。顶层特征图的大小为 ${28\times 28}$。'
- en: '|  |  | GlaS |  | CAMELYON16 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GlaS |  | CAMELYON16 |'
- en: '|  |  | VGG | Inception | ResNet | Mean |  | VGG | Inception | ResNet | Mean
    |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  |  | VGG | Inception | ResNet | 平均 |  | VGG | Inception | ResNet | 平均 |'
- en: '| Methods / Metric |  | PxAP: B-LOC/B-CL |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 方法 / 评价指标 |  | PxAP: B-LOC/B-CL |'
- en: '| Bottom-up WSOL |  |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 自下而上的 WSOL |  |  |'
- en: '| GAP (Lin et al., [2013](#bib.bib90))  *(corr,2013)* |  | $58.5/53.5$ | $57.5/61.0$
    | $60.3/56.2$ | 57.4/56.9 |  | $37.5/37.5$ | $24.6/15.8$ | $43.7/39.9$ | 35.2/31.0
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| GAP (林等, [2013](#bib.bib90))  *(corr,2013)* |  | $58.5/53.5$ | $57.5/61.0$
    | $60.3/56.2$ | 57.4/56.9 |  | $37.5/37.5$ | $24.6/15.8$ | $43.7/39.9$ | 35.2/31.0
    |'
- en: '| MAX-Pool (Oquab et al., [2015](#bib.bib105))  *(cvpr,2015)* |  | $58.5/58.5$
    | $57.1/56.2$ | $46.2/59.6$ | 53.9/58.1 |  | $42.1/28.3$ | $40.9/35.1$ | $20.2/19.4$
    | 34.4/27.6 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| MAX-Pool (奥夸布等, [2015](#bib.bib105))  *(cvpr,2015)* |  | $58.5/58.5$ | $57.1/56.2$
    | $46.2/59.6$ | 53.9/58.1 |  | $42.1/28.3$ | $40.9/35.1$ | $20.2/19.4$ | 34.4/27.6
    |'
- en: '| LSE (Sun et al., [2016](#bib.bib138))  *(cvpr,2016)* |  | $63.9/62.2$ | $62.8/61.3$
    | $59.1/58.1$ | 61.9/60.5 |  | $63.1/25.7$ | $29.0/27.9$ | $42.1/32.0$ | 44.7/28.5
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| LSE (孙等, [2016](#bib.bib138))  *(cvpr,2016)* |  | $63.9/62.2$ | $62.8/61.3$
    | $59.1/58.1$ | 61.9/60.5 |  | $63.1/25.7$ | $29.0/27.9$ | $42.1/32.0$ | 44.7/28.5
    |'
- en: '| CAM (Zhou et al., [2016](#bib.bib172))  *(cvpr,2016)* |  | $68.5/60.3$ |
    $50.5/53.3$ | $64.4/63.4$ | 61.1/58.2 |  | $25.4/25.4$ | $48.7/39.4$ | $27.5/14.8$
    | 33.8/26.5 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| CAM (周等, [2016](#bib.bib172))  *(cvpr,2016)* |  | $68.5/60.3$ | $50.5/53.3$
    | $64.4/63.4$ | 61.1/58.2 |  | $25.4/25.4$ | $48.7/39.4$ | $27.5/14.8$ | 33.8/26.5
    |'
- en: '| HaS (Singh and Lee, [2017](#bib.bib128))  *(iccv,2017)* |  | $65.5/61.4$
    | $65.4/63.6$ | $63.5/59.9$ | 64.8/61.6 |  | $25.4/16.0$ | $47.1/47.8$ | $29.7/17.7$
    | 34.0/27.1 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| HaS (辛格和李, [2017](#bib.bib128))  *(iccv,2017)* |  | $65.5/61.4$ | $65.4/63.6$
    | $63.5/59.9$ | 64.8/61.6 |  | $25.4/16.0$ | $47.1/47.8$ | $29.7/17.7$ | 34.0/27.1
    |'
- en: '| WILDCAT (Durand et al., [2017](#bib.bib45))  *(cvpr,2017)* |  | $56.1/69.1$
    | $54.9/48.4$ | $60.1/56.5$ | 57.0/58.0 |  | $44.4/44.4$ | $31.4/35.7$ | $31.0/16.8$
    | 35.6/32.3 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| WILDCAT (杜兰等, [2017](#bib.bib45))  *(cvpr,2017)* |  | $56.1/69.1$ | $54.9/48.4$
    | $60.1/56.5$ | 57.0/58.0 |  | $44.4/44.4$ | $31.4/35.7$ | $31.0/16.8$ | 35.6/32.3
    |'
- en: '| ACoL (Zhang et al., [2018c](#bib.bib167))  *(cvpr,2018)* |  | $63.7/57.2$
    | $58.2/54.8$ | $54.2/53.0$ | 58.7/55.0 |  | $31.3/18.1$ | $39.3/42.2$ | $31.3/32.0$
    | 33.9/30.7 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| ACoL (Zhang 等, [2018c](#bib.bib167))  *(cvpr,2018)* |  | $63.7/57.2$ | $58.2/54.8$
    | $54.2/53.0$ | 58.7/55.0 |  | $31.3/18.1$ | $39.3/42.2$ | $31.3/32.0$ | 33.9/30.7
    |'
- en: '| SPG (Zhang et al., [2018d](#bib.bib168))  *(eccv,2018)* |  | $63.6/55.7$
    | $58.3/59.5$ | $51.4/61.3$ | 57.7/58.8 |  | $45.4/45.4$ | $24.5/14.9$ | $22.6/15.5$
    | 30.8/25.2 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| SPG (Zhang 等, [2018d](#bib.bib168))  *(eccv,2018)* |  | $63.6/55.7$ | $58.3/59.5$
    | $51.4/61.3$ | 57.7/58.8 |  | $45.4/45.4$ | $24.5/14.9$ | $22.6/15.5$ | 30.8/25.2
    |'
- en: '| Deep MIL (Ilse et al., [2018](#bib.bib69))  *(icml,2018)* |  | $66.6/63.7$
    | $61.8/57.4$ | $64.7/57.9$ | 64.3/59.6 |  | $53.8/49.8$ | $51.1/47.9$ | $57.9/56.9$
    | 54.2/51.5 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| Deep MIL (Ilse 等, [2018](#bib.bib69))  *(icml,2018)* |  | $66.6/63.7$ | $61.8/57.4$
    | $64.7/57.9$ | 64.3/59.6 |  | $53.8/49.8$ | $51.1/47.9$ | $57.9/56.9$ | 54.2/51.5
    |'
- en: '| PRM (Zhou et al., [2018](#bib.bib173))  *(cvpr,2018)* |  | $59.8/57.4$ |
    $53.1/52.1$ | $62.3/58.8$ | 58.4/56.1 |  | $46.0/46.0$ | $41.7/21.6$ | $23.2/16.0$
    | 36.9/27.8 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| PRM (Zhou 等, [2018](#bib.bib173))  *(cvpr,2018)* |  | $59.8/57.4$ | $53.1/52.1$
    | $62.3/58.8$ | 58.4/56.1 |  | $46.0/46.0$ | $41.7/21.6$ | $23.2/16.0$ | 36.9/27.8
    |'
- en: '| ADL (Choe and Shim, [2019](#bib.bib27))  *(cvpr,2019)* |  | $65.0/62.5$ |
    $60.6/49.5$ | $54.1/61.8$ | 59.9/57.9 |  | $19.0/19.0$ | $46.0/29.8$ | $46.0/16.0$
    | 37.0/21.6 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| ADL (Choe 和 Shim, [2019](#bib.bib27))  *(cvpr,2019)* |  | $65.0/62.5$ | $60.6/49.5$
    | $54.1/61.8$ | 59.9/57.9 |  | $19.0/19.0$ | $46.0/29.8$ | $46.0/16.0$ | 37.0/21.6
    |'
- en: '| CutMix (Yun et al., [2019](#bib.bib161))  *(eccv,2019)* |  | $59.9/55.2$
    | $50.4/49.9$ | $56.7/52.5$ | 55.6/52.5 |  | $56.4/25.4$ | $44.9/27.6$ | $20.7/14.7$
    | 40.6/22.5 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| CutMix (Yun 等, [2019](#bib.bib161))  *(eccv,2019)* |  | $59.9/55.2$ | $50.4/49.9$
    | $56.7/52.5$ | 55.6/52.5 |  | $56.4/25.4$ | $44.9/27.6$ | $20.7/14.7$ | 40.6/22.5
    |'
- en: '| TS-CAM (Gao et al., [2021](#bib.bib53))  *(corr,2021)* |  | t:$54.5/54.5$
    | b:$57.8/58.4$ | s:$55.1/54.7$ | 52.8/55.8 |  | t:$46.3/17.5$ | b:$21.6/34.1$
    | s:$42.2/42.2$ | 36.7/31.2 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| TS-CAM (Gao 等, [2021](#bib.bib53))  *(corr,2021)* |  | t:$54.5/54.5$ | b:$57.8/58.4$
    | s:$55.1/54.7$ | 52.8/55.8 |  | t:$46.3/17.5$ | b:$21.6/34.1$ | s:$42.2/42.2$
    | 36.7/31.2 |'
- en: '| MAXMIN (Belharbi et al., [2022b](#bib.bib14))  *(tmi,2022)* |  | $75.0/63.8$
    | $49.1/61.0$ | $81.2/82.3$ | 68.4/69.0 |  | $50.4/46.6$ | $80.8/78.1$ | $77.7/74.9$
    | 69.6/66.5 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| MAXMIN (Belharbi 等, [2022b](#bib.bib14))  *(tmi,2022)* |  | $75.0/63.8$ |
    $49.1/61.0$ | $81.2/82.3$ | 68.4/69.0 |  | $50.4/46.6$ | $80.8/78.1$ | $77.7/74.9$
    | 69.6/66.5 |'
- en: '| Top-down WSOL |  |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| Top-down WSOL |  |  |'
- en: '| GradCAM (Selvaraju et al., [2017](#bib.bib124))  *(iccv,2017)* |  | $75.7/65.8$
    | $56.9/52.6$ | $70.0/67.2$ | 67.5/61.8 |  | $40.2/22.7$ | $34.4/34.2$ | $29.1/29.1$
    | 34.5/28.6 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| GradCAM (Selvaraju 等, [2017](#bib.bib124))  *(iccv,2017)* |  | $75.7/65.8$
    | $56.9/52.6$ | $70.0/67.2$ | 67.5/61.8 |  | $40.2/22.7$ | $34.4/34.2$ | $29.1/29.1$
    | 34.5/28.6 |'
- en: '| GradCAM++ (Chattopadhyay et al., [2018](#bib.bib22))  *(wacv,2018)* |  |
    $76.1/67.3$ | $65.7/53.1$ | $70.7/69.0$ | 70.8/63.1 |  | $41.3/26.6$ | $43.9/42.5$
    | $25.8/26.8$ | 37.0/31.9 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| GradCAM++ (Chattopadhyay 等, [2018](#bib.bib22))  *(wacv,2018)* |  | $76.1/67.3$
    | $65.7/53.1$ | $70.7/69.0$ | 70.8/63.1 |  | $41.3/26.6$ | $43.9/42.5$ | $25.8/26.8$
    | 37.0/31.9 |'
- en: '| Smooth-GradCAM++ (Omeiza et al., [2019](#bib.bib104))  *(corr,2019)* |  |
    $71.3/67.9$ | $67.6/67.5$ | $75.5/66.3$ | 71.4/67.2 |  | $35.1/24.2$ | $31.6/31.6$
    | $25.1/25.3$ | 30.6/27.0 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| Smooth-GradCAM++ (Omeiza 等, [2019](#bib.bib104))  *(corr,2019)* |  | $71.3/67.9$
    | $67.6/67.5$ | $75.5/66.3$ | 71.4/67.2 |  | $35.1/24.2$ | $31.6/31.6$ | $25.1/25.3$
    | 30.6/27.0 |'
- en: '| XGradCAM (Fu et al., [2020](#bib.bib52))  *(bmvc,2020)* |  | $73.7/65.3$
    | $66.4/60.2$ | $62.6/67.1$ | 67.5/64.2 |  | $40.2/22.7$ | $33.0/18.8$ | $24.4/21.1$
    | 32.5/20.8 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| XGradCAM (Fu 等, [2020](#bib.bib52))  *(bmvc,2020)* |  | $73.7/65.3$ | $66.4/60.2$
    | $62.6/67.1$ | 67.5/64.2 |  | $40.2/22.7$ | $33.0/18.8$ | $24.4/21.1$ | 32.5/20.8
    |'
- en: '| LayerCAM (Jiang et al., [2021](#bib.bib74))  *(ieee,2021)* |  | $67.8/67.0$
    | $66.1/65.9$ | $70.9/70.9$ | 68.2/67.9 |  | $34.1/21.8$ | $25.0/20.1$ | $29.1/29.1$
    | 29.4/23.6 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| LayerCAM (Jiang 等, [2021](#bib.bib74))  *(ieee,2021)* |  | $67.8/67.0$ |
    $66.1/65.9$ | $70.9/70.9$ | 68.2/67.9 |  | $34.1/21.8$ | $25.0/20.1$ | $29.1/29.1$
    | 29.4/23.6 |'
- en: '| Fully supervised |  |  |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| Fully supervised |  |  |'
- en: '| U-Net (Ronneberger et al., [2015](#bib.bib118))*(miccai,2015)* |  | $96.8$
    | $95.4$ | $96.4$ | 96.2 |  | $83.0$ | $82.2$ | $83.6$ | 82.9 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| U-Net (Ronneberger 等, [2015](#bib.bib118))*(miccai,2015)* |  | $96.8$ | $95.4$
    | $96.4$ | 96.2 |  | $83.0$ | $82.2$ | $83.6$ | 82.9 |'
- en: 'Table 6: Comparison of localization performance (PxAP) with respect to model
    selection method: B-LOC/B-CL over GlaS and CAMELYON16 test sets. Colors: CL (B-LOC)
    < CL (B-CL) means that localization performance PxAP obtained using B-LOC is worse
    than that obtained using B-CL. CL (B-LOC) > CL (B-CL) means that localization
    performance obtained using B-LOC is better than that obtained using B-CL. Better
    visualized with color. The NEGEV method (Belharbi et al., [2022a](#bib.bib13))
    is not considered in this table because the classifier is pre-trained and frozen.
    Only B-LOC is used for model selection.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：按模型选择方法对定位性能（PxAP）的比较：在GlaS和CAMELYON16测试集上使用B-LOC/B-CL。颜色标记：CL (B-LOC) <
    CL (B-CL)表示使用B-LOC获得的定位性能PxAP低于使用B-CL获得的性能。CL (B-LOC) > CL (B-CL)表示使用B-LOC获得的定位性能高于使用B-CL获得的性能。更好地通过颜色可视化。NEGEV方法（Belharbi
    et al., [2022a](#bib.bib13)）未包含在此表中，因为分类器是预训练并被冻结的。模型选择中仅使用B-LOC。
- en: '|  |  | GlaS |  | CAMELYON16 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GlaS |  | CAMELYON16 |'
- en: '|  |  | VGG | Inception | ResNet | Mean |  | VGG | Inception | ResNet | Mean
    |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  |  | VGG | Inception | ResNet | Mean |  | VGG | Inception | ResNet | Mean
    |'
- en: '| Methods / Metric |  | CL: B-LOC/B-CL |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 方法 / 指标 |  | CL: B-LOC/B-CL |'
- en: '| WSOL |  |  |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| WSOL |  |  |'
- en: '| GAP (Lin et al., [2013](#bib.bib90))  *(corr,2013)* |  | $46.2/98.7$ | $93.7/96.2$
    | $87.5/95.0$ | 75.8/96.6 |  | $50.0/50.0$ | $50.0/86.3$ | $68.1/69.2$ | 56.0/68.5
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| GAP (Lin et al., [2013](#bib.bib90)) *(corr,2013)* |  | $46.2/98.7$ | $93.7/96.2$
    | $87.5/95.0$ | 75.8/96.6 |  | $50.0/50.0$ | $50.0/86.3$ | $68.1/69.2$ | 56.0/68.5
    |'
- en: '| MAX-Pool (Oquab et al., [2015](#bib.bib105))  *(cvpr,2015)* |  | $97.5/97.5$
    | $86.2/93.7$ | $46.2/58.7$ | 76.6/83.3 |  | $50.0/50.0$ | $82.0/86.7$ | $71.4/72.9$
    | 67.8/69.8 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| MAX-Pool (Oquab et al., [2015](#bib.bib105)) *(cvpr,2015)* |  | $97.5/97.5$
    | $86.2/93.7$ | $46.2/58.7$ | 76.6/83.3 |  | $50.0/50.0$ | $82.0/86.7$ | $71.4/72.9$
    | 67.8/69.8 |'
- en: '| LSE (Sun et al., [2016](#bib.bib138))  *(cvpr,2016)* |  | $92.5/93.7$ | $92.5/91.2$
    | $100/96.2$ | 95.0/93.7 |  | $67.8/86.7$ | $50.0/77.8$ | $61.0/86.2$ | 59.6/83.5
    |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| LSE (Sun et al., [2016](#bib.bib138)) *(cvpr,2016)* |  | $92.5/93.7$ | $92.5/91.2$
    | $100/96.2$ | 95.0/93.7 |  | $67.8/86.7$ | $50.0/77.8$ | $61.0/86.2$ | 59.6/83.5
    |'
- en: '| CAM (Zhou et al., [2016](#bib.bib172))  *(cvpr,2016)* |  | $100/100$ | $53.7/88.7$
    | $97.5/98.7$ | 83.7/95.8 |  | $62.2/62.2$ | $51.3/84.7$ | $53.5/71.0$ | 55.6/72.6
    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| CAM (Zhou et al., [2016](#bib.bib172)) *(cvpr,2016)* |  | $100/100$ | $53.7/88.7$
    | $97.5/98.7$ | 83.7/95.8 |  | $62.2/62.2$ | $51.3/84.7$ | $53.5/71.0$ | 55.6/72.6
    |'
- en: '| HaS (Singh and Lee, [2017](#bib.bib128))  *(iccv,2017)* |  | $100/97.5$ |
    $86.2/97.5$ | $93.7/100$ | 93.3/98.3 |  | $62.2/87.5$ | $50.0/50.0$ | $51.0/82.3$
    | 54.4/73.2 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| HaS (Singh and Lee, [2017](#bib.bib128)) *(iccv,2017)* |  | $100/97.5$ |
    $86.2/97.5$ | $93.7/100$ | 93.3/98.3 |  | $62.2/87.5$ | $50.0/50.0$ | $51.0/82.3$
    | 54.4/73.2 |'
- en: '| WILDCAT (Durand et al., [2017](#bib.bib45))  *(cvpr,2017)* |  | $55.0/100$
    | $86.2/95.0$ | $96.2/100$ | 79.1/98.3 |  | $50.0/50.0$ | $50.0/50.0$ | $50.0/77.0$
    | 50.0/59.0 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| WILDCAT (Durand et al., [2017](#bib.bib45)) *(cvpr,2017)* |  | $55.0/100$
    | $86.2/95.0$ | $96.2/100$ | 79.1/98.3 |  | $50.0/50.0$ | $50.0/50.0$ | $50.0/77.0$
    | 50.0/59.0 |'
- en: '| ACoL (Zhang et al., [2018c](#bib.bib167))  *(cvpr,2018)* |  | $100/92.5$
    | $95.0/96.2$ | $46.2/53.7$ | 80.4/80.8 |  | $50.0/86.3$ | $50.0/86.4$ | $50.0/50.0$
    | 50.0/74.2 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| ACoL (Zhang et al., [2018c](#bib.bib167)) *(cvpr,2018)* |  | $100/92.5$ |
    $95.0/96.2$ | $46.2/53.7$ | 80.4/80.8 |  | $50.0/86.3$ | $50.0/86.4$ | $50.0/50.0$
    | 50.0/74.2 |'
- en: '| SPG (Zhang et al., [2018d](#bib.bib168))  *(eccv,2018)* |  | $53.7/83.7$
    | $53.7/93.7$ | $72.5/97.5$ | 59.9/91.6 |  | $65.1/65.1$ | $50.0/50.0$ | $49.4/88.5$
    | 54.8/67.8 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| SPG (Zhang et al., [2018d](#bib.bib168)) *(eccv,2018)* |  | $53.7/83.7$ |
    $53.7/93.7$ | $72.5/97.5$ | 59.9/91.6 |  | $65.1/65.1$ | $50.0/50.0$ | $49.4/88.5$
    | 54.8/67.8 |'
- en: '| Deep MIL (Ilse et al., [2018](#bib.bib69))  *(icml,2018)* |  | $96.2/100$
    | $81.2/92.5$ | $98.7/95.0$ | 92.0/95.8 |  | $86.6/87.0$ | $71.3/88.0$ | $88.1/87.8$
    | 82.0/87.6 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| Deep MIL (Ilse et al., [2018](#bib.bib69)) *(icml,2018)* |  | $96.2/100$
    | $81.2/92.5$ | $98.7/95.0$ | 92.0/95.8 |  | $86.6/87.0$ | $71.3/88.0$ | $88.1/87.8$
    | 82.0/87.6 |'
- en: '| PRM (Zhou et al., [2018](#bib.bib173))  *(cvpr,2018)* |  | $96.2/100$ | $53.7/90.0$
    | $96.2/92.5$ | 82.0/94.1 |  | $50.0/50.0$ | $75.5/88.6$ | $50.0/50.8$ | 58.5/63.1
    |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| PRM (Zhou et al., [2018](#bib.bib173)) *(cvpr,2018)* |  | $96.2/100$ | $53.7/90.0$
    | $96.2/92.5$ | 82.0/94.1 |  | $50.0/50.0$ | $75.5/88.6$ | $50.0/50.8$ | 58.5/63.1
    |'
- en: '| ADL (Choe and Shim, [2019](#bib.bib27))  *(cvpr,2019)* |  | $100/100$ | $77.5/93.7$
    | $93.7/95.0$ | 90.4/96.2 |  | $50.0/50.0$ | $50.0/50.1$ | $56.6/84.1$ | 52.2/61.4
    |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| ADL (Choe and Shim, [2019](#bib.bib27)) *(cvpr,2019)* |  | $100/100$ | $77.5/93.7$
    | $93.7/95.0$ | 90.4/96.2 |  | $50.0/50.0$ | $50.0/50.1$ | $56.6/84.1$ | 52.2/61.4
    |'
- en: '| CutMix (Yun et al., [2019](#bib.bib161))  *(eccv,2019)* |  | $100/88.7$ |
    $86.2/95.0$ | $100/96.2$ | 95.4/93.3 |  | $66.8/62.2$ | $80.8/88.2$ | $53.0/70.3$
    | 66.8/73.5 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| CutMix (Yun et al., [2019](#bib.bib161)) *(eccv,2019)* |  | $100/88.7$ |
    $86.2/95.0$ | $100/96.2$ | 95.4/93.3 |  | $66.8/62.2$ | $80.8/88.2$ | $53.0/70.3$
    | 66.8/73.5 |'
- en: '| TS-CAM (Gao et al., [2021](#bib.bib53))  *(corr,2021)* |  | t:$100/100$ |
    b:$92.5/95.0$ | s:$90.0/90.0$ | 94.1/95.0 |  | t:$50.0/54.4$ | b:$48.3/88.1$ |
    s:$50.0/50.0$ | 49.4/64.1 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| TS-CAM (Gao et al., [2021](#bib.bib53))  *(corr,2021)* |  | t:$100/100$ |
    b:$92.5/95.0$ | s:$90.0/90.0$ | 94.1/95.0 |  | t:$50.0/54.4$ | b:$48.3/88.1$ |
    s:$50.0/50.0$ | 49.4/64.1 |'
- en: '| MAXMIN (Belharbi et al., [2022b](#bib.bib14))  *(tmi,2022)* |  | $83.7/100$
    | $50.0/96.2$ | $95.0/100$ | 76.2/98.7 |  | $50.0/50.0$ | $92.4/90.3$ | $89.2/91.3$
    | 77.2/77.2 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| MAXMIN (Belharbi et al., [2022b](#bib.bib14))  *(tmi,2022)* |  | $83.7/100$
    | $50.0/96.2$ | $95.0/100$ | 76.2/98.7 |  | $50.0/50.0$ | $92.4/90.3$ | $89.2/91.3$
    | 77.2/77.2 |'
- en: '| Top-down WSOL |  |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| Top-down WSOL |  |  |'
- en: '| GradCAM (Selvaraju et al., [2017](#bib.bib124))  *(iccv,2017)* |  | $97.5/100$
    | $85.0/93.7$ | $98.7/95.0$ | 93.7/96.2 |  | $40.2/86.6$ | $34.4/88.7$ | $29.1/82.3$
    | 34.5/85.8 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| GradCAM (Selvaraju et al., [2017](#bib.bib124))  *(iccv,2017)* |  | $97.5/100$
    | $85.0/93.7$ | $98.7/95.0$ | 93.7/96.2 |  | $40.2/86.6$ | $34.4/88.7$ | $29.1/82.3$
    | 34.5/85.8 |'
- en: '| GradCAM++ (Chattopadhyay et al., [2018](#bib.bib22))  *(wacv,2018)* |  |
    $97.5/100$ | $87.5/100$ | $53.7/53.7$ | 79.5/84.5 |  | $50.0/62.2$ | $88.9/89.3$
    | $78.6/85.3$ | 72.5/78.9 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| GradCAM++ (Chattopadhyay et al., [2018](#bib.bib22))  *(wacv,2018)* |  |
    $97.5/100$ | $87.5/100$ | $53.7/53.7$ | 79.5/84.5 |  | $50.0/62.2$ | $88.9/89.3$
    | $78.6/85.3$ | 72.5/78.9 |'
- en: '| Smooth-GradCAM++ (Omeiza et al., [2019](#bib.bib104))  *(corr,2019)* |  |
    $100/100$ | $97.5/98.7$ | $97.5/97.5$ | 98.3/98.7 |  | $50.0/62.2$ | $88.5/88.5$
    | $51.0/82.3$ | 63.1/77.6 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| Smooth-GradCAM++ (Omeiza et al., [2019](#bib.bib104))  *(corr,2019)* |  |
    $100/100$ | $97.5/98.7$ | $97.5/97.5$ | 98.3/98.7 |  | $50.0/62.2$ | $88.5/88.5$
    | $51.0/82.3$ | 63.1/77.6 |'
- en: '| XGradCAM (Fu et al., [2020](#bib.bib52))  *(bmvc,2020)* |  | $100/100$ |
    $91.2/91.2$ | $88.7/96.2$ | 93.3/95.8 |  | $82.1/86.6$ | $88.9/81.1$ | $82.3/71.0$
    | 84.4/79.5 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| XGradCAM (Fu et al., [2020](#bib.bib52))  *(bmvc,2020)* |  | $100/100$ |
    $91.2/91.2$ | $88.7/96.2$ | 93.3/95.8 |  | $82.1/86.6$ | $88.9/81.1$ | $82.3/71.0$
    | 84.4/79.5 |'
- en: '| LayerCAM (Jiang et al., [2021](#bib.bib74))  *(ieee,2022)* |  | $100/100$
    | $90.0/97.5$ | $53.7/53.7$ | 81.2/83.7 |  | $85.8/86.6$ | $47.4/62.9$ | $82.1/82.1$
    | 71.7/77.2 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| LayerCAM (Jiang et al., [2021](#bib.bib74))  *(ieee,2022)* |  | $100/100$
    | $90.0/97.5$ | $53.7/53.7$ | 81.2/83.7 |  | $85.8/86.6$ | $47.4/62.9$ | $82.1/82.1$
    | 71.7/77.2 |'
- en: 'Table 7: Comparison of classification accuracy (CL) with different model selection
    method: B-LOC/B-CL on GlaS and CAMELYON16 test sets. Colors: CL (B-LOC) < CL (B-CL)
    means that the classification accuracy obtained using B-LOC is worse than when
    using B-CL. CL (B-LOC) > CL (B-CL) means that the classification accuracy obtained
    using B-LOC is better than when using B-CL. Better visualized with color. The
    NEGEV method (Belharbi et al., [2022a](#bib.bib13)) is not considered in this
    table because the classifier (CAM (Zhou et al., [2016](#bib.bib172))) is pre-trained
    and frozen. Only B-LOC is used for model selection.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：使用不同模型选择方法的分类准确度（CL）比较：GlaS和CAMELYON16测试集上的B-LOC/B-CL。颜色：CL (B-LOC) < CL
    (B-CL) 表示使用B-LOC获得的分类准确度比使用B-CL时差。CL (B-LOC) > CL (B-CL) 表示使用B-LOC获得的分类准确度比使用B-CL时好。更好地通过颜色可视化。由于分类器（CAM (Zhou
    et al., [2016](#bib.bib172)))是预训练和冻结的，因此此表未考虑NEGEV方法 (Belharbi et al., [2022a](#bib.bib13))。仅使用B-LOC进行模型选择。
- en: 5 Conclusion and future directions
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与未来方向
- en: Training deep models for ROI localization in histology images requires costly
    dense annotation. In addition, such labeling is performed by medical experts.
    A weakly supervised object localization framework provides different techniques
    for low-cost training of deep models. Using only image-class annotation, WSOL
    methods can be trained to classify an image and yield a localization of ROIs via
    CAMs. Despite its success, the WSOL framework still faces a major challenge, namely,
    correctly transferring image-class labels to the pixel-level. Moreover, histology
    images present additional challenges over natural images, including their size,
    stain variation, and ambiguity of labels. Most importantly, ROIs in histology
    data are less salient, which makes spotting them much more difficult. This easily
    opens WSOL models up to false positives/negatives.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在组织学图像中训练深度模型以定位ROI需要昂贵的密集注释。此外，这种标注由医学专家执行。弱监督目标定位框架提供了不同的技术，以低成本训练深度模型。仅使用图像类别注释，WSOL方法可以被训练来分类图像并通过CAMs提供ROI的定位。尽管取得了成功，WSOL框架仍面临一个主要挑战，即正确地将图像类别标签转移到像素级。此外，组织学图像相比自然图像还存在额外的挑战，包括图像的大小、染色变化和标签的模糊性。最重要的是，组织学数据中的ROI不够显著，这使得识别它们变得更加困难。这容易使WSOL模型出现假阳性/假阴性。
- en: 'In this work, we have presented a review of several deep WSOL methods covering
    the 2013 to early 2022 period. We divided them into two main categories based
    on the information flow in the model: bottom-up, and top-down methods. The latter
    have seen limited progress, while bottom-up methods are the current driving force
    behind WSOL task. They have undergone several major changes which have greatly
    improved the task. Early works focused on designing different spatial pooling
    functions. However, these methods quickly peaked in term of performance, revealing
    a major limitation to CAMs, namely, under-activation. Subsequent works aimed to
    alleviate this issue and recover the complete object using different techniques
    including: perturbation, self-attention, shallow features, pseudo-annotation,
    and tasks decoupling. Recent state-of-the-art methods combine several of these
    techniques.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们回顾了从2013年到2022年初的几种深度WSOL方法。我们根据模型中的信息流将它们分为两大类：自下而上方法和自上而下方法。后者进展有限，而自下而上方法是当前推动WSOL任务的主要力量。它们经历了几次重大变化，大大改进了任务。早期的工作专注于设计不同的空间池化函数。然而，这些方法很快在性能上达到了顶峰，暴露出CAMs的一个主要限制，即激活不足。随后的工作旨在缓解这一问题，并使用不同的技术恢复完整的对象，包括：扰动、自注意力、浅层特征、伪标注和任务解耦。近期的最先进方法结合了这些技术中的几种。
- en: 'To assess the localization and classification performance of WSOL techniques
    over histology data, we selected representative methods in our taxonomy for experimentation.
    We evaluated them over two public histology datasets: one for colon cancer (GlaS)
    and a second dataset for breast cancer (CAMELYON16), using the standard protocol
    for a WSOL task (Choe et al., [2020](#bib.bib28)). Overall, the results indicate
    poor localization performance, particularly for generic methods that were designed
    and evaluated over natural images. Methods designed considering histology data
    challenges yielded good results. In general, all methods suffer from high false
    positive/negative localization. Furthermore, our analysis showed the following
    issues:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估WSOL技术在组织学数据上的定位和分类性能，我们选择了分类法中的代表性方法进行实验。我们在两个公共组织学数据集上进行了评估：一个是结肠癌数据集（GlaS），另一个是乳腺癌数据集（CAMELYON16），使用了WSOL任务的标准协议（Choe
    et al., [2020](#bib.bib28)）。总体而言，结果显示定位性能较差，特别是对于那些设计和评估过自然图像的通用方法。考虑到组织学数据挑战的设计方法取得了良好的结果。通常，所有方法都存在高假阳性/假阴性定位问题。此外，我们的分析显示了以下问题：
- en: '- Under-activation, where CAMs activate only over a small discriminative region,
    which increases false negatives. This is a documented behavior over natural images.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '- 激活不足，即CAMs仅在一个小的区分区域内激活，从而增加了假阴性。这是在自然图像中记录的行为。'
- en: '- Over-activation, where CAMs activate over the entire image, and increases
    false positives. This is a new behavior of WSOL which is mostly caused by the
    similarity between foreground and background regions. This makes discrimination
    between both regions difficult. The following are common strategies to reduce
    both issues: 1) Use of priors over the region size (Belharbi et al., [2022b](#bib.bib14)).
    The size of both the foreground and background in an image is constrained to be
    as large as possible. However, the analytical solution to their constraint peaks
    when each region covers half of the image. Results showed that such a trivial
    solution does not occur in practice due to the existence of other competing constraints.
    2) Use of pseudo-annotations (Belharbi et al., [2022a](#bib.bib13)). The authors
    used pixel-wise evidence for the foreground and background from a pre-trained
    classifier. This explicitly provides pixel-wise supervision to the model. Since
    the pre-trained classifier could easily yield wrong pseudo-labels, this makes
    the model vulnerable to learning from wrong labels and ties its performance to
    the performance of the pre-trained classifier. A better solution consists in building
    more reliable pseudo-labels. Synthesizing better substitution to full supervision
    via pseudo-annotation is a potential path to explore. Using noisy pseudo-labels
    has already shown interesting results over medical and natural data (Song et al.,
    [2020](#bib.bib130)).'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '- 过度激活，即 CAMs 在整个图像上激活，增加了假阳性。这是 WSOL 的一种新行为，主要是由于前景和背景区域之间的相似性。这使得区分这两个区域变得困难。以下是减少这两个问题的常见策略：1）对区域大小使用先验知识（Belharbi
    et al., [2022b](#bib.bib14)）。图像中的前景和背景大小都被限制为尽可能大。然而，这种约束的解析解决方案在每个区域覆盖图像的一半时达到峰值。结果表明，由于存在其他竞争约束，这种简单的解决方案在实践中并不会出现。2）使用伪标注（Belharbi
    et al., [2022a](#bib.bib13)）。作者使用来自预训练分类器的逐像素前景和背景证据。这明确地为模型提供了逐像素的监督。由于预训练分类器可能会产生错误的伪标签，这使得模型容易受到错误标签的影响，并将其性能与预训练分类器的性能挂钩。更好的解决方案是建立更可靠的伪标签。通过伪标注合成更好的替代全监督的方法是一个值得探索的潜在途径。使用噪声伪标签已经在医学和自然数据中展示了有趣的结果（Song
    et al., [2020](#bib.bib130)）。'
- en: '- Sensitivity to thresholding which is another documented issue for CAMs over
    natural images, which was observed over histology data as well. A typical solution
    is to push CAMs’ activation to be more confident. However, altering the distribution
    of CAMs could deteriorate the classification performance. A possible solution
    is to separate the classification scoring function from CAMs such as in the architecture
    proposed in (Belharbi et al., [2022a](#bib.bib13), [c](#bib.bib15), [2023](#bib.bib16)).'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对于自然图像中 CAMs 的另一个记录问题是对阈值的敏感性，这在组织学数据中也有所观察。一个典型的解决方案是使 CAMs 的激活更加自信。然而，改变
    CAMs 的分布可能会恶化分类性能。一个可能的解决方案是将分类评分函数与 CAMs 分开，如在(Belharbi et al., [2022a](#bib.bib13),
    [c](#bib.bib15), [2023](#bib.bib16))中提出的架构。'
- en: '- Model selection. Using only classification accuracy allowed to select models
    having high classification performance, but poor localization. On the other hand,
    using localization performance for model selection yielded the opposite. This
    is a common issue in the WSOL task. Recent works have suggested separating both
    tasks to obtain a framework that yields the best performance for both. The architecture
    proposed in (Belharbi et al., [2022a](#bib.bib13)) represents a key solution to
    this issue. First, a classifier is trained, and then it is frozen. Next, the localizer
    is trained. When trained properly, the final model is expected to yield the best
    performance in both tasks.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '- 模型选择。仅使用分类准确率来选择模型能够获得高分类性能，但定位效果较差。另一方面，使用定位性能进行模型选择则得到了相反的结果。这是在 WSOL 任务中常见的问题。最近的研究建议将这两个任务分开，以获得一个对两者都能提供最佳性能的框架。在(Belharbi
    et al., [2022a](#bib.bib13))中提出的架构代表了这个问题的一个关键解决方案。首先训练一个分类器，然后将其冻结。接下来，训练定位器。当训练得当时，最终模型预计在两个任务中都能提供最佳性能。'
- en: Our final conclusion in this work is that the localization performance obtained
    with WSOL methods when applied to histology data still lags behind performance
    with full supervision. The methods are still unable to accurately localize ROI,
    mainly due to their non-saliency. We have cited several key issues to be considered
    when designing future WSOL techniques for histology data in order to close the
    performance gap between weakly and fully supervised methods.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终结论是，当WSOL方法应用于组织学数据时，定位性能仍然落后于全监督的性能。这些方法仍无法准确定位ROI，主要是由于其非显著性。我们列举了在设计未来用于组织学数据的WSOL技术时需要考虑的几个关键问题，以缩小弱监督和全监督方法之间的性能差距。
- en: Acknowledgments
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢
- en: This research was supported in part by the Canadian Institutes of Health Research,
    the Natural Sciences and Engineering Research Council of Canada, and the Digital
    Research Alliance of Canada (alliancecan.ca).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分得到了加拿大卫生研究院、加拿大自然科学与工程研究委员会以及加拿大数字研究联盟 (alliancecan.ca) 的支持。
- en: Ethical Standards
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理标准
- en: The work follows appropriate ethical standards in conducting research and writing
    the manuscript, following all applicable laws and regulations regarding treatment
    of animals or human subjects.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作在研究和撰写手稿时遵循了适当的伦理标准，遵守了关于动物或人类受试者处理的所有适用法律和法规。
- en: Conflicts of Interest
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 利益冲突
- en: We declare we don’t have any conflicts of interest.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们声明我们没有任何利益冲突。
- en: References
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Adebayo et al. (2018) J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt,
    and B. Kim. Sanity checks for saliency maps. In *NeurIPS*, 2018.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adebayo 等 (2018) J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt 和
    B. Kim. 对显著性图的理智检查。发表于 *NeurIPS*, 2018年。
- en: Alber et al. (2019) M. Alber, S. Lapuschkin, P. Seegerer, M. Hägele, K.T. Schütt,
    G. Montavon, W. Samek, K.-R. Müller, S. Dähne, and P.-J. Kindermans. innvestigate
    neural networks! *JMLR*, 20:93:1–93:8, 2019.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alber 等 (2019) M. Alber, S. Lapuschkin, P. Seegerer, M. Hägele, K.T. Schütt,
    G. Montavon, W. Samek, K.-R. Müller, S. Dähne 和 P.-J. Kindermans. 研究神经网络！*JMLR*,
    20:93:1–93:8, 2019。
- en: 'Aresta et al. (2018) G. Aresta, T. Araújo, S. Kwok, et al. Bach: Grand challenge
    on breast cancer histology images. *CoRR*, abs/1808.04277, 2018.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aresta 等 (2018) G. Aresta, T. Araújo, S. Kwok 等. 巴赫：乳腺癌组织学图像的重大挑战。*CoRR*, abs/1808.04277,
    2018。
- en: Bahdanau et al. (2015) D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation
    by jointly learning to align and translate. In *ICLR*, 2015.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 等 (2015) D. Bahdanau, K. Cho 和 Y. Bengio. 通过联合学习对齐和翻译进行神经机器翻译。发表于 *ICLR*,
    2015年。
- en: Bamford and Lovell (2001) P. Bamford and B. Lovell. Method for accurate unsupervised
    cell nucleus segmentation. In *Intern. Conf. of the IEEE Engineering in Medicine
    and Biology Society*, 2001.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bamford 和 Lovell (2001) P. Bamford 和 B. Lovell. 准确的无监督细胞核分割方法。发表于 *IEEE医学与生物工程学会国际会议*，2001年。
- en: Bartels et al. (1992) P.H. Bartels, D. Thompson, M. Bibbo, et al. Bayesian belief
    networks in quantitative histopathology. *Analytical and Quantitative Cytology
    and Histology*, 14(6):459–473, 1992.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bartels 等 (1992) P.H. Bartels, D. Thompson, M. Bibbo 等. 定量组织病理学中的贝叶斯信念网络。*Analytical
    and Quantitative Cytology and Histology*, 14(6):459–473, 1992。
- en: Bateson et al. (2019) Mathilde Bateson, Hoel Kervadec, Jose Dolz, Hervé Lombaert,
    and Ismail Ben Ayed. Constrained domain adaptation for segmentation. In *MICCAI*,
    2019.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bateson 等 (2019) Mathilde Bateson, Hoel Kervadec, Jose Dolz, Hervé Lombaert
    和 Ismail Ben Ayed. 用于分割的约束领域适应。发表于 *MICCAI*, 2019年。
- en: 'Bau et al. (2017) D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba. Network
    dissection: Quantifying interpretability of deep visual representations. In *CVPR*,
    2017.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bau 等 (2017) D. Bau, B. Zhou, A. Khosla, A. Oliva 和 A. Torralba. 网络解剖：定量深度视觉表示的可解释性。发表于
    *CVPR*, 2017年。
- en: 'Bearman et al. (2016) A. Bearman, O. Russakovsky, V. Ferrari, et al. What’s
    the point: Semantic segmentation with point supervision. In *ECCV*, 2016.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bearman 等 (2016) A. Bearman, O. Russakovsky, V. Ferrari 等. 要点是什么：具有点监督的语义分割。发表于
    *ECCV*, 2016年。
- en: Beck and Kastner (2009) D.M. Beck and S. Kastner. Top-down and bottom-up mechanisms
    in biasing competition in the human brain. *Vision research*, 49(10):1154–1165,
    Jun 2009.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beck 和 Kastner (2009) D.M. Beck 和 S. Kastner. 人脑中的自上而下和自下而上机制在偏倚竞争中的作用。*Vision
    research*, 49(10):1154–1165, 2009年6月。
- en: Belharbi et al. (2019) S. Belharbi, J. Rony, J. Dolz, I. Ben Ayed, L. McCaffrey,
    and E. Granger. Min-max entropy for weakly supervised pointwise localization.
    *CoRR*, abs/1907.12934, 2019.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belharbi 等 (2019) S. Belharbi, J. Rony, J. Dolz, I. Ben Ayed, L. McCaffrey 和
    E. Granger. 用于弱监督点位定位的最小-最大熵。*CoRR*, abs/1907.12934, 2019。
- en: Belharbi et al. (2021) S. Belharbi, I. Ben Ayed, L. McCaffrey, and E. Granger.
    Deep active learning for joint classification & segmentation with weak annotator.
    In *WACV*, 2021.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belharbi 等人 (2021) S. Belharbi, I. Ben Ayed, L. McCaffrey 和 E. Granger. 深度主动学习用于具有弱标注者的联合分类和分割。发表于
    *弱视计算机视觉会议*，2021年。
- en: Belharbi et al. (2022a) S. Belharbi, M Pedersoli, I. Ben Ayed, L. McCaffrey,
    and E. Granger. Negative evidence matters in interpretable histology image classification.
    In *Medical Imaging with Deep Learning (MIDL)*, 2022a.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belharbi 等人 (2022a) S. Belharbi, M. Pedersoli, I. Ben Ayed, L. McCaffrey 和 E.
    Granger. 可解释的组织学图像分类中的负证据问题。发表于 *深度学习医学成像会议 (MIDL)*，2022a年。
- en: Belharbi et al. (2022b) S. Belharbi, J. Rony, J. Dolz, I. Ben Ayed, L. McCaffrey,
    and E. Granger. Deep interpretable classification and weakly-supervised segmentation
    of histology images via max-min uncertainty. *IEEE Transactions on Medical Imaging*,
    41:702–714, 2022b.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belharbi 等人 (2022b) S. Belharbi, J. Rony, J. Dolz, I. Ben Ayed, L. McCaffrey
    和 E. Granger. 通过最大-最小不确定性进行深度可解释分类和弱监督分割。发表于 *IEEE医学成像学报*，41:702–714，2022b年。
- en: 'Belharbi et al. (2022c) S. Belharbi, A. Sarraf, M. Pedersoli, I. Ben Ayed,
    L. McCaffrey, and E. Granger. F-cam: Full resolution class activation maps via
    guided parametric upscaling. In *WACV*, 2022c.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belharbi 等人 (2022c) S. Belharbi, A. Sarraf, M. Pedersoli, I. Ben Ayed, L. McCaffrey
    和 E. Granger. F-CAM：通过引导参数上采样获得的全分辨率类别激活图。发表于 *弱视计算机视觉会议*，2022c年。
- en: 'Belharbi et al. (2023) S. Belharbi, I. Ben Ayed, L. McCaffrey, and E. Granger.
    Ftcam: Temporal class activation maps for object localization in weakly-labeled
    unconstrained videos. In *WACV*, 2023.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belharbi 等人 (2023) S. Belharbi, I. Ben Ayed, L. McCaffrey 和 E. Granger. FT-CAM：用于弱标记无约束视频中的目标定位的时间类别激活图。发表于
    *弱视计算机视觉会议*，2023年。
- en: Bilgin et al. (2007) C. Bilgin, C. Demir, C. Nagi, et al. Cell-graph mining
    for breast tissue modeling and classification. In *Intern. Conf. of the IEEE Engineering
    in Medicine and Biology Society*, 2007.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bilgin 等人 (2007) C. Bilgin, C. Demir, C. Nagi 等. 用于乳腺组织建模和分类的细胞图挖掘。发表于 *国际医学与生物工程学会会议*，2007年。
- en: Caicedo et al. (2011) J. C. Caicedo, F. A. González, and E. Romero. Content-based
    histopathology image retrieval using a kernel-based semantic annotation framework.
    *Journal of Biomedical Informatics*, 44(4):519–528, 2011.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caicedo 等人 (2011) J. C. Caicedo, F. A. González 和 E. Romero. 使用基于内核的语义注释框架进行基于内容的组织病理图像检索。发表于
    *生物医学信息学杂志*，44(4):519–528，2011年。
- en: Caie et al. (2014) P. D. Caie, A. K. Turnbull, S. M. Farrington, et al. Quantification
    of tumour budding, lymphatic vessel density and invasion through image analysis
    in colorectal cancer. *Journal of Translational Medicine*, 12(1):156, 2014.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caie 等人 (2014) P. D. Caie, A. K. Turnbull, S. M. Farrington 等. 通过图像分析量化结肠癌中的肿瘤芽、淋巴管密度和侵袭。发表于
    *转化医学杂志*，12(1):156，2014年。
- en: 'Cao et al. (2015) C. Cao, X. Liu, Y. Yang, et al. Look and think twice: Capturing
    top-down visual attention with feedback convolutional neural networks. In *ICCV*,
    2015.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等人 (2015) C. Cao, X. Liu, Y. Yang 等. 仔细观察再思考：通过反馈卷积神经网络捕捉自上而下的视觉注意力。发表于
    *国际计算机视觉会议*，2015年。
- en: 'Carbonneau et al. (2018) M.-A. Carbonneau, V. Cheplygina, E. Granger, and G. Gagnon.
    Multiple instance learning: A survey of problem characteristics and applications.
    *Pattern Recognition*, 77:329–353, 2018.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carbonneau 等人 (2018) M.-A. Carbonneau, V. Cheplygina, E. Granger 和 G. Gagnon.
    多实例学习：问题特征和应用的综述。发表于 *模式识别*，77:329–353，2018年。
- en: 'Chattopadhyay et al. (2018) A. Chattopadhyay, A. Sarkar, P. Howlader, and V. N.
    Balasubramanian. Grad-cam++: Generalized gradient-based visual explanations for
    deep convolutional networks. In *WACV*, 2018.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chattopadhyay 等人 (2018) A. Chattopadhyay, A. Sarkar, P. Howlader 和 V. N. Balasubramanian.
    Grad-CAM++：深度卷积网络的广义梯度基础可视化解释。发表于 *弱视计算机视觉会议*，2018年。
- en: 'Chen et al. (2016) H. Chen, X. Qi, L. Yu, et al. Dcan: deep contour-aware networks
    for accurate gland segmentation. In *CVPR*, 2016.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2016) H. Chen, X. Qi, L. Yu 等. DCAN：用于精确腺体分割的深度轮廓感知网络。发表于 *计算机视觉与模式识别会议*，2016年。
- en: Chen et al. (2015) L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A.L.
    Yuille. Semantic image segmentation with deep convolutional nets and fully connected
    crfs. In *ICLR*, 2015.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2015) L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy 和 A.L. Yuille.
    使用深度卷积网络和全连接 CRF 的语义图像分割。发表于 *国际学习表示会议*，2015年。
- en: 'Chen et al. (2018) L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A.L.
    Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous
    convolution, and fully connected crfs. *PAMI*, 40(4):834–848, 2018.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2018) L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy 和 A.L. Yuille.
    Deeplab：通过深度卷积网络、空洞卷积和全连接 CRF 进行语义图像分割。发表于 *PAMI*，40(4):834–848，2018年。
- en: 'Cheplygina et al. (2019) V. Cheplygina, M. de Bruijne, and J.P.W. Pluim. Not-so-supervised:
    A survey of semi-supervised, multi-instance, and transfer learning in medical
    image analysis. *Medical Image Analysis*, 54:280–296, 2019.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheplygina 等 (2019) V. Cheplygina, M. de Bruijne 和 J.P.W. Pluim. 不完全监督：医疗图像分析中的半监督、多实例和迁移学习的调查。发表于
    *Medical Image Analysis*，54:280–296，2019年。
- en: Choe and Shim (2019) J. Choe and H. Shim. Attention-based dropout layer for
    weakly supervised object localization. In *CVPR*, 2019.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choe 和 Shim (2019) J. Choe 和 H. Shim. 基于注意力的 dropout 层用于弱监督目标定位。发表于 *CVPR*，2019年。
- en: Choe et al. (2020) J. Choe, S. J. Oh, S. Lee, S. Chun, Z. Akata, and H. Shim.
    Evaluating weakly supervised object localization methods right. In *CVPR*, 2020.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choe 等 (2020) J. Choe, S. J. Oh, S. Lee, S. Chun, Z. Akata 和 H. Shim. 正确评估弱监督目标定位方法。发表于
    *CVPR*，2020年。
- en: Cireşan et al. (2013) D. C. Cireşan, A. Giusti, L. M. Gambardella, et al. Mitosis
    detection in breast cancer histology images with deep neural networks. In *MICCAI*,
    2013.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cireşan 等 (2013) D. C. Cireşan, A. Giusti, L. M. Gambardella 等. 使用深度神经网络进行乳腺癌组织图像的有丝分裂检测。发表于
    *MICCAI*，2013年。
- en: 'Courtiol et al. (2018) P. Courtiol, E. W. Tramel, M. Sanselme, et al. Classification
    and disease localization in histopathology using only global labels: A weakly-supervised
    approach. *CoRR*, abs/1802.02212, 2018.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Courtiol 等 (2018) P. Courtiol, E. W. Tramel, M. Sanselme 等. 仅使用全局标签进行组织病理学分类和疾病定位：一种弱监督方法。发表于
    *CoRR*，abs/1802.02212，2018年。
- en: Cruz-Roa et al. (2013) A. A. Cruz-Roa, J.E. A. Ovalle, A. Madabhushi, and F.A.G.
    Osorio. A deep learning architecture for image representation, visual interpretability
    and automated basal-cell carcinoma cancer detection. In *MICCAI*, 2013.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cruz-Roa 等 (2013) A. A. Cruz-Roa, J.E. A. Ovalle, A. Madabhushi 和 F.A.G. Osorio.
    一种用于图像表示、视觉可解释性和自动化基底细胞癌检测的深度学习架构。发表于 *MICCAI*，2013年。
- en: Dabkowski and Gal (2017) P. Dabkowski and Y. Gal. Real time image saliency for
    black box classifiers. In *NeurIPS*, 2017.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dabkowski 和 Gal (2017) P. Dabkowski 和 Y. Gal. 黑箱分类器的实时图像显著性。发表于 *NeurIPS*，2017年。
- en: 'Dai et al. (2015) J. Dai, K. He, and J. Sun. Boxsup: Exploiting bounding boxes
    to supervise convolutional networks for semantic segmentation. In *ICCV*, 2015.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等 (2015) J. Dai, K. He 和 J. Sun. Boxsup: 利用边界框监督卷积网络进行语义分割。发表于 *ICCV*，2015年。'
- en: Daisuke and Shumpei (2018) K. Daisuke and I. Shumpei. Machine learning methods
    for histopathological image analysis. *Computational and Structural Biotechnology
    Journal*, 16, 2018.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daisuke 和 Shumpei (2018) K. Daisuke 和 I. Shumpei. 用于组织病理图像分析的机器学习方法。发表于 *计算与结构生物技术期刊*，16，2018年。
- en: De La Torre et al. (2020) J. De La Torre, A. Valls, and D. Puig. A deep learning
    interpretable classifier for diabetic retinopathy disease grading. *Neurocomputing*,
    396:465–476, 2020.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De La Torre 等 (2020) J. De La Torre, A. Valls 和 D. Puig. 用于糖尿病视网膜病变分级的深度学习可解释分类器。发表于
    *Neurocomputing*，396:465–476，2020年。
- en: 'Desai and Ramaswamy (2020) S. Desai and H.G. Ramaswamy. Ablation-cam: Visual
    explanations for deep convolutional network via gradient-free localization. In
    *WACV*, 2020.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Desai 和 Ramaswamy (2020) S. Desai 和 H.G. Ramaswamy. Ablation-cam: 通过无梯度定位对深度卷积网络的可视化解释。发表于
    *WACV*，2020年。'
- en: Desimone (1998) R. Desimone. Visual attention mediated by biased competition
    in extrastriate visual cortex. *Philosophical transactions of the Royal Society
    of London. Series B, Biological sciences*, 353(1373):1245–1255, Aug 1998.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Desimone (1998) R. Desimone. 由偏倚竞争介导的视觉注意力在外侧视皮层中的作用。发表于 *伦敦皇家学会哲学学报. B系列, 生物科学*，353(1373):1245–1255，1998年8月。
- en: Desimone and Duncan (1995) R. Desimone and J. Duncan. Neural mechanisms of selective
    visual attention. *Annual Review of Neuroscience*, 18(1):193–222, 1995.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Desimone 和 Duncan (1995) R. Desimone 和 J. Duncan. 选择性视觉注意的神经机制。发表于 *神经科学年鉴*，18(1):193–222，1995年。
- en: Devries and Taylor (2017) T. Devries and G. W. Taylor. Improved regularization
    of convolutional neural networks with cutout. *CoRR*, abs/1708.04552, 2017.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devries 和 Taylor (2017) T. Devries 和 G. W. Taylor. 通过 cutout 改进卷积神经网络的正则化。发表于
    *CoRR*，abs/1708.04552，2017年。
- en: 'Dimitriou et al. (2019) N. Dimitriou, O. Arandjelović, and P.D. Caie. Deep
    learning for whole slide image analysis: An overview. *Frontiers in Medicine*,
    6:264, 2019.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dimitriou 等 (2019) N. Dimitriou, O. Arandjelović 和 P.D. Caie. 用于全切片图像分析的深度学习：概述。发表于
    *Frontiers in Medicine*，6:264，2019年。
- en: 'Dolz et al. (2018) J. Dolz, C. Desrosiers, and I. Ben Ayed. 3D fully convolutional
    networks for subcortical segmentation in MRI: A large-scale study. *NeuroImage*,
    2018.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dolz 等 (2018) J. Dolz, C. Desrosiers 和 I. Ben Ayed. 用于 MRI 子皮层分割的 3D 完全卷积网络：大规模研究。发表于
    *NeuroImage*，2018年。
- en: 'Dosovitskiy et al. (2021) A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,
    X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,
    and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition
    at scale. In *ICLR*, 2021.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等（2021）A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,
    X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit
    和 N. Houlsby。图像值 16x16 个词：大规模图像识别的 Transformers。发表于 *ICLR*，2021。
- en: Doyle et al. (2006) S. Doyle, C. Rodriguez, A. Madabhushi, et al. Detecting
    prostatic adenocarcinoma from digitized histology using a multi-scale hierarchical
    classification approach. In *Intern. Conf. of the IEEE Engineering in Medicine
    and Biology Society*, 2006.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doyle 等（2006）S. Doyle, C. Rodriguez, A. Madabhushi 等。使用多尺度层次分类方法从数字化组织学中检测前列腺腺癌。发表于
    *Intern. Conf. of the IEEE Engineering in Medicine and Biology Society*，2006。
- en: 'Durand et al. (2016) T. Durand, N. Thome, and M. Cord. Weldon: Weakly supervised
    learning of deep convolutional neural networks. In *CVPR*, 2016.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Durand 等（2016）T. Durand, N. Thome 和 M. Cord。Weldon：弱监督深度卷积神经网络学习。发表于 *CVPR*，2016。
- en: 'Durand et al. (2017) T. Durand, T. Mordan, N. Thome, et al. Wildcat: Weakly
    supervised learning of deep convnets for image classification, pointwise localization
    and segmentation. In *CVPR*, 2017.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Durand 等（2017）T. Durand, T. Mordan, N. Thome 等。Wildcat：深度卷积网络的弱监督学习用于图像分类、逐点定位和分割。发表于
    *CVPR*，2017。
- en: Ehteshami Bejnordi et al. (2017) B. Ehteshami Bejnordi, M. Veta, P. Johannes van
    Diest, et al. Diagnostic Assessment of Deep Learning Algorithms for Detection
    of Lymph Node Metastases in Women With Breast Cancer. *Journal of the American
    Medical Association*, 318(22):2199–2210, 2017.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ehteshami Bejnordi 等（2017）B. Ehteshami Bejnordi, M. Veta, P. Johannes van Diest
    等。针对乳腺癌女性淋巴结转移检测的深度学习算法的诊断评估。*美国医学会杂志*，318(22):2199–2210，2017。
- en: Fan et al. (2020) M. Fan, T. Chakraborti, E. I.-C. Chang, Y. Xu, and J. Rittscher.
    Microscopic fine-grained instance classification through deep attention. In *MICCAI*,
    Lecture Notes in Computer Science, 2020.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等（2020）M. Fan, T. Chakraborti, E. I.-C. Chang, Y. Xu 和 J. Rittscher。通过深度注意实现显微镜细粒度实例分类。发表于
    *MICCAI*，计算机科学讲义，2020。
- en: Feng et al. (2017) X. Feng, J. Yang, A. F. Laine, et al. Discriminative localization
    in cnns for weakly-supervised segmentation of pulmonary nodules. In *MICCAI*,
    2017.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等（2017）X. Feng, J. Yang, A. F. Laine 等。弱监督肺结节分割中的 CNN 识别定位。发表于 *MICCAI*，2017。
- en: Fong et al. (2019) R. Fong, M. Patrick, and A. Vedaldi. Understanding deep networks
    via extremal perturbations and smooth masks. In *ICCV*, 2019.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fong 等（2019）R. Fong, M. Patrick 和 A. Vedaldi。通过极端扰动和光滑掩膜理解深度网络。发表于 *ICCV*，2019。
- en: Fong and Vedaldi (2017) Ruth C. Fong and Andrea Vedaldi. Interpretable explanations
    of black boxes by meaningful perturbation. In *ICCV*, 2017.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fong 和 Vedaldi（2017）Ruth C. Fong 和 Andrea Vedaldi。通过有意义的扰动解释黑箱。发表于 *ICCV*，2017。
- en: 'Frenay and Verleysen (2014) B. Frenay and M. Verleysen. Classification in the
    presence of label noise: A survey. *TNNLS*, 25, 2014.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frenay 和 Verleysen（2014）B. Frenay 和 M. Verleysen。标签噪声存在下的分类：一项调查。*TNNLS*，25，2014。
- en: 'Fu et al. (2020) R. Fu, Q. Hu, X. Dong, Y. Guo, Y. Gao, and B. Li. Axiom-based
    grad-cam: Towards accurate visualization and explanation of cnns. In *BMVC*, 2020.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等（2020）R. Fu, Q. Hu, X. Dong, Y. Guo, Y. Gao 和 B. Li。基于公理的 grad-cam：向准确的
    CNN 可视化和解释迈进。发表于 *BMVC*，2020。
- en: 'Gao et al. (2021) W. Gao, F. Wan, X. Pan, Z. Peng, Q. Tian, Z. Han, B. Zhou,
    and Q. Ye. TS-CAM: token semantic coupled attention map for weakly supervised
    object localization. In *ICCV*, 2021.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2021）W. Gao, F. Wan, X. Pan, Z. Peng, Q. Tian, Z. Han, B. Zhou 和 Q. Ye。TS-CAM：弱监督目标定位的令牌语义耦合注意图。发表于
    *ICCV*，2021。
- en: Gertych et al. (2015) A. Gertych, N. Ing, Z. Ma, et al. Machine learning approaches
    to analyze histological images of tissues from radical prostatectomies. *Computerized
    Medical Imaging and Graphics*, 46, 2015.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gertych 等（2015）A. Gertych, N. Ing, Z. Ma 等。分析来自根治性前列腺切除术组织的组织学图像的机器学习方法。*计算机医学成像与图形*，46，2015。
- en: Ghosal and Shah (2020) S. Ghosal and P. Shah. Interpretable and synergistic
    deep learning for visual explanation and statistical estimations of segmentation
    of disease features from medical images. *CoRR*, abs/2011.05791, 2020.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghosal 和 Shah（2020）S. Ghosal 和 P. Shah。用于可解释和协同深度学习的视觉解释和统计估计疾病特征分割的医学图像。*CoRR*，abs/2011.05791，2020。
- en: Goh et al. (2020) G. SW. Goh, S. Lapuschkin, L. Weber, W. Samek, and A. Binder.
    Understanding integrated gradients with smoothtaylor for deep neural network attribution.
    *CoRR*, abs/2004.10484, 2020.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goh 等（2020）G. SW. Goh, S. Lapuschkin, L. Weber, W. Samek 和 A. Binder。理解深度神经网络归因的平滑梯度。*CoRR*，abs/2004.10484，2020。
- en: Gondal et al. (2017) W. M. Gondal, J. M. Köhler, R. Grzeszick, et al. Weakly-supervised
    localization of diabetic retinopathy lesions in retinal fundus images. In *Int.
    Conf. on Image Processing*, 2017.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gondal 等（2017）W. M. Gondal, J. M. Köhler, R. Grzeszick, 等。糖尿病视网膜病变病灶的弱监督定位。发表于*Int.
    Conf. on Image Processing*，2017年。
- en: Goodfellow et al. (2016) I. Goodfellow, Y. Bengio, and A. Courville. *Deep Learning*.
    MIT Press, 2016. [http://www.deeplearningbook.org](http://www.deeplearningbook.org).
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等（2016）I. Goodfellow, Y. Bengio, 和 A. Courville. *深度学习*。MIT Press，2016年。[http://www.deeplearningbook.org](http://www.deeplearningbook.org)。
- en: 'Gurcan et al. (2006) M. N. Gurcan, T. Pan, H. Shimada, et al. Image analysis
    for neuroblastoma classification: segmentation of cell nuclei. In *Intern. Conf.
    of the IEEE Engineering in Medicine and Biology Society*, 2006.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gurcan 等（2006）M. N. Gurcan, T. Pan, H. Shimada, 等。神经母细胞瘤分类的图像分析：细胞核的分割。发表于*Intern.
    Conf. of the IEEE Engineering in Medicine and Biology Society*，2006年。
- en: 'Gurcan et al. (2009) M. N. Gurcan, L. Boucheron, A. Can, et al. Histopathological
    image analysis: A review. *IEEE reviews in Biomedical Engineering*, 2:147, 2009.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gurcan 等（2009）M. N. Gurcan, L. Boucheron, A. Can, 等。组织病理图像分析：综述。*IEEE reviews
    in Biomedical Engineering*，2：147，2009年。
- en: Hägele et al. (2020) M. Hägele, P. Seegerer, S. Lapuschkin, M. Bockmayr, W. Samek,
    F. Klauschen, K.-R. Müller, and A. Binder. Resolving challenges in deep learning-based
    analyses of histopathological images using explanation methods. *Scientific Reports*,
    10(1):6423, 2020.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hägele 等（2020）M. Hägele, P. Seegerer, S. Lapuschkin, M. Bockmayr, W. Samek,
    F. Klauschen, K.-R. Müller, 和 A. Binder. 使用解释方法解决基于深度学习的组织病理图像分析中的挑战。*Scientific
    Reports*，10（1）：6423，2020年。
- en: Hamilton et al. (1994) P.W. Hamilton, N. Anderson, P.H. Bartels, et al. Expert
    system support using bayesian belief networks in the diagnosis of fine needle
    aspiration biopsy specimens of the breast. *Journal of clinical pathology*, 47,
    1994.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamilton 等（1994）P.W. Hamilton, N. Anderson, P.H. Bartels, 等。使用贝叶斯信念网络的专家系统支持在乳腺细针穿刺活检标本诊断中的应用。*Journal
    of clinical pathology*，47，1994年。
- en: 'Hao et al. (2019) J. Hao, S.C. Kosaraju, N.Z. Tsaku, D.H. Song, and M. Kang.
    Page-net: Interpretable and integrative deep learning for survival analysis using
    histopathological images and genomic data. In *Pacific Symposium on Biocomputing*,
    2019.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao 等（2019）J. Hao, S.C. Kosaraju, N.Z. Tsaku, D.H. Song, 和 M. Kang. Page-net：使用组织病理图像和基因组数据进行可解释和综合的生存分析深度学习。发表于*Pacific
    Symposium on Biocomputing*，2019年。
- en: He et al. (2016) K. He, X. Zhang, S.g Ren, and J. Sun. Deep residual learning
    for image recognition. In *CVPR*, 2016.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2016）K. He, X. Zhang, S.g Ren, 和 J. Sun. 图像识别的深度残差学习。发表于*CVPR*，2016年。
- en: He et al. (2012) L. He, L. R. Long, S. Antani, et al. Histology image analysis
    for carcinoma detection and grading. *Computer Methods and Programs in Biomedicine*,
    107(3):538–556, 2012.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2012）L. He, L. R. Long, S. Antani, 等。用于癌症检测和分级的组织学图像分析。*Computer Methods
    and Programs in Biomedicine*，107（3）：538–556，2012年。
- en: Hipp et al. (2011) J. D. Hipp, A. Fernandez, C. C. Compton, et al. Why a pathology
    image should not be considered as a radiology image. *Journal of Pathology Informatics*,
    2, 2011.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hipp 等（2011）J. D. Hipp, A. Fernandez, C. C. Compton, 等。为什么病理图像不应被视为放射学图像。*Journal
    of Pathology Informatics*，2，2011年。
- en: Hou et al. (2016) L. Hou, D. Samaras, T. M. Kurc, et al. Patch-based convolutional
    neural network for whole slide tissue image classification. In *CVPR*, 2016.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou 等（2016）L. Hou, D. Samaras, T. M. Kurc, 等。基于补丁的卷积神经网络用于全切片组织图像分类。发表于*CVPR*，2016年。
- en: Iizuka et al. (2020) O. Iizuka, F. Kanavati, K. Kato, M. Rambeau, K. Arihiro,
    and M. Tsuneki. Deep learning models for histopathological classification of gastric
    and colonic epithelial tumours. *Scientific Reports*, 10(1):1–11, 2020.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iizuka 等（2020）O. Iizuka, F. Kanavati, K. Kato, M. Rambeau, K. Arihiro, 和 M.
    Tsuneki. 用于胃和结肠上皮肿瘤的组织病理分类的深度学习模型。*Scientific Reports*，10（1）：1–11，2020年。
- en: Ilse et al. (2018) M. Ilse, J. M. Tomczak, and M. Welling. Attention-based deep
    multiple instance learning. In *ICML*, 2018.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilse 等（2018）M. Ilse, J. M. Tomczak, 和 M. Welling. 基于注意力的深度多实例学习。发表于*ICML*，2018年。
- en: Izadyyazdanabadi et al. (2018) M. Izadyyazdanabadi, E. Belykh, C. Cavallo, et al.
    Weakly-supervised learning-based feature localization in confocal laser endomicroscopy
    glioma images. *CoRR*, abs/1804.09428, 2018.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izadyyazdanabadi 等（2018）M. Izadyyazdanabadi, E. Belykh, C. Cavallo, 等。基于弱监督学习的特征定位在共聚焦激光内窥镜胶质瘤图像中的应用。*CoRR*，abs/1804.09428，2018年。
- en: Jaderberg et al. (2015) M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial
    transformer networks. In *NIPS*, 2015.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg 等（2015）M. Jaderberg, K. Simonyan, A. Zisserman, 等。空间变换网络。发表于*NIPS*，2015年。
- en: 'Janowczyk and Madabhushi (2016) A. Janowczyk and A. Madabhushi. Deep learning
    for digital pathology image analysis: A comprehensive tutorial with selected use
    cases. *Journal of Pathology Informatics*, 7, 2016.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Janowczyk 和 Madabhushi（2016）A. Janowczyk 和 A. Madabhushi。数字病理图像分析中的深度学习：带有选定用例的全面教程。*病理信息学杂志*，7，2016。
- en: Jia et al. (2017) Z. Jia, X. Huang, I. Eric, C. Chang, and Y. Xu. Constrained
    deep weak supervision for histopathology image segmentation. *IEEE Transactions
    on Medical Imaging*, 36(11):2376–2388, 2017.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等（2017）Z. Jia, X. Huang, I. Eric, C. Chang, 和 Y. Xu。受限深度弱监督用于组织病理图像分割。*IEEE
    医学成像交易*，36(11):2376–2388，2017。
- en: 'Jiang et al. (2021) P.-T. Jiang, C.-B. Zhang, Q. Hou, M.-M. Cheng, and Y. Wei.
    Layercam: Exploring hierarchical class activation maps for localization. *IEEE
    Trans. Image Process.*, 30:5875–5888, 2021.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2021）P.-T. Jiang, C.-B. Zhang, Q. Hou, M.-M. Cheng, 和 Y. Wei。Layercam：探索用于定位的层次类激活图。*IEEE
    图像处理汇刊*，30:5875–5888，2021。
- en: 'Kandemir and Hamprecht (2015) M. Kandemir and F.A. Hamprecht. Computer-aided
    diagnosis from weak supervision: A benchmarking study. *Computerized Medical Imaging
    and Graphics*, 42:44–50, 2015.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kandemir 和 Hamprecht（2015）M. Kandemir 和 F.A. Hamprecht。基于弱监督的计算机辅助诊断：基准测试研究。*计算机医学成像与图形*，42:44–50，2015。
- en: Kervadec et al. (2019a) H. Kervadec, J. Dolz, E. Granger, and I. Ben Ayed. Curriculum
    semi-supervised segmentation. In *MICCAI*, 2019a.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kervadec 等（2019a）H. Kervadec, J. Dolz, E. Granger, 和 I. Ben Ayed。课程半监督分割。在 *MICCAI*，2019a。
- en: Kervadec et al. (2019b) H. Kervadec, J. Dolz, M. Tang, et al. Constrained-cnn
    losses for weakly supervised segmentation. *Medical Image Analysis*, 54:88–99,
    2019b.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kervadec 等（2019b）H. Kervadec, J. Dolz, M. Tang, 等。用于弱监督分割的约束-cnn 损失。*医学图像分析*，54:88–99，2019b。
- en: 'Khoreva et al. (2017) A. Khoreva, R. Benenson, J.H. Hosang, M. Hein, and B. Schiele.
    Simple does it: Weakly supervised instance and semantic segmentation. In *CVPR*,
    2017.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khoreva 等（2017）A. Khoreva, R. Benenson, J.H. Hosang, M. Hein, 和 B. Schiele。简单有效：弱监督实例和语义分割。在
    *CVPR*，2017。
- en: Ki et al. (2020) M. Ki, Y. Uh, W. Lee, and H. Byun. In-sample contrastive learning
    and consistent attention for weakly supervised object localization. In *ACCV*,
    2020.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ki 等（2020）M. Ki, Y. Uh, W. Lee, 和 H. Byun。样本内对比学习和一致注意力用于弱监督物体定位。在 *ACCV*，2020。
- en: 'Kieffer et al. (2017) B. Kieffer, M. Babaie, S. Kalra, et al. Convolutional
    neural networks for histopathology image classification: Training vs. using pre-trained
    networks. In *Int. Conf. on Image Processing Theory, Tools and Applications*,
    2017.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kieffer 等（2017）B. Kieffer, M. Babaie, S. Kalra, 等。用于组织病理图像分类的卷积神经网络：训练与使用预训练网络。在
    *国际图像处理理论、工具与应用会议*，2017。
- en: Kim et al. (2017) D. Kim, D. Cho, D. Yoo, and I. So Kweon. Two-phase learning
    for weakly supervised object localization. In *ICCV*, 2017.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2017）D. Kim, D. Cho, D. Yoo, 和 I. So Kweon。弱监督物体定位的两阶段学习。在 *ICCV*，2017。
- en: 'Kim et al. (2020) J.-H. Kim, W. Choo, and H.O. Song. Puzzle mix: Exploiting
    saliency and local statistics for optimal mixup. In *ICML*, 2020.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2020）J.-H. Kim, W. Choo, 和 H.O. Song。Puzzle mix：利用显著性和局部统计进行最优混合。在 *ICML*，2020。
- en: Kindermans et al. (2019) P.-J. Kindermans, S. Hooker, J. Adebayo, M. Alber,
    K. T. Schütt, S. Dähne, D. Erhan, and B. Kim. *The (Un)reliability of Saliency
    Methods*, pages 267–280. Springer International Publishing, 2019.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kindermans 等（2019）P.-J. Kindermans, S. Hooker, J. Adebayo, M. Alber, K. T. Schütt,
    S. Dähne, D. Erhan, 和 B. Kim。*显著性方法的（不）可靠性*，第 267–280 页。Springer International
    Publishing，2019。
- en: 'Korbar et al. (2017) B. Korbar, A.M. Olofson, A.P. Miraflor, C.M. Nicka, M.A.
    Suriawinata, L. Torresani, A.A. Suriawinata, and S. Hassanpour. Looking under
    the hood: Deep neural network visualization to interpret whole-slide image analysis
    outcomes for colorectal polyps. In *CVPR workshops*, 2017.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korbar 等（2017）B. Korbar, A.M. Olofson, A.P. Miraflor, C.M. Nicka, M.A. Suriawinata,
    L. Torresani, A.A. Suriawinata, 和 S. Hassanpour。揭开神秘面纱：深度神经网络可视化以解释结直肠息肉的全切片图像分析结果。在
    *CVPR 研讨会*，2017。
- en: Krizhevsky et al. (2012) A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
    classification with deep convolutional neural networks. In *NeurIPS*. 2012.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等（2012）A. Krizhevsky, I. Sutskever, 和 G. E. Hinton。使用深度卷积神经网络进行 Imagenet
    分类。在 *NeurIPS*，2012。
- en: 'Lee et al. (2019) J. Lee, E. Kim, S. Lee, J. Lee, and S. Yoon. Ficklenet: Weakly
    and semi-supervised semantic image segmentation using stochastic inference. In
    *CVPR*, 2019.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2019）J. Lee, E. Kim, S. Lee, J. Lee, 和 S. Yoon。Ficklenet：使用随机推断进行弱监督和半监督语义图像分割。在
    *CVPR*，2019。
- en: 'Li et al. (2018) K. Li, Z. Wu, K.-C. Peng, et al. Tell me where to look: Guided
    attention inference network. In *CVPR*, 2018.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2018）K. Li, Z. Wu, K.-C. Peng, 等。告诉我看哪里：引导注意力推断网络。在 *CVPR*，2018。
- en: Li and Ping (2018) Y. Li and W. Ping. Cancer metastasis detection with neural
    conditional random field. In *Medical Imaging with Deep Learning*, 2018.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Ping（2018）Y. Li 和 W. Ping。使用神经条件随机场检测癌症转移。在 *Medical Imaging with Deep
    Learning*，2018年。
- en: 'Lin et al. (2016) D. Lin, J. Dai, J. Jia, et al. Scribblesup: Scribble-supervised
    convolutional networks for semantic segmentation. In *CVPR*, 2016.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人（2016）D. Lin, J. Dai, J. Jia 等。Scribblesup：用于语义分割的标注监督卷积网络。在 *CVPR*，2016年。
- en: Lin et al. (2013) M. Lin, Q. Chen, and S. Yan. Network in network. *coRR*, abs/1312.4400,
    2013.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人（2013）M. Lin, Q. Chen 和 S. Yan。网络中的网络。*coRR*，abs/1312.4400，2013年。
- en: Litjens et al. (2017) G. Litjens, T. Kooi, B. E. Bejnordi, et al. A survey on
    deep learning in medical image analysis. *Medical Image Analysis*, 42:60 – 88,
    2017.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Litjens 等人（2017）G. Litjens, T. Kooi, B. E. Bejnordi 等。关于医学图像分析中的深度学习的调查。*Medical
    Image Analysis*，42:60 – 88，2017年。
- en: Lu et al. (2020) W. Lu, X. Jia, W. Xie, L. Shen, Y. Zhou, and J. Duan. Geometry
    constrained weakly supervised object localization. In *ECCV*, 2020.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人（2020）W. Lu, X. Jia, W. Xie, L. Shen, Y. Zhou 和 J. Duan。几何约束弱监督目标定位。在 *ECCV*，2020年。
- en: 'Madabhushi (2009) A. Madabhushi. Digital pathology image analysis: opportunities
    and challenges. *Imaging in Medicine*, 1(1):7, 2009.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madabhushi（2009）A. Madabhushi。数字病理图像分析：机遇与挑战。*Imaging in Medicine*，1(1):7，2009年。
- en: 'Mai et al. (2020) J. Mai, M. Yang, and W. Luo. Erasing integrated learning:
    A simple yet effective approach for weakly supervised object localization. In
    *CVPR*, 2020.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mai 等人（2020）J. Mai, M. Yang 和 W. Luo。擦除集成学习：一种简单而有效的弱监督目标定位方法。在 *CVPR*，2020年。
- en: Meethal et al. (2020) A. Meethal, M. Pedersoli, S. Belharbi, and E. Granger.
    Convolutional stn for weakly supervised object localization and beyond. In *ICPR*,
    2020.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meethal 等人（2020）A. Meethal, M. Pedersoli, S. Belharbi 和 E. Granger。用于弱监督目标定位及其他任务的卷积
    STN。在 *ICPR*，2020年。
- en: 'Mungle et al. (2017) T. Mungle, S. Tewary, D.K. Das, et al. Mrf-ann: a machine
    learning approach for automated er scoring of breast cancer immunohistochemical
    images. *Journal of Microscopy*, 267(2):117–129, 2017.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mungle 等人（2017）T. Mungle, S. Tewary, D.K. Das 等。MRF-ANN：一种用于自动化 ER 评分的乳腺癌免疫组化图像的机器学习方法。*Journal
    of Microscopy*，267(2):117–129，2017年。
- en: Murdoch et al. (2019) W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and
    B. Yu. Definitions, methods, and applications in interpretable machine learning.
    *Proceedings of the National Academy of Sciences*, 116(44):22071–22080, 2019.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murdoch 等人（2019）W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl 和 B. Yu。在可解释的机器学习中的定义、方法和应用。*Proceedings
    of the National Academy of Sciences*，116(44):22071–22080，2019年。
- en: Murtaza et al. (2022) S. Murtaza, S. Belharbi, M. Pedersoli, A. Sarraf, and
    E. Granger. Constrained sampling for class-agnostic weakly supervised object localization.
    In *Montreal AI symposium*, 2022.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murtaza 等人（2022）S. Murtaza, S. Belharbi, M. Pedersoli, A. Sarraf 和 E. Granger。用于类无关的弱监督目标定位的约束采样。在
    *Montreal AI symposium*，2022年。
- en: Murtaza et al. (2023) S. Murtaza, S. Belharbi, M. Pedersoli, A. Sarraf, and
    E. Granger. Discriminative sampling of proposals in self-supervised transformers
    for weakly supervised object localization. In *WACV workshop*, 2023.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murtaza 等人（2023）S. Murtaza, S. Belharbi, M. Pedersoli, A. Sarraf 和 E. Granger。自监督变换器中的提案判别采样用于弱监督目标定位。在
    *WACV workshop*，2023年。
- en: 'Naidu and Michael (2020) R. Naidu and J. Michael. SS-CAM: smoothed score-cam
    for sharper visual feature localization. *CoRR*, abs/2006.14255, 2020.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naidu 和 Michael（2020）R. Naidu 和 J. Michael。SS-CAM：平滑分数 CAM，用于更清晰的视觉特征定位。*CoRR*，abs/2006.14255，2020年。
- en: 'Naidu et al. (2020) R. Naidu, A. Ghosh, Y. Maurya, S. R. Nayak K, and S. S.
    Kundu. IS-CAM: integrated score-cam for axiomatic-based explanations. *CoRR*,
    abs/2010.03023, 2020.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naidu 等人（2020）R. Naidu, A. Ghosh, Y. Maurya, S. R. Nayak K 和 S. S. Kundu。IS-CAM：集成的分数
    CAM，用于公理基础的解释。*CoRR*，abs/2010.03023，2020年。
- en: Naik et al. (2007) S. Naik, S. Doyle, A. Madabhushi, et al. Automated gland
    segmentation and gleason grading of prostate histology by integrating low-, high-level
    and domain specific information. In *Workshop on Microscopic Image Analysis with
    Applications in Biology*, 2007.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naik 等人（2007）S. Naik, S. Doyle, A. Madabhushi 等。通过整合低层次、高层次和领域特定信息来实现自动化腺体分割和前列腺组织学的
    Gleason 分级。在 *Workshop on Microscopic Image Analysis with Applications in Biology*，2007年。
- en: Nair and Hinton (2010) V. Nair and G.E. Hinton. Rectified linear units improve
    restricted boltzmann machines. In *ICML*, 2010.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair 和 Hinton（2010）V. Nair 和 G.E. Hinton。整流线性单元改进了限制玻尔兹曼机。在 *ICML*，2010年。
- en: 'Omeiza et al. (2019) D. Omeiza, S. Speakman, C. Cintas, and K. Weldemariam.
    Smooth grad-cam++: An enhanced inference level visualization technique for deep
    convolutional neural network models. *CoRR*, abs/1908.01224, 2019.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Omeiza 等人 (2019) D. Omeiza, S. Speakman, C. Cintas, 和 K. Weldemariam. Smooth
    grad-cam++: 一种增强的深度卷积神经网络模型推理级可视化技术。*CoRR*, abs/1908.01224, 2019。'
- en: Oquab et al. (2015) M. Oquab, L. Bottou, I. Laptev, et al. Is object localization
    for free?-weakly-supervised learning with convolutional neural networks. In *CVPR*,
    2015.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oquab 等人 (2015) M. Oquab, L. Bottou, I. Laptev, 等人. 目标定位是否免费？——使用卷积神经网络进行弱监督学习。在
    *CVPR*, 2015。
- en: Otsu (1979) N. Otsu. A threshold selection method from gray-level histograms.
    *IEEE Transactions on Systems, Man, and Cybernetics*, 9(1):62–66, 1979.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Otsu (1979) N. Otsu. 一种基于灰度直方图的阈值选择方法。*IEEE Transactions on Systems, Man, and
    Cybernetics*, 9(1):62–66, 1979。
- en: Pathak et al. (2015) D. Pathak, P. Krahenbuhl, and T. Darrell. Constrained convolutional
    neural networks for weakly supervised segmentation. In *ICCV*, 2015.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pathak 等人 (2015) D. Pathak, P. Krahenbuhl, 和 T. Darrell. 用于弱监督分割的约束卷积神经网络。在
    *ICCV*, 2015。
- en: 'Petsiuk et al. (2018) V. Petsiuk, A. Das, and K. Saenko. RISE: randomized input
    sampling for explanation of black-box models. In *BMVC*, 2018.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Petsiuk 等人 (2018) V. Petsiuk, A. Das, 和 K. Saenko. RISE: 随机输入采样以解释黑箱模型。在 *BMVC*,
    2018。'
- en: Petsiuk et al. (2020) V. Petsiuk et al. Black-box explanation of object detectors
    via saliency maps. *CoRR*, abs/2006.03204, 2020.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petsiuk 等人 (2020) V. Petsiuk 等人. 通过显著性图对目标检测器的黑箱解释。*CoRR*, abs/2006.03204, 2020。
- en: Petushi et al. (2006) S. Petushi, F. U. Garcia, M. M. Haber, et al. Large-scale
    computations on histology images reveal grade-differentiating parameters for breast
    cancer. *BMC medical imaging*, 6(1):14, 2006.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petushi 等人 (2006) S. Petushi, F. U. Garcia, M. M. Haber, 等人. 大规模计算在组织学图像中揭示乳腺癌的分级差异参数。*BMC
    medical imaging*, 6(1):14, 2006。
- en: Pinheiro and Collobert (2015) P. O. Pinheiro and R. Collobert. From image-level
    to pixel-level labeling with convolutional networks. In *CVPR*, 2015.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinheiro 和 Collobert (2015) P. O. Pinheiro 和 R. Collobert. 从图像级标记到像素级标记的卷积网络。在
    *CVPR*, 2015。
- en: Quellec et al. (2017) G. Quellec, G. Cazuguel, B. Cochener, and M. Lamard. Multiple-instance
    learning for medical image and video analysis. *IEEE reviews in Biomedical Engineering*,
    10:213–234, 2017.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quellec 等人 (2017) G. Quellec, G. Cazuguel, B. Cochener, 和 M. Lamard. 医学图像和视频分析的多实例学习。*IEEE
    reviews in Biomedical Engineering*, 10:213–234, 2017。
- en: Qureshi et al. (2008) H. Qureshi, O. Sertel, N. Rajpoot, R. Wilson, and M. Gurcan.
    Adaptive discriminant wavelet packet transform and local binary patterns for meningioma
    subtype classification. In *MICCAI*, 2008.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qureshi 等人 (2008) H. Qureshi, O. Sertel, N. Rajpoot, R. Wilson, 和 M. Gurcan.
    自适应判别小波包变换和局部二值模式用于脑膜瘤亚型分类。在 *MICCAI*, 2008。
- en: Rahimi et al. (2020) A. Rahimi, A. Shaban, T. Ajanthan, R.I. Hartley, and B. Boots.
    Pairwise similarity knowledge transfer for weakly supervised object localization.
    In *ECCV*, 2020.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rahimi 等人 (2020) A. Rahimi, A. Shaban, T. Ajanthan, R.I. Hartley, 和 B. Boots.
    用于弱监督目标定位的成对相似性知识迁移。在 *ECCV*, 2020。
- en: 'Redmon et al. (2016) J. Redmon, S. K. Divvala, R. B. Girshick, et al. You only
    look once: Unified, real-time object detection. In *CVPR*, 2016.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Redmon 等人 (2016) J. Redmon, S. K. Divvala, R. B. Girshick, 等人. 你只看一次: 统一的实时目标检测。在
    *CVPR*, 2016。'
- en: 'Ren et al. (2015) S. Ren, K. He, R.B. Girshick, and J. Sun. Faster R-CNN: towards
    real-time object detection with region proposal networks. In *NeurIPS*, 2015.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren 等人 (2015) S. Ren, K. He, R.B. Girshick, 和 J. Sun. Faster R-CNN: 朝着使用区域提议网络的实时目标检测迈进。在
    *NeurIPS*, 2015。'
- en: 'Ribeiro et al. (2016) M. T. Ribeiro, S. Singh, and C. Guestrin. "why should
    i trust you?": Explaining the predictions of any classifier. In *ACM SIGKDD 2016*,
    2016.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ribeiro 等人 (2016) M. T. Ribeiro, S. Singh, 和 C. Guestrin. “我为什么应该相信你?”: 解释任何分类器的预测。在
    *ACM SIGKDD 2016*, 2016。'
- en: 'Ronneberger et al. (2015) O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional
    networks for biomedical image segmentation. In *MICCAI*, 2015.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ronneberger 等人 (2015) O. Ronneberger, P. Fischer, 和 T. Brox. U-net: 用于生物医学图像分割的卷积网络。在
    *MICCAI*, 2015。'
- en: Roux et al. (2013) L. Roux, D. Racoceanu, N. Loménie, et al. Mitosis detection
    in breast cancer histological images an icpr 2012 contest. *Journal of Pathology
    Informatics*, 4, 2013.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roux 等人 (2013) L. Roux, D. Racoceanu, N. Loménie, 等人. 乳腺癌组织学图像中的有丝分裂检测——一个 ICPR
    2012 比赛。*Journal of Pathology Informatics*, 4, 2013。
- en: Saleem et al. (2021) H. Saleem, A. Raza Shahid, and B. Raza. Visual interpretability
    in 3d brain tumor segmentation network. *Comput. Biol. Medicine*, 133:104410,
    2021.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saleem 等人 (2021) H. Saleem, A. Raza Shahid, 和 B. Raza. 3D 脑肿瘤分割网络的视觉可解释性。*Comput.
    Biol. Medicine*, 133:104410, 2021。
- en: 'Samek et al. (2019) W. Samek, G. Montavon, A. Vedaldi, L. K. Hansen, and K.-R.
    Müller, editors. *Explainable AI: Interpreting, Explaining and Visualizing Deep
    Learning*, volume 11700 of *Lecture Notes in Computer Science*. Springer, 2019.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samek 等 (2019) W. Samek, G. Montavon, A. Vedaldi, L. K. Hansen 和 K.-R. Müller，编辑。*可解释人工智能：深度学习的解释、解释和可视化*，*Lecture
    Notes in Computer Science* 第11700卷。Springer，2019。
- en: 'Samek et al. (2020) W. Samek, G. Montavon, S. Lapuschkin, C. J. Anders, and
    K.-R. Müller. Toward interpretable machine learning: Transparent deep neural networks
    and beyond. *CoRR*, abs/2003.07631, 2020.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samek 等 (2020) W. Samek, G. Montavon, S. Lapuschkin, C. J. Anders 和 K.-R. Müller.
    朝着可解释的机器学习迈进：透明的深度神经网络及其超越。*CoRR*，abs/2003.07631，2020。
- en: Sedai et al. (2018) S. Sedai, D. Mahapatra, Z. Ge, et al. Deep multiscale convolutional
    feature learning for weakly supervised localization of chest pathologies in x-ray
    images. In *Intern. Workshop on Machine Learning in Medical Imaging*, 2018.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sedai 等 (2018) S. Sedai, D. Mahapatra, Z. Ge 等。用于 X 射线图像胸部病理学的弱监督定位的深度多尺度卷积特征学习。*Intern.
    Workshop on Machine Learning in Medical Imaging*，2018。
- en: 'Selvaraju et al. (2017) R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam,
    D. Parikh, and D. Batra. Grad-cam: Visual explanations from deep networks via
    gradient-based localization. In *ICCV*, 2017.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Selvaraju 等 (2017) R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh
    和 D. Batra. Grad-cam：通过基于梯度的定位从深度网络中获取视觉解释。*ICCV*，2017。
- en: Shah et al. (2017) M. Shah, D. Wang, C. Rubadue, et al. Deep learning assessment
    of tumor proliferation in breast cancer histological images. In *Int. Conf. on
    Bioinformatics and Biomedicine*, 2017.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shah 等 (2017) M. Shah, D. Wang, C. Rubadue 等。乳腺癌组织学图像中肿瘤增殖的深度学习评估。*Int. Conf.
    on Bioinformatics and Biomedicine*，2017。
- en: Sheikhzadeh et al. (2016) F. Sheikhzadeh, M. Guillaud, and R. K. Ward. Automatic
    labeling of molecular biomarkers of whole slide immunohistochemistry images using
    fully convolutional networks. *CoRR*, abs/1612.09420, 2016.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheikhzadeh 等 (2016) F. Sheikhzadeh, M. Guillaud 和 R. K. Ward. 使用全卷积网络对全幻灯片免疫组织化学图像的分子生物标志物进行自动标记。*CoRR*，abs/1612.09420，2016。
- en: Simonyan and Zisserman (2015) K. Simonyan and A. Zisserman. Very deep convolutional
    networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors,
    *ICLR*, 2015.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman (2015) K. Simonyan 和 A. Zisserman. 用于大规模图像识别的非常深的卷积网络。编辑：Yoshua
    Bengio 和 Yann LeCun，*ICLR*，2015。
- en: 'Singh and Lee (2017) K.K. Singh and Y.J. Lee. Hide-and-seek: Forcing a network
    to be meticulous for weakly-supervised object and action localization. In *ICCV*,
    2017.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 和 Lee (2017) K.K. Singh 和 Y.J. Lee. 藏猫猫：迫使网络在弱监督的物体和动作定位中变得细致。*ICCV*，2017。
- en: 'Sirinukunwattana et al. (2017) K. Sirinukunwattana, J. P.W. Pluim, H. Chen,
    et al. Gland segmentation in colon histology images: The glas challenge contest.
    *Medical Image Analysis*, 35:489–502, 2017.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sirinukunwattana 等 (2017) K. Sirinukunwattana, J. P.W. Pluim, H. Chen 等。结肠组织学图像中的腺体分割：The
    GLAS Challenge 竞赛。*Medical Image Analysis*，35:489–502，2017。
- en: 'Song et al. (2020) H. Song, M. Kim, D. Park, and J.-G. Lee. Learning from noisy
    labels with deep neural networks: A survey. *CoRR*, abs/2007.08199, 2020.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等 (2020) H. Song, M. Kim, D. Park 和 J.-G. Lee. 从嘈杂标签中学习深度神经网络：一项综述。*CoRR*，abs/2007.08199，2020。
- en: Spanhol et al. (2016a) F. A. Spanhol, L. S. Oliveira, C. Petitjean, et al. Breast
    cancer histopathological image classification using convolutional neural networks.
    In *International Joint Conference on Neural Network*, 2016a.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spanhol 等 (2016a) F. A. Spanhol, L. S. Oliveira, C. Petitjean 等。使用卷积神经网络进行乳腺癌组织病理图像分类。*International
    Joint Conference on Neural Network*，2016a。
- en: Spanhol et al. (2016b) F. A. Spanhol, L. S. Oliveira, C. Petitjean, et al. A
    dataset for breast cancer histopathological image classification. *IEEE Transactions
    on Biomedical Engineering*, 63(7):1455–1462, 2016b.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spanhol 等 (2016b) F. A. Spanhol, L. S. Oliveira, C. Petitjean 等。乳腺癌组织病理图像分类数据集。*IEEE
    Transactions on Biomedical Engineering*，63(7):1455–1462，2016b。
- en: 'Srinidhi et al. (2019) C.L. Srinidhi, O. Ciga, and A.L. Martel. Deep neural
    network models for computational histopathology: A survey. *CoRR*, abs/1912.12378,
    2019.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srinidhi 等 (2019) C.L. Srinidhi, O. Ciga 和 A.L. Martel. 用于计算组织病理学的深度神经网络模型：一项综述。*CoRR*，abs/1912.12378，2019。
- en: 'Srivastava et al. (2014) N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,
    and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting.
    *JMLR*, 15(56):1929–1958, 2014.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava 等 (2014) N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever 和
    R. Salakhutdinov. Dropout：一种防止神经网络过拟合的简单方法。*JMLR*，15(56):1929–1958，2014。
- en: 'Stegmüller et al. (2022) T. Stegmüller, A. Spahr, B. Bozorgtabar, and J.-P.
    Thiran. Scorenet: Learning non-uniform attention and augmentation for transformer-based
    histopathological image classification. *CoRR*, abs/2202.07570, 2022.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Stegmüller 等人（2022）T. Stegmüller, A. Spahr, B. Bozorgtabar 和 J.-P. Thiran.
    Scorenet: 学习非均匀注意力和增强用于基于变换器的组织病理学图像分类。*CoRR*，abs/2202.07570，2022。'
- en: Sudharshan et al. (2019) P.J. Sudharshan, C. Petitjean, F. Spanhol, et al. Multiple
    instance learning for histopathological breast cancer image classification. *Expert
    Systems with Applications*, 117:103 – 111, 2019.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sudharshan 等人（2019）P.J. Sudharshan, C. Petitjean, F. Spanhol 等. 用于组织病理学乳腺癌图像分类的多实例学习。*专家系统应用*，117:103
    – 111，2019。
- en: Sukhbaatar et al. (2014) S. Sukhbaatar, J. Bruna, M. Paluri, et al. Training
    convolutional networks with noisy labels. *coRR*, abs/1406.2080, 2014.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sukhbaatar 等人（2014）S. Sukhbaatar, J. Bruna, M. Paluri 等. 用带噪声标签训练卷积网络。*coRR*，abs/1406.2080，2014。
- en: 'Sun et al. (2016) C. Sun, M. Paluri, R. Collobert, et al. Pronet: Learning
    to propose object-specific boxes for cascaded neural networks. In *CVPR*, 2016.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等人（2016）C. Sun, M. Paluri, R. Collobert 等. Pronet: 学习为级联神经网络提出特定目标的框。在
    *CVPR*，2016。'
- en: Szegedy et al. (2016) C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
    Rethinking the inception architecture for computer vision. In *CVPR*, 2016.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等人（2016）C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens 和 Z. Wojna. 重新思考计算机视觉的
    inception 架构。在 *CVPR*，2016。
- en: Tabesh et al. (2007) A. Tabesh, M. Teverovskiy, H.-Y. Pang, et al. Multifeature
    prostate cancer diagnosis and gleason grading of histological images. *IEEE transactions
    on Medical Imaging*, 26(10):1366–1378, 2007.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tabesh 等人（2007）A. Tabesh, M. Teverovskiy, H.-Y. Pang 等. 多特征前列腺癌诊断及组织学图像的 Gleason
    评分。*IEEE 医学成像学报*，26(10):1366–1378，2007。
- en: 'Tang et al. (2009) J. Tang, R. M. Rangayyan, J. Xu, et al. Computer-aided detection
    and diagnosis of breast cancer with mammography: recent advances. *IEEE Transactions
    on Information Technology in Biomedicine*, 13(2):236–251, 2009.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人（2009）J. Tang, R. M. Rangayyan, J. Xu 等. 使用乳腺X光检查的计算机辅助乳腺癌检测和诊断：近期进展。*IEEE
    生物医学信息技术学报*，13(2):236–251，2009。
- en: Tang et al. (2018) M. Tang, A. Djelouah, F. Perazzi, Y. Boykov, and C. Schroers.
    Normalized Cut Loss for Weakly-supervised CNN Segmentation. In *CVPR*, 2018.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人（2018）M. Tang, A. Djelouah, F. Perazzi, Y. Boykov 和 C. Schroers. 用于弱监督
    CNN 分割的归一化剪切损失。在 *CVPR*，2018。
- en: Tavolara et al. (2020) T.E. Tavolara, M.K.K. Niazi, M. Ginese, C. Piedra-Mora,
    D. M. Gatti, G. Beamer, and M. N. Gurcan. Automatic discovery of clinically interpretable
    imaging biomarkers for mycobacterium tuberculosis supersusceptibility using deep
    learning. *EBioMedicine*, 62:103094, 2020.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tavolara 等人（2020）T.E. Tavolara, M.K.K. Niazi, M. Ginese, C. Piedra-Mora, D.
    M. Gatti, G. Beamer 和 M. N. Gurcan. 利用深度学习自动发现临床可解释的结核分枝杆菌超敏感性成像生物标志物。*EBioMedicine*，62:103094，2020。
- en: Teh et al. (2016) E.u. W. Teh, M. Rochan, and Y. Wang. Attention networks for
    weakly supervised object localization. In *BMVC*, 2016.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Teh 等人（2016）E.u. W. Teh, M. Rochan 和 Y. Wang. 用于弱监督目标定位的注意力网络。在 *BMVC*，2016。
- en: Touvron et al. (2021) H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles,
    and H. Jegou. Training data-efficient image transformers &; distillation through
    attention. In *ICML*, 2021.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2021）H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles 和 H.
    Jegou. 训练数据高效的图像变换器 &; 通过注意力进行蒸馏。在 *ICML*，2021。
- en: Tsotsos et al. (1995) J.K. Tsotsos, S. M. Culhane, W.Y.K. Wai, Y. Lai, N. Davis,
    and F. Nuflo. Modeling visual attention via selective tuning. *Artificial Intelligence*,
    78(1):507–545, 1995. Special Volume on Computer Vision.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsotsos 等人（1995）J.K. Tsotsos, S. M. Culhane, W.Y.K. Wai, Y. Lai, N. Davis 和
    F. Nuflo. 通过选择性调节建模视觉注意力。*人工智能*，78(1):507–545，1995。计算机视觉特刊。
- en: 'Uddin et al. (2021) A.F.M. Shahab Uddin, M.S. Monira, W. Shin, T. Chung, and
    S.-H. Bae. Saliencymix: A saliency guided data augmentation strategy for better
    regularization. In *ICLR*, 2021.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Uddin 等人（2021）A.F.M. Shahab Uddin, M.S. Monira, W. Shin, T. Chung 和 S.-H. Bae.
    Saliencymix: 一种以显著性为指导的数据增强策略以实现更好的正则化。在 *ICLR*，2021。'
- en: 'Veta et al. (2014) M. Veta, J.P.W. Pluim, P. J. Van Diest, et al. Breast cancer
    histopathology image analysis: A review. *IEEE Transactions on Biomedical Engineering*,
    61(5):1400–1411, 2014.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Veta 等人（2014）M. Veta, J.P.W. Pluim, P. J. Van Diest 等. 乳腺癌组织病理学图像分析：综述。*IEEE
    生物医学工程学报*，61(5):1400–1411，2014。
- en: Wan et al. (2018) F. Wan, P. Wei, J. Jiao, et al. Min-entropy latent model for
    weakly supervised object detection. In *CVPR*, 2018.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人（2018）F. Wan, P. Wei, J. Jiao 等. 用于弱监督目标检测的最小熵潜在模型。在 *CVPR*，2018。
- en: Wang et al. (2015) D. Wang, D. J. Foran, J. Ren, et al. Exploring automatic
    prostate histopathology image gleason grading via local structure modeling. In
    *Int. Conf. the IEEE Engineering in Medicine and Biology Society*, 2015.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2015) D. Wang, D. J. Foran, J. Ren, 等. 探索通过局部结构建模的自动前列腺组织病理图像Gleason分级。发表于
    *IEEE工程医学与生物学学会国际会议*，2015。
- en: 'Wang et al. (2020) H. Wang, Z. Wang, M. Du, F. Yang, Z. Zhang, S. Ding, P. Mardziel,
    and X. Hu. Score-cam: Score-weighted visual explanations for convolutional neural
    networks. In *CVPR workshop*, 2020.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2020) H. Wang, Z. Wang, M. Du, F. Yang, Z. Zhang, S. Ding, P.
    Mardziel, 和 X. Hu. Score-cam: 评分加权的卷积神经网络视觉解释。发表于 *CVPR workshop*，2020。'
- en: Wang et al. (2018) X. Wang, Y. Yan, P. Tang, X. Bai, and W. Liu. Revisiting
    multiple instance neural networks. *Pattern Recognition*, 74:15–24, 2018.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2018) X. Wang, Y. Yan, P. Tang, X. Bai, 和 W. Liu. 重新审视多实例神经网络。*模式识别*，74：15–24，2018。
- en: Wei et al. (2021) J. Wei, Q. Wang, Z. Li, S. Wang, S. K. Zhou, and S. Cui. Shallow
    feature matters for weakly supervised object localization. In *CVPR*, 2021.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2021) J. Wei, Q. Wang, Z. Li, S. Wang, S. K. Zhou, 和 S. Cui. 浅层特征对弱监督目标定位的重要性。发表于
    *CVPR*，2021。
- en: 'Wei et al. (2017) Y. Wei, J. Feng, X. Liang, M.-M. Cheng, Y. Zhao, and S. Yan.
    Object region mining with adversarial erasing: A simple classification to semantic
    segmentation approach. In *CVPR*, 2017.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2017) Y. Wei, J. Feng, X. Liang, M.-M. Cheng, Y. Zhao, 和 S. Yan.
    对抗性擦除下的目标区域挖掘：从分类到语义分割的简单方法。发表于 *CVPR*，2017。
- en: 'Wei et al. (2018) Y. Wei, H. Xiao, H. Shi, Z. Jie, J. Feng, and T.S. Huang.
    Revisiting dilated convolution: A simple approach for weakly- and semi-supervised
    semantic segmentation. In *CVPR*, 2018.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2018) Y. Wei, H. Xiao, H. Shi, Z. Jie, J. Feng, 和 T.S. Huang. 重新审视膨胀卷积：一种简单的方法用于弱监督和半监督语义分割。发表于
    *CVPR*，2018。
- en: 'Weind et al. (1998) K. L. Weind, C. F. Maier, B. K. Rutt, et al. Invasive carcinomas
    and fibroadenomas of the breast: comparison of microvessel distributions–implications
    for imaging modalities. *Radiology*, 208(2):477–483, 1998.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weind et al. (1998) K. L. Weind, C. F. Maier, B. K. Rutt, 等. 乳腺的侵袭性癌症和纤维腺瘤：微血管分布的比较——对成像模式的影响。*放射学*，208(2)：477–483，1998。
- en: Xie et al. (2019) J. Xie, R. Liu, I.V. Joseph Luttrell, et al. Deep learning
    based analysis of histopathological images of breast cancer. *Frontiers in Genetics*,
    10, 2019.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2019) J. Xie, R. Liu, I.V. Joseph Luttrell, 等. 基于深度学习的乳腺癌组织病理图像分析。*遗传学前沿*，10，2019。
- en: Xu et al. (2016) J. Xu, X. Luo, G. Wang, et al. A deep convolutional neural
    network for segmenting and classifying epithelial and stromal regions in histopathological
    images. *Neurocomputing*, 191:214–223, 2016.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2016) J. Xu, X. Luo, G. Wang, 等. 一种深度卷积神经网络用于分割和分类组织病理图像中的上皮和基质区域。*神经计算*，191：214–223，2016。
- en: 'Xue et al. (2019) H. Xue, C. Liu, F. Wan, J. Jiao, X. Ji, and Q. Ye. Danet:
    Divergent activation for weakly supervised object localization. In *ICCV*, 2019.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xue et al. (2019) H. Xue, C. Liu, F. Wan, J. Jiao, X. Ji, 和 Q. Ye. Danet: 用于弱监督目标定位的发散激活。发表于
    *ICCV*，2019。'
- en: Yang et al. (2020) S. Yang, Y. Kim, Y. Kim, and C. Kim. Combinational class
    activation maps for weakly supervised object localization. In *WACV*, 2020.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2020) S. Yang, Y. Kim, Y. Kim, 和 C. Kim. 用于弱监督目标定位的组合类激活图。发表于 *WACV*，2020。
- en: 'Yun et al. (2019) S. Yun, D. Han, S. Chun, S.J. Oh, Y. Yoo, and J. Choe. Cutmix:
    Regularization strategy to train strong classifiers with localizable features.
    In *ICCV*, 2019.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yun et al. (2019) S. Yun, D. Han, S. Chun, S.J. Oh, Y. Yoo, 和 J. Choe. Cutmix:
    一种用可定位特征训练强分类器的正则化策略。发表于 *ICCV*，2019。'
- en: Zeiler and Fergus (2014) M. D. Zeiler and R. Fergus. Visualizing and understanding
    convolutional networks. In *ECCV*, 2014.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeiler and Fergus (2014) M. D. Zeiler 和 R. Fergus. 可视化和理解卷积网络。发表于 *ECCV*，2014。
- en: Zhang et al. (2017) C. Zhang, S. Bengio, M. Hardt, et al. Understanding deep
    learning requires rethinking generalization. *CoRR*, abs/1611.03530, 2017.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2017) C. Zhang, S. Bengio, M. Hardt, 等. 理解深度学习需要重新思考泛化。*CoRR*，abs/1611.03530，2017。
- en: Zhang et al. (2020a) C.-L. Zhang, Y.-H. Cao, and J. Wu. Rethinking the route
    towards weakly supervised object localization. In *CVPR*, 2020a.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020a) C.-L. Zhang, Y.-H. Cao, 和 J. Wu. 重新思考弱监督目标定位的路径。发表于 *CVPR*，2020a。
- en: 'Zhang et al. (2018a) H. Zhang, M. Cissé, Y.N. Dauphin, and D. Lopez-Paz. mixup:
    Beyond empirical risk minimization. In *ICLR*, 2018a.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2018a) H. Zhang, M. Cissé, Y.N. Dauphin, 和 D. Lopez-Paz. mixup:
    超越经验风险最小化。发表于 *ICLR*，2018a。'
- en: Zhang et al. (2018b) J. Zhang, S. A. Bargal, Z. Lin, et al. Top-down neural
    attention by excitation backprop. *International Journal of Computer Vision*,
    126(10):1084–1102, 2018b.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2018b) J. Zhang, S. A. Bargal, Z. Lin, 等. 通过激发反向传播的自上而下神经注意力。*计算机视觉国际期刊*，126(10)：1084–1102，2018b。
- en: Zhang et al. (2018c) X. Zhang, Y. Wei, J. Feng, Y. Yang, and T.S. Huang. Adversarial
    complementary learning for weakly supervised object localization. In *CVPR*, 2018c.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2018c) X. Zhang, Y. Wei, J. Feng, Y. Yang, and T.S. Huang. 对抗补充学习用于弱监督目标定位。在
    *CVPR*，2018c。
- en: Zhang et al. (2018d) X. Zhang, Y. Wei, G. Kang, Y. Yang, and T.S. Huang. Self-produced
    guidance for weakly-supervised object localization. In *ECCV*, 2018d.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2018d) X. Zhang, Y. Wei, G. Kang, Y. Yang, and T.S. Huang. 自生成指导用于弱监督目标定位。在
    *ECCV*，2018d。
- en: Zhang et al. (2020b) X. Zhang, N. Wang, H. Shen, S. Ji, X. Luo, and T. Wang.
    Interpretable deep learning under fire. In *USENIX Security Symposium*, 2020b.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020b) X. Zhang, N. Wang, H. Shen, S. Ji, X. Luo, and T. Wang.
    可解释的深度学习。在 *USENIX Security Symposium*，2020b。
- en: Zhang et al. (2020c) X. Zhang, Y. Wei, and Y. Yang. Inter-image communication
    for weakly supervised localization. In Andrea Vedaldi, Horst Bischof, Thomas Brox,
    and Jan-Michael Frahm, editors, *ECCV*, Lecture Notes in Computer Science, 2020c.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020c) X. Zhang, Y. Wei, 和 Y. Yang. 图像间通信用于弱监督定位。在 Andrea Vedaldi,
    Horst Bischof, Thomas Brox, 和 Jan-Michael Frahm 主编，*ECCV*，计算机科学讲义，2020c。
- en: Zhang et al. (2021) Y. Zhang, P. Tiño, A. Leonardis, and Ke Tang. A survey on
    neural network interpretability. *IEEE Trans. Emerg. Top. Comput. Intell.*, 5(5):726–742,
    2021.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2021) Y. Zhang, P. Tiño, A. Leonardis, 和 Ke Tang. 神经网络可解释性的综述。*IEEE
    Trans. Emerg. Top. Comput. Intell.*，5(5):726–742，2021。
- en: Zhou et al. (2016) B. Zhou, A. Khosla, A. Lapedriza, et al. Learning deep features
    for discriminative localization. In *CVPR*, 2016.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2016) B. Zhou, A. Khosla, A. Lapedriza, 等. 学习深度特征用于判别定位。在 *CVPR*，2016。
- en: Zhou et al. (2018) Y. Zhou, Y. Zhu, Q. Ye, Q. Qiu, and J. Jiao. Weakly supervised
    instance segmentation using class peak response. In *CVPR*, 2018.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2018) Y. Zhou, Y. Zhu, Q. Ye, Q. Qiu, 和 J. Jiao. 使用类别峰值响应的弱监督实例分割。在
    *CVPR*，2018。
- en: 'Zhou (2004) Z.-H. Zhou. Multi-instance learning: A survey. *Department of Computer
    Science & Technology, Nanjing University, Tech. Rep*, 2004.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou (2004) Z.-H. Zhou. 多实例学习：综述。*计算机科学与技术系，南京大学，技术报告*，2004。
- en: Zhou (2017) Z.-H. Zhou. A brief introduction to weakly supervised learning.
    *National Science Review*, 5(1):44–53, 2017.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou (2017) Z.-H. Zhou. 弱监督学习简要介绍。*国家科学评论*，5(1):44–53，2017。
- en: Zhu et al. (2017) Y. Zhu, Y. Zhou, Q. Ye, et al. Soft proposal networks for
    weakly supervised object localization. In *ICCV*, 2017.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2017) Y. Zhu, Y. Zhou, Q. Ye, 等. 用于弱监督目标定位的软提议网络。在 *ICCV*，2017。
- en: A Hyper-parameter search
  id: totrans-524
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数搜索
- en: '[Table 8](#S1.T8 "Table 8 ‣ A Hyper-parameter search ‣ Deep Weakly-Supervised
    Learning Methods for Classification and Localization in Histology Images: A Survey")
    presents the general hyper-parameters used for all methods. [Table 9](#S1.T9 "Table
    9 ‣ A Hyper-parameter search ‣ Deep Weakly-Supervised Learning Methods for Classification
    and Localization in Histology Images: A Survey") holds hyper-parameters for specific
    methods.'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '[表8](#S1.T8 "Table 8 ‣ A Hyper-parameter search ‣ Deep Weakly-Supervised Learning
    Methods for Classification and Localization in Histology Images: A Survey") 展示了所有方法使用的一般超参数。[表9](#S1.T9
    "Table 9 ‣ A Hyper-parameter search ‣ Deep Weakly-Supervised Learning Methods
    for Classification and Localization in Histology Images: A Survey") 则包含了特定方法的超参数。'
- en: 'Table 8: General hyper-parameters.'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：一般超参数。
- en: '| Hyper-parameter | Value |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| Fully sup. model f | U-Net |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| 全监督模型 f | U-Net |'
- en: '| Backbones | VGG16, InceptionV3, ResNet50. |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| 骨干网络 | VGG16, InceptionV3, ResNet50. |'
- en: '| Optimizer | SGD |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | SGD |'
- en: '| Nesterov acceleration | True |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| Nesterov 加速 | 真 |'
- en: '| Momentum | $0.9$ |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| 动量 | $0.9$ |'
- en: '| Weight decay | $0.0001$ |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| 权重衰减 | $0.0001$ |'
- en: '| Learning rate | ${\in\{0.01,0.001,0.1\}}$ |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | ${\in\{0.01,0.001,0.1\}}$ |'
- en: '| Learning rate decay | GlaS: 0.1 each 250 epochs. |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| 学习率衰减 | GlaS: 每250个epoch衰减0.1。 |'
- en: '| CAMELYON16: 0.1 each 5 epochs. |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| CAMELYON16: 每5个epoch衰减0.1。 |'
- en: '| Mini-batch size | $32$ |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| 小批量大小 | $32$ |'
- en: '| Random flip | Horizontal/vertical random flip |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| 随机翻转 | 水平/垂直随机翻转 |'
- en: '| Random color jittering | $\mathrm{Brightness}$, $\mathrm{contrast}$, |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| 随机颜色抖动 | $\mathrm{Brightness}$, $\mathrm{contrast}$, |'
- en: '| and $\mathrm{saturation}$ at $0.5$ and $\mathrm{hue}$ at $0.05$ |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 以及 $0.5$ 的 $\mathrm{saturation}$ 和 $0.05$ 的 $\mathrm{hue}$ |'
- en: '| Image size | Resize image to $225\times 225$. |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| 图像大小 | 将图像调整为 $225\times 225$。 |'
- en: '| Then, crop random patches of $224\times 224$ |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| 然后，裁剪随机补丁 $224\times 224$ |'
- en: '| Learning epochs | GlaS: $1000$, CAMELYON16: $20$ |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| 学习周期 | GlaS: $1000$，CAMELYON16: $20$ |'
- en: 'Table 9: Per-method hyper-parameters. Notation in this table follows the same
    notation in the original papers.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：每种方法的超参数。该表中的符号与原始论文中的符号一致。
- en: '| Hyper-parameter | Value |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| LSE (Sun et al., [2016](#bib.bib138)) | ${q\in\{1,2,3,4,5,6,7,8,9,10\}}$
    |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| LSE (Sun et al., [2016](#bib.bib138)) | ${q\in\{1,2,3,4,5,6,7,8,9,10\}}$
    |'
- en: '| HaS (Singh and Lee, [2017](#bib.bib128)) | Grid size ${\in\{8,16,32,44,56\}}$,
    |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| HaS (Singh 和 Lee，[2017](#bib.bib128)) | 网格大小 ${\in\{8,16,32,44,56\}}$， |'
- en: '| Drop rate ${\in\{0.2,0.3,0.4,0.5,0.6\}}$ |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| 丢弃率 ${\in\{0.2,0.3,0.4,0.5,0.6\}}$ |'
- en: '| WILDCAT (Durand et al., [2017](#bib.bib45)) | ${\alpha\in\{0.1,0.6\}}$ |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| WILDCAT (Durand 等，[2017](#bib.bib45)) | ${\alpha\in\{0.1,0.6\}}$ |'
- en: '| kmax ${\in\{0.1,0.3,0.5,0.6,0.7\}}$ |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| kmax ${\in\{0.1,0.3,0.5,0.6,0.7\}}$ |'
- en: '| kmin ${\in\{0.1,0.2,0.3\}}$ |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| kmin ${\in\{0.1,0.2,0.3\}}$ |'
- en: '| Modalities = 5 |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| 模态 = 5 |'
- en: '| ACoL (Zhang et al., [2018c](#bib.bib167)) | ${\delta\in\{0.5,0.6,0.7,0.8,0.9\}}$
    |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| ACoL (Zhang 等，[2018c](#bib.bib167)) | ${\delta\in\{0.5,0.6,0.7,0.8,0.9\}}$
    |'
- en: '| SPG (Zhang et al., [2018d](#bib.bib168)) | ${\delta_{1h}\in\{0.5,0.7\}}$
    |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| SPG (Zhang 等，[2018d](#bib.bib168)) | ${\delta_{1h}\in\{0.5,0.7\}}$ |'
- en: '| ${\delta_{1l}\in\{0.01,0.05,0.1\}}$ |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| ${\delta_{1l}\in\{0.01,0.05,0.1\}}$ |'
- en: '| ${\delta_{2h}\in\{0.5,0.6,0.7\}}$ |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| ${\delta_{2h}\in\{0.5,0.6,0.7\}}$ |'
- en: '| ${\delta_{2l}\in\{0.01,0.05,0.1\}}$ |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| ${\delta_{2l}\in\{0.01,0.05,0.1\}}$ |'
- en: '| ${\delta_{3h}\in\{0.5,0.6,0.7\}}$ |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| ${\delta_{3h}\in\{0.5,0.6,0.7\}}$ |'
- en: '| ${\delta_{3l}\in\{0.01,0.05,0.1\}}$ |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| ${\delta_{3l}\in\{0.01,0.05,0.1\}}$ |'
- en: '| Deep MIL (Ilse et al., [2018](#bib.bib69)) | Mid-channels = 128\. Gated attention:
    True/False. |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| Deep MIL (Ilse 等，[2018](#bib.bib69)) | 中间通道 = 128\. 门控注意力：真/假。 |'
- en: '| PRM (Zhou et al., [2018](#bib.bib173)) | ${r\in\{3,5,7,9,11,13\}}$ |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| PRM (Zhou 等，[2018](#bib.bib173)) | ${r\in\{3,5,7,9,11,13\}}$ |'
- en: '| Kernel stride ${\in\{1,3,5,7,9,11,13\}}$ |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| 核心步幅 ${\in\{1,3,5,7,9,11,13\}}$ |'
- en: '| ADL (Choe and Shim, [2019](#bib.bib27)) | Drop rate ${\in\{0.,0.25,0.35,0.45,0.50,0.75\}}$
    |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| ADL (Choe 和 Shim，[2019](#bib.bib27)) | 丢弃率 ${\in\{0.,0.25,0.35,0.45,0.50,0.75\}}$
    |'
- en: '| ${\gamma\in\{0.75,0.85,0.90\}}$ |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| ${\gamma\in\{0.75,0.85,0.90\}}$ |'
- en: '| CutMix (Yun et al., [2019](#bib.bib161)) | ${\alpha=1.0}$ |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| CutMix (Yun 等，[2019](#bib.bib161)) | ${\alpha=1.0}$ |'
- en: '| ${\lambda\in\{0.5,0.6,0.7,0.8,0.9,1.\}}$ |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| ${\lambda\in\{0.5,0.6,0.7,0.8,0.9,1.\}}$ |'
- en: '| MAXMIN (Belharbi et al., [2022b](#bib.bib14)) | Same as (Belharbi et al.,
    [2022b](#bib.bib14)) |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| MAXMIN (Belharbi 等，[2022b](#bib.bib14)) | 与 (Belharbi 等，[2022b](#bib.bib14))
    相同 |'
- en: '| NEGEV (Belharbi et al., [2022a](#bib.bib13)) | Same as (Belharbi et al.,
    [2022a](#bib.bib13)) |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| NEGEV (Belharbi 等，[2022a](#bib.bib13)) | 与 (Belharbi 等，[2022a](#bib.bib13))
    相同 |'
- en: B CAMELYON16 protocol for WSL
  id: totrans-569
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B CAMELYON16 协议用于 WSL
- en: 'This appendix provides details on our protocol for creating a WSOL benchmark
    from the CAMELYON16 dataset (Ehteshami Bejnordi et al., [2017](#bib.bib46)). Samples
    are patches from WSIs, and each patch has two levels of annotation:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录提供了从 CAMELYON16 数据集（Ehteshami Bejnordi 等，[2017](#bib.bib46)）创建 WSOL 基准的协议详细信息。样本是来自
    WSIs 的图块，每个图块有两个级别的注释：
- en: •
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Image-level label ${y}$: the class of the patch, where'
  id: totrans-572
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像级标签 ${y}$：图块的类别，其中
- en: ${{y}\in\{\texttt{normal},\texttt{metastatic}\}}$.
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ${{y}\in\{\texttt{normal},\texttt{metastatic}\}}$。
- en: •
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pixel-level label ${\bm{Y}=\{0,1\}^{H^{\text{in}}\times W^{\text{in}}}}$: a
    binary mask where the value $1$ indicates a metastatic pixel, and $0$ a normal
    pixel. For normal patches, this mask will contain $0$ only.'
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 像素级标签 ${\bm{Y}=\{0,1\}^{H^{\text{in}}\times W^{\text{in}}}}$：一个二进制掩码，其中值为 $1$
    表示转移性像素，$0$ 表示正常像素。对于正常图块，此掩码将仅包含 $0$。
- en: First, we split the CAMELYON16 dataset into training, validation, and test sets
    at the *WSI-level*. This prevents patches from the same WSI from ending up in
    different sets. All patches are sampled with the highest resolution from WSI –\ie,
    $\text{level}=0$ in WSI terminology–. Next, we present our methodology of sampling
    metastatic and normal patches.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将 CAMELYON16 数据集拆分为训练集、验证集和测试集，*以WSI级别*进行。这可以防止来自同一 WSI 的图块出现在不同的集合中。所有图块都从
    WSI 中以最高分辨率进行采样——即，WSI 术语中的 $\text{level}=0$。接下来，我们展示采样转移性和正常图块的方法。
- en: Sampling metastatic patches.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 采样转移性图块。
- en: 'Metastatic patches are sampled only from metastatic WSIs around the cancerous
    regions. Sampled patches will have an image-level label, and a pixel-level label.
    The sampling follows these steps:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 转移性图块仅从癌变区域的转移性 WSIs 中采样。采样的图块将有图像级标签和像素级标签。采样遵循以下步骤：
- en: '1.'
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Consider a metastatic WSI.
  id: totrans-580
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑一个转移性 WSI。
- en: '2.'
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Sample a patch $\bm{x}$ with size $(H,W)$.
  id: totrans-582
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 采样大小为 $(H,W)$ 的图块 $\bm{x}$。
- en: '3.'
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Binarize the patch into a mask $\bm{x}^{\text{b}}$ using the OTSU method (Otsu,
    [1979](#bib.bib106)). Pixels with value $1$ indicate tissue.
  id: totrans-584
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 OTSU 方法 (Otsu, [1979](#bib.bib106)) 将图块二值化为掩码 $\bm{x}^{\text{b}}$。值为 $1$
    的像素表示组织。
- en: '4.'
  id: totrans-585
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Let ${p^{\bm{x}^{\text{b}}}_{t}}$ be the tissue percentage within $\bm{x}^{\text{b}}$.
    If ${p^{\bm{x}^{\text{b}}}_{t}<p_{t}}$, discard the patch.
  id: totrans-586
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 ${p^{\bm{x}^{\text{b}}}_{t}}$ 为 $\bm{x}^{\text{b}}$ 内的组织百分比。如果 ${p^{\bm{x}^{\text{b}}}_{t}<p_{t}}$，则丢弃该图块。
- en: '5.'
  id: totrans-587
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Compute the metastatic binary mask $\bm{Y}$ of the patch $\bm{x}$ using the
    pixel-level annotation of the WSI (values of $1$ indicate a metastatic pixel).
  id: totrans-588
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 WSI 的像素级注释计算图块 $\bm{x}$ 的转移性二进制掩码 $\bm{Y}$（值为 $1$ 表示转移性像素）。
- en: '6.'
  id: totrans-589
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Compute the percentage $p^{\bm{x}}_{m}$ of metastatic pixels within $\bm{Y}$.
  id: totrans-590
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算$\bm{Y}$中转移性像素的百分比$p^{\bm{x}}_{m}$。
- en: '7.'
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: If $p^{\bm{x}}_{m}<p_{0}$, discard the patch. Else, keep the patch $\bm{x}$
    and set ${y}=\texttt{metastatic}$ and ${\bm{Y}}$ is its pixel-level annotation.
  id: totrans-592
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果$p^{\bm{x}}_{m}<p_{0}$，则丢弃该图块。否则，保留图块$\bm{x}$，并将${y}=\texttt{metastatic}$，${\bm{Y}}$是其像素级标注。
- en: 'We note that we sample *all* possible metastatic patches from CAMELYON16 using
    the above approach. Sampling using such an approach will lead to a large number
    of metastatic patches with a high percentage of cancerous pixels (patches sampled
    from the center of the cancerous regions). These patches will have their binary
    annotation mask $\bm{Y}$ full of 1s. Using these patches will shadow the performance
    measure of the localization of cancerous regions. To avoid this issue, we propose
    to perform a calibration of the sampled patches in order to remove most such patches.
    We define two categories of metastatic patches:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，我们从CAMELYON16中采样了*所有*可能的转移性图块。使用这种方法进行采样会导致大量癌性像素百分比高的转移性图块（从癌性区域中心采样的图块）。这些图块的二进制标注掩码$\bm{Y}$会充满1s。使用这些图块将掩盖癌性区域定位的性能度量。为了避免这个问题，我们建议对采样的图块进行校准，以去除大多数这样的图块。我们定义了两类转移性图块：
- en: '1.'
  id: totrans-594
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Category 1: Contains patches with ${p_{0}\leq p^{\bm{x}}_{m}}\leq p_{1}$. Such
    patches are rare, and contain only a small region of cancerous pixels. They are
    often located at the edge of the cancerous regions within a WSI.'
  id: totrans-595
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类别 1：包含${p_{0}\leq p^{\bm{x}}_{m}}\leq p_{1}$的图块。这些图块稀少，仅包含少量癌性像素区域。它们通常位于WSI中癌性区域的边缘。
- en: '2.'
  id: totrans-596
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Category 2: Contains patches with ${p^{\bm{x}}_{m}}>p_{1}$. Such patches are
    extremely abundant, and contain a very large region of cancerous pixels (most
    often the entire patch is cancerous). Such patches are often located inside the
    cancerous regions within a WSI.'
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类别 2：包含${p^{\bm{x}}_{m}}>p_{1}$的图块。这些图块极其丰富，包含很大一部分癌性像素（大多数情况下整个图块都是癌性的）。这些图块通常位于WSI中的癌性区域内部。
- en: 'Our calibration method consists in keeping all patches within Category 1, and
    discarding most of the patches in Category 2. To this end, we apply the following
    sampling approach:'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的校准方法包括保留所有类别 1 的图块，并丢弃大部分类别 2 的图块。为此，我们采用以下采样方法：
- en: '1.'
  id: totrans-599
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Assume we have $n$ patches in Category 1. We will sample ${n\times p_{n}}$ patches
    from Category 2, where $p_{n}$ is a predefined percentage.
  id: totrans-600
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假设我们在类别 1 中有$n$个图块。我们将从类别 2 中采样${n\times p_{n}}$个图块，其中$p_{n}$是预定义的百分比。
- en: '2.'
  id: totrans-601
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Compute the histogram of the frequency of the percentage of cancerous pixels
    within all patches, assuming a histogram with $b$ bins.
  id: totrans-602
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算所有图块中癌性像素百分比的频率直方图，假设直方图有$b$个区间。
- en: '3.'
  id: totrans-603
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Among all the bins with $p^{\bm{x}}_{m}>p_{1}$, pick a bin uniformly.
  id: totrans-604
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在所有$p^{\bm{x}}_{m}>p_{1}$的区域中，均匀地选择一个区域。
- en: '4.'
  id: totrans-605
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Pick a patch within that bin uniformly.
  id: totrans-606
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在该区间中均匀地选择一个图块。
- en: This procedure is repeated until we sample ${n\cdot p_{n}}$ patches from Category
    2.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会重复，直到我们从类别 2 中采样到${n\cdot p_{n}}$个图块。
- en: 'In our experiments, patches are not overlapping. We use the following configuration:
    ${p_{0}=20\%}$, ${p_{1}=50\%}$, ${p_{t}=10\%}$, ${p_{n}=1\%}$. The number of bins
    in the histogram is obtained by dividing the interval ${[0,1]}$ with a delta of
    $0.05$. These hyper-parameters are not validated to optimize the performance of
    the models, but are set in a reasonable way to *automatically* sample consistent
    and unbalanced patches without the need to manually check the samples. Patch size
    is set to $512\times 512$. [Figure 10](#S2.F10 "Figure 10 ‣ item Sampling metastatic
    patches. ‣ B CAMELYON16 protocol for WSL ‣ Deep Weakly-Supervised Learning Methods
    for Classification and Localization in Histology Images: A Survey") illustrates
    an example of metastatic patches and their corresponding masks.'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，图块不重叠。我们使用以下配置：${p_{0}=20\%}$，${p_{1}=50\%}$，${p_{t}=10\%}$，${p_{n}=1\%}$。直方图的区间数量是通过将区间${[0,1]}$按$0.05$的增量划分得到的。这些超参数没有经过验证以优化模型的性能，但被合理设置以*自动*采样一致且不平衡的图块，无需手动检查样本。图块大小设置为$512\times
    512$。[图 10](#S2.F10 "图 10 ‣ 项目 采样转移性图块。 ‣ B CAMELYON16协议用于WSL ‣ 深度弱监督学习方法在组织学图像中的分类与定位：一项调查")展示了转移性图块及其对应掩码的示例。
- en: '![Refer to caption](img/9833d674e45cae4d2600d8539dab4cac.png)'
  id: totrans-609
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9833d674e45cae4d2600d8539dab4cac.png)'
- en: 'Figure 10: Example of metastatic patches with size $512\times 512$ sampled
    from CAMELYON16 dataset (WSI: tumor_001.tif). *Top row*: Patches. *Bottom row*:
    Masks of metastatic regions (white color).'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10：来自 CAMELYON16 数据集（WSI: tumor_001.tif）的大小为 $512\times 512$ 的转移性病灶示例。*顶部行*：病灶块。*底部行*：转移区域的掩膜（白色）。'
- en: Sampling normal patches.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 采样正常病灶。
- en: Normal patches are sampled only from normal WSI. A normal patch is sampled randomly
    and uniformly from the WSI (without repetition or overlapping). If the patch has
    enough tissue (${p^{\bm{x}^{\text{b}}}_{t}\geq p_{t}}$), the patch is accepted.
    Tissue mass measurement is performed at $\text{level}=6$ where it is easy for
    the OTSU binarization method to split the tissue from the background. We double-check
    the tissue mass at $\text{level}=0$.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 正常病灶块仅从正常 WSI 中采样。正常病灶块从 WSI 中随机且均匀地采样（不重复或重叠）。如果病灶块有足够的组织 (${p^{\bm{x}^{\text{b}}}_{t}\geq
    p_{t}}$)，则接受该病灶块。组织量测量在 $\text{level}=6$ 进行，此时 OTSU 二值化方法容易将组织与背景分开。我们在 $\text{level}=0$
    进行二次检查组织量。
- en: 'Let us consider a set (train, validation, or test) at patch level. We first
    pick the corresponding metastatic patches from the metastatic WSI, assuming $n_{m}$
    is their total number. Assuming there are $h$ normal WSIs in this set, we sample
    the same number of normal patches as the total number of metastatic ones. In order
    to mix the patches from all the normal WSI, we sample ${\frac{n_{m}}{h}}$ normal
    patches per normal WSI. In our experiment, we use the same setup as in the metastatic
    patches sampling case: ${p_{t}=10\%}$. [Figure 11](#S2.F11 "Figure 11 ‣ item Sampling
    normal patches. ‣ B CAMELYON16 protocol for WSL ‣ Deep Weakly-Supervised Learning
    Methods for Classification and Localization in Histology Images: A Survey") illustrates
    an example of normal patches.'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个在病灶级别的集合（训练、验证或测试）。我们首先从转移性 WSI 中选择相应的转移性病灶块，假设 $n_{m}$ 是它们的总数。假设该集合中有
    $h$ 个正常 WSI，我们采样与转移性病灶总数相同数量的正常病灶块。为了混合所有正常 WSI 中的病灶块，我们从每个正常 WSI 中采样 ${\frac{n_{m}}{h}}$
    个正常病灶块。在我们的实验中，我们使用与转移性病灶采样案例相同的设置：${p_{t}=10\%}$。 [图 11](#S2.F11 "图 11 ‣ 项目 采样正常病灶。
    ‣ B CAMELYON16 WSL 协议 ‣ 深度弱监督学习方法在组织学图像中的分类和定位：综述") 说明了一个正常病灶块的示例。
- en: '![Refer to caption](img/b4fbb779a4b7d73ef907af781585947a.png)'
  id: totrans-614
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/b4fbb779a4b7d73ef907af781585947a.png)'
- en: 'Figure 11: Example of normal patches with size $512\times 512$ sampled from
    CAMELYON16 dataset (WSI: normal_001.tif).'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11：来自 CAMELYON16 数据集（WSI: normal_001.tif）的大小为 $512\times 512$ 的正常病灶示例。'
- en: C Visual results
  id: totrans-616
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C 视觉结果
- en: 'In this section, we provide visual results for the localization of different
    methods using the ResNet50 backbone: [Figure 12](#S3.F12 "Figure 12 ‣ C Visual
    results ‣ Deep Weakly-Supervised Learning Methods for Classification and Localization
    in Histology Images: A Survey"), and [Figure 13](#S3.F13 "Figure 13 ‣ C Visual
    results ‣ Deep Weakly-Supervised Learning Methods for Classification and Localization
    in Histology Images: A Survey") for GlaS test set; and [Figure 14](#S3.F14 "Figure
    14 ‣ C Visual results ‣ Deep Weakly-Supervised Learning Methods for Classification
    and Localization in Histology Images: A Survey") and [Figure 15](#S3.F15 "Figure
    15 ‣ C Visual results ‣ Deep Weakly-Supervised Learning Methods for Classification
    and Localization in Histology Images: A Survey") for CAMELYON16 test set.'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了使用 ResNet50 主干网络的不同方法的定位的视觉结果： [图 12](#S3.F12 "图 12 ‣ C 视觉结果 ‣ 深度弱监督学习方法在组织学图像中的分类和定位：综述")
    和 [图 13](#S3.F13 "图 13 ‣ C 视觉结果 ‣ 深度弱监督学习方法在组织学图像中的分类和定位：综述") 用于 GlaS 测试集； [图
    14](#S3.F14 "图 14 ‣ C 视觉结果 ‣ 深度弱监督学习方法在组织学图像中的分类和定位：综述") 和 [图 15](#S3.F15 "图 15
    ‣ C 视觉结果 ‣ 深度弱监督学习方法在组织学图像中的分类和定位：综述") 用于 CAMELYON16 测试集。
- en: '![Refer to caption](img/c572ff962318bfa65dde782ed39cb927.png)![Refer to caption](img/0bc19a9cc5dda40381ed9ef2df963b52.png)![Refer
    to caption](img/b2fd46414e4d944dbfef685a21a81863.png)![Refer to caption](img/e78a9dc24066fb738b02a89c00e329b2.png)![Refer
    to caption](img/7c1b45957296bb6e283698cd42e3783d.png)![Refer to caption](img/1ab7623f91e6e742a8be385e58977b8d.png)![Refer
    to caption](img/36335d5cf4f973ad27efabb792e31421.png)'
  id: totrans-618
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c572ff962318bfa65dde782ed39cb927.png)![参见说明文字](img/0bc19a9cc5dda40381ed9ef2df963b52.png)![参见说明文字](img/b2fd46414e4d944dbfef685a21a81863.png)![参见说明文字](img/e78a9dc24066fb738b02a89c00e329b2.png)![参见说明文字](img/7c1b45957296bb6e283698cd42e3783d.png)![参见说明文字](img/1ab7623f91e6e742a8be385e58977b8d.png)![参见说明文字](img/36335d5cf4f973ad27efabb792e31421.png)'
- en: 'Figure 12: CAM predictions over benign test samples for GlaS. Ground truth
    ROIs are indicated with a red mask highlighting glands. In all predictions, strong
    CAM’s activations indicate glands. Backbone: ResNet50.'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：GlaS的良性测试样本预测。地面真实区域（ROIs）用红色面罩标出，突出显示腺体。在所有预测中，强烈的CAM激活表明腺体。骨干网络：ResNet50。
- en: '![Refer to caption](img/fcfc744c7d415e1db715b845cae495d6.png)![Refer to caption](img/ff8c7050d3e149c203cb53a86648b799.png)![Refer
    to caption](img/7fce24a1b39311aa79f2e87d06444882.png)![Refer to caption](img/9a7d2dd49a792a11c7d27d671f80b8d7.png)![Refer
    to caption](img/76417fe852ad2813818ebe052af109e7.png)![Refer to caption](img/04c0bb3f3417469a72d3b3d65f7b9ebd.png)![Refer
    to caption](img/36335d5cf4f973ad27efabb792e31421.png)'
  id: totrans-620
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/fcfc744c7d415e1db715b845cae495d6.png)![参见说明文字](img/ff8c7050d3e149c203cb53a86648b799.png)![参见说明文字](img/7fce24a1b39311aa79f2e87d06444882.png)![参见说明文字](img/9a7d2dd49a792a11c7d27d671f80b8d7.png)![参见说明文字](img/76417fe852ad2813818ebe052af109e7.png)![参见说明文字](img/04c0bb3f3417469a72d3b3d65f7b9ebd.png)![参见说明文字](img/36335d5cf4f973ad27efabb792e31421.png)'
- en: 'Figure 13: Predictions over malignant test samples for GlaS. Ground truth ROIs
    are indicated with a red mask highlighting glands. In all predictions, strong
    CAM’s activations indicate glands. Backbone: ResNet50.'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：GlaS的恶性测试样本预测。地面真实区域（ROIs）用红色面罩标出，突出显示腺体。在所有预测中，强烈的CAM激活表明腺体。骨干网络：ResNet50。
- en: '![Refer to caption](img/9b8e119944f42bdd3d57d8920177087a.png)![Refer to caption](img/21016c117f7b3c22c6dbf077bb13b464.png)![Refer
    to caption](img/acc505fe2823bfb8aed60dcb80751d6f.png)![Refer to caption](img/e0e7538b3195d4594181a4bd83e5579c.png)![Refer
    to caption](img/0cedddf273715383632046173d2fec1f.png)![Refer to caption](img/236fdf889d71aec5b8c43e393e225f08.png)![Refer
    to caption](img/46709b431f4e7127711e0944cd7243a3.png)'
  id: totrans-622
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/9b8e119944f42bdd3d57d8920177087a.png)![参见说明文字](img/21016c117f7b3c22c6dbf077bb13b464.png)![参见说明文字](img/acc505fe2823bfb8aed60dcb80751d6f.png)![参见说明文字](img/e0e7538b3195d4594181a4bd83e5579c.png)![参见说明文字](img/0cedddf273715383632046173d2fec1f.png)![参见说明文字](img/236fdf889d71aec5b8c43e393e225f08.png)![参见说明文字](img/46709b431f4e7127711e0944cd7243a3.png)'
- en: 'Figure 14: Predictions over normal test samples for CAMELYON16. Ground truth
    ROIs are indicated with a red mask highlighting metastatic regions. In all predictions,
    strong CAM’s activations indicate metastatic regions. Backbone: ResNet50.'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：CAMELYON16的正常测试样本预测。地面真实区域（ROIs）用红色面罩标出，突出显示转移区域。在所有预测中，强烈的CAM激活表明转移区域。骨干网络：ResNet50。
- en: '![Refer to caption](img/a1799911535883b55f35341a4a69a9c1.png)![Refer to caption](img/0e95747c2cba06c2be46f2fd9132bab1.png)![Refer
    to caption](img/f705dbcb38e8c32f8094e1cc0242e549.png)![Refer to caption](img/d6237c6b64b0dbcd441cae84bf345e4e.png)![Refer
    to caption](img/31337975fea23bc9493cb6b0f645cdff.png)![Refer to caption](img/7502163d180af3a9940b53570b90ada5.png)![Refer
    to caption](img/46709b431f4e7127711e0944cd7243a3.png)'
  id: totrans-624
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/a1799911535883b55f35341a4a69a9c1.png)![参见说明文字](img/0e95747c2cba06c2be46f2fd9132bab1.png)![参见说明文字](img/f705dbcb38e8c32f8094e1cc0242e549.png)![参见说明文字](img/d6237c6b64b0dbcd441cae84bf345e4e.png)![参见说明文字](img/31337975fea23bc9493cb6b0f645cdff.png)![参见说明文字](img/7502163d180af3a9940b53570b90ada5.png)![参见说明文字](img/46709b431f4e7127711e0944cd7243a3.png)'
- en: 'Figure 15: Predictions over metastatic test samples for CAMELYON16. Ground
    truth ROIs are indicated with a red mask highlighting metastatic regions. In all
    predictions, strong CAM’s activations indicate metastatic regions. Backbone: ResNet50.'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：CAMELYON16的转移性测试样本预测。地面真实区域（ROIs）用红色面罩标出，突出显示转移区域。在所有预测中，强烈的CAM激活表明转移区域。骨干网络：ResNet50。
