- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:47:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:47:01'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2204.07756] Visual Attention Methods in Deep Learning: An In-Depth Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2204.07756] 深度学习中的视觉注意力方法：深入调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2204.07756](https://ar5iv.labs.arxiv.org/html/2204.07756)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2204.07756](https://ar5iv.labs.arxiv.org/html/2204.07756)
- en: 'Visual Attention Methods in Deep Learning: An In-Depth Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的视觉注意力方法：深入调查
- en: 'Mohammed Hassanin, Saeed Anwar, Ibrahim Radwan, Fahad S Khan and Ajmal Mian
    Mohammed Hassanin is with UniSA (University of South Australia), and Faculty of
    Computers and Information, Fayoum University, Egypt. E-mail: mohammed.hassanin@unisa.edu.au.
    Saeed Anwar is with College of Engineering and Computer Science, The Australian
    National University, Canberra, Australia. He is also affiliated with Data61, CSIRO
    (The Commonwealth Scientific and Industrial Research Organisation), The University
    of Technology Sydney, and University of Canberra, Australia. Ibrahim Radwan is
    with University of Canberra, Australia. Fahad S. Khan is an Associate Professor
    with Mohammad Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE,
    and Computer Vision Laboratory, Linkoping University, Sweden. Ajmal Mian is a
    Professor of Computer Science with The University of Western Australia, Australia.
    Corresponding Author: Saeed Anwar (E-mail: saeed.anwar@anu.edu.au)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Mohammed Hassanin，Saeed Anwar，Ibrahim Radwan，Fahad S Khan 和 Ajmal Mian Mohammed
    Hassanin 在南澳大学（UniSA）以及埃及法尤姆大学计算机与信息学院工作。电子邮件：mohammed.hassanin@unisa.edu.au。Saeed
    Anwar 在澳大利亚国立大学工程与计算机科学学院工作。他还隶属于 Data61，CSIRO（联邦科学与工业研究组织），悉尼科技大学和堪培拉大学。Ibrahim
    Radwan 在堪培拉大学工作。Fahad S. Khan 是阿布扎比穆罕默德·本·扎耶德人工智能大学和瑞典林雪平大学计算机视觉实验室的副教授。Ajmal
    Mian 是西澳大学计算机科学教授。通讯作者：Saeed Anwar（电子邮件：saeed.anwar@anu.edu.au）
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Inspired by the human cognitive system, attention is a mechanism that imitates
    the human cognitive awareness about specific information, amplifying critical
    details to focus more on the essential aspects of data. Deep learning has employed
    attention to boost performance for many applications. Interestingly, the same
    attention design can suit processing different data modalities and can easily
    be incorporated into large networks. Furthermore, multiple complementary attention
    mechanisms can be incorporated in one network. Hence, attention techniques have
    become extremely attractive. However, the literature lacks a comprehensive survey
    specific to attention techniques to guide researchers in employing attention in
    their deep models. Note that, besides being demanding in terms of training data
    and computational resources, transformers only cover a single category in self-attention
    out of the many categories available. We fill this gap and provide an in-depth
    survey of 50 attention techniques categorizing them by their most prominent features.
    We initiate our discussion by introducing the fundamental concepts behind the
    success of attention mechanism. Next, we furnish some essentials such as the strengths
    and limitations of each attention category, describe their fundamental building
    blocks, basic formulations with primary usage, and applications specifically for
    computer vision. We also discuss the challenges and open questions related to
    attention mechanism in general. Finally, we recommend possible future research
    directions for deep attention.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 受人类认知系统的启发，注意力是一种模仿人类对特定信息的认知意识的机制，通过放大关键细节来更加关注数据的核心方面。深度学习已经利用注意力来提升许多应用的性能。有趣的是，相同的注意力设计适用于处理不同的数据模态，并且可以轻松地融入大型网络中。此外，可以在一个网络中结合多种互补的注意力机制。因此，注意力技术变得极具吸引力。然而，文献中缺乏关于注意力技术的全面调查，以指导研究人员在其深度模型中应用注意力。需要注意的是，除了在训练数据和计算资源方面要求高之外，变压器仅涵盖了自注意力的一个类别，而不是众多可用的类别。我们填补了这一空白，并提供了对50种注意力技术的深入调查，按其最显著的特征进行分类。我们通过介绍注意力机制成功的基本概念来启动讨论。接下来，我们提供一些基本信息，如每种注意力类别的优缺点，描述其基本构建块、基本公式及主要用途，以及计算机视觉中的应用。我们还讨论了与注意力机制相关的挑战和开放问题。最后，我们推荐了深度注意力的未来研究方向。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Attention Mechanisms, Deep Attention, Attention Modules, Attention in Computer
    Vision and Machine learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制，深度注意力，注意力模块，计算机视觉和机器学习中的注意力。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Attention has a natural bond with the human cognitive system. According to cognitive
    science, the human optic nerve receives massive amounts of data, more than it
    can process. Thus, the human brain weighs the input and pays attention only to
    the necessary information. With recent developments in machine learning, more
    specifically, deep learning, and the increasing ability to process large and multiple
    input data streams, researchers have adopted a similar concept in many domains
    and formulated various attention mechanisms to improve the performance of deep
    neural network models in machine translation [[1](#bib.bib1), [2](#bib.bib2)],
    visual recognition [[3](#bib.bib3)], generative models [[4](#bib.bib4)], multi-agent
    reinforcement learning [[5](#bib.bib5)], etc. Over the past decade, deep learning
    has advanced in leaps and bounds, leading to many deep neural network architectures
    capable of learning complex relationships in data. Generally, neural networks
    provide implicit attention to extract meaningful information from the data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力与人类认知系统有自然的联系。根据认知科学，人类的视神经接收大量的数据，超过了它能够处理的能力。因此，人脑会对输入信息进行权衡，仅关注必要的信息。随着机器学习，特别是深度学习的进展，以及处理大量和多重输入数据流的能力的提高，研究人员在许多领域采用了类似的概念，并制定了各种注意力机制，以提高深度神经网络模型在机器翻译 [[1](#bib.bib1),
    [2](#bib.bib2)]、视觉识别 [[3](#bib.bib3)]、生成模型 [[4](#bib.bib4)]、多智能体强化学习 [[5](#bib.bib5)]等方面的性能。在过去十年里，深度学习取得了飞跃式的发展，产生了许多能够学习数据中复杂关系的深度神经网络架构。一般来说，神经网络提供隐式注意力，从数据中提取有意义的信息。
- en: '| ![Refer to caption](img/59c81bee2d256b73204370ff732c5aac.png) | ![Refer to
    caption](img/b00c7ac9debb5343e393f0c02aa118bb.png) | ![Refer to caption](img/e311c519b1fe22bf4d6d32326babd782.png)
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| ![参考标题](img/59c81bee2d256b73204370ff732c5aac.png) | ![参考标题](img/b00c7ac9debb5343e393f0c02aa118bb.png)
    | ![参考标题](img/e311c519b1fe22bf4d6d32326babd782.png) |'
- en: '| Non-self attentions | Self-attention methods | All types of attentions |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 非自注意力 | 自注意力方法 | 所有类型的注意力 |'
- en: 'Figure 1: Visual charts show the increase in the number of attention related
    papers in the top conferences including CVPR, ICCV, ECCV, NeurIPS, ICML, and ICLR.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：视觉图表展示了包括CVPR、ICCV、ECCV、NeurIPS、ICML和ICLR在内的顶级会议中与注意力相关的论文数量的增加。
- en: Explicit attention mechanism in deep learning was first introduced to tackle
    the *forgetting* issue in encoder-decoder architectures designed for the machine
    translation problem [[6](#bib.bib6)]. Since the network’s encoder part focuses
    on generating a representative input vector, the decoder generates the output
    from the representation vector. A bi-directional Recurrent Neural Network (RNN) [[6](#bib.bib6)]
    is employed for solving the *forgetting* issue by generating a context vector
    from the input sequence and then decoding the output based on the context vector
    as well as the previous hidden states. The context vector is computed by a weighted
    sum of the intermediate representations which makes this method an example of
    explicit attention. Moreover, Long-Short-Term-Memory (LSTM) [[7](#bib.bib7)] is
    employed to generate both the context vector and the output. Both methods compute
    the context vector considering all the hidden states of the encoder. However, [[8](#bib.bib8)]
    introduced another idea by getting the attention mechanism to focus on only a
    subset of the hidden states to generate every item in the context vector. This
    was computationally less expensive compared to the previous attention methods
    and shows a trade-off between *global* and *local* attention mechanisms.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的显式注意力机制最早是为了应对在针对机器翻译问题设计的编码器-解码器架构中的*遗忘*问题而引入的 [[6](#bib.bib6)]。由于网络的编码器部分专注于生成代表性的输入向量，解码器则从表示向量生成输出。通过生成上下文向量来解决*遗忘*问题的方法是使用双向递归神经网络（RNN） [[6](#bib.bib6)]，然后基于上下文向量以及先前的隐藏状态解码输出。上下文向量通过对中间表示的加权和来计算，这使得这种方法成为显式注意力的一个例子。此外，长短期记忆（LSTM） [[7](#bib.bib7)]被用于生成上下文向量和输出。这两种方法计算上下文向量时都考虑了编码器的所有隐藏状态。然而， [[8](#bib.bib8)]
    提出了另一种思路，通过使注意力机制只关注隐藏状态的一个子集来生成上下文向量中的每一项。这比以前的注意力方法计算开销更小，展示了*全局*和*局部*注意力机制之间的权衡。
- en: Another attention-based breakthrough was made by Vaswani et al. [[2](#bib.bib2)],
    where an entire architecture was created based on the self-attention mechanism.
    The items in the input sequence are first encoded in parallel into multiple representations
    called key, query, and value. This architecture, coined the Transformer, helps
    capture the importance of each item relative to others in the input sequence more
    effectively. Recently, many researchers have extended the basic Transformer architecture
    for specific applications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个基于注意力的突破是由Vaswani等人[[2](#bib.bib2)]提出的，他们基于自注意力机制创建了一个完整的架构。输入序列中的项目首先被并行编码为多种表示，称为键、查询和值。这个被称为Transformer的架构，帮助更有效地捕捉输入序列中每个项目相对于其他项目的重要性。最近，许多研究人员已将基本的Transformer架构扩展到特定应用中。
- en: To pay attention to the significant parts in an image and suppress unnecessary
    information, advancements of attention-based learning have found their way into
    multiple computer vision tasks, either employing a different attention map for
    every image pixel, comparing it with the representations of other pixels [[3](#bib.bib3),
    [9](#bib.bib9), [4](#bib.bib4)] or generating an attention map to extract the
    global representation for the whole image [[10](#bib.bib10), [11](#bib.bib11)].
    However, the design of attention mechanism is highly dependent on the problem
    at hand. To enforce the selection of hidden states that correspond to the critical
    information in the input, attention techniques have been used as plug-in units
    in vision-based tasks, alleviating the risk of vanishing gradients. To sum up,
    attention scores are calculated, and hidden states are selected either deterministically
    or stochastically.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了关注图像中的重要部分并抑制不必要的信息，基于注意力的学习进展已经渗透到多个计算机视觉任务中，这些任务要么为每个图像像素使用不同的注意力图，将其与其他像素的表示进行比较[[3](#bib.bib3)，[9](#bib.bib9)，[4](#bib.bib4)]，要么生成一个注意力图来提取整个图像的全局表示[[10](#bib.bib10)，[11](#bib.bib11)]。然而，注意力机制的设计高度依赖于具体问题。为了强制选择与输入中关键信息相对应的隐藏状态，注意力技术已作为插件单元应用于基于视觉的任务中，减轻了梯度消失的风险。总而言之，注意力分数被计算，隐藏状态被选择，可能是确定性的也可能是随机的。
- en: '{forest}'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '{森林}'
- en: forked edges, for tree=thick,draw,align=left,edge=-latex,fill=white,blur shadow,
    rounded corners, top color=white, bottom color=blue!20, edge+=-¿, l sep’+=13pt,
    [Attention Types [Soft Attention [Channel [Squeeze $\&amp;$ Excitation
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 分叉边缘，树=厚，绘制，对齐=左，边=-latex，填充=白色，模糊阴影，圆角，上色=白色，下色=蓝色！20，边+=-¿，l sep’+=13pt，[注意力类型[软注意力[通道[挤压$\&amp;$
    激励
- en: Efficient Channel
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 高效通道
- en: Split-Attention
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 分割注意力
- en: Second-Order
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶
- en: High-Order
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 高阶
- en: Harmonious
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 和谐
- en: Auto Learning
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自学习
- en: Double
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 双重
- en: Dual
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 双重
- en: Frequency Channel ] ] [Spatial [Co-attention
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 频率通道]] [空间[共同注意力
- en: $\&amp;$ Co-excitation
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: $\&amp;$ 共同激励
- en: Spatial Pyramid
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 空间金字塔
- en: Spatial-Spectral
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 空间-光谱
- en: Pixel-wise Contextual
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 像素级上下文
- en: Pyramid Feature
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 金字塔特征
- en: Attention Pyramid
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力金字塔
- en: Region ] ] [Self [Transfomers
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 区域]] [自[变换器
- en: Standalone Self
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 独立自我
- en: Clustered
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类
- en: Slot
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 插槽
- en: Efficient
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 高效
- en: Random Feature
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随机特征
- en: Non-local
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 非局部
- en: Sparse
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏
- en: X-Linear
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: X-Linear
- en: Axial
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 轴向
- en: Efficient Mech.] ] [Arithmetic [Attention Dropout
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 高效机制]] [算术[注意力丢弃
- en: Mirror
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像
- en: Reverse ] ] [Muli-Modal [Cross
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 反向]] [多模态[交叉
- en: Criss-Cross
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉交错
- en: Perceiver
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器
- en: Stacked Cross
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠交叉
- en: Boosted ] ] [Logical [Sequential
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 提升]] [逻辑[顺序
- en: Permut. Invariant
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 排列不变
- en: Show-Attend-Tell
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 展示-关注-讲述
- en: Kalman Filtering
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 卡尔曼滤波
- en: Prophet ] ] ] [Hard Attention [Statistical [Bayesian [Belief Net.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 预言者]]]] [硬注意力[统计[贝叶斯[信念网络。
- en: Repulsive ] ] [Variational] ] [Reinforced-based [Self-Critical
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 排斥]] [变分]] [强化型[自我批判
- en: Reinforced-SA] ] [Gaussian [Self-Supervised
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 强化-SA]] [高斯[自我监督
- en: Uncertainty-Aware] ] [Clustering] ] ]
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性感知]] [聚类]]]]
- en: 'Figure 2: A taxonomy of attention types. The attentions are categorized based
    on the methodology adopted to perform attention. Some of the attention techniques
    can be accommodated in multiple categories; in this case, the attention is grouped
    based on the most dominant characteristic and primary application.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：注意力类型的分类法。这些注意力根据执行注意力的方法进行分类。一些注意力技术可以适应多个类别；在这种情况下，注意力会根据最主要的特征和主要应用进行分组。
- en: Attention has been the center of significant research efforts over the past
    few years and image attention has been flourishing in many different machine learning
    and vision applications, for example, classification [[12](#bib.bib12)], detection [[13](#bib.bib13)],
    image captioning [[14](#bib.bib14)], 3D analysis [[15](#bib.bib15)], etc. Despite
    the impressive performance of attention techniques employed in deep learning,
    there is no literature survey that comprehensively reviews all, especially deep
    learning based, attention mechanisms in vision to categorize them based on their
    basic underlying architectures and highlight their strengths and weaknesses. Recently,
    researchers surveyed application-specific attention techniques with emphasis on
    NLP-based [[16](#bib.bib16)], transformer-based [[17](#bib.bib17), [18](#bib.bib18)],
    and graph-based approaches [[19](#bib.bib19)]. However, no comprehensive study
    collates with the huge and diverse scope of all deep learning based attention
    techniques developed for visual inputs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，注意力机制成为了重要的研究中心，图像注意力在许多不同的机器学习和视觉应用中蓬勃发展，例如分类[[12](#bib.bib12)]、检测[[13](#bib.bib13)]、图像描述[[14](#bib.bib14)]、3D分析[[15](#bib.bib15)]等。尽管深度学习中应用的注意力技术表现出色，但目前尚无文献综述全面回顾所有，尤其是基于深度学习的视觉注意力机制，以便根据其基本结构进行分类，并突显其优缺点。最近，研究人员调查了应用特定的注意力技术，重点关注基于NLP的[[16](#bib.bib16)]、基于Transformer的[[17](#bib.bib17),
    [18](#bib.bib18)]和基于图的[[19](#bib.bib19)]方法。然而，尚无全面研究汇总所有为视觉输入开发的深度学习注意力技术。
- en: 'In this article, we review attention techniques specific to vision. Our survey
    covers the numerous basic building blocks (operations and functions) and complete
    architectures designed to learn suitable representations while making the models
    attentive to the relevant and important information in the input images or videos.
    Our survey broadly classifies attention mechanisms proposed in the computer vision
    literature including soft attention, hard attention, multi-modal, arithmetic,
    class attention, and logical attention. We note that some methods belong to more
    than one category; however, we assign each method to the category where it has
    a dominant association with other methods of that category. Following such a categorization
    helps track the common attention mechanism characteristics and offers insights
    that can potentially help in designing novel attention techniques. Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Visual Attention Methods in Deep Learning: An In-Depth
    Survey") shows the classification of the attention mechanisms.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们回顾了视觉特定的注意力技术。我们的综述涵盖了许多基本构建块（操作和功能）以及旨在学习适当表示的完整架构，同时使模型对输入图像或视频中的相关和重要信息保持关注。我们的综述大致分类了计算机视觉文献中提出的注意力机制，包括软注意力、硬注意力、多模态、算术、类别注意力和逻辑注意力。我们注意到一些方法属于多个类别；然而，我们将每种方法分配到它与该类别其他方法具有主导关联的类别。这样的分类有助于跟踪常见的注意力机制特征，并提供可能有助于设计新型注意力技术的见解。图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Visual Attention Methods in Deep Learning: An In-Depth
    Survey")展示了注意力机制的分类。'
- en: 'We emphasize that a survey is warranted for attention in vision due to the
    large number of papers published as outlined in Figure [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey").
    It is evident from Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Visual Attention
    Methods in Deep Learning: An In-Depth Survey") that the number of articles published
    in the last year has significantly increased compared to previous years, and we
    expect to see a similar trend in the coming years. Furthermore, our survey lists
    articles of significant importance to assist the computer vision and machine learning
    community in adopting the most suitable attention mechanisms in their models and
    avoid duplicating attention methodologies. It also identifies research gaps, provides
    the current research context, presents plausible research directions and future
    areas of focus.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '我们强调，由于如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Visual Attention Methods in
    Deep Learning: An In-Depth Survey")所述，已发布的论文数量巨大，视觉注意力领域的综述是必要的。从图[1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey")可以明显看出，去年发表的文章数量较之前几年显著增加，我们预计未来几年也会出现类似趋势。此外，我们的综述列出了对计算机视觉和机器学习社区至关重要的文章，以帮助其在模型中采用最合适的注意力机制，避免重复注意力方法。它还识别了研究空白，提供了当前研究背景，提出了可行的研究方向和未来关注领域。'
- en: Since transformers have been employed across many vision applications; a few
    surveys [[18](#bib.bib18), [17](#bib.bib17)] summarize the recent trends of transformers
    in computer vision. Although transformers offer high accuracy, this comes at the
    cost of very high computational complexity which hinders their feasibility for
    mobile and embedded system applications. Furthermore, transformer based models
    require substantially more training data than CNNs and lack efficient hardware
    designs and generalizability. According to our survey, transformers only cover
    a single category in self-attention out of the 50 different attention categories
    surveyed. Another significant difference is that our survey focuses on attention
    types rather than applications covered in transformer-based surveys [[18](#bib.bib18),
    [17](#bib.bib17)].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于变压器已被应用于许多视觉应用；一些调查 [[18](#bib.bib18), [17](#bib.bib17)] 总结了变压器在计算机视觉中的最新趋势。尽管变压器提供了高准确度，但这伴随着极高的计算复杂性，限制了其在移动和嵌入式系统应用中的可行性。此外，基于变压器的模型需要比卷积神经网络（CNNs）显著更多的训练数据，并且缺乏高效的硬件设计和通用性。根据我们的调查，变压器仅覆盖了50种不同注意力类别中的单一类别。另一个显著的区别是，我们的调查关注于注意力类型，而不是变压器基础调查
    [[18](#bib.bib18), [17](#bib.bib17)] 涵盖的应用。
- en: 2 Attention in Vision
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 视觉中的注意力
- en: 'The primary purpose of the attention in vision is to imitate the human visual
    cognitive system and focus on the essential features [[20](#bib.bib20)] in the
    input image. We categorize attention methods based on the main function used to
    generate attention scores, such as softmax or sigmoid. Table [I](#S2.T1 "TABLE
    I ‣ 2.1.1 Channel Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey") provides
    the summary, application, strengths, and limitations for the category presented
    in this survey.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '视觉中的注意力的主要目的是模仿人类视觉认知系统，并关注输入图像中的关键特征 [[20](#bib.bib20)]。我们根据生成注意力分数所用的主要功能（如softmax或sigmoid）对注意力方法进行分类。表[I](#S2.T1
    "TABLE I ‣ 2.1.1 Channel Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey") 提供了本调查中展示的类别的总结、应用、优点和局限性。'
- en: 2.1 Soft (Deterministic) Attention
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 软性（确定性）注意力
- en: This section reviews soft-attention methods such as channel attention, spatial
    attention, and self-attention. In channel attention, the scores are calculated
    channel wise because each one of the feature maps (channels) attend to specific
    parts of the input. In spatial attention, the main idea is to attend to the critical
    regions in the image. Attending over regions of interest facilitates object detection,
    semantic segmentation, and person re-identification. In contrast to channel attention,
    spatial attention attends to the important parts in the spatial map (bounded by
    width and height). It can be used independently or as a complementary mechanism
    to channel attention. On the other hand, self-attention is proposed to encode
    higher-order interactions and contextual information by extracting the relationships
    between input sequence tokens. It is different from channel attention in how it
    generates the attention scores, as it mainly calculates the similarity between
    two maps (K, Q) of the same input, whereas channel attention generates the scores
    from a single map. However, self attention and channel attention both operate
    on channels. Soft attention methods calculate the attention scores as the weighted
    sum of all the input entities [[8](#bib.bib8)] and mainly use soft functions such
    as softmax and sigmoid. Since these methods are differentiable, they can be trained
    through back-propagation techniques. However, they suffer from other issues such
    as high computational complexity and assigning weights to non-attended objects.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分介绍了通道注意力、空间注意力和自注意力等软注意力方法。在通道注意力中，由于每个特征图（通道）都会关注输入的特定部分，因此通道注意力会按通道进行评分计算。在空间注意力中，主要思想是关注图像中的关键区域。关注感兴趣区域有助于目标检测、语义分割和人物再识别。与通道注意力相比，空间注意力会关注空间地图（由宽度和高度界定）中的重要部分。它可以独立使用，也可以作为通道注意力的补充机制。另一方面，自注意力旨在通过提取输入序列标记之间的关系来编码高阶交互和上下文信息。与通道注意力不同的是，自注意力在生成注意力分数的方式上不同，因为它主要计算了相同输入的两个地图（K，Q）之间的相似性，而通道注意力则是从单个地图中生成分数。然而，自注意力和通道注意力都作用于通道。软注意力方法将注意力分数计算为所有输入实体的加权和[[8](#bib.bib8)]，主要使用softmax和sigmoid等软函数。由于这些方法可微分，因此可以通过反向传播技术进行训练。然而，它们也存在其他问题，如计算复杂度高和对未关注对象分配权重的问题。
- en: 2.1.1 Channel Attention
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 通道注意力
- en: 'Squeeze & Excitation Attention: The Squeeze-and-Excitation (SE) Block [[21](#bib.bib21)],
    shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.1.1 Channel Attention ‣ 2.1 Soft (Deterministic)
    Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning:
    An In-Depth Survey")(a), is a unit design to perform dynamic channel-wise feature
    attention. The SE attention takes the output of a convolution block and converts
    each channel to a single value via global average pooling; this process is called
    “squeeze”. The output channel ratio is reduced after passing through the fully
    connected layer and ReLU for adding non-linearity. The features are passed through
    the fully connected layer, followed by a sigmoid function to achieve a smooth
    gating operation. The convolutional block feature maps are weighted based on the
    side network’s output, called the “excitation”. The process can be summarized
    as'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**Squeeze & Excitation Attention**：Squeeze-and-Excitation（SE）块[[21](#bib.bib21)]，如图[3](#S2.F3
    "Figure 3 ‣ 2.1.1 Channel Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey")（a）所示，是一种用于执行动态通道注意力的单元设计。SE注意力接受卷积块的输出，并通过全局平均池化将每个通道转换为一个值；这个过程称为“挤压”。通过经过全连接层和ReLU进行降低输出通道比例和增加非线性。特征通过全连接层传递，然后通过sigmoid函数实现平滑的门控操作。卷积块的特征图根据辅助网络的输出进行加权，称为“激励”。该过程可以总结为'
- en: '|  | $f_{s}=\sigma(FC(ReLU(FC(f_{g})))),$ |  | (1) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{s}=\sigma(FC(ReLU(FC(f_{g})))),$ |  | (1) |'
- en: where $FC$ is the fully connected layer, $f_{g}$ is the average global pooling,
    $\sigma$ is the sigmoid operation. The main intuition is to choose the best representation
    of each channel in order to generate attention scores.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$FC$是全连接层，$f_{g}$是全局平均池化，$\sigma$是sigmoid操作。其主要思想是选择每个通道的最佳表示，以生成注意力分数。
- en: 'Efficient Channel Attention (ECA) [[22](#bib.bib22)] is based on squeeze $\&amp;$
    excitation network [[21](#bib.bib21)] and aims to increase efficiency as well
    as decrease model complexity by removing the dimensionality reduction. ECA (see
    Fig [3](#S2.F3 "Figure 3 ‣ 2.1.1 Channel Attention ‣ 2.1 Soft (Deterministic)
    Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning:
    An In-Depth Survey")(g)) achieves cross-channel interaction locally through analyzing
    each channel and its $k$ neighbors, following channel-wise global average pooling
    but with no dimensionality reduction. ECA accomplishes efficient processing via
    fast 1D convolutions. The size $k$ represents the number of neighbors that can
    participate in one channel attention prediction i.e. the coverage of local cross-channel
    interaction.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '高效通道注意力（ECA）[[22](#bib.bib22)] 基于 squeeze $\&amp;$ excitation 网络[[21](#bib.bib21)]，旨在通过去除降维来提高效率并减少模型复杂性。ECA（见图
    [3](#S2.F3 "Figure 3 ‣ 2.1.1 Channel Attention ‣ 2.1 Soft (Deterministic) Attention
    ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth
    Survey")(g)）通过分析每个通道及其 $k$ 个邻居，实现局部的跨通道交互，遵循通道级全局平均池化，但没有降维。ECA 通过快速 1D 卷积实现高效处理。大小
    $k$ 表示可以参与一个通道注意力预测的邻居数量，即局部跨通道交互的覆盖范围。'
- en: 'Split-Attention Networks: ResNest [[23](#bib.bib23)], a variant of ResNet [[24](#bib.bib24)],
    uses split attention blocks as shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.1.1 Channel
    Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual
    Attention Methods in Deep Learning: An In-Depth Survey")(h). Attention is obtained
    by summing the inputs from previous modules and applying global pooling, passing
    through a composite function i.e. convolutional layer-batch normalization-ReLU
    activation. The output is again passed through convolutional layers. Afterwards,
    a softmax is applied to normalize the values and then multiply them with the corresponding
    inputs. Finally, all the features are summed together. This mechanism is similar
    to the squeeze $\&amp;$ excitation attention [[21](#bib.bib21)]. ResNest is also
    a special type of squeeze $\&amp;$ excitation where it squeezes the channels using
    average pooling and summing of the split channels.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '分裂注意力网络：ResNest [[23](#bib.bib23)] 是 ResNet [[24](#bib.bib24)] 的一种变体，使用了分裂注意力块，如图
    [3](#S2.F3 "Figure 3 ‣ 2.1.1 Channel Attention ‣ 2.1 Soft (Deterministic) Attention
    ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth
    Survey")(h) 所示。注意力通过对之前模块的输入进行求和并应用全局池化来获得，经过复合函数处理，即卷积层-批量归一化-ReLU 激活。输出再次通过卷积层。之后，应用
    softmax 来归一化值，然后与相应的输入相乘。最后，将所有特征相加。这一机制类似于 squeeze $\&amp;$ excitation 注意力[[21](#bib.bib21)]。ResNest
    也是一种特殊的 squeeze $\&amp;$ excitation，其中通过平均池化和分裂通道的求和来挤压通道。'
- en: 'Channel Attention in CBAM: Convolutional Block Attention Module (CBAM) [[25](#bib.bib25)]
    employs channel attention and exploits the inter-channel feature relationship
    as each feature map channel is considered a feature detector focusing on the “what”
    part of the input image. The input feature map’s spatial dimensions are squeezed
    for computing the channel attention followed by aggregation while using both average-pooling
    and max-pooling to obtain two descriptors. These descriptors are forwarded to
    a three-layer shared multi-layer perceptron (MLP) to generate the attention map.
    Subsequently, the output of each MLP is summed element-wise and then passed through
    a sigmoid function as shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.1.1 Channel Attention
    ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual Attention
    Methods in Deep Learning: An In-Depth Survey")(b). In summary, the channel attention
    is computed as'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'CBAM 中的通道注意力：卷积块注意力模块（CBAM）[[25](#bib.bib25)] 采用通道注意力，并利用跨通道特征关系，因为每个特征图通道被视为一个特征检测器，专注于输入图像的“是什么”部分。输入特征图的空间维度被挤压以计算通道注意力，随后通过平均池化和最大池化获得两个描述符进行聚合。这些描述符被传递到一个三层共享多层感知器（MLP）中以生成注意力图。随后，每个
    MLP 的输出被逐元素相加，然后通过 sigmoid 函数处理，如图 [3](#S2.F3 "Figure 3 ‣ 2.1.1 Channel Attention
    ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual Attention
    Methods in Deep Learning: An In-Depth Survey")(b) 所示。总之，通道注意力的计算方式是'
- en: '|  | $f_{ch}=\sigma(MLP(MaxPool(f))+MLP(AvgPool(f))),$ |  | (2) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{ch}=\sigma(MLP(MaxPool(f))+MLP(AvgPool(f))),$ |  | (2) |'
- en: where $\sigma$ denotes the sigmoid function, and $f$ represents the input features.
    The ReLU activation function is employed in MLP after each convolutional layer.
    Channel attention in CBAM is the same as Squeeze and Excitation (SE) attention [[26](#bib.bib26)]
    if only average pooling is used.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\sigma$表示sigmoid函数，$f$表示输入特征。在MLP中，每个卷积层后采用ReLU激活函数。CBAM中的通道注意力与Squeeze and
    Excitation (SE)注意力相同[[26](#bib.bib26)]，前提是仅使用平均池化。
- en: 'Second-order Attention Network: For single image super-resolution, in [[27](#bib.bib27)],
    the authors presented a second-order channel attention module, abbreviated as
    SOCA, to learn feature interdependencies via second-order feature statistics.
    A covariance matrix ($\Sigma$) is first computed and normalized using the features
    map from the previous network layers to obtain discriminative representations.
    The symmetric positive semi-definite covariance matrix is decomposed into $\Sigma=U\Lambda
    U^{T}$, where $U$ is orthogonal, and $\Lambda$ is the diagonal matrix with non-increasing
    eigenvalues. The power of eigenvalues $\Sigma=U\Lambda^{\alpha}U^{T}$ help in
    achieving the attention mechanism, that is, if $\alpha<1$, then the eigenvalues
    larger than 1.0 will nonlinearly shrink while stretching others. The authors chose
    $\alpha<\frac{1}{2}$ based on previous work [[28](#bib.bib28)]. The subsequent
    attention mechanism is similar to SE [[21](#bib.bib21)] as shown in Figure [3](#S2.F3
    "Figure 3 ‣ 2.1.1 Channel Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey")(c),
    but instead of providing first-order statistics (i.e. global average pooling),
    the authors furnished second-order statistics (i.e. global covariance pooling).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '二阶注意力网络：对于单幅图像超分辨率，在[[27](#bib.bib27)]中，作者提出了一种二阶通道注意力模块，简称SOCA，通过二阶特征统计学习特征间依赖性。首先计算并规范化协方差矩阵（$\Sigma$），使用来自前一网络层的特征图以获得辨别表示。对称正半定协方差矩阵被分解为$\Sigma=U\Lambda
    U^{T}$，其中$U$是正交的，$\Lambda$是具有非递增特征值的对角矩阵。特征值的幂$\Sigma=U\Lambda^{\alpha}U^{T}$有助于实现注意力机制，即如果$\alpha<1$，则大于1.0的特征值会非线性地缩小，而其他特征值则伸展。作者根据之前的工作选择了$\alpha<\frac{1}{2}$[[28](#bib.bib28)]。后续的注意力机制类似于SE[[21](#bib.bib21)]，如图[3](#S2.F3
    "Figure 3 ‣ 2.1.1 Channel Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey")(c)所示，但作者提供的是二阶统计量（即全局协方差池化），而不是一阶统计量（即全局平均池化）。'
- en: 'High-Order Attention: To encode global information and contextual representations,
    [[29](#bib.bib29)], Ding et al. proposed High-order Attention (HA) with adaptive
    receptive fields and dynamic weights. HA mainly constructs a feature map for each
    pixel, including the relationships to other pixels. HA is required to address
    the issue of fixed-shape receptive fields that cause false prediction in case
    of high-shape objects i.e., similar shape objects. Specifically, after calculating
    the attention maps for each pixel, graph transduction is used to form the final
    feature map. This feature representation is used to update each pixel position
    by using the weighted sum of contextual information. High-order attention maps
    are calculated using Hadamard product [[30](#bib.bib30), [31](#bib.bib31)]. It
    is classified as channel attention because it is generate attention scores from
    channels as in SE [[26](#bib.bib26)].'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 高阶注意力：为了编码全局信息和上下文表示，[[29](#bib.bib29)]，Ding等人提出了具有自适应感受野和动态权重的高阶注意力（HA）。HA主要为每个像素构建一个特征图，包括与其他像素的关系。HA解决了固定形状感受野在高形状物体（即相似形状物体）情况下导致错误预测的问题。具体来说，在为每个像素计算注意力图后，使用图转导形成最终特征图。这种特征表示用于通过加权求和的上下文信息更新每个像素位置。高阶注意力图是通过Hadamard积计算的[[30](#bib.bib30),
    [31](#bib.bib31)]。由于它从通道中生成注意力分数，因此被归类为通道注意力，类似于SE[[26](#bib.bib26)]。
- en: 'Harmonious attention: proposes a joint attention module of soft pixel and hard
    regional attentions [[32](#bib.bib32)]. The main idea is to tackle the limitation
    of the previous attention modules in person Re-Identification by learning attention
    selection and feature representation jointly and hence solving the issue of misalignment
    calibration caused by constrained attention mechanisms [[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36)]. Specifically, harmonious attention learns
    two types of soft attention (spatial and channel) in one branch and hard attention
    in the other one. Moreover, it proposes cross-interaction attention harmonizing
    between these two attention types as shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.1.1
    Channel Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision
    ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey")(i).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 和谐注意力：提出了一种联合注意力模块，包括软像素和硬区域注意力 [[32](#bib.bib32)]。其主要思想是通过联合学习注意力选择和特征表示，解决之前的注意力模块在人员重识别中的局限性，从而解决受限注意力机制导致的对齐校准问题
    [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)]。具体来说，和谐注意力在一个分支中学习两种类型的软注意力（空间和通道），在另一个分支中学习硬注意力。此外，提出了跨交互注意力，协调这两种注意力类型，如图
    [3](#S2.F3 "图 3 ‣ 2.1.1 通道注意力 ‣ 2.1 软（确定性）注意力 ‣ 2 视觉中的注意力 ‣ 深度学习中的视觉注意力方法：深入调查")(i)所示。
- en: 'TABLE I: Summary of attention types along with their categorization, applications,
    strengths, and limitations. References to the original papers and links to Sections
    where they are discussed are also provided.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：总结了注意力类型及其分类、应用、优缺点。同时提供了原始论文的参考文献和讨论部分的链接。
- en: '| Type | Category | Section | References | Applications | Strengths | Limitations
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 类别 | 节点 | 参考文献 | 应用 | 优势 | 局限性 |'
- en: '|  | Channel | [2.1.1](#S2.SS1.SSS1 "2.1.1 Channel Attention ‣ 2.1 Soft (Deterministic)
    Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning:
    An In-Depth Survey") | SE-Net [[26](#bib.bib26)], ECA-Net [[22](#bib.bib22)],
    CBAM [[25](#bib.bib25)], [[32](#bib.bib32)], A2-Net [[29](#bib.bib29)], Dual [[37](#bib.bib37)]
    | visual recognition, person re-identification, medical image segmentation, video
    recognition | • Easy to model • Differentiable gradients • Non-local operations
    • Encoding context • Improving the performance | • Expensive in memory usage •
    High computation cost • Biased to softmax radial nature • Subject to attention
    collapse due to lack of diversity |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | 通道 | [2.1.1](#S2.SS1.SSS1 "2.1.1 通道注意力 ‣ 2.1 软（确定性）注意力 ‣ 2 视觉中的注意力 ‣ 深度学习中的视觉注意力方法：深入调查")
    | SE-Net [[26](#bib.bib26)], ECA-Net [[22](#bib.bib22)], CBAM [[25](#bib.bib25)],
    [[32](#bib.bib32)], A2-Net [[29](#bib.bib29)], Dual [[37](#bib.bib37)] | 视觉识别，人脸重识别，医学图像分割，视频识别
    | • 易于建模 • 可微分梯度 • 非局部操作 • 编码上下文 • 性能提升 | • 内存使用昂贵 • 计算成本高 • 偏向softmax径向性质 • 由于缺乏多样性可能导致注意力崩溃
    |'
- en: '|  | Spatial | [2.1.2](#S2.SS1.SSS2 "2.1.2 Spatial Attention ‣ 2.1 Soft (Deterministic)
    Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning:
    An In-Depth Survey") | CBAM [[25](#bib.bib25)], PFA [[38](#bib.bib38)], [[39](#bib.bib39)],
    [[40](#bib.bib40)] | Visual Recognition, domain adaptation, saliency detection
    |  |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | 空间 | [2.1.2](#S2.SS1.SSS2 "2.1.2 空间注意力 ‣ 2.1 软（确定性）注意力 ‣ 2 视觉中的注意力 ‣ 深度学习中的视觉注意力方法：深入调查")
    | CBAM [[25](#bib.bib25)], PFA [[38](#bib.bib38)], [[39](#bib.bib39)], [[40](#bib.bib40)]
    | 视觉识别，领域适应，显著性检测 |  |  |'
- en: '|  | Self-attentions | [2.1.3](#S2.SS1.SSS3 "2.1.3 Self-attention ‣ 2.1 Soft
    (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in
    Deep Learning: An In-Depth Survey") | Transformers [[2](#bib.bib2)], Image Transformers[[41](#bib.bib41)],
    [[42](#bib.bib42)], [[43](#bib.bib43)] | Visual Recognition, multi-modal tasks,
    video processing, low-level vision , video recognition , 3D analysis |  |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | 自注意力 | [2.1.3](#S2.SS1.SSS3 "2.1.3 自注意力 ‣ 2.1 软（确定性）注意力 ‣ 2 视觉中的注意力 ‣
    深度学习中的视觉注意力方法：深入调查") | Transformers [[2](#bib.bib2)], Image Transformers [[41](#bib.bib41)],
    [[42](#bib.bib42)], [[43](#bib.bib43)] | 视觉识别，多模态任务，视频处理，低级视觉，视频识别，3D分析 |  |  |'
- en: '| Soft (Deterministic) | Category-based | [2.1.7](#S2.SS1.SSS7 "2.1.7 Category-Based
    Attentions ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual
    Attention Methods in Deep Learning: An In-Depth Survey") | GAIN [[44](#bib.bib44)] [[45](#bib.bib45)]
    | explainable machine learning, person re-identification, semantic segmentation
    | • Providing gradient understanding • Does not require extra supervision | •
    Extra computation • Used only for supervised classification |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 软（确定性） | 基于类别的 | [2.1.7](#S2.SS1.SSS7 "2.1.7 基于类别的注意力 ‣ 2.1 软（确定性）注意力 ‣ 2
    视觉中的注意力 ‣ 深度学习中的视觉注意力方法：深入调查") | GAIN [[44](#bib.bib44)] [[45](#bib.bib45)] |
    可解释的机器学习，人物重识别，语义分割 | • 提供梯度理解 • 不需要额外监督 | • 额外计算 • 仅用于监督分类 |'
- en: '|  | Multi-modal | [2.1.5](#S2.SS1.SSS5 "2.1.5 Multi-modal attentions ‣ 2.1
    Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods
    in Deep Learning: An In-Depth Survey") | CAN [[46](#bib.bib46)], SCAN [[47](#bib.bib47)],
    Perceiver [[48](#bib.bib48)], Boosted [[49](#bib.bib49)] | few-shot classification,
    image-text matching, image captioning | • Benefiting visual-language-based applications
    • Providing attentive supervision signals • Achieving higher accuracy rates |
    • Expensive in memory usage • High computation cost • Inherit the limitations
    of soft and hard attentions |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | 多模态的 | [2.1.5](#S2.SS1.SSS5 "2.1.5 多模态注意力 ‣ 2.1 软（确定性）注意力 ‣ 2 视觉中的注意力
    ‣ 深度学习中的视觉注意力方法：深入调查") | CAN [[46](#bib.bib46)], SCAN [[47](#bib.bib47)], Perceiver [[48](#bib.bib48)],
    Boosted [[49](#bib.bib49)] | 少样本分类，图像-文本匹配，图像标注 | • 有利于基于视觉-语言的应用 • 提供有针对性的监督信号
    • 实现更高的准确率 | • 内存使用昂贵 • 高计算成本 • 继承软注意力和硬注意力的局限性 |'
- en: '|  | Arithmetic | [2.1.4](#S2.SS1.SSS4 "2.1.4 Arithmetic Attention ‣ 2.1 Soft
    (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in
    Deep Learning: An In-Depth Survey") | Drop-out [[50](#bib.bib50)], Mirror [[51](#bib.bib51)],
    Reverse [[52](#bib.bib52)], Inverse [[53](#bib.bib53)], Reciprocal [[54](#bib.bib54)]
    | weakly-supervised object localization, line detection, semantic segmentation,
    | • Efficient methods • Simple ideas • Easy to implement • Enriching the semantics
    of the models | • Limited to certain applications • Inability to scale up • Inherit
    the limitations of soft and hard attentions |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | 算术的 | [2.1.4](#S2.SS1.SSS4 "2.1.4 算术注意力 ‣ 2.1 软（确定性）注意力 ‣ 2 视觉中的注意力 ‣
    深度学习中的视觉注意力方法：深入调查") | Drop-out [[50](#bib.bib50)], Mirror [[51](#bib.bib51)],
    Reverse [[52](#bib.bib52)], Inverse [[53](#bib.bib53)], Reciprocal [[54](#bib.bib54)]
    | 弱监督目标定位，线条检测，语义分割 | • 高效的方法 • 简单的思想 • 易于实现 • 丰富模型的语义 | • 限于特定应用 • 无法扩展 • 继承软注意力和硬注意力的局限性
    |'
- en: '|  | Logical | [2.1.6](#S2.SS1.SSS6 "2.1.6 Logical Attention ‣ 2.1 Soft (Deterministic)
    Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning:
    An In-Depth Survey") | Recurrent [[55](#bib.bib55)], Sequential [[56](#bib.bib56),
    [57](#bib.bib57)], Permutation invariant [[58](#bib.bib58)] | image recognition,
    object detection and segmentation, adversarial image classification, image tagging,
    anomaly detection | • Overcoming the issues of soft attentions • Addressing hard
    attention disadvantages | • Complex architectures • High computation cost • Iterative
    processing |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | 逻辑的 | [2.1.6](#S2.SS1.SSS6 "2.1.6 逻辑注意力 ‣ 2.1 软（确定性）注意力 ‣ 2 视觉中的注意力 ‣
    深度学习中的视觉注意力方法：深入调查") | 循环的 [[55](#bib.bib55)], 序列的 [[56](#bib.bib56), [57](#bib.bib57)],
    排列不变的 [[58](#bib.bib58)] | 图像识别，目标检测与分割，对抗性图像分类，图像标记，异常检测 | • 克服软注意力的问题 • 解决硬注意力的缺点
    | • 复杂的架构 • 高计算成本 • 迭代处理 |'
- en: '| Hard (Stochastic) | Statistical | [2.2.1](#S2.SS2.SSS1 "2.2.1 Statistical-based
    attention ‣ 2.2 Hard (Stochastic) Attention ‣ 2 Attention in Vision ‣ Visual Attention
    Methods in Deep Learning: An In-Depth Survey") | Bayesian [[59](#bib.bib59)],
    Repulsive [[60](#bib.bib60)], Variational [[61](#bib.bib61)], [[62](#bib.bib62)]
    | visual-question answering, captioning, image translation, natural language processing
    |  |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 硬（随机） | 统计学的 | [2.2.1](#S2.SS2.SSS1 "2.2.1 基于统计的注意力 ‣ 2.2 硬（随机）注意力 ‣ 2 视觉中的注意力
    ‣ 深度学习中的视觉注意力方法：深入调查") | 贝叶斯 [[59](#bib.bib59)], 排斥性 [[60](#bib.bib60)], 变分 [[61](#bib.bib61)], [[62](#bib.bib62)]
    | 视觉问答，图像标注，图像翻译，自然语言处理 |  |  |'
- en: '|  | Reinforcement-based | [2.2.2](#S2.SS2.SSS2 "2.2.2 Reinforcement-based
    Attention ‣ 2.2 Hard (Stochastic) Attention ‣ 2 Attention in Vision ‣ Visual Attention
    Methods in Deep Learning: An In-Depth Survey") | RESA [[63](#bib.bib63)], [[64](#bib.bib64)],
    self-critic [[65](#bib.bib65)] | person re-identification, natural language processing
    | • Encoding context • Encoding higher-order interactions • Diverse attention
    scores • Higher improvements | • Expensive in memory usage • High computation
    cost • Non-differentiable • Gradient vanishing • Requires tricks for training
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | 基于强化学习 | [2.2.2](#S2.SS2.SSS2 "2.2.2 基于强化学习的注意力 ‣ 2.2 硬（随机）注意力 ‣ 2 视觉中的注意力
    ‣ 深度学习中的视觉注意力方法：深入调查") | RESA [[63](#bib.bib63)], [[64](#bib.bib64)], 自我批评 [[65](#bib.bib65)]
    | 人员再识别，自然语言处理 | • 编码上下文 • 编码高阶交互 • 多样的注意力分数 • 更高的改进 | • 内存使用开销大 • 计算成本高 • 不可微分
    • 梯度消失 • 训练需要技巧 |'
- en: '|  | Gaussian | [2.2.3](#S2.SS2.SSS3 "2.2.3 Gaussian-based Attention ‣ 2.2
    Hard (Stochastic) Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods
    in Deep Learning: An In-Depth Survey") | GatCluster [[66](#bib.bib66)], Uncertainty [[67](#bib.bib67)]
    | image clustering medical natural language processing |  |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | 高斯 | [2.2.3](#S2.SS2.SSS3 "2.2.3 基于高斯的注意力 ‣ 2.2 硬（随机）注意力 ‣ 2 视觉中的注意力 ‣
    深度学习中的视觉注意力方法：深入调查") | GatCluster [[66](#bib.bib66)], 不确定性 [[67](#bib.bib67)]
    | 图像聚类 医学 自然语言处理 |  |  |'
- en: '|  | Clustering | [2.2.4](#S2.SS2.SSS4 "2.2.4 Clustering ‣ 2.2 Hard (Stochastic)
    Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning:
    An In-Depth Survey") | Expectation Maximization [[68](#bib.bib68)], GatCluster [[66](#bib.bib66)]
    | semantic segmentation |  |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | 聚类 | [2.2.4](#S2.SS2.SSS4 "2.2.4 聚类 ‣ 2.2 硬（随机）注意力 ‣ 2 视觉中的注意力 ‣ 深度学习中的视觉注意力方法：深入调查")
    | 最大期望 [[68](#bib.bib68)], GatCluster [[66](#bib.bib66)] | 语义分割 |  |  |'
- en: 'Auto Learning Attention: Ma et al. [[57](#bib.bib57)] introduced a novel idea
    for designing attention automatically. The module, named Higher-Order Group Attention
    (HOGA), is in the form of a Directed Acyclic Graph (DAG) [[69](#bib.bib69), [70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72)] where each node represents a group and each
    edge represents a heterogeneous attention operation. There is a sequential connection
    between the nodes to represent hybrids of attention operations. Thus, these connections
    can be represented as K-order attention modules, where K is the number of attention
    operations. DARTS [[73](#bib.bib73)] is customized to facilitate the search process
    efficiently. This auto-learning module can be integrated into legacy architectures
    and performs better than manual ones. However, the core idea of attention modules
    remains the same as the previous architectures i.e. SE [[26](#bib.bib26)], CBAM
    [[25](#bib.bib25)], splat [[23](#bib.bib23)], mixed [[74](#bib.bib74)].'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 自动学习注意力：Ma等人[[57](#bib.bib57)]引入了一种自动设计注意力的新颖理念。该模块称为高阶组注意力（HOGA），其形式为一个有向无环图（DAG）[[69](#bib.bib69),
    [70](#bib.bib70), [71](#bib.bib71), [72](#bib.bib72)]，其中每个节点代表一个组，每条边代表一个异质注意力操作。节点之间存在顺序连接以表示注意力操作的混合。因此，这些连接可以表示为K阶注意力模块，其中K是注意力操作的数量。DARTS
    [[73](#bib.bib73)]被定制以有效地促进搜索过程。该自动学习模块可以集成到遗留架构中，并且表现优于手动设计的模块。然而，注意力模块的核心理念与之前的架构保持一致，即SE
    [[26](#bib.bib26)], CBAM [[25](#bib.bib25)], splat [[23](#bib.bib23)], mixed [[74](#bib.bib74)]。
- en: 'Double Attention Networks: Chen et al. [[75](#bib.bib75)] proposed Double Attention
    Network (A2-Nets), which attends over the input image in two steps. The first
    step gathers the required features using bilinear pooling to encode the second-order
    relationships between entities, and the second step distributes the features over
    the various locations adaptively. In this architecture, the second-order statistics,
    which are mostly lost with other functions such as average pooling of SE [[26](#bib.bib26)],
    of the pooled features are captured first by bilinear pooling. The attention scores
    are then calculated not from the whole image such as [[3](#bib.bib3)] but from
    a compact bag, hence, enriching the objects with the required context only. The
    first step i.e., feature gathering uses the outer product $\sum_{\forall i}a_{i}b_{i}^{T}$
    then softmax is used for attending the discriminative features. The second step
    i.e., distribution is based on complementing each location with the required features
    where their summation is $1$. The complete design of A2-Nets is shown in Figure [3](#S2.F3
    "Figure 3 ‣ 2.1.1 Channel Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey")(d).
    Experimental comparisons demonstrated that A2-Net improves the performance better
    than SE and non-local networks, and is more efficient in terms of memory and time.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '双重注意力网络：陈等人[[75](#bib.bib75)]提出了双重注意力网络（A2-Nets），该网络在两个步骤中关注输入图像。第一步使用双线性池化来编码实体之间的二阶关系，从而收集所需特征；第二步则自适应地将特征分布到不同位置。在这种架构中，首先通过双线性池化捕捉到的二阶统计数据（其他函数如SE的平均池化[[26](#bib.bib26)]中大多丢失的），然后计算注意力分数，不是从整个图像[[3](#bib.bib3)]中，而是从一个紧凑的包中，因此仅丰富所需上下文的对象。第一步，即特征收集，使用外积$\sum_{\forall
    i}a_{i}b_{i}^{T}$，然后使用softmax来关注判别特征。第二步，即分布，基于用所需特征补充每个位置，其总和为$1$。A2-Nets的完整设计如图[3](#S2.F3
    "Figure 3 ‣ 2.1.1 Channel Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey")(d)所示。实验比较表明，A2-Net在性能上优于SE和非局部网络，并且在内存和时间方面更为高效。'
- en: 'Dual Attention Network: Jun et al. [[37](#bib.bib37)] presented a dual attention
    network for scene segmentation composed of position attention and channel attention
    working in parallel. The position attention aims to encode the contextual features
    in local ones. The attention process is straightforward: the input features $f_{A}$
    are passed through three convolutional layers to generate three feature maps ($f_{B}$,
    $f_{C}$, and $f_{D}$), which are reshaped. Matrix multiplication is performed
    between the $f_{B}$ and the transpose of $f_{C}$, followed by softmax to obtain
    the spatial attention map. Again, matrix multiplication is performed between the
    generated $f_{D}$ features and the spatial attention map. Finally, the output
    is multiplied with a scalar and summed element-wise with the input features $f_{A}$
    as shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.1.1 Channel Attention ‣ 2.1 Soft (Deterministic)
    Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning:
    An In-Depth Survey")(f).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '双重注意力网络：Jun等人[[37](#bib.bib37)]提出了一种用于场景分割的双重注意力网络，该网络由位置注意力和通道注意力并行工作组成。位置注意力旨在编码局部上下文特征。注意力过程非常直接：输入特征$f_{A}$通过三个卷积层生成三个特征图（$f_{B}$、$f_{C}$和$f_{D}$），然后进行重塑。对$f_{B}$和$f_{C}$的转置进行矩阵乘法，然后使用softmax获得空间注意力图。接着，对生成的$f_{D}$特征和空间注意力图进行矩阵乘法。最后，输出与标量相乘，并逐元素与输入特征$f_{A}$相加，如图[3](#S2.F3
    "Figure 3 ‣ 2.1.1 Channel Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey")(f)所示。'
- en: 'Although channel attention involves similar steps as position attention, it
    is different because the features are used directly without passing through convolutional
    layers. The input features $f_{A}$ are reshaped, transposed, multiplied (i.e.
    $f_{A}\times f_{A}^{\prime}$), and then passed through the softmax layer to obtain
    the channel attention map. Moreover, the input features are multiplied with the
    channel attention map, followed by the element-wise summation, to give the final
    output as shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.1.1 Channel Attention ‣ 2.1
    Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods
    in Deep Learning: An In-Depth Survey")(e).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通道注意力涉及的步骤与位置注意力类似，但它不同于位置注意力，因为特征被直接使用，而不是通过卷积层。输入特征 $f_{A}$ 被重塑、转置、相乘（即
    $f_{A}\times f_{A}^{\prime}$），然后通过 softmax 层得到通道注意力图。此外，输入特征与通道注意力图相乘，然后进行逐元素求和，以得到最终输出，如图
    [3](#S2.F3 "图 3 ‣ 2.1.1 通道注意力 ‣ 2.1 软（确定性）注意力 ‣ 2 视觉中的注意力 ‣ 深度学习中的视觉注意力方法：深入调查")(e)
    所示。
- en: '| ![Refer to caption](img/47cc1bba97b585259144d2dc565d1aa0.png) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/47cc1bba97b585259144d2dc565d1aa0.png) |'
- en: '| (a) SENet [[21](#bib.bib21)] |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| (a) SENet [[21](#bib.bib21)] |'
- en: '| ![Refer to caption](img/c4345e4882f777b5b61e7dc7a106c7da.png) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/c4345e4882f777b5b61e7dc7a106c7da.png) |'
- en: '| (b) CBAM [[25](#bib.bib25)] |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| (b) CBAM [[25](#bib.bib25)] |'
- en: '| ![Refer to caption](img/80036e09e70a8f7bcd3e0acee6eaff2b.png) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/80036e09e70a8f7bcd3e0acee6eaff2b.png) |'
- en: '| (c) SOCA [[27](#bib.bib27)] |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| (c) SOCA [[27](#bib.bib27)] |'
- en: '| ![Refer to caption](img/6dc88c47c652f45c7f803eaa33dddf70.png) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/6dc88c47c652f45c7f803eaa33dddf70.png) |'
- en: '| (d) A${}^{2}-$Net [[75](#bib.bib75)] |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| (d) A${}^{2}-$Net [[75](#bib.bib75)] |'
- en: '| ![Refer to caption](img/fb2e2630f7f66b650c18c6eca3f55e16.png) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/fb2e2630f7f66b650c18c6eca3f55e16.png) |'
- en: '| (e) DAN Positional[[37](#bib.bib37)] |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| (e) DAN Positional [[37](#bib.bib37)] |'
- en: '| ![Refer to caption](img/3f6c7e0fae28f3f6e34d5d4d3b0a5803.png) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/3f6c7e0fae28f3f6e34d5d4d3b0a5803.png) |'
- en: '| (f) DAN Channel [[37](#bib.bib37)] |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| (f) DAN Channel [[37](#bib.bib37)] |'
- en: '| ![Refer to caption](img/cdf0a2508efe42f46b1320ec92659136.png) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/cdf0a2508efe42f46b1320ec92659136.png) |'
- en: '| (g) ECA-Net [[22](#bib.bib22)] |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| (g) ECA-Net [[22](#bib.bib22)] |'
- en: '| ![Refer to caption](img/7f814aac82a0fd6bdd118b9b19ed863a.png) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/7f814aac82a0fd6bdd118b9b19ed863a.png) |'
- en: '| (h) RESNest [[23](#bib.bib23)] |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| (h) RESNest [[23](#bib.bib23)] |'
- en: '| ![Refer to caption](img/f1e149944c84659c608392fd317c5585.png) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/f1e149944c84659c608392fd317c5585.png) |'
- en: '| (i) Harmonious [[32](#bib.bib32)] |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| (i) Harmonious [[32](#bib.bib32)] |'
- en: 'Figure 3: Core structures of the channel-based attention methods. Different
    methods to generate the attention scores including squeeze and excitation [[26](#bib.bib26)],
    splitting and squeezing [[23](#bib.bib23)], calculating the second order [[37](#bib.bib37)]
    or efficient squeezing and excitation [[22](#bib.bib22)]. Images are taken from
    the original papers and are best viewed in color.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：基于通道的注意力方法的核心结构。生成注意力分数的不同方法包括 squeeze 和 excitation [[26](#bib.bib26)]，分割和
    squeeze [[23](#bib.bib23)]，计算二阶 [[37](#bib.bib37)] 或高效的 squeeze 和 excitation [[22](#bib.bib22)]。图像取自原始论文，最佳效果为彩色显示。
- en: 'Frequency Channel Attention: Channel attention requires global average pooling
    as a pre-processing step. Qin et al. [[76](#bib.bib76)] argued that the global
    average pooling operation can be replaced with frequency components. The frequency
    attention views the discrete cosine transform as the weighted input sum with the
    cosine parts. As global average pooling is a particular case of frequency-domain
    feature decomposition, the authors use various frequency components of 2D discrete
    cosine transform, including the zero-frequency component, i.e. global average
    pooling.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 频率通道注意力：通道注意力需要全局平均池化作为预处理步骤。Qin 等人 [[76](#bib.bib76)] 认为全局平均池化操作可以用频率成分替代。频率注意力将离散余弦变换视为带余弦部分的加权输入和。由于全局平均池化是频域特征分解的特例，作者使用了包括零频成分在内的
    2D 离散余弦变换的各种频率成分，即全局平均池化。
- en: 2.1.2 Spatial Attention
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 空间注意力
- en: Different from channel attention that mainly generates channel-wise attention
    scores, spatial attention focuses on generating attention scores from spatial
    patches of the feature maps rather than the channels. However, the sequence of
    operations to generate the attentions are similar.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与主要生成通道注意力分数的通道注意力不同，空间注意力专注于从特征图的空间补丁中生成注意力分数，而不是通道。然而，生成注意力的操作顺序类似。
- en: 'Spatial Attention in CBAM uses the inter-spatial feature relationships to complement
    the channel attention [[25](#bib.bib25)]. The spatial attention focuses on an
    informative part and is computed by applying average pooling and max pooling channel-wise,
    followed by concatenating both to obtain a single feature descriptor. Furthermore,
    a convolution layer on the concatenated feature descriptor is applied to generate
    a spatial attention map that encodes to emphasize or suppress. The feature map
    channel information is aggregated via average-pooled features and max-pooled features
    and then concatenated and convolved to generate a 2D spatial attention map. The
    overall process is shown in Figure [4](#S2.F4 "Figure 4 ‣ 2.1.2 Spatial Attention
    ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual Attention
    Methods in Deep Learning: An In-Depth Survey")(a) and computed as'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 'CBAM中的空间关注使用空间特征关系来补充通道关注[[25](#bib.bib25)]. 空间关注侧重于关注信息部分，通过依次应用平均池化和最大池化来计算，并将两者连接以得到单个特征描述符。此外，对连接的特征描述符进行卷积操作以生成空间关注图，以加强或抑制。特征图通道信息通过平均池化特征和最大池化特征进行聚合，然后进行连接和卷积，生成2D空间关注图。整个过程如图[4](#S2.F4
    "Figure 4 ‣ 2.1.2 Spatial Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey")(a)所示，计算公式为：'
- en: '|  | $f_{sp}=\sigma(Conv_{7\times 7}([MaxPool(f);AvgPool(f)])),$ |  | (3) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| | $f_{sp}=\sigma(Conv_{7\times 7}([MaxPool(f);AvgPool(f)])),$ | | (3) |'
- en: where $Conv_{7\times 7}$ denotes a convolution operation with the 7 $\times$
    7 kernel size and $\sigma$ represents the sigmoid function.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$Conv_{7\times 7}$表示7×7卷积操作，$\sigma$表示sigmoid函数。
- en: 'Co-attention & Co-excitation: Hsieh et al. [[77](#bib.bib77)] proposed co-attention
    and co-excitation to detect all the instances that belong to the same target for
    one-shot detection. The main idea is to enrich the extracted feature representation
    using non-local networks that encode long-range dependencies and second-order
    interactions [[3](#bib.bib3)]. Co-excitation is based on squeeze-and-excite network
    [[26](#bib.bib26)] as shown in Figure [4](#S2.F4 "Figure 4 ‣ 2.1.2 Spatial Attention
    ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual Attention
    Methods in Deep Learning: An In-Depth Survey")(e). While squeeze uses global average
    pooling [[78](#bib.bib78)] to reweight the spatial positions, co-excite serves
    as a bridge between the feature of query and target. Encoding high-contextual
    representations using co-attention and co-excitation show improvements in one-shot
    detector performance achieving state-of-the-art results.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '协同关注和协同激励：Hsieh等人[[77](#bib.bib77)]提出了协同关注和协同激励以检测所有属于同一目标的实例进行单次检测。其主要思想是通过编码长范围依赖和二阶交互的非局部网络来丰富提取的特征表示[[3](#bib.bib3)].
    相关激励是基于squeeze-and-excite网络[[26](#bib.bib26)]，如图[4](#S2.F4 "Figure 4 ‣ 2.1.2 Spatial
    Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual
    Attention Methods in Deep Learning: An In-Depth Survey")(e)所示。其中，squeeze使用全局平均池化[[78](#bib.bib78)]对空间位置进行重新加权，而co-excite则作为查询和目标特征之间的桥梁。使用协同关注和协同激励编码高语境表示，可以改善单次检测器性能，实现最先进的结果。'
- en: 'Spatial Pyramid Attention Network abbreviated as SPAN [[79](#bib.bib79)], was
    proposed for localizing multiple types of image manipulations. It is composed
    of three main blocks i.e. feature extraction (head) module, pyramid spatial attention
    module, and decision (tail) module. The head module employs the Wider & Deeper
    VGG Network as the backbone, while Bayer and SRM layers extract features from
    visual artifacts and noise patterns. The spatial relationship of the pixels is
    achieved through five local self-attention blocks applied recursively, and to
    preserve the details, the input of the self-attention is added to the output of
    the block as shown in Figure [4](#S2.F4 "Figure 4 ‣ 2.1.2 Spatial Attention ‣
    2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual Attention
    Methods in Deep Learning: An In-Depth Survey")(c). These features are then fed
    into the final tail module of 2D convolutional blocks to generate the output mask
    after employing a sigmoid activation.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '空间金字塔注意力网络，简称SPAN [[79](#bib.bib79)]，被提出用于本地化多种图像操作。它由三个主要模块组成，即特征提取（头）模块、金字塔空间注意力模块和决策（尾）模块。头模块使用Wider
    & Deeper VGG网络作为骨干网络，而Bayer和SRM层从视觉伪影和噪声模式中提取特征。像素的空间关系通过五个局部自注意力块递归应用来实现，为了保留细节，自注意力的输入与块的输出相加，如图[4](#S2.F4
    "Figure 4 ‣ 2.1.2 Spatial Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey")(c)所示。这些特征然后被送入最终的2D卷积块尾模块，以在应用sigmoid激活后生成输出掩模。'
- en: 'Spatial-Spectral Self-Attention: Figure [4](#S2.F4 "Figure 4 ‣ 2.1.2 Spatial
    Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual
    Attention Methods in Deep Learning: An In-Depth Survey")(d) shows the architecture
    of spatial-spectral self-attention which is composed of two attention modules,
    namely, spatial attention and spectral attention, both utilizing self-attention.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '空间-频谱自注意力：图[4](#S2.F4 "Figure 4 ‣ 2.1.2 Spatial Attention ‣ 2.1 Soft (Deterministic)
    Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning:
    An In-Depth Survey")(d)展示了空间-频谱自注意力的架构，它由两个注意力模块组成，即空间注意力和频谱注意力，两者都利用自注意力。'
- en: '1.'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Spatial Attention: To model the non-local region information, Meng et al. [[40](#bib.bib40)]
    utilize a 3$\times$3 kernel to fuse the input features indicating the region-based
    correlation followed by a convolutional network mapping the fused features into
    Q $\&amp;$ K. The kernel number indicates the heads’ number and the size denotes
    the dimension. Moreover, the dimension-specified features from Q $\&amp;$ K build
    the related attention maps, then modulating the corresponding dimension in a sequence
    to achieve the order-independent property. Finally, to finish the spatial correlation
    modeling, the features are forwarded to a deconvolution layer.'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 空间注意力：为了建模非局部区域信息，Meng等人[[40](#bib.bib40)]利用3$\times$3卷积核来融合输入特征，以指示基于区域的相关性，然后通过卷积网络将融合特征映射到Q
    $\&amp;$ K。卷积核的数量表示头的数量，大小表示维度。此外，来自Q $\&amp;$ K的维度指定特征构建相关的注意力图，然后在序列中调制相应的维度，以实现顺序无关属性。最后，为了完成空间相关建模，特征被传递到反卷积层。
- en: '2.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Spectral Attention: First, the spectral channel samples are convolved with
    one kernel and flattened into a signle dimension, set as the feature vector for
    that channel. The input feature is converted to Q $\&amp;$ K, building attention
    maps for the spectral axis. The adjacent channels have a higher correlation due
    to the image patterns on the exact location, denoted via a spectral smoothness
    on the attention maps. The similarity is indicated by normalized cosine distance
    as spectral embedding where each similar score is scaled and summed with the coefficients
    in the attention maps, which is then to modulate “Value” in self-attention, inducing
    spectral smoothness constraint.'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 频谱注意力：首先，频谱通道样本与一个卷积核进行卷积，并展平成单一维度，作为该通道的特征向量。输入特征被转换为Q $\&amp;$ K，为频谱轴建立注意力图。由于图像模式在精确位置上的相邻通道具有更高的相关性，这通过注意力图上的频谱平滑度进行表示。相似度通过归一化的余弦距离表示为频谱嵌入，其中每个相似分数被缩放并与注意力图中的系数求和，然后用于调制自注意力中的“值”，从而引入频谱平滑度约束。
- en: 'Pixel-wise Contextual Attention: (PiCANet) [[55](#bib.bib55)] aims to learn
    accurate saliency detection. PiCANet generates a map at each pixel over the context
    region and constructs an accompanied contextual feature to enhance the feature
    representability at the local and global levels. To generate global attention,
    each pixel needs to “see” via ReNet [[80](#bib.bib80)] with four recurrent neural
    networks sweeping horizontally and vertically. The contexts from directions, using
    biLSTM, are blended propagating information of each pixel to all other pixels.
    Next, a convolutional layer transforms the feature maps to different channels,
    further normalized by a softmax function used to weight the feature maps. The
    local attention is performed on a local neighborhood forming a local feature cube
    where each pixel needs to “see” every other pixel in the local area using a few
    convolutional layers having the same receptive field as the patch. The features
    are then transformed to channel and normalized using softmax, which are further
    weighted summed to get the final attention.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 像素级上下文注意力（PiCANet）[[55](#bib.bib55)]旨在实现准确的显著性检测。PiCANet 在每个像素的上下文区域生成一个映射，并构建一个附加的上下文特征，以增强局部和全局水平的特征表现能力。为了生成全局注意力，每个像素需要通过
    ReNet [[80](#bib.bib80)] 进行“观察”，该网络包含四个水平和垂直的递归神经网络。来自不同方向的上下文使用 biLSTM 进行混合，将每个像素的信息传播到所有其他像素。接着，卷积层将特征图转换为不同的通道，并通过软最大化函数进行进一步的归一化，用于对特征图加权。局部注意力在局部邻域上进行，形成一个局部特征立方体，其中每个像素需要使用具有相同感受野的几个卷积层“观察”局部区域中的每个其他像素。然后，这些特征被转换为通道并使用
    softmax 进行归一化，进一步加权求和以得到最终的注意力。
- en: Pyramid Feature Attention extracts features from different levels of VGG [[38](#bib.bib38)].
    The low-level features extracted from lower layers of VGG are provided to the
    spatial attention mechanism [[25](#bib.bib25)], and the high-level features obtained
    from the higher layers are supplied to a channel attention mechanism [[25](#bib.bib25)].
    The term feature pyramid attention originates form the VGG features obtained from
    different layers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 金字塔特征注意力从不同层的 VGG [[38](#bib.bib38)] 中提取特征。低层 VGG 提取的低级特征被提供给空间注意力机制 [[25](#bib.bib25)]，而从高层获得的高级特征则被供给给通道注意力机制
    [[25](#bib.bib25)]。特征金字塔注意力一词源于从不同层获得的 VGG 特征。
- en: 'Spatial Attention Pyramid: For unsupervised domain adaptation, Li et al. [[39](#bib.bib39)]
    introduced a spatial attention pyramid that takes features from multiple average
    pooling layers with various sizes operating on feature maps. These features are
    forwarded to spatial attention followed by channel-wise attention. All the features
    after attention are concatenated to form a single semantic vector.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 空间注意力金字塔：为了进行无监督领域适应，Li 等 [[39](#bib.bib39)] 引入了一个空间注意力金字塔，该金字塔从多个具有不同大小的平均池化层中提取特征。这些特征被传递到空间注意力，然后是通道级注意力。所有经过注意力处理的特征被拼接成一个单一的语义向量。
- en: 'Region Attention Network: (RANet) [[81](#bib.bib81)] was proposed for semantic
    segmentation. It consists of novel network components, the Region Construction
    Block (RCB) and the Region Interaction Block (RIB), for constructing the contextual
    representations as illustrated in Figure [4](#S2.F4 "Figure 4 ‣ 2.1.2 Spatial
    Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual
    Attention Methods in Deep Learning: An In-Depth Survey")(b). The RCB analyzes
    the boundary score and the semantic score maps jointly to compute the attention
    region score for each image pixel pair. High attention score indicates that the
    pixels are from the same object region, dividing the image into various object
    regions. Subsequently, the RIB takes the region maps and selects the representative
    pixels in different regions where each representative pixel receives the context
    from other pixels to effectively represent the object region’s local content.
    Furthermore, capturing the spatial and category relationship between various objects
    communicating the representative pixels in the different regions yields the global
    contextual representation to augment the pixels, eventually forming the contextual
    feature map for segmentation.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 区域注意力网络（RANet） [[81](#bib.bib81)] 被提出用于语义分割。它包含了新颖的网络组件，即区域构建块（RCB）和区域交互块（RIB），用于构建上下文表示，如图 [4](#S2.F4
    "图 4 ‣ 2.1.2 空间注意力 ‣ 2.1 软（确定性）注意力 ‣ 2 注意力在视觉中的应用 ‣ 深度学习中的视觉注意力方法：深入调研")(b) 所示。RCB
    分析边界分数和语义分数图，以计算每个图像像素对的注意力区域分数。高注意力分数表明这些像素来自同一物体区域，从而将图像划分为不同的物体区域。随后，RIB 采用区域图，并选择不同区域中的代表性像素，每个代表性像素接收来自其他像素的上下文信息，以有效地表示物体区域的局部内容。此外，捕捉不同物体之间的空间和类别关系，交流不同区域的代表性像素，产生全局上下文表示以增强像素，最终形成用于分割的上下文特征图。
- en: '| ![Refer to caption](img/31758849ac217374304b66f8cf74b6be.png) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/31758849ac217374304b66f8cf74b6be.png) |'
- en: '| (a) Spatial Attention [[25](#bib.bib25)] |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| (a) 空间注意力 [[25](#bib.bib25)] |'
- en: '| ![Refer to caption](img/e7a99821a5352fb1e01cc4d48c055c28.png) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/e7a99821a5352fb1e01cc4d48c055c28.png) |'
- en: '| (b) RANet [[81](#bib.bib81)] |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| (b) RANet [[81](#bib.bib81)] |'
- en: '| ![Refer to caption](img/a6b4a1a66760ee6f166398382731cd94.png) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/a6b4a1a66760ee6f166398382731cd94.png) |'
- en: '| (c) Co-excite [[77](#bib.bib77)] |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| (c) Co-excite [[77](#bib.bib77)] |'
- en: 'Figure 4: The structures of the spatial-based attention methods, including
    RANet [[81](#bib.bib81)], and Co-excite [[77](#bib.bib77)]. These methods focus
    on attending to the most important parts in the spatial map. The images are taken
    from [[25](#bib.bib25), [81](#bib.bib81), [77](#bib.bib77)].'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：基于空间的注意力方法的结构，包括 RANet [[81](#bib.bib81)] 和 Co-excite [[77](#bib.bib77)]。这些方法关注于空间图中最重要的部分。图像来源于 [[25](#bib.bib25),
    [81](#bib.bib81), [77](#bib.bib77)]。
- en: 2.1.3 Self-attention
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 自注意力
- en: Self-attention, also known as *intra-attention*, is an attention mechanism that
    encodes the relationships between all the input entities. It is a process that
    enables input sequences to interact with each other and aggregate the attention
    scores, which illustrate how similar they are. The main idea is to replicate the
    feature maps into three copies and then measure the similarity between them. Apart
    from channel- and spatial-wise attention that use the physical feature maps, self-attention
    replicates feature copies to measure long-range dependencies. However, self-attention
    methods use channels to calculate attention scores. Cheng et al. extracted the
    correlations between the words of a single sentence using Long-Short-Term Memory
    (LSTM) [[42](#bib.bib42)]. An attention vector is produced from each hidden state
    during the recurrent iteration, which attends all the responses in a sequence
    for this position. In [[82](#bib.bib82)], a decomposable solution was proposed
    to divide the input into sub-problems, which improved the processing efficiency
    compared to [[42](#bib.bib42)]. The attention vector is calculated as an alignment
    factor to the content (bag-of-words). Although these methods introduced the idea
    of self-attention, they are very expensive in terms of resources and do not consider
    contextual information. Also, RNN models process input sequentially; hence, it
    is difficult to parallelize or process large-scale schema efficiently.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制，也被称为*内部注意力*，是一种编码所有输入实体之间关系的注意力机制。它是一种使输入序列相互作用并汇总注意力分数的过程，这些分数展示了它们的相似程度。其主要思想是将特征图复制成三个副本，然后测量它们之间的相似性。除了使用物理特征图的通道和空间注意力外，自注意力通过复制特征副本来测量长距离依赖关系。然而，自注意力方法使用通道来计算注意力分数。Cheng等人使用长短期记忆网络（LSTM）提取单个句子中单词之间的相关性[[42](#bib.bib42)]。在递归迭代过程中，每个隐藏状态产生一个注意力向量，这个向量关注序列中该位置的所有响应。在[[82](#bib.bib82)]中，提出了一种可分解的解决方案，将输入划分为子问题，相比于[[42](#bib.bib42)]，提高了处理效率。注意力向量被计算为内容（词袋）的对齐因子。尽管这些方法引入了自注意力的概念，但它们在资源消耗上非常昂贵，并且未考虑上下文信息。此外，RNN模型顺序处理输入，因此很难并行化或高效处理大规模方案。
- en: 'Transformers: Vaswani et al. [[2](#bib.bib2)] proposed a new method, called
    transformers, based on the self-attention concept without convolution or recurrent
    modules. As shown in Figure [5](#S2.F5 "Figure 5 ‣ 2.1.3 Self-attention ‣ 2.1
    Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods
    in Deep Learning: An In-Depth Survey") (f), it is mainly composed of encoder-decoder
    layers, where the encoder comprises of self-attention module followed by positional
    feed-forward layer and the decoder is the same as the encoder except that it has
    an encoder-decoder attention layer in between. Positional encoding is represented
    by a sine wave that incorporates the passage of time as input before the linear
    layer. This positional encoding serves as a generalization term to help recognize
    unseen sequences and encodes relative positions rather than absolute representations.
    Algorithm [1](#alg1 "In 2.1.3 Self-attention ‣ 2.1 Soft (Deterministic) Attention
    ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth
    Survey") shows the detailed steps of calculating self-attention (multi-head attention)
    using transformers. Although transformers have achieved much progress in the text-based
    models, it lacks in encoding the context of the sentence because it calculates
    the word’s attention for the left-side sequences. To address this issue, Bidirectional
    Encoder Representations from Transformers (BERT) learns the contextual information
    by encoding both sides of the sentence jointly [[83](#bib.bib83)].'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers：Vaswani等人[[2](#bib.bib2)]提出了一种基于自注意力概念的新方法，称为transformers，该方法不使用卷积或递归模块。如图[5](#S2.F5
    "图5 ‣ 2.1.3 自注意力 ‣ 2.1 软件（确定性）注意力 ‣ 2 视觉中的注意力 ‣ 深度学习中的视觉注意力方法：深入调查") (f)所示，它主要由编码器-解码器层组成，其中编码器包括自注意力模块，随后是位置前馈层，而解码器与编码器相同，只是在中间有一个编码器-解码器注意力层。位置编码由一个正弦波表示，该波在线性层之前将时间的推移作为输入。这种位置编码作为一个泛化项，帮助识别未见过的序列，并编码相对位置而不是绝对表示。算法[1](#alg1
    "在2.1.3 自注意力 ‣ 2.1 软件（确定性）注意力 ‣ 2 视觉中的注意力 ‣ 深度学习中的视觉注意力方法：深入调查")展示了使用transformers计算自注意力（多头注意力）的详细步骤。尽管transformers在基于文本的模型中取得了很大进展，但在编码句子上下文方面存在不足，因为它计算了左侧序列的词的注意力。为了解决这个问题，Bidirectional
    Encoder Representations from Transformers（BERT）通过联合编码句子的两侧来学习上下文信息[[83](#bib.bib83)]。
- en: 'Input : set of sequences $(x_{1},x_{2},...,x_{n})$ of an entity $\mathbf{X}\in\mathbf{R}$Output
    : attention scores of $\mathbf{X}$ sequences.1 Initialize weights: Key ($\mathbf{W_{K}}$),
    Query ($\mathbf{W_{Q}}$), Value ($\mathbf{W_{V}}$) for each input sequence.2 Derive
    Key, Query, Value for each input sequence and its corresponding weight, such that
    $\mathbf{Q=XW_{Q}}$, $\mathbf{K=XW_{K}}$, $\mathbf{V=XV_{Q}}$, respectively.3
    Compute attention scores by calculating the dot product between the query and
    key.4 Compute the scaled-dot product attention for these scores and Values $\mathbf{V}$,'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：实体$\mathbf{X}\in\mathbf{R}$的一组序列$(x_{1},x_{2},...,x_{n})$ 输出：$\mathbf{X}$序列的注意力得分。1
    初始化权重：每个输入序列的Key（$\mathbf{W_{K}}$）、Query（$\mathbf{W_{Q}}$）、Value（$\mathbf{W_{V}}$）。2
    为每个输入序列及其对应的权重推导Key、Query、Value，使得$\mathbf{Q=XW_{Q}}$，$\mathbf{K=XW_{K}}$，$\mathbf{V=XV_{Q}}$。3
    通过计算查询和键的点积来计算注意力得分。4 计算这些得分和Values $\mathbf{V}$的缩放点积注意力，
- en: '|  | $\mathrm{softmax}\left(\frac{\mathbf{QK^{T}}}{\sqrt{d_{k}}}\right)\mathbf{V}.$
    |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{softmax}\left(\frac{\mathbf{QK^{T}}}{\sqrt{d_{k}}}\right)\mathbf{V}.$
    |  |'
- en: 5 repeat steps from 1 to 4 for all the heads
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 5 对所有头重复步骤1到4
- en: Algorithm 1 The main steps of generating self-attention by transformers (multi-head
    attention)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 使用transformers（多头注意力）生成自注意力的主要步骤
- en: 'Standalone self-attention: As stated above, convolutional features do not consider
    the global information due to their local-biased receptive fields. Instead of
    augmenting attentional features to the convolutional ones, Ramachandran et al. [[43](#bib.bib43)]
    proposed a fully-attentional network that replaces spatial convolutions with self-attentional
    modules. The convolutional stem (the first few convolutions) is used to capture
    spatial information. They designed a small kernel (e.g. $n\times n$) instead of
    processing the whole image simultaneously. This design built a computationally
    efficient model that enables processing images with their original sizes without
    downsampling. The computation complexity is reduced to $\mathcal{O}(hwn^{2})$,
    where $h$ and $w$ denotes height and width, respectively. A patch is extracted
    as a query along with each local patch, while the identity image is used as Value
    and Key. Calculating the attention maps follows the same steps as in Algorithm
    [1](#alg1 "In 2.1.3 Self-attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey"). Although
    stand-alone self-attention shows competitive results compared to convolutional
    models, it suffers from encoding positional information.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 独立自注意力：如上所述，卷积特征由于其局部偏置的感受野而未考虑全局信息。Ramachandran 等人 [[43](#bib.bib43)] 提出了一个完全注意力的网络，用自注意力模块替代空间卷积。卷积干（前几层卷积）用于捕获空间信息。他们设计了一个小内核（例如
    $n\times n$），而不是同时处理整个图像。这一设计构建了一个计算高效的模型，使其能够处理原始大小的图像而无需下采样。计算复杂度减少到 $\mathcal{O}(hwn^{2})$，其中
    $h$ 和 $w$ 分别表示高度和宽度。提取一个补丁作为查询以及每个局部补丁，同时使用身份图像作为值和键。计算注意力图遵循与算法 [1](#alg1 "在
    2.1.3 自注意力 ‣ 2.1 软（确定性）注意力 ‣ 2 注意力在视觉中 ‣ 深度学习中的视觉注意力方法：深入调查") 相同的步骤。尽管独立自注意力与卷积模型相比显示了有竞争力的结果，但它在编码位置信息方面存在问题。
- en: 'Clustered Attention: To address the computational inefficiency of transformers,
    Vyas et al. [[84](#bib.bib84)] proposed a clustered attention mechanism that relies
    on the idea that correlated queries follow the same distribution around Euclidean
    centers. Based on this idea, they use the K-means algorithm with fixed centers
    to group similar queries together. Instead of calculating the queries for attention,
    they are calculated for clusters’ centers. Therefore, the total complexity is
    minimized to a linear form $\mathcal{O}(qc)$, where $q$ is the number of the queries
    while $c$ is the cluster number.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类注意力：为了应对变换器的计算低效问题，Vyas 等人 [[84](#bib.bib84)] 提出了一个聚类注意力机制，该机制依赖于这样的观点：相关的查询在欧几里得中心附近遵循相同的分布。基于这一观点，他们使用带有固定中心的K均值算法将相似的查询进行分组。计算时不是计算所有查询，而是计算聚类中心。因此，总复杂度被最小化为线性形式
    $\mathcal{O}(qc)$，其中 $q$ 是查询的数量，而 $c$ 是聚类的数量。
- en: '| ![Refer to caption](img/65311e0608f3a25767b5ea6fb4a5cfde.png) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/65311e0608f3a25767b5ea6fb4a5cfde.png) |'
- en: '| (a) Efficient Attention [[85](#bib.bib85)] |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| (a) 高效注意力 [[85](#bib.bib85)] |'
- en: '| ![Refer to caption](img/98c6e052393dec2d7e471ab814a5e1ea.png) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/98c6e052393dec2d7e471ab814a5e1ea.png) |'
- en: '| (b) Slot Attention [[86](#bib.bib86)] |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| (b) 插槽注意力 [[86](#bib.bib86)] |'
- en: '| ![Refer to caption](img/a2720a8d502b4d4cb64c1dd7eb5420fd.png) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/a2720a8d502b4d4cb64c1dd7eb5420fd.png) |'
- en: '| (c) RFA [[87](#bib.bib87)] |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| (c) RFA [[87](#bib.bib87)] |'
- en: '| ![Refer to caption](img/3ec4801e5b71cb0e1aa0dea45aa0c6c0.png) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/3ec4801e5b71cb0e1aa0dea45aa0c6c0.png) |'
- en: '| (d) X-Linear [[88](#bib.bib88)] |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| (d) X-Linear [[88](#bib.bib88)] |'
- en: '| ![Refer to caption](img/339ac8333b2da5e138631b80fddba552.png) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/339ac8333b2da5e138631b80fddba552.png) |'
- en: '| (e) Axial [[89](#bib.bib89)] |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| (e) 轴向 [[89](#bib.bib89)] |'
- en: '| ![Refer to caption](img/50b95bcaeaf321d08ac55b5d5c4b2aac.png) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/50b95bcaeaf321d08ac55b5d5c4b2aac.png) |'
- en: '| (f) Transformer [[2](#bib.bib2)] |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| (f) 变换器 [[2](#bib.bib2)] |'
- en: 'Figure 5: The architectures of self-attention methods: Transformers [[2](#bib.bib2)],
    Axial attention [[89](#bib.bib89)], X-Linear [[88](#bib.bib88)], Slot [[86](#bib.bib86)]
    and RFA [[87](#bib.bib87)] (pictures taken from the corresponding articles). All
    of these methods are self-attention, which generate the scores from measuring
    the similarity between two maps of the same input. However, there is difference
    in the way of processing.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：自注意力方法的架构：变换器 [[2](#bib.bib2)]、轴向注意力 [[89](#bib.bib89)]、X-Linear [[88](#bib.bib88)]、插槽 [[86](#bib.bib86)]
    和 RFA [[87](#bib.bib87)]（图片取自相关文章）。所有这些方法都是自注意力，它们通过测量相同输入的两个图的相似性来生成分数。然而，在处理方式上有所不同。
- en: 'Slot Attention: Locatello et al. [[90](#bib.bib90)] proposed slot attention,
    an attention mechanism that learns the objects’ representations in a scene. In
    general, it learns to disentangle the image into a series of slots. As shown in
    Figure [5](#S2.F5 "Figure 5 ‣ 2.1.3 Self-attention ‣ 2.1 Soft (Deterministic)
    Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning:
    An In-Depth Survey")(d), each slot represents a single object. Slot attention
    module is applied to a learned representation $h\in\mathbb{R}^{W\times H\times
    D}$, where $H$ is the height, $W$ is the width, and $D$ is the representation
    size. SAM has two main steps: learning $n$ slots using an iterative attention
    mechanism and representing individual objects (slots). Inside each iteration,
    two operations are implemented: 1) slot competition using softmax followed by
    normalization according to slot dimension using this equation'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 'Slot Attention: Locatello 等人[[90](#bib.bib90)] 提出了 slot attention，这是一种学习场景中物体表示的注意力机制。一般来说，它学习将图像解耦为一系列的
    slots。如图 [5](#S2.F5 "Figure 5 ‣ 2.1.3 Self-attention ‣ 2.1 Soft (Deterministic)
    Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning:
    An In-Depth Survey")(d) 所示，每个 slot 代表一个单一物体。Slot attention 模块应用于一个学习到的表示 $h\in\mathbb{R}^{W\times
    H\times D}$，其中 $H$ 是高度，$W$ 是宽度，$D$ 是表示大小。SAM 主要有两个步骤：使用迭代注意力机制学习 $n$ 个 slots，并表示各个物体（slots）。在每次迭代中，执行两个操作：1)
    使用 softmax 进行 slot 竞争，然后按照 slot 维度进行归一化，使用以下方程'
- en: '|  | $a=Softmax\bigg{(}\frac{1}{\sqrt{D}}n(h).q(c)^{T}\bigg{)}.$ |  | (4) |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $a=Softmax\bigg{(}\frac{1}{\sqrt{D}}n(h).q(c)^{T}\bigg{)}.$ |  | (4) |'
- en: 2) An aggregation process for the attended representations with a weighted mean
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 对被关注表示的加权均值聚合过程
- en: '|  | $r=Weightedmean\bigg{(}a,v(h)\bigg{)},$ |  | (5) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | $r=Weightedmean\bigg{(}a,v(h)\bigg{)},$ |  | (5) |'
- en: where $k,q,v$ are learnable variables as showed in [[2](#bib.bib2)]. Then, a
    feed-forward layer is used to predict the slot representations $s=fc(r)$.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $k,q,v$ 是可学习变量，如 [[2](#bib.bib2)] 所示。然后，使用前馈层预测 slot 表示 $s=fc(r)$。
- en: Slot attention is based on Transformer-like attention [[2](#bib.bib2)] on top
    of CNN-feature extractors. Given an image $\mathbb{I}$, the slot attention parses
    the scene into a set of slots, each one referring to an object $(z,x,m)$, where
    $z$ is the object feature, $x$ is the input image and $m$ is the mask. In the
    decoders, convolutional networks are used to learn slot representations and object
    masks. The training process is guided by $\ell_{2}$-norm loss
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Slot attention 基于 Transformer-like 注意力[[2](#bib.bib2)]，在 CNN 特征提取器之上。给定一个图像
    $\mathbb{I}$，slot attention 将场景解析为一组 slots，每个 slot 指代一个物体 $(z,x,m)$，其中 $z$ 是物体特征，$x$
    是输入图像，$m$ 是掩码。在解码器中，卷积网络用于学习 slot 表示和物体掩码。训练过程由 $\ell_{2}$-norm 损失引导
- en: '|  | $\mathbb{L}=\bigg{\rVert}\bigg{(}\sum_{k=1}^{K}mx_{k}\bigg{)}-\mathbb{I}\bigg{\lVert}_{2}^{2}$
    |  | (6) |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{L}=\bigg{\rVert}\bigg{(}\sum_{k=1}^{K}mx_{k}\bigg{)}-\mathbb{I}\bigg{\lVert}_{2}^{2}$
    |  | (6) |'
- en: Following the slot-attention module, Li et al. developed an explainable classifier
    based on slot attentions [[90](#bib.bib90)]. This method aims to find the positive
    supports and negatives ones for a class $l$. In this way, a classifier can also
    be explained rather than being completely black-box. The primary entity of this
    work is xSlot, a variant of slot attention [[86](#bib.bib86)], which is related
    to a category and gives confidence for inclusion of this category in the input
    image.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在 slot-attention 模块之后，Li 等人开发了一种基于 slot attentions 的可解释分类器[[90](#bib.bib90)]。该方法旨在为一个类别
    $l$ 找到正支持和负支持。通过这种方式，分类器不仅可以被解释，而不是完全成为黑箱。该工作的主要实体是 xSlot，一种 slot attention 的变体[[86](#bib.bib86)]，与一个类别相关，并对该类别在输入图像中的包含情况给出信心。
- en: 'Efficient Attention: Using Asymmetric Clustering (SMYRF) Daras et al. [[91](#bib.bib91)]
    proposed symmetric Locality Sensitive Hashing (LSH) clustering in a novel way
    to reduce the size of attention maps, therefore, developing efficient models.
    They observed that attention weights are sparse as well as the attention matrix
    is low-rank. As a result, pre-trained models pertain to decay in their values.
    In SMYRF, this issue is addressed by approximating attention maps through balanced
    clustering, produced by asymmetric transformations and an adaptive scheme. SMYRF
    is a drop-in replacement for pre-trained models for normal dense attention. Without
    retraining models after integrating this module, SMYRF showed significant effectiveness
    in memory, performance, and speed. Therefore, the feature maps can be scaled up
    to include contextual information. In some models, the memory usage is reduced
    by $50\%$. Although SMYRF enhanced memory usage in self-attention models, the
    improvement over efficient attention models is marginal (see Figure [5](#S2.F5
    "Figure 5 ‣ 2.1.3 Self-attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey") (a)).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '高效注意力：使用非对称聚类（SMYRF），Daras 等人 [[91](#bib.bib91)] 提出了对称局部敏感哈希 (LSH) 聚类的创新方法，以减少注意力图的大小，从而开发出高效的模型。他们观察到注意力权重是稀疏的，注意力矩阵是低秩的。因此，预训练模型的值会衰减。在
    SMYRF 中，通过平衡聚类（由非对称变换和自适应方案生成）来解决这个问题，从而近似注意力图。SMYRF 是一种替代预训练模型的正常密集注意力的模块，无需在集成此模块后重新训练模型，SMYRF
    在内存、性能和速度方面显示出了显著的效果。因此，特征图可以扩大以包含上下文信息。在某些模型中，内存使用减少了 $50\%$。尽管 SMYRF 在自注意力模型中提高了内存使用，但与高效注意力模型的改进幅度是微不足道的（见图
    [5](#S2.F5 "Figure 5 ‣ 2.1.3 Self-attention ‣ 2.1 Soft (Deterministic) Attention
    ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth
    Survey") (a)）。'
- en: 'Random Feature Attention: Transformers have a major shortcoming with regards
    to time and memory complexity, which hinder attention scaling up and thus limiting
    higher-order interactions. Peng et al. [[87](#bib.bib87)] proposed reducing the
    space and time complexity of transformers from quadratic to linear. They simply
    enhance softmax approximation using random functions. Random Feature Attention
    (RFA) [[92](#bib.bib92)] uses a variant of softmax that is sampled from simple
    distribution-based Fourier random features [[93](#bib.bib93), [94](#bib.bib94)].
    Using the kernel trick $exp(x.y)\approx\phi(x).\phi(y)$ of [[95](#bib.bib95)],
    softmax approximation is reduced to linear as shown in Figure [5](#S2.F5 "Figure
    5 ‣ 2.1.3 Self-attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in
    Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey") (c).
    Moreover, the similarity of RFA connections and recurrent networks help in developing
    a gating mechanism to learn recency bias [[96](#bib.bib96), [97](#bib.bib97),
    [98](#bib.bib98)]. RFA can be integrated into backbones easily to replace the
    normal softmax without much increase in the number of parameters, only $0.1\%$
    increase. Plugging RFA into a transformer show comparable results to softmax,
    while gating RFA outperformed it in language models. RFA executes 2$\times$ faster
    than a conventional transformer.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '随机特征注意力：变压器在时间和内存复杂性方面存在一个主要缺陷，这限制了注意力的扩展，并因此限制了更高阶的交互。Peng 等人 [[87](#bib.bib87)]
    提出了将变压器的空间和时间复杂性从二次方降低到线性。他们通过使用随机函数来简单地增强 softmax 近似。随机特征注意力 (RFA) [[92](#bib.bib92)]
    使用从基于简单分布的傅里叶随机特征 [[93](#bib.bib93), [94](#bib.bib94)] 中采样的 softmax 变体。利用 [[95](#bib.bib95)]
    的核技巧 $exp(x.y)\approx\phi(x).\phi(y)$，softmax 近似被简化为线性，如图 [5](#S2.F5 "Figure 5
    ‣ 2.1.3 Self-attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision
    ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey") (c) 所示。此外，RFA
    连接的相似性和递归网络有助于开发一种门控机制，以学习近期偏差 [[96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98)]。RFA
    可以轻松集成到骨干网络中，以替代普通的 softmax，而不会显著增加参数数量，仅增加了 $0.1\%$。将 RFA 插入到变压器中显示出与 softmax
    相当的结果，而门控 RFA 在语言模型中表现优于 softmax。RFA 的执行速度是传统变压器的 2$\times$ 快。'
- en: 'Non-local Networks: Recent breakthroughs in the field of artificial intelligence
    are mostly based on the success of Convolution Neural Networks (CNNs) [[99](#bib.bib99),
    [24](#bib.bib24)]. In particular, they can be processed in parallel mode and are
    inductive biases for the extracted features. However, CNNs fail to learn the context
    of the whole image due to their local-biased receptive fields. Therefore, long-range
    dependencies are disregarded in CNNs. In [[3](#bib.bib3)], Wang et al. proposed
    non-local networks to alleviate the bias of CNNs towards the local information
    and fuse global information into the network. It augments each pixel of the convolutional
    features with the contextual information, the weighted sum of the whole feature
    map. In this manner, the correlated patches in an image are encoded in a long-range
    fashion. Non-local networks showed significant improvement in long-range interaction
    tasks such as video classification [[100](#bib.bib100)] as well as low-level image
    processing [[101](#bib.bib101), [102](#bib.bib102)]. Non-local model attention
    in the network in a graphical fashion [[103](#bib.bib103)]. However, stacking
    multiple non-local modules in the same stage shows instability and ill-pose in
    the training process [[104](#bib.bib104)]. In [[105](#bib.bib105)], Liu et al.
    uses non-local networks to form self-mutual attention between two modalities (RGB
    and Depth) to learn global contextual information. The idea is straightforward
    i.e. to sum the corresponding features before softmax normalization such that
    $\mathrm{softmax}(f^{r}(\mathbb{X}^{r})+\alpha^{d}\bigodot f^{d}(\mathbb{X}^{d}))$
    for RGB attention and vice versa.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 非局部网络：近期在人工智能领域的突破大多基于卷积神经网络（CNNs）[[99](#bib.bib99), [24](#bib.bib24)] 的成功。特别是，它们可以并行处理，并且是提取特征的归纳偏差。然而，CNNs
    由于其局部偏向的接收域，未能学习整个图像的上下文。因此，CNNs 忽略了长距离依赖。在[[3](#bib.bib3)]中，Wang等人提出了非局部网络，以缓解CNNs对局部信息的偏差，并将全局信息融合到网络中。它通过整个特征图的加权和来增强卷积特征的每个像素。在这种方式下，图像中的相关补丁以长距离的方式进行编码。非局部网络在长距离交互任务（如视频分类[[100](#bib.bib100)]）以及低级图像处理[[101](#bib.bib101),
    [102](#bib.bib102)]中显示了显著的改善。非局部模型注意力在网络中以图形方式展现[[103](#bib.bib103)]。然而，在同一阶段堆叠多个非局部模块会显示出不稳定性和训练过程中的不良姿态[[104](#bib.bib104)]。在[[105](#bib.bib105)]中，Liu等人利用非局部网络形成两个模态（RGB和深度）之间的自互注意力，以学习全局上下文信息。其思路很简单，即在softmax归一化之前将相应的特征相加，如$\mathrm{softmax}(f^{r}(\mathbb{X}^{r})+\alpha^{d}\bigodot
    f^{d}(\mathbb{X}^{d}))$ 用于RGB注意力，反之亦然。
- en: 'Non-Local Sparse Attention (NLSA): Mei et al. [[106](#bib.bib106)] proposed
    a sparse non-local network to combine the benefits of non-local modules to encode
    long-range dependencies and sparse representations for robustness. Deep features
    are split into different groups (buckets) with high inner correlations. Locality
    Sensitive Hashing (LSH) [[107](#bib.bib107)] is used to find similar features
    to each bucket. Then, the Non-Local block processes the pixel within its bucket
    and the similar ones. NLSA reduces the complexity to asymptotic linear from quadratic
    as well as uses the power of sparse representations to focus on informative regions
    only.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 非局部稀疏注意力（NLSA）：Mei等人[[106](#bib.bib106)] 提出了一个稀疏的非局部网络，将非局部模块的优点结合起来，以编码长距离依赖和稀疏表示，从而提高鲁棒性。深度特征被分割成不同的组（桶），这些组内具有高内相关性。局部敏感哈希（LSH）[[107](#bib.bib107)]
    用于查找与每个桶相似的特征。然后，非局部块处理其桶内的像素以及相似的像素。NLSA 将复杂度从二次降至渐近线性，同时利用稀疏表示的优势，仅关注信息丰富的区域。
- en: 'X-Linear Attention: Bilinear pooling is a calculation process that computers
    the outer product between two entities rather than the inner product [[108](#bib.bib108),
    [109](#bib.bib109), [110](#bib.bib110), [111](#bib.bib111)] and has shown the
    ability to encode higher-order interaction and thus encourage more discriminability
    in the models. Moreover, it yields compact models with the required details even
    though it compresses the representations [[110](#bib.bib110)]. In particular,
    bilinear applications has shown significant improvements in fine-grained visual
    recognition [[112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114)] and visual
    question answering [[115](#bib.bib115)]. As Figure [5](#S2.F5 "Figure 5 ‣ 2.1.3
    Self-attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣
    Visual Attention Methods in Deep Learning: An In-Depth Survey")(e) depicts, a
    low-rank bilinear pooling is performed between queries and keys, and hence, the
    $2^{nd}$-order interactions between keys and queries are encoded. Through this
    query-key interaction, spatial-wise and channel-wise attention are aggregated
    with the values. The channel-wise attention is the same as squeeze-excitation
    attention [[26](#bib.bib26)]. The final output of the x-linear module is aggregated
    with the low-rank bilinear of keys and values [[88](#bib.bib88)]. They claimed
    that encoding higher interactions require only repeating the x-linear module accordingly
    (e.g. three iterative x-linear blocks for $4^{th}$-order interactions). Modeling
    infinity-order interaction is also explained by the usage of h Exponential Linear
    Unit [[116](#bib.bib116)]. X-Linear attention module proposes a novel mechanism
    to attentions, different from transformer [[2](#bib.bib2)]. It is able to encode
    the relations between input tokens without positional encoding with only linear
    complexity as opposed to quadratic in transformer.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 'X-Linear 注意力：双线性池化是一种计算过程，它计算两个实体之间的外积，而不是内积[[108](#bib.bib108), [109](#bib.bib109),
    [110](#bib.bib110), [111](#bib.bib111)]，并显示了编码高阶交互的能力，从而在模型中增强了可区分性。此外，它即使在压缩表示的情况下，也能生成具有所需细节的紧凑模型[[110](#bib.bib110)]。特别是，双线性应用在细粒度视觉识别[[112](#bib.bib112),
    [113](#bib.bib113), [114](#bib.bib114)]和视觉问答[[115](#bib.bib115)]中显示出了显著的改进。如图
    [5](#S2.F5 "Figure 5 ‣ 2.1.3 Self-attention ‣ 2.1 Soft (Deterministic) Attention
    ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth
    Survey")(e) 所示，在查询和键之间执行低秩双线性池化，因此，键和查询之间的 $2^{nd}$-阶交互被编码。通过这种查询-键交互，空间和通道注意力与值一起被聚合。通道注意力与
    squeeze-excitation 注意力[[26](#bib.bib26)]相同。x-linear 模块的最终输出与键和值的低秩双线性进行聚合[[88](#bib.bib88)]。他们声称，编码更高阶交互只需要相应地重复
    x-linear 模块（例如，对于 $4^{th}$-阶交互使用三个迭代 x-linear 块）。无穷阶交互的建模也通过使用 h Exponential Linear
    Unit [[116](#bib.bib116)] 进行了解释。X-Linear 注意力模块提出了一种新颖的注意力机制，与 transformer [[2](#bib.bib2)]
    不同。它能够在没有位置编码的情况下，以仅线性复杂度编码输入标记之间的关系，而 transformer 中是二次复杂度。'
- en: 'Axial-Attention: Wang et al. [[89](#bib.bib89)] proposed axial attention to
    encode global information and long-range context for the subject. Although conventional
    self-attention methods use fully-connected layers to encode non-local interactions,
    they are very expensive given their dense connections [[2](#bib.bib2), [117](#bib.bib117),
    [118](#bib.bib118), [41](#bib.bib41)]. Axial uses self-attention models in a non-local
    way without any constraints. Simply put, it factorizes the 2D self-attentions
    in two axes (width and height) of 1D-self attentions. This way, axial attention
    shows effectiveness in attending over wide regions. Moreover, unlike [[119](#bib.bib119),
    [120](#bib.bib120), [121](#bib.bib121)], axial attention uses positional information
    to include contextual information in an agnostic way. With axial attention, the
    computational complexity is reduced to $\mathcal{O}(hwm)$. Also, axial attention
    showed competitive performance not only in comparison to full-attention models
    [[122](#bib.bib122), [43](#bib.bib43)], but convolutional ones as well [[24](#bib.bib24),
    [123](#bib.bib123)].'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Axial-Attention：Wang 等人 [[89](#bib.bib89)] 提出了轴向注意力，用于编码主题的全球信息和长程上下文。尽管传统的自注意力方法使用全连接层来编码非局部交互，但由于其密集的连接，它们非常昂贵
    [[2](#bib.bib2), [117](#bib.bib117), [118](#bib.bib118), [41](#bib.bib41)]。轴向注意力以非局部的方式使用自注意力模型，没有任何限制。简单来说，它将二维自注意力分解为一维自注意力的两个轴（宽度和高度）。这样，轴向注意力在关注广泛区域时表现出有效性。此外，与
    [[119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121)] 不同，轴向注意力使用位置信息以无偏的方式包含上下文信息。通过轴向注意力，计算复杂度降至
    $\mathcal{O}(hwm)$。此外，轴向注意力在与全注意力模型 [[122](#bib.bib122), [43](#bib.bib43)] 以及卷积模型
    [[24](#bib.bib24), [123](#bib.bib123)] 的比较中显示出具有竞争力的表现。
- en: 'Efficient Attention Mechanism: Conventional attention mechanisms are built
    on double matrix multiplication that yields quadratic complexity $n\times n$ where
    $n$ is the size of the matrix. Many methods propose efficient architectures for
    attention [[124](#bib.bib124), [85](#bib.bib85), [125](#bib.bib125), [126](#bib.bib126)].
    In [[85](#bib.bib85)], Zhuoran et al. used the associative feature of matrix multiplication
    and suggested efficient attention. Formally, instead of using dot-product of the
    form $\rho(QK^{T})V$, they process it in an efficient sequence $\rho_{q}(Q)(\rho_{k}(K)^{T}V)$
    where $\rho$ denotes a normalization step. Regarding the normalization of $\mathrm{softmax}$,
    it is performed twice instead of once at the end. Hence, the complexity is reduced
    from quadratic $\mathcal{O}(n^{2})$ to linear $\mathcal{O}(n)$. Through a simple
    change, the complexity of processing and memory usage are reduced to enable the
    integration of attention modules in large-scale tasks.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 高效注意力机制：传统的注意力机制建立在双矩阵乘法上，导致 $n \times n$ 的二次复杂度，其中 $n$ 是矩阵的大小。许多方法提出了高效的注意力架构
    [[124](#bib.bib124), [85](#bib.bib85), [125](#bib.bib125), [126](#bib.bib126)]。在
    [[85](#bib.bib85)] 中，Zhuoran 等人利用了矩阵乘法的结合特性，提出了高效的注意力。形式上，它们没有使用 $\rho(QK^{T})V$
    形式的点积，而是以高效的序列 $\rho_{q}(Q)(\rho_{k}(K)^{T}V)$ 进行处理，其中 $\rho$ 表示一个归一化步骤。关于 $\mathrm{softmax}$
    的归一化，它在结束时执行了两次而不是一次。因此，复杂度从二次 $\mathcal{O}(n^{2})$ 降至线性 $\mathcal{O}(n)$。通过简单的变化，处理复杂度和内存使用被减少，使得注意力模块能够融入大规模任务中。
- en: 2.1.4 Arithmetic Attention
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4 算术注意力
- en: This part introduces arithmetic attention methods such as dropout, mirror, reverse,
    inverse, and reciprocal. We named it arithmetic because these methods are different
    from the above techniques even though they use their core. However, these methods
    mainly produce the final attention scores from simple arithmetic equations such
    as the reciprocal of the attention, etc.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分介绍了算术注意力方法，例如 dropout、mirror、reverse、inverse 和 reciprocal。我们称之为算术注意力，因为这些方法与上述技术有所不同，尽管它们使用了它们的核心。然而，这些方法主要通过简单的算术方程（如注意力的倒数等）来生成最终的注意力分数。
- en: 'Attention-based Dropout Layer: In weakly-supervised object localization, detecting
    the whole object without location annotation is a challenging task [[127](#bib.bib127),
    [127](#bib.bib127), [128](#bib.bib128)]. Choe et al. [[129](#bib.bib129)] proposed
    using the dropout layer to improve the localization accuracy through two steps:
    making the whole object location even by hiding the most discriminative part and
    attending over the whole area to improve the recognition performance. As Figure [6](#S2.F6
    "Figure 6 ‣ 2.1.4 Arithmetic Attention ‣ 2.1 Soft (Deterministic) Attention ‣
    2 Attention in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth
    Survey") (a) shows, ADL has two branches: 1) drop mask to conceal the discriminative
    part, which is performed by a threshold hyperparameter where values bigger than
    this threshold are set to zero and vice versa, and 2) importance map to give weight
    for the channels contributions by using a sigmoid function. Although the proposed
    idea is simple, experiments showed it is efficient (gained 15 $\%$ more than the
    state-of-the-art).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '基于注意力的Dropout层：在弱监督对象定位中，没有位置注释的情况下检测整个对象是一个具有挑战性的任务[[127](#bib.bib127), [127](#bib.bib127),
    [128](#bib.bib128)]。Choe等人[[129](#bib.bib129)]提出通过两个步骤使用Dropout层来提高定位准确性：通过隐藏最具辨别性的部分使整个对象位置均匀，并在整个区域上进行注意力处理以提高识别性能。如图[6](#S2.F6
    "Figure 6 ‣ 2.1.4 Arithmetic Attention ‣ 2.1 Soft (Deterministic) Attention ‣
    2 Attention in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth
    Survey") (a)所示，ADL有两个分支：1）丢弃掩模以隐藏辨别部分，通过一个阈值超参数执行，该值大于此阈值的部分设置为零，反之亦然；2）重要性图为通道贡献赋予权重，使用sigmoid函数。尽管提出的想法很简单，但实验表明它是有效的（比最先进的技术提高了15%）。'
- en: 'Mirror Attention: In a line detection application [[51](#bib.bib51)], Lee et al.
    developed mirrored attention to learn more semantic features. They flipped the
    feature map around the candidate line, and then concatenated the feature maps
    together. In case the line is not aligned, zero padding is applied.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像注意力：在一项线检测应用[[51](#bib.bib51)]中，Lee等人开发了镜像注意力以学习更多的语义特征。他们围绕候选线翻转特征图，然后将特征图拼接在一起。如果线条未对齐，则应用零填充。
- en: 'Reverse Attention: Huang et al. [[130](#bib.bib130)] proposed the negative
    context (e.g. what is not related to the class) in training to learn semantic
    features. They were motivated by less discriminability between the classes in
    the high-level semantic representations and the weak response to the correct class
    from the latent representations. The network is composed of two branches, the
    first one learns discriminative features using convolutions for the target class,
    and the second one learns the reverse attention scores that are not associated
    with the target class. These scores are aggregated together to form the final
    attentions shown in Figure [6](#S2.F6 "Figure 6 ‣ 2.1.4 Arithmetic Attention ‣
    2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual Attention
    Methods in Deep Learning: An In-Depth Survey") (b). A deeper look inside the reverse
    attention shows that it is mainly dependent on negating the extracted features
    of convolutions followed by sigmoid $\mathrm{sigmoid}(-F_{conv})$. However, for
    the purpose of convergence, this simple equation is changed to be $\mathrm{sigmoid}(\frac{1}{ReLU(F_{conv})+0.125}-4)$.
    On semantic segmentation datasets, reverse attention achieved significant improvement
    over state-of-the-art. In a similar work, Chen et al. [[52](#bib.bib52)] proposed
    using reverse attention for salient object detection. The main intuition was to
    erase the final predictions of the network and hence learn the missing parts of
    the objects. However, the calculation method of attention scores is different
    from [[130](#bib.bib130)], whereas they used $1-\mathrm{sigmoid}(F_{i+1})$ where
    $F_{i+1}$ denoted the features of the next stage.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 反向注意力：Huang 等人 [[130](#bib.bib130)] 提出了在训练中使用负背景（例如与类别无关的内容）来学习语义特征。他们的动机是高层语义表示中类别间的辨别能力较差，以及来自潜在表示的正确类别反应较弱。网络由两个分支组成，第一个分支通过卷积学习目标类别的区分特征，第二个分支学习与目标类别无关的反向注意力分数。这些分数被聚合在一起形成最终的注意力，如图[6](#S2.F6
    "图6 ‣ 2.1.4 算术注意力 ‣ 2.1 软（确定性）注意力 ‣ 2 注意力在视觉中的应用 ‣ 深度学习中的视觉注意力方法：深入调查") (b)所示。更深入地看反向注意力，它主要依赖于否定卷积提取的特征，随后是sigmoid
    $\mathrm{sigmoid}(-F_{conv})$。然而，为了收敛，这个简单的方程被改为 $\mathrm{sigmoid}(\frac{1}{ReLU(F_{conv})+0.125}-4)$。在语义分割数据集上，反向注意力在最先进技术上取得了显著改进。在类似的工作中，Chen 等人 [[52](#bib.bib52)]
    提出了使用反向注意力进行显著物体检测。主要直觉是擦除网络的最终预测，从而学习物体的缺失部分。然而，注意力分数的计算方法与 [[130](#bib.bib130)]
    不同，他们使用 $1-\mathrm{sigmoid}(F_{i+1})$ 其中 $F_{i+1}$ 表示下一阶段的特征。
- en: '| ![Refer to caption](img/61cd2a48020743954bc9ac4c83ae9354.png) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/61cd2a48020743954bc9ac4c83ae9354.png) |'
- en: '| (a) Attention-Based Dropout [[129](#bib.bib129)] |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| (a) 基于注意力的Dropout [[129](#bib.bib129)] |'
- en: '| ![Refer to caption](img/7fc45ce40474eac0241086fc92607558.png) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/7fc45ce40474eac0241086fc92607558.png) |'
- en: '| (b) Reverse Attention [[130](#bib.bib130)] |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| (b) 反向注意力 [[130](#bib.bib130)] |'
- en: 'Figure 6: The arithmetic-based attention methods i.e. Attention-based Dropout [[129](#bib.bib129)]
    and Reverse Attention [[130](#bib.bib130)]. Images are taken from the original
    papers. These methods use arithmetic operations to generate the attention scores
    such as reverse, dropout or reciprocal.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：基于算术的注意力方法，如基于注意力的Dropout [[129](#bib.bib129)]和反向注意力[[130](#bib.bib130)]。图像来自原始论文。这些方法使用算术运算生成注意力分数，如反向、Dropout或倒数。
- en: 2.1.5 Multi-modal attentions
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.5 多模态注意力
- en: As the name reveals, multi-modal attention is proposed to handle multi-modal
    tasks, using different modalities to generate attentions, such as text and image.
    It should be noted that some attention methods below, such as Perceiver [[48](#bib.bib48)]
    and Criss-Cross [[131](#bib.bib131), [132](#bib.bib132)], are transformer types [[2](#bib.bib2)],
    but are customized for multi-modal tasks by including text, audio, and image.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如名称所示，多模态注意力旨在处理多模态任务，利用不同的模态生成注意力，例如文本和图像。需要注意的是，下面的一些注意力方法，如Perceiver [[48](#bib.bib48)]和Criss-Cross
    [[131](#bib.bib131), [132](#bib.bib132)]，是变换器类型[[2](#bib.bib2)]，但通过包括文本、音频和图像来定制以适应多模态任务。
- en: 'Cross Attention Network: In [[46](#bib.bib46)], a cross attention module (CAN)
    was proposed to enhance the overall discrimination of few-shot classification
    [[133](#bib.bib133)]. Inspired by the human behavior of recognizing novel images,
    the similarity between seen and unseen parts is identified first. CAN learns to
    encode the correlation between the query and the target object. As Figure [7](#S2.F7
    "Figure 7 ‣ 2.1.5 Multi-modal attentions ‣ 2.1 Soft (Deterministic) Attention
    ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth
    Survey") (a) shows, the features of query and target are extracted independently,
    and then a correlation layer computes the interaction between them using cosine
    distance. Next, 1D convolution is applied to fuse correlation (GAP is performed
    first) and attentions, followed by softmax normalization. The output is reshaped
    to give a single channel feature map to preserve spatial representations. Although,
    experiments show that CAN produces state-of-the-art results, it depends on non-learnable
    functions such as the cosine correlation. Also, the design is suitable for few-shot
    classification but is not general because it depends on two streams (query and
    target).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '交叉注意力网络：在[[46](#bib.bib46)]中，提出了一个交叉注意力模块（CAN）来增强少样本分类的整体区分能力[[133](#bib.bib133)]。受到人类识别新图像行为的启发，首先识别已见和未见部分之间的相似性。CAN学习编码查询与目标对象之间的相关性。如图[7](#S2.F7
    "Figure 7 ‣ 2.1.5 Multi-modal attentions ‣ 2.1 Soft (Deterministic) Attention
    ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth
    Survey") (a)所示，查询和目标的特征被独立提取，然后通过相关层使用余弦距离计算它们之间的交互。接下来，应用1D卷积来融合相关性（首先进行GAP）和注意力，之后进行softmax归一化。输出被重塑为单通道特征图，以保持空间表示。尽管实验表明CAN产生了最先进的结果，但它依赖于不可学习的函数，如余弦相关性。此外，该设计适用于少样本分类，但并不通用，因为它依赖于两个流（查询和目标）。'
- en: 'Criss-Cross Attention: The contextual information is still very important for
    scene understanding [[131](#bib.bib131), [132](#bib.bib132)]. Criss-cross attention
    proposed encoding the context of each pixel in the image in the criss-cross path.
    By building recurrent modules of criss-cross attention, the whole context is encoded
    for each pixel. This module is more efficient than non-local block [[3](#bib.bib3)]
    in memory and time, where the memory is reduced by $11x$ and GFLOPS reduced by
    $85\%$. Since this survey focuses on the core attention ideas, we show the criss-cross
    module in Figure [7](#S2.F7 "Figure 7 ‣ 2.1.5 Multi-modal attentions ‣ 2.1 Soft
    (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in
    Deep Learning: An In-Depth Survey") (b). Initially, three $1\times 1$ convolutions
    are applied to the feature maps, whereas two of them are multiplied together (first
    map with each row of the second) to produce criss-cross attentions for each pixel.
    Then, softmax is applied to generate the attention scores, aggregated with the
    third convolution outcome. However, the encoded context captures only information
    in the criss-cross direction, and not the whole image. For this reason, the authors
    repeated the attention module by sharing the weights to form recurrent criss-cross,
    which includes the whole context. [[134](#bib.bib134)]'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '十字交叉注意力：上下文信息在场景理解中仍然非常重要[[131](#bib.bib131), [132](#bib.bib132)]。十字交叉注意力提出了在十字交叉路径中编码图像中每个像素的上下文。通过构建十字交叉注意力的递归模块，为每个像素编码整个上下文。这个模块在内存和时间上比非局部块[[3](#bib.bib3)]更高效，其中内存减少了$11x$，GFLOPS减少了$85\%$。由于本调查关注核心注意力思想，我们在图[7](#S2.F7
    "Figure 7 ‣ 2.1.5 Multi-modal attentions ‣ 2.1 Soft (Deterministic) Attention
    ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth
    Survey") (b)中展示了十字交叉模块。最初，对特征图应用了三个$1\times 1$卷积，其中两个卷积相互乘积（第一个图与第二个图的每一行），以生成每个像素的十字交叉注意力。然后，应用softmax生成注意力分数，并与第三个卷积的结果进行聚合。然而，编码的上下文仅捕捉了十字交叉方向的信息，而不是整个图像。为此，作者通过共享权重重复了注意力模块，形成递归的十字交叉，从而包括整个上下文。[[134](#bib.bib134)]'
- en: 'Perceiver Traditional: CNNs have achieved high performance in handling several
    tasks [[24](#bib.bib24), [135](#bib.bib135), [136](#bib.bib136)], however, they
    are designed and trained for a single domain rather than multi-modal tasks [[137](#bib.bib137),
    [138](#bib.bib138), [139](#bib.bib139)]. Inspired by biological systems that understand
    the environment through various modalities simultaneously, Jaegle et al. proposed
    perceiver that leverages the relations between these modalities iteratively. The
    main concept behind perceiver is to form an attention bottleneck composed of a
    set of latent units. This solves the scale of quadratic processing, as in traditional
    transformer, and encourages the model to focus on important features through iterative
    processing. To compensate for the spatial context, Fourier transform is used to
    encode the features [[140](#bib.bib140), [141](#bib.bib141), [142](#bib.bib142),
    [143](#bib.bib143)]. As Figure [7](#S2.F7 "Figure 7 ‣ 2.1.5 Multi-modal attentions
    ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual Attention
    Methods in Deep Learning: An In-Depth Survey") (c) shows, the perceiver is similar
    to RNN because of weights sharing. It composes two main components: cross attention
    to map the input image or input vector to a latent vector and transformer tower
    that maps the latent vector to a similar one with the same size. The architecture
    reveals that perceiver is an attention bottleneck that learns a mapping function
    from high-dimensional data to a low-dimensional one and then passes it to the
    transformer [[2](#bib.bib2)]. The cross-attention module has multi-byte attend
    layers to enrich the context, which might be limited from such mapping. This design
    reduces the quadratic processing $\mathcal{O}(M^{2})$ to $\mathcal{O}(MN)$, where
    $M$ is the sequence length and $N$ is a hyperparameter that can be chosen smaller
    than $M$. Additionally, sharing the weights of the iterative attention reduces
    the parameters to one-tenth and enhances the model’s generalization.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 'Perceiver 传统方法：CNN 在处理多个任务上已取得了很高的性能 [[24](#bib.bib24), [135](#bib.bib135),
    [136](#bib.bib136)]，但它们的设计和训练是针对单一领域，而非多模态任务 [[137](#bib.bib137), [138](#bib.bib138),
    [139](#bib.bib139)]。受生物系统通过多种模态同时理解环境的启发，Jaegle 等人提出了 Perceiver，它利用这些模态之间的关系进行迭代处理。Perceiver
    的主要概念是形成一个由一组潜在单元组成的注意力瓶颈。这解决了传统 Transformer 中的二次处理规模问题，并通过迭代处理促使模型专注于重要特征。为了弥补空间上下文，使用傅里叶变换对特征进行编码
    [[140](#bib.bib140), [141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143)]。如图
    [7](#S2.F7 "Figure 7 ‣ 2.1.5 Multi-modal attentions ‣ 2.1 Soft (Deterministic)
    Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning:
    An In-Depth Survey") (c) 所示，由于权重共享，Perceiver 类似于 RNN。它由两个主要组件组成：交叉注意力用于将输入图像或输入向量映射到潜在向量，Transformer
    塔将潜在向量映射到具有相同大小的类似向量。该架构表明 Perceiver 是一个注意力瓶颈，它学习从高维数据到低维数据的映射函数，然后将其传递给 Transformer
    [[2](#bib.bib2)]。交叉注意力模块具有多字节注意层，以丰富上下文，这可能会受到映射的限制。这种设计将二次处理的复杂度 $\mathcal{O}(M^{2})$
    降低到 $\mathcal{O}(MN)$，其中 $M$ 是序列长度，$N$ 是可以选择小于 $M$ 的超参数。此外，迭代注意力的权重共享将参数减少到十分之一，并增强了模型的泛化能力。'
- en: 'Stacked Cross Attention: Lee et al. [[47](#bib.bib47)] proposed a method to
    attend between an image and a sentence context. Given an image and sentence, it
    learns the attention of words in a sentence for each region in the image and then
    scores the image regions by comparing each region to the sentence. This way of
    processing enables stacked cross attention to discover all possible alignments
    between text and image. Firstly, they compute image-text cross attention by a
    few steps as follows: a) compute cosine similarity for all image-text pairs [[144](#bib.bib144)]
    followed by $\ell_{2}$ normalization [[145](#bib.bib145)], b) compute the weighted
    sum of these pairs attentions, where the image one is calculated by softmax [[146](#bib.bib146)],
    c) the final similarity between these pairs is computed using LogSumExp pooling
    [[147](#bib.bib147), [148](#bib.bib148)]. The same steps are repeated to get the
    text-image cross attention, but the attention in the second step uses text-based
    softmax. Although stacked attention enriches the semantics of multi-modal tasks
    by attending text over image and vice versa, shared semantics might lead to misalignment
    in case of lack of similarity. With slight changes to the main concept, several
    works in various paradigms such as question answering and image captioning [[149](#bib.bib149),
    [150](#bib.bib150), [151](#bib.bib151), [152](#bib.bib152), [153](#bib.bib153)]
    used the stacked-cross attention.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠交叉注意力：Lee等人[[47](#bib.bib47)]提出了一种在图像和句子上下文之间进行注意力的方法。给定图像和句子，它为图像中的每个区域学习句子中单词的注意力，然后通过将每个区域与句子进行比较来对图像区域进行评分。这种处理方式使得堆叠交叉注意力能够发现文本和图像之间的所有可能对齐方式。首先，他们通过以下几个步骤计算图像-文本交叉注意力：a)
    计算所有图像-文本对的余弦相似度[[144](#bib.bib144)]，然后进行$\ell_{2}$归一化[[145](#bib.bib145)]，b)
    计算这些对的注意力的加权和，其中图像部分通过softmax计算[[146](#bib.bib146)]，c) 使用LogSumExp池化[[147](#bib.bib147),
    [148](#bib.bib148)]计算这些对之间的最终相似度。重复相同的步骤以获得文本-图像交叉注意力，但第二步中的注意力使用基于文本的softmax。尽管堆叠注意力通过在图像上关注文本及反之亦然来丰富多模态任务的语义，但共享语义可能会在缺乏相似性的情况下导致对齐错误。对主要概念进行稍微的修改后，许多在问答和图像描述等不同范式中的工作[[149](#bib.bib149),
    [150](#bib.bib150), [151](#bib.bib151), [152](#bib.bib152), [153](#bib.bib153)]使用了堆叠交叉注意力。
- en: 'Boosted Attention: While top-down attention mechanisms [[154](#bib.bib154)]
    fail to focus on regions of interest without prior knowledge, visual stimuli methods [[155](#bib.bib155),
    [156](#bib.bib156)] alone are not sufficient to generate captions for images.
    For this reason, in [7](#S2.F7 "Figure 7 ‣ 2.1.5 Multi-modal attentions ‣ 2.1
    Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods
    in Deep Learning: An In-Depth Survey") (d), the authors proposed a boosted attention
    model to combine both of them in one approach to focus on top-down signals from
    the language and attend to the salient regions from stimuli independently. Firstly,
    they integrate stimuli attention with visual feature $I^{{}^{\prime}}=W\ I\circ\mathrm{log}(W_{sal}I+\epsilon))$,
    where $I$ is the extracted features from the backbone, $W_{sal}$ denotes the weight
    of the layer that produces stimuli attention, $W$ is the weight of the layer that
    output the visual feature layer. Boosted attention is achieved using the Hadamard
    product on $I^{{}^{\prime}}$. Their experiments work showed that boosted attention
    improved performance significantly.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '增强注意力：尽管自上而下的注意力机制[[154](#bib.bib154)]在没有先验知识的情况下无法聚焦于感兴趣的区域，视觉刺激方法[[155](#bib.bib155),
    [156](#bib.bib156)]本身也不足以生成图像的描述。因此，在[7](#S2.F7 "Figure 7 ‣ 2.1.5 Multi-modal
    attentions ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual
    Attention Methods in Deep Learning: An In-Depth Survey") (d)中，作者提出了一种增强注意力模型，将两者结合在一种方法中，以关注来自语言的自上而下信号，并独立关注来自刺激的显著区域。首先，他们将刺激注意力与视觉特征集成$
    I^{{}^{\prime}}=W\ I\circ\mathrm{log}(W_{sal}I+\epsilon))$，其中$I$是从骨干网络提取的特征，$W_{sal}$表示产生刺激注意力的层的权重，$W$是输出视觉特征层的层的权重。通过对$
    I^{{}^{\prime}}$使用Hadamard乘积来实现增强注意力。他们的实验结果表明，增强注意力显著提高了性能。'
- en: '| ![Refer to caption](img/2a29f3296bf5c07db612f7dd864a8f34.png) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| ![参考标题](img/2a29f3296bf5c07db612f7dd864a8f34.png) |'
- en: '| (a) Cross Attention [[46](#bib.bib46)] |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| (a) 交叉注意力[[46](#bib.bib46)] |'
- en: '| ![Refer to caption](img/c13b7fbaa9e6c55f8771b6aba17a6120.png) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| ![参考标题](img/c13b7fbaa9e6c55f8771b6aba17a6120.png) |'
- en: '| (b) Criss-Cross Attention [[134](#bib.bib134)] |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| (b) Criss-Cross 注意力[[134](#bib.bib134)] |'
- en: '| ![Refer to caption](img/edac4218658b65e13c29b70c9fa1290d.png) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| ![参考标题](img/edac4218658b65e13c29b70c9fa1290d.png) |'
- en: '| (c) Perceiver [[48](#bib.bib48)] |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| (c) Perceiver[[48](#bib.bib48)] |'
- en: '| ![Refer to caption](img/0dfd6c5b3d3b40c1f4edd74f71c4eb08.png) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| ![参考标题](img/0dfd6c5b3d3b40c1f4edd74f71c4eb08.png) |'
- en: '| (d) Boosted Attention [[49](#bib.bib49)] |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| (d) 增强注意力 [[49](#bib.bib49)] |'
- en: 'Figure 7: Multi-modal attention methods consisting of Attention-based Perceiver [[48](#bib.bib48)],
    Criss-Cross [[46](#bib.bib46)], Boosted attention [[49](#bib.bib49)], Cross-attention
    module [[46](#bib.bib46)]. The mentioned methods employ multi-modalities to generate
    the attention scores. Images are taken from the original papers.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：多模态注意力方法，包括基于注意力的感知器[[48](#bib.bib48)]、交叉注意力[[46](#bib.bib46)]、增强注意力[[49](#bib.bib49)]、交叉注意力模块[[46](#bib.bib46)]。上述方法使用多模态生成注意力分数。图像取自原始论文。
- en: 2.1.6 Logical Attention
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.6 逻辑注意力
- en: Similar to how human beings pay more attention to the crucial features, some
    methods have been proposed to use recurrences to encode better relationships.
    These methods rely on using RNNs or any type of sequential network to calculate
    the attentions. We named it logical methods because they use architectures similar
    to logic gates.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于人类更加关注关键特征，一些方法已被提出以使用递归更好地编码关系。这些方法依赖于使用RNN或任何类型的顺序网络来计算注意力。我们将其称为逻辑方法，因为它们使用类似于逻辑门的架构。
- en: 'Sequential Attention Models: Inspired by the primate visual system, Zoran et al. [[56](#bib.bib56)]
    proposed soft, sequential, spatial top-down attention method (S3TA) to focus more
    on attended regions of an image [[157](#bib.bib157)] (as shown in Figure [8](#S2.F8
    "Figure 8 ‣ 2.1.6 Logical Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey") (b)).
    At each step of the sequential process, the model queries the input and refines
    the total score based on spatial information in a top-down manner. Specifically,
    the backbone extracts features [[24](#bib.bib24), [123](#bib.bib123), [69](#bib.bib69)]
    channels that are split into keys and values. A Fourier transform encodes the
    spatial information for these two sets to preserve the spatial information from
    disappearing for later use. The main module is a top-down controller, which is
    a version of Long-Short Term Model (LSTM) [[96](#bib.bib96)], where its previous
    state is decoded as query vectors. The size of each query vector equals the sum
    of channels in keys and spatial basis. At each spatial location, the similarity
    between these vectors is calculated through the inner product, and then the softmax
    concludes the attention scores. These attention scores are multiplied by the values,
    and then the summation is taken to produce the corresponding answer vector for
    each query. All these steps are in the current step of LSTM and are then passed
    to the next step. Note that the input of the attention module is an output of
    the LSTM state to focus more on the relevant information as well as the attention
    map comprises only one channel to preserve the spatial information. Empirical
    evaluations show that attention is crucial for adversarial robustness because
    adversarial perturbations drag the object’s attention away to degrade the model
    performance. Such an attention model proved its ability to resist strong attacks
    [[158](#bib.bib158)] and natural noises [[159](#bib.bib159)]. Although S3TA provides
    a novel method to empower the attention modules using recurrent networks, it is
    inefficient.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '顺序注意力模型：受到灵长类动物视觉系统的启发，Zoran等人[[56](#bib.bib56)] 提出了软的、顺序的、空间的自上而下注意力方法（S3TA），以更多地关注图像的关注区域[[157](#bib.bib157)]（如图[8](#S2.F8
    "Figure 8 ‣ 2.1.6 Logical Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey") (b)所示）。在顺序处理的每一步，模型查询输入并基于空间信息以自上而下的方式细化总分。具体来说，骨干网络提取特征[[24](#bib.bib24),
    [123](#bib.bib123), [69](#bib.bib69)]通道，这些通道被拆分成键和值。傅里叶变换对这两组空间信息进行编码，以防空间信息在后续使用中消失。主要模块是一个自上而下的控制器，它是长短期记忆模型（LSTM）[[96](#bib.bib96)]的一个版本，其中其先前的状态被解码为查询向量。每个查询向量的大小等于键和空间基的通道之和。在每个空间位置，这些向量之间的相似性通过内积计算，然后softmax得出注意力分数。这些注意力分数与值相乘，然后求和，以生成每个查询的相应答案向量。所有这些步骤都在LSTM的当前步骤中完成，然后传递到下一步。请注意，注意力模块的输入是LSTM状态的输出，以更多关注相关信息，并且注意力图仅包含一个通道以保持空间信息。实证评估表明，注意力对于对抗鲁棒性至关重要，因为对抗扰动会使对象的注意力偏离，从而降低模型性能。这种注意力模型证明了其抵御强攻击[[158](#bib.bib158)]和自然噪声[[159](#bib.bib159)]的能力。尽管S3TA提供了一种通过递归网络增强注意力模块的新方法，但其效率较低。'
- en: 'Permutation invariant Attention: Initially, Zaheer et al. [[160](#bib.bib160)]
    suggested handling deep networks in the form of sets rather than ordered lists
    of elements. For instance, performing pooling over sets of extracted features
    e.g. $\rho(pool({\phi(x_{1}),\phi(x_{2}),\cdots,\phi(x_{n})}))$, where $\rho$
    and $\phi$ are continuous functions and pool can be the $sum$ function. Formally,
    any set of deep learning features is invariant permutation if $f(\pi x)=\pi f(x)$.
    Hence, Lee et al. [[58](#bib.bib58)] proposed an attention-based method that processes
    sets of data. In [[160](#bib.bib160)], simple functions ($sum$ or $mean$) are
    proposed to combine the different branches of the network, but they lose important
    information due to squashing the data. To address these issues, set transformer [[58](#bib.bib58)]
    parameterizes the pooling functions and provides richer representations that can
    encode higher-order interaction. They introduced three main contributions: a)
    Set Attention Block (SAB), which is similar to Multi-head Attention Block (MAB)
    layer [[2](#bib.bib2)], but without positional encoding and dropout; b) induced
    Set Attention Blocks (ISAB), which reduced complexity from $\mathcal{O}(n^{2})$
    to $\mathcal{O}(mn)$, where $m$ is the size of induced point vectors and c) pooling
    by Multihead Attention (PMA) uses MAB over the learnable set of seed vectors.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 排列不变注意力：最初，Zaheer等人[[160](#bib.bib160)]建议以集合的形式处理深度网络，而不是元素的有序列表。例如，对提取的特征集合进行池化，如$\rho(pool({\phi(x_{1}),\phi(x_{2}),\cdots,\phi(x_{n})}))$，其中$\rho$和$\phi$是连续函数，pool可以是$sum$函数。正式地，任何深度学习特征的集合都是排列不变的，如果$f(\pi
    x)=\pi f(x)$。因此，Lee等人[[58](#bib.bib58)]提出了一种基于注意力的方法，用于处理数据集合。在[[160](#bib.bib160)]中，提出了简单函数（$sum$或$mean$）来组合网络的不同分支，但由于压缩数据，丢失了重要信息。为了解决这些问题，集合变换器[[58](#bib.bib58)]对池化函数进行参数化，并提供了可以编码高阶交互的更丰富表示。他们提出了三项主要贡献：a)
    集合注意力块（SAB），类似于多头注意力块（MAB）层[[2](#bib.bib2)]，但没有位置编码和dropout；b) 诱导集合注意力块（ISAB），将复杂度从$\mathcal{O}(n^{2})$降低到$\mathcal{O}(mn)$，其中$m$是诱导点向量的大小；c)
    通过多头注意力（PMA）进行池化，对可学习的种子向量集合进行MAB操作。
- en: 'Show, Attend and Tell: Xu et al. [[62](#bib.bib62)] introduced two types of
    attentions to attend to specific image regions for generating a sequence of captions
    aligned with the image using LSTM [[161](#bib.bib161)]. They used two types of
    attention: hard attention and soft attention. Hard attention is applied to the
    latent variable after assigning multinoulli distribution to learn the likelihood
    of $logp(y|a)$, where $a$ is the latent variable. By using multinoulli distribution
    and reducing the variance of the estimator, they trained their model by maximizing
    a variational lower bound as pointed out in [[162](#bib.bib162), [163](#bib.bib163)]
    provided that attentions sum to $1$ at every point i.e. $\sum_{i}\alpha_{ti}=1$,
    where $\alpha$ refers to attentions scores. For soft attention, they used softmax
    to generate the attention scores, but for $p(s_{t}|a)$ as in [[50](#bib.bib50)],
    where $s_{t}$ is the extracted feature at this step. The training of soft attention
    is easily done by normal backpropagation and to minimize the negative log-likelihood
    of $-\log(p(y|a))+\sum_{i}(1-\sum_{t}\alpha_{ti})^{2}$. This model achieved the
    benchmark for visual captioning at the time as it paved the way for visual attention
    to progress.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 显示、注意与描述：Xu等人[[62](#bib.bib62)]引入了两种类型的注意力机制，用于关注特定图像区域，以生成与图像对齐的描述序列，使用了LSTM[[161](#bib.bib161)]。他们使用了两种类型的注意力：硬注意力和软注意力。硬注意力应用于在分配多项分布后学习$logp(y|a)$的潜变量，其中$a$是潜变量。通过使用多项分布并减少估计量的方差，他们通过最大化变分下界来训练模型，如[[162](#bib.bib162)、[163](#bib.bib163)]所指出，前提是注意力在每一点上的总和为$1$，即$\sum_{i}\alpha_{ti}=1$，其中$\alpha$指代注意力分数。对于软注意力，他们使用softmax生成注意力分数，但对于$p(s_{t}|a)$，如[[50](#bib.bib50)]中所述，其中$s_{t}$是该步骤提取的特征。软注意力的训练可以通过普通的反向传播完成，并最小化$-\log(p(y|a))+\sum_{i}(1-\sum_{t}\alpha_{ti})^{2}$的负对数似然。这一模型在当时达到了视觉描述的基准，并为视觉注意力的发展铺平了道路。
- en: 'Kalman Filtering Attention: Liu et al. identified two main limitations that
    hinder using attention in various fields where there is insufficient learning
    or history [[164](#bib.bib164)]. These issues are 1) object’s attention for input
    queries is covered by past training; and 2) conventional attentions do not encode
    hierarchical relationships between similar queries. To address these issues, they
    proposed the use of Kalman filter attention. Moreover, KFAtt-freq to capture the
    homogeneity of the same queries, correcting the bias towards frequent queries.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Kalman 过滤注意力：Liu 等人识别了两个主要限制，阻碍了注意力在各个领域中的使用，其中包括学习或历史不足 [[164](#bib.bib164)]。这些问题是
    1) 对输入查询的对象注意力被过去的训练所覆盖；2) 传统注意力未能编码类似查询之间的层次关系。为了解决这些问题，他们提出了使用 Kalman 过滤器注意力。此外，KFAtt-freq
    用于捕捉相同查询的同质性，纠正对频繁查询的偏差。
- en: 'Prophet Attention: In prophet attention [[165](#bib.bib165)], the authors noticed
    that the conventional attention models are biased and cause deviated focus to
    the tasks, especially in sequence models such as image captioning [[166](#bib.bib166),
    [167](#bib.bib167)] and visual grounding [[168](#bib.bib168), [169](#bib.bib169)].
    Further, this deviation happens because attention models utilize previous input
    in a sequence to attend to the image rather than the outputs. As shown in Figure [8](#S2.F8
    "Figure 8 ‣ 2.1.6 Logical Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey") (a),
    the model is attending on “yellow and umbrella” instead of “umbrella and wearing”.
    In a like-self-supervision way, they calculate the attention vectors based on
    the words generated in the future. Then, they guide the training process using
    these correct attentions, which can be considered a regularization of the whole
    model. Simply put, this method is based on summing the attentions of the post
    sequences in the same sentence to eliminate the impact of deviated focus towards
    the inputs. Overall, prophet attention addresses sequence-models biases towards
    history while disregarding the future.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 'Prophet 注意力：在 Prophet 注意力 [[165](#bib.bib165)] 中，作者注意到传统的注意力模型存在偏差，导致任务焦点偏离，尤其是在序列模型如图像描述
    [[166](#bib.bib166), [167](#bib.bib167)] 和视觉定位 [[168](#bib.bib168), [169](#bib.bib169)]
    中。此外，这种偏差发生是因为注意力模型利用序列中的前一个输入来关注图像，而非输出。如图 [8](#S2.F8 "Figure 8 ‣ 2.1.6 Logical
    Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention in Vision ‣ Visual
    Attention Methods in Deep Learning: An In-Depth Survey") (a) 所示，模型关注的是“黄色和伞”而不是“伞和穿戴”。类似自我监督的方式，他们基于未来生成的词计算注意力向量。然后，他们利用这些正确的注意力来指导训练过程，这可以视为对整个模型的正则化。简单来说，这种方法基于对同一句子中后续序列的注意力求和，以消除对输入的偏差。总体而言，Prophet
    注意力解决了序列模型对历史的偏见，同时忽视了未来。'
- en: '| ![Refer to caption](img/a14f5f8463b8b2472981684d6a30bb94.png) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/a14f5f8463b8b2472981684d6a30bb94.png) |'
- en: '| (a) Prophet   [[165](#bib.bib165)] |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| (a) Prophet   [[165](#bib.bib165)] |'
- en: '| ![Refer to caption](img/095a8f7ee76e7e6fbf83bcfe744ef58f.png) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/095a8f7ee76e7e6fbf83bcfe744ef58f.png) |'
- en: '| (b) S3TA  [[56](#bib.bib56)] |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| (b) S3TA  [[56](#bib.bib56)] |'
- en: 'Figure 8: The core structure of logic-based attention methods such as Prophet
    attention [[165](#bib.bib165)] and S3TA [[56](#bib.bib56)] which are a type of
    attention that use logical networks such as RNN to infer the attention scores.
    Images are taken from the original papers and are best viewed in color.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：基于逻辑的注意力方法的核心结构，如 Prophet 注意力 [[165](#bib.bib165)] 和 S3TA [[56](#bib.bib56)]，这些方法使用逻辑网络，如
    RNN，来推断注意力分数。图像来自原始论文，最佳效果请使用彩色查看。
- en: '![Refer to caption](img/dc46ee4156f9a7fc0442c45e361d0234.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dc46ee4156f9a7fc0442c45e361d0234.png)'
- en: 'Figure 9: An example of Guided Attention Inference Networks [[44](#bib.bib44)].'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：引导注意力推断网络的示例 [[44](#bib.bib44)]。
- en: 2.1.7 Category-Based Attentions
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.7 基于类别的注意力
- en: The above methods generate the attention scores from the features regardless
    of the presence of the class. On the other hand, some methods use class annotation
    to force the network to attend over specific regions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法根据特征生成注意力分数，无论类别是否存在。另一方面，一些方法使用类别注释来强制网络关注特定区域。
- en: 'Guided Attention Inference Network: In [[44](#bib.bib44)], the authors proposed
    class-aware attention, namely Guided Attention Inference Networks (GAIN), guided
    by the labels. Instead of focusing only on the most discriminative parts in the
    image [[170](#bib.bib170)], GAIN includes the contextual information in the feature
    maps. Following [[171](#bib.bib171)], GAIN obtains the attention maps from an
    inference branch, which are then used for training. As shown in Figure [9](#S2.F9
    "Figure 9 ‣ 2.1.6 Logical Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey"), through
    2D-convolutions, global average pooling, and ReLU, the important features are
    extracted $A^{c}$ for each class. Following this, the features of each class are
    obtained as $I-(T(A^{c})\bigodot I)$ where $\bigodot$ is matrix multiplication.
    $T(A^{c})=\frac{1}{1+exp(-w(A^{c}-\sigma))}$ where $\sigma$ is a threshold parameter
    and $w$ is a scaling parameter. Their experiments showed that without recursive
    runs, GAINS gained significant improvement over the state-of-the-art.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '引导注意力推断网络：在[[44](#bib.bib44)]中，作者提出了类别感知注意力，即由标签引导的引导注意力推断网络（GAIN）。与其仅专注于图像中的最具区分性的部分[[170](#bib.bib170)]不同，GAIN在特征图中包含了上下文信息。根据[[171](#bib.bib171)]，GAIN从推断分支中获取注意力图，这些图随后用于训练。如图[9](#S2.F9
    "Figure 9 ‣ 2.1.6 Logical Attention ‣ 2.1 Soft (Deterministic) Attention ‣ 2 Attention
    in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth Survey")所示，通过二维卷积、全局平均池化和ReLU，提取了每个类别的重要特征$A^{c}$。接着，得到每个类别的特征为$I-(T(A^{c})\bigodot
    I)$，其中$\bigodot$是矩阵乘法。$T(A^{c})=\frac{1}{1+exp(-w(A^{c}-\sigma))}$，其中$\sigma$是阈值参数，$w$是缩放参数。他们的实验表明，在没有递归运行的情况下，GAIN相比于最先进的技术有显著提升。'
- en: 'Curriculum Enhanced Supervised Attention Network: The majority of attention
    methods are trained in a weakly supervised manner, and hence, the attention scores
    are still far from the best representations [[2](#bib.bib2), [3](#bib.bib3)].
    In [[45](#bib.bib45)], the authors introduced a novel idea to generate a Supervised-Attention
    Network (SAN). Using the convolution layers, they defined the output of the last
    layer to be equal to the number of classes. Therefore, performing attention using
    global average pooling [[78](#bib.bib78)] yields a weight for each category. In
    a similar study, Fukui et al. proposed using a network composed of three branches
    to obtain class-specific attention scores: feature extractor to learn the discriminative
    features, attention branch to compute the attention scores based on a response
    model, perception to output the attention scores of each class by using the first
    two modules. The main objective was to increase the visual explanations [[170](#bib.bib170)]
    of the CNN networks as it showed significant improvements in various fields such
    as fine-grained recognition and image classification.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 课程增强监督注意力网络：大多数注意力方法以弱监督方式训练，因此，注意力分数仍然远未达到最佳表示[[2](#bib.bib2), [3](#bib.bib3)]。在[[45](#bib.bib45)]中，作者提出了一种新颖的方法来生成监督注意力网络（SAN）。通过卷积层，他们将最后一层的输出定义为等于类别数。因此，使用全局平均池化[[78](#bib.bib78)]进行注意力操作会为每个类别生成一个权重。在类似的研究中，Fukui等人提出使用由三个分支组成的网络来获取类别特定的注意力分数：特征提取器用于学习区分特征，注意力分支用于基于响应模型计算注意力分数，感知模块通过前两个模块输出每个类别的注意力分数。主要目标是增加CNN网络的视觉解释[[170](#bib.bib170)]，因为它在细粒度识别和图像分类等多个领域显示了显著的改进。
- en: 'Attentional Class Feature Network: Zhang et al. [[172](#bib.bib172)] introduced
    ACFNet, a novel idea to exploit the contextual information for improving semantic
    segmentation. Unlike conventional methods that learn spatial-based global information
    [[173](#bib.bib173)], this contextual information is categorial-based, firstly
    presenting the class-center concept and then employing it to aggregate all the
    corresponding pixels to form a specific class representation. In the training
    phase, ground-truth labels are used to learn class centers, while coarse segmentation
    results are used in the test phase. Finally, class-attention maps are the results
    of class centers and coarse segmentation outcomes. The results show significant
    improvement for semantic segmentation using ACFNet.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力类特征网络：张等人[[172](#bib.bib172)]提出了 ACFNet，这是一种利用上下文信息来改进语义分割的新颖方法。与传统的基于空间的全局信息学习方法[[173](#bib.bib173)]不同，这种上下文信息是基于类别的，首先提出了类中心的概念，然后利用它来汇聚所有对应的像素，以形成特定的类别表示。在训练阶段，使用真实标签来学习类别中心，而在测试阶段使用粗略的分割结果。最终，类注意力图是类别中心和粗略分割结果的综合。结果表明，使用
    ACFNet 对语义分割有显著的改进。
- en: 2.2 Hard (Stochastic) Attention
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 硬性（随机）注意力
- en: 'Instead of using the weighted average of the hidden states, hard attention
    selects one of the states as the attention score. Proposing hard attention depends
    on answering two questions: (1) how to model the problem, and (2) how to train
    it without vanishing the gradients. In this part, hard attention methods are discussed
    as well as their training mechanisms. It includes a discussion of Bayesian attention,
    variational inference, reinforced, and Gaussian attentions. The main idea of Bayesian
    attention and variational one is to use latent random variables as attention scores.
    Reinforced attention replaces softmax with a Bernoulli-sigmoid unit [[174](#bib.bib174)],
    whereas Gaussian attention uses a 2D Gaussian kernel instead. Similarly, self-critic
    attention [[65](#bib.bib65)] employs a re-enforcement technique to generate the
    attention scores, whereas Expectation-Maximization uses EM to generate the scores.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用隐藏状态的加权平均不同，硬性注意力选择其中一个状态作为注意力分数。提出硬性注意力依赖于回答两个问题：（1）如何建模问题，以及（2）如何在不消失梯度的情况下进行训练。在这一部分中，讨论了硬性注意力方法及其训练机制，包括贝叶斯注意力、变分推断、强化注意力和高斯注意力的讨论。贝叶斯注意力和变分注意力的主要思想是使用潜在随机变量作为注意力分数。强化注意力用伯努利-
    sigmoid 单元[[174](#bib.bib174)]替代 softmax，而高斯注意力则使用 2D 高斯核。同样，自我批评注意力[[65](#bib.bib65)]使用再强化技术生成注意力分数，而期望最大化使用
    EM 来生成分数。
- en: 2.2.1 Statistical-based attention
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 基于统计的注意力
- en: Bayesian Attention Modules (BAM) In contrast to the deterministic attention
    modules, Fan et al. [[59](#bib.bib59)] proposed a stochastic attention method
    based on Bayesian-graph models. Firstly, keys and queries are aligned to form
    distributions parameters for attention weights, treated as latent random variables.
    They trained the whole model by reparameterization, which results from weight
    normalization by Lognormal or Weibull distributions. Kullback–Leibler (KL) divergence
    is used as a regularizer to introduce contextual prior distribution in the form
    of keys’ functions. Their experiments illustrate that BAM significantly outperforms
    the state-of-the-art in various fields such as visual question answering, image
    captioning, and machine translation. However, this improvement happens on account
    of computational cost as well as memory usages. Compared to deterministic attention
    models, it is an efficient alternative in general, showing consistent effectiveness
    in language-vision tasks.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯注意力模块（BAM）：与确定性注意力模块不同，范等人[[59](#bib.bib59)]提出了一种基于贝叶斯图模型的随机注意力方法。首先，将键和值对齐以形成注意力权重的分布参数，处理为潜在随机变量。他们通过重参数化训练整个模型，结果来自于通过对数正态分布或威布尔分布进行权重归一化。使用
    Kullback–Leibler (KL) 散度作为正则化器，引入以键的函数形式存在的上下文先验分布。他们的实验表明，BAM 在视觉问答、图像字幕生成和机器翻译等多个领域显著超越了现有的最先进技术。然而，这种改进是以计算成本和内存使用为代价的。与确定性注意力模型相比，它通常是一个高效的替代方案，在语言-视觉任务中表现出一致的有效性。
- en: •
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Bayesian Attention Belief Networks: Zhang et al. [[175](#bib.bib175)] proposed
    using Bayesian Belief modules to generate attention scores given their ability
    to model high structured data along with uncertainty estimations. As shown in
    [10](#S2.F10 "Figure 10 ‣ 2.2.1 Statistical-based attention ‣ 2.2 Hard (Stochastic)
    Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning:
    An In-Depth Survey") (b), they introduced a simple structure to change any deterministic
    attention model into a stochastic one through four steps: 1) using Gamma distributions
    to build the decoder network 2) using Weibull distributions along with stochastic
    and deterministic paths for downward and upward, respectively 3) Parameterizing
    BABN distributions from the queries and keys of the current network 4) using evidence
    lower bound to optimize the encoder and decoder. The whole network is differentiable
    because of the existence of Weibull distributions in the encoder. In terms of
    accuracy and uncertainty, BABN proved improvement over the state-of-the-art in
    NLP tasks.'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贝叶斯注意力信念网络：张等人 [[175](#bib.bib175)] 提出了使用贝叶斯信念模块生成注意力得分，利用其建模高结构数据及不确定性估计的能力。如[10](#S2.F10
    "图10 ‣ 2.2.1 基于统计的注意力 ‣ 2.2 硬（随机）注意力 ‣ 2 视觉中的注意力 ‣ 深度学习中的视觉注意力方法：深入调查")（b）所示，他们介绍了一种简单结构，通过四个步骤将任何确定性注意力模型转换为随机模型：1)
    使用Gamma分布构建解码器网络 2) 使用Weibull分布以及随机和确定性路径分别用于向下和向上 3) 从当前网络的查询和键对BABN分布进行参数化 4)
    使用证据下界来优化编码器和解码器。由于编码器中存在Weibull分布，整个网络是可微分的。在准确性和不确定性方面，BABN在NLP任务中显示出相较于现有技术的改进。
- en: •
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Repulsive Attention: Multi-head attention [[2](#bib.bib2)] is the core of attention
    used in transformers. However, MHA may cause attention collapse when extracting
    the same features [[60](#bib.bib60), [176](#bib.bib176), [177](#bib.bib177)] and
    consequently, the discrimination power for feature representation will not be
    diverse. To address this issue, An et al. [[60](#bib.bib60)] adapted MHA to a
    Bayesian network with an underlying stochastic attention. MHA is considered a
    special case without sharing parameters and using a particle-optimization sample
    to perform Bayesian inference on the attention parameter imposes attention repulsiveness
    [[178](#bib.bib178)]. Through this sampling method, each MHA is considered a sample
    seeking posterior distribution approximation, far from other heads.'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 排斥注意力：多头注意力 [[2](#bib.bib2)] 是变换器中使用的核心注意力。然而，当提取相同特征时，MHA可能导致注意力崩溃 [[60](#bib.bib60),
    [176](#bib.bib176), [177](#bib.bib177)]，从而导致特征表示的区分能力不够多样。为解决此问题，An 等人 [[60](#bib.bib60)]
    将MHA适配到具有潜在随机注意力的贝叶斯网络中。MHA被视为不共享参数的特殊情况，并且使用粒子优化样本对注意力参数进行贝叶斯推断会施加注意力排斥性 [[178](#bib.bib178)]。通过这种采样方法，每个MHA被视为寻求后验分布近似的样本，远离其他头部。
- en: '| ![Refer to caption](img/58eb9227f8b63d3490a0e34016b235cd.png) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/58eb9227f8b63d3490a0e34016b235cd.png) |'
- en: '| (a) Self-Critic Attention [[65](#bib.bib65)] |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| (a) 自我批评注意力 [[65](#bib.bib65)] |'
- en: '| ![Refer to caption](img/f86b88c95293302d38a13ef6af026097.png) |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/f86b88c95293302d38a13ef6af026097.png) |'
- en: '| (c) Expectation-Maximization Attention   [[68](#bib.bib68)] |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| (c) 期望最大化注意力 [[68](#bib.bib68)] |'
- en: '| ![Refer to caption](img/9236ee7d122f1ac1ac909239e7c7e68b.png) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/9236ee7d122f1ac1ac909239e7c7e68b.png) |'
- en: '| (b) Bayesian Attention Belief Networks [[175](#bib.bib175)] |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| (b) 贝叶斯注意力信念网络 [[175](#bib.bib175)] |'
- en: '| ![Refer to caption](img/f5eb266b5962853ffb13b55b73c229d8.png) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/f5eb266b5962853ffb13b55b73c229d8.png) |'
- en: '| (d) Gaussian Attention [[66](#bib.bib66)] |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| (d) 高斯注意力 [[66](#bib.bib66)] |'
- en: 'Figure 10: Illustration of hard attention architectures. Building blocks of
    EMA [[68](#bib.bib68)], Gaussian [[66](#bib.bib66)], Self-critic [[65](#bib.bib65)]
    and Bayesian [[175](#bib.bib175)]. Images are taken from the original papers.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：硬注意力架构的插图。EMA [[68](#bib.bib68)]、高斯 [[66](#bib.bib66)]、自我批评 [[65](#bib.bib65)]
    和贝叶斯 [[175](#bib.bib175)] 的构建模块。图像取自原始论文。
- en: 'Variational Attention In a study to improve the latent variable alignments,
    Deng et al. [[61](#bib.bib61)] proposed using variational attention mechanism.
    A latent variable is crucial because it encodes the dependencies between entities,
    whereas variational inference methods represent it in a stochastic manner [[179](#bib.bib179),
    [180](#bib.bib180)]. On the other hand, soft attention can encode alignments,
    but it has poor representation because of the nature of softmax. Using stochastic
    methods show better performance when optimized well [[181](#bib.bib181), [182](#bib.bib182)].
    The main idea is to propose variational attention along with keeping the training
    tractable. They introduced two types of variational attention: categorical (hard)
    attention that uses amortized variational inference based on policy gradient and
    soft attention variance; relaxed (probabilistic soft attention) using Dirichlet
    distribution that allows attending over multiple sources. Regarding reparameterization,
    Dirichlet distribution is not parameterizable, and thus the gradient has high
    variances [[183](#bib.bib183)]. Inspired by [[61](#bib.bib61)], Bahuleyan et al.
    developed stochastic attention-based variational inference [[184](#bib.bib184)],
    but using a normal distribution instead of Dirichlet distribution. They observed
    that variational encoder-decoders should not have a direct connection; otherwise,
    traditional attentions serve as bypass connections.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 变分注意力 在一项旨在改善潜在变量对齐的研究中，Deng 等人 [[61](#bib.bib61)] 提出了使用变分注意力机制。潜在变量至关重要，因为它编码了实体之间的依赖关系，而变分推断方法以随机方式表示它
    [[179](#bib.bib179), [180](#bib.bib180)]。另一方面，软注意力可以编码对齐，但由于 softmax 的性质，表现较差。使用随机方法在优化得当时表现更佳
    [[181](#bib.bib181), [182](#bib.bib182)]。主要思想是提出变分注意力，同时保持训练的可处理性。他们介绍了两种类型的变分注意力：使用基于策略梯度和软注意力方差的摊销变分推断的类别（硬）注意力；以及使用
    Dirichlet 分布的放松（概率软注意力），允许关注多个来源。关于重参数化，Dirichlet 分布不可参数化，因此梯度具有较高的方差 [[183](#bib.bib183)]。受
    [[61](#bib.bib61)] 启发，Bahuleyan 等人开发了基于随机注意力的变分推断 [[184](#bib.bib184)]，但使用了正态分布代替
    Dirichlet 分布。他们观察到，变分编码器-解码器不应有直接连接，否则传统注意力会作为绕过连接。
- en: 2.2.2 Reinforcement-based Attention
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 基于强化学习的注意力
- en: 'Self-Critic Attention: Chen et al. [[65](#bib.bib65)] proposed a self-critic
    attention model that generates attention using an agent and re-evaluates the gain
    from this attention using the REINFORCE algorithm. They observed that most of
    the attention modules are trained in a weakly-supervised manner. Therefore, the
    attention maps are not always discriminative and lack supervisory signals during
    training [[185](#bib.bib185)]. To supervise the generation of attention maps,
    they used a reinforcement algorithm to guide the whole process. As shown in Figure [10](#S2.F10
    "Figure 10 ‣ 2.2.1 Statistical-based attention ‣ 2.2 Hard (Stochastic) Attention
    ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning: An In-Depth
    Survey") (a), the feature maps are evaluated to predict whether it needs self
    correctness or not.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '自我批评注意力：Chen 等人 [[65](#bib.bib65)] 提出了一个自我批评注意力模型，该模型使用代理生成注意力，并使用 REINFORCE
    算法重新评估这种注意力的收益。他们观察到，大多数注意力模块以弱监督方式训练。因此，注意力图并不总是具有区分性，并且在训练过程中缺乏监督信号 [[185](#bib.bib185)]。为了监督注意力图的生成，他们使用了一种强化算法来指导整个过程。如图
    [10](#S2.F10 "Figure 10 ‣ 2.2.1 Statistical-based attention ‣ 2.2 Hard (Stochastic)
    Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning:
    An In-Depth Survey") (a) 所示，特征图被评估以预测是否需要自我校正。'
- en: 'Reinforced Self-Attention Network: Shen et al. [[63](#bib.bib63)] used a reinforced
    technique to combine soft and hard attention in one method. Soft attention has
    shown effectiveness in modeling local and global dependencies, which output from
    the dot-product similarity [[6](#bib.bib6)]. However, soft attention is based
    on the softmax function that assigns values to each item, even the non-attended
    ones, which weakens the whole attention. On the other hand, hard attention [[12](#bib.bib12)]
    attends to important regions or tokens only and disregards others. Despite its
    importance to textual tasks, it is inefficient in terms of time and differentiability
    [[174](#bib.bib174)]. Shen et al. [[63](#bib.bib63)] used hard attention to extract
    rich information and then feed it into soft attention for further processing.
    Simultaneously, soft attention is used to reward hard attention and hence stabilize
    the training process. Specifically, they used hard attention to encode tokens
    from the input in parallel while combining it with soft attention [[186](#bib.bib186)]
    without any CNN/RNN modules (see Figure [10](#S2.F10 "Figure 10 ‣ 2.2.1 Statistical-based
    attention ‣ 2.2 Hard (Stochastic) Attention ‣ 2 Attention in Vision ‣ Visual Attention
    Methods in Deep Learning: An In-Depth Survey") (e)). In [[64](#bib.bib64)], reinforcement
    attention was proposed to extract better temporal context from video. Specifically,
    this attention module uses Bernoulli-sigmoid unit [[174](#bib.bib174)], a stochastic
    module. Thus, to train the whole system, REINFORCE algorithm is used to stabilize
    the gradients [[183](#bib.bib183)].'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '强化自注意力网络：Shen 等人 [[63](#bib.bib63)] 使用了一种强化技术，将软注意力和硬注意力结合在一起。软注意力在建模局部和全局依赖关系方面表现出有效性，其输出来自点积相似度
    [[6](#bib.bib6)]。然而，软注意力基于 softmax 函数，该函数为每个项目分配值，即使是未被关注的项目，这会削弱整体注意力。另一方面，硬注意力
    [[12](#bib.bib12)] 仅关注重要的区域或标记，并忽略其他部分。尽管它对文本任务很重要，但在时间效率和可微分性方面并不高效 [[174](#bib.bib174)]。Shen
    等人 [[63](#bib.bib63)] 使用硬注意力来提取丰富的信息，然后将其输入软注意力进行进一步处理。同时，软注意力用于奖励硬注意力，从而稳定训练过程。具体来说，他们使用硬注意力并行编码输入中的标记，同时将其与软注意力
    [[186](#bib.bib186)] 结合，不使用任何 CNN/RNN 模块（参见图 [10](#S2.F10 "Figure 10 ‣ 2.2.1 Statistical-based
    attention ‣ 2.2 Hard (Stochastic) Attention ‣ 2 Attention in Vision ‣ Visual Attention
    Methods in Deep Learning: An In-Depth Survey") (e)）。在 [[64](#bib.bib64)] 中，提出了强化注意力以从视频中提取更好的时间上下文。具体来说，这个注意力模块使用
    Bernoulli-sigmoid 单元 [[174](#bib.bib174)]，这是一个随机模块。因此，为了训练整个系统，使用 REINFORCE 算法来稳定梯度
    [[183](#bib.bib183)]。'
- en: 2.2.3 Gaussian-based Attention
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 基于高斯的注意力
- en: 'Self Supervised Gaussian-Attention: Most soft-attention models use softmax
    to predict the attention of feature maps [[2](#bib.bib2), [41](#bib.bib41), [23](#bib.bib23)]
    which suffers from various drawbacks. In [[187](#bib.bib187)], Niu et al. proposed
    replacing the classical softmax with a Gaussian attention module. As shown in
    Figure [10](#S2.F10 "Figure 10 ‣ 2.2.1 Statistical-based attention ‣ 2.2 Hard
    (Stochastic) Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep
    Learning: An In-Depth Survey") (d), they build a 2D Gaussian kernel to generate
    attention maps instead of softmax $K=e(-\frac{1}{\alpha}(u-\mu)^{T}\sum^{-1}(u-\mu))$
    for each individual element, where $u=[x,y]^{T}$, $\mu=[\mu_{x},\mu^{y}]^{T}$.
    A fully connected layer passes the extracted features, and then the Gaussian kernel
    is used to predict the attention scores. Using Gaussian kernels proved its effectiveness
    in discriminating the important features. Since it does not require any further
    learning steps, such as fully connected layers or convolutions, this significantly
    reduces the number of parameters. As stochastic training models need careful designs
    because of SGD mismatching [[188](#bib.bib188), [189](#bib.bib189), [190](#bib.bib190)],
    the Gaussian attention model developed binary classification loss that takes normalized
    logits to suppress the low scores and discriminate the high ones. This normalization
    uses a modified version of softmax, where the input is squared and divided by
    temperature value (e.g. batch size).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '自监督高斯注意力：大多数软注意力模型使用 softmax 来预测特征图的注意力 [[2](#bib.bib2), [41](#bib.bib41),
    [23](#bib.bib23)]，但这种方法存在各种缺陷。在 [[187](#bib.bib187)] 中，Niu 等人提出用高斯注意力模块替代经典的 softmax。正如图
    [10](#S2.F10 "Figure 10 ‣ 2.2.1 Statistical-based attention ‣ 2.2 Hard (Stochastic)
    Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep Learning:
    An In-Depth Survey") (d) 所示，他们构建了一个 2D 高斯核来生成注意力图，而不是对每个单独元素使用 softmax $K=e(-\frac{1}{\alpha}(u-\mu)^{T}\sum^{-1}(u-\mu))$，其中
    $u=[x,y]^{T}$，$\mu=[\mu_{x},\mu^{y}]^{T}$。一个全连接层传递提取的特征，然后使用高斯核来预测注意力分数。使用高斯核证明了其在区分重要特征方面的有效性。由于不需要进一步的学习步骤，如全连接层或卷积，这显著减少了参数数量。由于随机梯度下降（SGD）不匹配，随机训练模型需要精心设计
    [[188](#bib.bib188), [189](#bib.bib189), [190](#bib.bib190)]，高斯注意力模型开发了二分类损失，它采用归一化的
    logits 来抑制低分数并区分高分数。这种归一化使用了修改版的 softmax，其中输入值被平方并除以温度值（例如批量大小）。'
- en: 'Uncertainty-Aware Attention: Since attention is generated without full supervision
    (i.e. in a weakly-supervised manner), it lacks full reliability [[44](#bib.bib44)].
    To fix this issue, [[67](#bib.bib67)] proposed the use of uncertainty which is
    based on input. It generates varied attention maps according to the input and,
    therefore, learns higher variance for uncertain inputs. Gaussian distribution
    is used to handle attention weights, such that it gives small values in case of
    high confidence and vice versa [[191](#bib.bib191)]. Bayesian network is employed
    to build the model with variational inference as a solution [[192](#bib.bib192),
    [193](#bib.bib193)]. Note that this model is stochastic and SGD backpropagation
    flow can not work properly due to randomness [[194](#bib.bib194)]. For this reason,
    they used the reparameterization trick [[195](#bib.bib195), [196](#bib.bib196)]
    to train their model.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性感知注意力：由于注意力是在没有完全监督的情况下生成的（即以弱监督的方式），因此其可靠性不完全 [[44](#bib.bib44)]。为了解决这个问题，[[67](#bib.bib67)]
    提出了基于输入的不确定性。它根据输入生成不同的注意力图，从而为不确定的输入学习更高的方差。高斯分布用于处理注意力权重，使其在高置信度情况下给出小值，反之亦然
    [[191](#bib.bib191)]。贝叶斯网络被用来构建带有变分推断的模型作为解决方案 [[192](#bib.bib192), [193](#bib.bib193)]。注意，这个模型是随机的，并且由于随机性，SGD
    反向传播流程可能无法正常工作 [[194](#bib.bib194)]。因此，他们使用了重参数化技巧 [[195](#bib.bib195), [196](#bib.bib196)]
    来训练他们的模型。
- en: 2.2.4 Clustering
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 聚类
- en: 'Expectation Maximization attention: Traditional soft attention mechanisms can
    encode long-range dependencies by comparing each position to all positions, which
    is computationally very expensive [[3](#bib.bib3)]. In this regard, Li et al. [[68](#bib.bib68)]
    proposed using expectation maximization to build an attention method that iteratively
    forms a set of bases that compute the attention maps [[197](#bib.bib197)]. The
    main intuition is to use expectation maximization to select a compact basis set
    instead of using all the pixels as in [[3](#bib.bib3), [39](#bib.bib39)] (see
    Figure [10](#S2.F10 "Figure 10 ‣ 2.2.1 Statistical-based attention ‣ 2.2 Hard
    (Stochastic) Attention ‣ 2 Attention in Vision ‣ Visual Attention Methods in Deep
    Learning: An In-Depth Survey") (c)). These bases are regarded as the learning
    parameters, whereas the latent variables serve as the attention maps. The output
    is the weighted sum of bases, and the attention maps are the weights. The estimation
    step is defined by $z=\frac{\mathbb{K}(x_{n},\mu_{n})}{\sum_{j}\mathbb{K}(x_{n},\mu_{j})}$,
    where $\mathbb{K}$ denotes a kernel function. The maximization step updates $\mu$
    through data likelihood maximization such that $\mu=\frac{z_{nk}(x_{n},\mu_{n})}{\sum_{j}z_{jk}}$.
    Finally, the features are multiplied by attention scores $\mathbb{X}=\mathbb{Z}\mu$.
    Since EMA is a stochastic model, training the whole model needs special care.
    Firstly, the authors average the $\mu$ over the mini-batch and update the maximization
    step to train it stably. Secondly, they normalize the Value of $\mu$ to be within
    (1, $T$) by $\ell_{2}$-Norm. EMA has shown the ability to remove noisy representation
    and to give promising results after the third step. Also, it is worth noting that
    the complexity is reduced to a linear form $\mathcal{O}(NK)$ from a quadratic
    one $\mathcal{O}(N^{2})$.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '期望最大化注意力：传统的软注意力机制通过将每个位置与所有位置进行比较来编码长程依赖关系，这在计算上非常昂贵[[3](#bib.bib3)]。在这方面，Li等人[[68](#bib.bib68)]
    提出了使用期望最大化来构建一种注意力方法，该方法迭代地形成一组基础来计算注意力图[[197](#bib.bib197)]。主要直觉是使用期望最大化来选择一个紧凑的基础集，而不是像[[3](#bib.bib3),
    [39](#bib.bib39)]中那样使用所有像素（见图[10](#S2.F10 "Figure 10 ‣ 2.2.1 Statistical-based
    attention ‣ 2.2 Hard (Stochastic) Attention ‣ 2 Attention in Vision ‣ Visual Attention
    Methods in Deep Learning: An In-Depth Survey") (c)）。这些基础被视为学习参数，而潜在变量则作为注意力图。输出是基础的加权和，注意力图则是权重。估计步骤由
    $z=\frac{\mathbb{K}(x_{n},\mu_{n})}{\sum_{j}\mathbb{K}(x_{n},\mu_{j})}$ 定义，其中
    $\mathbb{K}$ 表示核函数。最大化步骤通过数据似然最大化更新 $\mu$ 使得 $\mu=\frac{z_{nk}(x_{n},\mu_{n})}{\sum_{j}z_{jk}}$。最后，特征与注意力分数相乘
    $\mathbb{X}=\mathbb{Z}\mu$。由于EMA是一个随机模型，训练整个模型需要特别注意。首先，作者对 $\mu$ 进行 mini-batch
    平均并更新最大化步骤以稳定训练。其次，他们通过 $\ell_{2}$-Norm 将 $\mu$ 的值规范化到 (1, $T$) 之间。EMA 已显示出去除噪声表示的能力，并在第三步之后提供了有希望的结果。此外，值得注意的是，复杂度从二次形式
    $\mathcal{O}(N^{2})$ 减少到线性形式 $\mathcal{O}(NK)$。'
- en: 3 Open Problems and Challenges
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 开放问题与挑战
- en: Despite the performance improvement and interesting salient features of attention
    models, various challenges are associated with their practical settings in computer
    vision applications. The essential impediments include a requirement for high
    computational costs, significant amounts of training data, the efficiency of the
    model, and a cost-benefit analysis of performance improvement. In addition, there
    have also been some challenges to visualize and interpret attention blocks. This
    section provides an overview of these challenges and limitations, mentions some
    recent efforts to address those limitations, and highlights the open research
    questions.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管注意力模型在性能提升和有趣的显著特征方面有所改进，但它们在计算机视觉应用中的实际设置仍面临各种挑战。主要的障碍包括对高计算成本、大量训练数据的需求、模型的效率，以及性能提升的成本效益分析。此外，如何可视化和解释注意力模块也面临一些挑战。本节概述了这些挑战和局限性，提及了为解决这些限制所做的一些最新努力，并突出了一些开放的研究问题。
- en: 'Generalization: Attention models’ generalization is a challenging task. Many
    of the proposed models are specific to the application underhand and only work
    well in the proposed settings. Whereas some models (e.g. channel and spatial attention)
    have performed better in classification since attention models are primarily designed
    for high-level tasks; they fail when applied directly on low-level vision tasks.
    Moreover, the data quality has a notable influence on the generalization and robustness
    of attention models. Thus, there is still a significant step to generalize pre-trained
    attention models on more generalized low-level vision tasks.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化：注意力模型的泛化是一个具有挑战性的任务。许多提出的模型特定于特定应用，只在提出的设置下表现良好。虽然一些模型（例如通道和空间注意力）在分类中表现更好，因为注意力模型主要设计用于高层任务，但在低层视觉任务中直接应用时会失败。此外，数据质量对注意力模型的泛化和鲁棒性有显著影响。因此，仍需在更广泛的低层视觉任务上泛化预训练的注意力模型。
- en: 'Efficiency: Efficiency of vision models is vital for many real-time computer
    vision applications. Unfortunately, current models focus more on performance than
    efficiency. Recently, self-attention has been successfully applied in transformers
    and shown to achieve better performance; however, at the cost of huge computational
    complexity e.g. the base ViT [[9](#bib.bib9)] has 18 billion FLOPs compared to
    the CNN models [[198](#bib.bib198), [199](#bib.bib199)] with 600 million FLOPs,
    achieving similar performance to process an image. Although fewer attempts such
    as Efficient Channel attention are made to make the attention models more efficient,
    they remain complex to train; hence, efficient models are required for deployment
    on real-time devices.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 效率：视觉模型的效率对许多实时计算机视觉应用至关重要。不幸的是，当前的模型更注重性能而非效率。最近，自注意力已经成功应用于变换器，并显示出更好的性能；然而，这带来了巨大的计算复杂度，例如基础ViT [[9](#bib.bib9)]
    的计算量为180亿FLOP，而CNN模型[[198](#bib.bib198), [199](#bib.bib199)] 为6亿FLOP，处理一张图像时性能相当。尽管像高效通道注意力这样的尝试使注意力模型更高效，但训练仍然复杂；因此，需要高效的模型来部署在实时设备上。
- en: 'Multi-Model Data: Attention has been applied mainly on single domain data and
    in a single task setting. An important question is whether the attention model
    can fuse input data in a meaningful manner and exploit multiple label types (or
    tasks) in the data. Also, it is yet to be seen that attention models can leverage
    the various labels available, such as combining the point clouds and the RGB images
    of the KITTI dataset, to provide a meaningful performance. Similarly, attention
    models can also be used to know whether they can predict relationships between
    the labels, actions, and attributes in a unified manner.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 多模型数据：注意力主要应用于单一领域数据和单一任务设置。一个重要的问题是注意力模型是否可以以有意义的方式融合输入数据并利用数据中的多种标签类型（或任务）。此外，尚未看到注意力模型是否能够利用各种标签，例如将KITTI数据集的点云和RGB图像结合起来，以提供有意义的性能。同样，注意力模型也可以用于了解它们是否能够以统一的方式预测标签、动作和属性之间的关系。
- en: 'Amount of Training Data: Attention models usually rely on much more training
    data to learn the important aspects, compared to simple non-attentional models.
    For example, self-attention employed in transformers needs to learn the invariance,
    translation etc. by themselves instead of non-attentional CNNs, where these properties
    are inbuilt due to operations such as pooling. The increase in data also means
    more training time and computational resources. Hence, an open question here is
    how to address this problem with more efficient attention models.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据量：注意力模型通常依赖于比简单非注意力模型更多的训练数据来学习重要的方面。例如，变换器中的自注意力需要自行学习不变性、平移等，而不是像非注意力CNN那样，这些属性由于池化等操作而内置。数据的增加也意味着更多的训练时间和计算资源。因此，如何用更高效的注意力模型解决这个问题仍是一个悬而未决的问题。
- en: 'Performance Comparisons: Models that employ attentional blocks mostly compare
    their performance against the baseline without having the attention while ignoring
    other attentional blocks. The lack of comparison between different attentional
    models provides little information about the actual performance improvement against
    other attentions. Therefore, there is a need to present a more in-depth analysis
    of the number of parameters increased versus the performance gain of different
    attentional models proposed in the literature.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 性能比较：采用注意力模块的模型通常将其性能与基线进行比较，而忽略其他注意力模块。不同注意力模型之间缺乏比较提供了关于相对于其他注意力的实际性能提升的信息。因此，需要对文献中提出的不同注意力模型的参数增加量与性能提升进行更深入的分析。
- en: 4 Conclusions
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: In this paper, we reviewed more than 70 articles related to various attention
    mechanisms used in vision applications. We provided a comprehensive discussion
    of the attention techniques along with their strengths and limitations. We provided
    a restructuring of the existing attention mechanisms proposed in the literature
    into a hierarchical framework based on how they compute their attention scores.
    Choosing the attention score calculation to group the reviewed techniques has
    been effective in determining how the attention-based models are built and which
    training strategies are employed therein.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们回顾了70多篇与视觉应用中使用的各种注意力机制相关的文章。我们对注意力技术进行了全面的讨论，包括其优缺点。我们将文献中提出的现有注意力机制重新结构化为基于计算其注意力分数的分层框架。选择注意力分数计算来对所审查的技术进行分组，对于确定基于注意力的模型如何构建以及所采用的训练策略是有效的。
- en: Although the capability of the developed attention-based techniques in modeling
    the salient features and boosting the performance is commendable, various challenges
    and open questions still remain unanswered, especially with the use of these techniques
    for computer vision tasks. We have listed these challenges and have highlighted
    research questions that still remain open. Despite some recent efforts introduced
    to cope with some of these limitations, we are still far from having solved the
    problems related to attention in vision. This survey will help researchers to
    better focus their efforts in addressing these challenges efficiently and in developing
    attention mechanisms that are better suited for vision-based applications.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管开发的基于注意力的技术在建模显著特征和提升性能方面表现出色，但仍存在各种挑战和未解答的问题，特别是在计算机视觉任务中使用这些技术时。我们列出了这些挑战，并突出了仍未解决的研究问题。尽管最近有一些努力试图解决这些限制，我们仍然距离解决视觉中的注意力问题还有很长的路要走。这项调查将帮助研究人员更好地集中精力有效应对这些挑战，并开发更适合视觉应用的注意力机制。
- en: 5 Acknowledgments
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 致谢
- en: Professor Ajmal Mian is the recipient of an Australian Research Council Future
    Fellowship Award (project number FT210100268) funded by the Australian Government.
    We thank Professor Mubarak Shah for his useful comments that significantly improved
    the presentation of the survey.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Ajmal Mian 教授是澳大利亚研究委员会未来研究奖（项目编号 FT210100268）的获得者，该奖项由澳大利亚政府资助。我们感谢 Mubarak
    Shah 教授的有益评论，这些评论显著改善了调查的展示效果。
- en: References
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin, “Convolutional
    sequence to sequence learning,” in *International Conference on Machine Learning*.   PMLR,
    2017.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. 盖林，M. 奥利，D. 格朗杰，D. 雅拉茨，和 Y. N. 多芬，“卷积序列到序列学习”，发表于*国际机器学习会议*。 PMLR，2017年。'
- en: '[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *NeurIPS*, 2017.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. 瓦斯瓦尼，N. 沙泽尔，N. 帕尔马尔，J. 乌斯科雷特，L. 琼斯，A. N. 戈麦斯，Ł. 凯瑟尔，和 I. 波洛苏金，“注意力是你需要的一切”，发表于*NeurIPS*，2017年。'
- en: '[3] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,”
    in *CVPR*, 2018.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] X. 王，R. 吉尔什克，A. 古普塔，和 K. 何，“非局部神经网络”，发表于*CVPR*，2018年。'
- en: '[4] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Self-attention generative
    adversarial networks,” in *International conference on machine learning*.   PMLR,
    2019.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] H. 张，I. 古德费洛，D. 梅塔克萨斯，和 A. 奥德纳，“自注意力生成对抗网络”，发表于*国际机器学习会议*。 PMLR，2019年。'
- en: '[5] S. Iqbal and F. Sha, “Actor-attention-critic for multi-agent reinforcement
    learning,” in *ICML*.   PMLR, 2019.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] S. 伊克巴尔和 F. 沙，“用于多智能体强化学习的演员-注意力-评论员”，发表于*ICML*。 PMLR，2019年。'
- en: '[6] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” *arXiv:1409.0473*, 2014.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] D. Bahdanau, K. Cho, 和 Y. Bengio, “通过共同学习对齐和翻译的神经机器翻译，” *arXiv:1409.0473*，2014年。'
- en: '[7] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in *Advances in neural information processing systems*,
    2014.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] I. Sutskever, O. Vinyals, 和 Q. V. Le, “基于神经网络的序列到序列学习，” 收录于*神经信息处理系统进展*，2014年。'
- en: '[8] M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based
    neural machine translation,” in *Proceedings of the 2015 Conference on Empirical
    Methods in Natural Language Processing*, 2015.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] M.-T. Luong, H. Pham, 和 C. D. Manning, “基于注意力的神经机器翻译的有效方法，” 收录于*2015年自然语言处理实证方法会议论文集*，2015年。'
- en: '[9] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth 16x16
    words: Transformers for image recognition at scale,” *arXiv preprint arXiv:2010.11929*,
    2020.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *等*，“图像价值16x16个单词：用于大规模图像识别的变换器，”
    *arXiv预印本arXiv:2010.11929*，2020年。'
- en: '[10] A. R. Kosiorek, A. Bewley, and I. Posner, “Hierarchical attentive recurrent
    tracking,” in *Proceedings of the 31st International Conference on Neural Information
    Processing Systems*, 2017.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. R. Kosiorek, A. Bewley, 和 I. Posner, “层次注意递归跟踪，” 收录于*第31届国际神经信息处理系统会议论文集*，2017年。'
- en: '[11] S. Jetley, N. A. Lord, N. Lee, and P. H. Torr, “Learn to pay attention,”
    in *International Conference on Learning Representations*, 2018.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. Jetley, N. A. Lord, N. Lee, 和 P. H. Torr, “学会注意，” 收录于*国际学习表征会议*，2018年。'
- en: '[12] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel,
    and Y. Bengio, “Show, attend and tell: Neural image caption generation with visual
    attention,” in *ICML*, 2015, pp. 2048–2057.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel,
    和 Y. Bengio, “展示、关注与讲述：带有视觉注意力的神经图像描述生成，” 收录于*ICML*，2015年，第2048–2057页。'
- en: '[13] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object detection with deep
    learning: A review,” *TNNLS*, 2019.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Z.-Q. Zhao, P. Zheng, S.-t. Xu, 和 X. Wu, “深度学习中的目标检测：综述，” *TNNLS*，2019年。'
- en: '[14] M. Z. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga, “A comprehensive
    survey of deep learning for image captioning,” *CSUR*, 2019.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] M. Z. Hossain, F. Sohel, M. F. Shiratuddin, 和 H. Laga, “深度学习在图像描述中的全面调查，”
    *CSUR*，2019年。'
- en: '[15] S. Qiu, Y. Wu, S. Anwar, and C. Li, “Investigating attention mechanism
    in 3d point cloud object detection,” in *International Conference on 3D Vision
    (3DV)*, 2021, pp. 403–412.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. Qiu, Y. Wu, S. Anwar, 和 C. Li, “调查3D点云目标检测中的注意力机制，” 收录于*国际3D视觉会议（3DV）*，2021年，第403–412页。'
- en: '[16] D. Hu, “An introductory survey on attention mechanisms in nlp problems,”
    in *SAI Intelligent Systems Conference*.   Springer, 2019.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] D. Hu, “自然语言处理问题中注意力机制的介绍性调查，” 收录于*SAI智能系统会议*。Springer，2019年。'
- en: '[17] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu,
    Y. Xu *et al.*, “A survey on visual transformer,” *arXiv preprint arXiv:2012.12556*,
    2020.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C.
    Xu, Y. Xu *等*，“视觉变换器综述，” *arXiv预印本arXiv:2012.12556*，2020年。'
- en: '[18] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah, “Transformers
    in vision: A survey,” *arXiv preprint arXiv:2101.01169*, 2021.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, 和 M. Shah, “视觉中的变换器：综述，”
    *arXiv预印本arXiv:2101.01169*，2021年。'
- en: '[19] J. B. Lee, R. A. Rossi, S. Kim, N. K. Ahmed, and E. Koh, “Attention models
    in graphs: A survey,” *ACM Transactions on Knowledge Discovery from Data (TKDD)*,
    2019.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. B. Lee, R. A. Rossi, S. Kim, N. K. Ahmed, 和 E. Koh, “图中的注意力模型：综述，”
    *ACM知识发现数据（TKDD）*，2019年。'
- en: '[20] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman,
    and P. Blunsom, “Teaching machines to read and comprehend,” *NIPS*, vol. 28, 2015.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman,
    和 P. Blunsom, “教机器阅读和理解，” *NIPS*，第28卷，2015年。'
- en: '[21] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in *CVPR*,
    2018.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Hu, L. Shen, 和 G. Sun, “挤压和激励网络，” 收录于*CVPR*，2018年。'
- en: '[22] W. Qilong, W. Banggu, Z. Pengfei, L. Peihua, Z. Wangmeng, and H. Qinghua,
    “Eca-net: Efficient channel attention for deep convolutional neural networks,”
    in *CVPR*, 2020.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] W. Qilong, W. Banggu, Z. Pengfei, L. Peihua, Z. Wangmeng, 和 H. Qinghua,
    “ECA-NET：高效的通道注意力用于深度卷积神经网络，” 收录于*CVPR*，2020年。'
- en: '[23] H. Zhang, C. Wu, Z. Zhang, Y. Zhu, H. Lin, Z. Zhang, Y. Sun, T. He, J. Mueller,
    R. Manmatha *et al.*, “Resnest: Split-attention networks,” *arXiv:2004.08955*,
    2020.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] H. Zhang, C. Wu, Z. Zhang, Y. Zhu, H. Lin, Z. Zhang, Y. Sun, T. He, J.
    Mueller, R. Manmatha *等*，“ResNeSt：分裂注意力网络，” *arXiv:2004.08955*，2020年。'
- en: '[24] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] K. He, X. Zhang, S. Ren, 和 J. Sun，"用于图像识别的深度残差学习"，发表于 *CVPR*，2016年。'
- en: '[25] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, “Cbam: Convolutional block
    attention module,” in *ECCV*, 2018.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. Woo, J. Park, J.-Y. Lee, 和 I. S. Kweon，"CBAM：卷积块注意力模块"，发表于 *ECCV*，2018年。'
- en: '[26] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in *CVPR*,
    2018.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. Hu, L. Shen, 和 G. Sun，"挤压和激励网络"，发表于 *CVPR*，2018年。'
- en: '[27] T. Dai, J. Cai, Y. Zhang, S.-T. Xia, and L. Zhang, “Second-order attention
    network for single image super-resolution,” in *CVPR*, June 2019.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] T. Dai, J. Cai, Y. Zhang, S.-T. Xia, 和 L. Zhang，"单幅图像超分辨率的二阶注意力网络"，发表于
    *CVPR*，2019年6月。'
- en: '[28] P. Li, J. Xie, Q. Wang, and W. Zuo, “Is second-order information helpful
    for large-scale visual recognition?” in *ICCV*, 2017.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] P. Li, J. Xie, Q. Wang, 和 W. Zuo，"二阶信息对于大规模视觉识别是否有帮助？"，发表于 *ICCV*，2017年。'
- en: '[29] F. Ding, G. Yang, J. Wu, D. Ding, J. Xv, G. Cheng, and X. Li, “High-order
    attention networks for medical image segmentation,” in *MICCAI*, 2020.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] F. Ding, G. Yang, J. Wu, D. Ding, J. Xv, G. Cheng, 和 X. Li，"用于医学图像分割的高阶注意力网络"，发表于
    *MICCAI*，2020年。'
- en: '[30] R. A. Horn, “The hadamard product,” in *Proc. Symp. Appl. Math*, vol. 40,
    1990.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] R. A. Horn，"哈达玛积"，发表于 *Proc. Symp. Appl. Math*，第40卷，1990年。'
- en: '[31] J.-H. Kim, K.-W. On, W. Lim, J. Kim, J.-W. Ha, and B.-T. Zhang, “Hadamard
    product for low-rank bilinear pooling,” *arXiv:1610.04325*, 2016.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J.-H. Kim, K.-W. On, W. Lim, J. Kim, J.-W. Ha, 和 B.-T. Zhang，"用于低秩双线性池化的哈达玛积"，*arXiv:1610.04325*，2016年。'
- en: '[32] W. Li, X. Zhu, and S. Gong, “Harmonious attention network for person re-identification,”
    in *CVPR*, June 2018.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] W. Li, X. Zhu, 和 S. Gong，"用于行人再识别的和谐注意力网络"，发表于 *CVPR*，2018年6月。'
- en: '[33] D. Yi, Z. Lei, S. Liao, and S. Z. Li, “Deep metric learning for person
    re-identification,” in *ICPR*, 2014.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] D. Yi, Z. Lei, S. Liao, 和 S. Z. Li，"用于行人再识别的深度度量学习"，发表于 *ICPR*，2014年。'
- en: '[34] D. Li, X. Chen, Z. Zhang, and K. Huang, “Learning deep context-aware features
    over body and latent parts for person re-identification,” in *CVPR*, 2017, pp.
    384–393.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] D. Li, X. Chen, Z. Zhang, 和 K. Huang，"通过身体和潜在部位学习深度上下文感知特征以进行行人再识别"，发表于
    *CVPR*，2017年，页384–393。'
- en: '[35] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, “Scalable person
    re-identification: A benchmark,” in *ICCV*, 2015.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, 和 Q. Tian，"可扩展的行人再识别：基准"，发表于
    *ICCV*，2015年。'
- en: '[36] W. Li, R. Zhao, T. Xiao, and X. Wang, “Deepreid: Deep filter pairing neural
    network for person re-identification,” in *CVPR*, 2014.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] W. Li, R. Zhao, T. Xiao, 和 X. Wang，"Deepreid：用于行人再识别的深度滤波配对神经网络"，发表于 *CVPR*，2014年。'
- en: '[37] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, “Dual attention
    network for scene segmentation,” in *CVPR*, 2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, 和 H. Lu，"用于场景分割的双重注意力网络"，发表于
    *CVPR*，2019年。'
- en: '[38] T. Zhao and X. Wu, “Pyramid feature attention network for saliency detection,”
    in *CVPR*, 2019.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] T. Zhao 和 X. Wu，"用于显著性检测的金字塔特征注意力网络"，发表于 *CVPR*，2019年。'
- en: '[39] C. Li, D. Du, L. Zhang, L. Wen, T. Luo, Y. Wu, and P. Zhu, “Spatial attention
    pyramid network for unsupervised domain adaptation,” in *ECCV*, 2020.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] C. Li, D. Du, L. Zhang, L. Wen, T. Luo, Y. Wu, 和 P. Zhu，"用于无监督领域适应的空间注意力金字塔网络"，发表于
    *ECCV*，2020年。'
- en: '[40] Z. Meng, J. Ma, and X. Yuan, “End-to-end low cost compressive spectral
    imaging with spatial-spectral self-attention,” in *ECCV*, 2020.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Z. Meng, J. Ma, 和 X. Yuan，"端到端低成本压缩光谱成像与空间-光谱自注意力"，发表于 *ECCV*，2020年。'
- en: '[41] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and
    D. Tran, “Image transformer,” in *ICML*, 2018.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, 和 D.
    Tran，"图像变换器"，发表于 *ICML*，2018年。'
- en: '[42] J. Cheng, L. Dong, and M. Lapata, “Long short-term memory-networks for
    machine reading,” in *EMNLP*, 2016.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] J. Cheng, L. Dong, 和 M. Lapata，"用于机器阅读的长短期记忆网络"，发表于 *EMNLP*，2016年。'
- en: '[43] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and J. Shlens,
    “Stand-alone self-attention in vision models,” in *NeurIPS*, 2019.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, 和 J. Shlens，"视觉模型中的独立自注意力"，发表于
    *NeurIPS*，2019年。'
- en: '[44] K. Li, Z. Wu, K.-C. Peng, J. Ernst, and Y. Fu, “Tell me where to look:
    Guided attention inference network,” in *CVPR*, 2018.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] K. Li, Z. Wu, K.-C. Peng, J. Ernst, 和 Y. Fu，"告诉我哪里看：引导注意力推断网络"，发表于 *CVPR*，2018年。'
- en: '[45] X. Zhu, J. Qian, H. Wang, and P. Liu, “Curriculum enhanced supervised
    attention network for person re-identification,” *Signal Processing Letters*,
    vol. 27, 2020.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] X. Zhu, J. Qian, H. Wang, 和 P. Liu，"课程增强的监督注意力网络用于行人再识别"，*信号处理快报*，第27卷，2020年。'
- en: '[46] R. Hou, H. Chang, B. Ma, S. Shan, and X. Chen, “Cross attention network
    for few-shot classification,” *arXiv:1910.07677*, 2019.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] R. Hou, H. Chang, B. Ma, S. Shan, 和 X. Chen，"用于少样本分类的交叉注意力网络"，*arXiv:1910.07677*，2019年。'
- en: '[47] K.-H. Lee, X. Chen, G. Hua, H. Hu, and X. He, “Stacked cross attention
    for image-text matching,” in *ECCV*, 2018, pp. 201–216.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] K.-H. Lee, X. Chen, G. Hua, H. Hu, 和 X. He，“用于图像-文本匹配的堆叠交叉注意力”，在*ECCV*，2018年，页码201–216。'
- en: '[48] A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and J. Carreira,
    “Perceiver: General perception with iterative attention,” *arXiv:2103.03206*,
    2021.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, 和 J. Carreira，“Perceiver：通过迭代注意力实现的通用感知”，*arXiv:2103.03206*，2021年。'
- en: '[49] S. Chen and Q. Zhao, “Boosted attention: Leveraging human attention for
    image captioning,” in *ECCV*, 2018, pp. 68–84.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] S. Chen 和 Q. Zhao，“增强注意力：利用人类注意力进行图像描述”，在*ECCV*，2018年，页码68–84。'
- en: '[50] P. Baldi and P. Sadowski, “The dropout learning algorithm,” *Artificial
    intelligence*, vol. 210, pp. 78–122, 2014.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] P. Baldi 和 P. Sadowski，“Dropout 学习算法”，*人工智能*，第210卷，页码78–122，2014年。'
- en: '[51] D. Jin, J. T. Lee, and C. S. Kim, “Semantic line detection using mirror
    attention and comparative ranking and matching,” in *ECCV*, 2020.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] D. Jin, J. T. Lee, 和 C. S. Kim，“利用镜像注意力和比较排名匹配的语义线检测”，在*ECCV*，2020年。'
- en: '[52] S. Chen, X. Tan, B. Wang, and X. Hu, “Reverse attention for salient object
    detection,” in *ECCV*, 2018.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] S. Chen, X. Tan, B. Wang, 和 X. Hu，“用于显著性物体检测的逆注意力”，在*ECCV*，2018年。'
- en: '[53] H. Zhang, H. Wang, Y. Cao, C. Shen, and Y. Li, “Robust watermarking using
    inverse gradient attention,” *arXiv preprint arXiv:2011.10850*, 2020.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] H. Zhang, H. Wang, Y. Cao, C. Shen, 和 Y. Li，“使用逆梯度注意力进行鲁棒水印”，*arXiv 预印本
    arXiv:2011.10850*，2020年。'
- en: '[54] C. Xia, J. Li, J. Su, and Y. Tian, “Exploring reciprocal attention for
    salient object detection by cooperative learning,” *arXiv preprint arXiv:1909.08269*,
    2019.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] C. Xia, J. Li, J. Su, 和 Y. Tian，“通过合作学习探索互惠注意力以检测显著性物体”，*arXiv 预印本 arXiv:1909.08269*，2019年。'
- en: '[55] N. Liu, J. Han, and M.-H. Yang, “Picanet: Learning pixel-wise contextual
    attention for saliency detection,” in *IEEE CVPR*, 2018.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] N. Liu, J. Han, 和 M.-H. Yang，“Picanet：学习像素级上下文注意力用于显著性检测”，在*IEEE CVPR*，2018年。'
- en: '[56] D. Zoran, M. Chrzanowski, P.-S. Huang, S. Gowal, A. Mott, and P. Kohli,
    “Towards robust image classification using sequential attention models,” in *CVPR*,
    2020.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] D. Zoran, M. Chrzanowski, P.-S. Huang, S. Gowal, A. Mott, 和 P. Kohli，“利用序列注意力模型进行鲁棒图像分类”，在*CVPR*，2020年。'
- en: '[57] B. Ma, J. Zhang, Y. Xia, and D. Tao, “Auto learning attention,” in *NIPS*,
    H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds., 2020.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] B. Ma, J. Zhang, Y. Xia, 和 D. Tao，“自动学习注意力”，在*NIPS*，H. Larochelle, M.
    Ranzato, R. Hadsell, M. F. Balcan, 和 H. Lin 编辑，2020年。'
- en: '[58] J. Lee, Y. Lee, J. Kim, A. Kosiorek, S. Choi, and Y. W. Teh, “Set transformer:
    A framework for attention-based permutation-invariant neural networks,” in *ICML*,
    2019.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] J. Lee, Y. Lee, J. Kim, A. Kosiorek, S. Choi, 和 Y. W. Teh，“集合变换器：基于注意力的置换不变神经网络框架”，在*ICML*，2019年。'
- en: '[59] X. Fan, S. Zhang, B. Chen, and M. Zhou, “Bayesian attention modules,”
    *arXiv:2010.10604*, 2020.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] X. Fan, S. Zhang, B. Chen, 和 M. Zhou，“贝叶斯注意力模块”，*arXiv:2010.10604*，2020年。'
- en: '[60] B. An, J. Lyu, Z. Wang, C. Li, C. Hu, F. Tan, R. Zhang, Y. Hu, and C. Chen,
    “Repulsive attention: Rethinking multi-head attention as bayesian inference,”
    in *EMNLP*, 2020.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] B. An, J. Lyu, Z. Wang, C. Li, C. Hu, F. Tan, R. Zhang, Y. Hu, 和 C. Chen，“排斥注意力：重新思考多头注意力作为贝叶斯推断”，在*EMNLP*，2020年。'
- en: '[61] Y. Deng, Y. Kim, J. Chiu, D. Guo, and A. M. Rush, “Latent alignment and
    variational attention,” in *NeurIPS*, 2018.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Y. Deng, Y. Kim, J. Chiu, D. Guo, 和 A. M. Rush，“潜在对齐与变分注意力”，在*NeurIPS*，2018年。'
- en: '[62] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel,
    and Y. Bengio, “Show, attend and tell: Neural image caption generation with visual
    attention,” in *ICML*, 2015, pp. 2048–2057.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel,
    和 Y. Bengio，“展示，关注和讲述：带有视觉注意力的神经图像描述生成”，在*ICML*，2015年，页码2048–2057。'
- en: '[63] T. Shen, T. Zhou, G. Long, J. Jiang, S. Wang, and C. Zhang, “Reinforced
    self-attention network: a hybrid of hard and soft attention for sequence modeling,”
    in *IJCAI*, 2018, pp. 4345–4352.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] T. Shen, T. Zhou, G. Long, J. Jiang, S. Wang, 和 C. Zhang，“强化自注意力网络：一种硬性和软性注意力的混合序列建模方法”，在*IJCAI*，2018年，页码4345–4352。'
- en: '[64] N. Karianakis, Z. Liu, Y. Chen, and S. Soatto, “Reinforced temporal attention
    and split-rate transfer for depth-based person re-identification,” in *ECCV*,
    2018, pp. 715–733.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] N. Karianakis, Z. Liu, Y. Chen, 和 S. Soatto，“用于深度基础人员再识别的强化时间注意力和分率传输”，在*ECCV*，2018年，页码715–733。'
- en: '[65] G. Chen, C. Lin, L. Ren, J. Lu, and J. Zhou, “Self-critical attention
    learning for person re-identification,” in *ICCV*, 2019.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] G. Chen, C. Lin, L. Ren, J. Lu, 和 J. Zhou，“用于人员再识别的自我批判性注意力学习”，在*ICCV*，2019年。'
- en: '[66] C. Niu, J. Zhang, G. Wang, and J. Liang, “Gatcluster: Self-supervised
    gaussian-attention network for image clustering,” in *ECCV*, 2020.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] C. Niu, J. Zhang, G. Wang, 和 J. Liang，“Gatcluster：用于图像聚类的自监督高斯注意力网络”，在*ECCV*，2020年。'
- en: '[67] J. Heo, H. B. Lee, S. Kim, J. Lee, K. J. Kim, E. Yang, and S. J. Hwang,
    “Uncertainty-aware attention for reliable interpretation and prediction,” in *NeurIPS*,
    2018.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] J. Heo, H. B. Lee, S. Kim, J. Lee, K. J. Kim, E. Yang, 和 S. J. Hwang,
    “面向不确定性的注意力机制用于可靠的解释和预测，”发表于*NeurIPS*，2018年。'
- en: '[68] X. Li, Z. Zhong, J. Wu, Y. Yang, Z. Lin, and H. Liu, “Expectation-maximization
    attention networks for semantic segmentation,” in *ICCV*, 2019.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] X. Li, Z. Zhong, J. Wu, Y. Yang, Z. Lin, 和 H. Liu, “期望最大化注意力网络用于语义分割，”发表于*ICCV*，2019年。'
- en: '[69] H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean, “Efficient neural architecture
    search via parameters sharing,” in *ICML*, 2018.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] H. Pham, M. Guan, B. Zoph, Q. Le, 和 J. Dean, “通过参数共享进行高效神经架构搜索，”发表于*ICML*，2018年。'
- en: '[70] S. Yang and D. Ramanan, “Multi-scale recognition with dag-cnns,” in *ICCV*,
    2015.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] S. Yang 和 D. Ramanan, “多尺度识别与DAG-CNNs，”发表于*ICCV*，2015年。'
- en: '[71] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *CVPR*,
    2015.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, 和 A. Rabinovich, “通过卷积深入探讨，”发表于*CVPR*，2015年。'
- en: '[72] L. Wang and H. Sahbi, “Directed acyclic graph kernels for action recognition,”
    in *ICCV*, 2013.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] L. Wang 和 H. Sahbi, “用于动作识别的有向无环图核函数，”发表于*ICCV*，2013年。'
- en: '[73] H. Liu, K. Simonyan, and Y. Yang, “Darts: Differentiable architecture
    search,” in *ICLR*, 2018.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] H. Liu, K. Simonyan, 和 Y. Yang, “Darts: 可微分架构搜索，”发表于*ICLR*，2018年。'
- en: '[74] B. Chen, W. Deng, and J. Hu, “Mixed high-order attention network for person
    re-identification,” in *ICCV*, 2019.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] B. Chen, W. Deng, 和 J. Hu, “用于行人重识别的混合高阶注意力网络，”发表于*ICCV*，2019年。'
- en: '[75] Y. Chen, Y. Kalantidis, J. Li, S. Yan, and J. Feng, “A 2-nets: double
    attention networks,” in *NeurIPS*, 2018, pp. 350–359.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Y. Chen, Y. Kalantidis, J. Li, S. Yan, 和 J. Feng, “A 2-nets: 双重注意力网络，”发表于*NeurIPS*，2018年，pp.
    350–359。'
- en: '[76] Z. Qin, P. Zhang, F. Wu, and X. Li, “Fcanet: Frequency channel attention
    networks,” *arXiv:2012.11879*, 2020.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Z. Qin, P. Zhang, F. Wu, 和 X. Li, “Fcanet: 频率通道注意力网络，”*arXiv:2012.11879*，2020年。'
- en: '[77] T.-I. Hsieh, Y.-C. Lo, H.-T. Chen, and T.-L. Liu, “One-shot object detection
    with co-attention and co-excitation,” in *NIPS*, H. Wallach, H. Larochelle, A. Beygelzimer,
    F. d''Alché-Buc, E. Fox, and R. Garnett, Eds., 2019.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] T.-I. Hsieh, Y.-C. Lo, H.-T. Chen, 和 T.-L. Liu, “基于协同注意力和协同激励的一次性目标检测，”发表于*NIPS*，H.
    Wallach, H. Larochelle, A. Beygelzimer, F. d''Alché-Buc, E. Fox, 和 R. Garnett
    编辑，2019年。'
- en: '[78] M. Lin, Q. Chen, and S. Yan, “Network in network,” *arXiv:1312.4400*,
    2013.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] M. Lin, Q. Chen, 和 S. Yan, “网络中的网络，”*arXiv:1312.4400*，2013年。'
- en: '[79] X. Hu, Z. Zhang, Z. Jiang, S. Chaudhuri, Z. Yang, and R. Nevatia, “Span:
    Spatial pyramid attention network for image manipulation localization,” in *ECCV*,
    2020.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] X. Hu, Z. Zhang, Z. Jiang, S. Chaudhuri, Z. Yang, 和 R. Nevatia, “Span:
    用于图像操控定位的空间金字塔注意力网络，”发表于*ECCV*，2020年。'
- en: '[80] F. Visin, K. Kastner, K. Cho, M. Matteucci, A. Courville, and Y. Bengio,
    “Renet: A recurrent neural network based alternative to convolutional networks,”
    *arXiv:1505.00393*, 2015.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] F. Visin, K. Kastner, K. Cho, M. Matteucci, A. Courville, 和 Y. Bengio,
    “Renet: 基于递归神经网络的卷积网络替代方案，”*arXiv:1505.00393*，2015年。'
- en: '[81] D. Shen, Y. Ji, P. Li, Y. Wang, and D. Lin, “Ranet: Region attention network
    for semantic segmentation,” *NIPS*, 2020.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] D. Shen, Y. Ji, P. Li, Y. Wang, 和 D. Lin, “Ranet: 用于语义分割的区域注意力网络，”*NIPS*，2020年。'
- en: '[82] A. Parikh, O. Täckström, D. Das, and J. Uszkoreit, “A decomposable attention
    model for natural language inference,” in *EMNLP*, 2016.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] A. Parikh, O. Täckström, D. Das, 和 J. Uszkoreit, “自然语言推理的可分解注意力模型，”发表于*EMNLP*，2016年。'
- en: '[83] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” in *Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies*, 2019.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova, “Bert: 深度双向变换器的预训练用于语言理解，”发表于*北美计算语言学协会:
    人类语言技术会议*，2019年。'
- en: '[84] A. Vyas, A. Katharopoulos, and F. Fleuret, “Fast transformers with clustered
    attention,” in *NeurIPS*, 2020.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] A. Vyas, A. Katharopoulos, 和 F. Fleuret, “具有聚类注意力的快速变换器，”发表于*NeurIPS*，2020年。'
- en: '[85] Z. Shen, M. Zhang, H. Zhao, S. Yi, and H. Li, “Efficient attention: Attention
    with linear complexities,” in *WACV*, 2021.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Z. Shen, M. Zhang, H. Zhao, S. Yi, 和 H. Li, “高效注意力: 具有线性复杂度的注意力，”发表于*WACV*，2021年。'
- en: '[86] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold,
    J. Uszkoreit, A. Dosovitskiy, and T. Kipf, “Object-centric learning with slot
    attention,” *arXiv:2006.15055*, 2020.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold,
    J. Uszkoreit, A. Dosovitskiy, 和 T. Kipf, “基于插槽注意力的对象中心学习，”*arXiv:2006.15055*，2020年。'
- en: '[87] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. Smith, and L. Kong, “Random
    feature attention,” in *ICLR*, 2021.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. Smith, 和 L. Kong, “随机特征注意力，”发表于
    *ICLR*，2021年。'
- en: '[88] Y. Pan, T. Yao, Y. Li, and T. Mei, “X-linear attention networks for image
    captioning,” in *CVPR*, 2020.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Y. Pan, T. Yao, Y. Li, 和 T. Mei, “用于图像描述的 X-linear 注意力网络，”发表于 *CVPR*，2020年。'
- en: '[89] H. Wang, Y. Zhu, B. Green, H. Adam, A. Yuille, and L.-C. Chen, “Axial-deeplab:
    Stand-alone axial-attention for panoptic segmentation,” in *ECCV*, 2020.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] H. Wang, Y. Zhu, B. Green, H. Adam, A. Yuille, 和 L.-C. Chen, “Axial-deeplab:
    用于全景分割的独立轴注意力，”发表于 *ECCV*，2020年。'
- en: '[90] L. Li, B. Wang, M. Verma, Y. Nakashima, R. Kawasaki, and H. Nagahara,
    “Scouter: Slot attention-based classifier for explainable image recognition,”
    *arXiv:2009.06138*, 2020.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] L. Li, B. Wang, M. Verma, Y. Nakashima, R. Kawasaki, 和 H. Nagahara, “Scouter:
    基于槽注意力的可解释图像识别分类器，” *arXiv:2009.06138*，2020年。'
- en: '[91] G. Daras, N. Kitaev, A. Odena, and A. G. Dimakis, “Smyrf: Efficient attention
    using asymmetric clustering,” *arXiv:2010.05315*, 2020.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] G. Daras, N. Kitaev, A. Odena, 和 A. G. Dimakis, “Smyrf: 使用不对称聚类的高效注意力，”
    *arXiv:2010.05315*，2020年。'
- en: '[92] A. S. Rawat, J. Chen, X. Y. Felix, A. T. Suresh, and S. Kumar, “Sampled
    softmax with random fourier features.” in *NeurIPS*, 2019.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] A. S. Rawat, J. Chen, X. Y. Felix, A. T. Suresh, 和 S. Kumar, “使用随机傅里叶特征的采样
    softmax。”发表于 *NeurIPS*，2019年。'
- en: '[93] A. Rahimi, B. Recht *et al.*, “Random features for large-scale kernel
    machines.” in *NIPS*, vol. 3, no. 4, 2007.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] A. Rahimi, B. Recht *等*，“用于大规模核机器的随机特征。”发表于 *NIPS*，第3卷，第4期，2007年。'
- en: '[94] J. Yang, V. Sindhwani, H. Avron, and M. Mahoney, “Quasi-monte carlo feature
    maps for shift-invariant kernels,” in *ICML*, 2014.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] J. Yang, V. Sindhwani, H. Avron, 和 M. Mahoney, “用于平移不变核的准蒙特卡罗特征映射，”发表于
    *ICML*，2014年。'
- en: '[95] T. Hofmann, B. Schölkopf, and A. J. Smola, “Kernel methods in machine
    learning,” *The annals of statistics*, 2008.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] T. Hofmann, B. Schölkopf, 和 A. J. Smola, “机器学习中的核方法，” *The annals of statistics*，2008年。'
- en: '[96] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, 1997.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] S. Hochreiter 和 J. Schmidhuber, “长短期记忆，” *Neural computation*，第9卷，第8期，1997年。'
- en: '[97] K. Cho, B. van Merrienboer, Ç. Gülçehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder-decoder for
    statistical machine translation,” in *EMNLP*, 2014.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] K. Cho, B. van Merrienboer, Ç. Gülçehre, D. Bahdanau, F. Bougares, H. Schwenk,
    和 Y. Bengio, “使用 RNN 编码器-解码器学习短语表示用于统计机器翻译，”发表于 *EMNLP*，2014年。'
- en: '[98] J. Schmidhuber, “Learning to control fast-weight memories: An alternative
    to dynamic recurrent networks,” *Neural Computation*, 1992.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] J. Schmidhuber, “学习控制快速权重记忆：动态递归网络的替代方案，” *Neural Computation*，1992年。'
- en: '[99] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*, vol. 521,
    no. 7553, 2015.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Y. LeCun, Y. Bengio, 和 G. Hinton, “深度学习，” *nature*，第521卷，第7553期，2015年。'
- en: '[100] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan,
    F. Viola, T. Green, T. Back, P. Natsev *et al.*, “The kinetics human action video
    dataset,” *arXiv:1705.06950*, 2017.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan,
    F. Viola, T. Green, T. Back, P. Natsev *等*，“The kinetics human action video dataset，”
    *arXiv:1705.06950*，2017年。'
- en: '[101] A. Buades, B. Coll, and J.-M. Morel, “A non-local algorithm for image
    denoising,” in *CVPR*, vol. 2, 2005.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] A. Buades, B. Coll, 和 J.-M. Morel, “一种非局部图像去噪算法，”发表于 *CVPR*，第2卷，2005年。'
- en: '[102] C. Xie, Y. Wu, L. v. d. Maaten, A. L. Yuille, and K. He, “Feature denoising
    for improving adversarial robustness,” in *CVPR*, 2019.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] C. Xie, Y. Wu, L. v. d. Maaten, A. L. Yuille, 和 K. He, “特征去噪以提高对抗性鲁棒性，”发表于
    *CVPR*，2019年。'
- en: '[103] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio,
    “Graph attention networks,” in *ICLR*, 2018\. [Online]. Available: [https://openreview.net/forum?id=rJXMpikCZ](https://openreview.net/forum?id=rJXMpikCZ)'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò, 和 Y. Bengio,
    “图注意力网络，”发表于 *ICLR*，2018年。 [在线]. 可用: [https://openreview.net/forum?id=rJXMpikCZ](https://openreview.net/forum?id=rJXMpikCZ)'
- en: '[104] Y. Tao, Q. Sun, Q. Du, and W. Liu, “Nonlocal neural networks, nonlocal
    diffusion and nonlocal modeling,” in *NeurIPS*, 2018.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Y. Tao, Q. Sun, Q. Du, 和 W. Liu, “非局部神经网络、非局部扩散和非局部建模，”发表于 *NeurIPS*，2018年。'
- en: '[105] N. Liu, N. Zhang, and J. Han, “Learning selective self-mutual attention
    for rgb-d saliency detection,” in *CVPR*, 2020.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] N. Liu, N. Zhang, 和 J. Han, “学习选择性自我互注意力以进行 RGB-D 显著性检测，”发表于 *CVPR*，2020年。'
- en: '[106] Y. Mei, Y. Fan, and Y. Zhou, “Image super-resolution with non-local sparse
    attention,” in *CVPR*, 2021, pp. 3517–3526.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Y. Mei, Y. Fan, 和 Y. Zhou, “具有非局部稀疏注意力的图像超分辨率，”发表于 *CVPR*，2021年，第3517–3526页。'
- en: '[107] A. Gionis, P. Indyk, R. Motwani *et al.*, “Similarity search in high
    dimensions via hashing,” in *Vldb*, vol. 99, no. 6, 1999, pp. 518–529.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] A. Gionis, P. Indyk, R. Motwani *等*，“通过哈希进行高维相似性搜索，”发表于 *Vldb*，第99卷，第6期，1999年，第518–529页。'
- en: '[108] J.-H. Kim, K.-W. On, W. Lim, J. Kim, J.-W. Ha, and B.-T. Zhang, “Hadamard
    product for low-rank bilinear pooling,” in *ICLR*, 2017.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] J.-H. Kim, K.-W. On, W. Lim, J. Kim, J.-W. Ha, 和 B.-T. Zhang, “用于低秩双线性池化的Hadamard积”，发表于*ICLR*，2017年。'
- en: '[109] T.-Y. Lin, A. RoyChowdhury, and S. Maji, “Bilinear cnn models for fine-grained
    visual recognition,” in *ICCV*, 2015.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] T.-Y. Lin, A. RoyChowdhury, 和 S. Maji, “用于细粒度视觉识别的双线性cnn模型”，发表于*ICCV*，2015年。'
- en: '[110] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell, “Compact bilinear pooling,”
    in *CVPR*, 2016.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Y. Gao, O. Beijbom, N. Zhang, 和 T. Darrell, “紧凑双线性池化”，发表于*CVPR*，2016年。'
- en: '[111] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach,
    “Multimodal compact bilinear pooling for visual question answering and visual
    grounding,” in *EMNLP*, 2016.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, 和 M. Rohrbach,
    “用于视觉问答和视觉定位的多模态紧凑双线性池化”，发表于*EMNLP*，2016年。'
- en: '[112] T.-Y. Lin, A. RoyChowdhury, and S. Maji, “Bilinear cnn models for fine-grained
    visual recognition,” in *ICCV*, 2015.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] T.-Y. Lin, A. RoyChowdhury, 和 S. Maji, “用于细粒度视觉识别的双线性cnn模型”，发表于*ICCV*，2015年。'
- en: '[113] C. Yu, X. Zhao, Q. Zheng, P. Zhang, and X. You, “Hierarchical bilinear
    pooling for fine-grained visual recognition,” in *ECCV*, 2018.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] C. Yu, X. Zhao, Q. Zheng, P. Zhang, 和 X. You, “用于细粒度视觉识别的分层双线性池化”，发表于*ECCV*，2018年。'
- en: '[114] S. Kong and C. Fowlkes, “Low-rank bilinear pooling for fine-grained classification,”
    in *CVPR*, 2017.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] S. Kong 和 C. Fowlkes, “用于细粒度分类的低秩双线性池化”，发表于*CVPR*，2017年。'
- en: '[115] Z. Yu, J. Yu, J. Fan, and D. Tao, “Multi-modal factorized bilinear pooling
    with co-attention learning for visual question answering,” in *ICCV*, 2017.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Z. Yu, J. Yu, J. Fan, 和 D. Tao, “具有协同注意力学习的多模态分解双线性池化用于视觉问答”，发表于*ICCV*，2017年。'
- en: '[116] J. T. Barron, “Continuously differentiable exponential linear units,”
    *arXiv:1704.07483*, 2017.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] J. T. Barron, “连续可微的指数线性单元”，*arXiv:1704.07483*，2017年。'
- en: '[117] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” in *Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies*, 2019.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova, “Bert: 深度双向变换器的预训练用于语言理解”，发表于*Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies*，2019年。'
- en: '[118] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko,
    “End-to-end object detection with transformers,” in *ECCV*, 2020.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, 和 S. Zagoruyko,
    “基于变换器的端到端目标检测”，发表于*ECCV*，2020年。'
- en: '[119] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, “Attention augmented
    convolutional networks,” in *ICCV*, 2019.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] I. Bello, B. Zoph, A. Vaswani, J. Shlens, 和 Q. V. Le, “注意力增强卷积网络”，发表于*ICCV*，2019年。'
- en: '[120] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and J. Shlens,
    “Stand-alone self-attention in vision models,” *arXiv:1906.05909*, 2019.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, 和 J. Shlens,
    “视觉模型中的独立自注意力”，*arXiv:1906.05909*，2019年。'
- en: '[121] H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for image
    recognition,” in *ICCV*, 2019.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] H. Hu, Z. Zhang, Z. Xie, 和 S. Lin, “图像识别的局部关系网络”，发表于*ICCV*，2019年。'
- en: '[122] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, “Attention augmented
    convolutional networks,” in *ICCV*, 2019.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] I. Bello, B. Zoph, A. Vaswani, J. Shlens, 和 Q. V. Le, “注意力增强卷积网络”，发表于*ICCV*，2019年。'
- en: '[123] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *CVPR*, 2017.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] G. Huang, Z. Liu, L. Van Der Maaten, 和 K. Q. Weinberger, “密集连接卷积网络”，发表于*CVPR*，2017年。'
- en: '[124] N. Kitaev, L. Kaiser, and A. Levskaya, “Reformer: The efficient transformer,”
    in *ICLR*, 2019.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] N. Kitaev, L. Kaiser, 和 A. Levskaya, “Reformer: 高效的变换器”，发表于*ICLR*，2019年。'
- en: '[125] L. Wu, X. Liu, and Q. Liu, “Centroid transformers: Learning to abstract
    with attention,” *arXiv preprint arXiv:2102.08606*, 2021.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] L. Wu, X. Liu, 和 Q. Liu, “中心变换器: 通过注意力学习抽象”，*arXiv preprint arXiv:2102.08606*，2021年。'
- en: '[126] Y. J. Kim and H. Hassan, “Fastformers: Highly efficient transformer models
    for natural language understanding,” in *Proceedings of SustaiNLP: Workshop on
    Simple and Efficient Natural Language Processing*, 2020.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Y. J. Kim 和 H. Hassan, “Fastformers: 高效的自然语言理解变换器模型”，发表于*Proceedings
    of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing*，2020年。'
- en: '[127] M. Pandey and S. Lazebnik, “Scene recognition and weakly supervised object
    localization with deformable part-based models,” in *ICCV*, 2011, pp. 1307–1314.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] M. Pandey 和 S. Lazebnik, “基于可变形部件模型的场景识别和弱监督目标定位”，发表于*ICCV*，2011年，页码1307–1314。'
- en: '[128] R. Gokberk Cinbis, J. Verbeek, and C. Schmid, “Multi-fold mil training
    for weakly supervised object localization,” in *CVPR*, 2014, pp. 2409–2416.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] R. Gokberk Cinbis, J. Verbeek 和 C. Schmid，“多折MIL训练用于弱监督目标定位”，发表于 *CVPR*，2014，页码
    2409–2416。'
- en: '[129] J. Choe and H. Shim, “Attention-based dropout layer for weakly supervised
    object localization,” in *CVPR*, 2019, pp. 2219–2228.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] J. Choe 和 H. Shim，“基于注意力的 dropout 层用于弱监督目标定位”，发表于 *CVPR*，2019，页码 2219–2228。'
- en: '[130] Q. Huang, C. Wu, C. Xia, Y. Wang, and C. J. Kuo, “Semantic segmentation
    with reverse attention,” in *BMVC*, 2017.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Q. Huang, C. Wu, C. Xia, Y. Wang 和 C. J. Kuo，“具有逆向注意力的语义分割”，发表于 *BMVC*，2017。'
- en: '[131] D. Lin, Y. Ji, D. Lischinski, D. Cohen-Or, and H. Huang, “Multi-scale
    context intertwining for semantic segmentation,” in *ECCV*, 2018, pp. 603–619.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] D. Lin, Y. Ji, D. Lischinski, D. Cohen-Or 和 H. Huang，“用于语义分割的多尺度上下文交织”，发表于
    *ECCV*，2018，页码 603–619。'
- en: '[132] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and A. Agrawal,
    “Context encoding for semantic segmentation,” in *CVPR*, 2018, pp. 7151–7160.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi 和 A. Agrawal，“用于语义分割的上下文编码”，发表于
    *CVPR*，2018，页码 7151–7160。'
- en: '[133] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales,
    “Learning to compare: Relation network for few-shot learning,” in *CVPR*, 2018.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr 和 T. M. Hospedales，“学习比较：用于少样本学习的关系网络”，发表于
    *CVPR*，2018。'
- en: '[134] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, “Ccnet: Criss-cross
    attention for semantic segmentation,” in *ICCV*, 2019, pp. 603–612.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei 和 W. Liu，“CCNet：用于语义分割的交叉注意力”，发表于
    *ICCV*，2019，页码 603–612。'
- en: '[135] X. Chen, X.-T. Yuan, Q. Chen, S. Yan, and T.-S. Chua, “Multi-label visual
    classification with label exclusive context,” in *ICCV*, 2011.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] X. Chen, X.-T. Yuan, Q. Chen, S. Yan 和 T.-S. Chua，“具有标签独占上下文的多标签视觉分类”，发表于
    *ICCV*，2011。'
- en: '[136] M. Hassanin, I. Radwan, N. Moustafa, M. Tahtali, and N. Kumar, “Mitigating
    the impact of adversarial attacks in very deep networks,” *Applied Soft Computing*,
    2021.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] M. Hassanin, I. Radwan, N. Moustafa, M. Tahtali 和 N. Kumar，“减轻对非常深网络的对抗攻击影响”，*应用软计算*，2021。'
- en: '[137] Y. Luo, Y. Wen, D. Tao, J. Gui, and C. Xu, “Large margin multi-modal
    multi-task feature extraction for image classification,” *TIP*, 2015.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Y. Luo, Y. Wen, D. Tao, J. Gui 和 C. Xu，“用于图像分类的大边距多模态多任务特征提取”，*TIP*，2015。'
- en: '[138] W. Xu, W. Liu, X. Huang, J. Yang, and S. Qiu, “Multi-modal self-paced
    learning for image classification,” *Neurocomputing*, vol. 309, 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] W. Xu, W. Liu, X. Huang, J. Yang 和 S. Qiu，“用于图像分类的多模态自适应学习”，*神经计算*，第
    309 卷，2018。'
- en: '[139] E. Alberts, G. Tetteh, S. Trebeschi, M. Bieth, A. Valentinitsch, B. Wiestler,
    C. Zimmer, and B. H. Menze, “Multi-modal image classification using low-dimensional
    texture features for genomic brain tumor recognition,” in *Graphs in Biomedical
    Image Analysis, Computational Anatomy and Imaging Genetics*, 2017.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] E. Alberts, G. Tetteh, S. Trebeschi, M. Bieth, A. Valentinitsch, B. Wiestler,
    C. Zimmer 和 B. H. Menze，“使用低维纹理特征的多模态图像分类用于基因组脑肿瘤识别”，发表于 *生物医学图像分析中的图形、计算解剖学与成像遗传学*，2017。'
- en: '[140] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,”
    in *ECCV*, 2020.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi
    和 R. Ng，“NeRF：将场景表示为神经辐射场以进行视图合成”，发表于 *ECCV*，2020。'
- en: '[141] E. R. Kandel, J. H. Schwartz, T. M. Jessell, S. Siegelbaum, A. J. Hudspeth,
    and S. Mack, *Principles of neural science*.   McGraw-hill New York, 2000.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] E. R. Kandel, J. H. Schwartz, T. M. Jessell, S. Siegelbaum, A. J. Hudspeth
    和 S. Mack，*神经科学原理*。 McGraw-hill 纽约，2000。'
- en: '[142] K. O. Stanley, “Compositional pattern producing networks: A novel abstraction
    of development,” *Genetic programming and evolvable machines*, vol. 8, no. 2,
    2007.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] K. O. Stanley，“组成模式生成网络：一种发展抽象的新方法”，*遗传编程与可演化机器*，第 8 卷，第 2 期，2007。'
- en: '[143] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and
    D. Tran, “Image transformer,” in *ICML*, 2018.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku 和 D.
    Tran，“图像变换器”，发表于 *ICML*，2018。'
- en: '[144] A. Karpathy, A. Joulin, and L. Fei-Fei, “Deep fragment embeddings for
    bidirectional image sentence mapping,” *arXiv:1406.5679*, 2014.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] A. Karpathy, A. Joulin 和 L. Fei-Fei，“用于双向图像句子映射的深度片段嵌入”，*arXiv:1406.5679*，2014。'
- en: '[145] F. Wang, X. Xiang, J. Cheng, and A. L. Yuille, “Normface: L2 hypersphere
    embedding for face verification,” in *International conference on Multimedia*,
    2017, pp. 1041–1049.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] F. Wang, X. Xiang, J. Cheng 和 A. L. Yuille，“NormFace：用于人脸验证的 L2 超球面嵌入”，发表于
    *国际多媒体会议*，2017，页码 1041–1049。'
- en: '[146] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, “Attention-based
    models for speech recognition,” in *NeurIPS*, 2015, pp. 577–585.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, 和 Y. Bengio，“基于注意力的语音识别模型”，发表在*NeurIPS*，2015年，第577–585页。'
- en: '[147] X. He, L. Deng, and W. Chou, “Discriminative learning in sequential pattern
    recognition,” *Signal Processing Magazine*, pp. 14–36, 2008.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] X. He, L. Deng, 和 W. Chou，“序列模式识别中的判别学习”，*信号处理杂志*，2008年，第14–36页。'
- en: '[148] Y. Huang, Q. Wu, C. Song, and L. Wang, “Learning semantic concepts and
    order for image and sentence matching,” in *CVPR*, 2018.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Y. Huang, Q. Wu, C. Song, 和 L. Wang，“学习图像和句子匹配的语义概念和顺序”，发表在*CVPR*，2018年。'
- en: '[149] J. Lu, C. Xiong, D. Parikh, and R. Socher, “Knowing when to look: Adaptive
    attention via a visual sentinel for image captioning,” in *CVPR*, 2017.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] J. Lu, C. Xiong, D. Parikh, 和 R. Socher，“知道何时查看：通过视觉哨兵的自适应注意力用于图像描述”，发表在*CVPR*，2017年。'
- en: '[150] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and T.-S. Chua,
    “Sca-cnn: Spatial and channel-wise attention in convolutional networks for image
    captioning,” in *CVPR*, 2017, pp. 5659–5667.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, 和 T.-S. Chua，“Sca-cnn:
    卷积网络中用于图像描述的空间和通道注意力”，发表在*CVPR*，2017年，第5659–5667页。'
- en: '[151] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked attention networks
    for image question answering,” in *CVPR*, 2016, pp. 21–29.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Z. Yang, X. He, J. Gao, L. Deng, 和 A. Smola，“用于图像问答的堆叠注意力网络”，发表在*CVPR*，2016年，第21–29页。'
- en: '[152] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang,
    “Bottom-up and top-down attention for image captioning and visual question answering,”
    in *CVPR*, 2018, pp. 6077–6086.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, 和 L.
    Zhang，“图像描述和视觉问答的自下而上和自上而下注意力”，发表在*CVPR*，2018年，第6077–6086页。'
- en: '[153] D.-K. Nguyen and T. Okatani, “Improved fusion of visual and language
    representations by dense symmetric co-attention for visual question answering,”
    in *CVPR*, 2018.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] D.-K. Nguyen 和 T. Okatani，“通过密集对称共同注意力改进视觉和语言表示的融合，用于视觉问答”，发表在*CVPR*，2018年。'
- en: '[154] J. Lu, C. Xiong, D. Parikh, and R. Socher, “Knowing when to look: Adaptive
    attention via a visual sentinel for image captioning,” in *CVPR*, 2017, pp. 375–383.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] J. Lu, C. Xiong, D. Parikh, 和 R. Socher，“知道何时查看：通过视觉哨兵的自适应注意力用于图像描述”，发表在*CVPR*，2017年，第375–383页。'
- en: '[155] H. R. Tavakoli, R. Shetty, A. Borji, and J. Laaksonen, “Paying attention
    to descriptions generated by image captioning models,” in *ICCV*, 2017, pp. 2487–2496.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] H. R. Tavakoli, R. Shetty, A. Borji, 和 J. Laaksonen，“关注图像描述生成模型生成的描述”，发表在*ICCV*，2017年，第2487–2496页。'
- en: '[156] Y. Sugano and A. Bulling, “Seeing with humans: Gaze-assisted neural image
    captioning,” *arXiv:1608.05203*, 2016.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Y. Sugano 和 A. Bulling，“与人类一起看：注视辅助的神经图像描述”，*arXiv:1608.05203*，2016年。'
- en: '[157] A. Mott, D. Zoran, M. Chrzanowski, D. Wierstra, and D. J. Rezende, “Towards
    interpretable reinforcement learning using attention augmented agents,” *arXiv:1906.02500*,
    2019.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] A. Mott, D. Zoran, M. Chrzanowski, D. Wierstra, 和 D. J. Rezende，“利用注意力增强代理朝向可解释的强化学习”，*arXiv:1906.02500*，2019年。'
- en: '[158] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
    deep learning models resistant to adversarial attacks,” in *ICLR*, 2018.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, 和 A. Vladu，“朝向对抗攻击有抵抗力的深度学习模型”，发表在*ICLR*，2018年。'
- en: '[159] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song, “Natural
    adversarial examples,” *arXiv:1907.07174*, 2019.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, 和 D. Song，“自然对抗样本”，*arXiv:1907.07174*，2019年。'
- en: '[160] M. Zaheer, S. Kottur, S. Ravanbhakhsh, B. Póczos, R. Salakhutdinov, and
    A. J. Smola, “Deep sets,” in *NeurIPS*, 2017.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] M. Zaheer, S. Kottur, S. Ravanbhakhsh, B. Póczos, R. Salakhutdinov, 和
    A. J. Smola，“深度集合”，发表在*NeurIPS*，2017年。'
- en: '[161] W. Zaremba, I. Sutskever, and O. Vinyals, “Recurrent neural network regularization,”
    *arXiv:1409.2329*, 2014.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] W. Zaremba, I. Sutskever, 和 O. Vinyals，“递归神经网络正则化”，*arXiv:1409.2329*，2014年。'
- en: '[162] V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu, “Recurrent models of
    visual attention,” in *NeurIPS*, 2014, pp. 2204–2212.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] V. Mnih, N. Heess, A. Graves, 和 K. Kavukcuoglu， “视觉注意力的递归模型”，发表在*NeurIPS*，2014年，第2204–2212页。'
- en: '[163] J. Ba, V. Mnih, and K. Kavukcuoglu, “Multiple object recognition with
    visual attention,” in *ICLR (Poster)*, 2015.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] J. Ba, V. Mnih, 和 K. Kavukcuoglu，“带有视觉注意力的多对象识别”，发表在*ICLR（海报）*，2015年。'
- en: '[164] H. Liu, J. LU, X. Zhao, S. Xu, H. Peng, Y. Liu, Z. Zhang, J. Li, J. Jin,
    Y. Bao, and W. Yan, “Kalman filtering attention for user behavior modeling in
    ctr prediction,” in *NIPS*, 2020.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] H. Liu, J. LU, X. Zhao, S. Xu, H. Peng, Y. Liu, Z. Zhang, J. Li, J. Jin,
    Y. Bao, 和 W. Yan，“卡尔曼滤波注意力在CTR预测中的用户行为建模”，发表在*NIPS*，2020年。'
- en: '[165] F. Liu, X. Ren, X. Wu, S. Ge, W. Fan, Y. Zou, and X. Sun, “Prophet attention:
    Predicting attention with future attention,” *NIPS*, vol. 33, 2020.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] F. Liu, X. Ren, X. Wu, S. Ge, W. Fan, Y. Zou 和 X. Sun, “先知注意力：用未来注意力预测注意力”，*NIPS*，第33卷，2020年。'
- en: '[166] J.-Y. Pan, H.-J. Yang, P. Duygulu, and C. Faloutsos, “Automatic image
    captioning,” in *ICME*, 2004.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] J.-Y. Pan, H.-J. Yang, P. Duygulu 和 C. Faloutsos, “自动图像描述”，见 *ICME*，2004年。'
- en: '[167] M. Z. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga, “A comprehensive
    survey of deep learning for image captioning,” *ACM Computing Surveys*, 2019.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] M. Z. Hossain, F. Sohel, M. F. Shiratuddin 和 H. Laga, “深度学习在图像描述中的全面调查”，*ACM
    Computing Surveys*，2019年。'
- en: '[168] C. Deng, Q. Wu, Q. Wu, F. Hu, F. Lyu, and M. Tan, “Visual grounding via
    accumulated attention,” in *CVPR*, 2018.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] C. Deng, Q. Wu, Q. Wu, F. Hu, F. Lyu 和 M. Tan, “通过累积注意力进行视觉定位”，见 *CVPR*，2018年。'
- en: '[169] G. A. Sigurdsson, J.-B. Alayrac, A. Nematzadeh, L. Smaira, M. Malinowski,
    J. Carreira, P. Blunsom, and A. Zisserman, “Visual grounding in video for unsupervised
    word translation,” in *CVPR*, 2020.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] G. A. Sigurdsson, J.-B. Alayrac, A. Nematzadeh, L. Smaira, M. Malinowski,
    J. Carreira, P. Blunsom 和 A. Zisserman, “视频中的视觉定位用于无监督词翻译”，见 *CVPR*，2020年。'
- en: '[170] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning
    deep features for discriminative localization,” in *CVPR*, 2016.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva 和 A. Torralba, “学习用于区分性定位的深度特征”，见
    *CVPR*，2016年。'
- en: '[171] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra,
    “Grad-cam: Visual explanations from deep networks via gradient-based localization,”
    in *ICCV*, 2017.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh 和 D. Batra,
    “Grad-cam：通过基于梯度的定位从深度网络中获取视觉解释”，见 *ICCV*，2017年。'
- en: '[172] F. Zhang, Y. Chen, Z. Li, Z. Hong, J. Liu, F. Ma, J. Han, and E. Ding,
    “Acfnet: Attentional class feature network for semantic segmentation,” in *ICCV*,
    2019, pp. 6798–6807.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] F. Zhang, Y. Chen, Z. Li, Z. Hong, J. Liu, F. Ma, J. Han 和 E. Ding, “Acfnet：用于语义分割的注意力类特征网络”，见
    *ICCV*，2019年，第6798–6807页。'
- en: '[173] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous
    convolution for semantic image segmentation,” *arXiv preprint arXiv:1706.05587*,
    2017.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] L.-C. Chen, G. Papandreou, F. Schroff 和 H. Adam, “重新思考用于语义图像分割的空洞卷积”，*arXiv
    预印本 arXiv:1706.05587*，2017年。'
- en: '[174] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” *Machine learning*, pp. 229–256, 1992.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] R. J. Williams, “用于连接主义强化学习的简单统计梯度跟随算法”，*Machine learning*，第229–256页，1992年。'
- en: '[175] S. Zhang, X. Fan, B. Chen, and M. Zhou, “Bayesian attention belief networks,”
    *arXiv preprint arXiv:2106.05251*, 2021.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] S. Zhang, X. Fan, B. Chen 和 M. Zhou, “贝叶斯注意力信念网络”，*arXiv 预印本 arXiv:2106.05251*，2021年。'
- en: '[176] A. Prakash, J. Storer, D. Florencio, and C. Zhang, “Repr: Improved training
    of convolutional filters,” in *CVPR*, 2019.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] A. Prakash, J. Storer, D. Florencio 和 C. Zhang, “Repr：改进卷积滤波器的训练”，见 *CVPR*，2019年。'
- en: '[177] S. Han, J. Pool, S. Narang, H. Mao, E. Gong, S. Tang, E. Elsen, P. Vajda,
    M. Paluri, J. Tran *et al.*, “Dsd: Dense-sparse-dense training for deep neural
    networks,” *arXiv:1607.04381*, 2016.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] S. Han, J. Pool, S. Narang, H. Mao, E. Gong, S. Tang, E. Elsen, P. Vajda,
    M. Paluri, J. Tran *等*，“Dsd：深度神经网络的稠密-稀疏-稠密训练”，*arXiv:1607.04381*，2016年。'
- en: '[178] Q. Liu and D. Wang, “Stein variational gradient descent: a general purpose
    bayesian inference algorithm,” in *30th NeurIPS*, 2016.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Q. Liu 和 D. Wang, “Stein 变分梯度下降：一种通用贝叶斯推断算法”，见 *第30届 NeurIPS*，2016年。'
- en: '[179] H. Salimbeni, V. Dutordoir, J. Hensman, and M. Deisenroth, “Deep gaussian
    processes with importance-weighted variational inference,” in *ICML*, 2019.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] H. Salimbeni, V. Dutordoir, J. Hensman 和 M. Deisenroth, “通过重要性加权变分推断的深度高斯过程”，见
    *ICML*，2019年。'
- en: '[180] I. Drori, “Deep variational inference,” in *Handbook of Variational Methods
    for Nonlinear Geometric Data*, 2020.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] I. Drori, “深度变分推断”，见 *非线性几何数据的变分方法手册*，2020年。'
- en: '[181] J. W.-B. Lin and J. D. Neelin, “Toward stochastic deep convective parameterization
    in general circulation models,” *Geophysical research letters*, vol. 30, no. 4,
    2003.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] J. W.-B. Lin 和 J. D. Neelin, “面向一般环流模型的随机深层对流参数化”，*Geophysical research
    letters*，第30卷，第4期，2003年。'
- en: '[182] H. Wang and D.-Y. Yeung, “A survey on bayesian deep learning,” *ACM Computing
    Surveys*, vol. 53, no. 5, 2020.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] H. Wang 和 D.-Y. Yeung, “关于贝叶斯深度学习的调查”，*ACM Computing Surveys*，第53卷，第5期，2020年。'
- en: '[183] M. Jankowiak and F. Obermeyer, “Pathwise derivatives beyond the reparameterization
    trick,” in *ICML*, 2018.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] M. Jankowiak 和 F. Obermeyer, “超越重参数化技巧的路径导数”，见 *ICML*，2018年。'
- en: '[184] H. Bahuleyan, L. Mou, O. Vechtomova, and P. Poupart, “Variational attention
    for sequence-to-sequence models,” in *COLING*, 2018.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] H. Bahuleyan, L. Mou, O. Vechtomova 和 P. Poupart, “用于序列到序列模型的变分注意力”，见
    *COLING*，2018年。'
- en: '[185] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, “Deeply-supervised
    nets,” in *Artificial intelligence and statistics*, 2015.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang 和 Z. Tu，"深度监督网络"，在 *人工智能与统计*，2015年。'
- en: '[186] T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan, and C. Zhang, “Disan: Directional
    self-attention network for rnn/cnn-free language understanding,” in *AAAI Conference
    on Artificial Intelligence*, 2018.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan 和 C. Zhang，"Disan: 用于 RNN/CNN
    自注意力网络的方向性自注意力网络"，在 *AAAI 人工智能大会*，2018年。'
- en: '[187] C. Niu, J. Zhang, G. Wang, and J. Liang, “Gatcluster: Self-supervised
    gaussian-attention network for image clustering,” in *ECCV*.   Springer, 2020.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] C. Niu, J. Zhang, G. Wang 和 J. Liang，"Gatcluster: 自监督高斯注意力网络用于图像聚类"，在
    *ECCV*，Springer，2020年。'
- en: '[188] G. Heigold, E. McDermott, V. Vanhoucke, A. Senior, and M. Bacchiani,
    “Asynchronous stochastic optimization for sequence training of deep neural networks,”
    in *ICASSP*, 2014.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] G. Heigold, E. McDermott, V. Vanhoucke, A. Senior 和 M. Bacchiani，"深度神经网络序列训练的异步随机优化"，在
    *ICASSP*，2014年。'
- en: '[189] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, “Deep networks
    with stochastic depth,” in *ECCV*, 2016.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] G. Huang, Y. Sun, Z. Liu, D. Sedra 和 K. Q. Weinberger，"具有随机深度的深度网络"，在
    *ECCV*，2016年。'
- en: '[190] G. Heigold, E. McDermott, V. Vanhoucke, A. Senior, and M. Bacchiani,
    “Asynchronous stochastic optimization for sequence training of deep neural networks,”
    in *ICASSP*, 2014.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] G. Heigold, E. McDermott, V. Vanhoucke, A. Senior 和 M. Bacchiani，"深度神经网络序列训练的异步随机优化"，在
    *ICASSP*，2014年。'
- en: '[191] A. Kendall and Y. Gal, “What uncertainties do we need in bayesian deep
    learning for computer vision?” in *NeurIPS*, 2017.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] A. Kendall 和 Y. Gal，"我们在计算机视觉中的贝叶斯深度学习中需要哪些不确定性？"，在 *NeurIPS*，2017年。'
- en: '[192] N. L. Zhang and D. Poole, “A simple approach to bayesian network computations,”
    in *Canadian Conference on AI*, 1994.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] N. L. Zhang 和 D. Poole，"贝叶斯网络计算的简单方法"，在 *加拿大人工智能大会*，1994年。'
- en: '[193] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational inference:
    A review for statisticians,” *Journal of the American statistical Association*,
    2017.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] D. M. Blei, A. Kucukelbir 和 J. D. McAuliffe，"变分推断：统计学家的回顾"，*美国统计协会杂志*，2017年。'
- en: '[194] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” *arXiv:1312.6114*,
    2013.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] D. P. Kingma 和 M. Welling，"自编码变分贝叶斯"，*arXiv:1312.6114*，2013年。'
- en: '[195] Y. Gal, J. Hron, and A. Kendall, “Concrete dropout,” in *NeurIPS*, 2017.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Y. Gal, J. Hron 和 A. Kendall，"Concrete dropout"，在 *NeurIPS*，2017年。'
- en: '[196] D. P. Kingma, T. Salimans, and M. Welling, “Variational dropout and the
    local reparameterization trick,” in *NeurIPS*, 2015.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] D. P. Kingma, T. Salimans 和 M. Welling，"变分 dropout 和局部重参数化技巧"，在 *NeurIPS*，2015年。'
- en: '[197] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood from
    incomplete data via the em algorithm,” *Journal of the Royal Statistical Society:
    Series B (Methodological)*, 1977.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] A. P. Dempster, N. M. Laird 和 D. B. Rubin，"通过 EM 算法从不完整数据中最大似然估计"，*皇家统计学会杂志：B
    系列（方法学）*，1977年。'
- en: '[198] K. Han, Y. Wang, Q. Tian, J. Guo, C. Xu, and C. Xu, “Ghostnet: More features
    from cheap operations,” in *CVPR*, 2020.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] K. Han, Y. Wang, Q. Tian, J. Guo, C. Xu 和 C. Xu，"Ghostnet: 从廉价操作中获取更多特征"，在
    *CVPR*，2020年。'
- en: '[199] S. Anwar and N. Barnes, “Real image denoising with feature attention,”
    in *ICCV*, 2019.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] S. Anwar 和 N. Barnes，"特征注意力下的真实图像去噪"，在 *ICCV*，2019年。'
