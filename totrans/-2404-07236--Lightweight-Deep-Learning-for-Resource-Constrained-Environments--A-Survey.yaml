- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:33:22'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:33:22'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2404.07236] Lightweight Deep Learning for Resource-Constrained Environments:
    A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2404.07236] 资源受限环境下的轻量级深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.07236](https://ar5iv.labs.arxiv.org/html/2404.07236)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.07236](https://ar5iv.labs.arxiv.org/html/2404.07236)
- en: 'Lightweight Deep Learning for Resource-Constrained Environments: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源受限环境下的轻量级深度学习：综述
- en: Hou-I Liu [k39967.c@nycu.edu.tw](mailto:k39967.c@nycu.edu.tw) [0000-0002-2101-2997](https://orcid.org/0000-0002-2101-2997
    "ORCID identifier") Department of Electronics and Electrical Engineering, National
    Yang Ming Chiao Tung UniversityHsinchuTaiwan, ROC300 ,  Marco Galindo [marcodavidg@gmail.com](mailto:marcodavidg@gmail.com)
    Department of Electrical Engineering and Computer Science, National Yang Ming
    Chiao Tung UniversityHsinchuTaiwan, ROC300 ,  Hongxia Xie College of Computer
    Science and Technology, Jilin UniversityKey Laboratory of Symbolic Computation
    and Knowledge Engineering of Ministry of Education, Jilin UniversityChangchunChina130000
    [hongxiaxie.ee08@nycu.edu.tw](mailto:hongxiaxie.ee08@nycu.edu.tw) ,  Lai-Kuan
    Wong [lkwong@mmu.edu.my](mailto:lkwong@mmu.edu.my) Faculty of Computing and Informatics,
    Multimedia UniversityCyberjayaMalaysia63100 ,  Hong-Han Shuai Department of Electronics
    and Electrical Engineering, National Yang Ming Chiao Tung UniversityHsinchuTaiwan,
    ROC300 [hhshuai@nycu.edu.tw](mailto:hhshuai@nycu.edu.tw) ,  Yung-Hui Li Hon Hai
    Research InstituteTaipeiTaiwan, ROC114 [yunghui.li@foxconn.com](mailto:yunghui.li@foxconn.com)
     and  Wen-Huang Cheng Department of Computer Science and Information Engineering,
    National Taiwan UniversityTaipeiTaiwan, ROC106 [wenhuang@csie.ntu.edu.tw](mailto:wenhuang@csie.ntu.edu.tw)(2022)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Hou-I Liu [k39967.c@nycu.edu.tw](mailto:k39967.c@nycu.edu.tw) [0000-0002-2101-2997](https://orcid.org/0000-0002-2101-2997
    "ORCID identifier") 国立阳明交通大学电子与电气工程系，新竹，台湾，ROC300，Marco Galindo [marcodavidg@gmail.com](mailto:marcodavidg@gmail.com)
    国立阳明交通大学电气工程与计算机科学系，新竹，台湾，ROC300，Hongxia Xie 吉林大学计算机科学与技术学院，教育部符号计算与知识工程重点实验室，吉林大学，长春，中国，130000
    [hongxiaxie.ee08@nycu.edu.tw](mailto:hongxiaxie.ee08@nycu.edu.tw)，Lai-Kuan Wong
    [lkwong@mmu.edu.my](mailto:lkwong@mmu.edu.my) 多媒体大学计算与信息学院，赛城，马来西亚，63100，Hong-Han
    Shuai 国立阳明交通大学电子与电气工程系，新竹，台湾，ROC300 [hhshuai@nycu.edu.tw](mailto:hhshuai@nycu.edu.tw)，Yung-Hui
    Li 鸿海研究院，台北，台湾，ROC114 [yunghui.li@foxconn.com](mailto:yunghui.li@foxconn.com)
    和 Wen-Huang Cheng 国立台湾大学计算机科学与信息工程系，台北，台湾，ROC106 [wenhuang@csie.ntu.edu.tw](mailto:wenhuang@csie.ntu.edu.tw)(2022)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Over the past decade, the dominance of deep learning has prevailed across various
    domains of artificial intelligence, including natural language processing, computer
    vision, and biomedical signal processing. While there have been remarkable improvements
    in model accuracy, deploying these models on lightweight devices, such as mobile
    phones and microcontrollers, is constrained by limited resources. In this survey,
    we provide comprehensive design guidance tailored for these devices, detailing
    the meticulous design of lightweight models, compression methods, and hardware
    acceleration strategies. The principal goal of this work is to explore methods
    and concepts for getting around hardware constraints without compromising the
    model’s accuracy. Additionally, we explore two notable paths for lightweight deep
    learning in the future: deployment techniques for TinyML and Large Language Models.
    Although these paths undoubtedly have potential, they also present significant
    challenges, encouraging research into unexplored areas.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年里，深度学习在自然语言处理、计算机视觉和生物医学信号处理等人工智能各个领域占据了主导地位。尽管模型的准确性取得了显著提高，但在轻量级设备上部署这些模型，如手机和微控制器，由于资源有限而受到限制。在本综述中，我们提供了针对这些设备的全面设计指导，详细介绍了轻量级模型的精细设计、压缩方法和硬件加速策略。本研究的主要目标是探索在不妥协模型准确性的情况下绕过硬件限制的方法和概念。此外，我们还探讨了未来轻量级深度学习的两个重要方向：TinyML和大语言模型的部署技术。尽管这些方向无疑具有潜力，但也面临着重大挑战，鼓励对未探索领域进行研究。
- en: 'Lightweight model, efficient transformer, model compression, quantization,
    tinyML, large language models^†^†copyright: acmcopyright^†^†journalyear: 2022^†^†doi:
    XXXXXXX.XXXXXXX^†^†booktitle: Woodstock ’18: ACM Symposium on Neural Gaze Detection,
    June 03–05, 2018, Woodstock, NY^†^†price: 15.00^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs:
    Computing methodologies Neural networks^†^†ccs: Computing methodologies Artificial
    intelligence^†^†ccs: Computing methodologies Computer vision^†^†ccs: Computing
    methodologies Model compression^†^†ccs: Computer systems organization Embedded
    systems^†^†ccs: Software and its engineering Designing software^†^†ccs: Software
    and its engineering Software design techniques'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '轻量级模型，高效变换器，模型压缩，量化，tinyML，大型语言模型^†^†版权：acmcopyright^†^†期刊年份：2022^†^†doi: XXXXXXX.XXXXXXX^†^†书名：Woodstock
    ’18: ACM Symposium on Neural Gaze Detection, June 03–05, 2018, Woodstock, NY^†^†价格：15.00^†^†isbn:
    978-1-4503-XXXX-X/18/06^†^†ccs: 计算方法 神经网络^†^†ccs: 计算方法 人工智能^†^†ccs: 计算方法 计算机视觉^†^†ccs:
    计算方法 模型压缩^†^†ccs: 计算机系统组织 嵌入式系统^†^†ccs: 软件及其工程 软件设计^†^†ccs: 软件及其工程 软件设计技术'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Over recent years, the importance of neural networks (NNs) has escalated tremendously,
    with their applications permeating various aspects of daily life and extending
    to support complex tasks (Hidayati et al., [2020](#bib.bib85); Xie et al., [2023](#bib.bib223);
    Chen et al., [2021b](#bib.bib19)). However, since the publication of AlexNet (Krizhevsky
    et al., [2012](#bib.bib111)) in 2012, there has been a prevailing trend toward
    creating deeper and more intricate networks to enhance accuracy. For instance,
    Model Soups (Wortsman et al., [2022](#bib.bib216)) has achieved remarkable accuracy
    on the ImageNet dataset, but it comes at the cost of over 1,843 million parameters.
    Similarly, GPT-4 (Bastian, [2023](#bib.bib11)) has demonstrated outstanding performance
    on natural language processing (NLP) benchmarks, albeit with a staggering 1.76
    trillion parameters. Notably, Amodei et al. (Amodei and Hernandez, [2018](#bib.bib5))
    indicated that the computational demands of deep learning (DL) have surged dramatically,
    increasing by approximately 300,000 times from 2012 to 2018\. This dramatic increase
    in size sets the stage for the challenges and developments explored in this paper.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，神经网络（NNs）的重要性急剧上升，其应用渗透到日常生活的各个方面，并扩展到支持复杂任务中（Hidayati et al., [2020](#bib.bib85)；Xie
    et al., [2023](#bib.bib223)；Chen et al., [2021b](#bib.bib19)）。然而，自2012年AlexNet（Krizhevsky
    et al., [2012](#bib.bib111)）发布以来，创建更深、更复杂的网络以提高准确性已成为一种主流趋势。例如，Model Soups（Wortsman
    et al., [2022](#bib.bib216)）在ImageNet数据集上取得了显著的准确性，但其参数超过18.43亿。同样，GPT-4（Bastian,
    [2023](#bib.bib11)）在自然语言处理（NLP）基准测试中表现出色，尽管其参数高达惊人的1.76万亿。值得注意的是，Amodei等人（Amodei和Hernandez,
    [2018](#bib.bib5)）指出，深度学习（DL）的计算需求急剧增加，从2012年到2018年增长了约30万倍。这种规模的急剧增加为本文探讨的挑战和发展奠定了基础。
- en: Concurrently, Green AI (Schwartz et al., [2020](#bib.bib170); Talwalkar, [2020](#bib.bib189))
    has arisen as a prominent concern over the past few years, labeling hefty DL models
    unsuitable due to their substantial GPU and training time demands, which can contribute
    to environmental degradation. Strubell et al. (Strubell et al., [2019](#bib.bib179))
    extensively analyze the carbon footprint of language models trained on multiple
    GPUs. In parallel, lightweight devices have garnered increased attention due to
    their versatile applications and portability. According to Sinha (Sinha, [2023](#bib.bib175)),
    the number of connected IoT devices grew by 18% in 2022, reaching 14.4 billion,
    and has a projected escalation to 29.0 billion by 2027\. A testament to this growing
    demand is the production of over 200 million iPhones since 2016\. On the other
    hand, edge devices offer superior automation and energy efficiency compared to
    mobile devices, especially the deployment of ultra-low-cost microcontrollers (MCUs)
    in devices like pacemakers and forehead thermometers (Dubey et al., [2019](#bib.bib47)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，绿色人工智能（Schwartz 等人，[2020](#bib.bib170)；Talwalkar，[2020](#bib.bib189)）在过去几年中成为一个突出的关注点，因其重型深度学习模型由于高额的
    GPU 和训练时间需求，被标记为不适用，这可能导致环境恶化。Strubell 等人（Strubell 等人，[2019](#bib.bib179)）对在多
    GPU 上训练的语言模型的碳足迹进行了广泛分析。同时，轻量级设备因其多功能应用和便携性而受到越来越多的关注。根据 Sinha（Sinha，[2023](#bib.bib175)），2022年连接的物联网设备数量增长了18%，达到144亿，并预计到2027年将增加至290亿。自2016年以来生产了超过2亿部
    iPhone，证明了这一日益增长的需求。另一方面，边缘设备相比移动设备提供了更高的自动化和能效，特别是超低成本的微控制器（MCU）在起搏器和额头温度计等设备中的应用（Dubey
    等人，[2019](#bib.bib47)）。
- en: In response to the practical demands outlined above, a significant body of research
    has emerged in recent years, focusing on lightweight modeling, model compression,
    and acceleration techniques. The Annual Mobile AI (MAI) workshops have been held
    consecutively during CVPR 2021-2023 (MAI, [2021](#bib.bib140), [2022](#bib.bib141),
    [2023](#bib.bib142)), with a primary emphasis on the deployment of DL models for
    image processing on resource-constrained devices, such as ARM Mali GPUs and Raspberry
    Pi 4\. Additionally, the Advances in Image Manipulation (AIM) workshops conducted
    at ICCV 2019, ICCV 2021, and ECCV 2022 (AIM, [2022](#bib.bib4)) have organized
    challenges centered around image/video manipulation, restoration, and enhancement
    on mobile devices.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 针对上述实际需求，近年来涌现出大量研究，重点关注轻量级建模、模型压缩和加速技术。年度移动人工智能（MAI）研讨会连续在 CVPR 2021-2023（MAI，[2021](#bib.bib140)，[2022](#bib.bib141)，[2023](#bib.bib142)）期间举行，主要强调在资源受限的设备上部署深度学习模型，如
    ARM Mali GPU 和 Raspberry Pi 4。此外，在 ICCV 2019、ICCV 2021 和 ECCV 2022 举办的图像处理进展（AIM）研讨会（AIM，[2022](#bib.bib4)）围绕移动设备上的图像/视频处理、恢复和增强组织了挑战。
- en: 'From our study, we discovered that the most effective approach for analyzing
    the development of an efficient, lightweight model, spanning from its design phase
    to deployment, involves incorporating three key elements into the pipeline: NN
    architecture design, compression methods, and hardware acceleration for lightweight
    DL models. Previous surveys (Gou et al., [2021](#bib.bib63); Gupta and Agrawal,
    [2022](#bib.bib70); Rokh et al., [2023](#bib.bib166); Berthelier et al., [2021](#bib.bib12);
    Liang et al., [2021b](#bib.bib122)) often focus on specific aspects of this pipeline,
    such as discussing only quantization methods, offering detailed insights into
    those segments. However, these surveys may not provide a comprehensive view of
    the entire process, potentially overlooking significant alternative approaches
    and techniques. In contrast, our survey covers lightweight architectures, compression
    methods, and hardware acceleration algorithms.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的研究中，我们发现分析高效、轻量级模型开发的最有效方法，从设计阶段到部署阶段，需要将三大关键要素融入流程中：神经网络架构设计、压缩方法和轻量级深度学习模型的硬件加速。之前的调查（Gou
    等人，[2021](#bib.bib63)；Gupta 和 Agrawal，[2022](#bib.bib70)；Rokh 等人，[2023](#bib.bib166)；Berthelier
    等人，[2021](#bib.bib12)；Liang 等人，[2021b](#bib.bib122)）通常关注流程中的特定方面，例如仅讨论量化方法，提供对这些细节的深入见解。然而，这些调查可能无法提供对整个过程的全面视角，可能忽略了重要的替代方法和技术。相反，我们的调查涵盖了轻量级架构、压缩方法和硬件加速算法。
- en: 1.1\. Neural Network Design
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1. 神经网络设计
- en: 'In the first part of this article, Section [2](#S2 "2\. Lightweight Architecture
    Design ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey"),
    we examine the classic lightweight architectures, categorizing them into family
    series for improved clarity. Some of these architectures made significant strides
    by introducing innovative convolution blocks. For instance, depthwise separable
    convolutions (Chollet, [2017](#bib.bib36)) prioritize high accuracy and reduced
    computational demand. Sandler et al. (Sandler et al., [2018](#bib.bib169)) introduce
    an inverted residual bottleneck to enhance gradient propagation. Other architectures,
    such as ShuffleNet (Zhang et al., [2018c](#bib.bib249)), were able to develop
    an optimized convolution operation, which applies group convolution (Krizhevsky
    et al., [2012](#bib.bib111)) to achieve a parallel design and further improve
    the transferability between groups of data through shuffle operations. The ShiftNet (Wu
    et al., [2018](#bib.bib218)) achieves an equivalence effect of traditional convolution
    with no parameters or Floating Point Operations (FLOPs). The AdderNet (Chen et al.,
    [2020b](#bib.bib22)) replaces the multiplication operation with the addition operation,
    greatly reducing computation requirements.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的第一部分，即第[2](#S2 "2\. 轻量级架构设计 ‣ 资源受限环境中的轻量级深度学习：一项综述")节中，我们研究了经典的轻量级架构，并将它们分类为系列，以提高清晰度。这些架构中的一些通过引入创新的卷积块取得了显著的进展。例如，深度可分离卷积 (Chollet,
    [2017](#bib.bib36))优先考虑高精度和降低计算需求。Sandler等人 (Sandler et al., [2018](#bib.bib169))引入了倒残差瓶颈，以增强梯度传播。其他架构，如ShuffleNet (Zhang
    et al., [2018c](#bib.bib249))，能够开发出优化的卷积操作，该操作应用了组卷积 (Krizhevsky et al., [2012](#bib.bib111))以实现并行设计，并通过洗牌操作进一步提高数据组之间的可迁移性。ShiftNet (Wu
    et al., [2018](#bib.bib218))在没有参数或浮点运算（FLOPs）的情况下实现了传统卷积的等效效果。AdderNet (Chen et al.,
    [2020b](#bib.bib22))用加法操作替代了乘法操作，大大降低了计算需求。
- en: It is also important to note that parameters and FLOPs do not consistently correlate
    with inference time. Early lightweight architectures, such as SqueezeNet (Iandola
    et al., [2017](#bib.bib99)) and MobileNet (Howard et al., [2017](#bib.bib90)),
    aim to reduce parameters and FLOPs. However, this reduction often increases Memory
    Access Cost (MAC) (Ma et al., [2018](#bib.bib139)), leading to slower inference.
    Hence, we aim to contribute to the application of lightweight models by providing
    a more comprehensive and insightful review.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，参数和FLOPs（浮点运算次数）与推理时间并不总是具有一致的相关性。早期的轻量级架构，如SqueezeNet (Iandola et al.,
    [2017](#bib.bib99))和MobileNet (Howard et al., [2017](#bib.bib90))，旨在减少参数和FLOPs。然而，这种减少往往会增加内存访问成本（MAC） (Ma
    et al., [2018](#bib.bib139))，导致推理速度变慢。因此，我们的目标是通过提供更全面和有洞察力的评估，来推动轻量级模型的应用。
- en: 1.2\. Neural Network Compression
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 神经网络压缩
- en: 'In addition to lightweight architecture designs, Section [3](#S3 "3\. Fundamental
    methods in model compression ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") mentions various efficient algorithms that can be applied
    to compress a given architecture. For example, quantization methods (Yao et al.,
    [2021](#bib.bib231); Liu et al., [2021b](#bib.bib133); Hubara et al., [2016](#bib.bib98))
    aim to reduce the required storage for data, often by substituting 32-bit floating-point
    numbers with 8-bit or 16-bit numbers or even utilizing binary values to represent
    the data. Pruning algorithms (Frankle and Carbin, [2019](#bib.bib55); Guo et al.,
    [2016](#bib.bib68); Lee et al., [2019](#bib.bib115)), in their simplest form,
    remove parameters from a model to eliminate unnecessary redundancies within the
    network. Yet, more sophisticated algorithms may remove entire channels or filters
    from the network (He et al., [2019a](#bib.bib82); Liu et al., [2019a](#bib.bib136)).
    Knowledge distillation (KD) techniques (Hinton et al., [2015](#bib.bib86); Gou
    et al., [2021](#bib.bib63)) explore the concept of transferring knowledge from
    one model, referred to as the ”teacher”, to another, called the ”student”. The
    teacher represents a large pre-trained model with the desired knowledge, whereas
    the student denotes an untrained smaller model tasked with extracting knowledge
    from the teacher. However, as methods evolved, some algorithms (Yuan et al., [2020](#bib.bib240);
    An et al., [2022](#bib.bib6)) modify the methodology by using the same network
    twice, eliminating the need for an extra teacher model. As these various compression
    methods progress, it is common to observe the adoption of two or more techniques,
    exemplified by the fusion of methods such as pruning and quantization in the same
    model.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了轻量级架构设计，章节 [3](#S3 "3\. 基本的模型压缩方法 ‣ 面向资源受限环境的轻量级深度学习：一项综述") 提到了一些可以应用于压缩给定架构的高效算法。例如，量化方法
    (Yao et al., [2021](#bib.bib231); Liu et al., [2021b](#bib.bib133); Hubara et
    al., [2016](#bib.bib98)) 旨在减少数据所需的存储，通常通过用 8 位或 16 位数字甚至二进制值替代 32 位浮点数来表示数据。剪枝算法
    (Frankle and Carbin, [2019](#bib.bib55); Guo et al., [2016](#bib.bib68); Lee et
    al., [2019](#bib.bib115)) 在其最简单的形式下，从模型中删除参数，以消除网络中的不必要冗余。然而，更复杂的算法可能会从网络中移除整个通道或过滤器
    (He et al., [2019a](#bib.bib82); Liu et al., [2019a](#bib.bib136))。知识蒸馏（KD）技术
    (Hinton et al., [2015](#bib.bib86); Gou et al., [2021](#bib.bib63)) 探讨了将知识从一个称为“教师”的模型转移到另一个称为“学生”的模型的概念。教师代表一个具有所需知识的大型预训练模型，而学生则表示一个未经过训练的小型模型，负责从教师那里提取知识。然而，随着方法的发展，一些算法
    (Yuan et al., [2020](#bib.bib240); An et al., [2022](#bib.bib6)) 修改了方法，通过两次使用相同的网络，消除了额外教师模型的需求。随着这些压缩方法的进展，常常可以观察到两种或更多技术的结合，例如在同一模型中融合剪枝和量化方法。
- en: Additionally, we discuss Neural Architecture Search (NAS) algorithms, a set
    of techniques designed to automate the model creation process while reducing human
    intervention. These algorithms autonomously search for optimal factors within
    a defined search space, such as network depth and filter settings. Research in
    this domain primarily focuses on refining the definition, traversal, and evaluation
    of the search space to achieve high accuracy without excessive time and resource
    consumption.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还讨论了神经结构搜索（NAS）算法，这是一组旨在自动化模型创建过程并减少人工干预的技术。这些算法自主地在定义的搜索空间内（如网络深度和过滤器设置）寻找最优因素。该领域的研究主要集中在优化搜索空间的定义、遍历和评估，以在不消耗过多时间和资源的情况下实现高准确率。
- en: 1.3\. Neural Network Deployment
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3\. 神经网络部署
- en: 'In Section [4](#S4 "4\. Hardware Acceleration of Deep Learning Models ‣ Lightweight
    Deep Learning for Resource-Constrained Environments: A Survey"), we navigate through
    the landscape of prevalent hardware accelerators dedicated to DL applications,
    including Graphics Processing Units (GPUs), Field-Programmable Gate Arrays (FPGAs),
    and Tensor Processing Units (TPUs). Moreover, we describe various dataflow types (Chen
    et al., [2014a](#bib.bib24); Jouppi et al., [2017](#bib.bib104); Guo et al., [2017](#bib.bib66);
    Lin and Chang, [2017](#bib.bib129)) and delve into data locality optimization
    methods (Mezdour et al., [2023](#bib.bib147); Zhang et al., [2015](#bib.bib242);
    Stoutchinin et al., [2019](#bib.bib178)), exploring the intricate techniques that
    underpin efficient processing in DL workflows. The narrative further unfolds with
    a discussion of popular DL libraries (Abadi et al., [2016](#bib.bib2); Paszke
    et al., [2019](#bib.bib154); Chen et al., [2016](#bib.bib25)) tailored for accelerating
    DL processes. This review encompasses the diverse tools and frameworks playing
    pivotal roles in optimizing the utilization of hardware accelerators. Additionally,
    we investigate co-designed solutions (Wang et al., [2020b](#bib.bib213); Parashar
    et al., [2017](#bib.bib153); Cho et al., [2021](#bib.bib33)), where achieving
    optimized and holistic results in accelerated DL requires careful consideration
    of hardware architecture and compression methods.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[4](#S4 "4\. 硬件加速深度学习模型 ‣ 资源受限环境下的轻量级深度学习：调查")节中，我们探讨了针对深度学习应用的流行硬件加速器的现状，包括图形处理单元（GPUs）、现场可编程门阵列（FPGAs）和张量处理单元（TPUs）。此外，我们描述了各种数据流类型（Chen
    et al., [2014a](#bib.bib24); Jouppi et al., [2017](#bib.bib104); Guo et al., [2017](#bib.bib66);
    Lin and Chang, [2017](#bib.bib129)），并深入研究了数据局部性优化方法（Mezdour et al., [2023](#bib.bib147);
    Zhang et al., [2015](#bib.bib242); Stoutchinin et al., [2019](#bib.bib178)），探索了支撑深度学习工作流程中高效处理的复杂技术。叙述还进一步展开，讨论了为加速深度学习过程而量身定制的流行深度学习库（Abadi
    et al., [2016](#bib.bib2); Paszke et al., [2019](#bib.bib154); Chen et al., [2016](#bib.bib25)）。本综述涵盖了在优化硬件加速器利用方面发挥关键作用的多样化工具和框架。此外，我们还调查了共同设计的解决方案（Wang
    et al., [2020b](#bib.bib213); Parashar et al., [2017](#bib.bib153); Cho et al.,
    [2021](#bib.bib33)），在加速深度学习中实现优化和整体结果需要仔细考虑硬件架构和压缩方法。
- en: 1.4\. Challenge and Future work
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4\. 挑战与未来工作
- en: 'Lastly, in Section [5](#S5 "5\. Challenge and Future work ‣ Lightweight Deep
    Learning for Resource-Constrained Environments: A Survey"), we embark on an exploration
    of emerging TinyML techniques designed to execute DL models on ultra-low-power
    devices, like MCUs, which typically consume less than 1 mW of power. Additionally,
    our paper delves into the intricacies of Large Language Models (LLMs), which present
    deployment challenges on devices with limited resources due to their enormous
    model sizes. As promising avenues in computer vision, deploying these methods
    on edge devices is crucial for widespread application.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第[5](#S5 "5\. 挑战与未来工作 ‣ 资源受限环境下的轻量级深度学习：调查")节中，我们展开了对新兴TinyML技术的探索，这些技术旨在在超低功耗设备（如MCUs）上执行深度学习模型，这些设备通常消耗不到1
    mW的功率。此外，我们的论文深入探讨了大型语言模型（LLMs）的复杂性，由于其巨大的模型规模，这些模型在资源有限的设备上部署面临挑战。作为计算机视觉中的有前途的途径，将这些方法部署到边缘设备上对于广泛应用至关重要。
- en: 1.5\. Contributions
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5\. 贡献
- en: 'This paper aims to describe in a simple but accurate manner how lightweight
    architectures, compression methods, and hardware techniques can be leveraged to
    implement an accurate model in a resource-constrained device. Our main contributions
    are summarized below:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文旨在以简单但准确的方式描述如何利用轻量级架构、压缩方法和硬件技术在资源受限的设备上实现准确的模型。我们的主要贡献总结如下：
- en: (1)
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Previous surveys only briefly reference a small number of works on lightweight
    architecture. We organize lightweight architectures into series, such as grouping
    MobileNetV1-V3 and MobileNeXt in the MobileNet series, and provide a history of
    lightweight architectures from their inception to the present.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之前的调查仅简要提及了一小部分关于轻量级架构的工作。我们将轻量级架构组织成系列，例如将MobileNetV1-V3和MobileNeXt归入MobileNet系列，并提供了轻量级架构从起源到现在的发展历史。
- en: (2)
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: To cover the entire lightweight DL applications, we also cover the compression
    and hardware acceleration methods. Unlike many other surveys that do not explicitly
    establish connections between these techniques, our survey offers a thorough overview
    of each domain, providing a comprehensive understanding of their interconnections.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了涵盖整个轻量级深度学习应用，我们还讨论了压缩和硬件加速方法。与许多其他调查未能明确建立这些技术之间的联系不同，我们的调查提供了对每个领域的全面概述，提供了对它们相互关系的深入理解。
- en: (3)
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: As part of the forefront advancements in lightweight DL, we review the present
    challenges and explore future works. Firstly, we explore TinyML, an emerging approach
    engineered for deploying DL models on devices with remarkably constrained resources.
    Subsequently, we investigate various contemporary initiatives harnessing LLMs
    on edge devices, a promising direction in the realm of lightweight DL.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作为轻量级深度学习前沿进展的一部分，我们回顾了目前的挑战并探索了未来的工作。首先，我们探讨了TinyML，这是一种新兴的方法，旨在将深度学习模型部署在资源极其受限的设备上。随后，我们研究了利用边缘设备上大规模语言模型（LLMs）的各种现代举措，这是轻量级深度学习领域中的一个有前景的方向。
- en: 2\. Lightweight Architecture Design
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 轻量级架构设计
- en: To ease readers’ comprehension, we first introduce the fundamental knowledge
    of lightweight architecture, including the general metrics to estimate the computation
    cost of the NN and the widely used mechanisms of model compression. Following
    that, we outline the lightweight CNN architecture and separate the sections by
    series, such as ShuffleNet and MobileNet series, according to their chronological
    order so that they can reflect the evolution of lightweight design and the advantage
    of its efficiency. Additionally, we discuss the efficient transformer, which offers
    a promising model capacity while maintaining a lightweight architecture.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于读者理解，我们首先介绍了轻量级架构的基础知识，包括估算神经网络计算成本的一般指标以及广泛使用的模型压缩机制。接下来，我们概述了轻量级卷积神经网络（CNN）架构，并按系列划分章节，如ShuffleNet和MobileNet系列，按照时间顺序反映轻量级设计的演变及其效率优势。此外，我们讨论了高效的变换器，这提供了一个有前景的模型能力，同时保持轻量级架构。
- en: 2.1\. Prior Knowledge of Lightweight Architecture
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1. 轻量级架构的前知识
- en: 2.1.1\. Evaluation metrics for deep learning model
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1. 深度学习模型的评估指标
- en: In DL, the three most commonly used metrics for model compression are Floating
    Point Operations (FLOPs), Multiply-Accumulate Operations (MACs), and Memory Access
    Cost (MAC). FLOPs is the number of arithmetic operations the model performs on
    the floating points, including addition, subtraction, multiplication, and division (Asperti
    et al., [2021](#bib.bib8)). Similar to FLOPs, MACs also represent the total number
    of the floating point operations; however, MACs treat addition and multiplication
    as equivalent operations, in contrast to FLOPs, which distinguish between them (Getzner
    et al., [2023](#bib.bib58)). Consequently, FLOPs $\approx$ 2$\times$MACs. On the
    other hand, MAC represents the amount of memory footprint of an NN, which corresponds
    to RAM usage (Ma et al., [2018](#bib.bib139)). Let $H$ and $W$ be the spatial
    size of the input and output feature maps for a convolution layer, $C_{in}$ is
    the number of input channels, $C_{out}$ is the number of output channels, and
    the kernel size is $k$,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，模型压缩的三种最常用指标是浮点运算（FLOPs）、乘加运算（MACs）和内存访问成本（MAC）。FLOPs是模型在浮点数上执行的算术操作的数量，包括加法、减法、乘法和除法（Asperti
    et al., [2021](#bib.bib8)）。类似于FLOPs，MACs也表示浮点运算的总数；然而，MACs将加法和乘法视为等效操作，而FLOPs则区分这两者（Getzner
    et al., [2023](#bib.bib58)）。因此，FLOPs $\approx$ 2$\times$MACs。另一方面，MAC表示神经网络的内存占用量，相当于RAM使用（Ma
    et al., [2018](#bib.bib139)）。设$H$和$W$为卷积层的输入和输出特征图的空间大小，$C_{in}$为输入通道数，$C_{out}$为输出通道数，$k$为卷积核大小，
- en: '| (1) |  | $\displaystyle MAC=H\cdot W(C_{in}+C_{out})+k\cdot k(C_{in}\times
    C_{out}).$ |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\displaystyle MAC=H\cdot W(C_{in}+C_{out})+k\cdot k(C_{in}\times
    C_{out}).$ |  |'
- en: 'Specifically, the first and second terms of Eq. [1](#S2.E1 "In 2.1.1\. Evaluation
    metrics for deep learning model ‣ 2.1\. Prior Knowledge of Lightweight Architecture
    ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") depict the memory footprint of the feature maps and weights
    for that particular convolution layer.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，方程[1](#S2.E1 "In 2.1.1\. Evaluation metrics for deep learning model ‣
    2.1\. Prior Knowledge of Lightweight Architecture ‣ 2\. Lightweight Architecture
    Design ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey")的第一项和第二项描述了特定卷积层的特征图和权重的内存占用。'
- en: Furthermore, the most widely used metrics for measuring the inference speed
    of a model are throughput and latency. Throughput refers to the amount of data
    that can be processed or the number of tasks executed within a specified period.
    During inference, throughput is measured by the number of inferences per second.
    Latency is a measure of timing between the input data arriving at a system and
    the output data being generated and can be expressed in seconds per inference.
    The relationship between throughput and latency can be derived directly, and the
    detailed formula can be found in (Sze et al., [2020](#bib.bib185)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最广泛使用的模型推理速度度量指标是吞吐量和延迟。吞吐量指的是在指定时间内可以处理的数据量或执行的任务数量。在推理过程中，吞吐量通过每秒推理次数来衡量。延迟是指从输入数据到达系统到输出数据生成之间的时间间隔，可以用每次推理的秒数来表示。吞吐量和延迟之间的关系可以直接推导，详细公式见(Sze等，[2020](#bib.bib185))。
- en: 2.1.2\. Pointwise convolution
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2\. 点卷积
- en: The pointwise convolution, also known as a $1\times 1$ convolution, was first
    introduced in the inception module (Szegedy et al., [2015](#bib.bib187)). The
    inception module inserts the pointwise convolutions at the bottleneck to obtain
    deeper features with fewer FLOPs. Empowered by the adaptability of pointwise convolutions
    to accommodate modifications to the channel’s dimensions, the Inception series
    of works was born (Szegedy et al., [2016](#bib.bib188), [2017](#bib.bib186); Chollet,
    [2017](#bib.bib36)). Significantly, pointwise convolutions directly affect the
    model’s computation time and the information richness of the architecture.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 点卷积，也称为$1\times 1$卷积，首次在Inception模块中引入（Szegedy等，[2015](#bib.bib187)）。Inception模块在瓶颈处插入点卷积，以获得更深层的特征，同时减少FLOP数。得益于点卷积对通道维度调整的适应性，Inception系列的工作应运而生（Szegedy等，[2016](#bib.bib188)，[2017](#bib.bib186)；Chollet，[2017](#bib.bib36)）。显著的是，点卷积直接影响模型的计算时间和架构的信息丰富性。
- en: 2.1.3\. Group convolution
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3\. 分组卷积
- en: The group convolution idea was proposed by AlexNet (Krizhevsky et al., [2012](#bib.bib111)).
    Group convolutions aim to divide the channels of feature maps into several groups
    and apply convolutions separately to each group. This process helps to reduce
    computational complexity by $N$ times, where $N$ represents the number of groups.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 分组卷积的概念由AlexNet提出（Krizhevsky等，[2012](#bib.bib111)）。分组卷积旨在将特征图的通道划分为多个组，并分别对每个组进行卷积。这一过程有助于将计算复杂度降低$N$倍，其中$N$表示组的数量。
- en: However, there are still several shortcomings in group convolutions. Firstly,
    group assignments are fixed, and this factor restricts the information flow between
    groups, inevitably harming performance. Secondly, group convolutions cost additional
    MAC, especially when the number of groups is large, resulting in a much longer
    inference time. To solve the first problem, ShuffleNet (Zhang et al., [2018c](#bib.bib249))
    shared group features to obtain deeper channel information. CondenseNet (Huang
    et al., [2018](#bib.bib94)) progressively prunes the unimportant connections using
    learned group convolutions (LGCs). Several works (Wang et al., [2019](#bib.bib211);
    Zhang et al., [2019a](#bib.bib252)) attempt to improve the original LGC to learn
    better optimal group structures. Furthermore, Dynamic Group Convolution (DGC) (Su
    et al., [2020](#bib.bib180)) highlights the importance of input channels via a
    salience generator and then uses a channel selector to assign groups adaptively.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，组卷积仍然存在几个缺点。首先，组分配是固定的，这限制了组之间的信息流动，不可避免地损害性能。其次，组卷积会额外消耗 MAC，特别是当组的数量很大时，导致推断时间更长。为了解决第一个问题，ShuffleNet（Zhang
    等人，[2018c](#bib.bib249)）共享组特征以获得更深的通道信息。CondenseNet（Huang 等人，[2018](#bib.bib94)）通过使用学习到的组卷积（LGCs）逐步修剪不重要的连接。几种方法（Wang等人，[2019](#bib.bib211)；Zhang等人，[2019a](#bib.bib252)）尝试改进原始的LGC，以学习更好的最优组结构。此外，动态组卷积
    (DGC)（Su等人，[2020](#bib.bib180)）通过一个显著性生成器突出显示输入通道的重要性，然后使用通道选择器自适应地分配组。
- en: 2.1.4\. Depthwise separable convolution
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4\. 深度可分离卷积
- en: The idea of a depthwise separable convolution was proposed in Xception (Chollet,
    [2017](#bib.bib36)), which is the advanced version of the Inception family (Szegedy
    et al., [2016](#bib.bib188), [2017](#bib.bib186)). A depthwise separable convolution
    consists of a depthwise convolution followed by a pointwise convolution. According
    to the MAC, this is a computation-saving but time-consuming operation. To address
    this issue, Tan et al. (Tan and Le, [2019b](#bib.bib194)) aggregate multiple kernel
    sizes into a single depthwise convolution and use AutoML (He et al., [2021](#bib.bib79))
    for navigating the search space.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 深度可分离卷积的思想是在 Xception（Chollet，[2017](#bib.bib36)）中提出的，它是 Inception 系列的高级版本（Szegedy
    等人，[2016](#bib.bib188) ，[2017](#bib.bib186)）。深度可分离卷积由深度卷积和逐点卷积组成。根据 MAC，这是一种节省计算但耗时的操作。为了解决这个问题，Tan
    等人（Tan 和 Le，[2019b](#bib.bib194)）将多个内核大小聚合成一个单独的深度卷积，并使用 AutoML（He 等人，[2021](#bib.bib79)）进行导航搜索空间。
- en: 2.2\. Lightweight CNN Architecture
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 轻量级卷积神经网络架构
- en: 2.2.1\. SqueezeNet series
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. SqueezeNet 系列
- en: SqueezeNet series (Iandola et al., [2017](#bib.bib99); Gholami et al., [2018](#bib.bib60))
    is an early application to reduce parameters using pointwise convolution. SqueezeNet (Iandola
    et al., [2017](#bib.bib99)) proposes the fire module that constitutes the squeeze
    layer and the expand layer. The squeeze layer consists of pointwise convolution.
    It first squeezes features into lower dimensions and then passes them through
    an expansion layer, which separates the convolution operation into a pointwise
    convolution and a 3$\times$3 convolution. To solve the gradient vanishing problem
    and decrease the computation cost, SqueezeNext (Gholami et al., [2018](#bib.bib60))
    keeps the shortcut concept from ResNet (He et al., [2016](#bib.bib78)) and decomposed
    3$\times$3 kernel into two low-rank kernels, with sizes of 3$\times$1 and 1$\times$3\.
    This augmented design reduces the parameters of the kernels from $k^{2}$ to $2k$,
    hence solving the inefficient problem of using depthwise separable convolutions.
    Compared to AlexNet (Krizhevsky et al., [2012](#bib.bib111)), SqueezeNet and SqueezeNext
    reduce the parameters by 50$\times$ and 112$\times$, respectively, while keeping
    AlexNet’s level of accuracy on the ImageNet dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: SqueezeNet 系列 （Iandola 等人，[2017](#bib.bib99)；Gholami 等人，[2018](#bib.bib60)）是早期应用的一个例子，使用逐点卷积来减少参数。SqueezeNet（Iandola
    等人，[2017](#bib.bib99)）提出了火模块，它由挤压层和扩展层构成。挤压层由逐点卷积组成。它首先将特征挤压到较低的维度，然后通过扩展层进行传递，将卷积操作分为逐点卷积和
    3×3 卷积。为了解决梯度消失问题和降低计算成本，SqueezeNext（Gholami 等人，[2018](#bib.bib60)）保留了ResNet（He
    等人，[2016](#bib.bib78)）中的快捷连接概念，并将 3×3 的卷积核分解为两个低秩卷积核，大小分别为 3×1 和 1×3。这种增强的设计将卷积核的参数从
    $k^{2}$ 减少到 $2k$，从而解决了使用深度可分离卷积的低效问题。与 AlexNet（Krizhevsky 等人，[2012](#bib.bib111)）相比，SqueezeNet
    和 SqueezeNext 分别将参数减少了 50 倍和 112 倍，同时保持了 AlexNet 在 ImageNet 数据集上的准确性水平。
- en: 2.2.2\. ShuffleNet series
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. ShuffleNet 系列
- en: The primary purpose of the ShuffleNet series (Zhang et al., [2018c](#bib.bib249);
    Ma et al., [2018](#bib.bib139)) is to improve the performance of group convolutions
    and the memory efficiency of depthwise separable convolutions. After a group convolution,
    each group’s output features form an individual channel, and performance suffers
    due to information not being shared between channels. To address this limitation,
    ShuffleNet (Zhang et al., [2018c](#bib.bib249)) applies a channel shuffle mechanism
    after the 1$\times$1 group convolution to facilitate cross-group information exchange
    so that features can maintain more global information channels.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ShuffleNet 系列 (Zhang et al., [2018c](#bib.bib249); Ma et al., [2018](#bib.bib139))
    的主要目的是提升组卷积的性能和深度可分卷积的内存效率。经过组卷积后，每个组的输出特征形成一个独立的通道，性能受到影响，因为通道之间的信息未被共享。为了解决这一限制，ShuffleNet (Zhang
    et al., [2018c](#bib.bib249)) 在 1$\times$1 组卷积之后应用了通道洗牌机制，以促进组间信息的交换，从而使特征能够保持更多的全局信息通道。
- en: ShuffleNetV2 (Ma et al., [2018](#bib.bib139)) investigates four practical guidelines
    to design a memory-efficient and lightweight model that can avoid heavy MAC problems.
    Firstly, equal input and output dimensions mean a smaller MAC. Secondly, MAC is
    large when groups are large, particularly for depthwise separable convolutions.
    Thirdly, it is best to avoid designing a wide network like the Inception series (Szegedy
    et al., [2015](#bib.bib187), [2016](#bib.bib188), [2017](#bib.bib186)) because
    network fragments can result in a large MAC. Lastly, since element-wise manipulation
    in a network requires extra computation, avoiding it is efficient. This is often
    overlooked because it represents only a few FLOPs but increases MAC, as in depthwise
    separable convolutions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ShuffleNetV2 (Ma et al., [2018](#bib.bib139)) 研究了设计一个内存高效且轻量化模型的四个实用指南，以避免严重的
    MAC 问题。首先，相等的输入和输出维度意味着较小的 MAC。其次，当组很大时，MAC 会很大，特别是对于深度可分卷积。第三，最好避免设计像 Inception
    系列 (Szegedy et al., [2015](#bib.bib187), [2016](#bib.bib188), [2017](#bib.bib186))
    这样的宽网络，因为网络碎片可能导致较大的 MAC。最后，由于网络中的逐元素操作需要额外的计算，避免这种操作是高效的。由于它仅占少量的 FLOPs，但会增加
    MAC，这一点常被忽视，例如在深度可分卷积中。
- en: 2.2.3\. CondenseNet series
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3\. CondenseNet 系列
- en: Since shortcut connections effectively prevent the gradient vanishing problem (He
    et al., [2016](#bib.bib78)), some studies, such as DenseNet (Huang et al., [2017](#bib.bib95))
    and the CondenseNet series (Huang et al., [2018](#bib.bib94); Yang et al., [2021a](#bib.bib228))
    attempt to optimize NN structure based on shortcut connections. DenseNet (Huang
    et al., [2017](#bib.bib95)) replaces the shortcut connections with dense connections,
    thus improving gradient flow within the bottleneck. Although dense connections
    increase the accuracy, CondenseNet (Huang et al., [2018](#bib.bib94)) observes
    that the magnitude of the connections far from the layers will decay exponentially,
    causing them to be heavy and slow. To this end, CondenseNet utilizes learned group
    convolutions (LGCs) to prune connections progressively. Before training, the filters
    are split into G groups of equal size. Suppose $C_{in}$ is the number of input
    channels, $C_{out}$ is the number of output channels, and $F_{ij}^{g}$ denotes
    the kernel weights, including the weights of $j_{th}$ input, and $i_{th}$ output
    within a group $g\in G$. The importance of the $j_{th}$ incoming feature map for
    the filter group $g$ is computed by the averaged absolute value of weights between
    them across all outputs within the group, i.e., $\sum_{i=1}^{\frac{C_{out}}{G}}{|F_{ij}^{g}|}$,
    where columns in $F^{g}$ with small L1-norm value can be removed by zeroing them
    out. The structured sparsity within a group can be evaluated by applying the group-lasso
    regularizer (Yuan and Lin, [2006](#bib.bib241)),
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于快捷连接有效地防止了梯度消失问题 (He et al., [2016](#bib.bib78))，一些研究，如 DenseNet (Huang et
    al., [2017](#bib.bib95)) 和 CondenseNet 系列 (Huang et al., [2018](#bib.bib94); Yang
    et al., [2021a](#bib.bib228))，尝试基于快捷连接优化神经网络结构。DenseNet (Huang et al., [2017](#bib.bib95))
    用稠密连接替代了快捷连接，从而改善了瓶颈中的梯度流。虽然稠密连接提高了准确性，但 CondenseNet (Huang et al., [2018](#bib.bib94))
    观察到，远离层的连接幅度会呈指数衰减，导致其变得沉重且缓慢。为此，CondenseNet 利用学习的组卷积 (LGCs) 逐步剪枝连接。在训练之前，滤波器被分成
    G 个相同大小的组。设 $C_{in}$ 为输入通道数，$C_{out}$ 为输出通道数，$F_{ij}^{g}$ 表示包括第 $j$ 个输入和第 $i$
    个输出在组 $g\in G$ 内的核权重。第 $j$ 个输入特征图对滤波器组 $g$ 的重要性通过在组内所有输出之间权重的绝对值的平均值计算，即 $\sum_{i=1}^{\frac{C_{out}}{G}}{|F_{ij}^{g}|}$，其中
    $F^{g}$ 中列的小 L1-范数值可以通过将其置零来移除。组内的结构稀疏性可以通过应用组套索正则化器 (Yuan and Lin, [2006](#bib.bib241))
    来评估。
- en: '| (2) |  | $r=\sum_{g=1}^{G}\sum_{j=1}^{C_{in}}\sqrt{\sum_{i=1}^{\frac{C_{out}}{G}}{F_{ij}^{g}}^{2}}.$
    |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $r=\sum_{g=1}^{G}\sum_{j=1}^{C_{in}}\sqrt{\sum_{i=1}^{\frac{C_{out}}{G}}{F_{ij}^{g}}^{2}}.$
    |  |'
- en: 'By using Eq. [2](#S2.E2 "In 2.2.3\. CondenseNet series ‣ 2.2\. Lightweight
    CNN Architecture ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey"), connections to less important
    features, represented by a small sparsity value, will be removed, resulting in
    effective pruning.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 Eq. [2](#S2.E2 "在 2.2.3\. CondenseNet 系列 ‣ 2.2\. 轻量级 CNN 架构 ‣ 2\. 轻量级架构设计
    ‣ 针对资源受限环境的轻量级深度学习：综述")，对不重要特征的连接（由小稀疏值表示）将被移除，从而实现有效的剪枝。
- en: Recently, CondenseNetV2 (Yang et al., [2021a](#bib.bib228)) pointed out that
    the fixed connection mode limits the opportunities for feature reuse. To address
    this limitation, CondenseNetV2 aims to reactivate outdated features through a
    novel sparse feature reactivation module. In CondenseNetV2, the weight connections
    within each block are learned during training, as opposed to CondenseNet, which
    fixes the model’s weight connections after pruning. As a result, this approach
    results in a performance gain by leveraging the underlying connection.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，CondenseNetV2 (Yang et al., [2021a](#bib.bib228)) 指出，固定的连接模式限制了特征重用的机会。为了解决这一限制，CondenseNetV2
    旨在通过一种新颖的稀疏特征重新激活模块重新激活过时的特征。在 CondenseNetV2 中，每个块内的权重连接在训练过程中进行学习，而 CondenseNet
    则在剪枝后固定模型的权重连接。因此，这种方法通过利用潜在连接带来了性能提升。
- en: 'Fig. [1](#S2.F1 "Figure 1 ‣ 2.2.3\. CondenseNet series ‣ 2.2\. Lightweight
    CNN Architecture ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey") illustrates a graphical comparison,
    highlighting the architectural differences between DenseNet, CondenseNet, and
    CondenseNetV2\. In DenseNet, weights between layers in a block are fully-connected,
    where weights in one layer are connected to weights in all other layers (solid-colored
    arrows). To make the network more efficient, CondenseNet uses LGCs to prune weight
    connections (gray dashed arrows), and once pruned, the connections for every block
    remain fixed, e.g., the connections in Block 1 and Block 2 are identical. CondenseNetV2
    proposes a sparse feature reactivation mechanism to learn the connections’ weights
    automatically during training. From Fig. [1](#S2.F1 "Figure 1 ‣ 2.2.3\. CondenseNet
    series ‣ 2.2\. Lightweight CNN Architecture ‣ 2\. Lightweight Architecture Design
    ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey"),
    we can observe that in Block 2 of CondenseNetV2, two pruned connections in Block
    1 are reactivated while another two previously active connections in Block 1 are
    removed, demonstrating the dynamic nature of CondenseNetV2.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1](#S2.F1 "Figure 1 ‣ 2.2.3\. CondenseNet series ‣ 2.2\. Lightweight CNN
    Architecture ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey") 展示了图形比较，突出了 DenseNet、CondenseNet
    和 CondenseNetV2 之间的架构差异。在 DenseNet 中，块内层之间的权重是全连接的，其中一层的权重与所有其他层的权重相连（实线箭头）。为了提高网络的效率，CondenseNet
    使用 LGCs 来修剪权重连接（灰色虚线箭头），修剪后，每个块的连接保持固定，例如，Block 1 和 Block 2 中的连接是相同的。CondenseNetV2
    提出了一个稀疏特征重新激活机制，以在训练过程中自动学习连接的权重。从图 [1](#S2.F1 "Figure 1 ‣ 2.2.3\. CondenseNet
    series ‣ 2.2\. Lightweight CNN Architecture ‣ 2\. Lightweight Architecture Design
    ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey")
    中，我们可以观察到，在 CondenseNetV2 的 Block 2 中，Block 1 中的两个被修剪的连接被重新激活，同时 Block 1 中的另外两个先前活跃的连接被移除，展示了
    CondenseNetV2 的动态特性。'
- en: '![Refer to caption](img/157296ae52c20bb1feb6f90685e95aeb.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/157296ae52c20bb1feb6f90685e95aeb.png)'
- en: Figure 1\. Comparison of DenseNet, CondenseNet, and CondenseNetV2. Active weight
    connections are represented by solid color arrows, and pruned weight connections
    are represented by gray dashed arrows.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. DenseNet、CondenseNet 和 CondenseNetV2 的比较。活跃的权重连接用实色箭头表示，被修剪的权重连接用灰色虚线箭头表示。
- en: 2.2.4\. MobileNet Series
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4\. MobileNet 系列
- en: This series (Howard et al., [2017](#bib.bib90); Sandler et al., [2018](#bib.bib169);
    Howard et al., [2019](#bib.bib89); Zhou et al., [2020](#bib.bib254)) includes
    prominent CNN models that can be deployed on IoT devices. Based on VGG (Simonyan
    and Zisserman, [2015](#bib.bib174)) architecture, MobileNet (Howard et al., [2017](#bib.bib90))
    applies depthwise separable convolutions to create an efficient model, which is
    shown to perform significantly faster across a broad range of tasks and applications.
    Discovering that ReLU activations can lead to severe information loss of features
    with lower dimensions, MobileNetV2 (Sandler et al., [2018](#bib.bib169)) replaces
    the ReLU activation with a linear combination in the last layer of the residual
    bottleneck to mitigate the information loss. In addition, MobileNetV2 introduces
    an inverted residual block, where the number of channels is first increased and
    then recovered in the residual bottleneck, improving the accuracy. Shortcut connections (He
    et al., [2016](#bib.bib78)) are also added to enhance the gradient propagation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这一系列 (Howard 等人，[2017](#bib.bib90)；Sandler 等人，[2018](#bib.bib169)；Howard 等人，[2019](#bib.bib89)；Zhou
    等人，[2020](#bib.bib254)) 包括了可以在 IoT 设备上部署的显著 CNN 模型。基于 VGG (Simonyan 和 Zisserman，[2015](#bib.bib174))
    架构，MobileNet (Howard 等人，[2017](#bib.bib90)) 采用深度可分离卷积来创建高效的模型，这表明其在广泛任务和应用中的表现显著更快。发现
    ReLU 激活可能导致低维特征的严重信息丢失，MobileNetV2 (Sandler 等人，[2018](#bib.bib169)) 用线性组合替代了残差瓶颈中的
    ReLU 激活，以减少信息丢失。此外，MobileNetV2 引入了倒置残差块，其中通道数首先增加，然后在残差瓶颈中恢复，提高了准确性。还添加了快捷连接 (He
    等人，[2016](#bib.bib78)) 以增强梯度传播。
- en: NetAdapt (Yang et al., [2018](#bib.bib229)) applies layer-wise optimization
    to simplify the network and to achieve high accuracy within limited hardware resources.
    Based on this, MobileNetV3 (Howard et al., [2019](#bib.bib89)) leverages platform-aware
    NAS (Tan et al., [2019a](#bib.bib191)) to optimize the block-wise structure and
    implements SENet (Hu et al., [2018](#bib.bib92)) (channel attention module) in
    the bottleneck structures, resulting in better accuracy. To reduce MAC and establish
    a quantization-friendly network, ReLU is replaced with H-swish activation. As
    an alternative to the inverted residual block, MobileNeXt (Zhou et al., [2020](#bib.bib254))
    develops a Sandglass block by flipping the inverted residual block to enhance
    the transmission of wider architectures since wider layers might lead to more
    gradient confusion, making model training harder.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: NetAdapt (Yang et al., [2018](#bib.bib229)) 通过逐层优化来简化网络，并在有限的硬件资源下实现高精度。基于此，MobileNetV3
    (Howard et al., [2019](#bib.bib89)) 利用平台感知 NAS (Tan et al., [2019a](#bib.bib191))
    来优化块级结构，并在瓶颈结构中实现 SENet (Hu et al., [2018](#bib.bib92))（通道注意模块），从而实现更好的准确性。为了减少
    MAC 并建立量化友好的网络，将 ReLU 替换为 H-swish 激活。作为倒残差块的替代方案，MobileNeXt (Zhou et al., [2020](#bib.bib254))
    通过翻转倒残差块开发了沙漏块，以增强更宽架构的传输，因为更宽的层可能导致更多的梯度混淆，使得模型训练更加困难。
- en: 2.2.5\. Shift-based series
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.5\. 基于位移的系列
- en: CNN is computationally expensive due to many multiplication and addition operations.
    ShiftNet (Wu et al., [2018](#bib.bib218)) pioneered the replacement of spatial
    convolutions with Group Shift convolution. Unlike standard convolutions, shift
    convolutions only perform shifting operations on feature maps and apply padding
    to those offset areas. In contrast to multiplication operations, shift convolution
    can achieve zero parameters and FLOPs, thus drastically reducing their number.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 由于大量的乘法和加法运算而计算成本高。ShiftNet (Wu et al., [2018](#bib.bib218)) 首先将空间卷积替换为组位移卷积。与标准卷积不同，位移卷积仅对特征图执行位移操作，并对那些偏移区域应用填充。与乘法运算相比，位移卷积可以实现零参数和
    FLOPs，从而大幅减少它们的数量。
- en: 'Some studies attempt to improve the performance based on shift convolution
    layers. For example, Jeon et al. (Jeon and Kim, [2018](#bib.bib101)) propose an
    Active Shift Layer that makes shifts learnable instead of heuristic assignments.
    Chen et al. (Chen et al., [2019a](#bib.bib27)) point out that because the number
    of shifts is fixed, implementing them requires a lot of trial and error, limiting
    the network’s functionality. Thus, they propose a Sparse Shift Layer to eliminate
    meaningless memory movement. The non-shift channels remain the same. Fig. [2](#S2.F2
    "Figure 2 ‣ 2.2.5\. Shift-based series ‣ 2.2\. Lightweight CNN Architecture ‣
    2\. Lightweight Architecture Design ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") compares these three shift operations.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '一些研究试图基于位移卷积层提高性能。例如，Jeon et al. (Jeon and Kim, [2018](#bib.bib101)) 提出了一个主动位移层，使得位移可学习，而不是启发式分配。Chen
    et al. (Chen et al., [2019a](#bib.bib27)) 指出，由于位移数量是固定的，实现它们需要大量的试错，这限制了网络的功能。因此，他们提出了一个稀疏位移层，以消除无意义的内存移动。非位移通道保持不变。图
    [2](#S2.F2 "Figure 2 ‣ 2.2.5\. Shift-based series ‣ 2.2\. Lightweight CNN Architecture
    ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") 比较了这三种位移操作。'
- en: '![Refer to caption](img/93a4b75c9297e83643a060487831f069.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/93a4b75c9297e83643a060487831f069.png)'
- en: (a) Group Shift
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 组位移
- en: '![Refer to caption](img/856d8054ceef40cee9ba10063001d386.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/856d8054ceef40cee9ba10063001d386.png)'
- en: (b) Active Shift
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 主动位移
- en: '![Refer to caption](img/b56fd3ea0c29c99fce5eefd8456d83f4.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/b56fd3ea0c29c99fce5eefd8456d83f4.png)'
- en: (c) Sparse Shift
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 稀疏位移
- en: Figure 2\. The variant of Shift-based convolution (Chen et al., [2019a](#bib.bib27)).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 基于位移的卷积变体 (Chen et al., [2019a](#bib.bib27))。
- en: AddressNet (He et al., [2019b](#bib.bib83)) observes that a smaller amount of
    parameters or computation (FLOPs) does not always lead to a direct reduction in
    inference time, even with shift convolution’s zero parameters and zero FLOPs (Wu
    et al., [2018](#bib.bib218)). To optimize the speed of GPU-based machines, AddressNet
    changes channel shuffle (Zhang et al., [2018c](#bib.bib249)) to channel shift
    since channel shuffle produces additional memory space and time-consuming permutations,
    further eliminating the redundant direction. Similar to AdderNet (Chen et al.,
    [2020b](#bib.bib22)), DeepShift (Elhoushi et al., [2021](#bib.bib49)) is constructed
    solely with addition operations, replacing all multiplications with bit-wise shifts
    and sign flips, significantly reduces the operation time and energy consumption.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: AddressNet (He et al., [2019b](#bib.bib83)) 观察到较少的参数或计算量（FLOPs）并不总是直接导致推理时间的减少，即使是移位卷积的零参数和零
    FLOPs (Wu et al., [2018](#bib.bib218))。为了优化基于 GPU 的机器的速度，AddressNet 将通道洗牌 (Zhang
    et al., [2018c](#bib.bib249)) 改为通道移位，因为通道洗牌会产生额外的内存空间和耗时的排列，进一步消除冗余方向。类似于 AdderNet
    (Chen et al., [2020b](#bib.bib22))，DeepShift (Elhoushi et al., [2021](#bib.bib49))
    仅使用加法运算，使用位移和符号翻转代替所有乘法，显著减少了操作时间和能量消耗。
- en: 2.2.6\. Add-based Series
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.6\. 基于加法的系列
- en: Multiplication and addition operations constitute many convolution operations,
    resulting in extra calculations. AdderNet (Chen et al., [2020b](#bib.bib22)) attempts
    to exclusively use additions, using an L1-norm distance as a response criterion
    between filters and feature maps. This operation is known as Absolute-difference-accumulation (Um
    et al., [2021](#bib.bib202)), and it accelerates the network and allows the reuse
    of computation results in order to reduce energy consumption.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法和加法运算构成了许多卷积操作，从而导致额外的计算。AdderNet (Chen et al., [2020b](#bib.bib22)) 尝试专门使用加法，通过使用
    L1 范数距离作为滤波器与特征图之间的响应标准。这种操作被称为绝对差异累积 (Um et al., [2021](#bib.bib202))，它加速了网络，并允许重用计算结果以减少能量消耗。
- en: 'You et al. (You et al., [2020](#bib.bib236)) introduce ShiftAddnet, focusing
    more on hardware efficiency. ShiftAddnet proposes a new metric for performance
    comparison, expressive capacity, which refers to the accuracy achieved by the
    model under similar hardware conditions. Experimental results show that shift-based
    networks (Wu et al., [2018](#bib.bib218); Jeon and Kim, [2018](#bib.bib101); Chen
    et al., [2019a](#bib.bib27); He et al., [2019b](#bib.bib83); Elhoushi et al.,
    [2021](#bib.bib49)) provide greater hardware efficiency but have a lower expressive
    capacity than multiplication-based networks. Conversely, the fully additive network (Chen
    et al., [2020b](#bib.bib22)) is inefficient since repeated additions are used
    to replace multiplications, although it can achieve better accuracy. Therefore,
    ShiftAddnet combines the benefits of bit-wise shifts (Elhoushi et al., [2021](#bib.bib49))
    and the efficiency of additive networks (Chen et al., [2020b](#bib.bib22)) to
    achieve state-of-the-art results on two IoT datasets: FlatCam (Tan et al., [2019b](#bib.bib190))
    and Head Pose (Viet et al., [2021](#bib.bib205)).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: You et al. (You et al., [2020](#bib.bib236)) 引入了 ShiftAddnet，更关注硬件效率。ShiftAddnet
    提出了一个新的性能比较指标，即表达能力，指的是模型在类似硬件条件下实现的准确性。实验结果表明，基于移位的网络 (Wu et al., [2018](#bib.bib218);
    Jeon and Kim, [2018](#bib.bib101); Chen et al., [2019a](#bib.bib27); He et al.,
    [2019b](#bib.bib83); Elhoushi et al., [2021](#bib.bib49)) 提供了更高的硬件效率，但其表达能力低于基于乘法的网络。相反，完全加法网络
    (Chen et al., [2020b](#bib.bib22)) 效率较低，因为使用重复的加法代替乘法，尽管它可以实现更好的准确性。因此，ShiftAddnet
    结合了位移 (Elhoushi et al., [2021](#bib.bib49)) 的优势和加法网络 (Chen et al., [2020b](#bib.bib22))
    的效率，在两个 IoT 数据集：FlatCam (Tan et al., [2019b](#bib.bib190)) 和 Head Pose (Viet et
    al., [2021](#bib.bib205)) 上取得了最先进的结果。
- en: 2.2.7\. EfficientNet Series
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.7\. EfficientNet 系列
- en: Almost all networks attempt to improve performance by adjusting depth, width,
    and resolution. To achieve the best performance and lightweight combination, it
    is crucial to pick the right combination. EfficientNet (Tan and Le, [2019a](#bib.bib192))
    proposes a simple grid search algorithm, compound scaling, to seek scaling factors
    (depth, width, and resolution), achieving accuracy with lower computation costs.
    EffectiveNetV2 (Tan and Le, [2021](#bib.bib193)) proposes a training-aware NAS
    to find a good trade-off for accuracy $A$, training speed $S$, and parameters
    $P$. It uses a search reward formulated as a simple weighted product, $A\cdot
    S^{w}\cdot P^{v}$, where $w=-0.07$ and $v=-0.05$ are empirically determined to
    balance the trade-off. To address the inefficiency of depthwise convolution, EfficientNetV2
    replaces stage 1-3 MBConv (Sandler et al., [2018](#bib.bib169)) with Fused-MBConv (Gupta
    and Akin, [2020](#bib.bib72)) in its architecture design, offering better performance
    and trade-off in terms of accuracy, parameters, and FLOPs. Besides, for a more
    robust network, EfficientNetV2 selects adaptive regulation during the training
    process because using identical regularization terms for images of different resolutions
    is inefficient.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有网络都尝试通过调整深度、宽度和分辨率来提高性能。要实现最佳性能和轻量级组合，选择正确的组合至关重要。EfficientNet (Tan 和 Le,
    [2019a](#bib.bib192)) 提出了一个简单的网格搜索算法，复合缩放，以寻找缩放因子（深度、宽度和分辨率），以较低的计算成本实现准确性。EffectiveNetV2
    (Tan 和 Le, [2021](#bib.bib193)) 提出了一个训练感知的 NAS 以找到准确性 $A$、训练速度 $S$ 和参数 $P$ 的良好权衡。它使用一个简单的加权乘积作为搜索奖励，$A\cdot
    S^{w}\cdot P^{v}$，其中 $w=-0.07$ 和 $v=-0.05$ 是经验确定的，用于平衡权衡。为了解决深度卷积的低效问题，EfficientNetV2
    在其架构设计中用 Fused-MBConv (Gupta 和 Akin, [2020](#bib.bib72)) 替代了阶段 1-3 的 MBConv (Sandler
    等人, [2018](#bib.bib169))，在准确性、参数和 FLOPs 方面提供了更好的性能和权衡。此外，为了获得更强健的网络，EfficientNetV2
    在训练过程中选择了自适应正则化，因为对不同分辨率的图像使用相同的正则化项是低效的。
- en: 2.2.8\. Discussion and Summary
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.8\. 讨论与总结
- en: 'Table [1](#S2.T1 "Table 1 ‣ 2.2.8\. Discussion and Summary ‣ 2.2\. Lightweight
    CNN Architecture ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey") compares the performance of
    lightweight CNN architectures on the ImageNet dataset. The horizontal lines separate
    the models of different series. From the table, we can observe that there is no
    one-size-fits-all architecture. Oftentimes, it is a trade-off between accuracy
    and efficiency. For example, AddressNet-20 maximizes efficiency at the expense
    of accuracy. Conversely, the most accurate variants of the EfficientNet series
    are among the least efficient ones. Drawing from this analysis, we provide recommendations
    on selecting the suitable models and hardware.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '表[1](#S2.T1 "Table 1 ‣ 2.2.8\. Discussion and Summary ‣ 2.2\. Lightweight CNN
    Architecture ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey") 比较了轻量级 CNN 架构在 ImageNet 数据集上的性能。水平线将不同系列的模型分开。从表中我们可以观察到，没有一种适用于所有情况的架构。通常，这是在准确性和效率之间的权衡。例如，AddressNet-20
    在牺牲准确性的情况下最大化效率。相反，EfficientNet 系列中最准确的变体通常是效率最低的。根据这一分析，我们提供了选择合适模型和硬件的建议。'
- en: How to choose an adequate lightweight model and compatible hardware? The first
    crucial step is to check lightweight models’ specifications and hardware compatibility.
    For example, depthwise separable convolutions have huge MAC and high RAM requirements.
    It is, therefore, imperative to employ a network on hardware that considers both
    RAM and storage capacity. To this end, Fan et al. (Fan et al., [2021](#bib.bib51))
    redesign the depthwise separable convolution and channel shuffle modules to be
    hardware-friendly on FPGA. Moreover, to minimize the inference time and to support
    deployment on a small target device, replacing multiplication with shift or add
    operations can effectively reduce the total parameters and MACs/FLOPs. Thus, ShiftNet
    or AdderNet series can be a good choice since they require smaller parameters
    and MACs. Within these two series, AddressNet-20 gives the best performance. For
    target devices with relatively more storage, such as mobile phones or GPUs, models
    with higher accuracy are preferred for a better user experience. EfficientNetV2-L
    can thus be considered since it achieves the highest Top-1 accuracy. However,
    it is important to note that the EfficientNet series costs a disproportionately
    higher number of parameters and MACs, which limits the application under low-end
    devices. Another way to achieve a better trade-off model is to apply fundamental
    compression methods such as pruning, quantization, and NAS (Chen et al., [2019b](#bib.bib30);
    Tan et al., [2019a](#bib.bib191)) (see Section 3) to adjust the architecture.
    This can be an efficient technique to reduce MACs/FLOPs, parameters, and inference
    time.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如何选择合适的轻量级模型和兼容的硬件？第一个关键步骤是检查轻量级模型的规格和硬件兼容性。例如，深度可分离卷积对 MAC 和 RAM 有较高要求。因此，必须在考虑
    RAM 和存储容量的硬件上使用网络。为此，Fan 等人 (Fan et al., [2021](#bib.bib51)) 重新设计了深度可分离卷积和通道洗牌模块，以便在
    FPGA 上对硬件友好。此外，为了最小化推理时间并支持在小型目标设备上部署，可以通过将乘法操作替换为移位或加法操作来有效减少总参数和 MACs/FLOPs。因此，ShiftNet
    或 AdderNet 系列可能是不错的选择，因为它们需要更小的参数和 MACs。在这两个系列中，AddressNet-20 的性能最佳。对于具有较多存储的目标设备，如手机或
    GPU，建议使用更高准确性的模型以获得更好的用户体验。因此，可以考虑 EfficientNetV2-L，因为它实现了最高的 Top-1 准确性。然而，值得注意的是，EfficientNet
    系列的参数和 MACs 的消耗不成比例地高，这限制了其在低端设备上的应用。另一种实现更好权衡模型的方法是应用基本的压缩方法，如剪枝、量化和 NAS (Chen
    et al., [2019b](#bib.bib30); Tan et al., [2019a](#bib.bib191)) (参见第 3 节) 来调整架构。这可以是减少
    MACs/FLOPs、参数和推理时间的有效技术。
- en: Some lightweight methods, such as SqueezeNet and ShuffeNet, may not be able
    to take full advantage of GPU-accelerated performance due to the lack of customized
    designs (Um et al., [2021](#bib.bib202)). Additionally, if pruning is applied
    to a network, like the CondenseNet series, the network structure might be irregular,
    preventing the target device from supporting it. In such a scenario, parallelism
    requires specifically designed computing hardware. Fortunately, customized hardware
    can be designed to fit a lightweight model. For instance, Um et al. (Um et al.,
    [2021](#bib.bib202)) note that CIM is incompatible with AdderNet because it cannot
    predict details of an absolute difference nor reuse the computation results. Therefore,
    they designed a novel ADA-CIM processor offering low-cost sign prediction and
    higher energy efficiency.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一些轻量级方法，如 SqueezeNet 和 ShuffleNet，由于缺乏定制化设计，可能无法充分发挥 GPU 加速性能 (Um et al., [2021](#bib.bib202))。此外，如果对网络应用剪枝，如
    CondenseNet 系列，网络结构可能会不规则，从而导致目标设备无法支持它。在这种情况下，平行计算需要专门设计的计算硬件。幸运的是，可以设计定制硬件以适应轻量级模型。例如，Um
    等人 (Um et al., [2021](#bib.bib202)) 指出 CIM 与 AdderNet 不兼容，因为它无法预测绝对差异的细节，也不能重用计算结果。因此，他们设计了一种新型
    ADA-CIM 处理器，提供低成本的符号预测和更高的能效。
- en: Table 1\. Comparison of Lightweight CNN Architectures on the ImageNet dataset.
    Note that we use bold to emphasize the models with the best accuracy, least parameters,
    and lowest MACs, with the respective values being also underlined for enhanced
    readability.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 轻量级 CNN 架构在 ImageNet 数据集上的比较。请注意，我们使用**粗体**来强调具有最佳准确性、最少参数和最低 MACs 的模型，并将相应的值也用*下划线*标出以增强可读性。
- en: '| Model | Top-1 | Top-5 | Params. (M) | MACs (G) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Top-1 | Top-5 | 参数 (M) | MACs (G) |'
- en: '| AlexNet (Krizhevsky et al., [2012](#bib.bib111)) | 57.1 | 80.3 | 60.9 | 0.725
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| AlexNet (Krizhevsky et al., [2012](#bib.bib111)) | 57.1 | 80.3 | 60.9 | 0.725
    |'
- en: '| ResNet-50 (He et al., [2016](#bib.bib78)) | 76.0 | 93.0 | 26.0 | 4.100 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-50 (He et al., [2016](#bib.bib78)) | 76.0 | 93.0 | 26.0 | 4.100 |'
- en: '| SqueezeNet (Iandola et al., [2017](#bib.bib99)) | 57.5 | 80.3 | 1.2 | 0.837
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| SqueezeNet (Iandola et al., [2017](#bib.bib99)) | 57.5 | 80.3 | 1.2 | 0.837
    |'
- en: '| SqueezeNext (Gholami et al., [2018](#bib.bib60)) | 59.1 | 82.6 | 0.7 | 0.282
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| SqueezeNext (Gholami et al., [2018](#bib.bib60)) | 59.1 | 82.6 | 0.7 | 0.282
    |'
- en: '| ShuffleNetV1-1.5 (Zhang et al., [2018c](#bib.bib249)) | 71.5 | - | 3.4 |
    0.292 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| ShuffleNetV1-1.5 (Zhang et al., [2018c](#bib.bib249)) | 71.5 | - | 3.4 |
    0.292 |'
- en: '| ShuffleNetV2-1.5 (Ma et al., [2018](#bib.bib139)) | 72.6 | 90.6 | 3.5 | 0.299
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| ShuffleNetV2-1.5 (Ma et al., [2018](#bib.bib139)) | 72.6 | 90.6 | 3.5 | 0.299
    |'
- en: '| 1.0-MobileNetV1 (Howard et al., [2017](#bib.bib90)) | 70.6 | - | 4.2 | 0.569
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 1.0-MobileNetV1 (Howard et al., [2017](#bib.bib90)) | 70.6 | - | 4.2 | 0.569
    |'
- en: '| MobileNetV2-1.4 (Sandler et al., [2018](#bib.bib169)) | 74.7 | - | 6.9 |
    0.585 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| MobileNetV2-1.4 (Sandler et al., [2018](#bib.bib169)) | 74.7 | - | 6.9 |
    0.585 |'
- en: '| MobileV3-S (Howard et al., [2019](#bib.bib89)) | 67.4 | - | 2.5 | 0.056 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| MobileV3-S (Howard et al., [2019](#bib.bib89)) | 67.4 | - | 2.5 | 0.056 |'
- en: '| MobileV3-L (Howard et al., [2019](#bib.bib89)) | 75.2 | - | 5.4 | 0.219 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| MobileV3-L (Howard et al., [2019](#bib.bib89)) | 75.2 | - | 5.4 | 0.219 |'
- en: '| MobileNeXt-1.0 (Zhou et al., [2020](#bib.bib254)) | 74.0 | - | 3.4 | 0.300
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| MobileNeXt-1.0 (Zhou et al., [2020](#bib.bib254)) | 74.0 | - | 3.4 | 0.300
    |'
- en: '| ShiftResNet-20 (Wu et al., [2018](#bib.bib218)) | 68.6 | - | 0.2 | 0.046
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ShiftResNet-20 (Wu et al., [2018](#bib.bib218)) | 68.6 | - | 0.2 | 0.046
    |'
- en: '| ShiftResNet-56 (Wu et al., [2018](#bib.bib218)) | 72.1 | - | 0.6 | 0.102
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| ShiftResNet-56 (Wu et al., [2018](#bib.bib218)) | 72.1 | - | 0.6 | 0.102
    |'
- en: '| ShiftNet-A (Wu et al., [2018](#bib.bib218)) | 70.1 | 89.7 | 4.1 | 1.400 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ShiftNet-A (Wu et al., [2018](#bib.bib218)) | 70.1 | 89.7 | 4.1 | 1.400 |'
- en: '| ShiftNet-B (Wu et al., [2018](#bib.bib218)) | 61.2 | 83.6 | 1.1 | 0.371 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ShiftNet-B (Wu et al., [2018](#bib.bib218)) | 61.2 | 83.6 | 1.1 | 0.371 |'
- en: '| FE-Net-1.0 (Chen et al., [2019a](#bib.bib27)) | 72.9 | - | 3.7 | 0.301 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| FE-Net-1.0 (Chen et al., [2019a](#bib.bib27)) | 72.9 | - | 3.7 | 0.301 |'
- en: '| FE-Net-1.37 (Chen et al., [2019a](#bib.bib27)) | 75.0 | - | 5.9 | 0.563 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| FE-Net-1.37 (Chen et al., [2019a](#bib.bib27)) | 75.0 | - | 5.9 | 0.563 |'
- en: '| AddressNet-20 (He et al., [2019b](#bib.bib83)) | 68.7 | - | 0.1 | 0.022 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| AddressNet-20 (He et al., [2019b](#bib.bib83)) | 68.7 | - | 0.1 | 0.022 |'
- en: '| AddressNet-44 (He et al., [2019b](#bib.bib83)) | 73.3 | - | 0.2 | 0.053 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| AddressNet-44 (He et al., [2019b](#bib.bib83)) | 73.3 | - | 0.2 | 0.053 |'
- en: '| AdderNet-Resnet18 (Chen et al., [2020b](#bib.bib22)) | 67.0 | 87.6 | 3.6
    | - |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| AdderNet-Resnet18 (Chen et al., [2020b](#bib.bib22)) | 67.0 | 87.6 | 3.6
    | - |'
- en: '| AdderNet-Resnet50 (Chen et al., [2020b](#bib.bib22)) | 74.9 | 91.7 | 7.7
    | - |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| AdderNet-Resnet50 (Chen et al., [2020b](#bib.bib22)) | 74.9 | 91.7 | 7.7
    | - |'
- en: '| DenseNet-169 (Huang et al., [2017](#bib.bib95)) | 76.2 | 93.2 | 14.0 | 3.500
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| DenseNet-169 (Huang et al., [2017](#bib.bib95)) | 76.2 | 93.2 | 14.0 | 3.500
    |'
- en: '| DenseNet-264 (Huang et al., [2017](#bib.bib95)) | 77.9 | 93.9 | 34.0 | 6.000
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| DenseNet-264 (Huang et al., [2017](#bib.bib95)) | 77.9 | 93.9 | 34.0 | 6.000
    |'
- en: '| CondenseNet (Huang et al., [2018](#bib.bib94)) | 71.0 | 90.0 | 2.9 | 0.274
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| CondenseNet (Huang et al., [2018](#bib.bib94)) | 71.0 | 90.0 | 2.9 | 0.274
    |'
- en: '| CondenseV2-A (Yang et al., [2021a](#bib.bib228)) | 64.4 | 84.5 | 2.0 | 0.046
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| CondenseV2-A (Yang et al., [2021a](#bib.bib228)) | 64.4 | 84.5 | 2.0 | 0.046
    |'
- en: '| CondenseV2-B (Yang et al., [2021a](#bib.bib228)) | 71.9 | 90.3 | 3.6 | 0.146
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| CondenseV2-B (Yang et al., [2021a](#bib.bib228)) | 71.9 | 90.3 | 3.6 | 0.146
    |'
- en: '| EfficientNet-B1 (Tan and Le, [2019a](#bib.bib192)) | 79.2 | 94.5 | 7.8 |
    0.700 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNet-B1 (Tan and Le, [2019a](#bib.bib192)) | 79.2 | 94.5 | 7.8 |
    0.700 |'
- en: '| EfficientNet-B7 (Tan and Le, [2019a](#bib.bib192)) | 84.4 | 97.1 | 66.0 |
    37.000 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNet-B7 (Tan and Le, [2019a](#bib.bib192)) | 84.4 | 97.1 | 66.0 |
    37.000 |'
- en: '| EfficientNet-X-B7 (Li et al., [2021](#bib.bib119)) | 84.7 | - | 73.0 | 91.000
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNet-X-B7 (Li et al., [2021](#bib.bib119)) | 84.7 | - | 73.0 | 91.000
    |'
- en: '| EfficientNetV2-S (Tan and Le, [2021](#bib.bib193)) | 83.9 | - | 24.0 | 8.800
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNetV2-S (Tan and Le, [2021](#bib.bib193)) | 83.9 | - | 24.0 | 8.800
    |'
- en: '| EfficientNetV2-M (Tan and Le, [2021](#bib.bib193)) | 85.1 | - | 55.0 | 24.000
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNetV2-M (Tan and Le, [2021](#bib.bib193)) | 85.1 | - | 55.0 | 24.000
    |'
- en: '| EfficientNetV2-L (Tan and Le, [2021](#bib.bib193)) | 85.7 | - | 121.0 | 53.000
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNetV2-L (Tan and Le, [2021](#bib.bib193)) | 85.7 | - | 121.0 | 53.000
    |'
- en: 2.3\. Transformer-based Series
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. Transformer-based Series
- en: 'Transformer models are widely used in NLP (Vaswani et al., [2017](#bib.bib204))
    and have recently obtained promising results in computer vision tasks (Liu et al.,
    [2021a](#bib.bib135), [2022](#bib.bib134); Zhang et al., [2023a](#bib.bib245)).
    Fig. [3](#S2.F3 "Figure 3 ‣ 2.3\. Transformer-based Series ‣ 2\. Lightweight Architecture
    Design ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey")
    shows the architecture of a typical vision transformer. Transformers are notable
    for having a significant drawback in that they require a large number of parameters
    and a high MAC to maintain their performance, which results in a significant amount
    of time needed for both the training and inference phases, particularly when the
    input sequence is long. Additionally, the computation and network structures inside
    transformers are more complex than those of CNNs. The huge number of FLOPs and
    parameters make practical inference and hardware deployment more difficult. To
    bridge the gap between transformers and real-world applications, efficient transformers
    will be discussed in the following sub-sections.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '变换器模型在自然语言处理（Vaswani 等，[2017](#bib.bib204)）中被广泛应用，并且最近在计算机视觉任务中获得了令人满意的结果（Liu
    等，[2021a](#bib.bib135)，[2022](#bib.bib134)；Zhang 等，[2023a](#bib.bib245)）。图 [3](#S2.F3
    "Figure 3 ‣ 2.3\. Transformer-based Series ‣ 2\. Lightweight Architecture Design
    ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey")
    显示了典型视觉变换器的架构。变换器因需要大量的参数和高 MAC 以维持其性能而显著地存在一个缺陷，这导致训练和推理阶段需要大量时间，特别是当输入序列较长时。此外，变换器内部的计算和网络结构比
    CNN 更加复杂。大量的 FLOPs 和参数使得实际推理和硬件部署变得更加困难。为了弥合变换器与实际应用之间的差距，接下来的子节将讨论高效的变换器。'
- en: '![Refer to caption](img/e2ed4cb49e2680de867f8cd4b7a959ba.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e2ed4cb49e2680de867f8cd4b7a959ba.png)'
- en: Figure 3\. Standard Vision Transformer, where $P=h\times w$, $h,w$ represents
    the height and the width of the images. $N$ is the number of image patches, $L$
    is the number of transformer blocks, and $d$ is the dimension (Mehta and Rastegari,
    [2022](#bib.bib146)).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 标准视觉变换器，其中 $P=h\times w$，$h$ 和 $w$ 分别表示图像的高度和宽度。$N$ 是图像块的数量，$L$ 是变换器块的数量，$d$
    是维度（Mehta 和 Rastegari，[2022](#bib.bib146)）。
- en: 2.3.1\. Lite attention module
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1\. 轻量级注意力模块
- en: 'To address the heavy MAC and huge computation requirements in the self-attention
    layers, Long-Short Range Attention (LSRA) (Wu et al., [2020](#bib.bib221)) was
    proposed to extract the global and local features separately, alleviating the
    attention computations in the feed-forward network (FFN). Child et al. (Child
    et al., [2019](#bib.bib32)) effectively exploit stride and fixed operations to
    form a sparse connectivity matrix. Linformer (Wang et al., [2020a](#bib.bib210))
    decomposes self-attention into several low-rank matrices using linear projection,
    reducing the complexity of self-attention from $N^{2}$ to $N$, where $N$ denotes
    the sequence length. Choromanski et al. (Choromanski et al., [2021](#bib.bib37))
    proposes a linear self-attention mechanism based on the FAVOR+ (fast attention
    with positive orthogonal random features) approach to construct an approximate
    softmax operation. FAVOR+ enables unbiased estimation of self-attention with low
    estimation variance, reducing spatial and temporal complexity. Reformer (Kitaev
    et al., [2020](#bib.bib109)) utilizes locality-sensitive hashing to replace dot
    product operations in attention. It directly decreases the computation requirements
    from $N^{2}$ to $Nlog(N)$, allowing longer sequence inputs to be considered. In
    addition, Reformer employs a reverse residual layer (Gomez et al., [2017](#bib.bib61))
    to save GPU memory by $L$ times (number of layers). Unlike traditional residuals,
    a reverse residual layer does not require activation data to be stored in each
    layer. The complexity of these efficient transformers is depicted in Table [2](#S2.T2
    "Table 2 ‣ 2.3.1\. Lite attention module ‣ 2.3\. Transformer-based Series ‣ 2\.
    Lightweight Architecture Design ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey").'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '为了应对自注意力层中的大量 MAC 和巨大的计算需求，Long-Short Range Attention (LSRA) (Wu et al., [2020](#bib.bib221))
    被提出，用于分别提取全局和局部特征，从而减轻前馈网络 (FFN) 中的注意力计算。Child et al. (Child et al., [2019](#bib.bib32))
    有效地利用步幅和固定操作来形成稀疏连接矩阵。Linformer (Wang et al., [2020a](#bib.bib210)) 通过线性投影将自注意力分解为几个低秩矩阵，将自注意力的复杂度从
    $N^{2}$ 降低到 $N$，其中 $N$ 表示序列长度。Choromanski et al. (Choromanski et al., [2021](#bib.bib37))
    提出了基于 FAVOR+ (fast attention with positive orthogonal random features) 方法的线性自注意力机制，以构建近似的
    softmax 操作。FAVOR+ 能够以低估计方差对自注意力进行无偏估计，从而减少空间和时间复杂度。Reformer (Kitaev et al., [2020](#bib.bib109))
    利用局部敏感哈希替代注意力中的点积操作，将计算需求直接从 $N^{2}$ 降低到 $Nlog(N)$，允许考虑更长的序列输入。此外，Reformer 采用了一种反向残差层
    (Gomez et al., [2017](#bib.bib61))，通过 $L$ 倍 (层数) 节省 GPU 内存。与传统的残差不同，反向残差层不需要在每一层存储激活数据。这些高效变压器的复杂性如表
    [2](#S2.T2 "Table 2 ‣ 2.3.1\. Lite attention module ‣ 2.3\. Transformer-based
    Series ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") 所示。'
- en: 'In addition, transformers stack many FFNs to obtain better-integrated features.
    Specifically, an FFN is a series of linear transformations that require a lot
    of calculations due to its dense connections. To tackle this issue, Mehta et al. (Mehta
    et al., [2018](#bib.bib144)) introduce grouped linear transformations (GLTs),
    which incorporate the concept of group convolution to make the transformer block
    more lightweight. Facing the same shortcoming from the group convolutions (as
    presented in Section [2.1.3](#S2.SS1.SSS3 "2.1.3\. Group convolution ‣ 2.1\. Prior
    Knowledge of Lightweight Architecture ‣ 2\. Lightweight Architecture Design ‣
    Lightweight Deep Learning for Resource-Constrained Environments: A Survey")),
    the hierarchical group transformation (HGT) (Mehta et al., [2020](#bib.bib145))
    aims to enhance the information flow between groups using a split layer and a
    skip connection operation. DeLighT (Mehta et al., [2021](#bib.bib143)) exploits
    GLTs to make feature dimensions wider and deeper, making it possible to use single-head
    attention instead of multi-head attention. This technique decreases the computation
    cost in the attention operation from $d_{m}N^{2}$ to $d_{o}N^{2}$, where $d_{m}$,
    and $d_{o}$ are the input dimension and output dimension respectively.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，变换器堆叠了许多前馈网络（FFNs）以获得更好的特征集成。具体来说，FFN是一系列线性变换，由于其密集的连接需要大量的计算。为了解决这个问题，Mehta
    等人（Mehta et al., [2018](#bib.bib144)）引入了分组线性变换（GLTs），它结合了分组卷积的概念，使得变换器块更加轻量化。面对来自分组卷积的相同缺点（如第
    [2.1.3](#S2.SS1.SSS3 "2.1.3\. Group convolution ‣ 2.1\. Prior Knowledge of Lightweight
    Architecture ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey") 节所述），层次分组变换（HGT）（Mehta et al.,
    [2020](#bib.bib145)）旨在使用拆分层和跳跃连接操作来增强组之间的信息流。DeLighT（Mehta et al., [2021](#bib.bib143)）利用GLTs来拓宽和加深特征维度，从而使得可以使用单头注意力而不是多头注意力。这项技术将注意力操作中的计算成本从
    $d_{m}N^{2}$ 降低到 $d_{o}N^{2}$，其中 $d_{m}$ 和 $d_{o}$ 分别是输入维度和输出维度。'
- en: Table 2\. The complexity of Efficient Transformers (Wang et al., [2020a](#bib.bib210)).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. Efficient Transformers 的复杂度（Wang et al., [2020a](#bib.bib210)）。
- en: '| Model | Complexity per Layer | Sequential Operation |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 每层复杂度 | 顺序操作 |'
- en: '| --- | --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Transformer (Vaswani et al., [2017](#bib.bib204)) | $O(N\textsuperscript{2})$
    | $O(N)$ |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Transformer（Vaswani et al., [2017](#bib.bib204)） | $O(N\textsuperscript{2})$
    | $O(N)$ |'
- en: '| Sparse Transformer (Child et al., [2019](#bib.bib32)) | $O(N\sqrt{N})$ |
    $O(1)$ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Sparse Transformer（Child et al., [2019](#bib.bib32)） | $O(N\sqrt{N})$ | $O(1)$
    |'
- en: '| Linformer (Wang et al., [2020a](#bib.bib210)) | $O(N)$ | $O(1)$ |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Linformer（Wang et al., [2020a](#bib.bib210)） | $O(N)$ | $O(1)$ |'
- en: '| Reformer (Kitaev et al., [2020](#bib.bib109)) | $O(Nlog(N))$ | $O(log(N))$
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Reformer（Kitaev et al., [2020](#bib.bib109)） | $O(Nlog(N))$ | $O(log(N))$
    |'
- en: 2.3.2\. Token sparsing
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2\. 令牌稀疏
- en: Vision transformer (ViT) (Dosovitskiy et al., [2021](#bib.bib45)) is the earliest
    work that applied transformers to solve an image classification task. It first
    splits an image into several patches and flattens them so that it can be passed
    in as an embedding sequence input to the transformer architecture. As the resolution
    of images in ImageNet is 224x224, their tokens require significantly more computation
    compared to other datasets with smaller resolutions, such as CIFAR-10 and CIFAR-100
    (32x32).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Vision transformer（ViT）（Dosovitskiy et al., [2021](#bib.bib45)）是最早将变换器应用于解决图像分类任务的工作。它首先将图像分割成若干补丁并将其展平，然后作为嵌入序列输入变换器架构。由于ImageNet中图像的分辨率为224x224，它们的令牌相比于其他分辨率较小的数据集（如CIFAR-10和CIFAR-100（32x32））需要显著更多的计算。
- en: To address this, T2T-ViT (Yuan et al., [2021](#bib.bib239)) observes that image
    splitting in transformers causes a loss of local relationships between tokens
    since there is no overlap between the tokens. Hence, they employ soft unfolding
    to combine the surrounding spatial tokens into high-dimensional manifolds, enabling
    smaller MLP sizes and increasing memory efficiency.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，T2T-ViT（Yuan et al., [2021](#bib.bib239)）观察到，变换器中的图像分割导致令牌之间局部关系的丧失，因为令牌之间没有重叠。因此，他们采用软展开技术将周围的空间令牌组合成高维流形，从而实现较小的MLP大小并提高内存效率。
- en: An extensive study on transformers (Naseer et al., [2021](#bib.bib149)) demonstrates
    that transformers are robust to patch drops, with only a slight decrease in accuracy
    when patches suffer from distortion or occlusions. DynamicViT (Rao et al., [2021](#bib.bib160))
    integrates a prediction module between transformer blocks to mask the less significant
    tokens. The prediction module is a binary decision mask in the range (0,1) that
    measures the importance of tokens. EViT (Liang et al., [2021a](#bib.bib123)) computes
    attentiveness scores from class tokens and other tokens and keeps top-K tokens,
    representing the highest positive correlation to the prediction. A-ViT (Yin et al.,
    [2022](#bib.bib234)) adaptively changes the number of tokens at different depths
    based on the complexity of the input image to reduce the inference time in ViT.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对transformers的广泛研究（Naseer et al., [2021](#bib.bib149)）表明，transformers对补丁丢失具有鲁棒性，当补丁遭受失真或遮挡时，准确性仅有轻微下降。DynamicViT（Rao
    et al., [2021](#bib.bib160)）在transformer块之间集成了一个预测模块，以屏蔽不太重要的标记。预测模块是一个范围在（0,1）之间的二元决策掩码，用于衡量标记的重要性。EViT（Liang
    et al., [2021a](#bib.bib123)）从类别标记和其他标记中计算注意力分数，并保留前K个标记，表示与预测的正相关性最高。A-ViT（Yin
    et al., [2022](#bib.bib234)）根据输入图像的复杂性自适应地改变不同深度的标记数量，以减少ViT的推理时间。
- en: 2.3.3\. Lightweight hybrid models
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 轻量级混合模型
- en: Due to the long-range dependence property inherent in attention mechanisms,
    transformer networks outperform CNN in accuracy. However, a transformer network
    lacks strong inductive biases (Dai et al., [2021a](#bib.bib39); Graham et al.,
    [2021](#bib.bib64); Liu et al., [2021a](#bib.bib135)), making it difficult to
    train and requires extra data augmentation and heavy regularization to maintain
    performance (Touvron et al., [2021](#bib.bib199)). On the other hand, CNN extracts
    features based on sliding windows, resulting in stronger inductive biases, which
    make models easier to train and have better generalizability. Interestingly, the
    aggregation of CNN and transformer networks (Wu et al., [2021](#bib.bib219); Srinivas
    et al., [2021](#bib.bib177); d’Ascoli et al., [2021](#bib.bib48); Xiao et al.,
    [2021](#bib.bib222)) produces versatile and powerful models. Since the hybrid
    models would have many parameters, DeiT (Touvron et al., [2021](#bib.bib199))
    applies KD and achieves better accuracy with less latency than CNN under comparable
    parameters. To improve data efficiency and simplify model complexity, the student
    model, a ViT model, added a distillation token to provide insight into the inductive
    biases of a CNN-based teacher model. MobileViT (Mehta and Rastegari, [2022](#bib.bib146))
    points out that transformer-based networks perform worse than CNN networks under
    similar parameters because they are still bulky. MobileViT employs MobileNetV2 (Sandler
    et al., [2018](#bib.bib169)) as the CNN backbone to obtain inductive biases and
    replaces the MBconv block in MobileNetV2 with a MobileViT block with unfolding
    and folding operations, which can compute long-range dependencies like a transformer.
    Similarly, MobileFormer (Chen et al., [2022a](#bib.bib28)) devises a parallel
    structure consisting of CNNs and transformers to achieve feature fusion. Inductive
    bias and the ability to capture global features are incorporated via two-way cross-attention.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于注意力机制固有的长程依赖特性，transformer网络在准确性上优于CNN。然而，transformer网络缺乏强大的归纳偏置（Dai et al.,
    [2021a](#bib.bib39); Graham et al., [2021](#bib.bib64); Liu et al., [2021a](#bib.bib135)），这使得训练变得困难，需要额外的数据增强和严格的正则化来保持性能（Touvron
    et al., [2021](#bib.bib199)）。另一方面，CNN基于滑动窗口提取特征，产生更强的归纳偏置，使得模型更易训练且具有更好的泛化能力。有趣的是，CNN和transformer网络的结合（Wu
    et al., [2021](#bib.bib219); Srinivas et al., [2021](#bib.bib177); d’Ascoli et
    al., [2021](#bib.bib48); Xiao et al., [2021](#bib.bib222)）产生了多功能且强大的模型。由于混合模型具有许多参数，DeiT（Touvron
    et al., [2021](#bib.bib199)）应用了KD，并在参数相当的情况下实现了比CNN更高的准确性和更低的延迟。为了提高数据效率和简化模型复杂度，学生模型ViT模型添加了一个蒸馏标记，以洞察基于CNN的教师模型的归纳偏置。MobileViT（Mehta
    and Rastegari, [2022](#bib.bib146)）指出，由于仍然笨重，transformer-based网络在类似参数下表现不如CNN网络。MobileViT采用MobileNetV2（Sandler
    et al., [2018](#bib.bib169)）作为CNN骨干网，以获得归纳偏置，并将MobileNetV2中的MBconv块替换为具有展开和折叠操作的MobileViT块，这可以像transformer一样计算长程依赖性。类似地，MobileFormer（Chen
    et al., [2022a](#bib.bib28)）设计了一个由CNN和transformer组成的并行结构来实现特征融合。归纳偏置和捕捉全局特征的能力通过双向交叉注意力来实现。
- en: 2.3.4\. Discussion and Summary
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4 讨论与总结
- en: 'Recent transformer models focus on lighter and more powerful architectures.
    This observation is apparent from Table [3](#S2.T3 "Table 3 ‣ 2.3.4\. Discussion
    and Summary ‣ 2.3\. Transformer-based Series ‣ 2\. Lightweight Architecture Design
    ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey"),
    where many recent transformers, such as T2T-ViT (Yuan et al., [2021](#bib.bib239))
    and DymViT-LVit (Rao et al., [2021](#bib.bib160)), are shown to achieve higher
    accuracy with significantly fewer parameters and lower FLOPS than the original
    ViT and ResNet-based CNNs. Specifically, we split the discussion into 3 sub-sections
    with bold headings.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '近期的 transformer 模型关注于更轻量且更强大的架构。从表 [3](#S2.T3 "Table 3 ‣ 2.3.4\. Discussion
    and Summary ‣ 2.3\. Transformer-based Series ‣ 2\. Lightweight Architecture Design
    ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey")
    可以明显看出，许多近期的 transformer，如 T2T-ViT（Yuan 等，[2021](#bib.bib239)）和 DymViT-LVit（Rao
    等，[2021](#bib.bib160)），在准确率上表现更高，参数更少，FLOPS 更低，相较于原始的 ViT 和基于 ResNet 的 CNN。具体而言，我们将讨论分为
    3 个小节，并用**粗体**标题。'
- en: VIT & KD transformer. Inspired by (Hinton et al., [2015](#bib.bib86)), several
    papers (Touvron et al., [2021](#bib.bib199); Lin et al., [2022a](#bib.bib126);
    Chen et al., [2021a](#bib.bib23)) apply KD to distill the inductive bias from
    the CNN-based teacher models to the transformer-based student models. For example,
    the design of DeiT-B (Touvron et al., [2021](#bib.bib199)) architecture integrates
    a CNN-based teacher model, a RegNetY-16G (Radosavovic et al., [2020](#bib.bib159)),
    and a transformer-based student model, ViT-B (Dosovitskiy et al., [2021](#bib.bib45)).
    Results show that DeiT-B outperforms all the models in terms of Top-1 accuracy,
    achieving an accuracy of 84.5%. Despite their stronger abilities, transformer-based
    student models require a large network to maintain their performance since they
    are harder to converge than CNN models (Dai et al., [2021a](#bib.bib39)).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: VIT & KD transformer。受到（Hinton 等，[2015](#bib.bib86)）的启发，几篇论文（Touvron 等，[2021](#bib.bib199)；Lin
    等，[2022a](#bib.bib126)；Chen 等，[2021a](#bib.bib23)）将 KD 应用于将基于 CNN 的教师模型的归纳偏差蒸馏到基于
    transformer 的学生模型中。例如，DeiT-B（Touvron 等，[2021](#bib.bib199)）架构的设计整合了一个基于 CNN 的教师模型，一个
    RegNetY-16G（Radosavovic 等，[2020](#bib.bib159)）和一个基于 transformer 的学生模型，ViT-B（Dosovitskiy
    等，[2021](#bib.bib45)）。结果显示，DeiT-B 在 Top-1 准确率方面优于所有模型，达到了 84.5% 的准确率。尽管它们具有更强的能力，但基于
    transformer 的学生模型需要较大的网络来保持其性能，因为它们比 CNN 模型更难以收敛（Dai 等，[2021a](#bib.bib39)）。
- en: 'VIT & CNN hybrid transformer. To overcome the shortcomings of the KD-based
    transformer models, the hybrid models (Dai et al., [2021a](#bib.bib39); Mehta
    and Rastegari, [2022](#bib.bib146); Chen et al., [2022a](#bib.bib28)) utilize
    both the convolution and transformer layers in the network. By doing so, they
    can obtain stronger inductive bias, leading to better convergence during training.
    Thus, hybrid models typically have fewer FLOPs and parameters. For example, Mobile-Former-96M (Chen
    et al., [2022a](#bib.bib28)) achieves the lowest FLOPs of 0.096G while MobileViT-XS (Mehta
    and Rastegari, [2022](#bib.bib146)) has the lowest parameters, which is 2.3 M.
    These hybrid models are extremely lightweight but sometimes, efficiency is achieved
    at the expense of accuracy, as we can observe from their performance in Table
    [2](#S2.T2 "Table 2 ‣ 2.3.1\. Lite attention module ‣ 2.3\. Transformer-based
    Series ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey"). For instance, MobileVit-XS has roughly half the total
    parameters of MobileViT-S, its counterpart, but its accuracy has significantly
    dropped by 3.6%. Another noteworthy observation shows that although Mobile-Former-96M
    achieves the lowest FLOPS, its parameter size was doubled, and accuracy is 2.0%
    lower compared to MobileVit-XS. This demonstrates that there is not always a correlation
    between FLOPs and total parameters and that lowering FLOPs appears to have a greater
    impact on accuracy than lowering parameters.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: VIT & Token sparsing transformer. Another series of efficient transformers (Yuan
    et al., [2021](#bib.bib239); Rao et al., [2021](#bib.bib160); Naseer et al., [2021](#bib.bib149);
    Liang et al., [2021a](#bib.bib123); Yin et al., [2022](#bib.bib234)) aim to prune
    the transformer structure efficiently via token sparsing. From the results, token
    sparsing-based models achieve a competitive accuracy with fewer parameters and
    FLOPs. It is worth noting that EViT-DeiT-S (k=0.7) (Liang et al., [2021a](#bib.bib123))
    can reach the highest throughput, 5408 images per second. Therefore, for a faster
    transformer-based model, such as accomplishing a real-time system, aggregating
    tokens into smaller amounts may provide a promising solution.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Due to their competitive accuracy and lightweight design (Kang et al., [2023](#bib.bib106);
    Luo et al., [2022](#bib.bib137)), lightweight transformer models are gaining popularity
    in a wide range of applications, such as edge AI and mobile AI; more details of
    efficient transformers can be found in (Han et al., [2023](#bib.bib75); Tay et al.,
    [2021](#bib.bib197)).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Comparison of Lightweight Transformer Models on the ImageNet dataset.
    We use bold to emphasize the models with the least parameters, highest throughput,
    lowest FLOPs, and best accuracy, with the corresponding values also underlined
    for enhanced readability.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '| Categories | Model |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Size) &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '| Params. (M) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '&#124; Throughput &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (image/s) &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '| FLOPs(G) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '&#124; ImageNet &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Top-1 &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '| CNN | ResNet50 (He et al., [2016](#bib.bib78)) | 224$\times$224 | 25.5 |
    - | 4.13 | 76.2 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| ResNet101 (He et al., [2016](#bib.bib78)) | 224$\times$224 | 44.6 | - | 7.9
    | 77.4 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| ResNet152 (He et al., [2016](#bib.bib78)) | 224$\times$224 | 60.2 | - | 11.0
    | 78.3 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| RegNetY-16GF (Radosavovic et al., [2020](#bib.bib159)) | 224$\times$224 |
    84.0 | 334.7 | - | 82.9 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| ViT | ViT-B/16 (Dosovitskiy et al., [2021](#bib.bib45)) | 384$\times$384
    | 86.6 | 85.9 | 17.6 | 77.9 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| ViT-L/16 (Dosovitskiy et al., [2021](#bib.bib45)) | 384$\times$384 | 307.0
    | 27.3 | 63.6 | 76.5 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| ViT & KD | DeiT-Ti (Touvron et al., [2021](#bib.bib199)) | 224$\times$224
    | 5.0 | 2536.5 | - | 72.2 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| DeiT-Ti (Touvron et al., [2021](#bib.bib199)) | 224$\times$224 | 6.0 | 2529.5
    | - | 74.5 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| DeiT-S (Touvron et al., [2021](#bib.bib199)) | 224$\times$224 | 22.0 | 936.2
    | 4.6 | 81.2 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| DeiT-B (Touvron et al., [2021](#bib.bib199)) | 224$\times$224 | 87.0 | 290.9
    | 17.6 | 83.4 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| DeiT-B (Touvron et al., [2021](#bib.bib199)) | 384$\times$384 | 87.0 | 85.8
    | 17.6 | 84.5 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| ViT & Token Sparsing | T2T-ViT-14 (Yuan et al., [2021](#bib.bib239)) | 224$\times$224
    | 21.5 | - | 5.2 | 81.5 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| T2T-ViT-14 (Yuan et al., [2021](#bib.bib239)) | 384$\times$384 | 21.5 | -
    | 17.1 | 83.3 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| T2T-ViT-19 (Yuan et al., [2021](#bib.bib239)) | 224$\times$224 | 39.2 | -
    | 8.9 | 81.9 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| DymViT-LViT-S/0.5 (Rao et al., [2021](#bib.bib160)) | 224$\times$224 | 26.9
    | - | 3.7 | 82.0 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| DymViT-LViT-M/0.7 (Rao et al., [2021](#bib.bib160)) | 224$\times$224 | 57.1
    | - | 8.5 | 83.8 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| EViT-DeiT-S (k=0.5) (Liang et al., [2021a](#bib.bib123)) | 224$\times$224
    | 22.0 | 4385 | 3.0 | 79.5 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| EViT-DeiT-S (k=0.7) (Liang et al., [2021a](#bib.bib123)) | 224$\times$224
    | 22.0 | 5408 | 2.3 | 78.5 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| EViT-LViT-S (k=0.5) (Liang et al., [2021a](#bib.bib123)) | 224$\times$224
    | 26.2 | 3603 | 3.9 | 82.5 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| EViT-LViT-S (k=0.7) (Liang et al., [2021a](#bib.bib123)) | 224$\times$224
    | 26.2 | 2954 | 4.7 | 83.0 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| A-ViT-T (Yin et al., [2022](#bib.bib234)) | 224$\times$224 | 5.0 | 3400 |
    0.8 | 71.0 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| A-ViT-S (Yin et al., [2022](#bib.bib234)) | 224$\times$224 | 22.0 | 1100
    | 3.6 | 78.6 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| ViT & CNN (Hybrid models) | Mobile-Former-96M (Chen et al., [2022a](#bib.bib28))
    | 224$\times$224 | 4.6 | - | 0.096 | 72.8 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| Mobile-Former-29(Chen et al., [2022a](#bib.bib28)) | 224$\times$224 | 11.4
    | - | 0.294 | 77.9 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| Mobile-Former-508M (Chen et al., [2022a](#bib.bib28)) | 224$\times$224 |
    14.0 | - | 0.508 | 79.3 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| MobileViT-XS (Mehta and Rastegari, [2022](#bib.bib146)) | 224$\times$224
    | 2.3 | - | 0.7 | 74.8 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| MobileViT-S (Mehta and Rastegari, [2022](#bib.bib146)) | 224$\times$224 |
    5.6 | - | - | 78.4 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: 3\. Fundamental methods in model compression
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we explore popular compression methods used in recent years
    and their improvements over time. These techniques encompass pruning (LeCun et al.,
    [1989](#bib.bib114); Hassibi et al., [1993](#bib.bib77); Frankle and Carbin, [2019](#bib.bib55);
    He et al., [2019a](#bib.bib82); Hu et al., [2023](#bib.bib93)), quantization (Dong
    et al., [2019](#bib.bib44); Faghri et al., [2020](#bib.bib50); Hubara et al.,
    [2016](#bib.bib98)), knowledge distillation (Hinton et al., [2015](#bib.bib86);
    Gou et al., [2021](#bib.bib63); Zhang et al., [2018b](#bib.bib251)), and neural
    architecture search (Liu et al., [2019b](#bib.bib131); Wu et al., [2019](#bib.bib217)),
    which are widely adopted for designing efficient models. We further unveil a detailed
    exploration of each method, offering deeper insights that stem from their distinctive
    characteristics.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们探讨了近年来流行的压缩方法及其随时间的改进。这些技术包括剪枝（LeCun 等，[1989](#bib.bib114)；Hassibi 等，[1993](#bib.bib77)；Frankle
    和 Carbin，[2019](#bib.bib55)；He 等，[2019a](#bib.bib82)；Hu 等，[2023](#bib.bib93)）、量化（Dong
    等，[2019](#bib.bib44)；Faghri 等，[2020](#bib.bib50)；Hubara 等，[2016](#bib.bib98)）、知识蒸馏（Hinton
    等，[2015](#bib.bib86)；Gou 等，[2021](#bib.bib63)；Zhang 等，[2018b](#bib.bib251)）和神经架构搜索（Liu
    等，[2019b](#bib.bib131)；Wu 等，[2019](#bib.bib217)），这些方法被广泛采用以设计高效的模型。我们进一步详细探讨了每种方法，提供了源自其独特特征的深入见解。
- en: 3.1\. Pruning
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 剪枝
- en: DL models frequently comprise numerous learnable parameters, requiring extensive
    training. Pruning methods aim to compress and expedite NNs by removing redundant
    weights. These pruning methods can be categorized as either unstructured or structured.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型通常包含大量可学习参数，需要广泛的训练。剪枝方法旨在通过移除冗余权重来压缩和加速神经网络。这些剪枝方法可以分为无结构剪枝和结构剪枝。
- en: 3.1.1\. Unstructured pruning
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 无结构剪枝
- en: 'Unstructured pruning aims to identify and eliminate individual weights from
    the network, regardless of where they are located. This method imposes no restrictions
    or rules on weight trimming. Specifically, the nodes with the removed weights
    are not physically removed from the network; instead, the weights are set to zero.
    Since this operation results in numerous zero multiplications, models can be significantly
    compressed for faster inference. As illustrated in Fig. [4](#S3.F4 "Figure 4 ‣
    3.1.2\. Structured Pruning ‣ 3.1\. Pruning ‣ 3\. Fundamental methods in model
    compression ‣ Lightweight Deep Learning for Resource-Constrained Environments:
    A Survey") (left), unstructured pruning may cause the pruned network to have an
    irregular structure. Early works in pruning, such as Optimal Brain Damage (LeCun
    et al., [1989](#bib.bib114)) and Optimal Brain Surgeon (Hassibi et al., [1993](#bib.bib77)),
    utilize second-order derivatives and Hessian matrices to assess the importance
    of weights in the network and subsequently prune them. While these methods demonstrate
    impressive performance, they demand substantial computational power.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '无结构剪枝旨在识别和消除网络中的单个权重，无论其位置如何。这种方法对权重修剪不施加任何限制或规则。具体而言，被移除权重的节点不会从网络中物理移除；相反，权重被设置为零。由于这一操作会导致大量的零乘法，模型可以显著压缩，从而加快推理速度。如图[4](#S3.F4
    "Figure 4 ‣ 3.1.2\. Structured Pruning ‣ 3.1\. Pruning ‣ 3\. Fundamental methods
    in model compression ‣ Lightweight Deep Learning for Resource-Constrained Environments:
    A Survey")（左）所示，无结构剪枝可能导致剪枝后的网络具有不规则的结构。早期的剪枝工作，如 Optimal Brain Damage（LeCun 等，[1989](#bib.bib114)）和
    Optimal Brain Surgeon（Hassibi 等，[1993](#bib.bib77)），利用二阶导数和 Hessian 矩阵来评估网络中权重的重要性，并随后进行剪枝。尽管这些方法表现出色，但它们需要大量的计算能力。'
- en: To this end, Dong et al. (Dong et al., [2017](#bib.bib42)) introduce a method
    that restricts the computation of second-order derivatives. This approach does
    not require the computation of the Hessian matrix over all parameters; instead,
    it focuses on specific layers of the model. Similarly, Frankle et al. (Frankle
    and Carbin, [2019](#bib.bib55)) propose the lottery ticket hypothesis, where they
    attempt to find more manageable and pruned sub-networks while maintaining a performance
    comparable to the original network. In their approach, they prune the nodes, subsequently
    restoring the original pre-training initialization values of the untouched nodes,
    and repeat this cycle until a certain level of sparsity is achieved.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，Dong 等（Dong 等，[2017](#bib.bib42)）引入了一种限制二阶导数计算的方法。这种方法不需要对所有参数计算Hessian矩阵；而是专注于模型的特定层。同样，Frankle
    等（Frankle 和 Carbin，[2019](#bib.bib55)）提出了彩票票据假设，他们试图找到更易管理和剪枝的子网络，同时保持与原始网络相当的性能。在他们的方法中，他们剪枝节点，随后恢复未触及节点的原始预训练初始化值，并重复这一过程，直到达到一定的稀疏水平。
- en: However, unstructured pruning can significantly reduce accuracy when weights
    are pruned during the training process before the network converges. Unfortunately,
    the pruned connections cannot be restored. To address this limitation, Guo et
    al. (Guo et al., [2016](#bib.bib68)) introduce a splicing algorithm capable of
    recovering previously deleted connections that are discovered to be important
    at any point in time. Furthermore, Namhoon et al. (Lee et al., [2019](#bib.bib115))
    propose a single-shot network pruning approach in which they prune the network
    before the training begins. Instead of analyzing the model’s final weights after
    training, they examine the response of the loss function to variance scaling during
    initialization. This innovative approach allows the network to be pruned just
    once before training, providing a more convenient and effective pruning method.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当在网络尚未收敛的训练过程中剪枝权重时，非结构化剪枝可能会显著降低准确性。不幸的是，被剪枝的连接无法恢复。为了解决这一限制，Guo 等（Guo 等，[2016](#bib.bib68)）引入了一种拼接算法，能够恢复在任何时候被发现重要的先前删除的连接。此外，Namhoon
    等（Lee 等，[2019](#bib.bib115)）提出了一种单次网络剪枝方法，其中他们在训练开始之前对网络进行剪枝。他们并不是在训练后分析模型的最终权重，而是检查损失函数对初始化期间方差缩放的响应。这种创新的方法允许在训练之前对网络进行一次剪枝，提供了一种更便捷有效的剪枝方法。
- en: 3.1.2\. Structured Pruning
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 结构化剪枝
- en: 'Structured pruning methods remove pruned components from a pre-trained network
    and preserve its regular structure, as shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.2\.
    Structured Pruning ‣ 3.1\. Pruning ‣ 3\. Fundamental methods in model compression
    ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey")
    (right). Common structured pruning methods include filter pruning (He et al.,
    [2019a](#bib.bib82), [2018](#bib.bib81), [2020](#bib.bib80); Zhang and Freris,
    [2023](#bib.bib250)) and channel pruning (He et al., [2017](#bib.bib84); Peng
    et al., [2019](#bib.bib155); Hu et al., [2023](#bib.bib93)).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '结构化剪枝方法从预训练网络中去除剪枝的组件，并保留其常规结构，如图[4](#S3.F4 "Figure 4 ‣ 3.1.2\. Structured
    Pruning ‣ 3.1\. Pruning ‣ 3\. Fundamental methods in model compression ‣ Lightweight
    Deep Learning for Resource-Constrained Environments: A Survey")（右图）所示。常见的结构化剪枝方法包括滤波器剪枝（He
    等， [2019a](#bib.bib82)，[2018](#bib.bib81)，[2020](#bib.bib80)；Zhang 和 Freris，[2023](#bib.bib250)）和通道剪枝（He
    等， [2017](#bib.bib84)；Peng 等，[2019](#bib.bib155)；Hu 等，[2023](#bib.bib93)）。'
- en: 1) Filter pruning. Most pruning approaches rely on the ”smaller-norm-less-important”
    criterion, which involves pruning filters with lower norm values in the network
    (Li et al., [2017](#bib.bib116); Ye et al., [2018](#bib.bib233)). However, He
    et al. (He et al., [2019a](#bib.bib82)) point out the limitations of this criterion-based
    approach. They propose a novel technique for calculating the Geometric Median
    of filters within the same layer. By doing so, they prune filters that make the
    most replaceable contribution instead of those with comparatively less contribution.
    Criterion-based pruning methods tend to reduce model capacity due to fixed pruning
    thresholds. To address this, He et al. (He et al., [2020](#bib.bib80)) introduce
    learnable pruning thresholds for each layer using a differentiable criterion sampler,
    which can be updated during training. Additionally, Zhang et al. (Zhang and Freris,
    [2023](#bib.bib250)) propose an adaptive pruning threshold based on the sensitivity
    of the loss to the threshold value.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 过滤器剪枝。大多数剪枝方法依赖于“较小的范数更不重要”标准，这涉及到剪枝网络中范数值较低的过滤器（Li et al., [2017](#bib.bib116);
    Ye et al., [2018](#bib.bib233)）。然而，He et al.（He et al., [2019a](#bib.bib82)）指出了这种基于标准的方法的局限性。他们提出了一种新颖的技术，通过计算同一层内过滤器的几何中位数来进行剪枝。这样，他们剪枝那些贡献最可替代的过滤器，而不是那些相对贡献较少的过滤器。基于标准的剪枝方法往往由于固定的剪枝阈值而降低模型容量。为了解决这个问题，He
    et al.（He et al., [2020](#bib.bib80)）引入了可学习的剪枝阈值，通过可微分的标准采样器对每一层进行阈值调整，这些阈值可以在训练过程中更新。此外，Zhang
    et al.（Zhang and Freris, [2023](#bib.bib250)）提出了一种基于损失对阈值值敏感性的自适应剪枝阈值。
- en: '![Refer to caption](img/b72d28023b1beed70fe125ae42bf8ecd.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b72d28023b1beed70fe125ae42bf8ecd.png)'
- en: 'Figure 4\. Illustration of pruning methods: unstructured pruning (left), and
    structured pruning (right). Pruned components are shown in white color. Take note
    of the change in the pruned component’s output dimensions.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. 剪枝方法的示意图：非结构化剪枝（左），结构化剪枝（右）。剪枝的组件以白色显示。注意剪枝组件输出维度的变化。
- en: 2) Channel pruning. Channel pruning is another effective approach for reducing
    FLOPs and inference time, complementing filter pruning. He et al. (He et al.,
    [2017](#bib.bib84)) first implement channel pruning by focusing on eliminating
    redundant channels by evaluating the L1 norm. Peng et al. (Peng et al., [2019](#bib.bib155))
    take a different approach by using the Hessian matrix to model inter-channel dependencies
    and select channels using sequential quadratic programming. For more complex modules
    like group convolutions and depthwise convolutions, Liu et al. (Liu et al., [2021c](#bib.bib132))
    introduce a layer grouping mechanism to search for coupled channels automatically.
    The importance of these channels is calculated based on Fisher’s information.
    CATRO (Hu et al., [2023](#bib.bib93)) leverages feature space discrimination to
    assess the joint impact of multiple channels while consolidating the layer-by-layer
    impact of preserved channels.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 通道剪枝。通道剪枝是另一种有效的减少 FLOPs 和推理时间的方法，补充了过滤器剪枝。He et al.（He et al., [2017](#bib.bib84)）首先通过评估
    L1 范数来实施通道剪枝，重点在于去除冗余通道。Peng et al.（Peng et al., [2019](#bib.bib155)）采取了不同的方法，通过使用
    Hessian 矩阵来建模通道间依赖关系，并使用序列二次规划选择通道。对于更复杂的模块，如组卷积和深度卷积，Liu et al.（Liu et al., [2021c](#bib.bib132)）引入了层分组机制，以自动搜索耦合通道。这些通道的重要性基于
    Fisher 信息进行计算。CATRO（Hu et al., [2023](#bib.bib93)）利用特征空间的区分来评估多个通道的联合影响，同时巩固保留通道的逐层影响。
- en: 3.1.3\. Comparison of pruning methods
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3. 剪枝方法的比较
- en: 'Table [4](#S3.T4 "Table 4 ‣ 3.1.3\. Comparison of pruning methods ‣ 3.1\. Pruning
    ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep Learning for
    Resource-Constrained Environments: A Survey") displays the accuracy after pruning
    and the corresponding pruned FLOPs of the various structure pruning methods. While
    one might initially assume that the best-performing methods prune the highest
    number of FLOPs, in reality, we often perceive the ”best” as those that effectively
    balance the trade-off between pruned FLOPs and the associated drop in accuracy.
    For instance, while GFP attains the highest pruned accuracy, its reduction in
    FLOPs is limited to 50.6%. In contrast, ASTER removes the most FLOPs, yet its
    pruned accuracy falls short of being the best. In summary, filter and channel
    pruning methods can efficiently decrease the FLOPs while maintaining similar accuracy.
    We advocate choosing a pruning method that seamlessly integrates with the current
    network architecture, prioritizing ease of implementation. For example, if the
    network’s feature map boasts over a thousand channels but only uses a few filters,
    opting for channel pruning would be more beneficial.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. Comparison of different pruning methods using ResNet50 on the ImageNet
    dataset. The methods that achieve the highest percentage of pruned FLOPs are marked
    in bold.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Method (30%) | Baseline (%) | Pruned Acc. (%) | Pruned FLOPs (%) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| - | ResNet50 | 76.15 | - | - |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| Filter | SFP (He et al., [2018](#bib.bib81)) | 76.15 | 74.61 (-1.54) | 41.8
    |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| FPGM (He et al., [2019a](#bib.bib82)) | 76.15 | 75.59 (-0.56) | 42.2 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| LFPC (He et al., [2020](#bib.bib80)) | 76.15 | 74.46 (-1.69) | 60.8 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| ASTER (Zhang and Freris, [2023](#bib.bib250)) | 76.15 | 75.27 (-0.88) | 63.2
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| Channel | CCP (Peng et al., [2019](#bib.bib155)) | 76.15 | 75.50 (-0.65)
    | 48.8 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| GFP (Liu et al., [2021c](#bib.bib132)) | 76.79 | 76.42 (-0.37) | 50.6 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| SCP (Kang and Han, [2020](#bib.bib107)) | 75.89 | 74.20 (-1.69) | 54.3 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| CATRO (Hu et al., [2023](#bib.bib93)) | 75.98 | 75.84 (-0.14) | 45.8 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/33075eac13ea5c0990feb97e108b9d6a.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Symmetric (left) and asymmetric (right) quantization representation (Gholami
    et al., [2022](#bib.bib59)). Note that r represents the real value, S represents
    the real-valued scaling factor, and Z represents the integer zero point.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Quantization
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pruning is an efficient way to compress the model. However, after pruning,
    the remaining weights, typically stored as full-precision 32-bit floating-point
    numbers (float32), still demand significant memory. To address this, quantization (Gray
    and Neuhoff, [1998](#bib.bib65)), a technique that allows parameters to be represented
    with reduced bit precision, becomes a desirable solution. Specifically, quantization
    maps weights and activations to a set of finite numbers through a calibration
    process that determines potential values using a symmetric or asymmetric representation.
    As depicted in Fig. [5](#S3.F5 "Figure 5 ‣ 3.1.3\. Comparison of pruning methods
    ‣ 3.1\. Pruning ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep
    Learning for Resource-Constrained Environments: A Survey"), both methods define
    a range [$\alpha$, $\beta$], in symmetric quantization, -$\alpha$ = $\beta$, whereas
    in the asymmetric quantization, -$\alpha\neq\beta$.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '剪枝是一种高效的模型压缩方法。然而，在剪枝之后，剩余的权重通常以全精度的 32 位浮点数（float32）存储，仍然需要大量内存。为了解决这个问题，量化（Gray
    and Neuhoff, [1998](#bib.bib65)）成为一种可取的解决方案，量化是一种允许以减少位精度表示参数的技术。具体来说，量化通过一个校准过程将权重和激活映射到有限的数集，通过对称或非对称表示来确定潜在值。如图
    [5](#S3.F5 "Figure 5 ‣ 3.1.3\. Comparison of pruning methods ‣ 3.1\. Pruning ‣
    3\. Fundamental methods in model compression ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") 所示，两种方法定义了一个范围 [$\alpha$, $\beta$]，在对称量化中，- $\alpha$
    = $\beta$，而在非对称量化中，- $\alpha \neq \beta$。'
- en: 'The calibration of this range, as outlined by Gholami et al. (Gholami et al.,
    [2022](#bib.bib59)), falls into two categories: dynamic and static calibration.
    The first one is accurate but computationally demanding, as it computes $[\alpha,\beta]$
    for each feature map. The latter is a computationally lighter alternative because
    it calculates the range based on typical values after several iterations, albeit
    with less accuracy. Both dynamic and static calibration are pivotal for optimizing
    the quantization process.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 该范围的校准，如 Gholami 等人（Gholami et al., [2022](#bib.bib59)）所述，分为动态校准和静态校准两类。动态校准准确但计算开销大，因为它为每个特征图计算
    $[\alpha,\beta]$。静态校准则计算开销较小，因为它基于几次迭代后的典型值计算范围，尽管精度较低。动态和静态校准对于优化量化过程都是至关重要的。
- en: Quantization theory has been applied to NN from various perspectives over time.
    For instance, Gupta et al. (Gupta et al., [2015](#bib.bib71)) introduce the use
    of fixed-point numbers during the model’s training process to enhance the algorithm’s
    noise tolerance. They also employ stochastic rounding as an alternative to the
    round-to-nearest strategy to counteract the adverse effects of fixed-point numbers.
    In another approach, Faghri et al. (Faghri et al., [2020](#bib.bib50)) introduce
    two adaptive quantization methods, Adaptive Level Quantization (ALQ) and Adaptive
    Multiplier Quantization (AMQ), which update their compression method in parallel
    during training to quantize the gradients in data-parallel stochastic gradient
    descent adaptively. This adaptation aims to reduce communication costs between
    the processors. Lastly, Wang et al. (Wang et al., [2022a](#bib.bib208)) treat
    the quantization problem as a differentiable lookup operation. They jointly optimized
    both the network and the associated tables during training.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 量化理论从不同角度被应用于神经网络。例如，Gupta 等人（Gupta et al., [2015](#bib.bib71)）在模型训练过程中引入了固定点数，以增强算法的噪声容忍度。他们还采用了随机舍入作为替代最近舍入策略的方法，以抵消固定点数的不利影响。在另一种方法中，Faghri
    等人（Faghri et al., [2020](#bib.bib50)）提出了两种自适应量化方法，分别是自适应水平量化（ALQ）和自适应乘法器量化（AMQ），这两种方法在训练过程中并行更新其压缩方法，以自适应地量化数据并行随机梯度下降中的梯度。这种适应旨在减少处理器之间的通信成本。最后，Wang
    等人（Wang et al., [2022a](#bib.bib208)）将量化问题视为一个可微分的查找操作。他们在训练过程中联合优化了网络和相关表格。
- en: 3.2.1\. Half-precision and Mixed-precision training
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 半精度和混合精度训练
- en: Mixed-precision training involves using lower-precision values while retaining
    full-precision values for crucial information (Micikevicius et al., [2018](#bib.bib148)).
    For instance, in a notable series of works, HAWQ (Dong et al., [2019](#bib.bib44))
    implements an automatic approach based on the Hessian of the model to determine
    the optimal mixed-precision settings for weight values. Subsequently, the HAWQ-V2
    model (Dong et al., [2020](#bib.bib43)) introduces mixed-precision quantization
    for activation values. The HAWQ-V3 model (Yao et al., [2021](#bib.bib231)) further
    improves it by focusing on integer-only quantization. Interestingly, Liu et al. (Liu
    et al., [2021b](#bib.bib133)) introduce a method that utilizes a linear combination
    of multiple low-bit vectors to approximate a full-precision vector. This approach
    achieves ”mixed-precision training” with a single precision level by varying the
    number of vectors to approximate different weights.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Quantization using fewer bits
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In an early work by Banner et al. (Banner et al., [2018](#bib.bib10)), the quantization
    of weights, activations, and most gradient streams in all layers of an NN is performed
    using 8-bit precision by replacing traditional batch-norm with ranged batch-norm
    layers. Another technique proposed by Wang et al. (Wang et al., [2018](#bib.bib209))
    allows matrix and convolutional operations to also be implemented using 8-bit
    numbers. Furthermore, there have also been methods that use ternary values to
    quantize an NN. In an important work done by Liu et al., TWN (Liu et al., [2023](#bib.bib130))
    manages to constrain weights to +1, 0, and -1 values, achieving a 16x compression
    of the model. This idea is extended in TTQ (Zhu et al., [2017](#bib.bib257)),
    where the positive and negative weights use two different learnable scales $w_{1}$
    and $w_{2}$, resulting in possible values of $-w_{1}$, $0$, and $w_{2}$.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: More aggressive approaches have sought to reduce quantization levels further
    by implementing NN binarization. This approach uses binary values instead of floating-point
    or integer values for faster computations, lower memory usage, and reduced power
    consumption. Courbariaux et al.’s pioneering work (Hubara et al., [2016](#bib.bib98))
    binarizes networks by restricting the weights and activations to either +1 or
    -1, determining the final values by evaluating the sign of the real values. Variations
    of this work include topologies such as XNOR-Net (Rastegari et al., [2016](#bib.bib161))
    and the Least Squares method (Pouransari et al., [2020](#bib.bib156)), which introduce
    an additional activation layer after the binary convolutions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3\. Quantization Aware Training (QAT)
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the early stages of quantization research, a prevalent approach was first
    to train an unquantized model, apply a quantization process, and then retrain
    or fine-tune the model to achieve an acceptable level of accuracy. This methodology,
    known as Post-training Quantization (PTQ), proved to be an effective strategy
    for achieving significant compression, especially when the pre-trained model has
    ample representational capacity. The success of PTQ lies in its ability to balance
    compression gains and maintain satisfactory model accuracy, making it a pivotal
    technique in model optimization and deployment. However, quantization is a lossy
    process, which can lead to a significant drop in model accuracy. To address this
    issue, Jacob et al. (Jacob et al., [2018](#bib.bib100)) introduced QAT, a technique
    that computes inference-time quantization errors during the model training stage,
    allowing the model to become aware of these errors and make adjustments accordingly.
    This process simulates inference-time errors through a process known as FakeQuant.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Improvements to the core QAT technique have been explored by introducing learnable
    clipping scalars (Choi et al., [2018](#bib.bib34)). In a recent development, Sakr
    et al. (Sakr et al., [2022](#bib.bib168)) achieved state-of-the-art performance
    by identifying the MSE-minimizing clipping scalars and implementing 4-bit quantization.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4\. Comparison of quantization methods
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [5](#S3.T5 "Table 5 ‣ 3.2.4\. Comparison of quantization methods ‣ 3.2\.
    Quantization ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep
    Learning for Resource-Constrained Environments: A Survey") compares the performance
    of quantization methods on the ImageNet dataset, emphasizing the trade-off between
    compression and accuracy loss. Notably, binarized networks aiming for a 32x compression
    and speedup show significant accuracy drops. On the other hand, approaches with
    4-bit quantization, except (Liu et al., [2021b](#bib.bib133)), result in little
    loss of accuracy and can, therefore, be a good choice of precision for quantization.
    However, theoretical compression and speedup expectations may not align with actual
    results due to additional operations like quantization and dequantization. This
    may explain why some works opt not to conduct an in-depth analysis of the quantized
    model size, although (Liu et al., [2021b](#bib.bib133)) does provide such an analysis
    and successfully achieves an approximately 8-fold reduction in the model size
    (42.56 MB to 5.37 MB).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Table 5\. Comparison of several quantization methods using different levels
    of precision to compress a ResNet18 on the ImageNet dataset.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Initial Accuracy. (%) | Quantized accuracy (%) | Precision |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| QIL (Jung et al., [2019](#bib.bib105)) | 70.2 | 70.1 (-0.1) | 4-bit |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| (Liu et al., [2021b](#bib.bib133)) | 69.8 | 61.7 (-8.1) | 4-bit |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| LLT (Wang et al., [2022a](#bib.bib208)) | 69.8 | 70.4 (+0.6) | 4-bit |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| LLT (Wang et al., [2022a](#bib.bib208)) | 69.8 | 69.5 (-0.3) | 3-bit |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| HAWQ-V3 (Yao et al., [2021](#bib.bib231)) | 71.5 | 68.5 (-3.0) | MP |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| TWN (Liu et al., [2023](#bib.bib130)) | 65.4 | 61.8 (-3.6) | 2-bit |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| TTQ (Zhu et al., [2017](#bib.bib257)) | 69.6 | 66.6 (-3.0) | 2-bit |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| XNOR-Net (Rastegari et al., [2016](#bib.bib161)) | 69.3 | 51.2 (-18.1) |
    1-bit |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| Least Squares (Pouransari et al., [2020](#bib.bib156)) | 69.6 | 63.4 (-6.2)
    | 1-bit |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: 3.3\. Knowledge Distillation (KD)
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'KD is a model compression technique designed to transfer knowledge from a large
    network to a smaller one (Hinton et al., [2015](#bib.bib86); Gou et al., [2021](#bib.bib63)).
    Its simplest form is illustrated in Fig. [6](#S3.F6 "Figure 6 ‣ 3.3\. Knowledge
    Distillation (KD) ‣ 3\. Fundamental methods in model compression ‣ Lightweight
    Deep Learning for Resource-Constrained Environments: A Survey")(a), where the
    larger model is referred to as the teacher and the smaller model as the student.
    In the approach proposed by Hinton et al. (Hinton et al., [2015](#bib.bib86)),
    the teacher model is initially trained to generate soft labels. Then, the training
    of the student model leverages ground-truth labels and the teacher’s predictions
    on the same data. This combination enables the student to attain performance comparable
    to the teacher using fewer parameters.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'KD algorithms can be categorized into three types: offline, online, and self-distillation,
    as illustrated in Fig. [6](#S3.F6 "Figure 6 ‣ 3.3\. Knowledge Distillation (KD)
    ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep Learning for
    Resource-Constrained Environments: A Survey"). The key distinction lies in the
    teacher’s definition and training strategy. For instance, in offline distillation,
    teacher and student training processes are performed sequentially, whereas in
    online distillation, the teacher can continue or initiate training alongside the
    student. On the other hand, in self-distillation, the student becomes its own
    teacher.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8116c3e209a39e9e43d54eddbe734e9d.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. (a) Offline Distillation (Hinton et al., [2015](#bib.bib86)). (b)
    Online Distillation (Zhang et al., [2018b](#bib.bib251)). (c) Self-Distillation (Zhang
    et al., [2019b](#bib.bib247)). We use orange lines to indicate the gradient update.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1\. Offline Distillation
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most of the earlier KD works fall under the category of offline distillation.
    In offline distillation, a pre-trained teacher model is required, as seen in the
    case of the vanilla KD (Hinton et al., [2015](#bib.bib86)). While offline distillation
    is relatively easy to implement, it comes with the unavoidable overhead of time
    and computational resources required to train a large teacher model first.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Various methods have been explored to enhance KD algorithms, including introducing
    alternative loss functions such as contrastive-based loss (Tian et al., [2020](#bib.bib198))
    and minimizing the maximum mean discrepancy between models (Huang and Wang, [2019](#bib.bib97)).
    Significant size disparities between teacher and student models can impact results,
    leading Zhao et al. (Zhao et al., [2022](#bib.bib253)) to redefine logit distillation
    by decoupling the influence of target and non-target classes. Lin et al. (Lin
    et al., [2022b](#bib.bib127)) address the semantic information gap in KD by dynamically
    distilling each pixel of the teacher features to all spatial locations of the
    student features, guided by a similarity measure from the transformer.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Recently, SimKD (Chen et al., [2022b](#bib.bib20)) proposed a straightforward
    distillation approach, reusing the teacher’s classifier and aligning intermediate
    features with an L2 loss. SemCKD (Chen et al., [2021c](#bib.bib21)) involves student
    learning through feature embedding, preserving feature similarities in the intermediate
    layers of the teacher network.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Online Distillation
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Offline distillation can be problematic when obtaining a pre-trained large teacher
    model is not feasible, rendering many of the previously mentioned methods unusable.
    Online distillation introduces an end-to-end training strategy that overcomes
    this limitation by concurrently training the teacher and student networks, challenging
    the traditional concept of a ”single large teacher” (Zhang et al., [2018b](#bib.bib251);
    Guo et al., [2020](#bib.bib67); Li et al., [2023](#bib.bib118)).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'The Deep Mutual Learning (DML) algorithm, proposed in (Zhang et al., [2018b](#bib.bib251)),
    eliminates the need for a pre-trained teacher in the KD process, as depicted in
    Fig. [6](#S3.F6 "Figure 6 ‣ 3.3\. Knowledge Distillation (KD) ‣ 3\. Fundamental
    methods in model compression ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey")(b). Instead, this approach advocates simultaneous learning
    of a cohort of networks, with each network incorporating the predictions of the
    others in its loss functions. This change enables all networks in the cohort to
    benefit from each other’s knowledge, even improving networks that are large enough
    to have acted as teachers in a conventional KD process. These large networks can
    enhance their results with knowledge distilled from other untrained, smaller networks.
    Further refinements of this approach have been made in (Guo et al., [2020](#bib.bib67);
    Li et al., [2023](#bib.bib118)). Online distillation techniques can also incorporate
    adversarial concepts. Zhang et al. (Zhang et al., [2021](#bib.bib244)) propose
    an adversarial co-distillation approach that employs Generative Adversarial Networks
    (GANs) to explore ”divergent examples” and enhance knowledge transfer.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, online distillation has demonstrated notable efficacy in scenarios
    requiring generating pseudo labels for data. The widely adopted mean teacher framework (Tarvainen
    and Valpola, [2017](#bib.bib196)) introduces the concept of employing two identical
    models; specifically, the teacher model has the same structure as the student
    model. The primary idea involves updating the teacher’s weights through an exponential
    moving average (EMA) of the student’s weights. In various unsupervised contexts
    (Deng et al., [2021](#bib.bib41); Yu et al., [2022](#bib.bib238)), this principle
    is leveraged to create pseudo labels for training the student via a supervised
    loss. Notably, each prediction made by the teacher model can be viewed as an ensemble
    incorporating the current and past iterations of the student model, rendering
    it inherently more robust and stable.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3\. Self-Distillation
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As depicted in Fig. [6](#S3.F6 "Figure 6 ‣ 3.3\. Knowledge Distillation (KD)
    ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep Learning for
    Resource-Constrained Environments: A Survey")(c), self-distillation techniques
    involve the process of KD, where a model distills knowledge from itself. In this
    scenario, during the training process, a single instance of the model simultaneously
    acts as both the teacher and student. Strategies in this distillation approach
    encompass using the same model saved at different epochs (Yang et al., [2019](#bib.bib226))
    and leveraging various model layers for self-instruction (Yuan et al., [2020](#bib.bib240);
    Hou et al., [2019](#bib.bib88)).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al. (Zhang et al., [2019b](#bib.bib247)) pioneered self-distillation
    from deeper to shallower layers of the model. Their innovation improves results
    and reduces training time by eliminating the need for additional networks. Similarly,
    Hou et al. (Hou et al., [2019](#bib.bib88)) harness knowledge transfer through
    attention maps from deeper layers. Yang et al. (Yang et al., [2019](#bib.bib226))
    use the weights of previous iterations for knowledge distillation instead of using
    deeper layers of the model. Kim et al. (Kim et al., [2021](#bib.bib108)) elevate
    self-distillation with a sophisticated progressive framework, incorporating adaptive
    gradient rescaling for hard example mining.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: In an important study, Yuan et al. (Yuan et al., [2020](#bib.bib240)) challenge
    the foundations of conventional KD by introducing the Teacher-free KD (Tf-KD).
    They explore the intricate relationship between KD and Label Smoothing Regularization
    (LSR) techniques and suggest employing self-training or manually-designed regularization
    terms for improving the student model’s accuracy when faced with the difficulty
    of a powerful teacher model. Additionally, self-distillation methods have successfully
    been applied to domain adaptation tasks (Yoon et al., [2022](#bib.bib235); Sultana
    et al., [2022](#bib.bib181)).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6\. KD methods evaluated on the CIFAR-100 dataset. ↑ indicates an improvement
    over the baseline. Note: The pair of accuracies in the online distillation methods
    represent the teacher and student models’ performances after distillation.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '| Methodology | Algorithm | Teacher (baseline) | Student (baseline) | Improved
    Accuracy |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: '| Offline distillation | SimKD (Chen et al., [2022b](#bib.bib20)) | ResNet32
    (79.42) | ResNet8 (73.09) | 78.08 (4.99 ↑) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| SemCKD (Chen et al., [2021c](#bib.bib21)) | ResNet32 (79.42) | ResNet8 (73.09)
    | 76.23 (3.14 ↑) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| SRRL (Yang et al., [2021c](#bib.bib227)) | ResNet32 (79.42) | ResNet8 (73.09)
    | 75.39 (2.30 ↑) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| SemCKD (Chen et al., [2021c](#bib.bib21)) | ResNet32 (79.42) | WRN-40-2 (76.35)
    | 79.29 (2.94 ↑) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '| Online distillation | DML (Zhang et al., [2018b](#bib.bib251)) | WRN-28-10
    (78.69) | WRN-28-10 (78.69) | 80.28, 80.08 (1.39 ↑) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: '| DML (Zhang et al., [2018b](#bib.bib251)) | WRN-28-10 (78.69) | ResNet32 (68.99)
    | 78.96, 70.73 (1.74 ↑) |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: '| FFSD (Li et al., [2023](#bib.bib118)) | ResNet56 (71.55) | ResNet32 (69.96)
    | 75.78, 74.85 (4.90 ↑) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
- en: '| KDCL (Guo et al., [2020](#bib.bib67)) | WRN-16-2 (72.20) | ResNet32 (69.90)
    | 75.50, 74.30 (4.40 ↑) |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '| Self-distillation | SD (Yang et al., [2019](#bib.bib226)) | – | ResNet32
    (68.39) | 71.29 (2.90↑) |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| Tf-KD (Yuan et al., [2020](#bib.bib240)) | – | ResNet18 (75.87) | 77.10 (1.23↑)
    |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| PS-KD (Kim et al., [2021](#bib.bib108)) | – | ResNet18 (75.82) | 79.18 (3.36↑)
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| Tf-KD (Yuan et al., [2020](#bib.bib240)) | – | ShuffleNetV2 (70.34) | 72.23
    (1.89↑) |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: 3.3.4\. Comparison of KD methods
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [6](#S3.T6 "Table 6 ‣ 3.3.3\. Self-Distillation ‣ 3.3\. Knowledge Distillation
    (KD) ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey") compares several distillation
    methods and analyzes their respective outcomes on the CIFAR-100 dataset. These
    findings challenge the perception that offline distillation methods are outdated
    and too simplistic. For example, SimKD recently achieved state-of-the-art performance
    with a ResNet32 as the teacher and a ResNet8 as the student. Additionally, our
    analysis demonstrates the efficacy of online distillation, showcasing instances
    where a teacher can improve its own performance despite instructing a student
    with significantly lower accuracy. Notably, the WRN-28-10 achieves a 0.27% (78.69%
    to 78.96%) improvement even when paired with a ResNet32 that initially achieves
    nearly 10% (78.69% to 68.99%) less accuracy. Furthermore, self-distillation emerges
    as a promising strategy, necessitating only one model, exemplified by a ResNet18
    achieving 3.36% gains through the PS-KD method, albeit not surpassing the improvements
    seen in other methods. To address this limitation, it is advisable to complement
    self-distillation with other forms of distillation or compression methods for
    enhanced performance. Ultimately, a comparison between methodologies is hard,
    as performance heavily depends on implementation details. Therefore, we advocate
    for adopting a strategy that is easier to implement and aligns most logically
    with the ongoing development objectives.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Neural Architecture Search (NAS)
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even if DL techniques excel in numerous tasks, it is true that they often depend
    heavily on human expertise to find the best trade-off between performance and
    complexity. Optimizing a model can be exceptionally challenging due to a multitude
    of choices involving hyperparameters, network layers, hardware devices, etc.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'In response to this challenge, Automated Machine Learning (AutoML), which aims
    to automatically build ML systems without much requirement for ML expertise and
    human intervention, is being extensively studied (He et al., [2021](#bib.bib79)).
    Several mature tools exist for AutoML applications, such as Auto-WEKA (Kotthoff
    et al., [2019](#bib.bib110)) and Auto-sklearn (Feurer et al., [2019](#bib.bib52)).
    In this paper, our primary focus is NAS, a crucial section of AutoML. The fundamental
    concepts of NAS are outlined as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Search Space: The search space encompasses the possible combinations of hyperparameters,
    including kernel size, channel size, convolution stride, depth, and more. A larger
    search space that covers a wider range of possibilities increases the likelihood
    of discovering a highly accurate model. However, a vast search space can lead
    to longer search times.'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Search Algorithm: This refers to the algorithm used to find the optimal combination
    within the search space. Common strategies include random search, grid search,
    reinforcement learning (RL) (Zoph and Le, [2017](#bib.bib258); Tan et al., [2019a](#bib.bib191)),
    evolutionary algorithms (EA) (Real et al., [2017](#bib.bib163); Xue et al., [2023](#bib.bib225)),
    and gradient optimization (Liu et al., [2019b](#bib.bib131); Wu et al., [2019](#bib.bib217)).
    An efficient search strategy can significantly reduce search time, especially
    in extensive search spaces.'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance Evaluation Strategy: This defines the criteria for selecting the
    neural architecture that maximizes specific performance metrics among all the
    models generated through NAS. Performance metrics, such as Top-1 or Top-5 scores
    for classification and average precision (AP) or F1 scores for object detection,
    reflect the suitability of the hyperparameter combinations for the given task.'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this section, we explore various approaches in the field of NAS, including
    RL-based NAS, EA-based NAS, Gradient-based NAS, and other related works, all based
    on different search algorithms.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1\. RL-based NAS
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this pioneering work of adopting RL for NAS, Zoph et al. (Zoph and Le, [2017](#bib.bib258))
    utilize a recurrent neural network (RNN) controller (called an agent) to generate
    candidate hyperparameters for constructing child networks (environments). The
    child network then receives a score (reward) based on metrics like accuracy and
    AP. The RNN controller updates itself according to the reward and refines the
    hyperparameters for the child network iteratively. A detailed process is illustrated
    in Fig. [7](#S3.F7 "Figure 7 ‣ 3.4.1\. RL-based NAS ‣ 3.4\. Neural Architecture
    Search (NAS) ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep
    Learning for Resource-Constrained Environments: A Survey"). Moving forward, MnasNet (Tan
    et al., [2019a](#bib.bib191)) considers latency and employs RL to identify Pareto
    optimal solutions that balance latency and performance. This approach also introduces
    a factorized hierarchical search space, which organizes the CNN into predefined
    blocks and explores different connections and operations within each block.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/46271ec97023d377983183c0c1be1aa8.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. NAS with RL (Zoph and Le, [2017](#bib.bib258)).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2\. EA-based NAS
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To enhance model performance, Real et al. (Real et al., [2017](#bib.bib163))
    introduce an EA-based approach for NAS. This method continuously evolves model
    architectures. The evolution process begins with workers generating an initial
    set of models, forming what is known as a population. During the evolution step,
    two models are randomly selected from the population, and their accuracy on the
    validation set is evaluated. The weaker-performing model is removed from the population,
    while the better model becomes the parent model. In the mutation step, the parent
    model is duplicated, producing two identical copies. One of these copies is reintroduced
    into the population, while the other undergoes mutation to create a new model,
    referred to as the child model. Subsequently, the workers train and assess the
    child model’s performance before adding it back to the population. This process
    is iteratively repeated, resulting in increasingly improved models within the
    population.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: However, a random search approach within a large population can be highly inefficient
    when dealing with a vast search space. To address this concern, Sun et al. (Sun
    et al., [2020](#bib.bib184)) develop an encoding mechanism that maps CNN features
    to numerical values. This enables the acceleration of the evolutionary process
    by using a CNN architecture as an input to the Random Forest. More recently, Xue
    et al. (Xue et al., [2023](#bib.bib225)) proposed a queue mechanism to reduce
    the population and incorporate crossover and mutation operators to enhance the
    diversity of child networks.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3\. Gradient-based NAS
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The core concept of gradient-based NAS involves the transformation of a discrete
    search space into a continuous one, enabling the application of gradient descent
    techniques to discover optimal model architectures automatically. Inferring latency
    after each training is inefficient for the proposed NAS network, especially for
    research institutes with limited resources. Additionally, using gradient-based
    NAS methods is deemed more appropriate when formulating hardware-aware NAS approaches.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e775615b717867552b79070685023695.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
- en: Figure 8\. The DNAS pipeline in FBNet (Wu et al., [2019](#bib.bib217)).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'DARTS (Liu et al., [2019b](#bib.bib131)) presents an efficient architecture
    search algorithm based on gradient descent that avoids black-box search problems.
    It converts structural parameters from discrete to continuous, making them differentiable.
    As a result, DARTS provides accurate, efficient, and differentiable NAS. Inspired
    by works such as MnasNet (Tan et al., [2019a](#bib.bib191)), DARTS (Liu et al.,
    [2019b](#bib.bib131)), and NetAdaptV1 (Yang et al., [2018](#bib.bib229)), FBNet (Wu
    et al., [2019](#bib.bib217)) is a hardware-aware NAS breakthrough discovered through
    the differentiable NAS (DNAS) pipeline, depicted in Fig. [8](#S3.F8 "Figure 8
    ‣ 3.4.3\. Gradient-based NAS ‣ 3.4\. Neural Architecture Search (NAS) ‣ 3\. Fundamental
    methods in model compression ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey"). In FBNet, nine distinct blocks are designed within the
    layer, and 22 layers are utilized to construct a stochastic supernetwork, which
    is optimized using stochastic gradient descent (SGD). Additionally, FBNet devises
    a layer-wise search space, enabling each layer to select a different block. Furthermore,
    in order to reduce the layer-wise search space with lower latency, a latency lookup
    table is employed, and a latency-aware loss term is incorporated into the overall
    loss function, given by:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $L(a,w_{a})=CE(a,w_{a})\cdot\alpha\log(LAT(a))^{\beta}.$ |  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: where $a$ and $w_{a}$ denote the network architecture and network parameters
    for a specific device, while $CE$ represents the cross-entropy loss. $LAT$ stands
    for the latency of the architecture on the target device, which is determined
    using a lookup table. The parameters $\alpha$ and $\beta$ serve as the magnitude
    of the overall loss function and the latency term, respectively. For further details
    and related work on FBNet, please refer to (Wan et al., [2020](#bib.bib206); Dai
    et al., [2021b](#bib.bib38)).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.4\. Other NAS related works
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Numerous other NAS algorithms have been proposed. One example is the Symbolic
    DNN-Tuner (Fraccaroli et al., [2022](#bib.bib54)), which introduces an automatic
    software system for determining optimal tuning actions following each network
    training session using probabilistic symbolic rules. The system comprises a module
    for data processing, search space exploration, and Bayesian optimization. The
    controller module manages the training process and decides the tuning actions.
    Besides finding the best combination from a vast search space, testing the proposed
    combination network is also time-consuming. Measuring the latency of the entire
    model on the target device each time can be highly inefficient.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this issue, NetAdaptV1 (Yang et al., [2018](#bib.bib229)) employs
    an adaptive algorithm that considers energy consumption and memory usage, enabling
    it to respond more realistically to hardware constraints. The approach involves
    the creation of a layer-wise lookup table, as shown in Fig. [9](#S3.F9 "Figure
    9 ‣ 3.4.4\. Other NAS related works ‣ 3.4\. Neural Architecture Search (NAS) ‣
    3\. Fundamental methods in model compression ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey"), simplifying the search complexity for a pre-trained
    network. In this setup, the latency of each layer is pre-measured, and a lookup
    table is constructed to record latency based on the layer’s structure. For instance,
    as illustrated in Fig. [9](#S3.F9 "Figure 9 ‣ 3.4.4\. Other NAS related works
    ‣ 3.4\. Neural Architecture Search (NAS) ‣ 3\. Fundamental methods in model compression
    ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey"),
    Layer 1 consists of 3 channels with 4 filters and a measured latency of 6 ms,
    and Layer 2 consists of 4 channels with 6 filters and a measured latency of 4
    ms. The total latency is calculated as the sum of the latency for each layer,
    resulting in a total latency of 10 ms (6 + 4).'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, NetAdaptV2 (Yang et al., [2021b](#bib.bib230)) introduces Channel-Level
    Bypass Connections (CBCs), which combine depth and layer width in the original
    search space to enhance the efficiency of both training and testing. Moreover,
    Abdelfattah et al. (Abdelfattah et al., [2021](#bib.bib3)) leverages pruning-at-initialization (Lee
    et al., [2019](#bib.bib115)) and incorporates six zero-cost proxies for NAS proposal
    scoring. This innovative approach requires only a single minibatch of data and
    a single forward/backward propagation pass instead of full training, resulting
    in a more efficient NAS process.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a9465c85b0cad7680677a0de1cf8d7d7.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
- en: Figure 9\. Layer-wise look up table (Yang et al., [2018](#bib.bib229)).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Discussion and Summary
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section encapsulates a summary of the preceding discussion on model compression.
    Additionally, it provides valuable practical tips and guidance, aiming to offer
    actionable insights for effective implementation and application in relevant contexts.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Pruning. Although unstructured pruning methods (LeCun et al., [1989](#bib.bib114);
    Frankle and Carbin, [2019](#bib.bib55)) have made significant strides in parameter
    reduction, their irregular structures frequently pose compatibility issues with
    hardware accelerators. Therefore, structure pruning (He et al., [2019a](#bib.bib82),
    [2018](#bib.bib81); Hu et al., [2023](#bib.bib93)) has emerged as a preferable
    alternative, primarily due to its regular structure. Notably, modern DL frameworks,
    such as PyTorch and TensorFlow, have integrated built-in functionalities that
    facilitate the seamless implementation of structure pruning. This streamlined
    integration enhances the ease and efficiency with which structure pruning techniques
    can be applied.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Quantization. When considering quantization, the choice of technique depends
    on the hardware environment where the model will be deployed. Hardware specifications
    play a critical role, turning quantization from an optional optimization into
    an imperative requirement. For instance, specific MCUs or edge TPUs exclusively
    support integer operations, making full integer quantization essential for model
    implementation. TensorFlow Lite (TF-Lite) (Google, [2023](#bib.bib62)) effectively
    addresses this need, reducing the model size by up to four times and significantly
    accelerating inference by more than three times. In hardware with low-power CPUs,
    an 8-bit integer quantization strategy is often recommended, as CPUs exhibit exceptional
    computational efficiency when handling integer operations instead of floating-point
    values. Notably, when using 16-bit float quantization, values are subsequently
    de-quantized back to 32-bit float representations during execution on the CPU.
    For a deeper analysis of hardware support for quantization and facilitating libraries,
    see (Liang et al., [2021b](#bib.bib122)).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge distillation. KD techniques have significantly enhanced NNs by leveraging
    insights from other models. In practice, the offline KD process (Hinton et al.,
    [2015](#bib.bib86)) can be effectively utilized when training a large model is
    viable. On the other hand, online distillation stands forth as a promising solution.
    For example, the DML process (Zhang et al., [2018b](#bib.bib251)) has shown remarkable
    results without necessitating a pre-trained teacher model, making it adaptable
    to multi-GPU training with several small models. In situations characterized by
    a scarcity of labeled data or noisy labels, the mean teacher framework has emerged
    as a valuable and effective solution. Moreover, self-distillation and ongoing
    advancements in KD (Lin et al., [2022b](#bib.bib127); Zhao et al., [2022](#bib.bib253))
    open numerous possibilities for exploration and offer different options for the
    definition of the teacher and student networks.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: NAS. While both RL-based NAS (Tan et al., [2019a](#bib.bib191)) and EA-based
    NAS (Sun et al., [2020](#bib.bib184)) have demonstrated their capacity to achieve
    impressive accuracy, it is important to note that their training demands extensive
    resources and time, often spanning days or weeks and involving hundreds of GPUs.
    This resource-intensive nature has contributed to a relative decline in the number
    of studies in these areas. Therefore, when confronted with GPU limitations, gradient-based
    algorithms like DARTS (Liu et al., [2019b](#bib.bib131)) and FBNet (Wu et al.,
    [2019](#bib.bib217)), which introduce continuity into the search space, can be
    considered. This approach significantly reduces the training time. Alternative
    options include approaches like ”once for all” NAS (Cai et al., [2020a](#bib.bib14)),
    which tailor the extensive network into subnetworks optimized for different target
    devices. However, if ample computational resources are at hand, RL-based and EA-based
    NAS methods are viable options, and they also offer superior performance compared
    to gradient-based NAS (Ren et al., [2021](#bib.bib164)). Additionally, when memory
    footprint, energy consumption, and latency are key considerations, the hardware-aware
    NAS concepts introduced by studies like FBNet (Wu et al., [2019](#bib.bib217)),
    NetAdapt (Yang et al., [2018](#bib.bib229)), and NetAdaptV2 (Yang et al., [2021b](#bib.bib230))
    may be particularly relevant.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion. In conclusion, model compression approaches have their strengths
    and limitations. Quantization is a relatively simple but proven effective compression
    technique in many cases. It is essential to first match the selected quantization
    approach with the specific hardware requirements for floating-point or integer
    values. In scenarios where hardware constraints permit, starting with a 16-bit
    float quantization is often a prudent initial step. If there is a need for more
    substantial model compression, two viable options emerge. First, model pruning
    offers an effective solution, substantially reducing redundant network parameters
    while preserving performance integrity. This is particularly valuable when working
    with resource-constrained environments. Secondly, the KD framework proves advantageous,
    especially in scenarios with ample unlabeled data, as often encountered in applications
    like autonomous driving. The mean teacher structure, in particular, is a valuable
    tool for generating pseudo labels from unlabeled data, effectively incorporating
    this additional information into training and enhancing overall model performance.
    Finally, NAS can also be considered, particularly for tasks where it excels the
    most, such as image classification, where it can potentially discover optimal
    network architectures tailored to specific requirements. The choice among these
    approaches should be guided by the specific demands of the task and the available
    computational resources.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Hardware Acceleration of Deep Learning Models
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the advancements in GPUs, DL has risen to the forefront of artificial intelligence
    technology. DL models, such as CNNs, are computationally intensive. Hence, hardware
    acceleration is becoming imperative to render DL applications feasible and practical.
    In this section, we present an overview of prominent hardware accelerators of
    DL models. We then introduce typical dataflow and data locality optimization techniques,
    as well as widely adopted DL libraries. Finally, we discuss algorithms that employ
    a co-design approach for software/hardware deployment.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Hardware Architectures
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hardware accelerators for DL models encompass a range of options, including
    GPUs and CPUs based on temporal architecture, as well as FPGAs and ASICs rooted
    in spatial architecture. The basic components of a hardware accelerator are an
    arithmetic logic unit (ALU), a control unit, and a local memory unit (cache unit).
    In the temporal architecture, the control and local memory units are centralized,
    and the processing elements (PEs) only contain the ALUs. Data is accessed sequentially
    from centralized memory to PEs, with no interactions between the PEs (Capra et al.,
    [2020](#bib.bib17)). In contrast, spatial architecture entails PEs equipped with
    control units, ALUs, and local memory (register file). This allows independent
    data processing and direct communication between PEs.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1\. Temporal Architecture
  id: totrans-314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Temporal architectures are often adopted in general-purpose platforms, like
    CPUs and GPUs, which are optimized for sequential tasks and parallel tasks, respectively.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Central processing unit (CPU). CPUs process input data into usable information
    output, executing calculations sequentially through serial computing. A recent
    CPU-based acceleration technique, SLIDE (Chen et al., [2020a](#bib.bib18)), which
    leverages C++ OpenMP to combine intelligent randomized algorithms with multi-core
    parallelism and workload optimization, demonstrates that employing smart algorithms
    on a CPU can potentially achieve better speed than using an NVIDIA-V100 GPU.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Graphics processing unit (GPU). GPUs are designed for parallel computation.
    Their architecture may consist of thousands of cores. Hence, GPUs excel at parallel
    computing, enabling them to process multiple instructions simultaneously, making
    them highly efficient for tasks that involve simple and repetitive computations.
    Given that DL models often entail extensive matrix addition and multiplication
    operations, GPUs have emerged as the primary accelerators for the development
    of DL. Their parallel processing capabilities make them instrumental in accelerating
    DL tasks.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2\. Spatial Architecture
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: By utilizing PEs, spatial architectures often seen in FPGAs and application-specific
    integrated circuits (ASICs), the necessity for repeated and redundant access to
    external memory is reduced, leading to lower energy consumption.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: FPGAs. FGPAs consist of programmable logic blocks with logic gates capable of
    performing computations. Reprogrammable by nature, they can accelerate various
    DL structures effectively and better support pruning methods. Additionally, FPGAs
    can directly implement algorithms without any decoding and interpretation process.
    To enhance AI applications using FPGAs, Qi et al. (Qi et al., [2022](#bib.bib157))
    emphasize key concepts of parallel computing and demonstrate how these concepts
    can be implemented in FPGAs. Roggen et al. (Roggen et al., [2022](#bib.bib165))
    successfully implement digital signal processing (DSP) algorithms, such as filter
    finite impulse response filters on FPGA platforms, thereby improving support for
    wearable computing. For more references on FPGA AI applications, consult (Nechi
    et al., [2023](#bib.bib150); Seng et al., [2021](#bib.bib172)).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: ASICs. ASICs, customized for specific electronic systems, outperform FPGAs with
    superior speed, lower power consumption, and higher throughput. TPUs, prominent
    ASICs tailored for AI applications (Jouppi et al., [2017](#bib.bib104)), excel
    in efficiently executing matrix operations, a pivotal capability advantageous
    in deep learning computations with prevalent expansive matrix multiplications.
    In a recent development, the newly introduced TPU-v3 can connect 1024 TPU chips
    through a 2-D torus network (Kumar et al., [2019](#bib.bib112)). This innovation
    enhances parallelism and enables execution on more TPU-v3 accelerator cores through
    spatial partitioning and weight update-sharing mechanisms. The supercomputer TPU-v4 (Jouppi
    et al., [2023](#bib.bib103)) further elevates the capabilities by increasing the
    number of TPU chips to 4096\. TPU-v4 also introduces optical circuit switches
    (OCSes) that dynamically restructure their interconnection topology to improve
    scalability, accessibility, and utilization. As a result, TPU-v4 offers a 2.7
    times improvement in performance/watt and a tenfold increase in speed compared
    to TPU-v3.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3\. Discussion of CNN Accelerators
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CPUs are generally not well-suited for training and inference of typical DL
    models due to low FLOPs performance. GPUs, which can support parallel computation
    with thousands of cores, excel in parallel computing and are widely adopted in
    various AI applications. However, GPUs are known for their high power consumption,
    rendering them unsuitable for edge devices and IoT applications. On the other
    hand, FPGAs and ASICs offer more energy-efficient acceleration options for edge
    AI applications. The choice between FPGAs and ASICs often depends on the specific
    requirements. FPGAs are preferred for AI products that require rapid development
    or are produced in small batches. ASICs are more suitable for AI products that
    undergo mass production, especially highly mature or customized ones. For projects
    with ample budget, TPUs can be the top choice. TPUs boast exceptional computational
    power, making them ideal for handling extensive models with large batch sizes,
    such as the GPT-4 (OpenAI, [2023](#bib.bib152)) and LLaMA (Touvron et al., [2023a](#bib.bib200)),
    significantly reducing training and inference times.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Dataflow and the Data Locality Optimization
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The computational complexity and data storage demands of CNNs pose significant
    challenges to computational performance and energy efficiency. These challenges
    are particularly pronounced in smaller devices with limited memory, including
    constrained on-chip buffers (SRAM) and off-chip memory (DRAM). To address these
    issues, optimizing dataflow is crucial for enhancing memory and energy efficiency.
    The dataflow process in deep models generally consists of three main steps. Firstly,
    DL models are stored in off-chip memory, often referred to as external memory.
    Secondly, when convolution kernels are required, they are fetched from on-chip
    buffers. Finally, PEs are employed to execute the MACs.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1\. Dataflow types
  id: totrans-326
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hardware accelerators of DL models have different types of dataflow based on
    their applications and can be categorized into pipeline-like dataflow (Li et al.,
    [2016](#bib.bib117); Lin and Chang, [2017](#bib.bib129)), DaDianNao-like dataflow (Luo
    et al., [2016](#bib.bib138); Chen et al., [2014b](#bib.bib29)), Systolic-array-like
    dataflow (Jouppi et al., [2017](#bib.bib104); Wei et al., [2017](#bib.bib214);
    Zhang et al., [2018a](#bib.bib243)), and streaming-like dataflow (Du et al., [2017](#bib.bib46);
    Guo et al., [2017](#bib.bib66)).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline-like dataflow. In this dataflow, the input pixels (the pixels of the
    feature map) are passed on to individual PEs, and the model’s weights (representing
    model parameters) are fixed on each PE. Notably, the partial sum is then forwarded
    to the subsequent PE. This approach offers substantial parallelism, facilitating
    the concurrent processing of data by multiple stages, thereby enhancing computational
    efficiency. However, tasks are executed sequentially, with each stage dependent
    on the completion of the previous one, potentially resulting in increased latency.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: DaDianNao-like dataflow. In this dataflow, each PE can function like a neuron,
    processing input pixels in a way akin to an NN. Specifically, input pixels are
    routed to each PE, and the model’s weights are embedded within each PE. The computed
    partial sums are then aggregated using an adder tree. This type of dataflow can
    accommodate different kernel sizes, making it capable of handling intricate and
    irregular model structures. However, this dataflow approach is energy-intensive
    and demands substantial hardware resources due to the model’s complexity.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Systolic-array-like dataflow. This dataflow sequentially conveys input pixels
    and weights into the PEs, with PEs cascaded to enhance computational efficiency.
    Subsequently, an adder tree is employed to aggregate the partial sums. This dataflow
    approach optimizes the utilization of hardware resources, improves overall hardware
    efficiency, and mitigates timing issues in large designs. However, finding an
    appropriate mapping for CNNs onto a systolic array can be challenging.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Streaming-like dataflow. In this dataflow, input pixels are continuously sent
    to the following PE without pausing or needing intermediate storage, with weights
    being fixed on each PE. Subsequently, the adder tree accumulates the partial sums.
    This dataflow is particularly suitable for streaming data, such as audio and video
    processing, due to its high throughput and low latency. Nonetheless, applications
    requiring complex operations between stages or that rely on previous results may
    require additional processing and design. Fig. [10](#S4.F10 "Figure 10 ‣ 4.2.1\.
    Dataflow types ‣ 4.2\. Dataflow and the Data Locality Optimization ‣ 4\. Hardware
    Acceleration of Deep Learning Models ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") compares the types of dataflow.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a3d802ce2320e2b6adf0a1967c567bd1.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
- en: Figure 10\. A comparison of dataflow types (Hsu et al., [2020](#bib.bib91)).
    PE stands for processing element.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Data Locality Optimization
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CNNs deliver exceptional performance characterized by high throughput and energy
    consumption. However, their performance can be restricted by limited on-chip memory.
    Therefore, an effective locality optimization mechanism is essential. Data locality
    optimization focuses on devising a dataflow schedule that maximizes data reuse
    utilization and minimizes data movement. A prevalent approach involves applying
    loop transformation techniques, such as loop unrolling, loop tiling, and loop
    interchange, to optimize NN deployment. These techniques help maximize hardware
    utilization and minimize memory traffic, addressing the limitations of on-chip
    memory constraints.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Loop unrolling (Booshehri et al., [2013](#bib.bib13); Huang and Leng, [1999](#bib.bib96))
    is a method that involves expanding loop iterations into multiple sequential instructions.
    This technique significantly reduces the number of loop iterations in the CNN,
    resulting in faster CNN operations and improved hardware utilization through increased
    parallelization. However, it is important to note that loop unrolling may lead
    to code bloat, increased memory usage, and higher storage requirements, especially
    for larger CNN models.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Loop tilling (Qiu et al., [2016](#bib.bib158); Zhang et al., [2015](#bib.bib242);
    Stoutchinin et al., [2019](#bib.bib178)) involves partitioning the input data
    into several blocks to enable parallel computations for CNN acceleration. For
    example, an original input data of size $224\times 224\times 3$ can be divided
    into smaller blocks of size $112\times 112\times 3$. These smaller blocks are
    processed sequentially to mitigate buffer loading and memory constraints. This
    technique effectively adapts to limited on-chip memory and significantly enhances
    cache locality. However, for modern accelerators, such as GPUs, where memory access
    patterns are already optimized for high throughput, loop tilling may add extra
    complexity without appreciable gains in performance.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Loop interchange (Mezdour et al., [2023](#bib.bib147); Xu et al., [2023](#bib.bib224))
    involves changing the order of loops within a nested loop with the aim of improving
    data locality and extracting parallelism. Specifically, the order of the loops
    is optimized to allow each iteration of the outermost loop to utilize the same
    cache line, hence reducing memory access. Loop interchange can also accelerate
    CNN models by increasing the use of operators like addition and multiplication.
    Notably, some algorithms have complex intrinsic properties and special meanings
    in their loop orders. Therefore, altering the loop order may yield meaningless
    results and reduce performance.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we introduce typical types of dataflow and provide an overview
    of various mechanisms for data locality optimization. More in-depth details can
    be found in  (Fu et al., [2023](#bib.bib57); Wolf and Lam, [1991](#bib.bib215)).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Deep Learning Libraries
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To facilitate the deployment of a DL model, it is also essential to use DL libraries
    that provide high-level APIs to simplify the implementation, design, and training
    of complex NNs. We introduce several popular DL libraries supporting GPU acceleration
    and the auto gradient system.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow (Abadi et al., [2016](#bib.bib2)) supports static and dynamic graphs,
    allowing users to select the most suitable mode. With this flexibility, TensorFlow
    supports the research and development of custom DL models. Additionally, TensorFlow
    also provides extensive APIs for DL model implementation. For instance, a TensorFlow
    model can be converted into a TensorFlow-Lite (TF-Lite) (David et al., [2021](#bib.bib40))
    model, a smaller, more efficient ML model format that can be run on mobile and
    edge devices.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch (Paszke et al., [2019](#bib.bib154)) is a framework renowned for its
    remarkable capacity to facilitate the creation of intricate models and the fine-tuning
    of NNs down to the minute details, making it a favored choice within the research
    community. Its simplicity, user-friendliness, and intuitiveness made it a go-to
    tool for prototyping DL models. However, there are certain deployment-related
    limitations with its API, which might restrict its application in certain real-life
    scenarios.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: MXNet (Chen et al., [2016](#bib.bib25)) is a library that provides optimized
    building blocks for implementing CNNs. It is specially tailored for Intel processors,
    offering vectorized and threaded support for CNNs on Intel CPUs and GPUs. Moreover,
    the MXNet framework provides interfaces in multiple languages, including Python,
    Scala, Java, Clojure, and R, making it convenient for cross-domain DL developers.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA has been at the forefront of GPU hardware and software optimization for
    DL. cuDNN (Chetlur et al., [2014](#bib.bib31)) is a highly optimized library specifically
    designed for DL networks, providing acceleration for DNN-related tasks. In addition
    to cuDNN, NVIDIA offers a range of DL libraries included in CUDA-X (NVIDIA, [2023](#bib.bib151)).
    TensorRT (Vanholder, [2016](#bib.bib203)), another NVIDIA library, optimizes inference
    on NVIDIA GPUs by applying layer and tensor fusion, kernel auto-tuning, and dynamic
    tensor memory optimizations.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Each DL library has unique strengths and caters to specific use cases, allowing
    practitioners to choose one that best suits their projects. To address the interoperability
    challenges between DL libraries, Microsoft and Facebook introduced Open Neural
    Network Exchange (ONNX) (Foundation, [2017](#bib.bib53)), an open standard for
    machine learning interoperability. With ONNX, models created in different libraries
    can be easily shared and executed. For instance, a PyTorch model can be run on
    an Android device by converting it into TensorFlow format, eliminating the need
    for model retraining.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Co-Design of Hardware Architecture
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In DL, acceleration solutions relying solely on software techniques are primarily
    limited by their dependence on the intrinsic capabilities of general-purpose processors,
    potentially struggling to exploit specialized hardware features designed for specific
    DL tasks fully. Conversely, hardware-only solutions may face limitations in flexibility
    and adaptability, as dedicated hardware is often tailored for specific tasks or
    architectures, making updates or adaptations to new DL models challenging without
    hardware modifications. This underscores the value of co-designing a hardware
    and software approach for resource-constrained environments, employing a holistic
    optimization strategy. This approach includes refining the DL algorithm, optimizing
    and compressing the model, efficient memory management, software kernel implementation,
    and hardware architecture design. This section discusses solutions that adopt
    a holistic approach to address challenges related to irregular memory accesses,
    enhance the handling of sparsity resulting from compression methods, and explore
    improved solutions within NAS algorithms.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: In Section 3, we emphasize that many NN connections can be pruned effectively
    without substantial accuracy loss. However, in such models, only a subset of the
    NN’s weights are active, and their locations are irregular or non-contiguous.
    Efficiently accessing these weights, especially when using hardware accelerators
    like GPUs or TPUs, can be challenging due to the irregularity of weight locations.
    To tackle this issue, in earlier methods, like Cambricon-X (Zhang et al., [2016](#bib.bib248)),
    MAC operations utilize zero-weight connections and access required weights using
    sparse indices. However, irregular nonzero weight distribution caused issues such
    as indexing overhead, PE imbalances, and inefficient memory access. Later advancements,
    as seen in Cambricon-S (Zhou et al., [2018](#bib.bib255)), improve efficiency
    by enforcing regularity in filter sparsity through software/hardware integration.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Sparse-YOLO (Wang et al., [2020b](#bib.bib213)) introduces a dedicated sparse
    convolution unit tailored to handle quantized values and sparsity resulting from
    unstructured pruning techniques. Cho et al. (Cho et al., [2021](#bib.bib33)) propose
    an acceleration technique for a quantized binary NN. This approach utilizes an
    array of PEs, with each PE responsible for computing the output of a specific
    feature map, implementing inter-feature map parallelism. Moreover, optimizing
    the storage of sparse weights post-pruning has been explored. Han et al. (Han
    et al., [2016](#bib.bib76)) show that these sparse weights can be compressed,
    reducing memory access bandwidth by around 20%-30%. SCNN (Parashar et al., [2017](#bib.bib153))
    processes convolutional layers in their compressed format using an input stationary
    dataflow. This involves transmitting compressed weights and activations to a multiplier
    array, followed by a scatter network to add the scattered partial sums.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: In the NAS field, apart from the previously discussed hardware-aware NAS approaches
    that tailor models for specific hardware platforms, there are also co-designed
    solutions that initially remain hardware-agnostic. These co-designed systems seamlessly
    integrate hardware optimization within the NAS process, ensuring simultaneous
    hardware and DNN model optimization. Hardware settings can be explored in conjunction
    with DNN architectures using the same algorithm (Zhou et al., [2021](#bib.bib256);
    Choi et al., [2021](#bib.bib35); Li et al., [2020](#bib.bib120)) or through an
    external search algorithm (Sekanina, [2021](#bib.bib171); Lin et al., [2020b](#bib.bib128)).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [11](#S4.F11 "Figure 11 ‣ 4.4\. Co-Design of Hardware Architecture
    ‣ 4\. Hardware Acceleration of Deep Learning Models ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey")(a), the most direct approach
    for co-searching hardware and software settings involves creating CNN and accelerator
    pairs and evaluating the final model’s performance. One can opt to train the CNN
    each time a new pair is tested or follow the approach of Chen et al. (Chen et al.,
    [2020c](#bib.bib26)), where a supernet is employed to directly generate the weights
    of a DDN, and accuracy is assessed in a single testing run of the model. Fig. [11](#S4.F11
    "Figure 11 ‣ 4.4\. Co-Design of Hardware Architecture ‣ 4\. Hardware Acceleration
    of Deep Learning Models ‣ Lightweight Deep Learning for Resource-Constrained Environments:
    A Survey")(b) illustrates an alternative strategy employed by Lin et al. (Lin
    et al., [2020b](#bib.bib128)), where a hardware optimization algorithm takes a
    candidate CNN as input and optimizes the hardware accelerator to achieve specific
    objectives. The network is then trained and evaluated only if a viable hardware
    configuration is found. If no suitable hardware setting is identified, the network
    remains untrained until a viable configuration is found. This strategy allows
    for the avoidance of training the CNN, which is the most complex phase of the
    co-design process.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3fceaf38b2814b5a620f4748d29b032a.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
- en: Figure 11\. Two different approaches for implementing NAS and hardware co-design (Sekanina,
    [2021](#bib.bib171)).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the co-design of algorithms significantly improves compression and
    computational efficiency. However, these methods are inherently non-trivial and
    require in-depth exploration of software and hardware techniques.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Challenge and Future work
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we explore the sophisticated domain of lightweight models, compression
    methods, and hardware acceleration, showcasing their advanced technological capabilities
    applicable across a broad spectrum of general applications. Nonetheless, deploying
    these models in resource-constrained environments continues to present substantial
    challenges. This section is dedicated to unveiling novel techniques in TinyML
    and LLMs for accelerating and applying DL models, focusing on unresolved issues
    that warrant further investigation.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. TinyML
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TinyML is an emerging technology that enables DL algorithms to run on ultra-low-end
    IoT devices that consume less than 1mW of power. However, the extremely constrained
    hardware environment makes it challenging to design and develop a TinyML model.
    Low-end IoT devices predominantly employ MCUs due to their cost efficiency compared
    to CPUs and GPUs. However, MCU libraries, such as CMSIS-NN (Lai et al., [2018](#bib.bib113))
    and TinyEngine (Lin et al., [2020a](#bib.bib125)), are often platform-dependent,
    unlike GPU libraries like PyTorch and TensorFlow, which offer cross-platform support.
    Consequently, the design focus of TinyML leans more toward specialized applications
    rather than facilitating general-purpose research, potentially impeding the pace
    of overall research advancements.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: MCU-based libraries. Due to the resource-constrained environments in TinyML,
    MCU-based libraries are often designed for specific use cases. For instance, CMSIS-NN (Lai
    et al., [2018](#bib.bib113)), a pioneering work for MCU-based libraries developed
    on ARM Cortex-M devices, proposes an efficient kernel divided into NNfunctions
    and NNsupportfunctions. NNfunctions execute the main functions in the network,
    such as convolutions, poolings, and activations. NNsupportfunctions contain data
    conversions and activation tables. CMIX-NN (Capotondi et al., [2020](#bib.bib16))
    proposes an open-source mixed and low-precision tool that can support the model’s
    weights and activation to be quantized into 8, 4, and 2 bits arbitrarily. MCUNet (Lin
    et al., [2020a](#bib.bib125)) presents a co-design framework tailored for DL implementation
    on commercially available MCUs. This framework incorporates TinyNAS to search
    for the most accurate and lightweight model efficiently. Additionally, it leverages
    the TinyEngine, which encompasses code generator-based compilations and in-place
    depthwise convolution, effectively addressing peak memory constraints. Moving
    forward, MCUNetV2 (Lin et al., [2021](#bib.bib124)) introduces a patch-based inference
    mechanism that operates only on a small spatial region of the feature map, further
    reducing peak memory use. MicroNet (Banbury et al., [2021](#bib.bib9)) adopts
    differentiable NAS (DNAS) to search for efficient models with a low number of
    operations and supports the open-source platform Tensorflow Lite Micro (TFLM).
    MicroNet achieves state-of-the-art results for all TinyMLperf industry-standard
    benchmark tasks, i.e., Visual Wake Words, Google Speech Commands, and Anomaly
    detection.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: What hinders the rapid development of TinyML? Despite its progress, the growth
    of TinyML is hindered by several inherent key constraints, including resource
    constraints, hardware and software heterogeneity, and lack of datasets (Ray, [2022](#bib.bib162)).
    Extreme resource constraints, such as an incredibly small size of SRAM and less
    than 1 MB size flash memory, pose challenges in designing and deploying TinyML
    models on edge devices. Furthermore, due to hardware heterogeneity and a lack
    of framework compatibility, current TinyML solutions are tweaked for every individual
    device, complicating the wide-scale deployment of TinyML algorithms. Besides,
    existing datasets may not be suitable for TinyML architecture as the data may
    not correspond to the data generation feature from external sensors of edge devices.
    A set of standard datasets suitable for training TinyML models is needed to advance
    the development of effective TinyML systems. These open research challenges need
    to be addressed before mass deployment on IoT and edge devices is possible.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Building lightweight Large Language Models
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs have consistently exhibited outstanding performance across various tasks
    in the past two years (Anil et al., [2023](#bib.bib7); Touvron et al., [2023b](#bib.bib201);
    Ye et al., [2023](#bib.bib232)). LLMs hold significant potential for practical
    applications, especially when paired with human supervision. For instance, they
    can serve as co-pilots alongside autonomous agents or as sources of inspiration
    and suggestions. However, these models typically feature parameters at the billion
    scale. Deploying such models for inference generally demands GPU-level hardware
    and tens of gigabytes of memory, posing substantial challenges for everyday LLM
    utilization. For example, Tao et al. (Tao et al., [2022](#bib.bib195)) find it
    hard to quantize generative pre-trained language models due to homogeneous word
    embedding and varied weight distribution. Consequently, transforming a large,
    resource-intensive LLM model into a compact version suitable for deployment on
    resource-constrained mobile devices has emerged as a prominent future research
    direction.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: World-renowned enterprises have made significant strides in LLM deployment.
    In 2023, Qualcomm showcased the independent execution of the text-to-image model,
    Stable Diffusion (Rombach et al., [2022](#bib.bib167)) and the image-to-image
    model, ControlNet (Zhang et al., [2023b](#bib.bib246)) on mobile devices, thereby
    accelerating the deployment of large models to edge computing environments. Google
    also introduced several versions of its latest universal large model, PaLM 2 (Anil
    et al., [2023](#bib.bib7)), featuring a lightweight variant tailored for mobile
    platforms. This development has created new opportunities for migrating large
    models from cloud-based systems to edge devices. However, certain large models
    still require several gigabytes of physical storage and runtime memory. Consequently,
    efforts are being directed towards achieving a memory footprint of less than 1
    GB (Ray, [2022](#bib.bib162)), signifying that significant work is still needed
    in this area. This section outlines some key initiatives for easing the implementation
    of LLMs in resource-constrained environments.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1\. Pruning without re-training
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently, a substantial body of work has applied common DL quantization and
    pruning techniques to construct lightweight LLMs. Some approaches (Yu et al.,
    [2023](#bib.bib237); Wu et al., [2023](#bib.bib220)) focus on implementing quantization,
    where numerical precision is greatly reduced. SparseGPT (Frantar and Alistarh,
    [2023](#bib.bib56)) demonstrates, for the first time, that large-scale Generative
    Pre-trained Transformer (GPT) models can be pruned to at least 50% sparsity in
    a single step, without any subsequent retraining, with minimal loss of accuracy.
    Following this, Wanda (Pruning by Weights and Activations) (Sun et al., [2023](#bib.bib182)),
    specifically designed to induce sparsity in pre-trained LLMs, is introduced. Wanda
    prunes weights with the smallest magnitudes and does not require retraining or
    weight updates. The pruned LLM can be directly utilized, increasing its practicality.
    Notably, Wanda surpasses the established baseline of magnitude pruning and competes
    effectively with recent methods that involve extensive weight updates. These works
    set a significant milestone for future work in designing LLM pruning methods that
    do not require retraining.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2\. Model Design
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: From a model design perspective, we can create lightweight LLMs from the very
    inception, focusing on reducing the number of model parameters. One promising
    avenue in this endeavor is prompt tuning, which seeks to optimize the LLMs’ performance
    while maintaining efficiency and model size. A notable approach in this context
    is Visual Prompt Tuning (VPT) (Jia et al., [2022](#bib.bib102)), which emerges
    as an efficient and effective alternative to the comprehensive fine-tuning of
    large-scale Transformer models employed in vision-related tasks. VPT introduces
    a mere fraction, less than 1%, of trainable parameters within the input space
    while maintaining the integrity of the model’s backbone. Another noteworthy contribution
    is CALIP (Guo et al., [2023](#bib.bib69)), which introduces parameter-free attention
    mechanisms to facilitate effective interaction and communication between visual
    and text features. It yields text-aware image features and visual-guided text
    features, contributing to the development of more streamlined and efficient vision-language
    models. In the near future, one promising avenue for advancing lightweight LLM
    design is the development of adaptive fine-tuning strategies. These strategies
    would dynamically adjust the model’s architecture and parameters to align with
    specific task requirements. This adaptability ensures the model can optimize its
    performance for particular applications without incurring unnecessary parameter
    bloat.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3\. Building Lightweight Diffusion Model
  id: totrans-369
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In recent years, denoising diffusion-based generative models, particularly those
    of the score-based variety (Ho et al., [2020](#bib.bib87); Song et al., [2021](#bib.bib176)),
    have made notable strides in creating diverse and authentic data. However, the
    transition of the inference phase of a diffusion model to edge devices poses significant
    challenges. The inference phase reverses the transformation process to generate
    real data from Gaussian noise, commonly known as the denoising process. Moreover,
    when these models are compressed to reduce their footprint and computational demands,
    there is a potential risk of severe degradation in image quality. The compression
    process may need simplifications, approximations, or even the removal of essential
    model components, which could adversely affect the model’s ability to reconstruct
    data from Gaussian noise accurately. Consequently, a critical concern emerges
    in balancing model size reduction with preserving high-quality image generation,
    thereby presenting a formidable challenge in developing diffusion models in resource-constrained
    scenarios.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: In a very recent work, Shang et al. (Shang et al., [2023](#bib.bib173)) introduce
    post-training quantization (Cai et al., [2020b](#bib.bib15)) into the field of
    diffusion model acceleration. When applied in a training-free manner, this quantization
    approach exhibits the capability to enhance the efficiency of the denoising process
    while simultaneously reducing the storage requirements for diffusion model weights,
    a critical component in the acceleration of diffusion models. Nevertheless, there
    remain numerous opportunities for improvement in this domain to achieve a trade-off
    between high-quality and lightweight model solutions.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4\. Deployment of Vision Transformers (ViTs)
  id: totrans-372
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Despite the increasing prevalence of lightweight ViTs, deploying ViT in hardware-constrained
    environments remains a persistent concern. According to (Wang et al., [2022b](#bib.bib212)),
    ViT inference on mobile devices has a latency and energy consumption of up to
    40 times higher than CNN models. Hence, without modification, mobile devices cannot
    support the inference of ViTs. The self-attention operations in ViTs need to compute
    the pair-wise relations between image patches, and the computations grow quadratically
    with the number of patches. Moreover, computation for FFN layers is more time-consuming
    than attention layers (Wang et al., [2022b](#bib.bib212)). By removing the redundant
    attention heads and FFN layers, DeiT-Tiny can reduce latency by 23.2%, with negligible
    0.75% accuracy loss.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Several works designed NLP models for embedded systems such as FPGAs (Ham et al.,
    [2020](#bib.bib73), [2021](#bib.bib74); Wang et al., [2021](#bib.bib207)). More
    recently, DiVIT (Li et al., [2022](#bib.bib121)) and VAQF (Sun et al., [2022](#bib.bib183))
    proposed hardware-software co-designed solutions for ViTs. DiVIT proposes a delta
    patch encoding and novel differential attention at the algorithm level that leverages
    the patch locality during inference. In DiVIT, the design of a differential attention
    Processing Engine array with bit-saving techniques can calculate the delta with
    less computation and communicate with differential dataflow. Furthermore, the
    exponent operation is executed using a lookup table without additional computation
    and with minimal hardware overhead. VAQF first introduces binarization into ViTs,
    which can be used for FPGA mapping and quantization training. Specifically, VAQF
    can generate the required quantization precision and accelerator description for
    direct software and hardware implementation based on the target frame rate.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable the seamless deployment of ViTs in resource-constrained devices,
    we highlight two potential future directions:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 1) Algorithm optimizations. In addition to the design of efficient ViT models
    described in Section 2.3, the bottlenecks of ViTs should also be considered. For
    example, since MatMul operations cause a bottleneck in ViTs, these operations
    can be accelerated or reduced (Wang et al., [2022b](#bib.bib212)). Additionally,
    integer quantization and improvement to operator fusion can be considered.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 2) Hardware Accessibility. Unlike CNNs, which are well-supported on most mobile
    devices and AI accelerators, ViTs do not have specialized hardware support. For
    instance, ViT fails to run on mobile GPUs and Intel NCS2 VPU. Based on our findings,
    some important operators are not supported on specific hardware. Specifically,
    on the mobile GPU, the concatenate operator requires a 4-dimensional input tensor
    in TFLiteGPUDelegate, but the tensor in ViTs is 3-dimensional. On the other hand,
    Intel VPU does not support LayerNorm, which exists in the architecture of transformers
    but is uncommon in CNN. Hence, hardware support for ViTs on resource-constrained
    devices warrants further investigation.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Conclusion
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recently, computer vision applications have increasingly prioritized energy
    conservation, carbon footprint reduction, and cost-effectiveness, highlighting
    the growing importance of lightweight models, particularly in the context of edge
    AI. This paper conducts a comprehensive examination of lightweight deep learning
    (DL), exploring prominent models such as MobileNet and Efficient transformer variants,
    along with prevalent strategies for optimizing these models, including pruning,
    quantization, knowledge distillation, and neural architecture search. Beyond providing
    a detailed explanation of these methods, we offer practical guidance for crafting
    customized lightweight models, offering clarity through an analysis of their respective
    strengths and weaknesses.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we discussed hardware acceleration for DL models, delving into
    hardware architectures, distinct data flow types and data locality optimization
    techniques, and DL libraries to enhance comprehension of accelerating the training
    and inference processes. This investigation sheds light on the intricate interplay
    between hardware and software (Co-design), providing insights into expediting
    training and inference processes from a hardware perspective. Finally, we turn
    our gaze toward the future, recognizing that the deployment of lightweight DL
    models in TinyML and LLM technologies presents challenges that demand the exploration
    of creative solutions in these evolving fields.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Acknowledgement
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This work is partially supported by the National Science and Technology Council,
    Taiwan under Grants, NSTC-112-2628-E-002-033-MY4, NSTC-112-2634-F-002-002-MBK,
    and NSTC-112-2218-E-A49-023, and was financially supported in part (project number:
    112UA10019) by the Co-creation Platform of the Industry Academia Innovation School,
    NYCU, under the framework of the National Key Fields Industry-University Cooperation
    and Skilled Personnel Training Act, from the Ministry of Education (MOE) and industry
    partners in Taiwan.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abadi et al. (2016) M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
    M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. 2016. TensorFlow: A system
    for large-scale machine learning. In *OSDI*. 265–283.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abdelfattah et al. (2021) M. S. Abdelfattah, A. Mehrotra, Ł. Dudziak, and N. D.
    Lane. 2021. Zero-Cost Proxies for Lightweight NAS. (2021).
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AIM (2022) AIM. 2022. *Advances in Image Manipulation workshop in conjunction
    with ECCV 2022*. Retrieved November 2, 2023 from [https://data.vision.ee.ethz.ch/cvl/aim22/](https://data.vision.ee.ethz.ch/cvl/aim22/)
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amodei and Hernandez (2018) D. Amodei and D. Hernandez. 2018. *AI and Compute*.
    Retrieved November 2, 2023 from [https://openai.com/blog/ai-and-compute](https://openai.com/blog/ai-and-compute)
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An et al. (2022) S. An, Q. Liao, Z. Lu, and J.-H. Xue. 2022. Efficient semantic
    segmentation via self-attention and self-distillation. *T-ITS* 23, 9 (2022), 15256–15266.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anil et al. (2023) R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A.
    Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. 2023. PaLM 2 technical
    report. *arXiv preprint arXiv:2305.10403* (2023).
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asperti et al. (2021) A. Asperti, D. Evangelista, and M. Marzolla. 2021. Dissecting
    FLOPs along input dimensions for GreenAI cost estimations. In *LOD*. 86–100.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banbury et al. (2021) C. Banbury, C. Zhou, I. Fedorov, R. Matas, U. Thakker,
    D. Gope, V. Janapa Reddi, M. Mattina, and P. Whatmough. 2021. MicroNets: Neural
    network architectures for deploying TinyML applications on commodity microcontrollers.
    *MLSys* 3 (2021).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Banner et al. (2018) R. Banner, I. Hubara, E. Hoffer, and D. Soudry. 2018. Scalable
    methods for 8-bit training of neural networks. *NIPS* 31 (2018).
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bastian (2023) M. Bastian. 2023. *GPT-4 has more than a trillion parameters
    - Report*. Retrieved March 1, 2024 from [https://the-decoder.com/gpt-4-has-a-trillion-parameters/](https://the-decoder.com/gpt-4-has-a-trillion-parameters/)
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Berthelier et al. (2021) A. Berthelier, T. Chateau, S. Duffner, C. Garcia,
    and C. Blanc. 2021. Deep model compression and architecture optimization for embedded
    systems: A survey. *JSPS* 93, 8 (2021), 863–878.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Booshehri et al. (2013) M. Booshehri, A. Malekpour, and P. Luksch. 2013. An
    improving method for loop unrolling. *IJCSIS* 11, 5 (2013), 73–76.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2020a) H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han. 2020a. Once-for-All:
    Train One Network and Specialize it for Efficient Deployment. In *ICLR*.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2020b) Y. Cai, Z. Yao, Z. Dong, A. Gholami, M. W. Mahoney, and
    K. Keutzer. 2020b. ZeroQ: A novel zero shot quantization framework. In *CVPR*.
    13169–13178.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Capotondi et al. (2020) A. Capotondi, M. Rusci, M. Fariselli, and L. Benini.
    2020. CMix-NN: Mixed low-precision CNN library for memory-constrained edge devices.
    *TCAS-II* 67, 5 (2020), 871–875.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Capra et al. (2020) M. Capra, B. Bussolino, A. Marchisio, G. Masera, M. Martina,
    and M. Shafique. 2020. Hardware and Software Optimizations for Accelerating Deep
    Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead. *IEEE
    Access* 8 (2020), 225134–225180.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020a) B. Chen, T. Medini, J. Farwell, C. Tai, A. Shrivastava,
    et al. 2020a. SLIDE: In defense of smart algorithms over hardware acceleration
    for large-scale deep learning systems. *MLSys* 2 (2020), 291–306.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021b) C.-Y. Chen, L. Lo, P.-J. Huang, H.-H. Shuai, and W.-H.
    Cheng. 2021b. Fashionmirror: Co-attention feature-remapping virtual try-on with
    sequential template poses. In *ICCV*. 13809–13818.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2022b) D. Chen, J.-P. Mei, H. Zhang, C. Wang, Y. Feng, and C. Chen.
    2022b. Knowledge distillation with the reused teacher classifier. In *CVPR*. 11933–11942.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021c) D. Chen, J.-P. Mei, Y. Zhang, C. Wang, Z. Wang, Y. Feng,
    and C. Chen. 2021c. Cross-layer distillation with semantic calibration. In *AAAI*,
    Vol. 35\. 7028–7036.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020b) H. Chen, Y. Wang, C. Xu, B. Shi, C. Xu, Q. Tian, and C.
    Xu. 2020b. AdderNet: Do We Really Need Multiplications in Deep Learning?. In *CVPR*.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021a) P. Chen, S. Liu, H. Zhao, and J. Jia. 2021a. Distilling
    knowledge via knowledge review. In *CVPR*. 5008–5017.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2014a) T. Chen, Z. Du, N. Sun, J. Wang, C. Wu, Y. Chen, and O.
    Temam. 2014a. DianNao: A small-footprint high-throughput accelerator for ubiquitous
    machine-learning. *ACM SIGARCH Computer Architecture News* 42, 1 (2014), 269–284.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2016) T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao,
    B. Xu, C. Zhang, and Z. Zhang. 2016. MXNet: A flexible and efficient machine learning
    library for heterogeneous distributed systems. *NIPSW*.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020c) W. Chen, Y. Wang, S. Yang, C. Liu, and L. Zhang. 2020c.
    You Only Search Once: A Fast Automation Framework for Single-Stage DNN/Accelerator
    Co-design. In *DATE*. 1283–1286.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019a) W. Chen, D. Xie, Y. Zhang, and S. Pu. 2019a. All you need
    is a few shifts: Designing efficient convolutional neural networks for image classification.
    In *CVPR*. 7241–7250.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022a) Y. Chen, X. Dai, D. Chen, M. Liu, X. Dong, L. Yuan, and
    Z. Liu. 2022a. Mobile-Former: Bridging MobileNet and Transformer. In *CVPR*. 5270–5279.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2014b) Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li,
    T. Chen, Z. Xu, N. Sun, et al. 2014b. DaDianNao: A machine-learning supercomputer.
    In *MICRO*. 609–622.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019b) Y. Chen, T. Yang, X. Zhang, G. Meng, C. Pan, and J. Sun.
    2019b. Detnas: Neural architecture search on object detection. *NIPS* 1, 2 (2019),
    4–1.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chetlur et al. (2014) S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J.
    Tran, B. Catanzaro, and E. Shelhamer. 2014. cuDNN: Efficient primitives for deep
    learning. *arXiv preprint arXiv:1410.0759* (2014).'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Child et al. (2019) R. Child, S. Gray, A. Radford, and I. Sutskever. 2019. Generating
    long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509* (2019).
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho et al. (2021) J. Cho, Y. Jung, S. Lee, and Y. Jung. 2021. Reconfigurable
    binary neural network accelerator with adaptive parallelism scheme. *Electronics*
    10, 3 (2021), 230.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. (2018) J. Choi, Z. Wang, S. Venkataramani, P. I.-J. Chuang, V.
    Srinivasan, and K. Gopalakrishnan. 2018. Pact: Parameterized clipping activation
    for quantized neural networks. *arXiv preprint arXiv:1805.06085* (2018).'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. (2021) K. Choi, D. Hong, H. Yoon, J. Yu, Y. Kim, and J. Lee. 2021.
    Dance: Differentiable accelerator/network co-exploration. In *DAC*.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chollet (2017) F. Chollet. 2017. Xception: Deep learning with depthwise separable
    convolutions. In *CVPR*. 1251–1258.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choromanski et al. (2021) K. Choromanski, V. Likhosherstov, D. Dohan, X. Song,
    A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. 2021.
    Rethinking attention with performers. In *ICLR*.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2021b) X. Dai, A. Wan, P. Zhang, B. Wu, Z. He, Z. Wei, K. Chen,
    Y. Tian, M. Yu, P. Vajda, et al. 2021b. Fbnetv3: Joint architecture-recipe search
    using predictor pretraining. In *CVPR*. 16276–16285.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2021a) Z. Dai, H. Liu, Q. V. Le, and M. Tan. 2021a. CoAtNet: Marrying
    convolution and attention for all data sizes. *NIPS* 34 (2021), 3965–3977.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'David et al. (2021) R. David, J. Duke, A. Jain, V. Janapa Reddi, N. Jeffries,
    J. Li, N. Kreeger, I. Nappier, M. Natraj, T. Wang, P. Warden, and R. Rhodes. 2021.
    TensorFlow Lite Micro: Embedded Machine Learning for TinyML Systems. In *MLSys*,
    Vol. 3\. 800–811.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. (2021) J. Deng, W. Li, Y. Chen, and L. Duan. 2021. Unbiased mean
    teacher for cross-domain object detection. In *CVPR*. 4091–4101.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2017) X. Dong, S. Chen, and S. Pan. 2017. Learning to prune deep
    neural networks via layer-wise optimal brain surgeon. *NIPS* 30 (2017).
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2020) Z. Dong, Z. Yao, D. Arfeen, A. Gholami, M. W. Mahoney, and
    K. Keutzer. 2020. Hawq-v2: Hessian aware trace-weighted quantization of neural
    networks. *NIPS* 33 (2020), 18518–18529.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2019) Z. Dong, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer.
    2019. Hawq: Hessian aware quantization of neural networks with mixed-precision.
    In *ICCV*. 293–302.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2021) A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,
    X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,
    and N. Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition
    at Scale. In *ICLR*.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2017) L. Du, Y. Du, Y. Li, J. Su, Y.-C. Kuan, C.-C. Liu, and M.-C. F.
    Chang. 2017. A reconfigurable streaming deep convolutional neural network accelerator
    for Internet of Things. *TCAS-I* 65, 1 (2017), 198–208.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dubey et al. (2019) S. Dubey, V. K. Soni, B. K. Dubey, et al. 2019. Application
    of Microcontroller in Assembly Line for Safety and Controlling. *IJRAR* 6, 1 (2019),
    107–111.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'd’Ascoli et al. (2021) S. d’Ascoli, H. Touvron, M. L. Leavitt, A. S. Morcos,
    G. Biroli, and L. Sagun. 2021. Convit: Improving vision transformers with soft
    convolutional inductive biases. In *ICML*. 2286–2296.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elhoushi et al. (2021) M. Elhoushi, Z. Chen, F. Shafiq, Y. H. Tian, and J. Y.
    Li. 2021. Deepshift: Towards multiplication-less neural networks. In *CVPR*. 2359–2368.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faghri et al. (2020) F. Faghri, I. Tabrizian, I. Markov, D. Alistarh, D. M.
    Roy, and A. Ramezani-Kebrya. 2020. Adaptive Gradient Quantization for Data-Parallel
    SGD. *NIPS* 33 (2020), 3174–3185.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2021) Z. Fan, W. Hu, H. Guo, F. Liu, and D. Xu. 2021. Hardware and
    Algorithm Co-Optimization for pointwise convolution and channel shuffle in ShuffleNet
    V2\. In *SMC*. 3212–3217.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feurer et al. (2019) M. Feurer, A. Klein, K. Eggensperger, J. T. Springenberg,
    M. Blum, and F. Hutter. 2019. Auto-sklearn: efficient and robust automated machine
    learning. In *Automated Machine Learning*. 113–134.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foundation (2017) L. Foundation. 2017. *ONNX*. Retrieved November 2, 2023 from
    [https://onnx.ai/](https://onnx.ai/)
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fraccaroli et al. (2022) M. Fraccaroli, E. Lamma, and F. Riguzzi. 2022. Symbolic
    DNN-tuner. *Machine Learning* (2022), 1–26.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frankle and Carbin (2019) J. Frankle and M. Carbin. 2019. The lottery ticket
    hypothesis: Finding sparse, trainable neural networks. *ICLR*.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar and Alistarh (2023) E. Frantar and D. Alistarh. 2023. SparseGPT: Massive
    Language Models Can Be Accurately Pruned in One-Shot. *arXiv preprint arXiv:2301.00774*
    (2023).'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2023) Z. Fu, M. He, Z. Tang, and Y. Zhang. 2023. Optimizing data
    locality by executor allocation in spark computing environment. *ComSIS* 20, 1
    (2023), 491–512.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Getzner et al. (2023) J. Getzner, B. Charpentier, and S. Günnemann. 2023. Accuracy
    is not the only Metric that matters: Estimating the Energy Consumption of Deep
    Learning Models. In *ICLR*.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gholami et al. (2022) A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and
    K. Keutzer. 2022. A survey of quantization methods for efficient neural network
    inference. (2022), 291–326.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gholami et al. (2018) A. Gholami, K. Kwon, B. Wu, Z. Tai, X. Yue, P. Jin, S.
    Zhao, and K. Keutzer. 2018. SqueezeNext: Hardware-aware neural network design.
    In *CVPRW*. 1638–1647.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gomez et al. (2017) A. N. Gomez, M. Ren, R. Urtasun, and R. B. Grosse. 2017.
    The reversible residual network: Backpropagation without storing activations.
    *NIPS* 30 (2017).'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google (2023) Google. 2023. *Post-training quantization — TensorFlow Lite*.
    Retrieved November 2, 2023 from [https://www.tensorflow.org/lite/performance/post_training_quantization](https://www.tensorflow.org/lite/performance/post_training_quantization)
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gou et al. (2021) J. Gou, B. Yu, S. J. Maybank, and D. Tao. 2021. Knowledge
    distillation: A survey. *IJCV* 129, 6 (2021), 1789–1819.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Graham et al. (2021) B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin,
    H. Jégou, and M. Douze. 2021. LeViT: a Vision Transformer in ConvNet’s Clothing
    for Faster Inference. In *ICCV*. 12259–12269.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gray and Neuhoff (1998) R. M. Gray and D. L. Neuhoff. 1998. Quantization. *TIT*
    44, 6 (1998), 2325–2383.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2017) K. Guo, L. Sui, J. Qiu, J. Yu, J. Wang, S. Yao, S. Han, Y.
    Wang, and H. Yang. 2017. Angel-eye: A complete design flow for mapping CNN onto
    embedded FPGA. *TCAD* 37, 1 (2017), 35–47.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2020) Q. Guo, X. Wang, Y. Wu, Z. Yu, D. Liang, X. Hu, and P. Luo.
    2020. Online knowledge distillation via collaborative learning. In *CVPR*. 11020–11029.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2016) Y. Guo, A. Yao, and Y. Chen. 2016. Dynamic network surgery
    for efficient DNNs. *NIPS* 29 (2016).
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2023) Z. Guo, R. Zhang, L. Qiu, X. Ma, X. Miao, X. He, and B. Cui.
    2023. CALIP: Zero-shot enhancement of clip with parameter-free attention. In *AAAI*,
    Vol. 37\. 746–754.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta and Agrawal (2022) M. Gupta and P. Agrawal. 2022. Compression of deep
    learning models for text: A survey. *TKDD* 16, 4 (2022), 1–55.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta et al. (2015) S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan.
    2015. Deep learning with limited numerical precision. (2015), 1737–1746.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta and Akin (2020) S. Gupta and B. Akin. 2020. Accelerator-aware Neural Network
    Design using AutoML. *MLSysW* (2020).
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ham et al. (2020) T. J. Ham, S. J. Jung, S. Kim, Y. H. Oh, Y. Park, Y. Song,
    J.-H. Park, S. Lee, K. Park, J. W. Lee, et al. 2020. A^ 3: Accelerating attention
    mechanisms in neural networks with approximation. In *HPCA*. 328–341.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ham et al. (2021) T. J. Ham, Y. Lee, S. H. Seo, S. Kim, H. Choi, S. J. Jung,
    and J. W. Lee. 2021. ELSA: Hardware-Software co-design for efficient, lightweight
    self-attention mechanism in neural networks. In *ISCA*. 692–705.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2023) K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang,
    A. Xiao, C. Xu, Y. Xu, Z. Yang, Y. Zhang, and D. Tao. 2023. A Survey on Vision
    Transformer. *TPAMI* 45, 1 (2023), 87–110.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2016) S. Han, H. Mao, and W. J. Dally. 2016. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. In *ICLR*.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hassibi et al. (1993) B. Hassibi, D. G. Stork, and G. J. Wolff. 1993. Optimal
    brain surgeon and general network pruning. In *ICNN*. 293–299.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) K. He, X. Zhang, S. Ren, and J. Sun. 2016. Deep residual learning
    for image recognition. In *CVPR*. 770–778.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2021) X. He, K. Zhao, and X. Chu. 2021. AutoML: A Survey of the
    State-of-the-Art. *Knowledge-Based Systems* 212 (2021), 106622.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2020) Y. He, Y. Ding, P. Liu, L. Zhu, H. Zhang, and Y. Yang. 2020.
    Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration.
    In *CVPR*. 2006–2015.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2018) Y. He, G. Kang, X. Dong, Y. Fu, and Y. Yang. 2018. Soft filter
    pruning for accelerating deep convolutional neural networks. *IJCAI* (2018), 2234–2240.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2019a) Y. He, P. Liu, Z. Wang, Z. Hu, and Y. Yang. 2019a. Filter
    pruning via geometric median for deep convolutional neural networks acceleration.
    In *CVPR*. 4340–4349.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2019b) Y. He, X. Liu, H. Zhong, and Y. Ma. 2019b. AddressNet: Shift-based
    primitives for efficient convolutional neural networks. In *WACV*. 1213–1222.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2017) Y. He, X. Zhang, and J. Sun. 2017. Channel pruning for accelerating
    very deep neural networks. In *CVPR*. 1389–1397.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hidayati et al. (2020) S. C. Hidayati, T. W. Goh, J.-S. G. Chan, C.-C. Hsu,
    J. See, L.-K. Wong, K.-L. Hua, Y. Tsao, and W.-H. Cheng. 2020. Dress with style:
    Learning style from joint deep embedding of clothing styles and body shapes. *TMM*
    23 (2020), 365–377.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) G. Hinton, O. Vinyals, and J. Dean. 2015. Distilling the
    knowledge in a neural network. *arXiv preprint arXiv:1503.02531* (2015).
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2020) J. Ho, A. Jain, and P. Abbeel. 2020. Denoising diffusion probabilistic
    models. *NIPS* 33 (2020), 6840–6851.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou et al. (2019) Y. Hou, Z. Ma, C. Liu, and C. C. Loy. 2019. Learning lightweight
    lane detection CNNs by self attention distillation. In *ICCV*. 1013–1021.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Howard et al. (2019) A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M.
    Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, et al. 2019. Searching for mobilenetv3\.
    In *ICCV*. 1314–1324.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Howard et al. (2017) A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
    T. Weyand, M. Andreetto, and H. Adam. 2017. MobileNets: Efficient convolutional
    neural networks for mobile vision applications. *arXiv preprint arXiv:1704.04861*
    (2017).'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hsu et al. (2020) L.-C. Hsu, C.-T. Chiu, K.-T. Lin, H.-H. Chou, and Y.-Y. Pu.
    2020. ESSA: An energy-aware bit-serial streaming deep convolutional neural network
    accelerator. *JSA* 111 (2020), 101831.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2018) J. Hu, L. Shen, and G. Sun. 2018. Squeeze-and-excitation networks.
    In *CVPR*. 7132–7141.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2023) W. Hu, Z. Che, N. Liu, M. Li, J. Tang, C. Zhang, and J. Wang.
    2023. CATRO: Channel Pruning via Class-Aware Trace Ratio Optimization. *TNNLS*
    (2023), 1–13.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2018) G. Huang, S. Liu, L. Van der Maaten, and K. Q. Weinberger.
    2018. CondenseNet: An efficient DenseNet using learned group convolutions. In
    *CVPR*. 2752–2761.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2017) G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger.
    2017. Densely connected convolutional networks. In *CVPR*. 4700–4708.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang and Leng (1999) J.-C. Huang and T. Leng. 1999. Generalized loop-unrolling:
    a method for program speedup. In *ASSET*. 244–248.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang and Wang (2019) Z. Huang and N. Wang. 2019. Like what you like: Knowledge
    distill via neuron selectivity transfer. In *ICLR*.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hubara et al. (2016) I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
    Y. Bengio. 2016. Binarized neural networks. In *NIPS*. 4114–4122.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iandola et al. (2017) F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
    Dally, and K. Keutzer. 2017. SqueezeNet: AlexNet-level accuracy with 50x fewer
    parameters and¡ 0.5 MB model size. In *ICLR*.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacob et al. (2018) B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard,
    H. Adam, and D. Kalenichenko. 2018. Quantization and training of neural networks
    for efficient integer-arithmetic-only inference. (2018), 2704–2713.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeon and Kim (2018) Y. Jeon and J. Kim. 2018. Constructing fast network through
    deconstruction of convolution. *NIPS* 31 (2018).
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. (2022) M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan,
    and S.-N. Lim. 2022. Visual prompt tuning. In *ECCV*. 709–727.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jouppi et al. (2023) N. Jouppi, G. Kurian, S. Li, P. Ma, R. Nagarajan, L. Nai,
    N. Patil, S. Subramanian, A. Swing, B. Towles, et al. 2023. TPU v4: An optically
    reconfigurable supercomputer for machine learning with hardware support for embeddings.
    In *ISCA*. 1–14.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jouppi et al. (2017) N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal,
    R. Bajwa, S. Bates, S. Bhatia, N. Boden, A. Borchers, et al. 2017. In-datacenter
    performance analysis of a tensor processing unit. In *ISCA*. 1–12.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jung et al. (2019) S. Jung, C. Son, S. Lee, J. Son, J.-J. Han, Y. Kwak, S. J.
    Hwang, and C. Choi. 2019. Learning to quantize deep networks by optimizing quantization
    intervals with task loss. In *CVPR*. 4350–4359.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang et al. (2023) B. Kang, X. Chen, D. Wang, H. Peng, and H. Lu. 2023. Exploring
    Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking. In
    *ICCV*. 9612–9621.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang and Han (2020) M. Kang and B. Han. 2020. Operation-aware soft channel pruning
    using differentiable masks. In *ICML*. 7021–7032.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2021) K. Kim, B. Ji, D. Yoon, and S. Hwang. 2021. Self-knowledge
    distillation with progressive refinement of targets. In *ICCV*. 6567–6576.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kitaev et al. (2020) N. Kitaev, Ł. Kaiser, and A. Levskaya. 2020. Reformer:
    The efficient transformer. In *ICLR*.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kotthoff et al. (2019) L. Kotthoff, C. Thornton, H. H. Hoos, F. Hutter, and
    K. Leyton-Brown. 2019. Auto-WEKA: Automatic model selection and hyperparameter
    optimization in WEKA. In *Automated Machine Learning*. 81–95.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012.
    Imagenet classification with deep convolutional neural networks. *NIPS* 25 (2012),
    1097–1105.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. (2019) S. Kumar, V. Bitorff, D. Chen, C. Chou, B. Hechtman, H.
    Lee, N. Kumar, P. Mattson, S. Wang, T. Wang, et al. 2019. Scale MLPerf-0.6 models
    on google TPU-v3 pods. *arXiv preprint arXiv:1909.09756* (2019).
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lai et al. (2018) L. Lai, N. Suda, and V. Chandra. 2018. CMSIS-NN: Efficient
    neural network kernels for arm cortex-m CPUs. *arXiv preprint arXiv:1801.06601*
    (2018).'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) Y. LeCun, J. Denker, and S. Solla. 1989. Optimal brain damage.
    *NIPS* 2 (1989).
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2019) N. Lee, T. Ajanthan, and P. H. Torr. 2019. Snip: Single-shot
    network pruning based on connection sensitivity. *ICLR*.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017) H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. 2017.
    Pruning Filters for Efficient ConvNets. In *ICLR*.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016) N. Li, S. Takaki, Y. Tomiokay, and H. Kitazawa. 2016. A multistage
    dataflow implementation of a deep convolutional neural network based on FPGA for
    high-speed object recognition. In *SSIAI*. 165–168.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) S. Li, M. Lin, Y. Wang, Y. Wu, Y. Tian, L. Shao, and R. Ji.
    2023. Distilling a Powerful Student Model via Online Knowledge Distillation. *TNNLS*
    34, 11 (2023), 8743–8752.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021) S. Li, M. Tan, R. Pang, A. Li, L. Cheng, Q. V. Le, and N. P.
    Jouppi. 2021. Searching for fast model families on datacenter accelerators. In
    *CVPR*. 8085–8095.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Y. Li, C. Hao, X. Zhang, X. Liu, Y. Chen, J. Xiong, W.-m.
    Hwu, and D. Chen. 2020. EDD: Efficient differentiable DNN architecture and implementation
    co-search for embedded ai solutions. In *DAC*. 1–6.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022) Y. Li, Y. Hu, F. Wu, and K. Li. 2022. DiVIT: Algorithm and
    architecture co-design of differential attention in vision transformer. *JSA*
    (2022), 102520.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2021b) T. Liang, J. Glossner, L. Wang, S. Shi, and X. Zhang.
    2021b. Pruning and quantization for deep neural network acceleration: A survey.
    *Neurocomputing* 461 (2021), 370–403.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2021a) Y. Liang, G. Chongjian, Z. Tong, Y. Song, J. Wang, and
    P. Xie. 2021a. EViT: Expediting Vision Transformers via Token Reorganizations.
    In *ICLR*.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2021) J. Lin, W.-M. Chen, H. Cai, C. Gan, and S. Han. 2021. MCUNetV2:
    Memory-efficient patch-based inference for tiny deep learning. In *NIPS*.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2020a) J. Lin, W.-M. Chen, Y. Lin, C. Gan, S. Han, et al. 2020a.
    MCUNet: Tiny deep learning on iot devices. *NIPS* 33 (2020), 11711–11722.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2022a) S. Lin, H. Xie, B. Wang, K. Yu, X. Chang, X. Liang, and G.
    Wang. 2022a. Knowledge distillation via the target-aware transformer. In *CVPR*.
    10915–10924.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2022b) S. Lin, H. Xie, B. Wang, K. Yu, X. Chang, X. Liang, and G.
    Wang. 2022b. Knowledge Distillation via the Target-Aware Transformer. In *CVPR*.
    10915–10924.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2020b) Y. Lin, D. Hafdi, K. Wang, Z. Liu, and S. Han. 2020b. Neural-hardware
    architecture search. *NIPSWS* (2020).
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin and Chang (2017) Y.-J. Lin and T. S. Chang. 2017. Data and hardware efficient
    design for convolutional neural network. *TCAS-I* 65, 5 (2017), 1642–1651.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023) B. Liu, F. Li, X. Wang, B. Zhang, and J. Yan. 2023. Ternary
    weight networks. In *ICASSP*. 1–5.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019b) H. Liu, K. Simonyan, and Y. Yang. 2019b. DARTS: Differentiable
    Architecture Search. (2019).'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021c) L. Liu, S. Zhang, Z. Kuang, A. Zhou, J.-H. Xue, X. Wang,
    Y. Chen, W. Yang, Q. Liao, and W. Zhang. 2021c. Group fisher pruning for practical
    network compression. In *ICML*.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021b) X. Liu, M. Ye, D. Zhou, and Q. Liu. 2021b. Post-training
    quantization with multiple points: Mixed precision without mixed precision. In
    *AAAI*, Vol. 35\. 8697–8705.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022) Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y.
    Cao, Z. Zhang, L. Dong, F. Wei, and B. Guo. 2022. Swin Transformer V2: Scaling
    Up Capacity and Resolution. In *CVPR*. 12009–12019.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021a) Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin,
    and B. Guo. 2021a. Swin transformer: Hierarchical vision transformer using shifted
    windows. In *ICCV*. 10012–10022.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019a) Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, K.-T. Cheng, and
    J. Sun. 2019a. Metapruning: Meta learning for automatic neural network channel
    pruning. In *ICCV*. 3296–3305.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2022) G. Luo, Y. Zhou, X. Sun, Y. Wang, L. Cao, Y. Wu, F. Huang,
    and R. Ji. 2022. Towards lightweight transformer via group-wise transformation
    for vision-and-language tasks. *TIP* 31 (2022), 3386–3398.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2016) T. Luo, S. Liu, L. Li, Y. Wang, S. Zhang, T. Chen, Z. Xu,
    O. Temam, and Y. Chen. 2016. DaDianNao: A neural network supercomputer. *IEEE
    TC* 66, 1 (2016), 73–88.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2018) N. Ma, X. Zhang, H.-T. Zheng, and J. Sun. 2018. ShuffleNet
    V2: Practical guidelines for efficient CNN architecture design. In *ECCV*. 116–131.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAI (2021) MAI. 2021. *Mobile AI workshop 2021*. Retrieved November 2, 2023
    from [https://ai-benchmark.com/workshops/mai/2021/#challenges](https://ai-benchmark.com/workshops/mai/2021/#challenges)
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAI (2022) MAI. 2022. *Mobile AI workshop 2022*. Retrieved November 2, 2023
    from [https://ai-benchmark.com/workshops/mai/2022/#challenges](https://ai-benchmark.com/workshops/mai/2022/#challenges)
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAI (2023) MAI. 2023. *Mobile AI workshop 2023*. Retrieved November 2, 2023
    from [https://ai-benchmark.com/workshops/mai/2023/#challenges](https://ai-benchmark.com/workshops/mai/2023/#challenges)
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehta et al. (2021) S. Mehta, M. Ghazvininejad, S. Iyer, L. Zettlemoyer, and
    H. Hajishirzi. 2021. Delight: Very deep and light-weight transformer. In *ICLR*.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mehta et al. (2018) S. Mehta, R. Koncel-Kedziorski, M. Rastegari, and H. Hajishirzi.
    2018. Pyramidal recurrent unit for language modeling. In *EMNLP*.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehta et al. (2020) S. Mehta, R. Koncel-Kedziorski, M. Rastegari, and H. Hajishirzi.
    2020. Define: Deep factorized input token embeddings for neural sequence modeling.
    In *ICLR*.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehta and Rastegari (2022) S. Mehta and M. Rastegari. 2022. Mobilevit: light-weight,
    general-purpose, and mobile-friendly vision transformer. In *ICLR*.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mezdour et al. (2023) L. Mezdour, K. Kadem, M. Merouani, A. S. Haichour, S.
    Amarasinghe, and R. Baghdadi. 2023. A Deep Learning Model for Loop Interchange.
    In *ACM SIGPLAN CC*. 50–60.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Micikevicius et al. (2018) P. Micikevicius, S. Narang, J. Alben, G. Diamos,
    E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and H.
    Wu. 2018. Mixed Precision Training. (2018).
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naseer et al. (2021) M. M. Naseer, K. Ranasinghe, S. H. Khan, M. Hayat, F. Shahbaz Khan,
    and M.-H. Yang. 2021. Intriguing properties of vision transformers. *NIPS* 34
    (2021).
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nechi et al. (2023) A. Nechi, L. Groth, S. Mulhem, F. Merchant, R. Buchty,
    and M. Berekovic. 2023. FPGA-based Deep Learning Inference Accelerators: Where
    Are We Standing? *TRETS* 16, 4 (2023), 1–32.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NVIDIA (2023) NVIDIA. 2023. *NVIDIA CUDA-X: GPU Accelerated Libraries*. Retrieved
    November 2, 2023 from [https://developer.nvidia.com/gpu-accelerated-libraries](https://developer.nvidia.com/gpu-accelerated-libraries)'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. (2023).
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parashar et al. (2017) A. Parashar, M. Rhu, A. Mukkara, A. Puglielli, R. Venkatesan,
    B. Khailany, J. Emer, S. W. Keckler, and W. J. Dally. 2017. SCNN: An accelerator
    for compressed-sparse convolutional neural networks. *ACM SIGARCH Comput. Archit.
    News* 45, 2 (2017), 27–40.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paszke et al. (2019) A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,
    G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. 2019. Pytorch:
    An imperative style, high-performance deep learning library. *NIPS* 32 (2019).'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2019) H. Peng, J. Wu, S. Chen, and J. Huang. 2019. Collaborative
    Channel Pruning for Deep Networks. In *ICML*. 5113–5122.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pouransari et al. (2020) H. Pouransari, Z. Tu, and O. Tuzel. 2020. Least squares
    binary quantization of neural networks. In *CVPRW*. 698–699.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. (2022) Z. Qi, W. Chen, R. A. Naqvi, and K. Siddique. 2022. Designing
    Deep Learning Hardware Accelerator and Efficiency Evaluation. *Comput. Intell.
    and Neurosci.* 2022 (2022).
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiu et al. (2016) J. Qiu, J. Wang, S. Yao, K. Guo, B. Li, E. Zhou, J. Yu, T.
    Tang, N. Xu, S. Song, et al. 2016. Going deeper with embedded FPGA platform for
    convolutional neural network. In *ACM FPGA*. 26–35.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radosavovic et al. (2020) I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He,
    and P. Dollár. 2020. Designing network design spaces. In *CVPR*. 10428–10436.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rao et al. (2021) Y. Rao, W. Zhao, B. Liu, J. Lu, J. Zhou, and C.-J. Hsieh.
    2021. Dynamicvit: Efficient vision transformers with dynamic token sparsification.
    *NIPS* 34 (2021), 13937–13949.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rastegari et al. (2016) M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi.
    2016. XNOR-Net: Imagenet classification using binary convolutional neural networks.
    In *ECCV*. 525–542.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ray (2022) P. P. Ray. 2022. A review on TinyML: State-of-the-art and prospects.
    *Journal of King Saud University-Computer and Information Sciences* 34, 4 (2022),
    1595–1623.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real et al. (2017) E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J.
    Tan, Q. V. Le, and A. Kurakin. 2017. Large-scale evolution of image classifiers.
    In *ICML*. 2902–2911.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2021) P. Ren, Y. Xiao, X. Chang, P.-Y. Huang, Z. Li, X. Chen, and
    X. Wang. 2021. A comprehensive survey of neural architecture search: Challenges
    and solutions. *CSUR* 54, 4 (2021), 1–34.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roggen et al. (2022) D. Roggen, R. Cobden, A. Pouryazdan, and M. Zeeshan. 2022.
    Wearable FPGA platform for accelerated dsp and ai applications. In *PerComW*.
    66–69.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rokh et al. (2023) B. Rokh, A. Azarpeyvand, and A. Khanteymoori. 2023. A comprehensive
    survey on model quantization for deep neural networks in image classification.
    *TIST* 14, 6 (2023), 1–50.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rombach et al. (2022) R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B.
    Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion Models. In
    *CVPR*. 10684–10695.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sakr et al. (2022) C. Sakr, S. Dai, R. Venkatesan, B. Zimmer, W. Dally, and
    B. Khailany. 2022. Optimal clipping and magnitude-aware differentiation for improved
    quantization-aware training. In *ICML*. 19123–19138.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sandler et al. (2018) M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.
    Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. In *CVPR*.
    4510–4520.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schwartz et al. (2020) R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni. 2020.
    Green ai. *CACM* 63, 12 (2020), 54–63.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sekanina (2021) L. Sekanina. 2021. Neural architecture search and hardware
    accelerator co-search: A survey. *IEEE access* 9 (2021), 151337–151362.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Seng et al. (2021) K. P. Seng, P. J. Lee, and L. M. Ang. 2021. Embedded intelligence
    on FPGA: Survey, applications and challenges. *Electronics* 10, 8 (2021), 895.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shang et al. (2023) Y. Shang, Z. Yuan, B. Xie, B. Wu, and Y. Yan. 2023. Post-training
    quantization on diffusion models. In *CVPR*. 1972–1981.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman (2015) K. Simonyan and A. Zisserman. 2015. Very deep
    convolutional networks for large-scale image recognition. In *ICLR*.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sinha (2023) S. Sinha. 2023. *State of IoT 2023: Number of connected IoT devices
    growing 16% to 16.7 billion globally*. Retrieved November 2, 2023 from [https://iot-analytics.com/number-connected-iot-devices/](https://iot-analytics.com/number-connected-iot-devices/)'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2021) Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon,
    and B. Poole. 2021. Score-based generative modeling through stochastic differential
    equations. In *ICLR*.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srinivas et al. (2021) A. Srinivas, T.-Y. Lin, N. Parmar, J. Shlens, P. Abbeel,
    and A. Vaswani. 2021. Bottleneck transformers for visual recognition. In *CVPR*.
    16519–16529.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stoutchinin et al. (2019) A. Stoutchinin, F. Conti, and L. Benini. 2019. Optimally
    scheduling CNN convolutions for efficient memory access. *arXiv preprint arXiv:1902.01492*
    (2019).
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strubell et al. (2019) E. Strubell, A. Ganesh, and A. McCallum. 2019. Energy
    and policy considerations for deep learning in NLP. *ACL*.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2020) Z. Su, L. Fang, W. Kang, D. Hu, M. Pietikäinen, and L. Liu.
    2020. Dynamic group convolution for accelerating convolutional neural networks.
    In *ECCV*. 138–155.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sultana et al. (2022) M. Sultana, M. Naseer, M. H. Khan, S. Khan, and F. S.
    Khan. 2022. Self-Distilled Vision Transformer for Domain Generalization. In *ACCV*.
    3068–3085.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) M. Sun, Z. Liu, A. Bair, and J. Z. Kolter. 2023. A Simple
    and Effective Pruning Approach for Large Language Models. *arXiv preprint arXiv:2306.11695*
    (2023).
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2022) M. Sun, H. Ma, G. Kang, Y. Jiang, T. Chen, X. Ma, Z. Wang,
    and Y. Wang. 2022. VAQF: Fully Automatic Software-hardware Co-design Framework
    for Low-bit Vision Transformer. *arXiv preprint arXiv:2201.06618* (2022).'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2020) Y. Sun, H. Wang, B. Xue, Y. Jin, G. G. Yen, and M. Zhang.
    2020. Surrogate-Assisted Evolutionary Deep Learning Using an End-to-End Random
    Forest-Based Performance Predictor. *TEVC* 24, 2 (2020), 350–364.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sze et al. (2020) V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer. 2020. How
    to evaluate deep neural network processors: Tops/w (alone) considered harmful.
    *SSC-M* 12, 3 (2020), 28–41.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2017) C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi. 2017.
    Inception-v4, inception-resnet and the impact of residual connections on learning.
    In *AAAI*.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2015) C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
    D. Erhan, V. Vanhoucke, and A. Rabinovich. 2015. Going deeper with convolutions.
    In *CVPR*. 1–9.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2016) C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z.
    Wojna. 2016. Rethinking the inception architecture for computer vision. In *CVPR*.
    2818–2826.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Talwalkar (2020) A. Talwalkar. 2020. *The push for energy efficient ”Green AI”*.
    Retrieved November 2, 2023 from [https://spectrum.ieee.org/energy-efficient-green-ai-strategies](https://spectrum.ieee.org/energy-efficient-green-ai-strategies)
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. (2019b) J. Tan, L. Niu, J. K. Adams, V. Boominathan, J. T. Robinson,
    R. G. Baraniuk, and A. Veeraraghavan. 2019b. Face Detection and Verification Using
    Lensless Cameras. *TCI* 5, 2 (2019), 180–194.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. (2019a) M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard,
    and Q. V. Le. 2019a. MnasNet: Platform-aware neural architecture search for mobile.
    In *CVPR*. 2820–2828.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan and Le (2019a) M. Tan and Q. Le. 2019a. EfficientNet: Rethinking model
    scaling for convolutional neural networks. In *ICML*. 6105–6114.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan and Le (2021) M. Tan and Q. Le. 2021. EfficientNetV2: Smaller models and
    faster training. In *ICML*. 10096–10106.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan and Le (2019b) M. Tan and Q. V. Le. 2019b. MixConv: Mixed depthwise convolutional
    kernels. (2019).'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tao et al. (2022) C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo,
    and N. Wong. 2022. Compression of generative pre-trained language models via quantization.
    In *ACL*.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tarvainen and Valpola (2017) A. Tarvainen and H. Valpola. 2017. Mean teachers
    are better role models: Weight-averaged consistency targets improve semi-supervised
    deep learning results. In *NIPS*, Vol. 30.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tay et al. (2021) Y. Tay, M. Dehghani, D. Bahri, and D. Metzler. 2021. Efficient
    transformers: A survey. *CSUR* 54, 4 (2021), 1–41.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian et al. (2020) Y. Tian, D. Krishnan, and P. Isola. 2020. Contrastive Representation
    Distillation. (2020).
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Touvron et al. (2021) H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles,
    and H. Jégou. 2021. Training data-efficient image transformers & distillation
    through attention. In *ICML*. 10347–10357.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.
    Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. 2023a.
    Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*
    (2023).'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,
    Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. 2023b. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*
    (2023).'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Um et al. (2021) S. Um, S. Kim, S. Kim, and H.-J. Yoo. 2021. A 43.1 tops/w energy-efficient
    absolute-difference-accumulation operation computing-in-memory with computation
    reuse. *TCAS-II* 68, 5 (2021), 1605–1609.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vanholder (2016) H. Vanholder. 2016. Efficient inference with tensorrt. In *GPU
    Technology Conference*, Vol. 1.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, Ł. Kaiser, and I. Polosukhin. 2017. Attention is all you need. *NIPS*
    30 (2017).
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Viet et al. (2021) L. N. Viet, T. N. Dinh, D. T. Minh, H. N. Viet, and Q. L.
    Tran. 2021. UET-Headpose: A sensor-based top-view head pose dataset. In *KSE*.
    1–7.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wan et al. (2020) A. Wan, X. Dai, P. Zhang, Z. He, Y. Tian, S. Xie, B. Wu,
    M. Yu, T. Xu, K. Chen, et al. 2020. Fbnetv2: Differentiable neural architecture
    search for spatial and channel dimensions. In *CVPR*. 12965–12974.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) H. Wang, Z. Zhang, and S. Han. 2021. Spatten: Efficient
    sparse attention architecture with cascade token and head pruning. In *HPCA*.
    97–110.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022a) L. Wang, X. Dong, Y. Wang, L. Liu, W. An, and Y. Guo. 2022a.
    Learnable Lookup Table for Neural Network Quantization. In *CVPR*. 12423–12433.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) N. Wang, J. Choi, D. Brand, C.-Y. Chen, and K. Gopalakrishnan.
    2018. Training deep neural networks with 8-bit floating point numbers. In *NIPS*.
    7686–7695.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020a) S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. 2020a.
    Linformer: Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*
    (2020).'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) X. Wang, M. Kan, S. Shan, and X. Chen. 2019. Fully learnable
    group convolution for acceleration of deep neural networks. In *CVPR*. 9049–9058.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022b) X. Wang, L. L. Zhang, Y. Wang, and M. Yang. 2022b. Towards
    efficient vision transformer inference: a first study of transformers on mobile
    devices. In *WMCSA*. 1–7.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020b) Z. Wang, K. Xu, S. Wu, L. Liu, L. Liu, and D. Wang. 2020b.
    Sparse-YOLO: Hardware/software co-design of an FPGA accelerator for YOLOv2. *IEEE
    Access* 8 (2020), 116569–116585.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2017) X. Wei, C. H. Yu, P. Zhang, Y. Chen, Y. Wang, H. Hu, Y. Liang,
    and J. Cong. 2017. Automated systolic array architecture synthesis for high throughput
    CNN inference on FPGAs. In *DAC*. 1–6.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wolf and Lam (1991) M. E. Wolf and M. S. Lam. 1991. A data locality optimizing
    algorithm. In *PLDI*. 30–44.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wortsman et al. (2022) M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R.
    Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith,
    et al. 2022. Model soups: averaging weights of multiple fine-tuned models improves
    accuracy without increasing inference time. (2022), 23965–23998.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019) B. Wu, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian,
    P. Vajda, Y. Jia, and K. Keutzer. 2019. FBNet: Hardware-aware efficient ConvNet
    design via differentiable neural architecture search. In *CVPR*. 10734–10742.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2018) B. Wu, A. Wan, X. Yue, P. Jin, S. Zhao, N. Golmant, A. Gholaminejad,
    J. Gonzalez, and K. Keutzer. 2018. Shift: A zero flop, zero parameter alternative
    to spatial convolutions. In *CVPR*. 9127–9135.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2021) H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L.
    Zhang. 2021. Cvt: Introducing convolutions to vision transformers. In *ICCV*.
    22–31.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) X. Wu, C. Li, R. Y. Aminabadi, Z. Yao, and Y. He. 2023. Understanding
    INT4 Quantization for Transformer Models: Latency Speedup, Composability, and
    Failure Cases. *arXiv preprint arXiv:2301.12017* (2023).'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2020) Z. Wu, Z. Liu, J. Lin, Y. Lin, and S. Han. 2020. Lite transformer
    with long-short range attention. *ICLR*.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2021) T. Xiao, P. Dollar, M. Singh, E. Mintun, T. Darrell, and
    R. Girshick. 2021. Early convolutions help transformers see better. *NIPS* 34
    (2021).
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2023) H. Xie, M.-X. Lee, T.-J. Chen, H.-J. Chen, H.-I. Liu, H.-H.
    Shuai, and W.-H. Cheng. 2023. Most Important Person-guided Dual-branch Cross-Patch
    Attention for Group Affect Recognition. In *ICCV*. 20598–20608.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2023) R. Xu, E. H.-M. Sha, Q. Zhuge, Y. Song, and H. Wang. 2023.
    Loop interchange and tiling for multi-dimensional loops to minimize write operations
    on NVMs. *JSA* 135 (2023), 102799.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. (2023) Y. Xue, C. Chen, and A. Słowik. 2023. Neural Architecture
    Search Based on A Multi-objective Evolutionary Algorithm with Probability Stack.
    *TEVC* 27, 4 (2023).
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) C. Yang, L. Xie, C. Su, and A. L. Yuille. 2019. Snapshot
    distillation: Teacher-student optimization in one generation. In *CVPR*. 2859–2868.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2021c) J. Yang, B. Martinez, A. Bulat, G. Tzimiropoulos, et al.
    2021c. Knowledge distillation via softmax regression representation learning.
    In *ICLR*.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2021a) L. Yang, H. Jiang, R. Cai, Y. Wang, S. Song, G. Huang,
    and Q. Tian. 2021a. Condensenet v2: Sparse feature reactivation for deep networks.
    In *CVPR*. 3569–3578.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2018) T.-J. Yang, A. Howard, B. Chen, X. Zhang, A. Go, M. Sandler,
    V. Sze, and H. Adam. 2018. Netadapt: Platform-aware neural network adaptation
    for mobile applications. In *ECCV*. 285–300.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2021b) T.-J. Yang, Y.-L. Liao, and V. Sze. 2021b. Netadaptv2:
    Efficient neural architecture search with fast super-network training and architecture
    optimization. In *CVPR*. 2402–2411.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2021) Z. Yao, Z. Dong, Z. Zheng, A. Gholami, J. Yu, E. Tan, L.
    Wang, Q. Huang, Y. Wang, M. Mahoney, et al. 2021. Hawq-v3: Dyadic neural network
    quantization. In *ICML*. 11875–11886.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2023) J. Ye, X. Chen, N. Xu, C. Zu, Z. Shao, S. Liu, Y. Cui, Z. Zhou,
    C. Gong, Y. Shen, et al. 2023. A comprehensive capability analysis of GPT-3 and
    GPT-3.5 series models. *arXiv preprint arXiv:2303.10420* (2023).
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2018) J. Ye, X. Lu, Z. Lin, and J. Z. Wang. 2018. Rethinking the
    smaller-norm-less-informative assumption in channel pruning of convolution layers.
    In *ICLR*.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yin et al. (2022) H. Yin, A. Vahdat, J. Alvarez, A. Mallya, J. Kautz, and P.
    Molchanov. 2022. AdaViT: Adaptive Tokens for Efficient Vision Transformer. (2022),
    10809–10818.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yoon et al. (2022) J. Yoon, D. Kang, and M. Cho. 2022. Semi-supervised Domain
    Adaptation via Sample-to-Sample Self-Distillation. In *WACV*. 1978–1987.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You et al. (2020) H. You, X. Chen, Y. Zhang, C. Li, S. Li, Z. Liu, Z. Wang,
    and Y. Lin. 2020. ShiftAddNet: A Hardware-Inspired Deep Network. *NIPS* 33 (2020),
    2771–2783.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2023) C. Yu, T. Chen, and Z. Gan. 2023. Boost Transformer-based Language
    Models with GPU-Friendly Sparsity and Quantization. In *ACL*. 218–235.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2022) J. Yu, J. Liu, X. Wei, H. Zhou, Y. Nakata, D. Gudovskiy, T.
    Okuno, J. Li, K. Keutzer, and S. Zhang. 2022. Cross-domain object detection with
    mean-teacher transformer. In *ECCV*.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2021) L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z. Jiang, F. E.
    Tay, J. Feng, and S. Yan. 2021. Tokens-to-Token ViT: Training Vision Transformers
    from Scratch on ImageNet. In *ICCV*. 558–567.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2020) L. Yuan, F. E. Tay, G. Li, T. Wang, and J. Feng. 2020. Revisiting
    knowledge distillation via label smoothing regularization. In *CVPR*. 3903–3911.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan and Lin (2006) M. Yuan and Y. Lin. 2006. Model selection and estimation
    in regression with grouped variables. *J. R. Stat. Soc. B* 68, 1 (2006), 49–67.
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2015) C. Zhang, P. Li, G. Sun, Y. Guan, B. Xiao, and J. Cong.
    2015. Optimizing FPGA-based accelerator design for deep convolutional neural networks.
    In *ACM FPGA*. 161–170.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018a) C. Zhang, G. Sun, Z. Fang, P. Zhou, P. Pan, and J. Cong.
    2018a. Caffeine: Toward uniformed representation and acceleration for deep convolutional
    neural networks. *TCAD* 38, 11 (2018), 2072–2085.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021) H. Zhang, Z. Hu, W. Qin, M. Xu, and M. Wang. 2021. Adversarial
    co-distillation learning for image recognition. *Pattern Recognition* 111 (2021),
    107659.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023a) H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M.
    Ni, and H.-Y. Shum. 2023a. DINO: DETR with Improved DeNoising Anchor Boxes for
    End-to-End Object Detection. In *ICLR*.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023b) L. Zhang, A. Rao, and M. Agrawala. 2023b. Adding Conditional
    Control to Text-to-Image Diffusion Models. In *ICCV*. 3836–3847.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019b) L. Zhang, J. Song, A. Gao, J. Chen, C. Bao, and K. Ma.
    2019b. Be your own teacher: Improve the performance of convolutional neural networks
    via self distillation. In *ICCV*. 3713–3722.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2016) S. Zhang, Z. Du, L. Zhang, H. Lan, S. Liu, L. Li, Q. Guo,
    T. Chen, and Y. Chen. 2016. Cambricon-X: An accelerator for sparse neural networks.
    In *MICRO*. 1–12.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018c) X. Zhang, X. Zhou, M. Lin, and J. Sun. 2018c. ShuffleNet:
    An extremely efficient convolutional neural network for mobile devices. In *CVPR*.
    6848–6856.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Freris (2023) Y. Zhang and N. M. Freris. 2023. Adaptive Filter Pruning
    via Sensitivity Feedback. *TNNLS* (2023), 1–13.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018b) Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu. 2018b.
    Deep mutual learning. In *CVPR*. 4320–4328.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019a) Z. Zhang, J. Li, W. Shao, Z. Peng, R. Zhang, X. Wang, and
    P. Luo. 2019a. Differentiable learning-to-group channels via groupable convolutional
    neural networks. In *ICCV*. 3542–3551.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2022) B. Zhao, Q. Cui, R. Song, Y. Qiu, and J. Liang. 2022. Decoupled
    Knowledge Distillation. In *CVPR*. 11953–11962.
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2020) D. Zhou, Q. Hou, Y. Chen, J. Feng, and S. Yan. 2020. Rethinking
    bottleneck structure for efficient mobile network design. In *ECCV*. 680–697.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2018) X. Zhou, Z. Du, Q. Guo, S. Liu, C. Liu, C. Wang, X. Zhou,
    L. Li, T. Chen, and Y. Chen. 2018. Cambricon-S: Addressing irregularity in sparse
    neural networks through a cooperative software/hardware approach. In *MICRO*.
    15–28.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2021) Y. Zhou, X. Dong, B. Akin, M. Tan, D. Peng, T. Meng, A. Yazdanbakhsh,
    D. Huang, R. Narayanaswami, and J. Laudon. 2021. Rethinking co-design of neural
    architectures and hardware accelerators. *arXiv preprint arXiv:2102.08619* (2021).
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017) C. Zhu, S. Han, H. Mao, and W. J. Dally. 2017. Trained Ternary
    Quantization. In *ICLR*.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoph and Le (2017) B. Zoph and Q. V. Le. 2017. Neural architecture search with
    reinforcement learning. *ICLR* (2017).
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
