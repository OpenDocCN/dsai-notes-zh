- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:33:22'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:33:22'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2404.07236] Lightweight Deep Learning for Resource-Constrained Environments:
    A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2404.07236] 资源受限环境下的轻量级深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.07236](https://ar5iv.labs.arxiv.org/html/2404.07236)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.07236](https://ar5iv.labs.arxiv.org/html/2404.07236)
- en: 'Lightweight Deep Learning for Resource-Constrained Environments: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源受限环境下的轻量级深度学习：综述
- en: Hou-I Liu [k39967.c@nycu.edu.tw](mailto:k39967.c@nycu.edu.tw) [0000-0002-2101-2997](https://orcid.org/0000-0002-2101-2997
    "ORCID identifier") Department of Electronics and Electrical Engineering, National
    Yang Ming Chiao Tung UniversityHsinchuTaiwan, ROC300 ,  Marco Galindo [marcodavidg@gmail.com](mailto:marcodavidg@gmail.com)
    Department of Electrical Engineering and Computer Science, National Yang Ming
    Chiao Tung UniversityHsinchuTaiwan, ROC300 ,  Hongxia Xie College of Computer
    Science and Technology, Jilin UniversityKey Laboratory of Symbolic Computation
    and Knowledge Engineering of Ministry of Education, Jilin UniversityChangchunChina130000
    [hongxiaxie.ee08@nycu.edu.tw](mailto:hongxiaxie.ee08@nycu.edu.tw) ,  Lai-Kuan
    Wong [lkwong@mmu.edu.my](mailto:lkwong@mmu.edu.my) Faculty of Computing and Informatics,
    Multimedia UniversityCyberjayaMalaysia63100 ,  Hong-Han Shuai Department of Electronics
    and Electrical Engineering, National Yang Ming Chiao Tung UniversityHsinchuTaiwan,
    ROC300 [hhshuai@nycu.edu.tw](mailto:hhshuai@nycu.edu.tw) ,  Yung-Hui Li Hon Hai
    Research InstituteTaipeiTaiwan, ROC114 [yunghui.li@foxconn.com](mailto:yunghui.li@foxconn.com)
     and  Wen-Huang Cheng Department of Computer Science and Information Engineering,
    National Taiwan UniversityTaipeiTaiwan, ROC106 [wenhuang@csie.ntu.edu.tw](mailto:wenhuang@csie.ntu.edu.tw)(2022)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Hou-I Liu [k39967.c@nycu.edu.tw](mailto:k39967.c@nycu.edu.tw) [0000-0002-2101-2997](https://orcid.org/0000-0002-2101-2997
    "ORCID identifier") 国立阳明交通大学电子与电气工程系，新竹，台湾，ROC300，Marco Galindo [marcodavidg@gmail.com](mailto:marcodavidg@gmail.com)
    国立阳明交通大学电气工程与计算机科学系，新竹，台湾，ROC300，Hongxia Xie 吉林大学计算机科学与技术学院，教育部符号计算与知识工程重点实验室，吉林大学，长春，中国，130000
    [hongxiaxie.ee08@nycu.edu.tw](mailto:hongxiaxie.ee08@nycu.edu.tw)，Lai-Kuan Wong
    [lkwong@mmu.edu.my](mailto:lkwong@mmu.edu.my) 多媒体大学计算与信息学院，赛城，马来西亚，63100，Hong-Han
    Shuai 国立阳明交通大学电子与电气工程系，新竹，台湾，ROC300 [hhshuai@nycu.edu.tw](mailto:hhshuai@nycu.edu.tw)，Yung-Hui
    Li 鸿海研究院，台北，台湾，ROC114 [yunghui.li@foxconn.com](mailto:yunghui.li@foxconn.com)
    和 Wen-Huang Cheng 国立台湾大学计算机科学与信息工程系，台北，台湾，ROC106 [wenhuang@csie.ntu.edu.tw](mailto:wenhuang@csie.ntu.edu.tw)(2022)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Over the past decade, the dominance of deep learning has prevailed across various
    domains of artificial intelligence, including natural language processing, computer
    vision, and biomedical signal processing. While there have been remarkable improvements
    in model accuracy, deploying these models on lightweight devices, such as mobile
    phones and microcontrollers, is constrained by limited resources. In this survey,
    we provide comprehensive design guidance tailored for these devices, detailing
    the meticulous design of lightweight models, compression methods, and hardware
    acceleration strategies. The principal goal of this work is to explore methods
    and concepts for getting around hardware constraints without compromising the
    model’s accuracy. Additionally, we explore two notable paths for lightweight deep
    learning in the future: deployment techniques for TinyML and Large Language Models.
    Although these paths undoubtedly have potential, they also present significant
    challenges, encouraging research into unexplored areas.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年里，深度学习在自然语言处理、计算机视觉和生物医学信号处理等人工智能各个领域占据了主导地位。尽管模型的准确性取得了显著提高，但在轻量级设备上部署这些模型，如手机和微控制器，由于资源有限而受到限制。在本综述中，我们提供了针对这些设备的全面设计指导，详细介绍了轻量级模型的精细设计、压缩方法和硬件加速策略。本研究的主要目标是探索在不妥协模型准确性的情况下绕过硬件限制的方法和概念。此外，我们还探讨了未来轻量级深度学习的两个重要方向：TinyML和大语言模型的部署技术。尽管这些方向无疑具有潜力，但也面临着重大挑战，鼓励对未探索领域进行研究。
- en: 'Lightweight model, efficient transformer, model compression, quantization,
    tinyML, large language models^†^†copyright: acmcopyright^†^†journalyear: 2022^†^†doi:
    XXXXXXX.XXXXXXX^†^†booktitle: Woodstock ’18: ACM Symposium on Neural Gaze Detection,
    June 03–05, 2018, Woodstock, NY^†^†price: 15.00^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs:
    Computing methodologies Neural networks^†^†ccs: Computing methodologies Artificial
    intelligence^†^†ccs: Computing methodologies Computer vision^†^†ccs: Computing
    methodologies Model compression^†^†ccs: Computer systems organization Embedded
    systems^†^†ccs: Software and its engineering Designing software^†^†ccs: Software
    and its engineering Software design techniques'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '轻量级模型，高效变换器，模型压缩，量化，tinyML，大型语言模型^†^†版权：acmcopyright^†^†期刊年份：2022^†^†doi: XXXXXXX.XXXXXXX^†^†书名：Woodstock
    ’18: ACM Symposium on Neural Gaze Detection, June 03–05, 2018, Woodstock, NY^†^†价格：15.00^†^†isbn:
    978-1-4503-XXXX-X/18/06^†^†ccs: 计算方法 神经网络^†^†ccs: 计算方法 人工智能^†^†ccs: 计算方法 计算机视觉^†^†ccs:
    计算方法 模型压缩^†^†ccs: 计算机系统组织 嵌入式系统^†^†ccs: 软件及其工程 软件设计^†^†ccs: 软件及其工程 软件设计技术'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Over recent years, the importance of neural networks (NNs) has escalated tremendously,
    with their applications permeating various aspects of daily life and extending
    to support complex tasks (Hidayati et al., [2020](#bib.bib85); Xie et al., [2023](#bib.bib223);
    Chen et al., [2021b](#bib.bib19)). However, since the publication of AlexNet (Krizhevsky
    et al., [2012](#bib.bib111)) in 2012, there has been a prevailing trend toward
    creating deeper and more intricate networks to enhance accuracy. For instance,
    Model Soups (Wortsman et al., [2022](#bib.bib216)) has achieved remarkable accuracy
    on the ImageNet dataset, but it comes at the cost of over 1,843 million parameters.
    Similarly, GPT-4 (Bastian, [2023](#bib.bib11)) has demonstrated outstanding performance
    on natural language processing (NLP) benchmarks, albeit with a staggering 1.76
    trillion parameters. Notably, Amodei et al. (Amodei and Hernandez, [2018](#bib.bib5))
    indicated that the computational demands of deep learning (DL) have surged dramatically,
    increasing by approximately 300,000 times from 2012 to 2018\. This dramatic increase
    in size sets the stage for the challenges and developments explored in this paper.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，神经网络（NNs）的重要性急剧上升，其应用渗透到日常生活的各个方面，并扩展到支持复杂任务中（Hidayati et al., [2020](#bib.bib85)；Xie
    et al., [2023](#bib.bib223)；Chen et al., [2021b](#bib.bib19)）。然而，自2012年AlexNet（Krizhevsky
    et al., [2012](#bib.bib111)）发布以来，创建更深、更复杂的网络以提高准确性已成为一种主流趋势。例如，Model Soups（Wortsman
    et al., [2022](#bib.bib216)）在ImageNet数据集上取得了显著的准确性，但其参数超过18.43亿。同样，GPT-4（Bastian,
    [2023](#bib.bib11)）在自然语言处理（NLP）基准测试中表现出色，尽管其参数高达惊人的1.76万亿。值得注意的是，Amodei等人（Amodei和Hernandez,
    [2018](#bib.bib5)）指出，深度学习（DL）的计算需求急剧增加，从2012年到2018年增长了约30万倍。这种规模的急剧增加为本文探讨的挑战和发展奠定了基础。
- en: Concurrently, Green AI (Schwartz et al., [2020](#bib.bib170); Talwalkar, [2020](#bib.bib189))
    has arisen as a prominent concern over the past few years, labeling hefty DL models
    unsuitable due to their substantial GPU and training time demands, which can contribute
    to environmental degradation. Strubell et al. (Strubell et al., [2019](#bib.bib179))
    extensively analyze the carbon footprint of language models trained on multiple
    GPUs. In parallel, lightweight devices have garnered increased attention due to
    their versatile applications and portability. According to Sinha (Sinha, [2023](#bib.bib175)),
    the number of connected IoT devices grew by 18% in 2022, reaching 14.4 billion,
    and has a projected escalation to 29.0 billion by 2027\. A testament to this growing
    demand is the production of over 200 million iPhones since 2016\. On the other
    hand, edge devices offer superior automation and energy efficiency compared to
    mobile devices, especially the deployment of ultra-low-cost microcontrollers (MCUs)
    in devices like pacemakers and forehead thermometers (Dubey et al., [2019](#bib.bib47)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，绿色人工智能（Schwartz 等人，[2020](#bib.bib170)；Talwalkar，[2020](#bib.bib189)）在过去几年中成为一个突出的关注点，因其重型深度学习模型由于高额的
    GPU 和训练时间需求，被标记为不适用，这可能导致环境恶化。Strubell 等人（Strubell 等人，[2019](#bib.bib179)）对在多
    GPU 上训练的语言模型的碳足迹进行了广泛分析。同时，轻量级设备因其多功能应用和便携性而受到越来越多的关注。根据 Sinha（Sinha，[2023](#bib.bib175)），2022年连接的物联网设备数量增长了18%，达到144亿，并预计到2027年将增加至290亿。自2016年以来生产了超过2亿部
    iPhone，证明了这一日益增长的需求。另一方面，边缘设备相比移动设备提供了更高的自动化和能效，特别是超低成本的微控制器（MCU）在起搏器和额头温度计等设备中的应用（Dubey
    等人，[2019](#bib.bib47)）。
- en: In response to the practical demands outlined above, a significant body of research
    has emerged in recent years, focusing on lightweight modeling, model compression,
    and acceleration techniques. The Annual Mobile AI (MAI) workshops have been held
    consecutively during CVPR 2021-2023 (MAI, [2021](#bib.bib140), [2022](#bib.bib141),
    [2023](#bib.bib142)), with a primary emphasis on the deployment of DL models for
    image processing on resource-constrained devices, such as ARM Mali GPUs and Raspberry
    Pi 4\. Additionally, the Advances in Image Manipulation (AIM) workshops conducted
    at ICCV 2019, ICCV 2021, and ECCV 2022 (AIM, [2022](#bib.bib4)) have organized
    challenges centered around image/video manipulation, restoration, and enhancement
    on mobile devices.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 针对上述实际需求，近年来涌现出大量研究，重点关注轻量级建模、模型压缩和加速技术。年度移动人工智能（MAI）研讨会连续在 CVPR 2021-2023（MAI，[2021](#bib.bib140)，[2022](#bib.bib141)，[2023](#bib.bib142)）期间举行，主要强调在资源受限的设备上部署深度学习模型，如
    ARM Mali GPU 和 Raspberry Pi 4。此外，在 ICCV 2019、ICCV 2021 和 ECCV 2022 举办的图像处理进展（AIM）研讨会（AIM，[2022](#bib.bib4)）围绕移动设备上的图像/视频处理、恢复和增强组织了挑战。
- en: 'From our study, we discovered that the most effective approach for analyzing
    the development of an efficient, lightweight model, spanning from its design phase
    to deployment, involves incorporating three key elements into the pipeline: NN
    architecture design, compression methods, and hardware acceleration for lightweight
    DL models. Previous surveys (Gou et al., [2021](#bib.bib63); Gupta and Agrawal,
    [2022](#bib.bib70); Rokh et al., [2023](#bib.bib166); Berthelier et al., [2021](#bib.bib12);
    Liang et al., [2021b](#bib.bib122)) often focus on specific aspects of this pipeline,
    such as discussing only quantization methods, offering detailed insights into
    those segments. However, these surveys may not provide a comprehensive view of
    the entire process, potentially overlooking significant alternative approaches
    and techniques. In contrast, our survey covers lightweight architectures, compression
    methods, and hardware acceleration algorithms.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的研究中，我们发现分析高效、轻量级模型开发的最有效方法，从设计阶段到部署阶段，需要将三大关键要素融入流程中：神经网络架构设计、压缩方法和轻量级深度学习模型的硬件加速。之前的调查（Gou
    等人，[2021](#bib.bib63)；Gupta 和 Agrawal，[2022](#bib.bib70)；Rokh 等人，[2023](#bib.bib166)；Berthelier
    等人，[2021](#bib.bib12)；Liang 等人，[2021b](#bib.bib122)）通常关注流程中的特定方面，例如仅讨论量化方法，提供对这些细节的深入见解。然而，这些调查可能无法提供对整个过程的全面视角，可能忽略了重要的替代方法和技术。相反，我们的调查涵盖了轻量级架构、压缩方法和硬件加速算法。
- en: 1.1\. Neural Network Design
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1. 神经网络设计
- en: 'In the first part of this article, Section [2](#S2 "2\. Lightweight Architecture
    Design ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey"),
    we examine the classic lightweight architectures, categorizing them into family
    series for improved clarity. Some of these architectures made significant strides
    by introducing innovative convolution blocks. For instance, depthwise separable
    convolutions (Chollet, [2017](#bib.bib36)) prioritize high accuracy and reduced
    computational demand. Sandler et al. (Sandler et al., [2018](#bib.bib169)) introduce
    an inverted residual bottleneck to enhance gradient propagation. Other architectures,
    such as ShuffleNet (Zhang et al., [2018c](#bib.bib249)), were able to develop
    an optimized convolution operation, which applies group convolution (Krizhevsky
    et al., [2012](#bib.bib111)) to achieve a parallel design and further improve
    the transferability between groups of data through shuffle operations. The ShiftNet (Wu
    et al., [2018](#bib.bib218)) achieves an equivalence effect of traditional convolution
    with no parameters or Floating Point Operations (FLOPs). The AdderNet (Chen et al.,
    [2020b](#bib.bib22)) replaces the multiplication operation with the addition operation,
    greatly reducing computation requirements.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的第一部分，即第[2](#S2 "2\. 轻量级架构设计 ‣ 资源受限环境中的轻量级深度学习：一项综述")节中，我们研究了经典的轻量级架构，并将它们分类为系列，以提高清晰度。这些架构中的一些通过引入创新的卷积块取得了显著的进展。例如，深度可分离卷积 (Chollet,
    [2017](#bib.bib36))优先考虑高精度和降低计算需求。Sandler等人 (Sandler et al., [2018](#bib.bib169))引入了倒残差瓶颈，以增强梯度传播。其他架构，如ShuffleNet (Zhang
    et al., [2018c](#bib.bib249))，能够开发出优化的卷积操作，该操作应用了组卷积 (Krizhevsky et al., [2012](#bib.bib111))以实现并行设计，并通过洗牌操作进一步提高数据组之间的可迁移性。ShiftNet (Wu
    et al., [2018](#bib.bib218))在没有参数或浮点运算（FLOPs）的情况下实现了传统卷积的等效效果。AdderNet (Chen et al.,
    [2020b](#bib.bib22))用加法操作替代了乘法操作，大大降低了计算需求。
- en: It is also important to note that parameters and FLOPs do not consistently correlate
    with inference time. Early lightweight architectures, such as SqueezeNet (Iandola
    et al., [2017](#bib.bib99)) and MobileNet (Howard et al., [2017](#bib.bib90)),
    aim to reduce parameters and FLOPs. However, this reduction often increases Memory
    Access Cost (MAC) (Ma et al., [2018](#bib.bib139)), leading to slower inference.
    Hence, we aim to contribute to the application of lightweight models by providing
    a more comprehensive and insightful review.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，参数和FLOPs（浮点运算次数）与推理时间并不总是具有一致的相关性。早期的轻量级架构，如SqueezeNet (Iandola et al.,
    [2017](#bib.bib99))和MobileNet (Howard et al., [2017](#bib.bib90))，旨在减少参数和FLOPs。然而，这种减少往往会增加内存访问成本（MAC） (Ma
    et al., [2018](#bib.bib139))，导致推理速度变慢。因此，我们的目标是通过提供更全面和有洞察力的评估，来推动轻量级模型的应用。
- en: 1.2\. Neural Network Compression
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 神经网络压缩
- en: 'In addition to lightweight architecture designs, Section [3](#S3 "3\. Fundamental
    methods in model compression ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") mentions various efficient algorithms that can be applied
    to compress a given architecture. For example, quantization methods (Yao et al.,
    [2021](#bib.bib231); Liu et al., [2021b](#bib.bib133); Hubara et al., [2016](#bib.bib98))
    aim to reduce the required storage for data, often by substituting 32-bit floating-point
    numbers with 8-bit or 16-bit numbers or even utilizing binary values to represent
    the data. Pruning algorithms (Frankle and Carbin, [2019](#bib.bib55); Guo et al.,
    [2016](#bib.bib68); Lee et al., [2019](#bib.bib115)), in their simplest form,
    remove parameters from a model to eliminate unnecessary redundancies within the
    network. Yet, more sophisticated algorithms may remove entire channels or filters
    from the network (He et al., [2019a](#bib.bib82); Liu et al., [2019a](#bib.bib136)).
    Knowledge distillation (KD) techniques (Hinton et al., [2015](#bib.bib86); Gou
    et al., [2021](#bib.bib63)) explore the concept of transferring knowledge from
    one model, referred to as the ”teacher”, to another, called the ”student”. The
    teacher represents a large pre-trained model with the desired knowledge, whereas
    the student denotes an untrained smaller model tasked with extracting knowledge
    from the teacher. However, as methods evolved, some algorithms (Yuan et al., [2020](#bib.bib240);
    An et al., [2022](#bib.bib6)) modify the methodology by using the same network
    twice, eliminating the need for an extra teacher model. As these various compression
    methods progress, it is common to observe the adoption of two or more techniques,
    exemplified by the fusion of methods such as pruning and quantization in the same
    model.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了轻量级架构设计，章节 [3](#S3 "3\. 基本的模型压缩方法 ‣ 面向资源受限环境的轻量级深度学习：一项综述") 提到了一些可以应用于压缩给定架构的高效算法。例如，量化方法
    (Yao et al., [2021](#bib.bib231); Liu et al., [2021b](#bib.bib133); Hubara et
    al., [2016](#bib.bib98)) 旨在减少数据所需的存储，通常通过用 8 位或 16 位数字甚至二进制值替代 32 位浮点数来表示数据。剪枝算法
    (Frankle and Carbin, [2019](#bib.bib55); Guo et al., [2016](#bib.bib68); Lee et
    al., [2019](#bib.bib115)) 在其最简单的形式下，从模型中删除参数，以消除网络中的不必要冗余。然而，更复杂的算法可能会从网络中移除整个通道或过滤器
    (He et al., [2019a](#bib.bib82); Liu et al., [2019a](#bib.bib136))。知识蒸馏（KD）技术
    (Hinton et al., [2015](#bib.bib86); Gou et al., [2021](#bib.bib63)) 探讨了将知识从一个称为“教师”的模型转移到另一个称为“学生”的模型的概念。教师代表一个具有所需知识的大型预训练模型，而学生则表示一个未经过训练的小型模型，负责从教师那里提取知识。然而，随着方法的发展，一些算法
    (Yuan et al., [2020](#bib.bib240); An et al., [2022](#bib.bib6)) 修改了方法，通过两次使用相同的网络，消除了额外教师模型的需求。随着这些压缩方法的进展，常常可以观察到两种或更多技术的结合，例如在同一模型中融合剪枝和量化方法。
- en: Additionally, we discuss Neural Architecture Search (NAS) algorithms, a set
    of techniques designed to automate the model creation process while reducing human
    intervention. These algorithms autonomously search for optimal factors within
    a defined search space, such as network depth and filter settings. Research in
    this domain primarily focuses on refining the definition, traversal, and evaluation
    of the search space to achieve high accuracy without excessive time and resource
    consumption.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还讨论了神经结构搜索（NAS）算法，这是一组旨在自动化模型创建过程并减少人工干预的技术。这些算法自主地在定义的搜索空间内（如网络深度和过滤器设置）寻找最优因素。该领域的研究主要集中在优化搜索空间的定义、遍历和评估，以在不消耗过多时间和资源的情况下实现高准确率。
- en: 1.3\. Neural Network Deployment
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3\. 神经网络部署
- en: 'In Section [4](#S4 "4\. Hardware Acceleration of Deep Learning Models ‣ Lightweight
    Deep Learning for Resource-Constrained Environments: A Survey"), we navigate through
    the landscape of prevalent hardware accelerators dedicated to DL applications,
    including Graphics Processing Units (GPUs), Field-Programmable Gate Arrays (FPGAs),
    and Tensor Processing Units (TPUs). Moreover, we describe various dataflow types (Chen
    et al., [2014a](#bib.bib24); Jouppi et al., [2017](#bib.bib104); Guo et al., [2017](#bib.bib66);
    Lin and Chang, [2017](#bib.bib129)) and delve into data locality optimization
    methods (Mezdour et al., [2023](#bib.bib147); Zhang et al., [2015](#bib.bib242);
    Stoutchinin et al., [2019](#bib.bib178)), exploring the intricate techniques that
    underpin efficient processing in DL workflows. The narrative further unfolds with
    a discussion of popular DL libraries (Abadi et al., [2016](#bib.bib2); Paszke
    et al., [2019](#bib.bib154); Chen et al., [2016](#bib.bib25)) tailored for accelerating
    DL processes. This review encompasses the diverse tools and frameworks playing
    pivotal roles in optimizing the utilization of hardware accelerators. Additionally,
    we investigate co-designed solutions (Wang et al., [2020b](#bib.bib213); Parashar
    et al., [2017](#bib.bib153); Cho et al., [2021](#bib.bib33)), where achieving
    optimized and holistic results in accelerated DL requires careful consideration
    of hardware architecture and compression methods.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[4](#S4 "4\. 硬件加速深度学习模型 ‣ 资源受限环境下的轻量级深度学习：调查")节中，我们探讨了针对深度学习应用的流行硬件加速器的现状，包括图形处理单元（GPUs）、现场可编程门阵列（FPGAs）和张量处理单元（TPUs）。此外，我们描述了各种数据流类型（Chen
    et al., [2014a](#bib.bib24); Jouppi et al., [2017](#bib.bib104); Guo et al., [2017](#bib.bib66);
    Lin and Chang, [2017](#bib.bib129)），并深入研究了数据局部性优化方法（Mezdour et al., [2023](#bib.bib147);
    Zhang et al., [2015](#bib.bib242); Stoutchinin et al., [2019](#bib.bib178)），探索了支撑深度学习工作流程中高效处理的复杂技术。叙述还进一步展开，讨论了为加速深度学习过程而量身定制的流行深度学习库（Abadi
    et al., [2016](#bib.bib2); Paszke et al., [2019](#bib.bib154); Chen et al., [2016](#bib.bib25)）。本综述涵盖了在优化硬件加速器利用方面发挥关键作用的多样化工具和框架。此外，我们还调查了共同设计的解决方案（Wang
    et al., [2020b](#bib.bib213); Parashar et al., [2017](#bib.bib153); Cho et al.,
    [2021](#bib.bib33)），在加速深度学习中实现优化和整体结果需要仔细考虑硬件架构和压缩方法。
- en: 1.4\. Challenge and Future work
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4\. 挑战与未来工作
- en: 'Lastly, in Section [5](#S5 "5\. Challenge and Future work ‣ Lightweight Deep
    Learning for Resource-Constrained Environments: A Survey"), we embark on an exploration
    of emerging TinyML techniques designed to execute DL models on ultra-low-power
    devices, like MCUs, which typically consume less than 1 mW of power. Additionally,
    our paper delves into the intricacies of Large Language Models (LLMs), which present
    deployment challenges on devices with limited resources due to their enormous
    model sizes. As promising avenues in computer vision, deploying these methods
    on edge devices is crucial for widespread application.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第[5](#S5 "5\. 挑战与未来工作 ‣ 资源受限环境下的轻量级深度学习：调查")节中，我们展开了对新兴TinyML技术的探索，这些技术旨在在超低功耗设备（如MCUs）上执行深度学习模型，这些设备通常消耗不到1
    mW的功率。此外，我们的论文深入探讨了大型语言模型（LLMs）的复杂性，由于其巨大的模型规模，这些模型在资源有限的设备上部署面临挑战。作为计算机视觉中的有前途的途径，将这些方法部署到边缘设备上对于广泛应用至关重要。
- en: 1.5\. Contributions
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5\. 贡献
- en: 'This paper aims to describe in a simple but accurate manner how lightweight
    architectures, compression methods, and hardware techniques can be leveraged to
    implement an accurate model in a resource-constrained device. Our main contributions
    are summarized below:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文旨在以简单但准确的方式描述如何利用轻量级架构、压缩方法和硬件技术在资源受限的设备上实现准确的模型。我们的主要贡献总结如下：
- en: (1)
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Previous surveys only briefly reference a small number of works on lightweight
    architecture. We organize lightweight architectures into series, such as grouping
    MobileNetV1-V3 and MobileNeXt in the MobileNet series, and provide a history of
    lightweight architectures from their inception to the present.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之前的调查仅简要提及了一小部分关于轻量级架构的工作。我们将轻量级架构组织成系列，例如将MobileNetV1-V3和MobileNeXt归入MobileNet系列，并提供了轻量级架构从起源到现在的发展历史。
- en: (2)
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: To cover the entire lightweight DL applications, we also cover the compression
    and hardware acceleration methods. Unlike many other surveys that do not explicitly
    establish connections between these techniques, our survey offers a thorough overview
    of each domain, providing a comprehensive understanding of their interconnections.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了涵盖整个轻量级深度学习应用，我们还讨论了压缩和硬件加速方法。与许多其他调查未能明确建立这些技术之间的联系不同，我们的调查提供了对每个领域的全面概述，提供了对它们相互关系的深入理解。
- en: (3)
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: As part of the forefront advancements in lightweight DL, we review the present
    challenges and explore future works. Firstly, we explore TinyML, an emerging approach
    engineered for deploying DL models on devices with remarkably constrained resources.
    Subsequently, we investigate various contemporary initiatives harnessing LLMs
    on edge devices, a promising direction in the realm of lightweight DL.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作为轻量级深度学习前沿进展的一部分，我们回顾了目前的挑战并探索了未来的工作。首先，我们探讨了TinyML，这是一种新兴的方法，旨在将深度学习模型部署在资源极其受限的设备上。随后，我们研究了利用边缘设备上大规模语言模型（LLMs）的各种现代举措，这是轻量级深度学习领域中的一个有前景的方向。
- en: 2\. Lightweight Architecture Design
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 轻量级架构设计
- en: To ease readers’ comprehension, we first introduce the fundamental knowledge
    of lightweight architecture, including the general metrics to estimate the computation
    cost of the NN and the widely used mechanisms of model compression. Following
    that, we outline the lightweight CNN architecture and separate the sections by
    series, such as ShuffleNet and MobileNet series, according to their chronological
    order so that they can reflect the evolution of lightweight design and the advantage
    of its efficiency. Additionally, we discuss the efficient transformer, which offers
    a promising model capacity while maintaining a lightweight architecture.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于读者理解，我们首先介绍了轻量级架构的基础知识，包括估算神经网络计算成本的一般指标以及广泛使用的模型压缩机制。接下来，我们概述了轻量级卷积神经网络（CNN）架构，并按系列划分章节，如ShuffleNet和MobileNet系列，按照时间顺序反映轻量级设计的演变及其效率优势。此外，我们讨论了高效的变换器，这提供了一个有前景的模型能力，同时保持轻量级架构。
- en: 2.1\. Prior Knowledge of Lightweight Architecture
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1. 轻量级架构的前知识
- en: 2.1.1\. Evaluation metrics for deep learning model
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1. 深度学习模型的评估指标
- en: In DL, the three most commonly used metrics for model compression are Floating
    Point Operations (FLOPs), Multiply-Accumulate Operations (MACs), and Memory Access
    Cost (MAC). FLOPs is the number of arithmetic operations the model performs on
    the floating points, including addition, subtraction, multiplication, and division (Asperti
    et al., [2021](#bib.bib8)). Similar to FLOPs, MACs also represent the total number
    of the floating point operations; however, MACs treat addition and multiplication
    as equivalent operations, in contrast to FLOPs, which distinguish between them (Getzner
    et al., [2023](#bib.bib58)). Consequently, FLOPs $\approx$ 2$\times$MACs. On the
    other hand, MAC represents the amount of memory footprint of an NN, which corresponds
    to RAM usage (Ma et al., [2018](#bib.bib139)). Let $H$ and $W$ be the spatial
    size of the input and output feature maps for a convolution layer, $C_{in}$ is
    the number of input channels, $C_{out}$ is the number of output channels, and
    the kernel size is $k$,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，模型压缩的三种最常用指标是浮点运算（FLOPs）、乘加运算（MACs）和内存访问成本（MAC）。FLOPs是模型在浮点数上执行的算术操作的数量，包括加法、减法、乘法和除法（Asperti
    et al., [2021](#bib.bib8)）。类似于FLOPs，MACs也表示浮点运算的总数；然而，MACs将加法和乘法视为等效操作，而FLOPs则区分这两者（Getzner
    et al., [2023](#bib.bib58)）。因此，FLOPs $\approx$ 2$\times$MACs。另一方面，MAC表示神经网络的内存占用量，相当于RAM使用（Ma
    et al., [2018](#bib.bib139)）。设$H$和$W$为卷积层的输入和输出特征图的空间大小，$C_{in}$为输入通道数，$C_{out}$为输出通道数，$k$为卷积核大小，
- en: '| (1) |  | $\displaystyle MAC=H\cdot W(C_{in}+C_{out})+k\cdot k(C_{in}\times
    C_{out}).$ |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\displaystyle MAC=H\cdot W(C_{in}+C_{out})+k\cdot k(C_{in}\times
    C_{out}).$ |  |'
- en: 'Specifically, the first and second terms of Eq. [1](#S2.E1 "In 2.1.1\. Evaluation
    metrics for deep learning model ‣ 2.1\. Prior Knowledge of Lightweight Architecture
    ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") depict the memory footprint of the feature maps and weights
    for that particular convolution layer.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，方程[1](#S2.E1 "In 2.1.1\. Evaluation metrics for deep learning model ‣
    2.1\. Prior Knowledge of Lightweight Architecture ‣ 2\. Lightweight Architecture
    Design ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey")的第一项和第二项描述了特定卷积层的特征图和权重的内存占用。'
- en: Furthermore, the most widely used metrics for measuring the inference speed
    of a model are throughput and latency. Throughput refers to the amount of data
    that can be processed or the number of tasks executed within a specified period.
    During inference, throughput is measured by the number of inferences per second.
    Latency is a measure of timing between the input data arriving at a system and
    the output data being generated and can be expressed in seconds per inference.
    The relationship between throughput and latency can be derived directly, and the
    detailed formula can be found in (Sze et al., [2020](#bib.bib185)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最广泛使用的模型推理速度度量指标是吞吐量和延迟。吞吐量指的是在指定时间内可以处理的数据量或执行的任务数量。在推理过程中，吞吐量通过每秒推理次数来衡量。延迟是指从输入数据到达系统到输出数据生成之间的时间间隔，可以用每次推理的秒数来表示。吞吐量和延迟之间的关系可以直接推导，详细公式见(Sze等，[2020](#bib.bib185))。
- en: 2.1.2\. Pointwise convolution
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2\. 点卷积
- en: The pointwise convolution, also known as a $1\times 1$ convolution, was first
    introduced in the inception module (Szegedy et al., [2015](#bib.bib187)). The
    inception module inserts the pointwise convolutions at the bottleneck to obtain
    deeper features with fewer FLOPs. Empowered by the adaptability of pointwise convolutions
    to accommodate modifications to the channel’s dimensions, the Inception series
    of works was born (Szegedy et al., [2016](#bib.bib188), [2017](#bib.bib186); Chollet,
    [2017](#bib.bib36)). Significantly, pointwise convolutions directly affect the
    model’s computation time and the information richness of the architecture.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 点卷积，也称为$1\times 1$卷积，首次在Inception模块中引入（Szegedy等，[2015](#bib.bib187)）。Inception模块在瓶颈处插入点卷积，以获得更深层的特征，同时减少FLOP数。得益于点卷积对通道维度调整的适应性，Inception系列的工作应运而生（Szegedy等，[2016](#bib.bib188)，[2017](#bib.bib186)；Chollet，[2017](#bib.bib36)）。显著的是，点卷积直接影响模型的计算时间和架构的信息丰富性。
- en: 2.1.3\. Group convolution
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3\. 分组卷积
- en: The group convolution idea was proposed by AlexNet (Krizhevsky et al., [2012](#bib.bib111)).
    Group convolutions aim to divide the channels of feature maps into several groups
    and apply convolutions separately to each group. This process helps to reduce
    computational complexity by $N$ times, where $N$ represents the number of groups.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 分组卷积的概念由AlexNet提出（Krizhevsky等，[2012](#bib.bib111)）。分组卷积旨在将特征图的通道划分为多个组，并分别对每个组进行卷积。这一过程有助于将计算复杂度降低$N$倍，其中$N$表示组的数量。
- en: However, there are still several shortcomings in group convolutions. Firstly,
    group assignments are fixed, and this factor restricts the information flow between
    groups, inevitably harming performance. Secondly, group convolutions cost additional
    MAC, especially when the number of groups is large, resulting in a much longer
    inference time. To solve the first problem, ShuffleNet (Zhang et al., [2018c](#bib.bib249))
    shared group features to obtain deeper channel information. CondenseNet (Huang
    et al., [2018](#bib.bib94)) progressively prunes the unimportant connections using
    learned group convolutions (LGCs). Several works (Wang et al., [2019](#bib.bib211);
    Zhang et al., [2019a](#bib.bib252)) attempt to improve the original LGC to learn
    better optimal group structures. Furthermore, Dynamic Group Convolution (DGC) (Su
    et al., [2020](#bib.bib180)) highlights the importance of input channels via a
    salience generator and then uses a channel selector to assign groups adaptively.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，组卷积仍然存在几个缺点。首先，组分配是固定的，这限制了组之间的信息流动，不可避免地损害性能。其次，组卷积会额外消耗 MAC，特别是当组的数量很大时，导致推断时间更长。为了解决第一个问题，ShuffleNet（Zhang
    等人，[2018c](#bib.bib249)）共享组特征以获得更深的通道信息。CondenseNet（Huang 等人，[2018](#bib.bib94)）通过使用学习到的组卷积（LGCs）逐步修剪不重要的连接。几种方法（Wang等人，[2019](#bib.bib211)；Zhang等人，[2019a](#bib.bib252)）尝试改进原始的LGC，以学习更好的最优组结构。此外，动态组卷积
    (DGC)（Su等人，[2020](#bib.bib180)）通过一个显著性生成器突出显示输入通道的重要性，然后使用通道选择器自适应地分配组。
- en: 2.1.4\. Depthwise separable convolution
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4\. 深度可分离卷积
- en: The idea of a depthwise separable convolution was proposed in Xception (Chollet,
    [2017](#bib.bib36)), which is the advanced version of the Inception family (Szegedy
    et al., [2016](#bib.bib188), [2017](#bib.bib186)). A depthwise separable convolution
    consists of a depthwise convolution followed by a pointwise convolution. According
    to the MAC, this is a computation-saving but time-consuming operation. To address
    this issue, Tan et al. (Tan and Le, [2019b](#bib.bib194)) aggregate multiple kernel
    sizes into a single depthwise convolution and use AutoML (He et al., [2021](#bib.bib79))
    for navigating the search space.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 深度可分离卷积的思想是在 Xception（Chollet，[2017](#bib.bib36)）中提出的，它是 Inception 系列的高级版本（Szegedy
    等人，[2016](#bib.bib188) ，[2017](#bib.bib186)）。深度可分离卷积由深度卷积和逐点卷积组成。根据 MAC，这是一种节省计算但耗时的操作。为了解决这个问题，Tan
    等人（Tan 和 Le，[2019b](#bib.bib194)）将多个内核大小聚合成一个单独的深度卷积，并使用 AutoML（He 等人，[2021](#bib.bib79)）进行导航搜索空间。
- en: 2.2\. Lightweight CNN Architecture
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 轻量级卷积神经网络架构
- en: 2.2.1\. SqueezeNet series
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. SqueezeNet 系列
- en: SqueezeNet series (Iandola et al., [2017](#bib.bib99); Gholami et al., [2018](#bib.bib60))
    is an early application to reduce parameters using pointwise convolution. SqueezeNet (Iandola
    et al., [2017](#bib.bib99)) proposes the fire module that constitutes the squeeze
    layer and the expand layer. The squeeze layer consists of pointwise convolution.
    It first squeezes features into lower dimensions and then passes them through
    an expansion layer, which separates the convolution operation into a pointwise
    convolution and a 3$\times$3 convolution. To solve the gradient vanishing problem
    and decrease the computation cost, SqueezeNext (Gholami et al., [2018](#bib.bib60))
    keeps the shortcut concept from ResNet (He et al., [2016](#bib.bib78)) and decomposed
    3$\times$3 kernel into two low-rank kernels, with sizes of 3$\times$1 and 1$\times$3\.
    This augmented design reduces the parameters of the kernels from $k^{2}$ to $2k$,
    hence solving the inefficient problem of using depthwise separable convolutions.
    Compared to AlexNet (Krizhevsky et al., [2012](#bib.bib111)), SqueezeNet and SqueezeNext
    reduce the parameters by 50$\times$ and 112$\times$, respectively, while keeping
    AlexNet’s level of accuracy on the ImageNet dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: SqueezeNet 系列 （Iandola 等人，[2017](#bib.bib99)；Gholami 等人，[2018](#bib.bib60)）是早期应用的一个例子，使用逐点卷积来减少参数。SqueezeNet（Iandola
    等人，[2017](#bib.bib99)）提出了火模块，它由挤压层和扩展层构成。挤压层由逐点卷积组成。它首先将特征挤压到较低的维度，然后通过扩展层进行传递，将卷积操作分为逐点卷积和
    3×3 卷积。为了解决梯度消失问题和降低计算成本，SqueezeNext（Gholami 等人，[2018](#bib.bib60)）保留了ResNet（He
    等人，[2016](#bib.bib78)）中的快捷连接概念，并将 3×3 的卷积核分解为两个低秩卷积核，大小分别为 3×1 和 1×3。这种增强的设计将卷积核的参数从
    $k^{2}$ 减少到 $2k$，从而解决了使用深度可分离卷积的低效问题。与 AlexNet（Krizhevsky 等人，[2012](#bib.bib111)）相比，SqueezeNet
    和 SqueezeNext 分别将参数减少了 50 倍和 112 倍，同时保持了 AlexNet 在 ImageNet 数据集上的准确性水平。
- en: 2.2.2\. ShuffleNet series
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. ShuffleNet 系列
- en: The primary purpose of the ShuffleNet series (Zhang et al., [2018c](#bib.bib249);
    Ma et al., [2018](#bib.bib139)) is to improve the performance of group convolutions
    and the memory efficiency of depthwise separable convolutions. After a group convolution,
    each group’s output features form an individual channel, and performance suffers
    due to information not being shared between channels. To address this limitation,
    ShuffleNet (Zhang et al., [2018c](#bib.bib249)) applies a channel shuffle mechanism
    after the 1$\times$1 group convolution to facilitate cross-group information exchange
    so that features can maintain more global information channels.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ShuffleNet 系列 (Zhang et al., [2018c](#bib.bib249); Ma et al., [2018](#bib.bib139))
    的主要目的是提升组卷积的性能和深度可分卷积的内存效率。经过组卷积后，每个组的输出特征形成一个独立的通道，性能受到影响，因为通道之间的信息未被共享。为了解决这一限制，ShuffleNet (Zhang
    et al., [2018c](#bib.bib249)) 在 1$\times$1 组卷积之后应用了通道洗牌机制，以促进组间信息的交换，从而使特征能够保持更多的全局信息通道。
- en: ShuffleNetV2 (Ma et al., [2018](#bib.bib139)) investigates four practical guidelines
    to design a memory-efficient and lightweight model that can avoid heavy MAC problems.
    Firstly, equal input and output dimensions mean a smaller MAC. Secondly, MAC is
    large when groups are large, particularly for depthwise separable convolutions.
    Thirdly, it is best to avoid designing a wide network like the Inception series (Szegedy
    et al., [2015](#bib.bib187), [2016](#bib.bib188), [2017](#bib.bib186)) because
    network fragments can result in a large MAC. Lastly, since element-wise manipulation
    in a network requires extra computation, avoiding it is efficient. This is often
    overlooked because it represents only a few FLOPs but increases MAC, as in depthwise
    separable convolutions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ShuffleNetV2 (Ma et al., [2018](#bib.bib139)) 研究了设计一个内存高效且轻量化模型的四个实用指南，以避免严重的
    MAC 问题。首先，相等的输入和输出维度意味着较小的 MAC。其次，当组很大时，MAC 会很大，特别是对于深度可分卷积。第三，最好避免设计像 Inception
    系列 (Szegedy et al., [2015](#bib.bib187), [2016](#bib.bib188), [2017](#bib.bib186))
    这样的宽网络，因为网络碎片可能导致较大的 MAC。最后，由于网络中的逐元素操作需要额外的计算，避免这种操作是高效的。由于它仅占少量的 FLOPs，但会增加
    MAC，这一点常被忽视，例如在深度可分卷积中。
- en: 2.2.3\. CondenseNet series
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3\. CondenseNet 系列
- en: Since shortcut connections effectively prevent the gradient vanishing problem (He
    et al., [2016](#bib.bib78)), some studies, such as DenseNet (Huang et al., [2017](#bib.bib95))
    and the CondenseNet series (Huang et al., [2018](#bib.bib94); Yang et al., [2021a](#bib.bib228))
    attempt to optimize NN structure based on shortcut connections. DenseNet (Huang
    et al., [2017](#bib.bib95)) replaces the shortcut connections with dense connections,
    thus improving gradient flow within the bottleneck. Although dense connections
    increase the accuracy, CondenseNet (Huang et al., [2018](#bib.bib94)) observes
    that the magnitude of the connections far from the layers will decay exponentially,
    causing them to be heavy and slow. To this end, CondenseNet utilizes learned group
    convolutions (LGCs) to prune connections progressively. Before training, the filters
    are split into G groups of equal size. Suppose $C_{in}$ is the number of input
    channels, $C_{out}$ is the number of output channels, and $F_{ij}^{g}$ denotes
    the kernel weights, including the weights of $j_{th}$ input, and $i_{th}$ output
    within a group $g\in G$. The importance of the $j_{th}$ incoming feature map for
    the filter group $g$ is computed by the averaged absolute value of weights between
    them across all outputs within the group, i.e., $\sum_{i=1}^{\frac{C_{out}}{G}}{|F_{ij}^{g}|}$,
    where columns in $F^{g}$ with small L1-norm value can be removed by zeroing them
    out. The structured sparsity within a group can be evaluated by applying the group-lasso
    regularizer (Yuan and Lin, [2006](#bib.bib241)),
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于快捷连接有效地防止了梯度消失问题 (He et al., [2016](#bib.bib78))，一些研究，如 DenseNet (Huang et
    al., [2017](#bib.bib95)) 和 CondenseNet 系列 (Huang et al., [2018](#bib.bib94); Yang
    et al., [2021a](#bib.bib228))，尝试基于快捷连接优化神经网络结构。DenseNet (Huang et al., [2017](#bib.bib95))
    用稠密连接替代了快捷连接，从而改善了瓶颈中的梯度流。虽然稠密连接提高了准确性，但 CondenseNet (Huang et al., [2018](#bib.bib94))
    观察到，远离层的连接幅度会呈指数衰减，导致其变得沉重且缓慢。为此，CondenseNet 利用学习的组卷积 (LGCs) 逐步剪枝连接。在训练之前，滤波器被分成
    G 个相同大小的组。设 $C_{in}$ 为输入通道数，$C_{out}$ 为输出通道数，$F_{ij}^{g}$ 表示包括第 $j$ 个输入和第 $i$
    个输出在组 $g\in G$ 内的核权重。第 $j$ 个输入特征图对滤波器组 $g$ 的重要性通过在组内所有输出之间权重的绝对值的平均值计算，即 $\sum_{i=1}^{\frac{C_{out}}{G}}{|F_{ij}^{g}|}$，其中
    $F^{g}$ 中列的小 L1-范数值可以通过将其置零来移除。组内的结构稀疏性可以通过应用组套索正则化器 (Yuan and Lin, [2006](#bib.bib241))
    来评估。
- en: '| (2) |  | $r=\sum_{g=1}^{G}\sum_{j=1}^{C_{in}}\sqrt{\sum_{i=1}^{\frac{C_{out}}{G}}{F_{ij}^{g}}^{2}}.$
    |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $r=\sum_{g=1}^{G}\sum_{j=1}^{C_{in}}\sqrt{\sum_{i=1}^{\frac{C_{out}}{G}}{F_{ij}^{g}}^{2}}.$
    |  |'
- en: 'By using Eq. [2](#S2.E2 "In 2.2.3\. CondenseNet series ‣ 2.2\. Lightweight
    CNN Architecture ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey"), connections to less important
    features, represented by a small sparsity value, will be removed, resulting in
    effective pruning.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 Eq. [2](#S2.E2 "在 2.2.3\. CondenseNet 系列 ‣ 2.2\. 轻量级 CNN 架构 ‣ 2\. 轻量级架构设计
    ‣ 针对资源受限环境的轻量级深度学习：综述")，对不重要特征的连接（由小稀疏值表示）将被移除，从而实现有效的剪枝。
- en: Recently, CondenseNetV2 (Yang et al., [2021a](#bib.bib228)) pointed out that
    the fixed connection mode limits the opportunities for feature reuse. To address
    this limitation, CondenseNetV2 aims to reactivate outdated features through a
    novel sparse feature reactivation module. In CondenseNetV2, the weight connections
    within each block are learned during training, as opposed to CondenseNet, which
    fixes the model’s weight connections after pruning. As a result, this approach
    results in a performance gain by leveraging the underlying connection.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，CondenseNetV2 (Yang et al., [2021a](#bib.bib228)) 指出，固定的连接模式限制了特征重用的机会。为了解决这一限制，CondenseNetV2
    旨在通过一种新颖的稀疏特征重新激活模块重新激活过时的特征。在 CondenseNetV2 中，每个块内的权重连接在训练过程中进行学习，而 CondenseNet
    则在剪枝后固定模型的权重连接。因此，这种方法通过利用潜在连接带来了性能提升。
- en: 'Fig. [1](#S2.F1 "Figure 1 ‣ 2.2.3\. CondenseNet series ‣ 2.2\. Lightweight
    CNN Architecture ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey") illustrates a graphical comparison,
    highlighting the architectural differences between DenseNet, CondenseNet, and
    CondenseNetV2\. In DenseNet, weights between layers in a block are fully-connected,
    where weights in one layer are connected to weights in all other layers (solid-colored
    arrows). To make the network more efficient, CondenseNet uses LGCs to prune weight
    connections (gray dashed arrows), and once pruned, the connections for every block
    remain fixed, e.g., the connections in Block 1 and Block 2 are identical. CondenseNetV2
    proposes a sparse feature reactivation mechanism to learn the connections’ weights
    automatically during training. From Fig. [1](#S2.F1 "Figure 1 ‣ 2.2.3\. CondenseNet
    series ‣ 2.2\. Lightweight CNN Architecture ‣ 2\. Lightweight Architecture Design
    ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey"),
    we can observe that in Block 2 of CondenseNetV2, two pruned connections in Block
    1 are reactivated while another two previously active connections in Block 1 are
    removed, demonstrating the dynamic nature of CondenseNetV2.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1](#S2.F1 "Figure 1 ‣ 2.2.3\. CondenseNet series ‣ 2.2\. Lightweight CNN
    Architecture ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey") 展示了图形比较，突出了 DenseNet、CondenseNet
    和 CondenseNetV2 之间的架构差异。在 DenseNet 中，块内层之间的权重是全连接的，其中一层的权重与所有其他层的权重相连（实线箭头）。为了提高网络的效率，CondenseNet
    使用 LGCs 来修剪权重连接（灰色虚线箭头），修剪后，每个块的连接保持固定，例如，Block 1 和 Block 2 中的连接是相同的。CondenseNetV2
    提出了一个稀疏特征重新激活机制，以在训练过程中自动学习连接的权重。从图 [1](#S2.F1 "Figure 1 ‣ 2.2.3\. CondenseNet
    series ‣ 2.2\. Lightweight CNN Architecture ‣ 2\. Lightweight Architecture Design
    ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey")
    中，我们可以观察到，在 CondenseNetV2 的 Block 2 中，Block 1 中的两个被修剪的连接被重新激活，同时 Block 1 中的另外两个先前活跃的连接被移除，展示了
    CondenseNetV2 的动态特性。'
- en: '![Refer to caption](img/157296ae52c20bb1feb6f90685e95aeb.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/157296ae52c20bb1feb6f90685e95aeb.png)'
- en: Figure 1\. Comparison of DenseNet, CondenseNet, and CondenseNetV2. Active weight
    connections are represented by solid color arrows, and pruned weight connections
    are represented by gray dashed arrows.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. DenseNet、CondenseNet 和 CondenseNetV2 的比较。活跃的权重连接用实色箭头表示，被修剪的权重连接用灰色虚线箭头表示。
- en: 2.2.4\. MobileNet Series
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4\. MobileNet 系列
- en: This series (Howard et al., [2017](#bib.bib90); Sandler et al., [2018](#bib.bib169);
    Howard et al., [2019](#bib.bib89); Zhou et al., [2020](#bib.bib254)) includes
    prominent CNN models that can be deployed on IoT devices. Based on VGG (Simonyan
    and Zisserman, [2015](#bib.bib174)) architecture, MobileNet (Howard et al., [2017](#bib.bib90))
    applies depthwise separable convolutions to create an efficient model, which is
    shown to perform significantly faster across a broad range of tasks and applications.
    Discovering that ReLU activations can lead to severe information loss of features
    with lower dimensions, MobileNetV2 (Sandler et al., [2018](#bib.bib169)) replaces
    the ReLU activation with a linear combination in the last layer of the residual
    bottleneck to mitigate the information loss. In addition, MobileNetV2 introduces
    an inverted residual block, where the number of channels is first increased and
    then recovered in the residual bottleneck, improving the accuracy. Shortcut connections (He
    et al., [2016](#bib.bib78)) are also added to enhance the gradient propagation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这一系列 (Howard 等人，[2017](#bib.bib90)；Sandler 等人，[2018](#bib.bib169)；Howard 等人，[2019](#bib.bib89)；Zhou
    等人，[2020](#bib.bib254)) 包括了可以在 IoT 设备上部署的显著 CNN 模型。基于 VGG (Simonyan 和 Zisserman，[2015](#bib.bib174))
    架构，MobileNet (Howard 等人，[2017](#bib.bib90)) 采用深度可分离卷积来创建高效的模型，这表明其在广泛任务和应用中的表现显著更快。发现
    ReLU 激活可能导致低维特征的严重信息丢失，MobileNetV2 (Sandler 等人，[2018](#bib.bib169)) 用线性组合替代了残差瓶颈中的
    ReLU 激活，以减少信息丢失。此外，MobileNetV2 引入了倒置残差块，其中通道数首先增加，然后在残差瓶颈中恢复，提高了准确性。还添加了快捷连接 (He
    等人，[2016](#bib.bib78)) 以增强梯度传播。
- en: NetAdapt (Yang et al., [2018](#bib.bib229)) applies layer-wise optimization
    to simplify the network and to achieve high accuracy within limited hardware resources.
    Based on this, MobileNetV3 (Howard et al., [2019](#bib.bib89)) leverages platform-aware
    NAS (Tan et al., [2019a](#bib.bib191)) to optimize the block-wise structure and
    implements SENet (Hu et al., [2018](#bib.bib92)) (channel attention module) in
    the bottleneck structures, resulting in better accuracy. To reduce MAC and establish
    a quantization-friendly network, ReLU is replaced with H-swish activation. As
    an alternative to the inverted residual block, MobileNeXt (Zhou et al., [2020](#bib.bib254))
    develops a Sandglass block by flipping the inverted residual block to enhance
    the transmission of wider architectures since wider layers might lead to more
    gradient confusion, making model training harder.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: NetAdapt (Yang et al., [2018](#bib.bib229)) 通过逐层优化来简化网络，并在有限的硬件资源下实现高精度。基于此，MobileNetV3
    (Howard et al., [2019](#bib.bib89)) 利用平台感知 NAS (Tan et al., [2019a](#bib.bib191))
    来优化块级结构，并在瓶颈结构中实现 SENet (Hu et al., [2018](#bib.bib92))（通道注意模块），从而实现更好的准确性。为了减少
    MAC 并建立量化友好的网络，将 ReLU 替换为 H-swish 激活。作为倒残差块的替代方案，MobileNeXt (Zhou et al., [2020](#bib.bib254))
    通过翻转倒残差块开发了沙漏块，以增强更宽架构的传输，因为更宽的层可能导致更多的梯度混淆，使得模型训练更加困难。
- en: 2.2.5\. Shift-based series
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.5\. 基于位移的系列
- en: CNN is computationally expensive due to many multiplication and addition operations.
    ShiftNet (Wu et al., [2018](#bib.bib218)) pioneered the replacement of spatial
    convolutions with Group Shift convolution. Unlike standard convolutions, shift
    convolutions only perform shifting operations on feature maps and apply padding
    to those offset areas. In contrast to multiplication operations, shift convolution
    can achieve zero parameters and FLOPs, thus drastically reducing their number.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 由于大量的乘法和加法运算而计算成本高。ShiftNet (Wu et al., [2018](#bib.bib218)) 首先将空间卷积替换为组位移卷积。与标准卷积不同，位移卷积仅对特征图执行位移操作，并对那些偏移区域应用填充。与乘法运算相比，位移卷积可以实现零参数和
    FLOPs，从而大幅减少它们的数量。
- en: 'Some studies attempt to improve the performance based on shift convolution
    layers. For example, Jeon et al. (Jeon and Kim, [2018](#bib.bib101)) propose an
    Active Shift Layer that makes shifts learnable instead of heuristic assignments.
    Chen et al. (Chen et al., [2019a](#bib.bib27)) point out that because the number
    of shifts is fixed, implementing them requires a lot of trial and error, limiting
    the network’s functionality. Thus, they propose a Sparse Shift Layer to eliminate
    meaningless memory movement. The non-shift channels remain the same. Fig. [2](#S2.F2
    "Figure 2 ‣ 2.2.5\. Shift-based series ‣ 2.2\. Lightweight CNN Architecture ‣
    2\. Lightweight Architecture Design ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") compares these three shift operations.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '一些研究试图基于位移卷积层提高性能。例如，Jeon et al. (Jeon and Kim, [2018](#bib.bib101)) 提出了一个主动位移层，使得位移可学习，而不是启发式分配。Chen
    et al. (Chen et al., [2019a](#bib.bib27)) 指出，由于位移数量是固定的，实现它们需要大量的试错，这限制了网络的功能。因此，他们提出了一个稀疏位移层，以消除无意义的内存移动。非位移通道保持不变。图
    [2](#S2.F2 "Figure 2 ‣ 2.2.5\. Shift-based series ‣ 2.2\. Lightweight CNN Architecture
    ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") 比较了这三种位移操作。'
- en: '![Refer to caption](img/93a4b75c9297e83643a060487831f069.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/93a4b75c9297e83643a060487831f069.png)'
- en: (a) Group Shift
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 组位移
- en: '![Refer to caption](img/856d8054ceef40cee9ba10063001d386.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/856d8054ceef40cee9ba10063001d386.png)'
- en: (b) Active Shift
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 主动位移
- en: '![Refer to caption](img/b56fd3ea0c29c99fce5eefd8456d83f4.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/b56fd3ea0c29c99fce5eefd8456d83f4.png)'
- en: (c) Sparse Shift
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 稀疏位移
- en: Figure 2\. The variant of Shift-based convolution (Chen et al., [2019a](#bib.bib27)).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 基于位移的卷积变体 (Chen et al., [2019a](#bib.bib27))。
- en: AddressNet (He et al., [2019b](#bib.bib83)) observes that a smaller amount of
    parameters or computation (FLOPs) does not always lead to a direct reduction in
    inference time, even with shift convolution’s zero parameters and zero FLOPs (Wu
    et al., [2018](#bib.bib218)). To optimize the speed of GPU-based machines, AddressNet
    changes channel shuffle (Zhang et al., [2018c](#bib.bib249)) to channel shift
    since channel shuffle produces additional memory space and time-consuming permutations,
    further eliminating the redundant direction. Similar to AdderNet (Chen et al.,
    [2020b](#bib.bib22)), DeepShift (Elhoushi et al., [2021](#bib.bib49)) is constructed
    solely with addition operations, replacing all multiplications with bit-wise shifts
    and sign flips, significantly reduces the operation time and energy consumption.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: AddressNet (He et al., [2019b](#bib.bib83)) 观察到较少的参数或计算量（FLOPs）并不总是直接导致推理时间的减少，即使是移位卷积的零参数和零
    FLOPs (Wu et al., [2018](#bib.bib218))。为了优化基于 GPU 的机器的速度，AddressNet 将通道洗牌 (Zhang
    et al., [2018c](#bib.bib249)) 改为通道移位，因为通道洗牌会产生额外的内存空间和耗时的排列，进一步消除冗余方向。类似于 AdderNet
    (Chen et al., [2020b](#bib.bib22))，DeepShift (Elhoushi et al., [2021](#bib.bib49))
    仅使用加法运算，使用位移和符号翻转代替所有乘法，显著减少了操作时间和能量消耗。
- en: 2.2.6\. Add-based Series
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.6\. 基于加法的系列
- en: Multiplication and addition operations constitute many convolution operations,
    resulting in extra calculations. AdderNet (Chen et al., [2020b](#bib.bib22)) attempts
    to exclusively use additions, using an L1-norm distance as a response criterion
    between filters and feature maps. This operation is known as Absolute-difference-accumulation (Um
    et al., [2021](#bib.bib202)), and it accelerates the network and allows the reuse
    of computation results in order to reduce energy consumption.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法和加法运算构成了许多卷积操作，从而导致额外的计算。AdderNet (Chen et al., [2020b](#bib.bib22)) 尝试专门使用加法，通过使用
    L1 范数距离作为滤波器与特征图之间的响应标准。这种操作被称为绝对差异累积 (Um et al., [2021](#bib.bib202))，它加速了网络，并允许重用计算结果以减少能量消耗。
- en: 'You et al. (You et al., [2020](#bib.bib236)) introduce ShiftAddnet, focusing
    more on hardware efficiency. ShiftAddnet proposes a new metric for performance
    comparison, expressive capacity, which refers to the accuracy achieved by the
    model under similar hardware conditions. Experimental results show that shift-based
    networks (Wu et al., [2018](#bib.bib218); Jeon and Kim, [2018](#bib.bib101); Chen
    et al., [2019a](#bib.bib27); He et al., [2019b](#bib.bib83); Elhoushi et al.,
    [2021](#bib.bib49)) provide greater hardware efficiency but have a lower expressive
    capacity than multiplication-based networks. Conversely, the fully additive network (Chen
    et al., [2020b](#bib.bib22)) is inefficient since repeated additions are used
    to replace multiplications, although it can achieve better accuracy. Therefore,
    ShiftAddnet combines the benefits of bit-wise shifts (Elhoushi et al., [2021](#bib.bib49))
    and the efficiency of additive networks (Chen et al., [2020b](#bib.bib22)) to
    achieve state-of-the-art results on two IoT datasets: FlatCam (Tan et al., [2019b](#bib.bib190))
    and Head Pose (Viet et al., [2021](#bib.bib205)).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: You et al. (You et al., [2020](#bib.bib236)) 引入了 ShiftAddnet，更关注硬件效率。ShiftAddnet
    提出了一个新的性能比较指标，即表达能力，指的是模型在类似硬件条件下实现的准确性。实验结果表明，基于移位的网络 (Wu et al., [2018](#bib.bib218);
    Jeon and Kim, [2018](#bib.bib101); Chen et al., [2019a](#bib.bib27); He et al.,
    [2019b](#bib.bib83); Elhoushi et al., [2021](#bib.bib49)) 提供了更高的硬件效率，但其表达能力低于基于乘法的网络。相反，完全加法网络
    (Chen et al., [2020b](#bib.bib22)) 效率较低，因为使用重复的加法代替乘法，尽管它可以实现更好的准确性。因此，ShiftAddnet
    结合了位移 (Elhoushi et al., [2021](#bib.bib49)) 的优势和加法网络 (Chen et al., [2020b](#bib.bib22))
    的效率，在两个 IoT 数据集：FlatCam (Tan et al., [2019b](#bib.bib190)) 和 Head Pose (Viet et
    al., [2021](#bib.bib205)) 上取得了最先进的结果。
- en: 2.2.7\. EfficientNet Series
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.7\. EfficientNet 系列
- en: Almost all networks attempt to improve performance by adjusting depth, width,
    and resolution. To achieve the best performance and lightweight combination, it
    is crucial to pick the right combination. EfficientNet (Tan and Le, [2019a](#bib.bib192))
    proposes a simple grid search algorithm, compound scaling, to seek scaling factors
    (depth, width, and resolution), achieving accuracy with lower computation costs.
    EffectiveNetV2 (Tan and Le, [2021](#bib.bib193)) proposes a training-aware NAS
    to find a good trade-off for accuracy $A$, training speed $S$, and parameters
    $P$. It uses a search reward formulated as a simple weighted product, $A\cdot
    S^{w}\cdot P^{v}$, where $w=-0.07$ and $v=-0.05$ are empirically determined to
    balance the trade-off. To address the inefficiency of depthwise convolution, EfficientNetV2
    replaces stage 1-3 MBConv (Sandler et al., [2018](#bib.bib169)) with Fused-MBConv (Gupta
    and Akin, [2020](#bib.bib72)) in its architecture design, offering better performance
    and trade-off in terms of accuracy, parameters, and FLOPs. Besides, for a more
    robust network, EfficientNetV2 selects adaptive regulation during the training
    process because using identical regularization terms for images of different resolutions
    is inefficient.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有网络都尝试通过调整深度、宽度和分辨率来提高性能。要实现最佳性能和轻量级组合，选择正确的组合至关重要。EfficientNet (Tan 和 Le,
    [2019a](#bib.bib192)) 提出了一个简单的网格搜索算法，复合缩放，以寻找缩放因子（深度、宽度和分辨率），以较低的计算成本实现准确性。EffectiveNetV2
    (Tan 和 Le, [2021](#bib.bib193)) 提出了一个训练感知的 NAS 以找到准确性 $A$、训练速度 $S$ 和参数 $P$ 的良好权衡。它使用一个简单的加权乘积作为搜索奖励，$A\cdot
    S^{w}\cdot P^{v}$，其中 $w=-0.07$ 和 $v=-0.05$ 是经验确定的，用于平衡权衡。为了解决深度卷积的低效问题，EfficientNetV2
    在其架构设计中用 Fused-MBConv (Gupta 和 Akin, [2020](#bib.bib72)) 替代了阶段 1-3 的 MBConv (Sandler
    等人, [2018](#bib.bib169))，在准确性、参数和 FLOPs 方面提供了更好的性能和权衡。此外，为了获得更强健的网络，EfficientNetV2
    在训练过程中选择了自适应正则化，因为对不同分辨率的图像使用相同的正则化项是低效的。
- en: 2.2.8\. Discussion and Summary
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.8\. 讨论与总结
- en: 'Table [1](#S2.T1 "Table 1 ‣ 2.2.8\. Discussion and Summary ‣ 2.2\. Lightweight
    CNN Architecture ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey") compares the performance of
    lightweight CNN architectures on the ImageNet dataset. The horizontal lines separate
    the models of different series. From the table, we can observe that there is no
    one-size-fits-all architecture. Oftentimes, it is a trade-off between accuracy
    and efficiency. For example, AddressNet-20 maximizes efficiency at the expense
    of accuracy. Conversely, the most accurate variants of the EfficientNet series
    are among the least efficient ones. Drawing from this analysis, we provide recommendations
    on selecting the suitable models and hardware.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '表[1](#S2.T1 "Table 1 ‣ 2.2.8\. Discussion and Summary ‣ 2.2\. Lightweight CNN
    Architecture ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey") 比较了轻量级 CNN 架构在 ImageNet 数据集上的性能。水平线将不同系列的模型分开。从表中我们可以观察到，没有一种适用于所有情况的架构。通常，这是在准确性和效率之间的权衡。例如，AddressNet-20
    在牺牲准确性的情况下最大化效率。相反，EfficientNet 系列中最准确的变体通常是效率最低的。根据这一分析，我们提供了选择合适模型和硬件的建议。'
- en: How to choose an adequate lightweight model and compatible hardware? The first
    crucial step is to check lightweight models’ specifications and hardware compatibility.
    For example, depthwise separable convolutions have huge MAC and high RAM requirements.
    It is, therefore, imperative to employ a network on hardware that considers both
    RAM and storage capacity. To this end, Fan et al. (Fan et al., [2021](#bib.bib51))
    redesign the depthwise separable convolution and channel shuffle modules to be
    hardware-friendly on FPGA. Moreover, to minimize the inference time and to support
    deployment on a small target device, replacing multiplication with shift or add
    operations can effectively reduce the total parameters and MACs/FLOPs. Thus, ShiftNet
    or AdderNet series can be a good choice since they require smaller parameters
    and MACs. Within these two series, AddressNet-20 gives the best performance. For
    target devices with relatively more storage, such as mobile phones or GPUs, models
    with higher accuracy are preferred for a better user experience. EfficientNetV2-L
    can thus be considered since it achieves the highest Top-1 accuracy. However,
    it is important to note that the EfficientNet series costs a disproportionately
    higher number of parameters and MACs, which limits the application under low-end
    devices. Another way to achieve a better trade-off model is to apply fundamental
    compression methods such as pruning, quantization, and NAS (Chen et al., [2019b](#bib.bib30);
    Tan et al., [2019a](#bib.bib191)) (see Section 3) to adjust the architecture.
    This can be an efficient technique to reduce MACs/FLOPs, parameters, and inference
    time.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如何选择合适的轻量级模型和兼容的硬件？第一个关键步骤是检查轻量级模型的规格和硬件兼容性。例如，深度可分离卷积对 MAC 和 RAM 有较高要求。因此，必须在考虑
    RAM 和存储容量的硬件上使用网络。为此，Fan 等人 (Fan et al., [2021](#bib.bib51)) 重新设计了深度可分离卷积和通道洗牌模块，以便在
    FPGA 上对硬件友好。此外，为了最小化推理时间并支持在小型目标设备上部署，可以通过将乘法操作替换为移位或加法操作来有效减少总参数和 MACs/FLOPs。因此，ShiftNet
    或 AdderNet 系列可能是不错的选择，因为它们需要更小的参数和 MACs。在这两个系列中，AddressNet-20 的性能最佳。对于具有较多存储的目标设备，如手机或
    GPU，建议使用更高准确性的模型以获得更好的用户体验。因此，可以考虑 EfficientNetV2-L，因为它实现了最高的 Top-1 准确性。然而，值得注意的是，EfficientNet
    系列的参数和 MACs 的消耗不成比例地高，这限制了其在低端设备上的应用。另一种实现更好权衡模型的方法是应用基本的压缩方法，如剪枝、量化和 NAS (Chen
    et al., [2019b](#bib.bib30); Tan et al., [2019a](#bib.bib191)) (参见第 3 节) 来调整架构。这可以是减少
    MACs/FLOPs、参数和推理时间的有效技术。
- en: Some lightweight methods, such as SqueezeNet and ShuffeNet, may not be able
    to take full advantage of GPU-accelerated performance due to the lack of customized
    designs (Um et al., [2021](#bib.bib202)). Additionally, if pruning is applied
    to a network, like the CondenseNet series, the network structure might be irregular,
    preventing the target device from supporting it. In such a scenario, parallelism
    requires specifically designed computing hardware. Fortunately, customized hardware
    can be designed to fit a lightweight model. For instance, Um et al. (Um et al.,
    [2021](#bib.bib202)) note that CIM is incompatible with AdderNet because it cannot
    predict details of an absolute difference nor reuse the computation results. Therefore,
    they designed a novel ADA-CIM processor offering low-cost sign prediction and
    higher energy efficiency.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一些轻量级方法，如 SqueezeNet 和 ShuffleNet，由于缺乏定制化设计，可能无法充分发挥 GPU 加速性能 (Um et al., [2021](#bib.bib202))。此外，如果对网络应用剪枝，如
    CondenseNet 系列，网络结构可能会不规则，从而导致目标设备无法支持它。在这种情况下，平行计算需要专门设计的计算硬件。幸运的是，可以设计定制硬件以适应轻量级模型。例如，Um
    等人 (Um et al., [2021](#bib.bib202)) 指出 CIM 与 AdderNet 不兼容，因为它无法预测绝对差异的细节，也不能重用计算结果。因此，他们设计了一种新型
    ADA-CIM 处理器，提供低成本的符号预测和更高的能效。
- en: Table 1\. Comparison of Lightweight CNN Architectures on the ImageNet dataset.
    Note that we use bold to emphasize the models with the best accuracy, least parameters,
    and lowest MACs, with the respective values being also underlined for enhanced
    readability.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 轻量级 CNN 架构在 ImageNet 数据集上的比较。请注意，我们使用**粗体**来强调具有最佳准确性、最少参数和最低 MACs 的模型，并将相应的值也用*下划线*标出以增强可读性。
- en: '| Model | Top-1 | Top-5 | Params. (M) | MACs (G) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Top-1 | Top-5 | 参数 (M) | MACs (G) |'
- en: '| AlexNet (Krizhevsky et al., [2012](#bib.bib111)) | 57.1 | 80.3 | 60.9 | 0.725
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| AlexNet (Krizhevsky et al., [2012](#bib.bib111)) | 57.1 | 80.3 | 60.9 | 0.725
    |'
- en: '| ResNet-50 (He et al., [2016](#bib.bib78)) | 76.0 | 93.0 | 26.0 | 4.100 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-50 (He et al., [2016](#bib.bib78)) | 76.0 | 93.0 | 26.0 | 4.100 |'
- en: '| SqueezeNet (Iandola et al., [2017](#bib.bib99)) | 57.5 | 80.3 | 1.2 | 0.837
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| SqueezeNet (Iandola et al., [2017](#bib.bib99)) | 57.5 | 80.3 | 1.2 | 0.837
    |'
- en: '| SqueezeNext (Gholami et al., [2018](#bib.bib60)) | 59.1 | 82.6 | 0.7 | 0.282
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| SqueezeNext (Gholami et al., [2018](#bib.bib60)) | 59.1 | 82.6 | 0.7 | 0.282
    |'
- en: '| ShuffleNetV1-1.5 (Zhang et al., [2018c](#bib.bib249)) | 71.5 | - | 3.4 |
    0.292 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| ShuffleNetV1-1.5 (Zhang et al., [2018c](#bib.bib249)) | 71.5 | - | 3.4 |
    0.292 |'
- en: '| ShuffleNetV2-1.5 (Ma et al., [2018](#bib.bib139)) | 72.6 | 90.6 | 3.5 | 0.299
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| ShuffleNetV2-1.5 (Ma et al., [2018](#bib.bib139)) | 72.6 | 90.6 | 3.5 | 0.299
    |'
- en: '| 1.0-MobileNetV1 (Howard et al., [2017](#bib.bib90)) | 70.6 | - | 4.2 | 0.569
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 1.0-MobileNetV1 (Howard et al., [2017](#bib.bib90)) | 70.6 | - | 4.2 | 0.569
    |'
- en: '| MobileNetV2-1.4 (Sandler et al., [2018](#bib.bib169)) | 74.7 | - | 6.9 |
    0.585 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| MobileNetV2-1.4 (Sandler et al., [2018](#bib.bib169)) | 74.7 | - | 6.9 |
    0.585 |'
- en: '| MobileV3-S (Howard et al., [2019](#bib.bib89)) | 67.4 | - | 2.5 | 0.056 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| MobileV3-S (Howard et al., [2019](#bib.bib89)) | 67.4 | - | 2.5 | 0.056 |'
- en: '| MobileV3-L (Howard et al., [2019](#bib.bib89)) | 75.2 | - | 5.4 | 0.219 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| MobileV3-L (Howard et al., [2019](#bib.bib89)) | 75.2 | - | 5.4 | 0.219 |'
- en: '| MobileNeXt-1.0 (Zhou et al., [2020](#bib.bib254)) | 74.0 | - | 3.4 | 0.300
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| MobileNeXt-1.0 (Zhou et al., [2020](#bib.bib254)) | 74.0 | - | 3.4 | 0.300
    |'
- en: '| ShiftResNet-20 (Wu et al., [2018](#bib.bib218)) | 68.6 | - | 0.2 | 0.046
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ShiftResNet-20 (Wu et al., [2018](#bib.bib218)) | 68.6 | - | 0.2 | 0.046
    |'
- en: '| ShiftResNet-56 (Wu et al., [2018](#bib.bib218)) | 72.1 | - | 0.6 | 0.102
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| ShiftResNet-56 (Wu et al., [2018](#bib.bib218)) | 72.1 | - | 0.6 | 0.102
    |'
- en: '| ShiftNet-A (Wu et al., [2018](#bib.bib218)) | 70.1 | 89.7 | 4.1 | 1.400 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ShiftNet-A (Wu et al., [2018](#bib.bib218)) | 70.1 | 89.7 | 4.1 | 1.400 |'
- en: '| ShiftNet-B (Wu et al., [2018](#bib.bib218)) | 61.2 | 83.6 | 1.1 | 0.371 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ShiftNet-B (Wu et al., [2018](#bib.bib218)) | 61.2 | 83.6 | 1.1 | 0.371 |'
- en: '| FE-Net-1.0 (Chen et al., [2019a](#bib.bib27)) | 72.9 | - | 3.7 | 0.301 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| FE-Net-1.0 (Chen et al., [2019a](#bib.bib27)) | 72.9 | - | 3.7 | 0.301 |'
- en: '| FE-Net-1.37 (Chen et al., [2019a](#bib.bib27)) | 75.0 | - | 5.9 | 0.563 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| FE-Net-1.37 (Chen et al., [2019a](#bib.bib27)) | 75.0 | - | 5.9 | 0.563 |'
- en: '| AddressNet-20 (He et al., [2019b](#bib.bib83)) | 68.7 | - | 0.1 | 0.022 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| AddressNet-20 (He et al., [2019b](#bib.bib83)) | 68.7 | - | 0.1 | 0.022 |'
- en: '| AddressNet-44 (He et al., [2019b](#bib.bib83)) | 73.3 | - | 0.2 | 0.053 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| AddressNet-44 (He et al., [2019b](#bib.bib83)) | 73.3 | - | 0.2 | 0.053 |'
- en: '| AdderNet-Resnet18 (Chen et al., [2020b](#bib.bib22)) | 67.0 | 87.6 | 3.6
    | - |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| AdderNet-Resnet18 (Chen et al., [2020b](#bib.bib22)) | 67.0 | 87.6 | 3.6
    | - |'
- en: '| AdderNet-Resnet50 (Chen et al., [2020b](#bib.bib22)) | 74.9 | 91.7 | 7.7
    | - |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| AdderNet-Resnet50 (Chen et al., [2020b](#bib.bib22)) | 74.9 | 91.7 | 7.7
    | - |'
- en: '| DenseNet-169 (Huang et al., [2017](#bib.bib95)) | 76.2 | 93.2 | 14.0 | 3.500
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| DenseNet-169 (Huang et al., [2017](#bib.bib95)) | 76.2 | 93.2 | 14.0 | 3.500
    |'
- en: '| DenseNet-264 (Huang et al., [2017](#bib.bib95)) | 77.9 | 93.9 | 34.0 | 6.000
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| DenseNet-264 (Huang et al., [2017](#bib.bib95)) | 77.9 | 93.9 | 34.0 | 6.000
    |'
- en: '| CondenseNet (Huang et al., [2018](#bib.bib94)) | 71.0 | 90.0 | 2.9 | 0.274
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| CondenseNet (Huang et al., [2018](#bib.bib94)) | 71.0 | 90.0 | 2.9 | 0.274
    |'
- en: '| CondenseV2-A (Yang et al., [2021a](#bib.bib228)) | 64.4 | 84.5 | 2.0 | 0.046
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| CondenseV2-A (Yang et al., [2021a](#bib.bib228)) | 64.4 | 84.5 | 2.0 | 0.046
    |'
- en: '| CondenseV2-B (Yang et al., [2021a](#bib.bib228)) | 71.9 | 90.3 | 3.6 | 0.146
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| CondenseV2-B (Yang et al., [2021a](#bib.bib228)) | 71.9 | 90.3 | 3.6 | 0.146
    |'
- en: '| EfficientNet-B1 (Tan and Le, [2019a](#bib.bib192)) | 79.2 | 94.5 | 7.8 |
    0.700 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNet-B1 (Tan and Le, [2019a](#bib.bib192)) | 79.2 | 94.5 | 7.8 |
    0.700 |'
- en: '| EfficientNet-B7 (Tan and Le, [2019a](#bib.bib192)) | 84.4 | 97.1 | 66.0 |
    37.000 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNet-B7 (Tan and Le, [2019a](#bib.bib192)) | 84.4 | 97.1 | 66.0 |
    37.000 |'
- en: '| EfficientNet-X-B7 (Li et al., [2021](#bib.bib119)) | 84.7 | - | 73.0 | 91.000
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNet-X-B7 (Li et al., [2021](#bib.bib119)) | 84.7 | - | 73.0 | 91.000
    |'
- en: '| EfficientNetV2-S (Tan and Le, [2021](#bib.bib193)) | 83.9 | - | 24.0 | 8.800
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNetV2-S (Tan and Le, [2021](#bib.bib193)) | 83.9 | - | 24.0 | 8.800
    |'
- en: '| EfficientNetV2-M (Tan and Le, [2021](#bib.bib193)) | 85.1 | - | 55.0 | 24.000
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNetV2-M (Tan and Le, [2021](#bib.bib193)) | 85.1 | - | 55.0 | 24.000
    |'
- en: '| EfficientNetV2-L (Tan and Le, [2021](#bib.bib193)) | 85.7 | - | 121.0 | 53.000
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNetV2-L (Tan and Le, [2021](#bib.bib193)) | 85.7 | - | 121.0 | 53.000
    |'
- en: 2.3\. Transformer-based Series
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. Transformer-based Series
- en: 'Transformer models are widely used in NLP (Vaswani et al., [2017](#bib.bib204))
    and have recently obtained promising results in computer vision tasks (Liu et al.,
    [2021a](#bib.bib135), [2022](#bib.bib134); Zhang et al., [2023a](#bib.bib245)).
    Fig. [3](#S2.F3 "Figure 3 ‣ 2.3\. Transformer-based Series ‣ 2\. Lightweight Architecture
    Design ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey")
    shows the architecture of a typical vision transformer. Transformers are notable
    for having a significant drawback in that they require a large number of parameters
    and a high MAC to maintain their performance, which results in a significant amount
    of time needed for both the training and inference phases, particularly when the
    input sequence is long. Additionally, the computation and network structures inside
    transformers are more complex than those of CNNs. The huge number of FLOPs and
    parameters make practical inference and hardware deployment more difficult. To
    bridge the gap between transformers and real-world applications, efficient transformers
    will be discussed in the following sub-sections.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '变换器模型在自然语言处理（Vaswani 等，[2017](#bib.bib204)）中被广泛应用，并且最近在计算机视觉任务中获得了令人满意的结果（Liu
    等，[2021a](#bib.bib135)，[2022](#bib.bib134)；Zhang 等，[2023a](#bib.bib245)）。图 [3](#S2.F3
    "Figure 3 ‣ 2.3\. Transformer-based Series ‣ 2\. Lightweight Architecture Design
    ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey")
    显示了典型视觉变换器的架构。变换器因需要大量的参数和高 MAC 以维持其性能而显著地存在一个缺陷，这导致训练和推理阶段需要大量时间，特别是当输入序列较长时。此外，变换器内部的计算和网络结构比
    CNN 更加复杂。大量的 FLOPs 和参数使得实际推理和硬件部署变得更加困难。为了弥合变换器与实际应用之间的差距，接下来的子节将讨论高效的变换器。'
- en: '![Refer to caption](img/e2ed4cb49e2680de867f8cd4b7a959ba.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e2ed4cb49e2680de867f8cd4b7a959ba.png)'
- en: Figure 3\. Standard Vision Transformer, where $P=h\times w$, $h,w$ represents
    the height and the width of the images. $N$ is the number of image patches, $L$
    is the number of transformer blocks, and $d$ is the dimension (Mehta and Rastegari,
    [2022](#bib.bib146)).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 标准视觉变换器，其中 $P=h\times w$，$h$ 和 $w$ 分别表示图像的高度和宽度。$N$ 是图像块的数量，$L$ 是变换器块的数量，$d$
    是维度（Mehta 和 Rastegari，[2022](#bib.bib146)）。
- en: 2.3.1\. Lite attention module
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1\. 轻量级注意力模块
- en: 'To address the heavy MAC and huge computation requirements in the self-attention
    layers, Long-Short Range Attention (LSRA) (Wu et al., [2020](#bib.bib221)) was
    proposed to extract the global and local features separately, alleviating the
    attention computations in the feed-forward network (FFN). Child et al. (Child
    et al., [2019](#bib.bib32)) effectively exploit stride and fixed operations to
    form a sparse connectivity matrix. Linformer (Wang et al., [2020a](#bib.bib210))
    decomposes self-attention into several low-rank matrices using linear projection,
    reducing the complexity of self-attention from $N^{2}$ to $N$, where $N$ denotes
    the sequence length. Choromanski et al. (Choromanski et al., [2021](#bib.bib37))
    proposes a linear self-attention mechanism based on the FAVOR+ (fast attention
    with positive orthogonal random features) approach to construct an approximate
    softmax operation. FAVOR+ enables unbiased estimation of self-attention with low
    estimation variance, reducing spatial and temporal complexity. Reformer (Kitaev
    et al., [2020](#bib.bib109)) utilizes locality-sensitive hashing to replace dot
    product operations in attention. It directly decreases the computation requirements
    from $N^{2}$ to $Nlog(N)$, allowing longer sequence inputs to be considered. In
    addition, Reformer employs a reverse residual layer (Gomez et al., [2017](#bib.bib61))
    to save GPU memory by $L$ times (number of layers). Unlike traditional residuals,
    a reverse residual layer does not require activation data to be stored in each
    layer. The complexity of these efficient transformers is depicted in Table [2](#S2.T2
    "Table 2 ‣ 2.3.1\. Lite attention module ‣ 2.3\. Transformer-based Series ‣ 2\.
    Lightweight Architecture Design ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey").'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '为了应对自注意力层中的大量 MAC 和巨大的计算需求，Long-Short Range Attention (LSRA) (Wu et al., [2020](#bib.bib221))
    被提出，用于分别提取全局和局部特征，从而减轻前馈网络 (FFN) 中的注意力计算。Child et al. (Child et al., [2019](#bib.bib32))
    有效地利用步幅和固定操作来形成稀疏连接矩阵。Linformer (Wang et al., [2020a](#bib.bib210)) 通过线性投影将自注意力分解为几个低秩矩阵，将自注意力的复杂度从
    $N^{2}$ 降低到 $N$，其中 $N$ 表示序列长度。Choromanski et al. (Choromanski et al., [2021](#bib.bib37))
    提出了基于 FAVOR+ (fast attention with positive orthogonal random features) 方法的线性自注意力机制，以构建近似的
    softmax 操作。FAVOR+ 能够以低估计方差对自注意力进行无偏估计，从而减少空间和时间复杂度。Reformer (Kitaev et al., [2020](#bib.bib109))
    利用局部敏感哈希替代注意力中的点积操作，将计算需求直接从 $N^{2}$ 降低到 $Nlog(N)$，允许考虑更长的序列输入。此外，Reformer 采用了一种反向残差层
    (Gomez et al., [2017](#bib.bib61))，通过 $L$ 倍 (层数) 节省 GPU 内存。与传统的残差不同，反向残差层不需要在每一层存储激活数据。这些高效变压器的复杂性如表
    [2](#S2.T2 "Table 2 ‣ 2.3.1\. Lite attention module ‣ 2.3\. Transformer-based
    Series ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") 所示。'
- en: 'In addition, transformers stack many FFNs to obtain better-integrated features.
    Specifically, an FFN is a series of linear transformations that require a lot
    of calculations due to its dense connections. To tackle this issue, Mehta et al. (Mehta
    et al., [2018](#bib.bib144)) introduce grouped linear transformations (GLTs),
    which incorporate the concept of group convolution to make the transformer block
    more lightweight. Facing the same shortcoming from the group convolutions (as
    presented in Section [2.1.3](#S2.SS1.SSS3 "2.1.3\. Group convolution ‣ 2.1\. Prior
    Knowledge of Lightweight Architecture ‣ 2\. Lightweight Architecture Design ‣
    Lightweight Deep Learning for Resource-Constrained Environments: A Survey")),
    the hierarchical group transformation (HGT) (Mehta et al., [2020](#bib.bib145))
    aims to enhance the information flow between groups using a split layer and a
    skip connection operation. DeLighT (Mehta et al., [2021](#bib.bib143)) exploits
    GLTs to make feature dimensions wider and deeper, making it possible to use single-head
    attention instead of multi-head attention. This technique decreases the computation
    cost in the attention operation from $d_{m}N^{2}$ to $d_{o}N^{2}$, where $d_{m}$,
    and $d_{o}$ are the input dimension and output dimension respectively.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，变换器堆叠了许多前馈网络（FFNs）以获得更好的特征集成。具体来说，FFN是一系列线性变换，由于其密集的连接需要大量的计算。为了解决这个问题，Mehta
    等人（Mehta et al., [2018](#bib.bib144)）引入了分组线性变换（GLTs），它结合了分组卷积的概念，使得变换器块更加轻量化。面对来自分组卷积的相同缺点（如第
    [2.1.3](#S2.SS1.SSS3 "2.1.3\. Group convolution ‣ 2.1\. Prior Knowledge of Lightweight
    Architecture ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey") 节所述），层次分组变换（HGT）（Mehta et al.,
    [2020](#bib.bib145)）旨在使用拆分层和跳跃连接操作来增强组之间的信息流。DeLighT（Mehta et al., [2021](#bib.bib143)）利用GLTs来拓宽和加深特征维度，从而使得可以使用单头注意力而不是多头注意力。这项技术将注意力操作中的计算成本从
    $d_{m}N^{2}$ 降低到 $d_{o}N^{2}$，其中 $d_{m}$ 和 $d_{o}$ 分别是输入维度和输出维度。'
- en: Table 2\. The complexity of Efficient Transformers (Wang et al., [2020a](#bib.bib210)).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. Efficient Transformers 的复杂度（Wang et al., [2020a](#bib.bib210)）。
- en: '| Model | Complexity per Layer | Sequential Operation |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 每层复杂度 | 顺序操作 |'
- en: '| --- | --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Transformer (Vaswani et al., [2017](#bib.bib204)) | $O(N\textsuperscript{2})$
    | $O(N)$ |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Transformer（Vaswani et al., [2017](#bib.bib204)） | $O(N\textsuperscript{2})$
    | $O(N)$ |'
- en: '| Sparse Transformer (Child et al., [2019](#bib.bib32)) | $O(N\sqrt{N})$ |
    $O(1)$ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Sparse Transformer（Child et al., [2019](#bib.bib32)） | $O(N\sqrt{N})$ | $O(1)$
    |'
- en: '| Linformer (Wang et al., [2020a](#bib.bib210)) | $O(N)$ | $O(1)$ |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Linformer（Wang et al., [2020a](#bib.bib210)） | $O(N)$ | $O(1)$ |'
- en: '| Reformer (Kitaev et al., [2020](#bib.bib109)) | $O(Nlog(N))$ | $O(log(N))$
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Reformer（Kitaev et al., [2020](#bib.bib109)） | $O(Nlog(N))$ | $O(log(N))$
    |'
- en: 2.3.2\. Token sparsing
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2\. 令牌稀疏
- en: Vision transformer (ViT) (Dosovitskiy et al., [2021](#bib.bib45)) is the earliest
    work that applied transformers to solve an image classification task. It first
    splits an image into several patches and flattens them so that it can be passed
    in as an embedding sequence input to the transformer architecture. As the resolution
    of images in ImageNet is 224x224, their tokens require significantly more computation
    compared to other datasets with smaller resolutions, such as CIFAR-10 and CIFAR-100
    (32x32).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Vision transformer（ViT）（Dosovitskiy et al., [2021](#bib.bib45)）是最早将变换器应用于解决图像分类任务的工作。它首先将图像分割成若干补丁并将其展平，然后作为嵌入序列输入变换器架构。由于ImageNet中图像的分辨率为224x224，它们的令牌相比于其他分辨率较小的数据集（如CIFAR-10和CIFAR-100（32x32））需要显著更多的计算。
- en: To address this, T2T-ViT (Yuan et al., [2021](#bib.bib239)) observes that image
    splitting in transformers causes a loss of local relationships between tokens
    since there is no overlap between the tokens. Hence, they employ soft unfolding
    to combine the surrounding spatial tokens into high-dimensional manifolds, enabling
    smaller MLP sizes and increasing memory efficiency.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，T2T-ViT（Yuan et al., [2021](#bib.bib239)）观察到，变换器中的图像分割导致令牌之间局部关系的丧失，因为令牌之间没有重叠。因此，他们采用软展开技术将周围的空间令牌组合成高维流形，从而实现较小的MLP大小并提高内存效率。
- en: An extensive study on transformers (Naseer et al., [2021](#bib.bib149)) demonstrates
    that transformers are robust to patch drops, with only a slight decrease in accuracy
    when patches suffer from distortion or occlusions. DynamicViT (Rao et al., [2021](#bib.bib160))
    integrates a prediction module between transformer blocks to mask the less significant
    tokens. The prediction module is a binary decision mask in the range (0,1) that
    measures the importance of tokens. EViT (Liang et al., [2021a](#bib.bib123)) computes
    attentiveness scores from class tokens and other tokens and keeps top-K tokens,
    representing the highest positive correlation to the prediction. A-ViT (Yin et al.,
    [2022](#bib.bib234)) adaptively changes the number of tokens at different depths
    based on the complexity of the input image to reduce the inference time in ViT.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对transformers的广泛研究（Naseer et al., [2021](#bib.bib149)）表明，transformers对补丁丢失具有鲁棒性，当补丁遭受失真或遮挡时，准确性仅有轻微下降。DynamicViT（Rao
    et al., [2021](#bib.bib160)）在transformer块之间集成了一个预测模块，以屏蔽不太重要的标记。预测模块是一个范围在（0,1）之间的二元决策掩码，用于衡量标记的重要性。EViT（Liang
    et al., [2021a](#bib.bib123)）从类别标记和其他标记中计算注意力分数，并保留前K个标记，表示与预测的正相关性最高。A-ViT（Yin
    et al., [2022](#bib.bib234)）根据输入图像的复杂性自适应地改变不同深度的标记数量，以减少ViT的推理时间。
- en: 2.3.3\. Lightweight hybrid models
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 轻量级混合模型
- en: Due to the long-range dependence property inherent in attention mechanisms,
    transformer networks outperform CNN in accuracy. However, a transformer network
    lacks strong inductive biases (Dai et al., [2021a](#bib.bib39); Graham et al.,
    [2021](#bib.bib64); Liu et al., [2021a](#bib.bib135)), making it difficult to
    train and requires extra data augmentation and heavy regularization to maintain
    performance (Touvron et al., [2021](#bib.bib199)). On the other hand, CNN extracts
    features based on sliding windows, resulting in stronger inductive biases, which
    make models easier to train and have better generalizability. Interestingly, the
    aggregation of CNN and transformer networks (Wu et al., [2021](#bib.bib219); Srinivas
    et al., [2021](#bib.bib177); d’Ascoli et al., [2021](#bib.bib48); Xiao et al.,
    [2021](#bib.bib222)) produces versatile and powerful models. Since the hybrid
    models would have many parameters, DeiT (Touvron et al., [2021](#bib.bib199))
    applies KD and achieves better accuracy with less latency than CNN under comparable
    parameters. To improve data efficiency and simplify model complexity, the student
    model, a ViT model, added a distillation token to provide insight into the inductive
    biases of a CNN-based teacher model. MobileViT (Mehta and Rastegari, [2022](#bib.bib146))
    points out that transformer-based networks perform worse than CNN networks under
    similar parameters because they are still bulky. MobileViT employs MobileNetV2 (Sandler
    et al., [2018](#bib.bib169)) as the CNN backbone to obtain inductive biases and
    replaces the MBconv block in MobileNetV2 with a MobileViT block with unfolding
    and folding operations, which can compute long-range dependencies like a transformer.
    Similarly, MobileFormer (Chen et al., [2022a](#bib.bib28)) devises a parallel
    structure consisting of CNNs and transformers to achieve feature fusion. Inductive
    bias and the ability to capture global features are incorporated via two-way cross-attention.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于注意力机制固有的长程依赖特性，transformer网络在准确性上优于CNN。然而，transformer网络缺乏强大的归纳偏置（Dai et al.,
    [2021a](#bib.bib39); Graham et al., [2021](#bib.bib64); Liu et al., [2021a](#bib.bib135)），这使得训练变得困难，需要额外的数据增强和严格的正则化来保持性能（Touvron
    et al., [2021](#bib.bib199)）。另一方面，CNN基于滑动窗口提取特征，产生更强的归纳偏置，使得模型更易训练且具有更好的泛化能力。有趣的是，CNN和transformer网络的结合（Wu
    et al., [2021](#bib.bib219); Srinivas et al., [2021](#bib.bib177); d’Ascoli et
    al., [2021](#bib.bib48); Xiao et al., [2021](#bib.bib222)）产生了多功能且强大的模型。由于混合模型具有许多参数，DeiT（Touvron
    et al., [2021](#bib.bib199)）应用了KD，并在参数相当的情况下实现了比CNN更高的准确性和更低的延迟。为了提高数据效率和简化模型复杂度，学生模型ViT模型添加了一个蒸馏标记，以洞察基于CNN的教师模型的归纳偏置。MobileViT（Mehta
    and Rastegari, [2022](#bib.bib146)）指出，由于仍然笨重，transformer-based网络在类似参数下表现不如CNN网络。MobileViT采用MobileNetV2（Sandler
    et al., [2018](#bib.bib169)）作为CNN骨干网，以获得归纳偏置，并将MobileNetV2中的MBconv块替换为具有展开和折叠操作的MobileViT块，这可以像transformer一样计算长程依赖性。类似地，MobileFormer（Chen
    et al., [2022a](#bib.bib28)）设计了一个由CNN和transformer组成的并行结构来实现特征融合。归纳偏置和捕捉全局特征的能力通过双向交叉注意力来实现。
- en: 2.3.4\. Discussion and Summary
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4 讨论与总结
- en: 'Recent transformer models focus on lighter and more powerful architectures.
    This observation is apparent from Table [3](#S2.T3 "Table 3 ‣ 2.3.4\. Discussion
    and Summary ‣ 2.3\. Transformer-based Series ‣ 2\. Lightweight Architecture Design
    ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey"),
    where many recent transformers, such as T2T-ViT (Yuan et al., [2021](#bib.bib239))
    and DymViT-LVit (Rao et al., [2021](#bib.bib160)), are shown to achieve higher
    accuracy with significantly fewer parameters and lower FLOPS than the original
    ViT and ResNet-based CNNs. Specifically, we split the discussion into 3 sub-sections
    with bold headings.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '近期的 transformer 模型关注于更轻量且更强大的架构。从表 [3](#S2.T3 "Table 3 ‣ 2.3.4\. Discussion
    and Summary ‣ 2.3\. Transformer-based Series ‣ 2\. Lightweight Architecture Design
    ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey")
    可以明显看出，许多近期的 transformer，如 T2T-ViT（Yuan 等，[2021](#bib.bib239)）和 DymViT-LVit（Rao
    等，[2021](#bib.bib160)），在准确率上表现更高，参数更少，FLOPS 更低，相较于原始的 ViT 和基于 ResNet 的 CNN。具体而言，我们将讨论分为
    3 个小节，并用**粗体**标题。'
- en: VIT & KD transformer. Inspired by (Hinton et al., [2015](#bib.bib86)), several
    papers (Touvron et al., [2021](#bib.bib199); Lin et al., [2022a](#bib.bib126);
    Chen et al., [2021a](#bib.bib23)) apply KD to distill the inductive bias from
    the CNN-based teacher models to the transformer-based student models. For example,
    the design of DeiT-B (Touvron et al., [2021](#bib.bib199)) architecture integrates
    a CNN-based teacher model, a RegNetY-16G (Radosavovic et al., [2020](#bib.bib159)),
    and a transformer-based student model, ViT-B (Dosovitskiy et al., [2021](#bib.bib45)).
    Results show that DeiT-B outperforms all the models in terms of Top-1 accuracy,
    achieving an accuracy of 84.5%. Despite their stronger abilities, transformer-based
    student models require a large network to maintain their performance since they
    are harder to converge than CNN models (Dai et al., [2021a](#bib.bib39)).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: VIT & KD transformer。受到（Hinton 等，[2015](#bib.bib86)）的启发，几篇论文（Touvron 等，[2021](#bib.bib199)；Lin
    等，[2022a](#bib.bib126)；Chen 等，[2021a](#bib.bib23)）将 KD 应用于将基于 CNN 的教师模型的归纳偏差蒸馏到基于
    transformer 的学生模型中。例如，DeiT-B（Touvron 等，[2021](#bib.bib199)）架构的设计整合了一个基于 CNN 的教师模型，一个
    RegNetY-16G（Radosavovic 等，[2020](#bib.bib159)）和一个基于 transformer 的学生模型，ViT-B（Dosovitskiy
    等，[2021](#bib.bib45)）。结果显示，DeiT-B 在 Top-1 准确率方面优于所有模型，达到了 84.5% 的准确率。尽管它们具有更强的能力，但基于
    transformer 的学生模型需要较大的网络来保持其性能，因为它们比 CNN 模型更难以收敛（Dai 等，[2021a](#bib.bib39)）。
- en: 'VIT & CNN hybrid transformer. To overcome the shortcomings of the KD-based
    transformer models, the hybrid models (Dai et al., [2021a](#bib.bib39); Mehta
    and Rastegari, [2022](#bib.bib146); Chen et al., [2022a](#bib.bib28)) utilize
    both the convolution and transformer layers in the network. By doing so, they
    can obtain stronger inductive bias, leading to better convergence during training.
    Thus, hybrid models typically have fewer FLOPs and parameters. For example, Mobile-Former-96M (Chen
    et al., [2022a](#bib.bib28)) achieves the lowest FLOPs of 0.096G while MobileViT-XS (Mehta
    and Rastegari, [2022](#bib.bib146)) has the lowest parameters, which is 2.3 M.
    These hybrid models are extremely lightweight but sometimes, efficiency is achieved
    at the expense of accuracy, as we can observe from their performance in Table
    [2](#S2.T2 "Table 2 ‣ 2.3.1\. Lite attention module ‣ 2.3\. Transformer-based
    Series ‣ 2\. Lightweight Architecture Design ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey"). For instance, MobileVit-XS has roughly half the total
    parameters of MobileViT-S, its counterpart, but its accuracy has significantly
    dropped by 3.6%. Another noteworthy observation shows that although Mobile-Former-96M
    achieves the lowest FLOPS, its parameter size was doubled, and accuracy is 2.0%
    lower compared to MobileVit-XS. This demonstrates that there is not always a correlation
    between FLOPs and total parameters and that lowering FLOPs appears to have a greater
    impact on accuracy than lowering parameters.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 'VIT 与 CNN 混合 Transformer。为了克服基于 KD 的 Transformer 模型的不足，混合模型（Dai 等， [2021a](#bib.bib39)；Mehta
    和 Rastegari， [2022](#bib.bib146)；Chen 等， [2022a](#bib.bib28)）在网络中同时使用卷积层和 Transformer
    层。通过这样做，它们能够获得更强的归纳偏置，从而在训练过程中实现更好的收敛。因此，混合模型通常具有较少的 FLOPs 和参数。例如，Mobile-Former-96M（Chen
    等， [2022a](#bib.bib28)）实现了 0.096G 的最低 FLOPs，而 MobileViT-XS（Mehta 和 Rastegari，
    [2022](#bib.bib146)）则拥有最低的参数，即 2.3 M。这些混合模型极为轻量，但有时，效率的提升是以准确性为代价的，从表 [2](#S2.T2
    "Table 2 ‣ 2.3.1\. Lite attention module ‣ 2.3\. Transformer-based Series ‣ 2\.
    Lightweight Architecture Design ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") 的性能可以看出。例如，MobileVit-XS 的总参数大约是其对等模型 MobileViT-S 的一半，但其准确率下降了
    3.6%。另一个值得注意的观察是，尽管 Mobile-Former-96M 实现了最低的 FLOPS，但其参数大小翻了一倍，准确率比 MobileVit-XS
    低 2.0%。这表明 FLOPs 和总参数之间并不总是存在相关性，降低 FLOPs 对准确率的影响似乎大于降低参数。'
- en: VIT & Token sparsing transformer. Another series of efficient transformers (Yuan
    et al., [2021](#bib.bib239); Rao et al., [2021](#bib.bib160); Naseer et al., [2021](#bib.bib149);
    Liang et al., [2021a](#bib.bib123); Yin et al., [2022](#bib.bib234)) aim to prune
    the transformer structure efficiently via token sparsing. From the results, token
    sparsing-based models achieve a competitive accuracy with fewer parameters and
    FLOPs. It is worth noting that EViT-DeiT-S (k=0.7) (Liang et al., [2021a](#bib.bib123))
    can reach the highest throughput, 5408 images per second. Therefore, for a faster
    transformer-based model, such as accomplishing a real-time system, aggregating
    tokens into smaller amounts may provide a promising solution.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: VIT 与 Token 稀疏 Transformer。另一系列高效的 Transformer（Yuan 等， [2021](#bib.bib239)；Rao
    等， [2021](#bib.bib160)；Naseer 等， [2021](#bib.bib149)；Liang 等， [2021a](#bib.bib123)；Yin
    等， [2022](#bib.bib234)）旨在通过 Token 稀疏有效地修剪 Transformer 结构。从结果来看，基于 Token 稀疏的模型在参数和
    FLOPs 更少的情况下实现了具有竞争力的准确性。值得注意的是，EViT-DeiT-S (k=0.7)（Liang 等， [2021a](#bib.bib123)）可以达到最高的吞吐量，每秒
    5408 张图像。因此，对于更快的 Transformer 模型，例如实现实时系统，将 Token 聚合成更小的数量可能提供了一个有前景的解决方案。
- en: Due to their competitive accuracy and lightweight design (Kang et al., [2023](#bib.bib106);
    Luo et al., [2022](#bib.bib137)), lightweight transformer models are gaining popularity
    in a wide range of applications, such as edge AI and mobile AI; more details of
    efficient transformers can be found in (Han et al., [2023](#bib.bib75); Tay et al.,
    [2021](#bib.bib197)).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其竞争力的准确性和轻量设计（Kang 等， [2023](#bib.bib106)；Luo 等， [2022](#bib.bib137)），轻量级
    Transformer 模型在边缘 AI 和移动 AI 等广泛应用中越来越受欢迎；有关高效 Transformer 的更多细节可以参考（Han 等， [2023](#bib.bib75)；Tay
    等， [2021](#bib.bib197)）。
- en: Table 3\. Comparison of Lightweight Transformer Models on the ImageNet dataset.
    We use bold to emphasize the models with the least parameters, highest throughput,
    lowest FLOPs, and best accuracy, with the corresponding values also underlined
    for enhanced readability.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 对比轻量级 Transformer 模型在 ImageNet 数据集上的表现。我们使用**粗体**来强调参数最少、吞吐量最高、FLOPs 最低和准确率最好的模型，相应的数值也进行了*下划线*标注，以提高可读性。
- en: '| Categories | Model |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 模型 |'
- en: '&#124; Image &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; (Size) &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （大小） &#124;'
- en: '| Params. (M) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 参数（M） |'
- en: '&#124; Throughput &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Throughput &#124;'
- en: '&#124; (image/s) &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (image/s) &#124;'
- en: '| FLOPs(G) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| FLOPs(G) |'
- en: '&#124; ImageNet &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ImageNet &#124;'
- en: '&#124; Top-1 &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Top-1 &#124;'
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CNN | ResNet50 (He et al., [2016](#bib.bib78)) | 224$\times$224 | 25.5 |
    - | 4.13 | 76.2 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| CNN | ResNet50 (He et al., [2016](#bib.bib78)) | 224$\times$224 | 25.5 |
    - | 4.13 | 76.2 |'
- en: '| ResNet101 (He et al., [2016](#bib.bib78)) | 224$\times$224 | 44.6 | - | 7.9
    | 77.4 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| ResNet101 (He et al., [2016](#bib.bib78)) | 224$\times$224 | 44.6 | - | 7.9
    | 77.4 |'
- en: '| ResNet152 (He et al., [2016](#bib.bib78)) | 224$\times$224 | 60.2 | - | 11.0
    | 78.3 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| ResNet152 (He et al., [2016](#bib.bib78)) | 224$\times$224 | 60.2 | - | 11.0
    | 78.3 |'
- en: '| RegNetY-16GF (Radosavovic et al., [2020](#bib.bib159)) | 224$\times$224 |
    84.0 | 334.7 | - | 82.9 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| RegNetY-16GF (Radosavovic et al., [2020](#bib.bib159)) | 224$\times$224 |
    84.0 | 334.7 | - | 82.9 |'
- en: '| ViT | ViT-B/16 (Dosovitskiy et al., [2021](#bib.bib45)) | 384$\times$384
    | 86.6 | 85.9 | 17.6 | 77.9 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| ViT | ViT-B/16 (Dosovitskiy et al., [2021](#bib.bib45)) | 384$\times$384
    | 86.6 | 85.9 | 17.6 | 77.9 |'
- en: '| ViT-L/16 (Dosovitskiy et al., [2021](#bib.bib45)) | 384$\times$384 | 307.0
    | 27.3 | 63.6 | 76.5 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| ViT-L/16 (Dosovitskiy et al., [2021](#bib.bib45)) | 384$\times$384 | 307.0
    | 27.3 | 63.6 | 76.5 |'
- en: '| ViT & KD | DeiT-Ti (Touvron et al., [2021](#bib.bib199)) | 224$\times$224
    | 5.0 | 2536.5 | - | 72.2 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| ViT & KD | DeiT-Ti (Touvron et al., [2021](#bib.bib199)) | 224$\times$224
    | 5.0 | 2536.5 | - | 72.2 |'
- en: '| DeiT-Ti (Touvron et al., [2021](#bib.bib199)) | 224$\times$224 | 6.0 | 2529.5
    | - | 74.5 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| DeiT-Ti (Touvron et al., [2021](#bib.bib199)) | 224$\times$224 | 6.0 | 2529.5
    | - | 74.5 |'
- en: '| DeiT-S (Touvron et al., [2021](#bib.bib199)) | 224$\times$224 | 22.0 | 936.2
    | 4.6 | 81.2 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| DeiT-S (Touvron et al., [2021](#bib.bib199)) | 224$\times$224 | 22.0 | 936.2
    | 4.6 | 81.2 |'
- en: '| DeiT-B (Touvron et al., [2021](#bib.bib199)) | 224$\times$224 | 87.0 | 290.9
    | 17.6 | 83.4 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| DeiT-B (Touvron et al., [2021](#bib.bib199)) | 224$\times$224 | 87.0 | 290.9
    | 17.6 | 83.4 |'
- en: '| DeiT-B (Touvron et al., [2021](#bib.bib199)) | 384$\times$384 | 87.0 | 85.8
    | 17.6 | 84.5 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| DeiT-B (Touvron et al., [2021](#bib.bib199)) | 384$\times$384 | 87.0 | 85.8
    | 17.6 | 84.5 |'
- en: '| ViT & Token Sparsing | T2T-ViT-14 (Yuan et al., [2021](#bib.bib239)) | 224$\times$224
    | 21.5 | - | 5.2 | 81.5 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| ViT & Token Sparsing | T2T-ViT-14 (Yuan et al., [2021](#bib.bib239)) | 224$\times$224
    | 21.5 | - | 5.2 | 81.5 |'
- en: '| T2T-ViT-14 (Yuan et al., [2021](#bib.bib239)) | 384$\times$384 | 21.5 | -
    | 17.1 | 83.3 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| T2T-ViT-14 (Yuan et al., [2021](#bib.bib239)) | 384$\times$384 | 21.5 | -
    | 17.1 | 83.3 |'
- en: '| T2T-ViT-19 (Yuan et al., [2021](#bib.bib239)) | 224$\times$224 | 39.2 | -
    | 8.9 | 81.9 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| T2T-ViT-19 (Yuan et al., [2021](#bib.bib239)) | 224$\times$224 | 39.2 | -
    | 8.9 | 81.9 |'
- en: '| DymViT-LViT-S/0.5 (Rao et al., [2021](#bib.bib160)) | 224$\times$224 | 26.9
    | - | 3.7 | 82.0 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| DymViT-LViT-S/0.5 (Rao et al., [2021](#bib.bib160)) | 224$\times$224 | 26.9
    | - | 3.7 | 82.0 |'
- en: '| DymViT-LViT-M/0.7 (Rao et al., [2021](#bib.bib160)) | 224$\times$224 | 57.1
    | - | 8.5 | 83.8 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| DymViT-LViT-M/0.7 (Rao et al., [2021](#bib.bib160)) | 224$\times$224 | 57.1
    | - | 8.5 | 83.8 |'
- en: '| EViT-DeiT-S (k=0.5) (Liang et al., [2021a](#bib.bib123)) | 224$\times$224
    | 22.0 | 4385 | 3.0 | 79.5 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| EViT-DeiT-S (k=0.5) (Liang et al., [2021a](#bib.bib123)) | 224$\times$224
    | 22.0 | 4385 | 3.0 | 79.5 |'
- en: '| EViT-DeiT-S (k=0.7) (Liang et al., [2021a](#bib.bib123)) | 224$\times$224
    | 22.0 | 5408 | 2.3 | 78.5 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| EViT-DeiT-S (k=0.7) (Liang et al., [2021a](#bib.bib123)) | 224$\times$224
    | 22.0 | 5408 | 2.3 | 78.5 |'
- en: '| EViT-LViT-S (k=0.5) (Liang et al., [2021a](#bib.bib123)) | 224$\times$224
    | 26.2 | 3603 | 3.9 | 82.5 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| EViT-LViT-S (k=0.5) (Liang et al., [2021a](#bib.bib123)) | 224$\times$224
    | 26.2 | 3603 | 3.9 | 82.5 |'
- en: '| EViT-LViT-S (k=0.7) (Liang et al., [2021a](#bib.bib123)) | 224$\times$224
    | 26.2 | 2954 | 4.7 | 83.0 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| EViT-LViT-S (k=0.7) (Liang et al., [2021a](#bib.bib123)) | 224$\times$224
    | 26.2 | 2954 | 4.7 | 83.0 |'
- en: '| A-ViT-T (Yin et al., [2022](#bib.bib234)) | 224$\times$224 | 5.0 | 3400 |
    0.8 | 71.0 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| A-ViT-T (Yin et al., [2022](#bib.bib234)) | 224$\times$224 | 5.0 | 3400 |
    0.8 | 71.0 |'
- en: '| A-ViT-S (Yin et al., [2022](#bib.bib234)) | 224$\times$224 | 22.0 | 1100
    | 3.6 | 78.6 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| A-ViT-S (Yin et al., [2022](#bib.bib234)) | 224$\times$224 | 22.0 | 1100
    | 3.6 | 78.6 |'
- en: '| ViT & CNN (Hybrid models) | Mobile-Former-96M (Chen et al., [2022a](#bib.bib28))
    | 224$\times$224 | 4.6 | - | 0.096 | 72.8 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| ViT & CNN (混合模型) | Mobile-Former-96M (Chen et al., [2022a](#bib.bib28)) |
    224$\times$224 | 4.6 | - | 0.096 | 72.8 |'
- en: '| Mobile-Former-29(Chen et al., [2022a](#bib.bib28)) | 224$\times$224 | 11.4
    | - | 0.294 | 77.9 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Mobile-Former-29 (Chen et al., [2022a](#bib.bib28)) | 224$\times$224 | 11.4
    | - | 0.294 | 77.9 |'
- en: '| Mobile-Former-508M (Chen et al., [2022a](#bib.bib28)) | 224$\times$224 |
    14.0 | - | 0.508 | 79.3 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Mobile-Former-508M (Chen et al., [2022a](#bib.bib28)) | 224$\times$224 |
    14.0 | - | 0.508 | 79.3 |'
- en: '| MobileViT-XS (Mehta and Rastegari, [2022](#bib.bib146)) | 224$\times$224
    | 2.3 | - | 0.7 | 74.8 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| MobileViT-XS (Mehta and Rastegari, [2022](#bib.bib146)) | 224$\times$224
    | 2.3 | - | 0.7 | 74.8 |'
- en: '| MobileViT-S (Mehta and Rastegari, [2022](#bib.bib146)) | 224$\times$224 |
    5.6 | - | - | 78.4 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| MobileViT-S (Mehta and Rastegari, [2022](#bib.bib146)) | 224$\times$224 |
    5.6 | - | - | 78.4 |'
- en: 3\. Fundamental methods in model compression
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 模型压缩的基本方法
- en: In this section, we explore popular compression methods used in recent years
    and their improvements over time. These techniques encompass pruning (LeCun et al.,
    [1989](#bib.bib114); Hassibi et al., [1993](#bib.bib77); Frankle and Carbin, [2019](#bib.bib55);
    He et al., [2019a](#bib.bib82); Hu et al., [2023](#bib.bib93)), quantization (Dong
    et al., [2019](#bib.bib44); Faghri et al., [2020](#bib.bib50); Hubara et al.,
    [2016](#bib.bib98)), knowledge distillation (Hinton et al., [2015](#bib.bib86);
    Gou et al., [2021](#bib.bib63); Zhang et al., [2018b](#bib.bib251)), and neural
    architecture search (Liu et al., [2019b](#bib.bib131); Wu et al., [2019](#bib.bib217)),
    which are widely adopted for designing efficient models. We further unveil a detailed
    exploration of each method, offering deeper insights that stem from their distinctive
    characteristics.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们探讨了近年来流行的压缩方法及其随时间的改进。这些技术包括剪枝（LeCun 等，[1989](#bib.bib114)；Hassibi 等，[1993](#bib.bib77)；Frankle
    和 Carbin，[2019](#bib.bib55)；He 等，[2019a](#bib.bib82)；Hu 等，[2023](#bib.bib93)）、量化（Dong
    等，[2019](#bib.bib44)；Faghri 等，[2020](#bib.bib50)；Hubara 等，[2016](#bib.bib98)）、知识蒸馏（Hinton
    等，[2015](#bib.bib86)；Gou 等，[2021](#bib.bib63)；Zhang 等，[2018b](#bib.bib251)）和神经架构搜索（Liu
    等，[2019b](#bib.bib131)；Wu 等，[2019](#bib.bib217)），这些方法被广泛采用以设计高效的模型。我们进一步详细探讨了每种方法，提供了源自其独特特征的深入见解。
- en: 3.1\. Pruning
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 剪枝
- en: DL models frequently comprise numerous learnable parameters, requiring extensive
    training. Pruning methods aim to compress and expedite NNs by removing redundant
    weights. These pruning methods can be categorized as either unstructured or structured.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型通常包含大量可学习参数，需要广泛的训练。剪枝方法旨在通过移除冗余权重来压缩和加速神经网络。这些剪枝方法可以分为无结构剪枝和结构剪枝。
- en: 3.1.1\. Unstructured pruning
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 无结构剪枝
- en: 'Unstructured pruning aims to identify and eliminate individual weights from
    the network, regardless of where they are located. This method imposes no restrictions
    or rules on weight trimming. Specifically, the nodes with the removed weights
    are not physically removed from the network; instead, the weights are set to zero.
    Since this operation results in numerous zero multiplications, models can be significantly
    compressed for faster inference. As illustrated in Fig. [4](#S3.F4 "Figure 4 ‣
    3.1.2\. Structured Pruning ‣ 3.1\. Pruning ‣ 3\. Fundamental methods in model
    compression ‣ Lightweight Deep Learning for Resource-Constrained Environments:
    A Survey") (left), unstructured pruning may cause the pruned network to have an
    irregular structure. Early works in pruning, such as Optimal Brain Damage (LeCun
    et al., [1989](#bib.bib114)) and Optimal Brain Surgeon (Hassibi et al., [1993](#bib.bib77)),
    utilize second-order derivatives and Hessian matrices to assess the importance
    of weights in the network and subsequently prune them. While these methods demonstrate
    impressive performance, they demand substantial computational power.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '无结构剪枝旨在识别和消除网络中的单个权重，无论其位置如何。这种方法对权重修剪不施加任何限制或规则。具体而言，被移除权重的节点不会从网络中物理移除；相反，权重被设置为零。由于这一操作会导致大量的零乘法，模型可以显著压缩，从而加快推理速度。如图[4](#S3.F4
    "Figure 4 ‣ 3.1.2\. Structured Pruning ‣ 3.1\. Pruning ‣ 3\. Fundamental methods
    in model compression ‣ Lightweight Deep Learning for Resource-Constrained Environments:
    A Survey")（左）所示，无结构剪枝可能导致剪枝后的网络具有不规则的结构。早期的剪枝工作，如 Optimal Brain Damage（LeCun 等，[1989](#bib.bib114)）和
    Optimal Brain Surgeon（Hassibi 等，[1993](#bib.bib77)），利用二阶导数和 Hessian 矩阵来评估网络中权重的重要性，并随后进行剪枝。尽管这些方法表现出色，但它们需要大量的计算能力。'
- en: To this end, Dong et al. (Dong et al., [2017](#bib.bib42)) introduce a method
    that restricts the computation of second-order derivatives. This approach does
    not require the computation of the Hessian matrix over all parameters; instead,
    it focuses on specific layers of the model. Similarly, Frankle et al. (Frankle
    and Carbin, [2019](#bib.bib55)) propose the lottery ticket hypothesis, where they
    attempt to find more manageable and pruned sub-networks while maintaining a performance
    comparable to the original network. In their approach, they prune the nodes, subsequently
    restoring the original pre-training initialization values of the untouched nodes,
    and repeat this cycle until a certain level of sparsity is achieved.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，Dong 等（Dong 等，[2017](#bib.bib42)）引入了一种限制二阶导数计算的方法。这种方法不需要对所有参数计算Hessian矩阵；而是专注于模型的特定层。同样，Frankle
    等（Frankle 和 Carbin，[2019](#bib.bib55)）提出了彩票票据假设，他们试图找到更易管理和剪枝的子网络，同时保持与原始网络相当的性能。在他们的方法中，他们剪枝节点，随后恢复未触及节点的原始预训练初始化值，并重复这一过程，直到达到一定的稀疏水平。
- en: However, unstructured pruning can significantly reduce accuracy when weights
    are pruned during the training process before the network converges. Unfortunately,
    the pruned connections cannot be restored. To address this limitation, Guo et
    al. (Guo et al., [2016](#bib.bib68)) introduce a splicing algorithm capable of
    recovering previously deleted connections that are discovered to be important
    at any point in time. Furthermore, Namhoon et al. (Lee et al., [2019](#bib.bib115))
    propose a single-shot network pruning approach in which they prune the network
    before the training begins. Instead of analyzing the model’s final weights after
    training, they examine the response of the loss function to variance scaling during
    initialization. This innovative approach allows the network to be pruned just
    once before training, providing a more convenient and effective pruning method.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当在网络尚未收敛的训练过程中剪枝权重时，非结构化剪枝可能会显著降低准确性。不幸的是，被剪枝的连接无法恢复。为了解决这一限制，Guo 等（Guo 等，[2016](#bib.bib68)）引入了一种拼接算法，能够恢复在任何时候被发现重要的先前删除的连接。此外，Namhoon
    等（Lee 等，[2019](#bib.bib115)）提出了一种单次网络剪枝方法，其中他们在训练开始之前对网络进行剪枝。他们并不是在训练后分析模型的最终权重，而是检查损失函数对初始化期间方差缩放的响应。这种创新的方法允许在训练之前对网络进行一次剪枝，提供了一种更便捷有效的剪枝方法。
- en: 3.1.2\. Structured Pruning
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 结构化剪枝
- en: 'Structured pruning methods remove pruned components from a pre-trained network
    and preserve its regular structure, as shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.2\.
    Structured Pruning ‣ 3.1\. Pruning ‣ 3\. Fundamental methods in model compression
    ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey")
    (right). Common structured pruning methods include filter pruning (He et al.,
    [2019a](#bib.bib82), [2018](#bib.bib81), [2020](#bib.bib80); Zhang and Freris,
    [2023](#bib.bib250)) and channel pruning (He et al., [2017](#bib.bib84); Peng
    et al., [2019](#bib.bib155); Hu et al., [2023](#bib.bib93)).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '结构化剪枝方法从预训练网络中去除剪枝的组件，并保留其常规结构，如图[4](#S3.F4 "Figure 4 ‣ 3.1.2\. Structured
    Pruning ‣ 3.1\. Pruning ‣ 3\. Fundamental methods in model compression ‣ Lightweight
    Deep Learning for Resource-Constrained Environments: A Survey")（右图）所示。常见的结构化剪枝方法包括滤波器剪枝（He
    等， [2019a](#bib.bib82)，[2018](#bib.bib81)，[2020](#bib.bib80)；Zhang 和 Freris，[2023](#bib.bib250)）和通道剪枝（He
    等， [2017](#bib.bib84)；Peng 等，[2019](#bib.bib155)；Hu 等，[2023](#bib.bib93)）。'
- en: 1) Filter pruning. Most pruning approaches rely on the ”smaller-norm-less-important”
    criterion, which involves pruning filters with lower norm values in the network
    (Li et al., [2017](#bib.bib116); Ye et al., [2018](#bib.bib233)). However, He
    et al. (He et al., [2019a](#bib.bib82)) point out the limitations of this criterion-based
    approach. They propose a novel technique for calculating the Geometric Median
    of filters within the same layer. By doing so, they prune filters that make the
    most replaceable contribution instead of those with comparatively less contribution.
    Criterion-based pruning methods tend to reduce model capacity due to fixed pruning
    thresholds. To address this, He et al. (He et al., [2020](#bib.bib80)) introduce
    learnable pruning thresholds for each layer using a differentiable criterion sampler,
    which can be updated during training. Additionally, Zhang et al. (Zhang and Freris,
    [2023](#bib.bib250)) propose an adaptive pruning threshold based on the sensitivity
    of the loss to the threshold value.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 过滤器剪枝。大多数剪枝方法依赖于“较小的范数更不重要”标准，这涉及到剪枝网络中范数值较低的过滤器（Li et al., [2017](#bib.bib116);
    Ye et al., [2018](#bib.bib233)）。然而，He et al.（He et al., [2019a](#bib.bib82)）指出了这种基于标准的方法的局限性。他们提出了一种新颖的技术，通过计算同一层内过滤器的几何中位数来进行剪枝。这样，他们剪枝那些贡献最可替代的过滤器，而不是那些相对贡献较少的过滤器。基于标准的剪枝方法往往由于固定的剪枝阈值而降低模型容量。为了解决这个问题，He
    et al.（He et al., [2020](#bib.bib80)）引入了可学习的剪枝阈值，通过可微分的标准采样器对每一层进行阈值调整，这些阈值可以在训练过程中更新。此外，Zhang
    et al.（Zhang and Freris, [2023](#bib.bib250)）提出了一种基于损失对阈值值敏感性的自适应剪枝阈值。
- en: '![Refer to caption](img/b72d28023b1beed70fe125ae42bf8ecd.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b72d28023b1beed70fe125ae42bf8ecd.png)'
- en: 'Figure 4\. Illustration of pruning methods: unstructured pruning (left), and
    structured pruning (right). Pruned components are shown in white color. Take note
    of the change in the pruned component’s output dimensions.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. 剪枝方法的示意图：非结构化剪枝（左），结构化剪枝（右）。剪枝的组件以白色显示。注意剪枝组件输出维度的变化。
- en: 2) Channel pruning. Channel pruning is another effective approach for reducing
    FLOPs and inference time, complementing filter pruning. He et al. (He et al.,
    [2017](#bib.bib84)) first implement channel pruning by focusing on eliminating
    redundant channels by evaluating the L1 norm. Peng et al. (Peng et al., [2019](#bib.bib155))
    take a different approach by using the Hessian matrix to model inter-channel dependencies
    and select channels using sequential quadratic programming. For more complex modules
    like group convolutions and depthwise convolutions, Liu et al. (Liu et al., [2021c](#bib.bib132))
    introduce a layer grouping mechanism to search for coupled channels automatically.
    The importance of these channels is calculated based on Fisher’s information.
    CATRO (Hu et al., [2023](#bib.bib93)) leverages feature space discrimination to
    assess the joint impact of multiple channels while consolidating the layer-by-layer
    impact of preserved channels.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 通道剪枝。通道剪枝是另一种有效的减少 FLOPs 和推理时间的方法，补充了过滤器剪枝。He et al.（He et al., [2017](#bib.bib84)）首先通过评估
    L1 范数来实施通道剪枝，重点在于去除冗余通道。Peng et al.（Peng et al., [2019](#bib.bib155)）采取了不同的方法，通过使用
    Hessian 矩阵来建模通道间依赖关系，并使用序列二次规划选择通道。对于更复杂的模块，如组卷积和深度卷积，Liu et al.（Liu et al., [2021c](#bib.bib132)）引入了层分组机制，以自动搜索耦合通道。这些通道的重要性基于
    Fisher 信息进行计算。CATRO（Hu et al., [2023](#bib.bib93)）利用特征空间的区分来评估多个通道的联合影响，同时巩固保留通道的逐层影响。
- en: 3.1.3\. Comparison of pruning methods
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3. 剪枝方法的比较
- en: 'Table [4](#S3.T4 "Table 4 ‣ 3.1.3\. Comparison of pruning methods ‣ 3.1\. Pruning
    ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep Learning for
    Resource-Constrained Environments: A Survey") displays the accuracy after pruning
    and the corresponding pruned FLOPs of the various structure pruning methods. While
    one might initially assume that the best-performing methods prune the highest
    number of FLOPs, in reality, we often perceive the ”best” as those that effectively
    balance the trade-off between pruned FLOPs and the associated drop in accuracy.
    For instance, while GFP attains the highest pruned accuracy, its reduction in
    FLOPs is limited to 50.6%. In contrast, ASTER removes the most FLOPs, yet its
    pruned accuracy falls short of being the best. In summary, filter and channel
    pruning methods can efficiently decrease the FLOPs while maintaining similar accuracy.
    We advocate choosing a pruning method that seamlessly integrates with the current
    network architecture, prioritizing ease of implementation. For example, if the
    network’s feature map boasts over a thousand channels but only uses a few filters,
    opting for channel pruning would be more beneficial.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表[4](#S3.T4 "Table 4 ‣ 3.1.3\. 剪枝方法的比较 ‣ 3.1\. 剪枝 ‣ 3\. 模型压缩中的基本方法 ‣ 面向资源受限环境的轻量级深度学习：一项调查")显示了各种结构剪枝方法的剪枝后准确率和相应的剪枝FLOPs。尽管人们可能最初认为表现最好的方法剪枝了最多的FLOPs，但实际上，我们通常认为“最好”的是那些在剪枝FLOPs和准确率降低之间有效平衡权衡的方法。例如，尽管GFP获得了最高的剪枝准确率，但它的剪枝FLOPs减少仅限于50.6%。相比之下，ASTER删除了最多的FLOPs，但其剪枝准确率并没有名列前茅。总结起来，滤波器和通道剪枝方法能够有效降低FLOPs而保持类似的准确率。我们主张选择与当前网络架构无缝集成且易于实现的剪枝方法。例如，如果网络特征图具有一千多个通道但只使用了少量滤波器，那么选择通道剪枝会更有益处。
- en: Table 4\. Comparison of different pruning methods using ResNet50 on the ImageNet
    dataset. The methods that achieve the highest percentage of pruned FLOPs are marked
    in bold.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表4. 在ImageNet数据集上使用ResNet50对不同剪枝方法进行比较。以剪枝FLOPs最高的方法为标记。
- en: '| Type | Method (30%) | Baseline (%) | Pruned Acc. (%) | Pruned FLOPs (%) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 方法（30%） | 基线（%） | 剪枝准确率（%） | 剪枝FLOPs（%） |'
- en: '| - | ResNet50 | 76.15 | - | - |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| - | ResNet50 | 76.15 | - | - |'
- en: '| Filter | SFP (He et al., [2018](#bib.bib81)) | 76.15 | 74.61 (-1.54) | 41.8
    |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 滤波器 | SFP (He et al., [2018](#bib.bib81)) | 76.15 | 74.61 (-1.54) | 41.8
    |'
- en: '| FPGM (He et al., [2019a](#bib.bib82)) | 76.15 | 75.59 (-0.56) | 42.2 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| FPGM (He et al., [2019a](#bib.bib82)) | 76.15 | 75.59 (-0.56) | 42.2 |'
- en: '| LFPC (He et al., [2020](#bib.bib80)) | 76.15 | 74.46 (-1.69) | 60.8 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| LFPC (He et al., [2020](#bib.bib80)) | 76.15 | 74.46 (-1.69) | 60.8 |'
- en: '| ASTER (Zhang and Freris, [2023](#bib.bib250)) | 76.15 | 75.27 (-0.88) | 63.2
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| ASTER (Zhang and Freris, [2023](#bib.bib250)) | 76.15 | 75.27 (-0.88) | 63.2
    |'
- en: '| Channel | CCP (Peng et al., [2019](#bib.bib155)) | 76.15 | 75.50 (-0.65)
    | 48.8 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 通道 | CCP (Peng et al., [2019](#bib.bib155)) | 76.15 | 75.50 (-0.65) | 48.8
    |'
- en: '| GFP (Liu et al., [2021c](#bib.bib132)) | 76.79 | 76.42 (-0.37) | 50.6 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| GFP (Liu et al., [2021c](#bib.bib132)) | 76.79 | 76.42 (-0.37) | 50.6 |'
- en: '| SCP (Kang and Han, [2020](#bib.bib107)) | 75.89 | 74.20 (-1.69) | 54.3 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| SCP (Kang and Han, [2020](#bib.bib107)) | 75.89 | 74.20 (-1.69) | 54.3 |'
- en: '| CATRO (Hu et al., [2023](#bib.bib93)) | 75.98 | 75.84 (-0.14) | 45.8 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| CATRO (Hu et al., [2023](#bib.bib93)) | 75.98 | 75.84 (-0.14) | 45.8 |'
- en: '![Refer to caption](img/33075eac13ea5c0990feb97e108b9d6a.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/33075eac13ea5c0990feb97e108b9d6a.png)'
- en: Figure 5\. Symmetric (left) and asymmetric (right) quantization representation (Gholami
    et al., [2022](#bib.bib59)). Note that r represents the real value, S represents
    the real-valued scaling factor, and Z represents the integer zero point.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图5. 对称（左）和非对称（右）量化表示（Gholami et al., [2022](#bib.bib59)）。注意，r表示实际值，S表示实值的缩放因子，Z表示整数零点。
- en: 3.2\. Quantization
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 量化
- en: 'Pruning is an efficient way to compress the model. However, after pruning,
    the remaining weights, typically stored as full-precision 32-bit floating-point
    numbers (float32), still demand significant memory. To address this, quantization (Gray
    and Neuhoff, [1998](#bib.bib65)), a technique that allows parameters to be represented
    with reduced bit precision, becomes a desirable solution. Specifically, quantization
    maps weights and activations to a set of finite numbers through a calibration
    process that determines potential values using a symmetric or asymmetric representation.
    As depicted in Fig. [5](#S3.F5 "Figure 5 ‣ 3.1.3\. Comparison of pruning methods
    ‣ 3.1\. Pruning ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep
    Learning for Resource-Constrained Environments: A Survey"), both methods define
    a range [$\alpha$, $\beta$], in symmetric quantization, -$\alpha$ = $\beta$, whereas
    in the asymmetric quantization, -$\alpha\neq\beta$.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '剪枝是一种高效的模型压缩方法。然而，在剪枝之后，剩余的权重通常以全精度的 32 位浮点数（float32）存储，仍然需要大量内存。为了解决这个问题，量化（Gray
    and Neuhoff, [1998](#bib.bib65)）成为一种可取的解决方案，量化是一种允许以减少位精度表示参数的技术。具体来说，量化通过一个校准过程将权重和激活映射到有限的数集，通过对称或非对称表示来确定潜在值。如图
    [5](#S3.F5 "Figure 5 ‣ 3.1.3\. Comparison of pruning methods ‣ 3.1\. Pruning ‣
    3\. Fundamental methods in model compression ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") 所示，两种方法定义了一个范围 [$\alpha$, $\beta$]，在对称量化中，- $\alpha$
    = $\beta$，而在非对称量化中，- $\alpha \neq \beta$。'
- en: 'The calibration of this range, as outlined by Gholami et al. (Gholami et al.,
    [2022](#bib.bib59)), falls into two categories: dynamic and static calibration.
    The first one is accurate but computationally demanding, as it computes $[\alpha,\beta]$
    for each feature map. The latter is a computationally lighter alternative because
    it calculates the range based on typical values after several iterations, albeit
    with less accuracy. Both dynamic and static calibration are pivotal for optimizing
    the quantization process.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 该范围的校准，如 Gholami 等人（Gholami et al., [2022](#bib.bib59)）所述，分为动态校准和静态校准两类。动态校准准确但计算开销大，因为它为每个特征图计算
    $[\alpha,\beta]$。静态校准则计算开销较小，因为它基于几次迭代后的典型值计算范围，尽管精度较低。动态和静态校准对于优化量化过程都是至关重要的。
- en: Quantization theory has been applied to NN from various perspectives over time.
    For instance, Gupta et al. (Gupta et al., [2015](#bib.bib71)) introduce the use
    of fixed-point numbers during the model’s training process to enhance the algorithm’s
    noise tolerance. They also employ stochastic rounding as an alternative to the
    round-to-nearest strategy to counteract the adverse effects of fixed-point numbers.
    In another approach, Faghri et al. (Faghri et al., [2020](#bib.bib50)) introduce
    two adaptive quantization methods, Adaptive Level Quantization (ALQ) and Adaptive
    Multiplier Quantization (AMQ), which update their compression method in parallel
    during training to quantize the gradients in data-parallel stochastic gradient
    descent adaptively. This adaptation aims to reduce communication costs between
    the processors. Lastly, Wang et al. (Wang et al., [2022a](#bib.bib208)) treat
    the quantization problem as a differentiable lookup operation. They jointly optimized
    both the network and the associated tables during training.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 量化理论从不同角度被应用于神经网络。例如，Gupta 等人（Gupta et al., [2015](#bib.bib71)）在模型训练过程中引入了固定点数，以增强算法的噪声容忍度。他们还采用了随机舍入作为替代最近舍入策略的方法，以抵消固定点数的不利影响。在另一种方法中，Faghri
    等人（Faghri et al., [2020](#bib.bib50)）提出了两种自适应量化方法，分别是自适应水平量化（ALQ）和自适应乘法器量化（AMQ），这两种方法在训练过程中并行更新其压缩方法，以自适应地量化数据并行随机梯度下降中的梯度。这种适应旨在减少处理器之间的通信成本。最后，Wang
    等人（Wang et al., [2022a](#bib.bib208)）将量化问题视为一个可微分的查找操作。他们在训练过程中联合优化了网络和相关表格。
- en: 3.2.1\. Half-precision and Mixed-precision training
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 半精度和混合精度训练
- en: Mixed-precision training involves using lower-precision values while retaining
    full-precision values for crucial information (Micikevicius et al., [2018](#bib.bib148)).
    For instance, in a notable series of works, HAWQ (Dong et al., [2019](#bib.bib44))
    implements an automatic approach based on the Hessian of the model to determine
    the optimal mixed-precision settings for weight values. Subsequently, the HAWQ-V2
    model (Dong et al., [2020](#bib.bib43)) introduces mixed-precision quantization
    for activation values. The HAWQ-V3 model (Yao et al., [2021](#bib.bib231)) further
    improves it by focusing on integer-only quantization. Interestingly, Liu et al. (Liu
    et al., [2021b](#bib.bib133)) introduce a method that utilizes a linear combination
    of multiple low-bit vectors to approximate a full-precision vector. This approach
    achieves ”mixed-precision training” with a single precision level by varying the
    number of vectors to approximate different weights.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 混合精度训练涉及使用较低精度的值，同时保留关键数据的全精度值（Micikevicius et al., [2018](#bib.bib148)）。例如，在一系列重要的工作中，HAWQ（Dong
    et al., [2019](#bib.bib44)）实现了一种基于模型Hessian的自动方法，以确定权重值的最佳混合精度设置。随后，HAWQ-V2模型（Dong
    et al., [2020](#bib.bib43)）引入了对激活值的混合精度量化。HAWQ-V3模型（Yao et al., [2021](#bib.bib231)）通过关注仅整数量化进一步改进了这一点。有趣的是，Liu等人（Liu
    et al., [2021b](#bib.bib133)）介绍了一种利用多个低位向量的线性组合来近似全精度向量的方法。这种方法通过变化向量的数量来近似不同的权重，实现了具有单一精度级别的“混合精度训练”。
- en: 3.2.2\. Quantization using fewer bits
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 使用更少位的量化
- en: In an early work by Banner et al. (Banner et al., [2018](#bib.bib10)), the quantization
    of weights, activations, and most gradient streams in all layers of an NN is performed
    using 8-bit precision by replacing traditional batch-norm with ranged batch-norm
    layers. Another technique proposed by Wang et al. (Wang et al., [2018](#bib.bib209))
    allows matrix and convolutional operations to also be implemented using 8-bit
    numbers. Furthermore, there have also been methods that use ternary values to
    quantize an NN. In an important work done by Liu et al., TWN (Liu et al., [2023](#bib.bib130))
    manages to constrain weights to +1, 0, and -1 values, achieving a 16x compression
    of the model. This idea is extended in TTQ (Zhu et al., [2017](#bib.bib257)),
    where the positive and negative weights use two different learnable scales $w_{1}$
    and $w_{2}$, resulting in possible values of $-w_{1}$, $0$, and $w_{2}$.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在Banner等人的早期工作（Banner et al., [2018](#bib.bib10)）中，NN所有层中权重、激活和大多数梯度流的量化使用8位精度，通过将传统的批量归一化替换为范围批量归一化层来实现。Wang等人（Wang
    et al., [2018](#bib.bib209)）提出的另一种技术允许矩阵和卷积操作也使用8位数字。此外，还有一些方法使用三元值对NN进行量化。在Liu等人的重要工作中，TWN（Liu
    et al., [2023](#bib.bib130)）成功地将权重约束为+1、0和-1值，实现了模型的16倍压缩。这一思想在TTQ（Zhu et al.,
    [2017](#bib.bib257)）中得到扩展，其中正负权重使用两个不同的可学习尺度$w_{1}$和$w_{2}$，从而产生可能的值$-w_{1}$、$0$和$w_{2}$。
- en: More aggressive approaches have sought to reduce quantization levels further
    by implementing NN binarization. This approach uses binary values instead of floating-point
    or integer values for faster computations, lower memory usage, and reduced power
    consumption. Courbariaux et al.’s pioneering work (Hubara et al., [2016](#bib.bib98))
    binarizes networks by restricting the weights and activations to either +1 or
    -1, determining the final values by evaluating the sign of the real values. Variations
    of this work include topologies such as XNOR-Net (Rastegari et al., [2016](#bib.bib161))
    and the Least Squares method (Pouransari et al., [2020](#bib.bib156)), which introduce
    an additional activation layer after the binary convolutions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 更具攻击性的方法试图通过实施NN二值化进一步降低量化级别。该方法使用二进制值代替浮点或整数值，以实现更快的计算、更低的内存使用和减少的功耗。Courbariaux等人的开创性工作（Hubara
    et al., [2016](#bib.bib98)）通过将权重和激活限制为+1或-1来对网络进行二值化，通过评估实际值的符号来确定最终值。这项工作的变体包括如XNOR-Net（Rastegari
    et al., [2016](#bib.bib161)）和最小二乘法（Pouransari et al., [2020](#bib.bib156)）等拓扑，这些方法在二值卷积后引入了额外的激活层。
- en: 3.2.3\. Quantization Aware Training (QAT)
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 量化感知训练（QAT）
- en: In the early stages of quantization research, a prevalent approach was first
    to train an unquantized model, apply a quantization process, and then retrain
    or fine-tune the model to achieve an acceptable level of accuracy. This methodology,
    known as Post-training Quantization (PTQ), proved to be an effective strategy
    for achieving significant compression, especially when the pre-trained model has
    ample representational capacity. The success of PTQ lies in its ability to balance
    compression gains and maintain satisfactory model accuracy, making it a pivotal
    technique in model optimization and deployment. However, quantization is a lossy
    process, which can lead to a significant drop in model accuracy. To address this
    issue, Jacob et al. (Jacob et al., [2018](#bib.bib100)) introduced QAT, a technique
    that computes inference-time quantization errors during the model training stage,
    allowing the model to become aware of these errors and make adjustments accordingly.
    This process simulates inference-time errors through a process known as FakeQuant.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在量化研究的早期阶段，一种普遍的方法是首先训练一个未经量化的模型，然后应用量化过程，接着重新训练或微调模型以实现可接受的准确度。这种方法被称为后训练量化（PTQ），证明它是一种有效的策略，可以实现显著的压缩，特别是当预训练模型具有足够的表示能力时。PTQ的成功在于其能够平衡压缩收益和保持令人满意的模型准确度，使其成为模型优化和部署中的关键技术。然而，量化是一个有损过程，可能导致模型准确度显著下降。为了解决这一问题，Jacob等人（Jacob
    et al., [2018](#bib.bib100)）提出了QAT，一种在模型训练阶段计算推理时间量化误差的技术，使模型能够意识到这些误差并进行相应调整。这一过程通过称为FakeQuant的过程来模拟推理时间误差。
- en: Improvements to the core QAT technique have been explored by introducing learnable
    clipping scalars (Choi et al., [2018](#bib.bib34)). In a recent development, Sakr
    et al. (Sakr et al., [2022](#bib.bib168)) achieved state-of-the-art performance
    by identifying the MSE-minimizing clipping scalars and implementing 4-bit quantization.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对核心QAT技术的改进已经通过引入可学习的裁剪标量（Choi et al., [2018](#bib.bib34)）进行探索。在最近的发展中，Sakr等人（Sakr
    et al., [2022](#bib.bib168)）通过识别最小化MSE的裁剪标量并实现4位量化，达到了最先进的性能。
- en: 3.2.4\. Comparison of quantization methods
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. 量化方法比较
- en: 'Table [5](#S3.T5 "Table 5 ‣ 3.2.4\. Comparison of quantization methods ‣ 3.2\.
    Quantization ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep
    Learning for Resource-Constrained Environments: A Survey") compares the performance
    of quantization methods on the ImageNet dataset, emphasizing the trade-off between
    compression and accuracy loss. Notably, binarized networks aiming for a 32x compression
    and speedup show significant accuracy drops. On the other hand, approaches with
    4-bit quantization, except (Liu et al., [2021b](#bib.bib133)), result in little
    loss of accuracy and can, therefore, be a good choice of precision for quantization.
    However, theoretical compression and speedup expectations may not align with actual
    results due to additional operations like quantization and dequantization. This
    may explain why some works opt not to conduct an in-depth analysis of the quantized
    model size, although (Liu et al., [2021b](#bib.bib133)) does provide such an analysis
    and successfully achieves an approximately 8-fold reduction in the model size
    (42.56 MB to 5.37 MB).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '表[5](#S3.T5 "Table 5 ‣ 3.2.4\. Comparison of quantization methods ‣ 3.2\. Quantization
    ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep Learning for
    Resource-Constrained Environments: A Survey")比较了在ImageNet数据集上量化方法的性能，强调了压缩与准确度损失之间的权衡。值得注意的是，旨在实现32倍压缩和加速的二值化网络显示出显著的准确度下降。另一方面，4位量化的方法（除了(Liu
    et al., [2021b](#bib.bib133))）导致的准确度损失较小，因此可以成为量化的良好精度选择。然而，由于量化和反量化等附加操作，理论上的压缩和加速期望可能与实际结果不符。这可能解释了为什么一些研究选择不深入分析量化模型的大小，尽管(Liu
    et al., [2021b](#bib.bib133))提供了这样的分析，并成功实现了模型大小大约减少8倍（42.56 MB到5.37 MB）。'
- en: Table 5\. Comparison of several quantization methods using different levels
    of precision to compress a ResNet18 on the ImageNet dataset.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 表5\. 比较了使用不同精度水平量化ResNet18在ImageNet数据集上的几种量化方法。
- en: '| Method | Initial Accuracy. (%) | Quantized accuracy (%) | Precision |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 初始准确度 (%) | 量化准确度 (%) | 精度 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| QIL (Jung et al., [2019](#bib.bib105)) | 70.2 | 70.1 (-0.1) | 4-bit |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| QIL (Jung et al., [2019](#bib.bib105)) | 70.2 | 70.1 (-0.1) | 4-bit |'
- en: '| (Liu et al., [2021b](#bib.bib133)) | 69.8 | 61.7 (-8.1) | 4-bit |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| (Liu et al., [2021b](#bib.bib133)) | 69.8 | 61.7 (-8.1) | 4-bit |'
- en: '| LLT (Wang et al., [2022a](#bib.bib208)) | 69.8 | 70.4 (+0.6) | 4-bit |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| LLT (Wang et al., [2022a](#bib.bib208)) | 69.8 | 70.4 (+0.6) | 4-bit |'
- en: '| LLT (Wang et al., [2022a](#bib.bib208)) | 69.8 | 69.5 (-0.3) | 3-bit |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| LLT (Wang等，[2022a](#bib.bib208)) | 69.8 | 69.5 (-0.3) | 3-bit |'
- en: '| HAWQ-V3 (Yao et al., [2021](#bib.bib231)) | 71.5 | 68.5 (-3.0) | MP |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| HAWQ-V3 (Yao等，[2021](#bib.bib231)) | 71.5 | 68.5 (-3.0) | MP |'
- en: '| TWN (Liu et al., [2023](#bib.bib130)) | 65.4 | 61.8 (-3.6) | 2-bit |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| TWN (Liu等，[2023](#bib.bib130)) | 65.4 | 61.8 (-3.6) | 2-bit |'
- en: '| TTQ (Zhu et al., [2017](#bib.bib257)) | 69.6 | 66.6 (-3.0) | 2-bit |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| TTQ (Zhu等，[2017](#bib.bib257)) | 69.6 | 66.6 (-3.0) | 2-bit |'
- en: '| XNOR-Net (Rastegari et al., [2016](#bib.bib161)) | 69.3 | 51.2 (-18.1) |
    1-bit |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| XNOR-Net (Rastegari等，[2016](#bib.bib161)) | 69.3 | 51.2 (-18.1) | 1-bit |'
- en: '| Least Squares (Pouransari et al., [2020](#bib.bib156)) | 69.6 | 63.4 (-6.2)
    | 1-bit |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 最小二乘法（Pouransari等，[2020](#bib.bib156)） | 69.6 | 63.4 (-6.2) | 1-bit |'
- en: 3.3\. Knowledge Distillation (KD)
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 知识蒸馏（KD）
- en: 'KD is a model compression technique designed to transfer knowledge from a large
    network to a smaller one (Hinton et al., [2015](#bib.bib86); Gou et al., [2021](#bib.bib63)).
    Its simplest form is illustrated in Fig. [6](#S3.F6 "Figure 6 ‣ 3.3\. Knowledge
    Distillation (KD) ‣ 3\. Fundamental methods in model compression ‣ Lightweight
    Deep Learning for Resource-Constrained Environments: A Survey")(a), where the
    larger model is referred to as the teacher and the smaller model as the student.
    In the approach proposed by Hinton et al. (Hinton et al., [2015](#bib.bib86)),
    the teacher model is initially trained to generate soft labels. Then, the training
    of the student model leverages ground-truth labels and the teacher’s predictions
    on the same data. This combination enables the student to attain performance comparable
    to the teacher using fewer parameters.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 'KD是一种模型压缩技术，旨在将知识从大型网络转移到较小的网络（Hinton等，[2015](#bib.bib86)；Gou等，[2021](#bib.bib63)）。其最简单的形式如图[6](#S3.F6
    "Figure 6 ‣ 3.3\. Knowledge Distillation (KD) ‣ 3\. Fundamental methods in model
    compression ‣ Lightweight Deep Learning for Resource-Constrained Environments:
    A Survey")(a)所示，其中较大的模型称为教师，较小的模型称为学生。在Hinton等提出的方法（Hinton等，[2015](#bib.bib86)）中，教师模型首先被训练以生成软标签。然后，学生模型的训练利用真实标签和教师在相同数据上的预测。这种组合使学生能够使用更少的参数达到与教师相当的性能。'
- en: 'KD algorithms can be categorized into three types: offline, online, and self-distillation,
    as illustrated in Fig. [6](#S3.F6 "Figure 6 ‣ 3.3\. Knowledge Distillation (KD)
    ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep Learning for
    Resource-Constrained Environments: A Survey"). The key distinction lies in the
    teacher’s definition and training strategy. For instance, in offline distillation,
    teacher and student training processes are performed sequentially, whereas in
    online distillation, the teacher can continue or initiate training alongside the
    student. On the other hand, in self-distillation, the student becomes its own
    teacher.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 'KD算法可以分为三种类型：离线、在线和自蒸馏，如图[6](#S3.F6 "Figure 6 ‣ 3.3\. Knowledge Distillation
    (KD) ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey")所示。主要区别在于教师的定义和训练策略。例如，在离线蒸馏中，教师和学生的训练过程是顺序进行的，而在在线蒸馏中，教师可以在学生训练的同时继续或启动训练。另一方面，在自蒸馏中，学生成为自己的教师。'
- en: '![Refer to caption](img/8116c3e209a39e9e43d54eddbe734e9d.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8116c3e209a39e9e43d54eddbe734e9d.png)'
- en: Figure 6\. (a) Offline Distillation (Hinton et al., [2015](#bib.bib86)). (b)
    Online Distillation (Zhang et al., [2018b](#bib.bib251)). (c) Self-Distillation (Zhang
    et al., [2019b](#bib.bib247)). We use orange lines to indicate the gradient update.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. (a) 离线蒸馏（Hinton等，[2015](#bib.bib86)）。(b) 在线蒸馏（Zhang等，[2018b](#bib.bib251)）。(c)
    自蒸馏（Zhang等，[2019b](#bib.bib247)）。我们用橙色线条表示梯度更新。
- en: 3.3.1\. Offline Distillation
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 离线蒸馏
- en: Most of the earlier KD works fall under the category of offline distillation.
    In offline distillation, a pre-trained teacher model is required, as seen in the
    case of the vanilla KD (Hinton et al., [2015](#bib.bib86)). While offline distillation
    is relatively easy to implement, it comes with the unavoidable overhead of time
    and computational resources required to train a large teacher model first.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的大多数KD研究属于离线蒸馏的范畴。在离线蒸馏中，需要一个预训练的教师模型，如vanilla KD（Hinton等，[2015](#bib.bib86)）的情况。虽然离线蒸馏相对容易实现，但它不可避免地需要花费时间和计算资源来首先训练一个大型教师模型。
- en: Various methods have been explored to enhance KD algorithms, including introducing
    alternative loss functions such as contrastive-based loss (Tian et al., [2020](#bib.bib198))
    and minimizing the maximum mean discrepancy between models (Huang and Wang, [2019](#bib.bib97)).
    Significant size disparities between teacher and student models can impact results,
    leading Zhao et al. (Zhao et al., [2022](#bib.bib253)) to redefine logit distillation
    by decoupling the influence of target and non-target classes. Lin et al. (Lin
    et al., [2022b](#bib.bib127)) address the semantic information gap in KD by dynamically
    distilling each pixel of the teacher features to all spatial locations of the
    student features, guided by a similarity measure from the transformer.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强KD算法，已经探索了各种方法，包括引入对比损失（Tian et al., [2020](#bib.bib198)）以及最小化模型之间的最大均值差异（Huang
    and Wang, [2019](#bib.bib97)）。教师模型和学生模型之间的显著尺寸差异可能会影响结果，因此Zhao et al.（Zhao et
    al., [2022](#bib.bib253)）通过解耦目标类和非目标类的影响重新定义了logit蒸馏。Lin et al.（Lin et al., [2022b](#bib.bib127)）通过动态蒸馏教师特征的每个像素到学生特征的所有空间位置来解决KD中的语义信息差距，并由transformer的相似性度量指导。
- en: Recently, SimKD (Chen et al., [2022b](#bib.bib20)) proposed a straightforward
    distillation approach, reusing the teacher’s classifier and aligning intermediate
    features with an L2 loss. SemCKD (Chen et al., [2021c](#bib.bib21)) involves student
    learning through feature embedding, preserving feature similarities in the intermediate
    layers of the teacher network.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，SimKD（Chen et al., [2022b](#bib.bib20)）提出了一种简单的蒸馏方法，重用教师的分类器，并用L2损失对齐中间特征。SemCKD（Chen
    et al., [2021c](#bib.bib21)）通过特征嵌入进行学生学习，保持教师网络中间层的特征相似性。
- en: 3.3.2\. Online Distillation
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2. 在线蒸馏
- en: Offline distillation can be problematic when obtaining a pre-trained large teacher
    model is not feasible, rendering many of the previously mentioned methods unusable.
    Online distillation introduces an end-to-end training strategy that overcomes
    this limitation by concurrently training the teacher and student networks, challenging
    the traditional concept of a ”single large teacher” (Zhang et al., [2018b](#bib.bib251);
    Guo et al., [2020](#bib.bib67); Li et al., [2023](#bib.bib118)).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当获取一个预训练的大型教师模型不可行时，离线蒸馏可能会遇到问题，这使得许多之前提到的方法无法使用。在线蒸馏引入了一种端到端的训练策略，通过同时训练教师和学生网络来克服这一限制，挑战了“单一大型教师”的传统观念（Zhang
    et al., [2018b](#bib.bib251); Guo et al., [2020](#bib.bib67); Li et al., [2023](#bib.bib118)）。
- en: 'The Deep Mutual Learning (DML) algorithm, proposed in (Zhang et al., [2018b](#bib.bib251)),
    eliminates the need for a pre-trained teacher in the KD process, as depicted in
    Fig. [6](#S3.F6 "Figure 6 ‣ 3.3\. Knowledge Distillation (KD) ‣ 3\. Fundamental
    methods in model compression ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey")(b). Instead, this approach advocates simultaneous learning
    of a cohort of networks, with each network incorporating the predictions of the
    others in its loss functions. This change enables all networks in the cohort to
    benefit from each other’s knowledge, even improving networks that are large enough
    to have acted as teachers in a conventional KD process. These large networks can
    enhance their results with knowledge distilled from other untrained, smaller networks.
    Further refinements of this approach have been made in (Guo et al., [2020](#bib.bib67);
    Li et al., [2023](#bib.bib118)). Online distillation techniques can also incorporate
    adversarial concepts. Zhang et al. (Zhang et al., [2021](#bib.bib244)) propose
    an adversarial co-distillation approach that employs Generative Adversarial Networks
    (GANs) to explore ”divergent examples” and enhance knowledge transfer.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 'Deep Mutual Learning（DML）算法（Zhang et al., [2018b](#bib.bib251)）消除了KD过程对预训练教师的需求，如图[6](#S3.F6
    "Figure 6 ‣ 3.3. Knowledge Distillation (KD) ‣ 3. Fundamental methods in model
    compression ‣ Lightweight Deep Learning for Resource-Constrained Environments:
    A Survey")(b)所示。相反，这种方法倡导同时学习一组网络，每个网络在其损失函数中包含其他网络的预测。这一变化使得组中的所有网络都能从彼此的知识中受益，即使是那些足够大的网络，它们在传统的KD过程中充当了教师。这些大型网络可以通过其他未训练的小型网络提取的知识来提升其结果。这一方法的进一步改进已经在（Guo
    et al., [2020](#bib.bib67); Li et al., [2023](#bib.bib118)）中提出。在线蒸馏技术还可以融入对抗性概念。Zhang
    et al.（Zhang et al., [2021](#bib.bib244)）提出了一种对抗性共蒸馏方法，该方法利用生成对抗网络（GANs）探索“分歧样本”以增强知识转移。'
- en: Furthermore, online distillation has demonstrated notable efficacy in scenarios
    requiring generating pseudo labels for data. The widely adopted mean teacher framework (Tarvainen
    and Valpola, [2017](#bib.bib196)) introduces the concept of employing two identical
    models; specifically, the teacher model has the same structure as the student
    model. The primary idea involves updating the teacher’s weights through an exponential
    moving average (EMA) of the student’s weights. In various unsupervised contexts
    (Deng et al., [2021](#bib.bib41); Yu et al., [2022](#bib.bib238)), this principle
    is leveraged to create pseudo labels for training the student via a supervised
    loss. Notably, each prediction made by the teacher model can be viewed as an ensemble
    incorporating the current and past iterations of the student model, rendering
    it inherently more robust and stable.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在线蒸馏在需要生成伪标签的数据场景中表现出显著的效果。广泛采用的均值教师框架（Tarvainen and Valpola, [2017](#bib.bib196)）引入了使用两个相同结构模型的概念；具体而言，教师模型与学生模型具有相同的结构。主要思想是通过学生权重的指数移动平均（EMA）来更新教师的权重。在各种无监督上下文中（Deng
    et al., [2021](#bib.bib41); Yu et al., [2022](#bib.bib238)），这一原则被用来创建伪标签以通过监督损失训练学生。值得注意的是，教师模型做出的每一个预测都可以看作是当前和过去学生模型迭代的集合，使其本质上更加稳健和稳定。
- en: 3.3.3\. Self-Distillation
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3\. 自蒸馏
- en: 'As depicted in Fig. [6](#S3.F6 "Figure 6 ‣ 3.3\. Knowledge Distillation (KD)
    ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep Learning for
    Resource-Constrained Environments: A Survey")(c), self-distillation techniques
    involve the process of KD, where a model distills knowledge from itself. In this
    scenario, during the training process, a single instance of the model simultaneously
    acts as both the teacher and student. Strategies in this distillation approach
    encompass using the same model saved at different epochs (Yang et al., [2019](#bib.bib226))
    and leveraging various model layers for self-instruction (Yuan et al., [2020](#bib.bib240);
    Hou et al., [2019](#bib.bib88)).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[6](#S3.F6 "Figure 6 ‣ 3.3\. Knowledge Distillation (KD) ‣ 3\. Fundamental
    methods in model compression ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey")(c)所示，自蒸馏技术涉及KD过程，其中一个模型从自身中提取知识。在这种情况下，在训练过程中，单个模型同时作为教师和学生。该蒸馏方法的策略包括使用在不同时期保存的相同模型（Yang
    et al., [2019](#bib.bib226)）以及利用不同模型层进行自我指导（Yuan et al., [2020](#bib.bib240);
    Hou et al., [2019](#bib.bib88)）。'
- en: Zhang et al. (Zhang et al., [2019b](#bib.bib247)) pioneered self-distillation
    from deeper to shallower layers of the model. Their innovation improves results
    and reduces training time by eliminating the need for additional networks. Similarly,
    Hou et al. (Hou et al., [2019](#bib.bib88)) harness knowledge transfer through
    attention maps from deeper layers. Yang et al. (Yang et al., [2019](#bib.bib226))
    use the weights of previous iterations for knowledge distillation instead of using
    deeper layers of the model. Kim et al. (Kim et al., [2021](#bib.bib108)) elevate
    self-distillation with a sophisticated progressive framework, incorporating adaptive
    gradient rescaling for hard example mining.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 张等人（Zhang et al., [2019b](#bib.bib247)）首创了从模型的深层到浅层的自蒸馏。他们的创新通过消除对额外网络的需求，改进了结果并减少了训练时间。类似地，侯等人（Hou
    et al., [2019](#bib.bib88)）通过注意力图利用深层的知识迁移。杨等人（Yang et al., [2019](#bib.bib226)）使用之前迭代的权重进行知识蒸馏，而不是使用模型的深层。金等人（Kim
    et al., [2021](#bib.bib108)）通过复杂的渐进框架提升了自蒸馏，融入了适应性梯度重新缩放以进行困难样本挖掘。
- en: In an important study, Yuan et al. (Yuan et al., [2020](#bib.bib240)) challenge
    the foundations of conventional KD by introducing the Teacher-free KD (Tf-KD).
    They explore the intricate relationship between KD and Label Smoothing Regularization
    (LSR) techniques and suggest employing self-training or manually-designed regularization
    terms for improving the student model’s accuracy when faced with the difficulty
    of a powerful teacher model. Additionally, self-distillation methods have successfully
    been applied to domain adaptation tasks (Yoon et al., [2022](#bib.bib235); Sultana
    et al., [2022](#bib.bib181)).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在一项重要的研究中，袁等人（Yuan et al., [2020](#bib.bib240)）通过引入无教师知识蒸馏（Tf-KD）挑战了传统知识蒸馏（KD）的基础。他们探讨了KD与标签平滑正则化（LSR）技术之间复杂的关系，并建议在面对强大的教师模型时，采用自我训练或手动设计的正则化项以提高学生模型的准确性。此外，自蒸馏方法已成功应用于领域适应任务（Yoon
    et al., [2022](#bib.bib235); Sultana et al., [2022](#bib.bib181)）。
- en: 'Table 6\. KD methods evaluated on the CIFAR-100 dataset. ↑ indicates an improvement
    over the baseline. Note: The pair of accuracies in the online distillation methods
    represent the teacher and student models’ performances after distillation.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. 在 CIFAR-100 数据集上评估的 KD 方法。↑ 表示相较于基线的改进。注意：在线蒸馏方法中的一对准确率表示蒸馏后的教师和学生模型的表现。
- en: '| Methodology | Algorithm | Teacher (baseline) | Student (baseline) | Improved
    Accuracy |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 方法论 | 算法 | 教师（基线） | 学生（基线） | 改进的准确率 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Offline distillation | SimKD (Chen et al., [2022b](#bib.bib20)) | ResNet32
    (79.42) | ResNet8 (73.09) | 78.08 (4.99 ↑) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 离线蒸馏 | SimKD (Chen et al., [2022b](#bib.bib20)) | ResNet32 (79.42) | ResNet8
    (73.09) | 78.08 (4.99 ↑) |'
- en: '| SemCKD (Chen et al., [2021c](#bib.bib21)) | ResNet32 (79.42) | ResNet8 (73.09)
    | 76.23 (3.14 ↑) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| SemCKD (Chen et al., [2021c](#bib.bib21)) | ResNet32 (79.42) | ResNet8 (73.09)
    | 76.23 (3.14 ↑) |'
- en: '| SRRL (Yang et al., [2021c](#bib.bib227)) | ResNet32 (79.42) | ResNet8 (73.09)
    | 75.39 (2.30 ↑) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| SRRL (Yang et al., [2021c](#bib.bib227)) | ResNet32 (79.42) | ResNet8 (73.09)
    | 75.39 (2.30 ↑) |'
- en: '| SemCKD (Chen et al., [2021c](#bib.bib21)) | ResNet32 (79.42) | WRN-40-2 (76.35)
    | 79.29 (2.94 ↑) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| SemCKD (Chen et al., [2021c](#bib.bib21)) | ResNet32 (79.42) | WRN-40-2 (76.35)
    | 79.29 (2.94 ↑) |'
- en: '| Online distillation | DML (Zhang et al., [2018b](#bib.bib251)) | WRN-28-10
    (78.69) | WRN-28-10 (78.69) | 80.28, 80.08 (1.39 ↑) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 在线蒸馏 | DML (Zhang et al., [2018b](#bib.bib251)) | WRN-28-10 (78.69) | WRN-28-10
    (78.69) | 80.28, 80.08 (1.39 ↑) |'
- en: '| DML (Zhang et al., [2018b](#bib.bib251)) | WRN-28-10 (78.69) | ResNet32 (68.99)
    | 78.96, 70.73 (1.74 ↑) |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| DML (Zhang et al., [2018b](#bib.bib251)) | WRN-28-10 (78.69) | ResNet32 (68.99)
    | 78.96, 70.73 (1.74 ↑) |'
- en: '| FFSD (Li et al., [2023](#bib.bib118)) | ResNet56 (71.55) | ResNet32 (69.96)
    | 75.78, 74.85 (4.90 ↑) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| FFSD (Li et al., [2023](#bib.bib118)) | ResNet56 (71.55) | ResNet32 (69.96)
    | 75.78, 74.85 (4.90 ↑) |'
- en: '| KDCL (Guo et al., [2020](#bib.bib67)) | WRN-16-2 (72.20) | ResNet32 (69.90)
    | 75.50, 74.30 (4.40 ↑) |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| KDCL (Guo et al., [2020](#bib.bib67)) | WRN-16-2 (72.20) | ResNet32 (69.90)
    | 75.50, 74.30 (4.40 ↑) |'
- en: '| Self-distillation | SD (Yang et al., [2019](#bib.bib226)) | – | ResNet32
    (68.39) | 71.29 (2.90↑) |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 自蒸馏 | SD (Yang et al., [2019](#bib.bib226)) | – | ResNet32 (68.39) | 71.29
    (2.90↑) |'
- en: '| Tf-KD (Yuan et al., [2020](#bib.bib240)) | – | ResNet18 (75.87) | 77.10 (1.23↑)
    |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Tf-KD (Yuan et al., [2020](#bib.bib240)) | – | ResNet18 (75.87) | 77.10 (1.23↑)
    |'
- en: '| PS-KD (Kim et al., [2021](#bib.bib108)) | – | ResNet18 (75.82) | 79.18 (3.36↑)
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| PS-KD (Kim et al., [2021](#bib.bib108)) | – | ResNet18 (75.82) | 79.18 (3.36↑)
    |'
- en: '| Tf-KD (Yuan et al., [2020](#bib.bib240)) | – | ShuffleNetV2 (70.34) | 72.23
    (1.89↑) |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| Tf-KD (Yuan et al., [2020](#bib.bib240)) | – | ShuffleNetV2 (70.34) | 72.23
    (1.89↑) |'
- en: 3.3.4\. Comparison of KD methods
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4\. KD 方法比较
- en: 'Table [6](#S3.T6 "Table 6 ‣ 3.3.3\. Self-Distillation ‣ 3.3\. Knowledge Distillation
    (KD) ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey") compares several distillation
    methods and analyzes their respective outcomes on the CIFAR-100 dataset. These
    findings challenge the perception that offline distillation methods are outdated
    and too simplistic. For example, SimKD recently achieved state-of-the-art performance
    with a ResNet32 as the teacher and a ResNet8 as the student. Additionally, our
    analysis demonstrates the efficacy of online distillation, showcasing instances
    where a teacher can improve its own performance despite instructing a student
    with significantly lower accuracy. Notably, the WRN-28-10 achieves a 0.27% (78.69%
    to 78.96%) improvement even when paired with a ResNet32 that initially achieves
    nearly 10% (78.69% to 68.99%) less accuracy. Furthermore, self-distillation emerges
    as a promising strategy, necessitating only one model, exemplified by a ResNet18
    achieving 3.36% gains through the PS-KD method, albeit not surpassing the improvements
    seen in other methods. To address this limitation, it is advisable to complement
    self-distillation with other forms of distillation or compression methods for
    enhanced performance. Ultimately, a comparison between methodologies is hard,
    as performance heavily depends on implementation details. Therefore, we advocate
    for adopting a strategy that is easier to implement and aligns most logically
    with the ongoing development objectives.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [6](#S3.T6 "Table 6 ‣ 3.3.3\. Self-Distillation ‣ 3.3\. Knowledge Distillation
    (KD) ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey") 比较了几种蒸馏方法，并分析了它们在 CIFAR-100
    数据集上的结果。这些发现挑战了离线蒸馏方法已经过时且过于简单的观念。例如，SimKD 最近在以 ResNet32 作为教师和 ResNet8 作为学生的情况下取得了最先进的表现。此外，我们的分析展示了在线蒸馏的有效性，展示了即使在指导一个准确率显著较低的学生的情况下，教师也可以提升自己的表现。值得注意的是，即使与最初准确度低近
    10%（78.69% 降至 68.99%）的 ResNet32 配对，WRN-28-10 仍然取得了 0.27%（78.69% 至 78.96%）的提升。此外，自我蒸馏作为一种有前景的策略，只需一个模型，例如通过
    PS-KD 方法的 ResNet18 实现了 3.36% 的提升，尽管未超过其他方法的改进。为解决这一限制，建议将自我蒸馏与其他形式的蒸馏或压缩方法结合，以提高性能。最终，方法间的比较非常困难，因为性能在很大程度上依赖于实施细节。因此，我们倡导采用更容易实施且与当前开发目标最为逻辑契合的策略。'
- en: 3.4\. Neural Architecture Search (NAS)
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 神经架构搜索（NAS）
- en: Even if DL techniques excel in numerous tasks, it is true that they often depend
    heavily on human expertise to find the best trade-off between performance and
    complexity. Optimizing a model can be exceptionally challenging due to a multitude
    of choices involving hyperparameters, network layers, hardware devices, etc.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习技术在许多任务中表现出色，但确实需要大量人类专业知识来找到性能和复杂性之间的最佳平衡。优化模型可能非常具有挑战性，因为涉及到的超参数、网络层、硬件设备等选择众多。
- en: 'In response to this challenge, Automated Machine Learning (AutoML), which aims
    to automatically build ML systems without much requirement for ML expertise and
    human intervention, is being extensively studied (He et al., [2021](#bib.bib79)).
    Several mature tools exist for AutoML applications, such as Auto-WEKA (Kotthoff
    et al., [2019](#bib.bib110)) and Auto-sklearn (Feurer et al., [2019](#bib.bib52)).
    In this paper, our primary focus is NAS, a crucial section of AutoML. The fundamental
    concepts of NAS are outlined as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这一挑战，自动化机器学习（AutoML）正在被广泛研究，其目标是自动构建机器学习系统，而不需要大量机器学习专业知识和人工干预（He 等，[2021](#bib.bib79)）。目前已经存在一些成熟的
    AutoML 工具，例如 Auto-WEKA（Kotthoff 等，[2019](#bib.bib110)）和 Auto-sklearn（Feurer 等，[2019](#bib.bib52)）。在本文中，我们主要关注
    NAS，它是 AutoML 的一个关键部分。NAS 的基本概念概述如下：
- en: •
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Search Space: The search space encompasses the possible combinations of hyperparameters,
    including kernel size, channel size, convolution stride, depth, and more. A larger
    search space that covers a wider range of possibilities increases the likelihood
    of discovering a highly accurate model. However, a vast search space can lead
    to longer search times.'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 搜索空间：搜索空间包括超参数的所有可能组合，例如内核大小、通道大小、卷积步幅、深度等。更大的搜索空间覆盖更广泛的可能性，从而增加发现高准确度模型的可能性。然而，庞大的搜索空间可能导致更长的搜索时间。
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Search Algorithm: This refers to the algorithm used to find the optimal combination
    within the search space. Common strategies include random search, grid search,
    reinforcement learning (RL) (Zoph and Le, [2017](#bib.bib258); Tan et al., [2019a](#bib.bib191)),
    evolutionary algorithms (EA) (Real et al., [2017](#bib.bib163); Xue et al., [2023](#bib.bib225)),
    and gradient optimization (Liu et al., [2019b](#bib.bib131); Wu et al., [2019](#bib.bib217)).
    An efficient search strategy can significantly reduce search time, especially
    in extensive search spaces.'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 搜索算法：这指的是用于在搜索空间中找到最佳组合的算法。常见的策略包括随机搜索、网格搜索、强化学习（RL） (Zoph and Le, [2017](#bib.bib258);
    Tan et al., [2019a](#bib.bib191))、进化算法（EA） (Real et al., [2017](#bib.bib163);
    Xue et al., [2023](#bib.bib225))和梯度优化 (Liu et al., [2019b](#bib.bib131); Wu et al.,
    [2019](#bib.bib217))。高效的搜索策略可以显著减少搜索时间，尤其是在广泛的搜索空间中。
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Performance Evaluation Strategy: This defines the criteria for selecting the
    neural architecture that maximizes specific performance metrics among all the
    models generated through NAS. Performance metrics, such as Top-1 or Top-5 scores
    for classification and average precision (AP) or F1 scores for object detection,
    reflect the suitability of the hyperparameter combinations for the given task.'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能评估策略：这定义了选择神经架构的标准，该架构在所有通过NAS生成的模型中最大化特定的性能指标。性能指标，如分类任务中的Top-1或Top-5分数，以及目标检测中的平均精度（AP）或F1分数，反映了超参数组合对给定任务的适用性。
- en: In this section, we explore various approaches in the field of NAS, including
    RL-based NAS, EA-based NAS, Gradient-based NAS, and other related works, all based
    on different search algorithms.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了NAS领域的各种方法，包括基于RL的NAS、基于EA的NAS、基于梯度的NAS及其他相关工作，这些方法都基于不同的搜索算法。
- en: 3.4.1\. RL-based NAS
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1\. 基于RL的NAS
- en: 'In this pioneering work of adopting RL for NAS, Zoph et al. (Zoph and Le, [2017](#bib.bib258))
    utilize a recurrent neural network (RNN) controller (called an agent) to generate
    candidate hyperparameters for constructing child networks (environments). The
    child network then receives a score (reward) based on metrics like accuracy and
    AP. The RNN controller updates itself according to the reward and refines the
    hyperparameters for the child network iteratively. A detailed process is illustrated
    in Fig. [7](#S3.F7 "Figure 7 ‣ 3.4.1\. RL-based NAS ‣ 3.4\. Neural Architecture
    Search (NAS) ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep
    Learning for Resource-Constrained Environments: A Survey"). Moving forward, MnasNet (Tan
    et al., [2019a](#bib.bib191)) considers latency and employs RL to identify Pareto
    optimal solutions that balance latency and performance. This approach also introduces
    a factorized hierarchical search space, which organizes the CNN into predefined
    blocks and explores different connections and operations within each block.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在采用RL进行NAS的开创性工作中，Zoph等人 (Zoph and Le, [2017](#bib.bib258))利用一个递归神经网络（RNN）控制器（称为代理）生成用于构建子网络（环境）的候选超参数。子网络随后根据诸如准确率和AP等指标获得分数（奖励）。RNN控制器根据奖励自行更新，并迭代地优化子网络的超参数。详细过程见图 [7](#S3.F7
    "图 7 ‣ 3.4.1\. 基于RL的NAS ‣ 3.4\. 神经架构搜索（NAS） ‣ 3\. 模型压缩的基本方法 ‣ 针对资源受限环境的轻量级深度学习：综述")。随后，MnasNet (Tan
    et al., [2019a](#bib.bib191))考虑了延迟，并利用RL识别在延迟和性能之间平衡的帕累托最优解。这种方法还引入了分解的分层搜索空间，将CNN组织成预定义的块，并探索每个块内的不同连接和操作。
- en: '![Refer to caption](img/46271ec97023d377983183c0c1be1aa8.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/46271ec97023d377983183c0c1be1aa8.png)'
- en: Figure 7\. NAS with RL (Zoph and Le, [2017](#bib.bib258)).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 使用RL的NAS (Zoph and Le, [2017](#bib.bib258))。
- en: 3.4.2\. EA-based NAS
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2\. 基于EA的NAS
- en: To enhance model performance, Real et al. (Real et al., [2017](#bib.bib163))
    introduce an EA-based approach for NAS. This method continuously evolves model
    architectures. The evolution process begins with workers generating an initial
    set of models, forming what is known as a population. During the evolution step,
    two models are randomly selected from the population, and their accuracy on the
    validation set is evaluated. The weaker-performing model is removed from the population,
    while the better model becomes the parent model. In the mutation step, the parent
    model is duplicated, producing two identical copies. One of these copies is reintroduced
    into the population, while the other undergoes mutation to create a new model,
    referred to as the child model. Subsequently, the workers train and assess the
    child model’s performance before adding it back to the population. This process
    is iteratively repeated, resulting in increasingly improved models within the
    population.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提升模型性能，Real 等人（Real 等人，[2017](#bib.bib163)）引入了一种基于 EA 的 NAS 方法。这种方法不断进化模型架构。进化过程从工人生成一组初始模型开始，这些模型形成了所谓的种群。在进化步骤中，从种群中随机选择两个模型，并评估它们在验证集上的准确性。表现较差的模型从种群中移除，而表现较好的模型成为父模型。在变异步骤中，父模型被复制，生成两个相同的副本。其中一个副本被重新引入种群，而另一个副本经历变异以创建一个新的模型，称为子模型。随后，工人训练并评估子模型的性能，然后将其重新加入种群。这个过程反复进行，导致种群中的模型不断改进。
- en: However, a random search approach within a large population can be highly inefficient
    when dealing with a vast search space. To address this concern, Sun et al. (Sun
    et al., [2020](#bib.bib184)) develop an encoding mechanism that maps CNN features
    to numerical values. This enables the acceleration of the evolutionary process
    by using a CNN architecture as an input to the Random Forest. More recently, Xue
    et al. (Xue et al., [2023](#bib.bib225)) proposed a queue mechanism to reduce
    the population and incorporate crossover and mutation operators to enhance the
    diversity of child networks.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当处理广泛的搜索空间时，大规模种群中的随机搜索方法可能非常低效。为了解决这一问题，Sun 等人（Sun 等人，[2020](#bib.bib184)）开发了一种编码机制，将
    CNN 特征映射到数值值。这通过将 CNN 架构作为输入到随机森林中，加速了进化过程。最近，Xue 等人（Xue 等人，[2023](#bib.bib225)）提出了一种队列机制来减少种群，并结合交叉和变异操作来增强子网络的多样性。
- en: 3.4.3\. Gradient-based NAS
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.3\. 基于梯度的 NAS
- en: The core concept of gradient-based NAS involves the transformation of a discrete
    search space into a continuous one, enabling the application of gradient descent
    techniques to discover optimal model architectures automatically. Inferring latency
    after each training is inefficient for the proposed NAS network, especially for
    research institutes with limited resources. Additionally, using gradient-based
    NAS methods is deemed more appropriate when formulating hardware-aware NAS approaches.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的 NAS 的核心概念涉及将离散的搜索空间转化为连续的，从而使得可以应用梯度下降技术自动发现最佳模型架构。在每次训练后推断延迟对于提议的 NAS
    网络效率低，尤其对于资源有限的研究机构。此外，在制定硬件感知的 NAS 方法时，使用基于梯度的 NAS 方法被认为更为合适。
- en: '![Refer to caption](img/e775615b717867552b79070685023695.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e775615b717867552b79070685023695.png)'
- en: Figure 8\. The DNAS pipeline in FBNet (Wu et al., [2019](#bib.bib217)).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. FBNet 中的 DNAS 流程（Wu 等人，[2019](#bib.bib217)）。
- en: 'DARTS (Liu et al., [2019b](#bib.bib131)) presents an efficient architecture
    search algorithm based on gradient descent that avoids black-box search problems.
    It converts structural parameters from discrete to continuous, making them differentiable.
    As a result, DARTS provides accurate, efficient, and differentiable NAS. Inspired
    by works such as MnasNet (Tan et al., [2019a](#bib.bib191)), DARTS (Liu et al.,
    [2019b](#bib.bib131)), and NetAdaptV1 (Yang et al., [2018](#bib.bib229)), FBNet (Wu
    et al., [2019](#bib.bib217)) is a hardware-aware NAS breakthrough discovered through
    the differentiable NAS (DNAS) pipeline, depicted in Fig. [8](#S3.F8 "Figure 8
    ‣ 3.4.3\. Gradient-based NAS ‣ 3.4\. Neural Architecture Search (NAS) ‣ 3\. Fundamental
    methods in model compression ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey"). In FBNet, nine distinct blocks are designed within the
    layer, and 22 layers are utilized to construct a stochastic supernetwork, which
    is optimized using stochastic gradient descent (SGD). Additionally, FBNet devises
    a layer-wise search space, enabling each layer to select a different block. Furthermore,
    in order to reduce the layer-wise search space with lower latency, a latency lookup
    table is employed, and a latency-aware loss term is incorporated into the overall
    loss function, given by:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 'DARTS (Liu et al., [2019b](#bib.bib131)) 提出了一个基于梯度下降的高效架构搜索算法，避免了黑箱搜索问题。它将结构参数从离散转换为连续，使其可微分。因此，DARTS
    提供了准确、高效且可微分的 NAS。受到 MnasNet (Tan et al., [2019a](#bib.bib191))、DARTS (Liu et
    al., [2019b](#bib.bib131)) 和 NetAdaptV1 (Yang et al., [2018](#bib.bib229)) 等工作的启发，FBNet
    (Wu et al., [2019](#bib.bib217)) 是通过可微分 NAS (DNAS) 流水线发现的一个硬件感知 NAS 突破，如图 [8](#S3.F8
    "Figure 8 ‣ 3.4.3\. Gradient-based NAS ‣ 3.4\. Neural Architecture Search (NAS)
    ‣ 3\. Fundamental methods in model compression ‣ Lightweight Deep Learning for
    Resource-Constrained Environments: A Survey") 所示。在 FBNet 中，设计了九个不同的模块，并使用了 22
    层来构建一个随机超网络，该网络通过随机梯度下降 (SGD) 进行优化。此外，FBNet 设计了逐层搜索空间，使每一层可以选择不同的模块。为了减少具有较低延迟的逐层搜索空间，还采用了延迟查找表，并将一个考虑延迟的损失项纳入到整体损失函数中，其形式为：'
- en: '| (3) |  | $L(a,w_{a})=CE(a,w_{a})\cdot\alpha\log(LAT(a))^{\beta}.$ |  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $L(a,w_{a})=CE(a,w_{a})\cdot\alpha\log(LAT(a))^{\beta}.$ |  |'
- en: where $a$ and $w_{a}$ denote the network architecture and network parameters
    for a specific device, while $CE$ represents the cross-entropy loss. $LAT$ stands
    for the latency of the architecture on the target device, which is determined
    using a lookup table. The parameters $\alpha$ and $\beta$ serve as the magnitude
    of the overall loss function and the latency term, respectively. For further details
    and related work on FBNet, please refer to (Wan et al., [2020](#bib.bib206); Dai
    et al., [2021b](#bib.bib38)).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a$ 和 $w_{a}$ 表示特定设备的网络架构和网络参数，而 $CE$ 代表交叉熵损失。$LAT$ 代表在目标设备上架构的延迟，通过查找表确定。参数
    $\alpha$ 和 $\beta$ 分别表示整体损失函数和延迟项的幅度。有关 FBNet 的更多详细信息和相关工作，请参见 (Wan et al., [2020](#bib.bib206);
    Dai et al., [2021b](#bib.bib38))。
- en: 3.4.4\. Other NAS related works
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.4\. 其他 NAS 相关工作
- en: Numerous other NAS algorithms have been proposed. One example is the Symbolic
    DNN-Tuner (Fraccaroli et al., [2022](#bib.bib54)), which introduces an automatic
    software system for determining optimal tuning actions following each network
    training session using probabilistic symbolic rules. The system comprises a module
    for data processing, search space exploration, and Bayesian optimization. The
    controller module manages the training process and decides the tuning actions.
    Besides finding the best combination from a vast search space, testing the proposed
    combination network is also time-consuming. Measuring the latency of the entire
    model on the target device each time can be highly inefficient.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他 NAS 算法也已被提出。例如，Symbolic DNN-Tuner (Fraccaroli et al., [2022](#bib.bib54))
    引入了一个自动化的软件系统，用于根据每次网络训练后的概率符号规则确定最佳调优操作。该系统包括一个数据处理模块、搜索空间探索模块和贝叶斯优化模块。控制模块负责管理训练过程并决定调优操作。除了在广泛的搜索空间中找到最佳组合外，测试提出的组合网络也非常耗时。每次在目标设备上测量整个模型的延迟可能效率极低。
- en: 'To address this issue, NetAdaptV1 (Yang et al., [2018](#bib.bib229)) employs
    an adaptive algorithm that considers energy consumption and memory usage, enabling
    it to respond more realistically to hardware constraints. The approach involves
    the creation of a layer-wise lookup table, as shown in Fig. [9](#S3.F9 "Figure
    9 ‣ 3.4.4\. Other NAS related works ‣ 3.4\. Neural Architecture Search (NAS) ‣
    3\. Fundamental methods in model compression ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey"), simplifying the search complexity for a pre-trained
    network. In this setup, the latency of each layer is pre-measured, and a lookup
    table is constructed to record latency based on the layer’s structure. For instance,
    as illustrated in Fig. [9](#S3.F9 "Figure 9 ‣ 3.4.4\. Other NAS related works
    ‣ 3.4\. Neural Architecture Search (NAS) ‣ 3\. Fundamental methods in model compression
    ‣ Lightweight Deep Learning for Resource-Constrained Environments: A Survey"),
    Layer 1 consists of 3 channels with 4 filters and a measured latency of 6 ms,
    and Layer 2 consists of 4 channels with 6 filters and a measured latency of 4
    ms. The total latency is calculated as the sum of the latency for each layer,
    resulting in a total latency of 10 ms (6 + 4).'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一问题，NetAdaptV1 （Yang 等，[2018](#bib.bib229)）采用了一种自适应算法，考虑了能耗和内存使用，使其能更现实地响应硬件限制。该方法涉及创建一个层级查找表，如图 [9](#S3.F9
    "图 9 ‣ 3.4.4\. 其他 NAS 相关工作 ‣ 3.4\. 神经架构搜索（NAS） ‣ 3\. 模型压缩的基本方法 ‣ 面向资源受限环境的轻量级深度学习：综述")
    所示，简化了对预训练网络的搜索复杂性。在此设置中，每层的延迟被预先测量，并构建查找表以记录基于层结构的延迟。例如，如图 [9](#S3.F9 "图 9 ‣
    3.4.4\. 其他 NAS 相关工作 ‣ 3.4\. 神经架构搜索（NAS） ‣ 3\. 模型压缩的基本方法 ‣ 面向资源受限环境的轻量级深度学习：综述")
    所示，第 1 层由 3 个通道和 4 个滤波器组成，测得的延迟为 6 毫秒，第 2 层由 4 个通道和 6 个滤波器组成，测得的延迟为 4 毫秒。总延迟计算为各层延迟之和，得到总延迟为
    10 毫秒（6 + 4）。
- en: Moving forward, NetAdaptV2 (Yang et al., [2021b](#bib.bib230)) introduces Channel-Level
    Bypass Connections (CBCs), which combine depth and layer width in the original
    search space to enhance the efficiency of both training and testing. Moreover,
    Abdelfattah et al. (Abdelfattah et al., [2021](#bib.bib3)) leverages pruning-at-initialization (Lee
    et al., [2019](#bib.bib115)) and incorporates six zero-cost proxies for NAS proposal
    scoring. This innovative approach requires only a single minibatch of data and
    a single forward/backward propagation pass instead of full training, resulting
    in a more efficient NAS process.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 向前推进，NetAdaptV2 （Yang 等，[2021b](#bib.bib230)）引入了通道级跳过连接（CBCs），它将深度和层宽结合在原始搜索空间中，以提高训练和测试的效率。此外，Abdelfattah
    等（Abdelfattah 等，[2021](#bib.bib3)）利用了初始化修剪（Lee 等，[2019](#bib.bib115)）并整合了六个零成本代理用于
    NAS 提案评分。这一创新方法只需一个小批量数据和一次前向/反向传播过程，而无需完全训练，从而使 NAS 过程更加高效。
- en: '![Refer to caption](img/a9465c85b0cad7680677a0de1cf8d7d7.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a9465c85b0cad7680677a0de1cf8d7d7.png)'
- en: Figure 9\. Layer-wise look up table (Yang et al., [2018](#bib.bib229)).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 层级查找表 （Yang 等，[2018](#bib.bib229)）。
- en: 3.5\. Discussion and Summary
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 讨论与总结
- en: This section encapsulates a summary of the preceding discussion on model compression.
    Additionally, it provides valuable practical tips and guidance, aiming to offer
    actionable insights for effective implementation and application in relevant contexts.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 本节总结了前述关于模型压缩的讨论。此外，它提供了有价值的实践技巧和指导，旨在为相关背景下的有效实施和应用提供可操作的见解。
- en: Pruning. Although unstructured pruning methods (LeCun et al., [1989](#bib.bib114);
    Frankle and Carbin, [2019](#bib.bib55)) have made significant strides in parameter
    reduction, their irregular structures frequently pose compatibility issues with
    hardware accelerators. Therefore, structure pruning (He et al., [2019a](#bib.bib82),
    [2018](#bib.bib81); Hu et al., [2023](#bib.bib93)) has emerged as a preferable
    alternative, primarily due to its regular structure. Notably, modern DL frameworks,
    such as PyTorch and TensorFlow, have integrated built-in functionalities that
    facilitate the seamless implementation of structure pruning. This streamlined
    integration enhances the ease and efficiency with which structure pruning techniques
    can be applied.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝。尽管非结构化剪枝方法（LeCun 等，[1989](#bib.bib114)；Frankle 和 Carbin，[2019](#bib.bib55)）在参数减少方面取得了显著进展，但其不规则的结构经常会与硬件加速器兼容性存在问题。因此，结构化剪枝（He
    等，[2019a](#bib.bib82)，[2018](#bib.bib81)；Hu 等，[2023](#bib.bib93)）作为一种更可取的替代方案出现，主要是由于其规则的结构。值得注意的是，现代深度学习框架，如
    PyTorch 和 TensorFlow，已经集成了内置功能，便于顺利实现结构化剪枝。这种简化的集成提升了结构化剪枝技术应用的便捷性和效率。
- en: Quantization. When considering quantization, the choice of technique depends
    on the hardware environment where the model will be deployed. Hardware specifications
    play a critical role, turning quantization from an optional optimization into
    an imperative requirement. For instance, specific MCUs or edge TPUs exclusively
    support integer operations, making full integer quantization essential for model
    implementation. TensorFlow Lite (TF-Lite) (Google, [2023](#bib.bib62)) effectively
    addresses this need, reducing the model size by up to four times and significantly
    accelerating inference by more than three times. In hardware with low-power CPUs,
    an 8-bit integer quantization strategy is often recommended, as CPUs exhibit exceptional
    computational efficiency when handling integer operations instead of floating-point
    values. Notably, when using 16-bit float quantization, values are subsequently
    de-quantized back to 32-bit float representations during execution on the CPU.
    For a deeper analysis of hardware support for quantization and facilitating libraries,
    see (Liang et al., [2021b](#bib.bib122)).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 量化。在考虑量化时，所选技术取决于模型将要部署的硬件环境。硬件规格发挥着关键作用，将量化从一个可选的优化转变为一个必不可少的要求。例如，特定的 MCU
    或边缘 TPU 仅支持整数操作，使得完整的整数量化对于模型实现至关重要。TensorFlow Lite (TF-Lite)（Google，[2023](#bib.bib62)）有效地满足了这一需求，将模型大小减少了最多四倍，并显著加速了推理速度，提升了三倍以上。在低功耗
    CPU 的硬件中，通常推荐使用 8 位整数量化策略，因为 CPU 在处理整数操作时表现出卓越的计算效率，而非浮点值。值得注意的是，在使用 16 位浮点量化时，数值在
    CPU 执行期间会被重新量化回 32 位浮点表示。有关硬件对量化的支持和辅助库的深入分析，请参见（Liang 等，[2021b](#bib.bib122)）。
- en: Knowledge distillation. KD techniques have significantly enhanced NNs by leveraging
    insights from other models. In practice, the offline KD process (Hinton et al.,
    [2015](#bib.bib86)) can be effectively utilized when training a large model is
    viable. On the other hand, online distillation stands forth as a promising solution.
    For example, the DML process (Zhang et al., [2018b](#bib.bib251)) has shown remarkable
    results without necessitating a pre-trained teacher model, making it adaptable
    to multi-GPU training with several small models. In situations characterized by
    a scarcity of labeled data or noisy labels, the mean teacher framework has emerged
    as a valuable and effective solution. Moreover, self-distillation and ongoing
    advancements in KD (Lin et al., [2022b](#bib.bib127); Zhao et al., [2022](#bib.bib253))
    open numerous possibilities for exploration and offer different options for the
    definition of the teacher and student networks.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏。KD 技术通过利用其他模型的洞察力显著增强了神经网络。在实际应用中，当训练大型模型可行时，可以有效地利用离线 KD 过程（Hinton 等，[2015](#bib.bib86)）。另一方面，在线蒸馏作为一种有前途的解决方案脱颖而出。例如，DML
    过程（Zhang 等，[2018b](#bib.bib251)）在没有预训练教师模型的情况下显示出了显著的效果，使其能够适应使用多个小模型的多 GPU 训练。在标记数据稀缺或标签噪声的情况下，均值教师框架作为一种有价值且有效的解决方案脱颖而出。此外，自蒸馏和
    KD 的持续进展（Lin 等，[2022b](#bib.bib127)；Zhao 等，[2022](#bib.bib253)）为探索提供了诸多可能性，并为教师和学生网络的定义提供了不同的选项。
- en: NAS. While both RL-based NAS (Tan et al., [2019a](#bib.bib191)) and EA-based
    NAS (Sun et al., [2020](#bib.bib184)) have demonstrated their capacity to achieve
    impressive accuracy, it is important to note that their training demands extensive
    resources and time, often spanning days or weeks and involving hundreds of GPUs.
    This resource-intensive nature has contributed to a relative decline in the number
    of studies in these areas. Therefore, when confronted with GPU limitations, gradient-based
    algorithms like DARTS (Liu et al., [2019b](#bib.bib131)) and FBNet (Wu et al.,
    [2019](#bib.bib217)), which introduce continuity into the search space, can be
    considered. This approach significantly reduces the training time. Alternative
    options include approaches like ”once for all” NAS (Cai et al., [2020a](#bib.bib14)),
    which tailor the extensive network into subnetworks optimized for different target
    devices. However, if ample computational resources are at hand, RL-based and EA-based
    NAS methods are viable options, and they also offer superior performance compared
    to gradient-based NAS (Ren et al., [2021](#bib.bib164)). Additionally, when memory
    footprint, energy consumption, and latency are key considerations, the hardware-aware
    NAS concepts introduced by studies like FBNet (Wu et al., [2019](#bib.bib217)),
    NetAdapt (Yang et al., [2018](#bib.bib229)), and NetAdaptV2 (Yang et al., [2021b](#bib.bib230))
    may be particularly relevant.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: NAS。虽然基于 RL 的 NAS (Tan et al., [2019a](#bib.bib191)) 和基于 EA 的 NAS (Sun et al.,
    [2020](#bib.bib184)) 都展示了其实现令人印象深刻的准确性的能力，但需要注意的是，它们的训练要求大量的资源和时间，通常需要数天或数周，并涉及数百个
    GPU。这种资源密集型的特性导致了这些领域研究数量的相对下降。因此，当面临 GPU 限制时，可以考虑像 DARTS (Liu et al., [2019b](#bib.bib131))
    和 FBNet (Wu et al., [2019](#bib.bib217)) 这样的基于梯度的算法，这些算法引入了搜索空间的连续性，从而显著减少了训练时间。另一种选择是“once
    for all” NAS (Cai et al., [2020a](#bib.bib14))，它将广泛的网络优化为针对不同目标设备的子网络。然而，如果计算资源充足，基于
    RL 和 EA 的 NAS 方法仍然是可行的选项，并且它们相比基于梯度的 NAS (Ren et al., [2021](#bib.bib164)) 也提供了更优的性能。此外，当内存占用、能源消耗和延迟是关键考虑因素时，像
    FBNet (Wu et al., [2019](#bib.bib217))、NetAdapt (Yang et al., [2018](#bib.bib229))
    和 NetAdaptV2 (Yang et al., [2021b](#bib.bib230)) 等研究引入的硬件感知 NAS 概念可能特别相关。
- en: Conclusion. In conclusion, model compression approaches have their strengths
    and limitations. Quantization is a relatively simple but proven effective compression
    technique in many cases. It is essential to first match the selected quantization
    approach with the specific hardware requirements for floating-point or integer
    values. In scenarios where hardware constraints permit, starting with a 16-bit
    float quantization is often a prudent initial step. If there is a need for more
    substantial model compression, two viable options emerge. First, model pruning
    offers an effective solution, substantially reducing redundant network parameters
    while preserving performance integrity. This is particularly valuable when working
    with resource-constrained environments. Secondly, the KD framework proves advantageous,
    especially in scenarios with ample unlabeled data, as often encountered in applications
    like autonomous driving. The mean teacher structure, in particular, is a valuable
    tool for generating pseudo labels from unlabeled data, effectively incorporating
    this additional information into training and enhancing overall model performance.
    Finally, NAS can also be considered, particularly for tasks where it excels the
    most, such as image classification, where it can potentially discover optimal
    network architectures tailored to specific requirements. The choice among these
    approaches should be guided by the specific demands of the task and the available
    computational resources.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 结论。总之，模型压缩方法各有其优缺点。量化是一种相对简单但在许多情况下被证明有效的压缩技术。首先，需要将选定的量化方法与浮点或整数值的具体硬件要求匹配。在硬件条件允许的情况下，通常从
    16 位浮点量化开始是一个明智的初步步骤。如果需要更大程度的模型压缩，有两个可行的选项。首先，模型剪枝提供了一种有效的解决方案，可以显著减少冗余的网络参数，同时保持性能完整性。这在处理资源受限的环境时特别有价值。其次，KD
    框架在有大量未标记数据的情况下尤为有效，如自主驾驶等应用场景中经常遇到的情况。特别是均值教师结构是从未标记数据生成伪标签的有用工具，能够有效地将这些额外信息纳入训练，提升整体模型性能。最后，NAS
    也可以考虑，尤其是在图像分类等任务中，它可以发现适合特定要求的最优网络架构。这些方法的选择应根据任务的具体需求和可用的计算资源来指导。
- en: 4\. Hardware Acceleration of Deep Learning Models
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 深度学习模型的硬件加速
- en: With the advancements in GPUs, DL has risen to the forefront of artificial intelligence
    technology. DL models, such as CNNs, are computationally intensive. Hence, hardware
    acceleration is becoming imperative to render DL applications feasible and practical.
    In this section, we present an overview of prominent hardware accelerators of
    DL models. We then introduce typical dataflow and data locality optimization techniques,
    as well as widely adopted DL libraries. Finally, we discuss algorithms that employ
    a co-design approach for software/hardware deployment.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 GPU 技术的进步，深度学习（DL）已成为人工智能技术的前沿。深度学习模型，如卷积神经网络（CNN），计算密集型，因此硬件加速变得至关重要，以使深度学习应用变得可行和实用。在这一部分，我们将概述深度学习模型的主要硬件加速器。接着，我们介绍典型的数据流和数据局部性优化技术，以及广泛使用的深度学习库。最后，我们讨论采用软件/硬件共设计方法的算法。
- en: 4.1\. Hardware Architectures
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 硬件架构
- en: Hardware accelerators for DL models encompass a range of options, including
    GPUs and CPUs based on temporal architecture, as well as FPGAs and ASICs rooted
    in spatial architecture. The basic components of a hardware accelerator are an
    arithmetic logic unit (ALU), a control unit, and a local memory unit (cache unit).
    In the temporal architecture, the control and local memory units are centralized,
    and the processing elements (PEs) only contain the ALUs. Data is accessed sequentially
    from centralized memory to PEs, with no interactions between the PEs (Capra et al.,
    [2020](#bib.bib17)). In contrast, spatial architecture entails PEs equipped with
    control units, ALUs, and local memory (register file). This allows independent
    data processing and direct communication between PEs.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的硬件加速器包括一系列选项，包括基于时序架构的 GPU 和 CPU 以及根植于空间架构的 FPGA 和 ASIC。硬件加速器的基本组件是算术逻辑单元（ALU）、控制单元和局部存储单元（缓存单元）。在时序架构中，控制单元和局部存储单元是集中的，处理元素（PEs）只包含
    ALU。数据从集中式内存顺序访问到 PEs，PEs 之间没有交互（Capra et al., [2020](#bib.bib17)）。相比之下，空间架构包括配备了控制单元、ALU
    和局部存储（寄存器文件）的 PEs。这允许独立的数据处理和 PEs 之间的直接通信。
- en: 4.1.1\. Temporal Architecture
  id: totrans-314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 时序架构
- en: Temporal architectures are often adopted in general-purpose platforms, like
    CPUs and GPUs, which are optimized for sequential tasks and parallel tasks, respectively.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 时序架构通常用于通用平台，如 CPU 和 GPU，分别优化为处理串行任务和并行任务。
- en: Central processing unit (CPU). CPUs process input data into usable information
    output, executing calculations sequentially through serial computing. A recent
    CPU-based acceleration technique, SLIDE (Chen et al., [2020a](#bib.bib18)), which
    leverages C++ OpenMP to combine intelligent randomized algorithms with multi-core
    parallelism and workload optimization, demonstrates that employing smart algorithms
    on a CPU can potentially achieve better speed than using an NVIDIA-V100 GPU.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 中央处理单元（CPU）。CPU 将输入数据处理成可用的信息输出，通过串行计算执行计算。最近的一种基于 CPU 的加速技术 SLIDE（Chen et al.,
    [2020a](#bib.bib18)），利用 C++ OpenMP 结合智能随机算法与多核并行性和工作负载优化，证明在 CPU 上使用智能算法可以比使用
    NVIDIA-V100 GPU 实现更好的速度。
- en: Graphics processing unit (GPU). GPUs are designed for parallel computation.
    Their architecture may consist of thousands of cores. Hence, GPUs excel at parallel
    computing, enabling them to process multiple instructions simultaneously, making
    them highly efficient for tasks that involve simple and repetitive computations.
    Given that DL models often entail extensive matrix addition and multiplication
    operations, GPUs have emerged as the primary accelerators for the development
    of DL. Their parallel processing capabilities make them instrumental in accelerating
    DL tasks.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图形处理单元（GPU）。GPU 设计用于并行计算。它们的架构可能由数千个核心组成。因此，GPU 擅长并行计算，使其能够同时处理多个指令，极大地提高了处理简单和重复计算任务的效率。鉴于深度学习模型通常涉及大量的矩阵加法和乘法运算，GPU
    已成为深度学习发展的主要加速器。它们的并行处理能力使其在加速深度学习任务中发挥了重要作用。
- en: 4.1.2\. Spatial Architecture
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 空间架构
- en: By utilizing PEs, spatial architectures often seen in FPGAs and application-specific
    integrated circuits (ASICs), the necessity for repeated and redundant access to
    external memory is reduced, leading to lower energy consumption.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用处理元素（PEs），空间架构（通常见于 FPGA 和应用专用集成电路（ASICs））减少了对外部内存的重复和冗余访问，从而降低了能耗。
- en: FPGAs. FGPAs consist of programmable logic blocks with logic gates capable of
    performing computations. Reprogrammable by nature, they can accelerate various
    DL structures effectively and better support pruning methods. Additionally, FPGAs
    can directly implement algorithms without any decoding and interpretation process.
    To enhance AI applications using FPGAs, Qi et al. (Qi et al., [2022](#bib.bib157))
    emphasize key concepts of parallel computing and demonstrate how these concepts
    can be implemented in FPGAs. Roggen et al. (Roggen et al., [2022](#bib.bib165))
    successfully implement digital signal processing (DSP) algorithms, such as filter
    finite impulse response filters on FPGA platforms, thereby improving support for
    wearable computing. For more references on FPGA AI applications, consult (Nechi
    et al., [2023](#bib.bib150); Seng et al., [2021](#bib.bib172)).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: FPGAs。FPGAs由可编程逻辑块和逻辑门组成，能够执行计算。由于其可重新编程的特性，它们能够有效加速各种深度学习结构，并更好地支持剪枝方法。此外，FPGAs可以直接实现算法，无需解码和解释过程。为了提升使用FPGAs的AI应用，Qi等（Qi等，[2022](#bib.bib157)）强调了并行计算的关键概念，并展示了如何在FPGAs中实现这些概念。Roggen等（Roggen等，[2022](#bib.bib165)）成功在FPGA平台上实现了数字信号处理（DSP）算法，如滤波有限脉冲响应滤波器，从而改善了对可穿戴计算的支持。有关FPGA
    AI应用的更多参考，请查阅（Nechi等，[2023](#bib.bib150); Seng等，[2021](#bib.bib172)）。
- en: ASICs. ASICs, customized for specific electronic systems, outperform FPGAs with
    superior speed, lower power consumption, and higher throughput. TPUs, prominent
    ASICs tailored for AI applications (Jouppi et al., [2017](#bib.bib104)), excel
    in efficiently executing matrix operations, a pivotal capability advantageous
    in deep learning computations with prevalent expansive matrix multiplications.
    In a recent development, the newly introduced TPU-v3 can connect 1024 TPU chips
    through a 2-D torus network (Kumar et al., [2019](#bib.bib112)). This innovation
    enhances parallelism and enables execution on more TPU-v3 accelerator cores through
    spatial partitioning and weight update-sharing mechanisms. The supercomputer TPU-v4 (Jouppi
    et al., [2023](#bib.bib103)) further elevates the capabilities by increasing the
    number of TPU chips to 4096\. TPU-v4 also introduces optical circuit switches
    (OCSes) that dynamically restructure their interconnection topology to improve
    scalability, accessibility, and utilization. As a result, TPU-v4 offers a 2.7
    times improvement in performance/watt and a tenfold increase in speed compared
    to TPU-v3.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ASICs。定制化的ASICs针对特定电子系统，相较于FPGAs在速度、更低的功耗和更高的吞吐量方面表现更优。TPUs是专为AI应用设计的突出ASICs（Jouppi等，[2017](#bib.bib104)），在高效执行矩阵运算方面表现出色，这在深度学习计算中尤为重要，尤其是广泛的矩阵乘法。在最近的发展中，新推出的TPU-v3可以通过2-D环形网络连接1024个TPU芯片（Kumar等，[2019](#bib.bib112)）。这一创新提高了并行性，并通过空间分区和权重更新共享机制，允许在更多的TPU-v3加速器核心上执行。超级计算机TPU-v4（Jouppi等，[2023](#bib.bib103)）通过将TPU芯片数量增加到4096进一步提升了性能。TPU-v4还引入了光学电路开关（OCSes），这些开关可以动态重组其互连拓扑，以提高可扩展性、可访问性和利用率。因此，TPU-v4在性能/瓦特上提高了2.7倍，并且相较于TPU-v3速度提升了十倍。
- en: 4.1.3\. Discussion of CNN Accelerators
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3。CNN加速器的讨论
- en: CPUs are generally not well-suited for training and inference of typical DL
    models due to low FLOPs performance. GPUs, which can support parallel computation
    with thousands of cores, excel in parallel computing and are widely adopted in
    various AI applications. However, GPUs are known for their high power consumption,
    rendering them unsuitable for edge devices and IoT applications. On the other
    hand, FPGAs and ASICs offer more energy-efficient acceleration options for edge
    AI applications. The choice between FPGAs and ASICs often depends on the specific
    requirements. FPGAs are preferred for AI products that require rapid development
    or are produced in small batches. ASICs are more suitable for AI products that
    undergo mass production, especially highly mature or customized ones. For projects
    with ample budget, TPUs can be the top choice. TPUs boast exceptional computational
    power, making them ideal for handling extensive models with large batch sizes,
    such as the GPT-4 (OpenAI, [2023](#bib.bib152)) and LLaMA (Touvron et al., [2023a](#bib.bib200)),
    significantly reducing training and inference times.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: CPUs通常不适合训练和推断典型的DL模型，因为FLOPs性能较低。 GPU可以支持成千上万个核心的并行计算，在并行计算方面表现出色，并被广泛应用于各种人工智能应用。但是，GPU以高功耗而闻名，使其不适用于边缘设备和物联网应用。另一方面，FPGA和ASIC为边缘AI应用提供了更节能的加速选项。在FPGA和ASIC之间的选择通常取决于具体要求。对于需要快速开发或批量生产的AI产品，更倾向于选择FPGA。而对于大规模生产的AI产品，尤其是高度成熟或定制的产品，ASIC更为合适。对于预算充裕的项目，TPU可能是顶级选择。TPU具有卓越的计算能力，使其非常适合处理具有大批量数据的广泛模型，例如GPT-4 (OpenAI,
    [2023](#bib.bib152))和LLaMA (Touvron et al., [2023a](#bib.bib200))，显著缩短了训练和推断时间。
- en: 4.2\. Dataflow and the Data Locality Optimization
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 数据流和数据本地化优化
- en: The computational complexity and data storage demands of CNNs pose significant
    challenges to computational performance and energy efficiency. These challenges
    are particularly pronounced in smaller devices with limited memory, including
    constrained on-chip buffers (SRAM) and off-chip memory (DRAM). To address these
    issues, optimizing dataflow is crucial for enhancing memory and energy efficiency.
    The dataflow process in deep models generally consists of three main steps. Firstly,
    DL models are stored in off-chip memory, often referred to as external memory.
    Secondly, when convolution kernels are required, they are fetched from on-chip
    buffers. Finally, PEs are employed to execute the MACs.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的计算复杂性和数据存储需求对计算性能和能源效率构成重大挑战。这些挑战在具有有限内存的较小设备中尤为突出，包括受限的芯片上缓冲区（SRAM）和片外存储器（DRAM）。为了解决这些问题，优化数据流对增强内存和能源效率至关重要。深度模型的数据流过程通常包括三个主要步骤。首先，DL模型存储在片外存储器中，通常称为外部存储器。其次，当需要卷积内核时，它们会从芯片上的缓冲区中获取。最后，使用PE执行乘加运算。
- en: 4.2.1\. Dataflow types
  id: totrans-326
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 数据流类型
- en: Hardware accelerators of DL models have different types of dataflow based on
    their applications and can be categorized into pipeline-like dataflow (Li et al.,
    [2016](#bib.bib117); Lin and Chang, [2017](#bib.bib129)), DaDianNao-like dataflow (Luo
    et al., [2016](#bib.bib138); Chen et al., [2014b](#bib.bib29)), Systolic-array-like
    dataflow (Jouppi et al., [2017](#bib.bib104); Wei et al., [2017](#bib.bib214);
    Zhang et al., [2018a](#bib.bib243)), and streaming-like dataflow (Du et al., [2017](#bib.bib46);
    Guo et al., [2017](#bib.bib66)).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 基于其应用，DL模型的硬件加速器具有不同类型的数据流，可分为类似流水线的数据流 (Li et al., [2016](#bib.bib117); Lin
    and Chang, [2017](#bib.bib129))、类似DaDianNao的数据流 (Luo et al., [2016](#bib.bib138);
    Chen et al., [2014b](#bib.bib29))、类似Systolic-array的数据流 (Jouppi et al., [2017](#bib.bib104);
    Wei et al., [2017](#bib.bib214); Zhang et al., [2018a](#bib.bib243))以及类似流式的数据流 (Du
    et al., [2017](#bib.bib46); Guo et al., [2017](#bib.bib66))。
- en: Pipeline-like dataflow. In this dataflow, the input pixels (the pixels of the
    feature map) are passed on to individual PEs, and the model’s weights (representing
    model parameters) are fixed on each PE. Notably, the partial sum is then forwarded
    to the subsequent PE. This approach offers substantial parallelism, facilitating
    the concurrent processing of data by multiple stages, thereby enhancing computational
    efficiency. However, tasks are executed sequentially, with each stage dependent
    on the completion of the previous one, potentially resulting in increased latency.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 类流水线数据流。在这种数据流中，输入像素（特征图的像素）被传递给各个 PE，模型的权重（表示模型参数）固定在每个 PE 上。值得注意的是，部分和随后被转发到下一个
    PE。这种方法提供了显著的并行性，便于多个阶段同时处理数据，从而提高计算效率。然而，任务是顺序执行的，每个阶段依赖于前一个阶段的完成，这可能导致延迟增加。
- en: DaDianNao-like dataflow. In this dataflow, each PE can function like a neuron,
    processing input pixels in a way akin to an NN. Specifically, input pixels are
    routed to each PE, and the model’s weights are embedded within each PE. The computed
    partial sums are then aggregated using an adder tree. This type of dataflow can
    accommodate different kernel sizes, making it capable of handling intricate and
    irregular model structures. However, this dataflow approach is energy-intensive
    and demands substantial hardware resources due to the model’s complexity.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 类 DaDianNao 数据流。在这种数据流中，每个 PE 可以像神经元一样工作，以类似于 NN 的方式处理输入像素。具体而言，输入像素被路由到每个 PE，模型的权重嵌入在每个
    PE 中。计算出的部分和随后通过累加器树进行聚合。这种数据流可以适应不同的内核大小，使其能够处理复杂且不规则的模型结构。然而，这种数据流方法能耗高，因模型复杂性要求大量硬件资源。
- en: Systolic-array-like dataflow. This dataflow sequentially conveys input pixels
    and weights into the PEs, with PEs cascaded to enhance computational efficiency.
    Subsequently, an adder tree is employed to aggregate the partial sums. This dataflow
    approach optimizes the utilization of hardware resources, improves overall hardware
    efficiency, and mitigates timing issues in large designs. However, finding an
    appropriate mapping for CNNs onto a systolic array can be challenging.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 类膨胀数组数据流。这种数据流将输入像素和权重依次传递到 PE，PE 链接以提高计算效率。随后，使用累加器树聚合部分和。这种数据流方法优化了硬件资源的利用，提升了整体硬件效率，并减轻了大设计中的时序问题。然而，为
    CNN 寻找合适的映射到膨胀数组上可能具有挑战性。
- en: 'Streaming-like dataflow. In this dataflow, input pixels are continuously sent
    to the following PE without pausing or needing intermediate storage, with weights
    being fixed on each PE. Subsequently, the adder tree accumulates the partial sums.
    This dataflow is particularly suitable for streaming data, such as audio and video
    processing, due to its high throughput and low latency. Nonetheless, applications
    requiring complex operations between stages or that rely on previous results may
    require additional processing and design. Fig. [10](#S4.F10 "Figure 10 ‣ 4.2.1\.
    Dataflow types ‣ 4.2\. Dataflow and the Data Locality Optimization ‣ 4\. Hardware
    Acceleration of Deep Learning Models ‣ Lightweight Deep Learning for Resource-Constrained
    Environments: A Survey") compares the types of dataflow.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '类流式数据流。在这种数据流中，输入像素不断地传送到下一个 PE，不会暂停或需要中间存储，权重在每个 PE 上是固定的。随后，累加器树累积部分和。这种数据流特别适合流式数据，如音频和视频处理，因为其高吞吐量和低延迟。然而，要求阶段间复杂操作或依赖于先前结果的应用可能需要额外的处理和设计。图
    [10](#S4.F10 "Figure 10 ‣ 4.2.1\. Dataflow types ‣ 4.2\. Dataflow and the Data
    Locality Optimization ‣ 4\. Hardware Acceleration of Deep Learning Models ‣ Lightweight
    Deep Learning for Resource-Constrained Environments: A Survey") 比较了数据流类型。'
- en: '![Refer to caption](img/a3d802ce2320e2b6adf0a1967c567bd1.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a3d802ce2320e2b6adf0a1967c567bd1.png)'
- en: Figure 10\. A comparison of dataflow types (Hsu et al., [2020](#bib.bib91)).
    PE stands for processing element.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 数据流类型比较（Hsu 等，[2020](#bib.bib91)）。PE 代表处理元素。
- en: 4.2.2\. Data Locality Optimization
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 数据局部性优化
- en: CNNs deliver exceptional performance characterized by high throughput and energy
    consumption. However, their performance can be restricted by limited on-chip memory.
    Therefore, an effective locality optimization mechanism is essential. Data locality
    optimization focuses on devising a dataflow schedule that maximizes data reuse
    utilization and minimizes data movement. A prevalent approach involves applying
    loop transformation techniques, such as loop unrolling, loop tiling, and loop
    interchange, to optimize NN deployment. These techniques help maximize hardware
    utilization and minimize memory traffic, addressing the limitations of on-chip
    memory constraints.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 提供了卓越的性能，具有高吞吐量和能耗特征。然而，它们的性能可能受到有限的片上内存的限制。因此，有效的局部性优化机制至关重要。数据局部性优化专注于制定数据流调度，以最大化数据重用利用率并最小化数据移动。一种常见的方法是应用循环变换技术，如循环展开、循环分块和循环交互，以优化
    NN 部署。这些技术有助于最大化硬件利用率并最小化内存流量，从而解决片上内存限制的问题。
- en: Loop unrolling (Booshehri et al., [2013](#bib.bib13); Huang and Leng, [1999](#bib.bib96))
    is a method that involves expanding loop iterations into multiple sequential instructions.
    This technique significantly reduces the number of loop iterations in the CNN,
    resulting in faster CNN operations and improved hardware utilization through increased
    parallelization. However, it is important to note that loop unrolling may lead
    to code bloat, increased memory usage, and higher storage requirements, especially
    for larger CNN models.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 循环展开（Booshehri 等，[2013](#bib.bib13)；Huang 和 Leng，[1999](#bib.bib96)）是一种将循环迭代扩展为多个顺序指令的方法。该技术显著减少了
    CNN 中的循环迭代次数，从而加快了 CNN 操作并通过增加并行化提高了硬件利用率。然而，需要注意的是，循环展开可能导致代码膨胀、内存使用增加以及存储需求提高，特别是对于较大的
    CNN 模型。
- en: Loop tilling (Qiu et al., [2016](#bib.bib158); Zhang et al., [2015](#bib.bib242);
    Stoutchinin et al., [2019](#bib.bib178)) involves partitioning the input data
    into several blocks to enable parallel computations for CNN acceleration. For
    example, an original input data of size $224\times 224\times 3$ can be divided
    into smaller blocks of size $112\times 112\times 3$. These smaller blocks are
    processed sequentially to mitigate buffer loading and memory constraints. This
    technique effectively adapts to limited on-chip memory and significantly enhances
    cache locality. However, for modern accelerators, such as GPUs, where memory access
    patterns are already optimized for high throughput, loop tilling may add extra
    complexity without appreciable gains in performance.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 循环分块（Qiu 等，[2016](#bib.bib158)；Zhang 等，[2015](#bib.bib242)；Stoutchinin 等，[2019](#bib.bib178)）涉及将输入数据划分为多个块，以便进行并行计算，从而加速
    CNN。例如，原始输入数据的大小为 $224\times 224\times 3$，可以被划分为大小为 $112\times 112\times 3$ 的较小块。这些较小的块被顺序处理，以缓解缓冲区加载和内存限制。此技术有效适应有限的片上内存，并显著提升缓存局部性。然而，对于现代加速器，如
    GPU，其内存访问模式已经优化以实现高吞吐量，循环分块可能会增加额外的复杂性，而性能提升不明显。
- en: Loop interchange (Mezdour et al., [2023](#bib.bib147); Xu et al., [2023](#bib.bib224))
    involves changing the order of loops within a nested loop with the aim of improving
    data locality and extracting parallelism. Specifically, the order of the loops
    is optimized to allow each iteration of the outermost loop to utilize the same
    cache line, hence reducing memory access. Loop interchange can also accelerate
    CNN models by increasing the use of operators like addition and multiplication.
    Notably, some algorithms have complex intrinsic properties and special meanings
    in their loop orders. Therefore, altering the loop order may yield meaningless
    results and reduce performance.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 循环交互（Mezdour 等，[2023](#bib.bib147)；Xu 等，[2023](#bib.bib224)）涉及改变嵌套循环内循环的顺序，目的是提高数据局部性并提取并行性。具体而言，循环的顺序被优化，以使外层循环的每次迭代利用相同的缓存行，从而减少内存访问。循环交互还可以通过增加加法和乘法等操作符的使用来加速
    CNN 模型。值得注意的是，一些算法具有复杂的内在属性和特殊的循环顺序含义。因此，改变循环顺序可能会产生无意义的结果并降低性能。
- en: In this section, we introduce typical types of dataflow and provide an overview
    of various mechanisms for data locality optimization. More in-depth details can
    be found in  (Fu et al., [2023](#bib.bib57); Wolf and Lam, [1991](#bib.bib215)).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了典型的数据流类型，并概述了各种数据局部性优化机制。更多深入的细节可以在（Fu 等，[2023](#bib.bib57)；Wolf 和
    Lam，[1991](#bib.bib215)）中找到。
- en: 4.3\. Deep Learning Libraries
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 深度学习库
- en: To facilitate the deployment of a DL model, it is also essential to use DL libraries
    that provide high-level APIs to simplify the implementation, design, and training
    of complex NNs. We introduce several popular DL libraries supporting GPU acceleration
    and the auto gradient system.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便深度学习模型的部署，使用提供高级 API 的深度学习库以简化复杂神经网络的实现、设计和训练也是至关重要的。我们介绍了几种支持 GPU 加速和自动梯度系统的流行深度学习库。
- en: TensorFlow (Abadi et al., [2016](#bib.bib2)) supports static and dynamic graphs,
    allowing users to select the most suitable mode. With this flexibility, TensorFlow
    supports the research and development of custom DL models. Additionally, TensorFlow
    also provides extensive APIs for DL model implementation. For instance, a TensorFlow
    model can be converted into a TensorFlow-Lite (TF-Lite) (David et al., [2021](#bib.bib40))
    model, a smaller, more efficient ML model format that can be run on mobile and
    edge devices.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow（Abadi 等，[2016](#bib.bib2)）支持静态和动态图，允许用户选择最适合的模式。借助这种灵活性，TensorFlow
    支持自定义深度学习模型的研究和开发。此外，TensorFlow 还提供了广泛的 API 以便于深度学习模型的实现。例如，TensorFlow 模型可以转换为
    TensorFlow-Lite（TF-Lite）（David 等，[2021](#bib.bib40)）模型，这是一种较小、更高效的机器学习模型格式，可在移动和边缘设备上运行。
- en: PyTorch (Paszke et al., [2019](#bib.bib154)) is a framework renowned for its
    remarkable capacity to facilitate the creation of intricate models and the fine-tuning
    of NNs down to the minute details, making it a favored choice within the research
    community. Its simplicity, user-friendliness, and intuitiveness made it a go-to
    tool for prototyping DL models. However, there are certain deployment-related
    limitations with its API, which might restrict its application in certain real-life
    scenarios.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch（Paszke 等，[2019](#bib.bib154)）是一个以其在创建复杂模型和对神经网络进行细节调整方面的显著能力而闻名的框架，使其成为研究界的首选。其简洁性、用户友好性和直观性使其成为原型设计深度学习模型的首选工具。然而，其
    API 在某些部署相关的场景中存在一些限制，这可能会限制其在某些实际场景中的应用。
- en: MXNet (Chen et al., [2016](#bib.bib25)) is a library that provides optimized
    building blocks for implementing CNNs. It is specially tailored for Intel processors,
    offering vectorized and threaded support for CNNs on Intel CPUs and GPUs. Moreover,
    the MXNet framework provides interfaces in multiple languages, including Python,
    Scala, Java, Clojure, and R, making it convenient for cross-domain DL developers.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet（Chen 等，[2016](#bib.bib25)）是一个为实现 CNN 提供优化构建模块的库。它特别针对 Intel 处理器，提供对 Intel
    CPU 和 GPU 上的 CNN 的矢量化和多线程支持。此外，MXNet 框架还提供了多种语言的接口，包括 Python、Scala、Java、Clojure
    和 R，方便跨领域深度学习开发者使用。
- en: NVIDIA has been at the forefront of GPU hardware and software optimization for
    DL. cuDNN (Chetlur et al., [2014](#bib.bib31)) is a highly optimized library specifically
    designed for DL networks, providing acceleration for DNN-related tasks. In addition
    to cuDNN, NVIDIA offers a range of DL libraries included in CUDA-X (NVIDIA, [2023](#bib.bib151)).
    TensorRT (Vanholder, [2016](#bib.bib203)), another NVIDIA library, optimizes inference
    on NVIDIA GPUs by applying layer and tensor fusion, kernel auto-tuning, and dynamic
    tensor memory optimizations.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA 一直处于 GPU 硬件和软件优化的前沿。cuDNN（Chetlur 等，[2014](#bib.bib31)）是一个专门为深度学习网络设计的高度优化库，为
    DNN 相关任务提供加速。除了 cuDNN，NVIDIA 还提供了一系列包含在 CUDA-X（NVIDIA，[2023](#bib.bib151)）中的深度学习库。另一个
    NVIDIA 库 TensorRT（Vanholder，[2016](#bib.bib203)）通过应用层和张量融合、内核自动调优以及动态张量内存优化来优化在
    NVIDIA GPU 上的推理。
- en: Each DL library has unique strengths and caters to specific use cases, allowing
    practitioners to choose one that best suits their projects. To address the interoperability
    challenges between DL libraries, Microsoft and Facebook introduced Open Neural
    Network Exchange (ONNX) (Foundation, [2017](#bib.bib53)), an open standard for
    machine learning interoperability. With ONNX, models created in different libraries
    can be easily shared and executed. For instance, a PyTorch model can be run on
    an Android device by converting it into TensorFlow format, eliminating the need
    for model retraining.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 每种深度学习库都有其独特的优势，并针对特定的使用场景，为从业人员提供了最适合其项目的选择。为了解决深度学习库之间的互操作性挑战，微软和 Facebook
    推出了开放神经网络交换（ONNX）（Foundation，[2017](#bib.bib53)），这是一个用于机器学习互操作性的开放标准。通过 ONNX，使用不同库创建的模型可以轻松共享和执行。例如，可以将
    PyTorch 模型转换为 TensorFlow 格式，然后在 Android 设备上运行，从而避免了模型重新训练的需要。
- en: 4.4\. Co-Design of Hardware Architecture
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4. 硬件架构的共同设计
- en: In DL, acceleration solutions relying solely on software techniques are primarily
    limited by their dependence on the intrinsic capabilities of general-purpose processors,
    potentially struggling to exploit specialized hardware features designed for specific
    DL tasks fully. Conversely, hardware-only solutions may face limitations in flexibility
    and adaptability, as dedicated hardware is often tailored for specific tasks or
    architectures, making updates or adaptations to new DL models challenging without
    hardware modifications. This underscores the value of co-designing a hardware
    and software approach for resource-constrained environments, employing a holistic
    optimization strategy. This approach includes refining the DL algorithm, optimizing
    and compressing the model, efficient memory management, software kernel implementation,
    and hardware architecture design. This section discusses solutions that adopt
    a holistic approach to address challenges related to irregular memory accesses,
    enhance the handling of sparsity resulting from compression methods, and explore
    improved solutions within NAS algorithms.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习（DL）中，仅依赖软件技术的加速解决方案主要受到通用处理器固有能力的限制，可能难以充分利用为特定DL任务设计的专用硬件特性。相反，仅使用硬件的解决方案可能在灵活性和适应性方面面临限制，因为专用硬件通常针对特定任务或架构进行定制，使得在没有硬件修改的情况下更新或适应新的DL模型变得困难。这突显了在资源受限环境中共同设计硬件和软件方法的价值，采用整体优化策略。这种方法包括优化DL算法、优化和压缩模型、高效内存管理、软件内核实现和硬件架构设计。本节讨论了采用整体方法解决与不规则内存访问相关的挑战，增强由于压缩方法导致的稀疏处理，以及在NAS算法中探索改进解决方案。
- en: In Section 3, we emphasize that many NN connections can be pruned effectively
    without substantial accuracy loss. However, in such models, only a subset of the
    NN’s weights are active, and their locations are irregular or non-contiguous.
    Efficiently accessing these weights, especially when using hardware accelerators
    like GPUs or TPUs, can be challenging due to the irregularity of weight locations.
    To tackle this issue, in earlier methods, like Cambricon-X (Zhang et al., [2016](#bib.bib248)),
    MAC operations utilize zero-weight connections and access required weights using
    sparse indices. However, irregular nonzero weight distribution caused issues such
    as indexing overhead, PE imbalances, and inefficient memory access. Later advancements,
    as seen in Cambricon-S (Zhou et al., [2018](#bib.bib255)), improve efficiency
    by enforcing regularity in filter sparsity through software/hardware integration.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3节中，我们强调许多神经网络（NN）连接可以在不显著降低准确度的情况下有效地进行修剪。然而，在这样的模型中，只有一部分NN的权重是活动的，它们的位置是不规则或不连续的。尤其是在使用像GPU或TPU这样的硬件加速器时，高效访问这些权重可能会面临挑战，因为权重位置的不规则性。为了解决这个问题，早期的方法如Cambricon-X（Zhang等，[2016](#bib.bib248)）使用零权重连接来进行MAC操作，并利用稀疏索引访问所需的权重。然而，不规则的非零权重分布带来了索引开销、PE不平衡和内存访问低效等问题。后来的进展，如Cambricon-S（Zhou等，[2018](#bib.bib255)），通过软件/硬件集成强制实施滤波器稀疏性的规则性，从而提高了效率。
- en: Sparse-YOLO (Wang et al., [2020b](#bib.bib213)) introduces a dedicated sparse
    convolution unit tailored to handle quantized values and sparsity resulting from
    unstructured pruning techniques. Cho et al. (Cho et al., [2021](#bib.bib33)) propose
    an acceleration technique for a quantized binary NN. This approach utilizes an
    array of PEs, with each PE responsible for computing the output of a specific
    feature map, implementing inter-feature map parallelism. Moreover, optimizing
    the storage of sparse weights post-pruning has been explored. Han et al. (Han
    et al., [2016](#bib.bib76)) show that these sparse weights can be compressed,
    reducing memory access bandwidth by around 20%-30%. SCNN (Parashar et al., [2017](#bib.bib153))
    processes convolutional layers in their compressed format using an input stationary
    dataflow. This involves transmitting compressed weights and activations to a multiplier
    array, followed by a scatter network to add the scattered partial sums.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: Sparse-YOLO（Wang et al., [2020b](#bib.bib213)）引入了一种专门的稀疏卷积单元，旨在处理量化值和由非结构化剪枝技术导致的稀疏性。Cho
    et al.（Cho et al., [2021](#bib.bib33)）提出了一种用于量化二进制神经网络的加速技术。这种方法利用了一组 PEs，每个 PE
    负责计算特定特征图的输出，实现了特征图间的并行处理。此外，优化剪枝后稀疏权重的存储也已被探索。Han et al.（Han et al., [2016](#bib.bib76)）展示了这些稀疏权重可以被压缩，从而将内存访问带宽减少了约
    20%-30%。SCNN（Parashar et al., [2017](#bib.bib153)）使用输入静态数据流处理其压缩格式的卷积层。这涉及将压缩权重和激活值传输到乘法器数组，然后通过散射网络添加散布的部分和。
- en: In the NAS field, apart from the previously discussed hardware-aware NAS approaches
    that tailor models for specific hardware platforms, there are also co-designed
    solutions that initially remain hardware-agnostic. These co-designed systems seamlessly
    integrate hardware optimization within the NAS process, ensuring simultaneous
    hardware and DNN model optimization. Hardware settings can be explored in conjunction
    with DNN architectures using the same algorithm (Zhou et al., [2021](#bib.bib256);
    Choi et al., [2021](#bib.bib35); Li et al., [2020](#bib.bib120)) or through an
    external search algorithm (Sekanina, [2021](#bib.bib171); Lin et al., [2020b](#bib.bib128)).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NAS 领域，除了之前讨论的针对特定硬件平台的硬件感知 NAS 方法，还有一些最初保持硬件无关的共同设计解决方案。这些共同设计的系统在 NAS 过程中无缝集成硬件优化，确保硬件和
    DNN 模型的同时优化。硬件设置可以与 DNN 架构一起通过相同的算法进行探索（Zhou et al., [2021](#bib.bib256); Choi
    et al., [2021](#bib.bib35); Li et al., [2020](#bib.bib120)），或者通过外部搜索算法进行探索（Sekanina,
    [2021](#bib.bib171); Lin et al., [2020b](#bib.bib128)）。
- en: 'As shown in Fig. [11](#S4.F11 "Figure 11 ‣ 4.4\. Co-Design of Hardware Architecture
    ‣ 4\. Hardware Acceleration of Deep Learning Models ‣ Lightweight Deep Learning
    for Resource-Constrained Environments: A Survey")(a), the most direct approach
    for co-searching hardware and software settings involves creating CNN and accelerator
    pairs and evaluating the final model’s performance. One can opt to train the CNN
    each time a new pair is tested or follow the approach of Chen et al. (Chen et al.,
    [2020c](#bib.bib26)), where a supernet is employed to directly generate the weights
    of a DDN, and accuracy is assessed in a single testing run of the model. Fig. [11](#S4.F11
    "Figure 11 ‣ 4.4\. Co-Design of Hardware Architecture ‣ 4\. Hardware Acceleration
    of Deep Learning Models ‣ Lightweight Deep Learning for Resource-Constrained Environments:
    A Survey")(b) illustrates an alternative strategy employed by Lin et al. (Lin
    et al., [2020b](#bib.bib128)), where a hardware optimization algorithm takes a
    candidate CNN as input and optimizes the hardware accelerator to achieve specific
    objectives. The network is then trained and evaluated only if a viable hardware
    configuration is found. If no suitable hardware setting is identified, the network
    remains untrained until a viable configuration is found. This strategy allows
    for the avoidance of training the CNN, which is the most complex phase of the
    co-design process.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [11](#S4.F11 "Figure 11 ‣ 4.4\. 硬件架构共同设计 ‣ 4\. 深度学习模型的硬件加速 ‣ 轻量级深度学习用于资源受限环境：调研")(a)所示，最直接的硬件和软件设置共同搜索方法涉及创建CNN和加速器对，并评估最终模型的性能。可以选择在每次测试新的对时训练CNN，或者遵循Chen等人（Chen
    et al., [2020c](#bib.bib26)）的方法，其中使用超网络直接生成DDN的权重，并在一次模型测试中评估准确性。图 [11](#S4.F11
    "Figure 11 ‣ 4.4\. 硬件架构共同设计 ‣ 4\. 深度学习模型的硬件加速 ‣ 轻量级深度学习用于资源受限环境：调研")(b)展示了Lin等人（Lin
    et al., [2020b](#bib.bib128)）采用的另一种策略，其中硬件优化算法将候选CNN作为输入，并优化硬件加速器以实现特定目标。网络仅在找到可行的硬件配置后才会进行训练和评估。如果没有找到合适的硬件设置，网络将保持未训练状态，直到找到可行的配置。该策略避免了训练CNN，这是共同设计过程中的最复杂阶段。
- en: '![Refer to caption](img/3fceaf38b2814b5a620f4748d29b032a.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3fceaf38b2814b5a620f4748d29b032a.png)'
- en: Figure 11\. Two different approaches for implementing NAS and hardware co-design (Sekanina,
    [2021](#bib.bib171)).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 实现NAS和硬件共同设计的两种不同方法（Sekanina, [2021](#bib.bib171)）。
- en: In summary, the co-design of algorithms significantly improves compression and
    computational efficiency. However, these methods are inherently non-trivial and
    require in-depth exploration of software and hardware techniques.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，算法的共同设计显著提高了压缩和计算效率。然而，这些方法本质上并非易事，需要深入探讨软件和硬件技术。
- en: 5\. Challenge and Future work
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 挑战与未来工作
- en: In this survey, we explore the sophisticated domain of lightweight models, compression
    methods, and hardware acceleration, showcasing their advanced technological capabilities
    applicable across a broad spectrum of general applications. Nonetheless, deploying
    these models in resource-constrained environments continues to present substantial
    challenges. This section is dedicated to unveiling novel techniques in TinyML
    and LLMs for accelerating and applying DL models, focusing on unresolved issues
    that warrant further investigation.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次调研中，我们探讨了轻量级模型、压缩方法和硬件加速的复杂领域，展示了它们在广泛应用中的先进技术能力。然而，在资源受限的环境中部署这些模型仍然面临着重大挑战。本节致力于揭示TinyML和LLMs在加速和应用深度学习模型方面的新技术，重点关注那些需要进一步研究的未解决问题。
- en: 5.1\. TinyML
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. TinyML
- en: TinyML is an emerging technology that enables DL algorithms to run on ultra-low-end
    IoT devices that consume less than 1mW of power. However, the extremely constrained
    hardware environment makes it challenging to design and develop a TinyML model.
    Low-end IoT devices predominantly employ MCUs due to their cost efficiency compared
    to CPUs and GPUs. However, MCU libraries, such as CMSIS-NN (Lai et al., [2018](#bib.bib113))
    and TinyEngine (Lin et al., [2020a](#bib.bib125)), are often platform-dependent,
    unlike GPU libraries like PyTorch and TensorFlow, which offer cross-platform support.
    Consequently, the design focus of TinyML leans more toward specialized applications
    rather than facilitating general-purpose research, potentially impeding the pace
    of overall research advancements.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: TinyML 是一项新兴技术，它使深度学习算法能够在功耗低于 1mW 的超低端物联网设备上运行。然而，极其受限的硬件环境使得设计和开发 TinyML 模型变得具有挑战性。低端物联网设备主要使用微控制器（MCU），因为与中央处理器（CPU）和图形处理器（GPU）相比，MCU
    成本更为高效。然而，MCU 库，如 CMSIS-NN（Lai 等，[2018](#bib.bib113)）和 TinyEngine（Lin 等，[2020a](#bib.bib125)），往往依赖于特定平台，而不像
    GPU 库如 PyTorch 和 TensorFlow 提供跨平台支持。因此，TinyML 的设计重点更多地倾向于专用应用，而非促进通用研究，这可能会阻碍整体研究进展的速度。
- en: MCU-based libraries. Due to the resource-constrained environments in TinyML,
    MCU-based libraries are often designed for specific use cases. For instance, CMSIS-NN (Lai
    et al., [2018](#bib.bib113)), a pioneering work for MCU-based libraries developed
    on ARM Cortex-M devices, proposes an efficient kernel divided into NNfunctions
    and NNsupportfunctions. NNfunctions execute the main functions in the network,
    such as convolutions, poolings, and activations. NNsupportfunctions contain data
    conversions and activation tables. CMIX-NN (Capotondi et al., [2020](#bib.bib16))
    proposes an open-source mixed and low-precision tool that can support the model’s
    weights and activation to be quantized into 8, 4, and 2 bits arbitrarily. MCUNet (Lin
    et al., [2020a](#bib.bib125)) presents a co-design framework tailored for DL implementation
    on commercially available MCUs. This framework incorporates TinyNAS to search
    for the most accurate and lightweight model efficiently. Additionally, it leverages
    the TinyEngine, which encompasses code generator-based compilations and in-place
    depthwise convolution, effectively addressing peak memory constraints. Moving
    forward, MCUNetV2 (Lin et al., [2021](#bib.bib124)) introduces a patch-based inference
    mechanism that operates only on a small spatial region of the feature map, further
    reducing peak memory use. MicroNet (Banbury et al., [2021](#bib.bib9)) adopts
    differentiable NAS (DNAS) to search for efficient models with a low number of
    operations and supports the open-source platform Tensorflow Lite Micro (TFLM).
    MicroNet achieves state-of-the-art results for all TinyMLperf industry-standard
    benchmark tasks, i.e., Visual Wake Words, Google Speech Commands, and Anomaly
    detection.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 MCU 的库。由于 TinyML 中的资源受限环境，基于 MCU 的库通常是为特定用例设计的。例如，CMSIS-NN（Lai 等，[2018](#bib.bib113)），作为在
    ARM Cortex-M 设备上开发的基于 MCU 的库的开创性工作，提出了一种高效的内核，分为 NNfunctions 和 NNsupportfunctions。NNfunctions
    执行网络中的主要功能，如卷积、池化和激活。NNsupportfunctions 包含数据转换和激活表。CMIX-NN（Capotondi 等，[2020](#bib.bib16)）提出了一种开源的混合和低精度工具，可以将模型的权重和激活量化为
    8、4 和 2 位。MCUNet（Lin 等，[2020a](#bib.bib125)）提供了一个针对商业可用 MCU 的深度学习实现的协同设计框架。该框架包括
    TinyNAS，用于高效地搜索最准确和最轻量的模型。此外，它利用了 TinyEngine，该引擎包含基于代码生成的编译和就地深度卷积，有效地解决了峰值内存限制。展望未来，MCUNetV2（Lin
    等，[2021](#bib.bib124)）引入了一种基于补丁的推理机制，仅在特征图的小空间区域上操作，进一步减少了峰值内存使用。MicroNet（Banbury
    等，[2021](#bib.bib9)）采用可微分 NAS（DNAS）来寻找操作次数较少的高效模型，并支持开源平台 Tensorflow Lite Micro（TFLM）。MicroNet
    在所有 TinyMLperf 行业标准基准任务中取得了最先进的成果，即视觉唤醒词、谷歌语音命令和异常检测。
- en: What hinders the rapid development of TinyML? Despite its progress, the growth
    of TinyML is hindered by several inherent key constraints, including resource
    constraints, hardware and software heterogeneity, and lack of datasets (Ray, [2022](#bib.bib162)).
    Extreme resource constraints, such as an incredibly small size of SRAM and less
    than 1 MB size flash memory, pose challenges in designing and deploying TinyML
    models on edge devices. Furthermore, due to hardware heterogeneity and a lack
    of framework compatibility, current TinyML solutions are tweaked for every individual
    device, complicating the wide-scale deployment of TinyML algorithms. Besides,
    existing datasets may not be suitable for TinyML architecture as the data may
    not correspond to the data generation feature from external sensors of edge devices.
    A set of standard datasets suitable for training TinyML models is needed to advance
    the development of effective TinyML systems. These open research challenges need
    to be addressed before mass deployment on IoT and edge devices is possible.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: TinyML的快速发展受到什么阻碍？尽管TinyML取得了进展，但其增长受到若干固有关键限制的制约，包括资源约束、硬件和软件的异质性以及数据集的缺乏（Ray，[2022](#bib.bib162)）。极端的资源限制，如极小的SRAM和不足1
    MB的闪存，给在边缘设备上设计和部署TinyML模型带来了挑战。此外，由于硬件异质性和缺乏框架兼容性，目前的TinyML解决方案需要针对每个单独设备进行调整，这使得TinyML算法的大规模部署变得复杂。而且，现有的数据集可能不适用于TinyML架构，因为数据可能与边缘设备外部传感器的数据生成特征不对应。需要一套标准的数据集来训练TinyML模型，以推进有效TinyML系统的发展。在大规模部署到物联网和边缘设备之前，这些开放的研究挑战需要解决。
- en: 5.2\. Building lightweight Large Language Models
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 构建轻量级的大型语言模型
- en: LLMs have consistently exhibited outstanding performance across various tasks
    in the past two years (Anil et al., [2023](#bib.bib7); Touvron et al., [2023b](#bib.bib201);
    Ye et al., [2023](#bib.bib232)). LLMs hold significant potential for practical
    applications, especially when paired with human supervision. For instance, they
    can serve as co-pilots alongside autonomous agents or as sources of inspiration
    and suggestions. However, these models typically feature parameters at the billion
    scale. Deploying such models for inference generally demands GPU-level hardware
    and tens of gigabytes of memory, posing substantial challenges for everyday LLM
    utilization. For example, Tao et al. (Tao et al., [2022](#bib.bib195)) find it
    hard to quantize generative pre-trained language models due to homogeneous word
    embedding and varied weight distribution. Consequently, transforming a large,
    resource-intensive LLM model into a compact version suitable for deployment on
    resource-constrained mobile devices has emerged as a prominent future research
    direction.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去两年中，大型语言模型（LLMs）在各种任务中始终展现出卓越的表现（Anil et al., [2023](#bib.bib7)；Touvron et
    al., [2023b](#bib.bib201)；Ye et al., [2023](#bib.bib232)）。LLMs在实际应用中具有重要潜力，特别是在与人类监督相结合时。例如，它们可以作为自主代理的副驾驶，或提供灵感和建议。然而，这些模型通常具有亿级参数。部署这些模型进行推理通常需要GPU级硬件和数十GB的内存，这对日常LLM的使用提出了重大挑战。例如，Tao
    et al.（Tao et al., [2022](#bib.bib195)）发现，由于同质化的词嵌入和变化的权重分布，量化生成预训练语言模型非常困难。因此，将大型、资源密集型的LLM模型转化为适合在资源有限的移动设备上部署的紧凑版本，已成为未来研究的一个重要方向。
- en: World-renowned enterprises have made significant strides in LLM deployment.
    In 2023, Qualcomm showcased the independent execution of the text-to-image model,
    Stable Diffusion (Rombach et al., [2022](#bib.bib167)) and the image-to-image
    model, ControlNet (Zhang et al., [2023b](#bib.bib246)) on mobile devices, thereby
    accelerating the deployment of large models to edge computing environments. Google
    also introduced several versions of its latest universal large model, PaLM 2 (Anil
    et al., [2023](#bib.bib7)), featuring a lightweight variant tailored for mobile
    platforms. This development has created new opportunities for migrating large
    models from cloud-based systems to edge devices. However, certain large models
    still require several gigabytes of physical storage and runtime memory. Consequently,
    efforts are being directed towards achieving a memory footprint of less than 1
    GB (Ray, [2022](#bib.bib162)), signifying that significant work is still needed
    in this area. This section outlines some key initiatives for easing the implementation
    of LLMs in resource-constrained environments.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 世界知名企业在LLM部署方面取得了显著进展。2023年，高通展示了在移动设备上独立执行文本到图像模型Stable Diffusion（Rombach等，[2022](#bib.bib167)）和图像到图像模型ControlNet（Zhang等，[2023b](#bib.bib246)），从而加速了大型模型向边缘计算环境的部署。谷歌还推出了其最新通用大模型PaLM
    2（Anil等，[2023](#bib.bib7)）的多个版本，其中包括针对移动平台量身定制的轻量级变体。这一发展为将大型模型从基于云的系统迁移到边缘设备创造了新的机会。然而，某些大型模型仍需数GB的物理存储和运行时内存。因此，努力的方向是实现小于1GB的内存占用（Ray，[2022](#bib.bib162)），这表明这一领域仍需大量工作。本节概述了一些在资源受限环境中简化LLM实施的关键举措。
- en: 5.2.1\. Pruning without re-training
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 无需重新训练的剪枝
- en: Recently, a substantial body of work has applied common DL quantization and
    pruning techniques to construct lightweight LLMs. Some approaches (Yu et al.,
    [2023](#bib.bib237); Wu et al., [2023](#bib.bib220)) focus on implementing quantization,
    where numerical precision is greatly reduced. SparseGPT (Frantar and Alistarh,
    [2023](#bib.bib56)) demonstrates, for the first time, that large-scale Generative
    Pre-trained Transformer (GPT) models can be pruned to at least 50% sparsity in
    a single step, without any subsequent retraining, with minimal loss of accuracy.
    Following this, Wanda (Pruning by Weights and Activations) (Sun et al., [2023](#bib.bib182)),
    specifically designed to induce sparsity in pre-trained LLMs, is introduced. Wanda
    prunes weights with the smallest magnitudes and does not require retraining or
    weight updates. The pruned LLM can be directly utilized, increasing its practicality.
    Notably, Wanda surpasses the established baseline of magnitude pruning and competes
    effectively with recent methods that involve extensive weight updates. These works
    set a significant milestone for future work in designing LLM pruning methods that
    do not require retraining.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大量工作将常见的深度学习量化和剪枝技术应用于构建轻量级LLM。一些方法（Yu等，[2023](#bib.bib237)；Wu等，[2023](#bib.bib220)）侧重于实现量化，其中数值精度大幅降低。SparseGPT（Frantar和Alistarh，[2023](#bib.bib56)）首次展示了大规模生成预训练变换器（GPT）模型可以在一步中剪枝到至少50%的稀疏性，且无需后续的重新训练，且精度损失最小。随后，引入了专门设计用于在预训练LLM中引入稀疏性的Wanda（基于权重和激活的剪枝）（Sun等，[2023](#bib.bib182)）。Wanda剪枝权重的最小幅度，无需重新训练或权重更新。剪枝后的LLM可以直接使用，提高了其实用性。值得注意的是，Wanda超越了已建立的幅度剪枝基准，并与涉及广泛权重更新的最新方法有效竞争。这些工作为未来设计不需要重新训练的LLM剪枝方法设立了重要里程碑。
- en: 5.2.2\. Model Design
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 模型设计
- en: From a model design perspective, we can create lightweight LLMs from the very
    inception, focusing on reducing the number of model parameters. One promising
    avenue in this endeavor is prompt tuning, which seeks to optimize the LLMs’ performance
    while maintaining efficiency and model size. A notable approach in this context
    is Visual Prompt Tuning (VPT) (Jia et al., [2022](#bib.bib102)), which emerges
    as an efficient and effective alternative to the comprehensive fine-tuning of
    large-scale Transformer models employed in vision-related tasks. VPT introduces
    a mere fraction, less than 1%, of trainable parameters within the input space
    while maintaining the integrity of the model’s backbone. Another noteworthy contribution
    is CALIP (Guo et al., [2023](#bib.bib69)), which introduces parameter-free attention
    mechanisms to facilitate effective interaction and communication between visual
    and text features. It yields text-aware image features and visual-guided text
    features, contributing to the development of more streamlined and efficient vision-language
    models. In the near future, one promising avenue for advancing lightweight LLM
    design is the development of adaptive fine-tuning strategies. These strategies
    would dynamically adjust the model’s architecture and parameters to align with
    specific task requirements. This adaptability ensures the model can optimize its
    performance for particular applications without incurring unnecessary parameter
    bloat.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型设计的角度来看，我们可以从一开始就创建轻量级的LLMs，专注于减少模型参数的数量。在这一努力中，一个有前景的途径是提示调整（prompt tuning），它旨在优化LLMs的性能，同时保持效率和模型规模。在这一背景下，一个值得注意的方法是视觉提示调整（Visual
    Prompt Tuning, VPT）（Jia et al., [2022](#bib.bib102)），它作为一个高效且有效的替代方案，与在视觉相关任务中使用的大规模Transformer模型的全面微调相比，展现了优势。VPT在输入空间中引入了不到1%的可训练参数，同时保持了模型骨干的完整性。另一个值得注意的贡献是CALIP（Guo
    et al., [2023](#bib.bib69)），它引入了无参数的注意机制，以促进视觉和文本特征之间的有效互动和沟通。它产生了文本感知的图像特征和视觉引导的文本特征，有助于开发更为简化和高效的视觉语言模型。在不久的将来，推进轻量级LLM设计的一个有前景的途径是开发自适应微调策略。这些策略将动态调整模型的架构和参数，以符合特定任务的要求。这种适应性确保了模型能够优化其在特定应用中的性能，而不会产生不必要的参数膨胀。
- en: 5.2.3\. Building Lightweight Diffusion Model
  id: totrans-369
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 构建轻量级扩散模型
- en: In recent years, denoising diffusion-based generative models, particularly those
    of the score-based variety (Ho et al., [2020](#bib.bib87); Song et al., [2021](#bib.bib176)),
    have made notable strides in creating diverse and authentic data. However, the
    transition of the inference phase of a diffusion model to edge devices poses significant
    challenges. The inference phase reverses the transformation process to generate
    real data from Gaussian noise, commonly known as the denoising process. Moreover,
    when these models are compressed to reduce their footprint and computational demands,
    there is a potential risk of severe degradation in image quality. The compression
    process may need simplifications, approximations, or even the removal of essential
    model components, which could adversely affect the model’s ability to reconstruct
    data from Gaussian noise accurately. Consequently, a critical concern emerges
    in balancing model size reduction with preserving high-quality image generation,
    thereby presenting a formidable challenge in developing diffusion models in resource-constrained
    scenarios.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，基于去噪扩散的生成模型，特别是基于评分的方法（Ho et al., [2020](#bib.bib87); Song et al., [2021](#bib.bib176)），在创建多样化和真实的数据方面取得了显著进展。然而，扩散模型推理阶段转移到边缘设备上面临着重大挑战。推理阶段逆转了转换过程，从高斯噪声生成真实数据，这通常被称为去噪过程。此外，当这些模型被压缩以减少其占用空间和计算需求时，可能会严重影响图像质量。压缩过程可能需要简化、近似，甚至移除模型的关键组件，这可能会不利于模型从高斯噪声中准确重建数据。因此，在资源受限的情况下开发扩散模型时，平衡模型大小减少与保持高质量图像生成之间的关键问题成为了一个严峻的挑战。
- en: In a very recent work, Shang et al. (Shang et al., [2023](#bib.bib173)) introduce
    post-training quantization (Cai et al., [2020b](#bib.bib15)) into the field of
    diffusion model acceleration. When applied in a training-free manner, this quantization
    approach exhibits the capability to enhance the efficiency of the denoising process
    while simultaneously reducing the storage requirements for diffusion model weights,
    a critical component in the acceleration of diffusion models. Nevertheless, there
    remain numerous opportunities for improvement in this domain to achieve a trade-off
    between high-quality and lightweight model solutions.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的一项工作中，Shang 等人（Shang et al., [2023](#bib.bib173)）将后训练量化（Cai et al., [2020b](#bib.bib15)）引入了扩散模型加速领域。当以无训练的方式应用时，这种量化方法表现出提升去噪过程效率的能力，同时减少了扩散模型权重的存储需求，这是加速扩散模型的关键组成部分。然而，在该领域仍然有许多改进的机会，以实现高质量与轻量级模型解决方案之间的权衡。
- en: 5.2.4\. Deployment of Vision Transformers (ViTs)
  id: totrans-372
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4\. Vision Transformers (ViTs) 的部署
- en: Despite the increasing prevalence of lightweight ViTs, deploying ViT in hardware-constrained
    environments remains a persistent concern. According to (Wang et al., [2022b](#bib.bib212)),
    ViT inference on mobile devices has a latency and energy consumption of up to
    40 times higher than CNN models. Hence, without modification, mobile devices cannot
    support the inference of ViTs. The self-attention operations in ViTs need to compute
    the pair-wise relations between image patches, and the computations grow quadratically
    with the number of patches. Moreover, computation for FFN layers is more time-consuming
    than attention layers (Wang et al., [2022b](#bib.bib212)). By removing the redundant
    attention heads and FFN layers, DeiT-Tiny can reduce latency by 23.2%, with negligible
    0.75% accuracy loss.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管轻量级 ViTs 的普及度不断增加，但在硬件受限环境中部署 ViT 仍然是一个持续关注的问题。根据 (Wang et al., [2022b](#bib.bib212))，ViT
    在移动设备上的推理延迟和能耗比 CNN 模型高出多达 40 倍。因此，未经修改的移动设备无法支持 ViTs 的推理。ViTs 中的自注意力操作需要计算图像补丁之间的成对关系，而计算量随着补丁数量的增加呈二次增长。此外，FFN
    层的计算比注意力层更为耗时 (Wang et al., [2022b](#bib.bib212))。通过去除多余的注意力头和 FFN 层，DeiT-Tiny
    可以将延迟减少 23.2%，而准确度损失几乎可以忽略不计，仅为 0.75%。
- en: Several works designed NLP models for embedded systems such as FPGAs (Ham et al.,
    [2020](#bib.bib73), [2021](#bib.bib74); Wang et al., [2021](#bib.bib207)). More
    recently, DiVIT (Li et al., [2022](#bib.bib121)) and VAQF (Sun et al., [2022](#bib.bib183))
    proposed hardware-software co-designed solutions for ViTs. DiVIT proposes a delta
    patch encoding and novel differential attention at the algorithm level that leverages
    the patch locality during inference. In DiVIT, the design of a differential attention
    Processing Engine array with bit-saving techniques can calculate the delta with
    less computation and communicate with differential dataflow. Furthermore, the
    exponent operation is executed using a lookup table without additional computation
    and with minimal hardware overhead. VAQF first introduces binarization into ViTs,
    which can be used for FPGA mapping and quantization training. Specifically, VAQF
    can generate the required quantization precision and accelerator description for
    direct software and hardware implementation based on the target frame rate.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作为嵌入式系统（如 FPGA）设计了 NLP 模型 (Ham et al., [2020](#bib.bib73), [2021](#bib.bib74);
    Wang et al., [2021](#bib.bib207))。最近，DiVIT (Li et al., [2022](#bib.bib121)) 和
    VAQF (Sun et al., [2022](#bib.bib183)) 提出了 ViTs 的硬件-软件协同设计解决方案。DiVIT 提出了在算法层面利用补丁局部性的
    delta 补丁编码和新型差分注意力。在 DiVIT 中，通过使用节省位的技术设计差分注意力处理引擎阵列，可以以较少的计算量计算 delta 并与差分数据流进行通信。此外，指数操作使用查找表执行，无需额外计算且硬件开销最小。VAQF
    首次将二值化引入 ViTs，这可以用于 FPGA 映射和量化训练。具体来说，VAQF 可以根据目标帧率生成所需的量化精度和加速器描述，用于直接的软件和硬件实现。
- en: 'To enable the seamless deployment of ViTs in resource-constrained devices,
    we highlight two potential future directions:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在资源受限的设备上实现 Vision Transformers (ViTs) 的无缝部署，我们强调了两个潜在的未来方向：
- en: 1) Algorithm optimizations. In addition to the design of efficient ViT models
    described in Section 2.3, the bottlenecks of ViTs should also be considered. For
    example, since MatMul operations cause a bottleneck in ViTs, these operations
    can be accelerated or reduced (Wang et al., [2022b](#bib.bib212)). Additionally,
    integer quantization and improvement to operator fusion can be considered.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 算法优化。除了第2.3节中描述的高效ViT模型设计外，还应考虑ViTs的瓶颈。例如，由于MatMul操作会导致ViTs的瓶颈，这些操作可以被加速或减少（Wang
    et al., [2022b](#bib.bib212)）。此外，还可以考虑整数量化和操作符融合的改进。
- en: 2) Hardware Accessibility. Unlike CNNs, which are well-supported on most mobile
    devices and AI accelerators, ViTs do not have specialized hardware support. For
    instance, ViT fails to run on mobile GPUs and Intel NCS2 VPU. Based on our findings,
    some important operators are not supported on specific hardware. Specifically,
    on the mobile GPU, the concatenate operator requires a 4-dimensional input tensor
    in TFLiteGPUDelegate, but the tensor in ViTs is 3-dimensional. On the other hand,
    Intel VPU does not support LayerNorm, which exists in the architecture of transformers
    but is uncommon in CNN. Hence, hardware support for ViTs on resource-constrained
    devices warrants further investigation.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 硬件可用性。与大多数移动设备和AI加速器上广泛支持的CNNs不同，ViTs没有专门的硬件支持。例如，ViT无法在移动GPU和Intel NCS2
    VPU上运行。根据我们的发现，一些重要的操作符在特定硬件上不被支持。具体来说，在移动GPU上，连接操作符在TFLiteGPUDelegate中需要4维输入张量，但ViTs中的张量是3维的。另一方面，Intel
    VPU不支持LayerNorm，而LayerNorm存在于transformers的架构中，但在CNN中并不常见。因此，ViTs在资源受限设备上的硬件支持需要进一步研究。
- en: 6\. Conclusion
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: Recently, computer vision applications have increasingly prioritized energy
    conservation, carbon footprint reduction, and cost-effectiveness, highlighting
    the growing importance of lightweight models, particularly in the context of edge
    AI. This paper conducts a comprehensive examination of lightweight deep learning
    (DL), exploring prominent models such as MobileNet and Efficient transformer variants,
    along with prevalent strategies for optimizing these models, including pruning,
    quantization, knowledge distillation, and neural architecture search. Beyond providing
    a detailed explanation of these methods, we offer practical guidance for crafting
    customized lightweight models, offering clarity through an analysis of their respective
    strengths and weaknesses.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，计算机视觉应用越来越重视能源节约、减少碳足迹和成本效益，突显了轻量级模型的重要性，尤其是在边缘AI的背景下。本文对轻量级深度学习（DL）进行了全面的考察，探讨了诸如MobileNet和Efficient
    transformer变体等显著模型，以及优化这些模型的常见策略，包括剪枝、量化、知识蒸馏和神经架构搜索。除了详细解释这些方法外，我们还提供了定制轻量级模型的实用指导，通过分析其各自的优缺点提供了清晰的认识。
- en: Furthermore, we discussed hardware acceleration for DL models, delving into
    hardware architectures, distinct data flow types and data locality optimization
    techniques, and DL libraries to enhance comprehension of accelerating the training
    and inference processes. This investigation sheds light on the intricate interplay
    between hardware and software (Co-design), providing insights into expediting
    training and inference processes from a hardware perspective. Finally, we turn
    our gaze toward the future, recognizing that the deployment of lightweight DL
    models in TinyML and LLM technologies presents challenges that demand the exploration
    of creative solutions in these evolving fields.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们讨论了DL模型的硬件加速，*深入探讨*了硬件架构、不同的数据流类型和数据局部性优化技术，以及DL库，以增强对加速训练和推理过程的理解。这项研究揭示了硬件和软件（Co-design）之间的复杂相互作用，从硬件角度提供了加速训练和推理过程的见解。最后，我们展望未来，认识到轻量级DL模型在TinyML和LLM技术中的应用面临的挑战，这些挑战要求在这些不断发展的领域中探索创造性的解决方案。
- en: 7\. Acknowledgement
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 致谢
- en: 'This work is partially supported by the National Science and Technology Council,
    Taiwan under Grants, NSTC-112-2628-E-002-033-MY4, NSTC-112-2634-F-002-002-MBK,
    and NSTC-112-2218-E-A49-023, and was financially supported in part (project number:
    112UA10019) by the Co-creation Platform of the Industry Academia Innovation School,
    NYCU, under the framework of the National Key Fields Industry-University Cooperation
    and Skilled Personnel Training Act, from the Ministry of Education (MOE) and industry
    partners in Taiwan.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分由台湾国家科学技术委员会资助，资助编号 NSTC-112-2628-E-002-033-MY4、NSTC-112-2634-F-002-002-MBK
    和 NSTC-112-2218-E-A49-023，并且在一定程度上（项目编号：112UA10019）由国立阳明交通大学产业学合作及技能人才培训法框架下的共创平台资助，来自教育部（MOE）及台湾的产业合作伙伴。
- en: References
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （1）
- en: 'Abadi et al. (2016) M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
    M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. 2016. TensorFlow: A system
    for large-scale machine learning. In *OSDI*. 265–283.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abadi 等人（2016）M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
    S. Ghemawat, G. Irving, M. Isard 等人。2016。TensorFlow：一个大规模机器学习系统。在 *OSDI*。265–283。
- en: Abdelfattah et al. (2021) M. S. Abdelfattah, A. Mehrotra, Ł. Dudziak, and N. D.
    Lane. 2021. Zero-Cost Proxies for Lightweight NAS. (2021).
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdelfattah 等人（2021）M. S. Abdelfattah, A. Mehrotra, Ł. Dudziak 和 N. D. Lane。2021。轻量级
    NAS 的零成本代理。（2021）。
- en: AIM (2022) AIM. 2022. *Advances in Image Manipulation workshop in conjunction
    with ECCV 2022*. Retrieved November 2, 2023 from [https://data.vision.ee.ethz.ch/cvl/aim22/](https://data.vision.ee.ethz.ch/cvl/aim22/)
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AIM（2022）AIM。2022。*与 ECCV 2022 一同举行的图像处理进展研讨会*。检索于2023年11月2日，来自 [https://data.vision.ee.ethz.ch/cvl/aim22/](https://data.vision.ee.ethz.ch/cvl/aim22/)
- en: Amodei and Hernandez (2018) D. Amodei and D. Hernandez. 2018. *AI and Compute*.
    Retrieved November 2, 2023 from [https://openai.com/blog/ai-and-compute](https://openai.com/blog/ai-and-compute)
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amodei 和 Hernandez（2018）D. Amodei 和 D. Hernandez。2018。*AI 和计算*。检索于2023年11月2日，来自
    [https://openai.com/blog/ai-and-compute](https://openai.com/blog/ai-and-compute)
- en: An et al. (2022) S. An, Q. Liao, Z. Lu, and J.-H. Xue. 2022. Efficient semantic
    segmentation via self-attention and self-distillation. *T-ITS* 23, 9 (2022), 15256–15266.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: An 等人（2022）S. An, Q. Liao, Z. Lu 和 J.-H. Xue。2022。通过自注意力和自蒸馏实现高效语义分割。*T-ITS*
    23, 9（2022），15256–15266。
- en: Anil et al. (2023) R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A.
    Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. 2023. PaLM 2 technical
    report. *arXiv preprint arXiv:2305.10403* (2023).
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil 等人（2023）R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,
    S. Shakeri, E. Taropa, P. Bailey, Z. Chen 等人。2023。PaLM 2 技术报告。*arXiv 预印本 arXiv:2305.10403*（2023）。
- en: Asperti et al. (2021) A. Asperti, D. Evangelista, and M. Marzolla. 2021. Dissecting
    FLOPs along input dimensions for GreenAI cost estimations. In *LOD*. 86–100.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Asperti 等人（2021）A. Asperti, D. Evangelista 和 M. Marzolla。2021。沿输入维度剖析 FLOPs
    用于 GreenAI 成本估算。在 *LOD*。86–100。
- en: 'Banbury et al. (2021) C. Banbury, C. Zhou, I. Fedorov, R. Matas, U. Thakker,
    D. Gope, V. Janapa Reddi, M. Mattina, and P. Whatmough. 2021. MicroNets: Neural
    network architectures for deploying TinyML applications on commodity microcontrollers.
    *MLSys* 3 (2021).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banbury 等人（2021）C. Banbury, C. Zhou, I. Fedorov, R. Matas, U. Thakker, D. Gope,
    V. Janapa Reddi, M. Mattina 和 P. Whatmough。2021。MicroNets：用于在商品微控制器上部署 TinyML
    应用的神经网络架构。*MLSys* 3（2021）。
- en: Banner et al. (2018) R. Banner, I. Hubara, E. Hoffer, and D. Soudry. 2018. Scalable
    methods for 8-bit training of neural networks. *NIPS* 31 (2018).
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banner 等人（2018）R. Banner, I. Hubara, E. Hoffer 和 D. Soudry。2018。8位神经网络训练的可扩展方法。*NIPS*
    31（2018）。
- en: Bastian (2023) M. Bastian. 2023. *GPT-4 has more than a trillion parameters
    - Report*. Retrieved March 1, 2024 from [https://the-decoder.com/gpt-4-has-a-trillion-parameters/](https://the-decoder.com/gpt-4-has-a-trillion-parameters/)
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bastian（2023）M. Bastian。2023。*GPT-4 拥有超过一万亿个参数 - 报告*。检索于2024年3月1日，来自 [https://the-decoder.com/gpt-4-has-a-trillion-parameters/](https://the-decoder.com/gpt-4-has-a-trillion-parameters/)
- en: 'Berthelier et al. (2021) A. Berthelier, T. Chateau, S. Duffner, C. Garcia,
    and C. Blanc. 2021. Deep model compression and architecture optimization for embedded
    systems: A survey. *JSPS* 93, 8 (2021), 863–878.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berthelier 等人（2021）A. Berthelier, T. Chateau, S. Duffner, C. Garcia 和 C. Blanc。2021。嵌入式系统的深度模型压缩与架构优化：综述。*JSPS*
    93, 8（2021），863–878。
- en: Booshehri et al. (2013) M. Booshehri, A. Malekpour, and P. Luksch. 2013. An
    improving method for loop unrolling. *IJCSIS* 11, 5 (2013), 73–76.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Booshehri 等人（2013）M. Booshehri, A. Malekpour 和 P. Luksch。2013。改进的循环展开方法。*IJCSIS*
    11, 5（2013），73–76。
- en: 'Cai et al. (2020a) H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han. 2020a. Once-for-All:
    Train One Network and Specialize it for Efficient Deployment. In *ICLR*.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等人（2020a）H. Cai, C. Gan, T. Wang, Z. Zhang 和 S. Han。2020a。Once-for-All：训练一个网络并将其专门化以实现高效部署。在
    *ICLR*。
- en: 'Cai et al. (2020b) Y. Cai, Z. Yao, Z. Dong, A. Gholami, M. W. Mahoney, and
    K. Keutzer. 2020b. ZeroQ: A novel zero shot quantization framework. In *CVPR*.
    13169–13178.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai等（2020b）Y. Cai，Z. 姚，Z. 董，A. Gholami，M. W. Mahoney和K. Keutzer。2020b。《ZeroQ：一种新颖的零样本量化框架》。在*CVPR*。13169–13178。
- en: 'Capotondi et al. (2020) A. Capotondi, M. Rusci, M. Fariselli, and L. Benini.
    2020. CMix-NN: Mixed low-precision CNN library for memory-constrained edge devices.
    *TCAS-II* 67, 5 (2020), 871–875.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Capotondi等（2020）A. Capotondi，M. Rusci，M. Fariselli和L. Benini。2020。《CMix-NN：适用于内存受限边缘设备的混合低精度CNN库》。《TCAS-II》67，5（2020），871–875。
- en: 'Capra et al. (2020) M. Capra, B. Bussolino, A. Marchisio, G. Masera, M. Martina,
    and M. Shafique. 2020. Hardware and Software Optimizations for Accelerating Deep
    Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead. *IEEE
    Access* 8 (2020), 225134–225180.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Capra等（2020）M. Capra，B. Bussolino，A. Marchisio，G. Masera，M. Martina和M. Shafique。2020。《加速深度神经网络的硬件和软件优化：当前趋势、挑战和未来的道路》。《IEEE
    Access》8（2020），225134–225180。
- en: 'Chen et al. (2020a) B. Chen, T. Medini, J. Farwell, C. Tai, A. Shrivastava,
    et al. 2020a. SLIDE: In defense of smart algorithms over hardware acceleration
    for large-scale deep learning systems. *MLSys* 2 (2020), 291–306.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2020a）B. 陈，T. Medini，J. Farwell，C. Tai，A. Shrivastava等。2020a。《SLIDE：为大规模深度学习系统辩护智能算法而非硬件加速》。*MLSys*
    2（2020），291–306。
- en: 'Chen et al. (2021b) C.-Y. Chen, L. Lo, P.-J. Huang, H.-H. Shuai, and W.-H.
    Cheng. 2021b. Fashionmirror: Co-attention feature-remapping virtual try-on with
    sequential template poses. In *ICCV*. 13809–13818.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021b）C.-Y. 陈，L. 罗，P.-J. 黄，H.-H. 帅和W.-H. 程。2021b。《Fashionmirror：具有顺序模板姿势的共同注意力特征重映射虚拟试穿》。在*ICCV*。13809–13818。
- en: Chen et al. (2022b) D. Chen, J.-P. Mei, H. Zhang, C. Wang, Y. Feng, and C. Chen.
    2022b. Knowledge distillation with the reused teacher classifier. In *CVPR*. 11933–11942.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2022b）D. 陈，J.-P. Mei，H. 张，C. 王，Y. 凤和C. 陈。2022b。《使用重复教师分类器的知识蒸馏》。在*CVPR*。11933–11942。
- en: Chen et al. (2021c) D. Chen, J.-P. Mei, Y. Zhang, C. Wang, Z. Wang, Y. Feng,
    and C. Chen. 2021c. Cross-layer distillation with semantic calibration. In *AAAI*,
    Vol. 35\. 7028–7036.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021c）D. 陈，J.-P. Mei，Y. 张，C. 王，Z. 王，Y. 凤和C. 陈。2021c。《跨层蒸馏与语义校准》。在*AAAI*，第35卷，7028–7036。
- en: 'Chen et al. (2020b) H. Chen, Y. Wang, C. Xu, B. Shi, C. Xu, Q. Tian, and C.
    Xu. 2020b. AdderNet: Do We Really Need Multiplications in Deep Learning?. In *CVPR*.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2020b）H. 陈，Y. 王，C. 徐，B. 石，C. 徐，Q. 田和C. 徐。2020b。《AdderNet：我们在深度学习中真的需要乘法吗？》。在*CVPR*会议上。
- en: Chen et al. (2021a) P. Chen, S. Liu, H. Zhao, and J. Jia. 2021a. Distilling
    knowledge via knowledge review. In *CVPR*. 5008–5017.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021a）P. 陈，S. 刘，H. 赵和J. 贾。2021a。《通过知识回顾蒸馏知识》。在*CVPR*。5008–5017。
- en: 'Chen et al. (2014a) T. Chen, Z. Du, N. Sun, J. Wang, C. Wu, Y. Chen, and O.
    Temam. 2014a. DianNao: A small-footprint high-throughput accelerator for ubiquitous
    machine-learning. *ACM SIGARCH Computer Architecture News* 42, 1 (2014), 269–284.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2014a）T. 陈，Z. 杜，N. 孙，J. 王，C. 吴，Y. 陈和O. Temam。2014a。《DianNao：用于普及机器学习的小型高吞吐量加速器》。《ACM
    SIGARCH Computer Architecture News》42，1（2014），269–284。
- en: 'Chen et al. (2016) T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao,
    B. Xu, C. Zhang, and Z. Zhang. 2016. MXNet: A flexible and efficient machine learning
    library for heterogeneous distributed systems. *NIPSW*.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2016）T. 陈，M. 李，Y. 李，M. 林，N. 王，M. 王，T. 肖，B. 徐，C. 张和Z. 张。2016。《MXNet：用于异构分布式系统的灵活高效机器学习库》。*NIPSW*。
- en: 'Chen et al. (2020c) W. Chen, Y. Wang, S. Yang, C. Liu, and L. Zhang. 2020c.
    You Only Search Once: A Fast Automation Framework for Single-Stage DNN/Accelerator
    Co-design. In *DATE*. 1283–1286.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2020c）W. 陈，Y. 王，S. 杨，C. 刘和L. 张。2020c。《你只需搜索一次：用于单阶段DNN/加速器共设计的快速自动化框架》。在*DATE*。1283–1286。
- en: 'Chen et al. (2019a) W. Chen, D. Xie, Y. Zhang, and S. Pu. 2019a. All you need
    is a few shifts: Designing efficient convolutional neural networks for image classification.
    In *CVPR*. 7241–7250.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2019a）W. 陈，D. 谢，Y. 张和S. 蒲。2019a。《你需要的只是几个移位：为图像分类设计高效卷积神经网络》。在*CVPR*。7241–7250。
- en: 'Chen et al. (2022a) Y. Chen, X. Dai, D. Chen, M. Liu, X. Dong, L. Yuan, and
    Z. Liu. 2022a. Mobile-Former: Bridging MobileNet and Transformer. In *CVPR*. 5270–5279.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2022a）Y. 陈，X. 戴，D. 陈，M. 刘，X. 董，L. 袁和Z. 刘。2022a。《Mobile-Former：连接MobileNet和Transformer》。在*CVPR*。5270–5279。
- en: 'Chen et al. (2014b) Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li,
    T. Chen, Z. Xu, N. Sun, et al. 2014b. DaDianNao: A machine-learning supercomputer.
    In *MICRO*. 609–622.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2014b）Y. 陈，T. 罗，S. 刘，S. 张，L. 贺，J. 王，L. 李，T. 陈，Z. 徐，N. 孙等。2014b。《DaDianNao：一台机器学习超级计算机》。在*MICRO*。609–622。
- en: 'Chen et al. (2019b) Y. Chen, T. Yang, X. Zhang, G. Meng, C. Pan, and J. Sun.
    2019b. Detnas: Neural architecture search on object detection. *NIPS* 1, 2 (2019),
    4–1.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2019b）Y. 陈、T. 杨、X. 张、G. 孟、C. 潘 和 J. 孙。2019b。Detnas：对象检测中的神经网络架构搜索。*NIPS*
    1, 2（2019），4–1。
- en: 'Chetlur et al. (2014) S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J.
    Tran, B. Catanzaro, and E. Shelhamer. 2014. cuDNN: Efficient primitives for deep
    learning. *arXiv preprint arXiv:1410.0759* (2014).'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chetlur 等（2014）S. Chetlur、C. Woolley、P. Vandermersch、J. Cohen、J. Tran、B. Catanzaro
    和 E. Shelhamer。2014。cuDNN：深度学习的高效原语。*arXiv 预印本 arXiv:1410.0759*（2014）。
- en: Child et al. (2019) R. Child, S. Gray, A. Radford, and I. Sutskever. 2019. Generating
    long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509* (2019).
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child 等（2019）R. Child、S. Gray、A. Radford 和 I. Sutskever。2019。使用稀疏变换器生成长序列。*arXiv
    预印本 arXiv:1904.10509*（2019）。
- en: Cho et al. (2021) J. Cho, Y. Jung, S. Lee, and Y. Jung. 2021. Reconfigurable
    binary neural network accelerator with adaptive parallelism scheme. *Electronics*
    10, 3 (2021), 230.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等（2021）J. Cho、Y. Jung、S. Lee 和 Y. Jung。2021。具有自适应并行方案的可重构二进制神经网络加速器。*Electronics*
    10, 3（2021），230。
- en: 'Choi et al. (2018) J. Choi, Z. Wang, S. Venkataramani, P. I.-J. Chuang, V.
    Srinivasan, and K. Gopalakrishnan. 2018. Pact: Parameterized clipping activation
    for quantized neural networks. *arXiv preprint arXiv:1805.06085* (2018).'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等（2018）J. Choi、Z. Wang、S. Venkataramani、P. I.-J. Chuang、V. Srinivasan 和
    K. Gopalakrishnan。2018。Pact：用于量化神经网络的参数化剪裁激活。*arXiv 预印本 arXiv:1805.06085*（2018）。
- en: 'Choi et al. (2021) K. Choi, D. Hong, H. Yoon, J. Yu, Y. Kim, and J. Lee. 2021.
    Dance: Differentiable accelerator/network co-exploration. In *DAC*.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等（2021）K. Choi、D. Hong、H. Yoon、J. Yu、Y. Kim 和 J. Lee。2021。Dance：可微分加速器/网络共同探索。见
    *DAC*。
- en: 'Chollet (2017) F. Chollet. 2017. Xception: Deep learning with depthwise separable
    convolutions. In *CVPR*. 1251–1258.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet（2017）F. Chollet。2017。Xception：使用深度可分卷积的深度学习。见 *CVPR*。1251–1258。
- en: Choromanski et al. (2021) K. Choromanski, V. Likhosherstov, D. Dohan, X. Song,
    A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. 2021.
    Rethinking attention with performers. In *ICLR*.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choromanski 等（2021）K. Choromanski、V. Likhosherstov、D. Dohan、X. Song、A. Gane、T.
    Sarlos、P. Hawkins、J. Davis、A. Mohiuddin、L. Kaiser 等。2021。用表演者重新思考注意力。见 *ICLR*。
- en: 'Dai et al. (2021b) X. Dai, A. Wan, P. Zhang, B. Wu, Z. He, Z. Wei, K. Chen,
    Y. Tian, M. Yu, P. Vajda, et al. 2021b. Fbnetv3: Joint architecture-recipe search
    using predictor pretraining. In *CVPR*. 16276–16285.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等（2021b）X. Dai、A. Wan、P. Zhang、B. Wu、Z. He、Z. Wei、K. Chen、Y. Tian、M. Yu、P.
    Vajda 等。2021b。Fbnetv3：使用预测器预训练的联合架构-配方搜索。见 *CVPR*。16276–16285。
- en: 'Dai et al. (2021a) Z. Dai, H. Liu, Q. V. Le, and M. Tan. 2021a. CoAtNet: Marrying
    convolution and attention for all data sizes. *NIPS* 34 (2021), 3965–3977.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等（2021a）Z. Dai、H. Liu、Q. V. Le 和 M. Tan。2021a。CoAtNet：将卷积与注意力结合以适应所有数据规模。*NIPS*
    34（2021），3965–3977。
- en: 'David et al. (2021) R. David, J. Duke, A. Jain, V. Janapa Reddi, N. Jeffries,
    J. Li, N. Kreeger, I. Nappier, M. Natraj, T. Wang, P. Warden, and R. Rhodes. 2021.
    TensorFlow Lite Micro: Embedded Machine Learning for TinyML Systems. In *MLSys*,
    Vol. 3\. 800–811.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: David 等（2021）R. David、J. Duke、A. Jain、V. Janapa Reddi、N. Jeffries、J. Li、N. Kreeger、I.
    Nappier、M. Natraj、T. Wang、P. Warden 和 R. Rhodes。2021。TensorFlow Lite Micro：针对
    TinyML 系统的嵌入式机器学习。见 *MLSys*，第 3 卷，800–811。
- en: Deng et al. (2021) J. Deng, W. Li, Y. Chen, and L. Duan. 2021. Unbiased mean
    teacher for cross-domain object detection. In *CVPR*. 4091–4101.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等（2021）J. Deng、W. Li、Y. Chen 和 L. Duan。2021。无偏均值教师用于跨域对象检测。见 *CVPR*。4091–4101。
- en: Dong et al. (2017) X. Dong, S. Chen, and S. Pan. 2017. Learning to prune deep
    neural networks via layer-wise optimal brain surgeon. *NIPS* 30 (2017).
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2017）X. Dong、S. Chen 和 S. Pan。2017。通过逐层最优脑外科医生学习修剪深度神经网络。*NIPS* 30（2017）。
- en: 'Dong et al. (2020) Z. Dong, Z. Yao, D. Arfeen, A. Gholami, M. W. Mahoney, and
    K. Keutzer. 2020. Hawq-v2: Hessian aware trace-weighted quantization of neural
    networks. *NIPS* 33 (2020), 18518–18529.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2020）Z. Dong、Z. Yao、D. Arfeen、A. Gholami、M. W. Mahoney 和 K. Keutzer。2020。Hawq-v2：Hessian
    相关的跟踪加权神经网络量化。*NIPS* 33（2020），18518–18529。
- en: 'Dong et al. (2019) Z. Dong, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer.
    2019. Hawq: Hessian aware quantization of neural networks with mixed-precision.
    In *ICCV*. 293–302.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2019）Z. Dong、Z. Yao、A. Gholami、M. W. Mahoney 和 K. Keutzer。2019。Hawq：具有混合精度的神经网络的
    Hessian 相关量化。见 *ICCV*。293–302。
- en: 'Dosovitskiy et al. (2021) A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,
    X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,
    and N. Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition
    at Scale. In *ICLR*.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等（2021）A. Dosovitskiy、L. Beyer、A. Kolesnikov、D. Weissenborn、X. Zhai、T.
    Unterthiner、M. Dehghani、M. Minderer、G. Heigold、S. Gelly、J. Uszkoreit 和 N. Houlsby。2021。图像的价值是16x16个单词：用于大规模图像识别的变换器。见
    *ICLR*。
- en: Du et al. (2017) L. Du, Y. Du, Y. Li, J. Su, Y.-C. Kuan, C.-C. Liu, and M.-C. F.
    Chang. 2017. A reconfigurable streaming deep convolutional neural network accelerator
    for Internet of Things. *TCAS-I* 65, 1 (2017), 198–208.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等 (2017) L. Du, Y. Du, Y. Li, J. Su, Y.-C. Kuan, C.-C. Liu, 和 M.-C. F. Chang.
    2017. 一种可重配置的流式深度卷积神经网络加速器，用于物联网。*TCAS-I* 65, 1 (2017), 198–208.
- en: Dubey et al. (2019) S. Dubey, V. K. Soni, B. K. Dubey, et al. 2019. Application
    of Microcontroller in Assembly Line for Safety and Controlling. *IJRAR* 6, 1 (2019),
    107–111.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey 等 (2019) S. Dubey, V. K. Soni, B. K. Dubey, 等. 2019. 微控制器在生产线中的应用：安全与控制。*IJRAR*
    6, 1 (2019), 107–111.
- en: 'd’Ascoli et al. (2021) S. d’Ascoli, H. Touvron, M. L. Leavitt, A. S. Morcos,
    G. Biroli, and L. Sagun. 2021. Convit: Improving vision transformers with soft
    convolutional inductive biases. In *ICML*. 2286–2296.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: d’Ascoli 等 (2021) S. d’Ascoli, H. Touvron, M. L. Leavitt, A. S. Morcos, G. Biroli,
    和 L. Sagun. 2021. Convit：通过软卷积归纳偏差改进视觉变换器。发表于 *ICML*。2286–2296.
- en: 'Elhoushi et al. (2021) M. Elhoushi, Z. Chen, F. Shafiq, Y. H. Tian, and J. Y.
    Li. 2021. Deepshift: Towards multiplication-less neural networks. In *CVPR*. 2359–2368.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elhoushi 等 (2021) M. Elhoushi, Z. Chen, F. Shafiq, Y. H. Tian, 和 J. Y. Li. 2021.
    Deepshift：朝向无乘法神经网络。发表于 *CVPR*。2359–2368.
- en: Faghri et al. (2020) F. Faghri, I. Tabrizian, I. Markov, D. Alistarh, D. M.
    Roy, and A. Ramezani-Kebrya. 2020. Adaptive Gradient Quantization for Data-Parallel
    SGD. *NIPS* 33 (2020), 3174–3185.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faghri 等 (2020) F. Faghri, I. Tabrizian, I. Markov, D. Alistarh, D. M. Roy,
    和 A. Ramezani-Kebrya. 2020. 数据并行 SGD 的自适应梯度量化。*NIPS* 33 (2020), 3174–3185.
- en: Fan et al. (2021) Z. Fan, W. Hu, H. Guo, F. Liu, and D. Xu. 2021. Hardware and
    Algorithm Co-Optimization for pointwise convolution and channel shuffle in ShuffleNet
    V2\. In *SMC*. 3212–3217.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等 (2021) Z. Fan, W. Hu, H. Guo, F. Liu, 和 D. Xu. 2021. 硬件与算法的协同优化：在 ShuffleNet
    V2 中进行点卷积和通道洗牌的优化。发表于 *SMC*。3212–3217.
- en: 'Feurer et al. (2019) M. Feurer, A. Klein, K. Eggensperger, J. T. Springenberg,
    M. Blum, and F. Hutter. 2019. Auto-sklearn: efficient and robust automated machine
    learning. In *Automated Machine Learning*. 113–134.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feurer 等 (2019) M. Feurer, A. Klein, K. Eggensperger, J. T. Springenberg, M.
    Blum, 和 F. Hutter. 2019. Auto-sklearn：高效且稳健的自动化机器学习。在 *自动化机器学习* 中。113–134.
- en: Foundation (2017) L. Foundation. 2017. *ONNX*. Retrieved November 2, 2023 from
    [https://onnx.ai/](https://onnx.ai/)
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foundation (2017) L. Foundation. 2017. *ONNX*。检索日期：2023年11月2日，网址 [https://onnx.ai/](https://onnx.ai/)
- en: Fraccaroli et al. (2022) M. Fraccaroli, E. Lamma, and F. Riguzzi. 2022. Symbolic
    DNN-tuner. *Machine Learning* (2022), 1–26.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fraccaroli 等 (2022) M. Fraccaroli, E. Lamma, 和 F. Riguzzi. 2022. Symbolic DNN-tuner.
    *机器学习* (2022), 1–26.
- en: 'Frankle and Carbin (2019) J. Frankle and M. Carbin. 2019. The lottery ticket
    hypothesis: Finding sparse, trainable neural networks. *ICLR*.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle 和 Carbin (2019) J. Frankle 和 M. Carbin. 2019. 彩票票假设：寻找稀疏的、可训练的神经网络。*ICLR*.
- en: 'Frantar and Alistarh (2023) E. Frantar and D. Alistarh. 2023. SparseGPT: Massive
    Language Models Can Be Accurately Pruned in One-Shot. *arXiv preprint arXiv:2301.00774*
    (2023).'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 和 Alistarh (2023) E. Frantar 和 D. Alistarh. 2023. SparseGPT：大规模语言模型可以一次性准确修剪。*arXiv
    预印本 arXiv:2301.00774* (2023).
- en: Fu et al. (2023) Z. Fu, M. He, Z. Tang, and Y. Zhang. 2023. Optimizing data
    locality by executor allocation in spark computing environment. *ComSIS* 20, 1
    (2023), 491–512.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 (2023) Z. Fu, M. He, Z. Tang, 和 Y. Zhang. 2023. 在 Spark 计算环境中通过执行器分配优化数据局部性。*ComSIS*
    20, 1 (2023), 491–512.
- en: 'Getzner et al. (2023) J. Getzner, B. Charpentier, and S. Günnemann. 2023. Accuracy
    is not the only Metric that matters: Estimating the Energy Consumption of Deep
    Learning Models. In *ICLR*.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Getzner 等 (2023) J. Getzner, B. Charpentier, 和 S. Günnemann. 2023. 准确性并非唯一重要的指标：估算深度学习模型的能耗。发表于
    *ICLR*.
- en: Gholami et al. (2022) A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and
    K. Keutzer. 2022. A survey of quantization methods for efficient neural network
    inference. (2022), 291–326.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gholami 等 (2022) A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, 和 K. Keutzer.
    2022. 高效神经网络推理的量化方法综述。 (2022), 291–326.
- en: 'Gholami et al. (2018) A. Gholami, K. Kwon, B. Wu, Z. Tai, X. Yue, P. Jin, S.
    Zhao, and K. Keutzer. 2018. SqueezeNext: Hardware-aware neural network design.
    In *CVPRW*. 1638–1647.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gholami 等 (2018) A. Gholami, K. Kwon, B. Wu, Z. Tai, X. Yue, P. Jin, S. Zhao,
    和 K. Keutzer. 2018. SqueezeNext：面向硬件的神经网络设计。在 *CVPRW* 中。1638–1647.
- en: 'Gomez et al. (2017) A. N. Gomez, M. Ren, R. Urtasun, and R. B. Grosse. 2017.
    The reversible residual network: Backpropagation without storing activations.
    *NIPS* 30 (2017).'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gomez 等 (2017) A. N. Gomez, M. Ren, R. Urtasun, 和 R. B. Grosse. 2017. 可逆残差网络：无需存储激活的反向传播。*NIPS*
    30 (2017).
- en: Google (2023) Google. 2023. *Post-training quantization — TensorFlow Lite*.
    Retrieved November 2, 2023 from [https://www.tensorflow.org/lite/performance/post_training_quantization](https://www.tensorflow.org/lite/performance/post_training_quantization)
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google (2023) Google. 2023. *后训练量化 — TensorFlow Lite*. 于2023年11月2日从[https://www.tensorflow.org/lite/performance/post_training_quantization](https://www.tensorflow.org/lite/performance/post_training_quantization)检索
- en: 'Gou et al. (2021) J. Gou, B. Yu, S. J. Maybank, and D. Tao. 2021. Knowledge
    distillation: A survey. *IJCV* 129, 6 (2021), 1789–1819.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 苟等（2021）J. Gou, B. Yu, S. J. Maybank, 和 D. Tao. 2021. 知识蒸馏：一项调查。*IJCV* 129,
    6 (2021), 1789–1819.
- en: 'Graham et al. (2021) B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin,
    H. Jégou, and M. Douze. 2021. LeViT: a Vision Transformer in ConvNet’s Clothing
    for Faster Inference. In *ICCV*. 12259–12269.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graham等（2021）B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin, H. Jégou,
    和 M. Douze. 2021. LeViT：一个穿着ConvNet外衣的视觉变压器，用于更快的推断。在 *ICCV* 中。12259–12269.
- en: Gray and Neuhoff (1998) R. M. Gray and D. L. Neuhoff. 1998. Quantization. *TIT*
    44, 6 (1998), 2325–2383.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gray和Neuhoff（1998）R. M. Gray 和 D. L. Neuhoff. 1998. 量化。*TIT* 44, 6 (1998), 2325–2383.
- en: 'Guo et al. (2017) K. Guo, L. Sui, J. Qiu, J. Yu, J. Wang, S. Yao, S. Han, Y.
    Wang, and H. Yang. 2017. Angel-eye: A complete design flow for mapping CNN onto
    embedded FPGA. *TCAD* 37, 1 (2017), 35–47.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郭等（2017）K. Guo, L. Sui, J. Qiu, J. Yu, J. Wang, S. Yao, S. Han, Y. Wang, 和 H.
    Yang. 2017. Angel-eye：用于将CNN映射到嵌入式FPGA的完整设计流程。*TCAD* 37, 1 (2017), 35–47.
- en: Guo et al. (2020) Q. Guo, X. Wang, Y. Wu, Z. Yu, D. Liang, X. Hu, and P. Luo.
    2020. Online knowledge distillation via collaborative learning. In *CVPR*. 11020–11029.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郭等（2020）Q. Guo, X. Wang, Y. Wu, Z. Yu, D. Liang, X. Hu, 和 P. Luo. 2020. 在 *CVPR*
    中通过协作学习进行在线知识蒸馏。11020–11029.
- en: Guo et al. (2016) Y. Guo, A. Yao, and Y. Chen. 2016. Dynamic network surgery
    for efficient DNNs. *NIPS* 29 (2016).
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郭等（2016）Y. Guo, A. Yao, 和 Y. Chen. 2016. 用于高效DNN的动态网络手术。*NIPS* 29 (2016).
- en: 'Guo et al. (2023) Z. Guo, R. Zhang, L. Qiu, X. Ma, X. Miao, X. He, and B. Cui.
    2023. CALIP: Zero-shot enhancement of clip with parameter-free attention. In *AAAI*,
    Vol. 37\. 746–754.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郭等（2023）Z. Guo, R. Zhang, L. Qiu, X. Ma, X. Miao, X. He, 和 B. Cui. 2023. CALIP：无需参数的增强Clip的零-shot注意力。在
    *AAAI*, Vol. 37\. 746–754.
- en: 'Gupta and Agrawal (2022) M. Gupta and P. Agrawal. 2022. Compression of deep
    learning models for text: A survey. *TKDD* 16, 4 (2022), 1–55.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta和Agrawal（2022）M. Gupta 和 P. Agrawal. 2022. 对文本的深度学习模型进行压缩：一项调查。*TKDD* 16,
    4 (2022), 1–55.
- en: Gupta et al. (2015) S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan.
    2015. Deep learning with limited numerical precision. (2015), 1737–1746.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta等（2015）S. Gupta, A. Agrawal, K. Gopalakrishnan, 和 P. Narayanan. 2015. 有限数值精度下的深度学习。(2015),
    1737–1746.
- en: Gupta and Akin (2020) S. Gupta and B. Akin. 2020. Accelerator-aware Neural Network
    Design using AutoML. *MLSysW* (2020).
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta和Akin（2020）S. Gupta 和 B. Akin. 2020. 使用AutoML进行加速器感知神经网络设计。  *MLSysW* (2020).
- en: 'Ham et al. (2020) T. J. Ham, S. J. Jung, S. Kim, Y. H. Oh, Y. Park, Y. Song,
    J.-H. Park, S. Lee, K. Park, J. W. Lee, et al. 2020. A^ 3: Accelerating attention
    mechanisms in neural networks with approximation. In *HPCA*. 328–341.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ham等（2020）T. J. Ham, S. J. Jung, S. Kim, Y. H. Oh, Y. Park, Y. Song, J.-H. Park,
    S. Lee, K. Park, J. W. Lee, 等。 2020. A^ 3：通过近似加速神经网络中的注意机制。在 *HPCA* 中。328–341.
- en: 'Ham et al. (2021) T. J. Ham, Y. Lee, S. H. Seo, S. Kim, H. Choi, S. J. Jung,
    and J. W. Lee. 2021. ELSA: Hardware-Software co-design for efficient, lightweight
    self-attention mechanism in neural networks. In *ISCA*. 692–705.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ham等（2021）T. J. Ham, Y. Lee, S. H. Seo, S. Kim, H. Choi, S. J. Jung, 和 J. W.
    Lee. 2021. ELSA：用于神经网络中高效、轻量级自我注意机制的硬件-软件协同设计。在 *ISCA* 中。692–705.
- en: Han et al. (2023) K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang,
    A. Xiao, C. Xu, Y. Xu, Z. Yang, Y. Zhang, and D. Tao. 2023. A Survey on Vision
    Transformer. *TPAMI* 45, 1 (2023), 87–110.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 韩等（2023）K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao,
    C. Xu, Y. Xu, Z. Yang, Y. Zhang, 和 D. Tao. 2023. 有关Vision Transformer的调查。*TPAMI*
    45, 1 (2023), 87–110.
- en: 'Han et al. (2016) S. Han, H. Mao, and W. J. Dally. 2016. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. In *ICLR*.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 韩等（2016）S. Han, H. Mao, 和 W. J. Dally. 2016. 深度压缩：通过修剪、训练后量化和哈夫曼编码压缩深度神经网络。在
    *ICLR* 中。
- en: Hassibi et al. (1993) B. Hassibi, D. G. Stork, and G. J. Wolff. 1993. Optimal
    brain surgeon and general network pruning. In *ICNN*. 293–299.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi等（1993）B. Hassibi, D. G. Stork, 和 G. J. Wolff. 1993. 最优脑外科医生和通用网络修剪。在
    *ICNN* 中。293–299.
- en: He et al. (2016) K. He, X. Zhang, S. Ren, and J. Sun. 2016. Deep residual learning
    for image recognition. In *CVPR*. 770–778.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等（2016）K. He, X. Zhang, S. Ren, 和 J. Sun. 2016. 图像识别的深度残差学习。在 *CVPR* 中。770–778.
- en: 'He et al. (2021) X. He, K. Zhao, and X. Chu. 2021. AutoML: A Survey of the
    State-of-the-Art. *Knowledge-Based Systems* 212 (2021), 106622.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. (2021) X. He, K. Zhao, 和 X. Chu. 2021. AutoML: 最先进技术的综述。*Knowledge-Based
    Systems* 212 (2021), 106622。'
- en: He et al. (2020) Y. He, Y. Ding, P. Liu, L. Zhu, H. Zhang, and Y. Yang. 2020.
    Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration.
    In *CVPR*. 2006–2015.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2020) Y. He, Y. Ding, P. Liu, L. Zhu, H. Zhang, 和 Y. Yang. 2020.
    为深度卷积神经网络加速学习滤波器剪枝标准。发表于 *CVPR*。2006–2015。
- en: He et al. (2018) Y. He, G. Kang, X. Dong, Y. Fu, and Y. Yang. 2018. Soft filter
    pruning for accelerating deep convolutional neural networks. *IJCAI* (2018), 2234–2240.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2018) Y. He, G. Kang, X. Dong, Y. Fu, 和 Y. Yang. 2018. 通过软滤波器剪枝加速深度卷积神经网络。*IJCAI*
    (2018), 2234–2240。
- en: He et al. (2019a) Y. He, P. Liu, Z. Wang, Z. Hu, and Y. Yang. 2019a. Filter
    pruning via geometric median for deep convolutional neural networks acceleration.
    In *CVPR*. 4340–4349.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2019a) Y. He, P. Liu, Z. Wang, Z. Hu, 和 Y. Yang. 2019a. 通过几何中位数的滤波器剪枝用于深度卷积神经网络加速。发表于
    *CVPR*。4340–4349。
- en: 'He et al. (2019b) Y. He, X. Liu, H. Zhong, and Y. Ma. 2019b. AddressNet: Shift-based
    primitives for efficient convolutional neural networks. In *WACV*. 1213–1222.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. (2019b) Y. He, X. Liu, H. Zhong, 和 Y. Ma. 2019b. AddressNet: 基于移位的原语用于高效的卷积神经网络。发表于
    *WACV*。1213–1222。'
- en: He et al. (2017) Y. He, X. Zhang, and J. Sun. 2017. Channel pruning for accelerating
    very deep neural networks. In *CVPR*. 1389–1397.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2017) Y. He, X. Zhang, 和 J. Sun. 2017. 通道剪枝用于加速非常深的神经网络。发表于 *CVPR*。1389–1397。
- en: 'Hidayati et al. (2020) S. C. Hidayati, T. W. Goh, J.-S. G. Chan, C.-C. Hsu,
    J. See, L.-K. Wong, K.-L. Hua, Y. Tsao, and W.-H. Cheng. 2020. Dress with style:
    Learning style from joint deep embedding of clothing styles and body shapes. *TMM*
    23 (2020), 365–377.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hidayati et al. (2020) S. C. Hidayati, T. W. Goh, J.-S. G. Chan, C.-C. Hsu,
    J. See, L.-K. Wong, K.-L. Hua, Y. Tsao, 和 W.-H. Cheng. 2020. 时尚穿搭：通过服装风格和身体形状的联合深度嵌入学习风格。*TMM*
    23 (2020), 365–377。
- en: Hinton et al. (2015) G. Hinton, O. Vinyals, and J. Dean. 2015. Distilling the
    knowledge in a neural network. *arXiv preprint arXiv:1503.02531* (2015).
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton et al. (2015) G. Hinton, O. Vinyals, 和 J. Dean. 2015. 蒸馏神经网络中的知识。*arXiv
    预印本 arXiv:1503.02531* (2015)。
- en: Ho et al. (2020) J. Ho, A. Jain, and P. Abbeel. 2020. Denoising diffusion probabilistic
    models. *NIPS* 33 (2020), 6840–6851.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho et al. (2020) J. Ho, A. Jain, 和 P. Abbeel. 2020. 去噪扩散概率模型。*NIPS* 33 (2020),
    6840–6851。
- en: Hou et al. (2019) Y. Hou, Z. Ma, C. Liu, and C. C. Loy. 2019. Learning lightweight
    lane detection CNNs by self attention distillation. In *ICCV*. 1013–1021.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou et al. (2019) Y. Hou, Z. Ma, C. Liu, 和 C. C. Loy. 2019. 通过自注意力蒸馏学习轻量级车道检测CNN。发表于
    *ICCV*。1013–1021。
- en: Howard et al. (2019) A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M.
    Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, et al. 2019. Searching for mobilenetv3\.
    In *ICCV*. 1314–1324.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard et al. (2019) A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M.
    Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, 等. 2019. 寻找 mobilenetv3。发表于 *ICCV*。1314–1324。
- en: 'Howard et al. (2017) A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
    T. Weyand, M. Andreetto, and H. Adam. 2017. MobileNets: Efficient convolutional
    neural networks for mobile vision applications. *arXiv preprint arXiv:1704.04861*
    (2017).'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Howard et al. (2017) A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
    T. Weyand, M. Andreetto, 和 H. Adam. 2017. MobileNets: 适用于移动视觉应用的高效卷积神经网络。*arXiv
    预印本 arXiv:1704.04861* (2017)。'
- en: 'Hsu et al. (2020) L.-C. Hsu, C.-T. Chiu, K.-T. Lin, H.-H. Chou, and Y.-Y. Pu.
    2020. ESSA: An energy-aware bit-serial streaming deep convolutional neural network
    accelerator. *JSA* 111 (2020), 101831.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hsu et al. (2020) L.-C. Hsu, C.-T. Chiu, K.-T. Lin, H.-H. Chou, 和 Y.-Y. Pu.
    2020. ESSA: 一种节能的位串行流深卷积神经网络加速器。*JSA* 111 (2020), 101831。'
- en: Hu et al. (2018) J. Hu, L. Shen, and G. Sun. 2018. Squeeze-and-excitation networks.
    In *CVPR*. 7132–7141.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2018) J. Hu, L. Shen, 和 G. Sun. 2018. 挤压与激励网络。发表于 *CVPR*。7132–7141。
- en: 'Hu et al. (2023) W. Hu, Z. Che, N. Liu, M. Li, J. Tang, C. Zhang, and J. Wang.
    2023. CATRO: Channel Pruning via Class-Aware Trace Ratio Optimization. *TNNLS*
    (2023), 1–13.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2023) W. Hu, Z. Che, N. Liu, M. Li, J. Tang, C. Zhang, 和 J. Wang.
    2023. CATRO: 通过类感知跟踪比优化的通道剪枝。*TNNLS* (2023), 1–13。'
- en: 'Huang et al. (2018) G. Huang, S. Liu, L. Van der Maaten, and K. Q. Weinberger.
    2018. CondenseNet: An efficient DenseNet using learned group convolutions. In
    *CVPR*. 2752–2761.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2018) G. Huang, S. Liu, L. Van der Maaten, 和 K. Q. Weinberger.
    2018. CondenseNet: 使用学习到的组卷积的高效 DenseNet。发表于 *CVPR*。2752–2761。'
- en: Huang et al. (2017) G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger.
    2017. Densely connected convolutional networks. In *CVPR*. 4700–4708.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2017) G. Huang, Z. Liu, L. Van Der Maaten, 和 K. Q. Weinberger.
    2017. 密集连接的卷积网络。发表于 *CVPR*。4700–4708。
- en: 'Huang and Leng (1999) J.-C. Huang and T. Leng. 1999. Generalized loop-unrolling:
    a method for program speedup. In *ASSET*. 244–248.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 和 Leng（1999）J.-C. Huang 和 T. Leng。1999。广义循环展开：一种程序加速的方法。在 *ASSET*。244–248。
- en: 'Huang and Wang (2019) Z. Huang and N. Wang. 2019. Like what you like: Knowledge
    distill via neuron selectivity transfer. In *ICLR*.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 和 Wang（2019）Z. Huang 和 N. Wang。2019。像你喜欢的那样：通过神经元选择性转移进行知识蒸馏。在 *ICLR*。
- en: Hubara et al. (2016) I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
    Y. Bengio. 2016. Binarized neural networks. In *NIPS*. 4114–4122.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara 等人（2016）I. Hubara、M. Courbariaux、D. Soudry、R. El-Yaniv 和 Y. Bengio。2016。二值化神经网络。在
    *NIPS*。4114–4122。
- en: 'Iandola et al. (2017) F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
    Dally, and K. Keutzer. 2017. SqueezeNet: AlexNet-level accuracy with 50x fewer
    parameters and¡ 0.5 MB model size. In *ICLR*.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iandola 等人（2017）F. N. Iandola、S. Han、M. W. Moskewicz、K. Ashraf、W. J. Dally 和
    K. Keutzer。2017。SqueezeNet：具有 50 倍更少参数和小于 0.5 MB 模型大小的 AlexNet 级别准确度。在 *ICLR*。
- en: Jacob et al. (2018) B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard,
    H. Adam, and D. Kalenichenko. 2018. Quantization and training of neural networks
    for efficient integer-arithmetic-only inference. (2018), 2704–2713.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob 等人（2018）B. Jacob、S. Kligys、B. Chen、M. Zhu、M. Tang、A. Howard、H. Adam 和
    D. Kalenichenko。2018。用于高效整数算术推理的神经网络量化与训练。（2018），2704–2713。
- en: Jeon and Kim (2018) Y. Jeon and J. Kim. 2018. Constructing fast network through
    deconstruction of convolution. *NIPS* 31 (2018).
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeon 和 Kim（2018）Y. Jeon 和 J. Kim。2018。通过卷积的解构构建快速网络。*NIPS* 31（2018）。
- en: Jia et al. (2022) M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan,
    and S.-N. Lim. 2022. Visual prompt tuning. In *ECCV*. 709–727.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等人（2022）M. Jia、L. Tang、B.-C. Chen、C. Cardie、S. Belongie、B. Hariharan 和 S.-N.
    Lim。2022。视觉提示调整。在 *ECCV*。709–727。
- en: 'Jouppi et al. (2023) N. Jouppi, G. Kurian, S. Li, P. Ma, R. Nagarajan, L. Nai,
    N. Patil, S. Subramanian, A. Swing, B. Towles, et al. 2023. TPU v4: An optically
    reconfigurable supercomputer for machine learning with hardware support for embeddings.
    In *ISCA*. 1–14.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jouppi 等人（2023）N. Jouppi、G. Kurian、S. Li、P. Ma、R. Nagarajan、L. Nai、N. Patil、S.
    Subramanian、A. Swing、B. Towles 等人。2023。TPU v4：一个光学可重构的机器学习超级计算机，具有嵌入硬件支持。在 *ISCA*。1–14。
- en: Jouppi et al. (2017) N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal,
    R. Bajwa, S. Bates, S. Bhatia, N. Boden, A. Borchers, et al. 2017. In-datacenter
    performance analysis of a tensor processing unit. In *ISCA*. 1–12.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jouppi 等人（2017）N. P. Jouppi、C. Young、N. Patil、D. Patterson、G. Agrawal、R. Bajwa、S.
    Bates、S. Bhatia、N. Boden、A. Borchers 等人。2017。张量处理单元的数据中心性能分析。在 *ISCA*。1–12。
- en: Jung et al. (2019) S. Jung, C. Son, S. Lee, J. Son, J.-J. Han, Y. Kwak, S. J.
    Hwang, and C. Choi. 2019. Learning to quantize deep networks by optimizing quantization
    intervals with task loss. In *CVPR*. 4350–4359.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jung 等人（2019）S. Jung、C. Son、S. Lee、J. Son、J.-J. Han、Y. Kwak、S. J. Hwang 和 C.
    Choi。2019。通过优化量化间隔与任务损失来学习量化深度网络。在 *CVPR*。4350–4359。
- en: Kang et al. (2023) B. Kang, X. Chen, D. Wang, H. Peng, and H. Lu. 2023. Exploring
    Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking. In
    *ICCV*. 9612–9621.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang 等人（2023）B. Kang、X. Chen、D. Wang、H. Peng 和 H. Lu。2023。探索轻量级层次化视觉变换器以实现高效视觉跟踪。在
    *ICCV*。9612–9621。
- en: Kang and Han (2020) M. Kang and B. Han. 2020. Operation-aware soft channel pruning
    using differentiable masks. In *ICML*. 7021–7032.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang 和 Han（2020）M. Kang 和 B. Han。2020。使用可微掩码的操作感知软通道剪枝。在 *ICML*。7021–7032。
- en: Kim et al. (2021) K. Kim, B. Ji, D. Yoon, and S. Hwang. 2021. Self-knowledge
    distillation with progressive refinement of targets. In *ICCV*. 6567–6576.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人（2021）K. Kim、B. Ji、D. Yoon 和 S. Hwang。2021。通过逐步细化目标进行自知识蒸馏。在 *ICCV*。6567–6576。
- en: 'Kitaev et al. (2020) N. Kitaev, Ł. Kaiser, and A. Levskaya. 2020. Reformer:
    The efficient transformer. In *ICLR*.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kitaev 等人（2020）N. Kitaev、Ł. Kaiser 和 A. Levskaya。2020。Reformer：高效的变换器。在 *ICLR*。
- en: 'Kotthoff et al. (2019) L. Kotthoff, C. Thornton, H. H. Hoos, F. Hutter, and
    K. Leyton-Brown. 2019. Auto-WEKA: Automatic model selection and hyperparameter
    optimization in WEKA. In *Automated Machine Learning*. 81–95.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kotthoff 等人（2019）L. Kotthoff、C. Thornton、H. H. Hoos、F. Hutter 和 K. Leyton-Brown。2019。Auto-WEKA：WEKA中的自动模型选择和超参数优化。在
    *Automated Machine Learning*。81–95。
- en: Krizhevsky et al. (2012) A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012.
    Imagenet classification with deep convolutional neural networks. *NIPS* 25 (2012),
    1097–1105.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等人（2012）A. Krizhevsky、I. Sutskever 和 G. E. Hinton。2012。使用深度卷积神经网络进行图像分类。*NIPS*
    25（2012），1097–1105。
- en: Kumar et al. (2019) S. Kumar, V. Bitorff, D. Chen, C. Chou, B. Hechtman, H.
    Lee, N. Kumar, P. Mattson, S. Wang, T. Wang, et al. 2019. Scale MLPerf-0.6 models
    on google TPU-v3 pods. *arXiv preprint arXiv:1909.09756* (2019).
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等人 (2019) S. Kumar, V. Bitorff, D. Chen, C. Chou, B. Hechtman, H. Lee,
    N. Kumar, P. Mattson, S. Wang, T. Wang 等人. 2019. 在 Google TPU-v3 节点上扩展 MLPerf-0.6
    模型。*arXiv preprint arXiv:1909.09756* (2019)。
- en: 'Lai et al. (2018) L. Lai, N. Suda, and V. Chandra. 2018. CMSIS-NN: Efficient
    neural network kernels for arm cortex-m CPUs. *arXiv preprint arXiv:1801.06601*
    (2018).'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lai 等人 (2018) L. Lai, N. Suda, 和 V. Chandra. 2018. CMSIS-NN: 针对 ARM Cortex-M
    CPU 的高效神经网络内核。*arXiv preprint arXiv:1801.06601* (2018)。'
- en: LeCun et al. (1989) Y. LeCun, J. Denker, and S. Solla. 1989. Optimal brain damage.
    *NIPS* 2 (1989).
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等人 (1989) Y. LeCun, J. Denker, 和 S. Solla. 1989. 最优脑损伤。*NIPS* 2 (1989)。
- en: 'Lee et al. (2019) N. Lee, T. Ajanthan, and P. H. Torr. 2019. Snip: Single-shot
    network pruning based on connection sensitivity. *ICLR*.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee 等人 (2019) N. Lee, T. Ajanthan, 和 P. H. Torr. 2019. Snip: 基于连接敏感性的单次网络剪枝。*ICLR*。'
- en: Li et al. (2017) H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. 2017.
    Pruning Filters for Efficient ConvNets. In *ICLR*.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2017) H. Li, A. Kadav, I. Durdanovic, H. Samet, 和 H. P. Graf. 2017. 用于高效
    ConvNets 的滤波器剪枝。发表于 *ICLR*。
- en: Li et al. (2016) N. Li, S. Takaki, Y. Tomiokay, and H. Kitazawa. 2016. A multistage
    dataflow implementation of a deep convolutional neural network based on FPGA for
    high-speed object recognition. In *SSIAI*. 165–168.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2016) N. Li, S. Takaki, Y. Tomiokay, 和 H. Kitazawa. 2016. 基于 FPGA 的深度卷积神经网络的多阶段数据流实现用于高速目标识别。发表于
    *SSIAI*。165–168。
- en: Li et al. (2023) S. Li, M. Lin, Y. Wang, Y. Wu, Y. Tian, L. Shao, and R. Ji.
    2023. Distilling a Powerful Student Model via Online Knowledge Distillation. *TNNLS*
    34, 11 (2023), 8743–8752.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2023) S. Li, M. Lin, Y. Wang, Y. Wu, Y. Tian, L. Shao, 和 R. Ji. 2023.
    通过在线知识蒸馏提炼强大的学生模型。*TNNLS* 34, 11 (2023), 8743–8752。
- en: Li et al. (2021) S. Li, M. Tan, R. Pang, A. Li, L. Cheng, Q. V. Le, and N. P.
    Jouppi. 2021. Searching for fast model families on datacenter accelerators. In
    *CVPR*. 8085–8095.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2021) S. Li, M. Tan, R. Pang, A. Li, L. Cheng, Q. V. Le, 和 N. P. Jouppi.
    2021. 在数据中心加速器上寻找快速模型家族。发表于 *CVPR*。8085–8095。
- en: 'Li et al. (2020) Y. Li, C. Hao, X. Zhang, X. Liu, Y. Chen, J. Xiong, W.-m.
    Hwu, and D. Chen. 2020. EDD: Efficient differentiable DNN architecture and implementation
    co-search for embedded ai solutions. In *DAC*. 1–6.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2020) Y. Li, C. Hao, X. Zhang, X. Liu, Y. Chen, J. Xiong, W.-m. Hwu,
    和 D. Chen. 2020. EDD: 高效的可微分 DNN 结构与实现协同搜索用于嵌入式 AI 解决方案。发表于 *DAC*。1–6。'
- en: 'Li et al. (2022) Y. Li, Y. Hu, F. Wu, and K. Li. 2022. DiVIT: Algorithm and
    architecture co-design of differential attention in vision transformer. *JSA*
    (2022), 102520.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2022) Y. Li, Y. Hu, F. Wu, 和 K. Li. 2022. DiVIT: 视觉变换器中差分注意力的算法与架构共同设计。*JSA*
    (2022), 102520。'
- en: 'Liang et al. (2021b) T. Liang, J. Glossner, L. Wang, S. Shi, and X. Zhang.
    2021b. Pruning and quantization for deep neural network acceleration: A survey.
    *Neurocomputing* 461 (2021), 370–403.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人 (2021b) T. Liang, J. Glossner, L. Wang, S. Shi, 和 X. Zhang. 2021b.
    深度神经网络加速的剪枝与量化：综述。*Neurocomputing* 461 (2021), 370–403。
- en: 'Liang et al. (2021a) Y. Liang, G. Chongjian, Z. Tong, Y. Song, J. Wang, and
    P. Xie. 2021a. EViT: Expediting Vision Transformers via Token Reorganizations.
    In *ICLR*.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang 等人 (2021a) Y. Liang, G. Chongjian, Z. Tong, Y. Song, J. Wang, 和 P. Xie.
    2021a. EViT: 通过 Token 重组加速 Vision Transformers。发表于 *ICLR*。'
- en: 'Lin et al. (2021) J. Lin, W.-M. Chen, H. Cai, C. Gan, and S. Han. 2021. MCUNetV2:
    Memory-efficient patch-based inference for tiny deep learning. In *NIPS*.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 (2021) J. Lin, W.-M. Chen, H. Cai, C. Gan, 和 S. Han. 2021. MCUNetV2:
    记忆高效的基于补丁的微型深度学习推理。发表于 *NIPS*。'
- en: 'Lin et al. (2020a) J. Lin, W.-M. Chen, Y. Lin, C. Gan, S. Han, et al. 2020a.
    MCUNet: Tiny deep learning on iot devices. *NIPS* 33 (2020), 11711–11722.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 (2020a) J. Lin, W.-M. Chen, Y. Lin, C. Gan, S. Han 等人. 2020a. MCUNet:
    在 IoT 设备上的微型深度学习。*NIPS* 33 (2020), 11711–11722。'
- en: Lin et al. (2022a) S. Lin, H. Xie, B. Wang, K. Yu, X. Chang, X. Liang, and G.
    Wang. 2022a. Knowledge distillation via the target-aware transformer. In *CVPR*.
    10915–10924.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 (2022a) S. Lin, H. Xie, B. Wang, K. Yu, X. Chang, X. Liang, 和 G. Wang.
    2022a. 通过目标感知变换器进行知识蒸馏。发表于 *CVPR*。10915–10924。
- en: Lin et al. (2022b) S. Lin, H. Xie, B. Wang, K. Yu, X. Chang, X. Liang, and G.
    Wang. 2022b. Knowledge Distillation via the Target-Aware Transformer. In *CVPR*.
    10915–10924.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 (2022b) S. Lin, H. Xie, B. Wang, K. Yu, X. Chang, X. Liang, 和 G. Wang.
    2022b. 通过目标感知变换器进行知识蒸馏。发表于 *CVPR*。10915–10924。
- en: Lin et al. (2020b) Y. Lin, D. Hafdi, K. Wang, Z. Liu, and S. Han. 2020b. Neural-hardware
    architecture search. *NIPSWS* (2020).
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 (2020b) Y. Lin, D. Hafdi, K. Wang, Z. Liu, 和 S. Han. 2020b. 神经硬件架构搜索。*NIPSWS*
    (2020)。
- en: Lin and Chang (2017) Y.-J. Lin and T. S. Chang. 2017. Data and hardware efficient
    design for convolutional neural network. *TCAS-I* 65, 5 (2017), 1642–1651.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin and Chang (2017) Y.-J. Lin and T. S. Chang. 2017. 卷积神经网络的数据和硬件高效设计。*TCAS-I*
    65, 5 (2017), 1642–1651。
- en: Liu et al. (2023) B. Liu, F. Li, X. Wang, B. Zhang, and J. Yan. 2023. Ternary
    weight networks. In *ICASSP*. 1–5.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) B. Liu, F. Li, X. Wang, B. Zhang, and J. Yan. 2023. 三值权重网络。见*ICASSP*。1–5。
- en: 'Liu et al. (2019b) H. Liu, K. Simonyan, and Y. Yang. 2019b. DARTS: Differentiable
    Architecture Search. (2019).'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019b) H. Liu, K. Simonyan, and Y. Yang. 2019b. DARTS：可微分架构搜索。（2019）。
- en: Liu et al. (2021c) L. Liu, S. Zhang, Z. Kuang, A. Zhou, J.-H. Xue, X. Wang,
    Y. Chen, W. Yang, Q. Liao, and W. Zhang. 2021c. Group fisher pruning for practical
    network compression. In *ICML*.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021c) L. Liu, S. Zhang, Z. Kuang, A. Zhou, J.-H. Xue, X. Wang,
    Y. Chen, W. Yang, Q. Liao, and W. Zhang. 2021c. 用于实际网络压缩的群体Fisher剪枝。见*ICML*。
- en: 'Liu et al. (2021b) X. Liu, M. Ye, D. Zhou, and Q. Liu. 2021b. Post-training
    quantization with multiple points: Mixed precision without mixed precision. In
    *AAAI*, Vol. 35\. 8697–8705.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021b) X. Liu, M. Ye, D. Zhou, and Q. Liu. 2021b. 多点后训练量化：无混合精度的混合精度。见*AAAI*，第35卷。8697–8705。
- en: 'Liu et al. (2022) Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y.
    Cao, Z. Zhang, L. Dong, F. Wei, and B. Guo. 2022. Swin Transformer V2: Scaling
    Up Capacity and Resolution. In *CVPR*. 12009–12019.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022) Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y.
    Cao, Z. Zhang, L. Dong, F. Wei, and B. Guo. 2022. Swin Transformer V2：扩展容量与分辨率。见*CVPR*。12009–12019。
- en: 'Liu et al. (2021a) Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin,
    and B. Guo. 2021a. Swin transformer: Hierarchical vision transformer using shifted
    windows. In *ICCV*. 10012–10022.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021a) Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin,
    and B. Guo. 2021a. Swin变压器：使用位移窗口的层次视觉变压器。见*ICCV*。10012–10022。
- en: 'Liu et al. (2019a) Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, K.-T. Cheng, and
    J. Sun. 2019a. Metapruning: Meta learning for automatic neural network channel
    pruning. In *ICCV*. 3296–3305.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019a) Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, K.-T. Cheng, and
    J. Sun. 2019a. 元剪枝：用于自动神经网络通道剪枝的元学习。见*ICCV*。3296–3305。
- en: Luo et al. (2022) G. Luo, Y. Zhou, X. Sun, Y. Wang, L. Cao, Y. Wu, F. Huang,
    and R. Ji. 2022. Towards lightweight transformer via group-wise transformation
    for vision-and-language tasks. *TIP* 31 (2022), 3386–3398.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2022) G. Luo, Y. Zhou, X. Sun, Y. Wang, L. Cao, Y. Wu, F. Huang,
    and R. Ji. 2022. 通过组级变换实现轻量化变压器用于视觉与语言任务。*TIP* 31 (2022), 3386–3398。
- en: 'Luo et al. (2016) T. Luo, S. Liu, L. Li, Y. Wang, S. Zhang, T. Chen, Z. Xu,
    O. Temam, and Y. Chen. 2016. DaDianNao: A neural network supercomputer. *IEEE
    TC* 66, 1 (2016), 73–88.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2016) T. Luo, S. Liu, L. Li, Y. Wang, S. Zhang, T. Chen, Z. Xu,
    O. Temam, and Y. Chen. 2016. DaDianNao：一个神经网络超级计算机。*IEEE TC* 66, 1 (2016), 73–88。
- en: 'Ma et al. (2018) N. Ma, X. Zhang, H.-T. Zheng, and J. Sun. 2018. ShuffleNet
    V2: Practical guidelines for efficient CNN architecture design. In *ECCV*. 116–131.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2018) N. Ma, X. Zhang, H.-T. Zheng, and J. Sun. 2018. ShuffleNet
    V2：高效CNN架构设计的实际指南。见*ECCV*。116–131。
- en: MAI (2021) MAI. 2021. *Mobile AI workshop 2021*. Retrieved November 2, 2023
    from [https://ai-benchmark.com/workshops/mai/2021/#challenges](https://ai-benchmark.com/workshops/mai/2021/#challenges)
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAI (2021) MAI. 2021. *2021年移动AI研讨会*。检索于2023年11月2日，网址为 [https://ai-benchmark.com/workshops/mai/2021/#challenges](https://ai-benchmark.com/workshops/mai/2021/#challenges)。
- en: MAI (2022) MAI. 2022. *Mobile AI workshop 2022*. Retrieved November 2, 2023
    from [https://ai-benchmark.com/workshops/mai/2022/#challenges](https://ai-benchmark.com/workshops/mai/2022/#challenges)
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAI (2022) MAI. 2022. *2022年移动AI研讨会*。检索于2023年11月2日，网址为 [https://ai-benchmark.com/workshops/mai/2022/#challenges](https://ai-benchmark.com/workshops/mai/2022/#challenges)。
- en: MAI (2023) MAI. 2023. *Mobile AI workshop 2023*. Retrieved November 2, 2023
    from [https://ai-benchmark.com/workshops/mai/2023/#challenges](https://ai-benchmark.com/workshops/mai/2023/#challenges)
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAI (2023) MAI. 2023. *2023年移动AI研讨会*。检索于2023年11月2日，网址为 [https://ai-benchmark.com/workshops/mai/2023/#challenges](https://ai-benchmark.com/workshops/mai/2023/#challenges)。
- en: 'Mehta et al. (2021) S. Mehta, M. Ghazvininejad, S. Iyer, L. Zettlemoyer, and
    H. Hajishirzi. 2021. Delight: Very deep and light-weight transformer. In *ICLR*.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehta et al. (2021) S. Mehta, M. Ghazvininejad, S. Iyer, L. Zettlemoyer, and
    H. Hajishirzi. 2021. Delight：非常深且轻量化的变压器。见*ICLR*。
- en: Mehta et al. (2018) S. Mehta, R. Koncel-Kedziorski, M. Rastegari, and H. Hajishirzi.
    2018. Pyramidal recurrent unit for language modeling. In *EMNLP*.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehta et al. (2018) S. Mehta, R. Koncel-Kedziorski, M. Rastegari, and H. Hajishirzi.
    2018. 用于语言建模的金字塔递归单元。见*EMNLP*。
- en: 'Mehta et al. (2020) S. Mehta, R. Koncel-Kedziorski, M. Rastegari, and H. Hajishirzi.
    2020. Define: Deep factorized input token embeddings for neural sequence modeling.
    In *ICLR*.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehta et al. (2020) S. Mehta, R. Koncel-Kedziorski, M. Rastegari, and H. Hajishirzi.
    2020. Define：用于神经序列建模的深度因子化输入标记嵌入。见*ICLR*。
- en: 'Mehta and Rastegari (2022) S. Mehta and M. Rastegari. 2022. Mobilevit: light-weight,
    general-purpose, and mobile-friendly vision transformer. In *ICLR*.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mehta 和 Rastegari (2022) S. Mehta 和 M. Rastegari. 2022. Mobilevit: 轻量级、通用型和移动友好的视觉变换器.
    在 *ICLR*.'
- en: Mezdour et al. (2023) L. Mezdour, K. Kadem, M. Merouani, A. S. Haichour, S.
    Amarasinghe, and R. Baghdadi. 2023. A Deep Learning Model for Loop Interchange.
    In *ACM SIGPLAN CC*. 50–60.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mezdour et al. (2023) L. Mezdour, K. Kadem, M. Merouani, A. S. Haichour, S.
    Amarasinghe, 和 R. Baghdadi. 2023. 用于循环交换的深度学习模型. 在 *ACM SIGPLAN CC*. 50–60.
- en: Micikevicius et al. (2018) P. Micikevicius, S. Narang, J. Alben, G. Diamos,
    E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and H.
    Wu. 2018. Mixed Precision Training. (2018).
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micikevicius et al. (2018) P. Micikevicius, S. Narang, J. Alben, G. Diamos,
    E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, 和 H.
    Wu. 2018. 混合精度训练. (2018).
- en: Naseer et al. (2021) M. M. Naseer, K. Ranasinghe, S. H. Khan, M. Hayat, F. Shahbaz Khan,
    and M.-H. Yang. 2021. Intriguing properties of vision transformers. *NIPS* 34
    (2021).
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naseer et al. (2021) M. M. Naseer, K. Ranasinghe, S. H. Khan, M. Hayat, F. Shahbaz
    Khan, 和 M.-H. Yang. 2021. 视觉变换器的有趣特性. *NIPS* 34 (2021).
- en: 'Nechi et al. (2023) A. Nechi, L. Groth, S. Mulhem, F. Merchant, R. Buchty,
    and M. Berekovic. 2023. FPGA-based Deep Learning Inference Accelerators: Where
    Are We Standing? *TRETS* 16, 4 (2023), 1–32.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nechi et al. (2023) A. Nechi, L. Groth, S. Mulhem, F. Merchant, R. Buchty, 和
    M. Berekovic. 2023. 基于FPGA的深度学习推理加速器：我们现在处于何处？ *TRETS* 16, 4 (2023), 1–32.
- en: 'NVIDIA (2023) NVIDIA. 2023. *NVIDIA CUDA-X: GPU Accelerated Libraries*. Retrieved
    November 2, 2023 from [https://developer.nvidia.com/gpu-accelerated-libraries](https://developer.nvidia.com/gpu-accelerated-libraries)'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'NVIDIA (2023) NVIDIA. 2023. *NVIDIA CUDA-X: GPU 加速库*. 取自 2023年11月2日 [https://developer.nvidia.com/gpu-accelerated-libraries](https://developer.nvidia.com/gpu-accelerated-libraries)'
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. (2023).
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. GPT-4 技术报告. (2023).
- en: 'Parashar et al. (2017) A. Parashar, M. Rhu, A. Mukkara, A. Puglielli, R. Venkatesan,
    B. Khailany, J. Emer, S. W. Keckler, and W. J. Dally. 2017. SCNN: An accelerator
    for compressed-sparse convolutional neural networks. *ACM SIGARCH Comput. Archit.
    News* 45, 2 (2017), 27–40.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Parashar et al. (2017) A. Parashar, M. Rhu, A. Mukkara, A. Puglielli, R. Venkatesan,
    B. Khailany, J. Emer, S. W. Keckler, 和 W. J. Dally. 2017. SCNN: 用于压缩稀疏卷积神经网络的加速器.
    *ACM SIGARCH 计算机架构新闻* 45, 2 (2017), 27–40.'
- en: 'Paszke et al. (2019) A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,
    G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. 2019. Pytorch:
    An imperative style, high-performance deep learning library. *NIPS* 32 (2019).'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paszke et al. (2019) A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,
    G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, 等. 2019. Pytorch: 一种命令式风格的高性能深度学习库.
    *NIPS* 32 (2019).'
- en: Peng et al. (2019) H. Peng, J. Wu, S. Chen, and J. Huang. 2019. Collaborative
    Channel Pruning for Deep Networks. In *ICML*. 5113–5122.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. (2019) H. Peng, J. Wu, S. Chen, 和 J. Huang. 2019. 深度网络的协同通道剪枝. 在
    *ICML*. 5113–5122.
- en: Pouransari et al. (2020) H. Pouransari, Z. Tu, and O. Tuzel. 2020. Least squares
    binary quantization of neural networks. In *CVPRW*. 698–699.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pouransari et al. (2020) H. Pouransari, Z. Tu, 和 O. Tuzel. 2020. 神经网络的最小二乘二进制量化.
    在 *CVPRW*. 698–699.
- en: Qi et al. (2022) Z. Qi, W. Chen, R. A. Naqvi, and K. Siddique. 2022. Designing
    Deep Learning Hardware Accelerator and Efficiency Evaluation. *Comput. Intell.
    and Neurosci.* 2022 (2022).
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi et al. (2022) Z. Qi, W. Chen, R. A. Naqvi, 和 K. Siddique. 2022. 设计深度学习硬件加速器和效率评估.
    *计算智能与神经科学* 2022 (2022).
- en: Qiu et al. (2016) J. Qiu, J. Wang, S. Yao, K. Guo, B. Li, E. Zhou, J. Yu, T.
    Tang, N. Xu, S. Song, et al. 2016. Going deeper with embedded FPGA platform for
    convolutional neural network. In *ACM FPGA*. 26–35.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu et al. (2016) J. Qiu, J. Wang, S. Yao, K. Guo, B. Li, E. Zhou, J. Yu, T.
    Tang, N. Xu, S. Song, 等. 2016. 利用嵌入FPGA平台深入研究卷积神经网络. 在 *ACM FPGA*. 26–35.
- en: Radosavovic et al. (2020) I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He,
    and P. Dollár. 2020. Designing network design spaces. In *CVPR*. 10428–10436.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radosavovic et al. (2020) I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He,
    和 P. Dollár. 2020. 设计网络设计空间. 在 *CVPR*. 10428–10436.
- en: 'Rao et al. (2021) Y. Rao, W. Zhao, B. Liu, J. Lu, J. Zhou, and C.-J. Hsieh.
    2021. Dynamicvit: Efficient vision transformers with dynamic token sparsification.
    *NIPS* 34 (2021), 13937–13949.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rao et al. (2021) Y. Rao, W. Zhao, B. Liu, J. Lu, J. Zhou, 和 C.-J. Hsieh. 2021.
    Dynamicvit: 具有动态令牌稀疏化的高效视觉变换器. *NIPS* 34 (2021), 13937–13949.'
- en: 'Rastegari et al. (2016) M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi.
    2016. XNOR-Net: Imagenet classification using binary convolutional neural networks.
    In *ECCV*. 525–542.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rastegari et al. (2016) M. Rastegari, V. Ordonez, J. Redmon, 和 A. Farhadi.
    2016. XNOR-Net: 使用二进制卷积神经网络进行Imagenet分类. 在 *ECCV*. 525–542.'
- en: 'Ray (2022) P. P. Ray. 2022. A review on TinyML: State-of-the-art and prospects.
    *Journal of King Saud University-Computer and Information Sciences* 34, 4 (2022),
    1595–1623.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ray（2022）P. P. Ray. 2022. *TinyML综述：前沿技术与展望*。*King Saud University计算机与信息科学学报*
    34, 4 (2022), 1595–1623。
- en: Real et al. (2017) E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J.
    Tan, Q. V. Le, and A. Kurakin. 2017. Large-scale evolution of image classifiers.
    In *ICML*. 2902–2911.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Real 等人（2017）E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan,
    Q. V. Le, 和 A. Kurakin. 2017. *大规模图像分类器进化*。发表于 *ICML*。2902–2911。
- en: 'Ren et al. (2021) P. Ren, Y. Xiao, X. Chang, P.-Y. Huang, Z. Li, X. Chen, and
    X. Wang. 2021. A comprehensive survey of neural architecture search: Challenges
    and solutions. *CSUR* 54, 4 (2021), 1–34.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人（2021）P. Ren, Y. Xiao, X. Chang, P.-Y. Huang, Z. Li, X. Chen, 和 X. Wang.
    2021. 《神经架构搜索的全面调查：挑战与解决方案》。*CSUR* 54, 4 (2021), 1–34。
- en: Roggen et al. (2022) D. Roggen, R. Cobden, A. Pouryazdan, and M. Zeeshan. 2022.
    Wearable FPGA platform for accelerated dsp and ai applications. In *PerComW*.
    66–69.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roggen 等人（2022）D. Roggen, R. Cobden, A. Pouryazdan, 和 M. Zeeshan. 2022. *用于加速DSP和AI应用的可穿戴FPGA平台*。发表于
    *PerComW*。66–69。
- en: Rokh et al. (2023) B. Rokh, A. Azarpeyvand, and A. Khanteymoori. 2023. A comprehensive
    survey on model quantization for deep neural networks in image classification.
    *TIST* 14, 6 (2023), 1–50.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rokh 等人（2023）B. Rokh, A. Azarpeyvand, 和 A. Khanteymoori. 2023. *关于图像分类中深度神经网络模型量化的全面调查*。*TIST*
    14, 6 (2023), 1–50。
- en: Rombach et al. (2022) R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B.
    Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion Models. In
    *CVPR*. 10684–10695.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rombach 等人（2022）R. Rombach, A. Blattmann, D. Lorenz, P. Esser, 和 B. Ommer. 2022.
    *使用潜在扩散模型进行高分辨率图像合成*。发表于 *CVPR*。10684–10695。
- en: Sakr et al. (2022) C. Sakr, S. Dai, R. Venkatesan, B. Zimmer, W. Dally, and
    B. Khailany. 2022. Optimal clipping and magnitude-aware differentiation for improved
    quantization-aware training. In *ICML*. 19123–19138.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakr 等人（2022）C. Sakr, S. Dai, R. Venkatesan, B. Zimmer, W. Dally, 和 B. Khailany.
    2022. *改进量化感知训练的最优裁剪与幅度感知微分*。发表于 *ICML*。19123–19138。
- en: 'Sandler et al. (2018) M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.
    Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. In *CVPR*.
    4510–4520.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sandler 等人（2018）M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, 和 L.-C. Chen. 2018.
    *Mobilenetv2：反向残差和线性瓶颈*。发表于 *CVPR*。4510–4520。
- en: Schwartz et al. (2020) R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni. 2020.
    Green ai. *CACM* 63, 12 (2020), 54–63.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwartz 等人（2020）R. Schwartz, J. Dodge, N. A. Smith, 和 O. Etzioni. 2020. 《绿色人工智能》。*CACM*
    63, 12 (2020), 54–63。
- en: 'Sekanina (2021) L. Sekanina. 2021. Neural architecture search and hardware
    accelerator co-search: A survey. *IEEE access* 9 (2021), 151337–151362.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sekanina（2021）L. Sekanina. 2021. *神经架构搜索与硬件加速器共同搜索：综述*。*IEEE Access* 9 (2021),
    151337–151362。
- en: 'Seng et al. (2021) K. P. Seng, P. J. Lee, and L. M. Ang. 2021. Embedded intelligence
    on FPGA: Survey, applications and challenges. *Electronics* 10, 8 (2021), 895.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seng 等人（2021）K. P. Seng, P. J. Lee, 和 L. M. Ang. 2021. *FPGA上的嵌入式智能：调查、应用和挑战*。*Electronics*
    10, 8 (2021), 895。
- en: Shang et al. (2023) Y. Shang, Z. Yuan, B. Xie, B. Wu, and Y. Yan. 2023. Post-training
    quantization on diffusion models. In *CVPR*. 1972–1981.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang 等人（2023）Y. Shang, Z. Yuan, B. Xie, B. Wu, 和 Y. Yan. 2023. *扩散模型的训练后量化*。发表于
    *CVPR*。1972–1981。
- en: Simonyan and Zisserman (2015) K. Simonyan and A. Zisserman. 2015. Very deep
    convolutional networks for large-scale image recognition. In *ICLR*.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman（2015）K. Simonyan 和 A. Zisserman. 2015. *用于大规模图像识别的非常深的卷积网络*。发表于
    *ICLR*。
- en: 'Sinha (2023) S. Sinha. 2023. *State of IoT 2023: Number of connected IoT devices
    growing 16% to 16.7 billion globally*. Retrieved November 2, 2023 from [https://iot-analytics.com/number-connected-iot-devices/](https://iot-analytics.com/number-connected-iot-devices/)'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sinha（2023）S. Sinha. 2023. *2023年物联网状态：全球连接的物联网设备数量增长16%达到167亿台*。2023年11月2日取自
    [https://iot-analytics.com/number-connected-iot-devices/](https://iot-analytics.com/number-connected-iot-devices/)
- en: Song et al. (2021) Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon,
    and B. Poole. 2021. Score-based generative modeling through stochastic differential
    equations. In *ICLR*.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人（2021）Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, 和
    B. Poole. 2021. *通过随机微分方程进行基于评分的生成建模*。发表于 *ICLR*。
- en: Srinivas et al. (2021) A. Srinivas, T.-Y. Lin, N. Parmar, J. Shlens, P. Abbeel,
    and A. Vaswani. 2021. Bottleneck transformers for visual recognition. In *CVPR*.
    16519–16529.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srinivas 等人（2021）A. Srinivas, T.-Y. Lin, N. Parmar, J. Shlens, P. Abbeel, 和
    A. Vaswani. 2021. *用于视觉识别的瓶颈变压器*。发表于 *CVPR*。16519–16529。
- en: Stoutchinin et al. (2019) A. Stoutchinin, F. Conti, and L. Benini. 2019. Optimally
    scheduling CNN convolutions for efficient memory access. *arXiv preprint arXiv:1902.01492*
    (2019).
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stoutchinin 等人（2019）A. Stoutchinin, F. Conti, 和 L. Benini. 2019. *优化调度CNN卷积以实现高效的内存访问*。*arXiv
    预印本 arXiv:1902.01492* (2019)。
- en: Strubell et al. (2019) E. Strubell, A. Ganesh, and A. McCallum. 2019. Energy
    and policy considerations for deep learning in NLP. *ACL*.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strubell et al. (2019) E. Strubell, A. Ganesh, and A. McCallum. 2019. 深度学习在自然语言处理中的能源与政策考虑。*ACL*。
- en: Su et al. (2020) Z. Su, L. Fang, W. Kang, D. Hu, M. Pietikäinen, and L. Liu.
    2020. Dynamic group convolution for accelerating convolutional neural networks.
    In *ECCV*. 138–155.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2020) Z. Su, L. Fang, W. Kang, D. Hu, M. Pietikäinen, and L. Liu.
    2020. 动态组卷积加速卷积神经网络。在 *ECCV*。138–155。
- en: Sultana et al. (2022) M. Sultana, M. Naseer, M. H. Khan, S. Khan, and F. S.
    Khan. 2022. Self-Distilled Vision Transformer for Domain Generalization. In *ACCV*.
    3068–3085.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sultana et al. (2022) M. Sultana, M. Naseer, M. H. Khan, S. Khan, and F. S.
    Khan. 2022. 用于领域泛化的自蒸馏视觉变换器。在 *ACCV*。3068–3085。
- en: Sun et al. (2023) M. Sun, Z. Liu, A. Bair, and J. Z. Kolter. 2023. A Simple
    and Effective Pruning Approach for Large Language Models. *arXiv preprint arXiv:2306.11695*
    (2023).
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2023) M. Sun, Z. Liu, A. Bair, and J. Z. Kolter. 2023. 一种简单有效的大型语言模型剪枝方法。*arXiv
    预印本 arXiv:2306.11695* (2023)。
- en: 'Sun et al. (2022) M. Sun, H. Ma, G. Kang, Y. Jiang, T. Chen, X. Ma, Z. Wang,
    and Y. Wang. 2022. VAQF: Fully Automatic Software-hardware Co-design Framework
    for Low-bit Vision Transformer. *arXiv preprint arXiv:2201.06618* (2022).'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2022) M. Sun, H. Ma, G. Kang, Y. Jiang, T. Chen, X. Ma, Z. Wang,
    and Y. Wang. 2022. VAQF: 完全自动的软件-硬件共同设计框架，用于低位视觉变换器。*arXiv 预印本 arXiv:2201.06618*
    (2022)。'
- en: Sun et al. (2020) Y. Sun, H. Wang, B. Xue, Y. Jin, G. G. Yen, and M. Zhang.
    2020. Surrogate-Assisted Evolutionary Deep Learning Using an End-to-End Random
    Forest-Based Performance Predictor. *TEVC* 24, 2 (2020), 350–364.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2020) Y. Sun, H. Wang, B. Xue, Y. Jin, G. G. Yen, and M. Zhang.
    2020. 使用端到端基于随机森林的性能预测器的代理辅助进化深度学习。*TEVC* 24, 2 (2020), 350–364。
- en: 'Sze et al. (2020) V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer. 2020. How
    to evaluate deep neural network processors: Tops/w (alone) considered harmful.
    *SSC-M* 12, 3 (2020), 28–41.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sze et al. (2020) V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer. 2020. 如何评估深度神经网络处理器：Tops/w（单独）被认为有害。*SSC-M*
    12, 3 (2020), 28–41。
- en: Szegedy et al. (2017) C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi. 2017.
    Inception-v4, inception-resnet and the impact of residual connections on learning.
    In *AAAI*.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy et al. (2017) C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi. 2017.
    Inception-v4、inception-resnet 以及残差连接对学习的影响。在 *AAAI*。
- en: Szegedy et al. (2015) C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
    D. Erhan, V. Vanhoucke, and A. Rabinovich. 2015. Going deeper with convolutions.
    In *CVPR*. 1–9.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy et al. (2015) C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
    D. Erhan, V. Vanhoucke, and A. Rabinovich. 2015. 通过卷积深入。在 *CVPR*。1–9。
- en: Szegedy et al. (2016) C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z.
    Wojna. 2016. Rethinking the inception architecture for computer vision. In *CVPR*.
    2818–2826.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy et al. (2016) C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z.
    Wojna. 2016. 重新思考计算机视觉的 inception 架构。在 *CVPR*。2818–2826。
- en: Talwalkar (2020) A. Talwalkar. 2020. *The push for energy efficient ”Green AI”*.
    Retrieved November 2, 2023 from [https://spectrum.ieee.org/energy-efficient-green-ai-strategies](https://spectrum.ieee.org/energy-efficient-green-ai-strategies)
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talwalkar (2020) A. Talwalkar. 2020. *推动能源高效的“绿色 AI”*。2023年11月2日获取自 [https://spectrum.ieee.org/energy-efficient-green-ai-strategies](https://spectrum.ieee.org/energy-efficient-green-ai-strategies)
- en: Tan et al. (2019b) J. Tan, L. Niu, J. K. Adams, V. Boominathan, J. T. Robinson,
    R. G. Baraniuk, and A. Veeraraghavan. 2019b. Face Detection and Verification Using
    Lensless Cameras. *TCI* 5, 2 (2019), 180–194.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan et al. (2019b) J. Tan, L. Niu, J. K. Adams, V. Boominathan, J. T. Robinson,
    R. G. Baraniuk, and A. Veeraraghavan. 2019b. 使用无镜头相机的人脸检测与验证。*TCI* 5, 2 (2019),
    180–194。
- en: 'Tan et al. (2019a) M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard,
    and Q. V. Le. 2019a. MnasNet: Platform-aware neural architecture search for mobile.
    In *CVPR*. 2820–2828.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan et al. (2019a) M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard,
    and Q. V. Le. 2019a. MnasNet: 面向平台的移动神经网络架构搜索。在 *CVPR*。2820–2828。'
- en: 'Tan and Le (2019a) M. Tan and Q. Le. 2019a. EfficientNet: Rethinking model
    scaling for convolutional neural networks. In *ICML*. 6105–6114.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan and Le (2019a) M. Tan and Q. Le. 2019a. EfficientNet: 重新思考卷积神经网络的模型扩展。在
    *ICML*。6105–6114。'
- en: 'Tan and Le (2021) M. Tan and Q. Le. 2021. EfficientNetV2: Smaller models and
    faster training. In *ICML*. 10096–10106.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan and Le (2021) M. Tan and Q. Le. 2021. EfficientNetV2: 更小的模型和更快的训练。在 *ICML*。10096–10106。'
- en: 'Tan and Le (2019b) M. Tan and Q. V. Le. 2019b. MixConv: Mixed depthwise convolutional
    kernels. (2019).'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan and Le (2019b) M. Tan and Q. V. Le. 2019b. MixConv: 混合深度卷积核。（2019）。'
- en: Tao et al. (2022) C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo,
    and N. Wong. 2022. Compression of generative pre-trained language models via quantization.
    In *ACL*.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tao et al. (2022) C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo,
    和 N. Wong. 2022. 通过量化压缩生成预训练语言模型. 载于 *ACL*.
- en: 'Tarvainen and Valpola (2017) A. Tarvainen and H. Valpola. 2017. Mean teachers
    are better role models: Weight-averaged consistency targets improve semi-supervised
    deep learning results. In *NIPS*, Vol. 30.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tarvainen and Valpola (2017) A. Tarvainen 和 H. Valpola. 2017. 平均教师是更好的榜样：权重平均一致性目标改善半监督深度学习结果.
    载于 *NIPS*, 第 30 卷.
- en: 'Tay et al. (2021) Y. Tay, M. Dehghani, D. Bahri, and D. Metzler. 2021. Efficient
    transformers: A survey. *CSUR* 54, 4 (2021), 1–41.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tay et al. (2021) Y. Tay, M. Dehghani, D. Bahri, 和 D. Metzler. 2021. 高效的变换器：一项调查.
    *CSUR* 54, 4 (2021), 1–41.
- en: Tian et al. (2020) Y. Tian, D. Krishnan, and P. Isola. 2020. Contrastive Representation
    Distillation. (2020).
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian et al. (2020) Y. Tian, D. Krishnan, 和 P. Isola. 2020. 对比表示蒸馏. (2020).
- en: Touvron et al. (2021) H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles,
    and H. Jégou. 2021. Training data-efficient image transformers & distillation
    through attention. In *ICML*. 10347–10357.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2021) H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles,
    和 H. Jégou. 2021. 通过注意力训练数据高效的图像变换器与蒸馏. 载于 *ICML*. 10347–10357.
- en: 'Touvron et al. (2023a) H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.
    Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. 2023a.
    Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*
    (2023).'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023a) H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.
    Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, 等. 2023a. Llama:
    开放且高效的基础语言模型. *arXiv preprint arXiv:2302.13971* (2023).'
- en: 'Touvron et al. (2023b) H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,
    Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. 2023b. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*
    (2023).'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023b) H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,
    Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, 等. 2023b. Llama 2:
    开放的基础和微调的聊天模型. *arXiv preprint arXiv:2307.09288* (2023).'
- en: Um et al. (2021) S. Um, S. Kim, S. Kim, and H.-J. Yoo. 2021. A 43.1 tops/w energy-efficient
    absolute-difference-accumulation operation computing-in-memory with computation
    reuse. *TCAS-II* 68, 5 (2021), 1605–1609.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Um et al. (2021) S. Um, S. Kim, S. Kim, 和 H.-J. Yoo. 2021. 一种 43.1 tops/w 能效的绝对差异累积操作的内存计算，具有计算重用.
    *TCAS-II* 68, 5 (2021), 1605–1609.
- en: Vanholder (2016) H. Vanholder. 2016. Efficient inference with tensorrt. In *GPU
    Technology Conference*, Vol. 1.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vanholder (2016) H. Vanholder. 2016. 使用 tensorrt 的高效推理. 载于 *GPU Technology Conference*,
    第 1 卷.
- en: Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, Ł. Kaiser, and I. Polosukhin. 2017. Attention is all you need. *NIPS*
    30 (2017).
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, Ł. Kaiser, 和 I. Polosukhin. 2017. Attention is all you need. *NIPS*
    30 (2017).
- en: 'Viet et al. (2021) L. N. Viet, T. N. Dinh, D. T. Minh, H. N. Viet, and Q. L.
    Tran. 2021. UET-Headpose: A sensor-based top-view head pose dataset. In *KSE*.
    1–7.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Viet et al. (2021) L. N. Viet, T. N. Dinh, D. T. Minh, H. N. Viet, 和 Q. L.
    Tran. 2021. UET-Headpose: 基于传感器的顶视头部姿态数据集. 载于 *KSE*. 1–7.'
- en: 'Wan et al. (2020) A. Wan, X. Dai, P. Zhang, Z. He, Y. Tian, S. Xie, B. Wu,
    M. Yu, T. Xu, K. Chen, et al. 2020. Fbnetv2: Differentiable neural architecture
    search for spatial and channel dimensions. In *CVPR*. 12965–12974.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wan et al. (2020) A. Wan, X. Dai, P. Zhang, Z. He, Y. Tian, S. Xie, B. Wu,
    M. Yu, T. Xu, K. Chen, 等. 2020. Fbnetv2: 空间和通道维度的可微分神经架构搜索. 载于 *CVPR*. 12965–12974.'
- en: 'Wang et al. (2021) H. Wang, Z. Zhang, and S. Han. 2021. Spatten: Efficient
    sparse attention architecture with cascade token and head pruning. In *HPCA*.
    97–110.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2021) H. Wang, Z. Zhang, 和 S. Han. 2021. Spatten: 高效的稀疏注意力架构，具有级联令牌和头部剪枝.
    载于 *HPCA*. 97–110.'
- en: Wang et al. (2022a) L. Wang, X. Dong, Y. Wang, L. Liu, W. An, and Y. Guo. 2022a.
    Learnable Lookup Table for Neural Network Quantization. In *CVPR*. 12423–12433.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022a) L. Wang, X. Dong, Y. Wang, L. Liu, W. An, 和 Y. Guo. 2022a.
    用于神经网络量化的可学习查找表. 载于 *CVPR*. 12423–12433.
- en: Wang et al. (2018) N. Wang, J. Choi, D. Brand, C.-Y. Chen, and K. Gopalakrishnan.
    2018. Training deep neural networks with 8-bit floating point numbers. In *NIPS*.
    7686–7695.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2018) N. Wang, J. Choi, D. Brand, C.-Y. Chen, 和 K. Gopalakrishnan.
    2018. 使用 8 位浮点数训练深度神经网络. 载于 *NIPS*. 7686–7695.
- en: 'Wang et al. (2020a) S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. 2020a.
    Linformer: Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*
    (2020).'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2020a) S. Wang, B. Z. Li, M. Khabsa, H. Fang, 和 H. Ma. 2020a.
    Linformer: 线性复杂度的自注意力. *arXiv preprint arXiv:2006.04768* (2020).'
- en: Wang et al. (2019) X. Wang, M. Kan, S. Shan, and X. Chen. 2019. Fully learnable
    group convolution for acceleration of deep neural networks. In *CVPR*. 9049–9058.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2019）X. 王，M. 刊，S. 单，和 X. 陈。2019年。完全可学习的组卷积加速深度神经网络。在*CVPR*。9049–9058。
- en: 'Wang et al. (2022b) X. Wang, L. L. Zhang, Y. Wang, and M. Yang. 2022b. Towards
    efficient vision transformer inference: a first study of transformers on mobile
    devices. In *WMCSA*. 1–7.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2022b）X. 王，L. L. 张，Y. 王，和 M. 杨。2022b。面向高效视觉变换器推理：移动设备上变换器的初步研究。在*WMCSA*。1–7。
- en: 'Wang et al. (2020b) Z. Wang, K. Xu, S. Wu, L. Liu, L. Liu, and D. Wang. 2020b.
    Sparse-YOLO: Hardware/software co-design of an FPGA accelerator for YOLOv2. *IEEE
    Access* 8 (2020), 116569–116585.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2020b）Z. 王，K. 许，S. 吴，L. 刘，L. 刘，和 D. 王。2020b。Sparse-YOLO：YOLOv2 的 FPGA 加速器的硬件/软件协同设计。在*IEEE
    Access* 8（2020），116569–116585。
- en: Wei et al. (2017) X. Wei, C. H. Yu, P. Zhang, Y. Chen, Y. Wang, H. Hu, Y. Liang,
    and J. Cong. 2017. Automated systolic array architecture synthesis for high throughput
    CNN inference on FPGAs. In *DAC*. 1–6.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等（2017）X. 魏，C. H. 余，P. 张，Y. 陈，Y. 王，H. 胡，Y. 梁，和 J. 孔。2017年。用于 FPGA 上高吞吐量 CNN
    推理的自动化 systolic array 架构合成。在*DAC*。1–6。
- en: Wolf and Lam (1991) M. E. Wolf and M. S. Lam. 1991. A data locality optimizing
    algorithm. In *PLDI*. 30–44.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 狼和蓝姆（1991）M. E. 狼和 M. S. 蓝姆。1991年。一个数据局部性优化算法。在*PLDI*。30–44。
- en: 'Wortsman et al. (2022) M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R.
    Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith,
    et al. 2022. Model soups: averaging weights of multiple fine-tuned models improves
    accuracy without increasing inference time. (2022), 23965–23998.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沃茨曼等（2022）M. 沃茨曼，G. 伊尔哈科，S. Y. 加德，R. 罗伊洛夫斯，R. 冯提霍-洛佩斯，A. S. 莫尔科斯，H. 南孔，A. 法尔哈迪，Y.
    卡门，S. 科恩布利斯，等。2022年。模型浓汤：平均多个微调模型的权重提高了准确性而不增加推理时间。（2022年），23965–23998。
- en: 'Wu et al. (2019) B. Wu, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian,
    P. Vajda, Y. Jia, and K. Keutzer. 2019. FBNet: Hardware-aware efficient ConvNet
    design via differentiable neural architecture search. In *CVPR*. 10734–10742.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '吴等（2019）B. 吴，X. 戴，P. 张，Y. 王，F. 孙，Y. 吴，Y. 田，P. 瓦伊达，Y. 贾，和 K. 克茨特。2019年。FBNet:
    通过可微分神经架构搜索的硬件感知高效 ConvNet 设计。在*CVPR*。10734–10742。'
- en: 'Wu et al. (2018) B. Wu, A. Wan, X. Yue, P. Jin, S. Zhao, N. Golmant, A. Gholaminejad,
    J. Gonzalez, and K. Keutzer. 2018. Shift: A zero flop, zero parameter alternative
    to spatial convolutions. In *CVPR*. 9127–9135.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2018）B. 吴，A. 万，X. 岳，P. 金，S. 赵，N. 哥尔曼，A. 赫拉米贾德，J. 冯冈，和 K. 克茨特。2018年。Shift：一个零浮点，零参数的空间卷积替代方案。在*CVPR*。9127–9135。
- en: 'Wu et al. (2021) H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L.
    Zhang. 2021. Cvt: Introducing convolutions to vision transformers. In *ICCV*.
    22–31.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '吴等（2021）H. 吴，B. 肖，N. 科德拉，M. 刘，X. 戴，L. 袁，和 L. 张。2021年。Cvt: 将卷积引入视觉变换器。在*ICCV*。22–31。'
- en: 'Wu et al. (2023) X. Wu, C. Li, R. Y. Aminabadi, Z. Yao, and Y. He. 2023. Understanding
    INT4 Quantization for Transformer Models: Latency Speedup, Composability, and
    Failure Cases. *arXiv preprint arXiv:2301.12017* (2023).'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2023）X. 吴，C. 李，R. Y. 阿米纳巴迪，Z. 姚，和 Y. 赫。2023年。理解 INT4 量化在变换器模型中的作用：延迟加速、可组合性和失败案例。*arXiv
    预印本 arXiv:2301.12017*（2023）。
- en: Wu et al. (2020) Z. Wu, Z. Liu, J. Lin, Y. Lin, and S. Han. 2020. Lite transformer
    with long-short range attention. *ICLR*.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2020）Z. 吴，Z. 刘，J. 林，Y. 林，和 S. 韩。2020年。长短期注意力的轻量级变换器。在*ICLR*。
- en: Xiao et al. (2021) T. Xiao, P. Dollar, M. Singh, E. Mintun, T. Darrell, and
    R. Girshick. 2021. Early convolutions help transformers see better. *NIPS* 34
    (2021).
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肖等（2021）T. 肖，P. 道拉，M. 辛格，E. 明顿，T. 达雷尔，和 R. 吉尔希克。2021年。早期卷积帮助变换器更好地理解。在*NIPS*
    34（2021）。
- en: Xie et al. (2023) H. Xie, M.-X. Lee, T.-J. Chen, H.-J. Chen, H.-I. Liu, H.-H.
    Shuai, and W.-H. Cheng. 2023. Most Important Person-guided Dual-branch Cross-Patch
    Attention for Group Affect Recognition. In *ICCV*. 20598–20608.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谢等（2023）H. 谢，M.-X. 李，T.-J. 陈，H.-J. 陈，H.-I. 刘，H.-H. 帅，和 W.-H. 程。2023年。最重要的人员引导的双分支跨补丁注意力用于群体情感识别。在*ICCV*。20598–20608。
- en: Xu et al. (2023) R. Xu, E. H.-M. Sha, Q. Zhuge, Y. Song, and H. Wang. 2023.
    Loop interchange and tiling for multi-dimensional loops to minimize write operations
    on NVMs. *JSA* 135 (2023), 102799.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许等（2023）R. 许，E. H.-M. 沙，Q. 朱戈，Y. 宋，和 H. 王。2023年。循环交换和切片用于多维循环以最小化 NVM 上的写操作。在*JSA*
    135（2023），102799。
- en: Xue et al. (2023) Y. Xue, C. Chen, and A. Słowik. 2023. Neural Architecture
    Search Based on A Multi-objective Evolutionary Algorithm with Probability Stack.
    *TEVC* 27, 4 (2023).
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 薛等（2023）Y. 薛，C. 陈，和 A. 斯沃维克。2023年。基于多目标进化算法的神经架构搜索与概率堆栈。在*TEVC* 27，4（2023）。
- en: 'Yang et al. (2019) C. Yang, L. Xie, C. Su, and A. L. Yuille. 2019. Snapshot
    distillation: Teacher-student optimization in one generation. In *CVPR*. 2859–2868.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2019) C. Yang, L. Xie, C. Su, 和 A. L. Yuille. 2019. 快照蒸馏：一代中的师生优化。在*CVPR*。2859–2868。
- en: Yang et al. (2021c) J. Yang, B. Martinez, A. Bulat, G. Tzimiropoulos, et al.
    2021c. Knowledge distillation via softmax regression representation learning.
    In *ICLR*.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2021c) J. Yang, B. Martinez, A. Bulat, G. Tzimiropoulos等。2021c.
    通过softmax回归表示学习进行知识蒸馏。在*ICLR*。
- en: 'Yang et al. (2021a) L. Yang, H. Jiang, R. Cai, Y. Wang, S. Song, G. Huang,
    and Q. Tian. 2021a. Condensenet v2: Sparse feature reactivation for deep networks.
    In *CVPR*. 3569–3578.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2021a) L. Yang, H. Jiang, R. Cai, Y. Wang, S. Song, G. Huang, 和
    Q. Tian. 2021a. Condensenet v2：用于深度网络的稀疏特征重激活。在*CVPR*。3569–3578。
- en: 'Yang et al. (2018) T.-J. Yang, A. Howard, B. Chen, X. Zhang, A. Go, M. Sandler,
    V. Sze, and H. Adam. 2018. Netadapt: Platform-aware neural network adaptation
    for mobile applications. In *ECCV*. 285–300.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2018) T.-J. Yang, A. Howard, B. Chen, X. Zhang, A. Go, M. Sandler,
    V. Sze, 和 H. Adam. 2018. Netadapt：面向移动应用的系统感知神经网络适配。在*ECCV*。285–300。
- en: 'Yang et al. (2021b) T.-J. Yang, Y.-L. Liao, and V. Sze. 2021b. Netadaptv2:
    Efficient neural architecture search with fast super-network training and architecture
    optimization. In *CVPR*. 2402–2411.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2021b) T.-J. Yang, Y.-L. Liao, 和 V. Sze. 2021b. Netadaptv2：通过快速超级网络训练和架构优化进行高效神经架构搜索。在*CVPR*。2402–2411。
- en: 'Yao et al. (2021) Z. Yao, Z. Dong, Z. Zheng, A. Gholami, J. Yu, E. Tan, L.
    Wang, Q. Huang, Y. Wang, M. Mahoney, et al. 2021. Hawq-v3: Dyadic neural network
    quantization. In *ICML*. 11875–11886.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2021) Z. Yao, Z. Dong, Z. Zheng, A. Gholami, J. Yu, E. Tan, L. Wang,
    Q. Huang, Y. Wang, M. Mahoney等。2021. Hawq-v3：双重神经网络量化。在*ICML*。11875–11886。
- en: Ye et al. (2023) J. Ye, X. Chen, N. Xu, C. Zu, Z. Shao, S. Liu, Y. Cui, Z. Zhou,
    C. Gong, Y. Shen, et al. 2023. A comprehensive capability analysis of GPT-3 and
    GPT-3.5 series models. *arXiv preprint arXiv:2303.10420* (2023).
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye et al. (2023) J. Ye, X. Chen, N. Xu, C. Zu, Z. Shao, S. Liu, Y. Cui, Z. Zhou,
    C. Gong, Y. Shen等。2023. GPT-3和GPT-3.5系列模型的综合能力分析。*arXiv预印本 arXiv:2303.10420*（2023）。
- en: Ye et al. (2018) J. Ye, X. Lu, Z. Lin, and J. Z. Wang. 2018. Rethinking the
    smaller-norm-less-informative assumption in channel pruning of convolution layers.
    In *ICLR*.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye et al. (2018) J. Ye, X. Lu, Z. Lin, 和 J. Z. Wang. 2018. 重新思考卷积层通道修剪中的较小范数较少信息假设。在*ICLR*。
- en: 'Yin et al. (2022) H. Yin, A. Vahdat, J. Alvarez, A. Mallya, J. Kautz, and P.
    Molchanov. 2022. AdaViT: Adaptive Tokens for Efficient Vision Transformer. (2022),
    10809–10818.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2022) H. Yin, A. Vahdat, J. Alvarez, A. Mallya, J. Kautz, 和 P. Molchanov.
    2022. AdaViT：用于高效视觉变换器的自适应Token。（2022），10809–10818。
- en: Yoon et al. (2022) J. Yoon, D. Kang, and M. Cho. 2022. Semi-supervised Domain
    Adaptation via Sample-to-Sample Self-Distillation. In *WACV*. 1978–1987.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoon et al. (2022) J. Yoon, D. Kang, 和 M. Cho. 2022. 通过样本到样本自蒸馏的半监督领域适应。在*WACV*。1978–1987。
- en: 'You et al. (2020) H. You, X. Chen, Y. Zhang, C. Li, S. Li, Z. Liu, Z. Wang,
    and Y. Lin. 2020. ShiftAddNet: A Hardware-Inspired Deep Network. *NIPS* 33 (2020),
    2771–2783.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You et al. (2020) H. You, X. Chen, Y. Zhang, C. Li, S. Li, Z. Liu, Z. Wang,
    和 Y. Lin. 2020. ShiftAddNet：一种硬件启发的深度网络。*NIPS* 33（2020），2771–2783。
- en: Yu et al. (2023) C. Yu, T. Chen, and Z. Gan. 2023. Boost Transformer-based Language
    Models with GPU-Friendly Sparsity and Quantization. In *ACL*. 218–235.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2023) C. Yu, T. Chen, 和 Z. Gan. 2023. 通过GPU友好的稀疏性和量化增强基于Transformer的语言模型。在*ACL*。218–235。
- en: Yu et al. (2022) J. Yu, J. Liu, X. Wei, H. Zhou, Y. Nakata, D. Gudovskiy, T.
    Okuno, J. Li, K. Keutzer, and S. Zhang. 2022. Cross-domain object detection with
    mean-teacher transformer. In *ECCV*.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2022) J. Yu, J. Liu, X. Wei, H. Zhou, Y. Nakata, D. Gudovskiy, T.
    Okuno, J. Li, K. Keutzer, 和 S. Zhang. 2022. 通过mean-teacher transformer进行跨领域目标检测。在*ECCV*。
- en: 'Yuan et al. (2021) L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z. Jiang, F. E.
    Tay, J. Feng, and S. Yan. 2021. Tokens-to-Token ViT: Training Vision Transformers
    from Scratch on ImageNet. In *ICCV*. 558–567.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2021) L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z. Jiang, F. E.
    Tay, J. Feng, 和 S. Yan. 2021. Tokens-to-Token ViT：从头开始在ImageNet上训练视觉变换器。在*ICCV*。558–567。
- en: Yuan et al. (2020) L. Yuan, F. E. Tay, G. Li, T. Wang, and J. Feng. 2020. Revisiting
    knowledge distillation via label smoothing regularization. In *CVPR*. 3903–3911.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2020) L. Yuan, F. E. Tay, G. Li, T. Wang, and J. Feng. 2020. 通过标签平滑正则化重新审视知识蒸馏。在*CVPR*。3903–3911。
- en: Yuan and Lin (2006) M. Yuan and Y. Lin. 2006. Model selection and estimation
    in regression with grouped variables. *J. R. Stat. Soc. B* 68, 1 (2006), 49–67.
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan and Lin (2006) M. Yuan 和 Y. Lin. 2006. 分组变量回归中的模型选择与估计。*J. R. Stat. Soc.
    B* 68, 1 (2006), 49–67。
- en: Zhang et al. (2015) C. Zhang, P. Li, G. Sun, Y. Guan, B. Xiao, and J. Cong.
    2015. Optimizing FPGA-based accelerator design for deep convolutional neural networks.
    In *ACM FPGA*. 161–170.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2015）C. 张，P. 李，G. 孙，Y. 关，B. 肖和 J. 宗。2015年。优化基于 FPGA 的加速器设计以用于深度卷积神经网络。在 *ACM
    FPGA*。161–170。
- en: 'Zhang et al. (2018a) C. Zhang, G. Sun, Z. Fang, P. Zhou, P. Pan, and J. Cong.
    2018a. Caffeine: Toward uniformed representation and acceleration for deep convolutional
    neural networks. *TCAD* 38, 11 (2018), 2072–2085.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2018a）C. 张，G. 孙，Z. 方，P. 周，P. 潘和 J. 宗。2018a年。Caffeine：朝着统一表示和加速深度卷积神经网络的方向前进。*TCAD*
    38，第11期（2018），2072–2085。
- en: Zhang et al. (2021) H. Zhang, Z. Hu, W. Qin, M. Xu, and M. Wang. 2021. Adversarial
    co-distillation learning for image recognition. *Pattern Recognition* 111 (2021),
    107659.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2021）H. 张，Z. 胡，W. 秦，M. 徐和 M. 王。2021年。用于图像识别的对抗协同蒸馏学习。*Pattern Recognition*
    111（2021），107659。
- en: 'Zhang et al. (2023a) H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M.
    Ni, and H.-Y. Shum. 2023a. DINO: DETR with Improved DeNoising Anchor Boxes for
    End-to-End Object Detection. In *ICLR*.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2023a）H. 张，F. 李，S. 刘，L. 张，H. 苏，J. 朱，L. M. 倪和 H.-Y. 舒姆。2023a年。DINO：具有改进去噪锚点的
    DETR 端到端目标检测。在 *ICLR*。
- en: Zhang et al. (2023b) L. Zhang, A. Rao, and M. Agrawala. 2023b. Adding Conditional
    Control to Text-to-Image Diffusion Models. In *ICCV*. 3836–3847.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2023b）L. 张，A. Rao 和 M. 阿格拉瓦尔。2023b年。为文本到图像扩散模型添加条件控制。在 *ICCV*。3836–3847。
- en: 'Zhang et al. (2019b) L. Zhang, J. Song, A. Gao, J. Chen, C. Bao, and K. Ma.
    2019b. Be your own teacher: Improve the performance of convolutional neural networks
    via self distillation. In *ICCV*. 3713–3722.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2019b）L. 张，J. 宋，A. 高，J. 陈，C. 包和 K. 马。2019b年。做你自己的老师：通过自我蒸馏提升卷积神经网络的性能。在 *ICCV*。3713–3722。
- en: 'Zhang et al. (2016) S. Zhang, Z. Du, L. Zhang, H. Lan, S. Liu, L. Li, Q. Guo,
    T. Chen, and Y. Chen. 2016. Cambricon-X: An accelerator for sparse neural networks.
    In *MICRO*. 1–12.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2016）S. 张，Z. 杜，L. 张，H. 蓝，S. 刘，L. 李，Q. 郭，T. 陈和 Y. 陈。2016年。Cambricon-X：一种用于稀疏神经网络的加速器。在
    *MICRO*。1–12。
- en: 'Zhang et al. (2018c) X. Zhang, X. Zhou, M. Lin, and J. Sun. 2018c. ShuffleNet:
    An extremely efficient convolutional neural network for mobile devices. In *CVPR*.
    6848–6856.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2018c）X. 张，X. 周，M. 林和 J. 孙。2018c年。ShuffleNet：一种极其高效的移动设备卷积神经网络。在 *CVPR*。6848–6856。
- en: Zhang and Freris (2023) Y. Zhang and N. M. Freris. 2023. Adaptive Filter Pruning
    via Sensitivity Feedback. *TNNLS* (2023), 1–13.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张和弗雷里斯（2023）Y. 张和 N. M. 弗雷里斯。2023年。通过敏感度反馈进行自适应滤波器剪枝。*TNNLS*（2023），1–13。
- en: Zhang et al. (2018b) Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu. 2018b.
    Deep mutual learning. In *CVPR*. 4320–4328.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2018b）Y. 张，T. 项，T. M. 霍斯佩戴和 H. 陆。2018b年。深度互学习。在 *CVPR*。4320–4328。
- en: Zhang et al. (2019a) Z. Zhang, J. Li, W. Shao, Z. Peng, R. Zhang, X. Wang, and
    P. Luo. 2019a. Differentiable learning-to-group channels via groupable convolutional
    neural networks. In *ICCV*. 3542–3551.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2019a）Z. 张，J. 李，W. 肖，Z. 彭，R. 张，X. 王和 P. 罗。2019a年。通过可分组卷积神经网络学习可微分的组通道。在 *ICCV*。3542–3551。
- en: Zhao et al. (2022) B. Zhao, Q. Cui, R. Song, Y. Qiu, and J. Liang. 2022. Decoupled
    Knowledge Distillation. In *CVPR*. 11953–11962.
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等（2022）B. 赵，Q. 崔，R. 宋，Y. 邱和 J. 梁。2022年。解耦知识蒸馏。在 *CVPR*。11953–11962。
- en: Zhou et al. (2020) D. Zhou, Q. Hou, Y. Chen, J. Feng, and S. Yan. 2020. Rethinking
    bottleneck structure for efficient mobile network design. In *ECCV*. 680–697.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等（2020）D. 周，Q. 侯，Y. 陈，J. 冯和 S. 闫。2020年。重新思考瓶颈结构以提高移动网络设计的效率。在 *ECCV*。680–697。
- en: 'Zhou et al. (2018) X. Zhou, Z. Du, Q. Guo, S. Liu, C. Liu, C. Wang, X. Zhou,
    L. Li, T. Chen, and Y. Chen. 2018. Cambricon-S: Addressing irregularity in sparse
    neural networks through a cooperative software/hardware approach. In *MICRO*.
    15–28.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等（2018）X. 周，Z. 杜，Q. 郭，S. 刘，C. 刘，C. 王，X. 周，L. 李，T. 陈和 Y. 陈。2018年。Cambricon-S：通过协作的软件/硬件方法解决稀疏神经网络中的不规则性。在
    *MICRO*。15–28。
- en: Zhou et al. (2021) Y. Zhou, X. Dong, B. Akin, M. Tan, D. Peng, T. Meng, A. Yazdanbakhsh,
    D. Huang, R. Narayanaswami, and J. Laudon. 2021. Rethinking co-design of neural
    architectures and hardware accelerators. *arXiv preprint arXiv:2102.08619* (2021).
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等（2021）Y. 周，X. 董，B. 阿金，M. 谭，D. 彭，T. 孟，A. 雅兹丹巴赫，D. 黄，R. 纳拉亚纳斯瓦米和 J. 劳登。2021年。重新思考神经架构与硬件加速器的协同设计。*arXiv
    preprint arXiv:2102.08619*（2021）。
- en: Zhu et al. (2017) C. Zhu, S. Han, H. Mao, and W. J. Dally. 2017. Trained Ternary
    Quantization. In *ICLR*.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱等（2017）C. 朱，S. 韩，H. 毛和 W. J. 达利。2017年。训练的三值量化。在 *ICLR*。
- en: Zoph and Le (2017) B. Zoph and Q. V. Le. 2017. Neural architecture search with
    reinforcement learning. *ICLR* (2017).
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 佐普和乐（2017）B. 佐普和 Q. V. 乐。2017年。利用强化学习进行神经架构搜索。*ICLR*（2017）。
