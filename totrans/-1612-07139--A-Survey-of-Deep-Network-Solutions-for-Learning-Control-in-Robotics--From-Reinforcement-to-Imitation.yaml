- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:09:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:09:13
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1612.07139] A Survey of Deep Network Solutions for Learning Control in Robotics:
    From Reinforcement to Imitation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1612.07139] 深度网络解决方案的综述：用于机器人中的学习控制，从强化学习到模仿学习'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1612.07139](https://ar5iv.labs.arxiv.org/html/1612.07139)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1612.07139](https://ar5iv.labs.arxiv.org/html/1612.07139)
- en: A Survey of Deep Network Solutions
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度网络解决方案的综述
- en: 'for Learning Control in Robotics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 用于机器人中的学习控制：
- en: From Reinforcement to Imitation
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 从强化学习到模仿学习
- en: Lei Tai^(∗1), Jingwei Zhang^(∗2), Ming Liu¹, Joschka Boedecker², Wolfram Burgard²
    *indicates equal contribution.¹Lei Tai and Ming Liu are with The Hong Kong University
    of Science and Technology. {ltai, eelium}ust.hk²Jingwei Zhang, Joschka Boedecker
    and Wolfram Burgard are with University of Freiburg. {zhang, jboedeck, burgard}@informatik.uni-freiburg.de
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**雷泰**^(∗1)、**晶伟张**^(∗2)、**明刘**¹、**约施卡·博德克**²、**沃尔夫拉姆·布尔加德**² *表示等同贡献。¹雷泰和明刘来自香港科技大学。{ltai,
    eelium}ust.hk²晶伟张、约施卡·博德克和沃尔夫拉姆·布尔加德来自弗赖堡大学。{zhang, jboedeck, burgard}@informatik.uni-freiburg.de'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Deep learning techniques have been widely applied, achieving state-of-the-art
    results in various fields of study. This survey focuses on deep learning solutions
    that target learning control policies for robotics applications. We carry out
    our discussions on the two main paradigms for learning control with deep networks:
    Deep Reinforcement Learning and Imitation Learning. For Deep Reinforcement Learning
    (DRL), we begin from traditional reinforcement learning algorithms, showing how
    they are extended to the deep context and effective mechanisms that could be added
    on top of the DRL algorithms. We then introduce representative works that utilize
    DRL to solve navigation and manipulation tasks in robotics. We continue our discussion
    on methods addressing the challenge of the reality gap for transferring DRL policies
    trained in simulation to real-world scenarios, and summarize robotics simulation
    platforms for conducting DRL research. For Imitation Leaning, we go through its
    three main categories, behavior cloning, inverse reinforcement learning and generative
    adversarial imitation learning, by introducing their formulations and their corresponding
    robotics applications. Finally, we discuss the open challenges and research frontiers.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术已被广泛应用，在多个研究领域中取得了最先进的成果。本综述聚焦于针对机器人应用的学习控制策略的深度学习解决方案。我们讨论了两种主要的深度网络学习控制范式：深度强化学习和模仿学习。对于深度强化学习（DRL），我们从传统的强化学习算法入手，展示了它们如何扩展到深度背景下以及可以添加到DRL算法上的有效机制。然后我们介绍了利用DRL解决机器人导航和操作任务的代表性工作。我们继续讨论应对将训练于模拟中的DRL策略迁移到现实场景中的现实差距挑战的方法，并总结了用于进行DRL研究的机器人模拟平台。对于模仿学习，我们介绍了其三个主要类别：行为克隆、逆强化学习和生成对抗模仿学习，阐述了它们的公式及其相应的机器人应用。最后，我们讨论了开放的挑战和研究前沿。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep Learning, Robotics, Deep Reinforcement Learning, Imitation Learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习、机器人、深度强化学习、模仿学习。
- en: I Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: I-A Deep Learning
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I- 深度学习
- en: Deep learning, as a solution for artificial intelligence that is capable of
    building progressively more abstract representations of input data, plays an essential
    role in various fields of study (Goodfellow et al., [2016](#bib.bib30)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习作为一种人工智能解决方案，能够逐渐构建输入数据的抽象表示，在多个研究领域中发挥着至关重要的作用（Goodfellow et al., [2016](#bib.bib30)）。
- en: From image classification (Krizhevsky et al., [2012](#bib.bib52); He et al.,
    [2016](#bib.bib38); Huang et al., [2017](#bib.bib43)), to semantic segmentation
    (Long et al., [2015](#bib.bib60); Chen et al., [2016](#bib.bib11)), from playing
    Atari games at the human-level with only pixel inputs (Mnih et al., [2015](#bib.bib65),
    [2016](#bib.bib64)), to learning policies capable of driving real robotic systems
    in navigation (Zhu et al., [2017b](#bib.bib125); Zhang et al., [2017a](#bib.bib120);
    Tai et al., [2017](#bib.bib99)) and manipulation (Levine et al., [2016](#bib.bib56);
    Yu et al., [2018](#bib.bib119)) tasks, the learning power of deep networks drives
    the state-of-the-art in various research directions (Schmidhuber, [2015](#bib.bib86)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像分类（Krizhevsky et al.，[2012](#bib.bib52)；He et al.，[2016](#bib.bib38)；Huang
    et al.，[2017](#bib.bib43)），到语义分割（Long et al.，[2015](#bib.bib60)；Chen et al.，[2016](#bib.bib11)），从仅用像素输入进行人类水平的Atari游戏（Mnih
    et al.，[2015](#bib.bib65)，[2016](#bib.bib64)），到学习能够在导航（Zhu et al.，[2017b](#bib.bib125)；Zhang
    et al.，[2017a](#bib.bib120)；Tai et al.，[2017](#bib.bib99)）和操作（Levine et al.，[2016](#bib.bib56)；Yu
    et al.，[2018](#bib.bib119)）任务中驱动真实机器人系统的策略，深度网络的学习能力推动了各种研究方向的最先进技术（Schmidhuber，[2015](#bib.bib86)）。
- en: Recent years have witnessed a rapidly growing trend of utilizing deep learning
    techniques for robotics tasks. Replacing hand-crafted features with learned hierarchical
    distributed deep features, learning control policies directly from high-dimensional
    sensory inputs, the robotics community is making solid progress towards building
    fully autonomous intelligent systems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，利用深度学习技术进行机器人任务的趋势迅速增长。通过将手工特征替换为学习到的层次分布深度特征，直接从高维感官输入中学习控制策略，机器人社区在构建完全自主的智能系统方面取得了坚实的进展。
- en: 'I-B Deep Learning for Robotics: From Perception to Control'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 深度学习在机器人学中的应用：从感知到控制
- en: 'Autonomous intelligent robotics systems require two essential building blocks:
    perception and control.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自主智能机器人系统需要两个基本构建模块：感知和控制。
- en: 'The perception pipeline can be viewed as a passive procedure: intelligent agents
    receive observations from the environment, then infer desired properties or detect
    target quantities from those sensory inputs. We refer readers to Deng ([2014](#bib.bib17))
    and Guo et al. ([2016](#bib.bib34)) for a comprehensive overview of deep learning
    techniques for perception. Compared with pure perception, the problem of control
    for autonomous agents goes one step further, seeking to actively interact with
    or influence the environment by conducting sequences of actions. This active nature
    leads to the following major distinctions between perception and control, in terms
    of deep learning based approaches:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 感知管道可以被视为一种被动过程：智能代理从环境中接收观察，然后从这些传感器输入中推断所需的属性或检测目标量。我们建议读者参阅Deng（[2014](#bib.bib17)）和Guo等（[2016](#bib.bib34)）以获取深度学习技术在感知方面的全面概述。与纯粹的感知相比，自动化代理的控制问题更进一步，寻求通过执行一系列行动来积极互动或影响环境。这种主动性导致了感知与控制之间在深度学习方法方面的以下主要区别：
- en: 'Data distribution: When learning perception through supervised learning techniques,
    the training datasets are collected and labeled before the learning phase begins.
    In this case, the data points can be viewed as being independently and identically
    distributed (i.i.d), such that a direct mapping from the input to the labels can
    be learned via standard stochastic gradient descent methods and variants. In contrast,
    for control, the datasets are collected in an online manner, which makes the data
    points sequential in nature: the consecutive observations received by the agent
    are temporally correlated since the agent actively influences the data distribution
    by the actions it takes. Ignoring this underlying temporal correlation would lead
    to compounding errors (Bagnell, [2015](#bib.bib3)).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分布：在通过监督学习技术进行感知学习时，训练数据集在学习阶段开始之前被收集和标记。在这种情况下，数据点可以被视为独立同分布（i.i.d），这样可以通过标准的随机梯度下降方法及其变体来学习输入到标签的直接映射。相比之下，对于控制任务，数据集是以在线方式收集的，这使得数据点在本质上是序列化的：代理接收到的连续观察是时间上相关的，因为代理通过其行动积极影响数据分布。忽视这种潜在的时间相关性会导致累积错误（Bagnell，[2015](#bib.bib3)）。
- en: 'Supervision signal: The supervision for learning perception is often direct
    and strong, in that each training sample is provided along with its ground truth
    label. In control tasks, on the other hand, either only sparse reward signals
    are available when learning behaviors through deep reinforcement learning, or
    the feedback is often delayed and not instantaneous, even when demonstrations
    from experts are provided in the scenario of imitation learning, since the credit
    for achieving a certain goal needs to be correctly assigned to all the actions
    taken along the trajectory.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 监督信号：学习感知的监督通常是直接且强烈的，每个训练样本都会附带其真实标签。而在控制任务中，通过深度强化学习学习行为时，要么只有稀疏的奖励信号可用，要么反馈往往是延迟的而非即时的，即使在模仿学习场景中提供了专家的示范，因为实现特定目标的功劳需要正确地分配到沿轨迹采取的所有动作上。
- en: 'Data collection: As discussed before, the dataset for perception can be collected
    off-line, while the dataset for control has to be collected in an on-line manner,
    since actions are actively involved in the learning process. This greatly limits
    the number of samples one can collect, since executing actions in the real world
    with real robotics systems is a relatively expensive procedure. In cases where
    the control policies are trained in simulation, the problem of the reality gap
    arises when they are deployed in real-world scenarios, where the discrepancies
    between the modalities of the synthetic renderings and the real sensory readings
    impose major challenges.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集：如前所述，感知的数据集可以离线收集，而控制的数据集必须以在线方式收集，因为动作在学习过程中起着主动作用。这大大限制了可以收集的样本数量，因为在真实世界中执行动作的成本相对较高。如果控制策略是在模拟环境中训练的，当它们被部署到实际场景中时，就会出现现实差距的问题，合成渲染与真实感官读数之间的差异带来了主要挑战。
- en: 'Recognizing those distinctions, various deep learning based algorithms have
    been proposed to solve control for robotics. In this survey, we review the deep
    learning approaches for control tasks based on their underlying learning paradigms,
    and we carry out our discussion through the following sections:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到这些区别后，已经提出了各种基于深度学习的算法来解决机器人控制问题。在这次调查中，我们根据其基本学习范式回顾了用于控制任务的深度学习方法，并通过以下几个部分进行讨论：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sec. [II](#S2 "II Deep reinforcement learning ‣ A Survey of Deep Network Solutions
    for Learning Control in Robotics: From Reinforcement to Imitation") Deep Reinforcement
    Learning'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[II](#S2 "II Deep reinforcement learning ‣ A Survey of Deep Network Solutions
    for Learning Control in Robotics: From Reinforcement to Imitation")节 深度强化学习'
- en: –
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Sec. [II-A](#S2.SS1 "II-A RL Overview ‣ II Deep reinforcement learning ‣ A
    Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation") RL Overview'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[II-A](#S2.SS1 "II-A RL Overview ‣ II Deep reinforcement learning ‣ A Survey
    of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation")节 强化学习概述'
- en: –
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Sec. [II-B](#S2.SS2 "II-B RL Algorithms ‣ II Deep reinforcement learning ‣
    A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation") RL Algorithms'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[II-B](#S2.SS2 "II-B RL Algorithms ‣ II Deep reinforcement learning ‣ A Survey
    of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation")节 强化学习算法'
- en: –
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Sec. [II-C](#S2.SS3 "II-C DRL Algorithms ‣ II Deep reinforcement learning ‣
    A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation") DRL Algorithms'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[II-C](#S2.SS3 "II-C DRL Algorithms ‣ II Deep reinforcement learning ‣ A Survey
    of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation")节 DRL算法'
- en: –
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Sec. [II-D](#S2.SS4 "II-D DRL Mechanisms ‣ II Deep reinforcement learning ‣
    A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation") DRL Mechanisms'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[II-D](#S2.SS4 "II-D DRL Mechanisms ‣ II Deep reinforcement learning ‣ A Survey
    of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation")节 DRL机制'
- en: –
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Sec. [II-E](#S2.SS5 "II-E DRL for Navigation ‣ II Deep reinforcement learning
    ‣ A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation") DRL for Navigation'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[II-E](#S2.SS5 "II-E DRL for Navigation ‣ II Deep reinforcement learning ‣
    A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation")节 DRL在导航中的应用'
- en: –
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Sec. [II-F](#S2.SS6 "II-F DRL for Manipulation ‣ II Deep reinforcement learning
    ‣ A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation") DRL for Manipulation'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[II-F](#S2.SS6 "II-F DRL for Manipulation ‣ II Deep reinforcement learning
    ‣ A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation")节 DRL在操控中的应用'
- en: –
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Sec. [II-G](#S2.SS7 "II-G The Reality Gap: From Simulation to the Real World
    ‣ II Deep reinforcement learning ‣ A Survey of Deep Network Solutions for Learning
    Control in Robotics: From Reinforcement to Imitation") The Reality Gap: From Simulation
    to the Real World'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sec. [II-G](#S2.SS7 "II-G 现实差距：从模拟到现实世界 ‣ II 深度强化学习 ‣ 深度网络在机器人控制学习中的解决方案调查：从强化学习到模仿学习")
    现实差距：从模拟到现实世界
- en: –
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Sec. [II-H](#S2.SS8 "II-H Simulation Platforms ‣ II Deep reinforcement learning
    ‣ A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation") Simulation Platforms'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sec. [II-H](#S2.SS8 "II-H 模拟平台 ‣ II 深度强化学习 ‣ 深度网络在机器人控制学习中的解决方案调查：从强化学习到模仿学习")
    模拟平台
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sec. [III](#S3 "III Imitation Learning ‣ A Survey of Deep Network Solutions
    for Learning Control in Robotics: From Reinforcement to Imitation") Imitation
    Learning'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sec. [III](#S3 "III 模仿学习 ‣ 深度网络在机器人控制学习中的解决方案调查：从强化学习到模仿学习") 模仿学习
- en: –
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Sec. [III-A](#S3.SS1 "III-A Behavior Cloning ‣ III Imitation Learning ‣ A Survey
    of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation") Behavior Cloning'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sec. [III-A](#S3.SS1 "III-A 行为克隆 ‣ III 模仿学习 ‣ 深度网络在机器人控制学习中的解决方案调查：从强化学习到模仿学习")
    行为克隆
- en: –
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Sec. [III-B](#S3.SS2 "III-B Inverse Reinforcement Learning ‣ III Imitation
    Learning ‣ A Survey of Deep Network Solutions for Learning Control in Robotics:
    From Reinforcement to Imitation") Inverse Reinforcement Learning'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sec. [III-B](#S3.SS2 "III-B 逆向强化学习 ‣ III 模仿学习 ‣ 深度网络在机器人控制学习中的解决方案调查：从强化学习到模仿学习")
    逆向强化学习
- en: –
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Sec. [III-C](#S3.SS3 "III-C Generative Adversarial Imitation Learning ‣ III
    Imitation Learning ‣ A Survey of Deep Network Solutions for Learning Control in
    Robotics: From Reinforcement to Imitation") Generative Adversarial Imitation Learning'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sec. [III-C](#S3.SS3 "III-C 生成对抗模仿学习 ‣ III 模仿学习 ‣ 深度网络在机器人控制学习中的解决方案调查：从强化学习到模仿学习")
    生成对抗模仿学习
- en: II Deep reinforcement learning
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 深度强化学习
- en: Being the first to stabilize large-scale reinforcement learning with deep convolutional
    neural networks as function approximators, deep Q-networks (DQN) (Mnih et al.,
    [2015](#bib.bib65)) have brought increased research and applications of deep reinforcement
    learning (DRL) methods. In the following we first review the basic concepts and
    algorithms in traditional reinforcement learning (RL). Then we continue to the
    several most influential DRL algorithms and mechanisms, on the basis of which
    we discuss DRL solutions for robotics control, with an emphasis on navigation
    and manipulation applications.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作为首个通过深度卷积神经网络作为函数逼近器稳定大规模强化学习的方法，深度Q网络（DQN）（Mnih et al., [2015](#bib.bib65)）带动了深度强化学习（DRL）方法的研究和应用的增加。接下来我们首先回顾传统强化学习（RL）中的基本概念和算法。然后，我们继续介绍几个最具影响力的DRL算法和机制，基于这些，我们讨论了用于机器人控制的DRL解决方案，重点关注导航和操作应用。
- en: II-A RL Overview
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 强化学习概述
- en: 'We formalize a robotics task (e.g., navigation, manipulation) as a Markov Decision
    Process (MDP), in which the agent interacts with the environment through a sequence
    of observations, actions, and reward signals. An MDP is a $5-$tuple $\left<\mathcal{S},\mathcal{A},P,\mathcal{R},\gamma\right>$:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将机器人任务（例如，导航，操作）形式化为马尔可夫决策过程（MDP），其中代理通过一系列观察、动作和奖励信号与环境互动。MDP 是一个 $5-$元组
    $\left<\mathcal{S},\mathcal{A},P,\mathcal{R},\gamma\right>$：
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\mathcal{S}$: set of all states.'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\mathcal{S}$: 所有状态的集合。'
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\mathcal{A}$: set of all actions.'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\mathcal{A}$: 所有动作的集合。'
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\mathcal{P}$: the transition dynamics, where $P(\mathbf{s}^{\prime}|\mathbf{s},\mathbf{a})$
    defines the distribution of the next state $\mathbf{s}^{\prime}$ by taking action
    $\mathbf{a}$ in state $\mathbf{s}$, where $\mathbf{s},\mathbf{s}^{\prime}\in\mathcal{S},\mathbf{a}\in\mathcal{A}$.
    We also denote the initial state distribution $P(\mathbf{s}_{0})$ as $\rho_{0}$.'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\mathcal{P}$: 转移动态，其中 $P(\mathbf{s}^{\prime}|\mathbf{s},\mathbf{a})$ 定义了在状态
    $\mathbf{s}$ 下采取动作 $\mathbf{a}$ 后下一个状态 $\mathbf{s}^{\prime}$ 的分布，其中 $\mathbf{s},\mathbf{s}^{\prime}\in\mathcal{S},\mathbf{a}\in\mathcal{A}$。我们还将初始状态分布
    $P(\mathbf{s}_{0})$ 表示为 $\rho_{0}$。'
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\mathcal{R}$: set of all possible rewards. In the following, we denote the
    instantaneous scalar reward received by the agent by taking action $\mathbf{a}_{t}$
    from state $\mathbf{s}_{t}$ as $R_{t+1}(\mathbf{s}_{t},\mathbf{a}_{t})$, and use
    $R_{t+1}$ as short for $R_{t+1}(\mathbf{s}_{t},\mathbf{a}_{t})$. There also exist
    other definitions of the reward function that depend only on the state itself,
    in which $R(\mathbf{s})$ refers to the reward signal that the agent receives by
    arriving at state $\mathbf{s}$. In some of the following discussions, the negative
    counterpart of the reward function, the cost function, is used, and is denoted
    as $c(\mathbf{s})$.'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathcal{R}$：所有可能奖励的集合。在下文中，我们将智能体在状态$\mathbf{s}_{t}$下采取动作$\mathbf{a}_{t}$所获得的瞬时标量奖励表示为$R_{t+1}(\mathbf{s}_{t},\mathbf{a}_{t})$，并用$R_{t+1}$作为$R_{t+1}(\mathbf{s}_{t},\mathbf{a}_{t})$的简写。还有其他奖励函数的定义仅依赖于状态本身，其中$R(\mathbf{s})$指的是智能体到达状态$\mathbf{s}$时接收到的奖励信号。在接下来的讨论中，奖励函数的负面对应物，即成本函数，通常用$c(\mathbf{s})$表示。
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\gamma$: a discount factor in the range of $[0,1]$.'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\gamma$：范围为$[0,1]$的折扣因子。
- en: '![Refer to caption](img/d919447e176207db593d90b8afe647b2.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d919447e176207db593d90b8afe647b2.png)'
- en: 'Figure 1: The reinforcement learning loop in the context of robotics. In state
    $\mathbf{s}_{t}$, the autonomous agent takes and action $\mathbf{a}_{t}$, receives
    a reward $R_{t+1}$, and transits to the next state $\mathbf{s}_{t+1}$.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：在机器人领域中的强化学习循环。在状态$\mathbf{s}_{t}$下，自主智能体采取动作$\mathbf{a}_{t}$，获得奖励$R_{t+1}$，并转移到下一个状态$\mathbf{s}_{t+1}$。
- en: 'In an MDP, the agent takes an action $\mathbf{a}_{t}$ in state $\mathbf{s}_{t}$,
    receives a reward $R_{t+1}$, and transits to the next state $\mathbf{s}_{t+1}$
    following the transition dynamics $\mathcal{P}(\mathbf{s}_{t+1}|\mathbf{s}_{t},\mathbf{a}_{t})$.
    This process in the context of robotics is depicted in Fig.[1](#S2.F1 "Figure
    1 ‣ II-A RL Overview ‣ II Deep reinforcement learning ‣ A Survey of Deep Network
    Solutions for Learning Control in Robotics: From Reinforcement to Imitation").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDP中，智能体在状态$\mathbf{s}_{t}$下采取动作$\mathbf{a}_{t}$，获得奖励$R_{t+1}$，并根据转移动态$\mathcal{P}(\mathbf{s}_{t+1}|\mathbf{s}_{t},\mathbf{a}_{t})$转移到下一个状态$\mathbf{s}_{t+1}$。机器人领域中的这一过程如图[1](#S2.F1
    "图 1 ‣ II-A 强化学习概述 ‣ II 深度强化学习 ‣ 关于机器人控制学习的深度网络解决方案的综述：从强化学习到模仿学习")所示。
- en: In robotics, we mainly consider episodic MDPs, where there exists a terminal
    state (e.g., a mobile ground vehicle reaches a certain goal location, a manipulator
    successfully grabs a red cup) that, once reached, terminates the current episode.
    Also for an episodic MDP with a time horizon of $T$, an episode will still be
    terminated after a maximum of $T$ time steps, even if by then the terminal state
    has not yet been reached.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器人领域，我们主要考虑情节性MDP，其中存在一个终端状态（例如，移动地面车辆到达某个目标位置，操纵器成功抓取一个红杯），一旦达到该状态，当前情节即终止。对于具有时间范围$T$的情节性MDP，即使在时间范围$T$内终端状态尚未到达，情节仍将在最大$T$个时间步后终止。
- en: 'Another point worth mentioning is the partial observability. In a robotics
    task, an autonomous agent perceives the world with its onboard sensor (e.g., RGB/depth
    camera, IMU, laser range sensor, 3D Lidar), receiving one observation per time
    step. However, simply representing $\mathbf{s}_{t}$ by $\mathbf{x}_{t}$ often
    does not satisfy the Markov property: one such sensor reading can hardly capture
    all the necessary information for the agent to make decisions in the future, in
    which case the underlying procedure is called a Partial Observeble MDP (POMDP).
    This is often dealt with by either stacking several (e.g, $N$) consecutive observations
    $\{\mathbf{x}_{t-N+1},\mathbf{x}_{t-N+2},\dots,\mathbf{x}_{t}\}$ to represent
    $\mathbf{s}_{t}$, or by feeding $\mathbf{x}_{t}$ into a recurrent neural network
    instead of a feed forward one, such that the past information is naturally accounted
    with (e.g., by the cell state when using the long short-term memories (LSTMs).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得提及的点是部分可观测性。在机器人任务中，自主智能体通过其车载传感器（例如，RGB/深度相机、IMU、激光测距传感器、3D Lidar）感知世界，每个时间步接收一个观察值。然而，仅用$\mathbf{s}_{t}$表示为$\mathbf{x}_{t}$通常不能满足马尔可夫性质：一个传感器读数很难捕获所有必要的信息来帮助智能体做出未来的决策，这种情况下，基础过程称为部分可观测的MDP（POMDP）。这通常通过堆叠几个（例如，$N$）连续观察值$\{\mathbf{x}_{t-N+1},\mathbf{x}_{t-N+2},\dots,\mathbf{x}_{t}\}$来表示$\mathbf{s}_{t}$，或者将$\mathbf{x}_{t}$输入到递归神经网络中而不是前馈神经网络中，以便自然地考虑过去的信息（例如，当使用长短期记忆（LSTMs）时通过单元状态）。
- en: 'Reinforcement learning agents are designed to learn from interactions how to
    behave to achieve a certain goal (Sutton and Barto, [1998](#bib.bib96)). More
    precisely, here the objective of learning is to maximize the expected discounted
    return, where the discounted return is defined as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '强化学习代理旨在通过交互学习如何行为以实现某个目标（Sutton 和 Barto，[1998](#bib.bib96)）。更准确地说，这里的学习目标是最大化期望折扣回报，其中折扣回报定义如下:'
- en: '|  | $\displaystyle G_{t}$ | $\displaystyle=R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\dots+\gamma^{T-t-1}R_{T}$
    |  | (1) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{t}$ | $\displaystyle=R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\dots+\gamma^{T-t-1}R_{T}$
    |  | (1) |'
- en: '|  |  | $\displaystyle=\sum_{k=t}^{T}\gamma^{k-t}R_{k+1}.$ |  | (2) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{k=t}^{T}\gamma^{k-t}R_{k+1}.$ |  | (2) |'
- en: 'To solve control, two important definitions are introduced:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决控制问题，引入了两个重要定义:'
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Policies: $\pi,\mu$'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '策略: $\pi,\mu$'
- en: –
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: '$\pi(\mathbf{a}|\mathbf{s})$: stochastic policy, where actions are drawn from
    a probability distribution defined by $\pi(\mathbf{a}|\mathbf{s})$.'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\pi(\mathbf{a}|\mathbf{s})$: 随机策略，其中动作是从由 $\pi(\mathbf{a}|\mathbf{s})$ 定义的概率分布中抽取的。'
- en: –
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: '$\mu(\mathbf{s})$: determinstic policy, where actions are deterministically
    selected for a given state $\mathbf{s}$.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\mu(\mathbf{s})$: 确定性策略，其中在给定状态 $\mathbf{s}$ 时，动作是确定性的。'
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Value functions: $V,Q$'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '值函数: $V,Q$'
- en: –
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: '$V^{\pi}(\mathbf{s})$: state-value function, defined as the expected return
    when starting from state $\mathbf{s}$ and following policy $\pi$ thereafter:'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$V^{\pi}(\mathbf{s})$: 状态值函数，定义为从状态 $\mathbf{s}$ 开始并随后遵循策略 $\pi$ 的期望回报:'
- en: '|  | $\displaystyle V^{\pi}(\mathbf{s})$ | $\displaystyle=\mathbb{E}_{\pi}\left[G_{t}&#124;\mathbf{s}_{t}=\mathbf{s}\right]$
    |  | (3) |'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle V^{\pi}(\mathbf{s})$ | $\displaystyle=\mathbb{E}_{\pi}\left[G_{t}&#124;\mathbf{s}_{t}=\mathbf{s}\right]$
    |  | (3) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\pi}\left[\sum_{k=t}^{T}\gamma^{k-t}R_{k+1}&#124;\mathbf{s}_{t}=\mathbf{s}\right].$
    |  | (4) |'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\pi}\left[\sum_{k=t}^{T}\gamma^{k-t}R_{k+1}&#124;\mathbf{s}_{t}=\mathbf{s}\right].$
    |  | (4) |'
- en: –
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: '$Q^{\pi}(\mathbf{s},\mathbf{a})$: action-value function, defined as the expected
    return by taking the action $\mathbf{a}$ from state $\mathbf{s}$, then following
    $\pi$ thereafter:'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$Q^{\pi}(\mathbf{s},\mathbf{a})$: 行为值函数，定义为从状态 $\mathbf{s}$ 执行动作 $\mathbf{a}$
    后，随后遵循 $\pi$ 的期望回报:'
- en: '|  | $\displaystyle Q^{\pi}(\mathbf{s},\mathbf{a})$ | $\displaystyle=\mathbb{E}_{\pi}\left[G_{t}&#124;\mathbf{s}_{t}=\mathbf{s},\mathbf{a}_{t}=\mathbf{a}\right]$
    |  | (5) |'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{\pi}(\mathbf{s},\mathbf{a})$ | $\displaystyle=\mathbb{E}_{\pi}\left[G_{t}&#124;\mathbf{s}_{t}=\mathbf{s},\mathbf{a}_{t}=\mathbf{a}\right]$
    |  | (5) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\pi}\left[\sum_{k=t}^{T}\gamma^{k-t}R_{k+1}&#124;\mathbf{s}_{t}=\mathbf{s},\mathbf{a}_{t}=\mathbf{a}\right].$
    |  | (6) |'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\pi}\left[\sum_{k=t}^{T}\gamma^{k-t}R_{k+1}&#124;\mathbf{s}_{t}=\mathbf{s},\mathbf{a}_{t}=\mathbf{a}\right].$
    |  | (6) |'
- en: –
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: '$Q^{*}(\mathbf{s},\mathbf{a})$: optimal value function (We omit the case for
    state-value function $V$ here since the action-value function $Q$ is a much more
    effective representation for control.):'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$Q^{*}(\mathbf{s},\mathbf{a})$: 最优值函数（这里省略了状态值函数 $V$ 的情况，因为行为值函数 $Q$ 是控制问题的更有效表示。）'
- en: '|  | $\displaystyle Q^{*}(\mathbf{s},\mathbf{a})$ | $\displaystyle=\max_{\pi}Q^{\pi}(\mathbf{s},\mathbf{a}).$
    |  | (7) |'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{*}(\mathbf{s},\mathbf{a})$ | $\displaystyle=\max_{\pi}Q^{\pi}(\mathbf{s},\mathbf{a}).$
    |  | (7) |'
- en: –
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: '$\pi^{*}(\mathbf{a}|\mathbf{s})$: optimal policy:'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\pi^{*}(\mathbf{a}|\mathbf{s})$: 最优策略:'
- en: '|  | $\displaystyle\pi^{*}(\mathbf{a}&#124;\mathbf{s})$ | $\displaystyle=\operatorname*{\arg\!\max}_{\mathbf{a}}Q^{*}(\mathbf{s},\mathbf{a}).$
    |  | (8) |'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\pi^{*}(\mathbf{a}&#124;\mathbf{s})$ | $\displaystyle=\operatorname*{\arg\!\max}_{\mathbf{a}}Q^{*}(\mathbf{s},\mathbf{a}).$
    |  | (8) |'
- en: II-B RL Algorithms
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B RL 算法
- en: With the definitions of the core components, we now continue to discuss the
    different classes of RL algorithms. We emphasize those methods that have been
    extended with deep learning variants.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有了核心组件的定义，我们现在继续讨论不同类别的强化学习算法。我们特别强调那些已经扩展到深度学习变体的方法。
- en: II-B1 Value-based Methods
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 基于值的方法
- en: 'These methods are based on estimating the values of being in a given state,
    then extracting the control policies from the estimated values. The recursive
    value estimation procedures are based on the Bellman Equations. Below, we list
    the Bellman Expectation Equation (Eq. [9](#S2.E9 "In II-B1 Value-based Methods
    ‣ II-B RL Algorithms ‣ II Deep reinforcement learning ‣ A Survey of Deep Network
    Solutions for Learning Control in Robotics: From Reinforcement to Imitation"))
    and the Bellman Optimality Equation (Eq. [10](#S2.E10 "In II-B1 Value-based Methods
    ‣ II-B RL Algorithms ‣ II Deep reinforcement learning ‣ A Survey of Deep Network
    Solutions for Learning Control in Robotics: From Reinforcement to Imitation")):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法基于对在给定状态下的值进行估计，然后从估计的值中提取控制策略。递归值估计过程基于 Bellman 方程。以下是 Bellman 期望方程（公式
    [9](#S2.E9 "在 II-B1 基于价值的方法 ‣ II-B 强化学习算法 ‣ II 深度强化学习 ‣ 机器人学习控制的深度网络解决方案综述：从强化学习到模仿学习")）和
    Bellman 最优方程（公式 [10](#S2.E10 "在 II-B1 基于价值的方法 ‣ II-B 强化学习算法 ‣ II 深度强化学习 ‣ 机器人学习控制的深度网络解决方案综述：从强化学习到模仿学习")）：
- en: '|  | $\displaystyle Q^{\pi}(\mathbf{s},\mathbf{a})$ | $\displaystyle=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma
    Q^{\pi}(\mathbf{s}_{t+1},\mathbf{a}_{t+1})&#124;\mathbf{s}_{t}=\mathbf{s},\mathbf{a}_{t}=\mathbf{a}\right],$
    |  | (9) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{\pi}(\mathbf{s},\mathbf{a})$ | $\displaystyle=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma
    Q^{\pi}(\mathbf{s}_{t+1},\mathbf{a}_{t+1})\mid\mathbf{s}_{t}=\mathbf{s},\mathbf{a}_{t}=\mathbf{a}\right],$
    |  | (9) |'
- en: '|  | $\displaystyle Q^{*}(\mathbf{s},\mathbf{a})$ | $\displaystyle=\mathbb{E}\left[R_{t+1}+\gamma\max_{\mathbf{a}^{\prime}}Q^{*}(\mathbf{s}_{t+1},\mathbf{a}^{\prime})&#124;\mathbf{s}_{t}=\mathbf{s},\mathbf{a}_{t}=\mathbf{a}\right].$
    |  | (10) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{*}(\mathbf{s},\mathbf{a})$ | $\displaystyle=\mathbb{E}\left[R_{t+1}+\gamma\max_{\mathbf{a}^{\prime}}Q^{*}(\mathbf{s}_{t+1},\mathbf{a}^{\prime})\mid\mathbf{s}_{t}=\mathbf{s},\mathbf{a}_{t}=\mathbf{a}\right].$
    |  | (10) |'
- en: 'Following the formulations of Eq. [9](#S2.E9 "In II-B1 Value-based Methods
    ‣ II-B RL Algorithms ‣ II Deep reinforcement learning ‣ A Survey of Deep Network
    Solutions for Learning Control in Robotics: From Reinforcement to Imitation")
    and Eq. [10](#S2.E10 "In II-B1 Value-based Methods ‣ II-B RL Algorithms ‣ II Deep
    reinforcement learning ‣ A Survey of Deep Network Solutions for Learning Control
    in Robotics: From Reinforcement to Imitation") respectively, we have the two most
    well-known value-based RL methods: SARSA and Q-learning, which follow the same
    recursive backup procedures, given as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 根据公式 [9](#S2.E9 "在 II-B1 基于价值的方法 ‣ II-B 强化学习算法 ‣ II 深度强化学习 ‣ 机器人学习控制的深度网络解决方案综述：从强化学习到模仿学习")
    和公式 [10](#S2.E10 "在 II-B1 基于价值的方法 ‣ II-B 强化学习算法 ‣ II 深度强化学习 ‣ 机器人学习控制的深度网络解决方案综述：从强化学习到模仿学习")，我们得到了两种最著名的基于价值的强化学习方法：SARSA
    和 Q-learning，这两种方法遵循相同的递归备份过程，如下所示：
- en: '|  | $\displaystyle Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})$ | $\displaystyle\leftarrow
    Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})+\alpha\delta_{t},$ |  | (11) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})$ | $\displaystyle\leftarrow
    Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})+\alpha\delta_{t},$ |  | (11) |'
- en: '|  | $\displaystyle\delta_{t}$ | $\displaystyle=\mathbf{y}_{t}-Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t}).$
    |  | (12) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\delta_{t}$ | $\displaystyle=\mathbf{y}_{t}-Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t}).$
    |  | (12) |'
- en: In this estimation procedure, $Q$-values are recursively updated by a step size
    of $\alpha$ towards a target value $\mathbf{y}_{t}$. $\delta_{t}$ is termed the
    td-error (temporal difference error), and $\mathbf{y}_{t}$ the td-target.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个估计过程中，$Q$-值通过步长 $\alpha$ 递归地更新到目标值 $\mathbf{y}_{t}$。$\delta_{t}$ 被称为 td-error（时序差分误差），$\mathbf{y}_{t}$
    则是 td-target。
- en: 'The difference between SARSA and Q-learing comes in their td-target’s. Below,
    we list the td-targets for SARSA and Q-learing in Eq. [13](#S2.E13 "In II-B1 Value-based
    Methods ‣ II-B RL Algorithms ‣ II Deep reinforcement learning ‣ A Survey of Deep
    Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation")
    and Eq. [14](#S2.E14 "In II-B1 Value-based Methods ‣ II-B RL Algorithms ‣ II Deep
    reinforcement learning ‣ A Survey of Deep Network Solutions for Learning Control
    in Robotics: From Reinforcement to Imitation") respectively:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA 和 Q-learning 之间的区别在于它们的 td-target。以下是 SARSA 和 Q-learning 的 td-target，分别列在公式
    [13](#S2.E13 "在 II-B1 基于价值的方法 ‣ II-B 强化学习算法 ‣ II 深度强化学习 ‣ 机器人学习控制的深度网络解决方案综述：从强化学习到模仿学习")
    和公式 [14](#S2.E14 "在 II-B1 基于价值的方法 ‣ II-B 强化学习算法 ‣ II 深度强化学习 ‣ 机器人学习控制的深度网络解决方案综述：从强化学习到模仿学习")
    中：
- en: '|  | $\displaystyle\mathbf{y}_{t}^{\textit{SARSA}}$ | $\displaystyle=R_{t+1}+\gamma
    Q^{\pi}(\mathbf{s}_{t+1},\mathbf{a}_{t+1}),$ |  | (13) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{y}_{t}^{\textit{SARSA}}$ | $\displaystyle=R_{t+1}+\gamma
    Q^{\pi}(\mathbf{s}_{t+1},\mathbf{a}_{t+1}),$ |  | (13) |'
- en: '|  | $\displaystyle\mathbf{y}_{t}^{\textit{Q-learning}}$ | $\displaystyle=R_{t+1}+\gamma\max_{\mathbf{a}^{\prime}}Q^{\pi}(\mathbf{s}_{t+1},\mathbf{a}^{\prime}).$
    |  | (14) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{y}_{t}^{\textit{Q-learning}}$ | $\displaystyle=R_{t+1}+\gamma\max_{\mathbf{a}^{\prime}}Q^{\pi}(\mathbf{s}_{t+1},\mathbf{a}^{\prime}).$
    |  | (14) |'
- en: SARSA updates its $Q$-value estimates using the transitions generated by following
    the behavioural policy $\pi$, which makes SARSA an on-policy method; Q-learning,
    on the other hand, is off-policy, since its value estimations are updated not
    towards the behavioural policy, but towards a target optimal policy.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA使用跟随行为策略$\pi$生成的转移来更新其$Q$-值估计，这使得SARSA成为一种基于策略的方法；而Q-learning则是离策略的，因为其价值估计不是朝向行为策略更新，而是朝向目标最优策略更新。
- en: There are also other value-based methods, such as Monte-Carlo control, which
    uses the true return of complete trajectories as its update target instead of
    bootstrapping from old estimates, and $\lambda$-variants, which mix the sample
    return and 1-step lookahead estimations.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他基于价值的方法，如蒙特卡罗控制，它使用完整轨迹的真实回报作为更新目标，而不是从旧的估计中引导，和$\lambda$-变体，它混合了样本回报和1步前瞻估计。
- en: 'A reformulation of the $Q$-value function, the successor representation (Dayan,
    [1993](#bib.bib15)), is also studied in the recent literature (Kulkarni et al.,
    [2016](#bib.bib53); Barreto et al., [2017](#bib.bib5); Zhang et al., [2017a](#bib.bib120)):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: $Q$-值函数的一种重新表述，后继表示（Dayan, [1993](#bib.bib15)），在最近的文献中也有研究（Kulkarni et al.,
    [2016](#bib.bib53); Barreto et al., [2017](#bib.bib5); Zhang et al., [2017a](#bib.bib120)）：
- en: '|  | $\displaystyle R_{t+1}(\mathbf{s}_{t},\mathbf{a}_{t})$ | $\displaystyle=\phi(\mathbf{s}_{t},\mathbf{a}_{t})^{\top}\cdot\omega,$
    |  | (15) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R_{t+1}(\mathbf{s}_{t},\mathbf{a}_{t})$ | $\displaystyle=\phi(\mathbf{s}_{t},\mathbf{a}_{t})^{\top}\cdot\omega,$
    |  | (15) |'
- en: '|  | $\displaystyle Q^{\pi}(\mathbf{s},\mathbf{a})$ | $\displaystyle=\psi^{\pi}(\mathbf{s},\mathbf{a})^{\top}\cdot\omega,$
    |  | (16) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{\pi}(\mathbf{s},\mathbf{a})$ | $\displaystyle=\psi^{\pi}(\mathbf{s},\mathbf{a})^{\top}\cdot\omega,$
    |  | (16) |'
- en: where
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $\displaystyle\psi^{\pi}(\mathbf{s},\mathbf{a})$ | $\displaystyle=\mathbb{E}_{\pi}\left[\sum_{k=t}^{T}\gamma^{k-t}\phi(\mathbf{s}_{k},\mathbf{a}_{k})&#124;\mathbf{s}_{t}=\mathbf{s},\mathbf{a}_{t}=\mathbf{a}\right],$
    |  | (17) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\psi^{\pi}(\mathbf{s},\mathbf{a})$ | $\displaystyle=\mathbb{E}_{\pi}\left[\sum_{k=t}^{T}\gamma^{k-t}\phi(\mathbf{s}_{k},\mathbf{a}_{k})&#124;\mathbf{s}_{t}=\mathbf{s},\mathbf{a}_{t}=\mathbf{a}\right],$
    |  | (17) |'
- en: is termed the successor feature. This line of formulation decouples the task
    specific reward estimation into the estimation of representative features $\phi(\cdot)$
    and a reward weight $\omega$, and the estimation of the expected occurrence of
    the features $\phi(\cdot)$ under specific world dynamics following a specific
    policy. It combines the computational efficiency of model-free methods with the
    flexibility of some model-based methods. We refer readers to Dayan ([1993](#bib.bib15)),
    Kulkarni et al. ([2016](#bib.bib53)), Barreto et al. ([2017](#bib.bib5)) and Zhang
    et al. ([2017a](#bib.bib120)) for more detailed discussions and extensions.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为后继特征。这种表述将任务特定的回报估计解耦为代表性特征$\phi(\cdot)$的估计和回报权重$\omega$的估计，以及在特定策略下的特定世界动态下特征$\phi(\cdot)$的期望发生率的估计。它结合了无模型方法的计算效率和某些基于模型方法的灵活性。我们建议读者参考Dayan
    ([1993](#bib.bib15))、Kulkarni et al. ([2016](#bib.bib53))、Barreto et al. ([2017](#bib.bib5))
    和 Zhang et al. ([2017a](#bib.bib120)) 以获取更详细的讨论和扩展。
- en: II-B2 Policy-based Methods
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 基于策略的方法
- en: Unlike value-based methods, policy-based methods do not maintain value estimations,
    but work directly on policies. When it comes to high-dimensional or continuous
    action spaces, policy-based methods generally give much more effective solutions
    than value-based approaches. They can learn stochastic policies instead of just
    deterministic policies, and have better convergence properties.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于价值的方法不同，基于策略的方法不维护价值估计，而是直接作用于策略。对于高维或连续动作空间，基于策略的方法通常比基于价值的方法提供更有效的解决方案。它们可以学习随机策略而不仅仅是确定性策略，并且具有更好的收敛性。
- en: 'Policy-based approaches operate on parameterized policies, and search for parameters
    that maximize the policy objective function. The policy search can be carried
    out in two paradigms: gradient-free (Fu et al., [2005](#bib.bib27); Szita and
    Lörincz, [2006](#bib.bib97)) and gradient-based. We focus on the gradient descent
    methods from the gradient-based family as they remain the method of choice in
    recent studies. More formally, given policy $\pi_{\theta}(\cdot)$ with parameters
    $\theta$, policy optimization searches for the best $\theta$ that maximizes an
    objective function $\mathcal{J}(\pi_{\theta})$:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略的方法作用于参数化的策略，并搜索能够最大化策略目标函数的参数。策略搜索可以在两种范式中进行：无梯度（Fu et al., [2005](#bib.bib27);
    Szita 和 Lörincz, [2006](#bib.bib97)）和基于梯度的方法。我们关注于基于梯度的家族中的梯度下降方法，因为它们在最近的研究中仍然是首选的方法。更正式地说，给定具有参数$\theta$的策略$\pi_{\theta}(\cdot)$，策略优化搜索最佳的$\theta$，使目标函数$\mathcal{J}(\pi_{\theta})$最大化：
- en: '|  | $\displaystyle\mathcal{J}(\pi_{\theta})$ | $\displaystyle=\mathbb{E}_{\pi_{\theta}}[f_{\pi_{\theta}}(\cdot)].$
    |  | (18) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{J}(\pi_{\theta})$ | $\displaystyle=\mathbb{E}_{\pi_{\theta}}[f_{\pi_{\theta}}(\cdot)].$
    |  | (18) |'
- en: Here, $f_{\pi_{\theta}}(\cdot)$ is a score function, which judges the goodness
    of a policy. There are multiple valid choices for the score function; we refer
    readers to Schulman et al. ([2015b](#bib.bib88)) for a full discussion.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$f_{\pi_{\theta}}(\cdot)$是一个评分函数，用于判断策略的优劣。评分函数有多种有效选择；我们建议读者参考Schulman
    et al. ([2015b](#bib.bib88))以获得详细讨论。
- en: The policy gradient is defined as
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度定义为
- en: '|  | $\displaystyle\nabla_{\theta}\mathcal{J}(\pi_{\theta})$ | $\displaystyle=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}\cdot
    f_{\pi_{\theta}}(\cdot)\right].$ |  | (19) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{\theta}\mathcal{J}(\pi_{\theta})$ | $\displaystyle=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta}\log\pi_{\theta}\cdot
    f_{\pi_{\theta}}(\cdot)\right].$ |  | (19) |'
- en: 'Intuitively speaking, firstly, some actions, experiences or trajectories are
    sampled following the current policy $\pi_{\theta}$ and the goodness of those
    samples is given by $f_{\pi_{\theta}}(\cdot)$, the score function and $\nabla_{\theta}\log\pi_{\theta}$
    points out the direction in the parameter space that would lead to an increase
    of the probability of those actions being sampled. Thus, by ascending along the
    policy gradient given in Eq. [19](#S2.E19 "In II-B2 Policy-based Methods ‣ II-B
    RL Algorithms ‣ II Deep reinforcement learning ‣ A Survey of Deep Network Solutions
    for Learning Control in Robotics: From Reinforcement to Imitation"), we end up
    with policies that are capable of generating samples with higher scores.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，首先，某些动作、经历或轨迹是按照当前策略$\pi_{\theta}$进行采样的，这些样本的优劣由$f_{\pi_{\theta}}(\cdot)$评分函数给出，而$\nabla_{\theta}\log\pi_{\theta}$指出了在参数空间中能增加这些动作被采样概率的方向。因此，通过沿着方程[19](#S2.E19
    "在 II-B2 基于策略的方法 ‣ II-B 强化学习算法 ‣ II 深度强化学习 ‣ 深度网络解决方案概述：从强化学习到模仿学习")中给出的策略梯度上升，我们最终得到能够生成具有更高评分样本的策略。
- en: 'The standard REINFORCE algorithm (Williams, [1992](#bib.bib113)), a well-known
    method in RL, plugs in the sample return as the score function:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 标准REINFORCE算法（Williams, [1992](#bib.bib113)），是强化学习中的一种著名方法，将样本回报作为评分函数：
- en: '|  | $\displaystyle f_{\pi_{\theta}}(\cdot)$ | $\displaystyle=G_{t}.$ |  |
    (20) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f_{\pi_{\theta}}(\cdot)$ | $\displaystyle=G_{t}.$ |  |
    (20) |'
- en: 'This algorithm, however, suffers from the very high variance. A common way
    to reduce the variance of the estimation while keeping it unbiased is by subtracting
    a baseline $b(\mathbf{s})$ from the return:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，该算法存在非常高的方差。减少估计方差同时保持其无偏的一种常见方法是从回报中减去基线$b(\mathbf{s})$：
- en: '|  | $\displaystyle f_{\pi_{\theta}}(\cdot)$ | $\displaystyle=G_{t}-b_{t}(\mathbf{s}_{t}).$
    |  | (21) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f_{\pi_{\theta}}(\cdot)$ | $\displaystyle=G_{t}-b_{t}(\mathbf{s}_{t}).$
    |  | (21) |'
- en: A commonly used baseline is a learned estimate of the state-value function $V(\mathbf{s})$.
    This leads us to the actor-critic class of algorithms, since it involves estimating
    the value functions along with policy search.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常用的基线是对状态值函数$V(\mathbf{s})$的学习估计。这引导我们到演员-评论家类算法，因为它涉及估计值函数以及策略搜索。
- en: Before we go into actor-critic methods, several details are worthy of pointing
    out.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨演员-评论家方法之前，有几个细节值得指出。
- en: Firstly, directly following the policy gradient might not be desirable in the
    robotics setting, since hardware constraints and safety requirements should be
    carefully dealt with. Popular approaches for cautious exploration include avoiding
    significant changes in the policy, or explicitly discouraging entering undesired
    regions in the state space (Deisenroth et al., [2013](#bib.bib16)).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在机器人设置中，直接跟随政策梯度可能不太理想，因为硬件限制和安全要求需要谨慎处理。流行的谨慎探索方法包括避免政策的重大变化，或明确地阻止进入状态空间中的不期望区域（Deisenroth
    et al., [2013](#bib.bib16)）。
- en: 'We also note that, so far, we have only been discussing the policy gradient
    for the stochasitic polices, which integrate over both the state and action spaces,
    and might not be efficient in high-dimentional action spaces. The deterministic
    policy gradient (Silver et al., [2014](#bib.bib91)), on the other hand, only requires
    integrating over the state space, which makes it a much more sample-efficient
    algorithm. Below we list the stochastic policy gradient (for $\pi_{\theta}(\mathbf{a}|\mathbf{s})$,
    Eq. [22](#S2.E22 "In II-B2 Policy-based Methods ‣ II-B RL Algorithms ‣ II Deep
    reinforcement learning ‣ A Survey of Deep Network Solutions for Learning Control
    in Robotics: From Reinforcement to Imitation")) and the deterministic policy gradient
    (for $\mu_{\theta}(\mathbf{s})$, Eq. [23](#S2.E23 "In II-B2 Policy-based Methods
    ‣ II-B RL Algorithms ‣ II Deep reinforcement learning ‣ A Survey of Deep Network
    Solutions for Learning Control in Robotics: From Reinforcement to Imitation"))
    when using the $Q$-value function as their score function:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还注意到，到目前为止，我们只讨论了针对随机策略的政策梯度，它在状态和动作空间上进行积分，在高维动作空间中可能效率不高。另一方面，确定性政策梯度（Silver
    et al., [2014](#bib.bib91)）只需要在状态空间上进行积分，这使得它成为一个更具样本效率的算法。下面我们列出使用$Q$值函数作为其得分函数的随机政策梯度（针对$\pi_{\theta}(\mathbf{a}|\mathbf{s})$，方程[22](#S2.E22
    "在II-B2政策基础方法 ‣ II-B RL算法 ‣ II 深度强化学习 ‣ 机器人控制深度网络解决方案调查：从强化学习到模仿学习")）和确定性政策梯度（针对$\mu_{\theta}(\mathbf{s})$，方程[23](#S2.E23
    "在II-B2政策基础方法 ‣ II-B RL算法 ‣ II 深度强化学习 ‣ 机器人控制深度网络解决方案调查：从强化学习到模仿学习")）：
- en: '|  | $\displaystyle\nabla_{\theta}\mathcal{J}(\pi_{\theta})$ | $\displaystyle=\mathbb{E}_{\mathbf{s},\mathbf{a}}\left[\nabla_{\theta}\log\pi_{\theta}(\mathbf{a}&#124;\mathbf{s})\cdot
    Q^{\pi}(\mathbf{s},\mathbf{a})\right],$ |  | (22) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{\theta}\mathcal{J}(\pi_{\theta})$ | $\displaystyle=\mathbb{E}_{\mathbf{s},\mathbf{a}}\left[\nabla_{\theta}\log\pi_{\theta}(\mathbf{a}&#124;\mathbf{s})\cdot
    Q^{\pi}(\mathbf{s},\mathbf{a})\right],$ |  | (22) |'
- en: '|  | $\displaystyle\nabla_{\theta}\mathcal{J}(\mu_{\theta})$ | $\displaystyle=\mathbb{E}_{\mathbf{s}}\left[\nabla_{\theta}\mu_{\theta}(\mathbf{s})\cdot
    Q^{\mu}(\mathbf{s},\mu_{\theta}(\mathbf{s}))\right].$ |  | (23) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{\theta}\mathcal{J}(\mu_{\theta})$ | $\displaystyle=\mathbb{E}_{\mathbf{s}}\left[\nabla_{\theta}\mu_{\theta}(\mathbf{s})\cdot
    Q^{\mu}(\mathbf{s},\mu_{\theta}(\mathbf{s}))\right].$ |  | (23) |'
- en: II-B3 Actor-critic Methods
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B3 演员-评论家方法
- en: 'Following on from the discussions of the policy-based methods, actor-critic
    algorithms maintain an explicit representation of both the policy (the actor)
    and the value estimates (the critic). The most widely used actor-critic algorithms
    use the following score function:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 继政策基础方法的讨论之后，演员-评论家算法维护了对政策（演员）和价值估计（评论家）的明确表示。最广泛使用的演员-评论家算法使用以下得分函数：
- en: '|  | $\displaystyle f_{\pi_{\theta}}(\cdot)$ | $\displaystyle=Q^{\pi_{\theta}}(\mathbf{s}_{t},\mathbf{a}_{t})-V^{\pi_{\theta}}(\mathbf{s}_{t}).$
    |  | (24) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f_{\pi_{\theta}}(\cdot)$ | $\displaystyle=Q^{\pi_{\theta}}(\mathbf{s}_{t},\mathbf{a}_{t})-V^{\pi_{\theta}}(\mathbf{s}_{t}).$
    |  | (24) |'
- en: 'Compared againest Eq. [21](#S2.E21 "In II-B2 Policy-based Methods ‣ II-B RL
    Algorithms ‣ II Deep reinforcement learning ‣ A Survey of Deep Network Solutions
    for Learning Control in Robotics: From Reinforcement to Imitation"), Eq. [24](#S2.E24
    "In II-B3 Actor-critic Methods ‣ II-B RL Algorithms ‣ II Deep reinforcement learning
    ‣ A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation") replaces the return $G_{t}$ with its unbiased estimate $Q^{\pi_{\theta}}(\mathbf{s}_{t},\mathbf{a}_{t})$,
    and uses $V^{\pi_{\theta}}(\mathbf{s}_{t})$ as its baseline function to reduce
    variance. In fact,'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于方程[21](#S2.E21 "在II-B2政策基础方法 ‣ II-B RL算法 ‣ II 深度强化学习 ‣ 机器人控制深度网络解决方案调查：从强化学习到模仿学习")，方程[24](#S2.E24
    "在II-B3演员-评论家方法 ‣ II-B RL算法 ‣ II 深度强化学习 ‣ 机器人控制深度网络解决方案调查：从强化学习到模仿学习") 用其无偏估计$Q^{\pi_{\theta}}(\mathbf{s}_{t},\mathbf{a}_{t})$替代返回值$G_{t}$，并使用$V^{\pi_{\theta}}(\mathbf{s}_{t})$作为基线函数以减少方差。实际上，
- en: '|  | $\displaystyle A(\mathbf{s},\mathbf{a})$ | $\displaystyle=Q(\mathbf{s},\mathbf{a})-V(\mathbf{s})$
    |  | (25) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A(\mathbf{s},\mathbf{a})$ | $\displaystyle=Q(\mathbf{s},\mathbf{a})-V(\mathbf{s})$
    |  | (25) |'
- en: is called the advantage function, which estimates the advantage of taking a
    particular action $\mathbf{a}$ in state $\mathbf{s}$.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这称为优势函数，它估计在状态 $\mathbf{s}$ 下采取特定动作 $\mathbf{a}$ 的优势。
- en: II-B4 Integraing Planning and Learning
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B4 规划与学习的整合
- en: So far, we have been discussing model-free methods where the agent is not provided
    with the underlying transition model and simply learns optimal behaviors from
    experiences. There also exists another branch of model-based algorithms where
    a model is learned from experiences. with which the agent can interact and collect
    imaginary rollouts (Sutton, [1991](#bib.bib95)), It has also been extended with
    DRL methods (Weber et al., [2017](#bib.bib112); Kalweit and Boedecker, [2017](#bib.bib46)).
    However, the need for learning a model brings in another source of approximation
    error, and model-based RL can only perform as well as the estimated model. This
    problem might be partially dealt with by Model Predicted Control (MPC) methods,
    which are not a focus of this survey, so we will skip the details.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了无模型方法，其中代理没有提供底层过渡模型，而是通过经验学习最优行为。还有另一类基于模型的算法，其中从经验中学习模型，代理可以与模型互动并收集虚拟回合（Sutton,
    [1991](#bib.bib95)）。这些方法也已经与 DRL 方法结合扩展（Weber et al., [2017](#bib.bib112); Kalweit
    和 Boedecker, [2017](#bib.bib46)）。然而，学习模型的需求带来了另一种近似误差来源，而基于模型的强化学习的表现只能与估计模型的效果相匹配。这个问题可能会通过模型预测控制（MPC）方法部分解决，这些方法不是本次综述的重点，因此我们将跳过详细讨论。
- en: II-C DRL Algorithms
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C DRL 算法
- en: Recent successes of DRL have extended the aforementioned algorithms to the high-dimensional
    domain, by deploying deep neural networks as powerful non-linear function approximators
    for the optimal value functions $V^{*}(\mathbf{s}),Q^{*}(\mathbf{s},\mathbf{a}),A^{*}(\mathbf{s},\mathbf{a})$,
    and the optimal policies $\pi^{*}(\mathbf{a}|\mathbf{s})$, $\mu^{*}(\mathbf{s})$.
    They usually take the observations as input (e.g, raw pixel images from Atari
    emulators (Mnih et al., [2015](#bib.bib65)) or joint angles of robot arms ), and
    output either the $Q$-values, from which greedy actions are selected, or policies
    that can be directly used to execute agents. In the following, we cover the most
    influential DRL algorithms.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）的近期成功将上述算法扩展到了高维领域，通过将深度神经网络作为强大的非线性函数逼近器用于最优价值函数 $V^{*}(\mathbf{s}),Q^{*}(\mathbf{s},\mathbf{a}),A^{*}(\mathbf{s},\mathbf{a})$
    和最优策略 $\pi^{*}(\mathbf{a}|\mathbf{s})$, $\mu^{*}(\mathbf{s})$。它们通常将观察作为输入（例如，来自
    Atari 模拟器的原始像素图像（Mnih et al., [2015](#bib.bib65)）或机器人手臂的关节角度），并输出 $Q$-值，通过这些值选择贪婪动作，或者输出可以直接用于执行代理的策略。接下来，我们将介绍最具影响力的
    DRL 算法。
- en: II-C1 DQN (Mnih et al., [2015](#bib.bib65))
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 DQN (Mnih et al., [2015](#bib.bib65))
- en: ': As a value-based method, DQN approximates the optimal $Q$-value function
    with a deep convolutional neural network, called the deep Q-network, whose weights
    we denote as $\theta^{Q}$: $Q(\mathbf{s},\mathbf{a};\theta^{Q})\approx Q^{*}(\mathbf{s},\mathbf{a})$.
    In turn, the td-error (Eq. [12](#S2.E12 "In II-B1 Value-based Methods ‣ II-B RL
    Algorithms ‣ II Deep reinforcement learning ‣ A Survey of Deep Network Solutions
    for Learning Control in Robotics: From Reinforcement to Imitation")) and the td-target
    (Eq. [14](#S2.E14 "In II-B1 Value-based Methods ‣ II-B RL Algorithms ‣ II Deep
    reinforcement learning ‣ A Survey of Deep Network Solutions for Learning Control
    in Robotics: From Reinforcement to Imitation")) from the standard Q-learning are
    adopted into:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ': 作为一种基于价值的方法，DQN 使用深度卷积神经网络来逼近最优 $Q$-值函数，该网络称为深度 Q 网络，我们将其权重表示为 $\theta^{Q}$：$Q(\mathbf{s},\mathbf{a};\theta^{Q})\approx
    Q^{*}(\mathbf{s},\mathbf{a})$。反过来，从标准 Q 学习中采用了 td-误差（方程 [12](#S2.E12 "在 II-B1
    基于价值的方法 ‣ II-B RL 算法 ‣ II 深度强化学习 ‣ 深度网络解决方案概述：从强化学习到模仿学习")）和 td-目标（方程 [14](#S2.E14
    "在 II-B1 基于价值的方法 ‣ II-B RL 算法 ‣ II 深度强化学习 ‣ 深度网络解决方案概述：从强化学习到模仿学习")）：'
- en: '|  | $\displaystyle\delta_{t}^{\text{DQN}}$ | $\displaystyle=\mathbf{y}_{t}^{\text{DQN}}-Q(\mathbf{s}_{t},\mathbf{a}_{t};\theta^{Q}_{t}),$
    |  | (26) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\delta_{t}^{\text{DQN}}$ | $\displaystyle=\mathbf{y}_{t}^{\text{DQN}}-Q(\mathbf{s}_{t},\mathbf{a}_{t};\theta^{Q}_{t}),$
    |  | (26) |'
- en: '|  | $\displaystyle\mathbf{y}_{t}^{\text{DQN}}$ | $\displaystyle=R_{t+1}+\gamma\max_{\mathbf{a}^{\prime}}Q(\mathbf{s}_{t+1},\mathbf{a}^{\prime};\theta^{-}_{t}).$
    |  | (27) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{y}_{t}^{\text{DQN}}$ | $\displaystyle=R_{t+1}+\gamma\max_{\mathbf{a}^{\prime}}Q(\mathbf{s}_{t+1},\mathbf{a}^{\prime};\theta^{-}_{t}).$
    |  | (27) |'
- en: 'Then an update step is performed based on the following gradient calculation
    with a learning rate of $\alpha$:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后根据以下梯度计算进行更新步骤，学习率为$\alpha$：
- en: '|  | $\displaystyle\theta_{t+1}$ | $\displaystyle\leftarrow\theta_{t}-\alpha\cdot\left(\partial\left(\delta^{\text{DQN}}_{t}(\theta^{Q}_{t})\right)^{2}/\partial{\theta^{Q}_{t}}\right).$
    |  | (28) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta_{t+1}$ | $\displaystyle\leftarrow\theta_{t}-\alpha\cdot\left(\partial\left(\delta^{\text{DQN}}_{t}(\theta^{Q}_{t})\right)^{2}/\partial{\theta^{Q}_{t}}\right).$
    |  | (28) |'
- en: 'Two main techniques have been proposed in DQN to stabilize learning: target-network
    and experience replay.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在DQN中，提出了两种主要技术来稳定学习：目标网络和经验回放。
- en: 'Target-network: In Eq. [27](#S2.E27 "In II-C1 DQN (Mnih et al., 2015) ‣ II-C
    DRL Algorithms ‣ II Deep reinforcement learning ‣ A Survey of Deep Network Solutions
    for Learning Control in Robotics: From Reinforcement to Imitation"), the td-target
    is computed using the output from a target-network $\theta^{-}$, instead of the
    Q-network $\theta^{Q}$. The target-network and the Q-network share the same network
    architecture, but only the weights of the Q-network are learned and updated. The
    weights of the Q-network $\theta^{Q}$ are only periodically copied to the target-network
    $\theta^{-}$. This reduces the correlations of the estimated $Q$-values with the
    target estimations. There is also soft update (Lillicrap et al., [2015](#bib.bib59)),
    where a small portion of $\theta^{Q}$ are mixed into $\theta^{-}$ in every iteration,
    instead of the hard update used in the original DQN, where $\theta^{Q}$ are directly
    and completely copied to $\theta^{-}$ every several (e.g, $10,000$) iterations.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 目标网络：在公式 [27](#S2.E27 "在 II-C1 DQN (Mnih et al., 2015) ‣ II-C DRL 算法 ‣ II 深度强化学习
    ‣ 一项关于深度网络解决方案用于机器人学习控制的调查：从强化学习到模仿学习") 中，td-target是使用目标网络$\theta^{-}$的输出计算的，而不是Q网络$\theta^{Q}$的输出。目标网络和Q网络共享相同的网络架构，但只有Q网络的权重会被学习和更新。Q网络的权重$\theta^{Q}$只是定期复制到目标网络$\theta^{-}$中。这减少了估计的$Q$值与目标估计的相关性。还有一种软更新方法（Lillicrap等，[2015](#bib.bib59)），在每次迭代中将$\theta^{Q}$的小部分混入$\theta^{-}$中，而不是原始DQN中使用的硬更新方法，其中$\theta^{Q}$会在每几个（例如$10,000$）迭代中直接完全复制到$\theta^{-}$。
- en: 'Experience replay: In this technique, instead of directly using the incoming
    frames from the online interactions, the collected experiences are firstly stored
    into a replay memory. During training, random samples are drawn from the replay
    memory ($4$ consecutive observations are stacked together to form a state, so
    as to deal with the partial observability) to be fed into the network as mini-batches.
    This way, gradient descent methods from the supervised learning literature can
    be safely used, to minimize the min-squared error between the predicted $Q$-values
    (output by the Q-network) and the target $Q$-values (output by the target-network).
    Experience replay thereby removes the temporal correlations in the consecutive
    observations, and smoothes over changes in the online data distribution.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 经验回放：在这种技术中，不是直接使用来自在线交互的输入帧，而是将收集的经验首先存储到回放记忆中。在训练过程中，从回放记忆中随机抽取样本（将$4$个连续观察堆叠在一起形成一个状态，以应对部分可观察性）作为小批量输入网络。这样，可以安全地使用来自监督学习文献的梯度下降方法，以最小化预测$Q$值（由Q网络输出）和目标$Q$值（由目标网络输出）之间的最小平方误差。经验回放从而消除了连续观察中的时间相关性，并平滑了在线数据分布的变化。
- en: 'Further techniques have been proposed on the basis of DQN to stabilize learning
    and improve efficiency: Double DQN (Van Hasselt et al., [2016](#bib.bib107)) and
    Dueling DQN (Wang et al., [2016b](#bib.bib110)).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在DQN的基础上，提出了进一步的技术以稳定学习和提高效率：双重DQN（Van Hasselt等，[2016](#bib.bib107)）和对抗DQN（Wang等，[2016b](#bib.bib110)）。
- en: 'For Double DQN (Van Hasselt et al., [2016](#bib.bib107)), the greedy action
    is chosen based on the output from $\theta^{Q}$ (the original DQN uses $\theta^{-}$),
    then the target $Q$-value of the chosen greedy action is computed using $\theta^{-}$
    (Eq. [29](#S2.E29 "In II-C1 DQN (Mnih et al., 2015) ‣ II-C DRL Algorithms ‣ II
    Deep reinforcement learning ‣ A Survey of Deep Network Solutions for Learning
    Control in Robotics: From Reinforcement to Imitation")). This prevents overoptimistic
    value estimates and avoids upward bias:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于双重DQN（Van Hasselt等，[2016](#bib.bib107)），贪婪动作是基于$\theta^{Q}$的输出选择的（原始DQN使用$\theta^{-}$），然后选择的贪婪动作的目标$Q$值是使用$\theta^{-}$计算的（公式
    [29](#S2.E29 "在 II-C1 DQN (Mnih et al., 2015) ‣ II-C DRL 算法 ‣ II 深度强化学习 ‣ 一项关于深度网络解决方案用于机器人学习控制的调查：从强化学习到模仿学习")）。这防止了过于乐观的价值估计并避免了上升的偏差：
- en: '|  | $\displaystyle\mathbf{y}_{t}^{\text{Double}}$ | $\displaystyle=R_{t+1}+\gamma
    Q(\mathbf{s}_{t+1},\operatorname*{\arg\!\max}_{\mathbf{a}^{\prime}}Q(\mathbf{s}_{t+1},\mathbf{a}^{\prime};\theta^{Q}_{t});\theta^{-}_{t}).$
    |  | (29) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{y}_{t}^{\text{Double}}$ | $\displaystyle=R_{t+1}+\gamma
    Q(\mathbf{s}_{t+1},\operatorname*{\arg\!\max}_{\mathbf{a}^{\prime}}Q(\mathbf{s}_{t+1},\mathbf{a}^{\prime};\theta^{Q}_{t});\theta^{-}_{t}).$
    |  | (29) |'
- en: For Dueling DQN (Wang et al., [2016b](#bib.bib110)), two output heads are used
    to estimate the state-value $V$ and the advantage $A$ respectively for each action.
    This helps the agent to efficiently learn which states are valuable, without having
    to learn the effect of each action for each state.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Dueling DQN（Wang等，[2016b](#bib.bib110)），使用两个输出头分别估计每个动作的状态值$V$和优势$A$。这有助于智能体有效地学习哪些状态是有价值的，而无需为每个状态学习每个动作的效果。
- en: II-C2 DDPG (Lillicrap et al., [2015](#bib.bib59))
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 DDPG（Lillicrap等，[2015](#bib.bib59)）
- en: ': DQN can deal with high-dimensional state spaces, but is only capable of handling
    discrete and low-dimensional action spaces. The deep deterministic policy gradient
    (DDPG) combines techniques from DQN with actor-critic methods, targeting solving
    continuous control tasks from raw pixels inputs.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ': DQN可以处理高维状态空间，但仅能处理离散且低维的动作空间。深度确定性策略梯度（DDPG）结合了DQN和演员-评论员方法的技术，旨在解决从原始像素输入的连续控制任务。'
- en: 'If we write out the expectation in Eq. [9](#S2.E9 "In II-B1 Value-based Methods
    ‣ II-B RL Algorithms ‣ II Deep reinforcement learning ‣ A Survey of Deep Network
    Solutions for Learning Control in Robotics: From Reinforcement to Imitation")
    for the stochasitic policy $\pi(\mathbf{a}|\mathbf{s})$ and deterministic policy
    $\mu(\mathbf{s})$, we get ($E$ represents the environment that the agent is interacting
    with)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们写出方程[9](#S2.E9 "在II-B1 基于价值的方法 ‣ II-B 强化学习算法 ‣ II 深度强化学习 ‣ 机器人控制深度网络解决方案调查：从强化学习到模仿学习")中随机策略$\pi(\mathbf{a}|\mathbf{s})$和确定性策略$\mu(\mathbf{s})$的期望，我们得到（$E$代表智能体正在交互的环境）
- en: '|  | $\displaystyle Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})=$ |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})=$ |  |'
- en: '|  | $\displaystyle\mathbb{E}_{R_{t+1},\mathbf{s}_{t+1}\sim E}\left[R_{t+1}+\gamma\mathbb{E}_{\mathbf{a}_{t+1}\sim{\pi}}\left[Q^{\pi}(\mathbf{s}_{t+1},\mathbf{a}_{t+1})\right]\right],$
    |  | (30) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{R_{t+1},\mathbf{s}_{t+1}\sim E}\left[R_{t+1}+\gamma\mathbb{E}_{\mathbf{a}_{t+1}\sim{\pi}}\left[Q^{\pi}(\mathbf{s}_{t+1},\mathbf{a}_{t+1})\right]\right],$
    |  | (30) |'
- en: '|  | $\displaystyle Q^{\mu}(\mathbf{s}_{t},\mathbf{a}_{t})=$ |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{\mu}(\mathbf{s}_{t},\mathbf{a}_{t})=$ |  |'
- en: '|  | $\displaystyle\mathbb{E}_{R_{t+1},\mathbf{s}_{t+1}\sim E}\left[R_{t+1}+\gamma
    Q^{\mu}(\mathbf{s}_{t+1},\mu(\mathbf{s}_{t+1}))\right].$ |  | (31) |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{R_{t+1},\mathbf{s}_{t+1}\sim E}\left[R_{t+1}+\gamma
    Q^{\mu}(\mathbf{s}_{t+1},\mu(\mathbf{s}_{t+1}))\right].$ |  | (31) |'
- en: 'DDPG represents the $Q$-value estimates with $\theta^{Q}$, and the deterministic
    policy with $\theta^{\mu}$. $\theta^{\mu}$ is learned via the DPG given in Eq.
    [23](#S2.E23 "In II-B2 Policy-based Methods ‣ II-B RL Algorithms ‣ II Deep reinforcement
    learning ‣ A Survey of Deep Network Solutions for Learning Control in Robotics:
    From Reinforcement to Imitation"), and $\theta^{Q}$ is learned following Eq. [31](#S2.E31
    "In II-C2 DDPG (Lillicrap et al., 2015) ‣ II-C DRL Algorithms ‣ II Deep reinforcement
    learning ‣ A Survey of Deep Network Solutions for Learning Control in Robotics:
    From Reinforcement to Imitation"). (Note that different from DQN, where the dependence
    of the $Q$-value on $\mathbf{a}$ is represented by outputting one value for each
    action, the $Q$-network in DDPG deals with this dependence by taking the action
    as input for $\theta^{Q}$.)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG使用$\theta^{Q}$来表示$Q$-值估计，并使用$\theta^{\mu}$来表示确定性策略。$\theta^{\mu}$是通过方程[23](#S2.E23
    "在II-B2 基于策略的方法 ‣ II-B 强化学习算法 ‣ II 深度强化学习 ‣ 机器人控制深度网络解决方案调查：从强化学习到模仿学习")中的DPG学习得到的，而$\theta^{Q}$是按照方程[31](#S2.E31
    "在II-C2 DDPG（Lillicrap等，2015） ‣ II-C 深度强化学习算法 ‣ II 深度强化学习 ‣ 机器人控制深度网络解决方案调查：从强化学习到模仿学习")进行学习的。（注意，不同于DQN，其中$Q$-值对$\mathbf{a}$的依赖通过对每个动作输出一个值来表示，DDPG中的$Q$-网络通过将动作作为$\theta^{Q}$的输入来处理这种依赖。）
- en: II-C3 NAF (Gu et al., [2016](#bib.bib33))
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C3 NAF（Gu等，[2016](#bib.bib33)）
- en: ': Normalized advantage function offers another way to enable $Q$-learning in
    continuous action spaces with deep neural networks and is considerably simpler
    than DDPG. For continuous action problems, standard $Q$-learning is not easily
    directly applicable, as it requires maximizing a complex non-linear function for
    determining the greedy action. The key idea in NAF is to represent the $Q$-value
    function $Q(\mathbf{s},\mathbf{a})$ in such a way that its maximum $\operatorname*{\arg\!\max}_{\mathbf{a}}Q(\mathbf{s},\mathbf{a})$
    can be easily analytically determined during the $Q$-learning update.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ': 归一化优势函数提供了另一种方法，使 $Q$-学习在连续动作空间中能够应用深度神经网络，并且比 DDPG 简单得多。对于连续动作问题，标准 $Q$-学习不容易直接应用，因为它需要最大化一个复杂的非线性函数来确定贪婪动作。NAF
    的关键思想是以一种方式表示 $Q$-值函数 $Q(\mathbf{s},\mathbf{a})$，使得其最大值 $\operatorname*{\arg\!\max}_{\mathbf{a}}Q(\mathbf{s},\mathbf{a})$
    在 $Q$-学习更新期间可以容易地解析确定。'
- en: 'NAF uses the same techniques of target network and experience replay as DQN,
    but differs in the network outputs. Instead of directly outputting the $Q$-value
    estimates, its last hidden layer is connected to three output heads: $\theta^{V}$,
    $\theta^{\mu}$ and $\theta^{L}$. $\theta^{V}$ represents the state value $V(\mathbf{s})$,
    while $\theta^{\mu}$ and $\theta^{L}$ are used for estimating the advantage $A(\mathbf{s},\mathbf{a})$;
    then $Q(\mathbf{s},\mathbf{a})$ can be computed according to Eq. [25](#S2.E25
    "In II-B3 Actor-critic Methods ‣ II-B RL Algorithms ‣ II Deep reinforcement learning
    ‣ A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation"). To give a specific example, e.g, if both $\theta^{\mu}$ and $\theta^{L}$
    are represented with linear layers, with the number of outputs of $\theta^{L}$
    being the square of that of $\theta^{\mu}$ (equal to the action dimensions), then
    the output of $\theta^{L}$ is first reshaped into a matrix, from which $L(\mathbf{s};\theta^{L})$,
    being the lower-triangular of that matrix, is extracted, with the diagonal terms
    exponentiated. Then the advantage can be estimated by'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: NAF 使用了与 DQN 相同的目标网络和经验回放技术，但在网络输出上有所不同。它的最后隐藏层连接到三个输出头：$\theta^{V}$、$\theta^{\mu}$
    和 $\theta^{L}$。$\theta^{V}$ 表示状态值 $V(\mathbf{s})$，而 $\theta^{\mu}$ 和 $\theta^{L}$
    用于估计优势 $A(\mathbf{s},\mathbf{a})$；然后 $Q(\mathbf{s},\mathbf{a})$ 可以根据 Eq. [25](#S2.E25
    "在 II-B3 演员-评论家方法 ‣ II-B RL 算法 ‣ II 深度强化学习 ‣ 关于机器人控制的深度网络解决方案：从强化学习到模仿") 进行计算。举个具体的例子，例如，如果
    $\theta^{\mu}$ 和 $\theta^{L}$ 都通过线性层表示，且 $\theta^{L}$ 的输出数量是 $\theta^{\mu}$ 的输出数量的平方（等于动作维度），那么
    $\theta^{L}$ 的输出首先被重塑为一个矩阵，然后从中提取 $L(\mathbf{s};\theta^{L})$，作为该矩阵的下三角部分，对角线项进行指数化。然后可以通过
- en: '|  | $\displaystyle A(\mathbf{s},\mathbf{a};\theta^{\mu},\theta^{L})$ |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A(\mathbf{s},\mathbf{a};\theta^{\mu},\theta^{L})$ |  |'
- en: '|  | $\displaystyle=-\frac{1}{2}\left(\mathbf{a}-\mu(\mathbf{s};\theta^{\mu})\right)^{T}P(\mathbf{s};\theta^{L})\left(\mathbf{a}-\mu(\mathbf{s};\theta^{\mu})\right),$
    |  | (32) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=-\frac{1}{2}\left(\mathbf{a}-\mu(\mathbf{s};\theta^{\mu})\right)^{T}P(\mathbf{s};\theta^{L})\left(\mathbf{a}-\mu(\mathbf{s};\theta^{\mu})\right),$
    |  | (32) |'
- en: where
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $\displaystyle P(\mathbf{s};\theta^{L})$ | $\displaystyle=L(\mathbf{s};\theta^{L})L(\mathbf{s};\theta^{L})^{T}.$
    |  | (33) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P(\mathbf{s};\theta^{L})$ | $\displaystyle=L(\mathbf{s};\theta^{L})L(\mathbf{s};\theta^{L})^{T}.$
    |  | (33) |'
- en: Although this representation is more restritive than a general network approximator,
    the greedy action for the $Q$-value is always directly given by $\mu(\mathbf{s};\theta^{\mu})$.
    An asynchronous version of NAF has also been proposed (Gu et al., [2017](#bib.bib32)).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种表示比一般的网络逼近器更具限制性，但 $Q$-值的贪婪动作总是由 $\mu(\mathbf{s};\theta^{\mu})$ 直接给出。异步版本的
    NAF 也被提出了（Gu 等人，[2017](#bib.bib32)）。
- en: II-C4 A3C (Mnih et al., [2016](#bib.bib64))
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C4 A3C (Mnih 等人，[2016](#bib.bib64))
- en: ': Minh et. al. proposed several asynchronous DRL algorithms. They deploy multiple
    actor-learners to collect experiences on multiple instances of the environment,
    while each actor-learner accumulates gradients calculated from its own collected
    samples w.r.t. its own set of network parameters $\theta$; these gradients are
    used to update the weights of a shared model $\bm{\theta}$.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ': Minh 等人提出了几种异步 DRL 算法。他们部署了多个演员-学习者，以在环境的多个实例上收集经验，同时每个演员-学习者根据其自身的网络参数 $\theta$
    从自己收集的样本中累积梯度；这些梯度用于更新共享模型 $\bm{\theta}$ 的权重。'
- en: The most effective one, A3C (asynchronous advantage actor-critic), which has
    been very influential and become a standard baseline in recent DRL research, maintains
    a policy representation $\pi(\mathbf{a}|\mathbf{s};\bm{\theta}^{\pi})$ and a value
    estimate $V(\mathbf{s};\bm{\theta}^{V})$. It uses the advantage function as the
    score fucntion in its policy gradient, which is estimated using a mixture of $n$-step
    returns by each actor-learner. To be more specific, each actor-learner thread
    spawns its own copy of the environment and collects rollouts of experiences up
    to $T_{\max}$ (e.g, $20$) steps. After an actor-learner completes a segment of
    a rollout, it accumulates gradients from the experience of every time step contained
    in the rollout $\left\{0,1,\cdots,t,\cdots,T\right\}$ by first estimating the
    advantage function (e.g., for time step $t$) according to the following formulation
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最有效的算法之一是 A3C（异步优势演员-评论家），它在近期的深度强化学习研究中非常有影响力，并已成为标准基线。A3C 维护一个策略表示 $\pi(\mathbf{a}|\mathbf{s};\bm{\theta}^{\pi})$
    和一个价值估计 $V(\mathbf{s};\bm{\theta}^{V})$。它在策略梯度中使用优势函数作为评分函数，该函数通过每个演员-学习者使用 $n$
    步回报的混合来估计。更具体来说，每个演员-学习者线程生成自己环境的副本，并收集最多 $T_{\max}$（例如，$20$）步的经验回合。在演员-学习者完成回合的一个阶段后，它通过首先根据以下公式估计优势函数（例如，对于时间步
    $t$），然后积累回合 $\left\{0,1,\cdots,t,\cdots,T\right\}$ 中每个时间步的经验的梯度
- en: '|  | $\displaystyle A(\mathbf{s}_{t},\mathbf{a}_{t};\theta^{\pi},\theta^{V})=$
    |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A(\mathbf{s}_{t},\mathbf{a}_{t};\theta^{\pi},\theta^{V})=$
    |  |'
- en: '|  | $\displaystyle\left[\sum_{k=t}^{T-1}\left[\gamma^{k-t}R_{k+1}\right]+\gamma^{T-t}V(\mathbf{s}_{T};\theta^{V})-V(\mathbf{s}_{t};\theta^{V});\theta^{\pi}\right],$
    |  | (34) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left[\sum_{k=t}^{T-1}\left[\gamma^{k-t}R_{k+1}\right]+\gamma^{T-t}V(\mathbf{s}_{T};\theta^{V})-V(\mathbf{s}_{t};\theta^{V});\theta^{\pi}\right],$
    |  | (34) |'
- en: 'then calculating the corresponding gradients w.r.t. the its current set of
    network parameters $\theta^{\pi},\theta^{V}$, which are then used to update the
    shared model $\bm{\theta}^{\pi},\bm{\theta}^{V}$:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算相对于其当前网络参数集 $\theta^{\pi},\theta^{V}$ 的对应梯度，这些梯度随后用于更新共享模型 $\bm{\theta}^{\pi},\bm{\theta}^{V}$：
- en: '|  | $\displaystyle d\bm{\theta}^{\pi}$ | $\displaystyle\leftarrow d\bm{\theta}^{\pi}+\nabla_{\theta^{\pi}}\log\pi(\mathbf{a}_{t}&#124;\mathbf{s}_{t};\theta^{\pi})A(\mathbf{s}_{t},\mathbf{a}_{t};\theta^{\pi},\theta^{V}),$
    |  | (35) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle d\bm{\theta}^{\pi}$ | $\displaystyle\leftarrow d\bm{\theta}^{\pi}+\nabla_{\theta^{\pi}}\log\pi(\mathbf{a}_{t}&#124;\mathbf{s}_{t};\theta^{\pi})A(\mathbf{s}_{t},\mathbf{a}_{t};\theta^{\pi},\theta^{V}),$
    |  | (35) |'
- en: '|  | $\displaystyle d\bm{\theta}^{V}$ | $\displaystyle\leftarrow d\bm{\theta}^{V}+\partial
    A(\mathbf{s}_{t},\mathbf{a}_{t};\theta^{\pi},\theta^{V})^{2}/\partial\theta^{V}.$
    |  | (36) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle d\bm{\theta}^{V}$ | $\displaystyle\leftarrow d\bm{\theta}^{V}+\partial
    A(\mathbf{s}_{t},\mathbf{a}_{t};\theta^{\pi},\theta^{V})^{2}/\partial\theta^{V}.$
    |  | (36) |'
- en: The parallelization greatly stabilizes the update of the parameters as the samples
    collected by different actor-learners at the same time are much less correlated,
    which eliminates the requirement for keeping a replay memory. Also by running
    different exploration policies in different threads, the learners are very likely
    to explore different parts of the state space. Due to it being highly efficient,
    lightweight and conceptually simple, A3C is considered as a standard starting
    point in recent DRL research.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化大大稳定了参数的更新，因为不同演员-学习者在同一时间收集的样本相关性要小得多，这消除了保持回放记忆的需求。同时，通过在不同线程中运行不同的探索策略，学习者很可能会探索状态空间的不同部分。由于其高效、轻量且概念简单，A3C
    被认为是近期深度强化学习研究中的标准起点。
- en: II-C5 A2C (Wang et al., [2016a](#bib.bib109); Wu et al., [2017](#bib.bib114))
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C5 A2C（Wang 等，[2016a](#bib.bib109)；Wu 等，[2017](#bib.bib114)）
- en: ': Some recent works found that the asynchrony in A3C does not necessarily lead
    to improved performance compared to the synchronous version: A2C. Different from
    A3C, A2C waits for each actor to finish its segment of experience before performing
    an update, which is averaged over all actors. This detail allows for effective
    GPU implementation.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ': 一些近期的研究发现，相比于同步版本 A2C，A3C 的异步性不一定能带来性能提升。与 A3C 不同，A2C 等待每个演员完成其经验段后才进行更新，这一更新是所有演员的平均值。这个细节使得
    GPU 实现变得有效。'
- en: II-C6 GPS (Levine and Koltun, [2013](#bib.bib57))
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C6 GPS（Levine 和 Koltun，[2013](#bib.bib57)）
- en: ': As a model-based policy search algorithm, Guided Policy Search (GPS) is relatively
    sample efficient. GPS starts from guiding samples generated from some initial
    optimal control policies and augmented from samples generated from the current
    policy, from which at every iteration a set of training trajectories are sampled
    to optimize the current policy with supervised learning. The updated policy is
    then added as an additional cost term to bound the change in the policy, with
    which the trajectory optimization is performed again (e.g., with an LQR solver).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ': 作为一种基于模型的策略搜索算法，指导策略搜索（GPS）具有相对较高的样本效率。GPS 从一些初始最优控制策略生成的引导样本开始，并从当前策略生成的样本中进行增强，每次迭代时都会采样一组训练轨迹来使用监督学习优化当前策略。更新后的策略被添加为额外的成本项，以限制策略的变化，并重新进行轨迹优化（例如，使用
    LQR 求解器）。'
- en: II-C7 TRPO (Schulman et al., [2015a](#bib.bib87))
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C7 TRPO (Schulman 等人，[2015a](#bib.bib87))
- en: ': By making several approximations to the theoretically justified scheme, Schulman
    et al. ([2015a](#bib.bib87)) proposed a practical algorithm for optimizing large
    nonlinear policies, with guaranteed monotonic improvement.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ': 通过对理论上有依据的方案进行几次近似，Schulman 等人 ([2015a](#bib.bib87)) 提出了一个优化大型非线性策略的实际算法，保证了单调改进。'
- en: 'To illustrate the algorithm, let us first define the expected discounted cost
    for an infinite horizon MDP, which replaces the reward function $R$ in the expected
    discounted return with the cost function $c$:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明算法，我们首先定义了无限期 MDP 的期望折扣成本，它用成本函数 $c$ 替换了期望折扣回报中的奖励函数 $R$：
- en: '|  | $\displaystyle\eta(\pi)$ | $\displaystyle=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}c(\mathbf{s}_{t})&#124;\mathbf{s}_{0}\sim\rho_{0}\right].$
    |  | (37) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\eta(\pi)$ | $\displaystyle=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}c(\mathbf{s}_{t})\mid\mathbf{s}_{0}\sim\rho_{0}\right].$
    |  | (37) |'
- en: 'In turn, we can rewrite the definitions for the state-value functions Eq. [4](#S2.E4
    "In 1st item ‣ 2nd item ‣ II-A RL Overview ‣ II Deep reinforcement learning ‣
    A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation") and action-value functions Eq. [6](#S2.E6 "In 2nd item ‣ 2nd item
    ‣ II-A RL Overview ‣ II Deep reinforcement learning ‣ A Survey of Deep Network
    Solutions for Learning Control in Robotics: From Reinforcement to Imitation")
    in terms of the cost function $c$:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，我们可以用成本函数 $c$ 重写状态值函数 Eq. [4](#S2.E4 "在第 1 项 ‣ 第 2 项 ‣ II-A 强化学习概述 ‣ II
    深度强化学习 ‣ 机器人控制深度网络解决方案调查：从强化学习到模仿学习") 和动作值函数 Eq. [6](#S2.E6 "在第 2 项 ‣ 第 2 项 ‣
    II-A 强化学习概述 ‣ II 深度强化学习 ‣ 机器人控制深度网络解决方案调查：从强化学习到模仿学习") 的定义：
- en: '|  | $\displaystyle V^{\pi}(\mathbf{s})$ | $\displaystyle=\mathbb{E}_{\pi}\left[\sum_{k=t}^{\infty}\gamma^{k-t}c(\mathbf{s}_{k})&#124;\mathbf{s}_{t}=\mathbf{s}\right],$
    |  | (38) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle V^{\pi}(\mathbf{s})$ | $\displaystyle=\mathbb{E}_{\pi}\left[\sum_{k=t}^{\infty}\gamma^{k-t}c(\mathbf{s}_{k})\mid\mathbf{s}_{t}=\mathbf{s}\right],$
    |  | (38) |'
- en: '|  | $\displaystyle Q^{\pi}(\mathbf{s},\mathbf{a})$ | $\displaystyle=\mathbb{E}_{\pi}\left[\sum_{k=t}^{\infty}\gamma^{k-t}c(\mathbf{s}_{k})&#124;\mathbf{s}_{t}=\mathbf{s},\mathbf{a}_{t}=\mathbf{a}\right],$
    |  | (39) |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{\pi}(\mathbf{s},\mathbf{a})$ | $\displaystyle=\mathbb{E}_{\pi}\left[\sum_{k=t}^{\infty}\gamma^{k-t}c(\mathbf{s}_{k})\mid\mathbf{s}_{t}=\mathbf{s},\mathbf{a}_{t}=\mathbf{a}\right],$
    |  | (39) |'
- en: 'and in turn, we have the advantage function:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，我们得到了优势函数：
- en: '|  | $\displaystyle A^{\pi}(\mathbf{s},\mathbf{a})$ | $\displaystyle=Q^{\pi}(\mathbf{s},\mathbf{a})-V^{\pi}(\mathbf{s}).$
    |  | (40) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A^{\pi}(\mathbf{s},\mathbf{a})$ | $\displaystyle=Q^{\pi}(\mathbf{s},\mathbf{a})-V^{\pi}(\mathbf{s}).$
    |  | (40) |'
- en: 'Since we are looking for a step size for the policy update that can guarantee
    a monotonic improvement from an old policy $\pi_{\text{old}}$ to an updated policy
    $\pi$, it is beneficial to write the expected cost of $\pi$ in terms of that of
    $\pi_{\text{old}}$, which leads to the following identity:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在寻找一种可以保证从旧策略 $\pi_{\text{old}}$ 到更新策略 $\pi$ 的单调改进的步长，因此用 $\pi_{\text{old}}$
    的期望成本来表示 $\pi$ 的期望成本是有益的，这会导致以下恒等式：
- en: '|  | $\displaystyle\eta(\pi)$ | $\displaystyle=\eta(\pi_{\text{old}})+\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}A^{\pi_{\text{old}}}(\mathbf{s}_{t},\mathbf{a}_{t})&#124;\mathbf{s}_{0}\sim\rho_{0}\right].$
    |  | (41) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\eta(\pi)$ | $\displaystyle=\eta(\pi_{\text{old}})+\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}A^{\pi_{\text{old}}}(\mathbf{s}_{t},\mathbf{a}_{t})\mid\mathbf{s}_{0}\sim\rho_{0}\right].$
    |  | (41) |'
- en: Before continueing, we denote the (unnormalized) discounted visitation frequencies
    for state $\mathbf{s}$ under policy $\pi$ as $\rho^{\pi}(\mathbf{s})$, more formally,
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们将策略 $\pi$ 下的状态 $\mathbf{s}$ 的（未归一化）折扣访问频率记作 $\rho^{\pi}(\mathbf{s})$，更正式地，
- en: '|  | $\displaystyle\rho^{\pi}(\mathbf{s})$ | $\displaystyle=\left(P(\mathbf{s}_{0}=\mathbf{s})+\gamma
    P(\mathbf{s}_{1}=\mathbf{s})+\gamma^{2}P(\mathbf{s}_{2}=\mathbf{s})+\cdots\right)$
    |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\rho^{\pi}(\mathbf{s})$ | $\displaystyle=\left(P(\mathbf{s}_{0}=\mathbf{s})+\gamma
    P(\mathbf{s}_{1}=\mathbf{s})+\gamma^{2}P(\mathbf{s}_{2}=\mathbf{s})+\cdots\right)$
    |  |'
- en: '|  |  | $\displaystyle=\sum_{t=0}^{\infty}\gamma^{t}P(\mathbf{s}_{t}=\mathbf{s}),$
    |  | (42) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{t=0}^{\infty}\gamma^{t}P(\mathbf{s}_{t}=\mathbf{s}),$
    |  | (42) |'
- en: where $\mathbf{s}_{0}\sim\rho_{0}$, and the actions are selected following $\pi$.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{s}_{0}\sim\rho_{0}$，动作根据 $\pi$ 选择。
- en: 'Now, instead of summing over timesteps, if we sum over states, Eq. [41](#S2.E41
    "In II-C7 TRPO (Schulman et al., 2015a) ‣ II-C DRL Algorithms ‣ II Deep reinforcement
    learning ‣ A Survey of Deep Network Solutions for Learning Control in Robotics:
    From Reinforcement to Imitation") can be rewritten as'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们不再按时间步长求和，而是按状态求和，方程 [41](#S2.E41 "在 II-C7 TRPO (Schulman et al., 2015a)
    ‣ II-C DRL 算法 ‣ II 深度强化学习 ‣ 从强化学习到模仿的深度网络解决方案调查") 可以重写为
- en: '|  | $\displaystyle\eta(\pi)$ | $\displaystyle=\eta(\pi_{\text{old}})+\sum_{\mathbf{s}\sim\rho^{\pi}}\rho^{\pi}(\mathbf{s})\sum_{\mathbf{a}\sim\pi}\pi(\mathbf{a}&#124;\mathbf{s})A^{\pi_{\text{old}}}(\mathbf{s},\mathbf{a}).$
    |  | (43) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\eta(\pi)$ | $\displaystyle=\eta(\pi_{\text{old}})+\sum_{\mathbf{s}\sim\rho^{\pi}}\rho^{\pi}(\mathbf{s})\sum_{\mathbf{a}\sim\pi}\pi(\mathbf{a}&#124;\mathbf{s})A^{\pi_{\text{old}}}(\mathbf{s},\mathbf{a}).$
    |  | (43) |'
- en: 'This equation indicates that $\eta$ is guaranteed to decrease or stay constant
    if the expected advantage at every state has a non-positive value. Since Eq. [43](#S2.E43
    "In II-C7 TRPO (Schulman et al., 2015a) ‣ II-C DRL Algorithms ‣ II Deep reinforcement
    learning ‣ A Survey of Deep Network Solutions for Learning Control in Robotics:
    From Reinforcement to Imitation") is difficult to directly optimize, due to the
    complex dependency of $\rho^{\pi}$ on $\pi$, a local approximation ignoring changes
    in the state visitation density induced by the changes in the policy, that matches
    $\eta$ to the first order is introduced (the term $\eta(\pi)$ is left out here
    as it does not affect the solution to the underlying optimization problem):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程表明，如果每个状态的期望优势值为非正值，则 $\eta$ 保证会减少或保持不变。由于方程 [43](#S2.E43 "在 II-C7 TRPO (Schulman
    et al., 2015a) ‣ II-C DRL 算法 ‣ II 深度强化学习 ‣ 从强化学习到模仿的深度网络解决方案调查") 难以直接优化，鉴于 $\rho^{\pi}$
    对 $\pi$ 的复杂依赖关系，介绍了一个忽略政策变化引起的状态访问密度变化的局部近似，这个近似与 $\eta$ 的一阶匹配（这里省略了 $\eta(\pi)$
    因为它不影响底层优化问题的解决）：
- en: '|  | $\displaystyle L_{\pi_{\text{old}}}{(\pi)}=\sum_{\mathbf{s}\sim\rho^{\pi_{\text{old}}}}\rho^{\pi_{\text{old}}}(\mathbf{s})\sum_{\mathbf{a}\sim\pi}\pi(\mathbf{a}&#124;\mathbf{s})A^{\pi_{\text{old}}}(\mathbf{s},\mathbf{a}).$
    |  | (44) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{\pi_{\text{old}}}{(\pi)}=\sum_{\mathbf{s}\sim\rho^{\pi_{\text{old}}}}\rho^{\pi_{\text{old}}}(\mathbf{s})\sum_{\mathbf{a}\sim\pi}\pi(\mathbf{a}&#124;\mathbf{s})A^{\pi_{\text{old}}}(\mathbf{s},\mathbf{a}).$
    |  | (44) |'
- en: Standard policy gradient methods ascend on the $\nth{1}$ order gradient, where
    an increase on $L_{\theta_{\text{old}}}{(\theta)}$ does not guarantee an increase
    in $\eta({\pi_{\theta}})$ with large step sizes, due to the approximations made
    above.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 标准策略梯度方法对 $\nth{1}$ 阶梯度进行上升，其中，$L_{\theta_{\text{old}}}{(\theta)}$ 的增加不能保证在大步长下
    $\eta({\pi_{\theta}})$ 的增加，这是由于上述近似。
- en: TRPO extends the policy improvement bound in the mixture policies setting given
    by Kakade and Langford ([2002](#bib.bib45)) to general stochastic policies, and
    shows that
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO 将 Kakade 和 Langford ([2002](#bib.bib45)) 给出的混合策略设置中的策略改进界限扩展到一般随机策略，并表明
- en: '|  | $\displaystyle\eta(\pi)$ | $\displaystyle\leq L_{\pi_{\text{old}}}(\pi)+CD_{\text{KL}}^{\max}(\pi_{\text{old}},\pi),$
    |  | (45) |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\eta(\pi)$ | $\displaystyle\leq L_{\pi_{\text{old}}}(\pi)+CD_{\text{KL}}^{\max}(\pi_{\text{old}},\pi),$
    |  | (45) |'
- en: where
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $\displaystyle C$ | $\displaystyle=\frac{2\epsilon\gamma}{(1-\gamma)^{2}},$
    |  | (47) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle C$ | $\displaystyle=\frac{2\epsilon\gamma}{(1-\gamma)^{2}},$
    |  | (47) |'
- en: '|  | $\displaystyle\epsilon$ | $\displaystyle=\max_{\mathbf{s}}\left&#124;\mathbb{E}_{\mathbf{a}\sim\pi}\left[A^{\pi_{\text{old}}}(\mathbf{s},\mathbf{a})\right]\right&#124;.$
    |  | (48) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\epsilon$ | $\displaystyle=\max_{\mathbf{s}}\left&#124;\mathbb{E}_{\mathbf{a}\sim\pi}\left[A^{\pi_{\text{old}}}(\mathbf{s},\mathbf{a})\right]\right&#124;.$
    |  | (48) |'
- en: 'This means that by performing the following optimization (here we denote $L_{\theta_{\text{old}}}(\theta)\coloneqq
    L_{\pi_{\theta_{\text{old}}}}(\pi_{\theta})$) with parameterized policies), we
    are guaranteed to improve the true objective $\eta$:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着通过执行以下优化（在这里我们用 $L_{\theta_{\text{old}}}(\theta)\coloneqq L_{\pi_{\theta_{\text{old}}}}(\pi_{\theta})$
    来表示参数化策略），我们可以保证改进真实目标 $\eta$：
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}\left[L_{\theta_{\text{old}}}(\theta)+CD_{\text{KL}}^{\max}(\pi_{\theta_{\text{old}}},\pi_{\theta})\right].$
    |  | (49) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}\left[L_{\theta_{\text{old}}}(\theta)+CD_{\text{KL}}^{\max}(\pi_{\theta_{\text{old}}},\pi_{\theta})\right].$
    |  | (49) |'
- en: 'However, if the penalty coefficient $C$, as calculated in Eq. [47](#S2.E47
    "In II-C7 TRPO (Schulman et al., 2015a) ‣ II-C DRL Algorithms ‣ II Deep reinforcement
    learning ‣ A Survey of Deep Network Solutions for Learning Control in Robotics:
    From Reinforcement to Imitation"), is used in practice, the step sizes will be
    very small.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果在实际中使用在公式 [47](#S2.E47 "在 II-C7 TRPO (Schulman 等，2015a) ‣ II-C DRL 算法 ‣
    II 深度强化学习 ‣ 深度网络解决方案概述：从强化学习到模仿学习") 中计算的惩罚系数 $C$，步长将非常小。
- en: 'To deal with this, TRPO first replaces the sum over actions in Eq. [44](#S2.E44
    "In II-C7 TRPO (Schulman et al., 2015a) ‣ II-C DRL Algorithms ‣ II Deep reinforcement
    learning ‣ A Survey of Deep Network Solutions for Learning Control in Robotics:
    From Reinforcement to Imitation") by an importance sampling estimator (here, we
    only discuss the case for single path sampling, where $\pi_{\text{old}}$ is used
    to generate trajectories) ($A^{\pi_{\theta_{\text{old}}}}$ is replaced by $Q^{\pi_{\theta_{\text{old}}}}$
    which only changes the objective by a constant, and the $Q$-values are to be replaced
    by empirical estimates from sample averages, either single path or vine):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这个问题，TRPO 首先用重要性采样估计器替代了公式 [44](#S2.E44 "在 II-C7 TRPO (Schulman 等，2015a)
    ‣ II-C DRL 算法 ‣ II 深度强化学习 ‣ 深度网络解决方案概述：从强化学习到模仿学习") 中的动作求和（在这里，我们仅讨论单路径采样的情况，其中使用
    $\pi_{\text{old}}$ 来生成轨迹），$A^{\pi_{\theta_{\text{old}}}}$ 被替换为 $Q^{\pi_{\theta_{\text{old}}}}$，这仅通过一个常数改变了目标，$Q$-值将被从样本平均中得出的经验估计替代，无论是单路径还是竹藤路径：
- en: '|  | $\displaystyle L_{\theta_{\text{old}}}{(\theta)}={\mathbb{E}}_{\mathbf{s}\sim\rho^{\pi_{\theta{\text{old}}}},\mathbf{a}\sim\pi_{\theta{\text{old}}}}\left[\frac{\pi_{\theta}(\mathbf{a}&#124;\mathbf{s})}{\pi_{\theta{\text{old}}}(\mathbf{a}&#124;\mathbf{s})}A^{\pi_{\theta{\text{old}}}}(\mathbf{s},\mathbf{a})\right].$
    |  | (50) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{\theta_{\text{old}}}{(\theta)}={\mathbb{E}}_{\mathbf{s}\sim\rho^{\pi_{\theta{\text{old}}}},\mathbf{a}\sim\pi_{\theta{\text{old}}}}\left[\frac{\pi_{\theta}(\mathbf{a}|\mathbf{s})}{\pi_{\theta{\text{old}}}(\mathbf{a}|\mathbf{s})}A^{\pi_{\theta{\text{old}}}}(\mathbf{s},\mathbf{a})\right].$
    |  | (50) |'
- en: 'Then it turns the soft constraint in Eq. [49](#S2.E49 "In II-C7 TRPO (Schulman
    et al., 2015a) ‣ II-C DRL Algorithms ‣ II Deep reinforcement learning ‣ A Survey
    of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation") into the following hard constraint problem:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它将公式 [49](#S2.E49 "在 II-C7 TRPO (Schulman 等，2015a) ‣ II-C DRL 算法 ‣ II 深度强化学习
    ‣ 深度网络解决方案概述：从强化学习到模仿学习") 中的软约束转变为以下硬约束问题：
- en: '|  | $\displaystyle\underset{\theta}{\text{minimize}}$ | $\displaystyle{\mathbb{E}}_{\mathbf{s}\sim\rho^{\pi_{\theta{\text{old}}}},\mathbf{a}\sim\pi_{\theta{\text{old}}}}\left[\frac{\pi_{\theta}(\mathbf{a}&#124;\mathbf{s})}{\pi_{\theta{\text{old}}}(\mathbf{a}&#124;\mathbf{s})}Q^{\pi_{\theta{\text{old}}}}(\mathbf{s},\mathbf{a})\right],$
    |  | (51) |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\theta}{\text{minimize}}$ | $\displaystyle{\mathbb{E}}_{\mathbf{s}\sim\rho^{\pi_{\theta{\text{old}}}},\mathbf{a}\sim\pi_{\theta{\text{old}}}}\left[\frac{\pi_{\theta}(\mathbf{a}|\mathbf{s})}{\pi_{\theta{\text{old}}}(\mathbf{a}|\mathbf{s})}Q^{\pi_{\theta{\text{old}}}}(\mathbf{s},\mathbf{a})\right],$
    |  | (51) |'
- en: '|  | subject to | $\displaystyle{\mathbb{E}}_{\mathbf{s}\sim\rho^{\pi_{\theta{\text{old}}}}}\left[D_{\text{KL}}(\pi_{\theta_{\text{old}}}(\cdot&#124;\mathbf{s})&#124;&#124;\pi_{\theta}(\cdot&#124;\mathbf{s}))\right]\leq\delta.$
    |  | (52) |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | subject to | $\displaystyle{\mathbb{E}}_{\mathbf{s}\sim\rho^{\pi_{\theta{\text{old}}}}}\left[D_{\text{KL}}(\pi_{\theta_{\text{old}}}(\cdot|\mathbf{s})||\pi_{\theta}(\cdot|\mathbf{s}))\right]\leq\delta.$
    |  | (52) |'
- en: where $\delta$ is a hyper parameter for the upper bound of the KL divergence
    between the old and the updated policy (e.g., $\delta=0.01$). This constrained
    optimization problem is solved using a conjugate gradient followed by a line search;
    we refer readers to Schulman et al. ([2015a](#bib.bib87)) for a detailed description.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\delta$ 是用于表示旧策略与更新策略之间 KL 散度的上界的超参数（例如，$\delta=0.01$）。这个约束优化问题使用共轭梯度法和线搜索方法来解决；我们建议读者参考
    Schulman 等人（[2015a](#bib.bib87)）以获取详细描述。
- en: II-C8 PPO (Schulman et al., [2017](#bib.bib89))
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C8 PPO (Schulman et al., [2017](#bib.bib89))
- en: ': Instead of reformulating a hard constraint problem as in TRPO (Eq. [51](#S2.E51
    "In II-C7 TRPO (Schulman et al., 2015a) ‣ II-C DRL Algorithms ‣ II Deep reinforcement
    learning ‣ A Survey of Deep Network Solutions for Learning Control in Robotics:
    From Reinforcement to Imitation") and [52](#S2.E52 "In II-C7 TRPO (Schulman et
    al., 2015a) ‣ II-C DRL Algorithms ‣ II Deep reinforcement learning ‣ A Survey
    of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation")), PPO solves the original soft constraint optimization (Eq. [49](#S2.E49
    "In II-C7 TRPO (Schulman et al., 2015a) ‣ II-C DRL Algorithms ‣ II Deep reinforcement
    learning ‣ A Survey of Deep Network Solutions for Learning Control in Robotics:
    From Reinforcement to Imitation")) with \nth1-order SGD, adapting $C$ according
    to the KL divergence. Since it is much simpler implementation-wise compared to
    TRPO and gives a googd performance, PPO has become the default DRL algorithm at
    OpenAI. A distributed version of PPO has also been proposed (Heess et al., [2017](#bib.bib39)).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ': 相比于 TRPO 重新公式化一个硬约束问题 (Eq. [51](#S2.E51 "In II-C7 TRPO (Schulman et al.,
    2015a) ‣ II-C DRL Algorithms ‣ II Deep reinforcement learning ‣ A Survey of Deep
    Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation")
    和 [52](#S2.E52 "In II-C7 TRPO (Schulman et al., 2015a) ‣ II-C DRL Algorithms ‣
    II Deep reinforcement learning ‣ A Survey of Deep Network Solutions for Learning
    Control in Robotics: From Reinforcement to Imitation"))，PPO 使用 \nth1-order SGD
    解决原始软约束优化 (Eq. [49](#S2.E49 "In II-C7 TRPO (Schulman et al., 2015a) ‣ II-C DRL
    Algorithms ‣ II Deep reinforcement learning ‣ A Survey of Deep Network Solutions
    for Learning Control in Robotics: From Reinforcement to Imitation"))，根据 KL 散度调整
    $C$。由于与 TRPO 相比实现更简单并且表现良好，PPO 已成为 OpenAI 的默认 DRL 算法。PPO 的分布式版本也已被提出 (Heess et
    al., [2017](#bib.bib39))。'
- en: II-C9 ACKTR (Wu et al., [2017](#bib.bib114))
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C9 ACKTR (Wu et al., [2017](#bib.bib114))
- en: ': The Actor Critic Kronecker-Factored Trust Region (ACKTR) is a scalable trust
    region natural gradient method for a actor-critic, with the Kronecker-factored
    approximation to the curvature. It is more computationally efficient than TRPO,
    and is more sample efficient than those methods taking steps in the gradient direction
    (e.g, A2C) rather than the natural gradient direction.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ': **Actor Critic Kronecker-Factored Trust Region (ACKTR)** 是一种可扩展的信任区域自然梯度方法，用于
    actor-critic 算法，采用 Kronecker 分解的曲率近似。它比 TRPO 更加计算高效，并且比那些在梯度方向（例如 A2C）而非自然梯度方向上采取步伐的方法更为样本高效。'
- en: II-D DRL Mechanisms
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D DRL 机制
- en: Many useful mechanisms have also been proposed that can be added on top of the
    aforementioned DRL algorithms. These mechanisms generally work orthogonally with
    the algorithms, and some can accelerate the DRL training by a large margin. Below
    we list several conceptually simple yet very effective ones.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 许多有用的机制也被提出，可以在上述 DRL 算法之上进行扩展。这些机制通常与算法正交，有些可以大幅度加速 DRL 训练。下面我们列出了一些概念上简单但非常有效的机制。
- en: •
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Auxiliary Tasks (Mirowski et al., [2016](#bib.bib63); Jaderberg et al., [2016](#bib.bib44);
    Levine et al., [2016](#bib.bib56); Yu et al., [2018](#bib.bib119); Riedmiller
    et al., [2018](#bib.bib77)): Uses additional supervised or unsupervised tasks
    (e.g., regressing depth images from color images, detecting loop closures, predicting
    end-effector poses) alongside the main reinforcement learning task, to compensate
    for the sparse supervision signals usually provided to DRL agents.'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 辅助任务 (Mirowski et al., [2016](#bib.bib63); Jaderberg et al., [2016](#bib.bib44);
    Levine et al., [2016](#bib.bib56); Yu et al., [2018](#bib.bib119); Riedmiller
    et al., [2018](#bib.bib77))：在主要强化学习任务旁边使用额外的监督或无监督任务（例如，从彩色图像回归深度图像、检测回环闭合、预测末端执行器姿态），以弥补通常提供给
    DRL 代理的稀疏监督信号。
- en: •
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prioritized Experience Replay (Schaul et al., [2015b](#bib.bib85)): Prioritizes
    memory replay according to td-error; can be added to off-policy methods.'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优先经验回放 (Schaul et al., [2015b](#bib.bib85))：根据 td-error 优先级回放内存；可以添加到 off-policy
    方法中。
- en: •
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hindsight Experience Replay (Andrychowicz et al., [2017](#bib.bib1)): Relabels
    the reward for collected experiences to make better use of failure trajectories,
    and effectively speed up off-policy methods with binary or sparse reward structures.'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 事后经验回放 (Andrychowicz et al., [2017](#bib.bib1))：重新标记收集经验的奖励，以更好地利用失败轨迹，并有效加速具有二元或稀疏奖励结构的
    off-policy 方法。
- en: •
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Curriculum Learning (Bengio et al., [2009](#bib.bib6); Florensa et al., [2017](#bib.bib24);
    Zhang et al., [2017b](#bib.bib121)): Presents the learning agent with progressively
    more complex task settings, such that it can grasp gradually more sophasticated
    skills.'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 课程学习（Bengio 等，[2009](#bib.bib6)；Florensa 等，[2017](#bib.bib24)；Zhang 等，[2017b](#bib.bib121)）：向学习代理呈现逐渐复杂的任务设置，使其能够逐步掌握更复杂的技能。
- en: •
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Curiosity-driven Exploration (Pathak et al., [2017](#bib.bib71)): Augments
    the standard external reward with internal reward measured by intrinsic motivation.'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 好奇心驱动的探索（Pathak 等，[2017](#bib.bib71)）：通过内在动机测量的内部奖励来增强标准的外部奖励。
- en: •
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Asymmetric Self-replay for Exploration (Sukhbaatar et al., [2017](#bib.bib94)):
    Drives exploration through an automatic curricula generated via the interplay
    of two versions of the same agent.'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非对称自回放用于探索（Sukhbaatar 等，[2017](#bib.bib94)）：通过两个版本的同一代理之间的相互作用生成自动课程来驱动探索。
- en: •
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Noise in Parameter Space for Exploration (Fortunato et al., [2018](#bib.bib25);
    Plappert et al., [2018](#bib.bib74)): Pertubates network parameters to aid exploration.'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数空间中的噪声用于探索（Fortunato 等，[2018](#bib.bib25)；Plappert 等，[2018](#bib.bib74)）：扰动网络参数以辅助探索。
- en: II-E DRL for Navigation
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-E 深度强化学习（DRL）用于导航
- en: Autonomous navigation is one of the essential problems and challenges in mobile
    robotics. It can roughly be described as the ability of a robot to plan and follow
    a trajectory through the environment to reach a certain goal location without
    colliding with any obstacles in between. The recent literature has seen a growing
    number of proposed methods that tackle the task of autonomous navigation with
    DRL algorithms. Those works formulate the navigation problem as MDPs or POMDPs
    that first take in the sensor readings (color/depth images, laser scans, etc.)
    as observations and stack or augment them into states, and then search for the
    optimal policy that is capable of guiding the agent to navigate to goal locations
    in a timely and collision-free manner. Below we discuss several representative
    works in this category, that target the field of robotics.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 自主导航是移动机器人领域中的一个重要问题和挑战。它大致可以描述为机器人在环境中规划和跟随一条轨迹以到达某个目标位置的能力，同时在过程中避免与任何障碍物碰撞。最近的文献中出现了越来越多的使用深度强化学习（DRL）算法来解决自主导航任务的方法。这些工作将导航问题建模为马尔可夫决策过程（MDPs）或部分可观测马尔可夫决策过程（POMDPs），首先将传感器读取的数据（颜色/深度图像、激光扫描等）作为观察输入，并将其堆叠或增强为状态，然后搜索能够引导代理及时且无碰撞地导航到目标位置的最佳策略。以下我们讨论了这一领域中的几个代表性工作，它们针对机器人领域。
- en: Zhu et al. ([2017b](#bib.bib125)) input both the first-person view and the image
    of the target object to the A3C model, formulating a target-driven navigation
    problem based on the universal value function approximators (Schaul et al., [2015a](#bib.bib84)).
    The training of their model requires the features output from the pretrained ResNet-$50$
    (He et al., [2016](#bib.bib38)), and is performed in an indoor simulator (Kolve
    et al., [2017](#bib.bib50)) where each new room is regarded as a new scene for
    which several scene-specific layers are added as another output head of the model.
    The success rate for generalizing the navigation policies to new targets one step
    away from the trained targets is $70\%$, and around is $42\%$ for those that are
    two steps away. For navigation tasks with optimal solutions of $17.6$ steps, Zhu
    et al. ([2017b](#bib.bib125)) achieved $210.7$ average trajectory lengths after
    being trained on $100$ million frames with an A3C agent. The trained policy was
    able to navigate a real robot inside an office environment after being fine-tuned
    on images collected from the real scene.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Zhu 等（[2017b](#bib.bib125)）将第一人称视角和目标物体的图像输入到 A3C 模型中，基于通用价值函数逼近器（Schaul 等，[2015a](#bib.bib84)）构建了一个目标驱动的导航问题。他们的模型训练需要预训练
    ResNet-$50$（He 等，[2016](#bib.bib38)）输出的特征，并在一个室内模拟器（Kolve 等，[2017](#bib.bib50)）中进行训练，其中每个新房间被视为一个新的场景，为此模型添加了几个特定于场景的层作为另一输出头。将导航策略泛化到与训练目标相隔一步的新目标的成功率为
    $70\%$，与相隔两步的目标约为 $42\%$。对于最优解决方案为 $17.6$ 步的导航任务，Zhu 等（[2017b](#bib.bib125)）在
    A3C 代理上训练 $100$ 百万帧后，平均轨迹长度达到 $210.7$。经过在实际场景中收集的图像微调后，训练得到的策略能够在办公室环境中导航真实机器人。
- en: 'Zhang et al. ([2017a](#bib.bib120)) work on a deep successor representation
    formulation (Kulkarni et al., [2016](#bib.bib53); Barreto et al., [2017](#bib.bib5))
    for the $Q$-value function (Eqs. [15](#S2.E15 "In II-B1 Value-based Methods ‣
    II-B RL Algorithms ‣ II Deep reinforcement learning ‣ A Survey of Deep Network
    Solutions for Learning Control in Robotics: From Reinforcement to Imitation"),[16](#S2.E16
    "In II-B1 Value-based Methods ‣ II-B RL Algorithms ‣ II Deep reinforcement learning
    ‣ A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation"),[17](#S2.E17 "In II-B1 Value-based Methods ‣ II-B RL Algorithms
    ‣ II Deep reinforcement learning ‣ A Survey of Deep Network Solutions for Learning
    Control in Robotics: From Reinforcement to Imitation")), targeting learning representations
    that are transferrable between related navigation tasks. Following the observation
    that most of the value-based DRL methods, such as DQN, usually learn a black-box
    function approximator for the optimal value functions, which makes how to transfer
    the knowledge gained from one task to a related task unclear, they extend on the
    successor feature representation that decouples the learning of the optimal value
    functions into two parts, learning task-specific reward functions, and learning
    task-specific features, and how those features evolve under the current task dynamics.
    While this representation has been shown to work well on transferring learned
    policies to differently scaled reward functions and changed goals in fixed environments,
    Zhang et al. ([2017a](#bib.bib120)) extend the formulations to cope with transferring
    policies to new environments. Both experiments in a simulated 3D maze with RGB
    inputs and real-world robotic experiments with depth image inputs are presented.
    The trained agents, either pre-trained or transferred, all achieved near-optimal
    performance, validating the ability of the proposed method to transfer DRL navigation
    policies into new environments.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zhang 等人（[2017a](#bib.bib120)）研究了深度后继表示公式（Kulkarni 等人，[2016](#bib.bib53)；Barreto
    等人，[2017](#bib.bib5)），用于 $Q$-值函数（Eqs. [15](#S2.E15 "In II-B1 Value-based Methods
    ‣ II-B RL Algorithms ‣ II Deep reinforcement learning ‣ A Survey of Deep Network
    Solutions for Learning Control in Robotics: From Reinforcement to Imitation"),[16](#S2.E16
    "In II-B1 Value-based Methods ‣ II-B RL Algorithms ‣ II Deep reinforcement learning
    ‣ A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation"),[17](#S2.E17 "In II-B1 Value-based Methods ‣ II-B RL Algorithms
    ‣ II Deep reinforcement learning ‣ A Survey of Deep Network Solutions for Learning
    Control in Robotics: From Reinforcement to Imitation")），旨在学习在相关导航任务之间可转移的表示。根据观察，大多数基于价值的深度强化学习方法，如
    DQN，通常学习一个黑箱函数近似器来获得最优值函数，这使得如何将从一个任务中获得的知识转移到相关任务变得不明确，他们在后继特征表示上进行扩展，将最优值函数的学习分为两部分：学习任务特定的奖励函数和学习任务特定的特征，以及这些特征如何在当前任务动态下演变。虽然这种表示在将学习到的策略转移到不同尺度的奖励函数和变化目标的固定环境中表现良好，但
    Zhang 等人（[2017a](#bib.bib120)）将公式扩展到处理将策略转移到新环境中。在模拟的 3D 迷宫中进行的实验和在真实世界中使用深度图像输入的机器人实验均得到了展示。经过训练的代理，无论是预训练还是转移，均实现了接近最优的性能，验证了所提出方法将
    DRL 导航策略转移到新环境中的能力。'
- en: The two methods mentioned above propose to learn navigation policies without
    a requirement for performing localization or mapping as in the traditional planning
    pipelines in robotics. They deal with navigating to different targets either by
    feeding the target image as input (Zhu et al., [2017b](#bib.bib125)) or by treating
    it as a transfer problem (Zhang et al., [2017a](#bib.bib120)). Tai et al. ([2017](#bib.bib99)),
    in contrast, propose a learning-based mapless motion planner, under the assumption
    that the relative position of the target w.r.t. the robot can be obtained via
    cheap solutions such as wifi or visible light localization, which are applicable
    to indoor robotic systems such as vacuum robots. The inputs for the model is 10-dimensional
    laser ranges, and the network outputs continuous steering commands after being
    trained via an asynchronous version of the DDPG. Since the simulated laser ranges
    and the real laser readings are quite similar, the trained model is directly generalizable
    to indoor office environments.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两种方法提出了在不需要像传统机器人规划管道中那样进行定位或地图绘制的情况下学习导航策略。他们通过将目标图像作为输入 (Zhu 等人，[2017b](#bib.bib125))
    或将其视为迁移问题 (Zhang 等人，[2017a](#bib.bib120)) 来处理不同目标的导航。相比之下，Tai 等人 ([2017](#bib.bib99))
    提出了一个基于学习的无地图运动规划器，假设可以通过便宜的解决方案（如 wifi 或可见光定位）获得目标相对于机器人的相对位置，这些方案适用于室内机器人系统（如吸尘机器人）。模型的输入是
    10 维激光范围，经过异步版本的 DDPG 训练后，网络输出连续的转向指令。由于模拟激光范围和真实激光读数非常相似，训练后的模型可以直接推广到室内办公环境。
- en: Mirowski et al. ([2016](#bib.bib63)) greatly improve the data efficiency and
    task performance of their variant of an A3C agent when learning to navigate in
    simulated 3D mazes, by using additional supervision signals from auxiliary tasks.
    In particular, the learning agent is additionally supervised by losses from depth
    prediction and loop closure classification. Extensive experiments are presented,
    validating the ability of the proposed agent to localize and to navigate between
    frequently changing start and goal locations.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Mirowski 等人 ([2016](#bib.bib63)) 通过使用来自辅助任务的额外监督信号，显著提高了他们的 A3C 变体在模拟 3D 迷宫中学习导航的数据显示效率和任务性能。特别是，学习代理还受到深度预测和回环闭合分类损失的监督。进行了广泛的实验，验证了所提出的代理在定位和在频繁变化的起始和目标位置之间导航的能力。
- en: The aforementioned methods all deal with navigation in static environments.
    Chen et al. ([2017a](#bib.bib12)) propose a DRL based systematic solution for
    socially aware navigation in dynamic environments with pedestrians. They extend
    a prior work (Chen et al., [2017b](#bib.bib13)), and build a robotic system for
    their real-world experiment, where a differential-drive mobile robot is mounted
    with a Lidar for localization and three Intel Realsense’s for obstacle avoidance.
    From the sensor readings, the speed, velocity and radius of pedestrians are estimated,
    from which the reward (designed based on social norms) is calculated. Read robotic
    experiments show that the proposed method is capable of navigating agents at human
    walking speed in a dynamic environment with many pedestrians.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法都处理静态环境中的导航问题。Chen 等人 ([2017a](#bib.bib12)) 提出了一个基于 DRL 的系统解决方案，用于在包含行人的动态环境中进行社会感知导航。他们扩展了之前的工作
    (Chen 等人，[2017b](#bib.bib13))，并建立了一个用于现实世界实验的机器人系统，其中一个差分驱动的移动机器人配备了用于定位的 Lidar
    和三个 Intel Realsense 用于避障。从传感器读数中，估计行人的速度、速度和半径，并据此计算奖励（基于社会规范设计）。实际机器人实验表明，该方法能够在行人众多的动态环境中以人类步速导航代理。
- en: Long et al. ([2017](#bib.bib61)) deal with decentralized multi-agent collision
    avoidance with PPO. They supervised the agents with a well-shaped reward function,
    and test the algorithm under extensive simulated scenarios.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Long 等人 ([2017](#bib.bib61)) 使用 PPO 处理去中心化多智能体碰撞避免。他们使用了良好设计的奖励函数来监督代理，并在广泛的模拟场景下测试了算法。
- en: There is also a growing trend in the recent literature to incorporate traditional
    Simultaneous Localization and Mapping (SLAM) (Thrun et al., [2005](#bib.bib102))
    procedures, either partially or fully and embedded internally or externally, into
    DRL network architectures, with the intention to cultivate more sophisticated
    navigation capabilities in DRL agents (Stachenfeld et al., [2017](#bib.bib92);
    Kanitscheider and Fiete, [2017](#bib.bib47)). Below we review the most representative
    robotics works in this promising direction.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 近期文献中还出现了将传统的同时定位与地图构建（SLAM）（Thrun 等人，[2005](#bib.bib102)）程序部分或完全嵌入到 DRL 网络架构中的趋势，目的是培养
    DRL 代理更复杂的导航能力（Stachenfeld 等人，[2017](#bib.bib92)；Kanitscheider 和 Fiete，[2017](#bib.bib47)）。以下我们回顾了在这一有前景方向上最具代表性的机器人研究成果。
- en: 'Gupta et al. ([2017a](#bib.bib36)) train a Cognitive Mapping and Planning (CMP)
    model with DAGGer, which is an imitation learning algorithm that we will talk
    about in Sec. [III-A](#S3.SS1 "III-A Behavior Cloning ‣ III Imitation Learning
    ‣ A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation"). Although it dose not use DRL for training the navigation policies,
    we feel it fits best into this part of the discussion. CMP takes in the first-person
    view RGB images and applies egomotion to an internal mapper module, to encourage
    an egocentric multi-scale map representation to emerge out of the training process.
    Planning is done on this egocentric map utilizing Value Iteration Networks (VIN)
    (Tamar et al., [2016](#bib.bib101)). Also trained with DAGGer, Gupta et al. ([2017b](#bib.bib37))
    unify map-based spatial reasoning and path planning. Given the images and poses
    of several reference points and the starting point, as well as the pose for the
    goal, their proposed method is able to navigate toward the agent the desired goal
    location.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Gupta 等人 ([2017a](#bib.bib36)) 使用 DAGGer 训练了一个认知映射和规划（CMP）模型，DAGGer 是一种模仿学习算法，我们将在第
    [III-A](#S3.SS1 "III-A 行为克隆 ‣ III 模仿学习 ‣ 深度网络解决方案在机器人控制学习中的调查：从强化学习到模仿学习") 节中讨论。尽管它没有使用
    DRL 训练导航策略，但我们认为它最适合讨论的这一部分。CMP 采用第一人称视角的 RGB 图像，并将自我运动应用于内部映射模块，以鼓励在训练过程中生成以自我为中心的多尺度地图表示。规划是在这个自我中心地图上利用价值迭代网络（VIN）（Tamar
    等人，[2016](#bib.bib101)）完成的。Gupta 等人 ([2017b](#bib.bib37)) 同样使用 DAGGer 统一了基于地图的空间推理和路径规划。给定多个参考点和起点的图像和姿态，以及目标的姿态，他们提出的方法能够引导代理朝着期望的目标位置导航。
- en: Zhang et al. ([2017b](#bib.bib121)) proposed Neural SLAM, based on the Neural
    Map proposed by Parisotto and Salakhutdinov ([2017](#bib.bib70)), where the Neural
    Turing Machine (NTM) is deployed for the DRL agent to interact with. More specifically,
    Neural SLAM embeds the motion prediction step and the measurement update step
    of traditional SLAM into the network architecture, biasing the write/read operations
    in the NTM towards SLAM operations, and treats the external memory as an internal
    representation of the environment for the learning agent. The whole architecture
    is then trained via A3C, and experiments show that the embedded structures are
    able to encourage the evolution of cognitive mapping capabilities of the agent,
    during the course of its exploration through the environment.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等人 ([2017b](#bib.bib121)) 提出了基于 Parisotto 和 Salakhutdinov ([2017](#bib.bib70))
    提出的神经地图的神经 SLAM，其中神经图灵机（NTM）被部署以供 DRL 代理交互。更具体地说，神经 SLAM 将传统 SLAM 的运动预测步骤和测量更新步骤嵌入到网络架构中，将
    NTM 的写/读操作偏向 SLAM 操作，并将外部记忆视为学习代理的环境内部表示。整个架构通过 A3C 进行训练，实验表明，嵌入的结构能够在代理探索环境的过程中促进认知映射能力的演变。
- en: 'Khan et al. ([2017](#bib.bib48)) design a network architecture that contains
    three components: a convolution network for feature extraction, a planner module
    to pre-plan in the environment, and a controller module that learns to selectively
    store past information that could be useful for planning.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Khan 等人 ([2017](#bib.bib48)) 设计了一种包含三个组件的网络架构：用于特征提取的卷积网络、用于环境预规划的规划模块和一个控制模块，该模块学习选择性地存储可能对规划有用的过去信息。
- en: Bruce et al. ([2017](#bib.bib9)) propose a method that enables zero-shot transfer
    for a mobile robot to learn to navigate to a fixed goal in an environment with
    variations unseen during the training. They introduce interactive replay, in which
    a rough world model is built from a single traversal of the environment. The agent
    is then able to interact with this world model to generate a large number of diverse
    trajectories, which can substantially reduce the number of real experiences needed.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Bruce 等人 ([2017](#bib.bib9)) 提出了一种方法，使得移动机器人能够在训练中未见过的环境变化中学会导航到固定目标。他们引入了交互重放，其中通过对环境的单次遍历构建粗略的世界模型。然后，智能体可以与该世界模型交互，生成大量多样的轨迹，从而显著减少所需的真实经验数量。
- en: Chaplot et al. ([2018](#bib.bib10)) introduce a network structure mimicking
    the Bayesian filtering process with a specially designed perception model. It
    takes as input the agent’s observation, and outputs a likelihood map, inside which
    the belief is propagated through time following the classic filtering process
    used for localization in robotics (thrun2006probabilistic).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Chaplot 等人 ([2018](#bib.bib10)) 介绍了一种模仿贝叶斯滤波过程的网络结构，并设计了一个专门的感知模型。该模型以智能体的观察为输入，输出一个可能性地图，在其中，信念通过时间传播，遵循用于机器人定位的经典滤波过程
    (thrun2006probabilistic)。
- en: Inspired by graph-based SLAM algorithms (Thrun et al., [2005](#bib.bib102);
    Kümmerle et al., [2011](#bib.bib55)), Parisotto et al. ([2018](#bib.bib69)) embed
    the global pose optimization into the network architecture design for their Neural
    Graph Optimizer, which is composed of a local pose estimation model, a pose selection
    module and a graph optimization process. Savinov et al. ([2018](#bib.bib82)) introduce
    a memory architecture, Semi-parametric Topological Memory, for navigation in unseen
    environments. It contains a non-parametric graph with nodes representing locations
    in the environment, and a parametric deep network retrieving nodes from the graph
    based on observations.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 受图基 SLAM 算法 (Thrun 等人，[2005](#bib.bib102)；Kümmerle 等人，[2011](#bib.bib55)) 的启发，Parisotto
    等人 ([2018](#bib.bib69)) 将全局姿态优化嵌入到他们的神经图优化器的网络架构设计中，该优化器由局部姿态估计模型、姿态选择模块和图优化过程组成。Savinov
    等人 ([2018](#bib.bib82)) 引入了一种记忆架构，即半参数化拓扑记忆，用于在未知环境中导航。该架构包含一个非参数化图，其节点表示环境中的位置，并且一个参数化深度网络基于观察从图中检索节点。
- en: II-F DRL for Manipulation
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-F DRL用于操控
- en: In terms of manipulation, the tasks being considered for evaluating DRL algorithms
    are more standardized in the recent literature (Lillicrap et al., [2015](#bib.bib59);
    Schulman et al., [2015a](#bib.bib87); Mnih et al., [2016](#bib.bib64); Heess et al.,
    [2017](#bib.bib39); Wu et al., [2017](#bib.bib114)). Most of such works benchmark
    the proposed algorithms on standard tasks, including reaching, pushing, pick-and-place,
    etc., using the MuJoCo simulator (Todorov et al., [2012](#bib.bib104)). Below
    we focus on the works that are presented with real robotic experiments.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在操控方面，最近的文献中用于评估 DRL 算法的任务更为标准化 (Lillicrap 等人，[2015](#bib.bib59)；Schulman 等人，[2015a](#bib.bib87)；Mnih
    等人，[2016](#bib.bib64)；Heess 等人，[2017](#bib.bib39)；Wu 等人，[2017](#bib.bib114))。大多数此类工作在标准任务上对所提算法进行基准测试，包括达到、推送、抓取和放置等，使用
    MuJoCo 仿真器 (Todorov 等人，[2012](#bib.bib104))。下面我们关注于进行实际机器人实验的工作。
- en: Gu et al. ([2017](#bib.bib32)) propose an asynchronous version of NAF. Taking
    in the low dimensional states as inputs (joint angles, end effector poses, as
    well as their time derivatives, and the pose of the target), in addition to well-shaped
    reward signals, it allows the robot to learn a real-world door opening task in
    about $2.5$ hours in a completely end-to-end manner, achieving a $100\%$ success
    rate.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Gu 等人 ([2017](#bib.bib32)) 提出了 NAF 的异步版本。该方法以低维状态（关节角度、末端执行器姿态以及它们的时间导数和目标姿态）作为输入，并结合良好的奖励信号，使得机器人能够在约
    $2.5$ 小时内以完全端到端的方式学习实际门开启任务，成功率达到 $100\%$。
- en: Levine et al. ([2016](#bib.bib56)) successfully train deep visuomotor policies
    with GPS, a model-based approach. Their proposed visuomotor policy network takes
    as input monocular RGB images and passes them through several convolutional layers
    and a spatial soft argmax layer, which are then concatenated with the robot configurations
    (joint angles, end effector poses). These representations are then passed through
    several fully connected layers and used to predict the corresponding motor torques.
    Various experiments on a PR2 robot (with a $7$-DOF arm) such as hanging a coat
    hanger on a clothes rack, inserting a block into a shape sorting cube, or screwing
    on a bottle cap have demonstrated to validate the effectiveness of the approach.
    This method, however, requires a known and fully observed state space, which could
    limit its potential use cases.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Levine 等人 ([2016](#bib.bib56)) 成功地使用 GPS，一种基于模型的方法，训练了深度视觉运动策略。他们提出的视觉运动策略网络以单目
    RGB 图像为输入，经过几个卷积层和一个空间软最大层，然后与机器人配置（关节角度、末端执行器姿态）进行拼接。这些表示接着通过几个全连接层，并用于预测相应的电机扭矩。对
    PR2 机器人（具有 $7$ 自由度的臂）的各种实验，如将衣架挂在衣架上、将一个块放入形状分类立方体中，或拧上瓶盖，都验证了该方法的有效性。然而，这种方法需要已知且完全观察到的状态空间，这可能限制其潜在的使用场景。
- en: Model-based DRL methods are also utilized by Finn et al. ([2016](#bib.bib22))
    and Tzeng et al. ([2015](#bib.bib105)), learning useful state representations
    for generating successful control policies. Fu et al. ([2016](#bib.bib26)) proposed
    one-shot learning of manipulation skills through model-based reinforcement learning
    by leveraging the neural network priors as a dynamic model. Learning dexterous
    manipulation skills with multi-fingered hands, for which model-based (Kumar et al.,
    [2016](#bib.bib54); Gupta et al., [2016](#bib.bib35)) and model-free (Popov et al.,
    [2017](#bib.bib75)) DRL algorithms have been proposed and demonstrated in real
    robotic experiments, is quite challenging.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的 DRL 方法也被 Finn 等人 ([2016](#bib.bib22)) 和 Tzeng 等人 ([2015](#bib.bib105))
    使用，用于学习有用的状态表示以生成成功的控制策略。Fu 等人 ([2016](#bib.bib26)) 通过利用神经网络先验作为动态模型，提出了通过基于模型的强化学习进行一次性操作技能学习。学习多指手的灵巧操作技能，已经提出并在实际机器人实验中展示了基于模型
    (Kumar 等人，[2016](#bib.bib54)；Gupta 等人，[2016](#bib.bib35)) 和无模型 (Popov 等人，[2017](#bib.bib75))
    的 DRL 算法，这非常具有挑战性。
- en: While many works have carefully designed their reward structure to guide reinforcement
    learning, Riedmiller et al. ([2018](#bib.bib77)) propose a method to speed up
    learning from only binary or sparse rewards, under the observation that well-shaped
    rewards can often bias the learned control policy into potentially suboptimal
    directions. In contrast when only sparse reward signals are provided to the agent,
    the learner can discover novel and potentially preferable solutions. To achieve
    this, alongside the policy learning for the main task, Riedmiller et al. ([2018](#bib.bib77))
    learn policies (which they refer to as intentions) for a set of semantically grounded
    auxiliary tasks, whose supervision signals can be easily obtained by the activation
    of certain sensors. Then a scheduling policy is learned to sequence the intention-policies.
    Their proposed algorithm is able to learn to solve challenging manipulation tasks
    from scratch, such as stacking two blocks into a tower or cleaning up a desk by
    putting objects desk into a box with a lid that can be opened, with a $9$-DOF
    robot arm. Moreover, in their real-world experiments, a single robot arm learns
    a lifting task in about $10$ hours.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多工作已精心设计其奖励结构以指导强化学习，Riedmiller 等人 ([2018](#bib.bib77)) 提出了一个方法，通过仅从二进制或稀疏奖励中加速学习，观察到精心设计的奖励常常可能将学习到的控制策略偏向潜在的次优方向。相比之下，当仅向代理提供稀疏奖励信号时，学习者可以发现新的和潜在的更优解。为了实现这一点，除了主要任务的策略学习外，Riedmiller
    等人 ([2018](#bib.bib77)) 学习了一组语义上明确的辅助任务的策略（他们称之为意图），这些辅助任务的监督信号可以通过某些传感器的激活轻松获得。然后，学习一个调度策略来对意图策略进行排序。他们提出的算法能够从头开始学习解决具有挑战性的操作任务，例如将两个块堆叠成塔或通过将物体放入一个可以打开盖子的箱子里来清理桌子，使用的是
    $9$ 自由度的机器人臂。此外，在他们的实际实验中，单个机器人臂在大约 $10$ 小时内学习了一个提升任务。
- en: 'II-G The Reality Gap: From Simulation to the Real World'
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-G 现实差距：从仿真到现实世界
- en: Although DRL offers a general framework for agents to learn high-dimensional
    control policies, it typically requires several millions of training samples.
    This makes it infeasible to train DRL agents directly in real-world scenarios,
    since real robotic control experiences are relatively expensive to obtain. As
    a consequence, DRL algorithms are generally trained in simulated environments,
    then transferred to the real world and deployed onto real robotic systems. This
    brings about the problem of the reality gap, which refers to the discrepancies
    in lighting conditions, noise patterns, textures, etc., between synthetic renderings
    and real-world sensory readings. The reality gap imposes major challenges for
    generalizing the DRL policies trained in simulation to real scenarios.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度强化学习（DRL）为代理学习高维控制策略提供了一个通用框架，但它通常需要数百万个训练样本。这使得在现实世界场景中直接训练DRL代理变得不可行，因为实际的机器人控制经验相对昂贵。因此，DRL算法通常在模拟环境中进行训练，然后转移到现实世界并部署到实际的机器人系统中。这带来了现实差距问题，即合成渲染与真实世界传感器读数之间的光照条件、噪声模式、纹理等的差异。现实差距对将模拟中训练的DRL策略推广到现实场景提出了重大挑战。
- en: Since the problem of the reality gap is most severe in the visual domain, in
    that the aforementioned discrepancies are most significant between rendered color
    images and real color camera readings, some robotics works have proposed to circumvent
    this problem by using other input modalities whose domain variations are less
    distinct, such as depth images (Zhang et al., [2017a](#bib.bib120)) or laser ranges
    (Tai et al., [2017](#bib.bib99)). However, bridging the reality gap in the visual
    domain is of great importance and remains one of the focuses of recent works.
    Below, we review methods that deal with the reality gap for visual control.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现实差距问题在视觉领域最为严重，因为渲染的彩色图像与真实的彩色相机读取之间的差异最为显著，一些机器人研究提出通过使用领域变化较少的其他输入模态来规避这一问题，例如深度图像（Zhang等，[2017a](#bib.bib120)）或激光测距（Tai等，[2017](#bib.bib99)）。然而，弥合视觉领域的现实差距至关重要，并仍然是近期工作的重点之一。下面，我们回顾了处理视觉控制中现实差距的方法。
- en: II-G1 Domain Adaptation
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-G1 领域适应
- en: In the visual domain, domain adaptation can also be referred to as image-to-image
    translation, which focuses on translating images from a source domain to a target
    domains, It can be considered as the method of choice in the recent literature
    to tackle the reality gap for visual control. The domain confusion loss, as proposed
    by Tzeng et al. ([2014](#bib.bib106)), is another solution that learns a semantically
    meaningful and domain invariant representation. However, minimizing the domain
    confusion loss requires that the data from both the source and the target domain
    are available from the beginning of the whole learning pipeline, which might not
    be as flexible in the robotics context.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉领域，领域适应也可以称为图像到图像的转换，它专注于将图像从源领域转换到目标领域。在近期文献中，这被认为是应对视觉控制中现实差距的首选方法。由Tzeng等人提出的领域混淆损失（[2014](#bib.bib106)）是另一种解决方案，它学习一个语义上有意义且领域不变的表示。然而，最小化领域混淆损失要求从学习管道开始时就有源领域和目标领域的数据，这在机器人技术的背景下可能不够灵活。
- en: In the following, we first formalize the domain adaptation problem, then continue
    to introduce several of the most general methods that require the least human
    intervention and are most directly applicable to robotics control tasks.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 以下，我们首先形式化领域适应问题，然后继续介绍几种需要最少人工干预且最直接适用于机器人控制任务的一般方法。
- en: 'Consider visual data sources from two domains: $\mathbf{X}$ (e.g., synthetic
    images rendered by a simulator; $\mathbf{x}\sim p_{\text{sim}}$, where $p_{\text{sim}}$
    represents the simulated data distribution) and $\mathbf{Y}$ (e.g., real sensory
    readings from the onboard color camera of a mobile robot; $\mathbf{y}\sim p_{\text{real}}$,
    where $p_{\text{real}}$ represents the distribution of the real color image readings).
    As we have just discussed, DRL agents are typically trained in the synthetic domain
    $\mathbf{X}$, then deployed onto real robotic platforms to perform control tasks
    in the real-world domain $\mathbf{Y}$. Domain adaptation methods aim to learn
    a mapping between these two domains.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑来自两个领域的视觉数据源：$\mathbf{X}$（例如，通过模拟器渲染的合成图像；$\mathbf{x}\sim p_{\text{sim}}$，其中
    $p_{\text{sim}}$ 代表模拟数据分布）和 $\mathbf{Y}$（例如，来自移动机器人上的彩色相机的真实传感器读数；$\mathbf{y}\sim
    p_{\text{real}}$，其中 $p_{\text{real}}$ 代表真实彩色图像读数的分布）。正如我们刚刚讨论的那样，DRL 代理通常在合成领域
    $\mathbf{X}$ 中进行训练，然后部署到真实机器人平台上，以在真实世界领域 $\mathbf{Y}$ 中执行控制任务。领域自适应方法旨在学习这两个领域之间的映射。
- en: 'GANs: Most of the domain adaptation works are based on Generative Adversarial
    Networks (GANs) (Goodfellow et al., [2014](#bib.bib31); Radford et al., [2015](#bib.bib76);
    Arjovsky et al., [2017](#bib.bib2)). When learning a GAN model, a generator $G$
    and a discriminator $D$ are trained in an adversarial manner. In the context of
    domain adaptation for visual inputs, the generator $G$ takes images from the source
    domain, and tries to generate output images matching those from the target domain,
    while the discriminator $D$ learns to tell the generated target images and the
    real target images apart.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: GANs：大多数领域自适应工作基于生成对抗网络（GANs）（Goodfellow 等，[2014](#bib.bib31)；Radford 等，[2015](#bib.bib76)；Arjovsky
    等，[2017](#bib.bib2)）。在学习 GAN 模型时，生成器 $G$ 和鉴别器 $D$ 以对抗的方式进行训练。在视觉输入的领域自适应背景下，生成器
    $G$ 接受来自源领域的图像，并尝试生成与目标领域的图像匹配的输出图像，而鉴别器 $D$ 学会区分生成的目标图像和真实的目标图像。
- en: 'CycleGAN (Zhu et al., [2017a](#bib.bib124)): Zhu et al. propose one of the
    most popular unsupervised domain adaptation methods in the recent literature,
    Zhu et al. ([2017a](#bib.bib124)) proposed a simple yet very effective formulation
    that does not require paired data from the two domains of interest. Observing
    that the mapping from the source domain to the target domain, $G_{\mathbf{Y}}:\mathbf{X}\rightarrow\mathbf{Y}$,
    is highly under-constrained, CycleGAN proposes to add a cycle-consistent loss
    to enforce that a reverse mapping from the target domain back to the source domain
    exists: $G_{\mathbf{X}}:\mathbf{Y}\rightarrow\mathbf{X}$. More formally, CycleGAN
    learns two generative models to map between domains $\mathbf{X}$ and $\mathbf{Y}$:
    $G_{\mathbf{Y}}$ with its discriminator $D_{\mathbf{Y}}$, and $G_{\mathbf{X}}$
    with its discriminator $D_{\mathbf{X}}$, by training two GANs simultaneously:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN（Zhu 等，[2017a](#bib.bib124)）：Zhu 等提出了近年来最流行的无监督领域自适应方法之一，Zhu 等（[2017a](#bib.bib124)）提出了一种简单而非常有效的公式，无需来自两个感兴趣领域的配对数据。观察到从源领域到目标领域的映射
    $G_{\mathbf{Y}}:\mathbf{X}\rightarrow\mathbf{Y}$ 非常欠约束，CycleGAN 提出了添加循环一致性损失，以强制存在从目标领域反向映射到源领域的映射：$G_{\mathbf{X}}:\mathbf{Y}\rightarrow\mathbf{X}$。更正式地说，CycleGAN
    通过同时训练两个 GAN，学习在领域 $\mathbf{X}$ 和 $\mathbf{Y}$ 之间的两个生成模型：$G_{\mathbf{Y}}$ 及其鉴别器
    $D_{\mathbf{Y}}$，以及 $G_{\mathbf{X}}$ 及其鉴别器 $D_{\mathbf{X}}$：
- en: '|  | $\displaystyle\mathcal{L}_{\text{GAN}_{\mathbf{Y}}}(G_{\mathbf{Y}},D_{\mathbf{Y}};\mathbf{X},\mathbf{Y})=$
    |  | (53) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{GAN}_{\mathbf{Y}}}(G_{\mathbf{Y}},D_{\mathbf{Y}};\mathbf{X},\mathbf{Y})=$
    |  | (53) |'
- en: '|  | $\displaystyle\hskip 21.68121pt\mathbb{E}_{\mathbf{y}}\left[\log D_{\mathbf{Y}}(\mathbf{y})\right]+\mathbb{E}_{\mathbf{x}}\left[\log(1-D_{\mathbf{Y}}(G_{\mathbf{Y}}(\mathbf{x})))\right],$
    |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hskip 21.68121pt\mathbb{E}_{\mathbf{y}}\left[\log D_{\mathbf{Y}}(\mathbf{y})\right]+\mathbb{E}_{\mathbf{x}}\left[\log(1-D_{\mathbf{Y}}(G_{\mathbf{Y}}(\mathbf{x})))\right],$
    |  |'
- en: '|  | $\displaystyle\mathcal{L}_{\text{GAN}_{\mathbf{X}}}(G_{\mathbf{X}},D_{\mathbf{X}};\mathbf{Y},\mathbf{X})=$
    |  | (54) |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{GAN}_{\mathbf{X}}}(G_{\mathbf{X}},D_{\mathbf{X}};\mathbf{Y},\mathbf{X})=$
    |  | (54) |'
- en: '|  | $\displaystyle\hskip 21.68121pt\mathbb{E}_{\mathbf{x}}\left[\log D_{\mathbf{X}}(\mathbf{x})\right]+\mathbb{E}_{\mathbf{y}}\left[\log(1-D_{\mathbf{X}}(G_{\mathbf{X}}(\mathbf{y})))\right],$
    |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hskip 21.68121pt\mathbb{E}_{\mathbf{x}}\left[\log D_{\mathbf{X}}(\mathbf{x})\right]+\mathbb{E}_{\mathbf{y}}\left[\log(1-D_{\mathbf{X}}(G_{\mathbf{X}}(\mathbf{y})))\right],$
    |  |'
- en: 'on top of which two sets of cycle consistency loss are added to constrain the
    two mappings:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，添加了两组循环一致性损失，以约束这两个映射：
- en: '|  | $\displaystyle\mathcal{L}_{\text{cyc}_{\mathbf{Y}}}(G_{\mathbf{X}},G_{\mathbf{Y}};\mathbf{Y})$
    | $\displaystyle=\mathbb{E}_{\mathbf{y}}\left[\left&#124;\left&#124;G_{\mathbf{Y}}(G_{\mathbf{X}}(\mathbf{y}))-\mathbf{y}\right&#124;\right&#124;_{1}\right],$
    |  | (55) |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{cyc}_{\mathbf{Y}}}(G_{\mathbf{X}},G_{\mathbf{Y}};\mathbf{Y})$
    | $\displaystyle=\mathbb{E}_{\mathbf{y}}\left[\left\|\left\|G_{\mathbf{Y}}(G_{\mathbf{X}}(\mathbf{y}))-\mathbf{y}\right\|\right\|_{1}\right],$
    |  | (55) |'
- en: '|  | $\displaystyle\mathcal{L}_{\text{cyc}_{\mathbf{X}}}(G_{\mathbf{Y}},G_{\mathbf{X}};\mathbf{X})$
    | $\displaystyle=\mathbb{E}_{\mathbf{x}}\left[\left&#124;\left&#124;G_{\mathbf{X}}(G_{\mathbf{Y}}(\mathbf{x}))-\mathbf{x}\right&#124;\right&#124;_{1}\right].$
    |  | (56) |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{cyc}_{\mathbf{X}}}(G_{\mathbf{Y}},G_{\mathbf{X}};\mathbf{X})$
    | $\displaystyle=\mathbb{E}_{\mathbf{x}}\left[\left\|\left\|G_{\mathbf{X}}(G_{\mathbf{Y}}(\mathbf{x}))-\mathbf{x}\right\|\right\|_{1}\right].$
    |  | (56) |'
- en: The full objective of CycleGAN then adds up to ($\lambda$ denotes the weighting
    of the cycle consistency loss)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN 的完整目标则加起来为（$\lambda$ 表示循环一致性损失的权重）
- en: '|  | $\displaystyle\mathcal{L}(G_{\mathbf{Y}}$ | $\displaystyle,G_{\mathbf{X}},D_{\mathbf{Y}},D_{\mathbf{X}};\mathbf{X},\mathbf{Y})=$
    |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}(G_{\mathbf{Y}}$ | $\displaystyle,G_{\mathbf{X}},D_{\mathbf{Y}},D_{\mathbf{X}};\mathbf{X},\mathbf{Y})=$
    |  |'
- en: '|  |  | $\displaystyle\hskip 13.00806pt\mathcal{L}_{\text{GAN}_{\mathbf{Y}}}(G_{\mathbf{Y}},D_{\mathbf{Y}};\mathbf{X},\mathbf{Y})$
    |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\hskip 13.00806pt\mathcal{L}_{\text{GAN}_{\mathbf{Y}}}(G_{\mathbf{Y}},D_{\mathbf{Y}};\mathbf{X},\mathbf{Y})$
    |  |'
- en: '|  |  | $\displaystyle+\mathcal{L}_{\text{GAN}_{\mathbf{X}}}(G_{\mathbf{X}},D_{\mathbf{X}};\mathbf{Y},\mathbf{X})$
    |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\mathcal{L}_{\text{GAN}_{\mathbf{X}}}(G_{\mathbf{X}},D_{\mathbf{X}};\mathbf{Y},\mathbf{X})$
    |  |'
- en: '|  |  | $\displaystyle+\lambda_{\text{cyc}}\mathcal{L}_{\text{cyc}_{\mathbf{Y}}}(G_{\mathbf{X}},G_{\mathbf{Y}};\mathbf{Y})$
    |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\lambda_{\text{cyc}}\mathcal{L}_{\text{cyc}_{\mathbf{Y}}}(G_{\mathbf{X}},G_{\mathbf{Y}};\mathbf{Y})$
    |  |'
- en: '|  |  | $\displaystyle+\lambda_{\text{cyc}}\mathcal{L}_{\text{cyc}_{\mathbf{X}}}(G_{\mathbf{Y}},G_{\mathbf{X}};\mathbf{X}),$
    |  | (57) |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\lambda_{\text{cyc}}\mathcal{L}_{\text{cyc}_{\mathbf{X}}}(G_{\mathbf{Y}},G_{\mathbf{X}};\mathbf{X}),$
    |  | (57) |'
- en: 'which corresponds to the following optimization problem:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 对应于以下优化问题：
- en: '|  | $\displaystyle G_{\mathbf{Y}}^{*},G_{\mathbf{X}}^{*}$ | $\displaystyle=\arg\min_{G_{\mathbf{Y}},G_{\mathbf{X}}}\max_{D_{\mathbf{Y}},D_{\mathbf{X}}}\mathcal{L}(G_{\mathbf{Y}},G_{\mathbf{X}},D_{\mathbf{Y}},D_{\mathbf{X}}).$
    |  | (58) |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{\mathbf{Y}}^{*},G_{\mathbf{X}}^{*}$ | $\displaystyle=\arg\min_{G_{\mathbf{Y}},G_{\mathbf{X}}}\max_{D_{\mathbf{Y}},D_{\mathbf{X}}}\mathcal{L}(G_{\mathbf{Y}},G_{\mathbf{X}},D_{\mathbf{Y}},D_{\mathbf{X}}).$
    |  | (58) |'
- en: This conceptually simple method works surprisingly well in practice, especially
    in domains with relatively few semantic types (e.g., when the source domain images
    contain only horses and background, and the target domain images contain only
    zebras and background), where it is less challenging for the algorithm to find
    the matching semantics between the two domains (e.g., horse $\leftrightarrow$
    zebra). However, the results of CycleGAN on translating between more complex data
    distributions containing many more semantic types, such as between urban street
    scenario images and their corresponding semantic labels, are not as satisfactory,
    in that the generators often permute the labels for some semantics.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这种概念上简单的方法在实际中效果惊人地好，特别是在语义类型相对较少的领域（例如，当源域图像仅包含马和背景，目标域图像仅包含斑马和背景时），在这些情况下，算法在两个领域之间找到匹配的语义（例如，马
    $\leftrightarrow$ 斑马）挑战较小。然而，CycleGAN 在处理具有更多语义类型的更复杂数据分布（例如，城市街景图像及其对应的语义标签）时效果不如预期，因为生成器经常会对某些语义的标签进行置换。
- en: 'CyCADA (Hoffman et al., [2017](#bib.bib42)): The semantic consistency loss
    proposed in CyCADA offers a good solution to learning the mapping between more
    complex data distributions with relatively more semantic types. To be more specific,
    in CyCADA, a semantic segmentation network $f$ is first trained in the domain
    where semantic labels are available (e.g., $f_{\mathbf{X}}$ for the synthetic
    domain $\mathbf{X}$). (This is applicable for the domain adaptation between the
    simulated domain $\mathbf{X}$ and the real-world domain $\mathbf{Y}$ in the context
    of robotics, since many recent robotics simulators provide the ground truth semantic
    maps of the rendered images, while the labels for the real images are expensive
    to obtain.) Then this semantic segmentation network is used to constrain the semantic
    consistency between the input and the translated output images of the generators:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: CyCADA（Hoffman 等，[2017](#bib.bib42)）：CyCADA 中提出的语义一致性损失提供了一个很好的解决方案，用于学习复杂数据分布之间的映射，这些分布具有相对较多的语义类型。更具体地说，在
    CyCADA 中，首先在语义标签可用的领域（例如，合成领域 $\mathbf{X}$ 的 $f_{\mathbf{X}}$）训练一个语义分割网络 $f$。
    （在机器人技术的背景下，这适用于模拟领域 $\mathbf{X}$ 和真实世界领域 $\mathbf{Y}$ 之间的领域适应，因为许多最近的机器人模拟器提供了渲染图像的真实语义图，而获取真实图像的标签成本高昂。）然后，使用这个语义分割网络来约束生成器的输入图像和翻译输出图像之间的语义一致性：
- en: '|  | $\displaystyle\mathcal{L}_{\text{sem}_{\mathbf{Y}}}(G_{\mathbf{Y}};\mathbf{X},f_{\mathbf{X}})$
    | $\displaystyle=\text{CrossEnt}(f_{\mathbf{X}}(\mathbf{X}),f_{\mathbf{X}}(G_{\mathbf{Y}}(\mathbf{X}))),$
    |  | (59) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{sem}_{\mathbf{Y}}}(G_{\mathbf{Y}};\mathbf{X},f_{\mathbf{X}})$
    | $\displaystyle=\text{CrossEnt}(f_{\mathbf{X}}(\mathbf{X}),f_{\mathbf{X}}(G_{\mathbf{Y}}(\mathbf{X}))),$
    |  | (59) |'
- en: '|  | $\displaystyle\mathcal{L}_{\text{sem}_{\mathbf{X}}}(G_{\mathbf{X}};\mathbf{Y},f_{\mathbf{X}})$
    | $\displaystyle=\text{CrossEnt}(f_{\mathbf{X}}(\mathbf{Y}),f_{\mathbf{X}}(G_{\mathbf{X}}(\mathbf{Y}))),$
    |  | (60) |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{sem}_{\mathbf{X}}}(G_{\mathbf{X}};\mathbf{Y},f_{\mathbf{X}})$
    | $\displaystyle=\text{CrossEnt}(f_{\mathbf{X}}(\mathbf{Y}),f_{\mathbf{X}}(G_{\mathbf{X}}(\mathbf{Y}))),$
    |  | (60) |'
- en: 'where $\text{CrossEnt}(S_{\mathbf{X}},f_{\mathbf{X}}(\mathbf{X}))$ represents
    the cross-entropy loss between the semantics of $X$ predicted by the pretrained
    semantic segmentation network $f_{\mathbf{X}}$ and the ground truth label $S_{\mathbf{X}}$.
    The semantic consistency losses are then added to the CycleGAN objective (Eq.
    [58](#S2.E58 "In II-G1 Domain Adaptation ‣ II-G The Reality Gap: From Simulation
    to the Real World ‣ II Deep reinforcement learning ‣ A Survey of Deep Network
    Solutions for Learning Control in Robotics: From Reinforcement to Imitation")).'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\text{CrossEnt}(S_{\mathbf{X}},f_{\mathbf{X}}(\mathbf{X}))$ 表示由预训练语义分割网络
    $f_{\mathbf{X}}$ 预测的 $X$ 的语义与真实标签 $S_{\mathbf{X}}$ 之间的交叉熵损失。然后将语义一致性损失添加到 CycleGAN
    目标中（参见 Eq. [58](#S2.E58 "在 II-G1 领域适应 ‣ II-G 现实差距：从模拟到现实世界 ‣ II 深度强化学习 ‣ 机器人控制学习的深度网络解决方案综述：从强化学习到模仿学习")）。
- en: II-G2 Domain Adaptation for Visual DRL Policies
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-G2 视觉 DRL 策略的领域适应
- en: While many extensions and variants have been proposed for image-to-image translation
    in the computer vision literature, here we focus on those domain adaptation methods
    that specifically itarget transferring DRL control policies from simulation to
    real scenarios.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在计算机视觉文献中已经提出了许多用于图像到图像翻译的扩展和变体，但在这里我们专注于那些特别针对从模拟到真实场景传递深度强化学习（DRL）控制策略的领域适应方法。
- en: For manipulation tasks, Bousmalis et al. ([2017](#bib.bib8)) deal with the reality
    gap by adapting synthetic images to the realistic domain before feeding them into
    the DRL policy network during the training phase. However, the additional adaptation
    step required for every training iteration could significantly slow down the whole
    learning pipeline. Tobin et al. ([2017](#bib.bib103)) proposed to randomise the
    lighting conditions, viewing angles and textures of objects during the training
    phase of the DRL policies in simulation, in the hope that after being exposed
    to enough variations, the learned model can naturally generalize to real-world
    scenarios. However, this method can only be applied to simulators where such randomization
    can be easily achieved at a low cost, which is not the case for most of the popular
    robotic simulators. Moreover, there is no guarantee that for a random real-world
    scenario, its visual modality can be covered by the randomized simulations. Similarly,
    randomizing the dynamics of the simulator during training has also been proposed
    (Peng et al., [2017](#bib.bib72)) to bridge the reality gap. Rusu et al. ([2017](#bib.bib81))
    propose to progressively adapt the learned deep features and representations from
    the synthetic domain to the real-world domain. However, this method still requires
    going through an expensive DRL control policy training phase (although this procedure
    can be greatly sped up by initial training in the simulator) for each new visually
    different real-world scene.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 对于操控任务，Bousmalis 等人 ([2017](#bib.bib8)) 通过在将合成图像输入DRL策略网络之前将其适配到现实领域来解决现实差距问题。然而，每次训练迭代所需的额外适配步骤可能会显著减慢整个学习流程。Tobin
    等人 ([2017](#bib.bib103)) 提出了在DRL策略的训练阶段随机化光照条件、视角和物体纹理的方案，希望经过足够的变化后，学习到的模型能够自然地推广到现实场景中。然而，这种方法只能应用于那些能够以低成本轻松实现这种随机化的模拟器，而大多数流行的机器人模拟器并非如此。此外，随机化的模拟不能保证覆盖所有随机的现实世界场景的视觉模态。同样，在训练过程中随机化模拟器的动态也被提议（Peng
    等人，[2017](#bib.bib72)）以弥合现实差距。Rusu 等人 ([2017](#bib.bib81)) 提出了逐步将从合成领域中学到的深度特征和表示适配到现实世界领域。然而，这种方法仍然需要经过一个昂贵的DRL控制策略训练阶段（尽管这一过程可以通过在模拟器中进行初步训练大大加快），以应对每个新的视觉上不同的现实世界场景。
- en: The aforementioned methods realize domain adaptation via the sim-to-real direction,
    meaning that they either translate the synthetic images to the real-world domain
    during the training of DRL policies, or adapt the deep features of the simulated
    domain to those of the realistic domain. However, the DRL policy learning and
    the adaptation of the policies are entangled in this line of methods.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法通过从模拟到现实的方向实现领域适应，这意味着它们要么在DRL策略的训练过程中将合成图像转换为现实世界领域，要么将模拟领域的深度特征适配到现实领域。然而，这些方法中的DRL策略学习与策略适应是纠缠在一起的。
- en: The recently proposed model of VR Goggles (Zhang et al., [2018](#bib.bib122))
    decouples the two components of policy learning and policy adaptation, by tackling
    the reality gap from the real-to-sim direction, which requires no extra transfer
    steps during the expensive training of DRL policies. Specifically, the VR Goggles
    deal with the reality gap only during the actual deployment phase, by translating
    real-world sensor reading streams back to the simulated domain, so as to adapt
    the unseen or unfamiliar characteristics of the real scenes to the synthetic features,
    which the agent has already learned well how to deal with, to make the robot feel
    at home. To constrain the consistency between the generated subsequent frames,
    a shift loss is added to the optimization objective, which is inspired by the
    artistic style transfer for videos literature (Ruder et al., [2017](#bib.bib80)).
    This method is validated in transferring DRL navigation policies, which could
    be considered more challenging than manipulation tasks, since the environments
    the navigation agents operate in are typically at much larger scales than the
    confined workspace of manipulators.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 最近提出的VR Goggles模型（Zhang et al., [2018](#bib.bib122)）通过处理从真实到模拟的现实差距，将策略学习和策略适应这两个组件解耦，这在DRL策略的昂贵训练过程中不需要额外的转移步骤。具体来说，VR
    Goggles仅在实际部署阶段处理现实差距，通过将真实世界的传感器读取流翻译回模拟域，从而将真实场景中未见或不熟悉的特征适应为代理已经学会如何处理的合成特征，使机器人感到如同在家。为了约束生成的后续帧之间的一致性，优化目标中添加了一个位移损失，这受到视频艺术风格迁移文献（Ruder
    et al., [2017](#bib.bib80)）的启发。这种方法在转移DRL导航策略时得到了验证，这可能比操作任务更具挑战性，因为导航代理所操作的环境通常比操纵器的有限工作空间要大得多。
- en: Both results of outdoor and indoor scene adaptation have been presented. For
    the outdoor experiment, the synthetic data is collected from the CARLA simulator
    (Dosovitskiy et al., [2017](#bib.bib18))， which provides the ground truth semantic
    labels, and the real world data is gathered from the RobotCar dataset (Maddern
    et al., [2017](#bib.bib62)). The semantic consistency loss is added for the outdoor
    scenario, with a semantic segmentation network trained using the DeepLab model
    (Chen et al., [2016](#bib.bib11)). The semantic consistency is critical for outdoor
    scenes containing various semantic types, without such a constraint, permutation
    of semantics occurs. It is also critical for situations where the model fails
    to generate a virtual car at the position at which there is a real car in the
    real image (This kind of performance is as reported by Yang et al. ([2018](#bib.bib117))
    whithout constraining the semantic consistency), which could potentially lead
    to accidents in self-driving scenarios.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 已展示了户外和室内场景适应的结果。对于户外实验，合成数据来自CARLA模拟器（Dosovitskiy et al., [2017](#bib.bib18)），该模拟器提供了真实的语义标签，实际世界的数据来自RobotCar数据集（Maddern
    et al., [2017](#bib.bib62)）。对于户外场景，添加了语义一致性损失，并使用DeepLab模型（Chen et al., [2016](#bib.bib11)）训练了语义分割网络。语义一致性对于包含各种语义类型的户外场景至关重要，若没有这种约束，则会出现语义的排列组合。这对于模型无法在真实图像中出现真实车辆位置时生成虚拟车的情况也非常关键（这种表现如Yang
    et al. ([2018](#bib.bib117))所述，未约束语义一致性），这可能在自动驾驶场景中导致事故。
- en: For indoor scenes, the semantic loss is not added, as the simulated domain Gazebo
    (Koenig et al., [2004](#bib.bib49)) does not provide ground truth labels, and
    also the real scene, which is a real office environment, contains relatively fewer
    semantic types. A real robot (Turtlebot3 Waffle) is deployed in the office environment
    and feed its sensor readings (captured by a RealSense R200 camera) to the VR Goggles
    model. The translated Gazebo images are then fed to the DRL policy network to
    give control commands. The VR Goggles offer a lightweight and flexible solution
    for transferring DRL visual control policies from simulation to the real world,
    and should also be applicable to manipulation tasks.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 对于室内场景，没有添加语义损失，因为模拟域Gazebo（Koenig et al., [2004](#bib.bib49)）不提供真实标签，而且真实场景是一个真实的办公环境，包含的语义类型相对较少。将真实机器人（Turtlebot3
    Waffle）部署在办公环境中，并将其传感器读取（由RealSense R200相机捕获）馈送到VR Goggles模型中。然后，将翻译后的Gazebo图像馈送到DRL策略网络以生成控制命令。VR
    Goggles提供了一种轻量级且灵活的解决方案，用于将DRL视觉控制策略从模拟转移到现实世界，并且也应适用于操作任务。
- en: II-H Simulation Platforms
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-H 模拟平台
- en: As mentioned before, DRL algorithms, at their current state, are in general
    not sample efficient enough to be directly trained on real robotic platforms.
    Thus robotics simulators are utilized for the initial training of DRL policies.
    Here we review several of the most widely used simulation platforms that are suitable
    for DRL training.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，DRL 算法在当前状态下，通常不够样本高效，无法直接在真实机器人平台上进行训练。因此，机器人模拟器被用于 DRL 策略的初始训练。在这里，我们回顾了几个最广泛使用的适用于
    DRL 训练的模拟平台。
- en: 'TABLE I: Robotic Simulators.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：机器人模拟器。
- en: '| Simulator | Modalities | Framerate | Target Use Case |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 模拟器 | 模态 | 帧率 | 目标使用场景 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Gazebo (Koenig et al., [2004](#bib.bib49)) | Sensor Plugins | 10s+FPS | General
    Purposes |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| Gazebo (Koenig 等，[2004](#bib.bib49)) | 传感器插件 | 10s+FPS | 通用目的 |'
- en: '| Vrep (Rohmer et al., [2013](#bib.bib78)) | Sensor Plugins | 10s+FPS | General
    Purposes |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| Vrep (Rohmer 等，[2013](#bib.bib78)) | 传感器插件 | 10s+FPS | 通用目的 |'
- en: '| Airsim (Shah et al., [2017](#bib.bib90)) | Depth/Color/Semantics | 20s+FPS
    | Autonomous Driving |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| Airsim (Shah 等，[2017](#bib.bib90)) | 深度/颜色/语义 | 20s+FPS | 自动驾驶 |'
- en: '| Carla (Dosovitskiy et al., [2017](#bib.bib18)) | Depth/Color/Semantics |
    30s+FPS | Autonomous Driving |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Carla (Dosovitskiy 等，[2017](#bib.bib18)) | 深度/颜色/语义 | 30s+FPS | 自动驾驶 |'
- en: '| Torcs (You et al., [2017](#bib.bib118)) | Color/Semantics | 100s+FPS | Autonomous
    Driving |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Torcs (You 等，[2017](#bib.bib118)) | 颜色/语义 | 100s+FPS | 自动驾驶 |'
- en: '| AI2-Thor (Kolve et al., [2017](#bib.bib50)) | Color | 100s+FPS | Indoor Navigation
    |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| AI2-Thor (Kolve 等，[2017](#bib.bib50)) | 颜色 | 100s+FPS | 室内导航 |'
- en: '| Minos (Savva et al., [2017](#bib.bib83)) | Depth/Color/Semantics | 100s+FPS
    | Indoor Navigation |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Minos (Savva 等，[2017](#bib.bib83)) | 深度/颜色/语义 | 100s+FPS | 室内导航 |'
- en: '| House3D (Wu et al., [2018](#bib.bib115)) | Depth/Color/Semantics | 600s+FPS
    | Indoor Navigation |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| House3D (Wu 等，[2018](#bib.bib115)) | 深度/颜色/语义 | 600s+FPS | 室内导航 |'
- en: 'We summarize the most commonly used simulators in Table [I](#S2.T1 "TABLE I
    ‣ II-H Simulation Platforms ‣ II Deep reinforcement learning ‣ A Survey of Deep
    Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation"),
    listing their available sensor observation types and their target use cases.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格 [I](#S2.T1 "TABLE I ‣ II-H Simulation Platforms ‣ II Deep reinforcement
    learning ‣ A Survey of Deep Network Solutions for Learning Control in Robotics:
    From Reinforcement to Imitation") 中总结了最常用的模拟器，列出了它们可用的传感器观测类型及其目标使用场景。'
- en: III Imitation Learning
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 模仿学习
- en: DRL offers a formulation for control skills acquisition. However, relying on
    learning from trial and error, DRL methods typically require a significant amount
    of system interaction time. Also, a carefully designed well-shaped reward structure
    is necessary to guide the search of optimal policies, which can often be non-trivial
    in complex scenarios.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）提供了一种控制技能获取的公式。然而，依赖于试错学习，DRL 方法通常需要大量的系统交互时间。此外，需要精心设计的良好奖励结构来指导优化策略的搜索，这在复杂场景中往往并不简单。
- en: Imitation learning, as an alternative to learning control policies, guides the
    policy search, not by hand-designed reward signals, but by providing the learning
    agent with experts’ demonstrations (Bagnell, [2015](#bib.bib3)). It offers a paradigm
    for agents to learn successful policies in fields where people can easily demonstrate
    the desired behavior but find it difficult to hand program or hardcode the correct
    cost or reward function. This is especially useful for humanoid robots or manipulators
    with high degrees of freedom.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习，作为控制策略学习的替代方案，通过提供专家的示范（Bagnell，[2015](#bib.bib3)），引导策略搜索，而不是通过手工设计的奖励信号。它为代理提供了一种在可以轻松展示期望行为但难以手工编程或硬编码正确的成本或奖励函数的领域中学习成功策略的范式。这对于具有高自由度的人形机器人或操控器尤其有用。
- en: 'Perhaps the most simple method of imitation learning is addressing it as a
    standard supervised learning problem. But as we have discussed, as a method for
    learning policies to make sequential control decisions, imitation learning cannot
    be conducted effectively by directly applying the classical supervised learning
    approaches. We emphasize the most critical distinctions below:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 或许最简单的模仿学习方法是将其视为标准的监督学习问题。但正如我们所讨论的，作为一种学习策略以进行顺序控制决策的方法，模仿学习不能通过直接应用经典的监督学习方法来有效进行。我们在下面强调了最关键的区别：
- en: 'Independent VS. Compounding Errors: Standard supervised learning assumes that
    the predictions made by the learning agents do not affect the environment in which
    they operate; hence the data distribution they are to encounter is assumed to
    be the same as what they have experienced. However, although the learning errors
    are independent for each sample in supervised learning, they are compounded in
    imitation learning. This is due to the fact that the standard supervised learning
    algorithms are only expected to do well over samples that are drawn from the same
    distribution as they have been trained on. This i.i.d. assumption, however,is
    badly violated in imitaiton learning, in which an early error could potentially
    cascade to a sequence of mistakes, carried out by the control decisions that are
    made sequentially by the learning agent.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 独立错误与复合错误：标准监督学习假设学习代理所做的预测不会影响其操作的环境；因此，遇到的数据分布被假定为与他们所经历的一致。然而，尽管在监督学习中每个样本的学习误差是独立的，但在模仿学习中这些误差会被复合。这是因为标准监督学习算法仅期望在从与训练时相同分布中抽取的样本上表现良好。然而，这种独立同分布假设在模仿学习中严重违反，因为早期的错误可能会因学习代理按顺序做出的控制决策而导致一系列的错误。
- en: 'Single-Timestep VS. Multi-Timestep Decisions: Supervised learning agents are
    only capable of learning reactive policies, since they completely ignore the temporal
    dependence between subsequent decisions, which leads to myopic strategies. In
    contrast, for making informative decisions, classical planning approaches in robotics
    reason far into the future (but often require sophisticatedly designed cost functions).
    Also, a naive imitation of the experts’ demonstrations often misses the true learning
    objective: instead of copying the demonstrated behaviors given by the experts,
    the actual goal of imitation learning is in some cases quite different and inexplicitly
    optimized in the demonstrations, such as to increase the success rate of accomplishing
    a specific task, to minimize the probability of colliding with obstacles, or to
    minimize the total travel cost.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 单时间步与多时间步决策：监督学习代理只能学习反应策略，因为它们完全忽视了后续决策之间的时间依赖性，这导致了短视策略。相比之下，为了做出有信息的决策，机器人技术中的经典规划方法会考虑远期情况（但通常需要复杂设计的成本函数）。此外，简单地模仿专家的演示往往会忽略真正的学习目标：而不是复制专家所展示的行为，模仿学习的实际目标在某些情况下可能大相径庭且在演示中没有明确优化，例如提高完成特定任务的成功率，减少与障碍物碰撞的概率，或最小化总旅行成本。
- en: In the following, we proceed by going through the three most common approaches
    of imitation learning, which address the above issues from different perspectives,
    and introduce representative works in robotics for each.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨模仿学习的三种最常见的方法，这些方法从不同的角度解决了上述问题，并介绍了每种方法在机器人技术中的代表性工作。
- en: III-A Behavior Cloning
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 行为克隆
- en: Behavior cloning tackles the problem of imitation learning in a supervised manner,
    by directly learning the mapping between the input observations and their corresponding
    actions, which are given by the expert policy. This simple formulation can give
    a satisfactory performance when there is enough training data, but will lead to
    compounding errors, as we have just discussed. One of the most well-known algorithms
    to compensate for this is DAGGer (in which DAGG stands for Data AGGregation) (Ross
    et al., [2011](#bib.bib79)), which interleaves execution and learning. To be more
    specific, in the $i\text{th}$ iteration of DAGGer, the current learned policy
    $\pi_{i-1}$ will be executed to collect experiences. Then those newly recorded
    observations will be relabeled by the expert policy $\pi_{\text{E}}$. These corrected
    new experiences $D_{i}$ will be added to the existing dataset $D$, on which a
    new policy $\pi_{i}$ is trained. This interaction between execution and learning
    halts the error compounding and bounds the expected error to that in the standard
    supervised learning setting.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 行为克隆通过以监督的方式解决模仿学习的问题，直接学习输入观察值与其对应动作之间的映射，这些动作由专家策略提供。这个简单的公式在有足够的训练数据时可以提供令人满意的性能，但正如我们刚刚讨论的，它会导致累积误差。为补偿这种情况，其中一个最著名的算法是DAGGer（其中DAGG代表数据聚合）（Ross
    et al., [2011](#bib.bib79)），它交替进行执行和学习。更具体地说，在DAGGer的第$i\text{th}$次迭代中，当前学习到的策略$\pi_{i-1}$将被执行以收集经验。然后，这些新记录的观察值将由专家策略$\pi_{\text{E}}$进行重新标记。这些被纠正的新经验$D_{i}$将被添加到现有数据集$D$中，并在此基础上训练新的策略$\pi_{i}$。这种执行和学习之间的互动停止了误差的累积，并将期望误差控制在标准监督学习设置中的水平。
- en: Due to its simple formulation, behavior cloning has been widely studied and
    applied in robotics control problems.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其简单的公式，行为克隆已被广泛研究并应用于机器人控制问题。
- en: We start with the literature in the field of navigation and self-driving imitation.
    Bojarski et al. ([2016](#bib.bib7)) learn a direct mapping from raw first-person
    view color images to steering commands, on a training dataset collected by driving
    on a wide variety of roads and in diverse weather and lighting conditions, which
    in total adds up to $72$ hours of driving data. Tai et al. ([2016](#bib.bib98))
    drive an indoor mobile robot autonomously through a dataset based on joystick
    commands from human demonstrator. A depth visual image is taken as the only input
    in their implementation. Giusti et al. ([2016](#bib.bib29)) train a deep network
    to determine actions that can keep a quadrotor on a trail, by learning on single
    monocular images collected from the robot’s perspective. Eight hours of video
    is captured using three GoPros mounted on the head of a hiker, with one pointing
    to the left, one to the right, and one straight ahead. The optimal actions for
    the collected images can then be easily labeled; e.g., the quadrotor should turn
    right when facing an image collected from the left-facing camera. Codevilla et al.
    ([2017](#bib.bib14)) observes that the pure behavior cloning assumption could
    break under certain situations, such as when a driver approaches an intersection.
    The driver’s subsequent actions cannot be fully explained by the observations,
    since they are additionally affected by the driver’s internal throughouts, such
    as the intended destination. To address this, a conditional imitation learning
    method is proposed to additionally constrain the imitation learning additionally
    on a representation of the expert’s intention, so as to resolve the ambiguity
    in the perceptuomotor mapping. Both simulated and real-world experiments are conducted,
    in which the synthetic dataset is collected in the simulated self-driving environment
    Carla (Dosovitskiy et al., [2017](#bib.bib18)) and a real-world dataset from remote
    controlling a robotic truck in a residential area, each of which contains two
    hours of driving time.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从导航和自动驾驶模仿领域的文献开始。Bojarski 等（[2016](#bib.bib7)）学习从原始第一人称视角彩色图像到转向指令的直接映射，在一个通过在各种道路和不同天气及光照条件下驾驶收集的训练数据集上，总共积累了
    $72$ 小时的驾驶数据。Tai 等（[2016](#bib.bib98)）通过基于人类示范者的操纵杆指令的数据集自主驾驶一个室内移动机器人。他们的实现中深度视觉图像作为唯一输入。Giusti
    等（[2016](#bib.bib29)）训练一个深度网络来确定可以保持四旋翼在轨迹上的动作，通过学习从机器人视角收集的单幅单眼图像。使用安装在登山者头上的三台
    GoPro 摄像机拍摄了八小时的视频，其中一台指向左侧，一台指向右侧，一台指向前方。然后可以轻松标记收集图像的最佳动作；例如，当面对从左侧摄像机收集的图像时，四旋翼应该向右转。Codevilla
    等（[2017](#bib.bib14)）观察到纯行为克隆假设在某些情况下可能会失效，例如当司机接近交叉口时。由于司机的后续动作受到司机内部思想的额外影响，例如意图目的地，因此观察到的行为无法完全解释。为此，提出了一种条件模仿学习方法，以额外约束模仿学习在专家意图的表示上，以解决感知运动映射中的模糊性。在模拟和现实世界实验中进行，其中合成数据集是在模拟自动驾驶环境
    Carla（Dosovitskiy 等，[2017](#bib.bib18)）中收集的，现实世界数据集则是从远程控制住宅区的机器人卡车中获得的，每个数据集包含两个小时的驾驶时间。
- en: 'In terms of imitation learning for manipulation, a recently proposed line three
    works presents and improves on one-shot imitation learning: from taking low dimensional
    states and expert action pairs as demonstrations (Duan et al., [2017](#bib.bib19)),
    to learning from demonstrations of raw visual images paired with actions (Finn
    et al., [2017b](#bib.bib23)), and finally arriving at the current state of learning
    from human demonstration videos without labeled actions (Yu et al., [2018](#bib.bib119)).
    Below we discuss these methods in more detail.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在操控的模仿学习方面，最近提出的三种工作展示并改进了一次性模仿学习：从采用低维状态和专家动作对作为示范（Duan 等，[2017](#bib.bib19)），到从与动作配对的原始视觉图像示范中学习（Finn
    等，[2017b](#bib.bib23)），最终达到当前从没有标记动作的人类示范视频中学习的状态（Yu 等，[2018](#bib.bib119)）。以下我们将更详细地讨论这些方法。
- en: Duan et al. ([2017](#bib.bib19)) present the imitation agent with pairs of demonstrations
    for each iteration during training, in which the network takes as input the firsth
    demonstration and a state sampled from the second demonstration. The network is
    then trained using behavior cloning losses to predict the corresponding action
    of that sampled state. The concrete example used in their problem setting is a
    distribution of block stacking tasks, in which the goal is to control a robot
    arm to stack various numbers of cubic blocks into configurations specified by
    the user. Each observation is a list of the relative positions of the blocks w.r.t.
    the gripper, and information indicating whether the gripper is open or closed.
    Several architectural designs, such as temporal dropout and convolutions, neighborhood
    attention, are incorporated into their training pipeline to cope with variable-dimensional
    and potentially long sequences of inputs. In their experiments, the performance
    of pure behavior cloning achieves the same level of performance as training with
    DAGGer, suggesting that at least for this specific block-stacking task, the interactive
    supervision in DAGGer might not necessarily lead to a performance gain.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: Duan 等人 ([2017](#bib.bib19)) 在训练过程中为每次迭代提供了配对的示例，其中网络以第一个示例和从第二个示例中采样的状态作为输入。然后，使用行为克隆损失训练网络，以预测该采样状态的对应动作。他们问题设置中使用的具体示例是一个积木堆叠任务的分布，其目标是控制机器人手臂将各种数量的立方块堆叠成用户指定的配置。每个观察值是相对于夹具的块的相对位置列表，以及指示夹具是否打开或关闭的信息。他们的训练流程中结合了几种架构设计，如时间丢弃和卷积、邻域注意等，以应对可变维度和潜在的长序列输入。在他们的实验中，纯行为克隆的性能达到了与使用
    DAGGer 训练相同的水平，表明至少对于这个特定的积木堆叠任务，DAGGer 中的交互监督可能并不一定带来性能提升。
- en: 'Finn et al. ([2017b](#bib.bib23)) and Yu et al. ([2018](#bib.bib119)) both
    extend the Model-Agnostic Meta-Learning (MAML) method (Finn et al., [2017a](#bib.bib21)),
    which we will briefly review here before proceeding. The objective of MAML is
    to learn a model, such that, after being trained on a variety of learning tasks,
    it is able to learn to solve new tasks with only a small number of training samples.
    Formally, this model of interest is denoted as $f_{\theta}$ with weights $\theta$,
    and the meta-learning is considered over a distribution of tasks $p(\mathcal{T})$.
    The model parameters will be updated from $\theta$ to $\theta^{\prime}_{i}$, when
    adapting to a new task $\mathcal{T}_{i}$. This update is performed using gradient
    descent on task $\mathcal{T}_{i}$:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: Finn 等人 ([2017b](#bib.bib23)) 和 Yu 等人 ([2018](#bib.bib119)) 都扩展了 Model-Agnostic
    Meta-Learning (MAML) 方法（Finn 等人，[2017a](#bib.bib21)），我们将在此简要回顾这一方法。MAML 的目标是学习一个模型，使其在经过各种学习任务的训练后，能够仅用少量训练样本学习解决新任务。形式上，这个感兴趣的模型表示为
    $f_{\theta}$，其中 $\theta$ 为权重，而元学习则是在任务分布 $p(\mathcal{T})$ 上进行的。当适应新任务 $\mathcal{T}_{i}$
    时，模型参数将从 $\theta$ 更新为 $\theta^{\prime}_{i}$。这种更新是通过对任务 $\mathcal{T}_{i}$ 进行梯度下降来实现的：
- en: '|  | $\displaystyle\theta^{\prime}_{i}$ | $\displaystyle=\theta-\alpha\nabla_{\theta}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta}),$
    |  | (61) |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{\prime}_{i}$ | $\displaystyle=\theta-\alpha\nabla_{\theta}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta}),$
    |  | (61) |'
- en: 'where $\alpha$ denotes a step size, and $\mathcal{L}$ represents a behavior
    cloning loss function (e.g., mean squared error for continuous actions, cross-entropy
    loss for discrete actions). After the updated $\theta^{\prime}_{i}$ is obtained,
    its performance is optimized w.r.t. $\theta$ across tasks sampled from $p(\mathcal{T})$,
    leading to the following meta-learning objective:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 表示步长，$\mathcal{L}$ 代表行为克隆损失函数（例如，对于连续动作的均方误差，对于离散动作的交叉熵损失）。在获得更新后的
    $\theta^{\prime}_{i}$ 后，其性能将相对于 $\theta$ 在从 $p(\mathcal{T})$ 中采样的任务上进行优化，从而得到以下元学习目标：
- en: '|  | $\displaystyle\min\sum_{\mathcal{T}_{i}\sim p(\mathcal{T})}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta^{\prime}_{i}})$
    | $\displaystyle=\sum_{\mathcal{T}_{i}\sim p(\mathcal{T})}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta-\alpha\nabla_{\theta}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta})}),$
    |  | (62) |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min\sum_{\mathcal{T}_{i}\sim p(\mathcal{T})}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta^{\prime}_{i}})$
    | $\displaystyle=\sum_{\mathcal{T}_{i}\sim p(\mathcal{T})}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta-\alpha\nabla_{\theta}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta})}),$
    |  | (62) |'
- en: 'which is performed via SGD such that $\theta$ is updated as follows:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 该目标通过 SGD 进行，以便 $\theta$ 更新如下：
- en: '|  | $\displaystyle\theta$ | $\displaystyle\leftarrow\theta-\beta\nabla_{\theta}\sum_{\mathcal{T}_{i}\sim
    p(\mathcal{T})}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta^{\prime}_{i}}),$ |  | (63)
    |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta$ | $\displaystyle\leftarrow\theta-\beta\nabla_{\theta}\sum_{\mathcal{T}_{i}\sim
    p(\mathcal{T})}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta^{\prime}_{i}}),$ |  | (63)
    |'
- en: where $\beta$ is the meta step size.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\beta$ 是元步骤大小。
- en: Here, the meta-optimization is performed over $\theta$, while the loss is computed
    using the updated parameters $\theta^{\prime}$. This objective will help to find
    model parameters that are sensitive to changes in the task, such that small changes
    in the parameters could lead to large improvements in the performance on any task
    sampled from $p(\mathcal{T})$.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，元优化在 $\theta$ 上进行，而损失是使用更新后的参数 $\theta^{\prime}$ 计算的。这个目标将有助于找到对任务变化敏感的模型参数，以便参数的小变化可以在从
    $p(\mathcal{T})$ 采样的任何任务上带来性能的大幅提升。
- en: Based on the formulation of MAML, Finn et al. ([2017b](#bib.bib23)) learn policies
    that can be quickly adapted to new tasks using a single demonstration. Here each
    observation input into the model contains a color image from the robot’s perspective,
    and the robot configurations (joint angles, end-effector poses). While both Duan
    et al. ([2017](#bib.bib19)) and Finn et al. ([2017b](#bib.bib23)) use only robot
    demonstrations throughout training and testing, Yu et al. ([2018](#bib.bib119))
    is able to cope with domain shift by learning from both robot and human demonstrations,
    in which the human demonstrations are not labeled with expert actions. After meta-learning,
    the proposed method is capable of learning from human videos. To cope with the
    unlabelled human demonstrations, an adaptation loss function $\mathcal{L}_{\psi}$
    is learned alongside the meta-learning objective. During training, human demonstrations
    are used to compute the updated policy parameters $\theta^{\prime}_{i}$ with the
    gradients calculated using $\mathcal{L}_{\psi}$. Then the performance of $\theta^{\prime}_{i}$
    is evaluated using the behavior cloning loss to update both $\theta$ and $\psi$.
    Note that all the robot demonstrations are collected via teleoperation (Zhang
    et al., [2017c](#bib.bib123)).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 MAML 的表述，Finn 等人 ([2017b](#bib.bib23)) 学习了可以通过单次演示快速适应新任务的策略。这里，输入模型的每个观察包含来自机器人的视角的彩色图像和机器人配置（关节角度，末端执行器姿态）。虽然
    Duan 等人 ([2017](#bib.bib19)) 和 Finn 等人 ([2017b](#bib.bib23)) 在整个训练和测试过程中仅使用机器人演示，但
    Yu 等人 ([2018](#bib.bib119)) 能够通过同时从机器人和人类演示中学习来应对领域转移，其中人类演示没有标记专家动作。在元学习之后，所提出的方法能够从人类视频中学习。为了应对未标记的人类演示，学习了一个适应损失函数
    $\mathcal{L}_{\psi}$，与元学习目标一起进行训练。在训练过程中，人类演示用于计算使用 $\mathcal{L}_{\psi}$ 计算的更新策略参数
    $\theta^{\prime}_{i}$。然后，通过行为克隆损失评估 $\theta^{\prime}_{i}$ 的性能，以更新 $\theta$ 和 $\psi$。请注意，所有的机器人演示都是通过遥操作收集的（Zhang
    等人，[2017c](#bib.bib123)）。
- en: A recent work from Eitel et al. ([2017](#bib.bib20)) introduces a model that
    is able to propose push actions based on over-segmented RGB-D images, in order
    to separate unknown objects in cluttered environments.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: Eitel 等人 ([2017](#bib.bib20)) 的一项最新工作引入了一个模型，该模型能够根据过度分割的 RGB-D 图像提出推送动作，以便在杂乱环境中分离未知物体。
- en: III-B Inverse Reinforcement Learning
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 逆向强化学习
- en: 'Inverse reinforcement learning (IRL) frames imitation learning as solutions
    to MDPs, thus reducing the problem of learning to the problem of recovering a
    utility function that makes the demonstrated behavior (near-)optimal. After this
    utility function is obtained, reinforcement learning procedures can be performed
    on top to search for optimal policies. A representative IRL method, the Maximum
    Entropy IRL (Ziebart et al., [2008](#bib.bib127)), fits a cost function from a
    family of functions $\mathcal{C}$ to optimize the following objective:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 逆向强化学习（IRL）将模仿学习框架视为 MDP 的解，从而将学习问题简化为恢复使演示行为（近似）最优的效用函数的问题。在获得这个效用函数后，可以在其上执行强化学习程序，以搜索最优策略。一个代表性的
    IRL 方法是最大熵 IRL（Ziebart 等人，[2008](#bib.bib127)），它从函数族 $\mathcal{C}$ 中拟合一个成本函数，以优化以下目标：
- en: '|  | $\displaystyle\operatorname*{\arg\!\max}_{c\in\mathcal{C}}\left(\min_{\pi\in\prod}-H(\pi)+\mathbf{E}_{\pi}\left[c(\mathbf{s},\mathbf{a})\right]\right)-\mathbf{E}_{\pi^{\text{E}}}\left[c(\mathbf{s},\mathbf{a})\right],$
    |  | (64) |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\operatorname*{\arg\!\max}_{c\in\mathcal{C}}\left(\min_{\pi\in\prod}-H(\pi)+\mathbf{E}_{\pi}\left[c(\mathbf{s},\mathbf{a})\right]\right)-\mathbf{E}_{\pi^{\text{E}}}\left[c(\mathbf{s},\mathbf{a})\right],$
    |  | (64) |'
- en: where $\prod$ denotes the family of policies.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\prod$ 表示策略的族。
- en: In robotics, the formulation of IRL offers an efficient solution for learning
    policies for socially compliant navigation (Okal and Arras, [2016](#bib.bib68);
    Pfeiffer et al., [2016](#bib.bib73); Kretzschmar et al., [2016](#bib.bib51)),
    where the agent needs to not only avoid collisions with static obstacles but also
    to behave in a socially compliant manner. Thus, the underlying cost function is
    non-trivial to hand-design, but the hebaviors are easy to demonstrate.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器人领域，IRL 的形式化提供了一种有效的解决方案，用于学习社会合规导航策略（Okal 和 Arras，[2016](#bib.bib68)；Pfeiffer
    等人，[2016](#bib.bib73)；Kretzschmar 等人，[2016](#bib.bib51)），其中代理不仅需要避免与静态障碍物碰撞，还要以社会合规的方式行事。因此，基础的成本函数设计并不简单，但行为很容易演示。
- en: Wulfmeier et al. ([2015](#bib.bib116)) extends Maximum Entropy IRL under the
    deep learning context, utilizing fully convolutional neural networks as the approximator
    for learning the reward function. The proposed algorithm is successfully deployed
    for learning the cost map in urban environments, from a dataset of driving behaviors
    demonstrated by human experts.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: Wulfmeier 等人（[2015](#bib.bib116)）在深度学习背景下扩展了最大熵逆向强化学习（IRL），利用完全卷积神经网络作为奖励函数学习的近似器。所提算法成功应用于从人类专家展示的驾驶行为数据集中学习城市环境中的成本地图。
- en: III-C Generative Adversarial Imitation Learning
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 生成对抗模仿学习
- en: 'The learning process of IRL can be indirect and slow. Inspired by Generative
    Adversarial Networks (GANs) (Goodfellow et al., [2014](#bib.bib31)), Ho and Ermon
    ([2016](#bib.bib41)) propose Generative Adversarial Imitation Learning (GAIL),
    which surpasses the intermediate step of learning a reward function and is capable
    of directly learning a policy from expert demonstrations. To be more specific,
    in the GAIL model, a generator $\pi_{\theta}$ with parameters $\theta$ is trained
    to generate state-action ($\mathcal{S}\times\mathcal{A}$) pairs matching those
    of the expert demonstrations, while the discriminator $D_{\omega}$ learns to tell
    apart the generated policy $\pi_{\theta}$ from the expert (demonstrated) policy
    $\pi^{\text{E}}$. The GAIL optimization objective is defined as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: IRL 的学习过程可能是间接且缓慢的。受到生成对抗网络（GANs）（Goodfellow 等人，[2014](#bib.bib31)）的启发，Ho 和
    Ermon（[2016](#bib.bib41)）提出了生成对抗模仿学习（GAIL），它超越了学习奖励函数的中间步骤，能够直接从专家演示中学习策略。更具体地说，在
    GAIL 模型中，具有参数 $\theta$ 的生成器 $\pi_{\theta}$ 被训练以生成与专家演示匹配的状态-动作（$\mathcal{S}\times\mathcal{A}$）对，而判别器
    $D_{\omega}$ 学会将生成的策略 $\pi_{\theta}$ 与专家（演示）策略 $\pi^{\text{E}}$ 区分开来。GAIL 的优化目标定义如下：
- en: '|  | $\displaystyle\mathbf{E}_{\pi_{\theta}}\left[\log(D(\mathbf{s},\mathbf{a}))\right]+\mathbf{E}_{\pi^{\text{E}}}\left[\log(1-D(\mathbf{s},\mathbf{a}))\right]-\lambda
    H(\pi_{\theta}),$ |  | (65) |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{E}_{\pi_{\theta}}\left[\log(D(\mathbf{s},\mathbf{a}))\right]+\mathbf{E}_{\pi^{\text{E}}}\left[\log(1-D(\mathbf{s},\mathbf{a}))\right]-\lambda
    H(\pi_{\theta}),$ |  | (65) |'
- en: 'where $H(\pi_{\theta})$ denotes the causal entropy. The training of GAIL interleaves
    between updating parameters $\omega$ of the discriminator $D_{\omega}$ to maximize
    Eq. [65](#S3.E65 "In III-C Generative Adversarial Imitation Learning ‣ III Imitation
    Learning ‣ A Survey of Deep Network Solutions for Learning Control in Robotics:
    From Reinforcement to Imitation"), and utilizing DRL techniques such as TRPO to
    minimize Eq. [65](#S3.E65 "In III-C Generative Adversarial Imitation Learning
    ‣ III Imitation Learning ‣ A Survey of Deep Network Solutions for Learning Control
    in Robotics: From Reinforcement to Imitation") w.r.t. the parameters $\theta$
    of the generator $\pi_{\theta}$. The scores given out by the discriminator for
    the generated experiences are regarded as costs for those state-action pairs for
    TRPO. Several extensions of GAIL have also been proposed (Baram et al., [2016](#bib.bib4);
    Wang et al., [2017](#bib.bib111)).'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $H(\pi_{\theta})$ 表示因果熵。GAIL 的训练在更新判别器 $D_{\omega}$ 的参数 $\omega$ 以最大化 Eq.
    [65](#S3.E65 "In III-C Generative Adversarial Imitation Learning ‣ III Imitation
    Learning ‣ A Survey of Deep Network Solutions for Learning Control in Robotics:
    From Reinforcement to Imitation") 和利用 DRL 技术如 TRPO 最小化 Eq. [65](#S3.E65 "In III-C
    Generative Adversarial Imitation Learning ‣ III Imitation Learning ‣ A Survey
    of Deep Network Solutions for Learning Control in Robotics: From Reinforcement
    to Imitation") 相对于生成器 $\pi_{\theta}$ 的参数 $\theta$ 之间交替进行。判别器对生成经验的评分被视为 TRPO 的状态-动作对的成本。还提出了
    GAIL 的几个扩展（Baram 等人，[2016](#bib.bib4)；Wang 等人，[2017](#bib.bib111)）。'
- en: In the field of navigation, Li et al. ([2017](#bib.bib58)) successfully apply
    GAIL in simulated autonomous vehicle navigation scenarios with raw visual input.
    Tai et al. ([2018](#bib.bib100)) learn a socially compliant navigation policy
    through GAIL, based on raw depth input, and demonstrate the learned behaviors
    in real robotics experiments.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在导航领域，Li等人（[2017](#bib.bib58)）成功地将GAIL应用于模拟自动驾驶车辆导航场景中，使用原始视觉输入。Tai等人（[2018](#bib.bib100)）通过GAIL学习了一种社会合规的导航策略，基于原始深度输入，并在真实机器人实验中展示了学习到的行为。
- en: For manipulation, Stadie et al. ([2017](#bib.bib93)) extend the GAIL formulation
    with ideas from domain confusion loss (Tzeng et al., [2014](#bib.bib106)), and
    successfully utilize it to train agents to imitate third-person demonstrations,
    by learning a domain-agnostic representation of the agent’s observations.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 对于操作，Stadie等人（[2017](#bib.bib93)）扩展了GAIL公式，结合了领域混淆损失（Tzeng等人，[2014](#bib.bib106)）的思想，并成功地利用它训练代理模仿第三方演示，通过学习代理观察的领域无关表示。
- en: IV Challenges and Open Research Questions
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 挑战与未来研究问题
- en: Utilizing deep learning techniques for learning control for robotics tasks has
    shown great potential. Yet, there still remain many challenges for scaling up
    and stabilizing the aforementioned algorithms to meet the requirements of operating
    robotics systems in real-world applications. We list the critical challenges and
    the corresponding future research directions.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 利用深度学习技术学习机器人任务的控制显示出了巨大潜力。然而，仍然存在许多挑战，需要对上述算法进行扩展和稳定，以满足现实世界应用中机器人系统的要求。我们列出了关键挑战和相应的未来研究方向。
- en: •
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sample Efficiency: Gathering experiences by interacting with the environment
    for deep reinforcement learning, or collecting expert demonstrations for imitation
    learning, are both expensive procedures, in terms of executing control commands
    on real robotics systems. Thus, designing sample efficient algorithms is of critical
    importance.'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 样本效率：通过与环境互动获取深度强化学习的经验，或收集专家演示用于模仿学习，都是在执行真实机器人系统控制命令方面昂贵的过程。因此，设计样本高效的算法至关重要。
- en: •
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Strong Real-time Requirements: A single forward pass of very deep networks
    with millions of parameters could be relatively slow if not equipped with special
    computation hardware and might not meet the real-time requirement for controlling
    real robotics systems. Learning compact representations for dexterous policies
    is preferable.'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 强实时要求：如果没有配备特殊的计算硬件，具有数百万参数的非常深的网络单次前向传播可能相对较慢，可能无法满足控制真实机器人系统的实时要求。学习紧凑表示以实现灵活策略是更可取的。
- en: •
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Safety Concerns: Real robotics systems, such as mobile robots, quadrotors or
    self-driving cars, are expected to operate in environments that could be highly
    dynamic and potentially dangerous. Also, unlike a wrong prediction from a perception
    model, which would not cascade or affect the physical robotic systems or the environment,
    a single false output might lead to a serious accident. Thus, attention should
    be paid to include practical considerations to bound the uncertainty of the possible
    outcomes when deploying control policies on real autonomous systems.'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安全问题：真实的机器人系统，如移动机器人、四旋翼飞行器或自动驾驶汽车，预计会在可能高度动态和潜在危险的环境中操作。此外，与感知模型的错误预测不同，感知模型的错误预测不会级联或影响物理机器人系统或环境，而单次错误输出可能会导致严重事故。因此，在实际部署控制策略时，应该关注包括实际考虑因素，以限定可能结果的不确定性。
- en: •
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Stability, Robustness and Interpretability: DRL algorithms could be relatively
    unstable, and their performance might deviate a lot between configurations that
    only differ slightly from each other (Henderson et al., [2017](#bib.bib40)). To
    overcome this problem, gaining more insight into the learned representations and
    the policies, could be beneficial for detecting adversarial scenarios to prevent
    robotic systems from safety threats.'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稳定性、鲁棒性和可解释性：DRL算法可能相对不稳定，其性能可能在仅有细微差异的配置之间有很大偏差（Henderson等人，[2017](#bib.bib40)）。为解决这个问题，深入了解学习到的表示和策略，可能有助于检测对抗性场景，从而防止机器人系统面临安全威胁。
- en: •
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Lifelong Learning: The visual appearance of the environments that autonomous
    agents operate in cab vary dramatically during different seasons of the year,
    or even different times of day, which could hinder the performance of the learned
    control policies. Hence the ability of continuing to learn to adapt to environmental
    changes as well as preserving the solutions to the already experienced scenarios
    could be of critical value.'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 终身学习：自主代理操作的环境视觉外观可能会在不同季节或甚至不同时间段发生显著变化，这可能会影响学习控制策略的性能。因此，继续学习以适应环境变化的能力以及保存已遇到场景的解决方案可能具有关键价值。
- en: •
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Generalization Between Tasks: Most of the aforementioned algorithms are designed
    to excel in a particular task, which is not ideal, as intelligent robotic systems
    are expected to be capable of carrying out a set of tasks, with a minimal total
    training time for all considered tasks.'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务间的泛化：上述大多数算法设计用于在特定任务中表现出色，这并不理想，因为智能机器人系统预计能够执行一组任务，并且所有任务的总训练时间应尽量减少。
- en: In contrast, with the rapid development of deep learning, several research directions
    are gaining much attention for robotics.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 相对而言，随着深度学习的快速发展，几个研究方向在机器人领域引起了广泛关注。
- en: •
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Unifying Reinforcement Learning and Imitation Learning: Several recent works
    (Večerík et al., [2017](#bib.bib108); Nair et al., [2017](#bib.bib66); Gao et al.,
    [2018](#bib.bib28); Zhu et al., [2018](#bib.bib126)) have introduced algorithms
    that unify reinforcement learning and imitation learning such that the learning
    agent can benefit from both expert demonstrations and interactions with the environment.
    This setup can be beneficial for learning control, as pure DRL algorithms are,
    in general, relatively expensive to train, while learning purely by imitating
    demonstrated behaviors can restrict or bias the control policy in potentially
    suboptimal directions. Thus, the method of using demonstrations to kick-start
    the policy learning, then applying DRL methods to adjust the learned policy, can
    potentially lead to advanced performance.'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 统一强化学习与模仿学习：最近几项工作（Večerík 等，[2017](#bib.bib108)；Nair 等，[2017](#bib.bib66)；Gao
    等，[2018](#bib.bib28)；Zhu 等，[2018](#bib.bib126)）引入了统一强化学习与模仿学习的算法，使得学习代理可以同时从专家演示和与环境的互动中受益。这种设置对学习控制是有利的，因为纯粹的深度强化学习（DRL）算法通常训练成本较高，而仅通过模仿演示行为进行学习可能会将控制策略限制或偏向于潜在的次优方向。因此，利用演示启动策略学习，然后应用
    DRL 方法调整已学策略，可能会带来先进的性能。
- en: •
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Meta-learning: Finn et al. ([2017a](#bib.bib21)) and Nichol and Schulman ([2018](#bib.bib67))
    propose methods that can effectively lead the policy search to find parameters
    that can be adapted to give good performance on a new task with only a small number
    of training examples of the new task. Such formulations could be very beneficial,
    and have the potential to learn universal and robust policies.'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 元学习：Finn 等（[2017a](#bib.bib21)）和 Nichol 与 Schulman（[2018](#bib.bib67)）提出了可以有效引导策略搜索以找到可以适应新任务并在仅有少量新任务训练样本的情况下获得良好性能的参数的方法。这些公式可能非常有益，并具有学习通用且稳健的策略的潜力。
- en: V Conclusion
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: In this paper, we give a brief overview of deep learning solutions for robotics
    control tasks, focusing mainly on deep reinforcement learning and imitation learning
    algorithms. We mainly introduce the formulations for each learning paradigm and
    the corresponding representative works in robotics. Finally, We discuss the challenges
    and potential future research directions.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们简要概述了机器人控制任务的深度学习解决方案，主要关注深度强化学习和模仿学习算法。我们主要介绍了每种学习范式的公式以及机器人领域的相应代表性工作。最后，我们讨论了挑战和潜在的未来研究方向。
- en: References
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Andrychowicz et al. (2017) Andrychowicz M, Crow D, Ray A, Schneider J, Fong
    R, Welinder P, McGrew B, Tobin J, Abbeel OP and Zaremba W (2017) Hindsight experience
    replay. In: *Advances in Neural Information Processing Systems*. pp. 5055–5065.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrychowicz 等 (2017) Andrychowicz M, Crow D, Ray A, Schneider J, Fong R, Welinder
    P, McGrew B, Tobin J, Abbeel OP 和 Zaremba W (2017) 事后经验重放。在：*神经信息处理系统进展*。第 5055–5065
    页。
- en: Arjovsky et al. (2017) Arjovsky M, Chintala S and Bottou L (2017) Wasserstein
    gan. *arXiv preprint arXiv:1701.07875* .
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky 等 (2017) Arjovsky M, Chintala S 和 Bottou L (2017) Wasserstein gan。*arXiv
    预印本 arXiv:1701.07875*。
- en: Bagnell (2015) Bagnell JA (2015) An invitation to imitation. Technical report,
    CARNEGIE-MELLON UNIV PITTSBURGH PA ROBOTICS INST.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagnell (2015) Bagnell JA (2015) 邀请模仿。技术报告，卡内基梅隆大学匹兹堡 PA 机器人研究所。
- en: Baram et al. (2016) Baram N, Anschel O and Mannor S (2016) Model-based adversarial
    imitation learning. *arXiv preprint arXiv:1612.02179* .
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baram 等人 (2016) Baram N, Anschel O 和 Mannor S (2016) 基于模型的对抗模仿学习。*arXiv 预印本
    arXiv:1612.02179*。
- en: 'Barreto et al. (2017) Barreto A, Dabney W, Munos R, Hunt JJ, Schaul T, Silver
    D and van Hasselt HP (2017) Successor features for transfer in reinforcement learning.
    In: *Advances in Neural Information Processing Systems*. pp. 4058–4068.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barreto 等人 (2017) Barreto A, Dabney W, Munos R, Hunt JJ, Schaul T, Silver D
    和 van Hasselt HP (2017) 强化学习中的转移继承特征。在：*神经信息处理系统进展*。第4058–4068页。
- en: 'Bengio et al. (2009) Bengio Y, Louradour J, Collobert R and Weston J (2009)
    Curriculum learning. In: *Proceedings of the 26th annual international conference
    on machine learning*. ACM, pp. 41–48.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等人 (2009) Bengio Y, Louradour J, Collobert R 和 Weston J (2009) 课程学习。在：*第26届年度国际机器学习会议论文集*。ACM，第41–48页。
- en: Bojarski et al. (2016) Bojarski M, Del Testa D, Dworakowski D, Firner B, Flepp
    B, Goyal P, Jackel LD, Monfort M, Muller U, Zhang J et al. (2016) End to end learning
    for self-driving cars. *arXiv preprint arXiv:1604.07316* .
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bojarski 等人 (2016) Bojarski M, Del Testa D, Dworakowski D, Firner B, Flepp B,
    Goyal P, Jackel LD, Monfort M, Muller U, Zhang J 等人 (2016) 自驾车的端到端学习。*arXiv 预印本
    arXiv:1604.07316*。
- en: Bousmalis et al. (2017) Bousmalis K, Irpan A, Wohlhart P, Bai Y, Kelcey M, Kalakrishnan
    M, Downs L, Ibarz J, Pastor P, Konolige K et al. (2017) Using simulation and domain
    adaptation to improve efficiency of deep robotic grasping. *arXiv preprint arXiv:1709.07857*
    .
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bousmalis 等人 (2017) Bousmalis K, Irpan A, Wohlhart P, Bai Y, Kelcey M, Kalakrishnan
    M, Downs L, Ibarz J, Pastor P, Konolige K 等人 (2017) 使用仿真和领域适应提高深度机器人抓取的效率。*arXiv
    预印本 arXiv:1709.07857*。
- en: Bruce et al. (2017) Bruce J, Sünderhauf N, Mirowski P, Hadsell R and Milford
    M (2017) One-shot reinforcement learning for robot navigation with interactive
    replay. *arXiv preprint arXiv:1711.10137* .
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bruce 等人 (2017) Bruce J, Sünderhauf N, Mirowski P, Hadsell R 和 Milford M (2017)
    带交互重放的单次强化学习用于机器人导航。*arXiv 预印本 arXiv:1711.10137*。
- en: Chaplot et al. (2018) Chaplot DS, Parisotto E and Salakhutdinov R (2018) Active
    neural localization. *arXiv preprint arXiv:1801.08214* .
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chaplot 等人 (2018) Chaplot DS, Parisotto E 和 Salakhutdinov R (2018) 主动神经定位。*arXiv
    预印本 arXiv:1801.08214*。
- en: 'Chen et al. (2016) Chen LC, Papandreou G, Kokkinos I, Murphy K and Yuille AL
    (2016) Deeplab: Semantic image segmentation with deep convolutional nets, atrous
    convolution, and fully connected crfs. *arXiv preprint arXiv:1606.00915* .'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2016) Chen LC, Papandreou G, Kokkinos I, Murphy K 和 Yuille AL (2016)
    Deeplab：使用深度卷积网络、扩张卷积和全连接条件随机场进行语义图像分割。*arXiv 预印本 arXiv:1606.00915*。
- en: Chen et al. (2017a) Chen YF, Everett M, Liu M and How JP (2017a) Socially aware
    motion planning with deep reinforcement learning. *arXiv preprint arXiv:1703.08862*
    .
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2017a) Chen YF, Everett M, Liu M 和 How JP (2017a) 使用深度强化学习进行社会感知的运动规划。*arXiv
    预印本 arXiv:1703.08862*。
- en: 'Chen et al. (2017b) Chen YF, Liu M, Everett M and How JP (2017b) Decentralized
    non-communicating multiagent collision avoidance with deep reinforcement learning.
    In: *Robotics and Automation (ICRA), 2017 IEEE International Conference on*. IEEE,
    pp. 285–292.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2017b) Chen YF, Liu M, Everett M 和 How JP (2017b) 去中心化非通信多智能体碰撞避免与深度强化学习。在：*机器人与自动化
    (ICRA)，2017 年 IEEE 国际会议*。IEEE，第285–292页。
- en: Codevilla et al. (2017) Codevilla F, Müller M, Dosovitskiy A, López A and Koltun
    V (2017) End-to-end driving via conditional imitation learning. *arXiv preprint
    arXiv:1710.02410* .
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Codevilla 等人 (2017) Codevilla F, Müller M, Dosovitskiy A, López A 和 Koltun V
    (2017) 通过条件模仿学习实现端到端驾驶。*arXiv 预印本 arXiv:1710.02410*。
- en: 'Dayan (1993) Dayan P (1993) Improving generalization for temporal difference
    learning: The successor representation. *Neural Computation* 5(4): 613–624.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dayan (1993) Dayan P (1993) 改进时间差分学习的泛化能力：继承表示。*神经计算* 5(4)：613–624。
- en: 'Deisenroth et al. (2013) Deisenroth MP, Neumann G, Peters J et al. (2013) A
    survey on policy search for robotics. *Foundations and Trends® in Robotics* 2(1–2):
    1–142.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deisenroth 等人 (2013) Deisenroth MP, Neumann G, Peters J 等人 (2013) 关于机器人策略搜索的综述。*机器人学基础与趋势®*
    2(1–2)：1–142。
- en: Deng (2014) Deng L (2014) A tutorial survey of architectures, algorithms, and
    applications for deep learning. *APSIPA Transactions on Signal and Information
    Processing* 3.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng (2014) Deng L (2014) 深度学习架构、算法和应用的教程调查。*APSIPA 信号与信息处理学报* 3。
- en: 'Dosovitskiy et al. (2017) Dosovitskiy A, Ros G, Codevilla F, López A and Koltun
    V (2017) Carla: An open urban driving simulator. *arXiv preprint arXiv:1711.03938*
    .'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等人 (2017) Dosovitskiy A, Ros G, Codevilla F, López A 和 Koltun V
    (2017) Carla：一个开放的城市驾驶模拟器。*arXiv 预印本 arXiv:1711.03938*。
- en: 'Duan et al. (2017) Duan Y, Andrychowicz M, Stadie B, Ho OJ, Schneider J, Sutskever
    I, Abbeel P and Zaremba W (2017) One-shot imitation learning. In: *Advances in
    neural information processing systems*. pp. 1087–1098.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan 等人（2017）Duan Y, Andrychowicz M, Stadie B, Ho OJ, Schneider J, Sutskever
    I, Abbeel P 和 Zaremba W（2017）《一次性模仿学习》。在：*神经信息处理系统进展*。第1087–1098页。
- en: 'Eitel et al. (2017) Eitel A, Hauff N and Burgard W (2017) Learning to singulate
    objects using a push proposal network. In: *Proc. of the International Symposium
    on Robotics Research (ISRR)*. Puerto Varas, Chile.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eitel 等人（2017）Eitel A, Hauff N 和 Burgard W（2017）《使用推送提案网络进行对象单化学习》。在：*国际机器人研究研讨会（ISRR）*。智利，普托瓦尔斯。
- en: Finn et al. (2017a) Finn C, Abbeel P and Levine S (2017a) Model-agnostic meta-learning
    for fast adaptation of deep networks. *arXiv preprint arXiv:1703.03400* .
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn 等人（2017a）Finn C, Abbeel P 和 Levine S（2017a）《模型无关的元学习用于深度网络的快速适应》。*arXiv
    预印本 arXiv:1703.03400*。
- en: 'Finn et al. (2016) Finn C, Tan XY, Duan Y, Darrell T, Levine S and Abbeel P
    (2016) Deep spatial autoencoders for visuomotor learning. In: *Robotics and Automation
    (ICRA), 2016 IEEE International Conference on*. IEEE, pp. 512–519.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn 等人（2016）Finn C, Tan XY, Duan Y, Darrell T, Levine S 和 Abbeel P（2016）《用于视运动学习的深度空间自编码器》。在：*机器人与自动化（ICRA），2016
    IEEE国际会议*。IEEE，第512–519页。
- en: Finn et al. (2017b) Finn C, Yu T, Zhang T, Abbeel P and Levine S (2017b) One-shot
    visual imitation learning via meta-learning. *arXiv preprint arXiv:1709.04905*
    .
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn 等人（2017b）Finn C, Yu T, Zhang T, Abbeel P 和 Levine S（2017b）《通过元学习进行一次性视觉模仿学习》。*arXiv
    预印本 arXiv:1709.04905*。
- en: Florensa et al. (2017) Florensa C, Held D, Wulfmeier M and Abbeel P (2017) Reverse
    curriculum generation for reinforcement learning. *arXiv preprint arXiv:1707.05300*
    .
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Florensa 等人（2017）Florensa C, Held D, Wulfmeier M 和 Abbeel P（2017）《强化学习的逆向课程生成》。*arXiv
    预印本 arXiv:1707.05300*。
- en: 'Fortunato et al. (2018) Fortunato M, Azar MG, Piot B, Menick J, Osband I, Graves
    A, Mnih V, Munos R, Hassabis D, Pietquin O et al. (2018) Noisy networks for exploration.
    In: *International Conference on Learning Representations*. URL https://openreview.net/forum?id=rywHCPkAW.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fortunato 等人（2018）Fortunato M, Azar MG, Piot B, Menick J, Osband I, Graves A,
    Mnih V, Munos R, Hassabis D, Pietquin O 等人（2018）《用于探索的噪声网络》。在：*国际学习表征会议*。网址 https://openreview.net/forum?id=rywHCPkAW。
- en: 'Fu et al. (2016) Fu J, Levine S and Abbeel P (2016) One-shot learning of manipulation
    skills with online dynamics adaptation and neural network priors. In: *Intelligent
    Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on*. IEEE, pp.
    4019–4026.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等人（2016）Fu J, Levine S 和 Abbeel P（2016）《通过在线动态适应和神经网络先验进行一次性操控技能学习》。在：*智能机器人与系统（IROS），2016
    IEEE/RSJ国际会议*。IEEE，第4019–4026页。
- en: 'Fu et al. (2005) Fu MC, Glover FW and April J (2005) Simulation optimization:
    a review, new developments, and applications. In: *Proceedings of the 37th conference
    on Winter simulation*. Winter Simulation Conference, pp. 83–95.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等人（2005）Fu MC, Glover FW 和 April J（2005）《模拟优化：综述、新发展与应用》。在：*第37届冬季模拟会议论文集*。冬季模拟会议，第83–95页。
- en: Gao et al. (2018) Gao Y, Xu HH, Lin J, Yu F, Levine S and Darrell T (2018) Reinforcement
    learning from imperfect demonstrations .
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人（2018）Gao Y, Xu HH, Lin J, Yu F, Levine S 和 Darrell T（2018）《从不完美示范中进行强化学习》。
- en: 'Giusti et al. (2016) Giusti A, Guzzi J, Cireşan DC, He FL, Rodríguez JP, Fontana
    F, Faessler M, Forster C, Schmidhuber J, Di Caro G et al. (2016) A machine learning
    approach to visual perception of forest trails for mobile robots. *IEEE Robotics
    and Automation Letters* 1(2): 661–667.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Giusti 等人（2016）Giusti A, Guzzi J, Cireşan DC, He FL, Rodríguez JP, Fontana
    F, Faessler M, Forster C, Schmidhuber J, Di Caro G 等人（2016）《一种机器学习方法用于移动机器人对森林小径的视觉感知》。*IEEE机器人与自动化快报*
    1(2): 661–667。'
- en: Goodfellow et al. (2016) Goodfellow I, Bengio Y and Courville A (2016) *Deep
    Learning*. MIT Press. http://www.deeplearningbook.org.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等人（2016）Goodfellow I, Bengio Y 和 Courville A（2016）*《深度学习》*。MIT出版社。http://www.deeplearningbook.org。
- en: 'Goodfellow et al. (2014) Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley
    D, Ozair S, Courville A and Bengio Y (2014) Generative adversarial nets. In: *Advances
    in neural information processing systems*. pp. 2672–2680.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等人（2014）Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley
    D, Ozair S, Courville A 和 Bengio Y（2014）《生成对抗网络》。在：*神经信息处理系统进展*。第2672–2680页。
- en: 'Gu et al. (2017) Gu S, Holly E, Lillicrap T and Levine S (2017) Deep reinforcement
    learning for robotic manipulation with asynchronous off-policy updates. In: *Robotics
    and Automation (ICRA), 2017 IEEE International Conference on*. IEEE, pp. 3389–3396.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等人（2017）Gu S, Holly E, Lillicrap T 和 Levine S（2017）《用于机器人操控的深度强化学习与异步离策略更新》。在：*机器人与自动化（ICRA），2017
    IEEE国际会议*。IEEE，第3389–3396页。
- en: 'Gu et al. (2016) Gu S, Lillicrap T, Sutskever I and Levine S (2016) Continuous
    deep q-learning with model-based acceleration. In: *International Conference on
    Machine Learning (ICML-16)*.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等人（2016）Gu S, Lillicrap T, Sutskever I 和 Levine S（2016）基于模型加速的连续深度 Q 学习。载于：*国际机器学习会议（ICML-16）*。
- en: 'Guo et al. (2016) Guo Y, Liu Y, Oerlemans A, Lao S, Wu S and Lew MS (2016)
    Deep learning for visual understanding: A review. *Neurocomputing* 187: 27–48.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等人（2016）Guo Y, Liu Y, Oerlemans A, Lao S, Wu S 和 Lew MS（2016）视觉理解的深度学习：综述。*神经计算*
    187: 27–48。'
- en: 'Gupta et al. (2016) Gupta A, Eppner C, Levine S and Abbeel P (2016) Learning
    dexterous manipulation for a soft robotic hand from human demonstrations. In:
    *Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference
    on*. IEEE, pp. 3786–3793.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等人（2016）Gupta A, Eppner C, Levine S 和 Abbeel P（2016）从人类示范中学习柔性机器人手的灵巧操作。载于：*智能机器人与系统（IROS），2016
    IEEE/RSJ 国际会议*。IEEE，第 3786–3793 页。
- en: Gupta et al. (2017a) Gupta S, Davidson J, Levine S, Sukthankar R and Malik J
    (2017a) Cognitive mapping and planning for visual navigation. *arXiv preprint
    arXiv:1702.03920* .
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等人（2017a）Gupta S, Davidson J, Levine S, Sukthankar R 和 Malik J（2017a）认知地图与视觉导航规划。*arXiv
    预印本 arXiv:1702.03920*。
- en: Gupta et al. (2017b) Gupta S, Fouhey D, Levine S and Malik J (2017b) Unifying
    map and landmark based representations for visual navigation. *arXiv preprint
    arXiv:1712.08125* .
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等人（2017b）Gupta S, Fouhey D, Levine S 和 Malik J（2017b）统一地图和基于地标的视觉导航表示。*arXiv
    预印本 arXiv:1712.08125*。
- en: 'He et al. (2016) He K, Zhang X, Ren S and Sun J (2016) Deep residual learning
    for image recognition. In: *Proceedings of the IEEE conference on computer vision
    and pattern recognition*. pp. 770–778.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2016）He K, Zhang X, Ren S 和 Sun J（2016）用于图像识别的深度残差学习。载于：*IEEE 计算机视觉与模式识别会议论文集*。第
    770–778 页。
- en: Heess et al. (2017) Heess N, Sriram S, Lemmon J, Merel J, Wayne G, Tassa Y,
    Erez T, Wang Z, Eslami A, Riedmiller M et al. (2017) Emergence of locomotion behaviours
    in rich environments. *arXiv preprint arXiv:1707.02286* .
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heess 等人（2017）Heess N, Sriram S, Lemmon J, Merel J, Wayne G, Tassa Y, Erez T,
    Wang Z, Eslami A, Riedmiller M 等人（2017）在丰富环境中运动行为的出现。*arXiv 预印本 arXiv:1707.02286*。
- en: Henderson et al. (2017) Henderson P, Islam R, Bachman P, Pineau J, Precup D
    and Meger D (2017) Deep reinforcement learning that matters. *arXiv preprint arXiv:1709.06560*
    .
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson 等人（2017）Henderson P, Islam R, Bachman P, Pineau J, Precup D 和 Meger
    D（2017）*深度强化学习中的重要性*。*arXiv 预印本 arXiv:1709.06560*。
- en: 'Ho and Ermon (2016) Ho J and Ermon S (2016) Generative adversarial imitation
    learning. In: *Advances in Neural Information Processing Systems*. pp. 4565–4573.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 和 Ermon（2016）Ho J 和 Ermon S（2016）生成对抗模仿学习。载于：*神经信息处理系统进展*。第 4565–4573 页。
- en: 'Hoffman et al. (2017) Hoffman J, Tzeng E, Park T, Zhu JY, Isola P, Saenko K,
    Efros AA and Darrell T (2017) Cycada: Cycle-consistent adversarial domain adaptation.
    *arXiv preprint arXiv:1711.03213* .'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffman 等人（2017）Hoffman J, Tzeng E, Park T, Zhu JY, Isola P, Saenko K, Efros
    AA 和 Darrell T（2017）Cycada：周期一致的对抗性领域适应。*arXiv 预印本 arXiv:1711.03213*。
- en: 'Huang et al. (2017) Huang G, Liu Z, Weinberger KQ and van der Maaten L (2017)
    Densely connected convolutional networks. In: *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, volume 1\. p. 3.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2017）Huang G, Liu Z, Weinberger KQ 和 van der Maaten L（2017）密集连接的卷积网络。载于：*IEEE
    计算机视觉与模式识别会议论文集*，第 1 卷。第 3 页。
- en: Jaderberg et al. (2016) Jaderberg M, Mnih V, Czarnecki WM, Schaul T, Leibo JZ,
    Silver D and Kavukcuoglu K (2016) Reinforcement learning with unsupervised auxiliary
    tasks. *arXiv preprint arXiv:1611.05397* .
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg 等人（2016）Jaderberg M, Mnih V, Czarnecki WM, Schaul T, Leibo JZ, Silver
    D 和 Kavukcuoglu K（2016）带有无监督辅助任务的强化学习。*arXiv 预印本 arXiv:1611.05397*。
- en: 'Kakade and Langford (2002) Kakade S and Langford J (2002) Approximately optimal
    approximate reinforcement learning. In: *ICML*, volume 2\. pp. 267–274.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kakade 和 Langford（2002）Kakade S 和 Langford J（2002）近似最优的近似强化学习。载于：*ICML*，第 2
    卷。第 267–274 页。
- en: 'Kalweit and Boedecker (2017) Kalweit G and Boedecker J (2017) Uncertainty-driven
    imagination for continuous deep reinforcement learning. In: Levine S, Vanhoucke
    V and Goldberg K (eds.) *Proceedings of the 1st Annual Conference on Robot Learning*,
    *Proceedings of Machine Learning Research*, volume 78\. PMLR, pp. 195–206. URL
    http://proceedings.mlr.press/v78/kalweit17a.html.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalweit 和 Boedecker（2017）Kalweit G 和 Boedecker J（2017）基于不确定性的连续深度强化学习想象。载于：Levine
    S, Vanhoucke V 和 Goldberg K（编辑）*第 1 届年度机器人学习会议论文集*，*机器学习研究论文集*，第 78 卷。PMLR，第 195–206
    页。网址 [http://proceedings.mlr.press/v78/kalweit17a.html](http://proceedings.mlr.press/v78/kalweit17a.html)。
- en: 'Kanitscheider and Fiete (2017) Kanitscheider I and Fiete I (2017) Training
    recurrent networks to generate hypotheses about how the brain solves hard navigation
    problems. In: *Advances in Neural Information Processing Systems*. pp. 4532–4541.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanitscheider 和 Fiete (2017) Kanitscheider I 和 Fiete I (2017) 训练递归网络生成关于大脑如何解决复杂导航问题的假设。见于：*神经信息处理系统进展*。第4532–4541页。
- en: Khan et al. (2017) Khan A, Zhang C, Atanasov N, Karydis K, Kumar V and Lee DD
    (2017) Memory augmented control networks. *arXiv preprint arXiv:1709.05706* .
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan 等 (2017) Khan A, Zhang C, Atanasov N, Karydis K, Kumar V 和 Lee DD (2017)
    记忆增强控制网络。*arXiv 预印本 arXiv:1709.05706*。
- en: 'Koenig et al. (2004) Koenig N, A B and Howard A (2004) Design and use paradigms
    for gazebo, an open-source multi-robot simulator. In: *Intelligent Robots and
    Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference
    on*, volume 3\. IEEE, pp. 2149–2154.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koenig 等 (2004) Koenig N, A B 和 Howard A (2004) Gazebo的设计和使用范式，一个开源多机器人模拟器。见于：*智能机器人与系统，2004年（IROS
    2004）。2004 IEEE/RSJ 国际会议论文集*，第3卷。IEEE，第2149–2154页。
- en: 'Kolve et al. (2017) Kolve E, Mottaghi R, Gordon D, Zhu Y, Gupta A and Farhadi
    A (2017) Ai2-thor: An interactive 3d environment for visual ai. *arXiv preprint
    arXiv:1712.05474* .'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolve 等 (2017) Kolve E, Mottaghi R, Gordon D, Zhu Y, Gupta A 和 Farhadi A (2017)
    Ai2-thor：一个用于视觉AI的交互式3D环境。*arXiv 预印本 arXiv:1712.05474*。
- en: 'Kretzschmar et al. (2016) Kretzschmar H, Spies M, Sprunk C and Burgard W (2016)
    Socially compliant mobile robot navigation via inverse reinforcement learning.
    *The International Journal of Robotics Research* 35(11): 1289–1307.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kretzschmar 等 (2016) Kretzschmar H, Spies M, Sprunk C 和 Burgard W (2016) 通过逆向强化学习的社会适应型移动机器人导航。*国际机器人研究期刊*
    35(11): 1289–1307。'
- en: 'Krizhevsky et al. (2012) Krizhevsky A, Sutskever I and Hinton GE (2012) Imagenet
    classification with deep convolutional neural networks. In: *Advances in neural
    information processing systems*. pp. 1097–1105.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等 (2012) Krizhevsky A, Sutskever I 和 Hinton GE (2012) 使用深度卷积神经网络的Imagenet分类。见于：*神经信息处理系统进展*。第1097–1105页。
- en: Kulkarni et al. (2016) Kulkarni TD, Saeedi A, Gautam S and Gershman SJ (2016)
    Deep successor reinforcement learning. *arXiv preprint arXiv:1606.02396* .
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulkarni 等 (2016) Kulkarni TD, Saeedi A, Gautam S 和 Gershman SJ (2016) 深度继任者强化学习。*arXiv
    预印本 arXiv:1606.02396*。
- en: 'Kumar et al. (2016) Kumar V, Todorov E and Levine S (2016) Optimal control
    with learned local models: Application to dexterous manipulation. In: *Robotics
    and Automation (ICRA), 2016 IEEE International Conference on*. IEEE, pp. 378–383.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等 (2016) Kumar V, Todorov E 和 Levine S (2016) 使用学习到的局部模型的最优控制：应用于灵巧操控。见于：*2016
    IEEE 国际机器人与自动化会议 (ICRA)*。IEEE，第378–383页。
- en: 'Kümmerle et al. (2011) Kümmerle R, Grisetti G, Strasdat H, Konolige K and Burgard
    W (2011) g 2 o: A general framework for graph optimization. In: *Robotics and
    Automation (ICRA), 2011 IEEE International Conference on*. IEEE, pp. 3607–3613.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kümmerle 等 (2011) Kümmerle R, Grisetti G, Strasdat H, Konolige K 和 Burgard
    W (2011) g 2 o: 一种通用图优化框架。见于：*2011 IEEE 国际机器人与自动化会议 (ICRA)*。IEEE，第3607–3613页。'
- en: 'Levine et al. (2016) Levine S, Finn C, Darrell T and Abbeel P (2016) End-to-end
    training of deep visuomotor policies. *The Journal of Machine Learning Research*
    17(1): 1334–1373.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Levine 等 (2016) Levine S, Finn C, Darrell T 和 Abbeel P (2016) 深度视觉运动策略的端到端训练。*机器学习研究期刊*
    17(1): 1334–1373。'
- en: 'Levine and Koltun (2013) Levine S and Koltun V (2013) Guided policy search.
    In: *ICML (3)*. pp. 1–9.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levine 和 Koltun (2013) Levine S 和 Koltun V (2013) 指导策略搜索。见于：*ICML (3)*。第1–9页。
- en: Li et al. (2017) Li Y, Song J and Ermon S (2017) Inferring the latent structure
    of human decision-making from raw visual inputs. *arXiv preprint arXiv:1703.08840*
    .
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2017) Li Y, Song J 和 Ermon S (2017) 从原始视觉输入推断人类决策的潜在结构。*arXiv 预印本 arXiv:1703.08840*。
- en: 'Lillicrap et al. (2015) Lillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T,
    Tassa Y, Silver D and Wierstra D (2015) Continuous control with deep reinforcement
    learning. In: *Proceedings of the International Conference on Learning Representations
    (ICLR)*.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lillicrap 等 (2015) Lillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T, Tassa
    Y, Silver D 和 Wierstra D (2015) 使用深度强化学习的连续控制。见于：*国际学习表征会议 (ICLR) 会议论文集*。
- en: 'Long et al. (2015) Long J, Shelhamer E and Darrell T (2015) Fully convolutional
    networks for semantic segmentation. In: *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*. pp. 3431–3440.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long 等 (2015) Long J, Shelhamer E 和 Darrell T (2015) 用于语义分割的全卷积网络。见于：*IEEE 计算机视觉与模式识别会议论文集*。第3431–3440页。
- en: Long et al. (2017) Long P, Fan T, Liao X, Liu W, Zhang H and Pan J (2017) Towards
    optimally decentralized multi-robot collision avoidance via deep reinforcement
    learning. *arXiv preprint arXiv:1709.10082* .
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long 等（2017）Long P, Fan T, Liao X, Liu W, Zhang H 和 Pan J（2017）通过深度强化学习实现最优去中心化的多机器人碰撞避免。*arXiv
    预印本 arXiv:1709.10082*。
- en: 'Maddern et al. (2017) Maddern W, Pascoe G, Linegar C and Newman P (2017) 1
    Year, 1000km: The Oxford RobotCar Dataset. *The International Journal of Robotics
    Research (IJRR)* 36(1): 3–15. DOI:10.1177/0278364916679498. URL http://dx.doi.org/10.1177/0278364916679498.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Maddern 等（2017）Maddern W, Pascoe G, Linegar C 和 Newman P（2017）1 年，1000 公里：牛津
    RobotCar 数据集。*The International Journal of Robotics Research (IJRR)* 36(1): 3–15。DOI:10.1177/0278364916679498。网址
    http://dx.doi.org/10.1177/0278364916679498。'
- en: Mirowski et al. (2016) Mirowski P, Pascanu R, Viola F, Soyer H, Ballard A, Banino
    A, Denil M, Goroshin R, Sifre L, Kavukcuoglu K et al. (2016) Learning to navigate
    in complex environments. *arXiv preprint arXiv:1611.03673* .
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirowski 等（2016）Mirowski P, Pascanu R, Viola F, Soyer H, Ballard A, Banino A,
    Denil M, Goroshin R, Sifre L, Kavukcuoglu K 等（2016）在复杂环境中学习导航。*arXiv 预印本 arXiv:1611.03673*。
- en: Mnih et al. (2016) Mnih V, Badia AP, Mirza M, Graves A, Lillicrap TP, Harley
    T, Silver D and Kavukcuoglu K (2016) Asynchronous methods for deep reinforcement
    learning. *arXiv preprint arXiv:1602.01783* .
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等（2016）Mnih V, Badia AP, Mirza M, Graves A, Lillicrap TP, Harley T, Silver
    D 和 Kavukcuoglu K（2016）深度强化学习的异步方法。*arXiv 预印本 arXiv:1602.01783*。
- en: 'Mnih et al. (2015) Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare
    MG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski G et al. (2015) Human-level
    control through deep reinforcement learning. *Nature* 518(7540): 529–533.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mnih 等（2015）Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG,
    Graves A, Riedmiller M, Fidjeland AK, Ostrovski G 等（2015）通过深度强化学习实现人类水平的控制。*Nature*
    518(7540): 529–533。'
- en: Nair et al. (2017) Nair A, McGrew B, Andrychowicz M, Zaremba W and Abbeel P
    (2017) Overcoming exploration in reinforcement learning with demonstrations. *arXiv
    preprint arXiv:1709.10089* .
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair 等（2017）Nair A, McGrew B, Andrychowicz M, Zaremba W 和 Abbeel P（2017）通过演示克服强化学习中的探索问题。*arXiv
    预印本 arXiv:1709.10089*。
- en: 'Nichol and Schulman (2018) Nichol A and Schulman J (2018) Reptile: a scalable
    metalearning algorithm. *arXiv preprint arXiv:1803.02999* .'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nichol 和 Schulman（2018）Nichol A 和 Schulman J（2018）Reptile：一种可扩展的元学习算法。*arXiv
    预印本 arXiv:1803.02999*。
- en: 'Okal and Arras (2016) Okal B and Arras KO (2016) Learning socially normative
    robot navigation behaviors with bayesian inverse reinforcement learning. In: *ICRA*.
    pp. 2889–2895.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Okal 和 Arras（2016）Okal B 和 Arras KO（2016）使用贝叶斯逆强化学习学习社会规范的机器人导航行为。在：*ICRA*。第
    2889–2895 页。
- en: Parisotto et al. (2018) Parisotto E, Chaplot DS, Zhang J and Salakhutdinov R
    (2018) Global pose estimation with an attention-based recurrent network. *arXiv
    preprint arXiv:1802.06857* .
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parisotto 等（2018）Parisotto E, Chaplot DS, Zhang J 和 Salakhutdinov R（2018）基于注意力的递归网络进行全局姿态估计。*arXiv
    预印本 arXiv:1802.06857*。
- en: 'Parisotto and Salakhutdinov (2017) Parisotto E and Salakhutdinov R (2017) Neural
    map: Structured memory for deep reinforcement learning. *arXiv preprint arXiv:1702.08360*
    .'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parisotto 和 Salakhutdinov（2017）Parisotto E 和 Salakhutdinov R（2017）神经地图：深度强化学习的结构化记忆。*arXiv
    预印本 arXiv:1702.08360*。
- en: 'Pathak et al. (2017) Pathak D, Agrawal P, Efros AA and Darrell T (2017) Curiosity-driven
    exploration by self-supervised prediction. In: *International Conference on Machine
    Learning (ICML)*, volume 2017.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pathak 等（2017）Pathak D, Agrawal P, Efros AA 和 Darrell T（2017）通过自监督预测驱动的探索。在：*International
    Conference on Machine Learning (ICML)*，卷 2017。
- en: Peng et al. (2017) Peng XB, Andrychowicz M, Zaremba W and Abbeel P (2017) Sim-to-real
    transfer of robotic control with dynamics randomization. *arXiv preprint arXiv:1710.06537*
    .
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等（2017）Peng XB, Andrychowicz M, Zaremba W 和 Abbeel P（2017）通过动态随机化实现机器人控制的从模拟到现实的转移。*arXiv
    预印本 arXiv:1710.06537*。
- en: 'Pfeiffer et al. (2016) Pfeiffer M, Schwesinger U, Sommer H, Galceran E and
    Siegwart R (2016) Predicting actions to act predictably: Cooperative partial motion
    planning with maximum entropy models. In: *2016 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*. pp. 2096–2101.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pfeiffer 等（2016）Pfeiffer M, Schwesinger U, Sommer H, Galceran E 和 Siegwart R（2016）预测行为以便可预测：利用最大熵模型进行协作性部分运动规划。在：*2016
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*。第
    2096–2101 页。
- en: 'Plappert et al. (2018) Plappert M, Houthooft R, Dhariwal P, Sidor S, Chen RY,
    Chen X, Asfour T, Abbeel P and Andrychowicz M (2018) Parameter space noise for
    exploration. In: *International Conference on Learning Representations*. URL https://openreview.net/forum?id=ByBAl2eAZ.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Plappert 等（2018）Plappert M, Houthooft R, Dhariwal P, Sidor S, Chen RY, Chen
    X, Asfour T, Abbeel P 和 Andrychowicz M（2018）用于探索的参数空间噪声。在：*International Conference
    on Learning Representations*。网址 https://openreview.net/forum?id=ByBAl2eAZ。
- en: Popov et al. (2017) Popov I, Heess N, Lillicrap T, Hafner R, Barth-Maron G,
    Vecerik M, Lampe T, Tassa Y, Erez T and Riedmiller M (2017) Data-efficient deep
    reinforcement learning for dexterous manipulation. *arXiv preprint arXiv:1704.03073*
    .
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Popov等（2017）Popov I, Heess N, Lillicrap T, Hafner R, Barth-Maron G, Vecerik
    M, Lampe T, Tassa Y, Erez T和Riedmiller M（2017）数据高效的深度强化学习用于灵巧操作。*arXiv预印本 arXiv:1704.03073*。
- en: Radford et al. (2015) Radford A, Metz L and Chintala S (2015) Unsupervised representation
    learning with deep convolutional generative adversarial networks. *arXiv preprint
    arXiv:1511.06434* .
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等（2015）Radford A, Metz L和Chintala S（2015）使用深度卷积生成对抗网络进行无监督表示学习。*arXiv预印本
    arXiv:1511.06434*。
- en: Riedmiller et al. (2018) Riedmiller M, Hafner R, Lampe T, Neunert M, Degrave
    J, Van de Wiele V Tom adn Mnih, Heess N and Springenberg JT (2018) Learning by
    playing - solving sparse reward tasks from scratch. *arXiv preprint arXiv:1802.10567*
    .
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riedmiller等（2018）Riedmiller M, Hafner R, Lampe T, Neunert M, Degrave J, Van
    de Wiele V Tom和Mnih, Heess N和Springenberg JT（2018）通过玩游戏学习 - 从头解决稀疏奖励任务。*arXiv预印本
    arXiv:1802.10567*。
- en: 'Rohmer et al. (2013) Rohmer E, Singh SP and Freese M (2013) V-rep: A versatile
    and scalable robot simulation framework. In: *Intelligent Robots and Systems (IROS),
    2013 IEEE/RSJ International Conference on*. IEEE, pp. 1321–1326.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rohmer等（2013）Rohmer E, Singh SP和Freese M（2013）V-rep：一个多功能且可扩展的机器人仿真框架。在：*智能机器人与系统（IROS），2013
    IEEE/RSJ国际会议*。IEEE，第1321–1326页。
- en: 'Ross et al. (2011) Ross S, Gordon G and Bagnell D (2011) A reduction of imitation
    learning and structured prediction to no-regret online learning. In: *Proceedings
    of the fourteenth international conference on artificial intelligence and statistics*.
    pp. 627–635.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross等（2011）Ross S, Gordon G和Bagnell D（2011）将模仿学习和结构化预测简化为无悔在线学习。在：*第十四届人工智能与统计国际会议论文集*。第627–635页。
- en: Ruder et al. (2017) Ruder M, Dosovitskiy A and Brox T (2017) Artistic style
    transfer for videos and spherical images. *arXiv preprint arXiv:1708.04538* .
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruder等（2017）Ruder M, Dosovitskiy A和Brox T（2017）视频和球面图像的艺术风格迁移。*arXiv预印本 arXiv:1708.04538*。
- en: 'Rusu et al. (2017) Rusu AA, Večerík M, Rothörl T, Heess N, Pascanu R and Hadsell
    R (2017) Sim-to-real robot learning from pixels with progressive nets. In: *Conference
    on Robot Learning*. pp. 262–270.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rusu等（2017）Rusu AA, Večerík M, Rothörl T, Heess N, Pascanu R和Hadsell R（2017）从像素到真实的机器人学习与渐进网络。在：*机器人学习会议*。第262–270页。
- en: 'Savinov et al. (2018) Savinov N, Dosovitskiy A and Koltun V (2018) Semi-parametric
    topological memory for navigation. In: *International Conference on Learning Representations*.
    URL https://openreview.net/forum?id=SygwwGbRW.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Savinov等（2018）Savinov N, Dosovitskiy A和Koltun V（2018）用于导航的半参数拓扑记忆。在：*学习表征国际会议*。网址
    https://openreview.net/forum?id=SygwwGbRW。
- en: 'Savva et al. (2017) Savva M, Chang AX, Dosovitskiy A, Funkhouser T and Koltun
    V (2017) Minos: Multimodal indoor simulator for navigation in complex environments.
    *arXiv preprint arXiv:1712.03931* .'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Savva等（2017）Savva M, Chang AX, Dosovitskiy A, Funkhouser T和Koltun V（2017）Minos：用于复杂环境导航的多模态室内模拟器。*arXiv预印本
    arXiv:1712.03931*。
- en: 'Schaul et al. (2015a) Schaul T, Horgan D, Gregor K and Silver D (2015a) Universal
    value function approximators. In: *International Conference on Machine Learning*.
    pp. 1312–1320.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schaul等（2015a）Schaul T, Horgan D, Gregor K和Silver D（2015a）通用价值函数近似器。在：*国际机器学习会议*。第1312–1320页。
- en: Schaul et al. (2015b) Schaul T, Quan J, Antonoglou I and Silver D (2015b) Prioritized
    experience replay. *arXiv preprint arXiv:1511.05952* .
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schaul等（2015b）Schaul T, Quan J, Antonoglou I和Silver D（2015b）优先经验回放。*arXiv预印本
    arXiv:1511.05952*。
- en: 'Schmidhuber (2015) Schmidhuber J (2015) Deep learning in neural networks: An
    overview. *Neural networks* 61: 85–117.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schmidhuber（2015）Schmidhuber J（2015）神经网络中的深度学习：概述。*神经网络* 61: 85–117。'
- en: 'Schulman et al. (2015a) Schulman J, Levine S, Abbeel P, Jordan M and Moritz
    P (2015a) Trust region policy optimization. In: *International Conference on Machine
    Learning*. pp. 1889–1897.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman等（2015a）Schulman J, Levine S, Abbeel P, Jordan M和Moritz P（2015a）信任区域策略优化。在：*国际机器学习会议*。第1889–1897页。
- en: Schulman et al. (2015b) Schulman J, Moritz P, Levine S, Jordan M and Abbeel
    P (2015b) High-dimensional continuous control using generalized advantage estimation.
    *arXiv preprint arXiv:1506.02438* .
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman等（2015b）Schulman J, Moritz P, Levine S, Jordan M和Abbeel P（2015b）使用广义优势估计的高维连续控制。*arXiv预印本
    arXiv:1506.02438*。
- en: Schulman et al. (2017) Schulman J, Wolski F, Dhariwal P, Radford A and Klimov
    O (2017) Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*
    .
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman等（2017）Schulman J, Wolski F, Dhariwal P, Radford A和Klimov O（2017）邻近策略优化算法。*arXiv预印本
    arXiv:1707.06347*。
- en: 'Shah et al. (2017) Shah S, Dey D, Lovett C and Kapoor A (2017) Airsim: High-fidelity
    visual and physical simulation for autonomous vehicles. In: *Field and Service
    Robotics*. URL https://arxiv.org/abs/1705.05065.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Silver et al. (2014) Silver D, Lever G, Heess N, Degris T, Wierstra D and Riedmiller
    M (2014) Deterministic policy gradient algorithms. In: *Proceedings of the 31st
    International Conference on Machine Learning (ICML-14)*. pp. 387–395.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stachenfeld et al. (2017) Stachenfeld KL, Botvinick MM and Gershman SJ (2017)
    The hippocampus as a predictive map. *Nature neuroscience* 20(11): 1643.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stadie et al. (2017) Stadie BC, Abbeel P and Sutskever I (2017) Third-person
    imitation learning. *arXiv preprint arXiv:1703.01703* .
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sukhbaatar et al. (2017) Sukhbaatar S, Kostrikov I, Szlam A and Fergus R (2017)
    Intrinsic motivation and automatic curricula via asymmetric self-play. *arXiv
    preprint arXiv:1703.05407* .
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton (1991) Sutton RS (1991) Dyna, an integrated architecture for learning,
    planning, and reacting. *ACM SIGART Bulletin* 2(4): 160–163.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton and Barto (1998) Sutton RS and Barto AG (1998) *Reinforcement learning:
    An introduction*, volume 1. MIT press Cambridge.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Szita and Lörincz (2006) Szita I and Lörincz A (2006) Learning tetris using
    the noisy cross-entropy method. *Neural computation* 18(12): 2936–2941.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tai et al. (2016) Tai L, Li S and Liu M (2016) A deep-network solution towards
    model-less obstacle avoidance. In: *Intelligent Robots and Systems (IROS), 2016
    IEEE/RSJ International Conference on*. IEEE, pp. 2759–2764.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tai et al. (2017) Tai L, Paolo G and Liu M (2017) Virtual-to-real deep reinforcement
    learning: Continuous control of mobile robots for mapless navigation. In: *2017
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*. pp.
    31–36.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tai et al. (2018) Tai L, Zhang J, Liu M and Burgard W (2018) Socially-compliant
    navigation through raw depth inputs with generative adversarial imitation learning.
    In: *Robotics and Automation (ICRA), 2018 IEEE International Conference on*.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tamar et al. (2016) Tamar A, Wu Y, Thomas G, Levine S and Abbeel P (2016) Value
    iteration networks. In: *Advances in Neural Information Processing Systems*. pp.
    2154–2162.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thrun et al. (2005) Thrun S, Burgard W and Fox D (2005) *Probabilistic robotics*.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tobin et al. (2017) Tobin J, Fong R, Ray A, Schneider J, Zaremba W and Abbeel
    P (2017) Domain randomization for transferring deep neural networks from simulation
    to the real world. In: *Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International
    Conference on*. IEEE, pp. 23–30.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Todorov et al. (2012) Todorov E, Erez T and Tassa Y (2012) Mujoco: A physics
    engine for model-based control. In: *Intelligent Robots and Systems (IROS), 2012
    IEEE/RSJ International Conference on*. IEEE, pp. 5026–5033.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tzeng et al. (2015) Tzeng E, Devin C, Hoffman J, Finn C, Abbeel P, Levine S,
    Saenko K and Darrell T (2015) Towards adapting deep visuomotor representations
    from simulated to real environments. *arXiv preprint arXiv:1511.07111* .
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tzeng et al. (2015) Tzeng E, Devin C, Hoffman J, Finn C, Abbeel P, Levine S,
    Saenko K 和 Darrell T (2015) 朝着将深度视觉运动表示从模拟环境适应到真实环境的方向发展。*arXiv 预印本 arXiv:1511.07111*。
- en: 'Tzeng et al. (2014) Tzeng E, Hoffman J, Zhang N, Saenko K and Darrell T (2014)
    Deep domain confusion: Maximizing for domain invariance. *arXiv preprint arXiv:1412.3474*
    .'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tzeng et al. (2014) Tzeng E, Hoffman J, Zhang N, Saenko K 和 Darrell T (2014)
    深度领域混淆：最大化领域不变性。*arXiv 预印本 arXiv:1412.3474*。
- en: 'Van Hasselt et al. (2016) Van Hasselt H, Guez A and Silver D (2016) Deep reinforcement
    learning with double q-learning. In: *AAAI*, volume 16\. pp. 2094–2100.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Hasselt et al. (2016) Van Hasselt H, Guez A 和 Silver D (2016) 使用双重 Q 学习的深度强化学习。见于*AAAI*，第
    16 卷。第 2094–2100 页。
- en: Večerík et al. (2017) Večerík M, Hester T, Scholz J, Wang F, Pietquin O, Piot
    B, Heess N, Rothörl T, Lampe T and Riedmiller M (2017) Leveraging demonstrations
    for deep reinforcement learning on robotics problems with sparse rewards. *arXiv
    preprint arXiv:1707.08817* .
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Večerík et al. (2017) Večerík M, Hester T, Scholz J, Wang F, Pietquin O, Piot
    B, Heess N, Rothörl T, Lampe T 和 Riedmiller M (2017) 利用演示进行深度强化学习，解决机器人问题中的稀疏奖励。*arXiv
    预印本 arXiv:1707.08817*。
- en: Wang et al. (2016a) Wang JX, Kurth-Nelson Z, Tirumala D, Soyer H, Leibo JZ,
    Munos R, Blundell C, Kumaran D and Botvinick M (2016a) Learning to reinforcement
    learn. *arXiv preprint arXiv:1611.05763* .
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2016a) Wang JX, Kurth-Nelson Z, Tirumala D, Soyer H, Leibo JZ,
    Munos R, Blundell C, Kumaran D 和 Botvinick M (2016a) 学习强化学习。*arXiv 预印本 arXiv:1611.05763*。
- en: 'Wang et al. (2016b) Wang Z, de Freitas N and Lanctot M (2016b) Dueling network
    architectures for deep reinforcement learning. In: *Proceedings of The 33rd International
    Conference on Machine Learning*.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2016b) Wang Z, de Freitas N 和 Lanctot M (2016b) 用于深度强化学习的对抗网络架构。见于*第
    33 届国际机器学习大会论文集*。
- en: 'Wang et al. (2017) Wang Z, Merel JS, Reed SE, de Freitas N, Wayne G and Heess
    N (2017) Robust imitation of diverse behaviors. In: *Advances in Neural Information
    Processing Systems*. pp. 5326–5335.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2017) Wang Z, Merel JS, Reed SE, de Freitas N, Wayne G 和 Heess
    N (2017) 可靠模仿多样化行为。见于*神经信息处理系统进展*。第 5326–5335 页。
- en: Weber et al. (2017) Weber T, Racanière S, Reichert DP, Buesing L, Guez A, Rezende
    DJ, Badia AP, Vinyals O, Heess N, Li Y et al. (2017) Imagination-augmented agents
    for deep reinforcement learning. *arXiv preprint arXiv:1707.06203* .
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weber et al. (2017) Weber T, Racanière S, Reichert DP, Buesing L, Guez A, Rezende
    DJ, Badia AP, Vinyals O, Heess N, Li Y 等 (2017) 用于深度强化学习的想象增强代理。*arXiv 预印本 arXiv:1707.06203*。
- en: 'Williams (1992) Williams RJ (1992) Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. In: *Reinforcement Learning*. Springer,
    pp. 5–32.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams (1992) Williams RJ (1992) 简单的统计梯度跟随算法用于连接主义强化学习。见于*强化学习*。Springer，第
    5–32 页。
- en: 'Wu et al. (2017) Wu Y, Mansimov E, Grosse RB, Liao S and Ba J (2017) Scalable
    trust-region method for deep reinforcement learning using kronecker-factored approximation.
    In: *Advances in neural information processing systems*. pp. 5285–5294.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2017) Wu Y, Mansimov E, Grosse RB, Liao S 和 Ba J (2017) 基于 Kronecker
    近似的可扩展信任区域方法用于深度强化学习。见于*神经信息处理系统进展*。第 5285–5294 页。
- en: Wu et al. (2018) Wu Y, Wu Y, Gkioxari G and Tian Y (2018) Building generalizable
    agents with a realistic and rich 3d environment. *arXiv preprint arXiv:1801.02209*
    .
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2018) Wu Y, Wu Y, Gkioxari G 和 Tian Y (2018) 构建具有现实且丰富 3D 环境的可泛化代理。*arXiv
    预印本 arXiv:1801.02209*。
- en: Wulfmeier et al. (2015) Wulfmeier M, Ondruska P and Posner I (2015) Maximum
    entropy deep inverse reinforcement learning. *arXiv preprint arXiv:1507.04888*
    .
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wulfmeier et al. (2015) Wulfmeier M, Ondruska P 和 Posner I (2015) 最大熵深度逆强化学习。*arXiv
    预印本 arXiv:1507.04888*。
- en: Yang et al. (2018) Yang L, Liang X and Xing E (2018) Unsupervised real-to-virtual
    domain unification for end-to-end highway driving. *arXiv preprint arXiv:1801.03458*
    .
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2018) Yang L, Liang X 和 Xing E (2018) 无监督的真实到虚拟领域统一用于端到端高速公路驾驶。*arXiv
    预印本 arXiv:1801.03458*。
- en: You et al. (2017) You Y, Pan X, Wang Z and Lu C (2017) Virtual to real reinforcement
    learning for autonomous driving. *arXiv preprint arXiv:1704.03952* .
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You et al. (2017) You Y, Pan X, Wang Z 和 Lu C (2017) 虚拟到真实的自动驾驶强化学习。*arXiv 预印本
    arXiv:1704.03952*。
- en: Yu et al. (2018) Yu T, Finn C, Xie A, Dasari S, Zhang T, Abbeel P and Levine
    S (2018) One-shot imitation from observing humans via domain-adaptive meta-learning.
    *arXiv preprint arXiv:1802.01557* .
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2018) Yu T, Finn C, Xie A, Dasari S, Zhang T, Abbeel P 和 Levine S
    (2018) 通过领域自适应元学习从观察人类中进行一次性模仿。*arXiv 预印本 arXiv:1802.01557*。
- en: 'Zhang et al. (2017a) Zhang J, Springenberg JT, Boedecker J and Burgard W (2017a)
    Deep reinforcement learning with successor features for navigation across similar
    environments. In: *2017 IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS)*. pp. 2371–2378.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017b) Zhang J, Tai L, Boedecker J, Burgard W and Liu M (2017b)
    Neural slam. *arXiv preprint arXiv:1706.09520* .
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018) Zhang J, Tai L, Xiong Y, Liu M, Boedecker J and Burgard
    W (2018) Vr goggles for robots: Real-to-sim domain adaptation for visual control.
    *arXiv preprint arXiv:1802.00265* .'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017c) Zhang T, McCarthy Z, Jow O, Lee D, Goldberg K and Abbeel
    P (2017c) Deep imitation learning for complex manipulation tasks from virtual
    reality teleoperation. *arXiv preprint arXiv:1710.04615* .
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017a) Zhu JY, Park T, Isola P and Efros AA (2017a) Unpaired image-to-image
    translation using cycle-consistent adversarial networks. *arXiv preprint arXiv:1703.10593*
    .
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2017b) Zhu Y, Mottaghi R, Kolve E, Lim JJ, Gupta A, Fei-Fei L and
    Farhadi A (2017b) Target-driven visual navigation in indoor scenes using deep
    reinforcement learning. In: *Robotics and Automation (ICRA), 2017 IEEE International
    Conference on*. IEEE, pp. 3357–3364.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2018) Zhu Y, Wang Z, Merel J, Rusu A, Erez T, Cabi S, Tunyasuvunakool
    S, Kramár J, Hadsell R, de Freitas N et al. (2018) Reinforcement and imitation
    learning for diverse visuomotor skills. *arXiv preprint arXiv:1802.09564* .
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ziebart et al. (2008) Ziebart BD, Maas AL, Bagnell JA and Dey AK (2008) Maximum
    entropy inverse reinforcement learning. In: *AAAI*, volume 8\. Chicago, IL, USA,
    pp. 1433–1438.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
