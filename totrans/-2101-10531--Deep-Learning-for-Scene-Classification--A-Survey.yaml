- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:57:00'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2101.10531] Deep Learning for Scene Classification: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2101.10531](https://ar5iv.labs.arxiv.org/html/2101.10531)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for Scene Classification: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Delu Zeng, Minyu Liao, Mohammad Tavakolian, Yulan Guo
  prefs: []
  type: TYPE_NORMAL
- en: Bolei Zhou, Dewen Hu, Matti Pietikäinen, and Li Liu Delu Zeng (dlzeng@scut.edu.cn)
    and Minyu Liao (201820127075@mail.-scut.edu.cn) are with the South China University
    of Technology, China. Mohammad Tavakolian (mohammad.tavakolian@oulu.fi), Matti
    Pietikäinen (matti.pietikainen@oulu.fi) and Li Liu (li.liu@oulu.fi) are with the
    University of Oulu, Finland. Yulan Guo (yulan.guo@nudt.edu.cn), Dewen Hu (dwhu@nudt.edu.cn)
    and Li Liu are with the National University of Defense Technology. Bolei Zhou
    (bzhou@ie.cuhk.edu.hk) is with the Chinese University of Hong Kong, China. Li
    Liu is the corresponding author. Delu Zeng, Minlu Liao, and Li Liu have equal
    contribution to this work and are cofirst authors.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Scene classification, aiming at classifying a scene image to one of the predefined
    scene categories by comprehending the entire image, is a longstanding, fundamental
    and challenging problem in computer vision. The rise of large-scale datasets,
    which constitute the corresponding dense sampling of diverse real-world scenes,
    and the renaissance of deep learning techniques, which learn powerful feature
    representations directly from big raw data, have been bringing remarkable progress
    in the field of scene representation and classification. To help researchers master
    needed advances in this field, the goal of this paper is to provide a comprehensive
    survey of recent achievements in scene classification using deep learning. More
    than 200 major publications are included in this survey covering different aspects
    of scene classification, including challenges, benchmark datasets, taxonomy, and
    quantitative performance comparisons of the reviewed methods. In retrospect of
    what has been achieved so far, this paper is also concluded with a list of promising
    research opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Scene classification, Deep learning, Convolutional neural network, Scene representation,
    Literature survey.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of scene classification is to classify a scene image¹¹1A scene is a
    semantically coherent view of a real-world environment that contains background
    and multiple objects, organized in a spatially licensed manner [[1](#bib.bib1),
    [2](#bib.bib2)]. to one of the predefined scene categories (such as beach, kitchen,
    and bakery), based on the image’s ambient content, objects, and their layout.
    Visual scene understanding requires reasoning about the diverse and complicated
    environments that we encounter in our daily life. Recognizing visual categories
    such as objects, actions and events is no doubt the indispensable ability of a
    visual system. Moreover, recognizing the scene where the objects appear is of
    equal importance for an intelligent system to predict the context for the recognized
    objects by reasoning “What is happening? What will happen next?”. Humans are remarkably
    efficient at categorizing natural scenes [[3](#bib.bib3), [4](#bib.bib4)]. However,
    it is not an easy task for machines due to the scene’s semantic ambiguity, and
    the large intraclass variations caused by imaging conditions like variations in
    illumination, viewing angle and scale, imaging distance, *etc*. As a longstanding,
    fundamental and challenging problem in computer vision, scene classification has
    been an active area of research for several decades, and has a wide range of applications,
    such as content based image retrieval [[5](#bib.bib5), [6](#bib.bib6)], robot
    navigation [[7](#bib.bib7), [8](#bib.bib8)], intelligent video surveillance [[9](#bib.bib9),
    [10](#bib.bib10)], augmented reality [[11](#bib.bib11), [12](#bib.bib12)], and
    disaster detection applications [[13](#bib.bib13)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/787c67514e1a0a1c6b7459b2240c8462.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A performance overview of scene classification: we can observe a
    significant improvement on two benchmark datasets since the reignition of deep
    learning. Dense-SIFT [[14](#bib.bib14)], Object Bank [[15](#bib.bib15)] and OTC
    [[16](#bib.bib16)] are handcrafted methods, while the others are deep learning
    based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg  class="ltx_picture ltx_centering" height="282.56" overflow="visible"
    version="1.1" width="805.42"><g transform="translate(0,282.56) matrix(1 0 0 -1
    0 0) translate(15.47,0) translate(0,32.39)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -11.78 238.18)" fill="#000000"
    stroke="#000000"><foreignobject width="321.63" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Deep Learning based methods for Scene Classification (Section [3](#S3
    "3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey"))</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 35.46 224)" fill="#000000" stroke="#000000"><foreignobject
    width="171.17" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Main
    CNN Framework (Section [3.1](#S3.SS1 "3.1 Main CNN Framework ‣ 3 Deep Learning
    based Methods ‣ Deep Learning for Scene Classification: A Survey"))</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 209.83)" fill="#000000" stroke="#000000"><foreignobject
    width="120.14" height="7.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">*Pre-trained
    CNN Model*: Object-centric CNNs [[17](#bib.bib17)], Scene-centric CNNs [[18](#bib.bib18)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 198.43)" fill="#000000" stroke="#000000"><foreignobject
    width="117.92" height="7.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">*Fine-tuned
    CNN Model*: Scale-specific CNNs [[19](#bib.bib19)], DUCA [[20](#bib.bib20)], FTOTLM [[21](#bib.bib21)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 181.48)" fill="#000000" stroke="#000000"><foreignobject
    width="102.09" height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">*Specific
    CNN Model*: DL-CNN [[22](#bib.bib22)], GAP-CNN [[23](#bib.bib23)], CFA [[24](#bib.bib24)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 35.46 167.31)" fill="#000000" stroke="#000000"><foreignobject
    width="222.31" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CNN
    based Scene Representation (Section [3.2](#S3.SS2 "3.2 CNN based Scene Representation
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey"))</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 162.13)" fill="#000000" stroke="#000000"><foreignobject
    width="175.59" height="7.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">*Global
    CNN Feature based Method*: Places-CNN [[18](#bib.bib18), [25](#bib.bib25)], S2ICA
    [[26](#bib.bib26)], GAP-CNN [[23](#bib.bib23)], DL-CNN [[22](#bib.bib22)], HLSTM [[27](#bib.bib27)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 147.96)" fill="#000000" stroke="#000000"><foreignobject
    width="205.37" height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">*Spatially
    Invariant Feature based Method*: MOP-CNN [[28](#bib.bib28)], MPP-CNN [[29](#bib.bib29)],
    SFV [[30](#bib.bib30)], MFAFVNet [[31](#bib.bib31)], VSAD [[32](#bib.bib32)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 133.79)" fill="#000000" stroke="#000000"><foreignobject
    width="159.22" height="7.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">*Semantic
    Feature based Method*: MetaObject-CNN [[33](#bib.bib33)], WELDON [[34](#bib.bib34)],
    SDO [[35](#bib.bib35)], M2M BiLSTM [[36](#bib.bib36)]</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 82.71 119.61)" fill="#000000" stroke="#000000"><foreignobject width="168.78"
    height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">*Multi-layer
    Feature based Method*: DAG [[37](#bib.bib37)], Hybrid CNNs [[38](#bib.bib38)],
    G-MS2F [[39](#bib.bib39)], FTOTLM [[21](#bib.bib21)]</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 82.71 99.21)" fill="#000000" stroke="#000000"><foreignobject width="167.52"
    height="7.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">*Multi-view
    Feature based Method*: Scale-specific CNNs [[19](#bib.bib19)], LS-DHM [[40](#bib.bib40)],
    MR CNN [[41](#bib.bib41)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0
    1.0 35.46 82.27)" fill="#000000" stroke="#000000"><foreignobject width="277.99"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Strategy
    for Improving Scene Representation (Section [3.3](#S3.SS3 "3.3 Strategies for
    Improving Scene Representation ‣ 3 Deep Learning based Methods ‣ Deep Learning
    for Scene Classification: A Survey"))</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 82.71 77.09)" fill="#000000" stroke="#000000"><foreignobject width="93.05"
    height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">*Encoding
    Strategy*: Semantic FV [[30](#bib.bib30)], FCV [[40](#bib.bib40)], VSAD [[32](#bib.bib32)],
    MFA-FS [[42](#bib.bib42)], MFAFVNet [[31](#bib.bib31)]</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 82.71 53.93)" fill="#000000" stroke="#000000"><foreignobject width="94.12"
    height="9.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">*Attention
    Strategy*: Channel attention policy [[43](#bib.bib43)], Spatial attention policy [[44](#bib.bib44)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 39.75)" fill="#000000" stroke="#000000"><foreignobject
    width="101.19" height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">*Contextual
    Strategy*: Sequential model [[27](#bib.bib27)], Graph-related model [[45](#bib.bib45)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 34.57)" fill="#000000" stroke="#000000"><foreignobject
    width="117.98" height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">*Regularization
    Strategy*: Sparse Regularization [[22](#bib.bib22)], Structured Regularization [[44](#bib.bib44)],
    Supervised Regularization [[40](#bib.bib40)]</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 35.46 11.41)" fill="#000000" stroke="#000000"><foreignobject width="194.99"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RGB-D Scene
    Classification (Section [3.4](#S3.SS4 "3.4 RGB-D Scene Classification ‣ 3 Deep
    Learning based Methods ‣ Deep Learning for Scene Classification: A Survey"))</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 6.23)" fill="#000000" stroke="#000000"><foreignobject
    width="158.33" height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">*Depth-specific
    Feature Learning*: Fine-tuning RGB-CNNs [[46](#bib.bib46)], Weak-supervised learning [[47](#bib.bib47)],
    Semi-supervised learning [[48](#bib.bib48)]</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 82.71 -7.95)" fill="#000000" stroke="#000000"><foreignobject width="126.59"
    height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">*Multiple
    Modality Fusion*: Feature-level combination [[49](#bib.bib49)],Consistent-feature
    based fusion [[50](#bib.bib50)], Distinctive-feature based fusion [[44](#bib.bib44)]</foreignobject></g></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: A taxonomy of deep learning for scene classification. With the rise
    of large-scale datasets, powerful features are learned from pre-trained CNNs,
    fine-tuned CNNs, or specific CNNs, having made remarkable progress. The major
    features include global CNN features, spatially invariant features, semantic features,
    multi-layer features, and multi-view features. Meanwhile, many methods are improved
    via effective strategies, like encoding, attention learning, and context modeling.
    As a new issue, methods using RGB-D datasets, focus on learning depth specific
    features and fusing multiple modalities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the core of scene classification, scene representation is the process of
    transforming a scene image into its concise descriptors (*i.e.,* features), and
    still attracts tremendous and increasing attention. The recent revival of interest
    in Artificial Neural Networks (ANNs), particularly deep learning [[17](#bib.bib17)],
    has revolutionized computer vision and been ubiquitously used in various tasks
    like object classification and detection [[51](#bib.bib51), [52](#bib.bib52)],
    semantic segmentation [[53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55)] and
    scene classification [[28](#bib.bib28), [18](#bib.bib18)]. In 2012, object classification
    with the large-scale ImageNet dataset [[56](#bib.bib56)] achieved a significant
    breakthrough in performance by a Deep Neural Network (DNN) named AlexNet [[17](#bib.bib17)],
    which is arguably what reignited the field of ANNs and triggered the recent revolution
    in computer vision. Since then, research focus on scene classification has begun
    to move away from handcrafted feature engineering to deep learning, which can
    learn powerful representations directly from data. Recent advances in deep learning
    have opened the possibility of scene classification towards the datasets *of large
    scale* and *in the wild* [[18](#bib.bib18), [57](#bib.bib57), [25](#bib.bib25)],
    and many scene representations [[28](#bib.bib28), [58](#bib.bib58), [30](#bib.bib30),
    [21](#bib.bib21)] have been proposed. As illustrated in Fig. [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Deep Learning for Scene Classification: A Survey"), deep
    learning has brought significant improvements in scene classification. Given the
    exceptionally rapid rate of progress, this article attempts to track recent advances
    and summarize their achievements to gain a clearer picture of the current panorama
    in scene classification using deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, several surveys for scene classification have also been available,
    such as [[59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61)]. Cheng et al. [[60](#bib.bib60)]
    provided a recent comprehensive review of the recent progress for remote sensing
    image scene classification. Wei et al. [[59](#bib.bib59)] carried out an experimental
    study of 14 scene descriptors mainly in the handcrafted feature engineering way
    for scene classification. Xie et al. [[61](#bib.bib61)] reviewed scene recognition
    approaches in the past two decades, and most of discussed methods in their survey
    appeared in this handcrafted way. *As opposed to* these existing reviews [[60](#bib.bib60),
    [59](#bib.bib59), [61](#bib.bib61)], this work herein summarizes the striking
    success and dominance in indoor/outdoor scene classification using deep learning
    and its related methods, but not including other scene classification tasks, *e.g.,*
    remote sensing scene classification [[62](#bib.bib62), [63](#bib.bib63), [60](#bib.bib60)],
    acoustic scene classification [[64](#bib.bib64), [65](#bib.bib65)], place classification [[66](#bib.bib66),
    [67](#bib.bib67)], *etc*. The major contributions of this work can be summarized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As far as we know, this paper is the first to specifically focus on deep learning
    methods for indoor/outdoor scene classification, including RGB scene classification,
    as well as RGB-D scene classification.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We present a taxonomy (see Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep
    Learning for Scene Classification: A Survey")), covering the most recent and advanced
    progresses of deep learning for scene representation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comprehensive comparisons of existing methods on several public datasets are
    provided, meanwhile we also present brief summaries and insightful discussions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The remainder of this paper is organized as follows: Challenges and benchmark
    datasets are summarized in Section [2](#S2 "2 Background ‣ Deep Learning for Scene
    Classification: A Survey"). In section [3](#S3 "3 Deep Learning based Methods
    ‣ Deep Learning for Scene Classification: A Survey"), we present a taxonomy of
    the existing deep learning based methods. Then in section [4](#S4 "4 Performance
    Comparison ‣ Deep Learning for Scene Classification: A Survey"), we provide an
    overall discussion of their performance (Tables [II](#S4.T2 "TABLE II ‣ 4.1 Performance
    on RGB scene data ‣ 4 Performance Comparison ‣ Deep Learning for Scene Classification:
    A Survey"), [IV](#S4.T4 "TABLE IV ‣ 4.1 Performance on RGB scene data ‣ 4 Performance
    Comparison ‣ Deep Learning for Scene Classification: A Survey")). Followed by
    Section [5](#S5 "5 Conclusion and Outlook ‣ Deep Learning for Scene Classification:
    A Survey") we conclude important future research outlook.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 The Problem and Challenge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scene classification can be further dissected through analyzing its strong ties
    with related vision tasks, such as object classification and texture classification.
    As typical pattern recognition problems, these tasks all consist of feature representation
    and classification. However, in contrast to object classification (images are
    object-centric) or texture classification (images include only textures), the
    scene images are more complicated, and it is essential to further explore the
    content of scene, *e.g.,* what the semantic parts (*e.g.,* objects, textures,
    background) are, in what way they are organized together, and what their semantic
    connections with each other are. Despite over several decades of development in
    scene classification (shown in the appendix due to space limit), most of methods
    still have not been capable of performing at a level sufficient for various real-world
    scenes. The inherent difficulty is due to the nature of complexity and high variance
    of scenes. Overall, significant challenges in scene classification stem from large
    intraclass variations, semantic ambiguity, and computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e122f14e0b7e232244e0176c633b04a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Illustrations of *large intraclass variation* and *semantic ambiguity*.
    Top (large intraclass variation): The shopping malls are quite different caused
    by lighting and overall content. Below (semantic ambiguity): General layout and
    uniformly arranged objects are similar on archive, bookstore, and library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Large intraclass variation. Intraclass variation mainly originates from intrinsic
    factors of the scene itself and imaging conditions. In terms of intrinsic factors,
    each scene can have many different example images, possibly varying with large
    variations among various objects, background, or human activities. Imaging conditions
    like changes in illumination, viewpoint, scale and heavy occlusion, clutter, shading,
    blur, motion, *etc*. contribute to large intraclass variations. Further challenges
    may be added by digitization artifacts, noise corruption, poor resolution, and
    filtering distortion. For instance, three shopping malls (top row of Fig. [3](#S2.F3
    "Figure 3 ‣ 2.1 The Problem and Challenge ‣ 2 Background ‣ Deep Learning for Scene
    Classification: A Survey")) are shown with different lighting conditions, viewing
    angle, and objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic ambiguity. Since images of different classes may share similar objects,
    textures, background, *etc.*, they look very similar in visual appearances, which
    causes ambiguity among them [[68](#bib.bib68), [43](#bib.bib43)]. The bottom row
    of Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 The Problem and Challenge ‣ 2 Background ‣
    Deep Learning for Scene Classification: A Survey") depicts strong visual correlation
    between three different indoor scenes, *i.e.,* archive, bookstore, and library.
    With the emerging of new scene categories, the problem of semantic ambiguity would
    be more serious. In addition, scene category annotation is subjective, relying
    on the experience of the annotators, therefore a scene image may belong to multiple
    semantic categories [[68](#bib.bib68), [69](#bib.bib69)].'
  prefs: []
  type: TYPE_NORMAL
- en: Computational efficiency. The prevalence of social media networks and mobile/wearable
    devices has led to increasing demands for various computer vision tasks including
    scene recognition. However, mobile/wearable devices have constrained computing
    related resources, making efficient scene recognition a pressing requirement.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section reviews publicly available datasets for scene classification.
    The scene datasets (see Fig. [4](#S2.F4 "Figure 4 ‣ 2.2 Datasets ‣ 2 Background
    ‣ Deep Learning for Scene Classification: A Survey")) are broadly divided into
    two main categories based on the image type: RGB and RGB-D datasets. The datasets
    can further be divided into two categories in terms of their size. Small-size
    datasets (*e.g.,* Scene15 [[14](#bib.bib14)], MIT67 [[70](#bib.bib70)], SUN397 [[57](#bib.bib57)],
    NYUD2 [[71](#bib.bib71)], SUN RGBD [[72](#bib.bib72)]) are usually used for evaluation,
    while large-scale datasets, *e.g.,* ImageNet [[56](#bib.bib56)] and Places [[18](#bib.bib18),
    [25](#bib.bib25)], are essential for pre-training and developing deep learning
    models. Table [I](#S2.T1 "TABLE I ‣ 2.2 Datasets ‣ 2 Background ‣ Deep Learning
    for Scene Classification: A Survey") summarizes the characteristics of these datasets
    for scene classification.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d4037eace28cb7798d99004d5d41427b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Some example images for scene classification from benchmark datasets
    for a summary of these datasets. RGB-D images consist of RGB and a depth map.
    Moreover, Gupta et al. [[73](#bib.bib73)] proposed to convert depth image into
    three-channel feature maps, *i.e.,* Horizontal disparity, Height above the ground,
    and Angle of the surface norm (HHA). Such HHA encoding is useful for the visualization
    of depth data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Popular datasets for scene classification. “#” denotes *the number
    of*.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Dataset | #Images | #Class | Resolution | Class label |'
  prefs: []
  type: TYPE_TB
- en: '| RGB | Scene15 [[14](#bib.bib14)] | 4,488 | 15 | $\thickapprox$ 300$\times$250
    | Indoor/outdoor scene |'
  prefs: []
  type: TYPE_TB
- en: '|  | MIT67 [[70](#bib.bib70)] | 15,620 | 67 | $\geq$ 200$\times$200 | Indoor
    scene |'
  prefs: []
  type: TYPE_TB
- en: '|  | SUN397 [[57](#bib.bib57)] | 108,754 | 397 | $\thickapprox$ 500$\times$300
    | Indoor/outdoor scene |'
  prefs: []
  type: TYPE_TB
- en: '|  | ImageNet [[17](#bib.bib17)] | 14 million+ | 21,841 | $\thickapprox$ 500$\times$400
    | Object |'
  prefs: []
  type: TYPE_TB
- en: '|  | Places205 [[18](#bib.bib18)] | 7,076,580 | 205 | $\geq$ 200$\times$200
    | Indoor/outdoor scene |'
  prefs: []
  type: TYPE_TB
- en: '|  | Places88 [[18](#bib.bib18)] | $-$ | 88 | $\geq$ 200$\times$200 | Indoor/outdoor
    scene |'
  prefs: []
  type: TYPE_TB
- en: '|  | Places365-S [[25](#bib.bib25)] | 1,803,460 | 365 | $\geq$ 200$\times$200
    | Indoor/outdoor scene |'
  prefs: []
  type: TYPE_TB
- en: '|  | Places365-C [[25](#bib.bib25)] | 8 million+ | 365 | $\geq$ 200$\times$200
    | Indoor/outdoor scene |'
  prefs: []
  type: TYPE_TB
- en: '| RGB-D | NYUD2 [[71](#bib.bib71)] | 1,449 | 10 | $\thickapprox$ 640$\times$480
    | Indoor scene |'
  prefs: []
  type: TYPE_TB
- en: '|  | SUN RGBD [[72](#bib.bib72)] | 10,355 | 19 | $\geq$512$\times$424 | Indoor
    scene |'
  prefs: []
  type: TYPE_TB
- en: Scene15 dataset [[14](#bib.bib14)] is a small scene dataset containing 4,448
    grayscale images of 15 scene categories, *i.e.,* 5 indoor scene classes (*e.g.,*
    office, store, and kitchen) along with 10 outdoor scene classes (like suburb,
    forest, and tall building). Each class contains 210$-$410 scene images, and the
    image size is around 300$\times$250\. The dataset is divided into two splits;
    there are at least 100 images per class in the training set, and the rest are
    for testing.
  prefs: []
  type: TYPE_NORMAL
- en: MIT Indoor 67 (MIT67) dataset [[70](#bib.bib70)] covers a wide range of indoor
    scenes, *e.g.,* store, public space, and leisure. MIT67 comprises 15,620 scene
    images from 67 indoor categories, where each category has about 100 images. Moreover,
    all images have a minimum resolution of 200$\times$200 pixels on the smallest
    axis. Because of the shared similarities among objects in this dataset, the classification
    of images is challenging. There are 80 and 20 images per class in the training
    and testing set, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Scene UNderstanding 397 (SUN397) dataset [[57](#bib.bib57)] consists of 397
    scene categories, in which each category has more than 100 images. The dataset
    contains 108,754 images with an image size of about 500$\times$300 pixels. SUN397
    spans over 175 indoor, 220 outdoor scene classes, and two classes with mixed indoor
    and outdoor images, *e.g.,* a promenade deck with a ticket booth. There are several
    train/test split settings with 50 images per category in the testing.
  prefs: []
  type: TYPE_NORMAL
- en: ImageNet dataset [[56](#bib.bib56)] is one of the most famous large-scale image
    databases particularly used for visual tasks. It is organized in terms of the
    WordNet [[74](#bib.bib74)] hierarchy, each node of which is depicted by hundreds
    and thousands of images. Up to now, there are more than 14 million images and
    about 20 thousand notes in the ImageNet. Usually, a subset of ImageNet dataset
    (about 1000 categories with a total of 1.2 million images [[17](#bib.bib17)])
    is used to pre-train the CNN for scene classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Places dataset [[18](#bib.bib18), [25](#bib.bib25)] is a large-scale scene
    dataset with 434 scene categories, which provides an exhaustive list of the classes
    of environments encountered in the real world. The Places dataset has inherited
    the same list of scene categories from SUN397 [[57](#bib.bib57)]. Four benchmark
    subsets of Places are shown as follows: 1) Places205 [[18](#bib.bib18)] has 2.5
    million images from scene categories. The image number per class varies from 5,000
    to 15,000\. The training set has 2,448,873 images, with 100 images per category
    for validation and 200 images per category for testing. 2) Places88 [[18](#bib.bib18)]
    contains the 88 common scene categories among the ImageNet [[56](#bib.bib56)],
    SUN397 [[57](#bib.bib57)], and Places205 datasets. Places88 includes only the
    images obtained in the second round of annotation from the Places. 3) Places365-Standard [[25](#bib.bib25)]
    has 1,803,460 training images with the image number per class varying from 3,068
    to 5,000\. The validation set has 50 images/class, while the testing set has 900
    images/class. 4) Places365-Challenge contains the same categories as the Places365-Standard,
    but its training set is significantly larger with a total of 8 million images.
    This subset was released for the Places Challenge held in conjunction with ECCV,
    as part of the ILSVRC 2016 Challenge.'
  prefs: []
  type: TYPE_NORMAL
- en: NYU-Depth V2 (NYUD2) dataset [[71](#bib.bib71)] is comprised of video sequences
    from a variety of indoor scenes as recorded by both the RGB and depth cameras.
    The dataset consists of 1,449 densely labeled pairs of aligned RGB and depth images
    from 27 indoor scene categories. It features 464 scenes taken from 3 cities and
    407,024 unlabeled frames. With the publicly available split, NYUD2 for scene classification
    offers 795 images for training while 654 images for testing.
  prefs: []
  type: TYPE_NORMAL
- en: SUN RGBD dataset [[72](#bib.bib72)] consists of 10,335 RGB-D images with dense
    annotations in both 2D and 3D, for both objects and rooms. The dataset is collected
    by four different sensors at a similar scale as PASCAL VOC [[75](#bib.bib75)].
    The whole dataset is densely annotated and includes 146,617 2D polygons and 58,657
    3D bounding boxes with accurate object orientations, as well as a 3D room layout
    and category for scenes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2a49c8f9b17a7226bb695b29e94589ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Generic pipeline of deep learning for scene classification. An entire
    pipeline consists of a module in each of the three stages (local feature extraction,
    feature encoding and pooling, and category prediction). The common pipelines are
    shown with arrows in different colors, including global CNN feature based pipeline
    (blue arrows), spatially invariant feature based pipeline (green arrows), and
    semantic feature based pipeline (red arrows). Although the pipeline of some methods
    (like [[31](#bib.bib31), [32](#bib.bib32)]) are unified and trained in an end-to-end
    manner, they are virtually composed of these three stages.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Deep Learning based Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present a comprehensive review of deep learning methods
    for scene classification. A brief introduction of deep learning is provided in
    the appendix due to limit space. The most common deep learning architecture is
    Convolutional Neural Network (CNN) [[76](#bib.bib76)]. With CNN as feature extractor,
    Fig. [5](#S2.F5 "Figure 5 ‣ 2.2 Datasets ‣ 2 Background ‣ Deep Learning for Scene
    Classification: A Survey") shows the generic pipeline of most CNN based methods
    for scene classification. Almost without exception, given an input scene image,
    the first stage is to use CNN extractors to obtain local features. Then, the second
    process is to aggregate these features into an image-level representation via
    encoding, concatenating, or pooling. Finally, with the representation as input,
    the classification stage is to get a predicted category.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The taxonomy, shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning
    for Scene Classification: A Survey"), covers different aspects of deep learning
    for scene classification. In the following investigation, we firstly study the
    main CNN frameworks for scene classification. Then, we review existing CNN based
    scene representations. Furthermore, we explore various techniques for improving
    the obtained representations. Finally, as a supplement, we investigate scene classification
    using RGB-D data.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Main CNN Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolutional Neural Networks (CNNs) are common deep learning models to extract
    high quality representation. At the beginning, limited by computing resources
    and labeled data, scene features are extracted from pre-trained CNNs, which is
    usually combined to BoVW pipeline [[6](#bib.bib6)]. Then, fine-tuned CNN models
    are used to keep last layers more data-specific. Alternatively, specific CNN models
    have emerged to adapt to scene attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Pre-trained CNN Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The network architecture plays a pivotal role in the performance of deep models.
    In the beginning, AlexNet [[17](#bib.bib17)] served as the mainstream CNN model
    for feature representation and classification purposes. Later, Simonyan et al. [[77](#bib.bib77)]
    developed VGGNet and showed that, for a given receptive field, using multiple
    stacked small kernels is better than using a large convolution kernel, because
    applying non-linearity on multiple feature maps yields more discriminative representations.
    On the other hand, the reduction of kernels’ receptive filed size decreases the
    number of parameters for bigger networks. Therefore, VGGNet has 3$\times$3 convolution
    kernels instead of large convolution kernels (*i.e.,* 11$\times$11, 7$\times$7,
    and 5$\times$5) in AlexNet. Motivated by the idea that only a handful of neurons
    have an effective role in feature representation, Szegedy et al. [[78](#bib.bib78)]
    proposed an Inception module to make a sparse approximation of CNNs. Deeper the
    model, the more descriptive representations. This is the advantage of hierarchical
    feature extraction using CNN. However, constantly increasing CNN’s depth could
    result in gradient vanishing. To address this issue, He et al. [[79](#bib.bib79)]
    included skip connection to the hierarchical structure of CNN and proposed Residual
    Networks (ResNets), which are easier to optimize and can gain accuracy from considerably
    increased depth.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the network architecture, the performance of CNN interwinds with
    a sufficiently large amount of training data. However, the training data are scarce
    in certain applications, which results in the under-fitting of the model during
    the training process. To overcome this issue, pre-trained models can be employed
    to effectively extract feature representations of small datasets [[80](#bib.bib80)].
    Training CNN on large-scale datasets, such as the ImageNet [[56](#bib.bib56)]
    and the Places [[18](#bib.bib18), [25](#bib.bib25)], makes them learn enriched
    visual representations. Such models can further be used as pre-trained models
    for other tasks. However, the effectiveness of the employment of pre-trained models
    largely depends on the similarity between the source and target domains. Yosinski
    et al. [[81](#bib.bib81)] documented that the transferability of pre-trained CNN
    models decreases as the similarity of the target task and original source task
    decreases. Nevertheless, pre-trained models still have better performance than
    random initialization of the models [[81](#bib.bib81)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-trained CNNs, as fixed feature extractors, are divided into two categories:
    object-centric and scene-centric CNNs. Object-centric CNNs refer to the model
    pre-trained on object datasets, *e.g.,* the ImageNet [[56](#bib.bib56)], and deployed
    for scene classification. Since object images do not contain the diversity provided
    by the scene [[18](#bib.bib18)], object-centric CNNs have limited performance
    for scene classification. Hence, scene-centric CNNs, pre-trained on scene images,
    like Places [[18](#bib.bib18), [25](#bib.bib25)], are more effective to extract
    scene-related features.'
  prefs: []
  type: TYPE_NORMAL
- en: Object-centric CNNs. Cimpoi et al. [[82](#bib.bib82)] asserted that the feature
    representations obtained from object-centric CNNs are object descriptors since
    they have likely more object descriptive properties. The scene image is represented
    as a bag of semantics [[30](#bib.bib30)], and object-centric CNNs are sensitive
    to the overall shape of objects, so many methods [[28](#bib.bib28), [83](#bib.bib83),
    [30](#bib.bib30), [82](#bib.bib82), [31](#bib.bib31)] used object-centric CNNs
    to extract local features from different regions of the scene image. Another important
    factor in the effective deployment of object-centric CNNs is the relational size
    of images in the source and target datasets. Although CNNs are generally robust
    against size and scale, the performance of object-centric CNNs is influenced by
    scaling because such models are originally pre-trained on datasets to detect and/or
    recognize objects. Therefore, the shift to describing scenes, which have multiple
    objects with different scales, would drastically affect their performance [[19](#bib.bib19)].
    For instance, if the image size of the target dataset is smaller than the source
    dataset to a certain degree, the accuracy of the model would be compromised.
  prefs: []
  type: TYPE_NORMAL
- en: Scene-centric CNNs. Zhou et al. [[18](#bib.bib18), [25](#bib.bib25)] demonstrated
    the classification performance of scene-centric CNNs is better than object-centric
    CNNs since the former use the prior knowledge of the scene. Herranz et al. [[19](#bib.bib19)]
    found that Places-CNNs [[23](#bib.bib23)] achieve better performance at larger
    scales; therefore, scene-centric CNNs generally extract the representations in
    the whole range of scales. Guo et al. [[40](#bib.bib40)] noticed that the CONV
    layers of scene-centric CNNs capture more detail information of a scene, such
    as local semantic regions and fine-scale objects, which is crucial to discriminate
    the ambiguous scenes, while the feature representations obtained from the FC layers
    do not convey such perceptive quality. Zhou et al. [[84](#bib.bib84)] showed that
    scene-centric CNNs may also perform as object detectors without explicitly being
    trained on object datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Fine-tuned CNN Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Pre-trained CNNs, described in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Pre-trained
    CNN Model ‣ 3.1 Main CNN Framework ‣ 3 Deep Learning based Methods ‣ Deep Learning
    for Scene Classification: A Survey"), perform as feature extractor with prior
    knowledge of the training data [[6](#bib.bib6), [85](#bib.bib85)]. However, using
    only the pre-training strategy would prevent exploiting the full capability of
    the deep models in describing the target scenes adaptively. Hence, fine-tuning
    the pre-trained CNNs using the target scene dataset improves their performance
    by reducing the possible domain shift between two datasets [[85](#bib.bib85)].
    Notably, a suitable weight initialization becomes very important, because it is
    quite difficult to train a model with many adjustable parameters and non-convex
    loss functions [[86](#bib.bib86)]. Therefore, fine-tuning the pre-trained CNN
    contributes to the effective training process [[87](#bib.bib87), [30](#bib.bib30),
    [29](#bib.bib29), [34](#bib.bib34)].'
  prefs: []
  type: TYPE_NORMAL
- en: For CNNs, a common fine-tuning technique is the *freeze strategy*. In this method,
    the last FC layer of a pretrained model is replaced with a new FC layer with the
    same number of neurons as the classes in the target dataset (*i.e.,* MIT67, SUN397),
    while the previous CONV layers’ parameters are frozen, *i.e.,* they are not updated
    during fine-tuning. Then, this modified CNN is fine-tuned by training on the target
    dataset. Herein, the back-propagation is stopped after the last FC layers, which
    allows these layers to extract discriminative features from the previous learned
    layers. Through updating few parameters, training a complex model using small
    datasets would be affordable. Optionally, it is also possible to gradually unfreeze
    some layers to further enhance the learning quality as the earlier layers would
    adapt new representations from the target dataset. Alternatively, different learning
    rates could be assigned to different layers of CNN, in which the early layers
    of the model have very low learning rate and the last layers have higher learning
    rates. In this way, the early CONV layers that have more abstract representations
    are less affected, while the specialized FC layers are fine-tuned with higher
    speed.
  prefs: []
  type: TYPE_NORMAL
- en: Small-size training dataset limits the effective fine-tuning process, while
    *data augmentation* is one alternative to deal with this issue [[88](#bib.bib88),
    [20](#bib.bib20), [21](#bib.bib21), [48](#bib.bib48)]. Liu et al. [[85](#bib.bib85)]
    indicated that deep models may not benefit from fine-tuning on a small target
    dataset. In addition, fine-tuning may have negative effects since the specialized
    FC layers are changed while inadequate training data are provided for fine-tuning.
    To this end, Khan et al. [[20](#bib.bib20)] augmented the scene image dataset
    with flipped, cropped, and rotated versions to increase the size of the dataset
    and further improve the robustness of the learned representations. Liu et al. [[21](#bib.bib21)]
    proposed a method to select representative image patches of the original image.
  prefs: []
  type: TYPE_NORMAL
- en: There exists a problem via data augmentation to fine-tune CNNs for scene classification.
    Herranz et al. [[19](#bib.bib19)] asserted that fine-tuning a CNN model have certain
    “equalizing” effect between the input patch scale and final accuracy, *i.e.,*
    to some extent, with too small patches as CNN inputs, the final classification
    accuracy is worse. This is because the small patch inputs contain insufficient
    image information, while the final labels indicate scene categories [[32](#bib.bib32),
    [89](#bib.bib89)]. Moreover, the number of cropped patches is huge, so just a
    tiny part of these patches is used to fine-tune CNN models, rendering limited
    overall improvement [[19](#bib.bib19)]. On the other hand, Herranz et al. [[19](#bib.bib19)]
    also explored the effect of fine-tuning CNNs on different scales, *i.e.,* with
    different scale patches as inputs. From the practical results, there is a moderate
    accuracy gain in the range of scale patches where the original CNNs perform poorly,
    *e.g.,* in the cases of global scales for ImageNet-CNN and local scales for Places-CNN.
    However, there is marginal or no gain in ranges where CNN have already strong
    performance. For example, since Places-CNN has the best performance in the whole
    range of scale patches, in this case, fine-tuning on target dataset leads to negligible
    performance improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Specific CNN Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/34b3370671c0014b0178cb52f370d1a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Illustrations of three typical specific CNN models. (a) In GAP-CNN [[23](#bib.bib23)],
    to reduce parameters of the standard CNN, FC layers are removed, and GAP layer
    is introduced to form Class Activation Maps (CAMs). (b) In DL-CNN [[22](#bib.bib22)],
    to reduce parameters of CNN model and obtain enhanced sparse features, Dictionary
    Learning (DL) layers are proposed to replace FC layers. (c) In CFA [[24](#bib.bib24)],
    Sun et al. bypassed four directional LSTM layers on the CONV maps to capture contextual
    information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the generic CNN models, *i.e.,* pre-trained CNN models and the
    fine-tuned CNN models, another group of deep models are specifically designed
    for scene classification. These models are specifically developed to extract effective
    scene representations from the input by introducing new network architectures.
    As is shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1.3 Specific CNN Model ‣ 3.1 Main
    CNN Framework ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification:
    A Survey"), we only show four typical specific models [[22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24), [26](#bib.bib26)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'To capture discriminative information from regions of interest, Zhou et al. [[23](#bib.bib23)]
    replaced the FC layers in a CNN model with a Global Average Pooling (GAP) layer [[90](#bib.bib90)]
    followed by a Softmax layer, *i.e.,* GAP-CNN. As shown in Fig. [6](#S3.F6 "Figure
    6 ‣ 3.1.3 Specific CNN Model ‣ 3.1 Main CNN Framework ‣ 3 Deep Learning based
    Methods ‣ Deep Learning for Scene Classification: A Survey") (a), by a simple
    combination of the original GAP layer and the $1\times 1$ convolution operation
    to form a class activation map (CAM), GAP-CNN can focus on class-specific regions
    and perform scene classification well. Although the GAP layer has a lower number
    of parameters than the FC layer [[23](#bib.bib23), [48](#bib.bib48)], the GAP-CNN
    can obtain comparable classification accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hypothesizing that a certain amount of sparsity improves the discriminability
    of the feature representations [[91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93)],
    Liu et al. [[22](#bib.bib22)] proposed a sparsity model named Dictionary Learning
    CNN (DL-CNN), seen in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1.3 Specific CNN Model ‣ 3.1
    Main CNN Framework ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification:
    A Survey") (b). They replaced FC layers with new dictionary learning layers, which
    are composed of a finite number of recurrent units that correspond to iteration
    processes in the Approximate Message Passing [[94](#bib.bib94)]. In particular,
    these dictionary learning layers’ parameters are updated through back-propagation
    in an end-to-end manner.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the CONV layers perform local operations on small patches of the image,
    they are not able to explicitly describe the contextual relation between different
    regions of the scene image. To address this limitation, Sun et al. [[24](#bib.bib24)]
    proposed Contextual features in Appearance (CFA) based on LSTM [[95](#bib.bib95)].
    As shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1.3 Specific CNN Model ‣ 3.1 Main CNN
    Framework ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification:
    A Survey") (c), CONV feature maps are regarded as the input of LSTM layers, which
    is transformed into four directed sequences in an acyclic way. Finally, LSTM layers
    are used to describe spatial contextual dependencies, and the output of four LSTM
    modules are concatenated to describe contextual relations in appearance.'
  prefs: []
  type: TYPE_NORMAL
- en: Sequential operations of CONV and FC layers in standard CNNs retain the global
    spatial structure of the image, which shows global features are sensitive to geometrical
    variations [[28](#bib.bib28), [96](#bib.bib96)], *e.g.,* object translations and
    rotation directly affect the obtained deep features, which drastically limits
    the application of these features for scene classification. To achieve geometric
    invariance, Hayat et al. [[26](#bib.bib26)] designed a spatial unstructured layer
    via shuffling the original position of the original feature maps by swapping adjacent
    diagonal image blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Back-propagation algorithm is the essence of CNN training. It is the practice
    of fine-tuning the weights of a neural net based on the error rate (*i.e.,* loss)
    obtained in the previous epoch (*i.e.,* iteration). Proper tuning of the weights
    ensures lower error rates, making the model reliable by increasing its generalization.
    Therefore, many approaches [[67](#bib.bib67), [96](#bib.bib96), [34](#bib.bib34),
    [37](#bib.bib37)] developed new layers with parameters that can be updated via
    back-propagation. The end-to-end system is trained via back-propagation in a holistic
    manner, which has been proved as a powerful training manner in various domains,
    and scene classification is no exception. Many methods [[67](#bib.bib67), [34](#bib.bib34),
    [31](#bib.bib31), [96](#bib.bib96), [37](#bib.bib37)] are training in an end-to-end
    manner. According to our investigations, theoretically, these models can learn
    more discriminative information through end-to-end optimization; however, the
    optimization results may fall into bad local optima [[97](#bib.bib97), [96](#bib.bib96)],
    so methods training in a multi-stage manner may achieve better results in some
    case.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 CNN based Scene Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Scene representation, the core of scene classification, has been the focus
    of this research. Hence, many methods have been put forward for effective scene
    representations, broadly divided into five categories: global CNN features, spatially
    invariant features, semantic features, multi-layer features, and combined features,
    *i.e.,* multi-view features.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7077fa6ad1ab73b0500100d5fedf5876.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Five typical architectures to extract CNN based scene representations
    (see Section [3.2](#S3.SS2 "3.2 CNN based Scene Representation ‣ 3 Deep Learning
    based Methods ‣ Deep Learning for Scene Classification: A Survey")), respectively.
    Hourglass architectures are backbone networks, such as AlexNet or VGGNet. (a)
    HLSTM [[27](#bib.bib27)], a global CNN feature based method, extracts deep feature
    from the whole image. Spatial LSTM is used to model 2D characteristics among the
    spatial layout of image regions. Moreover, Zuo et al. captured cross-scale contextual
    dependencies via multiple LSTM layers. (b) SFV [[30](#bib.bib30)], a spatially
    invariant feature based method, extract local features from dense patches. The
    highlight of SFV is to add a natural parameterization to transform the semantic
    space into a natural parameter space. (c) WELDON [[34](#bib.bib34)], a semantic
    feature based method, extracts deep features from top evidence (red) and negative
    instances (yellow). In WSP scheme, Durand et al. used the max layer and min layer
    to select positive and negative instances, respectively. (d) FTOTLM [[21](#bib.bib21)],
    a typical multi-layer feature based method, extracts deep feature from each residual
    block. (e) Scale-specific network [[19](#bib.bib19)], a multi-view feature based
    architecture, used scene-centric CNN extract deep features from coarse versions,
    while object-centric CNN is used to extract features from fine patches. Two types
    of deep features complement each other.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Global CNN feature based Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Global CNN feature based methods directly predict the probabilities of scene
    categories from the whole scene image. Frequently, global CNN features are extracted
    from input images via generic CNN models, pre-trained on large-scale datasets
    (*e.g.,* ImageNet [[56](#bib.bib56)] and Places [[18](#bib.bib18), [25](#bib.bib25)]),
    or then fine-tuned on target datasets (*e.g.,* SUN397 [[57](#bib.bib57)] and MIT67 [[70](#bib.bib70)]).
    Owing to the available large datasets and powerful computing resources (e.g.,
    GPUs and parallel computing clusters) [[98](#bib.bib98)], deep networks have been
    developed into deeper and more complicated, and thus global representations from
    these networks are able to achieve a more advanced performance on many applications
    including scene classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Except for generic CNNs, some scene-specific CNNs are designed to extract global
    features. For instance, as shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.2 CNN based
    Scene Representation ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene
    Classification: A Survey") (a), Zuo et al. [[27](#bib.bib27)] proposed Hierarchical
    LSTM (HLSTM) to describe the contextual relation. They treated CONV maps as an
    undirected graph, which is transformed into four directed acyclic graphs, and
    LSTM modules are used to capture spatial contextual dependencies in an acyclic
    way. They also explored the potential spatial dependencies among different scale
    CONV maps, so HLSTM features not only involve the relations within the same feature
    maps but also contain the contextual dependencies among different scales. In addition,
    Liu et al. [[22](#bib.bib22)] proposed DL-CNN model to extract sparse global features
    from entire scene image. Xie et al. [[99](#bib.bib99)] presented InterActive,
    a novel global CNN feature extraction algorithm which integrates high-level visual
    context with low-level neuron responses. InterActive increases the receptive field
    size of low-level neurons by allowing the supervision of the high-level neurons.
    Hayat et al. [[26](#bib.bib26)] designed a spatial unstructured layer to address
    the challenges of large-scale spatial layout deformations and scale variations.
    Along this way, Xie et al. [[100](#bib.bib100)] designed a Reversal Invariant
    Convolution (RI-Conv) layer so that they can obtain the identical representation
    for an image and its left-right reversed copy. Nevertheless, global CNN feature
    based methods have not fully exploited the underlying geometric and appearance
    variability of scene images.'
  prefs: []
  type: TYPE_NORMAL
- en: The performance of global CNN features is greatly affected by the content of
    the input image. CNN models can extract generic global feature representations
    once trained on a sufficiently large and rich training dataset, as opposed to
    handcrafted feature extraction methods. It is noteworthy that global representations
    obtained by scene-centric CNN models yield more enriched spatial information than
    those obtained using object-centric CNN models, arguably since global representations
    from scene-centric CNNs contain spatial correlations between objects and global
    scene properties [[18](#bib.bib18), [25](#bib.bib25), [19](#bib.bib19)]. In addition,
    Herranz et al. [[19](#bib.bib19)] showed that the performance of a scene recognition
    system depends on the entities in the scene image, *i.e.,* when the global features
    are extracted from images with chaotic background, the model’s performance is
    degraded compared to the cases that the object is isolated from the background
    or the image has a plain background. This suggests that the background may introduce
    some noise into the feature that weakens the performance. Since contour symmetry
    provides a perceptual advantage when human observers recognize complex real-world
    scenes, Rezanejad et al. [[101](#bib.bib101)] studied global CNN features from
    the full image and only contour information and showed that the performance of
    the full image as input is better, because CNN captures potential information
    from images. Nevertheless, they still concluded that contour is an auxiliary clue
    to improve recognition accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Spatially Invariant Feature based Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To alleviate the problems caused by sequential operations in the standard CNN,
    a body of alternatives [[28](#bib.bib28), [82](#bib.bib82), [40](#bib.bib40)]
    proposed spatially invariant feature based methods to maintain spatial robustness.
    The “spatially invariant” means that the output features are robust against the
    geometrical variations of the input image [[96](#bib.bib96)].
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.2 CNN based Scene Representation
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey") (b),
    spatially invariant features are usually extracted from multiple local patches.
    The visualization of such a feature extraction process is shown in Fig. [5](#S2.F5
    "Figure 5 ‣ 2.2 Datasets ‣ 2 Background ‣ Deep Learning for Scene Classification:
    A Survey") (marked in green arrows). The entire process can be decomposed into
    five basic steps: 1) Local patch extraction: a given input image is divided into
    smaller local patches, which are used as the input to a CNN model, 2) Local feature
    extraction: deep features are extracted from either the CONV or FC layers of the
    model, 3) Codebook generation: this step is to generate a codebook with multiple
    codewords based on the extracted deep features from different regions of the image.
    The codewords usually are learned in an unsupervised way (*e.g.,* using GMM),
    4) Spatially invariant feature generation: given the generated codebook, deep
    features are encoded into a spatially invariant representation, and 5) Class prediction:
    the representation input is classified into a predefined scene category.'
  prefs: []
  type: TYPE_NORMAL
- en: As opposed to patch-based local feature extraction (each local feature is extracted
    from an original patch by independently using the CNN extractor), local features
    can also be extracted from the semantic CONV maps of a standard CNN [[29](#bib.bib29),
    [102](#bib.bib102), [103](#bib.bib103), [44](#bib.bib44)]. Specifically, since
    each cell (deep descriptor) of the feature map corresponds to one local image
    patch in the input image, each cell is regarded as a local feature. In this approach,
    the computation time is decreased, compared to independently processing of multiple
    spatial patches to obtain local features. For instance, Yoo et al. [[29](#bib.bib29)]
    replaced the FC layers with CONV layers to obtain large amount of local spatial
    features. They also used multi-scale CNN activations to achieve geometric robustness.
    Gao et al. [[102](#bib.bib102)] used a spatial pyramid to directly divide the
    activations into multi-level pyramids, which contain more discriminative spatial
    information.
  prefs: []
  type: TYPE_NORMAL
- en: The feature encoding technique, which aggregates the local features, is crucial
    in relating local features with the final feature representation, and it directly
    influences the accuracy and efficiency of the scene classification algorithms [[85](#bib.bib85)].
    Improved Fisher Vector (IFV) [[104](#bib.bib104)], Vector of Locally Aggregated
    Descriptors (VLAD) [[105](#bib.bib105)], and Bag-of-Visual-Word (BoVW) [[106](#bib.bib106)]
    are among the popular and effective encoding techniques that are used in deep
    learning based methods. For instance, many methods, like FV-CNN [[82](#bib.bib82)],
    MFA-FS [[42](#bib.bib42)], and MFAFVNet [[31](#bib.bib31)], apply IFV encoding
    to obtain the image embedding as spatially invariant representations, while MOP-CNN [[28](#bib.bib28)],
    SDO [[35](#bib.bib35)], *etc* utilize VLAD to cluster local features. Noteworthily,
    the codebook selection and encoding procedures result in disjoint training of
    the model. To this end, some works proposed networks that are trained in an end-to-end
    manner, *e.g.,* NetVLAD [[67](#bib.bib67)], MFAFVNet [[31](#bib.bib31)], and VSAD [[32](#bib.bib32)].
  prefs: []
  type: TYPE_NORMAL
- en: Spatially invariant feature based methods are efficient to achieve geometric
    robustness. Nevertheless, the sliding windows based paradigm requires multi-resolution
    scanning with fixed aspect ratios, which is not suitable for arbitrary objects
    with variable sizes or aspect ratios in the scene image. Moreover, using dense
    patches may introduce noise into the final representation, which decreases the
    classification accuracy. Therefore, extracting semantic features from salient
    regions of the scene image can circumvent these drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Semantic Feature based Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Processing all patches of the input image requires computational cost while
    yields redundant information. Object detection determines whether or not any instance
    of the salient regions is presented in an image [[107](#bib.bib107)]. Inspired
    by this, object detector based approaches allow identifying salient regions of
    the scene, which provide distinctive information about the context of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different methods have been put forward to effective saliency detection, such
    as selective search [[108](#bib.bib108)], unsupervised discovery [[109](#bib.bib109)],
    Multi-scale Combinatorial Grouping (MCG) [[110](#bib.bib110)], and object detection
    networks (*e.g.,* Fast RCNN [[51](#bib.bib51)], Faster RCNN [[52](#bib.bib52)],
    SSD [[111](#bib.bib111)], Yolo [[112](#bib.bib112), [113](#bib.bib113)]). For
    instance, since selective search combines the strengths of exhaustive search and
    segmentation, Liu et al. [[87](#bib.bib87)] used it to capture all possible semantic
    regions, and then used a pre-trained CNN to extract the feature maps of each region
    followed by a spatial pyramid to reduce map dimensions. Because the common objects
    or characteristics in different scenes lead to the commonality of different scenes,
    Cheng et al. [[35](#bib.bib35)] used a region proposal network [[52](#bib.bib52)]
    to extract the discriminative regions while discarde non-discriminative regions.
    These semantic feature based methods [[87](#bib.bib87), [35](#bib.bib35)] harvest
    many semantic regions, so encoding technology is adapted to aggregate key features,
    which pipeline is shown in Fig. [5](#S2.F5 "Figure 5 ‣ 2.2 Datasets ‣ 2 Background
    ‣ Deep Learning for Scene Classification: A Survey") (red arrows).'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, some semantic feature based methods [[33](#bib.bib33), [34](#bib.bib34)]
    are based on weakly supervised learning, which directly predicts categories by
    several semantic features of the scene. For instance, Wu et al. [[33](#bib.bib33)]
    generated high-quality proposal regions by using MCG [[110](#bib.bib110)], and
    then used SVM on each scene category to prune outliers and redundant regions.
    Semantic features from different scale patches supply complementary cues, since
    the coarser scales deal with larger objects, while the finer levels provide smaller
    objects or object parts. In practice, they found two semantic features sufficient
    to represent the whole scene, comparable to multiple semantic features. Training
    a deep model only using a single salient region may result in a suboptimal performance
    due to the possible existence of outliers in the training set. Hence, multiple
    regions can be selected to train the model together [[34](#bib.bib34)]. As shown
    in Fig. [7](#S3.F7 "Figure 7 ‣ 3.2 CNN based Scene Representation ‣ 3 Deep Learning
    based Methods ‣ Deep Learning for Scene Classification: A Survey") (c), Durand et
    al. [[34](#bib.bib34)] designed a Max layer to select the attention regions to
    enhance the discrimination. To provide a more robust strategy, they also designed
    a Min layer to capture the regions with the most negative evidence to further
    improve the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Although better performance can be obtained via using more semantic local features,
    semantic feature based methods deeply rely on the performance of object detection.
    Weak supervision settings (*i.e.,* without the patch labels of scene images) make
    it difficult to accurately identify the scene by the key information of an image [[34](#bib.bib34)].
    Moreover, the error accumulation problem and extra computation cost also limit
    the development of semantic feature based methods [[103](#bib.bib103)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Multi-layer Feature based Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Global feature based methods usually extract the high-layer CNN features, and
    feed them into a classifier to achieve classification task. Due to the compactness
    of such high-layer features, it is easy to miss some important slight clues [[40](#bib.bib40),
    [114](#bib.bib114)]. Features from different layers are complementary [[33](#bib.bib33),
    [115](#bib.bib115)]. Low-layer features generally capture small objects, while
    high-layer features capture big objects [[33](#bib.bib33)]. Moreover, semantic
    information of low-layer features is relatively less, but the object location
    is accurate [[115](#bib.bib115)]. To take full advantage of features from different
    layers, many methods [[39](#bib.bib39), [47](#bib.bib47), [21](#bib.bib21), [38](#bib.bib38)]
    used the high resolution features from the early layers along with the high semantic
    information of the features from the latest layers of hierarchical models (*e.g.,*
    CNNs).
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.2 CNN based Scene Representation
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey") (d),
    typical multi-layer feature formation process includes: 1) Feature extraction:
    the outputs (feature maps) of certain layers are extracted as deep features, 2)
    Feature vectorization: vectorize the extracted feature maps, 3) Multi-layer feature
    combination: multiple features from different layers are combined into a single
    feature vector, and 4) Feature classification: classify the given scene image
    based on the obtained combined feature.'
  prefs: []
  type: TYPE_NORMAL
- en: Although using all features from different layers seems to improve the final
    representation, it likely increases the chance of overfitting, and thus hurts
    performance [[37](#bib.bib37)]. Therefore, many methods [[38](#bib.bib38), [39](#bib.bib39),
    [21](#bib.bib21), [47](#bib.bib47), [37](#bib.bib37)] only extract features from
    certain layers. For instance, Xie et al. [[38](#bib.bib38)] constructed two dictionary-based
    representations, Convolution Fisher Vector (CFV), and Mid-level Local Discriminative
    Representation (MLR) to classify subsidiarily scene images. Tang et al. [[39](#bib.bib39)]
    divided GoogLeNet layers into three parts from bottom to top and extracted final
    feature maps of each part. Liu et al. [[21](#bib.bib21)] captured feature maps
    from each residual block from ResNet independently. Song et al. [[47](#bib.bib47)]
    selected discriminative combinations from different layers and different network
    branches via minimizing a weighted sum of the probability of error and the average
    correlation coefficient. Yang et al. [[37](#bib.bib37)] used greedily select to
    explore the best layer combinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature fusion in multi-layer feature based methods is another important direction.
    Feature fusion techniques are mainly divided into two groups [[116](#bib.bib116),
    [117](#bib.bib117), [118](#bib.bib118)]: 1) Early fusion: extracting multi-layer
    features and merging them into a comprehensive feature for scene classification,
    and 2) Late fusion: directly learning each multi-layer feature via a supervised
    learner, which enforces the features to be directly sensitive to the category
    label, and then merging them into a final feature. Although the performance of
    late fusion is better, it is more complex and time-consuming, so early fusion
    is more popular [[38](#bib.bib38), [39](#bib.bib39), [21](#bib.bib21), [47](#bib.bib47)].
    In addition, addition and product rules are usually applied to combine multiple
    features [[39](#bib.bib39)]. Since the feature spaces in different layers are
    disparate, product rule is better than addition rule to fusing features, and empirical
    experiments on [[39](#bib.bib39)] also show this statement. Moreover, Tang et
    al. [[39](#bib.bib39)] proposed two strategies to fuse multi-layer features, *i.e.,*
    ‘fusion with score’ and ‘fusion with features’. Fusion with score technique has
    obtained a better performance over fusion with feature thanks to the end-to-end
    training.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.5 Multiple-view Feature based Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Describing a complex scene just using a single and compact feature representation
    is a non-trivial task. Hence, there has been extensive effort to compute a comprehensive
    representation of a scene by integrating multiple features generated from complementary
    CNN models [[24](#bib.bib24), [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121),
    [41](#bib.bib41), [32](#bib.bib32)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Features generated from *networks trained on different datasets* usually are
    complementary. As shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.2 CNN based Scene Representation
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey")
    (e), Herranz et al.[[19](#bib.bib19)] found the best scale response of object-centric
    CNNs and scene-centric CNNs, and they combine the knowledge in a scale-adaptive
    way via either object-centric CNNs or scene-centric CNNs. This finding is widely
    used [[121](#bib.bib121), [122](#bib.bib122)]. For instance, the authors in [[121](#bib.bib121)]
    used an object-centric CNN to carry information about object depicted in the image,
    while a scene-centric CNN was used to capture global scene information. Along
    this way, Wang et al. [[32](#bib.bib32)] designed PatchNet, a weakly supervised
    learning method, which uses image-level supervision information as the supervision
    signal for effective extraction of the patch-level features. To enhance the recognition
    performance, Scene-PatchNet and Object-PatchNet jointly used to extract features
    for each patch.'
  prefs: []
  type: TYPE_NORMAL
- en: Employing *complementary CNN architectures* is essential for obtaining discriminative
    multi-view feature representations. Wang et al. [[41](#bib.bib41)] proposed a
    multi-resolution CNN (MR-CNN) architecture to capture visual content in multiple
    scale images. In their work, normal BN-Inception [[123](#bib.bib123)] is used
    to extract coarse resolution features, while deeper BN-Inception is employed to
    extract fine resolution features. Jin et al. [[124](#bib.bib124)] used global
    features and spatially invariant features to account for both the coarse layout
    of the scene and the transient objects. Sun et al. [[24](#bib.bib24)] separately
    extracted three representations, *i.e.,* object semantics representation, contextual
    information, and global appearance, from discriminative views, which are complementarity
    to each other. Specifically, the object semantic features of the scene image are
    extracted by a CNN followed by spatial fisher vectors, while the deep feature
    of a multi-direction LSTM-based model represents contextual information, and the
    FC feature represents global appearance. Li et al. [[119](#bib.bib119)] used ResNet18 [[79](#bib.bib79)]
    to generate discriminative attention maps, which is used as an explicit input
    of CNN together with the original image. Using global features extracted by ResNet18
    and attention map features extracted from the spatial feature transformer network,
    the attention map features are multiplied to the global features for adaptive
    feature refinement so that the network focuses on the most discriminative parts.
    Later, a multi-modal architecture is proposed in [[43](#bib.bib43)], composed
    of a deep branch and a semantic branch. The deep Branch extracts global CNN features,
    while semantic branch aims to extract meaningful scene objects and their relations
    from super pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Strategies for Improving Scene Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To obtain more discriminative representations for scene classification, a range
    of strategies has been proposed. Four major categories (*i.e.,* encoding strategy,
    attention strategy, contextual strategy, and regularization strategy) will be
    discussed below.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Encoding strategy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although the current driving force has been the incorporation of CNNs, encoding
    technology of the first generation methods have also been adapted in deep learning
    based methods. Fisher Vector (FV) coding [[125](#bib.bib125), [104](#bib.bib104)]
    is an encoding technique commonly used in scene classification. Fisher vector
    stores the mean and the covariance deviation vectors per component of the GMM
    and each element of the local features together. Thanks to the covariance deviation
    vectors, FV encoding leads to excellent results. Moreover, it is empirically proven
    that Fisher vectors are complementary to global CNN features [[30](#bib.bib30),
    [42](#bib.bib42), [46](#bib.bib46), [38](#bib.bib38), [40](#bib.bib40), [31](#bib.bib31)].
    Therefore, this survey takes FV-based approaches as the major cue and discusses
    the adapted combination of encoding technology and deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fd5ff5d4e394008366b26b7da35e3f32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Structure comparisons of (a) basic Fisher vector [[82](#bib.bib82)]
    and its variations. BoF denotes bag of features, while BoS represents bag of semantic
    probabilities. (b) In semantic FV [[30](#bib.bib30)], natural parameterization
    is added to map multinomial distribution (*i.e.,* $\pi$) to its natural parameter
    space (*i.e.,* $\nu$). (c) In MFAFVNet [[31](#bib.bib31)], GMM is replaced with
    MFA to build codebook. (d) In VSAD [[32](#bib.bib32)], codebook is constructed
    via exploiting semantics (*i.e.,* BoS) to aggregate local features (*i.e.,* BoF).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, CONV features and FC features are regarded as Bags of Features (BoF),
    they can be readily modeled by the Gaussian Mixture Model followed by Fisher Vector
    (GMM-FV) [[30](#bib.bib30), [31](#bib.bib31)]. To avoid the computation of the
    FC layers, Cimpoi et al. [[82](#bib.bib82)] utilized GMM-FV to aggregate BoF from
    different CONV layers, respectively. Comparing their experiment results, they
    asserted that the last CONV features can more effectively represent scenes. To
    rescue the fine-grained information of early/middle layers, Guo et al. [[40](#bib.bib40)]
    proposed Fisher Convolutional Vector (FCV) to encode the feature maps from multiple
    CONV layers. Wang et al. [[46](#bib.bib46)] extracted the feature maps from RGB,
    HHA, and surface normal images, and then directly encoded these maps by FV coding.
    In addition, through the performance comparisons of GMM-FV encoding on CONV features
    and FC features, respectively, Dixit et al. [[30](#bib.bib30)] asserted that the
    FC features is more effective for scene classification. However, since the CONV
    features and FC features do not derive from semantic probability space, it is
    likely to be both less discriminant and less abstract than the truly semantic
    features  [[82](#bib.bib82), [30](#bib.bib30)]. The activations of Softmax layer
    are probability vectors, inhabiting the probability simplex, which are more abstract
    and semantic, but it is difficult to implement an effective invariant coding (*e.g.,*
    GMM-FV) [[126](#bib.bib126), [30](#bib.bib30)]. To this end, Dixit et al. [[30](#bib.bib30)]
    proposed an indirect FV implementation to aggregate these semantic probability
    features, *i.e.,* adding a step to convert semantic multinomials from probability
    space to the natural parameter space, as shown in Fig. [8](#S3.F8 "Figure 8 ‣
    3.3.1 Encoding strategy ‣ 3.3 Strategies for Improving Scene Representation ‣
    3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey") (b).
    Inspired by FV and VLAD, Wang et al. [[32](#bib.bib32)] proposed Vector of Semantically
    Aggregated Descriptors (VSAD) to encode the probability features, as shown in
    Fig. [8](#S3.F8 "Figure 8 ‣ 3.3.1 Encoding strategy ‣ 3.3 Strategies for Improving
    Scene Representation ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene
    Classification: A Survey") (d). Comparing the discriminant probability learned
    by the weakly-supervised method (PatchNet) with the generative probability from
    an unsupervised method (GMM), the results show that the discriminant probability
    is more expressive in aggregating local features. From the above discussion, representation
    encoding local features on probability space outperforms that on non-probability
    space.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep features usually are high dimensional ones. Therefore, more Gaussian kernels
    are needed to accurately model the feature space [[83](#bib.bib83)]. However,
    this would a lot of overhead to the computations and, hence, it is not efficient.
    Liu et al. [[83](#bib.bib83)] empirically proved that the discriminative power
    of FV features increases slowly as the number of Gaussian kernels increases. Therefore,
    dimensionality reduction of the features is very important, as it directly affects
    the computational efficiency. A wide range of approaches [[83](#bib.bib83), [30](#bib.bib30),
    [29](#bib.bib29), [58](#bib.bib58), [40](#bib.bib40), [42](#bib.bib42), [32](#bib.bib32)]
    used poplular dimensionality reduction techniques, Principal Component Analysis
    (PCA), for pre-processing of the local features. Moreover, Liu et al. [[83](#bib.bib83)]
    drew local features from Gaussian distribution with a nearly zero mean, which
    ensures the sparsity of the resulting FV. Wang et al. [[46](#bib.bib46)] enforced
    intercomponent sparsity of GMM-FV features via component regularization to discount
    unnecessary components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the non-linear property of deep features and a limited ability of the
    covariance of GMM, a large number of diagonal GMM components are required to model
    deep features so that the FV has very high dimensions [[42](#bib.bib42), [31](#bib.bib31)].
    To address this issue, Dixit et al. [[42](#bib.bib42)] proposed MFA-FS, in which
    GMM is replaced by Mixtures of Factor Analysis (MFA) [[127](#bib.bib127), [128](#bib.bib128)],
    *i.e.,* a set of local linear subspaces is used to capture non-linear features.
    MFA-FS performs well but does not support end-to-end training. However, end-to-end
    training is more efficient than any disjoint training process [[31](#bib.bib31)].
    Therefore, Li et al. [[31](#bib.bib31)] proposed MFAFVnet, an improved variant
    of MFA-FS [[42](#bib.bib42)], which is conveniently embedded into the state-of-the-art
    network architectures. Fig. [8](#S3.F8 "Figure 8 ‣ 3.3.1 Encoding strategy ‣ 3.3
    Strategies for Improving Scene Representation ‣ 3 Deep Learning based Methods
    ‣ Deep Learning for Scene Classification: A Survey") (c) shows the MFA-FV layer
    of MFAFVNet, compared with the other two structures.'
  prefs: []
  type: TYPE_NORMAL
- en: In FV coding, local features are assumed to be independent and identically distributed
    (iid), which violates intrinsic image attributes that these patches are not iid.
    To this end, Cinbis et al. [[58](#bib.bib58)] introduced a non-iid model via treating
    the model parameters as latent variables, rendering features related locally.
    Later, Wei et al. [[129](#bib.bib129)] proposed a correlated topic vector, treated
    as an evolution oriented from Fisher kernel framework, to explore latent semantics,
    and consider semantic correlation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Attention strategy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As opposed to semantic feature based methods (focusing on key cues generally
    from original images), attention mechanism aims to capture distinguishing cues
    from the extracted feature space [[96](#bib.bib96), [122](#bib.bib122), [44](#bib.bib44),
    [43](#bib.bib43)]. The attention maps are learned without any explicit training
    signal, rather task-related loss function alone provides the training signal for
    the attention weights. Generally, attention policy mainly includes channel attention
    and spatial attention.
  prefs: []
  type: TYPE_NORMAL
- en: Channel attention policy. Channel activation maps (ChAMs) generated from attention
    policy refers to the weighted activation maps, which highlights the class-specific
    discriminative regions. For instance, class activation map [[23](#bib.bib23)]
    is a simple ChAM, widely used in many works [[130](#bib.bib130), [131](#bib.bib131)].
    Since the same semantic cue has different roles for different types of scenes
    in some cases, Li et al. [[96](#bib.bib96)] designed class-aware attentive pooling,
    including intra-modality attentive pooling and cross-aware attentive pooling,
    to learn the contributions of RGB and depth modalities, respectively. Here the
    attention strategies are also used to further fuse the learned discriminate semantic
    cues across RGB and depth modalities. Moreover, they also designed a class-agnostic
    attentive pooling to ignore some salient regions that may mislead classification.
    Inspired by the idea that specific objects are associated with a scene, Seong et
    al. [[132](#bib.bib132)] proposed correlative context gating to activate scene-specific
    object features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Channel attention maps can also be computed from different sources of information.
    With multiple salient regions on different scales as input, Xia et al. [[122](#bib.bib122)]
    designed a Weakly Supervised Attention Map (WS-AM) by proposing a gradient-weighted
    class activation mapping technique and privileging weakly supervised information.
    In another work [[43](#bib.bib43)], the input of semantic branch is a semantic
    segmentation score map, and semantic features are extracted from semantic branch
    via using three channel attention modules, shown in Fig. [9](#S3.F9 "Figure 9
    ‣ 3.3.2 Attention strategy ‣ 3.3 Strategies for Improving Scene Representation
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey")
    (a). Moreover, semantic features are used to gate global CNN features via another
    attention module.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/21c2282a78ec20d87efe33eec0f7f1c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Illustrations of two typical attentions. (a) In channel attention [[43](#bib.bib43)],
    the channel attention map is used to weight the input by a Hadamard product. (b)
    Spatial attention in [[44](#bib.bib44)] is used to enhance the local feature selection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spatial attention policy. Spatial attention policy infers attention maps along
    height and width of input feature maps, then the attention maps are combined to
    original maps for adaptive feature refinement. Joseph et al. [[133](#bib.bib133)]
    proposed a layer-spatial attention model, including a hard attention to select
    a CNN layer and a soft attention to achieve spatial localization within the selected
    layer. Attention maps are obtained from a Conv-LSTM architecture, where the layer
    attention uses the previous hidden states, and spatial attention uses both the
    selected layer and the previous hidden states. To enhance local feature selection,
    Xiong et al. [[103](#bib.bib103), [44](#bib.bib44)] designed a spatial attention
    module, shown in Fig. [9](#S3.F9 "Figure 9 ‣ 3.3.2 Attention strategy ‣ 3.3 Strategies
    for Improving Scene Representation ‣ 3 Deep Learning based Methods ‣ Deep Learning
    for Scene Classification: A Survey") (b), to generate attention masks. This attention
    masks of RGB and depth modalities are encouraged to be similar, and then learn
    the modal-consistent features.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 Contextual strategy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Contextual information (the correlations among image regions, and local features),
    and objects/scenes may provide beneficial information in disambiguating visual
    words [[134](#bib.bib134)]. However, convolution and pooling kernels are locally
    performed on image regions separately, and encoding technologies usually integrate
    multiple local features into an unstructured feature. As a result, contextual
    correlations among different regions have not been taken into account [[135](#bib.bib135)].
    To this end, contextual correlations have been further explored to focus on the
    global layout or local region coherence [[136](#bib.bib136)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The contextual relations can broadly be grouped into two major categories:
    1) spatial contextual relation: the correlations of neighboring regions, in which
    capturing spatial contextual relation usually encounters the problem of incomplete
    regions or noise caused by predefined grid patches, and 2) semantic contextual
    relation: the relations of salient regions. The network to extract semantic relations
    is often a two-stage framework (*i.e.,* detecting objects and classifying scenes).
    Therefore, accuracy is also influenced by object detection. Generally, there are
    three types of algorithms to capture contextual relations: 1) sequential model,
    like RNN [[137](#bib.bib137)] and LSTM [[95](#bib.bib95)], and 2) graph-related
    model, such as Markov Random Field (MRF) [[138](#bib.bib138), [139](#bib.bib139)],
    Correlated Topic Model (CTM) [[140](#bib.bib140), [129](#bib.bib129)] and graph
    convolution [[141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b7c9c60de3a83aec0a5bb7532959e1d4.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Sequential model
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/381360d70354bcb4287d0aa3922d426a.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Graph-related model
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Illustrations of a sequence model and a graph model to related contextual
    information. (a) Local feature is extracted from each salient region via a CNN,
    and a bidirectional LSTM is used to model synchronously many-to-many local feature
    relations [[36](#bib.bib36)]. (b) Graph is constructed by assigning selected key
    features to graph nodes (including the center nodes, sub-center nodes and other
    nodes) [[144](#bib.bib144)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequential Model. With the success of sequential models, such as RNN and LSTM,
    capturing the sequential information among local regions has shown promising performance
    for scene classification [[36](#bib.bib36)]. Spatial dependencies are captured
    from direct or indirect connections between each region and its surrounding neighbors.
    Zuo et al. [[27](#bib.bib27)] stacked multi-directional LSTM layers on the top
    of CONV feature maps to encode spatial contextual information in scene images.
    Furthermore, a hierarchical strategy was adopted to capture cross-scale contextual
    information. Like the work [[27](#bib.bib27)], Sun et al. [[24](#bib.bib24)] bypassed
    two sets of multi-directional LSTM layers on the CONV feature maps. In their framework,
    the outputs of all LSTM layers are concatenated to form a contextual representation.
    In addition, the works [[145](#bib.bib145), [36](#bib.bib36)] captured semantic
    contextual knowledge from variable salient regions. In [[145](#bib.bib145)], two
    types of representations, *i.e.,* COOR and SOOR, are proposed to describe object-to-object
    relations. Herein, COOR adapts the co-occurring frequency to represent the object-to-object
    relations, while SOOR is encoded with sequential model via regarding object sequences
    as sentences. Rooted in the work of Javed et al. [[146](#bib.bib146)], Laranjeira et
    al. [[36](#bib.bib36)] proposed a bidirectional LSTM to capture the contextual
    relations of regions of interest, as shown in Fig. [10](#S3.F10 "Figure 10 ‣ 3.3.3
    Contextual strategy ‣ 3.3 Strategies for Improving Scene Representation ‣ 3 Deep
    Learning based Methods ‣ Deep Learning for Scene Classification: A Survey") (a).
    Their model supports variable length sequences, because the number of object parts
    of each image are different.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph-related Model. The sequential models often simplify the contextual relations,
    while graph-related models can explore more complicated structural layouts. Song et
    al. [[147](#bib.bib147)] proposed a joint context model that uses MRFs to combine
    multiple scales, spatial relations, and multiple features among neighboring semantic
    multinomials, showing that this method can discover consistent co-occurrence patterns
    and filter out noisy ones. Based on CTM, Wei et al. [[129](#bib.bib129)] captured
    relations among latent themes as a semantic feature, *i.e.,* corrected topic vector
    (CTV). Later, with the development of Graph Neural Network (GNN) [[142](#bib.bib142),
    [143](#bib.bib143)], graph convolution has become increasingly popular to model
    contextual information for scene classification. Yuan et al. [[144](#bib.bib144)]
    used spectral graph convolution to mine the relations among the selected local
    CNN features, as shown in Fig. [10](#S3.F10 "Figure 10 ‣ 3.3.3 Contextual strategy
    ‣ 3.3 Strategies for Improving Scene Representation ‣ 3 Deep Learning based Methods
    ‣ Deep Learning for Scene Classification: A Survey") (b). To use the complementary
    cues of multiple modalities, Yuan et al. also considered the inter-modality correlations
    of RGB and depth modalities through a cross-modal graph. Chen et al. [[45](#bib.bib45)]
    used graph convolution [[148](#bib.bib148)] to model the more complex spatial
    structural layouts via pre-defining the features of discriminative regions as
    graph nodes. However, the spatial relation overlooks the semantic meanings of
    regions. To address this issue, Chen et al. also defined a similarity subgraph
    as a complement to the spatial subgraph.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.4 Regularization strategy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The training classifier not only requires a classification loss function, but
    it may also need multi-task learning with different regularization terms to reduce
    generalization error. The regularization strategies for scene classification mainly
    include sparse regularization, structured regularization, and supervised regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Regularization. Sparse regularization is a technique to reduce the complexity
    of the model to prevent overfitting and even improve generalization ability. Many
    works [[87](#bib.bib87), [149](#bib.bib149), [40](#bib.bib40), [22](#bib.bib22),
    [150](#bib.bib150)] include $\ell_{0}$, $\ell_{1}$, or $\ell_{2}$ norms to the
    base loss function for learning sparse features. For example, the sparse reconstruction
    term in [[87](#bib.bib87)] encourages the learned representations to be significantly
    informative. The loss in [[22](#bib.bib22)] combines the strength of the Mahalanobis
    and Euclidean distances to balance the accuracy and the generalization ability.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Regularization. Minimizing the triplet loss function minimizes the
    distance between the anchor and positive features with the same class labels while
    maximizing the distance between the anchor and negative features with one different
    class labels. In addition, according to the maximum margin theory in learning
    [[151](#bib.bib151)], hinge distance focus on the hard training samples. Hence,
    many research efforts [[149](#bib.bib149), [50](#bib.bib50), [152](#bib.bib152),
    [103](#bib.bib103), [44](#bib.bib44)] have utilized structured regularization
    of the triplet loss with hinge distance to learn robust feature representations.
    The structured regularization term is $\sum_{a,p,n}{max(d(x_{a},x_{p})-d(x_{a},x_{n})+\alpha,0)}$,
    where $x_{a},x_{p},x_{n}$ are anchor, positive, negative features, and $\alpha$
    is an adjustable parameter, while the function $d(x,y)$ denotes calculating a
    distance of $x$ and $y$. The structured regularization term promotes exemplar
    selection, while it also ignores noisy training examples that might overwhelm
    the useful discriminative patterns[[149](#bib.bib149), [103](#bib.bib103)].
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Regularization. Supervised regularization uses the label information
    for tuning the intermediate feature maps. The supervised regularization is generally
    expressed in terms of $\sum_{i}{d(y_{i},f(x_{i}))}$, where $x_{i}andy_{i}$ denote
    the middle-layer activated features and real label of the image $i$, respectively,
    and $f(x_{i})$ is a predicted label. For example, Guo et al. [[40](#bib.bib40)]
    utilized an auxiliary loss to directly propagate the label information to the
    CONV layers, and thus accurately captures the information of local objects and
    fine structures in the CONV layers. Similarly, these alternatives [[103](#bib.bib103),
    [144](#bib.bib144), [44](#bib.bib44)] used supervised regularization to learn
    modal-specific features.
  prefs: []
  type: TYPE_NORMAL
- en: Others. Extracting discriminative features by incorporating different regularization
    techniques has been always a mainstream topic in scene classification. For example,
    label consistent regularization [[87](#bib.bib87)] guarantees that inputs from
    different categories have discriminative responses. The shareable constraint in
    [[149](#bib.bib149)] can learn a flexible number of filters to represent common
    patterns across different categories. Clustering loss in [[124](#bib.bib124)]
    is utilized to further fine-tune confusing clusters to overcome the intra-class
    variation issues inherent. Since assigning soft labels to the samples cause a
    degree of ambiguity, which reaps high benefits when increasing the number of scene
    categories [[153](#bib.bib153)], Wang et al. [[41](#bib.bib41)] improved generalization
    ability by exploiting soft labels contained in knowledge networks as a bias term
    of the loss function. Noteworthily, optimizing proper loss function can pick up
    effective patches for image classification. In fast RCNN [[51](#bib.bib51)] and
    Faster RCNN [[52](#bib.bib52)], regression loss is used to learn effective region
    proposals. Wu et al. [[33](#bib.bib33)] adopted one-class SVMs [[154](#bib.bib154)]
    as discriminative models to get meta-objects. Inspired by MANTRA [[155](#bib.bib155)],
    the main intuition in [[34](#bib.bib34)] is to equip each possible output with
    pairs of latent variables, *i.e.,* top positive and negative patches, via optimizing
    max$+$min prediction problem.
  prefs: []
  type: TYPE_NORMAL
- en: Nearly all multi-task learning approaches using regularization aim to find a
    trade-off among conflicting requirements, *e.g.,* accuracy, generalization robustness,
    and efficiency. Researchers apply completely different supervision information
    to a variety of auxiliary tasks in an effort to facilitate the convergence of
    the major scene classification task [[40](#bib.bib40)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 RGB-D Scene Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RGB modality provides the intensity of the colors and texture cues, while depth
    modality carries information regarding the distance of the scene surfaces from
    a viewpoint. The depth information is invariant to lighting and color variations,
    and contains geometrical and shape cues, which is useful for scene representation
    [[156](#bib.bib156), [157](#bib.bib157), [158](#bib.bib158)]. Moreover, HHA data [[73](#bib.bib73)],
    an encoding result of depth image, depth information presents a certain color
    modality, which somewhat is similar to the RGB image. Hence, some CNNs trained
    on RGB images can transfer their knowledge and be used on HHA data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The depth information of RGB-D image can further improve the performance of
    CNN models compared to RGB images [[89](#bib.bib89)]. For the task of RGB-D scene
    classification, except for exploring suitable RGB features, described in Section [3.2](#S3.SS2
    "3.2 CNN based Scene Representation ‣ 3 Deep Learning based Methods ‣ Deep Learning
    for Scene Classification: A Survey"), there exists another two main problems,
    *i.e.,* 1) how to extract depth-specific features and 2) how to properly fuse
    features of RGB and depth modalities.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Depth-specific feature learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Depth information is usually scarce compared to RGB data. Therefore, it is non-trivial
    to train CNNs only on limited depth data to achieve depth-specific models [[89](#bib.bib89)],
    *i.e.,* depth-CNN. Hence, different training strategies are employed to train
    CNNs using limited amount of depth images.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning RGB-CNNs for depth images. Due to the availability of RGB data,
    many models [[49](#bib.bib49), [50](#bib.bib50), [46](#bib.bib46)] are first pre-trained
    on large-scale RGB datasets, such as ImageNet and Places, followed by fine-tuning
    on depth data. Fine-tuning only updates the last few FC layers, while the parameters
    of the previous layers are not adjusted. Therefore, the fine-tuned model’s layers
    do not fully leverage depth data [[89](#bib.bib89)]. However, abstract representations
    of early CONV layers play a crucial role in computing deep features using different
    modalities. Weakly-supervised learning and semi-supervised learning can enforce
    explicit adaptation in the previous layers.
  prefs: []
  type: TYPE_NORMAL
- en: Weak-supervised learning with patches of depth images. Song et al. [[89](#bib.bib89),
    [47](#bib.bib47)] proposed to learn depth features from scratch using weakly supervised
    learning. Song et al. [[89](#bib.bib89)] pointed out that the diversity and complexity
    of patterns in the depth images are significantly lower than those in the RGB
    images. Therefore, they designed a Depth-CNN (DCNN) with fewer layers for depth
    features extraction. They also trained the DCNN by three strategies of freezing,
    fine-tuning, and training from scratch to adequately capture depth information.
    Nevertheless, weakly-supervised learning is sensitive to the noise in the training
    data. As a result, the extracted features may not have good discriminative quality
    for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised learning with unlabeled images. Due to the convenient collection
    of unlabeled RGB-D data, semi-supervised learning can also be employed in the
    training of CNNs with a limited number of labeled samples compared to very large
    size of unlabeled data [[48](#bib.bib48), [158](#bib.bib158)]. Cheng et al. [[158](#bib.bib158)]
    trained a CNN using a very limited number of labeled RGB-D images while a massive
    amount of unlabeled RGB-D images via a co-training algorithm to preserve diversity.
    Subsequently, Du et al. [[48](#bib.bib48)] developed an encoder-decoder model
    to construct paired complementary-modal data of the input. In particular, the
    encoder is used as a modality-specific network to extract specific features for
    the subsequent classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Multiple modality fusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/df74223bf5ed4d585286655de016832d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Illustrations of multi-modal feature learning. (a) Three popular
    ways to achieve feature combination: directly concatenate features, combine weighted
    features and combine features with linear converting. (b) Three methods to achieve
    modal-consistent: minimize the pairwise distances between modalities [[50](#bib.bib50)];
    encourage the attention maps of modalities similar [[103](#bib.bib103)]; minimize
    the distances between modalities [[44](#bib.bib44), [152](#bib.bib152)]. (c) Three
    strategies to achieve modal-distinctive: learn the model structure via triplet
    loss [[50](#bib.bib50), [152](#bib.bib152)]; use label information to guide modal-specific
    learning [[103](#bib.bib103), [144](#bib.bib144)]; minimize cosine similarity
    between modalities [[44](#bib.bib44)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Various modality fusion methods [[159](#bib.bib159), [152](#bib.bib152), [44](#bib.bib44)]
    have been put forward to combine the information of different modalities to further
    enhance the performance of the classification model. The fusion strategies are
    mainly divided into three categories, *i.e.,* feature-level modal combination,
    consistent feature based fusion, and distinctive feature based fusion. Fig. [11](#S3.F11
    "Figure 11 ‣ 3.4.2 Multiple modality fusion ‣ 3.4 RGB-D Scene Classification ‣
    3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey")
    shows illustrations of the late three categories. Despite the existence of different
    fusion categories, some works[[46](#bib.bib46), [152](#bib.bib152), [44](#bib.bib44)]
    combine multiple fusion strategies to achieve better performance for scene classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Feature-level modal combination. Song et al. [[47](#bib.bib47)] proposed a multi-modal
    combination approach to select discriminative combinations of layers from different
    source models. They concatenated RGB and depth features for not losing the correlation
    between the RGB and depth data. Reducing the redundancy of features can significantly
    improve the performance when RGB and depth features have correlations; especially,
    in the case of extracting depth features merely via RGB-CNNs [[89](#bib.bib89)].
    Because of such correlation, direct concatenation of features may result in redundancy
    of information. To avoid this issue, Du et al. [[48](#bib.bib48)] performed global
    average pooling to reduce the feature dimensions after concatenating modality-specific
    features. Wang et al. [[46](#bib.bib46)] used the modality regularization based
    on exclusive group lasso to ensure feature sparsity and co-existence, while features
    within a modality are encouraged to compete. Li et al. [[96](#bib.bib96)] used
    an attention module to discern discriminative semantic cues from intra- and cross-modalities.
    Moreover, Cheng et al. [[49](#bib.bib49)] proposed a gated fusion layer to adjust
    the RGB and depth contributions on image pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Consistent-feature based modal fusion. Images may suffer from missing information
    or noise pollution so that multi-modal features are not consistent, hence it is
    essential to exploit the correlation between different modalities to exclude such
    issue. To drive feature consistency of different modalities, Zhu et al. [[50](#bib.bib50)]
    introduced an inter-modality correlation term to minimize pairwise distances of
    two modalities from the same class, while maximize pairwise distances from different
    classes. Zheng et al. [[160](#bib.bib160)] used multi-task metric learning to
    learn linear transformations of RGB and depth features, making full use of inter-modal
    relations. Li et al. [[152](#bib.bib152)] learned a correlative embedding module
    between the RGB and depth features inspired by Canonical Correlation Analysis [[161](#bib.bib161),
    [162](#bib.bib162)]. Xiong et al. [[103](#bib.bib103), [44](#bib.bib44)] proposed
    a learning approach to encourage two modal-specific networks to focus on features
    with similar spatial positions to learn more discriminative modal-consistent features.
  prefs: []
  type: TYPE_NORMAL
- en: Distinctive-feature based modal fusion. In addition to constructing multimodal
    consistent features, features can also be processed separately to increase discriminative
    capability. For instance, Li et al. [[152](#bib.bib152)] and Zhu et al. [[50](#bib.bib50)]
    adopted structured regularization in the triplet loss function, in which to encourage
    the model to learn the modal-specific features under the supervision of this regularization.
    Li et al. [[152](#bib.bib152)] designed a distinctive embedding module for each
    modality to learn distinctive features. Using labels for separate supervision
    of model-specific representation learning for each modality is also another technique
    of individual processing [[103](#bib.bib103), [144](#bib.bib144), [44](#bib.bib44)].
    Moreover, by minimizing the feature correlation, Xiong et al. [[44](#bib.bib44)]
    learned the modal distinctive features as the RGB and depth modalities have different
    characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Performance Comparison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Performance on RGB scene data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE II: Performance (%) summarization of some representative methods on popular
    benchmark datasets. All scores are quoted directly from the original papers. For
    each dataset, the highest three classification scores are highlighted. Some abbreviations.
    Column “Scheme”: Whole Image (WI), Dense Patches (DP), Regional Proposals (RP);
    Column “Init.”(Initialization): ImageNet (IN), Places205 (PL1), Places365 (PL2).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Group | Method | Input information | Feature Information | Architecture |
    Results (RGB) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Scale | Scheme | Data Aug. | Aggregation | Dimension | Init. | Backbone
    | Scene15 | MIT67 | SUN397 |'
  prefs: []
  type: TYPE_TB
- en: '| Global CNN features based methods | ImageNet-CNN [[17](#bib.bib17)] | Single
    | WI | ${\times}$ | pooling | 4,096 | IN | AlexNet | 84.2 | 56.8 | 42.6 |'
  prefs: []
  type: TYPE_TB
- en: '| PL1-CNN [[18](#bib.bib18)] | Single | WI | ${\times}$ | pooling | 4,096 |
    PL1 | VGGNet | 91.6 | 79.8 | 62.0 |'
  prefs: []
  type: TYPE_TB
- en: '| PL2-CNN [[25](#bib.bib25)] | Single | WI | ${\times}$ | pooling | 4,096 |
    PL2 | VGGNet | 92.0 | 76.5 | 63.2 |'
  prefs: []
  type: TYPE_TB
- en: '| S2ICA [[26](#bib.bib26)] | Multi | DP | ${\surd}$ | pooling | 8,192 | IN
    | AlexNet | 93.1 | 71.2 | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | GAP-CNN [[23](#bib.bib23)] | Single | WI | ${\surd}$ | GAP | 4,096 | PL1
    | GoogLeNet | 88.3 | 66.6 | 51.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | InterActive [[99](#bib.bib99)] | Single | WI | ${\times}$ | pooling |
    4,096 | IN | VGG19 | $-$ | 78.7 | 63.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | C-HLSTM [[27](#bib.bib27)] | Multi | WI | ${\surd}$ | LSTM | 4,096 | PL1
    | AlexNet | $-$ | 75.7 | 60.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | DL-CNN [[22](#bib.bib22)] | Single | WI | ${\times}$ | DL | $-$ | PL1
    | VGGNet | 96.0 | 86.4 | 70.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Spatially invariant features based methods | SCFVC [[83](#bib.bib83)] | Single
    | DP | ${\times}$ | FV | 200,000 | IN | AlexNet | $-$ | 68.2 | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| MOP-CNN [[28](#bib.bib28)] | Multi | DP | ${\times}$ | VLAD | 12,288 | IN
    | AlexNet | $-$ | 68.9 | 52.0 |'
  prefs: []
  type: TYPE_TB
- en: '| DSP [[102](#bib.bib102)] | Multi | WI | ${\times}$ | FV | 12,288 | IN | VGGNet
    | 91.8 | 78.3 | 59.8 |'
  prefs: []
  type: TYPE_TB
- en: '| MPP-CNN [[29](#bib.bib29)] | Multi | RP | ${\times}$ | FV | 65,536 | IN |
    AlexNet | $-$ | 80.8 | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| SFV [[30](#bib.bib30)] | Multi | DP, WI | ${\times}$ | FV | 9,216 | IN |
    AlexNet | $-$ | 72.8 | 54.4 |'
  prefs: []
  type: TYPE_TB
- en: '| FV-CNN [[82](#bib.bib82)] | Multi | RP, WI | ${\times}$ | FV | 4,096 | IN
    | VGGNet | $-$ | 81.0 | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| LatMoG [[58](#bib.bib58)] | Multi | RP | ${\times}$ | FV | $-$ | IN | AlexNet
    | $-$ | 69.1 | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| DUCA [[20](#bib.bib20)] | Single | DP | ${\surd}$ | CSMC | 4,096 | IN | AlexNet
    | 94.5 | 78.8 | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| D3 [[163](#bib.bib163)] | Single | DP | ${\times}$ | D3, FV | 1,048,576 |
    IN | VGG16 | 92.8 | 77.1 | 61.5 |'
  prefs: []
  type: TYPE_TB
- en: '| MFA-FS [[42](#bib.bib42)] | Multi | DP | ${\times}$ | MFA-FV | 5,000 | IN
    | VGGNet | $-$ | 81.4 | 63.3 |'
  prefs: []
  type: TYPE_TB
- en: '| CTV [[129](#bib.bib129)] | Multi | WI | ${\times}$ | CTV | $-$ | PL1 | AlexNet
    | $-$ | 73.9 | 58.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MFAFVNet [[31](#bib.bib31)] | Multi | DP | ${\times}$ | MFA-FV | 500,000
    | IN | VGG19 | $-$ | 82.7 | 64.6 |'
  prefs: []
  type: TYPE_TB
- en: '| EMFS [[147](#bib.bib147)] | Multi | DP | ${\times}$ | SM | 4,096 | IN, PL2
    | VGGNet | $-$ | 86.5 | 72.6 |'
  prefs: []
  type: TYPE_TB
- en: '| VSAD [[32](#bib.bib32)] | Multi | DP | ${\surd}$ | VSAD | 25,600 | IN, PL1
    | VGG16 | $-$ | 86.1 | 72.0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLC [[150](#bib.bib150)] | Single | DP | ${\times}$ | SSE | 3,072 | IN, PL2
    | VGG16 | $-$ | 79.6 | 57.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Semantic features based methods | URDL [[87](#bib.bib87)] | Multi | SS |
    ${\surd}$ | pooling | 4,096+ | IN | VGG16 | 91.2 | 71.9 | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| MetaObject-CNN [[33](#bib.bib33)] | Multi | RP | ${\surd}$ | LSAQ | 4,096
    | PL1 | AlexNet | $-$ | 78.9 | 58.1 |'
  prefs: []
  type: TYPE_TB
- en: '| SOAL [[164](#bib.bib164)] | Multi | RP | ${\times}$ | CRF | 1,024 | PL1 |
    VGGNet | $-$ | 82.5 | 75.5 |'
  prefs: []
  type: TYPE_TB
- en: '| WELDON [[34](#bib.bib34)] | Single | WI | ${\times}$ | pooling | 4,096 |
    IN | VGG16 | 94.3 | 78.0 | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Adi-Red [[165](#bib.bib165)] | Multi | DisNet | ${\times}$ | GAP | 12,288
    | IN, PL1-2 | ResNet | $-$ | $-$ | 73.6 |'
  prefs: []
  type: TYPE_TB
- en: '| SDO [[35](#bib.bib35)] | Multi | OMD | ${\times}$ | VLAD | 8,192 | PL1 |
    VGGNet | *95.9* | 86.8 | 73.4 |'
  prefs: []
  type: TYPE_TB
- en: '| M2M BiLSTM [[36](#bib.bib36)] | Single | SS | ${\times}$ | LSTM | $-$ | IN
    | ResNet | *96.3* | 88.3 | 71.8 |'
  prefs: []
  type: TYPE_TB
- en: '| LGN [[45](#bib.bib45)] | Single | WI | ${\times}$ | LGN | 8,192 | PL2 | ResNet
    | $-$ | 88.1 | 74.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Multilayer features based methods | Deep19-DAG [[37](#bib.bib37)] | Single
    | WI | ${\times}$ | pooling | 6,144 | IN | VGG19 | 92.9 | 77.5 | 56.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid CNNs [[38](#bib.bib38)] | Multi | SS | ${\times}$ | FV | 12,288+ |
    IN, PL1 | VGGNet | $-$ | 82.3 | 64.5 |'
  prefs: []
  type: TYPE_TB
- en: '| G-MS2F [[39](#bib.bib39)] | Single | WI | ${\surd}$ | pooling | 3,072 | IN,
    PL1 | GoogLeNet | 93.2 | 80.0 | 65.1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FTOTLM [[21](#bib.bib21)] | Single | WI | ${\times}$ | GAP | 3,968 | IN,
    PL2 | ResNet | 94.0 | 74.6 | $65.5$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | FTOTLM Aug. [[21](#bib.bib21)] | Single | WI | ${\surd}$ | GAP | 3,968
    | IN, PL2 | ResNet | *97.4^∗* | *94.1^∗* | *85.2^∗* |'
  prefs: []
  type: TYPE_TB
- en: '| Multiview features based methods | [[166](#bib.bib166)] | Multi | WI | ${\surd}$
    | pooling | 8,192 | IN, aratio | AlexNet | 92.1 | 70.1 | 54.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Scale-specific CNNs [[19](#bib.bib19)] | Multi | Crops | ${\times}$ | pooling
    | 4,096 | IN, PL1 | VGGNet | 95.2 | 86.0 | 70.2 |'
  prefs: []
  type: TYPE_TB
- en: '| LS-DHM [[40](#bib.bib40)] | Single | WI, DP | ${\times}$ | FV | 40,960 |
    IN | VGGNet | $-$ | 83.8 | 67.6 |'
  prefs: []
  type: TYPE_TB
- en: '| [[167](#bib.bib167)] | Multi | WI, DP | ${\times}$ | SC | 6,096 | IN, PL1
    | VGG16 | 95.7 | 87.2 | 71.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MR CNN [[41](#bib.bib41)] | Multi | WI | ${\surd}$ | pooling | $-$ | Places401
    | Inception 2 | $-$ | 86.7 | 72.0 |'
  prefs: []
  type: TYPE_TB
- en: '| SOSF+CFA+GAF [[24](#bib.bib24)] | Single | WI, DP | ${\surd}$ | SFV | 12,288
    | IN | VGG16 | $-$ | *89.5* | *78.9* |'
  prefs: []
  type: TYPE_TB
- en: '| FOSNet [[132](#bib.bib132)] | Single | WI | ${\surd}$ | GAP | 4,096 | PL2
    | ResNet | $-$ | *90.3* | *77.3* |'
  prefs: []
  type: TYPE_TB
- en: '|  | ChAM [[43](#bib.bib43)] | Single | WI | ${\surd}$ | pooling | 512 | PL2
    | ResNet | $-$ | *87.1* | *74.0* |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: Performance (%) comparison of related methods with/without concatenating
    global CNN feature on benchmark scene datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | DSFL [[149](#bib.bib149)] | SFV [[30](#bib.bib30)] | MFA$-$FS [[42](#bib.bib42)]
    | MFAFVNet [[31](#bib.bib31)] | VSAD [[32](#bib.bib32)] | SOSF$+$CFA [[24](#bib.bib24)]
    | SDO [[35](#bib.bib35)] | LGN [[45](#bib.bib45)] |'
  prefs: []
  type: TYPE_TB
- en: '| MIT67 | Baseline | 52.2 | 72.8 | 81.4 | 82.6 | 84.9 | 84.1 | 68.1 | 85.2
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $+$Global feature | 76.2 ($\uparrow 24$) | 79 ($\uparrow 6.2$) | 87.2
    ($\uparrow 5.8$) | 87.9 ($\uparrow 5.3$) | 85.3 ($\uparrow 0.4$) | 89.5 ($\uparrow
    5.4$) | 84 ($\uparrow 15.9$) | 85.4 ($\uparrow 0.2$) |'
  prefs: []
  type: TYPE_TB
- en: '| SUN397 | Baseline | $-$ | 54.4 | 63.3 | 64.6 | 71.7 | 66.5 | 54.8 | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | $+$Global feature | $-$ | 61.7 ($\uparrow 7.3$) | 71.1 ($\uparrow 7.8$)
    | 72 ($\uparrow 7.4$) | 72.5 ($\uparrow 0.8$) | 78.9 ($\uparrow 12.4$) | 67 ($\uparrow
    12.2$) | $-$ |'
  prefs: []
  type: TYPE_TB
- en: 'In contrast, CNN-based methods have quickly demonstrated their strengths in
    scene classification. Table [II](#S4.T2 "TABLE II ‣ 4.1 Performance on RGB scene
    data ‣ 4 Performance Comparison ‣ Deep Learning for Scene Classification: A Survey")
    compares the performance of deep models for scene classification on RGB datasets.
    To gain insight into the performance of the presented methods, we also provided
    input information, feature information, and architecture of each method. The results
    show that a simple deep model (*i.e.,* AlexNet), which is trained on ImageNet,
    achieves 84.23%, 56.79%, and 42.61% accuracy on Scene15, MIT67, and SUN397 datasets,
    respectively. This accuracy is comparable with the best non-deep learning methods.
    Starting from the generic deep models [[17](#bib.bib17), [18](#bib.bib18), [25](#bib.bib25)],
    CNN-based methods improve steadily when more effective strategies have been introduced.
    As a result, nearly all the approaches yielded an accuracy of 90% on the Scene15
    dataset. Moreover, FTOTLM [[21](#bib.bib21)] combined with a novel data augmentation
    outperforms other state-of-the-art models and achieves the best accuracy on three
    benchmark datasets so far.'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting global CNN features, which are computed using a pre-trained model,
    is faster than other deep feature representation techniques, but their quality
    is not good when there are large differences between the source and target datasets.
    Comparing these performances [[17](#bib.bib17), [18](#bib.bib18), [25](#bib.bib25)]
    demonstrates that the expressive power of global CNN features is improved as richer
    scene datasets appear. In GAP-CNN [[23](#bib.bib23)] and DL-CNN [[22](#bib.bib22)],
    new layers with a small number of parameters substitute for FC layers, but they
    can still achieve considerable results comparing with benchmark CNNs [[17](#bib.bib17),
    [18](#bib.bib18)].
  prefs: []
  type: TYPE_NORMAL
- en: Spatially invariant feature based methods are usually time-consuming, especially
    the computational time of sampling local patches, extracting individually local
    features, and building codebook. However, these methods are robust against geometrical
    variance, and thus improve the accuracy of benchmark CNNs, like SFV [[30](#bib.bib30)]
    vs. ImageNet-CNN [[17](#bib.bib17)], and MFA-FS [[42](#bib.bib42)] vs. PL1-CNN [[18](#bib.bib18)].
    Encoding technologies generally include more complicated training procedure, so
    some architectures (*e.g.,* MFAFVNet [[31](#bib.bib31)] and VSAD [[32](#bib.bib32)])
    are designed in a unified pipeline to reduce the operation complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic feature based methods [[165](#bib.bib165), [35](#bib.bib35), [36](#bib.bib36),
    [45](#bib.bib45)] demonstrate very competitive performance, due to the discriminative
    information laying on the salient regions, compared to global CNN feature based
    and spatially invariant feature based methods. Salient regions generally are generated
    by region selection algorithms, which may cause a two-stage training procedure
    and require more time and computations [[61](#bib.bib61)]. In addition, even though
    the contextual analysis demands more computational power, methods [[36](#bib.bib36),
    [45](#bib.bib45)], exploring the contextual relations among salient regions, can
    significantly improve the classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layer feature based methods employ the complementary features from different
    layers to improve performance. It is a simple way to use more feature cues, while
    it also does not require to add any other layers. However, these methods are structurally
    complicated and have high-dimensional features, which make training models difficult
    and prone to overfitting [[37](#bib.bib37)]. Nevertheless, owing to a novel data
    augmentation, FTOTLM [[21](#bib.bib21)] yields a gain of 19.5% and 19.7% on MIT67
    and SUN397, respectively, and has achieved the best results so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-view feature based methods take full advantage of features extracted
    from various CNNs to achieve high classification accuracy. For instance, Table [III](#S4.T3
    "TABLE III ‣ 4.1 Performance on RGB scene data ‣ 4 Performance Comparison ‣ Deep
    Learning for Scene Classification: A Survey") shows that combining global features
    with other baselines significantly improves their original classification accuracy,
    *e.g.,* a baseline model “SFV”[[30](#bib.bib30)] achieves 72.8% on MIT67, while
    “SFV$+$global feature” yields 79%. Moreover, there are two aspects to emphasize:
    1) Herranz et al. [[19](#bib.bib19)] empirically proved that combining too much
    invalid features is marginally helpful and significantly increases calculation
    and introduces noise into the final feature. 2) It is essential to improve the
    expression ability of each view feature, and thus enhance the entire ability of
    multi-view features.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the scene classification performance can be boosted by adopting
    more sophisticated deep models [[77](#bib.bib77), [79](#bib.bib79)] and large-scale
    datasets [[18](#bib.bib18), [25](#bib.bib25)]. Meanwhile, deep learning based
    methods can obtain relatively satisfied accuracy on public datasets via combining
    multiple features [[24](#bib.bib24)], focusing on semantic regions [[35](#bib.bib35)],
    augmenting data [[21](#bib.bib21)], and training in a unified pipeline [[132](#bib.bib132)].
    In addition, many methods also improve their accuracy via adopting different strategies,
    *i.e.,* improved encoding [[32](#bib.bib32), [31](#bib.bib31)], contextual modeling [[36](#bib.bib36),
    [45](#bib.bib45)], attention policy [[23](#bib.bib23), [43](#bib.bib43)], and
    multi-task learning [[22](#bib.bib22), [40](#bib.bib40)].
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Performance (%) comparison of representative methods on benchmark
    RGB-D scene datasets. For each dataset, the top three scores are highlighted.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Group | Method | Architecture | Detailed Information | Results |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | RGB-CNN | Depth-CNN | Dimension | Modal Fusion | Classifier | NYUD2
    | SUN RGBD |'
  prefs: []
  type: TYPE_TB
- en: '| Dataset | SUN RGBD[[72](#bib.bib72)] | PL1-AlexNet | 8,192 | Feature-level
    concatenation | SVM | $-$ | $39$ |'
  prefs: []
  type: TYPE_TB
- en: '| Feature learning | SS-CNN [[168](#bib.bib168)] | PL1-ASPP | 4,096 | Image-level
    stacking | Softmax | $-$ | $41.3$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | MMML [[160](#bib.bib160)] | IN-DeCAF | 256 | Feature-level concatenation
    | SVM | $61.4$ | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | MSMM [[47](#bib.bib47)] | PL1-AlexNet | 12,288+ | Feature-level concatenation
    | wSVM | $66.7$ | $52.3$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | MAPNet [[96](#bib.bib96)] | PL1-AlexNet | 5,120 | Local and semantic feature
    concatenation | Softmax | $67.7$ | *56.2* |'
  prefs: []
  type: TYPE_TB
- en: '|  | SOOR[[145](#bib.bib145)] | PL1-AlexNet | PL1-DCNN | 512 | Local and global
    feature concatenation | SVM | $67.4$ | $55.5$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | ACM [[144](#bib.bib144)] | PL2-AlexNet | 8192+ | Feature-level concatenation
    | Softmax | $67.2$ | $55.1$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | LM-CNN [[169](#bib.bib169)] | IN-AlexNet | 8,192 | Local feature concatenation
    | Softmax | $-$ | $48.7$ |'
  prefs: []
  type: TYPE_TB
- en: '| Depth feature learning | DCNN [[89](#bib.bib89)] | PL1-RCNN | PL1-DCNN |
    4,608 | Feature-level concatenation | wSVM | $67.5$ | $53.8$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | TRecgNet [[48](#bib.bib48)] | SUN RGBD & PL2 ResNet18 | 1024 | Feature-level
    concatenation | Softmax | *69.2* | *56.7* |'
  prefs: []
  type: TYPE_TB
- en: '| Multiple modal fusion | DMMF [[50](#bib.bib50)] | PL1-AlexNet | 4096 | Inter-
    and intra- modal correlation and distinction | L-SVM | $-$ | $41.5$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | MaCAFF [[46](#bib.bib46)] | PL1-AlexNet | $-$ | Local and global feature
    concatenation | L-SVM | $63.9$ | $48.1$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | DF2Net [[152](#bib.bib152)] | PL1-AlexNet | 512 | Modal correlation and
    distinction | Softmax | $65.4$ | $54.6$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | KFS [[103](#bib.bib103)] | PL1-AlexNet | 9,216+ | Modal correlation and
    distinction | Softmax | $67.8$ | 55.9 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CBSC [[170](#bib.bib170)] | PL2-VGG16 | $-$ | Feature-level concatenation
    | Softmax | *69.7^∗* | *57.8^∗* |'
  prefs: []
  type: TYPE_TB
- en: '|  | MSN [[44](#bib.bib44)] | PL1-AlexNet | 9,216+ | Modal correlation and
    distinction | Softmax | *68.1* | *56.2* |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: Ablation study on benchmark datasets to validate the performance (%)
    of depth information.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | MaCAFF[[46](#bib.bib46)] | MSMM[[47](#bib.bib47)] | DCNN[[89](#bib.bib89)]
    | DF2Net[[152](#bib.bib152)] | TRecgNet[[48](#bib.bib48)] | ACM[[144](#bib.bib144)]
    | CBCL[[170](#bib.bib170)] | KFS[[103](#bib.bib103)] | MSN[[44](#bib.bib44)] |'
  prefs: []
  type: TYPE_TB
- en: '| NYUD2 | Baseline | 53.5 | $-$ | 53.4 | 61.1 | 53.7 | 55.4 | 66.4 | 53.5 |
    53.5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $+$Depth | 63.9 ($\uparrow 10.4$) | $-$ | 67.5 ($\uparrow 14.1$) | 65.4 ($\uparrow
    4.3$) | 67.5 ($\uparrow 13.8$) | 67.4 ($\uparrow 12$) | 69.7 ($\uparrow 3.3$)
    | 67.8 ($\uparrow 14.3$) | 68.1 ($\uparrow 14.6$) |'
  prefs: []
  type: TYPE_TB
- en: '| SUN RGBD | Baseline | 40.4 | 41.5 | 44.3 | 46.3 | 42.6 | 45.7 | 48.8 | 36.1
    | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | $+$Depth | 48.1 ($\uparrow 7.7$) | 52.3 ($\uparrow 10.8$) | 53.8 ($\uparrow
    9.5$) | 54.6 ($\uparrow 8.3$) | 53.3 ($\uparrow 10.7$) | 55.1 ($\uparrow 9.4$)
    | 57.8 ($\uparrow 9.0$) | 41.3 ($\uparrow 5.2$) | $-$ |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Performance on RGB-D scene datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The accuracy of different methods on RGB-D datasets is summarized in Table [IV](#S4.T4
    "TABLE IV ‣ 4.1 Performance on RGB scene data ‣ 4 Performance Comparison ‣ Deep
    Learning for Scene Classification: A Survey"). By adding depth information with
    different fusion strategies, accuracy results (see Table [V](#S4.T5 "TABLE V ‣
    4.1 Performance on RGB scene data ‣ 4 Performance Comparison ‣ Deep Learning for
    Scene Classification: A Survey")) are improved over 10.8% and 8.8% on average
    on NYUD2 and SUN RGBD datasets, respectively. Since depth data provide extra information
    to train classification model, this observation is within expectation. Noteworthily,
    it is more difficult to improve the effect on a large dataset (SUN RGBD) than
    a small dataset (NYUD2). Moreover, the best results on NYUD2 and SUN RGBD datasets
    achieved by CBSC [[170](#bib.bib170)] are as high as 69.7% and 57.84%, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: RGB-D scene data for training are relatively limited, while the dimension of
    scene features is high. Hence, Support Vector Machines (SVMs) are commonly used
    in RGB-D scene classification [[72](#bib.bib72), [160](#bib.bib160), [47](#bib.bib47),
    [89](#bib.bib89)] in the early stages. Thanks to data augmentation and back-propagation,
    Softmax classifier becomes progressively popular, and it is an important reason
    to yield a comparable performance [[96](#bib.bib96), [48](#bib.bib48), [170](#bib.bib170),
    [44](#bib.bib44)].
  prefs: []
  type: TYPE_NORMAL
- en: Many methods, such as [[46](#bib.bib46), [50](#bib.bib50)], fine-tune RGB-CNNs
    to extract deep features of depth modality, where the training process is simple,
    and the computational cost is low. To adapt to depth data, Song et al. [[171](#bib.bib171),
    [89](#bib.bib89)] used weakly-supervised learning to train depth-specific models
    from scratch, which achieves a gain of 3.5% accuracy, compared to the fine-tuned
    RGB-CNN. TRecgNet [[48](#bib.bib48)], which is based on semi-supervised learning,
    requires complicated training process and high computational cost, but it obtains
    comparable results (69.2% on NYUD2 and 56.7% on SUN RGBD).
  prefs: []
  type: TYPE_NORMAL
- en: Feature-level fusion based methods are commonly used due to their high cost-effectiveness,
    *e.g.,* [[170](#bib.bib170), [96](#bib.bib96), [144](#bib.bib144)]. Along this
    way, consistent-feature based, and distinctive-feature based modal fusion use
    complex fusion layer with high cost, like inference speed and training complexity,
    but they generally yield more effective features [[152](#bib.bib152), [103](#bib.bib103),
    [44](#bib.bib44)].
  prefs: []
  type: TYPE_NORMAL
- en: We can observe that the field of RGB-D scene classification has constantly been
    improving. Weakly-supervised and semi-supervised learning are useful to learn
    depth-specific features [[47](#bib.bib47), [89](#bib.bib89), [48](#bib.bib48)].
    Moreover, multi-modal feature fusion is a major issue to improve performance on
    public datasets [[152](#bib.bib152), [170](#bib.bib170), [44](#bib.bib44)]. In
    addition, effective strategies (like contextual strategy [[145](#bib.bib145),
    [144](#bib.bib144)] and attention mechanism [[96](#bib.bib96), [44](#bib.bib44)])
    are also popular for RGB-D scene classification. Nevertheless, the accuracy achieved
    by current methods is far from expectation and there remains significant rooms
    for future improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion and Outlook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a contemporary survey for scene classification using deep learning, this
    paper has highlighted the recent achievements, provided some structural taxonomy
    for various methods according to their roles in scene representation for scene
    classification, analyzed their advantages and limitations, summarized existing
    popular scene datasets, and discussed performance for the most representative
    approaches. Despite great progress, there are still many unsolved problems. Thus
    in this section, we will point out these problems and introduce some promising
    trends for future research. We hope that this survey not only provides a better
    understanding of scene classification for researchers but also stimulates future
    research activities.
  prefs: []
  type: TYPE_NORMAL
- en: Develop more advanced network frameworks. With the development of deep CNN architectures,
    from generic CNNs [[17](#bib.bib17), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79)]
    to scene-specific CNNs [[23](#bib.bib23), [22](#bib.bib22)], the accuracy of scene
    classification is getting increasingly comparable. Nevertheless, there still exists
    lots of works to be explored on the theoretical research of deep learning [[172](#bib.bib172)].
    It is a further direction to solidify the theoretical basis so as to get more
    advanced network frameworks. In particular, it is essential to design specific
    frameworks for scene classification, such as using automated Neural Architecture
    Search (NAS) [[173](#bib.bib173), [174](#bib.bib174)], or according to scene attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Release rich scene datasets. Deep learning based models require enormous amounts
    of data to initialize their parameters so that they can learn the scene knowledge
    well [[18](#bib.bib18), [25](#bib.bib25)]. However, compared to scenes of real
    world, the publicly available datasets are not large or rich enough, so it is
    essential to release datasets that encompass richness and high-diversity of environmental
    scenes [[175](#bib.bib175)], especially large-scale RGB-D scene datasets. As opposed
    to object/texture datasets, scene appearance may be changed dramatically as time
    goes by, and there emerges new functional scenes as humans develop activity places.
    Therefore, it requires updating the original scene data and releasing new scene
    datasets regularly.
  prefs: []
  type: TYPE_NORMAL
- en: Reduce the dependence of labeled scene images. The success of deep learning
    heavily relies on gargantuan amounts of labeled images. However, the labeled training
    images are always very limited, so supervised learning is not scalable in the
    absence of fully labeled training data and its generalization ability to classify
    scenes frequently deteriorates. Therefore, it is desirable to reduce dependence
    on large amounts of labeled data. To alleviate this difficulty, if with large
    numbers of unlabel data, it is necessary to further study semi-supervised learning [[176](#bib.bib176)],
    unsupervised learning [[177](#bib.bib177)], or self-supervised learning [[178](#bib.bib178)].
    Even more constrained, without any unlabel training data, the ability to learn
    from only few labeled images, small-sample learning [[179](#bib.bib179)], is also
    appealing.
  prefs: []
  type: TYPE_NORMAL
- en: Few shot scene classification. The success of generic CNNs for scene classification
    relies heavily on gargantuan amounts of labeled training data [[107](#bib.bib107)].
    Due to the large intra-variation among scenes, scene datasets cannot cover various
    classes so that the performance of CNNs frequently deteriorates and fails to generalize
    well. In contrast, humans can learn a visual concept quickly from very few given
    examples and often generalize well [[180](#bib.bib180), [181](#bib.bib181)]. Inspired
    by this, domain adaptation approaches utilize the knowledge of labeled data in
    task-relevant domains to execute new tasks in target domain [[182](#bib.bib182),
    [183](#bib.bib183)]. Furthermore, domain generalization methods aim at learning
    generic representation from multiple task-irrelevant domains to generalize unseen
    scenarios [[184](#bib.bib184), [185](#bib.bib185)].
  prefs: []
  type: TYPE_NORMAL
- en: Robust scene classification. Once scene classification in the laboratory environment
    is deployed in the real application scenario, there will still be a variety of
    unacceptable phenomena, that is, the robustness in open environments is a bottleneck
    to restrict pattern recognition technology. The main reasons why the pattern recognition
    systems are not robust are basic assumptions, *e.g.,* closed world assumption,
    independent identical distribution and big data assumption [[186](#bib.bib186)],
    which are main differences between machine learning and human intelligence; hence,
    it is a fundamental challenge to improve the robustness by breaking these assumptions.
    It is a nature consider via utilizing adversarial training and optimization [[187](#bib.bib187),
    [188](#bib.bib188), [189](#bib.bib189)], which have been applied to pattern recognition [[190](#bib.bib190),
    [191](#bib.bib191)].
  prefs: []
  type: TYPE_NORMAL
- en: Realtime scene classification. Many methods for scene classification, trained
    in a multiple-stage manner, are computationally expensive for current mobile/wearable
    devices, which have limited storage and computational capability, therefore researchers
    have begun to develop convenient and efficient unified networks (encapsulating
    all computation in a one-stage network) [[31](#bib.bib31), [22](#bib.bib22), [44](#bib.bib44)].
    Moreover, it is also a challenge to keep the model scalable and efficient well
    when big data from smart wearables and mobile applications is growing rapidly
    in size temporally or spatially [[192](#bib.bib192)].
  prefs: []
  type: TYPE_NORMAL
- en: Imbalanced scene classification. The Places365 challenge dataset [[25](#bib.bib25)]
    has more than 8M training images, and the numbers of images in different classes
    range from 4,000 to 30,000 per class. It shows that scene categories are imbalanced,
    *i.e.,* some categories are abundant while others have scarce examples. Generally,
    the minority class samples are poorly predicted by the learned model [[193](#bib.bib193),
    [194](#bib.bib194)]. Therefore, learning a model which respects both type of categories
    and equally performs well on frequent and infrequent ones remains a great challenge
    and needs further exploration [[195](#bib.bib195), [196](#bib.bib196), [194](#bib.bib194)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuous scene classification. The ultimate goal is to develop methods capable
    of accurately and efficiently classifying samples in thousands or more unseen
    scene categories in open environments [[107](#bib.bib107)]. The classic deep learning
    paradigm learns in isolation, *i.e.,* it needs many training examples and is only
    suitable for well-defined tasks in closed environments [[76](#bib.bib76), [197](#bib.bib197)].
    In contrast, “human learning” is a continuous learning and adapting to new environments:
    humans accumulate the knowledge gained in the past and use this knowledge to help
    future learning and problem solving with possible adaptations [[198](#bib.bib198)].
    Ideally, it should also be capable to discover unknown scenarios and learn new
    works in a self-supervised manner. Inspired by this, it is necessary to do lifelong
    machine learning via developing versatile systems that continually accumulate
    and refine their knowledge over time [[199](#bib.bib199), [200](#bib.bib200)].
    Such lifelong machine learning has represented a long-standing challenge for deep
    learning and, consequently, artificial intelligence systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-label scene classification. Many scenes are semantic multiplicity [[68](#bib.bib68),
    [69](#bib.bib69)], *i.e.,* a scene image may belong to multiple semantic classes.
    Such a problem poses a challenge to the classic pattern recognition paradigm and
    requires developing multi-label learning methods [[69](#bib.bib69), [201](#bib.bib201)].
    Moreover, when constructing scene datasets, most researchers either avoid labeling
    multi-label images or use the most obvious class (single label) to annotate subjectively
    each image [[68](#bib.bib68)]. Hence, it is hard to improve the generalization
    ability of the model trained on single-label datasets, which also brings problems
    to classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Other-modal scene classification. RGB images provide key features such as color,
    texture, and spectrum of objects. Nevertheless, the scenes reproduced by RGB images
    may have uneven lighting, target occlusion, *etc*. Therefore, the robustness of
    RGB scene classification is poor, and it is difficult to accurately extract key
    information such as target contours and spatial positions. In contrast, the rapid
    development of sensors has made the acquisition of other modal data easier and
    easier, such as RGB-D [[72](#bib.bib72)], video [[202](#bib.bib202)], 3D point
    clouds [[203](#bib.bib203)]. Recently, research on recognizing and understanding
    various modalities has attracted an increasing attention [[89](#bib.bib89), [204](#bib.bib204),
    [205](#bib.bib205)].
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A A Road Map of Scene Classification in 20 years
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scene representation or scene feature extraction, the process of converting
    a scene image into feature vectors, plays the critical role in scene classification,
    and thus is the focus of research in this field. In the past two decades, remarkable
    progress has been witnessed in scene representation, which mainly consists of
    two important generations: handcrafted feature engineering, and deep learning
    (feature learning). The milestones of scene classification in the past two decades
    are presented in Fig. [12](#A1.F12 "Figure 12 ‣ Appendix A A Road Map of Scene
    Classification in 20 years ‣ Deep Learning for Scene Classification: A Survey"),
    in which two main stages (SIFT vs. DNN) are highlighted.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/245fff10d608348dfc1bde2d02ed1531.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Milestones of scene classification. Handcrafted features gained
    tremendous popularity, starting from SIFT [[206](#bib.bib206)] and GIST [[207](#bib.bib207)].
    Then, HoG [[208](#bib.bib208)] and CENTRIST [[209](#bib.bib209)] were proposed
    by Dalal et al. and Wu et al., respectively, further promoting the development
    of scene classification. In 2003, Sivic et al. [[210](#bib.bib210)] proposed BoVW
    model, marking the beginning of codebook learning. Along this way, more effective
    BoVW based methods, SPM [[14](#bib.bib14)], IFV [[104](#bib.bib104)] and VLAD [[105](#bib.bib105)],
    also emerged to deal with larger-scale tasks. In 2010, Object Bank [[15](#bib.bib15),
    [211](#bib.bib211)] represents the scene as object attributes, marking the beginning
    of more semantic representations. Then, Juneja et al. [[212](#bib.bib212)] proposed
    Bags of Part to learn distinctive parts of scenes automatically. In 2012, AlexNet [[17](#bib.bib17)]
    reignites the field of artificial neural networks. Since then, CNN-based methods,
    VGGNet [[77](#bib.bib77)], GoogLeNet [[78](#bib.bib78)] and ResNet [[79](#bib.bib79)],
    have begun to take over handcrafted methods. Additionally, Razavian et al. [[213](#bib.bib213)]
    highlights the effectiveness and generality of CNN representations for different
    tasks. Along this way, in 2014, Gong et al. [[28](#bib.bib28)] proposed MOP-CNN,
    the first deep learning methods for scene classification. Later, FV-CNN [[82](#bib.bib82)],
    Semantic FV [[30](#bib.bib30)] and GAP-CNN [[23](#bib.bib23)] are proposed one
    after another to learn more effective representations. For datasets, ImageNet
    [[56](#bib.bib56)] triggers the breakthrough of deep learning. Then, Xiao et al. [[57](#bib.bib57)]
    proposed SUN database to evaluate numerous algorithms for scene classification.
    Later, Places [[18](#bib.bib18), [25](#bib.bib25)], the largest scene database
    currently, emerged to satisfy the need of deep learning training. Additionally,
    SUN RGBD [[72](#bib.bib72)] has been introduced, marking the beginning of deep
    learning for RGB-D scene classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Handcrafted feature engineering era. From 1995 to 2012, the field was dominated
    by the Bag of Visual Word (BoVW) model [[214](#bib.bib214), [106](#bib.bib106),
    [210](#bib.bib210), [215](#bib.bib215)] borrowed from document classification
    which represents a document as a vector of word occurrence counts over a global
    word vocabulary. In the image domain, BoVW firstly probes an image with local
    feature descriptors such as Scale Invariant Feature Transform (SIFT) [[206](#bib.bib206),
    [216](#bib.bib216)], and then represents an image statistically as an orderless
    histogram over a pre-trained visual vocabulary, in a similar form to a document.
    Some important variants of BoVW such as Bag of Semantics [[15](#bib.bib15), [126](#bib.bib126),
    [140](#bib.bib140), [212](#bib.bib212)] and Improved Fisher Vector (IFV) [[104](#bib.bib104)],
    have also been proposed.
  prefs: []
  type: TYPE_NORMAL
- en: Local invariant feature descriptors play an important role in BoVW because they
    are discriminative, yet less sensitive to image variations such as illumination,
    scale, rotation, viewpoint *etc*, and thus have been widely studied. Representative
    local descriptors for scene classification have started from SIFT [[206](#bib.bib206),
    [216](#bib.bib216)] and Global Information Systems Technology (GIST) [[207](#bib.bib207),
    [217](#bib.bib217)]. Other local descriptors, such as Local Binary Patterns (LBP) [[218](#bib.bib218)],
    Deformable Part Model (DPM) [[219](#bib.bib219), [220](#bib.bib220), [221](#bib.bib221)],
    CENsus TRansform hISTogram (CENTRIST) [[209](#bib.bib209)], also contribute to
    the development of scene classification. To improve the performance, research
    focus shifts to feature encoding and aggregation, mainly including Bag-of-Visual-Words
    (BoVW) [[106](#bib.bib106)], Latent Dirichlet Allocation (LDA) [[222](#bib.bib222)],
    Histogram of Gradients (HoG) [[208](#bib.bib208)], Spatial Pyramid Matching (SPM) [[223](#bib.bib223),
    [14](#bib.bib14)], Vector of Locally Aggregated Descriptors (VLAD) [[105](#bib.bib105)],
    Fisher kernel coding [[125](#bib.bib125), [104](#bib.bib104)], Multi-Resolution
    BoVW (MR-BoVW) [[224](#bib.bib224)], and Orientational Pyramid Matching (OPM)
    [[225](#bib.bib225)]. The quality of the learned codebook has a great impact on
    the coding procedure. The generic codebooks mainly include Fisher kernels [[125](#bib.bib125),
    [104](#bib.bib104)], sparse codebook [[226](#bib.bib226), [227](#bib.bib227)],
    Locality-constrained Linear Codes (LLC) [[228](#bib.bib228)], Histogram Intersection
    Kernels (HIK) [[229](#bib.bib229)], contextual visual words [[230](#bib.bib230)],
    Efficient Match Kernels (EMK) [[231](#bib.bib231)] and Supervised Kernel Descriptors
    (SKDES) [[232](#bib.bib232)]. Particularly, semantic codebooks generate from salient
    regions, like Object Bank [[211](#bib.bib211), [15](#bib.bib15), [233](#bib.bib233)],
    object-to-class [[234](#bib.bib234)], Latent Pyramidal Regions (LPR) [[235](#bib.bib235)],
    Bags of Parts (BoP) [[212](#bib.bib212)] and Pairwise Constraints based Multiview
    Subspace Learning (PC-MSL) [[236](#bib.bib236)], capturing more discriminative
    features for scene classification.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning era. In 2012, Krizhevsky et al. [[17](#bib.bib17)] introduced
    a DNN, commonly referred to as “AlexNet”, for the object classification task,
    and achieved breakthrough performance surpassing the best result of hand-engineered
    features by a large margin, and thus triggered the recent revolution in AI. Since
    then, deep learning has started to dominate various tasks (like computer vision [[80](#bib.bib80),
    [237](#bib.bib237), [107](#bib.bib107), [238](#bib.bib238)], speech recognition [[239](#bib.bib239)],
    autonomous driving [[240](#bib.bib240)], cancer detection [[241](#bib.bib241),
    [242](#bib.bib242)], machine translation [[243](#bib.bib243)], playing complex
    games [[244](#bib.bib244), [245](#bib.bib245), [246](#bib.bib246), [247](#bib.bib247)],
    earthquake forecasting [[248](#bib.bib248)], medicine discovery [[249](#bib.bib249),
    [250](#bib.bib250)]), and scene classification is no exception, leading to a new
    generation of scene representation methods with remarkable performance improvements.
    Such substantial progress can be mainly attributed to advances in deep models
    including VGGNet [[77](#bib.bib77)], GoogLeNet [[78](#bib.bib78)], ResNet [[79](#bib.bib79)],
    *etc.*, the availability of large-scale image datasets like ImageNet [[56](#bib.bib56)]
    and Places [[18](#bib.bib18), [25](#bib.bib25)] and more powerful computational
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning networks have gradually replaced the local feature descriptors
    of the first generation methods and are certainly the engine for scene classification.
    Although the major driving force of progress in scene classification has been
    the incorporation of deep learning networks, the general pipelines like BoVW,
    feature encoding and aggregation methods like Fisher Vector, VLAD of the first
    generation methods have also been adapted in current deep learning based scene
    methods, *e.g.,* MOP-CNN [[28](#bib.bib28)], SCFVC [[83](#bib.bib83)], MPP-CNN [[29](#bib.bib29)],
    DSP [[102](#bib.bib102)], Semantic FV [[30](#bib.bib30)], LatMoG [[58](#bib.bib58)],
    MFA-FS [[42](#bib.bib42)] and DUCA [[20](#bib.bib20)]. To take fully advantage
    of back-propagation, scene representations are extracted from end-to-end trainable
    CNNs, like DAG-CNN [[37](#bib.bib37)], MFAFVNet [[31](#bib.bib31)], VSAD [[32](#bib.bib32)],
    G-MS2F [[39](#bib.bib39)], and DL-CNN [[22](#bib.bib22)]. To focus on main content
    of the scene, object detection is used to capture salient regions, such as MetaObject-CNN
    [[33](#bib.bib33)], WELDON [[34](#bib.bib34)], SDO [[35](#bib.bib35)], and BiLSTM
    [[36](#bib.bib36)]. Since features from multiple CNN layers or multiple views
    are complementary, many literatures [[19](#bib.bib19), [40](#bib.bib40), [41](#bib.bib41),
    [24](#bib.bib24), [21](#bib.bib21)] also explored their complementarity to improve
    performance. In addition, there exists many strategies (like attention mechanism,
    contextual modeling, multi-task learning with regularization terms) to enhance
    representation ability, such as CFA [[24](#bib.bib24)], BiLSTM [[36](#bib.bib36)],
    MAPNet [[96](#bib.bib96)], MSN [[44](#bib.bib44)], and LGN [[45](#bib.bib45)].
    For datasets, because depth images from RGB-D cameras are not vulnerable to illumination
    changes, since 2015, researchers have started to explore RGB-D scene recognition.
    Some works [[47](#bib.bib47), [89](#bib.bib89), [48](#bib.bib48)] focus on depth-specific
    feature learning, while other alternatives, like DMMF [[50](#bib.bib50)], ACM [[144](#bib.bib144)],
    and MSN [[44](#bib.bib44)] focus on multi-modal feature fusion.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B A Brief Introduction to Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b23802d25da463be9f70ee19c861fa47.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) VGG
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/62102b4ec42eda513cba7aa00192a52d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Convolution operation
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13: (a) A typical CNN architecture VGGNet [[77](#bib.bib77)] with 16
    weight layers. The network has 13 convolutional layers, 5 max pooling layers (The
    last one is global max pooling) and 3 fully-connected layers (The last one uses
    Softmax function as nonlinear activation function). The whole network can be learned
    from labeled training data by optimizing a loss function (*e.g.,* , cross-entropy
    loss). (b) Illustration of basic operations (*i.e.,* , convolution, nonlinearity
    and downsampling) that are repeatedly applied for a typical CNN. (b1) The outputs
    (called the feature maps) of each layer (horizontally) of a typical CNN applied
    to a scene image. Each feature map in the second row corresponds to the output
    for one of the learned 3D filters (see (b2)).'
  prefs: []
  type: TYPE_NORMAL
- en: In 2012, the breakthrough object classification result on the large scale ImageNet
    dataset [[56](#bib.bib56)] achieved by a deep learning network [[17](#bib.bib17)]
    is arguably what reignited the field of Artificial Neural Networks (ANNs) and
    triggered the recent revolution in AI. Since then, deep learning, or Deep Neural
    Networks (DNNs) [[251](#bib.bib251)], shines in a broad range of areas, including
    computer vision [[17](#bib.bib17), [80](#bib.bib80), [107](#bib.bib107), [238](#bib.bib238),
    [237](#bib.bib237)], speech recognition [[239](#bib.bib239)], autonomous driving [[240](#bib.bib240)],
    cancer detection [[241](#bib.bib241), [242](#bib.bib242)], machine translation [[243](#bib.bib243)],
    playing complex games [[244](#bib.bib244), [245](#bib.bib245), [246](#bib.bib246),
    [247](#bib.bib247)], earthquake forecasting [[248](#bib.bib248)], medicine discovery [[249](#bib.bib249),
    [250](#bib.bib250)], *etc.* In many of these domains, DNNs have reached breakthrough
    levels of performance, often approaching and sometimes exceeding the abilities
    of human experts. Thanks to the growth of big data and more powerful computational
    resources, deep learning and AI for scientific research are evolving quickly,
    with new developments appearing continually for analyzing datasets, discovering
    patterns, and predicting behaviors in almost all fields of science and technology [[98](#bib.bib98)].
  prefs: []
  type: TYPE_NORMAL
- en: 'In computer vision, the most commonly used type of DNNs is Convolutional Neural
    Networks (CNNs) [[76](#bib.bib76)]. As is illustrated in Figure [13](#A2.F13 "Figure
    13 ‣ Appendix B A Brief Introduction to Deep Learning ‣ Deep Learning for Scene
    Classification: A Survey") (a), the frontend of a typical CNN is a stack of CONV
    layers and pooling layers to learn generic-level features, and these features
    are further transformed into class-specific discriminative representations via
    training multiple layers on target datasets. As we slide a convolution filter
    over the width and height of the input of 3 color channels, we will produce a
    two-dimensional (2-D) activation map, as shown in Figure [13](#A2.F13 "Figure
    13 ‣ Appendix B A Brief Introduction to Deep Learning ‣ Deep Learning for Scene
    Classification: A Survey") (b1), giving the responses of that filter at every
    spatial position. As shown in Figure [13](#A2.F13 "Figure 13 ‣ Appendix B A Brief
    Introduction to Deep Learning ‣ Deep Learning for Scene Classification: A Survey") (b2),
    a 2-D convolution operates $x^{l-1}*w^{l}$, describing as a weighted average of
    an input map $x^{l-1}$ from previous layer $l-1$, where the corresponding weighting
    is given by $w^{l}$; since every filter extends through the full depth of the
    input maps with $C$ channels, so we calculate the sum of 2-D convolution as the
    result of a 3-D convolution, *i.e.,*'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sum_{i=1}^{C}{x_{i}^{l-1}*w_{i}^{l}}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: During the forward pass, the $j$ neuron in $l$ CONV layer operates a 3-D convolution
    with $N^{l-1}$ channels between input maps $x^{l-1}$ and the corresponding filter
    $w_{j}^{l}$, plus a bias term $b_{j}^{l}$, and passes the above result to a nonlinear
    function $\sigma(\cdot)$ to obtain the final output $x_{j}^{l}$, *i.e.,*
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x_{j}^{l}=\sigma(\sum_{i=1}^{N^{l-1}}{x_{i}^{l-1}}*w_{i,j}^{l}+b_{j}^{l})$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: The nonlinear function $\sigma(\cdot)$ is typically a rectified linear unit
    (ReLU) for an element $x$, *i.e.,* $\sigma(x)=\mathrm{max}(x,0)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is common to periodically insert a pooling (*i.e.,* downsampling) in-between
    successive convolutional layers in a CNN architecture. Its function is to progressively
    reduce the spatial size of the input maps, as shown in Figure [13](#A2.F13 "Figure
    13 ‣ Appendix B A Brief Introduction to Deep Learning ‣ Deep Learning for Scene
    Classification: A Survey") (b1), to reduce the number of parameters and computation
    in the network, and hence to also control overfitting. Finally, the output layer
    expresses a differentiable score function: from the discriminative representations
    $x$ on one end to class scores $y$ at the other.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised by scene labels, Softmax classifier uses cross-entropy loss function
    to estimate model parameters, and formulas are as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{softmax}=\sum_{i}^{N}L_{CE}(x_{i})$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'where $L_{CE}(x_{i})$ denotes the cross-entropy loss function of each image
    and its formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{CE}(x_{i})=-\sum_{k=1}^{K}(y_{k}\text{log}f_{k}(x_{i}))$ |  | (4)
    |'
  prefs: []
  type: TYPE_TB
- en: where $x_{i}$ denotes a discriminative feature of scene image $I_{i}$; $K$ denotes
    the count of scene categories; $y_{k}$ denotes the real label of scene image $I_{i}$,
    when $y_{k}=1$ represents that the real label of $I_{i}$ is category $k$, otherwise
    $y_{k}=0$.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, the score function $f(\cdot)$ is Softmax function, denoting the probability
    of estimating scene image $I_{i}$ in class $k$.
  prefs: []
  type: TYPE_NORMAL
- en: so the $k$ component of output layer is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{k}(x)=e^{w_{k}x_{k}}/\sum_{j=1}^{K}{e^{w_{j}x_{j}}}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: In addition to the CNN architectures and datasets, some other training strategies
    or tricks have been proposed to achieve better performance. Overfitting happens
    when CNNs learn the detail and noise in the training data to the extent that it
    negatively impacts the generalization ability of models. To this end, some strategies
    have been proposed to reduce the overfitting tendency of CNNs, such as data augmentation,
    early stopping, dropout [[17](#bib.bib17)], smaller convolution kernel size [[77](#bib.bib77),
    [252](#bib.bib252)], and multi-scale cropping/warping [[77](#bib.bib77)]. In addition,
    some optimization techniques have been proposed to overcome the difficulties encountered
    in the training of deep CNNs, such as batch normalization [[123](#bib.bib123),
    [253](#bib.bib253)] and relay back propagation [[193](#bib.bib193)]. Comprehensive
    review of deep learning is out the scope of this survey, and we refer readers
    to [[76](#bib.bib76), [254](#bib.bib254), [255](#bib.bib255), [256](#bib.bib256),
    [257](#bib.bib257)] for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors would like to thank the pioneer researchers in scene classification
    and other related fields. This work was supported in part by grants from National
    Science Foundation of China (61872379, 91846301, 61571005), the Academy of Finland
    (331883), the fundamental research program of Guangdong, China (2020B1515310023),
    the Hunan Science and Technology Plan Project (2019GK2131), the China Scholarship
    Council (201806155037), the Science and Technology Research Program of Guangzhou,
    China (201804010429).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] J. M. Henderson and A. Hollingworth, “High-level scene perception,” *Annual
    review of psychology*, vol. 50, no. 1, pp. 243–271, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] R. Epstein, “The cortical basis of visual scene processing,” *Visual Cognition*,
    vol. 12, no. 6, pp. 954–978, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] M. R. Greene and A. Oliva, “The briefest of glances: The time course of
    natural scene understanding,” *Psychological Science*, vol. 20, no. 4, pp. 464–472,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] D. B. Walther, B. Chai, E. Caddigan, D. M. Beck, and L. Fei-Fei, “Simple
    line drawings suffice for functional MRI decoding of natural scene categories,”
    *PNAS*, vol. 108, no. 23, pp. 9661–9666, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] J. Vogel and B. Schiele, “Semantic modeling of natural scenes for content-based
    image retrieval,” *IJCV*, vol. 72, no. 2, pp. 133–157, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] L. Zheng, Y. Yang, and Q. Tian, “SIFT meets CNN: A decade survey of instance
    retrieval,” *IEEE TPAMI*, vol. 40, no. 5, pp. 1224–1244, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] W. Zhang, X. Yu, and X. He, “Learning bidirectional temporal cues for video-based
    person re-identification,” *IEEE TCSVT*, vol. 28, no. 10, pp. 2768–2776, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Hou, H. Zeng, J. Zhu, J. Hou, J. Chen, and K.-K. Ma, “Deep quadruplet
    appearance learning for vehicle re-identification,” *IEEE TVT*, vol. 68, no. 9,
    pp. 8512–8522, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] T. Zhang, S. Liu, C. Xu, and H. Lu, “Mining semantic context information
    for intelligent video surveillance of traffic scenes,” *IEEE TII*, vol. 9, no. 1,
    pp. 149–160, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] G. Sreenu and M. S. Durai, “Intelligent video surveillance: A review through
    deep learning techniques for crowd analysis,” *Journal of Big Data*, vol. 6, no. 1,
    p. 48, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. H. Behzadan and V. R. Kamat, “Integrated information modeling and visual
    simulation of engineering operations using dynamic augmented reality scene graphs,”
    *ITcon*, vol. 16, no. 17, pp. 259–278, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Y. Nee, S. Ong, G. Chryssolouris, and D. Mourtzis, “Augmented reality
    applications in design and manufacturing,” *CIRP annals*, vol. 61, no. 2, pp.
    657–679, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] K. Muhammad, J. Ahmad, and S. W. Baik, “Early fire detection using convolutional
    neural networks during surveillance for effective disaster management,” *Neurocomputing*,
    vol. 288, pp. 30–42, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features: Spatial
    pyramid matching for recognizing natural scene categories,” in *CVPR*, vol. 2,
    2006, pp. 2169–2178, https://figshare.com/articles/15-Scene_Image_Dataset/7007177.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] L.-J. Li, H. Su, L. Fei-Fei, and E. P. Xing, “Object bank: A high-level
    image representation for scene classification & semantic feature sparsification,”
    in *NeurIPS*, 2010, pp. 1378–1386.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] R. Margolin, L. Zelnik-Manor, and A. Tal, “OTC: A novel local descriptor
    for scene classification,” in *ECCV*, 2014, pp. 377–391.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *NeurIPS*, 2012, pp. 1097–1105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva, “Learning deep
    features for scene recognition using places database,” in *NeurIPS*, 2014, pp.
    487–495, http://places.csail.mit.edu/downloadData.html.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] L. Herranz, S. Jiang, and X. Li, “Scene recognition with CNNs: Objects,
    scales and dataset bias,” in *CVPR*, 2016, pp. 571–579.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] S. H. Khan, M. Hayat, M. Bennamoun, R. Togneri, and F. A. Sohel, “A discriminative
    representation of convolutional features for indoor scene recognition,” *IEEE
    TIP*, vol. 25, no. 7, pp. 3372–3383, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] S. Liu, G. Tian, and Y. Xu, “A novel scene classification model combining
    ResNet based transfer learning and data augmentation with a filter,” *Neurocomputing*,
    vol. 338, pp. 191–206, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Y. Liu, Q. Chen, W. Chen, and I. Wassell, “Dictionary learning inspired
    deep network for scene recognition,” in *AAAI*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning
    deep features for discriminative localization,” in *CVPR*, 2016, pp. 2921–2929.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] N. Sun, W. Li, J. Liu, G. Han, and C. Wu, “Fusing object semantics and
    deep appearance features for scene recognition,” *IEEE TCSVT*, vol. 29, no. 6,
    pp. 1715–1728, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba, “Places:
    A 10 million image database for scene recognition,” *IEEE TPAMI*, vol. 40, no. 6,
    pp. 1452–1464, 2017, http://places2.csail.mit.edu/download.html.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] M. Hayat, S. H. Khan, M. Bennamoun, and S. An, “A spatial layout and scale
    invariant feature representation for indoor scene classification,” *IEEE TIP*,
    vol. 25, no. 10, pp. 4829–4841, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Z. Zuo, B. Shuai, G. Wang, X. Liu, X. Wang, B. Wang, and Y. Chen, “Learning
    contextual dependence with convolutional hierarchical recurrent neural networks,”
    *IEEE TIP*, vol. 25, no. 7, pp. 2983–2996, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Gong, L. Wang, R. Guo, and S. Lazebnik, “Multi-scale orderless pooling
    of deep convolutional activation features,” in *ECCV*, 2014, pp. 392–407.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] D. Yoo, S. Park, J.-Y. Lee, and I. So Kweon, “Multi-scale pyramid pooling
    for deep convolutional representation,” in *CVPRW*, 2015, pp. 71–80.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] M. Dixit, S. Chen, D. Gao, N. Rasiwasia, and N. Vasconcelos, “Scene classification
    with semantic fisher vectors,” in *CVPR*, 2015, pp. 2974–2983.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Y. Li, M. Dixit, and N. Vasconcelos, “Deep scene image classification
    with the MFAFVNet,” in *ICCV*, 2017, pp. 5746–5754.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Z. Wang, L. Wang, Y. Wang, B. Zhang, and Y. Qiao, “Weakly supervised patchnets:
    Describing and aggregating local patches for scene recognition,” *IEEE TIP*, vol. 26,
    no. 4, pp. 2028–2041, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] R. Wu, B. Wang, W. Wang, and Y. Yu, “Harvesting discriminative meta objects
    with deep CNN features for scene classification,” in *ICCV*, 2015, pp. 1287–1295.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] T. Durand, N. Thome, and M. Cord, “WELDON: Weakly supervised learning
    of deep convolutional neural networks,” in *CVPR*, 2016, pp. 4743–4752.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] X. Cheng, J. Lu, J. Feng, B. Yuan, and J. Zhou, “Scene recognition with
    objectness,” *Pattern Recognition*, vol. 74, pp. 474–487, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] C. Laranjeira, A. Lacerda, and E. R. Nascimento, “On modeling context
    from objects with a long short-term memory for indoor scene recognition,” in *SIBGRAPI*,
    2019, pp. 249–256.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Yang and D. Ramanan, “Multi-scale recognition with DAG-CNNs,” in *ICCV*,
    2015, pp. 1215–1223.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] G.-S. Xie, X.-Y. Zhang, S. Yan, and C.-L. Liu, “Hybrid CNN and dictionary-based
    models for scene recognition and domain adaptation,” *IEEE TCSVT*, vol. 27, no. 6,
    pp. 1263–1274, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] P. Tang, H. Wang, and S. Kwong, “G-MS2F: Googlenet based multi-stage feature
    fusion of deep CNN for scene recognition,” *Neurocomputing*, vol. 225, pp. 188–197,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] S. Guo, W. Huang, L. Wang, and Y. Qiao, “Locally supervised deep hybrid
    model for scene recognition,” *IEEE TIP*, vol. 26, no. 2, pp. 808–820, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] L. Wang, S. Guo, W. Huang, Y. Xiong, and Y. Qiao, “Knowledge guided disambiguation
    for large-scale scene classification with multi-resolution CNNs,” *IEEE TIP*,
    vol. 26, no. 4, pp. 2055–2068, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] M. D. Dixit and N. Vasconcelos, “Object based scene representations using
    fisher scores of local subspace projections,” in *NeurIPS*, 2016, pp. 2811–2819.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] A. López-Cifuentes, M. Escudero-Viñolo, J. Bescós, and Á. García-Martín,
    “Semantic-aware scene recognition,” *Pattern Recognition*, vol. 102, p. 107256,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Z. Xiong, Y. Yuan, and Q. Wang, “MSN: Modality separation networks for
    RGB-D scene recognition,” *Neurocomputing*, vol. 373, pp. 81–89, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] G. Chen, X. Song, H. Zeng, and S. Jiang, “Scene recognition with prototype-agnostic
    scene layout,” *IEEE TIP*, vol. 29, pp. 5877–5888, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] A. Wang, J. Cai, J. Lu, and T.-J. Cham, “Modality and component aware
    feature fusion for RGB-D scene classification,” in *CVPR*, 2016, pp. 5995–6004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] X. Song, S. Jiang, and L. Herranz, “Combining models from multiple sources
    for RGB-D scene recognition.” in *IJCAI*, 2017, pp. 4523–4529.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] D. Du, L. Wang, H. Wang, K. Zhao, and G. Wu, “Translate-to-recognize networks
    for RGB-D scene recognition,” in *CVPR*, 2019, pp. 11 836–11 845.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Y. Cheng, R. Cai, Z. Li, X. Zhao, and K. Huang, “Locality-sensitive deconvolution
    networks with gated fusion for RGB-D indoor semantic segmentation,” in *CVPR*,
    2017, pp. 3029–3037.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] H. Zhu, J.-B. Weibel, and S. Lu, “Discriminative multi-modal feature fusion
    for RGB-D indoor scene recognition,” in *CVPR*, 2016, pp. 2969–2976.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] R. Girshick, “Fast R-CNN,” in *ICCV*, 2015, pp. 1440–1448.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time
    object detection with region proposal networks,” *IEEE TPAMI*, vol. 39, no. 6,
    pp. 1137–1149, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] V. Badrinarayanan, A. Kendall, and R. Cipolla, “SegNet: A deep convolutional
    encoder-decoder architecture for image segmentation,” *IEEE TPAMI*, vol. 39, no. 12,
    pp. 2481–2495, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
    Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *IEEE TPAMI*, vol. 40, no. 4, pp. 834–848, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] S. Cai, J. Huang, D. Zeng, X. Ding, and J. Paisley, “MEnet: A metric expression
    network for salient object segmentation,” in *IJCAI*, 2018, pp. 598–605.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *CVPR*, 2009, pp. 248–255, http://image-net.org/download.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, “Sun database:
    Large-scale scene recognition from abbey to zoo,” in *CVPR*, 2010, pp. 3485–3492,
    http://places2.csail.mit.edu/download.html.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] R. G. Cinbis, J. Verbeek, and C. Schmid, “Approximate fisher kernels of
    non-iid image models for image categorization,” *IEEE TPAMI*, vol. 38, no. 6,
    pp. 1084–1098, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] X. Wei, S. L. Phung, and A. Bouzerdoum, “Visual descriptors for scene
    categorization: Experimental evaluation,” *AI Review*, vol. 45, no. 3, pp. 333–368,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] G. Cheng, J. Han, and X. Lu, “Remote sensing image scene classification:
    Benchmark and state of the art,” *Proceedings of the IEEE*, vol. 105, no. 10,
    pp. 1865–1883, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] L. Xie, F. Lee, L. Liu, K. Kotani, and Q. Chen, “Scene recognition: A
    comprehensive survey,” *Pattern Recognition*, vol. 102, p. 107205, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] K. Nogueira, O. A. Penatti, and J. A. Dos Santos, “Towards better exploiting
    convolutional neural networks for remote sensing scene classification,” *Pattern
    Recognition*, vol. 61, pp. 539–556, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] F. Hu, G.-S. Xia, J. Hu, and L. Zhang, “Transferring deep convolutional
    neural networks for the scene classification of high-resolution remote sensing
    imagery,” *Remote Sensing*, vol. 7, no. 11, pp. 14 680–14 707, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] A. Mesaros, T. Heittola, and T. Virtanen, “Tut database for acoustic scene
    classification and sound event detection,” in *EUSIPCO*, 2016, pp. 1128–1132.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Z. Ren, K. Qian, Y. Wang, Z. Zhang, V. Pandit, A. Baird, and B. Schuller,
    “Deep scalogram representations for acoustic scene classification,” *JAS*, vol. 5,
    no. 3, pp. 662–669, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] S. Lowry, N. Sünderhauf, P. Newman, J. J. Leonard, D. Cox, P. Corke, and
    M. J. Milford, “Visual place recognition: A survey,” *IEEE T-RO*, vol. 32, no. 1,
    pp. 1–19, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic, “NetVLAD:
    CNN architecture for weakly supervised place recognition,” in *CVPR*, 2016, pp.
    5297–5307.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] M. R. Boutell, J. Luo, X. Shen, and C. M. Brown, “Learning multi-label
    scene classification,” *Pattern Recognition*, vol. 37, no. 9, pp. 1757–1771, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] M.-L. Zhang and Z.-H. Zhou, “Multi-label learning by instance differentiation,”
    in *AAAI*, vol. 7, 2007, pp. 669–674.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] A. Quattoni and A. Torralba, “Recognizing indoor scenes,” in *CVPR*, 2009,
    pp. 413–420, http://web.mit.edu/torralba/www/indoor.html.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation
    and support inference from RGB-D images,” in *ECCV*, 2012, pp. 746–760, https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] S. Song, S. P. Lichtenberg, and J. Xiao, “SUN RGB-D: A RGB-D scene understanding
    benchmark suite,” in *CVPR*, 2015, pp. 567–576, https://github.com/ankurhanda/sunrgbd-meta-data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] S. Gupta, R. Girshick, P. Arbeláez, and J. Malik, “Learning rich features
    from RGB-D images for object detection and segmentation,” in *ECCV*, 2014, pp.
    345–360.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] G. A. Miller, “Wordnet: A lexical database for english,” *Communications
    of the ACM*, vol. 38, no. 11, pp. 39–41, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
    “The pascal visual object classes (voc) challenge,” *IJCV*, vol. 88, no. 2, pp.
    303–338, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *ICLR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *CVPR*,
    2015, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
    for accurate object detection and semantic segmentation,” in *CVPR*, 2014, pp.
    580–587.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are
    features in deep neural networks?” in *NeurIPS*, 2014, pp. 3320–3328.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] M. Cimpoi, S. Maji, and A. Vedaldi, “Deep filter banks for texture recognition
    and segmentation,” in *CVPR*, 2015, pp. 3828–3836.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] L. Liu, C. Shen, L. Wang, A. Van Den Hengel, and C. Wang, “Encoding high
    dimensional local features by sparse coding based fisher vectors,” in *NeurIPS*,
    2014, pp. 1143–1151.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Z. Bolei, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Object
    detectors emerge in deep scene CNNs,” *ICLR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] L. Liu, J. Chen, P. Fieguth, G. Zhao, R. Chellappa, and M. Pietikäinen,
    “From BoW to CNN: Two decades of texture representation for texture classification,”
    *IJCV*, vol. 127, no. 1, pp. 74–109, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the importance of
    initialization and momentum in deep learning,” in *ICML*, 2013, pp. 1139–1147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] B. Liu, J. Liu, J. Wang, and H. Lu, “Learning a representative and discriminative
    part model with deep convolutional features for scene recognition,” in *ACCV*,
    2014, pp. 643–658.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of the
    devil in the details: Delving deep into convolutional nets,” *arXiv:1405.3531*,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] X. Song, S. Jiang, L. Herranz, and C. Chen, “Learning effective RGB-D
    representations for scene recognition,” *IEEE TIP*, vol. 28, no. 2, pp. 980–993,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] M. Lin, Q. Chen, and S. Yan, “Network in network,” *ICLR*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] J. Sun and J. Ponce, “Learning discriminative part detectors for image
    classification and cosegmentation,” in *ICCV*, 2013, pp. 3400–3407.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] K. J. Shih, I. Endres, and D. Hoiem, “Learning discriminative collections
    of part detectors for object recognition,” *IEEE TPAMI*, vol. 37, no. 8, pp. 1571–1584,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Y. Sun, X. Wang, and X. Tang, “Deeply learned face representations are
    sparse, selective, and robust,” in *CVPR*, 2015, pp. 2892–2900.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] D. L. Donoho, A. Maleki, and A. Montanari, “Message-passing algorithms
    for compressed sensing,” *PNAS*, vol. 106, no. 45, pp. 18 914–18 919, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Y. Li, Z. Zhang, Y. Cheng, L. Wang, and T. Tan, “MAPNet: Multi-modal attentive
    pooling network for RGB-D indoor scene classification,” *Pattern Recognition*,
    vol. 90, pp. 436–449, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino,
    M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu *et al.*, “Learning to navigate
    in complex environments,” *ICLR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] T. J. Sejnowski, “The unreasonable effectiveness of deep learning in artificial
    intelligence,” *PNAS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] L. Xie, L. Zheng, J. Wang, A. L. Yuille, and Q. Tian, “Interactive: Inter-layer
    activeness propagation,” in *CVPR*, 2016, pp. 270–279.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] L. Xie, J. Wang, W. Lin, B. Zhang, and Q. Tian, “Towards reversal-invariant
    image representation,” *IJCV*, vol. 123, no. 2, pp. 226–250, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] M. Rezanejad, G. Downs, J. Wilder, D. B. Walther, A. Jepson, S. Dickinson,
    and K. Siddiqi, “Scene categorization from contours: Medial axis based salience
    measures,” in *CVPR*, 2019, pp. 4116–4124.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] B.-B. Gao, X.-S. Wei, J. Wu, and W. Lin, “Deep spatial pyramid: The devil
    is once again in the details,” *arXiv:1504.05277*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Z. Xiong, Y. Yuan, and Q. Wang, “RGB-D scene recognition via spatial-related
    multi-modal feature learning,” *IEEE Access*, vol. 7, pp. 106 739–106 747, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] J. Sánchez, F. Perronnin, T. Mensink, and J. Verbeek, “Image classification
    with the Fisher vector: Theory and practice,” *IJCV*, vol. 105, no. 3, pp. 222–245,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] H. Jégou, M. Douze, C. Schmid, and P. Pérez, “Aggregating local descriptors
    into a compact image representation,” in *CVPR*, 2010, pp. 3304–3311.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] G. Csurka, C. Dance, L. Fan, J. Willamowski, and C. Bray, “Visual categorization
    with bags of keypoints,” in *ECCVW*, 2004, pp. 1–2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and M. Pietikäinen,
    “Deep learning for generic object detection: A survey,” *IJCV*, vol. 128, no. 2,
    pp. 261–318, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders, “Selective
    search for object recognition,” *IJCV*, vol. 104, no. 2, pp. 154–171, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] S. Singh, A. Gupta, and A. A. Efros, “Unsupervised discovery of mid-level
    discriminative patches,” in *ECCV*, 2012, pp. 73–86.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] P. Arbeláez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik, “Multiscale
    combinatorial grouping,” in *CVPR*, 2014, pp. 328–335.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “SSD: Single shot multibox detector,” in *ECCV*, 2016, pp. 21–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] J. Redmon and A. Farhadi, “Yolo9000: Better, faster, stronger,” in *CVPR*,
    2017, pp. 7263–7271.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
    Unified, real-time object detection,” in *CVPR*, 2016, pp. 779–788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, “Deeply-supervised
    nets,” in *AISTATS*, 2015, pp. 562–570.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *CVPR*, 2017, pp. 2117–2125.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] C. G. Snoek, M. Worring, and A. W. Smeulders, “Early versus late fusion
    in semantic video analysis,” in *Proceedings of the 13th annual ACM international
    conference on Multimedia*, 2005, pp. 399–402.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] H. Gunes and M. Piccardi, “Affect recognition from face and body: Early
    fusion vs. late fusion,” in *International conference on systems, man and cybernetics*,
    vol. 4, 2005, pp. 3437–3443.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Y. Dong, S. Gao, K. Tao, J. Liu, and H. Wang, “Performance evaluation
    of early and late fusion methods for generic semantics indexing,” *Pattern Analysis
    and Applications*, vol. 17, no. 1, pp. 37–50, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] J. Li, D. Lin, Y. Wang, G. Xu, Y. Zhang, C. Ding, and Y. Zhou, “Deep
    discriminative representation learning with attention map for scene classification,”
    *Remote Sensing*, vol. 12, no. 9, p. 1366, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] F. Zhang, B. Du, and L. Zhang, “Scene classification via a gradient boosting
    random convolutional network framework,” *IEEE TGRS*, vol. 54, no. 3, pp. 1793–1802,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] L. Wang, Z. Wang, W. Du, and Y. Qiao, “Object-scene convolutional neural
    networks for event recognition in images,” in *CVPRW*, 2015, pp. 30–35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] S. Xia, J. Zeng, L. Leng, and X. Fu, “WS-AM: Weakly supervised attention
    map for scene recognition,” *Electronics*, vol. 8, no. 10, p. 1072, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
    training by reducing internal covariate shift,” *ICML*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] H. Jin Kim and J.-M. Frahm, “Hierarchy of alternating specialists for
    scene recognition,” in *ECCV*, 2018, pp. 451–467.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] F. Perronnin and C. Dance, “Fisher kernels on visual vocabularies for
    image categorization,” in *CVPR*, 2007, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] R. Kwitt, N. Vasconcelos, and N. Rasiwasia, “Scene recognition on the
    semantic manifold,” in *ECCV*, 2012, pp. 359–372.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Z. Ghahramani, G. E. Hinton *et al.*, “The em algorithm for mixtures
    of factor analyzers,” University of Toronto, Tech. Rep., 1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] J. Verbeek, “Learning nonlinear image manifolds by global alignment of
    local linear models,” *IEEE TPAMI*, vol. 28, no. 8, pp. 1236–1250, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] P. Wei, F. Qin, F. Wan, Y. Zhu, J. Jiao, and Q. Ye, “Correlated topic
    vector for scene classification,” *IEEE TIP*, vol. 26, no. 7, pp. 3221–3234, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra,
    “Grad-CAM: Visual explanations from deep networks via gradient-based localization,”
    in *ICCV*, 2017, pp. 618–626.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] J. Zhang, S. A. Bargal, Z. Lin, J. Brandt, X. Shen, and S. Sclaroff,
    “Top-down neural attention by excitation backprop,” *IJCV*, vol. 126, no. 10,
    pp. 1084–1102, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] H. Seong, J. Hyun, and E. Kim, “Fosnet: An end-to-end trainable deep
    neural network for scene recognition,” *IEEE Access*, vol. 8, pp. 82 066–82 077,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] T. Joseph, K. G. Derpanis, and F. Z. Qureshi, “Joint spatial and layer
    attention for convolutional networks,” *arXiv:1901.05376*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Z. Niu, G. Hua, X. Gao, and Q. Tian, “Context aware topic model for scene
    recognition,” in *CVPR*, 2012, pp. 2743–2750.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Z. Zuo, B. Shuai, G. Wang, X. Liu, X. Wang, B. Wang, and Y. Chen, “Convolutional
    recurrent neural networks: Learning spatial dependencies for image representation,”
    in *CVPRW*, 2015, pp. 18–26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] X. Wang and E. Grimson, “Spatial latent dirichlet allocation,” in *NeurIPS*,
    2008, pp. 1577–1584.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] J. L. Elman, “Finding structure in time,” *Cognitive science*, vol. 14,
    no. 2, pp. 179–211, 1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] G. R. Cross and A. K. Jain, “Markov random field texture models,” *IEEE
    TPAMI*, no. 1, pp. 25–39, 1983.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] S. Z. Li, *Markov random field modeling in image analysis*.   Springer,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] N. Rasiwasia and N. Vasconcelos, “Holistic context models for visual
    recognition,” *IEEE TPAMI*, vol. 34, no. 5, pp. 902–917, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and
    locally connected networks on graphs,” *arXiv:1312.6203*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” *ICLR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun,
    “Graph neural networks: A review of methods and applications,” *arXiv:1812.08434*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Y. Yuan, Z. Xiong, and Q. Wang, “Acm: Adaptive cross-modal graph convolutional
    neural networks for RGB-D scene recognition,” in *AAAI*, vol. 33, 2019, pp. 9176–9184.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] X. Song, S. Jiang, B. Wang, C. Chen, and G. Chen, “Image representations
    with spatial object-to-object relations for RGB-D scene recognition,” *IEEE TIP*,
    vol. 29, pp. 525–537, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] S. A. Javed and A. K. Nelakanti, “Object-level context modeling for scene
    classification with context-CNN,” *arXiv:1705.04358*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] X. Song, S. Jiang, and L. Herranz, “Multi-scale multi-feature context
    modeling for scene recognition in the semantic manifold,” *IEEE TIP*, vol. 26,
    no. 6, pp. 2721–2735, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” *ICLR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Z. Zuo, G. Wang, B. Shuai, L. Zhao, Q. Yang, and X. Jiang, “Learning
    discriminative and shareable features for scene classification,” in *ECCV*, 2014,
    pp. 552–568.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] S. Jiang, G. Chen, X. Song, and L. Liu, “Deep patch representations with
    shared codebook for scene classification,” *ACM TOMM*, vol. 15, no. 1s, pp. 1–17,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] B. E. Boser, I. M. Guyon, and V. N. Vapnik, “A training algorithm for
    optimal margin classifiers,” in *Annual workshop on Computational learning theory*,
    1992, pp. 144–152.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Y. Li, J. Zhang, Y. Cheng, K. Huang, and T. Tan, “Df2net: Discriminative
    feature learning and fusion network for RGB-D indoor scene classification,” in
    *AAAI*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] J. C. Van Gemert, C. J. Veenman, A. W. Smeulders, and J.-M. Geusebroek,
    “Visual word ambiguity,” *IEEE TPAMI*, vol. 32, no. 7, pp. 1271–1283, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] B. Schölkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson,
    “Estimating the support of a high-dimensional distribution,” *Neural computation*,
    vol. 13, no. 7, pp. 1443–1471, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] T. Durand, N. Thome, and M. Cord, “Mantra: Minimum maximum latent structural
    svm for image classification and ranking,” in *ICCV*, 2015, pp. 2713–2721.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] R. Socher, B. Huval, B. Bath, C. D. Manning, and A. Y. Ng, “Convolutional-recursive
    deep learning for 3d object classification,” in *NeurIPS*, 2012, pp. 656–664.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] A. Wang, J. Cai, J. Lu, and T.-J. Cham, “MMSS: Multi-modal sharable and
    specific feature learning for RGB-D object recognition,” in *ICCV*, 2015, pp.
    1125–1133.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Y. Cheng, X. Zhao, R. Cai, Z. Li, K. Huang, Y. Rui *et al.*, “Semi-supervised
    multimodal deep learning for RGB-D object recognition,” *IJCAI*, pp. 3346–3351,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Q. Wang, M. Chen, F. Nie, and X. Li, “Detecting coherent groups in crowd
    scenes by multiview clustering,” *IEEE TPAMI*, vol. 42, no. 1, pp. 46–58, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Y. Zheng and X. Gao, “Indoor scene recognition via multi-task metric
    multi-kernel learning from RGB-D images,” *Multimedia Tools and Applications*,
    vol. 76, no. 3, pp. 4427–4443, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] B. Thompson, “Canonical correlation analysis,” *Encyclopedia of statistics
    in behavioral science*, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] G. Andrew, R. Arora, J. Bilmes, and K. Livescu, “Deep canonical correlation
    analysis,” in *ICML*, 2013, pp. 1247–1255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] J. Wu, B.-B. Gao, and G. Liu, “Representing sets of instances for visual
    recognition,” in *AAAI*, 2016, pp. 2237–2243.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] J. H. Bappy, S. Paul, and A. K. Roy-Chowdhury, “Online adaptation for
    joint scene and object classification,” in *ECCV*, 2016, pp. 227–243.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Z. Zhao and M. Larson, “From volcano to toyshop: Adaptive discriminative
    region discovery for scene recognition,” in *ACM MM*, 2018, pp. 1760–1768.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] M. Koskela and J. Laaksonen, “Convolutional network features for scene
    recognition,” in *ACM MM*, 2014, pp. 1169–1172.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] G. Nascimento, C. Laranjeira, V. Braz, A. Lacerda, and E. R. Nascimento,
    “A robust indoor scene recognition method based on sparse representation,” in
    *CIARP*, 2017, pp. 408–415.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Y. Liao, S. Kodagoda, Y. Wang, L. Shi, and Y. Liu, “Understand scene
    categories by objects: A semantic regularized scene classifier using convolutional
    neural networks,” in *CRA*, 2016, pp. 2318–2325.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Z. Cai and L. Shao, “RGB-D scene classification via multi-modal feature
    learning,” *Cognitive Computation*, vol. 11, no. 6, pp. 825–840, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] A. Ayub and A. Wagner, “Cbcl: Brain-inspired model for RGB-D indoor scene
    classification,” *arXiv:1911.00155*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] X. Song, L. Herranz, and S. Jiang, “Depth CNNs for RGB-D scene recognition:
    Learning from scratch better than transferring from RGB-CNNs,” in *AAAI*, vol. 31,
    no. 1, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] M. M. Najafabadi, F. Villanustre, T. M. Khoshgoftaar, N. Seliya *et al.*,
    “Deep learning applications and challenges in big data analytics,” *Journal of
    Big Data*, vol. 2, no. 1, p. 1, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement
    learning,” *ICLR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] T. Elsken, J. H. Metzen, F. Hutter *et al.*, “Neural architecture search:
    A survey.” *JMLR*, vol. 20, no. 55, pp. 1–21, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] J. Xiao, K. A. Ehinger, J. Hays, A. Torralba, and A. Oliva, “SUN database:
    Exploring a large collection of scene categories,” *IJCV*, vol. 119, no. 1, pp.
    3–22, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] O. Chapelle, B. Scholkopf, and A. Zien, “Semi-supervised learning,” *IEEE
    TNN*, vol. 20, no. 3, pp. 542–542, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] H. B. Barlow, “Unsupervised learning,” *Neural computation*, vol. 1,
    no. 3, pp. 295–311, 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] A. Kolesnikov, X. Zhai, and L. Beyer, “Revisiting self-supervised visual
    representation learning,” in *CVPR*, 2019, pp. 1920–1929.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Y.-X. Wang and M. Hebert, “Learning to learn: Model regression networks
    for easy small sample learning,” in *ECCV*, 2016, pp. 616–634.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] L. Fei-Fei, R. Fergus, and P. Perona, “One-shot learning of object categories,”
    *IEEE TPAMI*, vol. 28, no. 4, pp. 594–611, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, “Human-level concept
    learning through probabilistic program induction,” *Science*, vol. 350, no. 6266,
    pp. 1332–1338, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] M. Long, Y. Cao, J. Wang, and M. Jordan, “Learning transferable features
    with deep adaptation networks,” in *ICML*, 2015, pp. 97–105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] M. Wang and W. Deng, “Deep visual domain adaptation: A survey,” *Neurocomputing*,
    vol. 312, pp. 135–153, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] K.-C. Peng, Z. Wu, and J. Ernst, “Zero-shot deep domain adaptation,”
    in *ECCV*, 2018, pp. 764–781.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] H. Li, S. Jialin Pan, S. Wang, and A. C. Kot, “Domain generalization
    with adversarial feature learning,” in *CVPR*, 2018, pp. 5400–5409.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] X.-Y. Zhang, C.-L. Liu, and C. Y. Suen, “Towards robust pattern recognition:
    A review,” *Proceedings of the IEEE*, vol. 108, no. 6, pp. 894–922, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] U. Shaham, Y. Yamada, and S. Negahban, “Understanding adversarial training:
    Increasing local stability of supervised models through robust optimization,”
    *Neurocomputing*, vol. 307, pp. 195–204, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] C. Qin, J. Martens, S. Gowal, D. Krishnan, K. Dvijotham, A. Fawzi, S. De,
    R. Stanforth, and P. Kohli, “Adversarial robustness through local linearization,”
    in *NeurIPS*, 2019, pp. 13 847–13 856.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] A. Shafahi, M. Najibi, M. A. Ghiasi, Z. Xu, J. Dickerson, C. Studer,
    L. S. Davis, G. Taylor, and T. Goldstein, “Adversarial training for free!” in
    *NeurIPS*, 2019, pp. 3358–3369.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
    M. Marchand, and V. Lempitsky, “Domain-adversarial training of neural networks,”
    *JMLR*, vol. 17, no. 1, pp. 2096–2030, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, “Synthesizing robust
    adversarial examples,” in *ICML*, 2018, pp. 284–293.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] A. R. Dargazany, P. Stegagno, and K. Mankodiya, “WearableDL: Wearable
    internet-of-things and deep learning for big data analytics—concept, literature,
    and future,” *Mobile Information Systems*, vol. 2018, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] L. Shen, Z. Lin, and Q. Huang, “Relay backpropagation for effective learning
    of deep convolutional neural networks,” in *ECCV*, 2016, pp. 467–482.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] F. Thabtah, S. Hammoud, F. Kamalov, and A. Gonsalves, “Data imbalance
    in classification: Experimental evaluation,” *Information Sciences*, vol. 513,
    pp. 429–441, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] M. Buda, A. Maki, and M. A. Mazurowski, “A systematic study of the class
    imbalance problem in convolutional neural networks,” *Neural Networks*, vol. 106,
    pp. 249–259, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] J. M. Johnson and T. M. Khoshgoftaar, “Survey on deep learning with class
    imbalance,” *Journal of Big Data*, vol. 6, no. 1, p. 27, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] Y. Guo, Y. Liu, A. Oerlemans, S. Lao, S. Wu, and M. S. Lew, “Deep learning
    for visual understanding: A review,” *Neurocomputing*, vol. 187, pp. 27–48, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Z. Chen and B. Liu, “Lifelong machine learning,” *Synthesis Lectures
    on Artificial Intelligence and Machine Learning*, vol. 12, no. 3, pp. 1–207, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] S. Thrun and T. M. Mitchell, “Lifelong robot learning,” *Robotics and
    autonomous systems*, vol. 15, no. 1-2, pp. 25–46, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] D. Hassabis, D. Kumaran, C. Summerfield, and M. Botvinick, “Neuroscience-inspired
    artificial intelligence,” *Neuron*, vol. 95, no. 2, pp. 245–258, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] M.-L. Zhang and Z.-H. Zhou, “Ml-knn: A lazy learning approach to multi-label
    learning,” *Pattern Recognition*, vol. 40, no. 7, pp. 2038–2048, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] C. Feichtenhofer, A. Pinz, and R. P. Wildes, “Temporal residual networks
    for dynamic scene recognition,” in *CVPR*, 2017, pp. 4728–4737.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner,
    “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in *CVPR*, 2017,
    pp. 5828–5839.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
    spatiotemporal features with 3d convolutional networks,” in *ICCV*, 2015, pp.
    4489–4497.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3D classification and segmentation,” in *CVPR*, 2018, pp. 652–660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] D. G. Lowe, “Object recognition from local scale-invariant features,”
    in *IJCV*, vol. 2, 1999, pp. 1150–1157.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] A. Oliva and A. Torralba, “Modeling the shape of the scene: A holistic
    representation of the spatial envelope,” *IJCV*, vol. 42, no. 3, pp. 145–175,
    2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,”
    in *CVPR*, vol. 1, 2005, pp. 886–893.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] J. Wu and J. M. Rehg, “CENTRIST: A visual descriptor for scene categorization,”
    *IEEE TPAMI*, vol. 33, no. 8, pp. 1489–1501, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] J. Sivic and A. Zisserman, “Video google: A text retrieval approach to
    object matching in videos,” in *ICCV*, vol. 2, 2003, p. 1470–1477.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] L.-J. Li, H. Su, Y. Lim, and L. Fei-Fei, “Objects as attributes for scene
    classification,” in *ECCV*, 2010, pp. 57–69.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] M. Juneja, A. Vedaldi, C. Jawahar, and A. Zisserman, “Blocks that shout:
    Distinctive parts for scene classification,” in *CVPR*, 2013, pp. 923–930.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, “CNN features
    off-the-shelf: An astounding baseline for recognition,” in *CVPRW*, 2014, pp.
    806–813.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] N. Vasconcelos and A. Lippman, “A probabilistic architecture for content-based
    image retrieval,” in *CVPR*, 2000, pp. 216–221.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] C. Wallraven, B. Caputo, and A. Graf, “Recognition with local features:
    The kernel recipe,” in *ICCV*, 2003, pp. 257–264.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
    *IJCV*, vol. 60, no. 2, pp. 91–110, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] A. Oliva and A. Torralba, “Building the gist of a scene: The role of
    global image features in recognition,” *Progress in brain research*, vol. 155,
    pp. 23–36, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] T. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution gray-scale
    and rotation invariant texture classification with local binary patterns,” *IEEE
    TPAMI*, vol. 24, no. 7, pp. 971–987, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] P. Felzenszwalb, D. McAllester, and D. Ramanan, “A discriminatively trained,
    multiscale, deformable part model,” in *CVPR*, 2008, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, “Object
    detection with discriminatively trained part-based models,” *IEEE TPAMI*, vol. 32,
    no. 9, pp. 1627–1645, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] M. Pandey and S. Lazebnik, “Scene recognition and weakly supervised object
    localization with deformable part-based models,” in *ICCV*, 2011, pp. 1307–1314.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,”
    *JMLR*, vol. 3, no. 1, pp. 993–1022, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] K. Grauman and T. Darrell, “The pyramid match kernel: Discriminative
    classification with sets of image features,” in *ICCV*, vol. 2, 2005, pp. 1458–1465.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] L. Zhou, Z. Zhou, and D. Hu, “Scene classification using a multi-resolution
    bag-of-features model,” *Pattern Recognition*, vol. 46, no. 1, pp. 424–433, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] L. Xie, J. Wang, B. Guo, B. Zhang, and Q. Tian, “Orientational pyramid
    matching for recognizing indoor scenes,” in *CVPR*, 2014, pp. 3734–3741.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] J. Yang, K. Yu, Y. Gong, and T. Huang, “Linear spatial pyramid matching
    using sparse coding for image classification,” in *CVPR*, 2009, pp. 1794–1801.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] S. Gao, I. W.-H. Tsang, L.-T. Chia, and P. Zhao, “Local features are
    not lonely–laplacian sparse coding for image classification,” in *CVPR*, 2010,
    pp. 3555–3561.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong, “Locality-constrained
    linear coding for image classification,” in *CVPR*, 2010, pp. 3360–3367.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] J. Wu and J. M. Rehg, “Beyond the euclidean distance: Creating effective
    visual codebooks using the histogram intersection kernel,” in *ICCV*, 2009, pp.
    630–637.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] J. Qin and N. H. Yung, “Scene categorization via contextual visual words,”
    *Pattern Recognition*, vol. 43, no. 5, pp. 1874–1888, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] L. Bo and C. Sminchisescu, “Efficient match kernel between sets of features
    for visual recognition,” in *NeurIPS*, 2009, pp. 135–143.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] P. Wang, J. Wang, G. Zeng, W. Xu, H. Zha, and S. Li, “Supervised kernel
    descriptors for visual recognition,” in *CVPR*, 2013, pp. 2858–2865.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] L.-J. Li, H. Su, Y. Lim, and L. Fei-Fei, “Object bank: An object-level
    image representation for high-level visual recognition,” *IJCV*, vol. 107, no. 1,
    pp. 20–39, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] L. Zhang, X. Zhen, and L. Shao, “Learning object-to-class kernels for
    scene classification,” *IEEE TIP*, vol. 23, no. 8, pp. 3241–3253, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] F. Sadeghi and M. F. Tappen, “Latent pyramidal regions for recognizing
    scenes,” in *ECCV*, 2012, pp. 228–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] J. Yu, D. Tao, Y. Rui, and J. Cheng, “Pairwise constraints based multiview
    features fusion for scene classification,” *Pattern Recognition*, vol. 46, no. 2,
    pp. 483–496, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, “Deepface: Closing the
    gap to human-level performance in face verification,” in *CVPR*, 2014, pp. 1701–1708.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun, “Deep learning
    for 3d point clouds: A survey,” *IEEE TPAMI*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed *et al.*, “Deep
    neural networks for acoustic modeling in speech recognition,” *IEEE Signal Processing
    Magazine*, vol. 29, no. 6, pp. 82–97, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] C. Chen, A. Seff, A. Kornhauser, and J. Xiao, “Deepdriving: Learning
    affordance for direct perception in autonomous driving,” in *ICCV*, 2015, pp.
    2722–2730.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau,
    and S. Thrun, “Dermatologist level classification of skin cancer with deep neural
    networks,” *Nature*, vol. 542, no. 7639, pp. 115–118, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] S. M. McKinney, M. Sieniek, V. Godbole, J. Godwin, N. Antropova *et al.*,
    “International evaluation of an AI system for breast cancer screening,” *Nature*,
    vol. 577, no. 7788, pp. 89–94, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi *et al.*, “Google’s
    neural machine translation system: Bridging the gap between human and machine
    translation,” *arXiv:1609.08144*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre *et al.*, “Mastering
    the game of go with deep neural networks and tree search,” *Nature*, vol. 529,
    no. 7587, pp. 484–489, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang *et al.*,
    “Mastering the game of go without human knowledge,” *Nature*, vol. 550, no. 7676,
    pp. 354–359, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai *et al.*,
    “A general reinforcement learning algorithm that masters chess, shogi, and go
    through self-play,” *Science*, vol. 362, no. 6419, pp. 1140–1144, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik *et al.*,
    “Grandmaster level in starcraft ii using multi-agent reinforcement learning,”
    *Nature*, vol. 575, no. 7782, pp. 350–354, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] P. M. DeVries, F. Viégas, M. Wattenberg, and B. J. Meade, “Deep learning
    of aftershock patterns following large earthquakes,” *Nature*, vol. 560, no. 7720,
    pp. 632–634, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] https://www.technologyreview.com/lists/technologies/2020/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] J. M. Stokes, K. Yang, K. Swanson, W. Jin, A. Cubillos-Ruiz, N. M. Donghia,
    C. R. MacNair, S. French, L. A. Carfrae, Z. Bloom-Ackerman *et al.*, “A deep learning
    approach to antibiotic discovery,” *Cell*, vol. 180, no. 4, pp. 688–702, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] J. Schmidhuber, “Deep learning in neural networks: An overview,” *Neural
    networks*, vol. 61, pp. 85–117, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional
    networks,” in *ECCV*, 2014, pp. 818–833.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] S. Ioffe, “Batch renormalization: Towards reducing minibatch dependence
    in batch-normalized models,” in *NeurIPS*, 2017, pp. 1945–1953.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] I. Goodfellow, Y. Bengio, and A. Courville, *Deep learning*.   MIT press,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi *et al.*,
    “A survey on deep learning in medical image analysis,” *Medical image analysis*,
    vol. 42, pp. 60–88, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *et al.*, “Recent advances in convolutional neural networks,”
    *Pattern Recognition*, vol. 77, pp. 354–377, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes, M.-L. Shyu,
    S.-C. Chen, and S. Iyengar, “A survey on deep learning: Algorithms, techniques,
    and applications,” *Computing Surveys*, vol. 51, no. 5, pp. 1–36, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Delu Zeng received his Ph.D. degree in electronic and information engineering
    from South China University of Technology, China, in 2009\. He is now a full professor
    in the School of Mathematics in South China University of Technology, China. He
    has been the visiting scholar of Columbia University, University of Oulu, University
    of Waterloo. He has been focusing his research in applied mathematics and its
    interdisciplinary applications. His research interests include numerical calculations,
    applications of partial differential equations, optimizations, machine learning
    and their applications in image processing, and data analysis. |'
  prefs: []
  type: TYPE_TB
- en: '| Minyu Liao received her B.S. degree in applied mathematics from Shantou University,
    China, in 2018\. She is currently pursuing the master’s degrees in computational
    mathematics with South China University of Technology, China. Her research interests
    include computer vision, scene recognition, and deep learning. |'
  prefs: []
  type: TYPE_TB
- en: '| Mohammad Tavakolian received the M.Sc. degree in electrical engineering from
    Tafresh University, Iran in 2013\. Currently, he is a Ph.D. student at the Center
    for Machine and Signal Analysis (CMVS) of the University of Oulu, Finland. He
    has authored several journal and conference papers in IJCV, PRL, ICCV, ECCV, and
    ACCV. His research interests include representation learning, data efficient learning,
    computer vision, healthcare, and face analysis. |'
  prefs: []
  type: TYPE_TB
- en: '| Yulan Guo He received his Ph.D. degrees from National University of Defense
    Technology (NUDT) in 2015, where he is currently an associate professor. He was
    a visiting Ph.D. student with the University of Western Australia from 2011 to
    2014\. He has authored over 90 articles in journals and conferences, such as the
    IEEE TPAMI and IJCV. His current research interests focus on 3D vision, particularly
    on 3D feature learning, 3D modeling, 3D object recognition, and scene understanding.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bolei Zhou is currently an assistant professor at the Chinese University
    of Hong Kong, China. He received his Ph.D. from the Massachusetts Institute of
    Technology in 2018\. He received his M.phil from the Chinese University of Hong
    Kong and B.Eng. degree from the Shanghai Jiao Tong University in 2010\. He has
    authored over 70 articles in journals and conferences, such as IEEE TPAMI, ECCV,
    CVPR and AAAI. He is interested in understanding various human-centric properties
    of AI models beyond their performance, such as explainability, interpretability,
    steerability, generalization, fairness and bias. |'
  prefs: []
  type: TYPE_TB
- en: '| Dewen Hu received the B.S. and M.S. degrees from Xi’an Jiaotong University,
    China, in 1983 and 1986, respectively, and the Ph.D. degree from the National
    University of Defense Technology in 1999\. He is currently a Professor at School
    of Intelligent Science, National University of Defense Technology. From October
    1995 to October 1996, he was a Visiting Scholar with the University of Sheffield,
    U.K. His research interests include image processing, system identification and
    control, neural networks, and cognitive science. |'
  prefs: []
  type: TYPE_TB
- en: '| Matti Pietikäinen received his Ph.D. degree from the University of Oulu,
    Finland. He is now emeritus professor with the Center for Machine Vision and Signal
    Analysis, University of Oulu. He is a fellow of the IEEE for fundamental contributions,
    *e.g.,* , to Local Binary Pattern (LBP) methodology, texture based image and video
    analysis, and facial image analysis. He has authored more than 350 refereed papers
    in international journals, books, and conferences. His papers have nearly 68,700
    citations in Google Scholar (h-index 92). He was the recipient of the IAPR King-Sun
    Fu Prize 2018 for fundamental contributions to texture analysis and facial image
    analysis. |'
  prefs: []
  type: TYPE_TB
- en: '| Li Liu received the Ph.D. degree in information and communication engineering
    from the National University of Defense Technology (NUDT), China, in 2012\. She
    is currently a professor with NUDT. She spent two years as a Visiting Student
    at the University of Waterloo, Canada, from 2008 to 2010\. From 2015 to 2016,
    she spent ten months visiting the Multimedia Laboratory at the Chinese University
    of Hong Kong. From 2016.12 to 2018.9, she worked as a senior researcher at the
    Machine Vision Group at the University of Oulu, Finland. Her current research
    interests include computer vision, pattern recognition and machine learning. Her
    papers have currently over 3500+ citations in Google Scholar. |'
  prefs: []
  type: TYPE_TB
