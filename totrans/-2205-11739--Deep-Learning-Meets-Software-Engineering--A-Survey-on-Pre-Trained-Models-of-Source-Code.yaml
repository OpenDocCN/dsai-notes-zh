- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:46:16'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:46:16
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2205.11739] Deep Learning Meets Software Engineering: A Survey on Pre-Trained
    Models of Source Code'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2205.11739] 深度学习与软件工程：源代码预训练模型的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2205.11739](https://ar5iv.labs.arxiv.org/html/2205.11739)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2205.11739](https://ar5iv.labs.arxiv.org/html/2205.11739)
- en: 'Deep Learning Meets Software Engineering:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习与软件工程：
- en: A Survey on Pre-Trained Models of Source Code
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码预训练模型的综述
- en: Changan Niu¹    Chuanyi Li¹    Bin Luo¹ and Vincent Ng²
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 常安·牛¹    楚安·李¹    卜宾¹ 和 文森特·吴²
- en: ¹State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing,
    China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹南京大学新型软件技术国家重点实验室，中国南京
- en: ²Human Language Technology Research Institute, University of Texas at Dallas,
    Richardson, Texas, USA
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ²人类语言技术研究所，德克萨斯大学达拉斯分校，美国德克萨斯州理查森
- en: niu.ca@outlook.com, {lcy,luobin}@nju.edu.cn, vince@hlt.utdallas.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: niu.ca@outlook.com, {lcy,luobin}@nju.edu.cn, vince@hlt.utdallas.edu
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent years have seen the successful application of deep learning to software
    engineering (SE). In particular, the development and use of pre-trained models
    of source code has enabled state-of-the-art results to be achieved on a wide variety
    of SE tasks. This paper provides an overview of this rapidly advancing field of
    research and reflects on future research directions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习在软件工程（SE）中的成功应用已经显著。特别是，源代码预训练模型的发展和使用使得在各种软件工程任务中取得了最先进的成果。本文概述了这一快速发展的研究领域，并反思了未来的研究方向。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Once upon a time the state of software intelligence in software engineering
    (SE) was very rudimentary, with many of the decisions supported by gut feeling
    and at best through consultation with senior developers Hassan and Xie ([2010](#bib.bib19)).
    As a wealth of data has been generated in the software development and evolution
    lifecycle over the years, the software development and evolution paradigm has
    also shifted from human experience-based to data-driven decision making. While
    AI researchers are fully aware of the impact deep learning has on AI application
    domains such as computer vision and natural language processing (NLP), many are
    not aware of the extensive and successful applications of deep learning technologies
    to SE tasks in recent years.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从前，软件工程（SE）中的软件智能状态非常原始，许多决策依赖于直觉，最多通过咨询资深开发者Hassan和Xie ([2010](#bib.bib19))。随着多年来软件开发和演化生命周期中生成的数据不断增加，软件开发和演化的范式也从基于人类经验转变为数据驱动的决策。虽然人工智能研究者完全意识到深度学习对计算机视觉和自然语言处理（NLP）等人工智能应用领域的影响，但许多人并未意识到近年来深度学习技术在软件工程任务中的广泛而成功的应用。
- en: Though successful, the application of deep learning is not without challenges.
    One such challenge concerns the need for a large, typically costly-to-obtain,
    annotated training set to train the millions or even billions of parameters in
    deep neural networks. To address this data annotation bottleneck, NLP researchers
    have come up with an idea that can arguably be considered a breakthrough in recent
    deep learning research, namely pre-training Dai and Le ([2015](#bib.bib10)); Howard
    and Ruder ([2018](#bib.bib22)); Peters et al. ([2018](#bib.bib46)). Rather than
    training a model from scratch (i.e., with randomly initialized network weights),
    which typically requires a lot of task-specific annotated data, one can first
    pre-train it on one or more so-called self-supervised tasks (i.e., tasks for which
    annotated data can be automatically generated and therefore large amounts of training
    data are readily available) so that its weights encode general linguistic and
    commonsense knowledge about language, and then the resulting pre-trained model
    can be fine-tuned to learn the target task using (a potentially small amount of)
    task-specific annotated training data in the usual supervised manner. A large
    number of pre-trained language models have been developed and widely used in NLP,
    such as BERT Devlin et al. ([2018](#bib.bib13)), XLNet Yang et al. ([2019](#bib.bib71)),
    RoBERTa Liu et al. ([2019](#bib.bib35)), ELECTRA Clark et al. ([2019](#bib.bib8)),
    GPT-2 Radford et al. ([2019](#bib.bib52)), T5 Raffel et al. ([2020](#bib.bib53)),
    and BART Lewis et al. ([2020](#bib.bib34)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了成功，深度学习的应用仍然面临挑战。其中一个挑战是需要一个大型、通常成本高昂的注释训练集，以训练深度神经网络中的数百万甚至数十亿个参数。为了应对这一数据注释瓶颈，自然语言处理（NLP）研究人员提出了一种可以被认为是近期深度学习研究突破的想法，即预训练**Dai
    和 Le** ([2015](#bib.bib10))；**Howard 和 Ruder** ([2018](#bib.bib22))；**Peters 等**
    ([2018](#bib.bib46))。与从头开始训练模型（即，使用随机初始化的网络权重）不同，这通常需要大量特定任务的注释数据，可以先在一个或多个所谓的自监督任务（即可以自动生成注释数据，因此大量训练数据随时可用的任务）上进行预训练，以使其权重编码关于语言的一般语言学和常识知识，然后可以将得到的预训练模型微调以使用（可能少量的）特定任务注释训练数据来学习目标任务。已经开发了大量预训练语言模型，并在NLP中广泛使用，例如**BERT**
    **Devlin 等** ([2018](#bib.bib13))，**XLNet** **Yang 等** ([2019](#bib.bib71))，**RoBERTa**
    **Liu 等** ([2019](#bib.bib35))，**ELECTRA** **Clark 等** ([2019](#bib.bib8))，**GPT-2**
    **Radford 等** ([2019](#bib.bib52))，**T5** **Raffel 等** ([2020](#bib.bib53))，和**BART**
    **Lewis 等** ([2020](#bib.bib34))。
- en: 'Can these pre-trained models be applied to SE tasks? Since source code can
    be viewed as a sequence of code tokens in the same way that natural language (NL)
    can be viewed as a sequence of word tokens, we can in principle retrain these
    models on source code and apply them to SE tasks. In practice, this is not ideal,
    as there are code-specific characteristics that may not be properly taken into
    account by these models. For instance, source code is not as homogeneous as NL:
    it is composed of both the code in a function body, which is written in programming
    language (PL), as well as optional comments written in NL. Treating both code
    and comments in a uniform manner (i.e., as a sequence of tokens) may not be the
    best way to exploit the two sources of information. In addition, code has syntactic
    structures (as defined in Abstract Syntax Trees (ASTs)) and semantic structures
    (as defined in Control Flow Graphs (CFGs)). While a few syntax-aware pre-trained
    models are recently developed in the NLP community (e.g., Xu et al. Xu et al.
    ([2021](#bib.bib70))), the majority of existing pre-trained models fail to exploit
    structured information. Consequently, SE researchers have developed a number of
    pre-trained models of source code (CodePTMs) that take into account the characteristics
    specific to source code in the past few years.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预训练模型可以应用于软件工程（SE）任务吗？由于源代码可以被视为代码令牌的序列，就像自然语言（NL）可以被视为单词令牌的序列一样，我们原则上可以对这些模型进行源代码的再训练，并将其应用于SE任务。然而，在实践中，这并不理想，因为存在一些代码特有的特征，可能没有被这些模型妥善考虑。例如，源代码不像NL那样同质化：它由函数体中的代码（用编程语言（PL）编写）和用NL编写的可选注释组成。以统一的方式（即，将两者视为令牌序列）对待代码和注释可能不是最好的利用这两种信息源的方法。此外，代码具有语法结构（如在抽象语法树（ASTs）中定义）和语义结构（如在控制流图（CFGs）中定义）。虽然最近NLP社区开发了一些语法感知的预训练模型（例如**Xu
    等 Xu 等** ([2021](#bib.bib70))），但大多数现有的预训练模型未能利用结构化信息。因此，SE研究人员在过去几年中开发了许多考虑到源代码特定特征的预训练模型（CodePTMs）。
- en: Our goal in this paper is to raise the awareness of the AI audience on the impact
    that AI technologies — in this case the development and use of pre-trained models
    — have on SE, an important AI application domain, specifically by providing them
    with a survey of the recent development of CodePTMs and their successful application
    to SE tasks. We believe this survey will be of particular interest to (1) NLP
    researchers, especially those focusing on text summarization and generation, since
    many SE tasks (e.g., code summarization) involve NL generation; and (2) applied
    machine learning researchers, since the development of these models could have
    a big impact on SE. Though our target audience is AI researchers, we believe this
    paper could also be of high interest for the SE technology providers, raising
    their awareness on the added value AI technology could have in augmenting SE tooling
    to leverage the increasing complexity of software systems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本文中的目标是提升AI受众对AI技术（在这种情况下是预训练模型的开发和使用）对SE（一个重要的AI应用领域）影响的意识，特别是通过提供CodePTMs的最新发展及其在SE任务中的成功应用的调查。我们相信这项调查将特别吸引（1）NLP研究人员，特别是那些专注于文本摘要和生成的研究人员，因为许多SE任务（例如代码摘要）涉及到NL生成；以及（2）应用机器学习研究人员，因为这些模型的发展可能会对SE产生重大影响。尽管我们的目标受众是AI研究人员，但我们认为这篇论文对SE技术提供者也非常有吸引力，提升他们对AI技术在增强SE工具方面的附加价值的认识，以应对软件系统日益增加的复杂性。
- en: '| Type | I-O | Task | Definition | ID - Dataset | Metrics |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | I-O | 任务 | 定义 | ID - 数据集 | 指标 |'
- en: '| Und. | C-V | WB | Wrong Binary Operator: Check if a given piece of code contains
    any incorrect binary operators. | K1 - Kanade et al. Kanade et al. ([2020](#bib.bib30))
    | Acc |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| Und. | C-V | WB | 错误的二进制运算符：检查给定代码是否包含任何不正确的二进制运算符。 | K1 - Kanade et al.
    Kanade et al. ([2020](#bib.bib30)) | Acc |'
- en: '| ET | Exception Type: Predict the precise exception type. | K1 - Kanade et
    al. Kanade et al. ([2020](#bib.bib30)) | Acc |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| ET | 异常类型：预测精确的异常类型。 | K1 - Kanade et al. Kanade et al. ([2020](#bib.bib30))
    | Acc |'
- en: '| BD | Bug Detection / Defect Detection: Check if a given function contains
    a defect. | D1 - Devign Zhou et al. ([2019](#bib.bib74)) | Acc |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| BD | 错误检测 / 缺陷检测：检查给定函数是否包含缺陷。 | D1 - Devign Zhou et al. ([2019](#bib.bib74))
    | Acc |'
- en: '| P1 - Pradel et al. Pradel and Sen ([2018](#bib.bib49)) | Acc |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| P1 - Pradel et al. Pradel and Sen ([2018](#bib.bib49)) | Acc |'
- en: '| CD | Clone Detection: Determine whether two code snippets are semantically
    equivalent. | B1 - BigCloneBench Svajlenko et al. ([2014](#bib.bib59)) | F1 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| CD | 克隆检测：确定两个代码片段是否在语义上等价。 | B1 - BigCloneBench Svajlenko et al. ([2014](#bib.bib59))
    | F1 |'
- en: '| C1 - CLCDSA Nafi et al. ([2019](#bib.bib41)) | P/R/F1 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| C1 - CLCDSA Nafi et al. ([2019](#bib.bib41)) | P/R/F1 |'
- en: '| CC | Code Classification: Classify the category of a given function. | P2
    - POJ-104 Mou et al. ([2016](#bib.bib40)) | Acc/MAP@R |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| CC | 代码分类：对给定函数的类别进行分类。 | P2 - POJ-104 Mou et al. ([2016](#bib.bib40)) |
    Acc/MAP@R |'
- en: '| FD | Function-Docstring Mismatch: Determine whether a given function and
    the docstring correspond to each other. | K1 - Kanade et al. Kanade et al. ([2020](#bib.bib30))
    | Acc |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| FD | 函数-文档字符串不匹配：确定给定函数与文档字符串是否相符。 | K1 - Kanade et al. Kanade et al. ([2020](#bib.bib30))
    | Acc |'
- en: '| C-C | CR | Code-to-Code Retrieval: Retrieve semantically similar code for
    a given piece of query code. | C1 - CLCDSA Nafi et al. ([2019](#bib.bib41)) |
    Acc/MRR/NDCG |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| C-C | CR | 代码到代码检索：检索与给定查询代码在语义上相似的代码。 | C1 - CLCDSA Nafi et al. ([2019](#bib.bib41))
    | Acc/MRR/NDCG |'
- en: '| P2 - POJ-104 Mou et al. ([2016](#bib.bib40)) | MAP@R |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| P2 - POJ-104 Mou et al. ([2016](#bib.bib40)) | MAP@R |'
- en: '| VM | Variable-Misuse Localization and Repair: Identify the location of a
    misused variable and return the correct one. | V1 - Vasic et al. Vasic et al.
    ([2019](#bib.bib64)) | Acc |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| VM | 变量误用定位和修复：识别误用变量的位置并返回正确的变量。 | V1 - Vasic et al. Vasic et al. ([2019](#bib.bib64))
    | Acc |'
- en: '| CT | Cloze Test: Predict the masked token from code. | D2 - De Sousa et al. de
    Sousa and Hasselbring ([2021](#bib.bib12)) | Acc |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| CT | 填空测试：从代码中预测被遮蔽的标记。 | D2 - De Sousa et al. de Sousa and Hasselbring ([2021](#bib.bib12))
    | Acc |'
- en: '| NL-C | CS | Code Search / Text-to-Code Retrieval: Find the most relevant
    piece of code from a set of candidates for a given natural language description.
    | C2 - CodeSearchNet Husain et al. ([2019](#bib.bib25)) | MRR |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| NL-C | CS | 代码搜索 / 文本到代码检索：从候选集中找到与给定自然语言描述最相关的代码。 | C2 - CodeSearchNet Husain
    et al. ([2019](#bib.bib25)) | MRR |'
- en: '| C3 - AdvText Lu et al. ([2021](#bib.bib37)) | MRR/F1/Acc |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| C3 - AdvText Lu et al. ([2021](#bib.bib37)) | MRR/F1/Acc |'
- en: '| Gen. | C-C | CP | Code Completion: Predict the missing/following token(s)
    of a given code context. | S1 - Svyatkovskiy et al. Svyatkovskiy et al. ([2020](#bib.bib60))
    | RL/EditSim. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| Gen. | C-C | CP | 代码补全：预测给定代码上下文中的缺失/后续标记。 | S1 - Svyatkovskiy 等人 Svyatkovskiy
    等人 ([2020](#bib.bib60)) | RL/EditSim. |'
- en: '| L1 - Liu et al. Liu et al. ([2020](#bib.bib36)) | Acc |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| L1 - Liu 等人 Liu 等人 ([2020](#bib.bib36)) | Acc |'
- en: '| A1 - Alon et al. Alon et al. ([2020](#bib.bib3)) | Acc@k |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| A1 - Alon 等人 Alon 等人 ([2020](#bib.bib3)) | Acc@k |'
- en: '| TL | Code Translation: Translate the code in one programming language to
    the code in another programming language. | C4 - Chen et al. Chen et al. ([2018](#bib.bib6))
    | BLEU/Acc/CBLEU |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| TL | 代码翻译：将一种编程语言中的代码翻译成另一种编程语言中的代码。 | C4 - Chen 等人 Chen 等人 ([2018](#bib.bib6))
    | BLEU/Acc/CBLEU |'
- en: '| T1 - TransCorder Roziere et al. ([2020](#bib.bib56)) | Acc |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| T1 - TransCorder Roziere 等人 ([2020](#bib.bib56)) | Acc |'
- en: '| C1 - CLCDSA Nafi et al. ([2019](#bib.bib41)) | BLEU/RL/CIDER |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| C1 - CLCDSA Nafi 等人 ([2019](#bib.bib41)) | BLEU/RL/CIDER |'
- en: '| BF | Bug Fixing: Repair buggy code by generating the correct version. | T2
    - Tufano et al. Tufano et al. ([2019b](#bib.bib63)) | BLEU/Acc/CBLEU |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| BF | 错误修复：通过生成正确版本修复有错误的代码。 | T2 - Tufano 等人 Tufano 等人 ([2019b](#bib.bib63))
    | BLEU/Acc/CBLEU |'
- en: '| MG | Mutant Generation: Inject in working code a mutant for a real bug. |
    T3 - Tufano et al. Tufano et al. ([2019a](#bib.bib62)) | Acc |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| MG | 变异生成：在有效代码中注入一个真实错误的变异体。 | T3 - Tufano 等人 Tufano 等人 ([2019a](#bib.bib62))
    | Acc |'
- en: '| AG | Assert Generation: Generate a correct unit test assert statement. |
    W1 - Watson et al. Watson et al. ([2020](#bib.bib69)) | Acc@k |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| AG | 断言生成：生成正确的单元测试断言语句。 | W1 - Watson 等人 Watson 等人 ([2020](#bib.bib69))
    | Acc@k |'
- en: '| C-NL | SU | Code Summarization / Code Documentation: Generate a textual descrip-
    tion that describes the functionality of a function. | C2 - CodeSearchNet Husain
    et al. ([2019](#bib.bib25)) | BLEU |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| C-NL | SU | 代码总结 / 代码文档：生成描述函数功能的文本描述。 | C2 - CodeSearchNet Husain 等人 ([2019](#bib.bib25))
    | BLEU |'
- en: '| H1 - Haque et al. Haque et al. ([2020](#bib.bib18)) | BLEU/RL |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| H1 - Haque 等人 Haque 等人 ([2020](#bib.bib18)) | BLEU/RL |'
- en: '| H2 - Hu et al. Hu et al. ([2018a](#bib.bib23)) | BLEU |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| H2 - Hu 等人 Hu 等人 ([2018a](#bib.bib23)) | BLEU |'
- en: '| H3 - Hu et al. Hu et al. ([2018b](#bib.bib24)) | BLEU/METEOR |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| H3 - Hu 等人 Hu 等人 ([2018b](#bib.bib24)) | BLEU/METEOR |'
- en: '| M1 - Miceli et al. Miceli-Barone and Sennrich ([2017](#bib.bib39)) | BLEU
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| M1 - Miceli 等人 Miceli-Barone 和 Sennrich ([2017](#bib.bib39)) | BLEU |'
- en: '| MN | Method Naming / Extreme Code Summarization: Predict the function name
    of a given function body. | A2 - Allamanis et al. Allamanis et al. ([2016](#bib.bib2))
    | P/R/F1 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| MN | 方法命名 / 极端代码总结：预测给定函数体的函数名称。 | A2 - Allamanis 等人 Allamanis 等人 ([2016](#bib.bib2))
    | P/R/F1 |'
- en: '| E1 - ETH Py150 Raychev et al. ([2016](#bib.bib54)) | P/R/F1 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| E1 - ETH Py150 Raychev 等人 ([2016](#bib.bib54)) | P/R/F1 |'
- en: '| NL-C | CG | Code Generation: Generate code given a natural language description.
    | C5 - CONCODE Iyer et al. ([2018](#bib.bib26)) | BLEU/Acc/CBLEU |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| NL-C | CG | 代码生成：根据自然语言描述生成代码。 | C5 - CONCODE Iyer 等人 ([2018](#bib.bib26))
    | BLEU/Acc/CBLEU |'
- en: 'Table 1: Categorization of the 18 SE tasks to which CodePTMs have been applied.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：应用 CodePTMs 的 18 个 SE 任务的分类。
- en: 2 SE Tasks, Datasets, and Evaluation Metrics
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 SE 任务、数据集和评估指标
- en: 'SE studies problems concerning the design, development, maintenance, testing,
    and evolution of software systems. Table [1](#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of
    Source Code") enumerates the key SE tasks to which pre-trained models have been
    applied. As can be seen in the first two columns, we classify each task along
    two dimensions: (1) whether the task concerns understanding (Und.) or generation
    (Gen.); and (2) the type of input assumed by the task and the type of output produced
    (I-O), where C, NL, and V denote code, natural language, and extracted/predicted
    value, respectively.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 'SE 研究涉及软件系统的设计、开发、维护、测试和演化问题。表格 [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Deep
    Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source
    Code") 列出了应用预训练模型的关键 SE 任务。从前两列可以看出，我们将每个任务分为两个维度：(1) 任务是否涉及理解（Und.）或生成（Gen.）；以及
    (2) 任务假设的输入类型和生成的输出类型（I-O），其中 C、NL 和 V 分别表示代码、自然语言和提取/预测值。'
- en: 'In addition, Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Deep Learning Meets
    Software Engineering: A Survey on Pre-Trained Models of Source Code") shows for
    each task the benchmark dataset(s) and the corresponding evaluation metric(s).
    These metrics are fairly standard. For retrieval and classification tasks, metrics
    such as Acc (Accuracy Kanade et al. ([2020](#bib.bib30))), Acc@k (Accuracy computed
    over the top $k$ predicted answers Watson et al. ([2020](#bib.bib69))), Precision(P)/Recall(R)/F1 Nafi
    et al. ([2019](#bib.bib41)), MRR (Mean Reciprocal Rank Husain et al. ([2019](#bib.bib25))),
    MAP@R (Mean Average Precision Mou et al. ([2016](#bib.bib40))), and NDCG (Normalized
    Discounted Cumulative Gain Nafi et al. ([2019](#bib.bib41))) are typically used.
    For generation tasks, metrics developed in the NLP community for summarization
    and translation tasks, such as BLEU Papineni et al. ([2002](#bib.bib44)), ROUGE-L
    (RL) Haque et al. ([2020](#bib.bib18)), METEOR Hu et al. ([2018b](#bib.bib24)),
    CIDER Zhang et al. ([2021](#bib.bib73)), and EditSim Svyatkovskiy et al. ([2020](#bib.bib60))
    (an edit distance-based metric), as well as variants developed in the SE community,
    such as CodeBLEU (CBLEU) Ren et al. ([2020](#bib.bib55)), are used.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，表格 [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Deep Learning Meets Software Engineering:
    A Survey on Pre-Trained Models of Source Code") 显示了每个任务的基准数据集和相应的评估指标。这些指标相当标准。对于检索和分类任务，通常使用诸如
    Acc（准确率 Kanade et al. ([2020](#bib.bib30)))、Acc@k（前 $k$ 个预测答案的准确率 Watson et al.
    ([2020](#bib.bib69)))、Precision(P)/Recall(R)/F1 Nafi et al. ([2019](#bib.bib41))、MRR（平均倒数排名
    Husain et al. ([2019](#bib.bib25)))、MAP@R（均值平均精度 Mou et al. ([2016](#bib.bib40)))
    和 NDCG（标准化折扣累积增益 Nafi et al. ([2019](#bib.bib41))) 等指标。对于生成任务，使用 NLP 社区为摘要和翻译任务开发的指标，如
    BLEU Papineni et al. ([2002](#bib.bib44))、ROUGE-L (RL) Haque et al. ([2020](#bib.bib18))、METEOR
    Hu et al. ([2018b](#bib.bib24))、CIDER Zhang et al. ([2021](#bib.bib73)) 和 EditSim
    Svyatkovskiy et al. ([2020](#bib.bib60))（基于编辑距离的指标），以及 SE 社区开发的变体，如 CodeBLEU (CBLEU)
    Ren et al. ([2020](#bib.bib55))。 |'
- en: 3 CodePTMs
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 代码预训练模型
- en: In this section, we provide an overview of 20 CodePTMs recently developed in
    the SE community. To enable the reader to better understand their similarities
    and differences, as well as their relative strengths and weaknesses, we classify
    them along four dimensions, as described below.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了 SE 社区最近开发的 20 种代码预训练模型。为了使读者更好地了解它们的相似性和差异，以及它们的相对优缺点，我们将它们从四个维度进行分类，如下所述。
- en: 3.1 Architecture
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 架构
- en: '|  | Type | Task | Full Name and Description |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | 类型 | 任务 | 完整名称及描述 |'
- en: '| N L P | LM | FLM  Svyatkovskiy et al. ([2020](#bib.bib60)) | Forward LM:
    maximizes the conditional probabilities of all the words by taking their previous
    words as contexts. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| N L P | LM | FLM  Svyatkovskiy et al. ([2020](#bib.bib60)) | 前向语言模型：通过将前面的词作为上下文，最大化所有词的条件概率。
    |'
- en: '| FNP Qi et al. ([2021](#bib.bib50)) | Future N-gram Prediction: a variant
    of FLM that involves predicting the next $n$ ($n>1$) tokens simultaneously instead
    of one token. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| FNP Qi et al. ([2021](#bib.bib50)) | 未来 N-gram 预测：FLM 的一种变体，涉及同时预测下一个 $n$
    个 ($n>1$) 标记，而不是一个标记。 |'
- en: '| BiLM  Karampatsis and Sutton ([2020](#bib.bib31)) | Bidirectional LM: combines
    a forward LM and a backward LM, and jointly maximizes the likelihood of the tokens
    both directions. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| BiLM  Karampatsis and Sutton ([2020](#bib.bib31)) | 双向语言模型：结合前向语言模型和后向语言模型，联合最大化两个方向上标记的可能性。
    |'
- en: '| MLM | BMLM  de Sousa and Hasselbring ([2021](#bib.bib12)) | Basic version
    of MLM: randomly masks a certain percentage of tokens in the input, then predicts
    the masked tokens. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| MLM | BMLM  de Sousa and Hasselbring ([2021](#bib.bib12)) | MLM 的基础版本：随机遮蔽输入中某个百分比的标记，然后预测这些被遮蔽的标记。
    |'
- en: '| WWM  Buratti et al. ([2020](#bib.bib5)) | Whole Word Masking: if a word is
    masked, mask all subwords/tokens in it; then predict these masked tokens. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| WWM  Buratti et al. ([2020](#bib.bib5)) | 全词遮蔽：如果一个词被遮蔽，则遮蔽该词中的所有子词/标记；然后预测这些被遮蔽的标记。
    |'
- en: '| MASS  Niu et al. ([2022](#bib.bib42)) | MAsked Seq2Seq: reconstructs the
    sentence fragment given the remaining part of the sentence in the encoder-decoder
    framework. |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| MASS  Niu et al. ([2022](#bib.bib42)) | 遮蔽序列到序列：在编码器-解码器框架中，根据句子的其余部分重建句子片段。
    |'
- en: '| SMLM  Mastropaolo et al. ([2021](#bib.bib38)) | Seq2Seq MLM: randomly masks
    a set of token spans in the input and sequentially predicts them in the encoder-decoder
    framework. |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| SMLM  Mastropaolo et al. ([2021](#bib.bib38)) | Seq2Seq MLM：在输入中随机遮蔽一组标记范围，并在编码器-解码器框架中顺序预测这些标记。
    |'
- en: '| DAE | DAE  Ahmad et al. ([2021](#bib.bib1)) | Denoising Auto-Encoding: corrupts
    the input (by masking, deleting tokens, etc.) and uses the model to recover the
    original input. |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| DAE | DAE  Ahmad 等人 ([2021](#bib.bib1)) | 去噪自编码：通过掩盖、删除 tokens 等方式损坏输入，并使用模型恢复原始输入。
    |'
- en: '| CTL | NSP  Kanade et al. ([2020](#bib.bib30)) | Next Sentence Prediction:
    determines whether two given sentences (i.e., logical lines of code) are coherent.
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| CTL | NSP  Kanade 等人 ([2020](#bib.bib30)) | 下一句预测：确定给定的两句话（即逻辑行代码）是否连贯。 |'
- en: '| RTD  Feng et al. ([2020](#bib.bib16)) | Replaced Token Detection: identifies
    the replaced tokens in the input (i.e., tokens produced by a small generator network).
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| RTD  Feng 等人 ([2020](#bib.bib16)) | 替换 token 检测：识别输入中被替换的 tokens（即由小型生成网络生成的
    tokens）。 |'
- en: '| S E | CA | IMLM  Liu et al. ([2020](#bib.bib36)) | Identifier MLM: an adaptation
    of MLM to source code that masks only the identifiers in the code text. |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| S E | CA | IMLM  Liu 等人 ([2020](#bib.bib36)) | 标识符 MLM：一种对源代码进行 MLM 的适应，只掩盖代码文本中的标识符。
    |'
- en: '| SIMLM  Wang et al. ([2021b](#bib.bib68)) | Seq2Seq IMLM: an adaptation of
    Seq2Seq MLM to source code that masks only the identifiers in the code text. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| SIMLM  Wang 等人 ([2021b](#bib.bib68)) | Seq2Seq IMLM：对源代码的 Seq2Seq MLM 适应，仅掩盖代码文本中的标识符。
    |'
- en: '| IT  Wang et al. ([2021b](#bib.bib68)) | Identifier Tagging: determines if
    the input token at each position is an identifier or not via binary classification.
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| IT  Wang 等人 ([2021b](#bib.bib68)) | 标识符标记：通过二分类确定每个位置的输入 token 是否为标识符。 |'
- en: '| CCL  Peng et al. ([2021](#bib.bib45)) | Code Contrastive Learning: minimizes/maximizes
    the distances between the representations of similar/dissimilar code snippets.
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| CCL  Peng 等人 ([2021](#bib.bib45)) | 代码对比学习：最小化/最大化相似/不相似代码片段表示之间的距离。 |'
- en: '| SA | EP  Guo et al. ([2021](#bib.bib17)) | Edge Prediction: masks the edges
    connecting randomly selected nodes in a DFG, then predicts the masked edges. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| SA | EP  Guo 等人 ([2021](#bib.bib17)) | 边预测：掩盖 DFG 中随机选择的节点连接的边，然后预测掩盖的边。
    |'
- en: '| NOP  Jiang et al. ([2021](#bib.bib29)) | Node Order Prediction: randomly
    changes the order of some nodes in an AST, then determines if a change occurs.
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| NOP  Jiang 等人 ([2021](#bib.bib29)) | 节点顺序预测：随机更改 AST 中某些节点的顺序，然后判断是否发生了变化。
    |'
- en: '| C M A | CN | BDG  Wang et al. ([2021b](#bib.bib68)) | Bimodal Dual Generation:
    generates a NL summary if code is given, and generates code if NL is given. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| C M A | CN | BDG  Wang 等人 ([2021b](#bib.bib68)) | 双模态双生成：如果给定代码，则生成 NL 摘要；如果给定
    NL，则生成代码。 |'
- en: '| MNG  Niu et al. ([2022](#bib.bib42)) | Method Name Generation: generates
    the sub-token sequence of the method name based on a given method body. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| MNG  Niu 等人 ([2022](#bib.bib42)) | 方法名生成：基于给定的方法体生成方法名的子 token 序列。 |'
- en: '| CS | NA  Guo et al. ([2021](#bib.bib17)) | Node Alignment: samples nodes
    in a DFG, masks the edge connecting each node to its code token, then predicts
    the masked edges. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| CS | NA  Guo 等人 ([2021](#bib.bib17)) | 节点对齐：在 DFG 中采样节点，掩盖连接每个节点与其代码 token
    的边，然后预测掩盖的边。 |'
- en: '| TMLM  Jiang et al. ([2021](#bib.bib29)) | Tree MLM: masks some terminal nodes/identifiers
    in ASTs/code on encoder/decoder side, then generates complete code sequence. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| TMLM  Jiang 等人 ([2021](#bib.bib29)) | 树状 MLM：在编码器/解码器侧掩盖 AST/code 中的某些终端节点/标识符，然后生成完整的代码序列。
    |'
- en: '| VGVAE  Zhang et al. ([2021](#bib.bib73)) | vMF-Gaussian Variational Autoencoder:
    disentangles code semantics from code syntax under the supervision of a masked
    AST. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| VGVAE  Zhang 等人 ([2021](#bib.bib73)) | vMF-高斯变分自编码器：在掩盖 AST 的监督下，将代码语义与代码语法解耦。
    |'
- en: '| CAP  Niu et al. ([2022](#bib.bib42)) | Code-AST Prediction: determines whether
    the given code and AST correspond to each other. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| CAP  Niu 等人 ([2022](#bib.bib42)) | 代码-AST 预测：确定给定的代码和 AST 是否相互对应。 |'
- en: '| CLR  Zhang et al. ([2021](#bib.bib73)) | Cross-Language Reconstruction: reconstructs
    the code snippet in one PL from functionally equivalent code snippets in other
    PLs. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| CLR  Zhang 等人 ([2021](#bib.bib73)) | 跨语言重建：从其他编程语言中功能等效的代码片段重建一种编程语言中的代码片段。
    |'
- en: '| PD  Zhang et al. ([2021](#bib.bib73)) | Posterior Distribution: reduces difference
    in distributions of functionally equiv. code snippets in different PLs over code
    semantics. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| PD  Zhang 等人 ([2021](#bib.bib73)) | 后验分布：减少在不同编程语言中功能等效的代码片段在代码语义上的分布差异。
    |'
- en: '| ACP  Zhang et al. ([2021](#bib.bib73)) | Attentive Code Position: predicts
    the node type of a code token in an AST through an attention mechanism. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ACP  Zhang 等人 ([2021](#bib.bib73)) | 注意力代码位置：通过注意力机制预测 AST 中代码 token 的节点类型。'
- en: '| CNS | MCL  Wang et al. ([2021a](#bib.bib67)) | Multi-modal Contrastive Learning:
    maximizes/minimizes the representation similarity between positive/negative samples.
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| CNS | MCL  王等人 ([2021a](#bib.bib67)) | 多模态对比学习：最大化/最小化正样本/负样本之间的表示相似性。 |'
- en: 'Table 2: Categorization and description of the pre-training tasks used by existing
    CodePTMs.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：现有CodePTMs使用的预训练任务的分类和描述。
- en: First, existing CodePTMs differ in terms of the underlying network architecture.
    To understand network architectures, we need to briefly introduce the concepts
    of encoding and decoding. An encoder encodes an input sequence as a fixed-length
    vector representation, whereas a decoder generates an output sequence based on
    the representation of an input.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，现有的CodePTMs在底层网络架构上有所不同。为了理解网络架构，我们需要简要介绍编码和解码的概念。编码器将输入序列编码为固定长度的向量表示，而解码器则根据输入的表示生成输出序列。
- en: 'Rather than designing new network architectures, SE researchers base the design
    of CodePTMs on existing architectures. Broadly, these architectures can be divided
    into four categories: (1) Long Short-Term Memory (LSTM Hochreiter and Schmidhuber
    ([1997](#bib.bib21))), which is a classical recurrent neural network architecture,
    (2) Transformer (TF) Vaswani et al. ([2017](#bib.bib65)), which is a comparatively
    newer encoder-decoder architecture¹¹1Recall that an encoder-decoder architecture
    is commonly used for sequence-to-sequence tasks, where the encoder encodes an
    input sequence as a fixed-length, typically task-specific, representation, and
    the decoder then generates an output sequence token by token based on the input
    and the tokens that have been generated so far. that is faster to train and can
    better capture long-distance dependencies than LSTM; (3) Transformer-Encoder (TE),
    which corresponds to the architecture of the encoder part of TF; and (4) Transformer-Decoder
    (TD), which corresponds to the architecture of the decoder part of TF. While it
    is possible to use encoder-only models (such as TE) and decoder-only models (such
    as TD) for sequence-to-sequence (seq2seq) tasks, it has been shown to be disadvantageous
    and impractical to do so Niu et al. ([2022](#bib.bib42)). In particular, encoder-only
    models and decoder-only models are disadvantaged when applied to generation/decoding
    and classification tasks, respectively.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: SE研究人员并未设计新的网络架构，而是基于现有架构设计CodePTMs。从大体上来看，这些架构可以分为四类：（1）长短期记忆网络（LSTM Hochreiter
    and Schmidhuber ([1997](#bib.bib21)))，这是一种经典的递归神经网络架构；（2）Transformer (TF) Vaswani
    et al. ([2017](#bib.bib65))，这是一种较新的编码器-解码器架构¹¹1回忆一下，编码器-解码器架构通常用于序列到序列的任务，其中编码器将输入序列编码为固定长度的、通常特定于任务的表示，而解码器则根据输入及已生成的令牌逐个生成输出序列。比LSTM更快地训练并且能更好地捕捉长距离依赖；（3）Transformer-Encoder
    (TE)，对应于TF的编码器部分架构；（4）Transformer-Decoder (TD)，对应于TF的解码器部分架构。虽然可以使用仅编码器模型（如TE）和仅解码器模型（如TD）进行序列到序列（seq2seq）任务，但研究表明，这样做是不利且不切实际的 Niu
    et al. ([2022](#bib.bib42))。特别是，仅编码器模型和仅解码器模型在应用于生成/解码和分类任务时，分别存在劣势。
- en: 3.2 Modality
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 模态
- en: When using a neural model to process source code, being able to integrate the
    NL embedded in the code (e.g., documentations, variable names) and the code structure
    (e.g., ASTs) can improve the model’s ability to understand the code Ernst ([2017](#bib.bib15));
    Hu et al. ([2018b](#bib.bib24)); LeClair et al. ([2019](#bib.bib32)); Zügner et
    al. ([2021](#bib.bib76)). Therefore, the use of NL and code structure as inputs
    in addition to the code itself has become a common practice in CodePTMs. As Code,
    NL, and Structure differ in representation and processing, they can be viewed
    as features of different input modalities. Hence, along the second dimension,
    we divide CodePTMs into three categories — unimodal (Uni), bimodal (Bi), and multimodal
    (Multi) — based on the number of input modalities they employ.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用神经模型处理源代码时，能够将嵌入在代码中的自然语言（例如文档、变量名）与代码结构（例如AST）结合起来，可以提高模型对代码的理解能力 Ernst
    ([2017](#bib.bib15)); Hu et al. ([2018b](#bib.bib24)); LeClair et al. ([2019](#bib.bib32));
    Zügner et al. ([2021](#bib.bib76))。因此，除了代码本身，使用自然语言和代码结构作为输入已成为CodePTMs中的一种常见做法。由于代码、自然语言和结构在表示和处理上存在差异，它们可以被视为不同输入模态的特征。因此，在第二个维度上，我们将CodePTMs分为三类——单模态（Uni）、双模态（Bi）和多模态（Multi）——基于它们所采用的输入模态数量。
- en: When a model employs more than one input modality, we can either (1) concatenate
    the features extracted from different modalities to form a single training instance
    or (2) use the features extracted from different modalities to create different
    training instances. We refer to these two strategies as Together and Standalone,
    respectively. As can be imagined, an advantage of Together over Standalone is
    that the former allows cross-modal representations to be learned by a model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型使用多个输入模态时，我们可以选择（1）将从不同模态提取的特征连接起来形成一个训练实例，或者（2）使用从不同模态提取的特征来创建不同的训练实例。我们将这两种策略分别称为
    Together 和 Standalone。可以想象，Together 相对于 Standalone 的一个优势是前者允许模型学习跨模态表示。
- en: 3.3 Pre-Training Tasks
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 预训练任务
- en: 'Along the third dimension, we differentiate CodePTMs based on the tasks used
    to pre-train them. At a high level, we can divide these tasks into two categories
    depending on whether the task originates in NLP (NLP) or is specifically designed
    for source code (SE), as shown in Table [2](#S3.T2 "Table 2 ‣ 3.1 Architecture
    ‣ 3 CodePTMs ‣ Deep Learning Meets Software Engineering: A Survey on Pre-Trained
    Models of Source Code").'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '在第三维度上，我们根据用于预训练 CodePTMs 的任务进行区分。从高层次来看，我们可以根据任务是否源于 NLP（NLP）或专门为源代码设计（SE）来将这些任务分为两类，如表 [2](#S3.T2
    "Table 2 ‣ 3.1 Architecture ‣ 3 CodePTMs ‣ Deep Learning Meets Software Engineering:
    A Survey on Pre-Trained Models of Source Code")所示。'
- en: 'As can be seen from the table, the NLP pre-training tasks can be subdivided
    into four categories: (1) Language modeling (LM) Qiu et al. ([2020](#bib.bib51)),
    which refers to the collection of tasks that aim to predict a given word given
    the surrounding context; (2) Masked Language Modeling (MLM) Devlin et al. ([2018](#bib.bib13)),
    which refers to the collection of tasks that aim to predict the masked tokens;
    (3) Denoising Auto-Encoding (DAE) Lewis et al. ([2020](#bib.bib34)), which aim
    to recover the original (i.e., uncorrupted) text from corrupted text; and (4)
    Contrastive Learning (CTL) Jain et al. ([2021](#bib.bib28)), which allows a model
    to learn which data points are similar or different. The SE pre-training tasks,
    on the other hand, can be subdivided into three categories according to their
    input modalities: (1) Code-Aware (CA) tasks, which aim to mine latent information
    from code text; (2) Structure-Aware (SA) tasks, which aim to learn representations
    of the code structure; and (3) Cross-Modal-Aware (CMA) tasks, which seek to acquire
    knowledge from multiple input modalities. The CMA tasks can be further subdivided
    into three categories based on which input modalities are involved, namely Code-NL
    (CN), Code-Structure (CS) and Code-NL-Structure (CNS).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从表中可以看出，NLP 预训练任务可以细分为四类：（1）语言建模（LM）Qiu 等人（[2020](#bib.bib51)），指的是旨在根据上下文预测给定单词的任务；（2）掩码语言建模（MLM）Devlin
    等人（[2018](#bib.bib13)），指的是旨在预测被掩盖的标记的任务；（3）去噪自编码（DAE）Lewis 等人（[2020](#bib.bib34)），旨在从受损文本中恢复原始（即未损坏）文本；（4）对比学习（CTL）Jain
    等人（[2021](#bib.bib28)），使模型能够学习数据点的相似性或差异性。另一方面，SE 预训练任务可以根据其输入模态分为三类：（1）代码感知（CA）任务，旨在从代码文本中挖掘潜在信息；（2）结构感知（SA）任务，旨在学习代码结构的表示；（3）跨模态感知（CMA）任务，旨在从多个输入模态中获取知识。CMA
    任务可以根据涉及的输入模态进一步细分为三类，即 Code-NL（CN）、Code-Structure（CS）和 Code-NL-Structure（CNS）。
- en: '| Arch. | Mod. | Pre-Training Tasks | PL | CodePTM | SE Understanding Tasks
    | SE Generation Tasks |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Arch. | Mod. | Pre-Training Tasks | PL | CodePTM | SE Understanding Tasks
    | SE Generation Tasks |'
- en: '| NLP | CA | SA | CMA | WB | ET | BD | CD | CC | FD | CR | VM | CT | CS | CP
    | TL | BF | MG | AG | SU | MN | CG |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| NLP | CA | SA | CMA | WB | ET | BD | CD | CC | FD | CR | VM | CT | CS | CP
    | TL | BF | MG | AG | SU | MN | CG |'
- en: '| LSTM | Uni | ✓ |  |  |  | Mono | SCELMo |  |  | P1 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | Uni | ✓ |  |  |  | Mono | SCELMo |  |  | P1 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| Bi |  |  |  | ✓ | Multi | CodeDisen |  |  |  | C1 |  |  | C1 |  |  |  |  |
    C1 |  |  |  |  |  |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Bi |  |  |  | ✓ | Multi | CodeDisen |  |  |  | C1 |  |  | C1 |  |  |  |  |
    C1 |  |  |  |  |  |  |'
- en: '| TE | Uni | ✓ |  |  |  | Mono | CuBERT | K1 | K1 |  |  |  | P2 |  | V1 |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| TE | Uni | ✓ |  |  |  | Mono | CuBERT | K1 | K1 |  |  |  | P2 |  | V1 |  |  |  |  |  |  |  |  |  |  |'
- en: '|  |  |  | C-BERT |  |  | D1 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | C-BERT |  |  | D1 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '|  |  |  | JavaBERT |  |  |  |  |  |  |  |  | D2 |  |  |  |  |  |  |  |  |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | JavaBERT |  |  |  |  |  |  |  |  | D2 |  |  |  |  |  |  |  |  |  |'
- en: '| ✓ | ✓ |  |  | Multi | CugLM |  |  |  |  |  |  |  |  |  |  | L1 |  |  |  |  |  |  |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ |  |  | Multi | CugLM |  |  |  |  |  |  |  |  |  |  | L1 |  |  |  |  |  |  |  |'
- en: '| Bi | ✓ |  |  |  | Multi | CodeBERT |  |  |  |  |  |  |  |  |  | C2 |  |  |  |  |  |
    C2 |  |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Bi | ✓ |  |  |  | Multi | CodeBERT |  |  |  |  |  |  |  |  |  | C2 |  |  |  |  |  |
    C2 |  |  |'
- en: '| ✓ | ✓ |  |  | Mono | OSCAR |  |  |  |  | P2 |  | P2 |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ |  |  | Mono | OSCAR |  |  |  |  | P2 |  | P2 |  |  |  |  |  |  |  |  |  |  |'
- en: '| Multi | ✓ |  |  | ✓ | Multi | GraphCodeBERT |  |  |  | B1 |  |  |  |  |  |
    C2 |  | C4 | T2 |  |  |  |  |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Multi | ✓ |  |  | ✓ | Multi | GraphCodeBERT |  |  |  | B1 |  |  |  |  |  |
    C2 |  | C4 | T2 |  |  |  |  |  |'
- en: '| ✓ | ✓ | ✓ | ✓ | Multi | SynCoBERT |  |  | D1 | B1 |  |  | P2 |  | C2,C3 |
    C4 |  |  |  |  |  |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✓ | ✓ | Multi | SynCoBERT |  |  | D1 | B1 |  |  | P2 |  | C2,C3 |
    C4 |  |  |  |  |  |  |'
- en: '| TD | Uni | ✓ |  |  |  | Multi | GPT-C |  |  |  |  |  |  |  |  |  |  | S1
    |  |  |  |  |  |  |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| TD | Uni | ✓ |  |  |  | Multi | GPT-C |  |  |  |  |  |  |  |  |  |  | S1
    |  |  |  |  |  |  |  |'
- en: '| TF | Uni |  | ✓ |  |  | Multi | DOBF |  |  |  | B1 |  |  |  |  |  | C3 |  |
    T1 |  |  |  | C2 |  |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| TF | Uni |  | ✓ |  |  | Multi | DOBF |  |  |  | B1 |  |  |  |  |  | C3 |  |
    T1 |  |  |  | C2 |  |  |'
- en: '| ✓ |  |  |  | Mono | DeepDebug |  |  |  |  |  |  |  |  |  |  |  |  | T2 |  |  |  |  |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ✓ |  |  |  | Mono | DeepDebug |  |  |  |  |  |  |  |  |  |  |  |  | T2 |  |  |  |  |  |'
- en: '| Bi | ✓ |  |  |  | Mono | T5-learning |  |  |  |  |  |  |  |  |  |  |  |  |
    T2 | T3 | W1 | H1 |  |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Bi | ✓ |  |  |  | Mono | T5-learning |  |  |  |  |  |  |  |  |  |  |  |  |
    T2 | T3 | W1 | H1 |  |  |'
- en: '|  |  |  | Multi | PLBART |  |  | D1 | B1 |  |  |  |  |  |  |  | C4 |  |  |  |
    C2 |  | C5 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | Multi | PLBART |  |  | D1 | B1 |  |  |  |  |  |  |  | C4 |  |  |  |
    C2 |  | C5 |'
- en: '|  |  |  | CoTexT |  |  | D1 |  |  |  |  |  |  |  |  |  | T2 |  |  | C2 |  |
    C5 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | CoTexT |  |  | D1 |  |  |  |  |  |  |  |  |  | T2 |  |  | C2 |  |
    C5 |'
- en: '|  |  |  | ProphetNet-Code |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | C2
    |  |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | ProphetNet-Code |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | C2
    |  |  |'
- en: '| ✓ | ✓ |  | ✓ | Multi | CodeT5 |  |  | D1 | B1 |  |  |  |  |  |  |  | C4 |
    T2 |  |  | C2 |  | C5 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ |  | ✓ | Multi | CodeT5 |  |  | D1 | B1 |  |  |  |  |  |  |  | C4 |
    T2 |  |  | C2 |  | C5 |'
- en: '|  |  | ✓ | ✓ | Multi | TreeBERT |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
    H2 | A2,E1 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ✓ | ✓ | Multi | TreeBERT |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
    H2 | A2,E1 |'
- en: '| Multi | ✓ |  |  | ✓ | Multi | SPT-Code |  |  |  |  |  |  |  |  |  | C2 |
    A1 | C4 | T2 |  | C2,H3,M1 |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Multi | ✓ |  |  | ✓ | Multi | SPT-Code |  |  |  |  |  |  |  |  |  | C2 |
    A1 | C4 | T2 |  | C2,H3,M1 |  |'
- en: 'Table 3: Categorization of existing CodePTMs along four dimensions and their
    performances on downstream SE tasks. If a CodePTM is applied to a task, we list
    the ID of the benchmark dataset on which the CodePTM was evaluated (see Table [1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ Deep Learning Meets Software Engineering: A Survey
    on Pre-Trained Models of Source Code") for the ID associated with each dataset),
    boldfacing the ID if the CodePTM achieved SOTA results on the corresponding dataset.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：现有 CodePTMs 在四个维度的分类及其在下游 SE 任务上的表现。如果一个 CodePTM 应用于某个任务，我们列出该 CodePTM 被评估的基准数据集的
    ID（参见表 [1](#S1.T1 "表 1 ‣ 1 介绍 ‣ 深度学习与软件工程：源代码预训练模型的调查") 以获取与每个数据集相关的 ID），如果 CodePTM
    在相应数据集上获得了 SOTA 结果，则加粗该 ID。
- en: When more than one task is used to pre-train a CodePTM, the tasks involved can
    be learned simultaneously (i.e., each data instance supports all of the tasks
    involved²²2A data instance supports a task if the task’s loss can be computed
    based on the instance. For example, a code-only data instance (i.e., a code snippet
    without the paired docstring) supports both MLM and NSP because the losses of
    both tasks can be calculated based on the code snippet. However, it does not support
    BDG because the code-docstring alignment is needed by BDG. and the task losses
    can be jointly minimized), sequentially (i.e., the model is first trained on the
    first task for a specified number of steps and then trained on the remaining tasks
    one by one), or alternately (i.e., the tasks are randomly optimized as batches
    of the data instances corresponding to a particular task are selected at random
    during training). Hence, simultaneous pre-training holds the strictest requirements
    on the data and the tasks because it requires that for each data instance, all
    the pre-training tasks can be completed in one forward propagation such that their
    losses can be added to form the final optimization objective and jointly minimized
    during backward propagation. In other words, if it can perform simultaneous pre-training,
    it will also be possible to perform sequential/alternate pre-training but not
    vice versa. Nevertheless, the selection of a pre-training strategy in existing
    CodePTMs seems random when multiple options are available³³3For example, IT in
    CodeT5 can be pre-trained simultaneously with any of the other tasks, but it is
    still pre-trained alternatively..
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当多个任务用于预训练 CodePTM 时，涉及的任务可以同时学习（即，每个数据实例支持所有相关任务²²2数据实例支持一个任务，如果该任务的损失可以基于实例计算。例如，仅有代码的数据实例（即，没有配对文档的代码片段）支持
    MLM 和 NSP，因为可以基于代码片段计算这两个任务的损失。然而，它不支持 BDG，因为 BDG 需要代码与文档的对齐。任务的损失可以联合最小化）、顺序（即，模型首先在第一个任务上训练指定的步数，然后逐个训练其余任务）或交替（即，任务以随机方式优化，在训练过程中随机选择特定任务的数据实例批次）。因此，同时预训练对数据和任务的要求最严格，因为它要求每个数据实例能够在一次前向传播中完成所有预训练任务，以便它们的损失可以加在一起形成最终的优化目标，并在反向传播期间联合最小化。换句话说，如果它能够执行同时预训练，那么也可以执行顺序/交替预训练，但反之则不然。然而，在现有
    CodePTM 中，预训练策略的选择似乎在多种选项可用时是随机的³³3例如，CodeT5 中的 IT 可以与其他任务同时预训练，但仍然是交替预训练的。
- en: 3.4 Programming Languages
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 编程语言
- en: Along the last dimension, we categorize CodePTMs depending on whether they are
    pre-trained on one PL (Monolingual (Mono)) or multiple PLs (Multilingual (Multi)).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个维度上，我们将 CodePTM 分类为是否在一个编程语言（单语（Mono））或多个编程语言（多语（Multi））上进行预训练。
- en: '| CodePTM | Input | Objective | Dataset | Dataset Size |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| CodePTM | 输入 | 目标 | 数据集 | 数据集大小 |'
- en: '| SCELMo Karampatsis and Sutton ([2020](#bib.bib31)) | Code | BiLM | JS GitHub
    Repos | 150K Files |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| SCELMo Karampatsis 和 Sutton ([2020](#bib.bib31)) | 代码 | BiLM | JS GitHub
    代码库 | 150K 文件 |'
- en: '| CodeDisen Zhang et al. ([2021](#bib.bib73)) | Code + AST Seq | VGVAE + CLR
    + PD + ACP | CLCDSA | 26K Functions |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| CodeDisen Zhang 等人 ([2021](#bib.bib73)) | 代码 + AST 序列 | VGVAE + CLR + PD
    + ACP | CLCDSA | 26K 函数 |'
- en: '| CuBERT Kanade et al. ([2020](#bib.bib30)) | Code | BMLM + NSP | Python from
    BigQuery | 7.4M Files |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| CuBERT Kanade 等人 ([2020](#bib.bib30)) | 代码 | BMLM + NSP | Python 来自 BigQuery
    | 7.4M 文件 |'
- en: '| C-BERT Buratti et al. ([2020](#bib.bib5)) | Code | WWM | C GiHub Repos |
    5.8GB |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| C-BERT Buratti 等人 ([2020](#bib.bib5)) | 代码 | WWM | C GitHub 代码库 | 5.8GB |'
- en: '| JavaBERT de Sousa and Hasselbring ([2021](#bib.bib12)) | Code | BMLM | Java
    GitHub Repos | 3M Files |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| JavaBERT de Sousa 和 Hasselbring ([2021](#bib.bib12)) | 代码 | BMLM | Java GitHub
    代码库 | 3M 文件 |'
- en: '| CugLM Liu et al. ([2020](#bib.bib36)) | Code | IMLM + NSP + FLM | Java, TS
    GitHub Repos | 617K Files |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| CugLM Liu 等人 ([2020](#bib.bib36)) | 代码 | IMLM + NSP + FLM | Java, TS GitHub
    代码库 | 617K 文件 |'
- en: '| CodeBERT Feng et al. ([2020](#bib.bib16)) | Code + Doc | BMLM & RTD | CodeSearchNet
    | 6.5M Functions |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| CodeBERT Feng 等人 ([2020](#bib.bib16)) | 代码 + 文档 | BMLM & RTD | CodeSearchNet
    | 6.5M 函数 |'
- en: '| OSCAR Peng et al. ([2021](#bib.bib45)) | IR + AEI | BMLM + CCL | C/C++ GitHub
    Repos | 500K Functions |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| OSCAR Peng 等人 ([2021](#bib.bib45)) | IR + AEI | BMLM + CCL | C/C++ GitHub
    代码库 | 500K 函数 |'
- en: '| GraphCodeBERT Guo et al. ([2021](#bib.bib17)) | Code + Doc + DFG Nodes |
    BMLM + EP + NA | CodeSearchNet (Bimodal) | 2.3M Functions |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| GraphCodeBERT Guo 等人 ([2021](#bib.bib17)) | 代码 + 文档 + DFG 节点 | BMLM + EP
    + NA | CodeSearchNet（双模态） | 2.3M 函数 |'
- en: '| SynCoBERT Wang et al. ([2021a](#bib.bib67)) | Code + Doc + AST Seq | BMLM
    + IT + TEP + MCL | CodeSearchNet | 6.5M Functions |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| SynCoBERT Wang 等人 ([2021a](#bib.bib67)) | 代码 + 文档 + AST 序列 | BMLM + IT +
    TEP + MCL | CodeSearchNet | 6.5M 函数 |'
- en: '| GPT-C Svyatkovskiy et al. ([2020](#bib.bib60)) | Code | FLM | Python, C#,
    JS/TS GitHub Repos | 4.7M Files |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| GPT-C Svyatkovskiy 等 ([2020](#bib.bib60)) | 代码 | FLM | Python, C#, JS/TS
    GitHub 仓库 | 4.7M 文件 |'
- en: '| DOBF Roziere et al. ([2021](#bib.bib57)) | Code | SIMLM(Seq2Seq IMLM) | Java,
    Python from BigQuery | 11.5M Files |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| DOBF Roziere 等 ([2021](#bib.bib57)) | 代码 | SIMLM（Seq2Seq IMLM） | 从 BigQuery
    获取的 Java, Python | 11.5M 文件 |'
- en: '| DeepDebug Drain et al. ([2021](#bib.bib14)) | Code | SMLM(Seq2Seq MLM) |
    Java GitHub Repos | 8M Files |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| DeepDebug Drain 等 ([2021](#bib.bib14)) | 代码 | SMLM（Seq2Seq MLM） | Java GitHub
    仓库 | 8M 文件 |'
- en: '| T5-learning Mastropaolo et al. ([2021](#bib.bib38)) | Code | SMLM(Seq2Seq
    MLM) | CodeSearchNet (Java) | 1.5M Functions |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| T5-learning Mastropaolo 等 ([2021](#bib.bib38)) | 代码 | SMLM（Seq2Seq MLM） |
    CodeSearchNet（Java） | 1.5M 函数 |'
- en: '| PLBART Ahmad et al. ([2021](#bib.bib1)) | Code & Posts | DAE (masking / deletion
    / infilling) | Java, Python GitHub Repos | 680M Functions |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| PLBART Ahmad 等 ([2021](#bib.bib1)) | 代码与帖子 | DAE（掩码 / 删除 / 填充） | Java, Python
    GitHub 仓库 | 680M 函数 |'
- en: '| StackOverflow Posts | 47M Posts |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| StackOverflow 帖子 | 47M 帖子 |'
- en: '| CoTexT Phan et al. ([2021](#bib.bib47)) | Code + Doc | SMLM(Seq2Seq MLM)
    | CodeSearchNet | 6.5M Functions |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| CoTexT Phan 等 ([2021](#bib.bib47)) | 代码 + 文档 | SMLM（Seq2Seq MLM） | CodeSearchNet
    | 6.5M 函数 |'
- en: '| Java, Python from BigQuery | 6.4M Functions |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 从 BigQuery 获取的 Java, Python | 6.4M 函数 |'
- en: '| ProphetNet-Code Qi et al. ([2021](#bib.bib50)) | Code & Doc | FNP | CodeSearchNet
    (Bimodal) | 2.3M Functions |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| ProphetNet-Code Qi 等 ([2021](#bib.bib50)) | 代码与文档 | FNP | CodeSearchNet（双模态）
    | 2.3M 函数 |'
- en: '| CodeT5 Wang et al. ([2021b](#bib.bib68)) | Code + Doc | SMLM(Seq2Seq MLM)
    / IT / | CodeSearchNet | 6.5M Functions |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| CodeT5 Wang 等 ([2021b](#bib.bib68)) | 代码 + 文档 | SMLM（Seq2Seq MLM） / IT /
    | CodeSearchNet | 6.5M 函数 |'
- en: '| SIMLM(Seq2seq IMLM) / BDG | C, C# from BigQuery | 1.85M Functions |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| SIMLM（Seq2seq IMLM） / BDG | 从 BigQuery 获取的 C, C# | 1.85M 函数 |'
- en: '| TreeBERT Jiang et al. ([2021](#bib.bib29)) | Code + AST Paths | TMLM + NOP
    | Java, Python from BigQuery | 21.3M Files |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| TreeBERT Jiang 等 ([2021](#bib.bib29)) | 代码 + AST 路径 | TMLM + NOP | 从 BigQuery
    获取的 Java, Python | 21.3M 文件 |'
- en: '| SPT-Code Niu et al. ([2022](#bib.bib42)) | Code + Names + AST Seq | CAP &
    MASS & MNG | CodeSearchNet | 6.5M Functions |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| SPT-Code Niu 等 ([2022](#bib.bib42)) | 代码 + 名称 + AST 序列 | CAP & MASS & MNG
    | CodeSearchNet | 6.5M 函数 |'
- en: 'Table 4: Details of how the CodePTMs are pre-trained. The pre-training scheme
    employed for each CodePTM is characterized by (1) the input modalities (if multiple
    modalities are involved, they can be handled via a Together (+) or Standalone
    (&) strategy); (2) the pre-training objectives (if multiple pre-training objectives
    are involved, they can be learned jointly (+), sequentially (&), or alternately
    (/)); (3) the dataset on which the CodePTM is pre-trained; and (4) the size of
    the dataset.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：CodePTM 的预训练细节。每个 CodePTM 的预训练方案的特征包括 (1) 输入模态（如果涉及多个模态，可以通过 Together (+)
    或 Standalone (&) 策略处理）；(2) 预训练目标（如果涉及多个预训练目标，可以通过联合学习（+）、顺序学习（&）或交替学习（/））；(3)
    CodePTM 的预训练数据集；以及 (4) 数据集的规模。
- en: 3.5 Categorization and Pre-Training Details
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 分类与预训练细节
- en: 'The first five columns of Table [3](#S3.T3 "Table 3 ‣ 3.3 Pre-Training Tasks
    ‣ 3 CodePTMs ‣ Deep Learning Meets Software Engineering: A Survey on Pre-Trained
    Models of Source Code") categorize 20 CodePTMs along the four dimensions discussed
    in the previous subsections, namely Architecture (Arch.), Modality (Mod., Pre-Training
    Tasks, and Programming Languages (PL). We believe this categorization can help
    the reader better understand the similarities and differences between different
    CodePTMs.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '表[3](#S3.T3 "Table 3 ‣ 3.3 Pre-Training Tasks ‣ 3 CodePTMs ‣ Deep Learning
    Meets Software Engineering: A Survey on Pre-Trained Models of Source Code")的前五列对
    20 种 CodePTM 进行分类，涉及前述子章节讨论的四个维度，即架构（Arch.）、模态（Mod.）、预训练任务（Pre-Training Tasks）和编程语言（PL）。我们相信这种分类可以帮助读者更好地理解不同
    CodePTM 之间的相似性和差异性。'
- en: 'Note, however, that Table [3](#S3.T3 "Table 3 ‣ 3.3 Pre-Training Tasks ‣ 3
    CodePTMs ‣ Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models
    of Source Code") only provides a high-level categorization of the CodePTMs. For
    instance, we still do not know which two input modalities are used by a bimodal
    CodePTM, and neither do we know which PLs are used to pre-train a multilingual
    CodePTM. Table [4](#S3.T4 "Table 4 ‣ 3.4 Programming Languages ‣ 3 CodePTMs ‣
    Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source
    Code") fills this gap by providing the details of how each CodePTM is pre-trained.
    Specifically, CodePTM cites the paper that proposed each CodePTM, whereas Input,
    Objective, and Dataset show the input modalities, the pre-training tasks, and
    the PLs involved in pre-training each CodePTM. The datasets can be divided into
    four types, namely, GitHub Repos (a dataset obtained from GitHub, e.g., JS GitHub
    Repos is a dataset built by GitHub JavaScript repositories), BigQuery (a platform
    that includes activity from over 3M open source GitHub repositories, e.g., “Python
    from BigQuery” is the dataset collected by querying Python functions on BigQuery),
    CodeSearchNet Husain et al. ([2019](#bib.bib25)) (a dataset that is obtained by
    scraping open-source repositories and pairing individual functions with their
    docstrings and which includes more than 6.4M codes of 6 PLs including Java, Python,
    JavaScript, PHP, Go and Ruby), and CLCDSA Nafi et al. ([2019](#bib.bib41)) (a
    dataset collected from Online Judge (OJ) sites across four PLs (i.e., Java, Python,
    C# and C++) where functionally similar solutions written in different PLs are
    available for a given problem).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，请注意，表 [3](#S3.T3 "Table 3 ‣ 3.3 Pre-Training Tasks ‣ 3 CodePTMs ‣ Deep Learning
    Meets Software Engineering: A Survey on Pre-Trained Models of Source Code") 仅提供了
    CodePTMs 的高级分类。例如，我们仍然不知道双模态 CodePTM 使用了哪两种输入模态，也不知道多语言 CodePTM 使用了哪些 PL 进行预训练。表
    [4](#S3.T4 "Table 4 ‣ 3.4 Programming Languages ‣ 3 CodePTMs ‣ Deep Learning Meets
    Software Engineering: A Survey on Pre-Trained Models of Source Code") 填补了这一空白，提供了每个
    CodePTM 如何进行预训练的详细信息。具体而言，CodePTM 引用了提出每个 CodePTM 的论文，而 Input、Objective 和 Dataset
    显示了输入模态、预训练任务和参与预训练每个 CodePTM 的 PL。数据集可以分为四类，即 GitHub Repos（从 GitHub 获取的数据集，例如，JS
    GitHub Repos 是由 GitHub JavaScript 仓库构建的数据集）、BigQuery（包括来自超过 300 万个开源 GitHub 仓库的活动的平台，例如，“Python
    from BigQuery”是通过在 BigQuery 上查询 Python 函数收集的数据集）、CodeSearchNet Husain 等人 ([2019](#bib.bib25))（通过抓取开源仓库并将单个函数与其文档字符串配对获得的数据集，包含超过
    640 万行代码，涵盖 Java、Python、JavaScript、PHP、Go 和 Ruby 等 6 种 PL），以及 CLCDSA Nafi 等人 ([2019](#bib.bib41))（从四种
    PL（即 Java、Python、C# 和 C++）的在线评测（OJ）网站收集的数据集，其中提供了针对给定问题用不同 PL 编写的功能类似的解决方案）。'
- en: 4 Discussion
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 讨论
- en: 'Next, we explore the relationship between CodePTMs (Section 3) and SE tasks
    (Section 2). The right half of Table [3](#S3.T3 "Table 3 ‣ 3.3 Pre-Training Tasks
    ‣ 3 CodePTMs ‣ Deep Learning Meets Software Engineering: A Survey on Pre-Trained
    Models of Source Code") depicts this relationship by showing whether a CodePTM
    has been applied to a particular SE task, and if so, which benchmark dataset(s)
    it has been evaluated on and whether state-of-the-art (SOTA) results have been
    achieved. Below we discuss our key observations, which are based in part on Table [3](#S3.T3
    "Table 3 ‣ 3.3 Pre-Training Tasks ‣ 3 CodePTMs ‣ Deep Learning Meets Software
    Engineering: A Survey on Pre-Trained Models of Source Code") and in part on conclusions
    drawn from the literature.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们探讨 CodePTMs（第 3 节）与 SE 任务（第 2 节）之间的关系。表 [3](#S3.T3 "Table 3 ‣ 3.3 Pre-Training
    Tasks ‣ 3 CodePTMs ‣ Deep Learning Meets Software Engineering: A Survey on Pre-Trained
    Models of Source Code") 的右半部分展示了这种关系，通过显示 CodePTM 是否已应用于特定的 SE 任务，以及如果是的话，在哪些基准数据集上进行了评估，以及是否达到了最先进（SOTA）的结果。下面我们将讨论我们的关键观察，这些观察部分基于表
    [3](#S3.T3 "Table 3 ‣ 3.3 Pre-Training Tasks ‣ 3 CodePTMs ‣ Deep Learning Meets
    Software Engineering: A Survey on Pre-Trained Models of Source Code")，部分基于从文献中得出的结论。'
- en: Architecture.
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 架构。
- en: 'As can be seen in Table [3](#S3.T3 "Table 3 ‣ 3.3 Pre-Training Tasks ‣ 3 CodePTMs
    ‣ Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of
    Source Code"), TE-based CodePTMs are applied mostly to Understanding tasks, whereas
    TD- and TF-based CodePTMs are applied mostly to Generation tasks. This is understandable.
    As mentioned in Section 3.1, encoder-only models are disadvantaged when applied
    to Generation tasks. The reason is that they can only map an input sequence to
    an output sequence with a priori known length, but for Generation tasks the output
    length is typically not known a priori. In contrast, the presence of decoders
    in TD- and TF-based CodePTMs naturally makes them more suited to Generation tasks.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [3](#S3.T3 "Table 3 ‣ 3.3 Pre-Training Tasks ‣ 3 CodePTMs ‣ Deep Learning
    Meets Software Engineering: A Survey on Pre-Trained Models of Source Code") 所示，基于
    TE 的 CodePTMs 主要用于理解任务，而基于 TD 和 TF 的 CodePTMs 则主要用于生成任务。这是可以理解的。如第 3.1 节所述，只有编码器的模型在应用于生成任务时处于劣势。原因在于它们只能将输入序列映射到长度已知的输出序列，但对于生成任务，输出长度通常是未知的。相比之下，基于
    TD 和 TF 的 CodePTMs 中解码器的存在自然使它们更适合生成任务。'
- en: Modality.
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模态。
- en: We make two modality-related observations. First, for CodePTMs that use structured
    information as input (e.g., features extracted from DFGs, ASTs, and AEI⁴⁴4AEI
    (Abstract Environment Information) describes a program’s semantics with a mathematical
    characterization of its behaviors.), removing such information from the input
    always reduces their performances on downstream SE tasks Guo et al. ([2021](#bib.bib17));
    Zhang et al. ([2021](#bib.bib73)); Wang et al. ([2021a](#bib.bib67)); Jiang et
    al. ([2021](#bib.bib29)).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做了两个与模态相关的观察。首先，对于使用结构化信息作为输入的 CodePTMs（例如，从 DFGs、ASTs 中提取的特征，以及 AEI⁴⁴4AEI（抽象环境信息）用数学方式描述程序行为的语义），从输入中去除这些信息总是会降低它们在下游
    SE 任务中的表现 Guo et al. ([2021](#bib.bib17)); Zhang et al. ([2021](#bib.bib73));
    Wang et al. ([2021a](#bib.bib67)); Jiang et al. ([2021](#bib.bib29))。
- en: Second, the use of NL as an input modality appears to contribute positively
    to model performance on a downstream task only if NL is present in the input or
    output of the task Feng et al. ([2020](#bib.bib16)); Niu et al. ([2022](#bib.bib42)).
    Otherwise, the use of NL could lead to a performance deterioration Phan et al.
    ([2021](#bib.bib47)); Niu et al. ([2022](#bib.bib42)). For example, CodeT5, which
    is pre-trained using NL and Code, achieves SOTA results on all the NL-related
    SE tasks to which it is applied (e.g., TL, SU, and MN), but it is surpassed by
    SynCoBERT, which is pre-trained only on Code, in performance on CD, a Code-related-only
    task.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，只有当 NL 存在于任务的输入或输出中时，作为输入模态的 NL 才会对模型在下游任务上的性能产生积极贡献 Feng et al. ([2020](#bib.bib16));
    Niu et al. ([2022](#bib.bib42))。否则，使用 NL 可能会导致性能下降 Phan et al. ([2021](#bib.bib47));
    Niu et al. ([2022](#bib.bib42))。例如，CodeT5 使用 NL 和 Code 进行预训练，在所有相关的 NL SE 任务中（如
    TL、SU 和 MN）都达到了 SOTA 结果，但在 CD 这一仅与 Code 相关的任务中，其性能被仅在 Code 上预训练的 SynCoBERT 超越。
- en: Pre-training tasks.
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预训练任务。
- en: We make two pre-training tasks-related observations. First, after fine-tuning
    on task-specific training data, a pre-trained model generally yields better results
    on SE downstream tasks than its ”no pre-training” counterpart that is trained
    only on task-specific training data, and the discrepancy in their performances
    is especially obvious when the amount of task-specific training data is small Zhou
    et al. ([2021](#bib.bib75)); Kanade et al. ([2020](#bib.bib30)); Buratti et al.
    ([2020](#bib.bib5)); Roziere et al. ([2021](#bib.bib57)). This is true even when
    the underlying pre-trained model is taken from the NLP domain, such as RoBERTa,
    without pre-training it again on source code Feng et al. ([2020](#bib.bib16));
    Ahmad et al. ([2021](#bib.bib1)).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做了两个与预训练任务相关的观察。首先，在任务特定训练数据上进行微调后，预训练模型通常在 SE 下游任务上的结果优于仅在任务特定训练数据上训练的“无预训练”对照模型，并且当任务特定训练数据量较少时，这种性能差异尤为明显 Zhou
    et al. ([2021](#bib.bib75)); Kanade et al. ([2020](#bib.bib30)); Buratti et al.
    ([2020](#bib.bib5)); Roziere et al. ([2021](#bib.bib57))。即使底层的预训练模型来自 NLP 领域，例如
    RoBERTa，而未在源代码上再次预训练，这一点也是成立的 Feng et al. ([2020](#bib.bib16)); Ahmad et al. ([2021](#bib.bib1))。
- en: Second, keeping the pre-training task’s type as similar as possible to that
    of the downstream task tends to yield the best results. Theoretically, pre-training
    will be beneficial for a downstream task precisely when the knowledge learned
    during pre-training can be successfully exploited when the model learns the downstream
    task. Such knowledge transfer tends to be more effective if the pre-training task
    is closer to the downstream task. For instance, for Understanding tasks, it is
    better to use a pre-training task that is also an Understanding task, such as
    MLM. Note that MLM is used by all the models that achieve SOTA results on Understanding
    tasks, such as CuBERT. In contrast, (I)MASS, which focuses on Generation, tends
    to work much better as a pre-training task than (I) MLM, which focuses on Understanding,
    on seq2seq downstream tasks such as code summarization Jiang et al. ([2021](#bib.bib29)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，将预训练任务的类型保持尽可能接近下游任务的类型往往能取得最佳效果。理论上，预训练对于下游任务的好处在于预训练过程中学到的知识能够在模型学习下游任务时被成功利用。如果预训练任务与下游任务更接近，这种知识迁移通常会更加有效。例如，对于理解任务，使用一个也是理解任务的预训练任务，如MLM，会更好。注意，所有在理解任务上达到SOTA结果的模型，如CuBERT，都使用了MLM。相比之下，专注于生成任务的(I)MASS，作为预训练任务比专注于理解的(I)MLM在seq2seq下游任务（如代码摘要）上效果要好得多（Jiang
    et al. ([2021](#bib.bib29))）。
- en: Programming languages.
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 编程语言。
- en: We make two PL-related observations. First, knowledge transfer tends to be a
    lot more effective if a CodePTM is trained on a PL that is syntactically similar
    to the one used in the downstream task. In contrast, knowledge learned by a CodePTM
    from PLs that are syntactically different from the one used in the downstream
    task may even lead to the performance degradation. For instance, PLBART, which
    is pre-trained on Java and Python, performs better on C# code translation but
    worse on PHP code summarization than RoBERTa, a PTM that is trained on NL text
    only. The reason is that C# is syntactically similar to Java, while PHP has a
    syntax mismatch with Java and Python.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做出了两个与编程语言相关的观察。首先，如果CodePTM在语法上与下游任务使用的编程语言相似的语言上进行训练，那么知识迁移往往会更加有效。相比之下，CodePTM从与下游任务使用的编程语言在语法上不同的语言中学到的知识甚至可能导致性能下降。例如，预训练于Java和Python的PLBART在C#代码翻译上表现更好，但在PHP代码摘要上表现比仅在NL文本上训练的RoBERTa更差。原因是C#在语法上与Java相似，而PHP与Java和Python的语法不匹配。
- en: Second, multilingual pre-training and fine-tuning generally yield better results
    than their monolingual counterparts. For example, CodeT5, which is pre-trained
    on 6 PLs, outperforms T5-learning and DeepDebug, both of which are only pre-trained
    on Java, on code translation. In addition, when performing multilingual pre-training,
    language-aware pre-training, where the training instances that belong to different
    PLs are being differentiated (by adding language-specific symbols to the input
    or appending a language-type embedding to each token, for instance), tend to yield
    a pre-trained model that can better discriminate between PLs than language-agnostic
    pre-training, as demonstrated via GPT-C on code completion.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，多语言预训练和微调通常比单语言预训练和微调效果更佳。例如，预训练于6种编程语言的CodeT5在代码翻译上优于仅预训练于Java的T5-learning和DeepDebug。此外，当进行多语言预训练时，语言感知的预训练（例如，通过向输入中添加特定语言的符号或将语言类型嵌入附加到每个标记）往往能够生成一个能更好区分编程语言的预训练模型，相比于语言无关的预训练，这一点在GPT-C的代码补全任务中得到了验证。
- en: 5 How Effective are CodePTMs?
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 CodePTMs的有效性如何？
- en: 'CodePTMs have been successfully applied to a variety of SE tasks, but how effective
    are they? To enable the reader to gain insights into this question, we present
    some quantitative results in this section. More specifically, we show in Table [5](#S5.T5
    "Table 5 ‣ 5 How Effective are CodePTMs? ‣ Deep Learning Meets Software Engineering:
    A Survey on Pre-Trained Models of Source Code") the best result achieved by a
    CodePTM on each commonly used evaluation dataset for each SE task (see the ”Best
    CodePTM” column). To help the reader gauge the effectiveness of CodePTMs, we show
    in the ”Best non-CodePTM” column the best result achieved by an approach that
    does not involve pre-training on each dataset. As can be seen, many of the best
    non-CodePTM-based approaches are neural models that involve Tree-LSTM and Transformer,
    for instance. The last column of the table shows for each dataset the relative
    error reduction rate, which is computed as the error reduced by the best-performing
    CodePTM relative to the error made by the best non-CodePTM-based system on the
    dataset. A positive value indicates that the SOTA result is achieved using a CodePTM.
    As can be seen, the SOTA results on all of the datasets are achieved using CodePTMs,
    with the relative error reduction rates ranging from 0.9–78.7 when expressed in
    percentages. These results provide suggestive evidence that CodePTMs are a promising
    approach to a wide variety of SE tasks. Nevertheless, it is clear that CodePTMs
    are more effective at relative error reduction on certain SE tasks/datasets than
    other tasks/datasets. Additional analysis is needed to determine the reason.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 'CodePTM 已成功应用于各种 SE 任务，但它们的有效性如何？为了让读者对这个问题有深入了解，我们在本节中展示了一些定量结果。具体而言，我们在表格
    [5](#S5.T5 "Table 5 ‣ 5 How Effective are CodePTMs? ‣ Deep Learning Meets Software
    Engineering: A Survey on Pre-Trained Models of Source Code") 中展示了每个 SE 任务中 CodePTM
    在每个常用评估数据集上的最佳结果（见“最佳 CodePTM”列）。为了帮助读者评估 CodePTM 的有效性，我们在“最佳非 CodePTM”列中展示了不涉及预训练的每个数据集上方法的最佳结果。如所示，许多最佳的非
    CodePTM 基于方法是涉及 Tree-LSTM 和 Transformer 的神经模型。例如，表格的最后一列显示了每个数据集的相对误差减少率，该率是通过最佳表现的
    CodePTM 相对于最佳非 CodePTM 系统在数据集上的误差减少计算得出的。正值表示使用 CodePTM 达到了 SOTA 结果。如所示，所有数据集上的
    SOTA 结果都是使用 CodePTM 实现的，相对误差减少率在 0.9–78.7% 之间。这些结果提供了 CodePTM 是各种 SE 任务有前景的方法的暗示性证据。然而，显然
    CodePTM 在某些 SE 任务/数据集上的相对误差减少效果优于其他任务/数据集。需要进一步分析以确定原因。'
- en: '| Task | DS | Best CodePTM | Best non-CodePTM | ER |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Task | DS | 最佳 CodePTM | 最佳非 CodePTM | ER |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| WB | K1 | 82.3 (CuBERT Kanade et al. ([2020](#bib.bib30))) | 73.8 (GREAT Hellendoorn
    et al. ([2020](#bib.bib20))) | 32.4 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| WB | K1 | 82.3 (CuBERT Kanade et al. ([2020](#bib.bib30))) | 73.8 (GREAT Hellendoorn
    et al. ([2020](#bib.bib20))) | 32.4 |'
- en: '| ET | K1 | 79.1 (CuBERT Kanade et al. ([2020](#bib.bib30))) | 49.5 (Transformer Kanade
    et al. ([2020](#bib.bib30))) | 58.6 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| ET | K1 | 79.1 (CuBERT Kanade et al. ([2020](#bib.bib30))) | 49.5 (Transformer Kanade
    et al. ([2020](#bib.bib30))) | 58.6 |'
- en: '| BD | D1 | 65.7 (CodeT5 Wang et al. ([2021b](#bib.bib68))) | 62.4 (code2vec Coimbra
    et al. ([2021](#bib.bib9))) | 8.8 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| BD | D1 | 65.7 (CodeT5 Wang et al. ([2021b](#bib.bib68))) | 62.4 (code2vec Coimbra
    et al. ([2021](#bib.bib9))) | 8.8 |'
- en: '| CD | B1 | 97.4 (SynCoBERT Wang et al. ([2021a](#bib.bib67))) | 95.0 (FA-AST Wang
    et al. ([2020](#bib.bib66))) | 48.0 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| CD | B1 | 97.4 (SynCoBERT Wang et al. ([2021a](#bib.bib67))) | 95.0 (FA-AST Wang
    et al. ([2020](#bib.bib66))) | 48.0 |'
- en: '| C1 | 90.0 (CodeDisen Zhang et al. ([2021](#bib.bib73))) | 81.0 (Tree-LSTM Shido
    et al. ([2019](#bib.bib58)) ) | 47.3 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| C1 | 90.0 (CodeDisen Zhang et al. ([2021](#bib.bib73))) | 81.0 (Tree-LSTM Shido
    et al. ([2019](#bib.bib58)) ) | 47.3 |'
- en: '| CC | P2 | 98.0 (OSCAR Peng et al. ([2021](#bib.bib45))) | 96.6 (ProGraML Peng
    et al. ([2021](#bib.bib45))) | 43.2 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| CC | P2 | 98.0 (OSCAR Peng et al. ([2021](#bib.bib45))) | 96.6 (ProGraML Peng
    et al. ([2021](#bib.bib45))) | 43.2 |'
- en: '| FD | K1 | 98.0 (CuBERT Kanade et al. ([2020](#bib.bib30))) | 91.0 (Transformer Kanade
    et al. ([2020](#bib.bib30))) | 78.7 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| FD | K1 | 98.0 (CuBERT Kanade et al. ([2020](#bib.bib30))) | 91.0 (Transformer Kanade
    et al. ([2020](#bib.bib30))) | 78.7 |'
- en: '| CR | C1 | 31.6 (CodeDisen Zhang et al. ([2021](#bib.bib73))) | 16.6 (Pontes
    et al. Pontes et al. ([2018](#bib.bib48)) ) | 17.9 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| CR | C1 | 31.6 (CodeDisen Zhang et al. ([2021](#bib.bib73))) | 16.6 (Pontes
    et al. Pontes et al. ([2018](#bib.bib48)) ) | 17.9 |'
- en: '| P2 | 88.2 (SynCoBERT Wang et al. ([2021a](#bib.bib67))) | 82.4 (MISIM Ye
    et al. ([2020](#bib.bib72)) ) | 32.9 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| P2 | 88.2 (SynCoBERT Wang et al. ([2021a](#bib.bib67))) | 82.4 (MISIM Ye
    et al. ([2020](#bib.bib72)) ) | 32.9 |'
- en: '| VM | V1 | 95.2 (CuBERT Kanade et al. ([2020](#bib.bib30))) | 80.5 (BiLSTM Kanade
    et al. ([2020](#bib.bib30))) | 75.4 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| VM | V1 | 95.2 (CuBERT Kanade et al. ([2020](#bib.bib30))) | 80.5 (BiLSTM Kanade
    et al. ([2020](#bib.bib30))) | 75.4 |'
- en: '| CT | D2 | 94.4 (JavaBERT de Sousa and Hasselbring ([2021](#bib.bib12))) |
    $-$ |  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| CT | D2 | 94.4 (JavaBERT de Sousa and Hasselbring ([2021](#bib.bib12))) |
    $-$ |  |'
- en: '| CS | C2 | 74.0 (SynCoBERT Wang et al. ([2021a](#bib.bib67))) | 41.9 (Transformer Guo
    et al. ([2021](#bib.bib17))) | 55.2 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| CS | C2 | 74.0 (SynCoBERT Wang et al. ([2021a](#bib.bib67))) | 41.9 (Transformer Guo
    et al. ([2021](#bib.bib17))) | 55.2 |'
- en: '| C3 | 38.1 (SynCoBERT Wang et al. ([2021a](#bib.bib67))) | $-$ |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| C3 | 38.1 (SynCoBERT Wang et al. ([2021a](#bib.bib67))) | $-$ |  |'
- en: '| CP | S1 | 82.8 (GPT-C Svyatkovskiy et al. ([2020](#bib.bib60))) | $-$ |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| CP | S1 | 82.8 (GPT-C Svyatkovskiy et al. ([2020](#bib.bib60))) | $-$ |  |'
- en: '| L1 | 81.9 (CugLM Liu et al. ([2020](#bib.bib36))) | 71.7 (Transformer-XL Dai
    et al. ([2019](#bib.bib11))) | 36.0 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| L1 | 81.9 (CugLM Liu et al. ([2020](#bib.bib36))) | 71.7 (Transformer-XL Dai
    et al. ([2019](#bib.bib11))) | 36.0 |'
- en: '| A1 | 26.5 (SPT-Code Niu et al. ([2022](#bib.bib42))) | 24.7 (SLM Alon et
    al. ([2020](#bib.bib3))) | 2.4 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| A1 | 26.5 (SPT-Code Niu et al. ([2022](#bib.bib42))) | 24.7 (SLM Alon et
    al. ([2020](#bib.bib3))) | 2.4 |'
- en: '| TL | C4 | 66.4 (CodeT5 Wang et al. ([2021b](#bib.bib68))) | 35.4 (Transformer Ahmad
    et al. ([2021](#bib.bib1))) | 47.9 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| TL | C4 | 66.4 (CodeT5 Wang et al. ([2021b](#bib.bib68))) | 35.4 (Transformer Ahmad
    et al. ([2021](#bib.bib1))) | 47.9 |'
- en: '| T1 | 41.8 (DOBF Roziere et al. ([2021](#bib.bib57))) | 34.7 (Transformer Roziere
    et al. ([2021](#bib.bib57))) | 10.9 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| T1 | 41.8 (DOBF Roziere et al. ([2021](#bib.bib57))) | 34.7 (Transformer Roziere
    et al. ([2021](#bib.bib57))) | 10.9 |'
- en: '| C1 | 29.7 (CodeDisen Zhang et al. ([2021](#bib.bib73))) | 25.8 (Tree-LSTM Shido
    et al. ([2019](#bib.bib58))) | 5.3 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| C1 | 29.7 (CodeDisen Zhang et al. ([2021](#bib.bib73))) | 25.8 (Tree-LSTM Shido
    et al. ([2019](#bib.bib58))) | 5.3 |'
- en: '| BF | T2 | 18.3 (CodeT5 Wang et al. ([2021b](#bib.bib68))) | 12.7 (S2S+COPY Panthaplackel
    et al. ([2021](#bib.bib43))) | 6.5 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| BF | T2 | 18.3 (CodeT5 Wang et al. ([2021b](#bib.bib68))) | 12.7 (S2S+COPY Panthaplackel
    et al. ([2021](#bib.bib43))) | 6.5 |'
- en: '| MG | T3 | 28.0 (T5-learning Mastropaolo et al. ([2021](#bib.bib38))) | 17.0
    (Tufano Tufano et al. ([2019a](#bib.bib62))) | 13.3 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| MG | T3 | 28.0 (T5-learning Mastropaolo et al. ([2021](#bib.bib38))) | 17.0
    (Tufano Tufano et al. ([2019a](#bib.bib62))) | 13.3 |'
- en: '| AG | W1 | 66.0 (T5-learning Mastropaolo et al. ([2021](#bib.bib38))) | 65.0
    (Watson et al. Watson et al. ([2020](#bib.bib69))) | 2.9 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| AG | W1 | 66.0 (T5-learning Mastropaolo et al. ([2021](#bib.bib38))) | 65.0
    (Watson et al. Watson et al. ([2020](#bib.bib69))) | 2.9 |'
- en: '| SU | C2 | 19.7 (CodeT5 Wang et al. ([2021b](#bib.bib68))) | 15.5 (Transformer Kanade
    et al. ([2020](#bib.bib30))) | 4.9 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| SU | C2 | 19.7 (CodeT5 Wang et al. ([2021b](#bib.bib68))) | 15.5 (Transformer Kanade
    et al. ([2020](#bib.bib30))) | 4.9 |'
- en: '| H1 | 21.0 (T5-learning Mastropaolo et al. ([2021](#bib.bib38))) | 19.0 (Haque
    et al. Haque et al. ([2020](#bib.bib18))) | 2.5 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| H1 | 21.0 (T5-learning Mastropaolo et al. ([2021](#bib.bib38))) | 19.0 (Haque
    et al. Haque et al. ([2020](#bib.bib18))) | 2.5 |'
- en: '| H2 | 20.4 (TreeBERT Jiang et al. ([2021](#bib.bib29))) | 19.7 (GNN+GRU LeClair
    et al. ([2020](#bib.bib33))) | 0.9 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| H2 | 20.4 (TreeBERT Jiang et al. ([2021](#bib.bib29))) | 19.7 (GNN+GRU LeClair
    et al. ([2020](#bib.bib33))) | 0.9 |'
- en: '| H3 | 49.1 (SPT-Code Niu et al. ([2022](#bib.bib42))) | 48.2 (AST-Trans Tang
    et al. ([2022](#bib.bib61))) | 1.6 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| H3 | 49.1 (SPT-Code Niu et al. ([2022](#bib.bib42))) | 48.2 (AST-Trans Tang
    et al. ([2022](#bib.bib61))) | 1.6 |'
- en: '| M1 | 36.1 (SPT-Code Niu et al. ([2022](#bib.bib42))) | 34.7 (AST-Trans Tang
    et al. ([2022](#bib.bib61))) | 2.1 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| M1 | 36.1 (SPT-Code Niu et al. ([2022](#bib.bib42))) | 34.7 (AST-Trans Tang
    et al. ([2022](#bib.bib61))) | 2.1 |'
- en: '| MN | A2 | 60.1 (TreeBERT Jiang et al. ([2021](#bib.bib29))) | 57.5 (GNN+GRU LeClair
    et al. ([2020](#bib.bib33))) | 6.1 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| MN | A2 | 60.1 (TreeBERT Jiang et al. ([2021](#bib.bib29))) | 57.5 (GNN+GRU LeClair
    et al. ([2020](#bib.bib33))) | 6.1 |'
- en: '| E1 | 39.0 (TreeBERT Jiang et al. ([2021](#bib.bib29))) | 34.4 (GNN+GRU LeClair
    et al. ([2020](#bib.bib33))) | 7.0 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| E1 | 39.0 (TreeBERT Jiang et al. ([2021](#bib.bib29))) | 34.4 (GNN+GRU LeClair
    et al. ([2020](#bib.bib33))) | 7.0 |'
- en: '| CG | C5 | 22.3 (CodeT5 Wang et al. ([2021b](#bib.bib68))) | 12.2 (Iyer et
    al. Iyer et al. ([2019](#bib.bib27))) | 11.5 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| CG | C5 | 22.3 (CodeT5 Wang et al. ([2021b](#bib.bib68))) | 12.2 (Iyer et
    al. Iyer et al. ([2019](#bib.bib27))) | 11.5 |'
- en: 'Table 5: Relative error reduction rates achieved by CodePTMs on SE tasks/datasets.
    ”DS” shows the commonly used evaluation datasets for each SE task (see Table [1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ Deep Learning Meets Software Engineering: A Survey
    on Pre-Trained Models of Source Code") for details on these datasets). ”Best CodePTM”
    shows the best result achieved to date by a CodePTM on the corresponding dataset
    and the name of the CodePTM. ”Best non-CodePTM” shows the best result achieved
    to date by an approach that does not involve pre-training on the corresponding
    dataset and the name of the approach (note that “$-$” indicates that non-CodePTM-based
    approaches have not been applied to the corresponding dataset). ”ER” shows the
    relative error reduction rate for each dataset. Information on the evaluation
    metric used for each dataset can be found in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of
    Source Code").'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '表5：CodePTMs在SE任务/数据集上的相对误差减少率。 ”DS”展示了每个SE任务的常用评估数据集（见表[1](#S1.T1 "Table 1
    ‣ 1 Introduction ‣ Deep Learning Meets Software Engineering: A Survey on Pre-Trained
    Models of Source Code")了解这些数据集的详细信息）。 ”最佳CodePTM”展示了CodePTM在相应数据集上迄今为止取得的最佳结果及其名称。
    ”最佳非CodePTM”展示了在相应数据集上迄今为止取得的最佳结果及其名称（注意“$-$”表示非CodePTM方法尚未应用于相应的数据集）。 ”ER”展示了每个数据集的相对误差减少率。每个数据集使用的评估指标的信息可以在表[1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ Deep Learning Meets Software Engineering: A Survey
    on Pre-Trained Models of Source Code")中找到。'
- en: 6 Concluding Remarks
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论性评述
- en: Though CodePTMs have proven their success in SE, we believe that they have not
    reached their full potential. In this section, we outline some promising future
    directions.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CodePTMs在SE领域已经证明了其成功，我们认为它们还未发挥出全部潜力。在本节中，我们概述了一些有前景的未来方向。
- en: 6.1 Thinking beyond NLP
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 超越NLP的思考
- en: Tokenization and embedding.
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分词和嵌入。
- en: 'Currently, CodePTMs use the tokenization and embedding methods developed in
    NLP. For example, they use SentencePiece as the tokenizer as well as token and
    position embeddings. However, code is not exactly the same as NL: code contains
    different types of lexical tokens such as variables, control symbols, and keywords.
    We speculate that NLP tokenization and embedding methods would not yield optimal
    performances for CodePTMs, and recommend that researchers look into the possibility
    of developing code-specific versions of these methods.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，CodePTMs使用在NLP中开发的分词和嵌入方法。例如，它们使用SentencePiece作为分词器，以及标记和位置嵌入。然而，代码并不完全等同于自然语言：代码包含不同类型的词汇标记，如变量、控制符号和关键字。我们推测，NLP的分词和嵌入方法可能不会为CodePTMs提供最佳性能，并建议研究人员探索开发这些方法的代码特定版本的可能性。
- en: Pre-training methods.
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预训练方法。
- en: 'Pre-training tasks that can better exploit code-specific characteristics (e.g.,
    code structure, the presence of branches, and the use of different identifiers
    taken from a largely unrestricted vocabulary to express the same meaning) may
    be needed in order to train more powerful CodePTMs. Most of the existing SE-specific
    pre-training tasks (see Section [3.3](#S3.SS3 "3.3 Pre-Training Tasks ‣ 3 CodePTMs
    ‣ Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of
    Source Code")) still do not completely step outside the NLP mindset. IMLM, for
    example, is just a version of MLM that masks identifiers, and in fact, pre-training
    on IMLM has even yielded worse results than pre-training on MLM for DOBF Roziere
    et al. ([2021](#bib.bib57)). We believe that the design of code-specific pre-training
    methods is currently limited in part by the NLP tokenization and embedding methods
    that are currently in use, and that a fundamental overhaul in the design of code-specific
    pre-training methods that involves designing code-specific tokenization and embedding
    methods will likely be needed.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '需要进行更好的利用代码特定特性的预训练任务（例如，代码结构、分支的存在以及使用从广泛不受限制的词汇中提取的不同标识符来表达相同含义）以训练更强大的CodePTMs。目前大多数现有的SE特定预训练任务（见第[3.3节](#S3.SS3
    "3.3 Pre-Training Tasks ‣ 3 CodePTMs ‣ Deep Learning Meets Software Engineering:
    A Survey on Pre-Trained Models of Source Code")）仍然未完全跳出NLP思维方式。例如，IMLM只是对MLM的一个版本，它掩盖了标识符，实际上，IMLM上的预训练甚至比MLM上的预训练在DOBF
    Roziere等人的研究中（[2021](#bib.bib57)）效果更差。我们认为，代码特定预训练方法的设计目前在一定程度上受到现有NLP分词和嵌入方法的限制，可能需要对代码特定预训练方法的设计进行根本性的改革，这包括设计代码特定的分词和嵌入方法。'
- en: 6.2 Learning Code Form and Functionality
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 学习代码形式和功能
- en: 'Code has both form, which is defined by combinations of particular code identifiers,
    and function, which is independent of any particular code identifiers Jain et
    al. ([2021](#bib.bib28)). Note that the CodePTMs listed in Table [4](#S3.T4 "Table
    4 ‣ 3.4 Programming Languages ‣ 3 CodePTMs ‣ Deep Learning Meets Software Engineering:
    A Survey on Pre-Trained Models of Source Code") all learn representations of source
    code from the “form” instead of the “function” perspective. Learning code functionality,
    however, will undoubtedly help CodePTMs understand the code better and achieve
    higher performances on SE tasks. So we believe that designing CodePTMs that can
    learn both code form and code functionality would be a valuable research direction.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '代码既有形式，这由特定代码标识符的组合定义，也有功能，这与任何特定的代码标识符无关 贾因等人 ([2021](#bib.bib28))。请注意，表 [4](#S3.T4
    "Table 4 ‣ 3.4 Programming Languages ‣ 3 CodePTMs ‣ Deep Learning Meets Software
    Engineering: A Survey on Pre-Trained Models of Source Code") 中列出的CodePTMs都是从“形式”而非“功能”角度学习源代码的。然而，学习代码功能无疑将帮助CodePTMs更好地理解代码，并在软件工程（SE）任务中取得更高的性能。因此，我们认为设计能够同时学习代码形式和代码功能的CodePTMs将是一个有价值的研究方向。'
- en: 6.3 Adaptation to Downstream Tasks
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 对下游任务的适应
- en: Currently, fine-tuning is the primary method for transferring the knowledge
    acquired during pre-training to downstream tasks. However, fine-tuning can be
    inefficient because all model parameters need to be updated. To mitigate this
    problem, the NLP community has proposed several solutions, such as (1) model reprogramming
    (i.e., freezing the original parameters of the PTMs and adding small fine-tunable
    adaption modules for specific tasks Chen ([2022](#bib.bib7))), (2) using prompt
    tuning Brown et al. ([2020](#bib.bib4)), and (3) using model compression (e.g.,
    pruning and knowledge distillation). How to adapt or extend these methods for
    fine-tuning CodePTMs is a promising research direction.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，微调是将预训练过程中获得的知识转移到下游任务的主要方法。然而，微调可能效率低下，因为所有模型参数都需要更新。为了缓解这个问题，自然语言处理（NLP）社区提出了几种解决方案，例如（1）模型重新编程（即，冻结PTMs的原始参数，并为特定任务添加小的可微调适配模块 陈
    ([2022](#bib.bib7)))，（2）使用提示微调 布朗等人 ([2020](#bib.bib4))，以及（3）使用模型压缩（如，剪枝和知识蒸馏）。如何调整或扩展这些方法以微调CodePTMs是一个有前景的研究方向。
- en: 6.4 CodePTMs for Niche Applications
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 适用于特定应用的代码预训练模型（CodePTMs）
- en: 'Rather than attempting to design a single CodePTM that works well on all SE
    tasks, we recommend that specialized CodePTMs be designed for different classes
    of SE tasks (e.g., Understanding vs. Generation). Our recommendation is based
    on our earlier observations that different model design choices may be better
    suited for different kinds of tasks. For instance, theoretically speaking, TE-based
    models tend to work better than TD- and TF-based models on Understanding tasks,
    whereas the reverse is generally true for Generation tasks. One may even go as
    far as designing task-specific CodePTMs. The reason is that having pre-training
    tasks that are more similar to the downstream task at hand could enable a more
    effective transfer of the knowledge acquired during pre-training, as discussed
    previously. We believe that specialized CodePTMs have an additional advantage:
    they tend to be smaller and hence may be more efficient, potentially allowing
    us to address the efficiency issues associated with model architecture (Section [6.1](#S6.SS1
    "6.1 Thinking beyond NLP ‣ 6 Concluding Remarks ‣ Deep Learning Meets Software
    Engineering: A Survey on Pre-Trained Models of Source Code")) and fine-tuning
    (Section [6.3](#S6.SS3 "6.3 Adaptation to Downstream Tasks ‣ 6 Concluding Remarks
    ‣ Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of
    Source Code")).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '与其尝试设计一个在所有SE任务中表现良好的单一CodePTM，我们建议为不同类别的SE任务（例如，理解与生成）设计专门的CodePTMs。我们的建议基于我们早期的观察，即不同的模型设计选择可能更适合不同类型的任务。例如，从理论上讲，基于TE的模型在理解任务中往往比基于TD和TF的模型表现更好，而在生成任务中则通常相反。甚至可以设计任务特定的CodePTMs。原因在于，具有与下游任务更相似的预训练任务可以更有效地转移预训练过程中获得的知识，如前所述。我们相信，专门化的CodePTMs还有一个额外的优势：它们通常更小，因此可能更高效，从而有助于解决与模型架构（第[6.1](#S6.SS1
    "6.1 Thinking beyond NLP ‣ 6 Concluding Remarks ‣ Deep Learning Meets Software
    Engineering: A Survey on Pre-Trained Models of Source Code")节）和微调（第[6.3](#S6.SS3
    "6.3 Adaptation to Downstream Tasks ‣ 6 Concluding Remarks ‣ Deep Learning Meets
    Software Engineering: A Survey on Pre-Trained Models of Source Code")节）相关的效率问题。'
- en: 6.5 Unified Evaluation and Analysis
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 统一评估与分析
- en: Our understanding of the strengths and weaknesses of existing CodePTMs is currently
    limited by the tasks on which they are evaluated. To better understand CodePTMs,
    it is important to conduct a systematic evaluation of all CodePTMs on all the
    benchmark datasets associated with the 18 SE tasks we discussed. In addition to
    a comprehensive quantitative evaluation, a qualitative analysis that involves
    analyzing the common errors made by each model would be important.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对现有 CodePTMs 的优缺点的理解目前受到它们被评估的任务的限制。为了更好地理解 CodePTMs，重要的是对所有 CodePTMs 在我们讨论的
    18 个 SE 任务相关的所有基准数据集上进行系统评估。除了全面的定量评估外，还需要进行定性分析，以分析每个模型常见的错误。
- en: Acknowledgments
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank the three anonymous reviewers for their helpful comments on an earlier
    draft of this paper. This work was supported in part by the National Natural Science
    Foundation of China (No. 61802167) and the US National Science Foundation (Grant
    IIS-1528037). Any opinions, findings, conclusions or recommendations expressed
    in this paper are those of the authors and do not necessarily reflect the views
    or official policies, either expressed or implied, of the funding agencies. Chuanyi
    Li is the corresponding author.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢三位匿名审稿人对早期稿件提供的宝贵意见。此项工作部分得到了中国国家自然科学基金（No. 61802167）和美国国家科学基金会（Grant IIS-1528037）的支持。本文中表达的任何观点、发现、结论或建议均为作者个人观点，并不一定反映资助机构的官方意见或政策。李传意为通讯作者。
- en: References
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ahmad et al. [2021] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei
    Chang. Unified pre-training for program understanding and generation. In NAACL-HLT,
    2021.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmad 等 [2021] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, 和 Kai-Wei Chang.
    统一预训练用于程序理解和生成。发表于 NAACL-HLT, 2021。
- en: Allamanis et al. [2016] Miltiadis Allamanis, Hao Peng, and Charles Sutton. A
    convolutional attention network for extreme summarization of source code. In ICML,
    2016.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis 等 [2016] Miltiadis Allamanis, Hao Peng, 和 Charles Sutton. 用于源代码极端摘要的卷积注意力网络。发表于
    ICML, 2016。
- en: Alon et al. [2020] Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. Structural
    language models of code. In ICML, 2020.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alon 等 [2020] Uri Alon, Roy Sadaka, Omer Levy, 和 Eran Yahav. 代码的结构性语言模型。发表于
    ICML, 2020。
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In NeurIPS, 2020.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei. 语言模型是少量样本学习者。发表于 NeurIPS, 2020。
- en: Buratti et al. [2020] Luca Buratti, Saurabh Pujar, Mihaela Bornea, Scott McCarley,
    Yunhui Zheng, Gaetano Rossiello, Alessandro Morari, Jim Laredo, Veronika Thost,
    Yufan Zhuang, et al. Exploring software naturalness through neural language models.
    arXiv:2006.12641, 2020.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buratti 等 [2020] Luca Buratti, Saurabh Pujar, Mihaela Bornea, Scott McCarley,
    Yunhui Zheng, Gaetano Rossiello, Alessandro Morari, Jim Laredo, Veronika Thost,
    Yufan Zhuang 等。通过神经语言模型探索软件自然性。arXiv:2006.12641, 2020。
- en: Chen et al. [2018] Xinyun Chen, Chang Liu, and Dawn Song. Tree-to-tree neural
    networks for program translation. In NeurIPS, 2018.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2018] Xinyun Chen, Chang Liu, 和 Dawn Song. 树到树的神经网络用于程序翻译。发表于 NeurIPS,
    2018。
- en: 'Chen [2022] Pin-Yu Chen. Model reprogramming: Resource-efficient cross-domain
    machine learning. arXiv preprint arXiv:2202.10629, 2022.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen [2022] Pin-Yu Chen. 模型重编程：资源高效的跨域机器学习。arXiv 预印本 arXiv:2202.10629, 2022。
- en: 'Clark et al. [2019] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D
    Manning. Electra: Pre-training text encoders as discriminators rather than generators.
    In ICLR, 2019.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等 [2019] Kevin Clark, Minh-Thang Luong, Quoc V Le, 和 Christopher D Manning.
    Electra：将文本编码器作为判别器而非生成器进行预训练。发表于 ICLR, 2019。
- en: Coimbra et al. [2021] David Coimbra, Sofia Reis, Rui Abreu, Corina Păsăreanu,
    and Hakan Erdogmus. On using distributed representations of source code for the
    detection of c security vulnerabilities. CoRR, 2021.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coimbra 等 [2021] David Coimbra, Sofia Reis, Rui Abreu, Corina Păsăreanu, 和 Hakan
    Erdogmus. 使用源代码的分布式表示来检测 C 语言安全漏洞。CoRR, 2021。
- en: Dai and Le [2015] Andrew M. Dai and Q. V. Le. Semi-supervised sequence learning.
    In NIPS, 2015.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 和 Le [2015]  Andrew M. Dai 和 Q. V. Le. 半监督序列学习。 在NIPS, 2015.
- en: 'Dai et al. [2019] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V
    Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond
    a fixed-length context. In ACL, 2019.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等 [2019]  Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V
    Le, 和 Ruslan Salakhutdinov. Transformer-xl: 超越固定长度上下文的关注语言模型。 在 ACL, 2019.'
- en: 'de Sousa and Hasselbring [2021] Nelson Tavares de Sousa and Wilhelm Hasselbring.
    Javabert: Training a transformer-based model for the java programming language.
    arXiv:2110.10404, 2021.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'de Sousa 和 Hasselbring [2021]  Nelson Tavares de Sousa 和 Wilhelm Hasselbring.
    Javabert: 为Java编程语言训练的基于变压器的模型。 arXiv:2110.10404, 2021.'
- en: 'Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. arXiv:1810.04805, 2018.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等 [2018]  Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    Bert: 深度双向转换器的语言理解预训练。 arXiv:1810.04805, 2018.'
- en: Drain et al. [2021] Dawn Drain, Chen Wu, Alexey Svyatkovskiy, and Neel Sundaresan.
    Generating bug-fixes using pretrained transformers. In Proceedings of the 5th
    ACM SIGPLAN International Symposium on Machine Programming, 2021.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Drain 等 [2021]  Dawn Drain, Chen Wu, Alexey Svyatkovskiy, 和 Neel Sundaresan.
    使用预训练变压器生成错误修复。 在第5届ACM SIGPLAN国际机器编程研讨会, 2021.
- en: 'Ernst [2017] Michael D. Ernst. Natural language is a programming language:
    Applying natural language processing to software development. In 2nd Summit on
    Advances in Programming Languages, 2017.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ernst [2017]  Michael D. Ernst. 自然语言是一种编程语言: 将自然语言处理应用于软件开发中。 在第2届高级编程语言峰会,
    2017.'
- en: 'Feng et al. [2020] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng
    Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert:
    A pre-trained model for programming and natural languages. In EMNLP: Findings,
    2020.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng 等 [2020]  Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng,
    Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, 等等. Codebert: 用于编程和自然语言的预训练模型。
    在 EMNLP: Findings, 2020.'
- en: 'Guo et al. [2021] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie
    Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun
    Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming
    Zhou. Graphcodebert: Pre-training code representations with data flow. In ICLR,
    2021.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等 [2021]  Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie
    Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun
    Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, 和 Ming
    Zhou. Graphcodebert: 使用数据流预训练代码表示。 在 ICLR, 2021.'
- en: Haque et al. [2020] Sakib Haque, Alexander LeClair, Lingfei Wu, and Collin McMillan.
    Improved automatic summarization of subroutines via attention to file context.
    In MSR, 2020.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haque 等 [2020]  Sakib Haque, Alexander LeClair, Lingfei Wu, 和 Collin McMillan.
    通过关注文件上下文改进子程序的自动摘要。 在 MSR, 2020.
- en: 'Hassan and Xie [2010] Ahmed E Hassan and Tao Xie. Software intelligence: the
    future of mining software engineering data. In FSE/SDP workshop, 2010.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hassan 和  Xie [2010]  Ahmed E Hassan 和 Tao Xie. 软件智能: 开采软件工程数据的未来。 在FSE/SDP研讨会,
    2010.'
- en: Hellendoorn et al. [2020] Vincent J Hellendoorn, Charles Sutton, Rishabh Singh,
    Petros Maniatis, and David Bieber. Global relational models of source code. In
    ICLR, 2020.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hellendoorn 等 [2020]  Vincent J Hellendoorn, Charles Sutton, Rishabh Singh,
    Petros Maniatis, 和 David Bieber. 源代码的全局关系模型. 在 ICLR, 2020.
- en: Hochreiter and Schmidhuber [1997] Sepp Hochreiter and Jürgen Schmidhuber. Long
    short-term memory. Neural Computation, 1997.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber [1997]  Sepp Hochreiter 和 Jürgen Schmidhuber. 长短期记忆。
    神经计算, 1997.
- en: Howard and Ruder [2018] Jeremy Howard and Sebastian Ruder. Universal language
    model fine-tuning for text classification. In ACL, 2018.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard 和  Ruder [2018]  Jeremy Howard 和 Sebastian Ruder. 通用语言模型微调用于文本分类. 在ACL,
    2018.
- en: Hu et al. [2018a] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. Deep code
    comment generation. In ICPC, 2018.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等 [2018a]  Xing Hu, Ge Li, Xin Xia, David Lo, 和 Zhi Jin. 深层代码注释生成。 在ICPC,
    2018.
- en: Hu et al. [2018b] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin.
    Summarizing source code with transferred api knowledge. In IJCAI, 2018.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等 [2018b]  Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, 和 Zhi Jin. 用转移的API知识概括源代码。
    在 IJCAI, 2018.
- en: 'Husain et al. [2019] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
    and Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic
    code search. arXiv:1909.09436, 2019.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Husain 等 [2019]  Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
    和 Marc Brockschmidt. Codesearchnet 挑战: 评估语义代码搜索的状态。 arXiv:1909.09436, 2019.'
- en: Iyer et al. [2018] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke
    Zettlemoyer. Mapping language to code in programmatic context. In EMNLP, 2018.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iyer 等人 [2018] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung 和 Luke Zettlemoyer。在编程上下文中将语言映射到代码。发表于
    EMNLP，2018年。
- en: Iyer et al. [2019] Srinivasan Iyer, Alvin Cheung, and Luke Zettlemoyer. Learning
    programmatic idioms for scalable semantic parsing. In EMNLP, 2019.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iyer 等人 [2019] Srinivasan Iyer, Alvin Cheung 和 Luke Zettlemoyer。学习可扩展的语义解析编程习语。发表于
    EMNLP，2019年。
- en: Jain et al. [2021] Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph
    Gonzalez, and Ion Stoica. Contrastive code representation learning. In EMNLP,
    2021.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等人 [2021] Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph Gonzalez
    和 Ion Stoica。对比代码表示学习。发表于 EMNLP，2021年。
- en: 'Jiang et al. [2021] Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu.
    Treebert: A tree-based pre-trained model for programming language. In UAI, 2021.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 [2021] Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li 和 Lei Lyu。Treebert：一种基于树的预训练编程语言模型。发表于
    UAI，2021年。
- en: Kanade et al. [2020] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and
    Kensen Shi. Learning and evaluating contextual embedding of source code. In ICML,
    2020.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanade 等人 [2020] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan 和 Kensen
    Shi。学习和评估源代码的上下文嵌入。发表于 ICML，2020年。
- en: 'Karampatsis and Sutton [2020] Rafael-Michael Karampatsis and Charles Sutton.
    Scelmo: Source code embeddings from language models. arXiv:2004.13214, 2020.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karampatsis 和 Sutton [2020] Rafael-Michael Karampatsis 和 Charles Sutton。Scelmo：来自语言模型的源代码嵌入。arXiv:2004.13214，2020年。
- en: LeClair et al. [2019] Alexander LeClair, Siyuan Jiang, and Collin McMillan.
    A neural model for generating natural language summaries of program subroutines.
    In ICSE, 2019.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeClair 等人 [2019] Alexander LeClair, Siyuan Jiang 和 Collin McMillan。生成程序子例程自然语言摘要的神经模型。发表于
    ICSE，2019年。
- en: LeClair et al. [2020] Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin
    McMillan. Improved code summarization via a graph neural network. In ICPC, 2020.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeClair 等人 [2020] Alexander LeClair, Sakib Haque, Lingfei Wu 和 Collin McMillan。通过图神经网络改进代码摘要。发表于
    ICPC，2020年。
- en: 'Lewis et al. [2020] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart:
    Denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. In ACL, 2020.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人 [2020] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
    Mohamed, Omer Levy, Veselin Stoyanov 和 Luke Zettlemoyer。Bart：用于自然语言生成、翻译和理解的序列到序列去噪预训练。发表于
    ACL，2020年。
- en: 'Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:
    A robustly optimized bert pretraining approach. arXiv:1907.11692, 2019.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi
    Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer 和 Veselin Stoyanov。Roberta：一种鲁棒优化的
    BERT 预训练方法。arXiv:1907.11692，2019年。
- en: Liu et al. [2020] Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. Multi-task learning
    based pre-trained language model for code completion. In ASE, 2020.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2020] Fang Liu, Ge Li, Yunfei Zhao 和 Zhi Jin。基于多任务学习的预训练语言模型用于代码补全。发表于
    ASE，2020年。
- en: 'Lu et al. [2021] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy,
    Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong
    Zhou, Linjun Shou, Long Zhou, Michele Tufano, MING GONG, Ming Zhou, Nan Duan,
    Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie LIU. CodeXGLUE: A machine
    learning benchmark dataset for code understanding and generation. In NeurIPS Datasets
    and Benchmarks Track (Round 1), 2021.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人 [2021] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy,
    Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong
    Zhou, Linjun Shou, Long Zhou, Michele Tufano, MING GONG, Ming Zhou, Nan Duan,
    Neel Sundaresan, Shao Kun Deng, Shengyu Fu 和 Shujie LIU。CodeXGLUE：用于代码理解和生成的机器学习基准数据集。发表于
    NeurIPS 数据集和基准 Track (Round 1)，2021年。
- en: Mastropaolo et al. [2021] Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper,
    David Nader Palacio, Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. Studying
    the usage of text-to-text transfer transformer to support code-related tasks.
    In ICSE, 2021.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mastropaolo 等人 [2021] Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper,
    David Nader Palacio, Denys Poshyvanyk, Rocco Oliveto 和 Gabriele Bavota。研究使用文本到文本转换
    Transformer 支持代码相关任务。发表于 ICSE，2021年。
- en: Miceli-Barone and Sennrich [2017] Antonio Valerio Miceli-Barone and Rico Sennrich.
    A parallel corpus of python functions and documentation strings for automated
    code documentation and code generation. In IJCNLP, 2017.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miceli-Barone 和 Sennrich [2017] Antonio Valerio Miceli-Barone 和 Rico Sennrich。用于自动化代码文档和代码生成的
    Python 函数及文档字符串的平行语料库。发表于 IJCNLP，2017年。
- en: Mou et al. [2016] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. Convolutional
    neural networks over tree structures for programming language processing. In AAAI,
    2016.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mou等人 [2016] Lili Mou, Ge Li, Lu Zhang, Tao Wang, 和 Zhi Jin. 基于树结构的卷积神经网络用于编程语言处理.
    见于AAAI, 2016.
- en: 'Nafi et al. [2019] Kawser Wazed Nafi, Tonny Shekha Kar, Banani Roy, Chanchal K
    Roy, and Kevin A Schneider. Clcdsa: cross language code clone detection using
    syntactical features and api documentation. In ASE, 2019.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nafi等人 [2019] Kawser Wazed Nafi, Tonny Shekha Kar, Banani Roy, Chanchal K Roy,
    和 Kevin A Schneider. CLCDSA: 利用语法特征和API文档进行跨语言代码克隆检测. 见于ASE, 2019.'
- en: 'Niu et al. [2022] Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang,
    and Bin Luo. Spt-code: Sequence-to-sequence pre-training for learning source code
    representations. arXiv:2201.01549, 2022.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Niu等人 [2022] Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, 和
    Bin Luo. SPT-CODE: 序列到序列的预训练以学习源代码表示. arXiv:2201.01549, 2022.'
- en: Panthaplackel et al. [2021] Sheena Panthaplackel, Miltiadis Allamanis, and Marc
    Brockschmidt. Copy that! editing sequences by copying spans. In AAAI, 2021.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Panthaplackel等人 [2021] Sheena Panthaplackel, Miltiadis Allamanis, 和 Marc Brockschmidt.
    复制它! 通过复制跨度编辑序列. 见于AAAI, 2021.
- en: 'Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papineni等人 [2002] Kishore Papineni, Salim Roukos, Todd Ward, 和 Wei-Jing Zhu.
    BLEU: 一种自动评估机器翻译的方法. 见于ACL, 2002.'
- en: Peng et al. [2021] Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, and
    Tie-Yan Liu. How could neural networks understand programs? In ICML, 2021.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng等人 [2021] Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, 和 Tie-Yan
    Liu. 神经网络如何理解程序？ 见于ICML, 2021.
- en: Peters et al. [2018] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word
    representations. In NAACL-HLT, 2018.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters等人 [2018] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, 和 Luke Zettlemoyer. 深度上下文化词表示. 见于NAACL-HLT, 2018.
- en: 'Phan et al. [2021] Long Phan, Hieu Tran, Daniel Le, Hieu Nguyen, James Anibal,
    Alec Peltekian, and Yanfang Ye. Cotext: Multi-task learning with code-text transformer.
    arXiv:2105.08645, 2021.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Phan等人 [2021] Long Phan, Hieu Tran, Daniel Le, Hieu Nguyen, James Anibal, Alec
    Peltekian, 和 Yanfang Ye. COTEXT: 基于代码-文本转换器的多任务学习. arXiv:2105.08645, 2021.'
- en: Pontes et al. [2018] Elvys Linhares Pontes, Stéphane Huet, Andréa Carneiro Linhares,
    and Juan-Manuel Torres-Moreno. Predicting the semantic textual similarity with
    siamese cnn and lstm. In TALN, 2018.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pontes等人 [2018] Elvys Linhares Pontes, Stéphane Huet, Andréa Carneiro Linhares,
    和 Juan-Manuel Torres-Moreno. 使用Siamese CNN和LSTM预测语义文本相似性. 见于TALN, 2018.
- en: 'Pradel and Sen [2018] Michael Pradel and Koushik Sen. Deepbugs: A learning
    approach to name-based bug detection. Proceedings of the ACM on Programming Languages,
    2018.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pradel和Sen [2018] Michael Pradel 和 Koushik Sen. Deepbugs: 基于名称的缺陷检测学习方法. ACM编程语言学会会议录,
    2018.'
- en: 'Qi et al. [2021] Weizhen Qi, Yeyun Gong, Yu Yan, Can Xu, Bolun Yao, Bartuer
    Zhou, Biao Cheng, Daxin Jiang, Jiusheng Chen, Ruofei Zhang, et al. Prophetnet-x:
    Large-scale pre-training models for english, chinese, multi-lingual, dialog, and
    code generation. arXiv:2104.08006, 2021.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qi等人 [2021] Weizhen Qi, Yeyun Gong, Yu Yan, Can Xu, Bolun Yao, Bartuer Zhou,
    Biao Cheng, Daxin Jiang, Jiusheng Chen, Ruofei Zhang等人. Prophetnet-x: 大规模预训练模型用于英语、中文、多语言、对话及代码生成.
    arXiv:2104.08006, 2021.'
- en: 'Qiu et al. [2020] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai,
    and Xuanjing Huang. Pre-trained models for natural language processing: A survey.
    Science China Technological Sciences, 2020.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qiu等人 [2020] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, 和 Xuanjing
    Huang. 自然语言处理的预训练模型: 调查. Science China Technological Sciences, 2020.'
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    In OpenAI Blog, 2019.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等人 [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever等人. 语言模型是无监督的多任务学习者. 见于OpenAI Blog, 2019.
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. JMLR,
    2020.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel等人 [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu. 使用统一的文本到文本转换器探索迁移学习的极限.
    JMLR, 2020.
- en: Raychev et al. [2016] Veselin Raychev, Pavol Bielik, and Martin Vechev. Probabilistic
    model for code with decision trees. In OOPSLA, 2016.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raychev等人 [2016] Veselin Raychev, Pavol Bielik, 和 Martin Vechev. 基于决策树的代码概率模型.
    见于OOPSLA, 2016.
- en: 'Ren et al. [2020] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu
    Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. Codebleu: a method
    for automatic evaluation of code synthesis. arXiv:2009.10297, 2020.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren et al. [2020] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu
    Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, 和 Shuai Ma。Codebleu: 一种自动评估代码合成的方法。arXiv:2009.10297,
    2020。'
- en: Roziere et al. [2020] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot,
    and Guillaume Lample. Unsupervised translation of programming languages. In NeurIPS,
    2020.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roziere et al. [2020] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot,
    和 Guillaume Lample。无监督编程语言翻译。发表于 NeurIPS, 2020。
- en: 'Roziere et al. [2021] Baptiste Roziere, Marie-Anne Lachaux, Marc Szafraniec,
    and Guillaume Lample. Dobf: A deobfuscation pre-training objective for programming
    languages. arXiv:2102.07492, 2021.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roziere et al. [2021] Baptiste Roziere, Marie-Anne Lachaux, Marc Szafraniec,
    和 Guillaume Lample。Dobf: 一种用于编程语言的去混淆预训练目标。arXiv:2102.07492, 2021。'
- en: Shido et al. [2019] Yusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto, Atsushi
    Miyamoto, and Tadayuki Matsumura. Automatic source code summarization with extended
    tree-lstm. In IJCNN, 2019.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shido et al. [2019] Yusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto, Atsushi
    Miyamoto, 和 Tadayuki Matsumura。使用扩展树-LSTM 的自动源代码总结。发表于 IJCNN, 2019。
- en: Svajlenko et al. [2014] Jeffrey Svajlenko, Judith F Islam, Iman Keivanloo, Chanchal K
    Roy, and Mohammad Mamun Mia. Towards a big data curated benchmark of inter-project
    code clones. In ICSME, 2014.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Svajlenko et al. [2014] Jeffrey Svajlenko, Judith F Islam, Iman Keivanloo, Chanchal
    K Roy, 和 Mohammad Mamun Mia。迈向大数据策划的跨项目代码克隆基准。发表于 ICSME, 2014。
- en: 'Svyatkovskiy et al. [2020] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
    and Neel Sundaresan. Intellicode compose: Code generation using transformer. In
    ESEC/FSE, 2020.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Svyatkovskiy et al. [2020] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
    和 Neel Sundaresan。Intellicode compose: 使用 Transformer 进行代码生成。发表于 ESEC/FSE, 2020。'
- en: 'Tang et al. [2022] Ze Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang,
    Zhelin Zhu, and Bin Luo. Ast-trans: Code summarization with efficient tree-structured
    attention. In ICSE, 2022.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tang et al. [2022] Ze Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang,
    Zhelin Zhu, 和 Bin Luo。Ast-trans: 高效树结构注意力的代码总结。发表于 ICSE, 2022。'
- en: Tufano et al. [2019a] Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele
    Bavota, and Denys Poshyvanyk. On learning meaningful code changes via neural machine
    translation. In ICSE, 2019.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tufano et al. [2019a] Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele
    Bavota, 和 Denys Poshyvanyk。通过神经机器翻译学习有意义的代码更改。发表于 ICSE, 2019。
- en: Tufano et al. [2019b] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di
    Penta, Martin White, and Denys Poshyvanyk. An empirical study on learning bug-fixing
    patches in the wild via neural machine translation. TOSEM, 2019.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tufano et al. [2019b] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano
    Di Penta, Martin White, 和 Denys Poshyvanyk。关于通过神经机器翻译学习现实环境中的错误修复补丁的实证研究。发表于 TOSEM,
    2019。
- en: Vasic et al. [2019] Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber,
    and Rishabh singh. Neural program repair by jointly learning to localize and repair.
    In ICLR, 2019.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vasic et al. [2019] Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber,
    和 Rishabh Singh。通过联合学习定位和修复实现神经程序修复。发表于 ICLR, 2019。
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In NeurIPS, 2017.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin。注意力机制是你所需的一切。发表于
    NeurIPS, 2017。
- en: Wang et al. [2020] Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. Detecting
    code clones with graph neural network and flow-augmented abstract syntax tree.
    In SANER, 2020.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2020] Wenhan Wang, Ge Li, Bo Ma, Xin Xia, 和 Zhi Jin。使用图神经网络和流增强抽象语法树检测代码克隆。发表于
    SANER, 2020。
- en: 'Wang et al. [2021a] Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao
    Liu, Li Li, Hao Wu, Jin Liu, and Xin Jiang. Syncobert: Syntax-guided multi-modal
    contrastive pre-training for code representation. arXiv:2108.04556, 2021.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2021a] Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao
    Liu, Li Li, Hao Wu, Jin Liu, 和 Xin Jiang。Syncobert: 语法引导的多模态对比预训练用于代码表示。arXiv:2108.04556,
    2021。'
- en: 'Wang et al. [2021b] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi.
    Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding
    and generation. In EMNLP, 2021.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2021b] Yue Wang, Weishi Wang, Shafiq Joty, 和 Steven CH Hoi。Codet5:
    识别符感知的统一预训练编码器-解码器模型用于代码理解和生成。发表于 EMNLP, 2021。'
- en: Watson et al. [2020] Cody Watson, Michele Tufano, Kevin Moran, Gabriele Bavota,
    and Denys Poshyvanyk. On learning meaningful assert statements for unit test cases.
    In ICSE, 2020.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watson et al. [2020] Cody Watson, Michele Tufano, Kevin Moran, Gabriele Bavota,
    和 Denys Poshyvanyk。学习有意义的断言语句用于单元测试用例。发表于 ICSE, 2020。
- en: Xu et al. [2021] Zenan Xu, Daya Guo, Duyu Tang, Qinliang Su, Linjun Shou, Ming
    Gong, Wanjun Zhong, Xiaojun Quan, Daxin Jiang, and Nan Duan. Syntax-enhanced pre-trained
    model. In ACL/IJCNLP (1), 2021.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 [2021] Zenan Xu, Daya Guo, Duyu Tang, Qinliang Su, Linjun Shou, Ming Gong,
    Wanjun Zhong, Xiaojun Quan, Daxin Jiang 和 Nan Duan. 语法增强的预训练模型。发表于 ACL/IJCNLP
    (1), 2021。
- en: 'Yang et al. [2019] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R
    Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for
    language understanding. NeurIPS, 32, 2019.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 [2019] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ
    R Salakhutdinov 和 Quoc V Le. Xlnet: 一种用于语言理解的广义自回归预训练方法。发表于 NeurIPS, 32, 2019。'
- en: 'Ye et al. [2020] Fangke Ye, Shengtian Zhou, Anand Venkat, Ryan Marcus, Nesime
    Tatbul, Jesmin Jahan Tithi, Niranjan Hasabnis, Paul Petersen, Timothy Mattson,
    Tim Kraska, et al. Misim: A neural code semantics similarity system using the
    context-aware semantics structure. CoRR, 2020.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye 等人 [2020] Fangke Ye, Shengtian Zhou, Anand Venkat, Ryan Marcus, Nesime Tatbul,
    Jesmin Jahan Tithi, Niranjan Hasabnis, Paul Petersen, Timothy Mattson, Tim Kraska
    等人。Misim: 一种使用上下文感知语义结构的神经代码语义相似度系统。发表于 CoRR, 2020。'
- en: 'Zhang et al. [2021] Jingfeng Zhang, Haiwen Hong, Yin Zhang, Yao Wan, Ye Liu,
    and Yulei Sui. Disentangled code representation learning for multiple programming
    languages. In ACL: Findings, 2021.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2021] Jingfeng Zhang, Haiwen Hong, Yin Zhang, Yao Wan, Ye Liu 和 Yulei
    Sui. 针对多种编程语言的解耦代码表示学习。发表于 ACL: Findings, 2021。'
- en: 'Zhou et al. [2019] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and
    Yang Liu. Devign: Effective vulnerability identification by learning comprehensive
    program semantics via graph neural networks. In NeurIPS, 2019.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人 [2019] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du 和 Yang
    Liu. Devign: 通过图神经网络学习全面的程序语义来有效识别漏洞。发表于 NeurIPS, 2019。'
- en: Zhou et al. [2021] Xin Zhou, DongGyun Han, and David Lo. Assessing generalizability
    of CodeBERT. In ICSME, 2021.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2021] Xin Zhou, DongGyun Han 和 David Lo. 评估 CodeBERT 的泛化能力。发表于 ICSME,
    2021。
- en: Zügner et al. [2021] Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure
    Leskovec, and Stephan Günnemann. Language-agnostic representation learning of
    source code from structure and context. In ICLR, 2021.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zügner 等人 [2021] Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec
    和 Stephan Günnemann. 从结构和上下文中进行语言无关的源代码表示学习。发表于 ICLR, 2021。
