- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:41:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:41:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2302.10473] Oriented Object Detection in Optical Remote Sensing Images using
    Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2302.10473] 基于深度学习的光学遥感图像中的定向目标检测：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2302.10473](https://ar5iv.labs.arxiv.org/html/2302.10473)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2302.10473](https://ar5iv.labs.arxiv.org/html/2302.10473)
- en: \cormark
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \cormark
- en: '[1] \cormark[1] \cormark[2]'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] \cormark[1] \cormark[2]'
- en: \cortext
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \cortext
- en: '[coauthor]Equal contribution. \cortext[cor1]Corresponding author. Email address:
    lizhang08@nudt.edu.cn.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[合著者]贡献相等。 \cortext[cor1]通讯作者。电子邮件地址：lizhang08@nudt.edu.cn。'
- en: 'Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的光学遥感图像中的定向目标检测：综述
- en: Kun Wang    Zi Wang    Zhang Li    Ang Su    Xichao Teng    Minhao Liu    Qifeng
    Yu College of Aerospace Science and Engineering, National University of Defense
    Technology, Changsha, 410000, China Hunan Provincial Key Laboratory of Image Measurement
    and Vision Navigation, Changsha, 410000, China
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 孙昆    王子    张莉    苏昂    滕西超    刘敏浩    于启峰 国防科技大学航天学院，中国长沙，410000 湖南省图像测量与视觉导航重点实验室，中国长沙，410000
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Oriented object detection is one of the most fundamental and challenging tasks
    in remote sensing, aiming at locating the oriented objects of numerous predefined
    object categories. Recently, deep learning based methods have achieved remarkable
    performance in detecting oriented objects in optical remote sensing imagery. However,
    a thorough review of the literature in remote sensing has not yet emerged. Therefore,
    we give a comprehensive survey of recent advances and cover many aspects of oriented
    object detection, including problem definition, commonly used datasets, evaluation
    protocols, detection frameworks, oriented object representations, and feature
    representations. Besides, the state-of-the-art methods are analyzed and discussed.
    We finally discuss future research directions to put forward some useful research
    guidance. We believe that this survey shall be valuable to researchers across
    academia and industry.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 定向目标检测是遥感领域中最基础和具有挑战性的任务之一，旨在定位多个预定义目标类别中的定向目标。最近，基于深度学习的方法在光学遥感图像中的定向目标检测中取得了显著的成果。然而，目前尚未出现对遥感领域文献的全面综述。因此，我们对近期的进展进行了全面的综述，涵盖了定向目标检测的许多方面，包括问题定义、常用数据集、评估协议、检测框架、定向目标表示和特征表示。此外，我们还分析和讨论了最先进的方法。最后，我们讨论了未来的研究方向，并提出了一些有用的研究指导。我们相信，这份综述将对学术界和工业界的研究人员具有重要价值。
- en: 'keywords:'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Oriented object detection \sepRemote sensing \sepDeep learning
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 定向目标检测 \sep遥感 \sep深度学习
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: With the rapid development of remote sensing (RS) technologies, an increasing
    number of images with various resolutions and different spectra can be easily
    obtained by satellites or unmanned aerial vehicles (UAVs). Naturally, it is an
    urgent demand of the research community to investigate a variety of advanced technologies
    for processing and analyzing massive RS images automatically and efficiently.
    As a crucial cornerstone of automatic analysis for RS images, object detection
    aims to recognize objects of predefined categories from given images and to regress
    a precise localization of each object instance (e.g., via an oriented bounding
    box). Object detection in RS images serves as an essential step for a broad range
    of applications, including intelligent monitoring (Zhao et al., [2018](#bib.bib199);
    Salvoldi et al., [2022](#bib.bib121)), urban planning (Burochin et al., [2014](#bib.bib7)),
    port management (Zhang et al., [2021a](#bib.bib195)), and military reconnaissance (Liu
    et al., [2022](#bib.bib98)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着遥感（RS）技术的快速发展，卫星或无人机（UAV）可以轻松获取越来越多分辨率不同、光谱多样的图像。自然地，研究界迫切需要调查各种先进技术，以自动高效地处理和分析大量遥感图像。作为自动分析遥感图像的一个关键基石，目标检测旨在从给定图像中识别预定义类别的目标，并回归每个目标实例的精确定位（例如，通过定向边界框）。遥感图像中的目标检测是广泛应用的关键步骤，包括智能监测（赵等，[2018](#bib.bib199)；萨尔沃尔迪等，[2022](#bib.bib121)）、城市规划（布罗钦等，[2014](#bib.bib7)）、港口管理（张等，[2021a](#bib.bib195)）和军事侦察（刘等，[2022](#bib.bib98)）。
- en: '![Refer to caption](img/3f62006171cfc8d5608da8ab871253f3.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3f62006171cfc8d5608da8ab871253f3.png)'
- en: (a) OBB representation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: (a) OBB 表示。
- en: '![Refer to caption](img/67751b92ca2cb05ce2b6d0c84edc48ac.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/67751b92ca2cb05ce2b6d0c84edc48ac.png)'
- en: (b) HBB representation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: (b) HBB 表示。
- en: 'Figure 1: Comparison between OBB and HBB (Xia et al., [2018](#bib.bib161);
    Ding et al., [2022](#bib.bib30)). (a) OBB representation of objects. (b) is a
    failure case of the HBB representation, which brings high overlap compared to
    (a).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：OBB 和 HBB 的比较 (Xia et al., [2018](#bib.bib161); Ding et al., [2022](#bib.bib30))。
    (a) 是物体的 OBB 表示。 (b) 是 HBB 表示的失败案例，与 (a) 相比带来了较高的重叠。
- en: 'As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Oriented Object
    Detection in Optical Remote Sensing Images using Deep Learning: A Survey"), RS
    object detection can be divided into two types: horizontal object detection and
    oriented object detection (also called rotated object detection), according to
    the representation style of objects. The former represents the detected object
    using a horizontal bounding box (HBB) with the format of $(x,y,w,h)$ (Everingham
    et al., [2010](#bib.bib34); Lin et al., [2014](#bib.bib90); Russakovsky et al.,
    [2015](#bib.bib120)), where $(x,y)$ denotes the coordinates of the bounding box
    center, $w$ and $h$ denote the width and height of the bounding box, respectively.
    The latter locates the detected object using an oriented bounding box (OBB) with
    the format of $(x,y,w,h,\theta)$, where $\theta$ denotes the rotation angle with
    respect to the horizontal direction. Hence, the second depicts a more accurate
    location by utilizing extra direction information.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Oriented Object Detection in Optical
    Remote Sensing Images using Deep Learning: A Survey") 所示，根据物体的表示方式，遥感（RS）物体检测可以分为两种类型：水平物体检测和定向物体检测（也称为旋转物体检测）。前者使用水平边界框（HBB）来表示检测到的物体，格式为
    $(x,y,w,h)$ (Everingham et al., [2010](#bib.bib34); Lin et al., [2014](#bib.bib90);
    Russakovsky et al., [2015](#bib.bib120))，其中 $(x,y)$ 表示边界框中心的坐标，$w$ 和 $h$ 分别表示边界框的宽度和高度。后者使用定向边界框（OBB）来定位检测到的物体，格式为
    $(x,y,w,h,\theta)$，其中 $\theta$ 表示相对于水平方向的旋转角度。因此，后者通过利用额外的方向信息来描绘更准确的位置。'
- en: 'Traditional detectors rely on handcrafted descriptors (Dalal and Triggs, [2005](#bib.bib27);
    Fei-Fei and Perona, [2005](#bib.bib35); Wright et al., [2009](#bib.bib158); Blaschke,
    [2010](#bib.bib3); Leitloff et al., [2010](#bib.bib79); Stankov and He, [2013](#bib.bib130);
    Blaschke et al., [2014](#bib.bib4)) and often show limited performance due to
    the shallow features. Recent years have seen impressive progress in computer vision
    with the advance of deep neural networks (DNN) (Hinton and Salakhutdinov, [2006](#bib.bib61);
    LeCun et al., [2015](#bib.bib78); Chen et al., [2018](#bib.bib13); He et al.,
    [2016](#bib.bib58); Krizhevsky et al., [2012](#bib.bib73), [2017](#bib.bib74)).
    Benefiting from the continuous improvement of computing resources, DNN can learn
    high-level patterns from large-scale datasets in an end-to-end fashion. Therefore,
    DNN-based methods can exploit representative and discriminative features. Recently,
    various DNN-based detectors have been proposed and have dominated the state-of-the-art.
    Most of these methods focus on designing horizontal object detectors (Girshick
    et al., [2014](#bib.bib41); Girshick, [2015](#bib.bib40); Ren et al., [2017](#bib.bib118);
    Liu et al., [2016a](#bib.bib97); Lin et al., [2020](#bib.bib89); Redmon et al.,
    [2016](#bib.bib116); Redmon and Farhadi, [2017](#bib.bib117); Hei and Jia, [2020](#bib.bib60);
    Duan et al., [2019](#bib.bib33); Zhou et al., [2019](#bib.bib206); Yang et al.,
    [2019b](#bib.bib181)) for natural scene images, which are taken from a horizontal
    perspective. In contrast, RS images are typically captured from the bird-eye view
    (BEV), posing the additional challenges for detection tasks as follows (Xia et al.,
    [2018](#bib.bib161)):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的检测器依赖于手工制作的描述符 (Dalal and Triggs, [2005](#bib.bib27); Fei-Fei and Perona,
    [2005](#bib.bib35); Wright et al., [2009](#bib.bib158); Blaschke, [2010](#bib.bib3);
    Leitloff et al., [2010](#bib.bib79); Stankov and He, [2013](#bib.bib130); Blaschke
    et al., [2014](#bib.bib4))，由于特征浅层，性能通常有限。近年来，随着深度神经网络（DNN） (Hinton and Salakhutdinov,
    [2006](#bib.bib61); LeCun et al., [2015](#bib.bib78); Chen et al., [2018](#bib.bib13);
    He et al., [2016](#bib.bib58); Krizhevsky et al., [2012](#bib.bib73), [2017](#bib.bib74))
    的进步，计算机视觉取得了显著的进展。得益于计算资源的持续改进，DNN 可以从大规模数据集中以端到端的方式学习高级模式。因此，基于 DNN 的方法可以利用具有代表性和区分性的特征。近年来，提出了各种基于
    DNN 的检测器，并主导了最新技术的发展。大多数这些方法专注于设计自然场景图像的水平物体检测器 (Girshick et al., [2014](#bib.bib41);
    Girshick, [2015](#bib.bib40); Ren et al., [2017](#bib.bib118); Liu et al., [2016a](#bib.bib97);
    Lin et al., [2020](#bib.bib89); Redmon et al., [2016](#bib.bib116); Redmon and
    Farhadi, [2017](#bib.bib117); Hei and Jia, [2020](#bib.bib60); Duan et al., [2019](#bib.bib33);
    Zhou et al., [2019](#bib.bib206); Yang et al., [2019b](#bib.bib181))，这些图像是从水平视角拍摄的。相比之下，RS
    图像通常从鸟瞰视角（BEV）拍摄，这为检测任务带来了额外的挑战 (Xia et al., [2018](#bib.bib161))：
- en: '![Refer to caption](img/76f7741d5be2760c813e2a0475f89b25.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/76f7741d5be2760c813e2a0475f89b25.png)'
- en: (a) Arbitrary orientations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 任意方向。
- en: '![Refer to caption](img/f0032539cc0304263d0293604abaefe3.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/f0032539cc0304263d0293604abaefe3.png)'
- en: (b) Dense arrangement.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 密集排列。
- en: '![Refer to caption](img/ced7db887bbcd7a4004cff6fd0d630fd.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/ced7db887bbcd7a4004cff6fd0d630fd.png)'
- en: (c) Large aspect ratio.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 大纵横比。
- en: 'Figure 2: Illustration of challenges in RS images (Xia et al., [2018](#bib.bib161);
    Ding et al., [2022](#bib.bib30)). (a) (c) Examples of arbitrary orientations,
    dense arrangement and large aspect ratio, respectively.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：遥感图像挑战的示意图 (Xia等，[2018](#bib.bib161)；Ding等，[2022](#bib.bib30))。 (a) (c) 分别是任意方向、密集排列和大纵横比的示例。
- en: 'Arbitrary orientations. In BEV, objects in RS images can appear in arbitrary
    orientations, resulting in an adverse impact on the detection performance, as
    shown in Figure [2(a)](#S1.F2.sf1 "In Figure 2 ‣ 1 Introduction ‣ Oriented Object
    Detection in Optical Remote Sensing Images using Deep Learning: A Survey").'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 任意方向。在BEV中，遥感图像中的物体可能以任意方向出现，从而对检测性能产生不利影响，如图 [2(a)](#S1.F2.sf1 "在图2 ‣ 1 介绍
    ‣ 使用深度学习的光学遥感图像中的定向目标检测：综述")所示。
- en: 'Dense arrangement. In some specific scenarios, there may be many small objects
    distributed densely, e.g., the ships in a harbor and the vehicles in a parking
    lot, as illustrated in Figure [2(b)](#S1.F2.sf2 "In Figure 2 ‣ 1 Introduction
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey"). As a result, one horizontal box predicted by detectors may contain
    multiple crowded objects, where mutual interference among multiple objects can
    pose a huge challenge.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 密集排列。在某些特定场景中，可能会有许多小物体密集分布，例如港口中的船只和停车场中的车辆，如图 [2(b)](#S1.F2.sf2 "在图2 ‣ 1 介绍
    ‣ 使用深度学习的光学遥感图像中的定向目标检测：综述")所示。因此，由检测器预测的一个水平框可能包含多个拥挤的物体，多个物体之间的相互干扰可能会带来巨大挑战。
- en: 'Large aspect ratio. As shown in Figure [2(c)](#S1.F2.sf3 "In Figure 2 ‣ 1 Introduction
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey"), RS images typically contain some categories with an extremely large
    aspect ratio, such as bridges, ships, harbors, etc. The localization accuracy
    of these categories is very sensitive to the orientation error.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 大纵横比。如图 [2(c)](#S1.F2.sf3 "在图2 ‣ 1 介绍 ‣ 使用深度学习的光学遥感图像中的定向目标检测：综述")所示，遥感图像通常包含一些纵横比极大的类别，如桥梁、船只、港口等。这些类别的定位精度对方向错误非常敏感。
- en: 'Therefore, although these horizontal object detectors perform well on natural
    scene images, they are not suitable for objects with arbitrary orientations in
    RS images. A HBB cannot depict the object orientation and contains redundant information
    in the background. What’s more, in dense arrangement scenarios (especially for
    objects with extremely large aspect ratios), the intersection-over-union (IoU)
    between an HBB and the adjacent HBBs can be very large, as illustrated in Figure
    [1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ Oriented Object Detection in
    Optical Remote Sensing Images using Deep Learning: A Survey"). Thus, the non-maximum
    suppression (NMS) technique tends to cause missed detections. To cope with these
    challenges, more efforts have been devoted to oriented object detection. Because
    an OBB can enclose the objects precisely and distinguish the object from the densely
    arranged adjacent objects. Several milestone methods and widely-used datasets
    are illustrated in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Oriented Object
    Detection in Optical Remote Sensing Images using Deep Learning: A Survey").'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管这些水平物体检测器在自然场景图像中表现良好，但它们不适合用于遥感图像中具有任意方向的物体。HBB无法描绘物体方向，并且包含背景中的冗余信息。此外，在密集排列的场景中（尤其是对具有极大纵横比的物体），HBB与相邻HBB之间的交并比（IoU）可能非常大，如图
    [1(b)](#S1.F1.sf2 "在图1 ‣ 1 介绍 ‣ 使用深度学习的光学遥感图像中的定向目标检测：综述")所示。因此，非极大值抑制（NMS）技术往往会导致漏检。为了应对这些挑战，更多的努力被投入到定向目标检测中。因为OBB可以精确地包围物体，并将物体与密集排列的相邻物体区分开来。一些里程碑式的方法和广泛使用的数据集如图
    [3](#S1.F3 "图3 ‣ 1 介绍 ‣ 使用深度学习的光学遥感图像中的定向目标检测：综述")所示。
- en: 'While enormous oriented object detection methods exist, a comprehensive survey
    of this subject is still lacking. Given the continued maturity and increasing
    concerns about this field, this paper attempts to present a thorough analysis
    of recent efforts and systematically summarize their achievements. Through reviewing
    a large number of contributions in the field of oriented object detection, our
    survey covers the following respects: problem definition, commonly used datasets,
    evaluation protocol, detection frameworks, OBB representation, and feature representation.
    Furthermore, a brief conclusion and an outlook toward future research are given.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在大量的定向物体检测方法，但对这一主题的全面综述仍然缺乏。鉴于该领域的持续成熟和日益关注，本文尝试对近期的努力进行深入分析，并系统总结其成果。通过回顾定向物体检测领域的大量贡献，我们的综述涵盖了以下几个方面：问题定义、常用数据集、评估协议、检测框架、OBB
    表示和特征表示。此外，还提供了简要的结论和对未来研究的展望。
- en: '![Refer to caption](img/5ba2d5eefb6eb36f65c474707a2e6729.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5ba2d5eefb6eb36f65c474707a2e6729.png)'
- en: 'Figure 3: Chronological overview of the milestones methods and the well-known
    datasets.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：里程碑方法和著名数据集的时间顺序概述。
- en: 1.1 Related Surveys
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 相关综述
- en: In the field of object detection, a number of prominent surveys have been published
    in recent years. Several efforts focus on a specific category, such as face detection (Zafeiriou
    et al., [2015](#bib.bib189); Wang et al., [2018a](#bib.bib146); Wu and Ji, [2019](#bib.bib160);
    Du et al., [2022](#bib.bib32)), text detection (Ye and Doermann, [2015](#bib.bib182);
    Yin et al., [2016](#bib.bib184)), pedestrian detection (Brunetti et al., [2018](#bib.bib6)),
    and ship detection (Li et al., [2021](#bib.bib81)). There are more surveys focus
    on generic horizontal object detection, aiming at detecting the objects of multiple
    predefined categories in natural scenarios (Jiao et al., [2019](#bib.bib70); Liu
    et al., [2020](#bib.bib94); Wu et al., [2020](#bib.bib159); Zhao et al., [2019b](#bib.bib202);
    Xiao et al., [2020a](#bib.bib162)). These works cover various aspects of generic
    horizontal object detection, including deep learning based detection frameworks,
    training strategies, feature representation, evaluation metrics, and typical application
    areas. What’s more, there are also surveys of generic horizontal object detection
    under specific conditions, including small object detection (Tong et al., [2020](#bib.bib138);
    Han et al., [2021c](#bib.bib53); Liu et al., [2021a](#bib.bib99)) and camouflaged
    object detection (Mondal, [2020](#bib.bib109)). Although a few surveys (Cheng
    and Han, [2016](#bib.bib16); Li et al., [2020](#bib.bib83)) analyze and summarize
    RS object detection, they focus only on traditional approaches and horizontal
    object detection. Hence, none of the above surveys focus on oriented object detection.
    To the best of our knowledge, this is the first survey paper that try to comprehensively
    cover deep learning methods for oriented object detection under RS scenarios.
    This survey focuses on the major advances of oriented object detection and also
    includes pivotal works on horizontal object detection for completeness and readability.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在物体检测领域，近年来发表了许多重要的综述。几项工作专注于特定类别，如人脸检测 (Zafeiriou et al., [2015](#bib.bib189);
    Wang et al., [2018a](#bib.bib146); Wu and Ji, [2019](#bib.bib160); Du et al.,
    [2022](#bib.bib32))、文本检测 (Ye and Doermann, [2015](#bib.bib182); Yin et al., [2016](#bib.bib184))、行人检测
    (Brunetti et al., [2018](#bib.bib6)) 和船舶检测 (Li et al., [2021](#bib.bib81))。还有更多综述关注通用水平物体检测，旨在检测自然场景中多个预定义类别的物体
    (Jiao et al., [2019](#bib.bib70); Liu et al., [2020](#bib.bib94); Wu et al., [2020](#bib.bib159);
    Zhao et al., [2019b](#bib.bib202); Xiao et al., [2020a](#bib.bib162))。这些工作涵盖了通用水平物体检测的各个方面，包括基于深度学习的检测框架、训练策略、特征表示、评估指标和典型应用领域。此外，还有关于特定条件下通用水平物体检测的综述，包括小物体检测
    (Tong et al., [2020](#bib.bib138); Han et al., [2021c](#bib.bib53); Liu et al.,
    [2021a](#bib.bib99)) 和伪装物体检测 (Mondal, [2020](#bib.bib109))。尽管有少数综述 (Cheng and
    Han, [2016](#bib.bib16); Li et al., [2020](#bib.bib83)) 分析和总结了遥感物体检测，但它们仅关注传统方法和水平物体检测。因此，以上综述都没有涉及定向物体检测。据我们所知，这是第一篇尝试全面覆盖遥感场景下定向物体检测的深度学习方法的综述论文。本综述重点介绍了定向物体检测的主要进展，并包括了水平物体检测的重要工作，以确保完整性和可读性。
- en: 1.2 Contributions
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 贡献
- en: 'The major contributions of this work are summarized as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的主要贡献总结如下：
- en: (1) Comprehensive survey of recent and advanced progress of deep learning for
    oriented object detection. We systematically summarize the commonly used datasets,
    deep learning network frameworks for oriented object detection, and state-of-the-art
    methods.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 对定向目标检测的深度学习的近期和先进进展的全面综述。我们系统总结了定向目标检测中常用的数据集、深度学习网络框架以及最先进的方法。
- en: (2) In-depth analysis and discussion of oriented object representations and
    feature representations. We discuss the challenges and corresponding solutions
    of oriented object representations, including inconsistency between metric and
    loss, angular boundary discontinuity and square-like problem, and vertexes sorting
    problem. Besides, we analyze and compare the existing methods for feature representation
    in oriented object detection, including enhanced and rotation-invariant feature
    representations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 对定向目标表示和特征表示的深入分析和讨论。我们讨论了定向目标表示的挑战及相应的解决方案，包括度量和损失之间的不一致、角界断裂和方形问题、以及顶点排序问题。此外，我们分析并比较了现有的定向目标检测特征表示方法，包括增强型和旋转不变特征表示。
- en: '(3) Overview of potential trends in the future. We shed light on possible directions
    in the future from seven aspects: domain adaptation, scale adaption, long-tailed
    oriented object detection, multi-modal information fusion, lightweight methods,
    video object detection, and object instance segmentation.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 对未来潜在趋势的概述。我们从七个方面探讨未来可能的发展方向：领域适应、规模适应、长尾定向目标检测、多模态信息融合、轻量级方法、视频目标检测和目标实例分割。
- en: 'The structure of this paper is organized as follows. We first introduce the
    problem definition of oriented object detection in Section [2](#S2 "2 Problem
    Definition ‣ Oriented Object Detection in Optical Remote Sensing Images using
    Deep Learning: A Survey"). Then, we present an overview of commonly used datasets
    and elaborate on the evaluation protocols in Section [3](#S3 "3 Datasets and Performance
    Evaluation ‣ Oriented Object Detection in Optical Remote Sensing Images using
    Deep Learning: A Survey"). After reviewing DNN-based frameworks in Section [4](#S4
    "4 Detection Frameworks ‣ Oriented Object Detection in Optical Remote Sensing
    Images using Deep Learning: A Survey"), we discuss the OBB representation and
    feature representation in Section [5](#S5 "5 OBB Representations ‣ Oriented Object
    Detection in Optical Remote Sensing Images using Deep Learning: A Survey") and
    Section [6](#S6 "6 Feature Representations ‣ Oriented Object Detection in Optical
    Remote Sensing Images using Deep Learning: A Survey"), respectively. Furthermore,
    we analyze and compare the state-of-the-art methods in Section [7](#S7 "7 State-of-the-Art
    Methods ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep
    Learning: A Survey"). Finally, potential future research directions are discussed
    in Section [8](#S8 "8 Conclusions and Future Directions ‣ Oriented Object Detection
    in Optical Remote Sensing Images using Deep Learning: A Survey").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的结构安排如下。我们首先在第[2](#S2 "2 Problem Definition ‣ Oriented Object Detection in
    Optical Remote Sensing Images using Deep Learning: A Survey")节介绍定向目标检测的问题定义。然后，我们在第[3](#S3
    "3 Datasets and Performance Evaluation ‣ Oriented Object Detection in Optical
    Remote Sensing Images using Deep Learning: A Survey")节概述常用的数据集，并详细说明评估协议。第[4](#S4
    "4 Detection Frameworks ‣ Oriented Object Detection in Optical Remote Sensing
    Images using Deep Learning: A Survey")节回顾基于深度神经网络的框架，接着在第[5](#S5 "5 OBB Representations
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey")节和第[6](#S6 "6 Feature Representations ‣ Oriented Object Detection in
    Optical Remote Sensing Images using Deep Learning: A Survey")节分别讨论OBB表示和特征表示。此外，第[7](#S7
    "7 State-of-the-Art Methods ‣ Oriented Object Detection in Optical Remote Sensing
    Images using Deep Learning: A Survey")节分析并比较了最新的技术。最后，第[8](#S8 "8 Conclusions
    and Future Directions ‣ Oriented Object Detection in Optical Remote Sensing Images
    using Deep Learning: A Survey")节讨论了未来潜在的研究方向。'
- en: 2 Problem Definition
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题定义
- en: Object detection involves localization (where are objects from predefined categories
    located in a given image?) and recognition (which predefined categories do these
    objects belong to?). Hence, a detector needs to distinguish objects of predefined
    categories from given images by predicting the precise localization and the correct
    categorical label. Specifically, the categorical label of a predicted object is
    represented as a $C+1$ dimensional probability distribution with the format of
    $c=(p_{0},p_{1},\cdots,p_{C})$, where $C$ is the number of predefined categories,
    $p_{0}$ and $p_{1},\cdots,p_{C}$ denotes the probability of one background category
    and $C$ predefined categories, respectively. The localization predicted by an
    oriented detector is represented as an OBB. For better comprehensibility, we provide
    a common formulation of the deep learning based oriented object detection problem.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测包括定位（预定义类别的物体在给定图像中的位置？）和识别（这些物体属于哪些预定义类别？）。因此，检测器需要通过预测精确的定位和正确的类别标签来区分给定图像中的预定义类别物体。具体而言，预测物体的类别标签表示为
    $C+1$ 维的概率分布，其格式为 $c=(p_{0},p_{1},\cdots,p_{C})$，其中 $C$ 是预定义类别的数量，$p_{0}$ 和 $p_{1},\cdots,p_{C}$
    分别表示一个背景类别和 $C$ 个预定义类别的概率。由有向检测器预测的定位表示为 OBB。为了更好地理解，我们提供了基于深度学习的有向目标检测问题的常见表述。
- en: 'Given an input image $\mathbf{I}\in\mathbb{R}^{H\times W\times 3}$, we assume
    that there are $N$ annotated or ground-truth (GT) objects belonging to predefined
    categories:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入图像 $\mathbf{I}\in\mathbb{R}^{H\times W\times 3}$，我们假设有 $N$ 个标注或真实（GT）物体属于预定义类别：
- en: '|  | $\mathcal{T}=\{(c_{1}^{t},b_{1}^{t}),(c_{2}^{t},b_{2}^{t}),\cdots,(c_{N}^{t},b_{N}^{t})\}$
    |  | (1) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{T}=\{(c_{1}^{t},b_{1}^{t}),(c_{2}^{t},b_{2}^{t}),\cdots,(c_{N}^{t},b_{N}^{t})\}$
    |  | (1) |'
- en: 'where $c_{n}^{t}$ and $b_{n}^{t}$ denote the categorical label and the GT OBB
    of $n$-th object, respectively. This format is also applied to the predictions
    of the detector:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c_{n}^{t}$ 和 $b_{n}^{t}$ 分别表示第 $n$ 个物体的类别标签和 GT OBB。此格式也适用于检测器的预测：
- en: '|  | $\mathcal{P}=\{(c_{1}^{p},b_{1}^{p}),(c_{2}^{p},b_{2}^{p}),\cdots,(c_{N_{p}}^{p},b_{N_{p}}^{p})\}$
    |  | (2) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{P}=\{(c_{1}^{p},b_{1}^{p}),(c_{2}^{p},b_{2}^{p}),\cdots,(c_{N_{p}}^{p},b_{N_{p}}^{p})\}$
    |  | (2) |'
- en: where $N_{p}$ indicates the number of predicted results, $c_{n}^{p}$ represents
    the $n$-th probability distribution of predefined categories calculated by the
    sigmoid function, and $b_{n}^{p}$ denotes the $n$-th regressed OBB.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N_{p}$ 表示预测结果的数量，$c_{n}^{p}$ 表示由 sigmoid 函数计算的第 $n$ 个预定义类别的概率分布，$b_{n}^{p}$
    表示第 $n$ 个回归的 OBB。
- en: 'To train the detector, each prediction is first assigned a positive or negative
    label. A prediction is positive only if there is at least one GT object that has
    an RIoU (the intersection over the union area of two OBBs) overlap higher than
    a preset threshold $T_{RIoU}$ with it. The threshold $T_{RIoU}$ is commonly set
    as 0.5. Otherwise, the prediction is negative. The RIoU between the regressed
    OBB $b^{p}$ and the GT OBB $b^{t}$ can be computed as:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练检测器，每个预测首先被分配一个正标签或负标签。仅当存在至少一个 GT 物体与其的 RIoU（两个 OBB 的交集与并集的比值）重叠高于预设阈值
    $T_{RIoU}$ 时，预测才被视为正样本。阈值 $T_{RIoU}$ 通常设定为 0.5。否则，预测为负样本。回归的 OBB $b^{p}$ 和 GT
    OBB $b^{t}$ 之间的 RIoU 可以计算为：
- en: '|  | $RIoU(b^{p},b^{t})=\frac{Area(b^{p}\cap b^{t})}{Area(b^{p}\cup b^{t})}$
    |  | (3) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $RIoU(b^{p},b^{t})=\frac{Area(b^{p}\cap b^{t})}{Area(b^{p}\cup b^{t})}$
    |  | (3) |'
- en: 'where $\cap$ and $\cup$ denote intersection and union, respectively. Then,
    for each positive prediction $(c_{n}^{p},b_{n}^{p})$, we assign the GT object
    $(\hat{c}_{n}^{t},\hat{b}_{n}^{t})$ with the highest RIoU overlap with it to itself.
    Note that a single annotated object may be assigned to multiple predictions. Finally,
    the detector is trained by minimizing the objective function, i.e. the multi-task
    loss:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\cap$ 和 $\cup$ 分别表示交集和并集。然后，对于每个正样本预测 $(c_{n}^{p},b_{n}^{p})$，我们将与其具有最高
    RIoU 重叠的 GT 物体 $(\hat{c}_{n}^{t},\hat{b}_{n}^{t})$ 分配给它。请注意，一个标注物体可能会被分配到多个预测中。最后，通过最小化目标函数，即多任务损失来训练检测器：
- en: '|  | $L(\mathcal{T},\mathcal{P})=\frac{1}{N_{pos}}\sum_{n=1}^{N_{pos}}obj_{n}\cdot
    L_{reg}(b_{n}^{p},\hat{b}_{n}^{t})+\frac{\lambda}{N_{p}}\sum_{n=1}^{N_{p}}L_{cls}(c_{n}^{p},\hat{c}_{n}^{t})$
    |  | (4) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $L(\mathcal{T},\mathcal{P})=\frac{1}{N_{pos}}\sum_{n=1}^{N_{pos}}obj_{n}\cdot
    L_{reg}(b_{n}^{p},\hat{b}_{n}^{t})+\frac{\lambda}{N_{p}}\sum_{n=1}^{N_{p}}L_{cls}(c_{n}^{p},\hat{c}_{n}^{t})$
    |  | (4) |'
- en: where $(\hat{c}_{n}^{t},\hat{b}_{n}^{t})\in\mathcal{T}$ is an annotated object
    associated with the prediction$(c_{n}^{p},b_{n}^{p})\in\mathcal{P}$. $obj_{n}$
    is 1 and 0 for positive and negative prediction respectively. $N_{pos}$ and $N_{p}$
    are the numbers of positive predictions and predicted results, respectively. $L_{reg}$
    and $L_{cls}$ denote regression loss and classification loss, respectively. The
    term $obj_{n}\cdot L_{reg}(b_{n}^{p},\hat{b}_{n}^{t})$ indicates that the regression
    loss is activated only for positive predictions and is disabled otherwise. A balancing
    parameter $\lambda$ controls the trade-off between classification and regression.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(\hat{c}_{n}^{t},\hat{b}_{n}^{t})\in\mathcal{T}$ 是与预测 $(c_{n}^{p},b_{n}^{p})\in\mathcal{P}$
    相关联的标注对象。$obj_{n}$ 对于正预测和负预测分别为 1 和 0。$N_{pos}$ 和 $N_{p}$ 分别表示正预测和预测结果的数量。$L_{reg}$
    和 $L_{cls}$ 分别表示回归损失和分类损失。术语 $obj_{n}\cdot L_{reg}(b_{n}^{p},\hat{b}_{n}^{t})$
    表示回归损失仅对正预测激活，其他情况下禁用。一个平衡参数 $\lambda$ 控制分类和回归之间的权衡。
- en: 'The main purpose of the loss functions is to quantify the difference between
    the predictions and the ground truth and to guide the training process of the
    detector. Therefore, the loss functions significantly impact the detection performance.
    As an extension of generic horizontal object detection, oriented object detection
    also adopts the cross-entropy loss (de Boer et al., [2005](#bib.bib5)) or focal
    loss (Lin et al., [2020](#bib.bib89)) in the classification task. On the other
    hand, although widely used in generic horizontal object detection, the smooth
    $L_{1}$ loss (Girshick, [2015](#bib.bib40)) is not suitable for oriented object
    detection, which needs to predict the object orientation. More details about loss
    functions will be discussed in Section [5](#S5 "5 OBB Representations ‣ Oriented
    Object Detection in Optical Remote Sensing Images using Deep Learning: A Survey").'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '损失函数的主要目的是量化预测与实际值之间的差异，并指导检测器的训练过程。因此，损失函数对检测性能有显著影响。作为通用水平目标检测的扩展，定向目标检测也在分类任务中采用了交叉熵损失（de
    Boer et al., [2005](#bib.bib5)）或焦点损失（Lin et al., [2020](#bib.bib89)）。另一方面，尽管在通用水平目标检测中被广泛使用，但平滑
    $L_{1}$ 损失（Girshick, [2015](#bib.bib40)）不适用于定向目标检测，因为它需要预测物体的方向。有关损失函数的更多细节将在第
    [5](#S5 "5 OBB Representations ‣ Oriented Object Detection in Optical Remote Sensing
    Images using Deep Learning: A Survey") 节中讨论。'
- en: 3 Datasets and Performance Evaluation
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数据集和性能评估
- en: 3.1 Datasets
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集
- en: As a data-driven technology, deep learning is inseparable from various datasets.
    Throughout the development of object detection based on deep learning, datasets
    have played an indispensable role not only in training models, but also served
    as common benchmarks to evaluate and verify model performance (Liu et al., [2020](#bib.bib94)).
    With the help of challenging datasets, object detection is advanced towards increasingly
    complex and tough scenarios. In horizontal object detection, a large number of
    datasets, including PASCAL VOC (Everingham et al., [2010](#bib.bib34)), ImageNet (Russakovsky
    et al., [2015](#bib.bib120)), Microsoft COCO (Lin et al., [2014](#bib.bib90)),
    and Open Images (Kuznetsova et al., [2020](#bib.bib75)), have emerged and pushed
    deep learning based methods to achieve tremendous successes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种数据驱动的技术，深度学习与各种数据集密不可分。在基于深度学习的目标检测的发展过程中，数据集不仅在模型训练中发挥了不可或缺的作用，还作为常见的基准来评估和验证模型性能（Liu
    et al., [2020](#bib.bib94)）。在具有挑战性的数据集的帮助下，目标检测不断向日益复杂和困难的场景发展。在水平目标检测中，出现了大量的数据集，包括
    PASCAL VOC（Everingham et al., [2010](#bib.bib34)）、ImageNet（Russakovsky et al.,
    [2015](#bib.bib120)）、Microsoft COCO（Lin et al., [2014](#bib.bib90)）和 Open Images（Kuznetsova
    et al., [2020](#bib.bib75)），这些数据集推动了基于深度学习的方法取得了巨大的成功。
- en: 'With the rapid development of Earth observation technologies, a vast number
    of high-quality RS images can be easily obtained to build large-scale datasets
    to study deep learning based algorithms in RS object detection. Recently, several
    research groups have released their public RS image datasets. Datasets annotated
    only with HBBs are not covered here, including DIOR (Li et al., [2020](#bib.bib83)),
    LEVIR (Zou and Shi, [2018](#bib.bib214)), NWPU VHR-10 (Cheng et al., [2014](#bib.bib17)),
    RSOD (Xiao et al., [2015](#bib.bib163); Long et al., [2017](#bib.bib102)), xView (Lam
    et al., [2018](#bib.bib76)), and HRRSD (Zhang et al., [2019](#bib.bib196)). In
    this subsection, we only focus on introducing RS image datasets annotated with
    OBBs, including SZTAKI-INRIA (Benedek et al., [2012](#bib.bib2)), 3K vehicle (Liu
    and Mattyus, [2015](#bib.bib93)), UCAS-AOD (Zhu et al., [2015](#bib.bib208)),
    VEDAI (Razakarivony and Jurie, [2016](#bib.bib115)), HRSC2016 (Liu et al., [2016b](#bib.bib101)),
    DOTA (Xia et al., [2018](#bib.bib161); Ding et al., [2022](#bib.bib30)), ShipRSImageNet (Zhang
    et al., [2021b](#bib.bib198)), and DIOR-R (Cheng et al., [2022a](#bib.bib18))].
    We introduce the above datasets in detail as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 随着地球观测技术的快速发展，大量高质量的遥感图像可以轻松获取，以构建大规模数据集来研究基于深度学习的遥感目标检测算法。近年来，多个研究小组已发布了他们的公共遥感图像数据集。这里不包括仅用HBBs标注的数据集，如DIOR （Li
    等，[2020](#bib.bib83)）、LEVIR （Zou 和 Shi，[2018](#bib.bib214)）、NWPU VHR-10 （Cheng
    等，[2014](#bib.bib17)）、RSOD （Xiao 等，[2015](#bib.bib163); Long 等，[2017](#bib.bib102)）、xView （Lam
    等，[2018](#bib.bib76)）和HRRSD （Zhang 等，[2019](#bib.bib196)）。在本小节中，我们仅重点介绍用OBBs标注的遥感图像数据集，包括SZTAKI-INRIA （Benedek
    等，[2012](#bib.bib2)）、3K vehicle （Liu 和 Mattyus，[2015](#bib.bib93)）、UCAS-AOD （Zhu
    等，[2015](#bib.bib208)）、VEDAI （Razakarivony 和 Jurie，[2016](#bib.bib115)）、HRSC2016 （Liu
    等，[2016b](#bib.bib101)）、DOTA （Xia 等，[2018](#bib.bib161); Ding 等，[2022](#bib.bib30)）、ShipRSImageNet （Zhang
    等，[2021b](#bib.bib198)）和DIOR-R （Cheng 等，[2022a](#bib.bib18)）。我们将详细介绍上述数据集：
- en: SZTAKI-INRIA (Benedek et al., [2012](#bib.bib2)) contains 665 buildings in 9
    multi-sensor aerial or satellite images taken from different cities. Due to the
    small capacity, this dataset is used to evaluate traditional object detection
    algorithms.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: SZTAKI-INRIA （Benedek 等，[2012](#bib.bib2)）包含了9张来自不同城市的665栋建筑的多传感器航拍或卫星图像。由于数据量较小，该数据集用于评估传统的目标检测算法。
- en: 3K vehicle (Liu and Mattyus, [2015](#bib.bib93)) is created for vehicle detection,
    comprising 20 images and 14,235 vehicles. The images have a resolution of 5616
    $\times$ 3744 and are captured by a DLR camera system at a height of 1,000m above
    the ground. Therefore, the ground sample distance (GSD) is approximately 13 cm,
    leading to smaller scale variations. Besides, the images have a similar background.
    Hence, this dataset is excluded from the evaluation of algorithms on complicated
    scenes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 3K vehicle （Liu 和 Mattyus，[2015](#bib.bib93)）为车辆检测而创建，包括20张图像和14,235辆车辆。这些图像的分辨率为5616
    $\times$ 3744，由DLR相机系统在距地面1,000米的高度拍摄。因此，地面采样距离（GSD）约为13厘米，导致规模变化较小。此外，这些图像背景类似。因此，该数据集被排除在复杂场景算法的评估之外。
- en: VEDAI (Razakarivony and Jurie, [2016](#bib.bib115)) is also proposed for vehicle
    detection, containing more categories and a wider variety of backgrounds, e.g.
    fields, grass, mountains, urban area, etc, making the detection more complicated.
    It comprises 1,210 images with a resolution of $1,024\times 1,024$. The images
    are cropped from Very-High-Resolution (VHR) satellite images with a GSD of 12.5cm.
    However, the dataset only consists of 3,640 instances, because the images with
    too many dense vehicles are excluded. It is worth mentioning that each image has
    four color channels, including three visible channels and one 8-bit near-infrared
    channel.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: VEDAI （Razakarivony 和 Jurie，[2016](#bib.bib115)）同样用于车辆检测，包含更多类别和更广泛的背景，例如田野、草地、山脉、城市区域等，使得检测更加复杂。它包含1,210张分辨率为
    $1,024\times 1,024$ 的图像。这些图像是从分辨率为12.5厘米的非常高分辨率（VHR）卫星图像中裁剪而来的。然而，该数据集仅包含3,640个实例，因为图像中过于密集的车辆被排除。值得一提的是，每张图像都有四个颜色通道，包括三个可见通道和一个8位近红外通道。
- en: UCAS-AOD (Zhu et al., [2015](#bib.bib208)) contains 7,482 planes in 1,000 images,
    7,114 cars in 510 images, and 910 negative images. All images in this dataset
    are cropped from Google Earth aerial images. Especially, the instances are carefully
    selected to ensure their orientations are distributed evenly.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: UCAS-AOD （Zhu 等，[2015](#bib.bib208)）包含7,482架飞机的1,000张图像、7,114辆汽车的510张图像，以及910张负样本图像。该数据集中的所有图像均为从Google
    Earth航拍图像中裁剪而来。特别地，这些实例经过仔细选择，以确保其方向分布均匀。
- en: HRSC2016 (Liu et al., [2016b](#bib.bib101)) is a widely-used dataset in ship
    detection. It contains 1,070 images and 2,976 instances collected from Google
    Earth. The image resolutions range from 300 $\times$ 300 to 1,500 $\times$ 900.
    Furthermore, this dataset contains more than 25 categories of ships with large
    varieties of scales, orientations, appearances, shapes, and backgrounds (e.g.
    sea, port). Currently, it is one of the most popular datasets for evaluating algorithms
    of oriented object detection.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: HRSC2016 (Liu et al., [2016b](#bib.bib101)) 是一个广泛使用的船舶检测数据集。它包含 1,070 张图像和 2,976
    个实例，这些数据来自 Google Earth。图像分辨率范围从 300 $\times$ 300 到 1,500 $\times$ 900。此外，该数据集包含超过
    25 类船舶，具有大范围的尺度、方向、外观、形状和背景（例如海洋、港口）。目前，它是评估面向对象检测算法的最受欢迎的数据集之一。
- en: '![Refer to caption](img/f3079f3318e70823bdeef75b76b87110.png)![Refer to caption](img/089c5d85333c3acbd60000cd98764c80.png)![Refer
    to caption](img/1869ad1d106f886b2946fb34262396a2.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f3079f3318e70823bdeef75b76b87110.png)![参见说明](img/089c5d85333c3acbd60000cd98764c80.png)![参见说明](img/1869ad1d106f886b2946fb34262396a2.png)'
- en: (a) Oblique view
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 斜视图
- en: '![Refer to caption](img/9a4752ab18c56f23b727ef6eada2beee.png)![Refer to caption](img/d57ffdca8a44dd40e93d25ca02d2fdaf.png)![Refer
    to caption](img/ceec8beac3ee5e93215163149c25f665.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9a4752ab18c56f23b727ef6eada2beee.png)![参见说明](img/d57ffdca8a44dd40e93d25ca02d2fdaf.png)![参见说明](img/ceec8beac3ee5e93215163149c25f665.png)'
- en: (b) Low foreground ratio
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 低前景比
- en: 'Figure 4: Characteristics of DOTA-V2.0 (Ding et al., [2022](#bib.bib30)).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：DOTA-V2.0 的特征（Ding et al., [2022](#bib.bib30)）。
- en: '![Refer to caption](img/a58e08aa782a782ab7b5331b7a483482.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a58e08aa782a782ab7b5331b7a483482.png)'
- en: 'Figure 5: Number of instances for each category in training and validation
    subsets of DOTA-V1.0, V1.5 and V2.0 (Xia et al., [2018](#bib.bib161); Ding et al.,
    [2022](#bib.bib30))'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：DOTA-V1.0、V1.5 和 V2.0 训练和验证子集中每个类别的实例数量（Xia et al., [2018](#bib.bib161);
    Ding et al., [2022](#bib.bib30)）
- en: '![Refer to caption](img/5e9b410444dd659f4c99ad647e748bb9.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5e9b410444dd659f4c99ad647e748bb9.png)'
- en: (a) Size distributions per category
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 每个类别的尺寸分布
- en: '![Refer to caption](img/976434d3dfc26809240562a3736a1d8f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/976434d3dfc26809240562a3736a1d8f.png)'
- en: (b) Ratio distributions per category
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 每个类别的比例分布
- en: 'Figure 6: Size and ratio distributions for each category in training and validation
    subsets of DOTA-V1.0, V1.5 and V2.0 (Xia et al., [2018](#bib.bib161); Ding et al.,
    [2022](#bib.bib30))'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：DOTA-V1.0、V1.5 和 V2.0 训练和验证子集中每个类别的尺寸和比例分布（Xia et al., [2018](#bib.bib161);
    Ding et al., [2022](#bib.bib30)）
- en: DOTA (Xia et al., [2018](#bib.bib161); Ding et al., [2022](#bib.bib30)) contains
    large quantities of objects with a considerable variety of orientations, scales,
    and appearances. The images are selected from different sensors and platforms,
    including Google Earth, GF-2 Satellite, and UAVs. The size of images ranges from
    $800\times 800$ to $20,000\times 20,000$ pixels. What’s more, there are three
    versions of this dataset.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: DOTA (Xia et al., [2018](#bib.bib161); Ding et al., [2022](#bib.bib30)) 包含大量具有相当多样的方向、尺度和外观的对象。这些图像选自不同的传感器和平台，包括
    Google Earth、GF-2 卫星和无人机。图像的尺寸范围从 $800\times 800$ 到 $20,000\times 20,000$ 像素。此外，该数据集有三个版本。
- en: 'Table 1: Comparison of the three versions of DOTA. The number of images and
    instances of each split subset is counted.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：DOTA 三个版本的比较。统计每个拆分子集的图像数量和实例数量。
- en: '|  | V1.0 | V1.5 | V2.0 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | V1.0 | V1.5 | V2.0 |'
- en: '| Images | Training | 1,411 | 1,830 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 图像 | 训练 | 1,411 | 1,830 |'
- en: '| Validation | 458 | 593 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 验证 | 458 | 593 |'
- en: '| Test/Test-dev | 937 | 2,792 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 测试/测试开发 | 937 | 2,792 |'
- en: '| Test-challenge | - | 6,053 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 测试挑战 | - | 6,053 |'
- en: '| Total | 2,806 | 11,268 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 2,806 | 11,268 |'
- en: '| Instances | Training | 98,990 | 210,631 | 268,627 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 实例 | 训练 | 98,990 | 210,631 | 268,627 |'
- en: '| Validation | 28,853 | 69,565 | 81,048 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 验证 | 28,853 | 69,565 | 81,048 |'
- en: '| Test/Test-dev | 60,439 | 121,893 | 353,346 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 测试/测试开发 | 60,439 | 121,893 | 353,346 |'
- en: '| Test-challenge | - | - | 1,090,637 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 测试挑战 | - | - | 1,090,637 |'
- en: '| Total | 188,282 | 403,318 | 1,793,658 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 188,282 | 403,318 | 1,793,658 |'
- en: 'The number of images and instances in three versions of DOTA are summarized
    in Table [1](#S3.T1 "Table 1 ‣ 3.1 Datasets ‣ 3 Datasets and Performance Evaluation
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey"). DOTA-V1.0 (Xia et al., [2018](#bib.bib161)) and DOTA-V1.5 share the
    same images, which are split into training, validation and test subsets. As an
    extension of DOTA-V1.0, DOTA-V1.5 annotates extremely small instances whose sizes
    are equal to or less than 10 pixels. Besides, DOTA-V1.5 extends a new category,
    namely container crane. Thus, the number of instances increased from 188,282 to
    403,318. Compared with the previous versions, DOTA-V2.0 (Ding et al., [2022](#bib.bib30))
    contains more images collected from Google Earth, GF-2 Satellites, and aerial
    platforms. In addition, a large number of images are taken under an oblique view
    and a lower foreground ratio to approach the real-world application scenes, as
    shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.1 Datasets ‣ 3 Datasets and Performance
    Evaluation ‣ Oriented Object Detection in Optical Remote Sensing Images using
    Deep Learning: A Survey"). It further adds two new categories, including airport
    and helipad. The number of instances is increased to about 1.8 million. Moreover,
    it contains two test subsets, namely test-dev and test-challenge. The latter comprises
    a greater number of object instances (around 1.1 million) and more complicated
    scenes, making the task more challenging.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '三个版本的 DOTA 中图像和实例的数量汇总在表格 [1](#S3.T1 "Table 1 ‣ 3.1 Datasets ‣ 3 Datasets and
    Performance Evaluation ‣ Oriented Object Detection in Optical Remote Sensing Images
    using Deep Learning: A Survey") 中。DOTA-V1.0（Xia et al., [2018](#bib.bib161)）和
    DOTA-V1.5 使用相同的图像，这些图像被划分为训练集、验证集和测试集。作为 DOTA-V1.0 的扩展，DOTA-V1.5 注释了极小的实例，其大小等于或小于
    10 像素。此外，DOTA-V1.5 扩展了一个新类别，即集装箱起重机。因此，实例数量从 188,282 增加到 403,318。与以前的版本相比，DOTA-V2.0（Ding
    et al., [2022](#bib.bib30)）包含了更多从 Google Earth、GF-2 卫星和空中平台收集的图像。此外，大量图像是在倾斜视角和较低前景比下拍摄的，以接近实际应用场景，如图
    [4](#S3.F4 "Figure 4 ‣ 3.1 Datasets ‣ 3 Datasets and Performance Evaluation ‣
    Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey") 所示。它进一步增加了两个新类别，包括机场和直升机停机坪。实例数量增加到约 180 万。此外，它包含两个测试子集，即 test-dev
    和 test-challenge。后者包括更多的目标实例（约 110 万）和更复杂的场景，使得任务更加具有挑战性。'
- en: 'Figure [5](#S3.F5 "Figure 5 ‣ 3.1 Datasets ‣ 3 Datasets and Performance Evaluation
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey") shows the number of instances for each category in training and validation
    subsets of DOTA-V1.0, V1.5 and V2.0. Note that the distributions of different
    categories are severely imbalanced. The instances of small-vehicle and ship have
    a large quantity, while nearly half of the other categories have quantities of
    less than 1,000, including plane, baseball diamond, ground track field, basketball
    court, soccer ball field, roundabout, helicopter, container crane, airport and
    helipad. The severe category imbalance makes the model seriously overfitting to
    the many-shot categories but underfitting to the low-shot categories (Gupta et al.,
    [2019](#bib.bib45); Cui et al., [2019](#bib.bib23); Wang et al., [2021b](#bib.bib150)).
    Figure [6](#S3.F6 "Figure 6 ‣ 3.1 Datasets ‣ 3 Datasets and Performance Evaluation
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey") further summarizes the size and ratio distributions for each category
    in three versions of DOTA, respectively. As shown in Figure [6(a)](#S3.F6.sf1
    "In Figure 6 ‣ 3.1 Datasets ‣ 3 Datasets and Performance Evaluation ‣ Oriented
    Object Detection in Optical Remote Sensing Images using Deep Learning: A Survey"),
    the minimum size is $3-4$ orders of magnitude lower than the maximum size in each
    category. Moreover, there is also a large range of size differences between categories.
    Both the inter- and intra-class size variations make the detection tasks more
    challenging. Figure [6(b)](#S3.F6.sf2 "In Figure 6 ‣ 3.1 Datasets ‣ 3 Datasets
    and Performance Evaluation ‣ Oriented Object Detection in Optical Remote Sensing
    Images using Deep Learning: A Survey") indicates that the aspect ratios of different
    categories vary greatly. Furthermore, some categories have an extremely large
    aspect ratio, such as bridge, harbor and airport. Up to now, DOTA is the most
    challenging dataset for oriented object detection, due to its tremendous object
    instances, large aspect ratio, significant size variance, and complicated aerial
    scenes.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [5](#S3.F5 "Figure 5 ‣ 3.1 Datasets ‣ 3 Datasets and Performance Evaluation
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey") 显示了 DOTA-V1.0、V1.5 和 V2.0 中训练和验证子集的每个类别的实例数量。需要注意的是，不同类别的分布严重不平衡。小型车辆和船舶的实例数量较多，而其他几乎一半的类别数量少于
    1,000，包括飞机、棒球场、跑道、篮球场、足球场、环形交叉路口、直升机、集装箱起重机、机场和直升机停机坪。严重的类别不平衡使得模型在许多实例类别上严重过拟合，而在少量实例类别上则表现不佳
    (Gupta et al., [2019](#bib.bib45); Cui et al., [2019](#bib.bib23); Wang et al.,
    [2021b](#bib.bib150))。图 [6](#S3.F6 "Figure 6 ‣ 3.1 Datasets ‣ 3 Datasets and Performance
    Evaluation ‣ Oriented Object Detection in Optical Remote Sensing Images using
    Deep Learning: A Survey") 进一步总结了 DOTA 三个版本中每个类别的大小和比例分布。如图 [6(a)](#S3.F6.sf1 "In
    Figure 6 ‣ 3.1 Datasets ‣ 3 Datasets and Performance Evaluation ‣ Oriented Object
    Detection in Optical Remote Sensing Images using Deep Learning: A Survey") 所示，每个类别的最小尺寸比最大尺寸小
    $3-4$ 个数量级。此外，类别之间的尺寸差异范围也很大。类内和类间的尺寸变化使得检测任务更加具有挑战性。图 [6(b)](#S3.F6.sf2 "In Figure
    6 ‣ 3.1 Datasets ‣ 3 Datasets and Performance Evaluation ‣ Oriented Object Detection
    in Optical Remote Sensing Images using Deep Learning: A Survey") 表明，不同类别的纵横比差异很大。此外，一些类别具有极大的纵横比，例如桥梁、港口和机场。至今，DOTA
    是最具挑战性的定向目标检测数据集，因其巨大的目标实例、较大的纵横比、显著的尺寸变化和复杂的空中场景。'
- en: FGSD (Chen et al., [2020](#bib.bib12)) is a new fine-grained ship detection
    dataset expanded based on HRSC2016. This dataset contains 2,612 RS images and
    5,634 ship instances from 17 large ports around the world. The instances are classified
    into 43 categories which are further divided into 4 high-level categories, including
    submarine, aircraft carrier, civil ship, and warship. Except for ships, a new
    category named dock is also annotated in this dataset for future research.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: FGSD (Chen et al., [2020](#bib.bib12)) 是一个基于 HRSC2016 扩展的新型细粒度船舶检测数据集。该数据集包含
    2,612 张遥感图像和来自全球 17 个大型港口的 5,634 个船舶实例。这些实例被分为 43 类，进一步划分为 4 个高级类别，包括潜艇、航母、民用船舶和军舰。除了船舶之外，该数据集还标注了一个名为“码头”的新类别，以便于未来的研究。
- en: ShipRSImageNet (Zhang et al., [2021b](#bib.bib198)) is the largest RS dataset
    for ship detection. It contains 3,435 images collected from xView (Lam et al.,
    [2018](#bib.bib76)), HRSC2016 (Liu et al., [2016b](#bib.bib101)), FGSD (Chen et al.,
    [2020](#bib.bib12)), Airbus Ship Detection Challenge, and Chinese satellites.
    Most of the images are sliced into about $930\times 930$. A total number of 17,573
    ships are divided into 50 categories. There are diverse spatial resolutions, scales,
    aspect ratios, backgrounds, and orientations in this dataset.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ShipRSImageNet (Zhang 等, [2021b](#bib.bib198)) 是用于船舶检测的最大 RS 数据集。它包含了从 xView
    (Lam 等, [2018](#bib.bib76))、HRSC2016 (Liu 等, [2016b](#bib.bib101))、FGSD (Chen
    等, [2020](#bib.bib12))、Airbus Ship Detection Challenge 和中国卫星中收集的 3,435 张图片。大多数图像被切割成大约
    $930\times 930$。总共有 17,573 艘船分为 50 类。在此数据集中，存在多样的空间分辨率、尺度、长宽比、背景和方向。
- en: DIOR-R (Cheng et al., [2022a](#bib.bib18)) contains 192,518 instances and 23,463
    images. The images are the same as the ones in DIOR (Li et al., [2020](#bib.bib83)),
    while the instances are annotated in the format of OBB. The GSD ranges from 0.5m
    to 30m. There are 20 common categories in DIOR-R, including airplane, airport,
    baseball field, basketball court, bridge, chimney, expressway service area, expressway
    toll station, dam, golf field, ground track field, harbor, overpass, ship, stadium,
    storage tank, tennis court, train station, vehicle, and windmill.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DIOR-R (Cheng 等, [2022a](#bib.bib18)) 包含 192,518 个实例和 23,463 张图像。这些图像与 DIOR
    (Li 等, [2020](#bib.bib83)) 中的图像相同，而实例则以 OBB 格式注释。GSD 范围从 0.5m 到 30m。DIOR-R 中有
    20 个常见类别，包括飞机、机场、棒球场、篮球场、桥梁、烟囱、高速公路服务区、高速公路收费站、水坝、高尔夫球场、田径场、港口、立交桥、船舶、体育场、储罐、网球场、火车站、车辆和风车。
- en: 'Table 2: Comparison of public RS image datasets.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：公共 RS 图像数据集的比较。
- en: '| Dataset | Category | Quantity | Instance | GSD | Resolution |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类别 | 数量 | 实例 | GSD | 分辨率 |'
- en: '| SZTAKI-INRIA (Benedek et al., [2012](#bib.bib2)) | 1 | 9 | 665 | - | $600\times
    500\sim 1400\times 800$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| SZTAKI-INRIA (Benedek 等, [2012](#bib.bib2)) | 1 | 9 | 665 | - | $600\times
    500\sim 1400\times 800$ |'
- en: '| 3K vehicle (Liu and Mattyus, [2015](#bib.bib93)) | 1 | 20 | 14235 | 0.13m
    | $5516\times 3744$ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 3K vehicle (Liu 和 Mattyus, [2015](#bib.bib93)) | 1 | 20 | 14235 | 0.13m |
    $5516\times 3744$ |'
- en: '| VEDAI (Razakarivony and Jurie, [2016](#bib.bib115)) | 9 | 1210 | 3640 | 0.125m
    | $1024\times 1024$ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| VEDAI (Razakarivony 和 Jurie, [2016](#bib.bib115)) | 9 | 1210 | 3640 | 0.125m
    | $1024\times 1024$ |'
- en: '| UCAS-AOD (Zhu et al., [2015](#bib.bib208)) | 2 | 2420 | 14596 | - | $1280\times
    659$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| UCAS-AOD (Zhu 等, [2015](#bib.bib208)) | 2 | 2420 | 14596 | - | $1280\times
    659$ |'
- en: '| HRSC2016 (Liu et al., [2016b](#bib.bib101)) | 25 | 1070 | 2976 | 0.4$\sim$2m
    | $300\times 300\sim 1500\times 900$ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| HRSC2016 (Liu 等, [2016b](#bib.bib101)) | 25 | 1070 | 2976 | 0.4$\sim$2m |
    $300\times 300\sim 1500\times 900$ |'
- en: '| DOTA-V1.0 (Xia et al., [2018](#bib.bib161)) | 15 | 2806 | 188282 | 0.1$\sim$4.5m
    | $800\times 800\sim 20000\times 20000$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| DOTA-V1.0 (Xia 等, [2018](#bib.bib161)) | 15 | 2806 | 188282 | 0.1$\sim$4.5m
    | $800\times 800\sim 20000\times 20000$ |'
- en: '| DOTA-V1.5 | 16 | 2806 | 403318 | 0.1$\sim$4.5m | $800\times 800\sim 20000\times
    20000$ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| DOTA-V1.5 | 16 | 2806 | 403318 | 0.1$\sim$4.5m | $800\times 800\sim 20000\times
    20000$ |'
- en: '| DOTA-V2.0 (Ding et al., [2022](#bib.bib30)) | 18 | 11268 | 1793658 | 0.1$\sim$4.5m
    | $800\times 800\sim 29200\times 27620$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| DOTA-V2.0 (Ding 等, [2022](#bib.bib30)) | 18 | 11268 | 1793658 | 0.1$\sim$4.5m
    | $800\times 800\sim 29200\times 27620$ |'
- en: '| FGSD (Chen et al., [2020](#bib.bib12)) | 43 | 5634 | 2612 | 0.12$\sim$1.93m
    | $930\times 930$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| FGSD (Chen 等, [2020](#bib.bib12)) | 43 | 5634 | 2612 | 0.12$\sim$1.93m |
    $930\times 930$ |'
- en: '| ShipRSImageNet (Zhang et al., [2021b](#bib.bib198)) | 50 | 3435 | 17573 |
    0.12$\sim$6m | $930\times 930\sim 1400\times 1000$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| ShipRSImageNet (Zhang 等, [2021b](#bib.bib198)) | 50 | 3435 | 17573 | 0.12$\sim$6m
    | $930\times 930\sim 1400\times 1000$ |'
- en: '| DIOR-R (Cheng et al., [2022a](#bib.bib18)) | 20 | 23463 | 192518 | 0.5$\sim$30m
    | $800\times 800$ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| DIOR-R (Cheng 等, [2022a](#bib.bib18)) | 20 | 23463 | 192518 | 0.5$\sim$30m
    | $800\times 800$ |'
- en: 'Table [2](#S3.T2 "Table 2 ‣ 3.1 Datasets ‣ 3 Datasets and Performance Evaluation
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey") lists the parameters of the above RS datasets for intuitive comparison.
    As an early large-scale dataset with tremendous instances and various categories,
    DOTA-V1.0 (Xia et al., [2018](#bib.bib161)) has been widely used to compare the
    performance of various detectors. Furthermore, as ships usually possess large
    aspect ratios, the early ship dataset HRSC2016 (Liu et al., [2016b](#bib.bib101))
    has also been used to evaluate different detectors.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [2](#S3.T2 "Table 2 ‣ 3.1 Datasets ‣ 3 Datasets and Performance Evaluation
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey") 列出了上述RS数据集的参数，便于直观比较。作为早期的大规模数据集，具有大量实例和多种类别，DOTA-V1.0 (Xia 等，[2018](#bib.bib161))
    已被广泛用于比较各种检测器的性能。此外，由于船舶通常具有较大的纵横比，早期的船舶数据集HRSC2016 (Liu 等，[2016b](#bib.bib101))
    也被用于评估不同的检测器。'
- en: 3.2 Evaluation Protocol
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 评估协议
- en: Accuracy and efficiency are both the most crucial criteria in evaluating the
    performance of oriented object detectors. The evaluation protocol for OBB is different
    from the one for HBB, because IoU is replaced with RIoU. On the other hand, the
    efficiency is evaluated using frame per second (FPS), which means the number of
    image frames processed by detectors per second. Accuracy evaluation takes into
    account both precision and recall. There are two universally-agreed metrics for
    accuracy evaluation, namely average precision (AP) and F-measure.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性和效率都是评估有向目标检测器性能最关键的标准。OBB的评估协议与HBB的协议不同，因为IoU被RIoU取代。另一方面，效率是用每秒帧数（FPS）来评估的，它表示检测器每秒处理的图像帧数。准确性评价同时考虑了精确度和召回率。对于准确性评价，有两个公认的度量标准，即平均精度（AP）和F度量。
- en: 'For the object detection task, the detector outputs a list of predicted results
    ${(b_{j},c_{j},s_{j})}_{j=1}^{M}$, where each item contains an OBB $b_{j}$, a
    category label $c_{j}$, and a confidence score $s_{j}$. $j$ is an index of object
    order, $M$ denotes the number of predicted results. Then, the predicted results
    whose confidence score is greater than a predefined confidence threshold $T_{s}$
    are assigned to GT objects ${(b_{k}^{*},c_{k}^{*})}_{k=1}^{N}$ based on RIoU and
    category, where $b_{k}^{*}$, $c_{k}^{*}$ and the superscript $*$ denotes the OBB,
    category label, and GT respectively. To calculate the precision and recall of
    the detector, the number of true positives (TP) in the predicted results is needed.
    A predicted result $(b,c,s)$ which is assigned a GT object $(b^{*},c^{*})$ is
    judged to be a TP if the following criteria are met:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于目标检测任务，检测器输出预测结果列表${(b_{j},c_{j},s_{j})}_{j=1}^{M}$，其中每个项目包含一个OBB $b_{j}$，一个类别标签
    $c_{j}$ 和一个置信度分数 $s_{j}$。$j$是目标顺序的索引，$M$表示预测结果的数量。然后，预测结果中置信度大于预定义的置信度阈值$T_{s}$的部分被分配给GT对象${(b_{k}^{*},c_{k}^{*})}_{k=1}^{N}$，基于RIoU和类别，其中$b_{k}^{*}$，$c_{k}^{*}$和上标$*$分别表示OBB、类别标签和GT。为了计算检测器的精确度和召回率，需要真阳性（TP）的数量。如果满足以下条件，分配给GT对象$(b^{*},c^{*})$的预测结果$(b,c,s)$被判定为TP：
- en: (1) The predicted label $c$ is equal to the label $c^{*}$ of GT object.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 预测标签$c$等于GT对象的标签$c^{*}”。
- en: (2) The RIoU between the predicted OBB $b$ and the GT OBB $b^{*}$, denoted by
    RIoU $(b,b^{*})$, is not smaller than the predefined RIoU threshold, which is
    denoted by $T_{RIoU}$ and is generally set to 0.5.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 预测OBB $b$和GT OBB $b^{*}$之间的RIoU，用RIoU $(b,b^{*})$表示，不小于预定义的RIoU阈值$T_{RIoU}$，一般设置为0.5。
- en: Otherwise, it is regarded as a false positive (FP).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，它被视为假阳性（FP）。
- en: 'Precision is the proportion of correctly predicted instances among the total
    predicted results, while recall is the proportion of all positive instances predicted
    by the detector among the total GT objects. The formulas are defined as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度是指在所有预测结果中正确预测的实例占总预测结果的比例，而召回率是指在所有GT对象中被检测器预测的所有正例实例占总GT对象的比例。其公式定义如下：
- en: '|  | $Prec(T_{s})=\frac{N_{TP}}{N_{TP}+N_{FP}}$ |  | (5) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $Prec(T_{s})=\frac{N_{TP}}{N_{TP}+N_{FP}}$ |  | (5) |'
- en: '|  | $Rec(T_{s})=\frac{N_{TP}}{N_{TP}+N_{FN}}=\frac{N_{TP}}{N}$ |  | (6) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | $Rec(T_{s})=\frac{N_{TP}}{N_{TP}+N_{FN}}=\frac{N_{TP}}{N}$ |  | (6) |'
- en: where $N_{TP}$, $N_{FP}$, and $N_{FN}$ denote the number of TP, FP, and false
    negative (FN), respectively, which are determined by $T_{s}$ and $T_{RIoU}$. Note
    that the precision and the recall are functions of the confidence threshold $T_{s}$
    with a fixed $T_{RIoU}$.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N_{TP}$、$N_{FP}$ 和 $N_{FN}$ 分别表示 TP、FP 和假阴性（FN）的数量，这些数量由 $T_{s}$ 和 $T_{RIoU}$
    决定。请注意，精度和召回率是置信度阈值 $T_{s}$ 的函数，且 $T_{RIoU}$ 固定。
- en: 'What’s more, neither precision nor recall can evaluate the accuracy of a detector
    independently, while the F-measure is a single measure that combines precision
    and recall using weighted harmonic mean:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，精度和召回率都不能独立地评估检测器的准确性，而 F-measure 是一个结合精度和召回率的单一度量，使用加权调和平均值：
- en: '|  | $F_{\alpha}=\frac{(1+\alpha^{2})Prec(T_{s})Rec(T_{s})}{\alpha^{2}Prec(T_{s})+Rec(T_{s})}$
    |  | (7) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{\alpha}=\frac{(1+\alpha^{2})Prec(T_{s})Rec(T_{s})}{\alpha^{2}Prec(T_{s})+Rec(T_{s})}$
    |  | (7) |'
- en: 'where $\alpha\in\mathbb{R}^{+}$ is a weighting parameter. The value of $\alpha$
    is generally set to 1 to balance the importance of precision and recall. In recent
    works, AP is the most frequently used metric for accuracy evaluation, which is
    usually computed for each category separately. For each category, AP is derived
    using the precision $Prec(T_{s})$ and the recall $Rec(T_{s})$. Specifically, by
    varying the confidence threshold $T_{s}$ from 1.0 to 0.0 gradually, the recall
    increases as $N_{TP}$ increases and a list of pairs $(Prec,Rec)$ can be obtained.
    This allows precision to be considered as a discrete function of recall. The discrete
    function is well-known as the precision-recall curve (PRC), denoted by $P(R)$.
    The AP value is obtained by computing the average value of precision $P(R)$ over
    the interval from $R=0.0$ to $R=1.0$:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha\in\mathbb{R}^{+}$ 是一个加权参数。$\alpha$ 的值通常设置为 1，以平衡精度和召回率的重要性。在最近的研究中，AP
    是最常用的准确性评估指标，通常是针对每个类别单独计算的。对于每个类别，AP 是通过精度 $Prec(T_{s})$ 和召回率 $Rec(T_{s})$ 得出的。具体来说，通过将置信度阈值
    $T_{s}$ 从 1.0 逐渐降低到 0.0，召回率随着 $N_{TP}$ 的增加而增加，可以获得一系列的 $(Prec,Rec)$ 对。这使得精度可以被视为召回率的离散函数。这个离散函数被称为精度-召回曲线（PRC），记作
    $P(R)$。AP 值是通过计算从 $R=0.0$ 到 $R=1.0$ 区间内精度 $P(R)$ 的平均值得到的：
- en: '|  | $AP=\frac{1}{N}\sum_{n=0}^{Rec(0)}\max_{R\geq\frac{n}{N}}P(R)$ |  | (8)
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $AP=\frac{1}{N}\sum_{n=0}^{Rec(0)}\max_{R\geq\frac{n}{N}}P(R)$ |  | (8)
    |'
- en: Therefore, AP also can be considered as the area under PRC. Finally, to evaluate
    the overall accuracy of all categories, the mean AP (mAP) averaged over all categories
    is adopted as the final metric of evaluation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，AP 也可以视为 PRC 下的面积。最后，为了评估所有类别的整体准确性，采用了所有类别的平均 AP（mAP）作为最终的评估指标。
- en: 4 Detection Frameworks
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 检测框架
- en: There are numerous oriented object detection methods built on generic horizontal
    object detection methods. Consequently, the deep learning models of mainstream
    oriented object detection can also be roughly divided into anchor-based and anchor-free
    methods.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 许多定向目标检测方法是建立在通用水平目标检测方法之上的。因此，主流的定向目标检测的深度学习模型也可以大致分为基于锚点的方法和无锚点的方法。
- en: '![Refer to caption](img/9da3b45550201742192763531637f9c4.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/9da3b45550201742192763531637f9c4.png)'
- en: 'Figure 7: The basic architecture of two-stage oriented detectors (Ren et al.,
    [2017](#bib.bib118); Ding et al., [2019](#bib.bib29)).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：双阶段定向检测器的基本架构（Ren et al., [2017](#bib.bib118); Ding et al., [2019](#bib.bib29)）。
- en: 4.1 Anchor-based
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基于锚点的
- en: 'Anchor-based methods localize objects via a regression mode, which can further
    be divided into two-stage (or multi-stage) and one-stage detection frameworks.
    In the pipeline of a two-stage detector, a sparse set of category-independent
    region proposals, that can potentially contain objects, are generated in the first
    stage (Chavali et al., [2016](#bib.bib10); Hosang et al., [2016](#bib.bib64)).
    In the second stage, the features in the region of interest (RoI) are extracted
    from the feature maps for each proposal, and then are used for classification
    and refined regression. Finally, post-processing operation, such as NMS, is adopted
    to output detection results. In contrast, there is no region proposal generation
    in a one-stage detector, which directly locates and classifies objects using DCNNs.
    Therefore, one-stage detectors have a simpler pipeline. Nevertheless, the accuracy
    of one-stage detectors is lower than two-stage detectors (Ding et al., [2019](#bib.bib29);
    Xie et al., [2021](#bib.bib166); Lin et al., [2020](#bib.bib89); Yang et al.,
    [2021b](#bib.bib174)). The primary advantage of one-stage detectors is the fast
    inference speed, which is desired in real-time applications. The properties of
    several two-stage and one-stage detection frameworks are summarized in Table [3](#S4.T3
    "Table 3 ‣ 4.1.1 Two-stage ‣ 4.1 Anchor-based ‣ 4 Detection Frameworks ‣ Oriented
    Object Detection in Optical Remote Sensing Images using Deep Learning: A Survey")
    and Table [4](#S4.T4 "Table 4 ‣ 4.1.2 One-stage ‣ 4.1 Anchor-based ‣ 4 Detection
    Frameworks ‣ Oriented Object Detection in Optical Remote Sensing Images using
    Deep Learning: A Survey") respectively.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '基于锚点的方法通过回归模式定位物体，这些方法可以进一步分为两阶段（或多阶段）和单阶段检测框架。在两阶段检测器的流程中，第一阶段生成一组稀疏的类别无关的区域提议，这些提议可能包含物体（Chavali
    et al., [2016](#bib.bib10); Hosang et al., [2016](#bib.bib64)）。在第二阶段，从每个提议的特征图中提取感兴趣区域（RoI）的特征，然后用于分类和精细回归。最后，采用后处理操作，如NMS，来输出检测结果。相比之下，单阶段检测器没有区域提议生成，直接使用DCNNs定位和分类物体。因此，单阶段检测器的流程更简单。然而，单阶段检测器的准确性低于两阶段检测器（Ding
    et al., [2019](#bib.bib29); Xie et al., [2021](#bib.bib166); Lin et al., [2020](#bib.bib89);
    Yang et al., [2021b](#bib.bib174)）。单阶段检测器的主要优势是快速推理速度，这在实时应用中是需要的。几种两阶段和单阶段检测框架的属性总结见表[3](#S4.T3
    "Table 3 ‣ 4.1.1 Two-stage ‣ 4.1 Anchor-based ‣ 4 Detection Frameworks ‣ Oriented
    Object Detection in Optical Remote Sensing Images using Deep Learning: A Survey")和表[4](#S4.T4
    "Table 4 ‣ 4.1.2 One-stage ‣ 4.1 Anchor-based ‣ 4 Detection Frameworks ‣ Oriented
    Object Detection in Optical Remote Sensing Images using Deep Learning: A Survey")。'
- en: 4.1.1 Two-stage
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 两阶段
- en: 'Table 3: Summary of properties of typical two-stage detection frameworks'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：典型两阶段检测框架的属性总结
- en: '| Method | Baseline | backbone | mAP¹ | Highlights |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 基线 | 骨干网络 | mAP¹ | 亮点 |'
- en: '| Rotated Faster RCNN | Faster RCNN | R-101² | 54.13 | A classical two-stage
    framework and a typical baseline for most of two-stage rotated detectors. |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 旋转Faster RCNN | Faster RCNN | R-101² | 54.13 | 一个经典的两阶段框架和大多数两阶段旋转检测器的典型基线。'
- en: '| (Ren et al., [2017](#bib.bib118)) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| (Ren et al., [2017](#bib.bib118)) |'
- en: '| RRPN | Rotated Faster RCNN | R-101 | 61.01 | Use rotated anchors to generate
    rotated proposals. |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| RRPN | 旋转Faster RCNN | R-101 | 61.01 | 使用旋转锚点生成旋转建议。 |'
- en: '| (Ma et al., [2018](#bib.bib104)) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| (Ma et al., [2018](#bib.bib104)) |'
- en: '|  | Rotated Faster RCNN | R-101 | 69.56 | Propose an RRoI learner to convert
    HRoIs to RRoIs and an RPS RoI Align to extract spatially rotation-invariant feature
    maps. |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | 旋转Faster RCNN | R-101 | 69.56 | 提出一种RRoI学习器，将HRoIs转换为RRoIs，并使用RPS RoI对齐提取空间旋转不变的特征图。
    |'
- en: '| RoI Transformer |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| RoI Transformer |'
- en: '| (Ding et al., [2019](#bib.bib29)) |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| (Ding et al., [2019](#bib.bib29)) |'
- en: '|  | RoI Transformer | ReR-50 | 80.10 | Use rotation equivariant networks and
    RiRoI Align to extract rotation-invariant feature in both spatial and orientation
    dimensions. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | RoI Transformer | ReR-50 | 80.10 | 使用旋转等变网络和RiRoI对齐在空间和方向维度上提取旋转不变特征。
    |'
- en: '| ReDet |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| ReDet |'
- en: '| (Han et al., [2021a](#bib.bib49)) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| (Han et al., [2021a](#bib.bib49)) |'
- en: '|  | ReDet | ReR-50 | 80.37 | Design a dynamic enhancement anchor network to
    generate more qualified positive samples and enhance the performance of small
    objects. |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | ReDet | ReR-50 | 80.37 | 设计一个动态增强锚点网络，以生成更多合格的正样本并提高小物体的性能。 |'
- en: '| DEA |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| DEA |'
- en: '| (Liang et al., [2022](#bib.bib86)) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| (Liang et al., [2022](#bib.bib86)) |'
- en: '|  | Rotated Faster RCNN | R-50 | 80.87 | Design a lightweight module to generate
    oriented proposals and a midpoint offset representation. Achieve competitive accuracy
    to advanced two-stage detectors and reach approximate efficiency to one-stage
    detectors. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | Rotated Faster RCNN | R-50 | 80.87 | 设计一个轻量级模块来生成定向提议和中点偏移表示。在高级两阶段检测器方面实现有竞争力的准确性，并达到一阶段检测器的近似效率。'
- en: '| Oriented RCNN |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Oriented RCNN |'
- en: '| (Xie et al., [2021](#bib.bib166)) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| (Xie et al., [2021](#bib.bib166)) |'
- en: '| KFIoU | RoI Transformer | Swin-T | 80.93 | Design the KFIoU loss based on
    Kalman filter to achieve the best trend-level alignment with RIoU. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| KFIoU | RoI Transformer | Swin-T | 80.93 | 设计了基于卡尔曼滤波器的 KFIoU 损失，以实现与 RIoU
    的最佳趋势级别对齐。'
- en: '| (Yang et al., [2022](#bib.bib180)) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| (Yang et al., [2022](#bib.bib180)) |'
- en: '|  | Oriented RCNN | ViTAE | 81.24 | Use MAE (He et al., [2022](#bib.bib56))
    to pretrain the plain ViTAE transformer. Employ RVSA to learn adaptive window
    sizes and orientations in a data-driven manner. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | Oriented RCNN | ViTAE | 81.24 | 使用 MAE (He et al., [2022](#bib.bib56))
    预训练普通的 ViTAE transformer。采用 RVSA 以数据驱动的方式学习自适应的窗口大小和方向。'
- en: '| RVSA |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| RVSA |'
- en: '| (Wang et al., [2022](#bib.bib143)) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| (Wang et al., [2022](#bib.bib143)) |'
- en: '1'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: The “mAP” column indicates the mAP on DOTA-V1.0 (Xia et al., [2018](#bib.bib161))
    when the RIoU threshold is set to 0.5\. The results of these methods are the best
    reported results from corresponding papers.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “mAP” 列指示在将 RIoU 阈值设为 0.5 时在 DOTA-V1.0 (Xia et al., [2018](#bib.bib161)) 上的
    mAP。这些方法的结果是相应论文中报告的最佳结果。
- en: '2'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2'
- en: R-101 denotes ResNet-101, likewise for R-50, R-152\. ReR-50 denotes the Rotation
    equivariant ResNet-50 (Han et al., [2021a](#bib.bib49)). Swin-T and ViTAE denote
    the tiny version of Swin Transformer (Liu et al., [2021b](#bib.bib100)) and Vision
    Transformer(Dosovitskiy et al., [2021](#bib.bib31); Xu et al., [2021b](#bib.bib169);
    Zhang et al., [2023](#bib.bib193))(the same below).
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: R-101 表示 ResNet-101，R-50、R-152 同理。ReR-50 表示旋转等变 ResNet-50 (Han et al., [2021a](#bib.bib49))。Swin-T
    和 ViTAE 表示 Swin Transformer 的迷你版本 (Liu et al., [2021b](#bib.bib100)) 和 Vision
    Transformer (Dosovitskiy et al., [2021](#bib.bib31); Xu et al., [2021b](#bib.bib169);
    Zhang et al., [2023](#bib.bib193))(以下同)。
- en: 'Rotated Faster RCNN. Recently, Faster RCNN (Ren et al., [2017](#bib.bib118))
    received considerable attention as a classical two-stage generic horizontal object
    detection framework, showing high accuracy and efficiency. As a result, a variety
    of improvements or extending efforts based on Faster RCNN are proposed, including
    FPN (Lin et al., [2017](#bib.bib88)), Cascade RCNN (Cai and Vasconcelos, [2018](#bib.bib8)),
    Mask RCNN (He et al., [2020b](#bib.bib57)), and DetectoRS (Qiao et al., [2021](#bib.bib114)).
    FPN can extract rich high-level semantic information at different scales via a
    top-down architecture with lateral connections, as well as detect region proposals
    at multi-level feature maps. The combination of Faster RCNN and FPN shows significant
    improvement in multi-scale detection, especially for small objects. Thus, Faster
    RCNN armed with FPN becomes a benchmark in the task for object detection. By adding
    an additional output channel to regress the orientation of each object, its extending
    work, termed Faster RCNN OBB or Rotated Faster RCNN (Ren et al., [2017](#bib.bib118);
    Xia et al., [2018](#bib.bib161)), can easily be employed for oriented object detection
    and also serves as a benchmark. As illustrated in Figure [7](#S4.F7 "Figure 7
    ‣ 4 Detection Frameworks ‣ Oriented Object Detection in Optical Remote Sensing
    Images using Deep Learning: A Survey"), the framework of Rotated Faster RCNN consists
    of the following pipeline:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '旋转 Faster RCNN。最近，Faster RCNN (Ren et al., [2017](#bib.bib118))作为一个经典的两阶段通用水平目标检测框架，显示出了高精度和高效率，受到了广泛关注。因此，基于
    Faster RCNN的各种改进和扩展工作相继提出，包括 FPN (Lin et al., [2017](#bib.bib88))，级联 RCNN (Cai
    and Vasconcelos, [2018](#bib.bib8))，Mask RCNN (He et al., [2020b](#bib.bib57))
    和 DetectoRS (Qiao et al., [2021](#bib.bib114))。FPN 可以通过自顶向下的架构和横向连接在不同尺度上提取丰富的高层语义信息，同时在多级特征图上检测区域提议。Faster
    RCNN和FPN的结合在多尺度检测方面显示出显著改进，特别是对于小物体。因此，Faster RCNN配备FPN成为了目标检测任务的基准。通过为每个物体添加额外的输出通道来回归每个物体的方向，其扩展工作，称为
    Faster RCNN OBB 或 Rotated Faster RCNN (Ren et al., [2017](#bib.bib118); Xia et al.,
    [2018](#bib.bib161))，可以轻松用于定向目标检测，并且也可以作为基准。如图 [7](#S4.F7 "图7 ‣ 4 Detection Frameworks
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey") 所示，Rotated Faster RCNN的框架包括以下流程：'
- en: (1) Feature maps generation. The backbone networks composed of CNN modules and
    the FPN structure are both utilized to extract multi-level feature maps with strong
    semantic information at multi-scales, which are also widely used in one-stage
    frameworks.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 特征图生成。由CNN模块和FPN结构组成的骨干网络被用来提取多尺度下具有强语义信息的多级特征图，这些特征图也广泛应用于单阶段框架中。
- en: '(2) Region proposal networks (RPN). RPN takes a feature map (of any size) as
    input and generates a collection of horizontal region proposals by sliding a tiny
    network over the input feature map. At each sliding position in the feature map,
    RPN first initializes a total of $k$ reference boxes of different scales and aspect
    ratios, where $k=N_{scales}\texttimes N_{ratios}$, where $N_{scales}$ and $N_{ratios}$
    denote the number of scales and aspect ratios, respectively. Each anchor of the
    input feature map is mapped to a lower-dimensional feature vector, which is then
    fed into two sibling fully connected (FC) layers: a binary classification layer
    that estimates the probability of objects existing and a regression layer that
    refines the location of anchor coarsely. Thus, RPN simultaneously predicts $k$
    region proposals at each sliding position. However, negative anchors will predominate
    as only a few locations contain objects, and dominate the gradient during training.
    To address this issue, a random sampling operator is adopted to make the proportion
    between positive and negative anchors up to 1:1. In a word, RPN outputs a certain
    number of horizontal region proposals with a coarse location.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 区域提议网络（RPN）。RPN以任意大小的特征图作为输入，并通过在输入特征图上滑动一个小型网络生成一组水平区域提议。在特征图的每个滑动位置，RPN首先初始化总共$k$个不同尺度和纵横比的参考框，其中$k=N_{scales}\texttimes
    N_{ratios}$，$N_{scales}$和$N_{ratios}$分别表示尺度和纵横比的数量。输入特征图的每个锚点被映射到一个低维特征向量，然后输入到两个兄弟全连接（FC）层：一个二分类层估计物体存在的概率，另一个回归层粗略地调整锚点的位置。因此，RPN在每个滑动位置同时预测$k$个区域提议。然而，负锚点将占主导地位，因为只有少数位置包含物体，并且在训练过程中主导梯度。为了解决这个问题，采用了随机采样操作符，使正负锚点的比例达到1:1。总之，RPN输出一定数量的水平区域提议，位置较粗略。
- en: '(3) Regions with CNN features (RCNN). An RoI operator, e.g. RoI Pooling (Ren
    et al., [2017](#bib.bib118)), RoI Align (He et al., [2020b](#bib.bib57)), or deformable
    RoI Pooling (Dai et al., [2017](#bib.bib25)), is adopted to convert the features
    map inside any region proposal of different spatial extent into a small feature
    map with a fixed size. Then each fixed-size feature map is fed into two sibling
    FC layers: one estimates probabilities over all categories plus background, and
    another regresses the orientation and refines the coarse location suggested by
    RPN.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 带有CNN特征的区域（RCNN）。采用RoI操作符，如RoI Pooling（Ren et al., [2017](#bib.bib118)）、RoI
    Align（He et al., [2020b](#bib.bib57)）或可变形RoI Pooling（Dai et al., [2017](#bib.bib25)），将不同空间范围内的任何区域提议中的特征图转换为固定大小的小特征图。然后，将每个固定大小的特征图输入到两个兄弟FC层中：一个对所有类别加背景进行概率估计，另一个回归方向并细化RPN建议的粗略位置。
- en: However, However, since the naive RPN only generates a set of horizontal region
    proposals (as RoIs), there are non-negligible misalignments between the HBBs and
    rotated objects. Especially, several oriented and crowded objects may be contained
    by one horizontal RoI (HRoI). As a result, the feature maps of these HRoIs contain
    irrelevant information, making classification and localization more challenging
    yet inspiring successive innovations.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于原始的RPN仅生成一组水平区域建议（即RoIs），因此HBBs和旋转物体之间存在不可忽视的错位。特别是，几个定向且拥挤的物体可能被一个水平RoI（HRoI）包含。因此，这些HRoIs的特征图包含了无关的信息，使得分类和定位更加具有挑战性，但也激发了后续的创新。
- en: Many recent advances in two-stage oriented object detection greatly benefit
    from the frameworks of Rotated Faster RCNN, leading to an enormous number of improved
    detection methods. Some typical two-stage oriented object detection methods are
    reviewed as follows, including RRPN (Ma et al., [2018](#bib.bib104); Yang et al.,
    [2018a](#bib.bib171); Zhang et al., [2018](#bib.bib197)), RoI Transformer (Ding
    et al., [2019](#bib.bib29)), oriented RCNN (Xie et al., [2021](#bib.bib166)),
    and DODet (Cheng et al., [2022b](#bib.bib19)).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在两阶段定向目标检测中的许多进展得益于**旋转 Faster RCNN**框架，从而带来了大量改进的检测方法。一些典型的两阶段定向目标检测方法包括
    RRPN (Ma et al., [2018](#bib.bib104); Yang et al., [2018a](#bib.bib171); Zhang
    et al., [2018](#bib.bib197))、RoI Transformer (Ding et al., [2019](#bib.bib29))、定向
    RCNN (Xie et al., [2021](#bib.bib166)) 和 DODet (Cheng et al., [2022b](#bib.bib19))。
- en: '![Refer to caption](img/a95108f762abf5b516eed247864a3a38.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a95108f762abf5b516eed247864a3a38.png)'
- en: 'Figure 8: The basic architecture of one-stage oriented detectors (Lin et al.,
    [2020](#bib.bib89)).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：单阶段定向检测器的基本架构 (Lin et al., [2020](#bib.bib89))。
- en: 'Rotated RPN (RRPN). As horizontal anchors and HRoIs are insufficient for oriented
    object detection in RS images, RRPN (Ma et al., [2018](#bib.bib104); Yang et al.,
    [2018a](#bib.bib171); Zhang et al., [2018](#bib.bib197)) are designed with rotated
    anchors to fit the objects with different orientations and generate rotated proposals.
    Specifically, in addition to scales and aspect ratios, different orientation parameters
    are added to further generate additional anchors, which are then fed into OBB
    regression layers to refine the rotated region proposals. To eliminate irrelevant
    interference information, the rotated RoI (RRoI) operators, including RRoI-Pooling
    or RRoI Align (Yang et al., [2018a](#bib.bib171)), are designed to extract a fixed-size
    feature map, according to the rotated proposals. Thanks to the rotated anchors
    and the RRoI operators, RRPN achieves better performance in terms of recall. However,
    RRPN still has several significant drawbacks: (1) To maintain the trade-offs between
    orientation coverage and computational complexity, the number of orientation samples
    is limited. Therefore, it is intractable to obtain accurate RRoIs to pair with
    all rotated objects. (2) The number of densely rotated anchors is $N_{orientations}$
    times that of anchors generated by RPN, leading to expensive computation and memory
    consumption, where $N_{orientations}$ denotes the number of orientations. (3)
    A large amount of rotated anchors degrades the efficiency of matching proposals
    with GT objects, because there are plenty of redundant RIoUs and the computation
    of RIoU is more complex than IoU.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转 RPN (RRPN)。由于水平锚点和 HRoI 对于 RS 图像中的定向目标检测不够充分，因此 RRPN (Ma et al., [2018](#bib.bib104);
    Yang et al., [2018a](#bib.bib171); Zhang et al., [2018](#bib.bib197)) 设计了旋转锚点，以适应不同方向的目标并生成旋转提议。具体而言，除了尺度和长宽比，还添加了不同的方向参数，以进一步生成额外的锚点，然后将这些锚点输入到
    OBB 回归层中以细化旋转区域提议。为了消除不相关的干扰信息，设计了旋转 RoI (RRoI) 操作符，包括 RRoI-Pooling 或 RRoI Align (Yang
    et al., [2018a](#bib.bib171))，以根据旋转提议提取固定大小的特征图。得益于旋转锚点和 RRoI 操作符，RRPN 在召回率方面实现了更好的性能。然而，RRPN
    仍然存在几个显著缺点：（1）为了在方向覆盖和计算复杂性之间保持平衡，方向样本的数量有限。因此，难以获得准确的 RRoI 来与所有旋转目标配对。（2）密集旋转锚点的数量是
    RPN 生成的锚点数量的 $N_{orientations}$ 倍，这导致计算和内存消耗昂贵，其中 $N_{orientations}$ 表示方向数量。（3）大量的旋转锚点降低了与
    GT 目标匹配提议的效率，因为存在大量冗余的 RIoU，且 RIoU 的计算比 IoU 更复杂。
- en: 'RoI Transformer. To reduce the number of rotated anchors, RoI Transformer (Ding
    et al., [2019](#bib.bib29)) retains a naive RPN structure and introduces a lightweight
    learnable module, named RoI Learner. Aiming at directly converting HRoIs to RRoIs,
    RRoI Learner is composed of three components: a position-sensitive RoI Align (Dai
    et al., [2016](#bib.bib24)) for extracting HRoI features, a lightweight FC layer
    for regressing the offsets between GT OBB and HRoI, and an OBB decoder for outputting
    the RRoI by decoding HRoI and offsets. Then a rotated position-sensitive RoI Align
    (RPS RoI Align) receives these RRoIs and further extracts spatially rotation-invariant
    feature maps, which are used in the final tasks of classification and regression.
    Such a design generates precise RRoIs without enormous rotated anchors, resulting
    in higher efficiency and accuracy.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: RoI Transformer。为了减少旋转锚点的数量，RoI Transformer（Ding 等人，[2019](#bib.bib29)）保留了一个简单的
    RPN 结构，并引入了一个轻量级的可学习模块，名为 RoI Learner。为了直接将 HRoIs 转换为 RRoIs，RRoI Learner 由三个组件组成：一个位置敏感的
    RoI Align（Dai 等人，[2016](#bib.bib24)）用于提取 HRoI 特征，一个轻量级 FC 层用于回归 GT OBB 和 HRoI
    之间的偏移，以及一个 OBB 解码器通过解码 HRoI 和偏移来输出 RRoI。然后，一个旋转位置敏感的 RoI Align（RPS RoI Align）接收这些
    RRoIs，并进一步提取空间旋转不变的特征图，这些特征图用于最终的分类和回归任务。这种设计在不需要大量旋转锚点的情况下生成精确的 RRoIs，从而提高了效率和准确性。
- en: Oriented RCNN. Although the RoI Transformer significantly boosts accuracy and
    efficiency, it involves an additional stage to produce RRoIs, making the networks
    complex and heavy. As a result,  Xie et al. ([2021](#bib.bib166)) designs a simpler
    structure, named oriented RPN, to generate high-quality RRoIs from horizontal
    anchors. To reduce computation costs, oriented RPN only contains a $3\times 3$
    convolutional layer and two sibling $1\times 1$ convolutional layers. This lightweight
    module benefits from the proposed novel OBB representation, named midpoint offset
    representation. For an oriented object, its midpoint offset representation consists
    of six parameters $(x,y,w,h,\Delta\alpha,\Delta\beta)$, where $(x,y,w,h)$ refers
    to its external HBB, $\Delta\alpha,\Delta\beta$ denote the offsets w.r.t the midpoints
    of the top and right sides of the external HBB, respectively. The external HBB
    can provide bounded constraint for OBB and the offsets $\Delta\alpha,\Delta\beta$
    can avoid the periodicity of angle (PoA) problem. Benefiting from the design of
    oriented RPN and midpoint offset representation, Oriented RCNN can achieve competitive
    accuracy to advanced two-stage detectors and reach approximate efficiency to one-stage
    detectors.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 定向 RCNN。尽管 RoI Transformer 显著提升了准确性和效率，但它涉及一个额外的阶段来生成 RRoIs，这使得网络复杂且沉重。因此，Xie
    等人（[2021](#bib.bib166)）设计了一种更简单的结构，名为定向 RPN，以从水平锚点生成高质量的 RRoIs。为了减少计算成本，定向 RPN
    仅包含一个 $3\times 3$ 的卷积层和两个同级的 $1\times 1$ 卷积层。这个轻量级模块得益于提出的新型 OBB 表示法，称为中点偏移表示。对于定向对象，其中点偏移表示由六个参数
    $(x,y,w,h,\Delta\alpha,\Delta\beta)$ 组成，其中 $(x,y,w,h)$ 代表其外部 HBB，$\Delta\alpha,\Delta\beta$
    分别表示相对于外部 HBB 顶部和右侧中点的偏移。外部 HBB 可以为 OBB 提供有界约束，而偏移量 $\Delta\alpha,\Delta\beta$
    可以避免角度（PoA）问题。得益于定向 RPN 和中点偏移表示的设计，定向 RCNN 可以实现与先进的两阶段检测器相媲美的准确性，并达到与单阶段检测器相近的效率。
- en: DODet. To avoid the spatial and feature misalignments between horizontal proposals
    and oriented objects, Cheng et al. ([2022b](#bib.bib19)) designs the dual-aligned
    oriented detector (DODet), consisting of an oriented proposal network (OPN) and
    a localization-guided detection head (LDH). OPN is a lightweight network to generate
    high-quality rotated proposals. Besides, a new OBB representation is designed
    to better adapt objects with large aspect ratios, which replaces the width and
    height with aspect ratio and area. LDH is used to address the feature misalignments
    between regression and classification. The regression head is first adopted to
    generate more accurate OBBs. These OBBs are then used as guidance to refine the
    classification features. With the design of new OBB representation and feature
    alignments, DODet can achieve substantial improvement for objects with large aspect
    ratios and reached $97.14\%$ mAP on HRSC2016 dataset.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: DODet。为了避免水平提议与定向物体之间的空间和特征错位，程等人（[2022b](#bib.bib19)）设计了双对齐定向检测器（DODet），由定向提议网络（OPN）和定位引导检测头（LDH）组成。OPN是一个轻量级网络，用于生成高质量的旋转提议。此外，设计了一种新的OBB表示方法，更好地适应具有大长宽比的物体，替代了宽度和高度，用长宽比和面积代替。LDH用于解决回归与分类之间的特征错位。首先采用回归头生成更准确的OBBs，然后这些OBBs作为指导来细化分类特征。通过新的OBB表示方法和特征对齐设计，DODet能够显著提高大长宽比物体的检测效果，并在HRSC2016数据集上达到$97.14\%$的mAP。
- en: 4.1.2 One-stage
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 一阶段
- en: 'Table 4: Summary of properties of typical one-stage detection frameworks'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：典型一阶段检测框架属性总结
- en: '| Method | Baseline | backbone | mAP | Highlights |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 基线 | 主干网络 | mAP | 亮点 |'
- en: '| Rotated RetinaNet | RetinaNet | R-50 | 68.43 | Design the focal loss to mitigate
    class imbalance. A typical baseline for most of one-stage rotated detectors. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 旋转版RetinaNet | RetinaNet | R-50 | 68.43 | 设计了焦点损失以缓解类别不平衡。大多数一阶段旋转检测器的典型基线。'
- en: '| (Lin et al., [2020](#bib.bib89)) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| (林等人，[2020](#bib.bib89)) |'
- en: '| R³Det | Rotated RetinaNet | R-152 | 76.47 | Use FRM to refine features and
    design a differentiable SkewIoU loss. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| R³Det | 旋转版RetinaNet | R-152 | 76.47 | 使用FRM细化特征，并设计可微分的SkewIoU损失。 |'
- en: '| (Yang et al., [2021b](#bib.bib174)) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| (杨等人，[2021b](#bib.bib174)) |'
- en: '|  | Deformable DETR | R-50 | 79.22 | An end-to-end transformer-based rotated
    detector. Use OPG to generate oriented proposals and design OPR to refine these
    oriented proposals. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | 可变形DETR | R-50 | 79.22 | 一个端到端的基于变压器的旋转检测器。使用OPG生成定向提议，并设计OPR来细化这些定向提议。
    |'
- en: '| AO2-DETR |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| AO2-DETR |'
- en: '| (Dai et al., [2022](#bib.bib26)) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| (戴等人，[2022](#bib.bib26)) |'
- en: '| S²A-Net | Rotated RetinaNet | R-50 | 79.42 | Use FAM to align features and
    adopt ODM to extract oriented-sensitive features. |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| S²A-Net | 旋转版RetinaNet | R-50 | 79.42 | 使用FAM对齐特征，并采用ODM提取定向敏感特征。 |'
- en: '| (Han et al., [2022a](#bib.bib47)) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| (韩等人，[2022a](#bib.bib47)) |'
- en: '| GWD | R³Det | R-152 | 80.23 | First present a Gaussian Wasserstein distance
    based loss to model the deviation between two OBBs. |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| GWD | R³Det | R-152 | 首次提出基于高斯Wasserstein距离的损失，以建模两个OBBs之间的偏差。 |'
- en: '| (Yang et al., [2021c](#bib.bib176)) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| (杨等人，[2021c](#bib.bib176)) |'
- en: '| KLD | R³Det | R-152 | 80.63 | Similar to GWD, it adopts KLD instead of GWD
    to achieve more accurate detection. |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| KLD | R³Det | R-152 | 类似于GWD，采用KLD代替GWD以实现更准确的检测。 |'
- en: '| Yang et al. ([2021d](#bib.bib178)) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 杨等人（[2021d](#bib.bib178)） |'
- en: '| KFIoU | R³Det | R-152 | 81.03 | Design the KFIoU loss based on Kalman filter
    to achieve the best trend-level alignment with RIoU. |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| KFIoU | R³Det | R-152 | 81.03 | 基于卡尔曼滤波器设计KFIoU损失，以实现与RIoU的最佳趋势级对齐。 |'
- en: '| (Yang et al., [2022](#bib.bib180)) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| (杨等人，[2022](#bib.bib180)) |'
- en: Different from two-stage detection frameworks working in a coarse-to-fine paradigm,
    one-stage detectors directly predict the class probabilities and locations of
    objects without using region proposal networks and RoI operators. Hence, one-stage
    detectors are therefore more efficient and better adapted for real-time detection.
    Recently, a line of classical one-stage algorithms has emerged and achieved significant
    progress, including Rotated RetinaNet (Lin et al., [2020](#bib.bib89)), R³Det (Yang
    et al., [2021b](#bib.bib174)), and S²A-Net (Han et al., [2022a](#bib.bib47)).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 与以粗到细的范式工作的两阶段检测框架不同，一阶段检测器直接预测类别概率和物体的位置，无需使用区域提议网络和RoI操作符。因此，一阶段检测器更高效，更适合实时检测。最近，出现了一系列经典的一阶段算法并取得了显著进展，包括旋转版RetinaNet（林等人，[2020](#bib.bib89)）、R³Det（杨等人，[2021b](#bib.bib174)）和S²A-Net（韩等人，[2022a](#bib.bib47)）。
- en: 'Rotated RetinaNet. Lin et al. ([2020](#bib.bib89)) proposed RetinaNet with
    focal loss to effectively mitigate class imbalance during the training process,
    achieving accuracy comparable to two-stage detectors. As a result, the RetinaNet-based
    rotated detector, named Rotated RetinaNet, is used as a benchmark. As illustrated
    in Figure[8](#S4.F8 "Figure 8 ‣ 4.1.1 Two-stage ‣ 4.1 Anchor-based ‣ 4 Detection
    Frameworks ‣ Oriented Object Detection in Optical Remote Sensing Images using
    Deep Learning: A Survey"), Rotated RetinaNet first extracts multi-level feature
    maps via CNN modules with FPN. Then, a certain number of anchors are initialized.
    For each anchor per spatial location, the relative offset between the anchor and
    the GT OBB is predicted by a regression head. Meanwhile, the class probability
    of each anchor is predicted by a classification head. In contrast to class-agnostic
    RPN which only distinguishes between the background and foreground of each anchor,
    the classification head predicts probabilities over all categories. What’s more,
    both heads are deeper than RPN and do not share parameters.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转 RetinaNet。Lin 等（[2020](#bib.bib89)）提出了带有焦点损失的 RetinaNet，有效缓解了训练过程中的类别不平衡，实现了与两阶段检测器相媲美的准确性。因此，基于
    RetinaNet 的旋转检测器，名为旋转 RetinaNet，被用作基准。如图 [8](#S4.F8 "图 8 ‣ 4.1.1 两阶段 ‣ 4.1 基于锚点的
    ‣ 4 检测框架 ‣ 使用深度学习的光学遥感图像中的面向物体检测：综述") 所示，旋转 RetinaNet 首先通过具有 FPN 的 CNN 模块提取多层级特征图。然后，初始化一定数量的锚点。对于每个空间位置的锚点，回归头预测锚点与
    GT OBB 之间的相对偏移。同时，分类头预测每个锚点的类别概率。与仅区分每个锚点背景和前景的类别无关的 RPN 相比，分类头对所有类别进行概率预测。此外，这两个头部比
    RPN 更深且不共享参数。
- en: Although its architecture is simple and computationally efficient, Rotated RetinaNet
    still lags behind the current advanced two-stage oriented detectors in terms of
    accuracy. The first reason is that horizontal anchors cannot tightly cover the
    oriented objects, leading to misalignment between objects and anchors. What’s
    more, the convolutional features are typically axis-aligned and possess fixed
    receptive fields, while objects are distributed with arbitrary orientations and
    various scales. As a result, the corresponding feature of an anchor is unable
    to precisely represent the object, especially when the object has an extreme aspect
    ratio. To solve these issues, numerous refined detectors (Yang et al., [2021b](#bib.bib174);
    Han et al., [2022a](#bib.bib47)) are proposed to align objects and anchors, using
    a fully convolution block instead of a RoI operator which requires massive time-consuming
    region-wise processes.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其架构简单且计算高效，但旋转 RetinaNet 在准确性方面仍落后于当前先进的两阶段面向检测器。第一个原因是水平锚点无法紧密覆盖面向物体，导致物体与锚点之间的错位。此外，卷积特征通常是轴对齐的，并且具有固定的感受野，而物体分布具有任意的方向和各种尺度。因此，锚点的相应特征无法准确表示物体，尤其是当物体具有极端长宽比时。为了解决这些问题，提出了许多精细化检测器（Yang
    等，[2021b](#bib.bib174)；Han 等，[2022a](#bib.bib47)），它们使用全卷积块代替需要大量时间消耗的区域处理的 RoI
    操作，以对齐物体和锚点。
- en: Refined Rotation RetinaNet (R³Det). R³Det (Yang et al., [2021b](#bib.bib174))
    refines Rotated RetinaNet using a feature refinement module (FRM). It works in
    a coarse-to-fine paradigm. Specifically, R³Det first generates multi-level feature
    maps and then transforms the horizontal anchors to refined rotated anchors, which
    can provide more accurate position information. To align and reconstruct feature
    maps, FRM employs pixel-wise feature interpolation to sample features from five
    locations (i.e., one center and four corners) of the corresponding refined rotated
    anchors and sums them up. In addition, an approximate RIoU loss, named SkewIoU
    loss, is designed to solve the indifferentiable problem of RIoU, enabling stable
    training and accurate localization.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 精细化旋转 RetinaNet（R³Det）。R³Det（Yang 等，[2021b](#bib.bib174)）通过使用特征精炼模块（FRM）对旋转
    RetinaNet 进行改进。它采用粗到细的工作模式。具体而言，R³Det 首先生成多层级的特征图，然后将水平锚点转换为精细化的旋转锚点，这可以提供更准确的位置信息。为了对齐和重建特征图，FRM
    采用逐像素特征插值，从相应的精细化旋转锚点的五个位置（即一个中心和四个角落）中采样特征并进行累加。此外，设计了一种近似 RIoU 损失，称为 SkewIoU
    损失，用于解决 RIoU 的不可微分问题，从而实现稳定的训练和准确的定位。
- en: Single-shot Alignment Network (S²A-Net). Similar to R³Det, S²A-Net (Han et al.,
    [2022a](#bib.bib47)) also selects Rotated RetinaNet as the baseline. To achieve
    feature alignment and alleviate the inconsistency between regression and classification,
    a feature alignment module (FAM) and an oriented detection module (ODM) are designed.
    FAM first generates high-quality rotated anchors from horizontal anchors via an
    anchor refinement network. Then, it adaptively aligns the features with an alignment
    convolution (AlignConv). Specifically, AlignConv is a variant of deformable convolution (Dai
    et al., [2017](#bib.bib25)), which infers the offsets with the guide of refined
    rotated anchors to extract rotated grid-distributed features. While the classification
    task requires orientation-invariant features, the regression task benefits from
    the orientation-sensitive features, resulting in the inconsistency between regression
    and classification. Therefore, inspired by Rotation-sensitive Regression Detector (Liao
    et al., [2018](#bib.bib87)), ODM adopts active rotating filters (Zhou et al.,
    [2017](#bib.bib207)) to extract orientation-sensitive features for regression.
    Then a pooling operator is employed to extract orientation-invariant features
    from the orientation-sensitive features for classification.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 单次对齐网络（S²A-Net）。与R³Det类似，S²A-Net（Han et al., [2022a](#bib.bib47)）也选择了Rotated
    RetinaNet作为基线。为了实现特征对齐并缓解回归与分类之间的不一致，设计了特征对齐模块（FAM）和方向检测模块（ODM）。FAM首先通过锚点精炼网络从水平锚点生成高质量的旋转锚点。然后，它通过对齐卷积（AlignConv）自适应地对齐特征。具体来说，AlignConv是可变形卷积（Dai
    et al., [2017](#bib.bib25)）的一种变体，它根据精炼的旋转锚点推断偏移量，以提取旋转网格分布特征。虽然分类任务需要方向无关的特征，但回归任务受益于方向敏感的特征，导致回归与分类之间的不一致。因此，受旋转敏感回归检测器（Liao
    et al., [2018](#bib.bib87)）的启发，ODM采用了主动旋转滤波器（Zhou et al., [2017](#bib.bib207)）来提取用于回归的方向敏感特征。然后，采用池化操作从方向敏感特征中提取方向无关特征以用于分类。
- en: 4.2 Anchor-Free
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 锚点自由
- en: '![Refer to caption](img/6624a763aeef754d5e958495f3167db4.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6624a763aeef754d5e958495f3167db4.png)'
- en: (a) Keypoint-based anchor-free detector.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 基于关键点的锚点自由检测器。
- en: '![Refer to caption](img/631a6bbca9ab080c712290364fd7f991.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/631a6bbca9ab080c712290364fd7f991.png)'
- en: (b) Center-based anchor-free detector.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 基于中心的锚点自由检测器。
- en: 'Figure 9: The basic architecture of keypoint-based and center-based anchor-free
    detectors.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：基于关键点和基于中心的锚点自由检测器的基本架构。
- en: While the anchor-based methods play a very important role and make significant
    improvements in object detection, they still suffer from some critical drawbacks.
    Firstly, the predefined anchors are designed manually and have several hand-crafted
    components, including scales, aspect ratios, and even orientations, which are
    fixed during training and cannot be adjusted adaptively. Secondly, the hand-crafted
    anchors have trouble matching objects with different scales or orientations. Thirdly,
    most of the anchors are negative, leading to an imbalance between positive and
    negative samples. To tackle the above issues, a constellation of anchor-free methods
    is developed to find objects without preset anchors. These anchor-free methods
    eliminate anchor-related hyper-parameters and have achieved comparable performance
    with anchor-based methods, showing potential in the generalization to wide applications (Zhang
    et al., [2020c](#bib.bib194)). According to the representation of OBB, anchor-free
    methods can be divided into keypoint-based methods (Guo et al., [2021](#bib.bib44);
    Wei et al., [2020](#bib.bib153)) and center-based methods (Guan et al., [2021](#bib.bib43);
    He et al., [2021](#bib.bib59); Lin et al., [2019](#bib.bib91); Xiao et al., [2020b](#bib.bib164);
    Yi et al., [2021](#bib.bib183); Zhang et al., [2022](#bib.bib192); Zhou et al.,
    [2020](#bib.bib205); Zhao et al., [2021](#bib.bib200)).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于锚点的方法在物体检测中扮演了非常重要的角色，并且在性能上取得了显著的改善，但它们仍然存在一些关键的缺陷。首先，预定义的锚点是手动设计的，具有多个手工设计的组件，包括尺度、长宽比，甚至方向，这些在训练过程中是固定的，无法自适应调整。其次，手工设计的锚点在匹配具有不同尺度或方向的物体时遇到困难。第三，大多数锚点都是负样本，导致正负样本之间的不平衡。为了应对这些问题，开发了一系列无锚点的方法来在没有预设锚点的情况下寻找物体。这些无锚点的方法消除了与锚点相关的超参数，并且在性能上与基于锚点的方法相当，显示了在广泛应用中的潜力 （Zhang
    et al., [2020c](#bib.bib194)）。根据OBB的表示，无锚点的方法可以分为基于关键点的方法 （Guo et al., [2021](#bib.bib44)；Wei
    et al., [2020](#bib.bib153)）和基于中心的方法 （Guan et al., [2021](#bib.bib43)；He et al.,
    [2021](#bib.bib59)；Lin et al., [2019](#bib.bib91)；Xiao et al., [2020b](#bib.bib164)；Yi
    et al., [2021](#bib.bib183)；Zhang et al., [2022](#bib.bib192)；Zhou et al., [2020](#bib.bib205)；Zhao
    et al., [2021](#bib.bib200)）。
- en: 'The keypoint-based methods first locate a set of adaptive or self-constrained
    key points and then circumscribe the spatial extent of the object, as shown in
    Figure [9(a)](#S4.F9.sf1 "In Figure 9 ‣ 4.2 Anchor-Free ‣ 4 Detection Frameworks
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey"). O²-DNet (Wei et al., [2020](#bib.bib153)) first locates the midpoints
    of four sides of the OBB by regressing the offsets from the center point. Then,
    two sets of opposite midpoints are connected to form two mutually perpendicular
    midlines which can be decoded to get the representation of OBB. In addition, a
    self-supervision loss is designed to constrain the perpendicular relationship
    between two middle lines and a collinear relationship between the center point
    and two opposite midpoints. Following the RepPoints (Yang et al., [2019b](#bib.bib181)),
    CFA (Guo et al., [2021](#bib.bib44)) utilizes the deformable convolution (Dai
    et al., [2017](#bib.bib25)) to generate a convex hull for each oriented object.
    The convex hull is represented by a set of irregular sample points bounding the
    spatial extent of an object, which are determined by the designed Convex Intersection
    over Union (CIoU) loss. To alleviate feature aliasing between densely packed objects,
    convex-hull set splitting and feature anti-aliasing strategies are designed to
    refine the convex-hulls and adaptively optimal feature assignment. To predict
    the high-quality oriented reppoints, Oriented RepPoints (Li et al., [2022b](#bib.bib85))
    further designs an Adaptive Points Assessment and Assignment (APAA) scheme to
    measure the quality of reppoints. Such a scheme assesses each set of reppoints
    from four aspects, including classification, localization, orientation alignment,
    and point-wise correlation. As a result, the high-quality reppoints obtained by
    APAA enable Oriented RepPoints to achieve state-of-the-art performance among anchor-free
    methods.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 基于关键点的方法首先定位一组自适应或自约束的关键点，然后勾画出物体的空间范围，如图[9(a)](#S4.F9.sf1 "在图9 ‣ 4.2 Anchor-Free
    ‣ 4 检测框架 ‣ 使用深度学习的面向对象光学遥感图像检测"). O²-DNet （Wei 等，[2020](#bib.bib153)）首先通过回归从中心点的偏移量来定位OBB四个边的中点。然后，将两组相对的中点连接起来，形成两条互相垂直的中线，可以解码得到OBB的表示。此外，设计了一个自监督损失来约束两条中线之间的垂直关系以及中心点与两相对中点之间的共线关系。继RepPoints （Yang
    等，[2019b](#bib.bib181)）之后，CFA （Guo 等，[2021](#bib.bib44)）利用可变形卷积 （Dai 等，[2017](#bib.bib25)）为每个定向物体生成凸包。凸包由一组不规则的样本点表示，这些点界定了物体的空间范围，由设计的凸包交集（CIoU）损失确定。为了缓解密集物体之间的特征混叠，设计了凸包集拆分和特征抗混叠策略，以细化凸包并自适应地优化特征分配。为了预测高质量的定向重叠点，Oriented
    RepPoints （Li 等，[2022b](#bib.bib85)）进一步设计了自适应点评估与分配（APAA）方案来衡量重叠点的质量。该方案从分类、定位、方向对齐和逐点相关性四个方面评估每组重叠点。因此，通过APAA获得的高质量重叠点使Oriented
    RepPoints在无锚方法中实现了最先进的性能。
- en: 'The center-based methods generally generate multiple probabilistic heatmaps
    and a series of feature maps. As shown in Figure [9(b)](#S4.F9.sf2 "In Figure
    9 ‣ 4.2 Anchor-Free ‣ 4 Detection Frameworks ‣ Oriented Object Detection in Optical
    Remote Sensing Images using Deep Learning: A Survey"), the heatmaps provide a
    set of candidates (peak points) as coarse center points. Meanwhile, the feature
    maps regress transformation parameters to accurately represent the OBB. The heatmaps
    are denoted by $M_{p}\in[0,1]^{(\frac{H}{s}\times\frac{W}{s}\times C)}$, where
    $W$ and $H$ denote the width and height of original image respectively, $C$ represents
    the number of predefined categories, $s$ is a scale factor. The GT heatmaps $M_{g}\in[0,1]^{(\frac{H}{s}\times\frac{W}{s}\times
    C)}$ are formed by producing a locally high energy region near the center point
    of each object. The value at the center point of each object on the heatmaps is
    set to 1, the value near the center point is determined by the Gaussian kernel,
    and the rest regions are set to $0$. The pipeline of center-based methods comprises
    two steps. A number of peak points are first selected as coarse center points
    from the probabilistic heatmaps. Then, transformation parameters, including the
    center points offset, object sizes, and angle, are regressed on the corresponding
    feature maps at the position of each coarse center point. However, the center-based
    methods typically follow the one-stage paradigms and tend to predict coarse locations,
    while state-of-the-art methods generally contain one or multiple refine stages
    to improve the performance. Thus, an effective scheme for performance improvement
    is leveraging anchor-free methods to generate coarse proposals that are then refined
    to generate high-quality proposals or detection results, e.g. AOPG (Cheng et al.,
    [2022a](#bib.bib18)), DEA (Liang et al., [2022](#bib.bib86)). This could be attributed
    to the fact that the anchor-free rotated proposal generation scheme can not only
    generate accurate proposals but also avoid the problems caused by horizontal anchors.
    Nevertheless, there is still a significant performance gap between plain center-based
    oriented methods and other state-of-the-art methods, necessitating further future
    research.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 基于中心的方法通常生成多个概率热图和一系列特征图。如图 [9(b)](#S4.F9.sf2 "图 9 ‣ 4.2 无锚点 ‣ 4 检测框架 ‣ 使用深度学习的光学遥感图像中的定向物体检测：综述")
    所示，热图提供了一组候选点（峰值点）作为粗略的中心点。同时，特征图回归变换参数以准确表示边界框（OBB）。热图记作 $M_{p}\in[0,1]^{(\frac{H}{s}\times\frac{W}{s}\times
    C)}$，其中 $W$ 和 $H$ 分别表示原始图像的宽度和高度，$C$ 代表预定义类别的数量，$s$ 是尺度因子。GT 热图 $M_{g}\in[0,1]^{(\frac{H}{s}\times\frac{W}{s}\times
    C)}$ 通过在每个物体的中心点附近生成局部高能量区域来形成。热图上每个物体的中心点的值设为 1，中心点附近的值由高斯核确定，其余区域设为 $0$。基于中心的方法的处理流程包括两个步骤。首先，从概率热图中选择一些峰值点作为粗略的中心点。然后，在每个粗略中心点的位置的对应特征图上回归变换参数，包括中心点的偏移、物体的大小和角度。然而，基于中心的方法通常遵循单阶段范式，并倾向于预测粗略的位置，而最先进的方法通常包含一个或多个细化阶段以提高性能。因此，一个有效的性能提升方案是利用无锚点的方法生成粗略建议，然后通过细化生成高质量的建议或检测结果，例如
    AOPG (Cheng et al., [2022a](#bib.bib18))、DEA (Liang et al., [2022](#bib.bib86))。这可以归因于无锚点的旋转建议生成方案不仅可以生成准确的建议，还可以避免水平锚点带来的问题。然而，纯基于中心的定向方法与其他最先进方法之间仍存在显著的性能差距，需要进一步的未来研究。
- en: 4.3 Anchor-based vs. Anchor-free
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 基于锚点的方法与无锚点的方法
- en: Although anchor-based and anchor-free frameworks show their own merits, there
    are also shortcomings in specific scenarios. For instance, in terms of small objects,
    it is difficult for anchor-based methods to obtain positive samples, due to the
    low IoU between small objects and predefined anchors. In contrast, anchor-free
    methods only need to select a point as a positive sample for a small object. On
    the other hand, anchor-free methods show worse performance for objects of large
    size and extreme aspect ratio.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于锚点的方法和无锚点的方法各有优缺点，但在特定场景中也存在不足。例如，对于小物体，基于锚点的方法由于小物体与预定义锚点之间的 IoU 低而难以获得正样本。相比之下，无锚点的方法只需选择一个点作为小物体的正样本。另一方面，无锚点的方法对大尺寸和极端长宽比的物体表现较差。
- en: 'To cope with the above dilemma, the suitable OBB representation and the powerful
    feature representation of oriented objects are also widely studied, since they
    can be seamlessly integrated into both anchor-based and anchor-free frameworks.
    In the following, we discuss the OBB representation and feature representation
    in Section [5](#S5 "5 OBB Representations ‣ Oriented Object Detection in Optical
    Remote Sensing Images using Deep Learning: A Survey") and Section [6](#S6 "6 Feature
    Representations ‣ Oriented Object Detection in Optical Remote Sensing Images using
    Deep Learning: A Survey"), respectively.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '为了应对上述困境，合适的 OBB 表示法和强大的方向对象特征表示法也得到了广泛研究，因为它们可以无缝集成到基于锚点和无锚点的框架中。接下来，我们将在第
    [5](#S5 "5 OBB Representations ‣ Oriented Object Detection in Optical Remote Sensing
    Images using Deep Learning: A Survey") 节和第 [6](#S6 "6 Feature Representations
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey") 节中分别讨论 OBB 表示法和特征表示法。'
- en: 5 OBB Representations
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 OBB 表示法
- en: '![Refer to caption](img/2cc13aeec3d61bb6e93b9215720a8895.png)![Refer to caption](img/27d1b7440456b51b7b6b4b4ea0901718.png)![Refer
    to caption](img/a49722fe37519b72281636d9d7638d6e.png)![Refer to caption](img/429e6962a64ebb0017bbcdbf78ec6f0d.png)![Refer
    to caption](img/45d00760801fd08d754813b9d9c9a565.png)![Refer to caption](img/b5d1c9eedb7661b87d71a6c4ec3b7d23.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2cc13aeec3d61bb6e93b9215720a8895.png)![参见说明](img/27d1b7440456b51b7b6b4b4ea0901718.png)![参见说明](img/a49722fe37519b72281636d9d7638d6e.png)![参见说明](img/429e6962a64ebb0017bbcdbf78ec6f0d.png)![参见说明](img/45d00760801fd08d754813b9d9c9a565.png)![参见说明](img/b5d1c9eedb7661b87d71a6c4ec3b7d23.png)'
- en: 'Figure 10: Definition of $\theta$-based representation. The OBBs depicted in
    the top/bottom row are the same. (a) OpenCV Definition ($\theta\in(0,\frac{\pi}{2}]$)(Top:
    height is longer than width. Bottom: width is longer than height). (b) Long edge
    definition with an angular range of $[-\frac{\pi}{2},\frac{\pi}{2})$. (c) Long
    edge definition with an angular range of $[-\frac{\pi}{4},\frac{3\pi}{4})$.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：基于 $\theta$ 的表示法定义。顶部/底部行中描绘的 OBBs 是相同的。 (a) OpenCV 定义（$\theta\in(0,\frac{\pi}{2}]$）（顶部：高度长于宽度。底部：宽度长于高度）。
    (b) 长边定义，角度范围为 $[-\frac{\pi}{2},\frac{\pi}{2})$。 (c) 长边定义，角度范围为 $[-\frac{\pi}{4},\frac{3\pi}{4})$。
- en: The most frequently-used methods of OBB representation are the $\theta$-based
    representation and the quadrilateral representation, consisting of five and eight
    parameters respectively.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: OBB 表示方法中最常用的是基于 $\theta$ 的表示法和四边形表示法，分别由五个和八个参数组成。
- en: 5.1 $\theta$-based representation
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基于 $\theta$ 的表示法
- en: 'The $\theta$-based representation adopts a vector in the format of $(x,y,w,h,\theta)$
    to define an OBB. The present approaches can be classified into two types according
    to the definition of the angle $\theta$, including the OpenCV definition (which
    follows the OpenCV protocol) and the long edge definition. As shown in Figure
    [10](#S5.F10 "Figure 10 ‣ 5 OBB Representations ‣ Oriented Object Detection in
    Optical Remote Sensing Images using Deep Learning: A Survey"), the former defines
    $\theta$ as the acute angle (i.e. the right angle) between the OBB and $x$-axis,
    leading to $\theta\in(0,\frac{\pi}{2}]$. Note that the width $w$ is defined as
    the side of the acute angle and can be shorter than the height $h$, which is shown
    in the top of Figure [10](#S5.F10 "Figure 10 ‣ 5 OBB Representations ‣ Oriented
    Object Detection in Optical Remote Sensing Images using Deep Learning: A Survey").
    To tackle this issue, the long edge definition is proposed by setting $\theta$
    as the angle between the long edge of the OBB and $x$-axis. Therefore, the angular
    range is $[-\frac{\pi}{2},\frac{\pi}{2})$ (Ding et al., [2019](#bib.bib29); Han
    et al., [2021a](#bib.bib49)) or $[-\frac{\pi}{4},\frac{3\pi}{4})$ (Han et al.,
    [2022b](#bib.bib48)), which are shown in Figure [10](#S5.F10 "Figure 10 ‣ 5 OBB
    Representations ‣ Oriented Object Detection in Optical Remote Sensing Images using
    Deep Learning: A Survey") and Figure [10](#S5.F10 "Figure 10 ‣ 5 OBB Representations
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey"), respectively. As shown in the bottom of Figure [10](#S5.F10 "Figure
    10 ‣ 5 OBB Representations ‣ Oriented Object Detection in Optical Remote Sensing
    Images using Deep Learning: A Survey"), the parameters of the same OBB have significant
    differences in different OBB representations.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '基于 $\theta$ 的表示采用格式为 $(x,y,w,h,\theta)$ 的向量来定义 OBB。目前的方法根据角度 $\theta$ 的定义可以分为两种类型，包括
    OpenCV 定义（遵循 OpenCV 协议）和长边定义。如图 [10](#S5.F10 "Figure 10 ‣ 5 OBB Representations
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey") 所示，前者将 $\theta$ 定义为 OBB 与 $x$-轴之间的锐角（即直角），导致 $\theta\in(0,\frac{\pi}{2}]$。请注意，宽度
    $w$ 定义为锐角的边，可能比高度 $h$ 短，如图 [10](#S5.F10 "Figure 10 ‣ 5 OBB Representations ‣ Oriented
    Object Detection in Optical Remote Sensing Images using Deep Learning: A Survey")
    顶部所示。为了解决这个问题，提出了长边定义，通过将 $\theta$ 设为 OBB 长边与 $x$-轴之间的角度。因此，角度范围为 $[-\frac{\pi}{2},\frac{\pi}{2})$（Ding
    et al., [2019](#bib.bib29); Han et al., [2021a](#bib.bib49)）或 $[-\frac{\pi}{4},\frac{3\pi}{4})$（Han
    et al., [2022b](#bib.bib48)），分别如图 [10](#S5.F10 "Figure 10 ‣ 5 OBB Representations
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey") 和图 [10](#S5.F10 "Figure 10 ‣ 5 OBB Representations ‣ Oriented Object
    Detection in Optical Remote Sensing Images using Deep Learning: A Survey") 所示。如图
    [10](#S5.F10 "Figure 10 ‣ 5 OBB Representations ‣ Oriented Object Detection in
    Optical Remote Sensing Images using Deep Learning: A Survey") 底部所示，同一 OBB 的参数在不同
    OBB 表示中存在显著差异。'
- en: 'Built upon well-designed horizontal detectors, most oriented object detectors
    predict OBBs in a regression fashion. In the $\theta$-based OBB representation,
    given an anchor box denoted by $b_{a}=(x_{a},y_{a},w_{a},h_{a},\theta_{a})$, the
    neural network first predicts the offsets between the predicted OBB and the anchor
    box:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 基于精心设计的水平检测器，大多数定向物体检测器以回归方式预测 OBB。在基于 $\theta$ 的 OBB 表示中，给定一个由 $b_{a}=(x_{a},y_{a},w_{a},h_{a},\theta_{a})$
    表示的锚框，神经网络首先预测预测 OBB 与锚框之间的偏移量：
- en: '|  | $\displaystyle t_{x}^{p}$ | $\displaystyle=\frac{x_{p}-x_{a}}{w_{a}},t_{y}^{p}=\frac{y_{p}-y_{a}}{h_{a}},$
    |  | (9) |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle t_{x}^{p}$ | $\displaystyle=\frac{x_{p}-x_{a}}{w_{a}},t_{y}^{p}=\frac{y_{p}-y_{a}}{h_{a}},$
    |  | (9) |'
- en: '|  | $\displaystyle t_{w}^{p}$ | $\displaystyle=\log\frac{w_{p}}{w_{a}},t_{h}^{p}=\log\frac{h_{p}}{h_{a}},t_{\theta}^{p}=f\left(\frac{\theta_{p}-\theta_{a}}{\pi}\right)$
    |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle t_{w}^{p}$ | $\displaystyle=\log\frac{w_{p}}{w_{a}},t_{h}^{p}=\log\frac{h_{p}}{h_{a}},t_{\theta}^{p}=f\left(\frac{\theta_{p}-\theta_{a}}{\pi}\right)$
    |  |'
- en: 'where $b_{p}=(x_{p},y_{p},w_{p},h_{p},\theta_{p})$ denotes the predicted OBB.
    $f(\cdot)$ is used to ensure that the angle difference stays within the preset
    range, thus avoiding the impact of PoA. Similarly, the GT offsets are denoted
    by:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b_{p}=(x_{p},y_{p},w_{p},h_{p},\theta_{p})$ 表示预测的 OBB。$f(\cdot)$ 用于确保角度差异保持在预设范围内，从而避免
    PoA 的影响。类似地，GT 偏移量表示为：
- en: '|  | $\displaystyle t_{x}^{g}$ | $\displaystyle=\frac{x_{g}-x_{a}}{w_{a}},t_{y}^{g}=\frac{y_{g}-y_{a}}{h_{a}},$
    |  | (10) |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle t_{x}^{g}$ | $\displaystyle=\frac{x_{g}-x_{a}}{w_{a}},t_{y}^{g}=\frac{y_{g}-y_{a}}{h_{a}},$
    |  | (10) |'
- en: '|  | $\displaystyle t_{w}^{g}$ | $\displaystyle=\log\frac{w_{g}}{w_{a}},t_{h}^{g}=\log\frac{h_{g}}{h_{a}},t_{\theta}^{g}=f\left(\frac{\theta_{g}-\theta_{a}}{\pi}\right)$
    |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle t_{w}^{g}$ | $\displaystyle=\log\frac{w_{g}}{w_{a}},t_{h}^{g}=\log\frac{h_{g}}{h_{a}},t_{\theta}^{g}=f\left(\frac{\theta_{g}-\theta_{a}}{\pi}\right)$
    |  |'
- en: 'where $b_{g}=(x_{g},y_{g},w_{g},h_{g},\theta_{g})$ denotes the GT OBB. Hence,
    the objective function for the regression task is:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b_{g}=(x_{g},y_{g},w_{g},h_{g},\theta_{g})$ 表示 GT OBB。因此，回归任务的目标函数为：
- en: '|  | $L_{reg}=\sum_{i\in\{x,y,w,h,\theta\}}L_{n}(t_{i}^{p}-t_{i}^{g})$ |  |
    (11) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{reg}=\sum_{i\in\{x,y,w,h,\theta\}}L_{n}(t_{i}^{p}-t_{i}^{g})$ |  |
    (11) |'
- en: where $L_{n}(\cdot)$ denotes the $L_{n}$ norm and the smooth $L_{1}$ loss (Girshick,
    [2015](#bib.bib40)) is widely adopted. Due to the PoA (Qian et al., [2021](#bib.bib112),
    [2022](#bib.bib113); Yang et al., [2021c](#bib.bib176), [2022b](#bib.bib179)),
    the OBB regression will encounter the following challenges.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L_{n}(\cdot)$ 表示 $L_{n}$ 范数，平滑的 $L_{1}$ 损失（Girshick，[2015](#bib.bib40)）被广泛采用。由于
    PoA（Qian 等，[2021](#bib.bib112)，[2022](#bib.bib113)；Yang 等，[2021c](#bib.bib176)，[2022b](#bib.bib179)），OBB
    回归将面临以下挑战。
- en: 5.1.1 Inconsistency between Metric and Loss
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 度量和损失之间的不一致
- en: 'Although the majority of detectors adopt the smooth L1 loss as the objective
    function of regression, the most commonly used metric for localization is RIoU,
    which is presented in Section [2](#S2 "2 Problem Definition ‣ Oriented Object
    Detection in Optical Remote Sensing Images using Deep Learning: A Survey"). Therefore,
    there is an inconsistency between the loss function and the evaluation metric,
    and an optimum choice for the regression task may not guarantee a high localization
    accuracy in terms of RIoU. What’s more, a good regression loss function should
    take into account the central point distance, aspect ratio, and overlap area,
    which has been demonstrated to be effective in horizontal object detection (Rezatofighi
    et al., [2019](#bib.bib119); Zheng et al., [2020](#bib.bib203)). However, the
    aspect ratio and the overlap area can be disregarded by the smooth L1 loss easily.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管大多数检测器采用平滑 L1 损失作为回归的目标函数，但最常用的定位度量是 RIoU，这在第 [2](#S2 "2 Problem Definition
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey") 节中介绍。因此，损失函数和评估指标之间存在不一致，并且回归任务的最佳选择可能无法保证在 RIoU 方面的高定位精度。此外，一个好的回归损失函数应考虑中心点距离、长宽比和重叠区域，这已被证明在水平目标检测中有效（Rezatofighi
    等，[2019](#bib.bib119)；Zheng 等，[2020](#bib.bib203)）。然而，平滑 L1 损失很容易忽略长宽比和重叠区域。'
- en: '![Refer to caption](img/dde064ffa9aebab3ee475f32d8a42fff.png)![Refer to caption](img/266435ecc8a8df1e94dba3523a3c75a8.png)![Refer
    to caption](img/c7f28c57c6308ede9ef0a977a36e4ba0.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dde064ffa9aebab3ee475f32d8a42fff.png)![参见说明](img/266435ecc8a8df1e94dba3523a3c75a8.png)![参见说明](img/c7f28c57c6308ede9ef0a977a36e4ba0.png)'
- en: 'Figure 11: Comparison between metric and loss (Qian et al., [2021](#bib.bib112),
    [2022](#bib.bib113); Yang et al., [2021c](#bib.bib176)). (a) A sketch of RIoU
    change caused by angle and aspect ratio (AR) variation. (b) and (c) depict the
    changes of the regression loss and RIoU with aspect ratio and angle difference,
    respectively.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：度量和损失之间的比较（Qian 等，[2021](#bib.bib112)，[2022](#bib.bib113)；Yang 等，[2021c](#bib.bib176)）。(a)
    角度和长宽比（AR）变化引起的 RIoU 变化示意图。(b) 和 (c) 分别描绘了回归损失和 RIoU 随长宽比和角度差异的变化。
- en: 'We illustrate the inconsistency between metric and loss in Figure [11](#S5.F11
    "Figure 11 ‣ 5.1.1 Inconsistency between Metric and Loss ‣ 5.1 𝜃-based representation
    ‣ 5 OBB Representations ‣ Oriented Object Detection in Optical Remote Sensing
    Images using Deep Learning: A Survey"). As shown in Figure [11](#S5.F11 "Figure
    11 ‣ 5.1.1 Inconsistency between Metric and Loss ‣ 5.1 𝜃-based representation
    ‣ 5 OBB Representations ‣ Oriented Object Detection in Optical Remote Sensing
    Images using Deep Learning: A Survey"), the top and the bottom rows have different
    angle differences, while the aspect ratio of the OBBs on the left is different
    from those on the right. Meanwhile, the center points, width and height of the
    four cases are the same. The orange area denotes the IoU between OBBs. Note that
    the regression loss is sensitive to angle variances but remains unchanged for
    different aspect ratios. Specifically, when the aspect ratio varies, the union
    of two OBBs will change but the intersection is constant, causing the change of
    RIoU. The same conclusion can be drawn from Figure [11](#S5.F11 "Figure 11 ‣ 5.1.1
    Inconsistency between Metric and Loss ‣ 5.1 𝜃-based representation ‣ 5 OBB Representations
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey"), which shows the variation curves of the RIoU and smooth L1 loss with
    respect to aspect ratio under different angle differences. Note that the RIoU
    changes drastically but the smooth L1 loss remains constant. Furthermore, Figure
    [11](#S5.F11 "Figure 11 ‣ 5.1.1 Inconsistency between Metric and Loss ‣ 5.1 𝜃-based
    representation ‣ 5 OBB Representations ‣ Oriented Object Detection in Optical
    Remote Sensing Images using Deep Learning: A Survey") shows the variation curve
    of RIoU and smooth L1 loss with respect to the angle under different aspect ratios.
    In the neighborhood of 0, both losses are consistent in monotonicity but not in
    convexity. The RIoU changes more intensely than the smooth L1 loss when the angle
    difference is close to zero.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[11](#S5.F11 "图 11 ‣ 5.1.1 度量与损失之间的不一致 ‣ 5.1 𝜃-基表示 ‣ 5 OBB 表示 ‣ 使用深度学习进行光学遥感图像中的定向目标检测：综述")中展示了度量与损失之间的不一致。如图[11](#S5.F11
    "图 11 ‣ 5.1.1 度量与损失之间的不一致 ‣ 5.1 𝜃-基表示 ‣ 5 OBB 表示 ‣ 使用深度学习进行光学遥感图像中的定向目标检测：综述")所示，上下两行的角度差异不同，而左侧的OBBs的长宽比与右侧的不同。同时，四种情况的中心点、宽度和高度是相同的。橙色区域表示OBBs之间的IoU。请注意，回归损失对角度变化敏感，但对不同的长宽比保持不变。具体而言，当长宽比变化时，两OBBs的并集会变化，但交集保持不变，从而导致RIoU的变化。从图[11](#S5.F11
    "图 11 ‣ 5.1.1 度量与损失之间的不一致 ‣ 5.1 𝜃-基表示 ‣ 5 OBB 表示 ‣ 使用深度学习进行光学遥感图像中的定向目标检测：综述")中可以得出相同的结论，该图展示了在不同角度差异下，RIoU和光滑L1损失随长宽比变化的曲线。请注意，RIoU变化剧烈，但光滑L1损失保持不变。此外，图[11](#S5.F11
    "图 11 ‣ 5.1.1 度量与损失之间的不一致 ‣ 5.1 𝜃-基表示 ‣ 5 OBB 表示 ‣ 使用深度学习进行光学遥感图像中的定向目标检测：综述")展示了在不同长宽比下，RIoU和光滑L1损失随角度变化的曲线。在接近0的邻域中，两种损失在单调性上是一致的，但在凸性上则不一致。当角度差异接近零时，RIoU的变化比光滑L1损失更为剧烈。
- en: 'Given the above limitations of the smooth L1 loss, numerous IoU-induced loss
    functions are proposed in horizontal detectors, such as GIoU (Rezatofighi et al.,
    [2019](#bib.bib119)) and DIoU (Zheng et al., [2020](#bib.bib203)). However, these
    IoU-induced losses cannot be incorporated directly into oriented object detection,
    because of the in-differentiable nature of RIoU (Yang et al., [2021c](#bib.bib176)).
    To tackle this problem, several differentiable functions are designed to approximate
    RIoU loss (Yang et al., [2021b](#bib.bib174), [2019a](#bib.bib177), [2022a](#bib.bib175)).
    Chen et al. ([2020b](#bib.bib15)) proposed PIoU by introducing a differentiable
    kernel function, which accumulates the contribution of interior overlapping pixels
    to approximate the intersection area. Other solutions (Yang et al., [2019a](#bib.bib177),
    [2022a](#bib.bib175), [2021b](#bib.bib174)) integrated RIoU as a weight of the
    smooth L1 loss:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于光滑L1损失的上述局限性，许多由IoU引发的损失函数在水平检测器中被提出，如GIoU (Rezatofighi et al., [2019](#bib.bib119))
    和DIoU (Zheng et al., [2020](#bib.bib203))。然而，这些IoU引发的损失不能直接纳入定向目标检测，因为RIoU的不可微分特性
    (Yang et al., [2021c](#bib.bib176))。为了解决这个问题，设计了几种可微分函数来逼近RIoU损失 (Yang et al.,
    [2021b](#bib.bib174), [2019a](#bib.bib177), [2022a](#bib.bib175))。Chen et al.
    ([2020b](#bib.bib15)) 提出了PIoU，通过引入可微分的核函数来累积内部重叠像素的贡献，以逼近交集区域。其他解决方案 (Yang et
    al., [2019a](#bib.bib177), [2022a](#bib.bib175), [2021b](#bib.bib174)) 将RIoU集成作为光滑L1损失的权重：
- en: '|  | $L_{RIoU}=\frac{L_{reg}}{\left&#124;L_{reg}\right&#124;}\cdot\left&#124;g(RIoU)\right&#124;$
    |  | (12) |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{RIoU}=\frac{L_{reg}}{\left|L_{reg}\right|}\cdot\left|g(RIoU)\right|$
    |  | (12) |'
- en: 'The regression loss $L_{reg}$ is defined in Eq. [11](#S5.E11 "In 5.1 𝜃-based
    representation ‣ 5 OBB Representations ‣ Oriented Object Detection in Optical
    Remote Sensing Images using Deep Learning: A Survey") and adopts the smooth L1
    loss. $g(\cdot)$ is a loss function related to RIoU, e.g. $log(\cdot)$. Compared
    with the smooth L1 loss, the IoU-smooth L1 loss can be divided into two parts,
    a normalized regression loss $\frac{L_{reg}}{\left|L_{reg}\right|}$ controlling
    the direction of gradient propagation, and a scalar $\left|-\log RIoU\right|$
    adjusting gradient magnitude. When the RIoU is close to 1, $g(RIoU)\approx 0$,
    and $L_{reg}$ is approximately equal to 0, which can effectively alleviate the
    inconsistency between the metric and regression loss. However, angle regression
    still suffers from the problem of PoA. Therefore, the loss functions can only
    alleviate the impact of the problem, which is not theoretically solved.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 回归损失$L_{reg}$在公式[11](#S5.E11 "在5.1 𝜃-based表示 ‣ 5 OBB表示 ‣ 使用深度学习进行光学遥感图像中的定向目标检测：综述")中定义，并采用平滑L1损失。$g(\cdot)$是与RIoU相关的损失函数，例如$log(\cdot)$。与平滑L1损失相比，IoU-平滑L1损失可以分为两部分：一个归一化的回归损失$\frac{L_{reg}}{\left|L_{reg}\right|}$用于控制梯度传播方向，和一个标量$\left|-\log
    RIoU\right|$用于调整梯度幅度。当RIoU接近1时，$g(RIoU)\approx 0$，且$L_{reg}$大约等于0，这可以有效缓解度量与回归损失之间的不一致。然而，角回归仍然面临PoA的问题。因此，损失函数只能缓解这一问题的影响，尚未从理论上解决。
- en: 5.1.2 Angular Boundary Discontinuity and Square-like Problem
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 角界面不连续性和类似方形的问题
- en: '![Refer to caption](img/c20b4fd5a0b9f35132f0a643e6258297.png)![Refer to caption](img/cbac1d8673d157f76ae8626558cc141e.png)![Refer
    to caption](img/d115d61c90d3b4e342dda1bda6726957.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/c20b4fd5a0b9f35132f0a643e6258297.png)![参见图注](img/cbac1d8673d157f76ae8626558cc141e.png)![参见图注](img/d115d61c90d3b4e342dda1bda6726957.png)'
- en: 'Figure 12: Illustration of angular boundary discontinuity (Yang et al., [2021c](#bib.bib176)).
    The predicted and GT OBB are represented by green and blue, respectively. (a)
    The ideal form of OBB representation. The two OBBs only differ slightly in terms
    of the angle and center point. (b) OBB representation with OpenCV definition,
    encountering PoA and exchangeability of edges (EoE). (c) OBB representation with
    long edge definition, encountering a significant angle difference.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：角界面不连续性的示意图（杨等，[2021c](#bib.bib176)）。预测的和GT的OBB分别用绿色和蓝色表示。(a) OBB表示的理想形式。两个OBB仅在角度和中心点上有微小差异。(b)
    使用OpenCV定义的OBB表示，遇到边缘的PoA和可交换性（EoE）。(c) 使用长边定义的OBB表示，遇到显著的角度差异。
- en: 'Because of the PoA problem (Yang et al., [2021c](#bib.bib176), [d](#bib.bib178),
    [2022](#bib.bib180); Qian et al., [2021](#bib.bib112), [2022](#bib.bib113)), the
    smooth L1 loss suffers from the problem of angular boundary discontinuity, which
    is illustrated in Figure [12](#S5.F12 "Figure 12 ‣ 5.1.2 Angular Boundary Discontinuity
    and Square-like Problem ‣ 5.1 𝜃-based representation ‣ 5 OBB Representations ‣
    Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey"). Specifically, a small angle difference may cause a large loss change
    when the angular value approaches the angular boundary range. Figure [12](#S5.F12
    "Figure 12 ‣ 5.1.2 Angular Boundary Discontinuity and Square-like Problem ‣ 5.1
    𝜃-based representation ‣ 5 OBB Representations ‣ Oriented Object Detection in
    Optical Remote Sensing Images using Deep Learning: A Survey") illustrates an ideal
    OBB representation, where the predicted and GT OBB only differ slightly in terms
    of the angle and center point. For OBBs with OpenCV definition, the angular value
    must be an acute or right angle, i.e. $\theta\in(0,\frac{\pi}{2}]$, as shown in
    Figure [12](#S5.F12 "Figure 12 ‣ 5.1.2 Angular Boundary Discontinuity and Square-like
    Problem ‣ 5.1 𝜃-based representation ‣ 5 OBB Representations ‣ Oriented Object
    Detection in Optical Remote Sensing Images using Deep Learning: A Survey"). As
    a result, the angle difference between the two OBBs increases sharply as the angular
    value is close to 0 or $\frac{\pi}{2}$. In addition, the width of the predicted
    OBB is the short edge, whereas the width of the GT OBB is the long edge, causing
    a significant regression loss of width and height. For OBBs under long edge definition
    with the angular range of $[-\frac{\pi}{2},\frac{\pi}{2})$, the angular boundary
    discontinuity leads to a significant angle difference, i.e. $|\theta_{g}-\theta_{p}|\approx\pi$,
    as shown in Figure [12](#S5.F12 "Figure 12 ‣ 5.1.2 Angular Boundary Discontinuity
    and Square-like Problem ‣ 5.1 𝜃-based representation ‣ 5 OBB Representations ‣
    Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey"). This problem will also occur in long edge definition with the angular
    range of $[-\frac{\pi}{4},\frac{3\pi}{4})$ when the angular value is close to
    $-\frac{\pi}{4}$ or $\frac{3\pi}{4}$.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PoA问题（Yang等人，[2021c](#bib.bib176)，[d](#bib.bib178)，[2022](#bib.bib180)；Qian等人，[2021](#bib.bib112)，[2022](#bib.bib113)），平滑L1损失存在角边界不连续的问题，如图[12](#S5.F12
    "图 12 ‣ 5.1.2 角边界不连续性和方形问题 ‣ 5.1 𝜃-基表示 ‣ 5 OBB表示 ‣ 使用深度学习的光学遥感图像中的定向物体检测：综述")所示。具体而言，当角值接近角边界范围时，小的角度差异可能会导致损失变化较大。图[12](#S5.F12
    "图 12 ‣ 5.1.2 角边界不连续性和方形问题 ‣ 5.1 𝜃-基表示 ‣ 5 OBB表示 ‣ 使用深度学习的光学遥感图像中的定向物体检测：综述")展示了一个理想的OBB表示，其中预测的OBB和GT
    OBB在角度和中心点方面仅有微小差异。对于OpenCV定义的OBB，角值必须是锐角或直角，即$\theta\in(0,\frac{\pi}{2}]$，如图[12](#S5.F12
    "图 12 ‣ 5.1.2 角边界不连续性和方形问题 ‣ 5.1 𝜃-基表示 ‣ 5 OBB表示 ‣ 使用深度学习的光学遥感图像中的定向物体检测：综述")所示。因此，当角值接近0或$\frac{\pi}{2}$时，两OBB之间的角度差异急剧增加。此外，预测OBB的宽度是短边，而GT
    OBB的宽度是长边，导致宽度和高度的回归损失显著。对于长边定义下角度范围为$[-\frac{\pi}{2},\frac{\pi}{2})$的OBB，角边界不连续性会导致显著的角度差异，即$|\theta_{g}-\theta_{p}|\approx\pi$，如图[12](#S5.F12
    "图 12 ‣ 5.1.2 角边界不连续性和方形问题 ‣ 5.1 𝜃-基表示 ‣ 5 OBB表示 ‣ 使用深度学习的光学遥感图像中的定向物体检测：综述")所示。当角度值接近$-\frac{\pi}{4}$或$\frac{3\pi}{4}$时，角度范围为$[-\frac{\pi}{4},\frac{3\pi}{4})$的长边定义下也会出现此问题。
- en: 'Additionally, for square-like objects, including storage-tank and roundabouts,
    the long edge definition will encounter a so-called square-like problem due to
    the difference of angle parameters  (Yang et al., [2021c](#bib.bib176), [d](#bib.bib178),
    [2022](#bib.bib180)). As shown in Figure [13](#S5.F13 "Figure 13 ‣ 5.1.2 Angular
    Boundary Discontinuity and Square-like Problem ‣ 5.1 𝜃-based representation ‣
    5 OBB Representations ‣ Oriented Object Detection in Optical Remote Sensing Images
    using Deep Learning: A Survey"), when the aspect ratio is close to 1 but the length
    and width of the predicted OBB are opposite to that of GT, the corresponding angle
    will differ by about $\frac{\pi}{2}$, leading to a large regression loss even
    if the RIoU is about 1.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，对于类似方形的物体，包括储罐和环形交叉口，由于角度参数的差异，长边定义会遇到所谓的方形问题（Yang et al., [2021c](#bib.bib176),
    [d](#bib.bib178), [2022](#bib.bib180)）。如图 [13](#S5.F13 "Figure 13 ‣ 5.1.2 Angular
    Boundary Discontinuity and Square-like Problem ‣ 5.1 𝜃-based representation ‣
    5 OBB Representations ‣ Oriented Object Detection in Optical Remote Sensing Images
    using Deep Learning: A Survey") 所示，当纵横比接近 1 但预测 OBB 的长度和宽度与 GT 相反时，相应的角度会相差约 $\frac{\pi}{2}$，即使
    RIoU 约为 1，也会导致较大的回归损失。'
- en: '![Refer to caption](img/0764d7f1f800756f96931b5a2b9cb00e.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0764d7f1f800756f96931b5a2b9cb00e.png)'
- en: (a) GT OBB
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GT OBB
- en: '![Refer to caption](img/b196942ac1947061b05c37ab7a063940.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b196942ac1947061b05c37ab7a063940.png)'
- en: (b) Predicted OBB
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 预测的 OBB
- en: 'Figure 13: Illustration of the square-like problem (Yang et al., [2021c](#bib.bib176),
    [a](#bib.bib170)).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：方形问题示意（Yang et al., [2021c](#bib.bib176), [a](#bib.bib170)）。
- en: 'Both boundary discontinuity and square-like problem can seriously confuse the
    network, leading to training instability. Thus, several methods have been proposed
    to address these issues, which can be divided into three types:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 边界不连续性和方形问题都可能严重困扰网络，导致训练不稳定。因此，已经提出了几种方法来解决这些问题，可以分为三种类型：
- en: 'Modulated rotated loss (Qian et al., [2021](#bib.bib112), [2022](#bib.bib113)).
    The modulated Rotated loss is designed for the OBB representation under the OpenCV
    definition, which adds an extra loss item based on the naive regression loss $L_{reg}$
    to eliminate the boundary discontinuity. Specifically, it can be expressed as
    follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 调制旋转损失（Qian et al., [2021](#bib.bib112), [2022](#bib.bib113)）。调制旋转损失是为 OpenCV
    定义下的 OBB 表示设计的，它在简单回归损失 $L_{reg}$ 基础上添加了额外的损失项，以消除边界不连续性。具体来说，可以表达为：
- en: '|  | $L_{mr}=\min\left\{L_{reg},L_{reg-EoE}\right\}$ |  | (13) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{mr}=\min\left\{L_{reg},L_{reg-EoE}\right\}$ |  | (13) |'
- en: 'Here, $L_{reg}$ and $L_{reg-EoE}$ can be calculated as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$L_{reg}$ 和 $L_{reg-EoE}$ 可以按如下方式计算：
- en: '|  | $\displaystyle L_{reg}=$ | $\displaystyle\sum_{i\in\{x,y,w,h,\theta\}}L_{1}(t_{i}^{p}-t_{i}^{g})$
    |  | (14) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{reg}=$ | $\displaystyle\sum_{i\in\{x,y,w,h,\theta\}}L_{1}(t_{i}^{p}-t_{i}^{g})$
    |  | (14) |'
- en: '|  | $\displaystyle L_{reg-EoE}=$ | $\displaystyle\sum_{j=\{x,y\}}L_{1}(t_{j}^{p},t_{j}^{g})+L_{1}(t_{w}^{p},t_{h}^{g}+log(r_{a}))$
    |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{reg-EoE}=$ | $\displaystyle\sum_{j=\{x,y\}}L_{1}(t_{j}^{p},t_{j}^{g})+L_{1}(t_{w}^{p},t_{h}^{g}+log(r_{a}))$
    |  |'
- en: '|  |  | $\displaystyle+L_{1}(t_{h}^{p}+log(r_{a}),t_{w}^{g})+\left&#124;L_{1}(t_{\theta}^{p},t_{\theta}^{g})-\frac{\pi}{2}\right&#124;$
    |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+L_{1}(t_{h}^{p}+log(r_{a}),t_{w}^{g})+\left\|L_{1}(t_{\theta}^{p},t_{\theta}^{g})-\frac{\pi}{2}\right\|$
    |  |'
- en: 'where $L_{1}(\cdot)$ represents the L1 loss function. The definitions of $\{t_{j}^{p}\}_{j=x,y,w,h,\theta}$
    and $\{t_{j}^{g}\}_{j=x,y,w,h,\theta}$ are the same as the Eq. [9](#S5.E9 "In
    5.1 𝜃-based representation ‣ 5 OBB Representations ‣ Oriented Object Detection
    in Optical Remote Sensing Images using Deep Learning: A Survey") and Eq. [10](#S5.E10
    "In 5.1 𝜃-based representation ‣ 5 OBB Representations ‣ Oriented Object Detection
    in Optical Remote Sensing Images using Deep Learning: A Survey") respectively.
    $r_{a}=\frac{h_{a}}{w_{a}}$ represents the aspect ratio. The motivation for adding
    a term $L_{reg-EoE}$ is to exchange the edges and eliminate the PoA problems.
    When the angular value is close to the boundary range, $L_{reg}$ may increase
    suddenly, which is much larger than $L_{reg-EoE}$. Therefore, $L_{reg-EoE}$ can
    effectively eliminate the boundary discontinuity, making the loss continuous.
    However, although the modulated loss can guarantee loss continuous, the gap between
    metric and loss still exists.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $L_{1}(\cdot)$ 代表 L1 损失函数。$\{t_{j}^{p}\}_{j=x,y,w,h,\theta}$ 和 $\{t_{j}^{g}\}_{j=x,y,w,h,\theta}$
    的定义与 Eq. [9](#S5.E9 "In 5.1 𝜃-based representation ‣ 5 OBB Representations ‣ Oriented
    Object Detection in Optical Remote Sensing Images using Deep Learning: A Survey")
    和 Eq. [10](#S5.E10 "In 5.1 𝜃-based representation ‣ 5 OBB Representations ‣ Oriented
    Object Detection in Optical Remote Sensing Images using Deep Learning: A Survey")
    相同。$r_{a}=\frac{h_{a}}{w_{a}}$ 代表长宽比。加入 $L_{reg-EoE}$ 项的动机是交换边界并消除 PoA 问题。当角度值接近边界范围时，$L_{reg}$
    可能会突然增加，这比 $L_{reg-EoE}$ 大得多。因此，$L_{reg-EoE}$ 可以有效消除边界不连续性，使损失连续。然而，尽管调制损失可以保证损失连续性，但度量和损失之间的差距仍然存在。'
- en: Angle coder (Yang and Yan, [2020](#bib.bib173); Yang et al., [2021a](#bib.bib170);
    Yu and Da, [2022](#bib.bib188)). Yang and Yan ([2020](#bib.bib173)) proposed a
    new baseline by transforming the angular regression task into a classification
    problem. The angle is discretized into a certain number of intervals, and then
    a discrete angle is predicted by classification. Then, the circular smooth label
    (CSL) technique is designed, which contains a circular label encoding with periodicity
    to handle the PoA problem. The label smooth technique is adopted using a window
    function to increase the error tolerance to adjacent angles. Although CSL eliminates
    the boundary discontinuity, its heavy prediction layer harms the efficiency and
    the square-like problem remains unsolved. To tackle these issues, Yang et al.
    ([2021a](#bib.bib170)) further adopted Densely Coded Labels (DCL) by introducing
    a lightweight prediction layer, which reduces the code length. DCL achieved a
    notable improvement in terms of both accuracy and speed. Moreover, Angle Distance
    and Aspect Ratio Sensitive Weighting (ADARSW) were designed to improve the accuracy
    of square-like objects. However, the hyper-parameters have a significant impact
    on the performance of CSL and DCL. Even worse, the optimal settings on different
    datasets are also different. Hence, laborious tuning is required to achieve the
    best performance on different datasets. To solve this problem, Yu and Da ([2022](#bib.bib188))
    designed a differentiable angle coder, named Phase-Shifting Coder (PSC). PSC encodes
    the angle into a periodic phase to solve the boundary discontinuity problem. Moreover,
    to further solve square-like problem, PSCD was proposed by mapping the angle into
    the phases of different frequencies, which is the advanced dual-frequency version
    of PSC.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 角度编码器（杨和颜，[2020](#bib.bib173)；杨等，[2021a](#bib.bib170)；余和达，[2022](#bib.bib188)）。杨和颜（[2020](#bib.bib173)）通过将角度回归任务转化为分类问题提出了一种新的基线。角度被离散化为一定数量的区间，然后通过分类预测一个离散角度。随后，设计了圆形平滑标签（CSL）技术，它包含具有周期性的圆形标签编码来处理
    PoA 问题。使用窗口函数的标签平滑技术增加了对邻近角度的误差容忍度。尽管 CSL 消除了边界不连续性，但其繁重的预测层损害了效率，类似方形的问题仍未解决。为了解决这些问题，杨等人（[2021a](#bib.bib170)）进一步采用了稠密编码标签（DCL），通过引入轻量级预测层，减少了代码长度。DCL
    在准确性和速度方面取得了显著改进。此外，设计了角度距离和长宽比敏感加权（ADARSW）来提高方形对象的准确性。然而，超参数对 CSL 和 DCL 的性能有显著影响。更糟糕的是，不同数据集上的最佳设置也不同。因此，需要费力的调优才能在不同数据集上获得最佳性能。为了解决这个问题，余和达（[2022](#bib.bib188)）设计了一种可微分的角度编码器，称为相位位移编码器（PSC）。PSC
    将角度编码为周期性相位，以解决边界不连续性问题。此外，为了进一步解决方形问题，提出了 PSCD 通过将角度映射到不同频率的相位中，这是 PSC 的高级双频版本。
- en: 'Gaussian distribution based methods (Yang et al., [2021c](#bib.bib176), [d](#bib.bib178),
    [2022](#bib.bib180), [2022b](#bib.bib179)). The Gaussian distribution based methods
    provide a unified and elegant solution to the problems of boundary discontinuity
    and the square-like issue. Firstly, the OBB $b=(x,y,w,h,\theta)$ is converted
    to a 2-D Gaussian distribution $\mathcal{N}(m,\Sigma)$ by the following formula:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 基于高斯分布的方法（杨等，[2021c](#bib.bib176)，[d](#bib.bib178)，[2022](#bib.bib180)，[2022b](#bib.bib179)）。基于高斯分布的方法提供了一个统一而优雅的解决方案，解决了边界不连续和类似方形的问题。首先，将
    OBB $b=(x,y,w,h,\theta)$ 转换为二维高斯分布 $\mathcal{N}(m,\Sigma)$，其公式如下：
- en: '|  | $\displaystyle m$ | $\displaystyle=(x,y)$ |  | (15) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle m$ | $\displaystyle=(x,y)$ |  | (15) |'
- en: '|  | $\displaystyle\Sigma^{\frac{1}{2}}$ | $\displaystyle=R\Lambda R^{T}$ |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Sigma^{\frac{1}{2}}$ | $\displaystyle=R\Lambda R^{T}$ |  |'
- en: '|  |  | <math   alttext="\displaystyle=\left[\begin{array}[]{cc}\cos{\theta}&amp;-\sin{\theta}\\
    \sin{\theta}&amp;\cos{\theta}\end{array}\right]\left[\begin{array}[]{cc}\frac{w}{2}&amp;0\\'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '|  |  | <math   alttext="\displaystyle=\left[\begin{array}[]{cc}\cos{\theta}&amp;-\sin{\theta}\\
    \sin{\theta}&amp;\cos{\theta}\end{array}\right]\left[\begin{array}[]{cc}\frac{w}{2}&amp;0\\'
- en: 0&amp;\frac{h}{2}\end{array}\right]\left[\begin{array}[]{cc}\cos{\theta}&amp;\sin{\theta}\\
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 0&amp;\frac{h}{2}\end{array}\right]\left[\begin{array}[]{cc}\cos{\theta}&amp;\sin{\theta}\\
- en: -\sin{\theta}&amp;\cos{\theta}\end{array}\right]" display="inline"><semantics
    ><mrow  ><mo >=</mo><mrow ><mrow  ><mo >[</mo><mtable columnspacing="5pt" rowspacing="0pt"  ><mtr
    ><mtd ><mrow  ><mi >cos</mi><mo lspace="0.167em"  >⁡</mo><mi >θ</mi></mrow></mtd><mtd
    ><mrow ><mo rspace="0.167em" >−</mo><mrow ><mi >sin</mi><mo lspace="0.167em" >⁡</mo><mi
    >θ</mi></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow  ><mi >sin</mi><mo lspace="0.167em"  >⁡</mo><mi
    >θ</mi></mrow></mtd><mtd ><mrow ><mi >cos</mi><mo lspace="0.167em" >⁡</mo><mi
    >θ</mi></mrow></mtd></mtr></mtable><mo >]</mo></mrow><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo >[</mo><mtable columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd  ><mfrac
    ><mi >w</mi><mn >2</mn></mfrac></mtd><mtd ><mn  >0</mn></mtd></mtr><mtr ><mtd
    ><mn  >0</mn></mtd><mtd ><mfrac ><mi >h</mi><mn >2</mn></mfrac></mtd></mtr></mtable><mo
    >]</mo></mrow><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo >[</mo><mtable
    columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd  ><mrow ><mi >cos</mi><mo lspace="0.167em"
    >⁡</mo><mi >θ</mi></mrow></mtd><mtd ><mrow  ><mi >sin</mi><mo lspace="0.167em"  >⁡</mo><mi
    >θ</mi></mrow></mtd></mtr><mtr ><mtd ><mrow  ><mo rspace="0.167em"  >−</mo><mrow
    ><mi >sin</mi><mo lspace="0.167em"  >⁡</mo><mi >θ</mi></mrow></mrow></mtd><mtd
    ><mrow ><mi >cos</mi><mo lspace="0.167em" >⁡</mo><mi >θ</mi></mrow></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply
    ><csymbol cd="latexml" >absent</csymbol><apply ><apply  ><csymbol cd="latexml"  >delimited-[]</csymbol><matrix
    ><matrixrow ><apply  ><ci >𝜃</ci></apply><apply ><apply ><ci >𝜃</ci></apply></apply></matrixrow><matrixrow
    ><apply ><ci >𝜃</ci></apply><apply ><ci >𝜃</ci></apply></matrixrow></matrix></apply><apply
    ><csymbol cd="latexml"  >delimited-[]</csymbol><matrix ><matrixrow ><apply  ><ci
    >𝑤</ci><cn type="integer"  >2</cn></apply><cn type="integer"  >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn><apply ><ci >ℎ</ci><cn type="integer" >2</cn></apply></matrixrow></matrix></apply><apply
    ><csymbol cd="latexml"  >delimited-[]</csymbol><matrix ><matrixrow ><apply  ><ci
    >𝜃</ci></apply><apply ><ci >𝜃</ci></apply></matrixrow><matrixrow ><apply  ><apply
    ><ci >𝜃</ci></apply></apply><apply ><ci >𝜃</ci></apply></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle=\left[\begin{array}[]{cc}\cos{\theta}&-\sin{\theta}\\
    \sin{\theta}&\cos{\theta}\end{array}\right]\left[\begin{array}[]{cc}\frac{w}{2}&0\\
    0&\frac{h}{2}\end{array}\right]\left[\begin{array}[]{cc}\cos{\theta}&\sin{\theta}\\
    -\sin{\theta}&\cos{\theta}\end{array}\right]</annotation></semantics></math> |  |
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: $\displaystyle=\left[\begin{array}[]{cc}\cos{\theta}&-\sin{\theta}\\ \sin{\theta}&\cos{\theta}\end{array}\right]\left[\begin{array}[]{cc}\frac{w}{2}&0\\
    0&\frac{h}{2}\end{array}\right]\left[\begin{array}[]{cc}\cos{\theta}&\sin{\theta}\\
    -\sin{\theta}&\cos{\theta}\end{array}\right]$
- en: '|  |  | $\displaystyle=\left[\begin{array}[]{cc}\frac{w}{2}\cos^{2}{\theta}+\frac{h}{2}\sin^{2}{\theta}&amp;\frac{w-h}{2}\cos{\theta}\sin{\theta}\\
    \frac{w-h}{2}\cos{\theta}\sin{\theta}&amp;\frac{h}{2}\cos^{2}{\theta}+\frac{w}{2}\sin^{2}{\theta}\end{array}\right]$
    |  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: $\displaystyle=\left[\begin{array}[]{cc}\frac{w}{2}\cos^{2}{\theta}+\frac{h}{2}\sin^{2}{\theta}&\frac{w-h}{2}\cos{\theta}\sin{\theta}\\
    \frac{w-h}{2}\cos{\theta}\sin{\theta}&\frac{h}{2}\cos^{2}{\theta}+\frac{w}{2}\sin^{2}{\theta}\end{array}\right]$
- en: 'where $R$ and $\Lambda$ represent the rotation matrix and the diagonal matrix
    of eigenvalues, respectively. As shown in Eq. [15](#S5.E15 "In 5.1.2 Angular Boundary
    Discontinuity and Square-like Problem ‣ 5.1 𝜃-based representation ‣ 5 OBB Representations
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey"), the angle and the aspect ratio are encoded into the $\Lambda$ matrix
    in the Gaussian distribution based method. Therefore, the merit is that the problems
    of PoA and boundary discontinuity are efficiently solved. As a result, the exchangeability
    of edges (EoE) and the square-like problem are eliminated.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $R$ 和 $\Lambda$ 分别表示旋转矩阵和特征值对角矩阵。如在公式 [15](#S5.E15 "在 5.1.2 角度边界不连续性和类似方形问题
    ‣ 5.1 𝜃 基础表示 ‣ 5 OBB 表示 ‣ 使用深度学习的光学遥感图像中的有向目标检测：综述") 所示，角度和长宽比被编码到高斯分布方法中的 $\Lambda$
    矩阵中。因此，其优点是有效解决了 PoA 和边界不连续性问题。结果，边缘可交换性（EoE）和类似方形问题被消除。
- en: 'Then, a distance function is used to measure two Gaussian distributions, such
    as Gaussian Wasserstein Distance (GWD) (Yang et al., [2021c](#bib.bib176)) or
    Kullback-Leibler Divergence (KLD) (Yang et al., [2021d](#bib.bib178)). Compared
    with the smooth L1 loss which optimizes all parameters independently, GWD is a
    semi-coupled measure, which can optimize width, height and angle jointly. Meanwhile,
    the center point $(x,y)$ is optimized independently, which may cause the regressed
    OBBs to slightly shifted. In addition, GWD is not scaled invariant, which is not
    robust to detect multi-scale objects. KLD can eliminate the above problem, which
    can be expressed as:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用距离函数来衡量两个高斯分布，例如高斯瓦瑟斯坦距离（GWD）（Yang 等， [2021c](#bib.bib176)）或 Kullback-Leibler
    散度（KLD）（Yang 等，[2021d](#bib.bib178)）。与优化所有参数独立的平滑 L1 损失相比，GWD 是一种半耦合度量，可以联合优化宽度、高度和角度。同时，中心点
    $(x,y)$ 是独立优化的，这可能导致回归的 OBB 稍微偏移。此外，GWD 不具备尺度不变性，这使得其在检测多尺度物体时不够鲁棒。KLD 可以解决上述问题，其表达式为：
- en: '|  | $\displaystyle D_{kld}(\mathcal{N}_{g}&#124;&#124;\mathcal{N}_{p})$ |
    $\displaystyle=\frac{1}{2}[(m_{p}-m_{g})^{T}\Sigma_{p}^{-1}(m_{p}-m_{g})$ |  |
    (16) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle D_{kld}(\mathcal{N}_{g}&#124;&#124;\mathcal{N}_{p})$ |
    $\displaystyle=\frac{1}{2}[(m_{p}-m_{g})^{T}\Sigma_{p}^{-1}(m_{p}-m_{g})$ |  |
    (16) |'
- en: '|  |  | $\displaystyle+Tr\left(\Sigma_{p}^{-1}\Sigma_{g}\right)+In\frac{&#124;\Sigma_{p}&#124;}{&#124;\Sigma_{g}&#124;}]-1$
    |  |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+Tr\left(\Sigma_{p}^{-1}\Sigma_{g}\right)+In\frac{&#124;\Sigma_{p}&#124;}{&#124;\Sigma_{g}&#124;}]-1$
    |  |'
- en: Note that KLD is a chain coupling of all parameters and is invariant to scale.
    Therefore, KLD can jointly optimize all parameters and is self-modulated during
    the training process.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，KLD 是所有参数的链式耦合，并且对尺度不变。因此，KLD 可以联合优化所有参数，并且在训练过程中是自我调节的。
- en: 'Finally, to obtain consistency between the metric and the regression loss,
    the distance between Gaussian distributions is converted into an approximate IoU
    loss using a nonlinear transformation:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了使度量与回归损失之间保持一致，高斯分布之间的距离通过非线性变换转换为近似的 IoU 损失：
- en: '|  | $L_{reg-gauss}=1-\frac{1}{\tau+f(D)}$ |  | (17) |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{reg-gauss}=1-\frac{1}{\tau+f(D)}$ |  | (17) |'
- en: where $D$ denotes the distance between Gaussian distributions, and $f(\cdot)$
    represents the non-linear function, such as $f(D)=\sqrt{D}$ or $f(D)=\ln(D+1)$.
    The hyperparameter $\tau$ modulates the entire loss.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D$ 表示高斯分布之间的距离，$f(\cdot)$ 代表非线性函数，例如 $f(D)=\sqrt{D}$ 或 $f(D)=\ln(D+1)$。超参数
    $\tau$ 调节整个损失。
- en: However, both GWD and KLD only maintain value-level consistency instead of trend-level
    consistency between RIoU and regression loss. To tackle this issue,  Yang et al.
    ([2022](#bib.bib180)) designed the KFIoU loss based on Kalman filter to achieve
    the best trend-level alignment with RIoU, which is differentiable and does not
    require additional hyperparameters. Based on the mechanism of RIoU, KFIoU calculates
    the overlapping area between two Gaussian distributions via a Kalman filter. Benefiting
    from the trend-level alignment strategy, KFIoU achieves better performance than
    GWD and KLD.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GWD 和 KLD 仅维持 RIoU 和回归损失之间的值级一致性，而非趋势级一致性。为了解决这个问题，Yang 等（[2022](#bib.bib180)）基于卡尔曼滤波器设计了
    KFIoU 损失，以实现与 RIoU 的最佳趋势级对齐，该方法是可微的且不需要额外的超参数。基于 RIoU 的机制，KFIoU 通过卡尔曼滤波器计算两个高斯分布之间的重叠区域。得益于趋势级对齐策略，KFIoU
    比 GWD 和 KLD 实现了更好的性能。
- en: 5.2 Quadrilateral representation
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 四边形表示
- en: '![Refer to caption](img/ca974afab37df05d1343b32919201e64.png)![Refer to caption](img/1e0ce15a288e91200fac9b4469dffaf8.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ca974afab37df05d1343b32919201e64.png)![参见说明](img/1e0ce15a288e91200fac9b4469dffaf8.png)'
- en: (a) Vertex sorting for annotated objects.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 注释对象的顶点排序。
- en: '![Refer to caption](img/77654ec84b48757e1e520bab1c7e35b8.png)![Refer to caption](img/1cc4fb2dfce1f068d0ec28a2bd650dbf.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/77654ec84b48757e1e520bab1c7e35b8.png)![参见图例](img/1cc4fb2dfce1f068d0ec28a2bd650dbf.png)'
- en: (b) Vertex sorting in detection process.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 检测过程中的顶点排序。
- en: 'Figure 14: Definition of quadrilateral representation. Top: the top-left vertex
    relative to the object direction is chosen as the start point. Bottom: the leftmost
    vertex is chosen as the starting point.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：四边形表示的定义。顶部：相对于对象方向的左上角顶点被选为起点。底部：最左侧的顶点被选为起点。
- en: 'The quadrilateral representation denotes an OBB as a vector $(x_{1},y_{1},x_{2},y_{2},x_{3},y_{3},x_{4},y_{4})$,
    where $(x_{i},y_{i})$ indicates the image coordinates of the $i_{th}$ vertex arranged
    in a clockwise order (Xu et al., [2021a](#bib.bib168)). This representation method
    can compactly enclose oriented objects with large deformation and has been widely
    adopted to annotate objects in large-scale RS datasets, including DOTA (Xia et al.,
    [2018](#bib.bib161); Ding et al., [2022](#bib.bib30)), and HRSC2016 (Liu et al.,
    [2016b](#bib.bib101)). Significantly, the top-left vertex relative to the object
    direction is chosen as the starting point $(x_{1},y_{1}$), as shown in Figure
    [14(a)](#S5.F14.sf1 "In Figure 14 ‣ 5.2 Quadrilateral representation ‣ 5 OBB Representations
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey").'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 四边形表示将 OBB 表示为一个向量 $(x_{1},y_{1},x_{2},y_{2},x_{3},y_{3},x_{4},y_{4})$，其中 $(x_{i},y_{i})$
    表示按顺时针顺序排列的第 $i$ 个顶点的图像坐标 (Xu et al., [2021a](#bib.bib168))。这种表示方法可以紧凑地包围具有大变形的定向对象，并已广泛用于标注大规模
    RS 数据集中的对象，包括 DOTA (Xia et al., [2018](#bib.bib161); Ding et al., [2022](#bib.bib30))
    和 HRSC2016 (Liu et al., [2016b](#bib.bib101))。重要的是，相对于对象方向的左上角顶点被选为起始点 $(x_{1},y_{1}$)，如图
    [14(a)](#S5.F14.sf1 "在图 14 ‣ 5.2 四边形表示 ‣ 5 OBB 表示 ‣ 基于深度学习的光学遥感图像定向对象检测：综述") 所示。
- en: 'For the quadrilateral representation, the detector outputs a vector $(\Delta
    x_{1}^{p},\Delta y_{1}^{p},\Delta x_{2}^{p},\Delta y_{2}^{p},\Delta x_{3}^{p},\Delta
    y_{3}^{p},\Delta x_{4}^{p},\Delta y_{4}^{p})$, where $(\Delta x_{i}^{p},\Delta
    y_{i}^{p})$ represent the relative offsets between the $i$-th vertex of the predicted
    OBB and the corresponding anchor box. Then, the predicted offsets are used to
    approximate the GT coordinate offsets $(\Delta x_{1}^{g},\Delta y_{1}^{g},\Delta
    x_{2}^{g},\Delta y_{2}^{g},\Delta x_{3}^{g},\Delta y_{3}^{g},\Delta x_{4}^{g},\Delta
    y_{4}^{g})$ between the $i$-th vertex of the GT OBB and that of the anchor box.
    The regression loss of quadrilateral OBB representation can be expressed as:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 对于四边形表示，检测器输出一个向量 $(\Delta x_{1}^{p},\Delta y_{1}^{p},\Delta x_{2}^{p},\Delta
    y_{2}^{p},\Delta x_{3}^{p},\Delta y_{3}^{p},\Delta x_{4}^{p},\Delta y_{4}^{p})$，其中
    $(\Delta x_{i}^{p},\Delta y_{i}^{p})$ 代表预测 OBB 的第 $i$ 个顶点与相应锚框之间的相对偏移量。然后，预测的偏移量用于近似
    GT 坐标偏移量 $(\Delta x_{1}^{g},\Delta y_{1}^{g},\Delta x_{2}^{g},\Delta y_{2}^{g},\Delta
    x_{3}^{g},\Delta y_{3}^{g},\Delta x_{4}^{g},\Delta y_{4}^{g})$，这些偏移量表示 GT OBB
    的第 $i$ 个顶点与锚框的顶点之间的偏移量。四边形 OBB 表示的回归损失可以表示为：
- en: '|  | $L_{reg}=\sum_{i=1}^{4}\left[L_{n}\left(\Delta x_{i}^{p}-\Delta x_{i}^{g}\right)+L_{n}\left(\Delta
    y_{i}^{p}-\Delta y_{i}^{g}\right)\right]$ |  | (18) |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{reg}=\sum_{i=1}^{4}\left[L_{n}\left(\Delta x_{i}^{p}-\Delta x_{i}^{g}\right)+L_{n}\left(\Delta
    y_{i}^{p}-\Delta y_{i}^{g}\right)\right]$ |  | (18) |'
- en: 'Generally, the anchor box selects the top-left vertex in the image as the starting
    point. To ensure consistency, the leftmost vertexes of the predicted OBB and the
    corresponding GT OBB are chosen as the starting point, as shown in Figure [14(b)](#S5.F14.sf2
    "In Figure 14 ‣ 5.2 Quadrilateral representation ‣ 5 OBB Representations ‣ Oriented
    Object Detection in Optical Remote Sensing Images using Deep Learning: A Survey").
    However, the inappropriate vertex sorting may cause inconsistencies between the
    vertex sequences of the anchor and the GT OBB, which is known as the vertexes
    sorting problem or the corners sorting problem (Qian et al., [2021](#bib.bib112),
    [2022](#bib.bib113); Xu et al., [2021a](#bib.bib168)). Figure [15](#S5.F15 "Figure
    15 ‣ 5.2 Quadrilateral representation ‣ 5 OBB Representations ‣ Oriented Object
    Detection in Optical Remote Sensing Images using Deep Learning: A Survey") shows
    a case of the problem. The anchor and the GT OBB are shown in blue and green respectively,
    and the dashed line and the solid line denote the actual and ideal vertexes matching
    during regression. In the ideal setting, the vertexes matching from the anchor
    to the GT is: ${\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{1},y_{1})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{2},y_{2})},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{2},y_{2})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{3},y_{3})},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{3},y_{3})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{4},y_{4})},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{4},y_{4})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{1},y_{1})}$.
    However, in the actual regression, the vertexes matching is: <math alttext="{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{1},y_{1})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{1},y_{1})},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{2},y_{2})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{2},y_{2})},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{3},y_{3})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{3},y_{3})},\\'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，锚框在图像中选择左上角作为起始点。为了确保一致性，选择预测的 OBB 和相应的 GT OBB 的最左端点作为起始点，如图 [14(b)](#S5.F14.sf2
    "图 14 ‣ 5.2 四边形表示 ‣ 5 OBB 表示 ‣ 基于深度学习的光学遥感图像定向目标检测：综述")所示。然而，不恰当的顶点排序可能导致锚框和 GT
    OBB 顶点序列之间的不一致，这被称为顶点排序问题或角点排序问题 (Qian 等，[2021](#bib.bib112)，[2022](#bib.bib113)；Xu
    等，[2021a](#bib.bib168))。图 [15](#S5.F15 "图 15 ‣ 5.2 四边形表示 ‣ 5 OBB 表示 ‣ 基于深度学习的光学遥感图像定向目标检测：综述")
    显示了这个问题的一个案例。锚框和 GT OBB 分别用蓝色和绿色表示，虚线和实线表示回归过程中的实际和理想顶点匹配。在理想情况下，锚框到 GT 的顶点匹配是：${\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{1},y_{1})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{2},y_{2})},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{2},y_{2})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{3},y_{3})},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{3},y_{3})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{4},y_{4})},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{4},y_{4})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{1},y_{1})}$。然而，在实际回归中，顶点匹配是：<math
    alttext="{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{1},y_{1})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{1},y_{1})},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{2},y_{2})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{2},y_{2})},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{3},y_{3})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{3},y_{3})},\\
- en: '{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{4},y_{4})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{4},y_{4})}"
    display="inline"><semantics ><mrow ><mrow  ><mrow ><mo mathcolor="#0000FF" stretchy="false"  >(</mo><msub
    ><mi mathcolor="#0000FF"  >x</mi><mn mathcolor="#0000FF"  >1</mn></msub><mo mathcolor="#0000FF"  >,</mo><msub
    ><mi mathcolor="#0000FF"  >y</mi><mn mathcolor="#0000FF"  >1</mn></msub><mo mathcolor="#0000FF"
    stretchy="false"  >)</mo></mrow><mo stretchy="false"  >→</mo><mrow ><mo mathcolor="#00FF00"
    stretchy="false"  >(</mo><msub ><mi mathcolor="#00FF00"  >x</mi><mn mathcolor="#00FF00"  >1</mn></msub><mo
    mathcolor="#00FF00"  >,</mo><msub ><mi mathcolor="#00FF00"  >y</mi><mn mathcolor="#00FF00"  >1</mn></msub><mo
    mathcolor="#00FF00" stretchy="false"  >)</mo></mrow></mrow><mo >,</mo><mrow ><mrow  ><mrow
    ><mo mathcolor="#0000FF" stretchy="false"  >(</mo><msub ><mi mathcolor="#0000FF"  >x</mi><mn
    mathcolor="#0000FF"  >2</mn></msub><mo mathcolor="#0000FF"  >,</mo><msub ><mi
    mathcolor="#0000FF"  >y</mi><mn mathcolor="#0000FF"  >2</mn></msub><mo mathcolor="#0000FF"
    stretchy="false"  >)</mo></mrow><mo stretchy="false"  >→</mo><mrow ><mo mathcolor="#00FF00"
    stretchy="false"  >(</mo><msub ><mi mathcolor="#00FF00"  >x</mi><mn mathcolor="#00FF00"  >2</mn></msub><mo
    mathcolor="#00FF00"  >,</mo><msub ><mi mathcolor="#00FF00"  >y</mi><mn mathcolor="#00FF00"  >2</mn></msub><mo
    mathcolor="#00FF00" stretchy="false"  >)</mo></mrow></mrow><mo >,</mo><mrow ><mrow
    ><mrow ><mo mathcolor="#0000FF" stretchy="false" >(</mo><msub ><mi mathcolor="#0000FF"
    >x</mi><mn mathcolor="#0000FF" >3</mn></msub><mo mathcolor="#0000FF"  >,</mo><msub
    ><mi mathcolor="#0000FF"  >y</mi><mn mathcolor="#0000FF"  >3</mn></msub><mo mathcolor="#0000FF"
    stretchy="false" >)</mo></mrow><mo stretchy="false" >→</mo><mrow ><mo mathcolor="#00FF00"
    stretchy="false" >(</mo><msub ><mi mathcolor="#00FF00" >x</mi><mn mathcolor="#00FF00"
    >3</mn></msub><mo mathcolor="#00FF00"  >,</mo><msub ><mi mathcolor="#00FF00"  >y</mi><mn
    mathcolor="#00FF00"  >3</mn></msub><mo mathcolor="#00FF00" stretchy="false" >)</mo></mrow></mrow><mo
    >,</mo><mrow ><mrow ><mo mathcolor="#0000FF" stretchy="false" >(</mo><msub ><mi
    mathcolor="#0000FF"  >x</mi><mn mathcolor="#0000FF"  >4</mn></msub><mo mathcolor="#0000FF"  >,</mo><msub
    ><mi mathcolor="#0000FF"  >y</mi><mn mathcolor="#0000FF"  >4</mn></msub><mo mathcolor="#0000FF"
    stretchy="false" >)</mo></mrow><mo stretchy="false" >→</mo><mrow ><mo mathcolor="#00FF00"
    stretchy="false" >(</mo><msub ><mi mathcolor="#00FF00" >x</mi><mn mathcolor="#00FF00"
    >4</mn></msub><mo mathcolor="#00FF00"  >,</mo><msub ><mi mathcolor="#00FF00"  >y</mi><mn
    mathcolor="#00FF00"  >4</mn></msub><mo mathcolor="#00FF00" stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><ci  >→</ci><interval closure="open"  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><cn type="integer"  >1</cn></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑦</ci><cn type="integer"  >1</cn></apply></interval><interval closure="open"  ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><cn type="integer"  >1</cn></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑦</ci><cn type="integer"  >1</cn></apply></interval></apply><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><ci >→</ci><interval
    closure="open" ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><cn
    type="integer" >2</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑦</ci><cn type="integer" >2</cn></apply></interval><interval closure="open" ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><cn type="integer"  >2</cn></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑦</ci><cn type="integer"  >2</cn></apply></interval></apply><apply
    ><csymbol cd="ambiguous"  >formulae-sequence</csymbol><apply ><ci >→</ci><interval
    closure="open"  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><cn
    type="integer"  >3</cn></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑦</ci><cn type="integer"  >3</cn></apply></interval><interval closure="open"  ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><cn type="integer"  >3</cn></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑦</ci><cn type="integer"  >3</cn></apply></interval></apply><apply
    ><ci >→</ci><interval closure="open"  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><cn type="integer"  >4</cn></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑦</ci><cn type="integer"  >4</cn></apply></interval><interval closure="open"  ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><cn type="integer"  >4</cn></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑦</ci><cn type="integer"  >4</cn></apply></interval></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{1},y_{1})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{1},y_{1})},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{2},y_{2})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{2},y_{2})},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{3},y_{3})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{3},y_{3})},\\
    {\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(x_{4},y_{4})}\rightarrow{\color[rgb]{0,1,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,1,0}(x_{4},y_{4})}</annotation></semantics></math>.
    Such inconsistency causes a large regression loss, confusing the network during
    the training process. Hence, it is critical to determine the sequence of vertexes
    in advance to stabilize the training process.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不一致性会导致较大的回归损失，在训练过程中使网络感到困惑。因此，提前确定顶点的顺序对稳定训练过程至关重要。
- en: '![Refer to caption](img/405c0c5413220240a23c9e77debafe13.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/405c0c5413220240a23c9e77debafe13.png)'
- en: 'Figure 15: Illustration of vertexes sorting problem. The dashed line and solid
    line represent the actual and ideal regression forms, respectively.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：顶点排序问题的示意图。虚线和实线分别代表实际和理想回归形式。
- en: 'To solve the above problem, an eight-parameter version of modulated loss (Qian
    et al., [2021](#bib.bib112), [2022](#bib.bib113)) was devised. Specifically, the
    order of the four vertices of the predicted box is moved clockwise and counterclockwise
    by one place respectively. Hence, the corresponding loss can be calculated by:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 为解决上述问题，设计了一种八参数版本的调制损失 (Qian et al., [2021](#bib.bib112), [2022](#bib.bib113))。具体来说，预测框的四个顶点的顺序分别顺时针和逆时针移动一个位置。因此，可以通过以下公式计算相应的损失：
- en: '|  | $\displaystyle L_{reg}^{{}^{\prime}}=$ | $\displaystyle\sum_{i=1}^{4}\left[L_{1}\left(t_{x_{(i+3)\%4}}^{p},t_{x_{i}}^{g}\right)+L_{1}\left(t_{y_{(i+3)\%4}}^{p},t_{y_{i}}^{g}\right)\right]$
    |  | (19) |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{reg}^{{}^{\prime}}=$ | $\displaystyle\sum_{i=1}^{4}\left[L_{1}\left(t_{x_{(i+3)\%4}}^{p},t_{x_{i}}^{g}\right)+L_{1}\left(t_{y_{(i+3)\%4}}^{p},t_{y_{i}}^{g}\right)\right]$
    |  | (19) |'
- en: '|  | $\displaystyle L_{reg}^{{}^{\prime\prime}}=$ | $\displaystyle\sum_{i=1}^{4}\left[L_{1}\left(t_{x_{(i+1)\%4}}^{p},t_{x_{i}}^{g}\right)+L_{1}\left(t_{y_{(i+1)\%4}}^{p},t_{y_{i}}^{g}\right)\right]$
    |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{reg}^{{}^{\prime\prime}}=$ | $\displaystyle\sum_{i=1}^{4}\left[L_{1}\left(t_{x_{(i+1)\%4}}^{p},t_{x_{i}}^{g}\right)+L_{1}\left(t_{y_{(i+1)\%4}}^{p},t_{y_{i}}^{g}\right)\right]$
    |  |'
- en: 'where $(t_{x_{i}}^{p},t_{y_{i}}^{p})$ and $(t_{x_{i}}^{g},t_{y_{i}}^{g})$ represent
    the offsets between the $i$-th vertex of the predicted OBB and the GT OBB, respectively.
    The formula of modulated loss can be expressed as:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(t_{x_{i}}^{p},t_{y_{i}}^{p})$ 和 $(t_{x_{i}}^{g},t_{y_{i}}^{g})$ 分别表示预测
    OBB 的第 $i$ 个顶点和 GT OBB 之间的偏移量。调制损失的公式可以表示为：
- en: '|  | $L_{mr}^{{}^{\prime}}=\min\left\{L_{reg},L_{reg}^{{}^{\prime}},L_{reg}^{{}^{\prime\prime}}\right\}$
    |  | (20) |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{mr}^{{}^{\prime}}=\min\left\{L_{reg},L_{reg}^{{}^{\prime}},L_{reg}^{{}^{\prime\prime}}\right\}$
    |  | (20) |'
- en: 'Here, $L_{reg}$ is defined in Eq. [18](#S5.E18 "In 5.2 Quadrilateral representation
    ‣ 5 OBB Representations ‣ Oriented Object Detection in Optical Remote Sensing
    Images using Deep Learning: A Survey") and adopts the L1 loss.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$L_{reg}$ 在公式 [18](#S5.E18 "在 5.2 四边形表示 ‣ 5 OBB 表示 ‣ 使用深度学习的光学遥感图像中的定向目标检测：综述")
    中定义，并采用 L1 损失。
- en: Xu et al. ([2021a](#bib.bib168)) proposed an effective way to avoid sorting
    vertex, which glides the vertex of the horizontal anchor on each corresponding
    side. Specifically, it regresses four length ratios representing the gliding offset
    on each corresponding side, which can eliminate the confusion caused by vertex
    sorting.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Xu 等人 ([2021a](#bib.bib168)) 提出了避免排序顶点的有效方法，该方法在每个相应的侧面上滑动水平锚点的顶点。具体而言，它回归四个长度比，表示每个相应侧面的滑动偏移，这可以消除由顶点排序引起的混淆。
- en: 5.3 $\theta$-based representation vs. Quadrilateral representation
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 $\theta$-基于表示与四边形表示
- en: As discussed in this section, an enormous amount of research effort is devoted
    to resolving the challenges encounter by $\theta$-based representation and quadrilateral
    representation. As $\theta$-based representation is improved from horizontal object
    representation and can represent a rectangular box, it receives more attention
    than quadrilateral representation. In contrast, a detector based on quadrilateral
    representation generally can not generate a rectangular box without a post-processing
    operator, thus quadrilateral representation is not suitable for a two-stage detector
    that contains an RRoI operator. As a result, $\theta$-based representation seems
    to be a more reasonable representation in oriented object detection. However,
    quadrilateral representation can represent objects more accurately as not all
    objects are rectangular.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如本节所述，大量的研究工作致力于解决 $\theta$-基于表示和四边形表示所遇到的挑战。由于 $\theta$-基于表示从水平物体表示中改进而来，并且能够表示矩形框，因此比四边形表示受到更多关注。相反，基于四边形表示的检测器通常无法在没有后处理操作员的情况下生成矩形框，因此四边形表示不适合包含
    RRoI 操作员的两阶段检测器。因此，$\theta$-基于表示在定向目标检测中似乎是更合理的表示方法。然而，四边形表示可以更准确地表示物体，因为并非所有物体都是矩形的。
- en: 6 Feature Representations
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 特征表示
- en: Extracting powerful feature representations plays a crucial role in high-precision
    detection (Dalal and Triggs, [2005](#bib.bib27); Girshick et al., [2014](#bib.bib41);
    Simonyan and Zisserman, [2015](#bib.bib126); He et al., [2016](#bib.bib58); Zhu
    et al., [2016](#bib.bib212)), because robust feature representations can promote
    the precision of localization and the accuracy of classification remarkably. Early
    efforts focused on designing local feature representations, such as HOG (Dalal
    and Triggs, [2005](#bib.bib27)), BoW (Fei-Fei and Perona, [2005](#bib.bib35)),
    and Haar (Leitloff et al., [2010](#bib.bib79)). These features require careful
    manual feature engineering and considerable domain knowledge. Recently, there
    are increasing deep learning networks with powerful feature representations springing
    up thick and fast, which design better network architectures to enhance the capability
    of feature representations, e.g. AlexNet (Krizhevsky et al., [2012](#bib.bib73),
    [2017](#bib.bib74)), VGGNet (Simonyan and Zisserman, [2015](#bib.bib126)), GoogLeNet (Szegedy
    et al., [2015](#bib.bib133)), Inception series (Ioffe and Szegedy, [2015](#bib.bib68);
    Szegedy et al., [2016](#bib.bib134), [2017](#bib.bib132)), ResNet (He et al.,
    [2016](#bib.bib58)), DenseNet (Huang et al., [2017](#bib.bib67)). For oriented
    object detection, a great deal of effort is devoted to improving feature representations
    by extracting rotation-invariant features and feature enhancement.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 提取强大的特征表示在高精度检测中起着至关重要的作用 （Dalal and Triggs, [2005](#bib.bib27)；Girshick et al.,
    [2014](#bib.bib41)；Simonyan and Zisserman, [2015](#bib.bib126)；He et al., [2016](#bib.bib58)；Zhu
    et al., [2016](#bib.bib212)），因为稳健的特征表示可以显著提高定位精度和分类准确性。早期的工作集中于设计局部特征表示，例如 HOG （Dalal
    and Triggs, [2005](#bib.bib27)）、BoW （Fei-Fei and Perona, [2005](#bib.bib35)）和
    Haar （Leitloff et al., [2010](#bib.bib79)）。这些特征需要精细的手工特征工程和相当的领域知识。近年来，越来越多具有强大特征表示能力的深度学习网络不断涌现，它们设计了更好的网络架构以增强特征表示能力，例如
    AlexNet （Krizhevsky et al., [2012](#bib.bib73)，[2017](#bib.bib74)）、VGGNet （Simonyan
    and Zisserman, [2015](#bib.bib126)）、GoogLeNet （Szegedy et al., [2015](#bib.bib133)）、Inception
    系列 （Ioffe and Szegedy, [2015](#bib.bib68)；Szegedy et al., [2016](#bib.bib134)，[2017](#bib.bib132)）、ResNet （He
    et al., [2016](#bib.bib58)）、DenseNet （Huang et al., [2017](#bib.bib67)）。对于定向目标检测，极大的努力被投入到通过提取旋转不变特征和特征增强来改善特征表示。
- en: 6.1 Rotation-Invariant Feature
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 旋转不变特征
- en: In order to obtain rotation-invariant features, RRoI operators and data augmentation
    have been fully explored. Specifically, there are various ROI operators have been
    proposed to extract rotation-invariant features from feature maps, including RRoI
    Pooling (Ma et al., [2018](#bib.bib104)), RRoI Align (Yang et al., [2018a](#bib.bib171);
    Ding et al., [2019](#bib.bib29)), and alignment convolution (Han et al., [2022a](#bib.bib47)).
    However, since the regular CNN-based blocks are only translation-invariant, the
    feature maps extracted by these blocks are sensitive to rotation. Therefore, the
    pipeline composed of regular CNNs and RRoI operators is sub-optimal, even if it
    can extract approximately rotation-invariant features by adopting large networks
    trained on an enormous number of samples. To remedy the above shortcomings, random
    rotation based data augmentation has been adopted during network training, since
    it can enhance the rotation variations of training samples. Although global generalization
    can be improved, local rotational information is abandoned to some degree when
    random rotation being used (Worrall et al., [2017](#bib.bib157)). In addition,
    it is difficult to quantify and interpret the rotational information extracted
    by CNN blocks trained with rotation-based data augmentation (Lenc and Vedaldi,
    [2015](#bib.bib80)).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得旋转不变特征，RRoI 操作符和数据增强方法已被充分探索。具体而言，已经提出了多种 ROI 操作符来从特征图中提取旋转不变特征，包括 RRoI
    Pooling （Ma et al., [2018](#bib.bib104)）、RRoI Align （Yang et al., [2018a](#bib.bib171)；Ding
    et al., [2019](#bib.bib29)）和对齐卷积 （Han et al., [2022a](#bib.bib47)）。然而，由于常规 CNN
    基块仅对平移不变，这些基块提取的特征图对旋转敏感。因此，由常规 CNN 和 RRoI 操作符组成的流水线是次优的，即使通过采用在大量样本上训练的大型网络也可以提取大致旋转不变的特征。为了解决上述缺陷，在网络训练期间采用了基于随机旋转的数据增强，因为它可以增强训练样本的旋转变化。虽然可以改善全局泛化，但在使用随机旋转时，局部旋转信息在一定程度上被忽略了 （Worrall
    et al., [2017](#bib.bib157)）。此外，量化和解释通过旋转基础数据增强训练的 CNN 基块提取的旋转信息也很困难 （Lenc and
    Vedaldi, [2015](#bib.bib80)）。
- en: 'Therefore, the above methods fail to extract exactly rotation-invariant features (Han
    et al., [2021c](#bib.bib53)). Recently, group equivariant convolutional neural
    networks (G-CNN) (Cohen and Welling, [2016](#bib.bib22)) is proposed to achieve
    rotation equivariance. G-CNN utilizes different channels to represent feature
    information from different orientations, showing powerful rotation-invariant feature
    extraction ability on the task of image classification. What’s more, several G-CNN-based
    variants (Worrall et al., [2017](#bib.bib157); Marcos et al., [2017](#bib.bib106);
    Weiler and Cesa, [2019](#bib.bib154); Weiler et al., [2018](#bib.bib155)) are
    proposed to extend rotation equivariance and achieve promising performance on
    the Rotated MNIST (Liu et al., [2003](#bib.bib92); Larochelle et al., [2007](#bib.bib77))
    dataset. The philosophy of G-CNN and its variants is transforming an image $x$
    by a transformation $T$ and then passing it through the network $f$ should give
    the same result as first extracting image features by the network and then transforming
    the feature map, which can be expressed as:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，上述方法无法精确提取旋转不变特征（Han 等，[2021c](#bib.bib53)）。最近，提出了群等变卷积神经网络（G-CNN）（Cohen
    和 Welling，[2016](#bib.bib22)）以实现旋转等变性。G-CNN 利用不同的通道来表示来自不同方向的特征信息，在图像分类任务中展现出强大的旋转不变特征提取能力。此外，还提出了几种基于
    G-CNN 的变体（Worrall 等，[2017](#bib.bib157)；Marcos 等，[2017](#bib.bib106)；Weiler 和
    Cesa，[2019](#bib.bib154)；Weiler 等，[2018](#bib.bib155)），旨在扩展旋转等变性，并在旋转 MNIST（Liu
    等，[2003](#bib.bib92)；Larochelle 等，[2007](#bib.bib77)）数据集上取得了良好的表现。G-CNN 及其变体的理念是将图像
    $x$ 通过变换 $T$ 进行转换，然后传递通过网络 $f$ 应该得到与首先由网络提取图像特征再对特征图进行变换相同的结果，可以表达为：
- en: '|  | $f\left[T(x)\right]=T^{*}\left[f(x)\right]$ |  | (21) |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | $f\left[T(x)\right]=T^{*}\left[f(x)\right]$ |  | (21) |'
- en: where $T$ and $T^{*}$ denote the transformations, while $x$ and $f$ denotes
    the input image and the neural network (Cohen and Welling, [2016](#bib.bib22)).
    Note that the transformation $T$ and $T^{*}$ need not be the same.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $T$ 和 $T^{*}$ 表示变换，而 $x$ 和 $f$ 表示输入图像和神经网络（Cohen 和 Welling，[2016](#bib.bib22)）。请注意，变换
    $T$ 和 $T^{*}$ 不必相同。
- en: Recently, inspired by the achievements of equivariant networks in image classification,
    some works incorporate rotation-equivariant networks into object detectors, including
    ReDet (Han et al., [2021a](#bib.bib49)), CHPDet (Zhang et al., [2022](#bib.bib192)).
    Based on the framework of RoI Transformer (Ding et al., [2019](#bib.bib29)), ReDet
    adopts rotation-equivariant networks instead of the ordinary ResNet modules (He
    et al., [2016](#bib.bib58)) as the backbone. Compared to CNNs which adopt translational
    weight sharing to achieve translation equivariance, rotation-equivariant networks
    further share weights over filter orientations to encode rotation equivariance.
    Thus, the orientation-dependent responses for different orientations can be obtained,
    which is useful for oriented object detection. Besides, the rotational weight
    sharing can reduce the model size greatly. To obtain the rotation-invariant features
    in both spatial and orientation dimensions from the rotation-equivariant feature,
    a Rotation-invariant RoI (RiRoI) Align operator is designed. Specifically, according
    to the orientation predicted by RoI Transformer (Ding et al., [2019](#bib.bib29)),
    orientation channels circular switch and interpolation are adopted to transform
    the rotation-equivariant features into rotation-invariant features.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，受到等变网络在图像分类中取得的成就的启发，一些工作将旋转等变网络整合到目标检测器中，包括 ReDet（Han 等，[2021a](#bib.bib49)）、CHPDet（Zhang
    等，[2022](#bib.bib192)）。基于 RoI Transformer（Ding 等，[2019](#bib.bib29)）的框架，ReDet
    采用旋转等变网络代替普通的 ResNet 模块（He 等，[2016](#bib.bib58)）作为骨干网。与采用平移权重共享以实现平移等变性的 CNN 相比，旋转等变网络进一步在滤波器方向上共享权重以编码旋转等变性。因此，可以获得不同方向的方向依赖响应，这对定向目标检测非常有用。此外，旋转权重共享可以大大减少模型大小。为了从旋转等变特征中获得在空间和方向维度上的旋转不变特征，设计了一种旋转不变
    RoI（RiRoI）对齐算子。具体来说，根据 RoI Transformer（Ding 等，[2019](#bib.bib29)）预测的方向，采用方向通道的圆形切换和插值将旋转等变特征转换为旋转不变特征。
- en: 6.2 Enhanced Feature Representations
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 增强的特征表示
- en: In order to accommodate the variations of scale, orientation, appearance, pose
    and background, a great deal of effort was devoted to enhancing object feature
    representations.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应尺度、方向、外观、姿态和背景的变化，投入了大量精力来增强对象特征表示。
- en: 'As discussed in Section [4](#S4 "4 Detection Frameworks ‣ Oriented Object Detection
    in Optical Remote Sensing Images using Deep Learning: A Survey"), two-stage and
    one-stage frameworks adopt deep CNN modules and FPN architectures to learn robust
    multi-level feature representations. Such a combination integrates semantic and
    spatial information from different layers to detect objects in multi-scales. To
    make full use of the multi-level feature representations extracted by the backbone,
    recent works have improved the FPN architectures in the tasks of oriented object
    detection. Yang et al. ([2018b](#bib.bib172)) designed a Rotation Dense Feature
    Pyramid Network, dubbed R-DFPN, which enhances feature fusion and reuses features
    through a dense connection. To ensure the discriminability of feature maps, RDD (Zhong
    and Ao, [2020](#bib.bib204)) added a $1\times 1$ convolutional layer before up-sampling
    the feature map and a $3\times 3$ convolutional layer after summing feature maps
    of the adjacent layers. In addition, there are also a series of works in generic
    object detection are proposed to develop the variants of FPN architecture, which
    achieves remarkable advances and can be used in oriented object detection, including
    PANet (Liu et al., [2018](#bib.bib96)), ASFF (Liu et al., [2019](#bib.bib95)),
    M2Det (Zhao et al., [2019a](#bib.bib201)), NAS-FPN (Ghiasi et al., [2019](#bib.bib37)),
    BiFPN (Tan et al., [2020b](#bib.bib136)), and Recursive FPN (Qiao et al., [2021](#bib.bib114)).'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[4](#S4 "4 Detection Frameworks ‣ Oriented Object Detection in Optical Remote
    Sensing Images using Deep Learning: A Survey")节所讨论的，两阶段和单阶段框架采用深度 CNN 模块和 FPN
    架构来学习鲁棒的多层次特征表示。这种组合集成了来自不同层次的语义和空间信息，以在多尺度上检测目标。为了充分利用骨干网络提取的多层次特征表示，最近的工作在有向目标检测任务中改进了
    FPN 架构。Yang 等（[2018b](#bib.bib172)）设计了一种旋转密集特征金字塔网络，称为 R-DFPN，通过密集连接来增强特征融合和复用特征。为了确保特征图的区分性，RDD（Zhong
    和 Ao，[2020](#bib.bib204)）在上采样特征图之前添加了一个 $1\times 1$ 卷积层，并在求和相邻层特征图之后添加了一个 $3\times
    3$ 卷积层。此外，还有一系列通用目标检测方面的工作提出了 FPN 架构的变体，包括 PANet（Liu 等，[2018](#bib.bib96)）、ASFF（Liu
    等，[2019](#bib.bib95)）、M2Det（Zhao 等，[2019a](#bib.bib201)）、NAS-FPN（Ghiasi 等，[2019](#bib.bib37)）、BiFPN（Tan
    等，[2020b](#bib.bib136)）和 Recursive FPN（Qiao 等，[2021](#bib.bib114)）。'
- en: On the other hand, attention mechanism shows promising performance in modeling
    long-range dependencies. It has been widely used in image classification (Mnih
    et al., [2014](#bib.bib108)), natural language processing (Vaswani et al., [2017](#bib.bib140)),
    and generic object detection (Yoo et al., [2015](#bib.bib185); Li et al., [2019](#bib.bib84);
    Tian et al., [2022](#bib.bib137); Yu and Ji, [2022](#bib.bib187); Wang et al.,
    [2019](#bib.bib152)). In recent years, several attention methods (Pan et al.,
    [2020](#bib.bib111); Yang et al., [2019a](#bib.bib177), [2022a](#bib.bib175))
    have been proposed to capture more effective feature representations, which usually
    combines both pixel-level attention network and channel-level attention network
    to suppress the noise and enhance the object information. SCRDet (Yang et al.,
    [2019a](#bib.bib177)) adopts the pixel attention network to generate a saliency
    map that can separate foreground from background, and use Squeeze-and-Excitation
    (SE) blocks (Hu et al., [2020](#bib.bib66)) as the channel attention network to
    further enhance the saliency map. Pan et al. ([2020](#bib.bib111)) designed a
    feature selection module to adjust receptive fields adaptively, which proposes
    a channel attention network to fuse the feature extracted by using kernels of
    different sizes, aspect ratios, and orientations adaptively.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，注意力机制在建模长程依赖关系方面显示出了有希望的表现。它已广泛应用于图像分类（Mnih 等，[2014](#bib.bib108)）、自然语言处理（Vaswani
    等，[2017](#bib.bib140)）和通用目标检测（Yoo 等，[2015](#bib.bib185)；Li 等，[2019](#bib.bib84)；Tian
    等，[2022](#bib.bib137)；Yu 和 Ji，[2022](#bib.bib187)；Wang 等，[2019](#bib.bib152)）。近年来，提出了几种注意力方法（Pan
    等，[2020](#bib.bib111)；Yang 等，[2019a](#bib.bib177)，[2022a](#bib.bib175)）来捕获更有效的特征表示，这些方法通常结合了像素级注意力网络和通道级注意力网络，以抑制噪声并增强目标信息。SCRDet（Yang
    等，[2019a](#bib.bib177)）采用像素注意力网络生成可以将前景与背景分离的显著性图，并使用 Squeeze-and-Excitation (SE)
    模块（Hu 等，[2020](#bib.bib66)）作为通道注意力网络进一步增强显著性图。Pan 等（[2020](#bib.bib111)）设计了一个特征选择模块来自适应调整感受野，提出了一个通道注意力网络，通过自适应地融合不同大小、纵横比和方向的内核提取的特征。
- en: 'Transformer has first applied to natural language processing (Vaswani et al.,
    [2017](#bib.bib140)), which mainly adopts the self-attention mechanism to capture
    global feature representations. Recently, such architecture has achieved significant
    success in the field of computer vision (Dosovitskiy et al., [2021](#bib.bib31);
    Liu et al., [2021b](#bib.bib100); Han et al., [2023](#bib.bib50)). Benefit from
    the exceptional feature representation capacity, more and more Transformer-based
    methods are proposed for generic object detection and obtain impressive performance,
    which is mainly divided into two groups (Han et al., [2023](#bib.bib50)):'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 首次应用于自然语言处理（Vaswani 等，[2017](#bib.bib140)），主要采用自注意力机制以捕捉全局特征表示。最近，这种架构在计算机视觉领域取得了显著成功（Dosovitskiy
    等，[2021](#bib.bib31)；Liu 等，[2021b](#bib.bib100)；Han 等，[2023](#bib.bib50)）。得益于出色的特征表示能力，越来越多基于
    Transformer 的方法被提出用于通用目标检测，并取得了令人印象深刻的表现，这些方法主要分为两类（Han 等，[2023](#bib.bib50)）：
- en: '1) Transformer-based set prediction, which utilize set prediction to address
    detection tasks to remove hand-crafted components (e.g., anchor and NMS), including
    DETR (Carion et al., [2020](#bib.bib9)) and its variants (Zhu et al., [2021](#bib.bib211);
    Sun et al., [2021](#bib.bib131); Gao et al., [2021](#bib.bib36)); Based on DETR (Carion
    et al., [2020](#bib.bib9)), O²DETR (Ma et al., [2021](#bib.bib105)) was proposed
    to utilize the Transformer for the oriented object detection task. In addition,
    the depthwise separable convolutions (Sifre and Mallat, [2013](#bib.bib125); Chollet,
    [2017](#bib.bib21); Haase and Amthor, [2020](#bib.bib46)) is introduced to replace
    the computationally complex self-attention mechanism, making networks more lightweight
    and speeding up the training. However, there are several problems with Transformer-based
    detectors, including misalignment, dense distribution and limited matching. To
    tackle these issues,  Dai et al. ([2022](#bib.bib26)) proposed AO2-DETR based
    on Deformable DETR (Zhu et al., [2021](#bib.bib211)), comprising three components:
    an oriented proposal generation (OPG) mechanism for generating oriented region
    proposals as object queries; an adaptive oriented proposal refinement (OPR) module
    for aligning the convolutional features and adjusting the oriented proposals;
    and a rotation-aware set matching loss for ensuring the correct match between
    the predicted OBBs and ground truth.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 基于 Transformer 的集合预测，利用集合预测来解决检测任务，从而去除手工设计的组件（例如锚点和 NMS），包括 DETR（Carion
    等，[2020](#bib.bib9)）及其变体（Zhu 等，[2021](#bib.bib211)；Sun 等，[2021](#bib.bib131)；Gao
    等，[2021](#bib.bib36)）；基于 DETR（Carion 等，[2020](#bib.bib9)），O²DETR（Ma 等，[2021](#bib.bib105)）被提出用于面向目标检测任务。此外，引入了深度可分离卷积（Sifre
    和 Mallat，[2013](#bib.bib125)；Chollet，[2017](#bib.bib21)；Haase 和 Amthor，[2020](#bib.bib46)）以替代计算复杂的自注意力机制，使网络更加轻量化并加速训练。然而，基于
    Transformer 的检测器存在几个问题，包括错位、密集分布和匹配限制。为了解决这些问题，Dai 等（[2022](#bib.bib26)）提出了基于可变形
    DETR（Zhu 等，[2021](#bib.bib211)）的 AO2-DETR，包括三个组件：用于生成面向区域提案的面向提案生成（OPG）机制；用于对齐卷积特征并调整面向提案的自适应面向提案细化（OPR）模块；以及旋转感知集合匹配损失，以确保预测的
    OBB 和真实值之间的正确匹配。
- en: 2) Transformer-based feature representations, which replace some components
    of existing detection frameworks with Transformer or redesigned Transformer architectures,
    such as backbone networks (Han et al., [2021b](#bib.bib51); Wang et al., [2021c](#bib.bib151)),
    FPN architecture (Zhang et al., [2020b](#bib.bib191)), and prediction head (Hu
    et al., [2018](#bib.bib65); Chi et al., [2020](#bib.bib20)). To reduce the computational
    cost and memory footprint of full attention in ViT (Dosovitskiy et al., [2021](#bib.bib31);
    Xu et al., [2021b](#bib.bib169); Zhang et al., [2023](#bib.bib193)), Wang et al.
    ([2022](#bib.bib143)) design local rotated window attention, named rotated varied-size
    window attention (RVSA). RVSA can learn trainable scale, offsets and orientation
    to obtain the locally oriented windows at different sizes, locations and angles
    adaptively, enabling it to extract more useful feature representations. Furthermore,
    to explore the effectiveness of unsupervised pretraining, MAE (He et al., [2022](#bib.bib56))
    is employed to pretrain the model on the MillionAID dataset (Long et al., [2021](#bib.bib103)).
    With the pretraining method MAE and RVSA, the detector outperformed all previous
    methods, achieving $81.24\%$ and $71.05\%$ mAP on DOTA-V1.0 and DIOR-R datasets,
    respectively. However, the main limitation of Transformer is the long training
    convergence time and heavy computation cost, compared to CNN. The future effort
    is demanded to explore the potential of Transformer for oriented object detection
    and solve the speed bottleneck problem.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Transformer 的特征表示，通过将现有检测框架的一些组件替换为 Transformer 或重新设计的 Transformer 架构，如主干网络
    (Han et al., [2021b](#bib.bib51); Wang et al., [2021c](#bib.bib151))、FPN 架构 (Zhang
    et al., [2020b](#bib.bib191)) 和预测头 (Hu et al., [2018](#bib.bib65); Chi et al.,
    [2020](#bib.bib20))。为了减少 ViT 中全注意力的计算成本和内存占用 (Dosovitskiy et al., [2021](#bib.bib31);
    Xu et al., [2021b](#bib.bib169); Zhang et al., [2023](#bib.bib193))，Wang et al.
    ([2022](#bib.bib143)) 设计了局部旋转窗口注意力，即旋转可变大小窗口注意力 (RVSA)。RVSA 能够学习可训练的尺度、偏移量和方向，自适应地获取不同大小、位置和角度的局部定向窗口，从而提取更多有用的特征表示。此外，为了探索无监督预训练的有效性，MAE
    (He et al., [2022](#bib.bib56)) 被用于在 MillionAID 数据集 (Long et al., [2021](#bib.bib103))
    上预训练模型。借助预训练方法 MAE 和 RVSA，该检测器超越了所有先前的方法，在 DOTA-V1.0 和 DIOR-R 数据集上分别实现了 $81.24\%$
    和 $71.05\%$ 的 mAP。然而，与 CNN 相比，Transformer 的主要限制是较长的训练收敛时间和较高的计算成本。未来需要探索 Transformer
    在定向目标检测中的潜力，并解决速度瓶颈问题。
- en: Learning effective and rich feature representations plays a critical role in
    the field of oriented object detection. Although current feature representation
    algorithms have demonstrated effective and powerful performance in recent years,
    there remains tremendous potential for further development.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 学习有效且丰富的特征表示在定向目标检测领域扮演着至关重要的角色。尽管当前的特征表示算法近年来展示了有效且强大的性能，但仍然存在巨大的进一步发展潜力。
- en: 7 State-of-the-Art Methods
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 最先进的方法
- en: 'As a comprehensive survey on oriented object detection, this paper introduces
    recent advances and provides a structural taxonomy based on meta frameworks, OBB
    representations, and feature presentations in Sections [4](#S4 "4 Detection Frameworks
    ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey"), [5](#S5 "5 OBB Representations ‣ Oriented Object Detection in Optical
    Remote Sensing Images using Deep Learning: A Survey"), and [6](#S6 "6 Feature
    Representations ‣ Oriented Object Detection in Optical Remote Sensing Images using
    Deep Learning: A Survey"), respectively. In this section, we select several publicly
    available detectors to compare them in a unified manner. Specifically, we take
    DOTA-V1.0 dataset since it contains almost all the typical challenges of this
    task, including arbitrary orientations, large scale variations, and large aspect
    ratio. We report the performance of the state-of-the-art detectors in terms of
    mAP in Table [5](#S7.T5 "Table 5 ‣ 7 State-of-the-Art Methods ‣ Oriented Object
    Detection in Optical Remote Sensing Images using Deep Learning: A Survey"). The
    top three scores in each category are marked in bold, red, and blue, respectively.
    According to the performance comparison and previous discussion, we concentrate
    on the key elements that evolved in oriented object detection, including detection
    frameworks, feature representations, loss functions, and data augmentation.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '作为关于定向物体检测的全面调查，本文介绍了最新的进展，并基于元框架、OBB 表示和特征呈现提供了结构化分类，分别见于章节 [4](#S4 "4 Detection
    Frameworks ‣ Oriented Object Detection in Optical Remote Sensing Images using
    Deep Learning: A Survey")、[5](#S5 "5 OBB Representations ‣ Oriented Object Detection
    in Optical Remote Sensing Images using Deep Learning: A Survey") 和 [6](#S6 "6
    Feature Representations ‣ Oriented Object Detection in Optical Remote Sensing
    Images using Deep Learning: A Survey")。在这一部分，我们选择了几种公开的检测器，以统一的方式进行比较。具体来说，我们选取了
    DOTA-V1.0 数据集，因为它包含了几乎所有典型的挑战，包括任意方向、大尺度变化和大纵横比。我们在表格 [5](#S7.T5 "Table 5 ‣ 7
    State-of-the-Art Methods ‣ Oriented Object Detection in Optical Remote Sensing
    Images using Deep Learning: A Survey") 中报告了最新检测器在 mAP 方面的表现。每个类别的前三名得分分别用粗体、红色和蓝色标记。根据性能比较和前述讨论，我们集中关注定向物体检测中的关键元素，包括检测框架、特征表示、损失函数和数据增强。'
- en: '(1) Detection frameworks: two-stage vs one-stage.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 检测框架：双阶段与单阶段。
- en: Two-stage detectors achieve the best performance in terms of mAP, since they
    can extract accurate region-based features which are more suited for the tasks
    of classification and regression. The typical two-stage oriented object detection
    methods commonly designed a rotated proposal generation scheme to obtain more
    accurate proposals, such as RoI Transformer (Ding et al., [2019](#bib.bib29))
    and Oriented RCNN (Xie et al., [2021](#bib.bib166)). Similarly, the majority of
    one-stage detectors introduce a refined stage to align features, including R³Det (Yang
    et al., [2021b](#bib.bib174)), S²ANet (Han et al., [2022a](#bib.bib47)), CFA (Guo
    et al., [2021](#bib.bib44)), and Oriented RepPoints (Li et al., [2022b](#bib.bib85)).
    Benefiting from the additional refined stage and the advanced loss functions,
    one-stage detectors can also reach approximate accuracy to two-stage detectors.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 双阶段检测器在 mAP 方面表现最佳，因为它们可以提取准确的基于区域的特征，这些特征更适合分类和回归任务。典型的双阶段定向物体检测方法通常设计了旋转提议生成方案，以获得更准确的提议，例如
    RoI Transformer (Ding et al., [2019](#bib.bib29)) 和 Oriented RCNN (Xie et al.,
    [2021](#bib.bib166))。类似地，大多数单阶段检测器引入了一个精细化阶段来对齐特征，包括 R³Det (Yang et al., [2021b](#bib.bib174))、S²ANet (Han
    et al., [2022a](#bib.bib47))、CFA (Guo et al., [2021](#bib.bib44)) 和 Oriented RepPoints (Li
    et al., [2022b](#bib.bib85))。得益于额外的精细化阶段和先进的损失函数，单阶段检测器也能达到接近双阶段检测器的精度。
- en: '(2) Feature representations: CNN vs. Transformer.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 特征表示：CNN 与 Transformer。
- en: As one of the most important components in oriented object detection, backbone
    networks play a critical role in learning high-level semantic feature representation.
    The most widely used backbone networks include ResNet (He et al., [2016](#bib.bib58);
    Xie et al., [2017](#bib.bib165)) series and Transformer architectures (Dosovitskiy
    et al., [2021](#bib.bib31); Xu et al., [2021b](#bib.bib169); Zhang et al., [2023](#bib.bib193);
    Liu et al., [2021b](#bib.bib100)). The ResNet-based backbone is normally combined
    with FPN. Although Transformer-based methods have dominated the majority of computer
    vision tasks, they slightly outperform CNN-based counterparts in oriented object
    detection. Specifically, Oriented RCNN-RVSA (Wang et al., [2022](#bib.bib143)),
    as the top-performing detector based on Transformer, only surpasses Oriented RCNN (Xie
    et al., [2021](#bib.bib166)) by $0.37\%$ in terms of mAP. In addition, compared
    to CNNs, Transformers suffer from longer training convergence time and expensive
    computing costs.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 作为定向目标检测中最重要的组成部分之一，骨干网络在学习高级语义特征表示中起着关键作用。最广泛使用的骨干网络包括 ResNet (He et al., [2016](#bib.bib58);
    Xie et al., [2017](#bib.bib165)) 系列和 Transformer 架构 (Dosovitskiy et al., [2021](#bib.bib31);
    Xu et al., [2021b](#bib.bib169); Zhang et al., [2023](#bib.bib193); Liu et al.,
    [2021b](#bib.bib100))。基于 ResNet 的骨干网通常与 FPN 结合使用。尽管基于 Transformer 的方法在大多数计算机视觉任务中占据主导地位，但在定向目标检测中，它们仅略微超越了基于
    CNN 的对应方法。具体而言，作为基于 Transformer 的顶级检测器，Oriented RCNN-RVSA (Wang et al., [2022](#bib.bib143))
    仅在 mAP 指标上超越了 Oriented RCNN (Xie et al., [2021](#bib.bib166)) $0.37\%$。此外，与 CNN
    相比，Transformer 的训练收敛时间较长且计算成本较高。
- en: (3) Loss functions.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 损失函数。
- en: Advanced loss functions are conducive to alleviating the problems caused by
    orientation parameters and achieving better regression in one-stage detectors,
    including GWD (Yang et al., [2021c](#bib.bib176)), KLD (Yang et al., [2021d](#bib.bib178)),
    and KFIoU (Yang et al., [2022](#bib.bib180)). As can be seen in the results of
    multi-scale training and testing, when taking ResNet-152 as the backbone R³Det-GWD,
    R³Det-KLD, and R³Det-KFIoU outperform R³Det (Yang et al., [2021b](#bib.bib174))
    by $3.76\%$, $4.16\%$, and $4.56\%$ in terms of mAP, respectively. In particular,
    for objects with a large aspect ratio, such as bridge (BR) and harbor (HB), as
    well as objects with a large scale, such as ground track field (GTF) and Roundabout
    (RA), R³Det with advanced loss functions can achieve approximate $10\%$ mAP improvement.
    The reason may be that Gaussian distribution based methods can jointly optimize
    the OBB parameters.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 高级损失函数有助于缓解方向参数造成的问题，并在一阶段检测器中实现更好的回归，包括 GWD (Yang et al., [2021c](#bib.bib176))、KLD (Yang
    et al., [2021d](#bib.bib178)) 和 KFIoU (Yang et al., [2022](#bib.bib180))。从多尺度训练和测试结果可以看出，以
    ResNet-152 作为骨干网的 R³Det-GWD、R³Det-KLD 和 R³Det-KFIoU 在 mAP 指标上分别超越了 R³Det (Yang
    et al., [2021b](#bib.bib174)) $3.76\%$、$4.16\%$ 和 $4.56\%$。特别是对于具有大长宽比的物体，如桥梁（BR）和港口（HB），以及具有大尺度的物体，如地面跟踪场（GTF）和环形交叉路口（RA），R³Det
    通过高级损失函数可以实现约 $10\%$ 的 mAP 改进。原因可能是基于高斯分布的方法可以联合优化 OBB 参数。
- en: (4) Data augmentation.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 数据增强。
- en: 'The multiscale training and testing (MS) generally first resize the original
    images to three scales, i.e. $\{0.5,1.0,1.5\}$, which are then cropped to patches
    of $1,024\times 1,024$ with a stride of 524. In contrast, the single scale training
    and testing (SS) only crops the original images to patches of $1,024\times 1,024$
    with a stride of 824. As can be seen in Table [5](#S7.T5 "Table 5 ‣ 7 State-of-the-Art
    Methods ‣ Oriented Object Detection in Optical Remote Sensing Images using Deep
    Learning: A Survey"), detectors with MS achieve an average approximate $3\%$ improvement
    in terms of mAP. For objects with a large scale, such as ground track field (GTF)
    and soccer ball field (SBF), and objects with small scale and weak features, such
    as helicopter (HC), MS promotes detectors by $6\sim 8\%$ in terms of mAP. Extensive
    experiments have proven that MS is a useful strategy to alleviate scale variations.
    However, MS suffers from extremely long times for training and inference, which
    are about 10 times that of SS.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '多尺度训练和测试（MS）通常首先将原始图像调整为三种尺度，即 $\{0.5,1.0,1.5\}$，然后裁剪为 $1,024\times 1,024$
    的块，步幅为 524。相比之下，单尺度训练和测试（SS）仅将原始图像裁剪为 $1,024\times 1,024$ 的块，步幅为 824。正如表 [5](#S7.T5
    "Table 5 ‣ 7 State-of-the-Art Methods ‣ Oriented Object Detection in Optical Remote
    Sensing Images using Deep Learning: A Survey") 所示，使用 MS 的检测器在 mAP 方面通常平均提高了约 $3\%$。对于大尺度物体，如地面跟踪场（GTF）和足球场（SBF），以及小尺度和特征弱的物体，如直升机（HC），MS
    在 mAP 方面将检测器性能提升了 $6\sim 8\%$。大量实验已证明 MS 是缓解尺度变化的有效策略。然而，MS 训练和推理时间极长，大约是 SS 的
    10 倍。'
- en: 'Table 5: Comparisons of state-of-the-art methods on DOTA-V1.0.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：DOTA-V1.0 上的先进方法比较。
- en: '|  | Method | Backbone | PL¹ | BD | BR | GTF | SV | LV | SH | TC | BC | ST
    | SBF | RA | HA | SP | HC | mAP |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 主干网络 | PL¹ | BD | BR | GTF | SV | LV | SH | TC | BC | ST | SBF |
    RA | HA | SP | HC | mAP |'
- en: '| Single-scale |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 单尺度 |'
- en: '| One-stage | R³Det (Yang et al., [2021b](#bib.bib174)) | R-101 | 88.76 | 83.09
    | 50.91 | 67.27 | 76.23 | 80.39 | 86.72 | 90.78 | 84.68 | 83.24 | 61.98 | 61.35
    | 66.91 | 70.63 | 53.94 | 73.79 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 单阶段 | R³Det（Yang 等， [2021b](#bib.bib174)） | R-101 | 88.76 | 83.09 | 50.91
    | 67.27 | 76.23 | 80.39 | 86.72 | 90.78 | 84.68 | 83.24 | 61.98 | 61.35 | 66.91
    | 70.63 | 53.94 | 73.79 |'
- en: '| S²ANet (Han et al., [2022a](#bib.bib47)) | R-101 | 88.70 | 81.41 | 54.28
    | 69.75 | 78.04 | 80.54 | 88.04 | 90.69 | 84.75 | 86.22 | 65.03 | 65.81 | 76.16
    | 73.37 | 58.86 | 76.11 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| S²ANet（Han 等， [2022a](#bib.bib47)） | R-101 | 88.70 | 81.41 | 54.28 | 69.75
    | 78.04 | 80.54 | 88.04 | 90.69 | 84.75 | 86.22 | 65.03 | 65.81 | 76.16 | 73.37
    | 58.86 | 76.11 |'
- en: '| CFA^∗ (Guo et al., [2021](#bib.bib44)) | R-152 | 89.08 | 83.20 | 54.37 |
    66.87 | 81.23 | 80.96 | 87.17 | 90.21 | 84.32 | 86.09 | 52.34 | 69.94 | 75.52
    | 80.76 | 67.96 | 76.67 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| CFA^∗（Guo 等， [2021](#bib.bib44)） | R-152 | 89.08 | 83.20 | 54.37 | 66.87
    | 81.23 | 80.96 | 87.17 | 90.21 | 84.32 | 86.09 | 52.34 | 69.94 | 75.52 | 80.76
    | 67.96 | 76.67 |'
- en: '| R³Det-KLD (Yang et al., [2021d](#bib.bib178)) | R-50 | 88.90 | 84.17 | 55.80
    | 69.35 | 78.72 | 84.08 | 87.00 | 89.75 | 84.32 | 85.73 | 64.74 | 61.80 | 76.62
    | 78.49 | 70.89 | 77.36 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| R³Det-KLD（Yang 等， [2021d](#bib.bib178)） | R-50 | 88.90 | 84.17 | 55.80 |
    69.35 | 78.72 | 84.08 | 87.00 | 89.75 | 84.32 | 85.73 | 64.74 | 61.80 | 76.62
    | 78.49 | 70.89 | 77.36 |'
- en: '| R³Det-GWD (Yang et al., [2021c](#bib.bib176)) | R-152 | 88.74 | 82.63 | 54.88
    | 70.11 | 78.87 | 84.59 | 87.37 | 89.81 | 84.79 | 86.47 | 66.58 | 64.11 | 75.31
    | 78.43 | 70.87 | 77.57 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| R³Det-GWD（Yang 等， [2021c](#bib.bib176)） | R-152 | 88.74 | 82.63 | 54.88 |
    70.11 | 78.87 | 84.59 | 87.37 | 89.81 | 84.79 | 86.47 | 66.58 | 64.11 | 75.31
    | 78.43 | 70.87 | 77.57 |'
- en: '| Oriented RepPoints^∗ (Li et al., [2022b](#bib.bib85)) | Swin-T | 89.11 |
    82.32 | 56.71 | 74.95 | 80.70 | 83.73 | 87.67 | 90.81 | 87.11 | 85.85 | 63.60
    | 68.60 | 75.95 | 73.54 | 63.76 | 77.63 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| Oriented RepPoints^∗（Li 等， [2022b](#bib.bib85)） | Swin-T | 89.11 | 82.32
    | 56.71 | 74.95 | 80.70 | 83.73 | 87.67 | 90.81 | 87.11 | 85.85 | 63.60 | 68.60
    | 75.95 | 73.54 | 63.76 | 77.63 |'
- en: '| Two-stage | ROI Trans. (Ding et al., [2019](#bib.bib29)) | R-101 | 88.64
    | 78.52 | 43.44 | 75.92 | 68.81 | 73.68 | 83.59 | 90.74 | 77.27 | 81.46 | 58.39
    | 53.54 | 62.83 | 58.93 | 47.67 | 69.56 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 两阶段 | ROI 转换（Ding 等， [2019](#bib.bib29)） | R-101 | 88.64 | 78.52 | 43.44
    | 75.92 | 68.81 | 73.68 | 83.59 | 90.74 | 77.27 | 81.46 | 58.39 | 53.54 | 62.83
    | 58.93 | 47.67 | 69.56 |'
- en: '| SCRDet (Yang et al., [2019a](#bib.bib177)) | R-101 | 89.98 | 80.65 | 52.09
    | 68.36 | 68.36 | 60.32 | 72.41 | 90.85 | 87.94 | 86.86 | 65.02 | 66.68 | 66.25
    | 68.24 | 65.21 | 72.61 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| SCRDet（Yang 等， [2019a](#bib.bib177)） | R-101 | 89.98 | 80.65 | 52.09 | 68.36
    | 68.36 | 60.32 | 72.41 | 90.85 | 87.94 | 86.86 | 65.02 | 66.68 | 66.25 | 68.24
    | 65.21 | 72.61 |'
- en: '| Gliding Vertex (Xu et al., [2021a](#bib.bib168)) | R-50 | 89.64 | 85.00 |
    52.26 | 77.34 | 73.01 | 73.14 | 86.82 | 90.74 | 79.02 | 86.81 | 59.55 | 70.91
    | 72.94 | 70.86 | 57.32 | 75.02 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| Gliding Vertex（Xu 等， [2021a](#bib.bib168)） | R-50 | 89.64 | 85.00 | 52.26
    | 77.34 | 73.01 | 73.14 | 86.82 | 90.74 | 79.02 | 86.81 | 59.55 | 70.91 | 72.94
    | 70.86 | 57.32 | 75.02 |'
- en: '| AOPG^∗ (Cheng et al., [2022a](#bib.bib18)) | R-101 | 89.14 | 82.74 | 51.87
    | 69.28 | 77.65 | 82.42 | 88.08 | 90.89 | 86.26 | 85.13 | 60.60 | 66.30 | 74.05
    | 67.76 | 58.77 | 75.39 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| AOPG^∗ (Cheng et al., [2022a](#bib.bib18)) | R-101 | 89.14 | 82.74 | 51.87
    | 69.28 | 77.65 | 82.42 | 88.08 | 90.89 | 86.26 | 85.13 | 60.60 | 66.30 | 74.05
    | 67.76 | 58.77 | 75.39 |'
- en: '| DODet (Cheng et al., [2022b](#bib.bib19)) | R-101 | 89.61 | 83.10 | 51.43
    | 72.02 | 79.16 | 81.99 | 87.71 | 90.89 | 86.53 | 84.56 | 62.21 | 65.38 | 71.98
    | 70.79 | 61.93 | 75.89 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| DODet (Cheng et al., [2022b](#bib.bib19)) | R-101 | 89.61 | 83.10 | 51.43
    | 72.02 | 79.16 | 81.99 | 87.71 | 90.89 | 86.53 | 84.56 | 62.21 | 65.38 | 71.98
    | 70.79 | 61.93 | 75.89 |'
- en: '| SCRDet++ (Yang et al., [2022a](#bib.bib175)) | R-101 | 89.77 | 83.90 | 56.30
    | 73.98 | 72.60 | 75.63 | 82.82 | 90.76 | 87.89 | 86.14 | 65.24 | 63.17 | 76.05
    | 68.06 | 70.24 | 76.20 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| SCRDet++ (Yang et al., [2022a](#bib.bib175)) | R-101 | 89.77 | 83.90 | 56.30
    | 73.98 | 72.60 | 75.63 | 82.82 | 90.76 | 87.89 | 86.14 | 65.24 | 63.17 | 76.05
    | 68.06 | 70.24 | 76.20 |'
- en: '| ReDet (Han et al., [2021a](#bib.bib49)) | ReR-50 | 88.79 | 82.64 | 53.97
    | 74.00 | 78.13 | 84.06 | 88.04 | 90.89 | 87.78 | 85.75 | 61.76 | 60.39 | 75.96
    | 68.07 | 63.59 | 76.25 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| ReDet (Han et al., [2021a](#bib.bib49)) | ReR-50 | 88.79 | 82.64 | 53.97
    | 74.00 | 78.13 | 84.06 | 88.04 | 90.89 | 87.78 | 85.75 | 61.76 | 60.39 | 75.96
    | 68.07 | 63.59 | 76.25 |'
- en: '| Oriented RCNN (Xie et al., [2021](#bib.bib166)) | R-101 | 88.86 | 83.48 |
    55.27 | 76.92 | 74.27 | 82.10 | 87.52 | 90.90 | 85.56 | 85.33 | 65.51 | 66.82
    | 74.36 | 70.15 | 57.28 | 76.28 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Oriented RCNN (Xie et al., [2021](#bib.bib166)) | R-101 | 88.86 | 83.48 |
    55.27 | 76.92 | 74.27 | 82.10 | 87.52 | 90.90 | 85.56 | 85.33 | 65.51 | 66.82
    | 74.36 | 70.15 | 57.28 | 76.28 |'
- en: '| AO2-DETR (Dai et al., [2022](#bib.bib26)) | R-50 | 89.27 | 84.97 | 56.67
    | 74.89 | 78.87 | 82.73 | 87.35 | 90.50 | 84.68 | 85.41 | 61.97 | 69.96 | 74.68
    | 72.39 | 71.62 | 77.73 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| AO2-DETR (Dai et al., [2022](#bib.bib26)) | R-50 | 89.27 | 84.97 | 56.67
    | 74.89 | 78.87 | 82.73 | 87.35 | 90.50 | 84.68 | 85.41 | 61.97 | 69.96 | 74.68
    | 72.39 | 71.62 | 77.73 |'
- en: '| Oriented RCN-RVSA(Wang et al., [2022](#bib.bib143)) | ViTAE | 89.38 | 84.26
    | 59.39 | 73.19 | 79.99 | 85.36 | 88.08 | 90.87 | 88.50 | 86.53 | 58.93 | 72.24
    | 77.31 | 79.59 | 71.24 | 78.99 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| Oriented RCN-RVSA (Wang et al., [2022](#bib.bib143)) | ViTAE | 89.38 | 84.26
    | 59.39 | 73.19 | 79.99 | 85.36 | 88.08 | 90.87 | 88.50 | 86.53 | 58.93 | 72.24
    | 77.31 | 79.59 | 71.24 | 78.99 |'
- en: '| Multi-scale |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| Multi-scale |'
- en: '| One-stage | R³Det (Yang et al., [2021b](#bib.bib174)) | R-152 | 89.80 | 83.77
    | 48.11 | 66.77 | 78.76 | 83.27 | 87.84 | 90.82 | 85.38 | 85.51 | 65.67 | 62.68
    | 67.53 | 78.56 | 72.62 | 76.47 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| One-stage | R³Det (Yang et al., [2021b](#bib.bib174)) | R-152 | 89.80 | 83.77
    | 48.11 | 66.77 | 78.76 | 83.27 | 87.84 | 90.82 | 85.38 | 85.51 | 65.67 | 62.68
    | 67.53 | 78.56 | 72.62 | 76.47 |'
- en: '| R³Det-DCL (Yang et al., [2021a](#bib.bib170)) | R-152 | 89.26 | 83.60 | 53.54
    | 72.76 | 79.04 | 82.56 | 87.31 | 90.67 | 86.59 | 86.98 | 67.49 | 66.88 | 73.29
    | 70.56 | 69.99 | 77.37 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| R³Det-DCL (Yang et al., [2021a](#bib.bib170)) | R-152 | 89.26 | 83.60 | 53.54
    | 72.76 | 79.04 | 82.56 | 87.31 | 90.67 | 86.59 | 86.98 | 67.49 | 66.88 | 73.29
    | 70.56 | 69.99 | 77.37 |'
- en: '| S²ANet (Han et al., [2022a](#bib.bib47)) | R-50 | 88.89 | 83.60 | 57.74 |
    81.95 | 79.94 | 83.19 | 89.11 | 90.78 | 84.87 | 87.81 | 70.30 | 68.25 | 78.30
    | 77.01 | 69.58 | 79.42 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| S²ANet (Han et al., [2022a](#bib.bib47)) | R-50 | 88.89 | 83.60 | 57.74 |
    81.95 | 79.94 | 83.19 | 89.11 | 90.78 | 84.87 | 87.81 | 70.30 | 68.25 | 78.30
    | 77.01 | 69.58 | 79.42 |'
- en: '| R³Det-GWD (Yang et al., [2021c](#bib.bib176)) | R-152 | 89.66 | 84.99 | 59.26
    | 82.19 | 78.97 | 84.83 | 87.70 | 90.21 | 86.54 | 86.85 | 73.47 | 67.77 | 76.92
    | 79.22 | 74.92 | 80.23 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| R³Det-GWD (Yang et al., [2021c](#bib.bib176)) | R-152 | 89.66 | 84.99 | 59.26
    | 82.19 | 78.97 | 84.83 | 87.70 | 90.21 | 86.54 | 86.85 | 73.47 | 67.77 | 76.92
    | 79.22 | 74.92 | 80.23 |'
- en: '| R³Det-KLD (Yang et al., [2021d](#bib.bib178)) | R-152 | 89.92 | 85.13 | 59.19
    | 81.33 | 78.82 | 84.38 | 87.50 | 89.80 | 87.33 | 87.00 | 72.57 | 71.35 | 77.12
    | 79.34 | 78.68 | 80.63 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| R³Det-KLD (Yang et al., [2021d](#bib.bib178)) | R-152 | 89.92 | 85.13 | 59.19
    | 81.33 | 78.82 | 84.38 | 87.50 | 89.80 | 87.33 | 87.00 | 72.57 | 71.35 | 77.12
    | 79.34 | 78.68 | 80.63 |'
- en: '| R³Det-KFIoU (Yang et al., [2022](#bib.bib180)) | R-152 | 88.89 | 85.14 |
    60.05 | 81.13 | 81.78 | 85.71 | 88.27 | 90.87 | 87.12 | 87.91 | 69.77 | 73.70
    | 79.25 | 81.31 | 74.56 | 81.03 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| R³Det-KFIoU (Yang et al., [2022](#bib.bib180)) | R-152 | 88.89 | 85.14 |
    60.05 | 81.13 | 81.78 | 85.71 | 88.27 | 90.87 | 87.12 | 87.91 | 69.77 | 73.70
    | 79.25 | 81.31 | 74.56 | 81.03 |'
- en: '| Two-stage | SCRDet++ (Yang et al., [2022a](#bib.bib175)) | R-101 | 90.05
    | 84.39 | 55.44 | 73.99 | 77.54 | 71.11 | 86.05 | 90.67 | 87.32 | 87.08 | 69.62
    | 68.90 | 73.74 | 71.29 | 65.08 | 76.81 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| Two-stage | SCRDet++ (Yang et al., [2022a](#bib.bib175)) | R-101 | 90.05
    | 84.39 | 55.44 | 73.99 | 77.54 | 71.11 | 86.05 | 90.67 | 87.32 | 87.08 | 69.62
    | 68.90 | 73.74 | 71.29 | 65.08 | 76.81 |'
- en: '| AO2-DETR (Dai et al., [2022](#bib.bib26)) | R-50 | 89.95 | 84.52 | 56.90
    | 74.83 | 80.86 | 83.47 | 88.47 | 90.87 | 86.12 | 88.55 | 63.21 | 65.09 | 79.09
    | 82.88 | 73.46 | 79.22 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| AO2-DETR (Dai et al., [2022](#bib.bib26)) | R-50 | 89.95 | 84.52 | 56.90
    | 74.83 | 80.86 | 83.47 | 88.47 | 90.87 | 86.12 | 88.55 | 63.21 | 65.09 | 79.09
    | 82.88 | 73.46 | 79.22 |'
- en: '| ReDet (Han et al., [2021a](#bib.bib49)) | ReR-50 | 88.81 | 82.48 | 60.83
    | 80.82 | 78.34 | 86.06 | 88.31 | 90.87 | 88.77 | 87.03 | 68.65 | 66.90 | 79.26
    | 79.71 | 74.67 | 80.10 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| ReDet (Han et al., [2021a](#bib.bib49)) | ReR-50 | 88.81 | 82.48 | 60.83
    | 80.82 | 78.34 | 86.06 | 88.31 | 90.87 | 88.77 | 87.03 | 68.65 | 66.90 | 79.26
    | 79.71 | 74.67 | 80.10 |'
- en: '| ReDet-DEA (Liang et al., [2022](#bib.bib86)) | ReR-50 | 89.92 | 83.84 | 59.65
    | 79.88 | 80.11 | 87.96 | 88.17 | 90.31 | 88.93 | 88.46 | 68.93 | 65.94 | 78.04
    | 79.69 | 75.78 | 80.37 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| ReDet-DEA (Liang et al., [2022](#bib.bib86)) | ReR-50 | 89.92 | 83.84 | 59.65
    | 79.88 | 80.11 | 87.96 | 88.17 | 90.31 | 88.93 | 88.46 | 68.93 | 65.94 | 78.04
    | 79.69 | 75.78 | 80.37 |'
- en: '| DODet (Cheng et al., [2022b](#bib.bib19)) | R-50 | 89.96 | 85.52 | 58.01
    | 81.22 | 78.71 | 85.46 | 88.59 | 90.89 | 87.12 | 87.80 | 70.50 | 71.54 | 82.06
    | 77.43 | 74.47 | 80.62 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| DODet (Cheng et al., [2022b](#bib.bib19)) | R-50 | 89.96 | 85.52 | 58.01
    | 81.22 | 78.71 | 85.46 | 88.59 | 90.89 | 87.12 | 87.80 | 70.50 | 71.54 | 82.06
    | 77.43 | 74.47 | 80.62 |'
- en: '| AOPG^∗ (Cheng et al., [2022a](#bib.bib18)) | R-50 | 89.88 | 85.57 | 60.90
    | 81.51 | 78.70 | 85.29 | 88.85 | 90.89 | 87.60 | 87.65 | 71.66 | 68.69 | 82.31
    | 77.32 | 73.10 | 80.66 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| AOPG^∗ (Cheng et al., [2022a](#bib.bib18)) | R-50 | 89.88 | 85.57 | 60.90
    | 81.51 | 78.70 | 85.29 | 88.85 | 90.89 | 87.60 | 87.65 | 71.66 | 68.69 | 82.31
    | 77.32 | 73.10 | 80.66 |'
- en: '| Oriented RCNN (Xie et al., [2021](#bib.bib166)) | R-50 | 89.84 | 85.43 |
    61.09 | 79.82 | 79.71 | 85.35 | 88.82 | 90.88 | 86.68 | 87.73 | 72.21 | 70.80
    | 82.42 | 78.18 | 74.11 | 80.87 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| Oriented RCNN (Xie et al., [2021](#bib.bib166)) | R-50 | 89.84 | 85.43 |
    61.09 | 79.82 | 79.71 | 85.35 | 88.82 | 90.88 | 86.68 | 87.73 | 72.21 | 70.80
    | 82.42 | 78.18 | 74.11 | 80.87 |'
- en: '| RoI Trans.-KFIoU (Yang et al., [2022](#bib.bib180)) | Swin-T | 89.44 | 84.41
    | 62.22 | 82.51 | 80.10 | 86.07 | 88.68 | 90.90 | 87.32 | 88.38 | 72.80 | 71.95
    | 78.96 | 74.95 | 75.27 | 80.93 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| RoI Trans.-KFIoU (Yang et al., [2022](#bib.bib180)) | Swin-T | 89.44 | 84.41
    | 62.22 | 82.51 | 80.10 | 86.07 | 88.68 | 90.90 | 87.32 | 88.38 | 72.80 | 71.95
    | 78.96 | 74.95 | 75.27 | 80.93 |'
- en: '| Oriented RCNN-RVSA(Wang et al., [2022](#bib.bib143)) | ViTAE | 88.97 | 85.76
    | 61.46 | 81.27 | 79.98 | 85.31 | 88.30 | 90.84 | 85.06 | 87.50 | 66.77 | 73.11
    | 84.75 | 81.88 | 77.58 | 81.24 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| Oriented RCNN-RVSA(Wang et al., [2022](#bib.bib143)) | ViTAE | 88.97 | 85.76
    | 61.46 | 81.27 | 79.98 | 85.31 | 88.30 | 90.84 | 85.06 | 87.50 | 66.77 | 73.11
    | 84.75 | 81.88 | 77.58 | 81.24 |'
- en: '1'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: 'PL: plane. BD: baseball diamond. BR: bridge. GTF: ground track field. SV: small
    vehicle. LV: large vehicle. SH: ship. TC: tennis court. BC: baseball court. ST:
    storage tank. SBF: soccer ball field. RA: roundabout. HA: harbor. SP: swimming
    pool. HC: helicopter.'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'PL: 平面。BD: 棒球场。BR: 桥梁。GTF: 路面跑道。SV: 小型车辆。LV: 大型车辆。SH: 船。TC: 网球场。BC: 棒球场。ST:
    储罐。SBF: 足球场。RA: 环形交叉口。HA: 港口。SP: 游泳池。HC: 直升机。'
- en: '2'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2'
- en: The results of these methods are the best reported results from corresponding
    papers.. ^∗ indicates anchor-free methods.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些方法的结果是相应论文中报告的最佳结果。^∗表示无锚方法。
- en: 8 Conclusions and Future Directions
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论与未来方向
- en: Oriented object detection in RS images is an important and challenging task
    in the field of RS and has been actively investigated. As summarized in this survey,
    a variety of methods have been developed rapidly in recent years, showing remarkable
    progress. In this survey, we review firstly problem definition and summarize commonly
    used datasets and evaluation metrics. Then, we provide a structural taxonomy for
    detection frameworks and highlight milestone detectors. We also present a detailed
    elaboration of OBB-based representations and feature representations. Finally,
    we discuss and compare the excellent methods emerging in recent years.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在遥感图像中的方向性物体检测是遥感领域一个重要且具有挑战性的任务，并且已受到积极研究。正如本调查总结的那样，近年来各种方法迅速发展，取得了显著进展。在本调查中，我们首先回顾了问题定义，并总结了常用的数据集和评估指标。然后，我们提供了检测框架的结构分类，并突出了里程碑式的检测器。我们还详细阐述了基于OBB的表示和特征表示。最后，我们讨论并比较了近年来出现的优秀方法。
- en: 'Despite oriented object detection is conducive to alleviating the difficulties
    stemming from arbitrary orientations, dense arrangement, large aspect ratio, and
    the following common challenges in RS are still far from being solved:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管方向性物体检测有助于缓解由任意方向、密集排列、大长宽比等引起的困难，但遥感中的以下常见挑战仍远未解决：
- en: Complex background. Due to the wide visual field and complex earth’s surface,
    RS images typically contain a variety of complex backgrounds, causing significant
    interference in detection. It is obvious that objects are frequently surrounded
    by different backgrounds, requiring detectors to possess sufficient discrimination.
    Besides, there may be backgrounds containing textures and shapes similar to objects,
    causing a large number of false alarms.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂背景。由于广阔的视野和复杂的地球表面，遥感图像通常包含多种复杂背景，导致检测中出现显著干扰。显而易见，物体经常被不同背景包围，这要求检测器具有足够的辨别能力。此外，背景中可能包含与物体相似的纹理和形状，导致大量虚警。
- en: Changing environmental conditions. The image quality can be easily affected
    by environmental conditions changes such as illumination, weather, seasons, and
    cloud. The originally clear images may appear as shadows, occlusion, blur, and
    noise, increasing the challenges of detection.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 环境条件变化。图像质量容易受到环境条件变化的影响，例如光照、天气、季节和云层。原本清晰的图像可能会出现阴影、遮挡、模糊和噪声，从而增加了检测的难度。
- en: Scale variations. As the ground sampling distance (GSD) of sensors can vary
    from a few centimeters to hundreds of meters, the RS images taken by different
    sensors at the same scene usually have large scale variations. Additionally, while
    different kinds of objects may have large scale variations, the instances of the
    same category also vary in size. Therefore, the inter-class and intra-class scale
    variations pose additional challenges.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 尺度变化。由于传感器的地面采样距离（GSD）可以从几厘米到几百米不等，因此不同传感器在同一场景下拍摄的RS图像通常具有较大的尺度变化。此外，虽然不同类型的对象可能具有较大的尺度变化，但同一类别的实例大小也会有所不同。因此，类间和类内尺度变化带来了额外的挑战。
- en: Imbalance problems. Firstly, the number of different categories may be imbalanced,
    i.e. the foreground-foreground imbalance. Thus, categories with more instances
    will dominate the gradients during training, leading to performance degradation.
    Secondly, only a small region of RS images contains objects, whereas the majority
    belongs to the background, resulting in an extreme foreground-background imbalance.
    As a result, most locations are backgrounds that can not provide useful information,
    causing inefficient training. Thirdly, the location distributions of objects are
    not uniform, most current detectors assume each part of the image with equal in
    weight.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡问题。首先，不同类别的数量可能不平衡，即前景-前景不平衡。因此，实例较多的类别将在训练过程中主导梯度，导致性能下降。其次，RS图像中只有小区域包含对象，而大多数属于背景，造成极端的前景-背景不平衡。因此，大多数位置是背景，无法提供有用的信息，导致训练效率低下。第三，对象的位置分布不均匀，目前的大多数检测器假设图像的每个部分具有相等的权重。
- en: Therefore, based on these challenges, we provide some developing directions
    for oriented RS object detection on future prospects.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基于这些挑战，我们提供了一些面向未来的RS目标检测的发展方向。
- en: Domain Adaptation. The training process of conventional deep learning based
    models is generally based on the i.i.d. assumption that the training and testing
    samples have identical and independent distribution (Schölkopf et al., [2007](#bib.bib122)).
    Generally, as data-driven techniques, conventional deep learning based methods
    rely heavily on the diversity of training data to adapt to different scenarios.
    However, collecting and annotating enough RS images of all possible domains for
    model training is expensive, time-consuming, and even prohibitively impossible,
    especially in the military field (Wang and Deng, [2018](#bib.bib145); Wang et al.,
    [2021a](#bib.bib144)). In real-world RS applications, oriented object detectors
    may need to process various images collected at different times (day or night,
    summer or winter), in different sensors (e.g., SAR, optical, LIDAR, infrared),
    or under different conditions (weather, illuminance, camera pose, image quality),
    resulting in the domain distribution gaps between training images and testing
    images and degrading the performance at test time. To this end, it is desirable
    to investigate useful and efficient domain adaptation theories and techniques
    that can guide model design and enhance the generalization capability of the detectors.
    Recently, a large amount of valuable and instructive methods for domain adaptation
    have achieved many inspiring results on visual tasks, including discrepancy-based
    methods (Yosinski et al., [2014](#bib.bib186)), adversarial-based methods (Goodfellow
    et al., [2014](#bib.bib42); Tzeng et al., [2017](#bib.bib139)), and reconstruction-based
    methods (Ghifary et al., [2015](#bib.bib38), [2016](#bib.bib39)). In future work,
    adopting the above techniques of domain adaptation in oriented object detection
    is a promising research direction.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 域自适应。传统深度学习模型的训练过程通常基于i.i.d.假设，即训练样本和测试样本具有相同且独立的分布（Schölkopf等，[2007](#bib.bib122)）。通常，作为数据驱动技术，传统深度学习方法在很大程度上依赖于训练数据的多样性来适应不同的场景。然而，收集和标注足够的所有可能领域的遥感图像用于模型训练是昂贵的、耗时的，甚至是无法实现的，特别是在军事领域（Wang和Deng，[2018](#bib.bib145)；Wang等，[2021a](#bib.bib144)）。在现实世界的遥感应用中，面向目标的检测器可能需要处理在不同时间（白天或夜晚，夏季或冬季）、不同传感器（例如，SAR、光学、LIDAR、红外）或不同条件（天气、光照、相机姿态、图像质量）下收集的各种图像，这会导致训练图像和测试图像之间的领域分布差异，并在测试时降低性能。为此，研究有用且高效的领域自适应理论和技术，以指导模型设计并增强检测器的泛化能力，是非常有价值的。最近，大量有价值且具有指导性的领域自适应方法在视觉任务中取得了许多鼓舞人心的成果，包括基于差异的方法（Yosinski等，[2014](#bib.bib186)）、基于对抗的方法（Goodfellow等，[2014](#bib.bib42)；Tzeng等，[2017](#bib.bib139)）和基于重建的方法（Ghifary等，[2015](#bib.bib38)，[2016](#bib.bib39)）。未来的工作中，将上述领域自适应技术应用于面向目标的检测是一个有前景的研究方向。
- en: 'Scale Adaption. Significant scale variations of various objects in RS images
    pose a great challenge for object detection. Especially, the most critical challenge
    mainly focuses on small object detection due to the following reasons: 1) Insufficient
    feature information. This is mainly due to the small objects occupying only a
    small area of the image. 2) Inaccurate localization. The down-sampling operation
    of backbone networks will lead to inaccurate localization, especially on the tasks
    of oriented object detection that are highly sensitive to angle regression error.
    In recent years, a large number of effective strategies have been made to enhance
    the robustness and adaptability of detectors for objects with various scales,
    mainly including: 1) Powerful backbone network, FPN (Lin et al., [2017](#bib.bib88))
    and its variants, which can generate semantic-strong high-resolution features.
    2) Data augmentation methods, e.g., copy-pasting small objects multiple times
    to enhance the diversity of small objects (Kisantal et al., [2019](#bib.bib72)),
    multi-scale training and test for improving the scale of small objects (Singh
    and Davis, [2018](#bib.bib127); Singh et al., [2018](#bib.bib128)). Despite these
    advances, there is still a significant gap between the detection accuracy of small
    and medium or large objects. Besides, due to computational cost, it is difficult
    to directly apply the above techniques in real-world applications. Therefore,
    there is still plenty of space to develop scale-adaption methods.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 尺度适应。遥感图像中各种物体的显著尺度变化对物体检测提出了很大挑战。尤其是，最关键的挑战主要集中在小物体检测上，原因如下：1) 特征信息不足。这主要是由于小物体仅占图像的很小区域。2)
    定位不准确。主干网络的下采样操作会导致定位不准确，特别是在对角度回归误差高度敏感的定向物体检测任务中。近年来，已经提出了大量有效的策略来增强检测器对各种尺度物体的鲁棒性和适应性，主要包括：1)
    强大的主干网络，FPN (Lin et al., [2017](#bib.bib88)) 及其变体，可以生成语义强的高分辨率特征。2) 数据增强方法，例如，通过多次复制粘贴小物体来增强小物体的多样性
    (Kisantal et al., [2019](#bib.bib72))，以及对小物体进行多尺度训练和测试以提高其尺度 (Singh and Davis,
    [2018](#bib.bib127); Singh et al., [2018](#bib.bib128))。尽管取得了这些进展，但小物体与中等或大型物体的检测精度之间仍存在显著差距。此外，由于计算成本，直接将上述技术应用于实际应用中仍然困难。因此，尺度适应方法的开发仍有很大的空间。
- en: 'Long-Tailed Oriented Object Detection. To address the problem of extreme class
    imbalance widely existing in real-world scenarios, several recent efforts start
    to focus on long-tailed object detection (Gupta et al., [2019](#bib.bib45); Wang
    et al., [2021b](#bib.bib150); Li et al., [2022a](#bib.bib82)). Common methods
    can be divided into data re-sampling and loss re-weighting. The former is achieved
    by over-sampling the tail or under-sampling the head classes. Hence, it is limited
    when encountering extreme class imbalance (Shen et al., [2016](#bib.bib123); Gupta
    et al., [2019](#bib.bib45); Wang et al., [2020](#bib.bib149)). The latter is a
    popular way that emphasizes the impact of tail categories by modifying the loss
    function (Tan et al., [2020a](#bib.bib135); Wang et al., [2021b](#bib.bib150);
    Li et al., [2022a](#bib.bib82)). Despite achieving significant success, almost
    all those methods are developed with horizontal object detectors. Thus, in the
    RS scenarios which also suffer from severe class imbalance, as shown in Figure
    [5](#S3.F5 "Figure 5 ‣ 3.1 Datasets ‣ 3 Datasets and Performance Evaluation ‣
    Oriented Object Detection in Optical Remote Sensing Images using Deep Learning:
    A Survey"), long-tailed oriented object detection has not been explored so far.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 长尾定向物体检测。为了解决现实场景中广泛存在的极端类别不平衡问题，最近一些研究开始关注长尾物体检测 (Gupta et al., [2019](#bib.bib45);
    Wang et al., [2021b](#bib.bib150); Li et al., [2022a](#bib.bib82))。常见的方法可以分为数据重采样和损失重加权。前者通过对尾部类别进行过采样或对头部类别进行欠采样来实现。因此，当遇到极端类别不平衡时，它的效果有限
    (Shen et al., [2016](#bib.bib123); Gupta et al., [2019](#bib.bib45); Wang et al.,
    [2020](#bib.bib149))。后者是一种流行的方法，通过修改损失函数来强调尾部类别的影响 (Tan et al., [2020a](#bib.bib135);
    Wang et al., [2021b](#bib.bib150); Li et al., [2022a](#bib.bib82))。尽管取得了显著成功，但几乎所有这些方法都是针对水平物体检测器开发的。因此，在同样面临严重类别不平衡的遥感场景中，如图
    [5](#S3.F5 "图 5 ‣ 3.1 数据集 ‣ 3 数据集和性能评估 ‣ 使用深度学习的光学遥感图像中的定向物体检测：综述") 所示，长尾定向物体检测尚未得到探索。
- en: Multimodal Information Fusion. Multimodal data has become easy to access in
    both military and civil fields with the rapid development of a variety of RS technologies,
    such as Global Positioning System (GPS), Inertial Measurement Unit (IMU), UAV,
    satellite, and various sensors. In recent years, the most popular multimodal information
    fusion methods are based on attention mechanisms (Nam et al., [2017](#bib.bib110))
    and bilinear pooling (Ben-younes et al., [2017](#bib.bib1)), which have provided
    remarkable improvements for applications related to image captioning (Vinyals
    et al., [2015](#bib.bib141)), text-to-image generation (Zhu et al., [2019](#bib.bib210)),
    and visual question answering (Wang et al., [2018b](#bib.bib147)). Although multimodal
    information fusion methods are attracting growing attention (Zhang et al., [2020a](#bib.bib190)),
    there are still a number of challenging problems in the fields of RS and oriented
    object detection that are far from being solved. The first problem is how to integrate
    various multimodal RS information, including the poses of sensors, multispectral
    data, and point clouds, to improve detection performance, since these data are
    drastically different from each other naturally. Furthermore, multimodal information
    fusion technologies are important in extending oriented object detection to other
    RS applications, including instance segmentation (Chen et al., [2019](#bib.bib11))
    and object tracking (Wen et al., [2021](#bib.bib156)). Finally, transferring well-trained
    detectors to other modalities is also a challenging task.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态信息融合。随着各种遥感技术如全球定位系统（GPS）、惯性测量单元（IMU）、无人机、卫星和各种传感器的迅速发展，多模态数据在军事和民用领域变得易于获取。近年来，最受欢迎的多模态信息融合方法基于注意力机制（Nam
    et al., [2017](#bib.bib110)）和双线性池化（Ben-younes et al., [2017](#bib.bib1)），这些方法在图像描述（Vinyals
    et al., [2015](#bib.bib141)）、文本到图像生成（Zhu et al., [2019](#bib.bib210)）和视觉问答（Wang
    et al., [2018b](#bib.bib147)）等应用中提供了显著的改进。尽管多模态信息融合方法正受到越来越多的关注（Zhang et al.,
    [2020a](#bib.bib190)），但在遥感和定向目标检测领域仍然存在许多具有挑战性的问题尚未解决。第一个问题是如何整合各种多模态遥感信息，包括传感器的姿态、多光谱数据和点云，以提高检测性能，因为这些数据自然差异巨大。此外，多模态信息融合技术在将定向目标检测扩展到其他遥感应用中也很重要，包括实例分割（Chen
    et al., [2019](#bib.bib11)）和目标跟踪（Wen et al., [2021](#bib.bib156)）。最后，将经过良好训练的检测器转移到其他模态也是一个具有挑战性的任务。
- en: Lightweight Methods. The demand for deploying a real-time object detector on
    resource-constrained mobile devices has grown rapidly. Therefore, achieving a
    better trade-off between accuracy and efficiency for detectors is of significant
    importance. To this end, various lightweight architectures have been proposed
    to reduce computational complexity and spatial complexity without harming accuracy.
    Several works try to redesign lightweight network architectures based on hand-crafted
    technologies (Jiang et al., [2022](#bib.bib69)) or Neural Architecture Search (Xiong
    et al., [2021](#bib.bib167)). Another branch of work presents various compression
    schemes for compressing network, including parameter pruning (Hanson and Pratt,
    [1988](#bib.bib54); Han et al., [2015](#bib.bib52)), quantization (Song et al.,
    [2016](#bib.bib129)), and knowledge distillation (Hinton et al., [2015](#bib.bib62)).
    Excellent lightweight generic object detection methods (Xiong et al., [2021](#bib.bib167);
    Jiang et al., [2022](#bib.bib69); Mehta and Rastegari, [2021](#bib.bib107)) depend
    on the ingenious designs of lightweight feature extraction networks and the efficient
    information transmission mechanism. Such practices provide an enormous reference
    value for oriented object detection. In addition, different lightweight methods
    may be synergistic and complementary to each other. Thus, reasonable design and
    combination can achieve outstanding performance. The development of lightweight
    methods is conducive to the promotion and real-world application of oriented object
    detection in the RS field, which drives the requirement for further research.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 轻量化方法。对资源受限的移动设备上部署实时物体检测器的需求迅速增长。因此，实现检测器在准确性和效率之间的更好权衡至关重要。为此，提出了各种轻量化架构，以减少计算复杂性和空间复杂性而不损害准确性。一些研究尝试基于手工设计技术（Jiang
    et al., [2022](#bib.bib69)）或神经架构搜索（Xiong et al., [2021](#bib.bib167)）重新设计轻量化网络架构。另一个研究方向提出了多种网络压缩方案，包括参数剪枝（Hanson
    and Pratt, [1988](#bib.bib54); Han et al., [2015](#bib.bib52)）、量化（Song et al.,
    [2016](#bib.bib129)）和知识蒸馏（Hinton et al., [2015](#bib.bib62)）。优秀的轻量化通用物体检测方法（Xiong
    et al., [2021](#bib.bib167); Jiang et al., [2022](#bib.bib69); Mehta and Rastegari,
    [2021](#bib.bib107)）依赖于轻量化特征提取网络的巧妙设计和高效的信息传递机制。这些做法为定向物体检测提供了巨大的参考价值。此外，不同的轻量化方法可能具有协同和互补作用。因此，合理的设计和组合可以实现卓越的性能。轻量化方法的发展有助于推动定向物体检测在遥感领域的推广和实际应用，进而推动进一步研究的需求。
- en: Video Oriented Object Detection. Aiming at exploiting temporal con-textual information
    across different frames, video oriented object detector (VO²D) estimates the location
    of different objects in each frame of a video. An accurate and efficient video
    object detection method is of great importance for real-time applications in RS
    scenarios, such as security monitoring and tracking. Nevertheless, VO²D suffers
    from performance degradation of image quality caused by motion blur, video defocus,
    scale variation, and occlusion. In addition, the change of viewpoint and scale
    caused by the pose variations of UAVs can also result in adverse influence and
    make the task more challenging. To solve the above issues, a vast number of techniques
    were exploited, including LSTM (Hochreiter and Schmidhuber, [1997](#bib.bib63);
    Zhu and Liu, [2018](#bib.bib209)), feature flow (Zhu et al., [2017](#bib.bib213)),
    feature calibration (Wang et al., [2018c](#bib.bib148)), and features aggregation (Shvets
    et al., [2019](#bib.bib124); Chen et al., [2020a](#bib.bib14); He et al., [2020a](#bib.bib55)).
    However, existing datasets for video object detection (Russakovsky et al., [2015](#bib.bib120);
    Damen et al., [2018](#bib.bib28)) only contain natural scene images and annotate
    objects with HBBs. As a consequence, the existing video object detectors are not
    suitable in RS scenarios, due to their challenging characteristics, including
    scale variation, viewpoint changes, complex background, and arbitrary orientation.
    Therefore, it is urgent to promote the construction of RS datasets for video object
    detection and to develop video-oriented object detection algorithms.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 视频导向的物体检测。旨在利用不同帧之间的时间上下文信息，视频导向物体检测器（VO²D）估计视频每一帧中不同物体的位置。对于实时应用，如安全监控和跟踪，一个准确高效的视频物体检测方法至关重要。然而，VO²D
    受到运动模糊、视频失焦、尺度变化和遮挡等因素造成的图像质量下降的影响。此外，无人机姿态变化导致的视角和尺度变化也会产生不利影响，使任务更加具有挑战性。为解决上述问题，开发了大量技术，包括
    LSTM（Hochreiter 和 Schmidhuber，[1997](#bib.bib63)；Zhu 和 Liu，[2018](#bib.bib209)），特征流（Zhu
    等，[2017](#bib.bib213)），特征校准（Wang 等，[2018c](#bib.bib148)），以及特征聚合（Shvets 等，[2019](#bib.bib124)；Chen
    等，[2020a](#bib.bib14)；He 等，[2020a](#bib.bib55)）。然而，现有的视频物体检测数据集（Russakovsky 等，[2015](#bib.bib120)；Damen
    等，[2018](#bib.bib28)）仅包含自然场景图像，并用 HBBs 标注物体。因此，现有的视频物体检测器不适用于 RS 场景，因为它们具有挑战性的特征，包括尺度变化、视角变化、复杂背景和任意方向。因此，迫切需要推动
    RS 数据集的建设和发展视频导向物体检测算法。
- en: Object Instance Segmentation. There are massive RS objects that are irregular,
    including roads and buildings, only using OBB to annotate them is not precise.
    To achieve a deeper and more detailed understanding of objects, it is necessary
    to research pixel-wise object segmentation. Recently, the success of the Segment
    Anything Model (SAM) shows its huge potential in computer vision tasks (Kirillov
    et al., [2023](#bib.bib71)). With the support of SAM, pixel-level semantic labels
    are obtained and large-scale segmentation RS datasets are constructed (Wang et al.,
    [2023](#bib.bib142)). All in all, it is a very promising research direction in
    the field of RS and can be significant in potential applications.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 物体实例分割。遥感图像中存在大量不规则物体，如道路和建筑，仅使用 OBB 进行标注是不精确的。为了更深入、详细地理解物体，需要研究像素级物体分割。最近，Segment
    Anything Model (SAM) 的成功展示了其在计算机视觉任务中的巨大潜力（Kirillov 等，[2023](#bib.bib71)）。在 SAM
    的支持下，获得了像素级语义标签，并构建了大规模的分割 RS 数据集（Wang 等，[2023](#bib.bib142)）。总之，这是遥感领域一个非常有前景的研究方向，并且在潜在应用中具有重要意义。
- en: References
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ben-younes et al. (2017) Ben-younes, H., Cadene, R., Cord, M., Thome, N., 2017.
    Mutan: Multimodal tucker fusion for visual question answering, in: Proceedings
    of the IEEE International Conference on Computer Vision, pp. 2631–2639. doi:[10.1109/ICCV.2017.285](https:/doi.org/10.1109/ICCV.2017.285).'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ben-younes 等（2017）Ben-younes, H., Cadene, R., Cord, M., Thome, N., 2017. Mutan:
    多模态塔克融合用于视觉问答，见：IEEE 国际计算机视觉会议论文集，第 2631–2639 页。doi:[10.1109/ICCV.2017.285](https:/doi.org/10.1109/ICCV.2017.285)。'
- en: Benedek et al. (2012) Benedek, C., Descombes, X., Zerubia, J., 2012. Building
    development monitoring in multitemporal remotely sensed image pairs with stochastic
    birth-death dynamics. IEEE Transactions on Pattern Analysis and Machine Intelligence.
    34, 33–50. doi:[10.1109/TPAMI.2011.94](https:/doi.org/10.1109/TPAMI.2011.94).
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benedek 等 (2012) Benedek, C., Descombes, X., Zerubia, J., 2012. 使用随机出生-死亡动态监测多时相遥感图像对中的建筑物发展。IEEE模式分析与机器智能学报，34，33–50。doi:[10.1109/TPAMI.2011.94](https:/doi.org/10.1109/TPAMI.2011.94)。
- en: Blaschke (2010) Blaschke, T., 2010. Object based image analysis for remote sensing.
    ISPRS Journal of Photogrammetry and Remote Sensing. 65, 2–16. doi:[10.1016/j.isprsjprs.2009.06.004](https:/doi.org/10.1016/j.isprsjprs.2009.06.004).
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blaschke (2010) Blaschke, T., 2010. 面向遥感的基于对象的图像分析。ISPRS摄影测量与遥感杂志，65，2–16。doi:[10.1016/j.isprsjprs.2009.06.004](https:/doi.org/10.1016/j.isprsjprs.2009.06.004)。
- en: Blaschke et al. (2014) Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann,
    P., Addink, E., Queiroz Feitosa, R., van der Meer, F., van der Werff, H., van
    Coillie, F., Tiede, D., 2014. Geographic object-based image analysis – towards
    a new paradigm. ISPRS Journal of Photogrammetry and Remote Sensing. 87, 180–191.
    doi:[10.1016/j.isprsjprs.2013.09.014](https:/doi.org/10.1016/j.isprsjprs.2013.09.014).
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blaschke 等 (2014) Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P.,
    Addink, E., Queiroz Feitosa, R., van der Meer, F., van der Werff, H., van Coillie,
    F., Tiede, D., 2014. 基于地理对象的图像分析——迈向新范式。ISPRS摄影测量与遥感杂志，87，180–191。doi:[10.1016/j.isprsjprs.2013.09.014](https:/doi.org/10.1016/j.isprsjprs.2013.09.014)。
- en: de Boer et al. (2005) de Boer, P.T., Kroese, D.P., Mannor, S., 2005. A tutorial
    on the cross-entropy method. Annals of Operations Research. 134, 19–67. doi:[10.1007/s10479-005-5724-z](https:/doi.org/10.1007/s10479-005-5724-z).
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Boer 等 (2005) de Boer, P.T., Kroese, D.P., Mannor, S., 2005. 交叉熵方法教程。运筹学年鉴，134，19–67。doi:[10.1007/s10479-005-5724-z](https:/doi.org/10.1007/s10479-005-5724-z)。
- en: 'Brunetti et al. (2018) Brunetti, A., Buongiorno, D., Trotta, G.F., Bevilacqua,
    V., 2018. Computer vision and deep learning techniques for pedestrian detection
    and tracking: A survey. Neurocomputing. 300, 17–33. doi:[10.1016/j.neucom.2018.01.092](https:/doi.org/10.1016/j.neucom.2018.01.092).'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brunetti 等 (2018) Brunetti, A., Buongiorno, D., Trotta, G.F., Bevilacqua, V.,
    2018. 用于行人检测和跟踪的计算机视觉与深度学习技术：综述。神经计算，300，17–33。doi:[10.1016/j.neucom.2018.01.092](https:/doi.org/10.1016/j.neucom.2018.01.092)。
- en: Burochin et al. (2014) Burochin, J.P., Vallet, B., Brédif, M., Mallet, C., Brosset,
    T., Paparoditis, N., 2014. Detecting blind building façades from highly overlapping
    wide angle aerial imagery. ISPRS Journal of Photogrammetry and Remote Sensing.
    96, 193–209. doi:[10.1016/j.isprsjprs.2014.07.011](https:/doi.org/10.1016/j.isprsjprs.2014.07.011).
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burochin 等 (2014) Burochin, J.P., Vallet, B., Brédif, M., Mallet, C., Brosset,
    T., Paparoditis, N., 2014. 从高度重叠的广角航拍图像中检测盲建筑立面。ISPRS摄影测量与遥感杂志，96，193–209。doi:[10.1016/j.isprsjprs.2014.07.011](https:/doi.org/10.1016/j.isprsjprs.2014.07.011)。
- en: 'Cai and Vasconcelos (2018) Cai, Z., Vasconcelos, N., 2018. Cascade r-cnn: Delving
    into high quality object detection, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 6154–6162. doi:[10.1109/CVPR.2018.00644](https:/doi.org/10.1109/CVPR.2018.00644).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai 和 Vasconcelos (2018) Cai, Z., Vasconcelos, N., 2018. Cascade r-cnn: 深入探讨高质量目标检测，载于：IEEE/CVF计算机视觉与模式识别会议论文集，第6154–6162页。doi:[10.1109/CVPR.2018.00644](https:/doi.org/10.1109/CVPR.2018.00644)。'
- en: 'Carion et al. (2020) Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,
    A., Zagoruyko, S., 2020. End-to-end object detection with transformers, in: Proceedings
    of the European Conference on Computer Vision, pp. 213–229. doi:[10.1007/978-3-030-58452-8_13](https:/doi.org/10.1007/978-3-030-58452-8_13).'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carion 等 (2020) Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,
    A., Zagoruyko, S., 2020. 基于变换器的端到端目标检测，载于：欧洲计算机视觉会议论文集，第213–229页。doi:[10.1007/978-3-030-58452-8_13](https:/doi.org/10.1007/978-3-030-58452-8_13)。
- en: 'Chavali et al. (2016) Chavali, N., Agrawal, H., Mahendru, A., Batra, D., 2016.
    Object-proposal evaluation protocol is ‘gameable’, in: Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 835–844. doi:[10.1109/CVPR.2016.97](https:/doi.org/10.1109/CVPR.2016.97).'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chavali 等 (2016) Chavali, N., Agrawal, H., Mahendru, A., Batra, D., 2016. 对象提议评估协议是‘可操控的’，载于：IEEE计算机视觉与模式识别会议论文集，第835–844页。doi:[10.1109/CVPR.2016.97](https:/doi.org/10.1109/CVPR.2016.97)。
- en: 'Chen et al. (2019) Chen, K., Pang, J., Wang, J., Xiong, Y., Li, X., Sun, S.,
    Feng, W., Liu, Z., Shi, J., Ouyang, W., Loy, C.C., Lin, D., 2019. Hybrid task
    cascade for instance segmentation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 4969–4978. doi:[10.1109/CVPR.2019.00511](https:/doi.org/10.1109/CVPR.2019.00511).'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2019） Chen, K., Pang, J., Wang, J., Xiong, Y., Li, X., Sun, S., Feng,
    W., Liu, Z., Shi, J., Ouyang, W., Loy, C.C., Lin, D., 2019. 实例分割的混合任务级联，见：IEEE/CVF
    计算机视觉与模式识别会议论文集，第 4969–4978 页。doi:[10.1109/CVPR.2019.00511](https:/doi.org/10.1109/CVPR.2019.00511)。
- en: 'Chen et al. (2020) Chen, K., Wu, M., Liu, J., Zhang, C., 2020. FGSD: A Dataset
    for Fine-Grained Ship Detection in High Resolution Satellite Images. arXiv e-prints.
    , arXiv:2003.06832.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2020） Chen, K., Wu, M., Liu, J., Zhang, C., 2020. FGSD：高分辨率卫星图像中细粒度船只检测数据集。arXiv
    电子预印本。 , arXiv:2003.06832。
- en: 'Chen et al. (2018) Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille,
    A.L., 2018. Deeplab: Semantic image segmentation with deep convolutional nets,
    atrous convolution, and fully connected crfs. IEEE Transactions on Pattern Analysis
    and Machine Intelligence. 40, 834–848. doi:[10.1109/TPAMI.2017.2699184](https:/doi.org/10.1109/TPAMI.2017.2699184).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2018） Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.,
    2018. Deeplab：基于深度卷积网络、膨胀卷积和完全连接的 CRF 的语义图像分割。IEEE 模式分析与机器智能学报。40, 834–848. doi:[10.1109/TPAMI.2017.2699184](https:/doi.org/10.1109/TPAMI.2017.2699184)。
- en: 'Chen et al. (2020a) Chen, Y., Cao, Y., Hu, H., Wang, L., 2020a. Memory enhanced
    global-local aggregation for video object detection, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 10334–10343. doi:[10.1109/CVPR42600.2020.01035](https:/doi.org/10.1109/CVPR42600.2020.01035).'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2020a） Chen, Y., Cao, Y., Hu, H., Wang, L., 2020a. 增强记忆的全局-局部聚合用于视频对象检测，见：IEEE/CVF
    计算机视觉与模式识别会议论文集，第 10334–10343 页。doi:[10.1109/CVPR42600.2020.01035](https:/doi.org/10.1109/CVPR42600.2020.01035)。
- en: 'Chen et al. (2020b) Chen, Z., Chen, K., Lin, W., See, J., Yu, H., Ke, Y., Yang,
    C., 2020b. Piou loss: Towards accurate oriented object detection in complex environments,
    in: Proceedings of the European Conference on Computer Vision, pp. 195–211. doi:[10.1007/978-3-030-58558-7_12](https:/doi.org/10.1007/978-3-030-58558-7_12).'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2020b） Chen, Z., Chen, K., Lin, W., See, J., Yu, H., Ke, Y., Yang, C.,
    2020b. Piou 损失：在复杂环境中实现准确的定向对象检测，见：欧洲计算机视觉会议论文集，第 195–211 页。doi:[10.1007/978-3-030-58558-7_12](https:/doi.org/10.1007/978-3-030-58558-7_12)。
- en: Cheng and Han (2016) Cheng, G., Han, J., 2016. A survey on object detection
    in optical remote sensing images. ISPRS Journal of Photogrammetry and Remote Sensing.
    117, 11–28. doi:[10.1016/j.isprsjprs.2016.03.014](https:/doi.org/10.1016/j.isprsjprs.2016.03.014).
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 和 Han（2016） Cheng, G., Han, J., 2016. 光学遥感图像中的对象检测综述。ISPRS 摄影测量与遥感学报。117,
    11–28. doi:[10.1016/j.isprsjprs.2016.03.014](https:/doi.org/10.1016/j.isprsjprs.2016.03.014)。
- en: Cheng et al. (2014) Cheng, G., Han, J., Zhou, P., Guo, L., 2014. Multi-class
    geospatial object detection and geographic image classification based on collection
    of part detectors. ISPRS Journal of Photogrammetry and Remote Sensing 98, 119–132.
    doi:[10.1016/j.isprsjprs.2014.10.002](https:/doi.org/10.1016/j.isprsjprs.2014.10.002).
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等（2014） Cheng, G., Han, J., Zhou, P., Guo, L., 2014. 基于部分检测器集合的多类别地理空间对象检测和地理图像分类。ISPRS
    摄影测量与遥感学报 98, 119–132. doi:[10.1016/j.isprsjprs.2014.10.002](https:/doi.org/10.1016/j.isprsjprs.2014.10.002)。
- en: Cheng et al. (2022a) Cheng, G., Wang, J., Li, K., Xie, X., Lang, C., Yao, Y.,
    Han, J., 2022a. Anchor-free oriented proposal generator for object detection.
    IEEE Transactions on Geoscience and Remote Sensing. 60, 1–11. doi:[10.1109/TGRS.2022.3183022](https:/doi.org/10.1109/TGRS.2022.3183022).
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等（2022a） Cheng, G., Wang, J., Li, K., Xie, X., Lang, C., Yao, Y., Han,
    J., 2022a. 无锚定向提议生成器用于对象检测。IEEE 地球科学与遥感学报。60, 1–11. doi:[10.1109/TGRS.2022.3183022](https:/doi.org/10.1109/TGRS.2022.3183022)。
- en: Cheng et al. (2022b) Cheng, G., Yao, Y., Li, S., Li, K., Xie, X., Wang, J.,
    Yao, X., Han, J., 2022b. Dual-aligned oriented detector. IEEE Transactions on
    Geoscience and Remote Sensing. 60, 1–11. doi:[10.1109/TGRS.2022.3149780](https:/doi.org/10.1109/TGRS.2022.3149780).
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等（2022b） Cheng, G., Yao, Y., Li, S., Li, K., Xie, X., Wang, J., Yao, X.,
    Han, J., 2022b. 双重对齐定向检测器。IEEE 地球科学与遥感学报。60, 1–11. doi:[10.1109/TGRS.2022.3149780](https:/doi.org/10.1109/TGRS.2022.3149780)。
- en: 'Chi et al. (2020) Chi, C., Wei, F., Hu, H., 2020. Relationnet++: Bridging visual
    representations for object detection via transformer decoder, in: Advances in
    Neural Information Processing Systems, pp. 13564–13574. URL: [https://proceedings.neurips.cc/paper/2020/file/9d684c589d67031a627ad33d59db65e5-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/9d684c589d67031a627ad33d59db65e5-Paper.pdf).'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chi 等人（2020）Chi, C., Wei, F., Hu, H., 2020. Relationnet++：通过变换器解码器桥接视觉表示以进行目标检测，载于《神经信息处理系统进展》，第13564–13574页。URL:
    [https://proceedings.neurips.cc/paper/2020/file/9d684c589d67031a627ad33d59db65e5-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/9d684c589d67031a627ad33d59db65e5-Paper.pdf)。'
- en: 'Chollet (2017) Chollet, F., 2017. Xception: Deep learning with depthwise separable
    convolutions, in: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 1800–1807. doi:[10.1109/CVPR.2017.195](https:/doi.org/10.1109/CVPR.2017.195).'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet（2017）Chollet, F., 2017. Xception：深度学习与深度可分卷积，载于《IEEE计算机视觉与模式识别会议论文集》，第1800–1807页。doi：[10.1109/CVPR.2017.195](https:/doi.org/10.1109/CVPR.2017.195)。
- en: 'Cohen and Welling (2016) Cohen, T.S., Welling, M., 2016. Group equivariant
    convolutional networks, in: Proceedings of the 33rd International Conference on
    Machine Learning, pp. 2990–2999. URL: [https://proceedings.mlr.press/v48/cohenc16.html](https://proceedings.mlr.press/v48/cohenc16.html).'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cohen 和 Welling（2016）Cohen, T.S., Welling, M., 2016. 群体等变卷积网络，载于《第33届国际机器学习会议论文集》，第2990–2999页。URL:
    [https://proceedings.mlr.press/v48/cohenc16.html](https://proceedings.mlr.press/v48/cohenc16.html)。'
- en: 'Cui et al. (2019) Cui, Y., Jia, M., Lin, T.Y., Song, Y., Belongie, S., 2019.
    Class-balanced loss based on effective number of samples, in: IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 9260–9269. doi:[10.1109/CVPR.2019.00949](https:/doi.org/10.1109/CVPR.2019.00949).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui 等人（2019）Cui, Y., Jia, M., Lin, T.Y., Song, Y., Belongie, S., 2019. 基于有效样本数量的类别平衡损失，载于《IEEE/CVF计算机视觉与模式识别会议论文集》，第9260–9269页。doi：[10.1109/CVPR.2019.00949](https:/doi.org/10.1109/CVPR.2019.00949)。
- en: 'Dai et al. (2016) Dai, J., Li, Y., He, K., Sun, J., 2016. R-fcn: Object detection
    via region-based fully convolutional networks, in: Proceedings of the 30th International
    Conference on Neural Information Processing Systems, pp. 379–387. doi:[10.5555/3157096.3157139](https:/doi.org/10.5555/3157096.3157139).'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人（2016）Dai, J., Li, Y., He, K., Sun, J., 2016. R-fcn：基于区域的全卷积网络进行目标检测，载于《第30届神经信息处理系统国际会议论文集》，第379–387页。doi：[10.5555/3157096.3157139](https:/doi.org/10.5555/3157096.3157139)。
- en: 'Dai et al. (2017) Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei,
    Y., 2017. Deformable convolutional networks, in: Proceedings of the IEEE International
    Conference on Computer Vision, pp. 764–773. doi:[10.1109/ICCV.2017.89](https:/doi.org/10.1109/ICCV.2017.89).'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人（2017）Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.,
    2017. 形变卷积网络，载于《IEEE国际计算机视觉会议论文集》，第764–773页。doi：[10.1109/ICCV.2017.89](https:/doi.org/10.1109/ICCV.2017.89)。
- en: 'Dai et al. (2022) Dai, L., Liu, H., Tang, H., Wu, Z., Song, P., 2022. Ao2-detr:
    Arbitrary-oriented object detection transformer. IEEE Transactions on Circuits
    and Systems for Video Technology. , 1–1doi:[10.1109/TCSVT.2022.3222906](https:/doi.org/10.1109/TCSVT.2022.3222906).'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人（2022）Dai, L., Liu, H., Tang, H., Wu, Z., Song, P., 2022. Ao2-detr：任意定向目标检测变换器。《IEEE电路与视频技术汇刊》，1–1。doi：[10.1109/TCSVT.2022.3222906](https:/doi.org/10.1109/TCSVT.2022.3222906)。
- en: 'Dalal and Triggs (2005) Dalal, N., Triggs, B., 2005. Histograms of oriented
    gradients for human detection, in: Proceedings of the IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition, pp. 886–893. doi:[10.1109/CVPR.2005.177](https:/doi.org/10.1109/CVPR.2005.177).'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalal 和 Triggs（2005）Dalal, N., Triggs, B., 2005. 面向人类检测的梯度方向直方图，载于《IEEE计算机视觉与模式识别会议论文集》，第886–893页。doi：[10.1109/CVPR.2005.177](https:/doi.org/10.1109/CVPR.2005.177)。
- en: 'Damen et al. (2018) Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari,
    A., Kazakos, E., Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.,
    2018. Scaling egocentric vision: The dataset, in: Proceedings of the European
    Conference on Computer Vision, pp. 753–771. doi:[10.1007/978-3-030-01225-0_44](https:/doi.org/10.1007/978-3-030-01225-0_44).'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Damen 等人（2018）Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari,
    A., Kazakos, E., Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.,
    2018. 缩放自我中心视角：数据集，载于《欧洲计算机视觉会议论文集》，第753–771页。doi：[10.1007/978-3-030-01225-0_44](https:/doi.org/10.1007/978-3-030-01225-0_44)。
- en: 'Ding et al. (2019) Ding, J., Xue, N., Long, Y., Xia, G.S., Lu, Q., 2019. Learning
    roi transformer for oriented object detection in aerial images, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2844–2853.
    doi:[10.1109/CVPR.2019.00296](https:/doi.org/10.1109/CVPR.2019.00296).'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等（2019）Ding, J., Xue, N., Long, Y., Xia, G.S., Lu, Q., 2019. 学习 ROI Transformer
    以进行空中图像中的定向目标检测，见：IEEE/CVF 计算机视觉与模式识别会议论文集，第 2844–2853 页。 doi:[10.1109/CVPR.2019.00296](https:/doi.org/10.1109/CVPR.2019.00296)。
- en: 'Ding et al. (2022) Ding, J., Xue, N., Xia, G.S., Bai, X., Yang, W., Yang, M.Y.,
    Belongie, S., Luo, J., Datcu, M., Pelillo, M., Zhang, L., 2022. Object detection
    in aerial images: A large-scale benchmark and challenges. IEEE Transactions on
    Pattern Analysis and Machine Intelligence 44, 7778–7796. doi:[10.1109/TPAMI.2021.3117983](https:/doi.org/10.1109/TPAMI.2021.3117983).'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等（2022）Ding, J., Xue, N., Xia, G.S., Bai, X., Yang, W., Yang, M.Y., Belongie,
    S., Luo, J., Datcu, M., Pelillo, M., Zhang, L., 2022. 空中图像中的目标检测：大规模基准和挑战。IEEE
    模式分析与机器智能汇刊 44，7778–7796。 doi:[10.1109/TPAMI.2021.3117983](https:/doi.org/10.1109/TPAMI.2021.3117983)。
- en: 'Dosovitskiy et al. (2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., 2021. An image is worth 16x16 words: Transformers for image recognition at
    scale, in: Proceedings of the International Conference on Learning Representations.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等（2021）Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., 2021. 一幅图像胜过 16x16 个词：大规模图像识别中的 Transformers，见：国际学习表征会议论文集。
- en: 'Du et al. (2022) Du, H., Shi, H., Zeng, D., Zhang, X.P., Mei, T., 2022. The
    elements of end-to-end deep face recognition: A survey of recent advances. ACM
    Computing Surveys. 54, 1–42. doi:[10.1145/3507902](https:/doi.org/10.1145/3507902).'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等（2022）Du, H., Shi, H., Zeng, D., Zhang, X.P., Mei, T., 2022. 端到端深度人脸识别的要素：近期进展的综述。ACM
    计算机调查。54，1–42。 doi:[10.1145/3507902](https:/doi.org/10.1145/3507902)。
- en: 'Duan et al. (2019) Duan, K., Bai, S., Xie, L., Qi, H., Huang, Q., Tian, Q.,
    2019. Centernet: Keypoint triplets for object detection, in: Proceedings of the
    IEEE/CVF International Conference on Computer Vision, pp. 6568–6577. doi:[10.1109/ICCV.2019.00667](https:/doi.org/10.1109/ICCV.2019.00667).'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan 等（2019）Duan, K., Bai, S., Xie, L., Qi, H., Huang, Q., Tian, Q., 2019. Centernet：用于目标检测的关键点三元组，见：IEEE/CVF
    国际计算机视觉会议论文集，第 6568–6577 页。 doi:[10.1109/ICCV.2019.00667](https:/doi.org/10.1109/ICCV.2019.00667)。
- en: Everingham et al. (2010) Everingham, M., Gool, L.V., Williams, C.K.I., Winn,
    J., Zisserman, A., 2010. The pascal visual object classes (voc) challenge. International
    Journal of Computer Vision. 88, 303–338. doi:[10.1007/s11263-009-0275-4](https:/doi.org/10.1007/s11263-009-0275-4).
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Everingham 等（2010）Everingham, M., Gool, L.V., Williams, C.K.I., Winn, J., Zisserman,
    A., 2010. Pascal 视觉目标类别（VOC）挑战。国际计算机视觉期刊。88，303–338。 doi:[10.1007/s11263-009-0275-4](https:/doi.org/10.1007/s11263-009-0275-4)。
- en: 'Fei-Fei and Perona (2005) Fei-Fei, L., Perona, P., 2005. A bayesian hierarchical
    model for learning natural scene categories, in: Proceedings of the IEEE Computer
    Society Conference on Computer Vision and Pattern Recognition, pp. 524–531. doi:[10.1109/CVPR.2005.16](https:/doi.org/10.1109/CVPR.2005.16).'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fei-Fei 和 Perona（2005）Fei-Fei, L., Perona, P., 2005. 用于学习自然场景类别的贝叶斯层次模型，见：IEEE
    计算机学会计算机视觉与模式识别会议论文集，第 524–531 页。 doi:[10.1109/CVPR.2005.16](https:/doi.org/10.1109/CVPR.2005.16)。
- en: 'Gao et al. (2021) Gao, P., Zheng, M., Wang, X., Dai, J., Li, H., 2021. Fast
    convergence of detr with spatially modulated co-attention, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision, pp. 3601–3610. doi:[10.1109/ICCV48922.2021.00360](https:/doi.org/10.1109/ICCV48922.2021.00360).'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2021）Gao, P., Zheng, M., Wang, X., Dai, J., Li, H., 2021. 基于空间调制的共同注意力机制的快速收敛，见：IEEE/CVF
    国际计算机视觉会议论文集，第 3601–3610 页。 doi:[10.1109/ICCV48922.2021.00360](https:/doi.org/10.1109/ICCV48922.2021.00360)。
- en: 'Ghiasi et al. (2019) Ghiasi, G., Lin, T.Y., Le, Q.V., 2019. Nas-fpn: Learning
    scalable feature pyramid architecture for object detection, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7029–7038.
    doi:[10.1109/CVPR.2019.00720](https:/doi.org/10.1109/CVPR.2019.00720).'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghiasi 等（2019）Ghiasi, G., Lin, T.Y., Le, Q.V., 2019. NAS-FPN：学习可扩展的特征金字塔架构用于目标检测，见：IEEE/CVF
    计算机视觉与模式识别会议论文集，第 7029–7038 页。 doi:[10.1109/CVPR.2019.00720](https:/doi.org/10.1109/CVPR.2019.00720)。
- en: 'Ghifary et al. (2015) Ghifary, M., Kleijn, W.B., Zhang, M., Balduzzi, D., 2015.
    Domain generalization for object recognition with multi-task autoencoders, in:
    Proceedings of the IEEE International Conference on Computer Vision, pp. 2551–2559.
    doi:[10.1109/ICCV.2015.293](https:/doi.org/10.1109/ICCV.2015.293).'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghifary 等 (2015) Ghifary, M., Kleijn, W.B., Zhang, M., Balduzzi, D., 2015. 基于多任务自编码器的物体识别领域泛化，见：IEEE
    国际计算机视觉会议论文集，第 2551–2559 页。doi:[10.1109/ICCV.2015.293](https:/doi.org/10.1109/ICCV.2015.293)。
- en: 'Ghifary et al. (2016) Ghifary, M., Kleijn, W.B., Zhang, M., Balduzzi, D., Li,
    W., 2016. Deep reconstruction-classification networks for unsupervised domain
    adaptation, in: Proceedings of the European Conference on Computer Vision, pp.
    597–613. doi:[10.1007/978-3-319-46493-0_36](https:/doi.org/10.1007/978-3-319-46493-0_36).'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghifary 等 (2016) Ghifary, M., Kleijn, W.B., Zhang, M., Balduzzi, D., Li, W.,
    2016. 用于无监督领域自适应的深度重建分类网络，见：欧洲计算机视觉会议论文集，第 597–613 页。doi:[10.1007/978-3-319-46493-0_36](https:/doi.org/10.1007/978-3-319-46493-0_36)。
- en: 'Girshick (2015) Girshick, R., 2015. Fast r-cnn, in: Proceedings of the IEEE
    International Conference on Computer Vision, pp. 1440–1448. doi:[10.1109/ICCV.2015.169](https:/doi.org/10.1109/ICCV.2015.169).'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girshick (2015) Girshick, R., 2015. Fast R-CNN，见：IEEE 国际计算机视觉会议论文集，第 1440–1448
    页。doi:[10.1109/ICCV.2015.169](https:/doi.org/10.1109/ICCV.2015.169)。
- en: 'Girshick et al. (2014) Girshick, R., Donahue, J., Darrell, T., Malik, J., 2014.
    Rich feature hierarchies for accurate object detection and semantic segmentation,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 580–587. doi:[10.1109/CVPR.2014.81](https:/doi.org/10.1109/CVPR.2014.81).'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girshick 等 (2014) Girshick, R., Donahue, J., Darrell, T., Malik, J., 2014. 用于准确物体检测和语义分割的丰富特征层次结构，见：IEEE
    计算机视觉与模式识别会议论文集，第 580–587 页。doi:[10.1109/CVPR.2014.81](https:/doi.org/10.1109/CVPR.2014.81)。
- en: 'Goodfellow et al. (2014) Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu,
    B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2014. Generative adversarial
    nets, in: Proceedings of the 27th International Conference on Neural Information
    Processing Systems - Volume 2, p. 2672–2680. doi:[10.5555/2969033.2969125](https:/doi.org/10.5555/2969033.2969125).'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 (2014) Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B.,
    Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2014. 生成对抗网络，见：第 27 届国际神经信息处理系统会议论文集
    - 第 2 卷，第 2672–2680 页。doi:[10.5555/2969033.2969125](https:/doi.org/10.5555/2969033.2969125)。
- en: 'Guan et al. (2021) Guan, Q., Qu, Z., Zeng, M., Shen, J., Du, J., 2021. Cgp
    box: An effective direction representation strategy for oriented object detection
    in remote sensing images. International Journal of Remote Sensing. 42, 6666–6687.
    doi:[10.1080/01431161.2021.1941389](https:/doi.org/10.1080/01431161.2021.1941389).'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guan 等 (2021) Guan, Q., Qu, Z., Zeng, M., Shen, J., Du, J., 2021. CGP Box：一种有效的方向表示策略，用于遥感图像中的定向物体检测。国际遥感期刊，第
    42 卷，第 6666–6687 页。doi:[10.1080/01431161.2021.1941389](https:/doi.org/10.1080/01431161.2021.1941389)。
- en: 'Guo et al. (2021) Guo, Z., Liu, C., Zhang, X., Jiao, J., Ji, X., Ye, Q., 2021.
    Beyond bounding-box: Convex-hull feature adaptation for oriented and densely packed
    object detection, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), pp. 8788–8797. doi:[10.1109/CVPR46437.2021.00868](https:/doi.org/10.1109/CVPR46437.2021.00868).'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2021) Guo, Z., Liu, C., Zhang, X., Jiao, J., Ji, X., Ye, Q., 2021. 超越边界框：面向和密集排列物体检测的凸包特征适配，见：IEEE/CVF
    计算机视觉与模式识别会议 (CVPR) 论文集，第 8788–8797 页。doi:[10.1109/CVPR46437.2021.00868](https:/doi.org/10.1109/CVPR46437.2021.00868)。
- en: 'Gupta et al. (2019) Gupta, A., Dollár, P., Girshick, R., 2019. Lvis: A dataset
    for large vocabulary instance segmentation, in: IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 5351–5359. doi:[10.1109/CVPR.2019.00550](https:/doi.org/10.1109/CVPR.2019.00550).'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等 (2019) Gupta, A., Dollár, P., Girshick, R., 2019. Lvis：一个用于大词汇量实例分割的数据集，见：IEEE/CVF
    计算机视觉与模式识别会议论文集，第 5351–5359 页。doi:[10.1109/CVPR.2019.00550](https:/doi.org/10.1109/CVPR.2019.00550)。
- en: 'Haase and Amthor (2020) Haase, D., Amthor, M., 2020. Rethinking depthwise separable
    convolutions: How intra-kernel correlations lead to improved mobilenets, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14588–14597.
    doi:[10.1109/CVPR42600.2020.01461](https:/doi.org/10.1109/CVPR42600.2020.01461).'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haase 和 Amthor (2020) Haase, D., Amthor, M., 2020. 重新思考深度可分离卷积：如何通过内核相关性提升 Mobilenet，见：IEEE/CVF
    计算机视觉与模式识别会议论文集，第 14588–14597 页。doi:[10.1109/CVPR42600.2020.01461](https:/doi.org/10.1109/CVPR42600.2020.01461)。
- en: Han et al. (2022a) Han, J., Ding, J., Li, J., Xia, G.S., 2022a. Align deep features
    for oriented object detection. IEEE Transactions on Geoscience and Remote Sensing.
    60, 1–11. doi:[10.1109/TGRS.2021.3062048](https:/doi.org/10.1109/TGRS.2021.3062048).
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2022a) Han, J., Ding, J., Li, J., Xia, G.S., 2022a. 对准深度特征以进行面向目标检测。《IEEE地球科学与遥感学报》。60,
    1–11. doi:[10.1109/TGRS.2021.3062048](https:/doi.org/10.1109/TGRS.2021.3062048)。
- en: Han et al. (2022b) Han, J., Ding, J., Li, J., Xia, G.S., 2022b. Align deep features
    for oriented object detection. IEEE Transactions on Geoscience and Remote Sensing.
    60, 1–11. doi:[10.1109/TGRS.2021.3062048](https:/doi.org/10.1109/TGRS.2021.3062048).
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2022b) Han, J., Ding, J., Li, J., Xia, G.S., 2022b. 对准深度特征以进行面向目标检测。《IEEE地球科学与遥感学报》。60,
    1–11. doi:[10.1109/TGRS.2021.3062048](https:/doi.org/10.1109/TGRS.2021.3062048)。
- en: 'Han et al. (2021a) Han, J., Ding, J., Xue, N., Xia, G.S., 2021a. Redet: A rotation-equivariant
    detector for aerial object detection, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 2785–2794. doi:[10.1109/CVPR46437.2021.00281](https:/doi.org/10.1109/CVPR46437.2021.00281).'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han 等 (2021a) Han, J., Ding, J., Xue, N., Xia, G.S., 2021a. Redet: 一种用于空中目标检测的旋转等变检测器，见：《IEEE/CVF计算机视觉与模式识别会议论文集》，第2785–2794页。doi:[10.1109/CVPR46437.2021.00281](https:/doi.org/10.1109/CVPR46437.2021.00281)。'
- en: Han et al. (2023) Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang,
    Y., Xiao, A., Xu, C., Xu, Y., Yang, Z., Zhang, Y., Tao, D., 2023. A survey on
    vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence
    45, 87–110. doi:[10.1109/TPAMI.2022.3152247](https:/doi.org/10.1109/TPAMI.2022.3152247).
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2023) Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang,
    Y., Xiao, A., Xu, C., Xu, Y., Yang, Z., Zhang, Y., Tao, D., 2023. 关于视觉变换器的综述。《IEEE模式分析与机器智能》45,
    87–110. doi:[10.1109/TPAMI.2022.3152247](https:/doi.org/10.1109/TPAMI.2022.3152247)。
- en: 'Han et al. (2021b) Han, K., Xiao, A., Wu, E., Guo, J., XU, C., Wang, Y., 2021b.
    Transformer in transformer, in: Proceedings of the Advances in Neural Information
    Processing Systems, pp. 15908–15919. URL: [https://proceedings.neurips.cc/paper/2021/file/854d9fca60b4bd07f9bb215d59ef5561-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/854d9fca60b4bd07f9bb215d59ef5561-Paper.pdf).'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han 等 (2021b) Han, K., Xiao, A., Wu, E., Guo, J., XU, C., Wang, Y., 2021b.
    变换器中的变换器，见：《神经信息处理系统大会论文集》，第15908–15919页。网址: [https://proceedings.neurips.cc/paper/2021/file/854d9fca60b4bd07f9bb215d59ef5561-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/854d9fca60b4bd07f9bb215d59ef5561-Paper.pdf)。'
- en: 'Han et al. (2015) Han, S., Pool, J., Tran, J., Dally, W.J., 2015. Learning
    both weights and connections for efficient neural networks, in: Proceedings of
    the 28th International Conference on Neural Information Processing Systems - Volume
    1, p. 1135–1143. doi:[10.5555/2969239.2969366](https:/doi.org/10.5555/2969239.2969366).'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2015) Han, S., Pool, J., Tran, J., Dally, W.J., 2015. 为高效神经网络学习权重和连接，见：《第28届国际神经信息处理系统大会论文集
    - 第1卷》，第1135–1143页。doi:[10.5555/2969239.2969366](https:/doi.org/10.5555/2969239.2969366)。
- en: 'Han et al. (2021c) Han, W., Chen, J., Wang, L., Feng, R., Li, F., Wu, L., Tian,
    T., Yan, J., 2021c. Methods for small, weak object detection in optical high-resolution
    remote sensing images: A survey of advances and challenges. IEEE Geoscience and
    Remote Sensing Magazine. 9, 8–34. doi:[10.1109/MGRS.2020.3041450](https:/doi.org/10.1109/MGRS.2020.3041450).'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2021c) Han, W., Chen, J., Wang, L., Feng, R., Li, F., Wu, L., Tian, T.,
    Yan, J., 2021c. 光学高分辨率遥感图像中小型、弱目标检测的方法：进展与挑战的综述。《IEEE地球科学与遥感学杂志》。9, 8–34. doi:[10.1109/MGRS.2020.3041450](https:/doi.org/10.1109/MGRS.2020.3041450)。
- en: 'Hanson and Pratt (1988) Hanson, S.J., Pratt, L.Y., 1988. Comparing biases for
    minimal network construction with back-propagation, in: Proceedings of the 1st
    International Conference on Neural Information Processing Systems, p. 177–185.
    doi:[10.5555/2969735.2969756](https:/doi.org/10.5555/2969735.2969756).'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanson 和 Pratt (1988) Hanson, S.J., Pratt, L.Y., 1988. 比较最小网络构建的偏差与反向传播，见：《第1届国际神经信息处理系统大会论文集》，第177–185页。doi:[10.5555/2969735.2969756](https:/doi.org/10.5555/2969735.2969756)。
- en: 'He et al. (2020a) He, F., Gao, N., Li, Q., Du, S., Zhao, X., Huang, K., 2020a.
    Temporal context enhanced feature aggregation for video object detection, in:
    Proceedings of the AAAI Conference on Artificial Intelligence, pp. 10941–10948.
    doi:[10.1609/aaai.v34i07.6727](https:/doi.org/10.1609/aaai.v34i07.6727).'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 (2020a) He, F., Gao, N., Li, Q., Du, S., Zhao, X., Huang, K., 2020a. 强化时间上下文的特征聚合用于视频目标检测，见：《AAAI人工智能大会论文集》，第10941–10948页。doi:[10.1609/aaai.v34i07.6727](https:/doi.org/10.1609/aaai.v34i07.6727)。
- en: 'He et al. (2022) He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.,
    2022. Masked autoencoders are scalable vision learners, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15979–15988.
    doi:[10.1109/CVPR52688.2022.01553](https:/doi.org/10.1109/CVPR52688.2022.01553).'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等（2022）He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R., 2022.
    **掩码自编码器是可扩展的视觉学习者**，载于《IEEE/CVF计算机视觉与模式识别会议论文集》，第15979–15988页。doi:[10.1109/CVPR52688.2022.01553](https:/doi.org/10.1109/CVPR52688.2022.01553)。
- en: He et al. (2020b) He, K., Gkioxari, G., Dollár, P., Girshick, R., 2020b. Mask
    r-cnn. IEEE Transactions on Pattern Analysis and Machine Intelligence. 42, 386–397.
    doi:[10.1109/TPAMI.2018.2844175](https:/doi.org/10.1109/TPAMI.2018.2844175).
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等（2020b）He, K., Gkioxari, G., Dollár, P., Girshick, R., 2020b. **掩码R-CNN**。《IEEE模式分析与机器智能汇刊》，42,
    386–397。doi:[10.1109/TPAMI.2018.2844175](https:/doi.org/10.1109/TPAMI.2018.2844175)。
- en: 'He et al. (2016) He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning
    for image recognition, in: Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 770–778. doi:[10.1109/CVPR.2016.90](https:/doi.org/10.1109/CVPR.2016.90).'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等（2016）He, K., Zhang, X., Ren, S., Sun, J., 2016. **用于图像识别的深度残差学习**，载于《IEEE计算机视觉与模式识别会议论文集》，第770–778页。doi:[10.1109/CVPR.2016.90](https:/doi.org/10.1109/CVPR.2016.90)。
- en: He et al. (2021) He, X., Ma, S., He, L., Ru, L., Wang, C., 2021. Learning rotated
    inscribed ellipse for oriented object detection in remote sensing images. Remote
    Sensing. 13. doi:[10.3390/rs13183622](https:/doi.org/10.3390/rs13183622).
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等（2021）He, X., Ma, S., He, L., Ru, L., Wang, C., 2021. **学习旋转内接椭圆以进行遥感图像中的定向物体检测**。《遥感》，13。doi:[10.3390/rs13183622](https:/doi.org/10.3390/rs13183622)。
- en: 'Hei and Jia (2020) Hei, L., Jia, D., 2020. Cornernet: Detecting objects as
    paired keypoints. International Journal of Computer Vision. 128, 642–656. doi:[10.1007/s11263-019-01204-1](https:/doi.org/10.1007/s11263-019-01204-1).'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hei和Jia（2020）Hei, L., Jia, D., 2020. **Cornernet：将物体检测为配对关键点**。《计算机视觉国际期刊》，128,
    642–656。doi:[10.1007/s11263-019-01204-1](https:/doi.org/10.1007/s11263-019-01204-1)。
- en: Hinton and Salakhutdinov (2006) Hinton, G., Salakhutdinov, R., 2006. Reducing
    the dimensionality of data with neural networks. Science. 313, 504–507. doi:[10.1126/science.1127647](https:/doi.org/10.1126/science.1127647).
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton和Salakhutdinov（2006）Hinton, G., Salakhutdinov, R., 2006. **用神经网络减少数据的维度**。《科学》，313,
    504–507。doi:[10.1126/science.1127647](https:/doi.org/10.1126/science.1127647)。
- en: Hinton et al. (2015) Hinton, G., Vinyals, O., Dean, J., 2015. Distilling the
    Knowledge in a Neural Network. arXiv e-prints. , arXiv:1503.02531.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton等（2015）Hinton, G., Vinyals, O., Dean, J., 2015. **在神经网络中提炼知识**。arXiv电子打印，arXiv:1503.02531。
- en: Hochreiter and Schmidhuber (1997) Hochreiter, S., Schmidhuber, J., 1997. Long
    Short-Term Memory. Neural Computation. 9, 1735–1780. doi:[10.1162/neco.1997.9.8.1735](https:/doi.org/10.1162/neco.1997.9.8.1735).
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter和Schmidhuber（1997）Hochreiter, S., Schmidhuber, J., 1997. **长短期记忆**。《神经计算》，9,
    1735–1780。doi:[10.1162/neco.1997.9.8.1735](https:/doi.org/10.1162/neco.1997.9.8.1735)。
- en: Hosang et al. (2016) Hosang, J., Benenson, R., Dollár, P., Schiele, B., 2016.
    What makes for effective detection proposals? IEEE Transactions on Pattern Analysis
    and Machine Intelligence. 38, 814–830. doi:[10.1109/TPAMI.2015.2465908](https:/doi.org/10.1109/TPAMI.2015.2465908).
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosang等（2016）Hosang, J., Benenson, R., Dollár, P., Schiele, B., 2016. **什么因素使检测建议有效？**。《IEEE模式分析与机器智能汇刊》，38,
    814–830。doi:[10.1109/TPAMI.2015.2465908](https:/doi.org/10.1109/TPAMI.2015.2465908)。
- en: 'Hu et al. (2018) Hu, H., Gu, J., Zhang, Z., Dai, J., Wei, Y., 2018. Relation
    networks for object detection, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 3588–3597. doi:[10.1109/CVPR.2018.00378](https:/doi.org/10.1109/CVPR.2018.00378).'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu等（2018）Hu, H., Gu, J., Zhang, Z., Dai, J., Wei, Y., 2018. **用于物体检测的关系网络**，载于《IEEE/CVF计算机视觉与模式识别会议论文集》，第3588–3597页。doi:[10.1109/CVPR.2018.00378](https:/doi.org/10.1109/CVPR.2018.00378)。
- en: Hu et al. (2020) Hu, J., Shen, L., Albanie, S., Sun, G., Wu, E., 2020. Squeeze-and-excitation
    networks. IEEE Transactions on Pattern Analysis and Machine Intelligence. 42,
    2011–2023. doi:[10.1109/TPAMI.2019.2913372](https:/doi.org/10.1109/TPAMI.2019.2913372).
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu等（2020）Hu, J., Shen, L., Albanie, S., Sun, G., Wu, E., 2020. **挤压与激励网络**。《IEEE模式分析与机器智能汇刊》，42,
    2011–2023。doi:[10.1109/TPAMI.2019.2913372](https:/doi.org/10.1109/TPAMI.2019.2913372)。
- en: 'Huang et al. (2017) Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.,
    2017. Densely connected convolutional networks, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 2261–2269. doi:[10.1109/CVPR.2017.243](https:/doi.org/10.1109/CVPR.2017.243).'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等（2017）Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., 2017.
    **密集连接卷积网络**，载于《IEEE计算机视觉与模式识别会议论文集》，第2261–2269页。doi:[10.1109/CVPR.2017.243](https:/doi.org/10.1109/CVPR.2017.243)。
- en: 'Ioffe and Szegedy (2015) Ioffe, S., Szegedy, C., 2015. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift, in: Proceedings
    of the 32nd International Conference on Machine Learning, pp. 448–456. URL: [https://proceedings.mlr.press/v37/ioffe15.html](https://proceedings.mlr.press/v37/ioffe15.html).'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ioffe和Szegedy（2015）Ioffe, S., Szegedy, C., 2015. 批量归一化：通过减少内部协变量偏移加速深度网络训练，载于：第32届国际机器学习大会论文集，页码
    448–456. URL: [https://proceedings.mlr.press/v37/ioffe15.html](https://proceedings.mlr.press/v37/ioffe15.html)。'
- en: 'Jiang et al. (2022) Jiang, Y., Tan, Z., Wang, J., Sun, X., Lin, M., Li, H.,
    2022. GiraffeDet: A Heavy-Neck Paradigm for Object Detection. arXiv e-prints.
    , arXiv:2202.04256.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang等（2022）Jiang, Y., Tan, Z., Wang, J., Sun, X., Lin, M., Li, H., 2022. GiraffeDet:
    一种用于物体检测的重颈架构。arXiv e-prints. , arXiv:2202.04256。'
- en: Jiao et al. (2019) Jiao, L., Zhang, F., Liu, F., Yang, S., Li, L., Feng, Z.,
    Qu, R., 2019. A survey of deep learning-based object detection. IEEE Access. 7,
    128837–128868. doi:[10.1109/ACCESS.2019.2939201](https:/doi.org/10.1109/ACCESS.2019.2939201).
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiao等（2019）Jiao, L., Zhang, F., Liu, F., Yang, S., Li, L., Feng, Z., Qu, R.,
    2019. 基于深度学习的物体检测综述。IEEE Access. 7, 128837–128868. doi:[10.1109/ACCESS.2019.2939201](https:/doi.org/10.1109/ACCESS.2019.2939201)。
- en: Kirillov et al. (2023) Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland,
    C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P.,
    Girshick, R., 2023. Segment Anything. arXiv e-prints , arXiv:2304.02643.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirillov等（2023）Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson,
    L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R., 2023.
    任何分割。arXiv e-prints , arXiv:2304.02643。
- en: Kisantal et al. (2019) Kisantal, M., Wojna, Z., Murawski, J., Naruniec, J.,
    Cho, K., 2019. Augmentation for small object detection. arXiv e-prints. , arXiv:1902.07296.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kisantal等（2019）Kisantal, M., Wojna, Z., Murawski, J., Naruniec, J., Cho, K.,
    2019. 小物体检测的增强方法。arXiv e-prints. , arXiv:1902.07296。
- en: 'Krizhevsky et al. (2012) Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012.
    Imagenet classification with deep convolutional neural networks, in: Proceedings
    of the International Conference on Neural Information Processing Systems, Curran
    Associates Inc., Red Hook, NY, USA. pp. 1097–1105.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky等（2012）Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. 使用深度卷积神经网络进行Imagenet分类，载于：国际神经信息处理系统会议论文集，Curran
    Associates Inc., Red Hook, NY, USA. 页码 1097–1105。
- en: Krizhevsky et al. (2017) Krizhevsky, A., Sutskever, I., Hinton, G.E., 2017.
    Imagenet classification with deep convolutional neural networks. Communications
    of the ACM. 60, 84–90. doi:[10.1145/3065386](https:/doi.org/10.1145/3065386).
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky等（2017）Krizhevsky, A., Sutskever, I., Hinton, G.E., 2017. 使用深度卷积神经网络进行Imagenet分类。ACM通讯.
    60, 84–90. doi:[10.1145/3065386](https:/doi.org/10.1145/3065386)。
- en: 'Kuznetsova et al. (2020) Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J.,
    Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A.,
    Duerig, T., Ferrari, V., 2020. The open images dataset v4: Unified image classification,
    object detection, and visual relationship detection at scale. International Journal
    of Computer Vision. , 1956–1981doi:[10.1007/S11263-020-01316-Z](https:/doi.org/10.1007/S11263-020-01316-Z).'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuznetsova等（2020）Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin,
    I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., Duerig,
    T., Ferrari, V., 2020. 开放图像数据集v4：统一的图像分类、物体检测和视觉关系检测。计算机视觉国际杂志。 , 1956–1981 doi:[10.1007/S11263-020-01316-Z](https:/doi.org/10.1007/S11263-020-01316-Z)。
- en: 'Lam et al. (2018) Lam, D., Kuzma, R., McGee, K., Dooley, S., Laielli, M., Klaric,
    M., Bulatov, Y., McCord, B., 2018. xView: Objects in Context in Overhead Imagery.
    arXiv e-prints. , arXiv:1802.07856.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lam等（2018）Lam, D., Kuzma, R., McGee, K., Dooley, S., Laielli, M., Klaric, M.,
    Bulatov, Y., McCord, B., 2018. xView：高空影像中的上下文物体。arXiv e-prints. , arXiv:1802.07856。
- en: 'Larochelle et al. (2007) Larochelle, H., Erhan, D., Courville, A., Bergstra,
    J., Bengio, Y., 2007. An empirical evaluation of deep architectures on problems
    with many factors of variation, in: Proceedings of the 24th International Conference
    on Machine Learning, p. 473–480.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Larochelle等（2007）Larochelle, H., Erhan, D., Courville, A., Bergstra, J., Bengio,
    Y., 2007. 在具有许多变化因素的问题上的深度架构的实证评估，载于：第24届国际机器学习大会论文集，页码 473–480。
- en: LeCun et al. (2015) LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning.
    Nature. 521, 436–444. doi:[10.1038/nature14539](https:/doi.org/10.1038/nature14539).
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun等（2015）LeCun, Y., Bengio, Y., Hinton, G., 2015. 深度学习。自然. 521, 436–444.
    doi:[10.1038/nature14539](https:/doi.org/10.1038/nature14539)。
- en: Leitloff et al. (2010) Leitloff, J., Hinz, S., Stilla, U., 2010. Vehicle detection
    in very high resolution satellite images of city areas. IEEE Transactions on Geoscience
    and Remote Sensing. 48, 2795–2806. doi:[10.1109/TGRS.2010.2043109](https:/doi.org/10.1109/TGRS.2010.2043109).
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leitloff 等（2010）Leitloff, J., Hinz, S., Stilla, U., 2010. 在城市区域的超高分辨率卫星图像中进行车辆检测。IEEE
    地球科学与遥感汇刊。48, 2795–2806。doi：[10.1109/TGRS.2010.2043109](https:/doi.org/10.1109/TGRS.2010.2043109)。
- en: 'Lenc and Vedaldi (2015) Lenc, K., Vedaldi, A., 2015. Understanding image representations
    by measuring their equivariance and equivalence, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 991–999. doi:[10.1109/CVPR.2015.7298701](https:/doi.org/10.1109/CVPR.2015.7298701).'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lenc 和 Vedaldi（2015）Lenc, K., Vedaldi, A., 2015. 通过测量图像表示的等变性和等价性来理解图像表示，发表于：IEEE
    计算机视觉与模式识别会议，页码 991–999。doi：[10.1109/CVPR.2015.7298701](https:/doi.org/10.1109/CVPR.2015.7298701)。
- en: 'Li et al. (2021) Li, B., Xie, X., Wei, X., Tang, W., 2021. Ship detection and
    classification from optical remote sensing images: A survey. Chinese Journal of
    Aeronautics. 34, 145–163. doi:[10.1016/j.cja.2020.09.022](https:/doi.org/10.1016/j.cja.2020.09.022).'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2021）Li, B., Xie, X., Wei, X., Tang, W., 2021. 从光学遥感图像中进行船舶检测与分类：综述。中国航空学报。34,
    145–163。doi：[10.1016/j.cja.2020.09.022](https:/doi.org/10.1016/j.cja.2020.09.022)。
- en: 'Li et al. (2022a) Li, B., Yao, Y., Tan, J., Zhang, G., Yu, F., Lu, J., Luo,
    Y., 2022a. Equalized focal loss for dense long-tailed object detection, in: IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 6980–6989. doi:[10.1109/CVPR52688.2022.00686](https:/doi.org/10.1109/CVPR52688.2022.00686).'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2022a）Li, B., Yao, Y., Tan, J., Zhang, G., Yu, F., Lu, J., Luo, Y., 2022a.
    针对密集长尾物体检测的均衡焦点损失，发表于：IEEE/CVF 计算机视觉与模式识别会议，页码 6980–6989。doi：[10.1109/CVPR52688.2022.00686](https:/doi.org/10.1109/CVPR52688.2022.00686)。
- en: 'Li et al. (2020) Li, K., Wan, G., Cheng, G., Meng, L., Han, J., 2020. Object
    detection in optical remote sensing images: A survey and a new benchmark. ISPRS
    Journal of Photogrammetry and Remote Sensing. 159, 296–307. doi:[10.1016/j.isprsjprs.2019.11.023](https:/doi.org/10.1016/j.isprsjprs.2019.11.023).'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2020）Li, K., Wan, G., Cheng, G., Meng, L., Han, J., 2020. 光学遥感图像中的物体检测：综述与新基准。ISPRS
    摄影测量与遥感杂志。159, 296–307。doi：[10.1016/j.isprsjprs.2019.11.023](https:/doi.org/10.1016/j.isprsjprs.2019.11.023)。
- en: 'Li et al. (2019) Li, L., Xu, M., Wang, X., Jiang, L., Liu, H., 2019. Attention
    based glaucoma detection: A large-scale database and cnn model, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10563–10572.
    doi:[10.1109/CVPR.2019.01082](https:/doi.org/10.1109/CVPR.2019.01082).'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2019）Li, L., Xu, M., Wang, X., Jiang, L., Liu, H., 2019. 基于注意力的青光眼检测：大规模数据库和
    CNN 模型，发表于：IEEE/CVF 计算机视觉与模式识别会议，页码 10563–10572。doi：[10.1109/CVPR.2019.01082](https:/doi.org/10.1109/CVPR.2019.01082)。
- en: 'Li et al. (2022b) Li, W., Chen, Y., Hu, K., Zhu, J., 2022b. Oriented reppoints
    for aerial object detection, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 1819–1828. doi:[10.1109/CVPR52688.2022.00187](https:/doi.org/10.1109/CVPR52688.2022.00187).'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2022b）Li, W., Chen, Y., Hu, K., Zhu, J., 2022b. 面向定向的 Reppoints 用于空中物体检测，发表于：IEEE/CVF
    计算机视觉与模式识别会议，页码 1819–1828。doi：[10.1109/CVPR52688.2022.00187](https:/doi.org/10.1109/CVPR52688.2022.00187)。
- en: Liang et al. (2022) Liang, D., Geng, Q., Wei, Z., Vorontsov, D.A., Kim, E.L.,
    Wei, M., Zhou, H., 2022. Anchor retouching via model interaction for robust object
    detection in aerial images. IEEE Transactions on Geoscience and Remote Sensing.
    60, 1–13. doi:[10.1109/TGRS.2021.3136350](https:/doi.org/10.1109/TGRS.2021.3136350).
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等（2022）Liang, D., Geng, Q., Wei, Z., Vorontsov, D.A., Kim, E.L., Wei,
    M., Zhou, H., 2022. 通过模型交互进行锚点修整以实现空中图像中的稳健物体检测。IEEE 地球科学与遥感汇刊。60, 1–13。doi：[10.1109/TGRS.2021.3136350](https:/doi.org/10.1109/TGRS.2021.3136350)。
- en: 'Liao et al. (2018) Liao, M., Zhu, Z., Shi, B., Xia, G.s., Bai, X., 2018. Rotation-sensitive
    regression for oriented scene text detection, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 5909–5918. doi:[10.1109/CVPR.2018.00619](https:/doi.org/10.1109/CVPR.2018.00619).'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao 等（2018）Liao, M., Zhu, Z., Shi, B., Xia, G.s., Bai, X., 2018. 面向方向的回归用于定向场景文本检测，发表于：IEEE/CVF
    计算机视觉与模式识别会议，页码 5909–5918。doi：[10.1109/CVPR.2018.00619](https:/doi.org/10.1109/CVPR.2018.00619)。
- en: 'Lin et al. (2017) Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B.,
    Belongie, S., 2017. Feature pyramid networks for object detection, in: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
    936–944. doi:[10.1109/CVPR.2017.106](https:/doi.org/10.1109/CVPR.2017.106).'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2017) Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B.,
    Belongie, S., 2017. 特征金字塔网络用于物体检测，见：IEEE计算机视觉与模式识别会议（CVPR）论文集，第936–944页。doi:[10.1109/CVPR.2017.106](https:/doi.org/10.1109/CVPR.2017.106)。
- en: Lin et al. (2020) Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P., 2020.
    Focal loss for dense object detection. IEEE Transactions on Pattern Analysis and
    Machine Intelligence. 42, 318–327. doi:[10.1109/TPAMI.2018.2858826](https:/doi.org/10.1109/TPAMI.2018.2858826).
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2020) Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollář, P., 2020.
    用于密集物体检测的焦点损失。IEEE模式分析与机器智能汇刊，42，318–327。doi:[10.1109/TPAMI.2018.2858826](https:/doi.org/10.1109/TPAMI.2018.2858826)。
- en: 'Lin et al. (2014) Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
    Ramanan, D., Dollár, P., Zitnick, C.L., 2014. Microsoft coco: Common objects in
    context, in: Proceedings of the European Conference on Computer Vision, pp. 740–755.
    doi:[10.1007/978-3-319-10602-1_48](https:/doi.org/10.1007/978-3-319-10602-1_48).'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2014) Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
    Ramanan, D., Dollář, P., Zitnick, C.L., 2014. Microsoft coco: 语境中的常见物体，见：欧洲计算机视觉会议论文集，第740–755页。doi:[10.1007/978-3-319-10602-1_48](https:/doi.org/10.1007/978-3-319-10602-1_48)。'
- en: 'Lin et al. (2019) Lin, Y., Feng, P., Guan, J., Wang, W., Chambers, J., 2019.
    IENet: Interacting Embranchment One Stage Anchor Free Detector for Orientation
    Aerial Object Detection. arXiv e-prints. , arXiv:1912.00969.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2019) Lin, Y., Feng, P., Guan, J., Wang, W., Chambers, J., 2019.
    IENet：用于定向航空物体检测的交互式分支单阶段无锚检测器。arXiv预印本，arXiv:1912.00969。
- en: 'Liu et al. (2003) Liu, C.L., Nakashima, K., Sako, H., Fujisawa, H., 2003. Handwritten
    digit recognition: benchmarking of state-of-the-art techniques. Pattern Recognition.
    36, 2271–2285. doi:[https://doi.org/10.1016/S0031-3203(03)00085-2](https:/doi.org/https://doi.org/10.1016/S0031-3203(03)00085-2).'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2003) Liu, C.L., Nakashima, K., Sako, H., Fujisawa, H., 2003. 手写数字识别：最新技术的基准测试。模式识别，36，2271–2285。doi:[https://doi.org/10.1016/S0031-3203(03)00085-2](https:/doi.org/https://doi.org/10.1016/S0031-3203(03)00085-2)。
- en: Liu and Mattyus (2015) Liu, K., Mattyus, G., 2015. Fast multiclass vehicle detection
    on aerial images. IEEE Geoscience and Remote Sensing Letters. 12, 1938–1942. doi:[10.1109/LGRS.2015.2439517](https:/doi.org/10.1109/LGRS.2015.2439517).
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu and Mattyus (2015) Liu, K., Mattyus, G., 2015. 航拍图像中的快速多类车辆检测。IEEE地球科学与遥感快报，12，1938–1942。doi:[10.1109/LGRS.2015.2439517](https:/doi.org/10.1109/LGRS.2015.2439517)。
- en: 'Liu et al. (2020) Liu, L., Ouyang, W., Wang, X., Fieguth, P., Chen, J., Liu,
    X., Pietikäinen, M., 2020. Deep learning for generic object detection: A survey.
    International Journal of Computer Vision. 128, 261–318. doi:[10.1007/s11263-019-01247-4](https:/doi.org/10.1007/s11263-019-01247-4).'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020) Liu, L., Ouyang, W., Wang, X., Fieguth, P., Chen, J., Liu,
    X., Pietikäinen, M., 2020. 通用物体检测的深度学习：综述。国际计算机视觉杂志，128，261–318。doi:[10.1007/s11263-019-01247-4](https:/doi.org/10.1007/s11263-019-01247-4)。
- en: Liu et al. (2019) Liu, S., Huang, D., Wang, Y., 2019. Learning Spatial Fusion
    for Single-Shot Object Detection. arXiv e-prints. .
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019) Liu, S., Huang, D., Wang, Y., 2019. 学习空间融合以进行单次检测。arXiv预印本。
- en: 'Liu et al. (2018) Liu, S., Qi, L., Qin, H., Shi, J., Jia, J., 2018. Path aggregation
    network for instance segmentation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 8759–8768. doi:[10.1109/CVPR.2018.00913](https:/doi.org/10.1109/CVPR.2018.00913).'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018) Liu, S., Qi, L., Qin, H., Shi, J., Jia, J., 2018. 实例分割的路径聚合网络，见：IEEE/CVF计算机视觉与模式识别会议论文集，第8759–8768页。doi:[10.1109/CVPR.2018.00913](https:/doi.org/10.1109/CVPR.2018.00913)。
- en: 'Liu et al. (2016a) Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.,
    Fu, C.Y., Berg, A.C., 2016a. Ssd: Single shot multibox detector, in: Proceedings
    of the European Conference on Computer Vision, pp. 21–37. doi:[10.1007/978-3-319-46448-0_2](https:/doi.org/10.1007/978-3-319-46448-0_2).'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2016a) Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.,
    Fu, C.Y., Berg, A.C., 2016a. SSD：单次多框检测器，见：欧洲计算机视觉会议论文集，第21–37页。doi:[10.1007/978-3-319-46448-0_2](https:/doi.org/10.1007/978-3-319-46448-0_2)。
- en: Liu et al. (2022) Liu, W., Zhang, T., Huang, S., Li, K., 2022. A hybrid optimization
    framework for uav reconnaissance mission planning. Computers & Industrial Engineering.
    173, 108653. doi:[10.1016/j.cie.2022.108653](https:/doi.org/10.1016/j.cie.2022.108653).
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022) Liu, W., Zhang, T., Huang, S., Li, K., 2022. 一种用于无人机侦察任务规划的混合优化框架。计算机与工业工程，173，108653。doi:[10.1016/j.cie.2022.108653](https:/doi.org/10.1016/j.cie.2022.108653)。
- en: Liu et al. (2021a) Liu, Y., Sun, P., Wergeles, N., Shang, Y., 2021a. A survey
    and performance evaluation of deep learning methods for small object detection.
    Expert Systems with Applications. 172, 114602. doi:[10.1016/j.eswa.2021.114602](https:/doi.org/10.1016/j.eswa.2021.114602).
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2021a）Liu, Y., Sun, P., Wergeles, N., Shang, Y., 2021a. 小物体检测的深度学习方法的综述和性能评估。专家系统应用。172,
    114602. doi:[10.1016/j.eswa.2021.114602](https:/doi.org/10.1016/j.eswa.2021.114602).
- en: 'Liu et al. (2021b) Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,
    S., Guo, B., 2021b. Swin transformer: Hierarchical vision transformer using shifted
    windows, in: Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pp. 9992–10002. doi:[10.1109/ICCV48922.2021.00986](https:/doi.org/10.1109/ICCV48922.2021.00986).'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2021b）Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo,
    B., 2021b. Swin transformer：使用移动窗口的分层视觉transformer，见：IEEE/CVF国际计算机视觉会议论文集，第9992–10002页。doi:[10.1109/ICCV48922.2021.00986](https:/doi.org/10.1109/ICCV48922.2021.00986).
- en: Liu et al. (2016b) Liu, Z., Wang, H., Weng, L., Yang, Y., 2016b. Ship rotated
    bounding box space for ship extraction from high-resolution optical satellite
    images with complex backgrounds. IEEE Geoscience and Remote Sensing Letters. 13,
    1074–1078. doi:[10.1109/LGRS.2016.2565705](https:/doi.org/10.1109/LGRS.2016.2565705).
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2016b）Liu, Z., Wang, H., Weng, L., Yang, Y., 2016b. 船舶旋转边界框空间用于从具有复杂背景的高分辨率光学卫星图像中提取船舶。IEEE地球科学与遥感学会快报。13,
    1074–1078. doi:[10.1109/LGRS.2016.2565705](https:/doi.org/10.1109/LGRS.2016.2565705).
- en: Long et al. (2017) Long, Y., Gong, Y., Xiao, Z., Liu, Q., 2017. Accurate object
    localization in remote sensing images based on convolutional neural networks.
    IEEE Transactions on Geoscience and Remote Sensing 55, 2486–2498. doi:[10.1109/TGRS.2016.2645610](https:/doi.org/10.1109/TGRS.2016.2645610).
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long等（2017）Long, Y., Gong, Y., Xiao, Z., Liu, Q., 2017. 基于卷积神经网络的遥感图像中的准确物体定位。IEEE地球科学与遥感学会期刊
    55, 2486–2498. doi:[10.1109/TGRS.2016.2645610](https:/doi.org/10.1109/TGRS.2016.2645610).
- en: 'Long et al. (2021) Long, Y., Xia, G.S., Li, S., Yang, W., Yang, M.Y., Zhu,
    X.X., Zhang, L., Li, D., 2021. On creating benchmark dataset for aerial image
    interpretation: Reviews, guidances, and million-aid. IEEE Journal of Selected
    Topics in Applied Earth Observations and Remote Sensing. 14, 4205–4230. doi:[10.1109/JSTARS.2021.3070368](https:/doi.org/10.1109/JSTARS.2021.3070368).'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long等（2021）Long, Y., Xia, G.S., Li, S., Yang, W., Yang, M.Y., Zhu, X.X., Zhang,
    L., Li, D., 2021. 创建用于航空图像解读的基准数据集：综述、指导和百万援助。IEEE应用地球观测与遥感学会期刊。14, 4205–4230.
    doi:[10.1109/JSTARS.2021.3070368](https:/doi.org/10.1109/JSTARS.2021.3070368).
- en: Ma et al. (2018) Ma, J., Shao, W., Ye, H., Wang, L., Wang, H., Zheng, Y., Xue,
    X., 2018. Arbitrary-oriented scene text detection via rotation proposals. IEEE
    Transactions on Multimedia. 20, 3111–3122. doi:[10.1109/TMM.2018.2818020](https:/doi.org/10.1109/TMM.2018.2818020).
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma等（2018）Ma, J., Shao, W., Ye, H., Wang, L., Wang, H., Zheng, Y., Xue, X., 2018.
    基于旋转提议的任意定向场景文本检测。IEEE多媒体期刊。20, 3111–3122. doi:[10.1109/TMM.2018.2818020](https:/doi.org/10.1109/TMM.2018.2818020).
- en: Ma et al. (2021) Ma, T., Mao, M., Zheng, H., Gao, P., Wang, X., Han, S., Ding,
    E., Zhang, B., Doermann, D., 2021. Oriented Object Detection with Transformer.
    arXiv e-prints. , arXiv:2106.03146.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma等（2021）Ma, T., Mao, M., Zheng, H., Gao, P., Wang, X., Han, S., Ding, E., Zhang,
    B., Doermann, D., 2021. 使用Transformer进行定向物体检测。arXiv e-prints. , arXiv:2106.03146.
- en: 'Marcos et al. (2017) Marcos, D., Volpi, M., Komodakis, N., Tuia, D., 2017.
    Rotation equivariant vector field networks, in: Proceedings of the IEEE International
    Conference on Computer Vision, pp. 5058–5067. doi:[10.1109/ICCV.2017.540](https:/doi.org/10.1109/ICCV.2017.540).'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marcos等（2017）Marcos, D., Volpi, M., Komodakis, N., Tuia, D., 2017. 旋转等变向量场网络，见：IEEE国际计算机视觉会议论文集，第5058–5067页。doi:[10.1109/ICCV.2017.540](https:/doi.org/10.1109/ICCV.2017.540).
- en: 'Mehta and Rastegari (2021) Mehta, S., Rastegari, M., 2021. MobileViT: Light-weight,
    General-purpose, and Mobile-friendly Vision Transformer. arXiv e-prints. , arXiv:2110.02178.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehta和Rastegari（2021）Mehta, S., Rastegari, M., 2021. MobileViT：轻量级、通用且适用于移动设备的视觉transformer。arXiv
    e-prints. , arXiv:2110.02178.
- en: 'Mnih et al. (2014) Mnih, V., Heess, N., Graves, A., Kavukcuoglu, K., 2014.
    Recurrent models of visual attention, in: Proceedings of the 27th International
    Conference on Neural Information Processing Systems - Volume 2, pp. 2204–2212.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih等（2014）Mnih, V., Heess, N., Graves, A., Kavukcuoglu, K., 2014. 视觉注意力的递归模型，见：第27届神经信息处理系统国际会议论文集
    - 卷2，第2204–2212页.
- en: 'Mondal (2020) Mondal, A., 2020. Camouflaged object detection and tracking:
    A survey. International Journal of Image & Graphics. 20, 2050028. doi:[10.1142/S021946782050028X](https:/doi.org/10.1142/S021946782050028X).'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mondal（2020）Mondal, A., 2020. 伪装物体检测与跟踪：综述。国际图像与图形期刊。20, 2050028. doi:[10.1142/S021946782050028X](https:/doi.org/10.1142/S021946782050028X).
- en: 'Nam et al. (2017) Nam, H., Ha, J.W., Kim, J., 2017. Dual attention networks
    for multimodal reasoning and matching, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 2156–2164. doi:[10.1109/CVPR.2017.232](https:/doi.org/10.1109/CVPR.2017.232).'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nam 等（2017）Nam, H., Ha, J.W., Kim, J., 2017. 《用于多模态推理和匹配的双重注意力网络》，发表于：IEEE计算机视觉与模式识别会议论文集，页码
    2156–2164。doi：[10.1109/CVPR.2017.232](https:/doi.org/10.1109/CVPR.2017.232)。
- en: 'Pan et al. (2020) Pan, X., Ren, Y., Sheng, K., Dong, W., Yuan, H., Guo, X.,
    Ma, C., Xu, C., 2020. Dynamic refinement network for oriented and densely packed
    object detection, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 11204–11213. doi:[10.1109/CVPR42600.2020.01122](https:/doi.org/10.1109/CVPR42600.2020.01122).'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等（2020）Pan, X., Ren, Y., Sheng, K., Dong, W., Yuan, H., Guo, X., Ma, C.,
    Xu, C., 2020. 《面向导向和密集堆叠目标检测的动态精细化网络》，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，页码 11204–11213。doi：[10.1109/CVPR42600.2020.01122](https:/doi.org/10.1109/CVPR42600.2020.01122)。
- en: 'Qian et al. (2021) Qian, W., Yang, X., Peng, S., Yan, J., Guo, Y., 2021. Learning
    modulated loss for rotated object detection, in: Proceedings of the AAAI Conference
    on Artificial Intelligence, pp. 2458–2466. doi:[10.1609/aaai.v35i3.16347](https:/doi.org/10.1609/aaai.v35i3.16347).'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等（2021）Qian, W., Yang, X., Peng, S., Yan, J., Guo, Y., 2021. 《用于旋转目标检测的调制损失学习》，发表于：AAAI人工智能会议论文集，页码
    2458–2466。doi：[10.1609/aaai.v35i3.16347](https:/doi.org/10.1609/aaai.v35i3.16347)。
- en: 'Qian et al. (2022) Qian, W., Yang, X., Peng, S., Zhang, X., Yan, J., 2022.
    Rsdet++: Point-based modulated loss for more accurate rotated object detection.
    IEEE Transactions on Circuits and Systems for Video Technology. 32, 7869–7879.
    doi:[10.1109/TCSVT.2022.3186070](https:/doi.org/10.1109/TCSVT.2022.3186070).'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等（2022）Qian, W., Yang, X., Peng, S., Zhang, X., Yan, J., 2022. 《Rsdet++：基于点的调制损失以实现更准确的旋转目标检测》。IEEE视频技术电路与系统汇刊。32，7869–7879。doi：[10.1109/TCSVT.2022.3186070](https:/doi.org/10.1109/TCSVT.2022.3186070)。
- en: 'Qiao et al. (2021) Qiao, S., Chen, L.C., Yuille, A., 2021. Detectors: Detecting
    objects with recursive feature pyramid and switchable atrous convolution, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 10208–10219. doi:[10.1109/CVPR46437.2021.01008](https:/doi.org/10.1109/CVPR46437.2021.01008).'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao 等（2021）Qiao, S., Chen, L.C., Yuille, A., 2021. 《检测器：使用递归特征金字塔和可切换的空洞卷积进行目标检测》，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，页码
    10208–10219。doi：[10.1109/CVPR46437.2021.01008](https:/doi.org/10.1109/CVPR46437.2021.01008)。
- en: 'Razakarivony and Jurie (2016) Razakarivony, S., Jurie, F., 2016. Vehicle detection
    in aerial imagery : A small target detection benchmark. Journal of Visual Communication
    and Image Representation. 34, 187–203. doi:[10.1016/j.jvcir.2015.11.002](https:/doi.org/10.1016/j.jvcir.2015.11.002).'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Razakarivony 和 Jurie（2016）Razakarivony, S., Jurie, F., 2016. 《航拍图像中的车辆检测：小目标检测基准》。视觉通信与图像表示杂志。34，187–203。doi：[10.1016/j.jvcir.2015.11.002](https:/doi.org/10.1016/j.jvcir.2015.11.002)。
- en: 'Redmon et al. (2016) Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016.
    You only look once: Unified, real-time object detection, in: Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 779–788. doi:[10.1109/CVPR.2016.91](https:/doi.org/10.1109/CVPR.2016.91).'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redmon 等（2016）Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. 《你只看一次：统一的实时目标检测》，发表于：IEEE计算机视觉与模式识别会议论文集，页码
    779–788。doi：[10.1109/CVPR.2016.91](https:/doi.org/10.1109/CVPR.2016.91)。
- en: 'Redmon and Farhadi (2017) Redmon, J., Farhadi, A., 2017. Yolo9000: Better,
    faster, stronger, in: Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pp. 6517–6525. doi:[10.1109/CVPR.2017.690](https:/doi.org/10.1109/CVPR.2017.690).'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redmon 和 Farhadi（2017）Redmon, J., Farhadi, A., 2017. 《Yolo9000：更好、更快、更强》，发表于：IEEE计算机视觉与模式识别会议论文集，页码
    6517–6525。doi：[10.1109/CVPR.2017.690](https:/doi.org/10.1109/CVPR.2017.690)。
- en: 'Ren et al. (2017) Ren, S., He, K., Girshick, R., Sun, J., 2017. Faster r-cnn:
    Towards real-time object detection with region proposal networks. IEEE Transactions
    on Pattern Analysis and Machine Intelligence. 39, 1137–1149. doi:[10.1109/TPAMI.2016.2577031](https:/doi.org/10.1109/TPAMI.2016.2577031).'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等（2017）Ren, S., He, K., Girshick, R., Sun, J., 2017. 《Faster R-CNN：基于区域提议网络的实时目标检测》，IEEE模式分析与机器智能汇刊。39，1137–1149。doi：[10.1109/TPAMI.2016.2577031](https:/doi.org/10.1109/TPAMI.2016.2577031)。
- en: 'Rezatofighi et al. (2019) Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A.,
    Reid, I., Savarese, S., 2019. Generalized intersection over union: A metric and
    a loss for bounding box regression, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 658–666. doi:[10.1109/CVPR.2019.00075](https:/doi.org/10.1109/CVPR.2019.00075).'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rezatofighi等人（2019）Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid,
    I., Savarese, S., 2019. 广义交并比：一种用于边界框回归的度量和损失，见：IEEE/CVF计算机视觉与模式识别会议，pp. 658–666.
    doi:[10.1109/CVPR.2019.00075](https:/doi.org/10.1109/CVPR.2019.00075).
- en: Russakovsky et al. (2015) Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh,
    S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei,
    L., 2015. Imagenet large scale visual recognition challenge. International Journal
    of Computer Vision. 115, 211–252. doi:[10.1007/s11263-015-0816-y](https:/doi.org/10.1007/s11263-015-0816-y).
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russakovsky等人（2015）Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh,
    S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei,
    L., 2015. Imagenet大规模视觉识别挑战。《计算机视觉国际期刊》, 115, 211–252. doi:[10.1007/s11263-015-0816-y](https:/doi.org/10.1007/s11263-015-0816-y).
- en: Salvoldi et al. (2022) Salvoldi, M., Cohen-Zada, A.L., Karnieli, A., 2022. Using
    the venµs super-spectral camera for detecting moving vehicles. ISPRS Journal of
    Photogrammetry and Remote Sensing. 192, 33–48. doi:[10.1016/j.isprsjprs.2022.08.005](https:/doi.org/10.1016/j.isprsjprs.2022.08.005).
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salvoldi等人（2022）Salvoldi, M., Cohen-Zada, A.L., Karnieli, A., 2022. 使用venµs超光谱相机检测移动车辆。《国际摄影测量与遥感学杂志》,
    192, 33–48. doi:[10.1016/j.isprsjprs.2022.08.005](https:/doi.org/10.1016/j.isprsjprs.2022.08.005).
- en: 'Schölkopf et al. (2007) Schölkopf, B., Platt, J., Hofmann, T., 2007. Analysis
    of representations for domain adaptation, in: Proceedings of the Advances in Neural
    Information Processing Systems, pp. 137–144.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schölkopf等人（2007）Schölkopf, B., Platt, J., Hofmann, T., 2007. 域适应表示分析，见：神经信息处理系统进展会议，pp.
    137–144.
- en: 'Shen et al. (2016) Shen, L., Lin, Z., Huang, Q., 2016. Relay backpropagation
    for effective learning of deep convolutional neural networks, in: Leibe, B., Matas,
    J., Sebe, N., Welling, M. (Eds.), Proceedings of the European Conference on Computer
    Vision, pp. 467–482. doi:[10.1007/978-3-319-46478-7_29](https:/doi.org/10.1007/978-3-319-46478-7_29).'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen等人（2016）Shen, L., Lin, Z., Huang, Q., 2016. 用于有效学习深度卷积神经网络的继电反向传播，见：Leibe,
    B., Matas, J., Sebe, N., Welling, M.（编），欧洲计算机视觉会议论文集，pp. 467–482. doi:[10.1007/978-3-319-46478-7_29](https:/doi.org/10.1007/978-3-319-46478-7_29).
- en: 'Shvets et al. (2019) Shvets, M., Liu, W., Berg, A., 2019. Leveraging long-range
    temporal relationships between proposals for video object detection, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp. 9755–9763. doi:[10.1109/ICCV.2019.00985](https:/doi.org/10.1109/ICCV.2019.00985).'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shvets等人（2019）Shvets, M., Liu, W., Berg, A., 2019. 利用提案之间的长时间序列关系进行视频对象检测，见：IEEE/CVF国际计算机视觉会议，pp.
    9755–9763. doi:[10.1109/ICCV.2019.00985](https:/doi.org/10.1109/ICCV.2019.00985).
- en: 'Sifre and Mallat (2013) Sifre, L., Mallat, S., 2013. Rotation, scaling and
    deformation invariant scattering for texture discrimination, in: Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1233–1240.
    doi:[10.1109/CVPR.2013.163](https:/doi.org/10.1109/CVPR.2013.163).'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sifre和Mallat（2013）Sifre, L., Mallat, S., 2013. 旋转、缩放和变形不变的散射用于纹理区分，见：IEEE计算机视觉与模式识别会议，pp.
    1233–1240. doi:[10.1109/CVPR.2013.163](https:/doi.org/10.1109/CVPR.2013.163).
- en: 'Simonyan and Zisserman (2015) Simonyan, K., Zisserman, A., 2015. Very Deep
    Convolutional Networks for Large-Scale Image Recognition, in: Proceedings of the
    International Conference on Learning Representations.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan和Zisserman（2015）Simonyan, K., Zisserman, A., 2015. 用于大规模图像识别的非常深卷积网络，见：国际学习表征会议.
- en: 'Singh and Davis (2018) Singh, B., Davis, L.S., 2018. An analysis of scale invariance
    in object detection - snip, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 3578–3587. doi:[10.1109/CVPR.2018.00377](https:/doi.org/10.1109/CVPR.2018.00377).'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh和Davis（2018）Singh, B., Davis, L.S., 2018. 对对象检测中的尺度不变性的分析 - snip，见：IEEE/CVF计算机视觉与模式识别会议，pp.
    3578–3587. doi:[10.1109/CVPR.2018.00377](https:/doi.org/10.1109/CVPR.2018.00377).
- en: 'Singh et al. (2018) Singh, B., Najibi, M., Davis, L.S., 2018. Sniper: Efficient
    multi-scale training, in: Proceedings of the Advances in Neural Information Processing
    Systems. URL: [https://proceedings.neurips.cc/paper/2018/file/166cee72e93a992007a89b39eb29628b-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/166cee72e93a992007a89b39eb29628b-Paper.pdf).'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh等（2018年）Singh, B., Najibi, M., Davis, L.S., 2018. Sniper：高效的多尺度训练，载于：神经信息处理系统进展会议论文集。URL:
    [https://proceedings.neurips.cc/paper/2018/file/166cee72e93a992007a89b39eb29628b-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/166cee72e93a992007a89b39eb29628b-Paper.pdf)。'
- en: 'Song et al. (2016) Song, H., Mao, H., Dally, W.J., 2016. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding, in: Proceedings of the International Conference on Learning Representations.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song等（2016年）Song, H., Mao, H., Dally, W.J., 2016. 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络，载于：国际学习表征会议论文集。
- en: Stankov and He (2013) Stankov, K., He, D.C., 2013. Building detection in very
    high spatial resolution multispectral images using the hit-or-miss transform.
    IEEE Geoscience and Remote Sensing Letters. 10, 86–90. doi:[10.1109/LGRS.2012.2193552](https:/doi.org/10.1109/LGRS.2012.2193552).
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stankov和He（2013年）Stankov, K., He, D.C., 2013. 使用命中或遗漏变换在极高空间分辨率多光谱图像中进行建筑物检测。IEEE地球科学与遥感通讯快报，第10期，86–90页。doi：[10.1109/LGRS.2012.2193552](https:/doi.org/10.1109/LGRS.2012.2193552)。
- en: 'Sun et al. (2021) Sun, Z., Cao, S., Yang, Y., Kitani, K., 2021. Rethinking
    transformer-based set prediction for object detection, in: Proceedings of the
    IEEE/CVF International Conference on Computer Vision, pp. 3591–3600. doi:[10.1109/ICCV48922.2021.00359](https:/doi.org/10.1109/ICCV48922.2021.00359).'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等（2021年）Sun, Z., Cao, S., Yang, Y., Kitani, K., 2021. 重新思考基于Transformer的集合预测在目标检测中的应用，载于：IEEE/CVF国际计算机视觉会议论文集，第3591–3600页。doi：[10.1109/ICCV48922.2021.00359](https:/doi.org/10.1109/ICCV48922.2021.00359)。
- en: 'Szegedy et al. (2017) Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A., 2017.
    Inception-v4, inception-resnet and the impact of residual connections on learning,
    in: Proceedings of the AAAI Conference on Artificial Intelligence. doi:[10.1609/aaai.v31i1.11231](https:/doi.org/10.1609/aaai.v31i1.11231).'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy等（2017年）Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A., 2017. Inception-v4、Inception-ResNet及残差连接对学习的影响，载于：AAAI人工智能会议论文集。doi：[10.1609/aaai.v31i1.11231](https:/doi.org/10.1609/aaai.v31i1.11231)。
- en: 'Szegedy et al. (2015) Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
    Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going deeper with
    convolutions, in: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 1–9. doi:[10.1109/CVPR.2015.7298594](https:/doi.org/10.1109/CVPR.2015.7298594).'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy等（2015年）Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov,
    D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. 通过卷积深入研究，载于：IEEE计算机视觉与模式识别会议论文集，第1–9页。doi：[10.1109/CVPR.2015.7298594](https:/doi.org/10.1109/CVPR.2015.7298594)。
- en: 'Szegedy et al. (2016) Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna,
    Z., 2016. Rethinking the inception architecture for computer vision, in: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818–2826.
    doi:[10.1109/CVPR.2016.308](https:/doi.org/10.1109/CVPR.2016.308).'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy等（2016年）Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.,
    2016. 重新思考计算机视觉中的Inception架构，载于：IEEE计算机视觉与模式识别会议论文集，第2818–2826页。doi：[10.1109/CVPR.2016.308](https:/doi.org/10.1109/CVPR.2016.308)。
- en: 'Tan et al. (2020a) Tan, J., Wang, C., Li, B., Li, Q., Ouyang, W., Yin, C.,
    Yan, J., 2020a. Equalization loss for long-tailed object recognition, in: IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 11659–11668. doi:[10.1109/CVPR42600.2020.01168](https:/doi.org/10.1109/CVPR42600.2020.01168).'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan等（2020年a）Tan, J., Wang, C., Li, B., Li, Q., Ouyang, W., Yin, C., Yan, J.,
    2020a. 长尾目标识别的均衡损失，载于：IEEE/CVF计算机视觉与模式识别会议论文集，第11659–11668页。doi：[10.1109/CVPR42600.2020.01168](https:/doi.org/10.1109/CVPR42600.2020.01168)。
- en: 'Tan et al. (2020b) Tan, M., Pang, R., Le, Q.V., 2020b. Efficientdet: Scalable
    and efficient object detection, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pp. 10778–10787. doi:[10.1109/CVPR42600.2020.01079](https:/doi.org/10.1109/CVPR42600.2020.01079).'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan等（2020年b）Tan, M., Pang, R., Le, Q.V., 2020b. EfficientDet：可扩展且高效的目标检测，载于：IEEE/CVF计算机视觉与模式识别会议论文集，第10778–10787页。doi：[10.1109/CVPR42600.2020.01079](https:/doi.org/10.1109/CVPR42600.2020.01079)。
- en: Tian et al. (2022) Tian, S., Kang, L., Xing, X., Tian, J., Fan, C., Zhang, Y.,
    2022. A relation-augmented embedded graph attention network for remote sensing
    object detection. IEEE Transactions on Geoscience and Remote Sensing. 60, 1–18.
    doi:[10.1109/TGRS.2021.3073269](https:/doi.org/10.1109/TGRS.2021.3073269).
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等人（2022） Tian, S., Kang, L., Xing, X., Tian, J., Fan, C., Zhang, Y., 2022.
    一种关系增强的嵌入图注意力网络用于遥感目标检测。IEEE 地球科学与遥感学报。60, 1–18. doi:[10.1109/TGRS.2021.3073269](https:/doi.org/10.1109/TGRS.2021.3073269).
- en: 'Tong et al. (2020) Tong, K., Wu, Y., Zhou, F., 2020. Recent advances in small
    object detection based on deep learning: A review. Image and Vision Computing.
    97, 103910. doi:[10.1016/j.imavis.2020.103910](https:/doi.org/10.1016/j.imavis.2020.103910).'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tong 等人（2020） Tong, K., Wu, Y., Zhou, F., 2020. 基于深度学习的小物体检测的最新进展：综述。图像与视觉计算。97,
    103910. doi:[10.1016/j.imavis.2020.103910](https:/doi.org/10.1016/j.imavis.2020.103910).
- en: 'Tzeng et al. (2017) Tzeng, E., Hoffman, J., Saenko, K., Darrell, T., 2017.
    Adversarial discriminative domain adaptation, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 2962–2971. doi:[10.1109/CVPR.2017.316](https:/doi.org/10.1109/CVPR.2017.316).'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tzeng 等人（2017） Tzeng, E., Hoffman, J., Saenko, K., Darrell, T., 2017. 对抗性判别领域适应，见：IEEE
    计算机视觉与模式识别会议论文集，第2962–2971页。doi:[10.1109/CVPR.2017.316](https:/doi.org/10.1109/CVPR.2017.316).
- en: 'Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
    Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I., 2017. Attention is all you
    need, in: Proceedings of the 31st International Conference on Neural Information
    Processing Systems, pp. 6000–6010.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人（2017） Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A.N., Kaiser, L., Polosukhin, I., 2017. 注意力机制即是你所需要的，见：第31届国际神经信息处理系统会议论文集，第6000–6010页.
- en: 'Vinyals et al. (2015) Vinyals, O., Toshev, A., Bengio, S., Erhan, D., 2015.
    Show and tell: A neural image caption generator, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 3156–3164. doi:[10.1109/CVPR.2015.7298935](https:/doi.org/10.1109/CVPR.2015.7298935).'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals 等人（2015） Vinyals, O., Toshev, A., Bengio, S., Erhan, D., 2015. 展示与讲述：一种神经图像描述生成器，见：IEEE
    计算机视觉与模式识别会议论文集，第3156–3164页。doi:[10.1109/CVPR.2015.7298935](https:/doi.org/10.1109/CVPR.2015.7298935).
- en: Wang et al. (2023) Wang, D., Zhang, J., Du, B., Tao, D., Zhang, L., 2023. Scaling-up
    Remote Sensing Segmentation Dataset with Segment Anything Model. arXiv e-prints
    , arXiv:2305.02034.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023） Wang, D., Zhang, J., Du, B., Tao, D., Zhang, L., 2023. 使用 Segment
    Anything 模型扩展遥感分割数据集。arXiv e-prints , arXiv:2305.02034.
- en: Wang et al. (2022) Wang, D., Zhang, Q., Xu, Y., Zhang, J., Du, B., Tao, D.,
    Zhang, L., 2022. Advancing plain vision transformer towards remote sensing foundation
    model. IEEE Transactions on Geoscience and Remote Sensing. , 1–1doi:[10.1109/TGRS.2022.3222818](https:/doi.org/10.1109/TGRS.2022.3222818).
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2022） Wang, D., Zhang, Q., Xu, Y., Zhang, J., Du, B., Tao, D., Zhang,
    L., 2022. 将普通视觉变换器推进到遥感基础模型。IEEE 地球科学与遥感学报。 , 1–1 doi:[10.1109/TGRS.2022.3222818](https:/doi.org/10.1109/TGRS.2022.3222818).
- en: 'Wang et al. (2021a) Wang, J., Lan, C., Liu, C., Ouyang, Y., Qin, T., 2021a.
    Generalizing to unseen domains: A survey on domain generalization, in: Proceedings
    of the Thirtieth International Joint Conference on Artificial Intelligence, pp.
    4627–4635. doi:[10.24963/ijcai.2021/628](https:/doi.org/10.24963/ijcai.2021/628).'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2021a） Wang, J., Lan, C., Liu, C., Ouyang, Y., Qin, T., 2021a. 泛化到未见领域：领域泛化综述，见：第三十届国际人工智能联合会议论文集，第4627–4635页。doi:[10.24963/ijcai.2021/628](https:/doi.org/10.24963/ijcai.2021/628).
- en: 'Wang and Deng (2018) Wang, M., Deng, W., 2018. Deep visual domain adaptation:
    A survey. Neurocomputing. 312, 135–153. doi:[10.1016/j.neucom.2018.05.083](https:/doi.org/10.1016/j.neucom.2018.05.083).'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Deng（2018） Wang, M., Deng, W., 2018. 深度视觉领域适应：综述。Neurocomputing. 312,
    135–153. doi:[10.1016/j.neucom.2018.05.083](https:/doi.org/10.1016/j.neucom.2018.05.083).
- en: 'Wang et al. (2018a) Wang, N., Gao, X., Tao, D., Yang, H., Li, X., 2018a. Facial
    feature point detection: A comprehensive survey. Neurocomputing. 275, 50–65. doi:[10.1016/j.neucom.2017.05.013](https:/doi.org/10.1016/j.neucom.2017.05.013).'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2018a） Wang, N., Gao, X., Tao, D., Yang, H., Li, X., 2018a. 面部特征点检测：综合调查。Neurocomputing.
    275, 50–65. doi:[10.1016/j.neucom.2017.05.013](https:/doi.org/10.1016/j.neucom.2017.05.013).
- en: 'Wang et al. (2018b) Wang, P., Wu, Q., Shen, C., Dick, A., van den Hengel, A.,
    2018b. Fvqa: Fact-based visual question answering. IEEE Transactions on Pattern
    Analysis and Machine Intelligence. 40, 2413–2427. doi:[10.1109/TPAMI.2017.2754246](https:/doi.org/10.1109/TPAMI.2017.2754246).'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2018b） Wang, P., Wu, Q., Shen, C., Dick, A., van den Hengel, A., 2018b.
    Fvqa：基于事实的视觉问答。IEEE 模式分析与机器智能学报。40, 2413–2427. doi:[10.1109/TPAMI.2017.2754246](https:/doi.org/10.1109/TPAMI.2017.2754246).
- en: 'Wang et al. (2018c) Wang, S., Zhou, Y., Yan, J., Deng, Z., 2018c. Fully motion-aware
    network for video object detection, in: Proceedings of the European Conference
    on Computer Vision, pp. 557–573. doi:[10.1007/978-3-030-01261-8_33](https:/doi.org/10.1007/978-3-030-01261-8_33).'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2018c）Wang, S., Zhou, Y., Yan, J., Deng, Z., 2018c. 完全运动感知网络用于视频目标检测，发表于：欧洲计算机视觉会议论文集，页码
    557–573. doi：[10.1007/978-3-030-01261-8_33](https:/doi.org/10.1007/978-3-030-01261-8_33)。
- en: 'Wang et al. (2020) Wang, T., Li, Y., Kang, B., Li, J., Liew, J., Tang, S.,
    Hoi, S., Feng, J., 2020. The devil is in classification: A simple framework for
    long-tail instance segmentation, in: Vedaldi, A., Bischof, H., Brox, T., Frahm,
    J.M. (Eds.), Proceedings of the European Conference on Computer Vision, pp. 728–744.
    doi:[10.1007/978-3-030-58568-6_43](https:/doi.org/10.1007/978-3-030-58568-6_43).'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020）Wang, T., Li, Y., Kang, B., Li, J., Liew, J., Tang, S., Hoi, S.,
    Feng, J., 2020. 分类中的魔鬼：长尾实例分割的简单框架，发表于：Vedaldi, A., Bischof, H., Brox, T., Frahm,
    J.M.（编），欧洲计算机视觉会议论文集，页码 728–744. doi：[10.1007/978-3-030-58568-6_43](https:/doi.org/10.1007/978-3-030-58568-6_43)。
- en: 'Wang et al. (2021b) Wang, T., Zhu, Y., Zhao, C., Zeng, W., Wang, J., Tang,
    M., 2021b. Adaptive class suppression loss for long-tail object detection, in:
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3102–3111.
    doi:[10.1109/CVPR46437.2021.00312](https:/doi.org/10.1109/CVPR46437.2021.00312).'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2021b）Wang, T., Zhu, Y., Zhao, C., Zeng, W., Wang, J., Tang, M., 2021b.
    长尾物体检测的自适应类抑制损失，发表于：IEEE/CVF 计算机视觉与模式识别会议，页码 3102–3111. doi：[10.1109/CVPR46437.2021.00312](https:/doi.org/10.1109/CVPR46437.2021.00312)。
- en: 'Wang et al. (2021c) Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang,
    D., Lu, T., Luo, P., Shao, L., 2021c. Pyramid vision transformer: A versatile
    backbone for dense prediction without convolutions, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pp. 548–558. doi:[10.1109/ICCV48922.2021.00061](https:/doi.org/10.1109/ICCV48922.2021.00061).'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等（2021c）Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu,
    T., Luo, P., Shao, L., 2021c. Pyramid vision transformer: 一种通用的密集预测骨干网络，无需卷积，发表于：IEEE/CVF
    国际计算机视觉大会论文集，页码 548–558. doi：[10.1109/ICCV48922.2021.00061](https:/doi.org/10.1109/ICCV48922.2021.00061)。'
- en: 'Wang et al. (2019) Wang, X., Cai, Z., Gao, D., Vasconcelos, N., 2019. Towards
    universal object detection by domain attention, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7281–7290. doi:[10.1109/CVPR.2019.00746](https:/doi.org/10.1109/CVPR.2019.00746).'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2019）Wang, X., Cai, Z., Gao, D., Vasconcelos, N., 2019. 通过领域注意力实现通用物体检测，发表于：IEEE/CVF
    计算机视觉与模式识别会议（CVPR）论文集，页码 7281–7290. doi：[10.1109/CVPR.2019.00746](https:/doi.org/10.1109/CVPR.2019.00746)。
- en: Wei et al. (2020) Wei, H., Zhang, Y., Chang, Z., Li, H., Wang, H., Sun, X.,
    2020. Oriented objects as pairs of middle lines. ISPRS Journal of Photogrammetry
    and Remote Sensing. 169, 268–279. doi:[https://doi.org/10.1016/j.isprsjprs.2020.09.022](https:/doi.org/https://doi.org/10.1016/j.isprsjprs.2020.09.022).
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2020）Wei, H., Zhang, Y., Chang, Z., Li, H., Wang, H., Sun, X., 2020. 定向物体作为中线对。ISPRS
    摄影测量与遥感杂志。169, 268–279. doi：[https://doi.org/10.1016/j.isprsjprs.2020.09.022](https:/doi.org/https://doi.org/10.1016/j.isprsjprs.2020.09.022)。
- en: 'Weiler and Cesa (2019) Weiler, M., Cesa, G., 2019. General e(2)-equivariant
    steerable cnns, in: Proceedings of the Advances in Neural Information Processing
    Systems. URL: [https://proceedings.neurips.cc/paper/2019/file/45d6637b718d0f24a237069fe41b0db4-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/45d6637b718d0f24a237069fe41b0db4-Paper.pdf).'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weiler 和 Cesa（2019）Weiler, M., Cesa, G., 2019. 一般的 e(2)-等变可控卷积神经网络，发表于：神经信息处理系统进展论文集。网址：[https://proceedings.neurips.cc/paper/2019/file/45d6637b718d0f24a237069fe41b0db4-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/45d6637b718d0f24a237069fe41b0db4-Paper.pdf)。
- en: 'Weiler et al. (2018) Weiler, M., Hamprecht, F.A., Storath, M., 2018. Learning
    steerable filters for rotation equivariant cnns, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 849–858. doi:[10.1109/CVPR.2018.00095](https:/doi.org/10.1109/CVPR.2018.00095).'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weiler 等（2018）Weiler, M., Hamprecht, F.A., Storath, M., 2018. 学习用于旋转等变卷积神经网络的可控滤波器，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，页码 849–858. doi：[10.1109/CVPR.2018.00095](https:/doi.org/10.1109/CVPR.2018.00095)。
- en: 'Wen et al. (2021) Wen, L., Du, D., Zhu, P., Hu, Q., Wang, Q., Bo, L., Lyu,
    S., 2021. Detection, tracking, and counting meets drones in crowds: A benchmark,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 7808–7817. doi:[10.1109/CVPR46437.2021.00772](https:/doi.org/10.1109/CVPR46437.2021.00772).'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等（2021）Wen, L., Du, D., Zhu, P., Hu, Q., Wang, Q., Bo, L., Lyu, S., 2021.
    在人群中检测、跟踪和计数无人机：基准测试，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，页码 7808–7817. doi：[10.1109/CVPR46437.2021.00772](https:/doi.org/10.1109/CVPR46437.2021.00772)。
- en: 'Worrall et al. (2017) Worrall, D.E., Garbin, S.J., Turmukhambetov, D., Brostow,
    G.J., 2017. Harmonic networks: Deep translation and rotation equivariance, in:
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 7168–7177. doi:[10.1109/CVPR.2017.758](https:/doi.org/10.1109/CVPR.2017.758).'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沃勒尔等（2017）沃勒尔, D.E., 加尔宾, S.J., 图尔穆罕贝托夫, D., 布罗斯托, G.J., 2017. 和谐网络：深度平移和旋转等变性，发表于：IEEE计算机视觉与模式识别会议论文集，第7168–7177页。doi:[10.1109/CVPR.2017.758](https:/doi.org/10.1109/CVPR.2017.758)。
- en: Wright et al. (2009) Wright, J., Yang, A.Y., Ganesh, A., Sastry, S.S., Ma, Y.,
    2009. Robust face recognition via sparse representation. IEEE Transactions on
    Pattern Analysis and Machine Intelligence. 31, 210–227. doi:[10.1109/TPAMI.2008.79](https:/doi.org/10.1109/TPAMI.2008.79).
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 怀特等（2009）怀特, J., 杨, A.Y., 加内什, A., 萨斯特里, S.S., 马, Y., 2009. 通过稀疏表示进行稳健的人脸识别。IEEE模式分析与机器智能汇刊。31,
    210–227. doi:[10.1109/TPAMI.2008.79](https:/doi.org/10.1109/TPAMI.2008.79)。
- en: Wu et al. (2020) Wu, X., Sahoo, D., Hoi, S.C., 2020. Recent advances in deep
    learning for object detection. Neurocomputing. 396, 39–64. doi:[10.1016/j.neucom.2020.01.085](https:/doi.org/10.1016/j.neucom.2020.01.085).
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2020）吴, X., 萨胡, D., 霍伊, S.C., 2020. 深度学习在目标检测中的最新进展。神经计算。396, 39–64. doi:[10.1016/j.neucom.2020.01.085](https:/doi.org/10.1016/j.neucom.2020.01.085)。
- en: 'Wu and Ji (2019) Wu, Y., Ji, Q., 2019. Facial landmark detection: A literature
    survey. International Journal of Computer Vision. 127, 115–142. doi:[10.1007/s11263-018-1097-z](https:/doi.org/10.1007/s11263-018-1097-z).'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴与季（2019）吴, Y., 季, Q., 2019. 面部关键点检测：文献综述。国际计算机视觉杂志。127, 115–142. doi:[10.1007/s11263-018-1097-z](https:/doi.org/10.1007/s11263-018-1097-z)。
- en: 'Xia et al. (2018) Xia, G.S., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo,
    J., Datcu, M., Pelillo, M., Zhang, L., 2018. Dota: A large-scale dataset for object
    detection in aerial images, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 3974–3983. doi:[10.1109/CVPR.2018.00418](https:/doi.org/10.1109/CVPR.2018.00418).'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 夏等（2018）夏, G.S., 白, X., 丁, J., 朱, Z., 贝隆吉, S., 罗, J., 达图, M., 佩里洛, M., 张, L.,
    2018. Dota：一个大规模的航空图像目标检测数据集，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，第3974–3983页。doi:[10.1109/CVPR.2018.00418](https:/doi.org/10.1109/CVPR.2018.00418)。
- en: Xiao et al. (2020a) Xiao, Y., Tian, Z., Yu, J., Y., Z., S., D., X., L., 2020a.
    A review of object detection based on deep learning. Multimedia Tools and Applications.
    79, 23729–23791. doi:[10.1007/s11042-020-08976-6](https:/doi.org/10.1007/s11042-020-08976-6).
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肖等（2020a）肖, Y., 田, Z., 余, J., Y., Z., S., D., X., L., 2020a. 基于深度学习的目标检测综述。多媒体工具与应用。79,
    23729–23791. doi:[10.1007/s11042-020-08976-6](https:/doi.org/10.1007/s11042-020-08976-6)。
- en: Xiao et al. (2015) Xiao, Z., Liu, Q., Tang, G., Zhai, X., 2015. Elliptic fourier
    transformation-based histograms of oriented gradients for rotationally invariant
    object detection in remote-sensing images. International Journal of Remote Sensing
    36, 618–644. doi:[10.1080/01431161.2014.999881](https:/doi.org/10.1080/01431161.2014.999881).
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肖等（2015）肖, Z., 刘, Q., 唐, G., 翟, X., 2015. 基于椭圆傅里叶变换的方向梯度直方图用于遥感图像中的旋转不变目标检测。国际遥感杂志
    36, 618–644. doi:[10.1080/01431161.2014.999881](https:/doi.org/10.1080/01431161.2014.999881)。
- en: Xiao et al. (2020b) Xiao, Z., Qian, L., Shao, W., Tan, X., Wang, K., 2020b.
    Axis learning for orientated objects detection in aerial images. Remote Sensing.
    12. doi:[10.3390/rs12060908](https:/doi.org/10.3390/rs12060908).
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肖等（2020b）肖, Z., 钱, L., 邵, W., 谭, X., 王, K., 2020b. 用于航空图像中目标检测的轴学习。遥感。12. doi:[10.3390/rs12060908](https:/doi.org/10.3390/rs12060908)。
- en: 'Xie et al. (2017) Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K., 2017.
    Aggregated residual transformations for deep neural networks, in: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5987–5995.
    doi:[10.1109/CVPR.2017.634](https:/doi.org/10.1109/CVPR.2017.634).'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谢等（2017）谢, S., 吉尔希克, R., 多拉, P., 涂, Z., 贺, K., 2017. 深度神经网络的聚合残差变换，发表于：IEEE计算机视觉与模式识别会议论文集，第5987–5995页。doi:[10.1109/CVPR.2017.634](https:/doi.org/10.1109/CVPR.2017.634)。
- en: 'Xie et al. (2021) Xie, X., Cheng, G., Wang, J., Yao, X., Han, J., 2021. Oriented
    r-cnn for object detection, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pp. 3500–3509. doi:[10.1109/ICCV48922.2021.00350](https:/doi.org/10.1109/ICCV48922.2021.00350).'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谢等（2021）谢, X., 程, G., 王, J., 姚, X., 韩, J., 2021. 用于目标检测的方向 r-cnn，发表于：IEEE/CVF国际计算机视觉会议论文集，第3500–3509页。doi:[10.1109/ICCV48922.2021.00350](https:/doi.org/10.1109/ICCV48922.2021.00350)。
- en: 'Xiong et al. (2021) Xiong, Y., Liu, H., Gupta, S., Akin, B., Bender, G., Wang,
    Y., Kindermans, P.J., Tan, M., Singh, V., Chen, B., 2021. Mobiledets: Searching
    for object detection architectures for mobile accelerators, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3824–3833.
    doi:[10.1109/CVPR46437.2021.00382](https:/doi.org/10.1109/CVPR46437.2021.00382).'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiong et al. (2021) Xiong, Y., Liu, H., Gupta, S., Akin, B., Bender, G., Wang,
    Y., Kindermans, P.J., Tan, M., Singh, V., Chen, B., 2021. Mobiledets: 为移动加速器搜索目标检测架构，载于《IEEE/CVF计算机视觉与模式识别会议论文集》，第3824–3833页。doi:[10.1109/CVPR46437.2021.00382](https:/doi.org/10.1109/CVPR46437.2021.00382)。'
- en: Xu et al. (2021a) Xu, Y., Fu, M., Wang, Q., Wang, Y., Chen, K., Xia, G.S., Bai,
    X., 2021a. Gliding vertex on the horizontal bounding box for multi-oriented object
    detection. IEEE Transactions on Pattern Analysis and Machine Intelligence. 43,
    1452–1459. doi:[10.1109/TPAMI.2020.2974745](https:/doi.org/10.1109/TPAMI.2020.2974745).
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2021a) Xu, Y., Fu, M., Wang, Q., Wang, Y., Chen, K., Xia, G.S., Bai,
    X., 2021a. 用于多方向目标检测的水平边界框上的滑动顶点。IEEE 《模式分析与机器智能学报》，43，1452–1459。doi:[10.1109/TPAMI.2020.2974745](https:/doi.org/10.1109/TPAMI.2020.2974745)。
- en: 'Xu et al. (2021b) Xu, Y., Zhang, Q., Zhang, J., Tao, D., 2021b. Vitae: Vision
    transformer advanced by exploring intrinsic inductive bias, in: Proceedings of
    the Advances in Neural Information Processing Systems. URL: [https://openreview.net/pdf?id=_RnHyIeu5Y5](https://openreview.net/pdf?id=_RnHyIeu5Y5).'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2021b) Xu, Y., Zhang, Q., Zhang, J., Tao, D., 2021b. Vitae: 通过探索内在归纳偏差来增强视觉变换器，载于《神经信息处理系统进展会议论文集》。网址:
    [https://openreview.net/pdf?id=_RnHyIeu5Y5](https://openreview.net/pdf?id=_RnHyIeu5Y5)。'
- en: 'Yang et al. (2021a) Yang, X., Hou, L., Zhou, Y., Wang, W., Yan, J., 2021a.
    Dense label encoding for boundary discontinuity free rotation detection, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15814–15824.
    doi:[10.1109/CVPR46437.2021.01556](https:/doi.org/10.1109/CVPR46437.2021.01556).'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2021a) Yang, X., Hou, L., Zhou, Y., Wang, W., Yan, J., 2021a. 边界不连续旋转检测的密集标签编码，载于《IEEE/CVF计算机视觉与模式识别会议论文集》，第15814–15824页。doi:[10.1109/CVPR46437.2021.01556](https:/doi.org/10.1109/CVPR46437.2021.01556)。
- en: Yang et al. (2018a) Yang, X., Sun, H., Fu, K., Yang, J., Sun, X., Yan, M., Guo,
    Z., 2018a. Automatic ship detection in remote sensing images from google earth
    of complex scenes based on multiscale rotation dense feature pyramid networks.
    Remote Sensing. 10. doi:[10.3390/rs10010132](https:/doi.org/10.3390/rs10010132).
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2018a) Yang, X., Sun, H., Fu, K., Yang, J., Sun, X., Yan, M., Guo,
    Z., 2018a. 基于多尺度旋转密集特征金字塔网络的复杂场景自动船舶检测，来自Google Earth的遥感图像。遥感。10。doi:[10.3390/rs10010132](https:/doi.org/10.3390/rs10010132)。
- en: Yang et al. (2018b) Yang, X., Sun, H., Fu, K., Yang, J., Sun, X., Yan, M., Guo,
    Z., 2018b. Automatic ship detection in remote sensing images from google earth
    of complex scenes based on multiscale rotation dense feature pyramid networks.
    Remote Sensing. 10. doi:[10.3390/rs10010132](https:/doi.org/10.3390/rs10010132).
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2018b) Yang, X., Sun, H., Fu, K., Yang, J., Sun, X., Yan, M., Guo,
    Z., 2018b. 基于多尺度旋转密集特征金字塔网络的复杂场景自动船舶检测，来自Google Earth的遥感图像。遥感。10。doi:[10.3390/rs10010132](https:/doi.org/10.3390/rs10010132)。
- en: 'Yang and Yan (2020) Yang, X., Yan, J., 2020. Arbitrary-oriented object detection
    with circular smooth label, in: Proceedings of the European Conference on Computer
    Vision, pp. 677–694. doi:[10.1007/978-3-030-58598-3_40](https:/doi.org/10.1007/978-3-030-58598-3_40).'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang and Yan (2020) Yang, X., Yan, J., 2020. 基于圆形平滑标签的任意方向目标检测，载于《欧洲计算机视觉会议论文集》，第677–694页。doi:[10.1007/978-3-030-58598-3_40](https:/doi.org/10.1007/978-3-030-58598-3_40)。
- en: 'Yang et al. (2021b) Yang, X., Yan, J., Feng, Z.and He, T., 2021b. R3det: Refined
    single-stage detector with feature refinement for rotating object, in: Proceedings
    of the AAAI Conference on Artificial Intelligence, pp. 3163–3171. doi:[10.1609/aaai.v35i4.16426](https:/doi.org/10.1609/aaai.v35i4.16426).'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2021b) Yang, X., Yan, J., Feng, Z.and He, T., 2021b. R3det: 具有特征精炼的改进单阶段检测器，用于旋转物体，载于《AAAI人工智能会议论文集》，第3163–3171页。doi:[10.1609/aaai.v35i4.16426](https:/doi.org/10.1609/aaai.v35i4.16426)。'
- en: 'Yang et al. (2022a) Yang, X., Yan, J., Liao, W., Yang, X., Tang, J., He, T.,
    2022a. Scrdet++: Detecting small, cluttered and rotated objects via instance-level
    feature denoising and rotation loss smoothing. IEEE Transactions on Pattern Analysis
    and Machine Intelligence. , 1–1doi:[10.1109/TPAMI.2022.3166956](https:/doi.org/10.1109/TPAMI.2022.3166956).'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2022a) Yang, X., Yan, J., Liao, W., Yang, X., Tang, J., He, T.,
    2022a. Scrdet++: 通过实例级特征去噪和旋转损失平滑检测小型、杂乱和旋转物体。IEEE 《模式分析与机器智能学报》，1–1，doi:[10.1109/TPAMI.2022.3166956](https:/doi.org/10.1109/TPAMI.2022.3166956)。'
- en: 'Yang et al. (2021c) Yang, X., Yan, J., Ming, Q., Wang, W., Zhang, X., Tian,
    Q., 2021c. Rethinking rotated object detection with gaussian wasserstein distance
    loss, in: Proceedings of the 38th International Conference on Machine Learning,
    pp. 11830–11841. URL: [https://proceedings.mlr.press/v139/yang21l.html](https://proceedings.mlr.press/v139/yang21l.html).'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang等（2021c）Yang, X., Yan, J., Ming, Q., Wang, W., Zhang, X., Tian, Q., 2021c.
    重新审视旋转物体检测中的高斯瓦瑟斯坦距离损失，载于：第38届国际机器学习会议论文集，第11830–11841页。URL: [https://proceedings.mlr.press/v139/yang21l.html](https://proceedings.mlr.press/v139/yang21l.html)。'
- en: 'Yang et al. (2019a) Yang, X., Yang, J., Yan, J., Zhang, Y., Zhang, T., Guo,
    Z., Sun, X., Fu, K., 2019a. Scrdet: Towards more robust detection for small, cluttered
    and rotated objects, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pp. 8231–8240. doi:[10.1109/ICCV.2019.00832](https:/doi.org/10.1109/ICCV.2019.00832).'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang等（2019a）Yang, X., Yang, J., Yan, J., Zhang, Y., Zhang, T., Guo, Z., Sun,
    X., Fu, K., 2019a. Scrdet: 旨在提高对小型、杂乱和旋转物体的检测鲁棒性，载于：IEEE/CVF计算机视觉国际会议论文集，第8231–8240页。doi：[10.1109/ICCV.2019.00832](https:/doi.org/10.1109/ICCV.2019.00832)。'
- en: 'Yang et al. (2021d) Yang, X., Yang, X., Yang, J., Ming, Q., Wang, W., Tian,
    Q., Yan, J., 2021d. Learning high-precision bounding box for rotated object detection
    via kullback-leibler divergence, in: Proceedings of the Advances in Neural Information
    Processing Systems, pp. 18381–18394. URL: [https://proceedings.neurips.cc/paper/2021/file/98f13708210194c475687be6106a3b84-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/98f13708210194c475687be6106a3b84-Paper.pdf).'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang等（2021d）Yang, X., Yang, X., Yang, J., Ming, Q., Wang, W., Tian, Q., Yan,
    J., 2021d. 通过Kullback-Leibler散度学习高精度边界框用于旋转物体检测，载于：神经信息处理系统进展论文集，第18381–18394页。URL:
    [https://proceedings.neurips.cc/paper/2021/file/98f13708210194c475687be6106a3b84-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/98f13708210194c475687be6106a3b84-Paper.pdf)。'
- en: Yang et al. (2022b) Yang, X., Zhang, G., Yang, X., Zhou, Y., Wang, W., Tang,
    J., He, T., Yan, J., 2022b. Detecting rotated objects as gaussian distributions
    and its 3-d generalization. IEEE Transactions on Pattern Analysis and Machine
    Intelligence. , 1–18doi:[10.1109/TPAMI.2022.3197152](https:/doi.org/10.1109/TPAMI.2022.3197152).
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2022b）Yang, X., Zhang, G., Yang, X., Zhou, Y., Wang, W., Tang, J., He,
    T., Yan, J., 2022b. 将旋转物体检测为高斯分布及其三维推广。IEEE模式分析与机器智能学报，1–18。doi：[10.1109/TPAMI.2022.3197152](https:/doi.org/10.1109/TPAMI.2022.3197152)。
- en: Yang et al. (2022) Yang, X., Zhou, Y., Zhang, G., Yang, J., Wang, W., Yan, J.,
    Zhang, X., Tian, Q., 2022. The KFIoU Loss for Rotated Object Detection. arXiv
    e-prints. , arXiv:2201.12558.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2022）Yang, X., Zhou, Y., Zhang, G., Yang, J., Wang, W., Yan, J., Zhang,
    X., Tian, Q., 2022. 用于旋转物体检测的KFIoU损失。arXiv e-prints，arXiv:2201.12558。
- en: 'Yang et al. (2019b) Yang, Z., Liu, S., Hu, H., Wang, L., Lin, S., 2019b. Reppoints:
    Point set representation for object detection, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pp. 9656–9665. doi:[10.1109/ICCV.2019.00975](https:/doi.org/10.1109/ICCV.2019.00975).'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang等（2019b）Yang, Z., Liu, S., Hu, H., Wang, L., Lin, S., 2019b. Reppoints:
    用于物体检测的点集表示，载于：IEEE/CVF计算机视觉国际会议论文集，第9656–9665页。doi：[10.1109/ICCV.2019.00975](https:/doi.org/10.1109/ICCV.2019.00975)。'
- en: 'Ye and Doermann (2015) Ye, Q., Doermann, D., 2015. Text detection and recognition
    in imagery: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence.
    37, 1480–1500. doi:[10.1109/TPAMI.2014.2366765](https:/doi.org/10.1109/TPAMI.2014.2366765).'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye和Doermann（2015）Ye, Q., Doermann, D., 2015. 图像中的文本检测与识别：综述。IEEE模式分析与机器智能学报，37，1480–1500。doi：[10.1109/TPAMI.2014.2366765](https:/doi.org/10.1109/TPAMI.2014.2366765)。
- en: 'Yi et al. (2021) Yi, J., Wu, P., Liu, B., Huang, Q., Qu, H., Metaxas, D., 2021.
    Oriented object detection in aerial images with box boundary-aware vectors, in:
    Proceedings of the IEEE Winter Conference on Applications of Computer Vision,
    pp. 2149–2158. doi:[10.1109/WACV48630.2021.00220](https:/doi.org/10.1109/WACV48630.2021.00220).'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yi等（2021）Yi, J., Wu, P., Liu, B., Huang, Q., Qu, H., Metaxas, D., 2021. 具有边界感知向量的航拍图像中的定向物体检测，载于：IEEE冬季计算机视觉应用会议论文集，第2149–2158页。doi：[10.1109/WACV48630.2021.00220](https:/doi.org/10.1109/WACV48630.2021.00220)。
- en: 'Yin et al. (2016) Yin, X.C., Zuo, Z.Y., Tian, S., Liu, C.L., 2016. Text detection,
    tracking and recognition in video: A comprehensive survey. IEEE Transactions on
    Image Processing. 25, 2752–2773. doi:[10.1109/TIP.2016.2554321](https:/doi.org/10.1109/TIP.2016.2554321).'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin等（2016）Yin, X.C., Zuo, Z.Y., Tian, S., Liu, C.L., 2016. 视频中的文本检测、跟踪和识别：综合调查。IEEE图像处理学报，25，2752–2773。doi：[10.1109/TIP.2016.2554321](https:/doi.org/10.1109/TIP.2016.2554321)。
- en: 'Yoo et al. (2015) Yoo, D., Park, S., Lee, J.Y., Paek, A.S., Kweon, I.S., 2015.
    Attentionnet: Aggregating weak directions for accurate object detection, in: Proceedings
    of the IEEE International Conference on Computer Vision, pp. 2659–2667. doi:[10.1109/ICCV.2015.305](https:/doi.org/10.1109/ICCV.2015.305).'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yoo 等（2015）Yoo, D., Park, S., Lee, J.Y., Paek, A.S., Kweon, I.S., 2015. Attentionnet:
    聚合弱方向以实现准确的目标检测, 见：IEEE 国际计算机视觉会议论文集, 第2659–2667页。doi:[10.1109/ICCV.2015.305](https:/doi.org/10.1109/ICCV.2015.305)。'
- en: 'Yosinski et al. (2014) Yosinski, J., Clune, J., Bengio, Y., Lipson, H., 2014.
    How transferable are features in deep neural networks?, in: Proceedings of the
    27th International Conference on Neural Information Processing Systems - Volume
    2, p. 3320–3328. doi:[10.5555/2969033.2969197](https:/doi.org/10.5555/2969033.2969197).'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yosinski 等（2014）Yosinski, J., Clune, J., Bengio, Y., Lipson, H., 2014. 深度神经网络中的特征可迁移性如何？见：第27届国际神经信息处理系统大会
    - 卷2, 第3320–3328页。doi:[10.5555/2969033.2969197](https:/doi.org/10.5555/2969033.2969197)。
- en: Yu and Ji (2022) Yu, D., Ji, S., 2022. A new spatial-oriented object detection
    framework for remote sensing images. IEEE Transactions on Geoscience and Remote
    Sensing. 60, 1–16. doi:[10.1109/TGRS.2021.3127232](https:/doi.org/10.1109/TGRS.2021.3127232).
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 和 Ji（2022）Yu, D., Ji, S., 2022. 一种新的空间导向目标检测框架用于遥感图像。IEEE 地球科学与遥感学报。60, 1–16。doi:[10.1109/TGRS.2021.3127232](https:/doi.org/10.1109/TGRS.2021.3127232)。
- en: 'Yu and Da (2022) Yu, Y., Da, F., 2022. Phase-Shifting Coder: Predicting Accurate
    Orientation in Oriented Object Detection. arXiv e-prints. , arXiv:2211.06368.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 和 Da（2022）Yu, Y., Da, F., 2022. Phase-Shifting Coder：预测定向物体检测中的准确方向。arXiv
    预印本。arXiv:2211.06368。
- en: 'Zafeiriou et al. (2015) Zafeiriou, S., Zhang, C., Zhang, Z., 2015. A survey
    on face detection in the wild: Past, present and future. Computer Vision and Image
    Understanding. 138, 1–24. doi:[10.1016/j.cviu.2015.03.015](https:/doi.org/10.1016/j.cviu.2015.03.015).'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zafeiriou 等（2015）Zafeiriou, S., Zhang, C., Zhang, Z., 2015. 一项关于野外人脸检测的综述：过去、现在和未来。计算机视觉与图像理解。138,
    1–24。doi:[10.1016/j.cviu.2015.03.015](https:/doi.org/10.1016/j.cviu.2015.03.015)。
- en: 'Zhang et al. (2020a) Zhang, C., Yang, Z., He, X., Deng, L., 2020a. Multimodal
    intelligence: Representation learning, information fusion, and applications. IEEE
    Journal of Selected Topics in Signal Processing. 14, 478–493. doi:[10.1109/JSTSP.2020.2987728](https:/doi.org/10.1109/JSTSP.2020.2987728).'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020a）Zhang, C., Yang, Z., He, X., Deng, L., 2020a. 多模态智能：表示学习、信息融合与应用。IEEE
    选定信号处理主题杂志。14, 478–493。doi:[10.1109/JSTSP.2020.2987728](https:/doi.org/10.1109/JSTSP.2020.2987728)。
- en: 'Zhang et al. (2020b) Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., Sun,
    Q., 2020b. Feature pyramid transformer, in: Proceedings of the European Conference
    on Computer Vision, pp. 323–339. doi:[10.1007/978-3-030-58604-1_20](https:/doi.org/10.1007/978-3-030-58604-1_20).'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020b）Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., Sun, Q., 2020b.
    特征金字塔变换器, 见：欧洲计算机视觉会议论文集, 第323–339页。doi:[10.1007/978-3-030-58604-1_20](https:/doi.org/10.1007/978-3-030-58604-1_20)。
- en: Zhang et al. (2022) Zhang, F., Wang, X., Zhou, S., Wang, Y., Hou, Y., 2022.
    Arbitrary-oriented ship detection through center-head point extraction. IEEE Transactions
    on Geoscience and Remote Sensing. 60, 1–14. doi:[10.1109/TGRS.2021.3120411](https:/doi.org/10.1109/TGRS.2021.3120411).
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2022）Zhang, F., Wang, X., Zhou, S., Wang, Y., Hou, Y., 2022. 通过中心头点提取进行任意方向船舶检测。IEEE
    地球科学与遥感学报。60, 1–14。doi:[10.1109/TGRS.2021.3120411](https:/doi.org/10.1109/TGRS.2021.3120411)。
- en: 'Zhang et al. (2023) Zhang, Q., Xu, Y., Zhang, J., Tao, D., 2023. Vitaev2: Vision
    transformer advanced by exploring inductive bias for image recognition and beyond.
    International Journal of Computer Vision. , 1573–1405doi:[10.1007/s11263-022-01739-w](https:/doi.org/10.1007/s11263-022-01739-w).'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023）Zhang, Q., Xu, Y., Zhang, J., Tao, D., 2023. Vitaev2：通过探索归纳偏差来提升视觉变换器在图像识别及其他领域的应用。国际计算机视觉杂志。1573–1405。doi:[10.1007/s11263-022-01739-w](https:/doi.org/10.1007/s11263-022-01739-w)。
- en: 'Zhang et al. (2020c) Zhang, S., Chi, C., Yao, Y., Lei, Z., Li, S.Z., 2020c.
    Bridging the gap between anchor-based and anchor-free detection via adaptive training
    sample selection, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 9756–9765. doi:[10.1109/CVPR42600.2020.00978](https:/doi.org/10.1109/CVPR42600.2020.00978).'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020c）Zhang, S., Chi, C., Yao, Y., Lei, Z., Li, S.Z., 2020c. 通过自适应训练样本选择弥合基于锚点和无锚点检测之间的差距,
    见：IEEE/CVF 计算机视觉与模式识别会议论文集, 第9756–9765页。doi:[10.1109/CVPR42600.2020.00978](https:/doi.org/10.1109/CVPR42600.2020.00978)。
- en: Zhang et al. (2021a) Zhang, T., Zhang, X., Liu, C., Shi, J., Wei, S., Ahmad,
    I., Zhan, X., Zhou, Y., Pan, D., Li, J., Su, H., 2021a. Balance learning for ship
    detection from synthetic aperture radar remote sensing imagery. ISPRS Journal
    of Photogrammetry and Remote Sensing. 182, 190–207. doi:[10.1016/j.isprsjprs.2021.10.010](https:/doi.org/10.1016/j.isprsjprs.2021.10.010).
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2021a）Zhang, T., Zhang, X., Liu, C., Shi, J., Wei, S., Ahmad, I., Zhan,
    X., Zhou, Y., Pan, D., Li, J., Su, H., 2021a. 基于合成孔径雷达遥感图像的船只检测平衡学习。ISPRS 摄影测量与遥感学杂志。182,
    190–207. doi:[10.1016/j.isprsjprs.2021.10.010](https:/doi.org/10.1016/j.isprsjprs.2021.10.010)。
- en: Zhang et al. (2019) Zhang, Y., Yuan, Y., Feng, Y., Lu, X., 2019. Hierarchical
    and robust convolutional neural network for very high-resolution remote sensing
    object detection. IEEE Transactions on Geoscience and Remote Sensing. 57, 5535–5548.
    doi:[10.1109/TGRS.2019.2900302](https:/doi.org/10.1109/TGRS.2019.2900302).
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2019）Zhang, Y., Yuan, Y., Feng, Y., Lu, X., 2019. 用于超高分辨率遥感目标检测的分层且稳健的卷积神经网络。IEEE
    地球科学与遥感学汇刊。57, 5535–5548. doi:[10.1109/TGRS.2019.2900302](https:/doi.org/10.1109/TGRS.2019.2900302)。
- en: Zhang et al. (2018) Zhang, Z., Guo, W., Zhu, S., Yu, W., 2018. Toward arbitrary-oriented
    ship detection with rotated region proposal and discrimination networks. IEEE
    Geoscience and Remote Sensing Letters. 15, 1745–1749. doi:[10.1109/LGRS.2018.2856921](https:/doi.org/10.1109/LGRS.2018.2856921).
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2018）Zhang, Z., Guo, W., Zhu, S., Yu, W., 2018. 基于旋转区域提议和判别网络的任意方向船只检测。IEEE
    地球科学与遥感学快报。15, 1745–1749. doi:[10.1109/LGRS.2018.2856921](https:/doi.org/10.1109/LGRS.2018.2856921)。
- en: 'Zhang et al. (2021b) Zhang, Z., Zhang, L., Wang, Y., Feng, P., He, R., 2021b.
    Shiprsimagenet: A large-scale fine-grained dataset for ship detection in high-resolution
    optical remote sensing images. IEEE Journal of Selected Topics in Applied Earth
    Observations and Remote Sensing. 14, 8458–8472. doi:[10.1109/JSTARS.2021.3104230](https:/doi.org/10.1109/JSTARS.2021.3104230).'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2021b）Zhang, Z., Zhang, L., Wang, Y., Feng, P., He, R., 2021b. Shiprsimagenet：一个大规模的细粒度数据集，用于高分辨率光学遥感图像中的船只检测。IEEE
    应用地球观测与遥感学汇刊。14, 8458–8472. doi:[10.1109/JSTARS.2021.3104230](https:/doi.org/10.1109/JSTARS.2021.3104230)。
- en: Zhao et al. (2018) Zhao, F., Xia, L., Kylling, A., Li, R., Shang, H., Xu, M.,
    2018. Detection flying aircraft from landsat 8 oli data. ISPRS Journal of Photogrammetry
    and Remote Sensing. 141, 176–184. doi:[10.1016/j.isprsjprs.2018.05.001](https:/doi.org/10.1016/j.isprsjprs.2018.05.001).
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等（2018）Zhao, F., Xia, L., Kylling, A., Li, R., Shang, H., Xu, M., 2018.
    从 Landsat 8 OLI 数据中检测飞行器。ISPRS 摄影测量与遥感学杂志。141, 176–184. doi:[10.1016/j.isprsjprs.2018.05.001](https:/doi.org/10.1016/j.isprsjprs.2018.05.001)。
- en: 'Zhao et al. (2021) Zhao, P., Qu, Z., Bu, Y., Tan, W., Guan, Q., 2021. Polardet:
    a fast, more precise detector for rotated target in aerial images. International
    Journal of Remote Sensing. 42, 5831–5861. doi:[10.1080/01431161.2021.1931535](https:/doi.org/10.1080/01431161.2021.1931535).'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等（2021）Zhao, P., Qu, Z., Bu, Y., Tan, W., Guan, Q., 2021. Polardet：一种快速且更精确的旋转目标检测器，应用于航空图像。国际遥感学杂志。42,
    5831–5861. doi:[10.1080/01431161.2021.1931535](https:/doi.org/10.1080/01431161.2021.1931535)。
- en: 'Zhao et al. (2019a) Zhao, Q., Sheng, T., Wang, Y., Tang, Z., Chen, Y., Cai,
    L., Ling, H., 2019a. M2det: A single-shot object detector based on multi-level
    feature pyramid network, in: Proceedings of the Thirty-Third AAAI Conference on
    Artificial Intelligence, pp. 9259–9266. doi:[10.1609/aaai.v33i01.33019259](https:/doi.org/10.1609/aaai.v33i01.33019259).'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等（2019a）Zhao, Q., Sheng, T., Wang, Y., Tang, Z., Chen, Y., Cai, L., Ling,
    H., 2019a. M2det：一种基于多层特征金字塔网络的单次目标检测器，发表于：第三十三届 AAAI 人工智能会议论文集，pp. 9259–9266.
    doi:[10.1609/aaai.v33i01.33019259](https:/doi.org/10.1609/aaai.v33i01.33019259)。
- en: 'Zhao et al. (2019b) Zhao, Z.Q., Zheng, P., Xu, S.T., Wu, X., 2019b. Object
    detection with deep learning: A review. IEEE Transactions on Neural Networks and
    Learning Systems. 30, 3212–3232. doi:[10.1109/TNNLS.2018.2876865](https:/doi.org/10.1109/TNNLS.2018.2876865).'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等（2019b）Zhao, Z.Q., Zheng, P., Xu, S.T., Wu, X., 2019b. 基于深度学习的目标检测：综述。IEEE
    神经网络与学习系统汇刊。30, 3212–3232. doi:[10.1109/TNNLS.2018.2876865](https:/doi.org/10.1109/TNNLS.2018.2876865)。
- en: 'Zheng et al. (2020) Zheng, Z., Wang, P., Liu, W., Li, J., Ye, R., Ren, D.,
    2020. Distance-iou loss: Faster and better learning for bounding box regression,
    in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 12993–13000.
    doi:[10.1609/aaai.v34i07.6999](https:/doi.org/10.1609/aaai.v34i07.6999).'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2020）Zheng, Z., Wang, P., Liu, W., Li, J., Ye, R., Ren, D., 2020. Distance-iou
    损失：更快更好的边界框回归学习，发表于：AAAI 人工智能会议论文集，pp. 12993–13000. doi:[10.1609/aaai.v34i07.6999](https:/doi.org/10.1609/aaai.v34i07.6999)。
- en: Zhong and Ao (2020) Zhong, B., Ao, K., 2020. Single-stage rotation-decoupled
    detector for oriented object. Remote Sensing. 12. doi:[10.3390/rs12193262](https:/doi.org/10.3390/rs12193262).
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong and Ao (2020) Zhong, B., Ao, K., 2020. 单阶段旋转解耦检测器用于定向物体。遥感。12. doi:[10.3390/rs12193262](https:/doi.org/10.3390/rs12193262)。
- en: Zhou et al. (2020) Zhou, L., Wei, H., Li, H., Zhao, W., Zhang, Y., Zhang, Y.,
    2020. Arbitrary-oriented object detection in remote sensing images based on polar
    coordinates. IEEE Access. 8, 223373–223384. doi:[10.1109/ACCESS.2020.3041025](https:/doi.org/10.1109/ACCESS.2020.3041025).
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2020) Zhou, L., Wei, H., Li, H., Zhao, W., Zhang, Y., Zhang, Y.,
    2020. 基于极坐标的遥感图像中任意方向物体检测。IEEE Access. 8, 223373–223384. doi:[10.1109/ACCESS.2020.3041025](https:/doi.org/10.1109/ACCESS.2020.3041025)。
- en: 'Zhou et al. (2019) Zhou, X., Zhuo, J., Krähenbühl, P., 2019. Bottom-up object
    detection by grouping extreme and center points, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 850–859. doi:[10.1109/CVPR.2019.00094](https:/doi.org/10.1109/CVPR.2019.00094).'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2019) Zhou, X., Zhuo, J., Krähenbühl, P., 2019. 通过分组极端点和中心点进行自下而上的物体检测，见：IEEE/CVF计算机视觉与模式识别会议论文集，第850–859页。doi:[10.1109/CVPR.2019.00094](https:/doi.org/10.1109/CVPR.2019.00094)。
- en: 'Zhou et al. (2017) Zhou, Y., Ye, Q., Qiu, Q., Jiao, J., 2017. Oriented response
    networks, in: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 4961–4970. doi:[10.1109/CVPR.2017.527](https:/doi.org/10.1109/CVPR.2017.527).'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2017) Zhou, Y., Ye, Q., Qiu, Q., Jiao, J., 2017. 定向响应网络，见：IEEE计算机视觉与模式识别会议论文集，第4961–4970页。doi:[10.1109/CVPR.2017.527](https:/doi.org/10.1109/CVPR.2017.527)。
- en: 'Zhu et al. (2015) Zhu, H., Chen, X., Dai, W., Fu, K., Ye, Q., Jiao, J., 2015.
    Orientation robust object detection in aerial images using deep convolutional
    neural network, in: 2015 IEEE International Conference on Image Processing, pp.
    3735–3739. doi:[10.1109/ICIP.2015.7351502](https:/doi.org/10.1109/ICIP.2015.7351502).'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2015) Zhu, H., Chen, X., Dai, W., Fu, K., Ye, Q., Jiao, J., 2015.
    使用深度卷积神经网络在航空图像中进行方向鲁棒物体检测，见：2015年IEEE国际图像处理会议论文集，第3735–3739页。doi:[10.1109/ICIP.2015.7351502](https:/doi.org/10.1109/ICIP.2015.7351502)。
- en: 'Zhu and Liu (2018) Zhu, M., Liu, M., 2018. Mobile video object detection with
    temporally-aware feature maps, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 5686–5695. doi:[10.1109/CVPR.2018.00596](https:/doi.org/10.1109/CVPR.2018.00596).'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu and Liu (2018) Zhu, M., Liu, M., 2018. 使用时序感知特征图的移动视频物体检测，见：IEEE/CVF计算机视觉与模式识别会议论文集，第5686–5695页。doi:[10.1109/CVPR.2018.00596](https:/doi.org/10.1109/CVPR.2018.00596)。
- en: 'Zhu et al. (2019) Zhu, M., Pan, P., Chen, W., Yang, Y., 2019. Dm-gan: Dynamic
    memory generative adversarial networks for text-to-image synthesis, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5795–5803.
    doi:[10.1109/CVPR.2019.00595](https:/doi.org/10.1109/CVPR.2019.00595).'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2019) Zhu, M., Pan, P., Chen, W., Yang, Y., 2019. Dm-gan: 用于文本到图像合成的动态记忆生成对抗网络，见：IEEE/CVF计算机视觉与模式识别会议论文集，第5795–5803页。doi:[10.1109/CVPR.2019.00595](https:/doi.org/10.1109/CVPR.2019.00595)。'
- en: 'Zhu et al. (2021) Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J., 2021.
    Deformable detr: Deformable transformers for end-to-end object detection, in:
    Proceedings of the International Conference on Learning Representations.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2021) Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J., 2021.
    Deformable detr: 用于端到端物体检测的可变形变换器，见：国际学习表征会议论文集。'
- en: Zhu et al. (2016) Zhu, X., Vondrick, C., Fowlkes, C.C., Ramanan, D., 2016. Do
    we need more training data? International Journal of Computer Vision. 119, 76–92.
    doi:[10.1007/s11263-015-0812-2](https:/doi.org/10.1007/s11263-015-0812-2).
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2016) Zhu, X., Vondrick, C., Fowlkes, C.C., Ramanan, D., 2016. 我们需要更多的训练数据吗？计算机视觉国际期刊。119,
    76–92. doi:[10.1007/s11263-015-0812-2](https:/doi.org/10.1007/s11263-015-0812-2)。
- en: 'Zhu et al. (2017) Zhu, X., Xiong, Y., Dai, J., Yuan, L., Wei, Y., 2017. Deep
    feature flow for video recognition, in: Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pp. 4141–4150. doi:[10.1109/CVPR.2017.441](https:/doi.org/10.1109/CVPR.2017.441).'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2017) Zhu, X., Xiong, Y., Dai, J., Yuan, L., Wei, Y., 2017. 用于视频识别的深度特征流，见：IEEE计算机视觉与模式识别会议论文集，第4141–4150页。doi:[10.1109/CVPR.2017.441](https:/doi.org/10.1109/CVPR.2017.441)。
- en: 'Zou and Shi (2018) Zou, Z., Shi, Z., 2018. Random access memories: A new paradigm
    for target detection in high resolution aerial remote sensing images. IEEE Transactions
    on Image Processing. 27, 1100–1111. doi:[10.1109/TIP.2017.2773199](https:/doi.org/10.1109/TIP.2017.2773199).'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou and Shi (2018) Zou, Z., Shi, Z., 2018. 随机存取记忆：一种用于高分辨率航空遥感图像目标检测的新范式。IEEE图像处理汇刊。27,
    1100–1111. doi:[10.1109/TIP.2017.2773199](https:/doi.org/10.1109/TIP.2017.2773199)。
