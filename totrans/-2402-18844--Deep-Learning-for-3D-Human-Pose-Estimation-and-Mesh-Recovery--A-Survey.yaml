- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 中国广东深圳中山大学电子与通信工程学院
- en: 'date: 2024-09-06 19:34:15'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在几个公开可用的数据集上提供了比较结果，以及有见地的观察和鼓舞人心的未来研究方向。可以在[https://github.com/liuyangme/SOTA-3DHPE-HMR](https://github.com/liuyangme/SOTA-3DHPE-HMR)找到定期更新的项目页面。
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 人类是各种社会活动的主要参与者，使得AI具有对人类的理解对于社会的发展至关重要。因此，有许多以人类为中心的任务成为研究的焦点。其中，3D人体姿势估计（HPE）和人体网格恢复（HMR）代表了计算机视觉领域中的重要任务，用于解释人类在复杂的现实环境中的状态和行为。
- en: '[2402.18844] Deep Learning for 3D Human Pose Estimation and Mesh Recovery:
    A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3D人体姿势估计和网格恢复的深度学习：一项调查
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.18844](https://ar5iv.labs.arxiv.org/html/2402.18844)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 杨柳，邱长振，张志勇
- en: 'Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: Yang Liu Changzhen Qiu Zhiyong Zhang School of Electronics and Communication
    Engineering, Sun Yat-sen University, Shenzhen, Guangdong, China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了关于3D人体姿势估计的深度学习方法的全面覆盖，包括单人和多人方法，以及人体网格恢复，涵盖了基于显式模型和隐式表示的方法。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 人体姿势估计，3D人体姿势，人体网格恢复，人体重建，深度学习，文献调查^†^†期刊：模式识别
- en: 3D human pose estimation and mesh recovery have attracted widespread research
    interest in many areas, such as computer vision, autonomous driving, and robotics.
    Deep learning on 3D human pose estimation and mesh recovery has recently thrived,
    with numerous methods proposed to address different problems in this area. In
    this paper, to stimulate future research, we present a comprehensive review of
    recent progress over the past five years in deep learning methods for this area
    by delving into over 200 references. To the best of our knowledge, this survey
    is arguably the first to comprehensively cover deep learning methods for 3D human
    pose estimation, including both single-person and multi-person approaches, as
    well as human mesh recovery, encompassing methods based on explicit models and
    implicit representations. We also present comparative results on several publicly
    available datasets, together with insightful observations and inspiring future
    research directions. A regularly updated project page can be found at [https://github.com/liuyangme/SOTA-3DHPE-HMR](https://github.com/liuyangme/SOTA-3DHPE-HMR).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[3D人体姿势估计](https://ar5iv.labs.arxiv.org/html/2402.18844)和网格恢复已经在许多领域引起了广泛的研究兴趣，例如计算机视觉、自动驾驶和机器人技术。最近，关于3D人体姿势估计和网格恢复的深度学习蓬勃发展，提出了许多方法来解决这一领域的不同问题。在这篇论文中，为了激励未来的研究，我们通过深入研究超过200篇参考文献，对过去五年来深度学习方法在这一领域取得的最新进展进行了全面的回顾。'
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Human pose estimation, 3D human pose, human mesh recovery, human reconstruction,
    deep learning, literature survey^†^†journal: Pattern Recognition'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年9月6日19:34:15
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.18844](https://ar5iv.labs.arxiv.org/html/2402.18844)
- en: 1.1 Motivation
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[2402.18844] 3D人体姿势估计和网格恢复的深度学习：一项调查'
- en: Humans are the foremost actors in various social activities, and it is imperative
    to equip AI with an understanding of humans for contributing to society. Consequently,
    there are many human-centric tasks positioning the epicenter of research. Among
    them, 3D Human Pose Estimation (HPE) and Human Mesh Recovery (HMR) represent crucial
    tasks in the field of computer vision to interpret the status and behavior of
    humans in complex real-world environments.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要
- en: 3D human pose estimation can accurately predict human body keypoint coordinates
    in three-dimensional space. This approach provides more comprehensive and accurate
    spatial information when compared to its 2D counterpart, thereby facilitating
    a better understanding of complex human behaviors in higher-level computer vision
    applications [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]. Human mesh recovery
    reconstructs a three-dimensional digital model of the body, which captures details
    such as shape [[4](#bib.bib4), [5](#bib.bib5)], gestures [[6](#bib.bib6)], clothing
    [[7](#bib.bib7), [8](#bib.bib8)], and facial expressions [[9](#bib.bib9)], thus
    offering direct insights into human interactions with the physical world. 3D pose
    estimation and mesh recovery have a broad range of applications, such as security
    and surveillance [[10](#bib.bib10)], human-computer interaction [[11](#bib.bib11)],
    autonomous driving [[12](#bib.bib12), [13](#bib.bib13)], and virtual reality [[14](#bib.bib14)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 人体姿态估计可以准确预测三维空间中人体关键点的坐标。与其二维对应方法相比，这种方法提供了更全面和准确的空间信息，从而促进了对复杂人体行为的更好理解，在更高级的计算机视觉应用中具有重要意义
    [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]。人体网格恢复重建了一个三维数字模型，该模型捕捉了形状 [[4](#bib.bib4),
    [5](#bib.bib5)]、手势 [[6](#bib.bib6)]、服装 [[7](#bib.bib7), [8](#bib.bib8)] 和面部表情
    [[9](#bib.bib9)] 等细节，从而提供了对人类与物理世界互动的直接洞察。3D 姿态估计和网格恢复具有广泛的应用，如安全监控 [[10](#bib.bib10)]、人机交互
    [[11](#bib.bib11)]、自动驾驶 [[12](#bib.bib12), [13](#bib.bib13)] 和虚拟现实 [[14](#bib.bib14)]。
- en: With advanced deep learning technology, 3D human pose estimation and mesh recovery
    With advanced deep learning technology, 3D human pose estimation and mesh recovery
    have garnered increasing attention in recent years. 3D pose estimation has evolved
    from concentrating on single individuals to encompassing multiple persons with
    more varied data inputs. In human mesh recovery, advancements have been made in
    terms of data inputs and in capturing more intricate details. The augmentation
    in explicit model parameters allows for a more nuanced representation of the human
    body, and the expansion in parameter types facilitates the portrayal of finer
    surfaces. With the progress of implicit rendering and its incorporation into human
    mesh recovery, more flexible body representations are achieved. However, both
    3D pose estimation and mesh recovery face significant challenges, such as multi-person
    scenarios, self-occlusion issues, and the detailed reconstruction of bodies. Thus,
    a systematic and comprehensive review of recent advancements in 3D human pose
    estimation and mesh recovery is essential.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习技术的进步，3D 人体姿态估计和网格恢复近年来受到了越来越多的关注。3D 姿态估计已经从关注单个人体演变为涵盖多个个体，并且数据输入更加多样化。在人体网格恢复方面，数据输入和细节捕捉方面都有了进展。显式模型参数的增加允许对人体进行更细致的表示，而参数类型的扩展则促进了更细腻表面的表现。随着隐式渲染的进步及其在人体网格恢复中的应用，更多灵活的身体表示得以实现。然而，3D
    姿态估计和网格恢复仍面临显著挑战，如多人场景、自遮挡问题和详细的身体重建。因此，系统而全面地回顾 3D 人体姿态估计和网格恢复的最新进展是至关重要的。
- en: 1.2 Scope of this survey
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 本调查的范围
- en: Over the past five years, numerous reviews have been on 3D human pose estimation
    and mesh recovery. The reviews [[15](#bib.bib15), [16](#bib.bib16)] primarily
    concentrate on 2D and 3D pose estimation, yet they devote less attention to HMR-related
    literature. The survey [[17](#bib.bib17)] provides a thorough review exclusively
    dedicated to mesh recovery, focusing on methods based on explicit models. The
    survey [[18](#bib.bib18)] is distinctly centered on mesh recovery using implicit
    rendering techniques; however, it falls short in offering an exhaustive overview
    of the latest implicit rendering methods and systems for 3D pose estimation and
    mesh recovery. These reviews only marginally explore 3D pose estimation and mesh
    recovery technologies but scarcely delve into their applications in other computer
    vision tasks and future challenges.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的五年里，关于 3D 人体姿态估计和网格恢复的综述文章已经很多。这些综述 [[15](#bib.bib15), [16](#bib.bib16)]
    主要集中于二维和三维姿态估计，但对 HMR 相关文献关注较少。调查 [[17](#bib.bib17)] 提供了一份专门致力于网格恢复的详尽综述，重点关注基于显式模型的方法。调查
    [[18](#bib.bib18)] 明确集中于使用隐式渲染技术的网格恢复；然而，它未能提供关于最新隐式渲染方法和系统在 3D 姿态估计和网格恢复中的全面概述。这些综述仅边际地探讨了
    3D 姿态估计和网格恢复技术，却鲜少深入探讨它们在其他计算机视觉任务中的应用和未来挑战。
- en: '*This review primarily concentrates on deep learning approaches to 3D human
    pose estimation and human mesh recovery. 3D pose estimation, both in single-person
    and multi-person scenarios, is considered. In human mesh recovery, it methodically
    reviews techniques grounded in both explicit and implicit models. As illustrated
    in Fig.[1](#S1.F1 "Figure 1 ‣ 1.2 Scope of this survey ‣ 1 Introduction ‣ Deep
    Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"), our survey
    comprehensively includes the most recent state-of-the-art publications (2019-2023)
    from mainstream computer vision conferences and journals.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*本综述主要集中在3D人体姿态估计和人体网格恢复的深度学习方法上。考虑了单人和多人场景中的3D姿态估计。在人体网格恢复方面，系统地回顾了基于显式和隐式模型的技术。如图[1](#S1.F1
    "图 1 ‣ 1.2 本综述的范围 ‣ 1 介绍 ‣ 深度学习用于3D人体姿态估计和网格恢复：综述")所示，我们的综述全面涵盖了主流计算机视觉会议和期刊中的最新前沿出版物（2019-2023）。*'
- en: '![Refer to caption](img/fe0804e59ead1db91fb20c6a46c8ec0f.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fe0804e59ead1db91fb20c6a46c8ec0f.png)'
- en: 'Figure 1: Recent research of deep learning for 3D HPE and HMR.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 深度学习在3D人体姿态估计和网格恢复中的最新研究。'
- en: 'The main contributions of this work compared to the existing literature can
    be summarized as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的主要贡献与现有文献相比可以总结如下：
- en: (1) To the best of our knowledge, this survey is arguably the first to comprehensively
    cover deep learning methods for 3D human pose estimation, including both single-person
    and multi-person approaches, as well as human mesh recovery, encompassing methods
    based on explicit models and implicit representations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 据我们所知，本综述可能是首个全面覆盖3D人体姿态估计的深度学习方法的综述，包括单人和多人方法，以及人体网格恢复，涵盖了基于显式模型和隐式表示的方法。
- en: (2) Unlike existing reviews, we have not overlooked the role of implicit representations
    in the methods, particularly with the recent rapid advances in implicit rendering.
    This approach can produce detailed outputs, including clothed human figures with
    expressions, movements, and other intricacies essential for achieving photorealism.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 与现有综述不同，我们没有忽视隐式表示在方法中的作用，特别是在隐式渲染的快速进展下。这种方法可以生成详细的输出，包括具有表情、动作和其他细节的着装人物，这些都是实现逼真效果所必需的。
- en: (3) This paper comprehensively reviews the most recent developments in deep
    learning for 3D pose estimation and mesh recovery, providing readers with a detailed
    overview of cutting-edge methodologies. Additionally, it explores how these advancements
    contribute to various other computer vision tasks and delves into the challenges
    within this domain.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 本文全面回顾了3D姿态估计和网格恢复领域中深度学习的最新进展，为读者提供了前沿方法的详细概述。此外，本文探讨了这些进展如何贡献于其他计算机视觉任务，并深入研究了该领域中的挑战。
- en: 'The structure of this paper is as follows: Section [2](#S2 "2 Background ‣
    Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey") introduces
    the sensor type and representation for the human body that have been widely used.
    Section [3](#S3 "3 Overview of Deep Learning for 3D HPE and HMR ‣ Deep Learning
    for 3D Human Pose Estimation and Mesh Recovery: A Survey") presents the overview
    of deep learning for 3D human pose estimation and mesh recovery. Section [4](#S4
    "4 3D Human Pose Estimation ‣ Deep Learning for 3D Human Pose Estimation and Mesh
    Recovery: A Survey") surveys existing single person and multi-person 3D pose estimation
    methods. Section [5](#S5 "5 3D Human Mesh Recovery ‣ Deep Learning for 3D Human
    Pose Estimation and Mesh Recovery: A Survey") reviews the methods for human mesh
    recovery, including template-based and template-free methods. Section [6](#S6
    "6 Evaluation ‣ Deep Learning for 3D Human Pose Estimation and Mesh Recovery:
    A Survey") introduces the evaluation metrics and datasets for the respective tasks.
    Moreover, Section [7](#S7 "7 Applications ‣ Deep Learning for 3D Human Pose Estimation
    and Mesh Recovery: A Survey") discusses the applications, along with their impact
    on other computer vision tasks. Finally, Section [8](#S8 "8 Challenges and Conclusion
    ‣ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey") concludes
    the paper. Additionally, we host a regularly updated project page, which can be
    accessed at: [https://github.com/liuyangme/SOTA-3DHPE-HMR](https://github.com/liuyangme/SOTA-3DHPE-HMR).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的结构如下：第 [2](#S2 "2 Background ‣ Deep Learning for 3D Human Pose Estimation
    and Mesh Recovery: A Survey") 节介绍了广泛使用的人体传感器类型和表示方法。第 [3](#S3 "3 Overview of Deep
    Learning for 3D HPE and HMR ‣ Deep Learning for 3D Human Pose Estimation and Mesh
    Recovery: A Survey") 节概述了深度学习在 3D 人体姿态估计和网格恢复中的应用。第 [4](#S4 "4 3D Human Pose Estimation
    ‣ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey") 节调查了现有的单人和多人
    3D 姿态估计方法。第 [5](#S5 "5 3D Human Mesh Recovery ‣ Deep Learning for 3D Human Pose
    Estimation and Mesh Recovery: A Survey") 节回顾了人体网格恢复的方法，包括基于模板和无模板的方法。第 [6](#S6
    "6 Evaluation ‣ Deep Learning for 3D Human Pose Estimation and Mesh Recovery:
    A Survey") 节介绍了各自任务的评估指标和数据集。此外，第 [7](#S7 "7 Applications ‣ Deep Learning for
    3D Human Pose Estimation and Mesh Recovery: A Survey") 节讨论了应用及其对其他计算机视觉任务的影响。最后，第
    [8](#S8 "8 Challenges and Conclusion ‣ Deep Learning for 3D Human Pose Estimation
    and Mesh Recovery: A Survey") 节总结了论文。此外，我们还维护一个定期更新的项目页面，可通过以下链接访问：[https://github.com/liuyangme/SOTA-3DHPE-HMR](https://github.com/liuyangme/SOTA-3DHPE-HMR)。'
- en: 2 Background
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 Sensors used for 3D HPE and HMR
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 用于 3D HPE 和 HMR 的传感器
- en: There are a variety of sensors that can be used for 3D human pose estimation
    and mesh recovery, which are mainly categorized as active sensors and passive
    sensors.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用于 3D 人体姿态估计和网格恢复的传感器种类繁多，主要分为主动传感器和被动传感器。
- en: 2.1.1 Active sensors
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 主动传感器
- en: Active sensors operate by emitting a set of signals and measuring by detecting
    their reflections, such as Motion Capture (MoCap) systems with reflective markers
    [[19](#bib.bib19)], tactile sensing [[20](#bib.bib20)], Time of Flight (ToF) cameras
    [[21](#bib.bib21), [22](#bib.bib22)], and Radio Frequency (RF) technologies [[23](#bib.bib23),
    [24](#bib.bib24)]. MoCap and tactile devices are only suitable for cooperative
    targets. Active cameras generally cannot be used outdoors, and using multiple
    active devices simultaneously may lead to mutual interference. Thus, these devices
    have limited application scenarios.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 主动传感器通过发射一组信号并通过检测其反射来进行操作，例如带有反射标记的动作捕捉（MoCap）系统 [[19](#bib.bib19)]、触觉传感 [[20](#bib.bib20)]、飞行时间（ToF）相机
    [[21](#bib.bib21), [22](#bib.bib22)] 和射频（RF）技术 [[23](#bib.bib23), [24](#bib.bib24)]。MoCap
    和触觉设备仅适用于合作目标。主动相机通常不能在户外使用，并且同时使用多个主动设备可能会导致相互干扰。因此，这些设备的应用场景有限。
- en: 2.1.2 Passive sensors
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 被动传感器
- en: Passive sensors do not actively emit any signals during the measurement; instead,
    they rely on signals from the objects or natural sources, including Inertial Measurement
    Units (IMUs) [[25](#bib.bib25), [26](#bib.bib26)] and image sensors [[27](#bib.bib27),
    [28](#bib.bib28)]. Among these, RGB image sensors are particularly notable for
    being simple, user-friendly, adaptable to various environments, and capable of
    capturing high-resolution color images. Additionally, multiple RGB image sensors
    can be combined to form multi-view systems [[29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31)].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 被动传感器在测量过程中不会主动发射任何信号；相反，它们依赖于来自物体或自然来源的信号，包括惯性测量单元（IMUs）[[25](#bib.bib25),
    [26](#bib.bib26)] 和图像传感器 [[27](#bib.bib27), [28](#bib.bib28)]。其中，RGB 图像传感器特别值得注意，因为它们简单、易用、适应各种环境，并能够捕捉高分辨率的彩色图像。此外，多个
    RGB 图像传感器可以组合成多视图系统 [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)]。
- en: In this survey, considering the widespread applicability of RGB image sensors
    and the length limitations of the article, we focus on 3D human pose estimation
    and mesh recovery using RGB image sensors.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，考虑到 RGB 图像传感器的广泛适用性和文章的长度限制，我们专注于使用 RGB 图像传感器进行 3D 人体姿态估计和网格恢复。
- en: 2.2 Representation for human body
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 人体表示
- en: 3D pose estimators and mesh reconstructors can generate corresponding outcomes
    from the sensor input described previously. The 3D human pose estimation output
    comprises 3D coordinates detailing positions and joint orientations of the human
    body, representing the spatial location of each joint (e.g., head, neck, shoulders,
    elbows, knees), and the skeleton maps the interconnections between joints. Typically,
    these coordinates are expressed in a global coordinate system or relative to the
    camera’s coordinate system. A keypoints tree structure is commonly employed to
    illustrate the human pose, where the tree nodes represent the joints and the edges
    denote the connections between joints.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 姿态估计器和网格重建器可以从之前描述的传感器输入生成相应的结果。3D 人体姿态估计的输出包含详细描述人体位置和关节方向的 3D 坐标，表示每个关节（例如头部、颈部、肩膀、肘部、膝盖）的空间位置，骨架则映射了关节之间的连接。通常，这些坐标以全球坐标系统或相对于摄像头坐标系统的形式表示。常用的关键点树结构用来说明人体姿态，其中树的节点表示关节，边缘表示关节之间的连接。
- en: However, the skeleton-based human pose representation method does not provide
    detailed information about the body’s surface. The output of human mesh recovery
    is generally a 3D body model, which encompasses a comprehensive depiction of the
    body’s shape and surface details and offers a more enriched representation. Statistical-based
    models are widely used in human mesh representations, such as SCAPE [[32](#bib.bib32)]
    and SMPL [[33](#bib.bib33)]. SMPL is a learnable skin-vertex model representing
    the human body as a 3D mesh with a topological structure. The pose and shape of
    the body are described by pose parameters $\theta$ and shape parameters $\beta$.
    The pose parameters control the joint angles and global posture, and the shape
    parameters determine the body’s shape. There are several models based on SMPL
    to expand the representational capabilities, such as the MANO model (SMPL+H) [[34](#bib.bib34)]
    with hand representation and the FLAME model [[35](#bib.bib35)] with facial representation.
    SMPL-X [[36](#bib.bib36)] is a comprehensive model that simultaneously captures
    the human body, face, and hands by incorporating the FLAME head model [[35](#bib.bib35)]
    and the MANO hand model [[34](#bib.bib34)]. H4D [[37](#bib.bib37)] can represent
    the dynamic human shape and pose by building upon the prior knowledge from the
    SMPL model and extending its capabilities by incorporating a temporal dimension.
    In addition to the explicit model representations mentioned above, recent years
    have seen the development of methods based on implicit models, which offer a more
    flexible representation of the human body through non-parametric models. For example,
    to address the limitations of previous representation models, Zheng et al. [[38](#bib.bib38)]
    designed the Parametric Model-Conditioned Implicit Representation (PaMIR), which
    utilizes the 2D image feature map and corresponding SMPL feature volume to generate
    an implicit surface representation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基于骨架的人体姿态表示方法并未提供身体表面的详细信息。人体网格恢复的输出通常是一个3D身体模型，它包括对身体形状和表面细节的全面描述，提供了更丰富的表示。统计模型广泛应用于人体网格表示，例如SCAPE
    [[32](#bib.bib32)] 和SMPL [[33](#bib.bib33)]。SMPL是一个可学习的皮肤顶点模型，将人体表示为具有拓扑结构的3D网格。身体的姿态和形状由姿态参数$\theta$和形状参数$\beta$描述。姿态参数控制关节角度和全局姿势，形状参数决定身体的形状。有几个基于SMPL的模型扩展了表示能力，例如带有手部表示的MANO模型（SMPL+H）[[34](#bib.bib34)]
    和带有面部表示的FLAME模型[[35](#bib.bib35)]。SMPL-X [[36](#bib.bib36)] 是一个综合模型，通过结合FLAME头部模型[[35](#bib.bib35)]和MANO手部模型[[34](#bib.bib34)]，同时捕捉人体、面部和手部。H4D
    [[37](#bib.bib37)] 通过在SMPL模型的基础上建立动态人体形状和姿态，扩展了其能力，加入了时间维度。除了上述显式模型表示，近年来还出现了基于隐式模型的方法，通过非参数模型提供了更灵活的人体表示。例如，为了应对以前表示模型的局限性，郑等人[[38](#bib.bib38)]
    设计了参数模型条件隐式表示（PaMIR），它利用2D图像特征图和相应的SMPL特征体积生成隐式表面表示。
- en: 3 Overview of Deep Learning for 3D HPE and HMR
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习在3D HPE和HMR中的概述
- en: '![Refer to caption](img/5407ad53dbe39043c904c74d4aea94e2.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5407ad53dbe39043c904c74d4aea94e2.png)'
- en: 'Figure 2: A basic framework of deep learning for 3D HPE and HMR.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：用于3D HPE和HMR的深度学习基本框架。
- en: 'This section provides an overview of the deep learning framework for 3D human
    pose estimation and mesh recovery. As shown in Fig.[2](#S3.F2 "Figure 2 ‣ 3 Overview
    of Deep Learning for 3D HPE and HMR ‣ Deep Learning for 3D Human Pose Estimation
    and Mesh Recovery: A Survey"), there are four components in deep learning-based
    systems. First, data are collected from various sensors, including RGB images,
    depth images, RF signals, IMUs, and so on. Second, the input data are processed
    through a deep learning model, which typically consists of an encoder and a decoder.
    The encoder extracts representational features from the input data, such as those
    obtained using architectures like ResNet [[39](#bib.bib39)] or HRNet [[40](#bib.bib40)].
    The decoder, which could be based on frameworks like MLP (Multi-Layer Perceptron)
    [[41](#bib.bib41)] or Transformer [[42](#bib.bib42)], then outputs the body’s
    3D pose and reconstruction model. This structure enables the model to efficiently
    process and translate complex input data into detailed and accurate 3D representations
    of human pose and mesh. Thirdly, various learning methods can be selected during
    the model learning process. In addition to fully supervised learning, to alleviate
    data dependency approaches such as weakly supervised learning [[43](#bib.bib43),
    [44](#bib.bib44)], unsupervised learning [[45](#bib.bib45), [46](#bib.bib46)],
    and few-shot learning [[47](#bib.bib47), [48](#bib.bib48)] are employed. To reduce
    the model size, techniques such as knowledge distillation [[49](#bib.bib49), [50](#bib.bib50)],
    model pruning [[51](#bib.bib51)], and parameter quantization [[52](#bib.bib52)]
    can be applied. Furthermore, methodologies such as meta-learning [[53](#bib.bib53)]
    and reinforcement learning [[54](#bib.bib54)] can also be incorporated, allowing
    the model to adapt to different scenarios and data constraints. Finally, the deep
    learning model outputs the results of 3D human pose estimation and mesh recovery.
    These results can be represented in various forms, including keypoints [[55](#bib.bib55),
    [56](#bib.bib56)], mesh [[33](#bib.bib33), [34](#bib.bib34), [57](#bib.bib57)],
    and voxels [[58](#bib.bib58), [59](#bib.bib59), [8](#bib.bib8)]. These representations
    contribute to a comprehensive understanding of the human body in three dimensions.
    In the following sections, we will delve into the specifics of 3D human pose estimation
    and mesh recovery, categorizing and elaborating on each aspect.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '本节提供了用于3D人体姿态估计和网格恢复的深度学习框架的概述。如图[2](#S3.F2 "Figure 2 ‣ 3 Overview of Deep
    Learning for 3D HPE and HMR ‣ Deep Learning for 3D Human Pose Estimation and Mesh
    Recovery: A Survey")所示，基于深度学习的系统包括四个组件。首先，从各种传感器（如RGB图像、深度图像、RF信号、IMU等）收集数据。其次，输入数据通过深度学习模型进行处理，该模型通常由编码器和解码器组成。编码器从输入数据中提取表征特征，例如使用ResNet
    [[39](#bib.bib39)]或HRNet [[40](#bib.bib40)]等架构获得的特征。解码器可以基于MLP（多层感知机）[[41](#bib.bib41)]或Transformer
    [[42](#bib.bib42)]等框架，然后输出身体的3D姿态和重建模型。这种结构使模型能够高效地处理和转换复杂的输入数据，生成详细且准确的人体姿态和网格的3D表示。第三，在模型学习过程中可以选择各种学习方法。除了完全监督学习，还可以采用弱监督学习
    [[43](#bib.bib43), [44](#bib.bib44)]、无监督学习 [[45](#bib.bib45), [46](#bib.bib46)]
    和少样本学习 [[47](#bib.bib47), [48](#bib.bib48)] 等方法，以减轻数据依赖。为了减少模型大小，可以应用知识蒸馏 [[49](#bib.bib49),
    [50](#bib.bib50)]、模型剪枝 [[51](#bib.bib51)] 和参数量化 [[52](#bib.bib52)] 等技术。此外，还可以结合元学习
    [[53](#bib.bib53)] 和强化学习 [[54](#bib.bib54)] 等方法，使模型能够适应不同的场景和数据约束。最后，深度学习模型输出3D人体姿态估计和网格恢复的结果。这些结果可以以多种形式表示，包括关键点
    [[55](#bib.bib55), [56](#bib.bib56)]、网格 [[33](#bib.bib33), [34](#bib.bib34), [57](#bib.bib57)]
    和体素 [[58](#bib.bib58), [59](#bib.bib59), [8](#bib.bib8)]。这些表示有助于全面理解人体的三维结构。在接下来的部分中，我们将深入探讨3D人体姿态估计和网格恢复的具体细节，对每个方面进行分类和详细阐述。'
- en: 4 3D Human Pose Estimation
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 3D人体姿态估计
- en: '3D human pose estimation can provide a more accurate pose by predicting the
    depth information of body keypoints, but it is much more challenging than 2D pose
    estimation. 3D pose estimation can be classified into single-person and multi-person
    estimation, according to the number of targets. Single-person 3D pose estimation
    has seen rapid progress due to the fast development of deep learning technology,
    but it still faces many challenges, such as efficiency and the invisibility of
    certain body parts. Multi-person estimation in crowded scenes is even more challenging
    because of the interaction and occlusion between bodies and scenes. In this section,
    we will detail the research progress in this field, categorizing it into two types.
    Table [1](#S4.T1 "Table 1 ‣ 4 3D Human Pose Estimation ‣ Deep Learning for 3D
    Human Pose Estimation and Mesh Recovery: A Survey") summarizes all the representative
    methods.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '3D 人体姿势估计通过预测身体关键点的深度信息提供更准确的姿势，但比 2D 姿势估计要具挑战性得多。3D 姿势估计可以根据目标数量分为单人估计和多人估计。由于深度学习技术的快速发展，单人
    3D 姿势估计取得了迅速进展，但仍面临许多挑战，如效率和某些身体部位的不可见性。多人估计在拥挤场景中更具挑战性，因为存在身体和场景之间的互动和遮挡。在本节中，我们将详细探讨这一领域的研究进展，并将其分为两类。表
    [1](#S4.T1 "Table 1 ‣ 4 3D Human Pose Estimation ‣ Deep Learning for 3D Human
    Pose Estimation and Mesh Recovery: A Survey") 总结了所有具有代表性的方法。'
- en: 'Table 1: Overview of 3D human pose estimation.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：3D 人体姿势估计概述。
- en: '| Motivations | Methods |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 动机 | 方法 |'
- en: '| Single Person | in Images | Solving Depth Ambiguity | $\bullet$ Optical-aware:
    VI-HC [[60](#bib.bib60)], Ray3D [[61](#bib.bib61)] |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 单人 | 在图像中 | 解决深度模糊 | $\bullet$ 光学感知：VI-HC [[60](#bib.bib60)], Ray3D [[61](#bib.bib61)]
    |'
- en: '| $\bullet$ Appropriate feature representation: HEMlets [[62](#bib.bib62)]
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 适当的特征表示：HEMlets [[62](#bib.bib62)] |'
- en: '| Solving Body Structure Understanding | $\bullet$ Joint aware: JRAN [[63](#bib.bib63)]
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 解决身体结构理解 | $\bullet$ 关节感知：JRAN [[63](#bib.bib63)] |'
- en: '| $\bullet$ Limb aware: Wu et al. [[64](#bib.bib64)], Deep grammar network
    [[65](#bib.bib65)] |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 四肢感知：Wu 等 [[64](#bib.bib64)], 深度语法网络 [[65](#bib.bib65)] |'
- en: '| $\bullet$ Orientation keypoints: Fisch et al. [[66](#bib.bib66)] |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 方向关键点：Fisch 等 [[66](#bib.bib66)] |'
- en: '| $\bullet$ Graph-based: Liu et al. [[67](#bib.bib67)], LCN [[68](#bib.bib68)],
    Modulated-GCN [[69](#bib.bib69)], Skeletal-GNN [[70](#bib.bib70)], HopFIR [[71](#bib.bib71)],
    RS-Net [[55](#bib.bib55)] |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 基于图的：Liu 等 [[67](#bib.bib67)], LCN [[68](#bib.bib68)], Modulated-GCN
    [[69](#bib.bib69)], Skeletal-GNN [[70](#bib.bib70)], HopFIR [[71](#bib.bib71)],
    RS-Net [[55](#bib.bib55)] |'
- en: '| Solving Occlusion Problems | $\bullet$ Learnable-triangulation [[72](#bib.bib72)]
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 解决遮挡问题 | $\bullet$ 可学习三角测量 [[72](#bib.bib72)] |'
- en: '| $\bullet$ RPSM [[73](#bib.bib73)] |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ RPSM [[73](#bib.bib73)] |'
- en: '| $\bullet$ Lightweight multi-view [[74](#bib.bib74)] |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 轻量级多视角 [[74](#bib.bib74)] |'
- en: '| $\bullet$ AdaFuse [[75](#bib.bib75)] |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ AdaFuse [[75](#bib.bib75)] |'
- en: '| $\bullet$ Bartol et al. [[76](#bib.bib76)] |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ Bartol 等 [[76](#bib.bib76)] |'
- en: '| $\bullet$ 3D pose consensus [[77](#bib.bib77)] |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 3D 姿势一致性 [[77](#bib.bib77)] |'
- en: '| $\bullet$ Probabilistic triangulation module [[29](#bib.bib29)] |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 概率三角测量模块 [[29](#bib.bib29)] |'
- en: '| Solving Data Lacking | $\bullet$ Unsupervised learning: Kudo et al. [[78](#bib.bib78)],
    Chen et al. [[79](#bib.bib79)], ElePose [[80](#bib.bib80)] |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 解决数据缺乏 | $\bullet$ 无监督学习：Kudo 等 [[78](#bib.bib78)], Chen 等 [[79](#bib.bib79)],
    ElePose [[80](#bib.bib80)] |'
- en: '| $\bullet$ Self-supervised learning: EpipolarPose [[81](#bib.bib81)], Wang
    et al. [[82](#bib.bib82)], MRP-Net [[83](#bib.bib83)], PoseTriplet [[54](#bib.bib54)]
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 自监督学习：EpipolarPose [[81](#bib.bib81)], Wang 等 [[82](#bib.bib82)],
    MRP-Net [[83](#bib.bib83)], PoseTriplet [[54](#bib.bib54)] |'
- en: '| $\bullet$ Weakly-supervised learning: Hua et al. [[84](#bib.bib84)], CameraPose
    [[43](#bib.bib43)] |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 弱监督学习：Hua 等 [[84](#bib.bib84)], CameraPose [[43](#bib.bib43)] |'
- en: '| $\bullet$ Transfer learning: Adaptpose [[85](#bib.bib85)] |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 迁移学习：Adaptpose [[85](#bib.bib85)] |'
- en: '| in Videos | Solving Single-frame Limitation | $\bullet$ VideoPose3D [[86](#bib.bib86)]
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 在视频中 | 解决单帧限制 | $\bullet$ VideoPose3D [[86](#bib.bib86)] |'
- en: '| $\bullet$ PoseFormer [[87](#bib.bib87)] |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ PoseFormer [[87](#bib.bib87)] |'
- en: '| $\bullet$ UniPose+ [[88](#bib.bib88)] |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ UniPose+ [[88](#bib.bib88)] |'
- en: '| $\bullet$ MHFormer [[89](#bib.bib89)] |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ MHFormer [[89](#bib.bib89)] |'
- en: '| $\bullet$ MixSTE [[90](#bib.bib90)] |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ MixSTE [[90](#bib.bib90)] |'
- en: '| $\bullet$ Honari et al. [[91](#bib.bib91)] |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ Honari 等 [[91](#bib.bib91)] |'
- en: '| $\bullet$ HSTFormer [[92](#bib.bib92)] |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ HSTFormer [[92](#bib.bib92)] |'
- en: '| $\bullet$ STCFormer [[93](#bib.bib93)] |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ STCFormer [[93](#bib.bib93)] |'
- en: '| Solving Real-time Problems | $\bullet$ Temporally sparse sampling: Einfalt
    et al. [[19](#bib.bib19)] |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 解决实时问题 | $\bullet$ 时间稀疏采样：Einfalt 等人 [[19](#bib.bib19)] |'
- en: '| $\bullet$ Spatio-temporal sparse sampling: MixSynthFormer [[94](#bib.bib94)]
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 时空稀疏采样：MixSynthFormer [[94](#bib.bib94)] |'
- en: '| Solving Body Structure Understanding | $\bullet$ Motion loss: Wang et al.
    [[95](#bib.bib95)] |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 解决身体结构理解 | $\bullet$ 运动损失：Wang 等人 [[95](#bib.bib95)] |'
- en: '| $\bullet$ Human-joint affinity: DG-Net [[96](#bib.bib96)] |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 人体关节亲和性：DG-Net [[96](#bib.bib96)] |'
- en: '| $\bullet$ Anatomy-aware: Chen et al. [[97](#bib.bib97)] |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 解剖学感知：Chen 等人 [[97](#bib.bib97)] |'
- en: '| $\bullet$ Part aware attention: Xue et al. [[98](#bib.bib98)] |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 部分感知注意力：Xue 等人 [[98](#bib.bib98)] |'
- en: '| Solving Occlusion Problems | $\bullet$ Optical-flow consistency constraint:
    Cheng et al. [[99](#bib.bib99)] |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 解决遮挡问题 | $\bullet$ 光流一致性约束：Cheng 等人 [[99](#bib.bib99)] |'
- en: '| $\bullet$ Multi-view: MTF-Transformer [[30](#bib.bib30)] |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 多视角：MTF-Transformer [[30](#bib.bib30)] |'
- en: '| Solving Data Lacking | $\bullet$ Unsupervised learning: Yu et al. [[100](#bib.bib100)]
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 解决数据缺乏 | $\bullet$ 无监督学习：Yu 等人 [[100](#bib.bib100)] |'
- en: '| $\bullet$ Weakly-supervised learning: Chen et al. [[101](#bib.bib101)] |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 弱监督学习：Chen 等人 [[101](#bib.bib101)] |'
- en: '| $\bullet$ Semi-supervised learning: MCSS [[102](#bib.bib102)] |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 半监督学习：MCSS [[102](#bib.bib102)] |'
- en: '| $\bullet$ Self-supervised learning: Kundu et al. [[103](#bib.bib103)], P-STMO
    [[104](#bib.bib104)] |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 自监督学习：Kundu 等人 [[103](#bib.bib103)], P-STMO [[104](#bib.bib104)]
    |'
- en: '| $\bullet$ Meta-learning: Cho et al. [[53](#bib.bib53)] |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 元学习：Cho 等人 [[53](#bib.bib53)] |'
- en: '| $\bullet$ Data augmentation: PoseAug [[105](#bib.bib105)], Zhang et al. [[106](#bib.bib106)]
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 数据增强：PoseAug [[105](#bib.bib105)], Zhang 等人 [[106](#bib.bib106)]
    |'
- en: '| Multi-person | Top-down | Solving Real-time Problems | $\bullet$ Multi-view:
    Chen et al. [[107](#bib.bib107)] |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 多人 | 自上而下 | 解决实时问题 | $\bullet$ 多视角：Chen 等人 [[107](#bib.bib107)] |'
- en: '| $\bullet$ Whole body: AlphaPose [[108](#bib.bib108)] |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 全身：AlphaPose [[108](#bib.bib108)] |'
- en: '| Solving Representation Limitation | $\bullet$ VoxelTrack [[2](#bib.bib2)]
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 解决表示限制 | $\bullet$ VoxelTrack [[2](#bib.bib2)] |'
- en: '| Solving Occlusion Problems | $\bullet$ Wu et al. [[109](#bib.bib109)] |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 解决遮挡问题 | $\bullet$ Wu 等人 [[109](#bib.bib109)] |'
- en: '| Solving Data Lacking | $\bullet$ Single-shot: PandaNet [[48](#bib.bib48)]
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 解决数据缺乏 | $\bullet$ 单次拍摄：PandaNet [[48](#bib.bib48)] |'
- en: '| $\bullet$ Optical-aware: Moon et al. [[110](#bib.bib110)] |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 光学感知：Moon 等人 [[110](#bib.bib110)] |'
- en: '| Bottom-up | Solving Real-time Problems | $\bullet$ Fabbri et al. [[111](#bib.bib111)]
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 自下而上 | 解决实时问题 | $\bullet$ Fabbri 等人 [[111](#bib.bib111)] |'
- en: '| Solving Supervisory Limitation | $\bullet$ HMOR [[112](#bib.bib112)] |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 解决监督限制 | $\bullet$ HMOR [[112](#bib.bib112)] |'
- en: '| Solving Data Lacking | $\bullet$ Single-shot: SMAP [[113](#bib.bib113)],
    Benzine et al. [[114](#bib.bib114)] |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 解决数据缺乏 | $\bullet$ 单次拍摄：SMAP [[113](#bib.bib113)], Benzine 等人 [[114](#bib.bib114)]
    |'
- en: '| Solving Occlusion Problems | $\bullet$ Mehta et al. [[115](#bib.bib115)]
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 解决遮挡问题 | $\bullet$ Mehta 等人 [[115](#bib.bib115)] |'
- en: '| $\bullet$ LCR-Net++ [[116](#bib.bib116)] |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ LCR-Net++ [[116](#bib.bib116)] |'
- en: '| Others | Single Stage | $\bullet$ Jin et al. [[117](#bib.bib117)] |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | 单阶段 | $\bullet$ Jin 等人 [[117](#bib.bib117)] |'
- en: '| Top-down + Bottom-up | $\bullet$ Cheng et al. [[118](#bib.bib118)] |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 自上而下 + 自下而上 | $\bullet$ Cheng 等人 [[118](#bib.bib118)] |'
- en: 4.1 Single person 3D pose estimation
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 单人 3D 姿态估计
- en: 'As illustrated in Fig.[3](#S4.F3 "Figure 3 ‣ 4.1 Single person 3D pose estimation
    ‣ 4 3D Human Pose Estimation ‣ Deep Learning for 3D Human Pose Estimation and
    Mesh Recovery: A Survey"), single person 3D pose estimation is mainly classified
    into the direct estimation method and the 2D to 3D lifting method. The direct
    method estimates the 3D human pose directly from the input using a predictor,
    and the 2D to 3D lifting method, which estimates the 3D pose from the results
    of 2D estimation, involves first detecting the coordinates of human keypoints
    in 2D space, and then lifting these 2D keypoints onto 3D space coordinates.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [3](#S4.F3 "图 3 ‣ 4.1 单人 3D 姿态估计 ‣ 4 3D 人体姿态估计 ‣ 基于深度学习的 3D 人体姿态估计与网格恢复：综述")
    所示，单人 3D 姿态估计主要分为直接估计方法和 2D 到 3D 提升方法。直接方法通过预测器直接从输入中估计 3D 人体姿态，而 2D 到 3D 提升方法则首先在
    2D 空间中检测人体关键点的坐标，然后将这些 2D 关键点提升到 3D 空间坐标。
- en: '![Refer to caption](img/7962b2f00cd2f09589496685e725412e.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/7962b2f00cd2f09589496685e725412e.png)'
- en: (a) The direct estimation method
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 直接估计方法
- en: '![Refer to caption](img/30dfbc63ad35353b71d6ae7ce2ad922e.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/30dfbc63ad35353b71d6ae7ce2ad922e.png)'
- en: (b) The 2D to 3D lifting method
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 2D 到 3D 提升方法
- en: 'Figure 3: Typical single person 3D human pose estimation. (a) The direct estimation
    method; (b) The 2D to 3D lifting method.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 典型的单人3D人体姿态估计。 (a) 直接估计方法; (b) 2D到3D提升方法。'
- en: 4.1.1 Single person 3D pose estimation in images
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 单人图像中的3D姿态估计
- en: 'Solving Depth Ambiguity. As shown in Fig.[4(a)](#S4.F4.sf1 "In Figure 4 ‣ 4.1.1
    Single person 3D pose estimation in images ‣ 4.1 Single person 3D pose estimation
    ‣ 4 3D Human Pose Estimation ‣ Deep Learning for 3D Human Pose Estimation and
    Mesh Recovery: A Survey"), different 3D pose coordinates projecting to 2D images
    may give the same results, leading to an ill-posed problem. This problem can be
    solved by using the propagation properties of light and camera imaging principle.
    To address the ill-posed problem, Wei et al. [[60](#bib.bib60)] introduced a view-invariant
    framework to moderate the effects of viewpoint diversity. A View-Invariant Hierarchical
    Correction (VI-HC) network predicts the 3D pose refinement with view-consistent
    constraints in the proposed framework. Additionally, a view-invariant discriminative
    network actualizes high-level constraints after the base network generates an
    initial estimation. Ray-based 3D (Ray3D) absolute estimation method [[61](#bib.bib61)]
    can convert the input from pixel space to 3D normalized rays. Another helpful
    approach for this problem is learning a more appropriate feature representation.
    Part-centric HEatMap triplets (HEMlets) framework [[62](#bib.bib62)] utilizes
    three joint-heatmaps to represent the end-joints relative depth information, which
    bridges the gap between the 2D location and the 3D human pose.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 解决深度歧义。如图[4(a)](#S4.F4.sf1 "图 4 ‣ 4.1.1 单人图像中的3D姿态估计 ‣ 4.1 单人3D姿态估计 ‣ 4 3D人体姿态估计
    ‣ 深度学习在3D人体姿态估计和网格恢复中的应用：综述")所示，不同的3D姿态坐标投影到2D图像上可能会给出相同的结果，导致问题不适定。这个问题可以通过利用光的传播特性和相机成像原理来解决。为了解决这个不适定问题，Wei等人[[60](#bib.bib60)]提出了一个视角不变的框架，以调节视点多样性的影响。视角不变的层次校正（VI-HC）网络在提出的框架中预测3D姿态的细化，并具有视角一致性约束。此外，一个视角不变的判别网络在基础网络生成初步估计后实现了高级约束。基于光线的3D（Ray3D）绝对估计方法[[61](#bib.bib61)]可以将输入从像素空间转换为3D归一化光线。另一个有助于解决此问题的方法是学习更合适的特征表示。以部分为中心的HEatMap三重体（HEMlets）框架[[62](#bib.bib62)]利用三个关节热图来表示末端关节的相对深度信息，这填补了2D位置与3D人体姿态之间的差距。
- en: '![Refer to caption](img/2a0770baa57bd7ffebfb653a8868d201.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2a0770baa57bd7ffebfb653a8868d201.png)'
- en: (a) Depth ambiguity
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 深度歧义
- en: '![Refer to caption](img/06f8f73f99c9c0abd2c34f4701598b55.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/06f8f73f99c9c0abd2c34f4701598b55.png)'
- en: (b) Graph-based representation
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 基于图的表示
- en: '![Refer to caption](img/a7e11be814d4dac392cdfcfc15cb0a9a.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a7e11be814d4dac392cdfcfc15cb0a9a.png)'
- en: (c) Transfer learning
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 转移学习
- en: 'Figure 4: (a) Depth ambiguity; (b) Graph-based representation for human body;
    (c) Transfer learning.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: (a) 深度歧义; (b) 人体的基于图的表示; (c) 转移学习。'
- en: Solving Body Structure Understanding. Unlike other computer vision tasks, the
    body’s unique structures can provide constraints or prior information to improve
    pose estimation performance. In the Joint Relationship Aware Network (JRAN) [[63](#bib.bib63)],
    a dual attention module was designed to generate both whole and local feature
    attention block weights. The limb poses aware network [[64](#bib.bib64)] leverages
    kinematic constraint and trajectory information to prevent errors from accumulating
    along the human body structure. Pose grammar in [[65](#bib.bib65)] learns a 2D-3D
    mapping function, enabling the input 2D pose to be transformed into 3D space,
    and integrates three aspects of human structure (kinematics, symmetry, motor coordination)
    into a Bi-directional Recurrent Neural Network (RNN). The orientation keypoints-based
    method [[66](#bib.bib66)] uses virtual markers to generate sufficient information
    for accurately inferring rotations through simple post-processing.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 解决身体结构理解问题。与其他计算机视觉任务不同，身体的独特结构可以提供约束或先验信息，以提高姿态估计性能。在关节关系感知网络（JRAN）[[63](#bib.bib63)]中，设计了一个双重注意模块，以生成整体和局部特征注意块权重。肢体姿态感知网络[[64](#bib.bib64)]利用运动约束和轨迹信息来防止错误沿人体结构积累。姿态语法在[[65](#bib.bib65)]中学习了一个2D-3D映射函数，使输入的2D姿态可以转换到3D空间，并将人体结构的三个方面（运动学、对称性、运动协调性）整合到双向递归神经网络（RNN）中。基于方向关键点的方法[[66](#bib.bib66)]使用虚拟标记生成足够的信息，以通过简单的后处理准确推断旋转。
- en: 'The Graph Neural Network (GNN) is a convolutional network defined on the graph
    data structure. It is difficult for typical neural networks to handle graph-structured
    data, such as body structure information. However, this challenge can be more
    easily addressed if graph-based learning methods are used, as shown in Fig.[4(b)](#S4.F4.sf2
    "In Figure 4 ‣ 4.1.1 Single person 3D pose estimation in images ‣ 4.1 Single person
    3D pose estimation ‣ 4 3D Human Pose Estimation ‣ Deep Learning for 3D Human Pose
    Estimation and Mesh Recovery: A Survey"). Liu et al. [[67](#bib.bib67)] proposed
    a feature boosting network in which features are learned by the convolutional
    layers and are boosted with Graphical ConvLSTM to perceive the graphical long
    short-term dependency among different body parts. The Locally Connected Network
    (LCN) [[68](#bib.bib68)] leverages the allocation of dedicated filters rather
    than sharing them for different joints. Modulated Graph Convolutional Network
    (Modulated-GCN) [[69](#bib.bib69)] includes weight and affinity modulation to
    learn modulation vectors for different body joints and to adjust the graph structure,
    respectively. Zeng et al. [[70](#bib.bib70)] designed a skeletal GNN learning
    framework to address the depth ambiguity and self-occlusion problems in 3D human
    pose estimation. In this framework, the proposed hop-aware hierarchical channel-squeezing
    fusion layer is designed to extract relevant information from neighboring nodes.
    Higher-order Regular Splitting graph Network (RS-Net) [[55](#bib.bib55)] captures
    long-range dependencies between body joints using multi-hop neighborhoods. It
    learns distinct modulation vectors for different body joints and adds modulation
    matrices to the corresponding skeletal adjacency matrices. Zhai et al. [[71](#bib.bib71)]
    employed intra-group joint refinement utilizing attention mechanisms to discover
    potential joint synergies to explore the potential synergies between joints.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNN）是一种定义在图数据结构上的卷积网络。典型的神经网络难以处理图结构数据，如身体结构信息。然而，如果使用基于图的学习方法，这个挑战可以更容易地解决，如图中的Fig.[4(b)](#S4.F4.sf2
    "在图4 ‣ 4.1.1 单人3D姿态估计 ‣ 4.1 单人3D姿态估计 ‣ 4 3D人体姿态估计 ‣ 深度学习用于3D人体姿态估计和网格恢复的综述")所示。刘等人[[67](#bib.bib67)]
    提出了一个特征增强网络，其中特征通过卷积层学习，并通过图形化ConvLSTM增强，以感知不同身体部位之间的图形长短期依赖关系。局部连接网络（LCN）[[68](#bib.bib68)]
    利用专用滤波器的分配，而不是将其共享用于不同的关节。调制图卷积网络（Modulated-GCN）[[69](#bib.bib69)] 包括权重和亲和力调制，分别用于学习不同身体关节的调制向量和调整图结构。曾等人[[70](#bib.bib70)]
    设计了一个骨架GNN学习框架，以解决3D人体姿态估计中的深度歧义和自遮挡问题。在这个框架中，提出的跳跃感知分层通道挤压融合层被设计用来从邻近节点中提取相关信息。高阶正则分裂图网络（RS-Net）[[55](#bib.bib55)]
    使用多跳邻域捕捉身体关节之间的长距离依赖关系。它为不同的身体关节学习不同的调制向量，并将调制矩阵添加到相应的骨架邻接矩阵中。翟等人[[71](#bib.bib71)]
    利用注意机制进行组内关节细化，以发现潜在的关节协同，探索关节之间的潜在协同效应。
- en: 'Solving Occlusion Problems. Partial occlusion of the human body, including
    self-occlusion and other occlusions (such as object occlusion and multi-person
    occlusion), is typical in various scenes. Occlusion may interfere with pose estimation,
    potentially resulting in the prediction of an error pose. Solving the occlusion
    problem through a multi-view approach is effective, where an occluded pose not
    visible in one view is likely to be visible in other views in a multi-view system.
    Thus, a multi-view based approach can provide more reliable results through cross-view
    frame inference. In practice, Iskakov et al. [[72](#bib.bib72)] combined algebraic
    and volumetric triangulation for 3D pose estimation from multi-view 2D images.
    The former method is based on a differentiable algebraic triangulation with confidence
    weights, while the latter method utilizes volumetric aggregation from intermediate
    2D backbone feature maps. Multi-view geometric priors have been incorporated in
    [[73](#bib.bib73)], which combines two steps: first, predicting the 2D poses in
    multi-view RGB images through cross-view fusion, and second, utilizing the proposed
    Recursive Pictorial Structure Model (RPSM) to recover the 3D poses from the previously
    predicted multi-view 2D poses. To improve the real-time performance of multi-view
    3D pose estimation, Remelli et al. [[74](#bib.bib74)] designed a lightweight framework
    with a differentiable Direct Linear Transform (DLT) layer. Adaptive multi-view
    fusion [[75](#bib.bib75)] enhances the features in occluded views by determining
    the correspondence between points in occluded and visible views. To address the
    problem of generalizability for multi-view estimation, Bartol et al. [[76](#bib.bib76)]
    proposed a stochastic learning framework for pose triangulation. Luvizon et al.
    [[77](#bib.bib77)] effectively integrated 2D annotated data and 3D poses to design
    a consensus-aware method, which optimizes multi-view poses from uncalibrated images
    by different coherent estimations up to a scale factor from the intrinsic parameters.
    Probabilistic triangulation module [[29](#bib.bib29)] embedded in a 3D pose estimation
    framework can extend multi-view methods to uncalibrated scenes. It models camera
    poses using probability distributions and iteratively updates them from 2D features,
    replacing the traditional update through camera poses and eliminating the dependency
    on camera calibration.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 解决遮挡问题。人体的部分遮挡，包括自遮挡和其他遮挡（如物体遮挡和多人遮挡），在各种场景中都很常见。遮挡可能会干扰姿态估计，从而导致错误姿态的预测。通过多视角方法解决遮挡问题是有效的，其中在一个视角中不可见的遮挡姿态可能在多视角系统中的其他视角中可见。因此，基于多视角的方法可以通过跨视角框架推断提供更可靠的结果。在实践中，Iskakov等人[[72](#bib.bib72)]结合了代数三角测量和体积三角测量，从多视角2D图像中进行3D姿态估计。前者方法基于带有置信度权重的可微分代数三角测量，而后者方法则利用来自中间2D骨干特征图的体积聚合。多视角几何先验已被纳入[[73](#bib.bib73)]，该方法结合了两个步骤：首先，通过视角融合预测多视角RGB图像中的2D姿态，其次，利用所提出的递归图像结构模型（RPSM）从先前预测的多视角2D姿态中恢复3D姿态。为了提高多视角3D姿态估计的实时性能，Remelli等人[[74](#bib.bib74)]设计了一个轻量级框架，具有可微分的直接线性变换（DLT）层。自适应多视角融合[[75](#bib.bib75)]通过确定遮挡视角和可见视角之间的点对应关系来增强遮挡视角中的特征。为了解决多视角估计的泛化问题，Bartol等人[[76](#bib.bib76)]提出了一种用于姿态三角测量的随机学习框架。Luvizon等人[[77](#bib.bib77)]有效整合了2D注释数据和3D姿态，设计了一种共识感知方法，通过不同的连贯估计优化来自未校准图像的多视角姿态，最大程度地消除了对内在参数的尺度因子的依赖。嵌入到3D姿态估计框架中的概率三角测量模块[[29](#bib.bib29)]可以将多视角方法扩展到未校准场景。它使用概率分布对相机姿态进行建模，并通过2D特征迭代更新，替代传统的通过相机姿态更新，从而消除了对相机校准的依赖。
- en: Solving Data Lacking. In recent years, the development of diverse and efficient
    feature representations, along with end-to-end training modes, has significantly
    enhanced the accuracy of deep learning models in 3D human pose estimation. However,
    a significant limitation of these models is their reliance on fully supervised
    training, necessitating vast amounts of expensive and labor-intensive labeled
    3D data, predominantly from indoor scenes. Addressing this challenge requires
    exploring alternative training strategies beyond fully supervised learning, such
    as unsupervised, semi-supervised, and few-shot learning approaches.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 解决数据缺乏问题。近年来，随着多样且高效的特征表示的发展以及端到端训练模式的出现，深度学习模型在3D人体姿态估计中的准确性得到了显著提升。然而，这些模型的一个重要限制是它们依赖于完全监督的训练，需要大量昂贵且劳动密集的标注3D数据，这些数据主要来自室内场景。解决这一挑战需要探索超越完全监督学习的替代训练策略，例如无监督、半监督和少样本学习方法。
- en: Unsupervised learning typically focuses on learning features rather than specific
    tasks from unlabeled training data, and it finds relationships between samples
    by mining the intrinsic features of the data. The first work [[78](#bib.bib78)]
    can predict 3D human pose without any 3D dataset by adversarial learning, which
    is based on the generative adversarial networks via unsupervised training. The
    approach [[79](#bib.bib79)] utilizes geometric self-supervision and randomly reprojects
    2D pose camera viewpoints from the recovered 3D skeleton during training, thereby
    forming a self-consistency loss in the lift-reproject-lift process. Elepose [[80](#bib.bib80)]
    utilizes random projections, estimating likelihood using normalizing flows on
    2D poses in a linear subspace. Furthermore, Elepose also learns the distribution
    of camera angles to reduce the dependence on camera rotation priors in the training
    dataset.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习通常专注于从未标注的训练数据中学习特征，而不是特定任务，并通过挖掘数据的内在特征来发现样本之间的关系。首个工作[[78](#bib.bib78)]可以通过对抗学习预测3D人体姿态，而无需任何3D数据集，该对抗学习基于生成对抗网络，通过无监督训练实现。方法[[79](#bib.bib79)]利用几何自监督，在训练过程中随机重新投影从恢复的3D骨架中获取的2D姿态相机视角，从而在升高-重新投影-升高过程中形成自一致性损失。Elepose
    [[80](#bib.bib80)]利用随机投影，通过在2D姿态的线性子空间上使用归一化流来估计可能性。此外，Elepose还学习相机角度的分布，以减少对训练数据集中相机旋转先验的依赖。
- en: 'Self-supervised learning is a branch of unsupervised learning where the model
    can learn by itself from unlabeled training data and acquire the representation
    model on unlabeled data through pretext tasks. During on-the-ground execution,
    EpipolarPose [[81](#bib.bib81)] trains the 3D pose estimator without any 3D ground-truth
    data or camera extrinsic parameters, predicting 2D poses from multi-view images
    and obtaining a 3D pose and camera geometry via epipolar geometry. Wang et al.
    [[82](#bib.bib82)] designed a simple yet effective self-supervised correction
    mechanism for 3D human pose estimation by learning all intrinsic structures of
    the human pose, which is divided into two parts: the 2D-to-3D pose transformation
    and 3D-to-2D pose projection. MRP-Net [[83](#bib.bib83)] reformulated the self-supervised
    3D human pose estimation as an unsupervised domain adaptation problem, which includes
    model-free joint localization and model-based parametric regression. To reduce
    dependence on the consistency loss that guides learning, unlike previous works,
    the method [[54](#bib.bib54)] generates 2D-3D pose pairs for augmenting supervision
    via the proposed self-enhancing dual-loop learning framework.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习是无监督学习的一个分支，其中模型可以从未标注的训练数据中自我学习，并通过前置任务在未标注数据上获得表示模型。在实际执行中，EpipolarPose
    [[81](#bib.bib81)]在没有任何3D真实数据或相机外部参数的情况下训练3D姿态估计器，通过多视角图像预测2D姿态，并通过极几何获得3D姿态和相机几何。王等人
    [[82](#bib.bib82)] 设计了一种简单而有效的自监督修正机制，通过学习人体姿态的所有内在结构来进行3D人体姿态估计，这一机制分为两个部分：2D到3D姿态转换和3D到2D姿态投影。MRP-Net
    [[83](#bib.bib83)] 将自监督的3D人体姿态估计重新表述为无监督领域适应问题，其中包括无模型的关节定位和基于模型的参数回归。为了减少对指导学习的一致性损失的依赖，与以前的工作不同，方法[[54](#bib.bib54)]通过提出的自增强双环学习框架生成2D-3D姿态对以增强监督。
- en: A weakly-supervised model, characterized by its reliance on only weak labels,
    often faces the challenge of performing complex tasks with limited or imprecise
    guidance. Despite the lack of detailed annotations typically required in fully
    supervised scenarios, these models derive meaningful insights, utilizing vague
    or less informative labels to infer intricate patterns and relationships within
    the data. This ability to operate with minimal supervision makes them particularly
    useful when acquiring comprehensive labeled data is impractical or costly. In
    practical applications, Hua et al. [[84](#bib.bib84)] proposed a weakly-supervised
    method that initially lifts the 2D keypoints into coarse 3D poses across two views
    using triangulation and then refines the 3D pose by employing spatial configurations
    and cross-view correlations. Yang et al. [[43](#bib.bib43)] designed a weakly-supervised
    framework that utilizes projection relationships to estimate 3D poses solely from
    2D pose annotations under the condition of known camera parameters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一个弱监督模型，其特征在于仅依赖于弱标签，通常面临在有限或不精确指导下执行复杂任务的挑战。尽管缺乏通常在完全监督场景中所需的详细注释，这些模型仍能得出有意义的见解，利用模糊或信息量较少的标签来推断数据中的复杂模式和关系。这种在最小监督下操作的能力使得它们在获取全面标注数据不切实际或昂贵时特别有用。在实际应用中，Hua
    等人 [[84](#bib.bib84)] 提出了一个弱监督方法，该方法首先通过三角测量将2D关键点提升为两个视角中的粗略3D姿态，然后通过采用空间配置和视角间相关性来细化3D姿态。Yang
    等人 [[43](#bib.bib43)] 设计了一个弱监督框架，利用投影关系仅根据已知相机参数的2D姿态注释来估计3D姿态。
- en: 'Transfer learning enables the transfer of model parameters from the source
    domain to the target domain, allowing us to share learned model parameters with
    a new model, as shown in Fig.[4(c)](#S4.F4.sf3 "In Figure 4 ‣ 4.1.1 Single person
    3D pose estimation in images ‣ 4.1 Single person 3D pose estimation ‣ 4 3D Human
    Pose Estimation ‣ Deep Learning for 3D Human Pose Estimation and Mesh Recovery:
    A Survey"). This approach speeds up and optimizes efficiency, eliminating the
    need to learn from scratch, as is common with most networks. For example, Adaptpose
    [[85](#bib.bib85)] is an end-to-end cross-dataset adaptation 3D human pose estimation
    predictor that is implemented using transfer learning for limited data applications.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习使得模型参数可以从源领域转移到目标领域，从而允许我们与新模型共享学习到的模型参数，如图[4(c)](#S4.F4.sf3 "在图4 ‣ 4.1.1
    单人3D姿态估计 ‣ 4.1 单人3D姿态估计 ‣ 4 3D人类姿态估计 ‣ 基于深度学习的3D人类姿态估计与网格恢复综述")所示。这种方法加快了速度并优化了效率，消除了从零开始学习的需要，这在大多数网络中是常见的。例如，Adaptpose
    [[85](#bib.bib85)] 是一个端到端的跨数据集适配3D人类姿态估计预测器，它通过迁移学习在有限数据应用中实现。
- en: 4.1.2 Single person 3D pose estimation in videos
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 视频中的单人3D姿态估计
- en: With hardware development, image data are often acquired and processed in videos.
    The video frame data can provide more continuous information than a single-frame
    image, predicting human pose estimation in sequence through the spatio-temporal
    domain. In addition, optical flow and scene flow information can be extracted
    from videos to predict 3D human pose from data of multimodal [[119](#bib.bib119),
    [56](#bib.bib56)].
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 随着硬件的发展，图像数据通常在视频中获得和处理。视频帧数据可以提供比单帧图像更连续的信息，通过时空域对人类姿态进行序列预测。此外，还可以从视频中提取光流和场景流信息，从多模态数据中预测3D人类姿态
    [[119](#bib.bib119), [56](#bib.bib56)]。
- en: Solving Single-frame Limitation. Using continuous image frames from video can
    capture dynamic changes and temporal information. By analyzing this information,
    deep learning models can extract motion features and spatio-temporal relationships,
    thereby effectively improving the effectiveness of 3D human pose estimation. Pavllo
    et al. [[86](#bib.bib86)] proposed a 3D human pose estimation method for video
    that extracts temporal cues with dilated convolutions over 2D keypoint trajectories,
    and then they designed a semi-supervised method with back-projection to improve
    performance. PoseFormer [[87](#bib.bib87)] as a spatial-temporal transformer-based
    framework predicts 3D body pose by analyzing human joint relations within each
    frame and their temporal correlations across frames. Based on previous work [[120](#bib.bib120)],
    unipose+ [[88](#bib.bib88)] leverages multi-scale feature representations to enhance
    the feature extractors in the framework’s backbone. This framework utilizes contextual
    information across scales and joint localization with Gaussian heatmap modulation
    to improve decoder estimation accuracy.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 解决单帧限制。使用视频中的连续图像帧可以捕捉动态变化和时间信息。通过分析这些信息，深度学习模型可以提取运动特征和时空关系，从而有效提高 3D 人体姿态估计的效果。Pavllo
    等人 [[86](#bib.bib86)] 提出了一个针对视频的 3D 人体姿态估计方法，该方法通过在 2D 关键点轨迹上应用膨胀卷积提取时间线索，并设计了一种具有反向投影的半监督方法来提高性能。PoseFormer
    [[87](#bib.bib87)] 作为一种基于空间-时间变换器的框架，通过分析每帧中的人体关节关系及其跨帧的时间相关性来预测 3D 身体姿态。基于之前的工作
    [[120](#bib.bib120)]，unipose+ [[88](#bib.bib88)] 利用多尺度特征表示来增强框架主干中的特征提取器。该框架利用跨尺度的上下文信息和高斯热图调制的关节定位来提高解码器的估计精度。
- en: Furthermore, Multi-Hypothesis transFormer (MHFormer) [[89](#bib.bib89)] learns
    spatio-temporal representations with multiple plausible pose hypotheses in a three-stage
    framework. In Mixed Spatio-Temporal Encoder (MixSTE) framework [[90](#bib.bib90)],
    the temporal transformer block learns the temporal motion of each joint and their
    inter-joint spatial correlation. Honari et al. [[91](#bib.bib91)] proposed an
    unsupervised feature extraction method using Contrastive Self-Supervised (CSS)
    learning to extract temporal information from videos. Extending this line of research,
    Hierarchical Spatial-Temporaltrans Formers (HSTFormer) [[92](#bib.bib92)] utilizes
    spatial-temporal correlations of joints at different levels simultaneously, marking
    a first in studying hierarchical transformer encoders with multi-level fusion.
    Recently, Spatio-Temporal Criss-cross (STC) attention block [[93](#bib.bib93)]
    decomposes correlation learning into space and time to reduce the computational
    cost of the joint-to-joint affinity matrix.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Multi-Hypothesis transFormer (MHFormer) [[89](#bib.bib89)] 在一个三阶段框架中学习具有多种可能姿态假设的时空表示。在
    Mixed Spatio-Temporal Encoder (MixSTE) 框架 [[90](#bib.bib90)] 中，时间变换器块学习每个关节的时间运动及其关节间的空间相关性。Honari
    等人 [[91](#bib.bib91)] 提出了一个利用对比自监督 (CSS) 学习的无监督特征提取方法，以从视频中提取时间信息。扩展这一研究方向，Hierarchical
    Spatial-Temporal Transformers (HSTFormer) [[92](#bib.bib92)] 同时利用不同层次上关节的时空相关性，这在研究具有多层融合的分层变换器编码器中开创了先河。最近，Spatio-Temporal
    Criss-cross (STC) 注意力块 [[93](#bib.bib93)] 将相关性学习分解为空间和时间，以减少关节间亲和矩阵的计算成本。
- en: Solving Real-time Problems. However, while various methods improve 3D pose estimation
    performance, they also introduce new problems and challenges in video-based estimation.
    For example, the continuous video frames add a substantial computational cost,
    which poses a challenge to the processing efficiency of the system and makes real-time
    processing more difficult. To reduce the total computational complexity, Einfalt
    et al. [[19](#bib.bib19)] designed a transformer-based scheme that uplifts dense
    3D poses from temporally sparse 2D pose sequences by temporal upsampling within
    Transformer blocks. Sun et al. [[94](#bib.bib94)] reduced the complexity of attention
    calculation in Transformers through spatio-temporal sparse sampling, enabling
    the estimation of 3D human poses in video sequences on computationally constrained
    platforms.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 解决实时问题。然而，尽管各种方法提高了 3D 姿态估计的性能，但它们也在基于视频的估计中引入了新的问题和挑战。例如，连续的视频帧增加了相当大的计算成本，这对系统的处理效率构成了挑战，并使实时处理变得更加困难。为了减少总计算复杂性，Einfalt
    等人 [[19](#bib.bib19)] 设计了一种基于变换器的方案，通过在变换器块内进行时间上采样，从时间稀疏的 2D 姿态序列中提升密集的 3D 姿态。Sun
    等人 [[94](#bib.bib94)] 通过时空稀疏采样减少了变换器中注意力计算的复杂性，使得在计算受限的平台上能够估计视频序列中的 3D 人体姿态。
- en: Solving Body Structure Understanding. At the same time, the continuous motion
    information provided by video has led researchers to combine videos with human
    kinematics. Wang et al. [[95](#bib.bib95)] designed a motion loss that computes
    the difference between the motion patterns of the predicted and ground truth keypoint
    trajectories, along with motion encoding, a simple yet effective representation
    of keypoint motion for 3D human pose estimation in videos. Dynamical Graph Network
    (DG-Net) [[96](#bib.bib96)] dynamically determines the human-joint affinity and
    adaptively predicts human pose via spatial-temporal joint relations in videos.
    To address the shortcoming of directly regressing the 3D joint locations, Chen
    et al. [[97](#bib.bib97)] proposed an anatomy-aware estimation framework. This
    human skeleton anatomy-based framework includes a bone direction prediction network
    and a bone length prediction network, and it effectively utilizes bone features
    to bridge the gap between 2D keypoints and 3D joints. Xue et al. [[98](#bib.bib98)]
    designed a part-aware temporal attention module capable of extracting each part’s
    temporal dependency separately.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 解决身体结构理解问题。与此同时，视频提供的连续运动信息促使研究人员将视频与人体运动学结合。Wang 等人 [[95](#bib.bib95)] 设计了一种运动损失函数，该函数计算预测的关键点轨迹与真实关键点轨迹之间的运动模式差异，同时采用运动编码，这是一种简单而有效的关键点运动表示方法，用于视频中的
    3D 人体姿态估计。动态图网络（DG-Net）[[96](#bib.bib96)] 动态确定人体关节亲和性，并通过视频中的时空关节关系自适应地预测人体姿态。为了解决直接回归
    3D 关节位置的不足，Chen 等人 [[97](#bib.bib97)] 提出了一个基于解剖学的估计框架。该基于人体骨架解剖学的框架包括一个骨头方向预测网络和一个骨头长度预测网络，并有效地利用骨头特征弥合
    2D 关键点与 3D 关节之间的差距。Xue 等人 [[98](#bib.bib98)] 设计了一个部分感知的时间注意模块，能够单独提取每个部分的时间依赖性。
- en: Solving Occlusion Problems. To address occlusion problems, video-based 3D human
    pose estimation enables the use of a multi-view approach and leverages the continuity
    of inputs between video frames to predict occluded body parts. Cheng et al. [[99](#bib.bib99)]
    introduced an occlusion-aware model that utilizes estimated 2D confidence heatmaps
    of keypoints and an optical-flow consistency constraint to generate a more complete
    3D pose in occlusion scenes. Additionally, Multi-view and Temporal Fusing Transformer
    (MTF-Transformer) [[30](#bib.bib30)] fuses multi-view sequences in uncalibrated
    scenes for 3D human pose estimation in videos. To reduce dependency on camera
    calibration, the framework infers the relationship between pairs of views with
    a relative attention mechanism.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 解决遮挡问题。为了解决遮挡问题，基于视频的 3D 人体姿态估计采用了多视角方法，并利用视频帧之间输入的连续性来预测被遮挡的身体部分。Cheng 等人 [[99](#bib.bib99)]
    提出了一个遮挡感知模型，该模型利用估计的 2D 关键点置信度热图和光流一致性约束，以生成遮挡场景中的更完整 3D 姿态。此外，多视角和时间融合变换器（MTF-Transformer）[[30](#bib.bib30)]
    在未标定场景中融合多视角序列，用于视频中的 3D 人体姿态估计。为了减少对相机标定的依赖，该框架通过相对注意机制推断视角对之间的关系。
- en: 'Solving Data Lacking. Human actions are inherently continuous, making video-based
    pose estimation critical for enhanced understanding. Compared with 3D annotated
    still images, high-quality, annotated 3D videos are relatively scarce. Nevertheless,
    the Internet abounds with a vast repository of unlabeled video data, the utilization
    of which for learning purposes holds considerable importance. Thus, video-based
    3D human pose estimation to alleviate data dependency is essential. In practice,
    Yu et al. [[100](#bib.bib100)] divided the unsupervised 3D pose estimation process
    into two sub-tasks: a scale estimation module and a pose lifting module. The scale
    estimation module optimizes the 2D input pose, while the pose lifting module maps
    the optimized 2D pose to its 3D counterpart. In weakly-supervised methods, Chen
    et al. [[101](#bib.bib101)] proposed a weakly-supervised method, employing a view
    synthesis framework and a geometry-aware representation. In this framework, the
    method leverages 2D keypoints for supervision and learns a shared 3D representation
    between viewpoints by synthesizing the human pose from one viewpoint to another.
    Semi-supervised learning utilizes only partially labeled data, consisting of a
    large amount of unlabeled data and a small amount of labeled data. In this context,
    Mitra et al. [[102](#bib.bib102)] present a multi-view consistent semi-supervised
    method, which can regress 3D human pose from unannotated, uncalibrated, but synchronized
    multi-view videos. In self-supervised methods, Kundu et al. [[103](#bib.bib103)]
    employed prior knowledge of the body skeleton and disentangles the inherent factors
    of variation through part-guided human image synthesis. Similarly, Shan et al.
    [[104](#bib.bib104)] randomly mask the body joints in spatial and temporal domains
    to better capture spatial and temporal dependencies. Meta-learning (a.k.a. ”learning
    to learn”) is a process where algorithms optimize their learning strategy based
    on experience. It involves training a model on various learning tasks, enabling
    it to learn new tasks more efficiently using fewer data samples. Cho et al. [[53](#bib.bib53)]
    present an optimization-based meta-learning algorithm for 3D human pose estimation.
    This algorithm can adapt to arbitrary camera distortion by generating synthetic
    distorted data from undistorted 2D keypoints during model training. Apart from
    changing the learning supervision methods, data augmentation is also a practical
    approach to increasing the amount of data and enhancing the model’s generalization
    performance. PoseAug [[105](#bib.bib105)] adjusts geometric factors such as posture,
    body shape, and viewpoint for model learning under the premise of the discriminative
    module controlling the feasibility of augmented poses. Zhang et al. [[106](#bib.bib106)]
    generates more diverse and challenging poses online.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 解决数据缺乏问题。人类动作本质上是连续的，因此基于视频的姿态估计对于深入理解至关重要。与3D标注静态图像相比，高质量的标注3D视频相对稀缺。然而，互联网充斥着大量未标记的视频数据，这些数据用于学习目的具有重要意义。因此，基于视频的3D人体姿态估计以减轻数据依赖性是必不可少的。在实践中，Yu
    等人[[100](#bib.bib100)]将无监督的3D姿态估计过程分为两个子任务：尺度估计模块和姿态提升模块。尺度估计模块优化2D输入姿态，而姿态提升模块将优化后的2D姿态映射到其3D对应物。在弱监督方法中，Chen
    等人[[101](#bib.bib101)]提出了一种弱监督方法，采用视图合成框架和几何感知表示。在该框架中，该方法利用2D关键点进行监督，并通过将一个视角的人体姿态合成到另一个视角，学习视角之间共享的3D表示。半监督学习只利用部分标记的数据，包括大量未标记的数据和少量标记的数据。在这种情况下，Mitra
    等人[[102](#bib.bib102)]提出了一种多视角一致的半监督方法，该方法可以从未标注、未校准但同步的多视角视频中回归3D人体姿态。在自监督方法中，Kundu
    等人[[103](#bib.bib103)]利用身体骨架的先验知识，并通过部位引导的人体图像合成解开固有的变异因素。同样，Shan 等人[[104](#bib.bib104)]在空间和时间域中随机遮挡身体关节，以更好地捕捉空间和时间依赖性。元学习（也称为“学习如何学习”）是一个算法根据经验优化其学习策略的过程。它涉及在各种学习任务上训练模型，使其能够更高效地使用较少的数据样本学习新任务。Cho
    等人[[53](#bib.bib53)]提出了一种基于优化的元学习算法用于3D人体姿态估计。该算法可以通过在模型训练期间从未失真2D关键点生成合成失真数据，适应任意的相机畸变。除了改变学习监督方法，数据增强也是增加数据量和提高模型泛化性能的实际方法。PoseAug
    [[105](#bib.bib105)]在区分模块控制增强姿态的可行性前提下，调整姿态、身体形状和视角等几何因素以进行模型学习。Zhang 等人[[106](#bib.bib106)]在线生成更多样化和具有挑战性的姿态。
- en: 4.2 Multi-person 3D pose estimation
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 多人3D姿态估计
- en: 'As depicted in Fig.[5](#S4.F5 "Figure 5 ‣ 4.2 Multi-person 3D pose estimation
    ‣ 4 3D Human Pose Estimation ‣ Deep Learning for 3D Human Pose Estimation and
    Mesh Recovery: A Survey"), multi-person 3D pose estimation can be divided into
    two main categories: the bottom-up method (estimation + association) and the top-down
    method (detection + estimation). With a two-stage pipeline, the top-down method
    first detects every person in the input images and then extracts each person’s
    keypoints in the previously detected bounding box. The bottom-up method initially
    detects all keypoints in one stage and subsequently associates them with their
    respective persons. For instance, Cheng et al. [[118](#bib.bib118)] integrated
    both top-down and bottom-up approaches in multi-person pose estimation, complementing
    each other’s shortcomings. The top-down method shows robustness against potential
    erroneous bounding boxes, while the bottom-up network is more robust in handling
    scale variations. Finally, the 3D poses estimated from both top-down and bottom-up
    networks are fed into an integrated network to obtain the final 3D pose. Some
    single-stage methods and approaches combine the two methods as mentioned above.
    Unlike mainstream two-stage solutions for multi-person 3D human pose estimation,
    Jin et al. [[117](#bib.bib117)] introduced a single-stage method. This method
    expresses the 2D pose in the image plane and the depth information of a 3D body
    instance via the proposed decoupled representation and predicts the scale information
    of instances by extracting 2D pose features and enabling depth regression.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[5](#S4.F5 "图 5 ‣ 4.2 多人 3D 姿态估计 ‣ 4 3D 人体姿态估计 ‣ 深度学习在人体 3D 姿态估计和网格恢复中的应用：综述")所示，多人
    3D 姿态估计可以分为两大类：自下而上的方法（估计 + 关联）和自上而下的方法（检测 + 估计）。自上而下的方法采用两阶段的流程，首先在输入图像中检测每个人，然后在之前检测到的边界框中提取每个人的关键点。自下而上的方法则在一个阶段中检测所有关键点，并随后将它们与各自的人进行关联。例如，Cheng
    等人 [[118](#bib.bib118)] 将自上而下和自下而上的方法结合在多人姿态估计中，相互补充对方的不足。自上而下的方法对潜在的错误边界框具有鲁棒性，而自下而上的网络在处理尺度变化时更具鲁棒性。最后，从自上而下和自下而上的网络估计的
    3D 姿态被输入到一个集成网络中，以获得最终的 3D 姿态。一些单阶段的方法和方法结合了上述两种方法。与主流的多人 3D 人体姿态估计两阶段解决方案不同，Jin
    等人 [[117](#bib.bib117)] 引入了一种单阶段方法。这种方法通过提出的解耦表示在图像平面上表达 2D 姿态和 3D 体实例的深度信息，并通过提取
    2D 姿态特征并实现深度回归来预测实例的尺度信息。
- en: '![Refer to caption](img/58c1ca72a706ceffe5f031b78c79d3c4.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/58c1ca72a706ceffe5f031b78c79d3c4.png)'
- en: 'Figure 5: Typical multi-person 3D pose estimation.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：典型的多人 3D 姿态估计。
- en: 4.2.1 Top-down methods
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 自上而下的方法
- en: In top-down methods, AlphaPose [[108](#bib.bib108)] predicts whole-body multi-person
    3D human pose including face, body, hand, and foot. In the proposed framework,
    the symmetric integral keypoint regression module achieves fast and fine localization,
    the parametric pose non-maximum suppression module helps to eliminate redundant
    human detections, and the pose aware identity embedding module enables joint pose
    estimation and tracking. To achieve better real-time performance in multi-person
    3D pose estimation, Chen et al. [[107](#bib.bib107)] proposed a multi-view temporal
    consistency method in real-time from videos, which matches the 2D inputs with
    3D poses directly in three-dimensional space and achieves over 150 FPS (frames
    per second) on a 12-camera setup and 34 FPS on a 28-camera setup. Additionally,
    choosing appropriate representations can effectively improve multi-person pose
    estimation results. For instance, Zhang et al. [[2](#bib.bib2)] employed a voxel
    representation to predict multi-person 3D pose and tracking, where the proposed
    voxel representation can determine whether each voxel contains a particular body
    joint. In multi-person scenarios, occlusion issues may be more severe and complex
    than in single-person scenarios. To address this challenge, Wu et al. [[109](#bib.bib109)]
    utilized graph neural networks to boost information passing efficiency for multi-person,
    multi-view 3D human pose estimation. The multi-view matching graph module associates
    coarse cross-view poses within the networks, and the center refinement graph module
    further refines the results. Single-shot learning trains the model using significantly
    less data than is typically required for supervised learning, aiming to improve
    effectiveness. PandaNet [[48](#bib.bib48)] as an anchor-based model introduces
    a pose-aware anchor selection strategy to discard ambiguous anchors. Moon et al.
    [[110](#bib.bib110)] proposed a top-down, camera distance-aware method for multi-person
    3D pose estimation without relying on ground-truth information.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在自顶向下的方法中，AlphaPose [[108](#bib.bib108)] 预测包括面部、身体、手部和脚部的全身多人的 3D 姿态。在提出的框架中，对称积分关键点回归模块实现了快速而精确的定位，参数化姿态非极大值抑制模块有助于消除冗余的人体检测，姿态感知身份嵌入模块实现了联合姿态估计和跟踪。为了在多人
    3D 姿态估计中获得更好的实时性能，Chen 等人 [[107](#bib.bib107)] 提出了一个从视频中实时获得的多视角时间一致性方法，该方法将 2D
    输入直接与三维空间中的 3D 姿态匹配，并在 12 个摄像头设置上实现了超过 150 FPS（每秒帧数），在 28 个摄像头设置上实现了 34 FPS。此外，选择适当的表示可以有效提高多人姿态估计结果。例如，Zhang
    等人 [[2](#bib.bib2)] 使用了体素表示来预测多人 3D 姿态和跟踪，其中提出的体素表示可以确定每个体素是否包含特定的身体关节。在多人场景中，遮挡问题可能比单人场景更为严重和复杂。为了解决这一挑战，Wu
    等人 [[109](#bib.bib109)] 利用图神经网络提升多人人物、多视角 3D 姿态估计的信息传递效率。多视角匹配图模块在网络中关联粗略的视角姿态，而中心精细化图模块进一步优化结果。单次学习使用远少于通常所需的监督学习数据来训练模型，旨在提高效果。PandaNet
    [[48](#bib.bib48)] 作为一种基于锚点的模型引入了一种姿态感知的锚点选择策略，以丢弃模糊的锚点。Moon 等人 [[110](#bib.bib110)]
    提出了一个自顶向下的、相机距离感知的方法来进行多人 3D 姿态估计，而不依赖于真实信息。
- en: 4.2.2 Bottom-up methods
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 自底向上的方法
- en: MubyNet [[121](#bib.bib121)] is a typical bottom-up, multi-task method for multi-person
    3D pose estimation, which allows for training all component parameters. In the
    research on the lightweight top-down framework, Fabbri et al. [[111](#bib.bib111)]
    utilized high-resolution volumetric heatmaps to improve the estimation performance
    and designed a volumetric heatmap autoencoder that can compress the size of the
    representation. Designing better supervisory methods could also be effective,
    Wang et al. [[112](#bib.bib112)] introduced a novel supervisory approach named
    Hierarchical Multi-person Ordinal Relations (HMOR) using a monocular camera and
    designed a comprehensive top-level model to learn these ordinal relations, enhancing
    the accuracy of human depth estimation through a coarse-to-fine architecture.
    In the top-down estimation on single-shot learning, Mehta et al. [[115](#bib.bib115)]
    inferred whole body pose under strong partial occlusions through occlusion-robust
    pose-maps in their proposed single-shot framework. Zhen et al. [[113](#bib.bib113)]
    designed a single-shot, bottom-up framework that estimates the absolute positions
    of multiple people by leveraging depth-related cues across the entire image. After
    their previous work [[48](#bib.bib48)], Benzine et al. [[114](#bib.bib114)] proposed
    a single-shot 3D human pose estimation method that predicts multi-person 3D poses
    without the need for bounding boxes. This method extends the Stacked Hourglass
    Network [[122](#bib.bib122)] to handle multi-person situations. To address the
    issue of occlusion, LCR-Net++ [[116](#bib.bib116)] integrates adjacent pose hypotheses
    to predict the multi-person 2D and 3D poses without approximating initial human
    localization when a person is partially occluded or truncated by the image boundaries.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: MubyNet [[121](#bib.bib121)] 是一种典型的自下而上、多任务的多人人体3D姿态估计方法，它允许对所有组件参数进行训练。在轻量级自上而下框架的研究中，Fabbri等人
    [[111](#bib.bib111)] 利用高分辨率体积热图来提高估计性能，并设计了一个可以压缩表示大小的体积热图自编码器。设计更好的监督方法也可能有效，Wang等人
    [[112](#bib.bib112)] 引入了一种新的监督方法，称为层次化多人人体序数关系（HMOR），使用单目相机设计了一个综合的顶层模型来学习这些序数关系，通过粗到精的架构提高了人体深度估计的准确性。在单次学习的自上而下估计中，Mehta等人
    [[115](#bib.bib115)] 通过其提出的单次框架中的抗遮挡姿态图，在强遮挡下推断全身姿态。Zhen等人 [[113](#bib.bib113)]
    设计了一个单次、自下而上的框架，通过利用整个图像中的深度相关线索来估计多人的绝对位置。在他们之前的工作 [[48](#bib.bib48)] 之后，Benzine等人
    [[114](#bib.bib114)] 提出了一个单次3D人体姿态估计方法，该方法无需边界框即可预测多人人体3D姿态。该方法扩展了堆叠的沙漏网络 [[122](#bib.bib122)]
    以处理多人人体情况。为了应对遮挡问题，LCR-Net++ [[116](#bib.bib116)] 集成了相邻的姿态假设，以预测多人的2D和3D姿态，而无需在一个人部分遮挡或被图像边界截断时近似初始人体定位。
- en: 4.3 Summary of 3D pose estimation
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3D姿态估计总结
- en: Research in single-person 3D pose estimation predominantly focuses on some of
    the aforementioned critical issues. While there are commendable studies in this
    field, the challenges are yet to be fully resolved, necessitating more comprehensive
    research. In contrast to image-based methods, utilizing video-based methods is
    unequivocally seen as the trajectory of future advancements. The methodology adopted
    in multi-person 3D pose estimation must be tailored to the specific context. The
    current prevalent techniques can be categorized into two main approaches, each
    with inherent limitations. Top-down methods depend heavily on human detection
    and are susceptible to detection inaccuracies, often leading to unreliable pose
    estimations in environments with multiple people. Conversely, bottom-up methods,
    which operate independently of human detection and thus are immune to errors,
    face challenges in accurately processing all individuals in a scene concurrently,
    particularly affecting the detection of smaller-scale figures. In line with other
    domains in computer vision, the one-stage, end-to-end methods represent this field’s
    future direction.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 单人3D姿态估计的研究主要关注上述一些关键问题。虽然该领域已有不少值得称道的研究，但挑战尚未完全解决，仍需更全面的研究。与基于图像的方法相比，利用基于视频的方法被明确视为未来发展的轨迹。多人人体3D姿态估计中采用的方法必须根据具体情况量身定制。目前流行的技术可以分为两种主要方法，每种方法都有其固有的局限性。自上而下的方法高度依赖于人体检测，容易受到检测不准确的影响，往往在多人环境中导致姿态估计不可靠。相反，自下而上的方法独立于人体检测，因此不会受到错误的影响，但在同时准确处理场景中的所有个体时面临挑战，特别是对小规模人物的检测。与计算机视觉领域的其他领域一致，单阶段、端到端的方法代表了该领域的未来方向。
- en: 5 3D Human Mesh Recovery
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 3D 人体网格恢复
- en: 'Human mesh recovery can be divided into two categories based on their representation
    models: template-based (parametric) methods and template-free (non-parametric)
    methods, as shown in Fig.[6](#S5.F6 "Figure 6 ‣ 5 3D Human Mesh Recovery ‣ Deep
    Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey"). Template-based
    human mesh recovery reconstructs predefined models (such as SCAPE [[32](#bib.bib32)],
    SMPL [[33](#bib.bib33)]) by estimating the model’s parameters. In contrast, template-free
    human mesh recovery predicts the 3D body directly from input data without reliance
    on predefined models. The parametric approaches can be more robust than non-parametric
    methods with the templates’ prior knowledge, but their flexibility and detail
    are inherently limited even when extended with more parameters like SMPL+H [[34](#bib.bib34)]
    and SMPL+X [[57](#bib.bib57)]. Human mesh recovery is a crucial technique for
    digitizing the human body, posing significant challenges in computer vision and
    computer graphics due to the complex nature of geometric textures and color variations.
    Moreover, human mesh recovery faces challenges similar to 3D pose estimation,
    including environmental interference, multi-person scenarios, and occlusion issues.
    In this section, we will introduce the dominant methods based on these categories
    and challenges, with a comprehensive summary provided in Table [2](#S5.T2 "Table
    2 ‣ 5 3D Human Mesh Recovery ‣ Deep Learning for 3D Human Pose Estimation and
    Mesh Recovery: A Survey").'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 人体网格恢复可以根据其表示模型分为两类：基于模板（参数化）的方法和无模板（非参数化）的方法，如图[6](#S5.F6 "图 6 ‣ 5 3D 人体网格恢复
    ‣ 3D 人体姿态估计和网格恢复的深度学习：综述")所示。基于模板的人体网格恢复通过估计模型的参数来重建预定义的模型（如 SCAPE [[32](#bib.bib32)]，SMPL
    [[33](#bib.bib33)]）。相比之下，无模板的人体网格恢复直接从输入数据中预测 3D 身体，而无需依赖预定义模型。参数化方法由于模板的先验知识可能比非参数化方法更为稳健，但即使在扩展更多参数如
    SMPL+H [[34](#bib.bib34)] 和 SMPL+X [[57](#bib.bib57)] 时，它们的灵活性和细节也固有限制。人体网格恢复是数字化人体的关键技术，由于几何纹理和颜色变化的复杂性，在计算机视觉和计算机图形学中面临重大挑战。此外，人体网格恢复面临与
    3D 姿态估计类似的挑战，包括环境干扰、多人体场景和遮挡问题。在本节中，我们将介绍基于这些类别和挑战的主要方法，并在表[2](#S5.T2 "表 2 ‣ 5
    3D 人体网格恢复 ‣ 3D 人体姿态估计和网格恢复的深度学习：综述")中提供综合总结。
- en: '![Refer to caption](img/62124fdb3a42da7d541aae230c255586.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/62124fdb3a42da7d541aae230c255586.png)'
- en: (a) Template based human mesh recovery
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 基于模板的人体网格恢复
- en: '![Refer to caption](img/418f303917719e0b9afa94d1054cae68.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/418f303917719e0b9afa94d1054cae68.png)'
- en: (b) Template-free human mesh recovery
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 无模板的人体网格恢复
- en: 'Figure 6: Typical human mesh recovery. (a) Template based human mesh recovery
    method; (b) Template-free human mesh recovery method.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：典型的人体网格恢复。 (a) 基于模板的人体网格恢复方法；(b) 无模板的人体网格恢复方法。
- en: 'Table 2: Overview of human mesh recovery.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：人体网格恢复概述。
- en: '| Main ideas | Methods |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 主要思想 | 方法 |'
- en: '| Template-based | Naked | Multimodal Methods | $\bullet$ Hybrid annotations:
    Rong et al. [[123](#bib.bib123)] |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 基于模板 | 裸露 | 多模态方法 | $\bullet$ 混合标注：Rong 等人 [[123](#bib.bib123)] |'
- en: '| $\bullet$ Optical flow: DTS-VIBE [[124](#bib.bib124)] |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 光流：DTS-VIBE [[124](#bib.bib124)] |'
- en: '| $\bullet$ Silhouettes: LASOR [[125](#bib.bib125)] |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 轮廓：LASOR [[125](#bib.bib125)] |'
- en: '| $\bullet$ Cropped image and bounding box: CLIFF [[126](#bib.bib126)] |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 裁剪图像和边界框：CLIFF [[126](#bib.bib126)] |'
- en: '| Utilizing Attention Mechanism | $\bullet$ Part-driven attention: PARE [[127](#bib.bib127)]
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 利用注意力机制 | $\bullet$ 部分驱动注意力：PARE [[127](#bib.bib127)] |'
- en: '| $\bullet$ Graph attention: Mesh Graphormer [[128](#bib.bib128)] |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 图注意力：Mesh Graphormer [[128](#bib.bib128)] |'
- en: '| $\bullet$ Spatio-temporal attention: MPS-Net [[129](#bib.bib129)], PSVT [[130](#bib.bib130)]
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 时空注意力：MPS-Net [[129](#bib.bib129)]，PSVT [[130](#bib.bib130)] |'
- en: '| $\bullet$ Efficient architecture: FastMETRO [[131](#bib.bib131)], Xue et
    al. [[132](#bib.bib132)] |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 高效架构：FastMETRO [[131](#bib.bib131)]，Xue 等人 [[132](#bib.bib132)]
    |'
- en: '| $\bullet$ End-to-end structure: METRO [[133](#bib.bib133)] |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 端到端结构：METRO [[133](#bib.bib133)] |'
- en: '| Exploiting Temporal Information | $\bullet$ Temporally encoding features:
    Kanazawa et al. [[134](#bib.bib134)] |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 利用时间信息 | $\bullet$ 时间编码特征：Kanazawa 等人 [[134](#bib.bib134)] |'
- en: '| $\bullet$ Self-attention temporal: VIBE [[135](#bib.bib135)] |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 自注意力时间：VIBE [[135](#bib.bib135)] |'
- en: '| $\bullet$ Temporally consistent: TCMR [[136](#bib.bib136)] |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 时间一致性：TCMR [[136](#bib.bib136)] |'
- en: '| $\bullet$ Multi-level spatial-temporal attention: MAED [[137](#bib.bib137)]
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 多级时空注意力：MAED [[137](#bib.bib137)] |'
- en: '| $\bullet$ Temporally embedded live stream: TePose [[138](#bib.bib138)] |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 时间嵌入实时流：TePose [[138](#bib.bib138)] |'
- en: '| $\bullet$ Short-term and long-term temporal correlations: GLoT [[139](#bib.bib139)]
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 短期和长期时间相关性：GLoT [[139](#bib.bib139)] |'
- en: '| Multi-view Methods | $\bullet$ Confidence-aware majority voting mechanism:
    Dong et al. [[140](#bib.bib140)] |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 多视角方法 | $\bullet$ 置信度感知多数投票机制：Dong 等 [[140](#bib.bib140)] |'
- en: '| $\bullet$ Probabilistic-based multi-view: Sengupta et al. [[141](#bib.bib141)]
    |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 基于概率的多视角：Sengupta 等 [[141](#bib.bib141)] |'
- en: '| $\bullet$ Dynamic physics-geometry consistency: Huang et al. [[31](#bib.bib31)]
    |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 动态物理几何一致性：Huang 等 [[31](#bib.bib31)] |'
- en: '| $\bullet$ Cross-view fusion: Zhuo et al. [[142](#bib.bib142)] |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 跨视角融合：Zhuo 等 [[142](#bib.bib142)] |'
- en: '| Boosting Efficiency | $\bullet$ Sparse constrained formulation: SCOPE [[143](#bib.bib143)]
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 提升效率 | $\bullet$ 稀疏约束公式：SCOPE [[143](#bib.bib143)] |'
- en: '| $\bullet$ Single-stage model: BMP [[144](#bib.bib144)] |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 单阶段模型：BMP [[144](#bib.bib144)] |'
- en: '| $\bullet$ Process heatmap inputs: HeatER [[145](#bib.bib145)] |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 处理热图输入：HeatER [[145](#bib.bib145)] |'
- en: '| $\bullet$ Removing redundant tokens: TORE [[146](#bib.bib146)] |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 去除冗余标记：TORE [[146](#bib.bib146)] |'
- en: '| Developing Various Representations | $\bullet$ Texture map: TexturePose [[147](#bib.bib147)]
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 开发多种表示 | $\bullet$ 纹理图：TexturePose [[147](#bib.bib147)] |'
- en: '| $\bullet$ UV map: Zhang et al. [[148](#bib.bib148)], DecoMR [[149](#bib.bib149)],
    Zhang et al. [[150](#bib.bib150)] |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ UV 图：Zhang 等 [[148](#bib.bib148)]，DecoMR [[149](#bib.bib149)]，Zhang
    等 [[150](#bib.bib150)] |'
- en: '| $\bullet$ Heat map: Sun et al. [[151](#bib.bib151)], 3DCrowdNet [[152](#bib.bib152)]
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 热图：Sun 等 [[151](#bib.bib151)]，3DCrowdNet [[152](#bib.bib152)] |'
- en: '| $\bullet$ Uniform representation: DSTformer [[153](#bib.bib153)] |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 统一表示：DSTformer [[153](#bib.bib153)] |'
- en: '| Utilizing Structural Information | $\bullet$ Part-based: holopose [[154](#bib.bib154)]
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 利用结构信息 | $\bullet$ 基于部分：holopose [[154](#bib.bib154)] |'
- en: '| $\bullet$ Skeleton disentangling: Sun et al. [[155](#bib.bib155)] |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 骨架解缠：Sun 等 [[155](#bib.bib155)] |'
- en: '| $\bullet$ Hybrid inverse kinematics: HybrIK [[156](#bib.bib156)], NIKI [[157](#bib.bib157)]
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 混合逆运动学：HybrIK [[156](#bib.bib156)]，NIKI [[157](#bib.bib157)] |'
- en: '| $\bullet$ Uncertainty-aware: Lee et al. [[158](#bib.bib158)] |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 不确定性感知：Lee 等 [[158](#bib.bib158)] |'
- en: '| $\bullet$ Kinematic tree structure: Sengupta et al. [[159](#bib.bib159)]
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 动作树结构：Sengupta 等 [[159](#bib.bib159)] |'
- en: '| $\bullet$ Kinematic chains: SGRE [[160](#bib.bib160)] |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 运动链：SGRE [[160](#bib.bib160)] |'
- en: '| Choosing Appropriate Learning Strategies | $\bullet$ Self-improving: SPIN
    [[161](#bib.bib161)], ReFit [[162](#bib.bib162)], You et al. [[4](#bib.bib4)]
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 选择合适的学习策略 | $\bullet$ 自我提升：SPIN [[161](#bib.bib161)]，ReFit [[162](#bib.bib162)]，You
    等 [[4](#bib.bib4)] |'
- en: '| $\bullet$ Novel losses: Zanfir et al. [[44](#bib.bib44)], Jiang et al. [[163](#bib.bib163)]
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 新颖损失：Zanfir 等 [[44](#bib.bib44)]，Jiang 等 [[163](#bib.bib163)] |'
- en: '| $\bullet$ Unsupervised learning: Madadi et al. [[164](#bib.bib164)], Yu et
    al. [[46](#bib.bib46)] |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 无监督学习：Madadi 等 [[164](#bib.bib164)]，Yu 等 [[46](#bib.bib46)] |'
- en: '| $\bullet$ Bilevel online adaptation: Guan et al. [[165](#bib.bib165)] |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 双层次在线自适应：Guan 等 [[165](#bib.bib165)] |'
- en: '| $\bullet$ Single-shot: Pose2UV [[166](#bib.bib166)] |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 单次：Pose2UV [[166](#bib.bib166)] |'
- en: '| $\bullet$ Contrastive learning: JOTR [[167](#bib.bib167)] |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 对比学习：JOTR [[167](#bib.bib167)] |'
- en: '| $\bullet$ Domain adaptation: Nam et al. [[168](#bib.bib168)] |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 领域自适应：Nam 等 [[168](#bib.bib168)] |'
- en: '| Detailed | With Clothes | $\bullet$ Alldieck et al. [[169](#bib.bib169)]
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 详细 | 带衣物 | $\bullet$ Alldieck 等 [[169](#bib.bib169)] |'
- en: '| $\bullet$ Multi-Garment Network (MGN) [[170](#bib.bib170)] |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 多服装网络（MGN） [[170](#bib.bib170)] |'
- en: '| $\bullet$ Texture map: Tex2Shape [[171](#bib.bib171)] |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 纹理图：Tex2Shape [[171](#bib.bib171)] |'
- en: '| $\bullet$ Layered garment representation: BCNet [[172](#bib.bib172)] |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 分层服装表示：BCNet [[172](#bib.bib172)] |'
- en: '| $\bullet$ Temporal span: H4D [[37](#bib.bib37)] |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 时间跨度：H4D [[37](#bib.bib37)] |'
- en: '| With Hands | $\bullet$ Linguistic priors: SGNify [[173](#bib.bib173)] |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 带手部 | $\bullet$ 语言先验：SGNify [[173](#bib.bib173)] |'
- en: '| $\bullet$ Two-hands interaction: [[174](#bib.bib174)] |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 双手交互：[[174](#bib.bib174)] |'
- en: '| $\bullet$ Hand-object interaction: [[175](#bib.bib175)] |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 手部与物体交互：[[175](#bib.bib175)] |'
- en: '| Whole Body | $\bullet$ PROX [[176](#bib.bib176)] |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 全身 | $\bullet$ PROX [[176](#bib.bib176)] |'
- en: '| $\bullet$ ExPose [[177](#bib.bib177)] |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ ExPose [[177](#bib.bib177)] |'
- en: '| $\bullet$ FrankMocap [[178](#bib.bib178)] |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ FrankMocap [[178](#bib.bib178)] |'
- en: '| $\bullet$ PIXIE [[179](#bib.bib179)] |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ PIXIE [[179](#bib.bib179)] |'
- en: '| $\bullet$ Moon et al. [[180](#bib.bib180)] |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ Moon 等 [[180](#bib.bib180)] |'
- en: '| $\bullet$ PyMAF [[181](#bib.bib181)] |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ PyMAF [[181](#bib.bib181)] |'
- en: '| $\bullet$ OSX [[182](#bib.bib182)] |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: $\bullet$ OSX [[182](#bib.bib182)] |
- en: '| $\bullet$ HybrIK-X [[183](#bib.bib183)] |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ HybrIK-X [[183](#bib.bib183)] |'
- en: '| Template-free | Regression-based | $\bullet$ FACSIMILE [[184](#bib.bib184)],
    PeeledHuman [[185](#bib.bib185)], GTA [[186](#bib.bib186)], NSF [[187](#bib.bib187)]
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 无模板 | 基于回归 | $\bullet$ FACSIMILE [[184](#bib.bib184)], PeeledHuman [[185](#bib.bib185)],
    GTA [[186](#bib.bib186)], NSF [[187](#bib.bib187)] |'
- en: '| Optimization-based Differentiable | $\bullet$ DiffPhy [[188](#bib.bib188)],
    AG3D [[189](#bib.bib189)] |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: 基于优化的可微分 | $\bullet$ DiffPhy [[188](#bib.bib188)], AG3D [[189](#bib.bib189)]
    |
- en: '| Implicit Representations | $\bullet$ PIFu [[190](#bib.bib190)], PIFuHD [[191](#bib.bib191)]
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 隐式表示 | $\bullet$ PIFu [[190](#bib.bib190)], PIFuHD [[191](#bib.bib191)] |'
- en: '| $\bullet$ Canonical space: ARCH [[192](#bib.bib192)], ARCH++ [[193](#bib.bib193)],
    CAR [[194](#bib.bib194)] |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: $\bullet$ 规范空间：ARCH [[192](#bib.bib192)], ARCH++ [[193](#bib.bib193)], CAR [[194](#bib.bib194)]
    |
- en: '| $\bullet$ Geometric priors: GeoPIFu [[195](#bib.bib195)] |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: $\bullet$ 几何先验：GeoPIFu [[195](#bib.bib195)] |
- en: '| $\bullet$ Novel representations: Peng et al. [[196](#bib.bib196)], 3DNBF
    [[197](#bib.bib197)] |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: $\bullet$ 新颖表征：彭等人 [[196](#bib.bib196)], 3DNBF [[197](#bib.bib197)] |
- en: '| Neural Radiance Fields | $\bullet$ Volume deformation scheme [[198](#bib.bib198)]
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: 神经辐射场 | $\bullet$ 体积变形方案 [[198](#bib.bib198)] |
- en: '| $\bullet$ ActorsNeRF [[47](#bib.bib47)] |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: $\bullet$ ActorsNeRF [[47](#bib.bib47)] |
- en: '| Diffusion Models | $\bullet$ HMDiff [[199](#bib.bib199)] |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: 扩散模型 | $\bullet$ HMDiff [[199](#bib.bib199)] |
- en: '| Implicit + Explicit | $\bullet$ HMD [[200](#bib.bib200)], IP-Net [[201](#bib.bib201)],
    PaMIR [[38](#bib.bib38)], Zhu et al. [[202](#bib.bib202)], ICON [[203](#bib.bib203)],
    ECON [[204](#bib.bib204)], DELTA [[9](#bib.bib9)], GETAvatar [[205](#bib.bib205)]
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 隐式 + 显式 | $\bullet$ HMD [[200](#bib.bib200)], IP-Net [[201](#bib.bib201)],
    PaMIR [[38](#bib.bib38)], 朱等人 [[202](#bib.bib202)], ICON [[203](#bib.bib203)],
    ECON [[204](#bib.bib204)], DELTA [[9](#bib.bib9)], GETAvatar [[205](#bib.bib205)]
    |'
- en: '| Diffusion + Explicit | $\bullet$ DINAR [[206](#bib.bib206)] |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Diffusion + 显式 | $\bullet$ DINAR [[206](#bib.bib206)] |'
- en: '| NeRF + Explicit | $\bullet$ TransHuman [[207](#bib.bib207)] |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| NeRF + 显式 | $\bullet$ TransHuman [[207](#bib.bib207)] |'
- en: '| Gaussian Splatting + Explicit | $\bullet$ Animatable 3D Gaussian [[208](#bib.bib208)]
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 高斯点图 + 显式 | $\bullet$ 可动态的3D高斯 [[208](#bib.bib208)] |'
- en: 5.1 Template-based human mesh recovery
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基于模板的人体网格恢复
- en: 5.1.1 Naked human body recovery
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 裸体人体恢复
- en: Multimodal Methods. Multimodality in human mesh recovery harnesses the potential
    by combining various modalities of data, such as RGB images, depth information,
    and optical flow. Integrating data in different modalities can significantly enhance
    the robustness and precision of mesh recovery. Rong et al. [[123](#bib.bib123)]
    proposed a hybrid annotation method with a hybrid training strategy for human
    mesh recovery to reduce annotation costs, which effectively utilizes various types
    of heterogeneous annotations, including 3D and 2D annotations, body part segmentation,
    and dense correspondence. Deep Two-Stream Video Inference for Human Body Pose
    and Shape Estimation (DTS-VIBE) [[124](#bib.bib124)] method redefines the task
    as a multimodal problem, merging RGB data with optical flow to achieve a more
    reliable estimation and address temporal inconsistencies from RGB videos. LASOR
    [[125](#bib.bib125)] estimates 3D pose and shape by synthesizing occlusion-aware
    silhouettes and 2D keypoints data in scenes with inter-person occlusion. CLIFF
    [[126](#bib.bib126)] utilizes cropped images and bounding box information from
    the pre-training phase as inputs to enhance the accuracy of global rotation estimation
    in the camera coordinate system.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态方法。人体网格恢复的多模态性通过结合各种数据的模态，如RGB图像、深度信息和光流，发挥潜力。整合不同模态的数据可以显著增强网格恢复的稳健性和精度。Rong等人
    [[123](#bib.bib123)] 提出了一种混合注释方法和一种混合训练策略，用于人体网格恢复，以降低注释成本，有效利用各种异质注释类型，包括3D和2D注释，身体部位分割和密集对应关系。深度双流视频推理用于人体姿势和形状估计（DTS-VIBE）[[124](#bib.bib124)]方法重新定义了任务作为一个多模态问题，将RGB数据与光流合并，以实现更可靠的估计，并解决来自RGB视频的时间不一致性。LASOR
    [[125](#bib.bib125)] 通过合成具有遮挡感知的轮廓和场景中的2D关键点数据，估计3D姿势和形状，用于解决人与人之间的遮挡问题。CLIFF
    [[126](#bib.bib126)] 利用预训练阶段的裁剪图像和边界框信息作为输入，以增强相机坐标系中全局旋转估计的准确性。
- en: Utilizing Attention Mechanism. Transformer [[209](#bib.bib209)] as a self-attention
    model has demonstrated remarkable success in Natural Language Processing (NLP)
    [[210](#bib.bib210)] [[211](#bib.bib211)]. Following this, the Vision Transformer
    (ViT) [[42](#bib.bib42)] has mirrored these successes in the field of computer
    vision [[41](#bib.bib41)] [[212](#bib.bib212)], and numerous Transformer-based
    methods are now being employed in mesh recovery. The attention mechanism serves
    to amplify the importance of certain parts in the neural network. To address the
    partial occlusion issues, Part Attention REgressor (PARE) [[127](#bib.bib127)]
    leverages the relationships between body parts derived from segmentation masks,
    prompting the network to enhance predictions for occluded body parts. Mesh Graphormer
    [[128](#bib.bib128)] utilizes a GCNN-reinforced transformer for estimating 3D
    mesh vertices and body joints, while a GCNN is employed to infer interactions
    among neighboring vertices based on pre-existing mesh topology. The architecture
    effectively merges graph-based networking with the attention mechanism of transformers,
    enabling the modeling of local and global interactions. MPS-Net [[129](#bib.bib129)]
    employs motion continuity attention to capture temporal coherence. This approach
    then leverages a hierarchical attentive feature mechanism to combine features
    from temporally adjacent representations more effectively. FastMETRO [[131](#bib.bib131)]
    leverages self-attention mechanisms for non-adjacent vertices based on the topology
    of the body’s triangular mesh to minimize memory overhead and accelerate inference
    speed. Xue et al. [[132](#bib.bib132)] proposed a learnable sampling module for
    human mesh recovery to reduce inherent depth ambiguity. This module aggregates
    global information by generating joint adaptive tokens, utilizing non-local information
    within the input image. METRO [[133](#bib.bib133)] is a mesh transformer for end-to-end
    human mesh recovery, in which the encoder captures interactions between vertices
    and joints, and the decoder outputs 3D joint coordinates and the mesh structure.
    Qiu et al. [[130](#bib.bib130)] proposed an end-to-end Transformer-based method
    for multi-person human mesh recovery in videos. This approach utilizes a spatio-temporal
    encoder to extract global features, followed by a spatio-temporal pose and shape
    decoder to predict human pose and mesh.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 利用注意力机制。Transformer [[209](#bib.bib209)] 作为自注意力模型在自然语言处理（NLP）领域取得了显著成功 [[210](#bib.bib210)]
    [[211](#bib.bib211)]。随后，Vision Transformer (ViT) [[42](#bib.bib42)] 在计算机视觉领域也取得了类似的成功
    [[41](#bib.bib41)] [[212](#bib.bib212)]，目前许多基于 Transformer 的方法正被应用于网格恢复。注意力机制用于放大神经网络中某些部分的重要性。为了解决部分遮挡问题，Part
    Attention REgressor (PARE) [[127](#bib.bib127)] 利用从分割掩膜中得出的身体部位之间的关系，促使网络增强对遮挡身体部位的预测。Mesh
    Graphormer [[128](#bib.bib128)] 利用 GCNN 强化的 Transformer 来估计 3D 网格顶点和身体关节，同时使用
    GCNN 推断基于预先存在网格拓扑的邻近顶点之间的交互。该架构有效地将基于图的网络与 Transformer 的注意力机制结合起来，实现了局部和全局交互的建模。MPS-Net
    [[129](#bib.bib129)] 采用运动连续性注意力来捕捉时间一致性。该方法利用层次化的注意力特征机制，更有效地结合来自时间上相邻表示的特征。FastMETRO
    [[131](#bib.bib131)] 基于身体三角网格的拓扑结构利用自注意力机制来处理非相邻顶点，以减少内存开销并加速推理速度。Xue 等 [[132](#bib.bib132)]
    提出了一个可学习的采样模块，用于人类网格恢复，以减少固有的深度歧义。该模块通过生成关节自适应标记来聚合全局信息，利用输入图像中的非局部信息。METRO [[133](#bib.bib133)]
    是一个用于端到端人类网格恢复的网格 Transformer，其中编码器捕获顶点和关节之间的交互，解码器输出 3D 关节坐标和网格结构。Qiu 等 [[130](#bib.bib130)]
    提出了一个基于 Transformer 的端到端方法，用于视频中的多人体网格恢复。该方法利用时空编码器提取全局特征，然后通过时空姿态和形状解码器来预测人体姿态和网格。
- en: Exploiting Temporal Information. With the advancement of video technology, video-based
    human mesh recovery methods that extract temporal information from adjacent frames
    have shown increased potential. To learn the 3D dynamics of the human body more
    accurately and effectively, Kanazawa et al. [[134](#bib.bib134)] proposed a framework
    that produces smooth 3D meshes from videos. This framework also predicts past
    and future 3D motions by temporally encoding image features. Video Inference for
    Body pose and shape Estimation (VIBE) [[135](#bib.bib135)] estimates kinematically
    plausible motion sequences through self-attention mechanism-based temporal network
    and adversarial training without requiring any ground-truth 3D labels. To diminish
    the dependency on static features in the current frame, addressing a limitation
    of previous temporal-based methods, Choi et al. [[136](#bib.bib136)] developed
    the Temporally Consistent Mesh Recovery (TCMR) system. The system utilizes temporal
    information to ensure consistency and effectively recovers smooth 3D human motion
    by incorporating data from past and future frames. Multi-level Attention Encoder-Decoder
    (MAED) network [[137](#bib.bib137)] captures relationships at multiple levels,
    including the spatial-temporal and human joint levels, through its multi-level
    attention mechanism. Wang et al. [[138](#bib.bib138)] introduced the Temporally
    embedded 3D human body Pose and shape estimation (TePose) method, specifically
    tailored for live stream videos. They designed a motion discriminator for adversarial
    training, utilizing datasets without any 3D labels, through a multi-scale spatio-temporal
    graph convolutional network. Additionally, they employed a sequential data loading
    strategy to accommodate the unique start-to-end data processing requirements of
    live streaming. To effectively balance the learning of short-term and long-term
    temporal correlations, Global-to-Local Transformer (GLoT) [[139](#bib.bib139)]
    structurally decouples the modeling of long-term and short-term correlations.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 利用时间信息。随着视频技术的进步，从相邻帧中提取时间信息的视频人类网格恢复方法展示了更大的潜力。为了更准确有效地学习人体的3D动态，Kanazawa 等人[[134](#bib.bib134)]
    提出了一个从视频中生成平滑3D网格的框架。该框架还通过时间编码图像特征来预测过去和未来的3D动作。Body pose and shape Estimation的Video
    Inference（VIBE）[[135](#bib.bib135)] 通过基于自注意力机制的时间网络和对抗训练来估计运动学上合理的运动序列，而不需要任何真实的3D标签。为了减少对当前帧中静态特征的依赖，解决了先前基于时间的方法的一个限制，Choi
    等人[[136](#bib.bib136)] 开发了时间一致网格恢复（TCMR）系统。该系统利用时间信息以确保一致性，并通过结合过去和未来帧的数据有效恢复平滑的3D人类动作。多级注意力编码解码器（MAED）网络[[137](#bib.bib137)]
    通过其多级注意力机制捕捉包括空间-时间和人体关节层次在内的多个层次的关系。Wang 等人[[138](#bib.bib138)] 提出了专门为实时流媒体视频量身定制的时间嵌入3D人体姿态和形状估计（TePose）方法。他们设计了一个用于对抗训练的运动鉴别器，通过一个多尺度空间-时间图卷积网络，利用没有3D标签的数据集。此外，他们采用了序列数据加载策略，以适应实时流媒体特有的从头到尾的数据处理需求。为了有效平衡短期和长期时间相关性的学习，Global-to-Local
    Transformer（GLoT）[[139](#bib.bib139)] 从结构上解耦了长期和短期相关性的建模。
- en: Multi-view Methods. Dong et al. [[140](#bib.bib140)] designed a practical multi-view
    framework that combines 2D observations from multi-view images into a unified
    3D representation for individual instances using a confidence-aware majority voting
    mechanism. Sengupta et al. [[141](#bib.bib141)] introduced a probabilistic-based
    multi-view method without constraints such as specific target poses, viewpoints,
    or background conditions across image sequences. Huang et al. [[31](#bib.bib31)]
    proposed a dynamic physics-geometry consistency approach for multi-person multi-view
    mesh recovery. This method integrates motion priors, extrinsic camera parameters,
    and human mesh data to mitigate the impact of noisy human semantic data. Zhuo
    et al. [[142](#bib.bib142)] proposed a cross-view fusion method that predicts
    foot posture by achieving a more refined 3D intermediate representation and alleviating
    inconsistencies across different views.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 多视角方法。Dong 等人[[140](#bib.bib140)] 设计了一个实用的多视角框架，该框架通过自信感知的多数投票机制将多视角图像中的2D观察数据合并成一个统一的3D表示，用于单个实例。Sengupta
    等人[[141](#bib.bib141)] 提出了一个基于概率的多视角方法，无需特定目标姿态、视角或图像序列中的背景条件等约束。Huang 等人[[31](#bib.bib31)]
    提出了一个动态物理-几何一致性的方法用于多人人物的多视角网格恢复。该方法整合了运动先验、外部相机参数和人类网格数据，以减轻噪声人类语义数据的影响。Zhuo
    等人[[142](#bib.bib142)] 提出了一个交叉视角融合方法，通过实现更精细的3D中间表示并缓解不同视角间的不一致性来预测脚部姿态。
- en: Boosting Efficiency. Maintaining excellent performance with a lightweight model
    and low computational cost is essential and challenging, especially in applications
    such as wearable devices, power-limited systems, and edge computing. SCOPE [[143](#bib.bib143)]
    efficiently computes the Gauss-Newton direction using 2D and 3D keypoints for
    human mesh recovery. The method capitalizes on inherent sparsity and employs a
    sparse constrained formulation, achieving real-time performance at over 30 FPS.
    To implement a single-stage multi-person human mesh recovery model, Body Meshes
    as Points (BMP) [[144](#bib.bib144)] represents multiple people as points and
    associates each with a single body mesh to significantly improve efficiency and
    performance. HeatER [[145](#bib.bib145)] processes heatmap inputs directly to
    reduce memory and computational costs. Dou et al. [[146](#bib.bib146)] designed
    an efficient transformer for human mesh recovery to reduce model complexity and
    computational cost by removing redundant tokens.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 提升效率。保持出色的性能同时使用轻量级模型和低计算成本是至关重要且具有挑战性的，特别是在可穿戴设备、电力受限系统和边缘计算等应用中。SCOPE [[143](#bib.bib143)]
    利用2D和3D关键点高效计算高斯-牛顿方向用于人体网格恢复。该方法利用了固有的稀疏性，并采用了稀疏约束公式，实现了超过30 FPS的实时性能。为了实现单阶段多人的人体网格恢复模型，Body
    Meshes as Points (BMP) [[144](#bib.bib144)] 将多个人表示为点，并将每个点与单一身体网格关联，从而显著提高了效率和性能。HeatER
    [[145](#bib.bib145)] 直接处理热图输入，以减少内存和计算成本。Dou等人 [[146](#bib.bib146)] 设计了一种高效的变换器用于人体网格恢复，通过去除冗余的标记来降低模型复杂性和计算成本。
- en: Developing Various Representations. Stable and effective feature representation
    is crucial for enhancing the capabilities of deep learning algorithms in human
    mesh recovery, as it enables the efficient extraction of meaningful patterns from
    complex input data. Based on the assumption that image texture remains constant
    across frames, TexturePose [[147](#bib.bib147)] leverages the appearance constancy
    of the body across different frames. It measures the texture map for each frame
    using a texture consistency loss. DenseRaC [[213](#bib.bib213)] generates a pixel-to-surface
    correspondence map to optimize the estimation of parameterized human pose and
    shape. Zhang et al. [[148](#bib.bib148)] established a connection between 2D pixels
    and 3D vertices by using dense correspondences of body parts, effectively addressing
    related issues. Their proposed DaNet model concentrates on learning the 2D-to-3D
    mapping, while the PartDrop strategy ensures that the model focuses more on complementary
    body parts and adjacent positional features. To address the lack of dense correspondence
    between image features and the 3D mesh, DecoMR [[149](#bib.bib149)] recovers the
    human mesh by establishing a pixel-to-surface dense correspondence map in the
    UV space. Zhang et al. [[150](#bib.bib150)] designed a two-branch network that
    utilizes a partial UV map to represent the human body when occluded by objects,
    effectively converting this map into an estimation of the 3D human shape. Sun
    et al. [[151](#bib.bib151)] proposed a body-center-guided representation method
    that predicts both the body center heatmap and the mesh parameter map. This approach
    describes the 3D body mesh at the pixel level, enabling one-stage multi-person
    3D mesh regression. 3DCrowdNet [[152](#bib.bib152)] employs a joint-based regressor
    to isolate target features from others for recovering multi-person meshes in crowded
    scenes, which utilizes a crowded scene-robust image feature heatmap instead of
    the full feature map within a bounding box. Dual-stream Spatio-temporal Transformer
    (DSTformer) [[153](#bib.bib153)] extracts long-range spatio-temporal relationships
    among skeletal joints to effectively capture unified human motion representations
    from large-scale and heterogeneous video data for human-centric tasks, such as
    human mesh recovery.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 发展多种表示形式。稳定且有效的特征表示对提升深度学习算法在人体网格恢复中的能力至关重要，因为它能够有效地从复杂的输入数据中提取有意义的模式。基于图像纹理在各帧之间保持不变的假设，TexturePose
    [[147](#bib.bib147)] 利用身体在不同帧中的外观一致性。它通过纹理一致性损失来测量每帧的纹理图。DenseRaC [[213](#bib.bib213)]
    生成一个像素到表面对应关系图，以优化参数化的人体姿态和形状的估计。张等人 [[148](#bib.bib148)] 通过使用身体部位的密集对应关系建立了2D像素与3D顶点之间的联系，有效地解决了相关问题。他们提出的
    DaNet 模型专注于学习2D到3D的映射，而 PartDrop 策略确保模型更多地关注互补的身体部位和相邻的位置信息。为了解决图像特征与3D网格之间缺乏密集对应关系的问题，DecoMR
    [[149](#bib.bib149)] 通过在 UV 空间中建立像素到表面的密集对应关系图来恢复人体网格。张等人 [[150](#bib.bib150)]
    设计了一个双分支网络，该网络利用部分 UV 图来表示被物体遮挡的人体，有效地将该图转换为对3D人体形状的估计。孙等人 [[151](#bib.bib151)]
    提出了一个以身体中心为引导的表示方法，该方法预测身体中心热图和网格参数图。这种方法在像素级别描述3D身体网格，从而实现了一阶段的多人体3D网格回归。3DCrowdNet
    [[152](#bib.bib152)] 采用基于关节的回归器从其他特征中隔离目标特征，以恢复拥挤场景中的多人体网格，它利用了拥挤场景鲁棒的图像特征热图，而不是边界框内的完整特征图。双流时空变换器（DSTformer）
    [[153](#bib.bib153)] 提取骨骼关节之间的长距离时空关系，以有效捕捉来自大规模和异质视频数据的统一人体运动表示，适用于以人为中心的任务，如人体网格恢复。
- en: Utilizing Structural Information. The structural information of the body acts
    as unique prior knowledge in human mesh recovery, enhancing the understanding
    of body relations. Furthermore, introducing additional physical constraints, which
    describe the interrelationships between various body structures, can significantly
    improve the performance. Holopose [[154](#bib.bib154)] employs a part-based multi-task
    regression network for 2D, 3D, and dense pose to estimate 3D human surfaces. Sun
    et al. [[155](#bib.bib155)] introduced an end-to-end method for human mesh recovery
    from single images and monocular videos. This method employs skeleton disentangling
    to reduce the complexity of decoupling and incorporates temporal coherence, efficiently
    capturing both short and long-term temporal cues. Hybrid Inverse Kinematics (HybrIK)
    [[156](#bib.bib156)] calculates the swing rotation from 3D joints and employs
    a network to predict the twist rotation through the twist-and-swing decomposition.
    The method can tackle non-linearity challenges and misalignment between images
    and models in mesh estimation from 3D poses. In their later work, Li et al. [[157](#bib.bib157)]
    designed NIKI, a model capable of learning from both the forward and inverse processes
    using invertible networks. To address the non-linear mapping and drifting joint
    position issues, Lee et al. [[158](#bib.bib158)] introduced an uncertainty-aware
    method for human mesh recovery, leveraging information from 2D poses to address
    the inherent ambiguities in 2D. Sengupta et al. [[159](#bib.bib159)] presented
    a probabilistic approach to circumvent the ill-posed problem, which integrates
    the kinematic tree structure of the human body with a Gaussian distribution over
    SMPL parameters. This method then predicts the hierarchical matrix-fisher distribution
    of 3D joint rotation matrices. SGRE [[160](#bib.bib160)] estimates the global
    rotation matrix of joints directly to avoid error accumulation along the kinematic
    chains in human mesh recovery.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 利用结构信息。身体的结构信息在人体网格恢复中充当独特的先验知识，增强了对身体关系的理解。此外，加入额外的物理约束，这些约束描述了各种身体结构之间的相互关系，可以显著提高性能。Holopose
    [[154](#bib.bib154)] 采用基于部件的多任务回归网络来估计 2D、3D 和密集姿态，从而估计 3D 人体表面。Sun 等人 [[155](#bib.bib155)]
    提出了一个从单张图像和单目视频中恢复人体网格的端到端方法。该方法采用骨架解缠技术来减少解耦的复杂性，并结合了时间一致性，有效捕捉了短期和长期的时间线索。Hybrid
    Inverse Kinematics (HybrIK) [[156](#bib.bib156)] 从 3D 关节计算摆动旋转，并使用网络通过扭转和摆动分解来预测扭转旋转。该方法可以应对非线性挑战和在从
    3D 姿态中进行网格估计时图像与模型之间的错位问题。在他们的后续工作中，Li 等人 [[157](#bib.bib157)] 设计了 NIKI，这是一种能够从正向和逆向过程学习的模型，使用可逆网络。为了应对非线性映射和关节位置漂移问题，Lee
    等人 [[158](#bib.bib158)] 引入了一种不确定性感知的方法来恢复人体网格，利用来自 2D 姿态的信息解决 2D 中固有的模糊性。Sengupta
    等人 [[159](#bib.bib159)] 提出了一种概率方法来规避不适定问题，该方法将人体的运动学树结构与 SMPL 参数的高斯分布结合在一起。该方法然后预测
    3D 关节旋转矩阵的层次矩阵-费舍尔分布。SGRE [[160](#bib.bib160)] 直接估计关节的全局旋转矩阵，以避免在人体网格恢复中的运动链中误差累积。
- en: 'Choosing Appropriate Learning Strategies. SPIN [[161](#bib.bib161)] incorporates
    an initial estimate optimization routine into the training loop by the self-improving
    neural network, which can fit the body mesh estimate to 2D joints. Zanfir et al.
    [[44](#bib.bib44)] developed a method integrating kinematic latent normalizing
    flow representations and dynamical models with structured, differentiable, semantic
    body part alignment loss functions aimed at enhancing semi-supervised and self-supervised
    3D human pose and shape estimation. Jiang et al. [[163](#bib.bib163)] introduced
    two novel loss functions for multi-person mesh recovery from single images: a
    distance field-based collision loss penalizing interpenetration between constructed
    figures and a depth ordering-aware loss addressing occlusions and promoting accurate
    depth ordering of targets. Madadi et al. [[164](#bib.bib164)] presented an unsupervised
    denoising autoencoder network to recover invisible landmarks using sparse motion
    capture data effectively. To tackle the challenges of pose failure and shape ambiguity
    in the unsupervised human mesh recovery task, Yu et al. [[46](#bib.bib46)] devised
    a strategy that decouples the task into unsupervised 3D pose estimation and leverages
    kinematic prior knowledge. Bilevel Online Adaptation (BOA) [[214](#bib.bib214)]
    employs bilevel optimization to reconcile conflicts between 2D and temporal constraints
    in out-of-domain streaming videos human mesh recovery. In their later work, Dynamic
    Bilevel Online Adaptation (DBOA) [[165](#bib.bib165)] integrates temporal constraints
    to compensate for the absence of 3D annotations. Huang et al. [[166](#bib.bib166)]
    developed Pose2UV, a single-shot human mesh recovery method capable of extracting
    target features under occlusions using a deep UV prior. JOTR[[167](#bib.bib167)]
    fuses 2D and 3D features and incorporates supervision for the 3D feature through
    a Transformer-based contrastive learning framework. ReFit [[162](#bib.bib162)]
    reprojects keypoints and refines the human model via a feedback-update loop mechanism.
    You et al. [[4](#bib.bib4)] introduced a co-evolution method for human mesh recovery
    that utilizes 3D pose as an intermediary. This method divides the process into
    two distinct stages: initially, it estimates the 3D human pose from video, and
    subsequently, it regresses mesh vertices based on the estimated 3D pose, combined
    with temporal image features. To bridge the gap between training and test data,
    CycleAdapt [[168](#bib.bib168)] proposed a domain adaptation method including
    a mesh reconstruction network and a motion denoising network enabling more effective
    adaptation.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适当的学习策略。SPIN [[161](#bib.bib161)] 通过自我改进的神经网络将初始估计优化例程融入训练循环中，可以将身体网格估计拟合到2D关节上。Zanfir
    等人 [[44](#bib.bib44)] 开发了一种方法，集成了运动学潜在归一化流表示和具有结构化、可微分的语义身体部位对齐损失函数的动态模型，旨在增强半监督和自监督的3D人体姿势和形状估计。Jiang
    等人 [[163](#bib.bib163)] 引入了两个新颖的损失函数用于从单张图像中恢复多人网格：基于距离场的碰撞损失，惩罚构建图形之间的交叉，以及深度排序感知损失，解决遮挡问题并促进目标的准确深度排序。Madadi
    等人 [[164](#bib.bib164)] 提出了一个无监督的去噪自编码网络，有效地利用稀疏运动捕捉数据恢复不可见的地标。为了应对无监督人体网格恢复任务中的姿势失败和形状模糊问题，Yu
    等人 [[46](#bib.bib46)] 设计了一种策略，将任务解耦为无监督的3D姿势估计，并利用运动学先验知识。双层在线适应（BOA） [[214](#bib.bib214)]
    采用双层优化来协调2D和时序约束之间的冲突，用于领域外流媒体视频中的人体网格恢复。在他们的后续工作中，动态双层在线适应（DBOA） [[165](#bib.bib165)]
    集成了时间约束，以弥补缺少3D注释的问题。Huang 等人 [[166](#bib.bib166)] 开发了Pose2UV，一种单次人类网格恢复方法，能够使用深度UV先验在遮挡下提取目标特征。JOTR
    [[167](#bib.bib167)] 融合了2D和3D特征，并通过基于Transformer的对比学习框架对3D特征进行监督。ReFit [[162](#bib.bib162)]
    通过反馈更新循环机制重新投影关键点并优化人类模型。You 等人 [[4](#bib.bib4)] 提出了一个用于人类网格恢复的共演化方法，该方法利用3D姿势作为中介。该方法将过程分为两个不同的阶段：首先，从视频中估计3D人体姿势，然后根据估计的3D姿势回归网格顶点，并结合时间图像特征。为了弥合训练数据和测试数据之间的差距，CycleAdapt
    [[168](#bib.bib168)] 提出了一个领域适应方法，包括一个网格重建网络和一个运动去噪网络，从而实现更有效的适应。
- en: 5.1.2 Detailed human body recovery
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 详细的人体恢复
- en: 'Model-based human mesh recovery has moderately satisfactory results but still
    lacks detailed body representations; expansions on the naked parametric model
    now allow for parameterized depictions of various body parts and details, including
    clothing [[172](#bib.bib172), [37](#bib.bib37)], hands [[34](#bib.bib34)], face
    [[35](#bib.bib35)], and the entire body [[36](#bib.bib36)], as discussed in Section
    [2.2](#S2.SS2 "2.2 Representation for human body ‣ 2 Background ‣ Deep Learning
    for 3D Human Pose Estimation and Mesh Recovery: A Survey").'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '基于模型的人体网格恢复结果适中，但仍缺乏详细的身体表示；对裸参数化模型的扩展现已允许对各种身体部位和细节，包括衣物 [[172](#bib.bib172),
    [37](#bib.bib37)]、手部 [[34](#bib.bib34)]、面部 [[35](#bib.bib35)]以及整个身体 [[36](#bib.bib36)]，进行参数化描述，详细内容在第[2.2节](#S2.SS2
    "2.2 Representation for human body ‣ 2 Background ‣ Deep Learning for 3D Human
    Pose Estimation and Mesh Recovery: A Survey")中讨论。'
- en: With Clothes. Alldieck et al. [[169](#bib.bib169)] estimated the parameters
    of the SMPL model, including clothing and hair, from 1 to 8 frames of a monocular
    video. Bhatnagar et al. [[170](#bib.bib170)] developed the Multi-Garment Network
    (MGN) to reconstruct body shape and clothing layers on top of the SMPL model.
    Tex2Shape [[171](#bib.bib171)] converts the human body mesh regression problem
    into an image-to-image estimation task. Specifically, it predicts a partial texture
    map of the visible region and then reconstructs the body shape, adding details
    to visible and occluded parts. BCNet [[172](#bib.bib172)] features a layered garment
    representation atop the SMPL model and innovatively decouples the skinning weight
    of the garment from the body mesh. Jiang et al. [[37](#bib.bib37)] introduced
    a method that employs a temporal span, SMPL parameters of shape and initial pose,
    and latent codes encoding motion and auxiliary information. This approach facilitates
    the recovery of detailed body shapes, including visible and occluded parts, by
    utilizing a texture map of the visible region.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**Alldieck**等人[[169](#bib.bib169)]估计了SMPL模型的参数，包括衣物和头发，从1到8帧的单目视频中提取数据。**Bhatnagar**等人[[170](#bib.bib170)]开发了Multi-Garment
    Network (MGN)，以在SMPL模型的基础上重建身体形状和衣物层次。**Tex2Shape** [[171](#bib.bib171)]将人体网格回归问题转换为图像到图像的估计任务。具体来说，它预测可见区域的部分纹理图，然后重建身体形状，添加到可见和遮挡部分的细节。**BCNet**
    [[172](#bib.bib172)] 在SMPL模型上具有分层的衣物表示，并创新性地将衣物的蒙皮权重与身体网格解耦。**Jiang**等人[[37](#bib.bib37)]提出了一种方法，利用时间跨度、SMPL的形状和初始姿势参数以及编码运动和辅助信息的潜在代码。这种方法通过利用可见区域的纹理图来促进详细身体形状的恢复，包括可见和遮挡部分。'
- en: With Hands. Forte et al. [[173](#bib.bib173)] proposed SGNify, a model that
    captures hand pose, facial expression, and body movement from sign language videos.
    It employs linguistic priors and constraints on 3D hand pose to effectively address
    the ambiguities in isolated signs. Additionally, the relationship between Two-Hands
    [[174](#bib.bib174)], and Hand-Object [[175](#bib.bib175)] effectively reconstructs
    the hand’s details.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**Forte**等人[[173](#bib.bib173)]提出了SGNify，一个从手语视频中捕捉手势、面部表情和身体动作的模型。它采用语言学先验和对3D手势的约束，有效地解决了孤立手势中的歧义。此外，Two-Hands
    [[174](#bib.bib174)]和Hand-Object [[175](#bib.bib175)]之间的关系有效地重建了手部的细节。'
- en: Whole Body. To address the inconsistency between 3D human mesh and 3D scenes,
    Hassan et al. [[176](#bib.bib176)] introduced a human whole body and scenes recovery
    method named Proximal Relationships with Object eXclusion (PROX). EXpressive POse
    and Shape rEgression (ExPose) framework [[177](#bib.bib177)] employs a body-driven
    attention mechanism and adopts a regression approach for holistic expressive body
    reconstruction to mitigate local optima issues in optimization-based methods.
    FrankMocap [[178](#bib.bib178)] operates by independently running 3D mesh recovery
    regression for face, hands, and body and subsequently combining the outputs through
    an integration module. PIXIE [[179](#bib.bib179)] integrates independent estimates
    from the body, face, and hands using the shared shape space of SMPL-X across all
    body parts. Moon et al. [[180](#bib.bib180)] developed an end-to-end framework
    for whole-body human mesh recovery named Hand4Whole, which employs joint features
    for 3D joint rotations to enhance the accuracy of 3D hand predictions. Zhang et
    al. [[181](#bib.bib181)] enhanced the PyMAF framework [[215](#bib.bib215)], developing
    PyMAF-X for the detailed reconstruction of full-body models. This advancement
    aims to resolve the misalignment issues in regression-based, one-stage human mesh
    recovery methods by employing a feature pyramid approach and refining the mesh-image
    alignment parameters. OSX [[182](#bib.bib182)] employs a simple yet effective
    component-aware transformer that includes a global body encoder and a local face/hand
    decoder instead of separate networks for each part. Li et al. [[183](#bib.bib183)]
    extended the HybrIK [[156](#bib.bib156)] framework and proposed HybrIK-X, a one-stage
    model based on a hybrid analytical-neural inverse kinematics framework to recover
    comprehensive whole-body meshes with details.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 整体身体。为了解决3D人体网格与3D场景之间的不一致性，Hassan等人[[176](#bib.bib176)] 提出了一个名为Proximal Relationships
    with Object eXclusion（PROX）的方法来进行人体全身和场景恢复。EXpressive POse and Shape rEgression（ExPose）框架[[177](#bib.bib177)]采用了基于身体的注意力机制，并采用回归方法进行整体的表达性身体重建，以缓解基于优化的方法中的局部最优问题。FrankMocap[[178](#bib.bib178)]通过独立运行面部、手部和身体的3D网格恢复回归，并通过集成模块将输出结果结合起来。PIXIE[[179](#bib.bib179)]利用SMPL-X共享形状空间将身体、面部和手部的独立估计整合在一起。Moon等人[[180](#bib.bib180)]开发了一个名为Hand4Whole的端到端框架，用于全身人体网格恢复，该框架采用关节特征进行3D关节旋转，以提高3D手部预测的准确性。Zhang等人[[181](#bib.bib181)]提升了PyMAF框架[[215](#bib.bib215)]，开发了PyMAF-X，以详细重建全身模型。这一进展旨在通过采用特征金字塔方法和优化网格-图像对齐参数来解决回归基于的一阶段人体网格恢复方法中的对齐问题。OSX[[182](#bib.bib182)]采用了一个简单而有效的组件感知变换器，包括一个全局身体编码器和一个局部面部/手部解码器，而不是为每个部分单独设置网络。Li等人[[183](#bib.bib183)]扩展了HybrIK[[156](#bib.bib156)]框架，提出了HybrIK-X，这是一种基于混合分析-神经逆向运动学框架的一阶段模型，用于恢复具有细节的全面身体网格。
- en: 5.2 Template-free human body recovery
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 无模板人体恢复
- en: Template-free methods for human mesh recovery, such as neural network regression
    models, optimization models based on differentiable rendering, implicit representation
    models, Neural Radiance Fields (NeRF), and Gaussian Splatting, demonstrate enhanced
    flexibility over template-based approaches, enabling the depiction of richer details.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 无模板的人体网格恢复方法，如神经网络回归模型、基于可微渲染的优化模型、隐式表示模型、神经辐射场（NeRF）和高斯溅射，展示了比基于模板的方法更强的灵活性，能够描绘更丰富的细节。
- en: '![Refer to caption](img/47ab4cf58dc5e5bc2adabec4d677b916.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/47ab4cf58dc5e5bc2adabec4d677b916.png)'
- en: (a) NeRF
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: （a）NeRF
- en: '![Refer to caption](img/64dcc3a06f1a6f85f981eaadcbe98e20.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/64dcc3a06f1a6f85f981eaadcbe98e20.png)'
- en: (b) 3D Guassian Splatting
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: （b）3D高斯溅射
- en: 'Figure 7: (a) NeRF; (b) 3D Guassian Splatting.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：（a）NeRF；（b）3D高斯溅射。
- en: Regression-based human mesh recovery bypasses the limitations and biases inherent
    in template-based models and directly outputs the template-free 3D models of the
    human body. This approach allows for a more dynamic and flexible generation of
    models, which can capture a wider variety of human body shapes and postures. FACSIMILE
    (FAX) utilizes an image-translation network to recover geometry at the original
    image resolution directly, bypassing the need for indirect output through representations.
    Jinka et al. [[185](#bib.bib185)] proposed a robust shape representation specifically
    for scenes with self-occlusions, where the method encodes the human body using
    peeled RGB and depth maps, significantly enhancing accuracy and efficiency during
    both training and inference. Neural Surface Fields (NSF) [[187](#bib.bib187)]
    models a continuous and flexible displacement field on the base surface for 3D
    clothed human mesh recovery from monocular depth. This approach adapts to base
    surfaces with varying resolutions and topologies without retraining during inference.
    Zhang et al. [[186](#bib.bib186)] introduced the Global-correlated 3D-decoupling
    Transformer for clothed Avatar reconstruction (GTA), a model that employs an encoder
    to capture globally-correlated image features and a decoder to decouple tri-plane
    features using cross-attention.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 基于回归的人体网格恢复绕过了模板模型固有的局限性和偏差，直接输出无模板的 3D 人体模型。这种方法允许更动态和灵活的模型生成，能够捕捉更多样的人体形状和姿态。FACSIMILE
    (FAX) 利用图像翻译网络直接在原始图像分辨率下恢复几何形状，绕过了通过表示间接输出的需求。Jinka 等人 [[185](#bib.bib185)] 提出了一个针对自遮挡场景的鲁棒形状表示，该方法使用剥离的
    RGB 和深度图编码人体，在训练和推理过程中显著提升了准确性和效率。Neural Surface Fields (NSF) [[187](#bib.bib187)]
    在基础表面上建模连续且灵活的位移场，用于从单目深度恢复 3D 穿衣人体网格。这种方法适应于具有不同分辨率和拓扑的基础表面，无需在推理过程中重新训练。Zhang
    等人 [[186](#bib.bib186)] 推出了用于穿衣化身重建的全局相关 3D 解耦 Transformer (GTA)，该模型使用编码器捕捉全局相关的图像特征，并使用交叉注意力解耦三平面特征。
- en: Optimization-based differentiable rendering integrates the rendering process
    into the optimization method by minimizing the rendering error. AG3D [[189](#bib.bib189)]
    captures the body’s and loose clothing’s shape and deformation by adopting a holistic
    3D generator and integrating geometric cues in the form of predicted 2D normal
    maps. Gärtner et al. [[188](#bib.bib188)] designed DiffPhy, a differentiable physics-based
    model that incorporates a physically plausible body representation with anatomical
    joint limits, significantly enhancing the performance and robustness of human
    body recovery.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 基于优化的可微渲染通过最小化渲染误差将渲染过程整合到优化方法中。AG3D [[189](#bib.bib189)] 通过采用整体 3D 生成器并将几何线索以预测的
    2D 法线图形式融入其中，捕捉身体及松散衣物的形状和变形。Gärtner 等人 [[188](#bib.bib188)] 设计了 DiffPhy，这是一种可微物理基础模型，它结合了具有解剖关节限制的物理上合理的身体表示，显著提升了人体恢复的性能和鲁棒性。
- en: Implicit representations do not directly represent an object’s geometric data,
    such as vertices or meshes, but rather define whether a point in space belongs
    to the object through a function. The advantage of implicit representations lies
    in their ability to compactly and flexibly represent complex shapes, including
    those with intricate topologies or discontinuities. Thus, they have achieved impressive
    outcomes in human body reconstruction. Saito et al. [[190](#bib.bib190)] proposed
    the Pixel-aligned Implicit Function (PIFu), an implicit representation that aligns
    pixels of 2D images with a 3D object, and designed an end-to-end deep learning
    framework for inferring both 3D surface and texture from a single image. PIFu
    was the first to apply implicit representation in human mesh recovery, enabling
    the reconstruction of geometric details of the human body in clothing. Subsequently,
    PIFuHD [[191](#bib.bib191)] extended the work of PIFu to 4K resolution images,
    further enhancing detail. Huang et al. [[192](#bib.bib192)] designed the Animatable
    Reconstruction of Clothed Humans (ARCH) method by creating a semantic space and
    a semantic deformation field. He et al. [[193](#bib.bib193)] introduced ARCH++,
    a co-supervising framework with cross-space consistency, which jointly estimates
    occupancy in both the posed and canonical spaces. They transform the human mesh
    recovery problem with implicit representation from pose space to canonical space
    for processing. However, this approach’s limitation is that it heavily relies
    on accurate pose estimation, and the clothing expression based on skinning weights
    still lacks sufficient naturalness in detail. GeoPIFu [[195](#bib.bib195)] learns
    latent voxel features using a structure-aware 3D U-Net incorporating geometric
    priors. To tackle the ill-posed problem in representation learning when views
    are incredibly sparse, Peng et al. [[196](#bib.bib196)] developed a novel human
    body representation. This representation assumes that the neural representations
    learned at different frames share latent codes anchored to a deformable mesh.
    Clothed Avatar Reconstruction (CAR) method [[194](#bib.bib194)] employs a learning-based
    implicit model to initially form the general shape in canonical space, followed
    by refining surface details through predicting non-rigid optimization deformation
    in the posed space. 3D-aware Neural Body Fitting (3DNBF) [[197](#bib.bib197)]
    addresses the challenges of occlusion and 2D-3D ambiguity by a volumetric human
    representation using Gaussian ellipsoidal kernels.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式表示并不直接表示对象的几何数据，如顶点或网格，而是通过函数定义空间中的某点是否属于对象。隐式表示的优势在于其能够紧凑且灵活地表示复杂的形状，包括具有复杂拓扑或不连续性的形状。因此，它们在人体重建中取得了令人印象深刻的成果。Saito
    等人 [[190](#bib.bib190)] 提出了像素对齐隐式函数（PIFu），这是一种隐式表示，将 2D 图像的像素与 3D 对象对齐，并设计了一个端到端深度学习框架，用于从单张图像推断
    3D 表面和纹理。PIFu 首次将隐式表示应用于人体网格恢复，使得可以在衣物中重建人体的几何细节。随后，PIFuHD [[191](#bib.bib191)]
    将 PIFu 的工作扩展到 4K 分辨率图像，进一步增强了细节。Huang 等人 [[192](#bib.bib192)] 通过创建语义空间和语义变形场设计了可动画重建的穿衣人类（ARCH）方法。He
    等人 [[193](#bib.bib193)] 引入了 ARCH++，一个具有跨空间一致性的共同监督框架，该框架在姿态空间和标准空间中联合估计占据情况。他们将隐式表示的人体网格恢复问题从姿态空间转换到标准空间进行处理。然而，这种方法的局限性在于它严重依赖于准确的姿态估计，而且基于蒙皮权重的衣物表现细节仍缺乏足够的自然性。GeoPIFu
    [[195](#bib.bib195)] 使用包含几何先验的结构感知 3D U-Net 学习潜在体素特征。为了解决在视图极其稀疏时表示学习中的病态问题，Peng
    等人 [[196](#bib.bib196)] 开发了一种新的人体表示。这种表示假设在不同帧学习到的神经表示共享锚定到可变形网格的潜在编码。穿衣化身重建（CAR）方法
    [[194](#bib.bib194)] 采用基于学习的隐式模型，初步形成标准空间中的一般形状，然后通过预测姿态空间中的非刚性优化变形来细化表面细节。3D
    觉知神经身体拟合（3DNBF） [[197](#bib.bib197)] 通过使用高斯椭球核的体积人体表示解决了遮挡和 2D-3D 模糊性的问题。
- en: 'As illustrated in Fig.[7(a)](#S5.F7.sf1 "In Figure 7 ‣ 5.2 Template-free human
    body recovery ‣ 5 3D Human Mesh Recovery ‣ Deep Learning for 3D Human Pose Estimation
    and Mesh Recovery: A Survey"), Neural Radiance Fields [[216](#bib.bib216)] is
    based on the principle of implicit representation, using neural networks to learn
    the continuous volumetric density and color distribution of a scene, allowing
    for generating a high-quality 3D model from arbitrary viewpoints. Gao et al. [[198](#bib.bib198)]
    utilized a specialized representation that combines a canonical Neural Radiance
    Field (NeRF) with a volume deformation scheme, enabling the recovery of novel
    views and poses for a person not seen during training. Mu et al. [[47](#bib.bib47)]
    introduce ActorsNeRF, a NeRF-based human representation for human mesh recovery
    from a few monocular images by encoding the category-level prior through parameter
    sharing with a 2-level canonical space.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[7(a)](#S5.F7.sf1 "图 7 ‣ 5.2 无模板人体重建 ‣ 5 3D 人体网格重建 ‣ 深度学习用于 3D 人体姿态估计和网格重建：综述")所示，神经辐射场（Neural
    Radiance Fields）[[216](#bib.bib216)] 基于隐式表示的原理，使用神经网络来学习场景的连续体积密度和颜色分布，从而能够从任意视角生成高质量的
    3D 模型。Gao 等人[[198](#bib.bib198)] 采用了一种结合经典神经辐射场（NeRF）和体积变形方案的特殊表示方法，使得能够恢复在训练过程中未见过的新视角和姿态。Mu
    等人[[47](#bib.bib47)] 介绍了 ActorsNeRF，这是一种基于 NeRF 的人体表示方法，通过在 2 级经典空间中进行参数共享来编码类别级先验，从而从少量单目图像中恢复人体网格。
- en: Diffusion models are based on a series of diffusion processes, transforming
    original data by adding random noise and then gradually removing this noise to
    generate new data through a reverse process. Human Mesh Diffusion (HMDiff) [[199](#bib.bib199)]
    treats mesh recovery as a reverse diffusion process, incorporates input-specific
    distribution information into the diffusion process, and introduces prior knowledge
    to simplify the task.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型基于一系列扩散过程，通过添加随机噪声对原始数据进行变换，然后逐渐去除这些噪声，以通过反向过程生成新数据。人类网格扩散（HMDiff）[[199](#bib.bib199)]
    将网格重建视为一种反向扩散过程，将输入特定的分布信息融入扩散过程中，并引入先验知识以简化任务。
- en: 'Methods based on implicit representations can recover free-form geometric shapes,
    but they may have excessive dependence on accurate poses or generate unsatisfactory
    shapes for novel poses or clothing. To increase robustness in these scenarios,
    existing work employs explicit body models to constrain mesh reconstruction in
    implicit methods. Researchers desire an approach that combines implicit and explicit
    methods based on a coarse body mesh’s geometry to refine and produce detailed
    human models meticulously. Hierarchical Mesh Deformation (HMD) [[200](#bib.bib200)]
    uses constraints from body joints, silhouettes, and per-pixel shading information
    to combine the robustness of a parametric model with the flexibility of free-form
    3D deformation. Bhatnagar et al. [[201](#bib.bib201)] introduced an Implicit Part
    Network (IP-Net) to predict the detailed human surface, including the 3D surface
    of the dressed person, the inner body surface, and the parametric body model.
    Parametric Model-Conditioned Implicit Representation (PaMIR) [[38](#bib.bib38)]
    combines a parametric body model with a free-form deep implicit function to enhance
    generalization through regularization. Zhu et al. [[202](#bib.bib202)] utilized
    a ’project-predict-deform’ strategy that refines the SMPL model generated by human
    recovery methods using supervision from joints, silhouettes, and shading information.
    ICON [[203](#bib.bib203)] generates a stable, coarse human mesh using the SMPL
    model, then renders the front and back body normals, which provide rich texture
    details and combine with the original image. Finally, the body’s normal clothed
    normal, and Signed Distance Function (SDF) information are fed into an implicit
    MLP to obtain the final result. Subsequently, Xiu et al. proposed ECON [[204](#bib.bib204)],
    which integrates normal estimation, normal integration, and shape completion by
    using SMPL-X depth as a soft geometric constraint within the optimization equation
    of Normal Integration. It endeavors to maintain coherence with nearby surfaces
    during the integration of normals. Feng et al. [[9](#bib.bib9)] presented Disentangled
    Avatars (DELTA), a model representing humans with hybrid explicit-implicit 3D
    representations. GETAvatar [[205](#bib.bib205)] directly produces explicit textured
    3D human meshes. GETAvatar creates an articulated 3D human representation with
    explicit surface modeling, enriches it with realistic surface details derived
    from 2D normal maps of 3D scan data, and utilizes a rasterization-based renderer
    for surface rendering. Diffusion Inpainting of Neural Avatars (DINAR) [[206](#bib.bib206)]
    combines neural textures with the SMPL-X body model and employs a latent diffusion
    model to recover textures of both seen and unseen regions, subsequently integrating
    these onto the base SMPL-X mesh. TransHuman [[207](#bib.bib207)] employs transformers
    to project the SMPL model into a canonical space and associates each output token
    with a deformable radiance field. This field encodes the query point in the observation
    space and is further utilized to integrate fine-grained information from reference
    images. Gaussian Splatting [[217](#bib.bib217)] is a technique for processing
    and rendering point clouds, using a Gaussian function to create an influence range
    for each point, resulting in a smoother and more natural representation in two-dimensional
    images, as shown in Fig.[7(b)](#S5.F7.sf2 "In Figure 7 ‣ 5.2 Template-free human
    body recovery ‣ 5 3D Human Mesh Recovery ‣ Deep Learning for 3D Human Pose Estimation
    and Mesh Recovery: A Survey"). It has achieved good results in SLAM (Simultaneous
    Localization and Mapping) [[218](#bib.bib218)], generative human modeling [[219](#bib.bib219)],
    dynamic scene reconstruction [[220](#bib.bib220)], and multimodal generation [[221](#bib.bib221)].
    Reconstruction based on Gaussian Splatting requires an initial point cloud input,
    which poses a challenge for human mesh recovery from image inputs. However, explicit
    models can provide initial points, serving as a stepping stone to make Gaussian
    splatting human mesh recovery from RGB inputs feasible. Animatable 3D Gaussian
    [[208](#bib.bib208)] learns human avatars from input images and poses by extending
    3D Gaussian to dynamic human scenes. In the proposed framework, they model a set
    of skinned 3D Gaussian and a corresponding skeleton in canonical space and deform
    3D Gaussian to posed space according to the input poses.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 基于隐式表示的方法可以恢复自由形式的几何形状，但它们可能过于依赖精确的姿势或在新颖姿势或衣物的情况下生成不令人满意的形状。为了在这些场景中提高鲁棒性，现有的工作采用显式体模型来约束隐式方法中的网格重建。研究人员希望一种结合隐式和显式方法的方案，基于粗略体网格的几何形状来精细化并生成详细的人体模型。层次化网格变形（HMD）[[200](#bib.bib200)]利用来自身体关节、轮廓和每像素着色信息的约束，将参数化模型的鲁棒性与自由形式3D变形的灵活性相结合。Bhatnagar等人[[201](#bib.bib201)]引入了一种隐式部件网络（IP-Net），用于预测详细的人体表面，包括穿衣人的3D表面、内部身体表面以及参数化体模型。参数化模型条件隐式表示（PaMIR）[[38](#bib.bib38)]将参数化体模型与自由形式深度隐式函数结合，通过正则化提高泛化能力。Zhu等人[[202](#bib.bib202)]利用了“投影-预测-变形”策略，通过关节、轮廓和着色信息的监督来精细化由人体恢复方法生成的SMPL模型。ICON
    [[203](#bib.bib203)]使用SMPL模型生成稳定的粗略人体网格，然后渲染前后身体法线，这些法线提供了丰富的纹理细节，并与原始图像结合。最后，将身体的法线、衣物法线和签名距离函数（SDF）信息输入隐式MLP以获得最终结果。随后，Xiu等人提出了ECON
    [[204](#bib.bib204)]，它通过在法线整合的优化方程中使用SMPL-X深度作为软几何约束，整合法线估计、法线整合和形状完成。它致力于在法线整合过程中保持与邻近表面的连贯性。Feng等人[[9](#bib.bib9)]提出了Disentangled
    Avatars（DELTA），这是一种用混合显式-隐式3D表示表示人的模型。GETAvatar [[205](#bib.bib205)]直接生成显式纹理3D人体网格。GETAvatar通过显式表面建模创建了一个关节3D人体表示，利用3D扫描数据的2D法线图增强其逼真的表面细节，并使用基于光栅化的渲染器进行表面渲染。神经头像的扩散修复（DINAR）[[206](#bib.bib206)]将神经纹理与SMPL-X体模型结合，采用潜在扩散模型恢复已见和未见区域的纹理，然后将这些纹理整合到基础SMPL-X网格上。TransHuman
    [[207](#bib.bib207)]利用变换器将SMPL模型投影到规范空间，并将每个输出令牌与一个可变形辐射场关联。该场编码观测空间中的查询点，并进一步用于整合来自参考图像的细粒度信息。高斯点渲染[[217](#bib.bib217)]是一种处理和渲染点云的技术，使用高斯函数为每个点创建影响范围，从而在二维图像中产生更平滑和自然的表示，如图[7(b)](#S5.F7.sf2
    "在图7 ‣ 5.2 无模板人体恢复 ‣ 5 3D人体网格恢复 ‣ 深度学习在3D人体姿态估计和网格恢复中的应用:综述")所示。它在SLAM（同时定位与地图构建）[[218](#bib.bib218)]、生成性人体建模[[219](#bib.bib219)]、动态场景重建[[220](#bib.bib220)]和多模态生成[[221](#bib.bib221)]中取得了良好的结果。基于高斯点渲染的重建需要初始点云输入，这对从图像输入恢复人体网格提出了挑战。然而，显式模型可以提供初始点，作为使高斯点渲染从RGB输入恢复人体网格成为可能的垫脚石。可动画的3D高斯[[208](#bib.bib208)]通过将3D高斯扩展到动态人体场景，从输入图像和姿势中学习人体头像。在提出的框架中，他们在规范空间中建模一组皮肤化的3D高斯及相应的骨架，并根据输入姿势将3D高斯变形到姿势空间。
- en: 5.3 Summary of human mesh recovery
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 人体网格恢复总结
- en: This chapter provides a comprehensive review of human mesh recovery utilizing
    both explicit and implicit models. Explicit models excel in robustly reconstructing
    human mesh but often fall short in capturing intricate details, prompting a range
    of extensions for greater detail accuracy. In contrast, implicit-based human mesh
    recovery tends to lack stability, which is known for its flexibility and adaptability.
    Consequently, several studies have explored integrating implicit models with explicit
    counterparts to synergize their respective strengths. The trade-off between flexibility
    and robustness in human mesh recovery represents a pivotal and enduring area of
    research.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了利用显式和隐式模型的人体网格恢复的综合回顾。显式模型在稳健地重建人体网格方面表现突出，但通常在捕捉细节方面有所不足，因此出现了许多针对更高细节准确度的扩展。相对而言，基于隐式的人体网格恢复往往缺乏稳定性，但因其灵活性和适应性而著称。因此，多个研究探讨了将隐式模型与显式模型结合起来，以发挥各自的优势。人体网格恢复中灵活性与鲁棒性之间的权衡代表了一个关键且持久的研究领域。
- en: 6 Evaluation
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 评估
- en: 6.1 Evaluation metrics
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 评估指标
- en: There are many evaluation metrics that can be used fairly to measure the performance
    of deep models in human pose estimation, and some of the key evaluation metrics
    are provided below.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多评估指标可以公平地衡量深度模型在人体姿态估计中的表现，以下提供了一些关键评估指标。
- en: 'Mean Per Joint Position Error (MPJPE) is widely used to evaluate the accuracy
    performance of 3D human pose estimation by calculating the L2 distance between
    the predicted joint coordinates and their ground truth counterparts. Denote the
    estimated coordinates of the j-th joint as $p_{j}^{*}$ and the ground truth coordinates
    as $p_{j}$. The MPJPE of $j$-th joint in the skeleton can be computed as:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 平均每个关节位置误差（MPJPE）被广泛用于评估三维人体姿态估计的准确性，通过计算预测关节坐标与真实坐标之间的 L2 距离。将第 $j$ 个关节的估计坐标记作
    $p_{j}^{*}$，真实坐标记作 $p_{j}$。骨架中第 $j$ 个关节的 MPJPE 可以计算为：
- en: '|  | $MPJPE=\frac{1}{N}\sum_{j=1}^{N}{\left\&#124;p_{j}-p_{j}^{*}\right\&#124;_{2}}$
    |  | (1) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | $MPJPE=\frac{1}{N}\sum_{j=1}^{N}{\left\&#124;p_{j}-p_{j}^{*}\right\&#124;_{2}}$
    |  | (1) |'
- en: where the skeleton comprises $N$ joints, and unlike previously in 2D, where
    the error is quantified in pixels, the joint coordinates in 3D are measured and
    reported in millimeters (mm).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 其中骨架包含 $N$ 个关节，与之前在二维中用像素量化误差不同，三维中的关节坐标是以毫米（mm）为单位进行测量和报告的。
- en: 'Mean Per Joint Angle Error (MPJAE) quantifies the angular discrepancy between
    the estimated and groundtruth joints. The function $r_{j}^{*}$ returns the estimated
    angle of $j$-th joint, and function $r_{j}$ returns the ground truth angle. The
    MPJAE is computed in three dimensions as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 平均每个关节角度误差（MPJAE）量化了估计关节与真实关节之间的角度差异。函数 $r_{j}^{*}$ 返回第 $j$ 个关节的估计角度，而函数 $r_{j}$
    返回真实角度。MPJAE 在三维空间中计算如下：
- en: '|  | $MPJAE=\frac{1}{3N}\sum_{j=1}^{3N}{\left&#124;\left(r_{j}-r_{j}^{*}\right)\mathrm{mod}\pm
    180\right&#124;}$ |  | (2) |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | $MPJAE=\frac{1}{3N}\sum_{j=1}^{3N}{\left&#124;\left(r_{j}-r_{j}^{*}\right)\mathrm{mod}\pm
    180\right&#124;}$ |  | (2) |'
- en: 'Mean Per Joint Localization Error (MPJLE) is a more perceptive and robust evaluation
    metric than MPJPE and MPJAE [[222](#bib.bib222)]. It allows for an adjustable
    tolerance level through a perceptual threshold $t$ :'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 平均每个关节定位误差（MPJLE）是一种比 MPJPE 和 MPJAE 更具感知性和鲁棒性的评估指标[[222](#bib.bib222)]。它允许通过感知阈值
    $t$ 调整容忍水平：
- en: '|  | $MPJLE=\frac{1}{N}\sum_{j=1}^{N}{\mathds{1}_{\left\&#124;l_{j}-l_{j}^{*}\right\&#124;_{2}\geq
    t}}$ |  | (3) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | $MPJLE=\frac{1}{N}\sum_{j=1}^{N}{\mathds{1}_{\left\&#124;l_{j}-l_{j}^{*}\right\&#124;_{2}\geq
    t}}$ |  | (3) |'
- en: where $l(\cdot)$ indicates the joint localization.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $l(\cdot)$ 表示关节定位。
- en: There are various modifications of MPJPE, MPJAE, and MPJLE, including Procrustes-aligned
    (PA-) MPJPE, MPJAE, MPJLE and Normalized (N-) MPJPE, MPJAE, MPJLE. The former
    refers to procrustes-aligned metrics, while the latter denotes normalized metrics.
    Additionally, some 2D metrics are adaptable to 3D, such as the 3D Percentage of
    Correct Keypoints (3D PCK) and the 3D Area Under the Curve (3D AUC). The PCK [[223](#bib.bib223)]
    measures the distance between predicted and groundtruth keypoints, considering
    keypoints correct if this distance is less than a predefined threshold. The AUC
    signifies the aggregate area beneath the PCK threshold curve as the threshold
    varies.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: MPJPE、MPJAE 和 MPJLE 有各种修改版本，包括 Procrustes 对齐（PA-）MPJPE、MPJAE、MPJLE 和标准化（N-）MPJPE、MPJAE、MPJLE。前者指的是
    Procrustes 对齐度量，而后者指的是标准化度量。此外，一些 2D 度量可适应于 3D，例如 3D 关键点百分比（3D PCK）和 3D 曲线下面积（3D
    AUC）。PCK [[223](#bib.bib223)] 测量预测关键点与真实关键点之间的距离，如果这个距离小于预定义的阈值，则认为关键点是正确的。AUC
    表示在阈值变化时，PCK 阈值曲线下的总面积。
- en: Mean Per Vertex Position Error (MPVPE) is a metric used to evaluate the human
    mesh reconstruction by computing the L2 distance between the predicted and ground
    truth mesh points. In some published articles, the MPVPE is also called Vertex-to-Vertex
    (V2V) error and Per-Vertex Error (PVE).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 平均每顶点位置误差（MPVPE）是一种用于评估人体网格重建的指标，通过计算预测网格点与真实网格点之间的L2距离来评估。在一些已发表的文章中，MPVPE
    也被称为顶点到顶点（V2V）误差和每顶点误差（PVE）。
- en: 6.2 Datasets
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 数据集
- en: 'Datasets are essential in developing deep learning-based human pose estimation
    and mesh recovery. Researchers have developed many datasets to train models and
    facilitate fair comparisons among different methods. In this section, we introduce
    the details of related datasets from recent years. Summary of these datasets,
    including information on 3D pose and 3D mesh, are presented in Table [3](#S6.T3
    "Table 3 ‣ 6.2 Datasets ‣ 6 Evaluation ‣ Deep Learning for 3D Human Pose Estimation
    and Mesh Recovery: A Survey").'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在开发基于深度学习的人体姿态估计和网格恢复中至关重要。研究人员开发了许多数据集来训练模型，并促进不同方法之间的公平比较。在这一部分，我们介绍了近年来相关数据集的详细信息。这些数据集的总结，包括
    3D 姿态和 3D 网格的信息，见表 [3](#S6.T3 "表 3 ‣ 6.2 数据集 ‣ 6 评估 ‣ 深度学习用于 3D 人体姿态估计和网格恢复：综述")。
- en: 'Table 3: The overview of the mainstream datasets.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：主流数据集概述。
- en: '| Dataset | Type | Data | Total frames | Feature | Download link |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类型 | 数据 | 总帧数 | 特征 | 下载链接 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Human3.6M [[222](#bib.bib222)] | 3D/Mesh | Video | 3.6M | multi-view | [Website](http://vision.imar.ro/human3.6m/description.php)
    |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Human3.6M [[222](#bib.bib222)] | 3D/网格 | 视频 | 3.6M | 多视角 | [网站](http://vision.imar.ro/human3.6m/description.php)
    |'
- en: '| 3DPW [[224](#bib.bib224)] | 3D/Mesh | Video | 51K | multi-person | [Website](https://virtualhumans.mpi-inf.mpg.de/3DPW/)
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 3DPW [[224](#bib.bib224)] | 3D/网格 | 视频 | 51K | 多人 | [网站](https://virtualhumans.mpi-inf.mpg.de/3DPW/)
    |'
- en: '| MPI-INF-3DPH [[225](#bib.bib225)] | 2D/3D | Video | 2K | in-wild | [Website](https://vcai.mpi-inf.mpg.de/3dhp-dataset/)
    |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| MPI-INF-3DPH [[225](#bib.bib225)] | 2D/3D | 视频 | 2K | 野外 | [网站](https://vcai.mpi-inf.mpg.de/3dhp-dataset/)
    |'
- en: '| HumanEva [[226](#bib.bib226)] | 3D | Video | 40K | multi-view | [Website](http://humaneva.is.tue.mpg.de/)
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| HumanEva [[226](#bib.bib226)] | 3D | 视频 | 40K | 多视角 | [网站](http://humaneva.is.tue.mpg.de/)
    |'
- en: '| CMU-Panoptic [[227](#bib.bib227)] | 3D | Video | 1.5M | multi-view/multi-person
    | [Website](https://domedb.perception.cs.cmu.edu/) |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| CMU-Panoptic [[227](#bib.bib227)] | 3D | 视频 | 1.5M | 多视角/多人 | [网站](https://domedb.perception.cs.cmu.edu/)
    |'
- en: '| MuCo-3DHP [[115](#bib.bib115)] | 3D | Image | 8K | multi-person/occluded
    scence | [Website](https://vcai.mpi-inf.mpg.de/projects/SingleShotMultiPerson/)
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| MuCo-3DHP [[115](#bib.bib115)] | 3D | 图像 | 8K | 多人/遮挡场景 | [网站](https://vcai.mpi-inf.mpg.de/projects/SingleShotMultiPerson/)
    |'
- en: '| SURREAL [[228](#bib.bib228)] | 2D/3D/Mesh | Video | 6.0M | synthetic model
    | [Website](https://www.di.ens.fr/willow/research/surreal/data/) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| SURREAL [[228](#bib.bib228)] | 2D/3D/网格 | 视频 | 6.0M | 合成模型 | [网站](https://www.di.ens.fr/willow/research/surreal/data/)
    |'
- en: '| 3DOH50K [[150](#bib.bib150)] | 2D/3D/Mesh | Image | 51K | object-occluded
    | [Website](https://www.yangangwang.com/#me) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 3DOH50K [[150](#bib.bib150)] | 2D/3D/网格 | 图像 | 51K | 物体遮挡 | [网站](https://www.yangangwang.com/#me)
    |'
- en: '| 3DCP [[229](#bib.bib229)] | Mesh | Mesh | 190 | contact | [Website](https://tuch.is.tue.mpg.de/)
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 3DCP [[229](#bib.bib229)] | 网格 | 网格 | 190 | 接触 | [网站](https://tuch.is.tue.mpg.de/)
    |'
- en: '| AMASS [[230](#bib.bib230)] | Mesh | Motion | 11K | soft-tissue dynamics |
    [Website](https://amass.is.tue.mpg.de/) |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| AMASS [[230](#bib.bib230)] | 网格 | 动作 | 11K | 软组织动态 | [网站](https://amass.is.tue.mpg.de/)
    |'
- en: '| DensePose [[231](#bib.bib231)] | Mesh | Image | 50K | multi-person | [Website](http://densepose.org/)
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| DensePose [[231](#bib.bib231)] | 网格 | 图像 | 50K | 多人 | [网站](http://densepose.org/)
    |'
- en: '| UP-3D [[232](#bib.bib232)] | 3D/Mesh | Image | 8K | sport scence | [Website](https://files.is.tuebingen.mpg.de/classner/up/)
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| UP-3D [[232](#bib.bib232)] | 3D/网格 | 图像 | 8K | 体育场景 | [网站](https://files.is.tuebingen.mpg.de/classner/up/)
    |'
- en: '| THuman2.0 [[233](#bib.bib233)] | Mesh | Image | 7K | textured surface | [Website](https://github.com/ytrock/THuman2.0-Dataset)
    |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| THuman2.0 [[233](#bib.bib233)] | 网格 | 图像 | 7K | 有纹理的表面 | [网站](https://github.com/ytrock/THuman2.0-Dataset)
    |'
- en: 'Human3.6M dataset [[222](#bib.bib222)] is the most widely used dataset in the
    evaluation of 3D human pose estimation. It encompasses a vast collection of 3.6
    million poses captured using RGB and ToF cameras from diverse viewpoints within
    a real-world setting. Additionally, this dataset incorporates high-resolution
    3D scanner data of body meshes, playing a pivotal role in the progression of human
    sensing systems. Table [4](#S6.T4 "Table 4 ‣ 6.2 Datasets ‣ 6 Evaluation ‣ Deep
    Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey") exhibits the
    performance of state-of-the-art 3D human pose estimation methods on the Human3.6M
    dataset.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Human3.6M 数据集 [[222](#bib.bib222)] 是在3D人体姿态估计评估中使用最广泛的数据集。它包含了通过RGB和ToF摄像头从不同视角捕获的360万姿态数据。此外，该数据集还包括高分辨率的3D扫描体网格数据，对人体感知系统的进展起着关键作用。表[4](#S6.T4
    "表4 ‣ 6.2 数据集 ‣ 6 评估 ‣ 深度学习在人类3D姿态估计和网格恢复中的应用：综述") 展示了在Human3.6M数据集上最先进的3D人体姿态估计方法的性能。
- en: 'Table 4: Comparisons of 3D pose estimation methods on Human3.6M [[222](#bib.bib222)].'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：Human3.6M [[222](#bib.bib222)] 上的3D姿态估计方法比较。
- en: '| Method | Year | Publication | Highlight | MPJPE$\downarrow$ | PMPJPE$\downarrow$
    | Code |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 发表刊物 | 亮点 | MPJPE$\downarrow$ | PMPJPE$\downarrow$ | 代码 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Graformer [[234](#bib.bib234)] | 2022 | CVPR’22 | graph-based transformer
    | 35.2 | - | [Code](https://github.com/Graformer/GraFormer) |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Graformer [[234](#bib.bib234)] | 2022 | CVPR’22 | 基于图的变换器 | 35.2 | - | [代码](https://github.com/Graformer/GraFormer)
    |'
- en: '| GLA-GCN [[235](#bib.bib235)] | 2023 | ICCV’23 | adaptive GCN | 34.4 | 37.8
    | [Code](https://github.com/bruceyo/GLA-GCN) |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| GLA-GCN [[235](#bib.bib235)] | 2023 | ICCV’23 | 自适应GCN | 34.4 | 37.8 | [代码](https://github.com/bruceyo/GLA-GCN)
    |'
- en: '| PoseDA [[45](#bib.bib45)] | 2023 | arXiv’23 | domain adaptation | 49.4 |
    34.2 | [Code](https://github.com/rese1f/PoseDA) |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| PoseDA [[45](#bib.bib45)] | 2023 | arXiv’23 | 领域适应 | 49.4 | 34.2 | [代码](https://github.com/rese1f/PoseDA)
    |'
- en: '| GFPose [[236](#bib.bib236)] | 2023 | CVPR’23 | gradient fields | 35.6 | 30.5
    | [Code](https://sites.google.com/view/gfpose/) |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| GFPose [[236](#bib.bib236)] | 2023 | CVPR’23 | 梯度场 | 35.6 | 30.5 | [代码](https://sites.google.com/view/gfpose/)
    |'
- en: '| TP-LSTMs [[237](#bib.bib237)] | 2022 | TPAMI’22 | pose similarity metric
    | 40.5 | 31.8 | - |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| TP-LSTMs [[237](#bib.bib237)] | 2022 | TPAMI’22 | 姿态相似度度量 | 40.5 | 31.8 |
    - |'
- en: '| FTCM [[119](#bib.bib119)] | 2023 | TCSVT’23 | frequency-temporal collaborative
    | 28.1 | - | [Code](https://github.com/zhenhuat/FTCM) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| FTCM [[119](#bib.bib119)] | 2023 | TCSVT’23 | 频率-时间协作 | 28.1 | - | [代码](https://github.com/zhenhuat/FTCM)
    |'
- en: '| VideoPose3D [[86](#bib.bib86)] | 2019 | CVPR’19 | semi-supervised | 46.8
    | 36.5 | [Code](https://github.com/facebookresearch/VideoPose3D) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| VideoPose3D [[86](#bib.bib86)] | 2019 | CVPR’19 | 半监督 | 46.8 | 36.5 | [代码](https://github.com/facebookresearch/VideoPose3D)
    |'
- en: '| PoseFormer [[87](#bib.bib87)] | 2021 | ICCV’21 | spatio-temporal transformer
    | 44.3 | 34.6 | [Code](https://github.com/zczcwh/PoseFormer) |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| PoseFormer [[87](#bib.bib87)] | 2021 | ICCV’21 | 时空变换器 | 44.3 | 34.6 | [代码](https://github.com/zczcwh/PoseFormer)
    |'
- en: '| STCFormer [[93](#bib.bib93)] | 2023 | CVPR’23 | spatio-temporal transformer
    | 40.5 | 31.8 | [Code](https://github.com/zhenhuat/STCFormer) |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| STCFormer [[93](#bib.bib93)] | 2023 | CVPR’23 | 时空变换器 | 40.5 | 31.8 | [代码](https://github.com/zhenhuat/STCFormer)
    |'
- en: '| 3Dpose_ssl [[82](#bib.bib82)] | 2020 | TPAMI’20 | self-supervised | 63.6
    | 63.7 | [Code](https://github.com/chanyn/3Dpose_ssl) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 3Dpose_ssl [[82](#bib.bib82)] | 2020 | TPAMI’20 | 自监督 | 63.6 | 63.7 | [代码](https://github.com/chanyn/3Dpose_ssl)
    |'
- en: '| MTF-Transformer [[30](#bib.bib30)] | 2022 | TPAMI’22 | multi-view temporal
    fusion | 26.2 | - | [Code](https://github.com/lelexx/MTF-Transformer) |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| MTF-Transformer [[30](#bib.bib30)] | 2022 | TPAMI’22 | 多视角时间融合 | 26.2 | -
    | [代码](https://github.com/lelexx/MTF-Transformer) |'
- en: '| AdaptPose [[85](#bib.bib85)] | 2022 | CVPR’22 | cross datasets | 42.5 | 34.0
    | [Code](https://github.com/mgholamikn/AdaptPose) |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| AdaptPose [[85](#bib.bib85)] | 2022 | CVPR’22 | 跨数据集 | 42.5 | 34.0 | [代码](https://github.com/mgholamikn/AdaptPose)
    |'
- en: '| 3D-HPE-PAA [[98](#bib.bib98)] | 2022 | TIP’22 | part aware attention | 43.1
    | 33.7 | [Code](https://github.com/thuxyz19/3D-HPE-PAA) |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 3D-HPE-PAA [[98](#bib.bib98)] | 2022 | TIP’22 | 部分感知注意力 | 43.1 | 33.7 | [代码](https://github.com/thuxyz19/3D-HPE-PAA)
    |'
- en: '| DeciWatch [[238](#bib.bib238)] | 2022 | ECCV’22 | efficient framework | 52.8
    | - | [Code](https://github.com/cure-lab/DeciWatch) |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| DeciWatch [[238](#bib.bib238)] | 2022 | ECCV’22 | 高效框架 | 52.8 | - | [代码](https://github.com/cure-lab/DeciWatch)
    |'
- en: '| Diffpose [[239](#bib.bib239)] | 2023 | CVPR’23 | pose refine | 36.9 | 28.7
    | [Code](https://gongjia0208.github.io/Diffpose/) |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Diffpose [[239](#bib.bib239)] | 2023 | CVPR’23 | 姿态精炼 | 36.9 | 28.7 | [代码](https://gongjia0208.github.io/Diffpose/)
    |'
- en: '| Elepose [[80](#bib.bib80)] | 2022 | CVPR’22 | unsupervised | - | 36.7 | [Code](https://github.com/bastianwandt/ElePose)
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| Elepose [[80](#bib.bib80)] | 2022 | CVPR’22 | 无监督 | - | 36.7 | [代码](https://github.com/bastianwandt/ElePose)
    |'
- en: '| Uplift and Upsample [[19](#bib.bib19)] | 2023 | CVPR’23 | efficient transformers
    | 48.1 | 37.6 | [Code](https://github.com/goldbricklemon/uplift-upsample-3dhpe)
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Uplift and Upsample [[19](#bib.bib19)] | 2023 | CVPR’23 | 高效变换器 | 48.1 |
    37.6 | [代码](https://github.com/goldbricklemon/uplift-upsample-3dhpe) |'
- en: '| RS-Net [[55](#bib.bib55)] | 2023 | TIP’23 | regular splitting graph network
    | 48.6 | 38.9 | [Code](https://github.com/nies14/RS-Net) |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| RS-Net [[55](#bib.bib55)] | 2023 | TIP’23 | 正则分割图网络 | 48.6 | 38.9 | [代码](https://github.com/nies14/RS-Net)
    |'
- en: '| HSTFormer [[92](#bib.bib92)] | 2023 | arXiv’23 | spatial-temporal transformers
    | 42.7 | 33.7 | [Code](https://github.com/qianxiaoye825/HSTFormer) |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| HSTFormer [[92](#bib.bib92)] | 2023 | arXiv’23 | 空间时间变换器 | 42.7 | 33.7 |
    [代码](https://github.com/qianxiaoye825/HSTFormer) |'
- en: '| PoseFormerV2 [[56](#bib.bib56)] | 2023 | CVPR’23 | frequency domain | 45.2
    | 35.6 | [Code](https://github.com/QitaoZhao/PoseFormerV2) |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| PoseFormerV2 [[56](#bib.bib56)] | 2023 | CVPR’23 | 频率域 | 45.2 | 35.6 | [代码](https://github.com/QitaoZhao/PoseFormerV2)
    |'
- en: '| DiffPose [[240](#bib.bib240)] | 2023 | ICCV’23 | diffusion models | 42.9
    | 30.8 | [Code](https://github.com/bastianwandt/DiffPose/) |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| DiffPose [[240](#bib.bib240)] | 2023 | ICCV’23 | 扩散模型 | 42.9 | 30.8 | [代码](https://github.com/bastianwandt/DiffPose/)
    |'
- en: 'MPI-INF-3DPH dataset [[225](#bib.bib225)] offers over 2K videos featuring joint
    annotations of 13 keypoints in outdoor scenes, apt for 2D and 3D human pose estimation.
    The ground truth was obtained using a multi-camera arrangement and a marker-less
    MoCap system, representing a shift from traditional marker-based MoCap systems
    that involve real individuals. Table [5](#S6.T5 "Table 5 ‣ 6.2 Datasets ‣ 6 Evaluation
    ‣ Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey") showcases
    the performance of state-of-the-art methods on the 3DPH dataset.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: MPI-INF-3DPH 数据集 [[225](#bib.bib225)] 提供了超过2000段视频，涵盖了户外场景中13个关键点的关节注释，适用于2D和3D人体姿态估计。地面真实数据是通过多摄像头配置和无标记运动捕捉系统获得的，代表了从传统的标记式运动捕捉系统转变的过程。表格
    [5](#S6.T5 "表格 5 ‣ 6.2 数据集 ‣ 6 评估 ‣ 深度学习在3D人体姿态估计与网格恢复中的应用：综述") 展示了在3DPH 数据集上最先进方法的表现。
- en: 'Table 5: Comparisons of 3D pose estimation methods on MPI-INF-3DPH [[225](#bib.bib225)].'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 5: MPI-INF-3DPH [[225](#bib.bib225)] 数据集上的3D姿态估计方法比较。'
- en: '| Method | Year | Publication | Highlight | MPJPE$\downarrow$ | PCK$\uparrow$
    | AUC$\uparrow$ | Code |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 出版物 | 亮点 | MPJPE$\downarrow$ | PCK$\uparrow$ | AUC$\uparrow$ |
    代码 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| HSTFormer [[92](#bib.bib92)] | 2023 | arXiv’23 | spatial-temporal transformers
    | 28.3 | 98.0 | 78.6 | [Code](https://github.com/qianxiaoye825/HSTFormer) |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| HSTFormer [[92](#bib.bib92)] | 2023 | arXiv’23 | 空间时间变换器 | 28.3 | 98.0 |
    78.6 | [代码](https://github.com/qianxiaoye825/HSTFormer) |'
- en: '| PoseFormerV2 [[56](#bib.bib56)] | 2023 | CVPR’23 | frequency domain | 27.8
    | 97.9 | 78.8 | [Code](https://github.com/QitaoZhao/PoseFormerV2) |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| PoseFormerV2 [[56](#bib.bib56)] | 2023 | CVPR’23 | 频率域 | 27.8 | 97.9 | 78.8
    | [代码](https://github.com/QitaoZhao/PoseFormerV2) |'
- en: '| Uplift and Upsample [[19](#bib.bib19)] | 2023 | CVPR’23 | efficient transformers
    | 46.9 | 95.4 | 67.6 | [Code](https://github.com/goldbricklemon/uplift-upsample-3dhpe)
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Uplift and Upsample [[19](#bib.bib19)] | 2023 | CVPR’23 | 高效变换器 | 46.9 |
    95.4 | 67.6 | [代码](https://github.com/goldbricklemon/uplift-upsample-3dhpe) |'
- en: '| RS-Net [[55](#bib.bib55)] | 2023 | TIP’23 | regular splitting graph network
    | - | 85.6 | 53.2 | [Code](https://github.com/nies14/RS-Net) |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| RS-Net [[55](#bib.bib55)] | 2023 | TIP’23 | 正则分割图网络 | - | 85.6 | 53.2 | [代码](https://github.com/nies14/RS-Net)
    |'
- en: '| Diffpose [[239](#bib.bib239)] | 2023 | CVPR’23 | pose refine | 29.1 | 98.0
    | 75.9 | [Code](https://gongjia0208.github.io/Diffpose/) |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| Diffpose [[239](#bib.bib239)] | 2023 | CVPR’23 | 姿态精炼 | 29.1 | 98.0 | 75.9
    | [代码](https://gongjia0208.github.io/Diffpose/) |'
- en: '| FTCM [[119](#bib.bib119)] | 2023 | TCSVT’23 | frequency-temporal collaborative
    | 31.2 | 97.9 | 79.8 | [Code](https://github.com/zhenhuat/FTCM) |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| FTCM [[119](#bib.bib119)] | 2023 | TCSVT’23 | 频率时间协作 | 31.2 | 97.9 | 79.8
    | [代码](https://github.com/zhenhuat/FTCM) |'
- en: '| STCFormer [[93](#bib.bib93)] | 2023 | CVPR’23 | spatio-temporal transformer
    | 23.1 | 98.7 | 83.9 | [Code](https://github.com/zhenhuat/STCFormer) |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| STCFormer [[93](#bib.bib93)] | 2023 | CVPR’23 | 空间时间变换器 | 23.1 | 98.7 | 83.9
    | [代码](https://github.com/zhenhuat/STCFormer) |'
- en: '| PoseDA [[45](#bib.bib45)] | 2023 | arXiv’23 | domain adaptation | 61.3 |
    92.0 | 62.5 | [Code](https://github.com/rese1f/PoseDA) |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| PoseDA [[45](#bib.bib45)] | 2023 | arXiv’23 | 域适应 | 61.3 | 92.0 | 62.5 |
    [代码](https://github.com/rese1f/PoseDA) |'
- en: '| TP-LSTMs [[237](#bib.bib237)] | 2022 | TPAMI’22 | pose similarity metric
    | 48.8 | 82.6 | 81.3 | - |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| TP-LSTMs [[237](#bib.bib237)] | 2022 | TPAMI’22 | 姿态相似度度量 | 48.8 | 82.6 |
    81.3 | - |'
- en: '| AdaptPose [[85](#bib.bib85)] | 2022 | CVPR’22 | cross datasets | 77.2 | 88.4
    | 54.2 | [Code](https://github.com/mgholamikn/AdaptPose) |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| AdaptPose [[85](#bib.bib85)] | 2022 | CVPR’22 | 跨数据集 | 77.2 | 88.4 | 54.2
    | [代码](https://github.com/mgholamikn/AdaptPose) |'
- en: '| 3D-HPE-PAA [[98](#bib.bib98)] | 2022 | TIP’22 | part aware attention | 69.4
    | 90.3 | 57.8 | [Code](https://github.com/thuxyz19/3D-HPE-PAA) |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 3D-HPE-PAA [[98](#bib.bib98)] | 2022 | TIP’22 | 部分关注注意力 | 69.4 | 90.3 | 57.8
    | [代码](https://github.com/thuxyz19/3D-HPE-PAA) |'
- en: '| Elepose [[80](#bib.bib80)] | 2022 | CVPR’22 | unsupervised | 54.0 | 86.0
    | 50.1 | [Code](https://github.com/bastianwandt/ElePose) |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| Elepose [[80](#bib.bib80)] | 2022 | CVPR’22 | 无监督 | 54.0 | 86.0 | 50.1 |
    [代码](https://github.com/bastianwandt/ElePose) |'
- en: '3DPW dataset [[224](#bib.bib224)] captures 51,000 sequences of single-view
    video, complemented by IMUs data. These videos were recorded using a handheld
    camera, with the IMU data facilitating the association of 2D poses with their
    3D counterparts. 3DPW stands out as one of the most formidable datasets, establishing
    itself as a benchmark for 3D pose estimation in multi-person, in-wild scenarios
    of recent times. Table [6](#S6.T6 "Table 6 ‣ 6.2 Datasets ‣ 6 Evaluation ‣ Deep
    Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey") shows the
    performance of state-of-the-art human mesh recovery methods on the Human3.6M and
    3DPW datasets.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '3DPW 数据集 [[224](#bib.bib224)] 捕捉了 51,000 个单视角视频序列，并配有 IMU 数据。这些视频使用手持摄像机录制，IMU
    数据帮助将 2D 姿态与其 3D 对应物关联起来。3DPW 脱颖而出，成为近年来多人的野外场景 3D 姿态估计的基准数据集。表格 [6](#S6.T6 "Table
    6 ‣ 6.2 Datasets ‣ 6 Evaluation ‣ Deep Learning for 3D Human Pose Estimation and
    Mesh Recovery: A Survey") 显示了最先进的人体网格恢复方法在 Human3.6M 和 3DPW 数据集上的表现。'
- en: 'Table 6: Comparisons of human mesh recovery methods on Human3.6M [[222](#bib.bib222)]
    and 3DPW [[224](#bib.bib224)].'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：Human3.6M [[222](#bib.bib222)] 和 3DPW [[224](#bib.bib224)] 上人体网格恢复方法的比较。
- en: '| Method | Publication | Highlight | Human3.6M | 3DPW | Code |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 发表 | 亮点 | Human3.6M | 3DPW | 代码 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| MPJPE$\downarrow$ | PA-MPJPE$\downarrow$ | MPJPE$\downarrow$ | PA-MPJPE$\downarrow$
    | PVE$\downarrow$ |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| MPJPE$\downarrow$ | PA-MPJPE$\downarrow$ | MPJPE$\downarrow$ | PA-MPJPE$\downarrow$
    | PVE$\downarrow$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| VirtualMarker [[241](#bib.bib241)] | CVPR’23 | novel intermediate representation
    | 47.3 | 32.0 | 67.5 | 41.3 | 77.9 | [Code](https://github.com/ShirleyMaxx/VirtualMarker)
    |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| VirtualMarker [[241](#bib.bib241)] | CVPR’23 | 新型中间表示 | 47.3 | 32.0 | 67.5
    | 41.3 | 77.9 | [代码](https://github.com/ShirleyMaxx/VirtualMarker) |'
- en: '| NIKI [[157](#bib.bib157)] | CVPR’23 | inverse kinematics | - | - | 71.3 |
    40.6 | 86.6 | [Code](https://github.com/Jeff-sjtu/NIKI) |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| NIKI [[157](#bib.bib157)] | CVPR’23 | 逆向运动学 | - | - | 71.3 | 40.6 | 86.6
    | [代码](https://github.com/Jeff-sjtu/NIKI)'
- en: '| TORE [[146](#bib.bib146)] | ICCV’23 | efficient transformer | 59.6 | 36.4
    | 72.3 | 44.4 | 88.2 | [Code](https://frank-zy-dou.github.io/projects/Tore/index.html)
    |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| TORE [[146](#bib.bib146)] | ICCV’23 | 高效变换器 | 59.6 | 36.4 | 72.3 | 44.4 |
    88.2 | [代码](https://frank-zy-dou.github.io/projects/Tore/index.html) |'
- en: '| JOTR [[167](#bib.bib167)] | ICCV’23 | contrastive learning | - | - | 76.4
    | 48.7 | 92.6 | [Code](https://github.com/xljh0520/JOTR) |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| JOTR [[167](#bib.bib167)] | ICCV’23 | 对比学习 | - | - | 76.4 | 48.7 | 92.6 |
    [代码](https://github.com/xljh0520/JOTR) |'
- en: '| HMDiff [[199](#bib.bib199)] | ICCV’23 | reverse diffusion processing | 49.3
    | 32.4 | 72.7 | 44.5 | 82.4 | [Code](https://gongjia0208.github.io/HMDiff/) |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| HMDiff [[199](#bib.bib199)] | ICCV’23 | 反向扩散处理 | 49.3 | 32.4 | 72.7 | 44.5
    | 82.4 | [代码](https://gongjia0208.github.io/HMDiff/) |'
- en: '| ReFit [[162](#bib.bib162)] | ICCV’23 | recurrent fitting network | 48.4 |
    32.2 | 65.8 | 41.0 | - | [Code](https://github.com/yufu-wang/ReFit) |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| ReFit [[162](#bib.bib162)] | ICCV’23 | 循环拟合网络 | 48.4 | 32.2 | 65.8 | 41.0
    | - | [代码](https://github.com/yufu-wang/ReFit) |'
- en: '| PyMAF-X [[181](#bib.bib181)] | TPAMI’23 | regression-based one-stage whole
    body | - | - | 74.2 | 45.3 | 87.0 | [Code](https://www.liuyebin.com/pymaf-x/)
    |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| PyMAF-X [[181](#bib.bib181)] | TPAMI’23 | 基于回归的一阶段全身 | - | - | 74.2 | 45.3
    | 87.0 | [代码](https://www.liuyebin.com/pymaf-x/) |'
- en: '| PointHMR [[242](#bib.bib242)] | CVPR’23 | vertex-relevant feature extraction
    | 48.3 | 32.9 | 73.9 | 44.9 | 85.5 | - |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| PointHMR [[242](#bib.bib242)] | CVPR’23 | 顶点相关特征提取 | 48.3 | 32.9 | 73.9 |
    44.9 | 85.5 | - |'
- en: '| PLIKS [[243](#bib.bib243)] | CVPR’23 | inverse kinematics | 47.0 | 34.5 |
    60.5 | 38.5 | 73.3 | [Code](https://github.com/karShetty/PLIKS) |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| PLIKS [[243](#bib.bib243)] | CVPR’23 | 逆向运动学 | 47.0 | 34.5 | 60.5 | 38.5
    | 73.3 | [代码](https://github.com/karShetty/PLIKS) |'
- en: '| ProPose [[244](#bib.bib244)] | CVPR’23 | learning analytical posterior probability
    | 45.7 | 29.1 | 68.3 | 40.6 | 79.4 | [Code](https://github.com/NetEase-GameAI/ProPose)
    |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| ProPose [[244](#bib.bib244)] | CVPR’23 | 学习分析后验概率 | 45.7 | 29.1 | 68.3 |
    40.6 | 79.4 | [Code](https://github.com/NetEase-GameAI/ProPose) |'
- en: '| POTTER [[245](#bib.bib245)] | CVPR’23 | pooling attention transformer | 56.5
    | 35.1 | 75.0 | 44.8 | 87.4 | [Code](https://github.com/zczcwh/POTTER) |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| POTTER [[245](#bib.bib245)] | CVPR’23 | 池化注意力变换器 | 56.5 | 35.1 | 75.0 | 44.8
    | 87.4 | [Code](https://github.com/zczcwh/POTTER) |'
- en: '| PoseExaminer [[246](#bib.bib246)] | ICCV’23 | automated testing of out-of-distribution
    | - | - | 74.5 | 46.5 | 88.6 | [Code](https://github.com/qihao067/PoseExaminer)
    |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| PoseExaminer [[246](#bib.bib246)] | ICCV’23 | 分布外自动化测试 | - | - | 74.5 | 46.5
    | 88.6 | [Code](https://github.com/qihao067/PoseExaminer) |'
- en: '| MotionBERT [[153](#bib.bib153)] | ICCV’23 | pretrained human representations
    | 43.1 | 27.8 | 68.8 | 40.6 | 79.4 | [Code](https://motionbert.github.io/) |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| MotionBERT [[153](#bib.bib153)] | ICCV’23 | 预训练的人类表示 | 43.1 | 27.8 | 68.8
    | 40.6 | 79.4 | [Code](https://motionbert.github.io/) |'
- en: '| 3DNBF [[197](#bib.bib197)] | ICCV’23 | analysis-by-synthesis approach | -
    | - | 88.8 | 53.3 | - | [Code](https://github.com/edz-o/3DNBF) |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 3DNBF [[197](#bib.bib197)] | ICCV’23 | 基于分析的合成方法 | - | - | 88.8 | 53.3 |
    - | [Code](https://github.com/edz-o/3DNBF) |'
- en: '| FastMETRO [[131](#bib.bib131)] | ECCV’22 | efficient architecture | 52.2
    | 33.7 | 73.5 | 44.6 | 84.1 | [Code](https://github.com/postech-ami/FastMETRO)
    |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| FastMETRO [[131](#bib.bib131)] | ECCV’22 | 高效架构 | 52.2 | 33.7 | 73.5 | 44.6
    | 84.1 | [Code](https://github.com/postech-ami/FastMETRO) |'
- en: '| CLIFF [[126](#bib.bib126)] | ECCV’22 | multi-modality inputs | 47.1 | 32.7
    | 69.0 | 43.0 | 81.2 | [Code](https://github.com/huawei-noah/noah-research/tree/master/CLIFF)
    |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| CLIFF [[126](#bib.bib126)] | ECCV’22 | 多模态输入 | 47.1 | 32.7 | 69.0 | 43.0
    | 81.2 | [Code](https://github.com/huawei-noah/noah-research/tree/master/CLIFF)
    |'
- en: '| PARE [[127](#bib.bib127)] | ICCV’21 | part-driven attention | - | - | 74.5
    | 46.5 | 88.6 | [Code](https://pare.is.tue.mpg.de/) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| PARE [[127](#bib.bib127)] | ICCV’21 | 部分驱动的注意力 | - | - | 74.5 | 46.5 | 88.6
    | [Code](https://pare.is.tue.mpg.de/) |'
- en: '| Graphormer [[128](#bib.bib128)] | ICCV’21 | GCNN-reinforced transformer |
    51.2 | 34.5 | 74.7 | 45.6 | 87.7 | [Code](https://github.com/microsoft/MeshGraphormer)
    |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| Graphormer [[128](#bib.bib128)] | ICCV’21 | GCNN增强的变换器 | 51.2 | 34.5 | 74.7
    | 45.6 | 87.7 | [Code](https://github.com/microsoft/MeshGraphormer) |'
- en: '| PSVT [[130](#bib.bib130)] | CVPR’23 | spatio-temporal encoder | - | - | 73.1
    | 43.5 | 84.0 | - |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| PSVT [[130](#bib.bib130)] | CVPR’23 | 时空编码器 | - | - | 73.1 | 43.5 | 84.0
    | - |'
- en: '| GLoT [[139](#bib.bib139)] | CVPR’23 | short-term and long-term temporal correlations
    | 67.0 | 46.3 | 80.7 | 50.6 | 96.3 | [Code](https://github.com/sxl142/GLoT) |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| GLoT [[139](#bib.bib139)] | CVPR’23 | 短期和长期时间相关性 | 67.0 | 46.3 | 80.7 | 50.6
    | 96.3 | [Code](https://github.com/sxl142/GLoT) |'
- en: '| MPS-Net [[129](#bib.bib129)] | CVPR’23 | temporally adjacent representations
    | 69.4 | 47.4 | 91.6 | 54.0 | 109.6 | [Code](https://mps-net.github.io/MPS-Net/)
    |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| MPS-Net [[129](#bib.bib129)] | CVPR’23 | 时间上相邻的表示 | 69.4 | 47.4 | 91.6 |
    54.0 | 109.6 | [Code](https://mps-net.github.io/MPS-Net/) |'
- en: '| MAED [[137](#bib.bib137)] | ICCV’21 | multi-level attention | 56.4 | 38.7
    | 79.1 | 45.7 | 92.6 | [Code](https://github.com/ziniuwan/maed) |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| MAED [[137](#bib.bib137)] | ICCV’21 | 多级注意力 | 56.4 | 38.7 | 79.1 | 45.7 |
    92.6 | [Code](https://github.com/ziniuwan/maed) |'
- en: '| Lee et al. [[158](#bib.bib158)] | ICCV’21 | uncertainty-aware | 58.4 | 38.4
    | 92.8 | 52.2 | 106.1 | - |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| Lee et al. [[158](#bib.bib158)] | ICCV’21 | 不确定性感知 | 58.4 | 38.4 | 92.8 |
    52.2 | 106.1 | - |'
- en: '| TCMR [[136](#bib.bib136)] | CVPR’21 | temporal consistency | 62.3 | 41.1
    | 95.0 | 55.8 | 111.3 | - |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| TCMR [[136](#bib.bib136)] | CVPR’21 | 时间一致性 | 62.3 | 41.1 | 95.0 | 55.8 |
    111.3 | - |'
- en: '| VIBE [[135](#bib.bib135)] | CVPR’20 | self-attention temporal network | 65.6
    | 41.4 | 82.9 | 51.9 | 99.1 | [Code](https://github.com/mkocabas/VIBE) |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| VIBE [[135](#bib.bib135)] | CVPR’20 | 自注意力时间网络 | 65.6 | 41.4 | 82.9 | 51.9
    | 99.1 | [Code](https://github.com/mkocabas/VIBE) |'
- en: '| ImpHMR [[247](#bib.bib247)] | CVPR’23 | implicitly imagine person in 3D space
    | - | - | 74.3 | 45.4 | 87.1 | - |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| ImpHMR [[247](#bib.bib247)] | CVPR’23 | 在3D空间中隐式想象人物 | - | - | 74.3 | 45.4
    | 87.1 | - |'
- en: '| SGRE [[160](#bib.bib160)] | ICCV’23 | sequentially global rotation estimation
    | - | - | 78.4 | 49.6 | 93.3 | [Code](https://github.com/kennethwdk/SGRE) |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| SGRE [[160](#bib.bib160)] | ICCV’23 | 顺序全局旋转估计 | - | - | 78.4 | 49.6 | 93.3
    | [Code](https://github.com/kennethwdk/SGRE) |'
- en: '| PMCE [[4](#bib.bib4)] | ICCV’23 | pose and mesh co-evolution network | 53.5
    | 37.7 | 69.5 | 46.7 | 84.8 | [Code](https://github.com/kasvii/PMCE) |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| PMCE [[4](#bib.bib4)] | ICCV’23 | 姿态与网格共同演进网络 | 53.5 | 37.7 | 69.5 | 46.7
    | 84.8 | [Code](https://github.com/kasvii/PMCE) |'
- en: 'HumanEva dataset [[226](#bib.bib226)] is a multi-view 3D human pose estimation
    dataset comprising two versions: HumanEva-I and HumanEva-II. In HumanEva-I, the
    dataset includes around 40,000 multi-view video frames captured from seven cameras
    positioned at the front, left, and right (RGB) and four corners (Mono). Additionally,
    HumanEva-II features approximately 2,460 frames, recorded with four cameras at
    each corner.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: HumanEva 数据集 [[226](#bib.bib226)] 是一个多视角 3D 人体姿态估计数据集，包括两个版本：HumanEva-I 和 HumanEva-II。在
    HumanEva-I 中，数据集包括约 40,000 张从前面、左侧和右侧（RGB）以及四个角落（Mono）摄像机捕获的多视角视频帧。此外，HumanEva-II
    包含约 2,460 张帧，使用四个摄像机在每个角落进行录制。
- en: CMU-Panoptic dataset [[227](#bib.bib227), [248](#bib.bib248)] includes 65 frame
    sequences, approximately 5.5 hours of footage, and features 1.5 million 3D annotated
    poses. Recorded via a massively multi-view system equipped with 511 calibrated
    cameras and 10 RGB-D sensors featuring hardware-based synchronization, this dataset
    is crucial for developing weakly supervised methods through multi-view geometry.
    These methods address the occlusion problems commonly encountered in traditional
    computer vision techniques.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: CMU-Panoptic 数据集 [[227](#bib.bib227), [248](#bib.bib248)] 包括 65 个帧序列，约 5.5 小时的录像，和
    150 万个 3D 注释姿态。通过一个配备 511 个校准摄像头和 10 个 RGB-D 传感器的大规模多视角系统记录，该数据集对于通过多视角几何学开发弱监督方法至关重要。这些方法解决了传统计算机视觉技术中常见的遮挡问题。
- en: Multiperson Composited 3D Human Pose (MuCo-3DHP) dataset [[115](#bib.bib115)]
    serves as a large-scale, multi-person occluded training set for 3D human pose
    estimation. Frames in the MuCo-3DHP are generated from the MPI-INF-3DPH dataset
    through a compositing and augmentation scheme.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: Multiperson Composited 3D Human Pose (MuCo-3DHP) 数据集 [[115](#bib.bib115)] 作为一个大规模、多人的遮挡训练集用于
    3D 人体姿态估计。MuCo-3DHP 中的帧通过合成和增强方案从 MPI-INF-3DPH 数据集中生成。
- en: SURREAL dataset [[228](#bib.bib228)] is a large synthetic human body dataset
    containing 6 million RGB video frames. It provides a range of accurate annotations,
    including depth, body parts, optical flow, 2D/3D poses, and surfaces. In the SURREAL
    dataset, images exhibit variations in texture, view, and pose, and the body models
    are based on the SMPL parameters, a widely-recognized mesh representation standard.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: SURREAL 数据集 [[228](#bib.bib228)] 是一个包含 600 万 RGB 视频帧的大型合成人体数据集。它提供了一系列准确的注释，包括深度、身体部位、光流、2D/3D
    姿态和表面。在 SURREAL 数据集中，图像展现了纹理、视角和姿态的变化，身体模型基于 SMPL 参数，这是一个广泛认可的网格表示标准。
- en: 3DOH50K dataset [[150](#bib.bib150)] offers a collection of 51,600 images obtained
    from six distinct viewpoints in real-world settings, predominantly featuring object
    occlusions. Each image is annotated with ground truth 2D and 3D poses, SMPL parameters,
    and a segmentation mask. Utilized for training human estimation and reconstruction
    models, the 3DOH50K dataset facilitates exceptional performance in occlusion scenarios.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 3DOH50K 数据集 [[150](#bib.bib150)] 提供了从六个不同视点在真实世界环境中获得的 51,600 张图像，主要特点是物体遮挡。每张图像都标注有地面真实的
    2D 和 3D 姿态、SMPL 参数以及分割掩码。用于训练人体估计和重建模型，3DOH50K 数据集在遮挡场景中表现出色。
- en: 3DCP dataset [[229](#bib.bib229)] represents a 3D human mesh dataset, derived
    from AMASS [[230](#bib.bib230)]. It includes 190 self-contact meshes spanning
    six human subjects (three males and three females), each modeled with an SMPL-X
    parameterized template.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 3DCP 数据集 [[229](#bib.bib229)] 代表了一个 3D 人体网格数据集，来源于 AMASS [[230](#bib.bib230)]。它包含
    190 个自接触网格，涉及六名受试者（三名男性和三名女性），每个网格均使用 SMPL-X 参数化模板建模。
- en: AMASS dataset [[230](#bib.bib230)] constitutes a comprehensive and diverse human
    motion dataset, encompassing over 11,000 motions from 300 subjects, totaling more
    than 40 hours. The motion data, accompanied by SMPL parameters for skeleton and
    mesh representation, is derived from a marker-based MoCap system utilizing 15
    optical markers.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: AMASS 数据集 [[230](#bib.bib230)] 组成了一个全面而多样化的人体运动数据集，涵盖了 300 名受试者的 11,000 多个运动，总时长超过
    40 小时。运动数据配有用于骨架和网格表示的 SMPL 参数，来源于使用 15 个光学标记的基于标记的 MoCap 系统。
- en: DensePose dataset [[231](#bib.bib231)] features 50,000 manually annotated real
    images, comprising 5 million image-to-surface correspondence pairs extracted from
    the COCO [[249](#bib.bib249)] dataset. This dataset proves instrumental for training
    in dense human pose estimation, as well as in detection and segmentation tasks.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: DensePose 数据集 [[231](#bib.bib231)] 特征包括 50,000 张手动标注的真实图像，包含从 COCO [[249](#bib.bib249)]
    数据集中提取的 500 万对图像到表面的对应关系。该数据集对于密集人体姿态估计的训练以及检测和分割任务都非常重要。
- en: UP-3D dataset [[232](#bib.bib232)] is a dedicated 3D human pose and shape estimation
    dataset featuring extensive annotations in sports scenarios. The UP-3D comprises
    approximately 8,000 images from the LSP and MPII datasets. Additionally, each
    image in UP-3D is accompanied by a metadata file indicating the quality (medium
    or high) of the 3D fit.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: UP-3D 数据集 [[232](#bib.bib232)] 是一个专门用于3D人体姿态和形状估计的数据集，具有广泛的运动场景注释。UP-3D包含大约8,000张来自LSP和MPII数据集的图像。此外，每张UP-3D中的图像都附有一个元数据文件，指示3D拟合的质量（中等或高）。
- en: THuman dataset [[233](#bib.bib233)] constitutes a 3D real-world human mesh dataset.
    It includes 7,000 RGBD images, each featuring a textured surface mesh obtained
    using a Kinect camera. Including surface mesh with detailed texture and the aligned
    SMPL model is anticipated to significantly enhance and stimulate future research
    in human mesh reconstruction.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: THuman 数据集 [[233](#bib.bib233)] 由3D真实世界人体网格数据集构成。它包含7,000张RGBD图像，每张图像都具有通过Kinect相机获得的纹理表面网格。包括带有详细纹理的表面网格以及对齐的SMPL模型预计将显著增强并激发未来的人体网格重建研究。
- en: 7 Applications
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 种应用
- en: In this section, we review related works about human pose estimation and mesh
    recovery for a few popular applications.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了有关人体姿态估计和网格恢复的相关工作，适用于一些流行应用。
- en: 'Motion Retargeting: Human motion retargeting can transfer human actions onto
    actors. Using pose estimation eliminates the need for motion capture systems and
    achieves image-to-image translation. Therefore, 3D human pose estimation is crucial
    for retargeting. Recently, there has been a significant amount of retargeting
    work based on 3D human pose estimation [[250](#bib.bib250), [251](#bib.bib251),
    [252](#bib.bib252)]. End-to-end methods and related datasets are also designed
    [[253](#bib.bib253)]. Additionally, unsupervised methods in In-the-Wild scenarios
    [[254](#bib.bib254)] have been developed, achieved through canonicalization operations
    and derived regularizations. Beyond bodily retargeting, facial retargeting [[255](#bib.bib255),
    [256](#bib.bib256)] has also gained prominence, wherein the intricacies of facial
    expressions offer a refined portrayal of the actor’s emotional and psychological
    states.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 动作重新定向：人体动作重新定向可以将人类动作转移到演员身上。使用姿态估计消除了对动作捕捉系统的需求，实现了图像到图像的转换。因此，3D人体姿态估计对重新定向至关重要。最近，基于3D人体姿态估计的重新定向工作显著增加[[250](#bib.bib250),
    [251](#bib.bib251), [252](#bib.bib252)]。还设计了端到端方法和相关数据集[[253](#bib.bib253)]。此外，还开发了在自然场景中的无监督方法[[254](#bib.bib254)]，这些方法通过标准化操作和导出正则化实现。除了身体重新定向，面部重新定向[[255](#bib.bib255),
    [256](#bib.bib256)]也获得了重视，其中面部表情的复杂性提供了演员情感和心理状态的细致描绘。
- en: 'Action Recognition: Action recognition employs algorithms to identify and analyze
    human movements from images or videos. The results derived from 3D human pose
    estimation and reconstruction play a pivotal role in deciphering the dynamics
    of human motion within a three-dimensional context, thereby transforming these
    movements into actionable behavioral insights [[257](#bib.bib257), [258](#bib.bib258),
    [1](#bib.bib1), [259](#bib.bib259)]. Moreover, these advancements can significantly
    augment the efficiency of action recognition [[260](#bib.bib260)]. It is also
    possible to address pose estimation and action recognition within the same framework
    through multi-task learning and sharing features [[261](#bib.bib261)].'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 动作识别：动作识别利用算法从图像或视频中识别和分析人体运动。源自3D人体姿态估计和重建的结果在解读三维背景下的人体运动动态中发挥着至关重要的作用，从而将这些动作转化为可操作的行为洞察[[257](#bib.bib257),
    [258](#bib.bib258), [1](#bib.bib1), [259](#bib.bib259)]。此外，这些进展可以显著提高动作识别的效率[[260](#bib.bib260)]。通过多任务学习和特征共享，还可以在同一框架下处理姿态估计和动作识别[[261](#bib.bib261)]。
- en: 'Security Monitoring: In video surveillance systems at public places or critical
    facilities, pedestrian tracking and re-identification are key tasks. Combined
    with human pose estimation, Human tracking is utilized for the surveillance and
    analysis of pedestrian flow, tracking specific targets, and tracking and analyzing
    human behavior in space. This approach is highly beneficial for tracking humans
    in complex scenarios [[262](#bib.bib262), [263](#bib.bib263), [264](#bib.bib264),
    [265](#bib.bib265)]. Considering the constrained viewpoints of individual cameras
    in such systems, Re-identification enables the recognition and tracking of the
    same person as they move across different camera views. Human pose estimation
    significantly contributes to the enhancement of this re-identification [[10](#bib.bib10)].'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 安全监控：在公共场所或关键设施的视频监控系统中，行人跟踪和重新识别是关键任务。结合人类姿态估计，**人类跟踪**用于监控和分析行人流动、跟踪特定目标，以及跟踪和分析空间中的人类行为。这种方法对于在复杂场景中跟踪人类非常有益[[262](#bib.bib262),
    [263](#bib.bib263), [264](#bib.bib264), [265](#bib.bib265)]。考虑到这些系统中单个摄像头的受限视角，重新识别可以在不同摄像头视角中识别和跟踪同一个人。人类姿态估计显著有助于提升这一重新识别[[10](#bib.bib10)]。
- en: 'SLAM: SLAM precisely estimates its location by gathering sensor data from its
    environment, employing technologies such as cameras and LiDAR. It simultaneously
    constructs or refines an environmental map, facilitating self-localization and
    map creation in unfamiliar territories. Diverging from the conventional SLAM systems
    that predominantly concentrate on objects, recent advancements have shifted focus
    towards incorporating humans within the environmental context. Notably, Dai et
    al. [[266](#bib.bib266)] have illustrated the significance of 3D human trajectory
    reconstruction in indoor settings, enhancing indoor navigation capabilities. Furthermore,
    Kocabas et al. [[267](#bib.bib267)] have innovatively integrated human motion
    priors into SLAM, effectively merging human pose estimation with scene analysis.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: SLAM：SLAM通过收集来自环境的传感器数据来精确估计其位置，使用诸如相机和LiDAR等技术。它同时构建或完善环境地图，便于在陌生区域进行自我定位和地图创建。与传统的SLAM系统主要关注物体不同，近期的进展将重点转向将人类纳入环境上下文。值得注意的是，Dai等人[[266](#bib.bib266)]展示了在室内环境中3D人类轨迹重建的重要性，提升了室内导航能力。此外，Kocabas等人[[267](#bib.bib267)]创新性地将人类运动先验融入SLAM，有效地将人类姿态估计与场景分析结合起来。
- en: 'Autonomous Driving: In robotic navigation and autonomous vehicle applications,
    estimating human pose enables these systems to comprehend human behavior and intentions
    better. This understanding facilitates more intelligent decision-making and interaction.
    Zheng et al. [[12](#bib.bib12)] propose a multi-modal approach employing 2D labels
    on RGB images as weak supervision for 3D human pose estimation in the context
    of autonomous vehicles. Wang et al. [[13](#bib.bib13)] have developed a comprehensive
    framework to learn physically plausible human dynamics from real driving scenarios,
    effectively bridging the gap between actual and simulated human behavior in safety-critical
    applications.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶：在机器人导航和自动驾驶车辆应用中，估计人类姿态使这些系统能够更好地理解人类行为和意图。这种理解促进了更智能的决策和互动。Zheng等人[[12](#bib.bib12)]提出了一种多模态方法，利用RGB图像上的2D标签作为3D人类姿态估计的弱监督，用于自动驾驶车辆环境。Wang等人[[13](#bib.bib13)]开发了一个综合框架，从真实驾驶场景中学习物理上合理的人类动态，有效地弥合了实际与模拟人类行为在安全关键应用中的差距。
- en: 'Human–Computer Interaction: Human-computer interaction entails the bidirectional
    communication between humans and computers, underscored by the critical need for
    computers to interpret human poses accurately. Liu et al. [[11](#bib.bib11)] advanced
    this field by proposing asymmetric relation-aware representation learning for
    head pose estimation in industrial human–computer interaction, which utilizes
    an effective Lorentz distribution learning scheme. In Augmented Reality (AR) systems,
    the accuracy of human pose estimation significantly elevates the interaction quality
    between virtual objects and the real environment, fostering more natural interactions
    between humans and virtual entities. In this context, Weng et al. [[14](#bib.bib14)]
    developed a novel method and application for animating a human subject from a
    single photo within AR.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 人机交互：人机交互涉及人类与计算机之间的双向通信，其中计算机准确解释人体姿态的关键需求尤为突出。Liu 等人 [[11](#bib.bib11)] 通过提出非对称关系感知表示学习来推进这一领域，用于工业人机交互中的头部姿态估计，这利用了有效的洛伦兹分布学习方案。在增强现实（AR）系统中，人类姿态估计的准确性显著提升了虚拟物体与现实环境之间的交互质量，促进了人类与虚拟实体之间更自然的互动。在这种背景下，Weng
    等人 [[14](#bib.bib14)] 开发了一种新方法和应用，用于在AR中从单张照片动画化人体对象。
- en: 8 Challenges and Conclusion
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 个挑战和结论
- en: In this survey, we have presented a contemporary overview of recent deep learning-based
    3D human pose estimation and mesh recovery methods. A comprehensive taxonomy and
    performance comparison of these methods has been covered. We further point out
    a few promising research directions, hoping to promote advances in this field.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们提供了关于近期基于深度学习的3D人体姿态估计和网格恢复方法的现代概述。我们涵盖了这些方法的全面分类和性能比较，并进一步指出了一些有前景的研究方向，希望能够推动该领域的进展。
- en: Large Models. The efficacy of large models in language [[268](#bib.bib268),
    [269](#bib.bib269)] and foundational computer vision tasks, such as segmentation
    [[270](#bib.bib270)] and tracking [[271](#bib.bib271)], has been universally acknowledged
    and is quite remarkable. Additionally, while there is burgeoning research in human-centric
    computer vision tasks [[272](#bib.bib272)], the area of 3D tasks still demands
    further investigation. Furthermore, the development of large models not only constitutes
    a significant area of research but also the exploration of their effective utilization
    presents substantial challenges and practical importance. Exploratory efforts
    to fuse pose estimation with large language models [[273](#bib.bib273)] and the
    combination of large visual models with pose estimation are also meaningful.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 大模型。大模型在语言 [[268](#bib.bib268), [269](#bib.bib269)] 和基础计算机视觉任务，如分割 [[270](#bib.bib270)]
    和跟踪 [[271](#bib.bib271)] 中的有效性已得到广泛认可，效果相当显著。此外，尽管在人本计算机视觉任务 [[272](#bib.bib272)]
    方面的研究蓬勃发展，但3D任务领域仍需进一步调查。此外，大模型的发展不仅构成了一个重要的研究领域，而且探索其有效利用也提出了重大的挑战和实际意义。探索将姿态估计与大语言模型
    [[273](#bib.bib273)] 融合，以及将大型视觉模型与姿态估计相结合也是有意义的。
- en: More Detailed Reconstruction. At present, explicit model-based methodologies
    such as SMPL [[33](#bib.bib33)] and SMPL-X [[57](#bib.bib57)], fall short of meeting
    the demands for detail that people expect. On the other hand, implicit representation
    approaches [[190](#bib.bib190), [192](#bib.bib192), [195](#bib.bib195)], as well
    as rendering techniques such as NeRF [[198](#bib.bib198), [47](#bib.bib47)] and
    Gaussian Splatting [[208](#bib.bib208)], are capable of capturing fine details
    but lack sufficient robustness in pose estimation. Bridging the gap between robust
    pose estimation and surface details remains a formidable challenge that necessitates
    collaborative efforts from computer vision and computer graphics researchers.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细的重建。目前，基于模型的显式方法，如SMPL [[33](#bib.bib33)] 和 SMPL-X [[57](#bib.bib57)]，尚未满足人们对细节的期望。另一方面，隐式表示方法
    [[190](#bib.bib190), [192](#bib.bib192), [195](#bib.bib195)] 以及渲染技术，如NeRF [[198](#bib.bib198),
    [47](#bib.bib47)] 和 Gaussian Splatting [[208](#bib.bib208)]，能够捕捉细微的细节，但在姿态估计方面缺乏足够的鲁棒性。弥合鲁棒姿态估计与表面细节之间的差距仍然是一个巨大的挑战，需要计算机视觉和计算机图形学研究人员的协作努力。
- en: Crowding and Occlusion Challenges. In open-world scenarios, the phenomena of
    crowding and occlusion are prevalent, and they represent long-standing challenges
    in the field of object detection. Currently, top-down methods depend on object
    detection, rendering these issues inescapable. Although bottom-up strategies may
    circumvent object detection, they confront formidable challenges regarding key
    point assembly.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 拥挤和遮挡挑战。在开放世界场景中，拥挤和遮挡现象普遍存在，并且它们代表了目标检测领域长期存在的挑战。目前，自上而下的方法依赖于目标检测，使这些问题不可避免。虽然自下而上的策略可能规避目标检测，但在关键点组装方面面临严峻挑战。
- en: Speed. Speed is an essential aspect to consider in the practical deployment
    of algorithms. While most current research papers report achieving real-time performance
    on GPUs, a wide array of applications necessitates real-time and efficient processing
    on edge computing platforms, notably on ARM processors within smartphones. The
    disparity in performance between ARM processors and GPUs is pronounced, underscoring
    the immense value of optimizing for speed.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 速度。速度是在实际部署算法时必须考虑的一个重要方面。尽管大多数当前研究论文报告了在GPU上实现实时性能，但广泛的应用需要在边缘计算平台上，特别是在智能手机中的ARM处理器上进行实时和高效处理。ARM处理器和GPU之间的性能差异显著，突显了优化速度的巨大价值。
- en: References
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] H. Duan, Y. Zhao, K. Chen, D. Lin, B. Dai, Revisiting skeleton-based action
    recognition, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, 2022, pp. 2969–2978.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] H. Duan, Y. Zhao, K. Chen, D. Lin, B. Dai, 重新审视基于骨架的动作识别，见：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第2969–2978页。'
- en: '[2] Y. Zhang, C. Wang, X. Wang, W. Liu, W. Zeng, Voxeltrack: Multi-person 3d
    human pose estimation and tracking in the wild, IEEE Transactions on Pattern Analysis
    and Machine Intelligence 45 (2) (2022) 2613–2626.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Y. Zhang, C. Wang, X. Wang, W. Liu, W. Zeng, Voxeltrack：野外多人体3D姿态估计与跟踪，IEEE模式分析与机器智能汇刊
    45 (2) (2022) 2613–2626。'
- en: '[3] Y. Zhu, H. Shuai, G. Liu, Q. Liu, Multilevel spatial–temporal excited graph
    network for skeleton-based action recognition, IEEE Transactions on Image Processing
    32 (2022) 496–508.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Zhu, H. Shuai, G. Liu, Q. Liu, 多层空间–时间激发图网络用于基于骨架的动作识别，IEEE图像处理汇刊 32
    (2022) 496–508。'
- en: '[4] Y. You, H. Liu, T. Wang, W. Li, R. Ding, X. Li, Co-evolution of pose and
    mesh for 3d human body estimation from video, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, 2023, pp. 14963–14973.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Y. You, H. Liu, T. Wang, W. Li, R. Ding, X. Li, 姿态和网格的共同进化用于从视频中进行3D人体估计，见：IEEE/CVF国际计算机视觉会议论文集，2023年，第14963–14973页。'
- en: '[5] S. Tripathi, L. Müller, C.-H. P. Huang, O. Taheri, M. J. Black, D. Tzionas,
    3d human pose estimation via intuitive physics, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2023, pp. 4713–4725.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] S. Tripathi, L. Müller, C.-H. P. Huang, O. Taheri, M. J. Black, D. Tzionas,
    通过直观物理学进行3D人体姿态估计，见：IEEE/CVF计算机视觉与模式识别会议论文集，2023年，第4713–4725页。'
- en: '[6] Z. Fan, M. Parelli, M. E. Kadoglou, M. Kocabas, X. Chen, M. J. Black, O. Hilliges,
    Hold: Category-agnostic 3d reconstruction of interacting hands and objects from
    video, arXiv preprint arXiv:2311.18448 (2023).'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Z. Fan, M. Parelli, M. E. Kadoglou, M. Kocabas, X. Chen, M. J. Black, O.
    Hilliges, Hold：从视频中进行类别无关的3D交互手和物体重建，arXiv预印本 arXiv:2311.18448 (2023)。'
- en: '[7] L. Dai, L. Ma, S. Qian, H. Liu, Z. Liu, H. Xiong, Cloth2body: Generating
    3d human body mesh from 2d clothing, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2023, pp. 15007–15017.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] L. Dai, L. Ma, S. Qian, H. Liu, Z. Liu, H. Xiong, Cloth2body：从2D服装生成3D人体网格，见：IEEE/CVF国际计算机视觉会议论文集，2023年，第15007–15017页。'
- en: '[8] S. Tang, G. Wang, Q. Ran, L. Li, L. Shen, P. Tan, High-resolution volumetric
    reconstruction for clothed humans, ACM Transactions on Graphics 42 (5) (2023)
    1–15.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] S. Tang, G. Wang, Q. Ran, L. Li, L. Shen, P. Tan, 高分辨率体积重建用于穿衣人类，ACM图形学汇刊
    42 (5) (2023) 1–15。'
- en: '[9] Y. Feng, W. Liu, T. Bolkart, J. Yang, M. Pollefeys, M. J. Black, Learning
    disentangled avatars with hybrid 3d representations, arXiv preprint arXiv:2309.06441
    (2023).'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Y. Feng, W. Liu, T. Bolkart, J. Yang, M. Pollefeys, M. J. Black, 学习具有混合3D表示的解耦头像，arXiv预印本
    arXiv:2309.06441 (2023)。'
- en: '[10] P. Wang, Z. Zhao, F. Su, X. Zu, N. V. Boulgouris, Horeid: deep high-order
    mapping enhances pose alignment for person re-identification, IEEE Transactions
    on Image Processing 30 (2021) 2908–2922.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] P. Wang, Z. Zhao, F. Su, X. Zu, N. V. Boulgouris, Horeid：深度高阶映射增强姿态对齐以进行人物再识别，IEEE图像处理汇刊
    30 (2021) 2908–2922。'
- en: '[11] H. Liu, T. Liu, Z. Zhang, A. K. Sangaiah, B. Yang, Y. Li, Arhpe: Asymmetric
    relation-aware representation learning for head pose estimation in industrial
    human–computer interaction, IEEE Transactions on Industrial Informatics 18 (10)
    (2022) 7107–7117.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] H. Liu, T. Liu, Z. Zhang, A. K. Sangaiah, B. Yang, Y. Li, Arhpe: 用于工业人机交互中的头部姿态估计的非对称关系感知表示学习,
    《IEEE工业信息学交易》18 (10) (2022) 7107–7117.'
- en: '[12] J. Zheng, X. Shi, A. Gorban, J. Mao, Y. Song, C. R. Qi, T. Liu, V. Chari,
    A. Cornman, Y. Zhou, et al., Multi-modal 3d human pose estimation with 2d weak
    supervision in autonomous driving, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2022, pp. 4478–4487.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] J. Zheng, X. Shi, A. Gorban, J. Mao, Y. Song, C. R. Qi, T. Liu, V. Chari,
    A. Cornman, Y. Zhou, 等, 在自动驾驶中使用 2D 弱监督进行多模态 3D 人体姿态估计, 见: 《IEEE/CVF计算机视觉与模式识别大会论文集》，2022年，pp.
    4478–4487.'
- en: '[13] J. Wang, Y. Yuan, Z. Luo, K. Xie, D. Lin, U. Iqbal, S. Fidler, S. Khamis,
    Learning human dynamics in autonomous driving scenarios, in: Proceedings of the
    IEEE/CVF International Conference on Computer Vision, 2023, pp. 20796–20806.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Wang, Y. Yuan, Z. Luo, K. Xie, D. Lin, U. Iqbal, S. Fidler, S. Khamis,
    在自动驾驶场景中学习人类动态, 见: 《IEEE/CVF国际计算机视觉大会论文集》，2023年，pp. 20796–20806.'
- en: '[14] C.-Y. Weng, B. Curless, I. Kemelmacher-Shlizerman, Photo wake-up: 3d character
    animation from a single photo, in: Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, 2019, pp. 5908–5917.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] C.-Y. Weng, B. Curless, I. Kemelmacher-Shlizerman, Photo wake-up: 从单张照片生成
    3D 角色动画, 见: 《IEEE/CVF计算机视觉与模式识别会议论文集》，2019年，pp. 5908–5917.'
- en: '[15] W. Liu, Q. Bao, Y. Sun, T. Mei, Recent advances of monocular 2d and 3d
    human pose estimation: A deep learning perspective, ACM Computing Surveys 55 (4)
    (2022) 1–41.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] W. Liu, Q. Bao, Y. Sun, T. Mei, 单目 2D 和 3D 人体姿态估计的最新进展: 深度学习视角, 《ACM计算机调查》55
    (4) (2022) 1–41.'
- en: '[16] C. Zheng, W. Wu, C. Chen, T. Yang, S. Zhu, J. Shen, N. Kehtarnavaz, M. Shah,
    Deep learning-based human pose estimation: A survey, ACM Computing Surveys 56 (1)
    (2023) 1–37.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] C. Zheng, W. Wu, C. Chen, T. Yang, S. Zhu, J. Shen, N. Kehtarnavaz, M.
    Shah, 基于深度学习的人体姿态估计: 综述, 《ACM计算机调查》56 (1) (2023) 1–37.'
- en: '[17] Y. Tian, H. Zhang, Y. Liu, L. Wang, Recovering 3d human mesh from monocular
    images: A survey, IEEE transactions on pattern analysis and machine intelligence
    (2023).'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Y. Tian, H. Zhang, Y. Liu, L. Wang, 从单目图像恢复 3D 人体网格: 综述, 《IEEE模式分析与机器智能交易》
    (2023).'
- en: '[18] L. Chen, S. Peng, X. Zhou, Towards efficient and photorealistic 3d human
    reconstruction: a brief survey, Visual Informatics 5 (4) (2021) 11–19.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] L. Chen, S. Peng, X. Zhou, 朝向高效且照片级真实感的 3D 人体重建: 简要综述, 《视觉信息学》5 (4) (2021)
    11–19.'
- en: '[19] M. Einfalt, K. Ludwig, R. Lienhart, Uplift and upsample: Efficient 3d
    human pose estimation with uplifting transformers, in: Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision, 2023, pp. 2903–2913.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] M. Einfalt, K. Ludwig, R. Lienhart, Uplift and upsample: 使用提升变换器进行高效 3D
    人体姿态估计, 见: 《IEEE/CVF冬季计算机视觉应用会议论文集》，2023年，pp. 2903–2913.'
- en: '[20] Y. Luo, Y. Li, M. Foshey, W. Shou, P. Sharma, T. Palacios, A. Torralba,
    W. Matusik, Intelligent carpet: Inferring 3d human pose from tactile signals,
    in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    2021, pp. 11255–11265.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. Luo, Y. Li, M. Foshey, W. Shou, P. Sharma, T. Palacios, A. Torralba,
    W. Matusik, Intelligent carpet: 从触觉信号推断 3D 人体姿态, 见: 《IEEE/CVF计算机视觉与模式识别会议论文集》，2021年，pp.
    11255–11265.'
- en: '[21] A. Ruget, M. Tyler, G. Mora Martín, S. Scholes, F. Zhu, I. Gyongy, B. Hearn,
    S. McLaughlin, A. Halimi, J. Leach, Pixels2pose: Super-resolution time-of-flight
    imaging for 3d pose estimation, Science Advances 8 (48) (2022) eade0123.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] A. Ruget, M. Tyler, G. Mora Martín, S. Scholes, F. Zhu, I. Gyongy, B.
    Hearn, S. McLaughlin, A. Halimi, J. Leach, Pixels2pose: 超分辨率飞行时间成像用于 3D 姿态估计,
    《科学进展》8 (48) (2022) eade0123.'
- en: '[22] R. Pandey, A. Tkach, S. Yang, P. Pidlypenskyi, J. Taylor, R. Martin-Brualla,
    A. Tagliasacchi, G. Papandreou, P. Davidson, C. Keskin, et al., Volumetric capture
    of humans with a single rgbd camera via semi-parametric learning, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp.
    9709–9718.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] R. Pandey, A. Tkach, S. Yang, P. Pidlypenskyi, J. Taylor, R. Martin-Brualla,
    A. Tagliasacchi, G. Papandreou, P. Davidson, C. Keskin, 等, 使用单个 RGBD 相机进行人体体积捕捉，通过半参数学习,
    见: 《IEEE/CVF计算机视觉与模式识别大会论文集》，2019年，pp. 9709–9718.'
- en: '[23] Y. Ren, Z. Wang, Y. Wang, S. Tan, Y. Chen, J. Yang, Gopose: 3d human pose
    estimation using wifi, Proceedings of the ACM on Interactive, Mobile, Wearable
    and Ubiquitous Technologies 6 (2) (2022) 1–25.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Y. Ren, Z. Wang, Y. Wang, S. Tan, Y. Chen, J. Yang, Gopose: 利用 WiFi 进行
    3D 人体姿态估计, 《ACM互动、移动、可穿戴和无处不在技术会议论文集》6 (2) (2022) 1–25.'
- en: '[24] T. Li, L. Fan, Y. Yuan, D. Katabi, Unsupervised learning for human sensing
    using radio signals, in: Proceedings of the IEEE/CVF Winter Conference on Applications
    of Computer Vision, 2022, pp. 3288–3297.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] T. Li, L. Fan, Y. Yuan, D. Katabi, 使用无线电信号进行人体感知的无监督学习，发表于：IEEE/CVF冬季计算机视觉应用会议论文集，2022，第3288–3297页。'
- en: '[25] J. L. Ponton, H. Yun, A. Aristidou, C. Andujar, N. Pelechano, Sparseposer:
    Real-time full-body motion reconstruction from sparse data, ACM Transactions on
    Graphics 43 (1) (2023) 1–14.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] J. L. Ponton, H. Yun, A. Aristidou, C. Andujar, N. Pelechano, Sparseposer：实时全身运动重建从稀疏数据，ACM计算机图形学汇刊43
    (1) (2023) 1–14。'
- en: '[26] F. Huang, A. Zeng, M. Liu, Q. Lai, Q. Xu, Deepfuse: An imu-aware network
    for real-time 3d human pose estimation from multi-view image, in: Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2020, pp.
    429–438.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] F. Huang, A. Zeng, M. Liu, Q. Lai, Q. Xu, Deepfuse：一个感知IMU的网络用于实时3D人体姿态估计，发表于：IEEE/CVF冬季计算机视觉应用会议论文集，2020，第429–438页。'
- en: '[27] S. Zou, X. Zuo, S. Wang, Y. Qian, C. Guo, L. Cheng, Human pose and shape
    estimation from single polarization images, IEEE Transactions on Multimedia (2022).'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. Zou, X. Zuo, S. Wang, Y. Qian, C. Guo, L. Cheng, 从单偏振图像中估计人体姿态和形状，IEEE多媒体汇刊（2022）。'
- en: '[28] L. Xu, W. Xu, V. Golyanik, M. Habermann, L. Fang, C. Theobalt, Eventcap:
    Monocular 3d capture of high-speed human motions using an event camera, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp.
    4968–4978.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] L. Xu, W. Xu, V. Golyanik, M. Habermann, L. Fang, C. Theobalt, Eventcap：使用事件相机单目捕捉高速人体动作，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2020，第4968–4978页。'
- en: '[29] B. Jiang, L. Hu, S. Xia, Probabilistic triangulation for uncalibrated
    multi-view 3d human pose estimation, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2023, pp. 14850–14860.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] B. Jiang, L. Hu, S. Xia, 概率三角测量用于未校准多视角3D人体姿态估计，发表于：IEEE/CVF国际计算机视觉会议论文集，2023，第14850–14860页。'
- en: '[30] H. Shuai, L. Wu, Q. Liu, Adaptive multi-view and temporal fusing transformer
    for 3d human pose estimation, IEEE Transactions on Pattern Analysis and Machine
    Intelligence 45 (4) (2022) 4122–4135.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] H. Shuai, L. Wu, Q. Liu, 自适应多视角和时间融合变换器用于3D人体姿态估计，IEEE模式分析与机器智能汇刊45 (4)
    (2022) 4122–4135。'
- en: '[31] B. Huang, Y. Shu, T. Zhang, Y. Wang, Dynamic multi-person mesh recovery
    from uncalibrated multi-view cameras, in: 2021 International Conference on 3D
    Vision (3DV), IEEE, 2021, pp. 710–720.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] B. Huang, Y. Shu, T. Zhang, Y. Wang, 从未校准的多视角摄像机中动态恢复多人体网格，发表于：2021年国际3D视觉会议（3DV），IEEE，2021，第710–720页。'
- en: '[32] D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers, J. Davis,
    Scape: shape completion and animation of people, in: ACM SIGGRAPH 2005 Papers,
    2005, pp. 408–416.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers, J. Davis,
    Scape：形状补全与人物动画，发表于：ACM SIGGRAPH 2005论文集，2005，第408–416页。'
- en: '[33] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, M. J. Black, Smpl: A skinned
    multi-person linear model, ACM transactions on graphics (TOG) 34 (6) (2015) 1–16.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, M. J. Black, SMPL：一个带皮肤的多人体线性模型，ACM计算机图形学汇刊（TOG）34
    (6) (2015) 1–16。'
- en: '[34] J. Romero, D. Tzionas, M. J. Black, Embodied hands: Modeling and capturing
    hands and bodies together, arXiv preprint arXiv:2201.02610 (2022).'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] J. Romero, D. Tzionas, M. J. Black, 具身手部：建模和捕捉手部与身体，arXiv预印本 arXiv:2201.02610
    (2022)。'
- en: '[35] T. Li, T. Bolkart, M. J. Black, H. Li, J. Romero, Learning a model of
    facial shape and expression from 4d scans., ACM Trans. Graph. 36 (6) (2017) 194–1.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] T. Li, T. Bolkart, M. J. Black, H. Li, J. Romero, 从4D扫描中学习面部形状和表情模型，ACM计算机图形学汇刊36
    (6) (2017) 194–1。'
- en: '[36] G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, A. A. Osman, D. Tzionas,
    M. J. Black, Expressive body capture: 3d hands, face, and body from a single image,
    in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    2019, pp. 10975–10985.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, A. A. Osman, D. Tzionas,
    M. J. Black, 表达性体捕捉：从单幅图像中获取3D手部、面部和身体，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2019，第10975–10985页。'
- en: '[37] B. Jiang, Y. Zhang, X. Wei, X. Xue, Y. Fu, H4d: Human 4d modeling by learning
    neural compositional representation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2022, pp. 19355–19365.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] B. Jiang, Y. Zhang, X. Wei, X. Xue, Y. Fu, H4D：通过学习神经组合表示进行人体4D建模，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2022，第19355–19365页。'
- en: '[38] Z. Zheng, T. Yu, Y. Liu, Q. Dai, Pamir: Parametric model-conditioned implicit
    representation for image-based human reconstruction, IEEE transactions on pattern
    analysis and machine intelligence 44 (6) (2021) 3170–3184.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Z. Zheng, T. Yu, Y. Liu, Q. Dai, Pamir: 参数化模型条件隐式表示用于基于图像的人类重建，IEEE模式分析与机器智能学报
    44 (6) (2021) 3170–3184。'
- en: '[39] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    2016, pp. 770–778.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] K. He, X. Zhang, S. Ren, J. Sun, 深度残差学习用于图像识别，见：IEEE计算机视觉与模式识别会议论文集，2016年，第770–778页。'
- en: '[40] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu,
    M. Tan, X. Wang, et al., Deep high-resolution representation learning for visual
    recognition, IEEE transactions on pattern analysis and machine intelligence 43 (10)
    (2020) 3349–3364.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu,
    M. Tan, X. Wang, 等, 深度高分辨率表示学习用于视觉识别，IEEE模式分析与机器智能学报 43 (10) (2020) 3349–3364。'
- en: '[41] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner,
    J. Yung, A. Steiner, D. Keysers, J. Uszkoreit, et al., Mlp-mixer: An all-mlp architecture
    for vision, Advances in neural information processing systems 34 (2021) 24261–24272.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner,
    J. Yung, A. Steiner, D. Keysers, J. Uszkoreit, 等, Mlp-mixer: 一种全MLP架构用于视觉，神经信息处理系统进展
    34 (2021) 24261–24272。'
- en: '[42] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., An image is worth 16x16
    words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929
    (2020).'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, 等, 一张图片价值16x16个词：用于大规模图像识别的Transformers，arXiv预印本
    arXiv:2010.11929 (2020)。'
- en: '[43] C.-Y. Yang, J. Luo, L. Xia, Y. Sun, N. Qiao, K. Zhang, Z. Jiang, J.-N.
    Hwang, C.-H. Kuo, Camerapose: Weakly-supervised monocular 3d human pose estimation
    by leveraging in-the-wild 2d annotations, in: Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision, 2023, pp. 2924–2933.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] C.-Y. Yang, J. Luo, L. Xia, Y. Sun, N. Qiao, K. Zhang, Z. Jiang, J.-N.
    Hwang, C.-H. Kuo, Camerapose: 通过利用野外2D标注进行弱监督单目3D人类姿态估计，见：IEEE/CVF冬季计算机视觉应用会议论文集，2023年，第2924–2933页。'
- en: '[44] A. Zanfir, E. G. Bazavan, H. Xu, W. T. Freeman, R. Sukthankar, C. Sminchisescu,
    Weakly supervised 3d human pose and shape reconstruction with normalizing flows,
    in: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part VI 16, Springer, 2020, pp. 465–481.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] A. Zanfir, E. G. Bazavan, H. Xu, W. T. Freeman, R. Sukthankar, C. Sminchisescu,
    弱监督3D人类姿态与形状重建与归一化流，见：计算机视觉–ECCV 2020: 第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第VI部分
    16，Springer，2020年，第465–481页。'
- en: '[45] W. Chai, Z. Jiang, J.-N. Hwang, G. Wang, Global adaptation meets local
    generalization: Unsupervised domain adaptation for 3d human pose estimation, arXiv
    preprint arXiv:2303.16456 (2023).'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] W. Chai, Z. Jiang, J.-N. Hwang, G. Wang, 全球适应遇上局部泛化：无监督领域适应用于3D人类姿态估计，arXiv预印本
    arXiv:2303.16456 (2023)。'
- en: '[46] Z. Yu, J. Wang, J. Xu, B. Ni, C. Zhao, M. Wang, W. Zhang, Skeleton2mesh:
    Kinematics prior injected unsupervised human mesh recovery, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision, 2021, pp. 8619–8629.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Z. Yu, J. Wang, J. Xu, B. Ni, C. Zhao, M. Wang, W. Zhang, Skeleton2mesh:
    运动学先验注入的无监督人类网格恢复，见：IEEE/CVF国际计算机视觉会议论文集，2021年，第8619–8629页。'
- en: '[47] J. Mu, S. Sang, N. Vasconcelos, X. Wang, Actorsnerf: Animatable few-shot
    human rendering with generalizable nerfs, arXiv preprint arXiv:2304.14401 (2023).'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] J. Mu, S. Sang, N. Vasconcelos, X. Wang, Actorsnerf: 可动画的少样本人类渲染与可泛化的nerfs，arXiv预印本
    arXiv:2304.14401 (2023)。'
- en: '[48] A. Benzine, F. Chabot, B. Luvison, Q. C. Pham, C. Achard, Pandanet: Anchor-based
    single-shot multi-person 3d pose estimation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2020, pp. 6856–6865.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] A. Benzine, F. Chabot, B. Luvison, Q. C. Pham, C. Achard, Pandanet: 基于锚点的单次多人物3D姿态估计，见：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，第6856–6865页。'
- en: '[49] Z. Yang, A. Zeng, C. Yuan, Y. Li, Effective whole-body pose estimation
    with two-stages distillation, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2023, pp. 4210–4220.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Z. Yang, A. Zeng, C. Yuan, Y. Li, 有效的全身姿态估计与两阶段蒸馏，见：IEEE/CVF国际计算机视觉会议论文集，2023年，第4210–4220页。'
- en: '[50] S. Tripathi, S. Ranade, A. Tyagi, A. Agrawal, Posenet3d: Learning temporally
    consistent 3d human pose via knowledge distillation, in: 2020 International Conference
    on 3D Vision (3DV), IEEE, 2020, pp. 311–321.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] S. Tripathi, S. Ranade, A. Tyagi, A. Agrawal, Posenet3d: 通过知识蒸馏学习时间一致的
    3D 人体姿态，发表于：2020 年国际 3D 视觉会议（3DV），IEEE，2020年，第311–321页。'
- en: '[51] H. Liu, C. Ren, An effective 3d human pose estimation method based on
    dilated convolutions for videos, in: 2019 IEEE International Conference on Robotics
    and Biomimetics (ROBIO), IEEE, 2019, pp. 2327–2331.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] H. Liu, C. Ren, 一种基于膨胀卷积的有效 3D 人体姿态估计方法，发表于：2019 IEEE 国际机器人与生物仿真会议（ROBIO），IEEE，2019年，第2327–2331页。'
- en: '[52] S. Choi, S. Choi, C. Kim, Mobilehumanpose: Toward real-time 3d human pose
    estimation in mobile devices, in: Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, 2021, pp. 2328–2338.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] S. Choi, S. Choi, C. Kim, Mobilehumanpose: 面向移动设备的实时 3D 人体姿态估计，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2021年，第2328–2338页。'
- en: '[53] H. Cho, Y. Cho, J. Yu, J. Kim, Camera distortion-aware 3d human pose estimation
    in video with optimization-based meta-learning, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, 2021, pp. 11169–11178.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] H. Cho, Y. Cho, J. Yu, J. Kim, 基于优化的元学习的相机畸变感知 3D 人体姿态估计，发表于：IEEE/CVF
    国际计算机视觉会议论文集，2021年，第11169–11178页。'
- en: '[54] K. Gong, B. Li, J. Zhang, T. Wang, J. Huang, M. B. Mi, J. Feng, X. Wang,
    Posetriplet: Co-evolving 3d human pose estimation, imitation, and hallucination
    under self-supervision, in: Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, 2022, pp. 11017–11027.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] K. Gong, B. Li, J. Zhang, T. Wang, J. Huang, M. B. Mi, J. Feng, X. Wang,
    Posetriplet: 在自我监督下共同进化的 3D 人体姿态估计、模仿和幻觉，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，2022年，第11017–11027页。'
- en: '[55] M. T. Hassan, A. B. Hamza, Regular splitting graph network for 3d human
    pose estimation, IEEE Transactions on Image Processing (2023).'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] M. T. Hassan, A. B. Hamza, 正则分割图网络用于 3D 人体姿态估计，IEEE 图像处理汇刊（2023）。'
- en: '[56] Q. Zhao, C. Zheng, M. Liu, P. Wang, C. Chen, Poseformerv2: Exploring frequency
    domain for efficient and robust 3d human pose estimation, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 8877–8886.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Q. Zhao, C. Zheng, M. Liu, P. Wang, C. Chen, Poseformerv2: 探索频域以实现高效且鲁棒的
    3D 人体姿态估计，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，2023年，第8877–8886页。'
- en: '[57] Z. Cai, W. Yin, A. Zeng, C. Wei, Q. Sun, Y. Wang, H. E. Pang, H. Mei,
    M. Zhang, L. Zhang, et al., Smpler-x: Scaling up expressive human pose and shape
    estimation, arXiv preprint arXiv:2309.17448 (2023).'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Z. Cai, W. Yin, A. Zeng, C. Wei, Q. Sun, Y. Wang, H. E. Pang, H. Mei,
    M. Zhang, L. Zhang, 等，Smpler-x: 扩展表现力的人体姿态和形状估计，arXiv 预印本 arXiv:2309.17448（2023）。'
- en: '[58] R. Li, Y. Xiu, S. Saito, Z. Huang, K. Olszewski, H. Li, Monocular real-time
    volumetric performance capture, in: Computer Vision–ECCV 2020: 16th European Conference,
    Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIII 16, Springer, 2020, pp.
    49–67.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] R. Li, Y. Xiu, S. Saito, Z. Huang, K. Olszewski, H. Li, 单目实时体积性能捕捉，发表于：计算机视觉–ECCV
    2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第XXIII卷，第16篇，Springer，2020年，第49–67页。'
- en: '[59] H. Onizuka, Z. Hayirci, D. Thomas, A. Sugimoto, H. Uchiyama, R.-i. Taniguchi,
    Tetratsdf: 3d human reconstruction from a single image with a tetrahedral outer
    shell, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, 2020, pp. 6011–6020.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] H. Onizuka, Z. Hayirci, D. Thomas, A. Sugimoto, H. Uchiyama, R.-i. Taniguchi,
    Tetratsdf: 从单张图像中通过四面体外壳进行 3D 人体重建，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，2020年，第6011–6020页。'
- en: '[60] G. Wei, C. Lan, W. Zeng, Z. Chen, View invariant 3d human pose estimation,
    IEEE Transactions on Circuits and Systems for Video Technology 30 (12) (2019)
    4601–4610.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] G. Wei, C. Lan, W. Zeng, Z. Chen, 视图不变的 3D 人体姿态估计，IEEE 图像处理汇刊 30 (12)
    (2019) 4601–4610。'
- en: '[61] Y. Zhan, F. Li, R. Weng, W. Choi, Ray3d: ray-based 3d human pose estimation
    for monocular absolute 3d localization, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2022, pp. 13116–13125.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Y. Zhan, F. Li, R. Weng, W. Choi, Ray3d: 基于光线的 3D 人体姿态估计用于单目绝对 3D 定位，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2022年，第13116–13125页。'
- en: '[62] K. Zhou, X. Han, N. Jiang, K. Jia, J. Lu, Hemlets posh: learning part-centric
    heatmap triplets for 3d human pose and shape estimation, IEEE Transactions on
    Pattern Analysis and Machine Intelligence 44 (6) (2021) 3000–3014.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] K. Zhou, X. Han, N. Jiang, K. Jia, J. Lu, Hemlets posh: 学习以部件为中心的热图三元组用于
    3D 人体姿态和形状估计，IEEE 模式分析与机器智能汇刊 44 (6) (2021) 3000–3014。'
- en: '[63] X. Zheng, X. Chen, X. Lu, A joint relationship aware neural network for
    single-image 3d human pose estimation, IEEE Transactions on Image Processing 29
    (2020) 4747–4758.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] X. Zheng, X. Chen, X. Lu, 一个联合关系感知的神经网络用于单幅图像的3D人体姿态估计，IEEE 图像处理学报 29
    (2020) 4747–4758。'
- en: '[64] L. Wu, Z. Yu, Y. Liu, Q. Liu, Limb pose aware networks for monocular 3d
    pose estimation, IEEE Transactions on Image Processing 31 (2021) 906–917.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] L. Wu, Z. Yu, Y. Liu, Q. Liu, 肢体姿态感知网络用于单目3D姿态估计，IEEE 图像处理学报 31 (2021)
    906–917。'
- en: '[65] Y. Xu, W. Wang, T. Liu, X. Liu, J. Xie, S.-C. Zhu, Monocular 3d pose estimation
    via pose grammar and data augmentation, IEEE Transactions on Pattern Analysis
    and Machine Intelligence 44 (10) (2021) 6327–6344.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Y. Xu, W. Wang, T. Liu, X. Liu, J. Xie, S.-C. Zhu, 通过姿态语法和数据增强进行单目3D姿态估计，IEEE
    计算机图像分析与机器智能学报 44 (10) (2021) 6327–6344。'
- en: '[66] M. Fisch, R. Clark, Orientation keypoints for 6d human pose estimation,
    IEEE Transactions on Pattern Analysis and Machine Intelligence 44 (12) (2021)
    10145–10158.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] M. Fisch, R. Clark, 6D人体姿态估计的方向关键点，IEEE 计算机图像分析与机器智能学报 44 (12) (2021)
    10145–10158。'
- en: '[67] J. Liu, H. Ding, A. Shahroudy, L.-Y. Duan, X. Jiang, G. Wang, A. C. Kot,
    Feature boosting network for 3d pose estimation, IEEE transactions on pattern
    analysis and machine intelligence 42 (2) (2019) 494–501.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] J. Liu, H. Ding, A. Shahroudy, L.-Y. Duan, X. Jiang, G. Wang, A. C. Kot,
    特征增强网络用于3D姿态估计，IEEE 计算机图像分析与机器智能学报 42 (2) (2019) 494–501。'
- en: '[68] H. Ci, X. Ma, C. Wang, Y. Wang, Locally connected network for monocular
    3d human pose estimation, IEEE Transactions on Pattern Analysis and Machine Intelligence
    44 (3) (2020) 1429–1442.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] H. Ci, X. Ma, C. Wang, Y. Wang, 局部连接网络用于单目3D人体姿态估计，IEEE 计算机图像分析与机器智能学报
    44 (3) (2020) 1429–1442。'
- en: '[69] Z. Zou, W. Tang, Modulated graph convolutional network for 3d human pose
    estimation, in: Proceedings of the IEEE/CVF International Conference on Computer
    Vision, 2021, pp. 11477–11487.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Z. Zou, W. Tang, 调制图卷积网络用于3D人体姿态估计，见：IEEE/CVF 国际计算机视觉会议论文集，2021，页11477–11487。'
- en: '[70] A. Zeng, X. Sun, L. Yang, N. Zhao, M. Liu, Q. Xu, Learning skeletal graph
    neural networks for hard 3d pose estimation, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2021, pp. 11436–11445.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] A. Zeng, X. Sun, L. Yang, N. Zhao, M. Liu, Q. Xu, 学习骨架图神经网络用于困难的3D姿态估计，见：IEEE/CVF
    国际计算机视觉会议论文集，2021，页11436–11445。'
- en: '[71] K. Zhai, Q. Nie, B. Ouyang, X. Li, S. Yang, Hopfir: Hop-wise graphformer
    with intragroup joint refinement for 3d human pose estimation, arXiv preprint
    arXiv:2302.14581 (2023).'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] K. Zhai, Q. Nie, B. Ouyang, X. Li, S. Yang, Hopfir: 基于跳跃的图变换器与组内联合精化用于3D人体姿态估计，arXiv
    预印本 arXiv:2302.14581 (2023)。'
- en: '[72] K. Iskakov, E. Burkov, V. Lempitsky, Y. Malkov, Learnable triangulation
    of human pose, in: Proceedings of the IEEE/CVF international conference on computer
    vision, 2019, pp. 7718–7727.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] K. Iskakov, E. Burkov, V. Lempitsky, Y. Malkov, 可学习的人体姿态三角化，见：IEEE/CVF
    国际计算机视觉会议论文集，2019，页7718–7727。'
- en: '[73] H. Qiu, C. Wang, J. Wang, N. Wang, W. Zeng, Cross view fusion for 3d human
    pose estimation, in: Proceedings of the IEEE/CVF international conference on computer
    vision, 2019, pp. 4342–4351.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] H. Qiu, C. Wang, J. Wang, N. Wang, W. Zeng, 交叉视角融合用于3D人体姿态估计，见：IEEE/CVF
    国际计算机视觉会议论文集，2019，页4342–4351。'
- en: '[74] E. Remelli, S. Han, S. Honari, P. Fua, R. Wang, Lightweight multi-view
    3d pose estimation through camera-disentangled representation, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp.
    6040–6049.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] E. Remelli, S. Han, S. Honari, P. Fua, R. Wang, 轻量级多视角3D姿态估计通过相机解耦表示，见：IEEE/CVF
    计算机视觉与模式识别会议论文集，2020，页6040–6049。'
- en: '[75] Z. Zhang, C. Wang, W. Qiu, W. Qin, W. Zeng, Adafuse: Adaptive multiview
    fusion for accurate human pose estimation in the wild, International Journal of
    Computer Vision 129 (2021) 703–718.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Z. Zhang, C. Wang, W. Qiu, W. Qin, W. Zeng, Adafuse: 自适应多视角融合用于准确的人体姿态估计，国际计算机视觉学报
    129 (2021) 703–718。'
- en: '[76] K. Bartol, D. Bojanić, T. Petković, T. Pribanić, Generalizable human pose
    triangulation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, 2022, pp. 11028–11037.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] K. Bartol, D. Bojanić, T. Petković, T. Pribanić, 可推广的人体姿态三角化，见：IEEE/CVF
    计算机视觉与模式识别会议论文集，2022，页11028–11037。'
- en: '[77] D. C. Luvizon, D. Picard, H. Tabia, Consensus-based optimization for 3d
    human pose estimation in camera coordinates, International Journal of Computer
    Vision 130 (3) (2022) 869–882.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] D. C. Luvizon, D. Picard, H. Tabia, 基于共识的优化用于相机坐标中的3D人体姿态估计，国际计算机视觉学报
    130 (3) (2022) 869–882。'
- en: '[78] Y. Kudo, K. Ogaki, Y. Matsui, Y. Odagiri, Unsupervised adversarial learning
    of 3d human pose from 2d joint locations, arXiv preprint arXiv:1803.08244 (2018).'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Y. Kudo, K. Ogaki, Y. Matsui, Y. Odagiri, 从2D关节位置中进行无监督对抗学习的3D人体姿态，arXiv预印本
    arXiv:1803.08244 (2018)。'
- en: '[79] C.-H. Chen, A. Tyagi, A. Agrawal, D. Drover, R. Mv, S. Stojanov, J. M.
    Rehg, Unsupervised 3d pose estimation with geometric self-supervision, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp.
    5714–5724.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] C.-H. Chen, A. Tyagi, A. Agrawal, D. Drover, R. Mv, S. Stojanov, J. M.
    Rehg, 使用几何自监督的无监督3D姿态估计，见：IEEE/CVF计算机视觉与模式识别会议论文集，2019年，第5714–5724页。'
- en: '[80] B. Wandt, J. J. Little, H. Rhodin, Elepose: Unsupervised 3d human pose
    estimation by predicting camera elevation and learning normalizing flows on 2d
    poses, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, 2022, pp. 6635–6645.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] B. Wandt, J. J. Little, H. Rhodin, Elepose: 通过预测相机高度并在2D姿态上学习归一化流进行无监督的3D人体姿态估计，见：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第6635–6645页。'
- en: '[81] M. Kocabas, S. Karagoz, E. Akbas, Self-supervised learning of 3d human
    pose using multi-view geometry, in: Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition, 2019, pp. 1077–1086.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M. Kocabas, S. Karagoz, E. Akbas, 使用多视角几何的自监督3D人体姿态学习，见：IEEE/CVF计算机视觉与模式识别会议论文集，2019年，第1077–1086页。'
- en: '[82] K. Wang, L. Lin, C. Jiang, C. Qian, P. Wei, 3d human pose machines with
    self-supervised learning, IEEE transactions on pattern analysis and machine intelligence
    42 (5) (2019) 1069–1082.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] K. Wang, L. Lin, C. Jiang, C. Qian, P. Wei, 具有自监督学习的3D人体姿态机器，IEEE模式分析与机器智能汇刊
    42 (5) (2019) 1069–1082。'
- en: '[83] J. N. Kundu, S. Seth, P. YM, V. Jampani, A. Chakraborty, R. V. Babu, Uncertainty-aware
    adaptation for self-supervised 3d human pose estimation, in: Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 20448–20459.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] J. N. Kundu, S. Seth, P. YM, V. Jampani, A. Chakraborty, R. V. Babu, 具有不确定性意识的自监督3D人体姿态估计适配，见：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第20448–20459页。'
- en: '[84] G. Hua, H. Liu, W. Li, Q. Zhang, R. Ding, X. Xu, Weakly-supervised 3d
    human pose estimation with cross-view u-shaped graph convolutional network, IEEE
    Transactions on Multimedia (2022).'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] G. Hua, H. Liu, W. Li, Q. Zhang, R. Ding, X. Xu, 使用交叉视图U形图卷积网络的弱监督3D人体姿态估计，IEEE多媒体汇刊
    (2022)。'
- en: '[85] M. Gholami, B. Wandt, H. Rhodin, R. Ward, Z. J. Wang, Adaptpose: Cross-dataset
    adaptation for 3d human pose estimation by learnable motion generation, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp.
    13075–13085.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] M. Gholami, B. Wandt, H. Rhodin, R. Ward, Z. J. Wang, Adaptpose: 通过可学习的运动生成进行跨数据集适配的3D人体姿态估计，见：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第13075–13085页。'
- en: '[86] D. Pavllo, C. Feichtenhofer, D. Grangier, M. Auli, 3d human pose estimation
    in video with temporal convolutions and semi-supervised training, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp.
    7753–7762.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] D. Pavllo, C. Feichtenhofer, D. Grangier, M. Auli, 使用时间卷积和半监督训练进行视频中的3D人体姿态估计，见：IEEE/CVF计算机视觉与模式识别会议论文集，2019年，第7753–7762页。'
- en: '[87] C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Chen, Z. Ding, 3d human pose
    estimation with spatial and temporal transformers, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, 2021, pp. 11656–11665.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Chen, Z. Ding, 使用空间和时间变换器进行3D人体姿态估计，见：IEEE/CVF国际计算机视觉会议论文集，2021年，第11656–11665页。'
- en: '[88] B. Artacho, A. Savakis, Unipose+: A unified framework for 2d and 3d human
    pose estimation in images and videos, IEEE Transactions on Pattern Analysis and
    Machine Intelligence 44 (12) (2021) 9641–9653.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] B. Artacho, A. Savakis, Unipose+: 用于图像和视频中的2D和3D人体姿态估计的统一框架，IEEE模式分析与机器智能汇刊
    44 (12) (2021) 9641–9653。'
- en: '[89] W. Li, H. Liu, H. Tang, P. Wang, L. Van Gool, Mhformer: Multi-hypothesis
    transformer for 3d human pose estimation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2022, pp. 13147–13156.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] W. Li, H. Liu, H. Tang, P. Wang, L. Van Gool, Mhformer: 多假设变换器用于3D人体姿态估计，见：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第13147–13156页。'
- en: '[90] J. Zhang, Z. Tu, J. Yang, Y. Chen, J. Yuan, Mixste: Seq2seq mixed spatio-temporal
    encoder for 3d human pose estimation in video, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2022, pp. 13232–13242.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] J. Zhang, Z. Tu, J. Yang, Y. Chen, J. Yuan, Mixste: Seq2seq混合时空编码器用于视频中的3D人体姿态估计，见：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第13232–13242页。'
- en: '[91] S. Honari, V. Constantin, H. Rhodin, M. Salzmann, P. Fua, Temporal representation
    learning on monocular videos for 3d human pose estimation, IEEE Transactions on
    Pattern Analysis and Machine Intelligence (2022).'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] S. Honari, V. Constantin, H. Rhodin, M. Salzmann, P. Fua, 单目视频中的时间表示学习用于3D人体姿态估计，《IEEE模式分析与机器智能学报》
    (2022)。'
- en: '[92] X. Qian, Y. Tang, N. Zhang, M. Han, J. Xiao, M.-C. Huang, R.-S. Lin, Hstformer:
    Hierarchical spatial-temporal transformers for 3d human pose estimation, arXiv
    preprint arXiv:2301.07322 (2023).'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] X. Qian, Y. Tang, N. Zhang, M. Han, J. Xiao, M.-C. Huang, R.-S. Lin, Hstformer:
    用于3D人体姿态估计的分层时空变换器，arXiv预印本 arXiv:2301.07322 (2023)。'
- en: '[93] Z. Tang, Z. Qiu, Y. Hao, R. Hong, T. Yao, 3d human pose estimation with
    spatio-temporal criss-cross attention, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2023, pp. 4790–4799.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Z. Tang, Z. Qiu, Y. Hao, R. Hong, T. Yao, 具有时空交叉注意力的3D人体姿态估计，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2023年，pp.
    4790–4799。'
- en: '[94] Y. Sun, A. W. Dougherty, Z. Zhang, Y. K. Choi, C. Wu, Mixsynthformer:
    A transformer encoder-like structure with mixed synthetic self-attention for efficient
    human pose estimation, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2023, pp. 14884–14893.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Y. Sun, A. W. Dougherty, Z. Zhang, Y. K. Choi, C. Wu, Mixsynthformer:
    一种具有混合合成自注意力的变换器编码器结构，用于高效的人体姿态估计，发表于：IEEE/CVF国际计算机视觉大会论文集，2023年，pp. 14884–14893。'
- en: '[95] J. Wang, S. Yan, Y. Xiong, D. Lin, Motion guided 3d pose estimation from
    videos, in: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,
    August 23–28, 2020, Proceedings, Part XIII 16, Springer, 2020, pp. 764–780.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] J. Wang, S. Yan, Y. Xiong, D. Lin, 动作引导的3D姿态估计来自视频，发表于：计算机视觉–ECCV 2020:
    第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第十三部分，Springer，2020年，pp. 764–780。'
- en: '[96] J. Zhang, Y. Wang, Z. Zhou, T. Luan, Z. Wang, Y. Qiao, Learning dynamical
    human-joint affinity for 3d pose estimation in videos, IEEE Transactions on Image
    Processing 30 (2021) 7914–7925.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] J. Zhang, Y. Wang, Z. Zhou, T. Luan, Z. Wang, Y. Qiao, 学习动态人体关节亲和力以进行视频中的3D姿态估计，《IEEE图像处理学报》30
    (2021) 7914–7925。'
- en: '[97] T. Chen, C. Fang, X. Shen, Y. Zhu, Z. Chen, J. Luo, Anatomy-aware 3d human
    pose estimation with bone-based pose decomposition, IEEE Transactions on Circuits
    and Systems for Video Technology 32 (1) (2021) 198–209.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] T. Chen, C. Fang, X. Shen, Y. Zhu, Z. Chen, J. Luo, 具有骨基姿态分解的解剖感知3D人体姿态估计，《IEEE电路与系统视频技术学报》32
    (1) (2021) 198–209。'
- en: '[98] Y. Xue, J. Chen, X. Gu, H. Ma, H. Ma, Boosting monocular 3d human pose
    estimation with part aware attention, IEEE Transactions on Image Processing 31
    (2022) 4278–4291.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Y. Xue, J. Chen, X. Gu, H. Ma, H. Ma, 通过部件感知注意力提升单目3D人体姿态估计，《IEEE图像处理学报》31
    (2022) 4278–4291。'
- en: '[99] Y. Cheng, B. Yang, B. Wang, W. Yan, R. T. Tan, Occlusion-aware networks
    for 3d human pose estimation in video, in: Proceedings of the IEEE/CVF international
    conference on computer vision, 2019, pp. 723–732.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Y. Cheng, B. Yang, B. Wang, W. Yan, R. T. Tan, 针对视频中3D人体姿态估计的遮挡感知网络，发表于：IEEE/CVF国际计算机视觉大会论文集，2019年，pp.
    723–732。'
- en: '[100] Z. Yu, B. Ni, J. Xu, J. Wang, C. Zhao, W. Zhang, Towards alleviating
    the modeling ambiguity of unsupervised monocular 3d human pose estimation, in:
    Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021,
    pp. 8651–8660.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Z. Yu, B. Ni, J. Xu, J. Wang, C. Zhao, W. Zhang, 旨在缓解无监督单目3D人体姿态估计的建模歧义，发表于：IEEE/CVF国际计算机视觉大会论文集，2021年，pp.
    8651–8660。'
- en: '[101] X. Chen, K.-Y. Lin, W. Liu, C. Qian, L. Lin, Weakly-supervised discovery
    of geometry-aware representation for 3d human pose estimation, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp.
    10895–10904.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] X. Chen, K.-Y. Lin, W. Liu, C. Qian, L. Lin, 弱监督几何感知表示的发现用于3D人体姿态估计，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2019年，pp.
    10895–10904。'
- en: '[102] R. Mitra, N. B. Gundavarapu, A. Sharma, A. Jain, Multiview-consistent
    semi-supervised learning for 3d human pose estimation, in: Proceedings of the
    ieee/cvf conference on computer vision and pattern recognition, 2020, pp. 6907–6916.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] R. Mitra, N. B. Gundavarapu, A. Sharma, A. Jain, 多视角一致的半监督学习用于3D人体姿态估计，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，pp.
    6907–6916。'
- en: '[103] J. N. Kundu, S. Seth, V. Jampani, M. Rakesh, R. V. Babu, A. Chakraborty,
    Self-supervised 3d human pose estimation via part guided novel image synthesis,
    in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    2020, pp. 6152–6162.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] J. N. Kundu, S. Seth, V. Jampani, M. Rakesh, R. V. Babu, A. Chakraborty,
    自监督3D人体姿态估计通过部件引导的新颖图像合成，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，pp. 6152–6162。'
- en: '[104] W. Shan, Z. Liu, X. Zhang, S. Wang, S. Ma, W. Gao, P-stmo: Pre-trained
    spatial temporal many-to-one model for 3d human pose estimation, in: Computer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part V, Springer, 2022, pp. 461–478.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] W. Shan, Z. Liu, X. Zhang, S. Wang, S. Ma, W. Gao, P-stmo: 预训练的空间时间多对一模型用于3D人体姿态估计,
    见: Computer Vision–ECCV 2022: 第17届欧洲会议, 以色列特拉维夫, 2022年10月23–27日, 论文集, 第V部分, Springer,
    2022, 第461–478页。'
- en: '[105] K. Gong, J. Zhang, J. Feng, Poseaug: A differentiable pose augmentation
    framework for 3d human pose estimation, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2021, pp. 8575–8584.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] K. Gong, J. Zhang, J. Feng, Poseaug: 用于3D人体姿态估计的可微姿态增强框架, 见: IEEE/CVF计算机视觉与模式识别会议论文集,
    2021, 第8575–8584页。'
- en: '[106] J. Zhang, K. Gong, X. Wang, J. Feng, Learning to augment poses for 3d
    human pose estimation in images and videos, IEEE Transactions on Pattern Analysis
    and Machine Intelligence (2023).'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] J. Zhang, K. Gong, X. Wang, J. Feng, 学习增强姿态用于图像和视频中的3D人体姿态估计, IEEE Transactions
    on Pattern Analysis and Machine Intelligence (2023)。'
- en: '[107] L. Chen, H. Ai, R. Chen, Z. Zhuang, S. Liu, Cross-view tracking for multi-human
    3d pose estimation at over 100 fps, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2020, pp. 3279–3288.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] L. Chen, H. Ai, R. Chen, Z. Zhuang, S. Liu, 超过100 fps的多人体3D姿态估计的跨视角跟踪,
    见: IEEE/CVF计算机视觉与模式识别会议论文集, 2020, 第3279–3288页。'
- en: '[108] H.-S. Fang, J. Li, H. Tang, C. Xu, H. Zhu, Y. Xiu, Y.-L. Li, C. Lu, Alphapose:
    Whole-body regional multi-person pose estimation and tracking in real-time, IEEE
    Transactions on Pattern Analysis and Machine Intelligence (2022).'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] H.-S. Fang, J. Li, H. Tang, C. Xu, H. Zhu, Y. Xiu, Y.-L. Li, C. Lu, Alphapose:
    实时全身区域多人的姿态估计与跟踪, IEEE Transactions on Pattern Analysis and Machine Intelligence
    (2022)。'
- en: '[109] S. Wu, S. Jin, W. Liu, L. Bai, C. Qian, D. Liu, W. Ouyang, Graph-based
    3d multi-person pose estimation using multi-view images, in: Proceedings of the
    IEEE/CVF international conference on computer vision, 2021, pp. 11148–11157.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] S. Wu, S. Jin, W. Liu, L. Bai, C. Qian, D. Liu, W. Ouyang, 基于图的3D多人人物姿态估计使用多视角图像,
    见: IEEE/CVF国际计算机视觉会议论文集, 2021, 第11148–11157页。'
- en: '[110] G. Moon, J. Y. Chang, K. M. Lee, Camera distance-aware top-down approach
    for 3d multi-person pose estimation from a single rgb image, in: Proceedings of
    the IEEE/CVF international conference on computer vision, 2019, pp. 10133–10142.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] G. Moon, J. Y. Chang, K. M. Lee, 相机距离感知的自上而下方法用于从单张RGB图像中进行3D多人人体姿态估计,
    见: IEEE/CVF国际计算机视觉会议论文集, 2019, 第10133–10142页。'
- en: '[111] M. Fabbri, F. Lanzi, S. Calderara, S. Alletto, R. Cucchiara, Compressed
    volumetric heatmaps for multi-person 3d pose estimation, in: Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 7204–7213.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] M. Fabbri, F. Lanzi, S. Calderara, S. Alletto, R. Cucchiara, 用于多人3D姿态估计的压缩体积热图,
    见: IEEE/CVF计算机视觉与模式识别会议论文集, 2020, 第7204–7213页。'
- en: '[112] C. Wang, J. Li, W. Liu, C. Qian, C. Lu, Hmor: Hierarchical multi-person
    ordinal relations for monocular multi-person 3d pose estimation, in: Computer
    Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part III 16, Springer, 2020, pp. 242–259.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] C. Wang, J. Li, W. Liu, C. Qian, C. Lu, Hmor: 用于单目多人人体3D姿态估计的分层多人人体序关系,
    见: Computer Vision–ECCV 2020: 第16届欧洲会议, 英国格拉斯哥, 2020年8月23–28日, 论文集, 第III部分16,
    Springer, 2020, 第242–259页。'
- en: '[113] J. Zhen, Q. Fang, J. Sun, W. Liu, W. Jiang, H. Bao, X. Zhou, Smap: Single-shot
    multi-person absolute 3d pose estimation, in: Computer Vision–ECCV 2020: 16th
    European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XV 16,
    Springer, 2020, pp. 550–566.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] J. Zhen, Q. Fang, J. Sun, W. Liu, W. Jiang, H. Bao, X. Zhou, Smap: 单次拍摄多人的绝对3D姿态估计,
    见: Computer Vision–ECCV 2020: 第16届欧洲会议, 英国格拉斯哥, 2020年8月23–28日, 论文集, 第XV部分16, Springer,
    2020, 第550–566页。'
- en: '[114] A. Benzine, B. Luvison, Q. C. Pham, C. Achard, Single-shot 3d multi-person
    pose estimation in complex images, Pattern Recognition 112 (2021) 107534.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] A. Benzine, B. Luvison, Q. C. Pham, C. Achard, 单次拍摄复杂图像中的3D多人人物姿态估计,
    Pattern Recognition 112 (2021) 107534。'
- en: '[115] D. Mehta, O. Sotnychenko, F. Mueller, W. Xu, S. Sridhar, G. Pons-Moll,
    C. Theobalt, Single-shot multi-person 3d pose estimation from monocular rgb, in:
    2018 International Conference on 3D Vision (3DV), IEEE, 2018, pp. 120–130.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] D. Mehta, O. Sotnychenko, F. Mueller, W. Xu, S. Sridhar, G. Pons-Moll,
    C. Theobalt, 单次拍摄多人的3D姿态估计来自单目RGB, 见: 2018年国际3D视觉会议 (3DV), IEEE, 2018, 第120–130页。'
- en: '[116] G. Rogez, P. Weinzaepfel, C. Schmid, Lcr-net++: Multi-person 2d and 3d
    pose detection in natural images, IEEE transactions on pattern analysis and machine
    intelligence 42 (5) (2019) 1146–1161.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] G. Rogez, P. Weinzaepfel, C. Schmid, Lcr-net++：自然图像中的多人物2D和3D姿态检测，《IEEE模式分析与机器智能学报》第42卷（5）（2019年）第1146–1161页。'
- en: '[117] L. Jin, C. Xu, X. Wang, Y. Xiao, Y. Guo, X. Nie, J. Zhao, Single-stage
    is enough: Multi-person absolute 3d pose estimation, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2022, pp. 13086–13095.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] L. Jin, C. Xu, X. Wang, Y. Xiao, Y. Guo, X. Nie, J. Zhao, 单阶段足够：多人绝对3D姿态估计，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第13086–13095页。'
- en: '[118] Y. Cheng, B. Wang, R. T. Tan, Dual networks based 3d multi-person pose
    estimation from monocular video, IEEE Transactions on Pattern Analysis and Machine
    Intelligence 45 (2) (2022) 1636–1651.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Y. Cheng, B. Wang, R. T. Tan, 基于双网络的3D多人物体姿态估计，《IEEE模式分析与机器智能学报》第45卷（2）（2022年）第1636–1651页。'
- en: '[119] Z. Tang, Y. Hao, J. Li, R. Hong, Ftcm: Frequency-temporal collaborative
    module for efficient 3d human pose estimation in video, IEEE Transactions on Circuits
    and Systems for Video Technology (2023).'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Z. Tang, Y. Hao, J. Li, R. Hong, Ftcm：用于高效3D人类姿态估计的频率-时间协作模块，《IEEE视频技术电路与系统学报》（2023年）。'
- en: '[120] B. Artacho, A. Savakis, Unipose: Unified human pose estimation in single
    images and videos, in: Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition, 2020, pp. 7035–7044.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] B. Artacho, A. Savakis, Unipose：单幅图像和视频中的统一人体姿态估计，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，第7035–7044页。'
- en: '[121] A. Zanfir, E. Marinoiu, M. Zanfir, A.-I. Popa, C. Sminchisescu, Deep
    network for the integrated 3d sensing of multiple people in natural images, Advances
    in neural information processing systems 31 (2018).'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] A. Zanfir, E. Marinoiu, M. Zanfir, A.-I. Popa, C. Sminchisescu, 用于自然图像中多人的集成3D传感的深度网络，《神经信息处理系统进展》第31卷（2018年）。'
- en: '[122] A. Newell, K. Yang, J. Deng, Stacked hourglass networks for human pose
    estimation, in: Computer Vision–ECCV 2016: 14th European Conference, Amsterdam,
    The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14, Springer, 2016,
    pp. 483–499.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] A. Newell, K. Yang, J. Deng, 用于人体姿态估计的堆叠沙漏网络，收录于：计算机视觉–ECCV 2016：第14届欧洲会议，阿姆斯特丹，荷兰，2016年10月11-14日，论文集，第八部分，Springer，2016年，第483–499页。'
- en: '[123] Y. Rong, Z. Liu, C. Li, K. Cao, C. C. Loy, Delving deep into hybrid annotations
    for 3d human recovery in the wild, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2019, pp. 5340–5348.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Y. Rong, Z. Liu, C. Li, K. Cao, C. C. Loy, 深入探讨用于野外3D人类恢复的混合注释，收录于：IEEE/CVF国际计算机视觉会议论文集，2019年，第5340–5348页。'
- en: '[124] Z. Li, B. Xu, H. Huang, C. Lu, Y. Guo, Deep two-stream video inference
    for human body pose and shape estimation, in: Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision, 2022, pp. 430–439.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Z. Li, B. Xu, H. Huang, C. Lu, Y. Guo, 用于人体姿态和形状估计的深度双流视频推断，收录于：IEEE/CVF冬季计算机视觉应用会议论文集，2022年，第430–439页。'
- en: '[125] K. Yang, R. Gu, M. Wang, M. Toyoura, G. Xu, Lasor: Learning accurate
    3d human pose and shape via synthetic occlusion-aware data and neural mesh rendering,
    IEEE Transactions on Image Processing 31 (2022) 1938–1948.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] K. Yang, R. Gu, M. Wang, M. Toyoura, G. Xu, Lasor：通过合成遮挡感知数据和神经网格渲染学习精确的3D人类姿态和形状，《IEEE图像处理学报》第31卷（2022年）第1938–1948页。'
- en: '[126] Z. Li, J. Liu, Z. Zhang, S. Xu, Y. Yan, Cliff: Carrying location information
    in full frames into human pose and shape estimation, in: Computer Vision–ECCV
    2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part V, Springer, 2022, pp. 590–606.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Z. Li, J. Liu, Z. Zhang, S. Xu, Y. Yan, Cliff：将完整帧中的位置信息融入人体姿态和形状估计，收录于：计算机视觉–ECCV
    2022：第17届欧洲会议，特拉维夫，以色列，2022年10月23–27日，论文集，第五部分，Springer，2022年，第590–606页。'
- en: '[127] M. Kocabas, C.-H. P. Huang, O. Hilliges, M. J. Black, Pare: Part attention
    regressor for 3d human body estimation, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2021, pp. 11127–11137.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] M. Kocabas, C.-H. P. Huang, O. Hilliges, M. J. Black, Pare：用于3D人体估计的部分注意回归器，收录于：IEEE/CVF国际计算机视觉会议论文集，2021年，第11127–11137页。'
- en: '[128] K. Lin, L. Wang, Z. Liu, Mesh graphormer, in: Proceedings of the IEEE/CVF
    international conference on computer vision, 2021, pp. 12939–12948.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] K. Lin, L. Wang, Z. Liu, Mesh graphormer，收录于：IEEE/CVF国际计算机视觉会议论文集，2021年，第12939–12948页。'
- en: '[129] W.-L. Wei, J.-C. Lin, T.-L. Liu, H.-Y. M. Liao, Capturing humans in motion:
    temporal-attentive 3d human pose and shape estimation from monocular video, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2022, pp. 13211–13220.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] W.-L. Wei, J.-C. Lin, T.-L. Liu, H.-Y. M. Liao, 捕捉动态中的人类: 从单目视频中进行时间注意的
    3D 人体姿态和形状估计, 载于: IEEE/CVF 计算机视觉与模式识别会议论文集, 2022 年, 第 13211–13220 页。'
- en: '[130] Z. Qiu, Q. Yang, J. Wang, H. Feng, J. Han, E. Ding, C. Xu, D. Fu, J. Wang,
    Psvt: End-to-end multi-person 3d pose and shape estimation with progressive video
    transformers, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, 2023, pp. 21254–21263.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Z. Qiu, Q. Yang, J. Wang, H. Feng, J. Han, E. Ding, C. Xu, D. Fu, J.
    Wang, PSVT: 基于渐进视频变换器的端到端多人人体 3D 姿态与形状估计, 载于: IEEE/CVF 计算机视觉与模式识别会议论文集, 2023 年,
    第 21254–21263 页。'
- en: '[131] J. Cho, K. Youwang, T.-H. Oh, Cross-attention of disentangled modalities
    for 3d human mesh recovery with transformers, in: Computer Vision–ECCV 2022: 17th
    European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part
    I, Springer, 2022, pp. 342–359.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] J. Cho, K. Youwang, T.-H. Oh, 基于变换器的解耦模态交叉注意力用于 3D 人体网格恢复, 载于: 计算机视觉–ECCV
    2022: 第 17 届欧洲会议, 特拉维夫, 以色列, 2022 年 10 月 23–27 日, 论文集, 第一部分, 施普林格, 2022 年, 第 342–359
    页。'
- en: '[132] Y. Xue, J. Chen, Y. Zhang, C. Yu, H. Ma, H. Ma, 3d human mesh reconstruction
    by learning to sample joint adaptive tokens for transformers, in: Proceedings
    of the 30th ACM International Conference on Multimedia, 2022, pp. 6765–6773.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Y. Xue, J. Chen, Y. Zhang, C. Yu, H. Ma, H. Ma, 通过学习采样联合自适应标记进行 3D 人体网格重建,
    载于: 第 30 届 ACM 国际多媒体会议论文集, 2022 年, 第 6765–6773 页。'
- en: '[133] K. Lin, L. Wang, Z. Liu, End-to-end human pose and mesh reconstruction
    with transformers, in: Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition, 2021, pp. 1954–1963.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] K. Lin, L. Wang, Z. Liu, 基于变换器的端到端人体姿态和网格重建, 载于: IEEE/CVF 计算机视觉与模式识别会议论文集,
    2021 年, 第 1954–1963 页。'
- en: '[134] A. Kanazawa, J. Y. Zhang, P. Felsen, J. Malik, Learning 3d human dynamics
    from video, in: Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition, 2019, pp. 5614–5623.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] A. Kanazawa, J. Y. Zhang, P. Felsen, J. Malik, 从视频中学习 3D 人体动态, 载于: IEEE/CVF
    计算机视觉与模式识别会议论文集, 2019 年, 第 5614–5623 页。'
- en: '[135] M. Kocabas, N. Athanasiou, M. J. Black, Vibe: Video inference for human
    body pose and shape estimation, in: Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition, 2020, pp. 5253–5263.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] M. Kocabas, N. Athanasiou, M. J. Black, VIBE: 用于人体姿态和形状估计的视频推断, 载于: IEEE/CVF
    计算机视觉与模式识别会议论文集, 2020 年, 第 5253–5263 页。'
- en: '[136] H. Choi, G. Moon, J. Y. Chang, K. M. Lee, Beyond static features for
    temporally consistent 3d human pose and shape from a video, in: Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp.
    1964–1973.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] H. Choi, G. Moon, J. Y. Chang, K. M. Lee, 超越静态特征的时间一致性 3D 人体姿态和形状估计,
    载于: IEEE/CVF 计算机视觉与模式识别会议论文集, 2021 年, 第 1964–1973 页。'
- en: '[137] Z. Wan, Z. Li, M. Tian, J. Liu, S. Yi, H. Li, Encoder-decoder with multi-level
    attention for 3d human shape and pose estimation, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, 2021, pp. 13033–13042.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Z. Wan, Z. Li, M. Tian, J. Liu, S. Yi, H. Li, 具有多层次注意力的编码器-解码器用于 3D 人体形状和姿态估计,
    载于: IEEE/CVF 国际计算机视觉会议论文集, 2021 年, 第 13033–13042 页。'
- en: '[138] Z. Wang, S. Ostadabbas, Live stream temporally embedded 3d human body
    pose and shape estimation, arXiv preprint arXiv:2207.12537 (2022).'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Z. Wang, S. Ostadabbas, 实时流中的时间嵌入 3D 人体姿态和形状估计, arXiv 预印本 arXiv:2207.12537
    (2022)。'
- en: '[139] X. Shen, Z. Yang, X. Wang, J. Ma, C. Zhou, Y. Yang, Global-to-local modeling
    for video-based 3d human pose and shape estimation, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2023, pp. 8887–8896.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] X. Shen, Z. Yang, X. Wang, J. Ma, C. Zhou, Y. Yang, 基于视频的 3D 人体姿态和形状的全局到局部建模,
    载于: IEEE/CVF 计算机视觉与模式识别会议论文集, 2023 年, 第 8887–8896 页。'
- en: '[140] Z. Dong, J. Song, X. Chen, C. Guo, O. Hilliges, Shape-aware multi-person
    pose estimation from multi-view images, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2021, pp. 11158–11168.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Z. Dong, J. Song, X. Chen, C. Guo, O. Hilliges, 形状感知的多视角图像中的多人姿态估计, 载于:
    IEEE/CVF 国际计算机视觉会议论文集, 2021 年, 第 11158–11168 页。'
- en: '[141] A. Sengupta, I. Budvytis, R. Cipolla, Probabilistic 3d human shape and
    pose estimation from multiple unconstrained images in the wild, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp.
    16094–16104.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] A. Sengupta, I. Budvytis, R. Cipolla, 从多个无约束的真实图像中进行概率性三维人体形状和姿态估计，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2021年，第16094–16104页。'
- en: '[142] L. Zhuo, J. Cao, Q. Wang, B. Zhang, L. Bo, Towards stable human pose
    estimation via cross-view fusion and foot stabilization, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 650–659.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] L. Zhuo, J. Cao, Q. Wang, B. Zhang, L. Bo, 通过跨视角融合和脚部稳定实现稳定的人体姿态估计，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2023年，第650–659页。'
- en: '[143] T. Fan, K. V. Alwala, D. Xiang, W. Xu, T. Murphey, M. Mukadam, Revitalizing
    optimization for 3d human pose and shape estimation: A sparse constrained formulation,
    in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021,
    pp. 11457–11466.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] T. Fan, K. V. Alwala, D. Xiang, W. Xu, T. Murphey, M. Mukadam, 复兴三维人体姿态和形状估计的优化：一种稀疏约束的公式，载于：IEEE/CVF国际计算机视觉会议论文集，2021年，第11457–11466页。'
- en: '[144] J. Zhang, D. Yu, J. H. Liew, X. Nie, J. Feng, Body meshes as points,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2021, pp. 546–556.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] J. Zhang, D. Yu, J. H. Liew, X. Nie, J. Feng, 身体网格作为点，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2021年，第546–556页。'
- en: '[145] C. Zheng, M. Mendieta, T. Yang, C. Chen, Heater: An efficient and unified
    network for human reconstruction via heatmap-based transformer, arXiv preprint
    arXiv:2205.15448 (2022).'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] C. Zheng, M. Mendieta, T. Yang, C. Chen, Heater：一种高效统一的人体重建网络，通过基于热图的变换器，arXiv预印本
    arXiv:2205.15448 (2022)。'
- en: '[146] Z. Dou, Q. Wu, C. Lin, Z. Cao, Q. Wu, W. Wan, T. Komura, W. Wang, Tore:
    Token reduction for efficient human mesh recovery with transformer, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 15143–15155.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Z. Dou, Q. Wu, C. Lin, Z. Cao, Q. Wu, W. Wan, T. Komura, W. Wang, Tore：用于高效人体网格恢复的Token减少方法，载于：IEEE/CVF国际计算机视觉会议论文集，2023年，第15143–15155页。'
- en: '[147] G. Pavlakos, N. Kolotouros, K. Daniilidis, Texturepose: Supervising human
    mesh estimation with texture consistency, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2019, pp. 803–812.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] G. Pavlakos, N. Kolotouros, K. Daniilidis, Texturepose：通过纹理一致性监督人体网格估计，载于：IEEE/CVF国际计算机视觉会议论文集，2019年，第803–812页。'
- en: '[148] H. Zhang, J. Cao, G. Lu, W. Ouyang, Z. Sun, Learning 3d human shape and
    pose from dense body parts, IEEE Transactions on Pattern Analysis and Machine
    Intelligence 44 (5) (2020) 2610–2627.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] H. Zhang, J. Cao, G. Lu, W. Ouyang, Z. Sun, 从密集的身体部位学习三维人体形状和姿态，《IEEE模式分析与机器智能汇刊》44(5)
    (2020) 2610–2627。'
- en: '[149] W. Zeng, W. Ouyang, P. Luo, W. Liu, X. Wang, 3d human mesh regression
    with dense correspondence, in: Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, 2020, pp. 7054–7063.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] W. Zeng, W. Ouyang, P. Luo, W. Liu, X. Wang, 带有密集对应的三维人体网格回归，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，第7054–7063页。'
- en: '[150] T. Zhang, B. Huang, Y. Wang, Object-occluded human shape and pose estimation
    from a single color image, in: Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, 2020, pp. 7376–7385.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] T. Zhang, B. Huang, Y. Wang, 从单一彩色图像中估计物体遮挡的人体形状和姿态，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，第7376–7385页。'
- en: '[151] Y. Sun, Q. Bao, W. Liu, Y. Fu, M. J. Black, T. Mei, Monocular, one-stage,
    regression of multiple 3d people, in: Proceedings of the IEEE/CVF international
    conference on computer vision, 2021, pp. 11179–11188.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Y. Sun, Q. Bao, W. Liu, Y. Fu, M. J. Black, T. Mei, 单目单阶段回归多个三维人物，载于：IEEE/CVF国际计算机视觉会议论文集，2021年，第11179–11188页。'
- en: '[152] H. Choi, G. Moon, J. Park, K. M. Lee, Learning to estimate robust 3d
    human mesh from in-the-wild crowded scenes, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2022, pp. 1475–1484.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] H. Choi, G. Moon, J. Park, K. M. Lee, 学习从真实世界拥挤场景中估计鲁棒的三维人体网格，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第1475–1484页。'
- en: '[153] W. Zhu, X. Ma, Z. Liu, L. Liu, W. Wu, Y. Wang, Motionbert: A unified
    perspective on learning human motion representations, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, 2023, pp. 15085–15099.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] W. Zhu, X. Ma, Z. Liu, L. Liu, W. Wu, Y. Wang, Motionbert：学习人体运动表示的统一视角，载于：IEEE/CVF国际计算机视觉会议论文集，2023年，第15085–15099页。'
- en: '[154] R. A. Guler, I. Kokkinos, Holopose: Holistic 3d human reconstruction
    in-the-wild, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, 2019, pp. 10884–10894.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] R. A. Guler, I. Kokkinos: Holopose: 野外环境下的整体3D人体重建，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2019年，第10884–10894页。'
- en: '[155] Y. Sun, Y. Ye, W. Liu, W. Gao, Y. Fu, T. Mei, Human mesh recovery from
    monocular images via a skeleton-disentangled representation, in: Proceedings of
    the IEEE/CVF international conference on computer vision, 2019, pp. 5349–5358.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Y. Sun, Y. Ye, W. Liu, W. Gao, Y. Fu, T. Mei: 通过骨架解耦表示从单目图像中恢复人体网格，发表于：IEEE/CVF国际计算机视觉会议论文集，2019年，第5349–5358页。'
- en: '[156] J. Li, C. Xu, Z. Chen, S. Bian, L. Yang, C. Lu, Hybrik: A hybrid analytical-neural
    inverse kinematics solution for 3d human pose and shape estimation, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp.
    3383–3393.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] J. Li, C. Xu, Z. Chen, S. Bian, L. Yang, C. Lu: Hybrik: 一种用于3D人体姿态和形状估计的混合分析-神经逆运动学解决方案，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2021年，第3383–3393页。'
- en: '[157] J. Li, S. Bian, Q. Liu, J. Tang, F. Wang, C. Lu, Niki: Neural inverse
    kinematics with invertible neural networks for 3d human pose and shape estimation,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2023, pp. 12933–12942.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] J. Li, S. Bian, Q. Liu, J. Tang, F. Wang, C. Lu, Niki: 使用可逆神经网络进行3D人体姿态和形状估计的神经逆运动学，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2023年，第12933–12942页。'
- en: '[158] G.-H. Lee, S.-W. Lee, Uncertainty-aware human mesh recovery from video
    by learning part-based 3d dynamics, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2021, pp. 12375–12384.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] G.-H. Lee, S.-W. Lee: 通过学习基于部件的3D动态进行不确定性感知的人体网格恢复，发表于：IEEE/CVF国际计算机视觉会议论文集，2021年，第12375–12384页。'
- en: '[159] A. Sengupta, I. Budvytis, R. Cipolla, Hierarchical kinematic probability
    distributions for 3d human shape and pose estimation from images in the wild,
    in: Proceedings of the IEEE/CVF international conference on computer vision, 2021,
    pp. 11219–11229.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] A. Sengupta, I. Budvytis, R. Cipolla: 从图像中估计3D人体形状和姿态的分层运动学概率分布，发表于：IEEE/CVF国际计算机视觉会议论文集，2021年，第11219–11229页。'
- en: '[160] D. Wang, S. Zhang, 3d human mesh recovery with sequentially global rotation
    estimation, in: Proceedings of the IEEE/CVF International Conference on Computer
    Vision, 2023, pp. 14953–14962.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] D. Wang, S. Zhang: 具有顺序全局旋转估计的3D人体网格恢复，发表于：IEEE/CVF国际计算机视觉会议论文集，2023年，第14953–14962页。'
- en: '[161] N. Kolotouros, G. Pavlakos, M. J. Black, K. Daniilidis, Learning to reconstruct
    3d human pose and shape via model-fitting in the loop, in: Proceedings of the
    IEEE/CVF international conference on computer vision, 2019, pp. 2252–2261.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] N. Kolotouros, G. Pavlakos, M. J. Black, K. Daniilidis: 通过模型拟合学习重建3D人体姿态和形状，发表于：IEEE/CVF国际计算机视觉会议论文集，2019年，第2252–2261页。'
- en: '[162] Y. Wang, K. Daniilidis, Refit: Recurrent fitting network for 3d human
    recovery, in: Proceedings of the IEEE/CVF International Conference on Computer
    Vision, 2023, pp. 14644–14654.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Y. Wang, K. Daniilidis: Refit: 用于3D人体恢复的递归拟合网络，发表于：IEEE/CVF国际计算机视觉会议论文集，2023年，第14644–14654页。'
- en: '[163] W. Jiang, N. Kolotouros, G. Pavlakos, X. Zhou, K. Daniilidis, Coherent
    reconstruction of multiple humans from a single image, in: Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 5579–5588.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] W. Jiang, N. Kolotouros, G. Pavlakos, X. Zhou, K. Daniilidis: 从单张图像中一致性重建多个人体，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，第5579–5588页。'
- en: '[164] M. Madadi, H. Bertiche, S. Escalera, Deep unsupervised 3d human body
    reconstruction from a sparse set of landmarks, International Journal of Computer
    Vision 129 (8) (2021) 2499–2512.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] M. Madadi, H. Bertiche, S. Escalera: 从稀疏标记点进行深度无监督3D人体重建，国际计算机视觉杂志129（8）（2021）2499–2512。'
- en: '[165] S. Guan, J. Xu, M. Z. He, Y. Wang, B. Ni, X. Yang, Out-of-domain human
    mesh reconstruction via dynamic bilevel online adaptation, IEEE Transactions on
    Pattern Analysis and Machine Intelligence 45 (4) (2022) 5070–5086.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] S. Guan, J. Xu, M. Z. He, Y. Wang, B. Ni, X. Yang: 通过动态双层在线适应进行领域外人体网格重建，IEEE模式分析与机器智能学报45（4）（2022）5070–5086。'
- en: '[166] B. Huang, T. Zhang, Y. Wang, Pose2uv: Single-shot multiperson mesh recovery
    with deep uv prior, IEEE Transactions on Image Processing 31 (2022) 4679–4692.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] B. Huang, T. Zhang, Y. Wang: Pose2uv: 采用深度UV先验的单次多人体网格恢复，IEEE图像处理学报31（2022）4679–4692。'
- en: '[167] J. Li, Z. Yang, X. Wang, J. Ma, C. Zhou, Y. Yang, Jotr: 3d joint contrastive
    learning with transformers for occluded human mesh recovery, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision, 2023, pp. 9110–9121.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] J. Li, Z. Yang, X. Wang, J. Ma, C. Zhou, Y. Yang, Jotr：用于遮挡人体网格恢复的3D联合对比学习，见：IEEE/CVF计算机视觉国际会议论文集，2023，页码9110–9121。'
- en: '[168] H. Nam, D. S. Jung, Y. Oh, K. M. Lee, Cyclic test-time adaptation on
    monocular video for 3d human mesh reconstruction, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, 2023, pp. 14829–14839.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] H. Nam, D. S. Jung, Y. Oh, K. M. Lee, 基于单目视频的3D人体网格重建的循环测试时间自适应，见：IEEE/CVF计算机视觉国际会议论文集，2023，页码14829–14839。'
- en: '[169] T. Alldieck, M. Magnor, B. L. Bhatnagar, C. Theobalt, G. Pons-Moll, Learning
    to reconstruct people in clothing from a single rgb camera, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp.
    1175–1186.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] T. Alldieck, M. Magnor, B. L. Bhatnagar, C. Theobalt, G. Pons-Moll, 学习从单RGB相机重建穿着衣物的人，见：IEEE/CVF计算机视觉与模式识别会议论文集，2019，页码1175–1186。'
- en: '[170] B. L. Bhatnagar, G. Tiwari, C. Theobalt, G. Pons-Moll, Multi-garment
    net: Learning to dress 3d people from images, in: Proceedings of the IEEE/CVF
    international conference on computer vision, 2019, pp. 5420–5430.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] B. L. Bhatnagar, G. Tiwari, C. Theobalt, G. Pons-Moll, Multi-garment
    net：从图像中学习给3D人物穿衣，见：IEEE/CVF计算机视觉国际会议论文集，2019，页码5420–5430。'
- en: '[171] T. Alldieck, G. Pons-Moll, C. Theobalt, M. Magnor, Tex2shape: Detailed
    full human body geometry from a single image, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, 2019, pp. 2293–2303.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] T. Alldieck, G. Pons-Moll, C. Theobalt, M. Magnor, Tex2shape：从单幅图像中获取详细的全身几何形状，见：IEEE/CVF计算机视觉国际会议论文集，2019，页码2293–2303。'
- en: '[172] B. Jiang, J. Zhang, Y. Hong, J. Luo, L. Liu, H. Bao, Bcnet: Learning
    body and cloth shape from a single image, in: Computer Vision–ECCV 2020: 16th
    European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XX 16,
    Springer, 2020, pp. 18–35.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] B. Jiang, J. Zhang, Y. Hong, J. Luo, L. Liu, H. Bao, Bcnet：从单幅图像中学习身体和衣物形状，见：计算机视觉–ECCV
    2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第XX部分16，施普林格，2020，页码18–35。'
- en: '[173] M.-P. Forte, P. Kulits, C.-H. P. Huang, V. Choutas, D. Tzionas, K. J.
    Kuchenbecker, M. J. Black, Reconstructing signing avatars from video using linguistic
    priors, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, 2023, pp. 12791–12801.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] M.-P. Forte, P. Kulits, C.-H. P. Huang, V. Choutas, D. Tzionas, K. J.
    Kuchenbecker, M. J. Black, 使用语言先验从视频中重建手语头像，见：IEEE/CVF计算机视觉与模式识别会议论文集，2023，页码12791–12801。'
- en: '[174] B. Zhang, Y. Wang, X. Deng, Y. Zhang, P. Tan, C. Ma, H. Wang, Interacting
    two-hand 3d pose and shape reconstruction from single color image, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 11354–11363.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] B. Zhang, Y. Wang, X. Deng, Y. Zhang, P. Tan, C. Ma, H. Wang, 从单幅彩色图像中交互式地重建双手3D姿态和形状，见：IEEE/CVF计算机视觉国际会议论文集，2021，页码11354–11363。'
- en: '[175] Y. Chen, Z. Tu, D. Kang, R. Chen, L. Bao, Z. Zhang, J. Yuan, Joint hand-object
    3d reconstruction from a single image with cross-branch feature fusion, IEEE Transactions
    on Image Processing 30 (2021) 4008–4021.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Y. Chen, Z. Tu, D. Kang, R. Chen, L. Bao, Z. Zhang, J. Yuan, 从单幅图像中通过跨分支特征融合进行联合手-物体3D重建，IEEE图像处理汇刊30
    (2021) 4008–4021。'
- en: '[176] M. Hassan, V. Choutas, D. Tzionas, M. J. Black, Resolving 3d human pose
    ambiguities with 3d scene constraints, in: Proceedings of the IEEE/CVF international
    conference on computer vision, 2019, pp. 2282–2292.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] M. Hassan, V. Choutas, D. Tzionas, M. J. Black, 通过3D场景约束解决3D人体姿态歧义，见：IEEE/CVF计算机视觉国际会议论文集，2019，页码2282–2292。'
- en: '[177] V. Choutas, G. Pavlakos, T. Bolkart, D. Tzionas, M. J. Black, Monocular
    expressive body regression through body-driven attention, in: Computer Vision–ECCV
    2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part X 16, Springer, 2020, pp. 20–40.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] V. Choutas, G. Pavlakos, T. Bolkart, D. Tzionas, M. J. Black, 通过身体驱动的注意力进行单目表情体重建，见：计算机视觉–ECCV
    2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第X部分16，施普林格，2020，页码20–40。'
- en: '[178] Y. Rong, T. Shiratori, H. Joo, Frankmocap: A monocular 3d whole-body
    pose estimation system via regression and integration, in: Proceedings of the
    IEEE/CVF International Conference on Computer Vision, 2021, pp. 1749–1759.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Y. Rong, T. Shiratori, H. Joo, Frankmocap：通过回归和集成的单目3D全身姿态估计系统，见：IEEE/CVF计算机视觉国际会议论文集，2021，页码1749–1759。'
- en: '[179] Y. Feng, V. Choutas, T. Bolkart, D. Tzionas, M. J. Black, Collaborative
    regression of expressive bodies using moderation, in: 2021 International Conference
    on 3D Vision (3DV), IEEE, 2021, pp. 792–804.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Y. Feng, V. Choutas, T. Bolkart, D. Tzionas, M. J. Black, 使用调节的表达体的协作回归，发表于：2021年国际3D视觉会议（3DV），IEEE，2021，第792–804页。'
- en: '[180] G. Moon, H. Choi, K. M. Lee, Accurate 3d hand pose estimation for whole-body
    3d human mesh estimation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2022, pp. 2308–2317.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] G. Moon, H. Choi, K. M. Lee, 精确的3D手部姿势估计用于全身3D人类网格估计，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2022，第2308–2317页。'
- en: '[181] H. Zhang, Y. Tian, Y. Zhang, M. Li, L. An, Z. Sun, Y. Liu, Pymaf-x: Towards
    well-aligned full-body model regression from monocular images, IEEE Transactions
    on Pattern Analysis and Machine Intelligence (2023).'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] H. Zhang, Y. Tian, Y. Zhang, M. Li, L. An, Z. Sun, Y. Liu, Pymaf-x: 从单目图像中进行良对齐全身模型回归，IEEE模式分析与机器智能汇刊（2023）。'
- en: '[182] J. Lin, A. Zeng, H. Wang, L. Zhang, Y. Li, One-stage 3d whole-body mesh
    recovery with component aware transformer, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2023, pp. 21159–21168.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] J. Lin, A. Zeng, H. Wang, L. Zhang, Y. Li, 一阶段3D全身网格恢复与组件感知变换器，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2023，第21159–21168页。'
- en: '[183] J. Li, S. Bian, C. Xu, Z. Chen, L. Yang, C. Lu, Hybrik-x: Hybrid analytical-neural
    inverse kinematics for whole-body mesh recovery, arXiv preprint arXiv:2304.05690
    (2023).'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] J. Li, S. Bian, C. Xu, Z. Chen, L. Yang, C. Lu, Hybrik-x: 用于全身网格恢复的混合分析-神经逆向运动学，arXiv预印本
    arXiv:2304.05690（2023）。'
- en: '[184] D. Smith, M. Loper, X. Hu, P. Mavroidis, J. Romero, Facsimile: Fast and
    accurate scans from an image in less than a second, in: Proceedings of the IEEE/CVF
    international conference on computer vision, 2019, pp. 5330–5339.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] D. Smith, M. Loper, X. Hu, P. Mavroidis, J. Romero, Facsimile: 从图像中快速且准确地扫描，少于一秒钟，发表于：IEEE/CVF国际计算机视觉会议论文集，2019，第5330–5339页。'
- en: '[185] S. S. Jinka, R. Chacko, A. Sharma, P. Narayanan, Peeledhuman: Robust
    shape representation for textured 3d human body reconstruction, in: 2020 International
    Conference on 3D Vision (3DV), IEEE, 2020, pp. 879–888.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] S. S. Jinka, R. Chacko, A. Sharma, P. Narayanan, Peeledhuman: 纹理3D人体重建的鲁棒形状表示，发表于：2020年国际3D视觉会议（3DV），IEEE，2020，第879–888页。'
- en: '[186] Z. Zhang, L. Sun, Z. Yang, L. Chen, Y. Yang, Global-correlated 3d-decoupling
    transformer for clothed avatar reconstruction, arXiv preprint arXiv:2309.13524
    (2023).'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Z. Zhang, L. Sun, Z. Yang, L. Chen, Y. Yang, 全球相关3D解耦变换器用于穿衣虚拟形象重建，arXiv预印本
    arXiv:2309.13524（2023）。'
- en: '[187] Y. Xue, B. L. Bhatnagar, R. Marin, N. Sarafianos, Y. Xu, G. Pons-Moll,
    T. Tung, Nsf: Neural surface fields for human modeling from monocular depth, in:
    Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023,
    pp. 15049–15060.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Y. Xue, B. L. Bhatnagar, R. Marin, N. Sarafianos, Y. Xu, G. Pons-Moll,
    T. Tung, Nsf: 从单目深度中进行人类建模的神经表面场，发表于：IEEE/CVF国际计算机视觉会议论文集，2023，第15049–15060页。'
- en: '[188] E. Gärtner, M. Andriluka, E. Coumans, C. Sminchisescu, Differentiable
    dynamics for articulated 3d human motion reconstruction, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 13190–13200.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] E. Gärtner, M. Andriluka, E. Coumans, C. Sminchisescu, 可微分动态用于关节3D人体运动重建，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2022，第13190–13200页。'
- en: '[189] Z. Dong, X. Chen, J. Yang, M. J. Black, O. Hilliges, A. Geiger, Ag3d:
    Learning to generate 3d avatars from 2d image collections, arXiv preprint arXiv:2305.02312
    (2023).'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Z. Dong, X. Chen, J. Yang, M. J. Black, O. Hilliges, A. Geiger, Ag3d:
    从2D图像集合中学习生成3D虚拟形象，arXiv预印本 arXiv:2305.02312（2023）。'
- en: '[190] S. Saito, Z. Huang, R. Natsume, S. Morishima, A. Kanazawa, H. Li, Pifu:
    Pixel-aligned implicit function for high-resolution clothed human digitization,
    in: Proceedings of the IEEE/CVF international conference on computer vision, 2019,
    pp. 2304–2314.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] S. Saito, Z. Huang, R. Natsume, S. Morishima, A. Kanazawa, H. Li, Pifu:
    高分辨率穿衣人像数字化的像素对齐隐式函数，发表于：IEEE/CVF国际计算机视觉会议论文集，2019，第2304–2314页。'
- en: '[191] S. Saito, T. Simon, J. Saragih, H. Joo, Pifuhd: Multi-level pixel-aligned
    implicit function for high-resolution 3d human digitization, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp.
    84–93.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] S. Saito, T. Simon, J. Saragih, H. Joo, Pifuhd: 高分辨率3D人体数字化的多层次像素对齐隐式函数，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2020，第84–93页。'
- en: '[192] Z. Huang, Y. Xu, C. Lassner, H. Li, T. Tung, Arch: Animatable reconstruction
    of clothed humans, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2020, pp. 3093–3102.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Z. Huang, Y. Xu, C. Lassner, H. Li, T. Tung, Arch：可动画化的穿衣人重建，载于：IEEE/CVF计算机视觉与模式识别大会论文集，2020，页3093–3102。'
- en: '[193] T. He, Y. Xu, S. Saito, S. Soatto, T. Tung, Arch++: Animation-ready clothed
    human reconstruction revisited, in: Proceedings of the IEEE/CVF international
    conference on computer vision, 2021, pp. 11046–11056.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] T. He, Y. Xu, S. Saito, S. Soatto, T. Tung, Arch++：动画准备的穿衣人重建回顾，载于：IEEE/CVF国际计算机视觉大会论文集，2021，页11046–11056。'
- en: '[194] T. Liao, X. Zhang, Y. Xiu, H. Yi, X. Liu, G.-J. Qi, Y. Zhang, X. Wang,
    X. Zhu, Z. Lei, High-fidelity clothed avatar reconstruction from a single image,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2023, pp. 8662–8672.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] T. Liao, X. Zhang, Y. Xiu, H. Yi, X. Liu, G.-J. Qi, Y. Zhang, X. Wang,
    X. Zhu, Z. Lei, 从单张图像中高保真地重建穿衣虚拟人，载于：IEEE/CVF计算机视觉与模式识别大会论文集，2023，页8662–8672。'
- en: '[195] T. He, J. Collomosse, H. Jin, S. Soatto, Geo-pifu: Geometry and pixel
    aligned implicit functions for single-view human reconstruction, Advances in Neural
    Information Processing Systems 33 (2020) 9276–9287.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] T. He, J. Collomosse, H. Jin, S. Soatto, Geo-pifu：几何和像素对齐的隐式函数用于单视图人类重建，《神经信息处理系统进展》第33卷（2020）9276–9287。'
- en: '[196] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, X. Zhou, Neural
    body: Implicit neural representations with structured latent codes for novel view
    synthesis of dynamic humans, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2021, pp. 9054–9063.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, X. Zhou, Neural
    body：具有结构化潜在编码的隐式神经表示用于动态人类的新视角合成，载于：IEEE/CVF计算机视觉与模式识别大会论文集，2021，页9054–9063。'
- en: '[197] Y. Zhang, P. Ji, A. Wang, J. Mei, A. Kortylewski, A. Yuille, 3d-aware
    neural body fitting for occlusion robust 3d human pose estimation, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 9399–9410.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Y. Zhang, P. Ji, A. Wang, J. Mei, A. Kortylewski, A. Yuille, 3d-aware神经体适应于遮挡鲁棒的3d人类姿态估计，载于：IEEE/CVF国际计算机视觉大会论文集，2023，页9399–9410。'
- en: '[198] X. Gao, J. Yang, J. Kim, S. Peng, Z. Liu, X. Tong, Mps-nerf: Generalizable
    3d human rendering from multiview images, IEEE Transactions on Pattern Analysis
    and Machine Intelligence (2022).'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] X. Gao, J. Yang, J. Kim, S. Peng, Z. Liu, X. Tong, Mps-nerf：从多视图图像中通用的3d人类渲染，《IEEE模式分析与机器智能汇刊》（2022）。'
- en: '[199] L. G. Foo, J. Gong, H. Rahmani, J. Liu, Distribution-aligned diffusion
    for human mesh recovery, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2023, pp. 9221–9232.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] L. G. Foo, J. Gong, H. Rahmani, J. Liu, 分布对齐扩散用于人类网格恢复，载于：IEEE/CVF国际计算机视觉大会论文集，2023，页9221–9232。'
- en: '[200] H. Zhu, X. Zuo, S. Wang, X. Cao, R. Yang, Detailed human shape estimation
    from a single image by hierarchical mesh deformation, in: Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, 2019, pp. 4491–4500.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] H. Zhu, X. Zuo, S. Wang, X. Cao, R. Yang, 通过层次网格变形从单张图像中详细估计人类形状，载于：IEEE/CVF计算机视觉与模式识别大会论文集，2019，页4491–4500。'
- en: '[201] B. L. Bhatnagar, C. Sminchisescu, C. Theobalt, G. Pons-Moll, Combining
    implicit function learning and parametric models for 3d human reconstruction,
    in: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part II 16, Springer, 2020, pp. 311–329.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] B. L. Bhatnagar, C. Sminchisescu, C. Theobalt, G. Pons-Moll, 结合隐式函数学习和参数模型进行3d人类重建，载于：计算机视觉–ECCV
    2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第II卷16，Springer，2020，页311–329。'
- en: '[202] H. Zhu, X. Zuo, H. Yang, S. Wang, X. Cao, R. Yang, Detailed avatar recovery
    from single image, IEEE Transactions on Pattern Analysis and Machine Intelligence
    44 (11) (2021) 7363–7379.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] H. Zhu, X. Zuo, H. Yang, S. Wang, X. Cao, R. Yang, 从单张图像中详细恢复虚拟人，《IEEE模式分析与机器智能汇刊》44卷（11）（2021）7363–7379。'
- en: '[203] Y. Xiu, J. Yang, D. Tzionas, M. J. Black, Icon: Implicit clothed humans
    obtained from normals, in: 2022 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), IEEE, 2022, pp. 13286–13296.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Y. Xiu, J. Yang, D. Tzionas, M. J. Black, Icon：从法线中获得的隐式穿衣人，载于：2022 IEEE/CVF计算机视觉与模式识别大会（CVPR），IEEE，2022，页13286–13296。'
- en: '[204] Y. Xiu, J. Yang, X. Cao, D. Tzionas, M. J. Black, Econ: Explicit clothed
    humans optimized via normal integration, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2023, pp. 512–523.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Y. Xiu, J. Yang, X. Cao, D. Tzionas, M. J. Black, Econ：通过法线积分优化的显式穿衣人，载于：IEEE/CVF计算机视觉与模式识别大会论文集，2023，页512–523。'
- en: '[205] X. Zhang, J. Zhang, R. Chacko, H. Xu, G. Song, Y. Yang, J. Feng, Getavatar:
    Generative textured meshes for animatable human avatars, in: Proceedings of the
    IEEE/CVF International Conference on Computer Vision, 2023, pp. 2273–2282.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] X. Zhang, J. Zhang, R. Chacko, H. Xu, G. Song, Y. Yang, J. Feng, Getavatar:
    用于可动画人类化身的生成纹理网格，收录于：IEEE/CVF 国际计算机视觉会议论文集，2023年，页码 2273–2282。'
- en: '[206] D. Svitov, D. Gudkov, R. Bashirov, V. Lempitsky, Dinar: Diffusion inpainting
    of neural textures for one-shot human avatars, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, 2023, pp. 7062–7072.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] D. Svitov, D. Gudkov, R. Bashirov, V. Lempitsky, Dinar: 用于一次性人类化身的神经纹理扩散修复，收录于：IEEE/CVF
    国际计算机视觉会议论文集，2023年，页码 7062–7072。'
- en: '[207] X. Pan, Z. Yang, J. Ma, C. Zhou, Y. Yang, Transhuman: A transformer-based
    human representation for generalizable neural human rendering, in: Proceedings
    of the IEEE/CVF International conference on computer vision, 2023, pp. 3544–3555.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] X. Pan, Z. Yang, J. Ma, C. Zhou, Y. Yang, Transhuman: 基于变换器的人类表示用于通用神经人类渲染，收录于：IEEE/CVF
    国际计算机视觉会议论文集，2023年，页码 3544–3555。'
- en: '[208] Y. Liu, X. Huang, M. Qin, Q. Lin, H. Wang, Animatable 3d gaussian: Fast
    and high-quality reconstruction of multiple human avatars, arXiv preprint arXiv:2311.16482
    (2023).'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Y. Liu, X. Huang, M. Qin, Q. Lin, H. Wang, Animatable 3d gaussian: 多人人物化身的快速高质量重建，arXiv
    预印本 arXiv:2311.16482 (2023)。'
- en: '[209] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural information
    processing systems 30 (2017).'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, I. Polosukhin, 注意力即一切，你所需要的，神经信息处理系统进展 30 (2017)。'
- en: '[210] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep
    bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805
    (2018).'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: 深度双向变换器的预训练用于语言理解，arXiv
    预印本 arXiv:1810.04805 (2018)。'
- en: '[211] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, et al., Language models are few-shot learners,
    Advances in neural information processing systems 33 (2020) 1877–1901.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, 等，语言模型是少样本学习者，神经信息处理系统进展 33 (2020) 1877–1901。'
- en: '[212] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin
    transformer: Hierarchical vision transformer using shifted windows, in: Proceedings
    of the IEEE/CVF international conference on computer vision, 2021, pp. 10012–10022.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin
    transformer: 使用偏移窗口的分层视觉变换器，收录于：IEEE/CVF 国际计算机视觉会议论文集，2021年，页码 10012–10022。'
- en: '[213] Y. Xu, S.-C. Zhu, T. Tung, Denserac: Joint 3d pose and shape estimation
    by dense render-and-compare, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2019, pp. 7760–7770.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] Y. Xu, S.-C. Zhu, T. Tung, Denserac: 通过密集渲染和比较进行联合3D姿态和形状估计，收录于：IEEE/CVF
    国际计算机视觉会议论文集，2019年，页码 7760–7770。'
- en: '[214] S. Guan, J. Xu, Y. Wang, B. Ni, X. Yang, Bilevel online adaptation for
    out-of-domain human mesh reconstruction, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2021, pp. 10472–10481.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] S. Guan, J. Xu, Y. Wang, B. Ni, X. Yang, 针对域外人类网格重建的双层次在线适应，收录于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2021年，页码 10472–10481。'
- en: '[215] H. Zhang, Y. Tian, X. Zhou, W. Ouyang, Y. Liu, L. Wang, Z. Sun, Pymaf:
    3d human pose and shape regression with pyramidal mesh alignment feedback loop,
    in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021,
    pp. 11446–11456.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] H. Zhang, Y. Tian, X. Zhou, W. Ouyang, Y. Liu, L. Wang, Z. Sun, Pymaf:
    带有金字塔网格对齐反馈循环的3D人类姿态和形状回归，收录于：IEEE/CVF 国际计算机视觉会议论文集，2021年，页码 11446–11456。'
- en: '[216] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    R. Ng, Nerf: Representing scenes as neural radiance fields for view synthesis,
    Communications of the ACM 65 (1) (2021) 99–106.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    R. Ng, Nerf: 将场景表示为神经辐射场以进行视图合成，ACM 通讯 65 (1) (2021) 99–106。'
- en: '[217] B. Kerbl, G. Kopanas, T. Leimkühler, G. Drettakis, 3d gaussian splatting
    for real-time radiance field rendering, ACM Transactions on Graphics 42 (4) (2023).'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] B. Kerbl, G. Kopanas, T. Leimkühler, G. Drettakis, 实时辐射场渲染的3D高斯溅射，ACM
    图形学事务 42 (4) (2023)。'
- en: '[218] C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, X. Li, Gs-slam: Dense
    visual slam with 3d gaussian splatting, arXiv preprint arXiv:2311.11700 (2023).'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, X. Li, Gs-slam: 使用3D高斯溅射的密集视觉SLAM，arXiv
    预印本 arXiv:2311.11700 (2023)。'
- en: '[219] X. Liu, X. Zhan, J. Tang, Y. Shan, G. Zeng, D. Lin, X. Liu, Z. Liu, Humangaussian:
    Text-driven 3d human generation with gaussian splatting, arXiv preprint arXiv:2311.17061
    (2023).'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] X. Liu, X. Zhan, J. Tang, Y. Shan, G. Zeng, D. Lin, X. Liu, Z. Liu, Humangaussian:
    基于文本驱动的3D人类生成与高斯溅射，arXiv预印本 arXiv:2311.17061 (2023)。'
- en: '[220] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, X. Wang,
    4d gaussian splatting for real-time dynamic scene rendering, arXiv preprint arXiv:2310.08528
    (2023).'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, X.
    Wang, 4d高斯溅射用于实时动态场景渲染，arXiv预印本 arXiv:2310.08528 (2023)。'
- en: '[221] Z. Chen, F. Wang, H. Liu, Text-to-3d using gaussian splatting, arXiv
    preprint arXiv:2309.16585 (2023).'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] Z. Chen, F. Wang, H. Liu, 基于高斯溅射的文本到3D，arXiv预印本 arXiv:2309.16585 (2023)。'
- en: '[222] C. Ionescu, D. Papava, V. Olaru, C. Sminchisescu, Human3.6M: Large Scale
    Datasets and Predictive Methods for 3D Human Sensing in Natural Environments,
    IEEE Transactions on Pattern Analysis and Machine Intelligence 36 (7) (2014) 1325–1339.
    [doi:10.1109/TPAMI.2013.248](https://doi.org/10.1109/TPAMI.2013.248).'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] C. Ionescu, D. Papava, V. Olaru, C. Sminchisescu, Human3.6M: 自然环境中大规模数据集和预测方法用于3D人类感知，IEEE模式分析与机器智能学报
    36 (7) (2014) 1325–1339。 [doi:10.1109/TPAMI.2013.248](https://doi.org/10.1109/TPAMI.2013.248)。'
- en: '[223] Y. Yang, D. Ramanan, Articulated human detection with flexible mixtures
    of parts, IEEE Transactions on Pattern Analysis and Machine Intelligence 35 (12)
    (2013) 2878–2890. [doi:10.1109/TPAMI.2012.261](https://doi.org/10.1109/TPAMI.2012.261).'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] Y. Yang, D. Ramanan, 具有灵活部件混合的关节人类检测，IEEE模式分析与机器智能学报 35 (12) (2013) 2878–2890。
    [doi:10.1109/TPAMI.2012.261](https://doi.org/10.1109/TPAMI.2012.261)。'
- en: '[224] T. Von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, G. Pons-Moll,
    Recovering accurate 3d human pose in the wild using imus and a moving camera,
    in: Proceedings of the European conference on computer vision (ECCV), 2018, pp.
    601–617.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] T. Von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, G. Pons-Moll,
    使用IMUs和移动摄像机恢复野外准确的3D人类姿态，见于：欧洲计算机视觉会议（ECCV）论文集，2018年，页码 601–617。'
- en: '[225] D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko, W. Xu, C. Theobalt,
    Monocular 3d human pose estimation in the wild using improved cnn supervision,
    in: 2017 international conference on 3D vision (3DV), IEEE, 2017, pp. 506–516.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko, W. Xu, C. Theobalt,
    在野外使用改进的CNN监督进行单目3D人类姿态估计，见于：2017年国际3D视觉会议（3DV），IEEE，2017年，页码 506–516。'
- en: '[226] L. Sigal, A. O. Balan, M. J. Black, Humaneva: Synchronized video and
    motion capture dataset and baseline algorithm for evaluation of articulated human
    motion, International journal of computer vision 87 (1-2) (2010) 4.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] L. Sigal, A. O. Balan, M. J. Black, Humaneva: 同步视频和运动捕捉数据集及基线算法，用于评估关节人类运动，计算机视觉国际期刊
    87 (1-2) (2010) 4。'
- en: '[227] H. Joo, H. Liu, L. Tan, L. Gui, B. Nabbe, I. Matthews, T. Kanade, S. Nobuhara,
    Y. Sheikh, Panoptic studio: A massively multiview system for social motion capture,
    in: Proceedings of the IEEE International Conference on Computer Vision (ICCV),
    2015.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] H. Joo, H. Liu, L. Tan, L. Gui, B. Nabbe, I. Matthews, T. Kanade, S.
    Nobuhara, Y. Sheikh, Panoptic studio: 一种大规模多视角社交运动捕捉系统，见于：IEEE国际计算机视觉会议（ICCV）论文集，2015年。'
- en: '[228] G. Varol, J. Romero, X. Martin, N. Mahmood, M. J. Black, I. Laptev, C. Schmid,
    Learning from synthetic humans, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2017, pp. 109–117.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] G. Varol, J. Romero, X. Martin, N. Mahmood, M. J. Black, I. Laptev, C.
    Schmid, 从合成的人类中学习，见于：IEEE计算机视觉与模式识别会议论文集，2017年，页码 109–117。'
- en: '[229] L. Muller, A. A. Osman, S. Tang, C.-H. P. Huang, M. J. Black, On self-contact
    and human pose, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2021, pp. 9990–9999.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] L. Muller, A. A. Osman, S. Tang, C.-H. P. Huang, M. J. Black, 关于自我接触和人类姿态，见于：IEEE/CVF计算机视觉与模式识别会议论文集，2021年，页码
    9990–9999。'
- en: '[230] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, M. J. Black, Amass:
    Archive of motion capture as surface shapes, in: Proceedings of the IEEE/CVF international
    conference on computer vision, 2019, pp. 5442–5451.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, M. J. Black, Amass:
    运动捕捉数据存档作为表面形状，见于：IEEE/CVF国际计算机视觉会议论文集，2019年，页码 5442–5451。'
- en: '[231] R. A. Güler, N. Neverova, I. Kokkinos, Densepose: Dense human pose estimation
    in the wild, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, 2018, pp. 7297–7306.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] R. A. Güler, N. Neverova, I. Kokkinos, Densepose: 野外密集人类姿态估计，见于：IEEE计算机视觉与模式识别会议论文集，2018年，页码
    7297–7306。'
- en: '[232] C. Lassner, J. Romero, M. Kiefel, F. Bogo, M. J. Black, P. V. Gehler,
    Unite the people: Closing the loop between 3d and 2d human representations, in:
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    2017, pp. 6050–6059.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] C. Lassner, J. Romero, M. Kiefel, F. Bogo, M. J. Black, P. V. Gehler,
    联合人群：在 3D 和 2D 人体表征之间闭环，见：2017 年 IEEE 计算机视觉与模式识别大会论文集，第 6050–6059 页。'
- en: '[233] Z. Zheng, T. Yu, Y. Wei, Q. Dai, Y. Liu, Deephuman: 3d human reconstruction
    from a single image, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2019, pp. 7739–7749.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] Z. Zheng, T. Yu, Y. Wei, Q. Dai, Y. Liu, Deephuman: 从单张图像进行 3D 人体重建，见：2019
    年 IEEE/CVF 国际计算机视觉大会论文集，第 7739–7749 页。'
- en: '[234] W. Zhao, W. Wang, Y. Tian, Graformer: Graph-oriented transformer for
    3d pose estimation, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2022, pp. 20438–20447.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] W. Zhao, W. Wang, Y. Tian, Graformer: 用于 3D 姿态估计的图形导向变换器，见：2022 年 IEEE/CVF
    计算机视觉与模式识别大会论文集，第 20438–20447 页。'
- en: '[235] B. X. Yu, Z. Zhang, Y. Liu, S.-h. Zhong, Y. Liu, C. W. Chen, Gla-gcn:
    Global-local adaptive graph convolutional network for 3d human pose estimation
    from monocular video, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2023, pp. 8818–8829.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] B. X. Yu, Z. Zhang, Y. Liu, S.-h. Zhong, Y. Liu, C. W. Chen, Gla-gcn:
    一种用于单目视频中 3D 人体姿态估计的全局-局部自适应图卷积网络，见：2023 年 IEEE/CVF 国际计算机视觉大会论文集，第 8818–8829 页。'
- en: '[236] H. Ci, M. Wu, W. Zhu, X. Ma, H. Dong, F. Zhong, Y. Wang, Gfpose: Learning
    3d human pose prior with gradient fields, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2023, pp. 4800–4810.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] H. Ci, M. Wu, W. Zhu, X. Ma, H. Dong, F. Zhong, Y. Wang, Gfpose: 利用梯度场学习
    3D 人体姿态先验，见：2023 年 IEEE/CVF 计算机视觉与模式识别大会论文集，第 4800–4810 页。'
- en: '[237] K. Lee, W. Kim, S. Lee, From human pose similarity metric to 3d human
    pose estimator: Temporal propagating lstm networks, IEEE transactions on pattern
    analysis and machine intelligence 45 (2) (2022) 1781–1797.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] K. Lee, W. Kim, S. Lee, 从人体姿态相似性度量到 3D 人体姿态估计器：时间传播 LSTM 网络，IEEE 计算机视觉与模式识别学报
    45 (2) (2022) 1781–1797。'
- en: '[238] A. Zeng, X. Ju, L. Yang, R. Gao, X. Zhu, B. Dai, Q. Xu, Deciwatch: A
    simple baseline for 10$\times$ efficient 2d and 3d pose estimation, in: European
    Conference on Computer Vision, Springer, 2022, pp. 607–624.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] A. Zeng, X. Ju, L. Yang, R. Gao, X. Zhu, B. Dai, Q. Xu, Deciwatch: 一个用于
    10$\times$ 高效 2D 和 3D 姿态估计的简单基线，见：欧洲计算机视觉大会，Springer，2022，第 607–624 页。'
- en: '[239] J. Gong, L. G. Foo, Z. Fan, Q. Ke, H. Rahmani, J. Liu, Diffpose: Toward
    more reliable 3d pose estimation, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, 2023, pp. 13041–13051.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] J. Gong, L. G. Foo, Z. Fan, Q. Ke, H. Rahmani, J. Liu, Diffpose: 更可靠的
    3D 姿态估计，见：2023 年 IEEE/CVF 计算机视觉与模式识别大会论文集，第 13041–13051 页。'
- en: '[240] K. Holmquist, B. Wandt, Diffpose: Multi-hypothesis human pose estimation
    using diffusion models, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2023, pp. 15977–15987.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] K. Holmquist, B. Wandt, Diffpose: 使用扩散模型的多假设人体姿态估计，见：2023 年 IEEE/CVF
    国际计算机视觉大会论文集，第 15977–15987 页。'
- en: '[241] X. Ma, J. Su, C. Wang, W. Zhu, Y. Wang, 3d human mesh estimation from
    virtual markers, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2023, pp. 534–543.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] X. Ma, J. Su, C. Wang, W. Zhu, Y. Wang, 从虚拟标记器进行 3D 人体网格估计，见：2023 年 IEEE/CVF
    计算机视觉与模式识别大会论文集，第 534–543 页。'
- en: '[242] J. Kim, M.-G. Gwon, H. Park, H. Kwon, G.-M. Um, W. Kim, Sampling is matter:
    Point-guided 3d human mesh reconstruction, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2023, pp. 12880–12889.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] J. Kim, M.-G. Gwon, H. Park, H. Kwon, G.-M. Um, W. Kim, 采样至关重要：点导向 3D
    人体网格重建，见：2023 年 IEEE/CVF 计算机视觉与模式识别大会论文集，第 12880–12889 页。'
- en: '[243] K. Shetty, A. Birkhold, S. Jaganathan, N. Strobel, M. Kowarschik, A. Maier,
    B. Egger, Pliks: A pseudo-linear inverse kinematic solver for 3d human body estimation,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2023, pp. 574–584.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] K. Shetty, A. Birkhold, S. Jaganathan, N. Strobel, M. Kowarschik, A. Maier,
    B. Egger, Pliks: 一种用于 3D 人体估计的伪线性逆向运动学求解器，见：2023 年 IEEE/CVF 计算机视觉与模式识别大会论文集，第
    574–584 页。'
- en: '[244] Q. Fang, K. Chen, Y. Fan, Q. Shuai, J. Li, W. Zhang, Learning analytical
    posterior probability for human mesh recovery, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2023, pp. 8781–8791.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] Q. Fang, K. Chen, Y. Fan, Q. Shuai, J. Li, W. Zhang, 学习用于人体网格恢复的解析后验概率，见：2023
    年 IEEE/CVF 计算机视觉与模式识别大会论文集，第 8781–8791 页。'
- en: '[245] C. Zheng, X. Liu, G.-J. Qi, C. Chen, Potter: Pooling attention transformer
    for efficient human mesh recovery, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2023, pp. 1611–1620.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] C. Zheng, X. Liu, G.-J. Qi, C. Chen, Potter：用于高效人类网格恢复的池化注意力变换器，见于《IEEE/CVF
    计算机视觉与模式识别会议论文集》，2023年，第1611–1620页。'
- en: '[246] Q. Liu, A. Kortylewski, A. L. Yuille, Poseexaminer: Automated testing
    of out-of-distribution robustness in human pose and shape estimation, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
    672–681.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] Q. Liu, A. Kortylewski, A. L. Yuille, Poseexaminer：人类姿态和形状估计中的分布外鲁棒性自动测试，见于《IEEE/CVF
    计算机视觉与模式识别会议论文集》，2023年，第672–681页。'
- en: '[247] H. Cho, Y. Cho, J. Ahn, J. Kim, Implicit 3d human mesh recovery using
    consistency with pose and shape from unseen-view, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2023, pp. 21148–21158.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] H. Cho, Y. Cho, J. Ahn, J. Kim, 利用从未见视角的一致性进行隐式3D人类网格恢复，见于《IEEE/CVF 计算机视觉与模式识别会议论文集》，2023年，第21148–21158页。'
- en: '[248] T. Simon, H. Joo, I. Matthews, Y. Sheikh, Hand keypoint detection in
    single images using multiview bootstrapping, in: Proceedings of the IEEE conference
    on Computer Vision and Pattern Recognition, 2017, pp. 1145–1153.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] T. Simon, H. Joo, I. Matthews, Y. Sheikh, 单图像中的手部关键点检测，采用多视图自举方法，见于《IEEE
    计算机视觉与模式识别会议论文集》，2017年，第1145–1153页。'
- en: '[249] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    C. L. Zitnick, Microsoft coco: Common objects in context, in: Computer Vision–ECCV
    2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,
    Part V 13, Springer, 2014, pp. 740–755.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P.
    Dollár, C. L. Zitnick, Microsoft coco：上下文中的常见对象，见于《计算机视觉–ECCV 2014：第13届欧洲会议》，瑞士苏黎世，2014年9月6-12日，论文集，第V部分13，第740–755页，Springer，2014年。'
- en: '[250] K. Aberman, P. Li, D. Lischinski, O. Sorkine-Hornung, D. Cohen-Or, B. Chen,
    Skeleton-aware networks for deep motion retargeting, ACM Transactions on Graphics
    (TOG) 39 (4) (2020) 62–1.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] K. Aberman, P. Li, D. Lischinski, O. Sorkine-Hornung, D. Cohen-Or, B.
    Chen, 针对深度动作重新定位的骨架感知网络，《ACM 图形学交易》 (TOG) 39 (4) (2020) 62–1。'
- en: '[251] Z. Yang, W. Zhu, W. Wu, C. Qian, Q. Zhou, B. Zhou, C. C. Loy, Transmomo:
    Invariance-driven unsupervised video motion retargeting, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 5306–5315.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] Z. Yang, W. Zhu, W. Wu, C. Qian, Q. Zhou, B. Zhou, C. C. Loy, Transmomo：基于不变性的无监督视频动作重新定位，见于《IEEE/CVF
    计算机视觉与模式识别会议论文集》，2020年，第5306–5315页。'
- en: '[252] W.-Y. Yu, L.-M. Po, R. C. Cheung, Y. Zhao, Y. Xue, K. Li, Bidirectionally
    deformable motion modulation for video-based human pose transfer, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 7502–7512.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] W.-Y. Yu, L.-M. Po, R. C. Cheung, Y. Zhao, Y. Xue, K. Li, 双向可变形运动调制用于基于视频的人类姿态转移，见于《IEEE/CVF
    国际计算机视觉会议论文集》，2023年，第7502–7512页。'
- en: '[253] T. L. Gomes, R. Martins, J. Ferreira, R. Azevedo, G. Torres, E. R. Nascimento,
    A shape-aware retargeting approach to transfer human motion and appearance in
    monocular videos, International Journal of Computer Vision 129 (7) (2021) 2057–2075.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] T. L. Gomes, R. Martins, J. Ferreira, R. Azevedo, G. Torres, E. R. Nascimento,
    一种形状感知的重新定位方法，用于单目视频中的人类动作和外观转移，《计算机视觉国际杂志》129 (7) (2021) 2057–2075。'
- en: '[254] W. Zhu, Z. Yang, Z. Di, W. Wu, Y. Wang, C. C. Loy, Mocanet: Motion retargeting
    in-the-wild via canonicalization networks, in: Proceedings of the AAAI Conference
    on Artificial Intelligence, Vol. 36, 2022, pp. 3617–3625.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] W. Zhu, Z. Yang, Z. Di, W. Wu, Y. Wang, C. C. Loy, Mocanet：通过标准化网络进行野外动作重新定位，见于《AAAI
    人工智能会议论文集》，第36卷，2022年，第3617–3625页。'
- en: '[255] L. Mo, H. Li, C. Zou, Y. Zhang, M. Yang, Y. Yang, M. Tan, Towards accurate
    facial motion retargeting with identity-consistent and expression-exclusive constraints,
    in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36, 2022,
    pp. 1981–1989.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] L. Mo, H. Li, C. Zou, Y. Zhang, M. Yang, Y. Yang, M. Tan, 面向准确的面部动作重新定位，采用身份一致和表情排他性约束，见于《AAAI
    人工智能会议论文集》，第36卷，2022年，第1981–1989页。'
- en: '[256] X. Chen, M. Mihajlovic, S. Wang, S. Prokudin, S. Tang, Morphable diffusion:
    3d-consistent diffusion for single-image avatar creation, arXiv preprint arXiv:2401.04728
    (2024).'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] X. Chen, M. Mihajlovic, S. Wang, S. Prokudin, S. Tang, Morphable diffusion：用于单图像头像创建的3D一致扩散，arXiv
    预印本 arXiv:2401.04728 (2024)。'
- en: '[257] H. Yang, D. Yan, L. Zhang, Y. Sun, D. Li, S. J. Maybank, Feedback graph
    convolutional network for skeleton-based action recognition, IEEE Transactions
    on Image Processing 31 (2021) 164–175.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] H. Yang, D. Yan, L. Zhang, Y. Sun, D. Li, S. J. Maybank, 针对骨架的动作识别的反馈图卷积网络，《IEEE图像处理汇刊》31
    (2021) 164–175。'
- en: '[258] V. Mazzia, S. Angarano, F. Salvetti, F. Angelini, M. Chiaberge, Action
    transformer: A self-attention model for short-time pose-based human action recognition,
    Pattern Recognition 124 (2022) 108487.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] V. Mazzia, S. Angarano, F. Salvetti, F. Angelini, M. Chiaberge, Action
    transformer: 一种自注意力模型用于短时间姿态基础的人类动作识别，《模式识别》124 (2022) 108487。'
- en: '[259] Z. Lu, H. Wang, Z. Chang, G. Yang, H. P. Shum, Hard no-box adversarial
    attack on skeleton-based human action recognition with skeleton-motion-informed
    gradient, in: Proceedings of the IEEE/CVF International Conference on Computer
    Vision, 2023, pp. 4597–4606.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] Z. Lu, H. Wang, Z. Chang, G. Yang, H. P. Shum, 基于骨架的动作识别的硬性无盒对抗攻击与骨架运动信息梯度，在：IEEE/CVF国际计算机视觉会议论文集，2023年，页4597–4606。'
- en: '[260] C. Bian, W. Feng, L. Wan, S. Wang, Structural knowledge distillation
    for efficient skeleton-based action recognition, IEEE Transactions on Image Processing
    30 (2021) 2963–2976.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] C. Bian, W. Feng, L. Wan, S. Wang, 结构知识蒸馏用于高效的骨架动作识别，《IEEE图像处理汇刊》30 (2021)
    2963–2976。'
- en: '[261] D. C. Luvizon, D. Picard, H. Tabia, Multi-task deep learning for real-time
    3d human pose estimation and action recognition, IEEE transactions on pattern
    analysis and machine intelligence 43 (8) (2020) 2752–2764.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] D. C. Luvizon, D. Picard, H. Tabia, 实时三维人类姿态估计与动作识别的多任务深度学习，《IEEE模式分析与机器智能汇刊》43
    (8) (2020) 2752–2764。'
- en: '[262] Q. Bao, W. Liu, Y. Cheng, B. Zhou, T. Mei, Pose-guided tracking-by-detection:
    Robust multi-person pose tracking, IEEE Transactions on Multimedia 23 (2020) 161–175.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] Q. Bao, W. Liu, Y. Cheng, B. Zhou, T. Mei, 姿态引导的基于检测的跟踪：鲁棒的多人物姿态跟踪，《IEEE多媒体汇刊》23
    (2020) 161–175。'
- en: '[263] N. D. Reddy, L. Guigues, L. Pishchulin, J. Eledath, S. G. Narasimhan,
    Tessetrack: End-to-end learnable multi-person articulated 3d pose tracking, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2021, pp. 15190–15200.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] N. D. Reddy, L. Guigues, L. Pishchulin, J. Eledath, S. G. Narasimhan,
    Tessetrack：端到端可学习的多人物关节三维姿态跟踪，在：IEEE/CVF计算机视觉与模式识别会议论文集，2021年，页15190–15200。'
- en: '[264] S. Goel, G. Pavlakos, J. Rajasegaran, A. Kanazawa, J. Malik, Humans in
    4d: Reconstructing and tracking humans with transformers, arXiv preprint arXiv:2305.20091
    (2023).'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] S. Goel, G. Pavlakos, J. Rajasegaran, A. Kanazawa, J. Malik, Humans in
    4d：用变换器重建和跟踪人类，arXiv 预印本 arXiv:2305.20091 (2023)。'
- en: '[265] Y. Sun, Q. Bao, W. Liu, T. Mei, M. J. Black, Trace: 5d temporal regression
    of avatars with dynamic cameras in 3d environments, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2023, pp. 8856–8866.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] Y. Sun, Q. Bao, W. Liu, T. Mei, M. J. Black, Trace：在三维环境中使用动态相机的五维时间回归，
    在：IEEE/CVF计算机视觉与模式识别会议论文集，2023年，页8856–8866。'
- en: '[266] Y. Dai, C. Wen, H. Wu, Y. Guo, L. Chen, C. Wang, Indoor 3d human trajectory
    reconstruction using surveillance camera videos and point clouds, IEEE Transactions
    on Circuits and Systems for Video Technology 32 (4) (2021) 2482–2495.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] Y. Dai, C. Wen, H. Wu, Y. Guo, L. Chen, C. Wang, 基于监控摄像头视频和点云的室内三维人类轨迹重建，《IEEE视频技术电路与系统汇刊》32
    (4) (2021) 2482–2495。'
- en: '[267] M. Kocabas, Y. Yuan, P. Molchanov, Y. Guo, M. J. Black, O. Hilliges,
    J. Kautz, U. Iqbal, Pace: Human and camera motion estimation from in-the-wild
    videos, arXiv preprint arXiv:2310.13768 (2023).'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] M. Kocabas, Y. Yuan, P. Molchanov, Y. Guo, M. J. Black, O. Hilliges,
    J. Kautz, U. Iqbal, Pace：从实际视频中估计人类和相机运动，arXiv 预印本 arXiv:2310.13768 (2023)。'
- en: '[268] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri,
    E. Taropa, P. Bailey, Z. Chen, et al., Palm 2 technical report, arXiv preprint
    arXiv:2305.10403 (2023).'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S.
    Shakeri, E. Taropa, P. Bailey, Z. Chen, 等，Palm 2 技术报告，arXiv 预印本 arXiv:2305.10403
    (2023)。'
- en: '[269] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat, et al., Gpt-4 technical report, arXiv
    preprint arXiv:2303.08774 (2023).'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D.
    Almeida, J. Altenschmidt, S. Altman, S. Anadkat, 等，GPT-4 技术报告，arXiv 预印本 arXiv:2303.08774
    (2023)。'
- en: '[270] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,
    S. Whitehead, A. C. Berg, W.-Y. Lo, et al., Segment anything, arXiv preprint arXiv:2304.02643
    (2023).'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T.
    Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, 等，Segment anything，arXiv 预印本 arXiv:2304.02643
    (2023)。'
- en: '[271] J. Yang, M. Gao, Z. Li, S. Gao, F. Wang, F. Zheng, Track anything: Segment
    anything meets videos, arXiv preprint arXiv:2304.11968 (2023).'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] J. Yang, M. Gao, Z. Li, S. Gao, F. Wang, F. Zheng, 追踪一切：**Segment Anything**
    遇见视频，arXiv 预印本 arXiv:2304.11968 (2023)。'
- en: '[272] Y. Ci, Y. Wang, M. Chen, S. Tang, L. Bai, F. Zhu, R. Zhao, F. Yu, D. Qi,
    W. Ouyang, Unihcp: A unified model for human-centric perceptions, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
    17840–17852.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] Y. Ci, Y. Wang, M. Chen, S. Tang, L. Bai, F. Zhu, R. Zhao, F. Yu, D. Qi,
    W. Ouyang, **Unihcp**：一种用于以人为本的感知的统一模型，收录于：IEEE/CVF 计算机视觉与模式识别会议论文集，2023年，页码17840–17852。'
- en: '[273] Y. Feng, J. Lin, S. K. Dwivedi, Y. Sun, P. Patel, M. J. Black, Posegpt:
    Chatting about 3d human pose, arXiv preprint arXiv:2311.18836 (2023).'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] Y. Feng, J. Lin, S. K. Dwivedi, Y. Sun, P. Patel, M. J. Black, **Posegpt**：关于3D人体姿态的对话，arXiv
    预印本 arXiv:2311.18836 (2023)。'
