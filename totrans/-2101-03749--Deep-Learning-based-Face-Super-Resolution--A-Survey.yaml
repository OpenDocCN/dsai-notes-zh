- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:57:20'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2101.03749] Deep Learning-based Face Super-Resolution: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2101.03749](https://ar5iv.labs.arxiv.org/html/2101.03749)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning-based Face Super-Resolution: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Junjun Jiang School of Computer Science and Technology, Harbin Institute of
    TechnologyHarbinChina [jiangjunjun@hit.edu.cn](mailto:jiangjunjun@hit.edu.cn)
    ,  Chenyang Wang School of Computer Science and Technology, Harbin Institute of
    TechnologyHarbinChina [wangchy02@hit.edu.cn](mailto:wangchy02@hit.edu.cn) ,  Xianming
    Liu School of Computer Science and Technology, Harbin Institute of TechnologyHarbinChina
    [csxm@hit.edu.cn](mailto:csxm@hit.edu.cn)  and  Jiayi Ma Electronic Information
    School, Wuhan UniversityWuhanChina(2021)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Face super-resolution (FSR), also known as face hallucination, which is aimed
    at enhancing the resolution of low-resolution (LR) face images to generate high-resolution
    (HR) face images, is a domain-specific image super-resolution problem. Recently,
    FSR has received considerable attention and witnessed dazzling advances with the
    development of deep learning techniques. To date, few summaries of the studies
    on the deep learning-based FSR are available. In this survey, we present a comprehensive
    review of deep learning-based FSR methods in a systematic manner¹¹1A curated list
    of papers and resources to face super-resolution at [https://github.com/junjun-jiang/Face-Hallucination-Benchmark](https://github.com/junjun-jiang/Face-Hallucination-Benchmark)..
    First, we summarize the problem formulation of FSR and introduce popular assessment
    metrics and loss functions. Second, we elaborate on the facial characteristics
    and popular datasets used in FSR. Third, we roughly categorize existing methods
    according to the utilization of facial characteristics. In each category, we start
    with a general description of design principles, then present an overview of representative
    approaches, and then discuss the pros and cons among them. Fourth, we evaluate
    the performance of some state-of-the-art methods. Fifth, joint FSR and other tasks,
    and FSR-related applications are roughly introduced. Finally, we envision the
    prospects of further technological advancement in this field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Face super-resolution and Face hallucination and Deep learning and Convolution
    neural networkface super-resolution, deep learning, survey, facial characteristics^†^†copyright:
    acmcopyright^†^†journalyear: 2021^†^†journal: CSUR^†^†journalvolume: 0^†^†journalnumber:
    0^†^†article: 0^†^†publicationmonth: 0^†^†ccs: General and reference Reference
    works^†^†ccs: General and reference Surveys and overviews'
  prefs: []
  type: TYPE_NORMAL
- en: The research is supported by the National Natural Science Foundation of China
    (61971165, 61922027, 61773295), and the Fundamental Research Funds for the Central
    Universities.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Face super-resolution (FSR), a domain-specific image super-resolution problem,
    refers to the technique of recovering high-resolution (HR) face images from low-resolution
    (LR) face images. It can increase the resolution of an LR face image of low quality
    and recover the details. In many real-world scenarios, limited by physical imaging
    systems and imaging conditions, the face images are always low quality. Thus,
    with a wide range of applications and notable advantages, FSR has always been
    a hot topic since its birth in image processing and computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Summary of face super-resolution surveys since 2010.
  prefs: []
  type: TYPE_NORMAL
- en: '| No. | Survey title | Year | Venue |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | A survey of face hallucination [[1](#bib.bib1)] | 2012 | CCBR |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | A comprehensive survey to face hallucination [[2](#bib.bib2)] | 2014
    | IJCV |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | A review of various approaches to face hallucination [[3](#bib.bib3)]
    | 2015 | ICACTA |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Face super resolution: a survey  [[4](#bib.bib4)] | 2017 | IJIGSP |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Super-resolution for biometrics: a comprehensive survey [[5](#bib.bib5)]
    | 2018 | PR |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Face hallucination techniques: a survey [[6](#bib.bib6)] | 2018 | CICT
    |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Survey on GAN-based face hallucination with its model development [[7](#bib.bib7)]
    | 2019 | IET |'
  prefs: []
  type: TYPE_TB
- en: 'The concept of FSR was first proposed in 2000 by Baker and Kanade [[8](#bib.bib8)],
    who are the pioneers of the FSR technique. They develop a multi-level learning
    and prediction model based on the Gaussian image pyramid to improve the resolution
    of an LR face image. Liu *et al*. [[9](#bib.bib9)] propose to integrate a global
    parametric principal component analysis (PCA) model with a local nonparametric
    Markov random field (MRF) model for FSR. Since then, a number of innovative methods
    have been proposed, and FSR has become the subject of active research efforts.
    Researchers super-resolve the LR face images by means of global face statistical
    models [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)], local patch-based representation
    methods [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)], or hybrid ones [[28](#bib.bib28), [29](#bib.bib29)].
    These methods have achieved good performance, however, have trouble when meeting
    requirements in practice. With the rapid development of deep learning technique,
    attractive advantages over previous attempts have been obtained and have been
    applied into image or video super-resolution. Many comprehensive surveys have
    reviewed recent achievements in these fields, *i.e.*, general image super-resolution
    surveys [[30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)], and video super-resolution
    survey [[33](#bib.bib33)]. Towards FSR, a domain-specific image super-resolution,
    a few surveys are listed in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣ Deep
    Learning-based Face Super-Resolution: A Survey"). In the early stage of research,
     [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6)] provide a comprehensive review of traditional FSR methods (mainly
    including patch-based super-resolution, PCA-based methods, *etc*.), while Liu
    *et al*. [[7](#bib.bib7)] offer a generative adversarial network (GAN) based FSR
    survey. However, so far no literature review is available on deep learning super-resolution
    specifically for human faces. In this paper, we present a comparative study of
    different deep learning-based FSR methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of this survey are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The survey provides a comprehensive review of recent techniques for FSR, including
    problem definition, commonly used evaluation metrics and loss functions, the characteristics
    of FSR, benchmark datasets, deep learning-based FSR methods, performance comparison
    of state-of-the-arts, methods that jointly perform FSR and other tasks, and FSR-related
    applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The survey summarizes how existing deep learning-based FSR methods explore the
    potential of network architecture and take advantage of the characteristics of
    face images, as well as compare the similarities and differences among these methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The survey discusses the challenges and envisions the prospects of future research
    in the FSR field.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the following, we will cover the existing deep learning-based FSR methods
    and Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Deep Learning-based Face Super-Resolution:
    A Survey") shows the taxonomy of FSR. Section [2](#S2 "2\. Background ‣ Deep Learning-based
    Face Super-Resolution: A Survey") introduces the problem definition of FSR, and
    commonly used assessment metrics and loss functions. Section [3](#S3 "3\. Characteristics
    of Face Images ‣ Deep Learning-based Face Super-Resolution: A Survey") presents
    the facial characteristics (*i.e.*, prior information, attribute information,
    and identity information) and reviews some mainstream face datasets. In Section [4](#S4
    "4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A Survey"), we discuss
    FSR methods. To avoid exhaustive enumeration and take facial characteristics into
    consideration, FSR methods are categorized according to facial characteristics
    used. In Section [4](#S4 "4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey"), five major categories are presented: general FSR methods, prior-guided
    FSR methods, attribute-constrained FSR methods, identity-preserving FSR methods,
    and reference FSR methods. Depending on the network architecture or the utilization
    of facial characteristics, every category is further divided into several subcategories.
    Moreover, Section [4](#S4 "4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey") compares the performance of some state-of-the-art methods. Besides,
    Section [4](#S4 "4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey") also reviews some methods dealing with joint tasks and FSR-related
    applications. Section [5](#S5 "5\. Conclusion and Future Directions ‣ Deep Learning-based
    Face Super-Resolution: A Survey") concludes the FSR and further discusses the
    limitations as well as envisions the prospects of further technological advancement.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/124ebc3936fda5dea4db133213352ab3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. The taxonomy of face super-resolution.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Problem Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'FSR focuses on recovering the corresponding HR face image from an observed
    LR face image. The image degradation model $\Phi$ can be mathematically written
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $I_{\text{LR}}=\Phi(I_{\text{HR}},\theta),$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\theta$ represents the model parameters including blurring kernel, downsampling
    operation, and noise, $I_{\text{LR}}$ is the observed LR face image, and $I_{\text{HR}}$
    is the original HR face image. FSR is devoted to simulating the inverse process
    of the degradation model and recovers the $I_{\text{SR}}$ from $I_{\text{LR}}$,
    which can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $I_{\text{SR}}=\Phi^{\text{-1}}(I_{\text{LR}},\delta)=F(I_{\text{LR}},\delta),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $F$ is the super-resolution model (inverse degradation model), $\delta$
    represents the parameters of $F$, and $I_{\text{SR}}$ represents the super-resolved
    result. The optimization of $\delta$ can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $\hat{\delta}=\underset{\delta}{\text{argmin}}\,\,\mathcal{L}(I_{\text{SR}},I_{\text{HR}}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{L}$ represents the loss between $I_{\text{SR}}$ and $I_{\text{HR}}$
    and $\hat{\delta}$ is the optimal parameter of the trained model. In FSR, MSE
    loss and $\mathcal{L}_{1}$ loss are the most popular loss functions, and some
    models tend to use a combination of multiple loss functions, which will be reviewed
    in Section [2.2](#S2.SS2 "2.2\. Assessment Metrics and Loss Functions ‣ 2\. Background
    ‣ Deep Learning-based Face Super-Resolution: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: The degradation model and parameters are all unavailable in a real-world environment,
    and $I_{\text{LR}}$ is the only given information. To simulate the image degradation
    process, researchers tend to use mathematical models to generate some LR and HR
    pairs to train the model. The simplest mathematical model is
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $I_{\text{LR}}=(I_{\text{HR}})\downarrow_{s},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\downarrow$ denotes the downsampling operation, and $s$ is the scaling
    factor. However, this pattern is too simple to match the real-world degradation
    process. To better mimic the real degradation process, researchers design a degradation
    process with the combination of many operations (*e.g.*, downsampling, blur, noise,
    and compression) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $I_{\text{LR}}=J((I_{\text{HR}}\otimes k)\downarrow_{s}+\,n),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $k$ is the blurring kernel, $\otimes$ represents the convolutional operation,
    $n$ denotes the noise, and $J$ denotes the image compression. Various combinations
    of different operations are used in FSR. They include the widely used bicubic
    model [[34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)], as well as the general
    degradation model used for blind FSR [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)].
    However, they are not introduced in detail in this survey.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Assessment Metrics and Loss Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In deep learning-based FSR methods, the loss function which measures the difference
    between $I_{\text{HR}}$ and $I_{\text{SR}}$ plays an important role in guiding
    the network training. Upon acquiring the trained network, the reconstruction performance
    of these methods can be evaluated by the assessment metrics. The preferences of
    different loss functions are different. For example, $\mathcal{L}_{2}$ loss tends
    to produce the result that is faithful to the original image (high PSNR value),
    and the perceptual and adversarial losses will generate subjectively pleasing
    results (low FID [[40](#bib.bib40)] and LPIPS [[41](#bib.bib41)] values). In practice,
    we can choose the appropriate loss function according to the needs. Considering
    the relationship between loss functions and assessment metrics, we introduce them
    together in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1\. Image Quality Assessment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generally, two main methods of quality evaluation are subjective and objective
    evaluation. Subjective evaluation relies on the judgement of humans, and tends
    to invite readers or interviewers to see and assess the quality of the generated
    images, leading to results always consistent with human perception but time-assuming,
    inconvenient and expensive. In contrast, the objective evaluation mainly utilizes
    statistical data to reflect the quality of the generated images. In general, the
    objective evaluation methods usually produce different results from subjective
    evaluation metrics, because the starting point of objective evaluation methods
    is mathematics instead of human visual perception, which leaves the assessment
    image quality in dispute. Here, we introduce some popular assessment metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Peak Signal-to-Noise Ratio (PSNR): PSNR is a commonly used objective assessment
    metric in FSR. Given $I_{\text{HR}}$ and $I_{\text{SR}}$, the mean square error
    (MSE) between them is firstly calculated, then the PSNR is obtained,'
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\text{MSE}=\frac{\text{1}}{hwc}\left\&#124;I_{\text{SR}}-I_{\text{HR}}\right\&#124;^{2}_{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (7) |  | $\text{PSNR}=10\,log_{\text{10}}(\frac{\text{M}^{2}}{\text{MSE}}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $h$, $w$, and $c$ denote the height, width, and channel of the image,
    and M is the maximum possible pixel value (*i.e.*, 255 for 8-bit images). The
    smaller the pixel-wise difference of the two images, the higher the PSNR. In this
    pattern, PSNR focuses on the distance between every pair of pixels in two images,
    which is inconsistent with human perception, resulting in poor performance when
    human perception is more important.
  prefs: []
  type: TYPE_NORMAL
- en: 'Structural Similarity Index (SSIM): SSIM [[42](#bib.bib42)] is also a popular
    objective assessment metric that measures the structural similarity between two
    images. To be specific, SSIM measures similarity from three aspects: luminance,
    contrast, and structure. Given $I_{\text{HR}}$ and $I_{\text{SR}}$, SSIM is obtained
    by'
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $\text{SSIM}=l(I_{\text{HR}},I_{\text{SR}})*C(I_{\text{HR}},I_{\text{SR}})*S(I_{\text{HR}},I_{\text{SR}}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $l(I_{\text{HR}},I_{\text{SR}})$, $C(I_{\text{HR}},I_{\text{SR}})$ and
    $S(I_{\text{HR}},I_{\text{SR}})$ denote the similarity of the luminance, contrast
    and structure. SSIM varies from 0 to 1\. The higher the structural similarity
    of the two images, the larger the SSIM. Considering the uneven distribution of
    the image, SSIM is not reliable enough. Thus, multi-scale structural similarity
    index measure (MS-SSIM) [[43](#bib.bib43)] is proposed, which divides the image
    into multiple windows, first assesses SSIM for every window separately, and then
    converges them to obtain MS-SSIM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learned Perceptual Image Patch Similarity (LPIPS): LPIPS [[41](#bib.bib41)]
    measures the distance between two images in a deep feature space. LPIPS is more
    in line with human judgement than PSNR and SSIM. The more similar the two images,
    the smaller the LPIPS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fréchet Inception Distance (FID): In contrast to PSNR and SSIM, FID [[40](#bib.bib40)]
    focuses on the difference between $I_{\text{HR}}$ and $I_{\text{SR}}$ in a distribution-wise
    manner, and it is always applied to assess the visual quality of face images.
    The better the visual quality, the smaller the FID.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Natural Image Quality Evaluator (NIQE): NIQE [[44](#bib.bib44)] is a no-reference
    metric that measures the distance between two multivariate Gaussian models fitting
    natural images and the evaluated images without ground truth images. Specifically,
    the fitting of multivariate Gaussian model is based on the quality-aware features
    derived from the natural scene statistic model. The better the visual quality,
    the smaller the NIQE.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean Opinion Score (MOS): MOS is a commonly used subjective assessment metric,
    in contrast to the above objective quantitative metrics. To obtain the MOS, human
    raters are asked to assign perceptual quality scores to the tested images. Finally,
    MOS is obtained by calculating the arithmetic mean ratings assigned by human raters.
    When the number of human raters is small, MOS would be biased while MOS would
    be faithful enough when the number of human raters is large.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2\. Loss Functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Initially, pixel-wise $\mathcal{L}_{2}$ loss (also known as MSE loss) is popular,
    however, researchers then find that models based on $\mathcal{L}_{2}$ loss tend
    to generate smooth results. Then many kinds of loss functions are employed, such
    as pixel-wise $\mathcal{L}_{1}$ loss, SSIM loss, perceptual loss, adversarial
    loss, *etc*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pixel-wise Loss: Pixel-wise loss measures the distance between the two images
    at pixel level, including $\mathcal{L}_{1}$ loss that calculates the mean absolute
    error, $\mathcal{L}_{2}$ loss that calculates the mean square error, Huber loss [[45](#bib.bib45)]
    and Carbonnier penalty function [[46](#bib.bib46)]. With the constrain of the
    pixel-wise loss, the obtained $I_{\text{SR}}$ can be close enough to the $I_{\text{HR}}$
    on the pixel value. From the definition, $\mathcal{L}_{2}$ loss is sensitive to
    large errors but indifferent to small errors, while $\mathcal{L}_{1}$ loss treats
    them equally. Therefore, $\mathcal{L}_{1}$ loss has advantages in improving the
    performance and convergence over $\mathcal{L}_{2}$ loss. Overall, pixel-wise loss
    can force the model to improve PSNR, but the generated images are always over-smooth
    and lack high-frequency details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SSIM Loss: Similar to pixel-wise loss, SSIM loss is designed to improve the
    structure similarity between super-resolved image and the original HR one:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $\mathcal{L}_{\text{SSIM}}(I_{\text{HR}},I_{\text{SR}})=\frac{1}{2}\left(1-F_{\text{SSIM}}(I_{\text{HR}},I_{\text{SR}})\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $F_{\text{SSIM}}$ denotes the function of SSIM. Except for SSIM loss,
    multi-scale SSIM loss can calculate SSIM loss at different scales.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perceptual Loss: To improve the perceptual quality, one solution is to minimize
    the perceptual loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (10) |  | $\mathcal{L}_{\text{Perceptual}}(I_{\text{HR}},I_{\text{SR}},\Psi,l)=\left\&#124;\Psi^{l}(I_{\text{HR}})-\Psi^{l}(I_{\text{SR}})\right\&#124;_{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\Psi$ is the pretrained network and $l$ is the $l$-th layer. In essence,
    the perceptual loss measures the distance between the features extracted from
    $\Psi$ (*e.g.*, VGG [[47](#bib.bib47)]), and it can evaluate the difference at
    the semantic level. Perceptual loss encourages the network to generate $I_{\text{SR}}$
    that is more perceptually similar to $I_{\text{HR}}$. The $I_{\text{SR}}$ predicted
    by the model with perceptual loss always looks more pleasant but usually has lower
    PSNR than those pixel-wise loss-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adversarial Loss: Adversarial loss, proposed in generative adversarial network
    (GAN) [[48](#bib.bib48)], is also widely used in FSR. For details, GAN is comprised
    of two models: a generator (G) and a discriminator (D). In FSR, GAN can be described
    as follows: G is the super-resolution model which generates the super-resolved
    face with an LR face image as input, and D discriminates whether the output result
    is generated or real. In the training phase, G and D are trained alternatively.
    Early methods  [[34](#bib.bib34), [49](#bib.bib49)] use cross entropy-based adversarial
    loss expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (11) |  | $\mathcal{L}_{\text{G}}(I_{\text{SR}})=-\text{log}(\mathcal{D}(I_{\text{SR}})),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (12) |  | $\mathcal{L}_{\text{D}}(I_{\text{HR}},I_{\text{SR}})=-\text{log}(\mathcal{D}(I_{\text{HR}}))-\text{log}(1-\mathcal{D}(I_{\text{SR}})),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}_{\text{D}}$ and $\mathcal{L}_{\text{G}}$ denotes the loss
    function of D and G, respectively, $\mathcal{D}$ denotes the function of D, and
    $I_{\text{HR}}$ is randomly sampled from HR training samples. However, the model
    trained with this adversarial loss is always unstable and may cause model collapse.
    Therefore, Wasserstein GAN [[50](#bib.bib50)] and WGAN-GP [[51](#bib.bib51)] are
    proposed to alleviate the training difficulties. The model trained with adversarial
    loss tends to introduce artificial details, leading to worse PSNR and SSIM but
    pleasing visual quality with smaller FID.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cycle Consistency Loss: Cycle consistency loss is proposed by CycleGAN [[52](#bib.bib52)].
    In CycleGAN-based FSR, two cooperated models are used: a super-resolution model
    super-resolves the $I_{\text{LR}}$ to recover the $I_{\text{SR}}$, and a degradation
    model downsamples the $I_{\text{SR}}$ back to $I_{\text{LR}^{{}^{\prime}}}$. In
    turn, the degradation model downsamples the HR face image to obtain $I_{\text{HLR}}$,
    and then the super-resolution model recovers the $I_{\text{HLR}}$ to generate
    $I_{\text{HR}^{{}^{\prime}}}$. The cycle consistent loss is aimed to keep the
    consistency between $I_{\text{LR}}$ ($I_{\text{LR}^{{}^{\prime}}}$) and $I_{\text{HR}}$
    ($I_{\text{HR}^{{}^{\prime}}}$),'
  prefs: []
  type: TYPE_NORMAL
- en: '| (13) |  | $\mathcal{L}_{\text{Cycle}}(I_{\text{LR}},I_{\text{LR}^{{}^{\prime}}},I_{\text{HR}},I_{\text{HR}^{{}^{\prime}}})=\left\&#124;I_{\text{LR}}-I_{\text{LR}^{{}^{\prime}}}\right\&#124;_{2}+\left\&#124;I_{\text{HR}}-I_{\text{HR}^{{}^{\prime}}}\right\&#124;_{2}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: In addition to the above loss functions, many other loss functions are also
    used in FSR, including style loss [[53](#bib.bib53)], feature match loss [[54](#bib.bib54)],
    *etc*. Due to the limitation of space, we do not introduce them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Characteristics of Face Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d1a67af7fc49c3631849dce7ba91d91.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Facial characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Human face is a highly structured object with its own unique characteristics,
    which can be explored and utilized in FSR task. In this section, we simply introduce
    these facial characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Prior Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3\. Characteristics of Face Images
    ‣ Deep Learning-based Face Super-Resolution: A Survey"), structural priors can
    be found in face images, such as facial landmarks, facial heatmaps and facial
    paring maps.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Facial landmarks*: These locate the key points of facial components. The number
    of landmarks varies in different datasets, such as CelebA [[55](#bib.bib55)],
    which provides five landmarks while Helen [[56](#bib.bib56)] offers 194 landmarks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Facial heatmaps*: These are generated from facial landmarks. Facial landmarks
    give accurate points of the facial components, while heatmaps give the probability
    of the point being a facial landmark. To generate the heatmaps, every landmark
    is represented by a Gaussian kernel centered on the location of the landmark.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Facial parsing maps*: These are semantic segmentation maps of face images
    separating the facial components from face images, including eyes, nose, mouth,
    skin, ears, hair, and others.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These face structure prior information can provide the location of facial components
    and facial structure information. We can expect to recover more reasonable target
    face images if we incorporate these prior knowledge to regularize or guide the
    FSR models.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Summary of public face image datasets for FSR.
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Number | #Attributes | #Landmarks | Parsing maps | Identity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CelebA [[55](#bib.bib55)] | 202,599 | 40 | 5 | $\times$ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| CelebAMask-HQ [[57](#bib.bib57)] | 30,000 | $\times$ | $\times$ | ✓ | $\times$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Helen [[56](#bib.bib56)] | 2,330 | $\times$ | 194 | ✓ | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| FFHQ [[58](#bib.bib58)] | 70,000 | $\times$ | 68 | $\times$ | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| AFLW [[59](#bib.bib59)] | 25,993 | $\times$ | 21 | $\times$ | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| 300W [[60](#bib.bib60)] | 3,837 | $\times$ | 68 | $\times$ | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| LS3D-W [[61](#bib.bib61)] | 230,000 | $\times$ | 68 | $\times$ | $\times$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Menpo [[62](#bib.bib62)] | 9,000 | $\times$ | 68 | $\times$ | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| LFW [[63](#bib.bib63)] | 13,233 | 73 | $\times$ | $\times$ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| LFWA [[64](#bib.bib64)] | 13,233 | 40 | $\times$ | $\times$ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| VGGFace [[65](#bib.bib65)] | 3,310,000 | $\times$ | $\times$ | $\times$ |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: 3.2\. Attribute Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Second, the attributes, such as gender, hair color, and others, are the affiliated
    features of face images and can be seen as semantic-level information. In FSR,
    because of one-to-many maps from LR images to HR ones, the recovered face image
    may contain artifacts and even wrong attributes. For example, the face in the
    recovered result does not wear but the ground truth wears eyeglasses. At this
    time, attribute information can remind the network which attribute should be covered
    in the result. From a different perspective, attribute information also contains
    facial details. Taking eyeglasses as an example, the attribute of wearing eyeglasses
    provides the details of the facial eyes. We provide a concise example of attribute
    information in Fig. [2](#S3.F2 "Figure 2 ‣ 3\. Characteristics of Face Images
    ‣ Deep Learning-based Face Super-Resolution: A Survey"). Moreover, these attributes
    are always binary in the face dataset, 1 denotes that the face image has the attribute,
    while 0 means there is no such information.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Identity Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Third, every face image corresponds to a person, which is enabled by identity
    information. This type of information is always used for keeping the identity
    consistency between the super-resolved result and the ground truth. On the one
    hand, the person should not be changed after super-resolution visually. On the
    other hand, FSR should facilitate the performance of face recognition. Similar
    to attribute information, identity also offers high-level constraints to the FSR
    task and is beneficial to face restoration.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Datasets for FSR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In recent years, many face image datasets are used for FSR, which differ in
    many aspects, *e.g.*, the number of samples, facial characteristics contained
    and others. In Table [2](#S3.T2 "Table 2 ‣ 3.1\. Prior Information ‣ 3\. Characteristics
    of Face Images ‣ Deep Learning-based Face Super-Resolution: A Survey"), we list
    a number of commonly used face image datasets and simply indicate their amount
    and the facial characteristics offered. For parsing maps and identity, we only
    present whether they are provided or not, while for attributes and landmarks,
    we offer the specific amount. Aside from these datasets, many other face datasets
    are used in FSR, including CACD200 [[66](#bib.bib66)], VGGFace2 [[67](#bib.bib67)],
    UMDFaces [[68](#bib.bib68)], CASIA-WebFace [[69](#bib.bib69)], and others. It
    is worth noting that all above-mentioned datasets only provide HR face images.
    If we want to use them for training and evaluating any super-resolution model,
    we need to generate the corresponding LR face images using the degradation model
    introduced in Section  [2](#S2 "2\. Background ‣ Deep Learning-based Face Super-Resolution:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. FSR Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At present, various deep learning FSR methods have been proposed. On the one
    hand, these methods tap the potential of the efficient network for FSR regardless
    of facial characteristics, *i.e.*, developing a basic convolution neural network
    (CNN) or generative adversarial network (GAN) for face reconstruction. On the
    other hand, some approaches focus on the utilization of facial characteristics,
    *e.g.*, using structure prior information to facilitate face restoration and so
    on. Furthermore, some recently proposed models introduce additional high-quality
    reference face images to assist the restoration. Here, according to the type of
    face image special information used, we divide FSR methods into five categories:
    general FSR, prior-guided FSR, attribute-constrained FSR, identity-preserving
    FSR, and reference FSR. In this section, we concentrate on every kind of FSR method
    and introduce each category in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. General FSR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'General FSR methods mainly focus on designing an efficient network and exploit
    the potential of efficient network structure for FSR without any facial characteristics.
    In the early days, most of these methods are based on CNN and incorporate various
    advanced architectures (including back projection, residual network, spatial or
    channel attention, *etc*.), to improve the representation ability of the network.
    Since then, many FSR methods by using advanced networks have been proposed. We
    divide general FSR methods into four categories: basic CNN-based methods, GAN-based
    methods, reinforcement learning-based methods, and ensemble learning-based methods.
    Aiming to present a clear and concise overview, we summarize the general FSR methods
    in Fig. [3](#S4.F3 "Figure 3 ‣ 4.1\. General FSR ‣ 4\. FSR Methods ‣ Deep Learning-based
    Face Super-Resolution: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e9e6428b57e8ddd7a2ebcda28b396f59.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Overview of general FSR methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1\. Basic CNN-based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Inspired by the pioneer deep learning general image super-resolution method
    [[70](#bib.bib70)], some researchers also propose to incorporate the CNN network
    into the FSR task. Depending on whether they consider the global information and
    local differences, we can further divide the basic CNN-based methods into three
    categories: global methods that feed the entire face into the network and recover
    face images globally, local methods that divide face images into different components
    and then recover them, and mixed methods that recover face images locally and
    globally.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Global Methods: In the early years, researchers treat a face image as a whole
    and recover it globally. Inspired by the strong representative ability of CNN,
    bi-channel convolutional neural network (BCCNN) [[71](#bib.bib71)] and  [[72](#bib.bib72)]
    directly learn a mapping from LR face images to HR ones. Then, benefiting from
    the performance gain of iterative back projection (IBP) in general image super-resolution,
    Huang *et al*. [[73](#bib.bib73)] introduce IBP to FSR as an extra post-processing
    step, developing the SRCNN-IBP method. After that, the thought of back projection
    is generally used in FSR [[74](#bib.bib74), [75](#bib.bib75)]. Later on, channel
    and spatial attention mechanisms greatly improve the general image super-resolution
    methods, which inspires researchers to explore their utilization in FSR. Thus,
    a number of innovative methods integrating the attention mechanism are proposed [[76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78)]. In these works, two representative methods
    are E-ComSupResNet [[77](#bib.bib77)] that introduces a channel attention mechanism
    and SPARNet [[78](#bib.bib78)] which has a well-designed spatial attention for
    FSR. Besides that, many researchers design the cascaded model and exploit multi-scale
    information to improve the restoration performance [[79](#bib.bib79), [80](#bib.bib80),
    [81](#bib.bib81)].'
  prefs: []
  type: TYPE_NORMAL
- en: It is observed that super-resolution in the image domain produces smooth results
    without high-frequency detail. Considering that wavelet transform can represent
    the textural and contextual information of the images, WaSRNet [[82](#bib.bib82)]
    and [[83](#bib.bib83)] transform face images into wavelet coefficients and super-resolve
    the face images in the wavelet coefficient domain to avoid over-smooth results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Local Methods: Global methods can capture global information but cannot well
    recover the face details. Thus, local methods are developed to recover different
    parts of a face image differently. Super-resolution technique based on de?nition-scalable
    inference (SRDSI) [[84](#bib.bib84)] decomposes the face into a basic face with
    low-frequency and a compensation face with high-frequency through PCA. Then, SRDSI
    recovers the basic face and the compensation face with very deep convolutional
    network (VDSR) [[85](#bib.bib85)] and sparse representation respectively. Finally,
    the two recovered faces are fused. After that, many patch-based methods have been
    proposed [[86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88)], all of which
    divide face images into several patches and train models for recovering the corresponding
    patches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mixed Methods: Considering that global methods can capture global structure
    but ignore local details while local methods focus on local details but lose global
    structure, a line of research naturally combines global and local methods for
    capturing global structure and recovering local details simultaneously. At first,
    global-local network [[89](#bib.bib89), [90](#bib.bib90)] develop a global upsampling
    network to model global constraints and a local enhancement network to learn face-specific
    details. To simultaneously capture global clues and recover local details, dual-path
    deep fusion network (DPDFN) [[91](#bib.bib91)] constructs two individual branches
    for learning global facial contours and local facial component details, and then
    fuses the result of the two branches to generate the final SR result.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2\. GAN-based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compared with CNN-based methods that utilize pixel-wise loss and generate smooth
    face images, GAN, first proposed by Goodfellow *et al*. [[48](#bib.bib48)], which
    can be applied to generate realistic-looking face images with more details, inspires
    researchers to design GAN-based methods. At first, researchers focus on designing
    various GANs to learn from paired or unpaired data. In recent years, how to utilize
    a pretrained generative model to boost FSR has attracted increasing attention.
    Therefore, GAN-based methods can be divided into general GAN-based methods and
    generative prior-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'General GAN-based Methods: In the early stage, Yu *et al*. [[34](#bib.bib34)]
    develop ultra-resolving face images by discriminative generative networks (URDGN),
    which consists of two subnetworks: a discriminative model to distinguish a real
    HR face image or an arti?cially super-resolved output, and a generative model
    to generate SR face images to fool the discriminative model and match the distribution
    of HR face images. MLGE [[92](#bib.bib92)] not only designs discriminators to
    distinguish face images but also applies edge maps of the face images to reconstruct
    HR face images. Recently, HiFaceGAN [[93](#bib.bib93)] and the works of [[94](#bib.bib94),
    [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97)] also super-resolve face
    images with generative models. Instead of directly feeding the whole face images
    into the discriminator, PCA-SRGAN [[98](#bib.bib98)] decomposes face images into
    components by PCA and progressively feeds increasing components of the face images
    into the discriminator to reduce the learning difficulty of the discriminator.
    The commonality of these types of GAN is that the discriminator outputs a single
    probability value to characterize whether the result is a real face image. However,
    Zhang *et al*. [[99](#bib.bib99)] assume that a single probability value is too
    fragile to represent a whole image, thus they design a supervised pixel-wise GAN
    (SPGAN) whose discriminator outputs a discriminative matrix with the same resolution
    as the input images, and design a supervised pixel-wise adversarial loss, thus
    recovering more photo-realistic face images.'
  prefs: []
  type: TYPE_NORMAL
- en: The above methods rely on the artificial LR and HR pairs generated by a known
    degradation. However, the quality of the real-world LR image is affected by a
    wide range of factors such as the imaging conditions and the imaging system, leading
    to the complicated unknown degradation of real LR images. The gap between real
    LR images and artificial LR ones is large and will inevitably decrease the performance
    when applying methods trained on the artificial pairs to real LR images [[100](#bib.bib100)].
    To settle this problem, real-world super-resolution (RWSR) [[101](#bib.bib101)]
    first estimates the parameters from real LR faces, such as the blur kernel, noise,
    and compression, and then generates the LR and HR face image pairs with estimated
    parameters for the training of the model.
  prefs: []
  type: TYPE_NORMAL
- en: LRGAN [[102](#bib.bib102)] proposes to learn the degradation before super-resolution
    from unpaired data. It designs a high-to-low GAN to learn the real degradation
    process from unpaired LR and HR face images and create paired LR and HR face images
    for training low-to-high GAN. Specifically, with HR face images as input, the
    high-to-low GAN generates LR face images (GLR) that should belong to the real
    LR distribution and be close to the corresponding downsampled HR face images.
    Then, for low-to-high GAN, GLRs are fed into the generator to recover the SR results
    which have to be close to HR face images and match the real HR distribution. Goswami
    *et al*. [[103](#bib.bib103)] further develop a robust FSR method and Zheng *et
    al*.  [[104](#bib.bib104)] utilize semi-dual optimal transport to guide model
    learning and develop semi-dual optimal transport CycleGAN. Considering that discrepancies
    between GLRs in the training phase and real LR face images in the testing phase
    still exist, researchers introduce the concept of characteristic regularization
    (CR) [[105](#bib.bib105)]. Different from LRGAN, CR transforms the real LR face
    images into artificial LR ones and then conducts super-resolution reconstruction
    in the artificial LR space. Based on CycleGAN, CR learns the mapping between real
    LR face images and artificial LR ones. Then, it uses the artificial LR face images
    generated from real LR ones to fine-tune the super-resolution model, which is
    pretrained by the artificial pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative prior-based methods: Recently, many face generation models, such
    as popular StyleGAN [[58](#bib.bib58)], StyleGAN v2 [[106](#bib.bib106)], ProGAN [[107](#bib.bib107)],
    StarGAN [[108](#bib.bib108)], *etc.*, have been proposed and they are capable
    of generating faithful faces with a high degree of variability. Thus, more and
    more researchers explore the generative prior of pretrained GAN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first generative prior-based FSR method is PULSE [[109](#bib.bib109)].
    It formulates FSR as a generation problem to generate high-quality SR face image
    so that the downsampled SR result is close to LR face image. Mathematically, the
    problem can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (14) |  | $min_{G}\left\&#124;G(z)\downarrow_{s}-I_{\text{LR}}\right\&#124;,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $z$ is a randomly sampled latent vector and the input of the pretrained
    StyleGAN [[58](#bib.bib58)], $\downarrow$ is the downsampling operation, $s$ is
    the downsampling factor, and $G$ denotes the function of the generator. PULSE
    solves FSR from a new perspective and this inspires many other works.
  prefs: []
  type: TYPE_NORMAL
- en: However, the latent code $z$ in PULSE is randomly sampled and in low dimension,
    making the generated images lose important spatial information. To overcome this
    problem, GLEAN [[110](#bib.bib110)], CFP-GAN [[111](#bib.bib111)] and GPEN [[112](#bib.bib112)]
    are developed. Rather than directly employing the pretrained StyleGAN [[58](#bib.bib58)],
    they develop their own networks and embed the pretrained generation network of
    StyleGAN [[58](#bib.bib58)] into their own networks to incorporate the generative
    prior. To maintain faithful information, they not only obtain latent code by encoding
    LR face images instead of randomly sampling, but also extract multi-scale features
    from LR face images and fuse the features into the generation network. In this
    way, the generative prior provided by the pretrained StyleGAN can be fully utilized
    and the important spatial information can be well maintained.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3\. Reinforcement Learning-based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Deep learning-based FSR methods learn the mapping from LR face images to HR
    ones, but ignore the contextual dependencies among the facial parts. Cao *et al*.
    propose to recurrently discover facial parts and enhance them by fully exploiting
    the global inter-dependency of the image, then attention-aware face hallucination
    via deep reinforcement learning (Attention-FH) is proposed [[113](#bib.bib113)].
    Specifically, Attention-FH has two subnetworks: a policy network that locates
    the region that needs to be enhanced in the current step, and a local enhancement
    network that enhances the selected region.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4\. Ensemble Learning-based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CNN-based methods utilize pixel-wise loss to recover face images with higher
    PSNR and smoother details while GAN-based methods can generate face images with
    lower PSNR but more high-frequency details. To combine the advantages of different
    types of methods, ensemble learning is used in adaptive threshold-based multi-model
    fusion network (ATFMN) [[114](#bib.bib114)]. Specifically, ATFMN uses three models
    (CNN-based, GAN-based, and RNN-based) to generate candidate SR faces, and then
    fuses all candidate SR faces to reconstruct the final SR result. In contrast to
    previous approaches, ATFMN exploits the potential of ensemble learning for FSR
    instead of focusing on a single model.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.5\. Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here we discuss the pros and cons among these sub-categories in general FSR
    methods. From a global perspective, the difference between CNN-based and GAN-based
    methods relies on adversarial learning. CNN-based methods tend to utilize pixel-wise
    loss, leading to higher PSNR and smoother results, while GAN-based methods might
    recover visually pleasing face images with more details but lower PSNR. Each of
    them has its own merits. Compared with them, ensemble learning-based method can
    combine their advantages to make up their deficiencies by integrating multiple
    models. However, ensemble learning inevitably results in the increase of memory,
    computation and parameters. Reinforcement learning-based methods recover the attentional
    local regions by sequentially searching, and consider the contextual dependency
    of patches from a global perspective, which brings improvement of performance
    but needs much more training time and computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/43492a3ffd0f576609289c5bf2abea63.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Framework of pre-prior methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/17873604e5799e6b84b85fe88e923f1f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Framework of parallel-prior methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2631825854efc7a6a422886dbd016867.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Framework of in-prior methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a514b776a4087baec8120b0e8a2b24d4.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Framework of post-prior methods.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4\. Four frameworks of prior-guided FSR methods. PEN is prior estimation
    network, SRN is super-resolution network, FEN is a feature extraction network,
    and P is prior information.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Prior-guided FSR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: General FSR methods aim to design efficient networks. Nevertheless, as a highly
    structured object, human face has some specific characteristics, such as prior
    information (including facial landmarks, facial parsing maps, and facial heatmaps),
    which are ignored by general FSR methods. Therefore, to recover facial images
    with a much clearer facial structure, researchers begin to develop prior-guided
    FSR methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7616b0f754e60f0b976f39601851e5a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Milestones of prior-guided FSR methods. We simply list their names
    and venues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prior-guided FSR methods refer to extracting facial prior information and utilizing
    it to facilitate face reconstruction. Considering the order of prior information
    extraction and FSR, we further divide the prior-guided FSR methods into four parts:
    i) pre-prior methods that extract prior information followed by FSR; ii) parallel-prior
    methods that extract prior information and FSR simultaneously; iii) in-prior methods
    that extract prior information from the intermediate results or features at the
    middle stag, and iv) post-prior methods that extract prior information from FSR
    results. We illustrate the main frameworks of the four categories in Fig. [4](#S4.F4
    "Figure 4 ‣ 4.1.5\. Discussion ‣ 4.1\. General FSR ‣ 4\. FSR Methods ‣ Deep Learning-based
    Face Super-Resolution: A Survey"), outline the development of prior-guided FSR
    methods in Fig. [5](#S4.F5 "Figure 5 ‣ 4.2\. Prior-guided FSR ‣ 4\. FSR Methods
    ‣ Deep Learning-based Face Super-Resolution: A Survey") and compare them on several
    key features in Table [3](#S4.T3 "Table 3 ‣ 4.2.1\. Pre-prior Methods ‣ 4.2\.
    Prior-guided FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1\. Pre-prior Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'These methods first extract face structure prior information and then feed
    the prior information to the beginning of FSR model. That is, they always extract
    prior information from LR face images by an extraction network which can be a
    pretrained network or a subnetwork associated with the FSR model, then take advantage
    of the prior information to facilitate FSR. To extract the accurate face structure
    prior, prior-based loss is always used in these methods to train their prior extraction
    network, which is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (15) |  | $\mathcal{L}_{\text{Prior}}=\left\&#124;P_{\text{HR}}-P\right\&#124;_{F},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $P_{\text{HR}}$ is the ground truth prior, $P$ is extracted prior from
    the super-resolved face image, $F$ can be 1 or 2, and the prior can be heatmap,
    landmark and parsing maps in different methods.
  prefs: []
  type: TYPE_NORMAL
- en: In the early years, both LCGE [[115](#bib.bib115)] and MNCEFH [[116](#bib.bib116)]
    extract landmarks from LR face images to crop the faces into different components,
    and then predict high-frequency details for different components. However, accurate
    landmarks are unavailable especially when LR face images are tiny (*i.e.*, 16$\times$16).
    Thus, researchers turn to facial parsing maps [[117](#bib.bib117), [118](#bib.bib118),
    [45](#bib.bib45), [119](#bib.bib119)]. PSFR-GAN [[117](#bib.bib117)], SeRNet [[118](#bib.bib118)]
    and CAGFace [[45](#bib.bib45)] all pretrain a face structure prior extraction
    network to extract facial parsing maps. Then all of them except SeRNet directly
    concatenate the prior and LR face images as the input of the super-resolution
    model while SeRNet designs its improved residual block (IRB) to fuse the prior
    and features from LR face images. In addition, PSFR-GAN designs a semantic aware
    style loss to calculate the gram matrix loss for each semantic region separately.
    Later on, super-resolution guided by 3D facial priors (FSRG3DFP) [[120](#bib.bib120)]
    estimates 3D priors instead of 2D priors to learn 3D facial details and capture
    facial component information by the spatial feature transform block (SFT).
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Comparison of prior-guided FSR methods. To be short, we use *Pre*,
    *Parallel*, *In*, and *Post* to denote different prior-guided methods.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Methods | Prior | Extraction | Fusion Strategies |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Pre | LCGE [[115](#bib.bib115)] | Landmark | Pretrained | Crop |'
  prefs: []
  type: TYPE_TB
- en: '|  | MNCEFH [[116](#bib.bib116)] | Landmark | Pretrained | Crop |'
  prefs: []
  type: TYPE_TB
- en: '|  | PSFR-GAN [[117](#bib.bib117)] | Parsing map | Pretrained | Concatenation
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | CAGFace [[45](#bib.bib45)] | Parsing map | Pretrained | Concatenation
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | FSRG3DFP [[120](#bib.bib120)] | 3D prior | Joint | SFT |'
  prefs: []
  type: TYPE_TB
- en: '|  | SeRNet [[118](#bib.bib118)] | Parsing map | Pretrained | IRB |'
  prefs: []
  type: TYPE_TB
- en: '| Parallel | CBN [[121](#bib.bib121)] | Dense correspondence field | Joint
    | Concatenation |'
  prefs: []
  type: TYPE_TB
- en: '|  | KPEFH [[122](#bib.bib122)] | Parsing map | Joint | $\mathcal{L}_{\text{Parsing}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | JASRNet [[123](#bib.bib123)] | Heatmap | Joint | $\mathcal{L}_{\text{Heatmap}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | ATSENet [[124](#bib.bib124)] | Facial boundary heatmap | Joint | FFU |'
  prefs: []
  type: TYPE_TB
- en: '| In | FSRNet [[35](#bib.bib35)] | Landmark, parsing map, heatmap | Joint |
    Concatenation |'
  prefs: []
  type: TYPE_TB
- en: '|  | FSRGFCH [[125](#bib.bib125)] | Heatmap | Joint | Concatenation |'
  prefs: []
  type: TYPE_TB
- en: '|  | DIC [[36](#bib.bib36)] | Heatmap | Joint | $\mathcal{L}_{\text{Heatmap}}$,
    AFM |'
  prefs: []
  type: TYPE_TB
- en: '| Post | Super-FAN [[126](#bib.bib126)] | Heatmap | Joint | $\mathcal{L}_{\text{Heatmap}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | PFSRNet [[127](#bib.bib127)] | Heatmap | Pretrained | $\mathcal{L}_{\text{Heatmap}}$,
    $\mathcal{L}_{\text{Attention}}$ |'
  prefs: []
  type: TYPE_TB
- en: 4.2.2\. Parallel-prior Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The above methods ignore the correlation between face structure prior estimation
    and FSR task: face prior estimation benefits from the enhancement of FSR and vice
    versa. Thus, parallel-prior methods that perform prior estimation and super-resolution
    in parallel are proposed, including cascaded bi-network (CBN) [[121](#bib.bib121)],
    KPEFH [[122](#bib.bib122)], JASRNet [[123](#bib.bib123)], SAAN [[128](#bib.bib128)],
    HaPFSR [[129](#bib.bib129)], OBC-FSR [[130](#bib.bib130)] and ATSENet [[124](#bib.bib124)].
    They train the prior estimation and super-resolution networks jointly and require
    ground truth prior to calculate prior-based loss like Eq. ([15](#S4.E15 "In 4.2.1\.
    Pre-prior Methods ‣ 4.2\. Prior-guided FSR ‣ 4\. FSR Methods ‣ Deep Learning-based
    Face Super-Resolution: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most representative parallel-prior methods is JASRNet. Specifically,
    JASRNet utilizes a shared encoder to extract features for super-resolution and
    prior estimation simultaneously. Through this design, the shared encoder can extract
    the most expressive information for both tasks. In contrast to JASRNet, ATSENet
    not only extracts shared features for the two tasks, but also feeds features from
    the prior estimation branch into the feature fusion unit (FFU) in the super-resolution
    branch.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3\. In-prior Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pre- and parallel-prior methods directly extract structure prior information
    from LR face images. Due to the low-quality of LR face images, extracting accurate
    prior information is challenging. To reduce the difficulty and improve the accuracy
    of prior estimation, researchers first coarsely recover LR face images and then
    extract prior information from the enhanced results of LR face images, including
    FSRNet [[35](#bib.bib35)], FSR guided by facial component heatmaps (FSRGFCH) [[125](#bib.bib125)],
    HCFR [[131](#bib.bib131)], deep-iterative-collaboration (DIC) [[36](#bib.bib36)]
    and [[132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136)]. Similar to parallel-prior methods, in-prior methods always
    jointly optimize the networks for two tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, FSRNet [[35](#bib.bib35)], FSRGFCH [[125](#bib.bib125)] and HCFR [[131](#bib.bib131)]
    first upsample the LR face images to obtain intermediate results, then extract
    face structure prior from the intermediate results, and finally make use of the
    prior and intermediate results to recover the final results. FSRNet and FSRGFCH
    concatenate the intermediate results and the prior and feed the concatenated results
    into the following network to recover final SR results while HCFR utilizes the
    prior to segment the intermediate results and recovers final SR results by random
    forests. Considering that FSR and prior extraction should facilitate each other,
    DIC [[36](#bib.bib36)] proposes to iteratively perform super-resolution and prior
    extraction tasks. In the first iteration, DIC recovers a face $I_{\text{SR}^{\text{1}}}$
    with super-resolution model and extracts prior (heatmaps) $P_{\text{1}}$ from
    $I_{\text{SR}^{\text{1}}}$. In the $i$-th iteration, both the LR face image and
    $P_{i-1}$ are fed into the super-resolution model to obtain $I_{\text{SR}^{i}}$,
    and then $P_{i}$ can be extracted. In this way, the two tasks can promote each
    other. Moreover, DIC builds an attention fusion module (AFM) to fuse facial prior
    and the LR face image efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4\. Post-prior Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In contrast to the above methods, post-prior methods extract the face structure
    prior from SR result rather than LR face image or intermediate result, and utilize
    the prior to design loss functions, including Super-FAN [[126](#bib.bib126)],
    progressive FSR network (PFSRNet) [[127](#bib.bib127)], and [[137](#bib.bib137)].
    Super-FAN [[126](#bib.bib126)] and PFSRNet [[127](#bib.bib127)] first super-resolve
    LR face images and obtain SR results, and then develops a prior estimation network
    to extract the heatmaps of SR face images and HR ones, and constrains the heatmaps
    of SR face images and HR ones to be close. PFSRNet further generates multi-scale
    super-resolved results and applies prior-based loss at every scale. In addition,
    PFSRNet utilizes heatmaps to generate a mask and calculates facial attention loss
    $\mathcal{L}_{\text{Attention}}$ based on the masked SR and HR face images. Compared
    with the above methods, post-prior methods do not require prior extraction during
    the inference.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.5\. Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All prior-guided FSR methods need the ground truth of the face structure prior
    to calculate loss in the training phase. During the testing phase, all prior-guided
    FSR methods except post-prior methods need to estimate the prior. Due to the loss
    of information caused by image degradation, LR face images increase the difficulty
    and limit the accuracy of prior extraction in pre-prior methods, further limiting
    the super-resolution performance. Although parallel-prior methods can facilitate
    prior extraction and super-resolution simultaneously by sharing feature extraction,
    the improvement is still limited. In-prior methods extract prior from the intermediate
    result, which can improve the performance but increase the memory and computation
    cost caused by iterative super-resolution procedure especially in the iterative
    method (DIC [[36](#bib.bib36)]). In post-prior methods, the prior only plays the
    role of the supervisor during training, while not participating in inference,
    and they cannot make full use of the specific prior of the input LR face image.
    Thus, a method that can exploit the prior fully without increasing additional
    memory or computation cost is on demand.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Attribute-constrained FSR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Facial attribute is also usually exploited in FSR, and they are called attribute-constrained
    FSR. As a kind of semantic information, facial attribute provides semantic knowledge,
    *e.g.*, whether people wear glasses, which is useful for FSR. In the following,
    we will introduce some attribute-constrained FSR methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1592a5b13d72e72a043a673fb6fef8ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Milestones of attribute-constrained FSR methods. Their names and
    venues are listed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different from face structure prior information of which acquisition relies
    on the image itself, attribute information can be available without LR face images,
    such as in criminal cases where attribute information may not be clear in LR face
    images but accurately known by witnesses. Thus, some researchers construct networks
    on the condition that attribute information is given, while others relax this
    by estimating attributes. According to this concept, attribute-constrained FSR
    methods can be divided into two frameworks: given attribute methods and estimated
    attribute methods. The overview is provided in Fig. [6](#S4.F6 "Figure 6 ‣ 4.3\.
    Attribute-constrained FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey") and Table [4](#S4.T4 "Table 4 ‣ 4.3.1\. Given Attribute Methods ‣ 4.3\.
    Attribute-constrained FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1\. Given Attribute Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given the attribute information, how to integrate it into the super-resolution
    model is the key. For this problem, attribute-guided conditional CycleGAN (AGCycleGAN) [[138](#bib.bib138)],
    FSR with supplementary attributes (FSRSA) [[139](#bib.bib139)], expansive FSR
    with supplementary attributes (EFSRSA), attribute transfer network (ATNet) [[140](#bib.bib140)]
    and ATSENet [[124](#bib.bib124)] all directly concatenate attribute information
    and LR face image (or features extracted from LR face image). AGCycleGAN and FSRSA
    also feed the attribute into their discriminators to force the super-resolution
    model to notice the attribute information and develop attribute-based loss to
    achieve attribute matching, which is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (16) |  | $\mathcal{L}_{\text{Attribute}_{D}}=-\log D(I_{\text{HR}},A)-\log\left(1-D(I_{\text{SR}},A)\right)-\log\left(1-D(I_{\text{HR}},\tilde{A})\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $A$ is attribute matched with $I_{\text{HR}}$ while $\tilde{A}$ is the
    mismatched one. ATSENet feeds the super-resolved result into an attribute analysis
    network to calculate attribute prediction loss,
  prefs: []
  type: TYPE_NORMAL
- en: '| (17) |  | $\mathcal{L}_{\text{Attribute}}=\left\&#124;A_{\text{P}}-A_{\text{HR}}\right\&#124;_{F},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $A_{\text{P}}$ is the predicted attribute of the network and $A_{\text{HR}}$
    is the ground truth attribute. However, Lee *et al*. [[141](#bib.bib141)] hold
    that LR face image and attributes belong to different domains, and direct concatenation
    is unsuitable and may decrease the performance. With regard to this view, Lee
    *et al*. construct an attribute augmented convolutional neural network (AACNN) [[141](#bib.bib141)],
    which extracts features from the attribute to boost face super-resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. Comparison of attribute-constrained FSR methods. ”NG” denotes that
    the information is not given.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Methods | #Attribute | Attribute embedding methods |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Given | FSRSA [[139](#bib.bib139)] | 18 | Concatenation and $\mathcal{L}_{\text{Attribute}_{D}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | EFSRSA [[142](#bib.bib142)] | 18 | Concatenation and $\mathcal{L}_{\text{Attribute}_{D}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | AGCycleGAN [[138](#bib.bib138)] | 18 | Concatenation and $\mathcal{L}_{\text{Attribute}_{D}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | AACNN [[141](#bib.bib141)] | 38 | Concatenation |'
  prefs: []
  type: TYPE_TB
- en: '|  | ATNet [[140](#bib.bib140)] | NG | Concatenation and $\mathcal{L}_{\text{Attribute}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | ATSENet [[124](#bib.bib124)] | NG | Concatenation and $\mathcal{L}_{\text{Attribute}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Estimated | RAAN [[143](#bib.bib143)] | NG | Attribute channel attention
    and $\mathcal{L}_{\text{Attribute}}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | FACN [[144](#bib.bib144)] | 18 | Attribute attention mask and $\mathcal{L}_{\text{Attribute}}$
    |'
  prefs: []
  type: TYPE_TB
- en: 4.3.2\. Estimated Attribute Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The above-mentioned given attribute methods work on the condition that all
    attributes are given, making them limited in real-world scenes where some attributes
    are missing. Although the missed attributes can be set as unknown, such as 0 or
    random values, the performance may drop sharply. To this end, researchers build
    modules to estimate attribute information for FSR. In estimated attribute methods,
    attribute-based loss forces the network to predict attribute information correctly,
    which is similar to Eq. ([17](#S4.E17 "In 4.3.1\. Given Attribute Methods ‣ 4.3\.
    Attribute-constrained FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey")). Estimated attribute methods include residual attribute attention
    network (RAAN) [[143](#bib.bib143)] and facial attribute capsule network (FACN) [[144](#bib.bib144)].
    RAAN is based on cascaded residual attribute attention blocks (RAAB). RAAB builds
    three branches to generate shape, texture, and attribute information, respectively,
    and introduces two attribute channel attention applied to shape and texture information.
    In contrast, FACN [[144](#bib.bib144)] integrates attributes in capsules. Specifically,
    FACN encodes LR face image into encoded features, and the features are fed into
    a capsule generation block that produces semantic capsules, probabilistic capsules,
    and facial attributes. Then, the attribute is viewed as a kind of mask to refine
    other features by multiplication or summation. With the combination of three information
    as input, the decoder of FACN can well recover the final SR results.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3\. Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given attribute methods require attribute information, making them only applicable
    in some restricted scenes. Although the attribute can be set as unknown in these
    methods, the performance may drop sharply. Towards the estimated attribute methods,
    they need to estimate the attribute and then utilize the attribute. Compared with
    given attribute methods, they have a wider range of applications but the accuracy
    of attribute estimation is difficult to guarantee in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Identity-preserving FSR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Compared with face structure prior and attribute information, identity information
    containing identity-aware details is essential and identity-preserving FSR methods
    have received an increasing amount of attention in recent years. They aim to maintain
    the identity consistency between SR face image and LR one and improve the performance
    of down-stream face recognition. We show the overview and comparison of some representative
    methods in Fig. [7](#S4.F7 "Figure 7 ‣ 4.4\. Identity-preserving FSR ‣ 4\. FSR
    Methods ‣ Deep Learning-based Face Super-Resolution: A Survey") and Table [5](#S4.T5
    "Table 5 ‣ 4.4.1\. Face Recognition-based Methods: ‣ 4.4\. Identity-preserving
    FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a8dfbc19ff6d6ac701be84cf8629f90c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. Milestones of identity-preserving FSR methods. Their names and venues
    are listed.
  prefs: []
  type: TYPE_NORMAL
- en: '4.4.1\. Face Recognition-based Methods:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To maintain identity consistency between $I_{\text{SR}}$ and $I_{\text{HR}}$,
    in the training phase, a commonly used design is utilizing face recognition network
    to define identity loss, *e.g.*, super-identity convolutional neural network (SICNN)
    [[145](#bib.bib145)], face hallucination generative adversarial network (FH-GAN) [[146](#bib.bib146)],
    WaSRGAN [[147](#bib.bib147)],  [[148](#bib.bib148)], identity preserving face
    hallucination (IPFH) [[149](#bib.bib149)], cascaded super-resolution and identity
    priors (C-SRIP) [[150](#bib.bib150)], [[151](#bib.bib151), [152](#bib.bib152),
    [153](#bib.bib153), [154](#bib.bib154)] and ATSENet [[124](#bib.bib124)]. The
    framework of these methods consists of two main components: a super-resolution
    model, and a pretrained face recognition network (FRN), probably an additional
    discriminator. The super-resolution model super-resolves the input LR face image,
    generating $I_{\text{SR}}$ which is fed into FRN to obtain its identity features.
    Simultaneously, $I_{\text{HR}}$ is also fed into FRN, obtaining its identity features.
    The identity loss is calculated by'
  prefs: []
  type: TYPE_NORMAL
- en: '| (18) |  | $\mathcal{L}_{\text{Identity}}=\left\&#124;\text{FR}(I_{\text{HR}})-\text{FR}(I_{\text{SR}})\right\&#124;_{F},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where FR is the function of FRN. $F$ is 1 in WaSRGAN [[147](#bib.bib147)] and
    2 in FH-GAN [[146](#bib.bib146)] and [[151](#bib.bib151)]. Some methods calculate
    the loss on normalized features [[145](#bib.bib145), [155](#bib.bib155)], and
    some use A-softmax loss [[156](#bib.bib156), [149](#bib.bib149)]. Rather than
    directly extracting identity features from $I_{\text{SR}}$ and $I_{\text{HR}}$,
    C-SRIP [[150](#bib.bib150)] feeds residual maps between $I_{\text{HR}}$ (or $I_{\text{SR}}$)
    and $I_{\text{LR}}^{\uparrow_{s}}$ (upsampled by bicubic interpolation), respectively,
    into FRN, and applies cross-entropy loss on them. Moreover, C-SRIP generates multi-scale
    face images which are fed into different scale face recognition networks.
  prefs: []
  type: TYPE_NORMAL
- en: To fully explore the identity prior, SPGAN [[99](#bib.bib99)] feeds identity
    information extracted by the pretrained FRN to the discriminator at different
    scales, and designs attention-based identity loss. Firstly, SPGAN generates two
    attention maps $M_{\text{G}}$ and $M_{\text{D}}$,
  prefs: []
  type: TYPE_NORMAL
- en: '| (19) |  | $E=\mathcal{D}(I_{\text{LR}},I_{\text{HR}})-\mathcal{D}(I_{\text{LR}},I_{\text{SR}}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (20) |  | $M_{\text{D}}=-\min\left(0,E-\left\&#124;I_{\text{HR}}-I_{\text{SR}}\right\&#124;_{2}\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (21) |  | $M_{\text{G}}=\alpha*E+b,$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $E$ denotes the difference, $*$ denotes the element-wise multiplication,
    $b$ is identity matrix, and $\alpha$ is a 0-1 matrix. At $i$-th row and $j$-th
    column, $\alpha_{i,j}$ is 0 when $E_{i,j}$ is negative, otherwise $\alpha_{i,j}$
    is 1\. Then two attention maps are applied to the identity loss $\mathcal{L}_{\text{Identity}}$,
  prefs: []
  type: TYPE_NORMAL
- en: '| (22) |  | $\mathcal{L}_{\text{Identity}_{\text{SP}}}=\mathcal{L}_{\text{Identity}}*M_{\text{G}}+\mathcal{L}_{\text{Identity}}*M_{\text{D}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}_{\text{Identity}_{\text{SP}}}$ is the identity loss of SPGAN.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5\. Comparison of identity-preserving FSR methods. Notably, $I_{\text{RH}}$
    ($I_{\text{RS}}$) is the residual map between $I_{\text{HR}}$ ($I_{\text{SR}}$)
    and $I_{\text{LR}}^{\uparrow_{s}}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Methods | Loss Functions |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Face Recognition-based | SICNN [[145](#bib.bib145)] | MSE loss on normalized
    $\text{FR}(I_{\text{SR}})$ and $\text{FR}(I_{\text{HR}})$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | FH-GAN [[146](#bib.bib146)] | MSE loss on $\text{FR}(I_{\text{SR}})$ and
    $\text{FR}(I_{\text{HR}})$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | WaSRGAN [[147](#bib.bib147)] | $\mathcal{L}_{1}$ loss on $\text{FR}(I_{\text{SR}})$
    and $\text{FR}(I_{\text{HR}})$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | C-SRIP [[150](#bib.bib150)] | Cross entropy loss on $\text{FR}(I_{\text{RS}})$
    and $\text{FR}(I_{\text{RH}})$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | IPFH [[149](#bib.bib149)] | A-softmax loss on $\text{FR}(I_{\text{SR}})$
    and $\text{FR}(I_{\text{HR}})$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | SPGAN [[99](#bib.bib99)] | Attention-based loss $\mathcal{L}_{\text{Identity}_{\text{SP}}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pairwise Data-based | SiGAN [[157](#bib.bib157)] | Pair contrastive loss
    $\mathcal{L}_{\text{Contrastive}}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | IADFH [[158](#bib.bib158)] | Adversarial face veri?cation loss $\mathcal{L}_{\text{AFVL}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '4.4.2\. Pairwise Data-based Methods:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The training of FRN needs well-labeled datasets. However, a large well-labeled
    dataset is very costly. One solution is based only on the weakly-labeled datasets.
    In consideration of this, siamese generative adversarial network (SiGAN) [[157](#bib.bib157)]
    takes advantage of the weak pairwise label (in which different LR face images
    correspond to different identities) to achieve identity preservation. Specifically,
    SiGAN has twin GANs ($G_{1}$ and $G_{2}$) that share the same architecture but
    super-resolve different LR face images ($I_{\text{LR}}^{1}$ and $I_{\text{LR}}^{2}$)
    at the same time. As the identities of different LR face images are different,
    the identities of SR results corresponding to LR face images are also varied.
    Based on this observation, SiGAN designs an identity-preserving contrastive loss
    that minimizes the difference between same-identity pairs and maximizes the difference
    between different-identity pairs,
  prefs: []
  type: TYPE_NORMAL
- en: '| (23) |  | $\mathcal{L}_{\text{Contrastive}}=(1-y)\frac{1}{2}\left[\text{max}(0,0.5-E_{\text{w}})\right]^{2}+y\frac{1}{2}(E_{\text{w}})^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (24) |  | $E_{\text{w}}=\left\&#124;F_{\text{E}}(I_{\text{LR}}^{\text{1}}),F_{\text{E}}(I_{\text{LR}}^{\text{2}})\right\&#124;_{1},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $F_{\text{E}}$ is a function used to extract features from the intermediate
    layers of the generators, $E_{\text{w}}$ measures the distance between the features
    of $I_{\text{LR}}^{1}$ and $I_{\text{LR}}^{2}$, $y$ is 1 when two LR face images
    belong to the same identity, and $y$ is 0 when LR face images belong to different
    identities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of feeding the pair data into twin generators, identity-aware deep
    face hallucination (IADFH) [[158](#bib.bib158)] feeds pair data into the discriminator.
    Its discriminator is a three-way classifier that generates fake, genuine and imposter:
    i) HR and SR face images with the same or different identities ($\text{pair}_{\text{1}}$
    or $\text{pair}_{\text{2}}$) correspond to the fake, which forces the discriminator
    to distinguish $I_{\text{HR}}$ and $I_{\text{SR}}$; ii) two different HR face
    images of the same identity ($\text{pair}_{\text{3}}$) correspond to the genuine;
    iii) two HR face images with different identities ($\text{pair}_{\text{4}}$) correspond
    to the imposter. The last two pairs force the discriminator to capture the identity
    feature. In this pattern, the generator can incorporate the identity information.
    The loss is called adversarial face veri?cation loss (AFVL),'
  prefs: []
  type: TYPE_NORMAL
- en: '| (25) |  | $\mathcal{L}_{\text{AFVL(D)}}=\log d_{f}(\text{pair}_{1})+\log
    d_{f}(\text{pair}_{2})\\ +\log d_{\text{gen}}(\text{pair}_{3})+\log d_{\text{imp}}(\text{pair}_{4}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (26) |  | $\mathcal{L}_{\text{AFVL(G)}}=\log d_{\text{gen}}(\text{pair}_{1})+\log
    d_{\text{imp}}(\text{pair}_{2}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}_{\text{AFVL(D)}}$ ($\mathcal{L}_{\text{AFVL(G)}}$) is the
    loss function of the discriminator (generator), and $d_{\text{f}},d_{\text{gen}},d_{\text{imp}}$
    (can be -1, 1, 0) are the outputs of the discriminator for fake, genuine and imposter
    pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3\. Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Face recognition-based methods design identity loss based on face recognition
    network which is always pretrained. The training of a face recognition network
    requires well-labeled datasets which are costly. Instead, pairwise data-based
    methods take advantage of the contrast between different identities and the similarity
    between the same identity to maintain identity consistency without well-labeled
    datasets, which has a wider range of applications.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5\. Reference FSR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The FSR networks discussed all exploit only the input LR face itself. In some
    conditions, we may obtain the high-quality face image of the same identity of
    the LR face image, for example, the person of the LR face image may have other
    high-quality face images. These high-quality face images can provide identity-aware
    face details for FSR. Thus, reference FSR methods utilize high-quality face image(s)
    as reference (R) to boost face restoration. Obviously, the reference face image
    can be only one image or multiple images. According to the number of R, a guided
    framework can be partitioned into single-face guided, multi-face guided, and dictionary-guided
    methods. An overview of reference FSR methods is shown in Fig. [8](#S4.F8 "Figure
    8 ‣ 4.5\. Reference FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey") and the comparison of them is shown in Table [6](#S4.T6 "Table 6 ‣
    4.5.1\. Single-face Guided Methods ‣ 4.5\. Reference FSR ‣ 4\. FSR Methods ‣ Deep
    Learning-based Face Super-Resolution: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb498b7d215a31300c2c21cec593b8fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8\. Milestones of reference FSR methods. We simply list their names and
    venues.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1\. Single-face Guided Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At first, a high-quality face image which shares the same identity with the
    LR face image serves as R, such as guided face restoration network (GFRNet) [[39](#bib.bib39)],
    GWAInet [[159](#bib.bib159)]. Since the reference face image and LR face image
    may have different poses and expressions, which may hinder the recovery of face
    images, single-face guided methods tend to perform the alignment between the reference
    face image and the LR face image. After alignment, both the LR face image and
    aligned reference face image (we name it $I_{\text{w}}$) are fed into a reconstruction
    network to recover the SR result. The differences between GFRNet and GWAInet include
    two aspects: i) GFRNet employs landmarks while GWAInet employs flow field to carry
    out the alignment; ii) in the reconstruction network, GFRNet directly concatenates
    the LR face image and $I_{\text{w}}$ as the input. Nevertheless, GWAInet builds
    a GFENet to extract features from $I_{w}$ and transferring useful features of
    $I_{w}$ to the reconstruction network to recover SR results.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6\. Comparison of reference FSR methods. ”-” denotes that the method does
    not contain the procedure.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Methods | Same identity | Alignment | Utilization of R |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Single-face guided | GFRNet [[39](#bib.bib39)] | ✓ | Landmark | Concatenation
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | GWAInet [[159](#bib.bib159)] | ✓ | Flow field | GFENet |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-face guided | ASFFNet [[37](#bib.bib37)] | ✓ | Moving least-square
    | AFFB |'
  prefs: []
  type: TYPE_TB
- en: '|  | MEFSR [[160](#bib.bib160)] | ✓ | - | PWAve |'
  prefs: []
  type: TYPE_TB
- en: '| Dictionary-guided | JSRFC [[161](#bib.bib161)] | $\times$ | Landmark | Concatenation
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | DFDNet [[38](#bib.bib38)] | $\times$ | - | DFT |'
  prefs: []
  type: TYPE_TB
- en: 4.5.2\. Multi-face Guided Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Single-face guided methods set the problem as an LR face image only has one
    high-quality reference face image, but in some applications many high-quality
    face images are available, and they can further provide more complementary information
    for FSR. Adaptive spatial feature fusion network (ASFFNet) [[37](#bib.bib37)]
    is the first to explore multi-face guided FSR. Given multiple reference images,
    ASFFNet first selects the best reference image which should have the most similar
    pose and expression with LR face image by guidance selection module. However,
    misalignment and illumination differences still exist in the reference face image
    and the LR face image. Thus, ASFFNet applies weighted least-square alignment [[162](#bib.bib162)]
    and AdaIN [[163](#bib.bib163)] to cope with these two problems. Finally, they
    design an adaptive feature fusion block (AFFB) to generate an attention mask that
    is used to complement the information from LR face image and R. Multiple exemplar
    FSR (MEFSR) [[160](#bib.bib160)] directly feed all reference faces into weighted
    pixel average (PWAve) module to extract information for face restoration.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.3\. Dictionary-guided Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is observed that different people may have similar facial components. According
    to this observation, dictionary-guided methods are proposed, including joint super-resolution
    and face composite (JSRFC) [[161](#bib.bib161)] and deep face dictionary network
    (DFDNet) [[38](#bib.bib38)]. Dictionary-guided methods do not require the identity
    consistency between the reference face image and the LR face image, but build
    a component dictionary to boost face restoration. For example, JSRFC selects reference
    images which have similar components with the LR face image (every reference face
    image is labeled with a vector to indicate which components are similar.). Then,
    it aligns LR face image with the reference face image and extracts the corresponding
    components as a component dictionary. Finally, the dictionary components are used
    for the following face restoration. Different from JSRFC, Li *et al*. [[38](#bib.bib38)]
    build multi-scale component dictionaries based on features of the entire dataset.
    They use pretrained VGGFace [[67](#bib.bib67)] to extract features in different
    scales from high-quality faces, and then crop and resample four components with
    landmarks, and then cluster obtain K classes for every component by K-means. Given
    component dictionaries, they first select the most similar atoms for every component
    by the inner product, and then transfer the features from dictionary to the LR
    face image by dictionary feature transfer (DFT).
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.4\. Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Single-face and multi-face guided FSR methods require one or multiple additional
    high-quality face image(s) with the same identity as the LR face image, which
    facilitates face restoration but limits their application since the reference
    image may not exist. In addition, the alignment between low-quality LR face image
    and high-quality reference face image is also challenging in the reference FSR.
    Dictionary-guided methods break the restriction of the same identity, broadening
    the application but increasing the difficulty of face reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6\. Experiments and Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To have a clear view of deep learning-based FSR methods, we compare the PSNR,
    SSIM and LPIPS performance of the state-of-the-art algorithms on commonly used
    benchmark datasets (including CelebA [[55](#bib.bib55)], VGGFace2 [[67](#bib.bib67)]
    and CASIA-WebFace [[69](#bib.bib69)]) with upscale $\times$4, $\times$8 and $\times$16\.
    Considering that the reference FSR methods are different from other FSR methods,
    we compare other FSR methods and reference FSR methods individually.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.1\. Comparison Results of FSR Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We first introduce the experimental settings and analyze the results of FSR
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experimental Setting: For CelebA [[55](#bib.bib55)] dataset, 168,854 images
    are used for training and 1,000 images for testing following DIC [[36](#bib.bib36)].
    All the images are cropped and resized into 128$\times$128 as $I_{\text{HR}}$.
    We apply the degradation model in Eq. ([4](#S2.E4 "In 2.1\. Problem Definition
    ‣ 2\. Background ‣ Deep Learning-based Face Super-Resolution: A Survey")) to generate
    $I_{\text{LR}}$. Facial landmarks are detected by [[164](#bib.bib164), [165](#bib.bib165),
    [166](#bib.bib166)] and heatmaps are generated according to the landmarks. For
    facial parsing map, we adopt pretrained BiSeNet [[167](#bib.bib167)] to extract
    the parsing map from $I_{\text{HR}}$. For quality evaluation, PSNR and SSIM are
    introduced and both of them are computed on the Y channel of YCbCr space, which
    also follows DIC [[36](#bib.bib36)]. In addition, we further introduce the LPIPS
    to evaluate the performance of all comparison approaches. For the optimizer and
    learning rate when retraining different methods, we follow the setting in their
    original papers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Experimental Results: We list and compare the results of some representative
    FSR methods in Table [7](#S4.T7 "Table 7 ‣ 4.6.1\. Comparison Results of FSR Methods
    ‣ 4.6\. Experiments and Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based Face
    Super-Resolution: A Survey"), including four general image super-resolution methods:
    super-resolution using deep convolutional networks (SRCNN) [[70](#bib.bib70)],
    VDSR [[85](#bib.bib85)], residual channel attention network (RCAN) [[168](#bib.bib168)],
    non-local sparse network (NLSN) [[169](#bib.bib169)], three general FSR methods:
    URDGN [[34](#bib.bib34)], WaSRNet [[82](#bib.bib82)], SPARNet [[78](#bib.bib78)],
    three prior-guided FSR methods: FSRNet [[35](#bib.bib35)], Super-FAN [[126](#bib.bib126)],
    DIC [[36](#bib.bib36)], two attribute-constrained FSR methods: FSRSA [[142](#bib.bib142)],
    AACNN [[141](#bib.bib141)], and three identity-preserving FSR methods: SICNN [[145](#bib.bib145)],
    SiGAN [[157](#bib.bib157)], and WaSRGAN [[147](#bib.bib147)]. Except that, we
    also report the parameters and FLOPs of these methods in the last two columns
    of Table [7](#S4.T7 "Table 7 ‣ 4.6.1\. Comparison Results of FSR Methods ‣ 4.6\.
    Experiments and Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey"). Note that the parameter and FLOPs are associated with the model with
    upscale $\times$8\. In addition, we also present the visual comparisons between
    a few state-of-the-art algorithms in Fig.[9](#S4.F9 "Figure 9 ‣ 4.6.1\. Comparison
    Results of FSR Methods ‣ 4.6\. Experiments and Analysis ‣ 4\. FSR Methods ‣ Deep
    Learning-based Face Super-Resolution: A Survey"), Fig.[10](#S4.F10 "Figure 10
    ‣ 4.6.1\. Comparison Results of FSR Methods ‣ 4.6\. Experiments and Analysis ‣
    4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A Survey") and Fig.[11](#S4.F11
    "Figure 11 ‣ 4.6.1\. Comparison Results of FSR Methods ‣ 4.6\. Experiments and
    Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'From these objective metrics and visual comparison results, we have the following
    observations:'
  prefs: []
  type: TYPE_NORMAL
- en: (i) The retrained state-of-the-art (SOTA) general image super-resolution methods,
    such as RCAN and NLSN, are very competitive and even outperform the best FSR methods
    in terms of PSNR and SSIM. Meanwhile, as a general FSR method, SPARNet obtains
    the best performance among all the FSR methods. RCAN, NLSN, and SPARNet all do
    not explicitly incorporate the prior knowledge of face image, but they have obtained
    outstanding results. It shows that the design and optimization of the network
    is very important, and a well-designed network will have stronger fitting capabilities
    (less reconstruction errors). This observation will enlighten us that when we
    are designing a FSR deep network, it should be based on a strong backbone network.
  prefs: []
  type: TYPE_NORMAL
- en: '(ii) The terms of RCAN* and NLSN* in Table [7](#S4.T7 "Table 7 ‣ 4.6.1\. Comparison
    Results of FSR Methods ‣ 4.6\. Experiments and Analysis ‣ 4\. FSR Methods ‣ Deep
    Learning-based Face Super-Resolution: A Survey") represent the pretrained models
    on general training images, and we directly download these models from the authors’
    pages. Note that the pretrained results under certain magnification factors are
    not given (indicated as ‘-’ in the table) because these methods are not trained
    under these magnification factors. RCAN and NLSN achieve better performance than
    RCAN* and NLSN*. This demonstrates that models trained by general images are not
    suitable for FSR but general image super-resolution methods trained by face images
    may perform well (sometimes even better than FSR methods on face images). Therefore,
    if we want to know and compare the performance of a newly proposed general image
    super-resolution on the task of FSR, we cannot directly use the pretrained model
    released by the authors, but should retrain the model on the face image dataset.
    It should be noted that the objective results of these GAN-based FSR methods (*e.g.*,
    URDGN, FSRSA, SiGAN and WaSRGAN) are worse than those of NLSN*. This is mainly
    because that they often cannot get a better MSE due to the introduction of adversarial
    losses, which tend to allow the models to obtain perceptually better SR results
    but large reconstruction errors.'
  prefs: []
  type: TYPE_NORMAL
- en: (iii) Compared with general image super-resolution methods and general FSR methods,
    these methods that incorporate facial characteristics do not perform well in terms
    of PSNR and SSIM. Nevertheless, we cannot conclude that it is meaningless to develop
    FSR methods that use facial characteristics. This is mainly because PSNR and SSIM
    may be not good assessment metrics for the task of image super-resolution [[41](#bib.bib41)],
    let alone for the task of FSR, in which human perception will be more important.
    To further exploit the super-resolution reconstruction capacity, we also introduce
    another assessment metric, LPIPS, which is more in line with human judgement.
    From the LPIPS results, we learn that these methods with low PSNR and SSIM may
    produce very good performance in terms of LPIPS, please refer to Super-FAN and
    SiGAN. This indicates that these methods that introduce facial characteristics
    can well represent the face image and recover the face contours and discriminant
    details.
  prefs: []
  type: TYPE_NORMAL
- en: (iv) When we compare FSR methods that use different facial characteristics,
    such as face structure prior, attributes, and identity, it is difficult to say
    which type of characteristic is more effective for FSR. Because these methods
    often use different backbone networks, and it is difficult to determine whether
    their performance changes are caused by the difference in the backbone network
    itself or because of the introduction of different facial characteristics. In
    practice, we can first develop a strong backbone and then incorporate facial characteristics
    to boost FSR.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7\. Quantitative evaluation of various FSR methods on CelebA, in terms
    of PSNR, SSIM and LPIPS for $\times$4, $\times$8 and $\times$16\. The best, the
    second-best and the third-best results are emphasized with red, blue and underscore
    respectively. Note that Params and FLOPs are calculated for $\times$8 super-resolution
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | $\times$4 | $\times$8 | $\times$16 | Params | FLOPs |'
  prefs: []
  type: TYPE_TB
- en: '|  | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ | PSNR$\uparrow$ |
    SSIM$\uparrow$ | LPIPS$\downarrow$ | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$
    |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| General Image Super-Resolution Methods |'
  prefs: []
  type: TYPE_TB
- en: '| SRCNN [[70](#bib.bib70)] | 28.04 | 0.837 | 0.160 | 23.93 | 0.635 | 0.256
    | 20.54 | 0.467 | 0.291 | 0.01M | 0.3G |'
  prefs: []
  type: TYPE_TB
- en: '| VDSR [[85](#bib.bib85)] | 31.25 | 0.906 | 0.055 | 26.36 | 0.761 | 0.112 |
    22.42 | 0.594 | 0.186 | 0.6M | 11.0G |'
  prefs: []
  type: TYPE_TB
- en: '| RCAN [[168](#bib.bib168)] | 31.69 | 0.913 | 0.051 | 27.30 | 0.799 | 0.100
    | 23.32 | 0.641 | 0.204 | 15.0M | 4.7G |'
  prefs: []
  type: TYPE_TB
- en: '| RCAN* [[168](#bib.bib168)] | 26.30 | 0.769 | 0.177 | 22.17 | 0.521 | 0.265
    | - | - | - | 15.0M | 4.7G |'
  prefs: []
  type: TYPE_TB
- en: '| NLSN [[169](#bib.bib169)] | 32.08 | 0.919 | 0.044 | 27.45 | 0.804 | 0.091
    | 23.69 | 0.671 | 0.154 | 43.4M | 22.9G |'
  prefs: []
  type: TYPE_TB
- en: '| NLSN* [[169](#bib.bib169)] | 30.82 | 0.899 | 0.065 | - | - | - | - | - |
    - | 43.4M | 22.9G |'
  prefs: []
  type: TYPE_TB
- en: '| General FSR Methods |'
  prefs: []
  type: TYPE_TB
- en: '| URDGN [[34](#bib.bib34)] | 30.11 | 0.884 | 0.075 | 25.62 | 0.726 | 0.148
    | 22.29 | 0.579 | 0.185 | 1.0M | 14.6G |'
  prefs: []
  type: TYPE_TB
- en: '| WaSRNet [[82](#bib.bib82)] | 30.92 | 0.908 | 0.051 | 26.83 | 0.787 | 0.089
    | 23.13 | 0.634 | 0.160 | 71.5M | 19.2G |'
  prefs: []
  type: TYPE_TB
- en: '| SPARNet [[78](#bib.bib78)] | 31.71 | 0.913 | 0.048 | 27.44 | 0.804 | 0.089
    | 23.68 | 0.674 | 0.139 | 10.0M | 7.2G |'
  prefs: []
  type: TYPE_TB
- en: '| Prior-guided FSR Methods |'
  prefs: []
  type: TYPE_TB
- en: '| FSRNet [[35](#bib.bib35)] | 31.46 | 0.908 | 0.052 | 26.66 | 0.771 | 0.110
    | 23.04 | 0.629 | 0.175 | 3.1M | 39.0G |'
  prefs: []
  type: TYPE_TB
- en: '| Super-FAN [[126](#bib.bib126)] | 31.17 | 0.905 | 0.040 | 27.08 | 0.788 |
    0.058 | 23.42 | 0.652 | 0.125 | 1.3M | 1.1G |'
  prefs: []
  type: TYPE_TB
- en: '| DIC [[36](#bib.bib36)] | 31.44 | 0.909 | 0.053 | 27.41 | 0.802 | 0.092 |
    23.47 | 0.657 | 0.160 | 20.8M | 14.8G |'
  prefs: []
  type: TYPE_TB
- en: '| Attribute-constrained FSR Methods |'
  prefs: []
  type: TYPE_TB
- en: '| FSRSA [[142](#bib.bib142)] | 30.80 | 0.898 | 0.058 | 26.19 | 0.757 | 0.111
    | 22.84 | 0.630 | 0.153 | 76.9M | 0.9G |'
  prefs: []
  type: TYPE_TB
- en: '| AACNN [[141](#bib.bib141)] | 31.30 | 0.907 | 0.052 | 26.68 | 0.773 | 0.100
    | 22.98 | 0.626 | 0.171 | 3.3M | 0.2G |'
  prefs: []
  type: TYPE_TB
- en: '| Identity-preserving FSR Methods |'
  prefs: []
  type: TYPE_TB
- en: '| SICNN [[145](#bib.bib145)] | 31.59 | 0.911 | 0.050 | 27.18 | 0.793 | 0.095
    | 23.50 | 0.662 | 0.152 | 4.9M | 5.4G |'
  prefs: []
  type: TYPE_TB
- en: '| SiGAN [[157](#bib.bib157)] | 30.68 | 0.892 | 0.034 | 25.63 | 0.740 | 0.062
    | 22.18 | 0.596 | 0.099 | 19.5M | 5.7G |'
  prefs: []
  type: TYPE_TB
- en: '| WaSRGAN [[147](#bib.bib147)] | 30.72 | 0.907 | 0.045 | 25.55 | 0.765 | 0.092
    | 22.78 | 0.625 | 0.148 | 71.5M | 19.2G |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/d715c86058c0d434d22517981bc3234c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9\. Qualitative comparison of different FSR approaches for $\times$4
    super-resolution reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/62334897cdd32103280f99262d900873.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10\. Qualitative comparison of different FSR approaches for $\times$8
    super-resolution reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2fc65fa725a110f0eccb37f3d1dbe89e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11\. Qualitative comparison of different FSR approaches for $\times$16
    super-resolution reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.2\. Comparison Results of Reference FSR Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The above FSR methods only require LR face images as input, while the reference
    FSR methods require LR face images and reference images. It is unfair to directly
    compare with these methods that do not use auxiliary high-resolution face images.
    Therefore, we compare the performance of the reference FSR methods individually.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experimental Setting: Following ASFFNet [[37](#bib.bib37)], VGGFace2 [[67](#bib.bib67)]
    is reorganized into 106,000 groups and every group has 3-10 high-quality face
    images of the same identity, in which 10,000 groups are used for training set,
    4,000 groups are for validation set and the remaining are testing set. In addition,
    two testing sets based on CelebA [[55](#bib.bib55)] and CASIA-WebFace [[69](#bib.bib69)]
    are also used, and each set contains 2,000 groups with 3-10 high-quality face
    images. We utilize facial landmarks to crop and resize all images into 256$\times$256
    as high-quality face images. To generate $I_{\text{LR}}$, the degradation model
    Eq. ([5](#S2.E5 "In 2.1\. Problem Definition ‣ 2\. Background ‣ Deep Learning-based
    Face Super-Resolution: A Survey")), where $J$ and $\downarrow$ are embodied as
    JPEG compression with quality $q$ and bicubic interpolation respectively, is applied
    to the high-quality images. We consider two types of blur kernels, *i.e.*, Gaussian
    blur and motion blur kernels, and randomly sample the scale $s$ from {1:0.1:8},
    the noise level from {0:1:15}, and the compression quality factor $q$ from {10
    : 1 : 60} [[37](#bib.bib37)]. PSNR, SSIM and LPIPS [[41](#bib.bib41)] are used
    as metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Experimental Results: The experimental results are shown in Table [8](#S4.T8
    "Table 8 ‣ 4.6.2\. Comparison Results of Reference FSR Methods ‣ 4.6\. Experiments
    and Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A
    Survey"). To be specific, we list the results of GFRNet [[39](#bib.bib39)], GWAInet [[159](#bib.bib159)]
    and the latest proposed ASFFNet [[37](#bib.bib37)] on CelebA [[55](#bib.bib55)],
    VGGFace2 [[67](#bib.bib67)] and CASIA-WebFace [[69](#bib.bib69)] with upscale
    $\times$8\. Note that all the results are copied from the paper [[37](#bib.bib37)]
    since we have difficulty in reproducing these methods. Note that GFRNet and GWAInet
    are single-face guided methods while ASFFNet is multi-face guided method. To be
    fair, the reference image of GFRNet and GWAInet is the same as the selected image
    in ASFFNet. From Table [8](#S4.T8 "Table 8 ‣ 4.6.2\. Comparison Results of Reference
    FSR Methods ‣ 4.6\. Experiments and Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based
    Face Super-Resolution: A Survey"), it is obvious that multi-face guided method
    ASFFNet performs better than single-face guided methods (GWAInet and GFRNet).
    ASFFNet considers the illumination difference between the reference face image
    and the LR face image, which is ignored by GFRNet and GWAInet, and builds a well-designed
    AFFB instead of simple concatenation to adaptively the features of the reference
    face image and the LR face image. These two points contribute to the excellent
    performance of ASFFNet. Thus, difference (*i.e.*, misalignment, illumination difference,
    *etc*.) elimination and effective information fusion of the reference face image
    and the LR face image are both important in reference FSR methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8\. Quantitative evaluation of various reference FSR methods on CelebA [[55](#bib.bib55)],
    VGGFace2 [[67](#bib.bib67)] and CASIA-WebFace [[69](#bib.bib69)], in terms of
    PSNR, SSIM and LPIPS for $\times$8\. The best, the second-best and the third-best
    results are emphasized with red, blue and underscore respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | CelebA [[55](#bib.bib55)] | VGGFace2 [[67](#bib.bib67)] | CASIA-WebFace [[69](#bib.bib69)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ | PSNR$\uparrow$ |
    SSIM$\uparrow$ | LPIPS$\downarrow$ | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GFRNet [[39](#bib.bib39)] | 25.93 | 0.901 | 0.227 | 23.85 | 0.879 | 0.263
    | 27.19 | 0.912 | 0.307 |'
  prefs: []
  type: TYPE_TB
- en: '| GWAInet [[159](#bib.bib159)] | 25.77 | 0.901 | 0.210 | 23.87 | 0.879 | 0.261
    | 27.18 | 0.910 | 0.250 |'
  prefs: []
  type: TYPE_TB
- en: '| ASFFNet [[37](#bib.bib37)] | 26.39 | 0.905 | 0.185 | 24.34 | 0.881 | 0.238
    | 27.69 | 0.921 | 0.219 |'
  prefs: []
  type: TYPE_TB
- en: 4.7\. Joint FSR and Other Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although the above FSR methods have achieved a breakthrough, FSR is still challenging
    and complex since the input face images are often affected by many factors, including
    shadow, occlusion, blur, abnormal illumination, *etc*. To recover these face images
    efficiently, some work is proposed to consider degradation caused by low-quality
    and other factors together. Moreover, researchers also jointly perform FSR and
    other tasks. In the following, we will review these joint FSR and other tasks
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4.7.1\. Joint Face Completion and Super-Resolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Both low-resolution and occlusion or shadowing always coexist in the real-world
    face images. Thus, the restoration of faces degraded by these two factors is important.
    The simplest way is to first complete the occluded part and then super-resolve
    the completed LR face images [[170](#bib.bib170)]. However, the results always
    contain large artifacts due to the accumulation of errors. Cai *et al*. [[171](#bib.bib171)]
    propose the FCSR-GAN method which pretrains a face completion model (FCM), and
    combines FCM with super-resolution model (SRM), then trains SRM with the fixed
    FCM, and finally finetunes the whole network. Then, Liu *et al*. [[172](#bib.bib172)]
    propose a graph convolution pyramid blocks, which only needs one step to be trained
    rather than multiple steps of FCSR-GAN. In contrast, Pro-UIGAN [[173](#bib.bib173)]
    utilizes facial landmark to capture facial geometric prior and recovers occluded
    LR face images progressively.
  prefs: []
  type: TYPE_NORMAL
- en: 4.7.2\. Joint Face Deblurring and Super-Resolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Blurry LR face images always arise in real surveillance and sports videos, which
    cannot be recovered effectively by a single task model, *e.g.*, super-resolution
    or deblurring model. In the literature, Yu *et al*. [[174](#bib.bib174)] develop
    SCGAN to deblur and super-resolve the input jointly. Then, Song *et al*. [[175](#bib.bib175)]
    find that the previous methods ignore the utilization of facial prior information
    and the recovered face image are lack of high-frequency details. Thus, they first
    utilize a parsing map and LR face image to recover a basic result, and then feed
    the basic result into detail enhancement module to compensate high-frequency details
    from the high-quality exemplar. Later on, DGFAN [[176](#bib.bib176)] develops
    two feature extraction modules for different tasks to extract features, and imports
    them into well-designed gated fusion modules to generate deblurred high-quality
    results. Xu *et al*. [[177](#bib.bib177)] incorporate face recognition network
    with face restoration to improve the identi?ability of the recovered face images.
  prefs: []
  type: TYPE_NORMAL
- en: 4.7.3\. Joint Illumination Compensation and FSR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Abnormal illumination FSR has also attracted the attention of many scholars.
    SeLENet [[178](#bib.bib178)] decomposes a face image into a normal face, an albedo
    map and a lighting coef?cient, then replaces the lighting coef?cient with the
    standard ambient white light coef?cient, and then reconstructs the corresponding
    neutral light face image. Ding *et al*. [[179](#bib.bib179)] build a pipeline
    of face detection, and then recover the detected faces with landmarks. Zhang *et
    al*. [[180](#bib.bib180)] utilize a normal illumination external HR guidance to
    guide abnormal illumination LR face images for illumination compensation. They
    develop a copy-and-paste GAN (CPGAN), including an internal copy-and-paste network
    to utilize face intern information for reconstruction, and an external copy-and-paste
    network is applied to compensate illumination. Based on CPGAN, they further improve
    the external copy-and-paste network by introducing recursive learning and incorporating
    landmark estimation and develop the recursive CPGAN [[181](#bib.bib181)]. In contrast,
    Yasarla *et al*. [[182](#bib.bib182)] introduce network architecture search into
    face enhancement to design efficient network and extract identity information
    from HR guidance to restore face images.
  prefs: []
  type: TYPE_NORMAL
- en: 4.7.4\. Joint Face Alignment and Super-Resolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The above FSR methods require the all the HR training sample to be aligned.
    Thus, the misalignment of the input LR face image to the training face images
    often leads to sharp performance decrease and artifacts. Therefore, a set of joint
    face alignment and super-resolution methods are developed. Yu *et al*. [[49](#bib.bib49)]
    insert multiple spatial transformer networks (STN) [[183](#bib.bib183)] into the
    generator to achieve face alignment, and develop TDN and MTDN [[184](#bib.bib184)].
    As LR face images can be noisy and unaligned, Yu *et al*. build the TDAE method [[185](#bib.bib185)].
    TDAE first upsamples and coarsely aligns LR face images to produce $I_{\text{CSR}}$,
    then downsamples $I_{\text{CSR}}$ and obtains $I_{\text{CLR}}$ to reduce noise,
    and then upsamples $I_{\text{CLR}}$ for the final reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: 4.7.5\. Joint Face Frontalization and Super-Resolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Faces in the real world have various poses, and some of them may not be frontal.
    When existing FSR methods are applied to non-frontal faces, the reconstruction
    performance drops sharply and has poor visual quality. Artifacts exist even when
    FSR and face frontalization are performed in sequence or inverse order. To alleviate
    this problem, the method in [[186](#bib.bib186)] first takes advantage of STN
    and CNN to coarsely frontalize and hallucinate the faces, and then designs a fine
    upsampling network for refining face details. Yu *et al*. [[187](#bib.bib187)]
    propose a transformative adversarial neural network for joint face frontalization
    and hallucination. The method builds a transformer network to encode non-frontal
    LR face images and frontal LR ones into the latent space and requires the non-frontal
    one to be close to the frontal one, and then the encoded latent representations
    are imported into the upsampling network to recover the final results. Tu *et
    al*. [[188](#bib.bib188)] first train face restoration network and face frontalization
    network separately, and then propose task-integrated training strategy to merge
    two networks into a unified network for face frontalization and super-resolution.
    Note that face alignment aims to generate SR face images with the same pose as
    HR ones while face frontalization is to recover frontal SR faces from non-frontal
    LR faces.
  prefs: []
  type: TYPE_NORMAL
- en: 4.8\. Related Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Except the above-mentioned FSR methods and joint methods, a large number of
    new methods related to FSR have emerged in recent years, including face video
    super-resolution, old photo restoration, audio-guided FSR, 3D FSR, *etc*., which
    are introduced in the following.
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.1\. Face Video Super-Resolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Faces usually appear in LR video sequences, such as surveillance. The correlation
    between frames can provide more complementary details, which benefit the face
    reconstruction. One direct solution is to fuse multi-frame information and exploit
    inter-frame dependency [[189](#bib.bib189)]. The approach of [[190](#bib.bib190)]
    employs a generator to generate the SR results for every frame, and a fusion module
    is applied to estimate the central frame. Considering that the aforementioned
    methods cannot model the complex temporal dependency, Xin *et al*. [[191](#bib.bib191)]
    propose a motion-adaptive feedback cell which captures inter-frame motion information
    and updates the current frames adaptively. In [[192](#bib.bib192)], based on the
    assumption that multiple super-resolved frames are crucial for the reconstruction
    of the subsequent frame, and thus it designs a recurrence strategy to make better
    use of inter-frame information. Inspired by the powerful transformer, the work
    of [[193](#bib.bib193)] develops the first pure transformer-based face video hallucination
    model. MDVDNet [[194](#bib.bib194)] incorporates multiple priors from the video,
    including speech, semantic elements and facial landmarks to enhance the capability
    of deep learning-based method.
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.2\. Old Photo Restoration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Restoration of old pictures is vital and difficult in the real world since the
    degradation is too complex to be stimulated. Naturally, one solution is to learn
    the mapping from a real LR face image (regarding real old images as real LR face
    images) to an artificial LR face images, and then apply the existing FSR methods
    to the generated artificial LR face image. BOPBL [[195](#bib.bib195)] proposes
    to transform images at latent space rather than image space. Specifically, BOPBL
    first encodes real and artificial LR face images into the same latent space $S_{1}$,
    and encodes HR face images into another latent space $S_{2}$, and then maps $S_{1}$
    into $S_{2}$ by a mapping network.
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.3\. Audio-guided FSR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Considering that audio carries face-related information [[196](#bib.bib196)],
    Meishvili *et al*. [[197](#bib.bib197)] develop the first audio-guided FSR method.
    Due to the difference of multi-modal, they build two encoders to encode image
    and audio information. Then the encoded representations of images and the audio
    are fused, and the fused results are fed into the generator to recover the final
    SR results. The introduction of the audio in FSR is novel and inspires researchers
    to exploit cross modal information, but is challenging due to the differences
    between different modalities.
  prefs: []
  type: TYPE_NORMAL
- en: 4.8.4\. 3D FSR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Human face is the most concerned object in the field of computer vision. With
    the development of 2D technology, a large number of 3D methods are often proposed
    because they can provide more useful features for face reconstruction and recognition.
    In the FSR society, the early 3D FSR approach is proposed by Pan *et al.* [[198](#bib.bib198)].
    In [[199](#bib.bib199)], Berretti *et al.* propose a superface model from a sequence
    of low-resolution 3D scans. The approach of [[200](#bib.bib200)] takes only the
    rough, noisy, and low-resolution depth image as input, and predicts the corresponding
    high-quality 3D face mesh. By establishing the correspondence between the input
    LR face and 3D textures, Qu *et al.* present a patch-based 3D FSR on the mesh
    [[201](#bib.bib201)]. Benefiting from the development of deep learning technology,
    most recently, a 3D face point cloud super-resolution network approach is developed
    to infer the high-resolution data from low-resolution 3D face point cloud data [[202](#bib.bib202)].
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusion and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this review, we have presented a taxonomy of deep learning-based FSR methods.
    According to facial characteristics, this field can be divided into five categories:
    general FSR methods, prior-guided FSR methods, attribute-constrained FSR methods,
    identity-preserving FSR methods, and reference FSR methods. Then, every category
    is further divided into some subcategories depending on the design of the network
    architecture or the specific utilization of facial characteristics. In particular,
    general FSR methods are further divided into basic CNN-based methods, GAN-based
    methods, reinforcement learning-based methods, and ensemble learning-based methods.
    Besides, other methods combining facial characteristics are categorized according
    to the specific utilization pattern of facial characteristics. We also compare
    the performance of state-of-the-arts and give some deep analysis. Of course, FSR
    technique is not limited to the methods we presented, and a panoramic view of
    this fast-expanding field is rather challenging, thereby resulting in possible
    omissions. Therefore, this review serves as a pedagogical tool, providing researchers
    with insights into typical methods of FSR. In practice, researchers could use
    these general guidelines to develop the most suitable technique for their specific
    studies.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite great breakthroughs, FSR still presents many challenges and is expected
    to continue its rapid growth. In the following, we simply provide an outlook on
    the problems to be solved and trends to expect in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Design of Network. From the comparison results with the SOTA general image super-resolution
    methods, we learn that the backbone network has a crucial impact on the performance,
    especially in terms of PSNR and SSIM. Therefore, we can learn from the general
    image super-resolution task, in which many well-designed network structures have
    been continuously proposed (IPT [[203](#bib.bib203)] and SwinIR [[204](#bib.bib204)]),
    and design an effective deep network that is more suitable for FSR task. In addition
    to the effectiveness, an efficient network is also needed in practice, where the
    large model (with a mass of parameters and high computation costs) is very difficult
    to be deployed in real-world applications. Hence, developing models with lighter
    structure and lower computational taxing is still a major challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Exploitation of Facial Prior. As a domain-specific super-resolution technique,
    FSR can be used to recover the facial details which are lost in the observed LR
    face images. The key to the success of FSR is to effectively exploit the prior
    knowledge of human faces, from 1D vector (identity and attributes), to 2D images
    (facial landmarks, facial heatmaps and parsing maps), and to 3D models. Therefore,
    discovering new prior knowledge of human face, how to model or represent these
    prior knowledge, and how to integrate this information organically into the end-to-end
    training framework are worthy of further discussion. In addition to these explicit
    prior knowledge, how to model and utilize the implicit prior that is learned from
    the data (such as the GAN prior [[58](#bib.bib58), [106](#bib.bib106)]), may be
    another direction.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics and Loss Functions. As we know, the pixel-wise $\mathcal{L}_{1}$ loss
    or $\mathcal{L}_{2}$ loss tend to produce the super-resolution results with high
    PSNR and SSIM values, while perceptual loss and adversarial loss are in favor
    of letting the model produce some visually pleasant results, *i.e.*, good performance
    in terms of LPIPS and FID. Therefore, the assessment metric plays an important
    role in guiding the model optimization and affecting the final results. If we
    want to obtain a trustable result (in criminal investigation application), PSNR
    and SSIM may be better metrics. In contrast, if we just want some visually pleasant
    results, employing LPIPS and FID metrics may be a good choice. As a result, there
    is no universal assessment metric that can make the best of both worlds. Therefore,
    assessment metrics for FSR need more exploration in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminate FSR. In most situations, our goal is not only to reconstruct a
    visually pleasing HR face image. Actually, we hope that the super-resolved results
    can improve the face recognition task by human or computer. Therefore, it would
    be beneficial to recover a discriminated HR face image (for human) or discriminated
    feature (for computers) from an LR face image. To enhance the discriminant of
    super-resolved face images, we can use the weakly-supervised information (paired
    positive or negative samples) of the training sample to force the model to be
    able to reconstruct a discriminative face image.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world FSR. The degradation process in the real world is too complex to
    be simulated, which results in a large gap between the synthesized LR and HR pairs
    and real-world data. When applying models trained by synthesized pairs to real-world
    LR face images, their performance drops dramatically. Given the HR training face
    images and the unpaired real-world LR face images, some methods [[102](#bib.bib102),
    [205](#bib.bib205), [206](#bib.bib206)] have been proposed to learn the real image
    degradation to create the sample pairs of synthesis LR face images and HR face
    images. These methods achieve better performance than previous approaches trained
    with the data produced by bicubic degradation. These methods actually have a potential
    assumption that all real-world LR face images share the same degradation, *i.e.*,
    captured from the same camera. However, the obtained real-world LR face images
    are very different, and their degradation processes are different. Therefore,
    designing a more robust real-world FSR method is one of the problem has to be
    settled urgently.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modal FSR. Due to the rapid development of sensing technology, multiple
    sensors in the same system, such as autonomous driving and robots, are becoming
    more and more common. The utilization of multi-modal information (including audio,
    depth, near infrared) will be increasingly promoted. Evidently, different modalities
    provide different clues. In this field, researchers always explore image-related
    information, such as attribute, identity, and others. Nevertheless, the emergence
    of audio-guided FSR [[197](#bib.bib197)] and hyperspectral FSR [[207](#bib.bib207)]
    inspire us to take advantage of information belonging to different modalities.
    This trend will undoubtedly continue and diffuse into every category in this field.
    The introduction of multi-modal information will also spur the development of
    FSR.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Y. Liang, J. H. Lai, W. S. Zheng, and Z. Cai. A survey of face hallucination.
    In CCBR, pages 83–93, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] N. Wang, D. Tao, X. Gao, X. Li, and J. Li. A comprehensive survey to face
    hallucination. International Journal of Computer Vision, 106(1):9–30, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] M. P. Autee, M. S. Mehta, M. S. Desai, V. Sawant, and A. Nagare. A review
    of various approaches to face hallucination. Procedia Computer Science, 45:361–369,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] S. Kanakaraj, V. K. Govindan, and S. Kalady. Face super resolution: A survey.
    International Journal of Image, Graphics and Signal Processing, 9:54–67, 05 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] K. Nguyen, C. Fookes, S. Sridharan, M. Tistarelli, and M. Nixon. Super-resolution
    for biometrics: A comprehensive survey. Pattern Recognition, 78:23–42, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] S. S. Rajput, K. V. Arya, V. Singh, and V. K. Bohat. Face hallucination
    techniques: A survey. In IC-ICTES, pages 1–6, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] H. Liu, X. Zheng, J. Han, Y. Chu, and T. Tao. Survey on GAN-based face
    hallucination with its model development. IET Image Processing, 13(14):2662–2672,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] S. Baker and T. Kanade. Hallucinating faces. In FG, pages 83–88, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] C. Liu, H. Y. Shum, and C. S. Zhang. A two-step approach to hallucinating
    faces: global parametric model and local nonparametric model. In CVPR, volume 1,
    pages I–I, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] B. K. Gunturk, A. U. Batur, Y. Altunbasak, M. H. Hayes, and R. M. Mersereau.
    Eigenface-domain super-resolution for face recognition. IEEE Transactions on Image
    Processing, 12(5):597–606, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] X. Wang and X. Tang. Hallucinating face by eigentransformation. IEEE Transactions
    on Systems, Man, and Cybernetics, Part C, 35(3):425–434, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Chakrabarti, A. N. Rajagopalan, and R. Chellappa. Super-resolution
    of face images using kernel PCA-based prior. IEEE Transactions on Multimedia,
    9(4):888–892, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. Park and S. Lee. An example-based face hallucination method for single-frame,
    low-resolution facial images. IEEE Transactions on Image Processing, 17(10):1806–1816,
    2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] P. Innerhofer and T. PockInnerhofer. A convex approach for image hallucination.
    In AAPRW, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Y. Liang, X. Xie, and J. H. Lai. Face hallucination based on morphological
    component analysis. Signal Processing, 93(2):445–458, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] C. Y. Yang, S. Liu, and M. H. Yang. Structured face hallucination. In
    CVPR, pages 1099–1106, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] H. Chang, D. Y. Yeung, and Y. Xiong. Super-resolution through neighbor
    embedding. In CVPR, volume 1, pages I–I, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] X. Ma, J. Zhang, and C. Qi. Hallucinating face by position-patch. Pattern
    Recognition, 43(6):2224–2236, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] C. Jung, L. Jiao, B. Liu, and M. Gong. Position-patch based face hallucination
    using convex optimization. IEEE Signal Processing Letters, 18(6):367–370, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] J. Jiang, R. Hu, Z. Wang, and Z. Han. Noise robust face hallucination
    via locality-constrained representation. IEEE Transactions on Multimedia, 16(5):1268–1281,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] R. A. Farrugia and C. Guillemot. Face hallucination using linear models
    of coupled sparse support. IEEE Transactions on Image Processing, 26(9):4562–4577,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. Jiang, Y. Yu, S. Tang, J. Ma, A. Aizawa, and K. Aizawa. Context-patch
    face hallucination based on thresholding locality-constrained representation and
    reproducing learning. IEEE transactions on cybernetics, 50(1):324–337, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J. Shi, X. Liu, Y. Zong, C. Qi, and G. Zhao. Hallucinating face image
    by regularization models in high-resolution feature space. IEEE Transactions on
    Image Processing, 27(6):2980–2995, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] L. Chen, J. Pan, and Q. Li. Robust face image super-resolution via joint
    learning of subdivided contextual model. IEEE Transactions on Image Processing,
    28(12):5897–5909, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] J. Shi and G. Zhao. Face hallucination via coarse-to-fine recursive kernel
    regression structure. IEEE Transactions on Multimedia, 21(9):2223–2236, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] L. Liu, C. P. Chen, and S. Li. Hallucinating color face image by learning
    graph representation in quaternion space. IEEE transactions on cybernetics, pages
    1–13, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] L. Chen, J. Pan, J. Jiang, J. Zhang, Z. Han, and L. Bao. Multi-stage degradation
    homogenization for super-resolution of face images with extreme degradations.
    IEEE Transactions on Image Processing, 30:5600–5612, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Zhuang, J. Zhang, and F. Wu. Hallucinating faces: LPH super-resolution
    and neighbor reconstruction for residue compensation. Pattern Recognitionm, 40(11):3178–3194,
    2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] H. Huang, H. He, X. Fan, and J. Zhang. Super-resolution of human face
    image using canonical correlation analysis. Pattern Recognition, 43(7):2532–2543,
    2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Z. Wang, J. Chen, and S. C. H. Hoi. Deep learning for image super-resolution:
    A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S. Anwar, S. Khan, and N. Barnes. A deep journey into super-resolution:
    A survey. ACM Computing Surveys, 53(3):1–34, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] W. Yang, X. Zhang, Y. Tian, W. Wang, J. H. Xue, and Q. Liao. Deep learning
    for single image super-resolution: A brief review. IEEE Transactions on Multimedia,
    21(12):3106–3121, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] H. Liu, Z. Ruan, P. Zhao, F. Shang, L. Yang, and Y. Liu. Video super resolution
    based on deep learning: A comprehensive survey. arXiv preprint arXiv:2007.12928,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] X. Yu and F. Porikli. Ultra-resolving face images by discriminative generative
    networks. In ECCV, pages 318–333, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Chen, Y. Tai, X. Liu, C. Shen, and J. Yang. FSRNet: End-to-end learning
    face super-resolution with facial priors. In CVPR, pages 2492–2501, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] C. Ma, Z. Jiang, Y. Rao, J. Lu, and J. Zhou. Deep face super-resolution
    with iterative collaboration between attentive recovery and landmark estimation.
    In CVPR, pages 5569–5578, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] X. Li, W. Li, D. Ren, H. Zhang, M. Wang, and W. Zuo. Enhanced blind face
    restoration with multi-exemplar images and adaptive spatial feature fusion. In
    CVPR, pages 2706–2715, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] X. Li, C. Chen, S. Zhou, X. Lin, W. Zuo, and L. Zhang. Blind face restoration
    via deep multi-scale component dictionaries. In ECCV, pages 399–415, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] X. Li, M. Liu, Y. Ye, W. Zuo, L. Lin, and R. Yang. Learning warped guidance
    for blind face restoration. In ECCV, pages 272–289, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter.
    Gans trained by a two time-scale update rule converge to a local nash equilibrium.
    In NIPS, pages 6626–6637, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable
    effectiveness of deep features as a perceptual metric. In CVPR, pages 586–595,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] W. Zhou and A. C. Bovik. A universal image quality index. IEEE Signal
    Processing Letters, 9(3):81–84, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale structural similarity
    for image quality assessment. In ACSSC, volume 2, pages 1398–1402, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] A. Mittal, R. Soundararajan, and A. C. Bovik. Making a ¡°completely blind¡±
    image quality analyzer. IEEE Signal Processing Letters, 20(3):209–212, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] R. Kalarot, T. Li, and F. Porikli. Component attention guided face super-resolution
    network: CAGFace. In WACV, pages 359–369, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] W. S. Lai, J. B. Huang, N. Ahuja, and M. H. Yang. Deep laplacian pyramid
    networks for fast and accurate super-resolution. In CVPR, pages 624–632, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale
    image recognition. CoRR, abs/1409.1556, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, X. Bing, and Y. Bengio.
    Generative adversarial nets. In NIPS, volume 2, pages 2672–2680, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] X. Yu and F. Porikli. Face hallucination with tiny unaligned images by
    transformative discriminative neural networks. In AAAI, pages 4327–4333, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial
    networks. In PMLR, volume 70, pages 214–223, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved
    training of wasserstein GANs. In NIPS, pages 5767–5777, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] J. Y. Zhu, T. Park, P P. Isola, and A. A. Efros. Unpaired image-to-image
    translation using cycle-consistent adversarial networks. In ICCV, pages 2223–2232,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional
    neural networks. In CVPR, pages 2414–2423, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] T. C. Wang, M. Y. Liu, J. Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro.
    High-resolution image synthesis and semantic manipulation with conditional GANs.
    In CVPR, pages 8798–8807, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in
    the wild. In ICCV, pages 3730–3738, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] V. Le, J. Brandt, L. Zhe, L. D. Bourdev, and T. S. Huang. Interactive
    facial feature localization. In ECCV, pages 679–692, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] C. H. Lee, Z. Liu, L. Wu, and P. Luo. MaskGAN: Towards diverse and interactive
    facial image manipulation. In CVPR, pages 5549–5558, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] T. Karras, S. Laine, and T. Aila. A style-based generator architecture
    for generative adversarial networks. In CVPR, pages 4396–4405, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] M. Koestinger, P. Wohlhart, P. M. Roth, and H. Bischof. Annotated facial
    landmarks in the wild: A large-scale, real-world database for facial landmark
    localization. In ICCVW, pages 2144–2151, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic. A semi-automatic
    methodology for facial landmark annotation. In CVPRW, pages 896–903, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] A. Bulat and G. Tzimiropoulos. How far are we from solving the 2D & 3D
    face alignment problem? (and a dataset of 230,000 3D facial landmarks). In ICCV,
    pages 1021–1030, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] S. Zafeiriou, G. Trigeorgis, G. Chrysos, J. Deng, and J. Shen. The menpo
    facial landmark localisation challenge: A step towards the solution. In CVPRW,
    pages 170–179, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller. Labeled faces
    in the wild: A database for studying face recognition in unconstrained environments.
    Technical Report 07-49, University of Massachusetts, Amherst, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] L. Wolf, T. Hassner, and Y. Taigman. Effective unconstrained face recognition
    by combining multiple descriptors and learned background statistics. IEEE Transactions
    on Pattern Analysis and Machine Intelligence, 33(10):1978–1990, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In
    BMVC, pages 1–12, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] B. Chen, C. Chen, and W. H. Hsu. Face recognition and retrieval using
    cross-age reference coding with cross-age celebrity dataset. IEEE Transactions
    on Multimedia, 17(6):804–815, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. VGGFace2: A dataset
    for recognising faces across pose and age. In FG, pages 67–74, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] A. Bansal, A. Nanduri, C. Castillo, R. Ranjan, and R. Chellappa. UMDFaces:
    An annotated face dataset for training deep networks. CoRR, abs/1611.01484, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Y. Dong, L. Zhen, S. Liao, and S. Z. Li. Learning face representation
    from scratch. CoRR, abs/1411.7923, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] C. Dong, C. C. Loy, K. He, and X. Tang. Image super-resolution using deep
    convolutional networks. IEEE Transactions on Pattern Analysis and Machine Intelligence,
    38(2):295–307, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] E. Zhou, H. Fan, Z. Cao, Y. Jiang, and Q. Yin. Learning face hallucination
    in the wild. In AAAI, pages 3871–3877, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] W. Huang, Y. Chen, M. Li, and Y. Hui. Super-resolution reconstruction
    of face image based on convolution network. In AISC, pages 288–294, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] D. Huang and H. Liu. Face hallucination using convolutional neural network
    with iterative back projection. In CCBR, pages 167–175, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] X. Chen, X. Wang, Y. Lu, W. Li, Z. Wang, and Z. Huang. RBPNET: An asymptotic
    residual back-projection network for super-resolution of very low-resolution face
    image. Neurocomputing, 376:119–127, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] X. Chen and Y. Wu. Efficient face super-resolution based on separable
    convolution projection networks. In CRC, pages 92–97, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Y. Liu, Z. Dong, K. Pang Lim, and N. Ling. A densely connected face super-resolution
    network based on attention mechanism. In ICIEA, pages 148–152, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] V. Chudasama, K. Nighania, K. Upla, K. Raja, R. Ramachandra, and C. Busch.
    E-ComSupResNet: Enhanced face super-resolution through compact network. IEEE Transactions
    on Biometrics, Behavior, and Identity Science, 3(2):166–179, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] C. Chen, D. Gong, H. Wang, Z. Li, and K. Y. K. Wong. Learning spatial
    attention for face super-resolution. IEEE Transactions on Image Processing, 30:1219–1231,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] L. Han, H. Zhen, G. Jin, and D. Xin. A noise robust face hallucination
    framework via cascaded model of deep convolutional networks and manifold learning.
    In ICME, pages 1–6, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] H. Nie, Y. Lu, and J. Ikram. Face hallucination via convolution neural
    network. In ICTAI, pages 485–489, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Z. Chen, J. Lin, T. Zhou, and F. Wu. Sequential gating ensemble network
    for noise robust multiscale face restoration. IEEE Transactions on Cybernetics,
    pages 1–11, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] H. Huang, R. He, Z. Sun, and T. Tan. Wavelet-SRNet: A wavelet-based CNN
    for multi-scale face super resolution. In ICCV, pages 1689–1697, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Y. Liu, D. Sun, F. Wang, L. K. Pang, and Y. Lai. Learning wavelet coefficients
    for face super-resolution. The Visual Computer, (3):1613–1622, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] X. Hu, P. Ma, Z. Mai, S. Peng, Z. Yang, and L. Wang. Face hallucination
    from low quality images using definition-scalable inference. Pattern Recognition,
    94:110–121, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] J. Kim, J. K. Lee, and K. M. Lee. Accurate image super-resolution using
    very deep convolutional networks. In CVPR, pages 1646–1654, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] W. Ko and S. Chien. Patch-based face hallucination with multitask deep
    neural network. In ICME, pages 1–6, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Z. Feng, J. Lai, X. Xie, D. Yang, and M. Ling. Face hallucination by deep
    traversal network. In ICPR, pages 3276–3281, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] T. Lu, H. Wang, Z. Xiong, J. Jiang, Y. Zhang, H. Zhou, and Z. Wang. Face
    hallucination using region-based deep convolutional networks. In ICIP, pages 1657–1661,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] O. Tuzel, Y. Taguchi, and J. R. Hershey. Global-local face upsampling
    network. ArXiv, abs/1603.07235, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] T. Lu, J. Wang, J. Jiang, and Y. Zhang. Global-local fusion network for
    face super-resolution. Neurocomputing, 387:309–320, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] K. Jiang, Z. Wang, P. Yi, T. Lu, J. Jiang, and Z. Xiong. Dual-path deep
    fusion network for face image hallucination. IEEE Transactions on Neural Networks
    and Learning Systems, pages 1–14, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] S. Ko and B. R. Dai. Multi-laplacian GAN with edge enhancement for face
    super resolution. In ICPR, pages 3505–3512, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] L. Yang, P. Wang, Z. Gao, S. Wang, P. Ren, S. Ma, and W. Gao. Implicit
    subspace prior learning for dual-blind face restoration. arXiv preprint arXiv:2010.05508,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Y. Luo and K. Huang. Super-resolving tiny faces with face feature vectors.
    In ICIST, pages 145–152, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] S. D. Indradi, A. Arifianto, and K. N. Ramadhani. Face image super-resolution
    using inception residual network and GAN framework. In ICOICT, pages 1–6, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Z. Chen and Y. Tong. Face super-resolution through wasserstein GANs. ArXiv,
    abs/1705.02438, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] B. Huang, W. Chen, X. Wu, and C. L. Lin. High-quality face image SR using
    conditional generative adversarial networks. ARXiv, abs/1707.00737, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] H. Dou, C. Chen, X. Hu, Z. Xuan, Z. Hu, and S. Peng. PCA-SRGAN: Incremental
    orthogonal projection discrimination for face super-resolution. In ACM MM, pages
    1891–1899, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] M. Zhang and Q. Ling. Supervised pixel-wise GAN for face super-resolution.
    IEEE Transactions on Multimedia, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] K. Grm, M. Pernus, L. Cluzel, W. Scheirer, S. Dobrisek, and V. Struc.
    Face hallucination revisited: An exploratory study on dataset bias. In CVPRW,
    pages 2405–2413, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] A. Aakerberg, K. Nasrollahi, and T. B. Moeslund. Real-world super-resolution
    of face-images from surveillance cameras. arXiv preprint arXiv:2102.03113, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] A. Bulat, Y. Jing, and G. Tzimiropoulos. To learn image super-resolution,
    use a GAN to learn how to do image degradation first. In ECCV, pages 187–202,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] S. Goswami, Aakanksha, and A. N. Rajagopalan. Robust super-resolution
    of real faces using smooth features. In ECCV Workshop, pages I–I, 11 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] W. Zheng, L. Yan, W. Zhang, C. Gou, and F. Wang. Guided cyclegan via
    semi-dual optimal transport for photo-realistic face super-resolution. In ICIP,
    pages 2851–2855, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Z. Cheng, X. Zhu, and S. Gong. Characteristic regularisation for super-resolving
    face images. In WACV, pages 2424–2433, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila.
    Analyzing and improving the image quality of stylegan. In CVPR, pages 8107–8116,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of
    GANs for improved quality, stability, and variation. In ICLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Y. Choi, M. Choi, M. Kim, J. W. Ha, S. Kim, and J. Choo. Stargan: Unified
    generative adversarial networks for multi-domain image-to-image translation. In
    CVPR, pages 8789–8797, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] S. Menon, A. Damian, S. Hu, N. Ravi, and C. Rudin. PULSE: Self-supervised
    photo upsampling via latent space exploration of generative models. In CVPR, pages
    2223–2232, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] K. C. K. Chan, X. Wang, X. Xu, J. Gu, and C. C. Loy. GLEAN: Generative
    latent bank for large-factor image super-resolution. In CVPR, pages 14245–14254,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] X. Wang, Y. Li, H. Zhang, and Y. Shan. Towards real-world blind face
    restoration with generative facial prior. In CVPR, pages 9168–9178, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] T. Yang, P. Ren, X. Xie, and L. Zhang. GAN prior embedded network for
    blind face restoration in the wild. In CVPR, pages 672–681, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Y. Shi, G. Li, Q. Cao, K. Wang, and L. Lin. Face hallucination by attentive
    sequence optimization with reinforcement learning. IEEE Transactions on Pattern
    Analysis and Machine Intelligence, 42(11):2809–2824, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] K. Jiang, Z. Wang, P. Yi, G. Wang, K. Gu, and J. Jiang. ATMFN: Adaptive-threshold-based
    multi-model fusion network for compressed face hallucination. IEEE Transactions
    on Multimedia, 22(10):2734–2747, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Y. Song, J. Zhang, S. He, L. Bao, and Q. Yang. Learning to hallucinate
    face images via component generation and enhancement. In IJCAI, pages 4537–4543,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] J. Jiang, Y. Yu, J. Hu, S. Tang, and J. Ma. Deep CNN denoiser and multi-layer
    neighbor component embedding for face hallucination. In IJCAI, pages 771¨C–778,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] C. Chen, X. Li, L. Yang, X. Lin, and K. Wong. Progressive semantic-aware
    style transformation for blind face restoration. In CVPR, pages 11896–11905, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] X. Yu, L. Zhang, and W. Xie. Semantic-driven face hallucination based
    on residual network. IEEE Transactions on Biometrics, Behavior, and Identity Science,
    3(2):214–228, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] C. Wang, Z. Zhong, J. Jiang, D. Zhai, and X. Liu. Parsing map guided
    multi-scale attention network for face hallucination. In ICASSP, pages 2518–2522,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] X. Hu, W. Ren, J. Lamaster, X. Cao, X. Li, Z. Li, B. Menze, and W. Liu.
    Face super-resolution guided by 3D facial priors. In ECCV, pages 763–780, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] S. Zhu, S. Liu, C. L. Chen, and X. Tang. Deep cascaded bi-network for
    face hallucination. In ECCV, volume 9909, pages 614–630, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] K. Li, B. Bare, B. Yan, B. Feng, and C. Yao. Face hallucination based
    on key parts enhancement. In ICASSP, volume 30, pages 1378–1382, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Y. Yin, J. P. Robinson, Y. Zhang, and Y. Fu. Joint super-resolution and
    alignment of tiny faces. In AAAI, pages 2693–12700, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] M. Li, Z. Zhang, J. Yu, and C. W. Chen. Learning face image super-resolution
    through facial semantic attribute transformation and self-attentive structure
    enhancement. IEEE Transactions on Multimedia, 23:468–483, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] X. Yu, B. Fernando, B. Ghanem, F. Porikli, and R. Hartley. Face super-resolution
    guided by facial component heatmaps. In ECCV, pages 217–233, September 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] A. Bulat and G. Tzimiropoulos. Super-FAN: Integrated facial landmark
    localization and super-resolution of real-world low resolution faces in arbitrary
    poses with GANs. In CVPR, pages 109–117, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] D. Kim, M. Kim, G. Kwon, and D. S. Kim. Progressive face super-resolution
    via attention to facial landmark. In BMCV, pages I–I, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] T. Zhao and C. Zhang. Saan: Semantic attention adaptation network for
    face super-resolution. In ICME, pages 1–6, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] C. Wang, J. Jiang, and X. Liu. Heatmap-aware pyramid face hallucination.
    In ICME, pages 1–6, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] J. Li, B. Bare, S. Zhou, B. Yan, and K. Li. Organ-branched cnn for robust
    face super-resolution. In ICME, pages 1–6, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Z. S. Liu, W. C. Siu, and Y. L. Chan. Features guided face super-resolution
    via hybrid model of deep learning and random forests. IEEE Transactions on Image
    Processing, 30:4157–4170, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] M. Li, Y. Sun, Z. Zhang, and J. Yu. A coarse-to-fine face hallucination
    method by exploiting facial prior knowledge. In ICIP, pages 61–65, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Y. Zhang, Y. Wu, and L. Chen. MSFSR: A multi-stage face super-resolution
    with accurate facial representation via enhanced facial boundaries. In CVPRW,
    pages 2120–2129, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] H. Wang, Q. Hu, C. Wu, J. Chi, X. Yu, and H. Wu. DCLNet: Dual closed-loop
    networks for face super-resolution. Knowledge-Based Systems, 222:106987, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] S. Liu, C. Xiong, X. Shi, and Z. Gao. Progressive face super-resolution
    with cascaded recurrent convolutional network. Neurocomputing, 449:357–367, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] S. Liu, C. Xiong, and Z. Gao. Face super-resolution network with incremental
    enhancement of facial parsing information. In ICPR, pages 7537–7543, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] L. Li, J. Tang, Z. Ye, B. Sheng, L. Mao, and L. Ma. Unsupervised face
    super-resolution via gradient enhancement and semantic guidance. The Visual Computer,
    pages 1–13, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Y. Lu, Y. W. Tai, and C. K. Tang. Attribute-guided face generation using
    conditional CycleGAN. In ECCV, pages 282–297, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] X. Yu, B. Fernando, R. Hartley, and F. Porikli. Super-resolving very
    low-resolution face images with supplementary attributes. In CVPR, pages 908–917,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] M. Li, Y. Sun, Z. Zhang, H. Xie, and J. Yu. Deep learning face hallucination
    via attributes transfer and enhancement. In ICME, pages 604–609, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] C. H. Lee, K. Zhang, H. C. Lee, C. W. Cheng, and W. Hsu. Attribute augmented
    convolutional neural network for face hallucination. In CVPRW, pages 721–729,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] X. Yu, B. Fernando, R. Hartley, and F. Porikli. Semantic face hallucination:
    Super-resolving very low-resolution face images with supplementary attributes.
    IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(11):2926–2943,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] J. Xin, N. Wang, X. Gao, and J. Li. Residual attribute attention network
    for face image super-resolution. In AAAI, pages 9054–9061, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] J. Xin, N. Wang, X. Jiang, J. Li, X. Gao, and Z. Li. Facial attribute
    capsules for noise face super resolution. In AAAI, volume 34, pages 12476–12483,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] K. Zhang, Z. Zhang, C. W. Cheng, W. H. Hsu, Y. Qiao, W. Liu, and T. Zhang.
    Super-identity convolutional neural network for face hallucination. In ECCV, pages
    183–198, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] B. Bayramli, U. Ali, T. Qi, and H. Lu. FH-GAN: Face hallucination and
    recognition using generative adversarial network. In ICONIP, pages 3–15, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] H. Huang, R. He, Z. Sun, and T. Tan. Wavelet domain generative adversarial
    network for multi-scale face hallucination. International Journal of Computer
    Vision, 127(6-7):763–784, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] S. Lai, C. He, and K. Lam. Low-resolution face recognition based on identity-preserved
    face hallucination. In ICIP, pages 1173–1177, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] X. Cheng, J. Lu, B. Yuan, and J. Zhou. Identity-preserving face hallucination
    via deep reinforcement learning. IEEE Transactions on Circuits and Systems for
    Video Technology, pages 4796–4809, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] K. Grm, W. J. Scheirer, and V. Štruc. Face hallucination using cascaded
    super-resolution and identity priors. IEEE Transactions on Image Processing, 29:2150–2165,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] A. A. Abello and R. Hirata. Optimizing super resolution for face recognition.
    In SIBGRAPI, pages 194–201, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] F. Cheng, T. Lu, Y. Wang, and Y. Zhang. Face super-resolution through
    dual-identity constraint. In ICME, pages 1–6, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] J. Kim, G. Li, I. Yun, C. Jung, and J. Kim. Edge and identity preserving
    network for face super-resolution. Neurocomputing, 446:11–22, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] E. Ataer-Cansizoglu, M. Jones, Z. Zhang, and A. Sullivan. Verification
    of very low-resolution faces using an identity-preserving deep face super-resolution
    network. ArXiv, abs/1903.10974, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] J. Chen, J. Chen, Z. Wang, C. Liang, and C. W. Lin. Identity-aware face
    super-resolution for low-resolution face recognition. IEEE Signal Processing Letters,
    27:645–649, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song. SphereFace: Deep hypersphere
    embedding for face recognition. In CVPR, pages 212–220, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] C. Hsu, C. Lin, W. Su, and G. Cheung. SiGAN: Siamese generative adversarial
    network for identity-preserving face hallucination. IEEE Transactions on Image
    Processing, 28(12):6225–6236, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] H. Kazemi, F. Taherkhani, and N. M. Nasrabadi. Identity-aware deep face
    hallucination via adversarial face verification. In BTAS, pages 1–10, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] B. Dogan, S. Gu, and R. Timofte. Exemplar guided face image super-resolution
    without facial landmarks. In CVPRW, pages 1814–1823, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] K. Wang, J. Oramas, and T. Tuytelaars. Multiple exemplars-based hallucination
    for face super-resolution and editing. In ACCV, November 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] X. Li, G. Duan, Z. Wang, J. Ren, Y. Zhang, J. Zhang, and K. Song. Recovering
    extremely degraded faces by joint super-resolution and facial composite. In ICTAI,
    pages 524–530, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] S. Schaefer, T. Mcphail, and J. Warren. Image deformation using moving
    least squares. In ACM SIGGRAPH, page 533¨C540, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] X. Huang and S. Belongie. Arbitrary style transfer in real-time with
    adaptive instance normalization. In ICCV, pages 1501–1510, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] T. Baltrusaitis, P. Robinson, and L. P. Morency. Constrained local neural
    fields for robust facial landmark detection in the wild. In ICCVW, pages 354–361,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] T. Baltrusaitis, A. Zadeh, Y. C. Lim, and L. P. Morency. OpenFace 2.0:
    Facial behavior analysis toolkit. In FG, pages 59–66, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] A. Zadeh, C. L. Yao, T. Baltruaitis, and L. P. Morency. Convolutional
    experts constrained local model for 3D facial landmark detection. In ICCVW, pages
    2519–2528, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang. BiSeNet: Bilateral
    segmentation network for real-time semantic segmentation. In ECCV, pages 325–341,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu. Image super-resolution
    using very deep residual channel attention networks. In ECCV, pages 286–301, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Y. Mei, Y. Fan, and Y. Zhou. Image super-resolution with non-local sparse
    attention. In CVPR, pages 3517–3526, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] L. Yang, B. Shao, T. Sun, S. Ding, and X. Zhang. Hallucinating very low-resolution
    and obscured face images. CoRR, abs/1811.04645, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] J. Cai, H. Hu, S. Shan, and X. Chen. FCSR-GAN: Joint face completion
    and super-resolution via multi-task learning. IEEE Transactions on Biometrics,
    Behavior, and Identity Science, 2:109–121, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Z. Liu, Y. Wu, L. Li, C. Zhang, and B. Wu. Joint face completion and
    super-resolution using multi-scale feature relation learning. arXiv preprint arXiv:2003.00255,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Y. Zhang, X. Yu, X. Lu, and P. Liu. Pro-uigan: Progressive face hallucination
    from occluded thumbnails. arXiv preprint arXiv:2108.00602, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] X. Xu, D. Sun, J. Pan, Y. Zhang, H. Pfister, and M. H. Yang. Learning
    to super-resolve blurry face and text images. In ICCV, pages 251–260, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Y. Song, J. Zhang, L. Gong, S. He, L. Bao, J. Pan, Q. Yang, and M. H.
    Yang. Joint face hallucination and deblurring via structure generation and detail
    enhancement. International Journal of Computer Vision, 127(6-7):785–800, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] C. H. Yang and L. W. Chang. Deblurring and super-resolution using deep
    gated fusion attention networks for face images. In ICASSP, pages 1623–1627, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Y. Xu, H. Zou, Y. Huang, L. Jin, and H. Ling. Super-resolving blurry
    face images with identity preservation. Pattern Recognition Letters, 146:158–164,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] H. A. Le and I. A. Kakadiaris. SeLENet: A semi-supervised low light face
    enhancement method for mobile face unlock. In ICB, pages 1–8, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] X. Ding and R. Hu. Learning to see faces in the dark. In ICME, pages
    1–6, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Y. Zhang, T. Tsang, Y. Luo, C. Hu, X. Lu, and X. Yu. Copy and paste gan:
    Face hallucination from shaded thumbnails. In CVPR, pages 7355–7364, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Y. Zhang, I. Tsang, Y. Luo, C. Hu, X. Lu, and X. Yu. Recursive copy and
    paste gan: Face hallucination from shaded thumbnails. IEEE Transactions on Pattern
    Analysis and Machine Intelligence, pages 1–1, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] R. Yasarla, H. Joze, and V. M. Patel. Network architecture search for
    face enhancement. arXiv preprint arXiv:2105.06528, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu. Spatial
    transformer networks. In NIPS, pages 2017–2025, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] X. Yu, F. Porikli, B. Fernando, and R. Hartley. Hallucinating unaligned
    face images by multiscale transformative discriminative networks. International
    Journal of Computer Vision, 128(2):500–526, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] X. Yu and F. Porikli. Hallucinating very low-resolution unaligned and
    noisy face images by transformative discriminative autoencoders. In CVPR, pages
    3760–3768, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Y. Zhang, I. W. Tsang, J. Li, P. Liu, X. Lu, and X. Yu. Face hallucination
    with finishing touches. IEEE Transactions on Image Processing, 30:1728–1743, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] X. Yu, F. Shiri, B. Ghanem, and F. Porikli. Can we see more? joint frontalization
    and hallucination of unaligned tiny faces. IEEE Transactions on Pattern Analysis
    and Machine Intelligence, 42(9):2148–2164, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] X. Tu, J. Zhao, Q. Liu, W. Ai, G. Guo, Z. Li, W. Liu, and J. Feng. Joint
    face image restoration and frontalization for recognition. IEEE Transactions on
    Circuits and Systems for Video Technology, pages 1–1, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] D. Li and Z. Wang. Face video super-resolution with identity guided generative
    adversarial networks. In CCCV, pages 357–369, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] E. Ataer-Cansizoglu and M. Jones. Super-resolution of very low-resolution
    faces from videos. In BMCV, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] J. Xin, N. Wang, J. Li, X. Gao, and Z. Li. Video face super-resolution
    with motion-adaptive feedback cell. AAAI, 34(7):12468–12475, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] C. Fang, G. Li, X. Han, and Y. Yu. Self-enhanced convolutional network
    for facial video hallucination. IEEE Transactions on Image Processing, 29:3078–3090,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Y. Gan, Y. Luo, X. Yu, B. Zhang, and Y. Yang. Vidface: A full-transformer
    solver for video facehallucination with unaligned tiny snapshots. ArXiv, abs/2105.14954,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] X. Zhang and X. Wu. Multi-modality deep restoration of extremely compressed
    face videos. ArXiv, abs/2107.05548, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Z. Wan, B. Zhang, D. Chen, P. Zhang, D. Chen, J. Liao, and F. Wen. Bringing
    old photos back to life. In CVPR, pages 2747–2757, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] T. H. Oh, T. Dekel, C. Kim, I. Mosseri, W. T. Freeman, M. Rubinstein,
    and W. Matusik. Speech2face: Learning the face behind a voice. In CVPR, pages
    7539–7548, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] G. Meishvili, S. Jenni, and P. Favaro. Learning to have an ear for face
    super-resolution. In CVPR, pages 1364–1374, June 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] G. Pan, S. Han, Z. Wu, and Y. Wang. Super-resolution of 3D face. In ECCV,
    pages 389–401\. Springer, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] S. Berretti, A. Del Bimbo, and P. Pala. Superfaces: A super-resolution
    model for 3D faces. In ECCV, pages 73–82\. Springer, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] L. Shu, I. Kemelmacher-Shlizerman, and L. G. Shapiro. 3D face hallucination
    from a single depth frame. pages 31–38, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] C. Qu, C. Herrmann, E. Monari, T. Schuchert, and J. Beyerer. Robust 3D
    patch-based face hallucination. In WACV, pages 1105–1114\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] J. Li, F. Zhu, X. Yang, and Q. Zhao. 3D face point cloud super-resolution
    network. In IJCB, pages 1–8\. IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] H Chen, Y Wang, T Guo, C Xu, Y Deng, Z Liu, S Ma, C Xu, C Xu, and W Gao.
    Pre-trained image processing transformer. In CVPR, pages 12299–12310, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] J. Liang, J. Cao, G. Sun, K. Zhang, Van G.L., and R. Timofte. SwinIR:
    Image restoration using swin transformer. ArXiv, abs/2108.10257, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] S. Goswami and A. N. Rajagopalan. Robust super-resolution of real faces
    using smooth features. In ECCV, pages 169–185\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] A. Aakerberg, K. Nasrollahi, and T.B. Moeslund. Real-world super-resolution
    of face-images from surveillance cameras. ArXiv, abs/2102.03113, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] J. Jiang, C. Wang, X. Liu, and J. Ma. Spectral splitting and aggregation
    network for hyperspectral face super-resolution. ArXiv, abs/2108.13584, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
