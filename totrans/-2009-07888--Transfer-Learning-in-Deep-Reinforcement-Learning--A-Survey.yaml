- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:59:31'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2009.07888] Transfer Learning in Deep Reinforcement Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2009.07888](https://ar5iv.labs.arxiv.org/html/2009.07888)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Transfer Learning in Deep Reinforcement Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Zhuangdi Zhu,  Kaixiang Lin, Anil K. Jain, and Jiayu Zhou Zhuangdi Zhu, Anil
    K. Jain, and Jiayu Zhou are with the Department of Computer Science and Engineering,
    Michigan State University, East Lansing, MI, 48824. E-mail: {zhuzhuan, jain, jiayuz}@msu.edu
    Kaixiang Lin is with the Amazon Alexa AI. E-mail: lkxcarson@gmail.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reinforcement learning is a learning paradigm for solving sequential decision-making
    problems. Recent years have witnessed remarkable progress in reinforcement learning
    upon the fast development of deep neural networks. Along with the promising prospects
    of reinforcement learning in numerous domains such as robotics and game-playing,
    transfer learning has arisen to tackle various challenges faced by reinforcement
    learning, by transferring knowledge from external expertise to facilitate the
    efficiency and effectiveness of the learning process. In this survey, we systematically
    investigate the recent progress of transfer learning approaches in the context
    of deep reinforcement learning. Specifically, we provide a framework for categorizing
    the state-of-the-art transfer learning approaches, under which we analyze their
    goals, methodologies, compatible reinforcement learning backbones, and practical
    applications. We also draw connections between transfer learning and other relevant
    topics from the reinforcement learning perspective and explore their potential
    challenges that await future research progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Transfer Learning, Reinforcement Learning, Deep Learning, Survey.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement Learning (RL) is an effective framework to solve sequential decision-making
    tasks, where a learning agent interacts with the environment to improve its performance
    through trial and error [[1](#bib.bib1)]. Originated from cybernetics and thriving
    in computer science, RL has been widely applied to tackle challenging tasks which
    were previously intractable. Traditional RL algorithms were mostly designed for
    tabular cases, which provide principled solutions to simple tasks but face difficulties
    when handling highly complex domains, e.g. tasks with 3D environments. With the
    recent advances in deep learning research, the combination of RL and deep neural
    networks is developed to address challenging tasks. The combination of deep learning
    with RL is hence referred to as Deep Reinforcement Learning (DRL) [[2](#bib.bib2)],
    which learns powerful function approximators using deep neural networks to address
    complicated domains. DRL has achieved notable success in applications such as
    robotics control [[3](#bib.bib3), [4](#bib.bib4)] and game playing [[5](#bib.bib5)].
    It also thrives in domains such as health informatics [[6](#bib.bib6)], electricity
    networks [[7](#bib.bib7)], intelligent transportation systems[[8](#bib.bib8),
    [9](#bib.bib9)], to name just a few.
  prefs: []
  type: TYPE_NORMAL
- en: Besides its remarkable advancement, RL still faces intriguing difficulties induced
    by the exploration-exploitation dilemma [[1](#bib.bib1)]. Specifically, for practical
    RL problems, the environment dynamics are usually unknown, and the agent cannot
    exploit knowledge about the environment until enough interaction experiences are
    collected via exploration. Due to the partial observability, sparse feedbacks,
    and the high complexity of state and action spaces, acquiring sufficient interaction
    samples can be prohibitive or even incur safety concerns for domains such as automatic-driving
    and health informatics. The abovementioned challenges have motivated various efforts
    to improve the current RL procedure. As a result, *transfer learning* (TL), or
    equivalently referred as *knowledge transfer*, which is a technique to utilize
    external expertise to benefit the learning process of the target domain, becomes
    a crucial topic in RL.
  prefs: []
  type: TYPE_NORMAL
- en: While TL techniques have been extensively studied in supervised learning [[10](#bib.bib10)],
    it is still an emerging topic for RL. Transfer learning can be more complicated
    for RL, in that the knowledge needs to transfer in the context of a Markov Decision
    Process. Moreover, due to the delicate components of the Markov decision process,
    expert knowledge may take different forms that need to transfer in different ways.
    Noticing that previous efforts on summarizing TL in the RL domain did not cover
    research of the last decade [[11](#bib.bib11), [12](#bib.bib12)], during which
    time considerate TL breakthroughs have been achieved empowered with deep learning
    techniques. Hence, in this survey, we make a comprehensive investigation of the
    latest TL approaches in RL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The contributions of our survey are multifold: 1) we investigated up-to-date
    research involving new DRL backbones and TL algorithms over the recent decade.
    To the best of our knowledge, this survey is the first attempt to survey TL approaches
    in the context of *deep* reinforcement learning. We reviewed TL methods that can
    tackle more evolved RL tasks, and also studied new TL schemes that are not deeply
    discussed by prior literatures, such as representation disentanglement (Sec [5.5](#S5.SS5
    "5.5 Representation Transfer ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer
    Learning in Deep Reinforcement Learning: A Survey")) and policy distillation (Sec
    [5.3](#S5.SS3 "5.3 Policy Transfer ‣ 5 Transfer Learning Approaches Deep Dive
    ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")). 2) We provided
    systematic categorizations that cover a broader and deeper view of TL developments
    in DRL. Our main analysis is anchored on a fundamental question, i.e. *what is
    the transferred knowledge in RL*, following which we conducted more refined analysis.
    Most TL strategies, including those discussed in prior surveys are well suited
    in our categorization framework. 3) Reflecting on the developments of TL methods
    in DRL, we brought new thoughts on its future directions, including how to do
    *reasoning* over miscellaneous knowledge forms and how to *leverage* knowledge
    in more efficient and principled manner. We also pointed out the prominent applications
    of TL for DRL and its opportunities to thrive in the future era of AGI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of this survey is organized as follows: In Section [2](#S2 "2 Deep
    Reinforcement Learning and Transfer Learning ‣ Transfer Learning in Deep Reinforcement
    Learning: A Survey") we introduce RL preliminaries, including the recent key development
    based on deep neural networks. Next, we discuss the definition of TL in the context
    of RL and its relevant topics (Section [2.4](#S2.SS4 "2.4 Related Topics ‣ 2 Deep
    Reinforcement Learning and Transfer Learning ‣ Transfer Learning in Deep Reinforcement
    Learning: A Survey")). In Section [3](#S3 "3 Analyzing Transfer Learning ‣ Transfer
    Learning in Deep Reinforcement Learning: A Survey"), we provide a framework to
    categorize TL approaches from multiple perspectives, analyze their fundamental
    differences, and summarize their evaluation metrics (Section [3.3](#S3.SS3 "3.3
    Evaluation metrics ‣ 3 Analyzing Transfer Learning ‣ Transfer Learning in Deep
    Reinforcement Learning: A Survey")). In Section [5](#S5 "5 Transfer Learning Approaches
    Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey"), we elaborate
    on different TL approaches in the context of DRL, organized by the format of transferred
    knowledge, such as reward shaping (Section [5.1](#S5.SS1 "5.1 Reward Shaping ‣
    5 Transfer Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement
    Learning: A Survey")), learning from demonstrations (Section [5.2](#S5.SS2 "5.2
    Learning from Demonstrations ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer
    Learning in Deep Reinforcement Learning: A Survey")), or learning from teacher
    policies (Section [5.3](#S5.SS3 "5.3 Policy Transfer ‣ 5 Transfer Learning Approaches
    Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")). We
    also investigate TL approaches by the way that knowledge transfer occurs, such
    as inter-task mapping (Section [5.4](#S5.SS4 "5.4 Inter-Task Mapping ‣ 5 Transfer
    Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning:
    A Survey")), or learning transferrable representations (Section [5.5](#S5.SS5
    "5.5 Representation Transfer ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer
    Learning in Deep Reinforcement Learning: A Survey")), etc. We discuss contemporary
    applications of TL in the context of DRL in Section [6](#S6 "6 Applications ‣
    Transfer Learning in Deep Reinforcement Learning: A Survey") and provide some
    future perspectives and open questions in Section [7](#S7 "7 Future Perspectives
    ‣ Transfer Learning in Deep Reinforcement Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Deep Reinforcement Learning and Transfer Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Reinforcement Learning Basics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Markov Decision Process:* A typical RL problem can be considered as training
    an agent to interact with an environment that follows a Markov Decision Process
    (MPD) [[13](#bib.bib13)]. The agent starts with an initial state and performs
    an *action* accordingly, which yields a *reward* to guide the agent actions. Once
    the action is taken, the MDP transits to the next state by following the underlying
    *transition dynamics* of the MDP. The agent accumulates the time-*discounted*
    rewards along with its interactions. A subsequence of interactions is referred
    to as an *episode*. The above-mentioned components in an MDP can be represented
    using a tuple, i.e. ${\mathcal{M}}=(\mu_{0},{\mathcal{S}},{\mathcal{A}},{\mathcal{T}},\gamma,{\mathcal{R}}$),
    in which:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mu_{0}$ is the set of initial states.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ${\mathcal{S}}$ is the state space.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ${\mathcal{A}}$ is the action space.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '${\mathcal{T}}$: $S\times A\times S\to\mathbb{R}$ is the transition probability
    distribution, where ${\mathcal{T}}(s^{\prime}|s,a)$ specifies the probability
    of the state transitioning to $s^{\prime}$ upon taking action $a$ from state $s$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ${\mathcal{R}}:S\times A\times S\to\mathbb{R}$ is the reward distribution, where
    ${\mathcal{R}}(s,a,s^{\prime})$ is the reward that an agent can get by taking
    action $a$ from state $s$ with the next state being $s^{\prime}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\gamma$ is a discounted factor, with $\gamma\in(0,1]$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A RL agent behaves in ${\mathcal{M}}$ by following its policy $\pi$, which
    is a mapping from states to actions: $\pi:{\mathcal{S}}\to{\mathcal{A}}$ . For
    a stochastic policy $\pi$, $\pi(a|s)$ denotes the probability of taking action
    $a$ from state $s$. Given an MDP ${\mathcal{M}}$ and a policy $\pi$, one can derive
    a value function $V_{\mathcal{M}}^{\pi}(s)$, which is defined over the state space:
    $V_{\mathcal{M}}^{\pi}(s)={\mathds{E}}\left[r_{0}+\gamma r_{1}+\gamma^{2}r_{2}+\dots;\pi,s\right],$
    where $r_{i}={\mathcal{R}}(s_{i},a_{i},s_{i+1})$ is the reward that an agent receives
    by taking action $a_{i}$ in the $i$-th state $s_{i}$, and the next state transits
    to $s_{i+1}$. The expectation ${\mathds{E}}$ is taken over ${s_{0}\sim\mu_{0},a_{i}\sim\pi(\cdot|s_{i}),s_{i+1}\sim{\mathcal{T}}(\cdot|s_{i},a_{i})}$.
    The value function estimates the quality of being in state $s$, by evaluating
    the expected rewards that an agent can get from $s$ following policy $\pi$. Similar
    to the value function, a policy also carries a $Q$-function, which estimates the
    quality of taking action $a$ from state $s$: $Q_{\mathcal{M}}^{\pi}(s,a)={\mathds{E}}_{s^{\prime}\sim{\mathcal{T}}(\cdot|s,a)}\left[{\mathcal{R}}(s,a,s^{\prime})+\gamma
    V_{\mathcal{M}}^{\pi}(s^{\prime})\right].$ *Reinforcement Learning Goals:* Standard
    RL aims to learn an optimal policy $\pi_{{\mathcal{M}}}^{*}$ with the optimal
    value and $Q$-function, s.t. $\forall s\in{\mathcal{S}},\pi_{\mathcal{M}}^{*}(s)=\underset{a\in
    A}{\arg\max}~{}~{}{Q_{\mathcal{M}}^{*}(s,a)},$ where $Q_{\mathcal{M}}^{*}(s,a)=\underset{\pi}{\sup}~{}~{}Q_{\mathcal{M}}^{\pi}(s,a)$.
    The learning objective can be reduced as maximizing the expected return:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle J(\pi):={\mathds{E}}_{(s,a)\sim\mu^{\pi}(s,a)}[\sum_{t}\gamma^{t}r_{t}],\vspace{-0.1in}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mu^{\pi}(s,a)$ is the *stationary state-action distribution* induced
    by $\pi$ [[14](#bib.bib14)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Built upon recent progress of DRL, some literature has extended the RL objective
    to achieving miscellaneous goals under different conditions, referred to as *Goal-Conditional
    RL* (GCRL). In GCRL, the agent policy $\pi(\cdot|s,g)$ is dependent not only on
    state observations $s$ but also the goal $g$ being optimized. Each individual
    goal $g\sim{\mathcal{G}}$ can be differentiated by its reward function $r(s_{t},a_{t},g)$,
    hence the objective for GCRL becomes maximizing the expected return over the distribution
    of goals: $J(\pi):={\mathds{E}}_{(s_{t},a_{t})\sim\mu^{\pi},g\sim{\mathcal{G}}}\left[\sum_{t}\gamma^{t}r(s,a,g)\right]$
     [[15](#bib.bib15)]. A prototype example of GCRL can be maze locomotion tasks,
    where the learning goals are manifested as desired locations in the maze [[16](#bib.bib16)].'
  prefs: []
  type: TYPE_NORMAL
- en: '*Episodic vs. Non-episodic Reinforcement Learning:* In episodic RL, the agent
    performs in finite episodes of length $H$, and will be *reset* to an initial state
    $\in\mu_{0}$ upon the episode ends[[1](#bib.bib1)]. Whereas in non-episodic RL,
    the learning agent continuously interacts with the MDP without any state reset [[17](#bib.bib17)].
    To encompass the episodic concept in infinite MDPs, episodic RL tasks usually
    assume the existence of a set of absorbing states  ${\mathcal{S}}_{0}$, which
    indicates the termination of episodic tasks [[18](#bib.bib18), [19](#bib.bib19)],
    and any action taken upon an absorbing state will only transit to itself with
    zero rewards.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Reinforcement Learning Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two major methods to conduct RL: *Model-Based* and *Model-Free*.
    In *model-based* RL, a learned or provided model of the MDP is used for policy
    learning. In *model-free* RL, optimal policy is learned without modeling the transition
    dynamics or reward functions. In this section, we start introducing RL techniques
    from a *model-free* perspective, due to its relatively simplicity, which also
    provides foundations for many model-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prediction and Control*: an RL problem can be disassembled into two subtasks:
    *prediction* and *control* [[1](#bib.bib1)]. In the *prediction* phase, the quality
    of the current policy is being evaluated. In the *control* phase or the *policy
    improvement* phase, the learning policy is adjusted based on evaluation results
    from the *prediction* step. Policies can be improved by iterating through these
    two steps, known as *policy iteration*.'
  prefs: []
  type: TYPE_NORMAL
- en: For *model-free* policy iterations, the target policy is optimized without requiring
    knowledge of the MDP transition dynamics. Traditional model-free RL includes *Monte-Carlo*
    methods, which estimates the value of each state using *samples of episodes* starting
    from that state. Monte-Carlo methods can be *on-policy* if the samples are collected
    by following the target policy, or *off-policy* if the episodic samples are collected
    by following a *behavior* policy that is different from the target policy.
  prefs: []
  type: TYPE_NORMAL
- en: '*Temporal Difference (TD) Learning* is an alternative to Monte-Carlo for solving
    the *prediction* problem. The key idea behind TD-learning is to learn the state
    quality function by *bootstrapping*. It can also be extended to solve the *control*
    problem so that both value function and policy can get improved simultaneously.
    Examples of *on-policy* TD-learning algorithms include *SARSA* [[20](#bib.bib20)],
    *Expected SARSA* [[21](#bib.bib21)], *Actor-Critic* [[22](#bib.bib22)], and its
    deep neural network extension called *A3C* [[23](#bib.bib23)]. The *off-policy*
    TD-learning approaches include SAC [[24](#bib.bib24)] for continuous state-action
    spaces, and $Q$-learning [[25](#bib.bib25)] for discrete state-action spaces,
    along with its variants built on deep-neural networks, such as DQN [[26](#bib.bib26)],
    Double-DQN [[26](#bib.bib26)], Rainbow [[27](#bib.bib27)], etc. TD-learning approaches
    focus more on estimating the state-action value functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Policy Gradient*, on the other hand, is a mechanism that emphasizes on direct
    optimization of a parameterizable policy. Traditional policy-gradient approaches
    include *REINFORCE* [[28](#bib.bib28)]. Recent years have witnessed the joint
    presence of TD-learning and policy-gradient approaches. Representative algorithms
    along this line include *Trust region policy optimization (TRPO)* [[29](#bib.bib29)],
    *Proximal Policy optimization (PPO)* [[30](#bib.bib30)], *Deterministic policy
    gradient (DPG)* [[31](#bib.bib31)] and its extensions such as *DDPG* [[32](#bib.bib32)]
    and *Twin Delayed DDPG* [[33](#bib.bib33)].'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike model-free methods that learn purely from trial-and-error, *Model-Based
    RL* (MBRL) explicitly learns the transition dynamics or cost functions of the
    environment. The dynamics model can sometimes be treated as a *black-box* for
    better *sampling-based planning*. Representative examples include the *Monte-Carlo*
    method dubbed *random shooting* [[34](#bib.bib34)] and its cross-entropy method
    (CEM) variants [[35](#bib.bib35), [36](#bib.bib36)]. The modeled dynamics can
    also facilitate learning with data generation [[37](#bib.bib37)] and value estimation [[38](#bib.bib38)].
    For MBRL with *white-box* modeling, the transition models become differentiable
    and can facilitate planning with direct gradient propogation. Methods along this
    line include *differential planning* for policy gradient [[39](#bib.bib39)] and
    action sequences search [[40](#bib.bib40)], and value gradient methods [[41](#bib.bib41),
    [42](#bib.bib42)]. One advantage of MBRL is its higher sample efficiency than
    model-free RL, although it can be challenging for complex domains, where it is
    usually more difficult to learn the dynamics than learning a policy.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Transfer Learning in the Context of Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remark 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Without losing clarify, for the rest of this survey, we refer to MDPs, domains,
    and tasks equivalently.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*[Transfer Learning in the Context of RL]* Given a set of source domains $\bm{{\mathcal{M}}}_{s}=\{{\mathcal{M}}_{s}|{\mathcal{M}}_{s}\in\bm{{\mathcal{M}}}_{s}\}$
    and a target domain ${\mathcal{M}}_{t}$, *Transfer Learning* aims to learn an
    optimal policy $\pi^{*}$ for the target domain, by leveraging exterior information
    ${\mathcal{I}}_{s}$ from $\bm{{\mathcal{M}}}_{s}$ as well as interior information
    ${\mathcal{I}}_{t}$ from ${\mathcal{M}}_{t}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle~{}\pi^{*}=\operatorname*{arg\,max}_{\pi}{\mathds{E}}_{s\sim\mu_{0}^{t},a\sim\pi}[Q^{\pi}_{{\mathcal{M}}}(s,a)],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where ${\pi}=\phi({\mathcal{I}}_{s}\sim\bm{{\mathcal{M}}}_{s},{\mathcal{I}}_{t}\sim{\mathcal{M}}_{t}):{\mathcal{S}}^{t}\to{\mathcal{A}}^{t}$
    is a policy learned for the target domain ${\mathcal{M}}_{t}$ based on information
    from both ${\mathcal{I}}_{t}$ and ${\mathcal{I}}_{s}$.
  prefs: []
  type: TYPE_NORMAL
- en: In the above definition, we use $\phi({\mathcal{I}})$ to denote the learned
    policy based on information ${\mathcal{I}}$, which is usually approximated with
    deep neural networks in DRL. For the simplistic case, knowledge can transfer between
    two agents within the same domain, resulting in $|\bm{{\mathcal{M}}}_{s}|=1$,
    and ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$. One can consider regular RL without
    TL as a special case of the above definition, by treating ${\mathcal{I}}_{s}=\emptyset$,
    so that a policy $\pi$ is learned purely on the feedback provided by the target
    domain, $i.e.~{}{\pi}=\phi({\mathcal{I}}_{t})$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Related Topics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to TL, other efforts have been made to benefit RL by leveraging
    different forms of supervision. In this section, we briefly discuss other techniques
    that are relevant to TL by analyzing the differences and connections between transfer
    learning and these relevant techniques, which we hope can further clarify the
    scope of this survey.
  prefs: []
  type: TYPE_NORMAL
- en: '*Continual Learning* is the ability of sequentially learning multiple tasks
    that are temporally or spatially related, without forgetting the previously acquired
    knowledge. Continual Learning is a specialized yet more challenging scenario of
    TL, in that the learned knowledge needs to be transferred along a sequence of
    dynamically-changing tasks that cannot be foreseen, rather than learning a fixed
    group of tasks. Hence, different from most TL methods discussed in this survey,
    the ability of *automatic task detection* and *avoiding catastrophic forgetting*
    is usually indispensable in continual learning [[43](#bib.bib43)].'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hierarchical RL* has been proposed to resolve complex real-world tasks. Different
    from traditional RL, for hierarchical RL, the action space is grouped into different
    granularities to form higher-level macro actions. Accordingly, the learning task
    is also decomposed into hierarchically dependent sub-goals. Well-known hierarchical
    RL frameworks include *Feudal learning* [[44](#bib.bib44)], *Options framework*[[45](#bib.bib45)],
    *Hierarchical Abstract Machines* [[46](#bib.bib46)], and *MAXQ* [[47](#bib.bib47)].
    Given the higher-level abstraction on tasks, actions, and state spaces, hierarchical
    RL can facilitate knowledge transfer across similar domains.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Multi-task RL* learns an agent with generalized skills across various tasks,
    hence it can solve MDPs randomly sampled from a fixed yet unknown distribution
    [[48](#bib.bib48)]. A larger concept of multi-task learning also incorporates
    multi-task supervised learning and unsupervised learning [[49](#bib.bib49)]. Multi-task
    learning is naturally related to TL, in that the learned skills, typically manifested
    as representations, need to be effectively shared among domains. Many TL techniques
    later discussed in this survey can be readily applied to solve multi-task RL scenarios,
    such as policy distillation [[50](#bib.bib50)], and representation sharing [[51](#bib.bib51)].
    One notable challenges in multi-task learning is negative transfer, which is induced
    by the irrelevance or conflicting property for learned tasks. Hence, some recent
    work in multi-task RL focused on a trade-off between sharing and individualizing
    function modules [[52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54)].'
  prefs: []
  type: TYPE_NORMAL
- en: '*Generalization in RL* refers to the ability of learning agents to adapt to
    *unseen* domains. Generalization is a crucial property for RL to achieve, especially
    when classical RL assumes identical training and inference MDPs, whereas the real
    world is constantly changing. Generalization in RL is considered more challenging
    than in supervised learning due to the non-stationarity of MDPs, where the latter
    has provided inspirations for the former [[55](#bib.bib55)]. *Meta-learning* is
    an effective direction towards generalization, which also draws close connections
    to TL. Some TL techniques discussed in this survey are actually designed for meta-RL.
    However, meta-learning is particularly focused on the learning methods that lead
    to *fast adaptation* to unseen domains, whereas TL is a broader concept and covers
    scenarios where the target environment can be (partially) observable. To tackle
    unseen tasks in RL, some meta-RL methods focused on training MDPs generation [[56](#bib.bib56)]
    and variations estimation [[57](#bib.bib57)]. We refer readers to [[58](#bib.bib58)]
    for a more focused survey on meta RL.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Analyzing Transfer Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss TL approaches in RL from different angles. We also
    use a prototype to illustrate the potential variants residing in knowledge transfer
    among domains, then summarize important metrics for TL evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Categorization of Transfer Learning Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TL approaches can be organized by answering the following key questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*What knowledge is transferred*: Knowledge from the source domain can take
    different forms, such as expert experiences [[59](#bib.bib59)], the action probability
    distribution of an expert policy [[60](#bib.bib60)], or even a potential function
    that estimates the quality of demonstrations in the target MDP [[61](#bib.bib61)].
    The divergence in representations and granularities of knowledge fundamentally
    influences how TL is performed. The quality of the transferred knowledge, e.g. whether
    it comes from an oracle  [[62](#bib.bib62)] or a suboptimal teacher [[63](#bib.bib63)]
    also affects the way TL methods are designed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*What RL frameworks fit the TL approach:* We can rephrase this question into
    other forms, e.g., is the TL approach policy-agnostic, or only applicable to certain
    RL backbones, such as the Temporal Difference (TD) methods? Answers to this question
    are closely related to the representaion of knowledge. For example, transferring
    knowledge from expert demonstrations are usually policy-agnostic (see Section [5.2](#S5.SS2
    "5.2 Learning from Demonstrations ‣ 5 Transfer Learning Approaches Deep Dive ‣
    Transfer Learning in Deep Reinforcement Learning: A Survey")), while policy distillation,
    to be discussed in Section [5.3](#S5.SS3 "5.3 Policy Transfer ‣ 5 Transfer Learning
    Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey"),
    may not be suitable for DQN backbone which does not explicitly learn a policy
    function.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*What is the difference between the source and the target domain*: Some TL
    approaches fit where the source domain ${\mathcal{M}}_{s}$ and the target domain
    ${\mathcal{M}}_{t}$ are equivalent, whereas others are designed to transfer knowledge
    between different domains. For example, in video gaming tasks where observations
    are RGB pixels, ${\mathcal{M}}_{s}$ and ${\mathcal{M}}_{t}$ may share the same
    action space (${\mathcal{A}}$) but differs in their observation spaces (${\mathcal{S}}$).
    For goal-conditioned RL [[64](#bib.bib64)], the two domains may differ only by
    the reward distribution: ${\mathcal{R}}_{s}\neq{\mathcal{R}}_{t}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*What information is available in the target domain:*  While knowledge from
    source domains is usually accessible, it can be prohibitive to sample from the
    target domain, or the reward signal can be sparse or delayed. Examples include
    adapting an auto-driving agent pre-trained in simulated platforms to real environments [[65](#bib.bib65)],
    The accessibility of information in the target domain can affect the way that
    TL approaches are designed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*How sample-efficient the TL approach is:* TL enables the RL with better initial
    performance, hence usually requires fewer interactions compared with learning
    from scratch. Based on the sampling cost, we can categorize TL approaches into
    the following classes: (i) *Zero-shot* transfer, which learns an agent that is
    directly applicable to the target domain without requiring any training interactions;
    (ii) *Few-shot* transfer, which only requires a few samples (interactions) from
    the target domain; (iii) *Sample-efficient* transfer, where an agent can benefit
    by TL to be more sample efficient compared to normal RL.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2 Case Analysis of Transfer Learning in the context of Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now use HalfCheetah¹¹1https://gym.openai.com/envs/HalfCheetah-v2/ as a working
    example to illustrate how TL can occur between the source and the target domain.
    HalfCheetah is a standard DRL benchmark for solving physical locomotion tasks,
    in which the objective is to train a two-leg agent to run fast without losing
    control of itself.
  prefs: []
  type: TYPE_NORMAL
- en: '3.2.1 Potential Domain Differences:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'During TL, the differences between the source and target domain may reside
    in any component of an MDP:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '${\mathcal{S}}$ (State-space): domains can be made different by extending or
    constraining the available positions for the HalfCheetah agent to move.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ${\mathcal{A}}$ (Action-space) can be adjusted by changing the range of available
    torques for the thigh, shin, or foot of the agent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '${\mathcal{R}}$ (Reward function): a domain can be simplified by using only
    the distance moved forward as rewards or be perplexed by using the scale of accelerated
    velocity in each direction as extra penalty costs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '${\mathcal{T}}$ (Transition dynamics): two domains can differ by following
    different physical rules, leading to different transition probabilities given
    the same state-action pairs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$\mu_{0}$ (Initial states): the source and target domains may have different
    initial states, specifying where and with what posture the agent can start moving.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$\tau$ (Trajectories): the source and target domains may allow a different
    number of steps for the agent to move before a task is done.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.2.2 Transferrable Knowledge:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Without losing generality, we list below some transferrable knowledge assuming
    that the source and target domains are variants of HalfCheetah:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Demonstrated trajectories: the target agent can learn from the behavior of
    a pre-trained expert, e.g. a sequence of running demonstrations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model dynamics: the RL agent may access a model of the physical dynamics for
    the source domain that is also partly applicable to the target domain. It can
    perform dynamic programming based on the physical rules, running fast without
    losing its control due to the accelerated velocity.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Teacher policies: an expert policy may be consulted by the learning agent,
    which outputs the probability of taking different actions upon a given state example.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Teacher value functions: besides teacher policy, the learning agent may also
    refer to the value function derived by a teacher policy, which implies the quality
    of state-actions from the teacher’s point of view.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.3 Evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we present some representative metrics for evaluating TL approaches,
    which have also been partly summarized in prior work [[11](#bib.bib11), [66](#bib.bib66)]:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jumpstart performance( jp)*: the initial performance (returns) of the agent.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Asymptotic performance (ap)*: the ultimate performance (returns) of the agent.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Accumulated rewards (ar)*: the area under the learning curve of the agent.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Transfer ratio (tr)*: the ratio between asymptotic performance of the agent
    with TL and asymptotic performance of the agent without TL.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Time to threshold (tt)*: the learning time (iterations) needed for the target
    agent to reach certain performance threshold.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Performance with fixed training epochs (pe)*: the performance achieved by
    the target agent after a specific number of training iterations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Performance sensitivity (ps)*: the variance in returns using different hyper-parameter
    settings.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The above criteria mainly focus on the *learning process* of the target agent.
    In addition, we introduce the following metrics from the perspective of *transferred
    knowledge*, which, although commensurately important for evaluation, have not
    been explicitly discussed by prior art:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Necessary knowledge amount (nka)*: the necessary *amount* of the knowledge
    required for TL in order to achieve certain performance thresholds. Examples along
    this line include the number of designed source tasks [[67](#bib.bib67)], the
    number of expert policies, or the number of demonstrated interactions [[68](#bib.bib68)]
    required to enable knowledge transfer.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Necessary knowledge quality (nkq)*: the guaranteed *quality* of the knowledge
    required to enable effective TL. This metric helps in answering questions such
    as (i) Does the TL approach rely on near-oracle knowledge, such as expert demonstrations/policies
    [[69](#bib.bib69)], or (ii) is the TL technique feasible even given suboptimal
    knowledge [[63](#bib.bib63)]?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: TL approaches differ in various perspectives, including the forms of transferred
    knowledge, the RL frameworks utilized to enable such transfer, and the gaps between
    the source and the target domain. It maybe biased to evaluate TL from just one
    viewpoint. We believe that explicating these TL related metrics helps in designing
    more generalizable and efficient TL approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, most of the abovementioned metrics can be considered as evaluating
    two abilities of a TL approach: the *mastery* and *generalization*. *Mastery*
    refers to how well the learned agent can ultimately perform in the target domain,
    while *generalization* refers to the ability of the learning agent to quickly
    adapt to the target domain.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are prior efforts in summarizing TL research in RL. One of the earliest
    literatures is [[11](#bib.bib11)] . Their main categorization is from the perspective
    of *problem setting*, in which the TL scenarios may vary in the number of domains
    involved, and the difference of state-action space among domains. Similar categorization
    is adopted by [[12](#bib.bib12)], with more refined analysis dimensions including
    the objective of TL. As pioneer surveys for TL in RL, neither [[11](#bib.bib11)]
    nor [[12](#bib.bib12)] covered recent research over the last decade. For instance,
    [[11](#bib.bib11)] emphasized on different *task-mapping* methods, which are more
    suitable for domains with tabular or mild state-action space dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other surveys focused on specific subtopics that interplay between
    RL and TL. For instance, [[70](#bib.bib70)] consolidated sim-to-real TL methods.
    They explored work that is more tailored for *robotics* domains, including domain
    generalization and zero-shot transfer, which is a favored application field of
    DRL as we discussed in Sec [6](#S6 "6 Applications ‣ Transfer Learning in Deep
    Reinforcement Learning: A Survey"). [[71](#bib.bib71)] conducted extensive database
    search and summarized benchmarks for evaluating TL algorithms in RL. [[72](#bib.bib72)]
    surveyed recent progress in multi-task RL. They partially shared research focus
    with us by studying certain TL oriented solutions towards multi-task RL, such
    as learning shared representations, pathNets, etc. We surveyed TL for RL with
    a broader spectrum in methodologies, applications, evaluations, which naturally
    draws connections to the above literatures.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d89797408668410787a1d1bc51767ba8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An overview of different TL approaches, organized by the format of
    transferred knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Transfer Learning Approaches Deep Dive
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we elaborate on various TL approaches and organize them into
    different sub-topics, mostly by answering the question of “what knowledge is transferred”.
    For each type of TL approach, we analyze them by following the other criteria
    mentioned in Section [3](#S3 "3 Analyzing Transfer Learning ‣ Transfer Learning
    in Deep Reinforcement Learning: A Survey") and and summarize the key evaluation
    metrics that are applicable to the discussed work. Figure [1](#S4.F1 "Figure 1
    ‣ 4 Related Work ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")
    presents an overview of different TL approaches discussed in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Reward Shaping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start by introducing the Reward Shaping approach, as it is applicable to
    most RL backbones and also largely overlaps with the other TL approaches discussed
    later. Reward Shaping (RS) is a technique that leverages the exterior knowledge
    to reconstruct the reward distribution of the target domain to guide the agent’s
    policy learning. More specifically, in addition to the environment reward signals,
    RS learns a reward-shaping function ${\mathcal{F}}:{\mathcal{S}}\times{\mathcal{S}}\times{\mathcal{A}}\to\mathbb{R}$
    to render auxiliary rewards, provided that the additional rewards contain external
    knowledge to guide the agent for better action selections. Intuitively, an RS
    strategy will assign higher rewards to more beneficial state-actions to navigate
    the agent to desired trajectories. As a result, the agent will learn its policy
    using the newly shaped rewards ${\mathcal{R}}^{\prime}$: ${\mathcal{R}}^{\prime}={\mathcal{R}}+{\mathcal{F}}$,
    which means that RS has altered the target domain with a different reward function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\mathcal{M}}=({\mathcal{S}},{\mathcal{A}},{\mathcal{T}},\gamma,{\mathcal{R}}))\to{\mathcal{M}}^{\prime}=({\mathcal{S}},{\mathcal{A}},{\mathcal{T}},\gamma,{\mathcal{R}}^{\prime}).\vspace{-0.1in}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'Along the line of RS, *Potential based Reward Shaping (PBRS)* is one of the
    most classical approaches. [[61](#bib.bib61)] proposed PBRS to form a shaping
    function $F$ as the difference between two potential functions ($\Phi(\cdot)$):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle F(s,a,s^{\prime})=\gamma\Phi(s^{\prime})-\Phi(s),\vspace{-.05in}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where the potential function $\Phi(\cdot)$ comes from the knowledge of expertise
    and evaluates the quality of a given state. It has been proved that, without further
    restrictions on the underlying MDP or the shaping function $F$, PBRS is sufficient
    and necessary to preserve the policy invariance. Moreover, the optimal $Q$-function
    in the original and transformed MDP are related by the potential function: $Q_{{\mathcal{M}}^{\prime}}^{*}(s,a)=Q_{{\mathcal{M}}}^{*}(s,a)-\Phi(s)$,
    which draws a connection between potential based reward-shaping and advantage-based
    learning approaches [[73](#bib.bib73)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea of *PBRS* was extended to [[74](#bib.bib74)], which formulated the
    potential as a function over both the state and the action spaces. This approach
    is called *Potential Based state-action Advice (PBA)*. The potential function
    $\Phi(s,a)$ therefore evaluates how beneficial an action $a$ is to take from state
    $s$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle F(s,a,s^{\prime},a^{\prime})=\gamma\Phi(s^{\prime},a^{\prime})-\Phi(s,a).$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'PBA requires on-policy learning and can be sample-costly, as in Equation ([3](#S5.E3
    "In 5.1 Reward Shaping ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer Learning
    in Deep Reinforcement Learning: A Survey")), $a^{\prime}$ is the action to take
    upon state $s$ is transitioning to $s^{\prime}$ by following the learning policy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional RS approaches assumed a static potential function, until [[75](#bib.bib75)]
    proposed a *Dynamic Potential Based (DPB)* approach which makes the potential
    a function of both states and time: $F(s,t,s^{\prime},t^{\prime})=\gamma\Phi(s^{\prime},t^{\prime})-\Phi(s,t).$They
    proved that this dynamic approach can still maintain policy invariance: $Q^{*}_{{\mathcal{M}}^{\prime}}(s,a)=Q^{*}_{\mathcal{M}}(s,a)-\Phi(s,t),$where
    $t$ is the current tilmestep. [[76](#bib.bib76)] later introduced a way to incorporate
    any prior knowledge into a dynamic potential function structure, which is called
    *Dynamic Value Function Advice (DPBA)*. The rationale behind DPBA is that, given
    any extra reward function $R^{+}$ from prior knowledge, in order to add this extra
    reward to the original reward function, the potential function should satisfy:
    $\gamma\Phi(s^{\prime},a^{\prime})-\Phi(s,a)=F(s,a)=R^{+}(s,a).$'
  prefs: []
  type: TYPE_NORMAL
- en: 'If $\Phi$ is not static but learned as an extra state-action Value function
    overtime, then the Bellman equation for $\Phi$ is : $\Phi^{\pi}(s,a)=r^{\Phi}(s,a)+\gamma\Phi(s^{\prime},a^{\prime}).$
    The shaping rewards $F(s,a)$ is therefore the negation of $r^{\Phi}(s,a)$ :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle F(s,a)=\gamma\Phi(s^{\prime},a^{\prime})-{\Phi}(s,a)=-r^{\Phi}(s,a).$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'This leads to the approach of using the negation of $R^{+}$ as the immediate
    reward to train an extra state-action Value function $\Phi$ and the policy simultaneously.
    Accordingly, the dynamic potential function $F$ becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle F_{t}(s,a)=\gamma\Phi_{t+1}(s^{\prime},a^{\prime})-{\Phi_{t}}(s,a).\vspace{-.05in}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: The advantage of DPBA is that it provides a framework to allow arbitrary knowledge
    to be shaped as auxiliary rewards.
  prefs: []
  type: TYPE_NORMAL
- en: 'Research along this line mainly focus on designing different shaping functions
    $F(s,a)$, while not much work has tackled the question of what knowledge can be
    used to derive this potential function. One work by [[77](#bib.bib77)] proposed
    to use RS to transfer an expert policy from the source domain ${\mathcal{M}}_{s}$
    to the target domain ${\mathcal{M}}_{t}$. This approach assumed the existence
    of two mapping functions $M_{S}$ and $M_{A}$ that can transform the state and
    action from the source to the target domain. Another work used demonstrated state-action
    samples from an expert policy to shape rewards [[78](#bib.bib78)]. Learning the
    augmented reward involves learning a discriminator to distinguish samples generated
    by an expert policy from samples generated by the target policy. The loss of the
    discriminator is applied to shape rewards to incentivize the learning agent to
    mimic the expert behavior. This work combines two TL approaches: RS and Learning
    from Demonstrations, the latter of which will be elaborated in Section [5.2](#S5.SS2
    "5.2 Learning from Demonstrations ‣ 5 Transfer Learning Approaches Deep Dive ‣
    Transfer Learning in Deep Reinforcement Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The above-mentioned RS approaches are summarized in Table [I](#S5.T1 "TABLE
    I ‣ 5.1 Reward Shaping ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer Learning
    in Deep Reinforcement Learning: A Survey"). They follow the potential based RS
    principle that has been developed systematically: from the classical *PBRS* which
    is built on a *static* potential shaping function of *states*, to *PBA* which
    generates the potential as a function of both *states* and *actions*, and *DPB*
    which learns a dynamic potential function of *states* and *time*, to the most
    recent *DPBA*, which involves a dynamic potential function of *states* and *actions*
    to be learned as an extra state-action Value function in parallel with the environment
    Value function. As an effective TL paradigm, RS has been widely applied to fields
    including robot training [[79](#bib.bib79)], spoken dialogue systems [[80](#bib.bib80)],
    and question answering [[81](#bib.bib81)]. It provides a feasible framework for
    transferring knowledge as the augmented reward and is generally applicable to
    various RL algorithms. RS has also been applied to multi-agent RL [[82](#bib.bib82)]
    and model-based RL [[83](#bib.bib83)]. Principled integration of RS with other
    TL approaches, such as *Learning from demonstrations* (Section [5.2](#S5.SS2 "5.2
    Learning from Demonstrations ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer
    Learning in Deep Reinforcement Learning: A Survey")) and *Policy Transfer* (Section
    [5.3](#S5.SS3 "5.3 Policy Transfer ‣ 5 Transfer Learning Approaches Deep Dive
    ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")) will be an intriguing
    question for ongoing research.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that RS approaches discussed so far are built upon a consensus that the
    source information for shaping the reward comes *externally*, which coincides
    with the notion of knowledge transfer. Some RS work also tackles the scenario
    where the shaped reward comes *intrinsically*. For instance, *Belief Reward Shaping*
    was proposed by [[84](#bib.bib84)], which utilizes a Bayesian reward shaping framework
    to generate the potential value that decays with experience, where the potential
    value comes from the critic itself.
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | MDP difference | Format of shaping reward | Knowledge source |
    Evaluation metrics |'
  prefs: []
  type: TYPE_TB
- en: '| PBRS | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F=\gamma\Phi(s^{\prime})-\Phi(s)$
    | ✗ | *ap, ar* |'
  prefs: []
  type: TYPE_TB
- en: '| PBA | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F=\gamma\Phi(s^{\prime},a^{\prime})-\Phi(s,a)$
    | ✗ | *ap, ar* |'
  prefs: []
  type: TYPE_TB
- en: '| DPB | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F=\gamma\Phi(s^{\prime},t^{\prime})-\Phi(s,t)$
    | ✗ | *ap, ar* |'
  prefs: []
  type: TYPE_TB
- en: '| DPBA | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F_{t}=\gamma\Phi_{t+1}(s^{\prime},a^{\prime})-{\Phi_{t}}(s,a)$
    , $\Phi$ learned as an extra Q function | ✗ | *ap, ar* |'
  prefs: []
  type: TYPE_TB
- en: '| [[77](#bib.bib77)] | ${\mathcal{S}}_{s}\neq{\mathcal{S}}_{t}$, ${\mathcal{A}}_{s}\neq{\mathcal{A}}_{t}$
    | $F_{t}=\gamma\Phi_{t+1}(s^{\prime},a^{\prime})-{\Phi_{t}}(s,a)$ | $\pi_{s}$
    | *ap, ar* |'
  prefs: []
  type: TYPE_TB
- en: '| [[78](#bib.bib78)] | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F_{t}=\gamma\Phi_{t+1}(s^{\prime},a^{\prime})-{\Phi_{t}}(s,a)$
    | $D_{E}$ | *ap, ar* |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE I: A comparison of reward shaping approaches. ✗ denotes that the information
    is not revealed in the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Learning from Demonstrations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Learning from Demonstrations (LfD)* is a technique to assist RL by utilizing
    external demonstrations for more efficient exploration. The demonstrations may
    come from different sources with varying qualities. Research along this line usually
    address a scenario where the source and the target MDPs are the same: ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$,
    although there has been work that learns from demonstrations generated in a different
    domain [[85](#bib.bib85), [86](#bib.bib86)].'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on *when* the demonstrations are used for knowledge transfer, approaches
    can be organized into *offline* and *online* methods. For *offline* approaches,
    demonstrations are either used for pre-training RL components, or for offline
    RL [[87](#bib.bib87), [88](#bib.bib88)]. When leveraging demonstrations for pre-training,
    RL components such as the value function $V(s)$ [[89](#bib.bib89)], the policy
    $\pi$ [[90](#bib.bib90)], or the model of transition dynamics [[91](#bib.bib91)],
    can be initialized by learning from demonstrations. For the *online* approach,
    demonstrations are directly used to guide agent actions for efficient explorations [[92](#bib.bib92)].
    Most work discussed in this section follows the online transfer paradigm or combines
    offline pre-training with online RL [[93](#bib.bib93)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Work along this line can also be categorized depending on *what* RL frameworks
    are compatible: some adopts the policy-iteration framework [[94](#bib.bib94),
    [59](#bib.bib59), [95](#bib.bib95)], some follow a $Q$-learning framework [[92](#bib.bib92),
    [96](#bib.bib96)], while recent work usually follows the policy-gradient framework [[78](#bib.bib78),
    [93](#bib.bib93), [97](#bib.bib97), [63](#bib.bib63)]. Demonstrations have been
    leveraged in the *policy iterations* framework by [[98](#bib.bib98)]. Later, [[94](#bib.bib94)]
    introduced the *Direct Policy Iteration with Demonstrations (DPID)* algorithm.
    This approach samples complete demonstrated rollouts $D_{E}$ from an expert policy
    $\pi_{E}$, in combination with the self-generated rollouts $D_{\pi}$ gathered
    from the learning agent. $D_{\pi}\cup D_{E}$ are used to learn a Monte-Carlo estimation
    of the Q-value: $\hat{Q}$, from which a learning policy can be derived greedily:
    $\pi(s)=\underset{a\in{\mathcal{A}}}{\arg\max}\hat{Q}(s,a)$. This policy $\pi$
    is further regularized by a loss function ${\mathcal{L}}(s,\pi_{E})$ to minimize
    its discrepancy from the expert policy decision.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example is the *Approximate Policy Iteration with Demonstration (APID)*
    algorithm, which was proposed by [[59](#bib.bib59)] and extended by [[95](#bib.bib95)].
    Different from *DPID* where both $D_{E}$ and $D_{\pi}$ are used for value estimation,
    the *APID* algorithm solely applies $D_{\pi}$ to approximate on the Q function.
    Expert demonstrations $D_{E}$ are used to learn the value function, which, given
    any state $s_{i}$, renders expert actions $\pi_{E}(s_{i})$ with higher $Q$-value
    margins compared with other actions that are not shown in $D_{E}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Q(s_{i},\pi_{E}(s_{i}))-\underset{a\in{\mathcal{A}}\backslash\pi_{E}(s_{i})}{\max}Q(s_{i},a)\geq
    1-\xi_{i}.\vspace{-0.05in}$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'The term $\xi_{i}$ is used to account for the case of imperfect demonstrations.
    [[95](#bib.bib95)] further extended the work of *APID* with a different evaluation
    loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\mathcal{L}}^{\pi}={\mathds{E}}_{(s,a)\sim{D_{\pi}}}\&#124;{\mathcal{T}}^{*}Q(s,a)-Q(s,a)\&#124;,$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where ${\mathcal{T}}^{*}Q(s,a)=R(s,a)+\gamma{\mathds{E}}_{s^{\prime}\sim p(.|s,a)}[\underset{a^{\prime}}{\max}Q(s^{\prime},a^{\prime})].$
    Their work theoretically converges to the optimal $Q$-function compared with *APID*,
    as ${\mathcal{L}}_{\pi}$ is minimizing the optimal Bellman residual instead of
    the empirical norm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to policy iteration, the following two approaches integrate demonstration
    data into the TD-learning framework, such as $Q$-learning. Specifically, [[92](#bib.bib92)]
    proposed the *DQfD* algorithm, which maintains two separate replay buffers to
    store demonstrated data and self-generated data, respectively, so that expert
    demonstrations can always be sampled with a certain probability. Their method
    leverages the refined priority replay mechanism [[99](#bib.bib99)] where the probability
    of sampling a transition $i$ is based on its priority $p_{i}$ with a temperature
    parameter $\alpha$: $P(i)=\frac{p_{i}^{\alpha}}{\sum_{k}{p_{k}^{\alpha}}}.$ Another
    algorithm named LfDS was proposed by [[96](#bib.bib96)], which draws a close connection
    to reward shaping (Section [5.1](#S5.SS1 "5.1 Reward Shaping ‣ 5 Transfer Learning
    Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")).
    LfDS builds the potential value of a state-action pair as the highest similarity
    between the given pair and the expert demonstrations. This augmented reward assigns
    more credits to state-actions that are more similar to expert demonstrations,
    encouraging the agent for expert-like behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides $Q$-learning, recent work has integrated LfD into *policy gradient* [[78](#bib.bib78),
    [93](#bib.bib93), [69](#bib.bib69), [97](#bib.bib97), [63](#bib.bib63)]. A representative
    work along this line is Generative Adversarial Imitation Learning (GAIL) [[69](#bib.bib69)].
    GAIL introduced the notion of *occupancy measure* $d_{\pi}$, which is the stationary
    state-action distributions derived from a policy $\pi$. Based on this notion,
    a new reward function is designed such that maximizing the accumulated new rewards
    encourages minimizing the distribution divergence between the *occupancy measure*
    of the current policy $\pi$ and the expert policy $\pi_{E}$. Specifically, the
    new reward is learned by adversarial training [[62](#bib.bib62)]: a discriminator
    $D$ is learned to distinguish interactions sampled from the current policy $\pi$
    and the expert policy $\pi_{E}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle J_{D}=\max_{D:{\mathcal{S}}\times{\mathcal{A}}\to(0,1)}{\mathds{E}}_{d_{\pi}}\log[1-D(s,a)]+{\mathds{E}}_{d_{E}}\log[D(s,a)]$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'Since $\pi_{E}$ is unknown, its state-action distribution $d_{E}$ is estimated
    based on the given expert demonstrations $D_{E}$. The output of the discriminator
    is used as new rewards to encourage distribution matching, with $r^{\prime}(s,a)=-\log(1-D(s,a))$.
    The RL process is naturally altered to perform distribution matching by min-max
    optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{\pi}\min_{D}J(\pi,D):$ | $\displaystyle={\mathds{E}}_{d_{\pi}}\log[1-D(s,a)]+{\mathds{E}}_{d_{E}}\log[D(s,a)].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The philosophy in *GAIL* of using expert demonstrations for distribution matching
    has inspired other LfD algorithms. For example, [[97](#bib.bib97)] extended GAIL
    with an algorithm called *Policy Optimization from Demonstrations (POfD)*, which
    combines the discriminator reward with the environment reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{\theta}={\mathds{E}}_{d_{\pi}}[r(s,a)]-\lambda D_{JS}[d_{\pi}&#124;&#124;d_{E}].$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'Both GAIL and POfD are under an *on-policy* RL framework. To further improve
    the sample efficiency of TL, some *off-policy* algorithms have been proposed,
    such as *DDPGfD* [[78](#bib.bib78)] which is built upon the DDPG framework. DDPGfD
    shares a similar idea as *DQfD* in that they both use a second replay buffer for
    storing demonstrated data, and each demonstrated sample holds a sampling priority
    $p_{i}$. For a demonstrated sample, its priority $p_{i}$ is augmented with a constant
    bias $\epsilon_{D}>0$ for encouraging more frequent sampling of expert demonstrations:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{i}=\delta_{i}^{2}+\lambda\&#124;\nabla_{a}Q(s_{i},a_{i}&#124;\theta^{Q})\&#124;^{2}+\epsilon+\epsilon_{D},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\delta_{i}$ is the TD-residual for transition, $\|\nabla_{a}Q(s_{i},a_{i}|\theta^{Q})\|^{2}$
    is the loss applied to the actor, and $\epsilon$ is a small positive constant
    to ensure all transitions are sampled with some probability. Another work also
    adopted the DDPG framework to learn from demonstrations [[93](#bib.bib93)]. Their
    approach differs from DDPGfD in that its objective function is augmented with
    a *Behavior Cloning Loss* to encourage imitating on provided demonstrations: ${\mathcal{L}}_{BC}=\sum_{i=1}^{|D_{E}|}||\pi(s_{i}|\theta_{\pi})-a_{i}||^{2}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To further address the issue of suboptimal demonstrations, in [[93](#bib.bib93)]
    the form of *Behavior Cloning Loss* is altered based on the critic output, so
    that only demonstration actions with higher $Q$ values will lead to the loss penalty:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\mathcal{L}}_{BC}=\sum_{i=1}^{&#124;D_{E}&#124;}\left\&#124;\pi(s_{i}&#124;\theta_{\pi})-a_{i}\right\&#124;^{2}{\mathbbm{1}}[Q(s_{i},a_{i})>Q(s_{i},\pi(s_{i}))].$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: There are several challenges faced by LfD, one of which is the *imperfect demonstrations*.
    Previous approaches usually presume near-oracle demonstrations. Towards tackling
    suboptimal demonstrations, [[59](#bib.bib59)] leveraged the hinge-loss function
    to allow occasional violations of the property that $Q(s_{i},\pi_{E}(s_{i}))-\underset{a\in{\mathcal{A}}\backslash\pi_{E}(s_{i})}{\max}Q(s_{i},a)\geq
    1$. Some other work uses regularized objective to alleviate overfitting on biased
    data [[92](#bib.bib92), [99](#bib.bib99)]. A different strategy is to leverage
    those sub-optimal demonstrations only to boost the initial learning stage. For
    instance, [[63](#bib.bib63)] proposed *Self-Adaptive Imitation Learning (SAIL)*,
    which learns from suboptimal demonstrations using generative adversarial training
    while gradually selecting self-generated trajectories with high qualities to replace
    less superior demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another challenge faced by LfD is *covariate drift* ([[100](#bib.bib100)]):
    demonstrations may be provided in limited numbers, which results in the learning
    agent lacking guidance on states that are unseen in the demonstration dataset.
    This challenge is aggravated in MDPs with sparse reward feedbacks, as the learning
    agent cannot obtain much supervision information from the environment either.
    Current efforts to address this challenge include encouraging explorations by
    using an entropy-regularized objective [[101](#bib.bib101)], decaying the effects
    of demonstration guidance by softening its regularization on policy learning over
    time [[102](#bib.bib102)], and introducing *disagreement regularizations* by training
    an ensemble of policies based on the given demonstrations, where the variance
    among policies serves as a negative reward function [[103](#bib.bib103)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'We summarize the above-discussed approaches in Table [II](#S5.T2 "TABLE II
    ‣ 5.2 Learning from Demonstrations ‣ 5 Transfer Learning Approaches Deep Dive
    ‣ Transfer Learning in Deep Reinforcement Learning: A Survey"). In general, demonstration
    data can help in both *offline* pre-training for better initialization and *online*
    RL for efficient exploration. During the RL phase, demonstration data can be used
    together with self-generated data to encourage expert-like behaviors (*DDPGfD,
    DQFD*), to shape value functions (*APID*), or to guide the policy update in the
    form of an auxiliary objective function (*PID,GAIL, POfD*). To validate the algorithm
    robustness given different knowledge resources, most *LfD* methods are evaluated
    using metrics that either indicate the performance under *limited* demonstrations
    (*nka*) or *suboptimal* demonstrations (*nka*). The integration of *LfD* with
    *off-policy* RL backbone makes it natural to adopt *pe* metrics for evaluating
    how learning efficiency can be further improved by knowledge transfer. Developing
    more general LfD approaches that are agnostic to RL frameworks and can learn from
    sub-optimal or limited demonstrations would be the ongoing focus for this research
    domain.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Optimality &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; guarantee &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Format of transferred demonstrations | RL framework | Evaluation metrics
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DPID | ✓ | Indicator binary-loss : ${\mathcal{L}}(s_{i})={\mathbbm{1}}\{\pi_{E}(s_{i})\neq\pi(s_{i})\}$
    | API | *ap, ar, nka* |'
  prefs: []
  type: TYPE_TB
- en: '| APID | ✗ | Hinge loss on the marginal-loss: $\big{[}{\mathcal{L}}(Q,\pi,\pi_{E})\big{]}_{+}$
    | API | *ap, ar, nta, nkq* |'
  prefs: []
  type: TYPE_TB
- en: '| APID extend | ✓ | Marginal-loss: ${\mathcal{L}}(Q,\pi,\pi_{E})$ | API | *ap,
    ar, nta, nkq* |'
  prefs: []
  type: TYPE_TB
- en: '| [[93](#bib.bib93)] | ✓ | Increasing sampling priority and behavior cloning
    loss | DDPG | *ap, ar, tr, pe, nkq* |'
  prefs: []
  type: TYPE_TB
- en: '| DQfD | ✗ | Cached transitions in the replay buffer | DQN | *ap, ar, tr* |'
  prefs: []
  type: TYPE_TB
- en: '| LfDS | ✗ | Reward shaping function | DQN | *ap, ar, tr* |'
  prefs: []
  type: TYPE_TB
- en: '| GAIL | ✓ | Reward shaping function: $-\lambda\log(1-D(s,a))$ | TRPO | *ap,
    ar, tr, pe, nka* |'
  prefs: []
  type: TYPE_TB
- en: '| POfD | ✓ | Reward shaping function: $r(s,a)-\lambda\log(1-D(s,a))$ | TRPO,PPO
    | *ap, ar, tr, pe, nka* |'
  prefs: []
  type: TYPE_TB
- en: '| DDPGfD (pe) | ✓ | Increasing sampling priority | DDPG | *ap, ar, tr, pe*
    |'
  prefs: []
  type: TYPE_TB
- en: '| SAIL | ✗ | Reward shaping function: $r(s,a)-\lambda\log(1-D(s,a))$ | DDPG
    | *ap, ar, tr, pe, nkq, nka* |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE II: A comparison of learning from demonstration approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Policy Transfer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Policy transfer is a TL approach where the external knowledge takes the form
    of pre-trained policies from one or multiple source domains. Work discussed in
    this section is built upon a *many-to-one* problem setting, described as below:'
  prefs: []
  type: TYPE_NORMAL
- en: Policy Transfer.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A set of teacher policies $\pi_{E_{1}},\pi_{E_{2}},\dots,\pi_{E_{K}}$ are trained
    on a set of source domains ${\mathcal{M}}_{1},{\mathcal{M}}_{2},\dots,{\mathcal{M}}_{K}$,
    respectively. A student policy $\pi$ is learned for a target domain by leveraging
    knowledge from $\{\pi_{E_{i}}\}_{i=1}^{K}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the *one-to-one* scenario with only one teacher policy, one can consider
    it as a special case of the above with $K=1$. Next, we categorize recent work
    of policy transfer into two techniques: *policy distillation* and *policy reuse*.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Transfer Learning via Policy Distillation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The idea of *knowledge distillation* has been applied to the field of RL to
    enable policy distillation. Knowledge distillation was first proposed by [[104](#bib.bib104)]
    as an approach of knowledge ensemble from multiple teacher models into a single
    student model. Conventional policy distillation approaches transfer the teacher
    policy following a supervised learning paradigm [[105](#bib.bib105), [106](#bib.bib106)].
    Specifically, a student policy is learned by minimizing the divergence of action
    distributions between the teacher policy $\pi_{E}$ and student policy $\pi_{\theta}$,
    which is denoted as ${\mathcal{H}}^{\times}(\pi_{E}(\tau_{t})|\pi_{\theta}(\tau_{t}))$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{\theta}{\mathds{E}}_{\tau\sim\pi_{E}}[\sum_{t=1}^{&#124;\tau&#124;}\nabla_{\theta}{\mathcal{H}}^{\times}(\pi_{E}(\tau_{t})&#124;\pi_{\theta}(\tau_{t}))].$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'The above expectation is taken over trajectories sampled from the teacher policy
    $\pi_{E}$, hence this approach is called *teacher distillation*. One example along
    this line is [[105](#bib.bib105)], in which $N$ teacher policies are learned for
    $N$ source tasks separately, and each teacher yields a dataset $D^{E}=\{s_{i},{\bm{q}}_{i}\}_{i=0}^{N}$
    consisting of observations $s$ and vectors of the corresponding $Q$-values ${\bm{q}}$,
    such that ${\bm{q}}_{i}=[Q(s_{i},a_{1}),Q(s_{i},a_{2}),...|a_{j}\in{\mathcal{A}}]$.
    Teacher policies are further distilled to a single student $\pi_{\theta}$ by minimizing
    the KL-Divergence between each teacher $\pi_{E_{i}}(a|s)$ and the student $\pi_{\theta}$,
    approximated using the dataset $D^{E}$: $\min_{\theta}{\mathcal{D}}_{KL}(\pi^{E}|\pi_{\theta})\approx\sum_{i=1}^{|D^{E}|}\text{softmax}\left(\frac{{\bm{q}}^{E}_{i}}{\tau}\right)\ln\left(\frac{\text{softmax}({\bm{q}}_{i}^{E})}{\text{softmax}({\bm{q}}_{i}^{\theta})}\right)$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another policy distillation approach is *student distillation* [[60](#bib.bib60),
    [51](#bib.bib51)], which is resemblant to teacher distillation except that during
    the optimization step, the objective expectation is taken over trajectories sampled
    from the student policy instead of the teacher policy, i.e.: $\min_{\theta}{\mathds{E}}_{\tau\sim\pi_{\theta}}\left[\sum_{t=1}^{|\tau|}\nabla_{\theta}{\mathcal{H}}^{\times}(\pi_{E}(\tau_{t})|\pi_{\theta}(\tau_{t}))\right]$.
    [[60](#bib.bib60)] summarized related work on both kinds of distillation approaches.
    Although it is feasible to combine both distillation approaches [[100](#bib.bib100)],
    we observe that more recent work focuses on student distillation, which empirically
    shows better exploration ability compared to teacher distillation, especially
    when the teacher policies are *deterministic*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking an alternative perspective, there are two approaches of policy distillation:
    (1) minimizing the cross-entropy between the teacher and student policy distributions
    over actions [[51](#bib.bib51), [107](#bib.bib107)]; and (2) maximizing the probability
    that the teacher policy will visit trajectories generated by the student, i.e. $\max_{\theta}P(\tau\sim\pi_{E}|\tau\sim\pi_{\theta})$
    [[50](#bib.bib50), [108](#bib.bib108)]. One example of approach (1) is the *Actor-mimic*
    algorithm [[51](#bib.bib51)]. This algorithm distills the knowledge of expert
    agents into the student by minimizing the cross entropy between the student policy
    $\pi_{\theta}$ and each teacher policy $\pi_{E_{i}}$ over actions: ${\mathcal{L}}^{i}(\theta)=\sum_{a\in{\mathcal{A}}_{E_{i}}}\pi_{E_{i}}(a|s)\log_{\pi_{\theta}}(a|s)$,
    where each teacher agent is learned using a DQN framework. The teacher policy
    is therefore derived from the Boltzmann distributions over the $Q$-function output:
    $\pi_{E_{i}}(a|s)=\frac{e^{\tau^{-1}Q_{E_{i}}(s,a)}}{\sum_{a^{\prime}\in{\mathcal{A}}_{E_{i}}}e^{\tau^{-1}Q_{E_{i}}(s,a^{\prime})}}$.
    An instantiation of approach (2) is the *Distral* algorithm [[50](#bib.bib50)].
    which learns a *centroid* policy $\pi_{\theta}$ that is derived from $K$ teacher
    policies. The knowledge in each teacher $\pi_{E_{i}}$ is distilled to the centroid
    and get transferred to the student, while both the transition dynamics ${\mathcal{T}}_{i}$
    and reward distributions ${\mathcal{R}}_{i}$ for source domain ${\mathcal{M}}_{i}$
    are heterogeneous. The student policy is learned by maximizing a multi-task learning
    objective $\max_{\theta}\sum_{i=1}^{K}J(\pi_{\theta},\pi_{E_{i}})$, where'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle J(\pi_{\theta},\pi_{E_{i}})=\sum_{t}$ | $\displaystyle{\mathds{E}}_{(s_{t},a_{t})\sim\pi_{\theta}}\Big{[}\sum_{t\geq
    0}\gamma^{t}(r_{i}(a_{t},s_{t})+$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\frac{\alpha}{\beta}\log\pi_{\theta}(a_{t}&#124;s_{t})-\frac{1}{\beta}\log(\pi_{E_{i}}(a_{t}&#124;s_{t})))\Big{]},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'in which both $\log\pi_{\theta}(a_{t}|s_{t})$ and $\pi_{\theta}$ are used as
    augmented rewards. Therefore, the above approach also draws a close connection
    to Reward Shaping (Section [5.1](#S5.SS1 "5.1 Reward Shaping ‣ 5 Transfer Learning
    Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")).
    In effect, the $\log\pi_{\theta}(a_{t}|s_{t})$ term guides the learning policy
    $\pi_{\theta}$ to yield actions that are more likely to be generated by the teacher
    policy, whereas the entropy term $-\log(\pi_{E_{i}}(a_{t}|s_{t})$ encourages exploration.
    A similar approach was proposed by [[107](#bib.bib107)] which only uses the cross-entropy
    between teacher and student policy $\lambda{\mathcal{H}}(\pi_{E}(a_{t}|s_{t})||\pi_{\theta}(a_{t}|s_{t}))$
    to reshape rewards. Moreover, they adopted a dynamically fading coefficient to
    alleviate the effect of the augmented reward so that the student policy becomes
    independent of the teachers after certain optimization iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Transfer Learning via Policy Reuse
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Policy reuse* directly reuses policies from source tasks to build the target
    policy. The notion of policy reuse was proposed by [[109](#bib.bib109)], which
    directly learns the target policy as a weighted combination of different source-domain
    policies, and the probability for each source domain policy to be used is related
    to its expected performance gain in the target domain: $P(\pi_{E_{i}})=\frac{\exp{(tW_{i})}}{\sum_{j=0}^{K}\exp{(tW_{j})}},$
    where $t$ is a dynamic temperature parameter that increases over time. Under a
    $Q$-learning framework, the $Q$-function of the target policy is learned in an
    iterative scheme: during every learning episode, $W_{i}$ is evaluated for each
    expert policy $\pi_{E_{i}}$, and $W_{0}$ is obtained for the learning policy,
    from which a reuse probability $P$ is derived. Next, a behavior policy is sampled
    from this probability $P$. After each training episode, both $W_{i}$ and the temperature
    $t$ for calculating the reuse probability is updated accordingly. One limitation
    of this approach is that the $W_{i}$, i.e. the expected return of each expert
    policy on the target task, needs to be evaluated frequently. This work was implemented
    in a tabular case, leaving the scalability issue unresolved. More recent work
    by [[110](#bib.bib110)] extended the *policy improvement* theorem [[111](#bib.bib111)]
    from one to multiple policies, which is named as *Generalized Policy Improvement*.
    We refer its main theorem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*[Generalized Policy Improvement (GPI)]* Let $\{\pi_{i}\}_{i=1}^{n}$ be $n$
    policies and let $\{\hat{Q}^{\pi_{i}}\}_{i=1}^{n}$ be their approximated action-value
    functions, s.t: $\Big{|}Q^{\pi_{i}}(s,a)-\hat{Q}^{\pi_{i}}(s,a)\Big{|}\leq\epsilon~{}\forall
    s\in{\mathcal{S}},a\in{\mathcal{A}}\text{, and }i\in[n]$. Define $\pi(s)=\underset{a}{\arg\max}~{}\underset{i}{\max}\hat{Q}^{\pi_{i}}(s,a)$,
    then: $Q^{\pi}(s,a)\geq\underset{i}{\max}Q^{\pi_{i}}(s,a)-\frac{2}{1-\gamma}\epsilon$,
    $\forall~{}s\in{\mathcal{S}},a\in{\mathcal{A}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this theorem, a policy improvement approach can be naturally derived
    by greedily choosing the action which renders the highest $Q$-value among all
    policies for a given state. Another work along this line is [[110](#bib.bib110)],
    in which an expert policy $\pi_{E_{i}}$ is also trained on a different source
    domain ${\mathcal{M}}_{i}$ with reward function ${\mathcal{R}}_{i}$, so that $Q^{\pi}_{{\mathcal{M}}_{0}}(s,a)\neq
    Q^{\pi}_{{\mathcal{M}}_{i}}(s,a)$. To efficiently evaluate the $Q$-functions of
    different source policies in the target MDP, a disentangled representation ${\bm{\psi}}(s,a)$
    over the states and actions is learned using neural networks and is generalized
    across multiple tasks. Next, a task (reward) mapper ${\mathbf{w}}_{i}$ is learned,
    based on which the $Q$-function can be derived: $Q^{\pi}_{i}(s,a)={\bm{\psi}}(s,a)^{T}{\mathbf{w}}_{i}.$
    [[110](#bib.bib110)] proved that the loss of GPI is bounded by the difference
    between the source and the target tasks. In addition to policy-reuse, their approach
    involves learning a shared representation ${\bm{\psi}}(s,a)$, which is also a
    form of transferred knowledge and will be elaborated more in Section [5.5.2](#S5.SS5.SSS2
    "5.5.2 Disentangling Representations ‣ 5.5 Representation Transfer ‣ 5 Transfer
    Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'We summarize the abovementioned policy transfer approaches in Table [III](#S5.T3
    "TABLE III ‣ 5.3.2 Transfer Learning via Policy Reuse ‣ 5.3 Policy Transfer ‣
    5 Transfer Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement
    Learning: A Survey"). In general, policy transfer can be realized by knowledge
    distillation, which can be either optimized from the student’s perspecive (student
    distillation), or from the teacher’s perspective (teacher distillation) Alternatively,
    teacher policies can also be directly reused to update the target policy. Regarding
    evaluation, most of the abovementioned work has investigated a multi-teacher transfer
    scenario, hence the *generalization* ability or *robustness* is largely evaluated
    on metrics such as *performance sensitivity(ps)* (e.g.  performance given different
    numbers of teacher policies or source tasks ). *Performance with fixed epochs
    (pe)* is another commonly shared metric to evaluate how the learned policy can
    quickly adapt to the target domain. All approaches discussed so far presumed one
    or multiple expert policies, which are always at the disposal of the learning
    agent. Open questions along this line include How to leverage imperfect policies
    for knowledge transfer, or How to refer to teacher policies within a budget.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Paper | Transfer approach | MDP difference | RL framework | Evaluation metrics
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[105](#bib.bib105)] | Distillation | ${\mathcal{S}},{\mathcal{A}}$ | DQN
    | $ap,ar$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[106](#bib.bib106)] | Distillation | ${\mathcal{S}},{\mathcal{A}}$ | DQN
    | $ap,ar,pe,ps$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[51](#bib.bib51)] | Distillation | ${\mathcal{S}},{\mathcal{A}}$ | Soft
    Q-learning | $ap,ar,tr$, $pe,ps$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[50](#bib.bib50)] | Distillation | ${\mathcal{S}},{\mathcal{A}}$ | A3C |
    $ap,ar,pe$, $tt$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[109](#bib.bib109)] | Reuse | ${\mathcal{R}}$ | Tabular Q-learning | $ap,ar,ps,tr$
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[110](#bib.bib110)] | Reuse | ${\mathcal{R}}$ | DQN | $ap,ar,pe,ps$ |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: A comparison of policy transfer approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Inter-Task Mapping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we review TL approaches that utilize mapping functions between
    the source and the target domains to assist knowledge transfer. Research in this
    domain can be analyzed from two perspectives: (1) *which domain does the mapping
    function apply to*, and (2) *how is the mapped representation utilized*. Most
    work discussed in this section shares a common assumption as below:'
  prefs: []
  type: TYPE_NORMAL
- en: Assumption.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One-to-one mappings exist between the source domain ${\mathcal{M}}_{s}$ and
    the target domain ${\mathcal{M}}_{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier work along this line requires a *given mapping function* [[66](#bib.bib66),
    [112](#bib.bib112)]. One examples is [[66](#bib.bib66)] which assumes that each
    target state (action) has a unique correspondence in the source domain, and two
    mapping functions $X_{S},X_{A}$ are provided over the state space and the action
    space, respectively, so that $X_{S}({\mathcal{S}}^{t})\to{\mathcal{S}}^{s}$, $X_{A}({\mathcal{A}}^{t})\to{\mathcal{A}}^{s}$.
    Based on $X_{S}$ and $X_{A}$, a mapping function over the $Q$-values $M(Q_{s})\to
    Q_{t}$ can be derived accordingly. Another work is done by [[112](#bib.bib112)]
    which transfers *advice* as the knowledge between two domains. In their settings,
    the *advice* comes from a human expert who provides the mapping function over
    the $Q$-values in the source domain and transfers it to the learning policy for
    the target domain. This advice encourages the learning agent to prefer certain
    good actions over others, which equivalently provides a relative ranking of actions
    in the new task.
  prefs: []
  type: TYPE_NORMAL
- en: 'More later research tackles the inter-task mapping problem by *learning* a
    mapping function [[113](#bib.bib113), [114](#bib.bib114), [115](#bib.bib115)].
    Most work learns a mapping function over the state space or a subset of the state
    space. In their work, state representations are usually divided into *agent-specific*
    and *task-specific* representations, denoted as $s_{agent}$ and $s_{env}$, respectively.
    In [[113](#bib.bib113)] and [[114](#bib.bib114)], the mapping function is learned
    on the *agent-specific* sub state, and the mapped representation is applied to
    reshape the immediate reward. For [[113](#bib.bib113)], the invariant feature
    space mapped from $s_{agent}$ can be applied across agents who have distinct action
    space but share some morphological similarity. Specifically, they assume that
    both agents have been trained on the same *proxy* task, based on which the mapping
    function is learned. The mapping function is learned using an encoder-decoder
    structure [[116](#bib.bib116)] to largely reserve information about the source
    domain. For transferring knowledge from the source agent to a new task, the environment
    reward is augmented with a shaped reward term to encourage the target agent to
    imitate the source agent on an embedded feature space:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle r^{\prime}(s,\cdot)=\alpha\left\&#124;f(s^{s}_{agent};\theta_{f})-g(s^{t}_{agent};\theta_{g})\right\&#124;,$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: where $f(s^{s}_{agent})$ is the agent-specific state in the source domain, and
    $g(s^{t}_{agent})$ is for the target domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another work is [[115](#bib.bib115)] which applied the Unsupervised Manifold
    Alignment (UMA) method [[117](#bib.bib117)] to automatically learn the state mapping.
    Their approach requires collecting trajectories from both the source and the target
    domain to learn such a mapping. While applying policy gradient learning, trajectories
    from the target domain ${\mathcal{M}}_{t}$ are first mapped back to the source:
    $\tau_{t}\to\tau_{s}$, then an expert policy in the source domain is applied to
    each initial state of those trajectories to generate near-optimal trajectories
    $\overset{\sim}{\tau_{s}}$, which are further mapped to the target domain: $\overset{\sim}{\tau_{s}}\to\overset{\sim}{\tau_{t}}$.
    The deviation between $\overset{\sim}{\tau_{t}}$ and $\tau_{t}$ are used as a
    loss to be minimized in order to improve the target policy. Similar ideas of using
    UMA for inter-task mapping can also be found in [[118](#bib.bib118)] and [[119](#bib.bib119)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to approaches that utilizes mapping over states or actions, [[120](#bib.bib120)]
    proposed to learn an inter-task mapping over the *transition dynamics* space:
    ${\mathcal{S}}\times{\mathcal{A}}\times{\mathcal{S}}$. Their work assumes that
    the source and target domains are different in terms of the transition space dimensionality.
    Transitions from both the source domain $\langle s^{s},a^{s},s^{\prime s}\rangle$
    and the target domain $\langle s^{t},a^{t},s^{\prime t}\rangle$ are mapped to
    a latent space $Z$. Given the latent feature representations, a similarity measure
    can be applied to find a correspondence between the source and target task triplets.
    Triplet pairs with the highest similarity in this feature space $Z$ are used to
    learn a mapping function ${\mathcal{X}}$: $\langle s^{t},a^{t},s^{\prime t}\rangle={\mathcal{X}}(\langle
    s^{s},a^{s},s^{\prime s}\rangle)$. After the transition mapping, states sampled
    from the expert policy in the source domain can be leveraged to render beneficial
    states in the target domain, which assists the target agent learning with a better
    initialization performance. A similar idea of mapping transition dynamics can
    be found in [[121](#bib.bib121)], which, however, requires a stronger assumption
    on the similarity of the transition probability and the state representations
    between the source and the target domains.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As summarized in Table [IV](#S5.T4 "TABLE IV ‣ 5.4 Inter-Task Mapping ‣ 5 Transfer
    Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning:
    A Survey"), for TL approaches that utilize an inter-task mapping, the mapped knowledge
    can be (a subset of) the state space [[113](#bib.bib113), [114](#bib.bib114)],
    the $Q$-function [[66](#bib.bib66)], or (representations of) the state-action-sate
    transitions  [[120](#bib.bib120)]. In addition to being directly applicable in
    the target domain [[120](#bib.bib120)], the mapped representation can also be
    used as an augmented shaping reward [[114](#bib.bib114), [113](#bib.bib113)] or
    a loss objective [[115](#bib.bib115)] in order to guide the agent learning in
    the target domain. Most inter-task mapping methods tackle domains with moderate
    state-action space dimensions, such as maze tasks or tabular MDPs, where the goal
    can be reaching a target state with a minimal number of transitions. Accordingly,
    *tt* has been used to measure TL performance. For tasks with limited and discrete
    state-action space, evaluation is also conducted with different number of initial
    states collected in the target domain (*nka*).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; framework &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MDP &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; difference &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mapping &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; function &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Usage of mapping |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Evaluation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; metrics &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[66](#bib.bib66)] | *SARSA* | ${\mathcal{S}}_{t}\neq{\mathcal{S}}_{t},{\mathcal{A}}_{s}\neq{\mathcal{A}}_{t}$
    | $M(Q_{s})$ $\to$ $Q_{t}$ | $Q$ value reuse | *ap, ar, tt, tr* |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[112](#bib.bib112)] | *Q-learning* | ${\mathcal{A}}_{s}\neq{\mathcal{A}}_{t}$,
    ${\mathcal{R}}_{s}\neq{\mathcal{R}}_{t}$ | $M(Q_{s})$ $\to$ *advice* | Relative
    $Q$ ranking | *ap, ar, tr* |'
  prefs: []
  type: TYPE_TB
- en: '| [[113](#bib.bib113)] | $-$ | ${\mathcal{S}}_{s}\neq{\mathcal{S}}_{t}$ | $M(s_{t})$
    $\to$ $r^{\prime}$ | Reward shaping | *ap, ar, pe, tr* |'
  prefs: []
  type: TYPE_TB
- en: '| [[114](#bib.bib114)] | *SARSA*$(\lambda)$ | ${\mathcal{S}}_{s}\neq{\mathcal{S}}_{t}$
    ${\mathcal{R}}_{s}\neq{\mathcal{R}}_{t}$ | $M(s_{t})$ $\to$ $r^{\prime}$ | Reward
    shaping | *ap, ar, pe, tt* |'
  prefs: []
  type: TYPE_TB
- en: '| [[115](#bib.bib115)] | Fitted Value Iteration | ${\mathcal{S}}_{s}\neq{\mathcal{S}}_{t}$
    | $M(s_{s})$ $\to$ $s_{t}$ | Penalty loss on state deviation from expert policy
    | *ap, ar, pe, tr* |'
  prefs: []
  type: TYPE_TB
- en: '| [[121](#bib.bib121)] | Fitted Q Iteration | ${\mathcal{S}}_{s}\times{\mathcal{A}}_{s}\neq{\mathcal{S}}_{t}\times{\mathcal{A}}_{t}$
    | $M\big{(}(s_{s},a_{s},s^{\prime}_{s})$ $\to$ $(s_{t},a_{t},s^{\prime}_{t})\big{)}$
    | Reduce random exploration | *ap, ar, pe, tr, nta* |'
  prefs: []
  type: TYPE_TB
- en: '| [[120](#bib.bib120)] | $-$ | ${\mathcal{S}}_{s}\times{\mathcal{A}}_{s}\neq{\mathcal{S}}_{t}\times{\mathcal{A}}_{t}$
    | $M\big{(}(s_{s},a_{s},s^{\prime}_{s})$ $\to$ $(s_{t},a_{t},s^{\prime}_{t})\big{)}$
    | Reduce random exploration | *ap, ar, pe, tr, nta* |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: A comparison of inter-task mapping approaches. “$-$” indicates no
    RL framework constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Representation Transfer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section review approaches that transfer knowledge in the form of representations
    learned by deep neural networks. They are built upon the following consensual
    assumption:'
  prefs: []
  type: TYPE_NORMAL
- en: Assumption.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*[Existence of Task-Invariance Subspace]*'
  prefs: []
  type: TYPE_NORMAL
- en: The state space (${\mathcal{S}}$), action space (${\mathcal{A}}$), or the reward
    space (${\mathcal{R}}$) can be disentangled into orthogonal subspaces, which are
    task-invariant such that knowledge can be transferred between domains on the universal
    subspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'We organize recent work along this line into two subtopics: 1) approaches that
    directly *reuse* representations from the source domain (Section [5.5.1](#S5.SS5.SSS1
    "5.5.1 Reusing Representations ‣ 5.5 Representation Transfer ‣ 5 Transfer Learning
    Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")),
    and 2) approaches that learn to *disentangle* the source domain representations
    into independent sub-feature representations, some of which are on the universal
    feature space shared by both the source and the target domains (Section [5.5.2](#S5.SS5.SSS2
    "5.5.2 Disentangling Representations ‣ 5.5 Representation Transfer ‣ 5 Transfer
    Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning:
    A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.1 Reusing Representations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A representative work of reusing representations is [[122](#bib.bib122)], which
    proposed the *progressive neural network* structure to enable knowledge transfer
    across multiple RL tasks in a progressive way. A progressive network is composed
    of multiple *columns*, and each column is a policy network for one specific task.
    It starts with one single column for training the first task, and then the number
    of columns increases with the number of new tasks. While training on a new task,
    neuron weights on the previous columns are frozen, and representations from those
    frozen tasks are applied to the new column via a collateral connection to assist
    in learning the new task.
  prefs: []
  type: TYPE_NORMAL
- en: '*Progressive network* comes with a cost of large network structures, as the
    network grows proportionally with the number of incoming tasks. A later framework
    called *PathNet* alleviates this issue by learning a network with a fixed size [[123](#bib.bib123)].
    *PathNet* contains *pathways*, which are subsets of neurons whose weights contain
    the knowledge of previous tasks and are frozen during training on new tasks. The
    population of *pathway* is evolved using a tournament selection genetic algorithm [[124](#bib.bib124)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach of reusing representations for TL is modular networks [[52](#bib.bib52),
    [53](#bib.bib53), [125](#bib.bib125)]. For example, [[52](#bib.bib52)] proposed
    to decompose the policy network into a task-specific module and agent-specific
    module. Specifically, let $\pi$ be a policy performed by any agent (robot) $r$
    over the task ${\mathcal{M}}_{k}$ as a function $\bm{\phi}$ over states $s$, it
    can be decomposed into two sub-modules $g_{k}$ and $f_{r}$, i.e.:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\pi(s):=\bm{\phi}(s_{env},s_{agent})=f_{r}(g_{k}(s_{env}),s_{agent}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $f_{r}$ is the agent-specific module and $g_{k}$ is the task-specific
    module. Their core idea is that the task-specific module can be applied to different
    agents performing the same task, which serves as a transferred knowledge. Accordingly,
    the agent-specific module can be applied to different tasks for the same agent.
  prefs: []
  type: TYPE_NORMAL
- en: A model-based approach along this line is [[125](#bib.bib125)], which learns
    a model to map the state observation $s$ to a latent-representation $z$. The transition
    probability is modeled on the latent space instead of the original state space,
    i.e. $\hat{z}_{t+1}=f_{\theta}(z_{t},a_{t})$, where $\theta$ is the parameter
    of the transition model, $z_{t}$ is the latent-representation of the state observation,
    and $a_{t}$ is the action accompanying that state. Next, a *reward* module learns
    the value function as well as the policy from the latent space $z$ using an actor-critic
    framework. One potential benefit of this latent representation is that knowledge
    can be transferred across tasks that have different rewards but share the same
    transition dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.2 Disentangling Representations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Methods discussed in this section mostly focus on learning a *disentangled*
    representation. Specifically, we elaborate on TL approaches that are derived from
    two techniques: *Successor Representation (SR)* and *Universal Value Function
    Approximating (UVFA)*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Successor Representations (SR) is an approach to decouple the state features
    of a domain from its reward distributions. It enables knowledge transfer across
    multiple domains: $\bm{{\mathcal{M}}}=\{{\mathcal{M}}_{1},{\mathcal{M}}_{2},\dots,{\mathcal{M}}_{K}\}$,
    so long as the only difference among them is the reward distributions: ${\mathcal{R}}_{i}\neq{\mathcal{R}}_{j}$.
    SR was originally derived from neuroscience, until [[126](#bib.bib126)] proposed
    to leverage it as a generalization mechanism for state representations in the
    RL domain.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Different from the $v$-value or $Q$-value that describes states as dependent
    on the reward function, SR features a state based on the *occupancy measure* of
    its *successor* states. Specifically, SR decomposes the value function of any
    policy into two independent components, ${\bm{\psi}}$ and $R$: $V^{\pi}(s)=\sum_{s^{\prime}}{\bm{\psi}}(s,s^{\prime}){\mathbf{w}}(s^{\prime})$,
    where ${\mathbf{w}}(s^{\prime})$ is a reward mapping function that maps states
    to scalar rewards, and ${\bm{\psi}}$ is the SR which describes any state $s$ as
    the occupancy measure of the future occurred states when following $\pi$, with
    ${\mathbbm{1}}[S=s^{\prime}]=1$ as an indicator function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\bm{\psi}}(s,s^{\prime})={\mathds{E}}_{\pi}[\sum_{i=t}^{\infty}\gamma^{i-t}{\mathbbm{1}}[S_{i}=s^{\prime}]&#124;S_{t}=s].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The *successor* nature of SR makes it learnable using any TD-learning algorithms.
    Especially, [[126](#bib.bib126)] proved the feasibility of learning such representation
    in a tabular case, in which the state transitions can be described using a matrix.
    SR was later extended by [[110](#bib.bib110)] from three perspectives: (i) the
    feature domain of SR is extended from states to state-action pairs; (ii) deep
    neural networks are used as function approximators to represent the SR ${\bm{\psi}}^{\pi}(s,a)$
    and the *reward mapper* ${\mathbf{w}}$; (iii) Generalized policy improvement (GPI)
    algorithm is introduced to accelerate policy transfer for multi-tasks (Section [5.3.2](#S5.SS3.SSS2
    "5.3.2 Transfer Learning via Policy Reuse ‣ 5.3 Policy Transfer ‣ 5 Transfer Learning
    Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")).
    These extensions, however, are built upon a stronger assumption about the MDP:'
  prefs: []
  type: TYPE_NORMAL
- en: Assumption.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*[Linearity of Reward Distributions]* The reward functions of all tasks can
    be computed as a linear combination of a fixed set of features: $r(s,a,s^{\prime})=\phi(s,a,s^{\prime})^{\top}{\mathbf{w}}$,
    where $\phi(s,a,s^{\prime})\in\mathbb{R}^{d}$ denotes the latent representation
    of the state transition, and ${\mathbf{w}}\in\mathbb{R}^{d}$ is the task-specific
    reward mapper.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this assumption, SR can be decoupled from the rewards when evaluating
    the Q-function of any policy $\pi$ in a task. The advantage of SR is that, when
    the knowledge of ${\bm{\psi}}^{\pi}(s,a)$ in the source domain ${\mathcal{M}}_{s}$
    is observed, one can quickly get the performance evaluation of the same policy
    in the target domain ${\mathcal{M}}_{t}$ by replacing ${\mathbf{w}}_{s}$ with
    ${\mathbf{w}}_{t}$: $Q^{\pi}_{{\mathcal{M}}_{t}}={\bm{\psi}}^{\pi}(s,a){\mathbf{w}}_{t}.$
    Similar ideas of learning SR as a TD-algorithm on a latent representation $\phi(s,a,s^{\prime})$
    can also be found in [[127](#bib.bib127), [128](#bib.bib128)]. Specifically, the
    work of [[127](#bib.bib127)] was developed based on a weaker assumption about
    the reward function: Instead of requiring linearly-decoupled rewards, the latent
    space $\phi(s,a,s^{\prime})$ is learned in an encoder-decoder structure to ensure
    that the information loss is minimized when mapping states to the latent space.
    This structure, therefore, comes with an extra cost of learning a decoder $f_{d}$
    to reconstruct the state: $f_{d}(\phi(s_{t}))\approx s_{t}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An intriguing question faced by the SR approach is: *Is there a way that evades
    the linearity assumption about reward functions and still enables learning the
    SR without extra modular cost?* An extended work of SR [[67](#bib.bib67)] answered
    this question affirmatively, which proved that the reward functions does not necessarily
    have to follow the linear structure, yet at the cost of a looser performance lower-bound
    while applying the GPI approach for policy improvement. Especially, rather than
    learning a reward-agnostic latent feature $\phi(s,a,s^{\prime})\in\mathbb{R}^{d}$
    for multiple tasks, [[67](#bib.bib67)] aims to learn a matrix ${\bm{\phi}}(s,a,s^{\prime})\in\mathbb{R}^{D\times
    d}$ to interpret the basis functions of the latent space instead, where $D$ is
    the number of seen tasks. Assuming $k$ out of $D$ tasks are linearly independent,
    this matrix forms $k$ basis functions for the latent space. Therefore, for any
    unseen task ${\mathcal{M}}_{i}$, its latent features can be built as a linear
    combination of these basis functions, as well as its reward functions $r_{i}(s,a,s^{\prime})$.
    Based on the idea of basis-functions for a task’s latent space, they proposed
    that ${\bm{\phi}}(s,a,s^{\prime})$ can be approximated as learning $\mathbb{R}(s,a,s^{\prime})$
    directly, where $\mathbb{R}(s,a,s^{\prime})\in\mathbb{R}^{D}$ is a vector of reward
    functions for each seen task:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{R}(s,a,s^{\prime})=\big{[}r_{1}(s,a,s^{\prime});r_{2}(s,a,s^{\prime}),\dots,r_{D}(s,a,s^{\prime})\big{]}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Accordingly, learning ${\bm{\psi}}(s,a)$ for any policy $\pi_{i}$ in ${\mathcal{M}}_{i}$
    becomes equivalent to learning a collection of Q-functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\psi}^{\pi_{i}}(s,a)=\big{[}Q^{\pi_{i}}_{1}(s,a),Q^{\pi_{i}}_{2}(s,a),\dots,Q^{\pi_{i}}_{D}(s,a)\big{]}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: A similar idea of using reward functions as features to represent unseen tasks
    is also proposed by [[129](#bib.bib129)], which assumes the ${\bm{\psi}}$ and
    ${\mathbf{w}}$ as observable quantities from the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Universal Function Approximation (UVFA) is an alternative approach of learning
    disentangled state representations [[64](#bib.bib64)]. Same as SR, UVFA allows
    TL for multiple tasks which differ only by their reward functions (goals). Different
    from SR which focuses on learning a reward-agnostic state representation, UVFA
    aims to find a function approximator that is generalized for both states and goals.
    The UVFA framework is built on a specific problem setting of *goal conditional
    RL*: *task goals are defined in terms of states, e.g. given the state space ${\mathcal{S}}$
    and the goal space ${\mathcal{G}}$, it satisfies that ${\mathcal{G}}\subseteq{\mathcal{S}}$.*
    One instantiation of this problem setting can be an agent exploring different
    locations in a maze, where the goals are described as certain locations inside
    the maze. Under this problem setting, a UVFA module can be decoupled into a state
    embedding $\phi(s)$ and a goal embedding ${\bm{\psi}}(g)$, by applying the technique
    of matrix factorization to a reward matrix describing the goal-conditional task.'
  prefs: []
  type: TYPE_NORMAL
- en: One merit of UVFA resides in its transferrable embedding $\phi(s)$ across tasks
    which only differ by goals. Another benefit is its ability of continual learning
    when the set of goals keeps expanding over time. On the other hand, a key challenge
    of UVFA is that applying the matrix factorization is time-consuming, which makes
    it a practical concern for complex environments with large state space $|{\mathcal{S}}|$.
    Even with the learned embedding networks, the third stage of fine-tuning these
    networks via end-to-end training is still necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'UVFA has been connected to SR by [[67](#bib.bib67)], in which a set of independent
    rewards (tasks) themselves can be used as features for state representations.
    Another extended work that combines UVFA with SR is called Universal Successor
    Feature Approximator (USFA), which is proposed by [[130](#bib.bib130)]. Following
    the same linearity assumption, USFA is proposed as a function over a triplet of
    the state, action, and a policy embedding $z$: $\phi(s,a,z):{\mathcal{S}}\times{\mathcal{A}}\times\mathbb{R}^{k}\rightarrow\mathbb{R}^{d}$,
    where $z$ is the output of a *policy-encoding mapping* $z=e(\pi):{\mathcal{S}}\times{\mathcal{A}}\rightarrow\mathbb{R}^{k}$.
    Based on USFA, the $Q$-function of any policy $\pi$ for a task specified by ${\mathbf{w}}$
    can be formularized as the product of a reward-agnostic Universal Successor Feature
    (USF) ${\bm{\psi}}$ and a reward mapper ${\mathbf{w}}$: $Q(s,a,{\mathbf{w}},z)={\bm{\psi}}(s,a,z)^{\top}{\mathbf{w}}.$
    Facilitated by the disentangled rewards and policy generalization, [[130](#bib.bib130)]
    further introduced a generalized TD-error as a function over tasks ${\mathbf{w}}$
    and policy $z$, which allows them to approximate the $Q$-function of any policy
    on any task using a TD-algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.3 Summary and Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We provide a summary of the discussed work in this section in Table [V](#S5.T5
    "TABLE V ‣ 5.5.3 Summary and Discussion ‣ 5.5 Representation Transfer ‣ 5 Transfer
    Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning:
    A Survey"). Representation transfer can facilitate TL in multiple ways based on
    assumptions about certain task-invariant property. Some assume that tasks are
    different only in terms of their reward distributions. Other stronger assumptions
    include (i) decoupled dynamics, rewards [[110](#bib.bib110)], or policies [[130](#bib.bib130)]
    from the $Q$-function representations, and (ii) the feasibility of defining tasks
    in terms of states [[130](#bib.bib130)]. Based on those assumptions, approaches
    such as TD-algorithms [[67](#bib.bib67)] or matrix-factorization [[64](#bib.bib64)]
    become applicable to learn such disentangled representations. To further exploit
    the effectiveness of disentangled structure, we consider that *generalization*
    approaches, which allow changing dynamics or state distributions, are important
    future work that is worth more attention in this domain.'
  prefs: []
  type: TYPE_NORMAL
- en: Most discussed work in this section tackles multi-task RL or meta-RL scenarios,
    hence the agent’s *generalization* ability is extensively investigated. For instance,
    methods of modular networks largely evaluated the *zero-shot* performance from
    the meta-RL perspective [[52](#bib.bib52), [130](#bib.bib130)]. Given a fixed
    number of training epochs (*pe*), *Transfer ratio (tr)* is manifested differently
    among these methods. It can be the relative performance of a modular net architecture
    compared with a baseline, or the accumulated return in modified target domains,
    where reward scores are negated for evaluating the dynamics transfer. *Performance
    sensitivity (ps)* is also broadly studied to estimate the *robustness* of TL.
    [[110](#bib.bib110)] analyzed the performance sensitivity given varying source
    tasks, while [[130](#bib.bib130)] studied the performance on different unseen
    target domains.
  prefs: []
  type: TYPE_NORMAL
- en: There are unresolved questions in this intriguing research topic. One is *how
    to handle drastic changes of reward functions between domains*. As discussed in [[131](#bib.bib131)],
    good policies in one MDP may perform poorly in another due to the fact that beneficial
    states or actions in ${\mathcal{M}}_{s}$ may become detrimental in ${\mathcal{M}}_{t}$
    with totally different reward functions. Learning a set of basis functions [[67](#bib.bib67)]
    to represent unseen tasks (reward functions), or decoupling policies from $Q$-function
    representation [[130](#bib.bib130)] may serve as a good start to address this
    issue, as they propose a generalized latent space, from which different tasks
    (reward functions) can be interpreted. However, the limitation of this work is
    that it is not clear how many and what kind of sub-tasks need to be learned to
    make the latent space generalizable enough.
  prefs: []
  type: TYPE_NORMAL
- en: Another question is *how to generalize the representation learning for TL across
    domains with different dynamics or state-action spaces*. A learned SR might not
    be transferrable to an MDP with different transition dynamics, as the distribution
    of occupancy measure for SR may no longer hold. Potential solutions may include
    model-based approaches that approximate the dynamics directly or training a latent
    representation space for states using multiple tasks with different dynamics for
    better generalization [[132](#bib.bib132)]. Alternatively, TL mechanisms from
    the supervised learning domain, such as meta-learning, which enables the ability
    of fast adaptation to new tasks [[133](#bib.bib133)], or importance sampling [[134](#bib.bib134)],
    which can compensate for the prior distribution changes [[10](#bib.bib10)], may
    also shed light on this question.
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Representations format | Assumptions | MDP difference | Learner
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Evaluation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; metrics &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Progressive Net [[122](#bib.bib122)] | Lateral connections to previously
    learned network modules | N/A | ${\mathcal{S}},{\mathcal{A}}$ | A3C | $ap,ar,pe,ps,tr$
    |'
  prefs: []
  type: TYPE_TB
- en: '| PathNet [[123](#bib.bib123)] | Selected neural paths | N/A | ${\mathcal{S}},{\mathcal{A}}$
    | A3C | $ap,ar,pe,tr$ |'
  prefs: []
  type: TYPE_TB
- en: '| Modular Net [[52](#bib.bib52)] | Task(agent)-specific network module | Disentangled
    state representation | ${\mathcal{S}},{\mathcal{A}}$ | Policy Gradient | $ap,ar,pe,tt$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Modular Net [[125](#bib.bib125)] | Dynamic transitions module learned on
    state latent representations. | N/A | ${\mathcal{S}},{\mathcal{A}}$ | A3C | $ap,ar,pe,tr,ps$
    |'
  prefs: []
  type: TYPE_TB
- en: '| SR [[110](#bib.bib110)] | SF | Reward function can be linearly decoupled
    | ${\mathcal{R}}$ | DQN | $ap,ar,nka,ps$ |'
  prefs: []
  type: TYPE_TB
- en: '| SR [[127](#bib.bib127)] | Encoder-decoder learned SF | N/A | ${\mathcal{R}}$
    | DQN | $ap,ar,pe,ps$ |'
  prefs: []
  type: TYPE_TB
- en: '| SR [[67](#bib.bib67)] | Encoder-decoder learned SF | Rewards can be represented
    by set of basis functions | ${\mathcal{R}}$ | $Q(\lambda)$ | $ap,pe$ |'
  prefs: []
  type: TYPE_TB
- en: '| UVFA [[64](#bib.bib64)] | Matrix-factorized UF | Goal conditional RL | ${\mathcal{R}}$
    | Tabular Q-learning | $ap,ar,pe,ps$ |'
  prefs: []
  type: TYPE_TB
- en: '| UVFA with SR [[130](#bib.bib130)] | Policy-encoded UF | Reward function can
    be linearly decoupled | ${\mathcal{R}}$ | $\epsilon$-greedy Q-learning | $ap,ar,pe$
    |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: A comparison of TL approaches of representation transfer.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we summarize recent applications that are closely related to
    using TL techniques for tackling RL domains.
  prefs: []
  type: TYPE_NORMAL
- en: '*Robotics learning* is a prominent application domain of RL. TL approaches
    in this field include *robotics learning from demonstrations*, where expert demonstrations
    from humans or other robots are leveraged [[135](#bib.bib135)] Another is *collaborative
    robotic training* [[136](#bib.bib136), [137](#bib.bib137)], in which knowledge
    from different robots is transferred by sharing their policies and episodic demonstrations.
    Recent research focus is this domain is fast and robust adaptation to unseen tasks.
    One example towards this goal is [[138](#bib.bib138)], in which robust robotics
    policies are trained using synthetic demonstrations to handle dynamic environments.
    Another solution is to learn domain-invariant latent representations. Examples
    include [[139](#bib.bib139)], which learns the latent representation using 3D
    CAD models, and [[140](#bib.bib140), [141](#bib.bib141)] which are derived based
    on the Generative-Adversarial Network. Another example is *DARLA* [[142](#bib.bib142)],
    which is a zero-shot transfer approach to learn disentangled representations that
    are robust against domain shifts. We refer readers to [[70](#bib.bib70), [143](#bib.bib143)]
    for detailed surveys along this direction.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Game Playing* is a common test-bed for TL and RL algorithms. It has evolved
    from classical benchmarks such as grid-world games to more complex settings such
    as online-strategy games or video games with multimodal inputs. One example is
    *AlphaGo*, which is an algorithm for learning the online chessboard games using
    both TL and RL techniques [[90](#bib.bib90)]. *AlphaGo* is first pre-trained offline
    using expert demonstrations and then learns to optimize its policy using Monte-Carlo
    Tree Search. Its successor, *AlphaGo Master* [[144](#bib.bib144)], even beat the
    world’s first ranked human player. TL-DRL approaches are also thriving in video
    game playing. Especially, OpenAI has trained *Dota2* agents that can surpass human
    experts [[145](#bib.bib145)]. State-of-the-art platforms include *MineCraft*,
    *Atari*, and *Starcraft*. [[146](#bib.bib146)] designed new RL benchmarks under
    the *MineCraft* platform. [[147](#bib.bib147)] provided a comprehensive survey
    on DL applications in video game playing, which also covers TL and RL strategies
    from certain perspectives. A large portion of TL approaches reviewed in this survey
    have been applied to the *Atari* platforms  [[148](#bib.bib148)].'
  prefs: []
  type: TYPE_NORMAL
- en: '*Natural Language Processing (NLP)* has evolved rapidly along with the advancement
    of DL and RL. Applications of RL to NLP range widely, from *Question Answering
    (QA)* [[149](#bib.bib149)], *Dialogue systems* [[150](#bib.bib150)], *Machine
    Translation* [[151](#bib.bib151)], to an integration of NLP and Computer Vision
    tasks, such as *Visual Question Answering (VQA)* [[152](#bib.bib152)], *Image
    Caption* [[153](#bib.bib153)], etc. Many NLP applications have implicitly applied
    TL approaches. Examples include learning from expert demonstrations for *Spoken
    Dialogue Systems* [[154](#bib.bib154)], *VQA* [[152](#bib.bib152)]; or reward
    shaping for *Sequence Generation* [[155](#bib.bib155)], *Spoken Dialogue Systems* [[80](#bib.bib80)],*QA* [[81](#bib.bib81),
    [156](#bib.bib156)], and *Image Caption* [[153](#bib.bib153)], or transferring
    policies for *Structured Prediction* [[157](#bib.bib157)] and *VQA* [[158](#bib.bib158)].'
  prefs: []
  type: TYPE_NORMAL
- en: '*Large Model Training*: RL from human and model-assisted feedback becomes indispensable
    in training large models (LMM), such as GPT4 [[159](#bib.bib159)], Sparrow [[160](#bib.bib160)],
    PaLM [[161](#bib.bib161)], LaMDA [[162](#bib.bib162)], which have shown tremendous
    breakthrough in dialogue applications, search engine answer improvement, artwork
    generation, etc. The TL method at the core of them is using human preferences
    as a reward signal for model fine-tuning, where the preference ranking itself
    is considered as shaped rewards. We believe that TL with carefully crafted human
    knowledge will help better align large models with human intent and hence achieve
    trustworthy and de-biased AI.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Health Informatics*: RL has been applied to various healthcare tasks [[163](#bib.bib163)],
    including *automatic medical diagnosis* [[164](#bib.bib164), [165](#bib.bib165)],
    *health resource scheduling* [[166](#bib.bib166)], and *drug discovery and development*, [[167](#bib.bib167),
    [168](#bib.bib168)], etc. Among these applications we observe emerging trends
    of leveraging prior knowledge to improve the RL procedure, especially given the
    difficulty of accessing large amounts of clinical data. Specifically, [[169](#bib.bib169)]
    utilized $Q$-learning for drug delivery individualization. They integrated the
    prior knowledge of the dose-response characteristics into their $Q$-learning framework
    to avoid random exploration. [[170](#bib.bib170)] applied a DQN framework for
    prescribing effective HIV treatments, in which they learned a latent representation
    to estimate the uncertainty when transferring a pertained policy to the unseen
    domains. [[171](#bib.bib171)] studied applying human-involved interactive RL training
    for health informatics.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Others*: RL has also been utilized in many other real-life applications. Applications
    in the *Transportation Systems* have adopted RL to address traffic congestion
    issues with better *traffic signal scheduling* and *transportation resource allocation* [[9](#bib.bib9),
    [172](#bib.bib172), [8](#bib.bib8), [173](#bib.bib173)]. We refer readers to [[174](#bib.bib174)]
    for a review along this line. Deep RL are also effective solutions to problems
    in *Finance*, including *portfolio management* [[175](#bib.bib175), [176](#bib.bib176)],
    *asset allocation* [[177](#bib.bib177)], and *trading optimization* [[178](#bib.bib178)].
    Another application is the *Electricity Systems*, especially the *intelligent
    electricity networks*, which can benefit from RL techniques for improved power-delivery
    decisions [[179](#bib.bib179), [180](#bib.bib180)] and active resource management [[181](#bib.bib181)].
    [[7](#bib.bib7)] provides a detailed survey of RL techniques for electric power
    system applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Future Perspectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we present some open challenges and future directions in TL
    that are closely related to the DRL domain, based on both retrospectives of the
    methods discussed in this survey and outlooks to the emerging trends of AI.
  prefs: []
  type: TYPE_NORMAL
- en: '*Transfer Learning from Black-Box:*  Ranging from exterior teacher demonstrations
    to pre-trained function approximators, black-box resource is more accessible and
    predominant than well-articulated knowledge. Therefore, leveraging such black-box
    resource is indispensable for practical TL in DRL. One of its main challenges
    resides in *estimating the optimality* of black-box resource, which can be potentially
    noisy or biased. We consider that efforts can be made from the following perspectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inferring the *reasoning* mechanism inside the black-box. Resemblant ideas have
    been explored in *inverse RL* and *model-based RL*, where the goal is to approximate
    the reward function or to learn the dynamics model under which the demonstrated
    knowledge becomes reasonable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Designing effective *feedback* schemes, including leveraging domain-provided
    rewards, intrinsic reward feedback, or using human preference as feedback.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improving the *interpretability* of the transferred knowledge[[182](#bib.bib182),
    [183](#bib.bib183)], which benefits in evaluating and explaining the process of
    TL from black-box. It can also alleviate catastrophic decision-making for high-stake
    tasks such as auto-driving.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Knowledge Disentanglement and Fusion* are both towards better knowledge sharing
    across domains. Disentangling knowledge is usually a *prerequisite* for efficient
    knowledge fusion, which may involve external knowledge from multiple source *domains*,
    with diverging *qualities*, presented in different *modalities*, etc. Disentangling
    knowledge in RL can be interpreted from different perspectives: i) disentangling
    the action, state, or reward representations, as discussed in Sec [5.5](#S5.SS5
    "5.5 Representation Transfer ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer
    Learning in Deep Reinforcement Learning: A Survey"); 2) decomposing complex tasks
    into multiple *skill snippets.* The former is an effective direction in tackling
    meta-RL and multi-task RL, although some solutions hinge on strict assumptions
    of the problem setting, such as linear dependence among domain dynamics or learning
    goals. The latter is relevant to hierarchical RL and *prototype learning* from
    sequence data[[184](#bib.bib184)]. It is relatively less discussed besides few
    pioneer research [[132](#bib.bib132)]. We believe that this direction is worth
    more research efforts, which not only benefits interpretable knowledge learning,
    but also aligns with human perception.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Framework-Agnostic Knowledge Transfer*: Most contemporary TL approaches are
    designed for certain RL frameworks. Some are applicable to RL algorithms designed
    for the discrete-action space, while others may only be feasible given a continuous
    action space. One fundamental reason behind is the diversified development of
    RL algorithms. We expect that unified RL frameworks would contribute to the standardization
    of TL approaches in this field.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Evaluation and Benchmarking*: Variant evaluation metrics have been proposed
    to measure TL from different but complementary perspectives, although no single
    metric can summarize the efficacy of a TL approach. Designing a set of generalized,
    novel metrics is beneficial for the development of TL in DRL. Moreover, with the
    effervescent development of large-scale models, it is crucial to standardize evaluation
    from the perspectives of *ethics* and *groundedness*. The *appropriateness* of
    the transferred knowledge, such as potential *stereotypes* in human preference,
    and the *bias* in the model itself should also be quantified as metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Knowledge Transfer to and from Pre-Trained Large Models:* By the time of this
    survey being finalized, unprecedented DL breakthroughs have been achieved in learning
    large-scale models built on massive computation resource and attributed data.
    One representative example is the Generative Pre-trained Transformer (GPT)[[159](#bib.bib159)].
    Considering them as complete *knowledge graphs* whose training process maybe inaccessible,
    there are more challenges in this direction besides learning from a *black-box*,
    which are faced by a larger AI community including the RL domain. We briefly point
    out two directions that are worth ongoing attention:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Efficient model fine-tuning with knowledge distillation.* One important method
    for fine-tuning large models is *RL with human feedback*, in which the quantity
    and quality of human ratings are critical for realizing a good reward model. We
    anticipate other forms of TL methods in RL to be explored to further improve the
    efficiency of fine-tuning, such as imitation learning with adversarial training
    to achieve human-level performance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Principled prompt-engineering for knowledge extraction*. More often the large
    model itself cannot be accessed, but only input and output of models are allowed.
    Such inference based knowledge extraction requires delicate prompt designs. Some
    efficacious efforts include designing prompts with mini task examples as one-shot
    learning, *decomposing* complex tasks into *architectural*, contextual prompts.
    Prompt engineering is being proved an important direction for effective knowledge
    extraction, which with proper design, can largely benefit downstream tasks that
    depend on large model resources.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 8 Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research was supported by the National Science Foundation (IIS-2212174,
    IIS-1749940), National Institute of Aging (IRF1AG072449), and the Office of Naval
    Research (N00014-20-1-2382).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] R. S. Sutton and A. G. Barto, *Reinforcement learning: An introduction*.   MIT
    press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “A brief
    survey of deep reinforcement learning,” *arXiv preprint arXiv:1708.05866*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of
    deep visuomotor policies,” *The Journal of Machine Learning Research*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen, “Learning
    hand-eye coordination for robotic grasping with deep learning and large-scale
    data collection,” *The International Journal of Robotics Research*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, “The arcade learning
    environment: An evaluation platform for general agents,” *Journal of Artificial
    Intelligence Research*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. R. Kosorok and E. E. Moodie, *Adaptive TreatmentStrategies in Practice:
    Planning Trials and Analyzing Data for Personalized Medicine*.   SIAM, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. Glavic, R. Fonteneau, and D. Ernst, “Reinforcement learning for electric
    power system decision and control: Past considerations and perspectives,” *IFAC-PapersOnLine*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] S. El-Tantawy, B. Abdulhai, and H. Abdelgawad, “Multiagent reinforcement
    learning for integrated network of adaptive traffic signal controllers (marlin-atsc):
    methodology and large-scale application on downtown toronto,” *IEEE Transactions
    on Intelligent Transportation Systems*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] H. Wei, G. Zheng, H. Yao, and Z. Li, “Intellilight: A reinforcement learning
    approach for intelligent traffic light control,” *ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] S. J. Pan and Q. Yang, “A survey on transfer learning,” *IEEE Transactions
    on knowledge and data engineering*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] M. E. Taylor and P. Stone, “Transfer learning for reinforcement learning
    domains: A survey,” *Journal of Machine Learning Research*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Lazaric, “Transfer in reinforcement learning: a framework and a survey.”   Springer,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] R. Bellman, “A markovian decision process,” *Journal of mathematics and
    mechanics*, 1957.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspective
    on reinforcement learning,” in *International conference on machine learning*.   PMLR,
    2017, pp. 449–458.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] M. Liu, M. Zhu, and W. Zhang, “Goal-conditioned reinforcement learning:
    Problems and solutions,” *arXiv preprint arXiv:2201.08299*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] C. Florensa, D. Held, X. Geng, and P. Abbeel, “Automatic goal generation
    for reinforcement learning agents,” in *International conference on machine learning*.   PMLR,
    2018, pp. 1515–1528.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Z. Xu and A. Tewari, “Reinforcement learning in factored mdps: Oracle-efficient
    algorithms and tighter regret bounds for the non-episodic setting,” *NeurIPS*,
    vol. 33, pp. 18 226–18 236, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu, “The
    surprising effectiveness of ppo in cooperative multi-agent games,” *NeurIPS*,
    vol. 35, pp. 24 611–24 624, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson, “Discriminator-actor-critic:
    Addressing sample inefficiency and reward bias in adversarial imitation learning,”
    *arXiv preprint arXiv:1809.02925*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] G. A. Rummery and M. Niranjan, *On-line Q-learning using connectionist
    systems*.   University of Cambridge, Department of Engineering Cambridge, England,
    1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] H. Van Seijen, H. Van Hasselt, S. Whiteson, and M. Wiering, “A theoretical
    and empirical analysis of expected sarsa,” *IEEE Symposium on Adaptive Dynamic
    Programming and Reinforcement Learning*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] V. Konda and J. Tsitsiklis, “Actor-critic algorithms,” *NeurIPS*, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” *ICML*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-policy
    maximum entropy deep reinforcement learning with a stochastic actor,” *International
    Conference on Machine Learning*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] C. J. Watkins and P. Dayan, “Q-learning,” *Machine learning*, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney,
    D. Horgan, B. Piot, M. Azar, and D. Silver, “Rainbow: Combining improvements in
    deep reinforcement learning,” *AAAI*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” *Machine learning*, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region
    policy optimization,” *ICML*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv preprint arXiv:1707.06347*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
    “Deterministic policy gradient algorithms,” 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    *arXiv preprint arXiv:1509.02971*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] S. Fujimoto, H. Van Hoof, and D. Meger, “Addressing function approximation
    error in actor-critic methods,” *arXiv preprint arXiv:1802.09477*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine, “Neural network dynamics
    for model-based deep reinforcement learning with model-free fine-tuning,” in *2018
    IEEE international conference on robotics and automation (ICRA)*.   IEEE, 2018,
    pp. 7559–7566.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Z. I. Botev, D. P. Kroese, R. Y. Rubinstein, and P. L’Ecuyer, “The cross-entropy
    method for optimization,” in *Handbook of statistics*.   Elsevier, 2013, vol. 31,
    pp. 35–59.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] K. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep reinforcement
    learning in a handful of trials using probabilistic dynamics models,” *NeurIPS*,
    vol. 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] R. S. Sutton, “Integrated architectures for learning, planning, and reacting
    based on approximating dynamic programming,” in *Machine learning proceedings
    1990*.   Elsevier, 1990, pp. 216–224.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] V. Feinberg, A. Wan, I. Stoica, M. I. Jordan, J. E. Gonzalez, and S. Levine,
    “Model-based value estimation for efficient model-free reinforcement learning,”
    *arXiv preprint arXiv:1803.00101*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S. Levine and V. Koltun, “Guided policy search,” in *International conference
    on machine learning*.   PMLR, 2013, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] H. Bharadhwaj, K. Xie, and F. Shkurti, “Model-predictive control via cross-entropy
    and gradient-based optimization,” in *Learning for Dynamics and Control*.   PMLR,
    2020, pp. 277–286.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] M. Deisenroth and C. E. Rasmussen, “Pilco: A model-based and data-efficient
    approach to policy search,” in *Proceedings of the 28th International Conference
    on machine learning (ICML-11)*, 2011, pp. 465–472.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Y. Gal, R. McAllister, and C. E. Rasmussen, “Improving pilco with bayesian
    neural network dynamics models,” in *Data-efficient machine learning workshop,
    ICML*, vol. 4, no. 34, 2016, p. 25.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] C. H. Lampert, H. Nickisch, and S. Harmeling, “Learning to detect unseen
    object classes by between-class attribute transfer,” *IEEE Conference on Computer
    Vision and Pattern Recognition*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] P. Dayan and G. E. Hinton, “Feudal reinforcement learning,” *NeurIPS*,
    1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] R. S. Sutton, D. Precup, and S. Singh, “Between mdps and semi-mdps: A
    framework for temporal abstraction in reinforcement learning,” *Artificial intelligence*,
    1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] R. Parr and S. J. Russell, “Reinforcement learning with hierarchies of
    machines,” *NeurIPS*, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] T. G. Dietterich, “Hierarchical reinforcement learning with the maxq value
    function decomposition,” *Journal of artificial intelligence research*, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] A. Lazaric and M. Ghavamzadeh, “Bayesian multi-task reinforcement learning,”
    in *ICML-27th international conference on machine learning*.   Omnipress, 2010,
    pp. 599–606.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Y. Zhang and Q. Yang, “A survey on multi-task learning,” *IEEE Transactions
    on Knowledge and Data Engineering*, vol. 34, no. 12, pp. 5586–5609, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Y. Teh, V. Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R. Hadsell,
    N. Heess, and R. Pascanu, “Distral: Robust multitask reinforcement learning,”
    *NeurIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] E. Parisotto, J. L. Ba, and R. Salakhutdinov, “Actor-mimic: Deep multitask
    and transfer reinforcement learning,” *ICLR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine, “Learning modular
    neural network policies for multi-task and multi-robot transfer,” *2017 IEEE International
    Conference on Robotics and Automation (ICRA)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] J. Andreas, D. Klein, and S. Levine, “Modular multitask reinforcement
    learning with policy sketches,” *ICML*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] R. Yang, H. Xu, Y. Wu, and X. Wang, “Multi-task reinforcement learning
    with soft modularization,” *NeurIPS*, vol. 33, pp. 4767–4777, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, “Meta-learning
    in neural networks: A survey,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 44, no. 9, pp. 5149–5169, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Z. Jia, X. Li, Z. Ling, S. Liu, Y. Wu, and H. Su, “Improving policy optimization
    with generalist-specialist learning,” in *International Conference on Machine
    Learning*.   PMLR, 2022, pp. 10 104–10 119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] W. Ding, H. Lin, B. Li, and D. Zhao, “Generalizing goal-conditioned reinforcement
    learning with variational causal reasoning,” *arXiv preprint arXiv:2207.09081*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] R. Kirk, A. Zhang, E. Grefenstette, and T. Rocktäschel, “A survey of zero-shot
    generalisation in deep reinforcement learning,” *Journal of Artificial Intelligence
    Research*, vol. 76, pp. 201–264, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] B. Kim, A.-m. Farahmand, J. Pineau, and D. Precup, “Learning from limited
    demonstrations,” *NeurIPS*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] W. Czarnecki, R. Pascanu, S. Osindero, S. Jayakumar, G. Swirszcz, and
    M. Jaderberg, “Distilling policy distillation,” *The 22nd International Conference
    on Artificial Intelligence and Statistics*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] A. Y. Ng, D. Harada, and S. Russell, “Policy invariance under reward transformations:
    Theory and application to reward shaping,” *ICML*, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” *NeurIPS*, pp. 2672–2680,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Z. Zhu, K. Lin, B. Dai, and J. Zhou, “Learning sparse rewarded tasks from
    sub-optimal demonstrations,” *arXiv preprint arXiv:2004.00530*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] T. Schaul, D. Horgan, K. Gregor, and D. Silver, “Universal value function
    approximators,” *ICML*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] C. Finn and S. Levine, “Meta-learning: from few-shot learning to rapid
    reinforcement learning,” *ICML*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] M. E. Taylor, P. Stone, and Y. Liu, “Transfer learning via inter-task
    mappings for temporal difference learning,” *Journal of Machine Learning Research*,
    2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] A. Barreto, D. Borsa, J. Quan, T. Schaul, D. Silver, M. Hessel, D. Mankowitz,
    A. Žídek, and R. Munos, “Transfer in deep reinforcement learning using successor
    features and generalised policy improvement,” *ICML*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Z. Zhu, K. Lin, B. Dai, and J. Zhou, “Off-policy imitation learning from
    observations,” *NeurIPS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] J. Ho and S. Ermon, “Generative adversarial imitation learning,” *NeurIPS*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] W. Zhao, J. P. Queralta, and T. Westerlund, “Sim-to-real transfer in deep
    reinforcement learning for robotics: a survey,” in *2020 IEEE symposium series
    on computational intelligence (SSCI)*.   IEEE, 2020, pp. 737–744.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. Muller-Brockhausen, M. Preuss, and A. Plaat, “Procedural content generation:
    Better benchmarks for transfer reinforcement learning,” in *2021 IEEE Conference
    on games (CoG)*.   IEEE, 2021, pp. 01–08.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] N. Vithayathil Varghese and Q. H. Mahmoud, “A survey of multi-task deep
    reinforcement learning,” *Electronics*, vol. 9, no. 9, p. 1363, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] R. J. Williams and L. C. Baird, “Tight performance bounds on greedy policies
    based on imperfect value functions,” Tech. Rep., 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] E. Wiewiora, G. W. Cottrell, and C. Elkan, “Principled methods for advising
    reinforcement learning agents,” *ICML*, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] S. M. Devlin and D. Kudenko, “Dynamic potential-based reward shaping,”
    *ICAAMAS*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] A. Harutyunyan, S. Devlin, P. Vrancx, and A. Nowé, “Expressing arbitrary
    reward functions as potential-based advice,” *AAAI*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] T. Brys, A. Harutyunyan, M. E. Taylor, and A. Nowé, “Policy transfer using
    reward shaping,” *ICAAMS*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] M. Večerík, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess,
    T. Rothörl, T. Lampe, and M. Riedmiller, “Leveraging demonstrations for deep reinforcement
    learning on robotics problems with sparse rewards,” *arXiv preprint arXiv:1707.08817*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] A. C. Tenorio-Gonzalez, E. F. Morales, and L. Villaseñor-Pineda, “Dynamic
    reward shaping: Training a robot by voice,” *Advances in Artificial Intelligence
    – IBERAMIA*, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] P.-H. Su, D. Vandyke, M. Gasic, N. Mrksic, T.-H. Wen, and S. Young, “Reward
    shaping with recurrent neural networks for speeding up on-line policy learning
    in spoken dialogue systems,” *arXiv preprint arXiv:1508.03391*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] X. V. Lin, R. Socher, and C. Xiong, “Multi-hop knowledge graph reasoning
    with reward shaping,” *arXiv preprint arXiv:1808.10568*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] S. Devlin, L. Yliniemi, D. Kudenko, and K. Tumer, “Potential-based difference
    rewards for multiagent reinforcement learning,” *ICAAMS*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] M. Grzes and D. Kudenko, “Learning shaping rewards in model-based reinforcement
    learning,” *Proc. AAMAS Workshop on Adaptive Learning Agents*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] O. Marom and B. Rosman, “Belief reward shaping in reinforcement learning,”
    *AAAI*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] F. Liu, Z. Ling, T. Mu, and H. Su, “State alignment-based imitation learning,”
    *arXiv preprint arXiv:1911.10947*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] K. Kim, Y. Gu, J. Song, S. Zhao, and S. Ermon, “Domain adaptive imitation
    learning,” *ICML*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Y. Ma, Y.-X. Wang, and B. Narayanaswamy, “Imitation-regularized offline
    learning,” *International Conference on Artificial Intelligence and Statistics*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] M. Yang and O. Nachum, “Representation matters: Offline pretraining for
    sequential decision making,” *arXiv preprint arXiv:2102.05815*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] X. Zhang and H. Ma, “Pretraining deep actor-critic reinforcement learning
    algorithms with expert demonstrations,” *arXiv preprint arXiv:1801.10459*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot *et al.*, “Mastering
    the game of go with deep neural networks and tree search,” *Nature*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] S. Schaal, “Learning from demonstration,” *NeurIPS*, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan,
    J. Quan, A. Sendonaris, I. Osband *et al.*, “Deep q-learning from demonstrations,”
    *AAAI*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel, “Overcoming
    exploration in reinforcement learning with demonstrations,” *IEEE International
    Conference on Robotics and Automation (ICRA)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] J. Chemali and A. Lazaric, “Direct policy iteration with demonstrations,”
    *International Joint Conference on Artificial Intelligence*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] B. Piot, M. Geist, and O. Pietquin, “Boosted bellman residual minimization
    handling expert demonstrations,” *Joint European Conference on Machine Learning
    and Knowledge Discovery in Databases*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] T. Brys, A. Harutyunyan, H. B. Suay, S. Chernova, M. E. Taylor, and A. Nowé,
    “Reinforcement learning from demonstration through shaping,” *International Joint
    Conference on Artificial Intelligence*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] B. Kang, Z. Jie, and J. Feng, “Policy optimization with demonstrations,”
    *ICML*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] D. P. Bertsekas, “Approximate policy iteration: A survey and some new
    methods,” *Journal of Control Theory and Applications*, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience
    replay,” *ICLR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning
    and structured prediction to no-regret online learning,” *AISTATS*, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Y. Gao, J. Lin, F. Yu, S. Levine, T. Darrell *et al.*, “Reinforcement
    learning from imperfect demonstrations,” *arXiv preprint arXiv:1802.05313*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] M. Jing, X. Ma, W. Huang, F. Sun, C. Yang, B. Fang, and H. Liu, “Reinforcement
    learning from imperfect demonstrations under soft expert guidance.” *AAAI*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] K. Brantley, W. Sun, and M. Henaff, “Disagreement-regularized imitation
    learning,” *ICLR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” *Deep Learning and Representation Learning Workshop, NeurIPS*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick,
    R. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell, “Policy distillation,” *arXiv
    preprint arXiv:1511.06295*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] H. Yin and S. J. Pan, “Knowledge transfer for deep reinforcement learning
    with hierarchical experience replay,” *AAAI*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] S. Schmitt, J. J. Hudson, A. Zidek, S. Osindero, C. Doersch, W. M. Czarnecki,
    J. Z. Leibo, H. Kuttler, A. Zisserman, K. Simonyan *et al.*, “Kickstarting deep
    reinforcement learning,” *arXiv preprint arXiv:1803.03835*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] J. Schulman, X. Chen, and P. Abbeel, “Equivalence between policy gradients
    and soft q-learning,” *arXiv preprint arXiv:1704.06440*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] F. Fernández and M. Veloso, “Probabilistic policy reuse in a reinforcement
    learning agent,” *Proceedings of the fifth international joint conference on Autonomous
    agents and multiagent systems*, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. P. van Hasselt,
    and D. Silver, “Successor features for transfer in reinforcement learning,” *NuerIPS*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] R. Bellman, “Dynamic programming,” *Science*, 1966.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] L. Torrey, T. Walker, J. Shavlik, and R. Maclin, “Using advice to transfer
    knowledge acquired in one reinforcement learning task to another,” *European Conference
    on Machine Learning*, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] A. Gupta, C. Devin, Y. Liu, P. Abbeel, and S. Levine, “Learning invariant
    feature spaces to transfer skills with reinforcement learning,” *ICLR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] G. Konidaris and A. Barto, “Autonomous shaping: Knowledge transfer in
    reinforcement learning,” *ICML*, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] H. B. Ammar and M. E. Taylor, “Reinforcement learning transfer via common
    subspaces,” *Proceedings of the 11th International Conference on Adaptive and
    Learning Agents*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional
    encoder-decoder architecture for image segmentation,” *IEEE transactions on pattern
    analysis and machine intelligence*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] C. Wang and S. Mahadevan, “Manifold alignment without correspondence,”
    *International Joint Conference on Artificial Intelligence*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] B. Bocsi, L. Csató, and J. Peters, “Alignment-based transfer learning
    for robot models,” *The 2013 International Joint Conference on Neural Networks
    (IJCNN)*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] H. B. Ammar, E. Eaton, P. Ruvolo, and M. E. Taylor, “Unsupervised cross-domain
    transfer in policy gradient reinforcement learning via manifold alignment,” *AAAI*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] H. B. Ammar, K. Tuyls, M. E. Taylor, K. Driessens, and G. Weiss, “Reinforcement
    learning transfer via sparse coding,” *ICAAMS*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] A. Lazaric, M. Restelli, and A. Bonarini, “Transfer of samples in batch
    reinforcement learning,” *ICML*, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,
    K. Kavukcuoglu, R. Pascanu, and R. Hadsell, “Progressive neural networks,” *arXiv
    preprint arXiv:1606.04671*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu, A. Pritzel,
    and D. Wierstra, “Pathnet: Evolution channels gradient descent in super neural
    networks,” *arXiv preprint arXiv:1701.08734*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] I. Harvey, “The microbial genetic algorithm,” *European Conference on
    Artificial Life*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] A. Zhang, H. Satija, and J. Pineau, “Decoupling dynamics and reward for
    transfer learning,” *arXiv preprint arXiv:1804.10689*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] P. Dayan, “Improving generalization for temporal difference learning:
    The successor representation,” *Neural Computation*, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] T. D. Kulkarni, A. Saeedi, S. Gautam, and S. J. Gershman, “Deep successor
    reinforcement learning,” *arXiv preprint arXiv:1606.02396*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] J. Zhang, J. T. Springenberg, J. Boedecker, and W. Burgard, “Deep reinforcement
    learning with successor features for navigation across similar environments,”
    *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] N. Mehta, S. Natarajan, P. Tadepalli, and A. Fern, “Transfer in variable-reward
    hierarchical reinforcement learning,” *Machine Learning*, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] D. Borsa, A. Barreto, J. Quan, D. Mankowitz, R. Munos, H. van Hasselt,
    D. Silver, and T. Schaul, “Universal successor features approximators,” *ICLR*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] L. Lehnert, S. Tellex, and M. L. Littman, “Advantages and limitations
    of using successor features for transfer in reinforcement learning,” *arXiv preprint
    arXiv:1708.00102*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] J. C. Petangoda, S. Pascual-Diaz, V. Adam, P. Vrancx, and J. Grau-Moya,
    “Disentangled skill embeddings for reinforcement learning,” *arXiv preprint arXiv:1906.09223*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” *ICML*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] B. Zadrozny, “Learning and evaluating classifiers under sample selection
    bias,” *ICML*, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, “A survey of robot
    learning from demonstration,” *Robotics and autonomous systems*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] B. Kehoe, S. Patil, P. Abbeel, and K. Goldberg, “A survey of research
    on cloud robotics and automation,” *IEEE Transactions on automation science and
    engineering*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement learning
    for robotic manipulation with asynchronous off-policy updates,” *IEEE international
    conference on robotics and automation (ICRA)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] W. Yu, J. Tan, C. K. Liu, and G. Turk, “Preparing for the unknown: Learning
    a universal policy with online system identification,” *arXiv preprint arXiv:1702.02453*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] F. Sadeghi and S. Levine, “Cad2rl: Real single-image flight without a
    single real image,” *arXiv preprint arXiv:1611.04201*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan,
    L. Downs, J. Ibarz, P. Pastor, K. Konolige *et al.*, “Using simulation and domain
    adaptation to improve efficiency of deep robotic grasping,” *IEEE International
    Conference on Robotics and Automation (ICRA)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] H. Bharadhwaj, Z. Wang, Y. Bengio, and L. Paull, “A data-efficient framework
    for training and sim-to-real transfer of navigation policies,” *International
    Conference on Robotics and Automation (ICRA)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] I. Higgins, A. Pal, A. Rusu, L. Matthey, C. Burgess, A. Pritzel, M. Botvinick,
    C. Blundell, and A. Lerchner, “Darla: Improving zero-shot transfer in reinforcement
    learning,” *ICML*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement learning in robotics:
    A survey,” *The International Journal of Robotics Research*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez,
    T. Hubert, L. Baker, M. Lai, A. Bolton *et al.*, “Mastering the game of go without
    human knowledge,” *Nature*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] OpenAI. (2019) Dotal2 blog. [Online]. Available: [https://openai.com/blog/openai-five/](https://openai.com/blog/openai-five/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] J. Oh, V. Chockalingam, S. Singh, and H. Lee, “Control of memory, active
    perception, and action in minecraft,” *arXiv preprint arXiv:1605.09128*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] N. Justesen, P. Bontrager, J. Togelius, and S. Risi, “Deep learning for
    video game playing,” *IEEE Transactions on Games*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,
    and M. Riedmiller, “Playing atari with deep reinforcement learning,” *arXiv preprint
    arXiv:1312.5602*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] H. Chen, X. Liu, D. Yin, and J. Tang, “A survey on dialogue systems:
    Recent advances and new frontiers,” *Acm Sigkdd Explorations Newsletter*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] S. P. Singh, M. J. Kearns, D. J. Litman, and M. A. Walker, “Reinforcement
    learning for spoken dialogue systems,” *NeurIPS*, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement
    learning,” *arXiv preprint arXiv:1611.01578*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko, “Learning
    to reason: End-to-end module networks for visual question answering,” *IEEE International
    Conference on Computer Vision*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Z. Ren, X. Wang, N. Zhang, X. Lv, and L.-J. Li, “Deep reinforcement learning-based
    image captioning with embedding reward,” *IEEE Conference on Computer Vision and
    Pattern Recognition*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, “Learning to compose
    neural networks for question answering,” *arXiv preprint arXiv:1601.01705*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville,
    and Y. Bengio, “An actor-critic algorithm for sequence prediction,” *arXiv preprint
    arXiv:1607.07086*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] F. Godin, A. Kumar, and A. Mittal, “Learning when not to answer: a ternary
    reward structure for reinforcement learning based question answering,” *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] K.-W. Chang, A. Krishnamurthy, A. Agarwal, J. Langford, and H. Daumé III,
    “Learning to search better than your teacher,” 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] J. Lu, A. Kannan, J. Yang, D. Parikh, and D. Batra, “Best of both worlds:
    Transferring knowledge from discriminative learning to a generative visual dialog
    model,” *NeurIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] OpenAI, “Gpt-4 technical report,” *arXiv*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds,
    M. Rauh, L. Weidinger, M. Chadwick, P. Thacker *et al.*, “Improving alignment
    of dialogue agents via targeted human judgements,” *arXiv preprint arXiv:2209.14375*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,
    P. Barham, H. W. Chung, C. Sutton, S. Gehrmann *et al.*, “Palm: Scaling language
    modeling with pathways,” *arXiv preprint arXiv:2204.02311*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.
    Cheng, A. Jin, T. Bos, L. Baker, Y. Du *et al.*, “Lamda: Language models for dialog
    applications,” *arXiv preprint arXiv:2201.08239*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] C. Yu, J. Liu, and S. Nemati, “Reinforcement learning in healthcare:
    A survey,” *arXiv preprint arXiv:1908.08796*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] A. Alansary, O. Oktay, Y. Li, L. Le Folgoc, B. Hou, G. Vaillant, K. Kamnitsas,
    A. Vlontzos, B. Glocker, B. Kainz *et al.*, “Evaluating reinforcement learning
    agents for anatomical landmark detection,” 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] K. Ma, J. Wang, V. Singh, B. Tamersoy, Y.-J. Chang, A. Wimmer, and T. Chen,
    “Multimodal image registration with deep context reinforcement learning,” *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] T. S. M. T. Gomes, “Reinforcement learning for primary care e appointment
    scheduling,” 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] A. Serrano, B. Imbernón, H. Pérez-Sánchez, J. M. Cecilia, A. Bueno-Crespo,
    and J. L. Abellán, “Accelerating drugs discovery with deep reinforcement learning:
    An early approach,” *International Conference on Parallel Processing Companion*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] M. Popova, O. Isayev, and A. Tropsha, “Deep reinforcement learning for
    de novo drug design,” *Science advances*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] A. E. Gaweda, M. K. Muezzinoglu, G. R. Aronoff, A. A. Jacobs, J. M. Zurada,
    and M. E. Brier, “Incorporating prior knowledge into q-learning for drug delivery
    individualization,” *Fourth International Conference on Machine Learning and Applications*,
    2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] T. W. Killian, S. Daulton, G. Konidaris, and F. Doshi-Velez, “Robust
    and efficient transfer learning with hidden parameter markov decision processes,”
    *NeurIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] A. Holzinger, “Interactive machine learning for health informatics: when
    do we need the human-in-the-loop?” *Brain Informatics*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] L. Li, Y. Lv, and F.-Y. Wang, “Traffic signal timing via deep reinforcement
    learning,” *IEEE/CAA Journal of Automatica Sinica*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] K. Lin, R. Zhao, Z. Xu, and J. Zhou, “Efficient large-scale fleet management
    via multi-agent deep reinforcement learning,” *ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] K.-L. A. Yau, J. Qadir, H. L. Khoo, M. H. Ling, and P. Komisarczuk, “A
    survey on reinforcement learning models and algorithms for traffic signal control,”
    *ACM Computing Surveys (CSUR)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] J. Moody, L. Wu, Y. Liao, and M. Saffell, “Performance functions and
    reinforcement learning for trading systems and portfolios,” *Journal of Forecasting*,
    1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Z. Jiang and J. Liang, “Cryptocurrency portfolio management with deep
    reinforcement learning,” *IEEE Intelligent Systems Conference (IntelliSys)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] R. Neuneier, “Enhancing q-learning for optimal asset allocation,” *NeurIPS*,
    1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Y. Deng, F. Bao, Y. Kong, Z. Ren, and Q. Dai, “Deep direct reinforcement
    learning for financial signal representation and trading,” *IEEE transactions
    on neural networks and learning systems*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] G. Dalal, E. Gilboa, and S. Mannor, “Hierarchical decision making in
    electricity grid management,” *International Conference on Machine Learning*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] F. Ruelens, B. J. Claessens, S. Vandael, B. De Schutter, R. Babuška,
    and R. Belmans, “Residential demand response of thermostatically controlled loads
    using batch reinforcement learning,” *IEEE Transactions on Smart Grid*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Z. Wen, D. O’Neill, and H. Maei, “Optimal demand response using device-based
    reinforcement learning,” *IEEE Transactions on Smart Grid*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Y. Li, J. Song, and S. Ermon, “Infogail: Interpretable imitation learning
    from visual demonstrations,” *NeurIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] R. Ramakrishnan and J. Shah, “Towards interpretable explanations for
    transfer learning in sequential tasks,” *AAAI Spring Symposium Series*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and W. Stewart,
    “Retain: An interpretable predictive model for healthcare using reverse time attention
    mechanism,” *NeurIPS*, vol. 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/b34d65ded02e12d33b5657669c3a5105.png) | Zhuangdi
    Zhu is currently a senior data and applied scientist with Microsoft. She obtained
    her Ph.D. degree from the Computer Science department of Michigan State University.
    Zhuangdi has regularly published on prestigious machine learning conferences including
    NeurIPs, ICML, KDD, AAAI, etc. Her research interests reside in both fundamental
    and applied machine learning. Her current research involves reinforcement learning
    and distributed machine learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/b22c54ee22751df2b5812a557e7b3f29.png) | Kaixiang
    Lin is an applied scientist at Amazon web services. He obtained his Ph.D. from
    Michigan State University. He has broad research interests across multiple fields,
    including reinforcement learning, human-robot interactions, and natural language
    processing. His research has been published on multiple top-tiered machine learning
    and data mining conferences such as ICLR, KDD, NeurIPS, etc. He serves as a reviewer
    for top machine learning conferences regularly. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/75592eae391f3c317ea8a710b4bba4ae.png) | Anil K.
    Jain is a University distinguished professor in the Department of Computer Science
    and Engineering at Michigan State University. His research interests include pattern
    recognition and biometric authentication. He served as the editor-in-chief of
    the IEEE Transactions on Pattern Analysis and Machine Intelligence and was a member
    of the United States Defense Science Board. He has received Fulbright, Guggenheim,
    Alexander von Humboldt, and IAPR King Sun Fu awards. He is a member of the National
    Academy of Engineering and a foreign fellow of the Indian National Academy of
    Engineering and the Chinese Academy of Sciences. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/960f94200352c9ade1bd175843ea8284.png) | Jiayu
    Zhou is an associate professor in the Department of Computer Science and Engineering
    at Michigan State University. He received his Ph.D. degree in computer science
    from Arizona State University in 2014\. He has broad research interests in the
    fields of large-scale machine learning and data mining as well as biomedical informatics.
    He has served as a technical program committee member for premier conferences
    such as NIPS, ICML, and SIGKDD. His papers have received the Best Student Paper
    Award at the 2014 IEEE International Conference on Data Mining (ICDM), the Best
    Student Paper Award at the 2016 International Symposium on Biomedical Imaging
    (ISBI) and the Best Paper Award at IEEE Big Data 2016. |'
  prefs: []
  type: TYPE_TB
