- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:59:31'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:59:31
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2009.07888] Transfer Learning in Deep Reinforcement Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2009.07888] 深度强化学习中的迁移学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2009.07888](https://ar5iv.labs.arxiv.org/html/2009.07888)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2009.07888](https://ar5iv.labs.arxiv.org/html/2009.07888)
- en: 'Transfer Learning in Deep Reinforcement Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习中的迁移学习：综述
- en: 'Zhuangdi Zhu,  Kaixiang Lin, Anil K. Jain, and Jiayu Zhou Zhuangdi Zhu, Anil
    K. Jain, and Jiayu Zhou are with the Department of Computer Science and Engineering,
    Michigan State University, East Lansing, MI, 48824. E-mail: {zhuzhuan, jain, jiayuz}@msu.edu
    Kaixiang Lin is with the Amazon Alexa AI. E-mail: lkxcarson@gmail.com'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 朱庄地、林凯翔、安尼尔·K·贾因和周佳宇。朱庄地、安尼尔·K·贾因和周佳宇均来自密歇根州立大学计算机科学与工程系，东兰辛，密歇根州，48824。电子邮件：{zhuzhuan,
    jain, jiayuz}@msu.edu 林凯翔在亚马逊Alexa AI工作。电子邮件：lkxcarson@gmail.com
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Reinforcement learning is a learning paradigm for solving sequential decision-making
    problems. Recent years have witnessed remarkable progress in reinforcement learning
    upon the fast development of deep neural networks. Along with the promising prospects
    of reinforcement learning in numerous domains such as robotics and game-playing,
    transfer learning has arisen to tackle various challenges faced by reinforcement
    learning, by transferring knowledge from external expertise to facilitate the
    efficiency and effectiveness of the learning process. In this survey, we systematically
    investigate the recent progress of transfer learning approaches in the context
    of deep reinforcement learning. Specifically, we provide a framework for categorizing
    the state-of-the-art transfer learning approaches, under which we analyze their
    goals, methodologies, compatible reinforcement learning backbones, and practical
    applications. We also draw connections between transfer learning and other relevant
    topics from the reinforcement learning perspective and explore their potential
    challenges that await future research progress.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种解决顺序决策问题的学习范式。近年来，随着深度神经网络的快速发展，强化学习取得了显著进展。随着强化学习在机器人技术和游戏等众多领域的前景广阔，迁移学习应运而生，旨在通过将外部知识转移以提高学习过程的效率和效果，从而应对强化学习面临的各种挑战。在这项综述中，我们系统地研究了在深度强化学习背景下的迁移学习方法的最新进展。具体而言，我们提供了一个框架来分类最先进的迁移学习方法，并在该框架下分析它们的目标、方法、兼容的强化学习骨干和实际应用。我们还从强化学习的角度探讨了迁移学习与其他相关主题之间的联系，并探讨了未来研究进展中可能面临的挑战。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Transfer Learning, Reinforcement Learning, Deep Learning, Survey.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习，强化学习，深度学习，综述。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Reinforcement Learning (RL) is an effective framework to solve sequential decision-making
    tasks, where a learning agent interacts with the environment to improve its performance
    through trial and error [[1](#bib.bib1)]. Originated from cybernetics and thriving
    in computer science, RL has been widely applied to tackle challenging tasks which
    were previously intractable. Traditional RL algorithms were mostly designed for
    tabular cases, which provide principled solutions to simple tasks but face difficulties
    when handling highly complex domains, e.g. tasks with 3D environments. With the
    recent advances in deep learning research, the combination of RL and deep neural
    networks is developed to address challenging tasks. The combination of deep learning
    with RL is hence referred to as Deep Reinforcement Learning (DRL) [[2](#bib.bib2)],
    which learns powerful function approximators using deep neural networks to address
    complicated domains. DRL has achieved notable success in applications such as
    robotics control [[3](#bib.bib3), [4](#bib.bib4)] and game playing [[5](#bib.bib5)].
    It also thrives in domains such as health informatics [[6](#bib.bib6)], electricity
    networks [[7](#bib.bib7)], intelligent transportation systems[[8](#bib.bib8),
    [9](#bib.bib9)], to name just a few.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是解决序列决策任务的有效框架，其中学习代理通过试错与环境交互以提高其性能[[1](#bib.bib1)]。起源于控制论并在计算机科学中蓬勃发展，RL已被广泛应用于解决以前难以处理的挑战任务。传统的RL算法大多为表格型问题设计，虽然它们提供了对简单任务的有原则的解决方案，但在处理高度复杂的领域时，如3D环境任务，面临困难。随着深度学习研究的最新进展，RL与深度神经网络的结合被开发出来以应对挑战性任务。因此，深度学习与RL的结合被称为深度强化学习（DRL）[[2](#bib.bib2)]，它使用深度神经网络学习强大的函数逼近器来解决复杂领域。DRL在机器人控制[[3](#bib.bib3),
    [4](#bib.bib4)]和游戏玩法[[5](#bib.bib5)]等应用中取得了显著成功。它还在健康信息学[[6](#bib.bib6)]、电力网络[[7](#bib.bib7)]、智能交通系统[[8](#bib.bib8),
    [9](#bib.bib9)]等领域表现突出。
- en: Besides its remarkable advancement, RL still faces intriguing difficulties induced
    by the exploration-exploitation dilemma [[1](#bib.bib1)]. Specifically, for practical
    RL problems, the environment dynamics are usually unknown, and the agent cannot
    exploit knowledge about the environment until enough interaction experiences are
    collected via exploration. Due to the partial observability, sparse feedbacks,
    and the high complexity of state and action spaces, acquiring sufficient interaction
    samples can be prohibitive or even incur safety concerns for domains such as automatic-driving
    and health informatics. The abovementioned challenges have motivated various efforts
    to improve the current RL procedure. As a result, *transfer learning* (TL), or
    equivalently referred as *knowledge transfer*, which is a technique to utilize
    external expertise to benefit the learning process of the target domain, becomes
    a crucial topic in RL.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 除了显著的进展，RL（强化学习）仍面临由探索与利用困境引发的有趣困难[[1](#bib.bib1)]。具体而言，对于实际的RL问题，环境动态通常是未知的，代理在通过探索收集足够的互动经验之前，无法利用关于环境的知识。由于部分可观察性、稀疏反馈以及状态和动作空间的高复杂性，获取足够的互动样本可能会变得困难甚至引发安全隐患，例如在自动驾驶和健康信息学领域。这些挑战促使了各种改进当前RL过程的努力。因此，*迁移学习*（TL），或等同于*知识迁移*，即利用外部专业知识来促进目标领域的学习过程，成为RL中的一个关键话题。
- en: While TL techniques have been extensively studied in supervised learning [[10](#bib.bib10)],
    it is still an emerging topic for RL. Transfer learning can be more complicated
    for RL, in that the knowledge needs to transfer in the context of a Markov Decision
    Process. Moreover, due to the delicate components of the Markov decision process,
    expert knowledge may take different forms that need to transfer in different ways.
    Noticing that previous efforts on summarizing TL in the RL domain did not cover
    research of the last decade [[11](#bib.bib11), [12](#bib.bib12)], during which
    time considerate TL breakthroughs have been achieved empowered with deep learning
    techniques. Hence, in this survey, we make a comprehensive investigation of the
    latest TL approaches in RL.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然TL技术在监督学习中得到了广泛研究[[10](#bib.bib10)]，但它在RL中仍是一个新兴话题。对于RL而言，迁移学习可能更加复杂，因为知识需要在马尔可夫决策过程中转移。此外，由于马尔可夫决策过程的细致组成部分，专家知识可能以不同的形式存在，需要以不同的方式进行迁移。注意到以前对RL领域TL总结的努力并未涵盖过去十年的研究[[11](#bib.bib11),
    [12](#bib.bib12)]，而在此期间，借助深度学习技术，取得了显著的TL突破。因此，在本次调查中，我们对RL中的最新TL方法进行了全面的研究。
- en: 'The contributions of our survey are multifold: 1) we investigated up-to-date
    research involving new DRL backbones and TL algorithms over the recent decade.
    To the best of our knowledge, this survey is the first attempt to survey TL approaches
    in the context of *deep* reinforcement learning. We reviewed TL methods that can
    tackle more evolved RL tasks, and also studied new TL schemes that are not deeply
    discussed by prior literatures, such as representation disentanglement (Sec [5.5](#S5.SS5
    "5.5 Representation Transfer ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer
    Learning in Deep Reinforcement Learning: A Survey")) and policy distillation (Sec
    [5.3](#S5.SS3 "5.3 Policy Transfer ‣ 5 Transfer Learning Approaches Deep Dive
    ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")). 2) We provided
    systematic categorizations that cover a broader and deeper view of TL developments
    in DRL. Our main analysis is anchored on a fundamental question, i.e. *what is
    the transferred knowledge in RL*, following which we conducted more refined analysis.
    Most TL strategies, including those discussed in prior surveys are well suited
    in our categorization framework. 3) Reflecting on the developments of TL methods
    in DRL, we brought new thoughts on its future directions, including how to do
    *reasoning* over miscellaneous knowledge forms and how to *leverage* knowledge
    in more efficient and principled manner. We also pointed out the prominent applications
    of TL for DRL and its opportunities to thrive in the future era of AGI.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查有多方面的贡献：1）我们调查了最近十年涉及新型深度强化学习（DRL）骨干网和迁移学习（TL）算法的最新研究。据我们所知，这项调查是首个尝试在*深度*强化学习背景下调研迁移学习方法的工作。我们回顾了能够解决更复杂强化学习任务的迁移学习方法，并研究了之前文献中未深入讨论的新迁移学习方案，例如表示解缠结（见
    [5.5](#S5.SS5 "5.5 表示迁移 ‣ 5 迁移学习方法深度分析 ‣ 深度强化学习中的迁移学习：一项调查")）和策略蒸馏（见 [5.3](#S5.SS3
    "5.3 策略迁移 ‣ 5 迁移学习方法深度分析 ‣ 深度强化学习中的迁移学习：一项调查")）。2）我们提供了系统的分类，涵盖了DRL中迁移学习发展的更广泛和深入的视角。我们的主要分析基于一个基本问题，即*强化学习中的迁移知识是什么*，在此基础上我们进行了更精细的分析。大多数迁移学习策略，包括之前调查中讨论的那些，均适用于我们的分类框架。3）在反思DRL中迁移学习方法的发展时，我们对其未来方向提出了新思路，包括如何对各种知识形式进行*推理*，以及如何*利用*知识以更高效和更有原则的方式。我们还指出了迁移学习在DRL中的突出应用及其在未来AGI时代蓬勃发展的机会。
- en: 'The rest of this survey is organized as follows: In Section [2](#S2 "2 Deep
    Reinforcement Learning and Transfer Learning ‣ Transfer Learning in Deep Reinforcement
    Learning: A Survey") we introduce RL preliminaries, including the recent key development
    based on deep neural networks. Next, we discuss the definition of TL in the context
    of RL and its relevant topics (Section [2.4](#S2.SS4 "2.4 Related Topics ‣ 2 Deep
    Reinforcement Learning and Transfer Learning ‣ Transfer Learning in Deep Reinforcement
    Learning: A Survey")). In Section [3](#S3 "3 Analyzing Transfer Learning ‣ Transfer
    Learning in Deep Reinforcement Learning: A Survey"), we provide a framework to
    categorize TL approaches from multiple perspectives, analyze their fundamental
    differences, and summarize their evaluation metrics (Section [3.3](#S3.SS3 "3.3
    Evaluation metrics ‣ 3 Analyzing Transfer Learning ‣ Transfer Learning in Deep
    Reinforcement Learning: A Survey")). In Section [5](#S5 "5 Transfer Learning Approaches
    Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey"), we elaborate
    on different TL approaches in the context of DRL, organized by the format of transferred
    knowledge, such as reward shaping (Section [5.1](#S5.SS1 "5.1 Reward Shaping ‣
    5 Transfer Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement
    Learning: A Survey")), learning from demonstrations (Section [5.2](#S5.SS2 "5.2
    Learning from Demonstrations ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer
    Learning in Deep Reinforcement Learning: A Survey")), or learning from teacher
    policies (Section [5.3](#S5.SS3 "5.3 Policy Transfer ‣ 5 Transfer Learning Approaches
    Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")). We
    also investigate TL approaches by the way that knowledge transfer occurs, such
    as inter-task mapping (Section [5.4](#S5.SS4 "5.4 Inter-Task Mapping ‣ 5 Transfer
    Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning:
    A Survey")), or learning transferrable representations (Section [5.5](#S5.SS5
    "5.5 Representation Transfer ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer
    Learning in Deep Reinforcement Learning: A Survey")), etc. We discuss contemporary
    applications of TL in the context of DRL in Section [6](#S6 "6 Applications ‣
    Transfer Learning in Deep Reinforcement Learning: A Survey") and provide some
    future perspectives and open questions in Section [7](#S7 "7 Future Perspectives
    ‣ Transfer Learning in Deep Reinforcement Learning: A Survey").'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的其余部分组织如下：在第[2](#S2 "2 深度强化学习与迁移学习 ‣ 深度强化学习中的迁移学习：综述")节中，我们介绍了强化学习的基础知识，包括基于深度神经网络的最新关键发展。接下来，我们讨论了在强化学习背景下迁移学习的定义及其相关主题（第[2.4](#S2.SS4
    "2.4 相关主题 ‣ 2 深度强化学习与迁移学习 ‣ 深度强化学习中的迁移学习：综述")节）。在第[3](#S3 "3 分析迁移学习 ‣ 深度强化学习中的迁移学习：综述")节中，我们提供了一个框架，从多个角度对迁移学习方法进行分类，分析其基本差异，并总结其评估指标（第[3.3](#S3.SS3
    "3.3 评估指标 ‣ 3 分析迁移学习 ‣ 深度强化学习中的迁移学习：综述")节）。在第[5](#S5 "5 迁移学习方法深度探讨 ‣ 深度强化学习中的迁移学习：综述")节中，我们详细阐述了不同的迁移学习方法，按照转移知识的形式进行组织，例如奖励塑造（第[5.1](#S5.SS1
    "5.1 奖励塑造 ‣ 5 迁移学习方法深度探讨 ‣ 深度强化学习中的迁移学习：综述")节）、从示例中学习（第[5.2](#S5.SS2 "5.2 从示例中学习
    ‣ 5 迁移学习方法深度探讨 ‣ 深度强化学习中的迁移学习：综述")节）或从教师策略中学习（第[5.3](#S5.SS3 "5.3 策略转移 ‣ 5 迁移学习方法深度探讨
    ‣ 深度强化学习中的迁移学习：综述")节）。我们还根据知识转移发生的方式研究迁移学习方法，例如任务间映射（第[5.4](#S5.SS4 "5.4 任务间映射
    ‣ 5 迁移学习方法深度探讨 ‣ 深度强化学习中的迁移学习：综述")节）或学习可迁移表示（第[5.5](#S5.SS5 "5.5 表示转移 ‣ 5 迁移学习方法深度探讨
    ‣ 深度强化学习中的迁移学习：综述")节）等。在第[6](#S6 "6 应用 ‣ 深度强化学习中的迁移学习：综述")节中，我们讨论了迁移学习在深度强化学习背景下的当代应用，并在第[7](#S7
    "7 未来展望 ‣ 深度强化学习中的迁移学习：综述")节中提供了一些未来的展望和开放性问题。
- en: 2 Deep Reinforcement Learning and Transfer Learning
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度强化学习与迁移学习
- en: 2.1 Reinforcement Learning Basics
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 强化学习基础
- en: '*Markov Decision Process:* A typical RL problem can be considered as training
    an agent to interact with an environment that follows a Markov Decision Process
    (MPD) [[13](#bib.bib13)]. The agent starts with an initial state and performs
    an *action* accordingly, which yields a *reward* to guide the agent actions. Once
    the action is taken, the MDP transits to the next state by following the underlying
    *transition dynamics* of the MDP. The agent accumulates the time-*discounted*
    rewards along with its interactions. A subsequence of interactions is referred
    to as an *episode*. The above-mentioned components in an MDP can be represented
    using a tuple, i.e. ${\mathcal{M}}=(\mu_{0},{\mathcal{S}},{\mathcal{A}},{\mathcal{T}},\gamma,{\mathcal{R}}$),
    in which:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*马尔可夫决策过程（MDP）:* 一个典型的强化学习问题可以被视为训练一个代理与遵循马尔可夫决策过程（MDP）的环境互动[[13](#bib.bib13)]。代理从初始状态开始，根据情况执行一个*动作*，该动作产生一个*奖励*来引导代理的行为。一旦动作执行完毕，MDP
    会按照其内在的*转移动态*转移到下一个状态。代理在互动过程中累计时间*折扣*奖励。互动的一个子序列被称为一个*回合*。上述提到的 MDP 组件可以使用一个元组表示，即
    ${\mathcal{M}}=(\mu_{0},{\mathcal{S}},{\mathcal{A}},{\mathcal{T}},\gamma,{\mathcal{R}}$)，其中：'
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mu_{0}$ is the set of initial states.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mu_{0}$ 是初始状态的集合。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ${\mathcal{S}}$ is the state space.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ${\mathcal{S}}$ 是状态空间。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ${\mathcal{A}}$ is the action space.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ${\mathcal{A}}$ 是动作空间。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '${\mathcal{T}}$: $S\times A\times S\to\mathbb{R}$ is the transition probability
    distribution, where ${\mathcal{T}}(s^{\prime}|s,a)$ specifies the probability
    of the state transitioning to $s^{\prime}$ upon taking action $a$ from state $s$.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '${\mathcal{T}}$: $S\times A\times S\to\mathbb{R}$ 是转移概率分布，其中 ${\mathcal{T}}(s^{\prime}|s,a)$
    指定了从状态 $s$ 采取动作 $a$ 后，状态转移到 $s^{\prime}$ 的概率。'
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ${\mathcal{R}}:S\times A\times S\to\mathbb{R}$ is the reward distribution, where
    ${\mathcal{R}}(s,a,s^{\prime})$ is the reward that an agent can get by taking
    action $a$ from state $s$ with the next state being $s^{\prime}$.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ${\mathcal{R}}:S\times A\times S\to\mathbb{R}$ 是奖励分布，其中 ${\mathcal{R}}(s,a,s^{\prime})$
    是代理从状态 $s$ 采取动作 $a$ 并使下一状态为 $s^{\prime}$ 时可以获得的奖励。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\gamma$ is a discounted factor, with $\gamma\in(0,1]$.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\gamma$ 是折扣因子，$\gamma\in(0,1]$。
- en: 'A RL agent behaves in ${\mathcal{M}}$ by following its policy $\pi$, which
    is a mapping from states to actions: $\pi:{\mathcal{S}}\to{\mathcal{A}}$ . For
    a stochastic policy $\pi$, $\pi(a|s)$ denotes the probability of taking action
    $a$ from state $s$. Given an MDP ${\mathcal{M}}$ and a policy $\pi$, one can derive
    a value function $V_{\mathcal{M}}^{\pi}(s)$, which is defined over the state space:
    $V_{\mathcal{M}}^{\pi}(s)={\mathds{E}}\left[r_{0}+\gamma r_{1}+\gamma^{2}r_{2}+\dots;\pi,s\right],$
    where $r_{i}={\mathcal{R}}(s_{i},a_{i},s_{i+1})$ is the reward that an agent receives
    by taking action $a_{i}$ in the $i$-th state $s_{i}$, and the next state transits
    to $s_{i+1}$. The expectation ${\mathds{E}}$ is taken over ${s_{0}\sim\mu_{0},a_{i}\sim\pi(\cdot|s_{i}),s_{i+1}\sim{\mathcal{T}}(\cdot|s_{i},a_{i})}$.
    The value function estimates the quality of being in state $s$, by evaluating
    the expected rewards that an agent can get from $s$ following policy $\pi$. Similar
    to the value function, a policy also carries a $Q$-function, which estimates the
    quality of taking action $a$ from state $s$: $Q_{\mathcal{M}}^{\pi}(s,a)={\mathds{E}}_{s^{\prime}\sim{\mathcal{T}}(\cdot|s,a)}\left[{\mathcal{R}}(s,a,s^{\prime})+\gamma
    V_{\mathcal{M}}^{\pi}(s^{\prime})\right].$ *Reinforcement Learning Goals:* Standard
    RL aims to learn an optimal policy $\pi_{{\mathcal{M}}}^{*}$ with the optimal
    value and $Q$-function, s.t. $\forall s\in{\mathcal{S}},\pi_{\mathcal{M}}^{*}(s)=\underset{a\in
    A}{\arg\max}~{}~{}{Q_{\mathcal{M}}^{*}(s,a)},$ where $Q_{\mathcal{M}}^{*}(s,a)=\underset{\pi}{\sup}~{}~{}Q_{\mathcal{M}}^{\pi}(s,a)$.
    The learning objective can be reduced as maximizing the expected return:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个RL智能体在${\mathcal{M}}$中通过遵循其策略$\pi$来行为，这是一种从状态到动作的映射：$\pi:{\mathcal{S}}\to{\mathcal{A}}$。对于随机策略$\pi$，$\pi(a|s)$表示从状态$s$采取动作$a$的概率。给定一个MDP
    ${\mathcal{M}}$和策略$\pi$，可以推导出一个值函数$V_{\mathcal{M}}^{\pi}(s)$，它定义在状态空间上：$V_{\mathcal{M}}^{\pi}(s)={\mathds{E}}\left[r_{0}+\gamma
    r_{1}+\gamma^{2}r_{2}+\dots;\pi,s\right]$，其中$r_{i}={\mathcal{R}}(s_{i},a_{i},s_{i+1})$是智能体在第$i$个状态$s_{i}$采取动作$a_{i}$所获得的奖励，且下一个状态转移到$s_{i+1}$。期望${\mathds{E}}$是对${s_{0}\sim\mu_{0},a_{i}\sim\pi(\cdot|s_{i}),s_{i+1}\sim{\mathcal{T}}(\cdot|s_{i},a_{i})}$进行的。值函数通过评估智能体在状态$s$时根据策略$\pi$可以获得的预期奖励，来估计处于状态$s$的质量。与值函数类似，策略也具有一个$Q$-函数，它估计从状态$s$采取动作$a$的质量：$Q_{\mathcal{M}}^{\pi}(s,a)={\mathds{E}}_{s^{\prime}\sim{\mathcal{T}}(\cdot|s,a)}\left[{\mathcal{R}}(s,a,s^{\prime})+\gamma
    V_{\mathcal{M}}^{\pi}(s^{\prime})\right]$。*强化学习目标：* 标准RL旨在学习一个具有最优值和$Q$-函数的最优策略$\pi_{{\mathcal{M}}}^{*}$，使得$\forall
    s\in{\mathcal{S}},\pi_{\mathcal{M}}^{*}(s)=\underset{a\in A}{\arg\max}~{}~{}{Q_{\mathcal{M}}^{*}(s,a)}$，其中$Q_{\mathcal{M}}^{*}(s,a)=\underset{\pi}{\sup}~{}~{}Q_{\mathcal{M}}^{\pi}(s,a)$。学习目标可以简化为最大化预期回报：
- en: '|  | $\displaystyle J(\pi):={\mathds{E}}_{(s,a)\sim\mu^{\pi}(s,a)}[\sum_{t}\gamma^{t}r_{t}],\vspace{-0.1in}$
    |  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J(\pi):={\mathds{E}}_{(s,a)\sim\mu^{\pi}(s,a)}[\sum_{t}\gamma^{t}r_{t}],\vspace{-0.1in}$
    |  |'
- en: where $\mu^{\pi}(s,a)$ is the *stationary state-action distribution* induced
    by $\pi$ [[14](#bib.bib14)].
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mu^{\pi}(s,a)$是由$\pi$引起的*平稳状态-动作分布* [[14](#bib.bib14)]。
- en: 'Built upon recent progress of DRL, some literature has extended the RL objective
    to achieving miscellaneous goals under different conditions, referred to as *Goal-Conditional
    RL* (GCRL). In GCRL, the agent policy $\pi(\cdot|s,g)$ is dependent not only on
    state observations $s$ but also the goal $g$ being optimized. Each individual
    goal $g\sim{\mathcal{G}}$ can be differentiated by its reward function $r(s_{t},a_{t},g)$,
    hence the objective for GCRL becomes maximizing the expected return over the distribution
    of goals: $J(\pi):={\mathds{E}}_{(s_{t},a_{t})\sim\mu^{\pi},g\sim{\mathcal{G}}}\left[\sum_{t}\gamma^{t}r(s,a,g)\right]$
     [[15](#bib.bib15)]. A prototype example of GCRL can be maze locomotion tasks,
    where the learning goals are manifested as desired locations in the maze [[16](#bib.bib16)].'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在DRL的最新进展基础上，一些文献扩展了RL目标，以在不同条件下实现各种目标，这被称为*目标条件强化学习*（GCRL）。在GCRL中，智能体策略$\pi(\cdot|s,g)$不仅依赖于状态观察$s$，还依赖于被优化的目标$g$。每个个体目标$g\sim{\mathcal{G}}$可以通过其奖励函数$r(s_{t},a_{t},g)$进行区分，因此GCRL的目标变为最大化目标分布下的预期回报：$J(\pi):={\mathds{E}}_{(s_{t},a_{t})\sim\mu^{\pi},g\sim{\mathcal{G}}}\left[\sum_{t}\gamma^{t}r(s,a,g)\right]$
    [[15](#bib.bib15)]。GCRL的一个原型例子可以是迷宫运动任务，其中学习目标表现为迷宫中的期望位置[[16](#bib.bib16)]。
- en: '*Episodic vs. Non-episodic Reinforcement Learning:* In episodic RL, the agent
    performs in finite episodes of length $H$, and will be *reset* to an initial state
    $\in\mu_{0}$ upon the episode ends[[1](#bib.bib1)]. Whereas in non-episodic RL,
    the learning agent continuously interacts with the MDP without any state reset [[17](#bib.bib17)].
    To encompass the episodic concept in infinite MDPs, episodic RL tasks usually
    assume the existence of a set of absorbing states  ${\mathcal{S}}_{0}$, which
    indicates the termination of episodic tasks [[18](#bib.bib18), [19](#bib.bib19)],
    and any action taken upon an absorbing state will only transit to itself with
    zero rewards.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*情景性强化学习与非情景性强化学习：* 在情景性 RL 中，智能体在有限长度为 $H$ 的回合中执行任务，并在回合结束时被*重置*到初始状态 $\in\mu_{0}$[[1](#bib.bib1)]。而在非情景性
    RL 中，学习智能体与 MDP 持续交互而不进行任何状态重置[[17](#bib.bib17)]。为了在无限 MDP 中涵盖情景性概念，情景性 RL 任务通常假设存在一组吸收状态
    ${\mathcal{S}}_{0}$，这些状态表示情景性任务的终止[[18](#bib.bib18), [19](#bib.bib19)]，并且在吸收状态下进行的任何动作只会转移到自身，并且奖励为零。'
- en: 2.2 Reinforcement Learning Algorithms
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 强化学习算法
- en: 'There are two major methods to conduct RL: *Model-Based* and *Model-Free*.
    In *model-based* RL, a learned or provided model of the MDP is used for policy
    learning. In *model-free* RL, optimal policy is learned without modeling the transition
    dynamics or reward functions. In this section, we start introducing RL techniques
    from a *model-free* perspective, due to its relatively simplicity, which also
    provides foundations for many model-based methods.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种主要的方法来进行 RL：*基于模型* 和 *无模型*。在 *基于模型* 的 RL 中，使用学习或提供的 MDP 模型来进行策略学习。在 *无模型*
    的 RL 中，优化策略是在不建模转移动态或奖励函数的情况下学习的。在本节中，我们从 *无模型* 视角开始介绍 RL 技术，由于其相对简单，这也为许多基于模型的方法提供了基础。
- en: '*Prediction and Control*: an RL problem can be disassembled into two subtasks:
    *prediction* and *control* [[1](#bib.bib1)]. In the *prediction* phase, the quality
    of the current policy is being evaluated. In the *control* phase or the *policy
    improvement* phase, the learning policy is adjusted based on evaluation results
    from the *prediction* step. Policies can be improved by iterating through these
    two steps, known as *policy iteration*.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*预测与控制*：一个 RL 问题可以分解为两个子任务：*预测* 和 *控制* [[1](#bib.bib1)]。在 *预测* 阶段，当前策略的质量被评估。在
    *控制* 阶段或 *策略改进* 阶段，学习策略根据 *预测* 步骤中的评估结果进行调整。策略可以通过迭代这两个步骤来改进，这被称为 *策略迭代*。'
- en: For *model-free* policy iterations, the target policy is optimized without requiring
    knowledge of the MDP transition dynamics. Traditional model-free RL includes *Monte-Carlo*
    methods, which estimates the value of each state using *samples of episodes* starting
    from that state. Monte-Carlo methods can be *on-policy* if the samples are collected
    by following the target policy, or *off-policy* if the episodic samples are collected
    by following a *behavior* policy that is different from the target policy.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *无模型* 策略迭代，目标策略的优化不需要了解 MDP 转移动态。传统的无模型 RL 包括 *蒙特卡洛* 方法，该方法通过使用 *回合样本* 来估计每个状态的值，这些样本是从该状态开始的。如果样本是通过跟随目标策略收集的，则蒙特卡洛方法可以是
    *在策略* 的，或者如果样本是通过跟随不同于目标策略的 *行为* 策略收集的，则可以是 *离策略* 的。
- en: '*Temporal Difference (TD) Learning* is an alternative to Monte-Carlo for solving
    the *prediction* problem. The key idea behind TD-learning is to learn the state
    quality function by *bootstrapping*. It can also be extended to solve the *control*
    problem so that both value function and policy can get improved simultaneously.
    Examples of *on-policy* TD-learning algorithms include *SARSA* [[20](#bib.bib20)],
    *Expected SARSA* [[21](#bib.bib21)], *Actor-Critic* [[22](#bib.bib22)], and its
    deep neural network extension called *A3C* [[23](#bib.bib23)]. The *off-policy*
    TD-learning approaches include SAC [[24](#bib.bib24)] for continuous state-action
    spaces, and $Q$-learning [[25](#bib.bib25)] for discrete state-action spaces,
    along with its variants built on deep-neural networks, such as DQN [[26](#bib.bib26)],
    Double-DQN [[26](#bib.bib26)], Rainbow [[27](#bib.bib27)], etc. TD-learning approaches
    focus more on estimating the state-action value functions.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*时间差分（TD）学习*是解决*预测*问题的另一种方法。TD学习的核心思想是通过*自举*来学习状态质量函数。它也可以扩展到解决*控制*问题，从而同时改善价值函数和策略。*在线*
    TD学习算法的例子包括*SARSA* [[20](#bib.bib20)]、*期望SARSA* [[21](#bib.bib21)]、*演员-评论家* [[22](#bib.bib22)]及其深度神经网络扩展版本*
    A3C* [[23](#bib.bib23)]。*离线* TD学习方法包括SAC [[24](#bib.bib24)]用于连续状态-动作空间，以及$Q$-学习 [[25](#bib.bib25)]用于离散状态-动作空间，还有基于深度神经网络的变体，如DQN [[26](#bib.bib26)]、双重DQN [[26](#bib.bib26)]、Rainbow [[27](#bib.bib27)]等。TD学习方法更注重于估计状态-动作值函数。'
- en: '*Policy Gradient*, on the other hand, is a mechanism that emphasizes on direct
    optimization of a parameterizable policy. Traditional policy-gradient approaches
    include *REINFORCE* [[28](#bib.bib28)]. Recent years have witnessed the joint
    presence of TD-learning and policy-gradient approaches. Representative algorithms
    along this line include *Trust region policy optimization (TRPO)* [[29](#bib.bib29)],
    *Proximal Policy optimization (PPO)* [[30](#bib.bib30)], *Deterministic policy
    gradient (DPG)* [[31](#bib.bib31)] and its extensions such as *DDPG* [[32](#bib.bib32)]
    and *Twin Delayed DDPG* [[33](#bib.bib33)].'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*策略梯度*，另一方面，是一种强调对可参数化策略进行直接优化的机制。传统的策略梯度方法包括*REINFORCE* [[28](#bib.bib28)]。近年来，TD学习和策略梯度方法的联合出现成为常态。代表性的算法包括*信任区域策略优化（TRPO）* [[29](#bib.bib29)]、*近端策略优化（PPO）* [[30](#bib.bib30)]、*确定性策略梯度（DPG）* [[31](#bib.bib31)]及其扩展，如*DDPG* [[32](#bib.bib32)]和*双延迟DDPG* [[33](#bib.bib33)]。'
- en: Unlike model-free methods that learn purely from trial-and-error, *Model-Based
    RL* (MBRL) explicitly learns the transition dynamics or cost functions of the
    environment. The dynamics model can sometimes be treated as a *black-box* for
    better *sampling-based planning*. Representative examples include the *Monte-Carlo*
    method dubbed *random shooting* [[34](#bib.bib34)] and its cross-entropy method
    (CEM) variants [[35](#bib.bib35), [36](#bib.bib36)]. The modeled dynamics can
    also facilitate learning with data generation [[37](#bib.bib37)] and value estimation [[38](#bib.bib38)].
    For MBRL with *white-box* modeling, the transition models become differentiable
    and can facilitate planning with direct gradient propogation. Methods along this
    line include *differential planning* for policy gradient [[39](#bib.bib39)] and
    action sequences search [[40](#bib.bib40)], and value gradient methods [[41](#bib.bib41),
    [42](#bib.bib42)]. One advantage of MBRL is its higher sample efficiency than
    model-free RL, although it can be challenging for complex domains, where it is
    usually more difficult to learn the dynamics than learning a policy.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与纯粹通过试错学习的无模型方法不同，*基于模型的强化学习*（MBRL）明确地学习环境的过渡动态或成本函数。动态模型有时可以被视为*黑箱*以便于*基于采样的规划*。代表性的例子包括被称为*随机射击*的*蒙特卡洛*方法 [[34](#bib.bib34)]及其交叉熵方法（CEM）变体 [[35](#bib.bib35),
    [36](#bib.bib36)]。建模的动态还可以促进数据生成 [[37](#bib.bib37)]和价值估计 [[38](#bib.bib38)]。对于具有*白箱*建模的MBRL，过渡模型变得可微分，并可以通过直接梯度传播来促进规划。这类方法包括用于策略梯度的*微分规划* [[39](#bib.bib39)]和动作序列搜索 [[40](#bib.bib40)]，以及价值梯度方法 [[41](#bib.bib41),
    [42](#bib.bib42)]。MBRL的一个优势是比无模型RL具有更高的样本效率，尽管对于复杂领域来说，这可能是一个挑战，因为在这些领域中，学习动态通常比学习策略更为困难。
- en: 2.3 Transfer Learning in the Context of Reinforcement Learning
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 强化学习中的迁移学习
- en: Remark 1.
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 1。
- en: Without losing clarify, for the rest of this survey, we refer to MDPs, domains,
    and tasks equivalently.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在不失去清晰度的情况下，在本调查的其余部分，我们将MDPs、领域和任务等同地 refer to。
- en: Remark 2.
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 2。
- en: '*[Transfer Learning in the Context of RL]* Given a set of source domains $\bm{{\mathcal{M}}}_{s}=\{{\mathcal{M}}_{s}|{\mathcal{M}}_{s}\in\bm{{\mathcal{M}}}_{s}\}$
    and a target domain ${\mathcal{M}}_{t}$, *Transfer Learning* aims to learn an
    optimal policy $\pi^{*}$ for the target domain, by leveraging exterior information
    ${\mathcal{I}}_{s}$ from $\bm{{\mathcal{M}}}_{s}$ as well as interior information
    ${\mathcal{I}}_{t}$ from ${\mathcal{M}}_{t}$:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*[迁移学习在RL中的应用]* 给定一组源领域 $\bm{{\mathcal{M}}}_{s}=\{{\mathcal{M}}_{s}|{\mathcal{M}}_{s}\in\bm{{\mathcal{M}}}_{s}\}$
    和一个目标领域 ${\mathcal{M}}_{t}$，*迁移学习* 旨在通过利用来自 $\bm{{\mathcal{M}}}_{s}$ 的外部信息 ${\mathcal{I}}_{s}$
    以及来自 ${\mathcal{M}}_{t}$ 的内部信息 ${\mathcal{I}}_{t}$，来为目标领域学习一个最优策略 $\pi^{*}$。'
- en: '|  | $\displaystyle~{}\pi^{*}=\operatorname*{arg\,max}_{\pi}{\mathds{E}}_{s\sim\mu_{0}^{t},a\sim\pi}[Q^{\pi}_{{\mathcal{M}}}(s,a)],$
    |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle~{}\pi^{*}=\operatorname*{arg\,max}_{\pi}{\mathds{E}}_{s\sim\mu_{0}^{t},a\sim\pi}[Q^{\pi}_{{\mathcal{M}}}(s,a)],$
    |  |'
- en: where ${\pi}=\phi({\mathcal{I}}_{s}\sim\bm{{\mathcal{M}}}_{s},{\mathcal{I}}_{t}\sim{\mathcal{M}}_{t}):{\mathcal{S}}^{t}\to{\mathcal{A}}^{t}$
    is a policy learned for the target domain ${\mathcal{M}}_{t}$ based on information
    from both ${\mathcal{I}}_{t}$ and ${\mathcal{I}}_{s}$.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\pi}=\phi({\mathcal{I}}_{s}\sim\bm{{\mathcal{M}}}_{s},{\mathcal{I}}_{t}\sim{\mathcal{M}}_{t}):{\mathcal{S}}^{t}\to{\mathcal{A}}^{t}$
    是一个为目标领域 ${\mathcal{M}}_{t}$ 学习的策略，基于来自 ${\mathcal{I}}_{t}$ 和 ${\mathcal{I}}_{s}$
    的信息。
- en: In the above definition, we use $\phi({\mathcal{I}})$ to denote the learned
    policy based on information ${\mathcal{I}}$, which is usually approximated with
    deep neural networks in DRL. For the simplistic case, knowledge can transfer between
    two agents within the same domain, resulting in $|\bm{{\mathcal{M}}}_{s}|=1$,
    and ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$. One can consider regular RL without
    TL as a special case of the above definition, by treating ${\mathcal{I}}_{s}=\emptyset$,
    so that a policy $\pi$ is learned purely on the feedback provided by the target
    domain, $i.e.~{}{\pi}=\phi({\mathcal{I}}_{t})$.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述定义中，我们用 $\phi({\mathcal{I}})$ 表示基于信息 ${\mathcal{I}}$ 学到的策略，通常在深度强化学习（DRL）中用深度神经网络来逼近。对于简化情况，知识可以在同一领域内的两个代理之间迁移，结果是
    $|\bm{{\mathcal{M}}}_{s}|=1$，并且 ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$。可以将没有TL的常规RL视为上述定义的特例，通过将
    ${\mathcal{I}}_{s}=\emptyset$，使得策略 $\pi$ 仅基于目标领域提供的反馈进行学习，即 ${\pi}=\phi({\mathcal{I}}_{t})$。
- en: 2.4 Related Topics
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 相关主题
- en: In addition to TL, other efforts have been made to benefit RL by leveraging
    different forms of supervision. In this section, we briefly discuss other techniques
    that are relevant to TL by analyzing the differences and connections between transfer
    learning and these relevant techniques, which we hope can further clarify the
    scope of this survey.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 除了迁移学习（TL），还通过利用不同形式的监督进行了其他努力，以造福强化学习（RL）。在本节中，我们通过分析迁移学习与这些相关技术之间的差异和联系，简要讨论了与TL相关的其他技术，希望能够进一步明确本调查的范围。
- en: '*Continual Learning* is the ability of sequentially learning multiple tasks
    that are temporally or spatially related, without forgetting the previously acquired
    knowledge. Continual Learning is a specialized yet more challenging scenario of
    TL, in that the learned knowledge needs to be transferred along a sequence of
    dynamically-changing tasks that cannot be foreseen, rather than learning a fixed
    group of tasks. Hence, different from most TL methods discussed in this survey,
    the ability of *automatic task detection* and *avoiding catastrophic forgetting*
    is usually indispensable in continual learning [[43](#bib.bib43)].'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*持续学习* 是指连续学习多个在时间上或空间上相关的任务的能力，而不会遗忘之前获得的知识。持续学习是TL的一种特殊且更具挑战性的情况，因为所学知识需要在一系列动态变化的任务中进行迁移，这些任务是无法预见的，而不是学习一组固定的任务。因此，与本调查中讨论的大多数TL方法不同，*自动任务检测*
    和 *避免灾难性遗忘* 的能力在持续学习中通常是不可或缺的 [[43](#bib.bib43)]。'
- en: '*Hierarchical RL* has been proposed to resolve complex real-world tasks. Different
    from traditional RL, for hierarchical RL, the action space is grouped into different
    granularities to form higher-level macro actions. Accordingly, the learning task
    is also decomposed into hierarchically dependent sub-goals. Well-known hierarchical
    RL frameworks include *Feudal learning* [[44](#bib.bib44)], *Options framework*[[45](#bib.bib45)],
    *Hierarchical Abstract Machines* [[46](#bib.bib46)], and *MAXQ* [[47](#bib.bib47)].
    Given the higher-level abstraction on tasks, actions, and state spaces, hierarchical
    RL can facilitate knowledge transfer across similar domains.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*层次化强化学习* 被提出用以解决复杂的现实世界任务。与传统强化学习不同，对于层次化强化学习，动作空间被分组到不同的粒度以形成更高层次的宏动作。因此，学习任务也被分解为层次化依赖的子目标。知名的层次化强化学习框架包括
    *封建学习* [[44](#bib.bib44)]，*选项框架* [[45](#bib.bib45)]，*层次化抽象机器* [[46](#bib.bib46)]，以及
    *MAXQ* [[47](#bib.bib47)]。鉴于任务、动作和状态空间的更高层次抽象，层次化强化学习可以促进在类似领域之间的知识转移。'
- en: '*Multi-task RL* learns an agent with generalized skills across various tasks,
    hence it can solve MDPs randomly sampled from a fixed yet unknown distribution
    [[48](#bib.bib48)]. A larger concept of multi-task learning also incorporates
    multi-task supervised learning and unsupervised learning [[49](#bib.bib49)]. Multi-task
    learning is naturally related to TL, in that the learned skills, typically manifested
    as representations, need to be effectively shared among domains. Many TL techniques
    later discussed in this survey can be readily applied to solve multi-task RL scenarios,
    such as policy distillation [[50](#bib.bib50)], and representation sharing [[51](#bib.bib51)].
    One notable challenges in multi-task learning is negative transfer, which is induced
    by the irrelevance or conflicting property for learned tasks. Hence, some recent
    work in multi-task RL focused on a trade-off between sharing and individualizing
    function modules [[52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54)].'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*多任务强化学习* 通过跨各种任务学习具有泛化技能的智能体，从而能够解决从固定但未知分布中随机采样的MDP[[48](#bib.bib48)]。多任务学习的更大概念还包括多任务监督学习和无监督学习[[49](#bib.bib49)]。多任务学习自然与迁移学习相关，因为所学习的技能，通常表现为表示，需要在各领域之间有效共享。此调查中后续讨论的许多迁移学习技术可以很容易地应用于解决多任务强化学习场景，例如策略蒸馏[[50](#bib.bib50)]
    和表示共享[[51](#bib.bib51)]。多任务学习中的一个显著挑战是负迁移，这由所学习任务的无关或冲突特性引起。因此，最近一些多任务强化学习工作专注于在共享和个性化功能模块之间的权衡[[52](#bib.bib52),
    [53](#bib.bib53), [54](#bib.bib54)]。'
- en: '*Generalization in RL* refers to the ability of learning agents to adapt to
    *unseen* domains. Generalization is a crucial property for RL to achieve, especially
    when classical RL assumes identical training and inference MDPs, whereas the real
    world is constantly changing. Generalization in RL is considered more challenging
    than in supervised learning due to the non-stationarity of MDPs, where the latter
    has provided inspirations for the former [[55](#bib.bib55)]. *Meta-learning* is
    an effective direction towards generalization, which also draws close connections
    to TL. Some TL techniques discussed in this survey are actually designed for meta-RL.
    However, meta-learning is particularly focused on the learning methods that lead
    to *fast adaptation* to unseen domains, whereas TL is a broader concept and covers
    scenarios where the target environment can be (partially) observable. To tackle
    unseen tasks in RL, some meta-RL methods focused on training MDPs generation [[56](#bib.bib56)]
    and variations estimation [[57](#bib.bib57)]. We refer readers to [[58](#bib.bib58)]
    for a more focused survey on meta RL.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习中的泛化* 指的是学习智能体适应 *未见过* 领域的能力。泛化是强化学习必须实现的关键特性，尤其是当传统的强化学习假设训练和推理的马尔可夫决策过程（MDP）相同，而现实世界却在不断变化。由于MDP的非平稳性，强化学习中的泛化被认为比监督学习中的泛化更具挑战性，后者为前者提供了灵感[[55](#bib.bib55)]。*元学习*
    是一种有效的泛化方向，它与迁移学习有着密切的联系。此调查中讨论的一些迁移学习技术实际上是为元强化学习设计的。然而，元学习特别关注于能够 *快速适应* 未见领域的学习方法，而迁移学习是一个更广泛的概念，涵盖了目标环境可以是（部分）可观测的场景。为了解决强化学习中的未见任务，一些元强化学习方法专注于MDP生成[[56](#bib.bib56)]
    和变化估计[[57](#bib.bib57)]。有关元强化学习的更详细调查，请参见[[58](#bib.bib58)]。'
- en: 3 Analyzing Transfer Learning
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 分析迁移学习
- en: In this section, we discuss TL approaches in RL from different angles. We also
    use a prototype to illustrate the potential variants residing in knowledge transfer
    among domains, then summarize important metrics for TL evaluation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从不同角度讨论 RL 中的 TL 方法。我们还使用一个原型来说明领域间知识转移的潜在变体，然后总结 TL 评估的重要指标。
- en: 3.1 Categorization of Transfer Learning Approaches
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 转移学习方法的分类
- en: 'TL approaches can be organized by answering the following key questions:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: TL 方法可以通过回答以下关键问题来组织：
- en: '1.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '*What knowledge is transferred*: Knowledge from the source domain can take
    different forms, such as expert experiences [[59](#bib.bib59)], the action probability
    distribution of an expert policy [[60](#bib.bib60)], or even a potential function
    that estimates the quality of demonstrations in the target MDP [[61](#bib.bib61)].
    The divergence in representations and granularities of knowledge fundamentally
    influences how TL is performed. The quality of the transferred knowledge, e.g. whether
    it comes from an oracle  [[62](#bib.bib62)] or a suboptimal teacher [[63](#bib.bib63)]
    also affects the way TL methods are designed.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*转移了什么知识*：来源领域中的知识可以采取不同形式，例如专家经验 [[59](#bib.bib59)]、专家策略的动作概率分布 [[60](#bib.bib60)]，甚至是估计目标
    MDP 中演示质量的潜在函数 [[61](#bib.bib61)]。知识表示和粒度的差异从根本上影响 TL 的执行方式。转移知识的质量，例如是否来自神谕 [[62](#bib.bib62)]
    或次优教师 [[63](#bib.bib63)]，也影响 TL 方法的设计方式。'
- en: '2.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: '*What RL frameworks fit the TL approach:* We can rephrase this question into
    other forms, e.g., is the TL approach policy-agnostic, or only applicable to certain
    RL backbones, such as the Temporal Difference (TD) methods? Answers to this question
    are closely related to the representaion of knowledge. For example, transferring
    knowledge from expert demonstrations are usually policy-agnostic (see Section [5.2](#S5.SS2
    "5.2 Learning from Demonstrations ‣ 5 Transfer Learning Approaches Deep Dive ‣
    Transfer Learning in Deep Reinforcement Learning: A Survey")), while policy distillation,
    to be discussed in Section [5.3](#S5.SS3 "5.3 Policy Transfer ‣ 5 Transfer Learning
    Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey"),
    may not be suitable for DQN backbone which does not explicitly learn a policy
    function.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*哪些 RL 框架适用于 TL 方法*：我们可以将这个问题重新表述为其他形式，例如 TL 方法是否对策略无关，还是仅适用于某些 RL 框架，例如时序差分（TD）方法？对此问题的回答与知识的表示密切相关。例如，从专家演示中转移知识通常是对策略无关的（见第[5.2节](#S5.SS2
    "5.2 Learning from Demonstrations ‣ 5 Transfer Learning Approaches Deep Dive ‣
    Transfer Learning in Deep Reinforcement Learning: A Survey")），而政策蒸馏（将在第[5.3节](#S5.SS3
    "5.3 Policy Transfer ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer Learning
    in Deep Reinforcement Learning: A Survey")讨论）可能不适用于不显式学习策略函数的 DQN 框架。'
- en: '3.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: '*What is the difference between the source and the target domain*: Some TL
    approaches fit where the source domain ${\mathcal{M}}_{s}$ and the target domain
    ${\mathcal{M}}_{t}$ are equivalent, whereas others are designed to transfer knowledge
    between different domains. For example, in video gaming tasks where observations
    are RGB pixels, ${\mathcal{M}}_{s}$ and ${\mathcal{M}}_{t}$ may share the same
    action space (${\mathcal{A}}$) but differs in their observation spaces (${\mathcal{S}}$).
    For goal-conditioned RL [[64](#bib.bib64)], the two domains may differ only by
    the reward distribution: ${\mathcal{R}}_{s}\neq{\mathcal{R}}_{t}$.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*源领域和目标领域之间的区别是什么*：一些 TL 方法适用于源领域 ${\mathcal{M}}_{s}$ 和目标领域 ${\mathcal{M}}_{t}$
    等效的情况，而其他方法则设计用于在不同领域之间转移知识。例如，在视频游戏任务中，当观察是 RGB 像素时，${\mathcal{M}}_{s}$ 和 ${\mathcal{M}}_{t}$
    可能共享相同的动作空间 (${\mathcal{A}}$) 但在观察空间 (${\mathcal{S}}$) 上有所不同。对于目标条件 RL [[64](#bib.bib64)]，两个领域之间的区别可能仅在于奖励分布：${\mathcal{R}}_{s}\neq{\mathcal{R}}_{t}$。'
- en: '4.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: '*What information is available in the target domain:*  While knowledge from
    source domains is usually accessible, it can be prohibitive to sample from the
    target domain, or the reward signal can be sparse or delayed. Examples include
    adapting an auto-driving agent pre-trained in simulated platforms to real environments [[65](#bib.bib65)],
    The accessibility of information in the target domain can affect the way that
    TL approaches are designed.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*目标领域中可用的信息是什么*：虽然来源领域中的知识通常是可以访问的，但从目标领域采样可能会受到限制，或者奖励信号可能是稀疏或延迟的。例如，将在模拟平台上预训练的自动驾驶代理适应到真实环境[[65](#bib.bib65)]。目标领域中信息的可用性可以影响
    TL 方法的设计方式。'
- en: '5.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: '*How sample-efficient the TL approach is:* TL enables the RL with better initial
    performance, hence usually requires fewer interactions compared with learning
    from scratch. Based on the sampling cost, we can categorize TL approaches into
    the following classes: (i) *Zero-shot* transfer, which learns an agent that is
    directly applicable to the target domain without requiring any training interactions;
    (ii) *Few-shot* transfer, which only requires a few samples (interactions) from
    the target domain; (iii) *Sample-efficient* transfer, where an agent can benefit
    by TL to be more sample efficient compared to normal RL.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*迁移学习方法的样本效率如何：* 迁移学习使得强化学习具有更好的初始性能，因此通常需要比从头开始学习更少的交互。根据采样成本，我们可以将迁移学习方法分为以下几类：（i）*零样本*迁移，它学习一个可以直接应用于目标领域的智能体，而不需要任何训练交互；（ii）*少样本*迁移，它只需要从目标领域获得少量样本（交互）；（iii）*样本高效*迁移，其中智能体可以通过迁移学习比普通强化学习更高效地使用样本。'
- en: 3.2 Case Analysis of Transfer Learning in the context of Reinforcement Learning
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 强化学习背景下的迁移学习案例分析
- en: We now use HalfCheetah¹¹1https://gym.openai.com/envs/HalfCheetah-v2/ as a working
    example to illustrate how TL can occur between the source and the target domain.
    HalfCheetah is a standard DRL benchmark for solving physical locomotion tasks,
    in which the objective is to train a two-leg agent to run fast without losing
    control of itself.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在以HalfCheetah¹¹1https://gym.openai.com/envs/HalfCheetah-v2/作为工作示例，说明迁移学习如何在源领域和目标领域之间发生。HalfCheetah是一个标准的深度强化学习基准测试，旨在解决物理运动任务，其目标是训练一个双腿智能体以快速奔跑而不失去控制。
- en: '3.2.1 Potential Domain Differences:'
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 潜在的领域差异：
- en: 'During TL, the differences between the source and target domain may reside
    in any component of an MDP:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习过程中，源领域和目标领域之间的差异可能存在于MDP的任何组件中：
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '${\mathcal{S}}$ (State-space): domains can be made different by extending or
    constraining the available positions for the HalfCheetah agent to move.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ${\mathcal{S}}$（状态空间）：领域可以通过扩展或限制HalfCheetah智能体可以移动的可用位置来使其不同。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ${\mathcal{A}}$ (Action-space) can be adjusted by changing the range of available
    torques for the thigh, shin, or foot of the agent.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ${\mathcal{A}}$（动作空间）可以通过改变智能体大腿、胫骨或脚的可用扭矩范围进行调整。
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '${\mathcal{R}}$ (Reward function): a domain can be simplified by using only
    the distance moved forward as rewards or be perplexed by using the scale of accelerated
    velocity in each direction as extra penalty costs.'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ${\mathcal{R}}$（奖励函数）：一个领域可以通过仅使用前进距离作为奖励来简化，或者通过使用每个方向的加速速度的规模作为额外的惩罚成本来使其复杂化。
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '${\mathcal{T}}$ (Transition dynamics): two domains can differ by following
    different physical rules, leading to different transition probabilities given
    the same state-action pairs.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ${\mathcal{T}}$（转移动态）：两个领域可以通过遵循不同的物理规则而有所不同，从而在相同的状态-动作对下导致不同的转移概率。
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\mu_{0}$ (Initial states): the source and target domains may have different
    initial states, specifying where and with what posture the agent can start moving.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mu_{0}$（初始状态）：源领域和目标领域可能有不同的初始状态，指定智能体可以从何处以及以何种姿势开始移动。
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\tau$ (Trajectories): the source and target domains may allow a different
    number of steps for the agent to move before a task is done.'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\tau$（轨迹）：源领域和目标领域可能允许智能体在完成任务之前移动的步数不同。
- en: '3.2.2 Transferrable Knowledge:'
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 可迁移知识：
- en: 'Without losing generality, we list below some transferrable knowledge assuming
    that the source and target domains are variants of HalfCheetah:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 不失一般性，我们列出一些可迁移的知识，假设源领域和目标领域是HalfCheetah的变体：
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Demonstrated trajectories: the target agent can learn from the behavior of
    a pre-trained expert, e.g. a sequence of running demonstrations.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 演示轨迹：目标智能体可以从预训练专家的行为中学习，例如一系列的跑步演示。
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model dynamics: the RL agent may access a model of the physical dynamics for
    the source domain that is also partly applicable to the target domain. It can
    perform dynamic programming based on the physical rules, running fast without
    losing its control due to the accelerated velocity.'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型动态：强化学习智能体可以访问一个源领域的物理动态模型，该模型也部分适用于目标领域。它可以基于物理规则进行动态规划，由于加速的速度，可以快速奔跑而不失去控制。
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Teacher policies: an expert policy may be consulted by the learning agent,
    which outputs the probability of taking different actions upon a given state example.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 教师策略：学习智能体可以参考专家策略，该策略输出在给定状态下采取不同动作的概率。
- en: •
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Teacher value functions: besides teacher policy, the learning agent may also
    refer to the value function derived by a teacher policy, which implies the quality
    of state-actions from the teacher’s point of view.'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 教师价值函数：除了教师策略，学习代理还可能参考由教师策略推导出的价值函数，这表示从教师角度看状态-动作对的质量。
- en: 3.3 Evaluation metrics
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 评估指标
- en: 'In this section, we present some representative metrics for evaluating TL approaches,
    which have also been partly summarized in prior work [[11](#bib.bib11), [66](#bib.bib66)]:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提出了一些用于评估转移学习方法的代表性指标，这些指标在之前的工作中也有所总结 [[11](#bib.bib11), [66](#bib.bib66)]：
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Jumpstart performance( jp)*: the initial performance (returns) of the agent.'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*起步性能 (jp)*：代理的初始性能（回报）。'
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Asymptotic performance (ap)*: the ultimate performance (returns) of the agent.'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*渐近性能 (ap)*：代理的最终性能（回报）。'
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Accumulated rewards (ar)*: the area under the learning curve of the agent.'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*累计奖励 (ar)*：代理学习曲线下的面积。'
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Transfer ratio (tr)*: the ratio between asymptotic performance of the agent
    with TL and asymptotic performance of the agent without TL.'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*转移比例 (tr)*：具有转移学习（TL）和没有转移学习的代理渐近性能之间的比例。'
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Time to threshold (tt)*: the learning time (iterations) needed for the target
    agent to reach certain performance threshold.'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*达到阈值的时间 (tt)*：目标代理达到某个性能阈值所需的学习时间（迭代次数）。'
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Performance with fixed training epochs (pe)*: the performance achieved by
    the target agent after a specific number of training iterations.'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*固定训练周期的性能 (pe)*：目标代理在特定训练迭代次数后所达到的性能。'
- en: •
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Performance sensitivity (ps)*: the variance in returns using different hyper-parameter
    settings.'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*性能敏感度 (ps)*：使用不同超参数设置下的回报方差。'
- en: 'The above criteria mainly focus on the *learning process* of the target agent.
    In addition, we introduce the following metrics from the perspective of *transferred
    knowledge*, which, although commensurately important for evaluation, have not
    been explicitly discussed by prior art:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 上述标准主要关注目标代理的*学习过程*。此外，我们还从*转移知识*的角度引入了以下指标，尽管这些指标在评估中同样重要，但在现有文献中并未被明确讨论：
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Necessary knowledge amount (nka)*: the necessary *amount* of the knowledge
    required for TL in order to achieve certain performance thresholds. Examples along
    this line include the number of designed source tasks [[67](#bib.bib67)], the
    number of expert policies, or the number of demonstrated interactions [[68](#bib.bib68)]
    required to enable knowledge transfer.'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*必要知识量 (nka)*：为了在转移学习中实现特定性能阈值所需的*知识量*。这一方面的例子包括设计的源任务数量 [[67](#bib.bib67)]，专家策略的数量，或实现知识转移所需的示范交互次数
    [[68](#bib.bib68)]。'
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Necessary knowledge quality (nkq)*: the guaranteed *quality* of the knowledge
    required to enable effective TL. This metric helps in answering questions such
    as (i) Does the TL approach rely on near-oracle knowledge, such as expert demonstrations/policies
    [[69](#bib.bib69)], or (ii) is the TL technique feasible even given suboptimal
    knowledge [[63](#bib.bib63)]?'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*必要知识质量 (nkq)*：保证能够实现有效转移学习的知识的*质量*。这一指标有助于回答诸如 (i) 转移学习方法是否依赖于近乎完美的知识，例如专家演示/策略
    [[69](#bib.bib69)]，或者 (ii) 即使在次优知识的情况下，转移学习技术是否仍然可行 [[63](#bib.bib63)] 等问题。'
- en: TL approaches differ in various perspectives, including the forms of transferred
    knowledge, the RL frameworks utilized to enable such transfer, and the gaps between
    the source and the target domain. It maybe biased to evaluate TL from just one
    viewpoint. We believe that explicating these TL related metrics helps in designing
    more generalizable and efficient TL approaches.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习方法在多个方面存在差异，包括转移知识的形式、用于实现这种转移的强化学习框架以及源领域与目标领域之间的差距。从单一视角评估转移学习可能存在偏差。我们认为，阐明这些转移学习相关指标有助于设计更具普遍性和高效性的转移学习方法。
- en: 'In general, most of the abovementioned metrics can be considered as evaluating
    two abilities of a TL approach: the *mastery* and *generalization*. *Mastery*
    refers to how well the learned agent can ultimately perform in the target domain,
    while *generalization* refers to the ability of the learning agent to quickly
    adapt to the target domain.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，上述大多数指标可以看作是评估转移学习方法的两个能力：*掌握* 和 *泛化*。*掌握* 指的是学习代理在目标领域中的最终表现如何，而 *泛化*
    指的是学习代理快速适应目标领域的能力。
- en: 4 Related Work
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: There are prior efforts in summarizing TL research in RL. One of the earliest
    literatures is [[11](#bib.bib11)] . Their main categorization is from the perspective
    of *problem setting*, in which the TL scenarios may vary in the number of domains
    involved, and the difference of state-action space among domains. Similar categorization
    is adopted by [[12](#bib.bib12)], with more refined analysis dimensions including
    the objective of TL. As pioneer surveys for TL in RL, neither [[11](#bib.bib11)]
    nor [[12](#bib.bib12)] covered recent research over the last decade. For instance,
    [[11](#bib.bib11)] emphasized on different *task-mapping* methods, which are more
    suitable for domains with tabular or mild state-action space dimensions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结 RL 中的迁移学习研究方面已有先前的努力。其中最早的文献之一是[[11](#bib.bib11)]。他们的主要分类是从*问题设置*的角度出发，其中迁移学习场景可能在涉及的领域数量和领域之间的状态-动作空间差异上有所不同。类似的分类由[[12](#bib.bib12)]
    采用，并增加了包括迁移学习目标在内的更精细的分析维度。作为 RL 中迁移学习的开创性综述，[[11](#bib.bib11)] 和[[12](#bib.bib12)]
    都未涵盖过去十年的最新研究。例如，[[11](#bib.bib11)] 强调了不同的*任务映射*方法，这些方法更适用于具有表格或轻度状态-动作空间维度的领域。
- en: 'There are other surveys focused on specific subtopics that interplay between
    RL and TL. For instance, [[70](#bib.bib70)] consolidated sim-to-real TL methods.
    They explored work that is more tailored for *robotics* domains, including domain
    generalization and zero-shot transfer, which is a favored application field of
    DRL as we discussed in Sec [6](#S6 "6 Applications ‣ Transfer Learning in Deep
    Reinforcement Learning: A Survey"). [[71](#bib.bib71)] conducted extensive database
    search and summarized benchmarks for evaluating TL algorithms in RL. [[72](#bib.bib72)]
    surveyed recent progress in multi-task RL. They partially shared research focus
    with us by studying certain TL oriented solutions towards multi-task RL, such
    as learning shared representations, pathNets, etc. We surveyed TL for RL with
    a broader spectrum in methodologies, applications, evaluations, which naturally
    draws connections to the above literatures.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '其他综述文章专注于 RL 和迁移学习之间相互作用的特定子主题。例如，[[70](#bib.bib70)] 汇总了从模拟到真实的迁移学习方法。他们探索了更适合*机器人*领域的工作，包括领域泛化和零样本迁移，这也是我们在第[6](#S6
    "6 Applications ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")节中讨论的
    DRL 的一个热门应用领域。[[71](#bib.bib71)] 进行了广泛的数据库搜索，并总结了评估 RL 中迁移学习算法的基准。[[72](#bib.bib72)]
    调查了多任务 RL 的最新进展。他们通过研究某些面向多任务 RL 的迁移学习解决方案（如学习共享表示、路径网络等），与我们部分共享了研究重点。我们对 RL
    的迁移学习进行了更广泛的方法、应用和评估的调查，自然与上述文献相关联。'
- en: '![Refer to caption](img/d89797408668410787a1d1bc51767ba8.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d89797408668410787a1d1bc51767ba8.png)'
- en: 'Figure 1: An overview of different TL approaches, organized by the format of
    transferred knowledge.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：不同迁移学习方法的概览，按转移知识的格式组织。
- en: 5 Transfer Learning Approaches Deep Dive
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 种迁移学习方法的深入探讨
- en: 'In this section, we elaborate on various TL approaches and organize them into
    different sub-topics, mostly by answering the question of “what knowledge is transferred”.
    For each type of TL approach, we analyze them by following the other criteria
    mentioned in Section [3](#S3 "3 Analyzing Transfer Learning ‣ Transfer Learning
    in Deep Reinforcement Learning: A Survey") and and summarize the key evaluation
    metrics that are applicable to the discussed work. Figure [1](#S4.F1 "Figure 1
    ‣ 4 Related Work ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")
    presents an overview of different TL approaches discussed in this survey.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们详细探讨了各种迁移学习方法，并将其组织成不同的子主题，主要通过回答“转移了什么知识”的问题。对于每种迁移学习方法，我们按照第[3](#S3
    "3 Analyzing Transfer Learning ‣ Transfer Learning in Deep Reinforcement Learning:
    A Survey")节中提到的其他标准进行分析，并总结了适用于讨论工作的关键评估指标。图[1](#S4.F1 "Figure 1 ‣ 4 Related Work
    ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")展示了本次调查中讨论的不同迁移学习方法的概览。'
- en: 5.1 Reward Shaping
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 奖励塑造
- en: 'We start by introducing the Reward Shaping approach, as it is applicable to
    most RL backbones and also largely overlaps with the other TL approaches discussed
    later. Reward Shaping (RS) is a technique that leverages the exterior knowledge
    to reconstruct the reward distribution of the target domain to guide the agent’s
    policy learning. More specifically, in addition to the environment reward signals,
    RS learns a reward-shaping function ${\mathcal{F}}:{\mathcal{S}}\times{\mathcal{S}}\times{\mathcal{A}}\to\mathbb{R}$
    to render auxiliary rewards, provided that the additional rewards contain external
    knowledge to guide the agent for better action selections. Intuitively, an RS
    strategy will assign higher rewards to more beneficial state-actions to navigate
    the agent to desired trajectories. As a result, the agent will learn its policy
    using the newly shaped rewards ${\mathcal{R}}^{\prime}$: ${\mathcal{R}}^{\prime}={\mathcal{R}}+{\mathcal{F}}$,
    which means that RS has altered the target domain with a different reward function:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍奖励塑造方法，因为它适用于大多数 RL 主干，并且与后面讨论的其他 TL 方法有很大重叠。奖励塑造 (RS) 是一种利用外部知识重建目标领域奖励分布以指导智能体策略学习的技术。更具体地说，除了环境奖励信号外，RS
    还学习一个奖励塑造函数 ${\mathcal{F}}:{\mathcal{S}}\times{\mathcal{S}}\times{\mathcal{A}}\to\mathbb{R}$，以提供辅助奖励，前提是额外奖励包含外部知识以指导智能体进行更好的动作选择。直观地说，RS
    策略将对更有益的状态-动作分配更高的奖励，以引导智能体达到期望的轨迹。因此，智能体将使用新塑造的奖励 ${\mathcal{R}}^{\prime}$ 学习其策略：${\mathcal{R}}^{\prime}={\mathcal{R}}+{\mathcal{F}}$，这意味着
    RS 已使用不同的奖励函数改变了目标领域：
- en: '|  | $\displaystyle{\mathcal{M}}=({\mathcal{S}},{\mathcal{A}},{\mathcal{T}},\gamma,{\mathcal{R}}))\to{\mathcal{M}}^{\prime}=({\mathcal{S}},{\mathcal{A}},{\mathcal{T}},\gamma,{\mathcal{R}}^{\prime}).\vspace{-0.1in}$
    |  | (1) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathcal{M}}=({\mathcal{S}},{\mathcal{A}},{\mathcal{T}},\gamma,{\mathcal{R}}))\to{\mathcal{M}}^{\prime}=({\mathcal{S}},{\mathcal{A}},{\mathcal{T}},\gamma,{\mathcal{R}}^{\prime}).\vspace{-0.1in}$
    |  | (1) |'
- en: 'Along the line of RS, *Potential based Reward Shaping (PBRS)* is one of the
    most classical approaches. [[61](#bib.bib61)] proposed PBRS to form a shaping
    function $F$ as the difference between two potential functions ($\Phi(\cdot)$):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RS 的领域中，*基于潜在的奖励塑造 (PBRS)* 是最经典的方法之一。 [[61](#bib.bib61)] 提出了 PBRS，形成了一个塑造函数
    $F$，作为两个潜在函数 ($\Phi(\cdot)$) 之间的差异：
- en: '|  | $\displaystyle F(s,a,s^{\prime})=\gamma\Phi(s^{\prime})-\Phi(s),\vspace{-.05in}$
    |  | (2) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle F(s,a,s^{\prime})=\gamma\Phi(s^{\prime})-\Phi(s),\vspace{-.05in}$
    |  | (2) |'
- en: 'where the potential function $\Phi(\cdot)$ comes from the knowledge of expertise
    and evaluates the quality of a given state. It has been proved that, without further
    restrictions on the underlying MDP or the shaping function $F$, PBRS is sufficient
    and necessary to preserve the policy invariance. Moreover, the optimal $Q$-function
    in the original and transformed MDP are related by the potential function: $Q_{{\mathcal{M}}^{\prime}}^{*}(s,a)=Q_{{\mathcal{M}}}^{*}(s,a)-\Phi(s)$,
    which draws a connection between potential based reward-shaping and advantage-based
    learning approaches [[73](#bib.bib73)].'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在函数 $\Phi(\cdot)$ 来自专业知识，并评估给定状态的质量。已经证明，在不对基础 MDP 或塑造函数 $F$ 施加进一步限制的情况下，PBRS
    既是保持策略不变性的充分条件，也是必要条件。此外，原始和转换后的 MDP 中的最优 $Q$-函数通过潜在函数相关联：$Q_{{\mathcal{M}}^{\prime}}^{*}(s,a)=Q_{{\mathcal{M}}}^{*}(s,a)-\Phi(s)$，这建立了基于潜在的奖励塑造和基于优势的学习方法之间的联系
    [[73](#bib.bib73)]。
- en: 'The idea of *PBRS* was extended to [[74](#bib.bib74)], which formulated the
    potential as a function over both the state and the action spaces. This approach
    is called *Potential Based state-action Advice (PBA)*. The potential function
    $\Phi(s,a)$ therefore evaluates how beneficial an action $a$ is to take from state
    $s$:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*PBRS* 的理念扩展到 [[74](#bib.bib74)]，其中将潜在定义为对状态和动作空间的函数。这种方法被称为 *基于潜在的状态-动作建议
    (PBA)*。因此，潜在函数 $\Phi(s,a)$ 评估从状态 $s$ 采取行动 $a$ 的益处：'
- en: '|  | $\displaystyle F(s,a,s^{\prime},a^{\prime})=\gamma\Phi(s^{\prime},a^{\prime})-\Phi(s,a).$
    |  | (3) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle F(s,a,s^{\prime},a^{\prime})=\gamma\Phi(s^{\prime},a^{\prime})-\Phi(s,a).$
    |  | (3) |'
- en: 'PBA requires on-policy learning and can be sample-costly, as in Equation ([3](#S5.E3
    "In 5.1 Reward Shaping ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer Learning
    in Deep Reinforcement Learning: A Survey")), $a^{\prime}$ is the action to take
    upon state $s$ is transitioning to $s^{\prime}$ by following the learning policy.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 'PBA 需要在策略学习上，并且可能样本成本较高，如方程 ([3](#S5.E3 "In 5.1 Reward Shaping ‣ 5 Transfer
    Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning:
    A Survey")) 所示，$a^{\prime}$ 是在状态 $s$ 转移到 $s^{\prime}$ 时，根据学习策略采取的行动。'
- en: 'Traditional RS approaches assumed a static potential function, until [[75](#bib.bib75)]
    proposed a *Dynamic Potential Based (DPB)* approach which makes the potential
    a function of both states and time: $F(s,t,s^{\prime},t^{\prime})=\gamma\Phi(s^{\prime},t^{\prime})-\Phi(s,t).$They
    proved that this dynamic approach can still maintain policy invariance: $Q^{*}_{{\mathcal{M}}^{\prime}}(s,a)=Q^{*}_{\mathcal{M}}(s,a)-\Phi(s,t),$where
    $t$ is the current tilmestep. [[76](#bib.bib76)] later introduced a way to incorporate
    any prior knowledge into a dynamic potential function structure, which is called
    *Dynamic Value Function Advice (DPBA)*. The rationale behind DPBA is that, given
    any extra reward function $R^{+}$ from prior knowledge, in order to add this extra
    reward to the original reward function, the potential function should satisfy:
    $\gamma\Phi(s^{\prime},a^{\prime})-\Phi(s,a)=F(s,a)=R^{+}(s,a).$'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的RS方法假设了一个静态的潜在函数，直到[[75](#bib.bib75)]提出了一种*基于动态潜在函数（DPB）*的方法，使得潜在函数既依赖于状态也依赖于时间：$F(s,t,s^{\prime},t^{\prime})=\gamma\Phi(s^{\prime},t^{\prime})-\Phi(s,t).$他们证明了这种动态方法仍然可以保持策略的不变性：$Q^{*}_{{\mathcal{M}}^{\prime}}(s,a)=Q^{*}_{\mathcal{M}}(s,a)-\Phi(s,t),$其中$t$是当前时间步。[[76](#bib.bib76)]后来介绍了一种将任何先验知识纳入动态潜在函数结构的方法，这被称为*动态价值函数建议（DPBA）*。DPBA的原理是，给定来自先验知识的任何额外奖励函数$R^{+}$，为了将这个额外奖励添加到原始奖励函数中，潜在函数应满足：$\gamma\Phi(s^{\prime},a^{\prime})-\Phi(s,a)=F(s,a)=R^{+}(s,a).$
- en: 'If $\Phi$ is not static but learned as an extra state-action Value function
    overtime, then the Bellman equation for $\Phi$ is : $\Phi^{\pi}(s,a)=r^{\Phi}(s,a)+\gamma\Phi(s^{\prime},a^{\prime}).$
    The shaping rewards $F(s,a)$ is therefore the negation of $r^{\Phi}(s,a)$ :'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果$\Phi$不是静态的，而是作为额外的状态-动作价值函数随着时间学习的，那么$\Phi$的贝尔曼方程是：$\Phi^{\pi}(s,a)=r^{\Phi}(s,a)+\gamma\Phi(s^{\prime},a^{\prime}).$
    因此，塑造奖励$F(s,a)$是$r^{\Phi}(s,a)$的负值：
- en: '|  | $\displaystyle F(s,a)=\gamma\Phi(s^{\prime},a^{\prime})-{\Phi}(s,a)=-r^{\Phi}(s,a).$
    |  | (4) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle F(s,a)=\gamma\Phi(s^{\prime},a^{\prime})-{\Phi}(s,a)=-r^{\Phi}(s,a).$
    |  | (4) |'
- en: 'This leads to the approach of using the negation of $R^{+}$ as the immediate
    reward to train an extra state-action Value function $\Phi$ and the policy simultaneously.
    Accordingly, the dynamic potential function $F$ becomes:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了使用$R^{+}$的负值作为即时奖励来同时训练额外的状态-动作价值函数$\Phi$和策略的方法。因此，动态潜在函数$F$变为：
- en: '|  | $\displaystyle F_{t}(s,a)=\gamma\Phi_{t+1}(s^{\prime},a^{\prime})-{\Phi_{t}}(s,a).\vspace{-.05in}$
    |  | (5) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle F_{t}(s,a)=\gamma\Phi_{t+1}(s^{\prime},a^{\prime})-{\Phi_{t}}(s,a).\vspace{-.05in}$
    |  | (5) |'
- en: The advantage of DPBA is that it provides a framework to allow arbitrary knowledge
    to be shaped as auxiliary rewards.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: DPBA的优势在于它提供了一个框架，使得任意知识可以被塑造为辅助奖励。
- en: 'Research along this line mainly focus on designing different shaping functions
    $F(s,a)$, while not much work has tackled the question of what knowledge can be
    used to derive this potential function. One work by [[77](#bib.bib77)] proposed
    to use RS to transfer an expert policy from the source domain ${\mathcal{M}}_{s}$
    to the target domain ${\mathcal{M}}_{t}$. This approach assumed the existence
    of two mapping functions $M_{S}$ and $M_{A}$ that can transform the state and
    action from the source to the target domain. Another work used demonstrated state-action
    samples from an expert policy to shape rewards [[78](#bib.bib78)]. Learning the
    augmented reward involves learning a discriminator to distinguish samples generated
    by an expert policy from samples generated by the target policy. The loss of the
    discriminator is applied to shape rewards to incentivize the learning agent to
    mimic the expert behavior. This work combines two TL approaches: RS and Learning
    from Demonstrations, the latter of which will be elaborated in Section [5.2](#S5.SS2
    "5.2 Learning from Demonstrations ‣ 5 Transfer Learning Approaches Deep Dive ‣
    Transfer Learning in Deep Reinforcement Learning: A Survey").'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '这方面的研究主要集中在设计不同的塑造函数$F(s,a)$，而没有太多工作解决可以用什么知识来推导这个潜在函数的问题。[[77](#bib.bib77)]的一项工作提议使用RS将专家策略从源领域${\mathcal{M}}_{s}$转移到目标领域${\mathcal{M}}_{t}$。这种方法假设存在两个映射函数$M_{S}$和$M_{A}$，可以将源领域的状态和动作转换到目标领域。另一项工作使用了来自专家策略的示例状态-动作样本来塑造奖励[[78](#bib.bib78)]。学习增强奖励涉及学习一个鉴别器，以区分由专家策略生成的样本和由目标策略生成的样本。鉴别器的损失用于塑造奖励，以激励学习代理模仿专家行为。这项工作结合了两种TL方法：RS和从示例中学习，后者将在第[5.2](#S5.SS2
    "5.2 Learning from Demonstrations ‣ 5 Transfer Learning Approaches Deep Dive ‣
    Transfer Learning in Deep Reinforcement Learning: A Survey")节中详细阐述。'
- en: 'The above-mentioned RS approaches are summarized in Table [I](#S5.T1 "TABLE
    I ‣ 5.1 Reward Shaping ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer Learning
    in Deep Reinforcement Learning: A Survey"). They follow the potential based RS
    principle that has been developed systematically: from the classical *PBRS* which
    is built on a *static* potential shaping function of *states*, to *PBA* which
    generates the potential as a function of both *states* and *actions*, and *DPB*
    which learns a dynamic potential function of *states* and *time*, to the most
    recent *DPBA*, which involves a dynamic potential function of *states* and *actions*
    to be learned as an extra state-action Value function in parallel with the environment
    Value function. As an effective TL paradigm, RS has been widely applied to fields
    including robot training [[79](#bib.bib79)], spoken dialogue systems [[80](#bib.bib80)],
    and question answering [[81](#bib.bib81)]. It provides a feasible framework for
    transferring knowledge as the augmented reward and is generally applicable to
    various RL algorithms. RS has also been applied to multi-agent RL [[82](#bib.bib82)]
    and model-based RL [[83](#bib.bib83)]. Principled integration of RS with other
    TL approaches, such as *Learning from demonstrations* (Section [5.2](#S5.SS2 "5.2
    Learning from Demonstrations ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer
    Learning in Deep Reinforcement Learning: A Survey")) and *Policy Transfer* (Section
    [5.3](#S5.SS3 "5.3 Policy Transfer ‣ 5 Transfer Learning Approaches Deep Dive
    ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")) will be an intriguing
    question for ongoing research.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '上述 RS 方法总结在表 [I](#S5.T1 "TABLE I ‣ 5.1 Reward Shaping ‣ 5 Transfer Learning
    Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")
    中。它们遵循潜在基于 RS 原则，该原则已经系统性地发展：从基于 *静态* 潜在塑造函数的经典 *PBRS*，到生成潜在作为 *状态* 和 *动作* 的函数的
    *PBA*，再到学习 *状态* 和 *时间* 的动态潜在函数的 *DPB*，以及最新的 *DPBA*，它涉及到作为额外的状态-动作值函数与环境值函数并行学习的动态潜在函数。作为一种有效的
    TL 范式，RS 已广泛应用于包括机器人训练 [[79](#bib.bib79)]、语音对话系统 [[80](#bib.bib80)] 和问答 [[81](#bib.bib81)]
    等领域。它提供了一个将知识转移为增强奖励的可行框架，并且通常适用于各种 RL 算法。RS 也被应用于多智能体 RL [[82](#bib.bib82)] 和基于模型的
    RL [[83](#bib.bib83)]。RS 与其他 TL 方法的原则性整合，如 *从示范中学习*（第 [5.2](#S5.SS2 "5.2 Learning
    from Demonstrations ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer Learning
    in Deep Reinforcement Learning: A Survey") 节）和 *策略转移*（第 [5.3](#S5.SS3 "5.3 Policy
    Transfer ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer Learning in Deep
    Reinforcement Learning: A Survey") 节），将是持续研究中的一个有趣问题。'
- en: Note that RS approaches discussed so far are built upon a consensus that the
    source information for shaping the reward comes *externally*, which coincides
    with the notion of knowledge transfer. Some RS work also tackles the scenario
    where the shaped reward comes *intrinsically*. For instance, *Belief Reward Shaping*
    was proposed by [[84](#bib.bib84)], which utilizes a Bayesian reward shaping framework
    to generate the potential value that decays with experience, where the potential
    value comes from the critic itself.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，到目前为止讨论的 RS 方法建立在一个共识上，即塑造奖励的源信息来自 *外部*，这与知识转移的概念一致。一些 RS 工作也处理了塑造奖励来自 *内部*
    的情况。例如，*信念奖励塑造* 是 [[84](#bib.bib84)] 提出的，它利用贝叶斯奖励塑造框架生成随经验衰减的潜在值，其中潜在值来自于评论者自身。
- en: '| Methods | MDP difference | Format of shaping reward | Knowledge source |
    Evaluation metrics |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | MDP 差异 | 奖励塑造的格式 | 知识来源 | 评估指标 |'
- en: '| PBRS | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F=\gamma\Phi(s^{\prime})-\Phi(s)$
    | ✗ | *ap, ar* |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| PBRS | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F=\gamma\Phi(s^{\prime})-\Phi(s)$
    | ✗ | *ap, ar* |'
- en: '| PBA | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F=\gamma\Phi(s^{\prime},a^{\prime})-\Phi(s,a)$
    | ✗ | *ap, ar* |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| PBA | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F=\gamma\Phi(s^{\prime},a^{\prime})-\Phi(s,a)$
    | ✗ | *ap, ar* |'
- en: '| DPB | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F=\gamma\Phi(s^{\prime},t^{\prime})-\Phi(s,t)$
    | ✗ | *ap, ar* |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| DPB | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F=\gamma\Phi(s^{\prime},t^{\prime})-\Phi(s,t)$
    | ✗ | *ap, ar* |'
- en: '| DPBA | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F_{t}=\gamma\Phi_{t+1}(s^{\prime},a^{\prime})-{\Phi_{t}}(s,a)$
    , $\Phi$ learned as an extra Q function | ✗ | *ap, ar* |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| DPBA | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F_{t}=\gamma\Phi_{t+1}(s^{\prime},a^{\prime})-{\Phi_{t}}(s,a)$，$\Phi$
    作为额外的 Q 函数进行学习 | ✗ | *ap, ar* |'
- en: '| [[77](#bib.bib77)] | ${\mathcal{S}}_{s}\neq{\mathcal{S}}_{t}$, ${\mathcal{A}}_{s}\neq{\mathcal{A}}_{t}$
    | $F_{t}=\gamma\Phi_{t+1}(s^{\prime},a^{\prime})-{\Phi_{t}}(s,a)$ | $\pi_{s}$
    | *ap, ar* |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| [[77](#bib.bib77)] | ${\mathcal{S}}_{s}\neq{\mathcal{S}}_{t}$，${\mathcal{A}}_{s}\neq{\mathcal{A}}_{t}$
    | $F_{t}=\gamma\Phi_{t+1}(s^{\prime},a^{\prime})-{\Phi_{t}}(s,a)$ | $\pi_{s}$
    | *ap, ar* |'
- en: '| [[78](#bib.bib78)] | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F_{t}=\gamma\Phi_{t+1}(s^{\prime},a^{\prime})-{\Phi_{t}}(s,a)$
    | $D_{E}$ | *ap, ar* |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| [[78](#bib.bib78)] | ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$ | $F_{t}=\gamma\Phi_{t+1}(s^{\prime},a^{\prime})-{\Phi_{t}}(s,a)$
    | $D_{E}$ | *ap, ar* |'
- en: 'TABLE I: A comparison of reward shaping approaches. ✗ denotes that the information
    is not revealed in the paper.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：奖励塑形方法的比较。✗ 表示信息在论文中未公开。
- en: 5.2 Learning from Demonstrations
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 从演示中学习
- en: '*Learning from Demonstrations (LfD)* is a technique to assist RL by utilizing
    external demonstrations for more efficient exploration. The demonstrations may
    come from different sources with varying qualities. Research along this line usually
    address a scenario where the source and the target MDPs are the same: ${\mathcal{M}}_{s}={\mathcal{M}}_{t}$,
    although there has been work that learns from demonstrations generated in a different
    domain [[85](#bib.bib85), [86](#bib.bib86)].'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*从演示中学习（LfD）* 是一种通过利用外部演示来辅助 RL 的技术，以实现更高效的探索。这些演示可能来自不同来源且质量各异。沿此方向的研究通常处理源
    MDP 和目标 MDP 相同的场景：${\mathcal{M}}_{s}={\mathcal{M}}_{t}$，尽管也有工作从在不同领域生成的演示中学习 [[85](#bib.bib85),
    [86](#bib.bib86)]。'
- en: Depending on *when* the demonstrations are used for knowledge transfer, approaches
    can be organized into *offline* and *online* methods. For *offline* approaches,
    demonstrations are either used for pre-training RL components, or for offline
    RL [[87](#bib.bib87), [88](#bib.bib88)]. When leveraging demonstrations for pre-training,
    RL components such as the value function $V(s)$ [[89](#bib.bib89)], the policy
    $\pi$ [[90](#bib.bib90)], or the model of transition dynamics [[91](#bib.bib91)],
    can be initialized by learning from demonstrations. For the *online* approach,
    demonstrations are directly used to guide agent actions for efficient explorations [[92](#bib.bib92)].
    Most work discussed in this section follows the online transfer paradigm or combines
    offline pre-training with online RL [[93](#bib.bib93)].
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 根据*何时*使用演示来进行知识传递，可以将方法组织为*离线*和*在线*方法。对于*离线*方法，演示要么用于预训练RL组件，要么用于离线RL [[87](#bib.bib87),
    [88](#bib.bib88)]。在利用演示进行预训练时，RL组件如价值函数 $V(s)$ [[89](#bib.bib89)]、策略 $\pi$ [[90](#bib.bib90)]
    或转移动态模型 [[91](#bib.bib91)] 可以通过从演示中学习来初始化。对于*在线*方法，演示被直接用于指导代理的动作，以便进行有效的探索 [[92](#bib.bib92)]。本节讨论的大多数工作遵循在线转移范式或将离线预训练与在线RL相结合
    [[93](#bib.bib93)]。
- en: 'Work along this line can also be categorized depending on *what* RL frameworks
    are compatible: some adopts the policy-iteration framework [[94](#bib.bib94),
    [59](#bib.bib59), [95](#bib.bib95)], some follow a $Q$-learning framework [[92](#bib.bib92),
    [96](#bib.bib96)], while recent work usually follows the policy-gradient framework [[78](#bib.bib78),
    [93](#bib.bib93), [97](#bib.bib97), [63](#bib.bib63)]. Demonstrations have been
    leveraged in the *policy iterations* framework by [[98](#bib.bib98)]. Later, [[94](#bib.bib94)]
    introduced the *Direct Policy Iteration with Demonstrations (DPID)* algorithm.
    This approach samples complete demonstrated rollouts $D_{E}$ from an expert policy
    $\pi_{E}$, in combination with the self-generated rollouts $D_{\pi}$ gathered
    from the learning agent. $D_{\pi}\cup D_{E}$ are used to learn a Monte-Carlo estimation
    of the Q-value: $\hat{Q}$, from which a learning policy can be derived greedily:
    $\pi(s)=\underset{a\in{\mathcal{A}}}{\arg\max}\hat{Q}(s,a)$. This policy $\pi$
    is further regularized by a loss function ${\mathcal{L}}(s,\pi_{E})$ to minimize
    its discrepancy from the expert policy decision.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 沿此方向的工作还可以根据*什么* RL 框架兼容进行分类：一些采用策略迭代框架 [[94](#bib.bib94), [59](#bib.bib59),
    [95](#bib.bib95)]，一些遵循 $Q$-学习框架 [[92](#bib.bib92), [96](#bib.bib96)]，而最近的工作通常遵循策略梯度框架
    [[78](#bib.bib78), [93](#bib.bib93), [97](#bib.bib97), [63](#bib.bib63)]。演示已被应用于*策略迭代*框架
    [[98](#bib.bib98)]。随后，[[94](#bib.bib94)] 引入了*带演示的直接策略迭代（DPID）*算法。该方法从专家策略 $\pi_{E}$
    中采样完整的演示回合 $D_{E}$，并结合从学习代理中获得的自生成回合 $D_{\pi}$。$D_{\pi}\cup D_{E}$ 被用于学习 Q 值的蒙特卡罗估计：$\hat{Q}$，从中可以贪婪地推导出学习策略：$\pi(s)=\underset{a\in{\mathcal{A}}}{\arg\max}\hat{Q}(s,a)$。该策略
    $\pi$ 通过损失函数 ${\mathcal{L}}(s,\pi_{E})$ 进一步正则化，以最小化与专家策略决策的差异。
- en: 'Another example is the *Approximate Policy Iteration with Demonstration (APID)*
    algorithm, which was proposed by [[59](#bib.bib59)] and extended by [[95](#bib.bib95)].
    Different from *DPID* where both $D_{E}$ and $D_{\pi}$ are used for value estimation,
    the *APID* algorithm solely applies $D_{\pi}$ to approximate on the Q function.
    Expert demonstrations $D_{E}$ are used to learn the value function, which, given
    any state $s_{i}$, renders expert actions $\pi_{E}(s_{i})$ with higher $Q$-value
    margins compared with other actions that are not shown in $D_{E}$:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是 *近似策略迭代与演示 (APID)* 算法，该算法由 [[59](#bib.bib59)] 提出，并由 [[95](#bib.bib95)]
    扩展。与 *DPID* 不同的是，*APID* 算法仅应用 $D_{\pi}$ 来对 Q 函数进行近似。专家演示 $D_{E}$ 用于学习价值函数，这样对于任何状态
    $s_{i}$，专家动作 $\pi_{E}(s_{i})$ 相比于 $D_{E}$ 中未显示的其他动作具有更高的 $Q$-值边际：
- en: '|  | $\displaystyle Q(s_{i},\pi_{E}(s_{i}))-\underset{a\in{\mathcal{A}}\backslash\pi_{E}(s_{i})}{\max}Q(s_{i},a)\geq
    1-\xi_{i}.\vspace{-0.05in}$ |  | (6) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q(s_{i},\pi_{E}(s_{i}))-\underset{a\in{\mathcal{A}}\backslash\pi_{E}(s_{i})}{\max}Q(s_{i},a)\geq
    1-\xi_{i}.\vspace{-0.05in}$ |  | (6) |'
- en: 'The term $\xi_{i}$ is used to account for the case of imperfect demonstrations.
    [[95](#bib.bib95)] further extended the work of *APID* with a different evaluation
    loss:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 $\xi_{i}$ 用于处理不完美演示的情况。[[95](#bib.bib95)] 进一步扩展了 *APID* 的工作，采用了不同的评估损失：
- en: '|  | $\displaystyle{\mathcal{L}}^{\pi}={\mathds{E}}_{(s,a)\sim{D_{\pi}}}\&#124;{\mathcal{T}}^{*}Q(s,a)-Q(s,a)\&#124;,$
    |  | (7) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathcal{L}}^{\pi}={\mathds{E}}_{(s,a)\sim{D_{\pi}}}\&#124;{\mathcal{T}}^{*}Q(s,a)-Q(s,a)\&#124;,$
    |  | (7) |'
- en: where ${\mathcal{T}}^{*}Q(s,a)=R(s,a)+\gamma{\mathds{E}}_{s^{\prime}\sim p(.|s,a)}[\underset{a^{\prime}}{\max}Q(s^{\prime},a^{\prime})].$
    Their work theoretically converges to the optimal $Q$-function compared with *APID*,
    as ${\mathcal{L}}_{\pi}$ is minimizing the optimal Bellman residual instead of
    the empirical norm.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\mathcal{T}}^{*}Q(s,a)=R(s,a)+\gamma{\mathds{E}}_{s^{\prime}\sim p(.|s,a)}[\underset{a^{\prime}}{\max}Q(s^{\prime},a^{\prime})]$。他们的工作在理论上相较于
    *APID* 收敛到最优 $Q$-函数，因为 ${\mathcal{L}}_{\pi}$ 是最小化最优贝尔曼残差，而不是经验范数。
- en: 'In addition to policy iteration, the following two approaches integrate demonstration
    data into the TD-learning framework, such as $Q$-learning. Specifically, [[92](#bib.bib92)]
    proposed the *DQfD* algorithm, which maintains two separate replay buffers to
    store demonstrated data and self-generated data, respectively, so that expert
    demonstrations can always be sampled with a certain probability. Their method
    leverages the refined priority replay mechanism [[99](#bib.bib99)] where the probability
    of sampling a transition $i$ is based on its priority $p_{i}$ with a temperature
    parameter $\alpha$: $P(i)=\frac{p_{i}^{\alpha}}{\sum_{k}{p_{k}^{\alpha}}}.$ Another
    algorithm named LfDS was proposed by [[96](#bib.bib96)], which draws a close connection
    to reward shaping (Section [5.1](#S5.SS1 "5.1 Reward Shaping ‣ 5 Transfer Learning
    Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")).
    LfDS builds the potential value of a state-action pair as the highest similarity
    between the given pair and the expert demonstrations. This augmented reward assigns
    more credits to state-actions that are more similar to expert demonstrations,
    encouraging the agent for expert-like behavior.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '除了策略迭代之外，以下两种方法将演示数据集成到 TD 学习框架中，例如 $Q$-学习。具体来说，[[92](#bib.bib92)] 提出了 *DQfD*
    算法，该算法维护两个独立的回放缓冲区，分别存储演示数据和自生成数据，以便专家演示可以始终以一定的概率被采样。他们的方法利用了精炼的优先级回放机制 [[99](#bib.bib99)]，其中采样转移
    $i$ 的概率基于其优先级 $p_{i}$ 和温度参数 $\alpha$：$P(i)=\frac{p_{i}^{\alpha}}{\sum_{k}{p_{k}^{\alpha}}}$。另一个名为
    LfDS 的算法由 [[96](#bib.bib96)] 提出，它与奖励塑形有很大的关联（见 [5.1](#S5.SS1 "5.1 Reward Shaping
    ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement
    Learning: A Survey")）。LfDS 将状态-动作对的潜在价值构建为给定对与专家演示之间的最高相似度。这种增强奖励对与专家演示更相似的状态-动作分配更多的信用，鼓励智能体表现出专家级行为。'
- en: 'Besides $Q$-learning, recent work has integrated LfD into *policy gradient* [[78](#bib.bib78),
    [93](#bib.bib93), [69](#bib.bib69), [97](#bib.bib97), [63](#bib.bib63)]. A representative
    work along this line is Generative Adversarial Imitation Learning (GAIL) [[69](#bib.bib69)].
    GAIL introduced the notion of *occupancy measure* $d_{\pi}$, which is the stationary
    state-action distributions derived from a policy $\pi$. Based on this notion,
    a new reward function is designed such that maximizing the accumulated new rewards
    encourages minimizing the distribution divergence between the *occupancy measure*
    of the current policy $\pi$ and the expert policy $\pi_{E}$. Specifically, the
    new reward is learned by adversarial training [[62](#bib.bib62)]: a discriminator
    $D$ is learned to distinguish interactions sampled from the current policy $\pi$
    and the expert policy $\pi_{E}$:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 除了$Q$-学习，最近的研究将学习从示范（LfD）整合到*策略梯度*中[[78](#bib.bib78), [93](#bib.bib93), [69](#bib.bib69),
    [97](#bib.bib97), [63](#bib.bib63)]。其中一个代表性工作是生成对抗模仿学习（GAIL）[[69](#bib.bib69)]。GAIL
    引入了*占据测量* $d_{\pi}$的概念，该概念是从策略$\pi$中推导出的平稳状态-动作分布。基于这一概念，设计了一个新的奖励函数，使得最大化累计的新奖励能鼓励最小化当前策略$\pi$与专家策略$\pi_{E}$之间的*占据测量*的分布差异。具体地，新奖励通过对抗训练来学习[[62](#bib.bib62)]：学习一个判别器$D$，以区分来自当前策略$\pi$和专家策略$\pi_{E}$的交互：
- en: '|  | $\displaystyle J_{D}=\max_{D:{\mathcal{S}}\times{\mathcal{A}}\to(0,1)}{\mathds{E}}_{d_{\pi}}\log[1-D(s,a)]+{\mathds{E}}_{d_{E}}\log[D(s,a)]$
    |  | (8) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J_{D}=\max_{D:{\mathcal{S}}\times{\mathcal{A}}\to(0,1)}{\mathds{E}}_{d_{\pi}}\log[1-D(s,a)]+{\mathds{E}}_{d_{E}}\log[D(s,a)]$
    |  | (8) |'
- en: 'Since $\pi_{E}$ is unknown, its state-action distribution $d_{E}$ is estimated
    based on the given expert demonstrations $D_{E}$. The output of the discriminator
    is used as new rewards to encourage distribution matching, with $r^{\prime}(s,a)=-\log(1-D(s,a))$.
    The RL process is naturally altered to perform distribution matching by min-max
    optimization:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由于$\pi_{E}$未知，其状态-动作分布$d_{E}$是基于给定的专家示范$D_{E}$来估计的。判别器的输出被用作新的奖励，以鼓励分布匹配，新的奖励函数为$r^{\prime}(s,a)=-\log(1-D(s,a))$。通过最小-最大优化，自然地改变了强化学习（RL）过程，以执行分布匹配：
- en: '|  | $\displaystyle\max_{\pi}\min_{D}J(\pi,D):$ | $\displaystyle={\mathds{E}}_{d_{\pi}}\log[1-D(s,a)]+{\mathds{E}}_{d_{E}}\log[D(s,a)].$
    |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\pi}\min_{D}J(\pi,D):$ | $\displaystyle={\mathds{E}}_{d_{\pi}}\log[1-D(s,a)]+{\mathds{E}}_{d_{E}}\log[D(s,a)].$
    |  |'
- en: 'The philosophy in *GAIL* of using expert demonstrations for distribution matching
    has inspired other LfD algorithms. For example, [[97](#bib.bib97)] extended GAIL
    with an algorithm called *Policy Optimization from Demonstrations (POfD)*, which
    combines the discriminator reward with the environment reward:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*GAIL*中使用专家示范进行分布匹配的理念启发了其他LfD算法。例如，[[97](#bib.bib97)] 扩展了GAIL，提出了一种叫做*从示范中优化策略（POfD）*的算法，它将判别器奖励与环境奖励结合起来：'
- en: '|  | $\displaystyle\max_{\theta}={\mathds{E}}_{d_{\pi}}[r(s,a)]-\lambda D_{JS}[d_{\pi}&#124;&#124;d_{E}].$
    |  | (9) |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\theta}={\mathds{E}}_{d_{\pi}}[r(s,a)]-\lambda D_{JS}[d_{\pi}&#124;&#124;d_{E}].$
    |  | (9) |'
- en: 'Both GAIL and POfD are under an *on-policy* RL framework. To further improve
    the sample efficiency of TL, some *off-policy* algorithms have been proposed,
    such as *DDPGfD* [[78](#bib.bib78)] which is built upon the DDPG framework. DDPGfD
    shares a similar idea as *DQfD* in that they both use a second replay buffer for
    storing demonstrated data, and each demonstrated sample holds a sampling priority
    $p_{i}$. For a demonstrated sample, its priority $p_{i}$ is augmented with a constant
    bias $\epsilon_{D}>0$ for encouraging more frequent sampling of expert demonstrations:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: GAIL 和 POfD 都是在*在线* RL 框架下的。为了进一步提高迁移学习（TL）的样本效率，提出了一些*离线*算法，例如*DDPGfD*[[78](#bib.bib78)]，它建立在DDPG框架之上。DDPGfD与*DQfD*有类似的思想，它们都使用第二个重放缓冲区来存储示范数据，每个示范样本都有一个采样优先级$p_{i}$。对于一个示范样本，其优先级$p_{i}$被增加了一个常数偏差$\epsilon_{D}>0$，以鼓励更频繁地采样专家示范：
- en: '|  | $\displaystyle p_{i}=\delta_{i}^{2}+\lambda\&#124;\nabla_{a}Q(s_{i},a_{i}&#124;\theta^{Q})\&#124;^{2}+\epsilon+\epsilon_{D},$
    |  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{i}=\delta_{i}^{2}+\lambda\&#124;\nabla_{a}Q(s_{i},a_{i}&#124;\theta^{Q})\&#124;^{2}+\epsilon+\epsilon_{D},$
    |  |'
- en: 'where $\delta_{i}$ is the TD-residual for transition, $\|\nabla_{a}Q(s_{i},a_{i}|\theta^{Q})\|^{2}$
    is the loss applied to the actor, and $\epsilon$ is a small positive constant
    to ensure all transitions are sampled with some probability. Another work also
    adopted the DDPG framework to learn from demonstrations [[93](#bib.bib93)]. Their
    approach differs from DDPGfD in that its objective function is augmented with
    a *Behavior Cloning Loss* to encourage imitating on provided demonstrations: ${\mathcal{L}}_{BC}=\sum_{i=1}^{|D_{E}|}||\pi(s_{i}|\theta_{\pi})-a_{i}||^{2}$.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\delta_{i}$ 是过渡的 TD 残差，$\|\nabla_{a}Q(s_{i},a_{i}|\theta^{Q})\|^{2}$ 是对演员施加的损失，$\epsilon$
    是一个小的正常数，以确保所有过渡以某种概率进行采样。另一项工作还采用了 DDPG 框架从演示中学习 [[93](#bib.bib93)]。他们的方法与 DDPGfD
    的不同之处在于其目标函数通过 *行为克隆损失* 进行了增强，以鼓励模仿提供的演示：${\mathcal{L}}_{BC}=\sum_{i=1}^{|D_{E}|}||\pi(s_{i}|\theta_{\pi})-a_{i}||^{2}$。
- en: 'To further address the issue of suboptimal demonstrations, in [[93](#bib.bib93)]
    the form of *Behavior Cloning Loss* is altered based on the critic output, so
    that only demonstration actions with higher $Q$ values will lead to the loss penalty:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步解决次优演示的问题，在 [[93](#bib.bib93)] 中，*行为克隆损失* 的形式基于评论家输出进行了调整，以便只有 $Q$ 值更高的演示动作会导致损失惩罚：
- en: '|  | $\displaystyle{\mathcal{L}}_{BC}=\sum_{i=1}^{&#124;D_{E}&#124;}\left\&#124;\pi(s_{i}&#124;\theta_{\pi})-a_{i}\right\&#124;^{2}{\mathbbm{1}}[Q(s_{i},a_{i})>Q(s_{i},\pi(s_{i}))].$
    |  | (10) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathcal{L}}_{BC}=\sum_{i=1}^{|D_{E}|}\left\|\pi(s_{i}|\theta_{\pi})-a_{i}\right\|^{2}{\mathbbm{1}}[Q(s_{i},a_{i})>Q(s_{i},\pi(s_{i}))].$
    |  | (10) |'
- en: There are several challenges faced by LfD, one of which is the *imperfect demonstrations*.
    Previous approaches usually presume near-oracle demonstrations. Towards tackling
    suboptimal demonstrations, [[59](#bib.bib59)] leveraged the hinge-loss function
    to allow occasional violations of the property that $Q(s_{i},\pi_{E}(s_{i}))-\underset{a\in{\mathcal{A}}\backslash\pi_{E}(s_{i})}{\max}Q(s_{i},a)\geq
    1$. Some other work uses regularized objective to alleviate overfitting on biased
    data [[92](#bib.bib92), [99](#bib.bib99)]. A different strategy is to leverage
    those sub-optimal demonstrations only to boost the initial learning stage. For
    instance, [[63](#bib.bib63)] proposed *Self-Adaptive Imitation Learning (SAIL)*,
    which learns from suboptimal demonstrations using generative adversarial training
    while gradually selecting self-generated trajectories with high qualities to replace
    less superior demonstrations.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: LfD 面临几个挑战，其中之一是 *不完美的演示*。之前的方法通常假设接近理想的演示。为了应对次优演示，[[59](#bib.bib59)] 利用铰链损失函数允许偶尔违反
    $Q(s_{i},\pi_{E}(s_{i}))-\underset{a\in{\mathcal{A}}\backslash\pi_{E}(s_{i})}{\max}Q(s_{i},a)\geq
    1$ 的属性。一些其他工作使用正则化目标来缓解对偏置数据的过拟合 [[92](#bib.bib92), [99](#bib.bib99)]。另一种策略是仅利用这些次优演示来提升初始学习阶段。例如，[[63](#bib.bib63)]
    提出了 *自适应模仿学习 (SAIL)*，该方法通过生成对抗训练从次优演示中学习，同时逐渐选择高质量的自生成轨迹以替代较差的演示。
- en: 'Another challenge faced by LfD is *covariate drift* ([[100](#bib.bib100)]):
    demonstrations may be provided in limited numbers, which results in the learning
    agent lacking guidance on states that are unseen in the demonstration dataset.
    This challenge is aggravated in MDPs with sparse reward feedbacks, as the learning
    agent cannot obtain much supervision information from the environment either.
    Current efforts to address this challenge include encouraging explorations by
    using an entropy-regularized objective [[101](#bib.bib101)], decaying the effects
    of demonstration guidance by softening its regularization on policy learning over
    time [[102](#bib.bib102)], and introducing *disagreement regularizations* by training
    an ensemble of policies based on the given demonstrations, where the variance
    among policies serves as a negative reward function [[103](#bib.bib103)].'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: LfD 面临的另一个挑战是 *协变量漂移* ([[100](#bib.bib100)])：演示可能提供数量有限，这导致学习代理在演示数据集中未见的状态上缺乏指导。这个挑战在奖励反馈稀疏的
    MDP 中加剧，因为学习代理也无法从环境中获得大量的监督信息。目前应对这一挑战的努力包括通过使用熵正则化目标来鼓励探索 [[101](#bib.bib101)]，通过随时间软化对策略学习的正则化来衰减演示指导的效果
    [[102](#bib.bib102)]，以及通过基于给定演示训练一组策略引入 *分歧正则化*，其中策略之间的方差作为负奖励函数 [[103](#bib.bib103)]。
- en: 'We summarize the above-discussed approaches in Table [II](#S5.T2 "TABLE II
    ‣ 5.2 Learning from Demonstrations ‣ 5 Transfer Learning Approaches Deep Dive
    ‣ Transfer Learning in Deep Reinforcement Learning: A Survey"). In general, demonstration
    data can help in both *offline* pre-training for better initialization and *online*
    RL for efficient exploration. During the RL phase, demonstration data can be used
    together with self-generated data to encourage expert-like behaviors (*DDPGfD,
    DQFD*), to shape value functions (*APID*), or to guide the policy update in the
    form of an auxiliary objective function (*PID,GAIL, POfD*). To validate the algorithm
    robustness given different knowledge resources, most *LfD* methods are evaluated
    using metrics that either indicate the performance under *limited* demonstrations
    (*nka*) or *suboptimal* demonstrations (*nka*). The integration of *LfD* with
    *off-policy* RL backbone makes it natural to adopt *pe* metrics for evaluating
    how learning efficiency can be further improved by knowledge transfer. Developing
    more general LfD approaches that are agnostic to RL frameworks and can learn from
    sub-optimal or limited demonstrations would be the ongoing focus for this research
    domain.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表[II](#S5.T2 "TABLE II ‣ 5.2 从示范学习 ‣ 5 转移学习方法深度分析 ‣ 深度强化学习中的转移学习: 一项调查")中总结了上述讨论的方法。一般来说，示范数据可以帮助在*离线*预训练中获得更好的初始化，并在*在线*
    RL中进行高效探索。在RL阶段，示范数据可以与自生成的数据一起使用，以鼓励类似专家的行为（*DDPGfD, DQFD*），塑造价值函数（*APID*），或以辅助目标函数的形式指导策略更新（*PID,
    GAIL, POfD*）。为了验证在不同知识资源下算法的鲁棒性，大多数*LfD*方法使用的指标要么表明在*有限*示范下的性能（*nka*），要么表明在*次优*示范下的性能（*nka*）。将*LfD*与*离策略*
    RL 主干整合，使得采用*pe*指标来评估通过知识转移进一步提高学习效率成为自然选择。开发更通用的LfD方法，这些方法对RL框架不敏感，并能够从次优或有限的示范中学习，将是该研究领域的持续重点。'
- en: '| Methods |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |'
- en: '&#124; Optimality &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 最优性 &#124;'
- en: '&#124; guarantee &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 保证 &#124;'
- en: '| Format of transferred demonstrations | RL framework | Evaluation metrics
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 转移示范的格式 | RL 框架 | 评估指标 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| DPID | ✓ | Indicator binary-loss : ${\mathcal{L}}(s_{i})={\mathbbm{1}}\{\pi_{E}(s_{i})\neq\pi(s_{i})\}$
    | API | *ap, ar, nka* |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| DPID | ✓ | 指标二进制损失 : ${\mathcal{L}}(s_{i})={\mathbbm{1}}\{\pi_{E}(s_{i})\neq\pi(s_{i})\}$
    | API | *ap, ar, nka* |'
- en: '| APID | ✗ | Hinge loss on the marginal-loss: $\big{[}{\mathcal{L}}(Q,\pi,\pi_{E})\big{]}_{+}$
    | API | *ap, ar, nta, nkq* |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| APID | ✗ | 边际损失上的铰链损失: $\big{[}{\mathcal{L}}(Q,\pi,\pi_{E})\big{]}_{+}$ |
    API | *ap, ar, nta, nkq* |'
- en: '| APID extend | ✓ | Marginal-loss: ${\mathcal{L}}(Q,\pi,\pi_{E})$ | API | *ap,
    ar, nta, nkq* |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| APID 扩展 | ✓ | 边际损失: ${\mathcal{L}}(Q,\pi,\pi_{E})$ | API | *ap, ar, nta,
    nkq* |'
- en: '| [[93](#bib.bib93)] | ✓ | Increasing sampling priority and behavior cloning
    loss | DDPG | *ap, ar, tr, pe, nkq* |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| [[93](#bib.bib93)] | ✓ | 增加采样优先级和行为克隆损失 | DDPG | *ap, ar, tr, pe, nkq* |'
- en: '| DQfD | ✗ | Cached transitions in the replay buffer | DQN | *ap, ar, tr* |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| DQfD | ✗ | 重放缓冲区中的缓存过渡 | DQN | *ap, ar, tr* |'
- en: '| LfDS | ✗ | Reward shaping function | DQN | *ap, ar, tr* |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| LfDS | ✗ | 奖励塑形函数 | DQN | *ap, ar, tr* |'
- en: '| GAIL | ✓ | Reward shaping function: $-\lambda\log(1-D(s,a))$ | TRPO | *ap,
    ar, tr, pe, nka* |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| GAIL | ✓ | 奖励塑形函数: $-\lambda\log(1-D(s,a))$ | TRPO | *ap, ar, tr, pe, nka*
    |'
- en: '| POfD | ✓ | Reward shaping function: $r(s,a)-\lambda\log(1-D(s,a))$ | TRPO,PPO
    | *ap, ar, tr, pe, nka* |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| POfD | ✓ | 奖励塑形函数: $r(s,a)-\lambda\log(1-D(s,a))$ | TRPO,PPO | *ap, ar, tr,
    pe, nka* |'
- en: '| DDPGfD (pe) | ✓ | Increasing sampling priority | DDPG | *ap, ar, tr, pe*
    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| DDPGfD (pe) | ✓ | 增加采样优先级 | DDPG | *ap, ar, tr, pe* |'
- en: '| SAIL | ✗ | Reward shaping function: $r(s,a)-\lambda\log(1-D(s,a))$ | DDPG
    | *ap, ar, tr, pe, nkq, nka* |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| SAIL | ✗ | 奖励塑形函数: $r(s,a)-\lambda\log(1-D(s,a))$ | DDPG | *ap, ar, tr, pe,
    nkq, nka* |'
- en: 'TABLE II: A comparison of learning from demonstration approaches.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 从示范学习方法的比较。'
- en: 5.3 Policy Transfer
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 策略转移
- en: 'Policy transfer is a TL approach where the external knowledge takes the form
    of pre-trained policies from one or multiple source domains. Work discussed in
    this section is built upon a *many-to-one* problem setting, described as below:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 策略转移是一种TL方法，其中外部知识以一个或多个源领域的预训练策略的形式存在。本节讨论的工作建立在*多对一*问题设置的基础上，如下所述：
- en: Policy Transfer.
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 策略转移。
- en: A set of teacher policies $\pi_{E_{1}},\pi_{E_{2}},\dots,\pi_{E_{K}}$ are trained
    on a set of source domains ${\mathcal{M}}_{1},{\mathcal{M}}_{2},\dots,{\mathcal{M}}_{K}$,
    respectively. A student policy $\pi$ is learned for a target domain by leveraging
    knowledge from $\{\pi_{E_{i}}\}_{i=1}^{K}$.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一组教师策略$\pi_{E_{1}},\pi_{E_{2}},\dots,\pi_{E_{K}}$分别在一组源领域${\mathcal{M}}_{1},{\mathcal{M}}_{2},\dots,{\mathcal{M}}_{K}$上进行训练。学生策略$\pi$通过利用来自$\{\pi_{E_{i}}\}_{i=1}^{K}$的知识来为目标领域学习。
- en: 'For the *one-to-one* scenario with only one teacher policy, one can consider
    it as a special case of the above with $K=1$. Next, we categorize recent work
    of policy transfer into two techniques: *policy distillation* and *policy reuse*.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于只有一个教师策略的*一对一*场景，可以将其视为上述情况的特例，其中$K=1$。接下来，我们将最近的策略转移工作分为两种技术：*策略蒸馏*和*策略重用*。
- en: 5.3.1 Transfer Learning via Policy Distillation
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 通过策略蒸馏的迁移学习
- en: 'The idea of *knowledge distillation* has been applied to the field of RL to
    enable policy distillation. Knowledge distillation was first proposed by [[104](#bib.bib104)]
    as an approach of knowledge ensemble from multiple teacher models into a single
    student model. Conventional policy distillation approaches transfer the teacher
    policy following a supervised learning paradigm [[105](#bib.bib105), [106](#bib.bib106)].
    Specifically, a student policy is learned by minimizing the divergence of action
    distributions between the teacher policy $\pi_{E}$ and student policy $\pi_{\theta}$,
    which is denoted as ${\mathcal{H}}^{\times}(\pi_{E}(\tau_{t})|\pi_{\theta}(\tau_{t}))$:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*知识蒸馏*的概念已经应用于强化学习领域，以实现策略蒸馏。知识蒸馏最早由[[104](#bib.bib104)]提出，作为一种将多个教师模型的知识集合到单一学生模型中的方法。传统的策略蒸馏方法遵循监督学习范式来转移教师策略[[105](#bib.bib105),
    [106](#bib.bib106)]。具体而言，学生策略通过最小化教师策略$\pi_{E}$和学生策略$\pi_{\theta}$之间的动作分布的差异来学习，这个差异表示为${\mathcal{H}}^{\times}(\pi_{E}(\tau_{t})|\pi_{\theta}(\tau_{t}))$：'
- en: '|  | $\displaystyle\min_{\theta}{\mathds{E}}_{\tau\sim\pi_{E}}[\sum_{t=1}^{&#124;\tau&#124;}\nabla_{\theta}{\mathcal{H}}^{\times}(\pi_{E}(\tau_{t})&#124;\pi_{\theta}(\tau_{t}))].$
    |  | (11) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{\theta}{\mathds{E}}_{\tau\sim\pi_{E}}[\sum_{t=1}^{&#124;\tau&#124;}\nabla_{\theta}{\mathcal{H}}^{\times}(\pi_{E}(\tau_{t})&#124;\pi_{\theta}(\tau_{t}))].$
    |  | (11) |'
- en: 'The above expectation is taken over trajectories sampled from the teacher policy
    $\pi_{E}$, hence this approach is called *teacher distillation*. One example along
    this line is [[105](#bib.bib105)], in which $N$ teacher policies are learned for
    $N$ source tasks separately, and each teacher yields a dataset $D^{E}=\{s_{i},{\bm{q}}_{i}\}_{i=0}^{N}$
    consisting of observations $s$ and vectors of the corresponding $Q$-values ${\bm{q}}$,
    such that ${\bm{q}}_{i}=[Q(s_{i},a_{1}),Q(s_{i},a_{2}),...|a_{j}\in{\mathcal{A}}]$.
    Teacher policies are further distilled to a single student $\pi_{\theta}$ by minimizing
    the KL-Divergence between each teacher $\pi_{E_{i}}(a|s)$ and the student $\pi_{\theta}$,
    approximated using the dataset $D^{E}$: $\min_{\theta}{\mathcal{D}}_{KL}(\pi^{E}|\pi_{\theta})\approx\sum_{i=1}^{|D^{E}|}\text{softmax}\left(\frac{{\bm{q}}^{E}_{i}}{\tau}\right)\ln\left(\frac{\text{softmax}({\bm{q}}_{i}^{E})}{\text{softmax}({\bm{q}}_{i}^{\theta})}\right)$.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 上述期望值是对从教师策略$\pi_{E}$中采样的轨迹取的，因此这种方法称为*教师蒸馏*。沿着这条路线的一个例子是[[105](#bib.bib105)]，其中$N$个教师策略分别为$N$个源任务学习，每个教师生成一个数据集$D^{E}=\{s_{i},{\bm{q}}_{i}\}_{i=0}^{N}$，包括观察值$s$和对应的$Q$-值向量${\bm{q}}$，使得${\bm{q}}_{i}=[Q(s_{i},a_{1}),Q(s_{i},a_{2}),...|a_{j}\in{\mathcal{A}}]$。教师策略进一步被蒸馏到单一学生策略$\pi_{\theta}$，通过最小化每个教师$\pi_{E_{i}}(a|s)$与学生$\pi_{\theta}$之间的KL散度，使用数据集$D^{E}$进行近似：$\min_{\theta}{\mathcal{D}}_{KL}(\pi^{E}|\pi_{\theta})\approx\sum_{i=1}^{|D^{E}|}\text{softmax}\left(\frac{{\bm{q}}^{E}_{i}}{\tau}\right)\ln\left(\frac{\text{softmax}({\bm{q}}_{i}^{E})}{\text{softmax}({\bm{q}}_{i}^{\theta})}\right)$。
- en: 'Another policy distillation approach is *student distillation* [[60](#bib.bib60),
    [51](#bib.bib51)], which is resemblant to teacher distillation except that during
    the optimization step, the objective expectation is taken over trajectories sampled
    from the student policy instead of the teacher policy, i.e.: $\min_{\theta}{\mathds{E}}_{\tau\sim\pi_{\theta}}\left[\sum_{t=1}^{|\tau|}\nabla_{\theta}{\mathcal{H}}^{\times}(\pi_{E}(\tau_{t})|\pi_{\theta}(\tau_{t}))\right]$.
    [[60](#bib.bib60)] summarized related work on both kinds of distillation approaches.
    Although it is feasible to combine both distillation approaches [[100](#bib.bib100)],
    we observe that more recent work focuses on student distillation, which empirically
    shows better exploration ability compared to teacher distillation, especially
    when the teacher policies are *deterministic*.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种政策蒸馏方法是 *student distillation* [[60](#bib.bib60), [51](#bib.bib51)]，这与教师蒸馏相似，只是在优化步骤中，目标期望是基于从学生政策采样的轨迹，而不是教师政策，即：$\min_{\theta}{\mathds{E}}_{\tau\sim\pi_{\theta}}\left[\sum_{t=1}^{|\tau|}\nabla_{\theta}{\mathcal{H}}^{\times}(\pi_{E}(\tau_{t})|\pi_{\theta}(\tau_{t}))\right]$。[[60](#bib.bib60)]
    总结了关于这两种蒸馏方法的相关工作。虽然结合这两种蒸馏方法是可行的 [[100](#bib.bib100)]，但我们观察到更近期的工作集中于学生蒸馏，实证表明与教师蒸馏相比，尤其是在教师政策是
    *deterministic* 时，它显示出更好的探索能力。
- en: 'Taking an alternative perspective, there are two approaches of policy distillation:
    (1) minimizing the cross-entropy between the teacher and student policy distributions
    over actions [[51](#bib.bib51), [107](#bib.bib107)]; and (2) maximizing the probability
    that the teacher policy will visit trajectories generated by the student, i.e. $\max_{\theta}P(\tau\sim\pi_{E}|\tau\sim\pi_{\theta})$
    [[50](#bib.bib50), [108](#bib.bib108)]. One example of approach (1) is the *Actor-mimic*
    algorithm [[51](#bib.bib51)]. This algorithm distills the knowledge of expert
    agents into the student by minimizing the cross entropy between the student policy
    $\pi_{\theta}$ and each teacher policy $\pi_{E_{i}}$ over actions: ${\mathcal{L}}^{i}(\theta)=\sum_{a\in{\mathcal{A}}_{E_{i}}}\pi_{E_{i}}(a|s)\log_{\pi_{\theta}}(a|s)$,
    where each teacher agent is learned using a DQN framework. The teacher policy
    is therefore derived from the Boltzmann distributions over the $Q$-function output:
    $\pi_{E_{i}}(a|s)=\frac{e^{\tau^{-1}Q_{E_{i}}(s,a)}}{\sum_{a^{\prime}\in{\mathcal{A}}_{E_{i}}}e^{\tau^{-1}Q_{E_{i}}(s,a^{\prime})}}$.
    An instantiation of approach (2) is the *Distral* algorithm [[50](#bib.bib50)].
    which learns a *centroid* policy $\pi_{\theta}$ that is derived from $K$ teacher
    policies. The knowledge in each teacher $\pi_{E_{i}}$ is distilled to the centroid
    and get transferred to the student, while both the transition dynamics ${\mathcal{T}}_{i}$
    and reward distributions ${\mathcal{R}}_{i}$ for source domain ${\mathcal{M}}_{i}$
    are heterogeneous. The student policy is learned by maximizing a multi-task learning
    objective $\max_{\theta}\sum_{i=1}^{K}J(\pi_{\theta},\pi_{E_{i}})$, where'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一个角度来看，政策蒸馏有两种方法：(1) 最小化教师和学生政策分布在动作上的交叉熵 [[51](#bib.bib51), [107](#bib.bib107)];
    和 (2) 最大化教师政策访问由学生生成的轨迹的概率，即 $\max_{\theta}P(\tau\sim\pi_{E}|\tau\sim\pi_{\theta})$
    [[50](#bib.bib50), [108](#bib.bib108)]。方法 (1) 的一个例子是 *Actor-mimic* 算法 [[51](#bib.bib51)]。该算法通过最小化学生政策
    $\pi_{\theta}$ 和每个教师政策 $\pi_{E_{i}}$ 在动作上的交叉熵来蒸馏专家代理的知识：${\mathcal{L}}^{i}(\theta)=\sum_{a\in{\mathcal{A}}_{E_{i}}}\pi_{E_{i}}(a|s)\log_{\pi_{\theta}}(a|s)$，其中每个教师代理是使用
    DQN 框架学习的。因此，教师政策来源于 $Q$-函数输出的 Boltzmann 分布：$\pi_{E_{i}}(a|s)=\frac{e^{\tau^{-1}Q_{E_{i}}(s,a)}}{\sum_{a^{\prime}\in{\mathcal{A}}_{E_{i}}}e^{\tau^{-1}Q_{E_{i}}(s,a^{\prime})}}$。方法
    (2) 的一个实例是 *Distral* 算法 [[50](#bib.bib50)]。该算法学习一个 *centroid* 政策 $\pi_{\theta}$，它来源于
    $K$ 个教师政策。每个教师 $\pi_{E_{i}}$ 的知识被蒸馏到质心并转移到学生，而源域 ${\mathcal{M}}_{i}$ 的过渡动态 ${\mathcal{T}}_{i}$
    和奖励分布 ${\mathcal{R}}_{i}$ 是异质的。通过最大化多任务学习目标 $\max_{\theta}\sum_{i=1}^{K}J(\pi_{\theta},\pi_{E_{i}})$
    来学习学生政策，其中
- en: '|  | $\displaystyle J(\pi_{\theta},\pi_{E_{i}})=\sum_{t}$ | $\displaystyle{\mathds{E}}_{(s_{t},a_{t})\sim\pi_{\theta}}\Big{[}\sum_{t\geq
    0}\gamma^{t}(r_{i}(a_{t},s_{t})+$ |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J(\pi_{\theta},\pi_{E_{i}})=\sum_{t}$ | $\displaystyle{\mathds{E}}_{(s_{t},a_{t})\sim\pi_{\theta}}\Big{[}\sum_{t\geq
    0}\gamma^{t}(r_{i}(a_{t},s_{t})+$ |  |'
- en: '|  |  | $\displaystyle\frac{\alpha}{\beta}\log\pi_{\theta}(a_{t}&#124;s_{t})-\frac{1}{\beta}\log(\pi_{E_{i}}(a_{t}&#124;s_{t})))\Big{]},$
    |  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\frac{\alpha}{\beta}\log\pi_{\theta}(a_{t}&#124;s_{t})-\frac{1}{\beta}\log(\pi_{E_{i}}(a_{t}&#124;s_{t})))\Big{]},$
    |  |'
- en: 'in which both $\log\pi_{\theta}(a_{t}|s_{t})$ and $\pi_{\theta}$ are used as
    augmented rewards. Therefore, the above approach also draws a close connection
    to Reward Shaping (Section [5.1](#S5.SS1 "5.1 Reward Shaping ‣ 5 Transfer Learning
    Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")).
    In effect, the $\log\pi_{\theta}(a_{t}|s_{t})$ term guides the learning policy
    $\pi_{\theta}$ to yield actions that are more likely to be generated by the teacher
    policy, whereas the entropy term $-\log(\pi_{E_{i}}(a_{t}|s_{t})$ encourages exploration.
    A similar approach was proposed by [[107](#bib.bib107)] which only uses the cross-entropy
    between teacher and student policy $\lambda{\mathcal{H}}(\pi_{E}(a_{t}|s_{t})||\pi_{\theta}(a_{t}|s_{t}))$
    to reshape rewards. Moreover, they adopted a dynamically fading coefficient to
    alleviate the effect of the augmented reward so that the student policy becomes
    independent of the teachers after certain optimization iterations.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '在该方法中，$\log\pi_{\theta}(a_{t}|s_{t})$ 和 $\pi_{\theta}$ 被用作增强奖励。因此，上述方法也与奖励塑造（见第
    [5.1](#S5.SS1 "5.1 Reward Shaping ‣ 5 Transfer Learning Approaches Deep Dive ‣
    Transfer Learning in Deep Reinforcement Learning: A Survey") 节）密切相关。实际上，$\log\pi_{\theta}(a_{t}|s_{t})$
    项引导学习策略 $\pi_{\theta}$ 产生更可能由教师策略生成的动作，而熵项 $-\log(\pi_{E_{i}}(a_{t}|s_{t})$ 则鼓励探索。[[107](#bib.bib107)]
    提出的类似方法仅使用教师和学生策略之间的交叉熵 $\lambda{\mathcal{H}}(\pi_{E}(a_{t}|s_{t})||\pi_{\theta}(a_{t}|s_{t}))$
    来塑造奖励。此外，他们采用了动态衰退系数以减轻增强奖励的影响，从而使学生策略在经过一定优化迭代后独立于教师策略。'
- en: 5.3.2 Transfer Learning via Policy Reuse
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 通过策略重用进行迁移学习
- en: '*Policy reuse* directly reuses policies from source tasks to build the target
    policy. The notion of policy reuse was proposed by [[109](#bib.bib109)], which
    directly learns the target policy as a weighted combination of different source-domain
    policies, and the probability for each source domain policy to be used is related
    to its expected performance gain in the target domain: $P(\pi_{E_{i}})=\frac{\exp{(tW_{i})}}{\sum_{j=0}^{K}\exp{(tW_{j})}},$
    where $t$ is a dynamic temperature parameter that increases over time. Under a
    $Q$-learning framework, the $Q$-function of the target policy is learned in an
    iterative scheme: during every learning episode, $W_{i}$ is evaluated for each
    expert policy $\pi_{E_{i}}$, and $W_{0}$ is obtained for the learning policy,
    from which a reuse probability $P$ is derived. Next, a behavior policy is sampled
    from this probability $P$. After each training episode, both $W_{i}$ and the temperature
    $t$ for calculating the reuse probability is updated accordingly. One limitation
    of this approach is that the $W_{i}$, i.e. the expected return of each expert
    policy on the target task, needs to be evaluated frequently. This work was implemented
    in a tabular case, leaving the scalability issue unresolved. More recent work
    by [[110](#bib.bib110)] extended the *policy improvement* theorem [[111](#bib.bib111)]
    from one to multiple policies, which is named as *Generalized Policy Improvement*.
    We refer its main theorem as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*策略重用* 直接从源任务中重用策略来构建目标策略。*策略重用* 的概念由 [[109](#bib.bib109)] 提出，该方法直接将目标策略学习为不同源领域策略的加权组合，每个源领域策略被使用的概率与其在目标领域中的预期性能增益相关：$P(\pi_{E_{i}})=\frac{\exp{(tW_{i})}}{\sum_{j=0}^{K}\exp{(tW_{j})}},$
    其中 $t$ 是一个随着时间增加的动态温度参数。在 $Q$-学习框架下，目标策略的 $Q$-函数通过迭代方案进行学习：在每次学习期间，对每个专家策略 $\pi_{E_{i}}$
    评估 $W_{i}$，并获得学习策略的 $W_{0}$，从中推导出重用概率 $P$。接着，从这个概率 $P$ 中采样一个行为策略。在每次训练后，更新 $W_{i}$
    和计算重用概率的温度 $t$。这种方法的一个局限性是需要频繁评估 $W_{i}$，即每个专家策略在目标任务上的预期回报。该方法在表格情况下进行了实现，但未解决可扩展性问题。最近，[[110](#bib.bib110)]
    的工作扩展了 *策略改进* 定理 [[111](#bib.bib111)] 从一个策略到多个策略，这被称为 *广义策略改进*。我们将其主要定理如下：'
- en: Theorem.
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理。
- en: '*[Generalized Policy Improvement (GPI)]* Let $\{\pi_{i}\}_{i=1}^{n}$ be $n$
    policies and let $\{\hat{Q}^{\pi_{i}}\}_{i=1}^{n}$ be their approximated action-value
    functions, s.t: $\Big{|}Q^{\pi_{i}}(s,a)-\hat{Q}^{\pi_{i}}(s,a)\Big{|}\leq\epsilon~{}\forall
    s\in{\mathcal{S}},a\in{\mathcal{A}}\text{, and }i\in[n]$. Define $\pi(s)=\underset{a}{\arg\max}~{}\underset{i}{\max}\hat{Q}^{\pi_{i}}(s,a)$,
    then: $Q^{\pi}(s,a)\geq\underset{i}{\max}Q^{\pi_{i}}(s,a)-\frac{2}{1-\gamma}\epsilon$,
    $\forall~{}s\in{\mathcal{S}},a\in{\mathcal{A}}$.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*[广义策略改进（GPI）]* 设 $\{\pi_{i}\}_{i=1}^{n}$ 为 $n$ 个策略，$\{\hat{Q}^{\pi_{i}}\}_{i=1}^{n}$
    为它们的近似动作值函数，满足：$\Big{|}Q^{\pi_{i}}(s,a)-\hat{Q}^{\pi_{i}}(s,a)\Big{|}\leq\epsilon~{}\forall
    s\in{\mathcal{S}},a\in{\mathcal{A}}\text{, 和 }i\in[n]$。定义 $\pi(s)=\underset{a}{\arg\max}~{}\underset{i}{\max}\hat{Q}^{\pi_{i}}(s,a)$，则：$Q^{\pi}(s,a)\geq\underset{i}{\max}Q^{\pi_{i}}(s,a)-\frac{2}{1-\gamma}\epsilon$，$\forall~{}s\in{\mathcal{S}},a\in{\mathcal{A}}$。'
- en: 'Based on this theorem, a policy improvement approach can be naturally derived
    by greedily choosing the action which renders the highest $Q$-value among all
    policies for a given state. Another work along this line is [[110](#bib.bib110)],
    in which an expert policy $\pi_{E_{i}}$ is also trained on a different source
    domain ${\mathcal{M}}_{i}$ with reward function ${\mathcal{R}}_{i}$, so that $Q^{\pi}_{{\mathcal{M}}_{0}}(s,a)\neq
    Q^{\pi}_{{\mathcal{M}}_{i}}(s,a)$. To efficiently evaluate the $Q$-functions of
    different source policies in the target MDP, a disentangled representation ${\bm{\psi}}(s,a)$
    over the states and actions is learned using neural networks and is generalized
    across multiple tasks. Next, a task (reward) mapper ${\mathbf{w}}_{i}$ is learned,
    based on which the $Q$-function can be derived: $Q^{\pi}_{i}(s,a)={\bm{\psi}}(s,a)^{T}{\mathbf{w}}_{i}.$
    [[110](#bib.bib110)] proved that the loss of GPI is bounded by the difference
    between the source and the target tasks. In addition to policy-reuse, their approach
    involves learning a shared representation ${\bm{\psi}}(s,a)$, which is also a
    form of transferred knowledge and will be elaborated more in Section [5.5.2](#S5.SS5.SSS2
    "5.5.2 Disentangling Representations ‣ 5.5 Representation Transfer ‣ 5 Transfer
    Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning:
    A Survey").'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一定理，可以自然地通过贪婪地选择在给定状态下所有策略中具有最高 $Q$ 值的动作来推导出一种策略改进方法。另一个相关的工作是[[110](#bib.bib110)]，其中一个专家策略
    $\pi_{E_{i}}$ 也在不同的源领域 ${\mathcal{M}}_{i}$ 上训练，具有奖励函数 ${\mathcal{R}}_{i}$，以使得
    $Q^{\pi}_{{\mathcal{M}}_{0}}(s,a)\neq Q^{\pi}_{{\mathcal{M}}_{i}}(s,a)$。为了在目标
    MDP 中高效评估不同源策略的 $Q$-函数，利用神经网络学习了一个在状态和动作上的解耦表示 ${\bm{\psi}}(s,a)$ 并在多个任务中进行泛化。接下来，学习了一个任务（奖励）映射器
    ${\mathbf{w}}_{i}$，基于此可以推导出 $Q$-函数：$Q^{\pi}_{i}(s,a)={\bm{\psi}}(s,a)^{T}{\mathbf{w}}_{i}$。[[110](#bib.bib110)]
    证明了 GPI 的损失由源任务和目标任务之间的差异所界定。除了策略重用，他们的方法还涉及学习一个共享表示 ${\bm{\psi}}(s,a)$，这也是一种转移知识的形式，将在第
    [5.5.2](#S5.SS5.SSS2 "5.5.2 解耦表示 ‣ 5.5 表示转移 ‣ 5 转移学习方法深入 ‣ 深度强化学习中的转移学习：综述") 节中详细阐述。
- en: 'We summarize the abovementioned policy transfer approaches in Table [III](#S5.T3
    "TABLE III ‣ 5.3.2 Transfer Learning via Policy Reuse ‣ 5.3 Policy Transfer ‣
    5 Transfer Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement
    Learning: A Survey"). In general, policy transfer can be realized by knowledge
    distillation, which can be either optimized from the student’s perspecive (student
    distillation), or from the teacher’s perspective (teacher distillation) Alternatively,
    teacher policies can also be directly reused to update the target policy. Regarding
    evaluation, most of the abovementioned work has investigated a multi-teacher transfer
    scenario, hence the *generalization* ability or *robustness* is largely evaluated
    on metrics such as *performance sensitivity(ps)* (e.g.  performance given different
    numbers of teacher policies or source tasks ). *Performance with fixed epochs
    (pe)* is another commonly shared metric to evaluate how the learned policy can
    quickly adapt to the target domain. All approaches discussed so far presumed one
    or multiple expert policies, which are always at the disposal of the learning
    agent. Open questions along this line include How to leverage imperfect policies
    for knowledge transfer, or How to refer to teacher policies within a budget.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表 [III](#S5.T3 "表 III ‣ 5.3.2 通过政策复用进行的迁移学习 ‣ 5.3 政策转移 ‣ 5 转移学习方法深度解析 ‣ 深度强化学习中的迁移学习：综述")
    中总结了上述的政策转移方法。一般来说，政策转移可以通过知识蒸馏实现，这可以从学生的角度（学生蒸馏）或教师的角度（教师蒸馏）进行优化。或者，教师策略也可以直接复用来更新目标策略。关于评估，上述大多数工作都研究了多教师转移场景，因此
    *泛化* 能力或 *鲁棒性* 主要通过 *性能敏感性（ps）*（例如，给定不同数量的教师策略或源任务的性能）等指标进行评估。*固定周期内的性能（pe）* 是另一个常见的指标，用于评估学习的策略如何快速适应目标领域。迄今为止讨论的所有方法都假设有一个或多个专家策略，这些策略始终可供学习代理使用。沿着这条线的开放问题包括如何利用不完美的策略进行知识转移，或如何在预算范围内参考教师策略。
- en: '| Paper | Transfer approach | MDP difference | RL framework | Evaluation metrics
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 转移方法 | MDP 差异 | RL 框架 | 评估指标 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| [[105](#bib.bib105)] | Distillation | ${\mathcal{S}},{\mathcal{A}}$ | DQN
    | $ap,ar$ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| [[105](#bib.bib105)] | 蒸馏 | ${\mathcal{S}},{\mathcal{A}}$ | DQN | $ap,ar$
    |'
- en: '| [[106](#bib.bib106)] | Distillation | ${\mathcal{S}},{\mathcal{A}}$ | DQN
    | $ap,ar,pe,ps$ |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| [[106](#bib.bib106)] | 蒸馏 | ${\mathcal{S}},{\mathcal{A}}$ | DQN | $ap,ar,pe,ps$
    |'
- en: '| [[51](#bib.bib51)] | Distillation | ${\mathcal{S}},{\mathcal{A}}$ | Soft
    Q-learning | $ap,ar,tr$, $pe,ps$ |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| [[51](#bib.bib51)] | 蒸馏 | ${\mathcal{S}},{\mathcal{A}}$ | Soft Q 学习 | $ap,ar,tr$,
    $pe,ps$ |'
- en: '| [[50](#bib.bib50)] | Distillation | ${\mathcal{S}},{\mathcal{A}}$ | A3C |
    $ap,ar,pe$, $tt$ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| [[50](#bib.bib50)] | 蒸馏 | ${\mathcal{S}},{\mathcal{A}}$ | A3C | $ap,ar,pe$,
    $tt$ |'
- en: '| [[109](#bib.bib109)] | Reuse | ${\mathcal{R}}$ | Tabular Q-learning | $ap,ar,ps,tr$
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| [[109](#bib.bib109)] | 复用 | ${\mathcal{R}}$ | 表格 Q 学习 | $ap,ar,ps,tr$ |'
- en: '| [[110](#bib.bib110)] | Reuse | ${\mathcal{R}}$ | DQN | $ap,ar,pe,ps$ |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| [[110](#bib.bib110)] | 复用 | ${\mathcal{R}}$ | DQN | $ap,ar,pe,ps$ |'
- en: 'TABLE III: A comparison of policy transfer approaches.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 政策转移方法的比较。'
- en: 5.4 Inter-Task Mapping
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 任务间映射
- en: 'In this section, we review TL approaches that utilize mapping functions between
    the source and the target domains to assist knowledge transfer. Research in this
    domain can be analyzed from two perspectives: (1) *which domain does the mapping
    function apply to*, and (2) *how is the mapped representation utilized*. Most
    work discussed in this section shares a common assumption as below:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们回顾了利用源域与目标域之间映射函数来协助知识转移的迁移学习方法。这个领域的研究可以从两个角度来分析：（1）*映射函数适用于哪个领域*，以及（2）*映射后的表示是如何被利用的*。本节讨论的大多数工作有一个共同的假设如下：
- en: Assumption.
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设。
- en: One-to-one mappings exist between the source domain ${\mathcal{M}}_{s}$ and
    the target domain ${\mathcal{M}}_{t}$.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 源域 ${\mathcal{M}}_{s}$ 和目标域 ${\mathcal{M}}_{t}$ 之间存在一对一的映射关系。
- en: Earlier work along this line requires a *given mapping function* [[66](#bib.bib66),
    [112](#bib.bib112)]. One examples is [[66](#bib.bib66)] which assumes that each
    target state (action) has a unique correspondence in the source domain, and two
    mapping functions $X_{S},X_{A}$ are provided over the state space and the action
    space, respectively, so that $X_{S}({\mathcal{S}}^{t})\to{\mathcal{S}}^{s}$, $X_{A}({\mathcal{A}}^{t})\to{\mathcal{A}}^{s}$.
    Based on $X_{S}$ and $X_{A}$, a mapping function over the $Q$-values $M(Q_{s})\to
    Q_{t}$ can be derived accordingly. Another work is done by [[112](#bib.bib112)]
    which transfers *advice* as the knowledge between two domains. In their settings,
    the *advice* comes from a human expert who provides the mapping function over
    the $Q$-values in the source domain and transfers it to the learning policy for
    the target domain. This advice encourages the learning agent to prefer certain
    good actions over others, which equivalently provides a relative ranking of actions
    in the new task.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的相关工作需要*给定的映射函数* [[66](#bib.bib66), [112](#bib.bib112)]。其中一个例子是 [[66](#bib.bib66)]，它假设每个目标状态（动作）在源领域中都有唯一的对应关系，并且在状态空间和动作空间上分别提供了两个映射函数
    $X_{S}$ 和 $X_{A}$，使得 $X_{S}({\mathcal{S}}^{t})\to{\mathcal{S}}^{s}$，$X_{A}({\mathcal{A}}^{t})\to{\mathcal{A}}^{s}$。基于
    $X_{S}$ 和 $X_{A}$，可以相应地推导出一个 $Q$-值的映射函数 $M(Q_{s})\to Q_{t}$。另一项工作是由 [[112](#bib.bib112)]
    完成的，该工作将*建议*作为知识在两个领域之间转移。在他们的设置中，*建议*来自于一个人类专家，该专家提供了源领域中 $Q$-值的映射函数，并将其转移到目标领域的学习策略中。这种建议鼓励学习代理偏好某些优良动作而非其他动作，相当于在新任务中提供了动作的相对排名。
- en: 'More later research tackles the inter-task mapping problem by *learning* a
    mapping function [[113](#bib.bib113), [114](#bib.bib114), [115](#bib.bib115)].
    Most work learns a mapping function over the state space or a subset of the state
    space. In their work, state representations are usually divided into *agent-specific*
    and *task-specific* representations, denoted as $s_{agent}$ and $s_{env}$, respectively.
    In [[113](#bib.bib113)] and [[114](#bib.bib114)], the mapping function is learned
    on the *agent-specific* sub state, and the mapped representation is applied to
    reshape the immediate reward. For [[113](#bib.bib113)], the invariant feature
    space mapped from $s_{agent}$ can be applied across agents who have distinct action
    space but share some morphological similarity. Specifically, they assume that
    both agents have been trained on the same *proxy* task, based on which the mapping
    function is learned. The mapping function is learned using an encoder-decoder
    structure [[116](#bib.bib116)] to largely reserve information about the source
    domain. For transferring knowledge from the source agent to a new task, the environment
    reward is augmented with a shaped reward term to encourage the target agent to
    imitate the source agent on an embedded feature space:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 更晚的研究通过*学习*映射函数来解决任务间映射问题 [[113](#bib.bib113), [114](#bib.bib114), [115](#bib.bib115)]。大多数工作都在状态空间或状态空间的子集上学习映射函数。在这些研究中，状态表示通常被划分为*代理特定*和*任务特定*表示，分别记作
    $s_{agent}$ 和 $s_{env}$。在 [[113](#bib.bib113)] 和 [[114](#bib.bib114)] 中，映射函数在*代理特定*子状态上学习，映射的表示被应用于重塑即时奖励。对于 [[113](#bib.bib113)]，从
    $s_{agent}$ 映射得到的不变特征空间可以应用于具有不同动作空间但具有一定形态相似性的代理。具体来说，他们假设两个代理都已经在相同的*代理*任务上进行过训练，并基于此学习映射函数。映射函数使用编码器-解码器结构 [[116](#bib.bib116)]
    学习，以尽可能保留源领域的信息。为了将知识从源代理转移到新任务，环境奖励会通过一个加权奖励项进行增强，以鼓励目标代理在嵌入特征空间上模仿源代理：
- en: '|  | $\displaystyle r^{\prime}(s,\cdot)=\alpha\left\&#124;f(s^{s}_{agent};\theta_{f})-g(s^{t}_{agent};\theta_{g})\right\&#124;,$
    |  | (12) |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle r^{\prime}(s,\cdot)=\alpha\left\&#124;f(s^{s}_{agent};\theta_{f})-g(s^{t}_{agent};\theta_{g})\right\&#124;,$
    |  | (12) |'
- en: where $f(s^{s}_{agent})$ is the agent-specific state in the source domain, and
    $g(s^{t}_{agent})$ is for the target domain.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f(s^{s}_{agent})$ 是源领域中的代理特定状态，而 $g(s^{t}_{agent})$ 是目标领域中的代理特定状态。
- en: 'Another work is [[115](#bib.bib115)] which applied the Unsupervised Manifold
    Alignment (UMA) method [[117](#bib.bib117)] to automatically learn the state mapping.
    Their approach requires collecting trajectories from both the source and the target
    domain to learn such a mapping. While applying policy gradient learning, trajectories
    from the target domain ${\mathcal{M}}_{t}$ are first mapped back to the source:
    $\tau_{t}\to\tau_{s}$, then an expert policy in the source domain is applied to
    each initial state of those trajectories to generate near-optimal trajectories
    $\overset{\sim}{\tau_{s}}$, which are further mapped to the target domain: $\overset{\sim}{\tau_{s}}\to\overset{\sim}{\tau_{t}}$.
    The deviation between $\overset{\sim}{\tau_{t}}$ and $\tau_{t}$ are used as a
    loss to be minimized in order to improve the target policy. Similar ideas of using
    UMA for inter-task mapping can also be found in [[118](#bib.bib118)] and [[119](#bib.bib119)].'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项工作是[[115](#bib.bib115)]，它应用了无监督流形对齐（UMA）方法[[117](#bib.bib117)]来自动学习状态映射。他们的方法需要从源域和目标域收集轨迹，以学习这种映射。在应用策略梯度学习时，目标域${\mathcal{M}}_{t}$的轨迹首先映射回源域：$\tau_{t}\to\tau_{s}$，然后在源域中应用一个专家策略于这些轨迹的每个初始状态，以生成接近最优的轨迹$\overset{\sim}{\tau_{s}}$，这些轨迹进一步映射到目标域：$\overset{\sim}{\tau_{s}}\to\overset{\sim}{\tau_{t}}$。$\overset{\sim}{\tau_{t}}$与$\tau_{t}$之间的偏差被用作损失，需最小化以改进目标策略。在[[118](#bib.bib118)]和[[119](#bib.bib119)]中也可以找到类似的使用UMA进行任务间映射的想法。
- en: 'In addition to approaches that utilizes mapping over states or actions, [[120](#bib.bib120)]
    proposed to learn an inter-task mapping over the *transition dynamics* space:
    ${\mathcal{S}}\times{\mathcal{A}}\times{\mathcal{S}}$. Their work assumes that
    the source and target domains are different in terms of the transition space dimensionality.
    Transitions from both the source domain $\langle s^{s},a^{s},s^{\prime s}\rangle$
    and the target domain $\langle s^{t},a^{t},s^{\prime t}\rangle$ are mapped to
    a latent space $Z$. Given the latent feature representations, a similarity measure
    can be applied to find a correspondence between the source and target task triplets.
    Triplet pairs with the highest similarity in this feature space $Z$ are used to
    learn a mapping function ${\mathcal{X}}$: $\langle s^{t},a^{t},s^{\prime t}\rangle={\mathcal{X}}(\langle
    s^{s},a^{s},s^{\prime s}\rangle)$. After the transition mapping, states sampled
    from the expert policy in the source domain can be leveraged to render beneficial
    states in the target domain, which assists the target agent learning with a better
    initialization performance. A similar idea of mapping transition dynamics can
    be found in [[121](#bib.bib121)], which, however, requires a stronger assumption
    on the similarity of the transition probability and the state representations
    between the source and the target domains.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 除了利用状态或动作映射的方法外，[[120](#bib.bib120)]提出了在*转移动态*空间：${\mathcal{S}}\times{\mathcal{A}}\times{\mathcal{S}}$上学习任务间映射。他们的工作假设源域和目标域在转移空间维度上存在差异。源域$\langle
    s^{s},a^{s},s^{\prime s}\rangle$和目标域$\langle s^{t},a^{t},s^{\prime t}\rangle$的转移被映射到潜在空间$Z$。给定潜在特征表示后，可以应用相似度度量来寻找源任务和目标任务三元组之间的对应关系。在这个特征空间$Z$中相似度最高的三元组对用于学习映射函数${\mathcal{X}}$：$\langle
    s^{t},a^{t},s^{\prime t}\rangle={\mathcal{X}}(\langle s^{s},a^{s},s^{\prime s}\rangle)$。经过转移映射后，可以利用源域中的专家策略采样的状态来在目标域中渲染有益状态，这有助于目标代理以更好的初始化性能进行学习。在[[121](#bib.bib121)]中可以找到类似的转移动态映射的想法，但该方法要求源域和目标域之间转移概率和状态表示的相似性有更强的假设。
- en: 'As summarized in Table [IV](#S5.T4 "TABLE IV ‣ 5.4 Inter-Task Mapping ‣ 5 Transfer
    Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning:
    A Survey"), for TL approaches that utilize an inter-task mapping, the mapped knowledge
    can be (a subset of) the state space [[113](#bib.bib113), [114](#bib.bib114)],
    the $Q$-function [[66](#bib.bib66)], or (representations of) the state-action-sate
    transitions  [[120](#bib.bib120)]. In addition to being directly applicable in
    the target domain [[120](#bib.bib120)], the mapped representation can also be
    used as an augmented shaping reward [[114](#bib.bib114), [113](#bib.bib113)] or
    a loss objective [[115](#bib.bib115)] in order to guide the agent learning in
    the target domain. Most inter-task mapping methods tackle domains with moderate
    state-action space dimensions, such as maze tasks or tabular MDPs, where the goal
    can be reaching a target state with a minimal number of transitions. Accordingly,
    *tt* has been used to measure TL performance. For tasks with limited and discrete
    state-action space, evaluation is also conducted with different number of initial
    states collected in the target domain (*nka*).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[IV](#S5.T4 "TABLE IV ‣ 5.4 Inter-Task Mapping ‣ 5 Transfer Learning Approaches
    Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")所总结，对于利用任务间映射的迁移学习方法，映射的知识可以是（一个子集）状态空间
    [[113](#bib.bib113), [114](#bib.bib114)]，$Q$-函数 [[66](#bib.bib66)]，或者（状态-动作-状态转移的）表示
    [[120](#bib.bib120)]。除了可以直接应用于目标领域 [[120](#bib.bib120)]，映射的表示还可以用作增强的奖励塑形 [[114](#bib.bib114),
    [113](#bib.bib113)] 或者损失目标 [[115](#bib.bib115)]，以指导在目标领域中的代理学习。大多数任务间映射方法处理的是具有适中状态-动作空间维度的领域，如迷宫任务或表格MDP，其中目标可能是以最小的转移次数到达目标状态。因此，*tt*
    被用来衡量迁移学习性能。对于状态-动作空间有限且离散的任务，也会使用不同数量的初始状态进行评估（*nka*）。'
- en: '| Methods |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |'
- en: '&#124; RL &#124;'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 强化学习 &#124;'
- en: '&#124; framework &#124;'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 框架 &#124;'
- en: '|'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MDP &#124;'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MDP &#124;'
- en: '&#124; difference &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 差异 &#124;'
- en: '|'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Mapping &#124;'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 映射 &#124;'
- en: '&#124; function &#124;'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 功能 &#124;'
- en: '| Usage of mapping |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 映射的使用 |'
- en: '&#124; Evaluation &#124;'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估 &#124;'
- en: '&#124; metrics &#124;'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 指标 &#124;'
- en: '|'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| [[66](#bib.bib66)] | *SARSA* | ${\mathcal{S}}_{t}\neq{\mathcal{S}}_{t},{\mathcal{A}}_{s}\neq{\mathcal{A}}_{t}$
    | $M(Q_{s})$ $\to$ $Q_{t}$ | $Q$ value reuse | *ap, ar, tt, tr* |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| [[66](#bib.bib66)] | *SARSA* | ${\mathcal{S}}_{t}\neq{\mathcal{S}}_{t},{\mathcal{A}}_{s}\neq{\mathcal{A}}_{t}$
    | $M(Q_{s})$ $\to$ $Q_{t}$ | $Q$ 值重用 | *ap, ar, tt, tr* |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| [[112](#bib.bib112)] | *Q-learning* | ${\mathcal{A}}_{s}\neq{\mathcal{A}}_{t}$,
    ${\mathcal{R}}_{s}\neq{\mathcal{R}}_{t}$ | $M(Q_{s})$ $\to$ *advice* | Relative
    $Q$ ranking | *ap, ar, tr* |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| [[112](#bib.bib112)] | *Q-learning* | ${\mathcal{A}}_{s}\neq{\mathcal{A}}_{t}$,
    ${\mathcal{R}}_{s}\neq{\mathcal{R}}_{t}$ | $M(Q_{s})$ $\to$ *advice* | 相对 $Q$
    排名 | *ap, ar, tr* |'
- en: '| [[113](#bib.bib113)] | $-$ | ${\mathcal{S}}_{s}\neq{\mathcal{S}}_{t}$ | $M(s_{t})$
    $\to$ $r^{\prime}$ | Reward shaping | *ap, ar, pe, tr* |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| [[113](#bib.bib113)] | $-$ | ${\mathcal{S}}_{s}\neq{\mathcal{S}}_{t}$ | $M(s_{t})$
    $\to$ $r^{\prime}$ | 奖励塑形 | *ap, ar, pe, tr* |'
- en: '| [[114](#bib.bib114)] | *SARSA*$(\lambda)$ | ${\mathcal{S}}_{s}\neq{\mathcal{S}}_{t}$
    ${\mathcal{R}}_{s}\neq{\mathcal{R}}_{t}$ | $M(s_{t})$ $\to$ $r^{\prime}$ | Reward
    shaping | *ap, ar, pe, tt* |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| [[114](#bib.bib114)] | *SARSA*$(\lambda)$ | ${\mathcal{S}}_{s}\neq{\mathcal{S}}_{t}$
    ${\mathcal{R}}_{s}\neq{\mathcal{R}}_{t}$ | $M(s_{t})$ $\to$ $r^{\prime}$ | 奖励塑形
    | *ap, ar, pe, tt* |'
- en: '| [[115](#bib.bib115)] | Fitted Value Iteration | ${\mathcal{S}}_{s}\neq{\mathcal{S}}_{t}$
    | $M(s_{s})$ $\to$ $s_{t}$ | Penalty loss on state deviation from expert policy
    | *ap, ar, pe, tr* |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| [[115](#bib.bib115)] | 拟合值迭代 | ${\mathcal{S}}_{s}\neq{\mathcal{S}}_{t}$ |
    $M(s_{s})$ $\to$ $s_{t}$ | 针对状态偏离专家策略的惩罚损失 | *ap, ar, pe, tr* |'
- en: '| [[121](#bib.bib121)] | Fitted Q Iteration | ${\mathcal{S}}_{s}\times{\mathcal{A}}_{s}\neq{\mathcal{S}}_{t}\times{\mathcal{A}}_{t}$
    | $M\big{(}(s_{s},a_{s},s^{\prime}_{s})$ $\to$ $(s_{t},a_{t},s^{\prime}_{t})\big{)}$
    | Reduce random exploration | *ap, ar, pe, tr, nta* |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| [[121](#bib.bib121)] | 拟合 $Q$ 迭代 | ${\mathcal{S}}_{s}\times{\mathcal{A}}_{s}\neq{\mathcal{S}}_{t}\times{\mathcal{A}}_{t}$
    | $M\big{(}(s_{s},a_{s},s^{\prime}_{s})$ $\to$ $(s_{t},a_{t},s^{\prime}_{t})\big{)}$
    | 减少随机探索 | *ap, ar, pe, tr, nta* |'
- en: '| [[120](#bib.bib120)] | $-$ | ${\mathcal{S}}_{s}\times{\mathcal{A}}_{s}\neq{\mathcal{S}}_{t}\times{\mathcal{A}}_{t}$
    | $M\big{(}(s_{s},a_{s},s^{\prime}_{s})$ $\to$ $(s_{t},a_{t},s^{\prime}_{t})\big{)}$
    | Reduce random exploration | *ap, ar, pe, tr, nta* |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| [[120](#bib.bib120)] | $-$ | ${\mathcal{S}}_{s}\times{\mathcal{A}}_{s}\neq{\mathcal{S}}_{t}\times{\mathcal{A}}_{t}$
    | $M\big{(}(s_{s},a_{s},s^{\prime}_{s})$ $\to$ $(s_{t},a_{t},s^{\prime}_{t})\big{)}$
    | 减少随机探索 | *ap, ar, pe, tr, nta* |'
- en: 'TABLE IV: A comparison of inter-task mapping approaches. “$-$” indicates no
    RL framework constraints.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：任务间映射方法的比较。“$-$” 表示没有 RL 框架限制。
- en: 5.5 Representation Transfer
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 表示迁移
- en: 'This section review approaches that transfer knowledge in the form of representations
    learned by deep neural networks. They are built upon the following consensual
    assumption:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了以深度神经网络学习的表示形式转移知识的方法。它们建立在以下共识假设之上：
- en: Assumption.
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设。
- en: '*[Existence of Task-Invariance Subspace]*'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*[任务不变性子空间的存在]*'
- en: The state space (${\mathcal{S}}$), action space (${\mathcal{A}}$), or the reward
    space (${\mathcal{R}}$) can be disentangled into orthogonal subspaces, which are
    task-invariant such that knowledge can be transferred between domains on the universal
    subspace.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间 (${\mathcal{S}}$)、动作空间 (${\mathcal{A}}$) 或奖励空间 (${\mathcal{R}}$) 可以被解耦成正交子空间，这些子空间是任务不变的，因此知识可以在通用子空间中在领域之间转移。
- en: 'We organize recent work along this line into two subtopics: 1) approaches that
    directly *reuse* representations from the source domain (Section [5.5.1](#S5.SS5.SSS1
    "5.5.1 Reusing Representations ‣ 5.5 Representation Transfer ‣ 5 Transfer Learning
    Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")),
    and 2) approaches that learn to *disentangle* the source domain representations
    into independent sub-feature representations, some of which are on the universal
    feature space shared by both the source and the target domains (Section [5.5.2](#S5.SS5.SSS2
    "5.5.2 Disentangling Representations ‣ 5.5 Representation Transfer ‣ 5 Transfer
    Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning:
    A Survey")).'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将这方面的近期工作组织成两个子主题：1）直接 *重用* 源域中的表示的方法（第 [5.5.1](#S5.SS5.SSS1 "5.5.1 Reusing
    Representations ‣ 5.5 Representation Transfer ‣ 5 Transfer Learning Approaches
    Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey") 节），以及
    2）将源域表示学习成独立子特征表示的方法，其中一些表示在源域和目标域共享的通用特征空间上（第 [5.5.2](#S5.SS5.SSS2 "5.5.2 Disentangling
    Representations ‣ 5.5 Representation Transfer ‣ 5 Transfer Learning Approaches
    Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey") 节）。'
- en: 5.5.1 Reusing Representations
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1 重用表示
- en: A representative work of reusing representations is [[122](#bib.bib122)], which
    proposed the *progressive neural network* structure to enable knowledge transfer
    across multiple RL tasks in a progressive way. A progressive network is composed
    of multiple *columns*, and each column is a policy network for one specific task.
    It starts with one single column for training the first task, and then the number
    of columns increases with the number of new tasks. While training on a new task,
    neuron weights on the previous columns are frozen, and representations from those
    frozen tasks are applied to the new column via a collateral connection to assist
    in learning the new task.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重用表示的代表性工作是 [[122](#bib.bib122)]，该工作提出了 *渐进神经网络* 结构，以便以渐进的方式在多个 RL 任务之间实现知识转移。一个渐进网络由多个
    *列* 组成，每一列是一个针对特定任务的策略网络。它从一个单一列开始训练第一个任务，然后列的数量随着新任务的增加而增加。在训练新任务时，之前列上的神经元权重保持冻结，来自那些冻结任务的表示通过旁路连接应用到新列，以帮助学习新任务。
- en: '*Progressive network* comes with a cost of large network structures, as the
    network grows proportionally with the number of incoming tasks. A later framework
    called *PathNet* alleviates this issue by learning a network with a fixed size [[123](#bib.bib123)].
    *PathNet* contains *pathways*, which are subsets of neurons whose weights contain
    the knowledge of previous tasks and are frozen during training on new tasks. The
    population of *pathway* is evolved using a tournament selection genetic algorithm [[124](#bib.bib124)].'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '*渐进网络* 伴随着大规模网络结构的成本，因为网络会随着输入任务数量的增加而成比例增长。稍后的框架叫做 *PathNet*，通过学习一个固定大小的网络来缓解这个问题 [[123](#bib.bib123)]。*PathNet*
    包含 *路径*，这些路径是神经元的子集，其权重包含了先前任务的知识，并且在训练新任务时保持冻结。*路径* 的种群通过一种锦标赛选择遗传算法来进化 [[124](#bib.bib124)]。'
- en: 'Another approach of reusing representations for TL is modular networks [[52](#bib.bib52),
    [53](#bib.bib53), [125](#bib.bib125)]. For example, [[52](#bib.bib52)] proposed
    to decompose the policy network into a task-specific module and agent-specific
    module. Specifically, let $\pi$ be a policy performed by any agent (robot) $r$
    over the task ${\mathcal{M}}_{k}$ as a function $\bm{\phi}$ over states $s$, it
    can be decomposed into two sub-modules $g_{k}$ and $f_{r}$, i.e.:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种重用表示的方法是模块化网络 [[52](#bib.bib52), [53](#bib.bib53), [125](#bib.bib125)]。例如，[[52](#bib.bib52)]
    提出了将策略网络分解为任务特定模块和代理特定模块。具体地，设 $\pi$ 是任何代理（机器人） $r$ 在任务 ${\mathcal{M}}_{k}$ 上执行的策略，作为状态
    $s$ 上的函数 $\bm{\phi}$，它可以分解为两个子模块 $g_{k}$ 和 $f_{r}$，即：
- en: '|  | $\displaystyle\pi(s):=\bm{\phi}(s_{env},s_{agent})=f_{r}(g_{k}(s_{env}),s_{agent}),$
    |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\pi(s):=\bm{\phi}(s_{env},s_{agent})=f_{r}(g_{k}(s_{env}),s_{agent}),$
    |  |'
- en: where $f_{r}$ is the agent-specific module and $g_{k}$ is the task-specific
    module. Their core idea is that the task-specific module can be applied to different
    agents performing the same task, which serves as a transferred knowledge. Accordingly,
    the agent-specific module can be applied to different tasks for the same agent.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f_{r}$ 是代理特定模块，而 $g_{k}$ 是任务特定模块。他们的核心思想是任务特定模块可以应用于执行相同任务的不同代理，这就作为转移知识。因此，代理特定模块可以应用于同一代理的不同任务。
- en: A model-based approach along this line is [[125](#bib.bib125)], which learns
    a model to map the state observation $s$ to a latent-representation $z$. The transition
    probability is modeled on the latent space instead of the original state space,
    i.e. $\hat{z}_{t+1}=f_{\theta}(z_{t},a_{t})$, where $\theta$ is the parameter
    of the transition model, $z_{t}$ is the latent-representation of the state observation,
    and $a_{t}$ is the action accompanying that state. Next, a *reward* module learns
    the value function as well as the policy from the latent space $z$ using an actor-critic
    framework. One potential benefit of this latent representation is that knowledge
    can be transferred across tasks that have different rewards but share the same
    transition dynamics.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着这个方向的模型基方法是 [[125](#bib.bib125)]，它学习一个模型来将状态观察 $s$ 映射到潜在表示 $z$。转移概率在潜在空间中建模，而不是在原始状态空间中，即
    $\hat{z}_{t+1}=f_{\theta}(z_{t},a_{t})$，其中 $\theta$ 是转移模型的参数，$z_{t}$ 是状态观察的潜在表示，$a_{t}$
    是与该状态相关的动作。接下来，一个 *奖励* 模块在潜在空间 $z$ 中使用演员-评论家框架学习价值函数和策略。这种潜在表示的一个潜在好处是知识可以在具有不同奖励但共享相同转移动态的任务之间转移。
- en: 5.5.2 Disentangling Representations
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.2 解开表示
- en: 'Methods discussed in this section mostly focus on learning a *disentangled*
    representation. Specifically, we elaborate on TL approaches that are derived from
    two techniques: *Successor Representation (SR)* and *Universal Value Function
    Approximating (UVFA)*.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论的方法主要集中在学习 *解缠* 表示。具体而言，我们详细介绍了源自两种技术的 TL 方法：*继任表示（SR）* 和 *通用价值函数逼近（UVFA）*。
- en: 'Successor Representations (SR) is an approach to decouple the state features
    of a domain from its reward distributions. It enables knowledge transfer across
    multiple domains: $\bm{{\mathcal{M}}}=\{{\mathcal{M}}_{1},{\mathcal{M}}_{2},\dots,{\mathcal{M}}_{K}\}$,
    so long as the only difference among them is the reward distributions: ${\mathcal{R}}_{i}\neq{\mathcal{R}}_{j}$.
    SR was originally derived from neuroscience, until [[126](#bib.bib126)] proposed
    to leverage it as a generalization mechanism for state representations in the
    RL domain.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 继任表示（SR）是一种将领域的状态特征与奖励分布解耦的方法。它使得知识可以在多个领域之间转移：$\bm{{\mathcal{M}}}=\{{\mathcal{M}}_{1},{\mathcal{M}}_{2},\dots,{\mathcal{M}}_{K}\}$，只要它们之间的唯一区别是奖励分布：${\mathcal{R}}_{i}\neq{\mathcal{R}}_{j}$。SR
    最初源于神经科学，直到 [[126](#bib.bib126)] 提出了将其作为 RL 领域中状态表示的泛化机制。
- en: 'Different from the $v$-value or $Q$-value that describes states as dependent
    on the reward function, SR features a state based on the *occupancy measure* of
    its *successor* states. Specifically, SR decomposes the value function of any
    policy into two independent components, ${\bm{\psi}}$ and $R$: $V^{\pi}(s)=\sum_{s^{\prime}}{\bm{\psi}}(s,s^{\prime}){\mathbf{w}}(s^{\prime})$,
    where ${\mathbf{w}}(s^{\prime})$ is a reward mapping function that maps states
    to scalar rewards, and ${\bm{\psi}}$ is the SR which describes any state $s$ as
    the occupancy measure of the future occurred states when following $\pi$, with
    ${\mathbbm{1}}[S=s^{\prime}]=1$ as an indicator function:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于描述状态依赖于奖励函数的 $v$-值或 $Q$-值，SR 通过其*后继*状态的*占用测度*来定义状态。具体来说，SR 将任何策略的价值函数分解为两个独立的组件，${\bm{\psi}}$
    和 $R$：$V^{\pi}(s)=\sum_{s^{\prime}}{\bm{\psi}}(s,s^{\prime}){\mathbf{w}}(s^{\prime})$，其中
    ${\mathbf{w}}(s^{\prime})$ 是一个奖励映射函数，将状态映射到标量奖励，而 ${\bm{\psi}}$ 是 SR，它将任何状态 $s$
    描述为在遵循 $\pi$ 时未来发生状态的占用测度，其中 ${\mathbbm{1}}[S=s^{\prime}]=1$ 作为指示函数：
- en: '|  | $\displaystyle{\bm{\psi}}(s,s^{\prime})={\mathds{E}}_{\pi}[\sum_{i=t}^{\infty}\gamma^{i-t}{\mathbbm{1}}[S_{i}=s^{\prime}]&#124;S_{t}=s].$
    |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bm{\psi}}(s,s^{\prime})={\mathds{E}}_{\pi}[\sum_{i=t}^{\infty}\gamma^{i-t}{\mathbbm{1}}[S_{i}=s^{\prime}]&#124;S_{t}=s].$
    |  |'
- en: 'The *successor* nature of SR makes it learnable using any TD-learning algorithms.
    Especially, [[126](#bib.bib126)] proved the feasibility of learning such representation
    in a tabular case, in which the state transitions can be described using a matrix.
    SR was later extended by [[110](#bib.bib110)] from three perspectives: (i) the
    feature domain of SR is extended from states to state-action pairs; (ii) deep
    neural networks are used as function approximators to represent the SR ${\bm{\psi}}^{\pi}(s,a)$
    and the *reward mapper* ${\mathbf{w}}$; (iii) Generalized policy improvement (GPI)
    algorithm is introduced to accelerate policy transfer for multi-tasks (Section [5.3.2](#S5.SS3.SSS2
    "5.3.2 Transfer Learning via Policy Reuse ‣ 5.3 Policy Transfer ‣ 5 Transfer Learning
    Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")).
    These extensions, however, are built upon a stronger assumption about the MDP:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 'SR 的*后继*特性使其可以使用任何 TD 学习算法进行学习。特别是，[[126](#bib.bib126)] 证明了在表格情况下学习这种表示的可行性，其中状态转移可以用矩阵描述。SR
    后来由 [[110](#bib.bib110)] 从三个方面进行了扩展：(i) SR 的特征领域从状态扩展到状态-动作对；(ii) 使用深度神经网络作为函数近似器来表示
    SR ${\bm{\psi}}^{\pi}(s,a)$ 和*奖励映射器* ${\mathbf{w}}$；(iii) 引入了广义策略改进 (GPI) 算法来加速多任务的策略转移（第[5.3.2](#S5.SS3.SSS2
    "5.3.2 Transfer Learning via Policy Reuse ‣ 5.3 Policy Transfer ‣ 5 Transfer Learning
    Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")节）。然而，这些扩展是建立在对
    MDP 更强假设的基础上的。'
- en: Assumption.
  id: totrans-277
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设。
- en: '*[Linearity of Reward Distributions]* The reward functions of all tasks can
    be computed as a linear combination of a fixed set of features: $r(s,a,s^{\prime})=\phi(s,a,s^{\prime})^{\top}{\mathbf{w}}$,
    where $\phi(s,a,s^{\prime})\in\mathbb{R}^{d}$ denotes the latent representation
    of the state transition, and ${\mathbf{w}}\in\mathbb{R}^{d}$ is the task-specific
    reward mapper.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '*[奖励分布的线性]* 所有任务的奖励函数可以作为固定特征集合的线性组合来计算：$r(s,a,s^{\prime})=\phi(s,a,s^{\prime})^{\top}{\mathbf{w}}$，其中
    $\phi(s,a,s^{\prime})\in\mathbb{R}^{d}$ 表示状态转移的潜在表示，${\mathbf{w}}\in\mathbb{R}^{d}$
    是任务特定的奖励映射器。'
- en: 'Based on this assumption, SR can be decoupled from the rewards when evaluating
    the Q-function of any policy $\pi$ in a task. The advantage of SR is that, when
    the knowledge of ${\bm{\psi}}^{\pi}(s,a)$ in the source domain ${\mathcal{M}}_{s}$
    is observed, one can quickly get the performance evaluation of the same policy
    in the target domain ${\mathcal{M}}_{t}$ by replacing ${\mathbf{w}}_{s}$ with
    ${\mathbf{w}}_{t}$: $Q^{\pi}_{{\mathcal{M}}_{t}}={\bm{\psi}}^{\pi}(s,a){\mathbf{w}}_{t}.$
    Similar ideas of learning SR as a TD-algorithm on a latent representation $\phi(s,a,s^{\prime})$
    can also be found in [[127](#bib.bib127), [128](#bib.bib128)]. Specifically, the
    work of [[127](#bib.bib127)] was developed based on a weaker assumption about
    the reward function: Instead of requiring linearly-decoupled rewards, the latent
    space $\phi(s,a,s^{\prime})$ is learned in an encoder-decoder structure to ensure
    that the information loss is minimized when mapping states to the latent space.
    This structure, therefore, comes with an extra cost of learning a decoder $f_{d}$
    to reconstruct the state: $f_{d}(\phi(s_{t}))\approx s_{t}$.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个假设，SR可以在评估任务中任何策略$\pi$的Q函数时与奖励解耦。SR的优势在于，当源领域${\mathcal{M}}_{s}$中观察到${\bm{\psi}}^{\pi}(s,a)$的知识时，可以通过将${\mathbf{w}}_{s}$替换为${\mathbf{w}}_{t}$，迅速获得同一策略在目标领域${\mathcal{M}}_{t}$中的性能评估：$Q^{\pi}_{{\mathcal{M}}_{t}}={\bm{\psi}}^{\pi}(s,a){\mathbf{w}}_{t}$。类似的将SR作为潜在表示$\phi(s,a,s^{\prime})$上的TD算法学习的思想也可以在[[127](#bib.bib127)、[128](#bib.bib128)]中找到。具体来说，[[127](#bib.bib127)]的工作基于关于奖励函数的较弱假设进行开发：它没有要求奖励线性解耦，而是学习一个编码器-解码器结构中的潜在空间$\phi(s,a,s^{\prime})$，以确保在将状态映射到潜在空间时最小化信息损失。因此，这个结构带来了额外的学习解码器$f_{d}$以重建状态的成本：$f_{d}(\phi(s_{t}))\approx
    s_{t}$。
- en: 'An intriguing question faced by the SR approach is: *Is there a way that evades
    the linearity assumption about reward functions and still enables learning the
    SR without extra modular cost?* An extended work of SR [[67](#bib.bib67)] answered
    this question affirmatively, which proved that the reward functions does not necessarily
    have to follow the linear structure, yet at the cost of a looser performance lower-bound
    while applying the GPI approach for policy improvement. Especially, rather than
    learning a reward-agnostic latent feature $\phi(s,a,s^{\prime})\in\mathbb{R}^{d}$
    for multiple tasks, [[67](#bib.bib67)] aims to learn a matrix ${\bm{\phi}}(s,a,s^{\prime})\in\mathbb{R}^{D\times
    d}$ to interpret the basis functions of the latent space instead, where $D$ is
    the number of seen tasks. Assuming $k$ out of $D$ tasks are linearly independent,
    this matrix forms $k$ basis functions for the latent space. Therefore, for any
    unseen task ${\mathcal{M}}_{i}$, its latent features can be built as a linear
    combination of these basis functions, as well as its reward functions $r_{i}(s,a,s^{\prime})$.
    Based on the idea of basis-functions for a task’s latent space, they proposed
    that ${\bm{\phi}}(s,a,s^{\prime})$ can be approximated as learning $\mathbb{R}(s,a,s^{\prime})$
    directly, where $\mathbb{R}(s,a,s^{\prime})\in\mathbb{R}^{D}$ is a vector of reward
    functions for each seen task:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: SR方法面临的一个有趣问题是：*是否存在一种方法，能够避免关于奖励函数的线性假设，同时仍能在不增加额外模块成本的情况下学习SR？* SR的扩展工作[[67](#bib.bib67)]肯定地回答了这个问题，证明了奖励函数不一定需要遵循线性结构，但在应用GPI方法进行策略改进时代价是性能下界较松。特别是，[[67](#bib.bib67)]并不是为多个任务学习一个与奖励无关的潜在特征$\phi(s,a,s^{\prime})\in\mathbb{R}^{d}$，而是旨在学习一个矩阵${\bm{\phi}}(s,a,s^{\prime})\in\mathbb{R}^{D\times
    d}$，以解释潜在空间的基函数，其中$D$是已见任务的数量。假设$D$个任务中的$k$个是线性独立的，这个矩阵形成了潜在空间的$k$个基函数。因此，对于任何未见任务${\mathcal{M}}_{i}$，其潜在特征可以作为这些基函数的线性组合来构建，其奖励函数$r_{i}(s,a,s^{\prime})$也是如此。基于任务潜在空间的基函数的思想，他们提出${\bm{\phi}}(s,a,s^{\prime})$可以近似为直接学习$\mathbb{R}(s,a,s^{\prime})$，其中$\mathbb{R}(s,a,s^{\prime})\in\mathbb{R}^{D}$是每个已见任务的奖励函数向量：
- en: '|  | $\mathbb{R}(s,a,s^{\prime})=\big{[}r_{1}(s,a,s^{\prime});r_{2}(s,a,s^{\prime}),\dots,r_{D}(s,a,s^{\prime})\big{]}.$
    |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{R}(s,a,s^{\prime})=\big{[}r_{1}(s,a,s^{\prime});r_{2}(s,a,s^{\prime}),\dots,r_{D}(s,a,s^{\prime})\big{]}.$
    |  |'
- en: 'Accordingly, learning ${\bm{\psi}}(s,a)$ for any policy $\pi_{i}$ in ${\mathcal{M}}_{i}$
    becomes equivalent to learning a collection of Q-functions:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在${\mathcal{M}}_{i}$中学习任何策略$\pi_{i}$的${\bm{\psi}}(s,a)$就等同于学习一组Q函数：
- en: '|  | ${\psi}^{\pi_{i}}(s,a)=\big{[}Q^{\pi_{i}}_{1}(s,a),Q^{\pi_{i}}_{2}(s,a),\dots,Q^{\pi_{i}}_{D}(s,a)\big{]}.$
    |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\psi}^{\pi_{i}}(s,a)=\big{[}Q^{\pi_{i}}_{1}(s,a),Q^{\pi_{i}}_{2}(s,a),\dots,Q^{\pi_{i}}_{D}(s,a)\big{]}.$
    |  |'
- en: A similar idea of using reward functions as features to represent unseen tasks
    is also proposed by [[129](#bib.bib129)], which assumes the ${\bm{\psi}}$ and
    ${\mathbf{w}}$ as observable quantities from the environment.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 使用奖励函数作为特征来表示未见任务的类似想法也被提出了[[129](#bib.bib129)]，它假设 ${\bm{\psi}}$ 和 ${\mathbf{w}}$
    是从环境中可观测的量。
- en: 'Universal Function Approximation (UVFA) is an alternative approach of learning
    disentangled state representations [[64](#bib.bib64)]. Same as SR, UVFA allows
    TL for multiple tasks which differ only by their reward functions (goals). Different
    from SR which focuses on learning a reward-agnostic state representation, UVFA
    aims to find a function approximator that is generalized for both states and goals.
    The UVFA framework is built on a specific problem setting of *goal conditional
    RL*: *task goals are defined in terms of states, e.g. given the state space ${\mathcal{S}}$
    and the goal space ${\mathcal{G}}$, it satisfies that ${\mathcal{G}}\subseteq{\mathcal{S}}$.*
    One instantiation of this problem setting can be an agent exploring different
    locations in a maze, where the goals are described as certain locations inside
    the maze. Under this problem setting, a UVFA module can be decoupled into a state
    embedding $\phi(s)$ and a goal embedding ${\bm{\psi}}(g)$, by applying the technique
    of matrix factorization to a reward matrix describing the goal-conditional task.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 通用函数逼近（UVFA）是学习解耦状态表示的另一种方法[[64](#bib.bib64)]。与 SR 相同，UVFA 允许对仅通过奖励函数（目标）不同的多个任务进行迁移学习。不同于
    SR 关注学习无奖励状态表示，UVFA 旨在找到一个对状态和目标都能泛化的函数逼近器。UVFA 框架建立在*目标条件 RL*的特定问题设置上：*任务目标以状态为定义，例如，给定状态空间
    ${\mathcal{S}}$ 和目标空间 ${\mathcal{G}}$，满足 ${\mathcal{G}}\subseteq{\mathcal{S}}$*。这种问题设置的一个实例可以是一个代理在迷宫中探索不同的位置，其中目标被描述为迷宫内部的特定位置。在这种问题设置下，UVFA
    模块可以通过将矩阵分解技术应用于描述目标条件任务的奖励矩阵，将其解耦为状态嵌入 $\phi(s)$ 和目标嵌入 ${\bm{\psi}}(g)$。
- en: One merit of UVFA resides in its transferrable embedding $\phi(s)$ across tasks
    which only differ by goals. Another benefit is its ability of continual learning
    when the set of goals keeps expanding over time. On the other hand, a key challenge
    of UVFA is that applying the matrix factorization is time-consuming, which makes
    it a practical concern for complex environments with large state space $|{\mathcal{S}}|$.
    Even with the learned embedding networks, the third stage of fine-tuning these
    networks via end-to-end training is still necessary.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: UVFA 的一个优点在于其在仅通过目标不同的任务之间的可迁移嵌入 $\phi(s)$。另一个好处是它在目标集随时间不断扩展时的持续学习能力。另一方面，UVFA
    的一个关键挑战是应用矩阵分解耗时，这使得它在具有大状态空间 $|{\mathcal{S}}|$ 的复杂环境中成为实际问题。即使有了学习到的嵌入网络，通过端到端训练来微调这些网络的第三阶段仍然是必要的。
- en: 'UVFA has been connected to SR by [[67](#bib.bib67)], in which a set of independent
    rewards (tasks) themselves can be used as features for state representations.
    Another extended work that combines UVFA with SR is called Universal Successor
    Feature Approximator (USFA), which is proposed by [[130](#bib.bib130)]. Following
    the same linearity assumption, USFA is proposed as a function over a triplet of
    the state, action, and a policy embedding $z$: $\phi(s,a,z):{\mathcal{S}}\times{\mathcal{A}}\times\mathbb{R}^{k}\rightarrow\mathbb{R}^{d}$,
    where $z$ is the output of a *policy-encoding mapping* $z=e(\pi):{\mathcal{S}}\times{\mathcal{A}}\rightarrow\mathbb{R}^{k}$.
    Based on USFA, the $Q$-function of any policy $\pi$ for a task specified by ${\mathbf{w}}$
    can be formularized as the product of a reward-agnostic Universal Successor Feature
    (USF) ${\bm{\psi}}$ and a reward mapper ${\mathbf{w}}$: $Q(s,a,{\mathbf{w}},z)={\bm{\psi}}(s,a,z)^{\top}{\mathbf{w}}.$
    Facilitated by the disentangled rewards and policy generalization, [[130](#bib.bib130)]
    further introduced a generalized TD-error as a function over tasks ${\mathbf{w}}$
    and policy $z$, which allows them to approximate the $Q$-function of any policy
    on any task using a TD-algorithm.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: UVFA 已通过 [[67](#bib.bib67)] 与 SR 相关联，其中一组独立的奖励（任务）本身可以用作状态表示的特征。另一项将 UVFA 与
    SR 结合的扩展工作称为通用后继特征逼近器（USFA），由 [[130](#bib.bib130)] 提出。按照相同的线性假设，USFA 被提议为一个作用于状态、动作和策略嵌入
    $z$ 的三元组的函数：$\phi(s,a,z):{\mathcal{S}}\times{\mathcal{A}}\times\mathbb{R}^{k}\rightarrow\mathbb{R}^{d}$，其中
    $z$ 是 *策略编码映射* $z=e(\pi):{\mathcal{S}}\times{\mathcal{A}}\rightarrow\mathbb{R}^{k}$
    的输出。基于 USFA，任何策略 $\pi$ 在由 ${\mathbf{w}}$ 指定的任务上的 $Q$-函数可以被公式化为与奖励无关的通用后继特征（USF）
    ${\bm{\psi}}$ 和奖励映射器 ${\mathbf{w}}$ 的乘积：$Q(s,a,{\mathbf{w}},z)={\bm{\psi}}(s,a,z)^{\top}{\mathbf{w}}.$
    在解耦的奖励和策略泛化的促进下，[[130](#bib.bib130)] 进一步引入了作为任务 ${\mathbf{w}}$ 和策略 $z$ 函数的广义 TD-误差，使得他们可以使用
    TD 算法来逼近任何任务上任何策略的 $Q$-函数。
- en: 5.5.3 Summary and Discussion
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.3 总结与讨论
- en: 'We provide a summary of the discussed work in this section in Table [V](#S5.T5
    "TABLE V ‣ 5.5.3 Summary and Discussion ‣ 5.5 Representation Transfer ‣ 5 Transfer
    Learning Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning:
    A Survey"). Representation transfer can facilitate TL in multiple ways based on
    assumptions about certain task-invariant property. Some assume that tasks are
    different only in terms of their reward distributions. Other stronger assumptions
    include (i) decoupled dynamics, rewards [[110](#bib.bib110)], or policies [[130](#bib.bib130)]
    from the $Q$-function representations, and (ii) the feasibility of defining tasks
    in terms of states [[130](#bib.bib130)]. Based on those assumptions, approaches
    such as TD-algorithms [[67](#bib.bib67)] or matrix-factorization [[64](#bib.bib64)]
    become applicable to learn such disentangled representations. To further exploit
    the effectiveness of disentangled structure, we consider that *generalization*
    approaches, which allow changing dynamics or state distributions, are important
    future work that is worth more attention in this domain.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在本节的表 [V](#S5.T5 "TABLE V ‣ 5.5.3 Summary and Discussion ‣ 5.5 Representation
    Transfer ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer Learning in Deep
    Reinforcement Learning: A Survey") 中提供了所讨论工作的总结。基于某些任务不变属性的假设，表示转移可以以多种方式促进 TL。一些假设任务仅在其奖励分布上有所不同。其他更强的假设包括（i）将动态、奖励
    [[110](#bib.bib110)] 或策略 [[130](#bib.bib130)] 从 $Q$-函数表示中解耦，以及（ii）将任务定义为状态 [[130](#bib.bib130)]
    的可行性。基于这些假设，TD 算法 [[67](#bib.bib67)] 或矩阵分解 [[64](#bib.bib64)] 等方法可以应用于学习这些解耦的表示。为了进一步利用解耦结构的有效性，我们认为
    *泛化* 方法，即允许动态或状态分布变化的技术，是未来值得更多关注的重要工作。'
- en: Most discussed work in this section tackles multi-task RL or meta-RL scenarios,
    hence the agent’s *generalization* ability is extensively investigated. For instance,
    methods of modular networks largely evaluated the *zero-shot* performance from
    the meta-RL perspective [[52](#bib.bib52), [130](#bib.bib130)]. Given a fixed
    number of training epochs (*pe*), *Transfer ratio (tr)* is manifested differently
    among these methods. It can be the relative performance of a modular net architecture
    compared with a baseline, or the accumulated return in modified target domains,
    where reward scores are negated for evaluating the dynamics transfer. *Performance
    sensitivity (ps)* is also broadly studied to estimate the *robustness* of TL.
    [[110](#bib.bib110)] analyzed the performance sensitivity given varying source
    tasks, while [[130](#bib.bib130)] studied the performance on different unseen
    target domains.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论的大多数工作涉及多任务RL或元RL场景，因此代理的*泛化*能力得到了广泛研究。例如，模块网络的方法从元RL的角度大大评估了*零样本*性能[[52](#bib.bib52),
    [130](#bib.bib130)]。在给定固定数量的训练周期（*pe*）的情况下，*迁移比率 (tr)* 在这些方法中表现不同。它可以是模块化网络架构与基线相比的相对性能，或在修改的目标领域中的累计回报，其中奖励得分被否定用于评估动态迁移。*性能敏感性
    (ps)* 也被广泛研究，以估计TL的*鲁棒性*。[[110](#bib.bib110)]分析了在不同源任务下的性能敏感性，而[[130](#bib.bib130)]研究了在不同未见目标领域的性能。
- en: There are unresolved questions in this intriguing research topic. One is *how
    to handle drastic changes of reward functions between domains*. As discussed in [[131](#bib.bib131)],
    good policies in one MDP may perform poorly in another due to the fact that beneficial
    states or actions in ${\mathcal{M}}_{s}$ may become detrimental in ${\mathcal{M}}_{t}$
    with totally different reward functions. Learning a set of basis functions [[67](#bib.bib67)]
    to represent unseen tasks (reward functions), or decoupling policies from $Q$-function
    representation [[130](#bib.bib130)] may serve as a good start to address this
    issue, as they propose a generalized latent space, from which different tasks
    (reward functions) can be interpreted. However, the limitation of this work is
    that it is not clear how many and what kind of sub-tasks need to be learned to
    make the latent space generalizable enough.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这个引人入胜的研究主题中还有一些未解决的问题。一个问题是*如何处理领域之间奖励函数的剧烈变化*。正如[[131](#bib.bib131)]中讨论的那样，在一个MDP中表现良好的策略在另一个MDP中可能表现不佳，因为${\mathcal{M}}_{s}$中的有利状态或行动在具有完全不同奖励函数的${\mathcal{M}}_{t}$中可能变得有害。学习一组基函数[[67](#bib.bib67)]来表示未见的任务（奖励函数），或将策略从$Q$-函数表示中解耦[[130](#bib.bib130)]可能是解决这一问题的良好开端，因为它们提出了一个广义的潜在空间，从中可以解释不同的任务（奖励函数）。然而，这项工作的局限性在于，尚不清楚需要学习多少个以及什么类型的子任务才能使潜在空间具有足够的泛化能力。
- en: Another question is *how to generalize the representation learning for TL across
    domains with different dynamics or state-action spaces*. A learned SR might not
    be transferrable to an MDP with different transition dynamics, as the distribution
    of occupancy measure for SR may no longer hold. Potential solutions may include
    model-based approaches that approximate the dynamics directly or training a latent
    representation space for states using multiple tasks with different dynamics for
    better generalization [[132](#bib.bib132)]. Alternatively, TL mechanisms from
    the supervised learning domain, such as meta-learning, which enables the ability
    of fast adaptation to new tasks [[133](#bib.bib133)], or importance sampling [[134](#bib.bib134)],
    which can compensate for the prior distribution changes [[10](#bib.bib10)], may
    also shed light on this question.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是*如何将表示学习在不同动态或状态-行动空间的领域中进行泛化*。一个学到的SR可能无法转移到具有不同转移动态的MDP，因为SR的占用度量的分布可能不再适用。潜在的解决方案可能包括基于模型的方法，这些方法直接逼近动态，或使用具有不同动态的多个任务训练一个用于状态的潜在表示空间，以获得更好的泛化[[132](#bib.bib132)]。另外，来自监督学习领域的TL机制，例如能够快速适应新任务的元学习[[133](#bib.bib133)]，或可以补偿先验分布变化的重要性采样[[134](#bib.bib134)]，也可能为这个问题提供启示。
- en: '| Methods | Representations format | Assumptions | MDP difference | Learner
    |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 表示格式 | 假设 | MDP差异 | 学习者 |'
- en: '&#124; Evaluation &#124;'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估 &#124;'
- en: '&#124; metrics &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 指标 &#124;'
- en: '|'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Progressive Net [[122](#bib.bib122)] | Lateral connections to previously
    learned network modules | N/A | ${\mathcal{S}},{\mathcal{A}}$ | A3C | $ap,ar,pe,ps,tr$
    |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Progressive Net [[122](#bib.bib122)] | 与先前学习的网络模块的侧向连接 | 不适用 | ${\mathcal{S}},{\mathcal{A}}$
    | A3C | $ap,ar,pe,ps,tr$ |'
- en: '| PathNet [[123](#bib.bib123)] | Selected neural paths | N/A | ${\mathcal{S}},{\mathcal{A}}$
    | A3C | $ap,ar,pe,tr$ |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| PathNet [[123](#bib.bib123)] | 选择的神经路径 | 不适用 | ${\mathcal{S}},{\mathcal{A}}$
    | A3C | $ap,ar,pe,tr$ |'
- en: '| Modular Net [[52](#bib.bib52)] | Task(agent)-specific network module | Disentangled
    state representation | ${\mathcal{S}},{\mathcal{A}}$ | Policy Gradient | $ap,ar,pe,tt$
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 模块化网络 [[52](#bib.bib52)] | 任务（代理）特定网络模块 | 解耦状态表示 | ${\mathcal{S}},{\mathcal{A}}$
    | 策略梯度 | $ap,ar,pe,tt$ |'
- en: '| Modular Net [[125](#bib.bib125)] | Dynamic transitions module learned on
    state latent representations. | N/A | ${\mathcal{S}},{\mathcal{A}}$ | A3C | $ap,ar,pe,tr,ps$
    |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 模块化网络 [[125](#bib.bib125)] | 在状态潜在表示上学习的动态过渡模块。 | 不适用 | ${\mathcal{S}},{\mathcal{A}}$
    | A3C | $ap,ar,pe,tr,ps$ |'
- en: '| SR [[110](#bib.bib110)] | SF | Reward function can be linearly decoupled
    | ${\mathcal{R}}$ | DQN | $ap,ar,nka,ps$ |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| SR [[110](#bib.bib110)] | SF | 奖励函数可以线性解耦 | ${\mathcal{R}}$ | DQN | $ap,ar,nka,ps$
    |'
- en: '| SR [[127](#bib.bib127)] | Encoder-decoder learned SF | N/A | ${\mathcal{R}}$
    | DQN | $ap,ar,pe,ps$ |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| SR [[127](#bib.bib127)] | 编码器-解码器学习的 SF | 不适用 | ${\mathcal{R}}$ | DQN | $ap,ar,pe,ps$
    |'
- en: '| SR [[67](#bib.bib67)] | Encoder-decoder learned SF | Rewards can be represented
    by set of basis functions | ${\mathcal{R}}$ | $Q(\lambda)$ | $ap,pe$ |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| SR [[67](#bib.bib67)] | 编码器-解码器学习的 SF | 奖励可以由一组基函数表示 | ${\mathcal{R}}$ |
    $Q(\lambda)$ | $ap,pe$ |'
- en: '| UVFA [[64](#bib.bib64)] | Matrix-factorized UF | Goal conditional RL | ${\mathcal{R}}$
    | Tabular Q-learning | $ap,ar,pe,ps$ |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| UVFA [[64](#bib.bib64)] | 矩阵分解 UF | 目标条件 RL | ${\mathcal{R}}$ | 表格 Q 学习 |
    $ap,ar,pe,ps$ |'
- en: '| UVFA with SR [[130](#bib.bib130)] | Policy-encoded UF | Reward function can
    be linearly decoupled | ${\mathcal{R}}$ | $\epsilon$-greedy Q-learning | $ap,ar,pe$
    |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| UVFA 与 SR [[130](#bib.bib130)] | 策略编码 UF | 奖励函数可以线性解耦 | ${\mathcal{R}}$ |
    $\epsilon$-贪婪 Q 学习 | $ap,ar,pe$ |'
- en: 'TABLE V: A comparison of TL approaches of representation transfer.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：TL 方法在表示转移中的比较。
- en: 6 Applications
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 个应用
- en: In this section we summarize recent applications that are closely related to
    using TL techniques for tackling RL domains.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了最近的应用，这些应用与使用 TL 技术解决 RL 领域密切相关。
- en: '*Robotics learning* is a prominent application domain of RL. TL approaches
    in this field include *robotics learning from demonstrations*, where expert demonstrations
    from humans or other robots are leveraged [[135](#bib.bib135)] Another is *collaborative
    robotic training* [[136](#bib.bib136), [137](#bib.bib137)], in which knowledge
    from different robots is transferred by sharing their policies and episodic demonstrations.
    Recent research focus is this domain is fast and robust adaptation to unseen tasks.
    One example towards this goal is [[138](#bib.bib138)], in which robust robotics
    policies are trained using synthetic demonstrations to handle dynamic environments.
    Another solution is to learn domain-invariant latent representations. Examples
    include [[139](#bib.bib139)], which learns the latent representation using 3D
    CAD models, and [[140](#bib.bib140), [141](#bib.bib141)] which are derived based
    on the Generative-Adversarial Network. Another example is *DARLA* [[142](#bib.bib142)],
    which is a zero-shot transfer approach to learn disentangled representations that
    are robust against domain shifts. We refer readers to [[70](#bib.bib70), [143](#bib.bib143)]
    for detailed surveys along this direction.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '*机器人学习* 是 RL 的一个显著应用领域。该领域的 TL 方法包括 *从演示中学习机器人*，其中利用来自人类或其他机器人的专家演示 [[135](#bib.bib135)]。另一个是
    *协作机器人训练* [[136](#bib.bib136), [137](#bib.bib137)]，其中通过共享政策和情节演示转移不同机器人的知识。最近的研究重点是快速且稳健地适应未见过的任务。一个朝着这一目标的例子是
    [[138](#bib.bib138)]，其中使用合成演示训练稳健的机器人策略以处理动态环境。另一种解决方案是学习领域不变的潜在表示。例如 [[139](#bib.bib139)]
    使用 3D CAD 模型学习潜在表示，和 [[140](#bib.bib140), [141](#bib.bib141)] 基于生成对抗网络。另一个例子是
    *DARLA* [[142](#bib.bib142)]，这是一种零样本迁移方法，学习对领域变化具有鲁棒性的解耦表示。有关该方向的详细综述，请参阅 [[70](#bib.bib70),
    [143](#bib.bib143)]。'
- en: '*Game Playing* is a common test-bed for TL and RL algorithms. It has evolved
    from classical benchmarks such as grid-world games to more complex settings such
    as online-strategy games or video games with multimodal inputs. One example is
    *AlphaGo*, which is an algorithm for learning the online chessboard games using
    both TL and RL techniques [[90](#bib.bib90)]. *AlphaGo* is first pre-trained offline
    using expert demonstrations and then learns to optimize its policy using Monte-Carlo
    Tree Search. Its successor, *AlphaGo Master* [[144](#bib.bib144)], even beat the
    world’s first ranked human player. TL-DRL approaches are also thriving in video
    game playing. Especially, OpenAI has trained *Dota2* agents that can surpass human
    experts [[145](#bib.bib145)]. State-of-the-art platforms include *MineCraft*,
    *Atari*, and *Starcraft*. [[146](#bib.bib146)] designed new RL benchmarks under
    the *MineCraft* platform. [[147](#bib.bib147)] provided a comprehensive survey
    on DL applications in video game playing, which also covers TL and RL strategies
    from certain perspectives. A large portion of TL approaches reviewed in this survey
    have been applied to the *Atari* platforms  [[148](#bib.bib148)].'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '*游戏玩耍* 是TL和RL算法的常见测试平台。它已从经典的基准测试，如网格世界游戏，发展到更复杂的设置，如在线策略游戏或具有多模态输入的视频游戏。一个例子是*AlphaGo*，它是一种使用TL和RL技术学习在线棋盘游戏的算法 [[90](#bib.bib90)]。*AlphaGo*首先使用专家演示进行离线预训练，然后通过蒙特卡洛树搜索优化其策略。其继任者*AlphaGo
    Master* [[144](#bib.bib144)]甚至击败了世界排名第一的人类玩家。TL-DRL方法在视频游戏玩耍中也蓬勃发展。特别是，OpenAI训练了能够超越人类专家的*Dota2*代理 [[145](#bib.bib145)]。最先进的平台包括*MineCraft*、*Atari*和*Starcraft*。[[146](#bib.bib146)]
    在*MineCraft*平台下设计了新的RL基准测试。[[147](#bib.bib147)] 提供了关于视频游戏玩耍中DL应用的全面调查，也涵盖了从某些角度来看TL和RL策略。这次调查中回顾的许多TL方法已被应用于*Atari*平台 [[148](#bib.bib148)]。'
- en: '*Natural Language Processing (NLP)* has evolved rapidly along with the advancement
    of DL and RL. Applications of RL to NLP range widely, from *Question Answering
    (QA)* [[149](#bib.bib149)], *Dialogue systems* [[150](#bib.bib150)], *Machine
    Translation* [[151](#bib.bib151)], to an integration of NLP and Computer Vision
    tasks, such as *Visual Question Answering (VQA)* [[152](#bib.bib152)], *Image
    Caption* [[153](#bib.bib153)], etc. Many NLP applications have implicitly applied
    TL approaches. Examples include learning from expert demonstrations for *Spoken
    Dialogue Systems* [[154](#bib.bib154)], *VQA* [[152](#bib.bib152)]; or reward
    shaping for *Sequence Generation* [[155](#bib.bib155)], *Spoken Dialogue Systems* [[80](#bib.bib80)],*QA* [[81](#bib.bib81),
    [156](#bib.bib156)], and *Image Caption* [[153](#bib.bib153)], or transferring
    policies for *Structured Prediction* [[157](#bib.bib157)] and *VQA* [[158](#bib.bib158)].'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '*自然语言处理（NLP）* 随着深度学习（DL）和强化学习（RL）的进步迅速发展。RL在NLP中的应用范围广泛，从*问答（QA）* [[149](#bib.bib149)]、*对话系统* [[150](#bib.bib150)]、*机器翻译* [[151](#bib.bib151)]，到NLP与计算机视觉任务的结合，如*视觉问答（VQA）* [[152](#bib.bib152)]、*图像描述* [[153](#bib.bib153)]等。许多NLP应用已隐含地应用了TL方法。示例包括从专家演示中学习用于*口语对话系统* [[154](#bib.bib154)]、*VQA* [[152](#bib.bib152)]；或用于*序列生成* [[155](#bib.bib155)]、*口语对话系统* [[80](#bib.bib80)]、*QA* [[81](#bib.bib81),
    [156](#bib.bib156)]和*图像描述* [[153](#bib.bib153)]的奖励塑造，或用于*结构化预测* [[157](#bib.bib157)]和*VQA* [[158](#bib.bib158)]的策略转移。'
- en: '*Large Model Training*: RL from human and model-assisted feedback becomes indispensable
    in training large models (LMM), such as GPT4 [[159](#bib.bib159)], Sparrow [[160](#bib.bib160)],
    PaLM [[161](#bib.bib161)], LaMDA [[162](#bib.bib162)], which have shown tremendous
    breakthrough in dialogue applications, search engine answer improvement, artwork
    generation, etc. The TL method at the core of them is using human preferences
    as a reward signal for model fine-tuning, where the preference ranking itself
    is considered as shaped rewards. We believe that TL with carefully crafted human
    knowledge will help better align large models with human intent and hence achieve
    trustworthy and de-biased AI.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '*大规模模型训练*：从人类和模型辅助反馈中获得的强化学习在训练大规模模型（LMM）中变得不可或缺，如GPT4 [[159](#bib.bib159)]、Sparrow [[160](#bib.bib160)]、PaLM [[161](#bib.bib161)]、LaMDA [[162](#bib.bib162)]，这些模型在对话应用、搜索引擎答案改进、艺术作品生成等方面显示了巨大的突破。它们的核心方法是将人类偏好作为模型微调的奖励信号，其中偏好排名本身被视为塑造的奖励。我们相信，经过精心设计的人类知识的TL方法将有助于更好地将大规模模型与人类意图对齐，从而实现可信赖且去偏见的AI。'
- en: '*Health Informatics*: RL has been applied to various healthcare tasks [[163](#bib.bib163)],
    including *automatic medical diagnosis* [[164](#bib.bib164), [165](#bib.bib165)],
    *health resource scheduling* [[166](#bib.bib166)], and *drug discovery and development*, [[167](#bib.bib167),
    [168](#bib.bib168)], etc. Among these applications we observe emerging trends
    of leveraging prior knowledge to improve the RL procedure, especially given the
    difficulty of accessing large amounts of clinical data. Specifically, [[169](#bib.bib169)]
    utilized $Q$-learning for drug delivery individualization. They integrated the
    prior knowledge of the dose-response characteristics into their $Q$-learning framework
    to avoid random exploration. [[170](#bib.bib170)] applied a DQN framework for
    prescribing effective HIV treatments, in which they learned a latent representation
    to estimate the uncertainty when transferring a pertained policy to the unseen
    domains. [[171](#bib.bib171)] studied applying human-involved interactive RL training
    for health informatics.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '*健康信息学*: 强化学习（RL）已被应用于各种医疗任务[[163](#bib.bib163)]，包括*自动医学诊断*[[164](#bib.bib164),
    [165](#bib.bib165)]，*健康资源调度*[[166](#bib.bib166)]，以及*药物发现与开发*[[167](#bib.bib167),
    [168](#bib.bib168)]等。在这些应用中，我们观察到利用先验知识来改进RL过程的新兴趋势，尤其是在访问大量临床数据的困难情况下。具体而言，[[169](#bib.bib169)]
    使用$Q$-学习进行药物配送个性化。他们将剂量-反应特征的先验知识整合到$Q$-学习框架中，以避免随机探索。[[170](#bib.bib170)] 应用了DQN框架来开处有效的HIV治疗方案，其中他们学习了潜在表示，以在将已知策略转移到未见领域时估计不确定性。[[171](#bib.bib171)]
    研究了将人类参与的互动RL训练应用于健康信息学。'
- en: '*Others*: RL has also been utilized in many other real-life applications. Applications
    in the *Transportation Systems* have adopted RL to address traffic congestion
    issues with better *traffic signal scheduling* and *transportation resource allocation* [[9](#bib.bib9),
    [172](#bib.bib172), [8](#bib.bib8), [173](#bib.bib173)]. We refer readers to [[174](#bib.bib174)]
    for a review along this line. Deep RL are also effective solutions to problems
    in *Finance*, including *portfolio management* [[175](#bib.bib175), [176](#bib.bib176)],
    *asset allocation* [[177](#bib.bib177)], and *trading optimization* [[178](#bib.bib178)].
    Another application is the *Electricity Systems*, especially the *intelligent
    electricity networks*, which can benefit from RL techniques for improved power-delivery
    decisions [[179](#bib.bib179), [180](#bib.bib180)] and active resource management [[181](#bib.bib181)].
    [[7](#bib.bib7)] provides a detailed survey of RL techniques for electric power
    system applications.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '*其他*: RL还被应用于许多其他实际应用中。在*交通系统*中的应用已采用RL来解决交通拥堵问题，包括更好的*交通信号调度*和*交通资源分配*[[9](#bib.bib9),
    [172](#bib.bib172), [8](#bib.bib8), [173](#bib.bib173)]。我们建议读者参考[[174](#bib.bib174)]了解相关综述。深度RL也是解决*金融*问题的有效方法，包括*投资组合管理*[[175](#bib.bib175),
    [176](#bib.bib176)]，*资产配置*[[177](#bib.bib177)]，和*交易优化*[[178](#bib.bib178)]。另一个应用领域是*电力系统*，特别是*智能电力网络*，这些系统可以通过RL技术改善电力配送决策[[179](#bib.bib179),
    [180](#bib.bib180)]和主动资源管理[[181](#bib.bib181)]。[[7](#bib.bib7)] 提供了针对电力系统应用的RL技术的详细综述。'
- en: 7 Future Perspectives
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 未来展望
- en: In this section, we present some open challenges and future directions in TL
    that are closely related to the DRL domain, based on both retrospectives of the
    methods discussed in this survey and outlooks to the emerging trends of AI.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们基于对本调查中讨论的方法的回顾以及对人工智能新兴趋势的展望，提出一些在TL中与DRL领域密切相关的开放挑战和未来方向。
- en: '*Transfer Learning from Black-Box:*  Ranging from exterior teacher demonstrations
    to pre-trained function approximators, black-box resource is more accessible and
    predominant than well-articulated knowledge. Therefore, leveraging such black-box
    resource is indispensable for practical TL in DRL. One of its main challenges
    resides in *estimating the optimality* of black-box resource, which can be potentially
    noisy or biased. We consider that efforts can be made from the following perspectives:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '*黑箱中的迁移学习:* 从外部教师展示到预训练函数逼近器，黑箱资源比精确阐述的知识更易获得和占主导地位。因此，利用这些黑箱资源对于实际的迁移学习（TL）在深度强化学习（DRL）中是不可或缺的。其主要挑战之一在于*估计黑箱资源的最优性*，这些资源可能会存在噪声或偏差。我们认为可以从以下几个方面进行努力：'
- en: '1.'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Inferring the *reasoning* mechanism inside the black-box. Resemblant ideas have
    been explored in *inverse RL* and *model-based RL*, where the goal is to approximate
    the reward function or to learn the dynamics model under which the demonstrated
    knowledge becomes reasonable.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推断黑箱中的*推理*机制。类似的思想已在*逆向强化学习*和*基于模型的强化学习*中探索，其中目标是近似奖励函数或学习在该条件下示范知识变得合理的动态模型。
- en: '2.'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Designing effective *feedback* schemes, including leveraging domain-provided
    rewards, intrinsic reward feedback, or using human preference as feedback.
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设计有效的*反馈*方案，包括利用领域提供的奖励、内在奖励反馈或使用人类偏好作为反馈。
- en: '3.'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Improving the *interpretability* of the transferred knowledge[[182](#bib.bib182),
    [183](#bib.bib183)], which benefits in evaluating and explaining the process of
    TL from black-box. It can also alleviate catastrophic decision-making for high-stake
    tasks such as auto-driving.
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提高转移知识的*可解释性*[[182](#bib.bib182), [183](#bib.bib183)]，这有助于评估和解释从黑箱中转移学习的过程。它还可以缓解高风险任务（如自动驾驶）的灾难性决策。
- en: '*Knowledge Disentanglement and Fusion* are both towards better knowledge sharing
    across domains. Disentangling knowledge is usually a *prerequisite* for efficient
    knowledge fusion, which may involve external knowledge from multiple source *domains*,
    with diverging *qualities*, presented in different *modalities*, etc. Disentangling
    knowledge in RL can be interpreted from different perspectives: i) disentangling
    the action, state, or reward representations, as discussed in Sec [5.5](#S5.SS5
    "5.5 Representation Transfer ‣ 5 Transfer Learning Approaches Deep Dive ‣ Transfer
    Learning in Deep Reinforcement Learning: A Survey"); 2) decomposing complex tasks
    into multiple *skill snippets.* The former is an effective direction in tackling
    meta-RL and multi-task RL, although some solutions hinge on strict assumptions
    of the problem setting, such as linear dependence among domain dynamics or learning
    goals. The latter is relevant to hierarchical RL and *prototype learning* from
    sequence data[[184](#bib.bib184)]. It is relatively less discussed besides few
    pioneer research [[132](#bib.bib132)]. We believe that this direction is worth
    more research efforts, which not only benefits interpretable knowledge learning,
    but also aligns with human perception.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '*知识解缠和融合*旨在实现更好的跨领域知识共享。解缠知识通常是高效知识融合的*前提*，这可能涉及来自多个源*领域*的外部知识，这些领域具有不同的*质量*，以不同的*模态*呈现等。在强化学习中解缠知识可以从不同角度解读：i)
    解缠行动、状态或奖励表示，如在第[5.5](#S5.SS5 "5.5 Representation Transfer ‣ 5 Transfer Learning
    Approaches Deep Dive ‣ Transfer Learning in Deep Reinforcement Learning: A Survey")节中讨论的；2)
    将复杂任务分解为多个*技能片段*。前者是在解决元强化学习和多任务强化学习中的一个有效方向，尽管一些解决方案依赖于问题设定的严格假设，如领域动态或学习目标之间的线性依赖。后者与层次化强化学习和*原型学习*相关[[184](#bib.bib184)]。除了少数开创性研究[[132](#bib.bib132)]，这一方向相对较少讨论。我们认为这一方向值得更多研究努力，它不仅有助于可解释的知识学习，还与人类感知一致。'
- en: '*Framework-Agnostic Knowledge Transfer*: Most contemporary TL approaches are
    designed for certain RL frameworks. Some are applicable to RL algorithms designed
    for the discrete-action space, while others may only be feasible given a continuous
    action space. One fundamental reason behind is the diversified development of
    RL algorithms. We expect that unified RL frameworks would contribute to the standardization
    of TL approaches in this field.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '*框架无关的知识转移*：大多数现代的知识转移方法是为特定的强化学习框架设计的。一些方法适用于为离散动作空间设计的强化学习算法，而其他方法可能仅在连续动作空间下可行。其根本原因在于强化学习算法的发展多样化。我们期望统一的强化学习框架能促进该领域知识转移方法的标准化。'
- en: '*Evaluation and Benchmarking*: Variant evaluation metrics have been proposed
    to measure TL from different but complementary perspectives, although no single
    metric can summarize the efficacy of a TL approach. Designing a set of generalized,
    novel metrics is beneficial for the development of TL in DRL. Moreover, with the
    effervescent development of large-scale models, it is crucial to standardize evaluation
    from the perspectives of *ethics* and *groundedness*. The *appropriateness* of
    the transferred knowledge, such as potential *stereotypes* in human preference,
    and the *bias* in the model itself should also be quantified as metrics.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '*评估与基准测试*：已经提出了多种变体评估指标来从不同但互补的角度衡量迁移学习（TL），尽管没有单一指标能够总结TL方法的效果。设计一组通用的、新颖的指标对DRL中TL的发展是有益的。此外，随着大规模模型的蓬勃发展，从*伦理*和*实际性*的角度标准化评估变得尤为重要。转移知识的*适当性*，例如人类偏好中的潜在*刻板印象*，以及模型本身的*偏见*也应作为指标进行量化。'
- en: '*Knowledge Transfer to and from Pre-Trained Large Models:* By the time of this
    survey being finalized, unprecedented DL breakthroughs have been achieved in learning
    large-scale models built on massive computation resource and attributed data.
    One representative example is the Generative Pre-trained Transformer (GPT)[[159](#bib.bib159)].
    Considering them as complete *knowledge graphs* whose training process maybe inaccessible,
    there are more challenges in this direction besides learning from a *black-box*,
    which are faced by a larger AI community including the RL domain. We briefly point
    out two directions that are worth ongoing attention:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '*从预训练的大型模型中进行知识转移*：在本调查最终定稿时，在基于大规模计算资源和数据的学习中已经取得了前所未有的深度学习突破。一个代表性的例子是生成预训练变换器（GPT）[[159](#bib.bib159)]。考虑到它们作为完整的*知识图谱*，其训练过程可能不可访问，除了从*黑箱*中学习外，这一方向面临着更多挑战，包括RL领域在内的更大AI社区也面临这些挑战。我们简要指出两个值得持续关注的方向：'
- en: '1.'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '*Efficient model fine-tuning with knowledge distillation.* One important method
    for fine-tuning large models is *RL with human feedback*, in which the quantity
    and quality of human ratings are critical for realizing a good reward model. We
    anticipate other forms of TL methods in RL to be explored to further improve the
    efficiency of fine-tuning, such as imitation learning with adversarial training
    to achieve human-level performance.'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*通过知识蒸馏进行高效的模型微调*。一种重要的微调大模型的方法是*利用人类反馈的强化学习（RL）*，其中人类评分的数量和质量对实现良好的奖励模型至关重要。我们预计将会探索RL中其他形式的迁移学习（TL）方法，以进一步提高微调的效率，例如通过对抗训练的模仿学习来实现人类水平的性能。'
- en: '2.'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: '*Principled prompt-engineering for knowledge extraction*. More often the large
    model itself cannot be accessed, but only input and output of models are allowed.
    Such inference based knowledge extraction requires delicate prompt designs. Some
    efficacious efforts include designing prompts with mini task examples as one-shot
    learning, *decomposing* complex tasks into *architectural*, contextual prompts.
    Prompt engineering is being proved an important direction for effective knowledge
    extraction, which with proper design, can largely benefit downstream tasks that
    depend on large model resources.'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*以原则为基础的提示工程用于知识提取*。大模型本身通常无法访问，只能允许输入和输出模型。这种基于推断的知识提取需要精心设计的提示。一些有效的努力包括设计带有小任务示例的一次性学习提示，*分解*复杂任务为*架构*和上下文提示。提示工程被证明是有效知识提取的重要方向，通过适当的设计，可以极大地有利于依赖大模型资源的下游任务。'
- en: 8 Acknowledgements
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 致谢
- en: This research was supported by the National Science Foundation (IIS-2212174,
    IIS-1749940), National Institute of Aging (IRF1AG072449), and the Office of Naval
    Research (N00014-20-1-2382).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了国家科学基金（IIS-2212174, IIS-1749940）、国家老龄化研究所（IRF1AG072449）和海军研究办公室（N00014-20-1-2382）的支持。
- en: References
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. S. Sutton and A. G. Barto, *Reinforcement learning: An introduction*.   MIT
    press, 2018.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] R. S. Sutton 和 A. G. Barto, *强化学习：导论*。 MIT出版社, 2018。'
- en: '[2] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “A brief
    survey of deep reinforcement learning,” *arXiv preprint arXiv:1708.05866*, 2017.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] K. Arulkumaran, M. P. Deisenroth, M. Brundage, 和 A. A. Bharath, “深度强化学习的简要调查，”
    *arXiv 预印本 arXiv:1708.05866*，2017。'
- en: '[3] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of
    deep visuomotor policies,” *The Journal of Machine Learning Research*, 2016.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] S. Levine, C. Finn, T. Darrell, 和 P. Abbeel, “深度视觉运动策略的端到端训练，” *机器学习研究期刊*，2016。'
- en: '[4] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen, “Learning
    hand-eye coordination for robotic grasping with deep learning and large-scale
    data collection,” *The International Journal of Robotics Research*, 2018.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, 和 D. Quillen, “通过深度学习和大规模数据收集学习机器人抓取的手眼协调,”
    *国际机器人研究期刊*, 2018。'
- en: '[5] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, “The arcade learning
    environment: An evaluation platform for general agents,” *Journal of Artificial
    Intelligence Research*, 2013.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. G. Bellemare, Y. Naddaf, J. Veness, 和 M. Bowling, “街机学习环境：通用智能体的评估平台,”
    *人工智能研究期刊*, 2013。'
- en: '[6] M. R. Kosorok and E. E. Moodie, *Adaptive TreatmentStrategies in Practice:
    Planning Trials and Analyzing Data for Personalized Medicine*.   SIAM, 2015.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. R. Kosorok 和 E. E. Moodie, *实践中的适应性治疗策略：个性化医学的试验规划与数据分析*。SIAM, 2015。'
- en: '[7] M. Glavic, R. Fonteneau, and D. Ernst, “Reinforcement learning for electric
    power system decision and control: Past considerations and perspectives,” *IFAC-PapersOnLine*,
    2017.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] M. Glavic, R. Fonteneau, 和 D. Ernst, “电力系统决策与控制的强化学习：过去的考虑与展望,” *IFAC-PapersOnLine*,
    2017。'
- en: '[8] S. El-Tantawy, B. Abdulhai, and H. Abdelgawad, “Multiagent reinforcement
    learning for integrated network of adaptive traffic signal controllers (marlin-atsc):
    methodology and large-scale application on downtown toronto,” *IEEE Transactions
    on Intelligent Transportation Systems*, 2013.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] S. El-Tantawy, B. Abdulhai, 和 H. Abdelgawad, “用于适应性交通信号控制器网络的多智能体强化学习（MARLIN-ATSC）：方法论及其在多伦多市中心的大规模应用,”
    *IEEE 智能交通系统汇刊*, 2013。'
- en: '[9] H. Wei, G. Zheng, H. Yao, and Z. Li, “Intellilight: A reinforcement learning
    approach for intelligent traffic light control,” *ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining*, 2018.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] H. Wei, G. Zheng, H. Yao, 和 Z. Li, “Intellilight：一种用于智能交通信号控制的强化学习方法,”
    *ACM SIGKDD 知识发现与数据挖掘国际会议*, 2018。'
- en: '[10] S. J. Pan and Q. Yang, “A survey on transfer learning,” *IEEE Transactions
    on knowledge and data engineering*, 2009.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] S. J. Pan 和 Q. Yang, “迁移学习综述,” *IEEE 知识与数据工程汇刊*, 2009。'
- en: '[11] M. E. Taylor and P. Stone, “Transfer learning for reinforcement learning
    domains: A survey,” *Journal of Machine Learning Research*, 2009.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] M. E. Taylor 和 P. Stone, “强化学习领域的迁移学习：综述,” *机器学习研究期刊*, 2009。'
- en: '[12] A. Lazaric, “Transfer in reinforcement learning: a framework and a survey.”   Springer,
    2012.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Lazaric, “强化学习中的迁移：一个框架和综述。” Springer, 2012。'
- en: '[13] R. Bellman, “A markovian decision process,” *Journal of mathematics and
    mechanics*, 1957.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] R. Bellman, “马尔可夫决策过程,” *数学与力学期刊*, 1957。'
- en: '[14] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspective
    on reinforcement learning,” in *International conference on machine learning*.   PMLR,
    2017, pp. 449–458.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] M. G. Bellemare, W. Dabney, 和 R. Munos, “强化学习的分布视角,” 见于 *国际机器学习会议*。PMLR,
    2017, 页 449–458。'
- en: '[15] M. Liu, M. Zhu, and W. Zhang, “Goal-conditioned reinforcement learning:
    Problems and solutions,” *arXiv preprint arXiv:2201.08299*, 2022.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] M. Liu, M. Zhu, 和 W. Zhang, “目标条件强化学习：问题与解决方案,” *arXiv 预印本 arXiv:2201.08299*,
    2022。'
- en: '[16] C. Florensa, D. Held, X. Geng, and P. Abbeel, “Automatic goal generation
    for reinforcement learning agents,” in *International conference on machine learning*.   PMLR,
    2018, pp. 1515–1528.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] C. Florensa, D. Held, X. Geng, 和 P. Abbeel, “强化学习智能体的自动目标生成,” 见于 *国际机器学习会议*。PMLR,
    2018, 页 1515–1528。'
- en: '[17] Z. Xu and A. Tewari, “Reinforcement learning in factored mdps: Oracle-efficient
    algorithms and tighter regret bounds for the non-episodic setting,” *NeurIPS*,
    vol. 33, pp. 18 226–18 236, 2020.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Z. Xu 和 A. Tewari, “在因子化马尔可夫决策过程中的强化学习：针对非情节设置的高效算法和更紧的遗憾界限,” *NeurIPS*,
    卷 33, 页 18 226–18 236, 2020。'
- en: '[18] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu, “The
    surprising effectiveness of ppo in cooperative multi-agent games,” *NeurIPS*,
    vol. 35, pp. 24 611–24 624, 2022.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, 和 Y. Wu, “PPO
    在合作多智能体游戏中的惊人有效性,” *NeurIPS*, 卷 35, 页 24 611–24 624, 2022。'
- en: '[19] I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson, “Discriminator-actor-critic:
    Addressing sample inefficiency and reward bias in adversarial imitation learning,”
    *arXiv preprint arXiv:1809.02925*, 2018.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, 和 J. Tompson, “判别器-演员-评论员：解决对抗模仿学习中的样本效率和奖励偏差问题,”
    *arXiv 预印本 arXiv:1809.02925*, 2018。'
- en: '[20] G. A. Rummery and M. Niranjan, *On-line Q-learning using connectionist
    systems*.   University of Cambridge, Department of Engineering Cambridge, England,
    1994.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] G. A. Rummery 和 M. Niranjan，*基于连接主义系统的在线 Q 学习*。剑桥大学工程系，英国剑桥，1994 年。'
- en: '[21] H. Van Seijen, H. Van Hasselt, S. Whiteson, and M. Wiering, “A theoretical
    and empirical analysis of expected sarsa,” *IEEE Symposium on Adaptive Dynamic
    Programming and Reinforcement Learning*, 2009.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] H. Van Seijen, H. Van Hasselt, S. Whiteson 和 M. Wiering，“期望 sarsa 的理论和实证分析，”
    *IEEE 自适应动态规划与强化学习研讨会*，2009 年。'
- en: '[22] V. Konda and J. Tsitsiklis, “Actor-critic algorithms,” *NeurIPS*, 2000.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] V. Konda 和 J. Tsitsiklis，“演员-评论员算法，” *NeurIPS*，2000 年。'
- en: '[23] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” *ICML*,
    2016.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D.
    Silver 和 K. Kavukcuoglu，“深度强化学习的异步方法，” *ICML*，2016 年。'
- en: '[24] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-policy
    maximum entropy deep reinforcement learning with a stochastic actor,” *International
    Conference on Machine Learning*, 2018.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] T. Haarnoja, A. Zhou, P. Abbeel 和 S. Levine，“软演员-评论员：带有随机演员的非策略最大熵深度强化学习，”
    *国际机器学习会议*，2018 年。'
- en: '[25] C. J. Watkins and P. Dayan, “Q-learning,” *Machine learning*, 1992.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] C. J. Watkins 和 P. Dayan，“Q 学习，” *机器学习*，1992 年。'
- en: '[26] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, 2015.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *等*，“通过深度强化学习实现人类水平的控制，”
    *Nature*，2015 年。'
- en: '[27] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney,
    D. Horgan, B. Piot, M. Azar, and D. Silver, “Rainbow: Combining improvements in
    deep reinforcement learning,” *AAAI*, 2018.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney,
    D. Horgan, B. Piot, M. Azar 和 D. Silver，“彩虹：结合深度强化学习的改进，” *AAAI*，2018 年。'
- en: '[28] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” *Machine learning*, 1992.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] R. J. Williams，“用于连接主义强化学习的简单统计梯度跟随算法，” *机器学习*，1992 年。'
- en: '[29] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region
    policy optimization,” *ICML*, 2015.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Schulman, S. Levine, P. Abbeel, M. Jordan 和 P. Moritz，“信任区域策略优化，” *ICML*，2015
    年。'
- en: '[30] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv preprint arXiv:1707.06347*, 2017.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Schulman, F. Wolski, P. Dhariwal, A. Radford 和 O. Klimov，“近端策略优化算法，”
    *arXiv 预印本 arXiv:1707.06347*，2017 年。'
- en: '[31] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
    “Deterministic policy gradient algorithms,” 2014.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra 和 M. Riedmiller，“确定性策略梯度算法，”
    2014 年。'
- en: '[32] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    *arXiv preprint arXiv:1509.02971*, 2015.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver 和 D. Wierstra，“通过深度强化学习进行连续控制，” *arXiv 预印本 arXiv:1509.02971*，2015 年。'
- en: '[33] S. Fujimoto, H. Van Hoof, and D. Meger, “Addressing function approximation
    error in actor-critic methods,” *arXiv preprint arXiv:1802.09477*, 2018.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. Fujimoto, H. Van Hoof 和 D. Meger，“解决演员-评论员方法中的函数近似误差，” *arXiv 预印本 arXiv:1802.09477*，2018
    年。'
- en: '[34] A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine, “Neural network dynamics
    for model-based deep reinforcement learning with model-free fine-tuning,” in *2018
    IEEE international conference on robotics and automation (ICRA)*.   IEEE, 2018,
    pp. 7559–7566.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] A. Nagabandi, G. Kahn, R. S. Fearing 和 S. Levine，“用于模型基深度强化学习的神经网络动态与无模型微调，”
    收录于 *2018 IEEE 国际机器人与自动化会议 (ICRA)*。IEEE，2018 年，第 7559–7566 页。'
- en: '[35] Z. I. Botev, D. P. Kroese, R. Y. Rubinstein, and P. L’Ecuyer, “The cross-entropy
    method for optimization,” in *Handbook of statistics*.   Elsevier, 2013, vol. 31,
    pp. 35–59.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Z. I. Botev, D. P. Kroese, R. Y. Rubinstein 和 P. L’Ecuyer，“优化的交叉熵方法，”
    收录于 *统计学手册*。Elsevier，2013 年，第 31 卷，第 35–59 页。'
- en: '[36] K. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep reinforcement
    learning in a handful of trials using probabilistic dynamics models,” *NeurIPS*,
    vol. 31, 2018.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] K. Chua, R. Calandra, R. McAllister 和 S. Levine，“利用概率动态模型在少量试验中进行深度强化学习，”
    *NeurIPS*，第 31 卷，2018 年。'
- en: '[37] R. S. Sutton, “Integrated architectures for learning, planning, and reacting
    based on approximating dynamic programming,” in *Machine learning proceedings
    1990*.   Elsevier, 1990, pp. 216–224.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] R. S. Sutton，“基于动态规划近似的学习、规划和反应的综合架构，” 见 *1990年机器学习会议录*。 Elsevier，1990年，第216–224页。'
- en: '[38] V. Feinberg, A. Wan, I. Stoica, M. I. Jordan, J. E. Gonzalez, and S. Levine,
    “Model-based value estimation for efficient model-free reinforcement learning,”
    *arXiv preprint arXiv:1803.00101*, 2018.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] V. Feinberg、A. Wan、I. Stoica、M. I. Jordan、J. E. Gonzalez 和 S. Levine，“用于高效无模型强化学习的基于模型的价值估计，”
    *arXiv 预印本 arXiv:1803.00101*，2018年。'
- en: '[39] S. Levine and V. Koltun, “Guided policy search,” in *International conference
    on machine learning*.   PMLR, 2013, pp. 1–9.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Levine 和 V. Koltun，“引导策略搜索，” 见 *国际机器学习大会*。 PMLR，2013年，第1–9页。'
- en: '[40] H. Bharadhwaj, K. Xie, and F. Shkurti, “Model-predictive control via cross-entropy
    and gradient-based optimization,” in *Learning for Dynamics and Control*.   PMLR,
    2020, pp. 277–286.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] H. Bharadhwaj、K. Xie 和 F. Shkurti，“通过交叉熵和基于梯度的优化进行模型预测控制，” 见 *动态与控制学习*。
    PMLR，2020年，第277–286页。'
- en: '[41] M. Deisenroth and C. E. Rasmussen, “Pilco: A model-based and data-efficient
    approach to policy search,” in *Proceedings of the 28th International Conference
    on machine learning (ICML-11)*, 2011, pp. 465–472.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] M. Deisenroth 和 C. E. Rasmussen，“Pilco：一种基于模型的数据高效策略搜索方法，” 见 *第28届国际机器学习大会（ICML-11）*，2011年，第465–472页。'
- en: '[42] Y. Gal, R. McAllister, and C. E. Rasmussen, “Improving pilco with bayesian
    neural network dynamics models,” in *Data-efficient machine learning workshop,
    ICML*, vol. 4, no. 34, 2016, p. 25.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Gal、R. McAllister 和 C. E. Rasmussen，“通过贝叶斯神经网络动态模型改进 pilco，” 见 *数据高效机器学习研讨会，ICML*，第4卷，第34期，2016年，第25页。'
- en: '[43] C. H. Lampert, H. Nickisch, and S. Harmeling, “Learning to detect unseen
    object classes by between-class attribute transfer,” *IEEE Conference on Computer
    Vision and Pattern Recognition*, 2009.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] C. H. Lampert、H. Nickisch 和 S. Harmeling，“通过类间属性转移学习检测未见过的对象类别，” *IEEE计算机视觉与模式识别会议*，2009年。'
- en: '[44] P. Dayan and G. E. Hinton, “Feudal reinforcement learning,” *NeurIPS*,
    1993.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] P. Dayan 和 G. E. Hinton，“封建强化学习，” *NeurIPS*，1993年。'
- en: '[45] R. S. Sutton, D. Precup, and S. Singh, “Between mdps and semi-mdps: A
    framework for temporal abstraction in reinforcement learning,” *Artificial intelligence*,
    1999.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] R. S. Sutton、D. Precup 和 S. Singh，“在 mdps 和 semi-mdps 之间：强化学习中的时间抽象框架，”
    *人工智能*，1999年。'
- en: '[46] R. Parr and S. J. Russell, “Reinforcement learning with hierarchies of
    machines,” *NeurIPS*, 1998.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] R. Parr 和 S. J. Russell，“具有机器层级的强化学习，” *NeurIPS*，1998年。'
- en: '[47] T. G. Dietterich, “Hierarchical reinforcement learning with the maxq value
    function decomposition,” *Journal of artificial intelligence research*, 2000.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] T. G. Dietterich，“具有 maxq 值函数分解的层次强化学习，” *人工智能研究期刊*，2000年。'
- en: '[48] A. Lazaric and M. Ghavamzadeh, “Bayesian multi-task reinforcement learning,”
    in *ICML-27th international conference on machine learning*.   Omnipress, 2010,
    pp. 599–606.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] A. Lazaric 和 M. Ghavamzadeh，“贝叶斯多任务强化学习，” 见 *ICML-第27届国际机器学习大会*。 Omnipress，2010年，第599–606页。'
- en: '[49] Y. Zhang and Q. Yang, “A survey on multi-task learning,” *IEEE Transactions
    on Knowledge and Data Engineering*, vol. 34, no. 12, pp. 5586–5609, 2021.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Y. Zhang 和 Q. Yang，“多任务学习综述，” *IEEE知识与数据工程汇刊*，第34卷，第12期，第5586–5609页，2021年。'
- en: '[50] Y. Teh, V. Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R. Hadsell,
    N. Heess, and R. Pascanu, “Distral: Robust multitask reinforcement learning,”
    *NeurIPS*, 2017.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Y. Teh、V. Bapst、W. M. Czarnecki、J. Quan、J. Kirkpatrick、R. Hadsell、N. Heess
    和 R. Pascanu，“Distral：鲁棒的多任务强化学习，” *NeurIPS*，2017年。'
- en: '[51] E. Parisotto, J. L. Ba, and R. Salakhutdinov, “Actor-mimic: Deep multitask
    and transfer reinforcement learning,” *ICLR*, 2016.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] E. Parisotto、J. L. Ba 和 R. Salakhutdinov，“Actor-mimic：深度多任务和迁移强化学习，” *ICLR*，2016年。'
- en: '[52] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine, “Learning modular
    neural network policies for multi-task and multi-robot transfer,” *2017 IEEE International
    Conference on Robotics and Automation (ICRA)*, 2017.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] C. Devin、A. Gupta、T. Darrell、P. Abbeel 和 S. Levine，“为多任务和多机器人迁移学习模块化神经网络策略，”
    *2017年IEEE国际机器人与自动化会议（ICRA）*，2017年。'
- en: '[53] J. Andreas, D. Klein, and S. Levine, “Modular multitask reinforcement
    learning with policy sketches,” *ICML*, 2017.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] J. Andreas、D. Klein 和 S. Levine，“带有策略草图的模块化多任务强化学习，” *ICML*，2017年。'
- en: '[54] R. Yang, H. Xu, Y. Wu, and X. Wang, “Multi-task reinforcement learning
    with soft modularization,” *NeurIPS*, vol. 33, pp. 4767–4777, 2020.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] R. Yang、H. Xu、Y. Wu 和 X. Wang，“具有软模块化的多任务强化学习，” *NeurIPS*，第33卷，第4767–4777页，2020年。'
- en: '[55] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, “Meta-learning
    in neural networks: A survey,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 44, no. 9, pp. 5149–5169, 2021.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] T. Hospedales, A. Antoniou, P. Micaelli 和 A. Storkey，"神经网络中的元学习：综述"，*IEEE模式分析与机器智能学报*，第44卷，第9期，第5149–5169页，2021年。'
- en: '[56] Z. Jia, X. Li, Z. Ling, S. Liu, Y. Wu, and H. Su, “Improving policy optimization
    with generalist-specialist learning,” in *International Conference on Machine
    Learning*.   PMLR, 2022, pp. 10 104–10 119.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Z. Jia, X. Li, Z. Ling, S. Liu, Y. Wu 和 H. Su，"通过通才-专才学习改进策略优化"，见于*国际机器学习会议*。
    PMLR，2022年，第10 104–10 119页。'
- en: '[57] W. Ding, H. Lin, B. Li, and D. Zhao, “Generalizing goal-conditioned reinforcement
    learning with variational causal reasoning,” *arXiv preprint arXiv:2207.09081*,
    2022.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] W. Ding, H. Lin, B. Li 和 D. Zhao，"通过变分因果推理推广目标条件强化学习"，*arXiv预印本 arXiv:2207.09081*，2022年。'
- en: '[58] R. Kirk, A. Zhang, E. Grefenstette, and T. Rocktäschel, “A survey of zero-shot
    generalisation in deep reinforcement learning,” *Journal of Artificial Intelligence
    Research*, vol. 76, pp. 201–264, 2023.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] R. Kirk, A. Zhang, E. Grefenstette 和 T. Rocktäschel，"深度强化学习中的零样本泛化综述"，*人工智能研究期刊*，第76卷，第201–264页，2023年。'
- en: '[59] B. Kim, A.-m. Farahmand, J. Pineau, and D. Precup, “Learning from limited
    demonstrations,” *NeurIPS*, 2013.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] B. Kim, A.-m. Farahmand, J. Pineau 和 D. Precup，"从有限示范中学习"，*NeurIPS*，2013年。'
- en: '[60] W. Czarnecki, R. Pascanu, S. Osindero, S. Jayakumar, G. Swirszcz, and
    M. Jaderberg, “Distilling policy distillation,” *The 22nd International Conference
    on Artificial Intelligence and Statistics*, 2019.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] W. Czarnecki, R. Pascanu, S. Osindero, S. Jayakumar, G. Swirszcz 和 M.
    Jaderberg，"策略蒸馏"，*第22届国际人工智能与统计学会议*，2019年。'
- en: '[61] A. Y. Ng, D. Harada, and S. Russell, “Policy invariance under reward transformations:
    Theory and application to reward shaping,” *ICML*, 1999.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] A. Y. Ng, D. Harada 和 S. Russell，"奖励转换下的策略不变性：理论及其在奖励塑造中的应用"，*ICML*，1999年。'
- en: '[62] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” *NeurIPS*, pp. 2672–2680,
    2014.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville 和 Y. Bengio，"生成对抗网络"，*NeurIPS*，第2672–2680页，2014年。'
- en: '[63] Z. Zhu, K. Lin, B. Dai, and J. Zhou, “Learning sparse rewarded tasks from
    sub-optimal demonstrations,” *arXiv preprint arXiv:2004.00530*, 2020.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Z. Zhu, K. Lin, B. Dai 和 J. Zhou，"从次优示范中学习稀疏奖励任务"，*arXiv预印本 arXiv:2004.00530*，2020年。'
- en: '[64] T. Schaul, D. Horgan, K. Gregor, and D. Silver, “Universal value function
    approximators,” *ICML*, 2015.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] T. Schaul, D. Horgan, K. Gregor 和 D. Silver，"通用价值函数近似器"，*ICML*，2015年。'
- en: '[65] C. Finn and S. Levine, “Meta-learning: from few-shot learning to rapid
    reinforcement learning,” *ICML*, 2019.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] C. Finn 和 S. Levine，"元学习：从少量样本学习到快速强化学习"，*ICML*，2019年。'
- en: '[66] M. E. Taylor, P. Stone, and Y. Liu, “Transfer learning via inter-task
    mappings for temporal difference learning,” *Journal of Machine Learning Research*,
    2007.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] M. E. Taylor, P. Stone 和 Y. Liu，"通过任务间映射进行迁移学习以实现时间差分学习"，*机器学习研究期刊*，2007年。'
- en: '[67] A. Barreto, D. Borsa, J. Quan, T. Schaul, D. Silver, M. Hessel, D. Mankowitz,
    A. Žídek, and R. Munos, “Transfer in deep reinforcement learning using successor
    features and generalised policy improvement,” *ICML*, 2018.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] A. Barreto, D. Borsa, J. Quan, T. Schaul, D. Silver, M. Hessel, D. Mankowitz,
    A. Žídek 和 R. Munos，"使用继任特征和广义策略改进进行深度强化学习的迁移"，*ICML*，2018年。'
- en: '[68] Z. Zhu, K. Lin, B. Dai, and J. Zhou, “Off-policy imitation learning from
    observations,” *NeurIPS*, 2020.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Z. Zhu, K. Lin, B. Dai 和 J. Zhou，"从观察中进行离策略模仿学习"，*NeurIPS*，2020年。'
- en: '[69] J. Ho and S. Ermon, “Generative adversarial imitation learning,” *NeurIPS*,
    2016.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] J. Ho 和 S. Ermon，"生成对抗模仿学习"，*NeurIPS*，2016年。'
- en: '[70] W. Zhao, J. P. Queralta, and T. Westerlund, “Sim-to-real transfer in deep
    reinforcement learning for robotics: a survey,” in *2020 IEEE symposium series
    on computational intelligence (SSCI)*.   IEEE, 2020, pp. 737–744.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] W. Zhao, J. P. Queralta 和 T. Westerlund，"机器人深度强化学习中的模拟到现实迁移：综述"，见于*2020
    IEEE计算智能会议系列（SSCI）*。 IEEE，2020年，第737–744页。'
- en: '[71] M. Muller-Brockhausen, M. Preuss, and A. Plaat, “Procedural content generation:
    Better benchmarks for transfer reinforcement learning,” in *2021 IEEE Conference
    on games (CoG)*.   IEEE, 2021, pp. 01–08.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] M. Muller-Brockhausen, M. Preuss 和 A. Plaat，"程序化内容生成：改进迁移强化学习的基准测试"，见于*2021
    IEEE游戏大会（CoG）*。 IEEE，2021年，第01–08页。'
- en: '[72] N. Vithayathil Varghese and Q. H. Mahmoud, “A survey of multi-task deep
    reinforcement learning,” *Electronics*, vol. 9, no. 9, p. 1363, 2020.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] N. Vithayathil Varghese 和 Q. H. Mahmoud，"多任务深度强化学习综述"，*电子学*，第9卷，第9期，第1363页，2020年。'
- en: '[73] R. J. Williams and L. C. Baird, “Tight performance bounds on greedy policies
    based on imperfect value functions,” Tech. Rep., 1993.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] R. J. Williams 和 L. C. Baird，“基于不完美价值函数的贪婪策略的紧性能界限，” 技术报告，1993。'
- en: '[74] E. Wiewiora, G. W. Cottrell, and C. Elkan, “Principled methods for advising
    reinforcement learning agents,” *ICML*, 2003.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] E. Wiewiora, G. W. Cottrell 和 C. Elkan，“为强化学习代理提供建议的原则性方法，” *ICML*，2003。'
- en: '[75] S. M. Devlin and D. Kudenko, “Dynamic potential-based reward shaping,”
    *ICAAMAS*, 2012.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] S. M. Devlin 和 D. Kudenko，“基于动态潜力的奖励塑造，” *ICAAMAS*，2012。'
- en: '[76] A. Harutyunyan, S. Devlin, P. Vrancx, and A. Nowé, “Expressing arbitrary
    reward functions as potential-based advice,” *AAAI*, 2015.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] A. Harutyunyan, S. Devlin, P. Vrancx 和 A. Nowé，“将任意奖励函数表达为基于潜力的建议，” *AAAI*，2015。'
- en: '[77] T. Brys, A. Harutyunyan, M. E. Taylor, and A. Nowé, “Policy transfer using
    reward shaping,” *ICAAMS*, 2015.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] T. Brys, A. Harutyunyan, M. E. Taylor 和 A. Nowé，“使用奖励塑造的策略转移，” *ICAAMS*，2015。'
- en: '[78] M. Večerík, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess,
    T. Rothörl, T. Lampe, and M. Riedmiller, “Leveraging demonstrations for deep reinforcement
    learning on robotics problems with sparse rewards,” *arXiv preprint arXiv:1707.08817*,
    2017.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] M. Večerík, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess,
    T. Rothörl, T. Lampe 和 M. Riedmiller，“利用演示加速稀疏奖励的机器人深度强化学习，” *arXiv 预印本 arXiv:1707.08817*，2017。'
- en: '[79] A. C. Tenorio-Gonzalez, E. F. Morales, and L. Villaseñor-Pineda, “Dynamic
    reward shaping: Training a robot by voice,” *Advances in Artificial Intelligence
    – IBERAMIA*, 2010.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] A. C. Tenorio-Gonzalez, E. F. Morales 和 L. Villaseñor-Pineda，“动态奖励塑造：通过语音训练机器人，”
    *人工智能进展 – IBERAMIA*，2010。'
- en: '[80] P.-H. Su, D. Vandyke, M. Gasic, N. Mrksic, T.-H. Wen, and S. Young, “Reward
    shaping with recurrent neural networks for speeding up on-line policy learning
    in spoken dialogue systems,” *arXiv preprint arXiv:1508.03391*, 2015.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] P.-H. Su, D. Vandyke, M. Gasic, N. Mrksic, T.-H. Wen 和 S. Young，“使用递归神经网络的奖励塑造以加速语音对话系统中的在线策略学习，”
    *arXiv 预印本 arXiv:1508.03391*，2015。'
- en: '[81] X. V. Lin, R. Socher, and C. Xiong, “Multi-hop knowledge graph reasoning
    with reward shaping,” *arXiv preprint arXiv:1808.10568*, 2018.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] X. V. Lin, R. Socher 和 C. Xiong，“使用奖励塑造的多跳知识图推理，” *arXiv 预印本 arXiv:1808.10568*，2018。'
- en: '[82] S. Devlin, L. Yliniemi, D. Kudenko, and K. Tumer, “Potential-based difference
    rewards for multiagent reinforcement learning,” *ICAAMS*, 2014.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] S. Devlin, L. Yliniemi, D. Kudenko 和 K. Tumer，“用于多智能体强化学习的基于潜力的差异奖励，”
    *ICAAMS*，2014。'
- en: '[83] M. Grzes and D. Kudenko, “Learning shaping rewards in model-based reinforcement
    learning,” *Proc. AAMAS Workshop on Adaptive Learning Agents*, 2009.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] M. Grzes 和 D. Kudenko，“在基于模型的强化学习中学习塑造奖励，” *AAMAS 适应学习代理研讨会*，2009。'
- en: '[84] O. Marom and B. Rosman, “Belief reward shaping in reinforcement learning,”
    *AAAI*, 2018.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] O. Marom 和 B. Rosman，“强化学习中的信念奖励塑造，” *AAAI*，2018。'
- en: '[85] F. Liu, Z. Ling, T. Mu, and H. Su, “State alignment-based imitation learning,”
    *arXiv preprint arXiv:1911.10947*, 2019.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] F. Liu, Z. Ling, T. Mu 和 H. Su，“基于状态对齐的模仿学习，” *arXiv 预印本 arXiv:1911.10947*，2019。'
- en: '[86] K. Kim, Y. Gu, J. Song, S. Zhao, and S. Ermon, “Domain adaptive imitation
    learning,” *ICML*, 2020.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] K. Kim, Y. Gu, J. Song, S. Zhao 和 S. Ermon，“领域自适应模仿学习，” *ICML*，2020。'
- en: '[87] Y. Ma, Y.-X. Wang, and B. Narayanaswamy, “Imitation-regularized offline
    learning,” *International Conference on Artificial Intelligence and Statistics*,
    2019.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Y. Ma, Y.-X. Wang 和 B. Narayanaswamy，“模仿正则化离线学习，” *国际人工智能与统计会议*，2019。'
- en: '[88] M. Yang and O. Nachum, “Representation matters: Offline pretraining for
    sequential decision making,” *arXiv preprint arXiv:2102.05815*, 2021.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] M. Yang 和 O. Nachum，“表征重要：离线预训练用于序列决策，” *arXiv 预印本 arXiv:2102.05815*，2021。'
- en: '[89] X. Zhang and H. Ma, “Pretraining deep actor-critic reinforcement learning
    algorithms with expert demonstrations,” *arXiv preprint arXiv:1801.10459*, 2018.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] X. Zhang 和 H. Ma，“用专家演示预训练深度演员-评论家强化学习算法，” *arXiv 预印本 arXiv:1801.10459*，2018。'
- en: '[90] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot *et al.*, “Mastering
    the game of go with deep neural networks and tree search,” *Nature*, 2016.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot *等*，“利用深度神经网络和树搜索掌握围棋游戏，”
    *自然*，2016。'
- en: '[91] S. Schaal, “Learning from demonstration,” *NeurIPS*, 1997.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] S. Schaal，“从示范中学习，” *NeurIPS*，1997。'
- en: '[92] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan,
    J. Quan, A. Sendonaris, I. Osband *et al.*, “Deep q-learning from demonstrations,”
    *AAAI*, 2018.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D.
    Horgan, J. Quan, A. Sendonaris, I. Osband *等*，“来自演示的深度 Q 学习，” *AAAI*，2018年。'
- en: '[93] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel, “Overcoming
    exploration in reinforcement learning with demonstrations,” *IEEE International
    Conference on Robotics and Automation (ICRA)*, 2018.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba 和 P. Abbeel，“通过演示克服强化学习中的探索问题，”
    *IEEE 国际机器人与自动化会议 (ICRA)*，2018年。'
- en: '[94] J. Chemali and A. Lazaric, “Direct policy iteration with demonstrations,”
    *International Joint Conference on Artificial Intelligence*, 2015.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] J. Chemali 和 A. Lazaric，“带有演示的直接策略迭代，” *国际人工智能联合会议*，2015年。'
- en: '[95] B. Piot, M. Geist, and O. Pietquin, “Boosted bellman residual minimization
    handling expert demonstrations,” *Joint European Conference on Machine Learning
    and Knowledge Discovery in Databases*, 2014.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] B. Piot, M. Geist 和 O. Pietquin，“增强 Bellman 残差最小化处理专家演示，” *第六届欧洲机器学习与知识发现数据库联合会议*，2014年。'
- en: '[96] T. Brys, A. Harutyunyan, H. B. Suay, S. Chernova, M. E. Taylor, and A. Nowé,
    “Reinforcement learning from demonstration through shaping,” *International Joint
    Conference on Artificial Intelligence*, 2015.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] T. Brys, A. Harutyunyan, H. B. Suay, S. Chernova, M. E. Taylor 和 A. Nowé，“通过塑形从演示中进行强化学习，”
    *国际人工智能联合会议*，2015年。'
- en: '[97] B. Kang, Z. Jie, and J. Feng, “Policy optimization with demonstrations,”
    *ICML*, 2018.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] B. Kang, Z. Jie 和 J. Feng，“带有演示的策略优化，” *ICML*，2018年。'
- en: '[98] D. P. Bertsekas, “Approximate policy iteration: A survey and some new
    methods,” *Journal of Control Theory and Applications*, 2011.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] D. P. Bertsekas，“近似策略迭代：综述和一些新方法，” *控制理论与应用杂志*，2011年。'
- en: '[99] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience
    replay,” *ICLR*, 2016.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] T. Schaul, J. Quan, I. Antonoglou 和 D. Silver，“优先经验回放，” *ICLR*，2016年。'
- en: '[100] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning
    and structured prediction to no-regret online learning,” *AISTATS*, 2011.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] S. Ross, G. Gordon 和 D. Bagnell，“模仿学习和结构化预测的无悔在线学习的简化，” *AISTATS*，2011年。'
- en: '[101] Y. Gao, J. Lin, F. Yu, S. Levine, T. Darrell *et al.*, “Reinforcement
    learning from imperfect demonstrations,” *arXiv preprint arXiv:1802.05313*, 2018.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Y. Gao, J. Lin, F. Yu, S. Levine, T. Darrell *等*，“从不完美的演示中进行强化学习，” *arXiv
    预印本 arXiv:1802.05313*，2018年。'
- en: '[102] M. Jing, X. Ma, W. Huang, F. Sun, C. Yang, B. Fang, and H. Liu, “Reinforcement
    learning from imperfect demonstrations under soft expert guidance.” *AAAI*, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] M. Jing, X. Ma, W. Huang, F. Sun, C. Yang, B. Fang 和 H. Liu，“在软专家指导下从不完美的演示中进行强化学习。”
    *AAAI*，2020年。'
- en: '[103] K. Brantley, W. Sun, and M. Henaff, “Disagreement-regularized imitation
    learning,” *ICLR*, 2019.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] K. Brantley, W. Sun 和 M. Henaff，“不一致正则化的模仿学习，” *ICLR*，2019年。'
- en: '[104] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” *Deep Learning and Representation Learning Workshop, NeurIPS*, 2014.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] G. Hinton, O. Vinyals 和 J. Dean，“提炼神经网络中的知识，” *深度学习与表示学习研讨会，NeurIPS*，2014年。'
- en: '[105] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick,
    R. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell, “Policy distillation,” *arXiv
    preprint arXiv:1511.06295*, 2015.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick,
    R. Pascanu, V. Mnih, K. Kavukcuoglu 和 R. Hadsell，“策略蒸馏，” *arXiv 预印本 arXiv:1511.06295*，2015年。'
- en: '[106] H. Yin and S. J. Pan, “Knowledge transfer for deep reinforcement learning
    with hierarchical experience replay,” *AAAI*, 2017.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] H. Yin 和 S. J. Pan，“带有层次经验回放的深度强化学习中的知识迁移，” *AAAI*，2017年。'
- en: '[107] S. Schmitt, J. J. Hudson, A. Zidek, S. Osindero, C. Doersch, W. M. Czarnecki,
    J. Z. Leibo, H. Kuttler, A. Zisserman, K. Simonyan *et al.*, “Kickstarting deep
    reinforcement learning,” *arXiv preprint arXiv:1803.03835*, 2018.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] S. Schmitt, J. J. Hudson, A. Zidek, S. Osindero, C. Doersch, W. M. Czarnecki,
    J. Z. Leibo, H. Kuttler, A. Zisserman, K. Simonyan *等*，“启动深度强化学习，” *arXiv 预印本
    arXiv:1803.03835*，2018年。'
- en: '[108] J. Schulman, X. Chen, and P. Abbeel, “Equivalence between policy gradients
    and soft q-learning,” *arXiv preprint arXiv:1704.06440*, 2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] J. Schulman, X. Chen 和 P. Abbeel，“策略梯度与软 Q 学习之间的等价性，” *arXiv 预印本 arXiv:1704.06440*，2017年。'
- en: '[109] F. Fernández and M. Veloso, “Probabilistic policy reuse in a reinforcement
    learning agent,” *Proceedings of the fifth international joint conference on Autonomous
    agents and multiagent systems*, 2006.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] F. Fernández 和 M. Veloso，“强化学习代理中的概率性策略重用，” *第五届国际联合会议上的自主代理和多代理系统会议论文集*，2006年。'
- en: '[110] A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. P. van Hasselt,
    and D. Silver, “Successor features for transfer in reinforcement learning,” *NuerIPS*,
    2017.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. P. van Hasselt,
    和 D. Silver，“强化学习中的继任特征，” *NeurIPS*，2017。'
- en: '[111] R. Bellman, “Dynamic programming,” *Science*, 1966.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] R. Bellman，“动态规划，” *科学*，1966。'
- en: '[112] L. Torrey, T. Walker, J. Shavlik, and R. Maclin, “Using advice to transfer
    knowledge acquired in one reinforcement learning task to another,” *European Conference
    on Machine Learning*, 2005.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] L. Torrey, T. Walker, J. Shavlik, 和 R. Maclin，“利用建议将一种强化学习任务中获得的知识转移到另一任务中，”
    *欧洲机器学习会议*，2005。'
- en: '[113] A. Gupta, C. Devin, Y. Liu, P. Abbeel, and S. Levine, “Learning invariant
    feature spaces to transfer skills with reinforcement learning,” *ICLR*, 2017.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] A. Gupta, C. Devin, Y. Liu, P. Abbeel, 和 S. Levine，“学习不变特征空间以通过强化学习转移技能，”
    *ICLR*，2017。'
- en: '[114] G. Konidaris and A. Barto, “Autonomous shaping: Knowledge transfer in
    reinforcement learning,” *ICML*, 2006.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] G. Konidaris 和 A. Barto，“自主塑造：强化学习中的知识转移，” *ICML*，2006。'
- en: '[115] H. B. Ammar and M. E. Taylor, “Reinforcement learning transfer via common
    subspaces,” *Proceedings of the 11th International Conference on Adaptive and
    Learning Agents*, 2012.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] H. B. Ammar 和 M. E. Taylor，“通过共同子空间进行强化学习转移，” *第11届自适应和学习代理国际会议论文集*，2012。'
- en: '[116] V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional
    encoder-decoder architecture for image segmentation,” *IEEE transactions on pattern
    analysis and machine intelligence*, 2017.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] V. Badrinarayanan, A. Kendall, 和 R. Cipolla，“Segnet：一种用于图像分割的深度卷积编码器-解码器架构，”
    *IEEE模式分析与机器智能学报*，2017。'
- en: '[117] C. Wang and S. Mahadevan, “Manifold alignment without correspondence,”
    *International Joint Conference on Artificial Intelligence*, 2009.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] C. Wang 和 S. Mahadevan，“无对应关系的流形对齐，” *国际人工智能联合会议*，2009。'
- en: '[118] B. Bocsi, L. Csató, and J. Peters, “Alignment-based transfer learning
    for robot models,” *The 2013 International Joint Conference on Neural Networks
    (IJCNN)*, 2013.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] B. Bocsi, L. Csató, 和 J. Peters，“基于对齐的机器人模型转移学习，” *2013年国际神经网络联合会议（IJCNN）*，2013。'
- en: '[119] H. B. Ammar, E. Eaton, P. Ruvolo, and M. E. Taylor, “Unsupervised cross-domain
    transfer in policy gradient reinforcement learning via manifold alignment,” *AAAI*,
    2015.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] H. B. Ammar, E. Eaton, P. Ruvolo, 和 M. E. Taylor，“通过流形对齐在策略梯度强化学习中进行无监督跨领域转移，”
    *AAAI*，2015。'
- en: '[120] H. B. Ammar, K. Tuyls, M. E. Taylor, K. Driessens, and G. Weiss, “Reinforcement
    learning transfer via sparse coding,” *ICAAMS*, 2012.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] H. B. Ammar, K. Tuyls, M. E. Taylor, K. Driessens, 和 G. Weiss，“通过稀疏编码进行强化学习转移，”
    *ICAAMS*，2012。'
- en: '[121] A. Lazaric, M. Restelli, and A. Bonarini, “Transfer of samples in batch
    reinforcement learning,” *ICML*, 2008.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] A. Lazaric, M. Restelli, 和 A. Bonarini，“批量强化学习中的样本转移，” *ICML*，2008。'
- en: '[122] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,
    K. Kavukcuoglu, R. Pascanu, and R. Hadsell, “Progressive neural networks,” *arXiv
    preprint arXiv:1606.04671*, 2016.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,
    K. Kavukcuoglu, R. Pascanu, 和 R. Hadsell，“渐进神经网络，” *arXiv 预印本 arXiv:1606.04671*，2016。'
- en: '[123] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu, A. Pritzel,
    and D. Wierstra, “Pathnet: Evolution channels gradient descent in super neural
    networks,” *arXiv preprint arXiv:1701.08734*, 2017.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu, A. Pritzel,
    和 D. Wierstra，“Pathnet：在超神经网络中演化通道梯度下降，” *arXiv 预印本 arXiv:1701.08734*，2017。'
- en: '[124] I. Harvey, “The microbial genetic algorithm,” *European Conference on
    Artificial Life*, 2009.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] I. Harvey，“微生物遗传算法，” *欧洲人工生命会议*，2009。'
- en: '[125] A. Zhang, H. Satija, and J. Pineau, “Decoupling dynamics and reward for
    transfer learning,” *arXiv preprint arXiv:1804.10689*, 2018.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] A. Zhang, H. Satija, 和 J. Pineau，“分解动态和奖励以进行迁移学习，” *arXiv 预印本 arXiv:1804.10689*，2018。'
- en: '[126] P. Dayan, “Improving generalization for temporal difference learning:
    The successor representation,” *Neural Computation*, 1993.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] P. Dayan，“提高时间差分学习的一般化能力：继任表示，” *神经计算*，1993。'
- en: '[127] T. D. Kulkarni, A. Saeedi, S. Gautam, and S. J. Gershman, “Deep successor
    reinforcement learning,” *arXiv preprint arXiv:1606.02396*, 2016.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] T. D. Kulkarni, A. Saeedi, S. Gautam, 和 S. J. Gershman，“深度继任强化学习，” *arXiv
    预印本 arXiv:1606.02396*，2016。'
- en: '[128] J. Zhang, J. T. Springenberg, J. Boedecker, and W. Burgard, “Deep reinforcement
    learning with successor features for navigation across similar environments,”
    *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*,
    2017.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] J. Zhang, J. T. Springenberg, J. Boedecker, 和 W. Burgard，“利用继任特征的深度强化学习在相似环境中的导航，”
    *IEEE/RSJ 国际智能机器人与系统大会 (IROS)*，2017。'
- en: '[129] N. Mehta, S. Natarajan, P. Tadepalli, and A. Fern, “Transfer in variable-reward
    hierarchical reinforcement learning,” *Machine Learning*, 2008.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] N. Mehta, S. Natarajan, P. Tadepalli, 和 A. Fern，“在可变奖励层次强化学习中的迁移，” *机器学习*，2008。'
- en: '[130] D. Borsa, A. Barreto, J. Quan, D. Mankowitz, R. Munos, H. van Hasselt,
    D. Silver, and T. Schaul, “Universal successor features approximators,” *ICLR*,
    2019.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] D. Borsa, A. Barreto, J. Quan, D. Mankowitz, R. Munos, H. van Hasselt,
    D. Silver, 和 T. Schaul，“通用继任特征逼近器，” *ICLR*，2019。'
- en: '[131] L. Lehnert, S. Tellex, and M. L. Littman, “Advantages and limitations
    of using successor features for transfer in reinforcement learning,” *arXiv preprint
    arXiv:1708.00102*, 2017.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] L. Lehnert, S. Tellex, 和 M. L. Littman，“使用继任特征进行强化学习转移的优缺点，” *arXiv 预印本
    arXiv:1708.00102*，2017。'
- en: '[132] J. C. Petangoda, S. Pascual-Diaz, V. Adam, P. Vrancx, and J. Grau-Moya,
    “Disentangled skill embeddings for reinforcement learning,” *arXiv preprint arXiv:1906.09223*,
    2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] J. C. Petangoda, S. Pascual-Diaz, V. Adam, P. Vrancx, 和 J. Grau-Moya，“用于强化学习的解耦技能嵌入，”
    *arXiv 预印本 arXiv:1906.09223*，2019。'
- en: '[133] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” *ICML*, 2017.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] C. Finn, P. Abbeel, 和 S. Levine，“模型无关的元学习用于深度网络的快速适应，” *ICML*，2017。'
- en: '[134] B. Zadrozny, “Learning and evaluating classifiers under sample selection
    bias,” *ICML*, 2004.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] B. Zadrozny，“在样本选择偏差下学习和评估分类器，” *ICML*，2004。'
- en: '[135] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, “A survey of robot
    learning from demonstration,” *Robotics and autonomous systems*, 2009.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] B. D. Argall, S. Chernova, M. Veloso, 和 B. Browning，“基于演示的机器人学习综述，” *机器人与自主系统*，2009。'
- en: '[136] B. Kehoe, S. Patil, P. Abbeel, and K. Goldberg, “A survey of research
    on cloud robotics and automation,” *IEEE Transactions on automation science and
    engineering*, 2015.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] B. Kehoe, S. Patil, P. Abbeel, 和 K. Goldberg，“关于云机器人和自动化的研究综述，” *IEEE
    自动化科学与工程学报*，2015。'
- en: '[137] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement learning
    for robotic manipulation with asynchronous off-policy updates,” *IEEE international
    conference on robotics and automation (ICRA)*, 2017.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] S. Gu, E. Holly, T. Lillicrap, 和 S. Levine，“用于机器人操作的深度强化学习与异步脱离策略更新，”
    *IEEE 国际机器人与自动化大会 (ICRA)*，2017。'
- en: '[138] W. Yu, J. Tan, C. K. Liu, and G. Turk, “Preparing for the unknown: Learning
    a universal policy with online system identification,” *arXiv preprint arXiv:1702.02453*,
    2017.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] W. Yu, J. Tan, C. K. Liu, 和 G. Turk，“为未知做好准备：通过在线系统识别学习通用策略，” *arXiv
    预印本 arXiv:1702.02453*，2017。'
- en: '[139] F. Sadeghi and S. Levine, “Cad2rl: Real single-image flight without a
    single real image,” *arXiv preprint arXiv:1611.04201*, 2016.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] F. Sadeghi 和 S. Levine，“Cad2rl：无需真实图像的真实单图像飞行，” *arXiv 预印本 arXiv:1611.04201*，2016。'
- en: '[140] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan,
    L. Downs, J. Ibarz, P. Pastor, K. Konolige *et al.*, “Using simulation and domain
    adaptation to improve efficiency of deep robotic grasping,” *IEEE International
    Conference on Robotics and Automation (ICRA)*, 2018.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan,
    L. Downs, J. Ibarz, P. Pastor, K. Konolige *等*，“利用仿真和领域适应提高深度机器人抓取的效率，” *IEEE
    国际机器人与自动化大会 (ICRA)*，2018。'
- en: '[141] H. Bharadhwaj, Z. Wang, Y. Bengio, and L. Paull, “A data-efficient framework
    for training and sim-to-real transfer of navigation policies,” *International
    Conference on Robotics and Automation (ICRA)*, 2019.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] H. Bharadhwaj, Z. Wang, Y. Bengio, 和 L. Paull，“用于训练和仿真到现实转移的导航策略的数据高效框架，”
    *国际机器人与自动化大会 (ICRA)*，2019。'
- en: '[142] I. Higgins, A. Pal, A. Rusu, L. Matthey, C. Burgess, A. Pritzel, M. Botvinick,
    C. Blundell, and A. Lerchner, “Darla: Improving zero-shot transfer in reinforcement
    learning,” *ICML*, 2017.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] I. Higgins, A. Pal, A. Rusu, L. Matthey, C. Burgess, A. Pritzel, M. Botvinick,
    C. Blundell, 和 A. Lerchner，“Darla：改进强化学习中的零样本迁移，” *ICML*，2017。'
- en: '[143] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement learning in robotics:
    A survey,” *The International Journal of Robotics Research*, 2013.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] J. Kober, J. A. Bagnell, 和 J. Peters，“机器人中的强化学习：综述，” *国际机器人研究期刊*，2013。'
- en: '[144] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez,
    T. Hubert, L. Baker, M. Lai, A. Bolton *et al.*, “Mastering the game of go without
    human knowledge,” *Nature*, 2017.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A.
    Guez, T. Hubert, L. Baker, M. Lai, A. Bolton *等*，“在没有人类知识的情况下掌握围棋游戏，” *自然*，2017
    年。'
- en: '[145] OpenAI. (2019) Dotal2 blog. [Online]. Available: [https://openai.com/blog/openai-five/](https://openai.com/blog/openai-five/)'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] OpenAI. (2019) Dotal2 博客。 [在线]. 网址: [https://openai.com/blog/openai-five/](https://openai.com/blog/openai-five/)'
- en: '[146] J. Oh, V. Chockalingam, S. Singh, and H. Lee, “Control of memory, active
    perception, and action in minecraft,” *arXiv preprint arXiv:1605.09128*, 2016.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] J. Oh, V. Chockalingam, S. Singh 和 H. Lee, “在 Minecraft 中控制记忆、主动感知和行动，”
    *arXiv 预印本 arXiv:1605.09128*，2016 年。'
- en: '[147] N. Justesen, P. Bontrager, J. Togelius, and S. Risi, “Deep learning for
    video game playing,” *IEEE Transactions on Games*, 2019.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] N. Justesen, P. Bontrager, J. Togelius 和 S. Risi, “用于视频游戏的深度学习，” *IEEE
    游戏学报*，2019 年。'
- en: '[148] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,
    and M. Riedmiller, “Playing atari with deep reinforcement learning,” *arXiv preprint
    arXiv:1312.5602*, 2013.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra
    和 M. Riedmiller, “使用深度强化学习玩 Atari 游戏，” *arXiv 预印本 arXiv:1312.5602*，2013 年。'
- en: '[149] H. Chen, X. Liu, D. Yin, and J. Tang, “A survey on dialogue systems:
    Recent advances and new frontiers,” *Acm Sigkdd Explorations Newsletter*, 2017.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] H. Chen, X. Liu, D. Yin 和 J. Tang, “对话系统调查：近期进展与新前沿，” *ACM Sigkdd 探索通讯*，2017
    年。'
- en: '[150] S. P. Singh, M. J. Kearns, D. J. Litman, and M. A. Walker, “Reinforcement
    learning for spoken dialogue systems,” *NeurIPS*, 2000.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] S. P. Singh, M. J. Kearns, D. J. Litman 和 M. A. Walker, “用于口语对话系统的强化学习，”
    *NeurIPS*，2000 年。'
- en: '[151] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement
    learning,” *arXiv preprint arXiv:1611.01578*, 2016.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] B. Zoph 和 Q. V. Le, “使用强化学习进行神经网络架构搜索，” *arXiv 预印本 arXiv:1611.01578*，2016
    年。'
- en: '[152] R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko, “Learning
    to reason: End-to-end module networks for visual question answering,” *IEEE International
    Conference on Computer Vision*, 2017.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] R. Hu, J. Andreas, M. Rohrbach, T. Darrell 和 K. Saenko, “学习推理：用于视觉问题回答的端到端模块网络，”
    *IEEE 国际计算机视觉会议*，2017 年。'
- en: '[153] Z. Ren, X. Wang, N. Zhang, X. Lv, and L.-J. Li, “Deep reinforcement learning-based
    image captioning with embedding reward,” *IEEE Conference on Computer Vision and
    Pattern Recognition*, 2017.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Z. Ren, X. Wang, N. Zhang, X. Lv 和 L.-J. Li, “基于深度强化学习的图像描述生成与嵌入奖励，”
    *IEEE 计算机视觉与模式识别会议*，2017 年。'
- en: '[154] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, “Learning to compose
    neural networks for question answering,” *arXiv preprint arXiv:1601.01705*, 2016.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] J. Andreas, M. Rohrbach, T. Darrell 和 D. Klein, “学习为问题回答组合神经网络，” *arXiv
    预印本 arXiv:1601.01705*，2016 年。'
- en: '[155] D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville,
    and Y. Bengio, “An actor-critic algorithm for sequence prediction,” *arXiv preprint
    arXiv:1607.07086*, 2016.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville
    和 Y. Bengio, “用于序列预测的演员-评论家算法，” *arXiv 预印本 arXiv:1607.07086*，2016 年。'
- en: '[156] F. Godin, A. Kumar, and A. Mittal, “Learning when not to answer: a ternary
    reward structure for reinforcement learning based question answering,” *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, 2019.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] F. Godin, A. Kumar 和 A. Mittal, “学习何时不回答：用于强化学习的三元奖励结构的问题回答，” *2019 年北美计算语言学协会会议：人类语言技术*，2019
    年。'
- en: '[157] K.-W. Chang, A. Krishnamurthy, A. Agarwal, J. Langford, and H. Daumé III,
    “Learning to search better than your teacher,” 2015.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] K.-W. Chang, A. Krishnamurthy, A. Agarwal, J. Langford 和 H. Daumé III,
    “学习比你的老师更好地搜索，” 2015 年。'
- en: '[158] J. Lu, A. Kannan, J. Yang, D. Parikh, and D. Batra, “Best of both worlds:
    Transferring knowledge from discriminative learning to a generative visual dialog
    model,” *NeurIPS*, 2017.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] J. Lu, A. Kannan, J. Yang, D. Parikh 和 D. Batra, “两全其美：将辨别学习中的知识转移到生成视觉对话模型中，”
    *NeurIPS*，2017 年。'
- en: '[159] OpenAI, “Gpt-4 technical report,” *arXiv*, 2023.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] OpenAI, “GPT-4 技术报告，” *arXiv*，2023 年。'
- en: '[160] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds,
    M. Rauh, L. Weidinger, M. Chadwick, P. Thacker *et al.*, “Improving alignment
    of dialogue agents via targeted human judgements,” *arXiv preprint arXiv:2209.14375*,
    2022.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds,
    M. Rauh, L. Weidinger, M. Chadwick, P. Thacker *等*，“通过针对性人类判断提高对话体的对齐，” *arXiv
    预印本 arXiv:2209.14375*，2022 年。'
- en: '[161] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,
    P. Barham, H. W. Chung, C. Sutton, S. Gehrmann *et al.*, “Palm: Scaling language
    modeling with pathways,” *arXiv preprint arXiv:2204.02311*, 2022.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,
    P. Barham, H. W. Chung, C. Sutton, S. Gehrmann *等人*, “Palm：通过路径扩展语言建模，” *arXiv预印本
    arXiv:2204.02311*, 2022年。'
- en: '[162] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.
    Cheng, A. Jin, T. Bos, L. Baker, Y. Du *et al.*, “Lamda: Language models for dialog
    applications,” *arXiv preprint arXiv:2201.08239*, 2022.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.
    Cheng, A. Jin, T. Bos, L. Baker, Y. Du *等人*, “Lamda：对话应用的语言模型，” *arXiv预印本 arXiv:2201.08239*,
    2022年。'
- en: '[163] C. Yu, J. Liu, and S. Nemati, “Reinforcement learning in healthcare:
    A survey,” *arXiv preprint arXiv:1908.08796*, 2019.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] C. Yu, J. Liu, 和 S. Nemati, “医疗保健中的强化学习：综述，” *arXiv预印本 arXiv:1908.08796*,
    2019年。'
- en: '[164] A. Alansary, O. Oktay, Y. Li, L. Le Folgoc, B. Hou, G. Vaillant, K. Kamnitsas,
    A. Vlontzos, B. Glocker, B. Kainz *et al.*, “Evaluating reinforcement learning
    agents for anatomical landmark detection,” 2019.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] A. Alansary, O. Oktay, Y. Li, L. Le Folgoc, B. Hou, G. Vaillant, K. Kamnitsas,
    A. Vlontzos, B. Glocker, B. Kainz *等人*, “评估用于解剖标志检测的强化学习智能体，” 2019年。'
- en: '[165] K. Ma, J. Wang, V. Singh, B. Tamersoy, Y.-J. Chang, A. Wimmer, and T. Chen,
    “Multimodal image registration with deep context reinforcement learning,” *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*, 2017.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] K. Ma, J. Wang, V. Singh, B. Tamersoy, Y.-J. Chang, A. Wimmer, 和 T. Chen,
    “通过深度上下文强化学习进行多模态图像配准，” *国际医学图像计算与计算机辅助手术会议*, 2017年。'
- en: '[166] T. S. M. T. Gomes, “Reinforcement learning for primary care e appointment
    scheduling,” 2017.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] T. S. M. T. Gomes, “用于初级保健预约调度的强化学习，” 2017年。'
- en: '[167] A. Serrano, B. Imbernón, H. Pérez-Sánchez, J. M. Cecilia, A. Bueno-Crespo,
    and J. L. Abellán, “Accelerating drugs discovery with deep reinforcement learning:
    An early approach,” *International Conference on Parallel Processing Companion*,
    2018.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] A. Serrano, B. Imbernón, H. Pérez-Sánchez, J. M. Cecilia, A. Bueno-Crespo,
    和 J. L. Abellán, “通过深度强化学习加速药物发现：一种早期方法，” *国际并行处理会议附录*, 2018年。'
- en: '[168] M. Popova, O. Isayev, and A. Tropsha, “Deep reinforcement learning for
    de novo drug design,” *Science advances*, 2018.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] M. Popova, O. Isayev, 和 A. Tropsha, “用于新药设计的深度强化学习，” *Science advances*,
    2018年。'
- en: '[169] A. E. Gaweda, M. K. Muezzinoglu, G. R. Aronoff, A. A. Jacobs, J. M. Zurada,
    and M. E. Brier, “Incorporating prior knowledge into q-learning for drug delivery
    individualization,” *Fourth International Conference on Machine Learning and Applications*,
    2005.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] A. E. Gaweda, M. K. Muezzinoglu, G. R. Aronoff, A. A. Jacobs, J. M. Zurada,
    和 M. E. Brier, “将先验知识融入q学习以个性化药物递送，” *第四届国际机器学习与应用会议*, 2005年。'
- en: '[170] T. W. Killian, S. Daulton, G. Konidaris, and F. Doshi-Velez, “Robust
    and efficient transfer learning with hidden parameter markov decision processes,”
    *NeurIPS*, 2017.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] T. W. Killian, S. Daulton, G. Konidaris, 和 F. Doshi-Velez, “具有隐藏参数马尔可夫决策过程的鲁棒高效迁移学习，”
    *NeurIPS*, 2017年。'
- en: '[171] A. Holzinger, “Interactive machine learning for health informatics: when
    do we need the human-in-the-loop?” *Brain Informatics*, 2016.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] A. Holzinger, “健康信息学中的互动机器学习：我们何时需要人机协作？” *Brain Informatics*, 2016年。'
- en: '[172] L. Li, Y. Lv, and F.-Y. Wang, “Traffic signal timing via deep reinforcement
    learning,” *IEEE/CAA Journal of Automatica Sinica*, 2016.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] L. Li, Y. Lv, 和 F.-Y. Wang, “通过深度强化学习进行交通信号时序控制，” *IEEE/CAA 自动化学报*, 2016年。'
- en: '[173] K. Lin, R. Zhao, Z. Xu, and J. Zhou, “Efficient large-scale fleet management
    via multi-agent deep reinforcement learning,” *ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*, 2018.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] K. Lin, R. Zhao, Z. Xu, 和 J. Zhou, “通过多智能体深度强化学习实现大规模车队管理，” *ACM SIGKDD国际知识发现与数据挖掘会议*,
    2018年。'
- en: '[174] K.-L. A. Yau, J. Qadir, H. L. Khoo, M. H. Ling, and P. Komisarczuk, “A
    survey on reinforcement learning models and algorithms for traffic signal control,”
    *ACM Computing Surveys (CSUR)*, 2017.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] K.-L. A. Yau, J. Qadir, H. L. Khoo, M. H. Ling, 和 P. Komisarczuk, “关于交通信号控制的强化学习模型和算法的综述，”
    *ACM Computing Surveys (CSUR)*, 2017年。'
- en: '[175] J. Moody, L. Wu, Y. Liao, and M. Saffell, “Performance functions and
    reinforcement learning for trading systems and portfolios,” *Journal of Forecasting*,
    1998.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] J. Moody, L. Wu, Y. Liao, 和 M. Saffell, “用于交易系统和投资组合的绩效函数和强化学习，” *预测期刊*,
    1998年。'
- en: '[176] Z. Jiang and J. Liang, “Cryptocurrency portfolio management with deep
    reinforcement learning,” *IEEE Intelligent Systems Conference (IntelliSys)*, 2017.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Z. Jiang 和 J. Liang, “利用深度强化学习进行加密货币投资组合管理，” *IEEE智能系统会议 (IntelliSys)*,
    2017年。'
- en: '[177] R. Neuneier, “Enhancing q-learning for optimal asset allocation,” *NeurIPS*,
    1998.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] R. Neuneier, “增强 q 学习以实现最佳资产配置，” *NeurIPS*，1998 年。'
- en: '[178] Y. Deng, F. Bao, Y. Kong, Z. Ren, and Q. Dai, “Deep direct reinforcement
    learning for financial signal representation and trading,” *IEEE transactions
    on neural networks and learning systems*, 2016.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Y. Deng, F. Bao, Y. Kong, Z. Ren 和 Q. Dai, “用于金融信号表示和交易的深度直接强化学习，” *IEEE
    神经网络与学习系统交易*，2016 年。'
- en: '[179] G. Dalal, E. Gilboa, and S. Mannor, “Hierarchical decision making in
    electricity grid management,” *International Conference on Machine Learning*,
    2016.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] G. Dalal, E. Gilboa 和 S. Mannor, “电力网管理中的层次决策，” *国际机器学习会议*，2016 年。'
- en: '[180] F. Ruelens, B. J. Claessens, S. Vandael, B. De Schutter, R. Babuška,
    and R. Belmans, “Residential demand response of thermostatically controlled loads
    using batch reinforcement learning,” *IEEE Transactions on Smart Grid*, 2016.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] F. Ruelens, B. J. Claessens, S. Vandael, B. De Schutter, R. Babuška 和
    R. Belmans, “使用批量强化学习的热控负荷住宅需求响应，” *IEEE 智能电网交易*，2016 年。'
- en: '[181] Z. Wen, D. O’Neill, and H. Maei, “Optimal demand response using device-based
    reinforcement learning,” *IEEE Transactions on Smart Grid*, 2015.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Z. Wen, D. O’Neill 和 H. Maei, “基于设备的强化学习实现最佳需求响应，” *IEEE 智能电网交易*，2015
    年。'
- en: '[182] Y. Li, J. Song, and S. Ermon, “Infogail: Interpretable imitation learning
    from visual demonstrations,” *NeurIPS*, 2017.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Y. Li, J. Song 和 S. Ermon, “Infogail: 从视觉演示中学习的可解释模仿学习，” *NeurIPS*，2017
    年。'
- en: '[183] R. Ramakrishnan and J. Shah, “Towards interpretable explanations for
    transfer learning in sequential tasks,” *AAAI Spring Symposium Series*, 2016.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] R. Ramakrishnan 和 J. Shah, “迈向可解释的迁移学习解释用于顺序任务，” *AAAI 春季研讨会系列*，2016
    年。'
- en: '[184] E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and W. Stewart,
    “Retain: An interpretable predictive model for healthcare using reverse time attention
    mechanism,” *NeurIPS*, vol. 29, 2016.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz 和 W. Stewart, “Retain:
    一种使用逆向时间注意机制的可解释预测模型用于医疗保健，” *NeurIPS*，第 29 卷，2016 年。'
- en: '| ![[Uncaptioned image]](img/b34d65ded02e12d33b5657669c3a5105.png) | Zhuangdi
    Zhu is currently a senior data and applied scientist with Microsoft. She obtained
    her Ph.D. degree from the Computer Science department of Michigan State University.
    Zhuangdi has regularly published on prestigious machine learning conferences including
    NeurIPs, ICML, KDD, AAAI, etc. Her research interests reside in both fundamental
    and applied machine learning. Her current research involves reinforcement learning
    and distributed machine learning. |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/b34d65ded02e12d33b5657669c3a5105.png) | Zhuangdi Zhu 目前是微软的一名高级数据与应用科学家。她在密歇根州立大学计算机科学系获得了博士学位。Zhuangdi
    经常在包括 NeurIPs、ICML、KDD、AAAI 等在内的顶级机器学习会议上发表论文。她的研究兴趣涉及基础和应用机器学习。她目前的研究包括强化学习和分布式机器学习。
    |'
- en: '| ![[Uncaptioned image]](img/b22c54ee22751df2b5812a557e7b3f29.png) | Kaixiang
    Lin is an applied scientist at Amazon web services. He obtained his Ph.D. from
    Michigan State University. He has broad research interests across multiple fields,
    including reinforcement learning, human-robot interactions, and natural language
    processing. His research has been published on multiple top-tiered machine learning
    and data mining conferences such as ICLR, KDD, NeurIPS, etc. He serves as a reviewer
    for top machine learning conferences regularly. |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/b22c54ee22751df2b5812a557e7b3f29.png) | Kaixiang Lin 是亚马逊网络服务的一名应用科学家。他在密歇根州立大学获得了博士学位。他的研究兴趣广泛，涵盖多个领域，包括强化学习、人机交互和自然语言处理。他的研究成果已在多个顶级机器学习和数据挖掘会议上发表，如
    ICLR、KDD、NeurIPS 等。他定期担任顶级机器学习会议的审稿人。 |'
- en: '| ![[Uncaptioned image]](img/75592eae391f3c317ea8a710b4bba4ae.png) | Anil K.
    Jain is a University distinguished professor in the Department of Computer Science
    and Engineering at Michigan State University. His research interests include pattern
    recognition and biometric authentication. He served as the editor-in-chief of
    the IEEE Transactions on Pattern Analysis and Machine Intelligence and was a member
    of the United States Defense Science Board. He has received Fulbright, Guggenheim,
    Alexander von Humboldt, and IAPR King Sun Fu awards. He is a member of the National
    Academy of Engineering and a foreign fellow of the Indian National Academy of
    Engineering and the Chinese Academy of Sciences. |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/75592eae391f3c317ea8a710b4bba4ae.png) | **Anil K. Jain** 是密歇根州立大学计算机科学与工程系的大学杰出教授。他的研究兴趣包括模式识别和生物特征认证。他曾担任《IEEE模式分析与机器智能汇刊》的主编，并且是美国国防科学委员会的成员。他曾获得富布赖特奖、古根海姆奖、亚历山大·冯·洪堡奖和IAPR
    King Sun Fu奖。他是国家工程院院士，同时也是印度国家工程院和中国科学院的外籍院士。 |'
- en: '| ![[Uncaptioned image]](img/960f94200352c9ade1bd175843ea8284.png) | Jiayu
    Zhou is an associate professor in the Department of Computer Science and Engineering
    at Michigan State University. He received his Ph.D. degree in computer science
    from Arizona State University in 2014\. He has broad research interests in the
    fields of large-scale machine learning and data mining as well as biomedical informatics.
    He has served as a technical program committee member for premier conferences
    such as NIPS, ICML, and SIGKDD. His papers have received the Best Student Paper
    Award at the 2014 IEEE International Conference on Data Mining (ICDM), the Best
    Student Paper Award at the 2016 International Symposium on Biomedical Imaging
    (ISBI) and the Best Paper Award at IEEE Big Data 2016. |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/960f94200352c9ade1bd175843ea8284.png) | **Jiayu Zhou** 是密歇根州立大学计算机科学与工程系的副教授。他于2014年在亚利桑那州立大学获得计算机科学博士学位。他的研究兴趣广泛，涵盖大规模机器学习和数据挖掘以及生物医学信息学等领域。他曾作为技术程序委员会成员参与NIPS、ICML和SIGKDD等顶级会议。他的论文曾获得2014年IEEE国际数据挖掘大会（ICDM）最佳学生论文奖、2016年国际生物医学成像研讨会（ISBI）最佳学生论文奖以及IEEE
    Big Data 2016最佳论文奖。'
