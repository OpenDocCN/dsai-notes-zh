- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:44:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2208.13363] Survey: Exploiting Data Redundancy for Optimization of Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2208.13363](https://ar5iv.labs.arxiv.org/html/2208.13363)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Survey: Exploiting Data Redundancy for Optimization of Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jou-An Chen [jchen73@ncsu.edu](mailto:jchen73@ncsu.edu) Department of Computer
    Science, North Carolina State UniversityUSA ,  Wei Niu [wniu@email.wm.edu](mailto:wniu@email.wm.edu)
    ,  Bin Ren [bren@cs.wm.edu](mailto:bren@cs.wm.edu) Department of Computer Science,
    William & MaryUSA ,  Yanzhi Wang [yanz.wang@northeastern.edu](mailto:yanz.wang@northeastern.edu)
    Department of Electrical and Computer Engineering, Northeastern UniversityUSA
     and  Xipeng Shen [xshen5@ncsu.edu](mailto:xshen5@ncsu.edu) Department of Computer
    Science, North Carolina State UniversityUSA(2020)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Data redundancy is ubiquitous in the inputs and intermediate results of Deep
    Neural Networks (DNN). It offers many significant opportunities for improving
    DNN performance and efficiency and has been explored in a large body of work.
    These studies have scattered in many venues across several years. The targets
    they focus on range from images to videos and texts, and the techniques they use
    to detect and exploit data redundancy also vary in many aspects. There is not
    yet a systematic examination and summary of the many efforts, making it difficult
    for researchers to get a comprehensive view of the prior work, the state of the
    art, differences and shared principles, and the areas and directions yet to explore.
    This article tries to fill the void. It surveys hundreds of recent papers on the
    topic, introduces a novel taxonomy to put the various techniques into a single
    categorization framework, offers a comprehensive description of the main methods
    used for exploiting data redundancy in improving multiple kinds of DNNs on data,
    and points out a set of research opportunities for future to explore.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Redundancy, Representation Redundancy, Deep Neural Network, Convolutional
    Neural Network, Transformer^†^†copyright: acmcopyright^†^†journalyear: 2020^†^†doi:
    XXX^†^†booktitle: Survey: Leverage Data Redundancy for DNN Optimizations^†^†price:
    15.00^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs: Computing methodologies Machine
    Learning^†^†ccs: Computing methodologies Knowledge representation and reasoning'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning, primarily powered by Deep Neural Networks (DNN), has repeatedly
    demonstrated its success and tremendous potential in revolutionizing the development
    of Artificial Intelligence and its applications in various domains. At a high
    level, DNN has two pillars: algorithm and data. DNN models embody the algorithm,
    and data is represented by DNN inputs and intermediate results (or called activation
    maps). Both algorithm and data are essential for the quality of a DNN, determining
    its accuracy, speed, size, energy efficiency, robustness, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: For DNNs, redundancy exists in both models and data. As many studies have shown,
    reducing some DNN layers or parameters or precision often leaves the model accuracy
    intact. The observation has prompted a large body of research in DNN compression,
    which tries to find effective ways to reduce the size of a DNN model and achieve
    more compact models or/and faster speeds. Many of the efforts resort to model
    pruning and quantization, while some compress models in the frequency domains (Wang
    et al., [2016](#bib.bib154)) or optimize attentions in Transformer models (Clark
    et al., [2019](#bib.bib33); Michel et al., [2019](#bib.bib114); Voita et al.,
    [2019](#bib.bib148)). We call those efforts model redundancy exploitations. Several
    recent survey papers (Cheng et al., [2018](#bib.bib29); Blalock et al., [2020](#bib.bib10);
    Ganesh et al., [2021](#bib.bib52); Choudhary et al., [2020](#bib.bib32); Hubara
    et al., [2017](#bib.bib77); Guo, [2018](#bib.bib62)) have provided comprehensive
    overviews on the topic.
  prefs: []
  type: TYPE_NORMAL
- en: Data redundancy exploitation is no less critical for DNN and has received much
    and increasing attention in recent several years as well. Many studies have noticed
    similar patches within an image, activation maps, or between adjacent video frames.
    Other studies have confirmed that some words in a sentence carry no significant
    meanings for the sentence. These observations have prompted many recent efforts
    in creating methods for detecting and leveraging the redundancy of various dimensions
    in all kinds of data. Hundreds of papers have been published, scattering in many
    venues over several years. But unlike model redundancy, there is not yet a systematic
    examination and summary of the many efforts, making it difficult for researchers
    to get a comprehensive view of the prior work, to learn about state of the art,
    to understand the differences and common principles among the many published studies,
    or to find out the areas and directions yet to explore.
  prefs: []
  type: TYPE_NORMAL
- en: To the best of our knowledge, this paper offers the first one-stop resource
    for people interested in learning about studies on data redundancy for DNN. It
    provides a holistic view of the various techniques and their connections, offers
    insights on the limitations of state of the art, and potential directions and
    opportunities for the future to explore. The paper introduces the first known
    taxonomy on DNN data redundancy exploitation, putting the various topics’ various
    techniques into a single categorization framework. It offers a comprehensive description
    of the main techniques used for detecting and exploiting data redundancy in improving
    multiple kinds of DNNs, from CNNs to RNNs, Transformers, and so on. It presents
    numerous data redundancy opportunities in images, videos, and texts and how existing
    studies tap into them with various techniques at a spectrum of granularities and
    scopes. It discusses the commonalities among the techniques, their differences,
    limitations, and promising research directions worth future explorations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We organize the rest of the paper as follows. Section [2](#S2 "2\. Terminology
    and Scope of Discussion ‣ Survey: Exploiting Data Redundancy for Optimization
    of Deep Learning") provides a formal definition of data redundancy and defines
    the scope of this survey. Section [3](#S3 "3\. Taxonomy ‣ Survey: Exploiting Data
    Redundancy for Optimization of Deep Learning") presents a taxonomy of studies
    on DNN data redundancy. Sections [4](#S4 "4\. Leverage Redundancy in Image Data
    ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning"), [5](#S5
    "5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning"), and [6](#S6 "6\. Leverage Data Redundancy
    in Transformer Optimization for Text ‣ Survey: Exploiting Data Redundancy for
    Optimization of Deep Learning") describe the practical techniques that prior work
    has developed in detecting and exploiting data redundancy for DNNs respectively
    on images, videos, and text data. Section [7](#S7 "7\. Future Research Directions
    ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning") discusses
    the limitations of existing explorations and points out several future directions.
    Section [8](#S8 "8\. Conclusion ‣ Survey: Exploiting Data Redundancy for Optimization
    of Deep Learning") concludes the survey with a summary.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Terminology and Scope of Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start this section with clarifications of several terms essential for the
    rest of the paper, then define the scope of this survey, and explain the relations
    with several relevant concepts to our focus.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Terminology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Activation Map. In the traditional terminology, an activation map (or called
    feature map) is the activations of filters applied to the input of a DNN also
    to the output of the previous layer. For the visual representation of an image
    or a single image frame in a video with RGB channels, it is a 3D tensor with $Height\times
    Width\times Channel$; for a video clip, it is a 4D tensor with $Height\times Width\times
    Channel\times Depth$, the Depth is the number of frames. For ease of explanation,
    in this article, we extend the term activation map to include the inputs to a
    DNN (the values of the input layer neurons).
  prefs: []
  type: TYPE_NORMAL
- en: Hidden State. For text representation, before input to the first layer of the
    model, the text is transformed into word vectors representation via a word embedding
    (the collective name for a set of language modeling and feature learning techniques
    in natural language processing (NLP) where words or phrases from the vocabulary
    are mapped to vectors of real numbers (Embedding, [2020](#bib.bib44))). Multiplied
    with weights, the output activations of a layer are called a hidden state (which
    is also called encoder state). It is represented as a 2D tensor with sequence
    length $\times$ dimensionality of word embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Redundancy. According to the dictionary (of Redundancy by Oxford Dictionary,
    [2020](#bib.bib118)), redundancy is the state of being not or no longer needed
    or valuable. It is hence a concept relative to a particular purpose. In the context
    of DNN, being useful usually refers to contributing to the quality of the DNN
    outputs, which is often measured by a certain kind of accuracy metric. Data redundancy
    in DNN is hence defined as the data that is not or no longer useful for the quality
    of the outputs of DNN. Data redundancy exploitation for DNN refers to techniques
    that try to avoid data redundancy while keeping DNN outputs meeting the needs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Scope
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data redundancy in DNN manifests in different kinds of forms. Regardless of
    the concrete structure, data redundancy in DNN must fall into one of the following
    three categories: (1) Repeated information: Some part of the data conveys the
    identical (or similar) information as some other parts do. (2) Irrelevant information:
    The information conveyed by some part of the data is irrelevant to the targeted
    outputs of the DNN. (3) Over-detailed information: The information is conveyed
    in an unnecessarily detailed manner (e.g., image resolution). So any technique
    that tries to make a DNN avoid spending time and computations on any of the three
    types of information can be regarded as a technique for data redundancy exploitation.
    However, this general definition would blur the boundary between DNN algorithm,
    model, and data-centered optimizations. Some model optimizations (e.g., channel
    pruning (He et al., [2017](#bib.bib67); Wang et al., [2018](#bib.bib149); Zhou
    et al., [2019](#bib.bib165); Zhuang et al., [2018](#bib.bib171); Hou and Kung,
    [2020a](#bib.bib71); Li et al., [2020](#bib.bib102); Hou and Kung, [2020b](#bib.bib72);
    Liu et al., [2018b](#bib.bib111))) are fundamentally driven by data redundancy
    (e.g., redundancy across channels). There are already surveys, particularly on
    model optimizations, but this survey focuses on data redundancy exploitation beyond
    model optimizations. These techniques help DNN avoid data redundancy in its computations
    to efficiently execute DNN in inference or training or eliminate noise to boost
    DNN accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: There is a body of work on considering data redundancy in the hardware design
    of DNN accelerators, such as cache buffer designs for data reuse in 2D CNNs (Kim
    et al., [2020b](#bib.bib90); Mocerino et al., [2019](#bib.bib115); Jiao et al.,
    [2018](#bib.bib83); Ma et al., [2020](#bib.bib112); Salamat et al., [2018](#bib.bib129);
    Hegde et al., [2018b](#bib.bib69); Wang et al., [2019a](#bib.bib151)) and 3D CNNs (Wang
    et al., [2020](#bib.bib153), [2019b](#bib.bib152); Fan et al., [2017a](#bib.bib45);
    Wang et al., [2017](#bib.bib150); Shen et al., [2018](#bib.bib133); Hegde et al.,
    [2018a](#bib.bib68); Chen et al., [2019c](#bib.bib20)), or integrating activation
    pruning into the hardware architecture designs (Samal et al., [2020](#bib.bib130);
    Piyasena et al., [2019](#bib.bib124)). The techniques reduce the number of computations
    and bring speed and energy benefits. In this survey, we mainly focus our discussion
    on *software-based techniques* for exploiting data redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Relations with Relevant Concepts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several concepts are closely related to data redundancy exploitation. We next
    describe the relations with these concepts to further clarify our scope.
  prefs: []
  type: TYPE_NORMAL
- en: Model Redundancy Another aspect of redundancy in DNNs is model redundancy, referring
    to the redundancy within the parameters and architecture of a DNN. Model compression (Cheng
    et al., [2018](#bib.bib29); Blalock et al., [2020](#bib.bib10); Ganesh et al.,
    [2021](#bib.bib52); Choudhary et al., [2020](#bib.bib32); Hubara et al., [2017](#bib.bib77);
    Guo, [2018](#bib.bib62); Kim et al., [2020a](#bib.bib91); Acharya et al., [2019](#bib.bib2);
    Chen et al., [2016](#bib.bib26)), including knowledge distillation (Hinton et al.,
    [2014](#bib.bib70); Urban et al., [2017](#bib.bib147)), parameter pruning, and
    weight quantization, are popular approaches to exploit model redundancy. This
    article focuses on *data redundancy* of DNN, which refers to the redundancy within
    the input data of each DNN layer (usually in the form of multi-dimensional tensors).
  prefs: []
  type: TYPE_NORMAL
- en: Data Preprocessing and Augmentation Before data are fed into a DNN model, they
    often go through a preprocessing process. For training, that process is sometimes
    part of dataset augmentation (Shorten and Khoshgoftaar, [2019](#bib.bib137); Cubuk
    et al., [2019](#bib.bib34)), increasing data variance to increase the models’
    generality. In computer vision, example operations include rotating, inverting,
    equalizing, Color changes, brightness adjustment, etc. The samples created by
    the transformations naturally carry some redundancy with the original data. For
    instance, with images inverted, two images contain the same set of pixel values,
    despite that the positions of individual image patches differ in the images. Such
    redundancy is implicitly considered when a technique exploits redundancy in input
    data. This survey exploits data redundancy and leaves data augmentation out of
    the scope.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Extraction and Data Embedding. Feature extraction is a term in machine
    learning, referring to techniques that select or derive some features from a set
    of data that are regarded as most relevant to a specific machine learning task.
    Therefore, theoretically speaking, feature extraction can be viewed as a kind
    of exploitation of data redundancy as it reduces the raw data to an often smaller
    set of features. In a similar vein, data embeddings that map raw data to vectors
    in a space smaller than the original data space could also be regarded as a kind
    of reduction of data redundancy. Studies on these topics are usually considered
    as a separate research area named feature extraction and representation. They
    are typically not designed explicitly to exploit data redundancy, even though
    they may sometimes show such effects. We leave discussions on these studies out
    of the main focus of this paper.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Selection. In the learning phase of DNN, *training data selection* (Fan
    et al., [2017b](#bib.bib46); Feng et al., [2019](#bib.bib48); Zheng et al., [2017](#bib.bib164))
    is a process that tries to select the training data appropriate for the learner
    to achieve the learning task. To a certain degree, it may also remove some redundancy
    in the dataset (e.g., some data items in an over-sampled population). In this
    survey, we do not focus our discussion on this direction but on how data redundancy
    is addressed during the execution of the model (after the training dataset is
    selected or during inference).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Taxonomy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many studies detect or leverage data redundancy in DNN from several angles.
    A taxonomy that puts all the investigations into one categorization framework
    is essential for a holistic view of their differences, tradeoffs and shared principles.
    The taxonomy also offers a comprehensive view at the various dimensions of DNN
    data redundancy elimination, which can potentially guide the design of new redundancy
    elimination techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [1](#S3.F1 "Figure 1 ‣ 3\. Taxonomy ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning") presents a taxonomy we build after surveying
    hundreds of papers on DNN data redundancy exploitation. As far as we know, this
    is the first taxonomy on this topic. It shows the six most essential dimensions
    in DNN data redundancy exploitation. Data redundancy exploitation is usually the
    combination of one or more items in each dimension. We explain each of the dimensions
    next.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e2a26b8741a9b2e873e1fc259cb5f30a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Taxonomy of data redundancy exploitation in DNN.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Granularity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This dimension refers to the unit of the data examined for data redundancy,
    which we will call data unit in the following discussions. There are five granularities
    listed below in increasing order in size.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bit: At this granularity, every bit in a value is the unit for examination.
    Data quantization and precision relaxation to the shorter representation of a
    value are commonly seen strategies exploiting bit-level redundancy. Besides, bit
    representation can also be used directly for pattern-based bucketing. In RNSNet (Salamat
    et al., [2018](#bib.bib129)), for instance, an input value is transformed into
    its n-bits binary format and gets represented with a residue number system (RNS).
    The multiplications in neural networks are simplified to only addition and memory
    lookup, allowing memory-friendly operations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Neuron: The unit at this granularity is the value of a single neuron. An image
    is a pixel value at one channel or the value carried by a neuron in a DNN.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tile/Patch: The values carried by a set of neurons, which correspond to a part
    of an activation map. It could be, for instance, a bounding box or region of interest
    (RoI) or channel in an image; in the case of text, it could be one or multiple
    sub-word embeddings.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Activation Map: The values in an entire activation map.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batch or Sequence: A set of activation maps that the DNN carries at a layer,
    either at once or in order. The activation maps within the set can be either related
    or unrelated to each other. For example, sequential sentence input in a paragraph
    to a DNN can be correlated, while in a randomly shuffled set of training images,
    image activation maps are generally unrelated.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In general, the larger the granularity is, the less redundancy is there, but
    at the same time, the ratio between the benefits from removing a redundancy and
    the overhead in finding the redundancy is larger¹¹1Bit level is special, limited
    by the number of bits in one value, which is usually up to 32.: Avoiding processing
    a whole batch of images saves more time than avoiding processing a neuron, but
    the chances in finding two identical batches are generally smaller than finding
    two identical neurons. On the other hand, the removals of different granularities
    of redundancy are not exclusive; one optimization could exploit redundancies at
    multiple granularities.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Scope of Consideration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This dimension is about the scope in which different data units are compared
    to identify similar units and hence redundancy. This dimension is related to the
    previous extent, granularity, but differ: For a given data unit, such as a patch
    of activation map, the comparisons for similar units can still vary (e.g., within
    one activation map, across activation maps in a batch, or batches). Specifically,
    the scopes considered in prior studies fall into one or more following.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Single Value: Redundancy within a single representation of a value—the corresponding
    granularity is a bit.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Single Channel Activation Map: In the case of an image, it refers to redundancy
    within a single channel activation map; in the case of text, it relates to redundancy
    within a single hidden state. The corresponding granularity can be either neuron
    or tile/patch.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiple Channels of Activation Map: This is for visual data (image and video)
    only. Redundancy across multiple channels of an activation map between neurons,
    patches/tiles, or single-channel activation maps.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Temporal Sequence of Activation Maps: In the case of video, it refers to redundancy
    across a single video clip representation (4D tensor) between neurons, patches/tiles,
    single-channel activation maps, or activation maps. In the case of sequential
    text sequence input, it refers to redundancy across hidden states of the same
    sentence label (token type id), between neurons, sub-word embeddings, or single-sequence
    hidden states.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Within a Batch: Inputs to a DNN are often provided in a batch each time. The
    scope for data redundancy consideration can be across inputs within a batch.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cross Batches: The scope can also cross the boundaries of batches of inputs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cross Layers: All the earlier scopes typically assume that the comparisons
    are about activation maps at the same layer of a Neural Network. Some studies
    even expand the scope to activation maps on different layers of a Neural Network (Park
    and Kim, [2019](#bib.bib120); Dalvi et al., [2020](#bib.bib39)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In general, a large scope subsumes a smaller scope: Redundancy discoverable
    in a smaller scope must be discoverable in a larger scope. But on the other hand,
    checking values in a larger scope also entails more overhead and complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Types of Data Redundancy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data redundancy exploitation techniques can also be categorized based on what
    types of data redundancy they exploit. As earlier sections have mentioned, there
    are mainly three types of data redundancy. We list them here for the completeness
    of the discussion on the taxonomy.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Repeated Information: Similarity or duplication between data units. When the
    same operations operate on either identical or similar data units (e.g., multiplications
    on the same values, convolutions on similar image tiles), the results could be
    indistinguishable; the computations are redundant.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Irrelevant Information: Unnecessary data units; if they are removed, will not
    harm the DNN computation results.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Over-detailed Information: High-precision representation is not always necessary.
    For example, high-resolution images can be replaced with their low-resolution
    approximation in some application scenarios without causing the DNN to lose accuracy.
    Activation maps with lower precision sometimes give the same results for a DNN.
    A video with more frames than needed is another example. A sub-word vector with
    a higher-dimension representation (e.g., $1\times 512$ versus $1\times 256$ for
    a single word representation) or a higher precision value representation is also
    an example.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The types of data redundancy define how redundancy appears. These different
    types of redundancy are complementary to each other; they require different ways
    to detect but at the same time can be capitalized together. Previous work has
    each focused on one of the three types of redundancy; it remains yet to investigate
    how to effectively combine the removals of multiple types of redundancy in one
    framework. Among the potential challenges, how to minimize the overhead is one
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Ways to Detect Data Redundancy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The techniques can also be classified based on the ways they detect redundancy:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Value Similarity-based Grouping: This includes all kinds of similarity-based
    data grouping methods (e.g., various data clustering methods).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learning-based Methods: These methods learn data redundancy on some data samples
    (often sampled from the training or validation datasets) and apply the knowledge
    at runtime execution of the model. An example is the learning-based quantization
    of activation maps.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Projection: These techniques project the data representation to another domain
    space (e.g., frequency domain) and infer redundancy in that space.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Implicit Detection: These techniques assume redundancy exists in a specific
    scope based on some prior knowledge about the domain. One of the examples is the
    techniques that leverage similarities between neighboring video frames.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other Heuristics: Methods that leverage some other domain-specific heuristics,
    such as the relative attentions or the average number of zeros appearing at a
    specific pixel location.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The different ways apply to different domains and scenarios. The first two in
    the list are general, the third applies to the domains where projection across
    certain spaces makes sense, the fourth and fifth apply to domains where there
    is already certain prior knowledge about the redundancy in the domains.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Ways to Leverage Redundancy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The different techniques take different ways to leverage data redundancy. They,
    however, in principle fall into the following three main categories.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reuse: This approach reuses computation results on some data items for other
    data items. It applies to a similar kind of data redundancy. For example, Deep
    Reuse (Ning and Shen, [2019](#bib.bib117)) applies Locality Sensitive Hashing
    (LSH) (Gionis et al., [1999](#bib.bib57)), a fast unsupervised clustering scheme,
    to cluster data into similarity groups, and then uses the results computed on
    the cluster centroids as the results for all data items in the same cluster.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Skipping: This approach skips computations on some data items. It applies to
    data redundancy caused by irrelevant data items. For instance, in Perforated CNN (Figurnov
    et al., [2016](#bib.bib50)), the authors deal with spatial redundancy by masking
    some pixels in CNN feature maps. It also includes techniques that select a subset
    of data representations. Some work does ad-hoc sampling, while some others make
    the selection in a more sophisticated way. In PoWER-BERT (Goyal et al., [2020](#bib.bib59)),
    for instance, a retention configuration is defined to observe that cosine-similarity
    between vectors progressively increases as the layer goes deeper. Based on that
    setting, they employ strategies for word-vector selection. The retention configuration
    is further designed into learnable hyper-parameters.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Value Approximation: This approach uses an approximation to generate data representation.
    It corresponds to data redundancy caused by over-detailed information. Quantization
    and binarization on data items fall into this category. Previous reviews (Hubara
    et al., [2017](#bib.bib77); Simons and Lee, [2019](#bib.bib138); Qin et al., [2020](#bib.bib125))
    have provided comprehensive coverage on both research topics, so we do not emphasize
    them in the following discussion.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'These three ways to leverage redundancy are complementary. Even though only
    the third in the list carries ”approximation” in the name, all three could cause
    accuracy loss: Reuse may reuse results across similar but not identical data,
    and similarly, skipping may skip the computations of some similar but not identical
    data. These ways of leveraging redundancy can be used together. Some prior studies
    have already done that. One of them (Dalvi et al., [2020](#bib.bib39)), for instance,
    exploits a layer selector (”skipping”) and correlation clustering (”reuse”) practice
    at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6\. Types of Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can also classify the studies on data redundancy based on the types of data
    they deal with. Although there are many kinds of data, most existing studies on
    data redundancy are on the following three types of data:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Images. This type of data includes various kinds of images, including those
    generated from higher dimension sensors (e.g., those collected through Lidar).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Videos. This type of data features an extra-temporal dimension than images,
    which provides special opportunities for data redundancy exploitation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Texts. This type of data consists of texts from various kinds of sources, usually
    written in Natural Languages.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There are some other types of data that DNN has been applied to, such as graphs,
    genes, computer programs, and so on. We will briefly discuss them in Section [7](#S7
    "7\. Future Research Directions ‣ Survey: Exploiting Data Redundancy for Optimization
    of Deep Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.7\. Use of the Taxonomy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As far as we know, this is the first taxonomy on data redundancy exploitation.
    It was created based on our survey of hundreds of papers on data redundancy exploitation.
    The taxonomy can be used to position work in the big picture, to recognize the
    possible missing opportunities yet to explore for a certain kind of data, to guide
    the designs of future DNNs for efficiency, and to assist the possible creation
    of future automatic frameworks that may apply redundancy exploitation for new
    Deep Learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We categorize a set of representative papers listed in Table [1](#S3.T1 "Table
    1 ‣ 3.7\. Use of the Taxonomy ‣ 3\. Taxonomy ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning") using the taxonomy. We will discuss them in
    more detail in the next several sections. Specifically, we organize the following
    discussions based on several layers of the taxonomy. At the highest level, we
    divide the studies based on the types of data they handle into three sections,
    with Section [4](#S4 "4\. Leverage Redundancy in Image Data ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning") on images, Section [5](#S5
    "5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning") on videos, and Section [6](#S6 "6\. Leverage
    Data Redundancy in Transformer Optimization for Text ‣ Survey: Exploiting Data
    Redundancy for Optimization of Deep Learning") on texts. We organize the various
    studies based on a dimension that captures the most prominent differences among
    the studies in each area.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Some representative papers on data redundancy exploitation.
  prefs: []
  type: TYPE_NORMAL
- en: '| Data Type | Paper | Redundancy Type | Granularity | Scope | Detection | Exploitation
    |'
  prefs: []
  type: TYPE_TB
- en: '| Image | (Ning and Shen, [2019](#bib.bib117)) | Repeated | Patch/Tile | Multiple
    Channels, Within-Batch, Cross-Batch | Similarity | Reuse |'
  prefs: []
  type: TYPE_TB
- en: '| (Georgiadis, [2019](#bib.bib56)) | Irrelevant, Repeated | Bit, Neuron | Single
    Value, Cross-Layer | Learning, Similarity | Value Approximation, Reuse |'
  prefs: []
  type: TYPE_TB
- en: '| (Park and Kim, [2019](#bib.bib120)) | Repeated | Activation Map | Cross-Layer
    | Similarity | Reuse |'
  prefs: []
  type: TYPE_TB
- en: '| (de Moura et al., [2019](#bib.bib40)) | Repeated | Activation Map | Multiple
    Channels | Similarity | Reuse |'
  prefs: []
  type: TYPE_TB
- en: '| (Chen et al., [2019b](#bib.bib19)) | Repeated | Activation Map | Single Channel
    | Learning | Value Approximation, Reuse |'
  prefs: []
  type: TYPE_TB
- en: '| (Figurnov et al., [2016](#bib.bib50)) | Irrelevant | Neuron | Multiple Channels
    | Implicit Detection, Learning | Skipping |'
  prefs: []
  type: TYPE_TB
- en: '| (Akhlaghi et al., [2018](#bib.bib3); Hu et al., [2016](#bib.bib74); Shomron
    et al., [2020](#bib.bib136)) | Irrelevant | Neuron | Multiple Channels | Heuristic
    | Skipping |'
  prefs: []
  type: TYPE_TB
- en: '| (Chen et al., [2019d](#bib.bib23); Suzuki et al., [2020](#bib.bib144)) |
    Irrelevant | Neuron | Multiple Channels | Learning | Skipping |'
  prefs: []
  type: TYPE_TB
- en: '| (Gao et al., [2019](#bib.bib55)) | Over-detailed | Activation Map | Multiple
    Channel | Learning | Skipping |'
  prefs: []
  type: TYPE_TB
- en: '| (Ibrokhimov et al., [2020](#bib.bib79)) | Irrelevant | Neuron | Multiple
    Channels | Heuristic | Skipping (Subset Selection) |'
  prefs: []
  type: TYPE_TB
- en: '| (Lee and Nirjon, [2020](#bib.bib99)) | Irrelevant | Neuron | Cross-Layer
    | Heuristic | Skipping (Subset Selection) |'
  prefs: []
  type: TYPE_TB
- en: '| (Huang and Wang, [2018](#bib.bib76)) | Irrelevant | Neuron | Multiple Channels,
    Cross-Layer | Learning | Skipping (Subset Selection) |'
  prefs: []
  type: TYPE_TB
- en: '| (Chitsaz et al., [2020](#bib.bib31); Chen et al., [2020a](#bib.bib21), [2019a](#bib.bib25))
    | Over-detailed | Activation Map | Multiple Channels | Projection | Value Approximation
    |'
  prefs: []
  type: TYPE_TB
- en: '| (Chen et al., [2020b](#bib.bib28)) | Over-detailed | Activation Map | Multiple
    Channels | Implicit Detection | Skipping (Ad-hoc Sampling) |'
  prefs: []
  type: TYPE_TB
- en: '| (Gao et al., [2018](#bib.bib53); Li et al., [2019b](#bib.bib101)) | Irrelevant
    | Patch/Tile | Multiple Channels | Learning | Skipping (Subset Selection) |'
  prefs: []
  type: TYPE_TB
- en: '| Video | (Kang et al., [2017a](#bib.bib85)) | Irrelevant | Activation Map
    | Temporal Sequence | Similarity | Skipping |'
  prefs: []
  type: TYPE_TB
- en: '| (Cavigelli et al., [2017](#bib.bib17); Cavigelli and Benini, [2019](#bib.bib16))
    | Irrelevant | Neuron | Temporal Sequence | Similarity | Skipping |'
  prefs: []
  type: TYPE_TB
- en: '| (Chin et al., [2019](#bib.bib30)) | Over-detailed | Patch/Tile | Multiple
    Channels | Learning | Value Approximation |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang et al., [2017](#bib.bib163)) | Irrelevant | Patch/Tile | Multiple
    Channels | Implicit Detection | Skipping (Subset Selection) |'
  prefs: []
  type: TYPE_TB
- en: '| (Mao et al., [2019](#bib.bib113)) | Repeated | Patch/Tile | Temporal Sequence
    | Implicit Detection | Skipping (Subset Selection) |'
  prefs: []
  type: TYPE_TB
- en: '| (Yeung et al., [2016](#bib.bib161); Alwassel et al., [2018](#bib.bib5); Wu
    et al., [2019b](#bib.bib157), [a](#bib.bib155); Korbar et al., [2019](#bib.bib95))
    | Over-detailed | Activation Map | Temporal Sequence | Learning | Skipping (Subset
    Selection) |'
  prefs: []
  type: TYPE_TB
- en: '| (Su and Grauman, [2016](#bib.bib143)) | Over-detailed | Batch or Sequence
    | Temporal Sequence | Learning | Skipping (Subset Selection) |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhu et al., [2017b](#bib.bib170), [a](#bib.bib169), [2018](#bib.bib168))
    | Repeated | Activation Map | Temporal Sequence | Implicit Detection | Reuse (Temporal
    Propagation) |'
  prefs: []
  type: TYPE_TB
- en: '| (Kang et al., [2017b](#bib.bib87), [2018](#bib.bib88)) | Repeated | Activation
    Map | Temporal Sequence | Implicit Detection | Reuse (Temporal Propagation) |'
  prefs: []
  type: TYPE_TB
- en: '| (Chen et al., [2018](#bib.bib22)) | Repeated | Activation Map | Temporal
    Sequence | Implicit Detection | Reuse (Temporal Propagation) |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhu and Liu, [2018](#bib.bib166)) | Repeated | Activation Map | Temporal
    Sequence | Implicit Detection | Reuse (Temporal Propagation) |'
  prefs: []
  type: TYPE_TB
- en: '| Text | (Liu et al., [2018a](#bib.bib109); Dai et al., [2020](#bib.bib36))
    | Over-detailed | Patch/Tile | Activation Map | Implicit Detection | Skipping
    (Ad-hoc Sampling) |'
  prefs: []
  type: TYPE_TB
- en: '| (Goyal et al., [2020](#bib.bib59)) | Irrelevant | Patch/Tile | Activation
    Map | Learning | Skipping (Subset Selection) |'
  prefs: []
  type: TYPE_TB
- en: '| (Kitaev et al., [2020](#bib.bib93)) | Repeated | Patch/Tile | Activation
    Map | Similarity | Reuse |'
  prefs: []
  type: TYPE_TB
- en: '| (Dalvi et al., [2020](#bib.bib39)) | Repeated, Irrelevant | Batch or Sequence
    | Temporal Sequence, Cross-Layer | Similarity | Reuse, Skipping (Subset Selection)
    |'
  prefs: []
  type: TYPE_TB
- en: 'In Section [4](#S4 "4\. Leverage Redundancy in Image Data ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning"), we divide studies on image
    data redundancy based on the types of redundancy they are dealing with, as the
    number of works in the three sub-categories (Repeated Information, Irrelevant
    Information, and Over-detailed Information) are relatively balanced and offers
    comprehensive coverage of the differences among the techniques. We point out the
    discrepancies between the work in each group in other dimensions (e.g., data representations,
    redundancy exploitation techniques) when necessary during the discussion. While
    many image redundancy exploitation techniques could potentially be applied to
    each frame in a video, Section [5](#S5 "5\. Leverage Data Redundancy in DNN for
    Videos ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning")
    focuses on the methods specially designed for videos. These efforts extend the
    work in Section [4](#S4 "4\. Leverage Redundancy in Image Data ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning") by exploring how redundancy
    is leveraged on the patch unit and the temporal dimension. Section [6](#S6 "6\.
    Leverage Data Redundancy in Transformer Optimization for Text ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning") discusses how data redundancy
    in the text is handled. Most of the work is based on Transformer models. In both
    Sections [5](#S5 "5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning") and [6](#S6 "6\. Leverage
    Data Redundancy in Transformer Optimization for Text ‣ Survey: Exploiting Data
    Redundancy for Optimization of Deep Learning"), the primary dimension we use to
    group the various explorations is how data redundancy is exploited.'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Leverage Redundancy in Image Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Image data are generally presented as three-dimensional pixel values (height,
    width, and channel). The inherent locality within data values provides opportunities
    for advanced optimizations. This section groups the relevant studies at a high
    level based on the types of redundancy they mainly target: repeated information,
    irrelevant information, and over-detailed information. The discussion in each
    kind then divides the techniques to detect and handle data redundancy. Table LABEL:tab:image
    presents these works by their applications (tasks) and reported performance for
    fast reference. The second column (“Paper”) shows the references to the relevant
    papers; the third column (“Code”) shows the link to the source code of the implementation;
    the fourth column (”Key Idea”) briefly summarizes the key idea of each work. The
    fifth column (“Performance”) summarizes the performance gains by the data redundant
    optimization techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Repeated Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the common coherence of data values in an image, image data often show
    a certain degree of data locality along the spatial dimensions. The most common
    optimizations take advantage of the value similarities among pixels. The initial
    motivation is to minimize the storage space of CNN’s intermediate activation maps
    on a resource-constrained platform and reduce the data movements between CPU and
    GPU and between processing units and memory. The optimizations also reduce the
    number of computations and improve computation speeds.
  prefs: []
  type: TYPE_NORMAL
- en: Table (or cache) lookup (Park and Kim, [2019](#bib.bib120); de Moura et al.,
    [2019](#bib.bib40); Mocerino et al., [2019](#bib.bib115); Razlighi et al., [2017](#bib.bib126);
    Ma et al., [2020](#bib.bib112); Hegde et al., [2018b](#bib.bib69)) and clustering (Kim
    et al., [2020b](#bib.bib90); Ning and Shen, [2019](#bib.bib117)) are the most
    typical approaches taken to discover and leverage repeated information in images.
  prefs: []
  type: TYPE_NORMAL
- en: To detect similarity among data points, the various techniques differ in the
    level of pixel units used for the similarity detection, some at the level of bits (Jiao
    et al., [2018](#bib.bib83); Ma et al., [2020](#bib.bib112); Salamat et al., [2018](#bib.bib129)),
    some at the level of pixels (Razlighi et al., [2017](#bib.bib126)), patches (or
    tiles, sub-vectors) (Kim et al., [2020b](#bib.bib90); Ning and Shen, [2019](#bib.bib117);
    Hegde et al., [2018b](#bib.bib69)), or even entire feature maps as a whole (Park
    and Kim, [2019](#bib.bib120); de Moura et al., [2019](#bib.bib40); Mocerino et al.,
    [2019](#bib.bib115); Jiao et al., [2018](#bib.bib83)). Some use clustering with
    predefined distance metrics and thresholds to identify the similarity between
    units, while others use hash functions like locality sensitive hashing (Ning and
    Shen, [2019](#bib.bib117)) or bloom filters (Jiao et al., [2018](#bib.bib83)).
    RNSnet (Salamat et al., [2018](#bib.bib129)) utilizes a unique bit-handling technique
    that maps the binary representation into the Residue Number System (RNS). Each
    binary representation is divided by a modulus set; with the values represented
    by the corresponding modules and reminders, the technique efficiently identifies
    similar values. The method could be regarded as a kind of hashing function.
  prefs: []
  type: TYPE_NORMAL
- en: After gathering similar data points, each bucket (or cluster) of the data is
    represented by usually one or a few data points in the bucket. The determination
    of such representatives depends on the tasks (e.g., image classification, object
    detection) that the DNN performs. For a relatively simple image classification
    task, if the number of buckets is enough to showcase the overall discrepancy between
    predicted classes without hurting the training or inference accuracy, the representative
    can be simply an average of data values in that bucket. To the extreme, when the
    dataset contains a significant proportion of identical data samples, a representative
    is sometimes set as an arbitrary data point taken from a bucket. The representative
    is used in place of the other issues in the bucket during the CNN inference or
    training, such that similar computations can be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: 'Representative work in this category is the Deep Reuse work by Ning and others (Ning
    and Shen, [2019](#bib.bib117); Ning et al., [2019](#bib.bib116)). As shown in
    Fig. [2](#S4.F2 "Figure 2 ‣ 4.1\. Repeated Information ‣ 4\. Leverage Redundancy
    in Image Data ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning"),
    the approach divides input images or activation maps into sub-vectors during runtime.
    Based on their similarities, it clusters them into several buckets through efficient
    online LSH-based clustering (Gionis et al., [1999](#bib.bib57)), with each cluster
    represented by its centroid. Besides handling repeated information in a 2D scope,
    Deep Reuse also addresses the redundancy in data batches. Based on the clustering
    results, Deep Reuse reduces a convolution to multiplications between a small matrix
    formed by the clusters centroids and the filter matrix. The results are used to
    reconstruct the full activation map through data duplications. Even with the online
    clustering overhead, the work shows that the reuse of computation results yields
    an overall $1.77-2\times$ speedup (up to $4.3\times$ layer-wise) with negligible
    accuracy loss and without model retraining. The method yields more speedups when
    applied to CNN training in an adaptive manner (Ning et al., [2019](#bib.bib116)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec930134314b79166967a4545bbdee23.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Illustration of sub-vector clustering in Deep Reuse. In a convolution
    layer, the activation maps of 2D images are unfolded as $x$ and are multiplied
    by weights $W$ of corresponding filters. With sub-vector clustering, $x$ is first
    divided into sub-vectors and labeled with corresponding cluster ids. In the matrix
    multiplication, only the representative of each cluster is used. Afterward, the
    output activation maps are re-constructed with the representative results of each
    cluster. Figure adapted from reference (Ning and Shen, [2019](#bib.bib117)).
  prefs: []
  type: TYPE_NORMAL
- en: A fundamental tradeoff facing the methods in this category is the accuracy and
    the amount of computation reuse. It is determined by the aggressiveness in similarity-based
    clustering. For instance, in the Deep Reuse work, the aggressiveness is determined
    by two hyper-parameters, the length of a sub-vector in an activation map (granularity),
    and then the number of LSH hashing vectors. As the appropriate values of the hyperparameters
    are data and model dependent, the previous work (Ning and Shen, [2019](#bib.bib117))
    employs some offline tuning process to select the values such that the accuracy
    is not compromised. One of the directions worth some future explorations is to
    adapt the clustering method to data and models. For instance, in Deep Reuse, the
    hashing vectors are randomly generated. If they can be learned from data for a
    given CNN, more aggressive clustering might be possible without causing accuracy
    losses.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Irrelevant Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.2.1\. Feature Pruning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Image data or intermediate activation maps may have unnecessary information
    as well. For example, most well-trained CNNs can still maintain their prediction
    accuracy when some pixels in the activation maps are removed. Subsequently, the
    skipped portion of the data reduces the amount of computation and thus offers
    acceleration during inference or training. We introduce some details about how
    the optimizations in this line identify these irrelevant values at different scales
    and get rid of them while keeping the overall accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Hu et al. (Hu et al., [2016](#bib.bib74)) identify many zeros in the activation
    of CNNs. They quantify the neurons to be pruned based on the Average Percentage
    of Zeros (APoZ). APoZ is calculated on a per-layer basis to measure the probability
    each fixed position appears to be zero-valued in validation data samples. With
    the same motivation, Akhlaghi et al. (Akhlaghi et al., [2018](#bib.bib3)) and
    Piyasena et al. (Piyasena et al., [2019](#bib.bib124)) propose detecting zero
    pixels that may occur after the ReLU activations. It is by identifying the negative
    activations earlier using a low-cost approximation scheme since the negative values
    turn into zeros after going through the ReLU layers. In particular, Akhlaghi et
    al. (Akhlaghi et al., [2018](#bib.bib3)) construct SnaPEA with a runtime technique
    to speculate on the convolution outputs’ sign before going through negative weights.
    The aggressiveness of the speculation is controlled by thresholding on parameters
    and the predefined associated number of MAC operations that can maintain accuracy.
    The parameters are tuned offline within the search space. Piyasena et al. (Piyasena
    et al., [2019](#bib.bib124)) augment the CNN implementation with a lightweight
    approximation scheme that consists of ApproxConv and ReLupred stages. The two
    operations work sequentially to handle the sign prediction on an approximate convolution
    computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more direct way to remove irrelevant information is to assume the existence
    of redundancy based on domain knowledge and prune the data or activation maps
    at different scales. Figurnov et al. (Figurnov et al., [2016](#bib.bib50)) prunes
    the data at pixel and patch level. The proposed mechanism inherits the idea of
    the loop perforation technique in code optimization to skip certain spatial positions
    in images for CNN classification inference. The selection of pixels to prune is
    random within a predefined limited number of points. They exploit four input-independent
    perforation masks: Uniform mask, Grid mask, Pooling Structure mask, and Impact
    mask, as shown in Fig. [3](#S4.F3 "Figure 3 ‣ 4.2.1\. Feature Pruning ‣ 4.2\.
    Irrelevant Information ‣ 4\. Leverage Redundancy in Image Data ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning"). These masks are used to confine
    the random point selected into corresponding patterns. Afterward, perforated CNN
    uses interpolation to reconstruct the output feature maps. The CNN wights are
    retrained to adapt to this optimization as well.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a7733fbcdbc09c493c33bc2f22efb89b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Input-independent perforation masks in PerforatedCNNs. According
    to the paper, the mass layout is for the AlexNet Conv2 layer. (a) Uniform perforation
    mask is $N$ points chosen randomly without replacement from the set $\Omega$.
    (b) Grid perforation mask is a set of points $I={a(1),...,a(Kx)}\times{b(1),...,b(Ky)}$
    the values of a(i), b(i) using the pseudorandom integer sequence generation scheme (Graham,
    [2014](#bib.bib60)). (c) Pooling structure mask exploits the structure of the
    overlaps of pooling operators. Denote by $A(x,y)$ the number of times the convolutional
    layer output is used in the pooling operators. The pooling structure mask is obtained
    by picking top-N positions with the highest values of $A(x,y)$. (d) Impact mask
    estimates the impact of perforation of each position on the CNN loss function
    and then removes the least important positions. When activation maps are computed,
    the coordinates of black pixels are skipped. Figures and description adapted from
    reference (Figurnov et al., [2016](#bib.bib50)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from the pruning technique within the scope of a single activation map,
    irrelevant information can be addressed across the channel dimensions by, for
    instance, removing a set of outside 2D feature maps. Specifically, Gao et al. (Gao
    et al., [2019](#bib.bib55)) apply feature boosting and suppression by formulating
    a channel saliency predictor. The predictor serves as an indicator to skip execution
    at some feature maps at runtime. Chakraborty et al. (Chakraborty et al., [2019](#bib.bib18))
    show that in a simple handwritten digit recognition task, directly discarding
    redundant feature maps randomly before the full connection does not affect the
    prediction accuracy much. On the same axis, Singh et al. (Singh et al., [2019](#bib.bib141))
    conduct a comprehensive analysis on pruning redundant activation maps in 1D CNN,
    SoundNet (Aytar et al., [2016](#bib.bib6)), at a per map level. The pruned feature
    map selection is based on the following measurements: ANOVA-based method (Penny
    and Henson, [2006](#bib.bib121)), Entropy-based (DE) method (Pérez-Cruz, [2009](#bib.bib122)),
    Cosine-similarity (CS) based method, and a greedy algorithm for selection of feature
    maps based on KL divergence. Gaikwad et al. (Gaikwad and El-Sharkawy, [2019](#bib.bib51))
    apply the L2 norm to decide feature map importance in pruning feature maps of
    SqueezeNet (Iandola et al., [2016](#bib.bib78)). Although the technique is not
    for image data, the selection criteria are worthy of being noted under the context
    of irrelevancy elimination. Furthermore, multiple granularities of redundancy
    can be handled at the same time. For example, Yu et al. (Yu et al., [2020](#bib.bib162))
    combine spatial and channel-wise neuron pruning on activation maps. The criteria
    of what to be pruned are based on the attention mechanism. The attention mechanism
    is trained with targeted dropout, enhancing inference time robustness without
    accuracy loss after pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the optimizations can be classified by whether the irrelevancy is
    subject to the input datasets or not. Namely, they are input-dependent or input-independent.
    Input-dependent irrelevant information removal can be achieved by detecting data
    irrelevancy based on statistical (Hu et al., [2016](#bib.bib74); Singh et al.,
    [2019](#bib.bib141); Gao et al., [2019](#bib.bib55)) or learning-based heuristic (Akhlaghi
    et al., [2018](#bib.bib3); Yu et al., [2020](#bib.bib162)), or estimation of values (Piyasena
    et al., [2019](#bib.bib124)). On the contrary, input-independent approaches (Figurnov
    et al., [2016](#bib.bib50); Chakraborty et al., [2019](#bib.bib18)) get rid of
    those irrelevant values by implicitly assuming them to appear randomly with a
    limit threshold in structural or non-structural manners.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Neuron or Feature Subset Selection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another granularity used in feature map pruning is the selection of neurons
    (or activation). It could be regarded as a feature selection process at the value
    granularity. The neurons not chosen can be interpreted as ones being pruned. A
    subset of activations is determined based on specific criteria in neuron selection.
    Lee et al. (Lee and Nirjon, [2020](#bib.bib99)) apply ranking on the neurons of
    DNNs based on their contribution to the inference accuracy. Huang et al. (Huang
    and Wang, [2018](#bib.bib76)) add a sparsity regularization on neurons to force
    some of them to become zeros and guide the selection of more informative neurons
    without further tuning. Besides, Ibrokhimov et al. (Ibrokhimov et al., [2020](#bib.bib79))
    propose the technique that selects neurons based on the magnitude of their average
    activations and keeps only the exact amount of the most minor activated neurons
    that work well. They report a reduction in the total number of operations and
    corresponding speedups.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3\. Dynamic Region of Interest
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In object detection, dynamic zooming in (Gao et al., [2018](#bib.bib53); Li
    et al., [2019b](#bib.bib101)) of a particular region of interest can help reduce
    computations on irrelevant background information. Gao et al. (Gao et al., [2018](#bib.bib53))
    present such a technique. They employ zoom-in accuracy gain regression network
    (R-net) and zoom-in Q function network (Q-net). The R-net learns the correlation
    between coarse and fine detection and predicts the accuracy gain for zooming in
    to a specific region. The Q-net learns via reinforcement learning. It sequentially
    selects the optimal zoom locations and scales them correspondingly by analyzing
    the new output of R-net and its history. In the inference phase, a down-sampled
    image is input to the R-net and predicts the accuracy gain. The gain serves as
    an indicator for the Q-net to select the target on regions to concentrate sequentially.
    The workflow is illustrated in Fig. [4](#S4.F4 "Figure 4 ‣ 4.2.3\. Dynamic Region
    of Interest ‣ 4.2\. Irrelevant Information ‣ 4\. Leverage Redundancy in Image
    Data ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning").
    The mechanism reduces the total pixels in computation by over $50\%$ and reduces
    the average detection time by $25\%$ on the Caltech Pedestrian Detection dataset.
    It also reduces the pixels by $70\%$ and obtains over $50\%$ detection time reduction
    on the YFCC100M dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/41febdae49e67e56a841562a1790087b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. The workflow of R-net and Q-net. Given a down-sampled image as input,
    the R-net generates an initial accuracy gain (AG) map indicating the potential
    zoom-in accuracy gain of different regions (initial state). The Q-net is applied
    iteratively on the AG map to select regions. Once a region is selected, the AG
    map will be updated to reflect the history of actions. Two parallel pipelines
    are used for the Q-net, each of which outputs an action-reward map that corresponds
    to selecting zoom-in regions with a specific size. The map’s value indicates the
    likelihood that the action will increase accuracy at a low cost. Action rewards
    from all maps are considered to select the optimal zoom-in region at each iteration.
    The notation $128\times 15\times 20:(7,10)$ means 128 convolution kernels with
    size $15\times 20$, and stride of 7/10 in height/width. Each grid cell in the
    output maps is given a unique color, and a bounding box of the same color is drawn
    on the image to denote the corresponding zoom region size and location. Figure
    and description adapted from reference (Gao et al., [2018](#bib.bib53)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Over-detailed Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Yet another direction in image data redundancy exploitation is eliminating
    over-detailed information, represented by adaption to variant image resolutions,
    especially resolution lowering. Chen et al. (Chen et al., [2019a](#bib.bib25))
    propose Octave convolution (OctConv) (Fig. [5](#S4.F5 "Figure 5 ‣ 4.3\. Over-detailed
    Information ‣ 4\. Leverage Redundancy in Image Data ‣ Survey: Exploiting Data
    Redundancy for Optimization of Deep Learning")) to leverage the spatial redundancy
    and factorize the activation maps into high and low frequency (resolution) groups
    to save both memory and computation cost. The operations at each layer enable
    the information to be exchanged within both groups during computation. It is implemented
    as a portable plug-and-play convolution unit and can be applied together with
    a compressed model or optimization that reduces channel-wise redundancy. An OctConv-equipped
    ResNet-152 can achieve $82.9\%$ top-1 classification accuracy on ImageNet with
    $22.2$ GFLOPs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9114ead39ec0a042f316876409901a4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Illustration of Octave Conv in the factorization of high and low
    spatial frequency. (a) Motivation. The spatial frequency model for vision (Campbell
    and Robson, [1968](#bib.bib12); De Valois and De Valois, [1980](#bib.bib41)) shows
    that raw images can be decomposed into a low and a high spatial frequency part.
    (b) The output maps of a convolution layer can also be factorized and grouped
    by their spatial frequency. (c) The proposed multifrequency feature representation
    stores the smoothly changing, low-frequency maps in a low-resolution tensor to
    reduce spatial redundancy. (d) The proposed Octave Convolution operates directly
    on this representation. It updates the information for each group and further
    enables information exchange between groups. Figures and descriptions adapted
    from reference (Chen et al., [2019a](#bib.bib25)).
  prefs: []
  type: TYPE_NORMAL
- en: 'ViP proposed by Chen et al. (Chen et al., [2020b](#bib.bib28)) also targets
    the spatial redundancy in feature maps. It takes advantage of virtual pooling,
    as shown in Fig. [6](#S4.F6 "Figure 6 ‣ 4.3\. Over-detailed Information ‣ 4\.
    Leverage Redundancy in Image Data ‣ Survey: Exploiting Data Redundancy for Optimization
    of Deep Learning"), such that the larger stride is used to obtain a smaller feature
    map and at the same time save convolution computation. To recover the output feature
    map dimension, linear interpolation is applied. ViP delivers $2.1\times$ speedup
    with less than $1.5\%$ accuracy degradation in ImageNet classification on VGG16,
    and $1.8\times$ speedup with $0.025$ mAP degradation in PASCAL VOC object detection
    with Faster-RCNN. ViP also reduces mobile GPU and CPU energy consumption by up
    to $55\%$ and $70\%$, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c24fd92ac60afcaf6f0f4b3d56ba62d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Illustration of virtual polling (LinkViz, [2020](#bib.bib105)). By
    using a larger stride, it save computation in conv layers and, to recover the
    output size, it use linear interpolation which is fast to compute. Figure and
    description adapted from reference (Chen et al., [2020b](#bib.bib28)).
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Leverage Data Redundancy in DNN for Videos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Existing DNN frameworks for video analysis are built on well-developed CNNs
    such as VGG (Simonyan and Zisserman, [2015](#bib.bib140)) or ResNet (He et al.,
    [2016](#bib.bib66)) and image-based object detection algorithms such as Faster-RCNN (Ren
    et al., [2015](#bib.bib127)). They treat the video as a continuous input of images.
    Intuitively, the data redundancy in this line of works, especially the duplication
    along the temporal axis, provides much room for performance enhancement. DNN frameworks
    that leverage data redundancy in video data can be categorized based on how they
    exploit redundancy. These techniques fall into the three categories listed in
    our taxonomy (skipping, reuse, and approximation); this section groups them more
    detailed to capture the more complicated differences among the techniques. Table LABEL:tab:video
    shows the works with video data redundancy exploitation by their applications.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A way to avoid recomputing similar values across video frames is through skipping
    some of them along the temporal dimension (or the batch dimension in 2D CNN).
    Kang et al. propose NoScope (Kang et al., [2017a](#bib.bib85)) to optimize the
    processing of video querying. As shown in Fig. [7](#S5.F7 "Figure 7 ‣ 5.1\. Pruning
    ‣ 5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning"), they insert a difference detector and a specialized
    model with short-circuit evaluation that allows computation reduction on almost
    identical neighboring frames in a video sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4cdc162686a83ea8658a0617a0e51d59.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. Illustration of the NoScope framework. NoScope is a system for accelerating
    neural network analysis over videos via an inference-optimized model search. Given
    an input video, target object, and reference neural network, NoScope automatically
    searches for and trains a cascade of models—including difference detectors and
    specialized networks—that can reproduce the binarized outputs of the reference
    network with high accuracy—but up to three orders of magnitude faster. Figure
    and description adapted from reference (Kang et al., [2017a](#bib.bib85)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The redundancy across continuous frames can also be addressed at the pixel
    level. Cavigelli et al. propose CBinfer (Cavigelli et al., [2017](#bib.bib17);
    Cavigelli and Benini, [2019](#bib.bib16)), a change-based evaluation of CNN for
    video data recorded with a static camera, to exploit the spatial-temporal sparsity
    of pixel changes. As shown in Fig. [8](#S5.F8 "Figure 8 ‣ 5.2\. Approximate Representation
    ‣ 5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning"), they modify each convolution layer to include
    an additional change detection mechanism and change indexes extraction before
    the matrix multiplication. The output is updated with both non-changed output
    pixels from the previous frame and changed output pixels calculated at this time.
    CBinfer achieves an average speed-up of $8.6\times$ over a cuDNN baseline on a
    realistic benchmark with a negligible accuracy loss of less than $0.1\%$ and no
    network retraining. The resulting energy efficiency is $10\times$ higher than
    per-frame evaluation and reaches an equivalent of $328$ $GOp/s/W$ on the Nvidia
    Tegra X1 platform.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Approximate Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Regional re-scaling is a type of approximate representation. Chin et al. propose
    AdaScale (Chin et al., [2019](#bib.bib30)) that exploits the potential benefit
    bought by adaptive image scaling. They resort to image down-sampling to realize
    performance improvement on both accuracy and speedup for video object detection.
    To accomplish this, AdaScale adopts an object detector to generate the optimal
    scale labels for images carried out by a learning-based method. The generated
    optimal scale is later used to train the scale regressor that dynamically re-scales
    the image. The visualization of the effect is shown in Fig. [9](#S5.F9 "Figure
    9 ‣ 5.2\. Approximate Representation ‣ 5\. Leverage Data Redundancy in DNN for
    Videos ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning").
    When the images are down-sampled, they are supposed to reduce the number of false
    positives introduced by overly detail-oriented messages, which in turn dismiss
    the redundancy. The image scaling increases the number of true positives by turning
    the objects too large to smaller ones for the detector to be more confident. For
    ImageNet VID and mini YouTube-BB datasets, AdaScale demonstrates $1.3\%$ and $2.7\%$
    mAP improvement with $1.6\times$ and $1.8\times$ speedup respectively. The framework
    can also work complementary with video acceleration work (Zhu et al., [2017b](#bib.bib170)),
    of which they experience a speedup gain by $25\%$ and a slight mAP boost on the
    ImageNet VID dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/850f08f851748107e94a2093d68734bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8\. Processing flow of the change-based convolution algorithm. Custom
    processing kernels are shown in blue, processing steps using available libraries
    are shown in green, variables sharable among layers are shown in yellow, and variables
    to be stored per layer are colored orange. The size and data type of the tensor
    storing the intermediate results is indicated below each variable name. Figure
    and description adapted from reference (Cavigelli et al., [2017](#bib.bib17)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c37255a553734d364c5bf3481c80a7f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9\. AdaScale detection result visualization. Examples where down-sampled
    images have better detection results. Blue boxes are the detection results, and
    the numbers are the confidence. The detector is trained on a single scale (pixels
    of the shortest side) of 600\. Column (a) and (c) are tested at scale of 600.
    Column (b) is tested at scale 240, and column (d) is tested at scale 480\. Figures
    and descriptions adapted from reference (Chin et al., [2019](#bib.bib30)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Subset Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Subset selection is for obtaining a subgroup of potentially representative video
    frames or corresponding feature maps. These techniques adopt a variety of dimensions
    in preference, from the spatial to the temporal, batch, and streaming.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1\. Selection in the Spatial Dimension
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Patch-of-Interest
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Zhang et al. (Zhang et al., [2017](#bib.bib163)) propose Kill Two Bird with
    One Stone to aggregate patch-of-interest (usually the moving object within images)
    for construction of a compact patch composition for video object detection as
    shown in Fig. [10](#S5.F10 "Figure 10 ‣ Patch-of-Interest ‣ 5.3.1\. Selection
    in the Spatial Dimension ‣ 5.3\. Subset Selection ‣ 5\. Leverage Data Redundancy
    in DNN for Videos ‣ Survey: Exploiting Data Redundancy for Optimization of Deep
    Learning"). In the detection steps, the parts of the spatial dimension data patches
    are eliminated to save unnecessary computation. The set of patch-of-interest is
    mapped back by their numbers and location offsets to reconstruct the final result.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8deaf1050c985fd08232097d1c15b324.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10\. Patch-of-Interest composition. (a) The input image. (b) Detected
    patches. (c) The patch composition (left) and sub-frames(right). (d) Detection
    results on sub-frames. (e) Map back and get the final result on the original image.
    Figures and descriptions adapted from reference (Zhang et al., [2017](#bib.bib163)).
  prefs: []
  type: TYPE_NORMAL
- en: Patch Refinement
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Mao et al. (Mao et al., [2019](#bib.bib113)) present CaTDeT (Cascaded Tracking
    Detector), which applies a tracker as assistance for refining intermediate feature
    maps in video object detection. The illustration of the algorithm is shown in
    Fig. [11](#S5.F11 "Figure 11 ‣ Patch Refinement ‣ 5.3.1\. Selection in the Spatial
    Dimension ‣ 5.3\. Subset Selection ‣ 5\. Leverage Data Redundancy in DNN for Videos
    ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning"). The
    tracker provides information on the region of interest based on historical detection.
    It reduces the operation count by $5.1\times$ to $8.7\times$ with a slight delay.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/08e3f89fba83ae08a02342e41c3e809b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11\. Illustration of algorithm flow in CaTDeT. Compare the inference-time
    workflows of the standard Faster R-CNN model and the refinement network. (a) Standard
    Faster R-CNN detector: the RPN takes the feature maps from the feature extractor.
    (b) Faster R-CNN detector for the refinement network: the proposals from the proposal
    network and the tracker instruct the feature extractor only to compute features
    on regions of interest. Regions of interest are a mask of all proposals over the
    frame. Figures and descriptions adapted from reference (Mao et al., [2019](#bib.bib113)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2\. Selection in the Temporal Dimension
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Korbar et al. propose SCSampler (Korbar et al., [2019](#bib.bib95)), a lightweight
    model for clip sampling that can invoke recognition on only the most salient clips.
    The paper targets scenarios where the videos are untrimmed and long. They aim
    to reduce the high inference cost when every clip is executed on the clip classifier.
    They apply both visual and audio samplers and combine their saliency for prediction.
    The visual sampler proposes a learning-based clip-level saliency model that provides
    each clip a saliency score between [0,1]. In particular, the model takes the input
    clip features that are fast to compute from the raw clip and have low dimensionality
    (lower than that of the classifier) to analyze each clip very efficiently. The
    clips with top-K saliency scores are extracted with features by the classifier
    and aggregated together to form the representation for prediction. The sampler
    learning phase labels a saliency score of video with $1$ if the action is contained
    in that clip; otherwise, it is labeled as $0$. The approach elevates the accuracy
    of an already state-of-the-art action classifier by $7\%$ and reduces its computational
    cost by more than 15 times.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, subset selection in the temporal dimension of the video can
    sometimes help improve task accuracy. This work typically involves the addition
    of some sophisticated mechanisms into the DNN. As a result, they often slow down
    the DNN rather than speed it up. But their goal is on the accuracy of the outputs
    rather than speed. A class of the work uses recurrent Neural Networks, such as
    Long Short Term Memory networks (LSTM), for capturing long-term dependencies in
    sequence data. They use them to generate compact video representations along with
    CNN activation maps (Yeung et al., [2016](#bib.bib161); Alwassel et al., [2018](#bib.bib5);
    Wu et al., [2019b](#bib.bib157)). They mimic the behavior of humans viewing videos
    that often consist of a glimpse followed by several refinements. In addition,
    some work (Wu et al., [2019a](#bib.bib155)) combines gated recurrent units (GRUs)
    and policy networks in reinforcement learning to adjust video sampling location
    accordingly at each time step.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3\. Selection in the Streaming Scenario
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The selection in a streaming scenario is more challenging. For each selection,
    the mechanisms need to make decisions on future frames based on historical records.
    Su et al. (Su and Grauman, [2016](#bib.bib143)) introduce an active mechanism
    that prioritizes ”which feature to compute when” to make timely action predictions.
    The core idea is to learn a policy that dynamically schedules the sequence of
    features to compute on selected frames of a given test video. The selection procedure
    can be invoked for either batch or streaming dimension. They formulate the problem
    with a Markov decision process (MDP) to learn the feature prioritization policy.
    The MDP sequentially selects frames with the most promising bag-of-objects or
    CNN features that can improve accuracy when combined with accumulated history
    results. Fig. [12](#S5.F12 "Figure 12 ‣ 5.3.3\. Selection in the Streaming Scenario
    ‣ 5.3\. Subset Selection ‣ 5\. Leverage Data Redundancy in DNN for Videos ‣ Survey:
    Exploiting Data Redundancy for Optimization of Deep Learning") illustrates the
    action spaces explored in the mechanism. On two challenging datasets (Activities
    of Daily Living (Pirsiavash and Ramanan, [2012](#bib.bib123)) and UCF-101 (Soomro
    et al., [2012](#bib.bib142))), their method provides significantly better accuracy
    than previous techniques under given computational budgets on two challenging
    datasets (Activities of Daily Living  and UCF-101 ).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/31245a2dfc5829ada4a90076f5593b26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12\. Action spaces. Top: In batch, the whole video is divided into sub-volumes,
    and actions are defined by the volume and object category to detect. Middle: In
    streaming, the video is divided into segments by the buffer at each step, and
    actions are the object category to detect in the buffer plus a “skip” action.
    Bottom: Our method learns a video-specific policy to select a sequence of valuable
    features to extract dynamically. Figure and description adapted from reference (Su
    and Grauman, [2016](#bib.bib143)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. Temporal Propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The neighboring frames in a video usually show some inherent continuity. In
    this type of optimization, the intermediate computation results of adjacent video
    frames such as feature maps, bounding boxes, or classification probability and
    confidence are propagated along the temporal dimension to reduce the need for
    re-computation on similar contents. Usually, some key frames are chosen, and the
    results are propagated from them to other frames. The optimizations are different
    in the representations used in propagation and update. The representations of
    frame features include Optical Flow, Motion History Image (MHI), and Convolutional
    LSTMs. To clarify, in Convolutional LSTMs, frames are represented in normal CNN
    feature maps, but the corresponding information is updated via LSTM networks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1\. Using Optical flow
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Zhu et al. (Zhu et al., [2017b](#bib.bib170)) propose a deep feature flow network
    (DFF). In DFF, neighboring frames are propagated with feature maps via a flow
    field for partial updates. The updates are obtained by a flow estimation algorithm
    like SIFT Flow (Liu et al., [2008](#bib.bib106)) or FlowNet (Dosovitskiy et al.,
    [2015](#bib.bib42)). Fig. [13](#S5.F13 "Figure 13 ‣ 5.4.1\. Using Optical flow
    ‣ 5.4\. Temporal Propagation ‣ 5\. Leverage Data Redundancy in DNN for Videos
    ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning") shows
    how information is propagated within neighboring frames. Compared to re-computing
    each frame through convolution layers, DFF is $10\times$ faster with only a few
    percent accuracy loss. Subsequently, the team comes up with the flow-guided feature
    aggregation (FGFA) (Zhu et al., [2017a](#bib.bib169)) that associates and assembles
    the rich appearance information in consecutive frames to improve feature representation
    and accuracy. Afterward, the authors unify the solutions into a common framework (Zhu
    et al., [2018](#bib.bib168)), and achieve $77.8\%$ mAP score at a speed of $15.22$
    frame per second for video object detection.'
  prefs: []
  type: TYPE_NORMAL
- en: Kang et al. (Kang et al., [2018](#bib.bib88)) leverage motion-guided propagation
    for minor temporal information refinement across adjacent frames. The assistance
    of temporal information reduces the potential false negatives prediction when
    frames are processed per frame.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e7945e9a555fa31b74454bab14bb8c6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13\. Illustration of temporal propagation in deep feature flow. (a) Video
    recognition using per-frame network evaluation (b) The proposed deep feature flow.
    Figures and descriptions adapted from reference (Zhu et al., [2017b](#bib.bib170)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2\. Using Motion History Image
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Unlike previous approaches that propagate key frames information at fixed intervals,
    Chen et al. (Chen et al., [2018](#bib.bib22)) propose Scale-Time Lattice that
    selects the key frames adaptively. The Scale-Time Lattice is a directed acyclic
    graph as shown in Fig. [14](#S5.F14 "Figure 14 ‣ 5.4.2\. Using Motion History
    Image ‣ 5.4\. Temporal Propagation ‣ 5\. Leverage Data Redundancy in DNN for Videos
    ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning"). Inside
    the graph, the node stands for the bounding box detection result at a particular
    spatial resolution and time point; the edge represents an operation that performs
    temporal propagation or spatial refinement. During the execution, given a video
    input, the framework first applies expensive object detectors to the key frames
    selected sparsely and adaptively based on the object motion and scale. Next, within
    the lattice, those bounding boxes are propagated to intermediate frames and refined
    across scales (from coarse to fine) via cheaper networks. A Propagation Refinement
    Unit (PRU) takes the detection results of two consecutive frames as input, propagates
    them to other frames, and re-scales them. The authors adopt Motion History Image
    (MHI) (Bobick and Davis, [2001](#bib.bib11)) since the motion representation computation
    is relatively cheap than optical flows. Overall, the proposed method yields 79.6
    mAP at 20 FPS and 79.0 mAP at 62 FPS on ImageNet VID dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a006f6407c4ee5421324a38f176fc763.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14\. The Scale-Time Lattice, where each node represents the detection
    results at a specific scale and time point, and each edge represents an operation
    from one node to another. In particular, the horizontal edges (in blue color)
    represent the temporal propagation from one time step to the next, while the vertical
    edges (in green color) represent the spatial refinement from low to high resolutions.
    Given a video, the image-based detection is only done at sparsely chosen key frames,
    and the results are propagated along a pre-defined path to the bottom row. The
    final results at the bottom cover all the time points. Figure and description
    adapted from reference (Chen et al., [2018](#bib.bib22)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.3\. Using Convolutional LSTMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Liu et al. (Zhu and Liu, [2018](#bib.bib166)) argue that temporal cues can
    be efficiently propagated through an LSTM network. The LSTM serves as an augmented
    structure to assist in refining temporal context for the generated features of
    each frame in the video. In the presented architecture in Fig. [15](#S5.F15 "Figure
    15 ‣ 5.4.3\. Using Convolutional LSTMs ‣ 5.4\. Temporal Propagation ‣ 5\. Leverage
    Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy for Optimization
    of Deep Learning"), a hypothesis feature map for each frame is generated by the
    CNN feature extractor and then fed into the LSTM to be fused with temporal context
    from previous frames. The output for that frame is a temporally-aware refined
    feature map. The experiment integrates the convolutional LSTMs with Single Shot
    Detector (SSD) (Liu et al., [2016](#bib.bib110)) and provides a quick inference
    speed with only 15 FPS on a mobile CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a1170929078a35432f695df2184c47db.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15\. An example illustration of our joint LSTM-SSD model. Multiple Convolutional
    LSTM layers are inserted into the network. Each propagates and refines feature
    maps at a particular scale. Figure and description adapted from reference (Zhu
    and Liu, [2018](#bib.bib166)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Kang et al. (Kang et al., [2017b](#bib.bib87)) leverage LSTMs instead of bounding
    box proposal refinement in video object recognition. The temporal information
    is propagated across each proposal tublet to improve accuracy. As shown in Fig. [16](#S5.F16
    "Figure 16 ‣ 5.4.3\. Using Convolutional LSTMs ‣ 5.4\. Temporal Propagation ‣
    5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning"), an encoder-decoder LSTM structure is placed
    after the Tublet Proposal Network. It generates the output probability of each
    class label as a prediction result for each proposal.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9239909bcc281eed4acdaf64d1e19b19.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16\. The proposed object detection system consists of two main parts.
    The first is a tubelet proposal network to generate tubelet proposals efficiently.
    The tubelet proposal network extracts multi-frame features within the spatial
    anchors, predicts the object motion patterns relative to the spatial anchors,
    and generates tubelet proposals. The gray box indicates the video clip, and different
    colors indicate the proposal process of other spatial anchors. The second part
    is an encoder-decoder CNN-LSTM network to extract tubelet features and classify
    each proposal box into different classes. The tubelet features are first fed into
    the encoder LSTM by a forward pass to capture the appearance features of the entire
    sequence. Then the states are copied to the decoder LSTM for a backward pass with
    the tubelet features. The encoder-decoder LSTM processes the whole clip before
    outputting class probabilities for each frame. Figure and description reference
    adapted from reference (Kang et al., [2017b](#bib.bib87)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.5\. Data Redundancy based Optimizations on 3D CNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have discussed how the performance of 2D CNN is advanced with works
    that exploit redundancy, especially at the temporal dimension where contiguous
    frames involve much identical information. Most of the optimizations are for the
    performance of video object detection, which is rather heavy on computation as
    the DNN has an additional region proposal phase. Still, some DNNs enable the analytical
    capability for videos with either 3D CNN (Ji et al., [2013](#bib.bib81); Tran
    et al., [2015](#bib.bib146)) or two-stream models (Simonyan and Zisserman, [2014](#bib.bib139);
    Karpathy et al., [2014](#bib.bib89)). Unlike how 2D CNN is operated on video data,
    3D CNN can end-to-end training and inference on videos. It is augmented with capturing
    semantics along the temporal dimension using stridden convolution filters.
  prefs: []
  type: TYPE_NORMAL
- en: Redundancy removal on video data for 3D CNN has been less explored at software-based
    optimizations. Some works (Carreira and Zisserman, [2017](#bib.bib13); Kopuklu
    et al., [2019](#bib.bib94)) propose to analyze the potential redundancy of the
    underlying network structures by progressively expanding the operator dimension
    from a 2D CNN basis. For example, Feichtenhofer et al. (Feichtenhofer, [2020](#bib.bib47))
    explore the 2D to 3D extension by considering the necessity of parameters or data
    in multiple dimensions. These include frame rate under fixed duration, temporal
    duration, spatial resolution, number of layers per residual stage, number of channels,
    and inner channel width in a residual block. These initiatives introduce another
    angle of viewing data redundancy on video DNNs. Future works offer prospective
    clues for advancing redundancy removal-based algorithms on deep models.
  prefs: []
  type: TYPE_NORMAL
- en: Still, existing works (Wang et al., [2020](#bib.bib153), [2019b](#bib.bib152);
    Fan et al., [2017a](#bib.bib45); Wang et al., [2017](#bib.bib150); Shen et al.,
    [2018](#bib.bib133); Hegde et al., [2018a](#bib.bib68); Chen et al., [2019c](#bib.bib20))
    have developed performance optimizations for 3D CNN that extend the fruitful efforts
    on the 2D CNN counterparts in hardware-based optimizations. The promising outcomes
    lie in that computation along the temporal dimensions is only required on the
    deltas (value differences) between adjacent video frames. They effectively alleviate
    the memory consumption and inference time bottleneck on resource-constraint devices.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Leverage Data Redundancy in Transformer Optimization for Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text-based redundancy optimization can be applied to Recurrent Neural Networks
    (RNNs), the widely used deep learning model for texts before the invention of
    the Transformer-based model. Guan et al. (Guan et al., [2021](#bib.bib61)) propose
    to leverage context-free grammar (CFG) and a hierarchical compression algorithm
    to squeeze the repeated contexts in sequence prediction tasks. The optimization
    accelerates RNN inference up to a thousand times and expends the prediction scope
    without losing task accuracy. Similar optimization insights can be found in many
    recent Transformer models as well. Aside from reducing model execution time or
    increasing task-specific accuracy, many data redundancy-based optimizations in
    Transformer models are initially motivated by the need to scale up the capacity
    of the models. Each time, the model can only process a sequence limited in length
    (the number of token representations) for token-level text inputs, subject to
    the hardware capability. The problem is especially crucial when it is necessary
    to process the representations of a paragraph or an extended sequence at once.
    As a result, researchers have systematically identified redundancy within the
    raw data or intermediate data representations to scale the model correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike CNN optimizations which leverage data redundancy at input data or intermediate
    feature maps between convolution layers, optimizations on Transformers can occur
    at different stages of interim data representations. A sequence of text input
    to the transformer model goes through several encoding phases, each capturing
    an extra level of meaning. The stages focus on the initial sentences or sub-words
    input, the sub-words embeddings, the hidden states produced by positional encoding,
    multi-head attention, or the encoder layer as a whole (multi-head attention and
    feed-forward layer together). Data redundancy can exist in each of the phases.
    Existing studies leverage the redundancy of various insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first stage, sub-word segmentation algorithms have been carried out
    with the motivation to remove redundancy in text data. Byte-Pair Encoding (BPE) (Sennrich
    et al., [2016](#bib.bib132)) is one such case. For the second stage, redundancy
    may exist in input embeddings. Compression at the vector representation of word
    embeddings was introduced long before the wide use of Transformer models (Kim
    et al., [2020a](#bib.bib91); Acharya et al., [2019](#bib.bib2); Chen et al., [2016](#bib.bib26)).
    For the remaining stages, there are optimizations that leverage redundancy at
    intermediate representation, ”hidden state” (a single sample output of a transformer
    layer; the unit is the same as activation map in CNN), of the model in general.
    Optimizations that leverage data redundancy in Transformer are classified based
    on how they exploit redundancy: clustering for reuse, skipping, and other ad-hoc
    sampling techniques. Table [2](#S6.T2 "Table 2 ‣ 6\. Leverage Data Redundancy
    in Transformer Optimization for Text ‣ Survey: Exploiting Data Redundancy for
    Optimization of Deep Learning") lists the surveyed work by their applications
    for better reference.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Summary of works on text data redundancy
  prefs: []
  type: TYPE_NORMAL
- en: '| Application | Paper | Code | Key Idea | Performance |'
  prefs: []
  type: TYPE_TB
- en: '| text summarization | (Liu et al., [2018a](#bib.bib109)) | (Liu, [2018](#bib.bib108))
    | T-DMCA is a modified multi-head self-attention structure to reduce the memory
    footprint in self-attention layers | possible to learn on sequence length of 12,000
    on GPU (Nvidia P100) |'
  prefs: []
  type: TYPE_TB
- en: '| text classification | (Dai et al., [2020](#bib.bib36)) | (Dai, [2020](#bib.bib35))
    | Funnel-Transformer augments the transformer model with an encoder to gradually
    pooled the representation to reduce the sequence-length of the hidden states as
    the layer goes deeper | $1.3\times$-$1.5\times$ speedup on GPU/TPU |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Goyal et al., [2020](#bib.bib59)) | (Goyal, [2020](#bib.bib58)) | PoWER-BERT
    applies extract layers and learning-based retention configuration to retain only
    the key embedding vectors in the transformer model | $4.5\times$ speedup on BERT-base
    models ($6.8\times$ with ALBERT) on GPU (Nvidia K80) |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Dalvi et al., [2020](#bib.bib39)) | (Dalvi, [2020](#bib.bib37)) | Exploit
    LayerSelector (LS) and Correlation Clustering Feature Selection (CCFS) to select
    the essence layers and features for transformer models | Evaluated on BERT and
    XLNET. Reduce forward pass parameters up to $47\%$. Reduce feature set to $4\%$
    (sequence labeling), $<1\%$ (sequence classification) on GPU (Nvidia Titan X)
    |'
  prefs: []
  type: TYPE_TB
- en: '| question answering | (Kitaev et al., [2020](#bib.bib93)) | (Kitaev, [2020](#bib.bib92))
    | Propose Locality-sensitive Hashing Attention and Reversible Transformer to address
    data redundancy | Attention speed can maintain as low as 0.1-0.5 seconds per step
    when sequence length per batch scale from 32 to 32768s (baseline is 0.2-5 seconds
    per step) on GPU/TPU |'
  prefs: []
  type: TYPE_TB
- en: 6.1\. Reuse via Clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kitaev and Kaiser et al. propose Reformer (Kitaev et al., [2020](#bib.bib93))
    with two main contributions in improving the transformer model: Locality-sensitive
    Hashing Attention and Reversible Transformer. In particular, Locality-sensitive
    Hashing Attention tackles data redundancy with a specific function design. The
    function uses an approximation in the scaled dot-product attention in a transformer
    model. The scaled dot-product attention is defined as $\frac{QK^{T}}{\sqrt{d_{k}}}V$,
    which $Q$, $K$, and $V$ are the query, key, and value matrices, respectively.
    The authors identify that the computation in the softmax function is dominated
    by key-query pairs that are in proximity. It indicates the calculation can pay
    attention to only a small subset of the closet key-query pairs. As exemplified
    in Fig. [17](#S6.F17 "Figure 17 ‣ 6.1\. Reuse via Clustering ‣ 6\. Leverage Data
    Redundancy in Transformer Optimization for Text ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning"), locality-sensitive hashing (LSH) (Gionis
    et al., [1999](#bib.bib57)) is employed to first cluster the keys and queries
    into hash buckets. Queries and keys in each bucket are sorted to form chunks to
    avoid sparsity in the attention matrix. Afterward, the scaled dot-product attention
    is substituted by allowing multiplication only between pairs in the same hash
    buckets (or in chunks in the attention matrix). Furthermore, the Locality-sensitive
    Hashing Attention introduces a multi-round LSH attention that enables multiple
    rounds of hashing to alleviate the problems that similar items may fall into different
    buckets after hashing. In the decoder of the transformer model, a masking mechanism
    is implemented to re-order the position indices by the same permutations that
    were previously applied to sort the key and query vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c7b71d35c454f80c020696d6122c1e81.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17\. Locality-sensitive hashing attention in Reformer. LSH Attention
    shows the hash-bucketing, sorting, and chunking steps and the resulting causal
    attentions. (a-d) Attention matrices for these varieties of attention. Figures
    adapted from reference (Kitaev et al., [2020](#bib.bib93)).
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Skipping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Output representations from attention layers in Transformer models have been
    recognized as containing much redundant information (Michel et al., [2019](#bib.bib114);
    Voita et al., [2019](#bib.bib148)). They have been addressed through ways of attention
    head pruning (Michel et al., [2019](#bib.bib114); Voita et al., [2019](#bib.bib148)).
    Recent studies have investigated the criteria for pruning neurons (activations)
    in Transformer models. Specifically, Gupta et al. (Gupta et al., [2020](#bib.bib64))
    suggest neurons are to be pruned when either the importance of that neuron is
    low or when the same activation multiplies with the same weights. The neuron importance
    function (Sanh et al., [2019](#bib.bib131)) is defined on entropy, output-weights
    norm, or the input-weights norm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hidden state of a sequence of word vectors may contain repetitive information
    themselves. Goyal et al. (Goyal et al., [2020](#bib.bib59)) identify that the
    pair-wise cosine similarity between word vectors in hidden states increases as
    layers go progressively deeper (encode by more phases). They propose PoWER-BERT
    to exploit the redundancy to reduce the inference time of the BERT model in text
    classification and regression tasks. As illustrated in Fig. [18](#S6.F18 "Figure
    18 ‣ 6.2\. Skipping ‣ 6\. Leverage Data Redundancy in Transformer Optimization
    for Text ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning"),
    PoWER-BERT uses an Extract Layer to manage the retention configuration after the
    Self-Attention Layer and before the Feed Forward Network (FFN) in each encoder
    layer of the Transformer model. The retention configuration can be set manually
    or set with parameters from training. The model is trained with a loss function
    proposed to obtain the learning-based design. This method avoids the exponential
    search space in retention configuration. Furthermore, they employ two kinds of
    strategies to retain the word representations: static and the dynamic strategy.
    The static strategy keeps the word vectors at the same positions across all input
    sequences in the dataset. The dynamic strategy retains the word vectors based
    on the attention scoring and a soft-extract layer. In the experiment, they obtain
    a $4.5\times$ inference time acceleration over the baseline BERT model with less
    than $1\%$ accuracy loss. The proposed technique is also compatible with the compression
    scheme such as ALBERT (Lan et al., [2020](#bib.bib97)). The combination of both
    compression methods achieves a $6.8\times$ inference time reduction with less
    than $1\%$ accuracy loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/58656fad897364e9aca1943c0d6c6d26.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18\. Illustration of PoWER-BERT. Word-vector selection over the first
    two encoders. Here, $N=6$, $l_{1}=4$ and $l_{2}=2$. The first encoder eliminates
    two word-vectors $w2$ and $w4$ with least significance scores; the second encoder
    further eliminates word-vectors $w1$ and $w5$. Figure and description adapted
    from reference (Goyal et al., [2020](#bib.bib59)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Dalvi et al. (Dalvi et al., [2020](#bib.bib39)) propose an efficient transfer
    learning method that exploits LayerSelector (LS) and Correlation Clustering Feature
    Selection (CCFS) to select the essence layers and features required during transformer
    model execution. There are three main steps in the selection: (1) LayerSelector
    (LS) uses the layer-classifier to select the lowest layer that maintains oracle
    performance. (2) Correlation Clustering filters out redundant neurons given the
    hidden state’s output of the previous layer. (3) Feature Selection (FS) selects
    a minimal set of neurons required to reach optimum performance on the given task.'
  prefs: []
  type: TYPE_NORMAL
- en: For the first step, the authors analyze task-specific layer-level redundancy
    by training linear probing classifiers (Shi et al., [2016](#bib.bib134); Belinkov
    et al., [2017](#bib.bib8)) on each layer $l_{i}$ as layer-classifier. They use
    the layer-classifier to select the lowest layer that maintains oracle performance
    (maintaining $99\%$ performance). In LS, a concentration of all layers until the
    chosen layer is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the following two steps, the CCFS exploits data redundancy by a combination
    of clustering and subset selection:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Correlation Clustering (CC) (Bansal et al., [2004](#bib.bib7)): For every length-N
    sequence vector representation, the product-moment correlation between each of
    the two features is calculated. This gives a $N$-by-$N$ matrix $corr(x,y)$ representing
    the correlation between $fx$ and $fy$. The correlation value ranges from $-1$
    to $1$, which provides a relative scale to compare any two neurons. The distance
    metric $cdist(x,y)$ between these two features is defined by $1-|corr(x,y)|$.
    A hyper-parameter $ct$ defines the maximum distance between any two features to
    be considered as a cluster.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feature Selection (FS): It identifies a minimal set of neurons that match the
    oracle performance. The Linguistic Correlation Analysis (Dalvi et al., [2019](#bib.bib38))
    ranks the neurons concerning a downstream task.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Overall, the work finds out that up to $85\%$ and $95\%$ neurons are redundant
    in BERT and XLNet (Yang et al., [2019](#bib.bib159)) respectively. In the experiment,
    the proposed method accelerates sequence labeling tasks by $2.8\times$ and $6.2\times$
    on BERT and XLNet, respectively. While for sequence classification tasks, the
    average speedups are $1.1\times$ and $2.8\times$ correspondingly.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Other Sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Humans implicitly use prior linguistic knowledge to merge nearby tokens or
    words in paragraphs. This, in turn, forms more extensive semantic phrases or units
    to understand sentences or paragraphs in a general form. Inspired by that, Dai
    et al. (Dai et al., [2020](#bib.bib36)) have proposed Funnel-Transformer. As shown
    in Fig. [19](#S6.F19 "Figure 19 ‣ 6.3\. Other Sampling ‣ 6\. Leverage Data Redundancy
    in Transformer Optimization for Text ‣ Survey: Exploiting Data Redundancy for
    Optimization of Deep Learning"), the Funnel-Transformer is equipped with an additional
    encoder to gradually pool the representation to reduce the sequence length of
    the hidden states as the layer goes deeper. The sub-modules have an extra residual
    connection and layer normalization operation in the Self-Attention Layer and the
    Feed Forward Network (FFN) of the Transformer model. The decoder is applied to
    reconstruct the full-sequence of representation when the token-level outputs are
    required, such as pretraining.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/05e3da46148e89369dc676341f146f9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19\. Illustration of Pooling in Funnel-Transformer. The encoder on the
    left consists of several blocks of consecutive Transformer layers. Within each
    block, the sequence length of the hidden states always remains the same. But when
    going from a lower-level block to a higher-level block, the size of the hidden
    sequence is reduced by performing a particular type of pooling along the sequence
    dimension. The right part showcases the up-sampling to re-construct the arrangement
    of the original length. Figure and description adapted from reference (Dai et al.,
    [2020](#bib.bib36)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve the generation of Wikipedia-styled summarization over multi-documents,
    Liu et al. (Liu et al., [2018a](#bib.bib109)) introduce a compressive scheme for
    long text sequences. The Transformer Decoder with Memory-Compressed Attention(T-DMCA)
    is a modified multi-head self-attention structure to reduce the memory footprint
    via limiting the dot product between the query and key of a Self-Attention Layer.
    The mechanism consists of the Memory-Compressed Attention and the Local Attention
    as shown in Fig. [20](#S6.F20 "Figure 20 ‣ 6.3\. Other Sampling ‣ 6\. Leverage
    Data Redundancy in Transformer Optimization for Text ‣ Survey: Exploiting Data
    Redundancy for Optimization of Deep Learning"). The Memory-Compressed Attention
    reduces the number of keys and values by using a stridden convolution (for sampling).
    The Local Attention divides a sequence of tokens into blocks of similar length
    of tokens (they use 256 tokens) to allow constant attention memory cost per block
    regardless of sequence length. After feeding the sub-sequence to their corresponding
    Multi-head Attention to capture local information, the method merges the results
    to get the final output sequence. For both Local and Memory-Compressed Attention,
    masking is cast to prevent the queries from attending to future keys and values.
    Finally, the design enables the model to process 3$\times$-longer sequences than
    the default model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e8f2be5e05b7b970a963e7b035af32f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20\. Transformer Decoder with Memory-Compressed Attention(T-DMCA). Every
    attention layer takes a sequence of tokens as input and produces a sequence of
    similar length as the output. Left: Original self-attention as used in the transformer
    decoder. Middle: Memory-compressed attention, which reduces the number of keys/values.
    Right: Local attention, which splits the sequence into individual smaller sub-sequences.
    The sub-sequences are then merged to get the final output sequence. Figure and
    reference adapted from reference (Liu et al., [2018a](#bib.bib109)).'
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Future Research Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The many previous studies have clearly shown the significant benefits of exploitation
    of data redundancy in Deep Learning. In creating the summary of the many explorations
    from various aspects, we have recognized the broad coverage of the existing innovations,
    but at the same, observed some open problems and several research directions worth
    pursuing in the future.
  prefs: []
  type: TYPE_NORMAL
- en: (i) Principled understanding of the relations between data redundancy exploitation
    and accuracy. Data redundancy exploitation could cause changes in the output of
    a DNN because the removed data are often not guaranteed to be redundant. In most
    cases, it faces the risks of degrading the accuracy, but in some cases, as mentioned
    earlier (Chin et al., [2019](#bib.bib30); Korbar et al., [2019](#bib.bib95); Su
    and Grauman, [2016](#bib.bib143)), it may also improve the accuracy for its noise
    removal effects. There is a lack of principled understanding of how data redundancy
    elimination affects model accuracy. The accuracy of a DNN model can be affected
    by existing variance related to data. Dataset itself may already contain bias
    over DNN models (Torralba and Efros, [2011](#bib.bib145)). Data preprocessing,
    such as data augmentation (Shorten and Khoshgoftaar, [2019](#bib.bib137); Cubuk
    et al., [2019](#bib.bib34)), makes the discussion even more complicated. Previous
    work has either resorted to empirical tuning to reach a satisfying point or used
    some heuristic methods to rate the salience of some data. An open question is
    how to achieve a principled understanding of the impact and transform the knowledge
    into principled data redundancy exploitation techniques. Some rigorous studies
    may help.
  prefs: []
  type: TYPE_NORMAL
- en: (ii) Enhancing interpretability. Related to the first direction, the second
    direction that may be worth exploring is interpretability, which refers to both
    the interpretability of neuron representation and the interpretability of DNN
    in general. Utilizing evidence in data redundancy has led to some initial success
    in improving the interpretability of CNN’s feature representation, contributing
    to the CNN model pruning processes. For example, Li et al. (Li et al., [2019a](#bib.bib103))
    propose a kernel sparsity and entropy indicator (KSE) to quantify the importance
    of each pixel in the activation map to provide feature-agonistic guidance in weight
    compression. Identifications of data redundancy also benefit the interpretation
    of the importance of each word in text processing. For example, Dalvi et al. (Dalvi
    et al., [2019](#bib.bib38)) present Linguistic Correlation Analysis and Cross-model
    Correlation Analysis to recognize the relative importance of a neuron in a Transformer
    model. Their method offers an ablation view on the saliency of words in the paragraphs
    to guide future representation development. These several studies show some promise
    in exploring data redundancy for the interpretability of deep learning. As interpretability
    is important for connecting Deep Learning with domain understanding, more efforts
    are worthwhile to develop further.
  prefs: []
  type: TYPE_NORMAL
- en: (iii) Other data types and models. A large portion of the prior work on data
    redundancy is about image and video data. There is still some room left for more
    explorations on these data types. An example is learning from point cloud (Guo
    et al., [2020](#bib.bib63)) where data redundancy has not yet been explored. But
    in comparison to images and videos, redundancy in text data is much less explored,
    as Transformer is more recently developed than the more mature models used in
    images and videos. More explorations are especially in demand to better understand
    data redundancy in texts. Moreover, besides the three types of data, there are
    other types of data on which DNN is also playing an important role. These data
    include graphs, scientific simulation results, genes, computer programs, and so
    on. Correspondingly, the special properties of these data have prompted the development
    of some new DNN models, such as Graph Convolutional Networks (GCN) (Wu et al.,
    [2020](#bib.bib156)). The differences in data properties, DNN models, and learning
    tasks suggest that some research could be fruitful in understanding and exploiting
    data redundancy in these areas.
  prefs: []
  type: TYPE_NORMAL
- en: '(iv) Relations among optimizations and systematic solutions. One of the questions
    that have not received much attention is the relations among the many kinds of
    optimizations on data redundancy elimination, as well as their relations with
    model optimizations. On images, for instance, as Section [4](#S4 "4\. Leverage
    Redundancy in Image Data ‣ Survey: Exploiting Data Redundancy for Optimization
    of Deep Learning") and Table [1](#S3.T1 "Table 1 ‣ 3.7\. Use of the Taxonomy ‣
    3\. Taxonomy ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning")
    show, there are many ways to exploit data redundancy from various angles. Can
    these optimizations be used together? Is it worth doing that? Would there be some
    conflicts besides that they may all affect the model accuracy? How to use them
    together in the best way? More ambitiously, is it possible to build up frameworks
    that automatically apply one or more kinds of data redundancy exploitation techniques
    on a given dataset and learning task? How about the interplay with model optimizations?
    Answers to these questions may significantly advance the current understanding
    of data redundancy exploitation and tap into the potential better.'
  prefs: []
  type: TYPE_NORMAL
- en: (v) Synergy with DNN accelerator designs. Yet another direction worth looking
    into is the synergy between the many data redundancy exploitation techniques and
    DNN hardware designs. In modern application-specific integrated circuit (ASIC)
    design, dataflow analysis (Kwon et al., [2019](#bib.bib96); Ben-Nun et al., [2019](#bib.bib9);
    Ivanov et al., [2021](#bib.bib80)) has been recognized as a crucial consideration.
    Some work on edge devices has tried to address memory capacity issues by exploiting
    data redundancy (e.g., through data compression) (Mocerino et al., [2019](#bib.bib115);
    Salamat et al., [2018](#bib.bib129); Hegde et al., [2018b](#bib.bib69); Wang et al.,
    [2020](#bib.bib153), [2019b](#bib.bib152); Fan et al., [2017a](#bib.bib45); Wang
    et al., [2017](#bib.bib150); Shen et al., [2018](#bib.bib133); Hegde et al., [2018a](#bib.bib68)).
    But in general, there is no systematic understanding of the relations of the many
    data redundancy exploitation techniques and the designs of DNN hardware accelerators.
    On the one hand, do those techniques stay profitable when DNN runs on hardware
    accelerators? On the other hand, can hardware accelerator designs gain some efficiency
    benefits by adopting some of the ideas in those techniques? Answers to these questions
    could lead to the novel synergy between the two strands of efforts.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning is based on two pillars, models and data. As an essential part
    of deep learning, data plays an important role. Making the best use of data determines
    the quality, speed, efficiency, and interpretability of machine learning. This
    survey summarizes the efforts in the research community in detecting and leveraging
    data redundancy in a variety of dimensions, introduces the first known taxonomy
    to categorize the existing techniques, and points out a set of directions yet
    to explore. As the landscape of machine learning evolves continuously at an unprecedented
    speed, we hope that this survey can provide a one-stop resource for researchers
    to attain a quick understanding of state of the art and open issues, and hence
    benefit the industry practitioners and research community in selecting existing
    solutions for solving a problem at hands or creating new solutions to advance
    this critical field further.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acharya et al. (2019) Anish Acharya, Rahul Goel, Angeliki Metallinou, and Inderjit
    Dhillon. 2019. Online embedding compression for text classification using low
    rank matrix factorization. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 33\. None, None, 6196–6203.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Akhlaghi et al. (2018) Vahideh Akhlaghi, Amir Yazdanbakhsh, Kambiz Samadi,
    Rajesh K Gupta, and Hadi Esmaeilzadeh. 2018. Snapea: Predictive early activation
    for reducing computation in deep convolutional neural networks. In *Proceedings
    of the 45th ACM/IEEE Annual International Symposium on Computer Architecture (ISCA)*.
    None, None, 662–673.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alwassel (2017) Humam Alwassel. 2017. GitHub. Action Search: Spotting Targets
    in Videos and Its Application to Temporal Action Localization. [https://github.com/HumamAlwassel/action-search](https://github.com/HumamAlwassel/action-search)
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alwassel et al. (2018) Humam Alwassel, Fabian Caba Heilbron, and Bernard Ghanem.
    2018. Action search: Spotting actions in videos and its application to temporal
    action localization. In *Proceedings of the European Conference on Computer Vision
    (ECCV)*. None, None, 251–266.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aytar et al. (2016) Yusuf Aytar, Carl Vondrick, and Antonio Torralba. 2016.
    Soundnet: Learning sound representations from unlabeled video. In *Advances in
    Neural Information Processing Systems*. None, None, 892–900.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bansal et al. (2004) Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004. Correlation
    clustering. *Machine learning* 56, 1-3 (2004), 89–113.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Belinkov et al. (2017) Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan
    Sajjad, and James Glass. 2017. What do Neural Machine Translation Models Learn
    about Morphology?. In *Proceedings of the 55th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*. Association for Computational
    Linguistics, Vancouver, Canada, 861–872.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ben-Nun et al. (2019) Tal Ben-Nun, Johannes de Fine Licht, Alexandros N Ziogas,
    Timo Schneider, and Torsten Hoefler. 2019. Stateful Dataflow Multigraphs: A data-centric
    model for performance portability on heterogeneous architectures. In *Proceedings
    of the International Conference for High Performance Computing, Networking, Storage
    and Analysis*. None, None, 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blalock et al. (2020) Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle,
    and John Guttag. 2020. What is the state of neural network pruning? *Proceedings
    of machine learning and systems* 2, None (2020), 129–146.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bobick and Davis (2001) Aaron F. Bobick and James W. Davis. 2001. The recognition
    of human movement using temporal templates. *IEEE Transactions on pattern analysis
    and machine intelligence* 23, 3 (2001), 257–267.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Campbell and Robson (1968) Fergus W Campbell and John G Robson. 1968. Application
    of Fourier analysis to the visibility of gratings. *The Journal of physiology*
    197, 3 (1968), 551.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carreira and Zisserman (2017) Joao Carreira and Andrew Zisserman. 2017. Quo
    vadis, action recognition? a new model and the kinetics dataset. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*.
    None, None, 6299–6308.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cavigelli (2017) Lukas Cavigelli. 2017. Papers with Code. CBinfer: Change-Based
    Inference for Convolutional Neural Networks on Video Data. [https://paperswithcode.com/paper/cbinfer-change-based-inference-for](https://paperswithcode.com/paper/cbinfer-change-based-inference-for)
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cavigelli (2018) Lukas Cavigelli. 2018. Papers with Code. CBinfer: Exploiting
    Frame-to-Frame Locality for Faster Convolutional Network Inference on Video Streams.
    [https://paperswithcode.com/paper/cbinfer-exploiting-frame-to-frame-locality](https://paperswithcode.com/paper/cbinfer-exploiting-frame-to-frame-locality)
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cavigelli and Benini (2019) Lukas Cavigelli and Luca Benini. 2019. CBinfer:
    Exploiting frame-to-frame locality for faster convolutional network inference
    on video streams. *IEEE Transactions on Circuits and Systems for Video Technology*
    30, 5 (2019), 1451–1465.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cavigelli et al. (2017) Lukas Cavigelli, Philippe Degen, and Luca Benini. 2017.
    CBinfer: Change-Based Inference for Convolutional Neural Networks on Video Data.
    In *Proceedings of the 11th International Conference on Distributed Smart Cameras*
    (Stanford, CA, USA) *(ICDSC 2017)*. Association for Computing Machinery, New York,
    NY, USA, 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chakraborty et al. (2019) Sinjan Chakraborty, Sayantan Paul, Ram Sarkar, and
    Mita Nasipuri. 2019. Feature map reduction in CNN for handwritten digit recognition.
    In *Recent Developments in Machine Learning and Data Analytics*. Springer, None,
    143–148.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019b) Hesen Chen, Ming Lin, Xiuyu Sun, Qian Qi, Hao Li, and Rong
    Jin. 2019b. Muffnet: Multi-layer feature federation for mobile deep learning.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops*.
    None, None, 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019c) Huixiang Chen, Mingcong Song, Jiechen Zhao, Yuting Dai,
    and Tao Li. 2019c. 3D-based video recognition acceleration by leveraging temporal
    locality. In *Proceedings of the 46th ACM/IEEE Annual International Symposium
    on Computer Architecture (ISCA)*. None, None, 79–90.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020a) Hanting Chen, Yunhe Wang, Han Shu, Yehui Tang, Chunjing
    Xu, Boxin Shi, Chao Xu, Qi Tian, and Chang Xu. 2020a. Frequency Domain Compact
    3D Convolutional Neural Networks. In *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR)*. None, None, 1641–1650.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018) Kai Chen, Jiaqi Wang, Shuo Yang, Xingcheng Zhang, Yuanjun
    Xiong, Chen Change Loy, and Dahua Lin. 2018. Optimizing Video Object Detection
    via a Scale-Time Lattice. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*. None, None, 7814–7823.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019d) Weijie Chen, Yuan Zhang, Di Xie, and Shiliang Pu. 2019d.
    A layer decomposition-recomposition framework for neuron pruning towards accurate
    lightweight networks. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 33\. None, None, 3355–3362.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen (2019) Yunpeng Chen. 2019. Papers with Code. Drop an Octave: Reducing
    Spatial Redundancy in Convolutional Neural Networks with Octave Convolution. [https://paperswithcode.com/paper/drop-an-octave-reducing-spatial-redundancy-in](https://paperswithcode.com/paper/drop-an-octave-reducing-spatial-redundancy-in)
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019a) Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis
    Kalantidis, Marcus Rohrbach, Shuicheng Yan, and Jiashi Feng. 2019a. Drop an octave:
    Reducing spatial redundancy in convolutional neural networks with octave convolution.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*.
    None, None, 3435–3444.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2016) Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, and Zhi Jin. 2016.
    Compressing Neural Language Models by Sparse Word Representations. In *Proceedings
    of the 54th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*. Association for Computational Linguistics, Berlin, Germany,
    226–235.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen (2020) Zhuo Chen. 2020. Papers with Code. ViP: Virtual Pooling for Accelerating
    CNN-based Image Classification and Object Detection. [https://paperswithcode.com/paper/vip-virtual-pooling-for-accelerating-cnn](https://paperswithcode.com/paper/vip-virtual-pooling-for-accelerating-cnn)
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020b) Zhuo Chen, Jiyuan Zhang, Ruizhou Ding, and Diana Marculescu.
    2020b. Vip: Virtual pooling for accelerating cnn-based image classification and
    object detection. In *The IEEE Winter Conference on Applications of Computer Vision*.
    None, None, 1180–1189.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2018) Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2018. Model
    Compression and Acceleration for Deep Neural Networks: The Principles, Progress,
    and Challenges. *IEEE Signal Processing Magazine* 35, 1 (2018), 126–136.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chin et al. (2019) Ting-Wu Chin, Ruizhou Ding, and Diana Marculescu. 2019.
    AdaScale: Towards Real-time Video Object Detection using Adaptive Scaling. In
    *Proceedings of Machine Learning and Systems 2019*. None, None, 431–441.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chitsaz et al. (2020) Kamran Chitsaz, Mohsen Hajabdollahi, Nader Karimi, Shadrokh
    Samavi, and Shahram Shirani. 2020. Acceleration of convolutional neural network
    using FFT-based split convolutions. *arXiv preprint arXiv:2003.12621* None, None
    (2020), 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choudhary et al. (2020) Tejalal Choudhary, Vipul Mishra, Anurag Goswami, and
    Jagannathan Sarangapani. 2020. A comprehensive survey on model compression and
    acceleration. *Artificial Intelligence Review* None, None (2020), 1–43.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D.
    Manning. 2019. What Does BERT Look At? An Analysis of BERT’s Attention. In *BlackBoxNLP@ACL*.
    None, None, 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cubuk et al. (2019) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan,
    and Quoc V Le. 2019. Autoaugment: Learning augmentation strategies from data.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. None, None, 113–123.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai (2020) Zihang Dai. 2020. GitHub. Funnel-Transformer. [https://github.com/laiguokun/Funnel-Transformer](https://github.com/laiguokun/Funnel-Transformer)
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2020) Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-Transformer:
    Filtering out Sequential Redundancy for Efficient Language Processing. In *Advances
    in Neural Information Processing Systems*. None, None, 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dalvi (2020) Fahim Dalvi. 2020. Papers with Code. Analyzing Redundancy in Pretrained
    Transformer Models. [https://paperswithcode.com/paper/exploiting-redundancy-in-pre-trained-language](https://paperswithcode.com/paper/exploiting-redundancy-in-pre-trained-language)
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dalvi et al. (2019) Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov,
    Anthony Bau, and James Glass. 2019. What is one grain of sand in the desert? analyzing
    individual neurons in deep nlp models. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 33\. None, None, 6309–6317.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dalvi et al. (2020) Fahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov.
    2020. Analyzing redundancy in pretrained transformer models. In *Proceedings of
    the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*.
    None, None, 4908–4926.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de Moura et al. (2019) Rafael Fão de Moura, Paulo C Santos, João Paulo C de
    Lima, Marco AZ Alves, Antonio CS Beck, and Luigi Carro. 2019. Skipping CNN convolutions
    through efficient memoization. In *International Conference on Embedded Computer
    Systems*. Springer, None, None, 65–76.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: De Valois and De Valois (1980) Russell L De Valois and Karen K De Valois. 1980.
    Spatial vision. *Annual review of psychology* 31, 1 (1980), 309–341.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2015) Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
    Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers,
    and Thomas Brox. 2015. Flownet: Learning optical flow with convolutional networks.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*.
    None, None, 2758–2766.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du Tran et al. (2017) Heng Wang Du Tran, Lorenzo Torresani, Jamie Ray, Yann
    LeCun, and Manohar Paluri. 2017. A closer look at spatiotemporal convolutions
    for action recognition. 2018 IEEE. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. None, None, 6450–6459.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding (2020) Word Embedding. 2020. Retrieved from:. [https://en.wikipedia.org/wiki/Word_embedding](https://en.wikipedia.org/wiki/Word_embedding)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2017a) Hongxiang Fan, Xinyu Niu, Qiang Liu, and Wayne Luk. 2017a.
    F-c3d: Fpga-based 3-dimensional convolutional neural network. In *2017 27th International
    Conference on Field Programmable Logic and Applications (FPL)*. IEEE, None, None,
    1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2017b) Yang Fan, Fei Tian, Tao Qin, Jiang Bian, and Tie-Yan Liu.
    2017b. Neural data filter for boot-strapping stochastic gradient descent.. In
    *Proceedings of the 5th International Conference on Learning Representations (ICLR)*.
    None, None, 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feichtenhofer (2020) Christoph Feichtenhofer. 2020. X3D: Expanding Architectures
    for Efficient Video Recognition. In *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR)*. None, None, 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2019) Shuo Feng, Huiyu Zhou, and Hongbiao Dong. 2019. Using deep
    neural network with small dataset to predict material defects. *Materials & Design*
    162 (2019), 300–310.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figurnov (2016) Michael Figurnov. 2016. GitHub. perforated-cnn-matconvnet. [https://github.com/mfigurnov/perforated-cnn-matconvnet](https://github.com/mfigurnov/perforated-cnn-matconvnet)
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figurnov et al. (2016) Mikhail Figurnov, Aizhan Ibraimova, Dmitry P Vetrov,
    and Pushmeet Kohli. 2016. Perforatedcnns: Acceleration through elimination of
    redundant convolutions. In *Advances in Neural Information Processing Systems*.
    None, None, 947–955.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaikwad and El-Sharkawy (2019) Akash Sunil Gaikwad and Mohamed El-Sharkawy.
    2019. Pruning the Convolution Neural Network (SqueezeNet) based on L 2 Normalization
    of Activation Maps. In *2019 IEEE 9th Annual Computing and Communication Workshop
    and Conference (CCWC)*. IEEE, None, None, 0392–0396.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ganesh et al. (2021) Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan,
    Yin Yang, Hassan Sajjad, Preslav Nakov, Deming Chen, and Marianne Winslett. 2021.
    Compressing large-scale transformer-based models: A case study on bert. *Transactions
    of the Association for Computational Linguistics* 9, None (2021), 1061–1080.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2018) Mingfei Gao, Ruichi Yu, Ang Li, Vlad I Morariu, and Larry S
    Davis. 2018. Dynamic zoom-in network for fast object detection in large images.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. None, None, 6926–6935.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao (2019) Xitong Gao. 2019. Papers with Code. Dynamic Channel Pruning: Feature
    Boosting and Suppression. [https://paperswithcode.com/paper/dynamic-channel-pruning-feature-boosting-and](https://paperswithcode.com/paper/dynamic-channel-pruning-feature-boosting-and)
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2019) Xitong Gao, Yiren Zhao, Lukasz Dudziak, Robert D. Mullins,
    and Cheng-Zhong Xu. 2019. Dynamic Channel Pruning: Feature Boosting and Suppression.
    In *Proceedings of the 7th International Conference on Learning Representations
    (ICLR)*. None, None, 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Georgiadis (2019) Georgios Georgiadis. 2019. Accelerating Convolutional Neural
    Networks via Activation Map Compression. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. None, None, 7085–7095.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gionis et al. (1999) Aristides Gionis, Piotr Indyk, and Rajeev Motwani. 1999.
    Similarity Search in High Dimensions via Hashing. In *Proceedings of the 25th
    International Conference on Very Large Data Bases* *(VLDB ’99)*. Morgan Kaufmann
    Publishers Inc., San Francisco, CA, USA, 518–529.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goyal (2020) Saurabh Goyal. 2020. Papers with Code. PoWER-BERT: Accelerating
    BERT Inference via Progressive Word-vector Elimination. [https://paperswithcode.com/paper/power-bert-accelerating-bert-inference-for](https://paperswithcode.com/paper/power-bert-accelerating-bert-inference-for)
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goyal et al. (2020) Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan
    Chakaravarthy, Yogish Sabharwal, and Ashish Verma. 2020. PoWER-BERT: Accelerating
    BERT Inference via Progressive Word-vector Elimination. In *Proceedings of the
    International Conference on Machine Learning*, Vol. 119\. None, None, 3690–3699.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graham (2014) Benjamin Graham. 2014. Fractional Max-Pooling. *CoRR* abs/1412.6071
    (2014), 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guan et al. (2021) Hui Guan, Umang Chaudhary, Yuanchao Xu, Lin Ning, Lijun
    Zhang, and Xipeng Shen. 2021. Recurrent Neural Networks Meet Context-Free Grammar:
    Two Birds with One Stone. In *2021 IEEE International Conference on Data Mining
    (ICDM)*. None, None, 1078–1083.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo (2018) Yunhui Guo. 2018. A Survey on Methods and Theories of Quantized Neural
    Networks. *CoRR* abs/1808.04752 (2018), 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2020) Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and
    Mohammed Bennamoun. 2020. Deep learning for 3d point clouds: A survey. *IEEE transactions
    on pattern analysis and machine intelligence* None, None (2020), 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta et al. (2020) Manish Gupta, Vasudeva Varma, Sonam Damani, and Kedhar Nath
    Narahari. 2020. Compression of Deep Learning Models for NLP. In *Proceedings of
    the 29th ACM International Conference on Information & Knowledge Management*.
    None, None, 3507–3508.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2016) Wei Han, Pooya Khorrami, Tom Le Paine, Prajit Ramachandran,
    Mohammad Babaeizadeh, Honghui Shi, Jianan Li, Shuicheng Yan, and Thomas S. Huang.
    2016. Seq-NMS for Video Object Detection. *CoRR* abs/1602.08465, None (2016),
    0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*. None, None, 770–778.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2017) Yihui He, Xiangyu Zhang, and Jian Sun. 2017. Channel pruning
    for accelerating very deep neural networks. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*. None, None, 1389–1397.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hegde et al. (2018a) Kartik Hegde, Rohit Agrawal, Yulun Yao, and Christopher W
    Fletcher. 2018a. Morph: Flexible acceleration for 3D CNN-based video understanding.
    In *Proceedings of the 51th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*. None, None, 933–946.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hegde et al. (2018b) Kartik Hegde, Jiyong Yu, Rohit Agrawal, Mengjia Yan, Michael
    Pellauer, and Christopher Fletcher. 2018b. Ucnn: Exploiting computational reuse
    in deep neural networks via weight repetition. In *Proceedings of the 45th ACM/IEEE
    Annual International Symposium on Computer Architecture (ISCA)*. None, None, 674–687.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2014) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014. Distilling
    the Knowledge in a Neural Network. In *Advances in Neural Information Processing
    Systems Deep Learning Workshop*. None, None, 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou and Kung (2020a) Zejiang Hou and Sun-Yuan Kung. 2020a. A Feature-map Discriminant
    Perspective for Pruning Deep Neural Networks. *CoRR* abs/2005.13796, None (2020),
    0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou and Kung (2020b) Zejiang Hou and Sun-Yuan Kung. 2020b. Efficient Image Super
    Resolution Via Channel Discriminative Deep Neural Network Pruning. In *ICASSP
    2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*. IEEE, None, None, 3647–3651.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu (2016) Hengyuan Hu. 2016. Papers with Code. Network Trimming: A Data-Driven
    Neuron Pruning Approach towards Efficient Deep Architectures. [https://paperswithcode.com/paper/network-trimming-a-data-driven-neuron-pruning](https://paperswithcode.com/paper/network-trimming-a-data-driven-neuron-pruning)
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2016) Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. 2016.
    Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep
    Architectures. *CoRR* abs/1607.03250, None (2016), 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang (2018) Zehao Huang. 2018. Papers with Code. Data-Driven Sparse Structure
    Selection for Deep Neural Networks. [https://paperswithcode.com/paper/data-driven-sparse-structure-selection-for](https://paperswithcode.com/paper/data-driven-sparse-structure-selection-for)
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang and Wang (2018) Zehao Huang and Naiyan Wang. 2018. Data-driven sparse
    structure selection for deep neural networks. In *Proceedings of the European
    Conference on Computer Vision (ECCV)*. None, None, 304–320.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hubara et al. (2017) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran
    El-Yaniv, and Yoshua Bengio. 2017. Quantized neural networks: Training neural
    networks with low precision weights and activations. *The Journal of Machine Learning
    Research* 18, 1 (2017), 6869–6898.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iandola et al. (2016) Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf,
    Song Han, William J. Dally, and Kurt Keutzer. 2016. SqueezeNet: AlexNet-level
    accuracy with 50x fewer parameters and <1MB model size. *CoRR* abs/1602.07360,
    None (2016), 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ibrokhimov et al. (2020) Bunyodbek Ibrokhimov, Cheonghwan Hur, and Sanggil Kang.
    2020. Effective node selection technique towards sparse learning. *APPLIED INTELLIGENCE*
    None, None (2020), 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ivanov et al. (2021) Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li,
    and Torsten Hoefler. 2021. Data Movement Is All You Need: A Case Study on Optimizing
    Transformers. In *Proceedings of Machine Learning and Systems*, Vol. 3\. None,
    None, 711–732.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2013) S. Ji, W. Xu, M. Yang, and K. Yu. 2013. 3D Convolutional Neural
    Networks for Human Action Recognition. *IEEE Transactions on Pattern Analysis
    and Machine Intelligence* 35, 1 (2013), 221–231.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2017) Yu-Gang Jiang, Zuxuan Wu, Jun Wang, Xiangyang Xue, and Shih-Fu
    Chang. 2017. Exploiting feature and class relationships in video categorization
    with regularized deep neural networks. *IEEE transactions on pattern analysis
    and machine intelligence* 40, 2 (2017), 352–364.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiao et al. (2018) Xun Jiao, Vahideh Akhlaghi, Yu Jiang, and Rajesh K Gupta.
    2018. Energy-efficient neural networks using approximate computation reuse. In
    *2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)*. IEEE,
    None, None, 1223–1228.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang (2017a) Daniel Kang. 2017a. GitHub. NoScope. [https://github.com/stanford-futuredata/noscope](https://github.com/stanford-futuredata/noscope)
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. (2017a) Daniel Kang, John Emmons, Firas Abuzaid, Peter Bailis,
    and Matei Zaharia. 2017a. NoScope: Optimizing Neural Network Queries over Video
    at Scale. *Proc. VLDB Endow.* 10, 11 (Aug. 2017), 1586–1597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang (2017b) Kai Kang. 2017b. GitHub. TPN: Tubelet Proposal Network. [https://github.com/myfavouritekk/TPN](https://github.com/myfavouritekk/TPN)
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang et al. (2017b) Kai Kang, Hongsheng Li, Tong Xiao, Wanli Ouyang, Junjie
    Yan, Xihui Liu, and Xiaogang Wang. 2017b. Object Detection in Videos with Tubelet
    Proposal Networks. *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)* None, None (Jul 2017), 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. (2018) Kai Kang, Hongsheng Li, Junjie Yan, Xingyu Zeng, Bin Yang,
    Tong Xiao, Cong Zhang, Zhe Wang, Ruohui Wang, Xiaogang Wang, and et al. 2018.
    T-CNN: Tubelets With Convolutional Neural Networks for Object Detection From Videos.
    *IEEE Transactions on Circuits and Systems for Video Technology* 28, 10 (Oct 2018),
    2896–2907.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karpathy et al. (2014) Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas
    Leung, Rahul Sukthankar, and Li Fei-Fei. 2014. Large-scale Video Classification
    with Convolutional Neural Networks. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. None, None, 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2020b) Sangyeob Kim, Juhyoung Lee, Sanghoon Kang, Jinsu Lee, and
    Hoi-Jun Yoo. 2020b. A Power-Efficient CNN Accelerator With Similar Feature Skipping
    for Face Recognition in Mobile Devices. *IEEE Transactions on Circuits and Systems
    I: Regular Papers* 67, 4 (2020), 1181–1193.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2020a) Yeachan Kim, Kang-Min Kim, and SangKeun Lee. 2020a. Adaptive
    compression of word embeddings. In *Proceedings of the 58th annual meeting of
    the association for computational linguistics*. None, None, 3950–3959.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kitaev (2020) Nikita Kitaev. 2020. Papers with Code. Reformer: The Efficient
    Transformer. [https://paperswithcode.com/paper/reformer-the-efficient-transformer-1](https://paperswithcode.com/paper/reformer-the-efficient-transformer-1)
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020.
    Reformer: The Efficient Transformer. In *Proceedings of the 8th International
    Conference on Learning Representations (ICLR)*. None, None, 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kopuklu et al. (2019) Okan Kopuklu, Neslihan Kose, Ahmet Gunduz, and Gerhard
    Rigoll. 2019. Resource efficient 3d convolutional neural networks. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops*.
    None, None, 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Korbar et al. (2019) Bruno Korbar, Du Tran, and Lorenzo Torresani. 2019. Scsampler:
    Sampling salient clips from video for efficient action recognition. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision (ICCV)*. None, None,
    6232–6242.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kwon et al. (2019) Hyoukjun Kwon, Prasanth Chatarasi, Michael Pellauer, Angshuman
    Parashar, Vivek Sarkar, and Tushar Krishna. 2019. Understanding Reuse, Performance,
    and Hardware Cost of DNN Dataflow: A Data-Centric Approach. In *Proceedings of
    the 52th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)*
    (Columbus, OH, USA) *(MICRO ’52)*. Association for Computing Machinery, New York,
    NY, USA, 754–768.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised
    Learning of Language Representations. In *Proceedings of the 8th International
    Conference on Learning Representations (ICLR)*. None, None, 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee (2020) Seulki Lee. 2020. GitHub. [RTAS 2020] SubFlow: A Dynamic Induced-Subgraph
    Strategy Toward Real-Time DNN Inference and Training. [https://github.com/learning1234embed/SubFlow](https://github.com/learning1234embed/SubFlow)
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee and Nirjon (2020) Seulki Lee and Shahriar Nirjon. 2020. SubFlow: A Dynamic
    Induced-Subgraph Strategy Toward Real-Time DNN Inference and Training. In *2020
    IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)*. IEEE,
    None, None, 15–29.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li (2019) Hongyang Li. 2019. GitHub. Zoom-out-and-in Network for region proposal
    and object detection. [https://github.com/hli2020/zoom_network](https://github.com/hli2020/zoom_network)
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019b) Hongyang Li, Yu Liu, Wanli Ouyang, and Xiaogang Wang. 2019b.
    Zoom out-and-in network with map attention decision for region proposal and object
    detection. *International Journal of Computer Vision* 127, 3 (2019), 225–238.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Hang Li, Chen Ma, Wei Xu, and Xue Liu. 2020. Feature Statistics
    Guided Efficient Filter Pruning. In *Proceedings of the Twenty-Ninth International
    Joint Conference on Artificial Intelligence, IJCAI-20*. International Joint Conferences
    on Artificial Intelligence Organization, None, 2619–2625. [https://doi.org/10.24963/ijcai.2020/363](https://doi.org/10.24963/ijcai.2020/363)
    Main track.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019a) Yuchao Li, Shaohui Lin, Baochang Zhang, Jianzhuang Liu, David
    Doermann, Yongjian Wu, Feiyue Huang, and Rongrong Ji. 2019a. Exploiting kernel
    sparsity and entropy for interpretable CNN compression. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*. None,
    None, 2800–2809.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2014) Min Lin, Qiang Chen, and Shuicheng Yan. 2014. Network In Network.
    In *Proceedings of the 2nd International Conference on Learning Representations
    (ICLR)*. None, None, 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LinkViz (2020) LinkViz. 2020. Retrieved from:. [https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats5cc01b214e59](https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats5cc01b214e59)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2008) Ce Liu, Jenny Yuen, Antonio Torralba, Josef Sivic, and William T
    Freeman. 2008. Sift flow: Dense correspondence across different scenes. In *Proceedings
    of the European Conference on Computer Vision (ECCV)*. None, None, 28–42.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu (2017) Mason Liu. 2017. Papers with Code. Mobile Video Object Detection
    with Temporally-Aware Feature Maps. [https://paperswithcode.com/paper/mobile-video-object-detection-with-temporally](https://paperswithcode.com/paper/mobile-video-object-detection-with-temporally)
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu (2018) Peter J. Liu. 2018. Papers with Code. Generating Wikipedia by Summarizing
    Long Sequences. , 0–0 pages. [https://paperswithcode.com/paper/generating-wikipedia-by-summarizing-long](https://paperswithcode.com/paper/generating-wikipedia-by-summarizing-long)
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018a) Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan
    Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018a. Generating Wikipedia by Summarizing
    Long Sequences. In *Proceedings of the 6th International Conference on Learning
    Representations (ICLR)*. None, None, 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2016) Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy,
    Scott Reed, Cheng-Yang Fu, and Alexander C Berg. 2016. Ssd: Single shot multibox
    detector. In *Proceedings of the European Conference on Computer Vision (ECCV)*.
    Springer, None, None, 21–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018b) Zhenhua Liu, Jizheng Xu, Xiulian Peng, and Ruiqin Xiong.
    2018b. Frequency-domain dynamic pruning for convolutional neural networks. In
    *Advances in Neural Information Processing Systems*. None, None, 1043–1053.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2020) Dongning Ma, Xunzhao Yin, Michael Niemier, X Sharon Hu, and
    Xun Jiao. 2020. AxR-NN: Approximate Computation Reuse for Energy-Efficient Convolutional
    Neural Networks. In *Proceedings of the 2020 on Great Lakes Symposium on VLSI*.
    None, None, 363–368.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mao et al. (2019) Huizi Mao, Taeyoung Kong, and Bill Dally. 2019. CaTDet: Cascaded
    Tracked Detector for Efficient Object Detection from Video. In *Proceedings of
    Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April
    2, 2019*. mlsys.org, None, 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Michel et al. (2019) Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen
    heads really better than one?. In *Advances in Neural Information Processing Systems*.
    None, None, 14014–14024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mocerino et al. (2019) Luca Mocerino, Valerio Tenace, and Andrea Calimera. 2019.
    Energy-efficient convolutional neural networks via recurrent data reuse. In *2019
    Design, Automation & Test in Europe Conference & Exhibition (DATE)*. IEEE, None,
    None, 848–853.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ning et al. (2019) L. Ning, H. Guan, and X. Shen. 2019. Adaptive Deep Reuse:
    Accelerating CNN Training on the Fly. In *2019 IEEE 35th International Conference
    on Data Engineering (ICDE)*. None, None, 1538–1549.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ning and Shen (2019) Lin Ning and Xipeng Shen. 2019. Deep reuse: streamline
    CNN inference on the fly via coarse-grained computation reuse. In *Proceedings
    of the ACM International Conference on Supercomputing*. None, None, 438–448.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: of Redundancy by Oxford Dictionary (2020) Definition of Redundancy by Oxford Dictionary.
    2020. Retrieved from:. [https://www.lexico.com/en/definition/redundancy](https://www.lexico.com/en/definition/redundancy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ouyang et al. (2015) Wanli Ouyang, Xiaogang Wang, Xingyu Zeng, Shi Qiu, Ping
    Luo, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Chen-Change Loy, and Xiaoou
    Tang. 2015. DeepID-Net: Deformable deep convolutional neural networks for object
    detection. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)*. None, None, 2403–2412.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park and Kim (2019) Keunyoung Park and Doo-Hyun Kim. 2019. Accelerating image
    classification using feature map similarity in convolutional neural networks.
    *Applied Sciences* 9, 1 (2019), 108.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Penny and Henson (2006) W Penny and R Henson. 2006. Analysis of variance. *Statistical
    parametric mapping: The analysis of functional brain images* None, None (2006),
    166–177.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pérez-Cruz (2009) Fernando Pérez-Cruz. 2009. Estimation of information theoretic
    measures for continuous random variables. In *Advances in Neural Information Processing
    Systems*. None, None, 1257–1264.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pirsiavash and Ramanan (2012) Hamed Pirsiavash and Deva Ramanan. 2012. Detecting
    activities of daily living in first-person camera views. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*. IEEE,
    None, None, 2847–2854.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Piyasena et al. (2019) Duvindu Piyasena, Rukshan Wickramasinghe, Debdeep Paul,
    Siew-Kei Lam, and Meiqing Wu. 2019. Reducing dynamic power in streaming cnn hardware
    accelerators by exploiting computational redundancies. In *2019 29th International
    Conference on Field Programmable Logic and Applications (FPL)*. IEEE, None, None,
    354–359.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2020) Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan
    Song, and Nicu Sebe. 2020. Binary neural networks: A survey. *Pattern Recognition*
    None, None (2020), 107281.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Razlighi et al. (2017) Mohammad Samragh Razlighi, Mohsen Imani, Farinaz Koushanfar,
    and Tajana Rosing. 2017. Looknn: Neural network with no multiplication. In *Design,
    Automation & Test in Europe Conference & Exhibition (DATE), 2017*. IEEE, None,
    None, 1775–1780.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015.
    Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.
    In *Advances in Neural Information Processing Systems*, Vol. 28. Curran Associates,
    Inc., None, 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sainath and Parada (2015) Tara N Sainath and Carolina Parada. 2015. Convolutional
    neural networks for small-footprint keyword spotting. In *Sixteenth Annual Conference
    of the International Speech Communication Association*. None, None, 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salamat et al. (2018) Sahand Salamat, Mohsen Imani, Sarangh Gupta, and Tajana
    Rosing. 2018. Rnsnet: In-memory neural network acceleration using residue number
    system. In *2018 IEEE International Conference on Rebooting Computing (ICRC)*.
    IEEE, None, None, 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Samal et al. (2020) Kruttidipta Samal, Marilyn Wolf, and Saibal Mukhopadhyay.
    2020. Attention-based Activation Pruning to Reduce Data Movement in Real-time
    AI: A Case-study on Local Motion Planning in Autonomous Vehicles. *IEEE Journal
    on Emerging and Selected Topics in Circuits and Systems* None, None (2020), 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper
    and lighter. In *5th Workshop on Energy Efficient Machine Learning and Cognitive
    Computing NeurIPS 2019*. None, None, 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.
    Neural Machine Translation of Rare Words with Subword Units. In *Proceedings of
    the 54th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*. Association for Computational Linguistics, Berlin, Germany,
    1715–1725.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2018) Junzhong Shen, You Huang, Zelong Wang, Yuran Qiao, Mei Wen,
    and Chunyuan Zhang. 2018. Towards a uniform template-based architecture for accelerating
    2D and 3D CNNs on FPGA. In *Proceedings of the 2018 ACM/SIGDA International Symposium
    on Field-Programmable Gate Arrays*. None, None, 97–106.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2016) Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does string-based
    neural MT learn source syntax?. In *Proceedings of the 2016 Conference on Empirical
    Methods in Natural Language Processing*. None, None, 1526–1534.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shomron (2019) Gil Shomron. 2019. GitHub. Thanks for Nothing: Predicting Zero-Valued
    Activations with Lightweight Convolutional Neural Networks. [https://github.com/gilshm/zap](https://github.com/gilshm/zap)
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shomron et al. (2020) Gil Shomron, Ron Banner, Moran Shkolnik, and Uri Weiser.
    2020. Thanks for nothing: Predicting zero-valued activations with lightweight
    convolutional neural networks. In *Proceedings of the European Conference on Computer
    Vision (ECCV)*. Springer, None, None, 234–250.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shorten and Khoshgoftaar (2019) Connor Shorten and Taghi M Khoshgoftaar. 2019.
    A survey on image data augmentation for deep learning. *Journal of Big Data* 6,
    1 (2019), 1–48.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simons and Lee (2019) Taylor Simons and Dah-Jye Lee. 2019. A review of binarized
    neural networks. *Electronics* 8, 6 (2019), 661.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman (2014) Karen Simonyan and Andrew Zisserman. 2014. Two-Stream
    Convolutional Networks for Action Recognition in Videos. In *Advances in Neural
    Information Processing Systems*. Curran Associates, Inc., None, 568–576.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman (2015) Karen Simonyan and Andrew Zisserman. 2015. Very
    Deep Convolutional Networks for Large-Scale Image Recognition. In *Proceedings
    of the 3rd International Conference on Learning Representations (ICLR)*. None,
    None, 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh et al. (2019) Arshdeep Singh, Padmanabhan Rajan, and Arnav Bhavsar. 2019.
    Deep hidden analysis: A statistical framework to prune feature maps. In *ICASSP
    2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*. IEEE, None, None, 820–824.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Soomro et al. (2012) Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. 2012.
    UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild. *CoRR*
    abs/1212.0402, None (2012), 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su and Grauman (2016) Yu-Chuan Su and Kristen Grauman. 2016. Leaving some stones
    unturned: dynamic feature prioritization for activity detection in streaming video.
    In *European Conference on Computer Vision*. Springer, None, None, 783–800.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suzuki et al. (2020) Taiji Suzuki, Hiroshi Abe, Tomoya Murata, Shingo Horiuchi,
    Kotaro Ito, Tokuma Wachi, So Hirai, Masatoshi Yukishima, and Tomoaki Nishimura.
    2020. Spectral Pruning: Compressing Deep Neural Networks via Spectral Analysis
    and its Generalization Error. In *IJCAI*. None, None, 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torralba and Efros (2011) Antonio Torralba and Alexei A Efros. 2011. Unbiased
    look at dataset bias. In *CVPR 2011*. IEEE, None, None, 1521–1528.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tran et al. (2015) Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
    and Manohar Paluri. 2015. Learning Spatiotemporal Features with 3D Convolutional
    Networks. In *Proceedings of the IEEE/CVF International Conference on Computer
    Vision (ICCV)*. None, None, 4489–4497.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Urban et al. (2017) Gregor Urban, Krzysztof J Geras, Samira Ebrahimi Kahou,
    Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose,
    and Matt Richardson. 2017. Do deep convolutional nets really need to be deep and
    convolutional?. In *Proceedings of the 5th International Conference on Learning
    Representations (ICLR)*. None, None, 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. 2019. Analyzing Multi-Head Self-Attention: Specialized Heads Do
    the Heavy Lifting, the Rest Can Be Pruned. In *Proceedings of the 57th Annual
    Meeting of the Association for Computational Linguistics*. Association for Computational
    Linguistics, Florence, Italy, 5797–5808.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) Dong Wang, Lei Zhou, Xueni Zhang, Xiao Bai, and Jun Zhou.
    2018. Exploring Linear Relationship in Feature Map Subspace for ConvNets Compression.
    *CoRR* abs/1803.05729, None (2018), 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Hai Wang, Mengjun Shao, Yan Liu, and Wei Zhao. 2017. Enhanced
    efficiency 3D convolution based on optimal FPGA accelerator. *IEEE Access* 5,
    None (2017), 6909–6916.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019a) Ying Wang, Shengwen Liang, Huawei Li, and Xiaowei Li. 2019a.
    A None-Sparse Inference Accelerator that Distills and Reuses the Computation Redundancy
    in CNNs. In *Proceedings of the 56th Annual Design Automation Conference 2019*.
    None, None, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019b) Yongchen Wang, Ying Wang, Huawei Li, Cong Shi, and Xiaowei
    Li. 2019b. Systolic Cube: A Spatial 3D CNN Accelerator Architecture for Low Power
    Video Analysis. In *Proceedings of the 56th Annual Design Automation Conference
    2019*. None, None, 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Ying Wang, Yongchen Wang, Cong Shi, Long Cheng, Huawei Li,
    and Xiaowei Li. 2020. An Edge 3D CNN Accelerator for Low Power Activity Recognition.
    *IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems*
    None, None (2020), 0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2016) Yunhe Wang, Chang Xu, Shan You, Dacheng Tao, and Chao Xu.
    2016. Cnnpack: Packing convolutional neural networks in the frequency domain.
    In *Advances in Neural Information Processing Systems*. None, None, 253–261.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2019a) Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, and Shilei
    Wen. 2019a. Multi-agent reinforcement learning based frame sampling for effective
    untrimmed video recognition. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision (ICCV)*. None, None, 6222–6231.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2020) Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi
    Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks.
    *IEEE Transactions on Neural Networks and Learning Systems* None, None (2020),
    0–0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019b) Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, and
    Larry S Davis. 2019b. Adaframe: Adaptive frame selection for fast video recognition.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. None, None, 1278–1287.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2016) Bin Yang, Junjie Yan, Zhen Lei, and Stan Z Li. 2016. Craft
    objects from images. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*. None, None, 6043–6051.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R
    Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining
    for language understanding. In *Advances in Neural Information Processing Systems*.
    None, None, 5753–5763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yeung (2015) Serena Yeung. 2015. GitHub. End-to-end Learning of Action Detection
    from Frame Glimpses in Videos. [https://github.com/syyeung/frameglimpses](https://github.com/syyeung/frameglimpses)
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yeung et al. (2016) Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-Fei.
    2016. End-to-end learning of action detection from frame glimpses in videos. In
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. None, None, 2678–2687.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2020) Fuxun Yu, Chenchen Liu, Di Wang, Yanzhi Wang, and Xiang Chen.
    2020. AntiDote: attention-based dynamic optimization for neural network runtime
    efficiency. In *2020 Design, Automation & Test in Europe Conference & Exhibition
    (DATE)*. IEEE, None, None, 951–956.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017) Shihao Zhang, Weiyao Lin, Ping Lu, Weihua Li, and Shuo
    Deng. 2017. Kill two birds with one stone: Boosting both object detection accuracy
    and speed with adaptive patch-of-interest composition. In *2017 IEEE International
    Conference on Multimedia & Expo Workshops (ICMEW)*. IEEE, None, None, 447–452.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2017) Jian Zheng, Wei Yang, and Xiaohua Li. 2017. Training data
    reduction in deep neural networks with partial mutual information based feature
    selection and correlation matching based active learning. In *2017 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, None, None,
    2362–2366.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2019) Yuefu Zhou, Ya Zhang, Yanfeng Wang, and Qi Tian. 2019. Accelerate
    cnn via recursive bayesian pruning. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*. None, None, 3306–3315.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu and Liu (2018) Menglong Zhu and Mason Liu. 2018. Mobile Video Object Detection
    with Temporally-Aware Feature Maps. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. None, None, 5686–5695.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu (2017) Xizhou Zhu. 2017. Papers with Code. Towards High Performance Video
    Object Detection for Mobiles. [https://paperswithcode.com/paper/towards-high-performance-video-object](https://paperswithcode.com/paper/towards-high-performance-video-object)
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2018) Xizhou Zhu, Jifeng Dai, Lu Yuan, and Yichen Wei. 2018. Towards
    high performance video object detection. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. None, None, 7210–7218.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017a) Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, and Yichen Wei.
    2017a. Flow-guided feature aggregation for video object detection. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision (ICCV)*. None, None,
    408–417.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017b) Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen
    Wei. 2017b. Deep feature flow for video recognition. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*. None, None, 2349–2358.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuang et al. (2018) Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu,
    Yong Guo, Qingyao Wu, Junzhou Huang, and Jinhui Zhu. 2018. Discrimination-aware
    channel pruning for deep neural networks. In *Advances in Neural Information Processing
    Systems*. None, None, 875–886.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
