- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:32:49'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:32:49'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2405.07801] Deep Learning-Based Object Pose Estimation: A Comprehensive Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2405.07801] 基于深度学习的目标姿态估计：全面调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.07801](https://ar5iv.labs.arxiv.org/html/2405.07801)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.07801](https://ar5iv.labs.arxiv.org/html/2405.07801)
- en: 'Deep Learning-Based Object Pose Estimation:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的目标姿态估计：
- en: A Comprehensive Survey
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 全面调查
- en: Jian Liu, Wei Sun, Hui Yang, Zhiwen Zeng, Chongpei Liu, Jin Zheng, Xingyu Liu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 刘剑，孙伟，杨辉，曾志文，刘冲佩，郑进，刘星宇
- en: 'Hossein Rahmani, Nicu Sebe, , and Ajmal Mian Jian Liu, Wei Sun, Hui Yang, Zhiwen
    Zeng, and Chongpei Liu are with the National Engineering Research Center for Robot
    Visual Perception and Control Technology, College of Electrical and Information
    Engineering, and the State Key Laboratory of Advanced Design and Manufacturing
    for Vehicle Body, Hunan University, Changsha 410082, China. E-mail: (jianliu,
    wei_sun, huiyang, zingaltern, chongpei56)@hnu.edu.cn Jin Zheng is with the School
    of Architecture and Art, Central South University, Changsha, 410082, China. E-mail:
    zheng.jin@csu.edu.cn Xingyu Liu is with the Department of Automation, Tsinghua
    University, Beijing 100084, China. E-mail: liuxy21@mails.tsinghua.edu.cn Hossein
    Rahmani is with the School of Computing and Communications, Lancaster University,
    LA1 4YW, United Kingdom. E-mail: h.rahmani@lancaster.ac.uk Nicu Sebe is with the
    Department of Information Engineering and Computer Science, University of Trento,
    Trento 38123, Italy. E-mail: sebe@disi.unitn.it Ajmal Mian is with the Department
    of Computer Science, The University of Western Australia, WA 6009, Australia.
    E-mail: ajmal.mian@uwa.edu.au This work was done while Jian Liu was a visiting
    Ph.D. student with The University of Western Australia, supervised by Prof. Ajmal
    Mian, and Chongpei Liu was a visiting Ph.D. student with the University of Trento,
    supervised by Prof. Nicu Sebe.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hossein Rahmani、Nicu Sebe 和 Ajmal Mian，刘剑、孙伟、杨辉、曾志文和刘冲佩在湖南大学电气与信息工程学院、国家机器人视觉感知与控制技术工程研究中心以及车辆车身先进设计与制造国家重点实验室工作。电子邮件:
    (jianliu, wei_sun, huiyang, zingaltern, chongpei56)@hnu.edu.cn。郑进在中南大学建筑与艺术学院工作。电子邮件:
    zheng.jin@csu.edu.cn。刘星宇在清华大学自动化系工作。电子邮件: liuxy21@mails.tsinghua.edu.cn。Hossein
    Rahmani 在兰卡斯特大学计算与通信学院工作。电子邮件: h.rahmani@lancaster.ac.uk。Nicu Sebe 在特伦托大学信息工程与计算机科学系工作。电子邮件:
    sebe@disi.unitn.it。Ajmal Mian 在西澳大学计算机科学系工作。电子邮件: ajmal.mian@uwa.edu.au。本工作在刘剑作为西澳大学访问博士生期间完成，由
    Ajmal Mian 教授指导，而刘冲佩在特伦托大学作为访问博士生期间完成，由 Nicu Sebe 教授指导。'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Object pose estimation is a fundamental computer vision problem with broad applications
    in augmented reality and robotics. Over the past decade, deep learning models,
    due to their superior accuracy and robustness, have increasingly supplanted conventional
    algorithms reliant on engineered point pair features. Nevertheless, several challenges
    persist in contemporary methods, including their dependency on labeled training
    data, model compactness, robustness under challenging conditions, and their ability
    to generalize to novel unseen objects. A recent survey discussing the progress
    made on different aspects of this area, outstanding challenges, and promising
    future directions, is missing. To fill this gap, we discuss the recent advances
    in deep learning-based object pose estimation, covering all three formulations
    of the problem, *i.e.*, instance-level, category-level, and unseen object pose
    estimation. Our survey also covers multiple input data modalities, degrees-of-freedom
    of output poses, object properties, and downstream tasks, providing the readers
    with a holistic understanding of this field. Additionally, it discusses training
    paradigms of different domains, inference modes, application areas, evaluation
    metrics, and benchmark datasets, as well as reports the performance of current
    state-of-the-art methods on these benchmarks, thereby facilitating the readers
    in selecting the most suitable method for their application. Finally, the survey
    identifies key challenges, reviews the prevailing trends along with their pros
    and cons, and identifies promising directions for future research. We also keep
    tracing the latest works at [Awesome-Object-Pose-Estimation](https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 物体姿态估计是一个基础的计算机视觉问题，在增强现实和机器人领域有广泛应用。在过去十年中，由于其卓越的准确性和鲁棒性，深度学习模型逐渐取代了依赖于工程化点对特征的传统算法。然而，当前方法仍面临一些挑战，包括对标记训练数据的依赖、模型紧凑性、在困难条件下的鲁棒性，以及对新出现物体的泛化能力。最近的调查讨论了在该领域不同方面的进展、突出挑战和有前景的未来方向。为了填补这一空白，我们讨论了基于深度学习的物体姿态估计的最新进展，涵盖了该问题的所有三种形式，*即*实例级、类别级和未见物体姿态估计。我们的调查还涵盖了多种输入数据模态、输出姿态的自由度、物体属性和下游任务，为读者提供对该领域的全面理解。此外，它讨论了不同领域的训练范式、推理模式、应用领域、评估指标和基准数据集，并报告了当前最先进方法在这些基准上的表现，从而帮助读者选择最适合其应用的方法。最后，调查确定了关键挑战，回顾了流行趋势及其优缺点，并确定了未来研究的有前景方向。我们还持续跟踪最新的工作，见[Awesome-Object-Pose-Estimation](https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation)。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Object pose estimation, deep learning, comprehensive survey, 3D computer vision.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 物体姿态估计、深度学习、综合调查、3D计算机视觉。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Object pose estimation is a fundamental computer vision problem that aims to
    estimate the pose of an object in a given image relative to the camera that captured
    the image. Object pose estimation is a crucial technology for augmented reality[[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)], robotic manipulation[[4](#bib.bib4), [5](#bib.bib5)],
    hand-object interaction[[6](#bib.bib6), [7](#bib.bib7)], etc. Depending on the
    application needs, the object pose is estimated up to varying degrees of freedom
    (DoF) such as 3DoF that only includes 3D rotation, 6DoF that additionally includes
    3D translation, or 9DoF which includes estimating the 3D size of the object besides
    the 3D rotation and 3D translation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 物体姿态估计是一个基本的计算机视觉问题，旨在估计图像中物体相对于捕捉该图像的相机的姿态。物体姿态估计是增强现实[[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3)]、机器人操作[[4](#bib.bib4), [5](#bib.bib5)]、手物体交互[[6](#bib.bib6), [7](#bib.bib7)]等领域的重要技术。根据应用需求，物体姿态可以估计到不同的自由度（DoF），例如仅包括3D旋转的3DoF、额外包括3D平移的6DoF，或除了3D旋转和3D平移外，还包括估计物体3D尺寸的9DoF。
- en: In the pre-deep learning era, many hand-crafted feature-based approaches such
    as SIFT[[8](#bib.bib8)], FPFH[[9](#bib.bib9)], VFH[[10](#bib.bib10)], and Point
    Pair Features (PPF)[[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)]
    were designed for object pose estimation. However, these methods exhibit deficiencies
    in accuracy and robustness when confronted with complex scenes[[15](#bib.bib15),
    [16](#bib.bib16)]. These traditional methods have now been supplanted by data
    driven deep learning-based approaches that harness the power of deep neural networks
    to learn high-dimensional feature representations from data, leading to improved
    accuracy and robustness to handle complex environments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习前时代，许多手工制作的基于特征的方法，如SIFT[[8](#bib.bib8)]、FPFH[[9](#bib.bib9)]、VFH[[10](#bib.bib10)]和点对特征（PPF）[[11](#bib.bib11)、[12](#bib.bib12)、[13](#bib.bib13)、[14](#bib.bib14)]，被设计用于对象姿态估计。然而，当面对复杂场景时，这些方法在准确性和鲁棒性上存在缺陷[[15](#bib.bib15)、[16](#bib.bib16)]。这些传统方法现在已经被数据驱动的基于深度学习的方法所取代，这些方法利用深度神经网络的力量从数据中学习高维特征表示，从而提高了准确性和对复杂环境的鲁棒性。
- en: '![Refer to caption](img/b04f726d0d23afc45a36714d9f34d626.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b04f726d0d23afc45a36714d9f34d626.png)'
- en: 'Figure 1: Comparison of instance-level, category-level, and unseen object methods.
    Instance-level methods can only estimate the pose of specific object instances
    on which they are trained. Category-level methods can infer intra-class unseen
    instances rather than being limited to specific instances in the training data.
    In contrast, unseen object pose estimation methods have stronger generalization
    ability and can handle object categories not encountered during training.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：实例级、类别级和未见对象方法的比较。实例级方法只能估计特定对象实例的姿态，这些实例是在训练中使用的。类别级方法可以推断出类内未见实例，而不仅仅限于训练数据中的特定实例。相比之下，未见对象姿态估计方法具有更强的泛化能力，可以处理训练过程中未遇到的对象类别。
- en: '![Refer to caption](img/78b5cf433b76a024efaac33c3a18c2aa.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/78b5cf433b76a024efaac33c3a18c2aa.png)'
- en: 'Figure 2: A taxonomy of this survey. Firstly, we review the datasets and evaluation
    metrics used to evaluate object pose estimation. Next, we review the deep learning-based
    methods by dividing them into three categories: instance-level, category-level,
    and unseen methods. Instance-level methods can be further classified into correspondence-based,
    template-based, voting-based, and regression-based methods. Category-level methods
    can be further divided into shape prior-based and shape prior-free methods. Unseen
    methods can be further classified into CAD model-based and manual reference view-based
    methods.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：本调查的分类。首先，我们回顾用于评估对象姿态估计的数据集和评估指标。接下来，我们将深度学习方法分为三类进行回顾：实例级、类别级和未见方法。实例级方法可以进一步分类为基于对应关系的方法、基于模板的方法、基于投票的方法和基于回归的方法。类别级方法可以进一步分为基于形状先验的方法和无形状先验的方法。未见方法可以进一步分类为基于CAD模型的方法和基于手动参考视图的方法。
- en: 'Deep learning-based object pose estimation methods can be divided into instance-level,
    category-level, and unseen object methods according to the problem formulation.
    Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey") shows a comparison of the three methods. Early methods
    were mainly instance-level [[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20)], trained to estimate the pose of specific
    object instances. Instance-level methods can be further divided into correspondence-based,
    template-based, voting-based, and regression-based methods. Since instance-level
    methods are trained on instance-specific data, they can estimate pose with high
    precision for the given object instances. However, their generalization performance
    is poor because they are meant to be applied only to the instances on which they
    are trained. Moreover, many instance-level methods[[18](#bib.bib18), [21](#bib.bib21)]
    require CAD models of the objects. Recognizing these limitations, Wang *et al.*
    [[22](#bib.bib22)] proposed the first category-level object pose and size estimation
    method. They generalize to intra-class unseen objects without necessitating retraining
    and employing CAD models during inference. Subsequent category-level methods [[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)] can be
    divided into shape prior-based and shape prior-free methods. While improving the
    generalization ability within a category, these category-level methods still need
    to collect and label extensive training data for each object category. Moreover,
    these methods cannot generalize to unseen object categories. To this end, some
    unseen object pose estimation methods have been recently proposed [[1](#bib.bib1),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [3](#bib.bib3)], which can
    be further classified into CAD model-based and manual reference view-based methods.
    These methods further enhance the generalization of object pose estimation, *i.e.*,
    they can be generalized to unseen objects without retraining. Nevertheless, they
    still need to obtain the object CAD model or annotate a few reference images of
    the object.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '基于深度学习的目标姿态估计方法可以根据问题的定义分为实例级、类别级和未见目标方法。图[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")展示了这三种方法的比较。早期的方法主要是实例级[[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)]，用于估计特定目标实例的姿态。实例级方法可以进一步分为基于对应关系、基于模板、基于投票和基于回归的方法。由于实例级方法是在实例特定的数据上训练的，因此可以对给定的目标实例进行高精度的姿态估计。然而，它们的泛化性能较差，因为它们仅适用于训练时的实例。此外，许多实例级方法[[18](#bib.bib18),
    [21](#bib.bib21)]需要目标的CAD模型。认识到这些局限性，Wang *et al.* [[22](#bib.bib22)]提出了首个类别级目标姿态和尺寸估计方法。他们将方法推广到类别内的未见目标，无需在推理过程中重新训练和使用CAD模型。随后的类别级方法[[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)]可以分为基于形状先验和无形状先验的方法。虽然这些类别级方法在类别内提高了泛化能力，但仍需收集和标记每个目标类别的大量训练数据。此外，这些方法无法推广到未见的目标类别。为此，一些未见目标姿态估计方法最近被提出[[1](#bib.bib1),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [3](#bib.bib3)]，这些方法进一步分为基于CAD模型和基于手动参考视图的方法。这些方法进一步增强了目标姿态估计的泛化能力，*即*，它们可以推广到未见的目标而无需重新训练。然而，它们仍需获得目标的CAD模型或标注一些目标的参考图像。'
- en: Although significant progress has been made in the area of object pose estimation,
    several challenges persist in current methods, such as the reliance on labeled
    training data, difficulty in generalizing to novel unseen objects, model compactness,
    and robustness in challenging scenarios. To enable readers to swiftly grasp the
    current state-of-the-art (SOTA) in object pose estimation and facilitate further
    research in this direction, it is crucial to provide a thorough review of all
    the relevant problem formulations. A close examination of the existing academic
    literature reveals a significant gap when reviewing the various problem formulations
    in object pose estimation. Current prevailing reviews [[31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)] tend to exhibit a narrow
    focus, either confined to particular input modalities[[32](#bib.bib32), [33](#bib.bib33)]
    or tethered to specific application domains[[34](#bib.bib34), [35](#bib.bib35)].
    Furthermore, these reviews predominantly scrutinize instance-level and category-level
    methods, thus neglecting the exploration of the most practical problem formulation
    in the domain which is unseen object pose estimation. This hinders readers from
    gaining a comprehensive understanding of the area. For instance, Fan *et al.*
    [[33](#bib.bib33)] provided valuable insights into RGB image-based object pose
    estimation. However, their focus is limited to a singular modality, hindering
    readers from comprehensively understanding methods across various input modalities.
    Conversely, Du *et al.* [[34](#bib.bib34)] exclusively examined object pose estimation
    within the context of the robotic grasping task, which limits the readers to understand
    object pose estimation only from the perspective of a single specific application.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管物体姿态估计领域取得了显著进展，但当前方法仍面临多个挑战，如依赖标注训练数据、难以推广到新见物体、模型紧凑性以及在具有挑战性的场景中的鲁棒性。为了使读者能够迅速掌握物体姿态估计领域的最新技术（SOTA）并促进该方向的进一步研究，提供所有相关问题表述的详尽回顾至关重要。对现有学术文献的仔细审查揭示了在回顾物体姿态估计的各种问题表述时存在的重大差距。目前的主流综述[[31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)]往往关注狭窄，要么局限于特定的输入模态[[32](#bib.bib32),
    [33](#bib.bib33)]，要么束缚于特定应用领域[[34](#bib.bib34), [35](#bib.bib35)]。此外，这些综述主要审视实例级和类别级方法，从而忽视了在该领域中最实际的问题表述——未见物体姿态估计。这阻碍了读者对该领域的全面理解。例如，Fan
    *et al.* [[33](#bib.bib33)] 提供了关于基于RGB图像的物体姿态估计的宝贵见解。然而，他们的关注点限于单一模态，限制了读者对各种输入模态方法的全面理解。相反，Du
    *et al.* [[34](#bib.bib34)] 专门研究了在机器人抓取任务中的物体姿态估计，这使得读者只能从单一特定应用的角度理解物体姿态估计。
- en: 'To address the above problems, we present here a comprehensive survey of recent
    advancements in deep learning-based methods for object pose estimation. Our survey
    encompasses all problem formulations, including instance-level, category-level,
    and unseen object pose estimation, aiming to provide readers with a holistic understanding
    of this field. Additionally, we discuss different domain training paradigms, application
    areas, evaluation metrics, and benchmark datasets, as well as report the performance
    of state-of-the-art methods on these benchmarks, aiding readers in selecting suitable
    methods for their applications. Furthermore, we also highlight prevailing trends
    and discuss their strengths and weaknesses, as well as identify key challenges
    and promising avenues for future research. The taxonomy of this survey is shown
    in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning-Based Object Pose
    Estimation: A Comprehensive Survey").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述问题，我们在此提供了一个全面的调查，涵盖了基于深度学习的物体姿态估计方法的最新进展。我们的调查涵盖了所有问题的表述，包括实例级、类别级和未见物体姿态估计，旨在为读者提供对该领域的整体理解。此外，我们讨论了不同的领域训练范式、应用领域、评估指标和基准数据集，并报告了最先进的方法在这些基准上的性能，帮助读者选择适合其应用的方法。此外，我们还突出了当前的趋势，讨论了它们的优缺点，并确定了关键挑战和未来研究的有希望的方向。该调查的分类结构如图[2](#S1.F2
    "图 2 ‣ 1 介绍 ‣ 基于深度学习的物体姿态估计：综合调查")所示。
- en: 'Our main contributions and highlights are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献和亮点如下：
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present a *comprehensive survey* of deep learning-based object pose estimation
    methods. This is the *first* survey that covers all three problem formulations
    in the domain, including instance-level, category-level, and unseen object pose
    estimation.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了一个*全面的调研*，涵盖了基于深度学习的物体姿态估计方法。这是*第一个*涵盖该领域所有三种问题表述的调研，包括实例级、类别级和未见物体姿态估计。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our survey covers popular input data modalities (RGB images, depth images, RGBD
    images), the different degrees of freedom (3DoF, 6DoF, 9DoF) in output poses,
    object properties (rigid, articulated) for the task of pose estimation as well
    as tracking. It is crucial to cover all these aspects in a single survey to give
    a complete picture to readers, an aspect overlooked by existing surveys which
    only cover a few of these aspects.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的调研涵盖了流行的输入数据模态（RGB图像、深度图像、RGBD图像）、输出姿态中的不同自由度（3DoF、6DoF、9DoF）、物体属性（刚性、关节）用于姿态估计和跟踪任务。覆盖这些方面对于给读者提供完整的视角至关重要，这是现有调研所忽略的，它们只涵盖了这些方面中的一部分。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discuss different domain training paradigms, inference modes, application
    areas, evaluation metrics, and benchmark datasets as well as report the performance
    of existing SOTA methods on these benchmarks to help readers choose the most appropriate
    ones for deployment in their application.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了不同领域的训练范式、推理模式、应用领域、评估指标和基准数据集，并报告了现有最先进方法在这些基准上的性能，以帮助读者选择最适合其应用部署的方法。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We highlight popular trends in the evolution of object pose estimation techniques
    over the past decade and discuss their strengths and weaknesses. We also identify
    key challenges that are still outstanding in object pose estimation along with
    promising research directions to guide future efforts.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们突出展示了过去十年中物体姿态估计技术的发展趋势，并讨论了它们的优缺点。我们还识别了物体姿态估计中仍存在的关键挑战，并提出了有前景的研究方向以指导未来的努力。
- en: 'The rest of this article is organized as follows. Sec. [2](#S2 "2 Datasets
    and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")
    reviews the datasets and metrics used to evaluate the three categories of object
    pose estimation methods. We then review instance-level methods in Sec. [3](#S3
    "3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey"), category-level methods in Sec. [4](#S4 "4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey"), and unseen object pose estimation methods in Sec. [5](#S5 "5 Unseen
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey"). In the aforementioned three sections, we also discuss the training paradigms,
    inference modes, challenges, and popular trends associated with representative
    methods in the particular category. Next, Sec. [6](#S6 "6 Applications ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey") reviews the common
    applications of object pose estimation. Finally, Sec. [7](#S7 "7 Conclusion and
    Future Direction ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey") summarizes this article and provides an outlook on future research directions
    based on the challenges in the field.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的其余部分组织如下。第[2](#S2 "2 Datasets and Metrics ‣ Deep Learning-Based Object Pose
    Estimation: A Comprehensive Survey")节回顾了用于评估三类物体姿态估计方法的数据集和指标。接着，在第[3](#S3 "3
    Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")节中回顾实例级方法，第[4](#S4 "4 Category-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")节中回顾类别级方法，第[5](#S5
    "5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")节中回顾未见物体姿态估计方法。在上述三个部分中，我们还讨论了相关类别中代表性方法的训练范式、推理模式、挑战和流行趋势。接下来，第[6](#S6
    "6 Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")节回顾了物体姿态估计的常见应用。最后，第[7](#S7 "7 Conclusion and Future Direction ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey")节总结了本文内容，并基于该领域的挑战提供了未来研究方向的展望。'
- en: 2 Datasets and Metrics
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 数据集和指标
- en: 'The advancement of deep learning-based object pose estimation is closely linked
    to the creation and utilization of challenging and trustworthy large-scale datasets.
    This section introduces commonly used mainstream object pose estimation datasets,
    categorized into instance-level, category-level, and unseen object pose estimation
    methods based on problem formulation. The chronological overview is shown in Fig.
    [3](#S2.F3 "Figure 3 ‣ 2.1.2 Other Datasets ‣ 2.1 Datasets for Instance-Level
    Methods ‣ 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey"). In addition, we also conduct an overview of the related
    evaluation metrics.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '基于深度学习的物体姿态估计的进展与挑战性和可靠的大规模数据集的创建和利用密切相关。本节介绍了常用的主流物体姿态估计数据集，按照问题形式分类为实例级、类别级和未见物体姿态估计方法。时间顺序概述见图
    [3](#S2.F3 "Figure 3 ‣ 2.1.2 Other Datasets ‣ 2.1 Datasets for Instance-Level
    Methods ‣ 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")。此外，我们还对相关的评估指标进行了概述。'
- en: 2.1 Datasets for Instance-Level Methods
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 实例级方法的数据集
- en: Since the BOP Challenge datasets[[36](#bib.bib36)] are currently the most popular
    datasets for the evaluation of instance-level methods, we divide the instance-level
    datasets into BOP Challenge and other datasets for overview.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 BOP 挑战数据集[[36](#bib.bib36)] 目前是评估实例级方法的最受欢迎的数据集，我们将实例级数据集分为 BOP 挑战和其他数据集进行概述。
- en: 2.1.1 BOP Challenge Datasets
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 BOP 挑战数据集
- en: Linemod Dataset (LM)[[37](#bib.bib37)] comprises 15 RGBD sequences containing
    annotated RGBD images with ground-truth 6DoF object poses, object CAD models,
    2D bounding boxes, and binary masks. Typically following Brachmann *et al.*[[38](#bib.bib38)],
    approximately 15$\%$ of images from each sequence are allocated for training,
    with the remaining 85$\%$ reserved for testing. These sequences present challenging
    scenarios with cluttered scenes, texture-less objects, and varying lighting conditions,
    making accurate object pose estimation difficult.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Linemod 数据集 (LM)[[37](#bib.bib37)] 包含 15 个 RGBD 序列，其中包含带有真实 6DoF 物体姿态的标注 RGBD
    图像、物体 CAD 模型、2D 边界框和二值掩码。通常参考 Brachmann *et al.*[[38](#bib.bib38)] 的方法，每个序列大约有
    15$\%$ 的图像用于训练，其余的 85$\%$ 用于测试。这些序列展示了具有杂乱场景、无纹理物体和变化的光照条件的挑战性场景，使得准确的物体姿态估计变得困难。
- en: Linemod Occlusion Dataset (LM-O)[[39](#bib.bib39)] is an extension of the LM
    dataset[[37](#bib.bib37)] specifically designed to evaluate the performance in
    occlusion scenarios. This dataset consists of 1214 RGBD images from the basic
    sequence in the LM dataset for 8 heavily occluded objects. It is critical for
    evaluating and improving pose estimation algorithms in complex environments characterized
    by occlusion.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Linemod 遮挡数据集 (LM-O)[[39](#bib.bib39)] 是 LM 数据集[[37](#bib.bib37)] 的扩展，专门用于评估在遮挡场景中的性能。该数据集包含来自
    LM 数据集基础序列的 1214 张 RGBD 图像，涵盖 8 个严重遮挡的物体。对于在复杂环境中评估和改进姿态估计算法，这一点至关重要。
- en: 'IC-MI [[40](#bib.bib40)] / IC-BIN Dataset [[41](#bib.bib41)] contribute to
    texture-less object pose estimation. IC-MI comprises six objects: 2 texture-less
    and 4 textured household item models. IC-BIN dataset is specifically designed
    to address challenges posed by clutter and occlusion in robot garbage bin picking
    scenarios. IC-BIN includes 2 objects from the IC-MI.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: IC-MI [[40](#bib.bib40)] / IC-BIN 数据集 [[41](#bib.bib41)] 有助于无纹理物体姿态估计。IC-MI
    包含六个物体：2 个无纹理和 4 个有纹理的家用物品模型。IC-BIN 数据集专门设计用于解决机器人垃圾桶拣货场景中的杂乱和遮挡带来的挑战。IC-BIN 包含
    2 个来自 IC-MI 的物体。
- en: RU-APC Dataset [[42](#bib.bib42)] aims to tackle challenges in warehouse picking
    tasks and provides rich data for evaluating and improving the perception capabilities
    of robots in a warehouse automation context. The dataset comprises 10,368 registered
    depth and RGB images, covering 24 types of objects, which are placed in various
    poses within different boxes on warehouse shelves to simulate diverse experimental
    conditions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: RU-APC 数据集 [[42](#bib.bib42)] 旨在解决仓库拣货任务中的挑战，并提供丰富的数据用于评估和改进机器人在仓库自动化背景下的感知能力。该数据集包含
    10,368 张注册的深度图像和 RGB 图像，涵盖 24 种物体，这些物体以不同姿态放置在仓库货架上的各种箱子中，以模拟多样的实验条件。
- en: YCB-Video Dataset (YCB-V)[[15](#bib.bib15)] comprises 21 objects distributed
    across 92 RGBD videos, each video containing 3 to 9 objects from the YCB object
    dataset[[43](#bib.bib43)] (totaling 50 objects). It includes 133,827 frames with
    a resolution of 640$\times$480, making it well-suited for both object pose estimation
    and tracking tasks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: YCB-Video 数据集 (YCB-V) [[15](#bib.bib15)] 包含21个物体，分布在92个 RGBD 视频中，每个视频包含3到9个来自
    YCB 物体数据集 [[43](#bib.bib43)] 的物体（总共50个物体）。它包括133,827帧，分辨率为640$\times$480，非常适合于物体姿态估计和跟踪任务。
- en: T-LESS Dataset [[44](#bib.bib44)] is an RGBD dataset designed for texture-less
    objects commonly found in industrial settings. It includes 30 electrical objects
    with no obvious texture or distinguishable color properties. In addition, it includes
    images of varying resolutions. In the training set, images predominantly feature
    black backgrounds, while the test set showcases diverse backgrounds with varying
    lighting conditions and occlusions. T-LESS is challenging because of the absence
    of texture on objects and the intricate environmental settings.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: T-LESS 数据集 [[44](#bib.bib44)] 是一个 RGBD 数据集，专为工业环境中常见的无纹理物体设计。它包含30个电气物体，没有明显的纹理或可辨别的颜色属性。此外，它包括不同分辨率的图像。训练集中，图像主要以黑色背景为主，而测试集中则展示了具有不同光照条件和遮挡的多样背景。由于物体缺乏纹理和复杂的环境设置，T-LESS
    具有挑战性。
- en: ITODD Dataset [[45](#bib.bib45)] includes 28 real-world industrial objects distributed
    across over 800 scenes with around 3,500 images. This dataset leverages two industrial
    3D sensors and three high-resolution grayscale cameras to enable multi-angle observation
    of the scenes, providing comprehensive and detailed data for industrial object
    analysis and evaluation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ITODD 数据集 [[45](#bib.bib45)] 包括28种真实世界工业物体，分布在800多个场景中，共约3,500张图片。该数据集利用两个工业
    3D 传感器和三个高分辨率灰度摄像头来实现场景的多角度观察，为工业物体分析和评估提供了全面详细的数据。
- en: TYO-L / TUD-L Dataset [[36](#bib.bib36)] focus on different lighting conditions.
    Specifically, TYO-L provides observation of 3 objects under 8 lighting conditions.
    These scenes are designed to evaluate the robustness of pose estimation algorithms
    to lighting variations. Unlike TYO-L, the data collection method of TUD-L involves
    fixing the camera and manually moving the object, providing a more realistic representation
    of the object’s physical movement.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: TYO-L / TUD-L 数据集 [[36](#bib.bib36)] 关注不同的光照条件。特别是，TYO-L 提供了在8种光照条件下观察3个物体的场景。这些场景旨在评估姿态估计算法对光照变化的鲁棒性。与
    TYO-L 不同，TUD-L 的数据收集方法涉及固定摄像机并手动移动物体，提供了物体物理移动的更现实的表现。
- en: HB Dataset [[46](#bib.bib46)] covers various scenes with changes in occlusion
    and lighting conditions. It comprises 33 objects, including 17 toys, 8 household
    items, and 8 industry-related objects, distributed across 13 diverse scenes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: HB 数据集 [[46](#bib.bib46)] 包含了不同遮挡和光照条件下的各种场景。它包含33个物体，包括17个玩具、8个家庭物品和8个与工业相关的物品，分布在13个不同的场景中。
- en: HOPE Dataset [[47](#bib.bib47)] is specifically designed for household objects,
    containing 28 toy grocery objects. The HOPE-Image dataset includes objects from
    50 scenes across 10 home/office environments. Each scene includes up to 5 lighting
    variations, such as backlit and obliquely directed lighting, with shadow-casting
    effects. Additionally, the HOPE-Video dataset comprises 10 video sequences totaling
    2,038 frames, with each scene showcasing between 5 to 20 objects.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: HOPE 数据集 [[47](#bib.bib47)] 专门为家庭物品设计，包含28个玩具杂货物品。HOPE-Image 数据集包括来自10个家庭/办公室环境的50个场景中的物体。每个场景包含多达5种光照变化，如逆光和斜射光，并具有阴影效果。此外，HOPE-Video
    数据集包含10个视频序列，总计2,038帧，每个场景展示了5到20个物体。
- en: 2.1.2 Other Datasets
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 其他数据集
- en: 'YCBInEOAT Dataset [[48](#bib.bib48)] is designed for RGBD-based object pose
    tracking in robotic manipulation. It contains the ego-centric RGBD videos of a
    dual-arm robot manipulating the YCB objects [[43](#bib.bib43)]. There are 3 types
    of manipulation: single-arm pick-and-place, within-arm manipulation, and pick-and-place
    between arms. This dataset comprises annotations of ground-truth poses across
    7449 frames, encompassing 5 distinct objects depicted in 9 videos.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: YCBInEOAT 数据集 [[48](#bib.bib48)] 旨在用于基于 RGBD 的机器人操控物体姿态跟踪。它包含了双臂机器人操作 YCB 物体
    [[43](#bib.bib43)] 的自我中心 RGBD 视频。操作类型有3种：单臂拾取和放置、臂内操作和臂间拾取与放置。该数据集包含7449帧的真实姿态标注，涵盖了9个视频中的5种不同物体。
- en: ClearPose Dataset [[49](#bib.bib49)] is designed for transparent objects, which
    are widely prevalent in daily life, presenting significant challenges to visual
    perception and sensing systems due to their indistinct texture features and unreliable
    depth information. It encompasses over 350K real-world RGBD images and 5M instance
    annotations across 63 household objects.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ClearPose 数据集 [[49](#bib.bib49)] 设计用于透明物体，这些物体在日常生活中广泛存在，由于其模糊的纹理特征和不可靠的深度信息，对视觉感知和传感系统提出了重大挑战。它包括超过
    35 万张真实世界的 RGBD 图像和 500 万个实例标注，涵盖了 63 种家居物品。
- en: MP6D Dataset [[50](#bib.bib50)] is an RGBD dataset designed for object pose
    estimation of metal parts, featuring 20 texture-less metal components. It consists
    of 20,100 real-world images with object pose labels collected from various scenarios
    as well as 50K synthetic images, encompassing cluttered and occluded scenes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: MP6D 数据集 [[50](#bib.bib50)] 是一个 RGBD 数据集，专为金属部件的物体姿态估计设计，包含 20 个无纹理的金属组件。它由
    20,100 张真实世界的图像组成，这些图像包含从各种场景中收集的物体姿态标签以及 50K 张合成图像，涵盖了杂乱和遮挡的场景。
- en: '![Refer to caption](img/c77fd5515c61bcbbfd96c093997a7567.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c77fd5515c61bcbbfd96c093997a7567.png)'
- en: 'Figure 3: Chronological overview of the datasets for object pose estimation
    evaluation. Notably, the pink arrows represent the BOP Challenge datasets, which
    can be used to evaluate both instance-level and unseen object methods. The red
    references represent the datasets of articulated objects. From this, we can also
    see the development trend in the field of object pose estimation, *i.e.*, from
    instance-level methods to category-level and unseen methods.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：用于物体姿态估计评估的数据集的时间顺序概览。特别地，粉色箭头代表 BOP Challenge 数据集，可以用于评估实例级别和未见物体的方法。红色参考表示关节物体的数据集。从中我们还可以看到物体姿态估计领域的发展趋势，即从实例级别的方法到类别级别和未见方法。
- en: 2.2 Datasets for Category-Level Methods
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 类别级方法的数据集
- en: In this part, we divide the category-level datasets into rigid and articulated
    object datasets for elaboration.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将类别级数据集分为刚性和关节物体数据集进行详细说明。
- en: 2.2.1 Rigid Objects Datasets
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 刚性物体数据集
- en: 'CAMERA25 Dataset [[22](#bib.bib22)] incorporates 1085 instances across 6 object
    categories: bowl, bottle, can, camera, mug, and laptop. Notably, the object CAD
    models in CAMERA25 are sourced from the synthetic ShapeNet dataset [[51](#bib.bib51)].
    Each image within this dataset contains multiple instances, accompanied by segmentation
    masks and 9DoF pose labels.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: CAMERA25 数据集 [[22](#bib.bib22)] 包含了 1085 个实例，分为 6 个物体类别：碗、瓶子、罐子、相机、杯子和笔记本电脑。值得注意的是，CAMERA25
    中的物体 CAD 模型来自合成的 ShapeNet 数据集 [[51](#bib.bib51)]。每张图像中包含多个实例，附有分割掩码和 9DoF 姿态标签。
- en: 'REAL275 Dataset [[22](#bib.bib22)] is a real-world dataset comprising 18 videos
    and approximately 8K RGBD images. The dataset is divided into three subsets: a
    training set (7 videos), a validation set (5 videos), and a testing set (6 videos).
    It includes 42 object instances across 6 categories, consistent with those in
    the CAMERA25 dataset. REAL275 is a prominent real-world dataset extensively used
    for category-level object pose estimation in academic research.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: REAL275 数据集 [[22](#bib.bib22)] 是一个真实世界的数据集，包括 18 个视频和大约 8000 张 RGBD 图像。该数据集分为三个子集：训练集（7
    个视频）、验证集（5 个视频）和测试集（6 个视频）。它包括 42 个物体实例，涵盖 6 个类别，与 CAMERA25 数据集中的类别一致。REAL275
    是一个重要的真实世界数据集，广泛用于学术研究中的类别级物体姿态估计。
- en: kPAM Dataset [[52](#bib.bib52)] is tailored specifically for robotic applications,
    emphasizing the use of keypoints. Notably, it adopts a methodology involving 3D
    reconstruction followed by manual keypoint annotation on these reconstructions.
    With a total of 117 training sequences and 245 testing sequences, kPAM offers
    a substantial collection of data for training and evaluating algorithms related
    to robotic perception and manipulation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: kPAM 数据集 [[52](#bib.bib52)] 专为机器人应用量身定制，强调关键点的使用。值得注意的是，它采用了 3D 重建的方法，然后在这些重建上进行手动关键点标注。kPAM
    总共有 117 个训练序列和 245 个测试序列，提供了大量数据用于训练和评估与机器人感知和操作相关的算法。
- en: TOD Dataset [[53](#bib.bib53)] consists of 15 transparent objects categorized
    into 6 classes, each annotated with pertinent 3D keypoints. It encompasses a vast
    collection of 48K stereo and RGBD images capturing both transparent and opaque
    depth variations. The primary focus of the TOD dataset is on transparent 3D object
    applications, providing essential resources for tasks such as object detection
    and pose estimation in challenging scenarios involving transparency.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: TOD 数据集 [[53](#bib.bib53)] 包含 15 个透明物体，分为 6 个类别，每个物体都标注了相关的 3D 关键点。它涵盖了 48K
    张立体和 RGBD 图像，捕捉了透明和不透明深度变化。TOD 数据集的主要焦点是透明 3D 物体应用，为在涉及透明度的挑战性场景中的物体检测和姿态估计任务提供了重要资源。
- en: Objectron Dataset [[54](#bib.bib54)] contains 15K annotated video clips with
    over 4M labeled images belonging to categories of bottles, books, bikes, cameras,
    chairs, cereal boxes, cups, laptops, and shoes. This dataset is sourced from 10
    countries spanning 5 continents, ensuring diverse geographic representation. Due
    to its extensive content, it is highly advantageous for evaluating the RGB-based
    category-level object pose estimation and tracking methods.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Objectron 数据集 [[54](#bib.bib54)] 包含 15K 个标注的视频片段和超过 4M 张标注图像，涉及瓶子、书籍、单车、相机、椅子、谷物盒、杯子、笔记本电脑和鞋子等类别。该数据集来源于跨越
    5 大洲的 10 个国家，确保了地理上的多样性。由于其丰富的内容，对于评估基于 RGB 的类别级物体姿态估计和跟踪方法极具优势。
- en: Wild6D Dataset [[55](#bib.bib55)] is a substantial real-world dataset used to
    assess self-supervised category-level object pose estimation methods. It offers
    annotations exclusively for 486 test videos with diverse backgrounds, showcasing
    162 objects across 5 categories (excluding the ”can” category found in CAMERA25
    and REAL275).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Wild6D 数据集 [[55](#bib.bib55)] 是一个大规模的真实世界数据集，用于评估自监督类别级物体姿态估计方法。它仅提供 486 个测试视频的注释，背景多样，展示了
    162 个物体，涵盖 5 个类别（不包括 CAMERA25 和 REAL275 数据集中找到的“罐子”类别）。
- en: PhoCaL Dataset [[56](#bib.bib56)] incorporates both RGBD and RGB-P (Polarisation)
    modalities. It consists of 60 meticulously crafted 3D models representing household
    objects, including symmetric, transparent, and reflective items. PhoCaL focuses
    on 8 specific object categories across 24 sequences, deliberately introducing
    challenges such as occlusion and clutter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: PhoCaL 数据集 [[56](#bib.bib56)] 包括 RGBD 和 RGB-P（偏振）模式。它由 60 个精心制作的 3D 模型组成，代表家庭物体，包括对称、透明和反射物品。PhoCaL
    专注于 8 个特定物体类别，共 24 个序列，故意引入了遮挡和杂乱等挑战。
- en: HouseCat6D Dataset [[57](#bib.bib57)] is a comprehensive dataset designed for
    multi-modal category-level object pose estimation and grasping tasks. The dataset
    encompasses a wide range of household object categories, featuring 194 high-quality
    3D models. It includes objects of varying photometric complexity, such as transparent
    and reflective items, and spans 41 scenes with diverse viewpoints. The dataset
    is specifically curated to address challenges in object pose estimation, including
    occlusions and the absence of markers, making it suitable for evaluating algorithms
    under real-world conditions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: HouseCat6D 数据集 [[57](#bib.bib57)] 是一个综合数据集，旨在进行多模态类别级物体姿态估计和抓取任务。该数据集涵盖了广泛的家庭物体类别，包含
    194 个高质量的 3D 模型。它包括具有不同光度复杂度的物体，例如透明和反射物品，并涵盖了 41 个具有多样视角的场景。该数据集特别策划以应对物体姿态估计中的挑战，包括遮挡和标记缺失，使其适合于在现实世界条件下评估算法。
- en: 2.2.2 Articulated Objects Datasets
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 关节物体数据集
- en: 'BMVC Dataset [[58](#bib.bib58)] includes 4 articulated objects: laptop, cabinet,
    cupboard, and toy train. Each object is modeled as a motion chain comprising components
    and interconnected heads. Joints are constrained to one rotational and one translational
    DoF. This dataset provides CAD models and accompanying text files detailing the
    topology of the underlying motion chain structure for each object.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: BMVC 数据集 [[58](#bib.bib58)] 包括 4 种关节物体：笔记本电脑、橱柜、储物柜和玩具火车。每个物体被建模为一个由组件和互联头部组成的运动链。关节限制为一个旋转自由度和一个平移自由度。该数据集提供了
    CAD 模型和附带的文本文件，详细说明了每个物体的基础运动链结构的拓扑。
- en: RBO Dataset [[59](#bib.bib59)] contains 14 commonly found articulated objects
    in human environments, with 358 interaction sequences, resulting in a total of
    67 minutes of manual manipulation under different experimental conditions, including
    changes in interaction type, lighting, viewpoint, and background settings.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: RBO 数据集 [[59](#bib.bib59)] 包含 14 种在人工环境中常见的关节物体，具有 358 个交互序列，总共 67 分钟的手动操作记录，涵盖了不同的实验条件，包括交互类型、照明、视角和背景设置的变化。
- en: HOI4D Dataset [[60](#bib.bib60)] is pivotal for advancing research in category-level
    human-object interactions. It comprises 2.4M RGBD self-centered video frames depicting
    interactions between over 9 participants and 800 object instances. These instances
    are divided into 16 categories, including 7 rigid and 9 articulated objects.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: HOI4D 数据集 [[60](#bib.bib60)] 对于推进类别级人类-物体互动的研究至关重要。它包含 240 万个 RGBD 自中心视频帧，描绘了超过
    9 名参与者和 800 个物体实例之间的互动。这些实例被分为 16 类，包括 7 类刚性物体和 9 类关节物体。
- en: ReArtMix / ReArtVal Datasets [[61](#bib.bib61)] are formulated to tackle the
    challenge of partial-level multiple articulated objects pose estimation featuring
    unknown kinematic structures. The ReArtMix dataset encompasses over 100,000 RGBD
    images rendered against diverse background scenes. The ReArtVal dataset consists
    of 6 real-world desktop scenes comprising over 6,000 RGBD frames.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ReArtMix / ReArtVal 数据集 [[61](#bib.bib61)] 旨在解决部分级多关节物体姿态估计所面临的挑战，这些物体具有未知的运动学结构。ReArtMix
    数据集包括超过 100,000 张 RGBD 图像，这些图像呈现在各种背景场景中。ReArtVal 数据集包含 6 个现实世界的桌面场景，总计超过 6,000
    张 RGBD 帧。
- en: 'ContactArt Dataset [[62](#bib.bib62)] is generated using a remote operating
    system [[63](#bib.bib63)] to manipulate articulated objects in a simulation environment.
    This system utilizes smartphones and laptops to precisely annotate poses and contact
    information. This dataset contains 5 prevalent categories of articulated objects:
    laptops, drawers, safes, microwaves, and trash cans, for a total of 80 instances.
    All object models are sourced from the PartNet dataset[[64](#bib.bib64)], thus
    promoting scalability.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ContactArt 数据集 [[62](#bib.bib62)] 使用远程操作系统 [[63](#bib.bib63)] 在模拟环境中操作关节物体生成。该系统利用智能手机和笔记本电脑精确标注姿态和接触信息。此数据集包含
    5 种常见类别的关节物体：笔记本电脑、抽屉、保险箱、微波炉和垃圾桶，总计 80 个实例。所有物体模型均来源于 PartNet 数据集 [[64](#bib.bib64)]，从而促进了可扩展性。
- en: 2.3 Datasets for Unseen Methods
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 未见方法的数据集
- en: 'The current mainstream datasets for evaluating unseen methods are the BOP Challenge
    datasets, as discussed in Sec. [2.1.1](#S2.SS1.SSS1 "2.1.1 BOP Challenge Datasets
    ‣ 2.1 Datasets for Instance-Level Methods ‣ 2 Datasets and Metrics ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey"). Besides these BOP Challenge
    datasets, there are also some datasets designed for evaluating manual reference
    view-based methods as follows.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当前评估未见方法的主流数据集是 BOP Challenge 数据集，详见 Sec. [2.1.1](#S2.SS1.SSS1 "2.1.1 BOP Challenge
    数据集 ‣ 2.1 数据集 ‣ 2 数据集和指标 ‣ 基于深度学习的物体姿态估计：综合调查")。除了这些 BOP Challenge 数据集外，还有一些数据集是专门为评估手动参考视图方法设计的。
- en: MOPED Dataset [[65](#bib.bib65)] is a model-free object pose estimation dataset
    featuring 11 household objects. It includes reference and test images that encompass
    all views of the objects. Each object in the test sequences is depicted in five
    distinct environments, with approximately 300 test images per object.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: MOPED 数据集 [[65](#bib.bib65)] 是一个无模型物体姿态估计数据集，涵盖 11 种家庭物体。它包括参考和测试图像，涵盖物体的所有视角。测试序列中的每个物体在五种不同的环境中进行描绘，每种物体大约有
    300 张测试图像。
- en: GenMOP Dataset [[1](#bib.bib1)] includes 10 objects ranging from flat objects
    to thin structure objects. For each object, there are two video sequences collected
    from various backgrounds and lighting situations. Each video sequence consists
    of approximately 200 images.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: GenMOP 数据集 [[1](#bib.bib1)] 包含 10 种物体，从平面物体到薄结构物体不等。每种物体都有两段视频序列，这些视频序列采集于不同的背景和光照条件下。每段视频序列由大约
    200 张图像组成。
- en: OnePose Dataset [[66](#bib.bib66)] comprises over 450 real-world video sequences
    of 150 objects. These sequences are collected in a variety of background conditions
    and capture all angles of the objects. Each environment has an average duration
    of 30 seconds. The dataset is randomly partitioned into training and validation
    sets.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: OnePose 数据集 [[66](#bib.bib66)] 包含超过 450 段真实世界的视频序列，涵盖 150 种物体。这些序列在各种背景条件下采集，并捕捉物体的所有角度。每个环境的平均时长为
    30 秒。数据集随机分为训练集和验证集。
- en: 'OnePose-LowTexture Dataset [[2](#bib.bib2)] is introduced as a complement to
    the testing set of the existing OnePose dataset[[66](#bib.bib66)], which predominantly
    features textured objects. This dataset comprises 40 household objects with low
    texture. For each object, there are two video sequences: one serving as the reference
    video and the other for testing. Each video is captured at a resolution of 1920$\times$1440,
    30 Frames Per Second (FPS), and approximately 30 seconds in duration.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: OnePose-LowTexture 数据集[[2](#bib.bib2)]作为对现有OnePose数据集[[66](#bib.bib66)]测试集的补充被引入，该数据集主要包含纹理丰富的物体。该数据集包含40个低纹理的家用物品。每个物品有两个视频序列：一个作为参考视频，另一个用于测试。每个视频的分辨率为1920$\times$1440，30帧每秒（FPS），时长约为30秒。
- en: 2.4 Metrics
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 评估指标
- en: In this part, we divide the metrics into 3DoF, 6DoF, 9DoF, and other evaluation
    metrics for a comprehensive overview.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将评估指标分为3DoF、6DoF、9DoF及其他评估指标，以便进行全面的概述。
- en: 2.4.1 3DoF Evaluation Metrics
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1 3DoF 评估指标
- en: 'The geodesic distance [[67](#bib.bib67)] between the ground-truth and predicted
    3D rotations is a commonly used 3DoF pose estimation metric. Calculating the angle
    error between two rotation matrices can visually evaluate their relative deviation.
    It can be formulated as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 地面真实值与预测的3D旋转之间的测地距离[[67](#bib.bib67)]是常用的3DoF姿态估计指标。计算两个旋转矩阵之间的角度误差可以直观地评估它们的相对偏差。它可以被表述如下：
- en: '|  | $\begin{array}[]{l}d\left({R_{gt}},R\right)=\arccos\left(\frac{{tr}\left({R_{gt}}^{\top}R\right)-1}{2}\right)/\pi\end{array},\vspace{-0.5em}$
    |  | (1) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{array}[]{l}d\left({R_{gt}},R\right)=\arccos\left(\frac{{tr}\left({R_{gt}}^{\top}R\right)-1}{2}\right)/\pi\end{array},\vspace{-0.5em}$
    |  | (1) |'
- en: 'where ${R_{gt}}$ and $R$ denote the ground-truth and predicted 3D rotations,
    respectively. $\top$ represents matrix transpose. $tr$ denotes the trace of a
    matrix, which refers to the sum of the elements on the main diagonal. Typically,
    3D rotation estimation accuracy is defined as the percentage of objects whose
    angle error is below a specific threshold and whose predicted class is correct.
    It can be expressed as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，${R_{gt}}$和$R$分别表示地面真实值和预测的3D旋转。$\top$表示矩阵的转置。$tr$表示矩阵的迹，即主对角线元素的总和。通常，3D旋转估计的准确度定义为角度误差低于特定阈值且预测类别正确的对象的百分比。它可以被表述如下：
- en: '|  | <math   alttext="Acc.=\left\{{\begin{array}[]{*{20}{c}}{1,}\\ {0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20}{c}}{d({R_{gt}},R)<\lambda\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}c=c_{gt}}\end{array}}\\'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="Acc.=\left\{{\begin{array}[]{*{20}{c}}{1,}\\ {0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20}{c}}{d({R_{gt}},R)<\lambda\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}c=c_{gt}}\end{array}}\\'
- en: '{{\rm{otherwise}}}\end{array}," display="block"><semantics ><mrow ><mi >A</mi><mi
    >c</mi><mi >c</mi><mo lspace="0em" rspace="0.0835em" >.</mo><mo lspace="0.0835em"
    >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd ><mrow ><mn >1</mn><mo >,</mo></mrow></mtd></mtr><mtr ><mtd ><mrow
    ><mn >0</mn><mo >,</mo></mrow></mtd></mtr></mtable><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr ><mtd ><mrow ><mi >if</mi><mo lspace="0.167em"
    rspace="0em" >​</mo><mtable columnspacing="5pt" displaystyle="true" ><mtr ><mtd
    ><mrow ><mrow ><mi >d</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >R</mi><mrow ><mi >g</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >t</mi></mrow></msub><mo >,</mo><mi >R</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    ><</mo><mrow ><mi >λ</mi><mo lspace="0.167em" rspace="0em" >​</mo><mtable columnspacing="5pt"
    displaystyle="true" ><mtr ><mtd ><mi >and</mi></mtd></mtr></mtable><mo lspace="0.167em"
    rspace="0em" >​</mo><mi >c</mi></mrow><mo >=</mo><msub ><mi >c</mi><mrow ><mi
    >g</mi><mo lspace="0em" rspace="0em" >​</mo><mi >t</mi></mrow></msub></mrow></mtd></mtr></mtable></mrow></mtd></mtr><mtr
    ><mtd ><mi >otherwise</mi></mtd></mtr></mtable><mo lspace="0.167em" >,</mo></mrow></mrow><annotation
    encoding="application/x-tex" >Acc.=\left\{{\begin{array}[]{*{20}{c}}{1,}\\ {0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20}{c}}{d({R_{gt}},R)<\lambda\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}c=c_{gt}}\end{array}}\\
    {{\rm{otherwise}}}\end{array},</annotation></semantics></math> |  | (2) |'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '{{\rm{otherwise}}}\end{array}," display="block"><semantics ><mrow ><mi >A</mi><mi
    >c</mi><mi >c</mi><mo lspace="0em" rspace="0.0835em" >.</mo><mo lspace="0.0835em"
    rspace="0.0835em" >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd ><mrow ><mn >1</mn><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mn >0</mn><mo >,</mo></mrow></mtd></mtr></mtable><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr ><mtd ><mrow ><mi >if</mi><mo lspace="0.167em"
    rspace="0em" >​</mo><mtable columnspacing="5pt" displaystyle="true" ><mtr ><mtd
    ><mrow ><mrow ><mi >d</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >R</mi><mrow ><mi >g</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >t</mi></mrow></msub><mo >,</mo><mi >R</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    ><</mo><mrow ><mi >λ</mi><mo lspace="0.167em" rspace="0em" >​</mo><mtable columnspacing="5pt"
    displaystyle="true" ><mtr ><mtd ><mi >and</mi></mtd></mtr></mtable><mo lspace="0.167em"
    rspace="0em" >​</mo><mi >c</mi></mrow><mo >=</mo><msub ><mi >c</mi><mrow ><mi
    >g</mi><mo lspace="0em" rspace="0em" >​</mo><mi >t</mi></mrow></msub></mrow></mtd></mtr></mtable></mrow></mtd></mtr><mtr
    ><mtd ><mi >otherwise</mi></mtd></mtr></mtable><mo lspace="0.167em" >,</mo></mrow></mrow><annotation
    encoding="application/x-tex" >Acc.=\left\{{\begin{array}[]{*{20}{c}}{1,}\\ {0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20}{c}}{d({R_{gt}},R)<\lambda\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}c=c_{gt}}\end{array}}\\
    {{\rm{otherwise}}}\end{array},</annotation></semantics></math> |  | (2) |'
- en: where $c$, $c_{gt}$, and $\lambda$ denote the predicted class, ground-truth
    class and predefined threshold, respectively.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c$、$c_{gt}$ 和 $\lambda$ 分别表示预测类别、真实类别和预定义阈值。
- en: 2.4.2 6DoF Evaluation Metrics
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2 6DoF 评估指标
- en: 'Currently, the BOP metric (*BOP-M*)[[36](#bib.bib36)] is the most popular metric,
    which is the Average Recall (*AR*) of the Visible Surface Discrepancy (VSD), Maximum
    Symmetry-Aware Surface Distance (MSSD), and Maximum Symmetry-Aware Projection
    Distance (MSPD) metrics. Specifically, the VSD [[36](#bib.bib36)] metric treats
    poses that are indistinguishable in shape as equivalent by only measuring the
    misalignment of the visible object surface. It can be expressed as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，BOP 指标（*BOP-M*）[[36](#bib.bib36)] 是最受欢迎的指标，它是可见表面差异（VSD）、最大对称感知表面距离（MSSD）和最大对称感知投影距离（MSPD）指标的平均召回率（*AR*）。具体而言，VSD
    [[36](#bib.bib36)] 指标通过仅测量可见物体表面的错位，将形状不可区分的姿势视为等效。它可以表示为：
- en: '|  | <math   alttext="\begin{array}[]{l}{e_{VSD}}\left({\hat{D},\bar{D},\hat{V},\bar{V},\tau}\right)=\\
    av{g_{p\in\hat{V}\cup\bar{V}}}\left\{{\begin{array}[]{*{20}{l}}{0,}\\'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{array}[]{l}{e_{VSD}}\left({\hat{D},\bar{D},\hat{V},\bar{V},\tau}\right)=\\
    av{g_{p\in\hat{V}\cup\bar{V}}}\left\{{\begin{array}[]{*{20}{l}}{0,}\\'
- en: '{1,}\end{array}}\right.\begin{array}[]{*{20}{l}}{{\rm{if}}\begin{array}[]{*{20}{l}}{p\in\hat{V}\cap\bar{V}\wedge&#124;\hat{D}\left(p\right)-\bar{D}\left(p\right)&#124;<\tau}\end{array}}\\'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '{1,}\end{array}}\right.\begin{array}[]{*{20}{l}}{{\rm{if}}\begin{array}[]{*{20}{l}}{p\in\hat{V}\cap\bar{V}\wedge&#124;\hat{D}\left(p\right)-\bar{D}\left(p\right)&#124;<\tau}\end{array}}\\'
- en: '{{\rm{otherwise}}}\end{array},\end{array}\vspace{-0.5em}" display="block"><semantics
    ><mtable displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow
    ><mrow ><msub  ><mi >e</mi><mrow ><mi >V</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >S</mi><mo lspace="0em" rspace="0em" >​</mo><mi >D</mi></mrow></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo  >(</mo><mover accent="true"  ><mi >D</mi><mo  >^</mo></mover><mo
    >,</mo><mover accent="true"  ><mi >D</mi><mo  >¯</mo></mover><mo >,</mo><mover
    accent="true"  ><mi >V</mi><mo  >^</mo></mover><mo >,</mo><mover accent="true"  ><mi
    >V</mi><mo  >¯</mo></mover><mo >,</mo><mi >τ</mi><mo >)</mo></mrow></mrow><mo
    >=</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow  ><mrow ><mi >a</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >v</mi><mo lspace="0em" rspace="0em" >​</mo><msub
    ><mi >g</mi><mrow ><mi >p</mi><mo >∈</mo><mrow ><mover accent="true" ><mi >V</mi><mo
    >^</mo></mover><mo >∪</mo><mover accent="true" ><mi >V</mi><mo >¯</mo></mover></mrow></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow ><mn
    >0</mn><mo >,</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow ><mn  >1</mn><mo
    >,</mo></mrow></mtd></mtr></mtable></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow
    ><mi >if</mi><mo lspace="0.167em" rspace="0em"  >​</mo><mtable columnspacing="5pt"
    displaystyle="true"  ><mtr ><mtd columnalign="left" ><mrow ><mi >p</mi><mo >∈</mo><mrow
    ><mrow ><mover accent="true" ><mi >V</mi><mo >^</mo></mover><mo >∩</mo><mover
    accent="true" ><mi >V</mi><mo >¯</mo></mover></mrow><mo >∧</mo><mrow ><mo stretchy="false"  >&#124;</mo><mrow
    ><mrow ><mover accent="true"  ><mi >D</mi><mo >^</mo></mover><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo >(</mo><mi >p</mi><mo >)</mo></mrow></mrow><mo
    >−</mo><mrow ><mover accent="true"  ><mi >D</mi><mo >¯</mo></mover><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo >(</mo><mi >p</mi><mo >)</mo></mrow></mrow></mrow><mo
    stretchy="false"  >&#124;</mo></mrow></mrow><mo ><</mo><mi >τ</mi></mrow></mtd></mtr></mtable></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mi >otherwise</mi></mtd></mtr></mtable></mrow><mo
    lspace="0.167em"  >,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content"
    ><matrix  ><matrixrow ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑒</ci><apply ><ci >𝑉</ci><ci >𝑆</ci><ci >𝐷</ci></apply></apply><vector ><apply  ><ci
    >^</ci><ci >𝐷</ci></apply><apply ><ci  >¯</ci><ci >𝐷</ci></apply><apply ><ci >^</ci><ci  >𝑉</ci></apply><apply
    ><ci >¯</ci><ci  >𝑉</ci></apply><ci >𝜏</ci></vector></apply><csymbol cd="latexml"  >absent</csymbol></apply></matrixrow><matrixrow
    ><apply ><ci  >𝑎</ci><ci >𝑣</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑔</ci><apply ><ci >𝑝</ci><apply ><apply ><ci >^</ci><ci >𝑉</ci></apply><apply
    ><ci >¯</ci><ci >𝑉</ci></apply></apply></apply></apply><apply ><csymbol cd="latexml"  >cases</csymbol><matrix
    ><matrixrow ><cn type="integer" >0</cn><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><cn type="integer" >1</cn><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><matrix
    ><matrixrow ><apply ><ci >if</ci><matrix ><matrixrow ><apply ><apply ><ci >𝑝</ci><apply
    ><apply ><apply ><ci >^</ci><ci >𝑉</ci></apply><apply ><ci >¯</ci><ci >𝑉</ci></apply></apply><apply
    ><apply ><apply ><apply ><ci >^</ci><ci >𝐷</ci></apply><ci >𝑝</ci></apply><apply
    ><apply ><ci >¯</ci><ci >𝐷</ci></apply><ci >𝑝</ci></apply></apply></apply></apply></apply><apply
    ><ci >𝜏</ci></apply></apply><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror></matrixrow><matrixrow ><ci >otherwise</ci><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror></matrixrow></matrix></apply></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{l}{e_{VSD}}\left({\hat{D},\bar{D},\hat{V},\bar{V},\tau}\right)=\\
    av{g_{p\in\hat{V}\cup\bar{V}}}\left\{{\begin{array}[]{*{20}{l}}{0,}\\ {1,}\end{array}}\right.\begin{array}[]{*{20}{l}}{{\rm{if}}\begin{array}[]{*{20}{l}}{p\in\hat{V}\cap\bar{V}\wedge&#124;\hat{D}\left(p\right)-\bar{D}\left(p\right)&#124;<\tau}\end{array}}\\
    {{\rm{otherwise}}}\end{array},\end{array}\vspace{-0.5em}</annotation></semantics></math>
    |  | (3) |'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '{{\rm{否则}}}\end{array},\end{array}\vspace{-0.5em}" display="block"><semantics
    ><mtable displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow
    ><mrow ><msub  ><mi >e</mi><mrow ><mi >V</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >S</mi><mo lspace="0em" rspace="0em" >​</mo><mi >D</mi></mrow></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo  >(</mo><mover accent="true"  ><mi >D</mi><mo  >^</mo></mover><mo
    >,</mo><mover accent="true"  ><mi >D</mi><mo  >¯</mo></mover><mo >,</mo><mover
    accent="true"  ><mi >V</mi><mo  >^</mo></mover><mo >,</mo><mover accent="true"  ><mi
    >V</mi><mo  >¯</mo></mover><mo >,</mo><mi >τ</mi><mo >)</mo></mrow></mrow><mo
    >=</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow  ><mrow ><mi >a</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >v</mi><mo lspace="0em" rspace="0em" >​</mo><msub
    ><mi >g</mi><mrow ><mi >p</mi><mo >∈</mo><mrow ><mover accent="true" ><mi >V</mi><mo
    >^</mo></mover><mo >∪</mo><mover accent="true" ><mi >V</mi><mo >¯</mo></mover></mrow></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow ><mn
    >0</mn><mo >,</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow ><mn  >1</mn><mo
    >,</mo></mrow></mtd></mtr></mtable></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow
    ><mi >如果</mi><mo lspace="0.167em" rspace="0em"  >​</mo><mtable columnspacing="5pt"
    displaystyle="true"  ><mtr ><mtd columnalign="left" ><mrow ><mi >p</mi><mo >∈</mo><mrow
    ><mrow ><mover accent="true" ><mi >V</mi><mo >^</mo></mover><mo >∩</mo><mover
    accent="true" ><mi >V</mi><mo >¯</mo></mover></mrow><mo >∧</mo><mrow ><mo stretchy="false"  >&#124;</mo><mrow
    ><mrow ><mover accent="true"  ><mi >D</mi><mo >^</mo></mover><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo >(</mo><mi >p</mi><mo >)</mo></mrow></mrow><mo
    >−</mo><mrow ><mover accent="true"  ><mi >D</mi><mo >¯</mo></mover><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo >(</mo><mi >p</mi><mo >)</mo></mrow></mrow></mrow><mo
    stretchy="false"  >&#124;</mo></mrow></mrow><mo ><</mo><mi >τ</mi></mrow></mtd></mtr></mtable></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mi >否则</mi></mtd></mtr></mtable></mrow><mo lspace="0.167em"  >,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix  ><matrixrow ><apply ><apply  ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑒</ci><apply ><ci >𝑉</ci><ci >𝑆</ci><ci
    >𝐷</ci></apply></apply><vector ><apply  ><ci >^</ci><ci >𝐷</ci></apply><apply
    ><ci  >¯</ci><ci >𝐷</ci></apply><apply ><ci >^</ci><ci  >𝑉</ci></apply><apply
    ><ci >¯</ci><ci  >𝑉</ci></apply><ci >𝜏</ci></vector></apply><csymbol cd="latexml"  >absent</csymbol></apply></matrixrow><matrixrow
    ><apply ><ci  >𝑎</ci><ci >𝑣</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑔</ci><apply ><ci >𝑝</ci><apply ><apply ><ci >^</ci><ci >𝑉</ci></apply><apply
    ><ci >¯</ci><ci >𝑉</ci></apply></apply></apply></apply></apply></apply><apply
    ><csymbol cd="latexml"  >cases</csymbol><matrix ><matrixrow ><cn type="integer"
    >0</cn><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><cn type="integer" >1</cn><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol'
- en: 'where the symbols $\hat{D}$ and $\bar{D}$ represent distance maps generated
    by rendering the object model $M$ in two different poses: $\hat{P}$ (an estimated
    pose) and $\bar{P}$ (the ground-truth pose), respectively. In these maps, each
    pixel $p$ stores the distance from the camera center to a 3D point ${{\rm{x}}_{p}}$
    that projects onto $p$. These distance values are derived from depth maps, which
    are typical outputs of sensors like Kinect, containing the $Z$ coordinate of ${{\rm{x}}_{p}}$.
    These distance maps are compared with the distance map ${D_{I}}$ of the test image
    $I$ to derive visibility masks $\hat{V}$ and $\bar{V}$. These masks identify pixels
    where the model $M$ is visible in the image $I$. The parameter $\tau$ represents
    the tolerance for misalignment. In addition, the MSSD [[36](#bib.bib36)] metric
    is a suitable factor for determining the likelihood of successful robotic manipulation
    and is not significantly affected by object geometry or surface sampling density.
    It can be formulated as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中符号 $\hat{D}$ 和 $\bar{D}$ 分别表示通过渲染对象模型 $M$ 在两种不同姿态下生成的距离图：$\hat{P}$（估计姿态）和
    $\bar{P}$（真实姿态）。在这些图中，每个像素 $p$ 存储从相机中心到投影到 $p$ 的 3D 点 ${{\rm{x}}_{p}}$ 的距离。这些距离值来源于深度图，这是像
    Kinect 这样的传感器的典型输出，包含了 ${{\rm{x}}_{p}}$ 的 $Z$ 坐标。这些距离图与测试图像 $I$ 的距离图 ${D_{I}}$
    进行比较，以得出可见性掩模 $\hat{V}$ 和 $\bar{V}$。这些掩模识别模型 $M$ 在图像 $I$ 中可见的像素。参数 $\tau$ 表示对齐误差的容忍度。此外，MSSD
    [[36](#bib.bib36)] 度量指标是确定机器人操作成功可能性的合适因素，不受物体几何形状或表面采样密度的显著影响。它可以表示如下：
- en: '|  | ${e_{MSSD}}\left({\hat{P},\bar{P},{S_{M}},{V_{M}}}\right)={\min_{S\in{S_{M}}}}{\max_{{\rm{x}}\in{V_{M}}}}{\left\&#124;{\hat{P}{\rm{x}}-\bar{P}S{\rm{x}}}\right\&#124;_{2}},$
    |  | (4) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | ${e_{MSSD}}\left({\hat{P},\bar{P},{S_{M}},{V_{M}}}\right)={\min_{S\in{S_{M}}}}{\max_{{\rm{x}}\in{V_{M}}}}{\left\|{\hat{P}{\rm{x}}-\bar{P}S{\rm{x}}}\right\|_{2}},$
    |  | (4) |'
- en: 'where the set ${{S_{M}}}$ comprises global symmetry transformations for the
    object model $M$, while ${{V_{M}}}$ represents the vertices of the model. Furthermore,
    the MSPD [[36](#bib.bib36)] metric is ideal for evaluating RGB-only methods in
    augmented reality, focusing on perceivable discrepancies and excluding alignment
    along the optical (Z) axis, which can be represented as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，集合 ${{S_{M}}}$ 包含了对象模型 $M$ 的全局对称变换，而 ${{V_{M}}}$ 代表模型的顶点。此外，MSPD [[36](#bib.bib36)]
    度量指标非常适合用于评估增强现实中的仅 RGB 方法，重点关注可感知的差异，并排除了沿光学 (Z) 轴的对齐，可以表示如下：
- en: '|  | $\begin{array}[]{l}{e_{MSPD}}\left({\hat{P},\bar{P},{S_{M}},{V_{M}}}\right)=\\
    {\min_{S\in{S_{M}}}}{\max_{{\rm{x}}\in{V_{M}}}}{\left\&#124;{proj\left({\hat{P}{\rm{x}}}\right)-proj\left({\bar{P}S{\rm{x}}}\right)}\right\&#124;_{2}},\end{array}\vspace{-0.5em}$
    |  | (5) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{array}[]{l}{e_{MSPD}}\left({\hat{P},\bar{P},{S_{M}},{V_{M}}}\right)=\\
    {\min_{S\in{S_{M}}}}{\max_{{\rm{x}}\in{V_{M}}}}{\left\|{proj\left({\hat{P}{\rm{x}}}\right)-proj\left({\bar{P}S{\rm{x}}}\right)}\right\|_{2}},\end{array}\vspace{-0.5em}$
    |  | (5) |'
- en: where the function $proj()$ represents the 2D projection (pixel-level), and
    the other symbols have the same meanings as in MSSD.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 其中函数 $proj()$ 代表 2D 投影（像素级），其他符号在 MSSD 中具有相同的含义。
- en: 'Besides the *BOP-M*, the average point distance (ADD)[[37](#bib.bib37)] and
    average closest point distance (ADD-S)[[37](#bib.bib37)] are also commonly leveraged
    to evaluate the performance of 6DoF object pose estimation. They can intuitively
    quantify the geometric error between the estimated and the ground-truth poses
    by computing the average distance between corresponding points on the object CAD
    model. Specifically, the ADD metric is designed for asymmetric objects, while
    the ADD-S metric is designed explicitly for symmetric objects. Given the ground-truth
    rotation ${R_{gt}}$ and translation ${t_{gt}}$, as well as the estimated rotation
    $R$ and translation $t$, ADD calculates the average pairwise distance between
    the 3D model points ${x\in O}$ corresponding to the transformation between the
    ground truth and estimated pose:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 *BOP-M* 外，平均点距离 (ADD)[[37](#bib.bib37)] 和平均最近点距离 (ADD-S)[[37](#bib.bib37)]
    也常用于评估 6DoF 物体姿态估计的性能。它们可以通过计算对象 CAD 模型上对应点之间的平均距离，直观地量化估计姿态与真实姿态之间的几何误差。具体来说，ADD
    度量指标是为非对称物体设计的，而 ADD-S 度量指标则是专门为对称物体设计的。给定真实旋转 ${R_{gt}}$ 和平移 ${t_{gt}}$ 以及估计旋转
    $R$ 和平移 $t$，ADD 计算 3D 模型点 ${x\in O}$ 之间的平均成对距离，这些点对应于真实和估计姿态之间的变换：
- en: '|  | $ADD=\mathop{avg}\limits_{x\in O}\left\&#124;{\left({{R_{gt}}x+{t_{gt}}}\right)-\left({Rx+t}\right)}\right\&#124;.\vspace{-0.5em}$
    |  | (6) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | $ADD=\mathop{avg}\limits_{x\in O}\left\|{\left({{R_{gt}}x+{t_{gt}}}\right)-\left({Rx+t}\right)}\right\|.$\vspace{-0.5em}$
    |  | (6) |'
- en: 'For symmetric objects, the matching between points in certain views is inherently
    ambiguous. Therefore, the average distance is calculated using the nearest point
    distance as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对称物体，某些视角下的点匹配本质上是模糊的。因此，平均距离是通过以下方式计算最近点距离的：
- en: '|  | $ADD{\rm{-}}S=\mathop{avg}\limits_{{x_{1}}\in O}\mathop{\min}\limits_{{x_{2}}\in
    O}\left\&#124;{\left({{R_{gt}}{x_{1}}+{t_{gt}}}\right)-\left({R{x_{2}}+t}\right)}\right\&#124;.\vspace{-0.5em}$
    |  | (7) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $ADD{\rm{-}}S=\mathop{avg}\limits_{{x_{1}}\in O}\mathop{\min}\limits_{{x_{2}}\in
    O}\left\&#124;{\left({{R_{gt}}{x_{1}}+{t_{gt}}}\right)-\left({R{x_{2}}+t}\right)}\right\&#124;.\vspace{-0.5em}$
    |  | (7) |'
- en: Meanwhile, the area under the ADD and the ADD-S curve (AUC) are often leveraged
    for evaluation. Specifically, if the ADD and ADD-S are smaller than a given threshold,
    the predicted pose will be considered correct. Moreover, there are many methods
    [[68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)] that evaluate asymmetric
    and symmetric objects using ADD and ADD-S, respectively. This metric is termed
    ADD(S).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，ADD 和 ADD-S 曲线下的面积（AUC）通常用于评估。具体来说，如果 ADD 和 ADD-S 小于给定阈值，则预测的姿态将被视为正确。此外，还有许多方法
    [[68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)] 使用 ADD 和 ADD-S 分别评估非对称和对称物体。这个指标被称为
    ADD(S)。
- en: 'In addition, $n^{\circ}$$m{\rm{cm}}$ [[71](#bib.bib71)] is also currently a
    prevalent evaluation metric (especially in category-level object pose estimation).
    It directly quantifies the errors in predicted 3D rotation and 3D translation.
    An object pose prediction is deemed correct if its rotation error is below threshold
    $n^{\circ}$ and its translation error is below threshold $m{\rm{cm}}$. It can
    be defined as an indicator function as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，$n^{\circ}$$m{\rm{cm}}$ [[71](#bib.bib71)] 也是当前一种广泛使用的评估指标（特别是在类别级别的物体姿态估计中）。它直接量化预测的
    3D 旋转和 3D 平移的误差。如果物体姿态预测的旋转误差低于阈值 $n^{\circ}$，且平移误差低于阈值 $m{\rm{cm}}$，则认为其为正确。它可以定义为如下的指示函数：
- en: '|  | <math   alttext="\vspace{-0.5em}{I_{{n^{\circ}}m{\rm{cm}}}}({e_{R}},{e_{t}})=\left\{{\begin{array}[]{*{20}{c}}{1,}\\
    {0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20}{c}}{{e_{R}}<{n^{\circ}}\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}{e_{t}}<m{\rm{cm}}}\end{array}}\\'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\vspace{-0.5em}{I_{{n^{\circ}}m{\rm{cm}}}}({e_{R}},{e_{t}})=\left\{{\begin{array}[]{*{20}{c}}{1,}\\
    {0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20}{c}}{{e_{R}}<{n^{\circ}}\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}{e_{t}}<m{\rm{cm}}}\end{array}}\\'
- en: '{{\rm{otherwise}}}\end{array}," display="block"><semantics ><mrow ><mrow  ><mrow
    ><msub ><mi  >I</mi><mrow ><msup ><mi  >n</mi><mo >∘</mo></msup><mo lspace="0em"
    rspace="0em"  >​</mo><mi >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >cm</mi></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >e</mi><mi
    >R</mi></msub><mo >,</mo><msub ><mi  >e</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mrow ><mrow  ><mo >{</mo><mtable columnspacing="5pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd  ><mrow ><mn >1</mn><mo  >,</mo></mrow></mtd></mtr><mtr
    ><mtd  ><mrow ><mn >0</mn><mo  >,</mo></mrow></mtd></mtr></mtable></mrow><mo lspace="0.167em"
    rspace="0em"  >​</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd ><mrow ><mi  >if</mi><mo lspace="0.167em" rspace="0em"  >​</mo><mtable
    columnspacing="5pt" displaystyle="true"  ><mtr ><mtd ><mrow ><msub ><mi >e</mi><mi
    >R</mi></msub><mo ><</mo><mrow ><msup ><mi >n</mi><mo >∘</mo></msup><mo lspace="0.167em"
    rspace="0em"  >​</mo><mtable columnspacing="5pt" displaystyle="true" ><mtr ><mtd
    ><mi >and</mi></mtd></mtr></mtable><mo lspace="0.167em" rspace="0em"  >​</mo><msub
    ><mi >e</mi><mi >t</mi></msub></mrow><mo ><</mo><mrow ><mi >m</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >cm</mi></mrow></mrow></mtd></mtr></mtable></mrow></mtd></mtr><mtr
    ><mtd ><mi  >otherwise</mi></mtd></mtr></mtable></mrow></mrow><mo lspace="0.167em"  >,</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐼</ci><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑛</ci></apply><ci
    >𝑚</ci><ci >cm</ci></apply></apply><interval closure="open" ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑒</ci><ci >𝑅</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑒</ci><ci >𝑡</ci></apply></interval></apply><apply
    ><apply  ><csymbol cd="latexml"  >cases</csymbol><matrix ><matrixrow ><cn type="integer"  >1</cn><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror></matrixrow><matrixrow ><cn type="integer"
    >0</cn><cerror  ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><matrix
    ><matrixrow ><apply  ><ci >if</ci><matrix ><matrixrow ><apply ><apply ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑒</ci><ci >𝑅</ci></apply><apply
    ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑛</ci></apply><matrix
    ><matrixrow ><ci >and</ci><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror></matrixrow></matrix><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑒</ci><ci >𝑡</ci></apply></apply></apply><apply
    ><apply ><ci >𝑚</ci><ci >cm</ci></apply></apply></apply><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror></matrixrow><matrixrow ><ci >otherwise</ci><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\vspace{-0.5em}{I_{{n^{\circ}}m{\rm{cm}}}}({e_{R}},{e_{t}})=\left\{{\begin{array}[]{*{20}{c}}{1,}\\
    {0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20}{c}}{{e_{R}}<{n^{\circ}}\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}{e_{t}}<m{\rm{cm}}}\end{array}}\\
    {{\rm{otherwise}}}\end{array},</annotation></semantics></math> |  | (8) |'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '{{\rm{否则}}}\end{array}," display="block"><semantics ><mrow ><mrow  ><mrow ><msub
    ><mi  >I</mi><mrow ><msup ><mi  >n</mi><mo >∘</mo></msup><mo lspace="0em" rspace="0em"  >​</mo><mi
    >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >cm</mi></mrow></msub><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >e</mi><mi
    >R</mi></msub><mo >,</mo><msub ><mi  >e</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mrow ><mrow  ><mo >{</mo><mtable columnspacing="5pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd  ><mrow ><mn >1</mn><mo  >,</mo></mrow></mtd></mtr><mtr
    ><mtd  ><mrow ><mn >0</mn><mo  >,</mo></mrow></mtd></mtr></mtable></mrow><mo lspace="0.167em"
    rspace="0em"  >​</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd ><mrow ><mi  >if</mi><mo lspace="0.167em" rspace="0em"  >​</mo><mtable
    columnspacing="5pt" displaystyle="true"  ><mtr ><mtd ><mrow ><msub ><mi >e</mi><mi
    >R</mi></msub><mo ><</mo><mrow ><msup ><mi >n</mi><mo >∘</mo></msup><mo lspace="0.167em"
    rspace="0em"  >​</mo><mtable columnspacing="5pt" displaystyle="true" ><mtr ><mtd
    ><mi >and</mi></mtd></mtr></mtable><mo lspace="0.167em" rspace="0em"  >​</mo><msub
    ><mi >e</mi><mi >t</mi></msub></mrow><mo ><</mo><mrow ><mi >m</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >cm</mi></mrow></mrow></mtd></mtr></mtable></mrow></mtd></mtr><mtr
    ><mtd ><mi  >否则</mi></mtd></mtr></mtable></mrow></mrow><mo lspace="0.167em"  >,</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐼</ci><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑛</ci></apply><ci
    >𝑚</ci><ci >cm</ci></apply></apply><interval closure="open" ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑒</ci><ci >𝑅</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑒</ci><ci >𝑡</ci></apply></interval></apply><apply
    ><apply  ><csymbol cd="latexml"  >cases</csymbol><matrix ><matrixrow ><cn type="integer"  >1</cn><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror></matrixrow><matrixrow ><cn type="integer"
    >0</cn><cerror  ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><matrix
    ><matrixrow ><apply  ><ci >if</ci><matrix ><matrixrow ><apply ><apply ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑒</ci><ci >𝑅</ci></apply><apply
    ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑛</ci></apply><matrix
    ><matrixrow ><ci >and</ci><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><cerror ><csymbol
    cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></'
- en: where ${{e_{R}}}$ and ${{e_{t}}}$ represent the rotation and translation errors
    between the estimated and ground-truth values, respectively.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${{e_{R}}}$ 和 ${{e_{t}}}$ 分别表示估计值和真实值之间的旋转误差和平移误差。
- en: Furthermore, compared to directly comparing 6DoF pose in 3D space, the simplicity
    and practicality of the 2D Projection metric [[38](#bib.bib38)] make it suitable
    for evaluation as well, which quantifies the average distance between CAD model
    points when projected under the estimated object pose and the ground-truth pose.
    A pose is considered correct if the projected distances are less than 5 pixels.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与直接比较三维空间中的6自由度姿态相比，2D投影度量 [[38](#bib.bib38)] 的简单性和实用性使其也适合用于评估，它量化了在估计的物体姿态和真实姿态下，CAD模型点的平均投影距离。如果投影距离小于5像素，则认为姿态是正确的。
- en: 2.4.3 9DoF Evaluation Metric
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3 9自由度评估指标
- en: '$Io{U_{3D}}$ denotes the Intersection-over-Union (IoU)[[22](#bib.bib22)] percentage
    between the ground-truth and predicted 3D bounding boxes, which can evaluate the
    6DoF pose estimation as well as the 3DoF size estimation. It can be expressed
    as:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: $Io{U_{3D}}$ 表示真实值和预测的三维边界框之间的交并比（IoU）[[22](#bib.bib22)]百分比，能够评估6自由度姿态估计和3自由度尺寸估计。其表达式为：
- en: '|  | $Io{U_{3D}}=\frac{{{P_{B}}\cap{G_{B}}}}{{{P_{B}}\cup{G_{B}}}},\vspace{-0.5em}$
    |  | (9) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $Io{U_{3D}}=\frac{{{P_{B}}\cap{G_{B}}}}{{{P_{B}}\cup{G_{B}}}},\vspace{-0.5em}$
    |  | (9) |'
- en: where ${{G_{B}}}$ and ${{P_{B}}}$ represent the ground-truth and the predicted
    3D bounding boxes, respectively. The symbols $\cap$ and $\cup$ represent the intersection
    and union, respectively. The correctness of the predicted object pose is determined
    based on whether the $Io{U_{3D}}$ value exceeds a predefined threshold.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${{G_{B}}}$ 和 ${{P_{B}}}$ 分别表示真实值和预测的三维边界框。符号 $\cap$ 和 $\cup$ 分别表示交集和并集。预测物体姿态的正确性取决于
    $Io{U_{3D}}$ 值是否超过预定义的阈值。
- en: 2.4.4 Other Metric
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.4 其他度量
- en: 'Since some Normalized Object Coordinate Space (NOCS) shape alignment-based
    category-level methods reconstruct the 3D object shape before estimating the object
    pose, the Chamfer Distance (CD) metric[[72](#bib.bib72)], which not only captures
    the global shape deviation but is also sensitive to local shape differences, is
    commonly leveraged to evaluate the NOCS shape reconstruction accuracy of these
    methods as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一些基于归一化物体坐标空间（NOCS）形状对齐的类别级方法在估计物体姿态之前重建了三维物体形状，因此通常利用Chamfer距离（CD）度量[[72](#bib.bib72)]来评估这些方法的NOCS形状重建精度，因为CD度量不仅捕捉全局形状偏差，还对局部形状差异敏感。
- en: '|  | ${D_{cd}}=\sum\limits_{x\in N}{\mathop{\min}\limits_{y\in{M_{gt}}}}\parallel
    x-y\parallel_{2}^{2}+\sum\limits_{y\in{M_{gt}}}{\mathop{\min}\limits_{x\in N}}\parallel
    x-y\parallel_{2}^{2},\vspace{-0.5em}$ |  | (10) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | ${D_{cd}}=\sum\limits_{x\in N}{\mathop{\min}\limits_{y\in{M_{gt}}}}\parallel
    x-y\parallel_{2}^{2}+\sum\limits_{y\in{M_{gt}}}{\mathop{\min}\limits_{x\in N}}\parallel
    x-y\parallel_{2}^{2},\vspace{-0.5em}$ |  | (10) |'
- en: where $N$ and $M_{gt}$ represent the reconstructed and ground-truth NOCS shape,
    respectively.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 和 $M_{gt}$ 分别表示重建的NOCS形状和真实值。
- en: 3 Instance-Level Object Pose Estimation
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实例级物体姿态估计
- en: 'Instance-level object pose estimation describes the task of estimating the
    pose of the objects that have been seen during the training of the model. We classify
    existing instance-level methods into four categories: correspondence-based (Sec. [3.1](#S3.SS1
    "3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣
    Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")), template-based
    (Sec. [3.2](#S3.SS2 "3.2 Template-Based Methods ‣ 3 Instance-Level Object Pose
    Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")),
    voting-based (Sec. [3.3](#S3.SS3 "3.3 Voting-Based Methods ‣ 3 Instance-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")), and regression-based (Sec. [3.4](#S3.SS4 "3.4 Regression-Based Methods
    ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")) methods.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '实例级物体姿态估计描述了估计在模型训练过程中见过的物体姿态的任务。我们将现有的实例级方法分为四类：对应关系基础方法（Sec. [3.1](#S3.SS1
    "3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣
    Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")）、模板基础方法（Sec.
    [3.2](#S3.SS2 "3.2 Template-Based Methods ‣ 3 Instance-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")）、投票基础方法（Sec.
    [3.3](#S3.SS3 "3.3 Voting-Based Methods ‣ 3 Instance-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")）和回归基础方法（Sec.
    [3.4](#S3.SS4 "3.4 Regression-Based Methods ‣ 3 Instance-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")）方法。'
- en: 3.1 Correspondence-Based Methods
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于对应的方法
- en: 'Correspondence-based object pose estimation refers to techniques that involve
    identifying correspondences between the input data and the given complete object
    CAD model. Correspondence-based methods can be divided into sparse and dense correspondences.
    Sparse correspondence-based methods (Sec. [3.1.1](#S3.SS1.SSS1 "3.1.1 Sparse Correspondence
    Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")) involve
    detecting object keypoints in the input image or point cloud to establish 2D-3D
    or 3D-3D correspondences between the input data and the object CAD model, followed
    by the utilization of the Perspective-n-Point (PnP) algorithm [[73](#bib.bib73)]
    or least square method to determine the object pose. Dense correspondence-based
    methods (Sec. [3.1.2](#S3.SS1.SSS2 "3.1.2 Dense Correspondence Methods ‣ 3.1 Correspondence-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) aim to establish dense 2D-3D or 3D-3D
    correspondences, ultimately leading to more accurate object pose estimation. For
    the RGB image, they leverage every pixel or multiple patches to generate pixel-wise
    correspondences, while for the point cloud, they use the entire point cloud to
    find point-wise correspondences. The illustration of these two types of methods
    is shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.1 Sparse Correspondence Methods ‣
    3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey"). The attributes
    and performance of some representative methods are shown in Table [I](#S3.T1 "TABLE
    I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3
    Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey").'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对应的物体姿态估计指的是涉及识别输入数据与给定完整物体CAD模型之间的对应关系的技术。基于对应的方法可以分为稀疏对应和密集对应。稀疏对应方法（见第[3.1.1节](#S3.SS1.SSS1
    "3.1.1 稀疏对应方法 ‣ 3.1 基于对应的方法 ‣ 3 实例级物体姿态估计 ‣ 基于深度学习的物体姿态估计：全面综述")）涉及在输入图像或点云中检测物体关键点，以建立输入数据与物体CAD模型之间的2D-3D或3D-3D对应关系，然后利用透视-n-点（PnP）算法[[73](#bib.bib73)]或最小二乘法来确定物体姿态。密集对应方法（见第[3.1.2节](#S3.SS1.SSS2
    "3.1.2 密集对应方法 ‣ 3.1 基于对应的方法 ‣ 3 实例级物体姿态估计 ‣ 基于深度学习的物体姿态估计：全面综述")）旨在建立密集的2D-3D或3D-3D对应关系，从而实现更准确的物体姿态估计。对于RGB图像，它们利用每个像素或多个补丁生成逐像素对应关系，而对于点云，它们使用整个点云来寻找逐点对应关系。这两种方法的示意图见图[4](#S3.F4
    "图4 ‣ 3.1.1 稀疏对应方法 ‣ 3.1 基于对应的方法 ‣ 3 实例级物体姿态估计 ‣ 基于深度学习的物体姿态估计：全面综述")。一些代表性方法的属性和性能显示在表[I](#S3.T1
    "表I ‣ 3.1.1 稀疏对应方法 ‣ 3.1 基于对应的方法 ‣ 3 实例级物体姿态估计 ‣ 基于深度学习的物体姿态估计：全面综述")。
- en: 3.1.1 Sparse Correspondence Methods
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 稀疏对应方法
- en: As a representative method, Rad *et al.*[[74](#bib.bib74)] first used a segmentation
    method to detect the object of interest in an RGB image. Then, they predicted
    the 2D projections of the object’s 3D bounding box corners. Finally, they used
    the PnP algorithm[[73](#bib.bib73)] to estimate the object pose. Additionally,
    they employed a classifier to determine the pose range in real-time, addressing
    the issue of ambiguity in symmetric objects. Tekin *et al.*[[75](#bib.bib75)]
    proposed a CNN network inspired by YOLO[[76](#bib.bib76)] to integrate object
    detection and pose estimation, directly predicting the locations of the projected
    vertices of the 3D object bounding box. Unlike [[74](#bib.bib74)] and [[75](#bib.bib75)],
    Pavlakos *et al.*[[77](#bib.bib77)] predicted the 2D projections of predefined
    semantic keypoints. Doosti *et al.*[[78](#bib.bib78)] introduced a compact model
    comprising two adaptive graph convolutional neural networks (GCNNs)[[79](#bib.bib79)],
    collaborating to estimate object and hand poses. To further enhance the robustness
    of object pose estimation, Song *et al.*[[80](#bib.bib80)] employed a hybrid intermediate
    representation to convey geometric details in the input image, encompassing keypoints,
    edge vectors, and symmetry correspondences. Liu *et al.*[[81](#bib.bib81)] proposed
    a multi-directional feature pyramid network along with a method that calculates
    object pose estimation confidence by incorporating spatial and plane information.
    Hu et al.[[82](#bib.bib82)] introduced a single-stage hierarchical end-to-end
    trainable network to address pose estimation challenges associated with scale
    variations in aerospace objects. In a recent development, Lian *et al.*[[83](#bib.bib83)]
    increased the number of predefined 3D keypoints to enhance the establishment of
    correspondences. Moreover, they devised a hierarchical binary encoding approach
    for localizing keypoints, enabling gradual refinement of correspondences and transforming
    correspondence regression into a more efficient classification task. To estimate
    transparent object pose, Chang *et al.*[[84](#bib.bib84)] used a 3D bounding box
    prediction network and multi-view geometry techniques. The method first detects
    2D projections of 3D bounding box vertices, and then reconstructs 3D points based
    on the multi-view detected 2D projections incorporating camera motion data. Additionally,
    they introduced a generalized pose definition to address pose ambiguity for symmetric
    objects. To enhance the efficiency of pose estimation networks, Guo *et al.*[[85](#bib.bib85)]
    integrated knowledge distillation into object pose estimation by distilling the
    teacher’s distribution of local predictions into the student network. Liu *et
    al.*[[86](#bib.bib86)] argued that differentiable PnP strategies conflict with
    the averaging nature of the PnP problem, resulting in gradients that may encourage
    the network to degrade the accuracy of individual correspondences. To mitigate
    this, they introduced a linear covariance loss, which can be used for both sparse
    and dense correspondence-based methods.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种代表性方法，Rad *et al.*[[74](#bib.bib74)] 首先使用了分割方法来检测 RGB 图像中的感兴趣对象。然后，他们预测了对象
    3D 边界框角点的 2D 投影。最后，他们使用 PnP 算法[[73](#bib.bib73)] 来估计对象姿态。此外，他们还采用了分类器实时确定姿态范围，解决了对称对象的模糊问题。Tekin
    *et al.*[[75](#bib.bib75)] 提出了一个受 YOLO[[76](#bib.bib76)] 启发的 CNN 网络，用于整合对象检测和姿态估计，直接预测
    3D 对象边界框的投影顶点位置。与 [[74](#bib.bib74)] 和 [[75](#bib.bib75)] 不同，Pavlakos *et al.*[[77](#bib.bib77)]
    预测了预定义语义关键点的 2D 投影。Doosti *et al.*[[78](#bib.bib78)] 介绍了一个由两个自适应图卷积神经网络（GCNNs）[[79](#bib.bib79)]
    组成的紧凑模型，这两个网络协作以估计对象和手部姿态。为了进一步增强对象姿态估计的鲁棒性，Song *et al.*[[80](#bib.bib80)] 采用了一种混合中间表示来传达输入图像中的几何细节，包括关键点、边缘向量和对称性对应。Liu
    *et al.*[[81](#bib.bib81)] 提出了一个多方向特征金字塔网络，并且通过结合空间和平面信息的方法来计算对象姿态估计的置信度。Hu et
    al.[[82](#bib.bib82)] 介绍了一个单阶段分层端到端可训练的网络，以解决航空航天对象在尺度变化下的姿态估计挑战。在最近的发展中，Lian
    *et al.*[[83](#bib.bib83)] 增加了预定义 3D 关键点的数量，以增强对应关系的建立。此外，他们设计了一种分层二进制编码方法来定位关键点，实现了对应关系的逐步细化，并将对应回归转化为更高效的分类任务。为了估计透明对象姿态，Chang
    *et al.*[[84](#bib.bib84)] 使用了 3D 边界框预测网络和多视角几何技术。该方法首先检测 3D 边界框顶点的 2D 投影，然后根据多视角检测到的
    2D 投影和相机运动数据重建 3D 点。此外，他们引入了一种通用的姿态定义来解决对称对象的姿态模糊问题。为了提高姿态估计网络的效率，Guo *et al.*[[85](#bib.bib85)]
    将知识蒸馏整合到对象姿态估计中，通过将教师网络的局部预测分布蒸馏到学生网络中。Liu *et al.*[[86](#bib.bib86)] 认为，可微分的
    PnP 策略与 PnP 问题的平均特性冲突，导致梯度可能促使网络降低单个对应的准确性。为了缓解这一问题，他们引入了一种线性协方差损失，可用于稀疏和密集对应方法。
- en: To mitigate vulnerability caused by large occlusions, Crivellaro *et al.*[[87](#bib.bib87)]
    used several control points to represent each object part. Then, they predicted
    the 2D projections of these control points to calculate the object pose. Some
    researchers solved the occlusion problem by predicting keypoints using small patches.
    Oberweger *et al.*[[88](#bib.bib88)] processed each patch separately to generate
    heatmaps and then aggregated the results to achieve precise and reliable predictions.
    Additionally, they offered a straightforward but efficient strategy to resolve
    ambiguities between patches and heatmaps during training. Hu *et al.*[[89](#bib.bib89)]
    unveiled a segmentation-driven pose estimation framework in which every visible
    object part offers a local pose prediction through 2D keypoint locations. Furthermore,
    Huang *et al.*[[90](#bib.bib90)] conceptualized 2D keypoint locations as probabilistic
    distributions within the loss function and designed a confidence-based network.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解由于大面积遮挡带来的脆弱性，Crivellaro *等*[[87](#bib.bib87)] 使用了多个控制点来表示每个物体部件。然后，他们预测了这些控制点的2D投影来计算物体姿态。一些研究人员通过使用小块预测关键点来解决遮挡问题。Oberweger
    *等*[[88](#bib.bib88)] 分别处理每个小块以生成热图，然后将结果汇总以实现精确可靠的预测。此外，他们提供了一种简单但有效的策略来解决训练过程中小块和热图之间的歧义。Hu
    *等*[[89](#bib.bib89)] 揭示了一种基于分割的姿态估计框架，其中每个可见物体部件通过2D关键点位置提供局部姿态预测。此外，Huang *等*[[90](#bib.bib90)]
    将2D关键点位置概念化为损失函数中的概率分布，并设计了基于置信度的网络。
- en: Reducing the reliance on annotated real-world data is also an important task.
    Some methods exploit geometric consistency as additional information to alleviate
    the need for annotation. Zhao *et al.*[[91](#bib.bib91)] employed image pairs
    with object annotations and relative transformation between viewpoints to automatically
    identify objects’ 3D keypoints that are geometrically and visually consistent.
    In addition, Yang *et al.*[[92](#bib.bib92)] used a keypoint consistency regularization
    for dual-scale images with a labeled 2D bounding box. Using semi-supervised learning,
    Liu *et al.*[[93](#bib.bib93)] developed a unified framework for estimating 3D
    hand and object poses. They constructed a joint learning framework that conducts
    explicit contextual reasoning between hand and object representations. To generate
    pseudo labels in semi-supervised learning, they utilized the spatial-temporal
    consistency found in large-scale hand-object videos as a constraint. Synthetic
    data is also a way to solve the annotation problem. Georgakis *et al.*[[94](#bib.bib94)]
    reduced the need for expensive 3DoF pose annotations by selecting keypoints and
    maintaining viewpoint and modality invariance in RGB images and CAD model renderings.
    Sock *et al.*[[95](#bib.bib95)] utilized self-supervision to minimize the gap
    between synthetic and real data and enforced photometric consistency across different
    object views to fine-tune the model. Further, Zhang *et al.*[[96](#bib.bib96)]
    utilized the invariance of geometry relations between keypoints across real and
    synthetic domains to accomplish domain adaptation. Thalhammer *et al.*[[97](#bib.bib97)]
    introduced a specialized feature pyramid network to compute multi-scale features,
    enabling the simultaneous generation of pose hypotheses across various feature
    map resolutions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 减少对注释的真实世界数据的依赖也是一项重要任务。一些方法利用几何一致性作为额外信息来减轻对注释的需求。Zhao *等*[[91](#bib.bib91)]
    使用带有物体注释的图像对以及视点之间的相对变换，自动识别在几何和视觉上都一致的物体3D关键点。此外，Yang *等*[[92](#bib.bib92)] 对具有标注2D边界框的双尺度图像使用了关键点一致性正则化。通过半监督学习，Liu
    *等*[[93](#bib.bib93)] 开发了一个统一的3D手部和物体姿态估计框架。他们构建了一个联合学习框架，在手部和物体表示之间进行明确的上下文推理。为了在半监督学习中生成伪标签，他们利用了大规模手部-物体视频中的时空一致性作为约束。合成数据也是解决注释问题的一种方式。Georgakis
    *等*[[94](#bib.bib94)] 通过选择关键点并保持RGB图像和CAD模型渲染中的视角和模态不变，减少了对昂贵的3DoF姿态注释的需求。Sock
    *等*[[95](#bib.bib95)] 利用自我监督来缩小合成数据和真实数据之间的差距，并在不同物体视图之间强制执行光度一致性以微调模型。此外，Zhang
    *等*[[96](#bib.bib96)] 利用真实和合成领域中关键点之间几何关系的不变性来实现领域适应。Thalhammer *等*[[97](#bib.bib97)]
    引入了一种专门的特征金字塔网络来计算多尺度特征，使得可以在各种特征图分辨率下同时生成姿态假设。
- en: Overall, the sparse correspondence-based methods can estimate object pose efficiently.
    However, relying on only a few control points can lead to sub-optimal accuracy.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，基于稀疏对应的方法可以有效地估计物体姿态。然而，仅依赖少量控制点可能导致次优的精度。
- en: '![Refer to caption](img/baba466aa3245ec421cb6de495238ea9.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/baba466aa3245ec421cb6de495238ea9.png)'
- en: 'Figure 4: Illustration of the correspondence-based (Sec. [3.1](#S3.SS1 "3.1
    Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey")), template-based
    (Sec. [3.2](#S3.SS2 "3.2 Template-Based Methods ‣ 3 Instance-Level Object Pose
    Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")),
    voting-based (Sec. [3.3](#S3.SS3 "3.3 Voting-Based Methods ‣ 3 Instance-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")), and regression-based (Sec. [3.4](#S3.SS4 "3.4 Regression-Based Methods
    ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")) instance-level methods. Correspondence-based methods
    (Sec. [3.1](#S3.SS1 "3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) involve establishing correspondences between input data and a provided
    object CAD model. Template-based methods (Sec. [3.2](#S3.SS2 "3.2 Template-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) involve identifying the most similar
    template from a set of templates labeled with ground-truth object poses. Voting-based
    methods (Sec. [3.3](#S3.SS3 "3.3 Voting-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) determine object pose through a pixel-level or point-level voting scheme.
    Regression-based methods (Sec. [3.4](#S3.SS4 "3.4 Regression-Based Methods ‣ 3
    Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")) aim to obtain the object pose directly from the learned
    features.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '图4：展示了基于对应关系（第[3.1](#S3.SS1 "3.1 Correspondence-Based Methods ‣ 3 Instance-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")节）、基于模板（第[3.2](#S3.SS2 "3.2 Template-Based Methods ‣ 3 Instance-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")节）、基于投票（第[3.3](#S3.SS3 "3.3 Voting-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")节）和基于回归（第[3.4](#S3.SS4 "3.4 Regression-Based Methods ‣ 3 Instance-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")节）的实例级方法。基于对应关系的方法（第[3.1](#S3.SS1 "3.1 Correspondence-Based Methods ‣
    3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")节）涉及在输入数据和提供的物体CAD模型之间建立对应关系。基于模板的方法（第[3.2](#S3.SS2 "3.2
    Template-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")节）涉及从标注有真实物体姿态的模板集中识别最相似的模板。基于投票的方法（第[3.3](#S3.SS3
    "3.3 Voting-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")节）通过像素级或点级投票方案确定物体姿态。基于回归的方法（第[3.4](#S3.SS4
    "3.4 Regression-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey")节）旨在直接从学习到的特征中获取物体姿态。'
- en: 'TABLE I: Representative instance-level methods. For each method, we report
    its 10 properties: published year, training input, inference input, pose DoF (3DoF,
    6DoF, and 9DoF), object property (rigid, articulated), task (estimation, tracking,
    and refinement), domain training paradigm (source domain, domain adaptation, and
    domain generalization), inference mode, application area, and its performance
    of key metrics on key datasets. Notably, for the input of training and inference,
    we only focus on the input of the pose estimation model, not the input of the
    front-end segmentation method (because it can be obtained through RGB as well
    as through depth or RGBD). D, S, C, T, V, P, and R denote object detection, instance
    segmentation, correspondence prediction, template matching, voting, pose solution/regression,
    and pose refinement, respectively. We report the average recall of ADD(S) within
    10$\%$ of the object diameter (termed ADD(S)-0.1d) of LM-O and LM datasets, and
    the AUC of ADD-S ($<$0.1m) of YCB-V dataset (Sec. [2](#S2 "2 Datasets and Metrics
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：代表性实例级方法。对于每种方法，我们报告其10个属性：发布年份、训练输入、推理输入、姿态自由度（3DoF、6DoF和9DoF）、对象属性（刚性、关节性）、任务（估计、跟踪和精细化）、领域训练范式（源领域、领域适应和领域泛化）、推理模式、应用领域及其在关键数据集上的主要指标性能。特别地，对于训练和推理的输入，我们只关注姿态估计模型的输入，而非前端分割方法的输入（因为它可以通过RGB或深度或RGBD获得）。D、S、C、T、V、P和R分别表示对象检测、实例分割、对应预测、模板匹配、投票、姿态解算/回归和姿态精细化。我们报告LM-O和LM数据集在对象直径的10$\%$内的ADD(S)平均召回率（称为ADD(S)-0.1d），以及YCB-V数据集的ADD-S
    ($<$0.1m)的AUC（见第[2](#S2 "2 数据集和指标 ‣ 基于深度学习的对象姿态估计：全面综述")节）。
- en: '| Methods |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |'
- en: '&#124; Published &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 发布年份 &#124;'
- en: '&#124; Year &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 年份 &#124;'
- en: '|'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Training &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 &#124;'
- en: '&#124; Input &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入 &#124;'
- en: '|'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Inference &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 推理 &#124;'
- en: '&#124; Input &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入 &#124;'
- en: '|'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Pose &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 姿态 &#124;'
- en: '&#124; DoF &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自由度 &#124;'
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Object &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对象 &#124;'
- en: '&#124; Property &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 属性 &#124;'
- en: '| Task |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 任务 |'
- en: '&#124; Domain Training &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 领域训练 &#124;'
- en: '&#124; Paradigm &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 范式 &#124;'
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Inference &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 推理 &#124;'
- en: '&#124; Mode &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模式 &#124;'
- en: '|'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Application &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 应用 &#124;'
- en: '&#124; Area &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 领域 &#124;'
- en: '|'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LM-O $&#124;$ LM &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LM-O $&#124;$ LM &#124;'
- en: '&#124; ADD(S)-0.1d &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ADD(S)-0.1d &#124;'
- en: '|'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; YCB-V &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; YCB-V &#124;'
- en: '&#124; ADD-S ($<$0.1m) &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ADD-S ($<$0.1m) &#124;'
- en: '|'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Correspondence-Based Methods | Sparse correspondence | Rad *et al.*[[74](#bib.bib74)]
    | 2017 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 基于对应的方法 | 稀疏对应 | Rad *等*[[74](#bib.bib74)] | 2017 |'
- en: '&#124; RGB, &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; four-stage, &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 四阶段， &#124;'
- en: '&#124; S+C+P+R &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+C+P+R &#124;'
- en: '| symmetrical objects | - | 62.7 | - |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 对称对象 | - | 62.7 | - |'
- en: '| Tekin *et al.*[[75](#bib.bib75)] | 2018 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Tekin *等*[[75](#bib.bib75)] | 2018 |'
- en: '&#124; RGB, &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; C+P &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; C+P &#124;'
- en: '| general | - | 56.0 | - |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 56.0 | - |'
- en: '| Hu *et al.*[[89](#bib.bib89)] | 2019 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Hu *等*[[89](#bib.bib89)] | 2019 |'
- en: '&#124; RGB, &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; C+P &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; C+P &#124;'
- en: '| occlusion | 27.0 | - | - |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 遮挡 | 27.0 | - | - |'
- en: '| Song *et al.*[[80](#bib.bib80)] | 2020 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Song *等*[[80](#bib.bib80)] | 2020 |'
- en: '&#124; RGB, &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; C+P+R &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; C+P+R &#124;'
- en: '|'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; symmetrical objects, &#124;'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对称对象， &#124;'
- en: '&#124; occlusion &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 遮挡 &#124;'
- en: '| 47.5 | 91.3 | - |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 47.5 | 91.3 | - |'
- en: '| Hu *et al.*[[82](#bib.bib82)] | 2021 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Hu *等*[[82](#bib.bib82)] | 2021 |'
- en: '&#124; RGB, &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; C+P &#124;'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; C+P &#124;'
- en: '| large scale variations | 48.6 | - | - |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 大规模变化 | 48.6 | - | - |'
- en: '| Chang *et al.*[[84](#bib.bib84)] | 2021 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Chang *等*[[84](#bib.bib84)] | 2021 |'
- en: '&#124; RGB, &#124;'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 固定 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; C+P &#124;'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; C+P &#124;'
- en: '|'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; transparent, &#124;'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 透明，&#124;'
- en: '&#124; symmetrical objects &#124;'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对称物体 &#124;'
- en: '| - | - | - |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| - | - | - |'
- en: '| Guo *et al.*[[85](#bib.bib85)] | 2023 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Guo *等*[[85](#bib.bib85)] | 2023 |'
- en: '&#124; RGB, &#124;'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 固定 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段，&#124;'
- en: '&#124; D+C+P &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+C+P &#124;'
- en: '| general | 44.5 | - | - |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 44.5 | - | - |'
- en: '| Dense correspondence | Li *et al.*[[19](#bib.bib19)] | 2019 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 密集对应 | Li *等*[[19](#bib.bib19)] | 2019 |'
- en: '&#124; RGB, &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 固定 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段，&#124;'
- en: '&#124; D+C+P &#124;'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+C+P &#124;'
- en: '| general | - | 89.9 | - |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 89.9 | - |'
- en: '| Hodan *et al.*[[98](#bib.bib98)] | 2020 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| Hodan *等*[[98](#bib.bib98)] | 2020 |'
- en: '&#124; RGB, &#124;'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 固定 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; C+P &#124;'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; C+P &#124;'
- en: '| symmetrical objects | - | - | - |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 对称物体 | - | - | - |'
- en: '| Shugurov *et al.*[[99](#bib.bib99)] | 2021 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Shugurov *等*[[99](#bib.bib99)] | 2021 |'
- en: '&#124; RGB/RGBD, &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB/RGBD，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB/RGBD, &#124;'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB/RGBD，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 固定 | 估计 | 来源 |'
- en: '&#124; four-stage, &#124;'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 四阶段，&#124;'
- en: '&#124; D+C+P+R &#124;'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+C+P+R &#124;'
- en: '| general | - | 99.9 | - |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 99.9 | - |'
- en: '| Chen *et al.*[[100](#bib.bib100)] | 2022 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| Chen *等*[[100](#bib.bib100)] | 2022 |'
- en: '&#124; RGB, &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 固定 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段，&#124;'
- en: '&#124; D+C+P &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+C+P &#124;'
- en: '| general | - | 95.8 | - |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 95.8 | - |'
- en: '| Haugaard *et al.*[[101](#bib.bib101)] | 2022 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| Haugaard *等*[[101](#bib.bib101)] | 2022 |'
- en: '&#124; RGB/RGBD, &#124;'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB/RGBD，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB/RGBD, &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB/RGBD，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | generalization |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 固定 | 估计 | 泛化 |'
- en: '&#124; four-stage, &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 四阶段，&#124;'
- en: '&#124; D+C+P+R &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+C+P+R &#124;'
- en: '| general | - | - | - |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | - | - |'
- en: '| Li *et al.*[[102](#bib.bib102)] | 2023 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| Li *等*[[102](#bib.bib102)] | 2023 |'
- en: '&#124; RGB, &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 固定 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段，&#124;'
- en: '&#124; D+C+P &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+C+P &#124;'
- en: '| general | 51.4 | 97.8 | - |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 51.4 | 97.8 | - |'
- en: '| Xu *et al.*[[103](#bib.bib103)] | 2024 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| Xu *等*[[103](#bib.bib103)] | 2024 |'
- en: '&#124; RGB, &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | refinement | source |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 固定 | 精细化 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; P+R &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; P+R &#124;'
- en: '| occlusion | 60.7 | 97.4 | 85.7 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 遮挡 | 60.7 | 97.4 | 85.7 |'
- en: '| Template-Based Methods | RGB-based | Sundermeyer *et al.*[[104](#bib.bib104)]
    | 2018 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 基于模板的方法 | 基于 RGB | Sundermeyer *等*[[104](#bib.bib104)] | 2018 |'
- en: '&#124; RGB, &#124;'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | generalization |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 固定 | 估计 | 泛化 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段，&#124;'
- en: '&#124; D+T+P &#124;'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+T+P &#124;'
- en: '| general | - | 31.4 | - |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 31.4 | - |'
- en: '| Papaioannidis *et al.*[[105](#bib.bib105)] | 2020 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| Papaioannidis *等*[[105](#bib.bib105)] | 2020 |'
- en: '&#124; RGB, &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 3DoF | rigid | estimation | source |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 3DoF | 固定 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; D+T &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+T &#124;'
- en: '| general | - | - | - |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | - | - |'
- en: '| Li *et al.*[[106](#bib.bib106)] | 2020 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| Li *等*[[106](#bib.bib106)] | 2020 |'
- en: '&#124; RGB, &#124;'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段, &#124;'
- en: '&#124; D+T+R &#124;'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+T+R &#124;'
- en: '| general | - | 88.6 | - |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 88.6 | - |'
- en: '| Deng *et al.*[[107](#bib.bib107)] | 2021 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| Deng *等*[[107](#bib.bib107)] | 2021 |'
- en: '&#124; RGB, &#124;'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 6DoF | rigid | tracking | generalization |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 跟踪 | 泛化 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段, &#124;'
- en: '&#124; D+T &#124;'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+T &#124;'
- en: '| symmetrical objects | - | - | - |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 对称物体 | - | - | - |'
- en: '| Point cloud | Li *et al.*[[70](#bib.bib70)] | 2022 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 点云 | Li *等*[[70](#bib.bib70)] | 2022 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGBD, &#124;'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段, &#124;'
- en: '&#124; S+P+R &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P+R &#124;'
- en: '| general | 70.6 | 99.5 | 96.6 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 70.6 | 99.5 | 96.6 |'
- en: '| Jiang *et al.*[[108](#bib.bib108)] | 2023 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| Jiang *等*[[108](#bib.bib108)] | 2023 |'
- en: '&#124; Depth, &#124;'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Depth, &#124;'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段, &#124;'
- en: '&#124; S+P &#124;'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | - | - | - |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | - | - |'
- en: '| Dang *et al.*[[109](#bib.bib109)] | 2024 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| Dang *等*[[109](#bib.bib109)] | 2024 |'
- en: '&#124; Depth, &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Depth, &#124;'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段, &#124;'
- en: '&#124; S+P &#124;'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | 52.0 | 69.0 | - |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 52.0 | 69.0 | - |'
- en: '| Voting-Based Methods | Indirect voting | Peng *et al.*[[17](#bib.bib17)]
    | 2019 | RGB | RGB | 6DoF | rigid | estimation | source |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 基于投票的方法 | 间接投票 | Peng *等*[[17](#bib.bib17)] | 2019 | RGB | RGB | 6DoF | 刚性
    | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段, &#124;'
- en: '&#124; V+P &#124;'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; V+P &#124;'
- en: '| occlusion | 40.8 | 86.3 | - |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 遮挡 | 40.8 | 86.3 | - |'
- en: '| He *et al.*[[18](#bib.bib18)] | 2020 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| He *等*[[18](#bib.bib18)] | 2020 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGBD, &#124;'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段, &#124;'
- en: '&#124; S+V+P &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+V+P &#124;'
- en: '| general | - | 99.4 | 95.5 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 99.4 | 95.5 |'
- en: '| He *et al.*[[21](#bib.bib21)] | 2021 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| He *等*[[21](#bib.bib21)] | 2021 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGBD, &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段, &#124;'
- en: '&#124; S+V+P &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+V+P &#124;'
- en: '| general | 66.2 | 99.7 | 96.6 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 66.2 | 99.7 | 96.6 |'
- en: '| Cao *et al.*[[110](#bib.bib110)] | 2022 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| Cao *等*[[110](#bib.bib110)] | 2022 |'
- en: '&#124; RGB, &#124;'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| RGB | 6DoF | rigid | estimation | source | end to end | general | 58.7 |
    - | 90.9 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| RGB | 6DoF | 刚性 | 估计 | 来源 | 端到端 | 一般 | 58.7 | - | 90.9 |'
- en: '| Wu *et al.*[[111](#bib.bib111)] | 2022 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| Wu *等*[[111](#bib.bib111)] | 2022 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGBD, &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段, &#124;'
- en: '&#124; S+V+P &#124;'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+V+P &#124;'
- en: '| general | 70.2 | 99.4 | 96.6 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 70.2 | 99.4 | 96.6 |'
- en: '| Zhou *et al.*[[112](#bib.bib112)] | 2023 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| Zhou *等*[[112](#bib.bib112)] | 2023 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGBD, &#124;'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段, &#124;'
- en: '&#124; S+V+P &#124;'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+V+P &#124;'
- en: '| general | 77.7 | 99.8 | 96.7 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 77.7 | 99.8 | 96.7 |'
- en: '| Direct voting | Wang *et al.*[[16](#bib.bib16)] | 2019 | RGBD | RGBD | 6DoF
    | rigid | estimation | source |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 直接投票 | Wang *等*[[16](#bib.bib16)] | 2019 | RGBD | RGBD | 6DoF | 刚性 | 估计 |
    来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段, &#124;'
- en: '&#124; P+R &#124;'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; P+R &#124;'
- en: '| general | - | 94.3 | 93.1 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 94.3 | 93.1 |'
- en: '| Tian *et al.*[[113](#bib.bib113)] | 2020 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| Tian *等*[[113](#bib.bib113)] | 2020 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| RGBD | 6DoF | rigid | estimation | source |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段, &#124;'
- en: '&#124; S+V+P &#124;'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+V+P &#124;'
- en: '| general | - | 92.9 | 91.8 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 92.9 | 91.8 |'
- en: '| Zhou *et al.*[[114](#bib.bib114)] | 2021 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| Zhou *等*[[114](#bib.bib114)] | 2021 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| RGBD | 6DoF | rigid | estimation | source |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段, &#124;'
- en: '&#124; S+P &#124;'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | 65.0 | 99.6 | 95.8 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 通用 | 65.0 | 99.6 | 95.8 |'
- en: '| Mo *et al.*[[115](#bib.bib115)] | 2022 | RGBD | RGBD | 6DoF | rigid | estimation
    | source |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| Mo *等*[[115](#bib.bib115)] | 2022 | RGBD | RGBD | 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段, &#124;'
- en: '&#124; S+P &#124;'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | - | - | 93.6 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 通用 | - | - | 93.6 |'
- en: '| Hong *et al.*[[116](#bib.bib116)] | 2024 | RGBD | RGBD | 6DoF | rigid | estimation
    | source |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| Hong *等*[[116](#bib.bib116)] | 2024 | RGBD | RGBD | 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段, &#124;'
- en: '&#124; S+P &#124;'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | 71.1 | 96.7 | 92.7 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| 通用 | 71.1 | 96.7 | 92.7 |'
- en: '| Regression-Based Methods | Geometry-guided | Chen *et al.*[[117](#bib.bib117)]
    | 2020 | RGBD | RGBD | 6DoF | rigid | estimation | source | end to end | general
    | - | 98.7 | 92.4 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 基于回归的方法 | 几何引导 | Chen *等*[[117](#bib.bib117)] | 2020 | RGBD | RGBD | 6DoF
    | 刚性 | 估计 | 来源 | 端到端 | 通用 | - | 98.7 | 92.4 |'
- en: '| Hu *et al.*[[118](#bib.bib118)] | 2020 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| Hu *等*[[118](#bib.bib118)] | 2020 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| RGB | 6DoF | rigid | estimation | source | end to end | general | 43.3 |
    - | - |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| RGB | 6DoF | 刚性 | 估计 | 来源 | 端到端 | 通用 | 43.3 | - | - |'
- en: '| Labbé *et al.*[[119](#bib.bib119)] | 2020 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| Labbé *等*[[119](#bib.bib119)] | 2020 |'
- en: '&#124; RGB, &#124;'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 6DoF | rigid | estimation | source |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段, &#124;'
- en: '&#124; D+P+R &#124;'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+P+R &#124;'
- en: '| general | - | - | 93.4 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| 通用 | - | - | 93.4 |'
- en: '| Wang *et al.*[[120](#bib.bib120)] | 2021 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| Wang *等*[[120](#bib.bib120)] | 2021 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| RGB | 6DoF | rigid | estimation | source |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| RGB | 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段, &#124;'
- en: '&#124; D+P &#124;'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+P &#124;'
- en: '| general | 62.2 | - | 91.6 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| 通用 | 62.2 | - | 91.6 |'
- en: '| Di *et al.*[[121](#bib.bib121)] | 2021 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| Di *等*[[121](#bib.bib121)] | 2021 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| RGB | 6DoF | rigid | estimation | source |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| RGB | 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段, &#124;'
- en: '&#124; D+P &#124;'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+P &#124;'
- en: '| occlusion | 62.32 | 96.0 | 90.9 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| 遮挡 | 62.32 | 96.0 | 90.9 |'
- en: '| Wang *et al.*[[122](#bib.bib122)] | 2021 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| Wang *等*[[122](#bib.bib122)] | 2021 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| RGB | 6DoF | rigid | estimation | adaptation |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| RGB | 6DoF | 刚性 | 估计 | 适应 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段, &#124;'
- en: '&#124; D+P &#124;'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+P &#124;'
- en: '| occlusion | 59.8 | 85.6 | 90.5 |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 遮挡 | 59.8 | 85.6 | 90.5 |'
- en: '| Direct regression | Xiang *et al.*[[15](#bib.bib15)] | 2017 | RGB | RGB |
    6DoF | rigid | estimation | source |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| 直接回归 | Xiang *等*[[15](#bib.bib15)] | 2017 | RGB | RGB | 6DoF | 刚性 | 估计 |
    来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段, &#124;'
- en: '&#124; S+V+P &#124;'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+V+P &#124;'
- en: '| cluttered | 24.9 | - | 75.9 |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| 杂乱 | 24.9 | - | 75.9 |'
- en: '| Li *et al.*[[123](#bib.bib123)] | 2018 | RGBD | RGBD | 6DoF | rigid | estimation
    | source |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| Li *等*[[123](#bib.bib123)] | 2018 | RGBD | RGBD | 6DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段, &#124;'
- en: '&#124; P+R &#124;'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; P+R &#124;'
- en: '| general | - | - | 94.3 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| 通用 | - | - | 94.3 |'
- en: '| Li *et al.*[[124](#bib.bib124)] | 2018 |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| Li *等*[[124](#bib.bib124)] | 2018 |'
- en: '&#124; RGB, &#124;'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 6DoF | rigid |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 |'
- en: '&#124; refinement, &#124;'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精细化, &#124;'
- en: '&#124; tracking &#124;'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跟踪 &#124;'
- en: '| source | end to end | general | 55.5 | 88.6 | 81.9 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| 来源 | 端到端 | 通用 | 55.5 | 88.6 | 81.9 |'
- en: '| Manhardt *et al.*[[125](#bib.bib125)] | 2018 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| Manhardt *等*[[125](#bib.bib125)] | 2018 |'
- en: '&#124; RGB, &#124;'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 6DoF | rigid |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚性 |'
- en: '&#124; refinement, &#124;'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精细化, &#124;'
- en: '&#124; tracking &#124;'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跟踪 &#124;'
- en: '| generaliztion | end to end | general | - | - | - |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| 泛化 | 端到端 | 通用 | - | - | - |'
- en: '| Manhardt *et al.*[[126](#bib.bib126)] | 2019 | RGB | RGB | 6DoF | rigid |
    estimation | source | end to end | symmetrical objects | - | - | - |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| Manhardt *等*[[126](#bib.bib126)] | 2019 | RGB | RGB | 6DoF | 刚性 | 估计 | 来源
    | 端到端 | 对称物体 | - | - | - |'
- en: '| Papaioannidis *et al.*[[127](#bib.bib127)] | 2019 | RGB | RGB | 3DoF | rigid
    | estimation | source |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| Papaioannidis *等*[[127](#bib.bib127)] | 2019 | RGB | RGB | 3DoF | 刚性 | 估计
    | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段, &#124;'
- en: '&#124; D+P &#124;'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+P &#124;'
- en: '| general | - | - | - |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| 通用 | - | - | - |'
- en: '| Liu *et al.*[[128](#bib.bib128)] | 2019 | RGB | RGB | 3DoF | rigid | estimation
    | source |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| Liu *等*[[128](#bib.bib128)] | 2019 | RGB | RGB | 3DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段, &#124;'
- en: '&#124; D+P &#124;'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+P &#124;'
- en: '| texture-less | - | - | - |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| texture-less | - | - | - |'
- en: '| Wen *et al.*[[48](#bib.bib48)] | 2020 |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| Wen *等人*[[48](#bib.bib48)] | 2020 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD Model &#124;'
- en: '| RGBD | 6DoF | rigid | tracking | generaliztion | end to end | general | -
    | - | 93.9 |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 6DoF | rigid | tracking | generaliztion | end to end | general | -
    | - | 93.9 |'
- en: '| Wang *et al.*[[68](#bib.bib68)] | 2020 |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| Wang *等人*[[68](#bib.bib68)] | 2020 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD Model &#124;'
- en: '| RGBD | 6DoF | rigid | estimation | generaliztion | end to end | general |
    32.1 | 58.9 | - |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 6DoF | rigid | estimation | generaliztion | end to end | general |
    32.1 | 58.9 | - |'
- en: '| Jiang *et al.*[[69](#bib.bib69)] | 2022 | RGBD | RGBD | 6DoF | rigid | estimation
    | source | end to end | general | 30.8 | 97.0 | 95.2 |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| Jiang *等人*[[69](#bib.bib69)] | 2022 | RGBD | RGBD | 6DoF | rigid | estimation
    | source | end to end | general | 30.8 | 97.0 | 95.2 |'
- en: '| Hai *et al.*[[129](#bib.bib129)] | 2023 |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| Hai *等人*[[129](#bib.bib129)] | 2023 |'
- en: '&#124; RGB, &#124;'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD Model &#124;'
- en: '| RGB | 6DoF | rigid | refinement | source | end to end | general | 66.4 |
    99.3 | - |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| RGB | 6DoF | rigid | refinement | source | end to end | general | 66.4 |
    99.3 | - |'
- en: '| Li *et al.*[[130](#bib.bib130)] | 2024 |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| Li *等人*[[130](#bib.bib130)] | 2024 |'
- en: '&#124; RGB, &#124;'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD Model &#124;'
- en: '|'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD Model &#124;'
- en: '| 6DoF | rigid | refinement | source |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | rigid | refinement | source |'
- en: '&#124; two-stage, &#124;'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; two-stage, &#124;'
- en: '&#124; S+P &#124;'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | - | - | 97.0 |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| general | - | - | 97.0 |'
- en: 3.1.2 Dense Correspondence Methods
  id: totrans-544
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 密集对应方法
- en: Dense correspondence-based methods utilize a significantly larger number of
    correspondences compared to sparse correspondence-based methods. This enables
    them to achieve higher accuracy and handle occlusions more effectively. Li *et
    al.*[[19](#bib.bib19)] argued for the differentiation between rotation and translation,
    proposing the coordinates-based disentangled pose network. This network separates
    pose estimation into distinct predictions for rotation and translation. Zakharov
    *et al.*[[20](#bib.bib20)] introduced the dense multi-class 2D-3D correspondence-based
    object pose detector and a tailored deep learning-based refinement process. In
    addition, Cai *et al.*[[131](#bib.bib131)] proposed a technique to automatically
    identify and match image landmarks consistently across different views, aiming
    to enhance the process of learning 2D-3D mapping. Wang *et al.*[[132](#bib.bib132)]
    developed a pose estimation pipeline guided by reconstruction, capitalizing on
    geometric consistency. Further, Shugurov *et al.*[[99](#bib.bib99)] built upon
    Zakharov *et al.*[[20](#bib.bib20)] by developing a unified deep network capable
    of accommodating multiple image modalities (such as RGB and Depth) and integrating
    a differentiable rendering-based pose refinement method. Su *et al.*[[133](#bib.bib133)]
    introduced a discrete descriptor realized by hierarchical binary grouping, capable
    of densely representing the object surface. As a result, this method can predict
    fine-grained correspondences. Chen *et al.*[[100](#bib.bib100)] introduced a probabilistic
    PnP[[73](#bib.bib73)] layer designed for general end-to-end pose estimation. This
    layer generates a pose distribution on the SE(3) manifold. On the other hand,
    Xu *et al.*[[134](#bib.bib134)] argued that encoding pose-sensitive local features
    and modeling the statistical distribution of inlier poses are crucial for accurate
    and robust 6DoF pose estimation. Inspired by PPF[[11](#bib.bib11)], they exploited
    pose-sensitive information carried by each pair of oriented points and an ensemble
    of redundant pose predictions to achieve robust performance on severe inter-object
    occlusion and systematic noises in scene point clouds.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 基于稠密对应的方法相比于基于稀疏对应的方法利用了显著更多的对应点。这使得它们能够实现更高的准确度，并更有效地处理遮挡问题。李*等人*[[19](#bib.bib19)]主张区分旋转和位移，提出了基于坐标的解耦姿态网络。该网络将姿态估计分解为对旋转和位移的不同预测。扎哈罗夫*等人*[[20](#bib.bib20)]引入了基于稠密多类2D-3D对应的物体姿态检测器以及一个定制的深度学习修正过程。此外，蔡*等人*[[131](#bib.bib131)]提出了一种技术，能够在不同视角下自动识别和匹配图像标志，旨在提升2D-3D映射的学习过程。王*等人*[[132](#bib.bib132)]开发了一个以重建为指导的姿态估计流程，利用几何一致性。此外，舒古罗夫*等人*[[99](#bib.bib99)]在扎哈罗夫*等人*[[20](#bib.bib20)]的基础上，开发了一个统一的深度网络，能够处理多种图像模态（如RGB和深度），并集成了基于可微渲染的姿态修正方法。苏*等人*[[133](#bib.bib133)]引入了一种通过层次化二进制分组实现的离散描述符，能够密集表示物体表面。因此，该方法可以预测细粒度的对应点。陈*等人*[[100](#bib.bib100)]引入了一种为通用端到端姿态估计设计的概率PnP[[73](#bib.bib73)]层。该层在SE(3)流形上生成姿态分布。另一方面，徐*等人*[[134](#bib.bib134)]认为，编码姿态敏感的局部特征和建模内点姿态的统计分布对于准确和稳健的6DoF姿态估计至关重要。受PPF[[11](#bib.bib11)]的启发，他们利用了每对定向点所携带的姿态敏感信息和冗余姿态预测的集合，以在严重的物体间遮挡和场景点云的系统噪声中实现稳健的性能。
- en: Some methods recover object poses by establishing 3D-3D correspondences. Huang
    *et al.*[[135](#bib.bib135)] used an RGB image to predict 3D object coordinates
    in the camera frustum, thus establishing 3D-3D correspondences. Further, Jiang
    *et al.*[[136](#bib.bib136)] introduced a center-based decoupled framework, leveraging
    bird’s eye and front views for object center voting. They utilized feature similarity
    between the center-aligned object and the object CAD model to establish correspondences
    for Singular Value Decomposition (SVD)-based[[137](#bib.bib137)] rotation estimation.
    More recently, Lin *et al.*[[138](#bib.bib138)] utilized an RGBD image as input
    and employed point-to-surface matching to estimate the object surface correspondence.
    They establish 3D-3D correspondences by iteratively constricting the surface,
    transitioning it into a correspondence point while progressively eliminating outliers.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法通过建立 3D-3D 对应关系来恢复物体姿态。Huang *et al.*[[135](#bib.bib135)] 使用 RGB 图像预测相机视锥中的
    3D 物体坐标，从而建立 3D-3D 对应关系。此外，Jiang *et al.*[[136](#bib.bib136)] 引入了一个基于中心的解耦框架，利用鸟瞰图和正视图进行物体中心投票。他们利用中心对齐物体与物体CAD模型之间的特征相似性，建立了用于基于奇异值分解
    (SVD)[[137](#bib.bib137)] 旋转估计的对应关系。最近，Lin *et al.*[[138](#bib.bib138)] 采用RGBD图像作为输入，并使用点对表面匹配来估计物体表面对应关系。他们通过迭代约束表面，将其转化为对应点，同时逐步消除异常值，从而建立
    3D-3D 对应关系。
- en: 'Some methods put more effort into handling challenging cases, such as symmetric
    objects[[139](#bib.bib139), [98](#bib.bib98), [140](#bib.bib140)] and texture-less
    objects [[141](#bib.bib141)]. Park *et al.*[[139](#bib.bib139)] utilized generative
    adversarial training to reconstruct occluded parts to alleviate the impact of
    occlusion. They handle symmetric objects by guiding predictions towards the nearest
    symmetric pose. In addition, Hodan *et al.*[[98](#bib.bib98)] modeled an object
    using compact surface fragments to handle symmetries in object modeling effectively.
    For each pixel, the network predicts: the likelihood of each object’s presence,
    the probability of the fragments conditional on the object’s presence, and the
    exact 3D translation of each fragment. Finally, the object pose is determined
    using a robust and efficient version of the PnP-RANSAC algorithm [[73](#bib.bib73)].
    Further, Wu *et al.*[[140](#bib.bib140)] employed a geometric-aware dense matching
    network to acquire visible dense correspondences. Additionally, they utilized
    the distance consistency of these correspondences to mitigate ambiguity in symmetrical
    objects. For texture-less objects, Wu *et al.*[[141](#bib.bib141)] leveraged information
    from the object CAD model and established 2D-3D correspondences using a pseudo-Siamese
    neural network.'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法更加注重处理挑战性较大的情况，例如对称物体[[139](#bib.bib139), [98](#bib.bib98), [140](#bib.bib140)]和无纹理物体[[141](#bib.bib141)]。Park
    *et al.*[[139](#bib.bib139)] 利用生成对抗训练重建遮挡部分，以减轻遮挡的影响。他们通过引导预测接近对称姿态来处理对称物体。此外，Hodan
    *et al.*[[98](#bib.bib98)] 使用紧凑的表面碎片对物体进行建模，以有效处理物体建模中的对称性。对于每个像素，网络预测：每个物体存在的可能性，碎片在物体存在条件下的概率，以及每个碎片的准确
    3D 平移。最后，物体姿态使用一个强大而高效的PnP-RANSAC算法[[73](#bib.bib73)]来确定。此外，Wu *et al.*[[140](#bib.bib140)]
    采用了几何感知的稠密匹配网络来获取可见的稠密对应关系。此外，他们利用这些对应关系的距离一致性来减少对称物体中的歧义。对于无纹理物体，Wu *et al.*[[141](#bib.bib141)]
    利用物体CAD模型的信息，并通过伪Siamese神经网络建立2D-3D对应关系。
- en: With research development, domain adaptation, weak supervision, and self-supervision
    techniques have been introduced into pose estimation. Li *et al.*[[142](#bib.bib142)]
    noticed that images with varying levels of realism and semantics exhibit different
    transferability between synthetic and real domains. Consequently, they decomposed
    the input image into multi-level semantic representations and merged the strengths
    of these representations to mitigate the domain gap. Further, Hu *et al.*[[143](#bib.bib143)]
    introduced a method exclusively trained on synthetic images, which infers the
    necessary pose correction for refining rough poses. Haugaard *et al.*[[101](#bib.bib101)]
    utilized learned distributions to sample, score, and refine pose hypotheses. Correspondence
    distributions are learned using a contrastive loss. This method is unsupervised
    regarding visual ambiguities. More recently, Li *et al.*[[102](#bib.bib102)] introduced
    a weakly-supervised reconstruction-based pipeline. Initially, they reconstructed
    the objects from various viewpoints using an implicit neural representation. Subsequently,
    they trained a network to predict pixel-wise 2D-3D correspondences. Hai *et al.*[[144](#bib.bib144)]
    proposed a refinement strategy that uses the geometry constraint in synthetic-to-real
    image pairs captured from multiple viewpoints.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 随着研究的发展，领域适应、弱监督和自监督技术被引入姿态估计。Li *等*[[142](#bib.bib142)] 注意到具有不同现实主义和语义水平的图像在合成和真实领域之间展现出不同的可迁移性。因此，他们将输入图像分解为多级语义表示，并融合这些表示的优势以减少领域差距。此外，Hu
    *等*[[143](#bib.bib143)] 引入了一种专门在合成图像上训练的方法，用于推断必要的姿态修正，以优化粗略姿态。Haugaard *等*[[101](#bib.bib101)]
    利用学习到的分布来采样、评分和优化姿态假设。对应分布是通过对比损失学习的。该方法在视觉模糊方面是无监督的。最近，Li *等*[[102](#bib.bib102)]
    引入了一种弱监督的基于重建的管道。他们首先使用隐式神经表示从不同视角重建对象。随后，他们训练了一个网络来预测逐像素的2D-3D对应关系。Hai *等*[[144](#bib.bib144)]
    提出了一个利用在多个视角捕获的合成到真实图像对中的几何约束的优化策略。
- en: There are also methods that focus on pose refinement. Lipson *et al.*[[145](#bib.bib145)]
    iteratively refined pose and correspondences in a tightly coupled manner. They
    incorporated a differentiable layer to refine the pose by solving the bidirectional
    depth-augmented PnP problem. In addition, Xu *et al.*[[103](#bib.bib103)] formulated
    object pose refinement as a non-linear least squares problem using the estimated
    correspondence field, *i.e.*, the correspondence between the RGB image and the
    rendered image using the initial pose. The non-linear least squares problem is
    then solved by a differentiable levenberg-marquardt algorithm [[146](#bib.bib146)],
    enabling end-to-end training.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些方法专注于姿态优化。Lipson *等*[[145](#bib.bib145)] 以紧密耦合的方式迭代优化姿态和对应关系。他们引入了一个可微分层，通过解决双向深度增强的PnP问题来优化姿态。此外，Xu
    *等*[[103](#bib.bib103)] 将对象姿态优化形式化为一个非线性最小二乘问题，利用估计的对应场，即RGB图像与使用初始姿态渲染的图像之间的对应关系。非线性最小二乘问题随后通过可微分的Levenberg-Marquardt算法[[146](#bib.bib146)]
    解决，从而实现了端到端训练。
- en: In general, the aforementioned correspondence-based methods exhibit robustness
    to occlusion since they can utilize local correspondences to predict object pose.
    However, these methods may encounter challenges when handling objects that lack
    salient shape features or texture.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，前述基于对应关系的方法在遮挡情况下表现出鲁棒性，因为它们可以利用局部对应关系来预测对象姿态。然而，这些方法在处理缺乏显著形状特征或纹理的对象时可能会遇到挑战。
- en: 3.2 Template-Based Methods
  id: totrans-551
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于模板的方法
- en: 'By leveraging global information from the image, template-based methods can
    effectively address the challenges posed by texture-less objects. Template-based
    methods involve identifying the most similar template from a set of templates
    labeled with ground-truth object poses. They can be categorized into RGB-based
    template (Sec. [3.2.1](#S3.SS2.SSS1 "3.2.1 RGB-Based Template Methods ‣ 3.2 Template-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) and point cloud-based template (Sec.
    [3.2.2](#S3.SS2.SSS2 "3.2.2 Point Cloud-Based Template Methods ‣ 3.2 Template-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) methods. These two methods are illustrated
    in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey"). When the input is an RGB image, the
    templates comprise 2D projections extracted from object CAD models, with annotations
    of ground-truth poses. This process transforms object pose estimation into image
    retrieval. Conversely, when dealing with a point cloud, the template comprises
    the object CAD model with the canonical pose. Notably, we classify the methods
    that directly regress the relative pose between the object CAD model and the observed
    point cloud as template-based methods. This is because these methods can be interpreted
    as seeking the optimal relative pose that aligns the observed point cloud with
    the template. Consequently, the determined relative pose serves as the object
    pose. The characteristics and performance of some representative methods are shown
    in Table [I](#S3.T1 "TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey").'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '通过利用图像的全局信息，基于模板的方法可以有效应对无纹理物体带来的挑战。基于模板的方法涉及从标记有真实物体姿态的模板集合中识别最相似的模板。这些方法可以分为基于RGB的模板（参见[3.2.1](#S3.SS2.SSS1
    "3.2.1 RGB-Based Template Methods ‣ 3.2 Template-Based Methods ‣ 3 Instance-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")）和基于点云的模板（参见[3.2.2](#S3.SS2.SSS2 "3.2.2 Point Cloud-Based Template Methods
    ‣ 3.2 Template-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey")）方法。这两种方法在图[4](#S3.F4
    "Figure 4 ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods
    ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")中进行了说明。当输入是RGB图像时，模板由从物体CAD模型中提取的2D投影组成，并带有真实姿态的注释。这个过程将物体姿态估计转化为图像检索。相反，当处理点云时，模板由具有规范姿态的物体CAD模型组成。值得注意的是，我们将那些直接回归物体CAD模型与观察到的点云之间相对姿态的方法归类为基于模板的方法。这是因为这些方法可以解释为寻找最佳的相对姿态，使观察到的点云与模板对齐。因此，确定的相对姿态即为物体姿态。一些代表性方法的特点和性能见表[I](#S3.T1
    "TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods
    ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")。'
- en: 3.2.1 RGB-Based Template Methods
  id: totrans-553
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 基于RGB的模板方法
- en: As a seminal contribution, Sundermeyer *et al.*[[104](#bib.bib104)] achieved
    3D rotation estimation through a variant of denoising autoencoder, which learns
    an implicit representation of object rotation. If depth is available, it can be
    used for pose refinement. Liu *et al.*[[147](#bib.bib147)] developed a CNN akin
    to an autoencoder to reconstruct arbitrary scenes featuring the target object
    and extract the object area. In addition, Zhang *et al.*[[148](#bib.bib148)] utilized
    an object detector and a keypoint extractor to simplify the template search process.
    Papaioannidis *et al.*[[105](#bib.bib105)] suggested that estimating object poses
    in synthetic images is more straightforward. Therefore, they employed a generative
    adversarial network to convert real images into synthetic ones while preserving
    the object pose. Li *et al.*[[106](#bib.bib106)] utilized a new pose representation
    (*i.e.*, 3D location field) to guide an auto-encoder to distill pose-related features,
    thereby enhancing the handling of pose ambiguity. Stevšič *et al.*[[149](#bib.bib149)]
    proposed a spatial attention mechanism to identify and utilize spatial details
    for pose refinement. Different from the above methods, Deng *et al.*[[107](#bib.bib107)]
    addressed the 6DoF object pose tracking problem within the Rao-Blackwellized particle
    filtering [[150](#bib.bib150)] framework. They finely discretized the rotation
    space and trained an autoencoder network to build a codebook of feature embeddings
    for these discretized rotations. This method efficiently estimates the 3D translation
    along with the full distribution over the 3D rotation.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项开创性贡献，Sundermeyer *等人*[[104](#bib.bib104)] 通过一种去噪自编码器的变体实现了3D旋转估计，该变体学习了物体旋转的隐式表示。如果深度信息可用，则可以用于姿态优化。Liu
    *等人*[[147](#bib.bib147)] 开发了一种类似于自编码器的CNN，用于重建包含目标物体的任意场景并提取物体区域。此外，Zhang *等人*[[148](#bib.bib148)]
    利用物体检测器和关键点提取器来简化模板搜索过程。Papaioannidis *等人*[[105](#bib.bib105)] 认为在合成图像中估计物体姿态更为直接。因此，他们使用了生成对抗网络将真实图像转换为合成图像，同时保持物体姿态。Li
    *等人*[[106](#bib.bib106)] 利用了一种新的姿态表示（*即*，3D位置场）来指导自编码器提取姿态相关特征，从而提高对姿态歧义的处理能力。Stevšič
    *等人*[[149](#bib.bib149)] 提出了一个空间注意机制，用于识别和利用空间细节以进行姿态优化。与上述方法不同，Deng *等人*[[107](#bib.bib107)]
    在Rao-Blackwellized粒子滤波[[150](#bib.bib150)]框架内解决了6DoF物体姿态跟踪问题。他们对旋转空间进行了精细离散，并训练了一个自编码器网络来建立这些离散旋转的特征嵌入代码本。这种方法有效地估计了3D平移以及3D旋转的完整分布。
- en: RGB cameras are widely used as visual sensors, yet they struggle to capture
    sufficient information under poor lighting conditions. This results in poor pose
    estimation performance.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: RGB相机作为视觉传感器被广泛使用，但在光线条件差的情况下，它们难以捕获足够的信息。这导致了姿态估计性能不佳。
- en: 3.2.2 Point Cloud-Based Template Methods
  id: totrans-556
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 基于点云的模板方法
- en: With the popularity of consumer-grade 3D cameras, point cloud-based methods
    take full advantage of their ability to adapt to poor illumination and capture
    geometric information. Li *et al.*[[70](#bib.bib70)] adopted a feature disentanglement
    and alignment module to establish part-to-part correspondences between the partial
    point cloud and object CAD model, enhancing geometric constraint. Jiang *et al.*[[108](#bib.bib108)]
    proposed a point cloud registration framework based on the SE(3) diffusion model,
    which gradually perturbs the optimal rigid transformation of a pair of point clouds
    by continuously injecting perturbation transformation through the SE(3) forward
    diffusion process. Then, the SE(3) reverse denoising process is used to gradually
    denoise, making it closer to the optimal transformation for accurate pose estimation.
    Dang *et al.*[[109](#bib.bib109)] proposed two key contributions to enhance pose
    estimation performance on real-world data. First, they introduced a directly supervised
    loss function that bypasses the SVD[[137](#bib.bib137)] operation, mitigating
    the sensitivity of SVD-based loss functions to the rotation range between the
    input partial point cloud and the object CAD model. Second, they devised a match
    normalization strategy to address disparities in feature distributions between
    the partial point cloud and the CAD model.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 随着消费级3D相机的普及，基于点云的方法充分利用其适应光照不足和捕捉几何信息的能力。Li *et al.*[[70](#bib.bib70)] 采用了特征解耦和对齐模块来建立部分点云与物体CAD模型之间的部件对应关系，从而增强几何约束。Jiang
    *et al.*[[108](#bib.bib108)] 提出了一个基于SE(3)扩散模型的点云配准框架，该框架通过不断地通过SE(3)正向扩散过程注入扰动变换来逐步扰动一对点云的最优刚性变换。然后，使用SE(3)反向去噪过程逐渐去噪，使其更接近于最优变换，以实现准确的姿态估计。Dang
    *et al.*[[109](#bib.bib109)] 提出了两个关键贡献来提升在实际数据上的姿态估计性能。首先，他们引入了一个直接监督的损失函数，绕过了SVD[[137](#bib.bib137)]
    操作，从而减轻了SVD基于损失函数对输入部分点云和物体CAD模型之间旋转范围的敏感性。其次，他们设计了一种匹配归一化策略，以解决部分点云和CAD模型之间特征分布的差异。
- en: In general, template-based methods leverage global information from the image,
    enabling them to effectively handle texture-less objects. However, achieving high
    pose estimation accuracy may lead to increased memory usage by the templates and
    a rapid rise in computational complexity. Additionally, they may also exhibit
    poor performance when confronted with occluded objects.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，基于模板的方法利用图像中的全局信息，使其能够有效处理没有纹理的物体。然而，实现高姿态估计精度可能会导致模板的内存使用增加，并迅速提高计算复杂度。此外，当遇到被遮挡的物体时，它们也可能表现出较差的性能。
- en: 3.3 Voting-Based Methods
  id: totrans-559
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于投票的方法
- en: 'Voting-based methods determine object pose through a pixel-level or point-level
    voting scheme, which can be categorized into two main types: indirect voting and
    direct voting. Indirect voting methods (Sec. [3.3.1](#S3.SS3.SSS1 "3.3.1 Indirect
    Voting Methods ‣ 3.3 Voting-Based Methods ‣ 3 Instance-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")) estimate
    a set of pre-defined 2D keypoints from the RGB image through pixel-level voting,
    or a set of pre-defined 3D keypoints from the point cloud via point-level voting.
    Subsequently, the object pose is determined through 2D-3D or 3D-3D keypoint correspondences
    between the input image and the CAD model. Direct voting methods (Sec. [3.3.2](#S3.SS3.SSS2
    "3.3.2 Direct Voting Methods ‣ 3.3 Voting-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) directly predict the pose and confidence at the pixel-level or point-level,
    then select the pose with the highest confidence as the object pose. The illustration
    of these types of methods is shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.1 Sparse
    Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey"). The attributes and performance of some representative methods are shown
    in Table [I](#S3.T1 "TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey").'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '基于投票的方法通过像素级或点级投票方案确定物体姿态，这些方法可以分为两种主要类型：间接投票和直接投票。间接投票方法（参见[3.3.1](#S3.SS3.SSS1
    "3.3.1 Indirect Voting Methods ‣ 3.3 Voting-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")）通过像素级投票从RGB图像中估计一组预定义的2D关键点，或者通过点级投票从点云中估计一组预定义的3D关键点。随后，通过输入图像与CAD模型之间的2D-3D或3D-3D关键点对应关系来确定物体姿态。直接投票方法（参见[3.3.2](#S3.SS3.SSS2
    "3.3.2 Direct Voting Methods ‣ 3.3 Voting-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")）直接在像素级或点级预测姿态和置信度，然后选择置信度最高的姿态作为物体姿态。这些方法类型的示意图见图[4](#S3.F4 "Figure 4
    ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")。一些代表性方法的属性和性能见表[I](#S3.T1 "TABLE I ‣ 3.1.1 Sparse Correspondence Methods
    ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣
    Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")。'
- en: 3.3.1 Indirect Voting Methods
  id: totrans-561
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 间接投票方法
- en: Some researchers predicted 2D keypoints and then derived the object pose through
    2D-3D keypoints correspondence. Liu *et al.*[[151](#bib.bib151)] introduced a
    continuous representation method called keypoint distance field (KDF), which extracts
    2D keypoints by voting on each KDF. Meanwhile, Cao *et al.*[[110](#bib.bib110)]
    proposed a method called dynamic graph PnP[[73](#bib.bib73)] to learn the object
    pose from 2D-3D correspondence, enabling end-to-end training. Moreover, Liu *et
    al.*[[152](#bib.bib152)] introduced a bidirectional depth residual fusion network
    to fuse RGBD information, thereby estimating 2D keypoints precisely. Inspired
    by the diffusion model, Xu *et al.*[[153](#bib.bib153)] proposed a diffusion-based
    framework to formulate 2D keypoint detection as a denoising process to establish
    more accurate 2D-3D correspondences.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员预测了2D关键点，然后通过2D-3D关键点对应推导物体姿态。刘*等人*[[151](#bib.bib151)] 引入了一种称为关键点距离场（KDF）的连续表示方法，该方法通过对每个KDF进行投票来提取2D关键点。同时，曹*等人*[[110](#bib.bib110)]
    提出了一个称为动态图PnP[[73](#bib.bib73)] 的方法，通过2D-3D对应学习物体姿态，实现了端到端训练。此外，刘*等人*[[152](#bib.bib152)]
    引入了一个双向深度残差融合网络来融合RGBD信息，从而精确估计2D关键点。受扩散模型的启发，徐*等人*[[153](#bib.bib153)] 提出了一个基于扩散的框架，将2D关键点检测公式化为去噪过程，以建立更准确的2D-3D对应关系。
- en: Unlike the aforementioned methods that predict 2D keypoints, He *et al.*[[18](#bib.bib18)]
    proposed a depth hough voting network to predict 3D keypoints. Subsequently, they
    estimated the object pose through levenberg-marquardt algorithm[[146](#bib.bib146)].
    Furthermore, He *et al.*[[21](#bib.bib21)] introduced a bidirectional fusion network
    to complement RGB and depth heterogeneous data, thereby better predicting the
    3D keypoints. To better capture features among object points in 3D space, Mei
    *et al.*[[154](#bib.bib154)] utilized graph convolutional networks to facilitate
    feature exchange among points in 3D space, aiming to improve the accuracy of predicting
    3D keypoints. Wu *et al.*[[111](#bib.bib111)] proposed a 3D keypoint voting scheme
    based on cross-spherical surfaces, allowing for generating smaller and more dispersed
    3D keypoint sets, thus improving estimation efficiency. To obtain more accurate
    3D keypoints, Wang *et al.*[[155](#bib.bib155)] presented an iterative 3D keypoint
    voting network to refine the initial localization of 3D keypoints. Most recently,
    Zhou *et al.*[[112](#bib.bib112)] introduced a novel weighted vector 3D keypoints
    voting algorithm, which adopts a non-iterative global optimization strategy to
    precisely localize 3D keypoints, while also achieving near real-time inference
    speed.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 与前述预测 2D 关键点的方法不同，He *et al.*[[18](#bib.bib18)] 提出了一个深度霍夫投票网络来预测 3D 关键点。随后，他们通过
    levenberg-marquardt 算法[[146](#bib.bib146)] 估计了物体姿态。此外，He *et al.*[[21](#bib.bib21)]
    引入了一个双向融合网络来补充 RGB 和深度异质数据，从而更好地预测 3D 关键点。为了更好地捕捉 3D 空间中物体点的特征，Mei *et al.*[[154](#bib.bib154)]
    利用图卷积网络促进 3D 空间中点之间的特征交换，旨在提高 3D 关键点预测的准确性。Wu *et al.*[[111](#bib.bib111)] 提出了一个基于交叉球面表面的
    3D 关键点投票方案，允许生成更小且更分散的 3D 关键点集，从而提高估计效率。为了获得更准确的 3D 关键点，Wang *et al.*[[155](#bib.bib155)]
    提出了一个迭代 3D 关键点投票网络，以细化 3D 关键点的初始定位。最近，Zhou *et al.*[[112](#bib.bib112)] 引入了一种新型加权向量
    3D 关键点投票算法，该算法采用非迭代全局优化策略来精确定位 3D 关键点，同时实现了接近实时的推理速度。
- en: In response to challenging scenarios such as cluttered or occlusion, Peng *et
    al.*[[17](#bib.bib17)] introduced a pixel-wise voting network to regress pixel-level
    vectors pointing to 3D keypoints. These vectors create a flexible representation
    for locating occluded or truncated 3D keypoints. Since most industrial parts are
    parameterized, Zeng *et al.*[[156](#bib.bib156)] defined 3D keypoints linked to
    parameters through driven parameters and symmetries. This approach effectively
    addresses the pose estimation of objects in stacking scenes.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 针对拥挤或遮挡等挑战性场景，Peng *et al.*[[17](#bib.bib17)] 引入了一个逐像素投票网络来回归指向 3D 关键点的像素级向量。这些向量为定位遮挡或截断的
    3D 关键点提供了一种灵活的表示。由于大多数工业部件是参数化的，Zeng *et al.*[[156](#bib.bib156)] 通过驱动参数和对称性定义了与参数关联的
    3D 关键点。这种方法有效地解决了堆叠场景中物体的姿态估计问题。
- en: Rather than utilizing a single-view RGBD image as input, Duffhauss *et al.*[[157](#bib.bib157)]
    employed multi-view RGBD images as input. They extracted visual features from
    each RGB image, while geometric features were extracted from the object point
    cloud (generated by fusing all depth images). This multi-view RGBD feature fusion-based
    method can accurately predict object pose in cluttered scenes.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用单视图 RGBD 图像作为输入不同，Duffhauss *et al.*[[157](#bib.bib157)] 采用了多视图 RGBD 图像作为输入。他们从每个
    RGB 图像中提取视觉特征，同时从物体点云（由融合所有深度图像生成）中提取几何特征。这种基于多视图 RGBD 特征融合的方法可以准确预测拥挤场景中的物体姿态。
- en: Some researchers have proposed new training strategies to improve pose estimation
    performance. Yu *et al.*[[158](#bib.bib158)] developed a differentiable proxy
    voting loss that simulates hypothesis selection during the voting process, enabling
    end-to-end training. In addition, Lin *et al.*[[159](#bib.bib159)] proposed a
    novel learning framework, which utilizes the accurate result of the RGBD-based
    pose refinement method to supervise the RGB-based pose estimator. To bridge the
    domain gap between synthetic and real data, Ikeda *et al.*[[160](#bib.bib160)]
    introduced a method to transfer object style transfer from synthetic to realistic
    without manual intervention.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员提出了新的训练策略以提高姿态估计性能。Yu *et al.*[[158](#bib.bib158)] 开发了一种可微分代理投票损失，该损失模拟了投票过程中的假设选择，实现了端到端训练。此外，Lin
    *et al.*[[159](#bib.bib159)] 提出了一个新颖的学习框架，该框架利用基于 RGBD 的姿态优化方法的准确结果来监督基于 RGB 的姿态估计器。为了弥合合成数据和真实数据之间的领域差距，Ikeda
    *et al.*[[160](#bib.bib160)] 引入了一种将物体样式从合成到现实进行迁移的方法，无需人工干预。
- en: Overall, indirect voting-based methods provide an excellent solution for instance-level
    object pose estimation. However, the accuracy of pose estimation heavily relies
    on the quality of the keypoints, which can result in lower robustness.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，基于间接投票的方法为实例级物体姿态估计提供了出色的解决方案。然而，姿态估计的准确性严重依赖于关键点的质量，这可能导致较低的鲁棒性。
- en: 3.3.2 Direct Voting Methods
  id: totrans-568
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 直接投票方法
- en: The performance of the indirect voting methods heavily depends on the selection
    of keypoints. Consequently, direct voting methods have been proposed as an alternative
    solution. Tian *et al.*[[113](#bib.bib113)] uniformly sampled rotation anchors
    in SO(3). Subsequently, they predicted constraint deviations for each anchor towards
    the target, using the uncertainty score to select the best prediction. Then, they
    detected the 3D translation by aggregating point-to-center vectors towards the
    object center to recover the 6DoF pose. Wang *et al.*[[16](#bib.bib16)] fused
    RGB and depth features on a per-pixel basis and utilized a pose predictor to generate
    6DoF pose and confidence for each pixel. Subsequently, they selected the pose
    of the pixel with the highest confidence as the final pose. Zhou *et al.*[[161](#bib.bib161)]
    employed CNNs[[162](#bib.bib162)] to extract RGB features, which are then integrated
    into the point cloud to obtain fused features. Unlike [[16](#bib.bib16)], the
    fused features take the form of point sets rather than feature mappings.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 间接投票方法的性能在很大程度上依赖于关键点的选择。因此，直接投票方法被提出作为一种替代解决方案。Tian *et al.*[[113](#bib.bib113)]
    在SO(3)中均匀采样了旋转锚点。随后，他们预测了每个锚点相对于目标的约束偏差，使用不确定性评分来选择最佳预测。然后，他们通过汇聚指向物体中心的点到中心向量来检测3D平移，从而恢复6DoF姿态。Wang
    *et al.*[[16](#bib.bib16)] 在每个像素上融合了RGB和深度特征，并利用姿态预测器为每个像素生成6DoF姿态和置信度。随后，他们选择置信度最高的像素姿态作为最终姿态。Zhou
    *et al.*[[161](#bib.bib161)] 使用CNNs[[162](#bib.bib162)] 提取RGB特征，然后将这些特征整合到点云中以获得融合特征。与[[16](#bib.bib16)]不同，融合特征的形式是点集而非特征映射。
- en: However, the aforementioned RGBD fusion methods merely concatenate RGB and depth
    features without delving into their intrinsic relationship. Therefore, Zhou *et
    al.*[[114](#bib.bib114)] proposed a new multi-modal fusion graph convolutional
    network to enhance the fusion of RGB and depth images, capturing the inter-modality
    correlations through local information propagation. Liu *et al.*[[163](#bib.bib163)]
    decoupled scale-related and scale-invariant information in the depth image to
    guide the network in perceiving the scene’s 3D structure and provide scene texture
    for the RGB image feature extraction. Unlike the aforementioned approaches that
    use still images, Mu *et al.*[[164](#bib.bib164)] proposed a time fusion model
    integrating temporal motion information from RGBD images for 6DoF object pose
    estimation. This method effectively captures object motion and changes, thereby
    enhancing pose estimation accuracy and stability.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述RGBD融合方法仅仅是将RGB和深度特征连接起来，并没有深入探讨它们的内在关系。因此，Zhou *et al.*[[114](#bib.bib114)]
    提出了一个新的多模态融合图卷积网络，以增强RGB和深度图像的融合，通过局部信息传播捕捉模态间的相关性。Liu *et al.*[[163](#bib.bib163)]
    将深度图像中的尺度相关信息和尺度不变信息进行解耦，以引导网络感知场景的3D结构，并为RGB图像特征提取提供场景纹理。与上述使用静态图像的方法不同，Mu *et
    al.*[[164](#bib.bib164)] 提出了一个时间融合模型，将RGBD图像中的时间运动信息整合用于6DoF物体姿态估计。这种方法有效捕捉物体运动和变化，从而提高了姿态估计的准确性和稳定性。
- en: Symmetric objects may have multiple true poses, leading to ambiguity in pose
    estimation. To address this issue, Mo*et al.*[[115](#bib.bib115)] designed a symmetric-invariant
    pose distance metric, which enables the network to estimate symmetric objects
    accurately. Cai *et al.*[[165](#bib.bib165)] introduced a 3D rotation representation
    to learn the object implicit symmetry, eliminating the need for additional prior
    knowledge about object symmetry.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 对称物体可能具有多个真实姿态，从而导致姿态估计的模糊性。为了解决这个问题，Mo *et al.*[[115](#bib.bib115)] 设计了一个对称不变的姿态距离度量，这使得网络能够准确估计对称物体。Cai
    *et al.*[[165](#bib.bib165)] 引入了一种3D旋转表示法来学习物体的隐式对称性，消除了对物体对称性额外先验知识的需求。
- en: To reduce dependency on annotated real data, Zeng *et al.*[[166](#bib.bib166)]
    trained their model solely on synthetic dataset. Then, they utilized a sim-to-real
    learning network to improve their generalization ability. During pose estimation,
    they transformed scene points into centroid space and obtained object pose through
    clustering and voting.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少对注释真实数据的依赖，Zeng *et al.*[[166](#bib.bib166)] 仅在合成数据集上训练了他们的模型。然后，他们利用了一个从模拟到现实的学习网络来提高他们的泛化能力。在姿态估计过程中，他们将场景点转换到质心空间，并通过聚类和投票获得物体姿态。
- en: Overall, voting-based methods have demonstrated superior performance in pose
    estimation tasks. However, the voting process is time-consuming and increases
    computational complexity [[167](#bib.bib167)].
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，基于投票的方法在姿态估计任务中表现出了优越的性能。然而，投票过程耗时且增加了计算复杂度[[167](#bib.bib167)]。
- en: 3.4 Regression-Based Methods
  id: totrans-574
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 基于回归的方法
- en: 'Regression-based methods aim to directly obtain the object pose from the learned
    features. They can be divided into two main types: geometry-guided regression
    and direct regression. Geometry-guided regression methods (Sec. [3.4.1](#S3.SS4.SSS1
    "3.4.1 Geometry-Guided Regression Methods ‣ 3.4 Regression-Based Methods ‣ 3 Instance-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) leverage geometric information from RGBD images (such as object 3D structural
    features or 2D-3D geometric constraints) to assist in object pose estimation.
    Direct regression methods (Sec. [3.4.2](#S3.SS4.SSS2 "3.4.2 Direct Regression
    Methods ‣ 3.4 Regression-Based Methods ‣ 3 Instance-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")) directly
    regress the object pose, utilizing RGBD image information. The illustration of
    these two types of methods is shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.1 Sparse
    Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey"). The attributes and performance of some representative methods are shown
    in Table [I](#S3.T1 "TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey").'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '基于回归的方法旨在直接从学习到的特征中获得物体姿态。它们可以分为两种主要类型：几何引导回归和直接回归。几何引导回归方法（见[3.4.1](#S3.SS4.SSS1
    "3.4.1 Geometry-Guided Regression Methods ‣ 3.4 Regression-Based Methods ‣ 3 Instance-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")）利用RGBD图像中的几何信息（如物体的3D结构特征或2D-3D几何约束）来辅助物体姿态估计。直接回归方法（见[3.4.2](#S3.SS4.SSS2
    "3.4.2 Direct Regression Methods ‣ 3.4 Regression-Based Methods ‣ 3 Instance-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")）直接回归物体姿态，利用RGBD图像信息。这两种方法的示意图见图[4](#S3.F4 "Figure 4 ‣ 3.1.1 Sparse Correspondence
    Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")。一些代表性方法的属性和性能见表[I](#S3.T1
    "TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods
    ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")。'
- en: 3.4.1 Geometry-Guided Regression Methods
  id: totrans-576
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 几何引导的回归方法
- en: Gao *et al.*[[168](#bib.bib168)] employed decoupled networks for rotation and
    translation regression from the object point cloud. Meanwhile, Chen *et al.*[[117](#bib.bib117)]
    introduced a rotation residual estimator to estimate the residual between the
    predicted rotation and the ground truth, enhancing the accuracy of rotation prediction.
    Lin *et al.*[[169](#bib.bib169)] used a network to extract the geometric features
    of the object point cloud. Then, they enhanced the pairwise consistency of geometric
    features by applying spectral convolution on pairwise compatibility graphs. Additionally,
    Shi *et al.*[[170](#bib.bib170)] learned geometric and contextual features within
    point cloud blocks. Then, they trained a sub-block network to predict the pose
    of each point cloud block. Finally, the most reliable block pose is selected as
    the object pose. To address the challenge of point cloud-based object pose tracking,
    Liu *et al.*[[171](#bib.bib171)] proposed a shifted point convolution operation
    between the point clouds of adjacent frames to facilitate the local context interaction.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: Gao *等人*[[168](#bib.bib168)]采用了分离的网络来进行物体点云的旋转和位移回归。同时，Chen *等人*[[117](#bib.bib117)]引入了旋转残差估计器来估计预测旋转与实际值之间的残差，从而提高了旋转预测的准确性。Lin
    *等人*[[169](#bib.bib169)]使用网络提取物体点云的几何特征。然后，他们通过在成对兼容图上应用光谱卷积来增强几何特征的成对一致性。此外，Shi
    *等人*[[170](#bib.bib170)]在点云块内学习几何和上下文特征。然后，他们训练了一个子块网络来预测每个点云块的姿态。最后，选择最可靠的块姿态作为物体姿态。为了解决基于点云的物体姿态跟踪问题，Liu
    *等人*[[171](#bib.bib171)]提出了一种在相邻帧的点云之间进行偏移点卷积操作的方法，以促进局部上下文的交互。
- en: Approaches solely relying on object point cloud often overlook the object texture
    details. Therefore, Wen *et al.*[[172](#bib.bib172)] and An *et al.*[[173](#bib.bib173)]
    leveraged the complementary nature of RGB and depth information. They improved
    cross-modal fusion strategies by employing attention mechanisms to effectively
    align and integrate these two heterogeneous data sources, resulting in enhanced
    performance.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 仅依赖于物体点云的方法往往忽视了物体纹理细节。因此，Wen *等人*[[172](#bib.bib172)]和An *等人*[[173](#bib.bib173)]利用了RGB和深度信息的互补特性。他们通过使用注意力机制改进了跨模态融合策略，有效地对齐和整合这两种异质数据源，从而提高了性能。
- en: In contrast to the aforementioned methods that directly derive geometric information
    from the depth image or the object CAD model, many researchers focused more on
    generating geometric constraints from the RGB image. Hu *et al.*[[118](#bib.bib118)]
    learned the 2D offset from the CAD model center to the 3D bounding box corners
    from the RGB image, and then directly regressed the object pose from the 2D-3D
    correspondence. Di *et al.*[[121](#bib.bib121)] used a shared encoder and two
    independent decoders to generate 2D-3D correspondence and self-occlusion information,
    improving the robustness of object pose estimation under occlusion. Further, Wang
    *et al.*[[120](#bib.bib120)] proposed a Geometry-Guided Direct Regression Network
    (GDR-Net) to learn object pose from dense 2D-3D correspondence in an end-to-end
    manner. Wang *et al.*[[122](#bib.bib122)] introduced noise-augmented student training
    and differentiable rendering based on GDR-Net[[120](#bib.bib120)], enabling robustness
    to occlusion scenes through self-supervised learning with multiple geometric constraints.
    Zhang *et al.*[[174](#bib.bib174)] proposed a transformer-based pose estimation
    approach that consists of a patch-aware feature fusion module and a transformer-based
    pose refinement module to address the limitation of CNN-based networks in capturing
    global dependencies. Most recently, Feng *et al.*[[175](#bib.bib175)] decoupled
    rotation into two sets of corresponding 3D normals. This decoupling strategy significantly
    improves the rotation accuracy.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接从深度图像或物体 CAD 模型中导出几何信息的方法不同，许多研究者更多地集中在从 RGB 图像生成几何约束上。胡 *等人*[[118](#bib.bib118)]
    从 RGB 图像中学习了从 CAD 模型中心到 3D 边界框角点的 2D 偏移量，然后直接从 2D-3D 对应关系回归物体姿态。迪 *等人*[[121](#bib.bib121)]
    使用了一个共享编码器和两个独立解码器来生成 2D-3D 对应关系和自遮挡信息，提高了在遮挡情况下物体姿态估计的鲁棒性。此外，王 *等人*[[120](#bib.bib120)]
    提出了一个几何引导直接回归网络（GDR-Net），以端到端的方式从密集的 2D-3D 对应关系中学习物体姿态。王 *等人*[[122](#bib.bib122)]
    引入了基于 GDR-Net[[120](#bib.bib120)] 的噪声增强学生训练和可微渲染，通过具有多个几何约束的自监督学习增强了对遮挡场景的鲁棒性。张
    *等人*[[174](#bib.bib174)] 提出了基于变换器的姿态估计方法，该方法包括一个补丁感知特征融合模块和一个变换器基础的姿态精化模块，以解决
    CNN 网络在捕捉全局依赖性方面的限制。最近，冯 *等人*[[175](#bib.bib175)] 将旋转解耦为两组对应的 3D 法线。这种解耦策略显著提高了旋转精度。
- en: Given the labor-intensive nature of real-world data annotation, some methods
    leverage synthetic data training to generalize to the real world. Gao *et al.*[[176](#bib.bib176)]
    constructed a lightweight synthetic point cloud generation pipeline and leveraged
    an enhanced point cloud-based autoencoder to learn latent object pose information
    to regress object pose. To improve generalization to real-world scenes, Zhou *et
    al.* [[177](#bib.bib177)] utilized annotated synthetic data to supervise the network
    convergence. They proposed a self-supervised pipeline for unannotated real data
    by minimizing the distance between the CAD model transformed from the predicted
    pose and the input point cloud. Tan *et al.* [[178](#bib.bib178)] proposed a self-supervised
    monocular object pose estimation network consisting of teacher and student modules.
    The teacher module is trained on synthetic data for initial object pose estimation,
    and the student model predicts camera pose from the unannotated real image. The
    student module acquires knowledge of object pose estimation from the teacher module
    by imposing geometric constraints derived from the camera pose.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于现实世界数据标注的劳动密集型特性，一些方法利用合成数据训练以实现对现实世界的泛化。高 *等人*[[176](#bib.bib176)] 构建了一个轻量级的合成点云生成管道，并利用增强的点云基础自动编码器学习潜在物体姿态信息以回归物体姿态。为了提高对现实世界场景的泛化，周
    *等人* [[177](#bib.bib177)] 利用标注的合成数据来监督网络的收敛。他们提出了一种通过最小化从预测姿态转换的 CAD 模型和输入点云之间的距离来处理未标注真实数据的自监督管道。谭
    *等人* [[178](#bib.bib178)] 提出了一个自监督单目物体姿态估计网络，该网络包括教师模块和学生模块。教师模块在合成数据上进行训练以进行初步物体姿态估计，学生模型从未标注的真实图像中预测相机姿态。学生模块通过施加从相机姿态中推导出的几何约束，从教师模块中获取物体姿态估计的知识。
- en: Geometry-guided regression methods typically require additional processing steps
    to extract and handle geometric information, which increases computational costs
    and complexity.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 几何引导的回归方法通常需要额外的处理步骤来提取和处理几何信息，这增加了计算成本和复杂性。
- en: 3.4.2 Direct Regression Methods
  id: totrans-582
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 直接回归方法
- en: Direct regression methods aim to directly recover the object pose from the RGBD
    image without additional transformation steps, thus reducing complexity. These
    methods encompass various strategies, including coupled pose output, decoupled
    pose output, and 3D rotation (3DoF pose) output. Coupled pose involves predicting
    object rotation and translation together, while decoupled pose involves predicting
    them separately. Moreover, the 3DoF pose output focuses solely on predicting object
    rotation without considering translation. These strategies are discussed in detail
    below.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 直接回归方法旨在从RGBD图像中直接恢复对象姿态，而无需额外的变换步骤，从而减少复杂性。这些方法包括各种策略，包括耦合姿态输出、解耦姿态输出和3D旋转（3DoF姿态）输出。耦合姿态涉及一起预测对象的旋转和位移，而解耦姿态则分别预测它们。此外，3DoF姿态输出仅关注预测对象的旋转，而不考虑位移。以下将详细讨论这些策略。
- en: 'Coupled Pose: To overcome lighting variations in the environment, Rambach *et
    al.*[[179](#bib.bib179)] used a pencil filter to normalize the input image into
    light-invariant representations, and then directly regressed the object coupled
    pose using a CNN network. Additionally, Kleeberger *et al.*[[180](#bib.bib180)]
    introduced a robust framework to handle occlusions between objects and estimate
    the multiple objects pose in the image. This framework is capable of running in
    real-time at 65 FPS. Sarode *et al.*[[181](#bib.bib181)] introduced a PointNet-based[[182](#bib.bib182)]
    framework to align point clouds for pose estimation, aiming to reduce sensitivity
    to pose misalignment. Estimating object pose from a single RGB image introduces
    an inherent ambiguity problem. Manhardt *et al.*[[126](#bib.bib126)] suggested
    explicitly addressing these ambiguities. They predicted multiple 6DoF poses for
    each object to estimate specific pose distributions caused by symmetry and repetitive
    textures. Inspired by the visible surface difference metric, Bengtson *et al.*[[183](#bib.bib183)]
    relied on a differentiable renderer and the CAD model to generate multiple weighted
    poses, avoiding falling into local minima. Moreover, Park *et al.*[[184](#bib.bib184)]
    proposed a method for pose estimation based on the local grid in object space.
    The method locates the grid region of interest on a ray in camera space and transforms
    the grid into object space via the estimated pose. The transformed grid is a new
    standard for sampling mesh and estimating pose.'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 耦合姿态：为克服环境中的光照变化，Rambach *et al.*[[179](#bib.bib179)] 使用了一种铅笔滤波器将输入图像归一化为光照不变表示，然后通过CNN网络直接回归对象耦合姿态。此外，Kleeberger
    *et al.*[[180](#bib.bib180)] 引入了一个鲁棒框架来处理对象之间的遮挡，并估计图像中的多个对象姿态。该框架能够以65 FPS的速度实时运行。Sarode
    *et al.*[[181](#bib.bib181)] 引入了一个基于PointNet的[[182](#bib.bib182)]框架来对齐点云进行姿态估计，旨在减少对姿态不对齐的敏感性。从单一RGB图像中估计对象姿态引入了固有的模糊问题。Manhardt
    *et al.*[[126](#bib.bib126)] 建议明确解决这些模糊问题。他们为每个对象预测多个6DoF姿态，以估计由对称性和重复纹理引起的特定姿态分布。受可见表面差异度量的启发，Bengtson
    *et al.*[[183](#bib.bib183)] 依靠可微分渲染器和CAD模型生成多个加权姿态，避免陷入局部极小值。此外，Park *et al.*[[184](#bib.bib184)]
    提出了一种基于对象空间局部网格的姿态估计方法。该方法在相机空间的光线中定位感兴趣的网格区域，并通过估计的姿态将网格转换到对象空间。转换后的网格是用于采样网格和估计姿态的新标准。
- en: For object pose tracking, Garon *et al.*[[185](#bib.bib185)] proposed a real-time
    tracking method that learns transformation relationships from consecutive frames
    during training, and used FCN [[186](#bib.bib186)] to obtain the relative pose
    between two frames for training and inference.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 对象姿态跟踪方面，Garon *et al.*[[185](#bib.bib185)] 提出了一个实时跟踪方法，该方法在训练过程中学习连续帧之间的变换关系，并使用FCN
    [[186](#bib.bib186)] 获取两帧之间的相对姿态用于训练和推理。
- en: Coupled pose may lead to information coupling between rotation and translation,
    making it difficult to distinguish their relationship during the optimization
    process, thus affecting estimation accuracy.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 耦合姿态可能导致旋转和位移之间的信息耦合，使得在优化过程中难以区分它们的关系，从而影响估计精度。
- en: 'Decoupled Pose: Decoupling the 6DoF object pose enables explicit modeling of
    the dependencies and independencies between object rotation and translation[[15](#bib.bib15)].'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 解耦姿态：解耦6DoF对象姿态使得可以明确建模对象旋转和位移之间的依赖性和独立性[[15](#bib.bib15)]。
- en: In object pose estimation, Xiang *et al.*[[15](#bib.bib15)] estimated the 3D
    translation by locating the object center in the image and predicting the distance
    from the object center to the camera. They further estimated the 3D rotation by
    regressing to a quaternion representation, and introduced a novel loss function
    to handle symmetric objects better. Meanwhile, Kehl *et al.*[[187](#bib.bib187)]
    extended the SSD framework[[188](#bib.bib188)] to generate 2D bounding boxes,
    as well as confidence scores for each viewpoint and in-plane rotation. Then, they
    chose the 2D bounding box through non-maximum suppression, along with the highest
    confidence viewpoint and in-plane rotation to infer the 3D translation, resulting
    in the full 6DoF object pose. Wu *et al.*[[189](#bib.bib189)], Do*et al.*[[190](#bib.bib190)]
    and Bukschat *et al.*[[167](#bib.bib167)] used two parallel FCN [[186](#bib.bib186)]
    branches to regress the object rotation and translation independently. To eliminate
    the dependence on annotations of real data, Wang *et al.*[[68](#bib.bib68)] used
    synthetic RGB data for fully supervised training, and then leveraged neural rendering
    for self-supervised learning on unannotated real RGBD data. Moreover, Jiang *et
    al.*[[69](#bib.bib69)] fused RGBD, built-in 2D-pixel coordinate encoding, and
    depth normal vector features to better estimate the object rotation and translation.
    Single-view methods suffer from ambiguity, therefore, Li *et al.*[[123](#bib.bib123)]
    proposed a multi-view fusion framework to reduce the ambiguity inherent in single-view
    frameworks. Further, Labbé *et al.*[[119](#bib.bib119)] proposed a unified approach
    for multi-view, multi-object pose estimation. Initially, they utilized a single-view,
    single-object pose estimation technique to derive pose hypotheses for individual
    objects. Then, they aligned these object pose hypotheses across multiple input
    images to collectively infer both the camera viewpoints and object pose within
    a unified scene. Most recently, Hsiao *et al.*[[191](#bib.bib191)] introduced
    a score-based diffusion method to solve the pose ambiguity problem in RGB-based
    object pose estimation.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 在物体姿态估计中，Xiang *等人*[[15](#bib.bib15)] 通过定位图像中的物体中心并预测物体中心到相机的距离来估计3D平移。他们进一步通过回归四元数表示来估计3D旋转，并引入了一种新颖的损失函数，以更好地处理对称物体。与此同时，Kehl
    *等人*[[187](#bib.bib187)] 扩展了SSD框架[[188](#bib.bib188)] 以生成2D边界框，并为每个视点和面内旋转生成置信度分数。然后，他们通过非极大值抑制选择2D边界框，并结合置信度最高的视点和面内旋转来推断3D平移，从而得到完整的6自由度物体姿态。Wu
    *等人*[[189](#bib.bib189)]、Do*等人*[[190](#bib.bib190)] 和 Bukschat *等人*[[167](#bib.bib167)]
    使用两个并行的FCN [[186](#bib.bib186)] 分支独立回归物体旋转和翻译。为了消除对真实数据标注的依赖，Wang *等人*[[68](#bib.bib68)]
    使用合成RGB数据进行完全监督训练，然后利用神经渲染进行未标注真实RGBD数据的自监督学习。此外，Jiang *等人*[[69](#bib.bib69)]
    融合了RGBD、内置2D像素坐标编码和深度法向量特征，以更好地估计物体的旋转和翻译。单视角方法存在模糊性，因此，Li *等人*[[123](#bib.bib123)]
    提出了一个多视角融合框架，以减少单视角框架中固有的模糊性。此外，Labbé *等人*[[119](#bib.bib119)] 提出了一个统一的多视角、多物体姿态估计方法。最初，他们利用单视角、单物体姿态估计技术为个别物体推导姿态假设。然后，他们对齐这些物体姿态假设，跨多个输入图像共同推断相机视点和统一场景中的物体姿态。最近，Hsiao
    *等人*[[191](#bib.bib191)] 引入了一种基于评分的扩散方法，以解决基于RGB的物体姿态估计中的姿态模糊问题。
- en: For object pose tracking, Wen *et al.*[[48](#bib.bib48)] proposed a data-driven
    optimization strategy to stabilize the 6DoF object pose tracking. Specifically,
    they predicted the 6DoF pose by predicting the relative pose between the adjacent
    frames. Liu *et al.*[[192](#bib.bib192)] proposed a new subtraction feature fusion
    module based on [[48](#bib.bib48)] to establish sufficient spatiotemporal information
    interaction between adjacent frames, improving the robustness of object pose tracking
    in complex scenes. Different from the method based on RGBD input, Ge *et al.*[[193](#bib.bib193)]
    designed a novel deep neural network architecture that integrates visual and inertial
    features to predict the relative object pose between consecutive image frames.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 对于物体姿态跟踪，Wen *等人*[[48](#bib.bib48)] 提出了一个数据驱动的优化策略，以稳定6自由度物体姿态跟踪。具体来说，他们通过预测相邻帧之间的相对姿态来预测6自由度姿态。Liu
    *等人*[[192](#bib.bib192)] 提出了一个基于[[48](#bib.bib48)]的新型减法特征融合模块，以在相邻帧之间建立足够的时空信息交互，从而提高了复杂场景中物体姿态跟踪的鲁棒性。与基于RGBD输入的方法不同，Ge
    *等人*[[193](#bib.bib193)] 设计了一种新颖的深度神经网络架构，结合视觉和惯性特征，以预测连续图像帧之间的相对物体姿态。
- en: In object pose refinement, Li *et al.*[[124](#bib.bib124)] iteratively refined
    pose through aligning RGB image with rendered image of object CAD model. Additionally,
    they predicted optical flow and foreground masks to stabilize the training procedure.
    Manhardt *et al.*[[125](#bib.bib125)] refined the 6DoF pose by aligning object
    contour between the RGB image and rendered contour. The rendered contour is obtained
    from the object CAD model using the initial pose. Hai *et al.*[[129](#bib.bib129)]
    proposed a shape-constraint recursive matching framework to refine the initial
    pose. They first computed a pose-induced flow based on the initial and currently
    estimated pose, and then directly decoupled the 6DoF pose from the pose-induced
    flow. To address the low running efficiency of the pose refinement methods, Iwase
    *et al.*[[194](#bib.bib194)] introduced a deep texture rendering-based pose refinement
    method for fast feature extraction using an object CAD model with a learnable
    texture. Most recently, Li *et al.*[[130](#bib.bib130)] proposed a two-stage method.
    The first stage performs pose classification and renders the object CAD model
    in the classified poses. The second stage performs regression to predict fine-grained
    residual in the classified poses. This method improves robustness by guiding residual
    pose regression through pose classification.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 在物体姿态精化中，Li *等人*[[124](#bib.bib124)] 通过对齐RGB图像和物体CAD模型的渲染图像来迭代精化姿态。此外，他们预测了光流和前景掩膜以稳定训练过程。Manhardt
    *等人*[[125](#bib.bib125)] 通过对齐RGB图像和渲染轮廓之间的物体轮廓来精化6DoF姿态。渲染轮廓是通过使用初始姿态从物体CAD模型中获得的。Hai
    *等人*[[129](#bib.bib129)] 提出了一个形状约束递归匹配框架来精化初始姿态。他们首先基于初始姿态和当前估计姿态计算姿态引导的光流，然后直接从姿态引导的光流中解耦出6DoF姿态。为了解决姿态精化方法的低运行效率，Iwase
    *等人*[[194](#bib.bib194)] 引入了一种基于深度纹理渲染的姿态精化方法，用于快速特征提取，使用带有可学习纹理的物体CAD模型。最近，Li
    *等人*[[130](#bib.bib130)] 提出了一个两阶段的方法。第一阶段进行姿态分类，并在分类的姿态中渲染物体CAD模型。第二阶段进行回归以预测分类姿态中的细粒度残差。该方法通过姿态分类引导残差姿态回归，从而提高了鲁棒性。
- en: '3DoF Pose: Some researchers pursue more efficient and practical pose estimation
    by solely regressing the 3D rotation. Papaioannidis *et al.*[[127](#bib.bib127)]
    proposed a novel quaternion-based multi-objective loss function, which integrates
    manifold learning and regression for learning 3DoF pose descriptors. They obtained
    the 3DoF pose through the regression of the learned descriptors. Liu *et al.*[[128](#bib.bib128)]
    trained a triple network based on convolutional neural networks to extract discriminative
    features from binary images. They incorporated pose-guided methods and regression
    constraints into the constructed triple network to adapt the features for the
    regression task, enhancing robustness. In addition, Josifovski *et al.*[[195](#bib.bib195)]
    estimated the camera viewpoint related to the object coordinate system by constructing
    a viewpoint estimation model, thereby obtaining the 3DoF pose appearing in the
    bounding box.'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '3DoF Pose: 一些研究者通过仅回归3D旋转来追求更高效、更实际的姿态估计。Papaioannidis *等人*[[127](#bib.bib127)]
    提出了一个新颖的基于四元数的多目标损失函数，该函数将流形学习和回归结合在一起，用于学习3DoF姿态描述符。他们通过回归学习到的描述符来获得3DoF姿态。Liu
    *等人*[[128](#bib.bib128)] 基于卷积神经网络训练了一个三重网络，从二值图像中提取判别特征。他们将姿态引导方法和回归约束纳入构建的三重网络中，以适应回归任务，增强了鲁棒性。此外，Josifovski
    *等人*[[195](#bib.bib195)] 通过构建视点估计模型估计了与物体坐标系相关的相机视点，从而获得了出现在边界框中的3DoF姿态。'
- en: Overall, direct regression methods simplify the object pose estimation process
    and further enhance the performance of instance-level methods. However, instance-level
    methods can only estimate specific object instances in the training data, limiting
    their generalization to unseen objects. Additionally, most instance-level methods
    require accurate object CAD models, which is a challenge, especially for objects
    with complex shapes and textures.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，直接回归方法简化了物体姿态估计过程，并进一步提升了实例级方法的性能。然而，实例级方法只能估计训练数据中的特定物体实例，这限制了它们对未见物体的泛化能力。此外，大多数实例级方法需要准确的物体CAD模型，这对复杂形状和纹理的物体尤其具有挑战性。
- en: 4 Category-Level Object Pose Estimation
  id: totrans-593
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 类别级物体姿态估计
- en: 'Research on category-level methods has garnered significant attention due to
    their potential for generalizing to unseen objects within established categories
    [[196](#bib.bib196)]. In this section, we review category-level methods by dividing
    them into shape prior-based (Sec. [4.1](#S4.SS1 "4.1 Shape Prior-Based Methods
    ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")) and shape prior-free (Sec. [4.2](#S4.SS2 "4.2 Shape
    Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")) methods. The illustration of
    these two categories is shown in Fig. [5](#S4.F5 "Figure 5 ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey"). The characteristics and performance of some representative SOTA methods
    are shown in Table [II](#S4.T2 "TABLE II ‣ 4.1 Shape Prior-Based Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey").'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类别级方法的研究引起了广泛关注，因为它们具有将知识推广到未见过的物体中的潜力[[196](#bib.bib196)]。在本节中，我们通过将这些方法分为基于形状先验的方法（见[4.1节](#S4.SS1
    "4.1 形状先验方法 ‣ 4 类别级物体姿态估计 ‣ 深度学习基础的物体姿态估计：全面调查")）和无形状先验的方法（见[4.2节](#S4.SS2 "4.2
    无形状先验方法 ‣ 4 类别级物体姿态估计 ‣ 深度学习基础的物体姿态估计：全面调查")）来回顾类别级方法。这两种类别的示意图如图[5](#S4.F5 "图
    5 ‣ 4 类别级物体姿态估计 ‣ 深度学习基础的物体姿态估计：全面调查")所示。一些代表性SOTA方法的特点和性能显示在表[II](#S4.T2 "表 II
    ‣ 4.1 形状先验方法 ‣ 4 类别级物体姿态估计 ‣ 深度学习基础的物体姿态估计：全面调查")中。
- en: '![Refer to caption](img/6eb485ec579b51ebdaefe72f772aabeb.png)'
  id: totrans-595
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6eb485ec579b51ebdaefe72f772aabeb.png)'
- en: 'Figure 5: Illustration of the shape prior-based (Sec. [4.1](#S4.SS1 "4.1 Shape
    Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")) and shape prior-free (Sec. [4.2](#S4.SS2
    "4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey")) category-level
    methods. The dashed arrows indicate offline training, which means that we need
    to train a model offline using the category-level model library to obtain shape
    priors. (Sec. [4.1](#S4.SS1 "4.1 Shape Prior-Based Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")): Taking RGBD input as an example, NOCS shape alignment methods (Sec.
    [4.1.1](#S4.SS1.SSS1 "4.1.1 NOCS Shape Alignment Methods ‣ 4.1 Shape Prior-Based
    Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) first learn a model to predict the
    NOCS shape/map of the object, and then align the object point cloud with the NOCS
    shape/map through a non-differentiable pose solution method such as the Umeyama
    algorithm [[197](#bib.bib197)] to solve the object pose. In contrast, direct regress
    pose methods (Sec. [4.1.2](#S4.SS1.SSS2 "4.1.2 Direct Regress Pose Methods ‣ 4.1
    Shape Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")) directly regress the object
    pose from the extracted input features. On the other hand, the shape prior-free
    methods (Sec. [4.2](#S4.SS2 "4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) do not have the process of shape priors regression: Depth-guided geometry-aware
    methods (Sec. [4.2.1](#S4.SS2.SSS1 "4.2.1 Depth-Guided Geometry-Aware Methods
    ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey")) focus on perceiving
    the global and local geometric information of the object and leverage these 3D
    geometric features to estimate the object pose. Conversely, RGBD-guided semantic
    and geometry fusion methods (Sec. [4.2.2](#S4.SS2.SSS2 "4.2.2 RGBD-Guided Semantic
    and Geometry Fusion Methods ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) regress the object pose by fusing the 2D semantic and 3D geometric information
    of the object.'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5：展示了基于形状先验的（参见 [4.1](#S4.SS1 "4.1 Shape Prior-Based Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")）和无形状先验的（参见 [4.2](#S4.SS2 "4.2 Shape Prior-Free Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")）类别级方法。虚线箭头表示离线训练，这意味着我们需要离线使用类别级模型库训练模型以获取形状先验。（参见 [4.1](#S4.SS1 "4.1
    Shape Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")）：以 RGBD 输入为例，NOCS 形状对齐方法（参见 [4.1.1](#S4.SS1.SSS1
    "4.1.1 NOCS Shape Alignment Methods ‣ 4.1 Shape Prior-Based Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")）首先学习一个模型来预测对象的 NOCS 形状/地图，然后通过 Umeyama 算法 [[197](#bib.bib197)] 等非可微分的姿态解决方法将对象点云与
    NOCS 形状/地图对齐，以解决对象姿态。相比之下，直接回归姿态的方法（参见 [4.1.2](#S4.SS1.SSS2 "4.1.2 Direct Regress
    Pose Methods ‣ 4.1 Shape Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")）直接从提取的输入特征中回归对象姿态。另一方面，无形状先验的方法（参见
    [4.2](#S4.SS2 "4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")）没有形状先验回归的过程：深度引导的几何感知方法（参见
    [4.2.1](#S4.SS2.SSS1 "4.2.1 Depth-Guided Geometry-Aware Methods ‣ 4.2 Shape Prior-Free
    Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")）专注于感知对象的全局和局部几何信息，并利用这些 3D 几何特征来估计对象姿态。相反，RGBD
    引导的语义和几何融合方法（参见 [4.2.2](#S4.SS2.SSS2 "4.2.2 RGBD-Guided Semantic and Geometry
    Fusion Methods ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")）通过融合对象的
    2D 语义和 3D 几何信息来回归对象姿态。'
- en: 4.1 Shape Prior-Based Methods
  id: totrans-597
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基于形状先验的方法
- en: 'Shape prior-based methods first learn a neural network using CAD models of
    intra-class seen objects in offline mode to derive shape priors, and then utilize
    them as 3D geometry prior information to guide intra-class unseen object pose
    estimation. In this part, we divide the shape prior-based methods into two categories
    based on their approach to addressing object pose estimation. The first category
    is Normalized Object Coordinate Space (NOCS) shape alignment methods (Sec. [4.1.1](#S4.SS1.SSS1
    "4.1.1 NOCS Shape Alignment Methods ‣ 4.1 Shape Prior-Based Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")). They first predict the NOCS shape/map, and then use an offline pose
    solution method (such as the Umeyama algorithm[[197](#bib.bib197)]) to align the
    object point cloud with the predicted NOCS shape/map to obtain the object pose.
    The other category is the pose regression methods (Sec. [4.1.2](#S4.SS1.SSS2 "4.1.2
    Direct Regress Pose Methods ‣ 4.1 Shape Prior-Based Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")). They directly regress the object pose from the feature level, making
    the pose acquisition process differentiable. The illustration of these two categories
    is shown in Fig. [5](#S4.F5 "Figure 5 ‣ 4 Category-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey").'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '基于形状先验的方法首先使用离线模式下的CAD模型学习神经网络，以推导形状先验，然后利用这些形状先验作为3D几何先验信息来指导未见物体的姿态估计。在这部分，我们将基于形状先验的方法分为两类，根据它们处理物体姿态估计的方法进行分类。第一类是标准化物体坐标空间（NOCS）形状对齐方法（参见
    [4.1.1](#S4.SS1.SSS1 "4.1.1 NOCS Shape Alignment Methods ‣ 4.1 Shape Prior-Based
    Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")）。它们首先预测NOCS形状/地图，然后使用离线姿态解决方法（如Umeyama算法[[197](#bib.bib197)]）将物体点云与预测的NOCS形状/地图对齐，以获得物体姿态。另一类是姿态回归方法（参见
    [4.1.2](#S4.SS1.SSS2 "4.1.2 Direct Regress Pose Methods ‣ 4.1 Shape Prior-Based
    Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")）。它们直接从特征级别回归物体姿态，使得姿态获取过程可微分。这两类方法的说明见图
    [5](#S4.F5 "Figure 5 ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")。'
- en: 'TABLE II: Representative category-level methods. For each method, we report
    its 10 properties, which have the same meanings as described in Table. [I](#S3.T1
    "TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods
    ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey"). D, S, N, K, and P denote object detection, instance
    segmentation, NOCS shape/map regression, keypoints detection, and pose solution/regression,
    respectively. Moreover, we report the $5^{\circ}$$5{\rm{cm}}$ metric of CAMERA25
    and REAL275 datasets (Sec. [2](#S2 "2 Datasets and Metrics ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")).'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 代表性的类别级方法。对于每种方法，我们报告其10个属性，这些属性的含义与表 [I](#S3.T1 "TABLE I ‣ 3.1.1 Sparse
    Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey") 中描述的含义相同。D、S、N、K 和 P 分别表示物体检测、实例分割、NOCS形状/地图回归、关键点检测和姿态解决/回归。此外，我们报告了CAMERA25和REAL275数据集的
    $5^{\circ}$$5{\rm{cm}}$ 量度（参见 [2](#S2 "2 Datasets and Metrics ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")）。'
- en: '| Methods |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |'
- en: '&#124; Published &#124;'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 发布 &#124;'
- en: '&#124; Year &#124;'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 年份 &#124;'
- en: '|'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Training &#124;'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 &#124;'
- en: '&#124; Input &#124;'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入 &#124;'
- en: '|'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Inference &#124;'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 推理 &#124;'
- en: '&#124; Input &#124;'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入 &#124;'
- en: '|'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Pose &#124;'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 姿态 &#124;'
- en: '&#124; DoF &#124;'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DoF &#124;'
- en: '|'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Object &#124;'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 物体 &#124;'
- en: '&#124; Property &#124;'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 属性 &#124;'
- en: '| Task |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| 任务 |'
- en: '&#124; Domain Training &#124;'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 域训练 &#124;'
- en: '&#124; Paradigm &#124;'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 范式 &#124;'
- en: '|'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Inference &#124;'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 推理 &#124;'
- en: '&#124; Mode &#124;'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模式 &#124;'
- en: '|'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Application &#124;'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 应用 &#124;'
- en: '&#124; Area &#124;'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 区域 &#124;'
- en: '|'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CAMERA25 &#124;'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAMERA25 &#124;'
- en: '&#124; $5^{\circ}$$5{\rm{cm}}$ (mAP) &#124;'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $5^{\circ}$$5{\rm{cm}}$ (mAP) &#124;'
- en: '|'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; REAL275 &#124;'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; REAL275 &#124;'
- en: '&#124; $5^{\circ}$$5{\rm{cm}}$ (mAP) &#124;'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $5^{\circ}$$5{\rm{cm}}$ (mAP) &#124;'
- en: '|'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Shape Prior-Based Methods | NOCS shape alignment | Tian *et al.*[[72](#bib.bib72)]
    | 2020 |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '| 基于形状先验的方法 | NOCS形状对齐 | Tian *et al.*[[72](#bib.bib72)] | 2020 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| RGBD | 9DoF | rigid | estimation | source |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 9DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; S+N+P &#124;'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+N+P &#124;'
- en: '| general | 59.0 | 21.4 |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 59.0 | 21.4 |'
- en: '| Wang *et al.*[[198](#bib.bib198)] | 2021 |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| Wang *等*[[198](#bib.bib198)] | 2021 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| RGBD | 9DoF | rigid | estimation | source |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 9DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; S+N+P &#124;'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+N+P &#124;'
- en: '| general | 76.4 | 34.3 |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 76.4 | 34.3 |'
- en: '| Chen *et al.*[[23](#bib.bib23)] | 2021 |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '| Chen *等*[[23](#bib.bib23)] | 2021 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| RGBD | 9DoF | rigid | estimation | source |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 9DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; S+N+P &#124;'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+N+P &#124;'
- en: '| general | 74.5 | 39.6 |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 74.5 | 39.6 |'
- en: '| Zou *et al.*[[199](#bib.bib199)] | 2022 |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '| Zou *等*[[199](#bib.bib199)] | 2022 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| RGBD | 9DoF | rigid | estimation | source |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 9DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; S+N+P &#124;'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+N+P &#124;'
- en: '| general | 76.7 | 41.9 |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 76.7 | 41.9 |'
- en: '| Fan *et al.*[[200](#bib.bib200)] | 2022 |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| Fan *等*[[200](#bib.bib200)] | 2022 |'
- en: '&#124; RGB, &#124;'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| RGB | 9DoF | rigid | estimation | source |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '| RGB | 9DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; S+N+P &#124;'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+N+P &#124;'
- en: '| general | - | - |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | - |'
- en: '| Wei *et al.*[[201](#bib.bib201)] | 2023 |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| Wei *等*[[201](#bib.bib201)] | 2023 |'
- en: '&#124; RGB, &#124;'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| RGB | 9DoF | rigid | estimation | source |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| RGB | 9DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; S+N+P &#124;'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+N+P &#124;'
- en: '| general | - | - |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | - |'
- en: '| Direct regress pose | Irshad *et al.*[[202](#bib.bib202)] | 2022 |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '| 直接回归姿态 | Irshad *等*[[202](#bib.bib202)] | 2022 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| RGBD | 9DoF | rigid | estimation | source | end-to-end | general | 66.2 |
    29.1 |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 9DoF | 刚性 | 估计 | 来源 | 端到端 | 一般 | 66.2 | 29.1 |'
- en: '| Lin *et al.*[[203](#bib.bib203)] | 2022 | Depth | Depth | 9DoF | rigid |
    estimation | generalization |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| Lin *等*[[203](#bib.bib203)] | 2022 | 深度 | 深度 | 9DoF | 刚性 | 估计 | 泛化 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; S+P &#124;'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | 70.9 | 42.3 |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 70.9 | 42.3 |'
- en: '| Zhang *et al.*[[204](#bib.bib204)] | 2022 |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| Zhang *等*[[204](#bib.bib204)] | 2022 |'
- en: '&#124; Depth, &#124;'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| Depth | 9DoF | rigid | estimation | source |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '| 深度 | 9DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; S+P &#124;'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | 75.5 | 44.6 |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 75.5 | 44.6 |'
- en: '| Zhang *et al.*[[205](#bib.bib205)] | 2022 |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
  zh: '| Zhang *等*[[205](#bib.bib205)] | 2022 |'
- en: '&#124; Depth, &#124;'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| Depth | 9DoF | rigid | estimation | source |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| 深度 | 9DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; S+P &#124;'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | 79.6 | 48.1 |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 79.6 | 48.1 |'
- en: '| Liu *et al.*[[206](#bib.bib206)] | 2022 | Depth | Depth | 9DoF | rigid |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
  zh: '| Liu *等*[[206](#bib.bib206)] | 2022 | 深度 | 深度 | 9DoF | 刚性 |'
- en: '&#124; refinement, &#124;'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精细化， &#124;'
- en: '&#124; tracking &#124;'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跟踪 &#124;'
- en: '| source |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '| 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; S+P &#124;'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | 80.3 | 54.4 |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 80.3 | 54.4 |'
- en: '| Lin *et al.*[[24](#bib.bib24)] | 2022 |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
  zh: '| Lin *等*[[24](#bib.bib24)] | 2022 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| RGBD | 9DoF | rigid | estimation | generalization |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 9DoF | 刚性 | 估计 | 泛化 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; S+P &#124;'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | - | 45.0 |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 45.0 |'
- en: '| Ze *et al.*[[55](#bib.bib55)] | 2022 |'
  id: totrans-709
  prefs: []
  type: TYPE_TB
  zh: '| Ze *等*[[55](#bib.bib55)] | 2022 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| RGBD | 9DoF | rigid | estimation | adaptation |'
  id: totrans-712
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 9DoF | 刚性 | 估计 | 适应 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; S+P &#124;'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | - | 33.9 |'
  id: totrans-715
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 33.9 |'
- en: '| Liu *et al.*[[207](#bib.bib207)] | 2024 |'
  id: totrans-716
  prefs: []
  type: TYPE_TB
  zh: '| Liu *等*[[207](#bib.bib207)] | 2024 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| RGBD | 9DoF | rigid | estimation | generalization |'
  id: totrans-719
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 9DoF | 刚性 | 估计 | 泛化 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; S+P &#124;'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | - | 50.1 |'
  id: totrans-722
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 50.1 |'
- en: '| Shape Prior-Free Methods | Depth-guided geometry-aware | Li *et al.*[[208](#bib.bib208)]
    | 2020 | Depth | Depth | 9DoF | articulated | estimation | generalization |'
  id: totrans-723
  prefs: []
  type: TYPE_TB
  zh: '| 形状先验自由方法 | 深度引导的几何感知 | Li *等*[[208](#bib.bib208)] | 2020 | 深度 | 深度 | 9DoF
    | 有关节 | 估计 | 泛化 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; S+P &#124;'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | - | - |'
  id: totrans-726
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | - |'
- en: '| Chen *et al.*[[209](#bib.bib209)] | 2021 | Depth | Depth | 9DoF | rigid |
    estimation | source |'
  id: totrans-727
  prefs: []
  type: TYPE_TB
  zh: '| Chen *等*[[209](#bib.bib209)] | 2021 | 深度 | 深度 | 9DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; D+P &#124;'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+P &#124;'
- en: '| general | - | 28.2 |'
  id: totrans-730
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 28.2 |'
- en: '| Weng *et al.*[[210](#bib.bib210)] | 2021 | Depth | Depth | 9DoF |'
  id: totrans-731
  prefs: []
  type: TYPE_TB
  zh: '| Weng *et al.*[[210](#bib.bib210)] | 2021 | 深度 | 深度 | 9DoF |'
- en: '&#124; rigid, &#124;'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; rigid，&#124;'
- en: '&#124; articulated &#124;'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人体关节 &#124;'
- en: '| tracking | source |'
  id: totrans-734
  prefs: []
  type: TYPE_TB
  zh: '| tracking | source |'
- en: '&#124; two-stage, &#124;'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; N+P &#124;'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; N+P &#124;'
- en: '| general | - | 62.2 |'
  id: totrans-737
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 62.2 |'
- en: '| Di *et al.*[[25](#bib.bib25)] | 2022 | Depth | Depth | 9DoF | rigid | estimation
    | source |'
  id: totrans-738
  prefs: []
  type: TYPE_TB
  zh: '| Di *et al.*[[25](#bib.bib25)] | 2022 | 深度 | 深度 | 9DoF | rigid | estimation
    | source |'
- en: '&#124; two-stage, &#124;'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; S+P &#124;'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | 79.1 | 42.9 |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 79.1 | 42.9 |'
- en: '| You *et al.*[[211](#bib.bib211)] | 2022 | Depth | Depth | 9DoF | rigid |
    estimation | generalization |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
  zh: '| You *et al.*[[211](#bib.bib211)] | 2022 | 深度 | 深度 | 9DoF | rigid | estimation
    | generalization |'
- en: '&#124; two-stage, &#124;'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; S+P &#124;'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | - | 16.9 |'
  id: totrans-745
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 16.9 |'
- en: '| Zheng *et al.*[[26](#bib.bib26)] | 2023 | Depth | Depth | 9DoF | rigid |
    estimation | source |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
  zh: '| Zheng *et al.*[[26](#bib.bib26)] | 2023 | 深度 | 深度 | 9DoF | rigid | estimation
    | source |'
- en: '&#124; two-stage, &#124;'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; S+P &#124;'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | 80.5 | 55.2 |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 80.5 | 55.2 |'
- en: '| Zhang *et al.*[[212](#bib.bib212)] | 2023 | Depth | Depth | 6DoF | rigid
    |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
  zh: '| Zhang *et al.*[[212](#bib.bib212)] | 2023 | 深度 | 深度 | 6DoF | rigid |'
- en: '&#124; estimation, &#124;'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 估计，&#124;'
- en: '&#124; tracking &#124;'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; tracking &#124;'
- en: '| source |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
  zh: '| source |'
- en: '&#124; two-stage, &#124;'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; S+P &#124;'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | - | 60.9 |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 60.9 |'
- en: '| RGBD-guided semantic $\&amp;$ geometry fusion | Wang *et al.*[[22](#bib.bib22)]
    | 2019 | RGBD | RGBD | 9DoF | rigid | estimation | source |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
  zh: '| RGBD 引导的语义 $\&amp;$ 几何融合 | Wang *et al.*[[22](#bib.bib22)] | 2019 | RGBD
    | RGBD | 9DoF | rigid | estimation | source |'
- en: '&#124; two-stage, &#124;'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; (S+N)+P &#124;'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (S+N)+P &#124;'
- en: '| general | 40.9 | 10.0 |'
  id: totrans-760
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 40.9 | 10.0 |'
- en: '| Wang *et al.*[[213](#bib.bib213)] | 2020 | RGBD | RGBD | 6DoF | rigid | tracking
    | source |'
  id: totrans-761
  prefs: []
  type: TYPE_TB
  zh: '| Wang *et al.*[[213](#bib.bib213)] | 2020 | RGBD | RGBD | 6DoF | rigid | tracking
    | source |'
- en: '&#124; two-stage, &#124;'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; K+P &#124;'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; K+P &#124;'
- en: '| general | - | 33.3 |'
  id: totrans-764
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 33.3 |'
- en: '| Lin *et al.*[[214](#bib.bib214)] | 2021 | RGBD | RGBD | 9DoF | rigid | estimation
    | source |'
  id: totrans-765
  prefs: []
  type: TYPE_TB
  zh: '| Lin *et al.*[[214](#bib.bib214)] | 2021 | RGBD | RGBD | 9DoF | rigid | estimation
    | source |'
- en: '&#124; two-stage, &#124;'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; S+P &#124;'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | 70.7 | 35.9 |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 70.7 | 35.9 |'
- en: '| Wen *et al.*[[215](#bib.bib215)] | 2021 | RGBD | RGBD | 9DoF | rigid | tracking
    | source |'
  id: totrans-769
  prefs: []
  type: TYPE_TB
  zh: '| Wen *et al.*[[215](#bib.bib215)] | 2021 | RGBD | RGBD | 9DoF | rigid | tracking
    | source |'
- en: '&#124; three-stage, &#124;'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段，&#124;'
- en: '&#124; S+K+P &#124;'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+K+P &#124;'
- en: '| general | - | 87.4 |'
  id: totrans-772
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 87.4 |'
- en: '| Peng *et al.*[[216](#bib.bib216)] | 2022 |'
  id: totrans-773
  prefs: []
  type: TYPE_TB
  zh: '| Peng *et al.*[[216](#bib.bib216)] | 2022 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| RGBD | 9DoF | rigid | estimation | adaptation |'
  id: totrans-776
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 9DoF | rigid | estimation | adaptation |'
- en: '&#124; two-stage, &#124;'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; S+P &#124;'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | - | 33.4 |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 33.4 |'
- en: '| Lee *et al.*[[217](#bib.bib217)] | 2022 |'
  id: totrans-780
  prefs: []
  type: TYPE_TB
  zh: '| Lee *et al.*[[217](#bib.bib217)] | 2022 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| RGBD | 9DoF | rigid | estimation | adaptation |'
  id: totrans-783
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 9DoF | rigid | estimation | adaptation |'
- en: '&#124; three-stage, &#124;'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段，&#124;'
- en: '&#124; S+N+P &#124;'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+N+P &#124;'
- en: '| general | - | 34.8 |'
  id: totrans-786
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 34.8 |'
- en: '| Lee *et al.*[[218](#bib.bib218)] | 2023 |'
  id: totrans-787
  prefs: []
  type: TYPE_TB
  zh: '| Lee *et al.*[[218](#bib.bib218)] | 2023 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| RGBD | 9DoF | rigid | estimation | generalization |'
  id: totrans-790
  prefs: []
  type: TYPE_TB
  zh: '| RGBD | 9DoF | rigid | estimation | generalization |'
- en: '&#124; three-stage, &#124;'
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段，&#124;'
- en: '&#124; S+N+P &#124;'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+N+P &#124;'
- en: '| general | - | 35.9 |'
  id: totrans-793
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 35.9 |'
- en: '| Liu *et al.*[[27](#bib.bib27)] | 2023 | RGBD | RGBD | 9DoF | rigid | estimation
    | source |'
  id: totrans-794
  prefs: []
  type: TYPE_TB
  zh: '| Liu *et al.*[[27](#bib.bib27)] | 2023 | RGBD | RGBD | 9DoF | rigid | estimation
    | source |'
- en: '&#124; two-stage, &#124;'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; S+P &#124;'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | 79.9 | 53.4 |'
  id: totrans-797
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 79.9 | 53.4 |'
- en: '| Lin *et al.*[[219](#bib.bib219)] | 2023 |'
  id: totrans-798
  prefs: []
  type: TYPE_TB
  zh: '| Lin *et al.*[[219](#bib.bib219)] | 2023 |'
- en: '&#124; RGBD or &#124;'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD 或 &#124;'
- en: '&#124; Depth &#124;'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度 &#124;'
- en: '|'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGBD or &#124;'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD 或 &#124;'
- en: '&#124; Depth &#124;'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度 &#124;'
- en: '| 9DoF | rigid | estimation | source |'
  id: totrans-804
  prefs: []
  type: TYPE_TB
  zh: '| 9DoF | rigid | estimation | source |'
- en: '&#124; two-stage, &#124;'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; S+P &#124;'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | 81.4 | 57.6 |'
  id: totrans-807
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 81.4 | 57.6 |'
- en: '| Chen *et al.*[[220](#bib.bib220)] | 2024 | RGBD | RGBD | 9DoF | rigid | estimation
    | source |'
  id: totrans-808
  prefs: []
  type: TYPE_TB
  zh: '| Chen *et al.*[[220](#bib.bib220)] | 2024 | RGBD | RGBD | 9DoF | rigid | estimation
    | source |'
- en: '&#124; two-stage, &#124;'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; S+P &#124;'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | - | 63.6 |'
  id: totrans-811
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | 63.6 |'
- en: '| Others | Lee *et al.*[[221](#bib.bib221)] | 2021 |'
  id: totrans-812
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | Lee *et al.*[[221](#bib.bib221)] | 2021 |'
- en: '&#124; RGB, &#124;'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB，&#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| RGB | 9DoF | rigid | estimation | generalization |'
  id: totrans-815
  prefs: []
  type: TYPE_TB
  zh: '| RGB | 9DoF | rigid | estimation | generalization |'
- en: '&#124; three-stage, &#124;'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段，&#124;'
- en: '&#124; S+N+P &#124;'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+N+P &#124;'
- en: '| general | - | - |'
  id: totrans-818
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | - |'
- en: '| Lin *et al.*[[222](#bib.bib222)] | 2024 |'
  id: totrans-819
  prefs: []
  type: TYPE_TB
  zh: '| 林 *等*[[222](#bib.bib222)] | 2024 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD，&#124;'
- en: '&#124; Text &#124;'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文本 &#124;'
- en: '|'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGBD, &#124;'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD，&#124;'
- en: '&#124; Text &#124;'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文本 &#124;'
- en: '| 9DoF | rigid | estimation | source |'
  id: totrans-825
  prefs: []
  type: TYPE_TB
  zh: '| 9DoF | 刚性 | 估计 | 来源 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段，&#124;'
- en: '&#124; S+P &#124;'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+P &#124;'
- en: '| general | 82.2 | 58.3 |'
  id: totrans-828
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 82.2 | 58.3 |'
- en: 4.1.1 NOCS Shape Alignment Methods
  id: totrans-829
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 NOCS 形状对齐方法
- en: 'As a pioneering work, Tian *et al.*[[72](#bib.bib72)] first extracted the shape
    prior in offline mode, which is used to represent the mean shape of a category
    of objects. For example, mugs are composed of a cylindrical cup body and an arc-shaped
    handle. Next, they introduced a shape prior deformation network for the intra-class
    unseen object to reconstruct its NOCS shape. Finally, the Umeyama algorithm[[197](#bib.bib197)]
    is employed to solve the object pose by aligning the NOCS shape and the object
    point cloud. Following Tian *et al.*[[72](#bib.bib72)], some methods aim to reconstruct
    the NOCS shape more accurately. Specifically, Wang *et al.*[[198](#bib.bib198)]
    designed a recurrent reconstruction network to iteratively refine the reconstructed
    NOCS shape. Further, Chen *et al.*[[23](#bib.bib23)] adjusted the shape prior
    dynamically by using the structure similarity between the RGBD image and the shape
    prior. Zou *et al.*[[199](#bib.bib199)] proposed two multi-scale transformer-based
    networks (Pixelformer and Pointformer) for extracting RGB and point cloud features,
    and subsequently merging them for shape prior deformation. Different from the
    previous methods, Fan *et al.*[[223](#bib.bib223)] introduced an adversarial canonical
    representation reconstruction framework, which includes a reconstructor and a
    discriminator of NOCS representation. Specifically, the reconstructor mainly consists
    of a pose-irrelevant module and a relational reconstruction module to reduce the
    sensitivity to rotation and translation, as well as to generate high-quality features,
    respectively. Then, the discriminator is used to guide the reconstructor to generate
    realistic NOCS representations. Nie *et al.*[[224](#bib.bib224)] improved the
    accuracy of pose estimation via geometry-informed instance-specific priors and
    multi-stage shape reconstruction. More recently, Zhou *et al.*[[225](#bib.bib225)]
    designed a two-stage pipeline consisting of deformation and registration to improve
    accuracy. Zou *et al.*[[226](#bib.bib226)] introduced a graph-guided point transformer
    consisting of a graph-guided attention encoder and an iterative non-parametric
    decoder to further extract the point cloud feature. In addition, Li *et al.*[[227](#bib.bib227)]
    leveraged discrepancies in instance-category structures alongside potential geometric-semantic
    associations to better investigate intra-class shape information. Yu *et al.*[[228](#bib.bib228)]
    further divided the NOCS shape reconstruction process into three parts: coarse
    deformation, fine deformation, and recurrent refinement to enhance the accuracy
    of NOCS shape reconstruction.'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 作为开创性工作，田 *等*[[72](#bib.bib72)] 首次在离线模式中提取了形状先验，用于表示一个类别物体的平均形状。例如，杯子由圆柱形杯体和弧形手柄组成。接下来，他们引入了一个形状先验变形网络，用于内部类别未知物体以重建其NOCS形状。最后，使用Umeyama算法[[197](#bib.bib197)]
    通过对齐NOCS形状和物体点云来解决物体姿态。继田 *等*[[72](#bib.bib72)]之后，一些方法旨在更准确地重建NOCS形状。具体而言，王 *等*[[198](#bib.bib198)]
    设计了一个递归重建网络，以迭代地优化重建的NOCS形状。进一步地，陈 *等*[[23](#bib.bib23)] 通过利用RGBD图像和形状先验之间的结构相似性动态调整形状先验。邹
    *等*[[199](#bib.bib199)] 提出了两个基于多尺度变换器的网络（Pixelformer和Pointformer）来提取RGB和点云特征，并随后将其合并用于形状先验变形。与以前的方法不同，范
    *等*[[223](#bib.bib223)] 引入了一个对抗性规范化表示重建框架，其中包括一个NOCS表示的重建器和判别器。具体来说，重建器主要由一个与姿态无关的模块和一个关系重建模块组成，分别用于减少对旋转和位移的敏感性，以及生成高质量特征。然后，判别器用于指导重建器生成逼真的NOCS表示。聂
    *等*[[224](#bib.bib224)] 通过几何信息实例特定先验和多阶段形状重建提高了姿态估计的准确性。最近，周 *等*[[225](#bib.bib225)]
    设计了一个由变形和配准组成的两阶段管道，以提高准确性。邹 *等*[[226](#bib.bib226)] 引入了一个图引导点变换器，包含图引导注意力编码器和迭代非参数解码器，以进一步提取点云特征。此外，李
    *等*[[227](#bib.bib227)] 利用实例类别结构中的差异以及潜在的几何语义关联来更好地研究内部类别形状信息。余 *等*[[228](#bib.bib228)]
    进一步将NOCS形状重建过程分为三个部分：粗变形、精变形和递归优化，以增强NOCS形状重建的准确性。
- en: Given that the annotation of ground-truth object pose is time-consuming, He
    *et al.*[[229](#bib.bib229)] explored a self-supervised method via enforcing the
    geometric consistency between point cloud and category prior mesh, avoiding using
    the real-world pose annotation. Further, Li *et al.*[[230](#bib.bib230)] first
    extracted semantic primitives via a part segmentation network, and leveraged semantic
    primitives to compute SIM(3)-invariant shape descriptor to generate the optimized
    shape. Then, the Umeyama algorithm[[197](#bib.bib197)] is utilized to recover
    the object pose. Through this approach, they achieved domain generalization, bridging
    the gap between synthesis and real-world application.
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于真实对象姿态的标注非常耗时，He *et al.*[[229](#bib.bib229)] 通过强制点云与类别先验网格之间的几何一致性探索了一种自监督方法，从而避免了使用真实世界的姿态标注。进一步地，Li
    *et al.*[[230](#bib.bib230)] 首先通过部件分割网络提取了语义原语，并利用这些语义原语计算了SIM(3)-不变形状描述符，以生成优化后的形状。随后，使用了Umeyama算法[[197](#bib.bib197)]
    来恢复对象姿态。通过这种方法，他们实现了领域泛化，缩小了合成与现实世界应用之间的差距。
- en: Depth images may be unavailable in some challenging scenes (*e.g.*, under strong
    or low light conditions). Therefore, achieving monocular category-level object
    pose estimation is of great significance across various applications. Fan *et
    al.* [[200](#bib.bib200)] directly predicted object-level depth and NOCS shape
    from a monocular RGB image by deforming the shape prior, and subsequently leveraged
    the Umeyama algorithm [[197](#bib.bib197)] to solve the object pose. Unlike [[200](#bib.bib200)],
    Wei *et al.* [[201](#bib.bib201)] estimated the 2.5D sketch and separated scale
    recovery using the shape prior. They then reconstructed the NOCS shape, employing
    the RANSAC [[73](#bib.bib73)] algorithm to remove outliers, before utilizing the
    PnP algorithm for recovering the object pose. For transparent objects, Chen *et
    al.* [[231](#bib.bib231)] proposed a new solution based on stereo vision, which
    defines a back-view NOCS map to tackle the problem of image content aliasing.
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些具有挑战性的场景中（*例如*，在强光或弱光条件下），深度图像可能不可用。因此，实现单目类别级对象姿态估计在各种应用中具有重要意义。Fan *et
    al.*[[200](#bib.bib200)] 通过变形形状先验直接从单目RGB图像中预测了对象级别的深度和NOCS形状，随后利用Umeyama算法[[197](#bib.bib197)]
    来解决对象姿态问题。与[[200](#bib.bib200)] 不同，Wei *et al.*[[201](#bib.bib201)] 使用形状先验估计了2.5D草图，并分离了尺度恢复。他们随后重建了NOCS形状，采用了RANSAC
    [[73](#bib.bib73)] 算法来去除离群值，然后使用PnP算法来恢复对象姿态。对于透明物体，Chen *et al.*[[231](#bib.bib231)]
    提出了基于立体视觉的新解决方案，该方案定义了一个反向视图NOCS地图来解决图像内容混叠的问题。
- en: In general, although these NOCS shape alignment methods can recover the object
    pose, the alignment process is non-differentiable and is not integrated into the
    learning process. Thus, errors in predicting the NOCS shape/map have a significant
    impact on the accuracy of pose estimation.
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，虽然这些NOCS形状对齐方法可以恢复对象姿态，但对齐过程是非可微的，并且未集成到学习过程中。因此，预测NOCS形状/地图的误差对姿态估计的准确性有显著影响。
- en: 4.1.2 Direct Regress Pose Methods
  id: totrans-834
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 直接回归姿态方法
- en: Due to the non-differentiable nature of the NOCS shape alignment process, several
    direct regression-based pose methods have been proposed recently to enable end-to-end
    training. Irshad *et al.*[[202](#bib.bib202)] treated object instances as spatial
    centers and proposed an end-to-end method that combines object detection, reconstruction,
    and pose estimation. Wang *et al.*[[232](#bib.bib232)] developed a deformable
    template field to decouple shape and pose deformation, improving the accuracy
    of shape reconstruction and pose estimation. On the other hand, Zhang *et al.*[[204](#bib.bib204)]
    proposed a symmetry-aware shape prior deformation method, which integrates shape
    prior into a direct pose estimation network. Further, Zhang *et al.*[[205](#bib.bib205)]
    introduced a geometry-guided residual object bounding box projection framework
    to address the challenge of insufficient pose-sensitive feature extraction. In
    order to obtain a more precise object pose, Liu *et al.*[[206](#bib.bib206)] designed
    CATRE, a pose refinement method based on the alignment of the shape prior and
    the object point cloud to refine the object pose estimated by the above methods.
    Zheng *et al.*[[233](#bib.bib233)] extended CATRE[[206](#bib.bib206)] to address
    the geometric variation problem by integrating hybrid scope layers and learnable
    affine transformations.
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 NOCS 形状对齐过程的非可微性质，最近提出了几种基于直接回归的姿态方法，以实现端到端训练。Irshad *等*[[202](#bib.bib202)]
    将目标实例视为空间中心，提出了一种结合目标检测、重建和姿态估计的端到端方法。Wang *等*[[232](#bib.bib232)] 开发了一个可变形模板场，以解耦形状和姿态变形，提高了形状重建和姿态估计的准确性。另一方面，Zhang
    *等*[[204](#bib.bib204)] 提出了一个对称感知形状先验变形方法，将形状先验集成到直接姿态估计网络中。此外，Zhang *等*[[205](#bib.bib205)]
    引入了一个几何引导的残差对象边界框投影框架，以解决姿态敏感特征提取不足的问题。为了获得更精确的目标姿态，Liu *等*[[206](#bib.bib206)]
    设计了CATRE，一种基于形状先验和目标点云对齐的姿态优化方法，以优化上述方法估计的目标姿态。Zheng *等*[[233](#bib.bib233)] 扩展了CATRE[[206](#bib.bib206)]，通过集成混合范围层和可学习的仿射变换来解决几何变异问题。
- en: Due to the extensive manual effort required for annotating real-world training
    data, Lin *et al.*[[203](#bib.bib203)] explored the shape alignment of each intra-class
    unseen instance against its corresponding category-level shape prior, implicitly
    representing its 3D rotation. This approach facilitates domain generalization
    from synthesis to real-world scenarios. Further, Ze *et al.*[[55](#bib.bib55)]
    proposed a novel framework based on pose and shape differentiable rendering to
    achieve domain adaptation object pose estimation. In addition, they collected
    a large Wild6D dataset for category-level object pose estimation in the wild.
    Following Ze *et al.*[[55](#bib.bib55)], Zhang *et al.*[[234](#bib.bib234)] introduced
    2D-3D and 3D-2D geometry correspondences to enhance the ability of domain adaptation.
    Different from the previous approaches, Remus *et al.*[[235](#bib.bib235)] leveraged
    instance-level methods for domain-generalized category-level object pose estimation
    via a single RGB image. Lin *et al.*[[24](#bib.bib24)] proposed a deep prior deformation-based
    network and leveraged a parallel learning scheme to achieve domain generalization.
    More recently, Liu *et al.*[[207](#bib.bib207)] designed a multi-hypothesis consistency
    learning framework. This framework addresses the uncertainty problem and reduces
    the domain gap between synthetic and real-world datasets by employing multiple
    feature extraction and fusion techniques.
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 由于标注真实世界训练数据需要大量人工努力，Lin *等*[[203](#bib.bib203)] 探索了每个类别内部未见实例与其相应类别级形状先验的形状对齐，隐含表示其3D旋转。这种方法促进了从合成到真实世界场景的领域泛化。此外，Ze
    *等*[[55](#bib.bib55)] 提出了基于姿态和形状可微渲染的创新框架，以实现领域适应目标姿态估计。此外，他们还收集了大量 Wild6D 数据集，用于野外类别级目标姿态估计。在
    Ze *等*[[55](#bib.bib55)] 的基础上，Zhang *等*[[234](#bib.bib234)] 引入了2D-3D和3D-2D几何对应关系，以增强领域适应的能力。不同于之前的方法，Remus
    *等*[[235](#bib.bib235)] 利用实例级方法，通过单张RGB图像进行领域泛化的类别级目标姿态估计。Lin *等*[[24](#bib.bib24)]
    提出了基于深度先验变形的网络，并利用并行学习方案实现领域泛化。最近，Liu *等*[[207](#bib.bib207)] 设计了一个多假设一致性学习框架。该框架通过采用多种特征提取和融合技术，解决了不确定性问题，并减少了合成数据集与真实世界数据集之间的领域差距。
- en: Overall, while the shape prior-based methods mentioned above significantly improve
    pose estimation performance, obtaining the shape priors requires constructing
    category-level CAD model libraries and subsequently training a network, which
    is both cumbersome and time-consuming.
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，虽然上述基于形状先验的方法显著提高了姿态估计的性能，但获得形状先验需要构建类别级的CAD模型库，并随后训练网络，这既繁琐又耗时。
- en: 4.2 Shape Prior-Free Methods
  id: totrans-838
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 无形状先验方法
- en: 'Shape prior-free methods do not rely on using shape priors and thus have better
    generalization capabilities. These methods can be divided into three main categories:
    depth-guided geometry-aware (Sec. [4.2.1](#S4.SS2.SSS1 "4.2.1 Depth-Guided Geometry-Aware
    Methods ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")), RGBD-guided
    semantic and geometry fusion (Sec. [4.2.2](#S4.SS2.SSS2 "4.2.2 RGBD-Guided Semantic
    and Geometry Fusion Methods ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")), and other (Sec. [4.2.3](#S4.SS2.SSS3 "4.2.3 Others ‣ 4.2 Shape Prior-Free
    Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) methods. The illustration of the first
    two categories is shown in Fig. [5](#S4.F5 "Figure 5 ‣ 4 Category-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey").'
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: '无需形状先验的方法不依赖于形状先验，因此具有更好的泛化能力。这些方法可以分为三类：深度引导的几何感知（第[4.2.1](#S4.SS2.SSS1 "4.2.1
    Depth-Guided Geometry-Aware Methods ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")节）、RGBD引导的语义和几何融合（第[4.2.2](#S4.SS2.SSS2 "4.2.2 RGBD-Guided Semantic and
    Geometry Fusion Methods ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")节）以及其他（第[4.2.3](#S4.SS2.SSS3 "4.2.3 Others ‣ 4.2 Shape Prior-Free Methods
    ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")节）方法。前两类方法的示意图见图[5](#S4.F5 "Figure 5 ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")。'
- en: 4.2.1 Depth-Guided Geometry-Aware Methods
  id: totrans-840
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 深度引导的几何感知方法
- en: Thanks to the rapid development of 3D Graph Convolution (3DGC), Chen *et al.*[[209](#bib.bib209)]
    leveraged 3DGC and introduced a fast shape-based method, which consists of an
    RGB-based network to achieve 2D object detection, a shape-based network for 3D
    segmentation and rotation regression, and a residual-based network for translation
    and size regression. Inspired by Chen *et al.*[[209](#bib.bib209)], Liu *et al.*[[236](#bib.bib236)]
    improved the network with structure encoder and reasoning attention. Further,
    Di *et al.*[[25](#bib.bib25)] proposed a geometry-guided point-wise voting method
    that exploits geometric insights to enhance the learning of pose-sensitive features.
    Specifically, they designed a symmetry-aware point cloud reconstruction network
    and introduced a point-wise bounding box voting mechanism during training to add
    additional geometric guidance. Due to the translation and scale invariant properties
    of the 3DGC, these methods are limited in perceiving object translation and size
    information. Based on this, Zheng *et al.*[[26](#bib.bib26)] further designed
    a hybrid scope feature extraction layer, which can simultaneously perceive global
    and local geometric structures and encode size and translation information.
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 由于3D图卷积（3DGC）的快速发展，Chen *等人*[[209](#bib.bib209)] 利用3DGC并提出了一种快速的基于形状的方法，该方法包括一个基于RGB的网络用于实现2D目标检测，一个基于形状的网络用于3D分割和旋转回归，以及一个基于残差的网络用于平移和大小回归。受Chen
    *等人*[[209](#bib.bib209)] 的启发，Liu *等人*[[236](#bib.bib236)] 通过结构编码器和推理注意力改进了网络。此外，Di
    *等人*[[25](#bib.bib25)] 提出了一个几何引导的逐点投票方法，利用几何见解增强姿态敏感特征的学习。具体来说，他们设计了一个对称感知点云重建网络，并在训练期间引入了逐点边界框投票机制，以增加额外的几何引导。由于3DGC的平移和尺度不变特性，这些方法在感知目标平移和大小信息时受到限制。基于此，Zheng
    *等人*[[26](#bib.bib26)] 进一步设计了一个混合范围特征提取层，可以同时感知全局和局部几何结构，并编码大小和翻译信息。
- en: Besides the above 3DGC-based methods, Deng *et al.*[[237](#bib.bib237)] combined
    a category-level auto-encoder with a particle filter framework to achieve object
    pose estimation and tracking. Wang *et al.*[[238](#bib.bib238)] leveraged learnable
    sparse queries as implicit prior to perform deformation and matching for pose
    estimation. In addition, Wan *et al.*[[239](#bib.bib239)] developed a semantically-aware
    object coordinate space to address the semantically incoherent problem of NOCS[[22](#bib.bib22)].
    More recently, Zhang *et al.*[[212](#bib.bib212)] proposed a scored-based diffusion
    model to address the multi-hypothesis problem in symmetric objects and partial
    point clouds. They first leveraged the scored-based diffusion model to generate
    multiple pose candidates, and then utilized an energy-based diffusion model to
    remove abnormal poses. On the other hand, Lin *et al.*[[240](#bib.bib240)] first
    introduced an instance-adaptive keypoints detection method and then designed a
    geometric-aware global and local features aggregation network based on the detected
    keypoints for pose and size estimation. Li *et al.*[[241](#bib.bib241)] leveraged
    category-level method to determine part object poses for assembling multi-part
    multi-joint 3D shape.
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述基于3DGC的方法，邓*等*[[237](#bib.bib237)] 将类别级自动编码器与粒子滤波框架结合，实现了物体姿态估计和跟踪。王*等*[[238](#bib.bib238)]
    利用可学习的稀疏查询作为隐式先验，进行形变和匹配以进行姿态估计。此外，万*等*[[239](#bib.bib239)] 开发了一个语义感知的物体坐标空间，以解决NOCS[[22](#bib.bib22)]的语义不一致问题。最近，张*等*[[212](#bib.bib212)]
    提出了一个基于评分的扩散模型，以解决对称物体和部分点云中的多假设问题。他们首先利用评分的扩散模型生成多个姿态候选，然后使用基于能量的扩散模型去除异常姿态。另一方面，林*等*[[240](#bib.bib240)]
    首次引入了一种实例自适应关键点检测方法，然后设计了一个基于检测到的关键点的几何感知全局和局部特征聚合网络，用于姿态和尺寸估计。李*等*[[241](#bib.bib241)]
    利用类别级方法来确定部件物体姿态，以组装多部件多关节3D形状。
- en: To perform pose estimation on articulated objects, Li *et al.*[[208](#bib.bib208)]
    inspired by Wang *et al.*[[22](#bib.bib22)], introduced a standard representation
    for different articulated objects within a category by designing an articulation-aware
    normalized coordinate space hierarchy, which simultaneously constructs a canonical
    object space and a set of canonical part spaces. Weng *et al.*[[210](#bib.bib210)]
    further proposed CAPTRA, a unified framework that enables 9DoF pose tracking of
    rigid and articulated objects simultaneously. Due to the nearly unlimited freedom
    of garments and extreme self-occlusion, Chi *et al.*[[242](#bib.bib242)] introduce
    GarmentNets, which conceptualizes deformable object pose estimation as a shape
    completion problem within a canonical space. More recently, Liu *et al.*[[243](#bib.bib243)]
    developed a reinforcement learning-based pipeline to predict 9DoF articulated
    object pose via fitting joint states through reinforced agent training. Further,
    Liu *et al.*[[244](#bib.bib244)] learned part-level SE(3)-equivariant features
    via a pose-aware equivariant point convolution operator to address the issue of
    self-supervised articulated object pose estimation.
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对关节物体进行姿态估计，李*等*[[208](#bib.bib208)] 受王*等*[[22](#bib.bib22)] 的启发，通过设计一个关节感知的标准化坐标空间层次，引入了不同关节物体的标准表示，该层次同时构建了一个典型的物体空间和一组典型的部件空间。翁*等*[[210](#bib.bib210)]
    进一步提出了CAPTRA，这是一个统一框架，能够同时实现刚性和关节物体的9DoF姿态跟踪。由于服装的几乎无限自由度和极端的自遮挡，池*等*[[242](#bib.bib242)]
    引入了GarmentNets，将可变形物体姿态估计概念化为一个在标准空间内的形状完成问题。最近，刘*等*[[243](#bib.bib243)] 开发了一种基于强化学习的流程，通过强化学习代理训练拟合关节状态来预测9DoF关节物体姿态。此外，刘*等*[[244](#bib.bib244)]
    通过姿态感知的等变点卷积算子学习了部件级SE(3)-等变特征，以解决自监督关节物体姿态估计的问题。
- en: To avoid using extensive real-world labeled data for training, Li *et al.*[[245](#bib.bib245)]
    used SE(3) equivariant point cloud networks for self-supervised object pose estimation.
    You *et al.*[[211](#bib.bib211)] introduced a category-level point pair feature
    voting method to reduce the impact of synthetic to real-world domain gap, achieving
    generalizable object pose estimation in the wild.
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免使用大量的真实标签数据进行训练，李*等*[[245](#bib.bib245)] 使用了SE(3)等变点云网络进行自监督物体姿态估计。尤*等*[[211](#bib.bib211)]
    引入了一种类别级点对特征投票方法，以减少合成到真实世界领域间隙的影响，实现了在实际环境中的可泛化物体姿态估计。
- en: In general, these methods fully extract the pose-related geometric features.
    However, the absence of semantic information limits their better performance.
    Appropriate fusion of semantic and geometric information can significantly improve
    the robustness of pose estimation.
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些方法充分提取了与姿态相关的几何特征。然而，缺乏语义信息限制了它们的更好性能。适当融合语义和几何信息可以显著提高姿态估计的鲁棒性。
- en: 4.2.2 RGBD-Guided Semantic and Geometry Fusion Methods
  id: totrans-846
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 RGBD 引导的语义和几何融合方法
- en: As a groundbreaking research, Wang *et al.*[[22](#bib.bib22)] designed a normalized
    object coordinate space to provide a canonical representation for a category of
    objects. They first predicted the class label, mask, and NOCS map of the intra-class
    unseen object. Then, they utilized the Umeyama algorithm [[197](#bib.bib197)]
    to solve object pose by aligning the NOCS map with the object point cloud. To
    handle various shape changes of intra-class objects, Chen *et al.*[[246](#bib.bib246)]
    learned a canonical shape space as a unified representation. On the other hand,
    Lin *et al.*[[247](#bib.bib247)] explored the applicability of sparse steerable
    convolution (SSC) to object pose estimation and proposed an SSC-based pipeline.
    Further, Lin *et al.*[[214](#bib.bib214)] proposed a dual pose network, which
    consists of a shared pose encoder and two parallel explicit and implicit pose
    decoders. The implicit decoder can enforce predicted pose consistency when there
    are no CAD models during inference. Wang *et al.*[[248](#bib.bib248)] designed
    an attention-guided network with relation-aware and structure-aware for RGB image
    and point cloud features fusion. Very recently, Liu *et al.*[[27](#bib.bib27)]
    explored the necessity of shape priors for shape reconstruction of intra-class
    unseen objects. They demonstrated that the deformation process is more important
    than the shape prior and proposed a prior-free implicit space transformation network.
    Lin *et al.*[[219](#bib.bib219)] addressed the poor rotation estimation accuracy
    by decoupling the rotation estimation into viewpoint and in-plane rotation. In
    addition, they also proposed a spherical feature pyramid network based on spatial
    spherical convolution to process spherical signals. With the rapid development
    of the Large Vision Model (LVM), Chen *et al.*[[220](#bib.bib220)] further leveraged
    the LVM DINOv2[[249](#bib.bib249)] to extract the SE(3)-consistent semantic features
    and fused them with object-specific hierarchical geometric features to encapsulate
    category-level information for rotation estimation.
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项开创性的研究，王*等*[[22](#bib.bib22)] 设计了一个归一化的物体坐标空间，为一类物体提供了规范化表示。他们首先预测了类标签、掩码和类内未见物体的
    NOCS 图。然后，他们利用 Umeyama 算法 [[197](#bib.bib197)] 通过将 NOCS 图与物体点云对齐来解决物体姿态问题。为了处理类内物体的各种形状变化，陈*等*[[246](#bib.bib246)]
    学习了一个作为统一表示的规范形状空间。另一方面，林*等*[[247](#bib.bib247)] 探索了稀疏可引导卷积（SSC）在物体姿态估计中的适用性，并提出了基于
    SSC 的管道。此外，林*等*[[214](#bib.bib214)] 提出了一个双姿态网络，由一个共享姿态编码器和两个并行的显式和隐式姿态解码器组成。当推理过程中没有
    CAD 模型时，隐式解码器可以强制预测姿态的一致性。王*等*[[248](#bib.bib248)] 设计了一个关注引导网络，具有关系感知和结构感知，用于
    RGB 图像和点云特征融合。最近，刘*等*[[27](#bib.bib27)] 探索了形状先验在类内未见物体形状重建中的必要性。他们展示了变形过程比形状先验更重要，并提出了一种无先验的隐式空间变换网络。林*等*[[219](#bib.bib219)]
    通过将旋转估计解耦为视角和平面内旋转，解决了旋转估计精度差的问题。此外，他们还提出了一种基于空间球面卷积的球面特征金字塔网络，用于处理球面信号。随着大型视觉模型（LVM）的快速发展，陈*等*[[220](#bib.bib220)]
    进一步利用 LVM DINOv2[[249](#bib.bib249)] 提取 SE(3) 一致的语义特征，并将其与物体特定的层次几何特征融合，以封装类别级信息用于旋转估计。
- en: Since the above methods still require a large amount of real-world annotated
    training data, their applicability in real-world scenes is limited. To this end,
    Peng *et al.*[[216](#bib.bib216)] proposed a real-world self-supervised training
    framework based on deep implicit shape representation. They leveraged the deep
    signed distance function[[250](#bib.bib250)] as a 3D representation to achieve
    domain adaptation from synthesis to the real world. In addition, Lee *et al.*[[217](#bib.bib217)]
    introduced a teacher-student self-supervised learning mechanism. They used supervised
    training in the source domain and self-supervised training in the target domain,
    effectively achieving domain adaptation. Recently, Lee *et al.*[[218](#bib.bib218)]
    further proposed a test-time adaptation framework for domain-generalized category-level
    object pose estimation. Specifically, they first trained the model using labeled
    synthetic data and then leveraged the pre-trained model for test-time adaptation
    in the real world during inference.
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述方法仍然需要大量的真实世界标注训练数据，它们在真实场景中的适用性有限。为此，彭*等人*[[216](#bib.bib216)] 提出了基于深度隐式形状表示的真实世界自监督训练框架。他们利用深度符号距离函数[[250](#bib.bib250)]
    作为3D表示，实现了从合成到真实世界的领域适应。此外，李*等人*[[217](#bib.bib217)] 引入了师生自监督学习机制。他们在源领域进行了监督训练，在目标领域进行了自监督训练，有效地实现了领域适应。最近，李*等人*[[218](#bib.bib218)]
    进一步提出了一种用于领域泛化类别级物体姿态估计的测试时适应框架。具体而言，他们首先使用带标签的合成数据训练模型，然后利用预训练模型在推断过程中进行真实世界的测试时适应。
- en: To improve the running speed of the object pose estimation method, once the
    object pose of the first frame is acquired, continuous spatio-temporal information
    can be utilized to track the object pose. Wang *et al.*[[213](#bib.bib213)] proposed
    an anchors-based object pose tracking method. They first detected the anchors
    of each frame as the keypoints, and then solved the relative object pose through
    the keypoints correspondence. Wen *et al.*[[215](#bib.bib215)] first obtained
    continuous frame RGBD masks through the video segmentation network, transductive-VOS
    [[251](#bib.bib251)], and then leveraged LF-Net [[252](#bib.bib252)] for generalized
    keypoints detection. Next, they matched keypoints between consecutive frames and
    performed coarse registration to estimate the initial relative pose. Finally,
    a memory-augmented pose graph optimization method is proposed for continuous pose
    tracking.
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高物体姿态估计方法的运行速度，一旦获取了第一帧的物体姿态，可以利用连续的时空信息来跟踪物体姿态。王*等人*[[213](#bib.bib213)]
    提出了基于锚点的物体姿态跟踪方法。他们首先检测每一帧的锚点作为关键点，然后通过关键点对应解决相对物体姿态。温*等人*[[215](#bib.bib215)]
    首先通过视频分割网络转导VOS [[251](#bib.bib251)] 获得连续帧的RGBD掩码，然后利用LF-Net [[252](#bib.bib252)]
    进行广义关键点检测。接着，他们在连续帧之间匹配关键点，并进行粗略配准以估计初始相对姿态。最后，提出了一种记忆增强的姿态图优化方法用于连续姿态跟踪。
- en: Overall, these RGBD-guided semantic and geometry fusion methods achieve superior
    performance. However, if the input depth image contains errors, the accuracy of
    pose estimation can significantly decrease. Hence, ensuring robustness in pose
    estimation when dealing with erroneous or missing depth images is crucial.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，这些RGBD引导的语义和几何融合方法表现优越。然而，如果输入的深度图像存在错误，姿态估计的准确性可能会显著降低。因此，在处理有错误或缺失的深度图像时，确保姿态估计的鲁棒性至关重要。
- en: 4.2.3 Others
  id: totrans-851
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 其他
- en: Since most mobile devices are not equipped with depth cameras, Chen *et al.*[[253](#bib.bib253)]
    incorporated a neural synthesis module with a gradient-based fitting procedure
    to simultaneously predict object shape and pose, achieving monocular object pose
    estimation. Lee *et al.*[[221](#bib.bib221)] estimated the NOCS shape and the
    metric scale shape of the object, and performed a similarity transformation between
    them to solve the object pose and size. Further, Yen-Chen *et al.*[[254](#bib.bib254)]
    inverted neural radiance fields for monocular category-level pose estimation.
    Different from the previous methods, Lin *et al.*[[255](#bib.bib255)] proposed
    a keypoint-based single-stage pipeline via a single RGB image. Guo *et al.*[[256](#bib.bib256)]
    redefined the monocular category-level object pose estimation problem from a long-horizon
    visual navigation perspective. On the other hand, Ma *et al.*[[257](#bib.bib257)]
    enhanced the robustness of the monocular method in occlusion scenes through coarse-to-fine
    rendering of neural features. Given that transparent instances lack both color
    and depth information, Zhang *et al.*[[258](#bib.bib258)] proposed to utilize
    depth completion and surface normal estimation to achieve category-level pose
    estimation for transparent instances.
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数移动设备未配备深度相机，Chen *等人*[[253](#bib.bib253)] 结合了具有梯度拟合过程的神经合成模块，同时预测物体形状和姿态，实现了单目物体姿态估计。Lee
    *等人*[[221](#bib.bib221)] 估计了物体的NOCS形状和度量尺度形状，并在它们之间进行相似性变换，以解决物体的姿态和尺寸问题。此外，Yen-Chen
    *等人*[[254](#bib.bib254)] 对单目类别级姿态估计进行神经辐射场反演。与以前的方法不同，Lin *等人*[[255](#bib.bib255)]
    提出了通过单个RGB图像实现的基于关键点的单阶段管道。Guo *等人*[[256](#bib.bib256)] 从长视野视觉导航的角度重新定义了单目类别级物体姿态估计问题。另一方面，Ma
    *等人*[[257](#bib.bib257)] 通过对神经特征的粗到细渲染增强了单目方法在遮挡场景中的鲁棒性。考虑到透明实例缺乏颜色和深度信息，Zhang
    *等人*[[258](#bib.bib258)] 提出了利用深度补全和表面法线估计来实现透明实例的类别级姿态估计。
- en: In order to improve the running efficiency of the monocular method, Lin *et
    al.*[[259](#bib.bib259)] developed a keypoint-based monocular object pose tracking
    approach. This approach demonstrates the significance of integrating uncertainty
    estimation using a tracklet-conditioned deep network and probabilistic filtering.
    Following Lin *et al.*[[259](#bib.bib259)], Yu *et al.*[[260](#bib.bib260)] further
    improved the pose tracking accuracy through a network that combines convolutions
    and transformers.
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高单目方法的运行效率，Lin *等人*[[259](#bib.bib259)] 开发了一种基于关键点的单目物体姿态跟踪方法。该方法展示了通过使用轨迹条件深度网络和概率滤波整合不确定性估计的重要性。在Lin
    *等人*[[259](#bib.bib259)] 之后，Yu *等人*[[260](#bib.bib260)] 通过结合卷积和变换器的网络进一步提高了姿态跟踪的准确性。
- en: To further improve the generalization of category-level methods, Goodwin *et
    al.*[[261](#bib.bib261)] introduced a reference image-based zero-shot approach,
    which first extracts spatial feature descriptors and builds cyclical descriptor
    distances. Then, they established the top-k semantic correspondences for pose
    estimation. Zaccaria *et al.*[[262](#bib.bib262)] proposed a self-supervised framework
    via optical flow consistency. Very recently, Cai *et al.*[[263](#bib.bib263)]
    developed an open-vocabulary framework that aims to generalize to unseen categories
    using textual prompts in unseen scene images. Felice *et al.*[[264](#bib.bib264)]
    explored zero-shot novel view synthesis based on a diffusion model for 3D object
    reconstruction, and recovered the object pose through correspondences. Lin *et
    al.*[[222](#bib.bib222)] used a pre-trained vision-language model to make full
    use of rich semantic knowledge and align the representations of the three modalities
    (image, point cloud, and text) in the feature space through multi-modal contrastive
    learning.
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高类别级方法的泛化能力，Goodwin *等人*[[261](#bib.bib261)] 引入了一种基于参考图像的零样本方法，该方法首先提取空间特征描述符并建立循环描述符距离。然后，他们建立了用于姿态估计的top-k语义对应关系。Zaccaria
    *等人*[[262](#bib.bib262)] 提出了通过光流一致性进行自监督学习的框架。最近，Cai *等人*[[263](#bib.bib263)]
    开发了一个开放词汇框架，旨在通过在未见场景图像中使用文本提示来泛化到未见类别。Felice *等人*[[264](#bib.bib264)] 探索了基于扩散模型的零样本新视角合成方法用于3D物体重建，并通过对应关系恢复物体姿态。Lin
    *等人*[[222](#bib.bib222)] 使用预训练的视觉-语言模型充分利用丰富的语义知识，并通过多模态对比学习在特征空间中对齐三种模态（图像、点云和文本）的表示。
- en: On the whole, these shape prior-free methods circumvent the reliance on shape
    priors and further improve the generalization ability of category-level object
    pose estimation methods. Nevertheless, these methods are limited to generalizing
    within intra-class unseen objects. For objects of different categories, the training
    data need to be collected and the models need to be retrained, which remains a
    significant limitation.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这些不依赖形状先验的方法规避了对形状先验的依赖，进一步提高了类别级物体姿态估计方法的泛化能力。然而，这些方法仅限于在类内未见物体的泛化。对于不同类别的物体，需要收集训练数据并重新训练模型，这仍然是一个显著的限制。
- en: 5 Unseen Object Pose Estimation
  id: totrans-856
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 未见物体姿态估计
- en: 'Unseen object pose estimation methods can generalize to unseen objects without
    the need for retraining. Point Pair Features (PPF) [[11](#bib.bib11)] is a classical
    method for unseen object pose estimation that utilizes oriented point pair features
    to build global model description and a fast voting scheme to match locally. The
    final pose is solved by pose clustering and iterative closest point [[137](#bib.bib137)]
    refinement. However, PPF suffers from low accuracy and slow runtime, limiting
    its applicability. In contrast, deep learning-based methods leverage neural networks
    to learn more complex features from data without specifically designed feature
    engineering, thus enhancing accuracy and efficiency. In this section, we review
    the deep learning-based unseen object pose estimation methods and classify them
    into CAD model-based (Sec. [5.1](#S5.SS1 "5.1 CAD Model-Based Methods ‣ 5 Unseen
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) and manual reference view-based (Sec. [5.2](#S5.SS2 "5.2 Manual Reference
    View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) methods. The illustration of these
    two categories of methods is shown in Fig. [6](#S5.F6 "Figure 6 ‣ 5.1 CAD Model-Based
    Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey").'
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: 未见物体姿态估计方法可以在无需重新训练的情况下对未见物体进行泛化。点对特征（PPF）[[11](#bib.bib11)] 是一种经典的未见物体姿态估计方法，它利用定向点对特征构建全局模型描述，并使用快速投票方案进行局部匹配。最终姿态通过姿态聚类和迭代最近点[[137](#bib.bib137)]
    精化来解决。然而，PPF 的准确性低且运行速度慢，限制了其适用性。相比之下，基于深度学习的方法利用神经网络从数据中学习更复杂的特征，而无需特意设计的特征工程，从而提高了准确性和效率。在本节中，我们回顾了基于深度学习的未见物体姿态估计方法，并将其分为基于
    CAD 模型的方法（第 [5.1](#S5.SS1 "5.1 基于 CAD 模型的方法 ‣ 5 未见物体姿态估计 ‣ 深度学习基础物体姿态估计：综合调查")
    节）和基于人工参考视图的方法（第 [5.2](#S5.SS2 "5.2 基于人工参考视图的方法 ‣ 5 未见物体姿态估计 ‣ 深度学习基础物体姿态估计：综合调查")
    节）。这两类方法的示意图见图 [6](#S5.F6 "图 6 ‣ 5.1 基于 CAD 模型的方法 ‣ 5 未见物体姿态估计 ‣ 深度学习基础物体姿态估计：综合调查")。
- en: 5.1 CAD Model-Based Methods
  id: totrans-858
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基于 CAD 模型的方法
- en: 'The CAD model-based methods involve utilizing the object CAD model as prior
    knowledge during the process of estimating the pose of an unseen object. These
    methods can be further categorized into feature matching-based and template matching-based
    methods. Feature matching-based methods (Sec. [5.1.1](#S5.SS1.SSS1 "5.1.1 Feature
    Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")) focus
    on designing a network to match features between the CAD model and the query image,
    establishing 2D-3D or 3D-3D correspondences, and solving the pose by the PnP algorithm
    or least squares method. Template matching-based methods (Sec. [5.1.2](#S5.SS1.SSS2
    "5.1.2 Template Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) utilize rendered templates from the CAD model for retrieval. The initial
    pose is acquired based on the most similar template, and further refinement is
    necessary using a refiner to obtain a more accurate pose. The illustration of
    these two categories of methods is shown in Fig. [6](#S5.F6 "Figure 6 ‣ 5.1 CAD
    Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey"). The characteristics and performance
    of some representative methods are shown in Table [III](#S5.T3 "TABLE III ‣ 5.1.1
    Feature Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey").'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 基于CAD模型的方法涉及在估计未见物体姿态的过程中利用物体CAD模型作为先验知识。这些方法可以进一步分类为基于特征匹配的方法和基于模板匹配的方法。基于特征匹配的方法（参见[5.1.1节](#S5.SS1.SSS1
    "5.1.1 特征匹配方法 ‣ 5.1 CAD模型基础的方法 ‣ 5 未见物体姿态估计 ‣ 基于深度学习的物体姿态估计：综合调查")）专注于设计一个网络来匹配CAD模型和查询图像之间的特征，建立2D-3D或3D-3D的对应关系，并通过PnP算法或最小二乘法求解姿态。基于模板匹配的方法（参见[5.1.2节](#S5.SS1.SSS2
    "5.1.2 模板匹配方法 ‣ 5.1 CAD模型基础的方法 ‣ 5 未见物体姿态估计 ‣ 基于深度学习的物体姿态估计：综合调查")）利用从CAD模型渲染的模板进行检索。初始姿态是基于最相似的模板获得的，随后需要使用精化器进行进一步的细化，以获得更准确的姿态。这两类方法的示意图见图[6](#S5.F6
    "图6 ‣ 5.1 CAD模型基础的方法 ‣ 5 未见物体姿态估计 ‣ 基于深度学习的物体姿态估计：综合调查")。一些代表性方法的特点和性能见表[III](#S5.T3
    "表III ‣ 5.1.1 特征匹配方法 ‣ 5.1 CAD模型基础的方法 ‣ 5 未见物体姿态估计 ‣ 基于深度学习的物体姿态估计：综合调查")。
- en: '![Refer to caption](img/8f5314bf8af0191f170ac1afb68891d9.png)'
  id: totrans-860
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/8f5314bf8af0191f170ac1afb68891d9.png)'
- en: 'Figure 6: Illustration of the CAD model-based (Sec. [5.1](#S5.SS1 "5.1 CAD
    Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) and manual reference view-based (Sec.
    [5.2](#S5.SS2 "5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object Pose
    Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"))
    methods for unseen object pose estimation. (Sec. [5.1](#S5.SS1 "5.1 CAD Model-Based
    Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")): The feature matching-based methods (Sec. [5.1.1](#S5.SS1.SSS1
    "5.1.1 Feature Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) focus on designing a network to match features between the CAD model
    and the query image, establishing correspondences (2D-3D or 3D-3D), and solving
    the pose using the PnP algorithm or least squares method. The template matching-based
    methods (Sec. [5.1.2](#S5.SS1.SSS2 "5.1.2 Template Matching-Based Methods ‣ 5.1
    CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")) utilize rendered templates from
    the CAD model for retrieval. The initial pose is acquired based on the most similar
    template, and further refinement is necessary using a refiner to obtain a more
    accurate pose. (Sec. [5.2](#S5.SS2 "5.2 Manual Reference View-Based Methods ‣
    5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")): There are two types of feature matching-based methods
    (Sec. [5.2.1](#S5.SS2.SSS1 "5.2.1 Feature Matching-Based Methods ‣ 5.2 Manual
    Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")). One involves extracting features
    from reference views and the query image, obtaining 3D-3D correspondences through
    a feature matching network. The other initially reconstructs the 3D object representation
    using the reference views and establishes the 2D-3D correspondences between the
    query image and the 3D representation. The object pose is solved using correspondence-based
    algorithms, like PnP or the least squares method. Template matching-based methods
    (Sec. [5.2.2](#S5.SS2.SSS2 "5.2.2 Template Matching-Based Methods ‣ 5.2 Manual
    Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")) also have two types. One reconstructs
    the 3D object representation using the reference views and then renders multiple
    templates. The initial pose is acquired by retrieving the most similar template
    and then refining it to get the final pose. The other directly uses the reference
    views as the templates for template matching.'
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6：展示了基于 CAD 模型（第 [5.1](#S5.SS1 "5.1 CAD Model-Based Methods ‣ 5 Unseen Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey") 节）和手动参考视图（第 [5.2](#S5.SS2 "5.2 Manual Reference View-Based Methods ‣
    5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey") 节）的方法用于未知物体姿态估计。第 [5.1](#S5.SS1 "5.1 CAD Model-Based
    Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey") 节：基于特征匹配的方法（第 [5.1.1](#S5.SS1.SSS1 "5.1.1 Feature Matching-Based
    Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey") 节）专注于设计网络以匹配 CAD
    模型和查询图像之间的特征，建立对应关系（2D-3D 或 3D-3D），并使用 PnP 算法或最小二乘法来解决姿态。基于模板匹配的方法（第 [5.1.2](#S5.SS1.SSS2
    "5.1.2 Template Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey") 节）利用来自 CAD 模型的渲染模板进行检索。初始姿态基于最相似的模板获取，并且需要使用精化器进行进一步调整以获得更准确的姿态。第 [5.2](#S5.SS2
    "5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey") 节：基于特征匹配的方法有两种类型（第
    [5.2.1](#S5.SS2.SSS1 "5.2.1 Feature Matching-Based Methods ‣ 5.2 Manual Reference
    View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey") 节）。一种是从参考视图和查询图像中提取特征，通过特征匹配网络获取 3D-3D
    对应关系。另一种是初步使用参考视图重建 3D 物体表示，并在查询图像和 3D 表示之间建立 2D-3D 对应关系。物体姿态使用基于对应关系的算法，如 PnP
    或最小二乘法来解决。基于模板匹配的方法（第 [5.2.2](#S5.SS2.SSS2 "5.2.2 Template Matching-Based Methods
    ‣ 5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣
    Deep Learning-Based Object Pose Estimation: A Comprehensive Survey") 节）也有两种类型。一种是使用参考视图重建
    3D 物体表示，然后渲染多个模板。通过检索最相似的模板来获取初始姿态，然后对其进行精化以获得最终姿态。另一种是直接使用参考视图作为模板进行模板匹配。'
- en: 5.1.1 Feature Matching-Based Methods
  id: totrans-862
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 基于特征匹配的方法
- en: As an early exploratory work, Pitteri *et al.*[[265](#bib.bib265)] proposed
    a 3DoF pose estimation approach that approximates object’s geometry using only
    the corner points of the CAD model. Nonetheless, it only works effectively on
    objects having specific corners. Hence, Pitteri *et al.*[[266](#bib.bib266)] further
    introduced an embedding that captures the local geometry of 3D points on the object
    surface. Matching these embeddings can create 2D-3D correspondences, and the pose
    is then determined using the PnP+RANSAC[[73](#bib.bib73)] algorithm. However,
    these methods only estimate the 3DoF pose.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 作为早期的探索性工作，皮特里*等人*[[265](#bib.bib265)] 提出了一个3DoF姿态估计方法，仅使用CAD模型的角点来近似物体的几何形状。然而，这只对具有特定角点的物体有效。因此，皮特里*等人*[[266](#bib.bib266)]
    进一步引入了一种嵌入，捕捉物体表面3D点的局部几何形状。匹配这些嵌入可以创建2D-3D对应关系，然后使用PnP+RANSAC[[73](#bib.bib73)]算法来确定姿态。然而，这些方法只能估计3DoF姿态。
- en: Gou *et al.*[[267](#bib.bib267)] defined the challenge of estimating the 6DoF
    pose of unseen objects, offering a baseline solution through the identification
    of 3D correspondences between object and scene point clouds. Similarly, Hagelskjær
    *et al.*[[268](#bib.bib268)] trained a network to match keypoints from the CAD
    model to the object point cloud. Yet, it focuses on bin picking with homogeneous
    bins, which only demonstrates that generalized pose estimation can achieve outstanding
    performance in restricted scenarios.
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: 勾*等人*[[267](#bib.bib267)] 定义了估计未知物体6DoF姿态的挑战，通过识别物体与场景点云之间的3D对应关系提供了基线解决方案。同样，哈格尔斯克雅*等人*[[268](#bib.bib268)]
    训练了一个网络，将CAD模型的关键点与物体点云进行匹配。然而，它主要关注于均质箱子的分拣，这只表明一般化的姿态估计在受限场景中可以实现出色的性能。
- en: Inspired by point cloud registration methods on unseen objects, Zhao *et al.*[[269](#bib.bib269)]
    proposed a geometry correspondence-based method using generic and object-agnostic
    geometry features to establish unambiguous and robust 3D-3D correspondences. Nevertheless,
    it still needs to get the class label and segmentation mask of unseen objects
    through other methods such as Mask-RCNN [[270](#bib.bib270)]. To this end, Chen
    *et al.*[[271](#bib.bib271)] explored a framework named ZeroPose, which realizes
    joint instance segmentation and pose estimation of unseen objects. Specifically,
    they utilized the foundation model SAM[[272](#bib.bib272)] to generate possible
    object proposals and adopted a template matching method to accomplish instance
    segmentation. After that, they developed a hierarchical geometric feature matching
    network based on GeoTransformer[[273](#bib.bib273)] to establish correspondences.
    Following ZeroPose, Lin *et al.*[[30](#bib.bib30)] devised a novel matching score
    in terms of semantics, appearance, and geometry to obtain better segmentation.
    As for pose estimation, they proposed a two-stage partial-to-partial point matching
    model to construct dense 3D-3D correspondence effectively.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 受到对未知物体的点云配准方法的启发，赵*等人*[[269](#bib.bib269)] 提出了基于几何对应关系的方法，使用通用且与物体无关的几何特征来建立明确且稳健的3D-3D对应关系。然而，它仍需要通过其他方法如Mask-RCNN[[270](#bib.bib270)]获取未知物体的类别标签和分割掩码。为此，陈*等人*[[271](#bib.bib271)]
    探索了一个名为ZeroPose的框架，实现了对未知物体的联合实例分割和姿态估计。具体而言，他们利用基础模型SAM[[272](#bib.bib272)] 生成可能的物体提案，并采用模板匹配方法完成实例分割。之后，他们基于GeoTransformer[[273](#bib.bib273)]
    开发了一个层次化几何特征匹配网络来建立对应关系。在ZeroPose之后，林*等人*[[30](#bib.bib30)] 设计了一种新的匹配评分，从语义、外观和几何角度获取更好的分割。至于姿态估计，他们提出了一个两阶段的部分到部分点匹配模型，以有效构建密集的3D-3D对应关系。
- en: Besides these methods employing geometry features, Caraffa *et al.*[[274](#bib.bib274)]
    devised a method that fuses visual and geometric features extracted from different
    pre-trained models to enhance pose prediction stability and accuracy. It is the
    first technique to estimate the unseen object pose by utilizing the synergy between
    geometric and vision foundation models. Additionally, Huang *et al.*[[275](#bib.bib275)]
    proposed a method for object pose prediction from RGBD images by combining 2D
    texture and 3D geometric cues.
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些使用几何特征的方法，卡拉法*等人*[[274](#bib.bib274)] 设计了一种方法，通过融合从不同预训练模型中提取的视觉和几何特征来增强姿态预测的稳定性和准确性。这是第一种通过利用几何和视觉基础模型的协同来估计未知物体姿态的技术。此外，黄*等人*[[275](#bib.bib275)]
    提出了通过结合2D纹理和3D几何线索，从RGBD图像中进行物体姿态预测的方法。
- en: To sum up, feature matching-based methods aim to extract generic object-agnostic
    features and achieve strong correspondences by matching these features. However,
    these methods require not only robust feature matching models but also tailored
    designs to enhance the representation of object features, presenting a significant
    challenge.
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，基于特征匹配的方法旨在提取通用的与对象无关的特征，并通过匹配这些特征实现强的对应关系。然而，这些方法不仅需要强大的特征匹配模型，还需要量身定制的设计来增强对象特征的表现，这带来了巨大的挑战。
- en: 'TABLE III: Representative CAD-based methods. Since the domain training paradigm
    of most unseen methods is domain generalization, we report 9 properties for each
    method, which have the same meanings as described in the caption of Table. [I](#S3.T1
    "TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods
    ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey"). D, S, F, T, P, R, and V denote object detection, instance
    segmentation, feature matching to build correspondences, template matching to
    retrieve pose, pose solution/regression, pose refinement, and pose voting, respectively.
    We report the *BOP-M* across the LM-O and YCB-V datasets (Sec. [2](#S2 "2 Datasets
    and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"))
    for various methods. Notably, these methods use Mask-RCNN[[270](#bib.bib270)]
    (normal font), CNOS [[276](#bib.bib276)] or their own proposed methods [[271](#bib.bib271)]
    [[30](#bib.bib30)] [[277](#bib.bib277)] (bold), and a combination of PPF and SIFT
    [[278](#bib.bib278)] (italics) for unseen object location, respectively. Moreover,
    Örnek *et al.*[[279](#bib.bib279)] and Caraffa *et al.*[[274](#bib.bib274)] don’t
    require any task-specific training, we use ”$\times$” to denote it.'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 代表性的基于CAD的方法。由于大多数未见方法的领域训练范式是领域泛化，我们报告每种方法的9个属性，这些属性与表格的标题中描述的含义相同。[I](#S3.T1
    "表 I ‣ 3.1.1 稀疏对应方法 ‣ 3.1 对应方法 ‣ 3 实例级别物体姿态估计 ‣ 基于深度学习的物体姿态估计：综合调查")。D、S、F、T、P、R
    和 V 分别表示物体检测、实例分割、特征匹配以建立对应关系、模板匹配以检索姿态、姿态解/回归、姿态细化和姿态投票。我们报告了不同方法在 LM-O 和 YCB-V
    数据集上的 *BOP-M*（见 Sec. [2](#S2 "2 数据集和指标 ‣ 基于深度学习的物体姿态估计：综合调查")）。值得注意的是，这些方法分别使用了
    Mask-RCNN[[270](#bib.bib270)]（正常字体）、CNOS [[276](#bib.bib276)] 或它们自己提出的方法 [[271](#bib.bib271)]
    [[30](#bib.bib30)] [[277](#bib.bib277)]（粗体）以及 PPF 和 SIFT [[278](#bib.bib278)]（斜体）来处理未见物体的位置。此外，Örnek
    *等*[[279](#bib.bib279)] 和 Caraffa *等*[[274](#bib.bib274)] 不需要任何特定任务的训练，我们使用 ”$\times$”
    来表示这一点。'
- en: '| Methods |'
  id: totrans-869
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |'
- en: '&#124; Published &#124;'
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 发布 &#124;'
- en: '&#124; Year &#124;'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 年份 &#124;'
- en: '|'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Training &#124;'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 &#124;'
- en: '&#124; Input &#124;'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入 &#124;'
- en: '|'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Inference &#124;'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 推断 &#124;'
- en: '&#124; Input &#124;'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入 &#124;'
- en: '|'
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Pose &#124;'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 姿态 &#124;'
- en: '&#124; DoF &#124;'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DoF &#124;'
- en: '|'
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Object &#124;'
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 物体 &#124;'
- en: '&#124; Property &#124;'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 属性 &#124;'
- en: '| Task |'
  id: totrans-884
  prefs: []
  type: TYPE_TB
  zh: '| 任务 |'
- en: '&#124; Inference &#124;'
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 推断 &#124;'
- en: '&#124; Mode &#124;'
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模式 &#124;'
- en: '|'
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Application &#124;'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 应用 &#124;'
- en: '&#124; Area &#124;'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 区域 &#124;'
- en: '|'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LM-O &#124;'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LM-O &#124;'
- en: '&#124; *BOP-M* &#124;'
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; *BOP-M* &#124;'
- en: '|'
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; YCB-V &#124;'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; YCB-V &#124;'
- en: '&#124; *BOP-M* &#124;'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; *BOP-M* &#124;'
- en: '|'
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CAD Model-Based Methods | Feature matching | Pitteri *et al.*[[266](#bib.bib266)]
    | 2020 |'
  id: totrans-897
  prefs: []
  type: TYPE_TB
  zh: '| 基于CAD模型的方法 | 特征匹配 | Pitteri *等*[[266](#bib.bib266)] | 2020 |'
- en: '&#124; RGB, &#124;'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 3DoF | rigid | estimation |'
  id: totrans-903
  prefs: []
  type: TYPE_TB
  zh: '| 3DoF | 刚体 | 估计 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段, &#124;'
- en: '&#124; S+F+P &#124;'
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+F+P &#124;'
- en: '| general | - | - |'
  id: totrans-906
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | - |'
- en: '| Zhao *et al.*[[269](#bib.bib269)] | 2023 |'
  id: totrans-907
  prefs: []
  type: TYPE_TB
  zh: '| Zhao *等*[[269](#bib.bib269)] | 2023 |'
- en: '&#124; Depth, &#124;'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Depth, &#124;'
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 6DoF | rigid | estimation |'
  id: totrans-913
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚体 | 估计 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段, &#124;'
- en: '&#124; S+F+P &#124;'
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+F+P &#124;'
- en: '| general | 65.2 | - |'
  id: totrans-916
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 65.2 | - |'
- en: '| Chen *et al.*[[271](#bib.bib271)] | 2023 |'
  id: totrans-917
  prefs: []
  type: TYPE_TB
  zh: '| Chen *等*[[271](#bib.bib271)] | 2023 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGBD, &#124;'
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 6DoF | rigid | estimation |'
  id: totrans-923
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚体 | 估计 |'
- en: '&#124; four-stage, &#124;'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 四阶段, &#124;'
- en: '&#124; S+F+P+R &#124;'
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+F+P+R &#124;'
- en: '| general | 49.1 | 57.7 |'
  id: totrans-926
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 49.1 | 57.7 |'
- en: '| Lin *et al.*[[30](#bib.bib30)] | 2024 |'
  id: totrans-927
  prefs: []
  type: TYPE_TB
  zh: '| Lin *等*[[30](#bib.bib30)] | 2024 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '|'
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGBD, &#124;'
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD, &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD模型 &#124;'
- en: '| 6DoF | rigid | estimation |'
  id: totrans-933
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚体 | 估计 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; S+F+P &#124;'
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+F+P &#124;'
- en: '| general | 69.9 | 84.5 |'
  id: totrans-936
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 69.9 | 84.5 |'
- en: '| Huang *et al.*[[275](#bib.bib275)] | 2024 |'
  id: totrans-937
  prefs: []
  type: TYPE_TB
  zh: '| Huang *等*[[275](#bib.bib275)] | 2024 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGBD, &#124;'
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation |'
  id: totrans-943
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚体 | 估计 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; S+F+P &#124;'
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+F+P &#124;'
- en: '| general | 56.2 | 60.8 |'
  id: totrans-946
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 56.2 | 60.8 |'
- en: '| Caraffa *et al.*[[274](#bib.bib274)] | 2024 | $\times$ |'
  id: totrans-947
  prefs: []
  type: TYPE_TB
  zh: '| Caraffa *等*[[274](#bib.bib274)] | 2024 | $\times$ |'
- en: '&#124; RGBD, &#124;'
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation |'
  id: totrans-950
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚体 | 估计 |'
- en: '&#124; four-stage, &#124;'
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 四阶段， &#124;'
- en: '&#124; S+F+P+R &#124;'
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+F+P+R &#124;'
- en: '| general | 69.0 | 85.3 |'
  id: totrans-953
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 69.0 | 85.3 |'
- en: '| Template matching | Sundermeyer *et al.*[[280](#bib.bib280)] | 2020 |'
  id: totrans-954
  prefs: []
  type: TYPE_TB
  zh: '| 模板匹配 | Sundermeyer *等*[[280](#bib.bib280)] | 2020 |'
- en: '&#124; RGB, &#124;'
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation |'
  id: totrans-960
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚体 | 估计'
- en: '&#124; three-stage, &#124;'
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; D/S+T+R &#124;'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D/S+T+R &#124;'
- en: '| general | - | - |'
  id: totrans-963
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | - |'
- en: '| Okorn *et al.*[[278](#bib.bib278)] | 2021 |'
  id: totrans-964
  prefs: []
  type: TYPE_TB
  zh: '| Okorn *等*[[278](#bib.bib278)] | 2021 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGBD, &#124;'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation |'
  id: totrans-970
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚体 | 估计 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; P+V &#124;'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; P+V &#124;'
- en: '| general | 59.8 | 51.6 |'
  id: totrans-973
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 59.8 | 51.6 |'
- en: '| Shugurov *et al.*[[277](#bib.bib277)] | 2022 |'
  id: totrans-974
  prefs: []
  type: TYPE_TB
  zh: '| Shugurov *等*[[277](#bib.bib277)] | 2022 |'
- en: '&#124; RGB/RGBD, &#124;'
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB/RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB/RGBD, &#124;'
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB/RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation |'
  id: totrans-980
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚体 | 估计 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; S+T+P &#124;'
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+T+P &#124;'
- en: '| general | 46.2 | 54.2 |'
  id: totrans-983
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 46.2 | 54.2 |'
- en: '| Nguyen *et al.*[[281](#bib.bib281)] | 2022 |'
  id: totrans-984
  prefs: []
  type: TYPE_TB
  zh: '| Nguyen *等*[[281](#bib.bib281)] | 2022 |'
- en: '&#124; RGB, &#124;'
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 3DoF | rigid | estimation |'
  id: totrans-990
  prefs: []
  type: TYPE_TB
  zh: '| 3DoF | 刚体 | 估计 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; D+T &#124;'
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+T &#124;'
- en: '| occlusion | - | - |'
  id: totrans-993
  prefs: []
  type: TYPE_TB
  zh: '| 遮挡 | - | - |'
- en: '| Labbé *et al.*[[28](#bib.bib28)] | 2022 |'
  id: totrans-994
  prefs: []
  type: TYPE_TB
  zh: '| Labbé *等*[[28](#bib.bib28)] | 2022 |'
- en: '&#124; RGB/RGBD, &#124;'
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB/RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB/RGBD, &#124;'
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB/RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid |'
  id: totrans-1000
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚体 |'
- en: '&#124; estimation, &#124;'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 估计， &#124;'
- en: '&#124; refinement &#124;'
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精细化 &#124;'
- en: '|'
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; three-stage, &#124;'
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; D+T+R &#124;'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+T+R &#124;'
- en: '| general | 58.3 | 63.3 |'
  id: totrans-1006
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 58.3 | 63.3 |'
- en: '| Örnek *et al.*[[279](#bib.bib279)] | 2023 | $\times$ |'
  id: totrans-1007
  prefs: []
  type: TYPE_TB
  zh: '| Örnek *等*[[279](#bib.bib279)] | 2023 | $\times$ |'
- en: '&#124; RGB, &#124;'
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation |'
  id: totrans-1010
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚体 | 估计 |'
- en: '&#124; four-stage, &#124;'
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 四阶段， &#124;'
- en: '&#124; S+T+P+R &#124;'
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+T+P+R &#124;'
- en: '| general | 61.0 | 69.0 |'
  id: totrans-1013
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 61.0 | 69.0 |'
- en: '| Nguyen *et al.*[[29](#bib.bib29)] | 2024 |'
  id: totrans-1014
  prefs: []
  type: TYPE_TB
  zh: '| Nguyen *等*[[29](#bib.bib29)] | 2024 |'
- en: '&#124; RGB, &#124;'
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | estimation |'
  id: totrans-1020
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚体 | 估计 |'
- en: '&#124; four-stage, &#124;'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 四阶段， &#124;'
- en: '&#124; D+T+P+R &#124;'
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+T+P+R &#124;'
- en: '| general | 63.1 | 65.2 |'
  id: totrans-1023
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 63.1 | 65.2 |'
- en: '| Moon *et al.*[[282](#bib.bib282)] | 2024 |'
  id: totrans-1024
  prefs: []
  type: TYPE_TB
  zh: '| Moon *等*[[282](#bib.bib282)] | 2024 |'
- en: '&#124; RGB/RGBD, &#124;'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB/RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB/RGBD, &#124;'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB/RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid | refinement |'
  id: totrans-1030
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚体 | 精细化 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; D+R &#124;'
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+R &#124;'
- en: '| general | 62.9 | 82.5 |'
  id: totrans-1033
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 62.9 | 82.5 |'
- en: '| Wang *et al.*[[283](#bib.bib283)] | 2024 |'
  id: totrans-1034
  prefs: []
  type: TYPE_TB
  zh: '| Wang *等*[[283](#bib.bib283)] | 2024 |'
- en: '&#124; RGB, &#124;'
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB, &#124;'
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 3DoF | rigid | estimation |'
  id: totrans-1040
  prefs: []
  type: TYPE_TB
  zh: '| 3DoF | 刚体 | 估计 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; D+T &#124;'
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+T &#124;'
- en: '| general | - | - |'
  id: totrans-1043
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - | - |'
- en: '| Wen *et al.*[[3](#bib.bib3)] | 2024 |'
  id: totrans-1044
  prefs: []
  type: TYPE_TB
  zh: '| Wen *等*[[3](#bib.bib3)] | 2024 |'
- en: '&#124; RGBD, &#124;'
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '|'
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGBD, &#124;'
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGBD， &#124;'
- en: '&#124; CAD Model &#124;'
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAD 模型 &#124;'
- en: '| 6DoF | rigid |'
  id: totrans-1050
  prefs: []
  type: TYPE_TB
  zh: '| 6DoF | 刚体 |'
- en: '&#124; estimation, &#124;'
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 估计， &#124;'
- en: '&#124; tracking &#124;'
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跟踪 &#124;'
- en: '|'
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; four-stage, &#124;'
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 四阶段， &#124;'
- en: '&#124; D+T+R+V &#124;'
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+T+R+V &#124;'
- en: '| general | 78.8 | 88.0 |'
  id: totrans-1056
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 78.8 | 88.0 |'
- en: 5.1.2 Template Matching-Based Methods
  id: totrans-1057
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 基于模板匹配的方法
- en: Template matching has been widely used in computer vision and stands as an effective
    solution for tackling the pose estimation challenges posed by unseen objects.
    Wohlhart *et al.*[[67](#bib.bib67)] and Balntas *et al.*[[284](#bib.bib284)] were
    pioneers in using deep pose descriptors for object matching and pose retrieval.
    However, their descriptors are tailored to specific orientations and categories,
    limiting their utility to objects with similar appearances. In contrast, Sundermeyer
    *et al.*[[280](#bib.bib280)] proposed a single-encoder-multi-decoder network for
    jointly estimating the 3D rotation of multiple objects. This approach eliminates
    the need to segregate views of different objects in the latent space and enables
    the sharing of common features in the encoder. Yet, it still requires training
    multiple decoders. Wen *et al.*[[285](#bib.bib285)] addressed this problem by
    decoupling object shape and pose in the latent representation, enabling auto-encoding
    without the necessity of multi-path decoders for different objects, thus enhancing
    scalability.
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: 模板匹配在计算机视觉中得到了广泛应用，并且作为应对未见物体姿态估计挑战的有效解决方案。Wohlhart *等人*[[67](#bib.bib67)] 和
    Balntas *等人*[[284](#bib.bib284)] 是使用深度姿态描述符进行物体匹配和姿态检索的先驱。然而，他们的描述符针对特定的方向和类别进行调整，限制了其对相似外观物体的实用性。相比之下，Sundermeyer
    *等人*[[280](#bib.bib280)] 提出了一个单编码器-多解码器网络，用于联合估计多个物体的 3D 旋转。这种方法消除了在潜在空间中分离不同物体视图的需要，并使编码器能够共享通用特征。然而，它仍然需要训练多个解码器。Wen
    *等人*[[285](#bib.bib285)] 通过在潜在表示中解耦物体形状和姿态来解决这一问题，从而实现了自动编码而不需要为不同物体设置多路径解码器，从而提高了可扩展性。
- en: Instead of training a network to learn features across objects, Okorn *et al.*[[278](#bib.bib278)]
    first generated candidate poses by PPF [[11](#bib.bib11)] and projected each into
    the scene. Later, they designed a scoring network to evaluate the hypothesis by
    comparing color and geometry differences between the projected object point cloud
    and RGBD image. Busam *et al.*[[286](#bib.bib286)] reformulated 6DoF pose retrieval
    as an action decision process and determined the final pose by iteratively estimating
    probable movements. Cai *et al.*[[287](#bib.bib287)] retrieved various candidate
    viewpoints from a target object viewpoint codebook, and then conducted in-plane
    2D rotational regression on each retrieved viewpoint to obtain a set of 3D rotation
    estimates. These estimates were evaluated using a consistency score to generate
    the final rotation prediction. Meanwhile, Shugurov *et al.*[[277](#bib.bib277)]
    matched the detected objects with the rendering database for initial viewpoint
    estimation. Then, they predicted the dense 2D-2D correspondences between the template
    and the image via feature matching. Pose estimation was eventually performed by
    using PnP+RANSAC[[73](#bib.bib73)] or Kabsch[[288](#bib.bib288)]+RANSAC.
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 与其训练一个网络以学习跨物体的特征，Okorn *等人*[[278](#bib.bib278)] 首先通过 PPF [[11](#bib.bib11)]
    生成候选姿态，并将每个姿态投影到场景中。随后，他们设计了一个评分网络，通过比较投影物体点云与 RGBD 图像之间的颜色和几何差异来评估假设。Busam *等人*[[286](#bib.bib286)]
    将 6DoF 姿态检索重新定义为一个动作决策过程，并通过迭代估计可能的运动来确定最终姿态。Cai *等人*[[287](#bib.bib287)] 从目标物体视角代码本中检索各种候选视点，然后对每个检索到的视点进行平面内
    2D 旋转回归，以获得一组 3D 旋转估计。这些估计通过一致性评分进行评估，以生成最终的旋转预测。同时，Shugurov *等人*[[277](#bib.bib277)]
    将检测到的物体与渲染数据库匹配以进行初步视角估计。然后，他们通过特征匹配预测模板与图像之间的密集 2D-2D 对应关系。最终，姿态估计通过使用 PnP+RANSAC[[73](#bib.bib73)]
    或 Kabsch[[288](#bib.bib288)]+RANSAC 完成。
- en: Since estimating the full 6DoF pose of an unseen object is extremely challenging,
    some works focus on estimating the 3D rotation to simplify it. Different from
    the previous works [[67](#bib.bib67), [284](#bib.bib284), [289](#bib.bib289),
    [104](#bib.bib104)] that exploited a global image representation to measure image
    similarity, Nguyen *et al.*[[281](#bib.bib281)] used CNN-extracted local features
    to compare the similarity between the input image and templates, showing better
    property and occlusion robustness over global representation. Another noteworthy
    approach is an image retrieval framework based on multi-scale local similarities
    developed by Zhao *et al.*[[290](#bib.bib290)]. They extracted feature maps of
    various sizes from the input image and devised a similarity fusion module to robustly
    predict image similarity scores from multi-scale pairwise feature maps. Further,
    Thalhammer *et al.*[[291](#bib.bib291)] and Ausserlechner *et al.*[[292](#bib.bib292)]
    extended the scheme of Nguyen *et al.*[[281](#bib.bib281)] and demonstrated that
    the pre-trained Vision-Transformer (ViT) [[293](#bib.bib293)] outperforms task-specific
    fine-tuned CNN [[162](#bib.bib162)] for template matching. However, these methods
    still have a noticeable performance gap between seen and unseen objects. To this
    end, Wang *et al.*[[283](#bib.bib283)] introduced diffusion features that show
    great potential in modeling unseen objects. Furthermore, they designed three aggregation
    networks to efficiently capture and aggregate diffusion features at different
    granularities, thus improving its generalizability.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 由于估计未见物体的完整6DoF姿态极具挑战性，一些研究工作集中在估计3D旋转以简化问题。与之前利用全球图像表示来测量图像相似性的工作[[67](#bib.bib67)、[284](#bib.bib284)、[289](#bib.bib289)、[104](#bib.bib104)]不同，Nguyen
    *et al.*[[281](#bib.bib281)] 使用CNN提取的局部特征来比较输入图像与模板之间的相似性，表现出比全局表示更好的属性和遮挡鲁棒性。另一个值得注意的方法是赵
    *et al.*[[290](#bib.bib290)] 开发的基于多尺度局部相似性的图像检索框架。他们从输入图像中提取了不同大小的特征图，并设计了一个相似性融合模块，以从多尺度成对特征图中稳健地预测图像相似性分数。此外，Thalhammer
    *et al.*[[291](#bib.bib291)] 和 Ausserlechner *et al.*[[292](#bib.bib292)] 扩展了Nguyen
    *et al.*[[281](#bib.bib281)] 的方案，并展示了预训练的视觉变换器（ViT）[[293](#bib.bib293)] 在模板匹配中优于任务特定的微调CNN[[162](#bib.bib162)]。然而，这些方法在见过和未见物体之间仍然存在明显的性能差距。为此，Wang
    *et al.*[[283](#bib.bib283)] 引入了扩散特征，这些特征在建模未见物体方面显示出了巨大潜力。此外，他们设计了三个聚合网络，以有效地捕捉和聚合不同粒度的扩散特征，从而提高了其泛化能力。
- en: In order to further improve the generalization and robustness of the 6DoF pose
    estimation, Labbé *et al.*[[28](#bib.bib28)] used a render-and-compare approach
    and a coarse-to-fine strategy. Notably, they leveraged a large-scale 3D model
    dataset to generate a synthetic dataset containing 2 million images and over 20,000
    models. It achieved strong generalization by training the network on this dataset.
    Compared to the non-differentiable rendering pipeline of Labbé *et al.*[[28](#bib.bib28)],
    Tremblay *et al.*[[294](#bib.bib294)] utilized recent advancements in differentiable
    rendering to design a flexible refiner, allowing fine-tuning the setup without
    retraining. On the other hand, Moon *et al.*[[282](#bib.bib282)] presented a shape-constraint
    recurrent flow framework, which predicts the optical flow between the template
    and query image and refines the pose iteratively. It took advantage of shape information
    directly to improve the accuracy and scalability. Recently, Wen *et al.*[[3](#bib.bib3)]
    inherited the idea of Labbé *et al.*[[28](#bib.bib28)] and developed a novel synthesis
    data generation pipeline using emerging large-scale 3D model databases, Large
    Language Models (LLMs), and diffusion models. It greatly expanded the amount and
    diversity of data, and ultimately achieved comparable results to instance-level
    methods in a render-and-compare manner.
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高6DoF姿态估计的泛化能力和鲁棒性，Labbé *et al.*[[28](#bib.bib28)] 使用了渲染与比较方法以及粗到细的策略。值得注意的是，他们利用了一个大规模的3D模型数据集生成了一个包含200万张图像和2万多个模型的合成数据集。通过在这个数据集上训练网络，实现了强大的泛化能力。相比于Labbé
    *et al.*[[28](#bib.bib28)] 的不可微分渲染管线，Tremblay *et al.*[[294](#bib.bib294)] 利用最近在可微分渲染方面的进展设计了一个灵活的精化器，允许在不重新训练的情况下对设置进行微调。另一方面，Moon
    *et al.*[[282](#bib.bib282)] 提出了一个形状约束的递归流框架，该框架预测模板和查询图像之间的光流并迭代地精化姿态。它直接利用了形状信息来提高准确性和可扩展性。最近，Wen
    *et al.*[[3](#bib.bib3)] 继承了Labbé *et al.*[[28](#bib.bib28)] 的思想，并开发了一种使用新兴的大规模3D模型数据库、大型语言模型（LLMs）和扩散模型的新型合成数据生成管线。这大大扩展了数据的数量和多样性，最终在渲染与比较的方式中取得了与实例级方法相当的结果。
- en: It is well known that template matching methods are sensitive to occlusions
    and require considerable time to match numerous templates. Therefore, Nguyen *et
    al.*[[29](#bib.bib29)] achieved rapid and robust pose estimation by finding the
    suitable trade-off between the use of template matching and patch correspondences.
    In particular, the features of the query image and templates are extracted using
    the ViT [[293](#bib.bib293)], followed by fast template matching using a sub-linear
    nearest neighbor search. The most similar template provides two DoFs for azimuth
    and elevation, while the remaining four DoFs are obtained by constructing correspondences
    between the query image and this template. Örnek *et al.*[[279](#bib.bib279)]
    utilized DINOv2[[249](#bib.bib249)] to extract descriptors for the query image
    and templates. Moreover, they introduced a fast template retrieval method based
    on visual words constructed from DINOv2 patch descriptors, thereby decreasing
    the reliance on extensive data and enhancing matching speed compared to Labbé
    *et al.*[[28](#bib.bib28)].
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，模板匹配方法对遮挡非常敏感，并且需要大量时间来匹配众多模板。因此，Nguyen *et al.*[[29](#bib.bib29)] 通过在模板匹配和补丁对应之间找到合适的权衡，成功实现了快速且鲁棒的姿态估计。特别地，查询图像和模板的特征通过
    ViT [[293](#bib.bib293)] 提取，然后使用次线性最近邻搜索进行快速模板匹配。最相似的模板提供了两个自由度（方位角和仰角），而其余四个自由度通过构建查询图像与该模板之间的对应关系获得。Örnek
    *et al.*[[279](#bib.bib279)] 利用 DINOv2[[249](#bib.bib249)] 提取查询图像和模板的描述符。此外，他们引入了一种基于
    DINOv2 补丁描述符构建的视觉词的快速模板检索方法，从而减少了对大量数据的依赖，并提高了匹配速度，相比于 Labbé *et al.*[[28](#bib.bib28)]。
- en: In summary, template matching-based methods make full use of the advantages
    provided by a multitude of templates, enabling high accuracy and strong generalization.
    Nonetheless, they have limitations in terms of time consumption, sensitivity to
    occlusions, and challenges posed by complex backgrounds and lighting variations.
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，基于模板匹配的方法充分利用了大量模板所提供的优势，实现了高准确性和强泛化能力。然而，它们在时间消耗、对遮挡的敏感性以及复杂背景和光照变化带来的挑战方面存在局限性。
- en: Whether the above-mentioned feature matching-based or template matching-based
    methods, they both require a CAD model of the target object to provide prior information.
    In practice, accurate CAD models often require specialized hardware to build,
    which limits the practical application of these methods to a certain extent.
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是上述的特征匹配方法还是基于模板匹配的方法，它们都需要目标对象的 CAD 模型以提供先验信息。在实际应用中，准确的 CAD 模型通常需要专业的硬件来构建，这在一定程度上限制了这些方法的实际应用。
- en: 5.2 Manual Reference View-Based Methods
  id: totrans-1065
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 手动参考视图基础方法
- en: 'Aside from these CAD model-based approaches, there are some manual reference
    view-based methods that do not require the unseen object CAD model as a prior
    condition but instead require providing some manual labeled reference views with
    the target object. Similar to CAD model-based methods, these methods are also
    categorized into two types: feature matching-based (Sec. [5.2.1](#S5.SS2.SSS1
    "5.2.1 Feature Matching-Based Methods ‣ 5.2 Manual Reference View-Based Methods
    ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")) and template matching-based (Sec. [5.2.2](#S5.SS2.SSS2
    "5.2.2 Template Matching-Based Methods ‣ 5.2 Manual Reference View-Based Methods
    ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")) methods. These two categories of methods are illustrated
    in Fig. [6](#S5.F6 "Figure 6 ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose
    Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey").
    The attributes and performance of some representative methods are shown in Table
    [IV](#S5.T4 "TABLE IV ‣ 5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey").'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: '除了这些基于CAD模型的方法，还有一些手动参考视图的方法，这些方法不需要未见物体的CAD模型作为先决条件，而是需要提供一些带有目标物体的手动标记参考视图。与基于CAD模型的方法类似，这些方法也分为两种类型：基于特征匹配的方法（参见
    [5.2.1](#S5.SS2.SSS1 "5.2.1 Feature Matching-Based Methods ‣ 5.2 Manual Reference
    View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")）和基于模板匹配的方法（参见 [5.2.2](#S5.SS2.SSS2 "5.2.2
    Template Matching-Based Methods ‣ 5.2 Manual Reference View-Based Methods ‣ 5
    Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A
    Comprehensive Survey")）。这两类方法在图 [6](#S5.F6 "Figure 6 ‣ 5.1 CAD Model-Based Methods
    ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey") 中进行了说明。一些代表性方法的属性和性能显示在表 [IV](#S5.T4 "TABLE IV ‣ 5.2
    Manual Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey") 中。'
- en: 'TABLE IV: Representative manual reference view-based methods. For each method,
    we report its 9 properties, which have the same meanings as described in the caption
    of Table. [I](#S3.T1 "TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey"). D, S, F, T, P, R, and V have the same
    meanings as Table. [III](#S5.T3 "TABLE III ‣ 5.1.1 Feature Matching-Based Methods
    ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey"). We report the average recall
    of ADD(S) within 10$\%$ of the object diameter, termed as ADD(S)-0.1d (Sec. [2](#S2
    "2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")). Notably, ”YOLOv5” and ”GT” denote the use of YOLOv5[[295](#bib.bib295)]
    and ground-truth bounding box/segmentation mask for object localization, respectively.
    For a fair comparison, we also report the number of used reference views. ”Full”
    represents all views.'
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 代表性的手动参考视图方法。对于每种方法，我们报告其9个属性，这些属性的含义与表 [I](#S3.T1 "TABLE I ‣ 3.1.1 Sparse
    Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey") 的说明相同。D、S、F、T、P、R 和 V 的含义与表 [III](#S5.T3 "TABLE III ‣ 5.1.1 Feature Matching-Based
    Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey") 相同。我们报告了 ADD(S)
    在物体直径 10$\%$ 以内的平均召回率，称为 ADD(S)-0.1d（参见 [2](#S2 "2 Datasets and Metrics ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey")）。特别地，“YOLOv5”和“GT”分别表示使用
    YOLOv5[[295](#bib.bib295)] 和真实边界框/分割掩膜进行物体定位。为了公平比较，我们还报告了使用的参考视图数量。“Full”表示所有视图。'
- en: '| Methods |'
  id: totrans-1068
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |'
- en: '&#124; Published &#124;'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 发布 &#124;'
- en: '&#124; Year &#124;'
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 年份 &#124;'
- en: '|'
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Training &#124;'
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 &#124;'
- en: '&#124; Input &#124;'
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入 &#124;'
- en: '|'
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Inference &#124;'
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 推理 &#124;'
- en: '&#124; Input &#124;'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入 &#124;'
- en: '|'
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Pose &#124;'
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 姿态 &#124;'
- en: '&#124; DoF &#124;'
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DoF &#124;'
- en: '|'
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Object &#124;'
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对象 &#124;'
- en: '&#124; Property &#124;'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 属性 &#124;'
- en: '| Task |'
  id: totrans-1083
  prefs: []
  type: TYPE_TB
  zh: '| 任务 |'
- en: '&#124; Inference &#124;'
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 推理 &#124;'
- en: '&#124; Mode &#124;'
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模式 &#124;'
- en: '|'
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Application &#124;'
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 应用 &#124;'
- en: '&#124; Area &#124;'
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 区域 &#124;'
- en: '|'
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LM &#124;'
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LM &#124;'
- en: '&#124; ADD(S)-0.1d &#124;'
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ADD(S)-0.1d &#124;'
- en: '|'
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Manual Reference View-Based Methods | Feature matching | He *et al.*[[296](#bib.bib296)]
    | 2022 | RGBD | RGBD | 6DoF | rigid | estimation |'
  id: totrans-1093
  prefs: []
  type: TYPE_TB
  zh: '| 手动参考视图方法 | 特征匹配 | He *et al.*[[296](#bib.bib296)] | 2022 | RGBD | RGBD |
    6DoF | 刚性 | 估计 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; D+F+P &#124;'
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+F+P &#124;'
- en: '| general |'
  id: totrans-1096
  prefs: []
  type: TYPE_TB
  zh: '| 一般 |'
- en: '&#124; 83.4 &#124;'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 83.4 &#124;'
- en: '&#124; (GT+16) &#124;'
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (GT+16) &#124;'
- en: '|'
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Sun *et al.*[[66](#bib.bib66)] | 2022 | RGB | RGB | 6DoF | rigid | estimation
    |'
  id: totrans-1100
  prefs: []
  type: TYPE_TB
  zh: '| Sun *等人*[[66](#bib.bib66)] | 2022 | RGB | RGB | 6DoF | 刚性 | 估计 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; D+F+P &#124;'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+F+P &#124;'
- en: '| general |'
  id: totrans-1103
  prefs: []
  type: TYPE_TB
  zh: '| 一般 |'
- en: '&#124; 63.6 &#124;'
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 63.6 &#124;'
- en: '&#124; (YOLOv5+Full) &#124;'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (YOLOv5+Full) &#124;'
- en: '|'
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| He *et al.*[[2](#bib.bib2)] | 2022 | RGB | RGB | 6DoF | rigid | estimation
    |'
  id: totrans-1107
  prefs: []
  type: TYPE_TB
  zh: '| He *等人*[[2](#bib.bib2)] | 2022 | RGB | RGB | 6DoF | 刚性 | 估计 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; D+F+P &#124;'
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+F+P &#124;'
- en: '| general |'
  id: totrans-1110
  prefs: []
  type: TYPE_TB
  zh: '| 一般 |'
- en: '&#124; 76.9 &#124;'
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 76.9 &#124;'
- en: '&#124; (YOLOv5+Full) &#124;'
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (YOLOv5+Full) &#124;'
- en: '|'
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Castro *et al.*[[297](#bib.bib297)] | 2023 | RGB | RGB | 6DoF | rigid | estimation
    |'
  id: totrans-1114
  prefs: []
  type: TYPE_TB
  zh: '| Castro *等人*[[297](#bib.bib297)] | 2023 | RGB | RGB | 6DoF | 刚性 | 估计 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; D+F+P &#124;'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+F+P &#124;'
- en: '| general |'
  id: totrans-1117
  prefs: []
  type: TYPE_TB
  zh: '| 一般 |'
- en: '&#124; 87.5 &#124;'
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 87.5 &#124;'
- en: '&#124; (GT+Full) &#124;'
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (GT+Full) &#124;'
- en: '|'
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Lee *et al.*[[298](#bib.bib298)] | 2024 | RGB | RGB | 6DoF | rigid | estimation
    |'
  id: totrans-1121
  prefs: []
  type: TYPE_TB
  zh: '| Lee *等人*[[298](#bib.bib298)] | 2024 | RGB | RGB | 6DoF | 刚性 | 估计 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; D+F+P &#124;'
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+F+P &#124;'
- en: '| general |'
  id: totrans-1124
  prefs: []
  type: TYPE_TB
  zh: '| 一般 |'
- en: '&#124; 78.4 &#124;'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 78.4 &#124;'
- en: '&#124; (YOLOv5+64) &#124;'
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (YOLOv5+64) &#124;'
- en: '|'
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Template matching | Park *et al.*[[65](#bib.bib65)] | 2020 | RGBD | RGBD
    | 6DoF | rigid | estimation |'
  id: totrans-1128
  prefs: []
  type: TYPE_TB
  zh: '| 模板匹配 | Park *等人*[[65](#bib.bib65)] | 2020 | RGBD | RGBD | 6DoF | 刚性 | 估计
    |'
- en: '&#124; three-stage, &#124;'
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; S+T+R &#124;'
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+T+R &#124;'
- en: '| general |'
  id: totrans-1131
  prefs: []
  type: TYPE_TB
  zh: '| 一般 |'
- en: '&#124; 87.1 &#124;'
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 87.1 &#124;'
- en: '&#124; (GT+16) &#124;'
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (GT+16) &#124;'
- en: '|'
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Nguyen *et al.*[[299](#bib.bib299)] | 2022 | RGB | RGB | 6DoF | rigid | tracking
    |'
  id: totrans-1135
  prefs: []
  type: TYPE_TB
  zh: '| Nguyen *等人*[[299](#bib.bib299)] | 2022 | RGB | RGB | 6DoF | 刚性 | 跟踪 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; D+S+P &#124;'
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+S+P &#124;'
- en: '| general | - |'
  id: totrans-1138
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - |'
- en: '| Liu *et al.*[[1](#bib.bib1)] | 2022 | RGB | RGB | 6DoF | rigid | estimation
    |'
  id: totrans-1139
  prefs: []
  type: TYPE_TB
  zh: '| Liu *等人*[[1](#bib.bib1)] | 2022 | RGB | RGB | 6DoF | 刚性 | 估计 |'
- en: '&#124; three-stage, &#124;'
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三阶段， &#124;'
- en: '&#124; D+T+R &#124;'
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+T+R &#124;'
- en: '| general | - |'
  id: totrans-1142
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | - |'
- en: '| Gao *et al.*[[300](#bib.bib300)] | 2023 | RGB | RGB | 6DoF | rigid | estimation
    |'
  id: totrans-1143
  prefs: []
  type: TYPE_TB
  zh: '| Gao *等人*[[300](#bib.bib300)] | 2023 | RGB | RGB | 6DoF | 刚性 | 估计 |'
- en: '&#124; four-stage, &#124;'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 四阶段， &#124;'
- en: '&#124; S+D+P+R &#124;'
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S+D+P+R &#124;'
- en: '| occlusion | - |'
  id: totrans-1146
  prefs: []
  type: TYPE_TB
  zh: '| 遮挡 | - |'
- en: '| Cai *et al.*[[301](#bib.bib301)] | 2024 | RGB | RGB | 6DoF | rigid | estimation
    |'
  id: totrans-1147
  prefs: []
  type: TYPE_TB
  zh: '| Cai *等人*[[301](#bib.bib301)] | 2024 | RGB | RGB | 6DoF | 刚性 | 估计 |'
- en: '&#124; two-stage, &#124;'
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两阶段， &#124;'
- en: '&#124; P+R &#124;'
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; P+R &#124;'
- en: '| general |'
  id: totrans-1150
  prefs: []
  type: TYPE_TB
  zh: '| 一般 |'
- en: '&#124; 81.7 &#124;'
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 81.7 &#124;'
- en: '&#124; (Full) &#124;'
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Full) &#124;'
- en: '|'
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Wen *et al.*[[3](#bib.bib3)] | 2024 | RGBD | RGBD | 6DoF | rigid |'
  id: totrans-1154
  prefs: []
  type: TYPE_TB
  zh: '| Wen *等人*[[3](#bib.bib3)] | 2024 | RGBD | RGBD | 6DoF | 刚性 |'
- en: '&#124; estimation, &#124;'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 估计， &#124;'
- en: '&#124; tracking &#124;'
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跟踪 &#124;'
- en: '|'
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; four-stage, &#124;'
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 四阶段， &#124;'
- en: '&#124; D+T+R+V &#124;'
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D+T+R+V &#124;'
- en: '| general |'
  id: totrans-1160
  prefs: []
  type: TYPE_TB
  zh: '| 一般 |'
- en: '&#124; 99.9 &#124;'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 99.9 &#124;'
- en: '&#124; (GT+16) &#124;'
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (GT+16) &#124;'
- en: '|'
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 5.2.1 Feature Matching-Based Methods
  id: totrans-1164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 特征匹配方法
- en: Different from CAD model-based feature matching methods, manual reference view-based
    feature matching methods primarily establish 3D-3D correspondences between the
    RGBD query image and RGBD reference images, or 2D-3D correspondences between the
    query image and sparse point cloud reconstructed by reference views. Subsequently,
    the object pose is solved according to the different correspondences. He *et al.*[[296](#bib.bib296)]
    proposed the first few-shot 6DoF object pose estimation method, which can estimate
    the pose of an unseen object by a few support views without extra training. Specifically,
    they desinged a dense RGBD prototype matching framework based on transformers
    to fully explore the semantic and geometric relationship between the query image
    and reference views. Corsetti *et al.*[[302](#bib.bib302)] used a textual prompt
    for object segmentation and reformulated the problem as a relative pose estimation
    between two scenes. The relative pose was obtained via point cloud registration.
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于CAD模型的特征匹配方法不同，手动参考视图的特征匹配方法主要建立RGBD查询图像与RGBD参考图像之间的3D-3D对应关系，或查询图像与由参考视图重建的稀疏点云之间的2D-3D对应关系。随后，根据不同的对应关系求解对象姿态。*等人*[[296](#bib.bib296)]
    提出了首个少量样本6DoF对象姿态估计方法，该方法可以通过少量支持视图在没有额外训练的情况下估计未见过的对象姿态。具体来说，他们设计了一个基于transformers的密集RGBD原型匹配框架，以充分探索查询图像与参考视图之间的语义和几何关系。Corsetti
    *等人*[[302](#bib.bib302)] 使用了文本提示进行对象分割，并将问题重新表述为两个场景之间的相对姿态估计。相对姿态是通过点云配准获得的。
- en: Some methods took an alternative route from the perspective of matching after
    reconstruction. Wu *et al.*[[303](#bib.bib303)] developed a global registration-based
    method that used reference and query images to reconstruct full-view and single-view
    models, and then searched for point matches between the two models. Sun *et al.*[[66](#bib.bib66)]
    drew inspiration from visual localization and revised the pipeline to adapt it
    for pose estimation. More precisely, they reconstructed a Structure from Motion
    (SfM) model of the unseen object using RGB sequences from all reference viewpoints.
    Then, they matched 2D keypoints in the query image with the 3D points in the SfM
    model by a graph attention network. Nevertheless, it performed poorly on low-textured
    objects because of its reliance on repeatably detected keypoints. To deal with
    this problem, He *et al.*[[2](#bib.bib2)] designed a new keypoint-free SfM method
    to reconstruct semi-dense point cloud models of low-textured objects based on
    the detector-free feature matching method LoFTR[[304](#bib.bib304)]. Castro *et
    al.*[[297](#bib.bib297)] pointed out that these pre-trained feature matching models
    [[305](#bib.bib305), [304](#bib.bib304)] fail to capture the optimal descriptions
    for pose estimation. Based on this, they redesigned the training pipeline based
    on a three-view system for one-shot object-to-image matching.
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法从重建后的匹配角度采取了替代路线。吴*等人*[[303](#bib.bib303)] 开发了一种基于全局配准的方法，该方法利用参考图像和查询图像重建全景和单视角模型，然后在这两个模型之间搜索点匹配。孙*等人*[[66](#bib.bib66)]
    从视觉定位中获得灵感，修订了管道以适应姿态估计。更具体地，他们使用来自所有参考视点的RGB序列重建了未见物体的结构从运动（SfM）模型。然后，他们通过图注意网络将查询图像中的2D关键点与SfM模型中的3D点进行匹配。然而，由于其依赖于重复检测的关键点，该方法在低纹理物体上表现不佳。为了解决这个问题，何*等人*[[2](#bib.bib2)]
    设计了一种新的无关键点SfM方法，基于无检测器特征匹配方法LoFTR[[304](#bib.bib304)] 重建低纹理物体的半密集点云模型。卡斯特罗*等人*[[297](#bib.bib297)]
    指出，这些预训练的特征匹配模型[[305](#bib.bib305), [304](#bib.bib304)] 未能捕捉到姿态估计的最佳描述。基于此，他们重新设计了基于三视图系统的一次性物体到图像匹配训练管道。
- en: The aforementioned works still require dense support views (*i.e.*, $\geq 32$
    views). To address this problem, Fan *et al.*[[306](#bib.bib306)] turned the 6DoF
    object pose estimation task into relative pose estimation between the retrieved
    object in the target view and the reference view. Given only one reference view,
    they achieved it by using the DINOv2 model [[249](#bib.bib249)] for global matching
    and the LoFTR model [[304](#bib.bib304)] for local matching. Note that this method
    cannot estimate absolute translation (or object scale), as this is an ill-posed
    problem when only considering two views. Beyond that, Lee *et al.*[[298](#bib.bib298)]
    applied a powerful pre-trained technique tailored for 3D vision[[307](#bib.bib307)]
    and demonstrated geometry-oriented visual pre-training can get better generalization
    capability with fewer reference views.
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: 上述工作仍需密集的支持视图（*即*，$\geq 32$ 视图）。为了解决这个问题，范*等人*[[306](#bib.bib306)] 将6DoF物体姿态估计任务转化为目标视图中检索到的物体与参考视图之间的相对姿态估计。在仅给出一个参考视图的情况下，他们通过使用DINOv2模型[[249](#bib.bib249)]
    进行全局匹配和LoFTR模型[[304](#bib.bib304)] 进行局部匹配实现了这一目标。请注意，这种方法无法估计绝对平移（或物体尺度），因为在仅考虑两个视图时，这是一个不适定问题。除此之外，李*等人*[[298](#bib.bib298)]
    应用了针对3D视觉的强大预训练技术[[307](#bib.bib307)]，并展示了面向几何的视觉预训练可以在较少参考视图的情况下获得更好的泛化能力。
- en: Generally, due to the lack of prior geometric information from CAD models, manual
    reference view-based feature matching methods often require special designs to
    extract the geometric features of unseen objects. The number of reference views
    also constrains the actual application of such approaches to a certain extent.
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，由于缺乏来自CAD模型的先验几何信息，基于手动参考视图的特征匹配方法通常需要特殊设计以提取未见物体的几何特征。参考视图的数量也在一定程度上限制了这些方法的实际应用。
- en: 5.2.2 Template Matching-Based Methods
  id: totrans-1169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 基于模板匹配的方法
- en: 'Template matching-based methods mainly adopt the strategy of retrieval and
    refinement. There are two types: one reconstructs a 3D object representation using
    reference views, renders multiple templates based on this 3D representation, and
    employs a similarity network to compare the query image with each template for
    the initial pose. A refiner is then used to refine this initial pose for increased
    accuracy. The other directly uses reference views as templates, requiring plenty
    of views for retrieval and greater reliance on a refiner for accuracy. Park *et
    al.*[[65](#bib.bib65)] introduced a novel framework for pose estimation of unseen
    objects without the CAD model. They reconstructed 3D object representations from
    a few reference views, followed by estimating translation using mask bounding
    boxes and corresponding depth values. The initial rotation was determined by sampling
    angles and refined using gradient updates via a render-and-compare approach. By
    training the network to render and reconstruct diverse 3D shapes, it achieved
    excellent generalization performance on unseen objects.'
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模板匹配的方法主要采用检索和细化策略。主要有两种类型：一种是使用参考视图重建3D物体表示，然后基于这个3D表示渲染多个模板，并通过相似性网络将查询图像与每个模板进行比较以获得初始姿态。然后使用细化器来进一步提高初始姿态的准确性。另一种直接使用参考视图作为模板，需要大量的视图进行检索，并且更依赖细化器以提高准确性。Park
    *et al.*[[65](#bib.bib65)] 引入了一种新颖的框架，用于在没有CAD模型的情况下估计未见物体的姿态。他们通过少量参考视图重建了3D物体表示，然后使用遮罩边界框和相应的深度值来估计平移。初始旋转通过角度采样确定，并通过渲染-比较方法使用梯度更新进行细化。通过训练网络渲染和重建多样的3D形状，它在未见物体上实现了优秀的泛化性能。
- en: Unlike Park *et al.*[[65](#bib.bib65)] that used the strategy of render-and-compare
    after reconstruction, Liu *et al.*[[1](#bib.bib1)] designed a pipeline for detection,
    retrieval, and refinement. They first designed a detector to identify object bounding
    boxes in the target view. Next, they compared the query and reference images at
    the pixel level to acquire the initial pose based on the similarity score. The
    pose was then refined using feature volume and multiple 3D convolution layers.
    However, object-centered reference images from cluttered scenes are constrained
    by actual segmentation or bounding box cropping, limiting its real-world applicability.
    To overcome this limitation, Gao *et al.*[[300](#bib.bib300)] proposed adaptive
    segmentation modules to learn distinguishable representations of unseen objects,
    and Zhao *et al.*[[308](#bib.bib308)] leveraged distributed reference kernels
    and translation estimator to achieve multi-scale correlation computation and object
    translation parameter prediction, thus robustly learning the prior translation
    of unseen objects.
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用重建后渲染-比较策略的Park *et al.*[[65](#bib.bib65)]不同，Liu *et al.*[[1](#bib.bib1)]
    设计了一个用于检测、检索和细化的管道。他们首先设计了一个检测器来识别目标视图中的物体边界框。接着，他们在像素级别比较查询图像和参考图像，以根据相似性得分获取初始姿态。然后，使用特征体积和多个3D卷积层来细化姿态。然而，来自混乱场景的物体中心参考图像受限于实际分割或边界框裁剪，限制了其在现实世界中的适用性。为克服这一限制，Gao
    *et al.*[[300](#bib.bib300)] 提出了自适应分割模块，以学习未见物体的可区分表示，而Zhao *et al.*[[308](#bib.bib308)]
    则利用分布式参考核和平移估计器实现了多尺度相关计算和物体平移参数预测，从而稳健地学习未见物体的先验平移。
- en: To further enhance the robustness of the translation estimation for object detection,
    Pan *et al.*[[309](#bib.bib309)] modified the framework of Liu *et al.*[[1](#bib.bib1)].
    Precisely, they utilized pre-trained ViT[[293](#bib.bib293)] to learn robust feature
    representations and adopted a top-K pose proposal scheme for pose initialization.
    Additionally, they applied a coarse-to-fine cascaded refinement process, incorporating
    feature pyramids and adaptive discrete pose hypotheses. Besides [[309](#bib.bib309)],
    Cai *et al.*[[301](#bib.bib301)] revisited the pipeline of Liu *et al.*[[1](#bib.bib1)].
    They proposed a generic joint segmentation method and an efficient 3D Gaussian
    Splatting-based refiner, improving the performance and robustness of object localization
    and pose estimation.
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步增强物体检测中平移估计的鲁棒性，Pan *et al.*[[309](#bib.bib309)] 修改了Liu *et al.*[[1](#bib.bib1)]的框架。具体而言，他们利用预训练的ViT[[293](#bib.bib293)]来学习鲁棒的特征表示，并采用top-K姿态提议方案进行姿态初始化。此外，他们应用了从粗到细的级联细化过程，结合特征金字塔和自适应离散姿态假设。除了[[309](#bib.bib309)]，Cai
    *et al.*[[301](#bib.bib301)] 重新审视了Liu *et al.*[[1](#bib.bib1)]的管道。他们提出了一种通用的联合分割方法和高效的3D高斯溅射细化器，提升了物体定位和姿态估计的性能和鲁棒性。
- en: In unseen object tracking, Nguyen *et al.*[[299](#bib.bib299)] proposed the
    first method that extended to invisible categories without requiring 3D information
    and extra reference images, given the ground-truth object pose in the first frame.
    Their transformer-based architecture outputs continuous relative object poses
    between consecutive frames, combined with the initial object pose, to provide
    the object pose for each frame. Wen *et al.*[[310](#bib.bib310)] used the collaborative
    design of concurrent tracking and neural object fields to perform 6DoF tracking
    from RGBD sequences. Key aspects of it include online pose graph optimization,
    concurrent neural object fields for 3D shape and appearance reconstruction, and
    a memory pool facilitating communication between the two processes.
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: 在未见物体跟踪中，Nguyen *等*[[299](#bib.bib299)] 提出了首个无需3D信息和额外参考图像即可扩展到不可见类别的方法，前提是给定第一帧的真实物体姿态。他们的基于变压器的架构输出连续的相对物体姿态，通过结合初始物体姿态，为每帧提供物体姿态。Wen
    *等*[[310](#bib.bib310)] 通过并行跟踪和神经物体场的协作设计，进行RGBD序列的6DoF跟踪。其关键方面包括在线姿态图优化、并行神经物体场用于3D形状和外观重建，以及促进两个过程之间通信的记忆池。
- en: More recently, Nguyen *et al.*[[311](#bib.bib311)] reconsidered template matching
    from the perspective of generating new views. Given a single reference view, they
    trained a model to directly predict the discriminative embeddings of the novel
    viewpoints of the object. In contrast, Wen *et al.*[[3](#bib.bib3)] applied an
    object-centric neural field representation for object modeling and RGBD rendering.
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Nguyen *等*[[311](#bib.bib311)] 从生成新视图的角度重新考虑了模板匹配。给定单个参考视图，他们训练了一种模型，直接预测物体的新视点的区分嵌入。相比之下，Wen
    *等*[[3](#bib.bib3)] 采用了以物体为中心的神经场表示进行物体建模和RGBD渲染。
- en: In summary, similar to template matching-based methods using CAD models, manual
    reference view-based methods also rely on massive templates. Moreover, due to
    limited reference views, these methods need to generate new templates or employ
    additional strategies to optimize the initial pose obtained through template matching.
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，与基于CAD模型的模板匹配方法类似，基于手动参考视图的方法也依赖于大量模板。此外，由于参考视图有限，这些方法需要生成新模板或采用额外策略以优化通过模板匹配获得的初始姿态。
- en: 6 Applications
  id: totrans-1176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 应用
- en: 'With the advancement of object pose estimation technology, several applications
    leveraging this progress have been deployed. In this section, we elaborate on
    the development trends of these applications. Specifically, these applications
    include robotic manipulation (Sec. [6.1](#S6.SS1 "6.1 Robotic Manipulation ‣ 6
    Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")),
    Augmented Reality (AR)/Virtual Reality (VR) (Sec. [6.2](#S6.SS2 "6.2 Augmented
    Reality/Virtual Reality ‣ 6 Applications ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")), aerospace (Sec. [6.3](#S6.SS3 "6.3 Aerospace ‣ 6 Applications
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")), hand-object
    interaction (Sec. [6.4](#S6.SS4 "6.4 Hand-Object Interaction ‣ 6 Applications
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")), and autonomous
    driving (Sec. [6.5](#S6.SS5 "6.5 Autonomous Driving ‣ 6 Applications ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")). The chronological overview
    is shown in Fig. [7](#S6.F7 "Figure 7 ‣ 6.1.1 Instance-Level Manipulation ‣ 6.1
    Robotic Manipulation ‣ 6 Applications ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey").'
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: 随着物体姿态估计技术的进步，多个利用这一进展的应用已经被部署。在本节中，我们详细阐述了这些应用的发展趋势。具体来说，这些应用包括机器人操作（第[6.1节](#S6.SS1
    "6.1 机器人操作 ‣ 6 应用 ‣ 基于深度学习的物体姿态估计：综合调查")）、增强现实（AR）/虚拟现实（VR）（第[6.2节](#S6.SS2 "6.2
    增强现实/虚拟现实 ‣ 6 应用 ‣ 基于深度学习的物体姿态估计：综合调查")）、航空航天（第[6.3节](#S6.SS3 "6.3 航空航天 ‣ 6 应用
    ‣ 基于深度学习的物体姿态估计：综合调查")）、手-物体交互（第[6.4节](#S6.SS4 "6.4 手-物体交互 ‣ 6 应用 ‣ 基于深度学习的物体姿态估计：综合调查")）和自动驾驶（第[6.5节](#S6.SS5
    "6.5 自动驾驶 ‣ 6 应用 ‣ 基于深度学习的物体姿态估计：综合调查")）。时间顺序的概览见图[7](#S6.F7 "图 7 ‣ 6.1.1 实例级操作
    ‣ 6.1 机器人操作 ‣ 6 应用 ‣ 基于深度学习的物体姿态估计：综合调查")。
- en: 6.1 Robotic Manipulation
  id: totrans-1178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 机器人操作
- en: We categorize the robotic manipulation application into instance-level, category-level,
    and unseen objects. This classification helps in better understanding the challenges
    and requirements across different levels.
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将机器人操作应用分为实例级、类别级和未见物体级别。这种分类有助于更好地理解不同层次的挑战和要求。
- en: 6.1.1 Instance-Level Manipulation
  id: totrans-1180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 实例级操作
- en: To tackle the challenge of annotating real data during training, many works
    utilize synthetic data for training as it is easy to acquire and annotate [[312](#bib.bib312),
    [313](#bib.bib313)]. At the same time, synthetic data can simulate various scenes
    and environmental changes, thus helping to improve the adaptability of robotic
    manipulation. Li *et al.*[[314](#bib.bib314)] used a large-scale synthetic dataset
    and a small-scale weakly labeled real-world dataset to reduce the difficulty of
    system deployment. Additionally, Chen *et al.*[[315](#bib.bib315)] proposed an
    iterative self-training framework, using a teacher network trained on synthetic
    data to generate pseudo-labels for real data. Meanwhile, Fu *et al.*[[316](#bib.bib316)]
    trained only on synthetic images based on physical rendering. One of the critical
    challenges of synthetic data is bridging the gap with reality, which Tremblay
    *et al.*[[317](#bib.bib317)] addressed by combining domain randomization with
    real data.
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对训练过程中对真实数据进行标注的挑战，许多研究利用合成数据进行训练，因为合成数据易于获取和标注[[312](#bib.bib312), [313](#bib.bib313)]。与此同时，合成数据可以模拟各种场景和环境变化，从而有助于提高机器人操作的适应性。李*等人*[[314](#bib.bib314)]使用了一个大规模的合成数据集和一个小规模的弱标记真实数据集，以降低系统部署的难度。此外，陈*等人*[[315](#bib.bib315)]提出了一个迭代自训练框架，使用在合成数据上训练的教师网络生成真实数据的伪标签。同时，傅*等人*[[316](#bib.bib316)]仅基于物理渲染训练了合成图像。合成数据的一个关键挑战是缩小与现实的差距，Tremblay*等人*[[317](#bib.bib317)]通过将领域随机化与真实数据相结合来解决这一问题。
- en: Handling stacked occlusion scenes is another significant challenge, especially
    in industrial automation and logistics. In these scenarios, robots must accurately
    identify and localize objects stacked on each other, which requires an effective
    process of occluded objects and accurate pose estimation. Dong *et al.*[[318](#bib.bib318)]
    argued that the regression poses of points from the same object should tightly
    reside in the pose space. Therefore, these points can be clustered into different
    instances, and their corresponding object poses can be estimated simultaneously.
    This method can handle severe object occlusion. Moreover, Zhuang *et al.*[[319](#bib.bib319)]
    established an end-to-end pipeline to synchronously regress all potential object
    poses from an unsegmented point cloud. Most recently, Wada *et al.*[[320](#bib.bib320)]
    proposed a system that fully utilized identified accurate object CAD models and
    non-parametric reconstruction of unrecognized structures to estimate the occluded
    objects pose in real-time.
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: 处理堆叠遮挡场景是另一个重要挑战，特别是在工业自动化和物流领域。在这些场景中，机器人必须准确识别和定位彼此堆叠的物体，这需要有效的遮挡物体处理过程和准确的姿态估计。董*等人*[[318](#bib.bib318)]认为，同一物体的点的回归姿态应该紧密地存在于姿态空间中。因此，这些点可以被聚类为不同的实例，并且可以同时估计其对应的物体姿态。这种方法可以处理严重的物体遮挡。此外，庄*等人*[[319](#bib.bib319)]建立了一个端到端的管道，从未分割的点云中同步回归所有潜在的物体姿态。最近，和田*等人*[[320](#bib.bib320)]提出了一个系统，充分利用识别的准确物体CAD模型和未识别结构的非参数重建来实时估计遮挡物体的姿态。
- en: Low-textured objects lack object surface texture information, making robotic
    manipulation challenging. Therefore, Zhang *et al.*[[321](#bib.bib321)] proposed
    a pose estimation method for texture-less industrial parts. Poor surface texture
    and brightness make it challenging to compute discriminative local appearance
    descriptors. This method achieves more accurate results by optimizing the pose
    in the edge image. In addition, Chang *et al.*[[322](#bib.bib322)] carried out
    transparent object grasping by estimating the object pose using a proposed model-free
    method that relies on multiview geometry. In agricultural scenes, Kim *et al.*[[323](#bib.bib323)]
    constructed an automated data collection scheme based on a 3D simulator environment
    to achieve three-level ripeness classification and pose estimation of target fruits.
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: 低纹理物体缺乏物体表面纹理信息，使得机器人操控变得具有挑战性。因此，张*等人*[[321](#bib.bib321)]提出了一种针对无纹理工业零件的姿态估计方法。较差的表面纹理和亮度使得计算区分性的局部外观描述符具有挑战性。该方法通过优化边缘图像中的姿态来实现更准确的结果。此外，常*等人*[[322](#bib.bib322)]通过估计物体姿态的无模型方法进行透明物体抓取，该方法依赖于多视角几何。在农业场景中，金*等人*[[323](#bib.bib323)]基于3D模拟环境构建了一种自动化数据收集方案，实现了目标水果的三层成熟度分类和姿态估计。
- en: '![Refer to caption](img/7a5a9b0cb8df631bb3c19206a4346199.png)'
  id: totrans-1184
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7a5a9b0cb8df631bb3c19206a4346199.png)'
- en: 'Figure 7: Chronological overview of some representative applications of object
    pose estimation methods. The black references, red references, and orange references
    represent the application of instance-level, category-level, and unseen methods,
    respectively. From this, we can also see the development trend, *i.e.*, from instance-level
    methods to category-level and unseen methods.'
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：一些代表性物体姿态估计方法的时间概述。黑色参考文献、红色参考文献和橙色参考文献分别表示实例级、类别级和未见方法的应用。从中，我们还可以看到发展趋势，*即*
    从实例级方法到类别级和未见方法。
- en: 6.1.2 Category-Level Manipulation
  id: totrans-1186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 类别级操控
- en: To investigate the application of category-level object pose estimation for
    robotic manipulation, Liu *et al.*[[324](#bib.bib324)] introduced a fine segmentation-guided
    category-level method with difference-aware shape deformation for robotic grasping.
    Yu *et al.*[[325](#bib.bib325)] proposed a shape prior-based approach and explored
    its application for robotic grasping. Further, Liu *et al.*[[4](#bib.bib4)] developed
    a robotic continuous grasping system with a pre-defined vector orientation-based
    grasping strategy, based on shape transformer-guided object pose estimation. To
    improve efficiency and enable the pose estimation method to be applied to tasks
    with higher real-time requirements, Sun *et al.*[[326](#bib.bib326)] utilized
    the inter-frame consistent keypoints to perform object pose tracking for aerial
    manipulation. To further avoid manual data annotation in the real-world scene,
    Yu *et al.*[[327](#bib.bib327)] built a robotic grasping platform and designed
    a self-supervised-based method for category-level robotic grasping. More recently,
    Liu *et al.*[[5](#bib.bib5)] explored a contrastive learning-guided prior-free
    object pose estimation method for domain-generalized robotic picking.
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究类别级物体姿态估计在机器人操控中的应用，刘*等人*[[324](#bib.bib324)]介绍了一种细分引导的类别级方法，具有差异感知形状变形，用于机器人抓取。余*等人*[[325](#bib.bib325)]提出了一种基于形状先验的方法，并探讨了其在机器人抓取中的应用。此外，刘*等人*[[4](#bib.bib4)]开发了一种基于预定义向量方向的抓取策略的机器人连续抓取系统，该系统基于形状转换器引导的物体姿态估计。为了提高效率并使姿态估计方法能够应用于实时性要求更高的任务，孙*等人*[[326](#bib.bib326)]利用帧间一致的关键点进行物体姿态跟踪，以便进行空中操控。为了进一步避免在实际场景中的手动数据标注，余*等人*[[327](#bib.bib327)]建立了一个机器人抓取平台，并设计了一种基于自我监督的类别级机器人抓取方法。最近，刘*等人*[[5](#bib.bib5)]探讨了一种对比学习引导的无先验物体姿态估计方法，用于领域泛化的机器人拾取。
- en: 6.1.3 Unseen Object Manipulation
  id: totrans-1188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3 未见物体操控
- en: Since unseen object pose estimation belongs to an emerging research, there is
    currently a lack of specialized designs for robotic. Here, we report several methods
    that validate the effectiveness of unseen object pose estimation through robotic
    manipulation. Okorn *et al.*[[278](#bib.bib278)] introduced a method for zero-shot
    object pose estimation in clutter. By scoring pose hypotheses and choosing the
    highest-scoring pose, they successfully grasped a novel drill object using a robotic
    arm. Labbé *et al.*[[28](#bib.bib28)] and Wen *et al.*[[3](#bib.bib3)] adopted
    the render-and-compare strategy and trained the network on a large-scale synthetic
    dataset, resulting in an outstanding generalization. They further verified the
    effectiveness of their methods through robotic grasping experiments.
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: 由于未见物体姿态估计属于新兴研究领域，目前在机器人领域缺乏专门的设计。在这里，我们报告了几种通过机器人操作验证未见物体姿态估计有效性的方法。Okorn
    *等*[[278](#bib.bib278)] 提出了一种在杂乱环境中进行零样本物体姿态估计的方法。通过对姿态假设进行评分并选择得分最高的姿态，他们成功地用机器人手臂抓取了一个新的钻头物体。Labbé
    *等*[[28](#bib.bib28)] 和 Wen *等*[[3](#bib.bib3)] 采用了渲染与比较策略，并在大规模合成数据集上训练了网络，从而取得了出色的泛化能力。他们进一步通过机器人抓取实验验证了其方法的有效性。
- en: 6.2 Augmented Reality/Virtual Reality
  id: totrans-1190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 增强现实/虚拟现实
- en: Object pose estimation has various specific applications in AR and VR fields.
    In AR, accurate pose estimation allows for a precise overlay of virtual objects
    onto the real world. The key to VR technology lies in tracking the head-mounted
    display pose and controller in 3D space.
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: 物体姿态估计在AR和VR领域有多种具体应用。在AR中，准确的姿态估计允许将虚拟物体精确叠加到现实世界中。VR技术的关键在于在三维空间中追踪头戴显示器的姿态和控制器。
- en: Su *et al.*[[328](#bib.bib328)] combined two CNN architectures[[162](#bib.bib162)]
    into a network, consisting of a state estimation branch and a pose estimation
    branch explicitly trained on synthetic images, to achieve AR assembly applications.
    Pandey *et al.*[[329](#bib.bib329)] introduced a method for automatically annotating
    the handheld objects pose in camera space, addressing the efficient 6Dof pose
    tracking problem for handheld controllers from the perspective of egocentric cameras.
    Liu *et al.*[[1](#bib.bib1)] presented a generalizable model-free 6DoF object
    pose estimator that has realized the complete object detection and pose estimation
    process. By simply capturing reference images of an unseen object and retrieving
    the poses of reference images, this method can predict the object pose on arbitrary
    query images and be easily applied to daily objects for AR/VR applications. He
    *et al.*[[2](#bib.bib2)] adopted matching after the reconstruction strategy, which
    establishes the correspondences between the query image and the reconstructed
    point cloud from reference views. This method does not rely on keypoint matching
    and allows for AR applications even on low-texture objects. Wen *et al.* [[3](#bib.bib3)]
    achieved strong generality by employing large-scale comprehensive training and
    innovative transformer-based architecture. This method has been successfully applied
    in various domains, including AR and robotic manipulation.
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: Su *等*[[328](#bib.bib328)] 将两个CNN架构[[162](#bib.bib162)]结合成一个网络，其中包括一个状态估计分支和一个在合成图像上显式训练的姿态估计分支，以实现AR装配应用。Pandey
    *等*[[329](#bib.bib329)] 提出了一种在相机空间自动标注手持物体姿态的方法，从自我中心相机的角度解决了手持控制器的高效6Dof姿态跟踪问题。Liu
    *等*[[1](#bib.bib1)] 提出了一个可推广的无模型6DoF物体姿态估计器，该估计器已实现完整的物体检测和姿态估计过程。通过简单地捕获未见物体的参考图像并检索参考图像的姿态，该方法可以预测任意查询图像上的物体姿态，并易于应用于日常物体的AR/VR应用。He
    *等*[[2](#bib.bib2)] 采用了重建后的匹配策略，该策略建立了查询图像与来自参考视图的重建点云之间的对应关系。这种方法不依赖于关键点匹配，即使在低纹理物体上也允许AR应用。Wen
    *等* [[3](#bib.bib3)] 通过采用大规模综合训练和创新的基于变换器的架构实现了强大的通用性。这种方法已成功应用于多个领域，包括AR和机器人操作。
- en: 6.3 Aerospace
  id: totrans-1193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 航空航天
- en: Estimating the object pose in space presents unique challenges not commonly
    encountered in terrestrial environments. One of the most significant differences
    is the lack of atmospheric scattering, which complicates lighting conditions and
    makes objects invisible over long distances. In-orbit proximity operations for
    space rendezvous, docking, and debris removal require precise pose estimation
    under diverse lighting conditions and on high-texture backgrounds. Proença *et
    al.*[[330](#bib.bib330)] proposed URSO, a simulator developed on Unreal Engine
    4 [[331](#bib.bib331)] for generating annotated images of spacecraft orbiting
    Earth, which can be used as valuable data for aerospace application. Hu *et al.*[[82](#bib.bib82)]
    proposed an encoder-decoder architecture that reliably handles large-scale changes
    under challenging conditions, enhancing robustness. Wang *et al.*[[332](#bib.bib332)]
    introduced a counterfactual analysis framework to achieve robust pose estimation
    of spaceborne targets in complex backgrounds. Ulmer *et al.*[[333](#bib.bib333)]
    generated multiple pose hypotheses for objects and introduced a pixel-level posterior
    formula to estimate the probability of each hypothesis. This approach can handle
    extreme visual conditions, including overexposure, high contrast, and low signal-to-noise
    ratio.
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: 估计空间中物体的姿态面临着在地面环境中不常遇到的独特挑战。其中最显著的区别之一是缺乏大气散射，这使得照明条件复杂，并且使得物体在长距离下不可见。空间对接、对接和碎片移除的在轨邻近操作要求在多种照明条件和高纹理背景下进行精确的姿态估计。Proença
    *et al.*[[330](#bib.bib330)] 提出了URSO，这是一种在虚幻引擎4 [[331](#bib.bib331)] 上开发的模拟器，用于生成地球轨道上飞行器的标注图像，这些图像可以作为航空航天应用的宝贵数据。Hu
    *et al.*[[82](#bib.bib82)] 提出了一个编码器-解码器架构，该架构在挑战性条件下可靠地处理大规模变化，提高了鲁棒性。Wang *et
    al.*[[332](#bib.bib332)] 引入了一个反事实分析框架，以实现复杂背景下的空间目标的鲁棒姿态估计。Ulmer *et al.*[[333](#bib.bib333)]
    为物体生成了多个姿态假设，并引入了像素级后验公式来估计每个假设的概率。这种方法可以处理极端视觉条件，包括过度曝光、高对比度和低信噪比。
- en: 6.4 Hand-Object Interaction
  id: totrans-1195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 手-物体交互
- en: When humans/robots interact with the physical world, they primarily do so through
    their hands. Therefore, accurately understanding how hands interact with objects
    is crucial.
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: 当人类/机器人与物理世界互动时，主要是通过他们的手。因此，准确理解手如何与物体互动至关重要。
- en: Hand-object interaction methods often rely on the object CAD model, and obtaining
    the object CAD model from daily life scenes is challenging. Patten *et al.*[[334](#bib.bib334)]
    reconstructed high-quality object CAD model to mitigate the reliance on object
    CAD model in hand-object interaction. To further enhance hand-object interaction,
    Lin *et al.*[[6](#bib.bib6)] utilized an effective attention model to improve
    the representation capability of hand and object features, thereby improving the
    accuracy of hand and object pose estimation. However, this method has limited
    utilization of the underlying geometric structures, leading to an increased reliance
    on visual features. Performance may degrade when objects lack visual features
    or when these features are occluded. Therefore, Rezazadeh *et al.*[[7](#bib.bib7)]
    introduced a hierarchical graph neural network architecture combined with multimodal
    (visual and tactile) data to compensate for visual deficiencies and improve robustness.
    Moreover, Qi *et al.*[[335](#bib.bib335)] introduced a hand-object pose estimation
    network guided by Signed Distance Fields (SDF), which jointly leverages the SDFs
    of both the hand and the object to provide a complete global implicit representation.
    This method aids in guiding the pose estimation of hands and objects in occlusion
    scenarios.
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
  zh: 手-物体交互方法通常依赖于物体的CAD模型，而从日常生活场景中获取物体CAD模型是具有挑战性的。Patten *et al.*[[334](#bib.bib334)]
    重建了高质量的物体CAD模型，以减少对手-物体交互中物体CAD模型的依赖。为了进一步增强手-物体交互，Lin *et al.*[[6](#bib.bib6)]
    利用了一种有效的注意力模型来提高手和物体特征的表示能力，从而提高了手和物体姿态估计的准确性。然而，这种方法对底层几何结构的利用有限，导致对视觉特征的依赖增加。当物体缺乏视觉特征或这些特征被遮挡时，性能可能会下降。因此，Rezazadeh
    *et al.*[[7](#bib.bib7)] 引入了一个层次图神经网络架构，结合了多模态（视觉和触觉）数据，以弥补视觉缺陷并提高鲁棒性。此外，Qi *et
    al.*[[335](#bib.bib335)] 引入了一种由有符号距离场（SDF）引导的手-物体姿态估计网络，该网络共同利用手和物体的SDF提供完整的全局隐式表示。这种方法有助于指导手和物体在遮挡场景中的姿态估计。
- en: 6.5 Autonomous Driving
  id: totrans-1198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 自动驾驶
- en: Object pose estimation can be used to perceive surrounding objects such as vehicles,
    pedestrians, and obstacles, aiding the autonomous driving system in making timely
    decisions.
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: 对象姿态估计可以用来感知周围的物体，如车辆、行人和障碍物，帮助自动驾驶系统做出及时决策。
- en: In order to address the pose estimation problem in autonomous driving, Hoque
    *et al.*[[336](#bib.bib336)] proposed a 6DoF pose hypothesis based on a deep hybrid
    structure composed of CNNs[[162](#bib.bib162)] and RNNs[[337](#bib.bib337)]. More
    recently, Sun *et al.*[[338](#bib.bib338)] designed an effective keypoint selection
    algorithm, which takes into account the shape information of panel objects within
    the scene of robot cabin inspection, addressing the challenge of 6DoF pose estimation
    of highly variable panel objects.
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决自动驾驶中的姿态估计问题，Hoque *et al.*[[336](#bib.bib336)] 提出了基于由CNNs[[162](#bib.bib162)]
    和RNNs[[337](#bib.bib337)] 组成的深度混合结构的6DoF姿态假设。最近，Sun *et al.*[[338](#bib.bib338)]
    设计了一种有效的关键点选择算法，该算法考虑了机器人驾驶舱检查场景中面板物体的形状信息，解决了高度可变面板物体的6DoF姿态估计挑战。
- en: 7 Conclusion and Future Direction
  id: totrans-1201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与未来方向
- en: 'In this survey, we have provided a systematic overview of the latest deep learning-based
    object pose estimation methods, covering a comprehensive classification, a comparison
    of their strengths and weaknesses, and an exploration of their applications. Despite
    the great success, many challenges still exist, as discussed in Sec. [3](#S3 "3
    Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey"), Sec. [4](#S4 "4 Category-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"), and Sec.
    [5](#S5 "5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey"). Based on these challenges, we further point out some
    promising future directions aimed at advancing research in object pose estimation.'
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项调查中，我们提供了最新的基于深度学习的对象姿态估计方法的系统概述，包括全面的分类、优缺点比较以及应用探索。尽管取得了很大成功，但仍然存在许多挑战，如在第[3](#S3
    "3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")节、第[4](#S4 "4 Category-Level Object Pose Estimation ‣
    Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")节和第[5](#S5
    "5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")节讨论的那样。基于这些挑战，我们进一步指出了一些有前景的未来方向，以推动对象姿态估计研究的进展。'
- en: 'From the perspective of label-efficient learning, prevailing methodologies
    predominantly rely on the utilization of real-world labeled datasets for training
    purposes. Nevertheless, the labor-intensive nature of manually collecting and
    annotating training data is widely acknowledged. Hence, we advocate for the exploration
    of label-efficient learning techniques for object pose estimation, which can be
    pursued through the following avenues: 1) LLMs/LVMs-guided weak/self-supervised
    learning methods. With the rapid advancements in pre-trained LLMs/LVMs, their
    versatile application in various scenarios through an unsupervised manner has
    become feasible. Leveraging LLMs/LVMs as prior knowledge holds promise for exploring
    weak or self-supervised learning techniques in object pose estimation. 2) Synthesis
    to real-world domain adaptation and generalization methods. Due to the high costs
    associated with acquiring real-world training data through manual efforts, synthetic
    data generation offers a cost-effective alternative. We believe that by exploring
    domain adaptation and generalization techniques from synthetic to real-world domains,
    we can mitigate domain gaps and achieve the capability to generalize synthetic
    data-trained models for real-world applications.'
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: 从标签效率学习的角度来看，现有的方法主要依赖于利用真实世界的标注数据集进行训练。然而，人工收集和标注训练数据的劳动密集性广泛被认可。因此，我们主张探索对象姿态估计的标签效率学习技术，这可以通过以下途径进行：1）LLMs/LVMs指导的弱监督/自监督学习方法。随着预训练LLMs/LVMs的快速发展，它们在各种场景中的多功能应用已变得可行。利用LLMs/LVMs作为先验知识对于探索对象姿态估计中的弱监督或自监督学习技术具有前景。2）合成到真实世界领域适应和泛化方法。由于通过人工手段获取真实世界训练数据的高成本，合成数据生成提供了一种具有成本效益的替代方案。我们相信，通过探索从合成到真实世界领域的领域适应和泛化技术，我们可以减小领域差距，实现将合成数据训练的模型推广到真实世界应用的能力。
- en: 'In terms of applications, facilitating the deployment of object pose estimation
    methods on mobile devices and robots is crucial. We argue that enhancing the deployability
    of existing methods can be achieved through the following approaches: 1) End-to-end
    methods integrating detection or segmentation. Current SOTA approaches typically
    require initial object detection or segmentation using a pre-trained model before
    inputting the image into a pose estimation model (indirect pose estimation models
    even need to use non-differentiable PnP or Umeyama algorithms to solve pose),
    which complicates deployment. Future research can enhance the deployability on
    mobile devices and robots by exploring end-to-end object pose estimation methods
    that seamlessly integrate detection or segmentation. 2) Single RGB image-based
    methods. Given that most mobile devices (such as smartphones and tablets) lack
    depth cameras, achieving high-precision estimation of unseen object poses using
    a single RGB image is crucial. Due to inherent geometric limitations in 2D images,
    future research can explore LVMs-based monocular depth estimation methods to enhance
    the accuracy of monocular object pose estimation by incorporating scene-level
    depth information. 3) Model lightweighting. Existing SOTA models often have large
    parameter sizes and inefficient running performance, which presents challenges
    for deployment on mobile devices and robots with limited computational resources.
    Future work can explore effective lightweight methods, such as teacher-student
    models, to research reducing model parameter count (GPU memory) and improving
    model running efficiency.'
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用方面，促进物体姿态估计方法在移动设备和机器人上的部署至关重要。我们认为，提升现有方法的可部署性可以通过以下途径实现：1）集成检测或分割的端到端方法。目前的最先进方法通常需要先用预训练模型进行物体检测或分割，然后再将图像输入姿态估计模型（间接姿态估计模型甚至需要使用非可微的PnP或Umeyama算法来解决姿态问题），这增加了部署的复杂性。未来的研究可以通过探索无缝集成检测或分割的端到端物体姿态估计方法来提升在移动设备和机器人上的可部署性。2）基于单一RGB图像的方法。由于大多数移动设备（如智能手机和平板电脑）缺乏深度摄像头，使用单一RGB图像实现对未见物体姿态的高精度估计至关重要。由于二维图像的固有几何限制，未来的研究可以探索基于LVMs的单目深度估计方法，通过融入场景级深度信息来提高单目物体姿态估计的准确性。3）模型轻量化。现有的最先进模型通常具有较大的参数量和低效的运行性能，这对计算资源有限的移动设备和机器人来说带来了挑战。未来的工作可以探索有效的轻量化方法，如教师-学生模型，研究减少模型参数数量（GPU内存）和提高模型运行效率。
- en: 'Existing methods are predominantly designed for common objects and scenes,
    rendering them ineffective for challenging objects and scenes. We believe that
    the applicability can be enhanced through the following avenues: 1) Articulated
    object pose estimation. Articulated objects (such as clothing and drawers) exhibit
    multiple DoF and significant self-occlusion compared to rigid objects, making
    pose estimation challenging. Achieving high-precision pose estimation for articulated
    objects is an important research problem that remains to be addressed in the future.
    2) Transparent object pose estimation. The simultaneous absence of texture, color,
    and depth information poses a significant challenge for estimating the pose of
    transparent objects. Future research endeavors could focus on enhancing the geometric
    information of transparent objects through depth augmentation or completion techniques,
    thereby improving the accuracy of pose estimation. 3) Robust methods for handling
    occlusion. Occlusion is the most common challenge. Currently, there exists no
    object pose estimation method that can effectively handle severe occlusion. Severe
    occlusion leads to an incomplete representation of texture and geometric features
    in objects, introducing uncertainty into the pose estimation model. Hence, improving
    the model’s ability to perceive severe occlusion is crucial for enhancing its
    robustness.'
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
  zh: 现有方法主要针对常见物体和场景进行设计，因此对于具有挑战性的物体和场景效果不佳。我们认为，可以通过以下途径提高其适用性：1）**关节物体姿态估计**。与刚性物体相比，关节物体（如衣物和抽屉）具有多个自由度和显著的自遮挡，使得姿态估计变得更加困难。实现高精度的关节物体姿态估计是一个重要的研究问题，未来需要加以解决。2）**透明物体姿态估计**。透明物体同时缺乏纹理、颜色和深度信息，这对姿态估计提出了重大挑战。未来的研究工作可以集中在通过深度增强或补全技术来提高透明物体的几何信息，从而提高姿态估计的准确性。3）**处理遮挡的鲁棒方法**。遮挡是最常见的挑战。目前，还没有一种物体姿态估计方法能够有效处理严重的遮挡。严重的遮挡导致物体的纹理和几何特征表示不完整，从而给姿态估计模型带来不确定性。因此，提高模型对严重遮挡的感知能力对于增强其鲁棒性至关重要。
- en: 'From the aspect of problem formulation, recent instance-level methods have
    achieved high precision but exhibit poor generalization. Category-level methods
    demonstrate good generalization for intra-class unseen objects but fail to generalize
    to unseen object categories. Unseen object pose estimation methods have the potential
    to generalize to any unseen object, yet they still rely on object CAD models or
    reference views. The following paths can be explored from the problem formulation
    to further enhance the generalization of object pose estimation: 1) Few-shot learning-based
    category-level methods for unseen categories. Since category-level methods need
    to re-obtain a large amount of annotated training data for unseen object categories,
    their generalization is severely limited. Therefore, future research could focus
    on exploring how to leverage few-shot learning to enable the rapid generalization
    of category-level methods to unseen object categories. 2) CAD model-free and sparse
    manual reference view-based unseen object pose estimation. While current unseen
    object pose estimation methods do not require retraining for unseen objects, they
    still rely on either the CAD models or extensive annotated reference views of
    unseen objects, both of which still require manual acquisition. To this end, exploring
    CAD model-free and sparse manual reference view-based unseen object pose estimation
    methods is crucial. 3) Open-vocabulary strong generalization methods. Given the
    broad applicability of object pose estimation in human-machine interaction scenes,
    future research could leverage open vocabulary provided by humans as prompts to
    enhance generalization to unseen objects and scenes.'
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: 从问题表述的角度来看，近期的实例级方法已实现高精度，但表现出较差的泛化能力。类别级方法对类内未见物体展示了良好的泛化能力，但未能对未见物体类别进行泛化。未见物体位姿估计方法具有对任何未见物体进行泛化的潜力，但仍然依赖于物体的CAD模型或参考视图。以下路径可以从问题表述中进一步探索，以增强物体位姿估计的泛化能力：1）基于少样本学习的类别级方法针对未见类别。由于类别级方法需要重新获得大量标注训练数据以处理未见物体类别，其泛化能力受到严重限制。因此，未来的研究可以关注如何利用少样本学习实现类别级方法对未见物体类别的快速泛化。2）无CAD模型和稀疏人工参考视图的未见物体位姿估计。虽然当前的未见物体位姿估计方法不需要对未见物体进行重新训练，但它们仍然依赖于CAD模型或大量标注的参考视图，这两者仍需人工获取。因此，探索无CAD模型和稀疏人工参考视图的未见物体位姿估计方法至关重要。3）开放词汇强泛化方法。鉴于物体位姿估计在人机交互场景中的广泛应用，未来的研究可以利用人类提供的开放词汇作为提示，以增强对未见物体和场景的泛化能力。
- en: Acknowledgments
  id: totrans-1207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported by the National Natural Science Foundation of China
    under Grant U22A2059, Shenzhen Science and Technology Foundation under Grant 2021Szvup035,
    China Scholarship Council under Grant 202306130074 and Grant 202206130048, Natural
    Science Foundation of Hunan Province under Grant 2024JJ5098, and by the State
    Key Laboratory of Advanced Design and Manufacturing for Vehicle Body Open Foundation.
    Ajmal Mian was supported by the Australian Research Council Future Fellowship
    Award funded by the Australian Government under Project FT210100268.
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了中国国家自然科学基金资助（资助号 U22A2059）、深圳市科技创新基金（资助号 2021Szvup035）、中国留学基金委（资助号 202306130074
    和 202206130048）、湖南省自然科学基金（资助号 2024JJ5098）及国家车辆车身先进设计与制造重点实验室开放基金的支持。Ajmal Mian
    获得了澳大利亚政府资助的澳大利亚研究委员会未来奖学金（资助号 FT210100268）的支持。
- en: References
  id: totrans-1209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Y. Liu and Y. Wen, “Gen6d: Generalizable model-free 6-dof object pose estimation
    from rgb images,” in ECCV, 2022.'
  id: totrans-1210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y. 刘和 Y. 温，“Gen6d：从 RGB 图像中进行可泛化的无模型 6-dof 物体位姿估计，”在 ECCV，2022。'
- en: '[2] X. He and J. Sun, “Onepose++: Keypoint-free one-shot object pose estimation
    without cad models,” in NeurIPS, 2022.'
  id: totrans-1211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] X. 何和 J. 孙，“Onepose++：无关键点一-shot物体位姿估计，无需 CAD 模型，”在 NeurIPS，2022。'
- en: '[3] B. Wen and W. Yang, “Foundationpose: Unified 6d pose estimation and tracking
    of novel objects,” in CVPR, 2024.'
  id: totrans-1212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] B. 温和 W. 杨，“Foundationpose：新颖物体的统一 6d 位姿估计与跟踪，”在 CVPR，2024。'
- en: '[4] J. Liu and W. Sun, “Robotic continuous grasping system by shape transformer-guided
    multi-object category-level 6d pose estimation,” IEEE TII, 2023.'
  id: totrans-1213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. 刘和 W. 孙，“由形状变换器指导的多对象类别级 6d 位姿估计的机器人连续抓取系统，”IEEE TII，2023。'
- en: '[5] J. Liu and W. Sun, “Domain-generalized robotic picking via contrastive
    learning-based 6-d pose estimation,” IEEE TII, 2024.'
  id: totrans-1214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. 刘和 W. 孙，“通过对比学习的领域通用机器人拾取的 6-d 位姿估计，”IEEE TII，2024。'
- en: '[6] Z. Lin and C. Ding, “Harmonious feature learning for interactive hand-object
    pose estimation,” in CVPR, 2023.'
  id: totrans-1215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Z. Lin 和 C. Ding，“用于交互式手-物体姿态估计的和谐特征学习，”在 CVPR，2023。'
- en: '[7] A. Rezazadeh and S. Dikhale, “Hierarchical graph nural networks for proprioceptive
    6d pose estimation of in-hand objects,” in ICRA, 2023.'
  id: totrans-1216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. Rezazadeh 和 S. Dikhale，“用于手持物体的 6d 姿态估计的层次图神经网络，”在 ICRA，2023。'
- en: '[8] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
    IJCV, 2004.'
  id: totrans-1217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] D. G. Lowe，“从尺度不变关键点提取独特的图像特征，”IJCV，2004。'
- en: '[9] R. B. Rusu and N. Blodow, “Fast point feature histograms (fpfh) for 3d
    registration,” in ICRA, 2009.'
  id: totrans-1218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] R. B. Rusu 和 N. Blodow，“用于 3d 配准的快速点特征直方图（fpfh），”在 ICRA，2009。'
- en: '[10] R. B. Rusu and G. Bradski, “Fast 3d recognition and pose using the viewpoint
    feature histogram,” in IROS, 2010.'
  id: totrans-1219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] R. B. Rusu 和 G. Bradski，“使用视点特征直方图进行快速 3d 识别和姿态估计，”在 IROS，2010。'
- en: '[11] B. Drost and M. Ulrich, “Model globally, match locally: Efficient and
    robust 3d object recognition,” in CVPR, 2010.'
  id: totrans-1220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] B. Drost 和 M. Ulrich，“全局建模，本地匹配：高效且鲁棒的 3d 物体识别，”在 CVPR，2010。'
- en: '[12] C. Choi and H. I. Christensen, “3d pose estimation of daily objects using
    an rgb-d camera,” in IROS, 2012.'
  id: totrans-1221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] C. Choi 和 H. I. Christensen，“使用 rgb-d 相机进行日常物体的 3d 姿态估计，”在 IROS，2012。'
- en: '[13] C. Choi and A. J. Trevor, “Rgb-d edge detection and edge-based registration,”
    in IROS, 2013.'
  id: totrans-1222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] C. Choi 和 A. J. Trevor，“rgb-d 边缘检测和基于边缘的配准，”在 IROS，2013。'
- en: '[14] T. Birdal and S. Ilic, “Point pair features based object detection and
    pose estimation revisited,” in 3DV, 2015.'
  id: totrans-1223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] T. Birdal 和 S. Ilic，“基于点对特征的物体检测和姿态估计重访，”在 3DV，2015。'
- en: '[15] Y. Xiang and T. Schmidt, “Posecnn: A convolutional neural network for
    6d object pose estimation in cluttered scenes,” arXiv preprint arXiv:1711.00199,
    2017.'
  id: totrans-1224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Xiang 和 T. Schmidt，“Posecnn：用于混乱场景中 6d 物体姿态估计的卷积神经网络，”arXiv 预印本 arXiv:1711.00199，2017。'
- en: '[16] C. Wang and D. Xu, “Densefusion: 6d object pose estimation by iterative
    dense fusion,” in CVPR, 2019.'
  id: totrans-1225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] C. Wang 和 D. Xu，“Densefusion：通过迭代密集融合进行 6d 物体姿态估计，”在 CVPR，2019。'
- en: '[17] S. Peng and Y. Liu, “Pvnet: Pixel-wise voting network for 6dof pose estimation,”
    in CVPR, 2019.'
  id: totrans-1226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. Peng 和 Y. Liu，“Pvnet：用于 6dof 姿态估计的逐像素投票网络，”在 CVPR，2019。'
- en: '[18] Y. He and W. Sun, “Pvn3d: A deep point-wise 3d keypoints voting network
    for 6dof pose estimation,” in CVPR, 2020.'
  id: totrans-1227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Y. He 和 W. Sun，“Pvn3d：用于 6dof 姿态估计的深度逐点 3d 关键点投票网络，”在 CVPR，2020。'
- en: '[19] Z. Li and G. Wang, “Cdpn: Coordinates-based disentangled pose network
    for real-time rgb-based 6-dof object pose estimation,” in ICCV, 2019.'
  id: totrans-1228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Z. Li 和 G. Wang，“Cdpn：基于坐标的解耦姿态网络用于实时 rgb 基于 6-dof 物体姿态估计，”在 ICCV，2019。'
- en: '[20] S. Zakharov and I. Shugurov, “Dpod: 6d pose object detector and refiner,”
    in ICCV, 2019.'
  id: totrans-1229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] S. Zakharov 和 I. Shugurov，“Dpod：6d 姿态物体检测器和精化器，”在 ICCV，2019。'
- en: '[21] Y. He and H. Huang, “Ffb6d: A full flow bidirectional fusion network for
    6d pose estimation,” in CVPR, 2021.'
  id: totrans-1230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. He 和 H. Huang，“Ffb6d：用于 6d 姿态估计的全流程双向融合网络，”在 CVPR，2021。'
- en: '[22] H. Wang and S. Sridhar, “Normalized object coordinate space for category-level
    6d object pose and size estimation,” in CVPR, 2019.'
  id: totrans-1231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] H. Wang 和 S. Sridhar，“用于类别级别 6d 物体姿态和尺寸估计的标准化物体坐标空间，”在 CVPR，2019。'
- en: '[23] K. Chen and Q. Dou, “Sgpa: Structure-guided prior adaptation for category-level
    6d object pose estimation,” in ICCV, 2021.'
  id: totrans-1232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] K. Chen 和 Q. Dou，“Sgpa：用于类别级别 6d 物体姿态估计的结构引导先验适应，”在 ICCV，2021。'
- en: '[24] J. Lin and Z. Wei, “Category-level 6d object pose and size estimation
    using self-supervised deep prior deformation networks,” in ECCV, 2022.'
  id: totrans-1233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Lin 和 Z. Wei，“使用自监督深度先验变形网络进行类别级别 6d 物体姿态和尺寸估计，”在 ECCV，2022。'
- en: '[25] Y. Di and R. Zhang, “Gpv-pose: Category-level object pose estimation via
    geometry-guided point-wise voting,” in CVPR, 2022.'
  id: totrans-1234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. Di 和 R. Zhang，“Gpv-pose：通过几何引导的逐点投票进行类别级别物体姿态估计，”在 CVPR，2022。'
- en: '[26] L. Zheng and C. Wang, “Hs-pose: Hybrid scope feature extraction for category-level
    object pose estimation,” in CVPR, 2023.'
  id: totrans-1235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] L. Zheng 和 C. Wang，“Hs-pose：用于类别级别物体姿态估计的混合范围特征提取，”在 CVPR，2023。'
- en: '[27] J. Liu and Y. Chen, “Ist-net: Prior-free category-level pose estimation
    with implicit space transformation,” in ICCV, 2023.'
  id: totrans-1236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] J. Liu 和 Y. Chen，“Ist-net：基于隐式空间变换的无先验类别级姿态估计，”在 ICCV，2023。'
- en: '[28] Y. Labbé and L. Manuelli, “Megapose: 6d pose estimation of novel objects
    via render & compare,” in CoRL, 2022.'
  id: totrans-1237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Labbé 和 L. Manuelli，“Megapose：通过渲染和比较进行的新物体 6d 姿态估计，”在 CoRL，2022。'
- en: '[29] V. N. Nguyen and T. Groueix, “Gigapose: Fast and robust novel object pose
    estimation via one correspondence,” in CVPR, 2024.'
  id: totrans-1238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] V. N. Nguyen 和 T. Groueix，“Gigapose：通过单一对应关系进行快速且鲁棒的新物体姿态估计，”在 CVPR，2024。'
- en: '[30] J. Lin and L. Liu, “Sam-6d: Segment anything model meets zero-shot 6d
    object pose estimation,” in CVPR, 2024.'
  id: totrans-1239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Lin 和 L. Liu， “Sam-6d：Segment Anything 模型与零样本 6D 物体姿态估计的结合，” 见 CVPR，2024。'
- en: '[31] S. Hoque and M. Y. Arafat, “A comprehensive review on 3d object detection
    and 6d pose estimation with deep learning,” IEEE Access, 2021.'
  id: totrans-1240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S. Hoque 和 M. Y. Arafat， “关于 3D 物体检测和 6D 姿态估计的深度学习综述，” IEEE Access，2021。'
- en: '[32] G. Marullo and L. Tanzi, “6d object position estimation from 2d images:
    A literature review,” Multimed. Tools Appl., 2023.'
  id: totrans-1241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] G. Marullo 和 L. Tanzi， “从 2D 图像中估计 6D 物体位置：文献综述，” Multimed. Tools Appl.，2023。'
- en: '[33] Z. Fan and Y. Zhu, “Deep learning on monocular object pose detection and
    tracking: A comprehensive overview,” ACM Comput. Surv., 2022.'
  id: totrans-1242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Z. Fan 和 Y. Zhu， “单目物体姿态检测和跟踪中的深度学习：全面概述，” ACM Comput. Surv.，2022。'
- en: '[34] G. Du and K. Wang, “Vision-based robotic grasping from object localization,
    object pose estimation to grasp estimation for parallel grippers: A review,” Artif
    Intell Rev., 2021.'
  id: totrans-1243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] G. Du 和 K. Wang， “基于视觉的机器人抓取：从物体定位、物体姿态估计到抓取估计的平行夹爪：综述，” Artif Intell
    Rev.，2021。'
- en: '[35] J. Guan and Y. Hao, “A survey of 6dof object pose estimation methods for
    different application scenarios,” Sensors, 2024.'
  id: totrans-1244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] J. Guan 和 Y. Hao， “不同应用场景下的 6DOF 物体姿态估计方法综述，” Sensors，2024。'
- en: '[36] T. Hodan and M. Sundermeyer, “Bop challenge 2023 on detection, segmentation
    and pose estimation of seen and unseen rigid objects,” arXiv preprint arXiv:2403.09799,
    2024.'
  id: totrans-1245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] T. Hodan 和 M. Sundermeyer， “Bop challenge 2023：关于已见和未见刚性物体的检测、分割和姿态估计，”
    arXiv 预印本 arXiv:2403.09799，2024。'
- en: '[37] S. Hinterstoisser and V. Lepetit, “Model based training, detection and
    pose estimation of texture-less 3d objects in heavily cluttered scenes,” in ACCV,
    2012.'
  id: totrans-1246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. Hinterstoisser 和 V. Lepetit， “基于模型的训练、检测和姿态估计：在高度混杂场景中进行无纹理 3D 物体的处理，”
    见 ACCV，2012。'
- en: '[38] E. Brachmann and F. Michel, “Uncertainty-driven 6d pose estimation of
    objects and scenes from a single rgb image,” in CVPR, 2016.'
  id: totrans-1247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] E. Brachmann 和 F. Michel， “基于不确定性的 6D 姿态估计：从单张 RGB 图像中获取物体和场景信息，” 见 CVPR，2016。'
- en: '[39] E. Brachmann and A. Krull, “Learning 6d object pose estimation using 3d
    object coordinates,” in ECCV, 2014.'
  id: totrans-1248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] E. Brachmann 和 A. Krull， “使用 3D 物体坐标学习 6D 物体姿态估计，” 见 ECCV，2014。'
- en: '[40] A. Tejani and D. Tang, “Latent-class hough forests for 3d object detection
    and pose estimation,” in ECCV, 2014.'
  id: totrans-1249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] A. Tejani 和 D. Tang， “用于 3D 物体检测和姿态估计的潜在类别霍夫森林，” 见 ECCV，2014。'
- en: '[41] A. Doumanoglou and R. Kouskouridas, “Recovering 6d object pose and predicting
    next-best-view in the crowd,” in CVPR, 2016.'
  id: totrans-1250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] A. Doumanoglou 和 R. Kouskouridas， “在拥挤环境中恢复 6D 物体姿态并预测最佳下一视角，” 见 CVPR，2016。'
- en: '[42] C. Rennie and R. Shome, “A dataset for improved rgbd-based object detection
    and pose estimation for warehouse pick-and-place,” IEEE RAL, 2016.'
  id: totrans-1251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] C. Rennie 和 R. Shome， “用于改进基于 RGBD 的物体检测和姿态估计的数据集，用于仓库的取放操作，” IEEE RAL，2016。'
- en: '[43] B. Calli and A. Singh, “The ycb object and model set: Towards common benchmarks
    for manipulation research,” in ICRA, 2015.'
  id: totrans-1252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] B. Calli 和 A. Singh， “YCB 物体和模型集：朝着操作研究的共同基准迈进，” 见 ICRA，2015。'
- en: '[44] T. Hodan and P. Haluza, “T-less: An rgb-d dataset for 6d pose estimation
    of texture-less objects,” in WACV, 2017.'
  id: totrans-1253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] T. Hodan 和 P. Haluza， “T-less：一个用于无纹理物体 6D 姿态估计的 RGB-D 数据集，” 见 WACV，2017。'
- en: '[45] B. Drost and M. Ulrich, “Introducing mvtec itodd - a dataset for 3d object
    recognition in industry,” in ICCVW, 2017.'
  id: totrans-1254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] B. Drost 和 M. Ulrich， “介绍 mvtec itodd - 一个用于工业中 3D 物体识别的数据集，” 见 ICCVW，2017。'
- en: '[46] R. Kaskman and S. Zakharov, “Homebreweddb: Rgb-d dataset for 6d pose estimation
    of 3d objects,” in ICCVW, 2019.'
  id: totrans-1255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] R. Kaskman 和 S. Zakharov， “Homebreweddb：用于 3D 物体 6D 姿态估计的 RGB-D 数据集，”
    见 ICCVW，2019。'
- en: '[47] S. Tyree and J. Tremblay, “6-dof pose estimation of household objects
    for robotic manipulation: An accessible dataset and benchmark,” in IROS, 2022.'
  id: totrans-1256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] S. Tyree 和 J. Tremblay， “用于机器人操作的家庭物体 6-DOF 姿态估计：一个可访问的数据集和基准，” 见 IROS，2022。'
- en: '[48] B. Wen and C. Mitash, “Se(3)-tracknet: Data-driven 6d pose tracking by
    calibrating image residuals in synthetic domains,” in IROS, 2020.'
  id: totrans-1257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] B. Wen 和 C. Mitash， “Se(3)-tracknet：通过校准合成领域中的图像残差进行数据驱动的 6D 姿态跟踪，” 见
    IROS，2020。'
- en: '[49] X. Chen and H. Zhang, “Clearpose: Large-scale transparent object dataset
    and benchmark,” in ECCV, 2022.'
  id: totrans-1258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] X. Chen 和 H. Zhang， “Clearpose：大规模透明物体数据集和基准，” 见 ECCV，2022。'
- en: '[50] L. Chen and H. Yang, “Mp6d: An rgb-d dataset for metal parts’ 6d pose
    estimation,” IEEE RAL, 2022.'
  id: totrans-1259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] L. Chen 和 H. Yang， “Mp6d：一个用于金属部件 6D 姿态估计的 RGB-D 数据集，” IEEE RAL，2022。'
- en: '[51] A. X. Chang and T. Funkhouser, “Shapenet: An information-rich 3d model
    repository,” arXiv preprint arXiv:1512.03012, 2015.'
  id: totrans-1260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] A. X. Chang 和 T. Funkhouser， “Shapenet：一个信息丰富的 3D 模型库，” arXiv 预印本 arXiv:1512.03012，2015。'
- en: '[52] L. Manuelli and W. Gao, “kpam: Keypoint affordances for category-level
    robotic manipulation,” in ISRR, 2019.'
  id: totrans-1261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] L. Manuelli 和 W. Gao，“kpam：类别级机器人操作的关键点适配性，”发表于 ISRR，2019。'
- en: '[53] X. Liu and R. Jonschkowski, “Keypose: Multi-view 3d labeling and keypoint
    estimation for transparent objects,” in CVPR, 2020.'
  id: totrans-1262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] X. Liu 和 R. Jonschkowski，“Keypose：用于透明物体的多视角 3D 标注和关键点估计，”发表于 CVPR，2020。'
- en: '[54] A. Ahmadyan and L. Zhang, “Objectron: A large scale dataset of object-centric
    videos in the wild with pose annotations,” in CVPR, 2021.'
  id: totrans-1263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] A. Ahmadyan 和 L. Zhang，“Objectron：一个大规模的以物体为中心的视频数据集，包含姿态标注，”发表于 CVPR，2021。'
- en: '[55] Y. Ze and X. Wang, “Category-level 6d object pose estimation in the wild:
    A semi-supervised learning approach and a new dataset,” in NeurIPS, 2022.'
  id: totrans-1264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Y. Ze 和 X. Wang，“野外场景下的类别级 6D 物体姿态估计：一种半监督学习方法和一个新数据集，”发表于 NeurIPS，2022。'
- en: '[56] P. Wang and H. Jung, “Phocal: A multi-modal dataset for category-level
    object pose estimation with photometrically challenging objects,” in CVPR, 2022.'
  id: totrans-1265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] P. Wang 和 H. Jung，“Phocal：一个用于类别级物体姿态估计的多模态数据集，包含光度挑战物体，”发表于 CVPR，2022。'
- en: '[57] H. Jung and S.-C. Wu, “Housecat6d–a large-scale multi-modal category level
    6d object pose dataset with household objects in realistic scenarios,” in CVPR,
    2024.'
  id: totrans-1266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] H. Jung 和 S.-C. Wu，“Housecat6d——一个大规模多模态类别级 6D 物体姿态数据集，包含现实场景中的家用物品，”发表于
    CVPR，2024。'
- en: '[58] F. Michel and A. Krull, “Pose estimation of kinematic chain instances
    via object coordinate regression.,” in BMVC, 2015.'
  id: totrans-1267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] F. Michel 和 A. Krull，“通过物体坐标回归进行运动链实例的姿态估计，”发表于 BMVC，2015。'
- en: '[59] R. Martín-Martín and C. Eppner, “The rbo dataset of articulated objects
    and interactions,” IJRR, 2019.'
  id: totrans-1268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] R. Martín-Martín 和 C. Eppner，“RBO 数据集：关节物体及其交互，”IJRR，2019。'
- en: '[60] Y. Liu and Y. Liu, “Hoi4d: A 4d egocentric dataset for category-level
    human-object interaction,” in CVPR, 2022.'
  id: totrans-1269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Y. Liu 和 Y. Liu，“Hoi4d：一个 4D 以自我为中心的类别级人机交互数据集，”发表于 CVPR，2022。'
- en: '[61] L. Liu and H. Xue, “Toward real-world category-level articulation pose
    estimation,” IEEE TIP, 2022.'
  id: totrans-1270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] L. Liu 和 H. Xue，“迈向现实世界的类别级关节姿态估计，”IEEE TIP，2022。'
- en: '[62] Z. Zhu and J. Wang, “Contactart: Learning 3d interaction priors for category-level
    articulated object and hand poses estimation,” arXiv preprint arXiv:2305.01618,
    2023.'
  id: totrans-1271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Z. Zhu 和 J. Wang，“Contactart：学习用于类别级关节物体和手部姿态估计的 3D 交互先验，”arXiv 预印本 arXiv:2305.01618，2023。'
- en: '[63] Y. Qin and H. Su, “From one hand to multiple hands: Imitation learning
    for dexterous manipulation from single-camera teleoperation,” IEEE RAL, 2022.'
  id: totrans-1272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Y. Qin 和 H. Su，“从一只手到多只手：从单目相机遥操作中学习灵巧操控的模仿学习，”IEEE RAL，2022。'
- en: '[64] K. Mo and S. Zhu, “Partnet: A large-scale benchmark for fine-grained and
    hierarchical part-level 3d object understanding,” in CVPR, 2019.'
  id: totrans-1273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] K. Mo 和 S. Zhu，“Partnet：用于细粒度和层次化部分级 3D 物体理解的大规模基准，”发表于 CVPR，2019。'
- en: '[65] K. Park and A. Mousavian, “Latentfusion: End-to-end differentiable reconstruction
    and rendering for unseen object pose estimation,” in CVPR, 2020.'
  id: totrans-1274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] K. Park 和 A. Mousavian，“Latentfusion：用于未见物体姿态估计的端到端可微重建和渲染，”发表于 CVPR，2020。'
- en: '[66] J. Sun and Z. Wang, “Onepose: One-shot object pose estimation without
    cad models,” in CVPR, 2022.'
  id: totrans-1275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] J. Sun 和 Z. Wang，“Onepose：无需 CAD 模型的一次性物体姿态估计，”发表于 CVPR，2022。'
- en: '[67] P. Wohlhart and V. Lepetit, “Learning descriptors for object recognition
    and 3d pose estimation,” in CVPR, 2015.'
  id: totrans-1276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] P. Wohlhart 和 V. Lepetit，“用于物体识别和 3D 姿态估计的描述符学习，”发表于 CVPR，2015。'
- en: '[68] G. Wang and F. Manhardt, “Self6d: Self-supervised monocular 6d object
    pose estimation,” in ECCV, 2020.'
  id: totrans-1277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] G. Wang 和 F. Manhardt，“Self6d：自监督单目 6D 物体姿态估计，”发表于 ECCV，2020。'
- en: '[69] X. Jiang and D. Li, “Uni6d: A unified cnn framework without projection
    breakdown for 6d pose estimation,” in CVPR, 2022.'
  id: totrans-1278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] X. Jiang 和 D. Li，“Uni6d：一个统一的 CNN 框架，无投影断裂的 6D 姿态估计，”发表于 CVPR，2022。'
- en: '[70] H. Li and J. Lin, “Dcl-net: Deep correspondence learning network for 6d
    pose estimation,” in ECCV, 2022.'
  id: totrans-1279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] H. Li 和 J. Lin，“Dcl-net：用于 6D 姿态估计的深度对应学习网络，”发表于 ECCV，2022。'
- en: '[71] J. Shotton and B. Glocker, “Scene coordinate regression forests for camera
    relocalization in rgb-d images,” in CVPR, 2013.'
  id: totrans-1280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] J. Shotton 和 B. Glocker，“用于 RGB-D 图像中的相机重定位的场景坐标回归森林，”发表于 CVPR，2013。'
- en: '[72] M. Tian and M. H. Ang, “Shape prior deformation for categorical 6d object
    pose and size estimation,” in ECCV, 2020.'
  id: totrans-1281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] M. Tian 和 M. H. Ang，“用于类别级 6D 物体姿态和大小估计的形状先验变形，”发表于 ECCV，2020。'
- en: '[73] M. A. Fischler and R. C. Bolles, “Random sample consensus,” COMMUN ACM,
    1981.'
  id: totrans-1282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] M. A. Fischler 和 R. C. Bolles，“随机样本一致性，”COMMUN ACM，1981。'
- en: '[74] M. Rad and V. Lepetit, “Bb8: A scalable, accurate, robust to partial occlusion
    method for predicting the 3d poses of challenging objects without using depth,”
    in ICCV, 2017.'
  id: totrans-1283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] M. Rad 和 V. Lepetit，“Bb8：一种可扩展、准确、对部分遮挡鲁棒的方法，用于预测具有挑战性的物体的 3d 姿态，无需使用深度”，发表于
    ICCV，2017。'
- en: '[75] B. Tekin and S. N. Sinha, “Real-time seamless single shot 6d object pose
    prediction,” in CVPR, 2018.'
  id: totrans-1284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] B. Tekin 和 S. N. Sinha，“实时无缝单次 6d 物体姿态预测”，发表于 CVPR，2018。'
- en: '[76] J. Redmon and S. Divvala, “You only look once: Unified, real-time object
    detection,” in CVPR, 2016.'
  id: totrans-1285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] J. Redmon 和 S. Divvala，“你只需看一次：统一的实时物体检测”，发表于 CVPR，2016。'
- en: '[77] G. Pavlakos and X. Zhou, “6-dof object pose from semantic keypoints,”
    in ICRA, 2017.'
  id: totrans-1286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] G. Pavlakos 和 X. Zhou，“从语义关键点获得 6-dof 物体姿态”，发表于 ICRA，2017。'
- en: '[78] B. Doosti and S. Naha, “Hope-net: A graph-based model for hand-object
    pose estimation,” in CVPR, 2020.'
  id: totrans-1287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] B. Doosti 和 S. Naha，“Hope-net：一种基于图的手物体姿态估计模型”，发表于 CVPR，2020。'
- en: '[79] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” arXiv preprint arXiv:1609.02907, 2016.'
  id: totrans-1288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] T. N. Kipf 和 M. Welling，“基于图卷积网络的半监督分类”，arXiv 预印本 arXiv:1609.02907，2016。'
- en: '[80] C. Song and J. Song, “Hybridpose: 6d object pose estimation under hybrid
    representations,” in CVPR, 2020.'
  id: totrans-1289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] C. Song 和 J. Song，“Hybridpose：在混合表示下的 6d 物体姿态估计”，发表于 CVPR，2020。'
- en: '[81] P. Liu and Q. Zhang, “Mfpn-6d : Real-time one-stage pose estimation of
    objects on rgb images,” in ICRA, 2021.'
  id: totrans-1290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] P. Liu 和 Q. Zhang，“Mfpn-6d：RGB 图像上物体的实时单阶段姿态估计”，发表于 ICRA，2021。'
- en: '[82] Y. Hu and S. Speierer, “Wide-depth-range 6d object pose estimation in
    space,” in CVPR, 2021.'
  id: totrans-1291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Y. Hu 和 S. Speierer，“广泛深度范围的 6d 物体姿态估计”，发表于 CVPR，2021。'
- en: '[83] R. Lian and H. Ling, “Checkerpose: Progressive dense keypoint localization
    for object pose estimation with graph neural network,” in ICCV, 2023.'
  id: totrans-1292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] R. Lian 和 H. Ling，“Checkerpose：使用图神经网络进行物体姿态估计的渐进密集关键点定位”，发表于 ICCV，2023。'
- en: '[84] J. Chang and M. Kim, “Ghostpose: Multi-view pose estimation of transparent
    objects for robot hand grasping,” in IROS, 2021.'
  id: totrans-1293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] J. Chang 和 M. Kim，“Ghostpose：透明物体的多视角姿态估计用于机器人手抓取”，发表于 IROS，2021。'
- en: '[85] S. Guo and Y. Hu, “Knowledge distillation for 6d pose estimation by aligning
    distributions of local predictions,” in CVPR, 2023.'
  id: totrans-1294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. Guo 和 Y. Hu，“通过对齐局部预测的分布进行 6d 姿态估计的知识蒸馏”，发表于 CVPR，2023。'
- en: '[86] F. Liu and Y. Hu, “Linear-covariance loss for end-to-end learning of 6d
    pose estimation,” in ICCV, 2023.'
  id: totrans-1295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] F. Liu 和 Y. Hu，“用于端到端 6d 姿态估计的线性协方差损失”，发表于 ICCV，2023。'
- en: '[87] A. Crivellaro and M. Rad, “Robust 3d object tracking from monocular images
    using stable parts,” IEEE TPAMI, 2017.'
  id: totrans-1296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] A. Crivellaro 和 M. Rad，“使用稳定部件从单目图像中进行鲁棒的 3d 物体跟踪”，IEEE TPAMI，2017。'
- en: '[88] M. Oberweger and M. Rad, “Making deep heatmaps robust to partial occlusions
    for 3d object pose estimation,” in ECCV, 2018.'
  id: totrans-1297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] M. Oberweger 和 M. Rad，“使深度热图对部分遮挡具有鲁棒性以进行 3d 物体姿态估计”，发表于 ECCV，2018。'
- en: '[89] Y. Hu and J. Hugonot, “Segmentation-driven 6d object pose estimation,”
    in CVPR, 2019.'
  id: totrans-1298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Y. Hu 和 J. Hugonot，“基于分割的 6d 物体姿态估计”，发表于 CVPR，2019。'
- en: '[90] W.-L. Huang and C.-Y. Hung, “Confidence-based 6d object pose estimation,”
    IEEE TMM, 2021.'
  id: totrans-1299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] W.-L. Huang 和 C.-Y. Hung，“基于置信度的 6d 物体姿态估计”，IEEE TMM，2021。'
- en: '[91] W. Zhao and S. Zhang, “Learning deep network for detecting 3d object keypoints
    and 6d poses,” in CVPR, 2020.'
  id: totrans-1300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] W. Zhao 和 S. Zhang，“学习深度网络以检测 3d 物体关键点和 6d 姿态”，发表于 CVPR，2020。'
- en: '[92] Z. Yang and X. Yu, “Dsc-posenet: Learning 6dof object pose estimation
    via dual-scale consistency,” in CVPR, 2021.'
  id: totrans-1301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Z. Yang 和 X. Yu，“Dsc-posenet：通过双尺度一致性学习 6dof 物体姿态估计”，发表于 CVPR，2021。'
- en: '[93] S. Liu and H. Jiang, “Semi-supervised 3d hand-object poses estimation
    with interactions in time,” in CVPR, 2021.'
  id: totrans-1302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] S. Liu 和 H. Jiang，“具有时间交互的半监督 3d 手物体姿态估计”，发表于 CVPR，2021。'
- en: '[94] G. Georgakis and S. Karanam, “Learning local rgb-to-cad correspondences
    for object pose estimation,” in ICCV, 2019.'
  id: totrans-1303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] G. Georgakis 和 S. Karanam，“学习局部 RGB 到 CAD 对应关系以进行物体姿态估计”，发表于 ICCV，2019。'
- en: '[95] J. Sock and G. Garcia-Hernando, “Introducing pose consistency and warp-alignment
    for self-supervised 6d object pose estimation in color images,” in 3DV, 2020.'
  id: totrans-1304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] J. Sock 和 G. Garcia-Hernando，“引入姿态一致性和变形对齐以进行自监督的 6d 物体姿态估计”，发表于 3DV，2020。'
- en: '[96] S. Zhang and W. Zhao, “Keypoint-graph-driven learning framework for object
    pose estimation,” in CVPR, 2021.'
  id: totrans-1305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] S. Zhang 和 W. Zhao，“用于物体姿态估计的关键点图驱动学习框架”，发表于 CVPR，2021。'
- en: '[97] S. Thalhammer and M. Leitner, “Pyrapose: Feature pyramids for fast and
    accurate object pose estimation under domain shift,” in ICRA, 2021.'
  id: totrans-1306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] S. Thalhammer 和 M. Leitner，“Pyrapose：用于在领域迁移下快速准确的物体姿态估计的特征金字塔”，发表于 ICRA，2021。'
- en: '[98] T. Hodan and D. Barath, “Epos: Estimating 6d pose of objects with symmetries,”
    in CVPR, 2020.'
  id: totrans-1307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] T. Hodan 和 D. Barath，“Epos：具有对称性的物体 6d 姿态估计”，发表于 CVPR，2020。'
- en: '[99] I. Shugurov and S. Zakharov, “Dpodv2: Dense correspondence-based 6 dof
    pose estimation,” IEEE TPAMI, 2021.'
  id: totrans-1308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] I. Shugurov 和 S. Zakharov，“Dpodv2：基于密集对应的 6 自由度姿态估计，” 发表在 IEEE TPAMI，2021。'
- en: '[100] H. Chen and P. Wang, “Epro-pnp: Generalized end-to-end probabilistic
    perspective-n-points for monocular object pose estimation,” in CVPR, 2022.'
  id: totrans-1309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] H. Chen 和 P. Wang，“Epro-pnp：一种广义的端到端概率透视-n-点用于单目物体姿态估计，” 发表在 CVPR，2022。'
- en: '[101] R. L. Haugaard and A. G. Buch, “Surfemb: Dense and continuous correspondence
    distributions for object pose estimation with learnt surface embeddings,” in CVPR,
    2022.'
  id: totrans-1310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] R. L. Haugaard 和 A. G. Buch，“Surfemb：密集且连续的对应分布用于具有学习表面嵌入的物体姿态估计，” 发表在
    CVPR，2022。'
- en: '[102] F. Li and S. R. Vutukur, “Nerf-pose: A first-reconstruct-then-regress
    approach for weakly-supervised 6d object pose estimation,” in ICCV, 2023.'
  id: totrans-1311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] F. Li 和 S. R. Vutukur，“Nerf-pose：一种先重建再回归的方法用于弱监督的 6D 物体姿态估计，” 发表在 ICCV，2023。'
- en: '[103] Y. Xu and K.-Y. Lin, “Rnnpose: 6-dof object pose estimation via recurrent
    correspondence field estimation and pose optimization,” IEEE TPAMI, 2024.'
  id: totrans-1312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Y. Xu 和 K.-Y. Lin，“Rnnpose：通过递归对应场估计和姿态优化进行 6 自由度物体姿态估计，” 发表在 IEEE TPAMI，2024。'
- en: '[104] M. Sundermeyer and Z.-C. Marton, “Implicit 3d orientation learning for
    6d object detection from rgb images,” in ECCV, 2018.'
  id: totrans-1313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] M. Sundermeyer 和 Z.-C. Marton，“隐式 3D 方向学习用于从 RGB 图像中进行 6D 物体检测，” 发表在
    ECCV，2018。'
- en: '[105] C. Papaioannidis and V. Mygdalis, “Domain-translated 3d object pose estimation,”
    IEEE TIP, 2020.'
  id: totrans-1314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] C. Papaioannidis 和 V. Mygdalis，“领域转化的 3D 物体姿态估计，” 发表在 IEEE TIP，2020。'
- en: '[106] Z. Li and X. Ji, “Pose-guided auto-encoder and feature-based refinement
    for 6-dof object pose regression,” in ICRA, 2020.'
  id: totrans-1315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Z. Li 和 X. Ji，“基于姿态引导的自编码器和特征基础的 6 自由度物体姿态回归的精炼，” 发表在 ICRA，2020。'
- en: '[107] X. Deng and A. Mousavian, “Poserbpf: A rao–blackwellized particle filter
    for 6-d object pose tracking,” IEEE TRO, 2021.'
  id: totrans-1316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] X. Deng 和 A. Mousavian，“Poserbpf：一种 Rao–Blackwell 化的粒子滤波器用于 6D 物体姿态跟踪，”
    发表在 IEEE TRO，2021。'
- en: '[108] H. Jiang and M. Salzmann, “Se(3) diffusion model-based point cloud registration
    for robust 6d object pose estimation,” in NeurIPS, 2023.'
  id: totrans-1317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] H. Jiang 和 M. Salzmann，“基于 SE(3) 扩散模型的点云配准用于鲁棒的 6D 物体姿态估计，” 发表在 NeurIPS，2023。'
- en: '[109] Z. Dang and L. Wang, “Match normalization: Learning-based point cloud
    registration for 6d object pose estimation in the real world,” IEEE TPAMI, 2024.'
  id: totrans-1318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Z. Dang 和 L. Wang，“匹配归一化：基于学习的点云配准用于现实世界中的 6D 物体姿态估计，” 发表在 IEEE TPAMI，2024。'
- en: '[110] T. Cao and F. Luo, “Dgecn: A depth-guided edge convolutional network
    for end-to-end 6d pose estimation,” in CVPR, 2022.'
  id: totrans-1319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] T. Cao 和 F. Luo，“Dgecn：一种深度引导的边缘卷积网络，用于端到端 6D 姿态估计，” 发表在 CVPR，2022。'
- en: '[111] Y. Wu and M. Zand, “Vote from the center: 6 dof pose estimation in rgb-d
    images by radial keypoint voting,” in ECCV, 2022.'
  id: totrans-1320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Y. Wu 和 M. Zand，“来自中心的投票：通过径向关键点投票进行 RGB-D 图像中的 6 自由度姿态估计，” 发表在 ECCV，2022。'
- en: '[112] J. Zhou and K. Chen, “Deep fusion transformer network with weighted vector-wise
    keypoints voting for robust 6d object pose estimation,” in ICCV, 2023.'
  id: totrans-1321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] J. Zhou 和 K. Chen，“深度融合变换器网络与加权向量级关键点投票用于鲁棒的 6D 物体姿态估计，” 发表在 ICCV，2023。'
- en: '[113] M. Tian and L. Pan, “Robust 6d object pose estimation by learning rgb-d
    features,” in ICRA, 2020.'
  id: totrans-1322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] M. Tian 和 L. Pan，“通过学习 RGB-D 特征进行鲁棒的 6D 物体姿态估计，” 发表在 ICRA，2020。'
- en: '[114] G. Zhou and H. Wang, “Pr-gcn: A deep graph convolutional network with
    point refinement for 6d pose estimation,” in ICCV, 2021.'
  id: totrans-1323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] G. Zhou 和 H. Wang，“Pr-gcn：一种深度图卷积网络与点精炼用于 6D 姿态估计，” 发表在 ICCV，2021。'
- en: '[115] N. Mo and W. Gan, “Es6d: A computation efficient and symmetry-aware 6d
    pose regression framework,” in CVPR, 2022.'
  id: totrans-1324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] N. Mo 和 W. Gan，“Es6d：一种计算高效且具备对称性意识的 6D 姿态回归框架，” 发表在 CVPR，2022。'
- en: '[116] J.-X. Hong and H.-B. Zhang, “A transformer-based multi-modal fusion network
    for 6d pose estimation,” Information Fusion, 2024.'
  id: totrans-1325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] J.-X. Hong 和 H.-B. Zhang，“基于变换器的多模态融合网络用于 6D 姿态估计，” 发表在 Information Fusion，2024。'
- en: '[117] W. Chen and X. Jia, “G2l-net: Global to local network for real-time 6d
    pose estimation with embedding vector features,” in CVPR, 2020.'
  id: totrans-1326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] W. Chen 和 X. Jia，“G2l-net：用于实时 6D 姿态估计的全局到局部网络及嵌入向量特征，” 发表在 CVPR，2020。'
- en: '[118] Y. Hu and P. Fua, “Single-stage 6d object pose estimation,” in CVPR,
    2020.'
  id: totrans-1327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Y. Hu 和 P. Fua，“单阶段 6D 物体姿态估计，” 发表在 CVPR，2020。'
- en: '[119] Y. Labbé and J. Carpentier, “Cosypose: Consistent multi-view multi-object
    6d pose estimation,” in ECCV, 2020.'
  id: totrans-1328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Y. Labbé 和 J. Carpentier，“Cosypose：一致的多视角多物体 6D 姿态估计，” 发表在 ECCV，2020。'
- en: '[120] G. Wang and F. Manhardt, “Gdr-net: Geometry-guided direct regression
    network for monocular 6d object pose estimation,” in CVPR, 2021.'
  id: totrans-1329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] G. Wang 和 F. Manhardt，“Gdr-net：几何引导的直接回归网络用于单目 6D 物体姿态估计，” 发表在 CVPR，2021。'
- en: '[121] Y. Di and F. Manhardt, “So-pose: Exploiting self-occlusion for direct
    6d pose estimation,” in ICCV, 2021.'
  id: totrans-1330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Y. Di 和 F. Manhardt, “So-pose：利用自遮挡进行直接 6d 姿态估计，” ICCV，2021。'
- en: '[122] G. Wang and F. Manhardt, “Occlusion-aware self-supervised monocular 6d
    object pose estimation,” IEEE TPAMI, 2021.'
  id: totrans-1331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] G. Wang 和 F. Manhardt, “考虑遮挡的自监督单目 6d 物体姿态估计，” IEEE TPAMI，2021。'
- en: '[123] C. Li and J. Bai, “A unified framework for multi-view multi-class object
    pose estimation,” in ECCV, 2018.'
  id: totrans-1332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] C. Li 和 J. Bai, “用于多视角多类别物体姿态估计的统一框架，” ECCV，2018。'
- en: '[124] Y. Li and G. Wang, “Deepim: Deep iterative matching for 6d pose estimation,”
    in ECCV, 2018.'
  id: totrans-1333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Y. Li 和 G. Wang, “Deepim：用于 6d 姿态估计的深度迭代匹配，” ECCV，2018。'
- en: '[125] F. Manhardt and W. Kehl, “Deep model-based 6d pose refinement in rgb,”
    in ECCV, 2018.'
  id: totrans-1334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] F. Manhardt 和 W. Kehl, “基于深度模型的 6d 姿态优化，” ECCV，2018。'
- en: '[126] F. Manhardt and D. M. Arroyo, “Explaining the ambiguity of object detection
    and 6d pose from visual data,” in ICCV, 2019.'
  id: totrans-1335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] F. Manhardt 和 D. M. Arroyo, “解释视觉数据中物体检测和 6d 姿态的歧义性，” ICCV，2019。'
- en: '[127] C. Papaioannidis and I. Pitas, “3d object pose estimation using multi-objective
    quaternion learning,” IEEE TCSVT, 2019.'
  id: totrans-1336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] C. Papaioannidis 和 I. Pitas, “使用多目标四元数学习进行 3d 物体姿态估计，” IEEE TCSVT，2019。'
- en: '[128] Y. Liu and L. Zhou, “Regression-based three-dimensional pose estimation
    for texture-less objects,” IEEE TMM, 2019.'
  id: totrans-1337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Y. Liu 和 L. Zhou, “基于回归的三维姿态估计用于无纹理物体，” IEEE TMM，2019。'
- en: '[129] Y. Hai and R. Song, “Shape-constraint recurrent flow for 6d object pose
    estimation,” in CVPR, 2023.'
  id: totrans-1338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Y. Hai 和 R. Song, “基于形状约束的递归流用于 6d 物体姿态估计，” CVPR，2023。'
- en: '[130] Y. Li and Y. Mao, “Mrc-net: 6-dof pose estimation with multiscale residual
    correlation,” in CVPR, 2024.'
  id: totrans-1339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Y. Li 和 Y. Mao, “Mrc-net：具有多尺度残差相关的 6-dof 姿态估计，” CVPR，2024。'
- en: '[131] M. Cai and I. Reid, “Reconstruct locally, localize globally: A model
    free method for object pose estimation,” in CVPR, 2020.'
  id: totrans-1340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] M. Cai 和 I. Reid, “局部重建，全球定位：一种无模型的方法用于物体姿态估计，” CVPR，2020。'
- en: '[132] D. Wang and G. Zhou, “Geopose: Dense reconstruction guided 6d object
    pose estimation with geometric consistency,” IEEE TMM, 2021.'
  id: totrans-1341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] D. Wang 和 G. Zhou, “Geopose：密集重建引导的 6d 物体姿态估计与几何一致性，” IEEE TMM，2021。'
- en: '[133] Y. Su and M. Saleh, “Zebrapose: Coarse to fine surface encoding for 6dof
    object pose estimation,” in CVPR, 2022.'
  id: totrans-1342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Y. Su 和 M. Saleh, “Zebrapose：从粗到细的表面编码用于 6dof 物体姿态估计，” CVPR，2022。'
- en: '[134] Z. Xu and Y. Zhang, “Bico-net: Regress globally, match locally for robust
    6d pose estimation,” in IJCAI, 2022.'
  id: totrans-1343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Z. Xu 和 Y. Zhang, “Bico-net：全局回归，局部匹配以获得鲁棒的 6d 姿态估计，” IJCAI，2022。'
- en: '[135] L. Huang and T. Hodan, “Neural correspondence field for object pose estimation,”
    in ECCV, 2022.'
  id: totrans-1344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] L. Huang 和 T. Hodan, “用于物体姿态估计的神经对应场，” ECCV，2022。'
- en: '[136] H. Jiang and Z. Dang, “Center-based decoupled point cloud registration
    for 6d object pose estimation,” in ICCV, 2023.'
  id: totrans-1345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] H. Jiang 和 Z. Dang, “基于中心的解耦点云配准用于 6d 物体姿态估计，” ICCV，2023。'
- en: '[137] P. Besl and N. D. McKay, “A method for registration of 3-d shapes,” IEEE
    TPAMI, 1992.'
  id: totrans-1346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] P. Besl 和 N. D. McKay, “一种 3d 形状配准的方法，” IEEE TPAMI，1992。'
- en: '[138] Y. Lin and Y. Su, “Hipose: Hierarchical binary surface encoding and correspondence
    pruning for rgb-d 6dof object pose estimation,” in CVPR, 2024.'
  id: totrans-1347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Y. Lin 和 Y. Su, “Hipose：用于 rgb-d 6dof 物体姿态估计的分层二进制表面编码和对应修剪，” CVPR，2024。'
- en: '[139] K. Park and T. Patten, “Pix2pose: Pixel-wise coordinate regression of
    objects for 6d pose estimation,” in ICCV, 2019.'
  id: totrans-1348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] K. Park 和 T. Patten, “Pix2pose：用于 6d 姿态估计的像素级坐标回归，” ICCV，2019。'
- en: '[140] C. Wu and L. Chen, “Geometric-aware dense matching network for 6d pose
    estimation of objects from rgb-d images,” PR, 2023.'
  id: totrans-1349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] C. Wu 和 L. Chen, “几何感知的密集匹配网络用于从 rgb-d 图像中进行 6d 姿态估计，” PR，2023。'
- en: '[141] C. Wu and L. Chen, “Pseudo-siamese graph matching network for textureless
    objects’ 6-d pose estimation,” IEEE TIE, 2021.'
  id: totrans-1350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] C. Wu 和 L. Chen, “伪孪生图匹配网络用于无纹理物体的 6-d 姿态估计，” IEEE TIE，2021。'
- en: '[142] Z. Li and Y. Hu, “Sd-pose: Semantic decomposition for cross-domain 6d
    object pose estimation,” in AAAI, 2021.'
  id: totrans-1351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Z. Li 和 Y. Hu, “Sd-pose：用于跨领域 6d 物体姿态估计的语义分解，” AAAI，2021。'
- en: '[143] Y. Hu and P. Fua, “Perspective flow aggregation for data-limited 6d object
    pose estimation,” in ECCV, 2022.'
  id: totrans-1352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Y. Hu 和 P. Fua, “用于数据有限的 6d 物体姿态估计的透视流聚合，” ECCV，2022。'
- en: '[144] Y. Hai and R. Song, “Pseudo flow consistency for self-supervised 6d object
    pose estimation,” in ICCV, 2023.'
  id: totrans-1353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Y. Hai 和 R. Song, “伪流一致性用于自监督 6d 物体姿态估计，” ICCV，2023。'
- en: '[145] L. Lipson and Z. Teed, “Coupled iterative refinement for 6d multi-object
    pose estimation,” in CVPR, 2022.'
  id: totrans-1354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] L. Lipson 和 Z. Teed, “用于 6d 多物体姿态估计的耦合迭代优化，” CVPR，2022。'
- en: '[146] J. J. Moré, “The levenberg-marquardt algorithm: Implementation and theory,”
    in Numerical Analysis, 1978.'
  id: totrans-1355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] J. J. Moré，“Levenberg-Marquardt 算法：实现与理论，” 见 Numerical Analysis, 1978。'
- en: '[147] X. Liu and J. Zhang, “6dof pose estimation with object cutout based on
    a deep autoencoder,” in ISMAR-Adjunct, 2019.'
  id: totrans-1356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] X. Liu 和 J. Zhang，“基于深度自编码器的物体剪切 6DOF 姿态估计，” 见 ISMAR-Adjunct, 2019。'
- en: '[148] Y. Zhang and C. Zhang, “6d object pose estimation algorithm using preprocessing
    of segmentation and keypoint extraction,” in I2MTC, 2020.'
  id: totrans-1357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Y. Zhang 和 C. Zhang，“使用分割和关键点提取预处理的 6D 物体姿态估计算法，” 见 I2MTC, 2020。'
- en: '[149] S. Stevšič and O. Hilliges, “Spatial attention improves iterative 6d
    object pose estimation,” in 3DV, 2020.'
  id: totrans-1358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] S. Stevšič 和 O. Hilliges，“空间注意力提升迭代 6D 物体姿态估计，” 见 3DV, 2020。'
- en: '[150] K. Murphy and S. Russell, Sequential Monte Carlo Methods in Practice,
    ch. Rao-blackwellised particle filtering for dynamic bayesian networks. Springer,
    2001.'
  id: totrans-1359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] K. Murphy 和 S. Russell，*Sequential Monte Carlo Methods in Practice*,
    ch. Rao-blackwellised particle filtering for dynamic bayesian networks. Springer,
    2001。'
- en: '[151] X. Liu and S. Iwase, “Kdfnet: Learning keypoint distance field for 6d
    object pose estimation,” in IROS, 2021.'
  id: totrans-1360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] X. Liu 和 S. Iwase，“Kdfnet: 学习关键点距离场以进行 6D 物体姿态估计，” 见 IROS, 2021。'
- en: '[152] P. Liu and Q. Zhang, “Bdr6d: Bidirectional deep residual fusion network
    for 6d pose estimation,” IEEE TASE, 2023.'
  id: totrans-1361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] P. Liu 和 Q. Zhang，“Bdr6d: 双向深度残差融合网络用于 6D 姿态估计，” IEEE TASE, 2023。'
- en: '[153] L. Xu and H. Qu, “6d-diff: A keypoint diffusion framework for 6d object
    pose estimation,” in CVPR, 2024.'
  id: totrans-1362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] L. Xu 和 H. Qu，“6D-diff: 一个关键点扩散框架用于 6D 物体姿态估计，” 见 CVPR, 2024。'
- en: '[154] J. Mei and X. Jiang, “Spatial feature mapping for 6dof object pose estimation,”
    PR, 2022.'
  id: totrans-1363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] J. Mei 和 X. Jiang，“用于 6DOF 物体姿态估计的空间特征映射，” PR, 2022。'
- en: '[155] F. Wang and X. Zhang, “Kvnet: An iterative 3d keypoints voting network
    for real-time 6-dof object pose estimation,” Neurocomputing, 2023.'
  id: totrans-1364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] F. Wang 和 X. Zhang，“Kvnet: 用于实时 6DOF 物体姿态估计的迭代 3D 关键点投票网络，” Neurocomputing,
    2023。'
- en: '[156] L. Zeng and W. J. Lv, “Parametricnet: 6dof pose estimation network for
    parametric shapes in stacked scenarios,” in ICRA, 2021.'
  id: totrans-1365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] L. Zeng 和 W. J. Lv，“Parametricnet: 堆叠场景中参数形状的 6DOF 姿态估计网络，” 见 ICRA, 2021。'
- en: '[157] F. Duffhauss and T. Demmler, “Mv6d: Multi-view 6d pose estimation on
    rgb-d frames using a deep point-wise voting network,” in IROS, 2022.'
  id: totrans-1366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] F. Duffhauss 和 T. Demmler，“Mv6d: 使用深度逐点投票网络进行 RGB-D 帧上的多视角 6D 姿态估计，”
    见 IROS, 2022。'
- en: '[158] X. Yu and Z. Zhuang, “6dof object pose estimation via differentiable
    proxy voting loss,” arXiv preprint arXiv:2002.03923, 2020.'
  id: totrans-1367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] X. Yu 和 Z. Zhuang，“通过可微代理投票损失进行 6DOF 物体姿态估计，” arXiv 预印本 arXiv:2002.03923,
    2020。'
- en: '[159] H. Lin and S. Peng, “Learning to estimate object poses without real image
    annotations.,” in IJCAI, 2022.'
  id: totrans-1368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] H. Lin 和 S. Peng，“学习在没有真实图像标注的情况下估计物体姿态，” 见 IJCAI, 2022。'
- en: '[160] T. Ikeda and S. Tanishige, “Sim2real instance-level style transfer for
    6d pose estimation,” in IROS, 2022.'
  id: totrans-1369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] T. Ikeda 和 S. Tanishige，“Sim2real 实例级风格迁移用于 6D 姿态估计，” 见 IROS, 2022。'
- en: '[161] G. Zhou and Y. Yan, “A novel depth and color feature fusion framework
    for 6d object pose estimation,” IEEE TMM, 2020.'
  id: totrans-1370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] G. Zhou 和 Y. Yan，“一种新颖的深度与颜色特征融合框架用于 6D 物体姿态估计，” IEEE TMM, 2020。'
- en: '[162] Y. LeCun and B. Boser, “Backpropagation applied to handwritten zip code
    recognition,” Neural Comput, 1989.'
  id: totrans-1371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Y. LeCun 和 B. Boser，“反向传播应用于手写邮政编码识别，” Neural Comput, 1989。'
- en: '[163] X. Liu and X. Yuan, “A depth adaptive feature extraction and dense prediction
    network for 6-d pose estimation in robotic grasping,” IEEE TII, 2023.'
  id: totrans-1372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] X. Liu 和 X. Yuan，“用于机器人抓取的 6D 姿态估计的深度自适应特征提取和密集预测网络，” IEEE TII, 2023。'
- en: '[164] F. Mu and R. Huang, “Temporalfusion: Temporal motion reasoning with multi-frame
    fusion for 6d object pose estimation,” in IROS, 2021.'
  id: totrans-1373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] F. Mu 和 R. Huang，“Temporalfusion: 通过多帧融合进行 6D 物体姿态估计的时间运动推理，” 见 IROS,
    2021。'
- en: '[165] D. Cai and J. Heikkilä, “Sc6d: Symmetry-agnostic and correspondence-free
    6d object pose estimation,” in 3DV, 2022.'
  id: totrans-1374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] D. Cai 和 J. Heikkilä，“Sc6d: 对称无关且无对应的 6D 物体姿态估计，” 见 3DV, 2022。'
- en: '[166] L. Zeng and W. J. Lv, “Ppr-net++: Accurate 6-d pose estimation in stacked
    scenarios,” IEEE TASE, 2021.'
  id: totrans-1375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] L. Zeng 和 W. J. Lv，“Ppr-net++: 在堆叠场景中的精确 6D 姿态估计，” IEEE TASE, 2021。'
- en: '[167] Y. Bukschat and M. Vetter, “Efficientpose: An efficient, accurate and
    scalable end-to-end 6d multi object pose estimation approach,” arXiv preprint
    arXiv:2011.04307, 2020.'
  id: totrans-1376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Y. Bukschat 和 M. Vetter，“Efficientpose: 一种高效、准确且可扩展的端到端 6D 多物体姿态估计方法，”
    arXiv 预印本 arXiv:2011.04307, 2020。'
- en: '[168] G. Gao and M. Lauri, “6d object pose regression via supervised learning
    on point clouds,” in ICRA, 2020.'
  id: totrans-1377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] G. Gao 和 M. Lauri，“通过点云上的监督学习进行 6D 物体姿态回归，” 见 ICRA, 2020。'
- en: '[169] M. Lin and V. Murali, “6d object pose estimation with pairwise compatible
    geometric features,” in ICRA, 2021.'
  id: totrans-1378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] M. Lin 和 V. Murali，“基于成对兼容几何特征的6d对象姿态估计”，发表于ICRA，2021。'
- en: '[170] Y. Shi and J. Huang, “Stablepose: Learning 6d object poses from geometrically
    stable patches,” in CVPR, 2021.'
  id: totrans-1379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Y. Shi 和 J. Huang，“Stablepose：从几何稳定的补丁中学习6d对象姿态”，发表于CVPR，2021。'
- en: '[171] Z. Liu and Q. Wang, “Pa-pose: Partial point cloud fusion based on reliable
    alignment for 6d pose tracking,” PR, 2024.'
  id: totrans-1380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Z. Liu 和 Q. Wang，“Pa-pose：基于可靠对齐的部分点云融合用于6d姿态跟踪”，发表于PR，2024。'
- en: '[172] Y. Wen and Y. Fang, “Gccn: Geometric constraint co-attention network
    for 6d object pose estimation,” in ACM MM, 2021.'
  id: totrans-1381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Y. Wen 和 Y. Fang，“Gccn：用于6d对象姿态估计的几何约束共同注意网络”，发表于ACM MM，2021。'
- en: '[173] Y. An and D. Yang, “Hft6d: Multimodal 6d object pose estimation based
    on hierarchical feature transformer,” Measurement, 2024.'
  id: totrans-1382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Y. An 和 D. Yang，“Hft6d：基于层次特征变换器的多模态6d对象姿态估计”，发表于Measurement，2024。'
- en: '[174] Z. Zhang and W. Chen, “Trans6d: Transformer-based 6d object pose estimation
    and refinement,” in ECCVW, 2022.'
  id: totrans-1383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Z. Zhang 和 W. Chen，“Trans6d：基于变换器的6d对象姿态估计和精细调整”，发表于ECCVW，2022。'
- en: '[175] G. Feng and T.-B. Xu, “Nvr-net: Normal vector guided regression network
    for disentangled 6d pose estimation,” IEEE TCSVT, 2023.'
  id: totrans-1384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] G. Feng 和 T.-B. Xu，“Nvr-net：用于解耦6d姿态估计的法向量引导回归网络”，发表于IEEE TCSVT，2023。'
- en: '[176] G. Gao and M. Lauri, “Cloudaae: Learning 6d object pose regression with
    on-line data synthesis on point clouds,” in ICRA, 2021.'
  id: totrans-1385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] G. Gao 和 M. Lauri，“Cloudaae：通过点云上的在线数据合成进行6d对象姿态回归”，发表于ICRA，2021。'
- en: '[177] G. Zhou and D. Wang, “Semi-supervised 6d object pose estimation without
    using real annotations,” IEEE TCSVT, 2021.'
  id: totrans-1386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] G. Zhou 和 D. Wang，“半监督6d对象姿态估计，无需使用真实标注”，发表于IEEE TCSVT，2021。'
- en: '[178] T. Tan and Q. Dong, “Smoc-net: Leveraging camera pose for self-supervised
    monocular object pose estimation,” in CVPR, 2023.'
  id: totrans-1387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] T. Tan 和 Q. Dong，“Smoc-net：利用相机姿态进行自监督单目对象姿态估计”，发表于CVPR，2023。'
- en: '[179] J. Rambach and C. Deng, “Learning 6dof object poses from synthetic single
    channel images,” in ISMAR-Adjunct, 2018.'
  id: totrans-1388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] J. Rambach 和 C. Deng，“从合成单通道图像中学习6dof对象姿态”，发表于ISMAR-Adjunct，2018。'
- en: '[180] K. Kleeberger and M. F. Huber, “Single shot 6d object pose estimation,”
    in ICRA, 2020.'
  id: totrans-1389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] K. Kleeberger 和 M. F. Huber，“单次拍摄6d对象姿态估计”，发表于ICRA，2020。'
- en: '[181] V. Sarode and X. Li, “Pcrnet: Point cloud registration network using
    pointnet encoding,” arXiv preprint arXiv:1908.07906, 2019.'
  id: totrans-1390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] V. Sarode 和 X. Li，“Pcrnet：使用pointnet编码的点云配准网络”，arXiv预印本 arXiv:1908.07906，2019。'
- en: '[182] C. R. Qi and H. Su, “Pointnet: Deep learning on point sets for 3d classification
    and segmentation,” in CVPR, 2017.'
  id: totrans-1391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] C. R. Qi 和 H. Su，“Pointnet：用于3d分类和分割的点集深度学习”，发表于CVPR，2017。'
- en: '[183] S. H. Bengtson and H. Åström, “Pose estimation from rgb images of highly
    symmetric objects using a novel multi-pose loss and differential rendering,” in
    IROS, 2021.'
  id: totrans-1392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] S. H. Bengtson 和 H. Åström，“使用新型多姿态损失和差分渲染从rgb图像中估计高对称对象的姿态”，发表于IROS，2021。'
- en: '[184] J. Park and N. Cho, “Dprost: Dynamic projective spatial transformer network
    for 6d pose estimation,” in ECCV, 2022.'
  id: totrans-1393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] J. Park 和 N. Cho，“Dprost：用于6d姿态估计的动态投影空间变换网络”，发表于ECCV，2022。'
- en: '[185] M. Garon and J.-F. Lalonde, “Deep 6-dof tracking,” IEEE TVCG, 2017.'
  id: totrans-1394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] M. Garon 和 J.-F. Lalonde，“深度6-dof跟踪”，发表于IEEE TVCG，2017。'
- en: '[186] J. Long and E. Shelhamer, “Fully convolutional networks for semantic
    segmentation,” in CVPR, 2015.'
  id: totrans-1395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] J. Long 和 E. Shelhamer，“用于语义分割的全卷积网络”，发表于CVPR，2015。'
- en: '[187] W. Kehl and F. Manhardt, “Ssd-6d: Making rgb-based 3d detection and 6d
    pose estimation great again,” in ICCV, 2017.'
  id: totrans-1396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] W. Kehl 和 F. Manhardt，“Ssd-6d：让基于rgb的3d检测和6d姿态估计重获新生”，发表于ICCV，2017。'
- en: '[188] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “Ssd: Single shot multibox detector,” in ECCV, 2016.'
  id: totrans-1397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] W. Liu，D. Anguelov，D. Erhan，C. Szegedy，S. Reed，C.-Y. Fu 和 A. C. Berg，“Ssd：单次拍摄多框检测器”，发表于ECCV，2016。'
- en: '[189] J. Wu and B. Zhou, “Real-time object pose estimation with pose interpreter
    networks,” in IROS, 2018.'
  id: totrans-1398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] J. Wu 和 B. Zhou，“基于姿态解释网络的实时对象姿态估计”，发表于IROS，2018。'
- en: '[190] T.-T. Do and M. Cai, “Deep-6dpose: Recovering 6d object pose from a single
    rgb image,” arXiv preprint arXiv:1802.10367, 2018.'
  id: totrans-1399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] T.-T. Do 和 M. Cai，“Deep-6dpose：从单个rgb图像中恢复6d对象姿态”，arXiv预印本 arXiv:1802.10367，2018。'
- en: '[191] T.-C. Hsiao and H.-W. Chen, “Confronting ambiguity in 6d object pose
    estimation via score-based diffusion on se(3),” in CVPR, 2024.'
  id: totrans-1400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] T.-C. Hsiao 和 H.-W. Chen，“通过在se(3)上的基于分数的扩散解决6d对象姿态估计中的模糊问题”，发表于CVPR，2024。'
- en: '[192] J. Liu and W. Sun, “Hff6d: Hierarchical feature fusion network for robust
    6d object pose tracking,” IEEE TCSVT, 2022.'
  id: totrans-1401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] J. Liu 和 W. Sun，“Hff6d：用于鲁棒6d对象姿态跟踪的层次特征融合网络”，发表于IEEE TCSVT，2022。'
- en: '[193] R. Ge and G. Loianno, “Vipose: Real-time visual-inertial 6d object pose
    tracking,” in IROS, 2021.'
  id: totrans-1402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] R. Ge 和 G. Loianno，“Vipose: 实时视觉-惯性6d物体姿态跟踪，”发表于 IROS，2021年。'
- en: '[194] S. Iwase and X. Liu, “Repose: Fast 6d object pose refinement via deep
    texture rendering,” in ICCV, 2021.'
  id: totrans-1403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] S. Iwase 和 X. Liu，“Repose: 通过深度纹理渲染快速6d物体姿态细化，”发表于 ICCV，2021年。'
- en: '[195] J. Josifovski and M. Kerzel, “Object detection and pose estimation based
    on convolutional neural networks trained with synthetic data,” in IROS, 2018.'
  id: totrans-1404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] J. Josifovski 和 M. Kerzel，“基于卷积神经网络的物体检测和姿态估计，使用合成数据训练，”发表于 IROS，2018年。'
- en: '[196] C. Sahin and T.-K. Kim, “Category-level 6d object pose recovery in depth
    images,” in ECCVW, 2018.'
  id: totrans-1405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] C. Sahin 和 T.-K. Kim，“深度图像中的类别级6d物体姿态恢复，”发表于 ECCVW，2018年。'
- en: '[197] S. Umeyama, “Least-squares estimation of transformation parameters between
    two point patterns,” IEEE TPAMI, 1991.'
  id: totrans-1406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] S. Umeyama，“两个点模式之间的变换参数的最小二乘估计，”IEEE TPAMI，1991年。'
- en: '[198] J. Wang and K. Chen, “Category-level 6d object pose estimation via cascaded
    relation and recurrent reconstruction networks,” in IROS, 2021.'
  id: totrans-1407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] J. Wang 和 K. Chen，“通过级联关系和递归重建网络进行类别级6d物体姿态估计，”发表于 IROS，2021年。'
- en: '[199] L. Zou and Z. Huang, “6d-vit: Category-level 6d object pose estimation
    via transformer-based instance representation learning,” IEEE TIP, 2022.'
  id: totrans-1408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] L. Zou 和 Z. Huang，“6d-vit: 通过基于变换器的实例表示学习进行类别级6d物体姿态估计，”IEEE TIP，2022年。'
- en: '[200] Z. Fan and Z. Song, “Object level depth reconstruction for category level
    6d object pose estimation from monocular rgb image,” in ECCV, 2022.'
  id: totrans-1409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] Z. Fan 和 Z. Song，“从单目Rgb图像进行类别级6d物体姿态估计的物体级深度重建，”发表于 ECCV，2022年。'
- en: '[201] J. Wei and X. Song, “Rgb-based category-level object pose estimation
    via decoupled metric scale recovery,” arXiv preprint arXiv:2309.10255, 2023.'
  id: totrans-1410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] J. Wei 和 X. Song，“基于Rgb的类别级物体姿态估计通过解耦度量尺度恢复，”arXiv 预印本 arXiv:2309.10255，2023年。'
- en: '[202] M. Z. Irshad and T. Kollar, “Centersnap: Single-shot multi-object 3d
    shape reconstruction and categorical 6d pose and size estimation,” in ICRA, 2022.'
  id: totrans-1411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] M. Z. Irshad 和 T. Kollar，“Centersnap: 单次拍摄的多物体3d形状重建及类别级6d姿态和尺寸估计，”发表于
    ICRA，2022年。'
- en: '[203] H. Lin and Z. Liu, “Sar-net: Shape alignment and recovery network for
    category-level 6d object pose and size estimation,” in CVPR, 2022.'
  id: totrans-1412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] H. Lin 和 Z. Liu，“Sar-net: 类别级6d物体姿态和尺寸估计的形状对齐和恢复网络，”发表于 CVPR，2022年。'
- en: '[204] R. Zhang and Y. Di, “Ssp-pose: Symmetry-aware shape prior deformation
    for direct category-level object pose estimation,” in IROS, 2022.'
  id: totrans-1413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] R. Zhang 和 Y. Di，“Ssp-pose: 对称感知形状先验变形直接进行类别级物体姿态估计，”发表于 IROS，2022年。'
- en: '[205] R. Zhang and Y. Di, “Rbp-pose: Residual bounding box projection for category-level
    pose estimation,” in ECCV, 2022.'
  id: totrans-1414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] R. Zhang 和 Y. Di，“Rbp-pose: 类别级姿态估计的残差边界框投影，”发表于 ECCV，2022年。'
- en: '[206] X. Liu and G. Wang, “Catre: Iterative point clouds alignment for category-level
    object pose refinement,” in ECCV, 2022.'
  id: totrans-1415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] X. Liu 和 G. Wang，“Catre: 类别级物体姿态细化的迭代点云对齐，”发表于 ECCV，2022年。'
- en: '[207] J. Liu and W. Sun, “Mh6d: Multi-hypothesis consistency learning for category-level
    6-d object pose estimation,” IEEE TNNLS, 2024.'
  id: totrans-1416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] J. Liu 和 W. Sun，“Mh6d: 类别级6-d物体姿态估计的多假设一致性学习，”IEEE TNNLS，2024年。'
- en: '[208] X. Li and H. Wang, “Category-level articulated object pose estimation,”
    in CVPR, 2020.'
  id: totrans-1417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] X. Li 和 H. Wang，“类别级关节物体姿态估计，”发表于 CVPR，2020年。'
- en: '[209] W. Chen and X. Jia, “Fs-net: Fast shape-based network for category-level
    6d object pose estimation with decoupled rotation mechanism,” in CVPR, 2021.'
  id: totrans-1418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] W. Chen 和 X. Jia，“Fs-net: 具有解耦旋转机制的类别级6d物体姿态估计的快速形状基础网络，”发表于 CVPR，2021年。'
- en: '[210] Y. Weng and H. Wang, “Captra: Category-level pose tracking for rigid
    and articulated objects from point clouds,” in ICCV, 2021.'
  id: totrans-1419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] Y. Weng 和 H. Wang，“Captra: 从点云中进行刚性和关节物体的类别级姿态跟踪，”发表于 ICCV，2021年。'
- en: '[211] Y. You and R. Shi, “Cppf: Towards robust category-level 9d pose estimation
    in the wild,” in CVPR, 2022.'
  id: totrans-1420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] Y. You 和 R. Shi，“Cppf: 朝着野外鲁棒类别级9d姿态估计的方向前进，”发表于 CVPR，2022年。'
- en: '[212] J. Zhang and M. Wu, “Generative category-level object pose estimation
    via diffusion models,” in NeurIPS, 2023.'
  id: totrans-1421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] J. Zhang 和 M. Wu，“通过扩散模型生成的类别级物体姿态估计，”发表于 NeurIPS，2023年。'
- en: '[213] C. Wang and Martín-Martín, “6-pack: Category-level 6d pose tracker with
    anchor-based keypoints,” in ICRA, 2020.'
  id: totrans-1422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] C. Wang 和 Martín-Martín，“6-pack: 具有基于锚点的关键点的类别级6d姿态跟踪器，”发表于 ICRA，2020年。'
- en: '[214] J. Lin and Z. Wei, “Dualposenet: Category-level 6d object pose and size
    estimation using dual pose network with refined learning of pose consistency,”
    in ICCV, 2021.'
  id: totrans-1423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] J. Lin 和 Z. Wei，“Dualposenet: 使用双重姿态网络进行类别级6d物体姿态和尺寸估计，通过姿态一致性精细学习，”发表于
    ICCV，2021年。'
- en: '[215] B. Wen and K. Bekris, “Bundletrack: 6d pose tracking for novel objects
    without instance or category-level 3d models,” in IROS, 2021.'
  id: totrans-1424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] W. Peng and J. Yan, “Self-supervised category-level 6d object pose estimation
    with deep implicit shape representation,” in AAAI, 2022.'
  id: totrans-1425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] T. Lee and B.-U. Lee, “Uda-cope: Unsupervised domain adaptation for category-level
    object pose estimation,” in CVPR, 2022.'
  id: totrans-1426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] T. Lee and J. Tremblay, “Tta-cope: Test-time adaptation for category-level
    object pose estimation,” in CVPR, 2023.'
  id: totrans-1427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] J. Lin and Z. Wei, “Vi-net: Boosting category-level 6d object pose estimation
    via learning decoupled rotations on the spherical representations,” in ICCV, 2023.'
  id: totrans-1428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Y. Chen and Y. Di, “Secondpose: Se (3)-consistent dual-stream feature
    fusion for category-level pose estimation,” in CVPR, 2024.'
  id: totrans-1429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] T. Lee and B.-U. Lee, “Category-level metric scale object shape and pose
    estimation,” IEEE RAL, 2021.'
  id: totrans-1430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] X. Lin and M. Zhu, “Clipose: Category-level object pose estimation with
    pre-trained vision-language knowledge,” arXiv preprint arXiv:2402.15726, 2024.'
  id: totrans-1431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Z. Fan and Z. Song, “Acr-pose: Adversarial canonical representation reconstruction
    network for category level 6d object pose estimation,” arXiv preprint arXiv:2111.10524,
    2021.'
  id: totrans-1432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] T. Nie and J. Ma, “Category-level 6d pose estimation using geometry-guided
    instance-aware prior and multi-stage reconstruction,” IEEE RAL, 2023.'
  id: totrans-1433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] L. Zhou and Z. Liu, “Dr-pose: A two-stage deformation-and-registration
    pipeline for category-level 6d object pose estimation,” in IROS, 2023.'
  id: totrans-1434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] L. Zou and Z. Huang, “Gpt-cope: A graph-guided point transformer for
    category-level object pose estimation,” IEEE TCSVT, 2023.'
  id: totrans-1435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] G. Li and D. Zhu, “Sd-pose: Structural discrepancy aware category-level
    6d object pose estimation,” in WACV, 2023.'
  id: totrans-1436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] S. Yu and D.-H. Zhai, “Catformer: Category-level 6d object pose estimation
    with transformer,” in AAAI, 2024.'
  id: totrans-1437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] Y. He and H. Fan, “Towards self-supervised category-level object pose
    and size estimation,” arXiv preprint arXiv:2203.02884, 2022.'
  id: totrans-1438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] G. Li and Y. Li, “Generative category-level shape and pose estimation
    with semantic primitives,” in CoRL, 2023.'
  id: totrans-1439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] K. Chen and S. James, “Stereopose: Category-level 6d transparent object
    pose estimation from stereo images via back-view nocs,” in ICRA, 2023.'
  id: totrans-1440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] H. Wang and Z. Fan, “Dtf-net: Category-level pose estimation and shape
    reconstruction via deformable template field,” in ACM MM, 2023.'
  id: totrans-1441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] L. Zheng and T. H. E. Tse, “Georef: Geometric alignment across shape
    variation for category-level object pose refinement,” in CVPR, 2024.'
  id: totrans-1442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] K. Zhang and Y. Fu, “Self-supervised geometric correspondence for category-level
    6d object pose estimation in the wild,” in ICLR, 2023.'
  id: totrans-1443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] A. Remus and S. D’Avella, “I2c-net: Using instance-level neural networks
    for monocular category-level 6d pose estimation,” IEEE RAL, 2023.'
  id: totrans-1444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] J. Liu and Z. Cao, “Category-level 6d object pose estimation with structure
    encoder and reasoning attention,” IEEE TCSVT, 2022.'
  id: totrans-1445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] J. Liu 和 Z. Cao, “类别级 6d 物体姿态估计与结构编码器和推理注意力,” IEEE TCSVT, 2022。'
- en: '[237] X. Deng and J. Geng, “icaps: Iterative category-level object pose and
    shape estimation,” IEEE RAL, 2022.'
  id: totrans-1446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] X. Deng 和 J. Geng, “icaps: 迭代类别级物体姿态和形状估计,” IEEE RAL, 2022。'
- en: '[238] R. Wang and X. Wang, “Query6dof: Learning sparse queries as implicit
    shape prior for category-level 6dof pose estimation,” in ICCV, 2023.'
  id: totrans-1447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] R. Wang 和 X. Wang, “Query6dof: 学习稀疏查询作为类别级 6dof 姿态估计的隐式形状先验,” 在 ICCV,
    2023。'
- en: '[239] B. Wan and Y. Shi, “Socs: Semantically-aware object coordinate space
    for category-level 6d object pose estimation under large shape variations,” in
    ICCV, 2023.'
  id: totrans-1448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] B. Wan 和 Y. Shi, “socs: 对类别级 6d 物体姿态估计的语义感知物体坐标空间，在大形状变化下,” 在 ICCV, 2023。'
- en: '[240] X. Lin and W. Yang, “Instance-adaptive and geometric-aware keypoint learning
    for category-level 6d object pose estimation,” in CVPR, 2024.'
  id: totrans-1449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] X. Lin 和 W. Yang, “实例自适应和几何感知关键点学习用于类别级 6d 物体姿态估计,” 在 CVPR, 2024。'
- en: '[241] Y. Li and K. Mo, “Category-level multi-part multi-joint 3d shape assembly,”
    in CVPR, 2024.'
  id: totrans-1450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] Y. Li 和 K. Mo, “类别级多部件多关节 3d 形状组装,” 在 CVPR, 2024。'
- en: '[242] C. Chi and S. Song, “Garmentnets: Category-level pose estimation for
    garments via canonical space shape completion,” in ICCV, 2021.'
  id: totrans-1451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] C. Chi 和 S. Song, “Garmentnets: 通过典型空间形状补全进行服装的类别级姿态估计,” 在 ICCV, 2021。'
- en: '[243] L. Liu and J. Du, “Category-level articulated object 9d pose estimation
    via reinforcement learning,” in ACM MM, 2023.'
  id: totrans-1452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] L. Liu 和 J. Du, “通过强化学习进行类别级关节物体 9d 姿态估计,” 在 ACM MM, 2023。'
- en: '[244] X. Liu and J. Zhang, “Self-supervised category-level articulated object
    pose estimation with part-level se (3) equivariance,” ICLR, 2023.'
  id: totrans-1453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] X. Liu 和 J. Zhang, “自监督类别级关节物体姿态估计与部分级 se (3) 等变性,” ICLR, 2023。'
- en: '[245] X. Li and Y. Weng, “Leveraging se (3) equivariance for self-supervised
    category-level object pose estimation from point clouds,” in NeurIPS, 2021.'
  id: totrans-1454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] X. Li 和 Y. Weng, “利用 se (3) 等变性进行自监督类别级物体姿态估计从点云中,” 在 NeurIPS, 2021。'
- en: '[246] D. Chen and J. Li, “Learning canonical shape space for category-level
    6d object pose and size estimation,” in CVPR, 2020.'
  id: totrans-1455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] D. Chen 和 J. Li, “学习类别级 6d 物体姿态和尺寸估计的典型形状空间,” 在 CVPR, 2020。'
- en: '[247] J. Lin and H. Li, “Sparse steerable convolutions: An efficient learning
    of se (3)-equivariant features for estimation and tracking of object poses in
    3d space,” in NeurIPS, 2021.'
  id: totrans-1456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] J. Lin 和 H. Li, “稀疏可操控卷积: 高效学习 se (3)-等变特征用于 3d 空间中物体姿态的估计和跟踪,” 在 NeurIPS,
    2021。'
- en: '[248] H. Wang and W. Li, “Attention-guided rgb-d fusion network for category-level
    6d object pose estimation,” in IROS, 2022.'
  id: totrans-1457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] H. Wang 和 W. Li, “基于注意力的 RGB-D 融合网络用于类别级 6d 物体姿态估计,” 在 IROS, 2022。'
- en: '[249] M. Oquab and T. Darcet, “Dinov2: Learning robust visual features without
    supervision,” arXiv preprint arXiv:2304.07193, 2023.'
  id: totrans-1458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] M. Oquab 和 T. Darcet, “Dinov2: 学习鲁棒视觉特征无监督,” arXiv 预印本 arXiv:2304.07193,
    2023。'
- en: '[250] J. J. Park and P. Florence, “Deepsdf: Learning continuous signed distance
    functions for shape representation,” in CVPR, 2019.'
  id: totrans-1459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] J. J. Park 和 P. Florence, “Deepsdf: 学习连续签名距离函数以进行形状表示,” 在 CVPR, 2019。'
- en: '[251] Y. Zhang and Z. Wu, “A transductive approach for video object segmentation,”
    in CVPR, 2020.'
  id: totrans-1460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] Y. Zhang 和 Z. Wu, “一种用于视频物体分割的迁移方法,” 在 CVPR, 2020。'
- en: '[252] Y. Ono and E. Trulls, “Lf-net: Learning local features from images,”
    in NeurIPS, 2018.'
  id: totrans-1461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] Y. Ono 和 E. Trulls, “Lf-net: 从图像中学习局部特征,” 在 NeurIPS, 2018。'
- en: '[253] X. Chen and Z. Dong, “Category level object pose estimation via neural
    analysis-by-synthesis,” in ECCV, 2020.'
  id: totrans-1462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] X. Chen 和 Z. Dong, “通过神经分析-合成进行类别级物体姿态估计,” 在 ECCV, 2020。'
- en: '[254] L. Yen-Chen and P. Florence, “inerf: Inverting neural radiance fields
    for pose estimation,” in IROS, 2021.'
  id: totrans-1463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] L. Yen-Chen 和 P. Florence, “inerf: 反演神经辐射场以进行姿态估计,” 在 IROS, 2021。'
- en: '[255] Y. Lin and J. Tremblay, “Single-stage keypoint-based category-level object
    pose estimation from an rgb image,” in ICRA, 2022.'
  id: totrans-1464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] Y. Lin 和 J. Tremblay, “基于关键点的单阶段类别级物体姿态估计从 RGB 图像中,” 在 ICRA, 2022。'
- en: '[256] J. Guo and F. Zhong, “A visual navigation perspective for category-level
    object pose estimation,” in ECCV, 2022.'
  id: totrans-1465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] J. Guo 和 F. Zhong, “从视觉导航的角度看类别级物体姿态估计,” 在 ECCV, 2022。'
- en: '[257] W. Ma and A. Wang, “Robust category-level 6d pose estimation with coarse-to-fine
    rendering of neural features,” in ECCV, 2022.'
  id: totrans-1466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] W. Ma 和 A. Wang, “通过神经特征的粗到细渲染进行鲁棒的类别级 6d 姿态估计,” 在 ECCV, 2022。'
- en: '[258] H. Zhang and A. Opipari, “Transnet: Category-level transparent object
    pose estimation,” in ECCV, 2022.'
  id: totrans-1467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] H. Zhang 和 A. Opipari, “Transnet: 类别级透明物体姿态估计,” 在 ECCV, 2022。'
- en: '[259] Y. Lin and J. Tremblay, “Keypoint-based category-level object pose tracking
    from an rgb sequence with uncertainty estimation,” in ICRA, 2022.'
  id: totrans-1468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] Y. Lin 和 J. Tremblay，“基于关键点的类别级别物体姿态跟踪，通过 rgb 序列进行不确定性估计，” 在 ICRA，2022。'
- en: '[260] S. Yu and D.-H. Zhai, “Cattrack: Single-stage category-level 6d object
    pose tracking via convolution and vision transformer,” IEEE TMM, 2023.'
  id: totrans-1469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] S. Yu 和 D.-H. Zhai，“Cattrack：通过卷积和视觉变换器的单阶段类别级别 6d 物体姿态跟踪，” IEEE TMM，2023。'
- en: '[261] W. Goodwin and S. Vaze, “Zero-shot category-level object pose estimation,”
    in ECCV, 2022.'
  id: totrans-1470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] W. Goodwin 和 S. Vaze，“零-shot 类别级别物体姿态估计，” 在 ECCV，2022。'
- en: '[262] M. Zaccaria and F. Manhardt, “Self-supervised category-level 6d object
    pose estimation with optical flow consistency,” IEEE RAL, 2023.'
  id: totrans-1471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] M. Zaccaria 和 F. Manhardt，“自监督类别级别 6d 物体姿态估计与光流一致性，” IEEE RAL，2023。'
- en: '[263] J. Cai and Y. He, “Ov9d: Open-vocabulary category-level 9d object pose
    and size estimation,” arXiv preprint arXiv:2403.12396, 2024.'
  id: totrans-1472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] J. Cai 和 Y. He，“Ov9d：开放词汇类别级别 9d 物体姿态和尺寸估计，” arXiv 预印本 arXiv:2403.12396，2024。'
- en: '[264] F. Di Felice and A. Remus, “Zero123-6d: Zero-shot novel view synthesis
    for rgb category-level 6d pose estimation,” arXiv preprint arXiv:2403.14279, 2024.'
  id: totrans-1473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] F. Di Felice 和 A. Remus，“Zero123-6d：用于 rgb 类别级别 6d 姿态估计的零-shot 新视图合成，”
    arXiv 预印本 arXiv:2403.14279，2024。'
- en: '[265] G. Pitteri and S. Ilic, “Cornet: Generic 3d corners for 6d pose estimation
    of new objects without retraining,” in ICCVW, 2019.'
  id: totrans-1474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] G. Pitteri 和 S. Ilic，“Cornet：用于新物体的 6d 姿态估计的通用 3d 角点，无需重新训练，” 在 ICCVW，2019。'
- en: '[266] G. Pitteri and A. Bugeau, “3d object detection and pose estimation of
    unseen objects in color images with local surface embeddings,” in ACCV, 2020.'
  id: totrans-1475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] G. Pitteri 和 A. Bugeau，“具有局部表面嵌入的颜色图像中未见物体的 3d 检测和姿态估计，” 在 ACCV，2020。'
- en: '[267] M. Gou and H. Pan, “Unseen object 6d pose estimation: A benchmark and
    baselines,” arXiv preprint arXiv:2206.11808, 2022.'
  id: totrans-1476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] M. Gou 和 H. Pan，“未见物体 6d 姿态估计：基准和基线，” arXiv 预印本 arXiv:2206.11808，2022。'
- en: '[268] F. Hagelskjær and R. L. Haugaard, “Keymatchnet: Zero-shot pose estimation
    in 3d point clouds by generalized keypoint matching,” arXiv preprint arXiv:2303.16102,
    2023.'
  id: totrans-1477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] F. Hagelskjær 和 R. L. Haugaard，“Keymatchnet：通过广义关键点匹配进行 3d 点云中的零-shot
    姿态估计，” arXiv 预印本 arXiv:2303.16102，2023。'
- en: '[269] H. Zhao and S. Wei, “Learning symmetry-aware geometry correspondences
    for 6d object pose estimation,” in ICCV, 2023.'
  id: totrans-1478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] H. Zhao 和 S. Wei，“学习对称感知几何对应关系进行 6d 物体姿态估计，” 在 ICCV，2023。'
- en: '[270] K. He and G. Gkioxari, “Mask r-cnn,” in ICCV, 2017.'
  id: totrans-1479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] K. He 和 G. Gkioxari，“Mask r-cnn，” 在 ICCV，2017。'
- en: '[271] J. Chen and M. Sun, “Zeropose: Cad-model-based zero-shot pose estimation,”
    arXiv preprint arXiv:2305.17934, 2023.'
  id: totrans-1480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] J. Chen 和 M. Sun，“Zeropose：基于 cad 模型的零-shot 姿态估计，” arXiv 预印本 arXiv:2305.17934，2023。'
- en: '[272] A. Kirillov and E. Mintun, “Segment anything,” in ICCV, 2023.'
  id: totrans-1481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] A. Kirillov 和 E. Mintun，“Segment anything，” 在 ICCV，2023。'
- en: '[273] Z. Qin and H. Yu, “Geometric transformer for fast and robust point cloud
    registration,” in CVPR, 2022.'
  id: totrans-1482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] Z. Qin 和 H. Yu，“几何变换器用于快速且鲁棒的点云配准，” 在 CVPR，2022。'
- en: '[274] A. Caraffa and D. Boscaini, “Freeze: Training-free zero-shot 6d pose
    estimation with geometric and vision foundation models,” arXiv preprint arXiv:2312.00947,
    2024.'
  id: totrans-1483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] A. Caraffa 和 D. Boscaini，“Freeze：无训练零-shot 6d 姿态估计与几何和视觉基础模型，” arXiv
    预印本 arXiv:2312.00947，2024。'
- en: '[275] J. Huang and H. Yu, “Matchu: Matching unseen objects for 6d pose estimation
    from rgb-d images,” in CVPR, 2024.'
  id: totrans-1484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] J. Huang 和 H. Yu，“Matchu：从 rgb-d 图像中匹配未见物体进行 6d 姿态估计，” 在 CVPR，2024。'
- en: '[276] V. N. Nguyen and T. Groueix, “Cnos: A strong baseline for cad-based novel
    object segmentation,” in ICCV, 2023.'
  id: totrans-1485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] V. N. Nguyen 和 T. Groueix，“Cnos：基于 cad 的新颖物体分割的强基线，” 在 ICCV，2023。'
- en: '[277] I. Shugurov and F. Li, “Osop: A multi-stage one shot object pose estimation
    framework,” in CVPR, 2022.'
  id: totrans-1486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] I. Shugurov 和 F. Li，“Osop：一个多阶段一次性物体姿态估计框架，” 在 CVPR，2022。'
- en: '[278] B. Okorn and Q. Gu, “Zephyr: Zero-shot pose hypothesis rating,” in ICRA,
    2021.'
  id: totrans-1487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] B. Okorn 和 Q. Gu，“Zephyr：零-shot 姿态假设评分，” 在 ICRA，2021。'
- en: '[279] E. P. Örnek and Y. Labbé, “Foundpose: Unseen object pose estimation with
    foundation features,” arXiv preprint arXiv:2311.18809, 2023.'
  id: totrans-1488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] E. P. Örnek 和 Y. Labbé，“Foundpose：使用基础特征的未见物体姿态估计，” arXiv 预印本 arXiv:2311.18809，2023。'
- en: '[280] M. Sundermeyer and M. Durner, “Multi-path learning for object pose estimation
    across domains,” in CVPR, 2020.'
  id: totrans-1489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] M. Sundermeyer 和 M. Durner，“跨领域物体姿态估计的多路径学习，” 在 CVPR，2020。'
- en: '[281] V. N. Nguyen and Y. Hu, “Templates for 3d object pose estimation revisited:
    Generalization to new objects and robustness to occlusions,” in CVPR, 2022.'
  id: totrans-1490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] V. N. Nguyen 和 Y. Hu，“重新审视 3d 物体姿态估计模板：对新物体的泛化和对遮挡的鲁棒性，” 在 CVPR，2022。'
- en: '[282] S. Moon and H. Son, “Genflow: Generalizable recurrent flow for 6d pose
    refinement of novel objects,” in CVPR, 2024.'
  id: totrans-1491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] S. Moon 和 H. Son，“Genflow：用于新物体 6D 姿态优化的可泛化递归流，”在 CVPR，2024。'
- en: '[283] T. Wang and G. Hu, “Object pose estimation via the aggregation of diffusion
    features,” in CVPR, 2024.'
  id: totrans-1492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[283] T. Wang 和 G. Hu，“通过扩散特征聚合进行物体姿态估计，”在 CVPR，2024。'
- en: '[284] V. Balntas and A. Doumanoglou, “Pose guided rgbd feature learning for
    3d object pose estimation,” in ICCV, 2017.'
  id: totrans-1493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[284] V. Balntas 和 A. Doumanoglou，“用于 3D 物体姿态估计的姿态引导 RGBD 特征学习，”在 ICCV，2017。'
- en: '[285] Y. Wen and X. Li, “Disp6d: Disentangled implicit shape and pose learning
    for scalable 6d pose estimation,” in ECCV, 2022.'
  id: totrans-1494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[285] Y. Wen 和 X. Li，“Disp6d：用于可扩展 6D 姿态估计的解耦隐式形状和姿态学习，”在 ECCV，2022。'
- en: '[286] B. Busam and H. J. Jung, “I like to move it: 6d pose estimation as an
    action decision process,” arXiv preprint arXiv:2009.12678, 2020.'
  id: totrans-1495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[286] B. Busam 和 H. J. Jung，“我喜欢移动它：将 6D 姿态估计作为行动决策过程，”arXiv 预印本 arXiv:2009.12678，2020。'
- en: '[287] D. Cai and J. Heikkilä, “Ove6d: Object viewpoint encoding for depth-based
    6d object pose estimation,” in CVPR, 2022.'
  id: totrans-1496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[287] D. Cai 和 J. Heikkilä，“Ove6d：用于基于深度的 6D 物体姿态估计的物体视点编码，”在 CVPR，2022。'
- en: '[288] W. Kabsch, “A solution for the best rotation to relate two sets of vectors,”
    Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and
    General Crystallography, 1976.'
  id: totrans-1497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[288] W. Kabsch，“有关将两组向量相关的最佳旋转的解决方案，”《晶体学学报 A：晶体物理学、衍射、理论与一般晶体学》，1976。'
- en: '[289] E. Corona and K. Kundu, “Pose estimation for objects with rotational
    symmetry,” in IROS, 2018.'
  id: totrans-1498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[289] E. Corona 和 K. Kundu，“具有旋转对称性的物体姿态估计，”在 IROS，2018。'
- en: '[290] C. Zhao and Y. Hu, “Fusing local similarities for retrieval-based 3d
    orientation estimation of unseen objects,” in ECCV, 2022.'
  id: totrans-1499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[290] C. Zhao 和 Y. Hu，“融合局部相似性进行未见物体的 3D 方向估计，”在 ECCV，2022。'
- en: '[291] S. Thalhammer and J.-B. Weibel, “Self-supervised vision transformers
    for 3d pose estimation of novel objects,” Image Vision Comput, 2023.'
  id: totrans-1500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[291] S. Thalhammer 和 J.-B. Weibel，“用于新物体 3D 姿态估计的自监督视觉变换器，”图像视觉计算，2023。'
- en: '[292] P. Ausserlechner and D. Haberger, “Zs6d: Zero-shot 6d object pose estimation
    using vision transformers,” arXiv preprint arXiv:2309.11986, 2023.'
  id: totrans-1501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[292] P. Ausserlechner 和 D. Haberger，“Zs6d：使用视觉变换器进行零样本 6D 物体姿态估计，”arXiv 预印本
    arXiv:2309.11986，2023。'
- en: '[293] A. Dosovitskiy and L. Beyer, “An image is worth 16x16 words: Transformers
    for image recognition at scale,” in CoLR, 2020.'
  id: totrans-1502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[293] A. Dosovitskiy 和 L. Beyer，“一张图像值 16x16 个词：用于大规模图像识别的变换器，”在 CoLR，2020。'
- en: '[294] J. Tremblay and B. Wen, “Diff-dope: Differentiable deep object pose estimation,”
    arXiv preprint arXiv:2310.00463, 2023.'
  id: totrans-1503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[294] J. Tremblay 和 B. Wen，“Diff-dope：可微分的深度物体姿态估计，”arXiv 预印本 arXiv:2310.00463，2023。'
- en: '[295] Ultralytics, “GitHub - ultralytics/yolov5.” [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5),
    2024.'
  id: totrans-1504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[295] Ultralytics，“GitHub - ultralytics/yolov5。” [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5)，2024。'
- en: '[296] Y. He and Y. Wang, “Fs6d: Few-shot 6d pose estimation of novel objects,”
    in CVPR, 2022.'
  id: totrans-1505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[296] Y. He 和 Y. Wang，“Fs6d：新物体的少样本 6D 姿态估计，”在 CVPR，2022。'
- en: '[297] P. Castro and T.-K. Kim, “Posematcher: One-shot 6d object pose estimation
    by deep feature matching,” in ICCVW, 2023.'
  id: totrans-1506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[297] P. Castro 和 T.-K. Kim，“Posematcher：通过深度特征匹配进行单次 6D 物体姿态估计，”在 ICCVW，2023。'
- en: '[298] J. Lee and Y. Cabon, “Mfos: Model-free & one-shot object pose estimation,”
    in AAAI, 2024.'
  id: totrans-1507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[298] J. Lee 和 Y. Cabon，“Mfos：无模型和单次物体姿态估计，”在 AAAI，2024。'
- en: '[299] Y. Du and Y. Xiao, “Pizza: A powerful image-only zero-shot zero-cad approach
    to 6 dof tracking,” in 3DV, 2022.'
  id: totrans-1508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[299] Y. Du 和 Y. Xiao，“Pizza：一种强大的仅图像零样本零 CAD 方法进行 6 自由度跟踪，”在 3DV，2022。'
- en: '[300] N. Gao and V. A. Ngo, “Sa6d: Self-adaptive few-shot 6d pose estimator
    for novel and occluded objects,” in CoRL, 2023.'
  id: totrans-1509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[300] N. Gao 和 V. A. Ngo，“Sa6d：自适应少样本 6D 姿态估计器，用于新物体和遮挡物体，”在 CoRL，2023。'
- en: '[301] D. Cai and J. Heikkilä, “Gs-pose: Cascaded framework for generalizable
    segmentation-based 6d object pose estimation,” arXiv preprint arXiv:2403.10683,
    2024.'
  id: totrans-1510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[301] D. Cai 和 J. Heikkilä，“Gs-pose：用于可泛化的分割基础 6D 物体姿态估计的级联框架，”arXiv 预印本 arXiv:2403.10683，2024。'
- en: '[302] C. Jaime and B. Davide, “Open-vocabulary object 6d pose estimation,”
    in CVPR, 2024.'
  id: totrans-1511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[302] C. Jaime 和 B. Davide，“开放词汇 6D 姿态估计，”在 CVPR，2024。'
- en: '[303] J. Wu and Y. Wang, “Unseen object pose estimation via registration,”
    in RCAR, 2021.'
  id: totrans-1512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[303] J. Wu 和 Y. Wang，“通过配准进行未见物体姿态估计，”在 RCAR，2021。'
- en: '[304] J. Sun and Z. Shen, “Loftr: Detector-free local feature matching with
    transformers,” in CVPR, 2021.'
  id: totrans-1513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[304] J. Sun 和 Z. Shen，“Loftr：无检测器的局部特征匹配与变换器，”在 CVPR，2021。'
- en: '[305] P.-E. Sarlin and D. DeTone, “Superglue: Learning feature matching with
    graph neural networks,” in CVPR, 2020.'
  id: totrans-1514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[305] P.-E. Sarlin 和 D. DeTone，“Superglue：使用图神经网络学习特征匹配，”在 CVPR，2020。'
- en: '[306] Z. Fan and P. Pan, “Pope: 6-dof promptable pose estimation of any object,
    in any scene, with one reference,” arXiv preprint arXiv:2305.15727, 2023.'
  id: totrans-1515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[306] Z. Fan 和 P. Pan，“Pope：任何物体、任何场景中的 6-dof 可提示姿态估计，利用一个参考”，arXiv 预印本 arXiv:2305.15727，2023。'
- en: '[307] P. Weinzaepfel and V. Leroy, “Croco: Self-supervised pre-training for
    3d vision tasks by cross-view completion,” in NeurIPS, 2022.'
  id: totrans-1516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[307] P. Weinzaepfel 和 V. Leroy，“Croco：通过交叉视图补全进行自监督预训练以解决 3d 视觉任务”，发表于 NeurIPS，2022。'
- en: '[308] C. Zhao and Y. Hu, “Locposenet: Robust location prior for unseen object
    pose estimation,” in 3DV, 2024.'
  id: totrans-1517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[308] C. Zhao 和 Y. Hu，“Locposenet：针对未见物体姿态估计的稳健位置先验”，发表于 3DV，2024。'
- en: '[309] P. Pan and Z. Fan, “Learning to estimate 6dof pose from limited data:
    A few-shot, generalizable approach using rgb images,” in 3DV, 2024.'
  id: totrans-1518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[309] P. Pan 和 Z. Fan，“从有限数据中学习估计 6dof 姿态：一种基于 RGB 图像的少量样本、可泛化方法”，发表于 3DV，2024。'
- en: '[310] B. Wen and J. Tremblay, “Bundlesdf: Neural 6-dof tracking and 3d reconstruction
    of unknown objects,” in CVPR, 2023.'
  id: totrans-1519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[310] B. Wen 和 J. Tremblay，“Bundlesdf：对未知物体的神经 6-dof 跟踪和 3d 重建”，发表于 CVPR，2023。'
- en: '[311] V. N. Nguyen and T. Groueix, “Nope: Novel object pose estimation from
    a single image,” in CVPR, 2024.'
  id: totrans-1520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311] V. N. Nguyen 和 T. Groueix，“Nope：从单张图像中估计新物体姿态”，发表于 CVPR，2024。'
- en: '[312] K. Park and T. Patten, “Neural object learning for 6d pose estimation
    using a few cluttered images,” in ECCV, 2020.'
  id: totrans-1521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[312] K. Park 和 T. Patten，“利用少量杂乱图像进行 6d 姿态估计的神经对象学习”，发表于 ECCV，2020。'
- en: '[313] H. Chen and F. Manhardt, “Texpose: Neural texture learning for self-supervised
    6d object pose estimation,” in CVPR, 2023.'
  id: totrans-1522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[313] H. Chen 和 F. Manhardt，“Texpose：用于自监督 6d 物体姿态估计的神经纹理学习”，发表于 CVPR，2023。'
- en: '[314] Y. Li and J. Sun, “Weakly supervised 6d pose estimation for robotic grasping,”
    in SIGGRAPH, 2018.'
  id: totrans-1523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[314] Y. Li 和 J. Sun，“针对机器人抓取的弱监督 6d 姿态估计”，发表于 SIGGRAPH，2018。'
- en: '[315] K. Chen and R. Cao, “Sim-to-real 6d object pose estimation via iterative
    self-training for robotic bin picking,” in ECCV, 2022.'
  id: totrans-1524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[315] K. Chen 和 R. Cao，“通过迭代自我训练实现的仿真到现实 6d 物体姿态估计，用于机器人箱体拾取”，发表于 ECCV，2022。'
- en: '[316] B. Fu and S. K. Leong, “6d robotic assembly based on rgb-only object
    pose estimation,” in IROS, 2022.'
  id: totrans-1525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[316] B. Fu 和 S. K. Leong，“基于仅 RGB 的物体姿态估计的 6d 机器人装配”，发表于 IROS，2022。'
- en: '[317] J. Tremblay and T. To, “Deep object pose estimation for semantic robotic
    grasping of household objects,” arXiv preprint arXiv:1809.10790, 2018.'
  id: totrans-1526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[317] J. Tremblay 和 T. To，“用于家用物体语义机器人抓取的深度物体姿态估计”，arXiv 预印本 arXiv:1809.10790，2018。'
- en: '[318] Z. Dong and S. Liu, “Ppr-net: Point-wise pose regression network for
    instance segmentation and 6dd pose estimation in bin-picking scenarios,” in IROS,
    2019.'
  id: totrans-1527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[318] Z. Dong 和 S. Liu，“Ppr-net：用于实例分割和 6d 姿态估计的点对点姿态回归网络，在箱体拾取场景中”，发表于 IROS，2019。'
- en: '[319] C. Zhuang and H. Wang, “Attentionvote: A coarse-to-fine voting network
    of anchor-free 6d pose estimation on point cloud for robotic bin-picking application,”
    Robot Cim-int Manuf, 2024.'
  id: totrans-1528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[319] C. Zhuang 和 H. Wang，“Attentionvote：用于机器人箱体拾取应用的无锚点 6d 姿态估计的粗到细投票网络”，Robot
    Cim-int Manuf，2024。'
- en: '[320] K. Wada and E. Sucar, “Morefusion: Multi-object reasoning for 6d pose
    estimation from volumetric fusion,” in CVPR, 2020.'
  id: totrans-1529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[320] K. Wada 和 E. Sucar，“Morefusion：基于体积融合的 6d 姿态估计的多物体推理”，发表于 CVPR，2020。'
- en: '[321] H. Zhang and Q. Cao, “Detect in rgb, optimize in edge: Accurate 6d pose
    estimation for texture-less industrial parts,” in ICRA, 2019.'
  id: totrans-1530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[321] H. Zhang 和 Q. Cao，“在 RGB 中检测，在边缘中优化：纹理无工业部件的准确 6d 姿态估计”，发表于 ICRA，2019。'
- en: '[322] J. Chang and M. Kim, “Ghostpose: Multi-view pose estimation of transparent
    objects for robot hand grasping,” in IROS, 2021.'
  id: totrans-1531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[322] J. Chang 和 M. Kim，“Ghostpose：用于机器人手抓取的透明物体的多视角姿态估计”，发表于 IROS，2021。'
- en: '[323] J. Kim and H. Pyo, “Tomato harvesting robotic system based on deep-tomatos:
    Deep learning network using transformation loss for 6d pose estimation of maturity
    classified tomatoes with side-stem,” Comput Electron Agric, 2022.'
  id: totrans-1532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[323] J. Kim 和 H. Pyo，“基于 Deep-tomatos 的番茄采摘机器人系统：使用变换损失的深度学习网络，针对带侧枝的成熟分类番茄进行
    6d 姿态估计”，Comput Electron Agric，2022。'
- en: '[324] C. Liu and W. Sun, “Fine segmentation and difference-aware shape adjustment
    for category-level 6dof object pose estimation,” Appl Intell, 2023.'
  id: totrans-1533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[324] C. Liu 和 W. Sun，“针对类别级 6dof 物体姿态估计的精细分割和差异感知形状调整”，Appl Intell，2023。'
- en: '[325] S. Yu and D.-H. Zhai, “Category-level 6-d object pose estimation with
    shape deformation for robotic grasp detection,” IEEE TNNLS, 2023.'
  id: totrans-1534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[325] S. Yu 和 D.-H. Zhai，“带有形状变形的类别级 6-d 物体姿态估计，用于机器人抓取检测”，IEEE TNNLS，2023。'
- en: '[326] J. Sun and Y. Wang, “Ick-track: A category-level 6-dof pose tracker using
    inter-frame consistent keypoints for aerial manipulation,” in IROS, 2022.'
  id: totrans-1535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[326] J. Sun 和 Y. Wang，“Ick-track：利用帧间一致的关键点进行的类别级 6-dof 姿态跟踪，用于空中操作”，发表于 IROS，2022。'
- en: '[327] S. Yu and D.-H. Zhai, “Robotic grasp detection based on category-level
    object pose estimation with self-supervised learning,” IEEE TMEC, 2023.'
  id: totrans-1536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[327] S. Yu 和 D.-H. Zhai，“基于类别级物体姿态估计的机器人抓取检测，结合自监督学习”，发表于IEEE TMEC, 2023。'
- en: '[328] Y. Su and J. Rambach, “Deep multi-state object pose estimation for augmented
    reality assembly,” in ISMAR-Adjunct, 2019.'
  id: totrans-1537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[328] Y. Su 和 J. Rambach，“增强现实装配中的深度多状态物体姿态估计”，发表于ISMAR-Adjunct, 2019。'
- en: '[329] R. Pandey and P. Pidlypenskyi, “Efficient 6-dof tracking of handheld
    objects from an egocentric viewpoint,” in ECCV, 2018.'
  id: totrans-1538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[329] R. Pandey 和 P. Pidlypenskyi，“从自我中心视角高效追踪手持物体的6-dof”，发表于ECCV, 2018。'
- en: '[330] P. F. Proença and Y. Gao, “Deep learning for spacecraft pose estimation
    from photorealistic rendering,” in ICRA, 2020.'
  id: totrans-1539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[330] P. F. Proença 和 Y. Gao，“从光学逼真渲染中进行航天器姿态估计的深度学习”，发表于ICRA, 2020。'
- en: '[331] U. E. 4, “Unreal engine 4.” [https://www.unrealengine.com](https://www.unrealengine.com).'
  id: totrans-1540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[331] U. E. 4，“虚幻引擎4。” [https://www.unrealengine.com](https://www.unrealengine.com)。'
- en: '[332] S. Wang and S. Wang, “Ca-spacenet: Counterfactual analysis for 6d pose
    estimation in space,” in IROS, 2022.'
  id: totrans-1541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[332] S. Wang 和 S. Wang，“Ca-spacenet：空间中6d姿态估计的反事实分析”，发表于IROS, 2022。'
- en: '[333] M. Ulmer and M. Durner, “6d object pose estimation from approximate 3d
    models for orbital robotics,” in IROS, 2023.'
  id: totrans-1542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[333] M. Ulmer 和 M. Durner，“基于近似3d模型的6d物体姿态估计，用于轨道机器人”，发表于IROS, 2023。'
- en: '[334] T. Patten and K. Park, “Object learning for 6d pose estimation and grasping
    from rgb-d videos of in-hand manipulation,” in IROS, 2021.'
  id: totrans-1543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[334] T. Patten 和 K. Park，“基于rgb-d视频的6d姿态估计和抓取的物体学习”，发表于IROS, 2021。'
- en: '[335] H. Qi and C. Zhao, “Hoisdf: Constraining 3d hand-object pose estimation
    with global signed distance fields,” in CVPR, 2024.'
  id: totrans-1544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[335] H. Qi 和 C. Zhao，“Hoisdf：利用全局有符号距离场约束3d手-物体姿态估计”，发表于CVPR, 2024。'
- en: '[336] S. Hoque and S. Xu, “Deep learning for 6d pose estimation of objects-a
    case study for autonomous driving,” Expert Syst Appl, 2023.'
  id: totrans-1545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[336] S. Hoque 和 S. Xu，“用于6d物体姿态估计的深度学习——以自动驾驶为例”，发表于Expert Syst Appl, 2023。'
- en: '[337] J. L. Elman, “Finding structure in time,” Cogn Sci, 1990.'
  id: totrans-1546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[337] J. L. Elman，“发现时间中的结构”，发表于Cogn Sci, 1990。'
- en: '[338] H. Sun and P. Ni, “Panelpose: A 6d pose estimation of highly-variable
    panel object for robotic robust cockpit panel inspection,” in IROS, 2023.'
  id: totrans-1547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[338] H. Sun 和 P. Ni，“Panelpose：一种用于机器人稳健驾驶舱面板检查的高度可变面板物体的6d姿态估计”，发表于IROS,
    2023。'
